creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8a2cc9ca90>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.674, val_acc:0.048]
Epoch [2/120    avg_loss:2.602, val_acc:0.185]
Epoch [3/120    avg_loss:2.529, val_acc:0.342]
Epoch [4/120    avg_loss:2.471, val_acc:0.404]
Epoch [5/120    avg_loss:2.419, val_acc:0.406]
Epoch [6/120    avg_loss:2.377, val_acc:0.410]
Epoch [7/120    avg_loss:2.318, val_acc:0.425]
Epoch [8/120    avg_loss:2.288, val_acc:0.487]
Epoch [9/120    avg_loss:2.239, val_acc:0.535]
Epoch [10/120    avg_loss:2.203, val_acc:0.556]
Epoch [11/120    avg_loss:2.157, val_acc:0.585]
Epoch [12/120    avg_loss:2.107, val_acc:0.562]
Epoch [13/120    avg_loss:2.043, val_acc:0.575]
Epoch [14/120    avg_loss:1.993, val_acc:0.583]
Epoch [15/120    avg_loss:1.925, val_acc:0.571]
Epoch [16/120    avg_loss:1.864, val_acc:0.579]
Epoch [17/120    avg_loss:1.796, val_acc:0.610]
Epoch [18/120    avg_loss:1.717, val_acc:0.633]
Epoch [19/120    avg_loss:1.660, val_acc:0.646]
Epoch [20/120    avg_loss:1.580, val_acc:0.662]
Epoch [21/120    avg_loss:1.534, val_acc:0.658]
Epoch [22/120    avg_loss:1.456, val_acc:0.704]
Epoch [23/120    avg_loss:1.411, val_acc:0.710]
Epoch [24/120    avg_loss:1.356, val_acc:0.710]
Epoch [25/120    avg_loss:1.267, val_acc:0.702]
Epoch [26/120    avg_loss:1.188, val_acc:0.779]
Epoch [27/120    avg_loss:1.110, val_acc:0.769]
Epoch [28/120    avg_loss:1.059, val_acc:0.783]
Epoch [29/120    avg_loss:0.988, val_acc:0.771]
Epoch [30/120    avg_loss:0.984, val_acc:0.806]
Epoch [31/120    avg_loss:0.930, val_acc:0.787]
Epoch [32/120    avg_loss:0.895, val_acc:0.846]
Epoch [33/120    avg_loss:0.876, val_acc:0.850]
Epoch [34/120    avg_loss:0.802, val_acc:0.812]
Epoch [35/120    avg_loss:0.754, val_acc:0.935]
Epoch [36/120    avg_loss:0.690, val_acc:0.927]
Epoch [37/120    avg_loss:0.653, val_acc:0.933]
Epoch [38/120    avg_loss:0.661, val_acc:0.838]
Epoch [39/120    avg_loss:0.624, val_acc:0.931]
Epoch [40/120    avg_loss:0.545, val_acc:0.921]
Epoch [41/120    avg_loss:0.519, val_acc:0.917]
Epoch [42/120    avg_loss:0.534, val_acc:0.929]
Epoch [43/120    avg_loss:0.519, val_acc:0.910]
Epoch [44/120    avg_loss:0.503, val_acc:0.938]
Epoch [45/120    avg_loss:0.473, val_acc:0.927]
Epoch [46/120    avg_loss:0.453, val_acc:0.933]
Epoch [47/120    avg_loss:0.437, val_acc:0.927]
Epoch [48/120    avg_loss:0.447, val_acc:0.938]
Epoch [49/120    avg_loss:0.450, val_acc:0.938]
Epoch [50/120    avg_loss:0.410, val_acc:0.942]
Epoch [51/120    avg_loss:0.359, val_acc:0.929]
Epoch [52/120    avg_loss:0.363, val_acc:0.950]
Epoch [53/120    avg_loss:0.326, val_acc:0.942]
Epoch [54/120    avg_loss:0.339, val_acc:0.956]
Epoch [55/120    avg_loss:0.311, val_acc:0.958]
Epoch [56/120    avg_loss:0.315, val_acc:0.948]
Epoch [57/120    avg_loss:0.321, val_acc:0.954]
Epoch [58/120    avg_loss:0.319, val_acc:0.950]
Epoch [59/120    avg_loss:0.256, val_acc:0.950]
Epoch [60/120    avg_loss:0.261, val_acc:0.948]
Epoch [61/120    avg_loss:0.316, val_acc:0.942]
Epoch [62/120    avg_loss:0.343, val_acc:0.954]
Epoch [63/120    avg_loss:0.305, val_acc:0.958]
Epoch [64/120    avg_loss:0.251, val_acc:0.958]
Epoch [65/120    avg_loss:0.252, val_acc:0.973]
Epoch [66/120    avg_loss:0.234, val_acc:0.946]
Epoch [67/120    avg_loss:0.254, val_acc:0.954]
Epoch [68/120    avg_loss:0.238, val_acc:0.956]
Epoch [69/120    avg_loss:0.196, val_acc:0.979]
Epoch [70/120    avg_loss:0.195, val_acc:0.975]
Epoch [71/120    avg_loss:0.177, val_acc:0.969]
Epoch [72/120    avg_loss:0.191, val_acc:0.979]
Epoch [73/120    avg_loss:0.215, val_acc:0.946]
Epoch [74/120    avg_loss:0.197, val_acc:0.960]
Epoch [75/120    avg_loss:0.179, val_acc:0.975]
Epoch [76/120    avg_loss:0.159, val_acc:0.975]
Epoch [77/120    avg_loss:0.164, val_acc:0.975]
Epoch [78/120    avg_loss:0.167, val_acc:0.983]
Epoch [79/120    avg_loss:0.149, val_acc:0.960]
Epoch [80/120    avg_loss:0.176, val_acc:0.963]
Epoch [81/120    avg_loss:0.165, val_acc:0.977]
Epoch [82/120    avg_loss:0.131, val_acc:0.975]
Epoch [83/120    avg_loss:0.170, val_acc:0.983]
Epoch [84/120    avg_loss:0.129, val_acc:0.983]
Epoch [85/120    avg_loss:0.120, val_acc:0.975]
Epoch [86/120    avg_loss:0.114, val_acc:0.985]
Epoch [87/120    avg_loss:0.118, val_acc:0.971]
Epoch [88/120    avg_loss:0.121, val_acc:0.973]
Epoch [89/120    avg_loss:0.150, val_acc:0.973]
Epoch [90/120    avg_loss:0.204, val_acc:0.971]
Epoch [91/120    avg_loss:0.151, val_acc:0.983]
Epoch [92/120    avg_loss:0.146, val_acc:0.983]
Epoch [93/120    avg_loss:0.124, val_acc:0.975]
Epoch [94/120    avg_loss:0.135, val_acc:0.967]
Epoch [95/120    avg_loss:0.124, val_acc:0.973]
Epoch [96/120    avg_loss:0.123, val_acc:0.988]
Epoch [97/120    avg_loss:0.089, val_acc:0.985]
Epoch [98/120    avg_loss:0.133, val_acc:0.981]
Epoch [99/120    avg_loss:0.092, val_acc:0.990]
Epoch [100/120    avg_loss:0.068, val_acc:0.985]
Epoch [101/120    avg_loss:0.074, val_acc:0.983]
Epoch [102/120    avg_loss:0.070, val_acc:0.994]
Epoch [103/120    avg_loss:0.076, val_acc:0.990]
Epoch [104/120    avg_loss:0.117, val_acc:0.981]
Epoch [105/120    avg_loss:0.143, val_acc:0.933]
Epoch [106/120    avg_loss:0.191, val_acc:0.944]
Epoch [107/120    avg_loss:0.194, val_acc:0.981]
Epoch [108/120    avg_loss:0.136, val_acc:0.967]
Epoch [109/120    avg_loss:0.096, val_acc:0.979]
Epoch [110/120    avg_loss:0.083, val_acc:0.988]
Epoch [111/120    avg_loss:0.084, val_acc:0.975]
Epoch [112/120    avg_loss:0.066, val_acc:0.988]
Epoch [113/120    avg_loss:0.065, val_acc:0.992]
Epoch [114/120    avg_loss:0.071, val_acc:0.979]
Epoch [115/120    avg_loss:0.084, val_acc:0.992]
Epoch [116/120    avg_loss:0.065, val_acc:0.994]
Epoch [117/120    avg_loss:0.050, val_acc:0.992]
Epoch [118/120    avg_loss:0.055, val_acc:0.992]
Epoch [119/120    avg_loss:0.046, val_acc:0.992]
Epoch [120/120    avg_loss:0.045, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 0.99780541 0.98871332 1.         0.94372294 0.90780142
 0.99277108 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9919290942637857
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2fb3bb2a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.636, val_acc:0.160]
Epoch [2/120    avg_loss:2.569, val_acc:0.160]
Epoch [3/120    avg_loss:2.503, val_acc:0.308]
Epoch [4/120    avg_loss:2.444, val_acc:0.331]
Epoch [5/120    avg_loss:2.395, val_acc:0.310]
Epoch [6/120    avg_loss:2.347, val_acc:0.304]
Epoch [7/120    avg_loss:2.297, val_acc:0.354]
Epoch [8/120    avg_loss:2.259, val_acc:0.408]
Epoch [9/120    avg_loss:2.211, val_acc:0.454]
Epoch [10/120    avg_loss:2.166, val_acc:0.515]
Epoch [11/120    avg_loss:2.116, val_acc:0.548]
Epoch [12/120    avg_loss:2.056, val_acc:0.573]
Epoch [13/120    avg_loss:2.008, val_acc:0.592]
Epoch [14/120    avg_loss:1.952, val_acc:0.596]
Epoch [15/120    avg_loss:1.868, val_acc:0.613]
Epoch [16/120    avg_loss:1.810, val_acc:0.621]
Epoch [17/120    avg_loss:1.748, val_acc:0.642]
Epoch [18/120    avg_loss:1.660, val_acc:0.642]
Epoch [19/120    avg_loss:1.610, val_acc:0.652]
Epoch [20/120    avg_loss:1.511, val_acc:0.671]
Epoch [21/120    avg_loss:1.440, val_acc:0.704]
Epoch [22/120    avg_loss:1.364, val_acc:0.710]
Epoch [23/120    avg_loss:1.275, val_acc:0.727]
Epoch [24/120    avg_loss:1.197, val_acc:0.762]
Epoch [25/120    avg_loss:1.160, val_acc:0.787]
Epoch [26/120    avg_loss:1.070, val_acc:0.744]
Epoch [27/120    avg_loss:1.009, val_acc:0.856]
Epoch [28/120    avg_loss:0.940, val_acc:0.748]
Epoch [29/120    avg_loss:0.953, val_acc:0.775]
Epoch [30/120    avg_loss:0.891, val_acc:0.883]
Epoch [31/120    avg_loss:0.810, val_acc:0.915]
Epoch [32/120    avg_loss:0.800, val_acc:0.915]
Epoch [33/120    avg_loss:0.718, val_acc:0.906]
Epoch [34/120    avg_loss:0.700, val_acc:0.906]
Epoch [35/120    avg_loss:0.680, val_acc:0.929]
Epoch [36/120    avg_loss:0.642, val_acc:0.933]
Epoch [37/120    avg_loss:0.637, val_acc:0.896]
Epoch [38/120    avg_loss:0.576, val_acc:0.904]
Epoch [39/120    avg_loss:0.554, val_acc:0.925]
Epoch [40/120    avg_loss:0.496, val_acc:0.933]
Epoch [41/120    avg_loss:0.461, val_acc:0.938]
Epoch [42/120    avg_loss:0.452, val_acc:0.929]
Epoch [43/120    avg_loss:0.447, val_acc:0.931]
Epoch [44/120    avg_loss:0.510, val_acc:0.908]
Epoch [45/120    avg_loss:0.482, val_acc:0.902]
Epoch [46/120    avg_loss:0.422, val_acc:0.927]
Epoch [47/120    avg_loss:0.397, val_acc:0.940]
Epoch [48/120    avg_loss:0.403, val_acc:0.935]
Epoch [49/120    avg_loss:0.374, val_acc:0.919]
Epoch [50/120    avg_loss:0.337, val_acc:0.931]
Epoch [51/120    avg_loss:0.371, val_acc:0.933]
Epoch [52/120    avg_loss:0.345, val_acc:0.944]
Epoch [53/120    avg_loss:0.284, val_acc:0.950]
Epoch [54/120    avg_loss:0.282, val_acc:0.950]
Epoch [55/120    avg_loss:0.270, val_acc:0.958]
Epoch [56/120    avg_loss:0.295, val_acc:0.954]
Epoch [57/120    avg_loss:0.257, val_acc:0.960]
Epoch [58/120    avg_loss:0.240, val_acc:0.946]
Epoch [59/120    avg_loss:0.277, val_acc:0.954]
Epoch [60/120    avg_loss:0.227, val_acc:0.960]
Epoch [61/120    avg_loss:0.223, val_acc:0.975]
Epoch [62/120    avg_loss:0.247, val_acc:0.975]
Epoch [63/120    avg_loss:0.265, val_acc:0.919]
Epoch [64/120    avg_loss:0.287, val_acc:0.975]
Epoch [65/120    avg_loss:0.215, val_acc:0.977]
Epoch [66/120    avg_loss:0.190, val_acc:0.969]
Epoch [67/120    avg_loss:0.236, val_acc:0.973]
Epoch [68/120    avg_loss:0.235, val_acc:0.983]
Epoch [69/120    avg_loss:0.198, val_acc:0.973]
Epoch [70/120    avg_loss:0.263, val_acc:0.973]
Epoch [71/120    avg_loss:0.203, val_acc:0.971]
Epoch [72/120    avg_loss:0.192, val_acc:0.971]
Epoch [73/120    avg_loss:0.186, val_acc:0.969]
Epoch [74/120    avg_loss:0.170, val_acc:0.979]
Epoch [75/120    avg_loss:0.155, val_acc:0.973]
Epoch [76/120    avg_loss:0.141, val_acc:0.983]
Epoch [77/120    avg_loss:0.149, val_acc:0.983]
Epoch [78/120    avg_loss:0.142, val_acc:0.958]
Epoch [79/120    avg_loss:0.235, val_acc:0.990]
Epoch [80/120    avg_loss:0.162, val_acc:0.979]
Epoch [81/120    avg_loss:0.240, val_acc:0.954]
Epoch [82/120    avg_loss:0.194, val_acc:0.992]
Epoch [83/120    avg_loss:0.142, val_acc:0.977]
Epoch [84/120    avg_loss:0.162, val_acc:0.983]
Epoch [85/120    avg_loss:0.147, val_acc:0.985]
Epoch [86/120    avg_loss:0.121, val_acc:0.990]
Epoch [87/120    avg_loss:0.127, val_acc:0.992]
Epoch [88/120    avg_loss:0.106, val_acc:0.990]
Epoch [89/120    avg_loss:0.119, val_acc:0.988]
Epoch [90/120    avg_loss:0.097, val_acc:0.990]
Epoch [91/120    avg_loss:0.118, val_acc:0.981]
Epoch [92/120    avg_loss:0.109, val_acc:0.981]
Epoch [93/120    avg_loss:0.096, val_acc:0.994]
Epoch [94/120    avg_loss:0.109, val_acc:0.992]
Epoch [95/120    avg_loss:0.174, val_acc:0.977]
Epoch [96/120    avg_loss:0.120, val_acc:0.992]
Epoch [97/120    avg_loss:0.102, val_acc:0.998]
Epoch [98/120    avg_loss:0.097, val_acc:0.994]
Epoch [99/120    avg_loss:0.087, val_acc:0.996]
Epoch [100/120    avg_loss:0.085, val_acc:0.988]
Epoch [101/120    avg_loss:0.114, val_acc:0.988]
Epoch [102/120    avg_loss:0.094, val_acc:0.998]
Epoch [103/120    avg_loss:0.104, val_acc:0.992]
Epoch [104/120    avg_loss:0.110, val_acc:0.988]
Epoch [105/120    avg_loss:0.108, val_acc:0.979]
Epoch [106/120    avg_loss:0.113, val_acc:0.988]
Epoch [107/120    avg_loss:0.138, val_acc:0.967]
Epoch [108/120    avg_loss:0.143, val_acc:0.988]
Epoch [109/120    avg_loss:0.121, val_acc:0.988]
Epoch [110/120    avg_loss:0.086, val_acc:0.996]
Epoch [111/120    avg_loss:0.104, val_acc:0.958]
Epoch [112/120    avg_loss:0.131, val_acc:0.985]
Epoch [113/120    avg_loss:0.094, val_acc:0.996]
Epoch [114/120    avg_loss:0.090, val_acc:0.994]
Epoch [115/120    avg_loss:0.065, val_acc:1.000]
Epoch [116/120    avg_loss:0.057, val_acc:0.998]
Epoch [117/120    avg_loss:0.049, val_acc:0.996]
Epoch [118/120    avg_loss:0.072, val_acc:0.996]
Epoch [119/120    avg_loss:0.076, val_acc:0.996]
Epoch [120/120    avg_loss:0.055, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   0   7   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   7   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 0.99486427 0.98148148 1.         0.97091723 0.95622896
 0.98329356 0.95876289 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9933548923188436
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f97d8a99b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.640, val_acc:0.185]
Epoch [2/120    avg_loss:2.571, val_acc:0.296]
Epoch [3/120    avg_loss:2.511, val_acc:0.298]
Epoch [4/120    avg_loss:2.452, val_acc:0.298]
Epoch [5/120    avg_loss:2.408, val_acc:0.298]
Epoch [6/120    avg_loss:2.354, val_acc:0.298]
Epoch [7/120    avg_loss:2.331, val_acc:0.298]
Epoch [8/120    avg_loss:2.290, val_acc:0.300]
Epoch [9/120    avg_loss:2.249, val_acc:0.392]
Epoch [10/120    avg_loss:2.215, val_acc:0.446]
Epoch [11/120    avg_loss:2.175, val_acc:0.481]
Epoch [12/120    avg_loss:2.138, val_acc:0.510]
Epoch [13/120    avg_loss:2.100, val_acc:0.535]
Epoch [14/120    avg_loss:2.052, val_acc:0.552]
Epoch [15/120    avg_loss:1.993, val_acc:0.560]
Epoch [16/120    avg_loss:1.935, val_acc:0.575]
Epoch [17/120    avg_loss:1.873, val_acc:0.594]
Epoch [18/120    avg_loss:1.817, val_acc:0.604]
Epoch [19/120    avg_loss:1.761, val_acc:0.644]
Epoch [20/120    avg_loss:1.696, val_acc:0.658]
Epoch [21/120    avg_loss:1.628, val_acc:0.667]
Epoch [22/120    avg_loss:1.570, val_acc:0.679]
Epoch [23/120    avg_loss:1.492, val_acc:0.698]
Epoch [24/120    avg_loss:1.435, val_acc:0.708]
Epoch [25/120    avg_loss:1.370, val_acc:0.700]
Epoch [26/120    avg_loss:1.295, val_acc:0.721]
Epoch [27/120    avg_loss:1.248, val_acc:0.723]
Epoch [28/120    avg_loss:1.232, val_acc:0.721]
Epoch [29/120    avg_loss:1.142, val_acc:0.746]
Epoch [30/120    avg_loss:1.058, val_acc:0.744]
Epoch [31/120    avg_loss:1.008, val_acc:0.740]
Epoch [32/120    avg_loss:0.968, val_acc:0.748]
Epoch [33/120    avg_loss:0.891, val_acc:0.773]
Epoch [34/120    avg_loss:0.849, val_acc:0.758]
Epoch [35/120    avg_loss:0.858, val_acc:0.785]
Epoch [36/120    avg_loss:0.777, val_acc:0.804]
Epoch [37/120    avg_loss:0.765, val_acc:0.792]
Epoch [38/120    avg_loss:0.773, val_acc:0.802]
Epoch [39/120    avg_loss:0.717, val_acc:0.827]
Epoch [40/120    avg_loss:0.646, val_acc:0.833]
Epoch [41/120    avg_loss:0.643, val_acc:0.829]
Epoch [42/120    avg_loss:0.593, val_acc:0.850]
Epoch [43/120    avg_loss:0.560, val_acc:0.858]
Epoch [44/120    avg_loss:0.525, val_acc:0.869]
Epoch [45/120    avg_loss:0.535, val_acc:0.844]
Epoch [46/120    avg_loss:0.497, val_acc:0.890]
Epoch [47/120    avg_loss:0.486, val_acc:0.942]
Epoch [48/120    avg_loss:0.447, val_acc:0.917]
Epoch [49/120    avg_loss:0.434, val_acc:0.860]
Epoch [50/120    avg_loss:0.427, val_acc:0.927]
Epoch [51/120    avg_loss:0.418, val_acc:0.927]
Epoch [52/120    avg_loss:0.442, val_acc:0.912]
Epoch [53/120    avg_loss:0.430, val_acc:0.904]
Epoch [54/120    avg_loss:0.496, val_acc:0.938]
Epoch [55/120    avg_loss:0.420, val_acc:0.946]
Epoch [56/120    avg_loss:0.372, val_acc:0.931]
Epoch [57/120    avg_loss:0.352, val_acc:0.942]
Epoch [58/120    avg_loss:0.316, val_acc:0.950]
Epoch [59/120    avg_loss:0.308, val_acc:0.952]
Epoch [60/120    avg_loss:0.288, val_acc:0.919]
Epoch [61/120    avg_loss:0.282, val_acc:0.952]
Epoch [62/120    avg_loss:0.310, val_acc:0.954]
Epoch [63/120    avg_loss:0.276, val_acc:0.948]
Epoch [64/120    avg_loss:0.355, val_acc:0.948]
Epoch [65/120    avg_loss:0.343, val_acc:0.946]
Epoch [66/120    avg_loss:0.279, val_acc:0.948]
Epoch [67/120    avg_loss:0.294, val_acc:0.954]
Epoch [68/120    avg_loss:0.340, val_acc:0.940]
Epoch [69/120    avg_loss:0.300, val_acc:0.896]
Epoch [70/120    avg_loss:0.285, val_acc:0.896]
Epoch [71/120    avg_loss:0.238, val_acc:0.956]
Epoch [72/120    avg_loss:0.216, val_acc:0.971]
Epoch [73/120    avg_loss:0.225, val_acc:0.973]
Epoch [74/120    avg_loss:0.284, val_acc:0.954]
Epoch [75/120    avg_loss:0.272, val_acc:0.956]
Epoch [76/120    avg_loss:0.235, val_acc:0.952]
Epoch [77/120    avg_loss:0.248, val_acc:0.950]
Epoch [78/120    avg_loss:0.258, val_acc:0.958]
Epoch [79/120    avg_loss:0.240, val_acc:0.967]
Epoch [80/120    avg_loss:0.215, val_acc:0.954]
Epoch [81/120    avg_loss:0.251, val_acc:0.942]
Epoch [82/120    avg_loss:0.231, val_acc:0.971]
Epoch [83/120    avg_loss:0.189, val_acc:0.967]
Epoch [84/120    avg_loss:0.189, val_acc:0.942]
Epoch [85/120    avg_loss:0.253, val_acc:0.977]
Epoch [86/120    avg_loss:0.178, val_acc:0.958]
Epoch [87/120    avg_loss:0.178, val_acc:0.969]
Epoch [88/120    avg_loss:0.165, val_acc:0.983]
Epoch [89/120    avg_loss:0.152, val_acc:0.973]
Epoch [90/120    avg_loss:0.141, val_acc:0.983]
Epoch [91/120    avg_loss:0.115, val_acc:0.954]
Epoch [92/120    avg_loss:0.144, val_acc:0.971]
Epoch [93/120    avg_loss:0.177, val_acc:0.985]
Epoch [94/120    avg_loss:0.181, val_acc:0.973]
Epoch [95/120    avg_loss:0.202, val_acc:0.981]
Epoch [96/120    avg_loss:0.147, val_acc:0.977]
Epoch [97/120    avg_loss:0.106, val_acc:0.983]
Epoch [98/120    avg_loss:0.103, val_acc:0.979]
Epoch [99/120    avg_loss:0.122, val_acc:0.975]
Epoch [100/120    avg_loss:0.107, val_acc:0.977]
Epoch [101/120    avg_loss:0.134, val_acc:0.977]
Epoch [102/120    avg_loss:0.139, val_acc:0.985]
Epoch [103/120    avg_loss:0.101, val_acc:0.985]
Epoch [104/120    avg_loss:0.144, val_acc:0.971]
Epoch [105/120    avg_loss:0.164, val_acc:0.979]
Epoch [106/120    avg_loss:0.122, val_acc:0.985]
Epoch [107/120    avg_loss:0.111, val_acc:0.973]
Epoch [108/120    avg_loss:0.174, val_acc:0.956]
Epoch [109/120    avg_loss:0.127, val_acc:0.983]
Epoch [110/120    avg_loss:0.102, val_acc:0.981]
Epoch [111/120    avg_loss:0.114, val_acc:0.977]
Epoch [112/120    avg_loss:0.116, val_acc:0.975]
Epoch [113/120    avg_loss:0.124, val_acc:0.981]
Epoch [114/120    avg_loss:0.108, val_acc:0.967]
Epoch [115/120    avg_loss:0.117, val_acc:0.983]
Epoch [116/120    avg_loss:0.104, val_acc:0.992]
Epoch [117/120    avg_loss:0.102, val_acc:0.981]
Epoch [118/120    avg_loss:0.140, val_acc:0.983]
Epoch [119/120    avg_loss:0.127, val_acc:0.985]
Epoch [120/120    avg_loss:0.071, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 164  60   0   0   0   0   0   0   3   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.40085287846482

F1 scores:
[       nan 0.99780541 0.97977528 1.         0.83887468 0.82857143
 0.99277108 0.96703297 0.998713   1.         1.         1.
 0.99448732 1.        ]

Kappa:
0.9822011006930413
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f741f114a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.629, val_acc:0.127]
Epoch [2/120    avg_loss:2.551, val_acc:0.179]
Epoch [3/120    avg_loss:2.471, val_acc:0.233]
Epoch [4/120    avg_loss:2.403, val_acc:0.298]
Epoch [5/120    avg_loss:2.340, val_acc:0.333]
Epoch [6/120    avg_loss:2.276, val_acc:0.329]
Epoch [7/120    avg_loss:2.212, val_acc:0.371]
Epoch [8/120    avg_loss:2.169, val_acc:0.400]
Epoch [9/120    avg_loss:2.105, val_acc:0.440]
Epoch [10/120    avg_loss:2.076, val_acc:0.465]
Epoch [11/120    avg_loss:2.011, val_acc:0.487]
Epoch [12/120    avg_loss:1.958, val_acc:0.517]
Epoch [13/120    avg_loss:1.921, val_acc:0.510]
Epoch [14/120    avg_loss:1.868, val_acc:0.562]
Epoch [15/120    avg_loss:1.812, val_acc:0.604]
Epoch [16/120    avg_loss:1.766, val_acc:0.660]
Epoch [17/120    avg_loss:1.748, val_acc:0.692]
Epoch [18/120    avg_loss:1.680, val_acc:0.694]
Epoch [19/120    avg_loss:1.609, val_acc:0.702]
Epoch [20/120    avg_loss:1.567, val_acc:0.727]
Epoch [21/120    avg_loss:1.512, val_acc:0.746]
Epoch [22/120    avg_loss:1.434, val_acc:0.744]
Epoch [23/120    avg_loss:1.368, val_acc:0.769]
Epoch [24/120    avg_loss:1.308, val_acc:0.762]
Epoch [25/120    avg_loss:1.238, val_acc:0.767]
Epoch [26/120    avg_loss:1.148, val_acc:0.790]
Epoch [27/120    avg_loss:1.084, val_acc:0.852]
Epoch [28/120    avg_loss:1.008, val_acc:0.838]
Epoch [29/120    avg_loss:0.945, val_acc:0.871]
Epoch [30/120    avg_loss:0.881, val_acc:0.875]
Epoch [31/120    avg_loss:0.856, val_acc:0.904]
Epoch [32/120    avg_loss:0.820, val_acc:0.910]
Epoch [33/120    avg_loss:0.731, val_acc:0.910]
Epoch [34/120    avg_loss:0.680, val_acc:0.917]
Epoch [35/120    avg_loss:0.673, val_acc:0.917]
Epoch [36/120    avg_loss:0.654, val_acc:0.935]
Epoch [37/120    avg_loss:0.645, val_acc:0.942]
Epoch [38/120    avg_loss:0.601, val_acc:0.965]
Epoch [39/120    avg_loss:0.571, val_acc:0.952]
Epoch [40/120    avg_loss:0.512, val_acc:0.948]
Epoch [41/120    avg_loss:0.508, val_acc:0.927]
Epoch [42/120    avg_loss:0.523, val_acc:0.948]
Epoch [43/120    avg_loss:0.444, val_acc:0.929]
Epoch [44/120    avg_loss:0.435, val_acc:0.863]
Epoch [45/120    avg_loss:0.493, val_acc:0.958]
Epoch [46/120    avg_loss:0.415, val_acc:0.963]
Epoch [47/120    avg_loss:0.441, val_acc:0.915]
Epoch [48/120    avg_loss:0.394, val_acc:0.908]
Epoch [49/120    avg_loss:0.388, val_acc:0.971]
Epoch [50/120    avg_loss:0.369, val_acc:0.973]
Epoch [51/120    avg_loss:0.311, val_acc:0.950]
Epoch [52/120    avg_loss:0.317, val_acc:0.979]
Epoch [53/120    avg_loss:0.342, val_acc:0.963]
Epoch [54/120    avg_loss:0.300, val_acc:0.967]
Epoch [55/120    avg_loss:0.311, val_acc:0.971]
Epoch [56/120    avg_loss:0.294, val_acc:0.983]
Epoch [57/120    avg_loss:0.267, val_acc:0.954]
Epoch [58/120    avg_loss:0.374, val_acc:0.931]
Epoch [59/120    avg_loss:0.323, val_acc:0.975]
Epoch [60/120    avg_loss:0.273, val_acc:0.983]
Epoch [61/120    avg_loss:0.254, val_acc:0.981]
Epoch [62/120    avg_loss:0.254, val_acc:0.979]
Epoch [63/120    avg_loss:0.259, val_acc:0.979]
Epoch [64/120    avg_loss:0.226, val_acc:0.981]
Epoch [65/120    avg_loss:0.206, val_acc:0.965]
Epoch [66/120    avg_loss:0.204, val_acc:0.973]
Epoch [67/120    avg_loss:0.248, val_acc:0.960]
Epoch [68/120    avg_loss:0.226, val_acc:0.977]
Epoch [69/120    avg_loss:0.230, val_acc:0.946]
Epoch [70/120    avg_loss:0.272, val_acc:0.977]
Epoch [71/120    avg_loss:0.224, val_acc:0.975]
Epoch [72/120    avg_loss:0.183, val_acc:0.988]
Epoch [73/120    avg_loss:0.175, val_acc:0.985]
Epoch [74/120    avg_loss:0.216, val_acc:0.985]
Epoch [75/120    avg_loss:0.171, val_acc:0.975]
Epoch [76/120    avg_loss:0.195, val_acc:0.985]
Epoch [77/120    avg_loss:0.213, val_acc:0.954]
Epoch [78/120    avg_loss:0.226, val_acc:0.981]
Epoch [79/120    avg_loss:0.190, val_acc:0.977]
Epoch [80/120    avg_loss:0.198, val_acc:0.990]
Epoch [81/120    avg_loss:0.190, val_acc:0.975]
Epoch [82/120    avg_loss:0.170, val_acc:0.981]
Epoch [83/120    avg_loss:0.147, val_acc:0.988]
Epoch [84/120    avg_loss:0.142, val_acc:0.975]
Epoch [85/120    avg_loss:0.148, val_acc:0.985]
Epoch [86/120    avg_loss:0.175, val_acc:0.988]
Epoch [87/120    avg_loss:0.183, val_acc:0.946]
Epoch [88/120    avg_loss:0.225, val_acc:0.977]
Epoch [89/120    avg_loss:0.175, val_acc:0.977]
Epoch [90/120    avg_loss:0.192, val_acc:0.935]
Epoch [91/120    avg_loss:0.235, val_acc:0.969]
Epoch [92/120    avg_loss:0.210, val_acc:0.985]
Epoch [93/120    avg_loss:0.161, val_acc:0.981]
Epoch [94/120    avg_loss:0.110, val_acc:0.990]
Epoch [95/120    avg_loss:0.107, val_acc:0.992]
Epoch [96/120    avg_loss:0.093, val_acc:0.992]
Epoch [97/120    avg_loss:0.101, val_acc:0.990]
Epoch [98/120    avg_loss:0.110, val_acc:0.990]
Epoch [99/120    avg_loss:0.091, val_acc:0.994]
Epoch [100/120    avg_loss:0.089, val_acc:0.992]
Epoch [101/120    avg_loss:0.080, val_acc:0.994]
Epoch [102/120    avg_loss:0.073, val_acc:0.994]
Epoch [103/120    avg_loss:0.094, val_acc:0.992]
Epoch [104/120    avg_loss:0.099, val_acc:0.992]
Epoch [105/120    avg_loss:0.084, val_acc:0.992]
Epoch [106/120    avg_loss:0.076, val_acc:0.996]
Epoch [107/120    avg_loss:0.076, val_acc:0.996]
Epoch [108/120    avg_loss:0.077, val_acc:0.992]
Epoch [109/120    avg_loss:0.084, val_acc:0.992]
Epoch [110/120    avg_loss:0.085, val_acc:0.992]
Epoch [111/120    avg_loss:0.078, val_acc:0.992]
Epoch [112/120    avg_loss:0.069, val_acc:0.992]
Epoch [113/120    avg_loss:0.075, val_acc:0.992]
Epoch [114/120    avg_loss:0.089, val_acc:0.994]
Epoch [115/120    avg_loss:0.072, val_acc:0.992]
Epoch [116/120    avg_loss:0.070, val_acc:0.994]
Epoch [117/120    avg_loss:0.075, val_acc:0.994]
Epoch [118/120    avg_loss:0.080, val_acc:0.994]
Epoch [119/120    avg_loss:0.071, val_acc:0.992]
Epoch [120/120    avg_loss:0.073, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 0.99854015 0.98426966 1.         0.94688222 0.92604502
 0.99757869 0.96132597 0.99870968 1.         1.         1.
 1.         1.        ]

Kappa:
0.9924041890532966
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f302fb4f9e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 33547==>0.03M
----------Training process----------
Epoch [1/120    avg_loss:2.653, val_acc:0.071]
Epoch [2/120    avg_loss:2.575, val_acc:0.421]
Epoch [3/120    avg_loss:2.498, val_acc:0.487]
Epoch [4/120    avg_loss:2.429, val_acc:0.477]
Epoch [5/120    avg_loss:2.369, val_acc:0.467]
Epoch [6/120    avg_loss:2.327, val_acc:0.483]
Epoch [7/120    avg_loss:2.276, val_acc:0.517]
Epoch [8/120    avg_loss:2.233, val_acc:0.531]
Epoch [9/120    avg_loss:2.186, val_acc:0.533]
Epoch [10/120    avg_loss:2.131, val_acc:0.517]
Epoch [11/120    avg_loss:2.079, val_acc:0.510]
Epoch [12/120    avg_loss:2.007, val_acc:0.525]
Epoch [13/120    avg_loss:1.954, val_acc:0.519]
Epoch [14/120    avg_loss:1.904, val_acc:0.540]
Epoch [15/120    avg_loss:1.857, val_acc:0.567]
Epoch [16/120    avg_loss:1.808, val_acc:0.562]
Epoch [17/120    avg_loss:1.778, val_acc:0.610]
Epoch [18/120    avg_loss:1.733, val_acc:0.577]
Epoch [19/120    avg_loss:1.669, val_acc:0.650]
Epoch [20/120    avg_loss:1.615, val_acc:0.685]
Epoch [21/120    avg_loss:1.560, val_acc:0.733]
Epoch [22/120    avg_loss:1.495, val_acc:0.752]
Epoch [23/120    avg_loss:1.439, val_acc:0.738]
Epoch [24/120    avg_loss:1.406, val_acc:0.665]
Epoch [25/120    avg_loss:1.365, val_acc:0.750]
Epoch [26/120    avg_loss:1.271, val_acc:0.817]
Epoch [27/120    avg_loss:1.202, val_acc:0.817]
Epoch [28/120    avg_loss:1.137, val_acc:0.848]
Epoch [29/120    avg_loss:1.101, val_acc:0.821]
Epoch [30/120    avg_loss:1.031, val_acc:0.802]
Epoch [31/120    avg_loss:0.963, val_acc:0.827]
Epoch [32/120    avg_loss:0.902, val_acc:0.871]
Epoch [33/120    avg_loss:0.848, val_acc:0.881]
Epoch [34/120    avg_loss:0.843, val_acc:0.877]
Epoch [35/120    avg_loss:0.782, val_acc:0.900]
Epoch [36/120    avg_loss:0.715, val_acc:0.883]
Epoch [37/120    avg_loss:0.697, val_acc:0.906]
Epoch [38/120    avg_loss:0.658, val_acc:0.883]
Epoch [39/120    avg_loss:0.624, val_acc:0.908]
Epoch [40/120    avg_loss:0.586, val_acc:0.902]
Epoch [41/120    avg_loss:0.589, val_acc:0.921]
Epoch [42/120    avg_loss:0.522, val_acc:0.940]
Epoch [43/120    avg_loss:0.552, val_acc:0.906]
Epoch [44/120    avg_loss:0.512, val_acc:0.890]
Epoch [45/120    avg_loss:0.490, val_acc:0.921]
Epoch [46/120    avg_loss:0.478, val_acc:0.850]
Epoch [47/120    avg_loss:0.570, val_acc:0.906]
Epoch [48/120    avg_loss:0.526, val_acc:0.921]
Epoch [49/120    avg_loss:0.522, val_acc:0.908]
Epoch [50/120    avg_loss:0.480, val_acc:0.910]
Epoch [51/120    avg_loss:0.448, val_acc:0.944]
Epoch [52/120    avg_loss:0.419, val_acc:0.910]
Epoch [53/120    avg_loss:0.380, val_acc:0.940]
Epoch [54/120    avg_loss:0.408, val_acc:0.902]
Epoch [55/120    avg_loss:0.390, val_acc:0.923]
Epoch [56/120    avg_loss:0.353, val_acc:0.942]
Epoch [57/120    avg_loss:0.348, val_acc:0.946]
Epoch [58/120    avg_loss:0.332, val_acc:0.946]
Epoch [59/120    avg_loss:0.298, val_acc:0.946]
Epoch [60/120    avg_loss:0.278, val_acc:0.950]
Epoch [61/120    avg_loss:0.293, val_acc:0.956]
Epoch [62/120    avg_loss:0.292, val_acc:0.958]
Epoch [63/120    avg_loss:0.264, val_acc:0.960]
Epoch [64/120    avg_loss:0.223, val_acc:0.960]
Epoch [65/120    avg_loss:0.217, val_acc:0.956]
Epoch [66/120    avg_loss:0.234, val_acc:0.963]
Epoch [67/120    avg_loss:0.246, val_acc:0.938]
Epoch [68/120    avg_loss:0.246, val_acc:0.958]
Epoch [69/120    avg_loss:0.204, val_acc:0.956]
Epoch [70/120    avg_loss:0.221, val_acc:0.933]
Epoch [71/120    avg_loss:0.214, val_acc:0.960]
Epoch [72/120    avg_loss:0.182, val_acc:0.963]
Epoch [73/120    avg_loss:0.180, val_acc:0.969]
Epoch [74/120    avg_loss:0.167, val_acc:0.954]
Epoch [75/120    avg_loss:0.188, val_acc:0.960]
Epoch [76/120    avg_loss:0.183, val_acc:0.969]
Epoch [77/120    avg_loss:0.211, val_acc:0.950]
Epoch [78/120    avg_loss:0.220, val_acc:0.940]
Epoch [79/120    avg_loss:0.230, val_acc:0.944]
Epoch [80/120    avg_loss:0.220, val_acc:0.975]
Epoch [81/120    avg_loss:0.150, val_acc:0.965]
Epoch [82/120    avg_loss:0.161, val_acc:0.971]
Epoch [83/120    avg_loss:0.170, val_acc:0.954]
Epoch [84/120    avg_loss:0.203, val_acc:0.956]
Epoch [85/120    avg_loss:0.174, val_acc:0.927]
Epoch [86/120    avg_loss:0.154, val_acc:0.969]
Epoch [87/120    avg_loss:0.201, val_acc:0.950]
Epoch [88/120    avg_loss:0.185, val_acc:0.919]
Epoch [89/120    avg_loss:0.140, val_acc:0.963]
Epoch [90/120    avg_loss:0.136, val_acc:0.971]
Epoch [91/120    avg_loss:0.137, val_acc:0.967]
Epoch [92/120    avg_loss:0.149, val_acc:0.967]
Epoch [93/120    avg_loss:0.137, val_acc:0.963]
Epoch [94/120    avg_loss:0.108, val_acc:0.971]
Epoch [95/120    avg_loss:0.099, val_acc:0.973]
Epoch [96/120    avg_loss:0.094, val_acc:0.969]
Epoch [97/120    avg_loss:0.103, val_acc:0.967]
Epoch [98/120    avg_loss:0.088, val_acc:0.973]
Epoch [99/120    avg_loss:0.094, val_acc:0.969]
Epoch [100/120    avg_loss:0.095, val_acc:0.967]
Epoch [101/120    avg_loss:0.086, val_acc:0.965]
Epoch [102/120    avg_loss:0.081, val_acc:0.967]
Epoch [103/120    avg_loss:0.095, val_acc:0.969]
Epoch [104/120    avg_loss:0.092, val_acc:0.975]
Epoch [105/120    avg_loss:0.089, val_acc:0.975]
Epoch [106/120    avg_loss:0.089, val_acc:0.971]
Epoch [107/120    avg_loss:0.085, val_acc:0.975]
Epoch [108/120    avg_loss:0.091, val_acc:0.967]
Epoch [109/120    avg_loss:0.071, val_acc:0.973]
Epoch [110/120    avg_loss:0.093, val_acc:0.975]
Epoch [111/120    avg_loss:0.076, val_acc:0.967]
Epoch [112/120    avg_loss:0.087, val_acc:0.969]
Epoch [113/120    avg_loss:0.089, val_acc:0.969]
Epoch [114/120    avg_loss:0.078, val_acc:0.967]
Epoch [115/120    avg_loss:0.093, val_acc:0.969]
Epoch [116/120    avg_loss:0.082, val_acc:0.971]
Epoch [117/120    avg_loss:0.087, val_acc:0.971]
Epoch [118/120    avg_loss:0.077, val_acc:0.967]
Epoch [119/120    avg_loss:0.078, val_acc:0.967]
Epoch [120/120    avg_loss:0.074, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 196  25   0   0   0   0   0   0   6   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8272921108742

F1 scores:
[       nan 0.99853801 0.98426966 1.         0.89497717 0.86666667
 0.99516908 0.96132597 1.         1.         1.         1.
 0.99342105 1.        ]

Kappa:
0.9869437206281166
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe1648eea90>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.592, val_acc:0.150]
Epoch [2/120    avg_loss:2.545, val_acc:0.267]
Epoch [3/120    avg_loss:2.490, val_acc:0.327]
Epoch [4/120    avg_loss:2.444, val_acc:0.344]
Epoch [5/120    avg_loss:2.398, val_acc:0.396]
Epoch [6/120    avg_loss:2.352, val_acc:0.408]
Epoch [7/120    avg_loss:2.300, val_acc:0.523]
Epoch [8/120    avg_loss:2.242, val_acc:0.548]
Epoch [9/120    avg_loss:2.188, val_acc:0.533]
Epoch [10/120    avg_loss:2.129, val_acc:0.529]
Epoch [11/120    avg_loss:2.070, val_acc:0.552]
Epoch [12/120    avg_loss:2.005, val_acc:0.567]
Epoch [13/120    avg_loss:1.949, val_acc:0.577]
Epoch [14/120    avg_loss:1.882, val_acc:0.602]
Epoch [15/120    avg_loss:1.833, val_acc:0.619]
Epoch [16/120    avg_loss:1.783, val_acc:0.646]
Epoch [17/120    avg_loss:1.718, val_acc:0.677]
Epoch [18/120    avg_loss:1.637, val_acc:0.685]
Epoch [19/120    avg_loss:1.577, val_acc:0.685]
Epoch [20/120    avg_loss:1.515, val_acc:0.679]
Epoch [21/120    avg_loss:1.457, val_acc:0.704]
Epoch [22/120    avg_loss:1.425, val_acc:0.713]
Epoch [23/120    avg_loss:1.328, val_acc:0.742]
Epoch [24/120    avg_loss:1.262, val_acc:0.760]
Epoch [25/120    avg_loss:1.182, val_acc:0.762]
Epoch [26/120    avg_loss:1.124, val_acc:0.769]
Epoch [27/120    avg_loss:1.054, val_acc:0.781]
Epoch [28/120    avg_loss:0.995, val_acc:0.823]
Epoch [29/120    avg_loss:0.929, val_acc:0.806]
Epoch [30/120    avg_loss:0.872, val_acc:0.877]
Epoch [31/120    avg_loss:0.815, val_acc:0.844]
Epoch [32/120    avg_loss:0.774, val_acc:0.827]
Epoch [33/120    avg_loss:0.709, val_acc:0.892]
Epoch [34/120    avg_loss:0.676, val_acc:0.863]
Epoch [35/120    avg_loss:0.644, val_acc:0.915]
Epoch [36/120    avg_loss:0.585, val_acc:0.925]
Epoch [37/120    avg_loss:0.555, val_acc:0.908]
Epoch [38/120    avg_loss:0.540, val_acc:0.848]
Epoch [39/120    avg_loss:0.493, val_acc:0.940]
Epoch [40/120    avg_loss:0.461, val_acc:0.912]
Epoch [41/120    avg_loss:0.484, val_acc:0.923]
Epoch [42/120    avg_loss:0.460, val_acc:0.952]
Epoch [43/120    avg_loss:0.470, val_acc:0.940]
Epoch [44/120    avg_loss:0.420, val_acc:0.925]
Epoch [45/120    avg_loss:0.407, val_acc:0.958]
Epoch [46/120    avg_loss:0.395, val_acc:0.933]
Epoch [47/120    avg_loss:0.352, val_acc:0.965]
Epoch [48/120    avg_loss:0.343, val_acc:0.944]
Epoch [49/120    avg_loss:0.412, val_acc:0.938]
Epoch [50/120    avg_loss:0.360, val_acc:0.896]
Epoch [51/120    avg_loss:0.353, val_acc:0.956]
Epoch [52/120    avg_loss:0.318, val_acc:0.935]
Epoch [53/120    avg_loss:0.335, val_acc:0.938]
Epoch [54/120    avg_loss:0.314, val_acc:0.969]
Epoch [55/120    avg_loss:0.263, val_acc:0.967]
Epoch [56/120    avg_loss:0.298, val_acc:0.960]
Epoch [57/120    avg_loss:0.254, val_acc:0.954]
Epoch [58/120    avg_loss:0.256, val_acc:0.944]
Epoch [59/120    avg_loss:0.262, val_acc:0.944]
Epoch [60/120    avg_loss:0.260, val_acc:0.971]
Epoch [61/120    avg_loss:0.236, val_acc:0.971]
Epoch [62/120    avg_loss:0.215, val_acc:0.979]
Epoch [63/120    avg_loss:0.205, val_acc:0.963]
Epoch [64/120    avg_loss:0.209, val_acc:0.965]
Epoch [65/120    avg_loss:0.206, val_acc:0.985]
Epoch [66/120    avg_loss:0.203, val_acc:0.969]
Epoch [67/120    avg_loss:0.198, val_acc:0.977]
Epoch [68/120    avg_loss:0.186, val_acc:0.973]
Epoch [69/120    avg_loss:0.177, val_acc:0.979]
Epoch [70/120    avg_loss:0.201, val_acc:0.971]
Epoch [71/120    avg_loss:0.214, val_acc:0.971]
Epoch [72/120    avg_loss:0.175, val_acc:0.956]
Epoch [73/120    avg_loss:0.177, val_acc:0.975]
Epoch [74/120    avg_loss:0.182, val_acc:0.988]
Epoch [75/120    avg_loss:0.168, val_acc:0.975]
Epoch [76/120    avg_loss:0.170, val_acc:0.981]
Epoch [77/120    avg_loss:0.140, val_acc:0.990]
Epoch [78/120    avg_loss:0.147, val_acc:0.990]
Epoch [79/120    avg_loss:0.135, val_acc:0.990]
Epoch [80/120    avg_loss:0.133, val_acc:0.975]
Epoch [81/120    avg_loss:0.133, val_acc:0.971]
Epoch [82/120    avg_loss:0.124, val_acc:0.996]
Epoch [83/120    avg_loss:0.104, val_acc:0.994]
Epoch [84/120    avg_loss:0.130, val_acc:0.985]
Epoch [85/120    avg_loss:0.121, val_acc:0.992]
Epoch [86/120    avg_loss:0.112, val_acc:0.988]
Epoch [87/120    avg_loss:0.121, val_acc:0.994]
Epoch [88/120    avg_loss:0.133, val_acc:0.971]
Epoch [89/120    avg_loss:0.150, val_acc:0.977]
Epoch [90/120    avg_loss:0.135, val_acc:0.988]
Epoch [91/120    avg_loss:0.108, val_acc:0.983]
Epoch [92/120    avg_loss:0.078, val_acc:0.983]
Epoch [93/120    avg_loss:0.122, val_acc:0.983]
Epoch [94/120    avg_loss:0.111, val_acc:0.990]
Epoch [95/120    avg_loss:0.111, val_acc:0.985]
Epoch [96/120    avg_loss:0.092, val_acc:0.994]
Epoch [97/120    avg_loss:0.078, val_acc:0.992]
Epoch [98/120    avg_loss:0.069, val_acc:0.994]
Epoch [99/120    avg_loss:0.066, val_acc:0.996]
Epoch [100/120    avg_loss:0.060, val_acc:0.996]
Epoch [101/120    avg_loss:0.071, val_acc:0.996]
Epoch [102/120    avg_loss:0.065, val_acc:0.996]
Epoch [103/120    avg_loss:0.064, val_acc:0.996]
Epoch [104/120    avg_loss:0.063, val_acc:0.996]
Epoch [105/120    avg_loss:0.064, val_acc:0.996]
Epoch [106/120    avg_loss:0.067, val_acc:0.996]
Epoch [107/120    avg_loss:0.061, val_acc:0.994]
Epoch [108/120    avg_loss:0.062, val_acc:0.996]
Epoch [109/120    avg_loss:0.055, val_acc:0.996]
Epoch [110/120    avg_loss:0.052, val_acc:0.994]
Epoch [111/120    avg_loss:0.053, val_acc:0.994]
Epoch [112/120    avg_loss:0.058, val_acc:0.996]
Epoch [113/120    avg_loss:0.048, val_acc:0.996]
Epoch [114/120    avg_loss:0.055, val_acc:0.994]
Epoch [115/120    avg_loss:0.051, val_acc:0.996]
Epoch [116/120    avg_loss:0.060, val_acc:0.998]
Epoch [117/120    avg_loss:0.056, val_acc:0.994]
Epoch [118/120    avg_loss:0.058, val_acc:0.998]
Epoch [119/120    avg_loss:0.052, val_acc:0.998]
Epoch [120/120    avg_loss:0.048, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 676   0   0   0   0   9   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.99338722 0.99545455 1.         0.95890411 0.94117647
 0.97862233 0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.993117809430266
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efd50e4aa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.664, val_acc:0.107]
Epoch [2/120    avg_loss:2.587, val_acc:0.392]
Epoch [3/120    avg_loss:2.516, val_acc:0.396]
Epoch [4/120    avg_loss:2.453, val_acc:0.379]
Epoch [5/120    avg_loss:2.384, val_acc:0.379]
Epoch [6/120    avg_loss:2.328, val_acc:0.383]
Epoch [7/120    avg_loss:2.265, val_acc:0.381]
Epoch [8/120    avg_loss:2.214, val_acc:0.379]
Epoch [9/120    avg_loss:2.166, val_acc:0.408]
Epoch [10/120    avg_loss:2.110, val_acc:0.427]
Epoch [11/120    avg_loss:2.048, val_acc:0.440]
Epoch [12/120    avg_loss:2.009, val_acc:0.475]
Epoch [13/120    avg_loss:1.978, val_acc:0.506]
Epoch [14/120    avg_loss:1.938, val_acc:0.512]
Epoch [15/120    avg_loss:1.885, val_acc:0.533]
Epoch [16/120    avg_loss:1.864, val_acc:0.556]
Epoch [17/120    avg_loss:1.816, val_acc:0.585]
Epoch [18/120    avg_loss:1.797, val_acc:0.608]
Epoch [19/120    avg_loss:1.747, val_acc:0.619]
Epoch [20/120    avg_loss:1.692, val_acc:0.646]
Epoch [21/120    avg_loss:1.649, val_acc:0.656]
Epoch [22/120    avg_loss:1.580, val_acc:0.662]
Epoch [23/120    avg_loss:1.525, val_acc:0.673]
Epoch [24/120    avg_loss:1.472, val_acc:0.692]
Epoch [25/120    avg_loss:1.429, val_acc:0.700]
Epoch [26/120    avg_loss:1.348, val_acc:0.708]
Epoch [27/120    avg_loss:1.305, val_acc:0.723]
Epoch [28/120    avg_loss:1.215, val_acc:0.742]
Epoch [29/120    avg_loss:1.170, val_acc:0.754]
Epoch [30/120    avg_loss:1.115, val_acc:0.767]
Epoch [31/120    avg_loss:1.110, val_acc:0.740]
Epoch [32/120    avg_loss:1.032, val_acc:0.773]
Epoch [33/120    avg_loss:0.991, val_acc:0.775]
Epoch [34/120    avg_loss:0.933, val_acc:0.773]
Epoch [35/120    avg_loss:0.897, val_acc:0.769]
Epoch [36/120    avg_loss:0.849, val_acc:0.775]
Epoch [37/120    avg_loss:0.874, val_acc:0.769]
Epoch [38/120    avg_loss:0.784, val_acc:0.765]
Epoch [39/120    avg_loss:0.774, val_acc:0.779]
Epoch [40/120    avg_loss:0.747, val_acc:0.779]
Epoch [41/120    avg_loss:0.694, val_acc:0.779]
Epoch [42/120    avg_loss:0.655, val_acc:0.798]
Epoch [43/120    avg_loss:0.632, val_acc:0.792]
Epoch [44/120    avg_loss:0.617, val_acc:0.796]
Epoch [45/120    avg_loss:0.571, val_acc:0.812]
Epoch [46/120    avg_loss:0.552, val_acc:0.823]
Epoch [47/120    avg_loss:0.534, val_acc:0.873]
Epoch [48/120    avg_loss:0.506, val_acc:0.850]
Epoch [49/120    avg_loss:0.501, val_acc:0.867]
Epoch [50/120    avg_loss:0.507, val_acc:0.900]
Epoch [51/120    avg_loss:0.428, val_acc:0.929]
Epoch [52/120    avg_loss:0.377, val_acc:0.908]
Epoch [53/120    avg_loss:0.415, val_acc:0.938]
Epoch [54/120    avg_loss:0.370, val_acc:0.910]
Epoch [55/120    avg_loss:0.415, val_acc:0.963]
Epoch [56/120    avg_loss:0.390, val_acc:0.950]
Epoch [57/120    avg_loss:0.321, val_acc:0.965]
Epoch [58/120    avg_loss:0.319, val_acc:0.946]
Epoch [59/120    avg_loss:0.323, val_acc:0.967]
Epoch [60/120    avg_loss:0.371, val_acc:0.860]
Epoch [61/120    avg_loss:0.559, val_acc:0.917]
Epoch [62/120    avg_loss:0.430, val_acc:0.948]
Epoch [63/120    avg_loss:0.345, val_acc:0.940]
Epoch [64/120    avg_loss:0.311, val_acc:0.965]
Epoch [65/120    avg_loss:0.288, val_acc:0.940]
Epoch [66/120    avg_loss:0.262, val_acc:0.900]
Epoch [67/120    avg_loss:0.379, val_acc:0.940]
Epoch [68/120    avg_loss:0.336, val_acc:0.960]
Epoch [69/120    avg_loss:0.291, val_acc:0.944]
Epoch [70/120    avg_loss:0.264, val_acc:0.956]
Epoch [71/120    avg_loss:0.273, val_acc:0.948]
Epoch [72/120    avg_loss:0.228, val_acc:0.973]
Epoch [73/120    avg_loss:0.218, val_acc:0.960]
Epoch [74/120    avg_loss:0.234, val_acc:0.954]
Epoch [75/120    avg_loss:0.198, val_acc:0.979]
Epoch [76/120    avg_loss:0.174, val_acc:0.985]
Epoch [77/120    avg_loss:0.150, val_acc:0.973]
Epoch [78/120    avg_loss:0.187, val_acc:0.960]
Epoch [79/120    avg_loss:0.188, val_acc:0.973]
Epoch [80/120    avg_loss:0.155, val_acc:0.965]
Epoch [81/120    avg_loss:0.157, val_acc:0.981]
Epoch [82/120    avg_loss:0.161, val_acc:0.983]
Epoch [83/120    avg_loss:0.236, val_acc:0.950]
Epoch [84/120    avg_loss:0.200, val_acc:0.950]
Epoch [85/120    avg_loss:0.238, val_acc:0.983]
Epoch [86/120    avg_loss:0.170, val_acc:0.981]
Epoch [87/120    avg_loss:0.164, val_acc:0.983]
Epoch [88/120    avg_loss:0.117, val_acc:0.985]
Epoch [89/120    avg_loss:0.139, val_acc:0.981]
Epoch [90/120    avg_loss:0.150, val_acc:0.985]
Epoch [91/120    avg_loss:0.115, val_acc:0.981]
Epoch [92/120    avg_loss:0.116, val_acc:0.981]
Epoch [93/120    avg_loss:0.123, val_acc:0.977]
Epoch [94/120    avg_loss:0.138, val_acc:0.977]
Epoch [95/120    avg_loss:0.155, val_acc:0.981]
Epoch [96/120    avg_loss:0.121, val_acc:0.975]
Epoch [97/120    avg_loss:0.139, val_acc:0.988]
Epoch [98/120    avg_loss:0.101, val_acc:0.973]
Epoch [99/120    avg_loss:0.116, val_acc:0.979]
Epoch [100/120    avg_loss:0.112, val_acc:0.985]
Epoch [101/120    avg_loss:0.090, val_acc:0.983]
Epoch [102/120    avg_loss:0.082, val_acc:0.990]
Epoch [103/120    avg_loss:0.144, val_acc:0.985]
Epoch [104/120    avg_loss:0.106, val_acc:0.988]
Epoch [105/120    avg_loss:0.107, val_acc:0.990]
Epoch [106/120    avg_loss:0.154, val_acc:0.954]
Epoch [107/120    avg_loss:0.226, val_acc:0.923]
Epoch [108/120    avg_loss:0.206, val_acc:0.967]
Epoch [109/120    avg_loss:0.146, val_acc:0.985]
Epoch [110/120    avg_loss:0.115, val_acc:0.979]
Epoch [111/120    avg_loss:0.107, val_acc:0.992]
Epoch [112/120    avg_loss:0.136, val_acc:0.967]
Epoch [113/120    avg_loss:0.130, val_acc:0.985]
Epoch [114/120    avg_loss:0.091, val_acc:0.990]
Epoch [115/120    avg_loss:0.091, val_acc:0.983]
Epoch [116/120    avg_loss:0.082, val_acc:0.992]
Epoch [117/120    avg_loss:0.101, val_acc:0.985]
Epoch [118/120    avg_loss:0.141, val_acc:0.990]
Epoch [119/120    avg_loss:0.103, val_acc:0.988]
Epoch [120/120    avg_loss:0.082, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 227   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0  30 115   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   1 467   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 0.99412628 1.         1.         0.93801653 0.88461538
 0.98095238 1.         0.998713   0.99893048 1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.9902680007065303
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0e1087db00>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.648, val_acc:0.101]
Epoch [2/120    avg_loss:2.578, val_acc:0.367]
Epoch [3/120    avg_loss:2.508, val_acc:0.350]
Epoch [4/120    avg_loss:2.443, val_acc:0.348]
Epoch [5/120    avg_loss:2.376, val_acc:0.450]
Epoch [6/120    avg_loss:2.330, val_acc:0.521]
Epoch [7/120    avg_loss:2.270, val_acc:0.558]
Epoch [8/120    avg_loss:2.215, val_acc:0.558]
Epoch [9/120    avg_loss:2.162, val_acc:0.567]
Epoch [10/120    avg_loss:2.121, val_acc:0.560]
Epoch [11/120    avg_loss:2.061, val_acc:0.575]
Epoch [12/120    avg_loss:1.998, val_acc:0.577]
Epoch [13/120    avg_loss:1.940, val_acc:0.577]
Epoch [14/120    avg_loss:1.880, val_acc:0.596]
Epoch [15/120    avg_loss:1.821, val_acc:0.598]
Epoch [16/120    avg_loss:1.786, val_acc:0.604]
Epoch [17/120    avg_loss:1.719, val_acc:0.631]
Epoch [18/120    avg_loss:1.654, val_acc:0.637]
Epoch [19/120    avg_loss:1.611, val_acc:0.696]
Epoch [20/120    avg_loss:1.569, val_acc:0.696]
Epoch [21/120    avg_loss:1.482, val_acc:0.731]
Epoch [22/120    avg_loss:1.410, val_acc:0.729]
Epoch [23/120    avg_loss:1.345, val_acc:0.744]
Epoch [24/120    avg_loss:1.296, val_acc:0.762]
Epoch [25/120    avg_loss:1.231, val_acc:0.798]
Epoch [26/120    avg_loss:1.217, val_acc:0.769]
Epoch [27/120    avg_loss:1.107, val_acc:0.798]
Epoch [28/120    avg_loss:1.108, val_acc:0.779]
Epoch [29/120    avg_loss:1.024, val_acc:0.781]
Epoch [30/120    avg_loss:0.946, val_acc:0.783]
Epoch [31/120    avg_loss:0.921, val_acc:0.785]
Epoch [32/120    avg_loss:0.853, val_acc:0.779]
Epoch [33/120    avg_loss:0.810, val_acc:0.783]
Epoch [34/120    avg_loss:0.740, val_acc:0.823]
Epoch [35/120    avg_loss:0.715, val_acc:0.819]
Epoch [36/120    avg_loss:0.676, val_acc:0.858]
Epoch [37/120    avg_loss:0.653, val_acc:0.823]
Epoch [38/120    avg_loss:0.589, val_acc:0.848]
Epoch [39/120    avg_loss:0.605, val_acc:0.873]
Epoch [40/120    avg_loss:0.589, val_acc:0.917]
Epoch [41/120    avg_loss:0.555, val_acc:0.919]
Epoch [42/120    avg_loss:0.502, val_acc:0.860]
Epoch [43/120    avg_loss:0.515, val_acc:0.927]
Epoch [44/120    avg_loss:0.482, val_acc:0.867]
Epoch [45/120    avg_loss:0.477, val_acc:0.910]
Epoch [46/120    avg_loss:0.413, val_acc:0.923]
Epoch [47/120    avg_loss:0.400, val_acc:0.898]
Epoch [48/120    avg_loss:0.377, val_acc:0.960]
Epoch [49/120    avg_loss:0.426, val_acc:0.887]
Epoch [50/120    avg_loss:0.382, val_acc:0.877]
Epoch [51/120    avg_loss:0.352, val_acc:0.944]
Epoch [52/120    avg_loss:0.315, val_acc:0.952]
Epoch [53/120    avg_loss:0.295, val_acc:0.954]
Epoch [54/120    avg_loss:0.277, val_acc:0.956]
Epoch [55/120    avg_loss:0.308, val_acc:0.938]
Epoch [56/120    avg_loss:0.299, val_acc:0.940]
Epoch [57/120    avg_loss:0.315, val_acc:0.940]
Epoch [58/120    avg_loss:0.251, val_acc:0.960]
Epoch [59/120    avg_loss:0.278, val_acc:0.931]
Epoch [60/120    avg_loss:0.252, val_acc:0.969]
Epoch [61/120    avg_loss:0.242, val_acc:0.975]
Epoch [62/120    avg_loss:0.230, val_acc:0.963]
Epoch [63/120    avg_loss:0.301, val_acc:0.954]
Epoch [64/120    avg_loss:0.296, val_acc:0.963]
Epoch [65/120    avg_loss:0.227, val_acc:0.973]
Epoch [66/120    avg_loss:0.241, val_acc:0.973]
Epoch [67/120    avg_loss:0.161, val_acc:0.975]
Epoch [68/120    avg_loss:0.183, val_acc:0.965]
Epoch [69/120    avg_loss:0.194, val_acc:0.965]
Epoch [70/120    avg_loss:0.155, val_acc:0.983]
Epoch [71/120    avg_loss:0.138, val_acc:0.979]
Epoch [72/120    avg_loss:0.150, val_acc:0.985]
Epoch [73/120    avg_loss:0.157, val_acc:0.977]
Epoch [74/120    avg_loss:0.155, val_acc:0.979]
Epoch [75/120    avg_loss:0.157, val_acc:0.971]
Epoch [76/120    avg_loss:0.153, val_acc:0.963]
Epoch [77/120    avg_loss:0.323, val_acc:0.860]
Epoch [78/120    avg_loss:0.353, val_acc:0.929]
Epoch [79/120    avg_loss:0.318, val_acc:0.963]
Epoch [80/120    avg_loss:0.194, val_acc:0.969]
Epoch [81/120    avg_loss:0.293, val_acc:0.954]
Epoch [82/120    avg_loss:0.195, val_acc:0.975]
Epoch [83/120    avg_loss:0.148, val_acc:0.979]
Epoch [84/120    avg_loss:0.135, val_acc:0.981]
Epoch [85/120    avg_loss:0.132, val_acc:0.973]
Epoch [86/120    avg_loss:0.145, val_acc:0.975]
Epoch [87/120    avg_loss:0.110, val_acc:0.983]
Epoch [88/120    avg_loss:0.087, val_acc:0.983]
Epoch [89/120    avg_loss:0.098, val_acc:0.985]
Epoch [90/120    avg_loss:0.094, val_acc:0.983]
Epoch [91/120    avg_loss:0.094, val_acc:0.983]
Epoch [92/120    avg_loss:0.096, val_acc:0.983]
Epoch [93/120    avg_loss:0.118, val_acc:0.981]
Epoch [94/120    avg_loss:0.091, val_acc:0.981]
Epoch [95/120    avg_loss:0.093, val_acc:0.983]
Epoch [96/120    avg_loss:0.093, val_acc:0.983]
Epoch [97/120    avg_loss:0.097, val_acc:0.983]
Epoch [98/120    avg_loss:0.104, val_acc:0.983]
Epoch [99/120    avg_loss:0.104, val_acc:0.983]
Epoch [100/120    avg_loss:0.092, val_acc:0.983]
Epoch [101/120    avg_loss:0.089, val_acc:0.985]
Epoch [102/120    avg_loss:0.095, val_acc:0.983]
Epoch [103/120    avg_loss:0.086, val_acc:0.983]
Epoch [104/120    avg_loss:0.096, val_acc:0.983]
Epoch [105/120    avg_loss:0.087, val_acc:0.988]
Epoch [106/120    avg_loss:0.083, val_acc:0.983]
Epoch [107/120    avg_loss:0.089, val_acc:0.983]
Epoch [108/120    avg_loss:0.083, val_acc:0.983]
Epoch [109/120    avg_loss:0.093, val_acc:0.990]
Epoch [110/120    avg_loss:0.084, val_acc:0.983]
Epoch [111/120    avg_loss:0.096, val_acc:0.983]
Epoch [112/120    avg_loss:0.090, val_acc:0.988]
Epoch [113/120    avg_loss:0.082, val_acc:0.990]
Epoch [114/120    avg_loss:0.080, val_acc:0.981]
Epoch [115/120    avg_loss:0.079, val_acc:0.983]
Epoch [116/120    avg_loss:0.079, val_acc:0.990]
Epoch [117/120    avg_loss:0.089, val_acc:0.983]
Epoch [118/120    avg_loss:0.085, val_acc:0.985]
Epoch [119/120    avg_loss:0.075, val_acc:0.988]
Epoch [120/120    avg_loss:0.076, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   0   7   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  14   0   0   0   0   0   0   1   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.53091684434968

F1 scores:
[       nan 0.99486427 1.         1.         0.96583144 0.95394737
 0.98329356 1.         1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9947787377530204
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f28dc51ba90>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.631, val_acc:0.067]
Epoch [2/120    avg_loss:2.547, val_acc:0.142]
Epoch [3/120    avg_loss:2.470, val_acc:0.156]
Epoch [4/120    avg_loss:2.399, val_acc:0.277]
Epoch [5/120    avg_loss:2.342, val_acc:0.388]
Epoch [6/120    avg_loss:2.294, val_acc:0.419]
Epoch [7/120    avg_loss:2.233, val_acc:0.433]
Epoch [8/120    avg_loss:2.174, val_acc:0.465]
Epoch [9/120    avg_loss:2.106, val_acc:0.477]
Epoch [10/120    avg_loss:2.044, val_acc:0.527]
Epoch [11/120    avg_loss:1.995, val_acc:0.573]
Epoch [12/120    avg_loss:1.930, val_acc:0.569]
Epoch [13/120    avg_loss:1.872, val_acc:0.560]
Epoch [14/120    avg_loss:1.817, val_acc:0.583]
Epoch [15/120    avg_loss:1.725, val_acc:0.608]
Epoch [16/120    avg_loss:1.674, val_acc:0.598]
Epoch [17/120    avg_loss:1.639, val_acc:0.631]
Epoch [18/120    avg_loss:1.552, val_acc:0.646]
Epoch [19/120    avg_loss:1.500, val_acc:0.658]
Epoch [20/120    avg_loss:1.438, val_acc:0.660]
Epoch [21/120    avg_loss:1.390, val_acc:0.702]
Epoch [22/120    avg_loss:1.301, val_acc:0.704]
Epoch [23/120    avg_loss:1.256, val_acc:0.698]
Epoch [24/120    avg_loss:1.180, val_acc:0.713]
Epoch [25/120    avg_loss:1.145, val_acc:0.717]
Epoch [26/120    avg_loss:1.097, val_acc:0.717]
Epoch [27/120    avg_loss:1.016, val_acc:0.738]
Epoch [28/120    avg_loss:0.968, val_acc:0.781]
Epoch [29/120    avg_loss:0.938, val_acc:0.794]
Epoch [30/120    avg_loss:0.872, val_acc:0.802]
Epoch [31/120    avg_loss:0.826, val_acc:0.835]
Epoch [32/120    avg_loss:0.809, val_acc:0.794]
Epoch [33/120    avg_loss:0.711, val_acc:0.850]
Epoch [34/120    avg_loss:0.645, val_acc:0.892]
Epoch [35/120    avg_loss:0.672, val_acc:0.910]
Epoch [36/120    avg_loss:0.695, val_acc:0.821]
Epoch [37/120    avg_loss:0.715, val_acc:0.819]
Epoch [38/120    avg_loss:0.612, val_acc:0.904]
Epoch [39/120    avg_loss:0.606, val_acc:0.885]
Epoch [40/120    avg_loss:0.561, val_acc:0.921]
Epoch [41/120    avg_loss:0.512, val_acc:0.940]
Epoch [42/120    avg_loss:0.494, val_acc:0.933]
Epoch [43/120    avg_loss:0.452, val_acc:0.948]
Epoch [44/120    avg_loss:0.430, val_acc:0.931]
Epoch [45/120    avg_loss:0.435, val_acc:0.952]
Epoch [46/120    avg_loss:0.395, val_acc:0.954]
Epoch [47/120    avg_loss:0.368, val_acc:0.933]
Epoch [48/120    avg_loss:0.344, val_acc:0.946]
Epoch [49/120    avg_loss:0.382, val_acc:0.877]
Epoch [50/120    avg_loss:0.376, val_acc:0.963]
Epoch [51/120    avg_loss:0.427, val_acc:0.904]
Epoch [52/120    avg_loss:0.435, val_acc:0.940]
Epoch [53/120    avg_loss:0.398, val_acc:0.942]
Epoch [54/120    avg_loss:0.329, val_acc:0.960]
Epoch [55/120    avg_loss:0.355, val_acc:0.973]
Epoch [56/120    avg_loss:0.348, val_acc:0.954]
Epoch [57/120    avg_loss:0.303, val_acc:0.923]
Epoch [58/120    avg_loss:0.351, val_acc:0.971]
Epoch [59/120    avg_loss:0.304, val_acc:0.965]
Epoch [60/120    avg_loss:0.339, val_acc:0.956]
Epoch [61/120    avg_loss:0.344, val_acc:0.965]
Epoch [62/120    avg_loss:0.289, val_acc:0.979]
Epoch [63/120    avg_loss:0.255, val_acc:0.971]
Epoch [64/120    avg_loss:0.210, val_acc:0.960]
Epoch [65/120    avg_loss:0.217, val_acc:0.975]
Epoch [66/120    avg_loss:0.205, val_acc:0.979]
Epoch [67/120    avg_loss:0.225, val_acc:0.977]
Epoch [68/120    avg_loss:0.174, val_acc:0.983]
Epoch [69/120    avg_loss:0.196, val_acc:0.983]
Epoch [70/120    avg_loss:0.213, val_acc:0.975]
Epoch [71/120    avg_loss:0.172, val_acc:0.981]
Epoch [72/120    avg_loss:0.191, val_acc:0.975]
Epoch [73/120    avg_loss:0.228, val_acc:0.956]
Epoch [74/120    avg_loss:0.275, val_acc:0.960]
Epoch [75/120    avg_loss:0.229, val_acc:0.952]
Epoch [76/120    avg_loss:0.179, val_acc:0.983]
Epoch [77/120    avg_loss:0.168, val_acc:0.985]
Epoch [78/120    avg_loss:0.184, val_acc:0.981]
Epoch [79/120    avg_loss:0.194, val_acc:0.973]
Epoch [80/120    avg_loss:0.176, val_acc:0.973]
Epoch [81/120    avg_loss:0.177, val_acc:0.983]
Epoch [82/120    avg_loss:0.150, val_acc:0.983]
Epoch [83/120    avg_loss:0.144, val_acc:0.975]
Epoch [84/120    avg_loss:0.133, val_acc:0.977]
Epoch [85/120    avg_loss:0.186, val_acc:0.969]
Epoch [86/120    avg_loss:0.145, val_acc:0.990]
Epoch [87/120    avg_loss:0.191, val_acc:0.979]
Epoch [88/120    avg_loss:0.197, val_acc:0.969]
Epoch [89/120    avg_loss:0.199, val_acc:0.988]
Epoch [90/120    avg_loss:0.148, val_acc:0.983]
Epoch [91/120    avg_loss:0.199, val_acc:0.990]
Epoch [92/120    avg_loss:0.154, val_acc:0.985]
Epoch [93/120    avg_loss:0.132, val_acc:0.973]
Epoch [94/120    avg_loss:0.137, val_acc:0.973]
Epoch [95/120    avg_loss:0.192, val_acc:0.977]
Epoch [96/120    avg_loss:0.156, val_acc:0.973]
Epoch [97/120    avg_loss:0.158, val_acc:0.973]
Epoch [98/120    avg_loss:0.131, val_acc:0.985]
Epoch [99/120    avg_loss:0.128, val_acc:0.988]
Epoch [100/120    avg_loss:0.174, val_acc:0.879]
Epoch [101/120    avg_loss:0.184, val_acc:0.990]
Epoch [102/120    avg_loss:0.138, val_acc:0.977]
Epoch [103/120    avg_loss:0.175, val_acc:0.990]
Epoch [104/120    avg_loss:0.125, val_acc:0.990]
Epoch [105/120    avg_loss:0.106, val_acc:0.990]
Epoch [106/120    avg_loss:0.094, val_acc:0.990]
Epoch [107/120    avg_loss:0.077, val_acc:0.990]
Epoch [108/120    avg_loss:0.098, val_acc:0.990]
Epoch [109/120    avg_loss:0.110, val_acc:0.985]
Epoch [110/120    avg_loss:0.079, val_acc:0.994]
Epoch [111/120    avg_loss:0.100, val_acc:0.973]
Epoch [112/120    avg_loss:0.123, val_acc:0.977]
Epoch [113/120    avg_loss:0.084, val_acc:0.988]
Epoch [114/120    avg_loss:0.113, val_acc:0.975]
Epoch [115/120    avg_loss:0.111, val_acc:0.981]
Epoch [116/120    avg_loss:0.149, val_acc:0.958]
Epoch [117/120    avg_loss:0.189, val_acc:0.967]
Epoch [118/120    avg_loss:0.148, val_acc:0.975]
Epoch [119/120    avg_loss:0.132, val_acc:0.992]
Epoch [120/120    avg_loss:0.150, val_acc:0.954]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 282   0   0   0   0 403   0   0   0   0   0   0   0]
 [  0   0 199   0   0   0   0  20   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 175  52   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
89.87206823027718

F1 scores:
[       nan 0.58324716 0.95215311 1.         0.87064677 0.84795322
 0.50552147 0.90384615 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.8883726742720397
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fced2ac2a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.629, val_acc:0.129]
Epoch [2/120    avg_loss:2.573, val_acc:0.229]
Epoch [3/120    avg_loss:2.512, val_acc:0.400]
Epoch [4/120    avg_loss:2.455, val_acc:0.463]
Epoch [5/120    avg_loss:2.402, val_acc:0.463]
Epoch [6/120    avg_loss:2.360, val_acc:0.523]
Epoch [7/120    avg_loss:2.320, val_acc:0.542]
Epoch [8/120    avg_loss:2.280, val_acc:0.552]
Epoch [9/120    avg_loss:2.239, val_acc:0.571]
Epoch [10/120    avg_loss:2.201, val_acc:0.569]
Epoch [11/120    avg_loss:2.160, val_acc:0.575]
Epoch [12/120    avg_loss:2.094, val_acc:0.592]
Epoch [13/120    avg_loss:2.058, val_acc:0.588]
Epoch [14/120    avg_loss:2.003, val_acc:0.594]
Epoch [15/120    avg_loss:1.943, val_acc:0.608]
Epoch [16/120    avg_loss:1.893, val_acc:0.613]
Epoch [17/120    avg_loss:1.858, val_acc:0.619]
Epoch [18/120    avg_loss:1.777, val_acc:0.621]
Epoch [19/120    avg_loss:1.709, val_acc:0.635]
Epoch [20/120    avg_loss:1.662, val_acc:0.646]
Epoch [21/120    avg_loss:1.606, val_acc:0.656]
Epoch [22/120    avg_loss:1.530, val_acc:0.679]
Epoch [23/120    avg_loss:1.477, val_acc:0.690]
Epoch [24/120    avg_loss:1.420, val_acc:0.715]
Epoch [25/120    avg_loss:1.382, val_acc:0.715]
Epoch [26/120    avg_loss:1.305, val_acc:0.740]
Epoch [27/120    avg_loss:1.234, val_acc:0.750]
Epoch [28/120    avg_loss:1.174, val_acc:0.758]
Epoch [29/120    avg_loss:1.124, val_acc:0.756]
Epoch [30/120    avg_loss:1.055, val_acc:0.773]
Epoch [31/120    avg_loss:1.003, val_acc:0.779]
Epoch [32/120    avg_loss:0.958, val_acc:0.802]
Epoch [33/120    avg_loss:0.892, val_acc:0.802]
Epoch [34/120    avg_loss:0.822, val_acc:0.840]
Epoch [35/120    avg_loss:0.762, val_acc:0.908]
Epoch [36/120    avg_loss:0.760, val_acc:0.898]
Epoch [37/120    avg_loss:0.684, val_acc:0.844]
Epoch [38/120    avg_loss:0.699, val_acc:0.867]
Epoch [39/120    avg_loss:0.643, val_acc:0.917]
Epoch [40/120    avg_loss:0.637, val_acc:0.921]
Epoch [41/120    avg_loss:0.563, val_acc:0.912]
Epoch [42/120    avg_loss:0.524, val_acc:0.929]
Epoch [43/120    avg_loss:0.506, val_acc:0.912]
Epoch [44/120    avg_loss:0.490, val_acc:0.940]
Epoch [45/120    avg_loss:0.489, val_acc:0.933]
Epoch [46/120    avg_loss:0.425, val_acc:0.921]
Epoch [47/120    avg_loss:0.455, val_acc:0.898]
Epoch [48/120    avg_loss:0.386, val_acc:0.931]
Epoch [49/120    avg_loss:0.399, val_acc:0.946]
Epoch [50/120    avg_loss:0.388, val_acc:0.931]
Epoch [51/120    avg_loss:0.328, val_acc:0.929]
Epoch [52/120    avg_loss:0.359, val_acc:0.958]
Epoch [53/120    avg_loss:0.319, val_acc:0.965]
Epoch [54/120    avg_loss:0.293, val_acc:0.954]
Epoch [55/120    avg_loss:0.289, val_acc:0.979]
Epoch [56/120    avg_loss:0.261, val_acc:0.965]
Epoch [57/120    avg_loss:0.275, val_acc:0.950]
Epoch [58/120    avg_loss:0.314, val_acc:0.960]
Epoch [59/120    avg_loss:0.279, val_acc:0.960]
Epoch [60/120    avg_loss:0.277, val_acc:0.969]
Epoch [61/120    avg_loss:0.218, val_acc:0.954]
Epoch [62/120    avg_loss:0.248, val_acc:0.963]
Epoch [63/120    avg_loss:0.280, val_acc:0.965]
Epoch [64/120    avg_loss:0.308, val_acc:0.946]
Epoch [65/120    avg_loss:0.262, val_acc:0.948]
Epoch [66/120    avg_loss:0.225, val_acc:0.963]
Epoch [67/120    avg_loss:0.193, val_acc:0.969]
Epoch [68/120    avg_loss:0.208, val_acc:0.960]
Epoch [69/120    avg_loss:0.202, val_acc:0.971]
Epoch [70/120    avg_loss:0.174, val_acc:0.973]
Epoch [71/120    avg_loss:0.148, val_acc:0.977]
Epoch [72/120    avg_loss:0.152, val_acc:0.975]
Epoch [73/120    avg_loss:0.144, val_acc:0.981]
Epoch [74/120    avg_loss:0.146, val_acc:0.981]
Epoch [75/120    avg_loss:0.143, val_acc:0.973]
Epoch [76/120    avg_loss:0.141, val_acc:0.975]
Epoch [77/120    avg_loss:0.143, val_acc:0.975]
Epoch [78/120    avg_loss:0.140, val_acc:0.975]
Epoch [79/120    avg_loss:0.151, val_acc:0.979]
Epoch [80/120    avg_loss:0.154, val_acc:0.975]
Epoch [81/120    avg_loss:0.140, val_acc:0.975]
Epoch [82/120    avg_loss:0.139, val_acc:0.979]
Epoch [83/120    avg_loss:0.138, val_acc:0.977]
Epoch [84/120    avg_loss:0.147, val_acc:0.979]
Epoch [85/120    avg_loss:0.135, val_acc:0.975]
Epoch [86/120    avg_loss:0.142, val_acc:0.975]
Epoch [87/120    avg_loss:0.136, val_acc:0.981]
Epoch [88/120    avg_loss:0.138, val_acc:0.979]
Epoch [89/120    avg_loss:0.136, val_acc:0.979]
Epoch [90/120    avg_loss:0.148, val_acc:0.977]
Epoch [91/120    avg_loss:0.140, val_acc:0.983]
Epoch [92/120    avg_loss:0.141, val_acc:0.981]
Epoch [93/120    avg_loss:0.120, val_acc:0.979]
Epoch [94/120    avg_loss:0.136, val_acc:0.979]
Epoch [95/120    avg_loss:0.126, val_acc:0.983]
Epoch [96/120    avg_loss:0.132, val_acc:0.983]
Epoch [97/120    avg_loss:0.123, val_acc:0.983]
Epoch [98/120    avg_loss:0.119, val_acc:0.983]
Epoch [99/120    avg_loss:0.136, val_acc:0.983]
Epoch [100/120    avg_loss:0.134, val_acc:0.983]
Epoch [101/120    avg_loss:0.148, val_acc:0.985]
Epoch [102/120    avg_loss:0.121, val_acc:0.983]
Epoch [103/120    avg_loss:0.138, val_acc:0.983]
Epoch [104/120    avg_loss:0.118, val_acc:0.983]
Epoch [105/120    avg_loss:0.119, val_acc:0.983]
Epoch [106/120    avg_loss:0.119, val_acc:0.981]
Epoch [107/120    avg_loss:0.119, val_acc:0.985]
Epoch [108/120    avg_loss:0.124, val_acc:0.983]
Epoch [109/120    avg_loss:0.129, val_acc:0.983]
Epoch [110/120    avg_loss:0.122, val_acc:0.985]
Epoch [111/120    avg_loss:0.127, val_acc:0.988]
Epoch [112/120    avg_loss:0.136, val_acc:0.981]
Epoch [113/120    avg_loss:0.125, val_acc:0.988]
Epoch [114/120    avg_loss:0.112, val_acc:0.985]
Epoch [115/120    avg_loss:0.114, val_acc:0.985]
Epoch [116/120    avg_loss:0.112, val_acc:0.985]
Epoch [117/120    avg_loss:0.117, val_acc:0.985]
Epoch [118/120    avg_loss:0.100, val_acc:0.985]
Epoch [119/120    avg_loss:0.100, val_acc:0.983]
Epoch [120/120    avg_loss:0.115, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   6 196  25   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 0.99412628 0.98426966 0.98712446 0.91803279 0.90675241
 0.98095238 0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9881337266762578
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8ee24fdb00>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.620, val_acc:0.198]
Epoch [2/120    avg_loss:2.544, val_acc:0.175]
Epoch [3/120    avg_loss:2.465, val_acc:0.323]
Epoch [4/120    avg_loss:2.401, val_acc:0.325]
Epoch [5/120    avg_loss:2.352, val_acc:0.350]
Epoch [6/120    avg_loss:2.296, val_acc:0.360]
Epoch [7/120    avg_loss:2.248, val_acc:0.385]
Epoch [8/120    avg_loss:2.205, val_acc:0.396]
Epoch [9/120    avg_loss:2.149, val_acc:0.408]
Epoch [10/120    avg_loss:2.092, val_acc:0.463]
Epoch [11/120    avg_loss:2.060, val_acc:0.498]
Epoch [12/120    avg_loss:2.001, val_acc:0.535]
Epoch [13/120    avg_loss:1.943, val_acc:0.571]
Epoch [14/120    avg_loss:1.877, val_acc:0.598]
Epoch [15/120    avg_loss:1.838, val_acc:0.623]
Epoch [16/120    avg_loss:1.758, val_acc:0.648]
Epoch [17/120    avg_loss:1.712, val_acc:0.677]
Epoch [18/120    avg_loss:1.643, val_acc:0.717]
Epoch [19/120    avg_loss:1.599, val_acc:0.700]
Epoch [20/120    avg_loss:1.541, val_acc:0.762]
Epoch [21/120    avg_loss:1.478, val_acc:0.762]
Epoch [22/120    avg_loss:1.425, val_acc:0.758]
Epoch [23/120    avg_loss:1.362, val_acc:0.781]
Epoch [24/120    avg_loss:1.303, val_acc:0.767]
Epoch [25/120    avg_loss:1.214, val_acc:0.771]
Epoch [26/120    avg_loss:1.153, val_acc:0.798]
Epoch [27/120    avg_loss:1.051, val_acc:0.794]
Epoch [28/120    avg_loss:0.978, val_acc:0.825]
Epoch [29/120    avg_loss:0.933, val_acc:0.823]
Epoch [30/120    avg_loss:0.887, val_acc:0.810]
Epoch [31/120    avg_loss:0.850, val_acc:0.815]
Epoch [32/120    avg_loss:0.763, val_acc:0.846]
Epoch [33/120    avg_loss:0.728, val_acc:0.850]
Epoch [34/120    avg_loss:0.660, val_acc:0.902]
Epoch [35/120    avg_loss:0.637, val_acc:0.850]
Epoch [36/120    avg_loss:0.615, val_acc:0.915]
Epoch [37/120    avg_loss:0.596, val_acc:0.933]
Epoch [38/120    avg_loss:0.547, val_acc:0.946]
Epoch [39/120    avg_loss:0.532, val_acc:0.917]
Epoch [40/120    avg_loss:0.540, val_acc:0.896]
Epoch [41/120    avg_loss:0.473, val_acc:0.910]
Epoch [42/120    avg_loss:0.462, val_acc:0.931]
Epoch [43/120    avg_loss:0.419, val_acc:0.938]
Epoch [44/120    avg_loss:0.406, val_acc:0.921]
Epoch [45/120    avg_loss:0.417, val_acc:0.950]
Epoch [46/120    avg_loss:0.414, val_acc:0.940]
Epoch [47/120    avg_loss:0.364, val_acc:0.935]
Epoch [48/120    avg_loss:0.352, val_acc:0.940]
Epoch [49/120    avg_loss:0.327, val_acc:0.952]
Epoch [50/120    avg_loss:0.357, val_acc:0.933]
Epoch [51/120    avg_loss:0.362, val_acc:0.956]
Epoch [52/120    avg_loss:0.300, val_acc:0.929]
Epoch [53/120    avg_loss:0.315, val_acc:0.942]
Epoch [54/120    avg_loss:0.320, val_acc:0.960]
Epoch [55/120    avg_loss:0.319, val_acc:0.929]
Epoch [56/120    avg_loss:0.277, val_acc:0.956]
Epoch [57/120    avg_loss:0.289, val_acc:0.948]
Epoch [58/120    avg_loss:0.253, val_acc:0.954]
Epoch [59/120    avg_loss:0.240, val_acc:0.963]
Epoch [60/120    avg_loss:0.245, val_acc:0.960]
Epoch [61/120    avg_loss:0.230, val_acc:0.963]
Epoch [62/120    avg_loss:0.252, val_acc:0.969]
Epoch [63/120    avg_loss:0.204, val_acc:0.967]
Epoch [64/120    avg_loss:0.226, val_acc:0.956]
Epoch [65/120    avg_loss:0.251, val_acc:0.969]
Epoch [66/120    avg_loss:0.257, val_acc:0.969]
Epoch [67/120    avg_loss:0.216, val_acc:0.979]
Epoch [68/120    avg_loss:0.190, val_acc:0.979]
Epoch [69/120    avg_loss:0.184, val_acc:0.950]
Epoch [70/120    avg_loss:0.260, val_acc:0.952]
Epoch [71/120    avg_loss:0.245, val_acc:0.960]
Epoch [72/120    avg_loss:0.233, val_acc:0.971]
Epoch [73/120    avg_loss:0.230, val_acc:0.979]
Epoch [74/120    avg_loss:0.181, val_acc:0.971]
Epoch [75/120    avg_loss:0.174, val_acc:0.973]
Epoch [76/120    avg_loss:0.161, val_acc:0.979]
Epoch [77/120    avg_loss:0.174, val_acc:0.979]
Epoch [78/120    avg_loss:0.165, val_acc:0.977]
Epoch [79/120    avg_loss:0.164, val_acc:0.973]
Epoch [80/120    avg_loss:0.151, val_acc:0.979]
Epoch [81/120    avg_loss:0.169, val_acc:0.944]
Epoch [82/120    avg_loss:0.190, val_acc:0.967]
Epoch [83/120    avg_loss:0.172, val_acc:0.983]
Epoch [84/120    avg_loss:0.136, val_acc:0.985]
Epoch [85/120    avg_loss:0.121, val_acc:0.988]
Epoch [86/120    avg_loss:0.132, val_acc:0.969]
Epoch [87/120    avg_loss:0.122, val_acc:0.992]
Epoch [88/120    avg_loss:0.100, val_acc:0.969]
Epoch [89/120    avg_loss:0.191, val_acc:0.992]
Epoch [90/120    avg_loss:0.172, val_acc:0.954]
Epoch [91/120    avg_loss:0.186, val_acc:0.988]
Epoch [92/120    avg_loss:0.124, val_acc:0.981]
Epoch [93/120    avg_loss:0.139, val_acc:0.963]
Epoch [94/120    avg_loss:0.136, val_acc:0.992]
Epoch [95/120    avg_loss:0.172, val_acc:0.992]
Epoch [96/120    avg_loss:0.167, val_acc:0.981]
Epoch [97/120    avg_loss:0.130, val_acc:0.983]
Epoch [98/120    avg_loss:0.103, val_acc:0.996]
Epoch [99/120    avg_loss:0.090, val_acc:0.983]
Epoch [100/120    avg_loss:0.089, val_acc:0.985]
Epoch [101/120    avg_loss:0.099, val_acc:0.985]
Epoch [102/120    avg_loss:0.086, val_acc:0.988]
Epoch [103/120    avg_loss:0.104, val_acc:0.996]
Epoch [104/120    avg_loss:0.111, val_acc:0.985]
Epoch [105/120    avg_loss:0.095, val_acc:0.985]
Epoch [106/120    avg_loss:0.134, val_acc:0.985]
Epoch [107/120    avg_loss:0.135, val_acc:0.983]
Epoch [108/120    avg_loss:0.147, val_acc:0.994]
Epoch [109/120    avg_loss:0.091, val_acc:0.988]
Epoch [110/120    avg_loss:0.073, val_acc:0.996]
Epoch [111/120    avg_loss:0.066, val_acc:0.992]
Epoch [112/120    avg_loss:0.101, val_acc:0.958]
Epoch [113/120    avg_loss:0.149, val_acc:0.992]
Epoch [114/120    avg_loss:0.097, val_acc:0.996]
Epoch [115/120    avg_loss:0.091, val_acc:0.992]
Epoch [116/120    avg_loss:0.068, val_acc:0.996]
Epoch [117/120    avg_loss:0.063, val_acc:0.994]
Epoch [118/120    avg_loss:0.061, val_acc:0.996]
Epoch [119/120    avg_loss:0.067, val_acc:0.996]
Epoch [120/120    avg_loss:0.058, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 672   0   0   0   0  13   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   4   0   0   0   0   0   0   1   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   2   5   0   0   0   0   0 381   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 0.98896247 0.98871332 1.         0.96732026 0.95070423
 0.96941176 1.         0.99089727 1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9916940705834492
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb34d06a58>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.605, val_acc:0.188]
Epoch [2/120    avg_loss:2.545, val_acc:0.269]
Epoch [3/120    avg_loss:2.478, val_acc:0.306]
Epoch [4/120    avg_loss:2.415, val_acc:0.327]
Epoch [5/120    avg_loss:2.355, val_acc:0.319]
Epoch [6/120    avg_loss:2.304, val_acc:0.319]
Epoch [7/120    avg_loss:2.234, val_acc:0.310]
Epoch [8/120    avg_loss:2.190, val_acc:0.340]
Epoch [9/120    avg_loss:2.124, val_acc:0.392]
Epoch [10/120    avg_loss:2.070, val_acc:0.429]
Epoch [11/120    avg_loss:2.022, val_acc:0.440]
Epoch [12/120    avg_loss:1.976, val_acc:0.473]
Epoch [13/120    avg_loss:1.921, val_acc:0.525]
Epoch [14/120    avg_loss:1.861, val_acc:0.550]
Epoch [15/120    avg_loss:1.823, val_acc:0.583]
Epoch [16/120    avg_loss:1.737, val_acc:0.635]
Epoch [17/120    avg_loss:1.684, val_acc:0.667]
Epoch [18/120    avg_loss:1.628, val_acc:0.658]
Epoch [19/120    avg_loss:1.574, val_acc:0.688]
Epoch [20/120    avg_loss:1.482, val_acc:0.715]
Epoch [21/120    avg_loss:1.410, val_acc:0.729]
Epoch [22/120    avg_loss:1.366, val_acc:0.740]
Epoch [23/120    avg_loss:1.283, val_acc:0.744]
Epoch [24/120    avg_loss:1.203, val_acc:0.758]
Epoch [25/120    avg_loss:1.136, val_acc:0.771]
Epoch [26/120    avg_loss:1.085, val_acc:0.792]
Epoch [27/120    avg_loss:1.009, val_acc:0.771]
Epoch [28/120    avg_loss:0.946, val_acc:0.775]
Epoch [29/120    avg_loss:0.920, val_acc:0.775]
Epoch [30/120    avg_loss:0.888, val_acc:0.790]
Epoch [31/120    avg_loss:0.857, val_acc:0.819]
Epoch [32/120    avg_loss:0.786, val_acc:0.804]
Epoch [33/120    avg_loss:0.726, val_acc:0.796]
Epoch [34/120    avg_loss:0.676, val_acc:0.831]
Epoch [35/120    avg_loss:0.665, val_acc:0.819]
Epoch [36/120    avg_loss:0.645, val_acc:0.827]
Epoch [37/120    avg_loss:0.624, val_acc:0.821]
Epoch [38/120    avg_loss:0.611, val_acc:0.835]
Epoch [39/120    avg_loss:0.565, val_acc:0.848]
Epoch [40/120    avg_loss:0.534, val_acc:0.917]
Epoch [41/120    avg_loss:0.489, val_acc:0.940]
Epoch [42/120    avg_loss:0.473, val_acc:0.933]
Epoch [43/120    avg_loss:0.469, val_acc:0.919]
Epoch [44/120    avg_loss:0.434, val_acc:0.950]
Epoch [45/120    avg_loss:0.440, val_acc:0.854]
Epoch [46/120    avg_loss:0.411, val_acc:0.948]
Epoch [47/120    avg_loss:0.370, val_acc:0.948]
Epoch [48/120    avg_loss:0.359, val_acc:0.944]
Epoch [49/120    avg_loss:0.351, val_acc:0.950]
Epoch [50/120    avg_loss:0.362, val_acc:0.938]
Epoch [51/120    avg_loss:0.310, val_acc:0.960]
Epoch [52/120    avg_loss:0.311, val_acc:0.910]
Epoch [53/120    avg_loss:0.287, val_acc:0.950]
Epoch [54/120    avg_loss:0.276, val_acc:0.954]
Epoch [55/120    avg_loss:0.273, val_acc:0.948]
Epoch [56/120    avg_loss:0.277, val_acc:0.960]
Epoch [57/120    avg_loss:0.295, val_acc:0.960]
Epoch [58/120    avg_loss:0.262, val_acc:0.960]
Epoch [59/120    avg_loss:0.227, val_acc:0.965]
Epoch [60/120    avg_loss:0.232, val_acc:0.971]
Epoch [61/120    avg_loss:0.235, val_acc:0.944]
Epoch [62/120    avg_loss:0.224, val_acc:0.981]
Epoch [63/120    avg_loss:0.247, val_acc:0.963]
Epoch [64/120    avg_loss:0.297, val_acc:0.963]
Epoch [65/120    avg_loss:0.239, val_acc:0.960]
Epoch [66/120    avg_loss:0.242, val_acc:0.977]
Epoch [67/120    avg_loss:0.265, val_acc:0.965]
Epoch [68/120    avg_loss:0.236, val_acc:0.973]
Epoch [69/120    avg_loss:0.209, val_acc:0.973]
Epoch [70/120    avg_loss:0.208, val_acc:0.944]
Epoch [71/120    avg_loss:0.216, val_acc:0.975]
Epoch [72/120    avg_loss:0.228, val_acc:0.973]
Epoch [73/120    avg_loss:0.235, val_acc:0.965]
Epoch [74/120    avg_loss:0.196, val_acc:0.981]
Epoch [75/120    avg_loss:0.157, val_acc:0.977]
Epoch [76/120    avg_loss:0.155, val_acc:0.971]
Epoch [77/120    avg_loss:0.144, val_acc:0.975]
Epoch [78/120    avg_loss:0.139, val_acc:0.977]
Epoch [79/120    avg_loss:0.149, val_acc:0.985]
Epoch [80/120    avg_loss:0.125, val_acc:0.990]
Epoch [81/120    avg_loss:0.100, val_acc:0.983]
Epoch [82/120    avg_loss:0.115, val_acc:0.990]
Epoch [83/120    avg_loss:0.107, val_acc:0.975]
Epoch [84/120    avg_loss:0.158, val_acc:0.960]
Epoch [85/120    avg_loss:0.135, val_acc:0.983]
Epoch [86/120    avg_loss:0.117, val_acc:0.985]
Epoch [87/120    avg_loss:0.132, val_acc:0.990]
Epoch [88/120    avg_loss:0.108, val_acc:0.990]
Epoch [89/120    avg_loss:0.084, val_acc:0.990]
Epoch [90/120    avg_loss:0.131, val_acc:0.988]
Epoch [91/120    avg_loss:0.152, val_acc:0.960]
Epoch [92/120    avg_loss:0.146, val_acc:0.950]
Epoch [93/120    avg_loss:0.122, val_acc:0.985]
Epoch [94/120    avg_loss:0.119, val_acc:0.992]
Epoch [95/120    avg_loss:0.115, val_acc:0.988]
Epoch [96/120    avg_loss:0.119, val_acc:0.979]
Epoch [97/120    avg_loss:0.189, val_acc:0.990]
Epoch [98/120    avg_loss:0.168, val_acc:0.979]
Epoch [99/120    avg_loss:0.102, val_acc:0.983]
Epoch [100/120    avg_loss:0.101, val_acc:0.979]
Epoch [101/120    avg_loss:0.094, val_acc:0.977]
Epoch [102/120    avg_loss:0.138, val_acc:0.958]
Epoch [103/120    avg_loss:0.216, val_acc:0.975]
Epoch [104/120    avg_loss:0.153, val_acc:0.988]
Epoch [105/120    avg_loss:0.165, val_acc:0.967]
Epoch [106/120    avg_loss:0.170, val_acc:0.983]
Epoch [107/120    avg_loss:0.097, val_acc:0.985]
Epoch [108/120    avg_loss:0.094, val_acc:0.988]
Epoch [109/120    avg_loss:0.090, val_acc:0.994]
Epoch [110/120    avg_loss:0.072, val_acc:0.994]
Epoch [111/120    avg_loss:0.081, val_acc:0.992]
Epoch [112/120    avg_loss:0.066, val_acc:0.992]
Epoch [113/120    avg_loss:0.065, val_acc:0.992]
Epoch [114/120    avg_loss:0.073, val_acc:0.992]
Epoch [115/120    avg_loss:0.064, val_acc:0.992]
Epoch [116/120    avg_loss:0.060, val_acc:0.992]
Epoch [117/120    avg_loss:0.057, val_acc:0.992]
Epoch [118/120    avg_loss:0.064, val_acc:0.992]
Epoch [119/120    avg_loss:0.065, val_acc:0.992]
Epoch [120/120    avg_loss:0.077, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 0.99560117 0.98648649 1.         0.9543379  0.93464052
 0.98564593 0.96703297 1.         1.         1.         0.99472296
 0.99556541 1.        ]

Kappa:
0.9914558850096933
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1741307b38>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.643, val_acc:0.125]
Epoch [2/120    avg_loss:2.567, val_acc:0.317]
Epoch [3/120    avg_loss:2.507, val_acc:0.348]
Epoch [4/120    avg_loss:2.451, val_acc:0.356]
Epoch [5/120    avg_loss:2.401, val_acc:0.356]
Epoch [6/120    avg_loss:2.356, val_acc:0.346]
Epoch [7/120    avg_loss:2.308, val_acc:0.369]
Epoch [8/120    avg_loss:2.273, val_acc:0.427]
Epoch [9/120    avg_loss:2.224, val_acc:0.485]
Epoch [10/120    avg_loss:2.186, val_acc:0.510]
Epoch [11/120    avg_loss:2.135, val_acc:0.562]
Epoch [12/120    avg_loss:2.100, val_acc:0.606]
Epoch [13/120    avg_loss:2.044, val_acc:0.631]
Epoch [14/120    avg_loss:1.986, val_acc:0.654]
Epoch [15/120    avg_loss:1.924, val_acc:0.644]
Epoch [16/120    avg_loss:1.868, val_acc:0.635]
Epoch [17/120    avg_loss:1.796, val_acc:0.642]
Epoch [18/120    avg_loss:1.714, val_acc:0.640]
Epoch [19/120    avg_loss:1.648, val_acc:0.652]
Epoch [20/120    avg_loss:1.578, val_acc:0.667]
Epoch [21/120    avg_loss:1.502, val_acc:0.677]
Epoch [22/120    avg_loss:1.427, val_acc:0.692]
Epoch [23/120    avg_loss:1.376, val_acc:0.710]
Epoch [24/120    avg_loss:1.330, val_acc:0.710]
Epoch [25/120    avg_loss:1.253, val_acc:0.729]
Epoch [26/120    avg_loss:1.207, val_acc:0.725]
Epoch [27/120    avg_loss:1.164, val_acc:0.738]
Epoch [28/120    avg_loss:1.130, val_acc:0.740]
Epoch [29/120    avg_loss:1.018, val_acc:0.762]
Epoch [30/120    avg_loss:0.996, val_acc:0.769]
Epoch [31/120    avg_loss:0.954, val_acc:0.769]
Epoch [32/120    avg_loss:0.938, val_acc:0.779]
Epoch [33/120    avg_loss:0.885, val_acc:0.777]
Epoch [34/120    avg_loss:0.846, val_acc:0.800]
Epoch [35/120    avg_loss:0.785, val_acc:0.838]
Epoch [36/120    avg_loss:0.727, val_acc:0.806]
Epoch [37/120    avg_loss:0.694, val_acc:0.796]
Epoch [38/120    avg_loss:0.704, val_acc:0.852]
Epoch [39/120    avg_loss:0.641, val_acc:0.850]
Epoch [40/120    avg_loss:0.625, val_acc:0.869]
Epoch [41/120    avg_loss:0.587, val_acc:0.904]
Epoch [42/120    avg_loss:0.564, val_acc:0.854]
Epoch [43/120    avg_loss:0.543, val_acc:0.915]
Epoch [44/120    avg_loss:0.535, val_acc:0.902]
Epoch [45/120    avg_loss:0.521, val_acc:0.938]
Epoch [46/120    avg_loss:0.474, val_acc:0.923]
Epoch [47/120    avg_loss:0.471, val_acc:0.840]
Epoch [48/120    avg_loss:0.496, val_acc:0.919]
Epoch [49/120    avg_loss:0.449, val_acc:0.912]
Epoch [50/120    avg_loss:0.416, val_acc:0.912]
Epoch [51/120    avg_loss:0.407, val_acc:0.923]
Epoch [52/120    avg_loss:0.345, val_acc:0.921]
Epoch [53/120    avg_loss:0.349, val_acc:0.954]
Epoch [54/120    avg_loss:0.318, val_acc:0.944]
Epoch [55/120    avg_loss:0.340, val_acc:0.933]
Epoch [56/120    avg_loss:0.332, val_acc:0.938]
Epoch [57/120    avg_loss:0.336, val_acc:0.960]
Epoch [58/120    avg_loss:0.367, val_acc:0.946]
Epoch [59/120    avg_loss:0.406, val_acc:0.946]
Epoch [60/120    avg_loss:0.318, val_acc:0.948]
Epoch [61/120    avg_loss:0.313, val_acc:0.967]
Epoch [62/120    avg_loss:0.276, val_acc:0.940]
Epoch [63/120    avg_loss:0.250, val_acc:0.958]
Epoch [64/120    avg_loss:0.223, val_acc:0.963]
Epoch [65/120    avg_loss:0.275, val_acc:0.931]
Epoch [66/120    avg_loss:0.305, val_acc:0.973]
Epoch [67/120    avg_loss:0.226, val_acc:0.983]
Epoch [68/120    avg_loss:0.228, val_acc:0.973]
Epoch [69/120    avg_loss:0.216, val_acc:0.975]
Epoch [70/120    avg_loss:0.243, val_acc:0.919]
Epoch [71/120    avg_loss:0.250, val_acc:0.965]
Epoch [72/120    avg_loss:0.193, val_acc:0.981]
Epoch [73/120    avg_loss:0.183, val_acc:0.979]
Epoch [74/120    avg_loss:0.186, val_acc:0.977]
Epoch [75/120    avg_loss:0.161, val_acc:0.969]
Epoch [76/120    avg_loss:0.191, val_acc:0.969]
Epoch [77/120    avg_loss:0.238, val_acc:0.981]
Epoch [78/120    avg_loss:0.222, val_acc:0.967]
Epoch [79/120    avg_loss:0.159, val_acc:0.983]
Epoch [80/120    avg_loss:0.147, val_acc:0.992]
Epoch [81/120    avg_loss:0.133, val_acc:0.975]
Epoch [82/120    avg_loss:0.156, val_acc:0.990]
Epoch [83/120    avg_loss:0.129, val_acc:0.994]
Epoch [84/120    avg_loss:0.154, val_acc:0.977]
Epoch [85/120    avg_loss:0.143, val_acc:0.985]
Epoch [86/120    avg_loss:0.128, val_acc:0.988]
Epoch [87/120    avg_loss:0.117, val_acc:0.940]
Epoch [88/120    avg_loss:0.133, val_acc:0.979]
Epoch [89/120    avg_loss:0.099, val_acc:0.988]
Epoch [90/120    avg_loss:0.100, val_acc:0.992]
Epoch [91/120    avg_loss:0.134, val_acc:0.985]
Epoch [92/120    avg_loss:0.128, val_acc:0.967]
Epoch [93/120    avg_loss:0.100, val_acc:0.988]
Epoch [94/120    avg_loss:0.105, val_acc:0.983]
Epoch [95/120    avg_loss:0.088, val_acc:0.988]
Epoch [96/120    avg_loss:0.088, val_acc:0.990]
Epoch [97/120    avg_loss:0.082, val_acc:0.994]
Epoch [98/120    avg_loss:0.071, val_acc:0.988]
Epoch [99/120    avg_loss:0.067, val_acc:0.988]
Epoch [100/120    avg_loss:0.061, val_acc:0.990]
Epoch [101/120    avg_loss:0.070, val_acc:0.990]
Epoch [102/120    avg_loss:0.067, val_acc:0.990]
Epoch [103/120    avg_loss:0.063, val_acc:0.992]
Epoch [104/120    avg_loss:0.069, val_acc:0.990]
Epoch [105/120    avg_loss:0.061, val_acc:0.990]
Epoch [106/120    avg_loss:0.057, val_acc:0.992]
Epoch [107/120    avg_loss:0.057, val_acc:0.992]
Epoch [108/120    avg_loss:0.067, val_acc:0.992]
Epoch [109/120    avg_loss:0.058, val_acc:0.990]
Epoch [110/120    avg_loss:0.056, val_acc:0.990]
Epoch [111/120    avg_loss:0.054, val_acc:0.990]
Epoch [112/120    avg_loss:0.062, val_acc:0.990]
Epoch [113/120    avg_loss:0.055, val_acc:0.990]
Epoch [114/120    avg_loss:0.066, val_acc:0.990]
Epoch [115/120    avg_loss:0.052, val_acc:0.990]
Epoch [116/120    avg_loss:0.064, val_acc:0.990]
Epoch [117/120    avg_loss:0.062, val_acc:0.990]
Epoch [118/120    avg_loss:0.063, val_acc:0.990]
Epoch [119/120    avg_loss:0.057, val_acc:0.990]
Epoch [120/120    avg_loss:0.064, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   2 209  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 0.996337   0.99545455 0.995671   0.94144144 0.91946309
 0.98800959 0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9921675200309716
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1012289b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.623, val_acc:0.094]
Epoch [2/120    avg_loss:2.568, val_acc:0.354]
Epoch [3/120    avg_loss:2.521, val_acc:0.394]
Epoch [4/120    avg_loss:2.467, val_acc:0.406]
Epoch [5/120    avg_loss:2.424, val_acc:0.394]
Epoch [6/120    avg_loss:2.373, val_acc:0.421]
Epoch [7/120    avg_loss:2.331, val_acc:0.512]
Epoch [8/120    avg_loss:2.291, val_acc:0.571]
Epoch [9/120    avg_loss:2.240, val_acc:0.596]
Epoch [10/120    avg_loss:2.195, val_acc:0.606]
Epoch [11/120    avg_loss:2.140, val_acc:0.608]
Epoch [12/120    avg_loss:2.090, val_acc:0.594]
Epoch [13/120    avg_loss:2.031, val_acc:0.583]
Epoch [14/120    avg_loss:1.978, val_acc:0.596]
Epoch [15/120    avg_loss:1.896, val_acc:0.613]
Epoch [16/120    avg_loss:1.833, val_acc:0.600]
Epoch [17/120    avg_loss:1.762, val_acc:0.608]
Epoch [18/120    avg_loss:1.701, val_acc:0.667]
Epoch [19/120    avg_loss:1.625, val_acc:0.688]
Epoch [20/120    avg_loss:1.549, val_acc:0.669]
Epoch [21/120    avg_loss:1.487, val_acc:0.685]
Epoch [22/120    avg_loss:1.420, val_acc:0.715]
Epoch [23/120    avg_loss:1.366, val_acc:0.715]
Epoch [24/120    avg_loss:1.304, val_acc:0.723]
Epoch [25/120    avg_loss:1.221, val_acc:0.738]
Epoch [26/120    avg_loss:1.148, val_acc:0.744]
Epoch [27/120    avg_loss:1.123, val_acc:0.750]
Epoch [28/120    avg_loss:1.092, val_acc:0.738]
Epoch [29/120    avg_loss:1.077, val_acc:0.735]
Epoch [30/120    avg_loss:1.008, val_acc:0.779]
Epoch [31/120    avg_loss:0.916, val_acc:0.783]
Epoch [32/120    avg_loss:0.848, val_acc:0.798]
Epoch [33/120    avg_loss:0.813, val_acc:0.804]
Epoch [34/120    avg_loss:0.775, val_acc:0.808]
Epoch [35/120    avg_loss:0.714, val_acc:0.838]
Epoch [36/120    avg_loss:0.672, val_acc:0.840]
Epoch [37/120    avg_loss:0.659, val_acc:0.848]
Epoch [38/120    avg_loss:0.588, val_acc:0.873]
Epoch [39/120    avg_loss:0.596, val_acc:0.856]
Epoch [40/120    avg_loss:0.594, val_acc:0.848]
Epoch [41/120    avg_loss:0.589, val_acc:0.904]
Epoch [42/120    avg_loss:0.625, val_acc:0.908]
Epoch [43/120    avg_loss:0.499, val_acc:0.902]
Epoch [44/120    avg_loss:0.479, val_acc:0.927]
Epoch [45/120    avg_loss:0.463, val_acc:0.940]
Epoch [46/120    avg_loss:0.486, val_acc:0.883]
Epoch [47/120    avg_loss:0.442, val_acc:0.963]
Epoch [48/120    avg_loss:0.392, val_acc:0.923]
Epoch [49/120    avg_loss:0.410, val_acc:0.933]
Epoch [50/120    avg_loss:0.362, val_acc:0.885]
Epoch [51/120    avg_loss:0.332, val_acc:0.973]
Epoch [52/120    avg_loss:0.326, val_acc:0.975]
Epoch [53/120    avg_loss:0.291, val_acc:0.977]
Epoch [54/120    avg_loss:0.286, val_acc:0.973]
Epoch [55/120    avg_loss:0.312, val_acc:0.952]
Epoch [56/120    avg_loss:0.292, val_acc:0.973]
Epoch [57/120    avg_loss:0.257, val_acc:0.971]
Epoch [58/120    avg_loss:0.320, val_acc:0.948]
Epoch [59/120    avg_loss:0.270, val_acc:0.954]
Epoch [60/120    avg_loss:0.270, val_acc:0.975]
Epoch [61/120    avg_loss:0.263, val_acc:0.988]
Epoch [62/120    avg_loss:0.288, val_acc:0.950]
Epoch [63/120    avg_loss:0.244, val_acc:0.969]
Epoch [64/120    avg_loss:0.230, val_acc:0.967]
Epoch [65/120    avg_loss:0.197, val_acc:0.977]
Epoch [66/120    avg_loss:0.208, val_acc:0.952]
Epoch [67/120    avg_loss:0.226, val_acc:0.950]
Epoch [68/120    avg_loss:0.216, val_acc:0.969]
Epoch [69/120    avg_loss:0.210, val_acc:0.969]
Epoch [70/120    avg_loss:0.170, val_acc:0.977]
Epoch [71/120    avg_loss:0.174, val_acc:0.977]
Epoch [72/120    avg_loss:0.172, val_acc:0.973]
Epoch [73/120    avg_loss:0.167, val_acc:0.981]
Epoch [74/120    avg_loss:0.152, val_acc:0.975]
Epoch [75/120    avg_loss:0.169, val_acc:0.985]
Epoch [76/120    avg_loss:0.130, val_acc:0.985]
Epoch [77/120    avg_loss:0.127, val_acc:0.988]
Epoch [78/120    avg_loss:0.113, val_acc:0.985]
Epoch [79/120    avg_loss:0.112, val_acc:0.990]
Epoch [80/120    avg_loss:0.125, val_acc:0.994]
Epoch [81/120    avg_loss:0.115, val_acc:0.988]
Epoch [82/120    avg_loss:0.098, val_acc:0.988]
Epoch [83/120    avg_loss:0.107, val_acc:0.988]
Epoch [84/120    avg_loss:0.101, val_acc:0.985]
Epoch [85/120    avg_loss:0.107, val_acc:0.985]
Epoch [86/120    avg_loss:0.114, val_acc:0.988]
Epoch [87/120    avg_loss:0.097, val_acc:0.983]
Epoch [88/120    avg_loss:0.101, val_acc:0.990]
Epoch [89/120    avg_loss:0.098, val_acc:0.990]
Epoch [90/120    avg_loss:0.101, val_acc:0.988]
Epoch [91/120    avg_loss:0.109, val_acc:0.992]
Epoch [92/120    avg_loss:0.098, val_acc:0.988]
Epoch [93/120    avg_loss:0.099, val_acc:0.988]
Epoch [94/120    avg_loss:0.107, val_acc:0.988]
Epoch [95/120    avg_loss:0.095, val_acc:0.988]
Epoch [96/120    avg_loss:0.095, val_acc:0.988]
Epoch [97/120    avg_loss:0.093, val_acc:0.988]
Epoch [98/120    avg_loss:0.082, val_acc:0.988]
Epoch [99/120    avg_loss:0.094, val_acc:0.988]
Epoch [100/120    avg_loss:0.081, val_acc:0.988]
Epoch [101/120    avg_loss:0.094, val_acc:0.988]
Epoch [102/120    avg_loss:0.087, val_acc:0.988]
Epoch [103/120    avg_loss:0.092, val_acc:0.988]
Epoch [104/120    avg_loss:0.099, val_acc:0.988]
Epoch [105/120    avg_loss:0.094, val_acc:0.988]
Epoch [106/120    avg_loss:0.098, val_acc:0.988]
Epoch [107/120    avg_loss:0.091, val_acc:0.988]
Epoch [108/120    avg_loss:0.088, val_acc:0.988]
Epoch [109/120    avg_loss:0.091, val_acc:0.990]
Epoch [110/120    avg_loss:0.091, val_acc:0.988]
Epoch [111/120    avg_loss:0.097, val_acc:0.990]
Epoch [112/120    avg_loss:0.091, val_acc:0.988]
Epoch [113/120    avg_loss:0.106, val_acc:0.988]
Epoch [114/120    avg_loss:0.100, val_acc:0.988]
Epoch [115/120    avg_loss:0.092, val_acc:0.988]
Epoch [116/120    avg_loss:0.090, val_acc:0.988]
Epoch [117/120    avg_loss:0.098, val_acc:0.988]
Epoch [118/120    avg_loss:0.092, val_acc:0.988]
Epoch [119/120    avg_loss:0.104, val_acc:0.988]
Epoch [120/120    avg_loss:0.113, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 676   0   0   0   0   9   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217   9   0   0   0   0   0   0   1   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99265786 0.98871332 1.         0.96444444 0.94880546
 0.97619048 0.9726776  1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9926423766413902
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:00:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:20
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe7dfe40b70>
supervision:full
center_pixel:True
Network :
Number of parameter: 36107==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.631, val_acc:0.119]
Epoch [2/120    avg_loss:2.575, val_acc:0.371]
Epoch [3/120    avg_loss:2.524, val_acc:0.471]
Epoch [4/120    avg_loss:2.475, val_acc:0.479]
Epoch [5/120    avg_loss:2.420, val_acc:0.502]
Epoch [6/120    avg_loss:2.373, val_acc:0.527]
Epoch [7/120    avg_loss:2.322, val_acc:0.521]
Epoch [8/120    avg_loss:2.272, val_acc:0.504]
Epoch [9/120    avg_loss:2.227, val_acc:0.487]
Epoch [10/120    avg_loss:2.166, val_acc:0.492]
Epoch [11/120    avg_loss:2.111, val_acc:0.502]
Epoch [12/120    avg_loss:2.056, val_acc:0.506]
Epoch [13/120    avg_loss:2.001, val_acc:0.500]
Epoch [14/120    avg_loss:1.951, val_acc:0.510]
Epoch [15/120    avg_loss:1.891, val_acc:0.529]
Epoch [16/120    avg_loss:1.831, val_acc:0.558]
Epoch [17/120    avg_loss:1.778, val_acc:0.604]
Epoch [18/120    avg_loss:1.739, val_acc:0.602]
Epoch [19/120    avg_loss:1.663, val_acc:0.623]
Epoch [20/120    avg_loss:1.606, val_acc:0.615]
Epoch [21/120    avg_loss:1.544, val_acc:0.623]
Epoch [22/120    avg_loss:1.480, val_acc:0.646]
Epoch [23/120    avg_loss:1.418, val_acc:0.671]
Epoch [24/120    avg_loss:1.368, val_acc:0.733]
Epoch [25/120    avg_loss:1.321, val_acc:0.681]
Epoch [26/120    avg_loss:1.256, val_acc:0.723]
Epoch [27/120    avg_loss:1.217, val_acc:0.794]
Epoch [28/120    avg_loss:1.150, val_acc:0.762]
Epoch [29/120    avg_loss:1.074, val_acc:0.746]
Epoch [30/120    avg_loss:1.005, val_acc:0.760]
Epoch [31/120    avg_loss:1.006, val_acc:0.812]
Epoch [32/120    avg_loss:0.930, val_acc:0.865]
Epoch [33/120    avg_loss:0.872, val_acc:0.848]
Epoch [34/120    avg_loss:0.819, val_acc:0.838]
Epoch [35/120    avg_loss:0.792, val_acc:0.873]
Epoch [36/120    avg_loss:0.735, val_acc:0.910]
Epoch [37/120    avg_loss:0.709, val_acc:0.885]
Epoch [38/120    avg_loss:0.623, val_acc:0.904]
Epoch [39/120    avg_loss:0.623, val_acc:0.933]
Epoch [40/120    avg_loss:0.559, val_acc:0.902]
Epoch [41/120    avg_loss:0.569, val_acc:0.912]
Epoch [42/120    avg_loss:0.530, val_acc:0.927]
Epoch [43/120    avg_loss:0.507, val_acc:0.942]
Epoch [44/120    avg_loss:0.536, val_acc:0.935]
Epoch [45/120    avg_loss:0.548, val_acc:0.875]
Epoch [46/120    avg_loss:0.465, val_acc:0.927]
Epoch [47/120    avg_loss:0.484, val_acc:0.927]
Epoch [48/120    avg_loss:0.436, val_acc:0.919]
Epoch [49/120    avg_loss:0.424, val_acc:0.929]
Epoch [50/120    avg_loss:0.380, val_acc:0.958]
Epoch [51/120    avg_loss:0.374, val_acc:0.950]
Epoch [52/120    avg_loss:0.507, val_acc:0.881]
Epoch [53/120    avg_loss:0.432, val_acc:0.865]
Epoch [54/120    avg_loss:0.395, val_acc:0.929]
Epoch [55/120    avg_loss:0.346, val_acc:0.960]
Epoch [56/120    avg_loss:0.295, val_acc:0.944]
Epoch [57/120    avg_loss:0.321, val_acc:0.950]
Epoch [58/120    avg_loss:0.334, val_acc:0.935]
Epoch [59/120    avg_loss:0.333, val_acc:0.952]
Epoch [60/120    avg_loss:0.312, val_acc:0.877]
Epoch [61/120    avg_loss:0.288, val_acc:0.944]
Epoch [62/120    avg_loss:0.296, val_acc:0.919]
Epoch [63/120    avg_loss:0.313, val_acc:0.960]
Epoch [64/120    avg_loss:0.327, val_acc:0.927]
Epoch [65/120    avg_loss:0.305, val_acc:0.952]
Epoch [66/120    avg_loss:0.266, val_acc:0.931]
Epoch [67/120    avg_loss:0.310, val_acc:0.956]
Epoch [68/120    avg_loss:0.238, val_acc:0.956]
Epoch [69/120    avg_loss:0.215, val_acc:0.952]
Epoch [70/120    avg_loss:0.210, val_acc:0.971]
Epoch [71/120    avg_loss:0.190, val_acc:0.971]
Epoch [72/120    avg_loss:0.183, val_acc:0.956]
Epoch [73/120    avg_loss:0.188, val_acc:0.979]
Epoch [74/120    avg_loss:0.181, val_acc:0.969]
Epoch [75/120    avg_loss:0.189, val_acc:0.950]
Epoch [76/120    avg_loss:0.196, val_acc:0.942]
Epoch [77/120    avg_loss:0.224, val_acc:0.950]
Epoch [78/120    avg_loss:0.185, val_acc:0.963]
Epoch [79/120    avg_loss:0.165, val_acc:0.965]
Epoch [80/120    avg_loss:0.178, val_acc:0.971]
Epoch [81/120    avg_loss:0.176, val_acc:0.963]
Epoch [82/120    avg_loss:0.166, val_acc:0.952]
Epoch [83/120    avg_loss:0.189, val_acc:0.967]
Epoch [84/120    avg_loss:0.158, val_acc:0.942]
Epoch [85/120    avg_loss:0.153, val_acc:0.965]
Epoch [86/120    avg_loss:0.135, val_acc:0.965]
Epoch [87/120    avg_loss:0.125, val_acc:0.979]
Epoch [88/120    avg_loss:0.113, val_acc:0.981]
Epoch [89/120    avg_loss:0.100, val_acc:0.979]
Epoch [90/120    avg_loss:0.090, val_acc:0.979]
Epoch [91/120    avg_loss:0.104, val_acc:0.981]
Epoch [92/120    avg_loss:0.099, val_acc:0.979]
Epoch [93/120    avg_loss:0.093, val_acc:0.977]
Epoch [94/120    avg_loss:0.105, val_acc:0.977]
Epoch [95/120    avg_loss:0.095, val_acc:0.979]
Epoch [96/120    avg_loss:0.102, val_acc:0.977]
Epoch [97/120    avg_loss:0.101, val_acc:0.983]
Epoch [98/120    avg_loss:0.096, val_acc:0.979]
Epoch [99/120    avg_loss:0.093, val_acc:0.981]
Epoch [100/120    avg_loss:0.098, val_acc:0.979]
Epoch [101/120    avg_loss:0.104, val_acc:0.983]
Epoch [102/120    avg_loss:0.090, val_acc:0.985]
Epoch [103/120    avg_loss:0.101, val_acc:0.985]
Epoch [104/120    avg_loss:0.088, val_acc:0.979]
Epoch [105/120    avg_loss:0.097, val_acc:0.979]
Epoch [106/120    avg_loss:0.099, val_acc:0.985]
Epoch [107/120    avg_loss:0.081, val_acc:0.983]
Epoch [108/120    avg_loss:0.093, val_acc:0.979]
Epoch [109/120    avg_loss:0.091, val_acc:0.985]
Epoch [110/120    avg_loss:0.084, val_acc:0.985]
Epoch [111/120    avg_loss:0.074, val_acc:0.979]
Epoch [112/120    avg_loss:0.090, val_acc:0.979]
Epoch [113/120    avg_loss:0.091, val_acc:0.981]
Epoch [114/120    avg_loss:0.085, val_acc:0.981]
Epoch [115/120    avg_loss:0.089, val_acc:0.983]
Epoch [116/120    avg_loss:0.072, val_acc:0.981]
Epoch [117/120    avg_loss:0.097, val_acc:0.983]
Epoch [118/120    avg_loss:0.076, val_acc:0.981]
Epoch [119/120    avg_loss:0.094, val_acc:0.981]
Epoch [120/120    avg_loss:0.088, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   3 219   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0  26 119   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.95522388059702

F1 scores:
[       nan 0.99560117 0.97986577 0.99352052 0.9279661  0.88475836
 0.98564593 0.94972067 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9883683193124871
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efe99912860>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.301, val_acc:0.531]
Epoch [2/120    avg_loss:1.803, val_acc:0.615]
Epoch [3/120    avg_loss:1.448, val_acc:0.762]
Epoch [4/120    avg_loss:1.215, val_acc:0.701]
Epoch [5/120    avg_loss:0.988, val_acc:0.781]
Epoch [6/120    avg_loss:0.946, val_acc:0.795]
Epoch [7/120    avg_loss:0.852, val_acc:0.766]
Epoch [8/120    avg_loss:0.782, val_acc:0.809]
Epoch [9/120    avg_loss:0.730, val_acc:0.783]
Epoch [10/120    avg_loss:0.668, val_acc:0.826]
Epoch [11/120    avg_loss:0.597, val_acc:0.883]
Epoch [12/120    avg_loss:0.588, val_acc:0.883]
Epoch [13/120    avg_loss:0.549, val_acc:0.877]
Epoch [14/120    avg_loss:0.466, val_acc:0.879]
Epoch [15/120    avg_loss:0.502, val_acc:0.895]
Epoch [16/120    avg_loss:0.487, val_acc:0.889]
Epoch [17/120    avg_loss:0.477, val_acc:0.881]
Epoch [18/120    avg_loss:0.517, val_acc:0.869]
Epoch [19/120    avg_loss:0.442, val_acc:0.910]
Epoch [20/120    avg_loss:0.388, val_acc:0.906]
Epoch [21/120    avg_loss:0.337, val_acc:0.900]
Epoch [22/120    avg_loss:0.428, val_acc:0.922]
Epoch [23/120    avg_loss:0.383, val_acc:0.902]
Epoch [24/120    avg_loss:0.369, val_acc:0.902]
Epoch [25/120    avg_loss:0.332, val_acc:0.895]
Epoch [26/120    avg_loss:0.357, val_acc:0.924]
Epoch [27/120    avg_loss:0.272, val_acc:0.908]
Epoch [28/120    avg_loss:0.336, val_acc:0.914]
Epoch [29/120    avg_loss:0.261, val_acc:0.926]
Epoch [30/120    avg_loss:0.258, val_acc:0.945]
Epoch [31/120    avg_loss:0.289, val_acc:0.910]
Epoch [32/120    avg_loss:0.273, val_acc:0.934]
Epoch [33/120    avg_loss:0.207, val_acc:0.928]
Epoch [34/120    avg_loss:0.258, val_acc:0.932]
Epoch [35/120    avg_loss:0.274, val_acc:0.947]
Epoch [36/120    avg_loss:0.220, val_acc:0.928]
Epoch [37/120    avg_loss:0.204, val_acc:0.932]
Epoch [38/120    avg_loss:0.219, val_acc:0.926]
Epoch [39/120    avg_loss:0.269, val_acc:0.869]
Epoch [40/120    avg_loss:0.215, val_acc:0.941]
Epoch [41/120    avg_loss:0.183, val_acc:0.943]
Epoch [42/120    avg_loss:0.185, val_acc:0.939]
Epoch [43/120    avg_loss:0.198, val_acc:0.939]
Epoch [44/120    avg_loss:0.222, val_acc:0.910]
Epoch [45/120    avg_loss:0.165, val_acc:0.941]
Epoch [46/120    avg_loss:0.202, val_acc:0.945]
Epoch [47/120    avg_loss:0.189, val_acc:0.930]
Epoch [48/120    avg_loss:0.176, val_acc:0.932]
Epoch [49/120    avg_loss:0.157, val_acc:0.945]
Epoch [50/120    avg_loss:0.111, val_acc:0.949]
Epoch [51/120    avg_loss:0.114, val_acc:0.949]
Epoch [52/120    avg_loss:0.088, val_acc:0.951]
Epoch [53/120    avg_loss:0.086, val_acc:0.951]
Epoch [54/120    avg_loss:0.094, val_acc:0.953]
Epoch [55/120    avg_loss:0.100, val_acc:0.949]
Epoch [56/120    avg_loss:0.086, val_acc:0.951]
Epoch [57/120    avg_loss:0.077, val_acc:0.953]
Epoch [58/120    avg_loss:0.084, val_acc:0.953]
Epoch [59/120    avg_loss:0.084, val_acc:0.953]
Epoch [60/120    avg_loss:0.079, val_acc:0.951]
Epoch [61/120    avg_loss:0.091, val_acc:0.951]
Epoch [62/120    avg_loss:0.083, val_acc:0.959]
Epoch [63/120    avg_loss:0.066, val_acc:0.961]
Epoch [64/120    avg_loss:0.080, val_acc:0.953]
Epoch [65/120    avg_loss:0.072, val_acc:0.953]
Epoch [66/120    avg_loss:0.070, val_acc:0.957]
Epoch [67/120    avg_loss:0.070, val_acc:0.957]
Epoch [68/120    avg_loss:0.070, val_acc:0.959]
Epoch [69/120    avg_loss:0.065, val_acc:0.959]
Epoch [70/120    avg_loss:0.057, val_acc:0.959]
Epoch [71/120    avg_loss:0.052, val_acc:0.963]
Epoch [72/120    avg_loss:0.066, val_acc:0.961]
Epoch [73/120    avg_loss:0.050, val_acc:0.963]
Epoch [74/120    avg_loss:0.045, val_acc:0.955]
Epoch [75/120    avg_loss:0.061, val_acc:0.961]
Epoch [76/120    avg_loss:0.063, val_acc:0.961]
Epoch [77/120    avg_loss:0.062, val_acc:0.959]
Epoch [78/120    avg_loss:0.067, val_acc:0.963]
Epoch [79/120    avg_loss:0.073, val_acc:0.957]
Epoch [80/120    avg_loss:0.059, val_acc:0.959]
Epoch [81/120    avg_loss:0.069, val_acc:0.959]
Epoch [82/120    avg_loss:0.061, val_acc:0.959]
Epoch [83/120    avg_loss:0.060, val_acc:0.967]
Epoch [84/120    avg_loss:0.045, val_acc:0.959]
Epoch [85/120    avg_loss:0.060, val_acc:0.961]
Epoch [86/120    avg_loss:0.060, val_acc:0.969]
Epoch [87/120    avg_loss:0.066, val_acc:0.963]
Epoch [88/120    avg_loss:0.052, val_acc:0.967]
Epoch [89/120    avg_loss:0.044, val_acc:0.965]
Epoch [90/120    avg_loss:0.055, val_acc:0.963]
Epoch [91/120    avg_loss:0.049, val_acc:0.963]
Epoch [92/120    avg_loss:0.054, val_acc:0.963]
Epoch [93/120    avg_loss:0.045, val_acc:0.961]
Epoch [94/120    avg_loss:0.054, val_acc:0.959]
Epoch [95/120    avg_loss:0.059, val_acc:0.961]
Epoch [96/120    avg_loss:0.055, val_acc:0.959]
Epoch [97/120    avg_loss:0.049, val_acc:0.959]
Epoch [98/120    avg_loss:0.042, val_acc:0.961]
Epoch [99/120    avg_loss:0.047, val_acc:0.959]
Epoch [100/120    avg_loss:0.042, val_acc:0.961]
Epoch [101/120    avg_loss:0.041, val_acc:0.961]
Epoch [102/120    avg_loss:0.041, val_acc:0.961]
Epoch [103/120    avg_loss:0.043, val_acc:0.959]
Epoch [104/120    avg_loss:0.052, val_acc:0.963]
Epoch [105/120    avg_loss:0.038, val_acc:0.961]
Epoch [106/120    avg_loss:0.048, val_acc:0.961]
Epoch [107/120    avg_loss:0.050, val_acc:0.963]
Epoch [108/120    avg_loss:0.042, val_acc:0.963]
Epoch [109/120    avg_loss:0.040, val_acc:0.963]
Epoch [110/120    avg_loss:0.039, val_acc:0.965]
Epoch [111/120    avg_loss:0.045, val_acc:0.965]
Epoch [112/120    avg_loss:0.048, val_acc:0.965]
Epoch [113/120    avg_loss:0.044, val_acc:0.965]
Epoch [114/120    avg_loss:0.050, val_acc:0.965]
Epoch [115/120    avg_loss:0.040, val_acc:0.965]
Epoch [116/120    avg_loss:0.048, val_acc:0.965]
Epoch [117/120    avg_loss:0.054, val_acc:0.965]
Epoch [118/120    avg_loss:0.044, val_acc:0.965]
Epoch [119/120    avg_loss:0.054, val_acc:0.965]
Epoch [120/120    avg_loss:0.040, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   1 211   0   1   0   0   5   0   0   0   0   1   0]
 [  0   0   0 215  11   0   0   0   4   0   0   0   0   0]
 [  0   0   1   6 194  26   0   0   0   0   0   0   0   0]
 [  0   0   0   2  19 123   0   0   1   0   0   0   0   0]
 [  0   4   0   0   0   0 202   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   2 466   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.01705756929637

F1 scores:
[       nan 0.99636364 0.96127563 0.94922737 0.85840708 0.83673469
 0.99019608 0.93548387 0.99106003 0.99785867 1.         0.99867198
 0.99669239 1.        ]

Kappa:
0.977920167563133
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff9dbee8898>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.339, val_acc:0.559]
Epoch [2/120    avg_loss:1.785, val_acc:0.693]
Epoch [3/120    avg_loss:1.394, val_acc:0.760]
Epoch [4/120    avg_loss:1.119, val_acc:0.775]
Epoch [5/120    avg_loss:1.004, val_acc:0.723]
Epoch [6/120    avg_loss:0.871, val_acc:0.785]
Epoch [7/120    avg_loss:0.827, val_acc:0.729]
Epoch [8/120    avg_loss:0.789, val_acc:0.842]
Epoch [9/120    avg_loss:0.721, val_acc:0.826]
Epoch [10/120    avg_loss:0.698, val_acc:0.844]
Epoch [11/120    avg_loss:0.560, val_acc:0.855]
Epoch [12/120    avg_loss:0.645, val_acc:0.854]
Epoch [13/120    avg_loss:0.538, val_acc:0.883]
Epoch [14/120    avg_loss:0.522, val_acc:0.887]
Epoch [15/120    avg_loss:0.429, val_acc:0.895]
Epoch [16/120    avg_loss:0.393, val_acc:0.918]
Epoch [17/120    avg_loss:0.418, val_acc:0.932]
Epoch [18/120    avg_loss:0.384, val_acc:0.910]
Epoch [19/120    avg_loss:0.458, val_acc:0.896]
Epoch [20/120    avg_loss:0.378, val_acc:0.910]
Epoch [21/120    avg_loss:0.334, val_acc:0.871]
Epoch [22/120    avg_loss:0.379, val_acc:0.918]
Epoch [23/120    avg_loss:0.336, val_acc:0.914]
Epoch [24/120    avg_loss:0.386, val_acc:0.898]
Epoch [25/120    avg_loss:0.346, val_acc:0.930]
Epoch [26/120    avg_loss:0.245, val_acc:0.928]
Epoch [27/120    avg_loss:0.233, val_acc:0.932]
Epoch [28/120    avg_loss:0.234, val_acc:0.930]
Epoch [29/120    avg_loss:0.259, val_acc:0.938]
Epoch [30/120    avg_loss:0.286, val_acc:0.939]
Epoch [31/120    avg_loss:0.215, val_acc:0.918]
Epoch [32/120    avg_loss:0.238, val_acc:0.924]
Epoch [33/120    avg_loss:0.207, val_acc:0.941]
Epoch [34/120    avg_loss:0.211, val_acc:0.943]
Epoch [35/120    avg_loss:0.189, val_acc:0.916]
Epoch [36/120    avg_loss:0.235, val_acc:0.934]
Epoch [37/120    avg_loss:0.308, val_acc:0.918]
Epoch [38/120    avg_loss:0.215, val_acc:0.959]
Epoch [39/120    avg_loss:0.185, val_acc:0.943]
Epoch [40/120    avg_loss:0.172, val_acc:0.936]
Epoch [41/120    avg_loss:0.243, val_acc:0.957]
Epoch [42/120    avg_loss:0.231, val_acc:0.957]
Epoch [43/120    avg_loss:0.158, val_acc:0.953]
Epoch [44/120    avg_loss:0.185, val_acc:0.941]
Epoch [45/120    avg_loss:0.182, val_acc:0.938]
Epoch [46/120    avg_loss:0.159, val_acc:0.951]
Epoch [47/120    avg_loss:0.140, val_acc:0.957]
Epoch [48/120    avg_loss:0.184, val_acc:0.930]
Epoch [49/120    avg_loss:0.176, val_acc:0.961]
Epoch [50/120    avg_loss:0.147, val_acc:0.949]
Epoch [51/120    avg_loss:0.137, val_acc:0.961]
Epoch [52/120    avg_loss:0.146, val_acc:0.961]
Epoch [53/120    avg_loss:0.196, val_acc:0.947]
Epoch [54/120    avg_loss:0.146, val_acc:0.936]
Epoch [55/120    avg_loss:0.095, val_acc:0.955]
Epoch [56/120    avg_loss:0.115, val_acc:0.969]
Epoch [57/120    avg_loss:0.148, val_acc:0.963]
Epoch [58/120    avg_loss:0.091, val_acc:0.961]
Epoch [59/120    avg_loss:0.102, val_acc:0.949]
Epoch [60/120    avg_loss:0.118, val_acc:0.971]
Epoch [61/120    avg_loss:0.141, val_acc:0.969]
Epoch [62/120    avg_loss:0.101, val_acc:0.982]
Epoch [63/120    avg_loss:0.078, val_acc:0.969]
Epoch [64/120    avg_loss:0.163, val_acc:0.941]
Epoch [65/120    avg_loss:0.124, val_acc:0.973]
Epoch [66/120    avg_loss:0.090, val_acc:0.965]
Epoch [67/120    avg_loss:0.138, val_acc:0.945]
Epoch [68/120    avg_loss:0.107, val_acc:0.961]
Epoch [69/120    avg_loss:0.110, val_acc:0.971]
Epoch [70/120    avg_loss:0.104, val_acc:0.969]
Epoch [71/120    avg_loss:0.090, val_acc:0.971]
Epoch [72/120    avg_loss:0.094, val_acc:0.955]
Epoch [73/120    avg_loss:0.117, val_acc:0.949]
Epoch [74/120    avg_loss:0.151, val_acc:0.957]
Epoch [75/120    avg_loss:0.091, val_acc:0.973]
Epoch [76/120    avg_loss:0.053, val_acc:0.977]
Epoch [77/120    avg_loss:0.059, val_acc:0.977]
Epoch [78/120    avg_loss:0.046, val_acc:0.977]
Epoch [79/120    avg_loss:0.049, val_acc:0.980]
Epoch [80/120    avg_loss:0.041, val_acc:0.982]
Epoch [81/120    avg_loss:0.045, val_acc:0.980]
Epoch [82/120    avg_loss:0.041, val_acc:0.980]
Epoch [83/120    avg_loss:0.047, val_acc:0.984]
Epoch [84/120    avg_loss:0.045, val_acc:0.982]
Epoch [85/120    avg_loss:0.042, val_acc:0.986]
Epoch [86/120    avg_loss:0.042, val_acc:0.982]
Epoch [87/120    avg_loss:0.039, val_acc:0.984]
Epoch [88/120    avg_loss:0.037, val_acc:0.984]
Epoch [89/120    avg_loss:0.043, val_acc:0.982]
Epoch [90/120    avg_loss:0.038, val_acc:0.986]
Epoch [91/120    avg_loss:0.030, val_acc:0.986]
Epoch [92/120    avg_loss:0.027, val_acc:0.986]
Epoch [93/120    avg_loss:0.036, val_acc:0.986]
Epoch [94/120    avg_loss:0.040, val_acc:0.986]
Epoch [95/120    avg_loss:0.033, val_acc:0.986]
Epoch [96/120    avg_loss:0.034, val_acc:0.984]
Epoch [97/120    avg_loss:0.032, val_acc:0.984]
Epoch [98/120    avg_loss:0.035, val_acc:0.986]
Epoch [99/120    avg_loss:0.038, val_acc:0.984]
Epoch [100/120    avg_loss:0.033, val_acc:0.984]
Epoch [101/120    avg_loss:0.039, val_acc:0.984]
Epoch [102/120    avg_loss:0.038, val_acc:0.986]
Epoch [103/120    avg_loss:0.030, val_acc:0.986]
Epoch [104/120    avg_loss:0.040, val_acc:0.984]
Epoch [105/120    avg_loss:0.032, val_acc:0.986]
Epoch [106/120    avg_loss:0.035, val_acc:0.984]
Epoch [107/120    avg_loss:0.030, val_acc:0.984]
Epoch [108/120    avg_loss:0.037, val_acc:0.986]
Epoch [109/120    avg_loss:0.030, val_acc:0.986]
Epoch [110/120    avg_loss:0.028, val_acc:0.986]
Epoch [111/120    avg_loss:0.028, val_acc:0.986]
Epoch [112/120    avg_loss:0.032, val_acc:0.986]
Epoch [113/120    avg_loss:0.033, val_acc:0.986]
Epoch [114/120    avg_loss:0.031, val_acc:0.986]
Epoch [115/120    avg_loss:0.034, val_acc:0.986]
Epoch [116/120    avg_loss:0.026, val_acc:0.986]
Epoch [117/120    avg_loss:0.035, val_acc:0.986]
Epoch [118/120    avg_loss:0.032, val_acc:0.986]
Epoch [119/120    avg_loss:0.027, val_acc:0.986]
Epoch [120/120    avg_loss:0.031, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 208   0   0   0   0  11   0   0   0   0   0   0]
 [  0   0   0 221   3   0   0   0   3   3   0   0   0   0]
 [  0   0   0   0 203  24   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.7633262260128

F1 scores:
[       nan 0.99854227 0.95412844 0.98004435 0.93119266 0.91318328
 0.99512195 0.90052356 0.99614891 0.99680511 1.         1.
 0.99889503 1.        ]

Kappa:
0.9862320108127456
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f503ed91860>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.335, val_acc:0.615]
Epoch [2/120    avg_loss:1.804, val_acc:0.691]
Epoch [3/120    avg_loss:1.402, val_acc:0.754]
Epoch [4/120    avg_loss:1.167, val_acc:0.775]
Epoch [5/120    avg_loss:0.985, val_acc:0.701]
Epoch [6/120    avg_loss:0.829, val_acc:0.750]
Epoch [7/120    avg_loss:0.801, val_acc:0.830]
Epoch [8/120    avg_loss:0.815, val_acc:0.812]
Epoch [9/120    avg_loss:0.708, val_acc:0.852]
Epoch [10/120    avg_loss:0.627, val_acc:0.803]
Epoch [11/120    avg_loss:0.588, val_acc:0.867]
Epoch [12/120    avg_loss:0.545, val_acc:0.857]
Epoch [13/120    avg_loss:0.567, val_acc:0.861]
Epoch [14/120    avg_loss:0.581, val_acc:0.873]
Epoch [15/120    avg_loss:0.517, val_acc:0.842]
Epoch [16/120    avg_loss:0.496, val_acc:0.879]
Epoch [17/120    avg_loss:0.400, val_acc:0.857]
Epoch [18/120    avg_loss:0.400, val_acc:0.881]
Epoch [19/120    avg_loss:0.419, val_acc:0.914]
Epoch [20/120    avg_loss:0.430, val_acc:0.924]
Epoch [21/120    avg_loss:0.418, val_acc:0.883]
Epoch [22/120    avg_loss:0.388, val_acc:0.904]
Epoch [23/120    avg_loss:0.360, val_acc:0.930]
Epoch [24/120    avg_loss:0.370, val_acc:0.930]
Epoch [25/120    avg_loss:0.341, val_acc:0.891]
Epoch [26/120    avg_loss:0.383, val_acc:0.908]
Epoch [27/120    avg_loss:0.307, val_acc:0.896]
Epoch [28/120    avg_loss:0.342, val_acc:0.914]
Epoch [29/120    avg_loss:0.299, val_acc:0.916]
Epoch [30/120    avg_loss:0.285, val_acc:0.922]
Epoch [31/120    avg_loss:0.274, val_acc:0.910]
Epoch [32/120    avg_loss:0.339, val_acc:0.834]
Epoch [33/120    avg_loss:0.326, val_acc:0.906]
Epoch [34/120    avg_loss:0.292, val_acc:0.902]
Epoch [35/120    avg_loss:0.260, val_acc:0.939]
Epoch [36/120    avg_loss:0.315, val_acc:0.924]
Epoch [37/120    avg_loss:0.246, val_acc:0.924]
Epoch [38/120    avg_loss:0.249, val_acc:0.930]
Epoch [39/120    avg_loss:0.300, val_acc:0.947]
Epoch [40/120    avg_loss:0.217, val_acc:0.945]
Epoch [41/120    avg_loss:0.164, val_acc:0.932]
Epoch [42/120    avg_loss:0.232, val_acc:0.924]
Epoch [43/120    avg_loss:0.153, val_acc:0.908]
Epoch [44/120    avg_loss:0.200, val_acc:0.918]
Epoch [45/120    avg_loss:0.207, val_acc:0.939]
Epoch [46/120    avg_loss:0.200, val_acc:0.916]
Epoch [47/120    avg_loss:0.140, val_acc:0.941]
Epoch [48/120    avg_loss:0.145, val_acc:0.930]
Epoch [49/120    avg_loss:0.238, val_acc:0.951]
Epoch [50/120    avg_loss:0.166, val_acc:0.943]
Epoch [51/120    avg_loss:0.151, val_acc:0.951]
Epoch [52/120    avg_loss:0.215, val_acc:0.938]
Epoch [53/120    avg_loss:0.181, val_acc:0.941]
Epoch [54/120    avg_loss:0.175, val_acc:0.963]
Epoch [55/120    avg_loss:0.121, val_acc:0.959]
Epoch [56/120    avg_loss:0.171, val_acc:0.947]
Epoch [57/120    avg_loss:0.146, val_acc:0.943]
Epoch [58/120    avg_loss:0.126, val_acc:0.947]
Epoch [59/120    avg_loss:0.152, val_acc:0.957]
Epoch [60/120    avg_loss:0.185, val_acc:0.928]
Epoch [61/120    avg_loss:0.149, val_acc:0.953]
Epoch [62/120    avg_loss:0.154, val_acc:0.953]
Epoch [63/120    avg_loss:0.086, val_acc:0.957]
Epoch [64/120    avg_loss:0.084, val_acc:0.951]
Epoch [65/120    avg_loss:0.106, val_acc:0.947]
Epoch [66/120    avg_loss:0.094, val_acc:0.926]
Epoch [67/120    avg_loss:0.194, val_acc:0.959]
Epoch [68/120    avg_loss:0.085, val_acc:0.961]
Epoch [69/120    avg_loss:0.080, val_acc:0.957]
Epoch [70/120    avg_loss:0.072, val_acc:0.967]
Epoch [71/120    avg_loss:0.083, val_acc:0.965]
Epoch [72/120    avg_loss:0.066, val_acc:0.965]
Epoch [73/120    avg_loss:0.059, val_acc:0.971]
Epoch [74/120    avg_loss:0.058, val_acc:0.971]
Epoch [75/120    avg_loss:0.050, val_acc:0.967]
Epoch [76/120    avg_loss:0.060, val_acc:0.971]
Epoch [77/120    avg_loss:0.056, val_acc:0.969]
Epoch [78/120    avg_loss:0.047, val_acc:0.969]
Epoch [79/120    avg_loss:0.059, val_acc:0.965]
Epoch [80/120    avg_loss:0.051, val_acc:0.967]
Epoch [81/120    avg_loss:0.059, val_acc:0.969]
Epoch [82/120    avg_loss:0.050, val_acc:0.973]
Epoch [83/120    avg_loss:0.054, val_acc:0.973]
Epoch [84/120    avg_loss:0.045, val_acc:0.971]
Epoch [85/120    avg_loss:0.049, val_acc:0.973]
Epoch [86/120    avg_loss:0.051, val_acc:0.973]
Epoch [87/120    avg_loss:0.046, val_acc:0.971]
Epoch [88/120    avg_loss:0.038, val_acc:0.979]
Epoch [89/120    avg_loss:0.041, val_acc:0.973]
Epoch [90/120    avg_loss:0.044, val_acc:0.975]
Epoch [91/120    avg_loss:0.056, val_acc:0.977]
Epoch [92/120    avg_loss:0.041, val_acc:0.973]
Epoch [93/120    avg_loss:0.050, val_acc:0.973]
Epoch [94/120    avg_loss:0.040, val_acc:0.973]
Epoch [95/120    avg_loss:0.038, val_acc:0.973]
Epoch [96/120    avg_loss:0.043, val_acc:0.975]
Epoch [97/120    avg_loss:0.047, val_acc:0.975]
Epoch [98/120    avg_loss:0.040, val_acc:0.979]
Epoch [99/120    avg_loss:0.036, val_acc:0.979]
Epoch [100/120    avg_loss:0.034, val_acc:0.975]
Epoch [101/120    avg_loss:0.039, val_acc:0.975]
Epoch [102/120    avg_loss:0.042, val_acc:0.979]
Epoch [103/120    avg_loss:0.029, val_acc:0.973]
Epoch [104/120    avg_loss:0.037, val_acc:0.971]
Epoch [105/120    avg_loss:0.031, val_acc:0.977]
Epoch [106/120    avg_loss:0.030, val_acc:0.977]
Epoch [107/120    avg_loss:0.032, val_acc:0.979]
Epoch [108/120    avg_loss:0.042, val_acc:0.977]
Epoch [109/120    avg_loss:0.028, val_acc:0.979]
Epoch [110/120    avg_loss:0.036, val_acc:0.975]
Epoch [111/120    avg_loss:0.028, val_acc:0.975]
Epoch [112/120    avg_loss:0.036, val_acc:0.977]
Epoch [113/120    avg_loss:0.030, val_acc:0.975]
Epoch [114/120    avg_loss:0.039, val_acc:0.980]
Epoch [115/120    avg_loss:0.032, val_acc:0.980]
Epoch [116/120    avg_loss:0.032, val_acc:0.977]
Epoch [117/120    avg_loss:0.042, val_acc:0.975]
Epoch [118/120    avg_loss:0.035, val_acc:0.977]
Epoch [119/120    avg_loss:0.048, val_acc:0.975]
Epoch [120/120    avg_loss:0.032, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   2 210   0   0   0   0   5   0   0   0   0   2   0]
 [  0   0   0 222   7   0   0   0   1   0   0   0   0   0]
 [  0   0   1   0 222   3   0   0   0   0   0   0   1   0]
 [  0   0   0   0  21 123   0   0   1   0   0   0   0   0]
 [  0   2   0   0   1   0 203   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   3   0   0   0   0 831]]

Accuracy:
98.63539445628997

F1 scores:
[       nan 0.99708879 0.94808126 0.98230088 0.92887029 0.90774908
 0.99266504 0.91208791 0.99359795 1.         1.         0.99867198
 0.99339207 0.9981982 ]

Kappa:
0.9848049158446164
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4b23d25860>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.348, val_acc:0.586]
Epoch [2/120    avg_loss:1.773, val_acc:0.607]
Epoch [3/120    avg_loss:1.344, val_acc:0.756]
Epoch [4/120    avg_loss:1.127, val_acc:0.797]
Epoch [5/120    avg_loss:0.948, val_acc:0.801]
Epoch [6/120    avg_loss:0.800, val_acc:0.803]
Epoch [7/120    avg_loss:0.797, val_acc:0.855]
Epoch [8/120    avg_loss:0.644, val_acc:0.873]
Epoch [9/120    avg_loss:0.585, val_acc:0.861]
Epoch [10/120    avg_loss:0.505, val_acc:0.844]
Epoch [11/120    avg_loss:0.515, val_acc:0.881]
Epoch [12/120    avg_loss:0.560, val_acc:0.879]
Epoch [13/120    avg_loss:0.500, val_acc:0.867]
Epoch [14/120    avg_loss:0.513, val_acc:0.914]
Epoch [15/120    avg_loss:0.436, val_acc:0.883]
Epoch [16/120    avg_loss:0.433, val_acc:0.891]
Epoch [17/120    avg_loss:0.502, val_acc:0.893]
Epoch [18/120    avg_loss:0.410, val_acc:0.877]
Epoch [19/120    avg_loss:0.416, val_acc:0.908]
Epoch [20/120    avg_loss:0.369, val_acc:0.883]
Epoch [21/120    avg_loss:0.423, val_acc:0.910]
Epoch [22/120    avg_loss:0.367, val_acc:0.908]
Epoch [23/120    avg_loss:0.413, val_acc:0.906]
Epoch [24/120    avg_loss:0.332, val_acc:0.898]
Epoch [25/120    avg_loss:0.318, val_acc:0.898]
Epoch [26/120    avg_loss:0.321, val_acc:0.904]
Epoch [27/120    avg_loss:0.317, val_acc:0.881]
Epoch [28/120    avg_loss:0.259, val_acc:0.930]
Epoch [29/120    avg_loss:0.228, val_acc:0.941]
Epoch [30/120    avg_loss:0.203, val_acc:0.941]
Epoch [31/120    avg_loss:0.213, val_acc:0.949]
Epoch [32/120    avg_loss:0.199, val_acc:0.941]
Epoch [33/120    avg_loss:0.211, val_acc:0.945]
Epoch [34/120    avg_loss:0.192, val_acc:0.947]
Epoch [35/120    avg_loss:0.197, val_acc:0.951]
Epoch [36/120    avg_loss:0.174, val_acc:0.953]
Epoch [37/120    avg_loss:0.166, val_acc:0.953]
Epoch [38/120    avg_loss:0.174, val_acc:0.957]
Epoch [39/120    avg_loss:0.169, val_acc:0.955]
Epoch [40/120    avg_loss:0.170, val_acc:0.953]
Epoch [41/120    avg_loss:0.178, val_acc:0.957]
Epoch [42/120    avg_loss:0.170, val_acc:0.963]
Epoch [43/120    avg_loss:0.167, val_acc:0.957]
Epoch [44/120    avg_loss:0.170, val_acc:0.963]
Epoch [45/120    avg_loss:0.162, val_acc:0.963]
Epoch [46/120    avg_loss:0.152, val_acc:0.967]
Epoch [47/120    avg_loss:0.160, val_acc:0.963]
Epoch [48/120    avg_loss:0.153, val_acc:0.961]
Epoch [49/120    avg_loss:0.139, val_acc:0.963]
Epoch [50/120    avg_loss:0.154, val_acc:0.961]
Epoch [51/120    avg_loss:0.144, val_acc:0.961]
Epoch [52/120    avg_loss:0.155, val_acc:0.959]
Epoch [53/120    avg_loss:0.163, val_acc:0.961]
Epoch [54/120    avg_loss:0.153, val_acc:0.963]
Epoch [55/120    avg_loss:0.135, val_acc:0.963]
Epoch [56/120    avg_loss:0.157, val_acc:0.967]
Epoch [57/120    avg_loss:0.131, val_acc:0.963]
Epoch [58/120    avg_loss:0.154, val_acc:0.961]
Epoch [59/120    avg_loss:0.139, val_acc:0.965]
Epoch [60/120    avg_loss:0.126, val_acc:0.961]
Epoch [61/120    avg_loss:0.144, val_acc:0.973]
Epoch [62/120    avg_loss:0.144, val_acc:0.971]
Epoch [63/120    avg_loss:0.123, val_acc:0.967]
Epoch [64/120    avg_loss:0.139, val_acc:0.969]
Epoch [65/120    avg_loss:0.138, val_acc:0.967]
Epoch [66/120    avg_loss:0.132, val_acc:0.963]
Epoch [67/120    avg_loss:0.112, val_acc:0.963]
Epoch [68/120    avg_loss:0.135, val_acc:0.963]
Epoch [69/120    avg_loss:0.125, val_acc:0.963]
Epoch [70/120    avg_loss:0.122, val_acc:0.967]
Epoch [71/120    avg_loss:0.135, val_acc:0.967]
Epoch [72/120    avg_loss:0.140, val_acc:0.971]
Epoch [73/120    avg_loss:0.133, val_acc:0.977]
Epoch [74/120    avg_loss:0.107, val_acc:0.973]
Epoch [75/120    avg_loss:0.106, val_acc:0.969]
Epoch [76/120    avg_loss:0.112, val_acc:0.977]
Epoch [77/120    avg_loss:0.125, val_acc:0.973]
Epoch [78/120    avg_loss:0.129, val_acc:0.961]
Epoch [79/120    avg_loss:0.112, val_acc:0.973]
Epoch [80/120    avg_loss:0.109, val_acc:0.971]
Epoch [81/120    avg_loss:0.135, val_acc:0.969]
Epoch [82/120    avg_loss:0.115, val_acc:0.965]
Epoch [83/120    avg_loss:0.113, val_acc:0.975]
Epoch [84/120    avg_loss:0.115, val_acc:0.971]
Epoch [85/120    avg_loss:0.106, val_acc:0.977]
Epoch [86/120    avg_loss:0.108, val_acc:0.975]
Epoch [87/120    avg_loss:0.104, val_acc:0.979]
Epoch [88/120    avg_loss:0.110, val_acc:0.973]
Epoch [89/120    avg_loss:0.098, val_acc:0.977]
Epoch [90/120    avg_loss:0.106, val_acc:0.971]
Epoch [91/120    avg_loss:0.110, val_acc:0.969]
Epoch [92/120    avg_loss:0.091, val_acc:0.973]
Epoch [93/120    avg_loss:0.096, val_acc:0.971]
Epoch [94/120    avg_loss:0.116, val_acc:0.965]
Epoch [95/120    avg_loss:0.110, val_acc:0.971]
Epoch [96/120    avg_loss:0.106, val_acc:0.979]
Epoch [97/120    avg_loss:0.094, val_acc:0.971]
Epoch [98/120    avg_loss:0.098, val_acc:0.980]
Epoch [99/120    avg_loss:0.094, val_acc:0.979]
Epoch [100/120    avg_loss:0.114, val_acc:0.975]
Epoch [101/120    avg_loss:0.094, val_acc:0.971]
Epoch [102/120    avg_loss:0.085, val_acc:0.979]
Epoch [103/120    avg_loss:0.088, val_acc:0.969]
Epoch [104/120    avg_loss:0.101, val_acc:0.975]
Epoch [105/120    avg_loss:0.090, val_acc:0.975]
Epoch [106/120    avg_loss:0.107, val_acc:0.969]
Epoch [107/120    avg_loss:0.100, val_acc:0.979]
Epoch [108/120    avg_loss:0.077, val_acc:0.977]
Epoch [109/120    avg_loss:0.100, val_acc:0.975]
Epoch [110/120    avg_loss:0.117, val_acc:0.980]
Epoch [111/120    avg_loss:0.089, val_acc:0.977]
Epoch [112/120    avg_loss:0.105, val_acc:0.975]
Epoch [113/120    avg_loss:0.086, val_acc:0.971]
Epoch [114/120    avg_loss:0.071, val_acc:0.975]
Epoch [115/120    avg_loss:0.067, val_acc:0.982]
Epoch [116/120    avg_loss:0.067, val_acc:0.979]
Epoch [117/120    avg_loss:0.106, val_acc:0.975]
Epoch [118/120    avg_loss:0.081, val_acc:0.975]
Epoch [119/120    avg_loss:0.081, val_acc:0.975]
Epoch [120/120    avg_loss:0.102, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 186   0   0   0   0  33   0   0   0   0   0   0]
 [  0   0   0 198  25   0   0   2   4   1   0   0   0   0]
 [  0   0   0   0 201  25   0   0   0   0   0   0   1   0]
 [  0   0   0   0  29 116   0   0   0   0   0   0   0   0]
 [  0  10   0   0   0   0 196   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   5   0   0   0   0   0   1   0   0   0 447   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0   0 832]]

Accuracy:
96.92963752665246

F1 scores:
[       nan 0.99275362 0.89208633 0.92523364 0.8340249  0.81118881
 0.97512438 0.81651376 0.99359795 0.99893276 1.         0.99867198
 0.99113082 0.99879952]

Kappa:
0.9658171085736013
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6767780898>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.322, val_acc:0.566]
Epoch [2/120    avg_loss:1.792, val_acc:0.611]
Epoch [3/120    avg_loss:1.401, val_acc:0.736]
Epoch [4/120    avg_loss:1.135, val_acc:0.771]
Epoch [5/120    avg_loss:1.002, val_acc:0.756]
Epoch [6/120    avg_loss:0.920, val_acc:0.822]
Epoch [7/120    avg_loss:0.803, val_acc:0.844]
Epoch [8/120    avg_loss:0.735, val_acc:0.773]
Epoch [9/120    avg_loss:0.639, val_acc:0.785]
Epoch [10/120    avg_loss:0.616, val_acc:0.855]
Epoch [11/120    avg_loss:0.591, val_acc:0.875]
Epoch [12/120    avg_loss:0.538, val_acc:0.857]
Epoch [13/120    avg_loss:0.537, val_acc:0.871]
Epoch [14/120    avg_loss:0.469, val_acc:0.881]
Epoch [15/120    avg_loss:0.433, val_acc:0.863]
Epoch [16/120    avg_loss:0.477, val_acc:0.898]
Epoch [17/120    avg_loss:0.488, val_acc:0.900]
Epoch [18/120    avg_loss:0.459, val_acc:0.863]
Epoch [19/120    avg_loss:0.447, val_acc:0.838]
Epoch [20/120    avg_loss:0.439, val_acc:0.891]
Epoch [21/120    avg_loss:0.348, val_acc:0.906]
Epoch [22/120    avg_loss:0.358, val_acc:0.914]
Epoch [23/120    avg_loss:0.355, val_acc:0.924]
Epoch [24/120    avg_loss:0.283, val_acc:0.930]
Epoch [25/120    avg_loss:0.392, val_acc:0.879]
Epoch [26/120    avg_loss:0.374, val_acc:0.910]
Epoch [27/120    avg_loss:0.287, val_acc:0.924]
Epoch [28/120    avg_loss:0.308, val_acc:0.934]
Epoch [29/120    avg_loss:0.292, val_acc:0.861]
Epoch [30/120    avg_loss:0.304, val_acc:0.914]
Epoch [31/120    avg_loss:0.281, val_acc:0.922]
Epoch [32/120    avg_loss:0.292, val_acc:0.904]
Epoch [33/120    avg_loss:0.267, val_acc:0.930]
Epoch [34/120    avg_loss:0.242, val_acc:0.936]
Epoch [35/120    avg_loss:0.257, val_acc:0.938]
Epoch [36/120    avg_loss:0.198, val_acc:0.932]
Epoch [37/120    avg_loss:0.210, val_acc:0.949]
Epoch [38/120    avg_loss:0.250, val_acc:0.906]
Epoch [39/120    avg_loss:0.208, val_acc:0.943]
Epoch [40/120    avg_loss:0.248, val_acc:0.965]
Epoch [41/120    avg_loss:0.243, val_acc:0.947]
Epoch [42/120    avg_loss:0.190, val_acc:0.947]
Epoch [43/120    avg_loss:0.179, val_acc:0.953]
Epoch [44/120    avg_loss:0.247, val_acc:0.926]
Epoch [45/120    avg_loss:0.205, val_acc:0.908]
Epoch [46/120    avg_loss:0.205, val_acc:0.959]
Epoch [47/120    avg_loss:0.169, val_acc:0.953]
Epoch [48/120    avg_loss:0.175, val_acc:0.957]
Epoch [49/120    avg_loss:0.134, val_acc:0.920]
Epoch [50/120    avg_loss:0.155, val_acc:0.949]
Epoch [51/120    avg_loss:0.168, val_acc:0.922]
Epoch [52/120    avg_loss:0.176, val_acc:0.930]
Epoch [53/120    avg_loss:0.152, val_acc:0.959]
Epoch [54/120    avg_loss:0.092, val_acc:0.965]
Epoch [55/120    avg_loss:0.086, val_acc:0.967]
Epoch [56/120    avg_loss:0.088, val_acc:0.969]
Epoch [57/120    avg_loss:0.095, val_acc:0.973]
Epoch [58/120    avg_loss:0.083, val_acc:0.971]
Epoch [59/120    avg_loss:0.066, val_acc:0.969]
Epoch [60/120    avg_loss:0.079, val_acc:0.971]
Epoch [61/120    avg_loss:0.087, val_acc:0.967]
Epoch [62/120    avg_loss:0.080, val_acc:0.971]
Epoch [63/120    avg_loss:0.081, val_acc:0.971]
Epoch [64/120    avg_loss:0.080, val_acc:0.969]
Epoch [65/120    avg_loss:0.069, val_acc:0.971]
Epoch [66/120    avg_loss:0.068, val_acc:0.971]
Epoch [67/120    avg_loss:0.077, val_acc:0.973]
Epoch [68/120    avg_loss:0.069, val_acc:0.973]
Epoch [69/120    avg_loss:0.078, val_acc:0.971]
Epoch [70/120    avg_loss:0.074, val_acc:0.975]
Epoch [71/120    avg_loss:0.069, val_acc:0.969]
Epoch [72/120    avg_loss:0.069, val_acc:0.975]
Epoch [73/120    avg_loss:0.079, val_acc:0.973]
Epoch [74/120    avg_loss:0.075, val_acc:0.973]
Epoch [75/120    avg_loss:0.064, val_acc:0.971]
Epoch [76/120    avg_loss:0.073, val_acc:0.969]
Epoch [77/120    avg_loss:0.061, val_acc:0.973]
Epoch [78/120    avg_loss:0.064, val_acc:0.973]
Epoch [79/120    avg_loss:0.062, val_acc:0.973]
Epoch [80/120    avg_loss:0.061, val_acc:0.971]
Epoch [81/120    avg_loss:0.061, val_acc:0.971]
Epoch [82/120    avg_loss:0.065, val_acc:0.973]
Epoch [83/120    avg_loss:0.055, val_acc:0.973]
Epoch [84/120    avg_loss:0.080, val_acc:0.979]
Epoch [85/120    avg_loss:0.059, val_acc:0.975]
Epoch [86/120    avg_loss:0.058, val_acc:0.973]
Epoch [87/120    avg_loss:0.081, val_acc:0.969]
Epoch [88/120    avg_loss:0.056, val_acc:0.971]
Epoch [89/120    avg_loss:0.059, val_acc:0.971]
Epoch [90/120    avg_loss:0.058, val_acc:0.971]
Epoch [91/120    avg_loss:0.060, val_acc:0.965]
Epoch [92/120    avg_loss:0.043, val_acc:0.969]
Epoch [93/120    avg_loss:0.057, val_acc:0.973]
Epoch [94/120    avg_loss:0.050, val_acc:0.973]
Epoch [95/120    avg_loss:0.059, val_acc:0.969]
Epoch [96/120    avg_loss:0.053, val_acc:0.971]
Epoch [97/120    avg_loss:0.046, val_acc:0.971]
Epoch [98/120    avg_loss:0.049, val_acc:0.969]
Epoch [99/120    avg_loss:0.055, val_acc:0.969]
Epoch [100/120    avg_loss:0.043, val_acc:0.967]
Epoch [101/120    avg_loss:0.061, val_acc:0.967]
Epoch [102/120    avg_loss:0.057, val_acc:0.967]
Epoch [103/120    avg_loss:0.053, val_acc:0.967]
Epoch [104/120    avg_loss:0.050, val_acc:0.967]
Epoch [105/120    avg_loss:0.047, val_acc:0.967]
Epoch [106/120    avg_loss:0.046, val_acc:0.967]
Epoch [107/120    avg_loss:0.052, val_acc:0.967]
Epoch [108/120    avg_loss:0.047, val_acc:0.969]
Epoch [109/120    avg_loss:0.057, val_acc:0.969]
Epoch [110/120    avg_loss:0.048, val_acc:0.969]
Epoch [111/120    avg_loss:0.048, val_acc:0.969]
Epoch [112/120    avg_loss:0.058, val_acc:0.969]
Epoch [113/120    avg_loss:0.049, val_acc:0.969]
Epoch [114/120    avg_loss:0.056, val_acc:0.969]
Epoch [115/120    avg_loss:0.044, val_acc:0.969]
Epoch [116/120    avg_loss:0.052, val_acc:0.969]
Epoch [117/120    avg_loss:0.054, val_acc:0.969]
Epoch [118/120    avg_loss:0.052, val_acc:0.969]
Epoch [119/120    avg_loss:0.045, val_acc:0.969]
Epoch [120/120    avg_loss:0.049, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   3   3   0   0   0   1   0]
 [  0   0   0 209  13   1   0   0   7   0   0   0   0   0]
 [  0   0   0   0 209  17   0   0   1   0   0   0   0   0]
 [  0   0   0   0  28 116   0   0   1   0   0   0   0   0]
 [  0   8   0   0   2   0 196   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   2 466   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 372   5   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.63326226012794

F1 scores:
[       nan 0.99419448 0.94222222 0.95216401 0.87265136 0.83154122
 0.97512438 0.9039548  0.98227848 0.99785867 1.         0.99332443
 0.98787211 1.        ]

Kappa:
0.9736408818636163
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f88e5cee908>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.309, val_acc:0.674]
Epoch [2/120    avg_loss:1.789, val_acc:0.670]
Epoch [3/120    avg_loss:1.410, val_acc:0.754]
Epoch [4/120    avg_loss:1.099, val_acc:0.801]
Epoch [5/120    avg_loss:0.954, val_acc:0.803]
Epoch [6/120    avg_loss:0.848, val_acc:0.836]
Epoch [7/120    avg_loss:0.764, val_acc:0.855]
Epoch [8/120    avg_loss:0.630, val_acc:0.830]
Epoch [9/120    avg_loss:0.703, val_acc:0.875]
Epoch [10/120    avg_loss:0.550, val_acc:0.754]
Epoch [11/120    avg_loss:0.572, val_acc:0.852]
Epoch [12/120    avg_loss:0.567, val_acc:0.738]
Epoch [13/120    avg_loss:0.492, val_acc:0.877]
Epoch [14/120    avg_loss:0.459, val_acc:0.902]
Epoch [15/120    avg_loss:0.482, val_acc:0.904]
Epoch [16/120    avg_loss:0.375, val_acc:0.928]
Epoch [17/120    avg_loss:0.410, val_acc:0.898]
Epoch [18/120    avg_loss:0.376, val_acc:0.916]
Epoch [19/120    avg_loss:0.397, val_acc:0.879]
Epoch [20/120    avg_loss:0.387, val_acc:0.938]
Epoch [21/120    avg_loss:0.419, val_acc:0.916]
Epoch [22/120    avg_loss:0.380, val_acc:0.902]
Epoch [23/120    avg_loss:0.379, val_acc:0.934]
Epoch [24/120    avg_loss:0.294, val_acc:0.930]
Epoch [25/120    avg_loss:0.335, val_acc:0.951]
Epoch [26/120    avg_loss:0.353, val_acc:0.932]
Epoch [27/120    avg_loss:0.277, val_acc:0.930]
Epoch [28/120    avg_loss:0.273, val_acc:0.953]
Epoch [29/120    avg_loss:0.205, val_acc:0.959]
Epoch [30/120    avg_loss:0.258, val_acc:0.939]
Epoch [31/120    avg_loss:0.242, val_acc:0.957]
Epoch [32/120    avg_loss:0.287, val_acc:0.844]
Epoch [33/120    avg_loss:0.326, val_acc:0.947]
Epoch [34/120    avg_loss:0.253, val_acc:0.953]
Epoch [35/120    avg_loss:0.229, val_acc:0.949]
Epoch [36/120    avg_loss:0.220, val_acc:0.941]
Epoch [37/120    avg_loss:0.199, val_acc:0.938]
Epoch [38/120    avg_loss:0.248, val_acc:0.932]
Epoch [39/120    avg_loss:0.215, val_acc:0.963]
Epoch [40/120    avg_loss:0.225, val_acc:0.953]
Epoch [41/120    avg_loss:0.152, val_acc:0.959]
Epoch [42/120    avg_loss:0.172, val_acc:0.920]
Epoch [43/120    avg_loss:0.226, val_acc:0.957]
Epoch [44/120    avg_loss:0.191, val_acc:0.969]
Epoch [45/120    avg_loss:0.151, val_acc:0.949]
Epoch [46/120    avg_loss:0.161, val_acc:0.953]
Epoch [47/120    avg_loss:0.219, val_acc:0.943]
Epoch [48/120    avg_loss:0.152, val_acc:0.949]
Epoch [49/120    avg_loss:0.144, val_acc:0.965]
Epoch [50/120    avg_loss:0.204, val_acc:0.955]
Epoch [51/120    avg_loss:0.170, val_acc:0.973]
Epoch [52/120    avg_loss:0.164, val_acc:0.977]
Epoch [53/120    avg_loss:0.144, val_acc:0.975]
Epoch [54/120    avg_loss:0.126, val_acc:0.971]
Epoch [55/120    avg_loss:0.145, val_acc:0.969]
Epoch [56/120    avg_loss:0.164, val_acc:0.957]
Epoch [57/120    avg_loss:0.149, val_acc:0.963]
Epoch [58/120    avg_loss:0.101, val_acc:0.973]
Epoch [59/120    avg_loss:0.094, val_acc:0.967]
Epoch [60/120    avg_loss:0.118, val_acc:0.973]
Epoch [61/120    avg_loss:0.110, val_acc:0.977]
Epoch [62/120    avg_loss:0.113, val_acc:0.973]
Epoch [63/120    avg_loss:0.133, val_acc:0.979]
Epoch [64/120    avg_loss:0.147, val_acc:0.973]
Epoch [65/120    avg_loss:0.119, val_acc:0.932]
Epoch [66/120    avg_loss:0.121, val_acc:0.967]
Epoch [67/120    avg_loss:0.094, val_acc:0.969]
Epoch [68/120    avg_loss:0.148, val_acc:0.955]
Epoch [69/120    avg_loss:0.096, val_acc:0.977]
Epoch [70/120    avg_loss:0.069, val_acc:0.980]
Epoch [71/120    avg_loss:0.078, val_acc:0.971]
Epoch [72/120    avg_loss:0.111, val_acc:0.955]
Epoch [73/120    avg_loss:0.054, val_acc:0.982]
Epoch [74/120    avg_loss:0.052, val_acc:0.971]
Epoch [75/120    avg_loss:0.077, val_acc:0.977]
Epoch [76/120    avg_loss:0.069, val_acc:0.977]
Epoch [77/120    avg_loss:0.085, val_acc:0.977]
Epoch [78/120    avg_loss:0.074, val_acc:0.979]
Epoch [79/120    avg_loss:0.059, val_acc:0.965]
Epoch [80/120    avg_loss:0.051, val_acc:0.961]
Epoch [81/120    avg_loss:0.086, val_acc:0.967]
Epoch [82/120    avg_loss:0.101, val_acc:0.939]
Epoch [83/120    avg_loss:0.096, val_acc:0.965]
Epoch [84/120    avg_loss:0.068, val_acc:0.990]
Epoch [85/120    avg_loss:0.044, val_acc:0.984]
Epoch [86/120    avg_loss:0.045, val_acc:0.982]
Epoch [87/120    avg_loss:0.073, val_acc:0.984]
Epoch [88/120    avg_loss:0.031, val_acc:0.986]
Epoch [89/120    avg_loss:0.085, val_acc:0.953]
Epoch [90/120    avg_loss:0.110, val_acc:0.969]
Epoch [91/120    avg_loss:0.073, val_acc:0.977]
Epoch [92/120    avg_loss:0.053, val_acc:0.980]
Epoch [93/120    avg_loss:0.051, val_acc:0.988]
Epoch [94/120    avg_loss:0.059, val_acc:0.975]
Epoch [95/120    avg_loss:0.027, val_acc:0.980]
Epoch [96/120    avg_loss:0.035, val_acc:0.988]
Epoch [97/120    avg_loss:0.045, val_acc:0.988]
Epoch [98/120    avg_loss:0.037, val_acc:0.986]
Epoch [99/120    avg_loss:0.028, val_acc:0.984]
Epoch [100/120    avg_loss:0.024, val_acc:0.986]
Epoch [101/120    avg_loss:0.023, val_acc:0.988]
Epoch [102/120    avg_loss:0.014, val_acc:0.986]
Epoch [103/120    avg_loss:0.017, val_acc:0.984]
Epoch [104/120    avg_loss:0.016, val_acc:0.988]
Epoch [105/120    avg_loss:0.016, val_acc:0.986]
Epoch [106/120    avg_loss:0.015, val_acc:0.986]
Epoch [107/120    avg_loss:0.014, val_acc:0.986]
Epoch [108/120    avg_loss:0.021, val_acc:0.986]
Epoch [109/120    avg_loss:0.015, val_acc:0.984]
Epoch [110/120    avg_loss:0.012, val_acc:0.986]
Epoch [111/120    avg_loss:0.016, val_acc:0.986]
Epoch [112/120    avg_loss:0.018, val_acc:0.986]
Epoch [113/120    avg_loss:0.014, val_acc:0.986]
Epoch [114/120    avg_loss:0.015, val_acc:0.986]
Epoch [115/120    avg_loss:0.016, val_acc:0.986]
Epoch [116/120    avg_loss:0.016, val_acc:0.986]
Epoch [117/120    avg_loss:0.016, val_acc:0.986]
Epoch [118/120    avg_loss:0.016, val_acc:0.986]
Epoch [119/120    avg_loss:0.014, val_acc:0.986]
Epoch [120/120    avg_loss:0.016, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 219   7   0   0   0   2   2   0   0   0   0]
 [  0   0   0   4 213  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 128   3   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   2 466   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 0.99927061 0.9977221  0.96688742 0.92407809 0.90459364
 0.99033816 0.99465241 0.99487179 0.9957265  1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9888420471631678
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3f07ccf860>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.263, val_acc:0.447]
Epoch [2/120    avg_loss:1.804, val_acc:0.691]
Epoch [3/120    avg_loss:1.450, val_acc:0.709]
Epoch [4/120    avg_loss:1.175, val_acc:0.768]
Epoch [5/120    avg_loss:1.012, val_acc:0.818]
Epoch [6/120    avg_loss:0.905, val_acc:0.793]
Epoch [7/120    avg_loss:0.816, val_acc:0.838]
Epoch [8/120    avg_loss:0.724, val_acc:0.746]
Epoch [9/120    avg_loss:0.727, val_acc:0.867]
Epoch [10/120    avg_loss:0.621, val_acc:0.840]
Epoch [11/120    avg_loss:0.643, val_acc:0.875]
Epoch [12/120    avg_loss:0.602, val_acc:0.869]
Epoch [13/120    avg_loss:0.527, val_acc:0.885]
Epoch [14/120    avg_loss:0.559, val_acc:0.896]
Epoch [15/120    avg_loss:0.448, val_acc:0.881]
Epoch [16/120    avg_loss:0.477, val_acc:0.881]
Epoch [17/120    avg_loss:0.457, val_acc:0.885]
Epoch [18/120    avg_loss:0.476, val_acc:0.896]
Epoch [19/120    avg_loss:0.377, val_acc:0.887]
Epoch [20/120    avg_loss:0.367, val_acc:0.879]
Epoch [21/120    avg_loss:0.443, val_acc:0.879]
Epoch [22/120    avg_loss:0.364, val_acc:0.900]
Epoch [23/120    avg_loss:0.402, val_acc:0.906]
Epoch [24/120    avg_loss:0.368, val_acc:0.902]
Epoch [25/120    avg_loss:0.392, val_acc:0.900]
Epoch [26/120    avg_loss:0.350, val_acc:0.906]
Epoch [27/120    avg_loss:0.418, val_acc:0.887]
Epoch [28/120    avg_loss:0.328, val_acc:0.877]
Epoch [29/120    avg_loss:0.306, val_acc:0.906]
Epoch [30/120    avg_loss:0.379, val_acc:0.904]
Epoch [31/120    avg_loss:0.275, val_acc:0.938]
Epoch [32/120    avg_loss:0.221, val_acc:0.908]
Epoch [33/120    avg_loss:0.301, val_acc:0.912]
Epoch [34/120    avg_loss:0.264, val_acc:0.930]
Epoch [35/120    avg_loss:0.235, val_acc:0.934]
Epoch [36/120    avg_loss:0.265, val_acc:0.930]
Epoch [37/120    avg_loss:0.258, val_acc:0.912]
Epoch [38/120    avg_loss:0.279, val_acc:0.891]
Epoch [39/120    avg_loss:0.281, val_acc:0.941]
Epoch [40/120    avg_loss:0.233, val_acc:0.924]
Epoch [41/120    avg_loss:0.181, val_acc:0.953]
Epoch [42/120    avg_loss:0.176, val_acc:0.877]
Epoch [43/120    avg_loss:0.277, val_acc:0.922]
Epoch [44/120    avg_loss:0.165, val_acc:0.953]
Epoch [45/120    avg_loss:0.212, val_acc:0.926]
Epoch [46/120    avg_loss:0.229, val_acc:0.918]
Epoch [47/120    avg_loss:0.202, val_acc:0.953]
Epoch [48/120    avg_loss:0.145, val_acc:0.938]
Epoch [49/120    avg_loss:0.156, val_acc:0.961]
Epoch [50/120    avg_loss:0.135, val_acc:0.951]
Epoch [51/120    avg_loss:0.126, val_acc:0.939]
Epoch [52/120    avg_loss:0.172, val_acc:0.924]
Epoch [53/120    avg_loss:0.149, val_acc:0.953]
Epoch [54/120    avg_loss:0.109, val_acc:0.961]
Epoch [55/120    avg_loss:0.131, val_acc:0.941]
Epoch [56/120    avg_loss:0.141, val_acc:0.957]
Epoch [57/120    avg_loss:0.115, val_acc:0.939]
Epoch [58/120    avg_loss:0.126, val_acc:0.947]
Epoch [59/120    avg_loss:0.119, val_acc:0.949]
Epoch [60/120    avg_loss:0.090, val_acc:0.955]
Epoch [61/120    avg_loss:0.131, val_acc:0.910]
Epoch [62/120    avg_loss:0.097, val_acc:0.963]
Epoch [63/120    avg_loss:0.079, val_acc:0.955]
Epoch [64/120    avg_loss:0.122, val_acc:0.918]
Epoch [65/120    avg_loss:0.144, val_acc:0.953]
Epoch [66/120    avg_loss:0.104, val_acc:0.959]
Epoch [67/120    avg_loss:0.104, val_acc:0.936]
Epoch [68/120    avg_loss:0.130, val_acc:0.947]
Epoch [69/120    avg_loss:0.097, val_acc:0.961]
Epoch [70/120    avg_loss:0.105, val_acc:0.959]
Epoch [71/120    avg_loss:0.043, val_acc:0.965]
Epoch [72/120    avg_loss:0.047, val_acc:0.967]
Epoch [73/120    avg_loss:0.079, val_acc:0.971]
Epoch [74/120    avg_loss:0.083, val_acc:0.953]
Epoch [75/120    avg_loss:0.089, val_acc:0.969]
Epoch [76/120    avg_loss:0.058, val_acc:0.969]
Epoch [77/120    avg_loss:0.079, val_acc:0.955]
Epoch [78/120    avg_loss:0.064, val_acc:0.965]
Epoch [79/120    avg_loss:0.061, val_acc:0.961]
Epoch [80/120    avg_loss:0.072, val_acc:0.969]
Epoch [81/120    avg_loss:0.051, val_acc:0.955]
Epoch [82/120    avg_loss:0.058, val_acc:0.969]
Epoch [83/120    avg_loss:0.056, val_acc:0.965]
Epoch [84/120    avg_loss:0.061, val_acc:0.963]
Epoch [85/120    avg_loss:0.072, val_acc:0.959]
Epoch [86/120    avg_loss:0.048, val_acc:0.961]
Epoch [87/120    avg_loss:0.033, val_acc:0.965]
Epoch [88/120    avg_loss:0.037, val_acc:0.965]
Epoch [89/120    avg_loss:0.028, val_acc:0.967]
Epoch [90/120    avg_loss:0.023, val_acc:0.969]
Epoch [91/120    avg_loss:0.021, val_acc:0.973]
Epoch [92/120    avg_loss:0.024, val_acc:0.973]
Epoch [93/120    avg_loss:0.023, val_acc:0.971]
Epoch [94/120    avg_loss:0.025, val_acc:0.973]
Epoch [95/120    avg_loss:0.021, val_acc:0.975]
Epoch [96/120    avg_loss:0.024, val_acc:0.975]
Epoch [97/120    avg_loss:0.019, val_acc:0.975]
Epoch [98/120    avg_loss:0.017, val_acc:0.973]
Epoch [99/120    avg_loss:0.025, val_acc:0.979]
Epoch [100/120    avg_loss:0.019, val_acc:0.975]
Epoch [101/120    avg_loss:0.015, val_acc:0.980]
Epoch [102/120    avg_loss:0.018, val_acc:0.979]
Epoch [103/120    avg_loss:0.034, val_acc:0.973]
Epoch [104/120    avg_loss:0.018, val_acc:0.979]
Epoch [105/120    avg_loss:0.023, val_acc:0.977]
Epoch [106/120    avg_loss:0.018, val_acc:0.977]
Epoch [107/120    avg_loss:0.019, val_acc:0.977]
Epoch [108/120    avg_loss:0.016, val_acc:0.975]
Epoch [109/120    avg_loss:0.020, val_acc:0.977]
Epoch [110/120    avg_loss:0.015, val_acc:0.977]
Epoch [111/120    avg_loss:0.017, val_acc:0.977]
Epoch [112/120    avg_loss:0.019, val_acc:0.975]
Epoch [113/120    avg_loss:0.017, val_acc:0.975]
Epoch [114/120    avg_loss:0.019, val_acc:0.975]
Epoch [115/120    avg_loss:0.015, val_acc:0.975]
Epoch [116/120    avg_loss:0.017, val_acc:0.975]
Epoch [117/120    avg_loss:0.015, val_acc:0.975]
Epoch [118/120    avg_loss:0.017, val_acc:0.975]
Epoch [119/120    avg_loss:0.016, val_acc:0.975]
Epoch [120/120    avg_loss:0.013, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 204  19   0   0   0   6   0   0   0   1   0]
 [  0   0   1   2 211  12   1   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   6   0   0   0   0 200   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   1 467   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 362   0   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   6   0   0   0   0   0   0   0 447   0]
 [  0   0   3   0   0   0   0   0   0   0   0   0   0 831]]

Accuracy:
98.33688699360341

F1 scores:
[       nan 0.99563953 0.98642534 0.93577982 0.88100209 0.9020979
 0.98280098 0.99470899 0.98976982 0.99893048 0.99724518 1.
 0.99003322 0.9981982 ]

Kappa:
0.981483406789944
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2f71545898>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.313, val_acc:0.559]
Epoch [2/120    avg_loss:1.777, val_acc:0.686]
Epoch [3/120    avg_loss:1.400, val_acc:0.816]
Epoch [4/120    avg_loss:1.117, val_acc:0.729]
Epoch [5/120    avg_loss:0.912, val_acc:0.859]
Epoch [6/120    avg_loss:0.814, val_acc:0.867]
Epoch [7/120    avg_loss:0.685, val_acc:0.812]
Epoch [8/120    avg_loss:0.691, val_acc:0.854]
Epoch [9/120    avg_loss:0.643, val_acc:0.854]
Epoch [10/120    avg_loss:0.633, val_acc:0.859]
Epoch [11/120    avg_loss:0.537, val_acc:0.893]
Epoch [12/120    avg_loss:0.521, val_acc:0.916]
Epoch [13/120    avg_loss:0.470, val_acc:0.902]
Epoch [14/120    avg_loss:0.502, val_acc:0.898]
Epoch [15/120    avg_loss:0.455, val_acc:0.809]
Epoch [16/120    avg_loss:0.446, val_acc:0.912]
Epoch [17/120    avg_loss:0.408, val_acc:0.906]
Epoch [18/120    avg_loss:0.403, val_acc:0.891]
Epoch [19/120    avg_loss:0.397, val_acc:0.910]
Epoch [20/120    avg_loss:0.410, val_acc:0.918]
Epoch [21/120    avg_loss:0.362, val_acc:0.928]
Epoch [22/120    avg_loss:0.298, val_acc:0.945]
Epoch [23/120    avg_loss:0.379, val_acc:0.912]
Epoch [24/120    avg_loss:0.318, val_acc:0.918]
Epoch [25/120    avg_loss:0.297, val_acc:0.904]
Epoch [26/120    avg_loss:0.295, val_acc:0.924]
Epoch [27/120    avg_loss:0.314, val_acc:0.877]
Epoch [28/120    avg_loss:0.323, val_acc:0.939]
Epoch [29/120    avg_loss:0.322, val_acc:0.932]
Epoch [30/120    avg_loss:0.254, val_acc:0.900]
Epoch [31/120    avg_loss:0.249, val_acc:0.908]
Epoch [32/120    avg_loss:0.299, val_acc:0.943]
Epoch [33/120    avg_loss:0.267, val_acc:0.936]
Epoch [34/120    avg_loss:0.193, val_acc:0.922]
Epoch [35/120    avg_loss:0.216, val_acc:0.928]
Epoch [36/120    avg_loss:0.174, val_acc:0.955]
Epoch [37/120    avg_loss:0.141, val_acc:0.963]
Epoch [38/120    avg_loss:0.110, val_acc:0.963]
Epoch [39/120    avg_loss:0.115, val_acc:0.965]
Epoch [40/120    avg_loss:0.142, val_acc:0.965]
Epoch [41/120    avg_loss:0.113, val_acc:0.963]
Epoch [42/120    avg_loss:0.116, val_acc:0.967]
Epoch [43/120    avg_loss:0.105, val_acc:0.969]
Epoch [44/120    avg_loss:0.118, val_acc:0.971]
Epoch [45/120    avg_loss:0.123, val_acc:0.969]
Epoch [46/120    avg_loss:0.108, val_acc:0.967]
Epoch [47/120    avg_loss:0.105, val_acc:0.967]
Epoch [48/120    avg_loss:0.114, val_acc:0.969]
Epoch [49/120    avg_loss:0.093, val_acc:0.969]
Epoch [50/120    avg_loss:0.104, val_acc:0.973]
Epoch [51/120    avg_loss:0.087, val_acc:0.971]
Epoch [52/120    avg_loss:0.114, val_acc:0.971]
Epoch [53/120    avg_loss:0.097, val_acc:0.975]
Epoch [54/120    avg_loss:0.105, val_acc:0.971]
Epoch [55/120    avg_loss:0.092, val_acc:0.973]
Epoch [56/120    avg_loss:0.096, val_acc:0.975]
Epoch [57/120    avg_loss:0.092, val_acc:0.973]
Epoch [58/120    avg_loss:0.105, val_acc:0.973]
Epoch [59/120    avg_loss:0.083, val_acc:0.967]
Epoch [60/120    avg_loss:0.082, val_acc:0.973]
Epoch [61/120    avg_loss:0.095, val_acc:0.973]
Epoch [62/120    avg_loss:0.078, val_acc:0.969]
Epoch [63/120    avg_loss:0.087, val_acc:0.975]
Epoch [64/120    avg_loss:0.082, val_acc:0.973]
Epoch [65/120    avg_loss:0.081, val_acc:0.977]
Epoch [66/120    avg_loss:0.078, val_acc:0.975]
Epoch [67/120    avg_loss:0.087, val_acc:0.975]
Epoch [68/120    avg_loss:0.094, val_acc:0.973]
Epoch [69/120    avg_loss:0.085, val_acc:0.971]
Epoch [70/120    avg_loss:0.083, val_acc:0.975]
Epoch [71/120    avg_loss:0.090, val_acc:0.973]
Epoch [72/120    avg_loss:0.085, val_acc:0.975]
Epoch [73/120    avg_loss:0.066, val_acc:0.971]
Epoch [74/120    avg_loss:0.077, val_acc:0.977]
Epoch [75/120    avg_loss:0.069, val_acc:0.977]
Epoch [76/120    avg_loss:0.073, val_acc:0.975]
Epoch [77/120    avg_loss:0.063, val_acc:0.979]
Epoch [78/120    avg_loss:0.074, val_acc:0.973]
Epoch [79/120    avg_loss:0.081, val_acc:0.979]
Epoch [80/120    avg_loss:0.083, val_acc:0.979]
Epoch [81/120    avg_loss:0.065, val_acc:0.975]
Epoch [82/120    avg_loss:0.068, val_acc:0.979]
Epoch [83/120    avg_loss:0.072, val_acc:0.973]
Epoch [84/120    avg_loss:0.080, val_acc:0.979]
Epoch [85/120    avg_loss:0.068, val_acc:0.977]
Epoch [86/120    avg_loss:0.067, val_acc:0.973]
Epoch [87/120    avg_loss:0.067, val_acc:0.975]
Epoch [88/120    avg_loss:0.070, val_acc:0.975]
Epoch [89/120    avg_loss:0.065, val_acc:0.980]
Epoch [90/120    avg_loss:0.081, val_acc:0.977]
Epoch [91/120    avg_loss:0.057, val_acc:0.980]
Epoch [92/120    avg_loss:0.066, val_acc:0.975]
Epoch [93/120    avg_loss:0.064, val_acc:0.977]
Epoch [94/120    avg_loss:0.069, val_acc:0.979]
Epoch [95/120    avg_loss:0.057, val_acc:0.982]
Epoch [96/120    avg_loss:0.055, val_acc:0.980]
Epoch [97/120    avg_loss:0.074, val_acc:0.979]
Epoch [98/120    avg_loss:0.057, val_acc:0.979]
Epoch [99/120    avg_loss:0.055, val_acc:0.979]
Epoch [100/120    avg_loss:0.046, val_acc:0.979]
Epoch [101/120    avg_loss:0.048, val_acc:0.975]
Epoch [102/120    avg_loss:0.061, val_acc:0.967]
Epoch [103/120    avg_loss:0.058, val_acc:0.984]
Epoch [104/120    avg_loss:0.073, val_acc:0.977]
Epoch [105/120    avg_loss:0.050, val_acc:0.977]
Epoch [106/120    avg_loss:0.058, val_acc:0.975]
Epoch [107/120    avg_loss:0.065, val_acc:0.980]
Epoch [108/120    avg_loss:0.052, val_acc:0.979]
Epoch [109/120    avg_loss:0.067, val_acc:0.980]
Epoch [110/120    avg_loss:0.071, val_acc:0.980]
Epoch [111/120    avg_loss:0.056, val_acc:0.977]
Epoch [112/120    avg_loss:0.054, val_acc:0.979]
Epoch [113/120    avg_loss:0.061, val_acc:0.979]
Epoch [114/120    avg_loss:0.066, val_acc:0.980]
Epoch [115/120    avg_loss:0.055, val_acc:0.984]
Epoch [116/120    avg_loss:0.056, val_acc:0.984]
Epoch [117/120    avg_loss:0.069, val_acc:0.980]
Epoch [118/120    avg_loss:0.050, val_acc:0.982]
Epoch [119/120    avg_loss:0.052, val_acc:0.982]
Epoch [120/120    avg_loss:0.045, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   9   0   0   0   0   0   0]
 [  0   0   2 200  23   0   0   0   3   2   0   0   0   0]
 [  0   0   1   4 200  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 141   1   0   0   0   0   0   0   0]
 [  0   6   0   0   2   0 198   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 452   1]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.2089552238806

F1 scores:
[       nan 0.99563953 0.96330275 0.92165899 0.87912088 0.91558442
 0.97777778 0.94358974 0.99356499 0.9978678  1.         0.99867198
 0.99779249 0.99940084]

Kappa:
0.9800584175587972
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:01:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f694e264908>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.347, val_acc:0.447]
Epoch [2/120    avg_loss:1.859, val_acc:0.623]
Epoch [3/120    avg_loss:1.424, val_acc:0.709]
Epoch [4/120    avg_loss:1.163, val_acc:0.705]
Epoch [5/120    avg_loss:0.971, val_acc:0.707]
Epoch [6/120    avg_loss:0.881, val_acc:0.820]
Epoch [7/120    avg_loss:0.784, val_acc:0.805]
Epoch [8/120    avg_loss:0.661, val_acc:0.867]
Epoch [9/120    avg_loss:0.600, val_acc:0.828]
Epoch [10/120    avg_loss:0.616, val_acc:0.844]
Epoch [11/120    avg_loss:0.545, val_acc:0.871]
Epoch [12/120    avg_loss:0.544, val_acc:0.871]
Epoch [13/120    avg_loss:0.462, val_acc:0.861]
Epoch [14/120    avg_loss:0.476, val_acc:0.869]
Epoch [15/120    avg_loss:0.424, val_acc:0.926]
Epoch [16/120    avg_loss:0.450, val_acc:0.846]
Epoch [17/120    avg_loss:0.402, val_acc:0.873]
Epoch [18/120    avg_loss:0.367, val_acc:0.926]
Epoch [19/120    avg_loss:0.338, val_acc:0.908]
Epoch [20/120    avg_loss:0.363, val_acc:0.910]
Epoch [21/120    avg_loss:0.341, val_acc:0.916]
Epoch [22/120    avg_loss:0.365, val_acc:0.891]
Epoch [23/120    avg_loss:0.304, val_acc:0.918]
Epoch [24/120    avg_loss:0.308, val_acc:0.893]
Epoch [25/120    avg_loss:0.277, val_acc:0.920]
Epoch [26/120    avg_loss:0.299, val_acc:0.908]
Epoch [27/120    avg_loss:0.315, val_acc:0.875]
Epoch [28/120    avg_loss:0.313, val_acc:0.912]
Epoch [29/120    avg_loss:0.324, val_acc:0.928]
Epoch [30/120    avg_loss:0.275, val_acc:0.904]
Epoch [31/120    avg_loss:0.321, val_acc:0.932]
Epoch [32/120    avg_loss:0.225, val_acc:0.922]
Epoch [33/120    avg_loss:0.315, val_acc:0.924]
Epoch [34/120    avg_loss:0.245, val_acc:0.947]
Epoch [35/120    avg_loss:0.238, val_acc:0.949]
Epoch [36/120    avg_loss:0.202, val_acc:0.949]
Epoch [37/120    avg_loss:0.213, val_acc:0.920]
Epoch [38/120    avg_loss:0.243, val_acc:0.941]
Epoch [39/120    avg_loss:0.193, val_acc:0.936]
Epoch [40/120    avg_loss:0.209, val_acc:0.943]
Epoch [41/120    avg_loss:0.221, val_acc:0.953]
Epoch [42/120    avg_loss:0.181, val_acc:0.967]
Epoch [43/120    avg_loss:0.267, val_acc:0.932]
Epoch [44/120    avg_loss:0.232, val_acc:0.922]
Epoch [45/120    avg_loss:0.164, val_acc:0.959]
Epoch [46/120    avg_loss:0.161, val_acc:0.924]
Epoch [47/120    avg_loss:0.215, val_acc:0.959]
Epoch [48/120    avg_loss:0.151, val_acc:0.953]
Epoch [49/120    avg_loss:0.162, val_acc:0.953]
Epoch [50/120    avg_loss:0.134, val_acc:0.951]
Epoch [51/120    avg_loss:0.122, val_acc:0.959]
Epoch [52/120    avg_loss:0.142, val_acc:0.939]
Epoch [53/120    avg_loss:0.150, val_acc:0.957]
Epoch [54/120    avg_loss:0.119, val_acc:0.955]
Epoch [55/120    avg_loss:0.193, val_acc:0.951]
Epoch [56/120    avg_loss:0.116, val_acc:0.957]
Epoch [57/120    avg_loss:0.105, val_acc:0.963]
Epoch [58/120    avg_loss:0.087, val_acc:0.967]
Epoch [59/120    avg_loss:0.085, val_acc:0.969]
Epoch [60/120    avg_loss:0.093, val_acc:0.969]
Epoch [61/120    avg_loss:0.078, val_acc:0.971]
Epoch [62/120    avg_loss:0.070, val_acc:0.967]
Epoch [63/120    avg_loss:0.077, val_acc:0.967]
Epoch [64/120    avg_loss:0.058, val_acc:0.969]
Epoch [65/120    avg_loss:0.072, val_acc:0.969]
Epoch [66/120    avg_loss:0.065, val_acc:0.969]
Epoch [67/120    avg_loss:0.071, val_acc:0.969]
Epoch [68/120    avg_loss:0.067, val_acc:0.973]
Epoch [69/120    avg_loss:0.058, val_acc:0.973]
Epoch [70/120    avg_loss:0.059, val_acc:0.969]
Epoch [71/120    avg_loss:0.065, val_acc:0.973]
Epoch [72/120    avg_loss:0.060, val_acc:0.973]
Epoch [73/120    avg_loss:0.057, val_acc:0.971]
Epoch [74/120    avg_loss:0.067, val_acc:0.973]
Epoch [75/120    avg_loss:0.064, val_acc:0.975]
Epoch [76/120    avg_loss:0.063, val_acc:0.971]
Epoch [77/120    avg_loss:0.060, val_acc:0.973]
Epoch [78/120    avg_loss:0.065, val_acc:0.973]
Epoch [79/120    avg_loss:0.050, val_acc:0.975]
Epoch [80/120    avg_loss:0.051, val_acc:0.973]
Epoch [81/120    avg_loss:0.056, val_acc:0.975]
Epoch [82/120    avg_loss:0.047, val_acc:0.975]
Epoch [83/120    avg_loss:0.048, val_acc:0.975]
Epoch [84/120    avg_loss:0.056, val_acc:0.973]
Epoch [85/120    avg_loss:0.055, val_acc:0.973]
Epoch [86/120    avg_loss:0.046, val_acc:0.971]
Epoch [87/120    avg_loss:0.057, val_acc:0.975]
Epoch [88/120    avg_loss:0.056, val_acc:0.977]
Epoch [89/120    avg_loss:0.051, val_acc:0.975]
Epoch [90/120    avg_loss:0.050, val_acc:0.975]
Epoch [91/120    avg_loss:0.048, val_acc:0.975]
Epoch [92/120    avg_loss:0.056, val_acc:0.975]
Epoch [93/120    avg_loss:0.052, val_acc:0.971]
Epoch [94/120    avg_loss:0.045, val_acc:0.975]
Epoch [95/120    avg_loss:0.050, val_acc:0.973]
Epoch [96/120    avg_loss:0.050, val_acc:0.971]
Epoch [97/120    avg_loss:0.038, val_acc:0.971]
Epoch [98/120    avg_loss:0.058, val_acc:0.973]
Epoch [99/120    avg_loss:0.053, val_acc:0.977]
Epoch [100/120    avg_loss:0.046, val_acc:0.971]
Epoch [101/120    avg_loss:0.039, val_acc:0.973]
Epoch [102/120    avg_loss:0.039, val_acc:0.975]
Epoch [103/120    avg_loss:0.053, val_acc:0.973]
Epoch [104/120    avg_loss:0.062, val_acc:0.973]
Epoch [105/120    avg_loss:0.037, val_acc:0.975]
Epoch [106/120    avg_loss:0.046, val_acc:0.979]
Epoch [107/120    avg_loss:0.053, val_acc:0.977]
Epoch [108/120    avg_loss:0.043, val_acc:0.979]
Epoch [109/120    avg_loss:0.040, val_acc:0.975]
Epoch [110/120    avg_loss:0.044, val_acc:0.977]
Epoch [111/120    avg_loss:0.043, val_acc:0.977]
Epoch [112/120    avg_loss:0.040, val_acc:0.979]
Epoch [113/120    avg_loss:0.040, val_acc:0.977]
Epoch [114/120    avg_loss:0.052, val_acc:0.975]
Epoch [115/120    avg_loss:0.047, val_acc:0.977]
Epoch [116/120    avg_loss:0.055, val_acc:0.977]
Epoch [117/120    avg_loss:0.044, val_acc:0.973]
Epoch [118/120    avg_loss:0.039, val_acc:0.979]
Epoch [119/120    avg_loss:0.039, val_acc:0.979]
Epoch [120/120    avg_loss:0.034, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 214   0   0   0   0   2   0   0   0   0   3   0]
 [  0   0   0 218   9   0   0   0   2   1   0   0   0   0]
 [  0   0   0   6 206  12   0   0   1   0   0   0   2   0]
 [  0   0   0   0  12 130   3   0   0   0   0   0   0   0]
 [  0   4   0   0   6   0 196   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   1 467   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   1   0   0   0   2 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.29424307036247

F1 scores:
[       nan 0.99708879 0.95964126 0.96035242 0.89565217 0.90592334
 0.96790123 0.91620112 0.99487179 0.99786325 1.         0.9973545
 0.99007718 1.        ]

Kappa:
0.9810051476454889
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2a52ae1898>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.313, val_acc:0.631]
Epoch [2/120    avg_loss:1.776, val_acc:0.666]
Epoch [3/120    avg_loss:1.341, val_acc:0.682]
Epoch [4/120    avg_loss:1.074, val_acc:0.738]
Epoch [5/120    avg_loss:0.903, val_acc:0.777]
Epoch [6/120    avg_loss:0.859, val_acc:0.814]
Epoch [7/120    avg_loss:0.736, val_acc:0.848]
Epoch [8/120    avg_loss:0.670, val_acc:0.824]
Epoch [9/120    avg_loss:0.612, val_acc:0.883]
Epoch [10/120    avg_loss:0.571, val_acc:0.857]
Epoch [11/120    avg_loss:0.619, val_acc:0.877]
Epoch [12/120    avg_loss:0.622, val_acc:0.834]
Epoch [13/120    avg_loss:0.577, val_acc:0.885]
Epoch [14/120    avg_loss:0.481, val_acc:0.893]
Epoch [15/120    avg_loss:0.521, val_acc:0.902]
Epoch [16/120    avg_loss:0.510, val_acc:0.867]
Epoch [17/120    avg_loss:0.475, val_acc:0.889]
Epoch [18/120    avg_loss:0.379, val_acc:0.916]
Epoch [19/120    avg_loss:0.417, val_acc:0.914]
Epoch [20/120    avg_loss:0.374, val_acc:0.871]
Epoch [21/120    avg_loss:0.396, val_acc:0.918]
Epoch [22/120    avg_loss:0.403, val_acc:0.918]
Epoch [23/120    avg_loss:0.310, val_acc:0.918]
Epoch [24/120    avg_loss:0.336, val_acc:0.912]
Epoch [25/120    avg_loss:0.316, val_acc:0.930]
Epoch [26/120    avg_loss:0.250, val_acc:0.932]
Epoch [27/120    avg_loss:0.288, val_acc:0.936]
Epoch [28/120    avg_loss:0.371, val_acc:0.930]
Epoch [29/120    avg_loss:0.283, val_acc:0.934]
Epoch [30/120    avg_loss:0.263, val_acc:0.926]
Epoch [31/120    avg_loss:0.329, val_acc:0.873]
Epoch [32/120    avg_loss:0.250, val_acc:0.939]
Epoch [33/120    avg_loss:0.226, val_acc:0.947]
Epoch [34/120    avg_loss:0.217, val_acc:0.945]
Epoch [35/120    avg_loss:0.229, val_acc:0.943]
Epoch [36/120    avg_loss:0.177, val_acc:0.955]
Epoch [37/120    avg_loss:0.259, val_acc:0.922]
Epoch [38/120    avg_loss:0.224, val_acc:0.938]
Epoch [39/120    avg_loss:0.149, val_acc:0.953]
Epoch [40/120    avg_loss:0.188, val_acc:0.936]
Epoch [41/120    avg_loss:0.249, val_acc:0.943]
Epoch [42/120    avg_loss:0.167, val_acc:0.953]
Epoch [43/120    avg_loss:0.173, val_acc:0.963]
Epoch [44/120    avg_loss:0.210, val_acc:0.900]
Epoch [45/120    avg_loss:0.165, val_acc:0.947]
Epoch [46/120    avg_loss:0.143, val_acc:0.957]
Epoch [47/120    avg_loss:0.156, val_acc:0.961]
Epoch [48/120    avg_loss:0.135, val_acc:0.949]
Epoch [49/120    avg_loss:0.114, val_acc:0.938]
Epoch [50/120    avg_loss:0.132, val_acc:0.930]
Epoch [51/120    avg_loss:0.161, val_acc:0.965]
Epoch [52/120    avg_loss:0.158, val_acc:0.955]
Epoch [53/120    avg_loss:0.084, val_acc:0.980]
Epoch [54/120    avg_loss:0.101, val_acc:0.936]
Epoch [55/120    avg_loss:0.124, val_acc:0.967]
Epoch [56/120    avg_loss:0.080, val_acc:0.973]
Epoch [57/120    avg_loss:0.106, val_acc:0.969]
Epoch [58/120    avg_loss:0.103, val_acc:0.951]
Epoch [59/120    avg_loss:0.065, val_acc:0.971]
Epoch [60/120    avg_loss:0.155, val_acc:0.953]
Epoch [61/120    avg_loss:0.119, val_acc:0.939]
Epoch [62/120    avg_loss:0.110, val_acc:0.953]
Epoch [63/120    avg_loss:0.147, val_acc:0.971]
Epoch [64/120    avg_loss:0.139, val_acc:0.961]
Epoch [65/120    avg_loss:0.135, val_acc:0.953]
Epoch [66/120    avg_loss:0.111, val_acc:0.971]
Epoch [67/120    avg_loss:0.079, val_acc:0.980]
Epoch [68/120    avg_loss:0.061, val_acc:0.979]
Epoch [69/120    avg_loss:0.049, val_acc:0.980]
Epoch [70/120    avg_loss:0.045, val_acc:0.982]
Epoch [71/120    avg_loss:0.062, val_acc:0.980]
Epoch [72/120    avg_loss:0.046, val_acc:0.982]
Epoch [73/120    avg_loss:0.046, val_acc:0.982]
Epoch [74/120    avg_loss:0.044, val_acc:0.980]
Epoch [75/120    avg_loss:0.046, val_acc:0.984]
Epoch [76/120    avg_loss:0.036, val_acc:0.986]
Epoch [77/120    avg_loss:0.043, val_acc:0.986]
Epoch [78/120    avg_loss:0.046, val_acc:0.986]
Epoch [79/120    avg_loss:0.033, val_acc:0.986]
Epoch [80/120    avg_loss:0.040, val_acc:0.986]
Epoch [81/120    avg_loss:0.043, val_acc:0.984]
Epoch [82/120    avg_loss:0.041, val_acc:0.984]
Epoch [83/120    avg_loss:0.040, val_acc:0.986]
Epoch [84/120    avg_loss:0.043, val_acc:0.984]
Epoch [85/120    avg_loss:0.046, val_acc:0.986]
Epoch [86/120    avg_loss:0.035, val_acc:0.988]
Epoch [87/120    avg_loss:0.033, val_acc:0.986]
Epoch [88/120    avg_loss:0.038, val_acc:0.988]
Epoch [89/120    avg_loss:0.036, val_acc:0.988]
Epoch [90/120    avg_loss:0.040, val_acc:0.986]
Epoch [91/120    avg_loss:0.035, val_acc:0.988]
Epoch [92/120    avg_loss:0.031, val_acc:0.988]
Epoch [93/120    avg_loss:0.033, val_acc:0.988]
Epoch [94/120    avg_loss:0.036, val_acc:0.988]
Epoch [95/120    avg_loss:0.036, val_acc:0.988]
Epoch [96/120    avg_loss:0.028, val_acc:0.984]
Epoch [97/120    avg_loss:0.032, val_acc:0.984]
Epoch [98/120    avg_loss:0.030, val_acc:0.986]
Epoch [99/120    avg_loss:0.026, val_acc:0.986]
Epoch [100/120    avg_loss:0.029, val_acc:0.986]
Epoch [101/120    avg_loss:0.027, val_acc:0.986]
Epoch [102/120    avg_loss:0.029, val_acc:0.984]
Epoch [103/120    avg_loss:0.029, val_acc:0.986]
Epoch [104/120    avg_loss:0.030, val_acc:0.986]
Epoch [105/120    avg_loss:0.027, val_acc:0.986]
Epoch [106/120    avg_loss:0.026, val_acc:0.986]
Epoch [107/120    avg_loss:0.035, val_acc:0.986]
Epoch [108/120    avg_loss:0.027, val_acc:0.986]
Epoch [109/120    avg_loss:0.025, val_acc:0.986]
Epoch [110/120    avg_loss:0.027, val_acc:0.986]
Epoch [111/120    avg_loss:0.028, val_acc:0.986]
Epoch [112/120    avg_loss:0.026, val_acc:0.986]
Epoch [113/120    avg_loss:0.025, val_acc:0.986]
Epoch [114/120    avg_loss:0.029, val_acc:0.986]
Epoch [115/120    avg_loss:0.023, val_acc:0.986]
Epoch [116/120    avg_loss:0.024, val_acc:0.986]
Epoch [117/120    avg_loss:0.027, val_acc:0.986]
Epoch [118/120    avg_loss:0.023, val_acc:0.986]
Epoch [119/120    avg_loss:0.021, val_acc:0.986]
Epoch [120/120    avg_loss:0.025, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 212  18   0   0   0   0   0   0   0   0   0]
 [  0   0   0   9 202  16   0   0   0   0   0   0   0   0]
 [  0   0   0   6  11 127   0   0   1   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   1 467   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.33688699360341

F1 scores:
[       nan 0.99708455 0.97333333 0.92778993 0.88209607 0.88194444
 0.9902439  0.94382022 0.996139   0.99893048 1.         1.
 0.99889503 1.        ]

Kappa:
0.9814823335782816
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f01b7361908>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.323, val_acc:0.528]
Epoch [2/120    avg_loss:1.840, val_acc:0.655]
Epoch [3/120    avg_loss:1.406, val_acc:0.764]
Epoch [4/120    avg_loss:1.119, val_acc:0.758]
Epoch [5/120    avg_loss:1.021, val_acc:0.780]
Epoch [6/120    avg_loss:0.871, val_acc:0.764]
Epoch [7/120    avg_loss:0.762, val_acc:0.825]
Epoch [8/120    avg_loss:0.651, val_acc:0.847]
Epoch [9/120    avg_loss:0.625, val_acc:0.780]
Epoch [10/120    avg_loss:0.597, val_acc:0.897]
Epoch [11/120    avg_loss:0.531, val_acc:0.861]
Epoch [12/120    avg_loss:0.487, val_acc:0.893]
Epoch [13/120    avg_loss:0.428, val_acc:0.899]
Epoch [14/120    avg_loss:0.449, val_acc:0.895]
Epoch [15/120    avg_loss:0.405, val_acc:0.863]
Epoch [16/120    avg_loss:0.405, val_acc:0.909]
Epoch [17/120    avg_loss:0.383, val_acc:0.915]
Epoch [18/120    avg_loss:0.344, val_acc:0.885]
Epoch [19/120    avg_loss:0.365, val_acc:0.901]
Epoch [20/120    avg_loss:0.322, val_acc:0.907]
Epoch [21/120    avg_loss:0.347, val_acc:0.911]
Epoch [22/120    avg_loss:0.359, val_acc:0.915]
Epoch [23/120    avg_loss:0.317, val_acc:0.919]
Epoch [24/120    avg_loss:0.328, val_acc:0.917]
Epoch [25/120    avg_loss:0.252, val_acc:0.925]
Epoch [26/120    avg_loss:0.352, val_acc:0.895]
Epoch [27/120    avg_loss:0.359, val_acc:0.907]
Epoch [28/120    avg_loss:0.244, val_acc:0.907]
Epoch [29/120    avg_loss:0.277, val_acc:0.915]
Epoch [30/120    avg_loss:0.188, val_acc:0.915]
Epoch [31/120    avg_loss:0.260, val_acc:0.929]
Epoch [32/120    avg_loss:0.329, val_acc:0.929]
Epoch [33/120    avg_loss:0.197, val_acc:0.925]
Epoch [34/120    avg_loss:0.251, val_acc:0.931]
Epoch [35/120    avg_loss:0.192, val_acc:0.933]
Epoch [36/120    avg_loss:0.158, val_acc:0.954]
Epoch [37/120    avg_loss:0.149, val_acc:0.948]
Epoch [38/120    avg_loss:0.175, val_acc:0.933]
Epoch [39/120    avg_loss:0.214, val_acc:0.940]
Epoch [40/120    avg_loss:0.162, val_acc:0.960]
Epoch [41/120    avg_loss:0.263, val_acc:0.925]
Epoch [42/120    avg_loss:0.167, val_acc:0.958]
Epoch [43/120    avg_loss:0.148, val_acc:0.948]
Epoch [44/120    avg_loss:0.135, val_acc:0.940]
Epoch [45/120    avg_loss:0.157, val_acc:0.956]
Epoch [46/120    avg_loss:0.112, val_acc:0.931]
Epoch [47/120    avg_loss:0.152, val_acc:0.970]
Epoch [48/120    avg_loss:0.191, val_acc:0.897]
Epoch [49/120    avg_loss:0.227, val_acc:0.919]
Epoch [50/120    avg_loss:0.171, val_acc:0.942]
Epoch [51/120    avg_loss:0.111, val_acc:0.964]
Epoch [52/120    avg_loss:0.133, val_acc:0.954]
Epoch [53/120    avg_loss:0.138, val_acc:0.954]
Epoch [54/120    avg_loss:0.154, val_acc:0.954]
Epoch [55/120    avg_loss:0.098, val_acc:0.962]
Epoch [56/120    avg_loss:0.072, val_acc:0.966]
Epoch [57/120    avg_loss:0.102, val_acc:0.950]
Epoch [58/120    avg_loss:0.119, val_acc:0.968]
Epoch [59/120    avg_loss:0.088, val_acc:0.970]
Epoch [60/120    avg_loss:0.144, val_acc:0.968]
Epoch [61/120    avg_loss:0.073, val_acc:0.976]
Epoch [62/120    avg_loss:0.077, val_acc:0.972]
Epoch [63/120    avg_loss:0.050, val_acc:0.974]
Epoch [64/120    avg_loss:0.095, val_acc:0.958]
Epoch [65/120    avg_loss:0.112, val_acc:0.962]
Epoch [66/120    avg_loss:0.095, val_acc:0.964]
Epoch [67/120    avg_loss:0.057, val_acc:0.974]
Epoch [68/120    avg_loss:0.117, val_acc:0.962]
Epoch [69/120    avg_loss:0.063, val_acc:0.976]
Epoch [70/120    avg_loss:0.041, val_acc:0.976]
Epoch [71/120    avg_loss:0.055, val_acc:0.974]
Epoch [72/120    avg_loss:0.093, val_acc:0.978]
Epoch [73/120    avg_loss:0.061, val_acc:0.974]
Epoch [74/120    avg_loss:0.060, val_acc:0.962]
Epoch [75/120    avg_loss:0.241, val_acc:0.871]
Epoch [76/120    avg_loss:0.190, val_acc:0.968]
Epoch [77/120    avg_loss:0.092, val_acc:0.968]
Epoch [78/120    avg_loss:0.080, val_acc:0.972]
Epoch [79/120    avg_loss:0.106, val_acc:0.962]
Epoch [80/120    avg_loss:0.109, val_acc:0.968]
Epoch [81/120    avg_loss:0.064, val_acc:0.974]
Epoch [82/120    avg_loss:0.066, val_acc:0.956]
Epoch [83/120    avg_loss:0.112, val_acc:0.946]
Epoch [84/120    avg_loss:0.067, val_acc:0.970]
Epoch [85/120    avg_loss:0.043, val_acc:0.974]
Epoch [86/120    avg_loss:0.041, val_acc:0.982]
Epoch [87/120    avg_loss:0.028, val_acc:0.980]
Epoch [88/120    avg_loss:0.027, val_acc:0.980]
Epoch [89/120    avg_loss:0.027, val_acc:0.980]
Epoch [90/120    avg_loss:0.028, val_acc:0.982]
Epoch [91/120    avg_loss:0.026, val_acc:0.982]
Epoch [92/120    avg_loss:0.019, val_acc:0.980]
Epoch [93/120    avg_loss:0.026, val_acc:0.980]
Epoch [94/120    avg_loss:0.026, val_acc:0.980]
Epoch [95/120    avg_loss:0.021, val_acc:0.978]
Epoch [96/120    avg_loss:0.022, val_acc:0.982]
Epoch [97/120    avg_loss:0.018, val_acc:0.980]
Epoch [98/120    avg_loss:0.021, val_acc:0.980]
Epoch [99/120    avg_loss:0.022, val_acc:0.980]
Epoch [100/120    avg_loss:0.022, val_acc:0.980]
Epoch [101/120    avg_loss:0.030, val_acc:0.980]
Epoch [102/120    avg_loss:0.023, val_acc:0.980]
Epoch [103/120    avg_loss:0.022, val_acc:0.980]
Epoch [104/120    avg_loss:0.026, val_acc:0.980]
Epoch [105/120    avg_loss:0.021, val_acc:0.980]
Epoch [106/120    avg_loss:0.024, val_acc:0.982]
Epoch [107/120    avg_loss:0.021, val_acc:0.982]
Epoch [108/120    avg_loss:0.015, val_acc:0.980]
Epoch [109/120    avg_loss:0.017, val_acc:0.984]
Epoch [110/120    avg_loss:0.014, val_acc:0.984]
Epoch [111/120    avg_loss:0.023, val_acc:0.984]
Epoch [112/120    avg_loss:0.016, val_acc:0.980]
Epoch [113/120    avg_loss:0.020, val_acc:0.982]
Epoch [114/120    avg_loss:0.015, val_acc:0.980]
Epoch [115/120    avg_loss:0.015, val_acc:0.980]
Epoch [116/120    avg_loss:0.019, val_acc:0.978]
Epoch [117/120    avg_loss:0.018, val_acc:0.982]
Epoch [118/120    avg_loss:0.014, val_acc:0.982]
Epoch [119/120    avg_loss:0.021, val_acc:0.982]
Epoch [120/120    avg_loss:0.026, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   7   0   0   0   0   2   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 201  20   0   0   0   0   0   0   6   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6   0 200   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.63539445628997

F1 scores:
[       nan 1.         0.97447796 0.99782135 0.88351648 0.85813149
 0.98522167 0.95876289 1.         1.         1.         1.
 0.99124726 1.        ]

Kappa:
0.9848066094893192
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd7a0ca5860>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.337, val_acc:0.565]
Epoch [2/120    avg_loss:1.845, val_acc:0.669]
Epoch [3/120    avg_loss:1.459, val_acc:0.706]
Epoch [4/120    avg_loss:1.170, val_acc:0.786]
Epoch [5/120    avg_loss:1.042, val_acc:0.744]
Epoch [6/120    avg_loss:0.855, val_acc:0.796]
Epoch [7/120    avg_loss:0.820, val_acc:0.833]
Epoch [8/120    avg_loss:0.727, val_acc:0.812]
Epoch [9/120    avg_loss:0.650, val_acc:0.867]
Epoch [10/120    avg_loss:0.592, val_acc:0.861]
Epoch [11/120    avg_loss:0.510, val_acc:0.875]
Epoch [12/120    avg_loss:0.528, val_acc:0.889]
Epoch [13/120    avg_loss:0.513, val_acc:0.887]
Epoch [14/120    avg_loss:0.476, val_acc:0.907]
Epoch [15/120    avg_loss:0.404, val_acc:0.843]
Epoch [16/120    avg_loss:0.415, val_acc:0.917]
Epoch [17/120    avg_loss:0.405, val_acc:0.923]
Epoch [18/120    avg_loss:0.381, val_acc:0.895]
Epoch [19/120    avg_loss:0.469, val_acc:0.863]
Epoch [20/120    avg_loss:0.359, val_acc:0.903]
Epoch [21/120    avg_loss:0.349, val_acc:0.905]
Epoch [22/120    avg_loss:0.339, val_acc:0.948]
Epoch [23/120    avg_loss:0.271, val_acc:0.919]
Epoch [24/120    avg_loss:0.269, val_acc:0.942]
Epoch [25/120    avg_loss:0.332, val_acc:0.919]
Epoch [26/120    avg_loss:0.276, val_acc:0.948]
Epoch [27/120    avg_loss:0.341, val_acc:0.931]
Epoch [28/120    avg_loss:0.250, val_acc:0.964]
Epoch [29/120    avg_loss:0.231, val_acc:0.946]
Epoch [30/120    avg_loss:0.248, val_acc:0.938]
Epoch [31/120    avg_loss:0.228, val_acc:0.909]
Epoch [32/120    avg_loss:0.383, val_acc:0.954]
Epoch [33/120    avg_loss:0.225, val_acc:0.952]
Epoch [34/120    avg_loss:0.246, val_acc:0.919]
Epoch [35/120    avg_loss:0.247, val_acc:0.952]
Epoch [36/120    avg_loss:0.166, val_acc:0.970]
Epoch [37/120    avg_loss:0.204, val_acc:0.972]
Epoch [38/120    avg_loss:0.178, val_acc:0.937]
Epoch [39/120    avg_loss:0.203, val_acc:0.927]
Epoch [40/120    avg_loss:0.178, val_acc:0.940]
Epoch [41/120    avg_loss:0.204, val_acc:0.974]
Epoch [42/120    avg_loss:0.168, val_acc:0.968]
Epoch [43/120    avg_loss:0.138, val_acc:0.962]
Epoch [44/120    avg_loss:0.253, val_acc:0.944]
Epoch [45/120    avg_loss:0.199, val_acc:0.954]
Epoch [46/120    avg_loss:0.194, val_acc:0.956]
Epoch [47/120    avg_loss:0.141, val_acc:0.968]
Epoch [48/120    avg_loss:0.143, val_acc:0.982]
Epoch [49/120    avg_loss:0.115, val_acc:0.966]
Epoch [50/120    avg_loss:0.132, val_acc:0.978]
Epoch [51/120    avg_loss:0.127, val_acc:0.976]
Epoch [52/120    avg_loss:0.159, val_acc:0.962]
Epoch [53/120    avg_loss:0.154, val_acc:0.974]
Epoch [54/120    avg_loss:0.096, val_acc:0.976]
Epoch [55/120    avg_loss:0.121, val_acc:0.972]
Epoch [56/120    avg_loss:0.103, val_acc:0.966]
Epoch [57/120    avg_loss:0.130, val_acc:0.976]
Epoch [58/120    avg_loss:0.126, val_acc:0.986]
Epoch [59/120    avg_loss:0.141, val_acc:0.982]
Epoch [60/120    avg_loss:0.165, val_acc:0.958]
Epoch [61/120    avg_loss:0.103, val_acc:0.978]
Epoch [62/120    avg_loss:0.065, val_acc:0.980]
Epoch [63/120    avg_loss:0.075, val_acc:0.982]
Epoch [64/120    avg_loss:0.047, val_acc:0.937]
Epoch [65/120    avg_loss:0.093, val_acc:0.972]
Epoch [66/120    avg_loss:0.067, val_acc:0.984]
Epoch [67/120    avg_loss:0.054, val_acc:0.974]
Epoch [68/120    avg_loss:0.109, val_acc:0.974]
Epoch [69/120    avg_loss:0.055, val_acc:0.976]
Epoch [70/120    avg_loss:0.058, val_acc:0.972]
Epoch [71/120    avg_loss:0.061, val_acc:0.970]
Epoch [72/120    avg_loss:0.063, val_acc:0.978]
Epoch [73/120    avg_loss:0.040, val_acc:0.978]
Epoch [74/120    avg_loss:0.036, val_acc:0.980]
Epoch [75/120    avg_loss:0.035, val_acc:0.980]
Epoch [76/120    avg_loss:0.022, val_acc:0.980]
Epoch [77/120    avg_loss:0.036, val_acc:0.982]
Epoch [78/120    avg_loss:0.047, val_acc:0.984]
Epoch [79/120    avg_loss:0.035, val_acc:0.986]
Epoch [80/120    avg_loss:0.024, val_acc:0.982]
Epoch [81/120    avg_loss:0.026, val_acc:0.982]
Epoch [82/120    avg_loss:0.026, val_acc:0.984]
Epoch [83/120    avg_loss:0.029, val_acc:0.984]
Epoch [84/120    avg_loss:0.025, val_acc:0.984]
Epoch [85/120    avg_loss:0.028, val_acc:0.982]
Epoch [86/120    avg_loss:0.025, val_acc:0.980]
Epoch [87/120    avg_loss:0.024, val_acc:0.984]
Epoch [88/120    avg_loss:0.022, val_acc:0.984]
Epoch [89/120    avg_loss:0.034, val_acc:0.980]
Epoch [90/120    avg_loss:0.022, val_acc:0.980]
Epoch [91/120    avg_loss:0.021, val_acc:0.984]
Epoch [92/120    avg_loss:0.018, val_acc:0.982]
Epoch [93/120    avg_loss:0.031, val_acc:0.984]
Epoch [94/120    avg_loss:0.025, val_acc:0.984]
Epoch [95/120    avg_loss:0.031, val_acc:0.982]
Epoch [96/120    avg_loss:0.020, val_acc:0.982]
Epoch [97/120    avg_loss:0.021, val_acc:0.982]
Epoch [98/120    avg_loss:0.023, val_acc:0.982]
Epoch [99/120    avg_loss:0.022, val_acc:0.982]
Epoch [100/120    avg_loss:0.022, val_acc:0.982]
Epoch [101/120    avg_loss:0.025, val_acc:0.982]
Epoch [102/120    avg_loss:0.016, val_acc:0.982]
Epoch [103/120    avg_loss:0.022, val_acc:0.982]
Epoch [104/120    avg_loss:0.022, val_acc:0.982]
Epoch [105/120    avg_loss:0.021, val_acc:0.982]
Epoch [106/120    avg_loss:0.018, val_acc:0.982]
Epoch [107/120    avg_loss:0.024, val_acc:0.982]
Epoch [108/120    avg_loss:0.019, val_acc:0.982]
Epoch [109/120    avg_loss:0.019, val_acc:0.982]
Epoch [110/120    avg_loss:0.024, val_acc:0.982]
Epoch [111/120    avg_loss:0.021, val_acc:0.982]
Epoch [112/120    avg_loss:0.021, val_acc:0.982]
Epoch [113/120    avg_loss:0.020, val_acc:0.982]
Epoch [114/120    avg_loss:0.029, val_acc:0.982]
Epoch [115/120    avg_loss:0.027, val_acc:0.982]
Epoch [116/120    avg_loss:0.021, val_acc:0.982]
Epoch [117/120    avg_loss:0.027, val_acc:0.982]
Epoch [118/120    avg_loss:0.023, val_acc:0.982]
Epoch [119/120    avg_loss:0.022, val_acc:0.982]
Epoch [120/120    avg_loss:0.022, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 1.         0.98648649 0.98678414 0.90869565 0.87586207
 1.         0.9726776  1.         1.         1.         1.
 0.99889503 1.        ]

Kappa:
0.9886054750510165
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f87bae8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.289, val_acc:0.482]
Epoch [2/120    avg_loss:1.787, val_acc:0.655]
Epoch [3/120    avg_loss:1.415, val_acc:0.702]
Epoch [4/120    avg_loss:1.146, val_acc:0.770]
Epoch [5/120    avg_loss:1.022, val_acc:0.800]
Epoch [6/120    avg_loss:0.889, val_acc:0.728]
Epoch [7/120    avg_loss:0.802, val_acc:0.837]
Epoch [8/120    avg_loss:0.747, val_acc:0.829]
Epoch [9/120    avg_loss:0.671, val_acc:0.877]
Epoch [10/120    avg_loss:0.581, val_acc:0.849]
Epoch [11/120    avg_loss:0.556, val_acc:0.883]
Epoch [12/120    avg_loss:0.541, val_acc:0.869]
Epoch [13/120    avg_loss:0.486, val_acc:0.857]
Epoch [14/120    avg_loss:0.500, val_acc:0.905]
Epoch [15/120    avg_loss:0.457, val_acc:0.895]
Epoch [16/120    avg_loss:0.405, val_acc:0.905]
Epoch [17/120    avg_loss:0.390, val_acc:0.885]
Epoch [18/120    avg_loss:0.430, val_acc:0.889]
Epoch [19/120    avg_loss:0.427, val_acc:0.913]
Epoch [20/120    avg_loss:0.333, val_acc:0.901]
Epoch [21/120    avg_loss:0.363, val_acc:0.940]
Epoch [22/120    avg_loss:0.341, val_acc:0.925]
Epoch [23/120    avg_loss:0.294, val_acc:0.925]
Epoch [24/120    avg_loss:0.292, val_acc:0.909]
Epoch [25/120    avg_loss:0.335, val_acc:0.937]
Epoch [26/120    avg_loss:0.302, val_acc:0.946]
Epoch [27/120    avg_loss:0.253, val_acc:0.938]
Epoch [28/120    avg_loss:0.242, val_acc:0.931]
Epoch [29/120    avg_loss:0.245, val_acc:0.960]
Epoch [30/120    avg_loss:0.276, val_acc:0.921]
Epoch [31/120    avg_loss:0.240, val_acc:0.927]
Epoch [32/120    avg_loss:0.260, val_acc:0.927]
Epoch [33/120    avg_loss:0.219, val_acc:0.931]
Epoch [34/120    avg_loss:0.244, val_acc:0.954]
Epoch [35/120    avg_loss:0.258, val_acc:0.933]
Epoch [36/120    avg_loss:0.224, val_acc:0.937]
Epoch [37/120    avg_loss:0.174, val_acc:0.964]
Epoch [38/120    avg_loss:0.169, val_acc:0.938]
Epoch [39/120    avg_loss:0.247, val_acc:0.950]
Epoch [40/120    avg_loss:0.203, val_acc:0.946]
Epoch [41/120    avg_loss:0.211, val_acc:0.931]
Epoch [42/120    avg_loss:0.217, val_acc:0.942]
Epoch [43/120    avg_loss:0.242, val_acc:0.948]
Epoch [44/120    avg_loss:0.182, val_acc:0.944]
Epoch [45/120    avg_loss:0.186, val_acc:0.954]
Epoch [46/120    avg_loss:0.133, val_acc:0.954]
Epoch [47/120    avg_loss:0.155, val_acc:0.966]
Epoch [48/120    avg_loss:0.136, val_acc:0.960]
Epoch [49/120    avg_loss:0.103, val_acc:0.968]
Epoch [50/120    avg_loss:0.126, val_acc:0.970]
Epoch [51/120    avg_loss:0.132, val_acc:0.968]
Epoch [52/120    avg_loss:0.136, val_acc:0.976]
Epoch [53/120    avg_loss:0.109, val_acc:0.960]
Epoch [54/120    avg_loss:0.080, val_acc:0.970]
Epoch [55/120    avg_loss:0.118, val_acc:0.960]
Epoch [56/120    avg_loss:0.101, val_acc:0.970]
Epoch [57/120    avg_loss:0.113, val_acc:0.958]
Epoch [58/120    avg_loss:0.096, val_acc:0.976]
Epoch [59/120    avg_loss:0.115, val_acc:0.958]
Epoch [60/120    avg_loss:0.120, val_acc:0.974]
Epoch [61/120    avg_loss:0.129, val_acc:0.938]
Epoch [62/120    avg_loss:0.168, val_acc:0.964]
Epoch [63/120    avg_loss:0.105, val_acc:0.978]
Epoch [64/120    avg_loss:0.074, val_acc:0.972]
Epoch [65/120    avg_loss:0.076, val_acc:0.966]
Epoch [66/120    avg_loss:0.062, val_acc:0.978]
Epoch [67/120    avg_loss:0.059, val_acc:0.968]
Epoch [68/120    avg_loss:0.118, val_acc:0.974]
Epoch [69/120    avg_loss:0.073, val_acc:0.970]
Epoch [70/120    avg_loss:0.097, val_acc:0.958]
Epoch [71/120    avg_loss:0.075, val_acc:0.972]
Epoch [72/120    avg_loss:0.062, val_acc:0.970]
Epoch [73/120    avg_loss:0.081, val_acc:0.968]
Epoch [74/120    avg_loss:0.080, val_acc:0.976]
Epoch [75/120    avg_loss:0.070, val_acc:0.980]
Epoch [76/120    avg_loss:0.075, val_acc:0.980]
Epoch [77/120    avg_loss:0.049, val_acc:0.960]
Epoch [78/120    avg_loss:0.052, val_acc:0.980]
Epoch [79/120    avg_loss:0.049, val_acc:0.982]
Epoch [80/120    avg_loss:0.038, val_acc:0.982]
Epoch [81/120    avg_loss:0.060, val_acc:0.984]
Epoch [82/120    avg_loss:0.056, val_acc:0.968]
Epoch [83/120    avg_loss:0.178, val_acc:0.966]
Epoch [84/120    avg_loss:0.089, val_acc:0.964]
Epoch [85/120    avg_loss:0.052, val_acc:0.988]
Epoch [86/120    avg_loss:0.105, val_acc:0.966]
Epoch [87/120    avg_loss:0.056, val_acc:0.990]
Epoch [88/120    avg_loss:0.041, val_acc:0.937]
Epoch [89/120    avg_loss:0.129, val_acc:0.982]
Epoch [90/120    avg_loss:0.052, val_acc:0.986]
Epoch [91/120    avg_loss:0.076, val_acc:0.984]
Epoch [92/120    avg_loss:0.038, val_acc:0.984]
Epoch [93/120    avg_loss:0.037, val_acc:0.980]
Epoch [94/120    avg_loss:0.047, val_acc:0.978]
Epoch [95/120    avg_loss:0.051, val_acc:0.990]
Epoch [96/120    avg_loss:0.035, val_acc:0.986]
Epoch [97/120    avg_loss:0.041, val_acc:0.988]
Epoch [98/120    avg_loss:0.036, val_acc:0.986]
Epoch [99/120    avg_loss:0.025, val_acc:0.994]
Epoch [100/120    avg_loss:0.022, val_acc:0.988]
Epoch [101/120    avg_loss:0.036, val_acc:0.986]
Epoch [102/120    avg_loss:0.057, val_acc:0.978]
Epoch [103/120    avg_loss:0.047, val_acc:0.982]
Epoch [104/120    avg_loss:0.026, val_acc:0.988]
Epoch [105/120    avg_loss:0.020, val_acc:0.988]
Epoch [106/120    avg_loss:0.021, val_acc:0.988]
Epoch [107/120    avg_loss:0.028, val_acc:0.990]
Epoch [108/120    avg_loss:0.015, val_acc:0.990]
Epoch [109/120    avg_loss:0.014, val_acc:0.990]
Epoch [110/120    avg_loss:0.063, val_acc:0.976]
Epoch [111/120    avg_loss:0.110, val_acc:0.978]
Epoch [112/120    avg_loss:0.039, val_acc:0.988]
Epoch [113/120    avg_loss:0.028, val_acc:0.986]
Epoch [114/120    avg_loss:0.020, val_acc:0.990]
Epoch [115/120    avg_loss:0.018, val_acc:0.990]
Epoch [116/120    avg_loss:0.027, val_acc:0.988]
Epoch [117/120    avg_loss:0.020, val_acc:0.990]
Epoch [118/120    avg_loss:0.014, val_acc:0.992]
Epoch [119/120    avg_loss:0.013, val_acc:0.994]
Epoch [120/120    avg_loss:0.014, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   3 212  10   0   0   0   0   0   0   2   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 1.         0.99095023 0.99134199 0.94854586 0.94557823
 0.99756691 0.97826087 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9935904895275645
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6895d79908>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.305, val_acc:0.595]
Epoch [2/120    avg_loss:1.831, val_acc:0.649]
Epoch [3/120    avg_loss:1.394, val_acc:0.770]
Epoch [4/120    avg_loss:1.134, val_acc:0.748]
Epoch [5/120    avg_loss:0.944, val_acc:0.798]
Epoch [6/120    avg_loss:0.797, val_acc:0.798]
Epoch [7/120    avg_loss:0.716, val_acc:0.829]
Epoch [8/120    avg_loss:0.674, val_acc:0.891]
Epoch [9/120    avg_loss:0.612, val_acc:0.891]
Epoch [10/120    avg_loss:0.529, val_acc:0.899]
Epoch [11/120    avg_loss:0.487, val_acc:0.899]
Epoch [12/120    avg_loss:0.532, val_acc:0.897]
Epoch [13/120    avg_loss:0.404, val_acc:0.905]
Epoch [14/120    avg_loss:0.462, val_acc:0.879]
Epoch [15/120    avg_loss:0.419, val_acc:0.917]
Epoch [16/120    avg_loss:0.378, val_acc:0.897]
Epoch [17/120    avg_loss:0.462, val_acc:0.883]
Epoch [18/120    avg_loss:0.412, val_acc:0.909]
Epoch [19/120    avg_loss:0.361, val_acc:0.935]
Epoch [20/120    avg_loss:0.407, val_acc:0.937]
Epoch [21/120    avg_loss:0.305, val_acc:0.938]
Epoch [22/120    avg_loss:0.317, val_acc:0.897]
Epoch [23/120    avg_loss:0.312, val_acc:0.925]
Epoch [24/120    avg_loss:0.354, val_acc:0.944]
Epoch [25/120    avg_loss:0.377, val_acc:0.905]
Epoch [26/120    avg_loss:0.347, val_acc:0.931]
Epoch [27/120    avg_loss:0.267, val_acc:0.946]
Epoch [28/120    avg_loss:0.272, val_acc:0.950]
Epoch [29/120    avg_loss:0.263, val_acc:0.946]
Epoch [30/120    avg_loss:0.260, val_acc:0.966]
Epoch [31/120    avg_loss:0.259, val_acc:0.954]
Epoch [32/120    avg_loss:0.254, val_acc:0.937]
Epoch [33/120    avg_loss:0.253, val_acc:0.954]
Epoch [34/120    avg_loss:0.223, val_acc:0.966]
Epoch [35/120    avg_loss:0.195, val_acc:0.966]
Epoch [36/120    avg_loss:0.245, val_acc:0.958]
Epoch [37/120    avg_loss:0.180, val_acc:0.950]
Epoch [38/120    avg_loss:0.174, val_acc:0.962]
Epoch [39/120    avg_loss:0.198, val_acc:0.978]
Epoch [40/120    avg_loss:0.176, val_acc:0.952]
Epoch [41/120    avg_loss:0.177, val_acc:0.968]
Epoch [42/120    avg_loss:0.176, val_acc:0.966]
Epoch [43/120    avg_loss:0.191, val_acc:0.962]
Epoch [44/120    avg_loss:0.162, val_acc:0.976]
Epoch [45/120    avg_loss:0.142, val_acc:0.972]
Epoch [46/120    avg_loss:0.182, val_acc:0.978]
Epoch [47/120    avg_loss:0.201, val_acc:0.950]
Epoch [48/120    avg_loss:0.146, val_acc:0.946]
Epoch [49/120    avg_loss:0.139, val_acc:0.972]
Epoch [50/120    avg_loss:0.147, val_acc:0.978]
Epoch [51/120    avg_loss:0.158, val_acc:0.964]
Epoch [52/120    avg_loss:0.116, val_acc:0.976]
Epoch [53/120    avg_loss:0.167, val_acc:0.966]
Epoch [54/120    avg_loss:0.112, val_acc:0.976]
Epoch [55/120    avg_loss:0.087, val_acc:0.976]
Epoch [56/120    avg_loss:0.181, val_acc:0.952]
Epoch [57/120    avg_loss:0.096, val_acc:0.968]
Epoch [58/120    avg_loss:0.108, val_acc:0.976]
Epoch [59/120    avg_loss:0.107, val_acc:0.968]
Epoch [60/120    avg_loss:0.152, val_acc:0.978]
Epoch [61/120    avg_loss:0.095, val_acc:0.982]
Epoch [62/120    avg_loss:0.070, val_acc:0.982]
Epoch [63/120    avg_loss:0.057, val_acc:0.982]
Epoch [64/120    avg_loss:0.055, val_acc:0.970]
Epoch [65/120    avg_loss:0.147, val_acc:0.962]
Epoch [66/120    avg_loss:0.095, val_acc:0.988]
Epoch [67/120    avg_loss:0.074, val_acc:0.972]
Epoch [68/120    avg_loss:0.091, val_acc:0.980]
Epoch [69/120    avg_loss:0.099, val_acc:0.964]
Epoch [70/120    avg_loss:0.082, val_acc:0.986]
Epoch [71/120    avg_loss:0.058, val_acc:0.994]
Epoch [72/120    avg_loss:0.061, val_acc:0.982]
Epoch [73/120    avg_loss:0.064, val_acc:0.992]
Epoch [74/120    avg_loss:0.078, val_acc:0.992]
Epoch [75/120    avg_loss:0.077, val_acc:0.986]
Epoch [76/120    avg_loss:0.063, val_acc:0.988]
Epoch [77/120    avg_loss:0.079, val_acc:0.984]
Epoch [78/120    avg_loss:0.047, val_acc:0.988]
Epoch [79/120    avg_loss:0.047, val_acc:0.986]
Epoch [80/120    avg_loss:0.067, val_acc:0.992]
Epoch [81/120    avg_loss:0.044, val_acc:0.980]
Epoch [82/120    avg_loss:0.067, val_acc:0.970]
Epoch [83/120    avg_loss:0.050, val_acc:0.992]
Epoch [84/120    avg_loss:0.048, val_acc:0.958]
Epoch [85/120    avg_loss:0.080, val_acc:0.972]
Epoch [86/120    avg_loss:0.042, val_acc:0.986]
Epoch [87/120    avg_loss:0.041, val_acc:0.986]
Epoch [88/120    avg_loss:0.031, val_acc:0.986]
Epoch [89/120    avg_loss:0.027, val_acc:0.988]
Epoch [90/120    avg_loss:0.024, val_acc:0.988]
Epoch [91/120    avg_loss:0.032, val_acc:0.988]
Epoch [92/120    avg_loss:0.031, val_acc:0.988]
Epoch [93/120    avg_loss:0.028, val_acc:0.990]
Epoch [94/120    avg_loss:0.026, val_acc:0.988]
Epoch [95/120    avg_loss:0.031, val_acc:0.988]
Epoch [96/120    avg_loss:0.022, val_acc:0.988]
Epoch [97/120    avg_loss:0.030, val_acc:0.988]
Epoch [98/120    avg_loss:0.025, val_acc:0.988]
Epoch [99/120    avg_loss:0.026, val_acc:0.988]
Epoch [100/120    avg_loss:0.017, val_acc:0.988]
Epoch [101/120    avg_loss:0.018, val_acc:0.988]
Epoch [102/120    avg_loss:0.022, val_acc:0.988]
Epoch [103/120    avg_loss:0.016, val_acc:0.988]
Epoch [104/120    avg_loss:0.020, val_acc:0.988]
Epoch [105/120    avg_loss:0.025, val_acc:0.988]
Epoch [106/120    avg_loss:0.022, val_acc:0.988]
Epoch [107/120    avg_loss:0.021, val_acc:0.988]
Epoch [108/120    avg_loss:0.031, val_acc:0.988]
Epoch [109/120    avg_loss:0.018, val_acc:0.988]
Epoch [110/120    avg_loss:0.021, val_acc:0.988]
Epoch [111/120    avg_loss:0.019, val_acc:0.988]
Epoch [112/120    avg_loss:0.016, val_acc:0.988]
Epoch [113/120    avg_loss:0.020, val_acc:0.988]
Epoch [114/120    avg_loss:0.018, val_acc:0.988]
Epoch [115/120    avg_loss:0.019, val_acc:0.988]
Epoch [116/120    avg_loss:0.019, val_acc:0.988]
Epoch [117/120    avg_loss:0.021, val_acc:0.988]
Epoch [118/120    avg_loss:0.020, val_acc:0.988]
Epoch [119/120    avg_loss:0.017, val_acc:0.988]
Epoch [120/120    avg_loss:0.017, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   2 227   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   3 194  25   0   0   0   0   0   0   5   0]
 [  0   0   0   7   5 132   1   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.72068230277185

F1 scores:
[       nan 0.99927061 0.97482838 0.97216274 0.91079812 0.87417219
 0.99514563 0.95287958 1.         0.99893276 1.         0.99867198
 0.99342105 1.        ]

Kappa:
0.9857567154884685
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3bd075d908>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.257, val_acc:0.599]
Epoch [2/120    avg_loss:1.709, val_acc:0.698]
Epoch [3/120    avg_loss:1.305, val_acc:0.774]
Epoch [4/120    avg_loss:1.056, val_acc:0.810]
Epoch [5/120    avg_loss:0.907, val_acc:0.792]
Epoch [6/120    avg_loss:0.820, val_acc:0.825]
Epoch [7/120    avg_loss:0.750, val_acc:0.847]
Epoch [8/120    avg_loss:0.673, val_acc:0.792]
Epoch [9/120    avg_loss:0.625, val_acc:0.784]
Epoch [10/120    avg_loss:0.621, val_acc:0.845]
Epoch [11/120    avg_loss:0.584, val_acc:0.815]
Epoch [12/120    avg_loss:0.549, val_acc:0.875]
Epoch [13/120    avg_loss:0.497, val_acc:0.877]
Epoch [14/120    avg_loss:0.563, val_acc:0.776]
Epoch [15/120    avg_loss:0.517, val_acc:0.873]
Epoch [16/120    avg_loss:0.481, val_acc:0.863]
Epoch [17/120    avg_loss:0.492, val_acc:0.893]
Epoch [18/120    avg_loss:0.412, val_acc:0.895]
Epoch [19/120    avg_loss:0.417, val_acc:0.875]
Epoch [20/120    avg_loss:0.430, val_acc:0.897]
Epoch [21/120    avg_loss:0.408, val_acc:0.893]
Epoch [22/120    avg_loss:0.375, val_acc:0.891]
Epoch [23/120    avg_loss:0.374, val_acc:0.885]
Epoch [24/120    avg_loss:0.365, val_acc:0.893]
Epoch [25/120    avg_loss:0.381, val_acc:0.907]
Epoch [26/120    avg_loss:0.361, val_acc:0.891]
Epoch [27/120    avg_loss:0.330, val_acc:0.893]
Epoch [28/120    avg_loss:0.294, val_acc:0.925]
Epoch [29/120    avg_loss:0.303, val_acc:0.907]
Epoch [30/120    avg_loss:0.312, val_acc:0.907]
Epoch [31/120    avg_loss:0.272, val_acc:0.909]
Epoch [32/120    avg_loss:0.315, val_acc:0.909]
Epoch [33/120    avg_loss:0.261, val_acc:0.907]
Epoch [34/120    avg_loss:0.245, val_acc:0.923]
Epoch [35/120    avg_loss:0.233, val_acc:0.891]
Epoch [36/120    avg_loss:0.292, val_acc:0.917]
Epoch [37/120    avg_loss:0.258, val_acc:0.913]
Epoch [38/120    avg_loss:0.218, val_acc:0.921]
Epoch [39/120    avg_loss:0.223, val_acc:0.935]
Epoch [40/120    avg_loss:0.222, val_acc:0.929]
Epoch [41/120    avg_loss:0.236, val_acc:0.923]
Epoch [42/120    avg_loss:0.194, val_acc:0.891]
Epoch [43/120    avg_loss:0.185, val_acc:0.931]
Epoch [44/120    avg_loss:0.174, val_acc:0.946]
Epoch [45/120    avg_loss:0.124, val_acc:0.950]
Epoch [46/120    avg_loss:0.154, val_acc:0.929]
Epoch [47/120    avg_loss:0.157, val_acc:0.913]
Epoch [48/120    avg_loss:0.178, val_acc:0.907]
Epoch [49/120    avg_loss:0.190, val_acc:0.927]
Epoch [50/120    avg_loss:0.179, val_acc:0.911]
Epoch [51/120    avg_loss:0.115, val_acc:0.946]
Epoch [52/120    avg_loss:0.139, val_acc:0.950]
Epoch [53/120    avg_loss:0.148, val_acc:0.940]
Epoch [54/120    avg_loss:0.093, val_acc:0.903]
Epoch [55/120    avg_loss:0.145, val_acc:0.950]
Epoch [56/120    avg_loss:0.129, val_acc:0.956]
Epoch [57/120    avg_loss:0.092, val_acc:0.956]
Epoch [58/120    avg_loss:0.093, val_acc:0.907]
Epoch [59/120    avg_loss:0.126, val_acc:0.935]
Epoch [60/120    avg_loss:0.097, val_acc:0.950]
Epoch [61/120    avg_loss:0.071, val_acc:0.964]
Epoch [62/120    avg_loss:0.106, val_acc:0.942]
Epoch [63/120    avg_loss:0.129, val_acc:0.960]
Epoch [64/120    avg_loss:0.073, val_acc:0.954]
Epoch [65/120    avg_loss:0.059, val_acc:0.956]
Epoch [66/120    avg_loss:0.083, val_acc:0.956]
Epoch [67/120    avg_loss:0.191, val_acc:0.919]
Epoch [68/120    avg_loss:0.184, val_acc:0.944]
Epoch [69/120    avg_loss:0.166, val_acc:0.946]
Epoch [70/120    avg_loss:0.130, val_acc:0.950]
Epoch [71/120    avg_loss:0.082, val_acc:0.964]
Epoch [72/120    avg_loss:0.049, val_acc:0.978]
Epoch [73/120    avg_loss:0.051, val_acc:0.974]
Epoch [74/120    avg_loss:0.050, val_acc:0.956]
Epoch [75/120    avg_loss:0.046, val_acc:0.964]
Epoch [76/120    avg_loss:0.071, val_acc:0.954]
Epoch [77/120    avg_loss:0.075, val_acc:0.964]
Epoch [78/120    avg_loss:0.037, val_acc:0.964]
Epoch [79/120    avg_loss:0.056, val_acc:0.970]
Epoch [80/120    avg_loss:0.062, val_acc:0.984]
Epoch [81/120    avg_loss:0.067, val_acc:0.899]
Epoch [82/120    avg_loss:0.035, val_acc:0.964]
Epoch [83/120    avg_loss:0.028, val_acc:0.962]
Epoch [84/120    avg_loss:0.041, val_acc:0.984]
Epoch [85/120    avg_loss:0.045, val_acc:0.970]
Epoch [86/120    avg_loss:0.021, val_acc:0.974]
Epoch [87/120    avg_loss:0.019, val_acc:0.968]
Epoch [88/120    avg_loss:0.084, val_acc:0.950]
Epoch [89/120    avg_loss:0.048, val_acc:0.978]
Epoch [90/120    avg_loss:0.028, val_acc:0.966]
Epoch [91/120    avg_loss:0.015, val_acc:0.978]
Epoch [92/120    avg_loss:0.012, val_acc:0.974]
Epoch [93/120    avg_loss:0.013, val_acc:0.986]
Epoch [94/120    avg_loss:0.016, val_acc:0.976]
Epoch [95/120    avg_loss:0.020, val_acc:0.980]
Epoch [96/120    avg_loss:0.010, val_acc:0.968]
Epoch [97/120    avg_loss:0.031, val_acc:0.964]
Epoch [98/120    avg_loss:0.061, val_acc:0.950]
Epoch [99/120    avg_loss:0.048, val_acc:0.952]
Epoch [100/120    avg_loss:0.038, val_acc:0.978]
Epoch [101/120    avg_loss:0.028, val_acc:0.972]
Epoch [102/120    avg_loss:0.049, val_acc:0.984]
Epoch [103/120    avg_loss:0.031, val_acc:0.982]
Epoch [104/120    avg_loss:0.011, val_acc:0.982]
Epoch [105/120    avg_loss:0.015, val_acc:0.980]
Epoch [106/120    avg_loss:0.015, val_acc:0.986]
Epoch [107/120    avg_loss:0.055, val_acc:0.972]
Epoch [108/120    avg_loss:0.035, val_acc:0.982]
Epoch [109/120    avg_loss:0.023, val_acc:0.976]
Epoch [110/120    avg_loss:0.019, val_acc:0.982]
Epoch [111/120    avg_loss:0.048, val_acc:0.974]
Epoch [112/120    avg_loss:0.030, val_acc:0.974]
Epoch [113/120    avg_loss:0.027, val_acc:0.980]
Epoch [114/120    avg_loss:0.065, val_acc:0.962]
Epoch [115/120    avg_loss:0.043, val_acc:0.970]
Epoch [116/120    avg_loss:0.061, val_acc:0.958]
Epoch [117/120    avg_loss:0.026, val_acc:0.980]
Epoch [118/120    avg_loss:0.022, val_acc:0.978]
Epoch [119/120    avg_loss:0.012, val_acc:0.980]
Epoch [120/120    avg_loss:0.011, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   2 206   0   0   0   0   9   1   0   0   0   1   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   9 216   2   0   0   0   0   0   0   0   0]
 [  0   0   0   2  29 114   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   4   0   0   0 384   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   6   0   0   0   0   0   0   0   0   3 444   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.46481876332622

F1 scores:
[       nan 0.99854227 0.95591647 0.97664544 0.9        0.87356322
 0.99019608 0.95431472 0.99353169 1.         1.         0.99603699
 0.98886414 1.        ]

Kappa:
0.9829081641304176
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe3b24a8898>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.273, val_acc:0.574]
Epoch [2/120    avg_loss:1.764, val_acc:0.703]
Epoch [3/120    avg_loss:1.397, val_acc:0.621]
Epoch [4/120    avg_loss:1.104, val_acc:0.773]
Epoch [5/120    avg_loss:0.995, val_acc:0.775]
Epoch [6/120    avg_loss:0.758, val_acc:0.830]
Epoch [7/120    avg_loss:0.782, val_acc:0.826]
Epoch [8/120    avg_loss:0.651, val_acc:0.857]
Epoch [9/120    avg_loss:0.642, val_acc:0.793]
Epoch [10/120    avg_loss:0.582, val_acc:0.887]
Epoch [11/120    avg_loss:0.494, val_acc:0.879]
Epoch [12/120    avg_loss:0.454, val_acc:0.852]
Epoch [13/120    avg_loss:0.512, val_acc:0.902]
Epoch [14/120    avg_loss:0.468, val_acc:0.879]
Epoch [15/120    avg_loss:0.448, val_acc:0.871]
Epoch [16/120    avg_loss:0.441, val_acc:0.863]
Epoch [17/120    avg_loss:0.474, val_acc:0.902]
Epoch [18/120    avg_loss:0.423, val_acc:0.895]
Epoch [19/120    avg_loss:0.415, val_acc:0.898]
Epoch [20/120    avg_loss:0.347, val_acc:0.914]
Epoch [21/120    avg_loss:0.317, val_acc:0.924]
Epoch [22/120    avg_loss:0.349, val_acc:0.916]
Epoch [23/120    avg_loss:0.305, val_acc:0.930]
Epoch [24/120    avg_loss:0.246, val_acc:0.918]
Epoch [25/120    avg_loss:0.362, val_acc:0.898]
Epoch [26/120    avg_loss:0.313, val_acc:0.922]
Epoch [27/120    avg_loss:0.313, val_acc:0.912]
Epoch [28/120    avg_loss:0.305, val_acc:0.910]
Epoch [29/120    avg_loss:0.360, val_acc:0.908]
Epoch [30/120    avg_loss:0.268, val_acc:0.936]
Epoch [31/120    avg_loss:0.245, val_acc:0.904]
Epoch [32/120    avg_loss:0.264, val_acc:0.904]
Epoch [33/120    avg_loss:0.262, val_acc:0.912]
Epoch [34/120    avg_loss:0.277, val_acc:0.936]
Epoch [35/120    avg_loss:0.276, val_acc:0.906]
Epoch [36/120    avg_loss:0.247, val_acc:0.934]
Epoch [37/120    avg_loss:0.188, val_acc:0.941]
Epoch [38/120    avg_loss:0.235, val_acc:0.953]
Epoch [39/120    avg_loss:0.168, val_acc:0.949]
Epoch [40/120    avg_loss:0.170, val_acc:0.920]
Epoch [41/120    avg_loss:0.253, val_acc:0.922]
Epoch [42/120    avg_loss:0.258, val_acc:0.926]
Epoch [43/120    avg_loss:0.283, val_acc:0.945]
Epoch [44/120    avg_loss:0.167, val_acc:0.945]
Epoch [45/120    avg_loss:0.222, val_acc:0.943]
Epoch [46/120    avg_loss:0.196, val_acc:0.953]
Epoch [47/120    avg_loss:0.125, val_acc:0.949]
Epoch [48/120    avg_loss:0.175, val_acc:0.945]
Epoch [49/120    avg_loss:0.151, val_acc:0.955]
Epoch [50/120    avg_loss:0.143, val_acc:0.943]
Epoch [51/120    avg_loss:0.128, val_acc:0.959]
Epoch [52/120    avg_loss:0.135, val_acc:0.959]
Epoch [53/120    avg_loss:0.131, val_acc:0.953]
Epoch [54/120    avg_loss:0.128, val_acc:0.973]
Epoch [55/120    avg_loss:0.109, val_acc:0.965]
Epoch [56/120    avg_loss:0.135, val_acc:0.969]
Epoch [57/120    avg_loss:0.105, val_acc:0.951]
Epoch [58/120    avg_loss:0.117, val_acc:0.959]
Epoch [59/120    avg_loss:0.120, val_acc:0.963]
Epoch [60/120    avg_loss:0.083, val_acc:0.957]
Epoch [61/120    avg_loss:0.083, val_acc:0.963]
Epoch [62/120    avg_loss:0.077, val_acc:0.949]
Epoch [63/120    avg_loss:0.079, val_acc:0.975]
Epoch [64/120    avg_loss:0.055, val_acc:0.963]
Epoch [65/120    avg_loss:0.110, val_acc:0.951]
Epoch [66/120    avg_loss:0.196, val_acc:0.951]
Epoch [67/120    avg_loss:0.085, val_acc:0.969]
Epoch [68/120    avg_loss:0.083, val_acc:0.977]
Epoch [69/120    avg_loss:0.093, val_acc:0.945]
Epoch [70/120    avg_loss:0.092, val_acc:0.947]
Epoch [71/120    avg_loss:0.061, val_acc:0.967]
Epoch [72/120    avg_loss:0.106, val_acc:0.955]
Epoch [73/120    avg_loss:0.088, val_acc:0.971]
Epoch [74/120    avg_loss:0.088, val_acc:0.967]
Epoch [75/120    avg_loss:0.173, val_acc:0.916]
Epoch [76/120    avg_loss:0.080, val_acc:0.957]
Epoch [77/120    avg_loss:0.087, val_acc:0.963]
Epoch [78/120    avg_loss:0.056, val_acc:0.961]
Epoch [79/120    avg_loss:0.050, val_acc:0.973]
Epoch [80/120    avg_loss:0.037, val_acc:0.975]
Epoch [81/120    avg_loss:0.058, val_acc:0.975]
Epoch [82/120    avg_loss:0.041, val_acc:0.977]
Epoch [83/120    avg_loss:0.031, val_acc:0.979]
Epoch [84/120    avg_loss:0.031, val_acc:0.975]
Epoch [85/120    avg_loss:0.028, val_acc:0.977]
Epoch [86/120    avg_loss:0.031, val_acc:0.979]
Epoch [87/120    avg_loss:0.028, val_acc:0.977]
Epoch [88/120    avg_loss:0.025, val_acc:0.975]
Epoch [89/120    avg_loss:0.022, val_acc:0.979]
Epoch [90/120    avg_loss:0.021, val_acc:0.977]
Epoch [91/120    avg_loss:0.022, val_acc:0.975]
Epoch [92/120    avg_loss:0.024, val_acc:0.975]
Epoch [93/120    avg_loss:0.028, val_acc:0.975]
Epoch [94/120    avg_loss:0.020, val_acc:0.979]
Epoch [95/120    avg_loss:0.018, val_acc:0.977]
Epoch [96/120    avg_loss:0.030, val_acc:0.979]
Epoch [97/120    avg_loss:0.027, val_acc:0.980]
Epoch [98/120    avg_loss:0.020, val_acc:0.977]
Epoch [99/120    avg_loss:0.028, val_acc:0.977]
Epoch [100/120    avg_loss:0.026, val_acc:0.979]
Epoch [101/120    avg_loss:0.028, val_acc:0.977]
Epoch [102/120    avg_loss:0.020, val_acc:0.977]
Epoch [103/120    avg_loss:0.020, val_acc:0.977]
Epoch [104/120    avg_loss:0.019, val_acc:0.977]
Epoch [105/120    avg_loss:0.017, val_acc:0.977]
Epoch [106/120    avg_loss:0.019, val_acc:0.979]
Epoch [107/120    avg_loss:0.015, val_acc:0.977]
Epoch [108/120    avg_loss:0.019, val_acc:0.977]
Epoch [109/120    avg_loss:0.017, val_acc:0.977]
Epoch [110/120    avg_loss:0.018, val_acc:0.979]
Epoch [111/120    avg_loss:0.016, val_acc:0.979]
Epoch [112/120    avg_loss:0.016, val_acc:0.979]
Epoch [113/120    avg_loss:0.021, val_acc:0.979]
Epoch [114/120    avg_loss:0.020, val_acc:0.980]
Epoch [115/120    avg_loss:0.017, val_acc:0.980]
Epoch [116/120    avg_loss:0.018, val_acc:0.979]
Epoch [117/120    avg_loss:0.014, val_acc:0.979]
Epoch [118/120    avg_loss:0.021, val_acc:0.979]
Epoch [119/120    avg_loss:0.017, val_acc:0.980]
Epoch [120/120    avg_loss:0.018, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 223   1   0   0   0   0   6   0   0   0   0]
 [  0   0   0   0 207  19   0   0   0   0   0   0   1   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 362   0   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 1.         0.98648649 0.98454746 0.91390728 0.89189189
 0.98771499 0.96703297 1.         0.99363057 0.99724518 1.
 0.99669967 1.        ]

Kappa:
0.9874173868964463
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:02:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f172fa29908>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.332, val_acc:0.591]
Epoch [2/120    avg_loss:1.825, val_acc:0.562]
Epoch [3/120    avg_loss:1.408, val_acc:0.673]
Epoch [4/120    avg_loss:1.126, val_acc:0.742]
Epoch [5/120    avg_loss:1.028, val_acc:0.810]
Epoch [6/120    avg_loss:0.862, val_acc:0.815]
Epoch [7/120    avg_loss:0.777, val_acc:0.821]
Epoch [8/120    avg_loss:0.771, val_acc:0.746]
Epoch [9/120    avg_loss:0.665, val_acc:0.863]
Epoch [10/120    avg_loss:0.615, val_acc:0.873]
Epoch [11/120    avg_loss:0.587, val_acc:0.859]
Epoch [12/120    avg_loss:0.531, val_acc:0.847]
Epoch [13/120    avg_loss:0.549, val_acc:0.895]
Epoch [14/120    avg_loss:0.497, val_acc:0.907]
Epoch [15/120    avg_loss:0.414, val_acc:0.897]
Epoch [16/120    avg_loss:0.421, val_acc:0.897]
Epoch [17/120    avg_loss:0.408, val_acc:0.903]
Epoch [18/120    avg_loss:0.331, val_acc:0.921]
Epoch [19/120    avg_loss:0.412, val_acc:0.913]
Epoch [20/120    avg_loss:0.398, val_acc:0.933]
Epoch [21/120    avg_loss:0.353, val_acc:0.911]
Epoch [22/120    avg_loss:0.370, val_acc:0.911]
Epoch [23/120    avg_loss:0.333, val_acc:0.925]
Epoch [24/120    avg_loss:0.340, val_acc:0.911]
Epoch [25/120    avg_loss:0.288, val_acc:0.929]
Epoch [26/120    avg_loss:0.263, val_acc:0.946]
Epoch [27/120    avg_loss:0.252, val_acc:0.933]
Epoch [28/120    avg_loss:0.287, val_acc:0.952]
Epoch [29/120    avg_loss:0.226, val_acc:0.942]
Epoch [30/120    avg_loss:0.220, val_acc:0.938]
Epoch [31/120    avg_loss:0.199, val_acc:0.946]
Epoch [32/120    avg_loss:0.209, val_acc:0.907]
Epoch [33/120    avg_loss:0.259, val_acc:0.944]
Epoch [34/120    avg_loss:0.264, val_acc:0.929]
Epoch [35/120    avg_loss:0.219, val_acc:0.940]
Epoch [36/120    avg_loss:0.209, val_acc:0.917]
Epoch [37/120    avg_loss:0.177, val_acc:0.875]
Epoch [38/120    avg_loss:0.263, val_acc:0.933]
Epoch [39/120    avg_loss:0.194, val_acc:0.946]
Epoch [40/120    avg_loss:0.223, val_acc:0.942]
Epoch [41/120    avg_loss:0.168, val_acc:0.935]
Epoch [42/120    avg_loss:0.134, val_acc:0.956]
Epoch [43/120    avg_loss:0.114, val_acc:0.960]
Epoch [44/120    avg_loss:0.100, val_acc:0.966]
Epoch [45/120    avg_loss:0.098, val_acc:0.966]
Epoch [46/120    avg_loss:0.097, val_acc:0.968]
Epoch [47/120    avg_loss:0.087, val_acc:0.966]
Epoch [48/120    avg_loss:0.085, val_acc:0.970]
Epoch [49/120    avg_loss:0.085, val_acc:0.966]
Epoch [50/120    avg_loss:0.100, val_acc:0.970]
Epoch [51/120    avg_loss:0.098, val_acc:0.966]
Epoch [52/120    avg_loss:0.085, val_acc:0.970]
Epoch [53/120    avg_loss:0.076, val_acc:0.966]
Epoch [54/120    avg_loss:0.081, val_acc:0.970]
Epoch [55/120    avg_loss:0.071, val_acc:0.972]
Epoch [56/120    avg_loss:0.093, val_acc:0.974]
Epoch [57/120    avg_loss:0.093, val_acc:0.972]
Epoch [58/120    avg_loss:0.084, val_acc:0.968]
Epoch [59/120    avg_loss:0.085, val_acc:0.970]
Epoch [60/120    avg_loss:0.083, val_acc:0.974]
Epoch [61/120    avg_loss:0.074, val_acc:0.968]
Epoch [62/120    avg_loss:0.080, val_acc:0.970]
Epoch [63/120    avg_loss:0.086, val_acc:0.972]
Epoch [64/120    avg_loss:0.082, val_acc:0.974]
Epoch [65/120    avg_loss:0.081, val_acc:0.974]
Epoch [66/120    avg_loss:0.066, val_acc:0.978]
Epoch [67/120    avg_loss:0.063, val_acc:0.980]
Epoch [68/120    avg_loss:0.077, val_acc:0.978]
Epoch [69/120    avg_loss:0.066, val_acc:0.974]
Epoch [70/120    avg_loss:0.065, val_acc:0.982]
Epoch [71/120    avg_loss:0.062, val_acc:0.978]
Epoch [72/120    avg_loss:0.069, val_acc:0.978]
Epoch [73/120    avg_loss:0.060, val_acc:0.972]
Epoch [74/120    avg_loss:0.071, val_acc:0.978]
Epoch [75/120    avg_loss:0.071, val_acc:0.982]
Epoch [76/120    avg_loss:0.067, val_acc:0.978]
Epoch [77/120    avg_loss:0.066, val_acc:0.976]
Epoch [78/120    avg_loss:0.068, val_acc:0.980]
Epoch [79/120    avg_loss:0.060, val_acc:0.980]
Epoch [80/120    avg_loss:0.052, val_acc:0.982]
Epoch [81/120    avg_loss:0.059, val_acc:0.978]
Epoch [82/120    avg_loss:0.065, val_acc:0.978]
Epoch [83/120    avg_loss:0.056, val_acc:0.978]
Epoch [84/120    avg_loss:0.077, val_acc:0.980]
Epoch [85/120    avg_loss:0.066, val_acc:0.980]
Epoch [86/120    avg_loss:0.059, val_acc:0.982]
Epoch [87/120    avg_loss:0.067, val_acc:0.980]
Epoch [88/120    avg_loss:0.059, val_acc:0.980]
Epoch [89/120    avg_loss:0.055, val_acc:0.982]
Epoch [90/120    avg_loss:0.052, val_acc:0.972]
Epoch [91/120    avg_loss:0.055, val_acc:0.980]
Epoch [92/120    avg_loss:0.051, val_acc:0.980]
Epoch [93/120    avg_loss:0.054, val_acc:0.978]
Epoch [94/120    avg_loss:0.060, val_acc:0.978]
Epoch [95/120    avg_loss:0.053, val_acc:0.978]
Epoch [96/120    avg_loss:0.054, val_acc:0.980]
Epoch [97/120    avg_loss:0.060, val_acc:0.982]
Epoch [98/120    avg_loss:0.049, val_acc:0.980]
Epoch [99/120    avg_loss:0.056, val_acc:0.980]
Epoch [100/120    avg_loss:0.051, val_acc:0.980]
Epoch [101/120    avg_loss:0.043, val_acc:0.984]
Epoch [102/120    avg_loss:0.050, val_acc:0.982]
Epoch [103/120    avg_loss:0.049, val_acc:0.980]
Epoch [104/120    avg_loss:0.047, val_acc:0.986]
Epoch [105/120    avg_loss:0.057, val_acc:0.986]
Epoch [106/120    avg_loss:0.053, val_acc:0.980]
Epoch [107/120    avg_loss:0.046, val_acc:0.984]
Epoch [108/120    avg_loss:0.055, val_acc:0.982]
Epoch [109/120    avg_loss:0.040, val_acc:0.982]
Epoch [110/120    avg_loss:0.045, val_acc:0.982]
Epoch [111/120    avg_loss:0.054, val_acc:0.982]
Epoch [112/120    avg_loss:0.050, val_acc:0.980]
Epoch [113/120    avg_loss:0.053, val_acc:0.982]
Epoch [114/120    avg_loss:0.051, val_acc:0.980]
Epoch [115/120    avg_loss:0.053, val_acc:0.980]
Epoch [116/120    avg_loss:0.044, val_acc:0.978]
Epoch [117/120    avg_loss:0.048, val_acc:0.980]
Epoch [118/120    avg_loss:0.046, val_acc:0.982]
Epoch [119/120    avg_loss:0.035, val_acc:0.982]
Epoch [120/120    avg_loss:0.043, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 204  20   3   0   0   3   0   0   0   0   0]
 [  0   0   0   2 210  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  22 123   0   0   0   0   0   0   0   0]
 [  0   6   0   0   0   0 200   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.12366737739872

F1 scores:
[       nan 0.99563953 0.97333333 0.93577982 0.87682672 0.86013986
 0.98522167 0.93785311 0.99485861 1.         1.         0.99341238
 0.99445061 1.        ]

Kappa:
0.9791057375132456
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe1f98fd8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.336, val_acc:0.545]
Epoch [2/120    avg_loss:1.874, val_acc:0.582]
Epoch [3/120    avg_loss:1.513, val_acc:0.688]
Epoch [4/120    avg_loss:1.194, val_acc:0.764]
Epoch [5/120    avg_loss:0.970, val_acc:0.779]
Epoch [6/120    avg_loss:0.876, val_acc:0.826]
Epoch [7/120    avg_loss:0.728, val_acc:0.848]
Epoch [8/120    avg_loss:0.623, val_acc:0.859]
Epoch [9/120    avg_loss:0.525, val_acc:0.885]
Epoch [10/120    avg_loss:0.552, val_acc:0.865]
Epoch [11/120    avg_loss:0.526, val_acc:0.893]
Epoch [12/120    avg_loss:0.475, val_acc:0.857]
Epoch [13/120    avg_loss:0.456, val_acc:0.887]
Epoch [14/120    avg_loss:0.484, val_acc:0.912]
Epoch [15/120    avg_loss:0.407, val_acc:0.869]
Epoch [16/120    avg_loss:0.400, val_acc:0.902]
Epoch [17/120    avg_loss:0.392, val_acc:0.898]
Epoch [18/120    avg_loss:0.340, val_acc:0.891]
Epoch [19/120    avg_loss:0.374, val_acc:0.914]
Epoch [20/120    avg_loss:0.427, val_acc:0.912]
Epoch [21/120    avg_loss:0.382, val_acc:0.908]
Epoch [22/120    avg_loss:0.334, val_acc:0.932]
Epoch [23/120    avg_loss:0.318, val_acc:0.875]
Epoch [24/120    avg_loss:0.381, val_acc:0.918]
Epoch [25/120    avg_loss:0.364, val_acc:0.939]
Epoch [26/120    avg_loss:0.291, val_acc:0.945]
Epoch [27/120    avg_loss:0.282, val_acc:0.904]
Epoch [28/120    avg_loss:0.290, val_acc:0.922]
Epoch [29/120    avg_loss:0.322, val_acc:0.932]
Epoch [30/120    avg_loss:0.279, val_acc:0.943]
Epoch [31/120    avg_loss:0.367, val_acc:0.871]
Epoch [32/120    avg_loss:0.262, val_acc:0.955]
Epoch [33/120    avg_loss:0.288, val_acc:0.914]
Epoch [34/120    avg_loss:0.262, val_acc:0.924]
Epoch [35/120    avg_loss:0.261, val_acc:0.936]
Epoch [36/120    avg_loss:0.230, val_acc:0.916]
Epoch [37/120    avg_loss:0.251, val_acc:0.904]
Epoch [38/120    avg_loss:0.243, val_acc:0.939]
Epoch [39/120    avg_loss:0.201, val_acc:0.959]
Epoch [40/120    avg_loss:0.191, val_acc:0.934]
Epoch [41/120    avg_loss:0.211, val_acc:0.941]
Epoch [42/120    avg_loss:0.128, val_acc:0.855]
Epoch [43/120    avg_loss:0.197, val_acc:0.947]
Epoch [44/120    avg_loss:0.170, val_acc:0.961]
Epoch [45/120    avg_loss:0.137, val_acc:0.957]
Epoch [46/120    avg_loss:0.157, val_acc:0.941]
Epoch [47/120    avg_loss:0.221, val_acc:0.949]
Epoch [48/120    avg_loss:0.153, val_acc:0.930]
Epoch [49/120    avg_loss:0.191, val_acc:0.934]
Epoch [50/120    avg_loss:0.136, val_acc:0.965]
Epoch [51/120    avg_loss:0.108, val_acc:0.943]
Epoch [52/120    avg_loss:0.100, val_acc:0.965]
Epoch [53/120    avg_loss:0.162, val_acc:0.918]
Epoch [54/120    avg_loss:0.171, val_acc:0.928]
Epoch [55/120    avg_loss:0.158, val_acc:0.969]
Epoch [56/120    avg_loss:0.096, val_acc:0.959]
Epoch [57/120    avg_loss:0.117, val_acc:0.949]
Epoch [58/120    avg_loss:0.101, val_acc:0.953]
Epoch [59/120    avg_loss:0.077, val_acc:0.963]
Epoch [60/120    avg_loss:0.088, val_acc:0.949]
Epoch [61/120    avg_loss:0.090, val_acc:0.957]
Epoch [62/120    avg_loss:0.104, val_acc:0.957]
Epoch [63/120    avg_loss:0.104, val_acc:0.961]
Epoch [64/120    avg_loss:0.093, val_acc:0.967]
Epoch [65/120    avg_loss:0.067, val_acc:0.963]
Epoch [66/120    avg_loss:0.108, val_acc:0.959]
Epoch [67/120    avg_loss:0.093, val_acc:0.928]
Epoch [68/120    avg_loss:0.126, val_acc:0.955]
Epoch [69/120    avg_loss:0.061, val_acc:0.967]
Epoch [70/120    avg_loss:0.042, val_acc:0.971]
Epoch [71/120    avg_loss:0.041, val_acc:0.969]
Epoch [72/120    avg_loss:0.051, val_acc:0.975]
Epoch [73/120    avg_loss:0.040, val_acc:0.977]
Epoch [74/120    avg_loss:0.038, val_acc:0.980]
Epoch [75/120    avg_loss:0.035, val_acc:0.979]
Epoch [76/120    avg_loss:0.041, val_acc:0.977]
Epoch [77/120    avg_loss:0.032, val_acc:0.977]
Epoch [78/120    avg_loss:0.029, val_acc:0.977]
Epoch [79/120    avg_loss:0.042, val_acc:0.979]
Epoch [80/120    avg_loss:0.027, val_acc:0.977]
Epoch [81/120    avg_loss:0.031, val_acc:0.977]
Epoch [82/120    avg_loss:0.030, val_acc:0.977]
Epoch [83/120    avg_loss:0.027, val_acc:0.977]
Epoch [84/120    avg_loss:0.031, val_acc:0.971]
Epoch [85/120    avg_loss:0.030, val_acc:0.975]
Epoch [86/120    avg_loss:0.034, val_acc:0.977]
Epoch [87/120    avg_loss:0.025, val_acc:0.977]
Epoch [88/120    avg_loss:0.026, val_acc:0.977]
Epoch [89/120    avg_loss:0.028, val_acc:0.977]
Epoch [90/120    avg_loss:0.035, val_acc:0.977]
Epoch [91/120    avg_loss:0.021, val_acc:0.977]
Epoch [92/120    avg_loss:0.023, val_acc:0.977]
Epoch [93/120    avg_loss:0.024, val_acc:0.977]
Epoch [94/120    avg_loss:0.026, val_acc:0.977]
Epoch [95/120    avg_loss:0.033, val_acc:0.977]
Epoch [96/120    avg_loss:0.022, val_acc:0.977]
Epoch [97/120    avg_loss:0.028, val_acc:0.977]
Epoch [98/120    avg_loss:0.029, val_acc:0.977]
Epoch [99/120    avg_loss:0.023, val_acc:0.977]
Epoch [100/120    avg_loss:0.024, val_acc:0.977]
Epoch [101/120    avg_loss:0.028, val_acc:0.977]
Epoch [102/120    avg_loss:0.027, val_acc:0.977]
Epoch [103/120    avg_loss:0.026, val_acc:0.977]
Epoch [104/120    avg_loss:0.026, val_acc:0.977]
Epoch [105/120    avg_loss:0.026, val_acc:0.977]
Epoch [106/120    avg_loss:0.027, val_acc:0.977]
Epoch [107/120    avg_loss:0.028, val_acc:0.977]
Epoch [108/120    avg_loss:0.025, val_acc:0.977]
Epoch [109/120    avg_loss:0.019, val_acc:0.977]
Epoch [110/120    avg_loss:0.030, val_acc:0.977]
Epoch [111/120    avg_loss:0.026, val_acc:0.977]
Epoch [112/120    avg_loss:0.033, val_acc:0.977]
Epoch [113/120    avg_loss:0.024, val_acc:0.977]
Epoch [114/120    avg_loss:0.026, val_acc:0.977]
Epoch [115/120    avg_loss:0.025, val_acc:0.977]
Epoch [116/120    avg_loss:0.032, val_acc:0.977]
Epoch [117/120    avg_loss:0.020, val_acc:0.977]
Epoch [118/120    avg_loss:0.027, val_acc:0.977]
Epoch [119/120    avg_loss:0.022, val_acc:0.977]
Epoch [120/120    avg_loss:0.022, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   3 203  24   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 197  30   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   3   0   0   0   0   0   0   0   0   0 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.4861407249467

F1 scores:
[       nan 1.         0.97091723 0.93764434 0.87168142 0.90625
 0.99019608 0.96216216 1.         1.         1.         1.
 0.99667774 1.        ]

Kappa:
0.9831483117593195
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f998b1008d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.291, val_acc:0.583]
Epoch [2/120    avg_loss:1.741, val_acc:0.716]
Epoch [3/120    avg_loss:1.404, val_acc:0.742]
Epoch [4/120    avg_loss:1.095, val_acc:0.784]
Epoch [5/120    avg_loss:0.932, val_acc:0.806]
Epoch [6/120    avg_loss:0.864, val_acc:0.861]
Epoch [7/120    avg_loss:0.767, val_acc:0.804]
Epoch [8/120    avg_loss:0.736, val_acc:0.847]
Epoch [9/120    avg_loss:0.646, val_acc:0.861]
Epoch [10/120    avg_loss:0.626, val_acc:0.863]
Epoch [11/120    avg_loss:0.583, val_acc:0.865]
Epoch [12/120    avg_loss:0.533, val_acc:0.827]
Epoch [13/120    avg_loss:0.580, val_acc:0.877]
Epoch [14/120    avg_loss:0.550, val_acc:0.879]
Epoch [15/120    avg_loss:0.520, val_acc:0.899]
Epoch [16/120    avg_loss:0.521, val_acc:0.893]
Epoch [17/120    avg_loss:0.436, val_acc:0.806]
Epoch [18/120    avg_loss:0.493, val_acc:0.895]
Epoch [19/120    avg_loss:0.399, val_acc:0.879]
Epoch [20/120    avg_loss:0.418, val_acc:0.839]
Epoch [21/120    avg_loss:0.378, val_acc:0.849]
Epoch [22/120    avg_loss:0.399, val_acc:0.909]
Epoch [23/120    avg_loss:0.354, val_acc:0.927]
Epoch [24/120    avg_loss:0.356, val_acc:0.927]
Epoch [25/120    avg_loss:0.361, val_acc:0.893]
Epoch [26/120    avg_loss:0.350, val_acc:0.921]
Epoch [27/120    avg_loss:0.405, val_acc:0.909]
Epoch [28/120    avg_loss:0.291, val_acc:0.851]
Epoch [29/120    avg_loss:0.307, val_acc:0.935]
Epoch [30/120    avg_loss:0.281, val_acc:0.956]
Epoch [31/120    avg_loss:0.247, val_acc:0.923]
Epoch [32/120    avg_loss:0.276, val_acc:0.942]
Epoch [33/120    avg_loss:0.254, val_acc:0.923]
Epoch [34/120    avg_loss:0.230, val_acc:0.938]
Epoch [35/120    avg_loss:0.261, val_acc:0.937]
Epoch [36/120    avg_loss:0.269, val_acc:0.946]
Epoch [37/120    avg_loss:0.308, val_acc:0.863]
Epoch [38/120    avg_loss:0.241, val_acc:0.946]
Epoch [39/120    avg_loss:0.194, val_acc:0.942]
Epoch [40/120    avg_loss:0.205, val_acc:0.925]
Epoch [41/120    avg_loss:0.196, val_acc:0.938]
Epoch [42/120    avg_loss:0.213, val_acc:0.954]
Epoch [43/120    avg_loss:0.213, val_acc:0.946]
Epoch [44/120    avg_loss:0.166, val_acc:0.948]
Epoch [45/120    avg_loss:0.125, val_acc:0.946]
Epoch [46/120    avg_loss:0.119, val_acc:0.952]
Epoch [47/120    avg_loss:0.120, val_acc:0.952]
Epoch [48/120    avg_loss:0.102, val_acc:0.952]
Epoch [49/120    avg_loss:0.106, val_acc:0.954]
Epoch [50/120    avg_loss:0.092, val_acc:0.958]
Epoch [51/120    avg_loss:0.103, val_acc:0.958]
Epoch [52/120    avg_loss:0.110, val_acc:0.960]
Epoch [53/120    avg_loss:0.109, val_acc:0.956]
Epoch [54/120    avg_loss:0.101, val_acc:0.956]
Epoch [55/120    avg_loss:0.103, val_acc:0.956]
Epoch [56/120    avg_loss:0.093, val_acc:0.958]
Epoch [57/120    avg_loss:0.076, val_acc:0.956]
Epoch [58/120    avg_loss:0.088, val_acc:0.956]
Epoch [59/120    avg_loss:0.095, val_acc:0.954]
Epoch [60/120    avg_loss:0.084, val_acc:0.954]
Epoch [61/120    avg_loss:0.110, val_acc:0.954]
Epoch [62/120    avg_loss:0.089, val_acc:0.958]
Epoch [63/120    avg_loss:0.091, val_acc:0.956]
Epoch [64/120    avg_loss:0.089, val_acc:0.954]
Epoch [65/120    avg_loss:0.100, val_acc:0.954]
Epoch [66/120    avg_loss:0.077, val_acc:0.954]
Epoch [67/120    avg_loss:0.082, val_acc:0.952]
Epoch [68/120    avg_loss:0.095, val_acc:0.954]
Epoch [69/120    avg_loss:0.079, val_acc:0.952]
Epoch [70/120    avg_loss:0.078, val_acc:0.954]
Epoch [71/120    avg_loss:0.077, val_acc:0.956]
Epoch [72/120    avg_loss:0.083, val_acc:0.956]
Epoch [73/120    avg_loss:0.080, val_acc:0.954]
Epoch [74/120    avg_loss:0.083, val_acc:0.954]
Epoch [75/120    avg_loss:0.070, val_acc:0.954]
Epoch [76/120    avg_loss:0.084, val_acc:0.954]
Epoch [77/120    avg_loss:0.078, val_acc:0.956]
Epoch [78/120    avg_loss:0.077, val_acc:0.956]
Epoch [79/120    avg_loss:0.084, val_acc:0.956]
Epoch [80/120    avg_loss:0.071, val_acc:0.956]
Epoch [81/120    avg_loss:0.069, val_acc:0.956]
Epoch [82/120    avg_loss:0.082, val_acc:0.956]
Epoch [83/120    avg_loss:0.079, val_acc:0.956]
Epoch [84/120    avg_loss:0.074, val_acc:0.956]
Epoch [85/120    avg_loss:0.106, val_acc:0.956]
Epoch [86/120    avg_loss:0.077, val_acc:0.956]
Epoch [87/120    avg_loss:0.076, val_acc:0.956]
Epoch [88/120    avg_loss:0.072, val_acc:0.956]
Epoch [89/120    avg_loss:0.085, val_acc:0.956]
Epoch [90/120    avg_loss:0.065, val_acc:0.956]
Epoch [91/120    avg_loss:0.094, val_acc:0.956]
Epoch [92/120    avg_loss:0.082, val_acc:0.956]
Epoch [93/120    avg_loss:0.067, val_acc:0.956]
Epoch [94/120    avg_loss:0.079, val_acc:0.956]
Epoch [95/120    avg_loss:0.077, val_acc:0.956]
Epoch [96/120    avg_loss:0.077, val_acc:0.956]
Epoch [97/120    avg_loss:0.070, val_acc:0.956]
Epoch [98/120    avg_loss:0.073, val_acc:0.956]
Epoch [99/120    avg_loss:0.080, val_acc:0.956]
Epoch [100/120    avg_loss:0.093, val_acc:0.956]
Epoch [101/120    avg_loss:0.075, val_acc:0.956]
Epoch [102/120    avg_loss:0.076, val_acc:0.956]
Epoch [103/120    avg_loss:0.084, val_acc:0.956]
Epoch [104/120    avg_loss:0.085, val_acc:0.956]
Epoch [105/120    avg_loss:0.088, val_acc:0.956]
Epoch [106/120    avg_loss:0.092, val_acc:0.956]
Epoch [107/120    avg_loss:0.085, val_acc:0.956]
Epoch [108/120    avg_loss:0.076, val_acc:0.956]
Epoch [109/120    avg_loss:0.085, val_acc:0.956]
Epoch [110/120    avg_loss:0.073, val_acc:0.956]
Epoch [111/120    avg_loss:0.080, val_acc:0.956]
Epoch [112/120    avg_loss:0.075, val_acc:0.956]
Epoch [113/120    avg_loss:0.083, val_acc:0.956]
Epoch [114/120    avg_loss:0.083, val_acc:0.956]
Epoch [115/120    avg_loss:0.071, val_acc:0.956]
Epoch [116/120    avg_loss:0.084, val_acc:0.956]
Epoch [117/120    avg_loss:0.077, val_acc:0.956]
Epoch [118/120    avg_loss:0.082, val_acc:0.956]
Epoch [119/120    avg_loss:0.067, val_acc:0.956]
Epoch [120/120    avg_loss:0.094, val_acc:0.956]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 206  18   0   0   0   5   0   0   0   0   0]
 [  0   0   0  19 186  22   0   0   0   0   0   0   0   0]
 [  0   0   0  10   2 133   0   0   0   0   0   0   0   0]
 [  0   6   0   0  11   0 189   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.6545842217484

F1 scores:
[       nan 0.99563953 0.96263736 0.88602151 0.83783784 0.88666667
 0.95696203 0.91954023 0.99230769 1.         1.         1.
 0.99889503 1.        ]

Kappa:
0.9738821652803812
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4f683158d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.316, val_acc:0.650]
Epoch [2/120    avg_loss:1.783, val_acc:0.639]
Epoch [3/120    avg_loss:1.392, val_acc:0.684]
Epoch [4/120    avg_loss:1.090, val_acc:0.756]
Epoch [5/120    avg_loss:0.926, val_acc:0.826]
Epoch [6/120    avg_loss:0.895, val_acc:0.836]
Epoch [7/120    avg_loss:0.748, val_acc:0.826]
Epoch [8/120    avg_loss:0.678, val_acc:0.797]
Epoch [9/120    avg_loss:0.616, val_acc:0.830]
Epoch [10/120    avg_loss:0.570, val_acc:0.916]
Epoch [11/120    avg_loss:0.566, val_acc:0.855]
Epoch [12/120    avg_loss:0.547, val_acc:0.900]
Epoch [13/120    avg_loss:0.511, val_acc:0.875]
Epoch [14/120    avg_loss:0.459, val_acc:0.902]
Epoch [15/120    avg_loss:0.457, val_acc:0.910]
Epoch [16/120    avg_loss:0.503, val_acc:0.873]
Epoch [17/120    avg_loss:0.445, val_acc:0.875]
Epoch [18/120    avg_loss:0.402, val_acc:0.889]
Epoch [19/120    avg_loss:0.457, val_acc:0.898]
Epoch [20/120    avg_loss:0.356, val_acc:0.918]
Epoch [21/120    avg_loss:0.331, val_acc:0.904]
Epoch [22/120    avg_loss:0.386, val_acc:0.896]
Epoch [23/120    avg_loss:0.330, val_acc:0.934]
Epoch [24/120    avg_loss:0.331, val_acc:0.947]
Epoch [25/120    avg_loss:0.276, val_acc:0.934]
Epoch [26/120    avg_loss:0.280, val_acc:0.912]
Epoch [27/120    avg_loss:0.353, val_acc:0.908]
Epoch [28/120    avg_loss:0.269, val_acc:0.896]
Epoch [29/120    avg_loss:0.251, val_acc:0.883]
Epoch [30/120    avg_loss:0.322, val_acc:0.904]
Epoch [31/120    avg_loss:0.293, val_acc:0.912]
Epoch [32/120    avg_loss:0.299, val_acc:0.936]
Epoch [33/120    avg_loss:0.243, val_acc:0.947]
Epoch [34/120    avg_loss:0.249, val_acc:0.945]
Epoch [35/120    avg_loss:0.193, val_acc:0.945]
Epoch [36/120    avg_loss:0.168, val_acc:0.955]
Epoch [37/120    avg_loss:0.213, val_acc:0.943]
Epoch [38/120    avg_loss:0.252, val_acc:0.941]
Epoch [39/120    avg_loss:0.186, val_acc:0.912]
Epoch [40/120    avg_loss:0.380, val_acc:0.910]
Epoch [41/120    avg_loss:0.223, val_acc:0.949]
Epoch [42/120    avg_loss:0.175, val_acc:0.949]
Epoch [43/120    avg_loss:0.182, val_acc:0.949]
Epoch [44/120    avg_loss:0.180, val_acc:0.961]
Epoch [45/120    avg_loss:0.174, val_acc:0.961]
Epoch [46/120    avg_loss:0.213, val_acc:0.949]
Epoch [47/120    avg_loss:0.158, val_acc:0.971]
Epoch [48/120    avg_loss:0.135, val_acc:0.969]
Epoch [49/120    avg_loss:0.122, val_acc:0.965]
Epoch [50/120    avg_loss:0.132, val_acc:0.977]
Epoch [51/120    avg_loss:0.179, val_acc:0.949]
Epoch [52/120    avg_loss:0.184, val_acc:0.951]
Epoch [53/120    avg_loss:0.219, val_acc:0.943]
Epoch [54/120    avg_loss:0.167, val_acc:0.965]
Epoch [55/120    avg_loss:0.135, val_acc:0.977]
Epoch [56/120    avg_loss:0.099, val_acc:0.980]
Epoch [57/120    avg_loss:0.062, val_acc:0.973]
Epoch [58/120    avg_loss:0.108, val_acc:0.961]
Epoch [59/120    avg_loss:0.188, val_acc:0.973]
Epoch [60/120    avg_loss:0.101, val_acc:0.971]
Epoch [61/120    avg_loss:0.067, val_acc:0.973]
Epoch [62/120    avg_loss:0.099, val_acc:0.973]
Epoch [63/120    avg_loss:0.099, val_acc:0.975]
Epoch [64/120    avg_loss:0.114, val_acc:0.975]
Epoch [65/120    avg_loss:0.090, val_acc:0.971]
Epoch [66/120    avg_loss:0.111, val_acc:0.977]
Epoch [67/120    avg_loss:0.099, val_acc:0.979]
Epoch [68/120    avg_loss:0.109, val_acc:0.984]
Epoch [69/120    avg_loss:0.059, val_acc:0.979]
Epoch [70/120    avg_loss:0.045, val_acc:0.980]
Epoch [71/120    avg_loss:0.062, val_acc:0.973]
Epoch [72/120    avg_loss:0.080, val_acc:0.975]
Epoch [73/120    avg_loss:0.115, val_acc:0.957]
Epoch [74/120    avg_loss:0.116, val_acc:0.979]
Epoch [75/120    avg_loss:0.078, val_acc:0.982]
Epoch [76/120    avg_loss:0.064, val_acc:0.979]
Epoch [77/120    avg_loss:0.067, val_acc:0.979]
Epoch [78/120    avg_loss:0.071, val_acc:0.967]
Epoch [79/120    avg_loss:0.045, val_acc:0.977]
Epoch [80/120    avg_loss:0.047, val_acc:0.977]
Epoch [81/120    avg_loss:0.080, val_acc:0.965]
Epoch [82/120    avg_loss:0.062, val_acc:0.975]
Epoch [83/120    avg_loss:0.056, val_acc:0.980]
Epoch [84/120    avg_loss:0.041, val_acc:0.982]
Epoch [85/120    avg_loss:0.048, val_acc:0.980]
Epoch [86/120    avg_loss:0.029, val_acc:0.980]
Epoch [87/120    avg_loss:0.030, val_acc:0.980]
Epoch [88/120    avg_loss:0.026, val_acc:0.982]
Epoch [89/120    avg_loss:0.030, val_acc:0.984]
Epoch [90/120    avg_loss:0.023, val_acc:0.982]
Epoch [91/120    avg_loss:0.029, val_acc:0.984]
Epoch [92/120    avg_loss:0.032, val_acc:0.982]
Epoch [93/120    avg_loss:0.030, val_acc:0.982]
Epoch [94/120    avg_loss:0.029, val_acc:0.982]
Epoch [95/120    avg_loss:0.020, val_acc:0.982]
Epoch [96/120    avg_loss:0.022, val_acc:0.982]
Epoch [97/120    avg_loss:0.025, val_acc:0.984]
Epoch [98/120    avg_loss:0.020, val_acc:0.982]
Epoch [99/120    avg_loss:0.016, val_acc:0.984]
Epoch [100/120    avg_loss:0.017, val_acc:0.982]
Epoch [101/120    avg_loss:0.025, val_acc:0.984]
Epoch [102/120    avg_loss:0.015, val_acc:0.982]
Epoch [103/120    avg_loss:0.022, val_acc:0.982]
Epoch [104/120    avg_loss:0.020, val_acc:0.984]
Epoch [105/120    avg_loss:0.016, val_acc:0.982]
Epoch [106/120    avg_loss:0.015, val_acc:0.984]
Epoch [107/120    avg_loss:0.018, val_acc:0.982]
Epoch [108/120    avg_loss:0.018, val_acc:0.982]
Epoch [109/120    avg_loss:0.018, val_acc:0.984]
Epoch [110/120    avg_loss:0.015, val_acc:0.982]
Epoch [111/120    avg_loss:0.016, val_acc:0.982]
Epoch [112/120    avg_loss:0.019, val_acc:0.982]
Epoch [113/120    avg_loss:0.022, val_acc:0.984]
Epoch [114/120    avg_loss:0.018, val_acc:0.982]
Epoch [115/120    avg_loss:0.020, val_acc:0.984]
Epoch [116/120    avg_loss:0.014, val_acc:0.986]
Epoch [117/120    avg_loss:0.013, val_acc:0.986]
Epoch [118/120    avg_loss:0.018, val_acc:0.986]
Epoch [119/120    avg_loss:0.013, val_acc:0.986]
Epoch [120/120    avg_loss:0.014, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   0 225   3   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 1.         0.97297297 0.98901099 0.92173913 0.88501742
 1.         0.95698925 0.99742931 1.         1.         1.
 0.99556541 1.        ]

Kappa:
0.9881310071592043
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f05efdbb908>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.331, val_acc:0.623]
Epoch [2/120    avg_loss:1.860, val_acc:0.637]
Epoch [3/120    avg_loss:1.450, val_acc:0.665]
Epoch [4/120    avg_loss:1.144, val_acc:0.694]
Epoch [5/120    avg_loss:1.021, val_acc:0.752]
Epoch [6/120    avg_loss:0.865, val_acc:0.736]
Epoch [7/120    avg_loss:0.796, val_acc:0.823]
Epoch [8/120    avg_loss:0.739, val_acc:0.855]
Epoch [9/120    avg_loss:0.666, val_acc:0.877]
Epoch [10/120    avg_loss:0.643, val_acc:0.835]
Epoch [11/120    avg_loss:0.562, val_acc:0.893]
Epoch [12/120    avg_loss:0.455, val_acc:0.899]
Epoch [13/120    avg_loss:0.549, val_acc:0.905]
Epoch [14/120    avg_loss:0.479, val_acc:0.873]
Epoch [15/120    avg_loss:0.477, val_acc:0.881]
Epoch [16/120    avg_loss:0.420, val_acc:0.929]
Epoch [17/120    avg_loss:0.426, val_acc:0.927]
Epoch [18/120    avg_loss:0.377, val_acc:0.923]
Epoch [19/120    avg_loss:0.313, val_acc:0.938]
Epoch [20/120    avg_loss:0.330, val_acc:0.937]
Epoch [21/120    avg_loss:0.371, val_acc:0.940]
Epoch [22/120    avg_loss:0.324, val_acc:0.917]
Epoch [23/120    avg_loss:0.310, val_acc:0.903]
Epoch [24/120    avg_loss:0.277, val_acc:0.937]
Epoch [25/120    avg_loss:0.325, val_acc:0.929]
Epoch [26/120    avg_loss:0.284, val_acc:0.946]
Epoch [27/120    avg_loss:0.267, val_acc:0.950]
Epoch [28/120    avg_loss:0.297, val_acc:0.927]
Epoch [29/120    avg_loss:0.270, val_acc:0.933]
Epoch [30/120    avg_loss:0.224, val_acc:0.956]
Epoch [31/120    avg_loss:0.230, val_acc:0.954]
Epoch [32/120    avg_loss:0.190, val_acc:0.954]
Epoch [33/120    avg_loss:0.228, val_acc:0.946]
Epoch [34/120    avg_loss:0.212, val_acc:0.948]
Epoch [35/120    avg_loss:0.247, val_acc:0.948]
Epoch [36/120    avg_loss:0.187, val_acc:0.933]
Epoch [37/120    avg_loss:0.201, val_acc:0.952]
Epoch [38/120    avg_loss:0.157, val_acc:0.968]
Epoch [39/120    avg_loss:0.197, val_acc:0.968]
Epoch [40/120    avg_loss:0.200, val_acc:0.956]
Epoch [41/120    avg_loss:0.169, val_acc:0.960]
Epoch [42/120    avg_loss:0.152, val_acc:0.954]
Epoch [43/120    avg_loss:0.242, val_acc:0.919]
Epoch [44/120    avg_loss:0.173, val_acc:0.956]
Epoch [45/120    avg_loss:0.114, val_acc:0.972]
Epoch [46/120    avg_loss:0.113, val_acc:0.978]
Epoch [47/120    avg_loss:0.132, val_acc:0.972]
Epoch [48/120    avg_loss:0.114, val_acc:0.976]
Epoch [49/120    avg_loss:0.146, val_acc:0.968]
Epoch [50/120    avg_loss:0.117, val_acc:0.954]
Epoch [51/120    avg_loss:0.116, val_acc:0.970]
Epoch [52/120    avg_loss:0.128, val_acc:0.966]
Epoch [53/120    avg_loss:0.103, val_acc:0.974]
Epoch [54/120    avg_loss:0.116, val_acc:0.974]
Epoch [55/120    avg_loss:0.086, val_acc:0.970]
Epoch [56/120    avg_loss:0.093, val_acc:0.974]
Epoch [57/120    avg_loss:0.140, val_acc:0.946]
Epoch [58/120    avg_loss:0.075, val_acc:0.978]
Epoch [59/120    avg_loss:0.078, val_acc:0.974]
Epoch [60/120    avg_loss:0.072, val_acc:0.976]
Epoch [61/120    avg_loss:0.061, val_acc:0.982]
Epoch [62/120    avg_loss:0.052, val_acc:0.988]
Epoch [63/120    avg_loss:0.076, val_acc:0.978]
Epoch [64/120    avg_loss:0.089, val_acc:0.974]
Epoch [65/120    avg_loss:0.068, val_acc:0.982]
Epoch [66/120    avg_loss:0.033, val_acc:0.982]
Epoch [67/120    avg_loss:0.035, val_acc:0.984]
Epoch [68/120    avg_loss:0.032, val_acc:0.972]
Epoch [69/120    avg_loss:0.078, val_acc:0.956]
Epoch [70/120    avg_loss:0.051, val_acc:0.980]
Epoch [71/120    avg_loss:0.142, val_acc:0.925]
Epoch [72/120    avg_loss:0.173, val_acc:0.962]
Epoch [73/120    avg_loss:0.074, val_acc:0.984]
Epoch [74/120    avg_loss:0.059, val_acc:0.986]
Epoch [75/120    avg_loss:0.139, val_acc:0.970]
Epoch [76/120    avg_loss:0.066, val_acc:0.986]
Epoch [77/120    avg_loss:0.053, val_acc:0.984]
Epoch [78/120    avg_loss:0.053, val_acc:0.986]
Epoch [79/120    avg_loss:0.047, val_acc:0.986]
Epoch [80/120    avg_loss:0.036, val_acc:0.986]
Epoch [81/120    avg_loss:0.047, val_acc:0.984]
Epoch [82/120    avg_loss:0.035, val_acc:0.984]
Epoch [83/120    avg_loss:0.042, val_acc:0.984]
Epoch [84/120    avg_loss:0.034, val_acc:0.984]
Epoch [85/120    avg_loss:0.027, val_acc:0.984]
Epoch [86/120    avg_loss:0.028, val_acc:0.984]
Epoch [87/120    avg_loss:0.037, val_acc:0.984]
Epoch [88/120    avg_loss:0.031, val_acc:0.984]
Epoch [89/120    avg_loss:0.029, val_acc:0.984]
Epoch [90/120    avg_loss:0.032, val_acc:0.984]
Epoch [91/120    avg_loss:0.026, val_acc:0.984]
Epoch [92/120    avg_loss:0.028, val_acc:0.984]
Epoch [93/120    avg_loss:0.035, val_acc:0.984]
Epoch [94/120    avg_loss:0.023, val_acc:0.984]
Epoch [95/120    avg_loss:0.025, val_acc:0.984]
Epoch [96/120    avg_loss:0.031, val_acc:0.984]
Epoch [97/120    avg_loss:0.025, val_acc:0.984]
Epoch [98/120    avg_loss:0.028, val_acc:0.984]
Epoch [99/120    avg_loss:0.035, val_acc:0.984]
Epoch [100/120    avg_loss:0.028, val_acc:0.984]
Epoch [101/120    avg_loss:0.025, val_acc:0.984]
Epoch [102/120    avg_loss:0.026, val_acc:0.984]
Epoch [103/120    avg_loss:0.029, val_acc:0.984]
Epoch [104/120    avg_loss:0.028, val_acc:0.984]
Epoch [105/120    avg_loss:0.028, val_acc:0.984]
Epoch [106/120    avg_loss:0.027, val_acc:0.984]
Epoch [107/120    avg_loss:0.032, val_acc:0.984]
Epoch [108/120    avg_loss:0.032, val_acc:0.984]
Epoch [109/120    avg_loss:0.029, val_acc:0.984]
Epoch [110/120    avg_loss:0.026, val_acc:0.984]
Epoch [111/120    avg_loss:0.032, val_acc:0.984]
Epoch [112/120    avg_loss:0.026, val_acc:0.984]
Epoch [113/120    avg_loss:0.029, val_acc:0.984]
Epoch [114/120    avg_loss:0.026, val_acc:0.984]
Epoch [115/120    avg_loss:0.030, val_acc:0.984]
Epoch [116/120    avg_loss:0.031, val_acc:0.984]
Epoch [117/120    avg_loss:0.028, val_acc:0.984]
Epoch [118/120    avg_loss:0.031, val_acc:0.984]
Epoch [119/120    avg_loss:0.027, val_acc:0.984]
Epoch [120/120    avg_loss:0.029, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 209  12   0   0   0   8   0   0   0   0   0]
 [  0   0   0   7 208  12   0   0   0   0   0   0   0   0]
 [  0   0   0   1  18 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10   0 196   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.31556503198294

F1 scores:
[       nan 1.         0.97550111 0.93512304 0.87578947 0.89045936
 0.97512438 0.94382022 0.98979592 1.         1.         1.
 1.         1.        ]

Kappa:
0.9812437559972168
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa64ae608d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.313, val_acc:0.655]
Epoch [2/120    avg_loss:1.839, val_acc:0.687]
Epoch [3/120    avg_loss:1.465, val_acc:0.752]
Epoch [4/120    avg_loss:1.179, val_acc:0.754]
Epoch [5/120    avg_loss:0.988, val_acc:0.810]
Epoch [6/120    avg_loss:0.861, val_acc:0.835]
Epoch [7/120    avg_loss:0.805, val_acc:0.754]
Epoch [8/120    avg_loss:0.736, val_acc:0.845]
Epoch [9/120    avg_loss:0.665, val_acc:0.845]
Epoch [10/120    avg_loss:0.628, val_acc:0.881]
Epoch [11/120    avg_loss:0.576, val_acc:0.839]
Epoch [12/120    avg_loss:0.528, val_acc:0.893]
Epoch [13/120    avg_loss:0.509, val_acc:0.889]
Epoch [14/120    avg_loss:0.427, val_acc:0.865]
Epoch [15/120    avg_loss:0.497, val_acc:0.897]
Epoch [16/120    avg_loss:0.409, val_acc:0.911]
Epoch [17/120    avg_loss:0.385, val_acc:0.887]
Epoch [18/120    avg_loss:0.410, val_acc:0.923]
Epoch [19/120    avg_loss:0.408, val_acc:0.883]
Epoch [20/120    avg_loss:0.412, val_acc:0.891]
Epoch [21/120    avg_loss:0.394, val_acc:0.893]
Epoch [22/120    avg_loss:0.359, val_acc:0.911]
Epoch [23/120    avg_loss:0.313, val_acc:0.921]
Epoch [24/120    avg_loss:0.345, val_acc:0.917]
Epoch [25/120    avg_loss:0.295, val_acc:0.929]
Epoch [26/120    avg_loss:0.331, val_acc:0.911]
Epoch [27/120    avg_loss:0.296, val_acc:0.913]
Epoch [28/120    avg_loss:0.279, val_acc:0.944]
Epoch [29/120    avg_loss:0.260, val_acc:0.923]
Epoch [30/120    avg_loss:0.232, val_acc:0.921]
Epoch [31/120    avg_loss:0.229, val_acc:0.885]
Epoch [32/120    avg_loss:0.224, val_acc:0.952]
Epoch [33/120    avg_loss:0.213, val_acc:0.899]
Epoch [34/120    avg_loss:0.179, val_acc:0.956]
Epoch [35/120    avg_loss:0.222, val_acc:0.950]
Epoch [36/120    avg_loss:0.257, val_acc:0.927]
Epoch [37/120    avg_loss:0.181, val_acc:0.946]
Epoch [38/120    avg_loss:0.166, val_acc:0.962]
Epoch [39/120    avg_loss:0.135, val_acc:0.942]
Epoch [40/120    avg_loss:0.130, val_acc:0.956]
Epoch [41/120    avg_loss:0.161, val_acc:0.950]
Epoch [42/120    avg_loss:0.185, val_acc:0.962]
Epoch [43/120    avg_loss:0.086, val_acc:0.962]
Epoch [44/120    avg_loss:0.094, val_acc:0.956]
Epoch [45/120    avg_loss:0.098, val_acc:0.948]
Epoch [46/120    avg_loss:0.114, val_acc:0.964]
Epoch [47/120    avg_loss:0.096, val_acc:0.954]
Epoch [48/120    avg_loss:0.127, val_acc:0.952]
Epoch [49/120    avg_loss:0.075, val_acc:0.956]
Epoch [50/120    avg_loss:0.081, val_acc:0.950]
Epoch [51/120    avg_loss:0.130, val_acc:0.952]
Epoch [52/120    avg_loss:0.182, val_acc:0.917]
Epoch [53/120    avg_loss:0.161, val_acc:0.958]
Epoch [54/120    avg_loss:0.120, val_acc:0.960]
Epoch [55/120    avg_loss:0.100, val_acc:0.938]
Epoch [56/120    avg_loss:0.072, val_acc:0.962]
Epoch [57/120    avg_loss:0.105, val_acc:0.946]
Epoch [58/120    avg_loss:0.070, val_acc:0.970]
Epoch [59/120    avg_loss:0.069, val_acc:0.944]
Epoch [60/120    avg_loss:0.079, val_acc:0.962]
Epoch [61/120    avg_loss:0.079, val_acc:0.956]
Epoch [62/120    avg_loss:0.052, val_acc:0.960]
Epoch [63/120    avg_loss:0.049, val_acc:0.970]
Epoch [64/120    avg_loss:0.044, val_acc:0.960]
Epoch [65/120    avg_loss:0.037, val_acc:0.966]
Epoch [66/120    avg_loss:0.090, val_acc:0.964]
Epoch [67/120    avg_loss:0.043, val_acc:0.966]
Epoch [68/120    avg_loss:0.058, val_acc:0.960]
Epoch [69/120    avg_loss:0.066, val_acc:0.964]
Epoch [70/120    avg_loss:0.057, val_acc:0.966]
Epoch [71/120    avg_loss:0.086, val_acc:0.944]
Epoch [72/120    avg_loss:0.108, val_acc:0.960]
Epoch [73/120    avg_loss:0.050, val_acc:0.960]
Epoch [74/120    avg_loss:0.045, val_acc:0.962]
Epoch [75/120    avg_loss:0.047, val_acc:0.948]
Epoch [76/120    avg_loss:0.036, val_acc:0.972]
Epoch [77/120    avg_loss:0.037, val_acc:0.974]
Epoch [78/120    avg_loss:0.022, val_acc:0.964]
Epoch [79/120    avg_loss:0.033, val_acc:0.974]
Epoch [80/120    avg_loss:0.015, val_acc:0.974]
Epoch [81/120    avg_loss:0.019, val_acc:0.976]
Epoch [82/120    avg_loss:0.017, val_acc:0.970]
Epoch [83/120    avg_loss:0.016, val_acc:0.964]
Epoch [84/120    avg_loss:0.012, val_acc:0.972]
Epoch [85/120    avg_loss:0.013, val_acc:0.970]
Epoch [86/120    avg_loss:0.017, val_acc:0.966]
Epoch [87/120    avg_loss:0.027, val_acc:0.970]
Epoch [88/120    avg_loss:0.027, val_acc:0.964]
Epoch [89/120    avg_loss:0.021, val_acc:0.976]
Epoch [90/120    avg_loss:0.050, val_acc:0.964]
Epoch [91/120    avg_loss:0.058, val_acc:0.966]
Epoch [92/120    avg_loss:0.022, val_acc:0.978]
Epoch [93/120    avg_loss:0.011, val_acc:0.976]
Epoch [94/120    avg_loss:0.014, val_acc:0.972]
Epoch [95/120    avg_loss:0.016, val_acc:0.972]
Epoch [96/120    avg_loss:0.009, val_acc:0.972]
Epoch [97/120    avg_loss:0.013, val_acc:0.968]
Epoch [98/120    avg_loss:0.010, val_acc:0.968]
Epoch [99/120    avg_loss:0.010, val_acc:0.970]
Epoch [100/120    avg_loss:0.009, val_acc:0.972]
Epoch [101/120    avg_loss:0.006, val_acc:0.976]
Epoch [102/120    avg_loss:0.007, val_acc:0.976]
Epoch [103/120    avg_loss:0.009, val_acc:0.976]
Epoch [104/120    avg_loss:0.011, val_acc:0.972]
Epoch [105/120    avg_loss:0.021, val_acc:0.946]
Epoch [106/120    avg_loss:0.020, val_acc:0.966]
Epoch [107/120    avg_loss:0.014, val_acc:0.968]
Epoch [108/120    avg_loss:0.010, val_acc:0.968]
Epoch [109/120    avg_loss:0.008, val_acc:0.970]
Epoch [110/120    avg_loss:0.008, val_acc:0.972]
Epoch [111/120    avg_loss:0.007, val_acc:0.970]
Epoch [112/120    avg_loss:0.012, val_acc:0.970]
Epoch [113/120    avg_loss:0.006, val_acc:0.970]
Epoch [114/120    avg_loss:0.007, val_acc:0.970]
Epoch [115/120    avg_loss:0.010, val_acc:0.974]
Epoch [116/120    avg_loss:0.007, val_acc:0.974]
Epoch [117/120    avg_loss:0.006, val_acc:0.974]
Epoch [118/120    avg_loss:0.008, val_acc:0.974]
Epoch [119/120    avg_loss:0.007, val_acc:0.974]
Epoch [120/120    avg_loss:0.006, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  12   0   0   0   0   0   0   8   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 1.         0.99095023 0.99782135 0.91390728 0.90972222
 0.98771499 0.98378378 1.         1.         1.         1.
 0.99124726 1.        ]

Kappa:
0.989791336166638
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd35af84860>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.303, val_acc:0.560]
Epoch [2/120    avg_loss:1.835, val_acc:0.637]
Epoch [3/120    avg_loss:1.428, val_acc:0.756]
Epoch [4/120    avg_loss:1.119, val_acc:0.833]
Epoch [5/120    avg_loss:0.916, val_acc:0.841]
Epoch [6/120    avg_loss:0.735, val_acc:0.798]
Epoch [7/120    avg_loss:0.685, val_acc:0.827]
Epoch [8/120    avg_loss:0.694, val_acc:0.899]
Epoch [9/120    avg_loss:0.553, val_acc:0.891]
Epoch [10/120    avg_loss:0.589, val_acc:0.893]
Epoch [11/120    avg_loss:0.557, val_acc:0.903]
Epoch [12/120    avg_loss:0.421, val_acc:0.915]
Epoch [13/120    avg_loss:0.385, val_acc:0.929]
Epoch [14/120    avg_loss:0.408, val_acc:0.909]
Epoch [15/120    avg_loss:0.396, val_acc:0.925]
Epoch [16/120    avg_loss:0.340, val_acc:0.917]
Epoch [17/120    avg_loss:0.326, val_acc:0.909]
Epoch [18/120    avg_loss:0.347, val_acc:0.938]
Epoch [19/120    avg_loss:0.338, val_acc:0.925]
Epoch [20/120    avg_loss:0.319, val_acc:0.909]
Epoch [21/120    avg_loss:0.340, val_acc:0.875]
Epoch [22/120    avg_loss:0.338, val_acc:0.927]
Epoch [23/120    avg_loss:0.306, val_acc:0.938]
Epoch [24/120    avg_loss:0.242, val_acc:0.938]
Epoch [25/120    avg_loss:0.357, val_acc:0.925]
Epoch [26/120    avg_loss:0.321, val_acc:0.935]
Epoch [27/120    avg_loss:0.257, val_acc:0.954]
Epoch [28/120    avg_loss:0.208, val_acc:0.956]
Epoch [29/120    avg_loss:0.248, val_acc:0.948]
Epoch [30/120    avg_loss:0.240, val_acc:0.921]
Epoch [31/120    avg_loss:0.245, val_acc:0.952]
Epoch [32/120    avg_loss:0.194, val_acc:0.940]
Epoch [33/120    avg_loss:0.161, val_acc:0.948]
Epoch [34/120    avg_loss:0.260, val_acc:0.950]
Epoch [35/120    avg_loss:0.185, val_acc:0.962]
Epoch [36/120    avg_loss:0.168, val_acc:0.940]
Epoch [37/120    avg_loss:0.203, val_acc:0.927]
Epoch [38/120    avg_loss:0.195, val_acc:0.964]
Epoch [39/120    avg_loss:0.206, val_acc:0.948]
Epoch [40/120    avg_loss:0.149, val_acc:0.964]
Epoch [41/120    avg_loss:0.153, val_acc:0.917]
Epoch [42/120    avg_loss:0.164, val_acc:0.958]
Epoch [43/120    avg_loss:0.135, val_acc:0.966]
Epoch [44/120    avg_loss:0.171, val_acc:0.962]
Epoch [45/120    avg_loss:0.180, val_acc:0.976]
Epoch [46/120    avg_loss:0.157, val_acc:0.954]
Epoch [47/120    avg_loss:0.106, val_acc:0.976]
Epoch [48/120    avg_loss:0.153, val_acc:0.966]
Epoch [49/120    avg_loss:0.142, val_acc:0.962]
Epoch [50/120    avg_loss:0.105, val_acc:0.946]
Epoch [51/120    avg_loss:0.157, val_acc:0.964]
Epoch [52/120    avg_loss:0.126, val_acc:0.954]
Epoch [53/120    avg_loss:0.140, val_acc:0.966]
Epoch [54/120    avg_loss:0.079, val_acc:0.952]
Epoch [55/120    avg_loss:0.055, val_acc:0.970]
Epoch [56/120    avg_loss:0.103, val_acc:0.966]
Epoch [57/120    avg_loss:0.152, val_acc:0.972]
Epoch [58/120    avg_loss:0.074, val_acc:0.968]
Epoch [59/120    avg_loss:0.089, val_acc:0.972]
Epoch [60/120    avg_loss:0.097, val_acc:0.956]
Epoch [61/120    avg_loss:0.098, val_acc:0.970]
Epoch [62/120    avg_loss:0.052, val_acc:0.976]
Epoch [63/120    avg_loss:0.061, val_acc:0.978]
Epoch [64/120    avg_loss:0.046, val_acc:0.978]
Epoch [65/120    avg_loss:0.038, val_acc:0.980]
Epoch [66/120    avg_loss:0.047, val_acc:0.982]
Epoch [67/120    avg_loss:0.050, val_acc:0.980]
Epoch [68/120    avg_loss:0.040, val_acc:0.980]
Epoch [69/120    avg_loss:0.046, val_acc:0.982]
Epoch [70/120    avg_loss:0.036, val_acc:0.980]
Epoch [71/120    avg_loss:0.034, val_acc:0.980]
Epoch [72/120    avg_loss:0.038, val_acc:0.980]
Epoch [73/120    avg_loss:0.031, val_acc:0.982]
Epoch [74/120    avg_loss:0.027, val_acc:0.982]
Epoch [75/120    avg_loss:0.028, val_acc:0.982]
Epoch [76/120    avg_loss:0.033, val_acc:0.982]
Epoch [77/120    avg_loss:0.030, val_acc:0.978]
Epoch [78/120    avg_loss:0.028, val_acc:0.982]
Epoch [79/120    avg_loss:0.033, val_acc:0.978]
Epoch [80/120    avg_loss:0.032, val_acc:0.980]
Epoch [81/120    avg_loss:0.028, val_acc:0.980]
Epoch [82/120    avg_loss:0.029, val_acc:0.978]
Epoch [83/120    avg_loss:0.034, val_acc:0.978]
Epoch [84/120    avg_loss:0.031, val_acc:0.984]
Epoch [85/120    avg_loss:0.028, val_acc:0.978]
Epoch [86/120    avg_loss:0.028, val_acc:0.982]
Epoch [87/120    avg_loss:0.031, val_acc:0.982]
Epoch [88/120    avg_loss:0.025, val_acc:0.980]
Epoch [89/120    avg_loss:0.028, val_acc:0.982]
Epoch [90/120    avg_loss:0.023, val_acc:0.982]
Epoch [91/120    avg_loss:0.028, val_acc:0.984]
Epoch [92/120    avg_loss:0.030, val_acc:0.982]
Epoch [93/120    avg_loss:0.023, val_acc:0.982]
Epoch [94/120    avg_loss:0.027, val_acc:0.978]
Epoch [95/120    avg_loss:0.024, val_acc:0.982]
Epoch [96/120    avg_loss:0.037, val_acc:0.978]
Epoch [97/120    avg_loss:0.031, val_acc:0.980]
Epoch [98/120    avg_loss:0.026, val_acc:0.980]
Epoch [99/120    avg_loss:0.023, val_acc:0.982]
Epoch [100/120    avg_loss:0.026, val_acc:0.980]
Epoch [101/120    avg_loss:0.030, val_acc:0.986]
Epoch [102/120    avg_loss:0.028, val_acc:0.986]
Epoch [103/120    avg_loss:0.025, val_acc:0.984]
Epoch [104/120    avg_loss:0.022, val_acc:0.984]
Epoch [105/120    avg_loss:0.021, val_acc:0.984]
Epoch [106/120    avg_loss:0.021, val_acc:0.982]
Epoch [107/120    avg_loss:0.029, val_acc:0.982]
Epoch [108/120    avg_loss:0.022, val_acc:0.980]
Epoch [109/120    avg_loss:0.022, val_acc:0.980]
Epoch [110/120    avg_loss:0.023, val_acc:0.982]
Epoch [111/120    avg_loss:0.025, val_acc:0.982]
Epoch [112/120    avg_loss:0.024, val_acc:0.982]
Epoch [113/120    avg_loss:0.021, val_acc:0.982]
Epoch [114/120    avg_loss:0.031, val_acc:0.988]
Epoch [115/120    avg_loss:0.026, val_acc:0.986]
Epoch [116/120    avg_loss:0.019, val_acc:0.986]
Epoch [117/120    avg_loss:0.026, val_acc:0.988]
Epoch [118/120    avg_loss:0.021, val_acc:0.986]
Epoch [119/120    avg_loss:0.021, val_acc:0.986]
Epoch [120/120    avg_loss:0.023, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   1   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 209  13   0   0   0   0   0   0   5   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   1 467   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.95522388059702

F1 scores:
[       nan 1.         0.98206278 0.99563319 0.91466083 0.8989547
 0.99019608 0.95555556 0.99742931 0.99893048 1.         1.
 0.99451153 1.        ]

Kappa:
0.9883668160910608
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0f38d08908>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.319, val_acc:0.556]
Epoch [2/120    avg_loss:1.800, val_acc:0.629]
Epoch [3/120    avg_loss:1.403, val_acc:0.704]
Epoch [4/120    avg_loss:1.112, val_acc:0.776]
Epoch [5/120    avg_loss:1.006, val_acc:0.790]
Epoch [6/120    avg_loss:0.884, val_acc:0.815]
Epoch [7/120    avg_loss:0.741, val_acc:0.829]
Epoch [8/120    avg_loss:0.685, val_acc:0.885]
Epoch [9/120    avg_loss:0.672, val_acc:0.863]
Epoch [10/120    avg_loss:0.617, val_acc:0.883]
Epoch [11/120    avg_loss:0.556, val_acc:0.827]
Epoch [12/120    avg_loss:0.476, val_acc:0.857]
Epoch [13/120    avg_loss:0.596, val_acc:0.893]
Epoch [14/120    avg_loss:0.482, val_acc:0.875]
Epoch [15/120    avg_loss:0.432, val_acc:0.905]
Epoch [16/120    avg_loss:0.445, val_acc:0.931]
Epoch [17/120    avg_loss:0.383, val_acc:0.927]
Epoch [18/120    avg_loss:0.336, val_acc:0.923]
Epoch [19/120    avg_loss:0.342, val_acc:0.927]
Epoch [20/120    avg_loss:0.321, val_acc:0.954]
Epoch [21/120    avg_loss:0.337, val_acc:0.970]
Epoch [22/120    avg_loss:0.241, val_acc:0.964]
Epoch [23/120    avg_loss:0.347, val_acc:0.909]
Epoch [24/120    avg_loss:0.264, val_acc:0.944]
Epoch [25/120    avg_loss:0.254, val_acc:0.948]
Epoch [26/120    avg_loss:0.242, val_acc:0.937]
Epoch [27/120    avg_loss:0.220, val_acc:0.956]
Epoch [28/120    avg_loss:0.244, val_acc:0.942]
Epoch [29/120    avg_loss:0.239, val_acc:0.940]
Epoch [30/120    avg_loss:0.237, val_acc:0.978]
Epoch [31/120    avg_loss:0.181, val_acc:0.958]
Epoch [32/120    avg_loss:0.185, val_acc:0.952]
Epoch [33/120    avg_loss:0.152, val_acc:0.960]
Epoch [34/120    avg_loss:0.202, val_acc:0.948]
Epoch [35/120    avg_loss:0.185, val_acc:0.964]
Epoch [36/120    avg_loss:0.224, val_acc:0.772]
Epoch [37/120    avg_loss:0.279, val_acc:0.946]
Epoch [38/120    avg_loss:0.143, val_acc:0.964]
Epoch [39/120    avg_loss:0.129, val_acc:0.976]
Epoch [40/120    avg_loss:0.152, val_acc:0.980]
Epoch [41/120    avg_loss:0.136, val_acc:0.974]
Epoch [42/120    avg_loss:0.181, val_acc:0.970]
Epoch [43/120    avg_loss:0.139, val_acc:0.968]
Epoch [44/120    avg_loss:0.117, val_acc:0.974]
Epoch [45/120    avg_loss:0.098, val_acc:0.990]
Epoch [46/120    avg_loss:0.067, val_acc:0.986]
Epoch [47/120    avg_loss:0.094, val_acc:0.978]
Epoch [48/120    avg_loss:0.079, val_acc:0.990]
Epoch [49/120    avg_loss:0.085, val_acc:0.944]
Epoch [50/120    avg_loss:0.098, val_acc:0.982]
Epoch [51/120    avg_loss:0.042, val_acc:0.986]
Epoch [52/120    avg_loss:0.050, val_acc:0.984]
Epoch [53/120    avg_loss:0.039, val_acc:0.996]
Epoch [54/120    avg_loss:0.108, val_acc:0.986]
Epoch [55/120    avg_loss:0.050, val_acc:0.996]
Epoch [56/120    avg_loss:0.048, val_acc:0.980]
Epoch [57/120    avg_loss:0.056, val_acc:0.994]
Epoch [58/120    avg_loss:0.041, val_acc:0.992]
Epoch [59/120    avg_loss:0.053, val_acc:0.962]
Epoch [60/120    avg_loss:0.051, val_acc:0.994]
Epoch [61/120    avg_loss:0.099, val_acc:0.986]
Epoch [62/120    avg_loss:0.037, val_acc:0.994]
Epoch [63/120    avg_loss:0.102, val_acc:0.988]
Epoch [64/120    avg_loss:0.057, val_acc:0.988]
Epoch [65/120    avg_loss:0.044, val_acc:0.992]
Epoch [66/120    avg_loss:0.035, val_acc:0.992]
Epoch [67/120    avg_loss:0.033, val_acc:0.990]
Epoch [68/120    avg_loss:0.051, val_acc:0.994]
Epoch [69/120    avg_loss:0.023, val_acc:0.998]
Epoch [70/120    avg_loss:0.015, val_acc:0.998]
Epoch [71/120    avg_loss:0.018, val_acc:0.998]
Epoch [72/120    avg_loss:0.015, val_acc:0.998]
Epoch [73/120    avg_loss:0.019, val_acc:0.998]
Epoch [74/120    avg_loss:0.016, val_acc:0.998]
Epoch [75/120    avg_loss:0.014, val_acc:0.998]
Epoch [76/120    avg_loss:0.018, val_acc:0.998]
Epoch [77/120    avg_loss:0.015, val_acc:0.998]
Epoch [78/120    avg_loss:0.012, val_acc:0.998]
Epoch [79/120    avg_loss:0.014, val_acc:0.998]
Epoch [80/120    avg_loss:0.018, val_acc:0.998]
Epoch [81/120    avg_loss:0.016, val_acc:0.998]
Epoch [82/120    avg_loss:0.015, val_acc:0.998]
Epoch [83/120    avg_loss:0.018, val_acc:0.998]
Epoch [84/120    avg_loss:0.019, val_acc:0.998]
Epoch [85/120    avg_loss:0.020, val_acc:0.998]
Epoch [86/120    avg_loss:0.017, val_acc:0.998]
Epoch [87/120    avg_loss:0.017, val_acc:0.998]
Epoch [88/120    avg_loss:0.018, val_acc:0.998]
Epoch [89/120    avg_loss:0.010, val_acc:0.998]
Epoch [90/120    avg_loss:0.016, val_acc:0.998]
Epoch [91/120    avg_loss:0.012, val_acc:0.998]
Epoch [92/120    avg_loss:0.012, val_acc:0.998]
Epoch [93/120    avg_loss:0.015, val_acc:0.998]
Epoch [94/120    avg_loss:0.013, val_acc:0.998]
Epoch [95/120    avg_loss:0.011, val_acc:0.998]
Epoch [96/120    avg_loss:0.013, val_acc:0.998]
Epoch [97/120    avg_loss:0.011, val_acc:0.998]
Epoch [98/120    avg_loss:0.012, val_acc:0.998]
Epoch [99/120    avg_loss:0.013, val_acc:0.998]
Epoch [100/120    avg_loss:0.020, val_acc:0.998]
Epoch [101/120    avg_loss:0.014, val_acc:0.998]
Epoch [102/120    avg_loss:0.010, val_acc:0.998]
Epoch [103/120    avg_loss:0.012, val_acc:0.998]
Epoch [104/120    avg_loss:0.012, val_acc:0.998]
Epoch [105/120    avg_loss:0.012, val_acc:0.998]
Epoch [106/120    avg_loss:0.013, val_acc:0.998]
Epoch [107/120    avg_loss:0.012, val_acc:0.998]
Epoch [108/120    avg_loss:0.011, val_acc:0.998]
Epoch [109/120    avg_loss:0.012, val_acc:0.998]
Epoch [110/120    avg_loss:0.010, val_acc:0.998]
Epoch [111/120    avg_loss:0.012, val_acc:0.998]
Epoch [112/120    avg_loss:0.009, val_acc:0.998]
Epoch [113/120    avg_loss:0.015, val_acc:0.998]
Epoch [114/120    avg_loss:0.010, val_acc:0.998]
Epoch [115/120    avg_loss:0.012, val_acc:0.998]
Epoch [116/120    avg_loss:0.012, val_acc:0.998]
Epoch [117/120    avg_loss:0.012, val_acc:0.998]
Epoch [118/120    avg_loss:0.017, val_acc:0.998]
Epoch [119/120    avg_loss:0.009, val_acc:0.998]
Epoch [120/120    avg_loss:0.012, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 1.         0.98871332 0.99782135 0.96086957 0.94405594
 0.99756691 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9945399581975515
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:03:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f72c7f5a400>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.307, val_acc:0.619]
Epoch [2/120    avg_loss:1.763, val_acc:0.601]
Epoch [3/120    avg_loss:1.365, val_acc:0.667]
Epoch [4/120    avg_loss:1.095, val_acc:0.790]
Epoch [5/120    avg_loss:0.939, val_acc:0.831]
Epoch [6/120    avg_loss:0.776, val_acc:0.819]
Epoch [7/120    avg_loss:0.741, val_acc:0.827]
Epoch [8/120    avg_loss:0.680, val_acc:0.827]
Epoch [9/120    avg_loss:0.589, val_acc:0.901]
Epoch [10/120    avg_loss:0.526, val_acc:0.863]
Epoch [11/120    avg_loss:0.551, val_acc:0.790]
Epoch [12/120    avg_loss:0.517, val_acc:0.875]
Epoch [13/120    avg_loss:0.517, val_acc:0.889]
Epoch [14/120    avg_loss:0.474, val_acc:0.849]
Epoch [15/120    avg_loss:0.447, val_acc:0.879]
Epoch [16/120    avg_loss:0.453, val_acc:0.885]
Epoch [17/120    avg_loss:0.437, val_acc:0.909]
Epoch [18/120    avg_loss:0.389, val_acc:0.929]
Epoch [19/120    avg_loss:0.335, val_acc:0.915]
Epoch [20/120    avg_loss:0.318, val_acc:0.863]
Epoch [21/120    avg_loss:0.365, val_acc:0.923]
Epoch [22/120    avg_loss:0.316, val_acc:0.942]
Epoch [23/120    avg_loss:0.250, val_acc:0.913]
Epoch [24/120    avg_loss:0.305, val_acc:0.911]
Epoch [25/120    avg_loss:0.306, val_acc:0.905]
Epoch [26/120    avg_loss:0.240, val_acc:0.913]
Epoch [27/120    avg_loss:0.239, val_acc:0.940]
Epoch [28/120    avg_loss:0.318, val_acc:0.837]
Epoch [29/120    avg_loss:0.272, val_acc:0.946]
Epoch [30/120    avg_loss:0.246, val_acc:0.954]
Epoch [31/120    avg_loss:0.242, val_acc:0.935]
Epoch [32/120    avg_loss:0.202, val_acc:0.942]
Epoch [33/120    avg_loss:0.195, val_acc:0.921]
Epoch [34/120    avg_loss:0.303, val_acc:0.935]
Epoch [35/120    avg_loss:0.187, val_acc:0.942]
Epoch [36/120    avg_loss:0.201, val_acc:0.964]
Epoch [37/120    avg_loss:0.241, val_acc:0.921]
Epoch [38/120    avg_loss:0.206, val_acc:0.960]
Epoch [39/120    avg_loss:0.147, val_acc:0.944]
Epoch [40/120    avg_loss:0.153, val_acc:0.964]
Epoch [41/120    avg_loss:0.135, val_acc:0.970]
Epoch [42/120    avg_loss:0.175, val_acc:0.954]
Epoch [43/120    avg_loss:0.126, val_acc:0.956]
Epoch [44/120    avg_loss:0.167, val_acc:0.954]
Epoch [45/120    avg_loss:0.112, val_acc:0.944]
Epoch [46/120    avg_loss:0.123, val_acc:0.958]
Epoch [47/120    avg_loss:0.175, val_acc:0.944]
Epoch [48/120    avg_loss:0.136, val_acc:0.958]
Epoch [49/120    avg_loss:0.154, val_acc:0.974]
Epoch [50/120    avg_loss:0.195, val_acc:0.931]
Epoch [51/120    avg_loss:0.163, val_acc:0.944]
Epoch [52/120    avg_loss:0.142, val_acc:0.954]
Epoch [53/120    avg_loss:0.083, val_acc:0.972]
Epoch [54/120    avg_loss:0.079, val_acc:0.954]
Epoch [55/120    avg_loss:0.159, val_acc:0.960]
Epoch [56/120    avg_loss:0.064, val_acc:0.968]
Epoch [57/120    avg_loss:0.065, val_acc:0.972]
Epoch [58/120    avg_loss:0.109, val_acc:0.974]
Epoch [59/120    avg_loss:0.105, val_acc:0.974]
Epoch [60/120    avg_loss:0.070, val_acc:0.962]
Epoch [61/120    avg_loss:0.060, val_acc:0.962]
Epoch [62/120    avg_loss:0.079, val_acc:0.982]
Epoch [63/120    avg_loss:0.056, val_acc:0.970]
Epoch [64/120    avg_loss:0.061, val_acc:0.964]
Epoch [65/120    avg_loss:0.086, val_acc:0.976]
Epoch [66/120    avg_loss:0.060, val_acc:0.986]
Epoch [67/120    avg_loss:0.139, val_acc:0.956]
Epoch [68/120    avg_loss:0.094, val_acc:0.980]
Epoch [69/120    avg_loss:0.102, val_acc:0.962]
Epoch [70/120    avg_loss:0.053, val_acc:0.952]
Epoch [71/120    avg_loss:0.083, val_acc:0.982]
Epoch [72/120    avg_loss:0.037, val_acc:0.980]
Epoch [73/120    avg_loss:0.037, val_acc:0.982]
Epoch [74/120    avg_loss:0.026, val_acc:0.976]
Epoch [75/120    avg_loss:0.119, val_acc:0.970]
Epoch [76/120    avg_loss:0.090, val_acc:0.972]
Epoch [77/120    avg_loss:0.059, val_acc:0.982]
Epoch [78/120    avg_loss:0.034, val_acc:0.978]
Epoch [79/120    avg_loss:0.026, val_acc:0.980]
Epoch [80/120    avg_loss:0.032, val_acc:0.986]
Epoch [81/120    avg_loss:0.021, val_acc:0.984]
Epoch [82/120    avg_loss:0.018, val_acc:0.986]
Epoch [83/120    avg_loss:0.017, val_acc:0.986]
Epoch [84/120    avg_loss:0.015, val_acc:0.986]
Epoch [85/120    avg_loss:0.016, val_acc:0.986]
Epoch [86/120    avg_loss:0.020, val_acc:0.986]
Epoch [87/120    avg_loss:0.014, val_acc:0.986]
Epoch [88/120    avg_loss:0.015, val_acc:0.986]
Epoch [89/120    avg_loss:0.016, val_acc:0.986]
Epoch [90/120    avg_loss:0.014, val_acc:0.986]
Epoch [91/120    avg_loss:0.014, val_acc:0.986]
Epoch [92/120    avg_loss:0.017, val_acc:0.986]
Epoch [93/120    avg_loss:0.015, val_acc:0.986]
Epoch [94/120    avg_loss:0.014, val_acc:0.986]
Epoch [95/120    avg_loss:0.015, val_acc:0.986]
Epoch [96/120    avg_loss:0.013, val_acc:0.986]
Epoch [97/120    avg_loss:0.016, val_acc:0.986]
Epoch [98/120    avg_loss:0.013, val_acc:0.986]
Epoch [99/120    avg_loss:0.014, val_acc:0.986]
Epoch [100/120    avg_loss:0.014, val_acc:0.986]
Epoch [101/120    avg_loss:0.012, val_acc:0.986]
Epoch [102/120    avg_loss:0.016, val_acc:0.986]
Epoch [103/120    avg_loss:0.013, val_acc:0.986]
Epoch [104/120    avg_loss:0.011, val_acc:0.988]
Epoch [105/120    avg_loss:0.013, val_acc:0.988]
Epoch [106/120    avg_loss:0.014, val_acc:0.988]
Epoch [107/120    avg_loss:0.012, val_acc:0.988]
Epoch [108/120    avg_loss:0.012, val_acc:0.988]
Epoch [109/120    avg_loss:0.013, val_acc:0.986]
Epoch [110/120    avg_loss:0.012, val_acc:0.986]
Epoch [111/120    avg_loss:0.012, val_acc:0.986]
Epoch [112/120    avg_loss:0.013, val_acc:0.986]
Epoch [113/120    avg_loss:0.011, val_acc:0.986]
Epoch [114/120    avg_loss:0.012, val_acc:0.986]
Epoch [115/120    avg_loss:0.014, val_acc:0.986]
Epoch [116/120    avg_loss:0.016, val_acc:0.986]
Epoch [117/120    avg_loss:0.010, val_acc:0.986]
Epoch [118/120    avg_loss:0.012, val_acc:0.986]
Epoch [119/120    avg_loss:0.010, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   0   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   1   1 141   2   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   7 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 1.         0.98648649 0.99346405 0.97065463 0.94630872
 0.99516908 0.96703297 0.998713   0.99893276 1.         0.98947368
 0.99111111 1.        ]

Kappa:
0.9924037953691394
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1d4948d898>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.327, val_acc:0.514]
Epoch [2/120    avg_loss:1.879, val_acc:0.542]
Epoch [3/120    avg_loss:1.474, val_acc:0.663]
Epoch [4/120    avg_loss:1.161, val_acc:0.770]
Epoch [5/120    avg_loss:0.974, val_acc:0.837]
Epoch [6/120    avg_loss:0.857, val_acc:0.823]
Epoch [7/120    avg_loss:0.716, val_acc:0.786]
Epoch [8/120    avg_loss:0.632, val_acc:0.877]
Epoch [9/120    avg_loss:0.610, val_acc:0.891]
Epoch [10/120    avg_loss:0.579, val_acc:0.889]
Epoch [11/120    avg_loss:0.532, val_acc:0.887]
Epoch [12/120    avg_loss:0.467, val_acc:0.899]
Epoch [13/120    avg_loss:0.430, val_acc:0.887]
Epoch [14/120    avg_loss:0.445, val_acc:0.879]
Epoch [15/120    avg_loss:0.368, val_acc:0.879]
Epoch [16/120    avg_loss:0.504, val_acc:0.913]
Epoch [17/120    avg_loss:0.443, val_acc:0.891]
Epoch [18/120    avg_loss:0.379, val_acc:0.915]
Epoch [19/120    avg_loss:0.423, val_acc:0.923]
Epoch [20/120    avg_loss:0.306, val_acc:0.905]
Epoch [21/120    avg_loss:0.334, val_acc:0.909]
Epoch [22/120    avg_loss:0.283, val_acc:0.925]
Epoch [23/120    avg_loss:0.268, val_acc:0.899]
Epoch [24/120    avg_loss:0.375, val_acc:0.929]
Epoch [25/120    avg_loss:0.262, val_acc:0.946]
Epoch [26/120    avg_loss:0.250, val_acc:0.940]
Epoch [27/120    avg_loss:0.244, val_acc:0.940]
Epoch [28/120    avg_loss:0.321, val_acc:0.927]
Epoch [29/120    avg_loss:0.228, val_acc:0.919]
Epoch [30/120    avg_loss:0.235, val_acc:0.935]
Epoch [31/120    avg_loss:0.222, val_acc:0.958]
Epoch [32/120    avg_loss:0.169, val_acc:0.962]
Epoch [33/120    avg_loss:0.194, val_acc:0.950]
Epoch [34/120    avg_loss:0.210, val_acc:0.960]
Epoch [35/120    avg_loss:0.152, val_acc:0.948]
Epoch [36/120    avg_loss:0.209, val_acc:0.964]
Epoch [37/120    avg_loss:0.124, val_acc:0.942]
Epoch [38/120    avg_loss:0.173, val_acc:0.940]
Epoch [39/120    avg_loss:0.137, val_acc:0.964]
Epoch [40/120    avg_loss:0.154, val_acc:0.946]
Epoch [41/120    avg_loss:0.178, val_acc:0.952]
Epoch [42/120    avg_loss:0.173, val_acc:0.935]
Epoch [43/120    avg_loss:0.141, val_acc:0.972]
Epoch [44/120    avg_loss:0.162, val_acc:0.938]
Epoch [45/120    avg_loss:0.166, val_acc:0.964]
Epoch [46/120    avg_loss:0.167, val_acc:0.952]
Epoch [47/120    avg_loss:0.110, val_acc:0.962]
Epoch [48/120    avg_loss:0.154, val_acc:0.931]
Epoch [49/120    avg_loss:0.159, val_acc:0.935]
Epoch [50/120    avg_loss:0.133, val_acc:0.962]
Epoch [51/120    avg_loss:0.151, val_acc:0.962]
Epoch [52/120    avg_loss:0.132, val_acc:0.964]
Epoch [53/120    avg_loss:0.094, val_acc:0.964]
Epoch [54/120    avg_loss:0.088, val_acc:0.968]
Epoch [55/120    avg_loss:0.095, val_acc:0.972]
Epoch [56/120    avg_loss:0.152, val_acc:0.935]
Epoch [57/120    avg_loss:0.134, val_acc:0.964]
Epoch [58/120    avg_loss:0.066, val_acc:0.970]
Epoch [59/120    avg_loss:0.109, val_acc:0.964]
Epoch [60/120    avg_loss:0.061, val_acc:0.970]
Epoch [61/120    avg_loss:0.058, val_acc:0.972]
Epoch [62/120    avg_loss:0.077, val_acc:0.972]
Epoch [63/120    avg_loss:0.052, val_acc:0.966]
Epoch [64/120    avg_loss:0.066, val_acc:0.970]
Epoch [65/120    avg_loss:0.056, val_acc:0.980]
Epoch [66/120    avg_loss:0.060, val_acc:0.972]
Epoch [67/120    avg_loss:0.056, val_acc:0.974]
Epoch [68/120    avg_loss:0.074, val_acc:0.976]
Epoch [69/120    avg_loss:0.066, val_acc:0.960]
Epoch [70/120    avg_loss:0.077, val_acc:0.976]
Epoch [71/120    avg_loss:0.066, val_acc:0.964]
Epoch [72/120    avg_loss:0.059, val_acc:0.974]
Epoch [73/120    avg_loss:0.041, val_acc:0.978]
Epoch [74/120    avg_loss:0.036, val_acc:0.976]
Epoch [75/120    avg_loss:0.044, val_acc:0.968]
Epoch [76/120    avg_loss:0.043, val_acc:0.972]
Epoch [77/120    avg_loss:0.058, val_acc:0.984]
Epoch [78/120    avg_loss:0.056, val_acc:0.976]
Epoch [79/120    avg_loss:0.021, val_acc:0.980]
Epoch [80/120    avg_loss:0.020, val_acc:0.982]
Epoch [81/120    avg_loss:0.063, val_acc:0.966]
Epoch [82/120    avg_loss:0.064, val_acc:0.974]
Epoch [83/120    avg_loss:0.062, val_acc:0.958]
Epoch [84/120    avg_loss:0.035, val_acc:0.972]
Epoch [85/120    avg_loss:0.032, val_acc:0.978]
Epoch [86/120    avg_loss:0.036, val_acc:0.984]
Epoch [87/120    avg_loss:0.023, val_acc:0.986]
Epoch [88/120    avg_loss:0.013, val_acc:0.980]
Epoch [89/120    avg_loss:0.013, val_acc:0.986]
Epoch [90/120    avg_loss:0.008, val_acc:0.986]
Epoch [91/120    avg_loss:0.017, val_acc:0.986]
Epoch [92/120    avg_loss:0.014, val_acc:0.984]
Epoch [93/120    avg_loss:0.013, val_acc:0.982]
Epoch [94/120    avg_loss:0.017, val_acc:0.978]
Epoch [95/120    avg_loss:0.018, val_acc:0.980]
Epoch [96/120    avg_loss:0.020, val_acc:0.986]
Epoch [97/120    avg_loss:0.012, val_acc:0.984]
Epoch [98/120    avg_loss:0.010, val_acc:0.984]
Epoch [99/120    avg_loss:0.012, val_acc:0.972]
Epoch [100/120    avg_loss:0.032, val_acc:0.970]
Epoch [101/120    avg_loss:0.038, val_acc:0.976]
Epoch [102/120    avg_loss:0.016, val_acc:0.976]
Epoch [103/120    avg_loss:0.017, val_acc:0.976]
Epoch [104/120    avg_loss:0.014, val_acc:0.986]
Epoch [105/120    avg_loss:0.052, val_acc:0.982]
Epoch [106/120    avg_loss:0.018, val_acc:0.976]
Epoch [107/120    avg_loss:0.018, val_acc:0.984]
Epoch [108/120    avg_loss:0.019, val_acc:0.976]
Epoch [109/120    avg_loss:0.008, val_acc:0.978]
Epoch [110/120    avg_loss:0.017, val_acc:0.980]
Epoch [111/120    avg_loss:0.011, val_acc:0.984]
Epoch [112/120    avg_loss:0.018, val_acc:0.968]
Epoch [113/120    avg_loss:0.016, val_acc:0.982]
Epoch [114/120    avg_loss:0.011, val_acc:0.982]
Epoch [115/120    avg_loss:0.013, val_acc:0.988]
Epoch [116/120    avg_loss:0.019, val_acc:0.980]
Epoch [117/120    avg_loss:0.027, val_acc:0.974]
Epoch [118/120    avg_loss:0.033, val_acc:0.976]
Epoch [119/120    avg_loss:0.018, val_acc:0.984]
Epoch [120/120    avg_loss:0.009, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 225   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  14 439   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 1.         0.98426966 1.         0.96359743 0.93862816
 1.         0.96132597 1.         1.         1.         0.98177083
 0.98430493 1.        ]

Kappa:
0.9909791042911614
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcdd0ea0898>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.271, val_acc:0.542]
Epoch [2/120    avg_loss:1.777, val_acc:0.669]
Epoch [3/120    avg_loss:1.422, val_acc:0.752]
Epoch [4/120    avg_loss:1.120, val_acc:0.784]
Epoch [5/120    avg_loss:0.925, val_acc:0.825]
Epoch [6/120    avg_loss:0.851, val_acc:0.810]
Epoch [7/120    avg_loss:0.730, val_acc:0.855]
Epoch [8/120    avg_loss:0.632, val_acc:0.865]
Epoch [9/120    avg_loss:0.564, val_acc:0.829]
Epoch [10/120    avg_loss:0.546, val_acc:0.804]
Epoch [11/120    avg_loss:0.541, val_acc:0.899]
Epoch [12/120    avg_loss:0.514, val_acc:0.806]
Epoch [13/120    avg_loss:0.415, val_acc:0.879]
Epoch [14/120    avg_loss:0.453, val_acc:0.855]
Epoch [15/120    avg_loss:0.420, val_acc:0.913]
Epoch [16/120    avg_loss:0.337, val_acc:0.935]
Epoch [17/120    avg_loss:0.338, val_acc:0.911]
Epoch [18/120    avg_loss:0.335, val_acc:0.915]
Epoch [19/120    avg_loss:0.401, val_acc:0.938]
Epoch [20/120    avg_loss:0.303, val_acc:0.919]
Epoch [21/120    avg_loss:0.260, val_acc:0.956]
Epoch [22/120    avg_loss:0.371, val_acc:0.909]
Epoch [23/120    avg_loss:0.249, val_acc:0.935]
Epoch [24/120    avg_loss:0.260, val_acc:0.962]
Epoch [25/120    avg_loss:0.240, val_acc:0.944]
Epoch [26/120    avg_loss:0.296, val_acc:0.927]
Epoch [27/120    avg_loss:0.247, val_acc:0.948]
Epoch [28/120    avg_loss:0.200, val_acc:0.950]
Epoch [29/120    avg_loss:0.206, val_acc:0.952]
Epoch [30/120    avg_loss:0.193, val_acc:0.950]
Epoch [31/120    avg_loss:0.200, val_acc:0.960]
Epoch [32/120    avg_loss:0.127, val_acc:0.966]
Epoch [33/120    avg_loss:0.119, val_acc:0.962]
Epoch [34/120    avg_loss:0.139, val_acc:0.974]
Epoch [35/120    avg_loss:0.131, val_acc:0.931]
Epoch [36/120    avg_loss:0.243, val_acc:0.970]
Epoch [37/120    avg_loss:0.180, val_acc:0.940]
Epoch [38/120    avg_loss:0.231, val_acc:0.974]
Epoch [39/120    avg_loss:0.127, val_acc:0.972]
Epoch [40/120    avg_loss:0.157, val_acc:0.956]
Epoch [41/120    avg_loss:0.138, val_acc:0.940]
Epoch [42/120    avg_loss:0.149, val_acc:0.960]
Epoch [43/120    avg_loss:0.161, val_acc:0.968]
Epoch [44/120    avg_loss:0.107, val_acc:0.956]
Epoch [45/120    avg_loss:0.105, val_acc:0.976]
Epoch [46/120    avg_loss:0.138, val_acc:0.966]
Epoch [47/120    avg_loss:0.120, val_acc:0.964]
Epoch [48/120    avg_loss:0.145, val_acc:0.970]
Epoch [49/120    avg_loss:0.085, val_acc:0.968]
Epoch [50/120    avg_loss:0.125, val_acc:0.970]
Epoch [51/120    avg_loss:0.074, val_acc:0.984]
Epoch [52/120    avg_loss:0.059, val_acc:0.996]
Epoch [53/120    avg_loss:0.067, val_acc:0.984]
Epoch [54/120    avg_loss:0.102, val_acc:0.984]
Epoch [55/120    avg_loss:0.088, val_acc:0.990]
Epoch [56/120    avg_loss:0.084, val_acc:0.976]
Epoch [57/120    avg_loss:0.112, val_acc:0.976]
Epoch [58/120    avg_loss:0.079, val_acc:0.988]
Epoch [59/120    avg_loss:0.049, val_acc:0.982]
Epoch [60/120    avg_loss:0.059, val_acc:0.988]
Epoch [61/120    avg_loss:0.067, val_acc:0.990]
Epoch [62/120    avg_loss:0.050, val_acc:0.986]
Epoch [63/120    avg_loss:0.026, val_acc:0.992]
Epoch [64/120    avg_loss:0.031, val_acc:0.986]
Epoch [65/120    avg_loss:0.029, val_acc:0.994]
Epoch [66/120    avg_loss:0.016, val_acc:0.992]
Epoch [67/120    avg_loss:0.020, val_acc:0.996]
Epoch [68/120    avg_loss:0.015, val_acc:0.996]
Epoch [69/120    avg_loss:0.021, val_acc:0.994]
Epoch [70/120    avg_loss:0.017, val_acc:0.992]
Epoch [71/120    avg_loss:0.020, val_acc:0.992]
Epoch [72/120    avg_loss:0.016, val_acc:0.994]
Epoch [73/120    avg_loss:0.017, val_acc:0.994]
Epoch [74/120    avg_loss:0.019, val_acc:0.994]
Epoch [75/120    avg_loss:0.018, val_acc:0.990]
Epoch [76/120    avg_loss:0.014, val_acc:0.990]
Epoch [77/120    avg_loss:0.012, val_acc:0.994]
Epoch [78/120    avg_loss:0.019, val_acc:0.996]
Epoch [79/120    avg_loss:0.015, val_acc:0.996]
Epoch [80/120    avg_loss:0.018, val_acc:0.996]
Epoch [81/120    avg_loss:0.019, val_acc:0.992]
Epoch [82/120    avg_loss:0.021, val_acc:0.992]
Epoch [83/120    avg_loss:0.015, val_acc:0.992]
Epoch [84/120    avg_loss:0.017, val_acc:0.992]
Epoch [85/120    avg_loss:0.018, val_acc:0.994]
Epoch [86/120    avg_loss:0.031, val_acc:0.994]
Epoch [87/120    avg_loss:0.011, val_acc:0.994]
Epoch [88/120    avg_loss:0.015, val_acc:0.994]
Epoch [89/120    avg_loss:0.018, val_acc:0.992]
Epoch [90/120    avg_loss:0.014, val_acc:0.994]
Epoch [91/120    avg_loss:0.013, val_acc:0.996]
Epoch [92/120    avg_loss:0.014, val_acc:0.998]
Epoch [93/120    avg_loss:0.013, val_acc:0.994]
Epoch [94/120    avg_loss:0.013, val_acc:0.994]
Epoch [95/120    avg_loss:0.018, val_acc:0.992]
Epoch [96/120    avg_loss:0.017, val_acc:0.996]
Epoch [97/120    avg_loss:0.013, val_acc:0.996]
Epoch [98/120    avg_loss:0.018, val_acc:0.994]
Epoch [99/120    avg_loss:0.013, val_acc:0.994]
Epoch [100/120    avg_loss:0.015, val_acc:0.994]
Epoch [101/120    avg_loss:0.017, val_acc:0.994]
Epoch [102/120    avg_loss:0.015, val_acc:0.994]
Epoch [103/120    avg_loss:0.013, val_acc:0.996]
Epoch [104/120    avg_loss:0.010, val_acc:0.996]
Epoch [105/120    avg_loss:0.015, val_acc:0.998]
Epoch [106/120    avg_loss:0.011, val_acc:0.998]
Epoch [107/120    avg_loss:0.013, val_acc:0.998]
Epoch [108/120    avg_loss:0.012, val_acc:0.996]
Epoch [109/120    avg_loss:0.012, val_acc:0.998]
Epoch [110/120    avg_loss:0.013, val_acc:0.998]
Epoch [111/120    avg_loss:0.011, val_acc:0.998]
Epoch [112/120    avg_loss:0.011, val_acc:0.994]
Epoch [113/120    avg_loss:0.015, val_acc:0.994]
Epoch [114/120    avg_loss:0.015, val_acc:0.998]
Epoch [115/120    avg_loss:0.013, val_acc:0.996]
Epoch [116/120    avg_loss:0.010, val_acc:0.996]
Epoch [117/120    avg_loss:0.017, val_acc:0.996]
Epoch [118/120    avg_loss:0.011, val_acc:0.996]
Epoch [119/120    avg_loss:0.013, val_acc:0.996]
Epoch [120/120    avg_loss:0.012, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   1 214  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 1.         0.99545455 0.99565217 0.9532294  0.93197279
 1.         0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9945402848476177
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6a0a9fb8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.335, val_acc:0.522]
Epoch [2/120    avg_loss:1.776, val_acc:0.647]
Epoch [3/120    avg_loss:1.386, val_acc:0.685]
Epoch [4/120    avg_loss:1.174, val_acc:0.788]
Epoch [5/120    avg_loss:1.082, val_acc:0.673]
Epoch [6/120    avg_loss:0.936, val_acc:0.740]
Epoch [7/120    avg_loss:0.836, val_acc:0.823]
Epoch [8/120    avg_loss:0.697, val_acc:0.772]
Epoch [9/120    avg_loss:0.617, val_acc:0.881]
Epoch [10/120    avg_loss:0.665, val_acc:0.829]
Epoch [11/120    avg_loss:0.524, val_acc:0.913]
Epoch [12/120    avg_loss:0.528, val_acc:0.810]
Epoch [13/120    avg_loss:0.502, val_acc:0.883]
Epoch [14/120    avg_loss:0.549, val_acc:0.879]
Epoch [15/120    avg_loss:0.418, val_acc:0.915]
Epoch [16/120    avg_loss:0.530, val_acc:0.861]
Epoch [17/120    avg_loss:0.410, val_acc:0.917]
Epoch [18/120    avg_loss:0.376, val_acc:0.909]
Epoch [19/120    avg_loss:0.375, val_acc:0.913]
Epoch [20/120    avg_loss:0.391, val_acc:0.903]
Epoch [21/120    avg_loss:0.364, val_acc:0.877]
Epoch [22/120    avg_loss:0.378, val_acc:0.919]
Epoch [23/120    avg_loss:0.371, val_acc:0.887]
Epoch [24/120    avg_loss:0.367, val_acc:0.905]
Epoch [25/120    avg_loss:0.306, val_acc:0.919]
Epoch [26/120    avg_loss:0.295, val_acc:0.937]
Epoch [27/120    avg_loss:0.320, val_acc:0.923]
Epoch [28/120    avg_loss:0.283, val_acc:0.921]
Epoch [29/120    avg_loss:0.311, val_acc:0.913]
Epoch [30/120    avg_loss:0.278, val_acc:0.931]
Epoch [31/120    avg_loss:0.281, val_acc:0.929]
Epoch [32/120    avg_loss:0.257, val_acc:0.905]
Epoch [33/120    avg_loss:0.379, val_acc:0.909]
Epoch [34/120    avg_loss:0.249, val_acc:0.935]
Epoch [35/120    avg_loss:0.230, val_acc:0.935]
Epoch [36/120    avg_loss:0.218, val_acc:0.933]
Epoch [37/120    avg_loss:0.245, val_acc:0.929]
Epoch [38/120    avg_loss:0.237, val_acc:0.944]
Epoch [39/120    avg_loss:0.210, val_acc:0.938]
Epoch [40/120    avg_loss:0.205, val_acc:0.950]
Epoch [41/120    avg_loss:0.184, val_acc:0.944]
Epoch [42/120    avg_loss:0.172, val_acc:0.944]
Epoch [43/120    avg_loss:0.148, val_acc:0.960]
Epoch [44/120    avg_loss:0.131, val_acc:0.946]
Epoch [45/120    avg_loss:0.190, val_acc:0.958]
Epoch [46/120    avg_loss:0.162, val_acc:0.940]
Epoch [47/120    avg_loss:0.177, val_acc:0.935]
Epoch [48/120    avg_loss:0.150, val_acc:0.938]
Epoch [49/120    avg_loss:0.192, val_acc:0.917]
Epoch [50/120    avg_loss:0.207, val_acc:0.950]
Epoch [51/120    avg_loss:0.186, val_acc:0.956]
Epoch [52/120    avg_loss:0.141, val_acc:0.954]
Epoch [53/120    avg_loss:0.166, val_acc:0.960]
Epoch [54/120    avg_loss:0.120, val_acc:0.952]
Epoch [55/120    avg_loss:0.146, val_acc:0.948]
Epoch [56/120    avg_loss:0.253, val_acc:0.948]
Epoch [57/120    avg_loss:0.158, val_acc:0.952]
Epoch [58/120    avg_loss:0.138, val_acc:0.958]
Epoch [59/120    avg_loss:0.133, val_acc:0.960]
Epoch [60/120    avg_loss:0.110, val_acc:0.966]
Epoch [61/120    avg_loss:0.078, val_acc:0.952]
Epoch [62/120    avg_loss:0.096, val_acc:0.958]
Epoch [63/120    avg_loss:0.090, val_acc:0.964]
Epoch [64/120    avg_loss:0.112, val_acc:0.950]
Epoch [65/120    avg_loss:0.161, val_acc:0.964]
Epoch [66/120    avg_loss:0.113, val_acc:0.956]
Epoch [67/120    avg_loss:0.100, val_acc:0.958]
Epoch [68/120    avg_loss:0.081, val_acc:0.954]
Epoch [69/120    avg_loss:0.099, val_acc:0.966]
Epoch [70/120    avg_loss:0.070, val_acc:0.960]
Epoch [71/120    avg_loss:0.090, val_acc:0.966]
Epoch [72/120    avg_loss:0.062, val_acc:0.964]
Epoch [73/120    avg_loss:0.103, val_acc:0.956]
Epoch [74/120    avg_loss:0.105, val_acc:0.966]
Epoch [75/120    avg_loss:0.062, val_acc:0.962]
Epoch [76/120    avg_loss:0.060, val_acc:0.962]
Epoch [77/120    avg_loss:0.059, val_acc:0.964]
Epoch [78/120    avg_loss:0.058, val_acc:0.962]
Epoch [79/120    avg_loss:0.089, val_acc:0.966]
Epoch [80/120    avg_loss:0.068, val_acc:0.958]
Epoch [81/120    avg_loss:0.069, val_acc:0.966]
Epoch [82/120    avg_loss:0.036, val_acc:0.970]
Epoch [83/120    avg_loss:0.058, val_acc:0.940]
Epoch [84/120    avg_loss:0.043, val_acc:0.968]
Epoch [85/120    avg_loss:0.033, val_acc:0.968]
Epoch [86/120    avg_loss:0.052, val_acc:0.958]
Epoch [87/120    avg_loss:0.038, val_acc:0.962]
Epoch [88/120    avg_loss:0.063, val_acc:0.958]
Epoch [89/120    avg_loss:0.039, val_acc:0.966]
Epoch [90/120    avg_loss:0.052, val_acc:0.970]
Epoch [91/120    avg_loss:0.083, val_acc:0.960]
Epoch [92/120    avg_loss:0.035, val_acc:0.968]
Epoch [93/120    avg_loss:0.026, val_acc:0.968]
Epoch [94/120    avg_loss:0.040, val_acc:0.956]
Epoch [95/120    avg_loss:0.043, val_acc:0.964]
Epoch [96/120    avg_loss:0.057, val_acc:0.964]
Epoch [97/120    avg_loss:0.136, val_acc:0.933]
Epoch [98/120    avg_loss:0.090, val_acc:0.962]
Epoch [99/120    avg_loss:0.070, val_acc:0.940]
Epoch [100/120    avg_loss:0.084, val_acc:0.964]
Epoch [101/120    avg_loss:0.055, val_acc:0.964]
Epoch [102/120    avg_loss:0.045, val_acc:0.966]
Epoch [103/120    avg_loss:0.047, val_acc:0.958]
Epoch [104/120    avg_loss:0.049, val_acc:0.966]
Epoch [105/120    avg_loss:0.023, val_acc:0.964]
Epoch [106/120    avg_loss:0.023, val_acc:0.968]
Epoch [107/120    avg_loss:0.019, val_acc:0.968]
Epoch [108/120    avg_loss:0.017, val_acc:0.966]
Epoch [109/120    avg_loss:0.019, val_acc:0.970]
Epoch [110/120    avg_loss:0.020, val_acc:0.968]
Epoch [111/120    avg_loss:0.015, val_acc:0.968]
Epoch [112/120    avg_loss:0.018, val_acc:0.966]
Epoch [113/120    avg_loss:0.012, val_acc:0.968]
Epoch [114/120    avg_loss:0.016, val_acc:0.970]
Epoch [115/120    avg_loss:0.017, val_acc:0.968]
Epoch [116/120    avg_loss:0.016, val_acc:0.970]
Epoch [117/120    avg_loss:0.016, val_acc:0.970]
Epoch [118/120    avg_loss:0.015, val_acc:0.968]
Epoch [119/120    avg_loss:0.021, val_acc:0.968]
Epoch [120/120    avg_loss:0.018, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 205  21   0   0   0   3   0   0   0   0   0]
 [  0   0   0   1 205  13   0   0   0   0   0   0   8   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.72068230277185

F1 scores:
[       nan 1.         0.98871332 0.94036697 0.88744589 0.92517007
 1.         0.98924731 0.99614891 1.         1.         1.
 0.98903509 1.        ]

Kappa:
0.9857559123214142
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f7e89c908>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.323, val_acc:0.567]
Epoch [2/120    avg_loss:1.809, val_acc:0.617]
Epoch [3/120    avg_loss:1.390, val_acc:0.665]
Epoch [4/120    avg_loss:1.175, val_acc:0.750]
Epoch [5/120    avg_loss:1.004, val_acc:0.760]
Epoch [6/120    avg_loss:0.877, val_acc:0.726]
Epoch [7/120    avg_loss:0.790, val_acc:0.839]
Epoch [8/120    avg_loss:0.705, val_acc:0.776]
Epoch [9/120    avg_loss:0.721, val_acc:0.823]
Epoch [10/120    avg_loss:0.626, val_acc:0.849]
Epoch [11/120    avg_loss:0.566, val_acc:0.796]
Epoch [12/120    avg_loss:0.580, val_acc:0.839]
Epoch [13/120    avg_loss:0.531, val_acc:0.911]
Epoch [14/120    avg_loss:0.400, val_acc:0.881]
Epoch [15/120    avg_loss:0.516, val_acc:0.897]
Epoch [16/120    avg_loss:0.421, val_acc:0.913]
Epoch [17/120    avg_loss:0.357, val_acc:0.889]
Epoch [18/120    avg_loss:0.387, val_acc:0.913]
Epoch [19/120    avg_loss:0.334, val_acc:0.921]
Epoch [20/120    avg_loss:0.339, val_acc:0.919]
Epoch [21/120    avg_loss:0.396, val_acc:0.913]
Epoch [22/120    avg_loss:0.303, val_acc:0.911]
Epoch [23/120    avg_loss:0.431, val_acc:0.905]
Epoch [24/120    avg_loss:0.292, val_acc:0.935]
Epoch [25/120    avg_loss:0.296, val_acc:0.954]
Epoch [26/120    avg_loss:0.225, val_acc:0.952]
Epoch [27/120    avg_loss:0.216, val_acc:0.925]
Epoch [28/120    avg_loss:0.243, val_acc:0.931]
Epoch [29/120    avg_loss:0.230, val_acc:0.948]
Epoch [30/120    avg_loss:0.165, val_acc:0.950]
Epoch [31/120    avg_loss:0.178, val_acc:0.938]
Epoch [32/120    avg_loss:0.169, val_acc:0.946]
Epoch [33/120    avg_loss:0.253, val_acc:0.923]
Epoch [34/120    avg_loss:0.242, val_acc:0.958]
Epoch [35/120    avg_loss:0.188, val_acc:0.966]
Epoch [36/120    avg_loss:0.162, val_acc:0.950]
Epoch [37/120    avg_loss:0.169, val_acc:0.946]
Epoch [38/120    avg_loss:0.156, val_acc:0.950]
Epoch [39/120    avg_loss:0.170, val_acc:0.960]
Epoch [40/120    avg_loss:0.174, val_acc:0.952]
Epoch [41/120    avg_loss:0.198, val_acc:0.946]
Epoch [42/120    avg_loss:0.170, val_acc:0.940]
Epoch [43/120    avg_loss:0.137, val_acc:0.940]
Epoch [44/120    avg_loss:0.153, val_acc:0.950]
Epoch [45/120    avg_loss:0.131, val_acc:0.950]
Epoch [46/120    avg_loss:0.134, val_acc:0.948]
Epoch [47/120    avg_loss:0.098, val_acc:0.940]
Epoch [48/120    avg_loss:0.166, val_acc:0.952]
Epoch [49/120    avg_loss:0.073, val_acc:0.968]
Epoch [50/120    avg_loss:0.082, val_acc:0.972]
Epoch [51/120    avg_loss:0.070, val_acc:0.974]
Epoch [52/120    avg_loss:0.072, val_acc:0.972]
Epoch [53/120    avg_loss:0.080, val_acc:0.970]
Epoch [54/120    avg_loss:0.072, val_acc:0.976]
Epoch [55/120    avg_loss:0.064, val_acc:0.978]
Epoch [56/120    avg_loss:0.062, val_acc:0.974]
Epoch [57/120    avg_loss:0.055, val_acc:0.976]
Epoch [58/120    avg_loss:0.073, val_acc:0.974]
Epoch [59/120    avg_loss:0.060, val_acc:0.976]
Epoch [60/120    avg_loss:0.059, val_acc:0.974]
Epoch [61/120    avg_loss:0.065, val_acc:0.978]
Epoch [62/120    avg_loss:0.060, val_acc:0.978]
Epoch [63/120    avg_loss:0.061, val_acc:0.976]
Epoch [64/120    avg_loss:0.067, val_acc:0.976]
Epoch [65/120    avg_loss:0.065, val_acc:0.976]
Epoch [66/120    avg_loss:0.062, val_acc:0.982]
Epoch [67/120    avg_loss:0.051, val_acc:0.982]
Epoch [68/120    avg_loss:0.053, val_acc:0.988]
Epoch [69/120    avg_loss:0.053, val_acc:0.984]
Epoch [70/120    avg_loss:0.050, val_acc:0.982]
Epoch [71/120    avg_loss:0.053, val_acc:0.982]
Epoch [72/120    avg_loss:0.055, val_acc:0.986]
Epoch [73/120    avg_loss:0.061, val_acc:0.984]
Epoch [74/120    avg_loss:0.056, val_acc:0.986]
Epoch [75/120    avg_loss:0.058, val_acc:0.986]
Epoch [76/120    avg_loss:0.048, val_acc:0.990]
Epoch [77/120    avg_loss:0.055, val_acc:0.990]
Epoch [78/120    avg_loss:0.040, val_acc:0.988]
Epoch [79/120    avg_loss:0.048, val_acc:0.990]
Epoch [80/120    avg_loss:0.046, val_acc:0.982]
Epoch [81/120    avg_loss:0.042, val_acc:0.986]
Epoch [82/120    avg_loss:0.047, val_acc:0.988]
Epoch [83/120    avg_loss:0.042, val_acc:0.984]
Epoch [84/120    avg_loss:0.054, val_acc:0.982]
Epoch [85/120    avg_loss:0.052, val_acc:0.982]
Epoch [86/120    avg_loss:0.043, val_acc:0.990]
Epoch [87/120    avg_loss:0.048, val_acc:0.988]
Epoch [88/120    avg_loss:0.051, val_acc:0.988]
Epoch [89/120    avg_loss:0.052, val_acc:0.988]
Epoch [90/120    avg_loss:0.041, val_acc:0.986]
Epoch [91/120    avg_loss:0.042, val_acc:0.990]
Epoch [92/120    avg_loss:0.041, val_acc:0.984]
Epoch [93/120    avg_loss:0.043, val_acc:0.986]
Epoch [94/120    avg_loss:0.039, val_acc:0.990]
Epoch [95/120    avg_loss:0.055, val_acc:0.982]
Epoch [96/120    avg_loss:0.032, val_acc:0.982]
Epoch [97/120    avg_loss:0.055, val_acc:0.984]
Epoch [98/120    avg_loss:0.043, val_acc:0.984]
Epoch [99/120    avg_loss:0.043, val_acc:0.988]
Epoch [100/120    avg_loss:0.037, val_acc:0.988]
Epoch [101/120    avg_loss:0.037, val_acc:0.988]
Epoch [102/120    avg_loss:0.037, val_acc:0.986]
Epoch [103/120    avg_loss:0.037, val_acc:0.982]
Epoch [104/120    avg_loss:0.038, val_acc:0.982]
Epoch [105/120    avg_loss:0.035, val_acc:0.988]
Epoch [106/120    avg_loss:0.040, val_acc:0.980]
Epoch [107/120    avg_loss:0.038, val_acc:0.986]
Epoch [108/120    avg_loss:0.033, val_acc:0.984]
Epoch [109/120    avg_loss:0.038, val_acc:0.984]
Epoch [110/120    avg_loss:0.038, val_acc:0.984]
Epoch [111/120    avg_loss:0.043, val_acc:0.984]
Epoch [112/120    avg_loss:0.033, val_acc:0.984]
Epoch [113/120    avg_loss:0.036, val_acc:0.986]
Epoch [114/120    avg_loss:0.034, val_acc:0.984]
Epoch [115/120    avg_loss:0.038, val_acc:0.988]
Epoch [116/120    avg_loss:0.035, val_acc:0.988]
Epoch [117/120    avg_loss:0.037, val_acc:0.988]
Epoch [118/120    avg_loss:0.036, val_acc:0.988]
Epoch [119/120    avg_loss:0.034, val_acc:0.988]
Epoch [120/120    avg_loss:0.035, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   0   0   0   0   1   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  16   0   0   0   0   0   0   3   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 1.         0.97104677 0.99782135 0.9122807  0.87412587
 0.99756691 0.93785311 1.         1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9874171110989057
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdb357498d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.311, val_acc:0.482]
Epoch [2/120    avg_loss:1.796, val_acc:0.603]
Epoch [3/120    avg_loss:1.433, val_acc:0.609]
Epoch [4/120    avg_loss:1.180, val_acc:0.722]
Epoch [5/120    avg_loss:0.934, val_acc:0.806]
Epoch [6/120    avg_loss:0.860, val_acc:0.819]
Epoch [7/120    avg_loss:0.794, val_acc:0.839]
Epoch [8/120    avg_loss:0.714, val_acc:0.867]
Epoch [9/120    avg_loss:0.666, val_acc:0.827]
Epoch [10/120    avg_loss:0.557, val_acc:0.827]
Epoch [11/120    avg_loss:0.528, val_acc:0.863]
Epoch [12/120    avg_loss:0.536, val_acc:0.895]
Epoch [13/120    avg_loss:0.505, val_acc:0.845]
Epoch [14/120    avg_loss:0.464, val_acc:0.895]
Epoch [15/120    avg_loss:0.397, val_acc:0.863]
Epoch [16/120    avg_loss:0.501, val_acc:0.911]
Epoch [17/120    avg_loss:0.402, val_acc:0.903]
Epoch [18/120    avg_loss:0.432, val_acc:0.897]
Epoch [19/120    avg_loss:0.328, val_acc:0.915]
Epoch [20/120    avg_loss:0.342, val_acc:0.911]
Epoch [21/120    avg_loss:0.394, val_acc:0.903]
Epoch [22/120    avg_loss:0.361, val_acc:0.915]
Epoch [23/120    avg_loss:0.349, val_acc:0.905]
Epoch [24/120    avg_loss:0.342, val_acc:0.927]
Epoch [25/120    avg_loss:0.313, val_acc:0.944]
Epoch [26/120    avg_loss:0.331, val_acc:0.879]
Epoch [27/120    avg_loss:0.273, val_acc:0.935]
Epoch [28/120    avg_loss:0.278, val_acc:0.917]
Epoch [29/120    avg_loss:0.271, val_acc:0.935]
Epoch [30/120    avg_loss:0.252, val_acc:0.942]
Epoch [31/120    avg_loss:0.227, val_acc:0.937]
Epoch [32/120    avg_loss:0.325, val_acc:0.948]
Epoch [33/120    avg_loss:0.213, val_acc:0.915]
Epoch [34/120    avg_loss:0.216, val_acc:0.946]
Epoch [35/120    avg_loss:0.229, val_acc:0.946]
Epoch [36/120    avg_loss:0.230, val_acc:0.925]
Epoch [37/120    avg_loss:0.217, val_acc:0.909]
Epoch [38/120    avg_loss:0.190, val_acc:0.956]
Epoch [39/120    avg_loss:0.155, val_acc:0.944]
Epoch [40/120    avg_loss:0.154, val_acc:0.970]
Epoch [41/120    avg_loss:0.177, val_acc:0.966]
Epoch [42/120    avg_loss:0.135, val_acc:0.968]
Epoch [43/120    avg_loss:0.148, val_acc:0.950]
Epoch [44/120    avg_loss:0.152, val_acc:0.960]
Epoch [45/120    avg_loss:0.166, val_acc:0.966]
Epoch [46/120    avg_loss:0.164, val_acc:0.962]
Epoch [47/120    avg_loss:0.126, val_acc:0.968]
Epoch [48/120    avg_loss:0.166, val_acc:0.966]
Epoch [49/120    avg_loss:0.133, val_acc:0.950]
Epoch [50/120    avg_loss:0.110, val_acc:0.974]
Epoch [51/120    avg_loss:0.084, val_acc:0.958]
Epoch [52/120    avg_loss:0.075, val_acc:0.972]
Epoch [53/120    avg_loss:0.139, val_acc:0.964]
Epoch [54/120    avg_loss:0.103, val_acc:0.962]
Epoch [55/120    avg_loss:0.086, val_acc:0.986]
Epoch [56/120    avg_loss:0.075, val_acc:0.938]
Epoch [57/120    avg_loss:0.139, val_acc:0.958]
Epoch [58/120    avg_loss:0.092, val_acc:0.966]
Epoch [59/120    avg_loss:0.086, val_acc:0.956]
Epoch [60/120    avg_loss:0.197, val_acc:0.960]
Epoch [61/120    avg_loss:0.109, val_acc:0.978]
Epoch [62/120    avg_loss:0.059, val_acc:0.978]
Epoch [63/120    avg_loss:0.043, val_acc:0.986]
Epoch [64/120    avg_loss:0.044, val_acc:0.966]
Epoch [65/120    avg_loss:0.055, val_acc:0.968]
Epoch [66/120    avg_loss:0.118, val_acc:0.964]
Epoch [67/120    avg_loss:0.060, val_acc:0.978]
Epoch [68/120    avg_loss:0.040, val_acc:0.972]
Epoch [69/120    avg_loss:0.056, val_acc:0.974]
Epoch [70/120    avg_loss:0.042, val_acc:0.978]
Epoch [71/120    avg_loss:0.057, val_acc:0.980]
Epoch [72/120    avg_loss:0.022, val_acc:0.982]
Epoch [73/120    avg_loss:0.028, val_acc:0.982]
Epoch [74/120    avg_loss:0.039, val_acc:0.976]
Epoch [75/120    avg_loss:0.136, val_acc:0.966]
Epoch [76/120    avg_loss:0.060, val_acc:0.964]
Epoch [77/120    avg_loss:0.065, val_acc:0.976]
Epoch [78/120    avg_loss:0.030, val_acc:0.978]
Epoch [79/120    avg_loss:0.035, val_acc:0.974]
Epoch [80/120    avg_loss:0.029, val_acc:0.976]
Epoch [81/120    avg_loss:0.028, val_acc:0.978]
Epoch [82/120    avg_loss:0.021, val_acc:0.980]
Epoch [83/120    avg_loss:0.027, val_acc:0.978]
Epoch [84/120    avg_loss:0.028, val_acc:0.978]
Epoch [85/120    avg_loss:0.021, val_acc:0.978]
Epoch [86/120    avg_loss:0.020, val_acc:0.978]
Epoch [87/120    avg_loss:0.022, val_acc:0.978]
Epoch [88/120    avg_loss:0.017, val_acc:0.978]
Epoch [89/120    avg_loss:0.025, val_acc:0.978]
Epoch [90/120    avg_loss:0.016, val_acc:0.978]
Epoch [91/120    avg_loss:0.022, val_acc:0.978]
Epoch [92/120    avg_loss:0.023, val_acc:0.978]
Epoch [93/120    avg_loss:0.018, val_acc:0.978]
Epoch [94/120    avg_loss:0.018, val_acc:0.978]
Epoch [95/120    avg_loss:0.019, val_acc:0.978]
Epoch [96/120    avg_loss:0.019, val_acc:0.978]
Epoch [97/120    avg_loss:0.022, val_acc:0.978]
Epoch [98/120    avg_loss:0.022, val_acc:0.980]
Epoch [99/120    avg_loss:0.017, val_acc:0.980]
Epoch [100/120    avg_loss:0.017, val_acc:0.980]
Epoch [101/120    avg_loss:0.016, val_acc:0.980]
Epoch [102/120    avg_loss:0.013, val_acc:0.980]
Epoch [103/120    avg_loss:0.020, val_acc:0.980]
Epoch [104/120    avg_loss:0.022, val_acc:0.980]
Epoch [105/120    avg_loss:0.015, val_acc:0.980]
Epoch [106/120    avg_loss:0.013, val_acc:0.980]
Epoch [107/120    avg_loss:0.023, val_acc:0.980]
Epoch [108/120    avg_loss:0.016, val_acc:0.980]
Epoch [109/120    avg_loss:0.017, val_acc:0.980]
Epoch [110/120    avg_loss:0.018, val_acc:0.980]
Epoch [111/120    avg_loss:0.015, val_acc:0.980]
Epoch [112/120    avg_loss:0.023, val_acc:0.980]
Epoch [113/120    avg_loss:0.021, val_acc:0.980]
Epoch [114/120    avg_loss:0.018, val_acc:0.980]
Epoch [115/120    avg_loss:0.016, val_acc:0.980]
Epoch [116/120    avg_loss:0.017, val_acc:0.980]
Epoch [117/120    avg_loss:0.015, val_acc:0.980]
Epoch [118/120    avg_loss:0.012, val_acc:0.980]
Epoch [119/120    avg_loss:0.019, val_acc:0.980]
Epoch [120/120    avg_loss:0.021, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 212  16   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 205  16   0   0   0   0   0   0   6   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8272921108742

F1 scores:
[       nan 1.         0.97550111 0.95927602 0.90507726 0.93023256
 1.         0.94382022 0.998713   1.         1.         1.
 0.99342105 1.        ]

Kappa:
0.9869428284187428
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff00b562940>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.317, val_acc:0.471]
Epoch [2/120    avg_loss:1.815, val_acc:0.697]
Epoch [3/120    avg_loss:1.421, val_acc:0.756]
Epoch [4/120    avg_loss:1.132, val_acc:0.795]
Epoch [5/120    avg_loss:0.939, val_acc:0.799]
Epoch [6/120    avg_loss:0.817, val_acc:0.826]
Epoch [7/120    avg_loss:0.728, val_acc:0.844]
Epoch [8/120    avg_loss:0.652, val_acc:0.867]
Epoch [9/120    avg_loss:0.575, val_acc:0.863]
Epoch [10/120    avg_loss:0.501, val_acc:0.871]
Epoch [11/120    avg_loss:0.581, val_acc:0.857]
Epoch [12/120    avg_loss:0.493, val_acc:0.889]
Epoch [13/120    avg_loss:0.449, val_acc:0.902]
Epoch [14/120    avg_loss:0.439, val_acc:0.904]
Epoch [15/120    avg_loss:0.383, val_acc:0.918]
Epoch [16/120    avg_loss:0.334, val_acc:0.908]
Epoch [17/120    avg_loss:0.351, val_acc:0.883]
Epoch [18/120    avg_loss:0.332, val_acc:0.908]
Epoch [19/120    avg_loss:0.374, val_acc:0.902]
Epoch [20/120    avg_loss:0.328, val_acc:0.934]
Epoch [21/120    avg_loss:0.350, val_acc:0.936]
Epoch [22/120    avg_loss:0.265, val_acc:0.947]
Epoch [23/120    avg_loss:0.307, val_acc:0.830]
Epoch [24/120    avg_loss:0.356, val_acc:0.918]
Epoch [25/120    avg_loss:0.270, val_acc:0.932]
Epoch [26/120    avg_loss:0.258, val_acc:0.920]
Epoch [27/120    avg_loss:0.257, val_acc:0.947]
Epoch [28/120    avg_loss:0.244, val_acc:0.943]
Epoch [29/120    avg_loss:0.262, val_acc:0.951]
Epoch [30/120    avg_loss:0.244, val_acc:0.953]
Epoch [31/120    avg_loss:0.251, val_acc:0.955]
Epoch [32/120    avg_loss:0.204, val_acc:0.951]
Epoch [33/120    avg_loss:0.160, val_acc:0.969]
Epoch [34/120    avg_loss:0.158, val_acc:0.941]
Epoch [35/120    avg_loss:0.182, val_acc:0.965]
Epoch [36/120    avg_loss:0.215, val_acc:0.951]
Epoch [37/120    avg_loss:0.185, val_acc:0.973]
Epoch [38/120    avg_loss:0.161, val_acc:0.959]
Epoch [39/120    avg_loss:0.113, val_acc:0.975]
Epoch [40/120    avg_loss:0.107, val_acc:0.955]
Epoch [41/120    avg_loss:0.162, val_acc:0.955]
Epoch [42/120    avg_loss:0.121, val_acc:0.945]
Epoch [43/120    avg_loss:0.150, val_acc:0.906]
Epoch [44/120    avg_loss:0.195, val_acc:0.963]
Epoch [45/120    avg_loss:0.133, val_acc:0.957]
Epoch [46/120    avg_loss:0.154, val_acc:0.959]
Epoch [47/120    avg_loss:0.118, val_acc:0.986]
Epoch [48/120    avg_loss:0.101, val_acc:0.980]
Epoch [49/120    avg_loss:0.172, val_acc:0.951]
Epoch [50/120    avg_loss:0.157, val_acc:0.965]
Epoch [51/120    avg_loss:0.205, val_acc:0.971]
Epoch [52/120    avg_loss:0.126, val_acc:0.986]
Epoch [53/120    avg_loss:0.080, val_acc:0.959]
Epoch [54/120    avg_loss:0.080, val_acc:0.990]
Epoch [55/120    avg_loss:0.120, val_acc:0.977]
Epoch [56/120    avg_loss:0.092, val_acc:0.977]
Epoch [57/120    avg_loss:0.085, val_acc:0.977]
Epoch [58/120    avg_loss:0.096, val_acc:0.986]
Epoch [59/120    avg_loss:0.068, val_acc:0.992]
Epoch [60/120    avg_loss:0.053, val_acc:0.984]
Epoch [61/120    avg_loss:0.047, val_acc:0.986]
Epoch [62/120    avg_loss:0.042, val_acc:0.994]
Epoch [63/120    avg_loss:0.065, val_acc:0.969]
Epoch [64/120    avg_loss:0.100, val_acc:0.975]
Epoch [65/120    avg_loss:0.042, val_acc:0.996]
Epoch [66/120    avg_loss:0.050, val_acc:0.988]
Epoch [67/120    avg_loss:0.075, val_acc:0.988]
Epoch [68/120    avg_loss:0.077, val_acc:0.990]
Epoch [69/120    avg_loss:0.110, val_acc:0.986]
Epoch [70/120    avg_loss:0.086, val_acc:0.967]
Epoch [71/120    avg_loss:0.053, val_acc:0.994]
Epoch [72/120    avg_loss:0.040, val_acc:0.992]
Epoch [73/120    avg_loss:0.047, val_acc:0.971]
Epoch [74/120    avg_loss:0.042, val_acc:0.994]
Epoch [75/120    avg_loss:0.040, val_acc:0.994]
Epoch [76/120    avg_loss:0.044, val_acc:0.994]
Epoch [77/120    avg_loss:0.030, val_acc:0.994]
Epoch [78/120    avg_loss:0.052, val_acc:0.984]
Epoch [79/120    avg_loss:0.031, val_acc:0.990]
Epoch [80/120    avg_loss:0.025, val_acc:0.994]
Epoch [81/120    avg_loss:0.026, val_acc:0.990]
Epoch [82/120    avg_loss:0.021, val_acc:0.990]
Epoch [83/120    avg_loss:0.022, val_acc:0.992]
Epoch [84/120    avg_loss:0.017, val_acc:0.994]
Epoch [85/120    avg_loss:0.021, val_acc:0.994]
Epoch [86/120    avg_loss:0.022, val_acc:0.994]
Epoch [87/120    avg_loss:0.020, val_acc:0.994]
Epoch [88/120    avg_loss:0.024, val_acc:0.994]
Epoch [89/120    avg_loss:0.023, val_acc:0.994]
Epoch [90/120    avg_loss:0.019, val_acc:0.994]
Epoch [91/120    avg_loss:0.019, val_acc:0.996]
Epoch [92/120    avg_loss:0.012, val_acc:0.996]
Epoch [93/120    avg_loss:0.016, val_acc:0.996]
Epoch [94/120    avg_loss:0.018, val_acc:0.996]
Epoch [95/120    avg_loss:0.017, val_acc:0.996]
Epoch [96/120    avg_loss:0.015, val_acc:0.996]
Epoch [97/120    avg_loss:0.016, val_acc:0.994]
Epoch [98/120    avg_loss:0.016, val_acc:0.994]
Epoch [99/120    avg_loss:0.017, val_acc:0.994]
Epoch [100/120    avg_loss:0.021, val_acc:0.996]
Epoch [101/120    avg_loss:0.013, val_acc:0.996]
Epoch [102/120    avg_loss:0.012, val_acc:0.996]
Epoch [103/120    avg_loss:0.019, val_acc:0.996]
Epoch [104/120    avg_loss:0.015, val_acc:0.996]
Epoch [105/120    avg_loss:0.019, val_acc:0.996]
Epoch [106/120    avg_loss:0.018, val_acc:0.996]
Epoch [107/120    avg_loss:0.018, val_acc:0.996]
Epoch [108/120    avg_loss:0.011, val_acc:0.996]
Epoch [109/120    avg_loss:0.017, val_acc:0.996]
Epoch [110/120    avg_loss:0.013, val_acc:0.996]
Epoch [111/120    avg_loss:0.012, val_acc:0.996]
Epoch [112/120    avg_loss:0.015, val_acc:0.996]
Epoch [113/120    avg_loss:0.012, val_acc:0.996]
Epoch [114/120    avg_loss:0.016, val_acc:0.994]
Epoch [115/120    avg_loss:0.014, val_acc:0.996]
Epoch [116/120    avg_loss:0.013, val_acc:0.996]
Epoch [117/120    avg_loss:0.011, val_acc:0.996]
Epoch [118/120    avg_loss:0.015, val_acc:0.996]
Epoch [119/120    avg_loss:0.014, val_acc:0.996]
Epoch [120/120    avg_loss:0.016, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 1.         0.99319728 1.         0.94877506 0.9220339
 1.         0.98378378 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9938280931090412
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6dc7306898>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.325, val_acc:0.484]
Epoch [2/120    avg_loss:1.817, val_acc:0.589]
Epoch [3/120    avg_loss:1.465, val_acc:0.679]
Epoch [4/120    avg_loss:1.143, val_acc:0.732]
Epoch [5/120    avg_loss:0.992, val_acc:0.808]
Epoch [6/120    avg_loss:0.847, val_acc:0.817]
Epoch [7/120    avg_loss:0.772, val_acc:0.806]
Epoch [8/120    avg_loss:0.662, val_acc:0.871]
Epoch [9/120    avg_loss:0.590, val_acc:0.847]
Epoch [10/120    avg_loss:0.560, val_acc:0.829]
Epoch [11/120    avg_loss:0.571, val_acc:0.893]
Epoch [12/120    avg_loss:0.419, val_acc:0.873]
Epoch [13/120    avg_loss:0.401, val_acc:0.929]
Epoch [14/120    avg_loss:0.381, val_acc:0.938]
Epoch [15/120    avg_loss:0.421, val_acc:0.901]
Epoch [16/120    avg_loss:0.319, val_acc:0.921]
Epoch [17/120    avg_loss:0.337, val_acc:0.927]
Epoch [18/120    avg_loss:0.346, val_acc:0.929]
Epoch [19/120    avg_loss:0.306, val_acc:0.903]
Epoch [20/120    avg_loss:0.269, val_acc:0.952]
Epoch [21/120    avg_loss:0.255, val_acc:0.950]
Epoch [22/120    avg_loss:0.233, val_acc:0.954]
Epoch [23/120    avg_loss:0.246, val_acc:0.948]
Epoch [24/120    avg_loss:0.215, val_acc:0.944]
Epoch [25/120    avg_loss:0.209, val_acc:0.970]
Epoch [26/120    avg_loss:0.213, val_acc:0.962]
Epoch [27/120    avg_loss:0.222, val_acc:0.958]
Epoch [28/120    avg_loss:0.176, val_acc:0.942]
Epoch [29/120    avg_loss:0.224, val_acc:0.952]
Epoch [30/120    avg_loss:0.183, val_acc:0.958]
Epoch [31/120    avg_loss:0.202, val_acc:0.958]
Epoch [32/120    avg_loss:0.215, val_acc:0.958]
Epoch [33/120    avg_loss:0.157, val_acc:0.972]
Epoch [34/120    avg_loss:0.155, val_acc:0.956]
Epoch [35/120    avg_loss:0.145, val_acc:0.966]
Epoch [36/120    avg_loss:0.124, val_acc:0.978]
Epoch [37/120    avg_loss:0.150, val_acc:0.982]
Epoch [38/120    avg_loss:0.118, val_acc:0.970]
Epoch [39/120    avg_loss:0.164, val_acc:0.950]
Epoch [40/120    avg_loss:0.146, val_acc:0.942]
Epoch [41/120    avg_loss:0.130, val_acc:0.980]
Epoch [42/120    avg_loss:0.093, val_acc:0.970]
Epoch [43/120    avg_loss:0.102, val_acc:0.968]
Epoch [44/120    avg_loss:0.077, val_acc:0.980]
Epoch [45/120    avg_loss:0.106, val_acc:0.974]
Epoch [46/120    avg_loss:0.100, val_acc:0.948]
Epoch [47/120    avg_loss:0.157, val_acc:0.952]
Epoch [48/120    avg_loss:0.133, val_acc:0.974]
Epoch [49/120    avg_loss:0.125, val_acc:0.960]
Epoch [50/120    avg_loss:0.065, val_acc:0.982]
Epoch [51/120    avg_loss:0.109, val_acc:0.970]
Epoch [52/120    avg_loss:0.079, val_acc:0.980]
Epoch [53/120    avg_loss:0.091, val_acc:0.980]
Epoch [54/120    avg_loss:0.076, val_acc:0.988]
Epoch [55/120    avg_loss:0.052, val_acc:0.974]
Epoch [56/120    avg_loss:0.086, val_acc:0.984]
Epoch [57/120    avg_loss:0.074, val_acc:0.988]
Epoch [58/120    avg_loss:0.056, val_acc:0.970]
Epoch [59/120    avg_loss:0.062, val_acc:0.986]
Epoch [60/120    avg_loss:0.054, val_acc:0.984]
Epoch [61/120    avg_loss:0.034, val_acc:0.982]
Epoch [62/120    avg_loss:0.075, val_acc:0.972]
Epoch [63/120    avg_loss:0.077, val_acc:0.962]
Epoch [64/120    avg_loss:0.075, val_acc:0.952]
Epoch [65/120    avg_loss:0.135, val_acc:0.984]
Epoch [66/120    avg_loss:0.041, val_acc:0.958]
Epoch [67/120    avg_loss:0.052, val_acc:0.976]
Epoch [68/120    avg_loss:0.056, val_acc:0.988]
Epoch [69/120    avg_loss:0.033, val_acc:0.982]
Epoch [70/120    avg_loss:0.028, val_acc:0.982]
Epoch [71/120    avg_loss:0.057, val_acc:0.988]
Epoch [72/120    avg_loss:0.031, val_acc:0.984]
Epoch [73/120    avg_loss:0.023, val_acc:0.984]
Epoch [74/120    avg_loss:0.045, val_acc:0.978]
Epoch [75/120    avg_loss:0.103, val_acc:0.988]
Epoch [76/120    avg_loss:0.035, val_acc:0.988]
Epoch [77/120    avg_loss:0.029, val_acc:0.990]
Epoch [78/120    avg_loss:0.049, val_acc:0.992]
Epoch [79/120    avg_loss:0.069, val_acc:0.984]
Epoch [80/120    avg_loss:0.023, val_acc:0.978]
Epoch [81/120    avg_loss:0.036, val_acc:0.986]
Epoch [82/120    avg_loss:0.023, val_acc:0.986]
Epoch [83/120    avg_loss:0.036, val_acc:0.978]
Epoch [84/120    avg_loss:0.019, val_acc:0.988]
Epoch [85/120    avg_loss:0.029, val_acc:0.986]
Epoch [86/120    avg_loss:0.021, val_acc:0.986]
Epoch [87/120    avg_loss:0.034, val_acc:0.980]
Epoch [88/120    avg_loss:0.032, val_acc:0.992]
Epoch [89/120    avg_loss:0.027, val_acc:0.978]
Epoch [90/120    avg_loss:0.042, val_acc:0.974]
Epoch [91/120    avg_loss:0.042, val_acc:0.988]
Epoch [92/120    avg_loss:0.017, val_acc:0.992]
Epoch [93/120    avg_loss:0.022, val_acc:0.976]
Epoch [94/120    avg_loss:0.030, val_acc:0.992]
Epoch [95/120    avg_loss:0.057, val_acc:0.972]
Epoch [96/120    avg_loss:0.040, val_acc:0.992]
Epoch [97/120    avg_loss:0.012, val_acc:0.992]
Epoch [98/120    avg_loss:0.013, val_acc:0.984]
Epoch [99/120    avg_loss:0.017, val_acc:0.986]
Epoch [100/120    avg_loss:0.029, val_acc:0.982]
Epoch [101/120    avg_loss:0.017, val_acc:0.986]
Epoch [102/120    avg_loss:0.009, val_acc:0.992]
Epoch [103/120    avg_loss:0.011, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.990]
Epoch [105/120    avg_loss:0.007, val_acc:0.990]
Epoch [106/120    avg_loss:0.011, val_acc:0.990]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.984]
Epoch [109/120    avg_loss:0.009, val_acc:0.988]
Epoch [110/120    avg_loss:0.006, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.008, val_acc:0.994]
Epoch [115/120    avg_loss:0.006, val_acc:0.994]
Epoch [116/120    avg_loss:0.006, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.992]
Epoch [118/120    avg_loss:0.004, val_acc:0.994]
Epoch [119/120    avg_loss:0.005, val_acc:0.994]
Epoch [120/120    avg_loss:0.005, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  12   0   0   0   0   0   0   1   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 0.99780541 0.99095023 1.         0.95535714 0.93559322
 0.99277108 0.97826087 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9935910643555225
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:04:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f20bbc8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.319, val_acc:0.524]
Epoch [2/120    avg_loss:1.862, val_acc:0.607]
Epoch [3/120    avg_loss:1.433, val_acc:0.724]
Epoch [4/120    avg_loss:1.189, val_acc:0.821]
Epoch [5/120    avg_loss:0.982, val_acc:0.819]
Epoch [6/120    avg_loss:0.858, val_acc:0.796]
Epoch [7/120    avg_loss:0.827, val_acc:0.841]
Epoch [8/120    avg_loss:0.690, val_acc:0.881]
Epoch [9/120    avg_loss:0.659, val_acc:0.893]
Epoch [10/120    avg_loss:0.567, val_acc:0.867]
Epoch [11/120    avg_loss:0.505, val_acc:0.921]
Epoch [12/120    avg_loss:0.533, val_acc:0.873]
Epoch [13/120    avg_loss:0.444, val_acc:0.935]
Epoch [14/120    avg_loss:0.441, val_acc:0.911]
Epoch [15/120    avg_loss:0.405, val_acc:0.911]
Epoch [16/120    avg_loss:0.378, val_acc:0.899]
Epoch [17/120    avg_loss:0.313, val_acc:0.931]
Epoch [18/120    avg_loss:0.295, val_acc:0.917]
Epoch [19/120    avg_loss:0.348, val_acc:0.903]
Epoch [20/120    avg_loss:0.387, val_acc:0.942]
Epoch [21/120    avg_loss:0.310, val_acc:0.933]
Epoch [22/120    avg_loss:0.282, val_acc:0.952]
Epoch [23/120    avg_loss:0.267, val_acc:0.919]
Epoch [24/120    avg_loss:0.328, val_acc:0.935]
Epoch [25/120    avg_loss:0.240, val_acc:0.942]
Epoch [26/120    avg_loss:0.364, val_acc:0.929]
Epoch [27/120    avg_loss:0.240, val_acc:0.954]
Epoch [28/120    avg_loss:0.264, val_acc:0.964]
Epoch [29/120    avg_loss:0.192, val_acc:0.972]
Epoch [30/120    avg_loss:0.204, val_acc:0.956]
Epoch [31/120    avg_loss:0.172, val_acc:0.956]
Epoch [32/120    avg_loss:0.175, val_acc:0.966]
Epoch [33/120    avg_loss:0.237, val_acc:0.940]
Epoch [34/120    avg_loss:0.168, val_acc:0.978]
Epoch [35/120    avg_loss:0.129, val_acc:0.970]
Epoch [36/120    avg_loss:0.192, val_acc:0.956]
Epoch [37/120    avg_loss:0.177, val_acc:0.962]
Epoch [38/120    avg_loss:0.233, val_acc:0.899]
Epoch [39/120    avg_loss:0.192, val_acc:0.946]
Epoch [40/120    avg_loss:0.142, val_acc:0.972]
Epoch [41/120    avg_loss:0.172, val_acc:0.964]
Epoch [42/120    avg_loss:0.135, val_acc:0.970]
Epoch [43/120    avg_loss:0.139, val_acc:0.968]
Epoch [44/120    avg_loss:0.113, val_acc:0.970]
Epoch [45/120    avg_loss:0.142, val_acc:0.970]
Epoch [46/120    avg_loss:0.115, val_acc:0.976]
Epoch [47/120    avg_loss:0.122, val_acc:0.982]
Epoch [48/120    avg_loss:0.108, val_acc:0.980]
Epoch [49/120    avg_loss:0.081, val_acc:0.976]
Epoch [50/120    avg_loss:0.079, val_acc:0.974]
Epoch [51/120    avg_loss:0.162, val_acc:0.982]
Epoch [52/120    avg_loss:0.086, val_acc:0.986]
Epoch [53/120    avg_loss:0.104, val_acc:0.980]
Epoch [54/120    avg_loss:0.118, val_acc:0.990]
Epoch [55/120    avg_loss:0.070, val_acc:0.978]
Epoch [56/120    avg_loss:0.080, val_acc:0.972]
Epoch [57/120    avg_loss:0.071, val_acc:0.982]
Epoch [58/120    avg_loss:0.117, val_acc:0.974]
Epoch [59/120    avg_loss:0.080, val_acc:0.988]
Epoch [60/120    avg_loss:0.053, val_acc:0.986]
Epoch [61/120    avg_loss:0.070, val_acc:0.986]
Epoch [62/120    avg_loss:0.058, val_acc:0.980]
Epoch [63/120    avg_loss:0.102, val_acc:0.988]
Epoch [64/120    avg_loss:0.054, val_acc:0.986]
Epoch [65/120    avg_loss:0.038, val_acc:0.990]
Epoch [66/120    avg_loss:0.034, val_acc:0.992]
Epoch [67/120    avg_loss:0.057, val_acc:0.978]
Epoch [68/120    avg_loss:0.049, val_acc:0.972]
Epoch [69/120    avg_loss:0.059, val_acc:0.976]
Epoch [70/120    avg_loss:0.080, val_acc:0.980]
Epoch [71/120    avg_loss:0.036, val_acc:0.988]
Epoch [72/120    avg_loss:0.051, val_acc:0.988]
Epoch [73/120    avg_loss:0.029, val_acc:0.984]
Epoch [74/120    avg_loss:0.049, val_acc:0.992]
Epoch [75/120    avg_loss:0.029, val_acc:0.988]
Epoch [76/120    avg_loss:0.036, val_acc:0.992]
Epoch [77/120    avg_loss:0.039, val_acc:0.988]
Epoch [78/120    avg_loss:0.031, val_acc:0.992]
Epoch [79/120    avg_loss:0.022, val_acc:0.988]
Epoch [80/120    avg_loss:0.041, val_acc:0.984]
Epoch [81/120    avg_loss:0.041, val_acc:0.986]
Epoch [82/120    avg_loss:0.059, val_acc:0.984]
Epoch [83/120    avg_loss:0.074, val_acc:0.986]
Epoch [84/120    avg_loss:0.062, val_acc:0.990]
Epoch [85/120    avg_loss:0.037, val_acc:0.986]
Epoch [86/120    avg_loss:0.030, val_acc:0.990]
Epoch [87/120    avg_loss:0.020, val_acc:0.990]
Epoch [88/120    avg_loss:0.029, val_acc:0.992]
Epoch [89/120    avg_loss:0.043, val_acc:0.986]
Epoch [90/120    avg_loss:0.028, val_acc:0.990]
Epoch [91/120    avg_loss:0.024, val_acc:0.990]
Epoch [92/120    avg_loss:0.030, val_acc:0.982]
Epoch [93/120    avg_loss:0.020, val_acc:0.992]
Epoch [94/120    avg_loss:0.025, val_acc:0.992]
Epoch [95/120    avg_loss:0.020, val_acc:0.992]
Epoch [96/120    avg_loss:0.013, val_acc:0.992]
Epoch [97/120    avg_loss:0.018, val_acc:0.992]
Epoch [98/120    avg_loss:0.020, val_acc:0.992]
Epoch [99/120    avg_loss:0.022, val_acc:0.994]
Epoch [100/120    avg_loss:0.011, val_acc:0.994]
Epoch [101/120    avg_loss:0.011, val_acc:0.990]
Epoch [102/120    avg_loss:0.114, val_acc:0.966]
Epoch [103/120    avg_loss:0.110, val_acc:0.994]
Epoch [104/120    avg_loss:0.024, val_acc:0.988]
Epoch [105/120    avg_loss:0.022, val_acc:0.994]
Epoch [106/120    avg_loss:0.012, val_acc:0.986]
Epoch [107/120    avg_loss:0.011, val_acc:0.990]
Epoch [108/120    avg_loss:0.014, val_acc:0.994]
Epoch [109/120    avg_loss:0.014, val_acc:0.992]
Epoch [110/120    avg_loss:0.014, val_acc:0.986]
Epoch [111/120    avg_loss:0.033, val_acc:0.986]
Epoch [112/120    avg_loss:0.023, val_acc:0.990]
Epoch [113/120    avg_loss:0.014, val_acc:0.992]
Epoch [114/120    avg_loss:0.009, val_acc:0.994]
Epoch [115/120    avg_loss:0.012, val_acc:0.990]
Epoch [116/120    avg_loss:0.007, val_acc:0.990]
Epoch [117/120    avg_loss:0.008, val_acc:0.992]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.009, val_acc:0.992]
Epoch [120/120    avg_loss:0.006, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 203  15   0   0   0   0   0   0   9   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 0.99780541 0.99319728 1.         0.91647856 0.90410959
 0.99277108 0.98378378 1.         1.         1.         1.
 0.99016393 1.        ]

Kappa:
0.9897921799807679
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1b8194a940>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.338, val_acc:0.530]
Epoch [2/120    avg_loss:1.840, val_acc:0.603]
Epoch [3/120    avg_loss:1.473, val_acc:0.647]
Epoch [4/120    avg_loss:1.160, val_acc:0.817]
Epoch [5/120    avg_loss:1.019, val_acc:0.712]
Epoch [6/120    avg_loss:0.832, val_acc:0.863]
Epoch [7/120    avg_loss:0.778, val_acc:0.821]
Epoch [8/120    avg_loss:0.749, val_acc:0.760]
Epoch [9/120    avg_loss:0.675, val_acc:0.859]
Epoch [10/120    avg_loss:0.587, val_acc:0.869]
Epoch [11/120    avg_loss:0.578, val_acc:0.829]
Epoch [12/120    avg_loss:0.526, val_acc:0.843]
Epoch [13/120    avg_loss:0.477, val_acc:0.889]
Epoch [14/120    avg_loss:0.399, val_acc:0.933]
Epoch [15/120    avg_loss:0.406, val_acc:0.887]
Epoch [16/120    avg_loss:0.423, val_acc:0.956]
Epoch [17/120    avg_loss:0.355, val_acc:0.923]
Epoch [18/120    avg_loss:0.365, val_acc:0.929]
Epoch [19/120    avg_loss:0.343, val_acc:0.944]
Epoch [20/120    avg_loss:0.295, val_acc:0.942]
Epoch [21/120    avg_loss:0.287, val_acc:0.960]
Epoch [22/120    avg_loss:0.240, val_acc:0.956]
Epoch [23/120    avg_loss:0.245, val_acc:0.931]
Epoch [24/120    avg_loss:0.285, val_acc:0.964]
Epoch [25/120    avg_loss:0.194, val_acc:0.962]
Epoch [26/120    avg_loss:0.238, val_acc:0.974]
Epoch [27/120    avg_loss:0.255, val_acc:0.903]
Epoch [28/120    avg_loss:0.247, val_acc:0.958]
Epoch [29/120    avg_loss:0.219, val_acc:0.964]
Epoch [30/120    avg_loss:0.155, val_acc:0.970]
Epoch [31/120    avg_loss:0.127, val_acc:0.976]
Epoch [32/120    avg_loss:0.223, val_acc:0.940]
Epoch [33/120    avg_loss:0.179, val_acc:0.960]
Epoch [34/120    avg_loss:0.140, val_acc:0.986]
Epoch [35/120    avg_loss:0.125, val_acc:0.946]
Epoch [36/120    avg_loss:0.125, val_acc:0.976]
Epoch [37/120    avg_loss:0.142, val_acc:0.948]
Epoch [38/120    avg_loss:0.110, val_acc:0.966]
Epoch [39/120    avg_loss:0.109, val_acc:0.972]
Epoch [40/120    avg_loss:0.092, val_acc:0.980]
Epoch [41/120    avg_loss:0.091, val_acc:0.988]
Epoch [42/120    avg_loss:0.142, val_acc:0.940]
Epoch [43/120    avg_loss:0.117, val_acc:0.978]
Epoch [44/120    avg_loss:0.121, val_acc:0.966]
Epoch [45/120    avg_loss:0.076, val_acc:0.986]
Epoch [46/120    avg_loss:0.171, val_acc:0.966]
Epoch [47/120    avg_loss:0.186, val_acc:0.966]
Epoch [48/120    avg_loss:0.163, val_acc:0.946]
Epoch [49/120    avg_loss:0.086, val_acc:0.974]
Epoch [50/120    avg_loss:0.086, val_acc:0.978]
Epoch [51/120    avg_loss:0.059, val_acc:0.986]
Epoch [52/120    avg_loss:0.064, val_acc:0.964]
Epoch [53/120    avg_loss:0.077, val_acc:0.980]
Epoch [54/120    avg_loss:0.081, val_acc:0.968]
Epoch [55/120    avg_loss:0.072, val_acc:0.976]
Epoch [56/120    avg_loss:0.047, val_acc:0.982]
Epoch [57/120    avg_loss:0.042, val_acc:0.990]
Epoch [58/120    avg_loss:0.041, val_acc:0.990]
Epoch [59/120    avg_loss:0.035, val_acc:0.988]
Epoch [60/120    avg_loss:0.033, val_acc:0.988]
Epoch [61/120    avg_loss:0.031, val_acc:0.992]
Epoch [62/120    avg_loss:0.029, val_acc:0.992]
Epoch [63/120    avg_loss:0.034, val_acc:0.992]
Epoch [64/120    avg_loss:0.028, val_acc:0.992]
Epoch [65/120    avg_loss:0.028, val_acc:0.992]
Epoch [66/120    avg_loss:0.026, val_acc:0.994]
Epoch [67/120    avg_loss:0.025, val_acc:0.994]
Epoch [68/120    avg_loss:0.029, val_acc:0.996]
Epoch [69/120    avg_loss:0.033, val_acc:0.994]
Epoch [70/120    avg_loss:0.023, val_acc:0.994]
Epoch [71/120    avg_loss:0.024, val_acc:0.994]
Epoch [72/120    avg_loss:0.026, val_acc:0.994]
Epoch [73/120    avg_loss:0.026, val_acc:0.994]
Epoch [74/120    avg_loss:0.026, val_acc:0.994]
Epoch [75/120    avg_loss:0.029, val_acc:0.994]
Epoch [76/120    avg_loss:0.028, val_acc:0.994]
Epoch [77/120    avg_loss:0.028, val_acc:0.994]
Epoch [78/120    avg_loss:0.027, val_acc:0.994]
Epoch [79/120    avg_loss:0.036, val_acc:0.994]
Epoch [80/120    avg_loss:0.026, val_acc:0.992]
Epoch [81/120    avg_loss:0.033, val_acc:0.992]
Epoch [82/120    avg_loss:0.027, val_acc:0.992]
Epoch [83/120    avg_loss:0.023, val_acc:0.994]
Epoch [84/120    avg_loss:0.023, val_acc:0.994]
Epoch [85/120    avg_loss:0.024, val_acc:0.994]
Epoch [86/120    avg_loss:0.020, val_acc:0.994]
Epoch [87/120    avg_loss:0.024, val_acc:0.994]
Epoch [88/120    avg_loss:0.027, val_acc:0.994]
Epoch [89/120    avg_loss:0.030, val_acc:0.994]
Epoch [90/120    avg_loss:0.025, val_acc:0.994]
Epoch [91/120    avg_loss:0.027, val_acc:0.994]
Epoch [92/120    avg_loss:0.025, val_acc:0.994]
Epoch [93/120    avg_loss:0.023, val_acc:0.994]
Epoch [94/120    avg_loss:0.024, val_acc:0.994]
Epoch [95/120    avg_loss:0.023, val_acc:0.994]
Epoch [96/120    avg_loss:0.023, val_acc:0.994]
Epoch [97/120    avg_loss:0.020, val_acc:0.994]
Epoch [98/120    avg_loss:0.020, val_acc:0.994]
Epoch [99/120    avg_loss:0.021, val_acc:0.994]
Epoch [100/120    avg_loss:0.029, val_acc:0.994]
Epoch [101/120    avg_loss:0.024, val_acc:0.994]
Epoch [102/120    avg_loss:0.025, val_acc:0.994]
Epoch [103/120    avg_loss:0.026, val_acc:0.994]
Epoch [104/120    avg_loss:0.023, val_acc:0.994]
Epoch [105/120    avg_loss:0.024, val_acc:0.994]
Epoch [106/120    avg_loss:0.027, val_acc:0.994]
Epoch [107/120    avg_loss:0.017, val_acc:0.994]
Epoch [108/120    avg_loss:0.030, val_acc:0.994]
Epoch [109/120    avg_loss:0.019, val_acc:0.994]
Epoch [110/120    avg_loss:0.030, val_acc:0.994]
Epoch [111/120    avg_loss:0.026, val_acc:0.994]
Epoch [112/120    avg_loss:0.021, val_acc:0.994]
Epoch [113/120    avg_loss:0.025, val_acc:0.994]
Epoch [114/120    avg_loss:0.023, val_acc:0.994]
Epoch [115/120    avg_loss:0.027, val_acc:0.994]
Epoch [116/120    avg_loss:0.019, val_acc:0.994]
Epoch [117/120    avg_loss:0.023, val_acc:0.994]
Epoch [118/120    avg_loss:0.019, val_acc:0.994]
Epoch [119/120    avg_loss:0.024, val_acc:0.994]
Epoch [120/120    avg_loss:0.025, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  13   0   0   0   0   0   0   2   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 1.         0.99319728 1.         0.94222222 0.91780822
 1.         0.98378378 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.993115706792582
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5af22eb898>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.298, val_acc:0.544]
Epoch [2/120    avg_loss:1.801, val_acc:0.613]
Epoch [3/120    avg_loss:1.478, val_acc:0.651]
Epoch [4/120    avg_loss:1.181, val_acc:0.762]
Epoch [5/120    avg_loss:0.957, val_acc:0.768]
Epoch [6/120    avg_loss:0.910, val_acc:0.746]
Epoch [7/120    avg_loss:0.777, val_acc:0.865]
Epoch [8/120    avg_loss:0.704, val_acc:0.885]
Epoch [9/120    avg_loss:0.589, val_acc:0.893]
Epoch [10/120    avg_loss:0.475, val_acc:0.919]
Epoch [11/120    avg_loss:0.496, val_acc:0.948]
Epoch [12/120    avg_loss:0.405, val_acc:0.944]
Epoch [13/120    avg_loss:0.375, val_acc:0.925]
Epoch [14/120    avg_loss:0.346, val_acc:0.871]
Epoch [15/120    avg_loss:0.366, val_acc:0.946]
Epoch [16/120    avg_loss:0.351, val_acc:0.899]
Epoch [17/120    avg_loss:0.363, val_acc:0.948]
Epoch [18/120    avg_loss:0.266, val_acc:0.952]
Epoch [19/120    avg_loss:0.350, val_acc:0.940]
Epoch [20/120    avg_loss:0.296, val_acc:0.966]
Epoch [21/120    avg_loss:0.261, val_acc:0.935]
Epoch [22/120    avg_loss:0.249, val_acc:0.931]
Epoch [23/120    avg_loss:0.219, val_acc:0.960]
Epoch [24/120    avg_loss:0.218, val_acc:0.869]
Epoch [25/120    avg_loss:0.243, val_acc:0.964]
Epoch [26/120    avg_loss:0.174, val_acc:0.958]
Epoch [27/120    avg_loss:0.178, val_acc:0.966]
Epoch [28/120    avg_loss:0.150, val_acc:0.956]
Epoch [29/120    avg_loss:0.176, val_acc:0.968]
Epoch [30/120    avg_loss:0.209, val_acc:0.952]
Epoch [31/120    avg_loss:0.159, val_acc:0.935]
Epoch [32/120    avg_loss:0.185, val_acc:0.948]
Epoch [33/120    avg_loss:0.191, val_acc:0.956]
Epoch [34/120    avg_loss:0.179, val_acc:0.968]
Epoch [35/120    avg_loss:0.147, val_acc:0.964]
Epoch [36/120    avg_loss:0.130, val_acc:0.968]
Epoch [37/120    avg_loss:0.120, val_acc:0.974]
Epoch [38/120    avg_loss:0.119, val_acc:0.966]
Epoch [39/120    avg_loss:0.089, val_acc:0.968]
Epoch [40/120    avg_loss:0.085, val_acc:0.966]
Epoch [41/120    avg_loss:0.164, val_acc:0.968]
Epoch [42/120    avg_loss:0.164, val_acc:0.962]
Epoch [43/120    avg_loss:0.085, val_acc:0.978]
Epoch [44/120    avg_loss:0.069, val_acc:0.974]
Epoch [45/120    avg_loss:0.218, val_acc:0.956]
Epoch [46/120    avg_loss:0.129, val_acc:0.958]
Epoch [47/120    avg_loss:0.079, val_acc:0.982]
Epoch [48/120    avg_loss:0.098, val_acc:0.968]
Epoch [49/120    avg_loss:0.206, val_acc:0.935]
Epoch [50/120    avg_loss:0.221, val_acc:0.946]
Epoch [51/120    avg_loss:0.179, val_acc:0.962]
Epoch [52/120    avg_loss:0.111, val_acc:0.970]
Epoch [53/120    avg_loss:0.092, val_acc:0.960]
Epoch [54/120    avg_loss:0.089, val_acc:0.976]
Epoch [55/120    avg_loss:0.072, val_acc:0.970]
Epoch [56/120    avg_loss:0.067, val_acc:0.974]
Epoch [57/120    avg_loss:0.044, val_acc:0.978]
Epoch [58/120    avg_loss:0.067, val_acc:0.962]
Epoch [59/120    avg_loss:0.054, val_acc:0.984]
Epoch [60/120    avg_loss:0.038, val_acc:0.980]
Epoch [61/120    avg_loss:0.036, val_acc:0.972]
Epoch [62/120    avg_loss:0.086, val_acc:0.942]
Epoch [63/120    avg_loss:0.115, val_acc:0.980]
Epoch [64/120    avg_loss:0.068, val_acc:0.978]
Epoch [65/120    avg_loss:0.056, val_acc:0.984]
Epoch [66/120    avg_loss:0.062, val_acc:0.974]
Epoch [67/120    avg_loss:0.064, val_acc:0.982]
Epoch [68/120    avg_loss:0.054, val_acc:0.970]
Epoch [69/120    avg_loss:0.043, val_acc:0.984]
Epoch [70/120    avg_loss:0.029, val_acc:0.984]
Epoch [71/120    avg_loss:0.056, val_acc:0.974]
Epoch [72/120    avg_loss:0.150, val_acc:0.968]
Epoch [73/120    avg_loss:0.054, val_acc:0.978]
Epoch [74/120    avg_loss:0.040, val_acc:0.978]
Epoch [75/120    avg_loss:0.031, val_acc:0.986]
Epoch [76/120    avg_loss:0.024, val_acc:0.986]
Epoch [77/120    avg_loss:0.016, val_acc:0.984]
Epoch [78/120    avg_loss:0.020, val_acc:0.988]
Epoch [79/120    avg_loss:0.021, val_acc:0.990]
Epoch [80/120    avg_loss:0.016, val_acc:0.988]
Epoch [81/120    avg_loss:0.020, val_acc:0.986]
Epoch [82/120    avg_loss:0.019, val_acc:0.984]
Epoch [83/120    avg_loss:0.024, val_acc:0.988]
Epoch [84/120    avg_loss:0.015, val_acc:0.982]
Epoch [85/120    avg_loss:0.025, val_acc:0.984]
Epoch [86/120    avg_loss:0.021, val_acc:0.992]
Epoch [87/120    avg_loss:0.017, val_acc:0.990]
Epoch [88/120    avg_loss:0.010, val_acc:0.982]
Epoch [89/120    avg_loss:0.010, val_acc:0.988]
Epoch [90/120    avg_loss:0.011, val_acc:0.990]
Epoch [91/120    avg_loss:0.011, val_acc:0.986]
Epoch [92/120    avg_loss:0.009, val_acc:0.982]
Epoch [93/120    avg_loss:0.011, val_acc:0.984]
Epoch [94/120    avg_loss:0.045, val_acc:0.982]
Epoch [95/120    avg_loss:0.015, val_acc:0.982]
Epoch [96/120    avg_loss:0.012, val_acc:0.982]
Epoch [97/120    avg_loss:0.012, val_acc:0.984]
Epoch [98/120    avg_loss:0.016, val_acc:0.986]
Epoch [99/120    avg_loss:0.033, val_acc:0.976]
Epoch [100/120    avg_loss:0.018, val_acc:0.980]
Epoch [101/120    avg_loss:0.012, val_acc:0.984]
Epoch [102/120    avg_loss:0.013, val_acc:0.984]
Epoch [103/120    avg_loss:0.011, val_acc:0.982]
Epoch [104/120    avg_loss:0.015, val_acc:0.982]
Epoch [105/120    avg_loss:0.011, val_acc:0.984]
Epoch [106/120    avg_loss:0.011, val_acc:0.984]
Epoch [107/120    avg_loss:0.007, val_acc:0.984]
Epoch [108/120    avg_loss:0.010, val_acc:0.984]
Epoch [109/120    avg_loss:0.009, val_acc:0.982]
Epoch [110/120    avg_loss:0.006, val_acc:0.982]
Epoch [111/120    avg_loss:0.010, val_acc:0.982]
Epoch [112/120    avg_loss:0.011, val_acc:0.980]
Epoch [113/120    avg_loss:0.014, val_acc:0.980]
Epoch [114/120    avg_loss:0.012, val_acc:0.980]
Epoch [115/120    avg_loss:0.009, val_acc:0.980]
Epoch [116/120    avg_loss:0.007, val_acc:0.980]
Epoch [117/120    avg_loss:0.012, val_acc:0.980]
Epoch [118/120    avg_loss:0.010, val_acc:0.980]
Epoch [119/120    avg_loss:0.008, val_acc:0.980]
Epoch [120/120    avg_loss:0.008, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212   9   0   0   0   0   0   0   6   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.95522388059702

F1 scores:
[       nan 0.996337   0.98426966 0.99782135 0.9197397  0.89208633
 0.98800959 0.96132597 1.         1.         1.         1.
 0.99342105 1.        ]

Kappa:
0.9883678277023262
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa74a2b37f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.299, val_acc:0.639]
Epoch [2/120    avg_loss:1.761, val_acc:0.734]
Epoch [3/120    avg_loss:1.360, val_acc:0.738]
Epoch [4/120    avg_loss:1.071, val_acc:0.742]
Epoch [5/120    avg_loss:0.974, val_acc:0.800]
Epoch [6/120    avg_loss:0.858, val_acc:0.780]
Epoch [7/120    avg_loss:0.837, val_acc:0.831]
Epoch [8/120    avg_loss:0.718, val_acc:0.794]
Epoch [9/120    avg_loss:0.719, val_acc:0.835]
Epoch [10/120    avg_loss:0.584, val_acc:0.851]
Epoch [11/120    avg_loss:0.602, val_acc:0.887]
Epoch [12/120    avg_loss:0.518, val_acc:0.877]
Epoch [13/120    avg_loss:0.492, val_acc:0.861]
Epoch [14/120    avg_loss:0.459, val_acc:0.895]
Epoch [15/120    avg_loss:0.443, val_acc:0.903]
Epoch [16/120    avg_loss:0.371, val_acc:0.808]
Epoch [17/120    avg_loss:0.422, val_acc:0.901]
Epoch [18/120    avg_loss:0.362, val_acc:0.925]
Epoch [19/120    avg_loss:0.328, val_acc:0.909]
Epoch [20/120    avg_loss:0.363, val_acc:0.933]
Epoch [21/120    avg_loss:0.326, val_acc:0.942]
Epoch [22/120    avg_loss:0.344, val_acc:0.923]
Epoch [23/120    avg_loss:0.248, val_acc:0.921]
Epoch [24/120    avg_loss:0.350, val_acc:0.923]
Epoch [25/120    avg_loss:0.279, val_acc:0.942]
Epoch [26/120    avg_loss:0.251, val_acc:0.931]
Epoch [27/120    avg_loss:0.302, val_acc:0.927]
Epoch [28/120    avg_loss:0.224, val_acc:0.956]
Epoch [29/120    avg_loss:0.253, val_acc:0.938]
Epoch [30/120    avg_loss:0.215, val_acc:0.950]
Epoch [31/120    avg_loss:0.197, val_acc:0.938]
Epoch [32/120    avg_loss:0.177, val_acc:0.942]
Epoch [33/120    avg_loss:0.196, val_acc:0.964]
Epoch [34/120    avg_loss:0.142, val_acc:0.954]
Epoch [35/120    avg_loss:0.212, val_acc:0.972]
Epoch [36/120    avg_loss:0.197, val_acc:0.954]
Epoch [37/120    avg_loss:0.134, val_acc:0.972]
Epoch [38/120    avg_loss:0.149, val_acc:0.968]
Epoch [39/120    avg_loss:0.187, val_acc:0.974]
Epoch [40/120    avg_loss:0.184, val_acc:0.954]
Epoch [41/120    avg_loss:0.136, val_acc:0.968]
Epoch [42/120    avg_loss:0.101, val_acc:0.982]
Epoch [43/120    avg_loss:0.125, val_acc:0.970]
Epoch [44/120    avg_loss:0.132, val_acc:0.972]
Epoch [45/120    avg_loss:0.083, val_acc:0.986]
Epoch [46/120    avg_loss:0.099, val_acc:0.988]
Epoch [47/120    avg_loss:0.151, val_acc:0.873]
Epoch [48/120    avg_loss:0.199, val_acc:0.972]
Epoch [49/120    avg_loss:0.116, val_acc:0.976]
Epoch [50/120    avg_loss:0.080, val_acc:0.980]
Epoch [51/120    avg_loss:0.084, val_acc:0.984]
Epoch [52/120    avg_loss:0.098, val_acc:0.962]
Epoch [53/120    avg_loss:0.117, val_acc:0.978]
Epoch [54/120    avg_loss:0.138, val_acc:0.964]
Epoch [55/120    avg_loss:0.100, val_acc:0.976]
Epoch [56/120    avg_loss:0.077, val_acc:0.980]
Epoch [57/120    avg_loss:0.061, val_acc:0.974]
Epoch [58/120    avg_loss:0.083, val_acc:0.972]
Epoch [59/120    avg_loss:0.059, val_acc:0.988]
Epoch [60/120    avg_loss:0.037, val_acc:0.990]
Epoch [61/120    avg_loss:0.052, val_acc:0.988]
Epoch [62/120    avg_loss:0.044, val_acc:0.980]
Epoch [63/120    avg_loss:0.067, val_acc:0.984]
Epoch [64/120    avg_loss:0.040, val_acc:0.988]
Epoch [65/120    avg_loss:0.035, val_acc:0.984]
Epoch [66/120    avg_loss:0.044, val_acc:0.986]
Epoch [67/120    avg_loss:0.076, val_acc:0.954]
Epoch [68/120    avg_loss:0.041, val_acc:0.984]
Epoch [69/120    avg_loss:0.043, val_acc:0.988]
Epoch [70/120    avg_loss:0.052, val_acc:0.970]
Epoch [71/120    avg_loss:0.040, val_acc:0.986]
Epoch [72/120    avg_loss:0.042, val_acc:0.990]
Epoch [73/120    avg_loss:0.032, val_acc:0.988]
Epoch [74/120    avg_loss:0.021, val_acc:0.988]
Epoch [75/120    avg_loss:0.027, val_acc:0.988]
Epoch [76/120    avg_loss:0.030, val_acc:0.986]
Epoch [77/120    avg_loss:0.038, val_acc:0.988]
Epoch [78/120    avg_loss:0.018, val_acc:0.990]
Epoch [79/120    avg_loss:0.010, val_acc:0.992]
Epoch [80/120    avg_loss:0.009, val_acc:0.992]
Epoch [81/120    avg_loss:0.021, val_acc:0.988]
Epoch [82/120    avg_loss:0.014, val_acc:0.984]
Epoch [83/120    avg_loss:0.012, val_acc:0.986]
Epoch [84/120    avg_loss:0.019, val_acc:0.988]
Epoch [85/120    avg_loss:0.014, val_acc:0.990]
Epoch [86/120    avg_loss:0.013, val_acc:0.992]
Epoch [87/120    avg_loss:0.014, val_acc:0.990]
Epoch [88/120    avg_loss:0.013, val_acc:0.988]
Epoch [89/120    avg_loss:0.014, val_acc:0.992]
Epoch [90/120    avg_loss:0.014, val_acc:0.994]
Epoch [91/120    avg_loss:0.045, val_acc:0.984]
Epoch [92/120    avg_loss:0.027, val_acc:0.984]
Epoch [93/120    avg_loss:0.033, val_acc:0.982]
Epoch [94/120    avg_loss:0.031, val_acc:0.994]
Epoch [95/120    avg_loss:0.023, val_acc:0.994]
Epoch [96/120    avg_loss:0.037, val_acc:0.990]
Epoch [97/120    avg_loss:0.049, val_acc:0.990]
Epoch [98/120    avg_loss:0.057, val_acc:0.986]
Epoch [99/120    avg_loss:0.020, val_acc:0.992]
Epoch [100/120    avg_loss:0.012, val_acc:0.996]
Epoch [101/120    avg_loss:0.020, val_acc:0.994]
Epoch [102/120    avg_loss:0.010, val_acc:0.996]
Epoch [103/120    avg_loss:0.016, val_acc:0.998]
Epoch [104/120    avg_loss:0.012, val_acc:0.996]
Epoch [105/120    avg_loss:0.044, val_acc:0.994]
Epoch [106/120    avg_loss:0.029, val_acc:0.992]
Epoch [107/120    avg_loss:0.034, val_acc:0.986]
Epoch [108/120    avg_loss:0.016, val_acc:0.994]
Epoch [109/120    avg_loss:0.016, val_acc:0.996]
Epoch [110/120    avg_loss:0.011, val_acc:0.990]
Epoch [111/120    avg_loss:0.019, val_acc:0.998]
Epoch [112/120    avg_loss:0.008, val_acc:0.998]
Epoch [113/120    avg_loss:0.017, val_acc:0.992]
Epoch [114/120    avg_loss:0.027, val_acc:0.992]
Epoch [115/120    avg_loss:0.030, val_acc:0.990]
Epoch [116/120    avg_loss:0.009, val_acc:0.992]
Epoch [117/120    avg_loss:0.013, val_acc:0.996]
Epoch [118/120    avg_loss:0.009, val_acc:0.996]
Epoch [119/120    avg_loss:0.008, val_acc:0.994]
Epoch [120/120    avg_loss:0.005, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 211  18   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 1.         0.99319728 0.9569161  0.93709328 0.96345515
 1.         0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9924040691069775
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc1dc369908>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.290, val_acc:0.532]
Epoch [2/120    avg_loss:1.796, val_acc:0.661]
Epoch [3/120    avg_loss:1.363, val_acc:0.671]
Epoch [4/120    avg_loss:1.101, val_acc:0.806]
Epoch [5/120    avg_loss:0.975, val_acc:0.782]
Epoch [6/120    avg_loss:0.818, val_acc:0.790]
Epoch [7/120    avg_loss:0.721, val_acc:0.738]
Epoch [8/120    avg_loss:0.692, val_acc:0.871]
Epoch [9/120    avg_loss:0.578, val_acc:0.831]
Epoch [10/120    avg_loss:0.578, val_acc:0.849]
Epoch [11/120    avg_loss:0.501, val_acc:0.909]
Epoch [12/120    avg_loss:0.458, val_acc:0.921]
Epoch [13/120    avg_loss:0.428, val_acc:0.909]
Epoch [14/120    avg_loss:0.413, val_acc:0.905]
Epoch [15/120    avg_loss:0.428, val_acc:0.919]
Epoch [16/120    avg_loss:0.387, val_acc:0.897]
Epoch [17/120    avg_loss:0.354, val_acc:0.859]
Epoch [18/120    avg_loss:0.344, val_acc:0.907]
Epoch [19/120    avg_loss:0.350, val_acc:0.921]
Epoch [20/120    avg_loss:0.312, val_acc:0.897]
Epoch [21/120    avg_loss:0.302, val_acc:0.925]
Epoch [22/120    avg_loss:0.258, val_acc:0.929]
Epoch [23/120    avg_loss:0.258, val_acc:0.885]
Epoch [24/120    avg_loss:0.282, val_acc:0.923]
Epoch [25/120    avg_loss:0.302, val_acc:0.954]
Epoch [26/120    avg_loss:0.272, val_acc:0.952]
Epoch [27/120    avg_loss:0.257, val_acc:0.946]
Epoch [28/120    avg_loss:0.228, val_acc:0.970]
Epoch [29/120    avg_loss:0.201, val_acc:0.935]
Epoch [30/120    avg_loss:0.230, val_acc:0.960]
Epoch [31/120    avg_loss:0.206, val_acc:0.931]
Epoch [32/120    avg_loss:0.254, val_acc:0.952]
Epoch [33/120    avg_loss:0.173, val_acc:0.962]
Epoch [34/120    avg_loss:0.146, val_acc:0.954]
Epoch [35/120    avg_loss:0.136, val_acc:0.974]
Epoch [36/120    avg_loss:0.164, val_acc:0.978]
Epoch [37/120    avg_loss:0.142, val_acc:0.976]
Epoch [38/120    avg_loss:0.190, val_acc:0.952]
Epoch [39/120    avg_loss:0.145, val_acc:0.992]
Epoch [40/120    avg_loss:0.175, val_acc:0.927]
Epoch [41/120    avg_loss:0.163, val_acc:0.960]
Epoch [42/120    avg_loss:0.112, val_acc:0.984]
Epoch [43/120    avg_loss:0.096, val_acc:0.954]
Epoch [44/120    avg_loss:0.142, val_acc:0.970]
Epoch [45/120    avg_loss:0.101, val_acc:0.978]
Epoch [46/120    avg_loss:0.075, val_acc:0.986]
Epoch [47/120    avg_loss:0.097, val_acc:0.988]
Epoch [48/120    avg_loss:0.098, val_acc:0.978]
Epoch [49/120    avg_loss:0.119, val_acc:0.986]
Epoch [50/120    avg_loss:0.099, val_acc:0.972]
Epoch [51/120    avg_loss:0.109, val_acc:0.950]
Epoch [52/120    avg_loss:0.100, val_acc:0.976]
Epoch [53/120    avg_loss:0.076, val_acc:0.986]
Epoch [54/120    avg_loss:0.056, val_acc:0.990]
Epoch [55/120    avg_loss:0.049, val_acc:0.990]
Epoch [56/120    avg_loss:0.049, val_acc:0.990]
Epoch [57/120    avg_loss:0.041, val_acc:0.992]
Epoch [58/120    avg_loss:0.043, val_acc:0.988]
Epoch [59/120    avg_loss:0.046, val_acc:0.988]
Epoch [60/120    avg_loss:0.046, val_acc:0.990]
Epoch [61/120    avg_loss:0.044, val_acc:0.992]
Epoch [62/120    avg_loss:0.041, val_acc:0.994]
Epoch [63/120    avg_loss:0.049, val_acc:0.994]
Epoch [64/120    avg_loss:0.046, val_acc:0.994]
Epoch [65/120    avg_loss:0.046, val_acc:0.994]
Epoch [66/120    avg_loss:0.051, val_acc:0.994]
Epoch [67/120    avg_loss:0.043, val_acc:0.994]
Epoch [68/120    avg_loss:0.043, val_acc:0.992]
Epoch [69/120    avg_loss:0.033, val_acc:0.994]
Epoch [70/120    avg_loss:0.031, val_acc:0.994]
Epoch [71/120    avg_loss:0.032, val_acc:0.992]
Epoch [72/120    avg_loss:0.043, val_acc:0.994]
Epoch [73/120    avg_loss:0.033, val_acc:0.994]
Epoch [74/120    avg_loss:0.032, val_acc:0.994]
Epoch [75/120    avg_loss:0.032, val_acc:0.994]
Epoch [76/120    avg_loss:0.037, val_acc:0.994]
Epoch [77/120    avg_loss:0.038, val_acc:0.990]
Epoch [78/120    avg_loss:0.036, val_acc:0.992]
Epoch [79/120    avg_loss:0.034, val_acc:0.994]
Epoch [80/120    avg_loss:0.040, val_acc:0.996]
Epoch [81/120    avg_loss:0.027, val_acc:0.994]
Epoch [82/120    avg_loss:0.025, val_acc:0.994]
Epoch [83/120    avg_loss:0.032, val_acc:0.994]
Epoch [84/120    avg_loss:0.038, val_acc:0.994]
Epoch [85/120    avg_loss:0.032, val_acc:0.996]
Epoch [86/120    avg_loss:0.033, val_acc:0.988]
Epoch [87/120    avg_loss:0.038, val_acc:0.992]
Epoch [88/120    avg_loss:0.026, val_acc:0.992]
Epoch [89/120    avg_loss:0.026, val_acc:0.992]
Epoch [90/120    avg_loss:0.024, val_acc:0.992]
Epoch [91/120    avg_loss:0.034, val_acc:0.992]
Epoch [92/120    avg_loss:0.031, val_acc:0.994]
Epoch [93/120    avg_loss:0.035, val_acc:0.992]
Epoch [94/120    avg_loss:0.032, val_acc:0.994]
Epoch [95/120    avg_loss:0.023, val_acc:0.994]
Epoch [96/120    avg_loss:0.023, val_acc:0.992]
Epoch [97/120    avg_loss:0.027, val_acc:0.996]
Epoch [98/120    avg_loss:0.026, val_acc:0.994]
Epoch [99/120    avg_loss:0.026, val_acc:0.994]
Epoch [100/120    avg_loss:0.024, val_acc:0.994]
Epoch [101/120    avg_loss:0.027, val_acc:0.996]
Epoch [102/120    avg_loss:0.027, val_acc:0.996]
Epoch [103/120    avg_loss:0.023, val_acc:0.996]
Epoch [104/120    avg_loss:0.029, val_acc:0.996]
Epoch [105/120    avg_loss:0.021, val_acc:0.994]
Epoch [106/120    avg_loss:0.024, val_acc:0.994]
Epoch [107/120    avg_loss:0.028, val_acc:0.994]
Epoch [108/120    avg_loss:0.025, val_acc:0.994]
Epoch [109/120    avg_loss:0.026, val_acc:0.992]
Epoch [110/120    avg_loss:0.023, val_acc:0.994]
Epoch [111/120    avg_loss:0.024, val_acc:0.994]
Epoch [112/120    avg_loss:0.022, val_acc:0.994]
Epoch [113/120    avg_loss:0.025, val_acc:0.992]
Epoch [114/120    avg_loss:0.022, val_acc:0.994]
Epoch [115/120    avg_loss:0.023, val_acc:0.994]
Epoch [116/120    avg_loss:0.023, val_acc:0.994]
Epoch [117/120    avg_loss:0.026, val_acc:0.996]
Epoch [118/120    avg_loss:0.023, val_acc:0.994]
Epoch [119/120    avg_loss:0.028, val_acc:0.992]
Epoch [120/120    avg_loss:0.017, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 227   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.97767857 0.99343545 0.94713656 0.92832765
 0.99756691 0.94972067 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9919286698092517
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f80c81c4898>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.333, val_acc:0.591]
Epoch [2/120    avg_loss:1.871, val_acc:0.673]
Epoch [3/120    avg_loss:1.450, val_acc:0.688]
Epoch [4/120    avg_loss:1.147, val_acc:0.819]
Epoch [5/120    avg_loss:0.953, val_acc:0.776]
Epoch [6/120    avg_loss:0.760, val_acc:0.762]
Epoch [7/120    avg_loss:0.682, val_acc:0.869]
Epoch [8/120    avg_loss:0.665, val_acc:0.855]
Epoch [9/120    avg_loss:0.602, val_acc:0.847]
Epoch [10/120    avg_loss:0.597, val_acc:0.885]
Epoch [11/120    avg_loss:0.507, val_acc:0.875]
Epoch [12/120    avg_loss:0.449, val_acc:0.831]
Epoch [13/120    avg_loss:0.480, val_acc:0.897]
Epoch [14/120    avg_loss:0.393, val_acc:0.913]
Epoch [15/120    avg_loss:0.378, val_acc:0.917]
Epoch [16/120    avg_loss:0.394, val_acc:0.895]
Epoch [17/120    avg_loss:0.443, val_acc:0.885]
Epoch [18/120    avg_loss:0.334, val_acc:0.915]
Epoch [19/120    avg_loss:0.300, val_acc:0.921]
Epoch [20/120    avg_loss:0.284, val_acc:0.927]
Epoch [21/120    avg_loss:0.336, val_acc:0.907]
Epoch [22/120    avg_loss:0.299, val_acc:0.917]
Epoch [23/120    avg_loss:0.255, val_acc:0.931]
Epoch [24/120    avg_loss:0.266, val_acc:0.950]
Epoch [25/120    avg_loss:0.248, val_acc:0.927]
Epoch [26/120    avg_loss:0.255, val_acc:0.946]
Epoch [27/120    avg_loss:0.274, val_acc:0.944]
Epoch [28/120    avg_loss:0.216, val_acc:0.946]
Epoch [29/120    avg_loss:0.180, val_acc:0.940]
Epoch [30/120    avg_loss:0.232, val_acc:0.956]
Epoch [31/120    avg_loss:0.232, val_acc:0.962]
Epoch [32/120    avg_loss:0.222, val_acc:0.935]
Epoch [33/120    avg_loss:0.234, val_acc:0.964]
Epoch [34/120    avg_loss:0.231, val_acc:0.944]
Epoch [35/120    avg_loss:0.204, val_acc:0.970]
Epoch [36/120    avg_loss:0.200, val_acc:0.968]
Epoch [37/120    avg_loss:0.143, val_acc:0.950]
Epoch [38/120    avg_loss:0.137, val_acc:0.960]
Epoch [39/120    avg_loss:0.228, val_acc:0.950]
Epoch [40/120    avg_loss:0.184, val_acc:0.964]
Epoch [41/120    avg_loss:0.142, val_acc:0.950]
Epoch [42/120    avg_loss:0.150, val_acc:0.972]
Epoch [43/120    avg_loss:0.144, val_acc:0.923]
Epoch [44/120    avg_loss:0.122, val_acc:0.972]
Epoch [45/120    avg_loss:0.100, val_acc:0.966]
Epoch [46/120    avg_loss:0.158, val_acc:0.960]
Epoch [47/120    avg_loss:0.132, val_acc:0.978]
Epoch [48/120    avg_loss:0.110, val_acc:0.986]
Epoch [49/120    avg_loss:0.134, val_acc:0.974]
Epoch [50/120    avg_loss:0.171, val_acc:0.954]
Epoch [51/120    avg_loss:0.103, val_acc:0.982]
Epoch [52/120    avg_loss:0.104, val_acc:0.972]
Epoch [53/120    avg_loss:0.093, val_acc:0.982]
Epoch [54/120    avg_loss:0.056, val_acc:0.982]
Epoch [55/120    avg_loss:0.065, val_acc:0.982]
Epoch [56/120    avg_loss:0.070, val_acc:0.988]
Epoch [57/120    avg_loss:0.049, val_acc:0.986]
Epoch [58/120    avg_loss:0.065, val_acc:0.990]
Epoch [59/120    avg_loss:0.043, val_acc:0.994]
Epoch [60/120    avg_loss:0.058, val_acc:0.992]
Epoch [61/120    avg_loss:0.057, val_acc:0.986]
Epoch [62/120    avg_loss:0.052, val_acc:0.988]
Epoch [63/120    avg_loss:0.068, val_acc:0.972]
Epoch [64/120    avg_loss:0.045, val_acc:0.984]
Epoch [65/120    avg_loss:0.040, val_acc:0.992]
Epoch [66/120    avg_loss:0.040, val_acc:0.972]
Epoch [67/120    avg_loss:0.086, val_acc:0.964]
Epoch [68/120    avg_loss:0.078, val_acc:0.988]
Epoch [69/120    avg_loss:0.039, val_acc:0.990]
Epoch [70/120    avg_loss:0.094, val_acc:0.968]
Epoch [71/120    avg_loss:0.119, val_acc:0.970]
Epoch [72/120    avg_loss:0.038, val_acc:0.974]
Epoch [73/120    avg_loss:0.039, val_acc:0.988]
Epoch [74/120    avg_loss:0.027, val_acc:0.998]
Epoch [75/120    avg_loss:0.022, val_acc:0.998]
Epoch [76/120    avg_loss:0.018, val_acc:0.998]
Epoch [77/120    avg_loss:0.016, val_acc:0.998]
Epoch [78/120    avg_loss:0.015, val_acc:0.998]
Epoch [79/120    avg_loss:0.018, val_acc:0.998]
Epoch [80/120    avg_loss:0.014, val_acc:0.998]
Epoch [81/120    avg_loss:0.018, val_acc:0.998]
Epoch [82/120    avg_loss:0.013, val_acc:0.998]
Epoch [83/120    avg_loss:0.021, val_acc:0.998]
Epoch [84/120    avg_loss:0.014, val_acc:0.998]
Epoch [85/120    avg_loss:0.017, val_acc:0.998]
Epoch [86/120    avg_loss:0.012, val_acc:0.998]
Epoch [87/120    avg_loss:0.013, val_acc:0.998]
Epoch [88/120    avg_loss:0.014, val_acc:0.998]
Epoch [89/120    avg_loss:0.014, val_acc:0.998]
Epoch [90/120    avg_loss:0.012, val_acc:0.998]
Epoch [91/120    avg_loss:0.013, val_acc:0.998]
Epoch [92/120    avg_loss:0.016, val_acc:0.998]
Epoch [93/120    avg_loss:0.015, val_acc:0.998]
Epoch [94/120    avg_loss:0.011, val_acc:0.998]
Epoch [95/120    avg_loss:0.014, val_acc:0.998]
Epoch [96/120    avg_loss:0.011, val_acc:0.998]
Epoch [97/120    avg_loss:0.015, val_acc:0.998]
Epoch [98/120    avg_loss:0.012, val_acc:0.998]
Epoch [99/120    avg_loss:0.014, val_acc:0.998]
Epoch [100/120    avg_loss:0.016, val_acc:0.998]
Epoch [101/120    avg_loss:0.010, val_acc:0.998]
Epoch [102/120    avg_loss:0.022, val_acc:0.998]
Epoch [103/120    avg_loss:0.010, val_acc:0.998]
Epoch [104/120    avg_loss:0.014, val_acc:0.998]
Epoch [105/120    avg_loss:0.012, val_acc:0.998]
Epoch [106/120    avg_loss:0.013, val_acc:0.998]
Epoch [107/120    avg_loss:0.009, val_acc:0.998]
Epoch [108/120    avg_loss:0.010, val_acc:0.998]
Epoch [109/120    avg_loss:0.009, val_acc:0.998]
Epoch [110/120    avg_loss:0.010, val_acc:0.998]
Epoch [111/120    avg_loss:0.011, val_acc:0.998]
Epoch [112/120    avg_loss:0.010, val_acc:0.998]
Epoch [113/120    avg_loss:0.011, val_acc:0.998]
Epoch [114/120    avg_loss:0.010, val_acc:0.998]
Epoch [115/120    avg_loss:0.011, val_acc:0.998]
Epoch [116/120    avg_loss:0.011, val_acc:0.998]
Epoch [117/120    avg_loss:0.012, val_acc:0.998]
Epoch [118/120    avg_loss:0.012, val_acc:0.998]
Epoch [119/120    avg_loss:0.010, val_acc:0.998]
Epoch [120/120    avg_loss:0.011, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  11   0   0   0   0   0   0   7   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 0.99926954 0.99319728 1.         0.94144144 0.93515358
 0.99757869 0.98378378 1.         1.         1.         1.
 0.99233297 1.        ]

Kappa:
0.9928781122573509
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faf96c84940>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.341, val_acc:0.504]
Epoch [2/120    avg_loss:1.838, val_acc:0.619]
Epoch [3/120    avg_loss:1.456, val_acc:0.696]
Epoch [4/120    avg_loss:1.181, val_acc:0.704]
Epoch [5/120    avg_loss:1.023, val_acc:0.817]
Epoch [6/120    avg_loss:0.897, val_acc:0.774]
Epoch [7/120    avg_loss:0.754, val_acc:0.810]
Epoch [8/120    avg_loss:0.695, val_acc:0.861]
Epoch [9/120    avg_loss:0.611, val_acc:0.867]
Epoch [10/120    avg_loss:0.525, val_acc:0.889]
Epoch [11/120    avg_loss:0.557, val_acc:0.873]
Epoch [12/120    avg_loss:0.539, val_acc:0.901]
Epoch [13/120    avg_loss:0.455, val_acc:0.857]
Epoch [14/120    avg_loss:0.437, val_acc:0.889]
Epoch [15/120    avg_loss:0.455, val_acc:0.895]
Epoch [16/120    avg_loss:0.393, val_acc:0.915]
Epoch [17/120    avg_loss:0.365, val_acc:0.909]
Epoch [18/120    avg_loss:0.408, val_acc:0.901]
Epoch [19/120    avg_loss:0.295, val_acc:0.915]
Epoch [20/120    avg_loss:0.301, val_acc:0.933]
Epoch [21/120    avg_loss:0.297, val_acc:0.917]
Epoch [22/120    avg_loss:0.303, val_acc:0.944]
Epoch [23/120    avg_loss:0.334, val_acc:0.935]
Epoch [24/120    avg_loss:0.272, val_acc:0.938]
Epoch [25/120    avg_loss:0.281, val_acc:0.954]
Epoch [26/120    avg_loss:0.232, val_acc:0.923]
Epoch [27/120    avg_loss:0.309, val_acc:0.903]
Epoch [28/120    avg_loss:0.267, val_acc:0.938]
Epoch [29/120    avg_loss:0.226, val_acc:0.954]
Epoch [30/120    avg_loss:0.225, val_acc:0.952]
Epoch [31/120    avg_loss:0.206, val_acc:0.962]
Epoch [32/120    avg_loss:0.207, val_acc:0.958]
Epoch [33/120    avg_loss:0.207, val_acc:0.948]
Epoch [34/120    avg_loss:0.193, val_acc:0.891]
Epoch [35/120    avg_loss:0.275, val_acc:0.950]
Epoch [36/120    avg_loss:0.213, val_acc:0.948]
Epoch [37/120    avg_loss:0.180, val_acc:0.944]
Epoch [38/120    avg_loss:0.190, val_acc:0.968]
Epoch [39/120    avg_loss:0.189, val_acc:0.956]
Epoch [40/120    avg_loss:0.145, val_acc:0.974]
Epoch [41/120    avg_loss:0.168, val_acc:0.935]
Epoch [42/120    avg_loss:0.134, val_acc:0.968]
Epoch [43/120    avg_loss:0.107, val_acc:0.958]
Epoch [44/120    avg_loss:0.117, val_acc:0.974]
Epoch [45/120    avg_loss:0.098, val_acc:0.964]
Epoch [46/120    avg_loss:0.195, val_acc:0.970]
Epoch [47/120    avg_loss:0.222, val_acc:0.913]
Epoch [48/120    avg_loss:0.194, val_acc:0.972]
Epoch [49/120    avg_loss:0.115, val_acc:0.960]
Epoch [50/120    avg_loss:0.114, val_acc:0.964]
Epoch [51/120    avg_loss:0.099, val_acc:0.950]
Epoch [52/120    avg_loss:0.091, val_acc:0.974]
Epoch [53/120    avg_loss:0.066, val_acc:0.974]
Epoch [54/120    avg_loss:0.093, val_acc:0.982]
Epoch [55/120    avg_loss:0.065, val_acc:0.992]
Epoch [56/120    avg_loss:0.065, val_acc:0.984]
Epoch [57/120    avg_loss:0.045, val_acc:0.986]
Epoch [58/120    avg_loss:0.084, val_acc:0.984]
Epoch [59/120    avg_loss:0.093, val_acc:0.984]
Epoch [60/120    avg_loss:0.178, val_acc:0.968]
Epoch [61/120    avg_loss:0.074, val_acc:0.984]
Epoch [62/120    avg_loss:0.067, val_acc:0.986]
Epoch [63/120    avg_loss:0.044, val_acc:0.982]
Epoch [64/120    avg_loss:0.065, val_acc:0.984]
Epoch [65/120    avg_loss:0.054, val_acc:0.966]
Epoch [66/120    avg_loss:0.065, val_acc:0.980]
Epoch [67/120    avg_loss:0.073, val_acc:0.988]
Epoch [68/120    avg_loss:0.061, val_acc:0.986]
Epoch [69/120    avg_loss:0.046, val_acc:0.988]
Epoch [70/120    avg_loss:0.043, val_acc:0.992]
Epoch [71/120    avg_loss:0.034, val_acc:0.994]
Epoch [72/120    avg_loss:0.028, val_acc:0.996]
Epoch [73/120    avg_loss:0.029, val_acc:0.996]
Epoch [74/120    avg_loss:0.026, val_acc:0.994]
Epoch [75/120    avg_loss:0.025, val_acc:0.994]
Epoch [76/120    avg_loss:0.027, val_acc:0.994]
Epoch [77/120    avg_loss:0.028, val_acc:0.994]
Epoch [78/120    avg_loss:0.025, val_acc:0.996]
Epoch [79/120    avg_loss:0.028, val_acc:0.996]
Epoch [80/120    avg_loss:0.028, val_acc:0.994]
Epoch [81/120    avg_loss:0.027, val_acc:0.992]
Epoch [82/120    avg_loss:0.020, val_acc:0.994]
Epoch [83/120    avg_loss:0.020, val_acc:0.992]
Epoch [84/120    avg_loss:0.025, val_acc:0.996]
Epoch [85/120    avg_loss:0.020, val_acc:0.994]
Epoch [86/120    avg_loss:0.021, val_acc:0.994]
Epoch [87/120    avg_loss:0.024, val_acc:0.994]
Epoch [88/120    avg_loss:0.018, val_acc:0.996]
Epoch [89/120    avg_loss:0.021, val_acc:0.994]
Epoch [90/120    avg_loss:0.018, val_acc:0.994]
Epoch [91/120    avg_loss:0.027, val_acc:0.994]
Epoch [92/120    avg_loss:0.017, val_acc:0.996]
Epoch [93/120    avg_loss:0.019, val_acc:0.994]
Epoch [94/120    avg_loss:0.016, val_acc:0.994]
Epoch [95/120    avg_loss:0.017, val_acc:0.994]
Epoch [96/120    avg_loss:0.020, val_acc:0.994]
Epoch [97/120    avg_loss:0.017, val_acc:0.994]
Epoch [98/120    avg_loss:0.021, val_acc:0.994]
Epoch [99/120    avg_loss:0.019, val_acc:0.994]
Epoch [100/120    avg_loss:0.020, val_acc:0.994]
Epoch [101/120    avg_loss:0.020, val_acc:0.994]
Epoch [102/120    avg_loss:0.018, val_acc:0.994]
Epoch [103/120    avg_loss:0.022, val_acc:0.994]
Epoch [104/120    avg_loss:0.019, val_acc:0.994]
Epoch [105/120    avg_loss:0.018, val_acc:0.994]
Epoch [106/120    avg_loss:0.019, val_acc:0.994]
Epoch [107/120    avg_loss:0.016, val_acc:0.994]
Epoch [108/120    avg_loss:0.019, val_acc:0.994]
Epoch [109/120    avg_loss:0.020, val_acc:0.994]
Epoch [110/120    avg_loss:0.017, val_acc:0.994]
Epoch [111/120    avg_loss:0.017, val_acc:0.994]
Epoch [112/120    avg_loss:0.014, val_acc:0.994]
Epoch [113/120    avg_loss:0.017, val_acc:0.994]
Epoch [114/120    avg_loss:0.018, val_acc:0.994]
Epoch [115/120    avg_loss:0.019, val_acc:0.994]
Epoch [116/120    avg_loss:0.025, val_acc:0.994]
Epoch [117/120    avg_loss:0.016, val_acc:0.994]
Epoch [118/120    avg_loss:0.016, val_acc:0.994]
Epoch [119/120    avg_loss:0.015, val_acc:0.994]
Epoch [120/120    avg_loss:0.019, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 204  26   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 226   0   0   0   0   0   0   0   1   0]
 [  0   0   0   0  24 121   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0 204   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 363   0   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.78464818763327

F1 scores:
[       nan 0.99926954 0.99545455 0.94009217 0.8950495  0.90636704
 0.99512195 0.98924731 1.         1.         0.99862448 1.
 0.99779736 1.        ]

Kappa:
0.9864679166924193
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5439f23940>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.366, val_acc:0.520]
Epoch [2/120    avg_loss:1.861, val_acc:0.663]
Epoch [3/120    avg_loss:1.481, val_acc:0.728]
Epoch [4/120    avg_loss:1.148, val_acc:0.831]
Epoch [5/120    avg_loss:0.917, val_acc:0.863]
Epoch [6/120    avg_loss:0.864, val_acc:0.867]
Epoch [7/120    avg_loss:0.721, val_acc:0.861]
Epoch [8/120    avg_loss:0.664, val_acc:0.849]
Epoch [9/120    avg_loss:0.535, val_acc:0.891]
Epoch [10/120    avg_loss:0.489, val_acc:0.901]
Epoch [11/120    avg_loss:0.551, val_acc:0.867]
Epoch [12/120    avg_loss:0.493, val_acc:0.907]
Epoch [13/120    avg_loss:0.420, val_acc:0.909]
Epoch [14/120    avg_loss:0.359, val_acc:0.905]
Epoch [15/120    avg_loss:0.393, val_acc:0.911]
Epoch [16/120    avg_loss:0.375, val_acc:0.925]
Epoch [17/120    avg_loss:0.410, val_acc:0.907]
Epoch [18/120    avg_loss:0.321, val_acc:0.893]
Epoch [19/120    avg_loss:0.356, val_acc:0.927]
Epoch [20/120    avg_loss:0.329, val_acc:0.911]
Epoch [21/120    avg_loss:0.281, val_acc:0.946]
Epoch [22/120    avg_loss:0.321, val_acc:0.950]
Epoch [23/120    avg_loss:0.275, val_acc:0.935]
Epoch [24/120    avg_loss:0.298, val_acc:0.907]
Epoch [25/120    avg_loss:0.263, val_acc:0.948]
Epoch [26/120    avg_loss:0.263, val_acc:0.958]
Epoch [27/120    avg_loss:0.269, val_acc:0.931]
Epoch [28/120    avg_loss:0.271, val_acc:0.940]
Epoch [29/120    avg_loss:0.271, val_acc:0.972]
Epoch [30/120    avg_loss:0.223, val_acc:0.952]
Epoch [31/120    avg_loss:0.208, val_acc:0.962]
Epoch [32/120    avg_loss:0.184, val_acc:0.960]
Epoch [33/120    avg_loss:0.222, val_acc:0.974]
Epoch [34/120    avg_loss:0.189, val_acc:0.960]
Epoch [35/120    avg_loss:0.285, val_acc:0.966]
Epoch [36/120    avg_loss:0.202, val_acc:0.964]
Epoch [37/120    avg_loss:0.157, val_acc:0.968]
Epoch [38/120    avg_loss:0.167, val_acc:0.968]
Epoch [39/120    avg_loss:0.170, val_acc:0.923]
Epoch [40/120    avg_loss:0.219, val_acc:0.940]
Epoch [41/120    avg_loss:0.170, val_acc:0.978]
Epoch [42/120    avg_loss:0.142, val_acc:0.974]
Epoch [43/120    avg_loss:0.116, val_acc:0.990]
Epoch [44/120    avg_loss:0.121, val_acc:0.974]
Epoch [45/120    avg_loss:0.127, val_acc:0.974]
Epoch [46/120    avg_loss:0.126, val_acc:0.974]
Epoch [47/120    avg_loss:0.098, val_acc:0.980]
Epoch [48/120    avg_loss:0.104, val_acc:0.988]
Epoch [49/120    avg_loss:0.131, val_acc:0.980]
Epoch [50/120    avg_loss:0.159, val_acc:0.980]
Epoch [51/120    avg_loss:0.121, val_acc:0.978]
Epoch [52/120    avg_loss:0.086, val_acc:0.988]
Epoch [53/120    avg_loss:0.075, val_acc:0.988]
Epoch [54/120    avg_loss:0.096, val_acc:0.988]
Epoch [55/120    avg_loss:0.135, val_acc:0.982]
Epoch [56/120    avg_loss:0.090, val_acc:0.978]
Epoch [57/120    avg_loss:0.084, val_acc:0.988]
Epoch [58/120    avg_loss:0.056, val_acc:0.988]
Epoch [59/120    avg_loss:0.047, val_acc:0.988]
Epoch [60/120    avg_loss:0.053, val_acc:0.988]
Epoch [61/120    avg_loss:0.059, val_acc:0.988]
Epoch [62/120    avg_loss:0.050, val_acc:0.990]
Epoch [63/120    avg_loss:0.052, val_acc:0.990]
Epoch [64/120    avg_loss:0.050, val_acc:0.988]
Epoch [65/120    avg_loss:0.044, val_acc:0.988]
Epoch [66/120    avg_loss:0.041, val_acc:0.988]
Epoch [67/120    avg_loss:0.036, val_acc:0.988]
Epoch [68/120    avg_loss:0.046, val_acc:0.988]
Epoch [69/120    avg_loss:0.047, val_acc:0.990]
Epoch [70/120    avg_loss:0.047, val_acc:0.990]
Epoch [71/120    avg_loss:0.038, val_acc:0.990]
Epoch [72/120    avg_loss:0.039, val_acc:0.990]
Epoch [73/120    avg_loss:0.040, val_acc:0.992]
Epoch [74/120    avg_loss:0.035, val_acc:0.990]
Epoch [75/120    avg_loss:0.044, val_acc:0.990]
Epoch [76/120    avg_loss:0.034, val_acc:0.990]
Epoch [77/120    avg_loss:0.034, val_acc:0.990]
Epoch [78/120    avg_loss:0.041, val_acc:0.990]
Epoch [79/120    avg_loss:0.040, val_acc:0.990]
Epoch [80/120    avg_loss:0.038, val_acc:0.990]
Epoch [81/120    avg_loss:0.035, val_acc:0.992]
Epoch [82/120    avg_loss:0.037, val_acc:0.990]
Epoch [83/120    avg_loss:0.034, val_acc:0.990]
Epoch [84/120    avg_loss:0.041, val_acc:0.990]
Epoch [85/120    avg_loss:0.040, val_acc:0.988]
Epoch [86/120    avg_loss:0.049, val_acc:0.990]
Epoch [87/120    avg_loss:0.037, val_acc:0.990]
Epoch [88/120    avg_loss:0.041, val_acc:0.990]
Epoch [89/120    avg_loss:0.035, val_acc:0.990]
Epoch [90/120    avg_loss:0.039, val_acc:0.990]
Epoch [91/120    avg_loss:0.043, val_acc:0.990]
Epoch [92/120    avg_loss:0.033, val_acc:0.990]
Epoch [93/120    avg_loss:0.032, val_acc:0.990]
Epoch [94/120    avg_loss:0.029, val_acc:0.990]
Epoch [95/120    avg_loss:0.035, val_acc:0.990]
Epoch [96/120    avg_loss:0.028, val_acc:0.990]
Epoch [97/120    avg_loss:0.030, val_acc:0.990]
Epoch [98/120    avg_loss:0.026, val_acc:0.990]
Epoch [99/120    avg_loss:0.028, val_acc:0.990]
Epoch [100/120    avg_loss:0.033, val_acc:0.988]
Epoch [101/120    avg_loss:0.034, val_acc:0.988]
Epoch [102/120    avg_loss:0.025, val_acc:0.990]
Epoch [103/120    avg_loss:0.033, val_acc:0.988]
Epoch [104/120    avg_loss:0.029, val_acc:0.988]
Epoch [105/120    avg_loss:0.029, val_acc:0.990]
Epoch [106/120    avg_loss:0.034, val_acc:0.990]
Epoch [107/120    avg_loss:0.037, val_acc:0.990]
Epoch [108/120    avg_loss:0.027, val_acc:0.990]
Epoch [109/120    avg_loss:0.030, val_acc:0.990]
Epoch [110/120    avg_loss:0.032, val_acc:0.990]
Epoch [111/120    avg_loss:0.036, val_acc:0.990]
Epoch [112/120    avg_loss:0.030, val_acc:0.990]
Epoch [113/120    avg_loss:0.034, val_acc:0.990]
Epoch [114/120    avg_loss:0.027, val_acc:0.990]
Epoch [115/120    avg_loss:0.033, val_acc:0.990]
Epoch [116/120    avg_loss:0.031, val_acc:0.990]
Epoch [117/120    avg_loss:0.035, val_acc:0.990]
Epoch [118/120    avg_loss:0.027, val_acc:0.990]
Epoch [119/120    avg_loss:0.044, val_acc:0.990]
Epoch [120/120    avg_loss:0.030, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   3 209  14   0   0   0   0   0   0   1   0]
 [  0   0   0   4  17 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 0.99707174 0.99095023 0.98501071 0.92070485 0.87632509
 0.98795181 0.97826087 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.988606021195473
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:05:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3036018908>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.290, val_acc:0.525]
Epoch [2/120    avg_loss:1.808, val_acc:0.629]
Epoch [3/120    avg_loss:1.436, val_acc:0.656]
Epoch [4/120    avg_loss:1.177, val_acc:0.709]
Epoch [5/120    avg_loss:1.041, val_acc:0.750]
Epoch [6/120    avg_loss:0.930, val_acc:0.736]
Epoch [7/120    avg_loss:0.761, val_acc:0.822]
Epoch [8/120    avg_loss:0.757, val_acc:0.756]
Epoch [9/120    avg_loss:0.737, val_acc:0.842]
Epoch [10/120    avg_loss:0.652, val_acc:0.807]
Epoch [11/120    avg_loss:0.586, val_acc:0.834]
Epoch [12/120    avg_loss:0.529, val_acc:0.875]
Epoch [13/120    avg_loss:0.471, val_acc:0.838]
Epoch [14/120    avg_loss:0.499, val_acc:0.887]
Epoch [15/120    avg_loss:0.429, val_acc:0.809]
Epoch [16/120    avg_loss:0.407, val_acc:0.865]
Epoch [17/120    avg_loss:0.354, val_acc:0.936]
Epoch [18/120    avg_loss:0.332, val_acc:0.930]
Epoch [19/120    avg_loss:0.308, val_acc:0.895]
Epoch [20/120    avg_loss:0.363, val_acc:0.910]
Epoch [21/120    avg_loss:0.272, val_acc:0.945]
Epoch [22/120    avg_loss:0.282, val_acc:0.908]
Epoch [23/120    avg_loss:0.235, val_acc:0.955]
Epoch [24/120    avg_loss:0.209, val_acc:0.953]
Epoch [25/120    avg_loss:0.194, val_acc:0.953]
Epoch [26/120    avg_loss:0.241, val_acc:0.932]
Epoch [27/120    avg_loss:0.235, val_acc:0.959]
Epoch [28/120    avg_loss:0.194, val_acc:0.941]
Epoch [29/120    avg_loss:0.166, val_acc:0.947]
Epoch [30/120    avg_loss:0.163, val_acc:0.924]
Epoch [31/120    avg_loss:0.151, val_acc:0.947]
Epoch [32/120    avg_loss:0.199, val_acc:0.953]
Epoch [33/120    avg_loss:0.144, val_acc:0.973]
Epoch [34/120    avg_loss:0.128, val_acc:0.969]
Epoch [35/120    avg_loss:0.145, val_acc:0.947]
Epoch [36/120    avg_loss:0.166, val_acc:0.963]
Epoch [37/120    avg_loss:0.125, val_acc:0.971]
Epoch [38/120    avg_loss:0.152, val_acc:0.959]
Epoch [39/120    avg_loss:0.134, val_acc:0.967]
Epoch [40/120    avg_loss:0.159, val_acc:0.965]
Epoch [41/120    avg_loss:0.145, val_acc:0.943]
Epoch [42/120    avg_loss:0.097, val_acc:0.945]
Epoch [43/120    avg_loss:0.107, val_acc:0.977]
Epoch [44/120    avg_loss:0.092, val_acc:0.973]
Epoch [45/120    avg_loss:0.119, val_acc:0.961]
Epoch [46/120    avg_loss:0.120, val_acc:0.961]
Epoch [47/120    avg_loss:0.149, val_acc:0.947]
Epoch [48/120    avg_loss:0.117, val_acc:0.975]
Epoch [49/120    avg_loss:0.101, val_acc:0.975]
Epoch [50/120    avg_loss:0.085, val_acc:0.969]
Epoch [51/120    avg_loss:0.062, val_acc:0.982]
Epoch [52/120    avg_loss:0.056, val_acc:0.977]
Epoch [53/120    avg_loss:0.053, val_acc:0.980]
Epoch [54/120    avg_loss:0.069, val_acc:0.977]
Epoch [55/120    avg_loss:0.052, val_acc:0.984]
Epoch [56/120    avg_loss:0.078, val_acc:0.982]
Epoch [57/120    avg_loss:0.061, val_acc:0.977]
Epoch [58/120    avg_loss:0.058, val_acc:0.977]
Epoch [59/120    avg_loss:0.047, val_acc:0.986]
Epoch [60/120    avg_loss:0.037, val_acc:0.980]
Epoch [61/120    avg_loss:0.038, val_acc:0.982]
Epoch [62/120    avg_loss:0.026, val_acc:0.988]
Epoch [63/120    avg_loss:0.030, val_acc:0.984]
Epoch [64/120    avg_loss:0.055, val_acc:0.957]
Epoch [65/120    avg_loss:0.058, val_acc:0.961]
Epoch [66/120    avg_loss:0.075, val_acc:0.982]
Epoch [67/120    avg_loss:0.067, val_acc:0.969]
Epoch [68/120    avg_loss:0.076, val_acc:0.945]
Epoch [69/120    avg_loss:0.052, val_acc:0.984]
Epoch [70/120    avg_loss:0.038, val_acc:0.980]
Epoch [71/120    avg_loss:0.030, val_acc:0.982]
Epoch [72/120    avg_loss:0.021, val_acc:0.984]
Epoch [73/120    avg_loss:0.025, val_acc:0.986]
Epoch [74/120    avg_loss:0.033, val_acc:0.984]
Epoch [75/120    avg_loss:0.021, val_acc:0.982]
Epoch [76/120    avg_loss:0.017, val_acc:0.982]
Epoch [77/120    avg_loss:0.018, val_acc:0.982]
Epoch [78/120    avg_loss:0.015, val_acc:0.984]
Epoch [79/120    avg_loss:0.013, val_acc:0.984]
Epoch [80/120    avg_loss:0.016, val_acc:0.984]
Epoch [81/120    avg_loss:0.013, val_acc:0.984]
Epoch [82/120    avg_loss:0.012, val_acc:0.984]
Epoch [83/120    avg_loss:0.014, val_acc:0.984]
Epoch [84/120    avg_loss:0.014, val_acc:0.984]
Epoch [85/120    avg_loss:0.013, val_acc:0.984]
Epoch [86/120    avg_loss:0.014, val_acc:0.986]
Epoch [87/120    avg_loss:0.015, val_acc:0.986]
Epoch [88/120    avg_loss:0.014, val_acc:0.986]
Epoch [89/120    avg_loss:0.016, val_acc:0.986]
Epoch [90/120    avg_loss:0.013, val_acc:0.986]
Epoch [91/120    avg_loss:0.014, val_acc:0.986]
Epoch [92/120    avg_loss:0.015, val_acc:0.986]
Epoch [93/120    avg_loss:0.012, val_acc:0.988]
Epoch [94/120    avg_loss:0.017, val_acc:0.986]
Epoch [95/120    avg_loss:0.014, val_acc:0.986]
Epoch [96/120    avg_loss:0.013, val_acc:0.986]
Epoch [97/120    avg_loss:0.013, val_acc:0.986]
Epoch [98/120    avg_loss:0.014, val_acc:0.986]
Epoch [99/120    avg_loss:0.020, val_acc:0.988]
Epoch [100/120    avg_loss:0.014, val_acc:0.988]
Epoch [101/120    avg_loss:0.015, val_acc:0.988]
Epoch [102/120    avg_loss:0.013, val_acc:0.988]
Epoch [103/120    avg_loss:0.017, val_acc:0.988]
Epoch [104/120    avg_loss:0.024, val_acc:0.988]
Epoch [105/120    avg_loss:0.013, val_acc:0.988]
Epoch [106/120    avg_loss:0.013, val_acc:0.988]
Epoch [107/120    avg_loss:0.022, val_acc:0.988]
Epoch [108/120    avg_loss:0.013, val_acc:0.988]
Epoch [109/120    avg_loss:0.017, val_acc:0.988]
Epoch [110/120    avg_loss:0.013, val_acc:0.988]
Epoch [111/120    avg_loss:0.013, val_acc:0.988]
Epoch [112/120    avg_loss:0.016, val_acc:0.988]
Epoch [113/120    avg_loss:0.018, val_acc:0.988]
Epoch [114/120    avg_loss:0.011, val_acc:0.988]
Epoch [115/120    avg_loss:0.016, val_acc:0.988]
Epoch [116/120    avg_loss:0.015, val_acc:0.988]
Epoch [117/120    avg_loss:0.012, val_acc:0.988]
Epoch [118/120    avg_loss:0.011, val_acc:0.988]
Epoch [119/120    avg_loss:0.014, val_acc:0.988]
Epoch [120/120    avg_loss:0.013, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   8   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.57356076759062

F1 scores:
[       nan 1.         0.9977221  0.98230088 0.95913978 0.96167247
 1.         0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9952522875192407
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe1961e5828>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.289, val_acc:0.599]
Epoch [2/120    avg_loss:1.809, val_acc:0.567]
Epoch [3/120    avg_loss:1.401, val_acc:0.738]
Epoch [4/120    avg_loss:1.122, val_acc:0.774]
Epoch [5/120    avg_loss:0.928, val_acc:0.772]
Epoch [6/120    avg_loss:0.813, val_acc:0.748]
Epoch [7/120    avg_loss:0.733, val_acc:0.831]
Epoch [8/120    avg_loss:0.716, val_acc:0.851]
Epoch [9/120    avg_loss:0.649, val_acc:0.821]
Epoch [10/120    avg_loss:0.533, val_acc:0.837]
Epoch [11/120    avg_loss:0.595, val_acc:0.859]
Epoch [12/120    avg_loss:0.479, val_acc:0.829]
Epoch [13/120    avg_loss:0.463, val_acc:0.905]
Epoch [14/120    avg_loss:0.388, val_acc:0.905]
Epoch [15/120    avg_loss:0.398, val_acc:0.831]
Epoch [16/120    avg_loss:0.414, val_acc:0.893]
Epoch [17/120    avg_loss:0.363, val_acc:0.905]
Epoch [18/120    avg_loss:0.364, val_acc:0.931]
Epoch [19/120    avg_loss:0.343, val_acc:0.923]
Epoch [20/120    avg_loss:0.320, val_acc:0.950]
Epoch [21/120    avg_loss:0.357, val_acc:0.879]
Epoch [22/120    avg_loss:0.328, val_acc:0.929]
Epoch [23/120    avg_loss:0.226, val_acc:0.948]
Epoch [24/120    avg_loss:0.266, val_acc:0.933]
Epoch [25/120    avg_loss:0.259, val_acc:0.944]
Epoch [26/120    avg_loss:0.213, val_acc:0.946]
Epoch [27/120    avg_loss:0.267, val_acc:0.956]
Epoch [28/120    avg_loss:0.204, val_acc:0.954]
Epoch [29/120    avg_loss:0.226, val_acc:0.938]
Epoch [30/120    avg_loss:0.243, val_acc:0.899]
Epoch [31/120    avg_loss:0.254, val_acc:0.935]
Epoch [32/120    avg_loss:0.192, val_acc:0.917]
Epoch [33/120    avg_loss:0.213, val_acc:0.966]
Epoch [34/120    avg_loss:0.154, val_acc:0.966]
Epoch [35/120    avg_loss:0.164, val_acc:0.954]
Epoch [36/120    avg_loss:0.209, val_acc:0.950]
Epoch [37/120    avg_loss:0.105, val_acc:0.962]
Epoch [38/120    avg_loss:0.139, val_acc:0.942]
Epoch [39/120    avg_loss:0.168, val_acc:0.954]
Epoch [40/120    avg_loss:0.151, val_acc:0.972]
Epoch [41/120    avg_loss:0.134, val_acc:0.958]
Epoch [42/120    avg_loss:0.117, val_acc:0.956]
Epoch [43/120    avg_loss:0.165, val_acc:0.960]
Epoch [44/120    avg_loss:0.088, val_acc:0.968]
Epoch [45/120    avg_loss:0.082, val_acc:0.942]
Epoch [46/120    avg_loss:0.077, val_acc:0.966]
Epoch [47/120    avg_loss:0.089, val_acc:0.966]
Epoch [48/120    avg_loss:0.116, val_acc:0.962]
Epoch [49/120    avg_loss:0.098, val_acc:0.974]
Epoch [50/120    avg_loss:0.087, val_acc:0.972]
Epoch [51/120    avg_loss:0.064, val_acc:0.980]
Epoch [52/120    avg_loss:0.067, val_acc:0.978]
Epoch [53/120    avg_loss:0.122, val_acc:0.940]
Epoch [54/120    avg_loss:0.150, val_acc:0.968]
Epoch [55/120    avg_loss:0.197, val_acc:0.984]
Epoch [56/120    avg_loss:0.089, val_acc:0.976]
Epoch [57/120    avg_loss:0.065, val_acc:0.972]
Epoch [58/120    avg_loss:0.046, val_acc:0.980]
Epoch [59/120    avg_loss:0.098, val_acc:0.970]
Epoch [60/120    avg_loss:0.067, val_acc:0.982]
Epoch [61/120    avg_loss:0.037, val_acc:0.980]
Epoch [62/120    avg_loss:0.035, val_acc:0.978]
Epoch [63/120    avg_loss:0.028, val_acc:0.982]
Epoch [64/120    avg_loss:0.044, val_acc:0.986]
Epoch [65/120    avg_loss:0.039, val_acc:0.986]
Epoch [66/120    avg_loss:0.035, val_acc:0.978]
Epoch [67/120    avg_loss:0.037, val_acc:0.976]
Epoch [68/120    avg_loss:0.057, val_acc:0.984]
Epoch [69/120    avg_loss:0.040, val_acc:0.980]
Epoch [70/120    avg_loss:0.051, val_acc:0.972]
Epoch [71/120    avg_loss:0.059, val_acc:0.940]
Epoch [72/120    avg_loss:0.041, val_acc:0.978]
Epoch [73/120    avg_loss:0.024, val_acc:0.986]
Epoch [74/120    avg_loss:0.031, val_acc:0.982]
Epoch [75/120    avg_loss:0.025, val_acc:0.986]
Epoch [76/120    avg_loss:0.025, val_acc:0.986]
Epoch [77/120    avg_loss:0.023, val_acc:0.980]
Epoch [78/120    avg_loss:0.028, val_acc:0.978]
Epoch [79/120    avg_loss:0.024, val_acc:0.988]
Epoch [80/120    avg_loss:0.028, val_acc:0.968]
Epoch [81/120    avg_loss:0.085, val_acc:0.942]
Epoch [82/120    avg_loss:0.076, val_acc:0.976]
Epoch [83/120    avg_loss:0.067, val_acc:0.978]
Epoch [84/120    avg_loss:0.035, val_acc:0.988]
Epoch [85/120    avg_loss:0.032, val_acc:0.986]
Epoch [86/120    avg_loss:0.029, val_acc:0.992]
Epoch [87/120    avg_loss:0.028, val_acc:0.986]
Epoch [88/120    avg_loss:0.017, val_acc:0.986]
Epoch [89/120    avg_loss:0.018, val_acc:0.988]
Epoch [90/120    avg_loss:0.017, val_acc:0.988]
Epoch [91/120    avg_loss:0.018, val_acc:0.976]
Epoch [92/120    avg_loss:0.037, val_acc:0.988]
Epoch [93/120    avg_loss:0.012, val_acc:0.992]
Epoch [94/120    avg_loss:0.023, val_acc:0.986]
Epoch [95/120    avg_loss:0.013, val_acc:0.988]
Epoch [96/120    avg_loss:0.011, val_acc:0.990]
Epoch [97/120    avg_loss:0.008, val_acc:0.988]
Epoch [98/120    avg_loss:0.014, val_acc:0.988]
Epoch [99/120    avg_loss:0.013, val_acc:0.990]
Epoch [100/120    avg_loss:0.010, val_acc:0.988]
Epoch [101/120    avg_loss:0.010, val_acc:0.988]
Epoch [102/120    avg_loss:0.100, val_acc:0.970]
Epoch [103/120    avg_loss:0.071, val_acc:0.964]
Epoch [104/120    avg_loss:0.081, val_acc:0.980]
Epoch [105/120    avg_loss:0.100, val_acc:0.976]
Epoch [106/120    avg_loss:0.059, val_acc:0.980]
Epoch [107/120    avg_loss:0.022, val_acc:0.986]
Epoch [108/120    avg_loss:0.026, val_acc:0.988]
Epoch [109/120    avg_loss:0.019, val_acc:0.988]
Epoch [110/120    avg_loss:0.014, val_acc:0.988]
Epoch [111/120    avg_loss:0.019, val_acc:0.988]
Epoch [112/120    avg_loss:0.021, val_acc:0.988]
Epoch [113/120    avg_loss:0.024, val_acc:0.988]
Epoch [114/120    avg_loss:0.015, val_acc:0.988]
Epoch [115/120    avg_loss:0.018, val_acc:0.988]
Epoch [116/120    avg_loss:0.014, val_acc:0.988]
Epoch [117/120    avg_loss:0.018, val_acc:0.988]
Epoch [118/120    avg_loss:0.016, val_acc:0.988]
Epoch [119/120    avg_loss:0.019, val_acc:0.988]
Epoch [120/120    avg_loss:0.015, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0   0 219   3   0   0   0   0   0   0   5   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 1.         0.99319728 0.99782135 0.95217391 0.9390681
 1.         0.97849462 1.         1.         1.         1.
 0.99451153 1.        ]

Kappa:
0.9938273726612139
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f51072e8828>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.343, val_acc:0.580]
Epoch [2/120    avg_loss:1.861, val_acc:0.598]
Epoch [3/120    avg_loss:1.489, val_acc:0.678]
Epoch [4/120    avg_loss:1.200, val_acc:0.715]
Epoch [5/120    avg_loss:0.991, val_acc:0.783]
Epoch [6/120    avg_loss:0.853, val_acc:0.775]
Epoch [7/120    avg_loss:0.753, val_acc:0.838]
Epoch [8/120    avg_loss:0.665, val_acc:0.865]
Epoch [9/120    avg_loss:0.610, val_acc:0.859]
Epoch [10/120    avg_loss:0.558, val_acc:0.863]
Epoch [11/120    avg_loss:0.485, val_acc:0.861]
Epoch [12/120    avg_loss:0.511, val_acc:0.875]
Epoch [13/120    avg_loss:0.481, val_acc:0.898]
Epoch [14/120    avg_loss:0.462, val_acc:0.941]
Epoch [15/120    avg_loss:0.423, val_acc:0.916]
Epoch [16/120    avg_loss:0.303, val_acc:0.928]
Epoch [17/120    avg_loss:0.295, val_acc:0.957]
Epoch [18/120    avg_loss:0.306, val_acc:0.893]
Epoch [19/120    avg_loss:0.355, val_acc:0.918]
Epoch [20/120    avg_loss:0.337, val_acc:0.959]
Epoch [21/120    avg_loss:0.287, val_acc:0.928]
Epoch [22/120    avg_loss:0.244, val_acc:0.957]
Epoch [23/120    avg_loss:0.220, val_acc:0.945]
Epoch [24/120    avg_loss:0.220, val_acc:0.953]
Epoch [25/120    avg_loss:0.174, val_acc:0.953]
Epoch [26/120    avg_loss:0.208, val_acc:0.898]
Epoch [27/120    avg_loss:0.247, val_acc:0.957]
Epoch [28/120    avg_loss:0.165, val_acc:0.957]
Epoch [29/120    avg_loss:0.143, val_acc:0.961]
Epoch [30/120    avg_loss:0.143, val_acc:0.967]
Epoch [31/120    avg_loss:0.176, val_acc:0.977]
Epoch [32/120    avg_loss:0.152, val_acc:0.963]
Epoch [33/120    avg_loss:0.115, val_acc:0.984]
Epoch [34/120    avg_loss:0.117, val_acc:0.953]
Epoch [35/120    avg_loss:0.131, val_acc:0.955]
Epoch [36/120    avg_loss:0.177, val_acc:0.957]
Epoch [37/120    avg_loss:0.133, val_acc:0.965]
Epoch [38/120    avg_loss:0.123, val_acc:0.963]
Epoch [39/120    avg_loss:0.135, val_acc:0.977]
Epoch [40/120    avg_loss:0.130, val_acc:0.969]
Epoch [41/120    avg_loss:0.107, val_acc:0.973]
Epoch [42/120    avg_loss:0.130, val_acc:0.982]
Epoch [43/120    avg_loss:0.093, val_acc:0.980]
Epoch [44/120    avg_loss:0.109, val_acc:0.963]
Epoch [45/120    avg_loss:0.133, val_acc:0.992]
Epoch [46/120    avg_loss:0.059, val_acc:0.984]
Epoch [47/120    avg_loss:0.065, val_acc:0.967]
Epoch [48/120    avg_loss:0.090, val_acc:0.988]
Epoch [49/120    avg_loss:0.063, val_acc:0.992]
Epoch [50/120    avg_loss:0.052, val_acc:0.990]
Epoch [51/120    avg_loss:0.135, val_acc:0.988]
Epoch [52/120    avg_loss:0.058, val_acc:0.986]
Epoch [53/120    avg_loss:0.070, val_acc:0.990]
Epoch [54/120    avg_loss:0.056, val_acc:0.973]
Epoch [55/120    avg_loss:0.145, val_acc:0.971]
Epoch [56/120    avg_loss:0.066, val_acc:0.982]
Epoch [57/120    avg_loss:0.043, val_acc:0.986]
Epoch [58/120    avg_loss:0.049, val_acc:0.980]
Epoch [59/120    avg_loss:0.079, val_acc:0.980]
Epoch [60/120    avg_loss:0.063, val_acc:0.969]
Epoch [61/120    avg_loss:0.071, val_acc:0.988]
Epoch [62/120    avg_loss:0.058, val_acc:0.990]
Epoch [63/120    avg_loss:0.040, val_acc:0.992]
Epoch [64/120    avg_loss:0.021, val_acc:0.994]
Epoch [65/120    avg_loss:0.030, val_acc:0.996]
Epoch [66/120    avg_loss:0.023, val_acc:0.996]
Epoch [67/120    avg_loss:0.029, val_acc:0.996]
Epoch [68/120    avg_loss:0.022, val_acc:0.996]
Epoch [69/120    avg_loss:0.023, val_acc:0.996]
Epoch [70/120    avg_loss:0.020, val_acc:0.996]
Epoch [71/120    avg_loss:0.023, val_acc:0.996]
Epoch [72/120    avg_loss:0.019, val_acc:0.996]
Epoch [73/120    avg_loss:0.022, val_acc:0.996]
Epoch [74/120    avg_loss:0.021, val_acc:0.996]
Epoch [75/120    avg_loss:0.029, val_acc:0.996]
Epoch [76/120    avg_loss:0.017, val_acc:0.996]
Epoch [77/120    avg_loss:0.018, val_acc:0.996]
Epoch [78/120    avg_loss:0.019, val_acc:0.996]
Epoch [79/120    avg_loss:0.021, val_acc:0.996]
Epoch [80/120    avg_loss:0.022, val_acc:0.996]
Epoch [81/120    avg_loss:0.018, val_acc:0.996]
Epoch [82/120    avg_loss:0.022, val_acc:0.994]
Epoch [83/120    avg_loss:0.024, val_acc:0.996]
Epoch [84/120    avg_loss:0.020, val_acc:0.996]
Epoch [85/120    avg_loss:0.024, val_acc:0.996]
Epoch [86/120    avg_loss:0.016, val_acc:0.996]
Epoch [87/120    avg_loss:0.020, val_acc:0.996]
Epoch [88/120    avg_loss:0.019, val_acc:0.996]
Epoch [89/120    avg_loss:0.018, val_acc:0.996]
Epoch [90/120    avg_loss:0.017, val_acc:0.996]
Epoch [91/120    avg_loss:0.015, val_acc:0.996]
Epoch [92/120    avg_loss:0.018, val_acc:0.996]
Epoch [93/120    avg_loss:0.024, val_acc:0.996]
Epoch [94/120    avg_loss:0.017, val_acc:0.996]
Epoch [95/120    avg_loss:0.015, val_acc:0.996]
Epoch [96/120    avg_loss:0.016, val_acc:0.996]
Epoch [97/120    avg_loss:0.034, val_acc:0.994]
Epoch [98/120    avg_loss:0.020, val_acc:0.996]
Epoch [99/120    avg_loss:0.021, val_acc:0.996]
Epoch [100/120    avg_loss:0.019, val_acc:0.996]
Epoch [101/120    avg_loss:0.015, val_acc:0.996]
Epoch [102/120    avg_loss:0.018, val_acc:0.996]
Epoch [103/120    avg_loss:0.019, val_acc:0.994]
Epoch [104/120    avg_loss:0.022, val_acc:0.996]
Epoch [105/120    avg_loss:0.016, val_acc:0.996]
Epoch [106/120    avg_loss:0.014, val_acc:0.996]
Epoch [107/120    avg_loss:0.016, val_acc:0.996]
Epoch [108/120    avg_loss:0.017, val_acc:0.996]
Epoch [109/120    avg_loss:0.019, val_acc:0.996]
Epoch [110/120    avg_loss:0.016, val_acc:0.996]
Epoch [111/120    avg_loss:0.016, val_acc:0.996]
Epoch [112/120    avg_loss:0.015, val_acc:0.996]
Epoch [113/120    avg_loss:0.020, val_acc:0.996]
Epoch [114/120    avg_loss:0.017, val_acc:0.994]
Epoch [115/120    avg_loss:0.017, val_acc:0.996]
Epoch [116/120    avg_loss:0.013, val_acc:0.996]
Epoch [117/120    avg_loss:0.017, val_acc:0.996]
Epoch [118/120    avg_loss:0.012, val_acc:0.996]
Epoch [119/120    avg_loss:0.014, val_acc:0.996]
Epoch [120/120    avg_loss:0.017, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   1   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 226   1   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.55223880597015

F1 scores:
[       nan 0.99853801 0.99545455 1.         0.96375267 0.93478261
 0.99757869 0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.995014873130318
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fca0e0608d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.328, val_acc:0.535]
Epoch [2/120    avg_loss:1.865, val_acc:0.590]
Epoch [3/120    avg_loss:1.441, val_acc:0.707]
Epoch [4/120    avg_loss:1.128, val_acc:0.768]
Epoch [5/120    avg_loss:0.928, val_acc:0.779]
Epoch [6/120    avg_loss:0.850, val_acc:0.758]
Epoch [7/120    avg_loss:0.856, val_acc:0.820]
Epoch [8/120    avg_loss:0.727, val_acc:0.830]
Epoch [9/120    avg_loss:0.614, val_acc:0.857]
Epoch [10/120    avg_loss:0.619, val_acc:0.871]
Epoch [11/120    avg_loss:0.523, val_acc:0.809]
Epoch [12/120    avg_loss:0.513, val_acc:0.900]
Epoch [13/120    avg_loss:0.494, val_acc:0.834]
Epoch [14/120    avg_loss:0.437, val_acc:0.908]
Epoch [15/120    avg_loss:0.420, val_acc:0.895]
Epoch [16/120    avg_loss:0.426, val_acc:0.902]
Epoch [17/120    avg_loss:0.482, val_acc:0.910]
Epoch [18/120    avg_loss:0.398, val_acc:0.926]
Epoch [19/120    avg_loss:0.311, val_acc:0.895]
Epoch [20/120    avg_loss:0.290, val_acc:0.930]
Epoch [21/120    avg_loss:0.293, val_acc:0.908]
Epoch [22/120    avg_loss:0.256, val_acc:0.953]
Epoch [23/120    avg_loss:0.290, val_acc:0.939]
Epoch [24/120    avg_loss:0.280, val_acc:0.924]
Epoch [25/120    avg_loss:0.248, val_acc:0.926]
Epoch [26/120    avg_loss:0.268, val_acc:0.898]
Epoch [27/120    avg_loss:0.282, val_acc:0.949]
Epoch [28/120    avg_loss:0.207, val_acc:0.947]
Epoch [29/120    avg_loss:0.215, val_acc:0.955]
Epoch [30/120    avg_loss:0.235, val_acc:0.949]
Epoch [31/120    avg_loss:0.209, val_acc:0.957]
Epoch [32/120    avg_loss:0.204, val_acc:0.963]
Epoch [33/120    avg_loss:0.233, val_acc:0.930]
Epoch [34/120    avg_loss:0.244, val_acc:0.930]
Epoch [35/120    avg_loss:0.265, val_acc:0.930]
Epoch [36/120    avg_loss:0.162, val_acc:0.965]
Epoch [37/120    avg_loss:0.134, val_acc:0.943]
Epoch [38/120    avg_loss:0.157, val_acc:0.947]
Epoch [39/120    avg_loss:0.160, val_acc:0.957]
Epoch [40/120    avg_loss:0.139, val_acc:0.957]
Epoch [41/120    avg_loss:0.136, val_acc:0.961]
Epoch [42/120    avg_loss:0.115, val_acc:0.961]
Epoch [43/120    avg_loss:0.130, val_acc:0.936]
Epoch [44/120    avg_loss:0.121, val_acc:0.949]
Epoch [45/120    avg_loss:0.177, val_acc:0.963]
Epoch [46/120    avg_loss:0.118, val_acc:0.975]
Epoch [47/120    avg_loss:0.125, val_acc:0.945]
Epoch [48/120    avg_loss:0.117, val_acc:0.967]
Epoch [49/120    avg_loss:0.125, val_acc:0.953]
Epoch [50/120    avg_loss:0.133, val_acc:0.922]
Epoch [51/120    avg_loss:0.118, val_acc:0.977]
Epoch [52/120    avg_loss:0.110, val_acc:0.977]
Epoch [53/120    avg_loss:0.073, val_acc:0.969]
Epoch [54/120    avg_loss:0.130, val_acc:0.965]
Epoch [55/120    avg_loss:0.107, val_acc:0.963]
Epoch [56/120    avg_loss:0.131, val_acc:0.965]
Epoch [57/120    avg_loss:0.111, val_acc:0.959]
Epoch [58/120    avg_loss:0.106, val_acc:0.973]
Epoch [59/120    avg_loss:0.146, val_acc:0.965]
Epoch [60/120    avg_loss:0.123, val_acc:0.982]
Epoch [61/120    avg_loss:0.086, val_acc:0.982]
Epoch [62/120    avg_loss:0.074, val_acc:0.988]
Epoch [63/120    avg_loss:0.065, val_acc:0.980]
Epoch [64/120    avg_loss:0.076, val_acc:0.982]
Epoch [65/120    avg_loss:0.056, val_acc:0.982]
Epoch [66/120    avg_loss:0.079, val_acc:0.963]
Epoch [67/120    avg_loss:0.125, val_acc:0.965]
Epoch [68/120    avg_loss:0.099, val_acc:0.973]
Epoch [69/120    avg_loss:0.071, val_acc:0.965]
Epoch [70/120    avg_loss:0.106, val_acc:0.977]
Epoch [71/120    avg_loss:0.074, val_acc:0.906]
Epoch [72/120    avg_loss:0.151, val_acc:0.980]
Epoch [73/120    avg_loss:0.087, val_acc:0.986]
Epoch [74/120    avg_loss:0.057, val_acc:0.977]
Epoch [75/120    avg_loss:0.055, val_acc:0.980]
Epoch [76/120    avg_loss:0.034, val_acc:0.986]
Epoch [77/120    avg_loss:0.046, val_acc:0.986]
Epoch [78/120    avg_loss:0.034, val_acc:0.986]
Epoch [79/120    avg_loss:0.026, val_acc:0.986]
Epoch [80/120    avg_loss:0.026, val_acc:0.988]
Epoch [81/120    avg_loss:0.026, val_acc:0.990]
Epoch [82/120    avg_loss:0.023, val_acc:0.988]
Epoch [83/120    avg_loss:0.031, val_acc:0.986]
Epoch [84/120    avg_loss:0.024, val_acc:0.988]
Epoch [85/120    avg_loss:0.030, val_acc:0.988]
Epoch [86/120    avg_loss:0.035, val_acc:0.988]
Epoch [87/120    avg_loss:0.021, val_acc:0.990]
Epoch [88/120    avg_loss:0.025, val_acc:0.988]
Epoch [89/120    avg_loss:0.027, val_acc:0.990]
Epoch [90/120    avg_loss:0.019, val_acc:0.988]
Epoch [91/120    avg_loss:0.035, val_acc:0.988]
Epoch [92/120    avg_loss:0.027, val_acc:0.988]
Epoch [93/120    avg_loss:0.021, val_acc:0.988]
Epoch [94/120    avg_loss:0.029, val_acc:0.988]
Epoch [95/120    avg_loss:0.021, val_acc:0.988]
Epoch [96/120    avg_loss:0.018, val_acc:0.990]
Epoch [97/120    avg_loss:0.022, val_acc:0.988]
Epoch [98/120    avg_loss:0.031, val_acc:0.988]
Epoch [99/120    avg_loss:0.021, val_acc:0.986]
Epoch [100/120    avg_loss:0.022, val_acc:0.988]
Epoch [101/120    avg_loss:0.022, val_acc:0.988]
Epoch [102/120    avg_loss:0.023, val_acc:0.988]
Epoch [103/120    avg_loss:0.027, val_acc:0.988]
Epoch [104/120    avg_loss:0.019, val_acc:0.988]
Epoch [105/120    avg_loss:0.018, val_acc:0.990]
Epoch [106/120    avg_loss:0.019, val_acc:0.990]
Epoch [107/120    avg_loss:0.019, val_acc:0.990]
Epoch [108/120    avg_loss:0.023, val_acc:0.988]
Epoch [109/120    avg_loss:0.019, val_acc:0.990]
Epoch [110/120    avg_loss:0.031, val_acc:0.988]
Epoch [111/120    avg_loss:0.020, val_acc:0.990]
Epoch [112/120    avg_loss:0.016, val_acc:0.990]
Epoch [113/120    avg_loss:0.021, val_acc:0.990]
Epoch [114/120    avg_loss:0.017, val_acc:0.990]
Epoch [115/120    avg_loss:0.027, val_acc:0.990]
Epoch [116/120    avg_loss:0.016, val_acc:0.990]
Epoch [117/120    avg_loss:0.029, val_acc:0.990]
Epoch [118/120    avg_loss:0.019, val_acc:0.990]
Epoch [119/120    avg_loss:0.027, val_acc:0.990]
Epoch [120/120    avg_loss:0.018, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 0.99853801 0.99545455 1.         0.95032397 0.91814947
 0.99516908 0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9935906909886016
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd866e11860>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.323, val_acc:0.611]
Epoch [2/120    avg_loss:1.811, val_acc:0.633]
Epoch [3/120    avg_loss:1.438, val_acc:0.685]
Epoch [4/120    avg_loss:1.163, val_acc:0.742]
Epoch [5/120    avg_loss:1.001, val_acc:0.808]
Epoch [6/120    avg_loss:0.879, val_acc:0.770]
Epoch [7/120    avg_loss:0.770, val_acc:0.859]
Epoch [8/120    avg_loss:0.630, val_acc:0.764]
Epoch [9/120    avg_loss:0.564, val_acc:0.861]
Epoch [10/120    avg_loss:0.607, val_acc:0.871]
Epoch [11/120    avg_loss:0.582, val_acc:0.867]
Epoch [12/120    avg_loss:0.440, val_acc:0.895]
Epoch [13/120    avg_loss:0.419, val_acc:0.883]
Epoch [14/120    avg_loss:0.426, val_acc:0.909]
Epoch [15/120    avg_loss:0.383, val_acc:0.909]
Epoch [16/120    avg_loss:0.357, val_acc:0.897]
Epoch [17/120    avg_loss:0.327, val_acc:0.915]
Epoch [18/120    avg_loss:0.284, val_acc:0.942]
Epoch [19/120    avg_loss:0.308, val_acc:0.940]
Epoch [20/120    avg_loss:0.323, val_acc:0.911]
Epoch [21/120    avg_loss:0.426, val_acc:0.901]
Epoch [22/120    avg_loss:0.272, val_acc:0.944]
Epoch [23/120    avg_loss:0.250, val_acc:0.929]
Epoch [24/120    avg_loss:0.236, val_acc:0.942]
Epoch [25/120    avg_loss:0.210, val_acc:0.960]
Epoch [26/120    avg_loss:0.241, val_acc:0.950]
Epoch [27/120    avg_loss:0.192, val_acc:0.940]
Epoch [28/120    avg_loss:0.212, val_acc:0.954]
Epoch [29/120    avg_loss:0.170, val_acc:0.929]
Epoch [30/120    avg_loss:0.193, val_acc:0.952]
Epoch [31/120    avg_loss:0.248, val_acc:0.915]
Epoch [32/120    avg_loss:0.202, val_acc:0.950]
Epoch [33/120    avg_loss:0.151, val_acc:0.944]
Epoch [34/120    avg_loss:0.143, val_acc:0.970]
Epoch [35/120    avg_loss:0.131, val_acc:0.962]
Epoch [36/120    avg_loss:0.156, val_acc:0.968]
Epoch [37/120    avg_loss:0.139, val_acc:0.942]
Epoch [38/120    avg_loss:0.126, val_acc:0.966]
Epoch [39/120    avg_loss:0.120, val_acc:0.958]
Epoch [40/120    avg_loss:0.169, val_acc:0.970]
Epoch [41/120    avg_loss:0.117, val_acc:0.956]
Epoch [42/120    avg_loss:0.111, val_acc:0.956]
Epoch [43/120    avg_loss:0.082, val_acc:0.968]
Epoch [44/120    avg_loss:0.123, val_acc:0.976]
Epoch [45/120    avg_loss:0.111, val_acc:0.966]
Epoch [46/120    avg_loss:0.059, val_acc:0.974]
Epoch [47/120    avg_loss:0.054, val_acc:0.972]
Epoch [48/120    avg_loss:0.083, val_acc:0.976]
Epoch [49/120    avg_loss:0.160, val_acc:0.964]
Epoch [50/120    avg_loss:0.081, val_acc:0.980]
Epoch [51/120    avg_loss:0.081, val_acc:0.970]
Epoch [52/120    avg_loss:0.111, val_acc:0.978]
Epoch [53/120    avg_loss:0.075, val_acc:0.972]
Epoch [54/120    avg_loss:0.091, val_acc:0.978]
Epoch [55/120    avg_loss:0.044, val_acc:0.978]
Epoch [56/120    avg_loss:0.027, val_acc:0.978]
Epoch [57/120    avg_loss:0.031, val_acc:0.982]
Epoch [58/120    avg_loss:0.089, val_acc:0.964]
Epoch [59/120    avg_loss:0.039, val_acc:0.982]
Epoch [60/120    avg_loss:0.041, val_acc:0.984]
Epoch [61/120    avg_loss:0.038, val_acc:0.986]
Epoch [62/120    avg_loss:0.025, val_acc:0.986]
Epoch [63/120    avg_loss:0.024, val_acc:0.984]
Epoch [64/120    avg_loss:0.049, val_acc:0.948]
Epoch [65/120    avg_loss:0.110, val_acc:0.905]
Epoch [66/120    avg_loss:0.070, val_acc:0.978]
Epoch [67/120    avg_loss:0.032, val_acc:0.984]
Epoch [68/120    avg_loss:0.041, val_acc:0.984]
Epoch [69/120    avg_loss:0.055, val_acc:0.986]
Epoch [70/120    avg_loss:0.119, val_acc:0.984]
Epoch [71/120    avg_loss:0.037, val_acc:0.984]
Epoch [72/120    avg_loss:0.026, val_acc:0.986]
Epoch [73/120    avg_loss:0.027, val_acc:0.984]
Epoch [74/120    avg_loss:0.024, val_acc:0.988]
Epoch [75/120    avg_loss:0.025, val_acc:0.988]
Epoch [76/120    avg_loss:0.029, val_acc:0.986]
Epoch [77/120    avg_loss:0.025, val_acc:0.984]
Epoch [78/120    avg_loss:0.016, val_acc:0.986]
Epoch [79/120    avg_loss:0.027, val_acc:0.970]
Epoch [80/120    avg_loss:0.140, val_acc:0.954]
Epoch [81/120    avg_loss:0.084, val_acc:0.978]
Epoch [82/120    avg_loss:0.030, val_acc:0.994]
Epoch [83/120    avg_loss:0.026, val_acc:0.986]
Epoch [84/120    avg_loss:0.025, val_acc:0.970]
Epoch [85/120    avg_loss:0.048, val_acc:0.986]
Epoch [86/120    avg_loss:0.033, val_acc:0.990]
Epoch [87/120    avg_loss:0.044, val_acc:0.988]
Epoch [88/120    avg_loss:0.031, val_acc:0.986]
Epoch [89/120    avg_loss:0.019, val_acc:0.988]
Epoch [90/120    avg_loss:0.011, val_acc:0.986]
Epoch [91/120    avg_loss:0.019, val_acc:0.988]
Epoch [92/120    avg_loss:0.013, val_acc:0.988]
Epoch [93/120    avg_loss:0.017, val_acc:0.990]
Epoch [94/120    avg_loss:0.011, val_acc:0.986]
Epoch [95/120    avg_loss:0.007, val_acc:0.990]
Epoch [96/120    avg_loss:0.015, val_acc:0.988]
Epoch [97/120    avg_loss:0.010, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.988]
Epoch [100/120    avg_loss:0.006, val_acc:0.988]
Epoch [101/120    avg_loss:0.009, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.988]
Epoch [104/120    avg_loss:0.007, val_acc:0.984]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.007, val_acc:0.988]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.014, val_acc:0.988]
Epoch [110/120    avg_loss:0.006, val_acc:0.988]
Epoch [111/120    avg_loss:0.006, val_acc:0.988]
Epoch [112/120    avg_loss:0.006, val_acc:0.988]
Epoch [113/120    avg_loss:0.007, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.005, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.009, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   0   0   0   0   0   0   0   6   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.996337   0.98871332 0.99782135 0.95878525 0.95683453
 0.98800959 0.9726776  1.         1.         1.         1.
 0.99342105 1.        ]

Kappa:
0.9931157402441901
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe931a3a940>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.347, val_acc:0.561]
Epoch [2/120    avg_loss:1.831, val_acc:0.680]
Epoch [3/120    avg_loss:1.440, val_acc:0.711]
Epoch [4/120    avg_loss:1.151, val_acc:0.715]
Epoch [5/120    avg_loss:0.950, val_acc:0.834]
Epoch [6/120    avg_loss:0.815, val_acc:0.869]
Epoch [7/120    avg_loss:0.799, val_acc:0.793]
Epoch [8/120    avg_loss:0.697, val_acc:0.861]
Epoch [9/120    avg_loss:0.578, val_acc:0.904]
Epoch [10/120    avg_loss:0.615, val_acc:0.910]
Epoch [11/120    avg_loss:0.518, val_acc:0.920]
Epoch [12/120    avg_loss:0.422, val_acc:0.850]
Epoch [13/120    avg_loss:0.450, val_acc:0.910]
Epoch [14/120    avg_loss:0.332, val_acc:0.930]
Epoch [15/120    avg_loss:0.339, val_acc:0.953]
Epoch [16/120    avg_loss:0.377, val_acc:0.947]
Epoch [17/120    avg_loss:0.375, val_acc:0.893]
Epoch [18/120    avg_loss:0.286, val_acc:0.951]
Epoch [19/120    avg_loss:0.265, val_acc:0.939]
Epoch [20/120    avg_loss:0.253, val_acc:0.961]
Epoch [21/120    avg_loss:0.252, val_acc:0.963]
Epoch [22/120    avg_loss:0.206, val_acc:0.947]
Epoch [23/120    avg_loss:0.259, val_acc:0.951]
Epoch [24/120    avg_loss:0.221, val_acc:0.963]
Epoch [25/120    avg_loss:0.246, val_acc:0.961]
Epoch [26/120    avg_loss:0.175, val_acc:0.953]
Epoch [27/120    avg_loss:0.186, val_acc:0.963]
Epoch [28/120    avg_loss:0.190, val_acc:0.969]
Epoch [29/120    avg_loss:0.144, val_acc:0.947]
Epoch [30/120    avg_loss:0.130, val_acc:0.973]
Epoch [31/120    avg_loss:0.167, val_acc:0.961]
Epoch [32/120    avg_loss:0.154, val_acc:0.982]
Epoch [33/120    avg_loss:0.106, val_acc:0.980]
Epoch [34/120    avg_loss:0.122, val_acc:0.990]
Epoch [35/120    avg_loss:0.075, val_acc:0.982]
Epoch [36/120    avg_loss:0.086, val_acc:0.980]
Epoch [37/120    avg_loss:0.101, val_acc:0.949]
Epoch [38/120    avg_loss:0.153, val_acc:0.953]
Epoch [39/120    avg_loss:0.163, val_acc:0.984]
Epoch [40/120    avg_loss:0.094, val_acc:0.980]
Epoch [41/120    avg_loss:0.116, val_acc:0.986]
Epoch [42/120    avg_loss:0.087, val_acc:0.982]
Epoch [43/120    avg_loss:0.126, val_acc:0.961]
Epoch [44/120    avg_loss:0.153, val_acc:0.973]
Epoch [45/120    avg_loss:0.099, val_acc:0.992]
Epoch [46/120    avg_loss:0.097, val_acc:0.980]
Epoch [47/120    avg_loss:0.079, val_acc:0.984]
Epoch [48/120    avg_loss:0.092, val_acc:0.967]
Epoch [49/120    avg_loss:0.095, val_acc:0.984]
Epoch [50/120    avg_loss:0.080, val_acc:0.980]
Epoch [51/120    avg_loss:0.065, val_acc:0.971]
Epoch [52/120    avg_loss:0.056, val_acc:0.986]
Epoch [53/120    avg_loss:0.050, val_acc:0.988]
Epoch [54/120    avg_loss:0.035, val_acc:0.984]
Epoch [55/120    avg_loss:0.026, val_acc:0.988]
Epoch [56/120    avg_loss:0.050, val_acc:0.984]
Epoch [57/120    avg_loss:0.045, val_acc:0.988]
Epoch [58/120    avg_loss:0.041, val_acc:0.982]
Epoch [59/120    avg_loss:0.029, val_acc:0.988]
Epoch [60/120    avg_loss:0.032, val_acc:0.988]
Epoch [61/120    avg_loss:0.028, val_acc:0.988]
Epoch [62/120    avg_loss:0.025, val_acc:0.988]
Epoch [63/120    avg_loss:0.020, val_acc:0.988]
Epoch [64/120    avg_loss:0.022, val_acc:0.990]
Epoch [65/120    avg_loss:0.019, val_acc:0.990]
Epoch [66/120    avg_loss:0.020, val_acc:0.988]
Epoch [67/120    avg_loss:0.020, val_acc:0.990]
Epoch [68/120    avg_loss:0.020, val_acc:0.988]
Epoch [69/120    avg_loss:0.016, val_acc:0.990]
Epoch [70/120    avg_loss:0.022, val_acc:0.992]
Epoch [71/120    avg_loss:0.016, val_acc:0.992]
Epoch [72/120    avg_loss:0.019, val_acc:0.992]
Epoch [73/120    avg_loss:0.017, val_acc:0.992]
Epoch [74/120    avg_loss:0.018, val_acc:0.992]
Epoch [75/120    avg_loss:0.020, val_acc:0.992]
Epoch [76/120    avg_loss:0.018, val_acc:0.992]
Epoch [77/120    avg_loss:0.020, val_acc:0.992]
Epoch [78/120    avg_loss:0.020, val_acc:0.992]
Epoch [79/120    avg_loss:0.018, val_acc:0.992]
Epoch [80/120    avg_loss:0.020, val_acc:0.992]
Epoch [81/120    avg_loss:0.017, val_acc:0.992]
Epoch [82/120    avg_loss:0.016, val_acc:0.992]
Epoch [83/120    avg_loss:0.024, val_acc:0.990]
Epoch [84/120    avg_loss:0.019, val_acc:0.990]
Epoch [85/120    avg_loss:0.015, val_acc:0.990]
Epoch [86/120    avg_loss:0.016, val_acc:0.990]
Epoch [87/120    avg_loss:0.017, val_acc:0.992]
Epoch [88/120    avg_loss:0.017, val_acc:0.992]
Epoch [89/120    avg_loss:0.016, val_acc:0.992]
Epoch [90/120    avg_loss:0.036, val_acc:0.992]
Epoch [91/120    avg_loss:0.017, val_acc:0.992]
Epoch [92/120    avg_loss:0.018, val_acc:0.992]
Epoch [93/120    avg_loss:0.019, val_acc:0.992]
Epoch [94/120    avg_loss:0.016, val_acc:0.994]
Epoch [95/120    avg_loss:0.016, val_acc:0.992]
Epoch [96/120    avg_loss:0.016, val_acc:0.992]
Epoch [97/120    avg_loss:0.016, val_acc:0.992]
Epoch [98/120    avg_loss:0.015, val_acc:0.992]
Epoch [99/120    avg_loss:0.020, val_acc:0.994]
Epoch [100/120    avg_loss:0.015, val_acc:0.994]
Epoch [101/120    avg_loss:0.018, val_acc:0.994]
Epoch [102/120    avg_loss:0.019, val_acc:0.992]
Epoch [103/120    avg_loss:0.017, val_acc:0.992]
Epoch [104/120    avg_loss:0.016, val_acc:0.992]
Epoch [105/120    avg_loss:0.019, val_acc:0.992]
Epoch [106/120    avg_loss:0.014, val_acc:0.992]
Epoch [107/120    avg_loss:0.014, val_acc:0.992]
Epoch [108/120    avg_loss:0.017, val_acc:0.992]
Epoch [109/120    avg_loss:0.014, val_acc:0.992]
Epoch [110/120    avg_loss:0.014, val_acc:0.992]
Epoch [111/120    avg_loss:0.016, val_acc:0.992]
Epoch [112/120    avg_loss:0.013, val_acc:0.992]
Epoch [113/120    avg_loss:0.016, val_acc:0.992]
Epoch [114/120    avg_loss:0.014, val_acc:0.992]
Epoch [115/120    avg_loss:0.012, val_acc:0.992]
Epoch [116/120    avg_loss:0.011, val_acc:0.992]
Epoch [117/120    avg_loss:0.014, val_acc:0.992]
Epoch [118/120    avg_loss:0.013, val_acc:0.992]
Epoch [119/120    avg_loss:0.014, val_acc:0.992]
Epoch [120/120    avg_loss:0.012, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 674   0   0   0   0  11   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 0.99190581 0.98871332 1.         0.94646681 0.90974729
 0.97399527 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9902691225986306
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fadce86b860>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.343, val_acc:0.549]
Epoch [2/120    avg_loss:1.785, val_acc:0.637]
Epoch [3/120    avg_loss:1.394, val_acc:0.701]
Epoch [4/120    avg_loss:1.148, val_acc:0.791]
Epoch [5/120    avg_loss:0.948, val_acc:0.738]
Epoch [6/120    avg_loss:0.770, val_acc:0.773]
Epoch [7/120    avg_loss:0.725, val_acc:0.852]
Epoch [8/120    avg_loss:0.630, val_acc:0.846]
Epoch [9/120    avg_loss:0.589, val_acc:0.883]
Epoch [10/120    avg_loss:0.564, val_acc:0.869]
Epoch [11/120    avg_loss:0.467, val_acc:0.830]
Epoch [12/120    avg_loss:0.407, val_acc:0.924]
Epoch [13/120    avg_loss:0.479, val_acc:0.912]
Epoch [14/120    avg_loss:0.407, val_acc:0.914]
Epoch [15/120    avg_loss:0.317, val_acc:0.904]
Epoch [16/120    avg_loss:0.288, val_acc:0.920]
Epoch [17/120    avg_loss:0.274, val_acc:0.945]
Epoch [18/120    avg_loss:0.289, val_acc:0.926]
Epoch [19/120    avg_loss:0.310, val_acc:0.908]
Epoch [20/120    avg_loss:0.306, val_acc:0.928]
Epoch [21/120    avg_loss:0.314, val_acc:0.947]
Epoch [22/120    avg_loss:0.275, val_acc:0.932]
Epoch [23/120    avg_loss:0.232, val_acc:0.945]
Epoch [24/120    avg_loss:0.263, val_acc:0.928]
Epoch [25/120    avg_loss:0.259, val_acc:0.941]
Epoch [26/120    avg_loss:0.194, val_acc:0.953]
Epoch [27/120    avg_loss:0.215, val_acc:0.949]
Epoch [28/120    avg_loss:0.180, val_acc:0.930]
Epoch [29/120    avg_loss:0.195, val_acc:0.920]
Epoch [30/120    avg_loss:0.194, val_acc:0.951]
Epoch [31/120    avg_loss:0.187, val_acc:0.957]
Epoch [32/120    avg_loss:0.179, val_acc:0.953]
Epoch [33/120    avg_loss:0.161, val_acc:0.961]
Epoch [34/120    avg_loss:0.158, val_acc:0.971]
Epoch [35/120    avg_loss:0.132, val_acc:0.971]
Epoch [36/120    avg_loss:0.142, val_acc:0.959]
Epoch [37/120    avg_loss:0.129, val_acc:0.943]
Epoch [38/120    avg_loss:0.130, val_acc:0.967]
Epoch [39/120    avg_loss:0.145, val_acc:0.971]
Epoch [40/120    avg_loss:0.170, val_acc:0.959]
Epoch [41/120    avg_loss:0.152, val_acc:0.971]
Epoch [42/120    avg_loss:0.118, val_acc:0.957]
Epoch [43/120    avg_loss:0.092, val_acc:0.957]
Epoch [44/120    avg_loss:0.100, val_acc:0.977]
Epoch [45/120    avg_loss:0.112, val_acc:0.963]
Epoch [46/120    avg_loss:0.102, val_acc:0.975]
Epoch [47/120    avg_loss:0.095, val_acc:0.986]
Epoch [48/120    avg_loss:0.071, val_acc:0.980]
Epoch [49/120    avg_loss:0.072, val_acc:0.973]
Epoch [50/120    avg_loss:0.119, val_acc:0.967]
Epoch [51/120    avg_loss:0.080, val_acc:0.975]
Epoch [52/120    avg_loss:0.048, val_acc:0.975]
Epoch [53/120    avg_loss:0.064, val_acc:0.982]
Epoch [54/120    avg_loss:0.075, val_acc:0.986]
Epoch [55/120    avg_loss:0.066, val_acc:0.980]
Epoch [56/120    avg_loss:0.120, val_acc:0.951]
Epoch [57/120    avg_loss:0.086, val_acc:0.953]
Epoch [58/120    avg_loss:0.099, val_acc:0.967]
Epoch [59/120    avg_loss:0.165, val_acc:0.971]
Epoch [60/120    avg_loss:0.093, val_acc:0.980]
Epoch [61/120    avg_loss:0.059, val_acc:0.980]
Epoch [62/120    avg_loss:0.053, val_acc:0.975]
Epoch [63/120    avg_loss:0.044, val_acc:0.975]
Epoch [64/120    avg_loss:0.040, val_acc:0.982]
Epoch [65/120    avg_loss:0.037, val_acc:0.984]
Epoch [66/120    avg_loss:0.053, val_acc:0.986]
Epoch [67/120    avg_loss:0.059, val_acc:0.988]
Epoch [68/120    avg_loss:0.062, val_acc:0.992]
Epoch [69/120    avg_loss:0.084, val_acc:0.980]
Epoch [70/120    avg_loss:0.072, val_acc:0.980]
Epoch [71/120    avg_loss:0.040, val_acc:0.986]
Epoch [72/120    avg_loss:0.032, val_acc:0.990]
Epoch [73/120    avg_loss:0.029, val_acc:0.988]
Epoch [74/120    avg_loss:0.023, val_acc:0.980]
Epoch [75/120    avg_loss:0.025, val_acc:0.988]
Epoch [76/120    avg_loss:0.019, val_acc:0.988]
Epoch [77/120    avg_loss:0.016, val_acc:0.988]
Epoch [78/120    avg_loss:0.021, val_acc:0.988]
Epoch [79/120    avg_loss:0.032, val_acc:0.986]
Epoch [80/120    avg_loss:0.049, val_acc:0.967]
Epoch [81/120    avg_loss:0.101, val_acc:0.973]
Epoch [82/120    avg_loss:0.066, val_acc:0.986]
Epoch [83/120    avg_loss:0.037, val_acc:0.986]
Epoch [84/120    avg_loss:0.025, val_acc:0.986]
Epoch [85/120    avg_loss:0.029, val_acc:0.984]
Epoch [86/120    avg_loss:0.026, val_acc:0.984]
Epoch [87/120    avg_loss:0.025, val_acc:0.984]
Epoch [88/120    avg_loss:0.021, val_acc:0.984]
Epoch [89/120    avg_loss:0.029, val_acc:0.984]
Epoch [90/120    avg_loss:0.020, val_acc:0.984]
Epoch [91/120    avg_loss:0.025, val_acc:0.984]
Epoch [92/120    avg_loss:0.020, val_acc:0.986]
Epoch [93/120    avg_loss:0.021, val_acc:0.984]
Epoch [94/120    avg_loss:0.017, val_acc:0.986]
Epoch [95/120    avg_loss:0.016, val_acc:0.986]
Epoch [96/120    avg_loss:0.016, val_acc:0.986]
Epoch [97/120    avg_loss:0.017, val_acc:0.984]
Epoch [98/120    avg_loss:0.015, val_acc:0.984]
Epoch [99/120    avg_loss:0.016, val_acc:0.984]
Epoch [100/120    avg_loss:0.017, val_acc:0.984]
Epoch [101/120    avg_loss:0.019, val_acc:0.984]
Epoch [102/120    avg_loss:0.016, val_acc:0.984]
Epoch [103/120    avg_loss:0.018, val_acc:0.986]
Epoch [104/120    avg_loss:0.019, val_acc:0.984]
Epoch [105/120    avg_loss:0.017, val_acc:0.986]
Epoch [106/120    avg_loss:0.020, val_acc:0.984]
Epoch [107/120    avg_loss:0.017, val_acc:0.984]
Epoch [108/120    avg_loss:0.017, val_acc:0.984]
Epoch [109/120    avg_loss:0.016, val_acc:0.984]
Epoch [110/120    avg_loss:0.018, val_acc:0.984]
Epoch [111/120    avg_loss:0.016, val_acc:0.984]
Epoch [112/120    avg_loss:0.015, val_acc:0.984]
Epoch [113/120    avg_loss:0.019, val_acc:0.984]
Epoch [114/120    avg_loss:0.013, val_acc:0.984]
Epoch [115/120    avg_loss:0.017, val_acc:0.984]
Epoch [116/120    avg_loss:0.020, val_acc:0.984]
Epoch [117/120    avg_loss:0.015, val_acc:0.984]
Epoch [118/120    avg_loss:0.017, val_acc:0.984]
Epoch [119/120    avg_loss:0.015, val_acc:0.984]
Epoch [120/120    avg_loss:0.017, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   6   0   0   0   0   0   0   1   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.99853801 0.99545455 1.         0.94827586 0.92142857
 0.99273608 0.98924731 1.         1.         1.         0.99867198
 0.99779736 1.        ]

Kappa:
0.9931157862396204
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc8adc94898>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.348, val_acc:0.572]
Epoch [2/120    avg_loss:1.858, val_acc:0.641]
Epoch [3/120    avg_loss:1.508, val_acc:0.691]
Epoch [4/120    avg_loss:1.167, val_acc:0.723]
Epoch [5/120    avg_loss:1.004, val_acc:0.793]
Epoch [6/120    avg_loss:0.860, val_acc:0.748]
Epoch [7/120    avg_loss:0.844, val_acc:0.844]
Epoch [8/120    avg_loss:0.685, val_acc:0.748]
Epoch [9/120    avg_loss:0.690, val_acc:0.740]
Epoch [10/120    avg_loss:0.597, val_acc:0.846]
Epoch [11/120    avg_loss:0.515, val_acc:0.869]
Epoch [12/120    avg_loss:0.510, val_acc:0.846]
Epoch [13/120    avg_loss:0.425, val_acc:0.914]
Epoch [14/120    avg_loss:0.443, val_acc:0.865]
Epoch [15/120    avg_loss:0.417, val_acc:0.908]
Epoch [16/120    avg_loss:0.424, val_acc:0.912]
Epoch [17/120    avg_loss:0.362, val_acc:0.926]
Epoch [18/120    avg_loss:0.347, val_acc:0.883]
Epoch [19/120    avg_loss:0.291, val_acc:0.908]
Epoch [20/120    avg_loss:0.296, val_acc:0.934]
Epoch [21/120    avg_loss:0.299, val_acc:0.951]
Epoch [22/120    avg_loss:0.258, val_acc:0.908]
Epoch [23/120    avg_loss:0.265, val_acc:0.939]
Epoch [24/120    avg_loss:0.274, val_acc:0.920]
Epoch [25/120    avg_loss:0.247, val_acc:0.947]
Epoch [26/120    avg_loss:0.282, val_acc:0.939]
Epoch [27/120    avg_loss:0.191, val_acc:0.951]
Epoch [28/120    avg_loss:0.330, val_acc:0.926]
Epoch [29/120    avg_loss:0.238, val_acc:0.969]
Epoch [30/120    avg_loss:0.250, val_acc:0.912]
Epoch [31/120    avg_loss:0.164, val_acc:0.949]
Epoch [32/120    avg_loss:0.167, val_acc:0.969]
Epoch [33/120    avg_loss:0.130, val_acc:0.977]
Epoch [34/120    avg_loss:0.132, val_acc:0.977]
Epoch [35/120    avg_loss:0.118, val_acc:0.975]
Epoch [36/120    avg_loss:0.121, val_acc:0.969]
Epoch [37/120    avg_loss:0.161, val_acc:0.965]
Epoch [38/120    avg_loss:0.138, val_acc:0.975]
Epoch [39/120    avg_loss:0.129, val_acc:0.982]
Epoch [40/120    avg_loss:0.130, val_acc:0.965]
Epoch [41/120    avg_loss:0.112, val_acc:0.967]
Epoch [42/120    avg_loss:0.096, val_acc:0.984]
Epoch [43/120    avg_loss:0.107, val_acc:0.988]
Epoch [44/120    avg_loss:0.107, val_acc:0.957]
Epoch [45/120    avg_loss:0.093, val_acc:0.965]
Epoch [46/120    avg_loss:0.106, val_acc:0.973]
Epoch [47/120    avg_loss:0.107, val_acc:0.957]
Epoch [48/120    avg_loss:0.111, val_acc:0.945]
Epoch [49/120    avg_loss:0.100, val_acc:0.980]
Epoch [50/120    avg_loss:0.075, val_acc:0.969]
Epoch [51/120    avg_loss:0.117, val_acc:0.975]
Epoch [52/120    avg_loss:0.064, val_acc:0.977]
Epoch [53/120    avg_loss:0.051, val_acc:0.984]
Epoch [54/120    avg_loss:0.043, val_acc:0.988]
Epoch [55/120    avg_loss:0.058, val_acc:0.984]
Epoch [56/120    avg_loss:0.049, val_acc:0.982]
Epoch [57/120    avg_loss:0.091, val_acc:0.988]
Epoch [58/120    avg_loss:0.074, val_acc:0.986]
Epoch [59/120    avg_loss:0.037, val_acc:0.992]
Epoch [60/120    avg_loss:0.035, val_acc:0.988]
Epoch [61/120    avg_loss:0.031, val_acc:0.988]
Epoch [62/120    avg_loss:0.040, val_acc:0.982]
Epoch [63/120    avg_loss:0.113, val_acc:0.934]
Epoch [64/120    avg_loss:0.082, val_acc:0.980]
Epoch [65/120    avg_loss:0.088, val_acc:0.977]
Epoch [66/120    avg_loss:0.051, val_acc:0.986]
Epoch [67/120    avg_loss:0.052, val_acc:0.982]
Epoch [68/120    avg_loss:0.061, val_acc:0.982]
Epoch [69/120    avg_loss:0.087, val_acc:0.986]
Epoch [70/120    avg_loss:0.033, val_acc:0.990]
Epoch [71/120    avg_loss:0.019, val_acc:0.992]
Epoch [72/120    avg_loss:0.016, val_acc:0.986]
Epoch [73/120    avg_loss:0.021, val_acc:0.992]
Epoch [74/120    avg_loss:0.014, val_acc:0.992]
Epoch [75/120    avg_loss:0.031, val_acc:0.965]
Epoch [76/120    avg_loss:0.055, val_acc:0.982]
Epoch [77/120    avg_loss:0.060, val_acc:0.973]
Epoch [78/120    avg_loss:0.053, val_acc:0.969]
Epoch [79/120    avg_loss:0.047, val_acc:0.984]
Epoch [80/120    avg_loss:0.040, val_acc:0.992]
Epoch [81/120    avg_loss:0.042, val_acc:0.990]
Epoch [82/120    avg_loss:0.041, val_acc:0.992]
Epoch [83/120    avg_loss:0.020, val_acc:0.990]
Epoch [84/120    avg_loss:0.016, val_acc:0.990]
Epoch [85/120    avg_loss:0.021, val_acc:0.992]
Epoch [86/120    avg_loss:0.014, val_acc:0.992]
Epoch [87/120    avg_loss:0.019, val_acc:0.994]
Epoch [88/120    avg_loss:0.013, val_acc:0.994]
Epoch [89/120    avg_loss:0.014, val_acc:0.994]
Epoch [90/120    avg_loss:0.012, val_acc:0.994]
Epoch [91/120    avg_loss:0.008, val_acc:0.992]
Epoch [92/120    avg_loss:0.007, val_acc:0.994]
Epoch [93/120    avg_loss:0.009, val_acc:0.994]
Epoch [94/120    avg_loss:0.014, val_acc:0.994]
Epoch [95/120    avg_loss:0.009, val_acc:0.996]
Epoch [96/120    avg_loss:0.012, val_acc:0.992]
Epoch [97/120    avg_loss:0.010, val_acc:0.996]
Epoch [98/120    avg_loss:0.007, val_acc:0.996]
Epoch [99/120    avg_loss:0.011, val_acc:0.984]
Epoch [100/120    avg_loss:0.073, val_acc:0.977]
Epoch [101/120    avg_loss:0.029, val_acc:0.986]
Epoch [102/120    avg_loss:0.024, val_acc:0.986]
Epoch [103/120    avg_loss:0.021, val_acc:0.988]
Epoch [104/120    avg_loss:0.143, val_acc:0.906]
Epoch [105/120    avg_loss:0.181, val_acc:0.969]
Epoch [106/120    avg_loss:0.084, val_acc:0.977]
Epoch [107/120    avg_loss:0.031, val_acc:0.986]
Epoch [108/120    avg_loss:0.019, val_acc:0.992]
Epoch [109/120    avg_loss:0.051, val_acc:0.971]
Epoch [110/120    avg_loss:0.041, val_acc:0.977]
Epoch [111/120    avg_loss:0.067, val_acc:0.984]
Epoch [112/120    avg_loss:0.026, val_acc:0.994]
Epoch [113/120    avg_loss:0.014, val_acc:0.994]
Epoch [114/120    avg_loss:0.022, val_acc:0.994]
Epoch [115/120    avg_loss:0.016, val_acc:0.994]
Epoch [116/120    avg_loss:0.011, val_acc:0.992]
Epoch [117/120    avg_loss:0.011, val_acc:0.992]
Epoch [118/120    avg_loss:0.022, val_acc:0.992]
Epoch [119/120    avg_loss:0.013, val_acc:0.992]
Epoch [120/120    avg_loss:0.014, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 225   2   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 0.99707174 1.         1.         0.95948827 0.93090909
 0.99038462 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9945404161046135
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb2f752d860>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.299, val_acc:0.555]
Epoch [2/120    avg_loss:1.806, val_acc:0.643]
Epoch [3/120    avg_loss:1.382, val_acc:0.674]
Epoch [4/120    avg_loss:1.084, val_acc:0.719]
Epoch [5/120    avg_loss:0.927, val_acc:0.807]
Epoch [6/120    avg_loss:0.805, val_acc:0.832]
Epoch [7/120    avg_loss:0.749, val_acc:0.768]
Epoch [8/120    avg_loss:0.592, val_acc:0.842]
Epoch [9/120    avg_loss:0.535, val_acc:0.830]
Epoch [10/120    avg_loss:0.549, val_acc:0.900]
Epoch [11/120    avg_loss:0.532, val_acc:0.906]
Epoch [12/120    avg_loss:0.419, val_acc:0.893]
Epoch [13/120    avg_loss:0.445, val_acc:0.906]
Epoch [14/120    avg_loss:0.434, val_acc:0.898]
Epoch [15/120    avg_loss:0.406, val_acc:0.941]
Epoch [16/120    avg_loss:0.386, val_acc:0.920]
Epoch [17/120    avg_loss:0.340, val_acc:0.957]
Epoch [18/120    avg_loss:0.294, val_acc:0.932]
Epoch [19/120    avg_loss:0.280, val_acc:0.947]
Epoch [20/120    avg_loss:0.279, val_acc:0.941]
Epoch [21/120    avg_loss:0.274, val_acc:0.939]
Epoch [22/120    avg_loss:0.299, val_acc:0.945]
Epoch [23/120    avg_loss:0.266, val_acc:0.957]
Epoch [24/120    avg_loss:0.234, val_acc:0.834]
Epoch [25/120    avg_loss:0.305, val_acc:0.967]
Epoch [26/120    avg_loss:0.236, val_acc:0.932]
Epoch [27/120    avg_loss:0.252, val_acc:0.936]
Epoch [28/120    avg_loss:0.238, val_acc:0.943]
Epoch [29/120    avg_loss:0.259, val_acc:0.922]
Epoch [30/120    avg_loss:0.251, val_acc:0.959]
Epoch [31/120    avg_loss:0.220, val_acc:0.934]
Epoch [32/120    avg_loss:0.182, val_acc:0.959]
Epoch [33/120    avg_loss:0.213, val_acc:0.963]
Epoch [34/120    avg_loss:0.236, val_acc:0.963]
Epoch [35/120    avg_loss:0.180, val_acc:0.971]
Epoch [36/120    avg_loss:0.153, val_acc:0.975]
Epoch [37/120    avg_loss:0.163, val_acc:0.943]
Epoch [38/120    avg_loss:0.218, val_acc:0.936]
Epoch [39/120    avg_loss:0.134, val_acc:0.982]
Epoch [40/120    avg_loss:0.159, val_acc:0.922]
Epoch [41/120    avg_loss:0.145, val_acc:0.971]
Epoch [42/120    avg_loss:0.118, val_acc:0.984]
Epoch [43/120    avg_loss:0.163, val_acc:0.930]
Epoch [44/120    avg_loss:0.150, val_acc:0.965]
Epoch [45/120    avg_loss:0.128, val_acc:0.992]
Epoch [46/120    avg_loss:0.107, val_acc:0.980]
Epoch [47/120    avg_loss:0.137, val_acc:0.982]
Epoch [48/120    avg_loss:0.122, val_acc:0.984]
Epoch [49/120    avg_loss:0.124, val_acc:0.980]
Epoch [50/120    avg_loss:0.118, val_acc:0.975]
Epoch [51/120    avg_loss:0.124, val_acc:0.982]
Epoch [52/120    avg_loss:0.085, val_acc:0.973]
Epoch [53/120    avg_loss:0.084, val_acc:0.986]
Epoch [54/120    avg_loss:0.060, val_acc:0.992]
Epoch [55/120    avg_loss:0.082, val_acc:0.971]
Epoch [56/120    avg_loss:0.105, val_acc:0.971]
Epoch [57/120    avg_loss:0.082, val_acc:0.977]
Epoch [58/120    avg_loss:0.098, val_acc:0.986]
Epoch [59/120    avg_loss:0.100, val_acc:0.980]
Epoch [60/120    avg_loss:0.064, val_acc:0.977]
Epoch [61/120    avg_loss:0.071, val_acc:0.957]
Epoch [62/120    avg_loss:0.107, val_acc:0.982]
Epoch [63/120    avg_loss:0.076, val_acc:0.975]
Epoch [64/120    avg_loss:0.099, val_acc:0.984]
Epoch [65/120    avg_loss:0.047, val_acc:0.994]
Epoch [66/120    avg_loss:0.100, val_acc:0.984]
Epoch [67/120    avg_loss:0.052, val_acc:0.986]
Epoch [68/120    avg_loss:0.037, val_acc:0.992]
Epoch [69/120    avg_loss:0.078, val_acc:0.973]
Epoch [70/120    avg_loss:0.094, val_acc:0.957]
Epoch [71/120    avg_loss:0.072, val_acc:0.992]
Epoch [72/120    avg_loss:0.081, val_acc:0.986]
Epoch [73/120    avg_loss:0.043, val_acc:0.986]
Epoch [74/120    avg_loss:0.036, val_acc:0.988]
Epoch [75/120    avg_loss:0.073, val_acc:0.980]
Epoch [76/120    avg_loss:0.050, val_acc:0.988]
Epoch [77/120    avg_loss:0.051, val_acc:0.988]
Epoch [78/120    avg_loss:0.039, val_acc:0.996]
Epoch [79/120    avg_loss:0.023, val_acc:0.998]
Epoch [80/120    avg_loss:0.025, val_acc:0.996]
Epoch [81/120    avg_loss:0.042, val_acc:0.977]
Epoch [82/120    avg_loss:0.074, val_acc:0.994]
Epoch [83/120    avg_loss:0.029, val_acc:0.990]
Epoch [84/120    avg_loss:0.024, val_acc:0.994]
Epoch [85/120    avg_loss:0.020, val_acc:0.998]
Epoch [86/120    avg_loss:0.018, val_acc:0.994]
Epoch [87/120    avg_loss:0.018, val_acc:1.000]
Epoch [88/120    avg_loss:0.013, val_acc:0.998]
Epoch [89/120    avg_loss:0.014, val_acc:0.982]
Epoch [90/120    avg_loss:0.018, val_acc:0.992]
Epoch [91/120    avg_loss:0.013, val_acc:0.998]
Epoch [92/120    avg_loss:0.021, val_acc:0.996]
Epoch [93/120    avg_loss:0.017, val_acc:0.988]
Epoch [94/120    avg_loss:0.015, val_acc:1.000]
Epoch [95/120    avg_loss:0.016, val_acc:0.994]
Epoch [96/120    avg_loss:0.015, val_acc:0.988]
Epoch [97/120    avg_loss:0.106, val_acc:0.977]
Epoch [98/120    avg_loss:0.033, val_acc:0.994]
Epoch [99/120    avg_loss:0.019, val_acc:0.984]
Epoch [100/120    avg_loss:0.016, val_acc:0.992]
Epoch [101/120    avg_loss:0.011, val_acc:1.000]
Epoch [102/120    avg_loss:0.013, val_acc:1.000]
Epoch [103/120    avg_loss:0.011, val_acc:0.996]
Epoch [104/120    avg_loss:0.016, val_acc:0.994]
Epoch [105/120    avg_loss:0.022, val_acc:0.994]
Epoch [106/120    avg_loss:0.009, val_acc:0.998]
Epoch [107/120    avg_loss:0.012, val_acc:0.994]
Epoch [108/120    avg_loss:0.012, val_acc:0.992]
Epoch [109/120    avg_loss:0.017, val_acc:0.992]
Epoch [110/120    avg_loss:0.008, val_acc:0.996]
Epoch [111/120    avg_loss:0.015, val_acc:0.996]
Epoch [112/120    avg_loss:0.009, val_acc:0.998]
Epoch [113/120    avg_loss:0.012, val_acc:0.994]
Epoch [114/120    avg_loss:0.008, val_acc:0.992]
Epoch [115/120    avg_loss:0.011, val_acc:0.992]
Epoch [116/120    avg_loss:0.011, val_acc:0.994]
Epoch [117/120    avg_loss:0.011, val_acc:0.992]
Epoch [118/120    avg_loss:0.006, val_acc:0.994]
Epoch [119/120    avg_loss:0.005, val_acc:0.994]
Epoch [120/120    avg_loss:0.007, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211   9   0   0   0   0   0   0   7   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 0.99707174 0.98871332 1.         0.94407159 0.93793103
 0.99038462 0.9726776  1.         1.         1.         1.
 0.99233297 1.        ]

Kappa:
0.9919289116598476
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f043c6ae898>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.295, val_acc:0.657]
Epoch [2/120    avg_loss:1.773, val_acc:0.692]
Epoch [3/120    avg_loss:1.361, val_acc:0.720]
Epoch [4/120    avg_loss:1.082, val_acc:0.754]
Epoch [5/120    avg_loss:0.982, val_acc:0.718]
Epoch [6/120    avg_loss:0.814, val_acc:0.788]
Epoch [7/120    avg_loss:0.693, val_acc:0.806]
Epoch [8/120    avg_loss:0.630, val_acc:0.806]
Epoch [9/120    avg_loss:0.574, val_acc:0.859]
Epoch [10/120    avg_loss:0.563, val_acc:0.875]
Epoch [11/120    avg_loss:0.533, val_acc:0.863]
Epoch [12/120    avg_loss:0.427, val_acc:0.907]
Epoch [13/120    avg_loss:0.408, val_acc:0.887]
Epoch [14/120    avg_loss:0.394, val_acc:0.895]
Epoch [15/120    avg_loss:0.317, val_acc:0.901]
Epoch [16/120    avg_loss:0.336, val_acc:0.891]
Epoch [17/120    avg_loss:0.321, val_acc:0.921]
Epoch [18/120    avg_loss:0.310, val_acc:0.903]
Epoch [19/120    avg_loss:0.314, val_acc:0.925]
Epoch [20/120    avg_loss:0.301, val_acc:0.931]
Epoch [21/120    avg_loss:0.284, val_acc:0.925]
Epoch [22/120    avg_loss:0.263, val_acc:0.913]
Epoch [23/120    avg_loss:0.273, val_acc:0.948]
Epoch [24/120    avg_loss:0.222, val_acc:0.946]
Epoch [25/120    avg_loss:0.226, val_acc:0.927]
Epoch [26/120    avg_loss:0.204, val_acc:0.946]
Epoch [27/120    avg_loss:0.157, val_acc:0.954]
Epoch [28/120    avg_loss:0.185, val_acc:0.933]
Epoch [29/120    avg_loss:0.168, val_acc:0.966]
Epoch [30/120    avg_loss:0.171, val_acc:0.948]
Epoch [31/120    avg_loss:0.191, val_acc:0.954]
Epoch [32/120    avg_loss:0.192, val_acc:0.960]
Epoch [33/120    avg_loss:0.176, val_acc:0.956]
Epoch [34/120    avg_loss:0.130, val_acc:0.960]
Epoch [35/120    avg_loss:0.154, val_acc:0.948]
Epoch [36/120    avg_loss:0.158, val_acc:0.962]
Epoch [37/120    avg_loss:0.172, val_acc:0.946]
Epoch [38/120    avg_loss:0.154, val_acc:0.935]
Epoch [39/120    avg_loss:0.126, val_acc:0.950]
Epoch [40/120    avg_loss:0.110, val_acc:0.982]
Epoch [41/120    avg_loss:0.093, val_acc:0.964]
Epoch [42/120    avg_loss:0.092, val_acc:0.974]
Epoch [43/120    avg_loss:0.063, val_acc:0.980]
Epoch [44/120    avg_loss:0.076, val_acc:0.974]
Epoch [45/120    avg_loss:0.087, val_acc:0.980]
Epoch [46/120    avg_loss:0.076, val_acc:0.978]
Epoch [47/120    avg_loss:0.095, val_acc:0.966]
Epoch [48/120    avg_loss:0.065, val_acc:0.946]
Epoch [49/120    avg_loss:0.129, val_acc:0.954]
Epoch [50/120    avg_loss:0.127, val_acc:0.940]
Epoch [51/120    avg_loss:0.155, val_acc:0.919]
Epoch [52/120    avg_loss:0.103, val_acc:0.980]
Epoch [53/120    avg_loss:0.078, val_acc:0.982]
Epoch [54/120    avg_loss:0.045, val_acc:0.984]
Epoch [55/120    avg_loss:0.060, val_acc:0.968]
Epoch [56/120    avg_loss:0.102, val_acc:0.976]
Epoch [57/120    avg_loss:0.057, val_acc:0.974]
Epoch [58/120    avg_loss:0.053, val_acc:0.976]
Epoch [59/120    avg_loss:0.048, val_acc:0.984]
Epoch [60/120    avg_loss:0.041, val_acc:0.984]
Epoch [61/120    avg_loss:0.048, val_acc:0.974]
Epoch [62/120    avg_loss:0.069, val_acc:0.976]
Epoch [63/120    avg_loss:0.034, val_acc:0.988]
Epoch [64/120    avg_loss:0.050, val_acc:0.984]
Epoch [65/120    avg_loss:0.030, val_acc:0.988]
Epoch [66/120    avg_loss:0.036, val_acc:0.980]
Epoch [67/120    avg_loss:0.025, val_acc:0.984]
Epoch [68/120    avg_loss:0.056, val_acc:0.982]
Epoch [69/120    avg_loss:0.055, val_acc:0.984]
Epoch [70/120    avg_loss:0.054, val_acc:0.990]
Epoch [71/120    avg_loss:0.040, val_acc:0.986]
Epoch [72/120    avg_loss:0.034, val_acc:0.990]
Epoch [73/120    avg_loss:0.027, val_acc:0.984]
Epoch [74/120    avg_loss:0.024, val_acc:0.984]
Epoch [75/120    avg_loss:0.041, val_acc:0.970]
Epoch [76/120    avg_loss:0.060, val_acc:0.978]
Epoch [77/120    avg_loss:0.050, val_acc:0.980]
Epoch [78/120    avg_loss:0.122, val_acc:0.946]
Epoch [79/120    avg_loss:0.142, val_acc:0.931]
Epoch [80/120    avg_loss:0.095, val_acc:0.974]
Epoch [81/120    avg_loss:0.052, val_acc:0.974]
Epoch [82/120    avg_loss:0.082, val_acc:0.978]
Epoch [83/120    avg_loss:0.052, val_acc:0.986]
Epoch [84/120    avg_loss:0.045, val_acc:0.988]
Epoch [85/120    avg_loss:0.030, val_acc:0.984]
Epoch [86/120    avg_loss:0.020, val_acc:0.984]
Epoch [87/120    avg_loss:0.025, val_acc:0.984]
Epoch [88/120    avg_loss:0.022, val_acc:0.986]
Epoch [89/120    avg_loss:0.013, val_acc:0.986]
Epoch [90/120    avg_loss:0.021, val_acc:0.988]
Epoch [91/120    avg_loss:0.022, val_acc:0.988]
Epoch [92/120    avg_loss:0.027, val_acc:0.986]
Epoch [93/120    avg_loss:0.016, val_acc:0.988]
Epoch [94/120    avg_loss:0.016, val_acc:0.988]
Epoch [95/120    avg_loss:0.013, val_acc:0.988]
Epoch [96/120    avg_loss:0.013, val_acc:0.988]
Epoch [97/120    avg_loss:0.016, val_acc:0.988]
Epoch [98/120    avg_loss:0.019, val_acc:0.986]
Epoch [99/120    avg_loss:0.026, val_acc:0.986]
Epoch [100/120    avg_loss:0.015, val_acc:0.986]
Epoch [101/120    avg_loss:0.017, val_acc:0.986]
Epoch [102/120    avg_loss:0.013, val_acc:0.986]
Epoch [103/120    avg_loss:0.014, val_acc:0.986]
Epoch [104/120    avg_loss:0.016, val_acc:0.986]
Epoch [105/120    avg_loss:0.019, val_acc:0.986]
Epoch [106/120    avg_loss:0.017, val_acc:0.988]
Epoch [107/120    avg_loss:0.016, val_acc:0.988]
Epoch [108/120    avg_loss:0.015, val_acc:0.988]
Epoch [109/120    avg_loss:0.013, val_acc:0.988]
Epoch [110/120    avg_loss:0.015, val_acc:0.988]
Epoch [111/120    avg_loss:0.015, val_acc:0.988]
Epoch [112/120    avg_loss:0.017, val_acc:0.988]
Epoch [113/120    avg_loss:0.012, val_acc:0.988]
Epoch [114/120    avg_loss:0.011, val_acc:0.988]
Epoch [115/120    avg_loss:0.016, val_acc:0.988]
Epoch [116/120    avg_loss:0.018, val_acc:0.988]
Epoch [117/120    avg_loss:0.013, val_acc:0.988]
Epoch [118/120    avg_loss:0.013, val_acc:0.988]
Epoch [119/120    avg_loss:0.018, val_acc:0.988]
Epoch [120/120    avg_loss:0.018, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 223   0   0   0   0   0   0   0   4   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 0.99926954 0.99095023 0.99782135 0.95913978 0.94927536
 0.99514563 0.97826087 0.998713   1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9940647871134051
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f11e5783898>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.467, val_acc:0.400]
Epoch [2/120    avg_loss:2.115, val_acc:0.506]
Epoch [3/120    avg_loss:1.844, val_acc:0.693]
Epoch [4/120    avg_loss:1.538, val_acc:0.705]
Epoch [5/120    avg_loss:1.352, val_acc:0.674]
Epoch [6/120    avg_loss:1.183, val_acc:0.662]
Epoch [7/120    avg_loss:1.077, val_acc:0.758]
Epoch [8/120    avg_loss:0.977, val_acc:0.830]
Epoch [9/120    avg_loss:0.873, val_acc:0.834]
Epoch [10/120    avg_loss:0.815, val_acc:0.867]
Epoch [11/120    avg_loss:0.738, val_acc:0.854]
Epoch [12/120    avg_loss:0.740, val_acc:0.873]
Epoch [13/120    avg_loss:0.722, val_acc:0.857]
Epoch [14/120    avg_loss:0.573, val_acc:0.844]
Epoch [15/120    avg_loss:0.565, val_acc:0.873]
Epoch [16/120    avg_loss:0.535, val_acc:0.881]
Epoch [17/120    avg_loss:0.506, val_acc:0.859]
Epoch [18/120    avg_loss:0.497, val_acc:0.842]
Epoch [19/120    avg_loss:0.458, val_acc:0.896]
Epoch [20/120    avg_loss:0.445, val_acc:0.895]
Epoch [21/120    avg_loss:0.467, val_acc:0.895]
Epoch [22/120    avg_loss:0.460, val_acc:0.807]
Epoch [23/120    avg_loss:0.490, val_acc:0.900]
Epoch [24/120    avg_loss:0.468, val_acc:0.893]
Epoch [25/120    avg_loss:0.322, val_acc:0.912]
Epoch [26/120    avg_loss:0.343, val_acc:0.924]
Epoch [27/120    avg_loss:0.332, val_acc:0.861]
Epoch [28/120    avg_loss:0.389, val_acc:0.883]
Epoch [29/120    avg_loss:0.354, val_acc:0.930]
Epoch [30/120    avg_loss:0.276, val_acc:0.943]
Epoch [31/120    avg_loss:0.279, val_acc:0.924]
Epoch [32/120    avg_loss:0.331, val_acc:0.924]
Epoch [33/120    avg_loss:0.284, val_acc:0.926]
Epoch [34/120    avg_loss:0.274, val_acc:0.873]
Epoch [35/120    avg_loss:0.285, val_acc:0.934]
Epoch [36/120    avg_loss:0.288, val_acc:0.906]
Epoch [37/120    avg_loss:0.288, val_acc:0.939]
Epoch [38/120    avg_loss:0.243, val_acc:0.920]
Epoch [39/120    avg_loss:0.333, val_acc:0.949]
Epoch [40/120    avg_loss:0.265, val_acc:0.938]
Epoch [41/120    avg_loss:0.210, val_acc:0.922]
Epoch [42/120    avg_loss:0.191, val_acc:0.957]
Epoch [43/120    avg_loss:0.206, val_acc:0.941]
Epoch [44/120    avg_loss:0.225, val_acc:0.947]
Epoch [45/120    avg_loss:0.228, val_acc:0.941]
Epoch [46/120    avg_loss:0.207, val_acc:0.945]
Epoch [47/120    avg_loss:0.225, val_acc:0.963]
Epoch [48/120    avg_loss:0.171, val_acc:0.941]
Epoch [49/120    avg_loss:0.136, val_acc:0.957]
Epoch [50/120    avg_loss:0.248, val_acc:0.943]
Epoch [51/120    avg_loss:0.191, val_acc:0.965]
Epoch [52/120    avg_loss:0.144, val_acc:0.971]
Epoch [53/120    avg_loss:0.179, val_acc:0.941]
Epoch [54/120    avg_loss:0.151, val_acc:0.955]
Epoch [55/120    avg_loss:0.137, val_acc:0.975]
Epoch [56/120    avg_loss:0.144, val_acc:0.975]
Epoch [57/120    avg_loss:0.145, val_acc:0.967]
Epoch [58/120    avg_loss:0.129, val_acc:0.975]
Epoch [59/120    avg_loss:0.160, val_acc:0.975]
Epoch [60/120    avg_loss:0.100, val_acc:0.977]
Epoch [61/120    avg_loss:0.129, val_acc:0.975]
Epoch [62/120    avg_loss:0.118, val_acc:0.975]
Epoch [63/120    avg_loss:0.163, val_acc:0.961]
Epoch [64/120    avg_loss:0.116, val_acc:0.977]
Epoch [65/120    avg_loss:0.094, val_acc:0.959]
Epoch [66/120    avg_loss:0.104, val_acc:0.969]
Epoch [67/120    avg_loss:0.112, val_acc:0.961]
Epoch [68/120    avg_loss:0.114, val_acc:0.965]
Epoch [69/120    avg_loss:0.177, val_acc:0.967]
Epoch [70/120    avg_loss:0.100, val_acc:0.963]
Epoch [71/120    avg_loss:0.098, val_acc:0.951]
Epoch [72/120    avg_loss:0.119, val_acc:0.969]
Epoch [73/120    avg_loss:0.085, val_acc:0.928]
Epoch [74/120    avg_loss:0.156, val_acc:0.943]
Epoch [75/120    avg_loss:0.145, val_acc:0.953]
Epoch [76/120    avg_loss:0.172, val_acc:0.945]
Epoch [77/120    avg_loss:0.086, val_acc:0.977]
Epoch [78/120    avg_loss:0.152, val_acc:0.943]
Epoch [79/120    avg_loss:0.151, val_acc:0.977]
Epoch [80/120    avg_loss:0.141, val_acc:0.963]
Epoch [81/120    avg_loss:0.150, val_acc:0.963]
Epoch [82/120    avg_loss:0.070, val_acc:0.979]
Epoch [83/120    avg_loss:0.063, val_acc:0.973]
Epoch [84/120    avg_loss:0.058, val_acc:0.961]
Epoch [85/120    avg_loss:0.149, val_acc:0.934]
Epoch [86/120    avg_loss:0.148, val_acc:0.955]
Epoch [87/120    avg_loss:0.121, val_acc:0.977]
Epoch [88/120    avg_loss:0.072, val_acc:0.979]
Epoch [89/120    avg_loss:0.073, val_acc:0.969]
Epoch [90/120    avg_loss:0.108, val_acc:0.965]
Epoch [91/120    avg_loss:0.068, val_acc:0.971]
Epoch [92/120    avg_loss:0.070, val_acc:0.969]
Epoch [93/120    avg_loss:0.115, val_acc:0.953]
Epoch [94/120    avg_loss:0.092, val_acc:0.963]
Epoch [95/120    avg_loss:0.070, val_acc:0.967]
Epoch [96/120    avg_loss:0.087, val_acc:0.979]
Epoch [97/120    avg_loss:0.075, val_acc:0.979]
Epoch [98/120    avg_loss:0.053, val_acc:0.975]
Epoch [99/120    avg_loss:0.055, val_acc:0.971]
Epoch [100/120    avg_loss:0.050, val_acc:0.963]
Epoch [101/120    avg_loss:0.062, val_acc:0.953]
Epoch [102/120    avg_loss:0.060, val_acc:0.977]
Epoch [103/120    avg_loss:0.053, val_acc:0.973]
Epoch [104/120    avg_loss:0.110, val_acc:0.967]
Epoch [105/120    avg_loss:0.058, val_acc:0.980]
Epoch [106/120    avg_loss:0.040, val_acc:0.982]
Epoch [107/120    avg_loss:0.026, val_acc:0.982]
Epoch [108/120    avg_loss:0.021, val_acc:0.984]
Epoch [109/120    avg_loss:0.034, val_acc:0.971]
Epoch [110/120    avg_loss:0.045, val_acc:0.984]
Epoch [111/120    avg_loss:0.088, val_acc:0.971]
Epoch [112/120    avg_loss:0.101, val_acc:0.965]
Epoch [113/120    avg_loss:0.065, val_acc:0.980]
Epoch [114/120    avg_loss:0.036, val_acc:0.967]
Epoch [115/120    avg_loss:0.057, val_acc:0.973]
Epoch [116/120    avg_loss:0.045, val_acc:0.982]
Epoch [117/120    avg_loss:0.023, val_acc:0.982]
Epoch [118/120    avg_loss:0.023, val_acc:0.984]
Epoch [119/120    avg_loss:0.017, val_acc:0.980]
Epoch [120/120    avg_loss:0.022, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 202  24   0   0   0   0   4   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0   0 382   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.5501066098081

F1 scores:
[       nan 1.         0.98190045 0.93518519 0.88372093 0.89491525
 1.         0.98947368 0.99220779 0.99574468 1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9838586476692822
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5a0f16c898>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.460, val_acc:0.355]
Epoch [2/120    avg_loss:2.122, val_acc:0.543]
Epoch [3/120    avg_loss:1.862, val_acc:0.639]
Epoch [4/120    avg_loss:1.589, val_acc:0.609]
Epoch [5/120    avg_loss:1.347, val_acc:0.738]
Epoch [6/120    avg_loss:1.192, val_acc:0.701]
Epoch [7/120    avg_loss:1.063, val_acc:0.791]
Epoch [8/120    avg_loss:0.942, val_acc:0.805]
Epoch [9/120    avg_loss:0.836, val_acc:0.787]
Epoch [10/120    avg_loss:0.795, val_acc:0.770]
Epoch [11/120    avg_loss:0.775, val_acc:0.855]
Epoch [12/120    avg_loss:0.696, val_acc:0.820]
Epoch [13/120    avg_loss:0.597, val_acc:0.859]
Epoch [14/120    avg_loss:0.568, val_acc:0.855]
Epoch [15/120    avg_loss:0.602, val_acc:0.875]
Epoch [16/120    avg_loss:0.559, val_acc:0.857]
Epoch [17/120    avg_loss:0.506, val_acc:0.873]
Epoch [18/120    avg_loss:0.483, val_acc:0.898]
Epoch [19/120    avg_loss:0.450, val_acc:0.824]
Epoch [20/120    avg_loss:0.480, val_acc:0.859]
Epoch [21/120    avg_loss:0.406, val_acc:0.898]
Epoch [22/120    avg_loss:0.424, val_acc:0.885]
Epoch [23/120    avg_loss:0.355, val_acc:0.887]
Epoch [24/120    avg_loss:0.350, val_acc:0.883]
Epoch [25/120    avg_loss:0.342, val_acc:0.900]
Epoch [26/120    avg_loss:0.396, val_acc:0.926]
Epoch [27/120    avg_loss:0.311, val_acc:0.902]
Epoch [28/120    avg_loss:0.354, val_acc:0.906]
Epoch [29/120    avg_loss:0.324, val_acc:0.914]
Epoch [30/120    avg_loss:0.303, val_acc:0.904]
Epoch [31/120    avg_loss:0.270, val_acc:0.914]
Epoch [32/120    avg_loss:0.303, val_acc:0.881]
Epoch [33/120    avg_loss:0.300, val_acc:0.926]
Epoch [34/120    avg_loss:0.252, val_acc:0.916]
Epoch [35/120    avg_loss:0.289, val_acc:0.916]
Epoch [36/120    avg_loss:0.300, val_acc:0.891]
Epoch [37/120    avg_loss:0.301, val_acc:0.865]
Epoch [38/120    avg_loss:0.218, val_acc:0.920]
Epoch [39/120    avg_loss:0.260, val_acc:0.926]
Epoch [40/120    avg_loss:0.279, val_acc:0.902]
Epoch [41/120    avg_loss:0.306, val_acc:0.846]
Epoch [42/120    avg_loss:0.274, val_acc:0.938]
Epoch [43/120    avg_loss:0.206, val_acc:0.916]
Epoch [44/120    avg_loss:0.237, val_acc:0.896]
Epoch [45/120    avg_loss:0.215, val_acc:0.939]
Epoch [46/120    avg_loss:0.211, val_acc:0.930]
Epoch [47/120    avg_loss:0.209, val_acc:0.920]
Epoch [48/120    avg_loss:0.217, val_acc:0.928]
Epoch [49/120    avg_loss:0.195, val_acc:0.904]
Epoch [50/120    avg_loss:0.167, val_acc:0.941]
Epoch [51/120    avg_loss:0.157, val_acc:0.943]
Epoch [52/120    avg_loss:0.231, val_acc:0.795]
Epoch [53/120    avg_loss:0.203, val_acc:0.932]
Epoch [54/120    avg_loss:0.193, val_acc:0.930]
Epoch [55/120    avg_loss:0.277, val_acc:0.939]
Epoch [56/120    avg_loss:0.228, val_acc:0.922]
Epoch [57/120    avg_loss:0.166, val_acc:0.945]
Epoch [58/120    avg_loss:0.177, val_acc:0.961]
Epoch [59/120    avg_loss:0.164, val_acc:0.945]
Epoch [60/120    avg_loss:0.186, val_acc:0.943]
Epoch [61/120    avg_loss:0.147, val_acc:0.959]
Epoch [62/120    avg_loss:0.112, val_acc:0.963]
Epoch [63/120    avg_loss:0.096, val_acc:0.967]
Epoch [64/120    avg_loss:0.142, val_acc:0.941]
Epoch [65/120    avg_loss:0.245, val_acc:0.941]
Epoch [66/120    avg_loss:0.192, val_acc:0.932]
Epoch [67/120    avg_loss:0.146, val_acc:0.938]
Epoch [68/120    avg_loss:0.164, val_acc:0.963]
Epoch [69/120    avg_loss:0.109, val_acc:0.965]
Epoch [70/120    avg_loss:0.100, val_acc:0.951]
Epoch [71/120    avg_loss:0.119, val_acc:0.961]
Epoch [72/120    avg_loss:0.123, val_acc:0.963]
Epoch [73/120    avg_loss:0.089, val_acc:0.963]
Epoch [74/120    avg_loss:0.098, val_acc:0.969]
Epoch [75/120    avg_loss:0.102, val_acc:0.967]
Epoch [76/120    avg_loss:0.108, val_acc:0.961]
Epoch [77/120    avg_loss:0.087, val_acc:0.961]
Epoch [78/120    avg_loss:0.117, val_acc:0.969]
Epoch [79/120    avg_loss:0.080, val_acc:0.969]
Epoch [80/120    avg_loss:0.099, val_acc:0.973]
Epoch [81/120    avg_loss:0.148, val_acc:0.934]
Epoch [82/120    avg_loss:0.094, val_acc:0.963]
Epoch [83/120    avg_loss:0.083, val_acc:0.965]
Epoch [84/120    avg_loss:0.072, val_acc:0.928]
Epoch [85/120    avg_loss:0.082, val_acc:0.930]
Epoch [86/120    avg_loss:0.091, val_acc:0.969]
Epoch [87/120    avg_loss:0.076, val_acc:0.963]
Epoch [88/120    avg_loss:0.057, val_acc:0.971]
Epoch [89/120    avg_loss:0.043, val_acc:0.969]
Epoch [90/120    avg_loss:0.064, val_acc:0.973]
Epoch [91/120    avg_loss:0.074, val_acc:0.959]
Epoch [92/120    avg_loss:0.081, val_acc:0.979]
Epoch [93/120    avg_loss:0.044, val_acc:0.961]
Epoch [94/120    avg_loss:0.102, val_acc:0.971]
Epoch [95/120    avg_loss:0.054, val_acc:0.939]
Epoch [96/120    avg_loss:0.053, val_acc:0.955]
Epoch [97/120    avg_loss:0.047, val_acc:0.959]
Epoch [98/120    avg_loss:0.072, val_acc:0.963]
Epoch [99/120    avg_loss:0.043, val_acc:0.957]
Epoch [100/120    avg_loss:0.046, val_acc:0.967]
Epoch [101/120    avg_loss:0.037, val_acc:0.975]
Epoch [102/120    avg_loss:0.071, val_acc:0.965]
Epoch [103/120    avg_loss:0.128, val_acc:0.947]
Epoch [104/120    avg_loss:0.107, val_acc:0.945]
Epoch [105/120    avg_loss:0.087, val_acc:0.965]
Epoch [106/120    avg_loss:0.066, val_acc:0.971]
Epoch [107/120    avg_loss:0.035, val_acc:0.971]
Epoch [108/120    avg_loss:0.037, val_acc:0.975]
Epoch [109/120    avg_loss:0.031, val_acc:0.975]
Epoch [110/120    avg_loss:0.023, val_acc:0.973]
Epoch [111/120    avg_loss:0.036, val_acc:0.973]
Epoch [112/120    avg_loss:0.031, val_acc:0.975]
Epoch [113/120    avg_loss:0.025, val_acc:0.977]
Epoch [114/120    avg_loss:0.034, val_acc:0.975]
Epoch [115/120    avg_loss:0.037, val_acc:0.975]
Epoch [116/120    avg_loss:0.029, val_acc:0.973]
Epoch [117/120    avg_loss:0.030, val_acc:0.975]
Epoch [118/120    avg_loss:0.029, val_acc:0.975]
Epoch [119/120    avg_loss:0.028, val_acc:0.975]
Epoch [120/120    avg_loss:0.021, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 214  14   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 130   1   0   0   0   0   0   0   0]
 [  0  11   0   0   5   1 189   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.33688699360341

F1 scores:
[       nan 0.99203476 0.9753915  0.96396396 0.891258   0.88435374
 0.95454545 0.93854749 0.99742931 1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9814781332660008
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7cef01c780>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.458, val_acc:0.490]
Epoch [2/120    avg_loss:2.097, val_acc:0.520]
Epoch [3/120    avg_loss:1.812, val_acc:0.654]
Epoch [4/120    avg_loss:1.532, val_acc:0.730]
Epoch [5/120    avg_loss:1.296, val_acc:0.754]
Epoch [6/120    avg_loss:1.156, val_acc:0.803]
Epoch [7/120    avg_loss:1.065, val_acc:0.801]
Epoch [8/120    avg_loss:0.957, val_acc:0.840]
Epoch [9/120    avg_loss:0.872, val_acc:0.830]
Epoch [10/120    avg_loss:0.849, val_acc:0.875]
Epoch [11/120    avg_loss:0.750, val_acc:0.885]
Epoch [12/120    avg_loss:0.666, val_acc:0.859]
Epoch [13/120    avg_loss:0.741, val_acc:0.885]
Epoch [14/120    avg_loss:0.598, val_acc:0.879]
Epoch [15/120    avg_loss:0.527, val_acc:0.885]
Epoch [16/120    avg_loss:0.513, val_acc:0.898]
Epoch [17/120    avg_loss:0.548, val_acc:0.893]
Epoch [18/120    avg_loss:0.512, val_acc:0.883]
Epoch [19/120    avg_loss:0.469, val_acc:0.869]
Epoch [20/120    avg_loss:0.539, val_acc:0.895]
Epoch [21/120    avg_loss:0.488, val_acc:0.898]
Epoch [22/120    avg_loss:0.403, val_acc:0.896]
Epoch [23/120    avg_loss:0.438, val_acc:0.889]
Epoch [24/120    avg_loss:0.547, val_acc:0.926]
Epoch [25/120    avg_loss:0.475, val_acc:0.916]
Epoch [26/120    avg_loss:0.411, val_acc:0.910]
Epoch [27/120    avg_loss:0.367, val_acc:0.902]
Epoch [28/120    avg_loss:0.388, val_acc:0.908]
Epoch [29/120    avg_loss:0.343, val_acc:0.922]
Epoch [30/120    avg_loss:0.365, val_acc:0.920]
Epoch [31/120    avg_loss:0.344, val_acc:0.930]
Epoch [32/120    avg_loss:0.315, val_acc:0.914]
Epoch [33/120    avg_loss:0.313, val_acc:0.920]
Epoch [34/120    avg_loss:0.356, val_acc:0.922]
Epoch [35/120    avg_loss:0.282, val_acc:0.918]
Epoch [36/120    avg_loss:0.311, val_acc:0.932]
Epoch [37/120    avg_loss:0.288, val_acc:0.934]
Epoch [38/120    avg_loss:0.250, val_acc:0.926]
Epoch [39/120    avg_loss:0.260, val_acc:0.928]
Epoch [40/120    avg_loss:0.284, val_acc:0.904]
Epoch [41/120    avg_loss:0.318, val_acc:0.922]
Epoch [42/120    avg_loss:0.325, val_acc:0.928]
Epoch [43/120    avg_loss:0.281, val_acc:0.914]
Epoch [44/120    avg_loss:0.250, val_acc:0.941]
Epoch [45/120    avg_loss:0.268, val_acc:0.943]
Epoch [46/120    avg_loss:0.205, val_acc:0.945]
Epoch [47/120    avg_loss:0.219, val_acc:0.953]
Epoch [48/120    avg_loss:0.245, val_acc:0.930]
Epoch [49/120    avg_loss:0.226, val_acc:0.945]
Epoch [50/120    avg_loss:0.211, val_acc:0.955]
Epoch [51/120    avg_loss:0.223, val_acc:0.949]
Epoch [52/120    avg_loss:0.169, val_acc:0.938]
Epoch [53/120    avg_loss:0.199, val_acc:0.953]
Epoch [54/120    avg_loss:0.202, val_acc:0.943]
Epoch [55/120    avg_loss:0.167, val_acc:0.941]
Epoch [56/120    avg_loss:0.181, val_acc:0.949]
Epoch [57/120    avg_loss:0.192, val_acc:0.930]
Epoch [58/120    avg_loss:0.164, val_acc:0.949]
Epoch [59/120    avg_loss:0.167, val_acc:0.949]
Epoch [60/120    avg_loss:0.225, val_acc:0.939]
Epoch [61/120    avg_loss:0.196, val_acc:0.922]
Epoch [62/120    avg_loss:0.179, val_acc:0.934]
Epoch [63/120    avg_loss:0.136, val_acc:0.932]
Epoch [64/120    avg_loss:0.130, val_acc:0.965]
Epoch [65/120    avg_loss:0.082, val_acc:0.965]
Epoch [66/120    avg_loss:0.082, val_acc:0.965]
Epoch [67/120    avg_loss:0.093, val_acc:0.965]
Epoch [68/120    avg_loss:0.081, val_acc:0.965]
Epoch [69/120    avg_loss:0.098, val_acc:0.969]
Epoch [70/120    avg_loss:0.082, val_acc:0.973]
Epoch [71/120    avg_loss:0.081, val_acc:0.973]
Epoch [72/120    avg_loss:0.088, val_acc:0.967]
Epoch [73/120    avg_loss:0.090, val_acc:0.969]
Epoch [74/120    avg_loss:0.078, val_acc:0.969]
Epoch [75/120    avg_loss:0.086, val_acc:0.967]
Epoch [76/120    avg_loss:0.069, val_acc:0.971]
Epoch [77/120    avg_loss:0.069, val_acc:0.971]
Epoch [78/120    avg_loss:0.073, val_acc:0.971]
Epoch [79/120    avg_loss:0.075, val_acc:0.971]
Epoch [80/120    avg_loss:0.068, val_acc:0.971]
Epoch [81/120    avg_loss:0.072, val_acc:0.975]
Epoch [82/120    avg_loss:0.076, val_acc:0.973]
Epoch [83/120    avg_loss:0.073, val_acc:0.971]
Epoch [84/120    avg_loss:0.074, val_acc:0.969]
Epoch [85/120    avg_loss:0.063, val_acc:0.969]
Epoch [86/120    avg_loss:0.071, val_acc:0.969]
Epoch [87/120    avg_loss:0.074, val_acc:0.969]
Epoch [88/120    avg_loss:0.073, val_acc:0.971]
Epoch [89/120    avg_loss:0.077, val_acc:0.971]
Epoch [90/120    avg_loss:0.063, val_acc:0.975]
Epoch [91/120    avg_loss:0.078, val_acc:0.973]
Epoch [92/120    avg_loss:0.066, val_acc:0.975]
Epoch [93/120    avg_loss:0.068, val_acc:0.975]
Epoch [94/120    avg_loss:0.067, val_acc:0.973]
Epoch [95/120    avg_loss:0.066, val_acc:0.975]
Epoch [96/120    avg_loss:0.078, val_acc:0.975]
Epoch [97/120    avg_loss:0.062, val_acc:0.975]
Epoch [98/120    avg_loss:0.069, val_acc:0.973]
Epoch [99/120    avg_loss:0.062, val_acc:0.971]
Epoch [100/120    avg_loss:0.054, val_acc:0.973]
Epoch [101/120    avg_loss:0.079, val_acc:0.969]
Epoch [102/120    avg_loss:0.070, val_acc:0.973]
Epoch [103/120    avg_loss:0.061, val_acc:0.973]
Epoch [104/120    avg_loss:0.066, val_acc:0.969]
Epoch [105/120    avg_loss:0.067, val_acc:0.975]
Epoch [106/120    avg_loss:0.063, val_acc:0.977]
Epoch [107/120    avg_loss:0.065, val_acc:0.977]
Epoch [108/120    avg_loss:0.055, val_acc:0.977]
Epoch [109/120    avg_loss:0.065, val_acc:0.975]
Epoch [110/120    avg_loss:0.064, val_acc:0.971]
Epoch [111/120    avg_loss:0.056, val_acc:0.977]
Epoch [112/120    avg_loss:0.055, val_acc:0.975]
Epoch [113/120    avg_loss:0.064, val_acc:0.977]
Epoch [114/120    avg_loss:0.063, val_acc:0.975]
Epoch [115/120    avg_loss:0.054, val_acc:0.979]
Epoch [116/120    avg_loss:0.065, val_acc:0.979]
Epoch [117/120    avg_loss:0.048, val_acc:0.979]
Epoch [118/120    avg_loss:0.063, val_acc:0.979]
Epoch [119/120    avg_loss:0.046, val_acc:0.977]
Epoch [120/120    avg_loss:0.053, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   1 208   0   0   0   0   9   0   0   0   0   1   0]
 [  0   0   0 221   7   0   0   0   1   1   0   0   0   0]
 [  0   0   0   2 208  16   0   0   0   0   0   0   1   0]
 [  0   0   0   2  19 124   0   0   0   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.40085287846482

F1 scores:
[       nan 0.99708879 0.9476082  0.97142857 0.90238612 0.87017544
 0.99266504 0.88648649 0.998713   0.99893276 1.         1.
 0.99779736 1.        ]

Kappa:
0.9821932048662289
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8ee891e908>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.436, val_acc:0.330]
Epoch [2/120    avg_loss:2.096, val_acc:0.584]
Epoch [3/120    avg_loss:1.878, val_acc:0.637]
Epoch [4/120    avg_loss:1.624, val_acc:0.738]
Epoch [5/120    avg_loss:1.448, val_acc:0.760]
Epoch [6/120    avg_loss:1.208, val_acc:0.811]
Epoch [7/120    avg_loss:1.059, val_acc:0.828]
Epoch [8/120    avg_loss:0.933, val_acc:0.779]
Epoch [9/120    avg_loss:0.812, val_acc:0.818]
Epoch [10/120    avg_loss:0.861, val_acc:0.834]
Epoch [11/120    avg_loss:0.867, val_acc:0.828]
Epoch [12/120    avg_loss:0.695, val_acc:0.904]
Epoch [13/120    avg_loss:0.558, val_acc:0.908]
Epoch [14/120    avg_loss:0.560, val_acc:0.908]
Epoch [15/120    avg_loss:0.539, val_acc:0.895]
Epoch [16/120    avg_loss:0.466, val_acc:0.908]
Epoch [17/120    avg_loss:0.451, val_acc:0.916]
Epoch [18/120    avg_loss:0.455, val_acc:0.852]
Epoch [19/120    avg_loss:0.481, val_acc:0.928]
Epoch [20/120    avg_loss:0.401, val_acc:0.918]
Epoch [21/120    avg_loss:0.390, val_acc:0.926]
Epoch [22/120    avg_loss:0.341, val_acc:0.930]
Epoch [23/120    avg_loss:0.403, val_acc:0.936]
Epoch [24/120    avg_loss:0.324, val_acc:0.941]
Epoch [25/120    avg_loss:0.316, val_acc:0.951]
Epoch [26/120    avg_loss:0.308, val_acc:0.963]
Epoch [27/120    avg_loss:0.349, val_acc:0.930]
Epoch [28/120    avg_loss:0.317, val_acc:0.939]
Epoch [29/120    avg_loss:0.274, val_acc:0.953]
Epoch [30/120    avg_loss:0.290, val_acc:0.943]
Epoch [31/120    avg_loss:0.251, val_acc:0.949]
Epoch [32/120    avg_loss:0.248, val_acc:0.941]
Epoch [33/120    avg_loss:0.271, val_acc:0.955]
Epoch [34/120    avg_loss:0.258, val_acc:0.949]
Epoch [35/120    avg_loss:0.245, val_acc:0.951]
Epoch [36/120    avg_loss:0.263, val_acc:0.969]
Epoch [37/120    avg_loss:0.246, val_acc:0.936]
Epoch [38/120    avg_loss:0.219, val_acc:0.959]
Epoch [39/120    avg_loss:0.202, val_acc:0.936]
Epoch [40/120    avg_loss:0.238, val_acc:0.955]
Epoch [41/120    avg_loss:0.187, val_acc:0.963]
Epoch [42/120    avg_loss:0.271, val_acc:0.973]
Epoch [43/120    avg_loss:0.231, val_acc:0.945]
Epoch [44/120    avg_loss:0.232, val_acc:0.941]
Epoch [45/120    avg_loss:0.139, val_acc:0.955]
Epoch [46/120    avg_loss:0.165, val_acc:0.943]
Epoch [47/120    avg_loss:0.170, val_acc:0.973]
Epoch [48/120    avg_loss:0.157, val_acc:0.910]
Epoch [49/120    avg_loss:0.187, val_acc:0.955]
Epoch [50/120    avg_loss:0.178, val_acc:0.955]
Epoch [51/120    avg_loss:0.134, val_acc:0.979]
Epoch [52/120    avg_loss:0.138, val_acc:0.969]
Epoch [53/120    avg_loss:0.156, val_acc:0.971]
Epoch [54/120    avg_loss:0.148, val_acc:0.953]
Epoch [55/120    avg_loss:0.174, val_acc:0.984]
Epoch [56/120    avg_loss:0.178, val_acc:0.955]
Epoch [57/120    avg_loss:0.149, val_acc:0.977]
Epoch [58/120    avg_loss:0.108, val_acc:0.971]
Epoch [59/120    avg_loss:0.185, val_acc:0.957]
Epoch [60/120    avg_loss:0.128, val_acc:0.949]
Epoch [61/120    avg_loss:0.128, val_acc:0.967]
Epoch [62/120    avg_loss:0.116, val_acc:0.973]
Epoch [63/120    avg_loss:0.108, val_acc:0.969]
Epoch [64/120    avg_loss:0.100, val_acc:0.979]
Epoch [65/120    avg_loss:0.083, val_acc:0.975]
Epoch [66/120    avg_loss:0.095, val_acc:0.975]
Epoch [67/120    avg_loss:0.077, val_acc:0.963]
Epoch [68/120    avg_loss:0.081, val_acc:0.969]
Epoch [69/120    avg_loss:0.055, val_acc:0.979]
Epoch [70/120    avg_loss:0.063, val_acc:0.979]
Epoch [71/120    avg_loss:0.049, val_acc:0.979]
Epoch [72/120    avg_loss:0.047, val_acc:0.980]
Epoch [73/120    avg_loss:0.057, val_acc:0.984]
Epoch [74/120    avg_loss:0.054, val_acc:0.984]
Epoch [75/120    avg_loss:0.044, val_acc:0.984]
Epoch [76/120    avg_loss:0.047, val_acc:0.986]
Epoch [77/120    avg_loss:0.047, val_acc:0.984]
Epoch [78/120    avg_loss:0.034, val_acc:0.982]
Epoch [79/120    avg_loss:0.051, val_acc:0.986]
Epoch [80/120    avg_loss:0.054, val_acc:0.986]
Epoch [81/120    avg_loss:0.039, val_acc:0.984]
Epoch [82/120    avg_loss:0.040, val_acc:0.984]
Epoch [83/120    avg_loss:0.045, val_acc:0.986]
Epoch [84/120    avg_loss:0.044, val_acc:0.986]
Epoch [85/120    avg_loss:0.054, val_acc:0.986]
Epoch [86/120    avg_loss:0.042, val_acc:0.980]
Epoch [87/120    avg_loss:0.038, val_acc:0.986]
Epoch [88/120    avg_loss:0.053, val_acc:0.984]
Epoch [89/120    avg_loss:0.051, val_acc:0.986]
Epoch [90/120    avg_loss:0.043, val_acc:0.986]
Epoch [91/120    avg_loss:0.049, val_acc:0.988]
Epoch [92/120    avg_loss:0.035, val_acc:0.986]
Epoch [93/120    avg_loss:0.055, val_acc:0.988]
Epoch [94/120    avg_loss:0.040, val_acc:0.986]
Epoch [95/120    avg_loss:0.050, val_acc:0.990]
Epoch [96/120    avg_loss:0.032, val_acc:0.988]
Epoch [97/120    avg_loss:0.037, val_acc:0.984]
Epoch [98/120    avg_loss:0.035, val_acc:0.986]
Epoch [99/120    avg_loss:0.039, val_acc:0.986]
Epoch [100/120    avg_loss:0.031, val_acc:0.986]
Epoch [101/120    avg_loss:0.041, val_acc:0.984]
Epoch [102/120    avg_loss:0.037, val_acc:0.986]
Epoch [103/120    avg_loss:0.042, val_acc:0.986]
Epoch [104/120    avg_loss:0.043, val_acc:0.988]
Epoch [105/120    avg_loss:0.046, val_acc:0.988]
Epoch [106/120    avg_loss:0.039, val_acc:0.988]
Epoch [107/120    avg_loss:0.041, val_acc:0.988]
Epoch [108/120    avg_loss:0.038, val_acc:0.988]
Epoch [109/120    avg_loss:0.036, val_acc:0.988]
Epoch [110/120    avg_loss:0.047, val_acc:0.988]
Epoch [111/120    avg_loss:0.051, val_acc:0.988]
Epoch [112/120    avg_loss:0.040, val_acc:0.988]
Epoch [113/120    avg_loss:0.041, val_acc:0.988]
Epoch [114/120    avg_loss:0.043, val_acc:0.988]
Epoch [115/120    avg_loss:0.036, val_acc:0.990]
Epoch [116/120    avg_loss:0.037, val_acc:0.990]
Epoch [117/120    avg_loss:0.043, val_acc:0.990]
Epoch [118/120    avg_loss:0.049, val_acc:0.990]
Epoch [119/120    avg_loss:0.039, val_acc:0.988]
Epoch [120/120    avg_loss:0.043, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0  22   0   0   0   0  72   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0   0 832]]

Accuracy:
98.59275053304904

F1 scores:
[       nan 1.         0.93534483 0.98678414 0.92241379 0.90657439
 0.99266504 0.85714286 1.         1.         1.         1.
 0.99556541 0.99879952]

Kappa:
0.984332458409571
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa26f4dd908>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.468, val_acc:0.475]
Epoch [2/120    avg_loss:2.116, val_acc:0.576]
Epoch [3/120    avg_loss:1.851, val_acc:0.674]
Epoch [4/120    avg_loss:1.616, val_acc:0.721]
Epoch [5/120    avg_loss:1.418, val_acc:0.738]
Epoch [6/120    avg_loss:1.238, val_acc:0.789]
Epoch [7/120    avg_loss:1.079, val_acc:0.795]
Epoch [8/120    avg_loss:0.968, val_acc:0.828]
Epoch [9/120    avg_loss:0.912, val_acc:0.867]
Epoch [10/120    avg_loss:0.824, val_acc:0.836]
Epoch [11/120    avg_loss:0.752, val_acc:0.873]
Epoch [12/120    avg_loss:0.669, val_acc:0.881]
Epoch [13/120    avg_loss:0.633, val_acc:0.822]
Epoch [14/120    avg_loss:0.564, val_acc:0.889]
Epoch [15/120    avg_loss:0.550, val_acc:0.914]
Epoch [16/120    avg_loss:0.525, val_acc:0.887]
Epoch [17/120    avg_loss:0.512, val_acc:0.908]
Epoch [18/120    avg_loss:0.539, val_acc:0.910]
Epoch [19/120    avg_loss:0.509, val_acc:0.811]
Epoch [20/120    avg_loss:0.517, val_acc:0.871]
Epoch [21/120    avg_loss:0.462, val_acc:0.910]
Epoch [22/120    avg_loss:0.405, val_acc:0.908]
Epoch [23/120    avg_loss:0.375, val_acc:0.914]
Epoch [24/120    avg_loss:0.407, val_acc:0.922]
Epoch [25/120    avg_loss:0.395, val_acc:0.912]
Epoch [26/120    avg_loss:0.382, val_acc:0.898]
Epoch [27/120    avg_loss:0.460, val_acc:0.910]
Epoch [28/120    avg_loss:0.338, val_acc:0.920]
Epoch [29/120    avg_loss:0.330, val_acc:0.918]
Epoch [30/120    avg_loss:0.366, val_acc:0.912]
Epoch [31/120    avg_loss:0.353, val_acc:0.928]
Epoch [32/120    avg_loss:0.313, val_acc:0.893]
Epoch [33/120    avg_loss:0.316, val_acc:0.900]
Epoch [34/120    avg_loss:0.286, val_acc:0.924]
Epoch [35/120    avg_loss:0.361, val_acc:0.928]
Epoch [36/120    avg_loss:0.255, val_acc:0.936]
Epoch [37/120    avg_loss:0.273, val_acc:0.902]
Epoch [38/120    avg_loss:0.231, val_acc:0.941]
Epoch [39/120    avg_loss:0.262, val_acc:0.908]
Epoch [40/120    avg_loss:0.243, val_acc:0.936]
Epoch [41/120    avg_loss:0.199, val_acc:0.918]
Epoch [42/120    avg_loss:0.305, val_acc:0.938]
Epoch [43/120    avg_loss:0.234, val_acc:0.949]
Epoch [44/120    avg_loss:0.218, val_acc:0.930]
Epoch [45/120    avg_loss:0.178, val_acc:0.941]
Epoch [46/120    avg_loss:0.190, val_acc:0.949]
Epoch [47/120    avg_loss:0.175, val_acc:0.936]
Epoch [48/120    avg_loss:0.186, val_acc:0.936]
Epoch [49/120    avg_loss:0.176, val_acc:0.932]
Epoch [50/120    avg_loss:0.204, val_acc:0.955]
Epoch [51/120    avg_loss:0.202, val_acc:0.934]
Epoch [52/120    avg_loss:0.243, val_acc:0.924]
Epoch [53/120    avg_loss:0.170, val_acc:0.961]
Epoch [54/120    avg_loss:0.197, val_acc:0.926]
Epoch [55/120    avg_loss:0.239, val_acc:0.936]
Epoch [56/120    avg_loss:0.169, val_acc:0.955]
Epoch [57/120    avg_loss:0.198, val_acc:0.928]
Epoch [58/120    avg_loss:0.170, val_acc:0.941]
Epoch [59/120    avg_loss:0.165, val_acc:0.959]
Epoch [60/120    avg_loss:0.137, val_acc:0.934]
Epoch [61/120    avg_loss:0.171, val_acc:0.967]
Epoch [62/120    avg_loss:0.095, val_acc:0.959]
Epoch [63/120    avg_loss:0.130, val_acc:0.941]
Epoch [64/120    avg_loss:0.178, val_acc:0.939]
Epoch [65/120    avg_loss:0.177, val_acc:0.951]
Epoch [66/120    avg_loss:0.213, val_acc:0.949]
Epoch [67/120    avg_loss:0.188, val_acc:0.957]
Epoch [68/120    avg_loss:0.153, val_acc:0.961]
Epoch [69/120    avg_loss:0.127, val_acc:0.941]
Epoch [70/120    avg_loss:0.173, val_acc:0.965]
Epoch [71/120    avg_loss:0.159, val_acc:0.949]
Epoch [72/120    avg_loss:0.180, val_acc:0.959]
Epoch [73/120    avg_loss:0.161, val_acc:0.947]
Epoch [74/120    avg_loss:0.165, val_acc:0.945]
Epoch [75/120    avg_loss:0.108, val_acc:0.967]
Epoch [76/120    avg_loss:0.083, val_acc:0.967]
Epoch [77/120    avg_loss:0.082, val_acc:0.971]
Epoch [78/120    avg_loss:0.073, val_acc:0.969]
Epoch [79/120    avg_loss:0.075, val_acc:0.963]
Epoch [80/120    avg_loss:0.084, val_acc:0.973]
Epoch [81/120    avg_loss:0.073, val_acc:0.969]
Epoch [82/120    avg_loss:0.078, val_acc:0.973]
Epoch [83/120    avg_loss:0.067, val_acc:0.973]
Epoch [84/120    avg_loss:0.064, val_acc:0.971]
Epoch [85/120    avg_loss:0.068, val_acc:0.971]
Epoch [86/120    avg_loss:0.071, val_acc:0.971]
Epoch [87/120    avg_loss:0.062, val_acc:0.971]
Epoch [88/120    avg_loss:0.060, val_acc:0.969]
Epoch [89/120    avg_loss:0.054, val_acc:0.969]
Epoch [90/120    avg_loss:0.065, val_acc:0.973]
Epoch [91/120    avg_loss:0.071, val_acc:0.971]
Epoch [92/120    avg_loss:0.058, val_acc:0.971]
Epoch [93/120    avg_loss:0.061, val_acc:0.971]
Epoch [94/120    avg_loss:0.056, val_acc:0.969]
Epoch [95/120    avg_loss:0.059, val_acc:0.969]
Epoch [96/120    avg_loss:0.052, val_acc:0.969]
Epoch [97/120    avg_loss:0.068, val_acc:0.969]
Epoch [98/120    avg_loss:0.069, val_acc:0.971]
Epoch [99/120    avg_loss:0.067, val_acc:0.969]
Epoch [100/120    avg_loss:0.056, val_acc:0.969]
Epoch [101/120    avg_loss:0.064, val_acc:0.971]
Epoch [102/120    avg_loss:0.055, val_acc:0.971]
Epoch [103/120    avg_loss:0.055, val_acc:0.973]
Epoch [104/120    avg_loss:0.051, val_acc:0.969]
Epoch [105/120    avg_loss:0.053, val_acc:0.975]
Epoch [106/120    avg_loss:0.050, val_acc:0.973]
Epoch [107/120    avg_loss:0.064, val_acc:0.971]
Epoch [108/120    avg_loss:0.056, val_acc:0.973]
Epoch [109/120    avg_loss:0.062, val_acc:0.973]
Epoch [110/120    avg_loss:0.060, val_acc:0.975]
Epoch [111/120    avg_loss:0.050, val_acc:0.973]
Epoch [112/120    avg_loss:0.052, val_acc:0.975]
Epoch [113/120    avg_loss:0.056, val_acc:0.975]
Epoch [114/120    avg_loss:0.056, val_acc:0.971]
Epoch [115/120    avg_loss:0.052, val_acc:0.975]
Epoch [116/120    avg_loss:0.062, val_acc:0.975]
Epoch [117/120    avg_loss:0.068, val_acc:0.969]
Epoch [118/120    avg_loss:0.043, val_acc:0.975]
Epoch [119/120    avg_loss:0.056, val_acc:0.975]
Epoch [120/120    avg_loss:0.053, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   5 210  14   0   0   0   0   1   0   0   0   0]
 [  0   0   1   2 207  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 135   1   0   0   0   0   0   0   0]
 [  0   9   0   0   6   0 191   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   1   0   0   0   0   0   4   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.01705756929637

F1 scores:
[       nan 0.99347353 0.93832599 0.95022624 0.89416847 0.90909091
 0.95979899 0.88268156 0.99487179 0.99893276 1.         0.99734043
 0.99224806 1.        ]

Kappa:
0.9779182001882701
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f671d8604a8>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.438, val_acc:0.422]
Epoch [2/120    avg_loss:2.134, val_acc:0.561]
Epoch [3/120    avg_loss:1.894, val_acc:0.611]
Epoch [4/120    avg_loss:1.614, val_acc:0.650]
Epoch [5/120    avg_loss:1.386, val_acc:0.721]
Epoch [6/120    avg_loss:1.206, val_acc:0.738]
Epoch [7/120    avg_loss:1.080, val_acc:0.744]
Epoch [8/120    avg_loss:0.941, val_acc:0.822]
Epoch [9/120    avg_loss:0.883, val_acc:0.785]
Epoch [10/120    avg_loss:0.811, val_acc:0.787]
Epoch [11/120    avg_loss:0.783, val_acc:0.850]
Epoch [12/120    avg_loss:0.736, val_acc:0.830]
Epoch [13/120    avg_loss:0.721, val_acc:0.832]
Epoch [14/120    avg_loss:0.763, val_acc:0.883]
Epoch [15/120    avg_loss:0.576, val_acc:0.895]
Epoch [16/120    avg_loss:0.533, val_acc:0.896]
Epoch [17/120    avg_loss:0.565, val_acc:0.869]
Epoch [18/120    avg_loss:0.513, val_acc:0.850]
Epoch [19/120    avg_loss:0.483, val_acc:0.881]
Epoch [20/120    avg_loss:0.464, val_acc:0.908]
Epoch [21/120    avg_loss:0.481, val_acc:0.912]
Epoch [22/120    avg_loss:0.362, val_acc:0.920]
Epoch [23/120    avg_loss:0.388, val_acc:0.930]
Epoch [24/120    avg_loss:0.435, val_acc:0.914]
Epoch [25/120    avg_loss:0.473, val_acc:0.932]
Epoch [26/120    avg_loss:0.401, val_acc:0.932]
Epoch [27/120    avg_loss:0.351, val_acc:0.922]
Epoch [28/120    avg_loss:0.371, val_acc:0.918]
Epoch [29/120    avg_loss:0.325, val_acc:0.916]
Epoch [30/120    avg_loss:0.361, val_acc:0.900]
Epoch [31/120    avg_loss:0.333, val_acc:0.828]
Epoch [32/120    avg_loss:0.399, val_acc:0.920]
Epoch [33/120    avg_loss:0.303, val_acc:0.941]
Epoch [34/120    avg_loss:0.252, val_acc:0.922]
Epoch [35/120    avg_loss:0.328, val_acc:0.941]
Epoch [36/120    avg_loss:0.272, val_acc:0.938]
Epoch [37/120    avg_loss:0.210, val_acc:0.922]
Epoch [38/120    avg_loss:0.292, val_acc:0.936]
Epoch [39/120    avg_loss:0.225, val_acc:0.934]
Epoch [40/120    avg_loss:0.373, val_acc:0.941]
Epoch [41/120    avg_loss:0.298, val_acc:0.928]
Epoch [42/120    avg_loss:0.208, val_acc:0.928]
Epoch [43/120    avg_loss:0.208, val_acc:0.961]
Epoch [44/120    avg_loss:0.198, val_acc:0.945]
Epoch [45/120    avg_loss:0.202, val_acc:0.951]
Epoch [46/120    avg_loss:0.210, val_acc:0.885]
Epoch [47/120    avg_loss:0.181, val_acc:0.955]
Epoch [48/120    avg_loss:0.165, val_acc:0.959]
Epoch [49/120    avg_loss:0.187, val_acc:0.961]
Epoch [50/120    avg_loss:0.233, val_acc:0.951]
Epoch [51/120    avg_loss:0.197, val_acc:0.955]
Epoch [52/120    avg_loss:0.175, val_acc:0.904]
Epoch [53/120    avg_loss:0.231, val_acc:0.926]
Epoch [54/120    avg_loss:0.166, val_acc:0.959]
Epoch [55/120    avg_loss:0.185, val_acc:0.932]
Epoch [56/120    avg_loss:0.233, val_acc:0.951]
Epoch [57/120    avg_loss:0.254, val_acc:0.938]
Epoch [58/120    avg_loss:0.150, val_acc:0.959]
Epoch [59/120    avg_loss:0.151, val_acc:0.959]
Epoch [60/120    avg_loss:0.139, val_acc:0.965]
Epoch [61/120    avg_loss:0.135, val_acc:0.941]
Epoch [62/120    avg_loss:0.121, val_acc:0.965]
Epoch [63/120    avg_loss:0.112, val_acc:0.943]
Epoch [64/120    avg_loss:0.148, val_acc:0.945]
Epoch [65/120    avg_loss:0.147, val_acc:0.967]
Epoch [66/120    avg_loss:0.136, val_acc:0.969]
Epoch [67/120    avg_loss:0.105, val_acc:0.975]
Epoch [68/120    avg_loss:0.085, val_acc:0.979]
Epoch [69/120    avg_loss:0.092, val_acc:0.977]
Epoch [70/120    avg_loss:0.128, val_acc:0.959]
Epoch [71/120    avg_loss:0.099, val_acc:0.973]
Epoch [72/120    avg_loss:0.085, val_acc:0.957]
Epoch [73/120    avg_loss:0.115, val_acc:0.977]
Epoch [74/120    avg_loss:0.103, val_acc:0.967]
Epoch [75/120    avg_loss:0.142, val_acc:0.957]
Epoch [76/120    avg_loss:0.130, val_acc:0.965]
Epoch [77/120    avg_loss:0.134, val_acc:0.979]
Epoch [78/120    avg_loss:0.078, val_acc:0.963]
Epoch [79/120    avg_loss:0.058, val_acc:0.977]
Epoch [80/120    avg_loss:0.080, val_acc:0.977]
Epoch [81/120    avg_loss:0.113, val_acc:0.945]
Epoch [82/120    avg_loss:0.098, val_acc:0.967]
Epoch [83/120    avg_loss:0.085, val_acc:0.980]
Epoch [84/120    avg_loss:0.109, val_acc:0.975]
Epoch [85/120    avg_loss:0.081, val_acc:0.951]
Epoch [86/120    avg_loss:0.115, val_acc:0.943]
Epoch [87/120    avg_loss:0.115, val_acc:0.982]
Epoch [88/120    avg_loss:0.063, val_acc:0.980]
Epoch [89/120    avg_loss:0.059, val_acc:0.938]
Epoch [90/120    avg_loss:0.087, val_acc:0.973]
Epoch [91/120    avg_loss:0.098, val_acc:0.973]
Epoch [92/120    avg_loss:0.085, val_acc:0.957]
Epoch [93/120    avg_loss:0.082, val_acc:0.984]
Epoch [94/120    avg_loss:0.066, val_acc:0.971]
Epoch [95/120    avg_loss:0.069, val_acc:0.977]
Epoch [96/120    avg_loss:0.062, val_acc:0.977]
Epoch [97/120    avg_loss:0.050, val_acc:0.975]
Epoch [98/120    avg_loss:0.055, val_acc:0.979]
Epoch [99/120    avg_loss:0.045, val_acc:0.980]
Epoch [100/120    avg_loss:0.054, val_acc:0.977]
Epoch [101/120    avg_loss:0.077, val_acc:0.977]
Epoch [102/120    avg_loss:0.095, val_acc:0.955]
Epoch [103/120    avg_loss:0.077, val_acc:0.977]
Epoch [104/120    avg_loss:0.038, val_acc:0.984]
Epoch [105/120    avg_loss:0.052, val_acc:0.977]
Epoch [106/120    avg_loss:0.070, val_acc:0.967]
Epoch [107/120    avg_loss:0.054, val_acc:0.971]
Epoch [108/120    avg_loss:0.069, val_acc:0.980]
Epoch [109/120    avg_loss:0.038, val_acc:0.980]
Epoch [110/120    avg_loss:0.028, val_acc:0.980]
Epoch [111/120    avg_loss:0.086, val_acc:0.980]
Epoch [112/120    avg_loss:0.149, val_acc:0.936]
Epoch [113/120    avg_loss:0.159, val_acc:0.971]
Epoch [114/120    avg_loss:0.095, val_acc:0.967]
Epoch [115/120    avg_loss:0.087, val_acc:0.957]
Epoch [116/120    avg_loss:0.060, val_acc:0.967]
Epoch [117/120    avg_loss:0.050, val_acc:0.959]
Epoch [118/120    avg_loss:0.041, val_acc:0.984]
Epoch [119/120    avg_loss:0.027, val_acc:0.986]
Epoch [120/120    avg_loss:0.026, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 213  15   0   0   0   1   1   0   0   0   0]
 [  0   0   0   3 209  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   4   0   0   0   0 202   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 0.99708879 0.97767857 0.95515695 0.91868132 0.93687708
 0.99019608 0.94382022 0.998713   0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9874171117358655
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbaa0ec48d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.521, val_acc:0.344]
Epoch [2/120    avg_loss:2.199, val_acc:0.678]
Epoch [3/120    avg_loss:1.981, val_acc:0.695]
Epoch [4/120    avg_loss:1.755, val_acc:0.695]
Epoch [5/120    avg_loss:1.537, val_acc:0.729]
Epoch [6/120    avg_loss:1.319, val_acc:0.754]
Epoch [7/120    avg_loss:1.159, val_acc:0.803]
Epoch [8/120    avg_loss:1.047, val_acc:0.789]
Epoch [9/120    avg_loss:0.922, val_acc:0.742]
Epoch [10/120    avg_loss:0.865, val_acc:0.828]
Epoch [11/120    avg_loss:0.766, val_acc:0.820]
Epoch [12/120    avg_loss:0.680, val_acc:0.838]
Epoch [13/120    avg_loss:0.683, val_acc:0.771]
Epoch [14/120    avg_loss:0.660, val_acc:0.834]
Epoch [15/120    avg_loss:0.574, val_acc:0.881]
Epoch [16/120    avg_loss:0.578, val_acc:0.893]
Epoch [17/120    avg_loss:0.676, val_acc:0.883]
Epoch [18/120    avg_loss:0.541, val_acc:0.900]
Epoch [19/120    avg_loss:0.509, val_acc:0.898]
Epoch [20/120    avg_loss:0.471, val_acc:0.900]
Epoch [21/120    avg_loss:0.483, val_acc:0.867]
Epoch [22/120    avg_loss:0.462, val_acc:0.916]
Epoch [23/120    avg_loss:0.394, val_acc:0.908]
Epoch [24/120    avg_loss:0.455, val_acc:0.891]
Epoch [25/120    avg_loss:0.469, val_acc:0.912]
Epoch [26/120    avg_loss:0.378, val_acc:0.918]
Epoch [27/120    avg_loss:0.362, val_acc:0.912]
Epoch [28/120    avg_loss:0.423, val_acc:0.920]
Epoch [29/120    avg_loss:0.344, val_acc:0.914]
Epoch [30/120    avg_loss:0.316, val_acc:0.936]
Epoch [31/120    avg_loss:0.355, val_acc:0.922]
Epoch [32/120    avg_loss:0.320, val_acc:0.934]
Epoch [33/120    avg_loss:0.292, val_acc:0.924]
Epoch [34/120    avg_loss:0.337, val_acc:0.926]
Epoch [35/120    avg_loss:0.333, val_acc:0.920]
Epoch [36/120    avg_loss:0.305, val_acc:0.951]
Epoch [37/120    avg_loss:0.271, val_acc:0.947]
Epoch [38/120    avg_loss:0.330, val_acc:0.951]
Epoch [39/120    avg_loss:0.232, val_acc:0.945]
Epoch [40/120    avg_loss:0.220, val_acc:0.943]
Epoch [41/120    avg_loss:0.312, val_acc:0.930]
Epoch [42/120    avg_loss:0.310, val_acc:0.943]
Epoch [43/120    avg_loss:0.211, val_acc:0.961]
Epoch [44/120    avg_loss:0.230, val_acc:0.955]
Epoch [45/120    avg_loss:0.245, val_acc:0.932]
Epoch [46/120    avg_loss:0.235, val_acc:0.955]
Epoch [47/120    avg_loss:0.200, val_acc:0.953]
Epoch [48/120    avg_loss:0.197, val_acc:0.951]
Epoch [49/120    avg_loss:0.197, val_acc:0.963]
Epoch [50/120    avg_loss:0.215, val_acc:0.955]
Epoch [51/120    avg_loss:0.216, val_acc:0.936]
Epoch [52/120    avg_loss:0.173, val_acc:0.943]
Epoch [53/120    avg_loss:0.169, val_acc:0.945]
Epoch [54/120    avg_loss:0.201, val_acc:0.949]
Epoch [55/120    avg_loss:0.193, val_acc:0.957]
Epoch [56/120    avg_loss:0.162, val_acc:0.963]
Epoch [57/120    avg_loss:0.140, val_acc:0.965]
Epoch [58/120    avg_loss:0.131, val_acc:0.951]
Epoch [59/120    avg_loss:0.165, val_acc:0.959]
Epoch [60/120    avg_loss:0.179, val_acc:0.941]
Epoch [61/120    avg_loss:0.157, val_acc:0.953]
Epoch [62/120    avg_loss:0.155, val_acc:0.914]
Epoch [63/120    avg_loss:0.149, val_acc:0.977]
Epoch [64/120    avg_loss:0.106, val_acc:0.965]
Epoch [65/120    avg_loss:0.112, val_acc:0.941]
Epoch [66/120    avg_loss:0.152, val_acc:0.959]
Epoch [67/120    avg_loss:0.112, val_acc:0.975]
Epoch [68/120    avg_loss:0.108, val_acc:0.969]
Epoch [69/120    avg_loss:0.083, val_acc:0.949]
Epoch [70/120    avg_loss:0.081, val_acc:0.973]
Epoch [71/120    avg_loss:0.079, val_acc:0.939]
Epoch [72/120    avg_loss:0.176, val_acc:0.945]
Epoch [73/120    avg_loss:0.100, val_acc:0.959]
Epoch [74/120    avg_loss:0.089, val_acc:0.967]
Epoch [75/120    avg_loss:0.087, val_acc:0.969]
Epoch [76/120    avg_loss:0.089, val_acc:0.967]
Epoch [77/120    avg_loss:0.087, val_acc:0.971]
Epoch [78/120    avg_loss:0.072, val_acc:0.979]
Epoch [79/120    avg_loss:0.070, val_acc:0.975]
Epoch [80/120    avg_loss:0.048, val_acc:0.979]
Epoch [81/120    avg_loss:0.045, val_acc:0.975]
Epoch [82/120    avg_loss:0.046, val_acc:0.979]
Epoch [83/120    avg_loss:0.045, val_acc:0.977]
Epoch [84/120    avg_loss:0.053, val_acc:0.975]
Epoch [85/120    avg_loss:0.051, val_acc:0.975]
Epoch [86/120    avg_loss:0.043, val_acc:0.979]
Epoch [87/120    avg_loss:0.046, val_acc:0.977]
Epoch [88/120    avg_loss:0.054, val_acc:0.975]
Epoch [89/120    avg_loss:0.044, val_acc:0.980]
Epoch [90/120    avg_loss:0.047, val_acc:0.979]
Epoch [91/120    avg_loss:0.040, val_acc:0.980]
Epoch [92/120    avg_loss:0.041, val_acc:0.982]
Epoch [93/120    avg_loss:0.047, val_acc:0.979]
Epoch [94/120    avg_loss:0.043, val_acc:0.980]
Epoch [95/120    avg_loss:0.046, val_acc:0.982]
Epoch [96/120    avg_loss:0.038, val_acc:0.979]
Epoch [97/120    avg_loss:0.051, val_acc:0.977]
Epoch [98/120    avg_loss:0.041, val_acc:0.980]
Epoch [99/120    avg_loss:0.039, val_acc:0.979]
Epoch [100/120    avg_loss:0.034, val_acc:0.980]
Epoch [101/120    avg_loss:0.045, val_acc:0.980]
Epoch [102/120    avg_loss:0.035, val_acc:0.982]
Epoch [103/120    avg_loss:0.045, val_acc:0.980]
Epoch [104/120    avg_loss:0.055, val_acc:0.979]
Epoch [105/120    avg_loss:0.040, val_acc:0.977]
Epoch [106/120    avg_loss:0.038, val_acc:0.979]
Epoch [107/120    avg_loss:0.033, val_acc:0.977]
Epoch [108/120    avg_loss:0.034, val_acc:0.980]
Epoch [109/120    avg_loss:0.037, val_acc:0.984]
Epoch [110/120    avg_loss:0.038, val_acc:0.979]
Epoch [111/120    avg_loss:0.036, val_acc:0.982]
Epoch [112/120    avg_loss:0.044, val_acc:0.980]
Epoch [113/120    avg_loss:0.031, val_acc:0.979]
Epoch [114/120    avg_loss:0.039, val_acc:0.980]
Epoch [115/120    avg_loss:0.037, val_acc:0.975]
Epoch [116/120    avg_loss:0.035, val_acc:0.980]
Epoch [117/120    avg_loss:0.038, val_acc:0.980]
Epoch [118/120    avg_loss:0.037, val_acc:0.980]
Epoch [119/120    avg_loss:0.035, val_acc:0.973]
Epoch [120/120    avg_loss:0.034, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   0   0   1   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 217  12   0   0   0   0   1   0   0   0   0]
 [  0   0   0   1 215  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   5   0   0   0   0 201   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   5   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.52878464818762

F1 scores:
[       nan 0.99563319 0.95768374 0.96875    0.92077088 0.91666667
 0.98771499 0.89265537 0.998713   0.99893276 0.99317872 0.99867198
 0.99334812 1.        ]

Kappa:
0.9836177998949769
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8ca10228d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.434, val_acc:0.404]
Epoch [2/120    avg_loss:2.110, val_acc:0.514]
Epoch [3/120    avg_loss:1.836, val_acc:0.723]
Epoch [4/120    avg_loss:1.549, val_acc:0.740]
Epoch [5/120    avg_loss:1.356, val_acc:0.797]
Epoch [6/120    avg_loss:1.144, val_acc:0.854]
Epoch [7/120    avg_loss:0.998, val_acc:0.854]
Epoch [8/120    avg_loss:0.886, val_acc:0.842]
Epoch [9/120    avg_loss:0.799, val_acc:0.855]
Epoch [10/120    avg_loss:0.710, val_acc:0.865]
Epoch [11/120    avg_loss:0.652, val_acc:0.867]
Epoch [12/120    avg_loss:0.605, val_acc:0.879]
Epoch [13/120    avg_loss:0.607, val_acc:0.875]
Epoch [14/120    avg_loss:0.552, val_acc:0.789]
Epoch [15/120    avg_loss:0.588, val_acc:0.912]
Epoch [16/120    avg_loss:0.513, val_acc:0.889]
Epoch [17/120    avg_loss:0.465, val_acc:0.826]
Epoch [18/120    avg_loss:0.492, val_acc:0.900]
Epoch [19/120    avg_loss:0.467, val_acc:0.881]
Epoch [20/120    avg_loss:0.427, val_acc:0.891]
Epoch [21/120    avg_loss:0.402, val_acc:0.891]
Epoch [22/120    avg_loss:0.440, val_acc:0.893]
Epoch [23/120    avg_loss:0.374, val_acc:0.908]
Epoch [24/120    avg_loss:0.421, val_acc:0.922]
Epoch [25/120    avg_loss:0.366, val_acc:0.900]
Epoch [26/120    avg_loss:0.378, val_acc:0.910]
Epoch [27/120    avg_loss:0.365, val_acc:0.889]
Epoch [28/120    avg_loss:0.334, val_acc:0.893]
Epoch [29/120    avg_loss:0.361, val_acc:0.920]
Epoch [30/120    avg_loss:0.315, val_acc:0.928]
Epoch [31/120    avg_loss:0.343, val_acc:0.912]
Epoch [32/120    avg_loss:0.311, val_acc:0.902]
Epoch [33/120    avg_loss:0.346, val_acc:0.936]
Epoch [34/120    avg_loss:0.277, val_acc:0.934]
Epoch [35/120    avg_loss:0.288, val_acc:0.922]
Epoch [36/120    avg_loss:0.295, val_acc:0.943]
Epoch [37/120    avg_loss:0.263, val_acc:0.949]
Epoch [38/120    avg_loss:0.271, val_acc:0.816]
Epoch [39/120    avg_loss:0.343, val_acc:0.930]
Epoch [40/120    avg_loss:0.277, val_acc:0.902]
Epoch [41/120    avg_loss:0.257, val_acc:0.936]
Epoch [42/120    avg_loss:0.256, val_acc:0.916]
Epoch [43/120    avg_loss:0.268, val_acc:0.945]
Epoch [44/120    avg_loss:0.266, val_acc:0.914]
Epoch [45/120    avg_loss:0.228, val_acc:0.924]
Epoch [46/120    avg_loss:0.223, val_acc:0.938]
Epoch [47/120    avg_loss:0.202, val_acc:0.938]
Epoch [48/120    avg_loss:0.236, val_acc:0.914]
Epoch [49/120    avg_loss:0.291, val_acc:0.918]
Epoch [50/120    avg_loss:0.262, val_acc:0.953]
Epoch [51/120    avg_loss:0.244, val_acc:0.957]
Epoch [52/120    avg_loss:0.178, val_acc:0.936]
Epoch [53/120    avg_loss:0.211, val_acc:0.938]
Epoch [54/120    avg_loss:0.231, val_acc:0.941]
Epoch [55/120    avg_loss:0.178, val_acc:0.947]
Epoch [56/120    avg_loss:0.177, val_acc:0.941]
Epoch [57/120    avg_loss:0.152, val_acc:0.961]
Epoch [58/120    avg_loss:0.193, val_acc:0.910]
Epoch [59/120    avg_loss:0.173, val_acc:0.943]
Epoch [60/120    avg_loss:0.210, val_acc:0.945]
Epoch [61/120    avg_loss:0.190, val_acc:0.934]
Epoch [62/120    avg_loss:0.191, val_acc:0.957]
Epoch [63/120    avg_loss:0.167, val_acc:0.961]
Epoch [64/120    avg_loss:0.143, val_acc:0.957]
Epoch [65/120    avg_loss:0.149, val_acc:0.965]
Epoch [66/120    avg_loss:0.192, val_acc:0.938]
Epoch [67/120    avg_loss:0.200, val_acc:0.947]
Epoch [68/120    avg_loss:0.154, val_acc:0.938]
Epoch [69/120    avg_loss:0.180, val_acc:0.955]
Epoch [70/120    avg_loss:0.178, val_acc:0.965]
Epoch [71/120    avg_loss:0.138, val_acc:0.953]
Epoch [72/120    avg_loss:0.092, val_acc:0.963]
Epoch [73/120    avg_loss:0.141, val_acc:0.969]
Epoch [74/120    avg_loss:0.127, val_acc:0.934]
Epoch [75/120    avg_loss:0.192, val_acc:0.949]
Epoch [76/120    avg_loss:0.109, val_acc:0.969]
Epoch [77/120    avg_loss:0.152, val_acc:0.959]
Epoch [78/120    avg_loss:0.101, val_acc:0.965]
Epoch [79/120    avg_loss:0.089, val_acc:0.959]
Epoch [80/120    avg_loss:0.102, val_acc:0.977]
Epoch [81/120    avg_loss:0.117, val_acc:0.951]
Epoch [82/120    avg_loss:0.107, val_acc:0.975]
Epoch [83/120    avg_loss:0.087, val_acc:0.971]
Epoch [84/120    avg_loss:0.122, val_acc:0.963]
Epoch [85/120    avg_loss:0.112, val_acc:0.961]
Epoch [86/120    avg_loss:0.164, val_acc:0.959]
Epoch [87/120    avg_loss:0.090, val_acc:0.969]
Epoch [88/120    avg_loss:0.093, val_acc:0.971]
Epoch [89/120    avg_loss:0.122, val_acc:0.963]
Epoch [90/120    avg_loss:0.087, val_acc:0.957]
Epoch [91/120    avg_loss:0.129, val_acc:0.924]
Epoch [92/120    avg_loss:0.127, val_acc:0.955]
Epoch [93/120    avg_loss:0.116, val_acc:0.969]
Epoch [94/120    avg_loss:0.068, val_acc:0.971]
Epoch [95/120    avg_loss:0.059, val_acc:0.971]
Epoch [96/120    avg_loss:0.062, val_acc:0.975]
Epoch [97/120    avg_loss:0.053, val_acc:0.975]
Epoch [98/120    avg_loss:0.058, val_acc:0.973]
Epoch [99/120    avg_loss:0.052, val_acc:0.975]
Epoch [100/120    avg_loss:0.049, val_acc:0.973]
Epoch [101/120    avg_loss:0.042, val_acc:0.973]
Epoch [102/120    avg_loss:0.055, val_acc:0.977]
Epoch [103/120    avg_loss:0.059, val_acc:0.973]
Epoch [104/120    avg_loss:0.051, val_acc:0.971]
Epoch [105/120    avg_loss:0.054, val_acc:0.975]
Epoch [106/120    avg_loss:0.045, val_acc:0.975]
Epoch [107/120    avg_loss:0.056, val_acc:0.977]
Epoch [108/120    avg_loss:0.056, val_acc:0.977]
Epoch [109/120    avg_loss:0.046, val_acc:0.977]
Epoch [110/120    avg_loss:0.056, val_acc:0.975]
Epoch [111/120    avg_loss:0.048, val_acc:0.977]
Epoch [112/120    avg_loss:0.048, val_acc:0.977]
Epoch [113/120    avg_loss:0.053, val_acc:0.973]
Epoch [114/120    avg_loss:0.044, val_acc:0.979]
Epoch [115/120    avg_loss:0.039, val_acc:0.979]
Epoch [116/120    avg_loss:0.041, val_acc:0.977]
Epoch [117/120    avg_loss:0.048, val_acc:0.975]
Epoch [118/120    avg_loss:0.038, val_acc:0.975]
Epoch [119/120    avg_loss:0.037, val_acc:0.977]
Epoch [120/120    avg_loss:0.048, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   1 206   0   0   0   0   9   0   0   0   0   3   0]
 [  0   0   4 195  13  16   0   0   2   0   0   0   0   0]
 [  0   0   1   0 213  11   2   0   0   0   0   0   0   0]
 [  0   0   0   0  32 113   0   0   0   0   0   0   0   0]
 [  0   4   0   0   1   0 201   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.48400852878464

F1 scores:
[       nan 0.99636364 0.91964286 0.91764706 0.87654321 0.79298246
 0.98288509 0.85555556 0.99742931 1.         1.         0.99867198
 0.99449945 1.        ]

Kappa:
0.9719826183084087
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f382bb8e828>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.444, val_acc:0.461]
Epoch [2/120    avg_loss:2.111, val_acc:0.531]
Epoch [3/120    avg_loss:1.851, val_acc:0.594]
Epoch [4/120    avg_loss:1.608, val_acc:0.711]
Epoch [5/120    avg_loss:1.413, val_acc:0.719]
Epoch [6/120    avg_loss:1.255, val_acc:0.771]
Epoch [7/120    avg_loss:1.089, val_acc:0.801]
Epoch [8/120    avg_loss:1.029, val_acc:0.820]
Epoch [9/120    avg_loss:0.920, val_acc:0.828]
Epoch [10/120    avg_loss:0.784, val_acc:0.852]
Epoch [11/120    avg_loss:0.649, val_acc:0.852]
Epoch [12/120    avg_loss:0.682, val_acc:0.822]
Epoch [13/120    avg_loss:0.619, val_acc:0.869]
Epoch [14/120    avg_loss:0.572, val_acc:0.852]
Epoch [15/120    avg_loss:0.580, val_acc:0.875]
Epoch [16/120    avg_loss:0.497, val_acc:0.883]
Epoch [17/120    avg_loss:0.497, val_acc:0.900]
Epoch [18/120    avg_loss:0.452, val_acc:0.867]
Epoch [19/120    avg_loss:0.465, val_acc:0.879]
Epoch [20/120    avg_loss:0.409, val_acc:0.875]
Epoch [21/120    avg_loss:0.477, val_acc:0.902]
Epoch [22/120    avg_loss:0.419, val_acc:0.906]
Epoch [23/120    avg_loss:0.418, val_acc:0.898]
Epoch [24/120    avg_loss:0.403, val_acc:0.887]
Epoch [25/120    avg_loss:0.370, val_acc:0.932]
Epoch [26/120    avg_loss:0.357, val_acc:0.910]
Epoch [27/120    avg_loss:0.357, val_acc:0.924]
Epoch [28/120    avg_loss:0.338, val_acc:0.900]
Epoch [29/120    avg_loss:0.315, val_acc:0.926]
Epoch [30/120    avg_loss:0.296, val_acc:0.947]
Epoch [31/120    avg_loss:0.260, val_acc:0.938]
Epoch [32/120    avg_loss:0.307, val_acc:0.904]
Epoch [33/120    avg_loss:0.286, val_acc:0.908]
Epoch [34/120    avg_loss:0.272, val_acc:0.945]
Epoch [35/120    avg_loss:0.194, val_acc:0.939]
Epoch [36/120    avg_loss:0.195, val_acc:0.930]
Epoch [37/120    avg_loss:0.225, val_acc:0.918]
Epoch [38/120    avg_loss:0.231, val_acc:0.906]
Epoch [39/120    avg_loss:0.247, val_acc:0.932]
Epoch [40/120    avg_loss:0.204, val_acc:0.955]
Epoch [41/120    avg_loss:0.192, val_acc:0.953]
Epoch [42/120    avg_loss:0.185, val_acc:0.967]
Epoch [43/120    avg_loss:0.187, val_acc:0.947]
Epoch [44/120    avg_loss:0.191, val_acc:0.957]
Epoch [45/120    avg_loss:0.150, val_acc:0.965]
Epoch [46/120    avg_loss:0.215, val_acc:0.926]
Epoch [47/120    avg_loss:0.283, val_acc:0.912]
Epoch [48/120    avg_loss:0.212, val_acc:0.961]
Epoch [49/120    avg_loss:0.246, val_acc:0.891]
Epoch [50/120    avg_loss:0.206, val_acc:0.941]
Epoch [51/120    avg_loss:0.166, val_acc:0.945]
Epoch [52/120    avg_loss:0.156, val_acc:0.955]
Epoch [53/120    avg_loss:0.145, val_acc:0.941]
Epoch [54/120    avg_loss:0.140, val_acc:0.969]
Epoch [55/120    avg_loss:0.103, val_acc:0.963]
Epoch [56/120    avg_loss:0.104, val_acc:0.975]
Epoch [57/120    avg_loss:0.109, val_acc:0.959]
Epoch [58/120    avg_loss:0.091, val_acc:0.971]
Epoch [59/120    avg_loss:0.096, val_acc:0.957]
Epoch [60/120    avg_loss:0.106, val_acc:0.955]
Epoch [61/120    avg_loss:0.178, val_acc:0.943]
Epoch [62/120    avg_loss:0.117, val_acc:0.959]
Epoch [63/120    avg_loss:0.112, val_acc:0.969]
Epoch [64/120    avg_loss:0.157, val_acc:0.959]
Epoch [65/120    avg_loss:0.186, val_acc:0.955]
Epoch [66/120    avg_loss:0.126, val_acc:0.955]
Epoch [67/120    avg_loss:0.132, val_acc:0.975]
Epoch [68/120    avg_loss:0.088, val_acc:0.973]
Epoch [69/120    avg_loss:0.089, val_acc:0.955]
Epoch [70/120    avg_loss:0.198, val_acc:0.953]
Epoch [71/120    avg_loss:0.141, val_acc:0.939]
Epoch [72/120    avg_loss:0.130, val_acc:0.973]
Epoch [73/120    avg_loss:0.084, val_acc:0.969]
Epoch [74/120    avg_loss:0.086, val_acc:0.957]
Epoch [75/120    avg_loss:0.174, val_acc:0.947]
Epoch [76/120    avg_loss:0.108, val_acc:0.967]
Epoch [77/120    avg_loss:0.088, val_acc:0.955]
Epoch [78/120    avg_loss:0.105, val_acc:0.932]
Epoch [79/120    avg_loss:0.074, val_acc:0.963]
Epoch [80/120    avg_loss:0.102, val_acc:0.953]
Epoch [81/120    avg_loss:0.068, val_acc:0.971]
Epoch [82/120    avg_loss:0.057, val_acc:0.967]
Epoch [83/120    avg_loss:0.042, val_acc:0.969]
Epoch [84/120    avg_loss:0.046, val_acc:0.967]
Epoch [85/120    avg_loss:0.036, val_acc:0.969]
Epoch [86/120    avg_loss:0.040, val_acc:0.971]
Epoch [87/120    avg_loss:0.044, val_acc:0.971]
Epoch [88/120    avg_loss:0.042, val_acc:0.973]
Epoch [89/120    avg_loss:0.035, val_acc:0.971]
Epoch [90/120    avg_loss:0.041, val_acc:0.971]
Epoch [91/120    avg_loss:0.042, val_acc:0.973]
Epoch [92/120    avg_loss:0.030, val_acc:0.971]
Epoch [93/120    avg_loss:0.043, val_acc:0.969]
Epoch [94/120    avg_loss:0.035, val_acc:0.969]
Epoch [95/120    avg_loss:0.039, val_acc:0.969]
Epoch [96/120    avg_loss:0.038, val_acc:0.969]
Epoch [97/120    avg_loss:0.027, val_acc:0.969]
Epoch [98/120    avg_loss:0.044, val_acc:0.969]
Epoch [99/120    avg_loss:0.029, val_acc:0.969]
Epoch [100/120    avg_loss:0.041, val_acc:0.969]
Epoch [101/120    avg_loss:0.034, val_acc:0.969]
Epoch [102/120    avg_loss:0.033, val_acc:0.969]
Epoch [103/120    avg_loss:0.041, val_acc:0.969]
Epoch [104/120    avg_loss:0.034, val_acc:0.969]
Epoch [105/120    avg_loss:0.041, val_acc:0.969]
Epoch [106/120    avg_loss:0.034, val_acc:0.971]
Epoch [107/120    avg_loss:0.029, val_acc:0.971]
Epoch [108/120    avg_loss:0.038, val_acc:0.971]
Epoch [109/120    avg_loss:0.031, val_acc:0.971]
Epoch [110/120    avg_loss:0.034, val_acc:0.971]
Epoch [111/120    avg_loss:0.032, val_acc:0.971]
Epoch [112/120    avg_loss:0.034, val_acc:0.971]
Epoch [113/120    avg_loss:0.037, val_acc:0.971]
Epoch [114/120    avg_loss:0.036, val_acc:0.971]
Epoch [115/120    avg_loss:0.032, val_acc:0.971]
Epoch [116/120    avg_loss:0.038, val_acc:0.971]
Epoch [117/120    avg_loss:0.035, val_acc:0.971]
Epoch [118/120    avg_loss:0.035, val_acc:0.971]
Epoch [119/120    avg_loss:0.034, val_acc:0.971]
Epoch [120/120    avg_loss:0.034, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 208  12   0   0   0   9   1   0   0   0   0]
 [  0   0   1   0 204  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.23027718550107

F1 scores:
[       nan 0.99854227 0.96052632 0.94977169 0.88503254 0.86394558
 0.99512195 0.92571429 0.98853503 0.99893276 1.         0.99867198
 0.9944629  1.        ]

Kappa:
0.9802943361111872
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9884952908>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.499, val_acc:0.322]
Epoch [2/120    avg_loss:2.150, val_acc:0.521]
Epoch [3/120    avg_loss:1.898, val_acc:0.584]
Epoch [4/120    avg_loss:1.669, val_acc:0.711]
Epoch [5/120    avg_loss:1.478, val_acc:0.701]
Epoch [6/120    avg_loss:1.280, val_acc:0.781]
Epoch [7/120    avg_loss:1.106, val_acc:0.781]
Epoch [8/120    avg_loss:0.994, val_acc:0.836]
Epoch [9/120    avg_loss:0.891, val_acc:0.822]
Epoch [10/120    avg_loss:0.876, val_acc:0.850]
Epoch [11/120    avg_loss:0.756, val_acc:0.852]
Epoch [12/120    avg_loss:0.691, val_acc:0.879]
Epoch [13/120    avg_loss:0.602, val_acc:0.895]
Epoch [14/120    avg_loss:0.623, val_acc:0.795]
Epoch [15/120    avg_loss:0.645, val_acc:0.855]
Epoch [16/120    avg_loss:0.659, val_acc:0.895]
Epoch [17/120    avg_loss:0.560, val_acc:0.871]
Epoch [18/120    avg_loss:0.537, val_acc:0.885]
Epoch [19/120    avg_loss:0.554, val_acc:0.885]
Epoch [20/120    avg_loss:0.407, val_acc:0.918]
Epoch [21/120    avg_loss:0.445, val_acc:0.900]
Epoch [22/120    avg_loss:0.497, val_acc:0.906]
Epoch [23/120    avg_loss:0.444, val_acc:0.916]
Epoch [24/120    avg_loss:0.477, val_acc:0.895]
Epoch [25/120    avg_loss:0.429, val_acc:0.896]
Epoch [26/120    avg_loss:0.415, val_acc:0.883]
Epoch [27/120    avg_loss:0.415, val_acc:0.924]
Epoch [28/120    avg_loss:0.319, val_acc:0.926]
Epoch [29/120    avg_loss:0.353, val_acc:0.896]
Epoch [30/120    avg_loss:0.436, val_acc:0.914]
Epoch [31/120    avg_loss:0.357, val_acc:0.934]
Epoch [32/120    avg_loss:0.383, val_acc:0.920]
Epoch [33/120    avg_loss:0.355, val_acc:0.945]
Epoch [34/120    avg_loss:0.323, val_acc:0.924]
Epoch [35/120    avg_loss:0.306, val_acc:0.922]
Epoch [36/120    avg_loss:0.348, val_acc:0.922]
Epoch [37/120    avg_loss:0.329, val_acc:0.918]
Epoch [38/120    avg_loss:0.298, val_acc:0.930]
Epoch [39/120    avg_loss:0.296, val_acc:0.945]
Epoch [40/120    avg_loss:0.320, val_acc:0.920]
Epoch [41/120    avg_loss:0.294, val_acc:0.924]
Epoch [42/120    avg_loss:0.248, val_acc:0.961]
Epoch [43/120    avg_loss:0.264, val_acc:0.904]
Epoch [44/120    avg_loss:0.289, val_acc:0.943]
Epoch [45/120    avg_loss:0.267, val_acc:0.932]
Epoch [46/120    avg_loss:0.269, val_acc:0.957]
Epoch [47/120    avg_loss:0.255, val_acc:0.912]
Epoch [48/120    avg_loss:0.223, val_acc:0.928]
Epoch [49/120    avg_loss:0.219, val_acc:0.936]
Epoch [50/120    avg_loss:0.182, val_acc:0.947]
Epoch [51/120    avg_loss:0.197, val_acc:0.910]
Epoch [52/120    avg_loss:0.193, val_acc:0.959]
Epoch [53/120    avg_loss:0.186, val_acc:0.914]
Epoch [54/120    avg_loss:0.144, val_acc:0.965]
Epoch [55/120    avg_loss:0.187, val_acc:0.955]
Epoch [56/120    avg_loss:0.167, val_acc:0.955]
Epoch [57/120    avg_loss:0.142, val_acc:0.957]
Epoch [58/120    avg_loss:0.155, val_acc:0.945]
Epoch [59/120    avg_loss:0.161, val_acc:0.928]
Epoch [60/120    avg_loss:0.173, val_acc:0.947]
Epoch [61/120    avg_loss:0.152, val_acc:0.945]
Epoch [62/120    avg_loss:0.151, val_acc:0.965]
Epoch [63/120    avg_loss:0.143, val_acc:0.953]
Epoch [64/120    avg_loss:0.176, val_acc:0.941]
Epoch [65/120    avg_loss:0.184, val_acc:0.879]
Epoch [66/120    avg_loss:0.146, val_acc:0.955]
Epoch [67/120    avg_loss:0.098, val_acc:0.971]
Epoch [68/120    avg_loss:0.137, val_acc:0.965]
Epoch [69/120    avg_loss:0.160, val_acc:0.949]
Epoch [70/120    avg_loss:0.146, val_acc:0.969]
Epoch [71/120    avg_loss:0.168, val_acc:0.934]
Epoch [72/120    avg_loss:0.208, val_acc:0.955]
Epoch [73/120    avg_loss:0.126, val_acc:0.963]
Epoch [74/120    avg_loss:0.116, val_acc:0.961]
Epoch [75/120    avg_loss:0.095, val_acc:0.963]
Epoch [76/120    avg_loss:0.163, val_acc:0.955]
Epoch [77/120    avg_loss:0.178, val_acc:0.924]
Epoch [78/120    avg_loss:0.152, val_acc:0.961]
Epoch [79/120    avg_loss:0.110, val_acc:0.977]
Epoch [80/120    avg_loss:0.102, val_acc:0.973]
Epoch [81/120    avg_loss:0.089, val_acc:0.957]
Epoch [82/120    avg_loss:0.104, val_acc:0.971]
Epoch [83/120    avg_loss:0.089, val_acc:0.971]
Epoch [84/120    avg_loss:0.083, val_acc:0.979]
Epoch [85/120    avg_loss:0.092, val_acc:0.951]
Epoch [86/120    avg_loss:0.119, val_acc:0.965]
Epoch [87/120    avg_loss:0.083, val_acc:0.980]
Epoch [88/120    avg_loss:0.050, val_acc:0.969]
Epoch [89/120    avg_loss:0.071, val_acc:0.969]
Epoch [90/120    avg_loss:0.062, val_acc:0.971]
Epoch [91/120    avg_loss:0.073, val_acc:0.975]
Epoch [92/120    avg_loss:0.089, val_acc:0.969]
Epoch [93/120    avg_loss:0.077, val_acc:0.967]
Epoch [94/120    avg_loss:0.058, val_acc:0.971]
Epoch [95/120    avg_loss:0.076, val_acc:0.975]
Epoch [96/120    avg_loss:0.095, val_acc:0.982]
Epoch [97/120    avg_loss:0.087, val_acc:0.969]
Epoch [98/120    avg_loss:0.066, val_acc:0.980]
Epoch [99/120    avg_loss:0.085, val_acc:0.973]
Epoch [100/120    avg_loss:0.058, val_acc:0.982]
Epoch [101/120    avg_loss:0.053, val_acc:0.977]
Epoch [102/120    avg_loss:0.055, val_acc:0.986]
Epoch [103/120    avg_loss:0.040, val_acc:0.982]
Epoch [104/120    avg_loss:0.044, val_acc:0.977]
Epoch [105/120    avg_loss:0.042, val_acc:0.979]
Epoch [106/120    avg_loss:0.073, val_acc:0.969]
Epoch [107/120    avg_loss:0.084, val_acc:0.947]
Epoch [108/120    avg_loss:0.150, val_acc:0.965]
Epoch [109/120    avg_loss:0.057, val_acc:0.977]
Epoch [110/120    avg_loss:0.053, val_acc:0.975]
Epoch [111/120    avg_loss:0.073, val_acc:0.953]
Epoch [112/120    avg_loss:0.107, val_acc:0.969]
Epoch [113/120    avg_loss:0.062, val_acc:0.980]
Epoch [114/120    avg_loss:0.049, val_acc:0.971]
Epoch [115/120    avg_loss:0.109, val_acc:0.963]
Epoch [116/120    avg_loss:0.064, val_acc:0.979]
Epoch [117/120    avg_loss:0.044, val_acc:0.977]
Epoch [118/120    avg_loss:0.037, val_acc:0.979]
Epoch [119/120    avg_loss:0.034, val_acc:0.979]
Epoch [120/120    avg_loss:0.032, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 213   5   0   0   0   8   3   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   2   0   0   2   0 202   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   3   0   0   0   0   0   1   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.46481876332622

F1 scores:
[       nan 0.99854227 0.96263736 0.96162528 0.91257996 0.89122807
 0.99019608 0.90697674 0.98979592 0.99680511 0.99862826 1.
 0.99556541 1.        ]

Kappa:
0.982904559315757
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4b46f71940>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.468, val_acc:0.404]
Epoch [2/120    avg_loss:2.133, val_acc:0.621]
Epoch [3/120    avg_loss:1.883, val_acc:0.637]
Epoch [4/120    avg_loss:1.646, val_acc:0.699]
Epoch [5/120    avg_loss:1.385, val_acc:0.713]
Epoch [6/120    avg_loss:1.239, val_acc:0.736]
Epoch [7/120    avg_loss:1.050, val_acc:0.773]
Epoch [8/120    avg_loss:0.954, val_acc:0.838]
Epoch [9/120    avg_loss:0.840, val_acc:0.834]
Epoch [10/120    avg_loss:0.737, val_acc:0.857]
Epoch [11/120    avg_loss:0.717, val_acc:0.869]
Epoch [12/120    avg_loss:0.647, val_acc:0.895]
Epoch [13/120    avg_loss:0.613, val_acc:0.889]
Epoch [14/120    avg_loss:0.574, val_acc:0.879]
Epoch [15/120    avg_loss:0.505, val_acc:0.883]
Epoch [16/120    avg_loss:0.557, val_acc:0.895]
Epoch [17/120    avg_loss:0.541, val_acc:0.895]
Epoch [18/120    avg_loss:0.482, val_acc:0.914]
Epoch [19/120    avg_loss:0.400, val_acc:0.838]
Epoch [20/120    avg_loss:0.482, val_acc:0.898]
Epoch [21/120    avg_loss:0.399, val_acc:0.914]
Epoch [22/120    avg_loss:0.368, val_acc:0.893]
Epoch [23/120    avg_loss:0.414, val_acc:0.910]
Epoch [24/120    avg_loss:0.348, val_acc:0.912]
Epoch [25/120    avg_loss:0.366, val_acc:0.881]
Epoch [26/120    avg_loss:0.334, val_acc:0.928]
Epoch [27/120    avg_loss:0.285, val_acc:0.928]
Epoch [28/120    avg_loss:0.295, val_acc:0.938]
Epoch [29/120    avg_loss:0.289, val_acc:0.922]
Epoch [30/120    avg_loss:0.318, val_acc:0.926]
Epoch [31/120    avg_loss:0.292, val_acc:0.922]
Epoch [32/120    avg_loss:0.296, val_acc:0.939]
Epoch [33/120    avg_loss:0.213, val_acc:0.943]
Epoch [34/120    avg_loss:0.356, val_acc:0.867]
Epoch [35/120    avg_loss:0.289, val_acc:0.926]
Epoch [36/120    avg_loss:0.227, val_acc:0.936]
Epoch [37/120    avg_loss:0.219, val_acc:0.953]
Epoch [38/120    avg_loss:0.235, val_acc:0.904]
Epoch [39/120    avg_loss:0.199, val_acc:0.936]
Epoch [40/120    avg_loss:0.165, val_acc:0.936]
Epoch [41/120    avg_loss:0.226, val_acc:0.938]
Epoch [42/120    avg_loss:0.164, val_acc:0.918]
Epoch [43/120    avg_loss:0.244, val_acc:0.914]
Epoch [44/120    avg_loss:0.277, val_acc:0.955]
Epoch [45/120    avg_loss:0.202, val_acc:0.934]
Epoch [46/120    avg_loss:0.206, val_acc:0.951]
Epoch [47/120    avg_loss:0.183, val_acc:0.947]
Epoch [48/120    avg_loss:0.156, val_acc:0.945]
Epoch [49/120    avg_loss:0.159, val_acc:0.936]
Epoch [50/120    avg_loss:0.142, val_acc:0.965]
Epoch [51/120    avg_loss:0.140, val_acc:0.945]
Epoch [52/120    avg_loss:0.171, val_acc:0.957]
Epoch [53/120    avg_loss:0.117, val_acc:0.963]
Epoch [54/120    avg_loss:0.097, val_acc:0.932]
Epoch [55/120    avg_loss:0.130, val_acc:0.957]
Epoch [56/120    avg_loss:0.108, val_acc:0.973]
Epoch [57/120    avg_loss:0.108, val_acc:0.961]
Epoch [58/120    avg_loss:0.101, val_acc:0.961]
Epoch [59/120    avg_loss:0.116, val_acc:0.967]
Epoch [60/120    avg_loss:0.122, val_acc:0.955]
Epoch [61/120    avg_loss:0.117, val_acc:0.967]
Epoch [62/120    avg_loss:0.114, val_acc:0.951]
Epoch [63/120    avg_loss:0.124, val_acc:0.957]
Epoch [64/120    avg_loss:0.105, val_acc:0.973]
Epoch [65/120    avg_loss:0.102, val_acc:0.965]
Epoch [66/120    avg_loss:0.101, val_acc:0.971]
Epoch [67/120    avg_loss:0.120, val_acc:0.959]
Epoch [68/120    avg_loss:0.126, val_acc:0.971]
Epoch [69/120    avg_loss:0.120, val_acc:0.965]
Epoch [70/120    avg_loss:0.075, val_acc:0.967]
Epoch [71/120    avg_loss:0.074, val_acc:0.977]
Epoch [72/120    avg_loss:0.103, val_acc:0.947]
Epoch [73/120    avg_loss:0.122, val_acc:0.963]
Epoch [74/120    avg_loss:0.120, val_acc:0.959]
Epoch [75/120    avg_loss:0.098, val_acc:0.973]
Epoch [76/120    avg_loss:0.075, val_acc:0.963]
Epoch [77/120    avg_loss:0.080, val_acc:0.967]
Epoch [78/120    avg_loss:0.048, val_acc:0.982]
Epoch [79/120    avg_loss:0.065, val_acc:0.957]
Epoch [80/120    avg_loss:0.051, val_acc:0.979]
Epoch [81/120    avg_loss:0.048, val_acc:0.975]
Epoch [82/120    avg_loss:0.030, val_acc:0.984]
Epoch [83/120    avg_loss:0.030, val_acc:0.979]
Epoch [84/120    avg_loss:0.063, val_acc:0.982]
Epoch [85/120    avg_loss:0.072, val_acc:0.979]
Epoch [86/120    avg_loss:0.074, val_acc:0.971]
Epoch [87/120    avg_loss:0.082, val_acc:0.982]
Epoch [88/120    avg_loss:0.051, val_acc:0.980]
Epoch [89/120    avg_loss:0.040, val_acc:0.980]
Epoch [90/120    avg_loss:0.079, val_acc:0.969]
Epoch [91/120    avg_loss:0.044, val_acc:0.982]
Epoch [92/120    avg_loss:0.029, val_acc:0.980]
Epoch [93/120    avg_loss:0.042, val_acc:0.973]
Epoch [94/120    avg_loss:0.030, val_acc:0.986]
Epoch [95/120    avg_loss:0.024, val_acc:0.979]
Epoch [96/120    avg_loss:0.041, val_acc:0.977]
Epoch [97/120    avg_loss:0.061, val_acc:0.977]
Epoch [98/120    avg_loss:0.028, val_acc:0.984]
Epoch [99/120    avg_loss:0.036, val_acc:0.977]
Epoch [100/120    avg_loss:0.080, val_acc:0.969]
Epoch [101/120    avg_loss:0.104, val_acc:0.975]
Epoch [102/120    avg_loss:0.063, val_acc:0.979]
Epoch [103/120    avg_loss:0.041, val_acc:0.967]
Epoch [104/120    avg_loss:0.056, val_acc:0.986]
Epoch [105/120    avg_loss:0.028, val_acc:0.984]
Epoch [106/120    avg_loss:0.026, val_acc:0.973]
Epoch [107/120    avg_loss:0.018, val_acc:0.986]
Epoch [108/120    avg_loss:0.021, val_acc:0.988]
Epoch [109/120    avg_loss:0.030, val_acc:0.982]
Epoch [110/120    avg_loss:0.014, val_acc:0.990]
Epoch [111/120    avg_loss:0.014, val_acc:0.988]
Epoch [112/120    avg_loss:0.010, val_acc:0.986]
Epoch [113/120    avg_loss:0.011, val_acc:0.988]
Epoch [114/120    avg_loss:0.010, val_acc:0.986]
Epoch [115/120    avg_loss:0.010, val_acc:0.984]
Epoch [116/120    avg_loss:0.009, val_acc:0.990]
Epoch [117/120    avg_loss:0.021, val_acc:0.986]
Epoch [118/120    avg_loss:0.015, val_acc:0.986]
Epoch [119/120    avg_loss:0.009, val_acc:0.982]
Epoch [120/120    avg_loss:0.010, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 205  24   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  22 122   1   0   0   0   0   0   0   0]
 [  0   1   0   0   4   0 201   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.65671641791045

F1 scores:
[       nan 0.99927061 0.9977221  0.94252874 0.88080808 0.88405797
 0.98529412 1.         1.         1.         1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9850438093838138
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb4cda02908>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.488, val_acc:0.379]
Epoch [2/120    avg_loss:2.148, val_acc:0.502]
Epoch [3/120    avg_loss:1.896, val_acc:0.596]
Epoch [4/120    avg_loss:1.651, val_acc:0.633]
Epoch [5/120    avg_loss:1.418, val_acc:0.686]
Epoch [6/120    avg_loss:1.233, val_acc:0.715]
Epoch [7/120    avg_loss:1.031, val_acc:0.867]
Epoch [8/120    avg_loss:0.927, val_acc:0.871]
Epoch [9/120    avg_loss:0.852, val_acc:0.848]
Epoch [10/120    avg_loss:0.720, val_acc:0.896]
Epoch [11/120    avg_loss:0.689, val_acc:0.893]
Epoch [12/120    avg_loss:0.649, val_acc:0.834]
Epoch [13/120    avg_loss:0.627, val_acc:0.916]
Epoch [14/120    avg_loss:0.572, val_acc:0.936]
Epoch [15/120    avg_loss:0.521, val_acc:0.898]
Epoch [16/120    avg_loss:0.502, val_acc:0.936]
Epoch [17/120    avg_loss:0.459, val_acc:0.908]
Epoch [18/120    avg_loss:0.426, val_acc:0.916]
Epoch [19/120    avg_loss:0.404, val_acc:0.943]
Epoch [20/120    avg_loss:0.425, val_acc:0.910]
Epoch [21/120    avg_loss:0.493, val_acc:0.945]
Epoch [22/120    avg_loss:0.400, val_acc:0.941]
Epoch [23/120    avg_loss:0.352, val_acc:0.930]
Epoch [24/120    avg_loss:0.429, val_acc:0.906]
Epoch [25/120    avg_loss:0.404, val_acc:0.934]
Epoch [26/120    avg_loss:0.356, val_acc:0.939]
Epoch [27/120    avg_loss:0.342, val_acc:0.957]
Epoch [28/120    avg_loss:0.295, val_acc:0.924]
Epoch [29/120    avg_loss:0.368, val_acc:0.955]
Epoch [30/120    avg_loss:0.351, val_acc:0.934]
Epoch [31/120    avg_loss:0.291, val_acc:0.957]
Epoch [32/120    avg_loss:0.218, val_acc:0.943]
Epoch [33/120    avg_loss:0.303, val_acc:0.963]
Epoch [34/120    avg_loss:0.317, val_acc:0.945]
Epoch [35/120    avg_loss:0.262, val_acc:0.947]
Epoch [36/120    avg_loss:0.230, val_acc:0.932]
Epoch [37/120    avg_loss:0.279, val_acc:0.963]
Epoch [38/120    avg_loss:0.217, val_acc:0.971]
Epoch [39/120    avg_loss:0.245, val_acc:0.961]
Epoch [40/120    avg_loss:0.259, val_acc:0.949]
Epoch [41/120    avg_loss:0.218, val_acc:0.951]
Epoch [42/120    avg_loss:0.220, val_acc:0.936]
Epoch [43/120    avg_loss:0.176, val_acc:0.934]
Epoch [44/120    avg_loss:0.199, val_acc:0.967]
Epoch [45/120    avg_loss:0.170, val_acc:0.943]
Epoch [46/120    avg_loss:0.173, val_acc:0.980]
Epoch [47/120    avg_loss:0.164, val_acc:0.969]
Epoch [48/120    avg_loss:0.197, val_acc:0.971]
Epoch [49/120    avg_loss:0.185, val_acc:0.973]
Epoch [50/120    avg_loss:0.186, val_acc:0.980]
Epoch [51/120    avg_loss:0.164, val_acc:0.959]
Epoch [52/120    avg_loss:0.160, val_acc:0.973]
Epoch [53/120    avg_loss:0.161, val_acc:0.973]
Epoch [54/120    avg_loss:0.170, val_acc:0.926]
Epoch [55/120    avg_loss:0.123, val_acc:0.963]
Epoch [56/120    avg_loss:0.083, val_acc:0.984]
Epoch [57/120    avg_loss:0.093, val_acc:0.982]
Epoch [58/120    avg_loss:0.118, val_acc:0.979]
Epoch [59/120    avg_loss:0.092, val_acc:0.980]
Epoch [60/120    avg_loss:0.079, val_acc:0.984]
Epoch [61/120    avg_loss:0.077, val_acc:0.979]
Epoch [62/120    avg_loss:0.089, val_acc:0.977]
Epoch [63/120    avg_loss:0.129, val_acc:0.963]
Epoch [64/120    avg_loss:0.089, val_acc:0.975]
Epoch [65/120    avg_loss:0.145, val_acc:0.977]
Epoch [66/120    avg_loss:0.110, val_acc:0.977]
Epoch [67/120    avg_loss:0.118, val_acc:0.973]
Epoch [68/120    avg_loss:0.130, val_acc:0.967]
Epoch [69/120    avg_loss:0.110, val_acc:0.967]
Epoch [70/120    avg_loss:0.126, val_acc:0.986]
Epoch [71/120    avg_loss:0.081, val_acc:0.986]
Epoch [72/120    avg_loss:0.071, val_acc:0.988]
Epoch [73/120    avg_loss:0.098, val_acc:0.979]
Epoch [74/120    avg_loss:0.078, val_acc:0.986]
Epoch [75/120    avg_loss:0.040, val_acc:0.980]
Epoch [76/120    avg_loss:0.054, val_acc:0.984]
Epoch [77/120    avg_loss:0.066, val_acc:0.957]
Epoch [78/120    avg_loss:0.063, val_acc:0.990]
Epoch [79/120    avg_loss:0.052, val_acc:0.980]
Epoch [80/120    avg_loss:0.045, val_acc:0.980]
Epoch [81/120    avg_loss:0.035, val_acc:0.990]
Epoch [82/120    avg_loss:0.036, val_acc:0.992]
Epoch [83/120    avg_loss:0.052, val_acc:0.986]
Epoch [84/120    avg_loss:0.048, val_acc:0.979]
Epoch [85/120    avg_loss:0.052, val_acc:0.990]
Epoch [86/120    avg_loss:0.032, val_acc:0.988]
Epoch [87/120    avg_loss:0.029, val_acc:0.990]
Epoch [88/120    avg_loss:0.040, val_acc:0.986]
Epoch [89/120    avg_loss:0.034, val_acc:0.990]
Epoch [90/120    avg_loss:0.033, val_acc:0.980]
Epoch [91/120    avg_loss:0.025, val_acc:0.988]
Epoch [92/120    avg_loss:0.031, val_acc:0.988]
Epoch [93/120    avg_loss:0.028, val_acc:0.986]
Epoch [94/120    avg_loss:0.028, val_acc:0.988]
Epoch [95/120    avg_loss:0.017, val_acc:0.988]
Epoch [96/120    avg_loss:0.018, val_acc:0.990]
Epoch [97/120    avg_loss:0.017, val_acc:0.992]
Epoch [98/120    avg_loss:0.015, val_acc:0.988]
Epoch [99/120    avg_loss:0.016, val_acc:0.990]
Epoch [100/120    avg_loss:0.013, val_acc:0.992]
Epoch [101/120    avg_loss:0.018, val_acc:0.992]
Epoch [102/120    avg_loss:0.013, val_acc:0.992]
Epoch [103/120    avg_loss:0.017, val_acc:0.988]
Epoch [104/120    avg_loss:0.011, val_acc:0.988]
Epoch [105/120    avg_loss:0.015, val_acc:0.986]
Epoch [106/120    avg_loss:0.016, val_acc:0.990]
Epoch [107/120    avg_loss:0.010, val_acc:0.990]
Epoch [108/120    avg_loss:0.013, val_acc:0.990]
Epoch [109/120    avg_loss:0.013, val_acc:0.990]
Epoch [110/120    avg_loss:0.026, val_acc:0.992]
Epoch [111/120    avg_loss:0.012, val_acc:0.990]
Epoch [112/120    avg_loss:0.013, val_acc:0.990]
Epoch [113/120    avg_loss:0.011, val_acc:0.990]
Epoch [114/120    avg_loss:0.015, val_acc:0.992]
Epoch [115/120    avg_loss:0.017, val_acc:0.990]
Epoch [116/120    avg_loss:0.012, val_acc:0.990]
Epoch [117/120    avg_loss:0.014, val_acc:0.990]
Epoch [118/120    avg_loss:0.012, val_acc:0.992]
Epoch [119/120    avg_loss:0.016, val_acc:0.988]
Epoch [120/120    avg_loss:0.012, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 219  10   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   2   0   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 0.99854227 0.99541284 0.97550111 0.94013304 0.94389439
 1.         0.98947368 1.         0.99893276 1.         1.
 0.99778761 1.        ]

Kappa:
0.9924040410420676
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:06:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f186daab828>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.506, val_acc:0.357]
Epoch [2/120    avg_loss:2.176, val_acc:0.560]
Epoch [3/120    avg_loss:1.929, val_acc:0.591]
Epoch [4/120    avg_loss:1.689, val_acc:0.637]
Epoch [5/120    avg_loss:1.443, val_acc:0.698]
Epoch [6/120    avg_loss:1.284, val_acc:0.778]
Epoch [7/120    avg_loss:1.126, val_acc:0.766]
Epoch [8/120    avg_loss:1.011, val_acc:0.726]
Epoch [9/120    avg_loss:0.899, val_acc:0.808]
Epoch [10/120    avg_loss:0.784, val_acc:0.855]
Epoch [11/120    avg_loss:0.732, val_acc:0.829]
Epoch [12/120    avg_loss:0.712, val_acc:0.857]
Epoch [13/120    avg_loss:0.740, val_acc:0.859]
Epoch [14/120    avg_loss:0.647, val_acc:0.823]
Epoch [15/120    avg_loss:0.606, val_acc:0.837]
Epoch [16/120    avg_loss:0.633, val_acc:0.827]
Epoch [17/120    avg_loss:0.568, val_acc:0.863]
Epoch [18/120    avg_loss:0.516, val_acc:0.887]
Epoch [19/120    avg_loss:0.486, val_acc:0.883]
Epoch [20/120    avg_loss:0.455, val_acc:0.871]
Epoch [21/120    avg_loss:0.470, val_acc:0.895]
Epoch [22/120    avg_loss:0.383, val_acc:0.885]
Epoch [23/120    avg_loss:0.460, val_acc:0.885]
Epoch [24/120    avg_loss:0.450, val_acc:0.909]
Epoch [25/120    avg_loss:0.444, val_acc:0.901]
Epoch [26/120    avg_loss:0.436, val_acc:0.897]
Epoch [27/120    avg_loss:0.361, val_acc:0.901]
Epoch [28/120    avg_loss:0.366, val_acc:0.909]
Epoch [29/120    avg_loss:0.360, val_acc:0.905]
Epoch [30/120    avg_loss:0.389, val_acc:0.891]
Epoch [31/120    avg_loss:0.414, val_acc:0.885]
Epoch [32/120    avg_loss:0.329, val_acc:0.909]
Epoch [33/120    avg_loss:0.291, val_acc:0.901]
Epoch [34/120    avg_loss:0.290, val_acc:0.931]
Epoch [35/120    avg_loss:0.259, val_acc:0.921]
Epoch [36/120    avg_loss:0.380, val_acc:0.913]
Epoch [37/120    avg_loss:0.236, val_acc:0.927]
Epoch [38/120    avg_loss:0.317, val_acc:0.841]
Epoch [39/120    avg_loss:0.335, val_acc:0.915]
Epoch [40/120    avg_loss:0.264, val_acc:0.929]
Epoch [41/120    avg_loss:0.223, val_acc:0.942]
Epoch [42/120    avg_loss:0.233, val_acc:0.944]
Epoch [43/120    avg_loss:0.225, val_acc:0.933]
Epoch [44/120    avg_loss:0.204, val_acc:0.929]
Epoch [45/120    avg_loss:0.212, val_acc:0.901]
Epoch [46/120    avg_loss:0.198, val_acc:0.946]
Epoch [47/120    avg_loss:0.188, val_acc:0.929]
Epoch [48/120    avg_loss:0.197, val_acc:0.913]
Epoch [49/120    avg_loss:0.272, val_acc:0.881]
Epoch [50/120    avg_loss:0.236, val_acc:0.956]
Epoch [51/120    avg_loss:0.250, val_acc:0.944]
Epoch [52/120    avg_loss:0.191, val_acc:0.927]
Epoch [53/120    avg_loss:0.201, val_acc:0.956]
Epoch [54/120    avg_loss:0.140, val_acc:0.962]
Epoch [55/120    avg_loss:0.154, val_acc:0.966]
Epoch [56/120    avg_loss:0.127, val_acc:0.950]
Epoch [57/120    avg_loss:0.134, val_acc:0.942]
Epoch [58/120    avg_loss:0.183, val_acc:0.954]
Epoch [59/120    avg_loss:0.156, val_acc:0.929]
Epoch [60/120    avg_loss:0.154, val_acc:0.948]
Epoch [61/120    avg_loss:0.136, val_acc:0.952]
Epoch [62/120    avg_loss:0.151, val_acc:0.962]
Epoch [63/120    avg_loss:0.151, val_acc:0.958]
Epoch [64/120    avg_loss:0.106, val_acc:0.960]
Epoch [65/120    avg_loss:0.119, val_acc:0.948]
Epoch [66/120    avg_loss:0.137, val_acc:0.952]
Epoch [67/120    avg_loss:0.147, val_acc:0.885]
Epoch [68/120    avg_loss:0.136, val_acc:0.964]
Epoch [69/120    avg_loss:0.085, val_acc:0.964]
Epoch [70/120    avg_loss:0.079, val_acc:0.972]
Epoch [71/120    avg_loss:0.055, val_acc:0.970]
Epoch [72/120    avg_loss:0.072, val_acc:0.970]
Epoch [73/120    avg_loss:0.052, val_acc:0.974]
Epoch [74/120    avg_loss:0.063, val_acc:0.974]
Epoch [75/120    avg_loss:0.053, val_acc:0.976]
Epoch [76/120    avg_loss:0.061, val_acc:0.982]
Epoch [77/120    avg_loss:0.061, val_acc:0.980]
Epoch [78/120    avg_loss:0.066, val_acc:0.980]
Epoch [79/120    avg_loss:0.056, val_acc:0.978]
Epoch [80/120    avg_loss:0.057, val_acc:0.982]
Epoch [81/120    avg_loss:0.062, val_acc:0.980]
Epoch [82/120    avg_loss:0.049, val_acc:0.982]
Epoch [83/120    avg_loss:0.055, val_acc:0.982]
Epoch [84/120    avg_loss:0.060, val_acc:0.984]
Epoch [85/120    avg_loss:0.065, val_acc:0.980]
Epoch [86/120    avg_loss:0.060, val_acc:0.986]
Epoch [87/120    avg_loss:0.047, val_acc:0.980]
Epoch [88/120    avg_loss:0.051, val_acc:0.980]
Epoch [89/120    avg_loss:0.048, val_acc:0.982]
Epoch [90/120    avg_loss:0.057, val_acc:0.986]
Epoch [91/120    avg_loss:0.046, val_acc:0.984]
Epoch [92/120    avg_loss:0.048, val_acc:0.984]
Epoch [93/120    avg_loss:0.049, val_acc:0.982]
Epoch [94/120    avg_loss:0.050, val_acc:0.980]
Epoch [95/120    avg_loss:0.046, val_acc:0.980]
Epoch [96/120    avg_loss:0.045, val_acc:0.978]
Epoch [97/120    avg_loss:0.045, val_acc:0.980]
Epoch [98/120    avg_loss:0.044, val_acc:0.982]
Epoch [99/120    avg_loss:0.053, val_acc:0.980]
Epoch [100/120    avg_loss:0.045, val_acc:0.980]
Epoch [101/120    avg_loss:0.043, val_acc:0.982]
Epoch [102/120    avg_loss:0.048, val_acc:0.982]
Epoch [103/120    avg_loss:0.044, val_acc:0.984]
Epoch [104/120    avg_loss:0.048, val_acc:0.984]
Epoch [105/120    avg_loss:0.044, val_acc:0.984]
Epoch [106/120    avg_loss:0.042, val_acc:0.984]
Epoch [107/120    avg_loss:0.040, val_acc:0.984]
Epoch [108/120    avg_loss:0.039, val_acc:0.984]
Epoch [109/120    avg_loss:0.044, val_acc:0.984]
Epoch [110/120    avg_loss:0.036, val_acc:0.984]
Epoch [111/120    avg_loss:0.047, val_acc:0.984]
Epoch [112/120    avg_loss:0.038, val_acc:0.984]
Epoch [113/120    avg_loss:0.042, val_acc:0.984]
Epoch [114/120    avg_loss:0.041, val_acc:0.984]
Epoch [115/120    avg_loss:0.047, val_acc:0.984]
Epoch [116/120    avg_loss:0.042, val_acc:0.982]
Epoch [117/120    avg_loss:0.056, val_acc:0.982]
Epoch [118/120    avg_loss:0.040, val_acc:0.982]
Epoch [119/120    avg_loss:0.043, val_acc:0.982]
Epoch [120/120    avg_loss:0.040, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 1.         0.97767857 0.99343545 0.91479821 0.88741722
 0.99756691 0.94972067 1.         1.         1.         1.
 0.99889503 1.        ]

Kappa:
0.9886057368839313
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f47e0299908>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.407, val_acc:0.363]
Epoch [2/120    avg_loss:2.080, val_acc:0.567]
Epoch [3/120    avg_loss:1.792, val_acc:0.639]
Epoch [4/120    avg_loss:1.540, val_acc:0.669]
Epoch [5/120    avg_loss:1.324, val_acc:0.712]
Epoch [6/120    avg_loss:1.133, val_acc:0.752]
Epoch [7/120    avg_loss:1.020, val_acc:0.722]
Epoch [8/120    avg_loss:0.958, val_acc:0.819]
Epoch [9/120    avg_loss:0.790, val_acc:0.875]
Epoch [10/120    avg_loss:0.700, val_acc:0.859]
Epoch [11/120    avg_loss:0.638, val_acc:0.821]
Epoch [12/120    avg_loss:0.618, val_acc:0.901]
Epoch [13/120    avg_loss:0.561, val_acc:0.915]
Epoch [14/120    avg_loss:0.538, val_acc:0.921]
Epoch [15/120    avg_loss:0.542, val_acc:0.867]
Epoch [16/120    avg_loss:0.522, val_acc:0.929]
Epoch [17/120    avg_loss:0.419, val_acc:0.919]
Epoch [18/120    avg_loss:0.460, val_acc:0.921]
Epoch [19/120    avg_loss:0.370, val_acc:0.935]
Epoch [20/120    avg_loss:0.387, val_acc:0.944]
Epoch [21/120    avg_loss:0.429, val_acc:0.921]
Epoch [22/120    avg_loss:0.374, val_acc:0.948]
Epoch [23/120    avg_loss:0.335, val_acc:0.940]
Epoch [24/120    avg_loss:0.330, val_acc:0.952]
Epoch [25/120    avg_loss:0.261, val_acc:0.899]
Epoch [26/120    avg_loss:0.318, val_acc:0.927]
Epoch [27/120    avg_loss:0.344, val_acc:0.927]
Epoch [28/120    avg_loss:0.333, val_acc:0.923]
Epoch [29/120    avg_loss:0.313, val_acc:0.929]
Epoch [30/120    avg_loss:0.271, val_acc:0.960]
Epoch [31/120    avg_loss:0.237, val_acc:0.940]
Epoch [32/120    avg_loss:0.278, val_acc:0.927]
Epoch [33/120    avg_loss:0.243, val_acc:0.956]
Epoch [34/120    avg_loss:0.233, val_acc:0.952]
Epoch [35/120    avg_loss:0.238, val_acc:0.956]
Epoch [36/120    avg_loss:0.221, val_acc:0.958]
Epoch [37/120    avg_loss:0.205, val_acc:0.952]
Epoch [38/120    avg_loss:0.221, val_acc:0.970]
Epoch [39/120    avg_loss:0.257, val_acc:0.942]
Epoch [40/120    avg_loss:0.295, val_acc:0.938]
Epoch [41/120    avg_loss:0.216, val_acc:0.962]
Epoch [42/120    avg_loss:0.181, val_acc:0.944]
Epoch [43/120    avg_loss:0.154, val_acc:0.962]
Epoch [44/120    avg_loss:0.170, val_acc:0.972]
Epoch [45/120    avg_loss:0.201, val_acc:0.960]
Epoch [46/120    avg_loss:0.174, val_acc:0.970]
Epoch [47/120    avg_loss:0.135, val_acc:0.978]
Epoch [48/120    avg_loss:0.115, val_acc:0.968]
Epoch [49/120    avg_loss:0.157, val_acc:0.968]
Epoch [50/120    avg_loss:0.144, val_acc:0.984]
Epoch [51/120    avg_loss:0.112, val_acc:0.970]
Epoch [52/120    avg_loss:0.116, val_acc:0.974]
Epoch [53/120    avg_loss:0.115, val_acc:0.976]
Epoch [54/120    avg_loss:0.098, val_acc:0.974]
Epoch [55/120    avg_loss:0.140, val_acc:0.972]
Epoch [56/120    avg_loss:0.111, val_acc:0.980]
Epoch [57/120    avg_loss:0.083, val_acc:0.976]
Epoch [58/120    avg_loss:0.138, val_acc:0.980]
Epoch [59/120    avg_loss:0.182, val_acc:0.978]
Epoch [60/120    avg_loss:0.106, val_acc:0.972]
Epoch [61/120    avg_loss:0.121, val_acc:0.988]
Epoch [62/120    avg_loss:0.078, val_acc:0.958]
Epoch [63/120    avg_loss:0.130, val_acc:0.972]
Epoch [64/120    avg_loss:0.169, val_acc:0.966]
Epoch [65/120    avg_loss:0.102, val_acc:0.990]
Epoch [66/120    avg_loss:0.101, val_acc:0.978]
Epoch [67/120    avg_loss:0.104, val_acc:0.982]
Epoch [68/120    avg_loss:0.116, val_acc:0.990]
Epoch [69/120    avg_loss:0.061, val_acc:0.988]
Epoch [70/120    avg_loss:0.096, val_acc:0.982]
Epoch [71/120    avg_loss:0.059, val_acc:0.992]
Epoch [72/120    avg_loss:0.065, val_acc:0.988]
Epoch [73/120    avg_loss:0.058, val_acc:0.974]
Epoch [74/120    avg_loss:0.055, val_acc:0.994]
Epoch [75/120    avg_loss:0.046, val_acc:0.988]
Epoch [76/120    avg_loss:0.081, val_acc:0.986]
Epoch [77/120    avg_loss:0.060, val_acc:0.990]
Epoch [78/120    avg_loss:0.037, val_acc:0.988]
Epoch [79/120    avg_loss:0.052, val_acc:0.968]
Epoch [80/120    avg_loss:0.076, val_acc:0.992]
Epoch [81/120    avg_loss:0.043, val_acc:0.990]
Epoch [82/120    avg_loss:0.059, val_acc:0.980]
Epoch [83/120    avg_loss:0.070, val_acc:0.990]
Epoch [84/120    avg_loss:0.070, val_acc:0.976]
Epoch [85/120    avg_loss:0.088, val_acc:0.972]
Epoch [86/120    avg_loss:0.072, val_acc:0.994]
Epoch [87/120    avg_loss:0.056, val_acc:0.990]
Epoch [88/120    avg_loss:0.039, val_acc:0.996]
Epoch [89/120    avg_loss:0.035, val_acc:0.992]
Epoch [90/120    avg_loss:0.055, val_acc:0.996]
Epoch [91/120    avg_loss:0.060, val_acc:0.992]
Epoch [92/120    avg_loss:0.032, val_acc:0.996]
Epoch [93/120    avg_loss:0.031, val_acc:0.990]
Epoch [94/120    avg_loss:0.029, val_acc:0.998]
Epoch [95/120    avg_loss:0.022, val_acc:0.996]
Epoch [96/120    avg_loss:0.025, val_acc:0.998]
Epoch [97/120    avg_loss:0.026, val_acc:0.994]
Epoch [98/120    avg_loss:0.018, val_acc:0.998]
Epoch [99/120    avg_loss:0.031, val_acc:0.994]
Epoch [100/120    avg_loss:0.046, val_acc:0.990]
Epoch [101/120    avg_loss:0.047, val_acc:0.996]
Epoch [102/120    avg_loss:0.035, val_acc:0.980]
Epoch [103/120    avg_loss:0.042, val_acc:0.998]
Epoch [104/120    avg_loss:0.030, val_acc:0.998]
Epoch [105/120    avg_loss:0.036, val_acc:0.994]
Epoch [106/120    avg_loss:0.038, val_acc:0.992]
Epoch [107/120    avg_loss:0.023, val_acc:0.990]
Epoch [108/120    avg_loss:0.016, val_acc:0.992]
Epoch [109/120    avg_loss:0.018, val_acc:0.992]
Epoch [110/120    avg_loss:0.013, val_acc:0.998]
Epoch [111/120    avg_loss:0.010, val_acc:0.998]
Epoch [112/120    avg_loss:0.012, val_acc:0.998]
Epoch [113/120    avg_loss:0.014, val_acc:0.998]
Epoch [114/120    avg_loss:0.012, val_acc:0.998]
Epoch [115/120    avg_loss:0.012, val_acc:0.996]
Epoch [116/120    avg_loss:0.008, val_acc:0.996]
Epoch [117/120    avg_loss:0.010, val_acc:0.998]
Epoch [118/120    avg_loss:0.014, val_acc:0.994]
Epoch [119/120    avg_loss:0.025, val_acc:0.992]
Epoch [120/120    avg_loss:0.026, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   1 217   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   1 205  19   0   0   0   5   0   0   0   0   0]
 [  0   0   0   0 210  14   0   0   1   0   2   0   0   0]
 [  0   0   0   0   3 136   6   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8272921108742

F1 scores:
[       nan 0.99927061 0.98861048 0.94252874 0.91503268 0.9220339
 0.98564593 0.98395722 0.99232737 1.         0.99726027 1.
 1.         1.        ]

Kappa:
0.9869432190231857
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcb1167a9b0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.461, val_acc:0.319]
Epoch [2/120    avg_loss:2.128, val_acc:0.429]
Epoch [3/120    avg_loss:1.887, val_acc:0.607]
Epoch [4/120    avg_loss:1.699, val_acc:0.724]
Epoch [5/120    avg_loss:1.477, val_acc:0.746]
Epoch [6/120    avg_loss:1.313, val_acc:0.766]
Epoch [7/120    avg_loss:1.119, val_acc:0.790]
Epoch [8/120    avg_loss:1.060, val_acc:0.808]
Epoch [9/120    avg_loss:0.919, val_acc:0.817]
Epoch [10/120    avg_loss:0.862, val_acc:0.831]
Epoch [11/120    avg_loss:0.796, val_acc:0.841]
Epoch [12/120    avg_loss:0.696, val_acc:0.849]
Epoch [13/120    avg_loss:0.634, val_acc:0.845]
Epoch [14/120    avg_loss:0.650, val_acc:0.915]
Epoch [15/120    avg_loss:0.587, val_acc:0.885]
Epoch [16/120    avg_loss:0.544, val_acc:0.877]
Epoch [17/120    avg_loss:0.505, val_acc:0.919]
Epoch [18/120    avg_loss:0.466, val_acc:0.847]
Epoch [19/120    avg_loss:0.481, val_acc:0.915]
Epoch [20/120    avg_loss:0.437, val_acc:0.901]
Epoch [21/120    avg_loss:0.453, val_acc:0.867]
Epoch [22/120    avg_loss:0.393, val_acc:0.921]
Epoch [23/120    avg_loss:0.370, val_acc:0.925]
Epoch [24/120    avg_loss:0.348, val_acc:0.931]
Epoch [25/120    avg_loss:0.323, val_acc:0.927]
Epoch [26/120    avg_loss:0.328, val_acc:0.929]
Epoch [27/120    avg_loss:0.286, val_acc:0.946]
Epoch [28/120    avg_loss:0.312, val_acc:0.909]
Epoch [29/120    avg_loss:0.362, val_acc:0.962]
Epoch [30/120    avg_loss:0.282, val_acc:0.931]
Epoch [31/120    avg_loss:0.247, val_acc:0.915]
Epoch [32/120    avg_loss:0.255, val_acc:0.954]
Epoch [33/120    avg_loss:0.256, val_acc:0.903]
Epoch [34/120    avg_loss:0.303, val_acc:0.962]
Epoch [35/120    avg_loss:0.232, val_acc:0.956]
Epoch [36/120    avg_loss:0.231, val_acc:0.956]
Epoch [37/120    avg_loss:0.281, val_acc:0.931]
Epoch [38/120    avg_loss:0.232, val_acc:0.935]
Epoch [39/120    avg_loss:0.214, val_acc:0.942]
Epoch [40/120    avg_loss:0.187, val_acc:0.931]
Epoch [41/120    avg_loss:0.169, val_acc:0.962]
Epoch [42/120    avg_loss:0.206, val_acc:0.952]
Epoch [43/120    avg_loss:0.159, val_acc:0.948]
Epoch [44/120    avg_loss:0.185, val_acc:0.942]
Epoch [45/120    avg_loss:0.169, val_acc:0.966]
Epoch [46/120    avg_loss:0.134, val_acc:0.966]
Epoch [47/120    avg_loss:0.144, val_acc:0.966]
Epoch [48/120    avg_loss:0.100, val_acc:0.972]
Epoch [49/120    avg_loss:0.137, val_acc:0.950]
Epoch [50/120    avg_loss:0.107, val_acc:0.966]
Epoch [51/120    avg_loss:0.092, val_acc:0.982]
Epoch [52/120    avg_loss:0.100, val_acc:0.938]
Epoch [53/120    avg_loss:0.114, val_acc:0.950]
Epoch [54/120    avg_loss:0.120, val_acc:0.950]
Epoch [55/120    avg_loss:0.130, val_acc:0.954]
Epoch [56/120    avg_loss:0.104, val_acc:0.972]
Epoch [57/120    avg_loss:0.104, val_acc:0.976]
Epoch [58/120    avg_loss:0.089, val_acc:0.970]
Epoch [59/120    avg_loss:0.067, val_acc:0.980]
Epoch [60/120    avg_loss:0.079, val_acc:0.974]
Epoch [61/120    avg_loss:0.106, val_acc:0.984]
Epoch [62/120    avg_loss:0.127, val_acc:0.976]
Epoch [63/120    avg_loss:0.101, val_acc:0.970]
Epoch [64/120    avg_loss:0.066, val_acc:0.982]
Epoch [65/120    avg_loss:0.087, val_acc:0.974]
Epoch [66/120    avg_loss:0.088, val_acc:0.974]
Epoch [67/120    avg_loss:0.121, val_acc:0.964]
Epoch [68/120    avg_loss:0.141, val_acc:0.958]
Epoch [69/120    avg_loss:0.152, val_acc:0.964]
Epoch [70/120    avg_loss:0.107, val_acc:0.968]
Epoch [71/120    avg_loss:0.079, val_acc:0.982]
Epoch [72/120    avg_loss:0.063, val_acc:0.960]
Epoch [73/120    avg_loss:0.215, val_acc:0.883]
Epoch [74/120    avg_loss:0.133, val_acc:0.970]
Epoch [75/120    avg_loss:0.082, val_acc:0.978]
Epoch [76/120    avg_loss:0.063, val_acc:0.980]
Epoch [77/120    avg_loss:0.059, val_acc:0.980]
Epoch [78/120    avg_loss:0.050, val_acc:0.980]
Epoch [79/120    avg_loss:0.051, val_acc:0.980]
Epoch [80/120    avg_loss:0.062, val_acc:0.980]
Epoch [81/120    avg_loss:0.056, val_acc:0.980]
Epoch [82/120    avg_loss:0.056, val_acc:0.982]
Epoch [83/120    avg_loss:0.042, val_acc:0.984]
Epoch [84/120    avg_loss:0.046, val_acc:0.982]
Epoch [85/120    avg_loss:0.038, val_acc:0.982]
Epoch [86/120    avg_loss:0.038, val_acc:0.982]
Epoch [87/120    avg_loss:0.034, val_acc:0.984]
Epoch [88/120    avg_loss:0.032, val_acc:0.984]
Epoch [89/120    avg_loss:0.035, val_acc:0.984]
Epoch [90/120    avg_loss:0.042, val_acc:0.984]
Epoch [91/120    avg_loss:0.045, val_acc:0.990]
Epoch [92/120    avg_loss:0.041, val_acc:0.986]
Epoch [93/120    avg_loss:0.027, val_acc:0.986]
Epoch [94/120    avg_loss:0.036, val_acc:0.986]
Epoch [95/120    avg_loss:0.028, val_acc:0.986]
Epoch [96/120    avg_loss:0.028, val_acc:0.988]
Epoch [97/120    avg_loss:0.029, val_acc:0.986]
Epoch [98/120    avg_loss:0.033, val_acc:0.990]
Epoch [99/120    avg_loss:0.032, val_acc:0.990]
Epoch [100/120    avg_loss:0.041, val_acc:0.990]
Epoch [101/120    avg_loss:0.025, val_acc:0.988]
Epoch [102/120    avg_loss:0.035, val_acc:0.990]
Epoch [103/120    avg_loss:0.038, val_acc:0.992]
Epoch [104/120    avg_loss:0.031, val_acc:0.992]
Epoch [105/120    avg_loss:0.024, val_acc:0.990]
Epoch [106/120    avg_loss:0.030, val_acc:0.988]
Epoch [107/120    avg_loss:0.023, val_acc:0.988]
Epoch [108/120    avg_loss:0.039, val_acc:0.990]
Epoch [109/120    avg_loss:0.024, val_acc:0.990]
Epoch [110/120    avg_loss:0.039, val_acc:0.990]
Epoch [111/120    avg_loss:0.032, val_acc:0.988]
Epoch [112/120    avg_loss:0.026, val_acc:0.990]
Epoch [113/120    avg_loss:0.026, val_acc:0.992]
Epoch [114/120    avg_loss:0.025, val_acc:0.992]
Epoch [115/120    avg_loss:0.022, val_acc:0.992]
Epoch [116/120    avg_loss:0.027, val_acc:0.992]
Epoch [117/120    avg_loss:0.025, val_acc:0.992]
Epoch [118/120    avg_loss:0.024, val_acc:0.992]
Epoch [119/120    avg_loss:0.021, val_acc:0.992]
Epoch [120/120    avg_loss:0.024, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   1 218   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 200  30   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.67803837953092

F1 scores:
[       nan 0.99927061 0.99771167 0.93023256 0.87474333 0.90344828
 0.99266504 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9852820227738401
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd5645a2978>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.459, val_acc:0.405]
Epoch [2/120    avg_loss:2.147, val_acc:0.587]
Epoch [3/120    avg_loss:1.902, val_acc:0.607]
Epoch [4/120    avg_loss:1.615, val_acc:0.651]
Epoch [5/120    avg_loss:1.373, val_acc:0.722]
Epoch [6/120    avg_loss:1.189, val_acc:0.804]
Epoch [7/120    avg_loss:1.057, val_acc:0.724]
Epoch [8/120    avg_loss:0.958, val_acc:0.833]
Epoch [9/120    avg_loss:0.860, val_acc:0.766]
Epoch [10/120    avg_loss:0.792, val_acc:0.837]
Epoch [11/120    avg_loss:0.785, val_acc:0.851]
Epoch [12/120    avg_loss:0.694, val_acc:0.851]
Epoch [13/120    avg_loss:0.639, val_acc:0.808]
Epoch [14/120    avg_loss:0.617, val_acc:0.827]
Epoch [15/120    avg_loss:0.560, val_acc:0.841]
Epoch [16/120    avg_loss:0.533, val_acc:0.859]
Epoch [17/120    avg_loss:0.512, val_acc:0.915]
Epoch [18/120    avg_loss:0.461, val_acc:0.915]
Epoch [19/120    avg_loss:0.423, val_acc:0.784]
Epoch [20/120    avg_loss:0.517, val_acc:0.897]
Epoch [21/120    avg_loss:0.426, val_acc:0.881]
Epoch [22/120    avg_loss:0.416, val_acc:0.933]
Epoch [23/120    avg_loss:0.387, val_acc:0.923]
Epoch [24/120    avg_loss:0.383, val_acc:0.923]
Epoch [25/120    avg_loss:0.299, val_acc:0.938]
Epoch [26/120    avg_loss:0.361, val_acc:0.921]
Epoch [27/120    avg_loss:0.380, val_acc:0.940]
Epoch [28/120    avg_loss:0.308, val_acc:0.899]
Epoch [29/120    avg_loss:0.318, val_acc:0.917]
Epoch [30/120    avg_loss:0.314, val_acc:0.925]
Epoch [31/120    avg_loss:0.271, val_acc:0.913]
Epoch [32/120    avg_loss:0.333, val_acc:0.944]
Epoch [33/120    avg_loss:0.290, val_acc:0.935]
Epoch [34/120    avg_loss:0.284, val_acc:0.915]
Epoch [35/120    avg_loss:0.271, val_acc:0.952]
Epoch [36/120    avg_loss:0.264, val_acc:0.929]
Epoch [37/120    avg_loss:0.198, val_acc:0.915]
Epoch [38/120    avg_loss:0.255, val_acc:0.893]
Epoch [39/120    avg_loss:0.244, val_acc:0.944]
Epoch [40/120    avg_loss:0.231, val_acc:0.895]
Epoch [41/120    avg_loss:0.258, val_acc:0.935]
Epoch [42/120    avg_loss:0.260, val_acc:0.921]
Epoch [43/120    avg_loss:0.218, val_acc:0.927]
Epoch [44/120    avg_loss:0.215, val_acc:0.907]
Epoch [45/120    avg_loss:0.195, val_acc:0.964]
Epoch [46/120    avg_loss:0.160, val_acc:0.964]
Epoch [47/120    avg_loss:0.147, val_acc:0.952]
Epoch [48/120    avg_loss:0.161, val_acc:0.940]
Epoch [49/120    avg_loss:0.179, val_acc:0.968]
Epoch [50/120    avg_loss:0.127, val_acc:0.952]
Epoch [51/120    avg_loss:0.134, val_acc:0.915]
Epoch [52/120    avg_loss:0.211, val_acc:0.942]
Epoch [53/120    avg_loss:0.162, val_acc:0.956]
Epoch [54/120    avg_loss:0.143, val_acc:0.958]
Epoch [55/120    avg_loss:0.134, val_acc:0.954]
Epoch [56/120    avg_loss:0.144, val_acc:0.942]
Epoch [57/120    avg_loss:0.146, val_acc:0.968]
Epoch [58/120    avg_loss:0.122, val_acc:0.962]
Epoch [59/120    avg_loss:0.095, val_acc:0.956]
Epoch [60/120    avg_loss:0.129, val_acc:0.968]
Epoch [61/120    avg_loss:0.151, val_acc:0.950]
Epoch [62/120    avg_loss:0.152, val_acc:0.952]
Epoch [63/120    avg_loss:0.111, val_acc:0.948]
Epoch [64/120    avg_loss:0.095, val_acc:0.960]
Epoch [65/120    avg_loss:0.096, val_acc:0.940]
Epoch [66/120    avg_loss:0.099, val_acc:0.970]
Epoch [67/120    avg_loss:0.089, val_acc:0.964]
Epoch [68/120    avg_loss:0.090, val_acc:0.966]
Epoch [69/120    avg_loss:0.106, val_acc:0.976]
Epoch [70/120    avg_loss:0.077, val_acc:0.960]
Epoch [71/120    avg_loss:0.094, val_acc:0.956]
Epoch [72/120    avg_loss:0.092, val_acc:0.966]
Epoch [73/120    avg_loss:0.092, val_acc:0.970]
Epoch [74/120    avg_loss:0.085, val_acc:0.952]
Epoch [75/120    avg_loss:0.123, val_acc:0.958]
Epoch [76/120    avg_loss:0.114, val_acc:0.938]
Epoch [77/120    avg_loss:0.101, val_acc:0.940]
Epoch [78/120    avg_loss:0.090, val_acc:0.964]
Epoch [79/120    avg_loss:0.105, val_acc:0.942]
Epoch [80/120    avg_loss:0.095, val_acc:0.970]
Epoch [81/120    avg_loss:0.092, val_acc:0.944]
Epoch [82/120    avg_loss:0.111, val_acc:0.960]
Epoch [83/120    avg_loss:0.082, val_acc:0.970]
Epoch [84/120    avg_loss:0.057, val_acc:0.974]
Epoch [85/120    avg_loss:0.050, val_acc:0.976]
Epoch [86/120    avg_loss:0.053, val_acc:0.974]
Epoch [87/120    avg_loss:0.040, val_acc:0.976]
Epoch [88/120    avg_loss:0.051, val_acc:0.976]
Epoch [89/120    avg_loss:0.043, val_acc:0.978]
Epoch [90/120    avg_loss:0.053, val_acc:0.974]
Epoch [91/120    avg_loss:0.038, val_acc:0.974]
Epoch [92/120    avg_loss:0.030, val_acc:0.976]
Epoch [93/120    avg_loss:0.035, val_acc:0.978]
Epoch [94/120    avg_loss:0.027, val_acc:0.974]
Epoch [95/120    avg_loss:0.043, val_acc:0.974]
Epoch [96/120    avg_loss:0.043, val_acc:0.974]
Epoch [97/120    avg_loss:0.033, val_acc:0.974]
Epoch [98/120    avg_loss:0.034, val_acc:0.974]
Epoch [99/120    avg_loss:0.035, val_acc:0.976]
Epoch [100/120    avg_loss:0.031, val_acc:0.976]
Epoch [101/120    avg_loss:0.027, val_acc:0.978]
Epoch [102/120    avg_loss:0.031, val_acc:0.978]
Epoch [103/120    avg_loss:0.026, val_acc:0.976]
Epoch [104/120    avg_loss:0.034, val_acc:0.980]
Epoch [105/120    avg_loss:0.038, val_acc:0.978]
Epoch [106/120    avg_loss:0.029, val_acc:0.980]
Epoch [107/120    avg_loss:0.032, val_acc:0.976]
Epoch [108/120    avg_loss:0.032, val_acc:0.982]
Epoch [109/120    avg_loss:0.026, val_acc:0.980]
Epoch [110/120    avg_loss:0.036, val_acc:0.982]
Epoch [111/120    avg_loss:0.027, val_acc:0.982]
Epoch [112/120    avg_loss:0.020, val_acc:0.982]
Epoch [113/120    avg_loss:0.034, val_acc:0.980]
Epoch [114/120    avg_loss:0.029, val_acc:0.982]
Epoch [115/120    avg_loss:0.035, val_acc:0.978]
Epoch [116/120    avg_loss:0.025, val_acc:0.982]
Epoch [117/120    avg_loss:0.021, val_acc:0.982]
Epoch [118/120    avg_loss:0.027, val_acc:0.982]
Epoch [119/120    avg_loss:0.028, val_acc:0.982]
Epoch [120/120    avg_loss:0.025, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   2 215   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 226   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   7 216   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0  25 120   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 0.99854227 0.97727273 0.9762419  0.91525424 0.89219331
 1.         0.95652174 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9881290267973677
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0f9d544908>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.521, val_acc:0.352]
Epoch [2/120    avg_loss:2.193, val_acc:0.488]
Epoch [3/120    avg_loss:2.011, val_acc:0.529]
Epoch [4/120    avg_loss:1.806, val_acc:0.666]
Epoch [5/120    avg_loss:1.594, val_acc:0.645]
Epoch [6/120    avg_loss:1.400, val_acc:0.760]
Epoch [7/120    avg_loss:1.218, val_acc:0.809]
Epoch [8/120    avg_loss:1.077, val_acc:0.715]
Epoch [9/120    avg_loss:0.973, val_acc:0.775]
Epoch [10/120    avg_loss:0.818, val_acc:0.865]
Epoch [11/120    avg_loss:0.797, val_acc:0.855]
Epoch [12/120    avg_loss:0.773, val_acc:0.875]
Epoch [13/120    avg_loss:0.655, val_acc:0.889]
Epoch [14/120    avg_loss:0.589, val_acc:0.902]
Epoch [15/120    avg_loss:0.574, val_acc:0.910]
Epoch [16/120    avg_loss:0.538, val_acc:0.887]
Epoch [17/120    avg_loss:0.488, val_acc:0.922]
Epoch [18/120    avg_loss:0.437, val_acc:0.891]
Epoch [19/120    avg_loss:0.509, val_acc:0.896]
Epoch [20/120    avg_loss:0.398, val_acc:0.910]
Epoch [21/120    avg_loss:0.384, val_acc:0.922]
Epoch [22/120    avg_loss:0.387, val_acc:0.914]
Epoch [23/120    avg_loss:0.376, val_acc:0.916]
Epoch [24/120    avg_loss:0.383, val_acc:0.932]
Epoch [25/120    avg_loss:0.359, val_acc:0.930]
Epoch [26/120    avg_loss:0.343, val_acc:0.932]
Epoch [27/120    avg_loss:0.358, val_acc:0.918]
Epoch [28/120    avg_loss:0.354, val_acc:0.934]
Epoch [29/120    avg_loss:0.323, val_acc:0.949]
Epoch [30/120    avg_loss:0.260, val_acc:0.932]
Epoch [31/120    avg_loss:0.252, val_acc:0.955]
Epoch [32/120    avg_loss:0.213, val_acc:0.957]
Epoch [33/120    avg_loss:0.214, val_acc:0.949]
Epoch [34/120    avg_loss:0.247, val_acc:0.961]
Epoch [35/120    avg_loss:0.212, val_acc:0.926]
Epoch [36/120    avg_loss:0.246, val_acc:0.936]
Epoch [37/120    avg_loss:0.233, val_acc:0.963]
Epoch [38/120    avg_loss:0.216, val_acc:0.953]
Epoch [39/120    avg_loss:0.228, val_acc:0.936]
Epoch [40/120    avg_loss:0.201, val_acc:0.953]
Epoch [41/120    avg_loss:0.209, val_acc:0.951]
Epoch [42/120    avg_loss:0.174, val_acc:0.959]
Epoch [43/120    avg_loss:0.199, val_acc:0.943]
Epoch [44/120    avg_loss:0.246, val_acc:0.943]
Epoch [45/120    avg_loss:0.224, val_acc:0.957]
Epoch [46/120    avg_loss:0.187, val_acc:0.959]
Epoch [47/120    avg_loss:0.152, val_acc:0.967]
Epoch [48/120    avg_loss:0.157, val_acc:0.965]
Epoch [49/120    avg_loss:0.174, val_acc:0.965]
Epoch [50/120    avg_loss:0.116, val_acc:0.963]
Epoch [51/120    avg_loss:0.116, val_acc:0.969]
Epoch [52/120    avg_loss:0.135, val_acc:0.973]
Epoch [53/120    avg_loss:0.151, val_acc:0.975]
Epoch [54/120    avg_loss:0.130, val_acc:0.965]
Epoch [55/120    avg_loss:0.120, val_acc:0.949]
Epoch [56/120    avg_loss:0.102, val_acc:0.979]
Epoch [57/120    avg_loss:0.109, val_acc:0.982]
Epoch [58/120    avg_loss:0.094, val_acc:0.984]
Epoch [59/120    avg_loss:0.108, val_acc:0.963]
Epoch [60/120    avg_loss:0.130, val_acc:0.965]
Epoch [61/120    avg_loss:0.136, val_acc:0.977]
Epoch [62/120    avg_loss:0.075, val_acc:0.980]
Epoch [63/120    avg_loss:0.092, val_acc:0.977]
Epoch [64/120    avg_loss:0.090, val_acc:0.982]
Epoch [65/120    avg_loss:0.068, val_acc:0.984]
Epoch [66/120    avg_loss:0.103, val_acc:0.967]
Epoch [67/120    avg_loss:0.078, val_acc:0.955]
Epoch [68/120    avg_loss:0.093, val_acc:0.971]
Epoch [69/120    avg_loss:0.071, val_acc:0.979]
Epoch [70/120    avg_loss:0.112, val_acc:0.953]
Epoch [71/120    avg_loss:0.142, val_acc:0.947]
Epoch [72/120    avg_loss:0.132, val_acc:0.980]
Epoch [73/120    avg_loss:0.073, val_acc:0.975]
Epoch [74/120    avg_loss:0.070, val_acc:0.977]
Epoch [75/120    avg_loss:0.075, val_acc:0.979]
Epoch [76/120    avg_loss:0.061, val_acc:0.982]
Epoch [77/120    avg_loss:0.050, val_acc:0.990]
Epoch [78/120    avg_loss:0.046, val_acc:0.980]
Epoch [79/120    avg_loss:0.059, val_acc:0.975]
Epoch [80/120    avg_loss:0.063, val_acc:0.982]
Epoch [81/120    avg_loss:0.053, val_acc:0.984]
Epoch [82/120    avg_loss:0.066, val_acc:0.949]
Epoch [83/120    avg_loss:0.103, val_acc:0.980]
Epoch [84/120    avg_loss:0.061, val_acc:0.969]
Epoch [85/120    avg_loss:0.045, val_acc:0.988]
Epoch [86/120    avg_loss:0.047, val_acc:0.986]
Epoch [87/120    avg_loss:0.063, val_acc:0.971]
Epoch [88/120    avg_loss:0.044, val_acc:0.979]
Epoch [89/120    avg_loss:0.051, val_acc:0.980]
Epoch [90/120    avg_loss:0.036, val_acc:0.984]
Epoch [91/120    avg_loss:0.026, val_acc:0.986]
Epoch [92/120    avg_loss:0.025, val_acc:0.988]
Epoch [93/120    avg_loss:0.026, val_acc:0.988]
Epoch [94/120    avg_loss:0.022, val_acc:0.986]
Epoch [95/120    avg_loss:0.019, val_acc:0.986]
Epoch [96/120    avg_loss:0.022, val_acc:0.988]
Epoch [97/120    avg_loss:0.019, val_acc:0.992]
Epoch [98/120    avg_loss:0.023, val_acc:0.992]
Epoch [99/120    avg_loss:0.017, val_acc:0.990]
Epoch [100/120    avg_loss:0.024, val_acc:0.992]
Epoch [101/120    avg_loss:0.018, val_acc:0.990]
Epoch [102/120    avg_loss:0.015, val_acc:0.990]
Epoch [103/120    avg_loss:0.018, val_acc:0.990]
Epoch [104/120    avg_loss:0.019, val_acc:0.990]
Epoch [105/120    avg_loss:0.015, val_acc:0.990]
Epoch [106/120    avg_loss:0.017, val_acc:0.990]
Epoch [107/120    avg_loss:0.021, val_acc:0.990]
Epoch [108/120    avg_loss:0.022, val_acc:0.988]
Epoch [109/120    avg_loss:0.021, val_acc:0.990]
Epoch [110/120    avg_loss:0.024, val_acc:0.990]
Epoch [111/120    avg_loss:0.016, val_acc:0.990]
Epoch [112/120    avg_loss:0.017, val_acc:0.990]
Epoch [113/120    avg_loss:0.018, val_acc:0.990]
Epoch [114/120    avg_loss:0.014, val_acc:0.990]
Epoch [115/120    avg_loss:0.016, val_acc:0.990]
Epoch [116/120    avg_loss:0.012, val_acc:0.990]
Epoch [117/120    avg_loss:0.016, val_acc:0.990]
Epoch [118/120    avg_loss:0.016, val_acc:0.990]
Epoch [119/120    avg_loss:0.015, val_acc:0.990]
Epoch [120/120    avg_loss:0.015, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   2 213   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 210  19   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 0.99854227 0.98611111 0.95454545 0.89727463 0.8951049
 1.         0.97916667 0.998713   1.         1.         1.
 1.         1.        ]

Kappa:
0.9867060591953916
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb7b47dc898>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.506, val_acc:0.488]
Epoch [2/120    avg_loss:2.143, val_acc:0.542]
Epoch [3/120    avg_loss:1.907, val_acc:0.581]
Epoch [4/120    avg_loss:1.686, val_acc:0.619]
Epoch [5/120    avg_loss:1.459, val_acc:0.639]
Epoch [6/120    avg_loss:1.296, val_acc:0.677]
Epoch [7/120    avg_loss:1.148, val_acc:0.774]
Epoch [8/120    avg_loss:1.057, val_acc:0.752]
Epoch [9/120    avg_loss:0.926, val_acc:0.825]
Epoch [10/120    avg_loss:0.840, val_acc:0.859]
Epoch [11/120    avg_loss:0.742, val_acc:0.865]
Epoch [12/120    avg_loss:0.645, val_acc:0.905]
Epoch [13/120    avg_loss:0.609, val_acc:0.871]
Epoch [14/120    avg_loss:0.611, val_acc:0.905]
Epoch [15/120    avg_loss:0.549, val_acc:0.881]
Epoch [16/120    avg_loss:0.536, val_acc:0.881]
Epoch [17/120    avg_loss:0.549, val_acc:0.909]
Epoch [18/120    avg_loss:0.444, val_acc:0.923]
Epoch [19/120    avg_loss:0.416, val_acc:0.891]
Epoch [20/120    avg_loss:0.411, val_acc:0.923]
Epoch [21/120    avg_loss:0.406, val_acc:0.899]
Epoch [22/120    avg_loss:0.396, val_acc:0.917]
Epoch [23/120    avg_loss:0.392, val_acc:0.891]
Epoch [24/120    avg_loss:0.379, val_acc:0.921]
Epoch [25/120    avg_loss:0.337, val_acc:0.919]
Epoch [26/120    avg_loss:0.364, val_acc:0.923]
Epoch [27/120    avg_loss:0.413, val_acc:0.897]
Epoch [28/120    avg_loss:0.366, val_acc:0.925]
Epoch [29/120    avg_loss:0.337, val_acc:0.942]
Epoch [30/120    avg_loss:0.266, val_acc:0.944]
Epoch [31/120    avg_loss:0.235, val_acc:0.956]
Epoch [32/120    avg_loss:0.233, val_acc:0.958]
Epoch [33/120    avg_loss:0.227, val_acc:0.948]
Epoch [34/120    avg_loss:0.321, val_acc:0.909]
Epoch [35/120    avg_loss:0.294, val_acc:0.942]
Epoch [36/120    avg_loss:0.246, val_acc:0.968]
Epoch [37/120    avg_loss:0.190, val_acc:0.974]
Epoch [38/120    avg_loss:0.198, val_acc:0.938]
Epoch [39/120    avg_loss:0.228, val_acc:0.907]
Epoch [40/120    avg_loss:0.238, val_acc:0.954]
Epoch [41/120    avg_loss:0.189, val_acc:0.964]
Epoch [42/120    avg_loss:0.221, val_acc:0.907]
Epoch [43/120    avg_loss:0.212, val_acc:0.968]
Epoch [44/120    avg_loss:0.158, val_acc:0.950]
Epoch [45/120    avg_loss:0.147, val_acc:0.968]
Epoch [46/120    avg_loss:0.132, val_acc:0.942]
Epoch [47/120    avg_loss:0.146, val_acc:0.968]
Epoch [48/120    avg_loss:0.141, val_acc:0.982]
Epoch [49/120    avg_loss:0.181, val_acc:0.921]
Epoch [50/120    avg_loss:0.235, val_acc:0.960]
Epoch [51/120    avg_loss:0.154, val_acc:0.978]
Epoch [52/120    avg_loss:0.131, val_acc:0.974]
Epoch [53/120    avg_loss:0.155, val_acc:0.958]
Epoch [54/120    avg_loss:0.133, val_acc:0.946]
Epoch [55/120    avg_loss:0.159, val_acc:0.968]
Epoch [56/120    avg_loss:0.139, val_acc:0.960]
Epoch [57/120    avg_loss:0.131, val_acc:0.958]
Epoch [58/120    avg_loss:0.205, val_acc:0.978]
Epoch [59/120    avg_loss:0.129, val_acc:0.940]
Epoch [60/120    avg_loss:0.128, val_acc:0.962]
Epoch [61/120    avg_loss:0.084, val_acc:0.974]
Epoch [62/120    avg_loss:0.101, val_acc:0.980]
Epoch [63/120    avg_loss:0.064, val_acc:0.986]
Epoch [64/120    avg_loss:0.069, val_acc:0.984]
Epoch [65/120    avg_loss:0.058, val_acc:0.982]
Epoch [66/120    avg_loss:0.056, val_acc:0.984]
Epoch [67/120    avg_loss:0.047, val_acc:0.986]
Epoch [68/120    avg_loss:0.052, val_acc:0.986]
Epoch [69/120    avg_loss:0.054, val_acc:0.986]
Epoch [70/120    avg_loss:0.054, val_acc:0.990]
Epoch [71/120    avg_loss:0.051, val_acc:0.986]
Epoch [72/120    avg_loss:0.046, val_acc:0.986]
Epoch [73/120    avg_loss:0.057, val_acc:0.988]
Epoch [74/120    avg_loss:0.050, val_acc:0.988]
Epoch [75/120    avg_loss:0.048, val_acc:0.990]
Epoch [76/120    avg_loss:0.050, val_acc:0.988]
Epoch [77/120    avg_loss:0.055, val_acc:0.988]
Epoch [78/120    avg_loss:0.051, val_acc:0.990]
Epoch [79/120    avg_loss:0.045, val_acc:0.988]
Epoch [80/120    avg_loss:0.038, val_acc:0.990]
Epoch [81/120    avg_loss:0.048, val_acc:0.990]
Epoch [82/120    avg_loss:0.042, val_acc:0.990]
Epoch [83/120    avg_loss:0.046, val_acc:0.990]
Epoch [84/120    avg_loss:0.048, val_acc:0.990]
Epoch [85/120    avg_loss:0.045, val_acc:0.990]
Epoch [86/120    avg_loss:0.043, val_acc:0.990]
Epoch [87/120    avg_loss:0.047, val_acc:0.988]
Epoch [88/120    avg_loss:0.038, val_acc:0.988]
Epoch [89/120    avg_loss:0.047, val_acc:0.988]
Epoch [90/120    avg_loss:0.038, val_acc:0.990]
Epoch [91/120    avg_loss:0.043, val_acc:0.990]
Epoch [92/120    avg_loss:0.035, val_acc:0.988]
Epoch [93/120    avg_loss:0.039, val_acc:0.990]
Epoch [94/120    avg_loss:0.043, val_acc:0.990]
Epoch [95/120    avg_loss:0.055, val_acc:0.988]
Epoch [96/120    avg_loss:0.046, val_acc:0.988]
Epoch [97/120    avg_loss:0.035, val_acc:0.990]
Epoch [98/120    avg_loss:0.043, val_acc:0.990]
Epoch [99/120    avg_loss:0.044, val_acc:0.990]
Epoch [100/120    avg_loss:0.039, val_acc:0.990]
Epoch [101/120    avg_loss:0.040, val_acc:0.988]
Epoch [102/120    avg_loss:0.039, val_acc:0.990]
Epoch [103/120    avg_loss:0.045, val_acc:0.990]
Epoch [104/120    avg_loss:0.034, val_acc:0.990]
Epoch [105/120    avg_loss:0.030, val_acc:0.990]
Epoch [106/120    avg_loss:0.039, val_acc:0.990]
Epoch [107/120    avg_loss:0.041, val_acc:0.990]
Epoch [108/120    avg_loss:0.042, val_acc:0.990]
Epoch [109/120    avg_loss:0.036, val_acc:0.988]
Epoch [110/120    avg_loss:0.034, val_acc:0.990]
Epoch [111/120    avg_loss:0.043, val_acc:0.990]
Epoch [112/120    avg_loss:0.039, val_acc:0.990]
Epoch [113/120    avg_loss:0.036, val_acc:0.990]
Epoch [114/120    avg_loss:0.035, val_acc:0.990]
Epoch [115/120    avg_loss:0.033, val_acc:0.990]
Epoch [116/120    avg_loss:0.033, val_acc:0.990]
Epoch [117/120    avg_loss:0.032, val_acc:0.990]
Epoch [118/120    avg_loss:0.040, val_acc:0.990]
Epoch [119/120    avg_loss:0.030, val_acc:0.990]
Epoch [120/120    avg_loss:0.035, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   2 227   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 1.         0.96674058 0.99343545 0.94954128 0.92857143
 1.         0.92655367 1.         0.99893276 1.         0.99603699
 0.99667774 1.        ]

Kappa:
0.9902674160184394
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd82b226860>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.498, val_acc:0.432]
Epoch [2/120    avg_loss:2.174, val_acc:0.502]
Epoch [3/120    avg_loss:1.957, val_acc:0.557]
Epoch [4/120    avg_loss:1.703, val_acc:0.633]
Epoch [5/120    avg_loss:1.480, val_acc:0.789]
Epoch [6/120    avg_loss:1.302, val_acc:0.738]
Epoch [7/120    avg_loss:1.173, val_acc:0.771]
Epoch [8/120    avg_loss:1.033, val_acc:0.820]
Epoch [9/120    avg_loss:0.943, val_acc:0.822]
Epoch [10/120    avg_loss:0.867, val_acc:0.826]
Epoch [11/120    avg_loss:0.773, val_acc:0.846]
Epoch [12/120    avg_loss:0.785, val_acc:0.854]
Epoch [13/120    avg_loss:0.683, val_acc:0.797]
Epoch [14/120    avg_loss:0.624, val_acc:0.881]
Epoch [15/120    avg_loss:0.563, val_acc:0.889]
Epoch [16/120    avg_loss:0.509, val_acc:0.838]
Epoch [17/120    avg_loss:0.617, val_acc:0.857]
Epoch [18/120    avg_loss:0.524, val_acc:0.893]
Epoch [19/120    avg_loss:0.468, val_acc:0.906]
Epoch [20/120    avg_loss:0.433, val_acc:0.885]
Epoch [21/120    avg_loss:0.452, val_acc:0.861]
Epoch [22/120    avg_loss:0.394, val_acc:0.912]
Epoch [23/120    avg_loss:0.427, val_acc:0.916]
Epoch [24/120    avg_loss:0.423, val_acc:0.918]
Epoch [25/120    avg_loss:0.416, val_acc:0.936]
Epoch [26/120    avg_loss:0.403, val_acc:0.896]
Epoch [27/120    avg_loss:0.364, val_acc:0.922]
Epoch [28/120    avg_loss:0.347, val_acc:0.928]
Epoch [29/120    avg_loss:0.347, val_acc:0.930]
Epoch [30/120    avg_loss:0.312, val_acc:0.926]
Epoch [31/120    avg_loss:0.278, val_acc:0.934]
Epoch [32/120    avg_loss:0.380, val_acc:0.902]
Epoch [33/120    avg_loss:0.278, val_acc:0.898]
Epoch [34/120    avg_loss:0.238, val_acc:0.930]
Epoch [35/120    avg_loss:0.286, val_acc:0.930]
Epoch [36/120    avg_loss:0.304, val_acc:0.934]
Epoch [37/120    avg_loss:0.272, val_acc:0.932]
Epoch [38/120    avg_loss:0.232, val_acc:0.951]
Epoch [39/120    avg_loss:0.180, val_acc:0.922]
Epoch [40/120    avg_loss:0.238, val_acc:0.932]
Epoch [41/120    avg_loss:0.239, val_acc:0.945]
Epoch [42/120    avg_loss:0.233, val_acc:0.934]
Epoch [43/120    avg_loss:0.276, val_acc:0.879]
Epoch [44/120    avg_loss:0.250, val_acc:0.936]
Epoch [45/120    avg_loss:0.154, val_acc:0.959]
Epoch [46/120    avg_loss:0.171, val_acc:0.961]
Epoch [47/120    avg_loss:0.197, val_acc:0.932]
Epoch [48/120    avg_loss:0.197, val_acc:0.959]
Epoch [49/120    avg_loss:0.133, val_acc:0.951]
Epoch [50/120    avg_loss:0.176, val_acc:0.957]
Epoch [51/120    avg_loss:0.162, val_acc:0.949]
Epoch [52/120    avg_loss:0.213, val_acc:0.941]
Epoch [53/120    avg_loss:0.110, val_acc:0.945]
Epoch [54/120    avg_loss:0.136, val_acc:0.939]
Epoch [55/120    avg_loss:0.152, val_acc:0.957]
Epoch [56/120    avg_loss:0.145, val_acc:0.945]
Epoch [57/120    avg_loss:0.161, val_acc:0.951]
Epoch [58/120    avg_loss:0.095, val_acc:0.961]
Epoch [59/120    avg_loss:0.078, val_acc:0.963]
Epoch [60/120    avg_loss:0.075, val_acc:0.975]
Epoch [61/120    avg_loss:0.073, val_acc:0.969]
Epoch [62/120    avg_loss:0.091, val_acc:0.979]
Epoch [63/120    avg_loss:0.095, val_acc:0.949]
Epoch [64/120    avg_loss:0.143, val_acc:0.969]
Epoch [65/120    avg_loss:0.111, val_acc:0.969]
Epoch [66/120    avg_loss:0.066, val_acc:0.973]
Epoch [67/120    avg_loss:0.061, val_acc:0.918]
Epoch [68/120    avg_loss:0.077, val_acc:0.949]
Epoch [69/120    avg_loss:0.054, val_acc:0.969]
Epoch [70/120    avg_loss:0.050, val_acc:0.975]
Epoch [71/120    avg_loss:0.078, val_acc:0.961]
Epoch [72/120    avg_loss:0.107, val_acc:0.977]
Epoch [73/120    avg_loss:0.080, val_acc:0.979]
Epoch [74/120    avg_loss:0.054, val_acc:0.959]
Epoch [75/120    avg_loss:0.059, val_acc:0.979]
Epoch [76/120    avg_loss:0.059, val_acc:0.988]
Epoch [77/120    avg_loss:0.039, val_acc:0.980]
Epoch [78/120    avg_loss:0.036, val_acc:0.990]
Epoch [79/120    avg_loss:0.029, val_acc:0.988]
Epoch [80/120    avg_loss:0.041, val_acc:0.979]
Epoch [81/120    avg_loss:0.052, val_acc:0.975]
Epoch [82/120    avg_loss:0.087, val_acc:0.949]
Epoch [83/120    avg_loss:0.057, val_acc:0.980]
Epoch [84/120    avg_loss:0.062, val_acc:0.969]
Epoch [85/120    avg_loss:0.085, val_acc:0.967]
Epoch [86/120    avg_loss:0.041, val_acc:0.975]
Epoch [87/120    avg_loss:0.034, val_acc:0.969]
Epoch [88/120    avg_loss:0.031, val_acc:0.990]
Epoch [89/120    avg_loss:0.032, val_acc:0.984]
Epoch [90/120    avg_loss:0.033, val_acc:0.973]
Epoch [91/120    avg_loss:0.047, val_acc:0.984]
Epoch [92/120    avg_loss:0.038, val_acc:0.988]
Epoch [93/120    avg_loss:0.024, val_acc:0.990]
Epoch [94/120    avg_loss:0.027, val_acc:0.980]
Epoch [95/120    avg_loss:0.025, val_acc:0.986]
Epoch [96/120    avg_loss:0.024, val_acc:0.984]
Epoch [97/120    avg_loss:0.013, val_acc:0.990]
Epoch [98/120    avg_loss:0.016, val_acc:0.994]
Epoch [99/120    avg_loss:0.030, val_acc:0.986]
Epoch [100/120    avg_loss:0.043, val_acc:0.969]
Epoch [101/120    avg_loss:0.042, val_acc:0.963]
Epoch [102/120    avg_loss:0.063, val_acc:0.979]
Epoch [103/120    avg_loss:0.058, val_acc:0.982]
Epoch [104/120    avg_loss:0.065, val_acc:0.977]
Epoch [105/120    avg_loss:0.037, val_acc:0.988]
Epoch [106/120    avg_loss:0.023, val_acc:0.986]
Epoch [107/120    avg_loss:0.030, val_acc:0.988]
Epoch [108/120    avg_loss:0.030, val_acc:0.980]
Epoch [109/120    avg_loss:0.013, val_acc:0.994]
Epoch [110/120    avg_loss:0.011, val_acc:0.984]
Epoch [111/120    avg_loss:0.049, val_acc:0.984]
Epoch [112/120    avg_loss:0.037, val_acc:0.986]
Epoch [113/120    avg_loss:0.021, val_acc:0.992]
Epoch [114/120    avg_loss:0.022, val_acc:0.988]
Epoch [115/120    avg_loss:0.011, val_acc:0.992]
Epoch [116/120    avg_loss:0.010, val_acc:0.990]
Epoch [117/120    avg_loss:0.012, val_acc:0.990]
Epoch [118/120    avg_loss:0.015, val_acc:0.996]
Epoch [119/120    avg_loss:0.019, val_acc:0.977]
Epoch [120/120    avg_loss:0.054, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   2 216   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   8 214   5   0   0   0   0   0   0   0   0]
 [  0   0   0   8  18 119   0   0   0   0   0   0   0   0]
 [  0  12   0   0   0   0 194   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 362   0   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0 372   5   0]
 [  0   0   0   0   1   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.65671641791045

F1 scores:
[       nan 0.98988439 0.99082569 0.96421053 0.93043478 0.88475836
 0.97       0.99470899 1.         1.         0.99724518 0.99332443
 0.99122807 1.        ]

Kappa:
0.9850384209824867
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f00c58f9828>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.443, val_acc:0.375]
Epoch [2/120    avg_loss:2.082, val_acc:0.438]
Epoch [3/120    avg_loss:1.825, val_acc:0.635]
Epoch [4/120    avg_loss:1.578, val_acc:0.647]
Epoch [5/120    avg_loss:1.348, val_acc:0.706]
Epoch [6/120    avg_loss:1.212, val_acc:0.756]
Epoch [7/120    avg_loss:1.097, val_acc:0.794]
Epoch [8/120    avg_loss:0.997, val_acc:0.782]
Epoch [9/120    avg_loss:0.895, val_acc:0.804]
Epoch [10/120    avg_loss:0.909, val_acc:0.827]
Epoch [11/120    avg_loss:0.816, val_acc:0.827]
Epoch [12/120    avg_loss:0.739, val_acc:0.873]
Epoch [13/120    avg_loss:0.673, val_acc:0.736]
Epoch [14/120    avg_loss:0.661, val_acc:0.835]
Epoch [15/120    avg_loss:0.593, val_acc:0.782]
Epoch [16/120    avg_loss:0.597, val_acc:0.885]
Epoch [17/120    avg_loss:0.609, val_acc:0.877]
Epoch [18/120    avg_loss:0.625, val_acc:0.821]
Epoch [19/120    avg_loss:0.536, val_acc:0.877]
Epoch [20/120    avg_loss:0.507, val_acc:0.895]
Epoch [21/120    avg_loss:0.475, val_acc:0.869]
Epoch [22/120    avg_loss:0.464, val_acc:0.905]
Epoch [23/120    avg_loss:0.455, val_acc:0.917]
Epoch [24/120    avg_loss:0.433, val_acc:0.915]
Epoch [25/120    avg_loss:0.365, val_acc:0.899]
Epoch [26/120    avg_loss:0.406, val_acc:0.899]
Epoch [27/120    avg_loss:0.376, val_acc:0.907]
Epoch [28/120    avg_loss:0.368, val_acc:0.919]
Epoch [29/120    avg_loss:0.382, val_acc:0.919]
Epoch [30/120    avg_loss:0.361, val_acc:0.925]
Epoch [31/120    avg_loss:0.299, val_acc:0.917]
Epoch [32/120    avg_loss:0.322, val_acc:0.907]
Epoch [33/120    avg_loss:0.280, val_acc:0.927]
Epoch [34/120    avg_loss:0.225, val_acc:0.925]
Epoch [35/120    avg_loss:0.281, val_acc:0.915]
Epoch [36/120    avg_loss:0.298, val_acc:0.929]
Epoch [37/120    avg_loss:0.265, val_acc:0.944]
Epoch [38/120    avg_loss:0.226, val_acc:0.929]
Epoch [39/120    avg_loss:0.385, val_acc:0.938]
Epoch [40/120    avg_loss:0.247, val_acc:0.933]
Epoch [41/120    avg_loss:0.226, val_acc:0.938]
Epoch [42/120    avg_loss:0.201, val_acc:0.952]
Epoch [43/120    avg_loss:0.244, val_acc:0.917]
Epoch [44/120    avg_loss:0.291, val_acc:0.913]
Epoch [45/120    avg_loss:0.245, val_acc:0.946]
Epoch [46/120    avg_loss:0.215, val_acc:0.935]
Epoch [47/120    avg_loss:0.164, val_acc:0.940]
Epoch [48/120    avg_loss:0.220, val_acc:0.956]
Epoch [49/120    avg_loss:0.189, val_acc:0.942]
Epoch [50/120    avg_loss:0.168, val_acc:0.938]
Epoch [51/120    avg_loss:0.179, val_acc:0.948]
Epoch [52/120    avg_loss:0.144, val_acc:0.958]
Epoch [53/120    avg_loss:0.139, val_acc:0.954]
Epoch [54/120    avg_loss:0.130, val_acc:0.950]
Epoch [55/120    avg_loss:0.156, val_acc:0.948]
Epoch [56/120    avg_loss:0.194, val_acc:0.964]
Epoch [57/120    avg_loss:0.160, val_acc:0.938]
Epoch [58/120    avg_loss:0.194, val_acc:0.948]
Epoch [59/120    avg_loss:0.182, val_acc:0.942]
Epoch [60/120    avg_loss:0.137, val_acc:0.942]
Epoch [61/120    avg_loss:0.093, val_acc:0.968]
Epoch [62/120    avg_loss:0.148, val_acc:0.952]
Epoch [63/120    avg_loss:0.161, val_acc:0.958]
Epoch [64/120    avg_loss:0.109, val_acc:0.956]
Epoch [65/120    avg_loss:0.127, val_acc:0.960]
Epoch [66/120    avg_loss:0.134, val_acc:0.950]
Epoch [67/120    avg_loss:0.125, val_acc:0.958]
Epoch [68/120    avg_loss:0.109, val_acc:0.964]
Epoch [69/120    avg_loss:0.113, val_acc:0.964]
Epoch [70/120    avg_loss:0.106, val_acc:0.956]
Epoch [71/120    avg_loss:0.094, val_acc:0.962]
Epoch [72/120    avg_loss:0.157, val_acc:0.940]
Epoch [73/120    avg_loss:0.120, val_acc:0.954]
Epoch [74/120    avg_loss:0.118, val_acc:0.952]
Epoch [75/120    avg_loss:0.092, val_acc:0.956]
Epoch [76/120    avg_loss:0.061, val_acc:0.962]
Epoch [77/120    avg_loss:0.059, val_acc:0.968]
Epoch [78/120    avg_loss:0.073, val_acc:0.972]
Epoch [79/120    avg_loss:0.050, val_acc:0.974]
Epoch [80/120    avg_loss:0.052, val_acc:0.972]
Epoch [81/120    avg_loss:0.048, val_acc:0.972]
Epoch [82/120    avg_loss:0.044, val_acc:0.970]
Epoch [83/120    avg_loss:0.052, val_acc:0.970]
Epoch [84/120    avg_loss:0.045, val_acc:0.974]
Epoch [85/120    avg_loss:0.052, val_acc:0.976]
Epoch [86/120    avg_loss:0.050, val_acc:0.974]
Epoch [87/120    avg_loss:0.044, val_acc:0.972]
Epoch [88/120    avg_loss:0.047, val_acc:0.972]
Epoch [89/120    avg_loss:0.051, val_acc:0.974]
Epoch [90/120    avg_loss:0.039, val_acc:0.976]
Epoch [91/120    avg_loss:0.043, val_acc:0.974]
Epoch [92/120    avg_loss:0.042, val_acc:0.976]
Epoch [93/120    avg_loss:0.050, val_acc:0.976]
Epoch [94/120    avg_loss:0.040, val_acc:0.978]
Epoch [95/120    avg_loss:0.041, val_acc:0.976]
Epoch [96/120    avg_loss:0.040, val_acc:0.980]
Epoch [97/120    avg_loss:0.039, val_acc:0.978]
Epoch [98/120    avg_loss:0.039, val_acc:0.980]
Epoch [99/120    avg_loss:0.036, val_acc:0.976]
Epoch [100/120    avg_loss:0.036, val_acc:0.978]
Epoch [101/120    avg_loss:0.040, val_acc:0.976]
Epoch [102/120    avg_loss:0.045, val_acc:0.980]
Epoch [103/120    avg_loss:0.050, val_acc:0.976]
Epoch [104/120    avg_loss:0.039, val_acc:0.976]
Epoch [105/120    avg_loss:0.038, val_acc:0.980]
Epoch [106/120    avg_loss:0.050, val_acc:0.976]
Epoch [107/120    avg_loss:0.038, val_acc:0.982]
Epoch [108/120    avg_loss:0.042, val_acc:0.978]
Epoch [109/120    avg_loss:0.035, val_acc:0.978]
Epoch [110/120    avg_loss:0.036, val_acc:0.980]
Epoch [111/120    avg_loss:0.041, val_acc:0.978]
Epoch [112/120    avg_loss:0.036, val_acc:0.982]
Epoch [113/120    avg_loss:0.040, val_acc:0.980]
Epoch [114/120    avg_loss:0.043, val_acc:0.980]
Epoch [115/120    avg_loss:0.047, val_acc:0.978]
Epoch [116/120    avg_loss:0.043, val_acc:0.980]
Epoch [117/120    avg_loss:0.043, val_acc:0.978]
Epoch [118/120    avg_loss:0.036, val_acc:0.978]
Epoch [119/120    avg_loss:0.036, val_acc:0.980]
Epoch [120/120    avg_loss:0.036, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   0   0   0   0   1   0]
 [  0   0   2 214  13   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   4  16 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.63539445628997

F1 scores:
[       nan 1.         0.9753915  0.95535714 0.89655172 0.8650519
 1.         0.95555556 0.998713   1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9848064910541484
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff6115c98d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.466, val_acc:0.369]
Epoch [2/120    avg_loss:2.139, val_acc:0.516]
Epoch [3/120    avg_loss:1.943, val_acc:0.565]
Epoch [4/120    avg_loss:1.737, val_acc:0.657]
Epoch [5/120    avg_loss:1.509, val_acc:0.706]
Epoch [6/120    avg_loss:1.299, val_acc:0.782]
Epoch [7/120    avg_loss:1.167, val_acc:0.776]
Epoch [8/120    avg_loss:0.977, val_acc:0.776]
Epoch [9/120    avg_loss:0.869, val_acc:0.883]
Epoch [10/120    avg_loss:0.824, val_acc:0.835]
Epoch [11/120    avg_loss:0.674, val_acc:0.851]
Epoch [12/120    avg_loss:0.673, val_acc:0.875]
Epoch [13/120    avg_loss:0.575, val_acc:0.885]
Epoch [14/120    avg_loss:0.560, val_acc:0.893]
Epoch [15/120    avg_loss:0.508, val_acc:0.877]
Epoch [16/120    avg_loss:0.473, val_acc:0.899]
Epoch [17/120    avg_loss:0.523, val_acc:0.903]
Epoch [18/120    avg_loss:0.474, val_acc:0.919]
Epoch [19/120    avg_loss:0.412, val_acc:0.905]
Epoch [20/120    avg_loss:0.420, val_acc:0.897]
Epoch [21/120    avg_loss:0.433, val_acc:0.885]
Epoch [22/120    avg_loss:0.386, val_acc:0.929]
Epoch [23/120    avg_loss:0.316, val_acc:0.927]
Epoch [24/120    avg_loss:0.330, val_acc:0.911]
Epoch [25/120    avg_loss:0.298, val_acc:0.921]
Epoch [26/120    avg_loss:0.289, val_acc:0.938]
Epoch [27/120    avg_loss:0.284, val_acc:0.921]
Epoch [28/120    avg_loss:0.292, val_acc:0.950]
Epoch [29/120    avg_loss:0.276, val_acc:0.944]
Epoch [30/120    avg_loss:0.333, val_acc:0.901]
Epoch [31/120    avg_loss:0.318, val_acc:0.962]
Epoch [32/120    avg_loss:0.256, val_acc:0.944]
Epoch [33/120    avg_loss:0.227, val_acc:0.950]
Epoch [34/120    avg_loss:0.219, val_acc:0.954]
Epoch [35/120    avg_loss:0.218, val_acc:0.968]
Epoch [36/120    avg_loss:0.253, val_acc:0.944]
Epoch [37/120    avg_loss:0.187, val_acc:0.958]
Epoch [38/120    avg_loss:0.187, val_acc:0.954]
Epoch [39/120    avg_loss:0.144, val_acc:0.970]
Epoch [40/120    avg_loss:0.170, val_acc:0.964]
Epoch [41/120    avg_loss:0.199, val_acc:0.970]
Epoch [42/120    avg_loss:0.142, val_acc:0.978]
Epoch [43/120    avg_loss:0.157, val_acc:0.960]
Epoch [44/120    avg_loss:0.179, val_acc:0.964]
Epoch [45/120    avg_loss:0.173, val_acc:0.964]
Epoch [46/120    avg_loss:0.132, val_acc:0.968]
Epoch [47/120    avg_loss:0.118, val_acc:0.966]
Epoch [48/120    avg_loss:0.121, val_acc:0.976]
Epoch [49/120    avg_loss:0.152, val_acc:0.958]
Epoch [50/120    avg_loss:0.139, val_acc:0.954]
Epoch [51/120    avg_loss:0.128, val_acc:0.978]
Epoch [52/120    avg_loss:0.154, val_acc:0.962]
Epoch [53/120    avg_loss:0.122, val_acc:0.986]
Epoch [54/120    avg_loss:0.092, val_acc:0.974]
Epoch [55/120    avg_loss:0.105, val_acc:0.980]
Epoch [56/120    avg_loss:0.105, val_acc:0.970]
Epoch [57/120    avg_loss:0.138, val_acc:0.982]
Epoch [58/120    avg_loss:0.083, val_acc:0.966]
Epoch [59/120    avg_loss:0.145, val_acc:0.966]
Epoch [60/120    avg_loss:0.112, val_acc:0.968]
Epoch [61/120    avg_loss:0.083, val_acc:0.990]
Epoch [62/120    avg_loss:0.079, val_acc:0.984]
Epoch [63/120    avg_loss:0.083, val_acc:0.986]
Epoch [64/120    avg_loss:0.115, val_acc:0.944]
Epoch [65/120    avg_loss:0.102, val_acc:0.984]
Epoch [66/120    avg_loss:0.100, val_acc:0.982]
Epoch [67/120    avg_loss:0.117, val_acc:0.978]
Epoch [68/120    avg_loss:0.104, val_acc:0.982]
Epoch [69/120    avg_loss:0.073, val_acc:0.988]
Epoch [70/120    avg_loss:0.103, val_acc:0.867]
Epoch [71/120    avg_loss:0.165, val_acc:0.978]
Epoch [72/120    avg_loss:0.119, val_acc:0.946]
Epoch [73/120    avg_loss:0.119, val_acc:0.988]
Epoch [74/120    avg_loss:0.062, val_acc:0.982]
Epoch [75/120    avg_loss:0.065, val_acc:0.984]
Epoch [76/120    avg_loss:0.036, val_acc:0.984]
Epoch [77/120    avg_loss:0.040, val_acc:0.984]
Epoch [78/120    avg_loss:0.045, val_acc:0.984]
Epoch [79/120    avg_loss:0.045, val_acc:0.982]
Epoch [80/120    avg_loss:0.034, val_acc:0.984]
Epoch [81/120    avg_loss:0.037, val_acc:0.986]
Epoch [82/120    avg_loss:0.034, val_acc:0.986]
Epoch [83/120    avg_loss:0.034, val_acc:0.986]
Epoch [84/120    avg_loss:0.030, val_acc:0.988]
Epoch [85/120    avg_loss:0.033, val_acc:0.990]
Epoch [86/120    avg_loss:0.036, val_acc:0.988]
Epoch [87/120    avg_loss:0.035, val_acc:0.990]
Epoch [88/120    avg_loss:0.031, val_acc:0.990]
Epoch [89/120    avg_loss:0.027, val_acc:0.992]
Epoch [90/120    avg_loss:0.037, val_acc:0.992]
Epoch [91/120    avg_loss:0.045, val_acc:0.990]
Epoch [92/120    avg_loss:0.027, val_acc:0.990]
Epoch [93/120    avg_loss:0.029, val_acc:0.988]
Epoch [94/120    avg_loss:0.027, val_acc:0.990]
Epoch [95/120    avg_loss:0.037, val_acc:0.992]
Epoch [96/120    avg_loss:0.031, val_acc:0.990]
Epoch [97/120    avg_loss:0.029, val_acc:0.990]
Epoch [98/120    avg_loss:0.031, val_acc:0.994]
Epoch [99/120    avg_loss:0.026, val_acc:0.994]
Epoch [100/120    avg_loss:0.032, val_acc:0.994]
Epoch [101/120    avg_loss:0.026, val_acc:0.996]
Epoch [102/120    avg_loss:0.028, val_acc:0.994]
Epoch [103/120    avg_loss:0.024, val_acc:0.994]
Epoch [104/120    avg_loss:0.026, val_acc:0.994]
Epoch [105/120    avg_loss:0.030, val_acc:0.996]
Epoch [106/120    avg_loss:0.025, val_acc:0.996]
Epoch [107/120    avg_loss:0.028, val_acc:0.996]
Epoch [108/120    avg_loss:0.033, val_acc:0.996]
Epoch [109/120    avg_loss:0.030, val_acc:0.996]
Epoch [110/120    avg_loss:0.027, val_acc:0.996]
Epoch [111/120    avg_loss:0.024, val_acc:0.996]
Epoch [112/120    avg_loss:0.024, val_acc:0.996]
Epoch [113/120    avg_loss:0.029, val_acc:0.996]
Epoch [114/120    avg_loss:0.023, val_acc:0.996]
Epoch [115/120    avg_loss:0.025, val_acc:0.996]
Epoch [116/120    avg_loss:0.024, val_acc:0.996]
Epoch [117/120    avg_loss:0.031, val_acc:0.996]
Epoch [118/120    avg_loss:0.023, val_acc:0.996]
Epoch [119/120    avg_loss:0.023, val_acc:0.996]
Epoch [120/120    avg_loss:0.036, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   1   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 1.         0.98648649 0.99563319 0.9321663  0.89583333
 1.         0.96703297 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9909789800922562
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd40b5d6908>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.509, val_acc:0.310]
Epoch [2/120    avg_loss:2.190, val_acc:0.526]
Epoch [3/120    avg_loss:1.984, val_acc:0.633]
Epoch [4/120    avg_loss:1.752, val_acc:0.593]
Epoch [5/120    avg_loss:1.499, val_acc:0.677]
Epoch [6/120    avg_loss:1.299, val_acc:0.700]
Epoch [7/120    avg_loss:1.133, val_acc:0.714]
Epoch [8/120    avg_loss:1.005, val_acc:0.774]
Epoch [9/120    avg_loss:0.921, val_acc:0.877]
Epoch [10/120    avg_loss:0.830, val_acc:0.885]
Epoch [11/120    avg_loss:0.728, val_acc:0.887]
Epoch [12/120    avg_loss:0.660, val_acc:0.901]
Epoch [13/120    avg_loss:0.622, val_acc:0.875]
Epoch [14/120    avg_loss:0.601, val_acc:0.895]
Epoch [15/120    avg_loss:0.610, val_acc:0.901]
Epoch [16/120    avg_loss:0.515, val_acc:0.897]
Epoch [17/120    avg_loss:0.464, val_acc:0.905]
Epoch [18/120    avg_loss:0.496, val_acc:0.927]
Epoch [19/120    avg_loss:0.486, val_acc:0.905]
Epoch [20/120    avg_loss:0.408, val_acc:0.917]
Epoch [21/120    avg_loss:0.400, val_acc:0.903]
Epoch [22/120    avg_loss:0.382, val_acc:0.887]
Epoch [23/120    avg_loss:0.431, val_acc:0.893]
Epoch [24/120    avg_loss:0.388, val_acc:0.940]
Epoch [25/120    avg_loss:0.358, val_acc:0.958]
Epoch [26/120    avg_loss:0.281, val_acc:0.911]
Epoch [27/120    avg_loss:0.287, val_acc:0.925]
Epoch [28/120    avg_loss:0.315, val_acc:0.942]
Epoch [29/120    avg_loss:0.299, val_acc:0.946]
Epoch [30/120    avg_loss:0.308, val_acc:0.919]
Epoch [31/120    avg_loss:0.322, val_acc:0.929]
Epoch [32/120    avg_loss:0.301, val_acc:0.938]
Epoch [33/120    avg_loss:0.234, val_acc:0.956]
Epoch [34/120    avg_loss:0.287, val_acc:0.923]
Epoch [35/120    avg_loss:0.342, val_acc:0.942]
Epoch [36/120    avg_loss:0.367, val_acc:0.835]
Epoch [37/120    avg_loss:0.292, val_acc:0.952]
Epoch [38/120    avg_loss:0.233, val_acc:0.946]
Epoch [39/120    avg_loss:0.191, val_acc:0.960]
Epoch [40/120    avg_loss:0.206, val_acc:0.960]
Epoch [41/120    avg_loss:0.187, val_acc:0.966]
Epoch [42/120    avg_loss:0.170, val_acc:0.974]
Epoch [43/120    avg_loss:0.170, val_acc:0.968]
Epoch [44/120    avg_loss:0.162, val_acc:0.960]
Epoch [45/120    avg_loss:0.172, val_acc:0.972]
Epoch [46/120    avg_loss:0.155, val_acc:0.972]
Epoch [47/120    avg_loss:0.191, val_acc:0.974]
Epoch [48/120    avg_loss:0.158, val_acc:0.984]
Epoch [49/120    avg_loss:0.136, val_acc:0.976]
Epoch [50/120    avg_loss:0.159, val_acc:0.972]
Epoch [51/120    avg_loss:0.140, val_acc:0.984]
Epoch [52/120    avg_loss:0.148, val_acc:0.984]
Epoch [53/120    avg_loss:0.147, val_acc:0.972]
Epoch [54/120    avg_loss:0.147, val_acc:0.978]
Epoch [55/120    avg_loss:0.152, val_acc:0.984]
Epoch [56/120    avg_loss:0.140, val_acc:0.980]
Epoch [57/120    avg_loss:0.149, val_acc:0.980]
Epoch [58/120    avg_loss:0.144, val_acc:0.976]
Epoch [59/120    avg_loss:0.148, val_acc:0.978]
Epoch [60/120    avg_loss:0.145, val_acc:0.988]
Epoch [61/120    avg_loss:0.131, val_acc:0.978]
Epoch [62/120    avg_loss:0.158, val_acc:0.984]
Epoch [63/120    avg_loss:0.126, val_acc:0.980]
Epoch [64/120    avg_loss:0.135, val_acc:0.984]
Epoch [65/120    avg_loss:0.122, val_acc:0.976]
Epoch [66/120    avg_loss:0.140, val_acc:0.984]
Epoch [67/120    avg_loss:0.117, val_acc:0.986]
Epoch [68/120    avg_loss:0.116, val_acc:0.980]
Epoch [69/120    avg_loss:0.125, val_acc:0.978]
Epoch [70/120    avg_loss:0.143, val_acc:0.990]
Epoch [71/120    avg_loss:0.119, val_acc:0.978]
Epoch [72/120    avg_loss:0.123, val_acc:0.980]
Epoch [73/120    avg_loss:0.126, val_acc:0.982]
Epoch [74/120    avg_loss:0.136, val_acc:0.984]
Epoch [75/120    avg_loss:0.111, val_acc:0.980]
Epoch [76/120    avg_loss:0.121, val_acc:0.986]
Epoch [77/120    avg_loss:0.120, val_acc:0.988]
Epoch [78/120    avg_loss:0.124, val_acc:0.976]
Epoch [79/120    avg_loss:0.119, val_acc:0.986]
Epoch [80/120    avg_loss:0.119, val_acc:0.984]
Epoch [81/120    avg_loss:0.115, val_acc:0.978]
Epoch [82/120    avg_loss:0.113, val_acc:0.980]
Epoch [83/120    avg_loss:0.105, val_acc:0.982]
Epoch [84/120    avg_loss:0.099, val_acc:0.984]
Epoch [85/120    avg_loss:0.092, val_acc:0.984]
Epoch [86/120    avg_loss:0.100, val_acc:0.982]
Epoch [87/120    avg_loss:0.108, val_acc:0.980]
Epoch [88/120    avg_loss:0.105, val_acc:0.984]
Epoch [89/120    avg_loss:0.107, val_acc:0.986]
Epoch [90/120    avg_loss:0.109, val_acc:0.986]
Epoch [91/120    avg_loss:0.096, val_acc:0.986]
Epoch [92/120    avg_loss:0.116, val_acc:0.984]
Epoch [93/120    avg_loss:0.105, val_acc:0.988]
Epoch [94/120    avg_loss:0.101, val_acc:0.988]
Epoch [95/120    avg_loss:0.102, val_acc:0.986]
Epoch [96/120    avg_loss:0.091, val_acc:0.988]
Epoch [97/120    avg_loss:0.117, val_acc:0.986]
Epoch [98/120    avg_loss:0.107, val_acc:0.986]
Epoch [99/120    avg_loss:0.111, val_acc:0.988]
Epoch [100/120    avg_loss:0.104, val_acc:0.986]
Epoch [101/120    avg_loss:0.102, val_acc:0.988]
Epoch [102/120    avg_loss:0.090, val_acc:0.988]
Epoch [103/120    avg_loss:0.102, val_acc:0.988]
Epoch [104/120    avg_loss:0.108, val_acc:0.986]
Epoch [105/120    avg_loss:0.111, val_acc:0.988]
Epoch [106/120    avg_loss:0.093, val_acc:0.988]
Epoch [107/120    avg_loss:0.127, val_acc:0.988]
Epoch [108/120    avg_loss:0.094, val_acc:0.988]
Epoch [109/120    avg_loss:0.083, val_acc:0.988]
Epoch [110/120    avg_loss:0.097, val_acc:0.988]
Epoch [111/120    avg_loss:0.104, val_acc:0.988]
Epoch [112/120    avg_loss:0.114, val_acc:0.988]
Epoch [113/120    avg_loss:0.094, val_acc:0.988]
Epoch [114/120    avg_loss:0.103, val_acc:0.988]
Epoch [115/120    avg_loss:0.096, val_acc:0.988]
Epoch [116/120    avg_loss:0.090, val_acc:0.988]
Epoch [117/120    avg_loss:0.097, val_acc:0.988]
Epoch [118/120    avg_loss:0.097, val_acc:0.988]
Epoch [119/120    avg_loss:0.106, val_acc:0.988]
Epoch [120/120    avg_loss:0.103, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 214   0   0   0   0   5   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0  10 208   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8272921108742

F1 scores:
[       nan 1.         0.96613995 0.9787234  0.9122807  0.89208633
 1.         0.91803279 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9869429368121663
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f28365f9908>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.438, val_acc:0.464]
Epoch [2/120    avg_loss:2.100, val_acc:0.581]
Epoch [3/120    avg_loss:1.856, val_acc:0.565]
Epoch [4/120    avg_loss:1.610, val_acc:0.623]
Epoch [5/120    avg_loss:1.408, val_acc:0.645]
Epoch [6/120    avg_loss:1.222, val_acc:0.702]
Epoch [7/120    avg_loss:1.095, val_acc:0.764]
Epoch [8/120    avg_loss:0.940, val_acc:0.780]
Epoch [9/120    avg_loss:0.829, val_acc:0.802]
Epoch [10/120    avg_loss:0.728, val_acc:0.802]
Epoch [11/120    avg_loss:0.765, val_acc:0.821]
Epoch [12/120    avg_loss:0.723, val_acc:0.873]
Epoch [13/120    avg_loss:0.668, val_acc:0.851]
Epoch [14/120    avg_loss:0.693, val_acc:0.897]
Epoch [15/120    avg_loss:0.559, val_acc:0.895]
Epoch [16/120    avg_loss:0.486, val_acc:0.911]
Epoch [17/120    avg_loss:0.493, val_acc:0.887]
Epoch [18/120    avg_loss:0.466, val_acc:0.905]
Epoch [19/120    avg_loss:0.447, val_acc:0.929]
Epoch [20/120    avg_loss:0.358, val_acc:0.921]
Epoch [21/120    avg_loss:0.487, val_acc:0.895]
Epoch [22/120    avg_loss:0.413, val_acc:0.895]
Epoch [23/120    avg_loss:0.341, val_acc:0.946]
Epoch [24/120    avg_loss:0.304, val_acc:0.927]
Epoch [25/120    avg_loss:0.318, val_acc:0.931]
Epoch [26/120    avg_loss:0.273, val_acc:0.913]
Epoch [27/120    avg_loss:0.296, val_acc:0.917]
Epoch [28/120    avg_loss:0.322, val_acc:0.946]
Epoch [29/120    avg_loss:0.318, val_acc:0.942]
Epoch [30/120    avg_loss:0.244, val_acc:0.940]
Epoch [31/120    avg_loss:0.268, val_acc:0.948]
Epoch [32/120    avg_loss:0.246, val_acc:0.958]
Epoch [33/120    avg_loss:0.239, val_acc:0.950]
Epoch [34/120    avg_loss:0.220, val_acc:0.954]
Epoch [35/120    avg_loss:0.281, val_acc:0.962]
Epoch [36/120    avg_loss:0.235, val_acc:0.917]
Epoch [37/120    avg_loss:0.212, val_acc:0.950]
Epoch [38/120    avg_loss:0.200, val_acc:0.946]
Epoch [39/120    avg_loss:0.205, val_acc:0.968]
Epoch [40/120    avg_loss:0.161, val_acc:0.958]
Epoch [41/120    avg_loss:0.165, val_acc:0.966]
Epoch [42/120    avg_loss:0.153, val_acc:0.970]
Epoch [43/120    avg_loss:0.134, val_acc:0.980]
Epoch [44/120    avg_loss:0.125, val_acc:0.968]
Epoch [45/120    avg_loss:0.135, val_acc:0.966]
Epoch [46/120    avg_loss:0.160, val_acc:0.956]
Epoch [47/120    avg_loss:0.131, val_acc:0.964]
Epoch [48/120    avg_loss:0.107, val_acc:0.962]
Epoch [49/120    avg_loss:0.170, val_acc:0.950]
Epoch [50/120    avg_loss:0.147, val_acc:0.970]
Epoch [51/120    avg_loss:0.158, val_acc:0.968]
Epoch [52/120    avg_loss:0.125, val_acc:0.978]
Epoch [53/120    avg_loss:0.110, val_acc:0.964]
Epoch [54/120    avg_loss:0.133, val_acc:0.978]
Epoch [55/120    avg_loss:0.106, val_acc:0.980]
Epoch [56/120    avg_loss:0.069, val_acc:0.978]
Epoch [57/120    avg_loss:0.138, val_acc:0.970]
Epoch [58/120    avg_loss:0.109, val_acc:0.917]
Epoch [59/120    avg_loss:0.128, val_acc:0.964]
Epoch [60/120    avg_loss:0.100, val_acc:0.944]
Epoch [61/120    avg_loss:0.107, val_acc:0.978]
Epoch [62/120    avg_loss:0.083, val_acc:0.974]
Epoch [63/120    avg_loss:0.116, val_acc:0.974]
Epoch [64/120    avg_loss:0.100, val_acc:0.984]
Epoch [65/120    avg_loss:0.094, val_acc:0.990]
Epoch [66/120    avg_loss:0.096, val_acc:0.988]
Epoch [67/120    avg_loss:0.107, val_acc:0.980]
Epoch [68/120    avg_loss:0.088, val_acc:0.964]
Epoch [69/120    avg_loss:0.074, val_acc:0.976]
Epoch [70/120    avg_loss:0.064, val_acc:0.992]
Epoch [71/120    avg_loss:0.050, val_acc:0.978]
Epoch [72/120    avg_loss:0.065, val_acc:0.984]
Epoch [73/120    avg_loss:0.059, val_acc:0.984]
Epoch [74/120    avg_loss:0.035, val_acc:0.988]
Epoch [75/120    avg_loss:0.046, val_acc:0.984]
Epoch [76/120    avg_loss:0.041, val_acc:0.986]
Epoch [77/120    avg_loss:0.039, val_acc:0.992]
Epoch [78/120    avg_loss:0.031, val_acc:0.986]
Epoch [79/120    avg_loss:0.028, val_acc:0.992]
Epoch [80/120    avg_loss:0.030, val_acc:0.988]
Epoch [81/120    avg_loss:0.037, val_acc:0.988]
Epoch [82/120    avg_loss:0.049, val_acc:0.984]
Epoch [83/120    avg_loss:0.045, val_acc:0.984]
Epoch [84/120    avg_loss:0.065, val_acc:0.990]
Epoch [85/120    avg_loss:0.065, val_acc:0.960]
Epoch [86/120    avg_loss:0.057, val_acc:0.988]
Epoch [87/120    avg_loss:0.046, val_acc:0.986]
Epoch [88/120    avg_loss:0.055, val_acc:0.980]
Epoch [89/120    avg_loss:0.062, val_acc:0.982]
Epoch [90/120    avg_loss:0.066, val_acc:0.982]
Epoch [91/120    avg_loss:0.049, val_acc:0.988]
Epoch [92/120    avg_loss:0.029, val_acc:0.986]
Epoch [93/120    avg_loss:0.021, val_acc:0.988]
Epoch [94/120    avg_loss:0.020, val_acc:0.990]
Epoch [95/120    avg_loss:0.025, val_acc:0.990]
Epoch [96/120    avg_loss:0.027, val_acc:0.990]
Epoch [97/120    avg_loss:0.021, val_acc:0.990]
Epoch [98/120    avg_loss:0.017, val_acc:0.990]
Epoch [99/120    avg_loss:0.021, val_acc:0.992]
Epoch [100/120    avg_loss:0.021, val_acc:0.990]
Epoch [101/120    avg_loss:0.021, val_acc:0.990]
Epoch [102/120    avg_loss:0.020, val_acc:0.990]
Epoch [103/120    avg_loss:0.017, val_acc:0.990]
Epoch [104/120    avg_loss:0.019, val_acc:0.990]
Epoch [105/120    avg_loss:0.019, val_acc:0.992]
Epoch [106/120    avg_loss:0.015, val_acc:0.992]
Epoch [107/120    avg_loss:0.016, val_acc:0.992]
Epoch [108/120    avg_loss:0.015, val_acc:0.992]
Epoch [109/120    avg_loss:0.014, val_acc:0.992]
Epoch [110/120    avg_loss:0.014, val_acc:0.992]
Epoch [111/120    avg_loss:0.013, val_acc:0.992]
Epoch [112/120    avg_loss:0.015, val_acc:0.992]
Epoch [113/120    avg_loss:0.018, val_acc:0.992]
Epoch [114/120    avg_loss:0.017, val_acc:0.992]
Epoch [115/120    avg_loss:0.014, val_acc:0.992]
Epoch [116/120    avg_loss:0.016, val_acc:0.992]
Epoch [117/120    avg_loss:0.011, val_acc:0.992]
Epoch [118/120    avg_loss:0.013, val_acc:0.992]
Epoch [119/120    avg_loss:0.014, val_acc:0.992]
Epoch [120/120    avg_loss:0.013, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 228   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 1.         0.99319728 0.99563319 0.93709328 0.8975265
 1.         0.98924731 0.998713   1.         1.         1.
 1.         1.        ]

Kappa:
0.9921660269790955
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe5c01ab908>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.433, val_acc:0.347]
Epoch [2/120    avg_loss:2.122, val_acc:0.464]
Epoch [3/120    avg_loss:1.898, val_acc:0.617]
Epoch [4/120    avg_loss:1.677, val_acc:0.655]
Epoch [5/120    avg_loss:1.460, val_acc:0.681]
Epoch [6/120    avg_loss:1.252, val_acc:0.671]
Epoch [7/120    avg_loss:1.119, val_acc:0.714]
Epoch [8/120    avg_loss:0.985, val_acc:0.802]
Epoch [9/120    avg_loss:0.905, val_acc:0.800]
Epoch [10/120    avg_loss:0.857, val_acc:0.861]
Epoch [11/120    avg_loss:0.751, val_acc:0.829]
Epoch [12/120    avg_loss:0.712, val_acc:0.853]
Epoch [13/120    avg_loss:0.619, val_acc:0.869]
Epoch [14/120    avg_loss:0.634, val_acc:0.885]
Epoch [15/120    avg_loss:0.571, val_acc:0.821]
Epoch [16/120    avg_loss:0.490, val_acc:0.919]
Epoch [17/120    avg_loss:0.558, val_acc:0.873]
Epoch [18/120    avg_loss:0.523, val_acc:0.903]
Epoch [19/120    avg_loss:0.420, val_acc:0.911]
Epoch [20/120    avg_loss:0.435, val_acc:0.877]
Epoch [21/120    avg_loss:0.389, val_acc:0.929]
Epoch [22/120    avg_loss:0.341, val_acc:0.931]
Epoch [23/120    avg_loss:0.330, val_acc:0.950]
Epoch [24/120    avg_loss:0.277, val_acc:0.946]
Epoch [25/120    avg_loss:0.303, val_acc:0.903]
Epoch [26/120    avg_loss:0.358, val_acc:0.909]
Epoch [27/120    avg_loss:0.335, val_acc:0.917]
Epoch [28/120    avg_loss:0.279, val_acc:0.960]
Epoch [29/120    avg_loss:0.258, val_acc:0.933]
Epoch [30/120    avg_loss:0.223, val_acc:0.954]
Epoch [31/120    avg_loss:0.273, val_acc:0.944]
Epoch [32/120    avg_loss:0.229, val_acc:0.958]
Epoch [33/120    avg_loss:0.218, val_acc:0.956]
Epoch [34/120    avg_loss:0.244, val_acc:0.954]
Epoch [35/120    avg_loss:0.168, val_acc:0.946]
Epoch [36/120    avg_loss:0.178, val_acc:0.956]
Epoch [37/120    avg_loss:0.221, val_acc:0.911]
Epoch [38/120    avg_loss:0.245, val_acc:0.907]
Epoch [39/120    avg_loss:0.265, val_acc:0.954]
Epoch [40/120    avg_loss:0.193, val_acc:0.962]
Epoch [41/120    avg_loss:0.171, val_acc:0.935]
Epoch [42/120    avg_loss:0.301, val_acc:0.968]
Epoch [43/120    avg_loss:0.167, val_acc:0.966]
Epoch [44/120    avg_loss:0.156, val_acc:0.972]
Epoch [45/120    avg_loss:0.191, val_acc:0.903]
Epoch [46/120    avg_loss:0.307, val_acc:0.964]
Epoch [47/120    avg_loss:0.209, val_acc:0.968]
Epoch [48/120    avg_loss:0.186, val_acc:0.946]
Epoch [49/120    avg_loss:0.158, val_acc:0.976]
Epoch [50/120    avg_loss:0.125, val_acc:0.962]
Epoch [51/120    avg_loss:0.113, val_acc:0.962]
Epoch [52/120    avg_loss:0.111, val_acc:0.966]
Epoch [53/120    avg_loss:0.133, val_acc:0.984]
Epoch [54/120    avg_loss:0.069, val_acc:0.974]
Epoch [55/120    avg_loss:0.102, val_acc:0.960]
Epoch [56/120    avg_loss:0.077, val_acc:0.974]
Epoch [57/120    avg_loss:0.095, val_acc:0.968]
Epoch [58/120    avg_loss:0.093, val_acc:0.990]
Epoch [59/120    avg_loss:0.084, val_acc:0.974]
Epoch [60/120    avg_loss:0.088, val_acc:0.964]
Epoch [61/120    avg_loss:0.108, val_acc:0.956]
Epoch [62/120    avg_loss:0.129, val_acc:0.978]
Epoch [63/120    avg_loss:0.124, val_acc:0.976]
Epoch [64/120    avg_loss:0.080, val_acc:0.982]
Epoch [65/120    avg_loss:0.078, val_acc:0.938]
Epoch [66/120    avg_loss:0.142, val_acc:0.974]
Epoch [67/120    avg_loss:0.099, val_acc:0.984]
Epoch [68/120    avg_loss:0.057, val_acc:0.980]
Epoch [69/120    avg_loss:0.069, val_acc:0.970]
Epoch [70/120    avg_loss:0.092, val_acc:0.982]
Epoch [71/120    avg_loss:0.106, val_acc:0.972]
Epoch [72/120    avg_loss:0.073, val_acc:0.988]
Epoch [73/120    avg_loss:0.051, val_acc:0.988]
Epoch [74/120    avg_loss:0.047, val_acc:0.992]
Epoch [75/120    avg_loss:0.032, val_acc:0.992]
Epoch [76/120    avg_loss:0.041, val_acc:0.992]
Epoch [77/120    avg_loss:0.032, val_acc:0.992]
Epoch [78/120    avg_loss:0.031, val_acc:0.992]
Epoch [79/120    avg_loss:0.029, val_acc:0.990]
Epoch [80/120    avg_loss:0.033, val_acc:0.992]
Epoch [81/120    avg_loss:0.039, val_acc:0.988]
Epoch [82/120    avg_loss:0.038, val_acc:0.994]
Epoch [83/120    avg_loss:0.030, val_acc:0.994]
Epoch [84/120    avg_loss:0.034, val_acc:0.992]
Epoch [85/120    avg_loss:0.035, val_acc:0.992]
Epoch [86/120    avg_loss:0.029, val_acc:0.992]
Epoch [87/120    avg_loss:0.034, val_acc:0.992]
Epoch [88/120    avg_loss:0.026, val_acc:0.994]
Epoch [89/120    avg_loss:0.027, val_acc:0.994]
Epoch [90/120    avg_loss:0.028, val_acc:0.994]
Epoch [91/120    avg_loss:0.024, val_acc:0.994]
Epoch [92/120    avg_loss:0.023, val_acc:0.994]
Epoch [93/120    avg_loss:0.023, val_acc:0.994]
Epoch [94/120    avg_loss:0.024, val_acc:0.994]
Epoch [95/120    avg_loss:0.022, val_acc:0.994]
Epoch [96/120    avg_loss:0.023, val_acc:0.994]
Epoch [97/120    avg_loss:0.024, val_acc:0.994]
Epoch [98/120    avg_loss:0.022, val_acc:0.994]
Epoch [99/120    avg_loss:0.028, val_acc:0.994]
Epoch [100/120    avg_loss:0.022, val_acc:0.994]
Epoch [101/120    avg_loss:0.024, val_acc:0.994]
Epoch [102/120    avg_loss:0.020, val_acc:0.994]
Epoch [103/120    avg_loss:0.024, val_acc:0.994]
Epoch [104/120    avg_loss:0.020, val_acc:0.994]
Epoch [105/120    avg_loss:0.018, val_acc:0.994]
Epoch [106/120    avg_loss:0.024, val_acc:0.994]
Epoch [107/120    avg_loss:0.021, val_acc:0.992]
Epoch [108/120    avg_loss:0.021, val_acc:0.992]
Epoch [109/120    avg_loss:0.025, val_acc:0.994]
Epoch [110/120    avg_loss:0.024, val_acc:0.994]
Epoch [111/120    avg_loss:0.021, val_acc:0.994]
Epoch [112/120    avg_loss:0.022, val_acc:0.994]
Epoch [113/120    avg_loss:0.031, val_acc:0.994]
Epoch [114/120    avg_loss:0.021, val_acc:0.994]
Epoch [115/120    avg_loss:0.020, val_acc:0.994]
Epoch [116/120    avg_loss:0.019, val_acc:0.994]
Epoch [117/120    avg_loss:0.021, val_acc:0.994]
Epoch [118/120    avg_loss:0.021, val_acc:0.994]
Epoch [119/120    avg_loss:0.029, val_acc:0.994]
Epoch [120/120    avg_loss:0.022, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 225   2   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 1.         0.99095023 0.98901099 0.95555556 0.93918919
 1.         0.98378378 1.         0.9978678  1.         0.9973545
 0.99778761 1.        ]

Kappa:
0.9933532553459425
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffaa121a8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.500, val_acc:0.423]
Epoch [2/120    avg_loss:2.159, val_acc:0.647]
Epoch [3/120    avg_loss:1.881, val_acc:0.679]
Epoch [4/120    avg_loss:1.615, val_acc:0.669]
Epoch [5/120    avg_loss:1.372, val_acc:0.724]
Epoch [6/120    avg_loss:1.147, val_acc:0.768]
Epoch [7/120    avg_loss:0.992, val_acc:0.839]
Epoch [8/120    avg_loss:0.908, val_acc:0.823]
Epoch [9/120    avg_loss:0.823, val_acc:0.861]
Epoch [10/120    avg_loss:0.749, val_acc:0.887]
Epoch [11/120    avg_loss:0.674, val_acc:0.891]
Epoch [12/120    avg_loss:0.622, val_acc:0.889]
Epoch [13/120    avg_loss:0.618, val_acc:0.861]
Epoch [14/120    avg_loss:0.537, val_acc:0.905]
Epoch [15/120    avg_loss:0.568, val_acc:0.901]
Epoch [16/120    avg_loss:0.513, val_acc:0.899]
Epoch [17/120    avg_loss:0.492, val_acc:0.907]
Epoch [18/120    avg_loss:0.437, val_acc:0.917]
Epoch [19/120    avg_loss:0.424, val_acc:0.913]
Epoch [20/120    avg_loss:0.362, val_acc:0.933]
Epoch [21/120    avg_loss:0.379, val_acc:0.948]
Epoch [22/120    avg_loss:0.383, val_acc:0.929]
Epoch [23/120    avg_loss:0.304, val_acc:0.911]
Epoch [24/120    avg_loss:0.304, val_acc:0.935]
Epoch [25/120    avg_loss:0.288, val_acc:0.950]
Epoch [26/120    avg_loss:0.234, val_acc:0.962]
Epoch [27/120    avg_loss:0.318, val_acc:0.962]
Epoch [28/120    avg_loss:0.299, val_acc:0.970]
Epoch [29/120    avg_loss:0.248, val_acc:0.950]
Epoch [30/120    avg_loss:0.255, val_acc:0.942]
Epoch [31/120    avg_loss:0.205, val_acc:0.958]
Epoch [32/120    avg_loss:0.219, val_acc:0.962]
Epoch [33/120    avg_loss:0.238, val_acc:0.974]
Epoch [34/120    avg_loss:0.236, val_acc:0.962]
Epoch [35/120    avg_loss:0.169, val_acc:0.968]
Epoch [36/120    avg_loss:0.188, val_acc:0.956]
Epoch [37/120    avg_loss:0.197, val_acc:0.968]
Epoch [38/120    avg_loss:0.177, val_acc:0.970]
Epoch [39/120    avg_loss:0.190, val_acc:0.935]
Epoch [40/120    avg_loss:0.210, val_acc:0.907]
Epoch [41/120    avg_loss:0.335, val_acc:0.923]
Epoch [42/120    avg_loss:0.168, val_acc:0.944]
Epoch [43/120    avg_loss:0.208, val_acc:0.956]
Epoch [44/120    avg_loss:0.225, val_acc:0.980]
Epoch [45/120    avg_loss:0.178, val_acc:0.944]
Epoch [46/120    avg_loss:0.147, val_acc:0.970]
Epoch [47/120    avg_loss:0.139, val_acc:0.964]
Epoch [48/120    avg_loss:0.144, val_acc:0.980]
Epoch [49/120    avg_loss:0.106, val_acc:0.982]
Epoch [50/120    avg_loss:0.094, val_acc:0.982]
Epoch [51/120    avg_loss:0.094, val_acc:0.974]
Epoch [52/120    avg_loss:0.153, val_acc:0.976]
Epoch [53/120    avg_loss:0.147, val_acc:0.982]
Epoch [54/120    avg_loss:0.129, val_acc:0.978]
Epoch [55/120    avg_loss:0.137, val_acc:0.982]
Epoch [56/120    avg_loss:0.099, val_acc:0.976]
Epoch [57/120    avg_loss:0.097, val_acc:0.980]
Epoch [58/120    avg_loss:0.151, val_acc:0.978]
Epoch [59/120    avg_loss:0.088, val_acc:0.978]
Epoch [60/120    avg_loss:0.115, val_acc:0.986]
Epoch [61/120    avg_loss:0.088, val_acc:0.964]
Epoch [62/120    avg_loss:0.092, val_acc:0.976]
Epoch [63/120    avg_loss:0.124, val_acc:0.976]
Epoch [64/120    avg_loss:0.096, val_acc:0.982]
Epoch [65/120    avg_loss:0.060, val_acc:0.984]
Epoch [66/120    avg_loss:0.067, val_acc:0.980]
Epoch [67/120    avg_loss:0.075, val_acc:0.990]
Epoch [68/120    avg_loss:0.050, val_acc:0.986]
Epoch [69/120    avg_loss:0.069, val_acc:0.976]
Epoch [70/120    avg_loss:0.077, val_acc:0.986]
Epoch [71/120    avg_loss:0.087, val_acc:0.990]
Epoch [72/120    avg_loss:0.058, val_acc:0.994]
Epoch [73/120    avg_loss:0.066, val_acc:0.992]
Epoch [74/120    avg_loss:0.139, val_acc:0.974]
Epoch [75/120    avg_loss:0.058, val_acc:0.988]
Epoch [76/120    avg_loss:0.060, val_acc:0.992]
Epoch [77/120    avg_loss:0.048, val_acc:0.986]
Epoch [78/120    avg_loss:0.046, val_acc:0.982]
Epoch [79/120    avg_loss:0.046, val_acc:0.992]
Epoch [80/120    avg_loss:0.043, val_acc:0.996]
Epoch [81/120    avg_loss:0.038, val_acc:0.998]
Epoch [82/120    avg_loss:0.025, val_acc:0.990]
Epoch [83/120    avg_loss:0.036, val_acc:0.980]
Epoch [84/120    avg_loss:0.036, val_acc:0.996]
Epoch [85/120    avg_loss:0.046, val_acc:0.984]
Epoch [86/120    avg_loss:0.051, val_acc:0.994]
Epoch [87/120    avg_loss:0.034, val_acc:0.988]
Epoch [88/120    avg_loss:0.074, val_acc:0.974]
Epoch [89/120    avg_loss:0.067, val_acc:0.982]
Epoch [90/120    avg_loss:0.049, val_acc:0.992]
Epoch [91/120    avg_loss:0.028, val_acc:0.992]
Epoch [92/120    avg_loss:0.021, val_acc:0.996]
Epoch [93/120    avg_loss:0.023, val_acc:0.996]
Epoch [94/120    avg_loss:0.020, val_acc:0.994]
Epoch [95/120    avg_loss:0.012, val_acc:0.994]
Epoch [96/120    avg_loss:0.014, val_acc:0.994]
Epoch [97/120    avg_loss:0.013, val_acc:0.994]
Epoch [98/120    avg_loss:0.011, val_acc:0.994]
Epoch [99/120    avg_loss:0.013, val_acc:0.994]
Epoch [100/120    avg_loss:0.013, val_acc:0.994]
Epoch [101/120    avg_loss:0.012, val_acc:0.994]
Epoch [102/120    avg_loss:0.015, val_acc:0.994]
Epoch [103/120    avg_loss:0.013, val_acc:0.994]
Epoch [104/120    avg_loss:0.012, val_acc:0.994]
Epoch [105/120    avg_loss:0.013, val_acc:0.996]
Epoch [106/120    avg_loss:0.012, val_acc:0.996]
Epoch [107/120    avg_loss:0.015, val_acc:0.994]
Epoch [108/120    avg_loss:0.011, val_acc:0.994]
Epoch [109/120    avg_loss:0.012, val_acc:0.994]
Epoch [110/120    avg_loss:0.017, val_acc:0.994]
Epoch [111/120    avg_loss:0.012, val_acc:0.994]
Epoch [112/120    avg_loss:0.012, val_acc:0.994]
Epoch [113/120    avg_loss:0.014, val_acc:0.994]
Epoch [114/120    avg_loss:0.012, val_acc:0.994]
Epoch [115/120    avg_loss:0.013, val_acc:0.994]
Epoch [116/120    avg_loss:0.013, val_acc:0.994]
Epoch [117/120    avg_loss:0.012, val_acc:0.994]
Epoch [118/120    avg_loss:0.014, val_acc:0.994]
Epoch [119/120    avg_loss:0.016, val_acc:0.994]
Epoch [120/120    avg_loss:0.012, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   8   0   0   0   0   0   0   1   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 1.         0.98871332 0.99782135 0.96035242 0.94482759
 1.         0.9726776  1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9945399921908779
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd68ed94940>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.428, val_acc:0.452]
Epoch [2/120    avg_loss:2.138, val_acc:0.562]
Epoch [3/120    avg_loss:1.877, val_acc:0.643]
Epoch [4/120    avg_loss:1.597, val_acc:0.677]
Epoch [5/120    avg_loss:1.346, val_acc:0.667]
Epoch [6/120    avg_loss:1.207, val_acc:0.726]
Epoch [7/120    avg_loss:1.023, val_acc:0.782]
Epoch [8/120    avg_loss:0.940, val_acc:0.825]
Epoch [9/120    avg_loss:0.859, val_acc:0.831]
Epoch [10/120    avg_loss:0.826, val_acc:0.812]
Epoch [11/120    avg_loss:0.765, val_acc:0.800]
Epoch [12/120    avg_loss:0.655, val_acc:0.776]
Epoch [13/120    avg_loss:0.560, val_acc:0.819]
Epoch [14/120    avg_loss:0.606, val_acc:0.875]
Epoch [15/120    avg_loss:0.580, val_acc:0.897]
Epoch [16/120    avg_loss:0.494, val_acc:0.871]
Epoch [17/120    avg_loss:0.497, val_acc:0.883]
Epoch [18/120    avg_loss:0.521, val_acc:0.887]
Epoch [19/120    avg_loss:0.450, val_acc:0.889]
Epoch [20/120    avg_loss:0.438, val_acc:0.849]
Epoch [21/120    avg_loss:0.432, val_acc:0.877]
Epoch [22/120    avg_loss:0.458, val_acc:0.899]
Epoch [23/120    avg_loss:0.432, val_acc:0.903]
Epoch [24/120    avg_loss:0.451, val_acc:0.919]
Epoch [25/120    avg_loss:0.372, val_acc:0.915]
Epoch [26/120    avg_loss:0.388, val_acc:0.940]
Epoch [27/120    avg_loss:0.339, val_acc:0.917]
Epoch [28/120    avg_loss:0.327, val_acc:0.925]
Epoch [29/120    avg_loss:0.317, val_acc:0.933]
Epoch [30/120    avg_loss:0.293, val_acc:0.903]
Epoch [31/120    avg_loss:0.305, val_acc:0.935]
Epoch [32/120    avg_loss:0.272, val_acc:0.927]
Epoch [33/120    avg_loss:0.311, val_acc:0.931]
Epoch [34/120    avg_loss:0.330, val_acc:0.899]
Epoch [35/120    avg_loss:0.317, val_acc:0.940]
Epoch [36/120    avg_loss:0.273, val_acc:0.946]
Epoch [37/120    avg_loss:0.223, val_acc:0.944]
Epoch [38/120    avg_loss:0.222, val_acc:0.950]
Epoch [39/120    avg_loss:0.214, val_acc:0.921]
Epoch [40/120    avg_loss:0.253, val_acc:0.873]
Epoch [41/120    avg_loss:0.249, val_acc:0.952]
Epoch [42/120    avg_loss:0.220, val_acc:0.935]
Epoch [43/120    avg_loss:0.195, val_acc:0.968]
Epoch [44/120    avg_loss:0.167, val_acc:0.962]
Epoch [45/120    avg_loss:0.206, val_acc:0.940]
Epoch [46/120    avg_loss:0.159, val_acc:0.954]
Epoch [47/120    avg_loss:0.184, val_acc:0.958]
Epoch [48/120    avg_loss:0.164, val_acc:0.931]
Epoch [49/120    avg_loss:0.234, val_acc:0.946]
Epoch [50/120    avg_loss:0.156, val_acc:0.966]
Epoch [51/120    avg_loss:0.229, val_acc:0.964]
Epoch [52/120    avg_loss:0.176, val_acc:0.962]
Epoch [53/120    avg_loss:0.141, val_acc:0.942]
Epoch [54/120    avg_loss:0.158, val_acc:0.966]
Epoch [55/120    avg_loss:0.144, val_acc:0.962]
Epoch [56/120    avg_loss:0.184, val_acc:0.925]
Epoch [57/120    avg_loss:0.102, val_acc:0.960]
Epoch [58/120    avg_loss:0.102, val_acc:0.968]
Epoch [59/120    avg_loss:0.097, val_acc:0.972]
Epoch [60/120    avg_loss:0.088, val_acc:0.970]
Epoch [61/120    avg_loss:0.083, val_acc:0.976]
Epoch [62/120    avg_loss:0.071, val_acc:0.974]
Epoch [63/120    avg_loss:0.071, val_acc:0.974]
Epoch [64/120    avg_loss:0.073, val_acc:0.972]
Epoch [65/120    avg_loss:0.071, val_acc:0.974]
Epoch [66/120    avg_loss:0.085, val_acc:0.974]
Epoch [67/120    avg_loss:0.071, val_acc:0.978]
Epoch [68/120    avg_loss:0.063, val_acc:0.978]
Epoch [69/120    avg_loss:0.068, val_acc:0.978]
Epoch [70/120    avg_loss:0.078, val_acc:0.978]
Epoch [71/120    avg_loss:0.069, val_acc:0.976]
Epoch [72/120    avg_loss:0.077, val_acc:0.974]
Epoch [73/120    avg_loss:0.081, val_acc:0.976]
Epoch [74/120    avg_loss:0.068, val_acc:0.970]
Epoch [75/120    avg_loss:0.067, val_acc:0.974]
Epoch [76/120    avg_loss:0.058, val_acc:0.976]
Epoch [77/120    avg_loss:0.057, val_acc:0.980]
Epoch [78/120    avg_loss:0.065, val_acc:0.980]
Epoch [79/120    avg_loss:0.073, val_acc:0.976]
Epoch [80/120    avg_loss:0.063, val_acc:0.972]
Epoch [81/120    avg_loss:0.064, val_acc:0.976]
Epoch [82/120    avg_loss:0.064, val_acc:0.984]
Epoch [83/120    avg_loss:0.060, val_acc:0.978]
Epoch [84/120    avg_loss:0.058, val_acc:0.980]
Epoch [85/120    avg_loss:0.056, val_acc:0.984]
Epoch [86/120    avg_loss:0.054, val_acc:0.980]
Epoch [87/120    avg_loss:0.067, val_acc:0.982]
Epoch [88/120    avg_loss:0.055, val_acc:0.978]
Epoch [89/120    avg_loss:0.059, val_acc:0.980]
Epoch [90/120    avg_loss:0.053, val_acc:0.980]
Epoch [91/120    avg_loss:0.045, val_acc:0.978]
Epoch [92/120    avg_loss:0.061, val_acc:0.978]
Epoch [93/120    avg_loss:0.058, val_acc:0.980]
Epoch [94/120    avg_loss:0.052, val_acc:0.982]
Epoch [95/120    avg_loss:0.045, val_acc:0.980]
Epoch [96/120    avg_loss:0.053, val_acc:0.982]
Epoch [97/120    avg_loss:0.048, val_acc:0.984]
Epoch [98/120    avg_loss:0.049, val_acc:0.984]
Epoch [99/120    avg_loss:0.055, val_acc:0.980]
Epoch [100/120    avg_loss:0.059, val_acc:0.980]
Epoch [101/120    avg_loss:0.049, val_acc:0.984]
Epoch [102/120    avg_loss:0.052, val_acc:0.982]
Epoch [103/120    avg_loss:0.052, val_acc:0.982]
Epoch [104/120    avg_loss:0.054, val_acc:0.984]
Epoch [105/120    avg_loss:0.054, val_acc:0.982]
Epoch [106/120    avg_loss:0.056, val_acc:0.982]
Epoch [107/120    avg_loss:0.056, val_acc:0.984]
Epoch [108/120    avg_loss:0.048, val_acc:0.982]
Epoch [109/120    avg_loss:0.050, val_acc:0.982]
Epoch [110/120    avg_loss:0.050, val_acc:0.982]
Epoch [111/120    avg_loss:0.041, val_acc:0.984]
Epoch [112/120    avg_loss:0.046, val_acc:0.982]
Epoch [113/120    avg_loss:0.041, val_acc:0.980]
Epoch [114/120    avg_loss:0.051, val_acc:0.980]
Epoch [115/120    avg_loss:0.041, val_acc:0.978]
Epoch [116/120    avg_loss:0.051, val_acc:0.984]
Epoch [117/120    avg_loss:0.057, val_acc:0.982]
Epoch [118/120    avg_loss:0.050, val_acc:0.984]
Epoch [119/120    avg_loss:0.047, val_acc:0.986]
Epoch [120/120    avg_loss:0.042, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 1.         0.98871332 0.98678414 0.90909091 0.875
 1.         0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9888426362234245
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efe825cf8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.460, val_acc:0.389]
Epoch [2/120    avg_loss:2.135, val_acc:0.559]
Epoch [3/120    avg_loss:1.880, val_acc:0.590]
Epoch [4/120    avg_loss:1.637, val_acc:0.666]
Epoch [5/120    avg_loss:1.386, val_acc:0.711]
Epoch [6/120    avg_loss:1.176, val_acc:0.740]
Epoch [7/120    avg_loss:1.017, val_acc:0.775]
Epoch [8/120    avg_loss:0.880, val_acc:0.770]
Epoch [9/120    avg_loss:0.812, val_acc:0.877]
Epoch [10/120    avg_loss:0.696, val_acc:0.848]
Epoch [11/120    avg_loss:0.724, val_acc:0.908]
Epoch [12/120    avg_loss:0.617, val_acc:0.873]
Epoch [13/120    avg_loss:0.567, val_acc:0.914]
Epoch [14/120    avg_loss:0.511, val_acc:0.883]
Epoch [15/120    avg_loss:0.479, val_acc:0.938]
Epoch [16/120    avg_loss:0.442, val_acc:0.955]
Epoch [17/120    avg_loss:0.450, val_acc:0.887]
Epoch [18/120    avg_loss:0.493, val_acc:0.916]
Epoch [19/120    avg_loss:0.414, val_acc:0.932]
Epoch [20/120    avg_loss:0.385, val_acc:0.951]
Epoch [21/120    avg_loss:0.413, val_acc:0.934]
Epoch [22/120    avg_loss:0.390, val_acc:0.941]
Epoch [23/120    avg_loss:0.296, val_acc:0.951]
Epoch [24/120    avg_loss:0.317, val_acc:0.930]
Epoch [25/120    avg_loss:0.328, val_acc:0.951]
Epoch [26/120    avg_loss:0.304, val_acc:0.932]
Epoch [27/120    avg_loss:0.277, val_acc:0.949]
Epoch [28/120    avg_loss:0.351, val_acc:0.918]
Epoch [29/120    avg_loss:0.301, val_acc:0.947]
Epoch [30/120    avg_loss:0.223, val_acc:0.963]
Epoch [31/120    avg_loss:0.212, val_acc:0.969]
Epoch [32/120    avg_loss:0.202, val_acc:0.971]
Epoch [33/120    avg_loss:0.183, val_acc:0.969]
Epoch [34/120    avg_loss:0.193, val_acc:0.973]
Epoch [35/120    avg_loss:0.221, val_acc:0.973]
Epoch [36/120    avg_loss:0.189, val_acc:0.975]
Epoch [37/120    avg_loss:0.199, val_acc:0.975]
Epoch [38/120    avg_loss:0.195, val_acc:0.975]
Epoch [39/120    avg_loss:0.180, val_acc:0.977]
Epoch [40/120    avg_loss:0.186, val_acc:0.971]
Epoch [41/120    avg_loss:0.172, val_acc:0.971]
Epoch [42/120    avg_loss:0.188, val_acc:0.975]
Epoch [43/120    avg_loss:0.177, val_acc:0.975]
Epoch [44/120    avg_loss:0.166, val_acc:0.973]
Epoch [45/120    avg_loss:0.164, val_acc:0.979]
Epoch [46/120    avg_loss:0.171, val_acc:0.977]
Epoch [47/120    avg_loss:0.192, val_acc:0.967]
Epoch [48/120    avg_loss:0.167, val_acc:0.979]
Epoch [49/120    avg_loss:0.149, val_acc:0.971]
Epoch [50/120    avg_loss:0.170, val_acc:0.977]
Epoch [51/120    avg_loss:0.183, val_acc:0.982]
Epoch [52/120    avg_loss:0.164, val_acc:0.980]
Epoch [53/120    avg_loss:0.165, val_acc:0.979]
Epoch [54/120    avg_loss:0.169, val_acc:0.979]
Epoch [55/120    avg_loss:0.177, val_acc:0.979]
Epoch [56/120    avg_loss:0.154, val_acc:0.977]
Epoch [57/120    avg_loss:0.167, val_acc:0.980]
Epoch [58/120    avg_loss:0.164, val_acc:0.973]
Epoch [59/120    avg_loss:0.159, val_acc:0.975]
Epoch [60/120    avg_loss:0.146, val_acc:0.979]
Epoch [61/120    avg_loss:0.160, val_acc:0.980]
Epoch [62/120    avg_loss:0.154, val_acc:0.980]
Epoch [63/120    avg_loss:0.163, val_acc:0.980]
Epoch [64/120    avg_loss:0.142, val_acc:0.980]
Epoch [65/120    avg_loss:0.152, val_acc:0.980]
Epoch [66/120    avg_loss:0.129, val_acc:0.980]
Epoch [67/120    avg_loss:0.145, val_acc:0.980]
Epoch [68/120    avg_loss:0.142, val_acc:0.980]
Epoch [69/120    avg_loss:0.146, val_acc:0.980]
Epoch [70/120    avg_loss:0.169, val_acc:0.979]
Epoch [71/120    avg_loss:0.141, val_acc:0.980]
Epoch [72/120    avg_loss:0.142, val_acc:0.979]
Epoch [73/120    avg_loss:0.140, val_acc:0.977]
Epoch [74/120    avg_loss:0.138, val_acc:0.979]
Epoch [75/120    avg_loss:0.144, val_acc:0.977]
Epoch [76/120    avg_loss:0.141, val_acc:0.977]
Epoch [77/120    avg_loss:0.137, val_acc:0.977]
Epoch [78/120    avg_loss:0.136, val_acc:0.977]
Epoch [79/120    avg_loss:0.156, val_acc:0.977]
Epoch [80/120    avg_loss:0.162, val_acc:0.977]
Epoch [81/120    avg_loss:0.143, val_acc:0.977]
Epoch [82/120    avg_loss:0.147, val_acc:0.977]
Epoch [83/120    avg_loss:0.144, val_acc:0.977]
Epoch [84/120    avg_loss:0.146, val_acc:0.977]
Epoch [85/120    avg_loss:0.137, val_acc:0.977]
Epoch [86/120    avg_loss:0.157, val_acc:0.977]
Epoch [87/120    avg_loss:0.146, val_acc:0.977]
Epoch [88/120    avg_loss:0.135, val_acc:0.977]
Epoch [89/120    avg_loss:0.141, val_acc:0.977]
Epoch [90/120    avg_loss:0.152, val_acc:0.979]
Epoch [91/120    avg_loss:0.146, val_acc:0.979]
Epoch [92/120    avg_loss:0.150, val_acc:0.979]
Epoch [93/120    avg_loss:0.128, val_acc:0.979]
Epoch [94/120    avg_loss:0.134, val_acc:0.979]
Epoch [95/120    avg_loss:0.138, val_acc:0.979]
Epoch [96/120    avg_loss:0.145, val_acc:0.979]
Epoch [97/120    avg_loss:0.143, val_acc:0.979]
Epoch [98/120    avg_loss:0.134, val_acc:0.979]
Epoch [99/120    avg_loss:0.145, val_acc:0.979]
Epoch [100/120    avg_loss:0.143, val_acc:0.979]
Epoch [101/120    avg_loss:0.149, val_acc:0.979]
Epoch [102/120    avg_loss:0.140, val_acc:0.979]
Epoch [103/120    avg_loss:0.143, val_acc:0.979]
Epoch [104/120    avg_loss:0.129, val_acc:0.979]
Epoch [105/120    avg_loss:0.141, val_acc:0.979]
Epoch [106/120    avg_loss:0.138, val_acc:0.979]
Epoch [107/120    avg_loss:0.138, val_acc:0.979]
Epoch [108/120    avg_loss:0.133, val_acc:0.979]
Epoch [109/120    avg_loss:0.149, val_acc:0.979]
Epoch [110/120    avg_loss:0.149, val_acc:0.979]
Epoch [111/120    avg_loss:0.152, val_acc:0.979]
Epoch [112/120    avg_loss:0.146, val_acc:0.979]
Epoch [113/120    avg_loss:0.139, val_acc:0.979]
Epoch [114/120    avg_loss:0.150, val_acc:0.979]
Epoch [115/120    avg_loss:0.142, val_acc:0.979]
Epoch [116/120    avg_loss:0.142, val_acc:0.979]
Epoch [117/120    avg_loss:0.157, val_acc:0.979]
Epoch [118/120    avg_loss:0.159, val_acc:0.979]
Epoch [119/120    avg_loss:0.151, val_acc:0.979]
Epoch [120/120    avg_loss:0.158, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   2 223   0   0   0   0   5   0   0   0   0   0]
 [  0   0   0   0 182  45   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.44349680170575

F1 scores:
[       nan 1.         0.9578714  0.98454746 0.88135593 0.85196375
 1.         0.9039548  0.99359795 1.         1.         1.
 1.         1.        ]

Kappa:
0.9826722229057554
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7a66cc58d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.498, val_acc:0.468]
Epoch [2/120    avg_loss:2.168, val_acc:0.546]
Epoch [3/120    avg_loss:1.939, val_acc:0.587]
Epoch [4/120    avg_loss:1.710, val_acc:0.645]
Epoch [5/120    avg_loss:1.546, val_acc:0.694]
Epoch [6/120    avg_loss:1.341, val_acc:0.653]
Epoch [7/120    avg_loss:1.175, val_acc:0.798]
Epoch [8/120    avg_loss:1.010, val_acc:0.855]
Epoch [9/120    avg_loss:0.896, val_acc:0.796]
Epoch [10/120    avg_loss:0.805, val_acc:0.855]
Epoch [11/120    avg_loss:0.726, val_acc:0.819]
Epoch [12/120    avg_loss:0.657, val_acc:0.853]
Epoch [13/120    avg_loss:0.685, val_acc:0.879]
Epoch [14/120    avg_loss:0.594, val_acc:0.899]
Epoch [15/120    avg_loss:0.547, val_acc:0.885]
Epoch [16/120    avg_loss:0.435, val_acc:0.897]
Epoch [17/120    avg_loss:0.424, val_acc:0.921]
Epoch [18/120    avg_loss:0.385, val_acc:0.907]
Epoch [19/120    avg_loss:0.431, val_acc:0.923]
Epoch [20/120    avg_loss:0.420, val_acc:0.911]
Epoch [21/120    avg_loss:0.338, val_acc:0.889]
Epoch [22/120    avg_loss:0.378, val_acc:0.915]
Epoch [23/120    avg_loss:0.351, val_acc:0.913]
Epoch [24/120    avg_loss:0.395, val_acc:0.935]
Epoch [25/120    avg_loss:0.293, val_acc:0.935]
Epoch [26/120    avg_loss:0.251, val_acc:0.905]
Epoch [27/120    avg_loss:0.300, val_acc:0.938]
Epoch [28/120    avg_loss:0.255, val_acc:0.958]
Epoch [29/120    avg_loss:0.261, val_acc:0.944]
Epoch [30/120    avg_loss:0.249, val_acc:0.923]
Epoch [31/120    avg_loss:0.248, val_acc:0.940]
Epoch [32/120    avg_loss:0.232, val_acc:0.950]
Epoch [33/120    avg_loss:0.207, val_acc:0.954]
Epoch [34/120    avg_loss:0.207, val_acc:0.966]
Epoch [35/120    avg_loss:0.187, val_acc:0.956]
Epoch [36/120    avg_loss:0.162, val_acc:0.956]
Epoch [37/120    avg_loss:0.157, val_acc:0.938]
Epoch [38/120    avg_loss:0.275, val_acc:0.952]
Epoch [39/120    avg_loss:0.179, val_acc:0.942]
Epoch [40/120    avg_loss:0.187, val_acc:0.929]
Epoch [41/120    avg_loss:0.210, val_acc:0.962]
Epoch [42/120    avg_loss:0.151, val_acc:0.954]
Epoch [43/120    avg_loss:0.184, val_acc:0.960]
Epoch [44/120    avg_loss:0.133, val_acc:0.964]
Epoch [45/120    avg_loss:0.123, val_acc:0.964]
Epoch [46/120    avg_loss:0.138, val_acc:0.968]
Epoch [47/120    avg_loss:0.159, val_acc:0.960]
Epoch [48/120    avg_loss:0.180, val_acc:0.952]
Epoch [49/120    avg_loss:0.148, val_acc:0.948]
Epoch [50/120    avg_loss:0.114, val_acc:0.966]
Epoch [51/120    avg_loss:0.125, val_acc:0.966]
Epoch [52/120    avg_loss:0.102, val_acc:0.948]
Epoch [53/120    avg_loss:0.113, val_acc:0.974]
Epoch [54/120    avg_loss:0.080, val_acc:0.976]
Epoch [55/120    avg_loss:0.076, val_acc:0.974]
Epoch [56/120    avg_loss:0.066, val_acc:0.974]
Epoch [57/120    avg_loss:0.059, val_acc:0.978]
Epoch [58/120    avg_loss:0.095, val_acc:0.964]
Epoch [59/120    avg_loss:0.174, val_acc:0.948]
Epoch [60/120    avg_loss:0.109, val_acc:0.954]
Epoch [61/120    avg_loss:0.079, val_acc:0.954]
Epoch [62/120    avg_loss:0.080, val_acc:0.974]
Epoch [63/120    avg_loss:0.064, val_acc:0.966]
Epoch [64/120    avg_loss:0.057, val_acc:0.948]
Epoch [65/120    avg_loss:0.119, val_acc:0.956]
Epoch [66/120    avg_loss:0.102, val_acc:0.958]
Epoch [67/120    avg_loss:0.065, val_acc:0.982]
Epoch [68/120    avg_loss:0.046, val_acc:0.978]
Epoch [69/120    avg_loss:0.051, val_acc:0.982]
Epoch [70/120    avg_loss:0.058, val_acc:0.966]
Epoch [71/120    avg_loss:0.058, val_acc:0.964]
Epoch [72/120    avg_loss:0.054, val_acc:0.970]
Epoch [73/120    avg_loss:0.049, val_acc:0.976]
Epoch [74/120    avg_loss:0.062, val_acc:0.972]
Epoch [75/120    avg_loss:0.047, val_acc:0.982]
Epoch [76/120    avg_loss:0.046, val_acc:0.950]
Epoch [77/120    avg_loss:0.063, val_acc:0.980]
Epoch [78/120    avg_loss:0.035, val_acc:0.980]
Epoch [79/120    avg_loss:0.035, val_acc:0.978]
Epoch [80/120    avg_loss:0.033, val_acc:0.982]
Epoch [81/120    avg_loss:0.049, val_acc:0.962]
Epoch [82/120    avg_loss:0.049, val_acc:0.964]
Epoch [83/120    avg_loss:0.080, val_acc:0.958]
Epoch [84/120    avg_loss:0.057, val_acc:0.970]
Epoch [85/120    avg_loss:0.036, val_acc:0.970]
Epoch [86/120    avg_loss:0.031, val_acc:0.966]
Epoch [87/120    avg_loss:0.034, val_acc:0.966]
Epoch [88/120    avg_loss:0.045, val_acc:0.968]
Epoch [89/120    avg_loss:0.028, val_acc:0.974]
Epoch [90/120    avg_loss:0.026, val_acc:0.976]
Epoch [91/120    avg_loss:0.057, val_acc:0.972]
Epoch [92/120    avg_loss:0.038, val_acc:0.974]
Epoch [93/120    avg_loss:0.021, val_acc:0.974]
Epoch [94/120    avg_loss:0.017, val_acc:0.974]
Epoch [95/120    avg_loss:0.017, val_acc:0.978]
Epoch [96/120    avg_loss:0.018, val_acc:0.978]
Epoch [97/120    avg_loss:0.015, val_acc:0.980]
Epoch [98/120    avg_loss:0.016, val_acc:0.980]
Epoch [99/120    avg_loss:0.017, val_acc:0.980]
Epoch [100/120    avg_loss:0.017, val_acc:0.980]
Epoch [101/120    avg_loss:0.015, val_acc:0.984]
Epoch [102/120    avg_loss:0.013, val_acc:0.984]
Epoch [103/120    avg_loss:0.014, val_acc:0.984]
Epoch [104/120    avg_loss:0.019, val_acc:0.982]
Epoch [105/120    avg_loss:0.015, val_acc:0.982]
Epoch [106/120    avg_loss:0.012, val_acc:0.982]
Epoch [107/120    avg_loss:0.014, val_acc:0.982]
Epoch [108/120    avg_loss:0.010, val_acc:0.982]
Epoch [109/120    avg_loss:0.011, val_acc:0.982]
Epoch [110/120    avg_loss:0.013, val_acc:0.982]
Epoch [111/120    avg_loss:0.011, val_acc:0.982]
Epoch [112/120    avg_loss:0.012, val_acc:0.982]
Epoch [113/120    avg_loss:0.013, val_acc:0.982]
Epoch [114/120    avg_loss:0.011, val_acc:0.982]
Epoch [115/120    avg_loss:0.012, val_acc:0.982]
Epoch [116/120    avg_loss:0.012, val_acc:0.984]
Epoch [117/120    avg_loss:0.016, val_acc:0.982]
Epoch [118/120    avg_loss:0.014, val_acc:0.982]
Epoch [119/120    avg_loss:0.013, val_acc:0.982]
Epoch [120/120    avg_loss:0.011, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 208  17   0   0   0   0   3   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.63539445628997

F1 scores:
[       nan 1.         0.98426966 0.94977169 0.90376569 0.8975265
 1.         0.9726776  1.         0.99680511 1.         0.98950131
 0.99109131 1.        ]

Kappa:
0.9848066394823486
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f894a49c908>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.460, val_acc:0.290]
Epoch [2/120    avg_loss:2.138, val_acc:0.484]
Epoch [3/120    avg_loss:1.907, val_acc:0.589]
Epoch [4/120    avg_loss:1.645, val_acc:0.679]
Epoch [5/120    avg_loss:1.444, val_acc:0.776]
Epoch [6/120    avg_loss:1.222, val_acc:0.762]
Epoch [7/120    avg_loss:1.075, val_acc:0.800]
Epoch [8/120    avg_loss:0.961, val_acc:0.827]
Epoch [9/120    avg_loss:0.858, val_acc:0.815]
Epoch [10/120    avg_loss:0.785, val_acc:0.839]
Epoch [11/120    avg_loss:0.728, val_acc:0.857]
Epoch [12/120    avg_loss:0.722, val_acc:0.851]
Epoch [13/120    avg_loss:0.598, val_acc:0.867]
Epoch [14/120    avg_loss:0.591, val_acc:0.871]
Epoch [15/120    avg_loss:0.655, val_acc:0.859]
Epoch [16/120    avg_loss:0.565, val_acc:0.881]
Epoch [17/120    avg_loss:0.467, val_acc:0.879]
Epoch [18/120    avg_loss:0.457, val_acc:0.853]
Epoch [19/120    avg_loss:0.496, val_acc:0.865]
Epoch [20/120    avg_loss:0.452, val_acc:0.889]
Epoch [21/120    avg_loss:0.404, val_acc:0.903]
Epoch [22/120    avg_loss:0.439, val_acc:0.909]
Epoch [23/120    avg_loss:0.448, val_acc:0.871]
Epoch [24/120    avg_loss:0.450, val_acc:0.905]
Epoch [25/120    avg_loss:0.351, val_acc:0.861]
Epoch [26/120    avg_loss:0.376, val_acc:0.901]
Epoch [27/120    avg_loss:0.374, val_acc:0.931]
Epoch [28/120    avg_loss:0.313, val_acc:0.929]
Epoch [29/120    avg_loss:0.293, val_acc:0.915]
Epoch [30/120    avg_loss:0.294, val_acc:0.929]
Epoch [31/120    avg_loss:0.254, val_acc:0.927]
Epoch [32/120    avg_loss:0.283, val_acc:0.931]
Epoch [33/120    avg_loss:0.300, val_acc:0.931]
Epoch [34/120    avg_loss:0.248, val_acc:0.857]
Epoch [35/120    avg_loss:0.325, val_acc:0.907]
Epoch [36/120    avg_loss:0.182, val_acc:0.948]
Epoch [37/120    avg_loss:0.222, val_acc:0.944]
Epoch [38/120    avg_loss:0.253, val_acc:0.917]
Epoch [39/120    avg_loss:0.239, val_acc:0.940]
Epoch [40/120    avg_loss:0.190, val_acc:0.940]
Epoch [41/120    avg_loss:0.205, val_acc:0.915]
Epoch [42/120    avg_loss:0.326, val_acc:0.879]
Epoch [43/120    avg_loss:0.289, val_acc:0.933]
Epoch [44/120    avg_loss:0.277, val_acc:0.933]
Epoch [45/120    avg_loss:0.208, val_acc:0.944]
Epoch [46/120    avg_loss:0.173, val_acc:0.952]
Epoch [47/120    avg_loss:0.168, val_acc:0.938]
Epoch [48/120    avg_loss:0.186, val_acc:0.940]
Epoch [49/120    avg_loss:0.196, val_acc:0.950]
Epoch [50/120    avg_loss:0.169, val_acc:0.952]
Epoch [51/120    avg_loss:0.196, val_acc:0.948]
Epoch [52/120    avg_loss:0.198, val_acc:0.954]
Epoch [53/120    avg_loss:0.179, val_acc:0.944]
Epoch [54/120    avg_loss:0.175, val_acc:0.950]
Epoch [55/120    avg_loss:0.116, val_acc:0.972]
Epoch [56/120    avg_loss:0.108, val_acc:0.964]
Epoch [57/120    avg_loss:0.103, val_acc:0.980]
Epoch [58/120    avg_loss:0.088, val_acc:0.974]
Epoch [59/120    avg_loss:0.102, val_acc:0.940]
Epoch [60/120    avg_loss:0.125, val_acc:0.966]
Epoch [61/120    avg_loss:0.137, val_acc:0.942]
Epoch [62/120    avg_loss:0.172, val_acc:0.942]
Epoch [63/120    avg_loss:0.142, val_acc:0.964]
Epoch [64/120    avg_loss:0.077, val_acc:0.978]
Epoch [65/120    avg_loss:0.132, val_acc:0.954]
Epoch [66/120    avg_loss:0.119, val_acc:0.966]
Epoch [67/120    avg_loss:0.088, val_acc:0.974]
Epoch [68/120    avg_loss:0.071, val_acc:0.974]
Epoch [69/120    avg_loss:0.054, val_acc:0.956]
Epoch [70/120    avg_loss:0.071, val_acc:0.986]
Epoch [71/120    avg_loss:0.146, val_acc:0.960]
Epoch [72/120    avg_loss:0.081, val_acc:0.978]
Epoch [73/120    avg_loss:0.060, val_acc:0.988]
Epoch [74/120    avg_loss:0.073, val_acc:0.980]
Epoch [75/120    avg_loss:0.059, val_acc:0.978]
Epoch [76/120    avg_loss:0.060, val_acc:0.970]
Epoch [77/120    avg_loss:0.041, val_acc:0.980]
Epoch [78/120    avg_loss:0.076, val_acc:0.978]
Epoch [79/120    avg_loss:0.077, val_acc:0.970]
Epoch [80/120    avg_loss:0.087, val_acc:0.982]
Epoch [81/120    avg_loss:0.036, val_acc:0.982]
Epoch [82/120    avg_loss:0.053, val_acc:0.982]
Epoch [83/120    avg_loss:0.107, val_acc:0.974]
Epoch [84/120    avg_loss:0.110, val_acc:0.956]
Epoch [85/120    avg_loss:0.077, val_acc:0.966]
Epoch [86/120    avg_loss:0.059, val_acc:0.982]
Epoch [87/120    avg_loss:0.036, val_acc:0.988]
Epoch [88/120    avg_loss:0.031, val_acc:0.988]
Epoch [89/120    avg_loss:0.029, val_acc:0.988]
Epoch [90/120    avg_loss:0.025, val_acc:0.988]
Epoch [91/120    avg_loss:0.027, val_acc:0.988]
Epoch [92/120    avg_loss:0.027, val_acc:0.988]
Epoch [93/120    avg_loss:0.024, val_acc:0.988]
Epoch [94/120    avg_loss:0.029, val_acc:0.988]
Epoch [95/120    avg_loss:0.028, val_acc:0.988]
Epoch [96/120    avg_loss:0.025, val_acc:0.988]
Epoch [97/120    avg_loss:0.021, val_acc:0.988]
Epoch [98/120    avg_loss:0.024, val_acc:0.988]
Epoch [99/120    avg_loss:0.026, val_acc:0.988]
Epoch [100/120    avg_loss:0.024, val_acc:0.988]
Epoch [101/120    avg_loss:0.024, val_acc:0.988]
Epoch [102/120    avg_loss:0.027, val_acc:0.988]
Epoch [103/120    avg_loss:0.025, val_acc:0.988]
Epoch [104/120    avg_loss:0.023, val_acc:0.988]
Epoch [105/120    avg_loss:0.021, val_acc:0.988]
Epoch [106/120    avg_loss:0.020, val_acc:0.988]
Epoch [107/120    avg_loss:0.022, val_acc:0.988]
Epoch [108/120    avg_loss:0.020, val_acc:0.988]
Epoch [109/120    avg_loss:0.018, val_acc:0.988]
Epoch [110/120    avg_loss:0.020, val_acc:0.988]
Epoch [111/120    avg_loss:0.019, val_acc:0.988]
Epoch [112/120    avg_loss:0.026, val_acc:0.988]
Epoch [113/120    avg_loss:0.016, val_acc:0.988]
Epoch [114/120    avg_loss:0.020, val_acc:0.988]
Epoch [115/120    avg_loss:0.019, val_acc:0.988]
Epoch [116/120    avg_loss:0.021, val_acc:0.988]
Epoch [117/120    avg_loss:0.022, val_acc:0.988]
Epoch [118/120    avg_loss:0.023, val_acc:0.988]
Epoch [119/120    avg_loss:0.019, val_acc:0.988]
Epoch [120/120    avg_loss:0.022, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 214  16   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 1.         0.98426966 0.96396396 0.92468619 0.92907801
 1.         0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.989791831207395
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f92c8c26908>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.479, val_acc:0.375]
Epoch [2/120    avg_loss:2.164, val_acc:0.571]
Epoch [3/120    avg_loss:1.919, val_acc:0.637]
Epoch [4/120    avg_loss:1.658, val_acc:0.665]
Epoch [5/120    avg_loss:1.423, val_acc:0.716]
Epoch [6/120    avg_loss:1.197, val_acc:0.782]
Epoch [7/120    avg_loss:1.087, val_acc:0.817]
Epoch [8/120    avg_loss:0.973, val_acc:0.762]
Epoch [9/120    avg_loss:0.866, val_acc:0.843]
Epoch [10/120    avg_loss:0.818, val_acc:0.869]
Epoch [11/120    avg_loss:0.717, val_acc:0.873]
Epoch [12/120    avg_loss:0.651, val_acc:0.869]
Epoch [13/120    avg_loss:0.609, val_acc:0.877]
Epoch [14/120    avg_loss:0.569, val_acc:0.877]
Epoch [15/120    avg_loss:0.545, val_acc:0.841]
Epoch [16/120    avg_loss:0.566, val_acc:0.893]
Epoch [17/120    avg_loss:0.452, val_acc:0.915]
Epoch [18/120    avg_loss:0.419, val_acc:0.925]
Epoch [19/120    avg_loss:0.437, val_acc:0.901]
Epoch [20/120    avg_loss:0.403, val_acc:0.927]
Epoch [21/120    avg_loss:0.405, val_acc:0.940]
Epoch [22/120    avg_loss:0.318, val_acc:0.917]
Epoch [23/120    avg_loss:0.389, val_acc:0.843]
Epoch [24/120    avg_loss:0.347, val_acc:0.907]
Epoch [25/120    avg_loss:0.354, val_acc:0.919]
Epoch [26/120    avg_loss:0.330, val_acc:0.905]
Epoch [27/120    avg_loss:0.321, val_acc:0.927]
Epoch [28/120    avg_loss:0.292, val_acc:0.940]
Epoch [29/120    avg_loss:0.324, val_acc:0.952]
Epoch [30/120    avg_loss:0.253, val_acc:0.946]
Epoch [31/120    avg_loss:0.243, val_acc:0.925]
Epoch [32/120    avg_loss:0.269, val_acc:0.931]
Epoch [33/120    avg_loss:0.223, val_acc:0.938]
Epoch [34/120    avg_loss:0.266, val_acc:0.940]
Epoch [35/120    avg_loss:0.224, val_acc:0.946]
Epoch [36/120    avg_loss:0.243, val_acc:0.919]
Epoch [37/120    avg_loss:0.265, val_acc:0.954]
Epoch [38/120    avg_loss:0.232, val_acc:0.956]
Epoch [39/120    avg_loss:0.162, val_acc:0.972]
Epoch [40/120    avg_loss:0.199, val_acc:0.970]
Epoch [41/120    avg_loss:0.184, val_acc:0.966]
Epoch [42/120    avg_loss:0.169, val_acc:0.970]
Epoch [43/120    avg_loss:0.154, val_acc:0.960]
Epoch [44/120    avg_loss:0.140, val_acc:0.982]
Epoch [45/120    avg_loss:0.162, val_acc:0.976]
Epoch [46/120    avg_loss:0.132, val_acc:0.968]
Epoch [47/120    avg_loss:0.160, val_acc:0.972]
Epoch [48/120    avg_loss:0.150, val_acc:0.978]
Epoch [49/120    avg_loss:0.231, val_acc:0.950]
Epoch [50/120    avg_loss:0.201, val_acc:0.956]
Epoch [51/120    avg_loss:0.216, val_acc:0.946]
Epoch [52/120    avg_loss:0.203, val_acc:0.986]
Epoch [53/120    avg_loss:0.114, val_acc:0.966]
Epoch [54/120    avg_loss:0.108, val_acc:0.960]
Epoch [55/120    avg_loss:0.126, val_acc:0.970]
Epoch [56/120    avg_loss:0.116, val_acc:0.972]
Epoch [57/120    avg_loss:0.149, val_acc:0.964]
Epoch [58/120    avg_loss:0.162, val_acc:0.958]
Epoch [59/120    avg_loss:0.114, val_acc:0.988]
Epoch [60/120    avg_loss:0.120, val_acc:0.986]
Epoch [61/120    avg_loss:0.104, val_acc:0.968]
Epoch [62/120    avg_loss:0.092, val_acc:0.972]
Epoch [63/120    avg_loss:0.071, val_acc:0.980]
Epoch [64/120    avg_loss:0.079, val_acc:0.980]
Epoch [65/120    avg_loss:0.088, val_acc:0.988]
Epoch [66/120    avg_loss:0.062, val_acc:0.984]
Epoch [67/120    avg_loss:0.093, val_acc:0.970]
Epoch [68/120    avg_loss:0.096, val_acc:0.966]
Epoch [69/120    avg_loss:0.052, val_acc:0.988]
Epoch [70/120    avg_loss:0.069, val_acc:0.978]
Epoch [71/120    avg_loss:0.065, val_acc:0.980]
Epoch [72/120    avg_loss:0.069, val_acc:0.986]
Epoch [73/120    avg_loss:0.044, val_acc:0.988]
Epoch [74/120    avg_loss:0.040, val_acc:0.986]
Epoch [75/120    avg_loss:0.047, val_acc:0.990]
Epoch [76/120    avg_loss:0.035, val_acc:0.982]
Epoch [77/120    avg_loss:0.035, val_acc:0.988]
Epoch [78/120    avg_loss:0.075, val_acc:0.976]
Epoch [79/120    avg_loss:0.055, val_acc:0.994]
Epoch [80/120    avg_loss:0.047, val_acc:0.956]
Epoch [81/120    avg_loss:0.102, val_acc:0.984]
Epoch [82/120    avg_loss:0.091, val_acc:0.927]
Epoch [83/120    avg_loss:0.094, val_acc:0.974]
Epoch [84/120    avg_loss:0.046, val_acc:0.990]
Epoch [85/120    avg_loss:0.098, val_acc:0.982]
Epoch [86/120    avg_loss:0.101, val_acc:0.978]
Epoch [87/120    avg_loss:0.089, val_acc:0.960]
Epoch [88/120    avg_loss:0.095, val_acc:0.986]
Epoch [89/120    avg_loss:0.064, val_acc:0.994]
Epoch [90/120    avg_loss:0.039, val_acc:0.990]
Epoch [91/120    avg_loss:0.032, val_acc:0.986]
Epoch [92/120    avg_loss:0.023, val_acc:0.992]
Epoch [93/120    avg_loss:0.039, val_acc:0.992]
Epoch [94/120    avg_loss:0.022, val_acc:0.990]
Epoch [95/120    avg_loss:0.039, val_acc:0.992]
Epoch [96/120    avg_loss:0.090, val_acc:0.992]
Epoch [97/120    avg_loss:0.036, val_acc:0.994]
Epoch [98/120    avg_loss:0.026, val_acc:0.990]
Epoch [99/120    avg_loss:0.041, val_acc:0.984]
Epoch [100/120    avg_loss:0.036, val_acc:0.988]
Epoch [101/120    avg_loss:0.034, val_acc:0.990]
Epoch [102/120    avg_loss:0.021, val_acc:0.994]
Epoch [103/120    avg_loss:0.015, val_acc:0.992]
Epoch [104/120    avg_loss:0.014, val_acc:0.994]
Epoch [105/120    avg_loss:0.017, val_acc:0.986]
Epoch [106/120    avg_loss:0.014, val_acc:0.992]
Epoch [107/120    avg_loss:0.022, val_acc:0.998]
Epoch [108/120    avg_loss:0.023, val_acc:0.986]
Epoch [109/120    avg_loss:0.020, val_acc:0.996]
Epoch [110/120    avg_loss:0.047, val_acc:0.990]
Epoch [111/120    avg_loss:0.030, val_acc:0.990]
Epoch [112/120    avg_loss:0.018, val_acc:0.988]
Epoch [113/120    avg_loss:0.018, val_acc:0.994]
Epoch [114/120    avg_loss:0.016, val_acc:0.992]
Epoch [115/120    avg_loss:0.018, val_acc:0.992]
Epoch [116/120    avg_loss:0.022, val_acc:0.992]
Epoch [117/120    avg_loss:0.027, val_acc:0.992]
Epoch [118/120    avg_loss:0.018, val_acc:0.992]
Epoch [119/120    avg_loss:0.013, val_acc:0.994]
Epoch [120/120    avg_loss:0.018, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 669   0   0   0   0  16   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   0   7   0   0   0   0   0]
 [  0   0   0 226   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8912579957356

F1 scores:
[       nan 0.98818316 0.9837587  0.99122807 0.93569845 0.91582492
 0.96261682 1.         0.99106003 1.         1.         1.
 1.         1.        ]

Kappa:
0.987660578061542
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f331cb80898>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.416, val_acc:0.407]
Epoch [2/120    avg_loss:2.141, val_acc:0.486]
Epoch [3/120    avg_loss:1.935, val_acc:0.571]
Epoch [4/120    avg_loss:1.718, val_acc:0.562]
Epoch [5/120    avg_loss:1.509, val_acc:0.667]
Epoch [6/120    avg_loss:1.352, val_acc:0.730]
Epoch [7/120    avg_loss:1.184, val_acc:0.778]
Epoch [8/120    avg_loss:1.024, val_acc:0.754]
Epoch [9/120    avg_loss:0.967, val_acc:0.738]
Epoch [10/120    avg_loss:0.882, val_acc:0.817]
Epoch [11/120    avg_loss:0.794, val_acc:0.752]
Epoch [12/120    avg_loss:0.758, val_acc:0.861]
Epoch [13/120    avg_loss:0.714, val_acc:0.845]
Epoch [14/120    avg_loss:0.615, val_acc:0.857]
Epoch [15/120    avg_loss:0.602, val_acc:0.893]
Epoch [16/120    avg_loss:0.598, val_acc:0.879]
Epoch [17/120    avg_loss:0.536, val_acc:0.881]
Epoch [18/120    avg_loss:0.478, val_acc:0.905]
Epoch [19/120    avg_loss:0.461, val_acc:0.942]
Epoch [20/120    avg_loss:0.403, val_acc:0.901]
Epoch [21/120    avg_loss:0.381, val_acc:0.921]
Epoch [22/120    avg_loss:0.401, val_acc:0.919]
Epoch [23/120    avg_loss:0.383, val_acc:0.891]
Epoch [24/120    avg_loss:0.410, val_acc:0.885]
Epoch [25/120    avg_loss:0.406, val_acc:0.895]
Epoch [26/120    avg_loss:0.357, val_acc:0.927]
Epoch [27/120    avg_loss:0.336, val_acc:0.907]
Epoch [28/120    avg_loss:0.380, val_acc:0.913]
Epoch [29/120    avg_loss:0.332, val_acc:0.942]
Epoch [30/120    avg_loss:0.280, val_acc:0.929]
Epoch [31/120    avg_loss:0.388, val_acc:0.935]
Epoch [32/120    avg_loss:0.309, val_acc:0.946]
Epoch [33/120    avg_loss:0.268, val_acc:0.962]
Epoch [34/120    avg_loss:0.235, val_acc:0.948]
Epoch [35/120    avg_loss:0.277, val_acc:0.962]
Epoch [36/120    avg_loss:0.280, val_acc:0.960]
Epoch [37/120    avg_loss:0.203, val_acc:0.952]
Epoch [38/120    avg_loss:0.241, val_acc:0.946]
Epoch [39/120    avg_loss:0.232, val_acc:0.962]
Epoch [40/120    avg_loss:0.202, val_acc:0.958]
Epoch [41/120    avg_loss:0.211, val_acc:0.960]
Epoch [42/120    avg_loss:0.221, val_acc:0.931]
Epoch [43/120    avg_loss:0.199, val_acc:0.944]
Epoch [44/120    avg_loss:0.242, val_acc:0.958]
Epoch [45/120    avg_loss:0.190, val_acc:0.968]
Epoch [46/120    avg_loss:0.156, val_acc:0.966]
Epoch [47/120    avg_loss:0.162, val_acc:0.974]
Epoch [48/120    avg_loss:0.182, val_acc:0.956]
Epoch [49/120    avg_loss:0.193, val_acc:0.970]
Epoch [50/120    avg_loss:0.192, val_acc:0.964]
Epoch [51/120    avg_loss:0.173, val_acc:0.970]
Epoch [52/120    avg_loss:0.143, val_acc:0.974]
Epoch [53/120    avg_loss:0.141, val_acc:0.964]
Epoch [54/120    avg_loss:0.139, val_acc:0.974]
Epoch [55/120    avg_loss:0.147, val_acc:0.966]
Epoch [56/120    avg_loss:0.089, val_acc:0.968]
Epoch [57/120    avg_loss:0.113, val_acc:0.974]
Epoch [58/120    avg_loss:0.105, val_acc:0.980]
Epoch [59/120    avg_loss:0.087, val_acc:0.970]
Epoch [60/120    avg_loss:0.096, val_acc:0.980]
Epoch [61/120    avg_loss:0.105, val_acc:0.982]
Epoch [62/120    avg_loss:0.107, val_acc:0.980]
Epoch [63/120    avg_loss:0.099, val_acc:0.970]
Epoch [64/120    avg_loss:0.111, val_acc:0.966]
Epoch [65/120    avg_loss:0.123, val_acc:0.966]
Epoch [66/120    avg_loss:0.144, val_acc:0.976]
Epoch [67/120    avg_loss:0.071, val_acc:0.978]
Epoch [68/120    avg_loss:0.074, val_acc:0.982]
Epoch [69/120    avg_loss:0.124, val_acc:0.970]
Epoch [70/120    avg_loss:0.060, val_acc:0.980]
Epoch [71/120    avg_loss:0.079, val_acc:0.980]
Epoch [72/120    avg_loss:0.088, val_acc:0.972]
Epoch [73/120    avg_loss:0.119, val_acc:0.962]
Epoch [74/120    avg_loss:0.247, val_acc:0.954]
Epoch [75/120    avg_loss:0.137, val_acc:0.960]
Epoch [76/120    avg_loss:0.120, val_acc:0.972]
Epoch [77/120    avg_loss:0.069, val_acc:0.986]
Epoch [78/120    avg_loss:0.076, val_acc:0.980]
Epoch [79/120    avg_loss:0.069, val_acc:0.986]
Epoch [80/120    avg_loss:0.080, val_acc:0.978]
Epoch [81/120    avg_loss:0.083, val_acc:0.956]
Epoch [82/120    avg_loss:0.097, val_acc:0.966]
Epoch [83/120    avg_loss:0.077, val_acc:0.982]
Epoch [84/120    avg_loss:0.056, val_acc:0.982]
Epoch [85/120    avg_loss:0.053, val_acc:0.986]
Epoch [86/120    avg_loss:0.085, val_acc:0.974]
Epoch [87/120    avg_loss:0.070, val_acc:0.976]
Epoch [88/120    avg_loss:0.085, val_acc:0.966]
Epoch [89/120    avg_loss:0.052, val_acc:0.978]
Epoch [90/120    avg_loss:0.064, val_acc:0.978]
Epoch [91/120    avg_loss:0.032, val_acc:0.984]
Epoch [92/120    avg_loss:0.042, val_acc:0.986]
Epoch [93/120    avg_loss:0.033, val_acc:0.984]
Epoch [94/120    avg_loss:0.037, val_acc:0.988]
Epoch [95/120    avg_loss:0.034, val_acc:0.980]
Epoch [96/120    avg_loss:0.024, val_acc:0.986]
Epoch [97/120    avg_loss:0.020, val_acc:0.988]
Epoch [98/120    avg_loss:0.029, val_acc:0.988]
Epoch [99/120    avg_loss:0.025, val_acc:0.984]
Epoch [100/120    avg_loss:0.023, val_acc:0.980]
Epoch [101/120    avg_loss:0.031, val_acc:0.982]
Epoch [102/120    avg_loss:0.031, val_acc:0.980]
Epoch [103/120    avg_loss:0.029, val_acc:0.982]
Epoch [104/120    avg_loss:0.017, val_acc:0.984]
Epoch [105/120    avg_loss:0.021, val_acc:0.974]
Epoch [106/120    avg_loss:0.031, val_acc:0.980]
Epoch [107/120    avg_loss:0.061, val_acc:0.986]
Epoch [108/120    avg_loss:0.032, val_acc:0.988]
Epoch [109/120    avg_loss:0.067, val_acc:0.980]
Epoch [110/120    avg_loss:0.044, val_acc:0.980]
Epoch [111/120    avg_loss:0.073, val_acc:0.974]
Epoch [112/120    avg_loss:0.033, val_acc:0.984]
Epoch [113/120    avg_loss:0.030, val_acc:0.986]
Epoch [114/120    avg_loss:0.038, val_acc:0.984]
Epoch [115/120    avg_loss:0.020, val_acc:0.984]
Epoch [116/120    avg_loss:0.029, val_acc:0.982]
Epoch [117/120    avg_loss:0.029, val_acc:0.988]
Epoch [118/120    avg_loss:0.032, val_acc:0.982]
Epoch [119/120    avg_loss:0.019, val_acc:0.984]
Epoch [120/120    avg_loss:0.020, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   6 208  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  17 436   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.78464818763327

F1 scores:
[       nan 1.         0.98206278 0.98712446 0.92857143 0.91408935
 0.99756691 0.95555556 1.         1.         1.         0.97795071
 0.98087739 1.        ]

Kappa:
0.9864694864979566
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc9b3b9f048>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.467, val_acc:0.440]
Epoch [2/120    avg_loss:2.117, val_acc:0.579]
Epoch [3/120    avg_loss:1.860, val_acc:0.631]
Epoch [4/120    avg_loss:1.610, val_acc:0.679]
Epoch [5/120    avg_loss:1.374, val_acc:0.717]
Epoch [6/120    avg_loss:1.206, val_acc:0.758]
Epoch [7/120    avg_loss:1.088, val_acc:0.794]
Epoch [8/120    avg_loss:0.978, val_acc:0.787]
Epoch [9/120    avg_loss:0.866, val_acc:0.783]
Epoch [10/120    avg_loss:0.804, val_acc:0.852]
Epoch [11/120    avg_loss:0.696, val_acc:0.887]
Epoch [12/120    avg_loss:0.641, val_acc:0.873]
Epoch [13/120    avg_loss:0.575, val_acc:0.887]
Epoch [14/120    avg_loss:0.489, val_acc:0.890]
Epoch [15/120    avg_loss:0.517, val_acc:0.898]
Epoch [16/120    avg_loss:0.480, val_acc:0.875]
Epoch [17/120    avg_loss:0.393, val_acc:0.919]
Epoch [18/120    avg_loss:0.400, val_acc:0.917]
Epoch [19/120    avg_loss:0.424, val_acc:0.848]
Epoch [20/120    avg_loss:0.389, val_acc:0.919]
Epoch [21/120    avg_loss:0.313, val_acc:0.915]
Epoch [22/120    avg_loss:0.346, val_acc:0.940]
Epoch [23/120    avg_loss:0.309, val_acc:0.917]
Epoch [24/120    avg_loss:0.310, val_acc:0.938]
Epoch [25/120    avg_loss:0.272, val_acc:0.915]
Epoch [26/120    avg_loss:0.337, val_acc:0.940]
Epoch [27/120    avg_loss:0.225, val_acc:0.923]
Epoch [28/120    avg_loss:0.257, val_acc:0.908]
Epoch [29/120    avg_loss:0.279, val_acc:0.902]
Epoch [30/120    avg_loss:0.252, val_acc:0.925]
Epoch [31/120    avg_loss:0.268, val_acc:0.940]
Epoch [32/120    avg_loss:0.248, val_acc:0.944]
Epoch [33/120    avg_loss:0.256, val_acc:0.929]
Epoch [34/120    avg_loss:0.240, val_acc:0.940]
Epoch [35/120    avg_loss:0.224, val_acc:0.940]
Epoch [36/120    avg_loss:0.222, val_acc:0.935]
Epoch [37/120    avg_loss:0.202, val_acc:0.956]
Epoch [38/120    avg_loss:0.215, val_acc:0.954]
Epoch [39/120    avg_loss:0.201, val_acc:0.965]
Epoch [40/120    avg_loss:0.225, val_acc:0.942]
Epoch [41/120    avg_loss:0.197, val_acc:0.954]
Epoch [42/120    avg_loss:0.191, val_acc:0.938]
Epoch [43/120    avg_loss:0.180, val_acc:0.944]
Epoch [44/120    avg_loss:0.188, val_acc:0.952]
Epoch [45/120    avg_loss:0.152, val_acc:0.942]
Epoch [46/120    avg_loss:0.152, val_acc:0.954]
Epoch [47/120    avg_loss:0.177, val_acc:0.965]
Epoch [48/120    avg_loss:0.147, val_acc:0.925]
Epoch [49/120    avg_loss:0.150, val_acc:0.975]
Epoch [50/120    avg_loss:0.107, val_acc:0.954]
Epoch [51/120    avg_loss:0.100, val_acc:0.967]
Epoch [52/120    avg_loss:0.113, val_acc:0.944]
Epoch [53/120    avg_loss:0.120, val_acc:0.965]
Epoch [54/120    avg_loss:0.125, val_acc:0.963]
Epoch [55/120    avg_loss:0.104, val_acc:0.960]
Epoch [56/120    avg_loss:0.109, val_acc:0.965]
Epoch [57/120    avg_loss:0.125, val_acc:0.965]
Epoch [58/120    avg_loss:0.097, val_acc:0.967]
Epoch [59/120    avg_loss:0.109, val_acc:0.983]
Epoch [60/120    avg_loss:0.071, val_acc:0.977]
Epoch [61/120    avg_loss:0.059, val_acc:0.954]
Epoch [62/120    avg_loss:0.089, val_acc:0.973]
Epoch [63/120    avg_loss:0.059, val_acc:0.977]
Epoch [64/120    avg_loss:0.057, val_acc:0.975]
Epoch [65/120    avg_loss:0.045, val_acc:0.979]
Epoch [66/120    avg_loss:0.078, val_acc:0.973]
Epoch [67/120    avg_loss:0.063, val_acc:0.979]
Epoch [68/120    avg_loss:0.096, val_acc:0.973]
Epoch [69/120    avg_loss:0.059, val_acc:0.977]
Epoch [70/120    avg_loss:0.056, val_acc:0.977]
Epoch [71/120    avg_loss:0.059, val_acc:0.979]
Epoch [72/120    avg_loss:0.072, val_acc:0.967]
Epoch [73/120    avg_loss:0.047, val_acc:0.983]
Epoch [74/120    avg_loss:0.039, val_acc:0.983]
Epoch [75/120    avg_loss:0.037, val_acc:0.981]
Epoch [76/120    avg_loss:0.038, val_acc:0.981]
Epoch [77/120    avg_loss:0.035, val_acc:0.979]
Epoch [78/120    avg_loss:0.034, val_acc:0.981]
Epoch [79/120    avg_loss:0.032, val_acc:0.981]
Epoch [80/120    avg_loss:0.030, val_acc:0.981]
Epoch [81/120    avg_loss:0.031, val_acc:0.981]
Epoch [82/120    avg_loss:0.030, val_acc:0.981]
Epoch [83/120    avg_loss:0.029, val_acc:0.977]
Epoch [84/120    avg_loss:0.031, val_acc:0.983]
Epoch [85/120    avg_loss:0.023, val_acc:0.981]
Epoch [86/120    avg_loss:0.028, val_acc:0.985]
Epoch [87/120    avg_loss:0.025, val_acc:0.981]
Epoch [88/120    avg_loss:0.023, val_acc:0.983]
Epoch [89/120    avg_loss:0.035, val_acc:0.985]
Epoch [90/120    avg_loss:0.030, val_acc:0.985]
Epoch [91/120    avg_loss:0.026, val_acc:0.985]
Epoch [92/120    avg_loss:0.023, val_acc:0.988]
Epoch [93/120    avg_loss:0.033, val_acc:0.988]
Epoch [94/120    avg_loss:0.022, val_acc:0.988]
Epoch [95/120    avg_loss:0.026, val_acc:0.988]
Epoch [96/120    avg_loss:0.026, val_acc:0.985]
Epoch [97/120    avg_loss:0.028, val_acc:0.983]
Epoch [98/120    avg_loss:0.031, val_acc:0.985]
Epoch [99/120    avg_loss:0.022, val_acc:0.985]
Epoch [100/120    avg_loss:0.023, val_acc:0.988]
Epoch [101/120    avg_loss:0.024, val_acc:0.988]
Epoch [102/120    avg_loss:0.021, val_acc:0.988]
Epoch [103/120    avg_loss:0.023, val_acc:0.988]
Epoch [104/120    avg_loss:0.027, val_acc:0.988]
Epoch [105/120    avg_loss:0.023, val_acc:0.988]
Epoch [106/120    avg_loss:0.022, val_acc:0.988]
Epoch [107/120    avg_loss:0.022, val_acc:0.985]
Epoch [108/120    avg_loss:0.024, val_acc:0.988]
Epoch [109/120    avg_loss:0.022, val_acc:0.988]
Epoch [110/120    avg_loss:0.026, val_acc:0.990]
Epoch [111/120    avg_loss:0.024, val_acc:0.988]
Epoch [112/120    avg_loss:0.024, val_acc:0.985]
Epoch [113/120    avg_loss:0.019, val_acc:0.988]
Epoch [114/120    avg_loss:0.023, val_acc:0.988]
Epoch [115/120    avg_loss:0.020, val_acc:0.983]
Epoch [116/120    avg_loss:0.021, val_acc:0.988]
Epoch [117/120    avg_loss:0.020, val_acc:0.988]
Epoch [118/120    avg_loss:0.023, val_acc:0.988]
Epoch [119/120    avg_loss:0.024, val_acc:0.988]
Epoch [120/120    avg_loss:0.024, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  15   0   0   0   0   0   0   2   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 1.         0.98206278 1.         0.95890411 0.94736842
 1.         0.95555556 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9938279872081315
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbe2ef43940>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.448, val_acc:0.438]
Epoch [2/120    avg_loss:2.129, val_acc:0.577]
Epoch [3/120    avg_loss:1.882, val_acc:0.583]
Epoch [4/120    avg_loss:1.654, val_acc:0.633]
Epoch [5/120    avg_loss:1.413, val_acc:0.722]
Epoch [6/120    avg_loss:1.227, val_acc:0.708]
Epoch [7/120    avg_loss:1.067, val_acc:0.784]
Epoch [8/120    avg_loss:0.964, val_acc:0.857]
Epoch [9/120    avg_loss:0.823, val_acc:0.857]
Epoch [10/120    avg_loss:0.729, val_acc:0.883]
Epoch [11/120    avg_loss:0.653, val_acc:0.879]
Epoch [12/120    avg_loss:0.638, val_acc:0.901]
Epoch [13/120    avg_loss:0.572, val_acc:0.903]
Epoch [14/120    avg_loss:0.597, val_acc:0.853]
Epoch [15/120    avg_loss:0.510, val_acc:0.899]
Epoch [16/120    avg_loss:0.506, val_acc:0.913]
Epoch [17/120    avg_loss:0.434, val_acc:0.913]
Epoch [18/120    avg_loss:0.466, val_acc:0.927]
Epoch [19/120    avg_loss:0.439, val_acc:0.915]
Epoch [20/120    avg_loss:0.376, val_acc:0.921]
Epoch [21/120    avg_loss:0.377, val_acc:0.905]
Epoch [22/120    avg_loss:0.379, val_acc:0.944]
Epoch [23/120    avg_loss:0.393, val_acc:0.950]
Epoch [24/120    avg_loss:0.317, val_acc:0.923]
Epoch [25/120    avg_loss:0.291, val_acc:0.921]
Epoch [26/120    avg_loss:0.298, val_acc:0.950]
Epoch [27/120    avg_loss:0.300, val_acc:0.923]
Epoch [28/120    avg_loss:0.281, val_acc:0.956]
Epoch [29/120    avg_loss:0.280, val_acc:0.889]
Epoch [30/120    avg_loss:0.300, val_acc:0.935]
Epoch [31/120    avg_loss:0.276, val_acc:0.933]
Epoch [32/120    avg_loss:0.266, val_acc:0.956]
Epoch [33/120    avg_loss:0.272, val_acc:0.966]
Epoch [34/120    avg_loss:0.183, val_acc:0.954]
Epoch [35/120    avg_loss:0.180, val_acc:0.958]
Epoch [36/120    avg_loss:0.216, val_acc:0.954]
Epoch [37/120    avg_loss:0.221, val_acc:0.954]
Epoch [38/120    avg_loss:0.208, val_acc:0.960]
Epoch [39/120    avg_loss:0.178, val_acc:0.968]
Epoch [40/120    avg_loss:0.168, val_acc:0.952]
Epoch [41/120    avg_loss:0.182, val_acc:0.944]
Epoch [42/120    avg_loss:0.198, val_acc:0.954]
Epoch [43/120    avg_loss:0.160, val_acc:0.958]
Epoch [44/120    avg_loss:0.153, val_acc:0.962]
Epoch [45/120    avg_loss:0.198, val_acc:0.958]
Epoch [46/120    avg_loss:0.158, val_acc:0.984]
Epoch [47/120    avg_loss:0.148, val_acc:0.960]
Epoch [48/120    avg_loss:0.169, val_acc:0.970]
Epoch [49/120    avg_loss:0.172, val_acc:0.966]
Epoch [50/120    avg_loss:0.122, val_acc:0.968]
Epoch [51/120    avg_loss:0.092, val_acc:0.980]
Epoch [52/120    avg_loss:0.136, val_acc:0.970]
Epoch [53/120    avg_loss:0.135, val_acc:0.972]
Epoch [54/120    avg_loss:0.116, val_acc:0.972]
Epoch [55/120    avg_loss:0.090, val_acc:0.976]
Epoch [56/120    avg_loss:0.112, val_acc:0.980]
Epoch [57/120    avg_loss:0.124, val_acc:0.978]
Epoch [58/120    avg_loss:0.115, val_acc:0.968]
Epoch [59/120    avg_loss:0.102, val_acc:0.970]
Epoch [60/120    avg_loss:0.072, val_acc:0.984]
Epoch [61/120    avg_loss:0.053, val_acc:0.986]
Epoch [62/120    avg_loss:0.057, val_acc:0.988]
Epoch [63/120    avg_loss:0.061, val_acc:0.988]
Epoch [64/120    avg_loss:0.052, val_acc:0.986]
Epoch [65/120    avg_loss:0.045, val_acc:0.986]
Epoch [66/120    avg_loss:0.046, val_acc:0.986]
Epoch [67/120    avg_loss:0.055, val_acc:0.986]
Epoch [68/120    avg_loss:0.051, val_acc:0.986]
Epoch [69/120    avg_loss:0.052, val_acc:0.986]
Epoch [70/120    avg_loss:0.039, val_acc:0.988]
Epoch [71/120    avg_loss:0.050, val_acc:0.986]
Epoch [72/120    avg_loss:0.041, val_acc:0.988]
Epoch [73/120    avg_loss:0.040, val_acc:0.988]
Epoch [74/120    avg_loss:0.040, val_acc:0.988]
Epoch [75/120    avg_loss:0.041, val_acc:0.988]
Epoch [76/120    avg_loss:0.039, val_acc:0.988]
Epoch [77/120    avg_loss:0.039, val_acc:0.988]
Epoch [78/120    avg_loss:0.040, val_acc:0.988]
Epoch [79/120    avg_loss:0.044, val_acc:0.988]
Epoch [80/120    avg_loss:0.048, val_acc:0.988]
Epoch [81/120    avg_loss:0.041, val_acc:0.988]
Epoch [82/120    avg_loss:0.035, val_acc:0.988]
Epoch [83/120    avg_loss:0.036, val_acc:0.988]
Epoch [84/120    avg_loss:0.039, val_acc:0.988]
Epoch [85/120    avg_loss:0.056, val_acc:0.988]
Epoch [86/120    avg_loss:0.053, val_acc:0.988]
Epoch [87/120    avg_loss:0.039, val_acc:0.988]
Epoch [88/120    avg_loss:0.056, val_acc:0.986]
Epoch [89/120    avg_loss:0.050, val_acc:0.988]
Epoch [90/120    avg_loss:0.054, val_acc:0.988]
Epoch [91/120    avg_loss:0.039, val_acc:0.988]
Epoch [92/120    avg_loss:0.031, val_acc:0.988]
Epoch [93/120    avg_loss:0.035, val_acc:0.988]
Epoch [94/120    avg_loss:0.038, val_acc:0.988]
Epoch [95/120    avg_loss:0.048, val_acc:0.988]
Epoch [96/120    avg_loss:0.037, val_acc:0.988]
Epoch [97/120    avg_loss:0.037, val_acc:0.988]
Epoch [98/120    avg_loss:0.037, val_acc:0.988]
Epoch [99/120    avg_loss:0.038, val_acc:0.988]
Epoch [100/120    avg_loss:0.038, val_acc:0.988]
Epoch [101/120    avg_loss:0.038, val_acc:0.988]
Epoch [102/120    avg_loss:0.040, val_acc:0.990]
Epoch [103/120    avg_loss:0.035, val_acc:0.988]
Epoch [104/120    avg_loss:0.035, val_acc:0.988]
Epoch [105/120    avg_loss:0.034, val_acc:0.988]
Epoch [106/120    avg_loss:0.035, val_acc:0.988]
Epoch [107/120    avg_loss:0.036, val_acc:0.988]
Epoch [108/120    avg_loss:0.030, val_acc:0.988]
Epoch [109/120    avg_loss:0.041, val_acc:0.988]
Epoch [110/120    avg_loss:0.039, val_acc:0.988]
Epoch [111/120    avg_loss:0.039, val_acc:0.988]
Epoch [112/120    avg_loss:0.035, val_acc:0.988]
Epoch [113/120    avg_loss:0.037, val_acc:0.988]
Epoch [114/120    avg_loss:0.035, val_acc:0.988]
Epoch [115/120    avg_loss:0.036, val_acc:0.988]
Epoch [116/120    avg_loss:0.039, val_acc:0.988]
Epoch [117/120    avg_loss:0.031, val_acc:0.988]
Epoch [118/120    avg_loss:0.035, val_acc:0.988]
Epoch [119/120    avg_loss:0.038, val_acc:0.988]
Epoch [120/120    avg_loss:0.032, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   0   0   0   0   1   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  12   0   0   0   0   0   0   5   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 0.99780541 0.98419865 1.         0.93333333 0.91349481
 0.99277108 0.96703297 1.         1.         1.         1.
 0.99342105 1.        ]

Kappa:
0.9905043770215844
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa822e3d898>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.472, val_acc:0.337]
Epoch [2/120    avg_loss:2.168, val_acc:0.524]
Epoch [3/120    avg_loss:1.943, val_acc:0.595]
Epoch [4/120    avg_loss:1.686, val_acc:0.623]
Epoch [5/120    avg_loss:1.468, val_acc:0.669]
Epoch [6/120    avg_loss:1.282, val_acc:0.722]
Epoch [7/120    avg_loss:1.183, val_acc:0.758]
Epoch [8/120    avg_loss:1.007, val_acc:0.764]
Epoch [9/120    avg_loss:0.902, val_acc:0.859]
Epoch [10/120    avg_loss:0.761, val_acc:0.861]
Epoch [11/120    avg_loss:0.748, val_acc:0.879]
Epoch [12/120    avg_loss:0.684, val_acc:0.879]
Epoch [13/120    avg_loss:0.658, val_acc:0.885]
Epoch [14/120    avg_loss:0.655, val_acc:0.889]
Epoch [15/120    avg_loss:0.564, val_acc:0.899]
Epoch [16/120    avg_loss:0.481, val_acc:0.905]
Epoch [17/120    avg_loss:0.461, val_acc:0.905]
Epoch [18/120    avg_loss:0.434, val_acc:0.925]
Epoch [19/120    avg_loss:0.408, val_acc:0.887]
Epoch [20/120    avg_loss:0.436, val_acc:0.944]
Epoch [21/120    avg_loss:0.403, val_acc:0.875]
Epoch [22/120    avg_loss:0.394, val_acc:0.935]
Epoch [23/120    avg_loss:0.349, val_acc:0.901]
Epoch [24/120    avg_loss:0.338, val_acc:0.905]
Epoch [25/120    avg_loss:0.374, val_acc:0.940]
Epoch [26/120    avg_loss:0.328, val_acc:0.925]
Epoch [27/120    avg_loss:0.313, val_acc:0.956]
Epoch [28/120    avg_loss:0.292, val_acc:0.927]
Epoch [29/120    avg_loss:0.280, val_acc:0.944]
Epoch [30/120    avg_loss:0.332, val_acc:0.952]
Epoch [31/120    avg_loss:0.274, val_acc:0.956]
Epoch [32/120    avg_loss:0.254, val_acc:0.903]
Epoch [33/120    avg_loss:0.350, val_acc:0.964]
Epoch [34/120    avg_loss:0.203, val_acc:0.966]
Epoch [35/120    avg_loss:0.235, val_acc:0.958]
Epoch [36/120    avg_loss:0.261, val_acc:0.960]
Epoch [37/120    avg_loss:0.205, val_acc:0.970]
Epoch [38/120    avg_loss:0.196, val_acc:0.972]
Epoch [39/120    avg_loss:0.212, val_acc:0.960]
Epoch [40/120    avg_loss:0.186, val_acc:0.978]
Epoch [41/120    avg_loss:0.225, val_acc:0.970]
Epoch [42/120    avg_loss:0.157, val_acc:0.962]
Epoch [43/120    avg_loss:0.158, val_acc:0.968]
Epoch [44/120    avg_loss:0.149, val_acc:0.978]
Epoch [45/120    avg_loss:0.146, val_acc:0.970]
Epoch [46/120    avg_loss:0.279, val_acc:0.956]
Epoch [47/120    avg_loss:0.177, val_acc:0.978]
Epoch [48/120    avg_loss:0.187, val_acc:0.962]
Epoch [49/120    avg_loss:0.207, val_acc:0.978]
Epoch [50/120    avg_loss:0.134, val_acc:0.980]
Epoch [51/120    avg_loss:0.091, val_acc:0.978]
Epoch [52/120    avg_loss:0.112, val_acc:0.968]
Epoch [53/120    avg_loss:0.090, val_acc:0.986]
Epoch [54/120    avg_loss:0.107, val_acc:0.984]
Epoch [55/120    avg_loss:0.110, val_acc:0.958]
Epoch [56/120    avg_loss:0.128, val_acc:0.986]
Epoch [57/120    avg_loss:0.069, val_acc:0.988]
Epoch [58/120    avg_loss:0.098, val_acc:0.986]
Epoch [59/120    avg_loss:0.116, val_acc:0.962]
Epoch [60/120    avg_loss:0.103, val_acc:0.986]
Epoch [61/120    avg_loss:0.094, val_acc:0.988]
Epoch [62/120    avg_loss:0.133, val_acc:0.978]
Epoch [63/120    avg_loss:0.093, val_acc:0.990]
Epoch [64/120    avg_loss:0.077, val_acc:0.992]
Epoch [65/120    avg_loss:0.082, val_acc:0.992]
Epoch [66/120    avg_loss:0.110, val_acc:0.980]
Epoch [67/120    avg_loss:0.076, val_acc:0.994]
Epoch [68/120    avg_loss:0.060, val_acc:0.986]
Epoch [69/120    avg_loss:0.079, val_acc:0.980]
Epoch [70/120    avg_loss:0.054, val_acc:0.992]
Epoch [71/120    avg_loss:0.065, val_acc:0.982]
Epoch [72/120    avg_loss:0.082, val_acc:0.964]
Epoch [73/120    avg_loss:0.068, val_acc:0.968]
Epoch [74/120    avg_loss:0.070, val_acc:0.988]
Epoch [75/120    avg_loss:0.060, val_acc:0.990]
Epoch [76/120    avg_loss:0.036, val_acc:0.992]
Epoch [77/120    avg_loss:0.053, val_acc:0.994]
Epoch [78/120    avg_loss:0.062, val_acc:0.972]
Epoch [79/120    avg_loss:0.085, val_acc:0.992]
Epoch [80/120    avg_loss:0.060, val_acc:0.992]
Epoch [81/120    avg_loss:0.029, val_acc:0.990]
Epoch [82/120    avg_loss:0.036, val_acc:0.982]
Epoch [83/120    avg_loss:0.049, val_acc:0.974]
Epoch [84/120    avg_loss:0.082, val_acc:0.974]
Epoch [85/120    avg_loss:0.058, val_acc:0.986]
Epoch [86/120    avg_loss:0.075, val_acc:0.944]
Epoch [87/120    avg_loss:0.077, val_acc:0.986]
Epoch [88/120    avg_loss:0.057, val_acc:0.994]
Epoch [89/120    avg_loss:0.042, val_acc:0.986]
Epoch [90/120    avg_loss:0.045, val_acc:0.992]
Epoch [91/120    avg_loss:0.033, val_acc:0.994]
Epoch [92/120    avg_loss:0.032, val_acc:0.994]
Epoch [93/120    avg_loss:0.019, val_acc:0.994]
Epoch [94/120    avg_loss:0.022, val_acc:0.994]
Epoch [95/120    avg_loss:0.017, val_acc:0.994]
Epoch [96/120    avg_loss:0.024, val_acc:0.996]
Epoch [97/120    avg_loss:0.020, val_acc:0.996]
Epoch [98/120    avg_loss:0.026, val_acc:0.994]
Epoch [99/120    avg_loss:0.035, val_acc:0.986]
Epoch [100/120    avg_loss:0.033, val_acc:0.994]
Epoch [101/120    avg_loss:0.035, val_acc:0.994]
Epoch [102/120    avg_loss:0.026, val_acc:0.994]
Epoch [103/120    avg_loss:0.022, val_acc:0.992]
Epoch [104/120    avg_loss:0.027, val_acc:0.990]
Epoch [105/120    avg_loss:0.017, val_acc:0.994]
Epoch [106/120    avg_loss:0.021, val_acc:0.994]
Epoch [107/120    avg_loss:0.014, val_acc:0.996]
Epoch [108/120    avg_loss:0.011, val_acc:0.994]
Epoch [109/120    avg_loss:0.014, val_acc:0.994]
Epoch [110/120    avg_loss:0.037, val_acc:0.996]
Epoch [111/120    avg_loss:0.020, val_acc:0.994]
Epoch [112/120    avg_loss:0.015, val_acc:0.984]
Epoch [113/120    avg_loss:0.027, val_acc:0.990]
Epoch [114/120    avg_loss:0.024, val_acc:0.996]
Epoch [115/120    avg_loss:0.057, val_acc:0.990]
Epoch [116/120    avg_loss:0.035, val_acc:0.990]
Epoch [117/120    avg_loss:0.036, val_acc:0.980]
Epoch [118/120    avg_loss:0.031, val_acc:0.992]
Epoch [119/120    avg_loss:0.025, val_acc:0.992]
Epoch [120/120    avg_loss:0.027, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.48827292110875

F1 scores:
[       nan 1.         0.98426966 0.99563319 0.96412556 0.95333333
 1.         0.96132597 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9943028090400896
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5718ce08d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.505, val_acc:0.472]
Epoch [2/120    avg_loss:2.141, val_acc:0.573]
Epoch [3/120    avg_loss:1.839, val_acc:0.643]
Epoch [4/120    avg_loss:1.581, val_acc:0.655]
Epoch [5/120    avg_loss:1.341, val_acc:0.720]
Epoch [6/120    avg_loss:1.161, val_acc:0.792]
Epoch [7/120    avg_loss:1.016, val_acc:0.794]
Epoch [8/120    avg_loss:0.928, val_acc:0.827]
Epoch [9/120    avg_loss:0.818, val_acc:0.881]
Epoch [10/120    avg_loss:0.699, val_acc:0.845]
Epoch [11/120    avg_loss:0.692, val_acc:0.782]
Epoch [12/120    avg_loss:0.590, val_acc:0.891]
Epoch [13/120    avg_loss:0.513, val_acc:0.897]
Epoch [14/120    avg_loss:0.474, val_acc:0.919]
Epoch [15/120    avg_loss:0.466, val_acc:0.847]
Epoch [16/120    avg_loss:0.365, val_acc:0.895]
Epoch [17/120    avg_loss:0.482, val_acc:0.901]
Epoch [18/120    avg_loss:0.398, val_acc:0.897]
Epoch [19/120    avg_loss:0.367, val_acc:0.921]
Epoch [20/120    avg_loss:0.281, val_acc:0.948]
Epoch [21/120    avg_loss:0.283, val_acc:0.897]
Epoch [22/120    avg_loss:0.273, val_acc:0.927]
Epoch [23/120    avg_loss:0.231, val_acc:0.960]
Epoch [24/120    avg_loss:0.250, val_acc:0.952]
Epoch [25/120    avg_loss:0.242, val_acc:0.956]
Epoch [26/120    avg_loss:0.173, val_acc:0.948]
Epoch [27/120    avg_loss:0.191, val_acc:0.958]
Epoch [28/120    avg_loss:0.160, val_acc:0.946]
Epoch [29/120    avg_loss:0.197, val_acc:0.962]
Epoch [30/120    avg_loss:0.191, val_acc:0.950]
Epoch [31/120    avg_loss:0.184, val_acc:0.952]
Epoch [32/120    avg_loss:0.155, val_acc:0.970]
Epoch [33/120    avg_loss:0.142, val_acc:0.960]
Epoch [34/120    avg_loss:0.160, val_acc:0.972]
Epoch [35/120    avg_loss:0.150, val_acc:0.976]
Epoch [36/120    avg_loss:0.119, val_acc:0.988]
Epoch [37/120    avg_loss:0.127, val_acc:0.974]
Epoch [38/120    avg_loss:0.140, val_acc:0.962]
Epoch [39/120    avg_loss:0.114, val_acc:0.968]
Epoch [40/120    avg_loss:0.095, val_acc:0.946]
Epoch [41/120    avg_loss:0.110, val_acc:0.940]
Epoch [42/120    avg_loss:0.105, val_acc:0.972]
Epoch [43/120    avg_loss:0.101, val_acc:0.966]
Epoch [44/120    avg_loss:0.095, val_acc:0.972]
Epoch [45/120    avg_loss:0.094, val_acc:0.962]
Epoch [46/120    avg_loss:0.075, val_acc:0.962]
Epoch [47/120    avg_loss:0.103, val_acc:0.964]
Epoch [48/120    avg_loss:0.081, val_acc:0.982]
Epoch [49/120    avg_loss:0.093, val_acc:0.972]
Epoch [50/120    avg_loss:0.068, val_acc:0.976]
Epoch [51/120    avg_loss:0.055, val_acc:0.980]
Epoch [52/120    avg_loss:0.052, val_acc:0.982]
Epoch [53/120    avg_loss:0.050, val_acc:0.984]
Epoch [54/120    avg_loss:0.047, val_acc:0.984]
Epoch [55/120    avg_loss:0.044, val_acc:0.986]
Epoch [56/120    avg_loss:0.037, val_acc:0.986]
Epoch [57/120    avg_loss:0.039, val_acc:0.982]
Epoch [58/120    avg_loss:0.037, val_acc:0.986]
Epoch [59/120    avg_loss:0.048, val_acc:0.982]
Epoch [60/120    avg_loss:0.041, val_acc:0.988]
Epoch [61/120    avg_loss:0.048, val_acc:0.988]
Epoch [62/120    avg_loss:0.050, val_acc:0.988]
Epoch [63/120    avg_loss:0.038, val_acc:0.988]
Epoch [64/120    avg_loss:0.036, val_acc:0.988]
Epoch [65/120    avg_loss:0.044, val_acc:0.988]
Epoch [66/120    avg_loss:0.039, val_acc:0.988]
Epoch [67/120    avg_loss:0.033, val_acc:0.984]
Epoch [68/120    avg_loss:0.042, val_acc:0.984]
Epoch [69/120    avg_loss:0.034, val_acc:0.988]
Epoch [70/120    avg_loss:0.035, val_acc:0.990]
Epoch [71/120    avg_loss:0.045, val_acc:0.988]
Epoch [72/120    avg_loss:0.034, val_acc:0.988]
Epoch [73/120    avg_loss:0.040, val_acc:0.986]
Epoch [74/120    avg_loss:0.033, val_acc:0.988]
Epoch [75/120    avg_loss:0.034, val_acc:0.988]
Epoch [76/120    avg_loss:0.035, val_acc:0.988]
Epoch [77/120    avg_loss:0.039, val_acc:0.990]
Epoch [78/120    avg_loss:0.038, val_acc:0.988]
Epoch [79/120    avg_loss:0.037, val_acc:0.990]
Epoch [80/120    avg_loss:0.035, val_acc:0.988]
Epoch [81/120    avg_loss:0.028, val_acc:0.988]
Epoch [82/120    avg_loss:0.036, val_acc:0.990]
Epoch [83/120    avg_loss:0.032, val_acc:0.990]
Epoch [84/120    avg_loss:0.031, val_acc:0.990]
Epoch [85/120    avg_loss:0.037, val_acc:0.986]
Epoch [86/120    avg_loss:0.032, val_acc:0.990]
Epoch [87/120    avg_loss:0.030, val_acc:0.990]
Epoch [88/120    avg_loss:0.029, val_acc:0.990]
Epoch [89/120    avg_loss:0.034, val_acc:0.990]
Epoch [90/120    avg_loss:0.025, val_acc:0.992]
Epoch [91/120    avg_loss:0.031, val_acc:0.990]
Epoch [92/120    avg_loss:0.036, val_acc:0.990]
Epoch [93/120    avg_loss:0.028, val_acc:0.988]
Epoch [94/120    avg_loss:0.034, val_acc:0.990]
Epoch [95/120    avg_loss:0.023, val_acc:0.992]
Epoch [96/120    avg_loss:0.030, val_acc:0.990]
Epoch [97/120    avg_loss:0.027, val_acc:0.992]
Epoch [98/120    avg_loss:0.032, val_acc:0.992]
Epoch [99/120    avg_loss:0.027, val_acc:0.990]
Epoch [100/120    avg_loss:0.028, val_acc:0.990]
Epoch [101/120    avg_loss:0.027, val_acc:0.990]
Epoch [102/120    avg_loss:0.027, val_acc:0.990]
Epoch [103/120    avg_loss:0.027, val_acc:0.992]
Epoch [104/120    avg_loss:0.025, val_acc:0.990]
Epoch [105/120    avg_loss:0.025, val_acc:0.988]
Epoch [106/120    avg_loss:0.024, val_acc:0.990]
Epoch [107/120    avg_loss:0.028, val_acc:0.990]
Epoch [108/120    avg_loss:0.037, val_acc:0.990]
Epoch [109/120    avg_loss:0.036, val_acc:0.990]
Epoch [110/120    avg_loss:0.025, val_acc:0.990]
Epoch [111/120    avg_loss:0.022, val_acc:0.990]
Epoch [112/120    avg_loss:0.021, val_acc:0.990]
Epoch [113/120    avg_loss:0.026, val_acc:0.990]
Epoch [114/120    avg_loss:0.023, val_acc:0.990]
Epoch [115/120    avg_loss:0.027, val_acc:0.990]
Epoch [116/120    avg_loss:0.029, val_acc:0.990]
Epoch [117/120    avg_loss:0.025, val_acc:0.990]
Epoch [118/120    avg_loss:0.034, val_acc:0.990]
Epoch [119/120    avg_loss:0.023, val_acc:0.990]
Epoch [120/120    avg_loss:0.027, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 1.         0.98648649 1.         0.94618834 0.91946309
 1.         0.96703297 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9928785224741076
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2c4cb77908>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.436, val_acc:0.403]
Epoch [2/120    avg_loss:2.100, val_acc:0.534]
Epoch [3/120    avg_loss:1.852, val_acc:0.619]
Epoch [4/120    avg_loss:1.656, val_acc:0.623]
Epoch [5/120    avg_loss:1.456, val_acc:0.679]
Epoch [6/120    avg_loss:1.264, val_acc:0.756]
Epoch [7/120    avg_loss:1.112, val_acc:0.770]
Epoch [8/120    avg_loss:0.970, val_acc:0.833]
Epoch [9/120    avg_loss:0.872, val_acc:0.883]
Epoch [10/120    avg_loss:0.735, val_acc:0.863]
Epoch [11/120    avg_loss:0.689, val_acc:0.893]
Epoch [12/120    avg_loss:0.619, val_acc:0.879]
Epoch [13/120    avg_loss:0.558, val_acc:0.927]
Epoch [14/120    avg_loss:0.501, val_acc:0.909]
Epoch [15/120    avg_loss:0.548, val_acc:0.883]
Epoch [16/120    avg_loss:0.490, val_acc:0.907]
Epoch [17/120    avg_loss:0.495, val_acc:0.921]
Epoch [18/120    avg_loss:0.436, val_acc:0.917]
Epoch [19/120    avg_loss:0.437, val_acc:0.935]
Epoch [20/120    avg_loss:0.355, val_acc:0.929]
Epoch [21/120    avg_loss:0.355, val_acc:0.919]
Epoch [22/120    avg_loss:0.374, val_acc:0.935]
Epoch [23/120    avg_loss:0.351, val_acc:0.929]
Epoch [24/120    avg_loss:0.289, val_acc:0.940]
Epoch [25/120    avg_loss:0.319, val_acc:0.925]
Epoch [26/120    avg_loss:0.324, val_acc:0.946]
Epoch [27/120    avg_loss:0.266, val_acc:0.962]
Epoch [28/120    avg_loss:0.278, val_acc:0.950]
Epoch [29/120    avg_loss:0.273, val_acc:0.940]
Epoch [30/120    avg_loss:0.245, val_acc:0.940]
Epoch [31/120    avg_loss:0.251, val_acc:0.927]
Epoch [32/120    avg_loss:0.216, val_acc:0.948]
Epoch [33/120    avg_loss:0.247, val_acc:0.966]
Epoch [34/120    avg_loss:0.194, val_acc:0.946]
Epoch [35/120    avg_loss:0.196, val_acc:0.940]
Epoch [36/120    avg_loss:0.138, val_acc:0.935]
Epoch [37/120    avg_loss:0.169, val_acc:0.956]
Epoch [38/120    avg_loss:0.190, val_acc:0.923]
Epoch [39/120    avg_loss:0.343, val_acc:0.921]
Epoch [40/120    avg_loss:0.224, val_acc:0.948]
Epoch [41/120    avg_loss:0.170, val_acc:0.933]
Epoch [42/120    avg_loss:0.163, val_acc:0.970]
Epoch [43/120    avg_loss:0.139, val_acc:0.970]
Epoch [44/120    avg_loss:0.128, val_acc:0.960]
Epoch [45/120    avg_loss:0.164, val_acc:0.954]
Epoch [46/120    avg_loss:0.161, val_acc:0.954]
Epoch [47/120    avg_loss:0.128, val_acc:0.952]
Epoch [48/120    avg_loss:0.120, val_acc:0.968]
Epoch [49/120    avg_loss:0.109, val_acc:0.968]
Epoch [50/120    avg_loss:0.116, val_acc:0.966]
Epoch [51/120    avg_loss:0.116, val_acc:0.978]
Epoch [52/120    avg_loss:0.089, val_acc:0.976]
Epoch [53/120    avg_loss:0.114, val_acc:0.982]
Epoch [54/120    avg_loss:0.116, val_acc:0.974]
Epoch [55/120    avg_loss:0.094, val_acc:0.970]
Epoch [56/120    avg_loss:0.100, val_acc:0.982]
Epoch [57/120    avg_loss:0.095, val_acc:0.966]
Epoch [58/120    avg_loss:0.128, val_acc:0.972]
Epoch [59/120    avg_loss:0.092, val_acc:0.976]
Epoch [60/120    avg_loss:0.106, val_acc:0.978]
Epoch [61/120    avg_loss:0.077, val_acc:0.980]
Epoch [62/120    avg_loss:0.090, val_acc:0.966]
Epoch [63/120    avg_loss:0.098, val_acc:0.990]
Epoch [64/120    avg_loss:0.070, val_acc:0.988]
Epoch [65/120    avg_loss:0.050, val_acc:0.976]
Epoch [66/120    avg_loss:0.069, val_acc:0.978]
Epoch [67/120    avg_loss:0.085, val_acc:0.974]
Epoch [68/120    avg_loss:0.100, val_acc:0.960]
Epoch [69/120    avg_loss:0.091, val_acc:0.978]
Epoch [70/120    avg_loss:0.066, val_acc:0.980]
Epoch [71/120    avg_loss:0.083, val_acc:0.984]
Epoch [72/120    avg_loss:0.080, val_acc:0.960]
Epoch [73/120    avg_loss:0.089, val_acc:0.988]
Epoch [74/120    avg_loss:0.063, val_acc:0.990]
Epoch [75/120    avg_loss:0.057, val_acc:0.974]
Epoch [76/120    avg_loss:0.064, val_acc:0.986]
Epoch [77/120    avg_loss:0.029, val_acc:0.988]
Epoch [78/120    avg_loss:0.050, val_acc:0.984]
Epoch [79/120    avg_loss:0.065, val_acc:0.984]
Epoch [80/120    avg_loss:0.066, val_acc:0.982]
Epoch [81/120    avg_loss:0.051, val_acc:0.988]
Epoch [82/120    avg_loss:0.052, val_acc:0.980]
Epoch [83/120    avg_loss:0.042, val_acc:0.986]
Epoch [84/120    avg_loss:0.030, val_acc:0.990]
Epoch [85/120    avg_loss:0.030, val_acc:0.986]
Epoch [86/120    avg_loss:0.020, val_acc:0.988]
Epoch [87/120    avg_loss:0.028, val_acc:0.988]
Epoch [88/120    avg_loss:0.039, val_acc:0.974]
Epoch [89/120    avg_loss:0.045, val_acc:0.972]
Epoch [90/120    avg_loss:0.073, val_acc:0.980]
Epoch [91/120    avg_loss:0.046, val_acc:0.980]
Epoch [92/120    avg_loss:0.034, val_acc:0.988]
Epoch [93/120    avg_loss:0.041, val_acc:0.980]
Epoch [94/120    avg_loss:0.057, val_acc:0.984]
Epoch [95/120    avg_loss:0.053, val_acc:0.980]
Epoch [96/120    avg_loss:0.045, val_acc:0.982]
Epoch [97/120    avg_loss:0.043, val_acc:0.988]
Epoch [98/120    avg_loss:0.020, val_acc:0.990]
Epoch [99/120    avg_loss:0.023, val_acc:0.990]
Epoch [100/120    avg_loss:0.024, val_acc:0.988]
Epoch [101/120    avg_loss:0.021, val_acc:0.990]
Epoch [102/120    avg_loss:0.019, val_acc:0.988]
Epoch [103/120    avg_loss:0.028, val_acc:0.990]
Epoch [104/120    avg_loss:0.020, val_acc:0.990]
Epoch [105/120    avg_loss:0.022, val_acc:0.990]
Epoch [106/120    avg_loss:0.019, val_acc:0.990]
Epoch [107/120    avg_loss:0.018, val_acc:0.990]
Epoch [108/120    avg_loss:0.018, val_acc:0.990]
Epoch [109/120    avg_loss:0.016, val_acc:0.990]
Epoch [110/120    avg_loss:0.017, val_acc:0.990]
Epoch [111/120    avg_loss:0.016, val_acc:0.988]
Epoch [112/120    avg_loss:0.025, val_acc:0.988]
Epoch [113/120    avg_loss:0.014, val_acc:0.988]
Epoch [114/120    avg_loss:0.021, val_acc:0.988]
Epoch [115/120    avg_loss:0.014, val_acc:0.988]
Epoch [116/120    avg_loss:0.016, val_acc:0.988]
Epoch [117/120    avg_loss:0.013, val_acc:0.988]
Epoch [118/120    avg_loss:0.015, val_acc:0.988]
Epoch [119/120    avg_loss:0.010, val_acc:0.988]
Epoch [120/120    avg_loss:0.022, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   5   0   0   0   0   0   0   1   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 1.         0.98206278 0.98901099 0.93842887 0.91696751
 1.         0.95555556 1.         1.         1.         0.99867198
 0.99779736 1.        ]

Kappa:
0.990978430288379
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f969b504908>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.503, val_acc:0.375]
Epoch [2/120    avg_loss:2.181, val_acc:0.556]
Epoch [3/120    avg_loss:1.936, val_acc:0.625]
Epoch [4/120    avg_loss:1.680, val_acc:0.609]
Epoch [5/120    avg_loss:1.429, val_acc:0.690]
Epoch [6/120    avg_loss:1.216, val_acc:0.726]
Epoch [7/120    avg_loss:1.056, val_acc:0.746]
Epoch [8/120    avg_loss:0.909, val_acc:0.808]
Epoch [9/120    avg_loss:0.847, val_acc:0.780]
Epoch [10/120    avg_loss:0.754, val_acc:0.859]
Epoch [11/120    avg_loss:0.683, val_acc:0.845]
Epoch [12/120    avg_loss:0.687, val_acc:0.833]
Epoch [13/120    avg_loss:0.634, val_acc:0.895]
Epoch [14/120    avg_loss:0.547, val_acc:0.917]
Epoch [15/120    avg_loss:0.523, val_acc:0.857]
Epoch [16/120    avg_loss:0.475, val_acc:0.887]
Epoch [17/120    avg_loss:0.445, val_acc:0.907]
Epoch [18/120    avg_loss:0.492, val_acc:0.903]
Epoch [19/120    avg_loss:0.394, val_acc:0.899]
Epoch [20/120    avg_loss:0.389, val_acc:0.923]
Epoch [21/120    avg_loss:0.394, val_acc:0.919]
Epoch [22/120    avg_loss:0.361, val_acc:0.913]
Epoch [23/120    avg_loss:0.405, val_acc:0.909]
Epoch [24/120    avg_loss:0.351, val_acc:0.911]
Epoch [25/120    avg_loss:0.390, val_acc:0.942]
Epoch [26/120    avg_loss:0.316, val_acc:0.929]
Epoch [27/120    avg_loss:0.317, val_acc:0.909]
Epoch [28/120    avg_loss:0.320, val_acc:0.923]
Epoch [29/120    avg_loss:0.311, val_acc:0.901]
Epoch [30/120    avg_loss:0.288, val_acc:0.863]
Epoch [31/120    avg_loss:0.275, val_acc:0.921]
Epoch [32/120    avg_loss:0.304, val_acc:0.897]
Epoch [33/120    avg_loss:0.267, val_acc:0.927]
Epoch [34/120    avg_loss:0.312, val_acc:0.929]
Epoch [35/120    avg_loss:0.262, val_acc:0.954]
Epoch [36/120    avg_loss:0.242, val_acc:0.938]
Epoch [37/120    avg_loss:0.286, val_acc:0.960]
Epoch [38/120    avg_loss:0.222, val_acc:0.923]
Epoch [39/120    avg_loss:0.283, val_acc:0.942]
Epoch [40/120    avg_loss:0.240, val_acc:0.948]
Epoch [41/120    avg_loss:0.213, val_acc:0.966]
Epoch [42/120    avg_loss:0.168, val_acc:0.958]
Epoch [43/120    avg_loss:0.177, val_acc:0.960]
Epoch [44/120    avg_loss:0.161, val_acc:0.970]
Epoch [45/120    avg_loss:0.201, val_acc:0.958]
Epoch [46/120    avg_loss:0.282, val_acc:0.923]
Epoch [47/120    avg_loss:0.224, val_acc:0.950]
Epoch [48/120    avg_loss:0.205, val_acc:0.956]
Epoch [49/120    avg_loss:0.195, val_acc:0.966]
Epoch [50/120    avg_loss:0.214, val_acc:0.923]
Epoch [51/120    avg_loss:0.180, val_acc:0.915]
Epoch [52/120    avg_loss:0.179, val_acc:0.966]
Epoch [53/120    avg_loss:0.151, val_acc:0.966]
Epoch [54/120    avg_loss:0.148, val_acc:0.978]
Epoch [55/120    avg_loss:0.126, val_acc:0.968]
Epoch [56/120    avg_loss:0.139, val_acc:0.952]
Epoch [57/120    avg_loss:0.109, val_acc:0.970]
Epoch [58/120    avg_loss:0.093, val_acc:0.974]
Epoch [59/120    avg_loss:0.082, val_acc:0.968]
Epoch [60/120    avg_loss:0.137, val_acc:0.968]
Epoch [61/120    avg_loss:0.109, val_acc:0.954]
Epoch [62/120    avg_loss:0.112, val_acc:0.968]
Epoch [63/120    avg_loss:0.132, val_acc:0.958]
Epoch [64/120    avg_loss:0.119, val_acc:0.984]
Epoch [65/120    avg_loss:0.132, val_acc:0.970]
Epoch [66/120    avg_loss:0.075, val_acc:0.970]
Epoch [67/120    avg_loss:0.076, val_acc:0.970]
Epoch [68/120    avg_loss:0.079, val_acc:0.978]
Epoch [69/120    avg_loss:0.079, val_acc:0.972]
Epoch [70/120    avg_loss:0.086, val_acc:0.978]
Epoch [71/120    avg_loss:0.062, val_acc:0.974]
Epoch [72/120    avg_loss:0.083, val_acc:0.968]
Epoch [73/120    avg_loss:0.091, val_acc:0.982]
Epoch [74/120    avg_loss:0.075, val_acc:0.988]
Epoch [75/120    avg_loss:0.067, val_acc:0.972]
Epoch [76/120    avg_loss:0.063, val_acc:0.964]
Epoch [77/120    avg_loss:0.113, val_acc:0.974]
Epoch [78/120    avg_loss:0.092, val_acc:0.990]
Epoch [79/120    avg_loss:0.064, val_acc:0.980]
Epoch [80/120    avg_loss:0.078, val_acc:0.980]
Epoch [81/120    avg_loss:0.058, val_acc:0.980]
Epoch [82/120    avg_loss:0.049, val_acc:0.978]
Epoch [83/120    avg_loss:0.045, val_acc:0.984]
Epoch [84/120    avg_loss:0.035, val_acc:0.988]
Epoch [85/120    avg_loss:0.044, val_acc:0.986]
Epoch [86/120    avg_loss:0.079, val_acc:0.982]
Epoch [87/120    avg_loss:0.057, val_acc:0.990]
Epoch [88/120    avg_loss:0.052, val_acc:0.990]
Epoch [89/120    avg_loss:0.038, val_acc:0.962]
Epoch [90/120    avg_loss:0.037, val_acc:0.984]
Epoch [91/120    avg_loss:0.027, val_acc:0.986]
Epoch [92/120    avg_loss:0.028, val_acc:0.992]
Epoch [93/120    avg_loss:0.034, val_acc:0.988]
Epoch [94/120    avg_loss:0.025, val_acc:0.990]
Epoch [95/120    avg_loss:0.024, val_acc:0.982]
Epoch [96/120    avg_loss:0.055, val_acc:0.976]
Epoch [97/120    avg_loss:0.085, val_acc:0.940]
Epoch [98/120    avg_loss:0.103, val_acc:0.944]
Epoch [99/120    avg_loss:0.080, val_acc:0.984]
Epoch [100/120    avg_loss:0.100, val_acc:0.974]
Epoch [101/120    avg_loss:0.059, val_acc:0.992]
Epoch [102/120    avg_loss:0.075, val_acc:0.974]
Epoch [103/120    avg_loss:0.111, val_acc:0.976]
Epoch [104/120    avg_loss:0.082, val_acc:0.972]
Epoch [105/120    avg_loss:0.059, val_acc:0.990]
Epoch [106/120    avg_loss:0.031, val_acc:0.986]
Epoch [107/120    avg_loss:0.046, val_acc:0.982]
Epoch [108/120    avg_loss:0.079, val_acc:0.986]
Epoch [109/120    avg_loss:0.063, val_acc:0.986]
Epoch [110/120    avg_loss:0.049, val_acc:0.992]
Epoch [111/120    avg_loss:0.027, val_acc:0.990]
Epoch [112/120    avg_loss:0.024, val_acc:0.990]
Epoch [113/120    avg_loss:0.019, val_acc:0.988]
Epoch [114/120    avg_loss:0.017, val_acc:0.992]
Epoch [115/120    avg_loss:0.022, val_acc:0.990]
Epoch [116/120    avg_loss:0.016, val_acc:0.988]
Epoch [117/120    avg_loss:0.013, val_acc:0.992]
Epoch [118/120    avg_loss:0.017, val_acc:0.980]
Epoch [119/120    avg_loss:0.060, val_acc:0.980]
Epoch [120/120    avg_loss:0.057, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 1.         0.98426966 0.99782135 0.95909091 0.9442623
 1.         0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.994065571161824
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbdb89a6978>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.529, val_acc:0.536]
Epoch [2/120    avg_loss:2.165, val_acc:0.595]
Epoch [3/120    avg_loss:1.905, val_acc:0.647]
Epoch [4/120    avg_loss:1.630, val_acc:0.663]
Epoch [5/120    avg_loss:1.391, val_acc:0.696]
Epoch [6/120    avg_loss:1.214, val_acc:0.712]
Epoch [7/120    avg_loss:1.040, val_acc:0.754]
Epoch [8/120    avg_loss:0.930, val_acc:0.764]
Epoch [9/120    avg_loss:0.839, val_acc:0.792]
Epoch [10/120    avg_loss:0.741, val_acc:0.845]
Epoch [11/120    avg_loss:0.642, val_acc:0.901]
Epoch [12/120    avg_loss:0.613, val_acc:0.917]
Epoch [13/120    avg_loss:0.546, val_acc:0.905]
Epoch [14/120    avg_loss:0.464, val_acc:0.895]
Epoch [15/120    avg_loss:0.500, val_acc:0.909]
Epoch [16/120    avg_loss:0.485, val_acc:0.893]
Epoch [17/120    avg_loss:0.469, val_acc:0.901]
Epoch [18/120    avg_loss:0.398, val_acc:0.903]
Epoch [19/120    avg_loss:0.390, val_acc:0.927]
Epoch [20/120    avg_loss:0.404, val_acc:0.931]
Epoch [21/120    avg_loss:0.378, val_acc:0.917]
Epoch [22/120    avg_loss:0.353, val_acc:0.931]
Epoch [23/120    avg_loss:0.314, val_acc:0.921]
Epoch [24/120    avg_loss:0.273, val_acc:0.940]
Epoch [25/120    avg_loss:0.307, val_acc:0.927]
Epoch [26/120    avg_loss:0.288, val_acc:0.925]
Epoch [27/120    avg_loss:0.248, val_acc:0.938]
Epoch [28/120    avg_loss:0.216, val_acc:0.948]
Epoch [29/120    avg_loss:0.251, val_acc:0.921]
Epoch [30/120    avg_loss:0.301, val_acc:0.962]
Epoch [31/120    avg_loss:0.237, val_acc:0.921]
Epoch [32/120    avg_loss:0.228, val_acc:0.948]
Epoch [33/120    avg_loss:0.191, val_acc:0.942]
Epoch [34/120    avg_loss:0.218, val_acc:0.905]
Epoch [35/120    avg_loss:0.317, val_acc:0.938]
Epoch [36/120    avg_loss:0.312, val_acc:0.935]
Epoch [37/120    avg_loss:0.292, val_acc:0.877]
Epoch [38/120    avg_loss:0.252, val_acc:0.933]
Epoch [39/120    avg_loss:0.199, val_acc:0.956]
Epoch [40/120    avg_loss:0.148, val_acc:0.958]
Epoch [41/120    avg_loss:0.197, val_acc:0.960]
Epoch [42/120    avg_loss:0.159, val_acc:0.968]
Epoch [43/120    avg_loss:0.159, val_acc:0.956]
Epoch [44/120    avg_loss:0.205, val_acc:0.946]
Epoch [45/120    avg_loss:0.186, val_acc:0.966]
Epoch [46/120    avg_loss:0.171, val_acc:0.968]
Epoch [47/120    avg_loss:0.258, val_acc:0.956]
Epoch [48/120    avg_loss:0.126, val_acc:0.962]
Epoch [49/120    avg_loss:0.147, val_acc:0.962]
Epoch [50/120    avg_loss:0.180, val_acc:0.970]
Epoch [51/120    avg_loss:0.219, val_acc:0.948]
Epoch [52/120    avg_loss:0.175, val_acc:0.946]
Epoch [53/120    avg_loss:0.157, val_acc:0.948]
Epoch [54/120    avg_loss:0.153, val_acc:0.966]
Epoch [55/120    avg_loss:0.218, val_acc:0.966]
Epoch [56/120    avg_loss:0.167, val_acc:0.960]
Epoch [57/120    avg_loss:0.193, val_acc:0.960]
Epoch [58/120    avg_loss:0.117, val_acc:0.974]
Epoch [59/120    avg_loss:0.104, val_acc:0.966]
Epoch [60/120    avg_loss:0.110, val_acc:0.962]
Epoch [61/120    avg_loss:0.110, val_acc:0.970]
Epoch [62/120    avg_loss:0.111, val_acc:0.966]
Epoch [63/120    avg_loss:0.108, val_acc:0.968]
Epoch [64/120    avg_loss:0.078, val_acc:0.958]
Epoch [65/120    avg_loss:0.097, val_acc:0.976]
Epoch [66/120    avg_loss:0.120, val_acc:0.968]
Epoch [67/120    avg_loss:0.099, val_acc:0.982]
Epoch [68/120    avg_loss:0.086, val_acc:0.974]
Epoch [69/120    avg_loss:0.108, val_acc:0.974]
Epoch [70/120    avg_loss:0.074, val_acc:0.980]
Epoch [71/120    avg_loss:0.062, val_acc:0.972]
Epoch [72/120    avg_loss:0.100, val_acc:0.974]
Epoch [73/120    avg_loss:0.083, val_acc:0.976]
Epoch [74/120    avg_loss:0.078, val_acc:0.956]
Epoch [75/120    avg_loss:0.087, val_acc:0.972]
Epoch [76/120    avg_loss:0.087, val_acc:0.968]
Epoch [77/120    avg_loss:0.083, val_acc:0.970]
Epoch [78/120    avg_loss:0.054, val_acc:0.978]
Epoch [79/120    avg_loss:0.049, val_acc:0.970]
Epoch [80/120    avg_loss:0.060, val_acc:0.980]
Epoch [81/120    avg_loss:0.043, val_acc:0.984]
Epoch [82/120    avg_loss:0.038, val_acc:0.982]
Epoch [83/120    avg_loss:0.030, val_acc:0.984]
Epoch [84/120    avg_loss:0.035, val_acc:0.984]
Epoch [85/120    avg_loss:0.043, val_acc:0.984]
Epoch [86/120    avg_loss:0.023, val_acc:0.984]
Epoch [87/120    avg_loss:0.035, val_acc:0.984]
Epoch [88/120    avg_loss:0.033, val_acc:0.984]
Epoch [89/120    avg_loss:0.034, val_acc:0.984]
Epoch [90/120    avg_loss:0.028, val_acc:0.984]
Epoch [91/120    avg_loss:0.027, val_acc:0.984]
Epoch [92/120    avg_loss:0.026, val_acc:0.986]
Epoch [93/120    avg_loss:0.027, val_acc:0.986]
Epoch [94/120    avg_loss:0.031, val_acc:0.986]
Epoch [95/120    avg_loss:0.032, val_acc:0.984]
Epoch [96/120    avg_loss:0.026, val_acc:0.986]
Epoch [97/120    avg_loss:0.025, val_acc:0.986]
Epoch [98/120    avg_loss:0.024, val_acc:0.986]
Epoch [99/120    avg_loss:0.027, val_acc:0.986]
Epoch [100/120    avg_loss:0.024, val_acc:0.986]
Epoch [101/120    avg_loss:0.024, val_acc:0.986]
Epoch [102/120    avg_loss:0.028, val_acc:0.988]
Epoch [103/120    avg_loss:0.026, val_acc:0.984]
Epoch [104/120    avg_loss:0.028, val_acc:0.988]
Epoch [105/120    avg_loss:0.030, val_acc:0.988]
Epoch [106/120    avg_loss:0.027, val_acc:0.988]
Epoch [107/120    avg_loss:0.032, val_acc:0.988]
Epoch [108/120    avg_loss:0.028, val_acc:0.988]
Epoch [109/120    avg_loss:0.025, val_acc:0.986]
Epoch [110/120    avg_loss:0.032, val_acc:0.986]
Epoch [111/120    avg_loss:0.030, val_acc:0.986]
Epoch [112/120    avg_loss:0.032, val_acc:0.988]
Epoch [113/120    avg_loss:0.029, val_acc:0.988]
Epoch [114/120    avg_loss:0.026, val_acc:0.988]
Epoch [115/120    avg_loss:0.030, val_acc:0.988]
Epoch [116/120    avg_loss:0.019, val_acc:0.988]
Epoch [117/120    avg_loss:0.023, val_acc:0.988]
Epoch [118/120    avg_loss:0.027, val_acc:0.988]
Epoch [119/120    avg_loss:0.021, val_acc:0.988]
Epoch [120/120    avg_loss:0.023, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 212  18   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 1.         0.97986577 0.95927602 0.91938998 0.94078947
 0.99756691 0.94972067 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9890804840300434
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:07:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd15a431978>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.435, val_acc:0.538]
Epoch [2/120    avg_loss:2.086, val_acc:0.615]
Epoch [3/120    avg_loss:1.819, val_acc:0.631]
Epoch [4/120    avg_loss:1.546, val_acc:0.671]
Epoch [5/120    avg_loss:1.332, val_acc:0.661]
Epoch [6/120    avg_loss:1.146, val_acc:0.714]
Epoch [7/120    avg_loss:0.998, val_acc:0.754]
Epoch [8/120    avg_loss:0.917, val_acc:0.740]
Epoch [9/120    avg_loss:0.855, val_acc:0.835]
Epoch [10/120    avg_loss:0.847, val_acc:0.746]
Epoch [11/120    avg_loss:0.708, val_acc:0.845]
Epoch [12/120    avg_loss:0.661, val_acc:0.812]
Epoch [13/120    avg_loss:0.576, val_acc:0.905]
Epoch [14/120    avg_loss:0.532, val_acc:0.895]
Epoch [15/120    avg_loss:0.541, val_acc:0.867]
Epoch [16/120    avg_loss:0.522, val_acc:0.903]
Epoch [17/120    avg_loss:0.467, val_acc:0.879]
Epoch [18/120    avg_loss:0.451, val_acc:0.841]
Epoch [19/120    avg_loss:0.483, val_acc:0.879]
Epoch [20/120    avg_loss:0.483, val_acc:0.899]
Epoch [21/120    avg_loss:0.428, val_acc:0.889]
Epoch [22/120    avg_loss:0.386, val_acc:0.911]
Epoch [23/120    avg_loss:0.533, val_acc:0.881]
Epoch [24/120    avg_loss:0.429, val_acc:0.905]
Epoch [25/120    avg_loss:0.392, val_acc:0.950]
Epoch [26/120    avg_loss:0.373, val_acc:0.942]
Epoch [27/120    avg_loss:0.377, val_acc:0.909]
Epoch [28/120    avg_loss:0.320, val_acc:0.933]
Epoch [29/120    avg_loss:0.293, val_acc:0.923]
Epoch [30/120    avg_loss:0.318, val_acc:0.938]
Epoch [31/120    avg_loss:0.312, val_acc:0.950]
Epoch [32/120    avg_loss:0.292, val_acc:0.915]
Epoch [33/120    avg_loss:0.285, val_acc:0.935]
Epoch [34/120    avg_loss:0.234, val_acc:0.954]
Epoch [35/120    avg_loss:0.224, val_acc:0.960]
Epoch [36/120    avg_loss:0.267, val_acc:0.929]
Epoch [37/120    avg_loss:0.229, val_acc:0.958]
Epoch [38/120    avg_loss:0.216, val_acc:0.968]
Epoch [39/120    avg_loss:0.244, val_acc:0.972]
Epoch [40/120    avg_loss:0.215, val_acc:0.960]
Epoch [41/120    avg_loss:0.184, val_acc:0.962]
Epoch [42/120    avg_loss:0.171, val_acc:0.974]
Epoch [43/120    avg_loss:0.215, val_acc:0.944]
Epoch [44/120    avg_loss:0.192, val_acc:0.978]
Epoch [45/120    avg_loss:0.172, val_acc:0.972]
Epoch [46/120    avg_loss:0.245, val_acc:0.970]
Epoch [47/120    avg_loss:0.151, val_acc:0.948]
Epoch [48/120    avg_loss:0.162, val_acc:0.978]
Epoch [49/120    avg_loss:0.136, val_acc:0.942]
Epoch [50/120    avg_loss:0.146, val_acc:0.976]
Epoch [51/120    avg_loss:0.265, val_acc:0.954]
Epoch [52/120    avg_loss:0.201, val_acc:0.976]
Epoch [53/120    avg_loss:0.173, val_acc:0.925]
Epoch [54/120    avg_loss:0.202, val_acc:0.958]
Epoch [55/120    avg_loss:0.175, val_acc:0.970]
Epoch [56/120    avg_loss:0.170, val_acc:0.952]
Epoch [57/120    avg_loss:0.127, val_acc:0.972]
Epoch [58/120    avg_loss:0.137, val_acc:0.988]
Epoch [59/120    avg_loss:0.096, val_acc:0.974]
Epoch [60/120    avg_loss:0.099, val_acc:0.982]
Epoch [61/120    avg_loss:0.078, val_acc:0.986]
Epoch [62/120    avg_loss:0.126, val_acc:0.978]
Epoch [63/120    avg_loss:0.152, val_acc:0.970]
Epoch [64/120    avg_loss:0.141, val_acc:0.974]
Epoch [65/120    avg_loss:0.099, val_acc:0.982]
Epoch [66/120    avg_loss:0.161, val_acc:0.974]
Epoch [67/120    avg_loss:0.168, val_acc:0.988]
Epoch [68/120    avg_loss:0.131, val_acc:0.982]
Epoch [69/120    avg_loss:0.173, val_acc:0.972]
Epoch [70/120    avg_loss:0.133, val_acc:0.976]
Epoch [71/120    avg_loss:0.096, val_acc:0.980]
Epoch [72/120    avg_loss:0.065, val_acc:0.986]
Epoch [73/120    avg_loss:0.117, val_acc:0.970]
Epoch [74/120    avg_loss:0.110, val_acc:0.946]
Epoch [75/120    avg_loss:0.136, val_acc:0.986]
Epoch [76/120    avg_loss:0.067, val_acc:0.988]
Epoch [77/120    avg_loss:0.093, val_acc:0.982]
Epoch [78/120    avg_loss:0.092, val_acc:0.976]
Epoch [79/120    avg_loss:0.143, val_acc:0.970]
Epoch [80/120    avg_loss:0.074, val_acc:0.990]
Epoch [81/120    avg_loss:0.108, val_acc:0.917]
Epoch [82/120    avg_loss:0.115, val_acc:0.986]
Epoch [83/120    avg_loss:0.047, val_acc:0.984]
Epoch [84/120    avg_loss:0.100, val_acc:0.984]
Epoch [85/120    avg_loss:0.100, val_acc:0.986]
Epoch [86/120    avg_loss:0.050, val_acc:0.984]
Epoch [87/120    avg_loss:0.073, val_acc:0.990]
Epoch [88/120    avg_loss:0.173, val_acc:0.982]
Epoch [89/120    avg_loss:0.102, val_acc:0.984]
Epoch [90/120    avg_loss:0.081, val_acc:0.984]
Epoch [91/120    avg_loss:0.057, val_acc:0.994]
Epoch [92/120    avg_loss:0.043, val_acc:0.992]
Epoch [93/120    avg_loss:0.084, val_acc:0.974]
Epoch [94/120    avg_loss:0.068, val_acc:0.986]
Epoch [95/120    avg_loss:0.093, val_acc:0.990]
Epoch [96/120    avg_loss:0.071, val_acc:0.992]
Epoch [97/120    avg_loss:0.058, val_acc:0.992]
Epoch [98/120    avg_loss:0.031, val_acc:0.994]
Epoch [99/120    avg_loss:0.076, val_acc:0.974]
Epoch [100/120    avg_loss:0.079, val_acc:0.970]
Epoch [101/120    avg_loss:0.034, val_acc:0.992]
Epoch [102/120    avg_loss:0.036, val_acc:0.994]
Epoch [103/120    avg_loss:0.026, val_acc:0.994]
Epoch [104/120    avg_loss:0.059, val_acc:0.992]
Epoch [105/120    avg_loss:0.047, val_acc:0.986]
Epoch [106/120    avg_loss:0.032, val_acc:0.982]
Epoch [107/120    avg_loss:0.037, val_acc:0.992]
Epoch [108/120    avg_loss:0.029, val_acc:0.994]
Epoch [109/120    avg_loss:0.018, val_acc:0.992]
Epoch [110/120    avg_loss:0.033, val_acc:0.990]
Epoch [111/120    avg_loss:0.029, val_acc:0.994]
Epoch [112/120    avg_loss:0.023, val_acc:0.992]
Epoch [113/120    avg_loss:0.033, val_acc:0.992]
Epoch [114/120    avg_loss:0.018, val_acc:0.996]
Epoch [115/120    avg_loss:0.022, val_acc:0.994]
Epoch [116/120    avg_loss:0.035, val_acc:0.976]
Epoch [117/120    avg_loss:0.128, val_acc:0.966]
Epoch [118/120    avg_loss:0.076, val_acc:0.988]
Epoch [119/120    avg_loss:0.038, val_acc:0.986]
Epoch [120/120    avg_loss:0.031, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 218   7   0   0   0   0   4   0   0   0   0]
 [  0   0   0   0 208  14   0   0   0   0   0   0   5   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 1.         0.9977221  0.97321429 0.92444444 0.92567568
 1.         1.         1.         0.99574468 1.         0.99341238
 0.98896247 1.        ]

Kappa:
0.9895545931950371
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f173590f940>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.497, val_acc:0.349]
Epoch [2/120    avg_loss:2.184, val_acc:0.540]
Epoch [3/120    avg_loss:1.954, val_acc:0.657]
Epoch [4/120    avg_loss:1.690, val_acc:0.653]
Epoch [5/120    avg_loss:1.470, val_acc:0.685]
Epoch [6/120    avg_loss:1.251, val_acc:0.714]
Epoch [7/120    avg_loss:1.128, val_acc:0.722]
Epoch [8/120    avg_loss:0.983, val_acc:0.798]
Epoch [9/120    avg_loss:0.861, val_acc:0.788]
Epoch [10/120    avg_loss:0.775, val_acc:0.823]
Epoch [11/120    avg_loss:0.749, val_acc:0.833]
Epoch [12/120    avg_loss:0.684, val_acc:0.883]
Epoch [13/120    avg_loss:0.607, val_acc:0.891]
Epoch [14/120    avg_loss:0.606, val_acc:0.853]
Epoch [15/120    avg_loss:0.549, val_acc:0.909]
Epoch [16/120    avg_loss:0.505, val_acc:0.897]
Epoch [17/120    avg_loss:0.493, val_acc:0.925]
Epoch [18/120    avg_loss:0.477, val_acc:0.911]
Epoch [19/120    avg_loss:0.370, val_acc:0.944]
Epoch [20/120    avg_loss:0.383, val_acc:0.938]
Epoch [21/120    avg_loss:0.370, val_acc:0.917]
Epoch [22/120    avg_loss:0.347, val_acc:0.927]
Epoch [23/120    avg_loss:0.411, val_acc:0.927]
Epoch [24/120    avg_loss:0.351, val_acc:0.933]
Epoch [25/120    avg_loss:0.334, val_acc:0.929]
Epoch [26/120    avg_loss:0.303, val_acc:0.938]
Epoch [27/120    avg_loss:0.267, val_acc:0.960]
Epoch [28/120    avg_loss:0.275, val_acc:0.938]
Epoch [29/120    avg_loss:0.236, val_acc:0.935]
Epoch [30/120    avg_loss:0.285, val_acc:0.948]
Epoch [31/120    avg_loss:0.272, val_acc:0.960]
Epoch [32/120    avg_loss:0.265, val_acc:0.909]
Epoch [33/120    avg_loss:0.272, val_acc:0.944]
Epoch [34/120    avg_loss:0.265, val_acc:0.948]
Epoch [35/120    avg_loss:0.260, val_acc:0.958]
Epoch [36/120    avg_loss:0.240, val_acc:0.935]
Epoch [37/120    avg_loss:0.227, val_acc:0.948]
Epoch [38/120    avg_loss:0.250, val_acc:0.913]
Epoch [39/120    avg_loss:0.269, val_acc:0.935]
Epoch [40/120    avg_loss:0.222, val_acc:0.966]
Epoch [41/120    avg_loss:0.198, val_acc:0.950]
Epoch [42/120    avg_loss:0.164, val_acc:0.954]
Epoch [43/120    avg_loss:0.205, val_acc:0.964]
Epoch [44/120    avg_loss:0.190, val_acc:0.938]
Epoch [45/120    avg_loss:0.144, val_acc:0.954]
Epoch [46/120    avg_loss:0.189, val_acc:0.954]
Epoch [47/120    avg_loss:0.178, val_acc:0.960]
Epoch [48/120    avg_loss:0.182, val_acc:0.964]
Epoch [49/120    avg_loss:0.156, val_acc:0.962]
Epoch [50/120    avg_loss:0.116, val_acc:0.958]
Epoch [51/120    avg_loss:0.134, val_acc:0.966]
Epoch [52/120    avg_loss:0.171, val_acc:0.958]
Epoch [53/120    avg_loss:0.201, val_acc:0.891]
Epoch [54/120    avg_loss:0.209, val_acc:0.960]
Epoch [55/120    avg_loss:0.157, val_acc:0.950]
Epoch [56/120    avg_loss:0.149, val_acc:0.958]
Epoch [57/120    avg_loss:0.128, val_acc:0.966]
Epoch [58/120    avg_loss:0.124, val_acc:0.954]
Epoch [59/120    avg_loss:0.159, val_acc:0.962]
Epoch [60/120    avg_loss:0.140, val_acc:0.923]
Epoch [61/120    avg_loss:0.154, val_acc:0.974]
Epoch [62/120    avg_loss:0.113, val_acc:0.966]
Epoch [63/120    avg_loss:0.112, val_acc:0.960]
Epoch [64/120    avg_loss:0.098, val_acc:0.958]
Epoch [65/120    avg_loss:0.117, val_acc:0.970]
Epoch [66/120    avg_loss:0.094, val_acc:0.968]
Epoch [67/120    avg_loss:0.106, val_acc:0.974]
Epoch [68/120    avg_loss:0.087, val_acc:0.986]
Epoch [69/120    avg_loss:0.105, val_acc:0.980]
Epoch [70/120    avg_loss:0.101, val_acc:0.980]
Epoch [71/120    avg_loss:0.092, val_acc:0.980]
Epoch [72/120    avg_loss:0.075, val_acc:0.980]
Epoch [73/120    avg_loss:0.075, val_acc:0.978]
Epoch [74/120    avg_loss:0.102, val_acc:0.972]
Epoch [75/120    avg_loss:0.073, val_acc:0.982]
Epoch [76/120    avg_loss:0.071, val_acc:0.964]
Epoch [77/120    avg_loss:0.082, val_acc:0.968]
Epoch [78/120    avg_loss:0.122, val_acc:0.970]
Epoch [79/120    avg_loss:0.086, val_acc:0.960]
Epoch [80/120    avg_loss:0.116, val_acc:0.980]
Epoch [81/120    avg_loss:0.090, val_acc:0.974]
Epoch [82/120    avg_loss:0.083, val_acc:0.976]
Epoch [83/120    avg_loss:0.048, val_acc:0.978]
Epoch [84/120    avg_loss:0.046, val_acc:0.980]
Epoch [85/120    avg_loss:0.038, val_acc:0.980]
Epoch [86/120    avg_loss:0.052, val_acc:0.982]
Epoch [87/120    avg_loss:0.046, val_acc:0.982]
Epoch [88/120    avg_loss:0.044, val_acc:0.982]
Epoch [89/120    avg_loss:0.044, val_acc:0.984]
Epoch [90/120    avg_loss:0.046, val_acc:0.984]
Epoch [91/120    avg_loss:0.042, val_acc:0.984]
Epoch [92/120    avg_loss:0.047, val_acc:0.984]
Epoch [93/120    avg_loss:0.034, val_acc:0.984]
Epoch [94/120    avg_loss:0.033, val_acc:0.984]
Epoch [95/120    avg_loss:0.034, val_acc:0.984]
Epoch [96/120    avg_loss:0.044, val_acc:0.984]
Epoch [97/120    avg_loss:0.037, val_acc:0.984]
Epoch [98/120    avg_loss:0.038, val_acc:0.984]
Epoch [99/120    avg_loss:0.037, val_acc:0.984]
Epoch [100/120    avg_loss:0.037, val_acc:0.984]
Epoch [101/120    avg_loss:0.038, val_acc:0.984]
Epoch [102/120    avg_loss:0.039, val_acc:0.984]
Epoch [103/120    avg_loss:0.034, val_acc:0.984]
Epoch [104/120    avg_loss:0.031, val_acc:0.984]
Epoch [105/120    avg_loss:0.034, val_acc:0.984]
Epoch [106/120    avg_loss:0.033, val_acc:0.984]
Epoch [107/120    avg_loss:0.039, val_acc:0.984]
Epoch [108/120    avg_loss:0.040, val_acc:0.984]
Epoch [109/120    avg_loss:0.041, val_acc:0.984]
Epoch [110/120    avg_loss:0.028, val_acc:0.984]
Epoch [111/120    avg_loss:0.032, val_acc:0.984]
Epoch [112/120    avg_loss:0.034, val_acc:0.984]
Epoch [113/120    avg_loss:0.035, val_acc:0.984]
Epoch [114/120    avg_loss:0.033, val_acc:0.984]
Epoch [115/120    avg_loss:0.036, val_acc:0.984]
Epoch [116/120    avg_loss:0.042, val_acc:0.984]
Epoch [117/120    avg_loss:0.042, val_acc:0.984]
Epoch [118/120    avg_loss:0.034, val_acc:0.984]
Epoch [119/120    avg_loss:0.031, val_acc:0.984]
Epoch [120/120    avg_loss:0.034, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 0.99926954 0.98426966 0.99782135 0.92576419 0.88111888
 0.99757869 0.96703297 1.         1.         1.         0.9973545
 0.99778761 1.        ]

Kappa:
0.9895549854900547
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8d30dbd898>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.456, val_acc:0.319]
Epoch [2/120    avg_loss:2.143, val_acc:0.558]
Epoch [3/120    avg_loss:1.926, val_acc:0.554]
Epoch [4/120    avg_loss:1.691, val_acc:0.641]
Epoch [5/120    avg_loss:1.488, val_acc:0.688]
Epoch [6/120    avg_loss:1.312, val_acc:0.714]
Epoch [7/120    avg_loss:1.112, val_acc:0.720]
Epoch [8/120    avg_loss:0.959, val_acc:0.772]
Epoch [9/120    avg_loss:0.852, val_acc:0.774]
Epoch [10/120    avg_loss:0.732, val_acc:0.889]
Epoch [11/120    avg_loss:0.612, val_acc:0.823]
Epoch [12/120    avg_loss:0.584, val_acc:0.875]
Epoch [13/120    avg_loss:0.568, val_acc:0.883]
Epoch [14/120    avg_loss:0.540, val_acc:0.905]
Epoch [15/120    avg_loss:0.489, val_acc:0.905]
Epoch [16/120    avg_loss:0.438, val_acc:0.905]
Epoch [17/120    avg_loss:0.352, val_acc:0.909]
Epoch [18/120    avg_loss:0.410, val_acc:0.905]
Epoch [19/120    avg_loss:0.334, val_acc:0.903]
Epoch [20/120    avg_loss:0.375, val_acc:0.923]
Epoch [21/120    avg_loss:0.316, val_acc:0.944]
Epoch [22/120    avg_loss:0.324, val_acc:0.911]
Epoch [23/120    avg_loss:0.332, val_acc:0.935]
Epoch [24/120    avg_loss:0.261, val_acc:0.921]
Epoch [25/120    avg_loss:0.283, val_acc:0.851]
Epoch [26/120    avg_loss:0.308, val_acc:0.942]
Epoch [27/120    avg_loss:0.260, val_acc:0.938]
Epoch [28/120    avg_loss:0.210, val_acc:0.948]
Epoch [29/120    avg_loss:0.397, val_acc:0.889]
Epoch [30/120    avg_loss:0.341, val_acc:0.942]
Epoch [31/120    avg_loss:0.306, val_acc:0.895]
Epoch [32/120    avg_loss:0.277, val_acc:0.950]
Epoch [33/120    avg_loss:0.220, val_acc:0.944]
Epoch [34/120    avg_loss:0.220, val_acc:0.942]
Epoch [35/120    avg_loss:0.200, val_acc:0.925]
Epoch [36/120    avg_loss:0.168, val_acc:0.966]
Epoch [37/120    avg_loss:0.159, val_acc:0.976]
Epoch [38/120    avg_loss:0.127, val_acc:0.968]
Epoch [39/120    avg_loss:0.194, val_acc:0.931]
Epoch [40/120    avg_loss:0.248, val_acc:0.968]
Epoch [41/120    avg_loss:0.195, val_acc:0.964]
Epoch [42/120    avg_loss:0.186, val_acc:0.950]
Epoch [43/120    avg_loss:0.151, val_acc:0.952]
Epoch [44/120    avg_loss:0.124, val_acc:0.968]
Epoch [45/120    avg_loss:0.123, val_acc:0.954]
Epoch [46/120    avg_loss:0.162, val_acc:0.940]
Epoch [47/120    avg_loss:0.179, val_acc:0.958]
Epoch [48/120    avg_loss:0.114, val_acc:0.958]
Epoch [49/120    avg_loss:0.167, val_acc:0.960]
Epoch [50/120    avg_loss:0.171, val_acc:0.958]
Epoch [51/120    avg_loss:0.150, val_acc:0.974]
Epoch [52/120    avg_loss:0.104, val_acc:0.974]
Epoch [53/120    avg_loss:0.089, val_acc:0.974]
Epoch [54/120    avg_loss:0.085, val_acc:0.972]
Epoch [55/120    avg_loss:0.092, val_acc:0.978]
Epoch [56/120    avg_loss:0.082, val_acc:0.974]
Epoch [57/120    avg_loss:0.084, val_acc:0.976]
Epoch [58/120    avg_loss:0.066, val_acc:0.982]
Epoch [59/120    avg_loss:0.060, val_acc:0.980]
Epoch [60/120    avg_loss:0.068, val_acc:0.980]
Epoch [61/120    avg_loss:0.065, val_acc:0.980]
Epoch [62/120    avg_loss:0.059, val_acc:0.980]
Epoch [63/120    avg_loss:0.071, val_acc:0.978]
Epoch [64/120    avg_loss:0.070, val_acc:0.976]
Epoch [65/120    avg_loss:0.060, val_acc:0.984]
Epoch [66/120    avg_loss:0.070, val_acc:0.984]
Epoch [67/120    avg_loss:0.060, val_acc:0.978]
Epoch [68/120    avg_loss:0.068, val_acc:0.978]
Epoch [69/120    avg_loss:0.063, val_acc:0.976]
Epoch [70/120    avg_loss:0.057, val_acc:0.978]
Epoch [71/120    avg_loss:0.067, val_acc:0.980]
Epoch [72/120    avg_loss:0.063, val_acc:0.978]
Epoch [73/120    avg_loss:0.075, val_acc:0.982]
Epoch [74/120    avg_loss:0.053, val_acc:0.984]
Epoch [75/120    avg_loss:0.049, val_acc:0.982]
Epoch [76/120    avg_loss:0.051, val_acc:0.982]
Epoch [77/120    avg_loss:0.061, val_acc:0.982]
Epoch [78/120    avg_loss:0.059, val_acc:0.986]
Epoch [79/120    avg_loss:0.045, val_acc:0.988]
Epoch [80/120    avg_loss:0.054, val_acc:0.984]
Epoch [81/120    avg_loss:0.046, val_acc:0.982]
Epoch [82/120    avg_loss:0.045, val_acc:0.982]
Epoch [83/120    avg_loss:0.045, val_acc:0.982]
Epoch [84/120    avg_loss:0.051, val_acc:0.982]
Epoch [85/120    avg_loss:0.048, val_acc:0.984]
Epoch [86/120    avg_loss:0.052, val_acc:0.984]
Epoch [87/120    avg_loss:0.057, val_acc:0.980]
Epoch [88/120    avg_loss:0.054, val_acc:0.982]
Epoch [89/120    avg_loss:0.064, val_acc:0.984]
Epoch [90/120    avg_loss:0.055, val_acc:0.984]
Epoch [91/120    avg_loss:0.044, val_acc:0.982]
Epoch [92/120    avg_loss:0.052, val_acc:0.984]
Epoch [93/120    avg_loss:0.053, val_acc:0.984]
Epoch [94/120    avg_loss:0.048, val_acc:0.984]
Epoch [95/120    avg_loss:0.040, val_acc:0.984]
Epoch [96/120    avg_loss:0.044, val_acc:0.986]
Epoch [97/120    avg_loss:0.046, val_acc:0.986]
Epoch [98/120    avg_loss:0.047, val_acc:0.986]
Epoch [99/120    avg_loss:0.046, val_acc:0.986]
Epoch [100/120    avg_loss:0.043, val_acc:0.986]
Epoch [101/120    avg_loss:0.047, val_acc:0.986]
Epoch [102/120    avg_loss:0.043, val_acc:0.984]
Epoch [103/120    avg_loss:0.049, val_acc:0.984]
Epoch [104/120    avg_loss:0.047, val_acc:0.984]
Epoch [105/120    avg_loss:0.047, val_acc:0.984]
Epoch [106/120    avg_loss:0.046, val_acc:0.984]
Epoch [107/120    avg_loss:0.038, val_acc:0.984]
Epoch [108/120    avg_loss:0.042, val_acc:0.984]
Epoch [109/120    avg_loss:0.046, val_acc:0.984]
Epoch [110/120    avg_loss:0.045, val_acc:0.984]
Epoch [111/120    avg_loss:0.039, val_acc:0.984]
Epoch [112/120    avg_loss:0.041, val_acc:0.984]
Epoch [113/120    avg_loss:0.049, val_acc:0.984]
Epoch [114/120    avg_loss:0.048, val_acc:0.984]
Epoch [115/120    avg_loss:0.046, val_acc:0.984]
Epoch [116/120    avg_loss:0.056, val_acc:0.984]
Epoch [117/120    avg_loss:0.038, val_acc:0.984]
Epoch [118/120    avg_loss:0.050, val_acc:0.984]
Epoch [119/120    avg_loss:0.047, val_acc:0.984]
Epoch [120/120    avg_loss:0.049, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   0   7   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 227   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99486427 0.99319728 1.         0.95578947 0.92592593
 0.98086124 0.98378378 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9926416654018132
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7b13502908>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.469, val_acc:0.371]
Epoch [2/120    avg_loss:2.152, val_acc:0.588]
Epoch [3/120    avg_loss:1.926, val_acc:0.625]
Epoch [4/120    avg_loss:1.724, val_acc:0.698]
Epoch [5/120    avg_loss:1.507, val_acc:0.675]
Epoch [6/120    avg_loss:1.310, val_acc:0.773]
Epoch [7/120    avg_loss:1.159, val_acc:0.762]
Epoch [8/120    avg_loss:0.977, val_acc:0.781]
Epoch [9/120    avg_loss:0.886, val_acc:0.800]
Epoch [10/120    avg_loss:0.770, val_acc:0.810]
Epoch [11/120    avg_loss:0.681, val_acc:0.823]
Epoch [12/120    avg_loss:0.625, val_acc:0.798]
Epoch [13/120    avg_loss:0.594, val_acc:0.898]
Epoch [14/120    avg_loss:0.552, val_acc:0.910]
Epoch [15/120    avg_loss:0.506, val_acc:0.858]
Epoch [16/120    avg_loss:0.509, val_acc:0.915]
Epoch [17/120    avg_loss:0.466, val_acc:0.879]
Epoch [18/120    avg_loss:0.433, val_acc:0.867]
Epoch [19/120    avg_loss:0.382, val_acc:0.925]
Epoch [20/120    avg_loss:0.335, val_acc:0.912]
Epoch [21/120    avg_loss:0.308, val_acc:0.883]
Epoch [22/120    avg_loss:0.310, val_acc:0.917]
Epoch [23/120    avg_loss:0.314, val_acc:0.940]
Epoch [24/120    avg_loss:0.308, val_acc:0.950]
Epoch [25/120    avg_loss:0.250, val_acc:0.956]
Epoch [26/120    avg_loss:0.244, val_acc:0.929]
Epoch [27/120    avg_loss:0.312, val_acc:0.942]
Epoch [28/120    avg_loss:0.230, val_acc:0.921]
Epoch [29/120    avg_loss:0.240, val_acc:0.931]
Epoch [30/120    avg_loss:0.194, val_acc:0.912]
Epoch [31/120    avg_loss:0.212, val_acc:0.963]
Epoch [32/120    avg_loss:0.186, val_acc:0.960]
Epoch [33/120    avg_loss:0.222, val_acc:0.923]
Epoch [34/120    avg_loss:0.224, val_acc:0.958]
Epoch [35/120    avg_loss:0.172, val_acc:0.952]
Epoch [36/120    avg_loss:0.134, val_acc:0.963]
Epoch [37/120    avg_loss:0.123, val_acc:0.931]
Epoch [38/120    avg_loss:0.170, val_acc:0.965]
Epoch [39/120    avg_loss:0.196, val_acc:0.958]
Epoch [40/120    avg_loss:0.162, val_acc:0.923]
Epoch [41/120    avg_loss:0.161, val_acc:0.963]
Epoch [42/120    avg_loss:0.136, val_acc:0.967]
Epoch [43/120    avg_loss:0.156, val_acc:0.963]
Epoch [44/120    avg_loss:0.117, val_acc:0.971]
Epoch [45/120    avg_loss:0.099, val_acc:0.948]
Epoch [46/120    avg_loss:0.124, val_acc:0.965]
Epoch [47/120    avg_loss:0.122, val_acc:0.965]
Epoch [48/120    avg_loss:0.142, val_acc:0.952]
Epoch [49/120    avg_loss:0.173, val_acc:0.973]
Epoch [50/120    avg_loss:0.110, val_acc:0.960]
Epoch [51/120    avg_loss:0.100, val_acc:0.965]
Epoch [52/120    avg_loss:0.072, val_acc:0.981]
Epoch [53/120    avg_loss:0.072, val_acc:0.975]
Epoch [54/120    avg_loss:0.094, val_acc:0.973]
Epoch [55/120    avg_loss:0.096, val_acc:0.965]
Epoch [56/120    avg_loss:0.103, val_acc:0.981]
Epoch [57/120    avg_loss:0.087, val_acc:0.977]
Epoch [58/120    avg_loss:0.097, val_acc:0.967]
Epoch [59/120    avg_loss:0.141, val_acc:0.979]
Epoch [60/120    avg_loss:0.083, val_acc:0.988]
Epoch [61/120    avg_loss:0.069, val_acc:0.971]
Epoch [62/120    avg_loss:0.075, val_acc:0.983]
Epoch [63/120    avg_loss:0.055, val_acc:0.988]
Epoch [64/120    avg_loss:0.072, val_acc:0.981]
Epoch [65/120    avg_loss:0.099, val_acc:0.981]
Epoch [66/120    avg_loss:0.060, val_acc:0.977]
Epoch [67/120    avg_loss:0.081, val_acc:0.935]
Epoch [68/120    avg_loss:0.064, val_acc:0.977]
Epoch [69/120    avg_loss:0.050, val_acc:0.983]
Epoch [70/120    avg_loss:0.052, val_acc:0.988]
Epoch [71/120    avg_loss:0.073, val_acc:0.979]
Epoch [72/120    avg_loss:0.076, val_acc:0.977]
Epoch [73/120    avg_loss:0.066, val_acc:0.969]
Epoch [74/120    avg_loss:0.141, val_acc:0.956]
Epoch [75/120    avg_loss:0.107, val_acc:0.977]
Epoch [76/120    avg_loss:0.054, val_acc:0.973]
Epoch [77/120    avg_loss:0.037, val_acc:0.979]
Epoch [78/120    avg_loss:0.041, val_acc:0.967]
Epoch [79/120    avg_loss:0.142, val_acc:0.921]
Epoch [80/120    avg_loss:0.084, val_acc:0.979]
Epoch [81/120    avg_loss:0.040, val_acc:0.979]
Epoch [82/120    avg_loss:0.038, val_acc:0.975]
Epoch [83/120    avg_loss:0.042, val_acc:0.990]
Epoch [84/120    avg_loss:0.027, val_acc:0.981]
Epoch [85/120    avg_loss:0.043, val_acc:0.971]
Epoch [86/120    avg_loss:0.035, val_acc:0.963]
Epoch [87/120    avg_loss:0.038, val_acc:0.979]
Epoch [88/120    avg_loss:0.045, val_acc:0.988]
Epoch [89/120    avg_loss:0.031, val_acc:0.990]
Epoch [90/120    avg_loss:0.037, val_acc:0.979]
Epoch [91/120    avg_loss:0.082, val_acc:0.979]
Epoch [92/120    avg_loss:0.053, val_acc:0.967]
Epoch [93/120    avg_loss:0.061, val_acc:0.971]
Epoch [94/120    avg_loss:0.036, val_acc:0.983]
Epoch [95/120    avg_loss:0.043, val_acc:0.990]
Epoch [96/120    avg_loss:0.033, val_acc:0.983]
Epoch [97/120    avg_loss:0.031, val_acc:0.979]
Epoch [98/120    avg_loss:0.017, val_acc:0.988]
Epoch [99/120    avg_loss:0.033, val_acc:0.988]
Epoch [100/120    avg_loss:0.026, val_acc:0.969]
Epoch [101/120    avg_loss:0.017, val_acc:0.988]
Epoch [102/120    avg_loss:0.016, val_acc:0.985]
Epoch [103/120    avg_loss:0.026, val_acc:0.981]
Epoch [104/120    avg_loss:0.051, val_acc:0.954]
Epoch [105/120    avg_loss:0.058, val_acc:0.979]
Epoch [106/120    avg_loss:0.047, val_acc:0.981]
Epoch [107/120    avg_loss:0.021, val_acc:0.979]
Epoch [108/120    avg_loss:0.017, val_acc:0.985]
Epoch [109/120    avg_loss:0.020, val_acc:0.990]
Epoch [110/120    avg_loss:0.013, val_acc:0.990]
Epoch [111/120    avg_loss:0.018, val_acc:0.990]
Epoch [112/120    avg_loss:0.015, val_acc:0.990]
Epoch [113/120    avg_loss:0.012, val_acc:0.990]
Epoch [114/120    avg_loss:0.014, val_acc:0.990]
Epoch [115/120    avg_loss:0.011, val_acc:0.990]
Epoch [116/120    avg_loss:0.013, val_acc:0.990]
Epoch [117/120    avg_loss:0.010, val_acc:0.990]
Epoch [118/120    avg_loss:0.009, val_acc:0.990]
Epoch [119/120    avg_loss:0.016, val_acc:0.990]
Epoch [120/120    avg_loss:0.010, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   3   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 0.99707174 0.9977221  1.         0.96103896 0.92631579
 0.99757869 0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.994540590735716
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f552d4668d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.515, val_acc:0.419]
Epoch [2/120    avg_loss:2.218, val_acc:0.473]
Epoch [3/120    avg_loss:1.962, val_acc:0.581]
Epoch [4/120    avg_loss:1.734, val_acc:0.642]
Epoch [5/120    avg_loss:1.481, val_acc:0.717]
Epoch [6/120    avg_loss:1.298, val_acc:0.721]
Epoch [7/120    avg_loss:1.133, val_acc:0.702]
Epoch [8/120    avg_loss:1.030, val_acc:0.756]
Epoch [9/120    avg_loss:0.893, val_acc:0.775]
Epoch [10/120    avg_loss:0.809, val_acc:0.762]
Epoch [11/120    avg_loss:0.798, val_acc:0.798]
Epoch [12/120    avg_loss:0.708, val_acc:0.794]
Epoch [13/120    avg_loss:0.701, val_acc:0.819]
Epoch [14/120    avg_loss:0.656, val_acc:0.848]
Epoch [15/120    avg_loss:0.631, val_acc:0.900]
Epoch [16/120    avg_loss:0.584, val_acc:0.923]
Epoch [17/120    avg_loss:0.524, val_acc:0.900]
Epoch [18/120    avg_loss:0.456, val_acc:0.877]
Epoch [19/120    avg_loss:0.444, val_acc:0.919]
Epoch [20/120    avg_loss:0.428, val_acc:0.925]
Epoch [21/120    avg_loss:0.409, val_acc:0.931]
Epoch [22/120    avg_loss:0.359, val_acc:0.925]
Epoch [23/120    avg_loss:0.387, val_acc:0.919]
Epoch [24/120    avg_loss:0.332, val_acc:0.952]
Epoch [25/120    avg_loss:0.331, val_acc:0.940]
Epoch [26/120    avg_loss:0.280, val_acc:0.921]
Epoch [27/120    avg_loss:0.280, val_acc:0.954]
Epoch [28/120    avg_loss:0.312, val_acc:0.938]
Epoch [29/120    avg_loss:0.291, val_acc:0.956]
Epoch [30/120    avg_loss:0.251, val_acc:0.942]
Epoch [31/120    avg_loss:0.267, val_acc:0.940]
Epoch [32/120    avg_loss:0.317, val_acc:0.963]
Epoch [33/120    avg_loss:0.210, val_acc:0.969]
Epoch [34/120    avg_loss:0.207, val_acc:0.967]
Epoch [35/120    avg_loss:0.166, val_acc:0.973]
Epoch [36/120    avg_loss:0.205, val_acc:0.963]
Epoch [37/120    avg_loss:0.223, val_acc:0.954]
Epoch [38/120    avg_loss:0.216, val_acc:0.971]
Epoch [39/120    avg_loss:0.170, val_acc:0.979]
Epoch [40/120    avg_loss:0.156, val_acc:0.977]
Epoch [41/120    avg_loss:0.199, val_acc:0.942]
Epoch [42/120    avg_loss:0.210, val_acc:0.971]
Epoch [43/120    avg_loss:0.184, val_acc:0.960]
Epoch [44/120    avg_loss:0.184, val_acc:0.946]
Epoch [45/120    avg_loss:0.133, val_acc:0.979]
Epoch [46/120    avg_loss:0.100, val_acc:0.981]
Epoch [47/120    avg_loss:0.120, val_acc:0.977]
Epoch [48/120    avg_loss:0.130, val_acc:0.979]
Epoch [49/120    avg_loss:0.133, val_acc:0.969]
Epoch [50/120    avg_loss:0.126, val_acc:0.979]
Epoch [51/120    avg_loss:0.125, val_acc:0.950]
Epoch [52/120    avg_loss:0.176, val_acc:0.977]
Epoch [53/120    avg_loss:0.197, val_acc:0.971]
Epoch [54/120    avg_loss:0.125, val_acc:0.983]
Epoch [55/120    avg_loss:0.105, val_acc:0.983]
Epoch [56/120    avg_loss:0.160, val_acc:0.944]
Epoch [57/120    avg_loss:0.165, val_acc:0.985]
Epoch [58/120    avg_loss:0.107, val_acc:0.994]
Epoch [59/120    avg_loss:0.090, val_acc:0.975]
Epoch [60/120    avg_loss:0.081, val_acc:0.990]
Epoch [61/120    avg_loss:0.068, val_acc:0.990]
Epoch [62/120    avg_loss:0.063, val_acc:0.990]
Epoch [63/120    avg_loss:0.070, val_acc:0.988]
Epoch [64/120    avg_loss:0.088, val_acc:0.975]
Epoch [65/120    avg_loss:0.065, val_acc:0.996]
Epoch [66/120    avg_loss:0.067, val_acc:0.988]
Epoch [67/120    avg_loss:0.078, val_acc:0.988]
Epoch [68/120    avg_loss:0.067, val_acc:0.988]
Epoch [69/120    avg_loss:0.068, val_acc:0.992]
Epoch [70/120    avg_loss:0.069, val_acc:0.994]
Epoch [71/120    avg_loss:0.043, val_acc:0.996]
Epoch [72/120    avg_loss:0.067, val_acc:0.988]
Epoch [73/120    avg_loss:0.092, val_acc:0.983]
Epoch [74/120    avg_loss:0.076, val_acc:0.988]
Epoch [75/120    avg_loss:0.100, val_acc:0.990]
Epoch [76/120    avg_loss:0.063, val_acc:0.994]
Epoch [77/120    avg_loss:0.079, val_acc:0.992]
Epoch [78/120    avg_loss:0.046, val_acc:0.996]
Epoch [79/120    avg_loss:0.056, val_acc:0.973]
Epoch [80/120    avg_loss:0.055, val_acc:0.996]
Epoch [81/120    avg_loss:0.038, val_acc:0.994]
Epoch [82/120    avg_loss:0.071, val_acc:0.990]
Epoch [83/120    avg_loss:0.049, val_acc:0.994]
Epoch [84/120    avg_loss:0.057, val_acc:0.992]
Epoch [85/120    avg_loss:0.030, val_acc:0.990]
Epoch [86/120    avg_loss:0.036, val_acc:0.996]
Epoch [87/120    avg_loss:0.049, val_acc:0.990]
Epoch [88/120    avg_loss:0.063, val_acc:0.992]
Epoch [89/120    avg_loss:0.047, val_acc:0.996]
Epoch [90/120    avg_loss:0.024, val_acc:0.998]
Epoch [91/120    avg_loss:0.026, val_acc:0.990]
Epoch [92/120    avg_loss:0.025, val_acc:0.988]
Epoch [93/120    avg_loss:0.025, val_acc:0.998]
Epoch [94/120    avg_loss:0.024, val_acc:0.996]
Epoch [95/120    avg_loss:0.044, val_acc:0.996]
Epoch [96/120    avg_loss:0.031, val_acc:0.996]
Epoch [97/120    avg_loss:0.023, val_acc:0.996]
Epoch [98/120    avg_loss:0.016, val_acc:0.998]
Epoch [99/120    avg_loss:0.020, val_acc:0.994]
Epoch [100/120    avg_loss:0.021, val_acc:0.998]
Epoch [101/120    avg_loss:0.016, val_acc:1.000]
Epoch [102/120    avg_loss:0.018, val_acc:0.994]
Epoch [103/120    avg_loss:0.016, val_acc:0.996]
Epoch [104/120    avg_loss:0.016, val_acc:0.998]
Epoch [105/120    avg_loss:0.015, val_acc:0.998]
Epoch [106/120    avg_loss:0.024, val_acc:0.996]
Epoch [107/120    avg_loss:0.058, val_acc:0.967]
Epoch [108/120    avg_loss:0.075, val_acc:0.983]
Epoch [109/120    avg_loss:0.078, val_acc:0.996]
Epoch [110/120    avg_loss:0.037, val_acc:0.983]
Epoch [111/120    avg_loss:0.024, val_acc:0.996]
Epoch [112/120    avg_loss:0.032, val_acc:0.983]
Epoch [113/120    avg_loss:0.025, val_acc:0.992]
Epoch [114/120    avg_loss:0.033, val_acc:0.996]
Epoch [115/120    avg_loss:0.017, val_acc:0.996]
Epoch [116/120    avg_loss:0.015, val_acc:0.996]
Epoch [117/120    avg_loss:0.026, val_acc:0.996]
Epoch [118/120    avg_loss:0.016, val_acc:0.998]
Epoch [119/120    avg_loss:0.014, val_acc:0.998]
Epoch [120/120    avg_loss:0.015, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.6588486140725

F1 scores:
[       nan 1.         0.99095023 1.         0.97297297 0.96
 1.         0.97826087 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9962019582387492
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f027cff3908>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.498, val_acc:0.371]
Epoch [2/120    avg_loss:2.200, val_acc:0.504]
Epoch [3/120    avg_loss:2.005, val_acc:0.607]
Epoch [4/120    avg_loss:1.777, val_acc:0.643]
Epoch [5/120    avg_loss:1.551, val_acc:0.690]
Epoch [6/120    avg_loss:1.324, val_acc:0.724]
Epoch [7/120    avg_loss:1.130, val_acc:0.724]
Epoch [8/120    avg_loss:0.983, val_acc:0.756]
Epoch [9/120    avg_loss:0.915, val_acc:0.798]
Epoch [10/120    avg_loss:0.834, val_acc:0.817]
Epoch [11/120    avg_loss:0.733, val_acc:0.776]
Epoch [12/120    avg_loss:0.700, val_acc:0.788]
Epoch [13/120    avg_loss:0.626, val_acc:0.835]
Epoch [14/120    avg_loss:0.595, val_acc:0.829]
Epoch [15/120    avg_loss:0.540, val_acc:0.798]
Epoch [16/120    avg_loss:0.516, val_acc:0.859]
Epoch [17/120    avg_loss:0.559, val_acc:0.919]
Epoch [18/120    avg_loss:0.483, val_acc:0.907]
Epoch [19/120    avg_loss:0.462, val_acc:0.911]
Epoch [20/120    avg_loss:0.417, val_acc:0.925]
Epoch [21/120    avg_loss:0.396, val_acc:0.915]
Epoch [22/120    avg_loss:0.378, val_acc:0.940]
Epoch [23/120    avg_loss:0.343, val_acc:0.917]
Epoch [24/120    avg_loss:0.336, val_acc:0.913]
Epoch [25/120    avg_loss:0.335, val_acc:0.931]
Epoch [26/120    avg_loss:0.336, val_acc:0.929]
Epoch [27/120    avg_loss:0.277, val_acc:0.927]
Epoch [28/120    avg_loss:0.261, val_acc:0.948]
Epoch [29/120    avg_loss:0.241, val_acc:0.940]
Epoch [30/120    avg_loss:0.268, val_acc:0.954]
Epoch [31/120    avg_loss:0.230, val_acc:0.954]
Epoch [32/120    avg_loss:0.209, val_acc:0.952]
Epoch [33/120    avg_loss:0.211, val_acc:0.952]
Epoch [34/120    avg_loss:0.214, val_acc:0.923]
Epoch [35/120    avg_loss:0.219, val_acc:0.954]
Epoch [36/120    avg_loss:0.212, val_acc:0.952]
Epoch [37/120    avg_loss:0.227, val_acc:0.966]
Epoch [38/120    avg_loss:0.286, val_acc:0.931]
Epoch [39/120    avg_loss:0.225, val_acc:0.944]
Epoch [40/120    avg_loss:0.181, val_acc:0.954]
Epoch [41/120    avg_loss:0.165, val_acc:0.960]
Epoch [42/120    avg_loss:0.254, val_acc:0.942]
Epoch [43/120    avg_loss:0.209, val_acc:0.964]
Epoch [44/120    avg_loss:0.160, val_acc:0.968]
Epoch [45/120    avg_loss:0.176, val_acc:0.925]
Epoch [46/120    avg_loss:0.222, val_acc:0.972]
Epoch [47/120    avg_loss:0.183, val_acc:0.954]
Epoch [48/120    avg_loss:0.141, val_acc:0.968]
Epoch [49/120    avg_loss:0.119, val_acc:0.972]
Epoch [50/120    avg_loss:0.124, val_acc:0.962]
Epoch [51/120    avg_loss:0.158, val_acc:0.974]
Epoch [52/120    avg_loss:0.119, val_acc:0.972]
Epoch [53/120    avg_loss:0.163, val_acc:0.970]
Epoch [54/120    avg_loss:0.133, val_acc:0.974]
Epoch [55/120    avg_loss:0.102, val_acc:0.962]
Epoch [56/120    avg_loss:0.136, val_acc:0.974]
Epoch [57/120    avg_loss:0.136, val_acc:0.976]
Epoch [58/120    avg_loss:0.094, val_acc:0.964]
Epoch [59/120    avg_loss:0.098, val_acc:0.972]
Epoch [60/120    avg_loss:0.098, val_acc:0.978]
Epoch [61/120    avg_loss:0.153, val_acc:0.942]
Epoch [62/120    avg_loss:0.121, val_acc:0.976]
Epoch [63/120    avg_loss:0.085, val_acc:0.974]
Epoch [64/120    avg_loss:0.073, val_acc:0.970]
Epoch [65/120    avg_loss:0.073, val_acc:0.974]
Epoch [66/120    avg_loss:0.089, val_acc:0.978]
Epoch [67/120    avg_loss:0.168, val_acc:0.976]
Epoch [68/120    avg_loss:0.085, val_acc:0.974]
Epoch [69/120    avg_loss:0.109, val_acc:0.970]
Epoch [70/120    avg_loss:0.059, val_acc:0.982]
Epoch [71/120    avg_loss:0.065, val_acc:0.968]
Epoch [72/120    avg_loss:0.116, val_acc:0.974]
Epoch [73/120    avg_loss:0.074, val_acc:0.980]
Epoch [74/120    avg_loss:0.072, val_acc:0.976]
Epoch [75/120    avg_loss:0.134, val_acc:0.942]
Epoch [76/120    avg_loss:0.169, val_acc:0.974]
Epoch [77/120    avg_loss:0.136, val_acc:0.972]
Epoch [78/120    avg_loss:0.081, val_acc:0.984]
Epoch [79/120    avg_loss:0.050, val_acc:0.986]
Epoch [80/120    avg_loss:0.067, val_acc:0.982]
Epoch [81/120    avg_loss:0.068, val_acc:0.984]
Epoch [82/120    avg_loss:0.069, val_acc:0.974]
Epoch [83/120    avg_loss:0.067, val_acc:0.980]
Epoch [84/120    avg_loss:0.056, val_acc:0.974]
Epoch [85/120    avg_loss:0.061, val_acc:0.982]
Epoch [86/120    avg_loss:0.055, val_acc:0.972]
Epoch [87/120    avg_loss:0.070, val_acc:0.976]
Epoch [88/120    avg_loss:0.063, val_acc:0.982]
Epoch [89/120    avg_loss:0.060, val_acc:0.976]
Epoch [90/120    avg_loss:0.043, val_acc:0.976]
Epoch [91/120    avg_loss:0.055, val_acc:0.978]
Epoch [92/120    avg_loss:0.046, val_acc:0.986]
Epoch [93/120    avg_loss:0.035, val_acc:0.982]
Epoch [94/120    avg_loss:0.031, val_acc:0.984]
Epoch [95/120    avg_loss:0.030, val_acc:0.978]
Epoch [96/120    avg_loss:0.050, val_acc:0.974]
Epoch [97/120    avg_loss:0.031, val_acc:0.982]
Epoch [98/120    avg_loss:0.033, val_acc:0.978]
Epoch [99/120    avg_loss:0.046, val_acc:0.982]
Epoch [100/120    avg_loss:0.066, val_acc:0.980]
Epoch [101/120    avg_loss:0.051, val_acc:0.980]
Epoch [102/120    avg_loss:0.036, val_acc:0.982]
Epoch [103/120    avg_loss:0.021, val_acc:0.982]
Epoch [104/120    avg_loss:0.021, val_acc:0.984]
Epoch [105/120    avg_loss:0.019, val_acc:0.976]
Epoch [106/120    avg_loss:0.027, val_acc:0.978]
Epoch [107/120    avg_loss:0.015, val_acc:0.982]
Epoch [108/120    avg_loss:0.020, val_acc:0.984]
Epoch [109/120    avg_loss:0.023, val_acc:0.984]
Epoch [110/120    avg_loss:0.017, val_acc:0.984]
Epoch [111/120    avg_loss:0.016, val_acc:0.986]
Epoch [112/120    avg_loss:0.016, val_acc:0.984]
Epoch [113/120    avg_loss:0.019, val_acc:0.986]
Epoch [114/120    avg_loss:0.014, val_acc:0.986]
Epoch [115/120    avg_loss:0.016, val_acc:0.984]
Epoch [116/120    avg_loss:0.018, val_acc:0.984]
Epoch [117/120    avg_loss:0.013, val_acc:0.984]
Epoch [118/120    avg_loss:0.019, val_acc:0.986]
Epoch [119/120    avg_loss:0.019, val_acc:0.986]
Epoch [120/120    avg_loss:0.015, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.59488272921108

F1 scores:
[       nan 0.996337   0.99545455 1.         0.98454746 0.97594502
 0.98800959 0.98924731 1.         1.         1.         0.99341238
 0.99445061 1.        ]

Kappa:
0.9954903474153152
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff76c577978>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.493, val_acc:0.323]
Epoch [2/120    avg_loss:2.176, val_acc:0.468]
Epoch [3/120    avg_loss:1.928, val_acc:0.567]
Epoch [4/120    avg_loss:1.765, val_acc:0.663]
Epoch [5/120    avg_loss:1.512, val_acc:0.692]
Epoch [6/120    avg_loss:1.314, val_acc:0.706]
Epoch [7/120    avg_loss:1.127, val_acc:0.722]
Epoch [8/120    avg_loss:0.970, val_acc:0.744]
Epoch [9/120    avg_loss:0.837, val_acc:0.772]
Epoch [10/120    avg_loss:0.737, val_acc:0.817]
Epoch [11/120    avg_loss:0.651, val_acc:0.815]
Epoch [12/120    avg_loss:0.620, val_acc:0.827]
Epoch [13/120    avg_loss:0.577, val_acc:0.889]
Epoch [14/120    avg_loss:0.516, val_acc:0.895]
Epoch [15/120    avg_loss:0.521, val_acc:0.843]
Epoch [16/120    avg_loss:0.481, val_acc:0.867]
Epoch [17/120    avg_loss:0.445, val_acc:0.923]
Epoch [18/120    avg_loss:0.464, val_acc:0.927]
Epoch [19/120    avg_loss:0.397, val_acc:0.881]
Epoch [20/120    avg_loss:0.412, val_acc:0.929]
Epoch [21/120    avg_loss:0.355, val_acc:0.931]
Epoch [22/120    avg_loss:0.373, val_acc:0.935]
Epoch [23/120    avg_loss:0.306, val_acc:0.927]
Epoch [24/120    avg_loss:0.321, val_acc:0.935]
Epoch [25/120    avg_loss:0.295, val_acc:0.950]
Epoch [26/120    avg_loss:0.268, val_acc:0.946]
Epoch [27/120    avg_loss:0.274, val_acc:0.895]
Epoch [28/120    avg_loss:0.282, val_acc:0.942]
Epoch [29/120    avg_loss:0.227, val_acc:0.962]
Epoch [30/120    avg_loss:0.206, val_acc:0.960]
Epoch [31/120    avg_loss:0.237, val_acc:0.948]
Epoch [32/120    avg_loss:0.211, val_acc:0.962]
Epoch [33/120    avg_loss:0.242, val_acc:0.944]
Epoch [34/120    avg_loss:0.246, val_acc:0.950]
Epoch [35/120    avg_loss:0.173, val_acc:0.968]
Epoch [36/120    avg_loss:0.198, val_acc:0.968]
Epoch [37/120    avg_loss:0.241, val_acc:0.958]
Epoch [38/120    avg_loss:0.160, val_acc:0.976]
Epoch [39/120    avg_loss:0.178, val_acc:0.968]
Epoch [40/120    avg_loss:0.186, val_acc:0.970]
Epoch [41/120    avg_loss:0.176, val_acc:0.956]
Epoch [42/120    avg_loss:0.174, val_acc:0.964]
Epoch [43/120    avg_loss:0.139, val_acc:0.972]
Epoch [44/120    avg_loss:0.150, val_acc:0.978]
Epoch [45/120    avg_loss:0.094, val_acc:0.980]
Epoch [46/120    avg_loss:0.128, val_acc:0.982]
Epoch [47/120    avg_loss:0.094, val_acc:0.984]
Epoch [48/120    avg_loss:0.106, val_acc:0.960]
Epoch [49/120    avg_loss:0.122, val_acc:0.976]
Epoch [50/120    avg_loss:0.116, val_acc:0.984]
Epoch [51/120    avg_loss:0.106, val_acc:0.972]
Epoch [52/120    avg_loss:0.144, val_acc:0.970]
Epoch [53/120    avg_loss:0.141, val_acc:0.980]
Epoch [54/120    avg_loss:0.118, val_acc:0.990]
Epoch [55/120    avg_loss:0.059, val_acc:0.984]
Epoch [56/120    avg_loss:0.087, val_acc:0.986]
Epoch [57/120    avg_loss:0.096, val_acc:0.966]
Epoch [58/120    avg_loss:0.117, val_acc:0.990]
Epoch [59/120    avg_loss:0.083, val_acc:0.980]
Epoch [60/120    avg_loss:0.053, val_acc:0.986]
Epoch [61/120    avg_loss:0.075, val_acc:0.972]
Epoch [62/120    avg_loss:0.065, val_acc:0.984]
Epoch [63/120    avg_loss:0.171, val_acc:0.962]
Epoch [64/120    avg_loss:0.091, val_acc:0.982]
Epoch [65/120    avg_loss:0.070, val_acc:0.992]
Epoch [66/120    avg_loss:0.064, val_acc:0.988]
Epoch [67/120    avg_loss:0.081, val_acc:0.976]
Epoch [68/120    avg_loss:0.081, val_acc:0.986]
Epoch [69/120    avg_loss:0.052, val_acc:0.992]
Epoch [70/120    avg_loss:0.055, val_acc:0.978]
Epoch [71/120    avg_loss:0.134, val_acc:0.990]
Epoch [72/120    avg_loss:0.058, val_acc:0.984]
Epoch [73/120    avg_loss:0.049, val_acc:0.992]
Epoch [74/120    avg_loss:0.041, val_acc:0.978]
Epoch [75/120    avg_loss:0.055, val_acc:0.972]
Epoch [76/120    avg_loss:0.074, val_acc:0.990]
Epoch [77/120    avg_loss:0.041, val_acc:0.992]
Epoch [78/120    avg_loss:0.039, val_acc:0.988]
Epoch [79/120    avg_loss:0.049, val_acc:0.994]
Epoch [80/120    avg_loss:0.029, val_acc:0.986]
Epoch [81/120    avg_loss:0.034, val_acc:0.994]
Epoch [82/120    avg_loss:0.029, val_acc:0.994]
Epoch [83/120    avg_loss:0.028, val_acc:0.990]
Epoch [84/120    avg_loss:0.038, val_acc:0.986]
Epoch [85/120    avg_loss:0.050, val_acc:0.982]
Epoch [86/120    avg_loss:0.119, val_acc:0.982]
Epoch [87/120    avg_loss:0.070, val_acc:0.984]
Epoch [88/120    avg_loss:0.079, val_acc:0.986]
Epoch [89/120    avg_loss:0.045, val_acc:0.996]
Epoch [90/120    avg_loss:0.037, val_acc:0.994]
Epoch [91/120    avg_loss:0.031, val_acc:0.998]
Epoch [92/120    avg_loss:0.024, val_acc:0.998]
Epoch [93/120    avg_loss:0.020, val_acc:0.996]
Epoch [94/120    avg_loss:0.019, val_acc:0.998]
Epoch [95/120    avg_loss:0.022, val_acc:0.994]
Epoch [96/120    avg_loss:0.018, val_acc:0.998]
Epoch [97/120    avg_loss:0.022, val_acc:0.996]
Epoch [98/120    avg_loss:0.021, val_acc:0.996]
Epoch [99/120    avg_loss:0.024, val_acc:0.996]
Epoch [100/120    avg_loss:0.015, val_acc:0.998]
Epoch [101/120    avg_loss:0.019, val_acc:0.996]
Epoch [102/120    avg_loss:0.012, val_acc:0.996]
Epoch [103/120    avg_loss:0.022, val_acc:0.998]
Epoch [104/120    avg_loss:0.018, val_acc:0.990]
Epoch [105/120    avg_loss:0.025, val_acc:0.998]
Epoch [106/120    avg_loss:0.019, val_acc:0.998]
Epoch [107/120    avg_loss:0.020, val_acc:0.992]
Epoch [108/120    avg_loss:0.046, val_acc:0.992]
Epoch [109/120    avg_loss:0.031, val_acc:0.982]
Epoch [110/120    avg_loss:0.027, val_acc:0.992]
Epoch [111/120    avg_loss:0.012, val_acc:0.998]
Epoch [112/120    avg_loss:0.012, val_acc:0.992]
Epoch [113/120    avg_loss:0.012, val_acc:0.998]
Epoch [114/120    avg_loss:0.011, val_acc:0.998]
Epoch [115/120    avg_loss:0.009, val_acc:0.996]
Epoch [116/120    avg_loss:0.009, val_acc:0.998]
Epoch [117/120    avg_loss:0.008, val_acc:0.998]
Epoch [118/120    avg_loss:0.010, val_acc:0.998]
Epoch [119/120    avg_loss:0.008, val_acc:0.998]
Epoch [120/120    avg_loss:0.022, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   1   1   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 0.99853801 0.97986577 1.         0.96137339 0.93571429
 1.         0.94972067 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.993352973074125
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6d3e91b7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.466, val_acc:0.440]
Epoch [2/120    avg_loss:2.130, val_acc:0.552]
Epoch [3/120    avg_loss:1.841, val_acc:0.567]
Epoch [4/120    avg_loss:1.620, val_acc:0.619]
Epoch [5/120    avg_loss:1.404, val_acc:0.688]
Epoch [6/120    avg_loss:1.237, val_acc:0.710]
Epoch [7/120    avg_loss:1.079, val_acc:0.742]
Epoch [8/120    avg_loss:0.939, val_acc:0.769]
Epoch [9/120    avg_loss:0.881, val_acc:0.740]
Epoch [10/120    avg_loss:0.811, val_acc:0.773]
Epoch [11/120    avg_loss:0.706, val_acc:0.796]
Epoch [12/120    avg_loss:0.658, val_acc:0.856]
Epoch [13/120    avg_loss:0.584, val_acc:0.894]
Epoch [14/120    avg_loss:0.466, val_acc:0.902]
Epoch [15/120    avg_loss:0.506, val_acc:0.881]
Epoch [16/120    avg_loss:0.433, val_acc:0.898]
Epoch [17/120    avg_loss:0.402, val_acc:0.885]
Epoch [18/120    avg_loss:0.375, val_acc:0.908]
Epoch [19/120    avg_loss:0.364, val_acc:0.890]
Epoch [20/120    avg_loss:0.338, val_acc:0.915]
Epoch [21/120    avg_loss:0.347, val_acc:0.921]
Epoch [22/120    avg_loss:0.314, val_acc:0.927]
Epoch [23/120    avg_loss:0.268, val_acc:0.923]
Epoch [24/120    avg_loss:0.295, val_acc:0.917]
Epoch [25/120    avg_loss:0.276, val_acc:0.919]
Epoch [26/120    avg_loss:0.294, val_acc:0.952]
Epoch [27/120    avg_loss:0.269, val_acc:0.929]
Epoch [28/120    avg_loss:0.250, val_acc:0.931]
Epoch [29/120    avg_loss:0.242, val_acc:0.921]
Epoch [30/120    avg_loss:0.334, val_acc:0.931]
Epoch [31/120    avg_loss:0.271, val_acc:0.944]
Epoch [32/120    avg_loss:0.196, val_acc:0.944]
Epoch [33/120    avg_loss:0.190, val_acc:0.952]
Epoch [34/120    avg_loss:0.203, val_acc:0.960]
Epoch [35/120    avg_loss:0.215, val_acc:0.944]
Epoch [36/120    avg_loss:0.204, val_acc:0.933]
Epoch [37/120    avg_loss:0.231, val_acc:0.960]
Epoch [38/120    avg_loss:0.205, val_acc:0.958]
Epoch [39/120    avg_loss:0.185, val_acc:0.950]
Epoch [40/120    avg_loss:0.228, val_acc:0.948]
Epoch [41/120    avg_loss:0.249, val_acc:0.931]
Epoch [42/120    avg_loss:0.268, val_acc:0.963]
Epoch [43/120    avg_loss:0.186, val_acc:0.965]
Epoch [44/120    avg_loss:0.217, val_acc:0.973]
Epoch [45/120    avg_loss:0.184, val_acc:0.954]
Epoch [46/120    avg_loss:0.201, val_acc:0.952]
Epoch [47/120    avg_loss:0.193, val_acc:0.960]
Epoch [48/120    avg_loss:0.164, val_acc:0.904]
Epoch [49/120    avg_loss:0.169, val_acc:0.950]
Epoch [50/120    avg_loss:0.133, val_acc:0.963]
Epoch [51/120    avg_loss:0.149, val_acc:0.965]
Epoch [52/120    avg_loss:0.115, val_acc:0.948]
Epoch [53/120    avg_loss:0.130, val_acc:0.975]
Epoch [54/120    avg_loss:0.141, val_acc:0.988]
Epoch [55/120    avg_loss:0.105, val_acc:0.981]
Epoch [56/120    avg_loss:0.119, val_acc:0.965]
Epoch [57/120    avg_loss:0.095, val_acc:0.973]
Epoch [58/120    avg_loss:0.110, val_acc:0.971]
Epoch [59/120    avg_loss:0.143, val_acc:0.954]
Epoch [60/120    avg_loss:0.117, val_acc:0.971]
Epoch [61/120    avg_loss:0.100, val_acc:0.975]
Epoch [62/120    avg_loss:0.101, val_acc:0.983]
Epoch [63/120    avg_loss:0.101, val_acc:0.988]
Epoch [64/120    avg_loss:0.099, val_acc:0.948]
Epoch [65/120    avg_loss:0.102, val_acc:0.992]
Epoch [66/120    avg_loss:0.094, val_acc:0.985]
Epoch [67/120    avg_loss:0.070, val_acc:0.985]
Epoch [68/120    avg_loss:0.047, val_acc:0.973]
Epoch [69/120    avg_loss:0.080, val_acc:0.977]
Epoch [70/120    avg_loss:0.050, val_acc:0.965]
Epoch [71/120    avg_loss:0.070, val_acc:0.985]
Epoch [72/120    avg_loss:0.079, val_acc:0.969]
Epoch [73/120    avg_loss:0.062, val_acc:0.967]
Epoch [74/120    avg_loss:0.054, val_acc:0.988]
Epoch [75/120    avg_loss:0.050, val_acc:0.983]
Epoch [76/120    avg_loss:0.054, val_acc:0.975]
Epoch [77/120    avg_loss:0.044, val_acc:0.981]
Epoch [78/120    avg_loss:0.072, val_acc:0.981]
Epoch [79/120    avg_loss:0.079, val_acc:0.983]
Epoch [80/120    avg_loss:0.042, val_acc:0.985]
Epoch [81/120    avg_loss:0.042, val_acc:0.988]
Epoch [82/120    avg_loss:0.026, val_acc:0.985]
Epoch [83/120    avg_loss:0.028, val_acc:0.985]
Epoch [84/120    avg_loss:0.031, val_acc:0.988]
Epoch [85/120    avg_loss:0.029, val_acc:0.988]
Epoch [86/120    avg_loss:0.035, val_acc:0.990]
Epoch [87/120    avg_loss:0.032, val_acc:0.990]
Epoch [88/120    avg_loss:0.032, val_acc:0.992]
Epoch [89/120    avg_loss:0.033, val_acc:0.990]
Epoch [90/120    avg_loss:0.028, val_acc:0.992]
Epoch [91/120    avg_loss:0.026, val_acc:0.992]
Epoch [92/120    avg_loss:0.035, val_acc:0.992]
Epoch [93/120    avg_loss:0.028, val_acc:0.992]
Epoch [94/120    avg_loss:0.027, val_acc:0.992]
Epoch [95/120    avg_loss:0.026, val_acc:0.992]
Epoch [96/120    avg_loss:0.025, val_acc:0.992]
Epoch [97/120    avg_loss:0.022, val_acc:0.992]
Epoch [98/120    avg_loss:0.032, val_acc:0.992]
Epoch [99/120    avg_loss:0.023, val_acc:0.992]
Epoch [100/120    avg_loss:0.025, val_acc:0.992]
Epoch [101/120    avg_loss:0.023, val_acc:0.992]
Epoch [102/120    avg_loss:0.024, val_acc:0.992]
Epoch [103/120    avg_loss:0.032, val_acc:0.992]
Epoch [104/120    avg_loss:0.029, val_acc:0.992]
Epoch [105/120    avg_loss:0.029, val_acc:0.992]
Epoch [106/120    avg_loss:0.024, val_acc:0.992]
Epoch [107/120    avg_loss:0.026, val_acc:0.992]
Epoch [108/120    avg_loss:0.021, val_acc:0.992]
Epoch [109/120    avg_loss:0.022, val_acc:0.992]
Epoch [110/120    avg_loss:0.023, val_acc:0.992]
Epoch [111/120    avg_loss:0.021, val_acc:0.992]
Epoch [112/120    avg_loss:0.024, val_acc:0.992]
Epoch [113/120    avg_loss:0.027, val_acc:0.992]
Epoch [114/120    avg_loss:0.024, val_acc:0.992]
Epoch [115/120    avg_loss:0.022, val_acc:0.992]
Epoch [116/120    avg_loss:0.022, val_acc:0.992]
Epoch [117/120    avg_loss:0.024, val_acc:0.992]
Epoch [118/120    avg_loss:0.027, val_acc:0.992]
Epoch [119/120    avg_loss:0.026, val_acc:0.992]
Epoch [120/120    avg_loss:0.028, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.53091684434968

F1 scores:
[       nan 0.99707174 0.99319728 1.         0.96659243 0.94915254
 0.99038462 0.98378378 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9947781236479387
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4cea2b4940>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.485, val_acc:0.504]
Epoch [2/120    avg_loss:2.115, val_acc:0.598]
Epoch [3/120    avg_loss:1.847, val_acc:0.692]
Epoch [4/120    avg_loss:1.597, val_acc:0.729]
Epoch [5/120    avg_loss:1.368, val_acc:0.752]
Epoch [6/120    avg_loss:1.146, val_acc:0.771]
Epoch [7/120    avg_loss:0.972, val_acc:0.775]
Epoch [8/120    avg_loss:0.841, val_acc:0.808]
Epoch [9/120    avg_loss:0.793, val_acc:0.769]
Epoch [10/120    avg_loss:0.704, val_acc:0.844]
Epoch [11/120    avg_loss:0.694, val_acc:0.823]
Epoch [12/120    avg_loss:0.626, val_acc:0.885]
Epoch [13/120    avg_loss:0.552, val_acc:0.848]
Epoch [14/120    avg_loss:0.538, val_acc:0.912]
Epoch [15/120    avg_loss:0.467, val_acc:0.898]
Epoch [16/120    avg_loss:0.542, val_acc:0.848]
Epoch [17/120    avg_loss:0.483, val_acc:0.927]
Epoch [18/120    avg_loss:0.423, val_acc:0.919]
Epoch [19/120    avg_loss:0.434, val_acc:0.923]
Epoch [20/120    avg_loss:0.356, val_acc:0.927]
Epoch [21/120    avg_loss:0.356, val_acc:0.927]
Epoch [22/120    avg_loss:0.411, val_acc:0.927]
Epoch [23/120    avg_loss:0.315, val_acc:0.931]
Epoch [24/120    avg_loss:0.393, val_acc:0.948]
Epoch [25/120    avg_loss:0.303, val_acc:0.935]
Epoch [26/120    avg_loss:0.310, val_acc:0.940]
Epoch [27/120    avg_loss:0.275, val_acc:0.950]
Epoch [28/120    avg_loss:0.260, val_acc:0.931]
Epoch [29/120    avg_loss:0.255, val_acc:0.958]
Epoch [30/120    avg_loss:0.228, val_acc:0.963]
Epoch [31/120    avg_loss:0.242, val_acc:0.940]
Epoch [32/120    avg_loss:0.221, val_acc:0.935]
Epoch [33/120    avg_loss:0.297, val_acc:0.929]
Epoch [34/120    avg_loss:0.267, val_acc:0.952]
Epoch [35/120    avg_loss:0.230, val_acc:0.948]
Epoch [36/120    avg_loss:0.239, val_acc:0.929]
Epoch [37/120    avg_loss:0.251, val_acc:0.921]
Epoch [38/120    avg_loss:0.246, val_acc:0.967]
Epoch [39/120    avg_loss:0.221, val_acc:0.946]
Epoch [40/120    avg_loss:0.184, val_acc:0.954]
Epoch [41/120    avg_loss:0.158, val_acc:0.967]
Epoch [42/120    avg_loss:0.180, val_acc:0.925]
Epoch [43/120    avg_loss:0.178, val_acc:0.948]
Epoch [44/120    avg_loss:0.160, val_acc:0.950]
Epoch [45/120    avg_loss:0.172, val_acc:0.973]
Epoch [46/120    avg_loss:0.197, val_acc:0.977]
Epoch [47/120    avg_loss:0.170, val_acc:0.971]
Epoch [48/120    avg_loss:0.135, val_acc:0.958]
Epoch [49/120    avg_loss:0.193, val_acc:0.963]
Epoch [50/120    avg_loss:0.151, val_acc:0.952]
Epoch [51/120    avg_loss:0.133, val_acc:0.954]
Epoch [52/120    avg_loss:0.176, val_acc:0.967]
Epoch [53/120    avg_loss:0.180, val_acc:0.923]
Epoch [54/120    avg_loss:0.159, val_acc:0.977]
Epoch [55/120    avg_loss:0.102, val_acc:0.977]
Epoch [56/120    avg_loss:0.114, val_acc:0.985]
Epoch [57/120    avg_loss:0.109, val_acc:0.990]
Epoch [58/120    avg_loss:0.101, val_acc:0.975]
Epoch [59/120    avg_loss:0.109, val_acc:0.985]
Epoch [60/120    avg_loss:0.142, val_acc:0.981]
Epoch [61/120    avg_loss:0.125, val_acc:0.971]
Epoch [62/120    avg_loss:0.101, val_acc:0.969]
Epoch [63/120    avg_loss:0.108, val_acc:0.983]
Epoch [64/120    avg_loss:0.132, val_acc:0.985]
Epoch [65/120    avg_loss:0.101, val_acc:0.971]
Epoch [66/120    avg_loss:0.124, val_acc:0.979]
Epoch [67/120    avg_loss:0.087, val_acc:0.992]
Epoch [68/120    avg_loss:0.084, val_acc:0.975]
Epoch [69/120    avg_loss:0.115, val_acc:0.985]
Epoch [70/120    avg_loss:0.082, val_acc:0.977]
Epoch [71/120    avg_loss:0.089, val_acc:0.994]
Epoch [72/120    avg_loss:0.054, val_acc:0.992]
Epoch [73/120    avg_loss:0.087, val_acc:0.981]
Epoch [74/120    avg_loss:0.057, val_acc:0.996]
Epoch [75/120    avg_loss:0.045, val_acc:0.998]
Epoch [76/120    avg_loss:0.081, val_acc:0.992]
Epoch [77/120    avg_loss:0.063, val_acc:0.990]
Epoch [78/120    avg_loss:0.056, val_acc:0.994]
Epoch [79/120    avg_loss:0.043, val_acc:0.996]
Epoch [80/120    avg_loss:0.046, val_acc:1.000]
Epoch [81/120    avg_loss:0.033, val_acc:0.998]
Epoch [82/120    avg_loss:0.040, val_acc:0.998]
Epoch [83/120    avg_loss:0.114, val_acc:0.983]
Epoch [84/120    avg_loss:0.087, val_acc:0.967]
Epoch [85/120    avg_loss:0.044, val_acc:1.000]
Epoch [86/120    avg_loss:0.044, val_acc:0.992]
Epoch [87/120    avg_loss:0.061, val_acc:0.998]
Epoch [88/120    avg_loss:0.044, val_acc:0.996]
Epoch [89/120    avg_loss:0.046, val_acc:0.994]
Epoch [90/120    avg_loss:0.031, val_acc:0.990]
Epoch [91/120    avg_loss:0.030, val_acc:0.994]
Epoch [92/120    avg_loss:0.040, val_acc:0.994]
Epoch [93/120    avg_loss:0.031, val_acc:0.988]
Epoch [94/120    avg_loss:0.078, val_acc:0.990]
Epoch [95/120    avg_loss:0.074, val_acc:0.981]
Epoch [96/120    avg_loss:0.135, val_acc:0.981]
Epoch [97/120    avg_loss:0.059, val_acc:0.988]
Epoch [98/120    avg_loss:0.060, val_acc:0.992]
Epoch [99/120    avg_loss:0.042, val_acc:0.994]
Epoch [100/120    avg_loss:0.039, val_acc:0.996]
Epoch [101/120    avg_loss:0.026, val_acc:0.994]
Epoch [102/120    avg_loss:0.027, val_acc:0.998]
Epoch [103/120    avg_loss:0.024, val_acc:0.998]
Epoch [104/120    avg_loss:0.029, val_acc:0.998]
Epoch [105/120    avg_loss:0.021, val_acc:0.998]
Epoch [106/120    avg_loss:0.025, val_acc:0.998]
Epoch [107/120    avg_loss:0.023, val_acc:0.998]
Epoch [108/120    avg_loss:0.020, val_acc:0.998]
Epoch [109/120    avg_loss:0.021, val_acc:0.998]
Epoch [110/120    avg_loss:0.028, val_acc:0.998]
Epoch [111/120    avg_loss:0.020, val_acc:0.998]
Epoch [112/120    avg_loss:0.019, val_acc:0.998]
Epoch [113/120    avg_loss:0.024, val_acc:0.998]
Epoch [114/120    avg_loss:0.020, val_acc:0.998]
Epoch [115/120    avg_loss:0.024, val_acc:0.998]
Epoch [116/120    avg_loss:0.019, val_acc:0.998]
Epoch [117/120    avg_loss:0.016, val_acc:0.998]
Epoch [118/120    avg_loss:0.021, val_acc:0.998]
Epoch [119/120    avg_loss:0.019, val_acc:0.998]
Epoch [120/120    avg_loss:0.018, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   1   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   7   0   0   0   0   1   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  12 441   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 0.99707174 0.99095023 1.         0.95633188 0.93006993
 0.99277108 0.97826087 1.         1.         0.99862826 0.9843342
 0.98657718 1.        ]

Kappa:
0.9905056269849111
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd1d6b3d828>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.541, val_acc:0.446]
Epoch [2/120    avg_loss:2.137, val_acc:0.556]
Epoch [3/120    avg_loss:1.874, val_acc:0.652]
Epoch [4/120    avg_loss:1.593, val_acc:0.700]
Epoch [5/120    avg_loss:1.350, val_acc:0.713]
Epoch [6/120    avg_loss:1.171, val_acc:0.760]
Epoch [7/120    avg_loss:0.982, val_acc:0.767]
Epoch [8/120    avg_loss:0.916, val_acc:0.773]
Epoch [9/120    avg_loss:0.779, val_acc:0.815]
Epoch [10/120    avg_loss:0.706, val_acc:0.892]
Epoch [11/120    avg_loss:0.637, val_acc:0.858]
Epoch [12/120    avg_loss:0.609, val_acc:0.900]
Epoch [13/120    avg_loss:0.585, val_acc:0.912]
Epoch [14/120    avg_loss:0.449, val_acc:0.921]
Epoch [15/120    avg_loss:0.443, val_acc:0.881]
Epoch [16/120    avg_loss:0.515, val_acc:0.925]
Epoch [17/120    avg_loss:0.414, val_acc:0.923]
Epoch [18/120    avg_loss:0.330, val_acc:0.900]
Epoch [19/120    avg_loss:0.381, val_acc:0.927]
Epoch [20/120    avg_loss:0.387, val_acc:0.894]
Epoch [21/120    avg_loss:0.340, val_acc:0.946]
Epoch [22/120    avg_loss:0.352, val_acc:0.898]
Epoch [23/120    avg_loss:0.278, val_acc:0.919]
Epoch [24/120    avg_loss:0.333, val_acc:0.935]
Epoch [25/120    avg_loss:0.268, val_acc:0.946]
Epoch [26/120    avg_loss:0.234, val_acc:0.948]
Epoch [27/120    avg_loss:0.268, val_acc:0.902]
Epoch [28/120    avg_loss:0.303, val_acc:0.904]
Epoch [29/120    avg_loss:0.289, val_acc:0.954]
Epoch [30/120    avg_loss:0.221, val_acc:0.960]
Epoch [31/120    avg_loss:0.205, val_acc:0.942]
Epoch [32/120    avg_loss:0.185, val_acc:0.960]
Epoch [33/120    avg_loss:0.221, val_acc:0.969]
Epoch [34/120    avg_loss:0.223, val_acc:0.958]
Epoch [35/120    avg_loss:0.255, val_acc:0.950]
Epoch [36/120    avg_loss:0.226, val_acc:0.967]
Epoch [37/120    avg_loss:0.175, val_acc:0.963]
Epoch [38/120    avg_loss:0.188, val_acc:0.975]
Epoch [39/120    avg_loss:0.119, val_acc:0.973]
Epoch [40/120    avg_loss:0.148, val_acc:0.954]
Epoch [41/120    avg_loss:0.164, val_acc:0.963]
Epoch [42/120    avg_loss:0.146, val_acc:0.975]
Epoch [43/120    avg_loss:0.118, val_acc:0.979]
Epoch [44/120    avg_loss:0.141, val_acc:0.975]
Epoch [45/120    avg_loss:0.122, val_acc:0.981]
Epoch [46/120    avg_loss:0.108, val_acc:0.979]
Epoch [47/120    avg_loss:0.090, val_acc:0.979]
Epoch [48/120    avg_loss:0.092, val_acc:0.979]
Epoch [49/120    avg_loss:0.088, val_acc:0.983]
Epoch [50/120    avg_loss:0.145, val_acc:0.950]
Epoch [51/120    avg_loss:0.124, val_acc:0.979]
Epoch [52/120    avg_loss:0.097, val_acc:0.981]
Epoch [53/120    avg_loss:0.083, val_acc:0.981]
Epoch [54/120    avg_loss:0.105, val_acc:0.988]
Epoch [55/120    avg_loss:0.106, val_acc:0.977]
Epoch [56/120    avg_loss:0.099, val_acc:0.988]
Epoch [57/120    avg_loss:0.066, val_acc:0.981]
Epoch [58/120    avg_loss:0.077, val_acc:0.979]
Epoch [59/120    avg_loss:0.103, val_acc:0.985]
Epoch [60/120    avg_loss:0.092, val_acc:0.981]
Epoch [61/120    avg_loss:0.087, val_acc:0.967]
Epoch [62/120    avg_loss:0.074, val_acc:0.973]
Epoch [63/120    avg_loss:0.071, val_acc:0.979]
Epoch [64/120    avg_loss:0.067, val_acc:0.985]
Epoch [65/120    avg_loss:0.063, val_acc:0.985]
Epoch [66/120    avg_loss:0.068, val_acc:0.983]
Epoch [67/120    avg_loss:0.055, val_acc:0.958]
Epoch [68/120    avg_loss:0.115, val_acc:0.985]
Epoch [69/120    avg_loss:0.077, val_acc:0.960]
Epoch [70/120    avg_loss:0.105, val_acc:0.985]
Epoch [71/120    avg_loss:0.044, val_acc:0.988]
Epoch [72/120    avg_loss:0.048, val_acc:0.985]
Epoch [73/120    avg_loss:0.044, val_acc:0.983]
Epoch [74/120    avg_loss:0.032, val_acc:0.985]
Epoch [75/120    avg_loss:0.031, val_acc:0.985]
Epoch [76/120    avg_loss:0.034, val_acc:0.985]
Epoch [77/120    avg_loss:0.033, val_acc:0.985]
Epoch [78/120    avg_loss:0.034, val_acc:0.985]
Epoch [79/120    avg_loss:0.032, val_acc:0.988]
Epoch [80/120    avg_loss:0.026, val_acc:0.988]
Epoch [81/120    avg_loss:0.032, val_acc:0.988]
Epoch [82/120    avg_loss:0.032, val_acc:0.988]
Epoch [83/120    avg_loss:0.027, val_acc:0.988]
Epoch [84/120    avg_loss:0.027, val_acc:0.988]
Epoch [85/120    avg_loss:0.031, val_acc:0.988]
Epoch [86/120    avg_loss:0.029, val_acc:0.988]
Epoch [87/120    avg_loss:0.024, val_acc:0.988]
Epoch [88/120    avg_loss:0.029, val_acc:0.988]
Epoch [89/120    avg_loss:0.023, val_acc:0.985]
Epoch [90/120    avg_loss:0.026, val_acc:0.988]
Epoch [91/120    avg_loss:0.026, val_acc:0.988]
Epoch [92/120    avg_loss:0.034, val_acc:0.988]
Epoch [93/120    avg_loss:0.023, val_acc:0.988]
Epoch [94/120    avg_loss:0.024, val_acc:0.988]
Epoch [95/120    avg_loss:0.023, val_acc:0.988]
Epoch [96/120    avg_loss:0.034, val_acc:0.988]
Epoch [97/120    avg_loss:0.026, val_acc:0.988]
Epoch [98/120    avg_loss:0.026, val_acc:0.988]
Epoch [99/120    avg_loss:0.023, val_acc:0.988]
Epoch [100/120    avg_loss:0.030, val_acc:0.988]
Epoch [101/120    avg_loss:0.026, val_acc:0.988]
Epoch [102/120    avg_loss:0.024, val_acc:0.988]
Epoch [103/120    avg_loss:0.021, val_acc:0.988]
Epoch [104/120    avg_loss:0.023, val_acc:0.988]
Epoch [105/120    avg_loss:0.028, val_acc:0.988]
Epoch [106/120    avg_loss:0.025, val_acc:0.988]
Epoch [107/120    avg_loss:0.030, val_acc:0.988]
Epoch [108/120    avg_loss:0.030, val_acc:0.988]
Epoch [109/120    avg_loss:0.019, val_acc:0.988]
Epoch [110/120    avg_loss:0.026, val_acc:0.988]
Epoch [111/120    avg_loss:0.028, val_acc:0.988]
Epoch [112/120    avg_loss:0.025, val_acc:0.990]
Epoch [113/120    avg_loss:0.025, val_acc:0.990]
Epoch [114/120    avg_loss:0.025, val_acc:0.990]
Epoch [115/120    avg_loss:0.031, val_acc:0.990]
Epoch [116/120    avg_loss:0.024, val_acc:0.990]
Epoch [117/120    avg_loss:0.027, val_acc:0.988]
Epoch [118/120    avg_loss:0.022, val_acc:0.990]
Epoch [119/120    avg_loss:0.024, val_acc:0.988]
Epoch [120/120    avg_loss:0.018, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   2   1   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   3   0   0   0   0   0   0   2   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 0.99560117 0.98871332 1.         0.9527897  0.93189964
 0.99277108 0.9726776  1.         1.         1.         0.99734043
 0.9956044  1.        ]

Kappa:
0.9921666815973957
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7907da58d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.465, val_acc:0.342]
Epoch [2/120    avg_loss:2.130, val_acc:0.525]
Epoch [3/120    avg_loss:1.866, val_acc:0.540]
Epoch [4/120    avg_loss:1.652, val_acc:0.596]
Epoch [5/120    avg_loss:1.448, val_acc:0.683]
Epoch [6/120    avg_loss:1.286, val_acc:0.700]
Epoch [7/120    avg_loss:1.078, val_acc:0.731]
Epoch [8/120    avg_loss:0.958, val_acc:0.746]
Epoch [9/120    avg_loss:0.849, val_acc:0.742]
Epoch [10/120    avg_loss:0.778, val_acc:0.771]
Epoch [11/120    avg_loss:0.740, val_acc:0.781]
Epoch [12/120    avg_loss:0.653, val_acc:0.771]
Epoch [13/120    avg_loss:0.648, val_acc:0.815]
Epoch [14/120    avg_loss:0.567, val_acc:0.844]
Epoch [15/120    avg_loss:0.532, val_acc:0.846]
Epoch [16/120    avg_loss:0.502, val_acc:0.871]
Epoch [17/120    avg_loss:0.428, val_acc:0.910]
Epoch [18/120    avg_loss:0.465, val_acc:0.902]
Epoch [19/120    avg_loss:0.418, val_acc:0.910]
Epoch [20/120    avg_loss:0.335, val_acc:0.898]
Epoch [21/120    avg_loss:0.338, val_acc:0.923]
Epoch [22/120    avg_loss:0.371, val_acc:0.908]
Epoch [23/120    avg_loss:0.303, val_acc:0.923]
Epoch [24/120    avg_loss:0.296, val_acc:0.921]
Epoch [25/120    avg_loss:0.326, val_acc:0.881]
Epoch [26/120    avg_loss:0.382, val_acc:0.906]
Epoch [27/120    avg_loss:0.303, val_acc:0.915]
Epoch [28/120    avg_loss:0.285, val_acc:0.923]
Epoch [29/120    avg_loss:0.268, val_acc:0.944]
Epoch [30/120    avg_loss:0.223, val_acc:0.940]
Epoch [31/120    avg_loss:0.180, val_acc:0.956]
Epoch [32/120    avg_loss:0.224, val_acc:0.946]
Epoch [33/120    avg_loss:0.242, val_acc:0.944]
Epoch [34/120    avg_loss:0.191, val_acc:0.942]
Epoch [35/120    avg_loss:0.173, val_acc:0.917]
Epoch [36/120    avg_loss:0.209, val_acc:0.940]
Epoch [37/120    avg_loss:0.188, val_acc:0.948]
Epoch [38/120    avg_loss:0.191, val_acc:0.956]
Epoch [39/120    avg_loss:0.201, val_acc:0.956]
Epoch [40/120    avg_loss:0.153, val_acc:0.977]
Epoch [41/120    avg_loss:0.141, val_acc:0.975]
Epoch [42/120    avg_loss:0.120, val_acc:0.981]
Epoch [43/120    avg_loss:0.122, val_acc:0.973]
Epoch [44/120    avg_loss:0.164, val_acc:0.971]
Epoch [45/120    avg_loss:0.150, val_acc:0.981]
Epoch [46/120    avg_loss:0.120, val_acc:0.983]
Epoch [47/120    avg_loss:0.097, val_acc:0.977]
Epoch [48/120    avg_loss:0.149, val_acc:0.975]
Epoch [49/120    avg_loss:0.092, val_acc:0.956]
Epoch [50/120    avg_loss:0.107, val_acc:0.969]
Epoch [51/120    avg_loss:0.093, val_acc:0.969]
Epoch [52/120    avg_loss:0.097, val_acc:0.977]
Epoch [53/120    avg_loss:0.095, val_acc:0.973]
Epoch [54/120    avg_loss:0.091, val_acc:0.985]
Epoch [55/120    avg_loss:0.132, val_acc:0.950]
Epoch [56/120    avg_loss:0.092, val_acc:0.965]
Epoch [57/120    avg_loss:0.062, val_acc:0.979]
Epoch [58/120    avg_loss:0.067, val_acc:0.983]
Epoch [59/120    avg_loss:0.051, val_acc:0.988]
Epoch [60/120    avg_loss:0.071, val_acc:0.992]
Epoch [61/120    avg_loss:0.056, val_acc:0.977]
Epoch [62/120    avg_loss:0.051, val_acc:0.990]
Epoch [63/120    avg_loss:0.039, val_acc:0.990]
Epoch [64/120    avg_loss:0.047, val_acc:0.985]
Epoch [65/120    avg_loss:0.046, val_acc:0.992]
Epoch [66/120    avg_loss:0.047, val_acc:0.992]
Epoch [67/120    avg_loss:0.041, val_acc:0.990]
Epoch [68/120    avg_loss:0.066, val_acc:0.985]
Epoch [69/120    avg_loss:0.057, val_acc:0.988]
Epoch [70/120    avg_loss:0.036, val_acc:0.996]
Epoch [71/120    avg_loss:0.038, val_acc:0.981]
Epoch [72/120    avg_loss:0.077, val_acc:0.996]
Epoch [73/120    avg_loss:0.098, val_acc:0.994]
Epoch [74/120    avg_loss:0.054, val_acc:0.977]
Epoch [75/120    avg_loss:0.049, val_acc:0.971]
Epoch [76/120    avg_loss:0.053, val_acc:0.990]
Epoch [77/120    avg_loss:0.032, val_acc:0.985]
Epoch [78/120    avg_loss:0.058, val_acc:0.979]
Epoch [79/120    avg_loss:0.043, val_acc:0.990]
Epoch [80/120    avg_loss:0.042, val_acc:0.994]
Epoch [81/120    avg_loss:0.039, val_acc:0.981]
Epoch [82/120    avg_loss:0.045, val_acc:0.992]
Epoch [83/120    avg_loss:0.041, val_acc:0.988]
Epoch [84/120    avg_loss:0.051, val_acc:0.994]
Epoch [85/120    avg_loss:0.050, val_acc:0.994]
Epoch [86/120    avg_loss:0.023, val_acc:0.994]
Epoch [87/120    avg_loss:0.022, val_acc:0.992]
Epoch [88/120    avg_loss:0.024, val_acc:0.994]
Epoch [89/120    avg_loss:0.019, val_acc:0.994]
Epoch [90/120    avg_loss:0.022, val_acc:0.992]
Epoch [91/120    avg_loss:0.026, val_acc:0.992]
Epoch [92/120    avg_loss:0.018, val_acc:0.992]
Epoch [93/120    avg_loss:0.024, val_acc:0.992]
Epoch [94/120    avg_loss:0.022, val_acc:0.996]
Epoch [95/120    avg_loss:0.017, val_acc:0.996]
Epoch [96/120    avg_loss:0.024, val_acc:0.996]
Epoch [97/120    avg_loss:0.016, val_acc:0.996]
Epoch [98/120    avg_loss:0.025, val_acc:0.992]
Epoch [99/120    avg_loss:0.019, val_acc:0.996]
Epoch [100/120    avg_loss:0.017, val_acc:0.996]
Epoch [101/120    avg_loss:0.018, val_acc:0.996]
Epoch [102/120    avg_loss:0.016, val_acc:0.996]
Epoch [103/120    avg_loss:0.015, val_acc:0.998]
Epoch [104/120    avg_loss:0.018, val_acc:0.996]
Epoch [105/120    avg_loss:0.016, val_acc:0.996]
Epoch [106/120    avg_loss:0.018, val_acc:0.996]
Epoch [107/120    avg_loss:0.012, val_acc:0.996]
Epoch [108/120    avg_loss:0.015, val_acc:0.996]
Epoch [109/120    avg_loss:0.017, val_acc:0.998]
Epoch [110/120    avg_loss:0.018, val_acc:0.992]
Epoch [111/120    avg_loss:0.013, val_acc:0.992]
Epoch [112/120    avg_loss:0.013, val_acc:0.992]
Epoch [113/120    avg_loss:0.022, val_acc:0.996]
Epoch [114/120    avg_loss:0.018, val_acc:0.996]
Epoch [115/120    avg_loss:0.013, val_acc:0.998]
Epoch [116/120    avg_loss:0.012, val_acc:0.998]
Epoch [117/120    avg_loss:0.018, val_acc:0.998]
Epoch [118/120    avg_loss:0.021, val_acc:0.996]
Epoch [119/120    avg_loss:0.011, val_acc:0.996]
Epoch [120/120    avg_loss:0.011, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.61620469083155

F1 scores:
[       nan 0.99707174 0.9977221  1.         0.97052154 0.95709571
 0.99038462 0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9957277516035228
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb7dcc358d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.450, val_acc:0.450]
Epoch [2/120    avg_loss:2.114, val_acc:0.597]
Epoch [3/120    avg_loss:1.866, val_acc:0.605]
Epoch [4/120    avg_loss:1.576, val_acc:0.605]
Epoch [5/120    avg_loss:1.398, val_acc:0.647]
Epoch [6/120    avg_loss:1.199, val_acc:0.706]
Epoch [7/120    avg_loss:1.066, val_acc:0.716]
Epoch [8/120    avg_loss:0.932, val_acc:0.744]
Epoch [9/120    avg_loss:0.899, val_acc:0.718]
Epoch [10/120    avg_loss:0.815, val_acc:0.750]
Epoch [11/120    avg_loss:0.721, val_acc:0.774]
Epoch [12/120    avg_loss:0.641, val_acc:0.768]
Epoch [13/120    avg_loss:0.611, val_acc:0.833]
Epoch [14/120    avg_loss:0.704, val_acc:0.847]
Epoch [15/120    avg_loss:0.553, val_acc:0.885]
Epoch [16/120    avg_loss:0.501, val_acc:0.875]
Epoch [17/120    avg_loss:0.464, val_acc:0.893]
Epoch [18/120    avg_loss:0.410, val_acc:0.938]
Epoch [19/120    avg_loss:0.489, val_acc:0.915]
Epoch [20/120    avg_loss:0.390, val_acc:0.935]
Epoch [21/120    avg_loss:0.392, val_acc:0.917]
Epoch [22/120    avg_loss:0.393, val_acc:0.915]
Epoch [23/120    avg_loss:0.390, val_acc:0.938]
Epoch [24/120    avg_loss:0.418, val_acc:0.940]
Epoch [25/120    avg_loss:0.356, val_acc:0.948]
Epoch [26/120    avg_loss:0.345, val_acc:0.915]
Epoch [27/120    avg_loss:0.315, val_acc:0.901]
Epoch [28/120    avg_loss:0.303, val_acc:0.952]
Epoch [29/120    avg_loss:0.282, val_acc:0.960]
Epoch [30/120    avg_loss:0.296, val_acc:0.871]
Epoch [31/120    avg_loss:0.317, val_acc:0.958]
Epoch [32/120    avg_loss:0.240, val_acc:0.950]
Epoch [33/120    avg_loss:0.225, val_acc:0.968]
Epoch [34/120    avg_loss:0.238, val_acc:0.948]
Epoch [35/120    avg_loss:0.257, val_acc:0.927]
Epoch [36/120    avg_loss:0.351, val_acc:0.952]
Epoch [37/120    avg_loss:0.255, val_acc:0.958]
Epoch [38/120    avg_loss:0.212, val_acc:0.964]
Epoch [39/120    avg_loss:0.215, val_acc:0.972]
Epoch [40/120    avg_loss:0.165, val_acc:0.974]
Epoch [41/120    avg_loss:0.219, val_acc:0.950]
Epoch [42/120    avg_loss:0.197, val_acc:0.954]
Epoch [43/120    avg_loss:0.195, val_acc:0.948]
Epoch [44/120    avg_loss:0.146, val_acc:0.974]
Epoch [45/120    avg_loss:0.192, val_acc:0.954]
Epoch [46/120    avg_loss:0.146, val_acc:0.978]
Epoch [47/120    avg_loss:0.143, val_acc:0.976]
Epoch [48/120    avg_loss:0.158, val_acc:0.927]
Epoch [49/120    avg_loss:0.144, val_acc:0.982]
Epoch [50/120    avg_loss:0.116, val_acc:0.988]
Epoch [51/120    avg_loss:0.111, val_acc:0.982]
Epoch [52/120    avg_loss:0.166, val_acc:0.982]
Epoch [53/120    avg_loss:0.131, val_acc:0.966]
Epoch [54/120    avg_loss:0.140, val_acc:0.988]
Epoch [55/120    avg_loss:0.141, val_acc:0.964]
Epoch [56/120    avg_loss:0.133, val_acc:0.980]
Epoch [57/120    avg_loss:0.109, val_acc:0.988]
Epoch [58/120    avg_loss:0.099, val_acc:0.992]
Epoch [59/120    avg_loss:0.125, val_acc:0.954]
Epoch [60/120    avg_loss:0.103, val_acc:0.952]
Epoch [61/120    avg_loss:0.070, val_acc:0.996]
Epoch [62/120    avg_loss:0.063, val_acc:0.996]
Epoch [63/120    avg_loss:0.057, val_acc:0.992]
Epoch [64/120    avg_loss:0.067, val_acc:0.988]
Epoch [65/120    avg_loss:0.047, val_acc:0.990]
Epoch [66/120    avg_loss:0.067, val_acc:0.990]
Epoch [67/120    avg_loss:0.056, val_acc:0.994]
Epoch [68/120    avg_loss:0.078, val_acc:0.984]
Epoch [69/120    avg_loss:0.062, val_acc:0.990]
Epoch [70/120    avg_loss:0.046, val_acc:0.994]
Epoch [71/120    avg_loss:0.057, val_acc:0.992]
Epoch [72/120    avg_loss:0.036, val_acc:0.996]
Epoch [73/120    avg_loss:0.041, val_acc:0.990]
Epoch [74/120    avg_loss:0.033, val_acc:0.986]
Epoch [75/120    avg_loss:0.035, val_acc:0.994]
Epoch [76/120    avg_loss:0.025, val_acc:0.992]
Epoch [77/120    avg_loss:0.036, val_acc:0.992]
Epoch [78/120    avg_loss:0.036, val_acc:0.992]
Epoch [79/120    avg_loss:0.077, val_acc:0.962]
Epoch [80/120    avg_loss:0.089, val_acc:0.968]
Epoch [81/120    avg_loss:0.095, val_acc:0.986]
Epoch [82/120    avg_loss:0.071, val_acc:0.984]
Epoch [83/120    avg_loss:0.051, val_acc:0.990]
Epoch [84/120    avg_loss:0.037, val_acc:0.992]
Epoch [85/120    avg_loss:0.028, val_acc:0.994]
Epoch [86/120    avg_loss:0.024, val_acc:0.994]
Epoch [87/120    avg_loss:0.027, val_acc:0.992]
Epoch [88/120    avg_loss:0.023, val_acc:0.994]
Epoch [89/120    avg_loss:0.019, val_acc:0.994]
Epoch [90/120    avg_loss:0.022, val_acc:0.992]
Epoch [91/120    avg_loss:0.020, val_acc:0.992]
Epoch [92/120    avg_loss:0.025, val_acc:0.994]
Epoch [93/120    avg_loss:0.024, val_acc:0.992]
Epoch [94/120    avg_loss:0.019, val_acc:0.992]
Epoch [95/120    avg_loss:0.018, val_acc:0.992]
Epoch [96/120    avg_loss:0.019, val_acc:0.992]
Epoch [97/120    avg_loss:0.024, val_acc:0.992]
Epoch [98/120    avg_loss:0.018, val_acc:0.996]
Epoch [99/120    avg_loss:0.017, val_acc:0.992]
Epoch [100/120    avg_loss:0.016, val_acc:0.992]
Epoch [101/120    avg_loss:0.025, val_acc:0.992]
Epoch [102/120    avg_loss:0.017, val_acc:0.992]
Epoch [103/120    avg_loss:0.018, val_acc:0.992]
Epoch [104/120    avg_loss:0.017, val_acc:0.994]
Epoch [105/120    avg_loss:0.019, val_acc:0.994]
Epoch [106/120    avg_loss:0.019, val_acc:0.994]
Epoch [107/120    avg_loss:0.020, val_acc:0.994]
Epoch [108/120    avg_loss:0.013, val_acc:0.994]
Epoch [109/120    avg_loss:0.015, val_acc:0.992]
Epoch [110/120    avg_loss:0.019, val_acc:0.994]
Epoch [111/120    avg_loss:0.020, val_acc:0.996]
Epoch [112/120    avg_loss:0.020, val_acc:0.992]
Epoch [113/120    avg_loss:0.017, val_acc:0.994]
Epoch [114/120    avg_loss:0.016, val_acc:0.992]
Epoch [115/120    avg_loss:0.023, val_acc:0.992]
Epoch [116/120    avg_loss:0.017, val_acc:0.992]
Epoch [117/120    avg_loss:0.024, val_acc:0.992]
Epoch [118/120    avg_loss:0.015, val_acc:0.992]
Epoch [119/120    avg_loss:0.012, val_acc:0.992]
Epoch [120/120    avg_loss:0.016, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 218  12   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 0.99926954 1.         0.97321429 0.93246187 0.93602694
 0.99757869 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9924042121194622
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0f3348d908>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.564, val_acc:0.101]
Epoch [2/120    avg_loss:2.335, val_acc:0.490]
Epoch [3/120    avg_loss:2.106, val_acc:0.581]
Epoch [4/120    avg_loss:1.913, val_acc:0.595]
Epoch [5/120    avg_loss:1.723, val_acc:0.601]
Epoch [6/120    avg_loss:1.528, val_acc:0.667]
Epoch [7/120    avg_loss:1.390, val_acc:0.688]
Epoch [8/120    avg_loss:1.231, val_acc:0.696]
Epoch [9/120    avg_loss:1.116, val_acc:0.732]
Epoch [10/120    avg_loss:1.028, val_acc:0.796]
Epoch [11/120    avg_loss:0.951, val_acc:0.812]
Epoch [12/120    avg_loss:0.895, val_acc:0.762]
Epoch [13/120    avg_loss:0.899, val_acc:0.861]
Epoch [14/120    avg_loss:0.825, val_acc:0.819]
Epoch [15/120    avg_loss:0.767, val_acc:0.865]
Epoch [16/120    avg_loss:0.724, val_acc:0.893]
Epoch [17/120    avg_loss:0.712, val_acc:0.875]
Epoch [18/120    avg_loss:0.657, val_acc:0.865]
Epoch [19/120    avg_loss:0.661, val_acc:0.885]
Epoch [20/120    avg_loss:0.593, val_acc:0.929]
Epoch [21/120    avg_loss:0.524, val_acc:0.907]
Epoch [22/120    avg_loss:0.540, val_acc:0.911]
Epoch [23/120    avg_loss:0.494, val_acc:0.903]
Epoch [24/120    avg_loss:0.473, val_acc:0.927]
Epoch [25/120    avg_loss:0.457, val_acc:0.925]
Epoch [26/120    avg_loss:0.427, val_acc:0.937]
Epoch [27/120    avg_loss:0.396, val_acc:0.933]
Epoch [28/120    avg_loss:0.387, val_acc:0.927]
Epoch [29/120    avg_loss:0.356, val_acc:0.946]
Epoch [30/120    avg_loss:0.319, val_acc:0.940]
Epoch [31/120    avg_loss:0.336, val_acc:0.937]
Epoch [32/120    avg_loss:0.374, val_acc:0.897]
Epoch [33/120    avg_loss:0.345, val_acc:0.942]
Epoch [34/120    avg_loss:0.363, val_acc:0.956]
Epoch [35/120    avg_loss:0.362, val_acc:0.937]
Epoch [36/120    avg_loss:0.306, val_acc:0.938]
Epoch [37/120    avg_loss:0.264, val_acc:0.958]
Epoch [38/120    avg_loss:0.269, val_acc:0.966]
Epoch [39/120    avg_loss:0.282, val_acc:0.954]
Epoch [40/120    avg_loss:0.286, val_acc:0.925]
Epoch [41/120    avg_loss:0.303, val_acc:0.942]
Epoch [42/120    avg_loss:0.282, val_acc:0.954]
Epoch [43/120    avg_loss:0.284, val_acc:0.931]
Epoch [44/120    avg_loss:0.255, val_acc:0.938]
Epoch [45/120    avg_loss:0.223, val_acc:0.935]
Epoch [46/120    avg_loss:0.223, val_acc:0.962]
Epoch [47/120    avg_loss:0.202, val_acc:0.950]
Epoch [48/120    avg_loss:0.297, val_acc:0.925]
Epoch [49/120    avg_loss:0.262, val_acc:0.948]
Epoch [50/120    avg_loss:0.239, val_acc:0.958]
Epoch [51/120    avg_loss:0.189, val_acc:0.964]
Epoch [52/120    avg_loss:0.172, val_acc:0.970]
Epoch [53/120    avg_loss:0.164, val_acc:0.970]
Epoch [54/120    avg_loss:0.143, val_acc:0.968]
Epoch [55/120    avg_loss:0.143, val_acc:0.970]
Epoch [56/120    avg_loss:0.138, val_acc:0.968]
Epoch [57/120    avg_loss:0.139, val_acc:0.970]
Epoch [58/120    avg_loss:0.130, val_acc:0.968]
Epoch [59/120    avg_loss:0.132, val_acc:0.978]
Epoch [60/120    avg_loss:0.125, val_acc:0.972]
Epoch [61/120    avg_loss:0.136, val_acc:0.974]
Epoch [62/120    avg_loss:0.141, val_acc:0.972]
Epoch [63/120    avg_loss:0.138, val_acc:0.974]
Epoch [64/120    avg_loss:0.139, val_acc:0.974]
Epoch [65/120    avg_loss:0.132, val_acc:0.972]
Epoch [66/120    avg_loss:0.135, val_acc:0.974]
Epoch [67/120    avg_loss:0.118, val_acc:0.974]
Epoch [68/120    avg_loss:0.127, val_acc:0.976]
Epoch [69/120    avg_loss:0.124, val_acc:0.972]
Epoch [70/120    avg_loss:0.112, val_acc:0.976]
Epoch [71/120    avg_loss:0.128, val_acc:0.976]
Epoch [72/120    avg_loss:0.126, val_acc:0.972]
Epoch [73/120    avg_loss:0.117, val_acc:0.974]
Epoch [74/120    avg_loss:0.120, val_acc:0.974]
Epoch [75/120    avg_loss:0.119, val_acc:0.978]
Epoch [76/120    avg_loss:0.114, val_acc:0.976]
Epoch [77/120    avg_loss:0.106, val_acc:0.976]
Epoch [78/120    avg_loss:0.119, val_acc:0.976]
Epoch [79/120    avg_loss:0.128, val_acc:0.976]
Epoch [80/120    avg_loss:0.110, val_acc:0.976]
Epoch [81/120    avg_loss:0.118, val_acc:0.978]
Epoch [82/120    avg_loss:0.110, val_acc:0.976]
Epoch [83/120    avg_loss:0.106, val_acc:0.978]
Epoch [84/120    avg_loss:0.121, val_acc:0.978]
Epoch [85/120    avg_loss:0.123, val_acc:0.978]
Epoch [86/120    avg_loss:0.129, val_acc:0.978]
Epoch [87/120    avg_loss:0.121, val_acc:0.978]
Epoch [88/120    avg_loss:0.114, val_acc:0.978]
Epoch [89/120    avg_loss:0.136, val_acc:0.978]
Epoch [90/120    avg_loss:0.105, val_acc:0.978]
Epoch [91/120    avg_loss:0.116, val_acc:0.978]
Epoch [92/120    avg_loss:0.115, val_acc:0.978]
Epoch [93/120    avg_loss:0.098, val_acc:0.980]
Epoch [94/120    avg_loss:0.121, val_acc:0.980]
Epoch [95/120    avg_loss:0.123, val_acc:0.976]
Epoch [96/120    avg_loss:0.132, val_acc:0.974]
Epoch [97/120    avg_loss:0.103, val_acc:0.974]
Epoch [98/120    avg_loss:0.106, val_acc:0.974]
Epoch [99/120    avg_loss:0.116, val_acc:0.974]
Epoch [100/120    avg_loss:0.106, val_acc:0.974]
Epoch [101/120    avg_loss:0.104, val_acc:0.972]
Epoch [102/120    avg_loss:0.110, val_acc:0.974]
Epoch [103/120    avg_loss:0.116, val_acc:0.978]
Epoch [104/120    avg_loss:0.113, val_acc:0.974]
Epoch [105/120    avg_loss:0.132, val_acc:0.976]
Epoch [106/120    avg_loss:0.113, val_acc:0.974]
Epoch [107/120    avg_loss:0.109, val_acc:0.974]
Epoch [108/120    avg_loss:0.108, val_acc:0.974]
Epoch [109/120    avg_loss:0.113, val_acc:0.974]
Epoch [110/120    avg_loss:0.116, val_acc:0.974]
Epoch [111/120    avg_loss:0.109, val_acc:0.974]
Epoch [112/120    avg_loss:0.106, val_acc:0.974]
Epoch [113/120    avg_loss:0.118, val_acc:0.974]
Epoch [114/120    avg_loss:0.095, val_acc:0.974]
Epoch [115/120    avg_loss:0.122, val_acc:0.974]
Epoch [116/120    avg_loss:0.105, val_acc:0.974]
Epoch [117/120    avg_loss:0.111, val_acc:0.974]
Epoch [118/120    avg_loss:0.114, val_acc:0.974]
Epoch [119/120    avg_loss:0.111, val_acc:0.974]
Epoch [120/120    avg_loss:0.113, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   9   0   0   0   0   0   0]
 [  0   0   0 216   3   0   0   2   8   1   0   0   0   0]
 [  0   0   0   7 191  25   0   0   1   0   3   0   0   0]
 [  0   0   0   1  25 118   1   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   4   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.71855010660981

F1 scores:
[       nan 1.         0.94170404 0.95154185 0.85650224 0.81944444
 0.99757869 0.84615385 0.98352345 0.99893276 0.99589603 1.
 0.99556541 1.        ]

Kappa:
0.974596712753986
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3b409618d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.528, val_acc:0.331]
Epoch [2/120    avg_loss:2.301, val_acc:0.466]
Epoch [3/120    avg_loss:2.116, val_acc:0.550]
Epoch [4/120    avg_loss:1.949, val_acc:0.571]
Epoch [5/120    avg_loss:1.801, val_acc:0.611]
Epoch [6/120    avg_loss:1.646, val_acc:0.615]
Epoch [7/120    avg_loss:1.463, val_acc:0.637]
Epoch [8/120    avg_loss:1.339, val_acc:0.671]
Epoch [9/120    avg_loss:1.221, val_acc:0.694]
Epoch [10/120    avg_loss:1.091, val_acc:0.812]
Epoch [11/120    avg_loss:0.961, val_acc:0.883]
Epoch [12/120    avg_loss:0.893, val_acc:0.746]
Epoch [13/120    avg_loss:0.826, val_acc:0.861]
Epoch [14/120    avg_loss:0.722, val_acc:0.881]
Epoch [15/120    avg_loss:0.691, val_acc:0.891]
Epoch [16/120    avg_loss:0.610, val_acc:0.863]
Epoch [17/120    avg_loss:0.552, val_acc:0.913]
Epoch [18/120    avg_loss:0.541, val_acc:0.893]
Epoch [19/120    avg_loss:0.493, val_acc:0.923]
Epoch [20/120    avg_loss:0.502, val_acc:0.903]
Epoch [21/120    avg_loss:0.461, val_acc:0.899]
Epoch [22/120    avg_loss:0.526, val_acc:0.897]
Epoch [23/120    avg_loss:0.451, val_acc:0.889]
Epoch [24/120    avg_loss:0.417, val_acc:0.927]
Epoch [25/120    avg_loss:0.395, val_acc:0.927]
Epoch [26/120    avg_loss:0.384, val_acc:0.927]
Epoch [27/120    avg_loss:0.386, val_acc:0.913]
Epoch [28/120    avg_loss:0.426, val_acc:0.921]
Epoch [29/120    avg_loss:0.392, val_acc:0.905]
Epoch [30/120    avg_loss:0.392, val_acc:0.913]
Epoch [31/120    avg_loss:0.402, val_acc:0.889]
Epoch [32/120    avg_loss:0.349, val_acc:0.907]
Epoch [33/120    avg_loss:0.338, val_acc:0.933]
Epoch [34/120    avg_loss:0.331, val_acc:0.835]
Epoch [35/120    avg_loss:0.355, val_acc:0.909]
Epoch [36/120    avg_loss:0.309, val_acc:0.931]
Epoch [37/120    avg_loss:0.324, val_acc:0.938]
Epoch [38/120    avg_loss:0.278, val_acc:0.944]
Epoch [39/120    avg_loss:0.257, val_acc:0.946]
Epoch [40/120    avg_loss:0.367, val_acc:0.925]
Epoch [41/120    avg_loss:0.296, val_acc:0.933]
Epoch [42/120    avg_loss:0.280, val_acc:0.931]
Epoch [43/120    avg_loss:0.262, val_acc:0.946]
Epoch [44/120    avg_loss:0.256, val_acc:0.923]
Epoch [45/120    avg_loss:0.290, val_acc:0.917]
Epoch [46/120    avg_loss:0.268, val_acc:0.940]
Epoch [47/120    avg_loss:0.220, val_acc:0.942]
Epoch [48/120    avg_loss:0.228, val_acc:0.952]
Epoch [49/120    avg_loss:0.246, val_acc:0.927]
Epoch [50/120    avg_loss:0.269, val_acc:0.944]
Epoch [51/120    avg_loss:0.233, val_acc:0.919]
Epoch [52/120    avg_loss:0.318, val_acc:0.954]
Epoch [53/120    avg_loss:0.191, val_acc:0.960]
Epoch [54/120    avg_loss:0.217, val_acc:0.956]
Epoch [55/120    avg_loss:0.202, val_acc:0.954]
Epoch [56/120    avg_loss:0.175, val_acc:0.950]
Epoch [57/120    avg_loss:0.175, val_acc:0.948]
Epoch [58/120    avg_loss:0.251, val_acc:0.952]
Epoch [59/120    avg_loss:0.216, val_acc:0.946]
Epoch [60/120    avg_loss:0.192, val_acc:0.958]
Epoch [61/120    avg_loss:0.149, val_acc:0.966]
Epoch [62/120    avg_loss:0.143, val_acc:0.952]
Epoch [63/120    avg_loss:0.141, val_acc:0.925]
Epoch [64/120    avg_loss:0.158, val_acc:0.940]
Epoch [65/120    avg_loss:0.162, val_acc:0.968]
Epoch [66/120    avg_loss:0.132, val_acc:0.972]
Epoch [67/120    avg_loss:0.129, val_acc:0.966]
Epoch [68/120    avg_loss:0.159, val_acc:0.956]
Epoch [69/120    avg_loss:0.157, val_acc:0.948]
Epoch [70/120    avg_loss:0.145, val_acc:0.964]
Epoch [71/120    avg_loss:0.156, val_acc:0.956]
Epoch [72/120    avg_loss:0.128, val_acc:0.964]
Epoch [73/120    avg_loss:0.108, val_acc:0.972]
Epoch [74/120    avg_loss:0.214, val_acc:0.948]
Epoch [75/120    avg_loss:0.216, val_acc:0.944]
Epoch [76/120    avg_loss:0.133, val_acc:0.966]
Epoch [77/120    avg_loss:0.148, val_acc:0.958]
Epoch [78/120    avg_loss:0.122, val_acc:0.954]
Epoch [79/120    avg_loss:0.131, val_acc:0.962]
Epoch [80/120    avg_loss:0.124, val_acc:0.962]
Epoch [81/120    avg_loss:0.134, val_acc:0.950]
Epoch [82/120    avg_loss:0.142, val_acc:0.958]
Epoch [83/120    avg_loss:0.110, val_acc:0.964]
Epoch [84/120    avg_loss:0.091, val_acc:0.972]
Epoch [85/120    avg_loss:0.089, val_acc:0.960]
Epoch [86/120    avg_loss:0.084, val_acc:0.976]
Epoch [87/120    avg_loss:0.089, val_acc:0.976]
Epoch [88/120    avg_loss:0.069, val_acc:0.978]
Epoch [89/120    avg_loss:0.087, val_acc:0.974]
Epoch [90/120    avg_loss:0.082, val_acc:0.950]
Epoch [91/120    avg_loss:0.087, val_acc:0.976]
Epoch [92/120    avg_loss:0.084, val_acc:0.978]
Epoch [93/120    avg_loss:0.111, val_acc:0.984]
Epoch [94/120    avg_loss:0.083, val_acc:0.950]
Epoch [95/120    avg_loss:0.162, val_acc:0.966]
Epoch [96/120    avg_loss:0.114, val_acc:0.974]
Epoch [97/120    avg_loss:0.095, val_acc:0.964]
Epoch [98/120    avg_loss:0.081, val_acc:0.980]
Epoch [99/120    avg_loss:0.067, val_acc:0.972]
Epoch [100/120    avg_loss:0.106, val_acc:0.940]
Epoch [101/120    avg_loss:0.103, val_acc:0.980]
Epoch [102/120    avg_loss:0.076, val_acc:0.974]
Epoch [103/120    avg_loss:0.077, val_acc:0.982]
Epoch [104/120    avg_loss:0.051, val_acc:0.974]
Epoch [105/120    avg_loss:0.069, val_acc:0.980]
Epoch [106/120    avg_loss:0.054, val_acc:0.986]
Epoch [107/120    avg_loss:0.106, val_acc:0.980]
Epoch [108/120    avg_loss:0.081, val_acc:0.982]
Epoch [109/120    avg_loss:0.051, val_acc:0.976]
Epoch [110/120    avg_loss:0.107, val_acc:0.974]
Epoch [111/120    avg_loss:0.051, val_acc:0.966]
Epoch [112/120    avg_loss:0.053, val_acc:0.984]
Epoch [113/120    avg_loss:0.032, val_acc:0.986]
Epoch [114/120    avg_loss:0.046, val_acc:0.984]
Epoch [115/120    avg_loss:0.056, val_acc:0.964]
Epoch [116/120    avg_loss:0.052, val_acc:0.984]
Epoch [117/120    avg_loss:0.032, val_acc:0.976]
Epoch [118/120    avg_loss:0.065, val_acc:0.964]
Epoch [119/120    avg_loss:0.054, val_acc:0.976]
Epoch [120/120    avg_loss:0.029, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 672   0   0   0   0  13   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 219   7   0   0   0   4   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  37 108   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.86780383795309

F1 scores:
[       nan 0.99042004 0.94922737 0.97550111 0.87551867 0.80297398
 0.96941176 0.88636364 0.99358151 1.         1.         1.
 0.99778761 1.        ]

Kappa:
0.9762654956834832
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f12d63b17f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.510, val_acc:0.355]
Epoch [2/120    avg_loss:2.215, val_acc:0.488]
Epoch [3/120    avg_loss:2.027, val_acc:0.571]
Epoch [4/120    avg_loss:1.840, val_acc:0.663]
Epoch [5/120    avg_loss:1.686, val_acc:0.712]
Epoch [6/120    avg_loss:1.510, val_acc:0.710]
Epoch [7/120    avg_loss:1.387, val_acc:0.750]
Epoch [8/120    avg_loss:1.253, val_acc:0.792]
Epoch [9/120    avg_loss:1.099, val_acc:0.770]
Epoch [10/120    avg_loss:1.024, val_acc:0.808]
Epoch [11/120    avg_loss:0.929, val_acc:0.855]
Epoch [12/120    avg_loss:0.845, val_acc:0.845]
Epoch [13/120    avg_loss:0.792, val_acc:0.843]
Epoch [14/120    avg_loss:0.727, val_acc:0.859]
Epoch [15/120    avg_loss:0.668, val_acc:0.881]
Epoch [16/120    avg_loss:0.640, val_acc:0.881]
Epoch [17/120    avg_loss:0.636, val_acc:0.907]
Epoch [18/120    avg_loss:0.631, val_acc:0.889]
Epoch [19/120    avg_loss:0.572, val_acc:0.907]
Epoch [20/120    avg_loss:0.567, val_acc:0.861]
Epoch [21/120    avg_loss:0.595, val_acc:0.889]
Epoch [22/120    avg_loss:0.508, val_acc:0.867]
Epoch [23/120    avg_loss:0.478, val_acc:0.919]
Epoch [24/120    avg_loss:0.469, val_acc:0.905]
Epoch [25/120    avg_loss:0.379, val_acc:0.915]
Epoch [26/120    avg_loss:0.386, val_acc:0.935]
Epoch [27/120    avg_loss:0.408, val_acc:0.905]
Epoch [28/120    avg_loss:0.415, val_acc:0.909]
Epoch [29/120    avg_loss:0.378, val_acc:0.927]
Epoch [30/120    avg_loss:0.403, val_acc:0.913]
Epoch [31/120    avg_loss:0.346, val_acc:0.923]
Epoch [32/120    avg_loss:0.377, val_acc:0.909]
Epoch [33/120    avg_loss:0.415, val_acc:0.925]
Epoch [34/120    avg_loss:0.400, val_acc:0.925]
Epoch [35/120    avg_loss:0.361, val_acc:0.935]
Epoch [36/120    avg_loss:0.344, val_acc:0.944]
Epoch [37/120    avg_loss:0.307, val_acc:0.946]
Epoch [38/120    avg_loss:0.317, val_acc:0.938]
Epoch [39/120    avg_loss:0.317, val_acc:0.937]
Epoch [40/120    avg_loss:0.290, val_acc:0.927]
Epoch [41/120    avg_loss:0.257, val_acc:0.938]
Epoch [42/120    avg_loss:0.281, val_acc:0.925]
Epoch [43/120    avg_loss:0.265, val_acc:0.938]
Epoch [44/120    avg_loss:0.250, val_acc:0.942]
Epoch [45/120    avg_loss:0.245, val_acc:0.958]
Epoch [46/120    avg_loss:0.228, val_acc:0.944]
Epoch [47/120    avg_loss:0.235, val_acc:0.954]
Epoch [48/120    avg_loss:0.220, val_acc:0.942]
Epoch [49/120    avg_loss:0.281, val_acc:0.831]
Epoch [50/120    avg_loss:0.348, val_acc:0.946]
Epoch [51/120    avg_loss:0.347, val_acc:0.931]
Epoch [52/120    avg_loss:0.249, val_acc:0.950]
Epoch [53/120    avg_loss:0.231, val_acc:0.944]
Epoch [54/120    avg_loss:0.224, val_acc:0.938]
Epoch [55/120    avg_loss:0.196, val_acc:0.942]
Epoch [56/120    avg_loss:0.242, val_acc:0.946]
Epoch [57/120    avg_loss:0.232, val_acc:0.915]
Epoch [58/120    avg_loss:0.276, val_acc:0.935]
Epoch [59/120    avg_loss:0.213, val_acc:0.940]
Epoch [60/120    avg_loss:0.172, val_acc:0.946]
Epoch [61/120    avg_loss:0.165, val_acc:0.952]
Epoch [62/120    avg_loss:0.156, val_acc:0.948]
Epoch [63/120    avg_loss:0.141, val_acc:0.952]
Epoch [64/120    avg_loss:0.155, val_acc:0.958]
Epoch [65/120    avg_loss:0.132, val_acc:0.958]
Epoch [66/120    avg_loss:0.138, val_acc:0.958]
Epoch [67/120    avg_loss:0.133, val_acc:0.956]
Epoch [68/120    avg_loss:0.140, val_acc:0.958]
Epoch [69/120    avg_loss:0.125, val_acc:0.958]
Epoch [70/120    avg_loss:0.143, val_acc:0.956]
Epoch [71/120    avg_loss:0.131, val_acc:0.954]
Epoch [72/120    avg_loss:0.120, val_acc:0.956]
Epoch [73/120    avg_loss:0.137, val_acc:0.958]
Epoch [74/120    avg_loss:0.128, val_acc:0.960]
Epoch [75/120    avg_loss:0.126, val_acc:0.960]
Epoch [76/120    avg_loss:0.133, val_acc:0.958]
Epoch [77/120    avg_loss:0.131, val_acc:0.958]
Epoch [78/120    avg_loss:0.120, val_acc:0.958]
Epoch [79/120    avg_loss:0.110, val_acc:0.956]
Epoch [80/120    avg_loss:0.121, val_acc:0.960]
Epoch [81/120    avg_loss:0.111, val_acc:0.962]
Epoch [82/120    avg_loss:0.127, val_acc:0.966]
Epoch [83/120    avg_loss:0.119, val_acc:0.964]
Epoch [84/120    avg_loss:0.102, val_acc:0.960]
Epoch [85/120    avg_loss:0.114, val_acc:0.958]
Epoch [86/120    avg_loss:0.131, val_acc:0.962]
Epoch [87/120    avg_loss:0.115, val_acc:0.964]
Epoch [88/120    avg_loss:0.115, val_acc:0.964]
Epoch [89/120    avg_loss:0.105, val_acc:0.962]
Epoch [90/120    avg_loss:0.100, val_acc:0.964]
Epoch [91/120    avg_loss:0.124, val_acc:0.960]
Epoch [92/120    avg_loss:0.115, val_acc:0.962]
Epoch [93/120    avg_loss:0.106, val_acc:0.960]
Epoch [94/120    avg_loss:0.110, val_acc:0.970]
Epoch [95/120    avg_loss:0.100, val_acc:0.958]
Epoch [96/120    avg_loss:0.104, val_acc:0.962]
Epoch [97/120    avg_loss:0.108, val_acc:0.966]
Epoch [98/120    avg_loss:0.099, val_acc:0.962]
Epoch [99/120    avg_loss:0.119, val_acc:0.966]
Epoch [100/120    avg_loss:0.096, val_acc:0.968]
Epoch [101/120    avg_loss:0.103, val_acc:0.964]
Epoch [102/120    avg_loss:0.111, val_acc:0.966]
Epoch [103/120    avg_loss:0.107, val_acc:0.968]
Epoch [104/120    avg_loss:0.102, val_acc:0.962]
Epoch [105/120    avg_loss:0.105, val_acc:0.966]
Epoch [106/120    avg_loss:0.098, val_acc:0.964]
Epoch [107/120    avg_loss:0.107, val_acc:0.964]
Epoch [108/120    avg_loss:0.107, val_acc:0.964]
Epoch [109/120    avg_loss:0.094, val_acc:0.964]
Epoch [110/120    avg_loss:0.097, val_acc:0.964]
Epoch [111/120    avg_loss:0.086, val_acc:0.966]
Epoch [112/120    avg_loss:0.106, val_acc:0.966]
Epoch [113/120    avg_loss:0.097, val_acc:0.966]
Epoch [114/120    avg_loss:0.091, val_acc:0.966]
Epoch [115/120    avg_loss:0.093, val_acc:0.966]
Epoch [116/120    avg_loss:0.100, val_acc:0.966]
Epoch [117/120    avg_loss:0.095, val_acc:0.966]
Epoch [118/120    avg_loss:0.095, val_acc:0.968]
Epoch [119/120    avg_loss:0.082, val_acc:0.968]
Epoch [120/120    avg_loss:0.107, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 207   0   0   0   0  12   0   0   0   0   0   0]
 [  0   0   1 208  20   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  28 117   0   0   0   0   0   0   0   0]
 [  0   2   0   1   9   0 194   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   1 833]]

Accuracy:
97.71855010660981

F1 scores:
[       nan 0.99854227 0.93877551 0.9476082  0.84787018 0.83571429
 0.97       0.86021505 1.         0.99893276 1.         1.
 0.99889746 0.99940012]

Kappa:
0.9745975781671289
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f93089c2860>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.529, val_acc:0.282]
Epoch [2/120    avg_loss:2.277, val_acc:0.357]
Epoch [3/120    avg_loss:2.114, val_acc:0.480]
Epoch [4/120    avg_loss:1.977, val_acc:0.520]
Epoch [5/120    avg_loss:1.805, val_acc:0.546]
Epoch [6/120    avg_loss:1.651, val_acc:0.567]
Epoch [7/120    avg_loss:1.514, val_acc:0.736]
Epoch [8/120    avg_loss:1.359, val_acc:0.754]
Epoch [9/120    avg_loss:1.248, val_acc:0.756]
Epoch [10/120    avg_loss:1.120, val_acc:0.861]
Epoch [11/120    avg_loss:1.003, val_acc:0.817]
Epoch [12/120    avg_loss:0.888, val_acc:0.806]
Epoch [13/120    avg_loss:0.809, val_acc:0.812]
Epoch [14/120    avg_loss:0.800, val_acc:0.849]
Epoch [15/120    avg_loss:0.753, val_acc:0.861]
Epoch [16/120    avg_loss:0.701, val_acc:0.893]
Epoch [17/120    avg_loss:0.670, val_acc:0.873]
Epoch [18/120    avg_loss:0.625, val_acc:0.901]
Epoch [19/120    avg_loss:0.535, val_acc:0.883]
Epoch [20/120    avg_loss:0.486, val_acc:0.897]
Epoch [21/120    avg_loss:0.525, val_acc:0.899]
Epoch [22/120    avg_loss:0.466, val_acc:0.899]
Epoch [23/120    avg_loss:0.501, val_acc:0.869]
Epoch [24/120    avg_loss:0.491, val_acc:0.927]
Epoch [25/120    avg_loss:0.430, val_acc:0.913]
Epoch [26/120    avg_loss:0.388, val_acc:0.907]
Epoch [27/120    avg_loss:0.459, val_acc:0.891]
Epoch [28/120    avg_loss:0.423, val_acc:0.921]
Epoch [29/120    avg_loss:0.378, val_acc:0.938]
Epoch [30/120    avg_loss:0.326, val_acc:0.935]
Epoch [31/120    avg_loss:0.406, val_acc:0.946]
Epoch [32/120    avg_loss:0.381, val_acc:0.909]
Epoch [33/120    avg_loss:0.325, val_acc:0.938]
Epoch [34/120    avg_loss:0.306, val_acc:0.950]
Epoch [35/120    avg_loss:0.260, val_acc:0.954]
Epoch [36/120    avg_loss:0.316, val_acc:0.931]
Epoch [37/120    avg_loss:0.321, val_acc:0.946]
Epoch [38/120    avg_loss:0.271, val_acc:0.952]
Epoch [39/120    avg_loss:0.273, val_acc:0.948]
Epoch [40/120    avg_loss:0.253, val_acc:0.944]
Epoch [41/120    avg_loss:0.236, val_acc:0.946]
Epoch [42/120    avg_loss:0.286, val_acc:0.956]
Epoch [43/120    avg_loss:0.277, val_acc:0.944]
Epoch [44/120    avg_loss:0.218, val_acc:0.952]
Epoch [45/120    avg_loss:0.254, val_acc:0.925]
Epoch [46/120    avg_loss:0.244, val_acc:0.946]
Epoch [47/120    avg_loss:0.241, val_acc:0.968]
Epoch [48/120    avg_loss:0.246, val_acc:0.946]
Epoch [49/120    avg_loss:0.248, val_acc:0.954]
Epoch [50/120    avg_loss:0.194, val_acc:0.956]
Epoch [51/120    avg_loss:0.196, val_acc:0.968]
Epoch [52/120    avg_loss:0.167, val_acc:0.952]
Epoch [53/120    avg_loss:0.150, val_acc:0.972]
Epoch [54/120    avg_loss:0.207, val_acc:0.964]
Epoch [55/120    avg_loss:0.173, val_acc:0.964]
Epoch [56/120    avg_loss:0.208, val_acc:0.962]
Epoch [57/120    avg_loss:0.187, val_acc:0.956]
Epoch [58/120    avg_loss:0.159, val_acc:0.958]
Epoch [59/120    avg_loss:0.294, val_acc:0.903]
Epoch [60/120    avg_loss:0.247, val_acc:0.946]
Epoch [61/120    avg_loss:0.182, val_acc:0.956]
Epoch [62/120    avg_loss:0.176, val_acc:0.962]
Epoch [63/120    avg_loss:0.148, val_acc:0.950]
Epoch [64/120    avg_loss:0.159, val_acc:0.960]
Epoch [65/120    avg_loss:0.145, val_acc:0.968]
Epoch [66/120    avg_loss:0.238, val_acc:0.899]
Epoch [67/120    avg_loss:0.311, val_acc:0.952]
Epoch [68/120    avg_loss:0.146, val_acc:0.968]
Epoch [69/120    avg_loss:0.143, val_acc:0.964]
Epoch [70/120    avg_loss:0.110, val_acc:0.974]
Epoch [71/120    avg_loss:0.101, val_acc:0.978]
Epoch [72/120    avg_loss:0.112, val_acc:0.982]
Epoch [73/120    avg_loss:0.081, val_acc:0.982]
Epoch [74/120    avg_loss:0.092, val_acc:0.982]
Epoch [75/120    avg_loss:0.108, val_acc:0.980]
Epoch [76/120    avg_loss:0.107, val_acc:0.986]
Epoch [77/120    avg_loss:0.087, val_acc:0.982]
Epoch [78/120    avg_loss:0.099, val_acc:0.986]
Epoch [79/120    avg_loss:0.105, val_acc:0.986]
Epoch [80/120    avg_loss:0.090, val_acc:0.982]
Epoch [81/120    avg_loss:0.095, val_acc:0.984]
Epoch [82/120    avg_loss:0.098, val_acc:0.984]
Epoch [83/120    avg_loss:0.074, val_acc:0.980]
Epoch [84/120    avg_loss:0.079, val_acc:0.982]
Epoch [85/120    avg_loss:0.072, val_acc:0.984]
Epoch [86/120    avg_loss:0.085, val_acc:0.986]
Epoch [87/120    avg_loss:0.074, val_acc:0.986]
Epoch [88/120    avg_loss:0.075, val_acc:0.988]
Epoch [89/120    avg_loss:0.069, val_acc:0.986]
Epoch [90/120    avg_loss:0.081, val_acc:0.986]
Epoch [91/120    avg_loss:0.091, val_acc:0.986]
Epoch [92/120    avg_loss:0.087, val_acc:0.980]
Epoch [93/120    avg_loss:0.085, val_acc:0.984]
Epoch [94/120    avg_loss:0.077, val_acc:0.988]
Epoch [95/120    avg_loss:0.065, val_acc:0.988]
Epoch [96/120    avg_loss:0.095, val_acc:0.988]
Epoch [97/120    avg_loss:0.089, val_acc:0.988]
Epoch [98/120    avg_loss:0.070, val_acc:0.986]
Epoch [99/120    avg_loss:0.065, val_acc:0.988]
Epoch [100/120    avg_loss:0.073, val_acc:0.988]
Epoch [101/120    avg_loss:0.076, val_acc:0.988]
Epoch [102/120    avg_loss:0.074, val_acc:0.988]
Epoch [103/120    avg_loss:0.071, val_acc:0.988]
Epoch [104/120    avg_loss:0.078, val_acc:0.992]
Epoch [105/120    avg_loss:0.073, val_acc:0.990]
Epoch [106/120    avg_loss:0.062, val_acc:0.986]
Epoch [107/120    avg_loss:0.075, val_acc:0.990]
Epoch [108/120    avg_loss:0.077, val_acc:0.988]
Epoch [109/120    avg_loss:0.067, val_acc:0.990]
Epoch [110/120    avg_loss:0.058, val_acc:0.990]
Epoch [111/120    avg_loss:0.081, val_acc:0.990]
Epoch [112/120    avg_loss:0.069, val_acc:0.990]
Epoch [113/120    avg_loss:0.091, val_acc:0.988]
Epoch [114/120    avg_loss:0.064, val_acc:0.986]
Epoch [115/120    avg_loss:0.080, val_acc:0.988]
Epoch [116/120    avg_loss:0.065, val_acc:0.988]
Epoch [117/120    avg_loss:0.063, val_acc:0.988]
Epoch [118/120    avg_loss:0.058, val_acc:0.988]
Epoch [119/120    avg_loss:0.061, val_acc:0.990]
Epoch [120/120    avg_loss:0.063, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   1   0   0   0   0   1   0]
 [  0   0   2 217   6   0   0   0   4   1   0   0   0   0]
 [  0   0   1   0 208  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   8   0   0   0   0 198   0   0   0   0   0   0   0]
 [  0   0  22   0   0   0   0  72   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.37953091684435

F1 scores:
[       nan 0.99419448 0.94143167 0.97091723 0.92650334 0.91333333
 0.98019802 0.86227545 0.99487179 0.99893276 1.         0.99472296
 0.9944629  1.        ]

Kappa:
0.9819531735382813
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3bf03208d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.489, val_acc:0.361]
Epoch [2/120    avg_loss:2.238, val_acc:0.433]
Epoch [3/120    avg_loss:2.036, val_acc:0.554]
Epoch [4/120    avg_loss:1.878, val_acc:0.615]
Epoch [5/120    avg_loss:1.750, val_acc:0.712]
Epoch [6/120    avg_loss:1.618, val_acc:0.673]
Epoch [7/120    avg_loss:1.469, val_acc:0.752]
Epoch [8/120    avg_loss:1.336, val_acc:0.746]
Epoch [9/120    avg_loss:1.215, val_acc:0.790]
Epoch [10/120    avg_loss:1.080, val_acc:0.728]
Epoch [11/120    avg_loss:1.005, val_acc:0.827]
Epoch [12/120    avg_loss:0.921, val_acc:0.810]
Epoch [13/120    avg_loss:0.811, val_acc:0.889]
Epoch [14/120    avg_loss:0.725, val_acc:0.897]
Epoch [15/120    avg_loss:0.708, val_acc:0.901]
Epoch [16/120    avg_loss:0.629, val_acc:0.889]
Epoch [17/120    avg_loss:0.605, val_acc:0.917]
Epoch [18/120    avg_loss:0.576, val_acc:0.915]
Epoch [19/120    avg_loss:0.529, val_acc:0.905]
Epoch [20/120    avg_loss:0.489, val_acc:0.907]
Epoch [21/120    avg_loss:0.482, val_acc:0.921]
Epoch [22/120    avg_loss:0.432, val_acc:0.915]
Epoch [23/120    avg_loss:0.419, val_acc:0.901]
Epoch [24/120    avg_loss:0.438, val_acc:0.887]
Epoch [25/120    avg_loss:0.442, val_acc:0.901]
Epoch [26/120    avg_loss:0.460, val_acc:0.919]
Epoch [27/120    avg_loss:0.419, val_acc:0.915]
Epoch [28/120    avg_loss:0.474, val_acc:0.881]
Epoch [29/120    avg_loss:0.416, val_acc:0.933]
Epoch [30/120    avg_loss:0.389, val_acc:0.927]
Epoch [31/120    avg_loss:0.317, val_acc:0.929]
Epoch [32/120    avg_loss:0.324, val_acc:0.942]
Epoch [33/120    avg_loss:0.320, val_acc:0.913]
Epoch [34/120    avg_loss:0.320, val_acc:0.917]
Epoch [35/120    avg_loss:0.317, val_acc:0.954]
Epoch [36/120    avg_loss:0.232, val_acc:0.940]
Epoch [37/120    avg_loss:0.259, val_acc:0.946]
Epoch [38/120    avg_loss:0.307, val_acc:0.933]
Epoch [39/120    avg_loss:0.307, val_acc:0.931]
Epoch [40/120    avg_loss:0.256, val_acc:0.942]
Epoch [41/120    avg_loss:0.255, val_acc:0.954]
Epoch [42/120    avg_loss:0.219, val_acc:0.966]
Epoch [43/120    avg_loss:0.248, val_acc:0.956]
Epoch [44/120    avg_loss:0.281, val_acc:0.931]
Epoch [45/120    avg_loss:0.278, val_acc:0.921]
Epoch [46/120    avg_loss:0.246, val_acc:0.952]
Epoch [47/120    avg_loss:0.201, val_acc:0.950]
Epoch [48/120    avg_loss:0.208, val_acc:0.954]
Epoch [49/120    avg_loss:0.229, val_acc:0.960]
Epoch [50/120    avg_loss:0.194, val_acc:0.954]
Epoch [51/120    avg_loss:0.167, val_acc:0.962]
Epoch [52/120    avg_loss:0.155, val_acc:0.958]
Epoch [53/120    avg_loss:0.192, val_acc:0.869]
Epoch [54/120    avg_loss:0.172, val_acc:0.950]
Epoch [55/120    avg_loss:0.141, val_acc:0.962]
Epoch [56/120    avg_loss:0.136, val_acc:0.966]
Epoch [57/120    avg_loss:0.106, val_acc:0.968]
Epoch [58/120    avg_loss:0.094, val_acc:0.976]
Epoch [59/120    avg_loss:0.156, val_acc:0.972]
Epoch [60/120    avg_loss:0.127, val_acc:0.964]
Epoch [61/120    avg_loss:0.124, val_acc:0.972]
Epoch [62/120    avg_loss:0.098, val_acc:0.972]
Epoch [63/120    avg_loss:0.122, val_acc:0.972]
Epoch [64/120    avg_loss:0.091, val_acc:0.976]
Epoch [65/120    avg_loss:0.091, val_acc:0.974]
Epoch [66/120    avg_loss:0.105, val_acc:0.976]
Epoch [67/120    avg_loss:0.098, val_acc:0.976]
Epoch [68/120    avg_loss:0.108, val_acc:0.972]
Epoch [69/120    avg_loss:0.094, val_acc:0.974]
Epoch [70/120    avg_loss:0.096, val_acc:0.974]
Epoch [71/120    avg_loss:0.101, val_acc:0.974]
Epoch [72/120    avg_loss:0.094, val_acc:0.974]
Epoch [73/120    avg_loss:0.106, val_acc:0.974]
Epoch [74/120    avg_loss:0.094, val_acc:0.974]
Epoch [75/120    avg_loss:0.083, val_acc:0.976]
Epoch [76/120    avg_loss:0.086, val_acc:0.978]
Epoch [77/120    avg_loss:0.090, val_acc:0.978]
Epoch [78/120    avg_loss:0.091, val_acc:0.978]
Epoch [79/120    avg_loss:0.101, val_acc:0.978]
Epoch [80/120    avg_loss:0.089, val_acc:0.980]
Epoch [81/120    avg_loss:0.082, val_acc:0.976]
Epoch [82/120    avg_loss:0.088, val_acc:0.976]
Epoch [83/120    avg_loss:0.084, val_acc:0.976]
Epoch [84/120    avg_loss:0.072, val_acc:0.978]
Epoch [85/120    avg_loss:0.095, val_acc:0.974]
Epoch [86/120    avg_loss:0.079, val_acc:0.976]
Epoch [87/120    avg_loss:0.078, val_acc:0.978]
Epoch [88/120    avg_loss:0.083, val_acc:0.978]
Epoch [89/120    avg_loss:0.093, val_acc:0.980]
Epoch [90/120    avg_loss:0.078, val_acc:0.978]
Epoch [91/120    avg_loss:0.074, val_acc:0.982]
Epoch [92/120    avg_loss:0.078, val_acc:0.978]
Epoch [93/120    avg_loss:0.093, val_acc:0.984]
Epoch [94/120    avg_loss:0.074, val_acc:0.984]
Epoch [95/120    avg_loss:0.069, val_acc:0.978]
Epoch [96/120    avg_loss:0.078, val_acc:0.978]
Epoch [97/120    avg_loss:0.072, val_acc:0.978]
Epoch [98/120    avg_loss:0.074, val_acc:0.984]
Epoch [99/120    avg_loss:0.088, val_acc:0.982]
Epoch [100/120    avg_loss:0.099, val_acc:0.982]
Epoch [101/120    avg_loss:0.069, val_acc:0.982]
Epoch [102/120    avg_loss:0.074, val_acc:0.976]
Epoch [103/120    avg_loss:0.073, val_acc:0.980]
Epoch [104/120    avg_loss:0.077, val_acc:0.978]
Epoch [105/120    avg_loss:0.074, val_acc:0.984]
Epoch [106/120    avg_loss:0.082, val_acc:0.980]
Epoch [107/120    avg_loss:0.076, val_acc:0.984]
Epoch [108/120    avg_loss:0.085, val_acc:0.976]
Epoch [109/120    avg_loss:0.068, val_acc:0.984]
Epoch [110/120    avg_loss:0.062, val_acc:0.978]
Epoch [111/120    avg_loss:0.057, val_acc:0.982]
Epoch [112/120    avg_loss:0.075, val_acc:0.978]
Epoch [113/120    avg_loss:0.068, val_acc:0.982]
Epoch [114/120    avg_loss:0.064, val_acc:0.980]
Epoch [115/120    avg_loss:0.074, val_acc:0.980]
Epoch [116/120    avg_loss:0.067, val_acc:0.980]
Epoch [117/120    avg_loss:0.062, val_acc:0.982]
Epoch [118/120    avg_loss:0.065, val_acc:0.980]
Epoch [119/120    avg_loss:0.053, val_acc:0.982]
Epoch [120/120    avg_loss:0.089, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   4 203  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   1   0   0   0   0   0   0   0 451   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0   0 833]]

Accuracy:
98.67803837953092

F1 scores:
[       nan 1.         0.93721973 0.98039216 0.92272727 0.92156863
 1.         0.85714286 1.         1.         1.         1.
 0.99778761 0.99940012]

Kappa:
0.9852835713783228
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8814f548d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.489, val_acc:0.339]
Epoch [2/120    avg_loss:2.271, val_acc:0.472]
Epoch [3/120    avg_loss:2.114, val_acc:0.565]
Epoch [4/120    avg_loss:1.969, val_acc:0.627]
Epoch [5/120    avg_loss:1.786, val_acc:0.657]
Epoch [6/120    avg_loss:1.607, val_acc:0.683]
Epoch [7/120    avg_loss:1.441, val_acc:0.692]
Epoch [8/120    avg_loss:1.271, val_acc:0.780]
Epoch [9/120    avg_loss:1.137, val_acc:0.808]
Epoch [10/120    avg_loss:1.000, val_acc:0.821]
Epoch [11/120    avg_loss:0.923, val_acc:0.863]
Epoch [12/120    avg_loss:0.817, val_acc:0.867]
Epoch [13/120    avg_loss:0.780, val_acc:0.863]
Epoch [14/120    avg_loss:0.755, val_acc:0.879]
Epoch [15/120    avg_loss:0.634, val_acc:0.893]
Epoch [16/120    avg_loss:0.596, val_acc:0.909]
Epoch [17/120    avg_loss:0.600, val_acc:0.909]
Epoch [18/120    avg_loss:0.532, val_acc:0.901]
Epoch [19/120    avg_loss:0.546, val_acc:0.925]
Epoch [20/120    avg_loss:0.462, val_acc:0.899]
Epoch [21/120    avg_loss:0.406, val_acc:0.919]
Epoch [22/120    avg_loss:0.417, val_acc:0.897]
Epoch [23/120    avg_loss:0.548, val_acc:0.871]
Epoch [24/120    avg_loss:0.458, val_acc:0.905]
Epoch [25/120    avg_loss:0.424, val_acc:0.909]
Epoch [26/120    avg_loss:0.447, val_acc:0.927]
Epoch [27/120    avg_loss:0.355, val_acc:0.935]
Epoch [28/120    avg_loss:0.343, val_acc:0.933]
Epoch [29/120    avg_loss:0.395, val_acc:0.903]
Epoch [30/120    avg_loss:0.392, val_acc:0.921]
Epoch [31/120    avg_loss:0.317, val_acc:0.942]
Epoch [32/120    avg_loss:0.309, val_acc:0.950]
Epoch [33/120    avg_loss:0.412, val_acc:0.950]
Epoch [34/120    avg_loss:0.384, val_acc:0.903]
Epoch [35/120    avg_loss:0.335, val_acc:0.929]
Epoch [36/120    avg_loss:0.319, val_acc:0.958]
Epoch [37/120    avg_loss:0.259, val_acc:0.885]
Epoch [38/120    avg_loss:0.291, val_acc:0.915]
Epoch [39/120    avg_loss:0.293, val_acc:0.919]
Epoch [40/120    avg_loss:0.328, val_acc:0.942]
Epoch [41/120    avg_loss:0.280, val_acc:0.933]
Epoch [42/120    avg_loss:0.255, val_acc:0.956]
Epoch [43/120    avg_loss:0.225, val_acc:0.958]
Epoch [44/120    avg_loss:0.237, val_acc:0.956]
Epoch [45/120    avg_loss:0.225, val_acc:0.952]
Epoch [46/120    avg_loss:0.204, val_acc:0.954]
Epoch [47/120    avg_loss:0.200, val_acc:0.956]
Epoch [48/120    avg_loss:0.189, val_acc:0.933]
Epoch [49/120    avg_loss:0.219, val_acc:0.952]
Epoch [50/120    avg_loss:0.189, val_acc:0.960]
Epoch [51/120    avg_loss:0.150, val_acc:0.954]
Epoch [52/120    avg_loss:0.196, val_acc:0.968]
Epoch [53/120    avg_loss:0.187, val_acc:0.970]
Epoch [54/120    avg_loss:0.198, val_acc:0.933]
Epoch [55/120    avg_loss:0.178, val_acc:0.952]
Epoch [56/120    avg_loss:0.193, val_acc:0.952]
Epoch [57/120    avg_loss:0.194, val_acc:0.940]
Epoch [58/120    avg_loss:0.174, val_acc:0.958]
Epoch [59/120    avg_loss:0.145, val_acc:0.972]
Epoch [60/120    avg_loss:0.134, val_acc:0.978]
Epoch [61/120    avg_loss:0.135, val_acc:0.970]
Epoch [62/120    avg_loss:0.174, val_acc:0.931]
Epoch [63/120    avg_loss:0.150, val_acc:0.970]
Epoch [64/120    avg_loss:0.180, val_acc:0.968]
Epoch [65/120    avg_loss:0.138, val_acc:0.966]
Epoch [66/120    avg_loss:0.136, val_acc:0.954]
Epoch [67/120    avg_loss:0.119, val_acc:0.913]
Epoch [68/120    avg_loss:0.125, val_acc:0.968]
Epoch [69/120    avg_loss:0.096, val_acc:0.966]
Epoch [70/120    avg_loss:0.090, val_acc:0.984]
Epoch [71/120    avg_loss:0.104, val_acc:0.976]
Epoch [72/120    avg_loss:0.113, val_acc:0.970]
Epoch [73/120    avg_loss:0.145, val_acc:0.972]
Epoch [74/120    avg_loss:0.139, val_acc:0.968]
Epoch [75/120    avg_loss:0.126, val_acc:0.976]
Epoch [76/120    avg_loss:0.136, val_acc:0.968]
Epoch [77/120    avg_loss:0.095, val_acc:0.978]
Epoch [78/120    avg_loss:0.096, val_acc:0.982]
Epoch [79/120    avg_loss:0.084, val_acc:0.972]
Epoch [80/120    avg_loss:0.134, val_acc:0.974]
Epoch [81/120    avg_loss:0.085, val_acc:0.968]
Epoch [82/120    avg_loss:0.087, val_acc:0.978]
Epoch [83/120    avg_loss:0.073, val_acc:0.972]
Epoch [84/120    avg_loss:0.077, val_acc:0.978]
Epoch [85/120    avg_loss:0.068, val_acc:0.978]
Epoch [86/120    avg_loss:0.050, val_acc:0.984]
Epoch [87/120    avg_loss:0.059, val_acc:0.986]
Epoch [88/120    avg_loss:0.053, val_acc:0.988]
Epoch [89/120    avg_loss:0.049, val_acc:0.984]
Epoch [90/120    avg_loss:0.056, val_acc:0.982]
Epoch [91/120    avg_loss:0.057, val_acc:0.984]
Epoch [92/120    avg_loss:0.054, val_acc:0.986]
Epoch [93/120    avg_loss:0.058, val_acc:0.980]
Epoch [94/120    avg_loss:0.064, val_acc:0.978]
Epoch [95/120    avg_loss:0.049, val_acc:0.986]
Epoch [96/120    avg_loss:0.047, val_acc:0.986]
Epoch [97/120    avg_loss:0.055, val_acc:0.986]
Epoch [98/120    avg_loss:0.047, val_acc:0.986]
Epoch [99/120    avg_loss:0.043, val_acc:0.982]
Epoch [100/120    avg_loss:0.055, val_acc:0.980]
Epoch [101/120    avg_loss:0.047, val_acc:0.984]
Epoch [102/120    avg_loss:0.048, val_acc:0.984]
Epoch [103/120    avg_loss:0.047, val_acc:0.984]
Epoch [104/120    avg_loss:0.048, val_acc:0.984]
Epoch [105/120    avg_loss:0.045, val_acc:0.984]
Epoch [106/120    avg_loss:0.049, val_acc:0.984]
Epoch [107/120    avg_loss:0.045, val_acc:0.984]
Epoch [108/120    avg_loss:0.052, val_acc:0.984]
Epoch [109/120    avg_loss:0.044, val_acc:0.984]
Epoch [110/120    avg_loss:0.040, val_acc:0.984]
Epoch [111/120    avg_loss:0.053, val_acc:0.984]
Epoch [112/120    avg_loss:0.049, val_acc:0.984]
Epoch [113/120    avg_loss:0.049, val_acc:0.984]
Epoch [114/120    avg_loss:0.039, val_acc:0.984]
Epoch [115/120    avg_loss:0.041, val_acc:0.984]
Epoch [116/120    avg_loss:0.057, val_acc:0.984]
Epoch [117/120    avg_loss:0.054, val_acc:0.984]
Epoch [118/120    avg_loss:0.043, val_acc:0.984]
Epoch [119/120    avg_loss:0.052, val_acc:0.984]
Epoch [120/120    avg_loss:0.044, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   9   0   0   0   0   0   0]
 [  0   0   0 219   9   0   0   0   2   0   0   0   0   0]
 [  0   0   0   1 208  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   2 832]]

Accuracy:
98.35820895522387

F1 scores:
[       nan 0.99927061 0.94170404 0.97333333 0.90434783 0.88356164
 0.99756691 0.85555556 0.99742931 1.         1.         0.99734043
 0.9956044  0.99879952]

Kappa:
0.981720705561632
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe7760f48d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.608, val_acc:0.323]
Epoch [2/120    avg_loss:2.334, val_acc:0.405]
Epoch [3/120    avg_loss:2.162, val_acc:0.520]
Epoch [4/120    avg_loss:1.998, val_acc:0.567]
Epoch [5/120    avg_loss:1.848, val_acc:0.619]
Epoch [6/120    avg_loss:1.692, val_acc:0.651]
Epoch [7/120    avg_loss:1.537, val_acc:0.704]
Epoch [8/120    avg_loss:1.383, val_acc:0.683]
Epoch [9/120    avg_loss:1.248, val_acc:0.762]
Epoch [10/120    avg_loss:1.123, val_acc:0.798]
Epoch [11/120    avg_loss:1.020, val_acc:0.819]
Epoch [12/120    avg_loss:0.917, val_acc:0.853]
Epoch [13/120    avg_loss:0.888, val_acc:0.798]
Epoch [14/120    avg_loss:0.834, val_acc:0.827]
Epoch [15/120    avg_loss:0.756, val_acc:0.875]
Epoch [16/120    avg_loss:0.712, val_acc:0.853]
Epoch [17/120    avg_loss:0.671, val_acc:0.883]
Epoch [18/120    avg_loss:0.600, val_acc:0.903]
Epoch [19/120    avg_loss:0.579, val_acc:0.901]
Epoch [20/120    avg_loss:0.559, val_acc:0.891]
Epoch [21/120    avg_loss:0.538, val_acc:0.913]
Epoch [22/120    avg_loss:0.482, val_acc:0.909]
Epoch [23/120    avg_loss:0.491, val_acc:0.907]
Epoch [24/120    avg_loss:0.513, val_acc:0.891]
Epoch [25/120    avg_loss:0.490, val_acc:0.925]
Epoch [26/120    avg_loss:0.431, val_acc:0.919]
Epoch [27/120    avg_loss:0.434, val_acc:0.911]
Epoch [28/120    avg_loss:0.495, val_acc:0.929]
Epoch [29/120    avg_loss:0.449, val_acc:0.952]
Epoch [30/120    avg_loss:0.418, val_acc:0.891]
Epoch [31/120    avg_loss:0.419, val_acc:0.869]
Epoch [32/120    avg_loss:0.378, val_acc:0.923]
Epoch [33/120    avg_loss:0.347, val_acc:0.944]
Epoch [34/120    avg_loss:0.313, val_acc:0.915]
Epoch [35/120    avg_loss:0.320, val_acc:0.903]
Epoch [36/120    avg_loss:0.336, val_acc:0.940]
Epoch [37/120    avg_loss:0.295, val_acc:0.940]
Epoch [38/120    avg_loss:0.333, val_acc:0.919]
Epoch [39/120    avg_loss:0.304, val_acc:0.960]
Epoch [40/120    avg_loss:0.299, val_acc:0.950]
Epoch [41/120    avg_loss:0.282, val_acc:0.883]
Epoch [42/120    avg_loss:0.336, val_acc:0.923]
Epoch [43/120    avg_loss:0.334, val_acc:0.944]
Epoch [44/120    avg_loss:0.258, val_acc:0.972]
Epoch [45/120    avg_loss:0.241, val_acc:0.966]
Epoch [46/120    avg_loss:0.288, val_acc:0.938]
Epoch [47/120    avg_loss:0.279, val_acc:0.980]
Epoch [48/120    avg_loss:0.271, val_acc:0.948]
Epoch [49/120    avg_loss:0.266, val_acc:0.952]
Epoch [50/120    avg_loss:0.182, val_acc:0.972]
Epoch [51/120    avg_loss:0.207, val_acc:0.974]
Epoch [52/120    avg_loss:0.202, val_acc:0.950]
Epoch [53/120    avg_loss:0.211, val_acc:0.954]
Epoch [54/120    avg_loss:0.145, val_acc:0.978]
Epoch [55/120    avg_loss:0.150, val_acc:0.974]
Epoch [56/120    avg_loss:0.188, val_acc:0.962]
Epoch [57/120    avg_loss:0.196, val_acc:0.968]
Epoch [58/120    avg_loss:0.207, val_acc:0.972]
Epoch [59/120    avg_loss:0.182, val_acc:0.933]
Epoch [60/120    avg_loss:0.236, val_acc:0.960]
Epoch [61/120    avg_loss:0.198, val_acc:0.976]
Epoch [62/120    avg_loss:0.140, val_acc:0.982]
Epoch [63/120    avg_loss:0.121, val_acc:0.978]
Epoch [64/120    avg_loss:0.122, val_acc:0.982]
Epoch [65/120    avg_loss:0.142, val_acc:0.982]
Epoch [66/120    avg_loss:0.113, val_acc:0.982]
Epoch [67/120    avg_loss:0.114, val_acc:0.984]
Epoch [68/120    avg_loss:0.116, val_acc:0.984]
Epoch [69/120    avg_loss:0.096, val_acc:0.980]
Epoch [70/120    avg_loss:0.118, val_acc:0.982]
Epoch [71/120    avg_loss:0.108, val_acc:0.982]
Epoch [72/120    avg_loss:0.125, val_acc:0.988]
Epoch [73/120    avg_loss:0.100, val_acc:0.988]
Epoch [74/120    avg_loss:0.094, val_acc:0.986]
Epoch [75/120    avg_loss:0.085, val_acc:0.984]
Epoch [76/120    avg_loss:0.087, val_acc:0.984]
Epoch [77/120    avg_loss:0.089, val_acc:0.984]
Epoch [78/120    avg_loss:0.102, val_acc:0.984]
Epoch [79/120    avg_loss:0.093, val_acc:0.984]
Epoch [80/120    avg_loss:0.097, val_acc:0.984]
Epoch [81/120    avg_loss:0.092, val_acc:0.984]
Epoch [82/120    avg_loss:0.084, val_acc:0.984]
Epoch [83/120    avg_loss:0.094, val_acc:0.984]
Epoch [84/120    avg_loss:0.083, val_acc:0.982]
Epoch [85/120    avg_loss:0.097, val_acc:0.984]
Epoch [86/120    avg_loss:0.075, val_acc:0.984]
Epoch [87/120    avg_loss:0.085, val_acc:0.984]
Epoch [88/120    avg_loss:0.099, val_acc:0.984]
Epoch [89/120    avg_loss:0.084, val_acc:0.984]
Epoch [90/120    avg_loss:0.097, val_acc:0.984]
Epoch [91/120    avg_loss:0.085, val_acc:0.984]
Epoch [92/120    avg_loss:0.089, val_acc:0.984]
Epoch [93/120    avg_loss:0.085, val_acc:0.984]
Epoch [94/120    avg_loss:0.097, val_acc:0.984]
Epoch [95/120    avg_loss:0.094, val_acc:0.984]
Epoch [96/120    avg_loss:0.074, val_acc:0.984]
Epoch [97/120    avg_loss:0.069, val_acc:0.984]
Epoch [98/120    avg_loss:0.082, val_acc:0.984]
Epoch [99/120    avg_loss:0.085, val_acc:0.984]
Epoch [100/120    avg_loss:0.079, val_acc:0.984]
Epoch [101/120    avg_loss:0.080, val_acc:0.984]
Epoch [102/120    avg_loss:0.068, val_acc:0.984]
Epoch [103/120    avg_loss:0.090, val_acc:0.984]
Epoch [104/120    avg_loss:0.081, val_acc:0.984]
Epoch [105/120    avg_loss:0.091, val_acc:0.984]
Epoch [106/120    avg_loss:0.076, val_acc:0.984]
Epoch [107/120    avg_loss:0.097, val_acc:0.984]
Epoch [108/120    avg_loss:0.089, val_acc:0.984]
Epoch [109/120    avg_loss:0.078, val_acc:0.984]
Epoch [110/120    avg_loss:0.086, val_acc:0.984]
Epoch [111/120    avg_loss:0.099, val_acc:0.984]
Epoch [112/120    avg_loss:0.080, val_acc:0.984]
Epoch [113/120    avg_loss:0.089, val_acc:0.984]
Epoch [114/120    avg_loss:0.075, val_acc:0.984]
Epoch [115/120    avg_loss:0.094, val_acc:0.984]
Epoch [116/120    avg_loss:0.089, val_acc:0.984]
Epoch [117/120    avg_loss:0.095, val_acc:0.984]
Epoch [118/120    avg_loss:0.081, val_acc:0.984]
Epoch [119/120    avg_loss:0.107, val_acc:0.984]
Epoch [120/120    avg_loss:0.087, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   7   0   0   0   0   1   0   0   0   0]
 [  0   0   1   2 211  11   0   0   0   0   1   0   1   0]
 [  0   0   0   0  31 113   0   0   1   0   0   0   0   0]
 [  0   5   0   0   0   0 201   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.18763326226012

F1 scores:
[       nan 0.99636364 0.9452954  0.97797357 0.88100209 0.8401487
 0.98771499 0.9005848  0.998713   0.99893276 0.99862826 1.
 0.9944629  1.        ]

Kappa:
0.9798163763639008
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f38f6b5b8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.569, val_acc:0.492]
Epoch [2/120    avg_loss:2.306, val_acc:0.542]
Epoch [3/120    avg_loss:2.114, val_acc:0.651]
Epoch [4/120    avg_loss:1.905, val_acc:0.692]
Epoch [5/120    avg_loss:1.690, val_acc:0.702]
Epoch [6/120    avg_loss:1.523, val_acc:0.784]
Epoch [7/120    avg_loss:1.364, val_acc:0.784]
Epoch [8/120    avg_loss:1.216, val_acc:0.766]
Epoch [9/120    avg_loss:1.088, val_acc:0.819]
Epoch [10/120    avg_loss:0.960, val_acc:0.823]
Epoch [11/120    avg_loss:0.881, val_acc:0.859]
Epoch [12/120    avg_loss:0.847, val_acc:0.798]
Epoch [13/120    avg_loss:0.809, val_acc:0.849]
Epoch [14/120    avg_loss:0.721, val_acc:0.877]
Epoch [15/120    avg_loss:0.673, val_acc:0.899]
Epoch [16/120    avg_loss:0.592, val_acc:0.899]
Epoch [17/120    avg_loss:0.592, val_acc:0.865]
Epoch [18/120    avg_loss:0.552, val_acc:0.897]
Epoch [19/120    avg_loss:0.565, val_acc:0.925]
Epoch [20/120    avg_loss:0.496, val_acc:0.899]
Epoch [21/120    avg_loss:0.521, val_acc:0.899]
Epoch [22/120    avg_loss:0.432, val_acc:0.921]
Epoch [23/120    avg_loss:0.468, val_acc:0.891]
Epoch [24/120    avg_loss:0.403, val_acc:0.931]
Epoch [25/120    avg_loss:0.447, val_acc:0.873]
Epoch [26/120    avg_loss:0.437, val_acc:0.899]
Epoch [27/120    avg_loss:0.466, val_acc:0.923]
Epoch [28/120    avg_loss:0.358, val_acc:0.913]
Epoch [29/120    avg_loss:0.376, val_acc:0.917]
Epoch [30/120    avg_loss:0.353, val_acc:0.919]
Epoch [31/120    avg_loss:0.355, val_acc:0.909]
Epoch [32/120    avg_loss:0.332, val_acc:0.891]
Epoch [33/120    avg_loss:0.311, val_acc:0.887]
Epoch [34/120    avg_loss:0.372, val_acc:0.903]
Epoch [35/120    avg_loss:0.332, val_acc:0.921]
Epoch [36/120    avg_loss:0.318, val_acc:0.917]
Epoch [37/120    avg_loss:0.314, val_acc:0.879]
Epoch [38/120    avg_loss:0.270, val_acc:0.937]
Epoch [39/120    avg_loss:0.236, val_acc:0.937]
Epoch [40/120    avg_loss:0.228, val_acc:0.940]
Epoch [41/120    avg_loss:0.206, val_acc:0.940]
Epoch [42/120    avg_loss:0.232, val_acc:0.944]
Epoch [43/120    avg_loss:0.227, val_acc:0.950]
Epoch [44/120    avg_loss:0.195, val_acc:0.946]
Epoch [45/120    avg_loss:0.208, val_acc:0.948]
Epoch [46/120    avg_loss:0.183, val_acc:0.942]
Epoch [47/120    avg_loss:0.219, val_acc:0.944]
Epoch [48/120    avg_loss:0.213, val_acc:0.948]
Epoch [49/120    avg_loss:0.187, val_acc:0.944]
Epoch [50/120    avg_loss:0.191, val_acc:0.950]
Epoch [51/120    avg_loss:0.216, val_acc:0.950]
Epoch [52/120    avg_loss:0.217, val_acc:0.950]
Epoch [53/120    avg_loss:0.185, val_acc:0.958]
Epoch [54/120    avg_loss:0.206, val_acc:0.956]
Epoch [55/120    avg_loss:0.206, val_acc:0.946]
Epoch [56/120    avg_loss:0.224, val_acc:0.956]
Epoch [57/120    avg_loss:0.185, val_acc:0.952]
Epoch [58/120    avg_loss:0.195, val_acc:0.958]
Epoch [59/120    avg_loss:0.193, val_acc:0.954]
Epoch [60/120    avg_loss:0.185, val_acc:0.954]
Epoch [61/120    avg_loss:0.199, val_acc:0.952]
Epoch [62/120    avg_loss:0.181, val_acc:0.952]
Epoch [63/120    avg_loss:0.174, val_acc:0.958]
Epoch [64/120    avg_loss:0.211, val_acc:0.956]
Epoch [65/120    avg_loss:0.165, val_acc:0.950]
Epoch [66/120    avg_loss:0.190, val_acc:0.960]
Epoch [67/120    avg_loss:0.176, val_acc:0.960]
Epoch [68/120    avg_loss:0.172, val_acc:0.956]
Epoch [69/120    avg_loss:0.179, val_acc:0.964]
Epoch [70/120    avg_loss:0.173, val_acc:0.964]
Epoch [71/120    avg_loss:0.169, val_acc:0.956]
Epoch [72/120    avg_loss:0.170, val_acc:0.954]
Epoch [73/120    avg_loss:0.165, val_acc:0.960]
Epoch [74/120    avg_loss:0.177, val_acc:0.964]
Epoch [75/120    avg_loss:0.187, val_acc:0.956]
Epoch [76/120    avg_loss:0.176, val_acc:0.952]
Epoch [77/120    avg_loss:0.164, val_acc:0.964]
Epoch [78/120    avg_loss:0.173, val_acc:0.962]
Epoch [79/120    avg_loss:0.177, val_acc:0.968]
Epoch [80/120    avg_loss:0.184, val_acc:0.966]
Epoch [81/120    avg_loss:0.159, val_acc:0.962]
Epoch [82/120    avg_loss:0.156, val_acc:0.966]
Epoch [83/120    avg_loss:0.178, val_acc:0.964]
Epoch [84/120    avg_loss:0.175, val_acc:0.968]
Epoch [85/120    avg_loss:0.151, val_acc:0.972]
Epoch [86/120    avg_loss:0.145, val_acc:0.974]
Epoch [87/120    avg_loss:0.161, val_acc:0.968]
Epoch [88/120    avg_loss:0.152, val_acc:0.970]
Epoch [89/120    avg_loss:0.136, val_acc:0.970]
Epoch [90/120    avg_loss:0.166, val_acc:0.974]
Epoch [91/120    avg_loss:0.161, val_acc:0.972]
Epoch [92/120    avg_loss:0.146, val_acc:0.968]
Epoch [93/120    avg_loss:0.152, val_acc:0.968]
Epoch [94/120    avg_loss:0.141, val_acc:0.970]
Epoch [95/120    avg_loss:0.161, val_acc:0.972]
Epoch [96/120    avg_loss:0.152, val_acc:0.968]
Epoch [97/120    avg_loss:0.145, val_acc:0.972]
Epoch [98/120    avg_loss:0.159, val_acc:0.968]
Epoch [99/120    avg_loss:0.146, val_acc:0.968]
Epoch [100/120    avg_loss:0.132, val_acc:0.974]
Epoch [101/120    avg_loss:0.133, val_acc:0.972]
Epoch [102/120    avg_loss:0.152, val_acc:0.972]
Epoch [103/120    avg_loss:0.139, val_acc:0.968]
Epoch [104/120    avg_loss:0.139, val_acc:0.970]
Epoch [105/120    avg_loss:0.138, val_acc:0.970]
Epoch [106/120    avg_loss:0.145, val_acc:0.968]
Epoch [107/120    avg_loss:0.143, val_acc:0.972]
Epoch [108/120    avg_loss:0.141, val_acc:0.970]
Epoch [109/120    avg_loss:0.127, val_acc:0.968]
Epoch [110/120    avg_loss:0.128, val_acc:0.970]
Epoch [111/120    avg_loss:0.142, val_acc:0.970]
Epoch [112/120    avg_loss:0.121, val_acc:0.974]
Epoch [113/120    avg_loss:0.124, val_acc:0.970]
Epoch [114/120    avg_loss:0.144, val_acc:0.972]
Epoch [115/120    avg_loss:0.116, val_acc:0.980]
Epoch [116/120    avg_loss:0.132, val_acc:0.968]
Epoch [117/120    avg_loss:0.122, val_acc:0.976]
Epoch [118/120    avg_loss:0.135, val_acc:0.972]
Epoch [119/120    avg_loss:0.127, val_acc:0.976]
Epoch [120/120    avg_loss:0.130, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   1 190   0   3   0   0  25   0   0   0   0   0   0]
 [  0   0   0 221   7   0   0   1   0   1   0   0   0   0]
 [  0   0   0   0 188  39   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   2   0   0   1   0 203   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 452   1]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.71855010660981

F1 scores:
[       nan 0.997815   0.90692124 0.98004435 0.85454545 0.83174603
 0.99266504 0.82352941 1.         0.99893276 1.         0.99734043
 0.99669239 0.99940084]

Kappa:
0.974602552184637
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f53323808d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.560, val_acc:0.339]
Epoch [2/120    avg_loss:2.299, val_acc:0.446]
Epoch [3/120    avg_loss:2.091, val_acc:0.554]
Epoch [4/120    avg_loss:1.888, val_acc:0.681]
Epoch [5/120    avg_loss:1.720, val_acc:0.714]
Epoch [6/120    avg_loss:1.515, val_acc:0.762]
Epoch [7/120    avg_loss:1.388, val_acc:0.756]
Epoch [8/120    avg_loss:1.231, val_acc:0.813]
Epoch [9/120    avg_loss:1.114, val_acc:0.865]
Epoch [10/120    avg_loss:1.002, val_acc:0.869]
Epoch [11/120    avg_loss:0.930, val_acc:0.873]
Epoch [12/120    avg_loss:0.832, val_acc:0.873]
Epoch [13/120    avg_loss:0.782, val_acc:0.835]
Epoch [14/120    avg_loss:0.674, val_acc:0.885]
Epoch [15/120    avg_loss:0.735, val_acc:0.853]
Epoch [16/120    avg_loss:0.690, val_acc:0.887]
Epoch [17/120    avg_loss:0.600, val_acc:0.885]
Epoch [18/120    avg_loss:0.612, val_acc:0.808]
Epoch [19/120    avg_loss:0.571, val_acc:0.891]
Epoch [20/120    avg_loss:0.511, val_acc:0.911]
Epoch [21/120    avg_loss:0.505, val_acc:0.871]
Epoch [22/120    avg_loss:0.476, val_acc:0.887]
Epoch [23/120    avg_loss:0.479, val_acc:0.903]
Epoch [24/120    avg_loss:0.486, val_acc:0.815]
Epoch [25/120    avg_loss:0.464, val_acc:0.889]
Epoch [26/120    avg_loss:0.449, val_acc:0.893]
Epoch [27/120    avg_loss:0.434, val_acc:0.881]
Epoch [28/120    avg_loss:0.399, val_acc:0.917]
Epoch [29/120    avg_loss:0.434, val_acc:0.905]
Epoch [30/120    avg_loss:0.463, val_acc:0.901]
Epoch [31/120    avg_loss:0.395, val_acc:0.929]
Epoch [32/120    avg_loss:0.360, val_acc:0.937]
Epoch [33/120    avg_loss:0.437, val_acc:0.825]
Epoch [34/120    avg_loss:0.429, val_acc:0.929]
Epoch [35/120    avg_loss:0.336, val_acc:0.921]
Epoch [36/120    avg_loss:0.311, val_acc:0.917]
Epoch [37/120    avg_loss:0.296, val_acc:0.925]
Epoch [38/120    avg_loss:0.276, val_acc:0.931]
Epoch [39/120    avg_loss:0.361, val_acc:0.927]
Epoch [40/120    avg_loss:0.306, val_acc:0.919]
Epoch [41/120    avg_loss:0.297, val_acc:0.875]
Epoch [42/120    avg_loss:0.302, val_acc:0.925]
Epoch [43/120    avg_loss:0.279, val_acc:0.927]
Epoch [44/120    avg_loss:0.262, val_acc:0.909]
Epoch [45/120    avg_loss:0.300, val_acc:0.948]
Epoch [46/120    avg_loss:0.243, val_acc:0.958]
Epoch [47/120    avg_loss:0.228, val_acc:0.950]
Epoch [48/120    avg_loss:0.269, val_acc:0.915]
Epoch [49/120    avg_loss:0.280, val_acc:0.921]
Epoch [50/120    avg_loss:0.246, val_acc:0.938]
Epoch [51/120    avg_loss:0.218, val_acc:0.954]
Epoch [52/120    avg_loss:0.213, val_acc:0.970]
Epoch [53/120    avg_loss:0.197, val_acc:0.944]
Epoch [54/120    avg_loss:0.219, val_acc:0.929]
Epoch [55/120    avg_loss:0.218, val_acc:0.937]
Epoch [56/120    avg_loss:0.232, val_acc:0.915]
Epoch [57/120    avg_loss:0.256, val_acc:0.929]
Epoch [58/120    avg_loss:0.223, val_acc:0.956]
Epoch [59/120    avg_loss:0.225, val_acc:0.962]
Epoch [60/120    avg_loss:0.185, val_acc:0.966]
Epoch [61/120    avg_loss:0.126, val_acc:0.978]
Epoch [62/120    avg_loss:0.160, val_acc:0.950]
Epoch [63/120    avg_loss:0.234, val_acc:0.933]
Epoch [64/120    avg_loss:0.225, val_acc:0.938]
Epoch [65/120    avg_loss:0.205, val_acc:0.931]
Epoch [66/120    avg_loss:0.187, val_acc:0.966]
Epoch [67/120    avg_loss:0.125, val_acc:0.958]
Epoch [68/120    avg_loss:0.147, val_acc:0.958]
Epoch [69/120    avg_loss:0.201, val_acc:0.950]
Epoch [70/120    avg_loss:0.166, val_acc:0.942]
Epoch [71/120    avg_loss:0.251, val_acc:0.942]
Epoch [72/120    avg_loss:0.161, val_acc:0.960]
Epoch [73/120    avg_loss:0.176, val_acc:0.958]
Epoch [74/120    avg_loss:0.234, val_acc:0.923]
Epoch [75/120    avg_loss:0.204, val_acc:0.956]
Epoch [76/120    avg_loss:0.119, val_acc:0.968]
Epoch [77/120    avg_loss:0.125, val_acc:0.968]
Epoch [78/120    avg_loss:0.107, val_acc:0.970]
Epoch [79/120    avg_loss:0.104, val_acc:0.980]
Epoch [80/120    avg_loss:0.104, val_acc:0.980]
Epoch [81/120    avg_loss:0.099, val_acc:0.976]
Epoch [82/120    avg_loss:0.124, val_acc:0.974]
Epoch [83/120    avg_loss:0.105, val_acc:0.974]
Epoch [84/120    avg_loss:0.091, val_acc:0.976]
Epoch [85/120    avg_loss:0.094, val_acc:0.972]
Epoch [86/120    avg_loss:0.097, val_acc:0.978]
Epoch [87/120    avg_loss:0.097, val_acc:0.976]
Epoch [88/120    avg_loss:0.084, val_acc:0.974]
Epoch [89/120    avg_loss:0.104, val_acc:0.972]
Epoch [90/120    avg_loss:0.098, val_acc:0.976]
Epoch [91/120    avg_loss:0.086, val_acc:0.976]
Epoch [92/120    avg_loss:0.094, val_acc:0.976]
Epoch [93/120    avg_loss:0.088, val_acc:0.978]
Epoch [94/120    avg_loss:0.091, val_acc:0.978]
Epoch [95/120    avg_loss:0.091, val_acc:0.978]
Epoch [96/120    avg_loss:0.094, val_acc:0.978]
Epoch [97/120    avg_loss:0.087, val_acc:0.978]
Epoch [98/120    avg_loss:0.087, val_acc:0.978]
Epoch [99/120    avg_loss:0.085, val_acc:0.974]
Epoch [100/120    avg_loss:0.085, val_acc:0.976]
Epoch [101/120    avg_loss:0.082, val_acc:0.974]
Epoch [102/120    avg_loss:0.079, val_acc:0.976]
Epoch [103/120    avg_loss:0.076, val_acc:0.976]
Epoch [104/120    avg_loss:0.089, val_acc:0.978]
Epoch [105/120    avg_loss:0.089, val_acc:0.978]
Epoch [106/120    avg_loss:0.082, val_acc:0.976]
Epoch [107/120    avg_loss:0.089, val_acc:0.976]
Epoch [108/120    avg_loss:0.070, val_acc:0.976]
Epoch [109/120    avg_loss:0.084, val_acc:0.976]
Epoch [110/120    avg_loss:0.089, val_acc:0.976]
Epoch [111/120    avg_loss:0.096, val_acc:0.976]
Epoch [112/120    avg_loss:0.095, val_acc:0.976]
Epoch [113/120    avg_loss:0.086, val_acc:0.976]
Epoch [114/120    avg_loss:0.075, val_acc:0.976]
Epoch [115/120    avg_loss:0.089, val_acc:0.976]
Epoch [116/120    avg_loss:0.102, val_acc:0.976]
Epoch [117/120    avg_loss:0.082, val_acc:0.976]
Epoch [118/120    avg_loss:0.082, val_acc:0.976]
Epoch [119/120    avg_loss:0.069, val_acc:0.976]
Epoch [120/120    avg_loss:0.090, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   0 220   8   0   0   0   2   0   0   0   0   0]
 [  0   0   1   3 207  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 125   4   0   0   0   0   0   0   0]
 [  0   8   0   0   0   0 198   0   0   0   0   0   0   0]
 [  0   0  23   0   0   0   0  71   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   3   0   0   0   0   0   0   0   0   0 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.99573560767591

F1 scores:
[       nan 0.99419448 0.91868132 0.97130243 0.90393013 0.87412587
 0.97058824 0.81142857 0.99742931 1.         1.         1.
 0.99667774 1.        ]

Kappa:
0.9776801648526843
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fda3f68c978>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.566, val_acc:0.296]
Epoch [2/120    avg_loss:2.334, val_acc:0.389]
Epoch [3/120    avg_loss:2.154, val_acc:0.516]
Epoch [4/120    avg_loss:1.989, val_acc:0.589]
Epoch [5/120    avg_loss:1.809, val_acc:0.663]
Epoch [6/120    avg_loss:1.640, val_acc:0.653]
Epoch [7/120    avg_loss:1.460, val_acc:0.726]
Epoch [8/120    avg_loss:1.329, val_acc:0.786]
Epoch [9/120    avg_loss:1.223, val_acc:0.784]
Epoch [10/120    avg_loss:1.107, val_acc:0.812]
Epoch [11/120    avg_loss:0.997, val_acc:0.841]
Epoch [12/120    avg_loss:0.986, val_acc:0.806]
Epoch [13/120    avg_loss:0.864, val_acc:0.881]
Epoch [14/120    avg_loss:0.831, val_acc:0.839]
Epoch [15/120    avg_loss:0.803, val_acc:0.891]
Epoch [16/120    avg_loss:0.705, val_acc:0.897]
Epoch [17/120    avg_loss:0.635, val_acc:0.899]
Epoch [18/120    avg_loss:0.651, val_acc:0.837]
Epoch [19/120    avg_loss:0.577, val_acc:0.909]
Epoch [20/120    avg_loss:0.575, val_acc:0.889]
Epoch [21/120    avg_loss:0.536, val_acc:0.907]
Epoch [22/120    avg_loss:0.510, val_acc:0.879]
Epoch [23/120    avg_loss:0.483, val_acc:0.905]
Epoch [24/120    avg_loss:0.472, val_acc:0.903]
Epoch [25/120    avg_loss:0.415, val_acc:0.889]
Epoch [26/120    avg_loss:0.418, val_acc:0.935]
Epoch [27/120    avg_loss:0.426, val_acc:0.933]
Epoch [28/120    avg_loss:0.363, val_acc:0.917]
Epoch [29/120    avg_loss:0.369, val_acc:0.917]
Epoch [30/120    avg_loss:0.348, val_acc:0.929]
Epoch [31/120    avg_loss:0.359, val_acc:0.927]
Epoch [32/120    avg_loss:0.310, val_acc:0.931]
Epoch [33/120    avg_loss:0.391, val_acc:0.919]
Epoch [34/120    avg_loss:0.388, val_acc:0.931]
Epoch [35/120    avg_loss:0.294, val_acc:0.931]
Epoch [36/120    avg_loss:0.315, val_acc:0.950]
Epoch [37/120    avg_loss:0.336, val_acc:0.917]
Epoch [38/120    avg_loss:0.240, val_acc:0.948]
Epoch [39/120    avg_loss:0.250, val_acc:0.935]
Epoch [40/120    avg_loss:0.273, val_acc:0.958]
Epoch [41/120    avg_loss:0.254, val_acc:0.956]
Epoch [42/120    avg_loss:0.223, val_acc:0.954]
Epoch [43/120    avg_loss:0.282, val_acc:0.931]
Epoch [44/120    avg_loss:0.193, val_acc:0.962]
Epoch [45/120    avg_loss:0.218, val_acc:0.950]
Epoch [46/120    avg_loss:0.260, val_acc:0.938]
Epoch [47/120    avg_loss:0.221, val_acc:0.935]
Epoch [48/120    avg_loss:0.212, val_acc:0.964]
Epoch [49/120    avg_loss:0.177, val_acc:0.964]
Epoch [50/120    avg_loss:0.219, val_acc:0.962]
Epoch [51/120    avg_loss:0.166, val_acc:0.937]
Epoch [52/120    avg_loss:0.158, val_acc:0.966]
Epoch [53/120    avg_loss:0.162, val_acc:0.944]
Epoch [54/120    avg_loss:0.173, val_acc:0.968]
Epoch [55/120    avg_loss:0.141, val_acc:0.958]
Epoch [56/120    avg_loss:0.199, val_acc:0.944]
Epoch [57/120    avg_loss:0.187, val_acc:0.942]
Epoch [58/120    avg_loss:0.173, val_acc:0.952]
Epoch [59/120    avg_loss:0.186, val_acc:0.958]
Epoch [60/120    avg_loss:0.136, val_acc:0.966]
Epoch [61/120    avg_loss:0.191, val_acc:0.958]
Epoch [62/120    avg_loss:0.197, val_acc:0.954]
Epoch [63/120    avg_loss:0.167, val_acc:0.944]
Epoch [64/120    avg_loss:0.205, val_acc:0.964]
Epoch [65/120    avg_loss:0.164, val_acc:0.960]
Epoch [66/120    avg_loss:0.194, val_acc:0.950]
Epoch [67/120    avg_loss:0.153, val_acc:0.962]
Epoch [68/120    avg_loss:0.141, val_acc:0.958]
Epoch [69/120    avg_loss:0.106, val_acc:0.966]
Epoch [70/120    avg_loss:0.104, val_acc:0.966]
Epoch [71/120    avg_loss:0.088, val_acc:0.966]
Epoch [72/120    avg_loss:0.109, val_acc:0.968]
Epoch [73/120    avg_loss:0.084, val_acc:0.966]
Epoch [74/120    avg_loss:0.080, val_acc:0.968]
Epoch [75/120    avg_loss:0.070, val_acc:0.972]
Epoch [76/120    avg_loss:0.081, val_acc:0.968]
Epoch [77/120    avg_loss:0.076, val_acc:0.968]
Epoch [78/120    avg_loss:0.087, val_acc:0.972]
Epoch [79/120    avg_loss:0.084, val_acc:0.970]
Epoch [80/120    avg_loss:0.088, val_acc:0.974]
Epoch [81/120    avg_loss:0.066, val_acc:0.974]
Epoch [82/120    avg_loss:0.085, val_acc:0.974]
Epoch [83/120    avg_loss:0.073, val_acc:0.974]
Epoch [84/120    avg_loss:0.068, val_acc:0.978]
Epoch [85/120    avg_loss:0.069, val_acc:0.978]
Epoch [86/120    avg_loss:0.075, val_acc:0.974]
Epoch [87/120    avg_loss:0.078, val_acc:0.976]
Epoch [88/120    avg_loss:0.075, val_acc:0.982]
Epoch [89/120    avg_loss:0.080, val_acc:0.980]
Epoch [90/120    avg_loss:0.070, val_acc:0.978]
Epoch [91/120    avg_loss:0.063, val_acc:0.980]
Epoch [92/120    avg_loss:0.070, val_acc:0.978]
Epoch [93/120    avg_loss:0.060, val_acc:0.974]
Epoch [94/120    avg_loss:0.074, val_acc:0.978]
Epoch [95/120    avg_loss:0.065, val_acc:0.970]
Epoch [96/120    avg_loss:0.068, val_acc:0.978]
Epoch [97/120    avg_loss:0.066, val_acc:0.978]
Epoch [98/120    avg_loss:0.069, val_acc:0.974]
Epoch [99/120    avg_loss:0.061, val_acc:0.976]
Epoch [100/120    avg_loss:0.062, val_acc:0.976]
Epoch [101/120    avg_loss:0.069, val_acc:0.980]
Epoch [102/120    avg_loss:0.058, val_acc:0.980]
Epoch [103/120    avg_loss:0.065, val_acc:0.980]
Epoch [104/120    avg_loss:0.061, val_acc:0.980]
Epoch [105/120    avg_loss:0.061, val_acc:0.982]
Epoch [106/120    avg_loss:0.074, val_acc:0.982]
Epoch [107/120    avg_loss:0.058, val_acc:0.982]
Epoch [108/120    avg_loss:0.058, val_acc:0.980]
Epoch [109/120    avg_loss:0.070, val_acc:0.980]
Epoch [110/120    avg_loss:0.056, val_acc:0.980]
Epoch [111/120    avg_loss:0.055, val_acc:0.978]
Epoch [112/120    avg_loss:0.056, val_acc:0.978]
Epoch [113/120    avg_loss:0.055, val_acc:0.978]
Epoch [114/120    avg_loss:0.054, val_acc:0.978]
Epoch [115/120    avg_loss:0.052, val_acc:0.978]
Epoch [116/120    avg_loss:0.054, val_acc:0.978]
Epoch [117/120    avg_loss:0.065, val_acc:0.974]
Epoch [118/120    avg_loss:0.051, val_acc:0.976]
Epoch [119/120    avg_loss:0.051, val_acc:0.976]
Epoch [120/120    avg_loss:0.057, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 214   0   0   0   0   5   0   0   0   0   0   0]
 [  0   0   5 214   7   0   0   0   1   3   0   0   0   0]
 [  0   0   1   1 213  12   0   0   0   0   0   0   0   0]
 [  0   0   0   6  10 129   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0  18   0   0   0   0  76   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.46481876332622

F1 scores:
[       nan 0.99854227 0.93654267 0.94900222 0.9321663  0.9020979
 0.99512195 0.86857143 0.998713   0.99680511 1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9829052508034806
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0b07b12908>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.556, val_acc:0.312]
Epoch [2/120    avg_loss:2.307, val_acc:0.484]
Epoch [3/120    avg_loss:2.105, val_acc:0.522]
Epoch [4/120    avg_loss:1.908, val_acc:0.567]
Epoch [5/120    avg_loss:1.693, val_acc:0.601]
Epoch [6/120    avg_loss:1.501, val_acc:0.665]
Epoch [7/120    avg_loss:1.355, val_acc:0.694]
Epoch [8/120    avg_loss:1.210, val_acc:0.823]
Epoch [9/120    avg_loss:1.107, val_acc:0.875]
Epoch [10/120    avg_loss:0.981, val_acc:0.863]
Epoch [11/120    avg_loss:0.868, val_acc:0.871]
Epoch [12/120    avg_loss:0.772, val_acc:0.782]
Epoch [13/120    avg_loss:0.721, val_acc:0.855]
Epoch [14/120    avg_loss:0.641, val_acc:0.893]
Epoch [15/120    avg_loss:0.580, val_acc:0.843]
Epoch [16/120    avg_loss:0.609, val_acc:0.901]
Epoch [17/120    avg_loss:0.536, val_acc:0.891]
Epoch [18/120    avg_loss:0.535, val_acc:0.867]
Epoch [19/120    avg_loss:0.538, val_acc:0.893]
Epoch [20/120    avg_loss:0.609, val_acc:0.829]
Epoch [21/120    avg_loss:0.508, val_acc:0.853]
Epoch [22/120    avg_loss:0.441, val_acc:0.875]
Epoch [23/120    avg_loss:0.426, val_acc:0.905]
Epoch [24/120    avg_loss:0.389, val_acc:0.917]
Epoch [25/120    avg_loss:0.384, val_acc:0.893]
Epoch [26/120    avg_loss:0.409, val_acc:0.933]
Epoch [27/120    avg_loss:0.468, val_acc:0.917]
Epoch [28/120    avg_loss:0.424, val_acc:0.927]
Epoch [29/120    avg_loss:0.363, val_acc:0.937]
Epoch [30/120    avg_loss:0.322, val_acc:0.935]
Epoch [31/120    avg_loss:0.313, val_acc:0.927]
Epoch [32/120    avg_loss:0.307, val_acc:0.938]
Epoch [33/120    avg_loss:0.313, val_acc:0.925]
Epoch [34/120    avg_loss:0.288, val_acc:0.948]
Epoch [35/120    avg_loss:0.305, val_acc:0.944]
Epoch [36/120    avg_loss:0.267, val_acc:0.942]
Epoch [37/120    avg_loss:0.261, val_acc:0.927]
Epoch [38/120    avg_loss:0.263, val_acc:0.958]
Epoch [39/120    avg_loss:0.237, val_acc:0.944]
Epoch [40/120    avg_loss:0.264, val_acc:0.940]
Epoch [41/120    avg_loss:0.259, val_acc:0.952]
Epoch [42/120    avg_loss:0.236, val_acc:0.954]
Epoch [43/120    avg_loss:0.252, val_acc:0.923]
Epoch [44/120    avg_loss:0.237, val_acc:0.940]
Epoch [45/120    avg_loss:0.248, val_acc:0.913]
Epoch [46/120    avg_loss:0.234, val_acc:0.950]
Epoch [47/120    avg_loss:0.289, val_acc:0.911]
Epoch [48/120    avg_loss:0.283, val_acc:0.853]
Epoch [49/120    avg_loss:0.276, val_acc:0.937]
Epoch [50/120    avg_loss:0.232, val_acc:0.946]
Epoch [51/120    avg_loss:0.224, val_acc:0.927]
Epoch [52/120    avg_loss:0.175, val_acc:0.956]
Epoch [53/120    avg_loss:0.151, val_acc:0.960]
Epoch [54/120    avg_loss:0.145, val_acc:0.962]
Epoch [55/120    avg_loss:0.135, val_acc:0.966]
Epoch [56/120    avg_loss:0.144, val_acc:0.968]
Epoch [57/120    avg_loss:0.127, val_acc:0.968]
Epoch [58/120    avg_loss:0.128, val_acc:0.968]
Epoch [59/120    avg_loss:0.151, val_acc:0.966]
Epoch [60/120    avg_loss:0.138, val_acc:0.966]
Epoch [61/120    avg_loss:0.137, val_acc:0.966]
Epoch [62/120    avg_loss:0.119, val_acc:0.968]
Epoch [63/120    avg_loss:0.126, val_acc:0.968]
Epoch [64/120    avg_loss:0.134, val_acc:0.968]
Epoch [65/120    avg_loss:0.125, val_acc:0.966]
Epoch [66/120    avg_loss:0.126, val_acc:0.970]
Epoch [67/120    avg_loss:0.134, val_acc:0.968]
Epoch [68/120    avg_loss:0.138, val_acc:0.972]
Epoch [69/120    avg_loss:0.115, val_acc:0.966]
Epoch [70/120    avg_loss:0.123, val_acc:0.966]
Epoch [71/120    avg_loss:0.111, val_acc:0.970]
Epoch [72/120    avg_loss:0.115, val_acc:0.972]
Epoch [73/120    avg_loss:0.130, val_acc:0.972]
Epoch [74/120    avg_loss:0.103, val_acc:0.974]
Epoch [75/120    avg_loss:0.123, val_acc:0.968]
Epoch [76/120    avg_loss:0.129, val_acc:0.970]
Epoch [77/120    avg_loss:0.114, val_acc:0.972]
Epoch [78/120    avg_loss:0.105, val_acc:0.974]
Epoch [79/120    avg_loss:0.120, val_acc:0.970]
Epoch [80/120    avg_loss:0.127, val_acc:0.974]
Epoch [81/120    avg_loss:0.115, val_acc:0.972]
Epoch [82/120    avg_loss:0.110, val_acc:0.978]
Epoch [83/120    avg_loss:0.104, val_acc:0.970]
Epoch [84/120    avg_loss:0.096, val_acc:0.974]
Epoch [85/120    avg_loss:0.104, val_acc:0.974]
Epoch [86/120    avg_loss:0.099, val_acc:0.974]
Epoch [87/120    avg_loss:0.099, val_acc:0.968]
Epoch [88/120    avg_loss:0.136, val_acc:0.972]
Epoch [89/120    avg_loss:0.107, val_acc:0.976]
Epoch [90/120    avg_loss:0.099, val_acc:0.974]
Epoch [91/120    avg_loss:0.087, val_acc:0.978]
Epoch [92/120    avg_loss:0.103, val_acc:0.974]
Epoch [93/120    avg_loss:0.094, val_acc:0.976]
Epoch [94/120    avg_loss:0.102, val_acc:0.972]
Epoch [95/120    avg_loss:0.085, val_acc:0.974]
Epoch [96/120    avg_loss:0.093, val_acc:0.970]
Epoch [97/120    avg_loss:0.097, val_acc:0.976]
Epoch [98/120    avg_loss:0.103, val_acc:0.974]
Epoch [99/120    avg_loss:0.086, val_acc:0.974]
Epoch [100/120    avg_loss:0.109, val_acc:0.972]
Epoch [101/120    avg_loss:0.085, val_acc:0.976]
Epoch [102/120    avg_loss:0.100, val_acc:0.974]
Epoch [103/120    avg_loss:0.112, val_acc:0.976]
Epoch [104/120    avg_loss:0.095, val_acc:0.978]
Epoch [105/120    avg_loss:0.102, val_acc:0.978]
Epoch [106/120    avg_loss:0.090, val_acc:0.980]
Epoch [107/120    avg_loss:0.093, val_acc:0.976]
Epoch [108/120    avg_loss:0.084, val_acc:0.978]
Epoch [109/120    avg_loss:0.087, val_acc:0.970]
Epoch [110/120    avg_loss:0.093, val_acc:0.974]
Epoch [111/120    avg_loss:0.109, val_acc:0.976]
Epoch [112/120    avg_loss:0.098, val_acc:0.976]
Epoch [113/120    avg_loss:0.084, val_acc:0.978]
Epoch [114/120    avg_loss:0.094, val_acc:0.978]
Epoch [115/120    avg_loss:0.082, val_acc:0.980]
Epoch [116/120    avg_loss:0.076, val_acc:0.982]
Epoch [117/120    avg_loss:0.099, val_acc:0.980]
Epoch [118/120    avg_loss:0.083, val_acc:0.978]
Epoch [119/120    avg_loss:0.077, val_acc:0.976]
Epoch [120/120    avg_loss:0.087, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 207   0   0   0   0  12   0   0   0   0   0   0]
 [  0   0   1 212  15   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 206  20   0   0   0   0   0   0   1   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.25159914712154

F1 scores:
[       nan 1.         0.94520548 0.95927602 0.88034188 0.86206897
 1.         0.87830688 0.99742931 1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9805343650352696
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f004d161908>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.585, val_acc:0.340]
Epoch [2/120    avg_loss:2.293, val_acc:0.458]
Epoch [3/120    avg_loss:2.087, val_acc:0.527]
Epoch [4/120    avg_loss:1.916, val_acc:0.606]
Epoch [5/120    avg_loss:1.767, val_acc:0.629]
Epoch [6/120    avg_loss:1.652, val_acc:0.671]
Epoch [7/120    avg_loss:1.518, val_acc:0.725]
Epoch [8/120    avg_loss:1.385, val_acc:0.767]
Epoch [9/120    avg_loss:1.247, val_acc:0.771]
Epoch [10/120    avg_loss:1.118, val_acc:0.796]
Epoch [11/120    avg_loss:1.027, val_acc:0.829]
Epoch [12/120    avg_loss:0.948, val_acc:0.825]
Epoch [13/120    avg_loss:0.883, val_acc:0.860]
Epoch [14/120    avg_loss:0.813, val_acc:0.829]
Epoch [15/120    avg_loss:0.760, val_acc:0.863]
Epoch [16/120    avg_loss:0.693, val_acc:0.887]
Epoch [17/120    avg_loss:0.641, val_acc:0.869]
Epoch [18/120    avg_loss:0.553, val_acc:0.875]
Epoch [19/120    avg_loss:0.629, val_acc:0.863]
Epoch [20/120    avg_loss:0.587, val_acc:0.877]
Epoch [21/120    avg_loss:0.461, val_acc:0.879]
Epoch [22/120    avg_loss:0.456, val_acc:0.898]
Epoch [23/120    avg_loss:0.424, val_acc:0.910]
Epoch [24/120    avg_loss:0.447, val_acc:0.898]
Epoch [25/120    avg_loss:0.489, val_acc:0.819]
Epoch [26/120    avg_loss:0.460, val_acc:0.894]
Epoch [27/120    avg_loss:0.470, val_acc:0.883]
Epoch [28/120    avg_loss:0.483, val_acc:0.885]
Epoch [29/120    avg_loss:0.394, val_acc:0.890]
Epoch [30/120    avg_loss:0.386, val_acc:0.921]
Epoch [31/120    avg_loss:0.387, val_acc:0.883]
Epoch [32/120    avg_loss:0.392, val_acc:0.915]
Epoch [33/120    avg_loss:0.358, val_acc:0.919]
Epoch [34/120    avg_loss:0.419, val_acc:0.910]
Epoch [35/120    avg_loss:0.344, val_acc:0.933]
Epoch [36/120    avg_loss:0.322, val_acc:0.923]
Epoch [37/120    avg_loss:0.331, val_acc:0.927]
Epoch [38/120    avg_loss:0.350, val_acc:0.900]
Epoch [39/120    avg_loss:0.337, val_acc:0.923]
Epoch [40/120    avg_loss:0.308, val_acc:0.933]
Epoch [41/120    avg_loss:0.345, val_acc:0.912]
Epoch [42/120    avg_loss:0.304, val_acc:0.910]
Epoch [43/120    avg_loss:0.274, val_acc:0.935]
Epoch [44/120    avg_loss:0.258, val_acc:0.950]
Epoch [45/120    avg_loss:0.223, val_acc:0.938]
Epoch [46/120    avg_loss:0.218, val_acc:0.927]
Epoch [47/120    avg_loss:0.207, val_acc:0.940]
Epoch [48/120    avg_loss:0.270, val_acc:0.925]
Epoch [49/120    avg_loss:0.222, val_acc:0.938]
Epoch [50/120    avg_loss:0.274, val_acc:0.931]
Epoch [51/120    avg_loss:0.280, val_acc:0.952]
Epoch [52/120    avg_loss:0.241, val_acc:0.944]
Epoch [53/120    avg_loss:0.228, val_acc:0.933]
Epoch [54/120    avg_loss:0.189, val_acc:0.946]
Epoch [55/120    avg_loss:0.184, val_acc:0.954]
Epoch [56/120    avg_loss:0.162, val_acc:0.956]
Epoch [57/120    avg_loss:0.167, val_acc:0.952]
Epoch [58/120    avg_loss:0.220, val_acc:0.931]
Epoch [59/120    avg_loss:0.193, val_acc:0.931]
Epoch [60/120    avg_loss:0.257, val_acc:0.931]
Epoch [61/120    avg_loss:0.219, val_acc:0.948]
Epoch [62/120    avg_loss:0.205, val_acc:0.927]
Epoch [63/120    avg_loss:0.178, val_acc:0.950]
Epoch [64/120    avg_loss:0.169, val_acc:0.948]
Epoch [65/120    avg_loss:0.168, val_acc:0.948]
Epoch [66/120    avg_loss:0.217, val_acc:0.952]
Epoch [67/120    avg_loss:0.184, val_acc:0.952]
Epoch [68/120    avg_loss:0.149, val_acc:0.952]
Epoch [69/120    avg_loss:0.145, val_acc:0.938]
Epoch [70/120    avg_loss:0.159, val_acc:0.956]
Epoch [71/120    avg_loss:0.098, val_acc:0.960]
Epoch [72/120    avg_loss:0.109, val_acc:0.960]
Epoch [73/120    avg_loss:0.109, val_acc:0.958]
Epoch [74/120    avg_loss:0.093, val_acc:0.956]
Epoch [75/120    avg_loss:0.107, val_acc:0.958]
Epoch [76/120    avg_loss:0.099, val_acc:0.960]
Epoch [77/120    avg_loss:0.098, val_acc:0.958]
Epoch [78/120    avg_loss:0.090, val_acc:0.958]
Epoch [79/120    avg_loss:0.107, val_acc:0.963]
Epoch [80/120    avg_loss:0.090, val_acc:0.963]
Epoch [81/120    avg_loss:0.090, val_acc:0.960]
Epoch [82/120    avg_loss:0.107, val_acc:0.960]
Epoch [83/120    avg_loss:0.092, val_acc:0.963]
Epoch [84/120    avg_loss:0.074, val_acc:0.958]
Epoch [85/120    avg_loss:0.101, val_acc:0.960]
Epoch [86/120    avg_loss:0.083, val_acc:0.963]
Epoch [87/120    avg_loss:0.085, val_acc:0.960]
Epoch [88/120    avg_loss:0.094, val_acc:0.958]
Epoch [89/120    avg_loss:0.102, val_acc:0.958]
Epoch [90/120    avg_loss:0.087, val_acc:0.963]
Epoch [91/120    avg_loss:0.087, val_acc:0.960]
Epoch [92/120    avg_loss:0.070, val_acc:0.960]
Epoch [93/120    avg_loss:0.076, val_acc:0.967]
Epoch [94/120    avg_loss:0.099, val_acc:0.967]
Epoch [95/120    avg_loss:0.078, val_acc:0.967]
Epoch [96/120    avg_loss:0.078, val_acc:0.967]
Epoch [97/120    avg_loss:0.082, val_acc:0.965]
Epoch [98/120    avg_loss:0.076, val_acc:0.969]
Epoch [99/120    avg_loss:0.092, val_acc:0.965]
Epoch [100/120    avg_loss:0.064, val_acc:0.969]
Epoch [101/120    avg_loss:0.081, val_acc:0.963]
Epoch [102/120    avg_loss:0.076, val_acc:0.965]
Epoch [103/120    avg_loss:0.070, val_acc:0.967]
Epoch [104/120    avg_loss:0.067, val_acc:0.969]
Epoch [105/120    avg_loss:0.077, val_acc:0.973]
Epoch [106/120    avg_loss:0.079, val_acc:0.967]
Epoch [107/120    avg_loss:0.073, val_acc:0.971]
Epoch [108/120    avg_loss:0.067, val_acc:0.969]
Epoch [109/120    avg_loss:0.071, val_acc:0.971]
Epoch [110/120    avg_loss:0.071, val_acc:0.971]
Epoch [111/120    avg_loss:0.085, val_acc:0.973]
Epoch [112/120    avg_loss:0.080, val_acc:0.967]
Epoch [113/120    avg_loss:0.086, val_acc:0.971]
Epoch [114/120    avg_loss:0.064, val_acc:0.969]
Epoch [115/120    avg_loss:0.069, val_acc:0.971]
Epoch [116/120    avg_loss:0.074, val_acc:0.971]
Epoch [117/120    avg_loss:0.085, val_acc:0.969]
Epoch [118/120    avg_loss:0.074, val_acc:0.969]
Epoch [119/120    avg_loss:0.066, val_acc:0.967]
Epoch [120/120    avg_loss:0.075, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 214  16   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0  24 121   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.44349680170575

F1 scores:
[       nan 1.         0.97550111 0.96396396 0.87341772 0.84615385
 1.         0.93785311 1.         1.         1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.9826695687558803
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff013e4b8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.585, val_acc:0.286]
Epoch [2/120    avg_loss:2.320, val_acc:0.331]
Epoch [3/120    avg_loss:2.124, val_acc:0.454]
Epoch [4/120    avg_loss:1.933, val_acc:0.504]
Epoch [5/120    avg_loss:1.793, val_acc:0.563]
Epoch [6/120    avg_loss:1.640, val_acc:0.675]
Epoch [7/120    avg_loss:1.519, val_acc:0.716]
Epoch [8/120    avg_loss:1.388, val_acc:0.734]
Epoch [9/120    avg_loss:1.239, val_acc:0.786]
Epoch [10/120    avg_loss:1.139, val_acc:0.790]
Epoch [11/120    avg_loss:1.052, val_acc:0.843]
Epoch [12/120    avg_loss:0.961, val_acc:0.851]
Epoch [13/120    avg_loss:0.894, val_acc:0.819]
Epoch [14/120    avg_loss:0.826, val_acc:0.877]
Epoch [15/120    avg_loss:0.766, val_acc:0.897]
Epoch [16/120    avg_loss:0.727, val_acc:0.893]
Epoch [17/120    avg_loss:0.695, val_acc:0.877]
Epoch [18/120    avg_loss:0.631, val_acc:0.897]
Epoch [19/120    avg_loss:0.625, val_acc:0.911]
Epoch [20/120    avg_loss:0.577, val_acc:0.911]
Epoch [21/120    avg_loss:0.531, val_acc:0.897]
Epoch [22/120    avg_loss:0.520, val_acc:0.929]
Epoch [23/120    avg_loss:0.468, val_acc:0.885]
Epoch [24/120    avg_loss:0.459, val_acc:0.937]
Epoch [25/120    avg_loss:0.368, val_acc:0.931]
Epoch [26/120    avg_loss:0.461, val_acc:0.863]
Epoch [27/120    avg_loss:0.420, val_acc:0.905]
Epoch [28/120    avg_loss:0.421, val_acc:0.895]
Epoch [29/120    avg_loss:0.393, val_acc:0.931]
Epoch [30/120    avg_loss:0.349, val_acc:0.907]
Epoch [31/120    avg_loss:0.403, val_acc:0.940]
Epoch [32/120    avg_loss:0.392, val_acc:0.899]
Epoch [33/120    avg_loss:0.364, val_acc:0.925]
Epoch [34/120    avg_loss:0.385, val_acc:0.925]
Epoch [35/120    avg_loss:0.376, val_acc:0.927]
Epoch [36/120    avg_loss:0.270, val_acc:0.944]
Epoch [37/120    avg_loss:0.333, val_acc:0.891]
Epoch [38/120    avg_loss:0.294, val_acc:0.937]
Epoch [39/120    avg_loss:0.279, val_acc:0.919]
Epoch [40/120    avg_loss:0.277, val_acc:0.944]
Epoch [41/120    avg_loss:0.302, val_acc:0.946]
Epoch [42/120    avg_loss:0.251, val_acc:0.946]
Epoch [43/120    avg_loss:0.220, val_acc:0.950]
Epoch [44/120    avg_loss:0.277, val_acc:0.935]
Epoch [45/120    avg_loss:0.347, val_acc:0.937]
Epoch [46/120    avg_loss:0.290, val_acc:0.923]
Epoch [47/120    avg_loss:0.330, val_acc:0.895]
Epoch [48/120    avg_loss:0.266, val_acc:0.942]
Epoch [49/120    avg_loss:0.205, val_acc:0.940]
Epoch [50/120    avg_loss:0.242, val_acc:0.960]
Epoch [51/120    avg_loss:0.218, val_acc:0.950]
Epoch [52/120    avg_loss:0.196, val_acc:0.958]
Epoch [53/120    avg_loss:0.176, val_acc:0.972]
Epoch [54/120    avg_loss:0.174, val_acc:0.958]
Epoch [55/120    avg_loss:0.169, val_acc:0.962]
Epoch [56/120    avg_loss:0.188, val_acc:0.944]
Epoch [57/120    avg_loss:0.177, val_acc:0.944]
Epoch [58/120    avg_loss:0.200, val_acc:0.946]
Epoch [59/120    avg_loss:0.218, val_acc:0.956]
Epoch [60/120    avg_loss:0.169, val_acc:0.966]
Epoch [61/120    avg_loss:0.137, val_acc:0.972]
Epoch [62/120    avg_loss:0.173, val_acc:0.972]
Epoch [63/120    avg_loss:0.133, val_acc:0.958]
Epoch [64/120    avg_loss:0.159, val_acc:0.946]
Epoch [65/120    avg_loss:0.214, val_acc:0.964]
Epoch [66/120    avg_loss:0.177, val_acc:0.933]
Epoch [67/120    avg_loss:0.147, val_acc:0.948]
Epoch [68/120    avg_loss:0.133, val_acc:0.962]
Epoch [69/120    avg_loss:0.094, val_acc:0.974]
Epoch [70/120    avg_loss:0.119, val_acc:0.978]
Epoch [71/120    avg_loss:0.106, val_acc:0.982]
Epoch [72/120    avg_loss:0.101, val_acc:0.972]
Epoch [73/120    avg_loss:0.093, val_acc:0.972]
Epoch [74/120    avg_loss:0.113, val_acc:0.964]
Epoch [75/120    avg_loss:0.117, val_acc:0.984]
Epoch [76/120    avg_loss:0.078, val_acc:0.976]
Epoch [77/120    avg_loss:0.107, val_acc:0.986]
Epoch [78/120    avg_loss:0.168, val_acc:0.966]
Epoch [79/120    avg_loss:0.084, val_acc:0.974]
Epoch [80/120    avg_loss:0.082, val_acc:0.976]
Epoch [81/120    avg_loss:0.057, val_acc:0.982]
Epoch [82/120    avg_loss:0.079, val_acc:0.974]
Epoch [83/120    avg_loss:0.123, val_acc:0.962]
Epoch [84/120    avg_loss:0.142, val_acc:0.980]
Epoch [85/120    avg_loss:0.109, val_acc:0.986]
Epoch [86/120    avg_loss:0.066, val_acc:0.976]
Epoch [87/120    avg_loss:0.062, val_acc:0.978]
Epoch [88/120    avg_loss:0.072, val_acc:0.980]
Epoch [89/120    avg_loss:0.064, val_acc:0.986]
Epoch [90/120    avg_loss:0.050, val_acc:0.980]
Epoch [91/120    avg_loss:0.066, val_acc:0.984]
Epoch [92/120    avg_loss:0.069, val_acc:0.964]
Epoch [93/120    avg_loss:0.071, val_acc:0.980]
Epoch [94/120    avg_loss:0.042, val_acc:0.980]
Epoch [95/120    avg_loss:0.041, val_acc:0.984]
Epoch [96/120    avg_loss:0.052, val_acc:0.978]
Epoch [97/120    avg_loss:0.049, val_acc:0.937]
Epoch [98/120    avg_loss:0.062, val_acc:0.980]
Epoch [99/120    avg_loss:0.041, val_acc:0.982]
Epoch [100/120    avg_loss:0.033, val_acc:0.986]
Epoch [101/120    avg_loss:0.033, val_acc:0.982]
Epoch [102/120    avg_loss:0.037, val_acc:0.980]
Epoch [103/120    avg_loss:0.037, val_acc:0.982]
Epoch [104/120    avg_loss:0.024, val_acc:0.986]
Epoch [105/120    avg_loss:0.024, val_acc:0.986]
Epoch [106/120    avg_loss:0.023, val_acc:0.984]
Epoch [107/120    avg_loss:0.026, val_acc:0.980]
Epoch [108/120    avg_loss:0.029, val_acc:0.984]
Epoch [109/120    avg_loss:0.037, val_acc:0.992]
Epoch [110/120    avg_loss:0.039, val_acc:0.992]
Epoch [111/120    avg_loss:0.062, val_acc:0.978]
Epoch [112/120    avg_loss:0.044, val_acc:0.982]
Epoch [113/120    avg_loss:0.033, val_acc:0.992]
Epoch [114/120    avg_loss:0.018, val_acc:0.990]
Epoch [115/120    avg_loss:0.014, val_acc:0.988]
Epoch [116/120    avg_loss:0.018, val_acc:0.986]
Epoch [117/120    avg_loss:0.031, val_acc:0.970]
Epoch [118/120    avg_loss:0.030, val_acc:0.990]
Epoch [119/120    avg_loss:0.021, val_acc:0.980]
Epoch [120/120    avg_loss:0.037, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 215   4   0   0   0   3   8   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0  38 107   0   0   0   0   0   0   0   0]
 [  0  13   0   0   0   0 193   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.25159914712154

F1 scores:
[       nan 0.99060014 0.97767857 0.96629213 0.90204082 0.82945736
 0.96741855 0.95555556 0.99356499 0.99152542 1.         1.
 1.         1.        ]

Kappa:
0.9805230102012368
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa14ead5828>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.531, val_acc:0.302]
Epoch [2/120    avg_loss:2.269, val_acc:0.437]
Epoch [3/120    avg_loss:2.120, val_acc:0.518]
Epoch [4/120    avg_loss:1.974, val_acc:0.583]
Epoch [5/120    avg_loss:1.794, val_acc:0.617]
Epoch [6/120    avg_loss:1.646, val_acc:0.683]
Epoch [7/120    avg_loss:1.442, val_acc:0.665]
Epoch [8/120    avg_loss:1.257, val_acc:0.698]
Epoch [9/120    avg_loss:1.171, val_acc:0.700]
Epoch [10/120    avg_loss:1.034, val_acc:0.766]
Epoch [11/120    avg_loss:0.902, val_acc:0.768]
Epoch [12/120    avg_loss:0.843, val_acc:0.857]
Epoch [13/120    avg_loss:0.776, val_acc:0.869]
Epoch [14/120    avg_loss:0.723, val_acc:0.875]
Epoch [15/120    avg_loss:0.674, val_acc:0.800]
Epoch [16/120    avg_loss:0.712, val_acc:0.887]
Epoch [17/120    avg_loss:0.677, val_acc:0.825]
Epoch [18/120    avg_loss:0.554, val_acc:0.895]
Epoch [19/120    avg_loss:0.580, val_acc:0.867]
Epoch [20/120    avg_loss:0.513, val_acc:0.909]
Epoch [21/120    avg_loss:0.445, val_acc:0.913]
Epoch [22/120    avg_loss:0.499, val_acc:0.919]
Epoch [23/120    avg_loss:0.426, val_acc:0.905]
Epoch [24/120    avg_loss:0.424, val_acc:0.915]
Epoch [25/120    avg_loss:0.392, val_acc:0.897]
Epoch [26/120    avg_loss:0.491, val_acc:0.895]
Epoch [27/120    avg_loss:0.432, val_acc:0.871]
Epoch [28/120    avg_loss:0.450, val_acc:0.881]
Epoch [29/120    avg_loss:0.430, val_acc:0.899]
Epoch [30/120    avg_loss:0.400, val_acc:0.915]
Epoch [31/120    avg_loss:0.414, val_acc:0.921]
Epoch [32/120    avg_loss:0.333, val_acc:0.929]
Epoch [33/120    avg_loss:0.325, val_acc:0.913]
Epoch [34/120    avg_loss:0.320, val_acc:0.929]
Epoch [35/120    avg_loss:0.322, val_acc:0.905]
Epoch [36/120    avg_loss:0.368, val_acc:0.927]
Epoch [37/120    avg_loss:0.280, val_acc:0.927]
Epoch [38/120    avg_loss:0.311, val_acc:0.927]
Epoch [39/120    avg_loss:0.222, val_acc:0.940]
Epoch [40/120    avg_loss:0.232, val_acc:0.940]
Epoch [41/120    avg_loss:0.231, val_acc:0.942]
Epoch [42/120    avg_loss:0.263, val_acc:0.917]
Epoch [43/120    avg_loss:0.230, val_acc:0.931]
Epoch [44/120    avg_loss:0.255, val_acc:0.950]
Epoch [45/120    avg_loss:0.255, val_acc:0.940]
Epoch [46/120    avg_loss:0.286, val_acc:0.923]
Epoch [47/120    avg_loss:0.269, val_acc:0.931]
Epoch [48/120    avg_loss:0.237, val_acc:0.931]
Epoch [49/120    avg_loss:0.223, val_acc:0.940]
Epoch [50/120    avg_loss:0.175, val_acc:0.944]
Epoch [51/120    avg_loss:0.200, val_acc:0.942]
Epoch [52/120    avg_loss:0.260, val_acc:0.938]
Epoch [53/120    avg_loss:0.177, val_acc:0.937]
Epoch [54/120    avg_loss:0.226, val_acc:0.927]
Epoch [55/120    avg_loss:0.229, val_acc:0.942]
Epoch [56/120    avg_loss:0.182, val_acc:0.956]
Epoch [57/120    avg_loss:0.167, val_acc:0.946]
Epoch [58/120    avg_loss:0.227, val_acc:0.935]
Epoch [59/120    avg_loss:0.203, val_acc:0.950]
Epoch [60/120    avg_loss:0.188, val_acc:0.940]
Epoch [61/120    avg_loss:0.172, val_acc:0.944]
Epoch [62/120    avg_loss:0.175, val_acc:0.946]
Epoch [63/120    avg_loss:0.173, val_acc:0.927]
Epoch [64/120    avg_loss:0.177, val_acc:0.952]
Epoch [65/120    avg_loss:0.134, val_acc:0.960]
Epoch [66/120    avg_loss:0.120, val_acc:0.950]
Epoch [67/120    avg_loss:0.139, val_acc:0.962]
Epoch [68/120    avg_loss:0.120, val_acc:0.958]
Epoch [69/120    avg_loss:0.130, val_acc:0.931]
Epoch [70/120    avg_loss:0.152, val_acc:0.938]
Epoch [71/120    avg_loss:0.126, val_acc:0.948]
Epoch [72/120    avg_loss:0.121, val_acc:0.954]
Epoch [73/120    avg_loss:0.110, val_acc:0.956]
Epoch [74/120    avg_loss:0.117, val_acc:0.966]
Epoch [75/120    avg_loss:0.129, val_acc:0.956]
Epoch [76/120    avg_loss:0.147, val_acc:0.927]
Epoch [77/120    avg_loss:0.175, val_acc:0.944]
Epoch [78/120    avg_loss:0.170, val_acc:0.948]
Epoch [79/120    avg_loss:0.121, val_acc:0.956]
Epoch [80/120    avg_loss:0.121, val_acc:0.958]
Epoch [81/120    avg_loss:0.082, val_acc:0.960]
Epoch [82/120    avg_loss:0.151, val_acc:0.950]
Epoch [83/120    avg_loss:0.166, val_acc:0.964]
Epoch [84/120    avg_loss:0.116, val_acc:0.966]
Epoch [85/120    avg_loss:0.134, val_acc:0.962]
Epoch [86/120    avg_loss:0.115, val_acc:0.950]
Epoch [87/120    avg_loss:0.135, val_acc:0.950]
Epoch [88/120    avg_loss:0.118, val_acc:0.956]
Epoch [89/120    avg_loss:0.140, val_acc:0.958]
Epoch [90/120    avg_loss:0.112, val_acc:0.964]
Epoch [91/120    avg_loss:0.117, val_acc:0.962]
Epoch [92/120    avg_loss:0.104, val_acc:0.950]
Epoch [93/120    avg_loss:0.103, val_acc:0.962]
Epoch [94/120    avg_loss:0.078, val_acc:0.972]
Epoch [95/120    avg_loss:0.058, val_acc:0.970]
Epoch [96/120    avg_loss:0.064, val_acc:0.958]
Epoch [97/120    avg_loss:0.063, val_acc:0.972]
Epoch [98/120    avg_loss:0.065, val_acc:0.962]
Epoch [99/120    avg_loss:0.074, val_acc:0.948]
Epoch [100/120    avg_loss:0.070, val_acc:0.966]
Epoch [101/120    avg_loss:0.071, val_acc:0.974]
Epoch [102/120    avg_loss:0.071, val_acc:0.966]
Epoch [103/120    avg_loss:0.065, val_acc:0.966]
Epoch [104/120    avg_loss:0.056, val_acc:0.974]
Epoch [105/120    avg_loss:0.042, val_acc:0.962]
Epoch [106/120    avg_loss:0.037, val_acc:0.972]
Epoch [107/120    avg_loss:0.090, val_acc:0.970]
Epoch [108/120    avg_loss:0.061, val_acc:0.978]
Epoch [109/120    avg_loss:0.057, val_acc:0.954]
Epoch [110/120    avg_loss:0.060, val_acc:0.954]
Epoch [111/120    avg_loss:0.083, val_acc:0.956]
Epoch [112/120    avg_loss:0.110, val_acc:0.952]
Epoch [113/120    avg_loss:0.150, val_acc:0.956]
Epoch [114/120    avg_loss:0.096, val_acc:0.972]
Epoch [115/120    avg_loss:0.080, val_acc:0.978]
Epoch [116/120    avg_loss:0.045, val_acc:0.972]
Epoch [117/120    avg_loss:0.059, val_acc:0.970]
Epoch [118/120    avg_loss:0.097, val_acc:0.964]
Epoch [119/120    avg_loss:0.068, val_acc:0.972]
Epoch [120/120    avg_loss:0.049, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 200   0   0   0   0  19   0   0   0   0   0   0]
 [  0   0   4 196  29   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 205  19   1   0   0   0   0   0   2   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.82515991471215

F1 scores:
[       nan 1.         0.92378753 0.92018779 0.85774059 0.87671233
 0.99757869 0.85279188 0.998713   1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9757881099769862
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8973759898>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.513, val_acc:0.286]
Epoch [2/120    avg_loss:2.270, val_acc:0.397]
Epoch [3/120    avg_loss:2.110, val_acc:0.595]
Epoch [4/120    avg_loss:1.934, val_acc:0.581]
Epoch [5/120    avg_loss:1.765, val_acc:0.615]
Epoch [6/120    avg_loss:1.589, val_acc:0.637]
Epoch [7/120    avg_loss:1.408, val_acc:0.651]
Epoch [8/120    avg_loss:1.280, val_acc:0.692]
Epoch [9/120    avg_loss:1.144, val_acc:0.728]
Epoch [10/120    avg_loss:1.022, val_acc:0.758]
Epoch [11/120    avg_loss:0.955, val_acc:0.812]
Epoch [12/120    avg_loss:0.835, val_acc:0.875]
Epoch [13/120    avg_loss:0.750, val_acc:0.883]
Epoch [14/120    avg_loss:0.688, val_acc:0.905]
Epoch [15/120    avg_loss:0.636, val_acc:0.883]
Epoch [16/120    avg_loss:0.672, val_acc:0.917]
Epoch [17/120    avg_loss:0.550, val_acc:0.911]
Epoch [18/120    avg_loss:0.531, val_acc:0.909]
Epoch [19/120    avg_loss:0.523, val_acc:0.927]
Epoch [20/120    avg_loss:0.469, val_acc:0.917]
Epoch [21/120    avg_loss:0.510, val_acc:0.907]
Epoch [22/120    avg_loss:0.468, val_acc:0.927]
Epoch [23/120    avg_loss:0.445, val_acc:0.917]
Epoch [24/120    avg_loss:0.485, val_acc:0.907]
Epoch [25/120    avg_loss:0.499, val_acc:0.863]
Epoch [26/120    avg_loss:0.449, val_acc:0.923]
Epoch [27/120    avg_loss:0.377, val_acc:0.917]
Epoch [28/120    avg_loss:0.392, val_acc:0.933]
Epoch [29/120    avg_loss:0.356, val_acc:0.929]
Epoch [30/120    avg_loss:0.337, val_acc:0.940]
Epoch [31/120    avg_loss:0.300, val_acc:0.933]
Epoch [32/120    avg_loss:0.333, val_acc:0.937]
Epoch [33/120    avg_loss:0.324, val_acc:0.931]
Epoch [34/120    avg_loss:0.294, val_acc:0.938]
Epoch [35/120    avg_loss:0.332, val_acc:0.948]
Epoch [36/120    avg_loss:0.276, val_acc:0.911]
Epoch [37/120    avg_loss:0.244, val_acc:0.948]
Epoch [38/120    avg_loss:0.249, val_acc:0.935]
Epoch [39/120    avg_loss:0.254, val_acc:0.960]
Epoch [40/120    avg_loss:0.218, val_acc:0.950]
Epoch [41/120    avg_loss:0.264, val_acc:0.935]
Epoch [42/120    avg_loss:0.247, val_acc:0.958]
Epoch [43/120    avg_loss:0.199, val_acc:0.952]
Epoch [44/120    avg_loss:0.195, val_acc:0.960]
Epoch [45/120    avg_loss:0.185, val_acc:0.946]
Epoch [46/120    avg_loss:0.236, val_acc:0.935]
Epoch [47/120    avg_loss:0.245, val_acc:0.948]
Epoch [48/120    avg_loss:0.205, val_acc:0.958]
Epoch [49/120    avg_loss:0.219, val_acc:0.960]
Epoch [50/120    avg_loss:0.186, val_acc:0.956]
Epoch [51/120    avg_loss:0.186, val_acc:0.929]
Epoch [52/120    avg_loss:0.163, val_acc:0.952]
Epoch [53/120    avg_loss:0.160, val_acc:0.972]
Epoch [54/120    avg_loss:0.214, val_acc:0.960]
Epoch [55/120    avg_loss:0.176, val_acc:0.952]
Epoch [56/120    avg_loss:0.171, val_acc:0.956]
Epoch [57/120    avg_loss:0.133, val_acc:0.958]
Epoch [58/120    avg_loss:0.150, val_acc:0.968]
Epoch [59/120    avg_loss:0.134, val_acc:0.978]
Epoch [60/120    avg_loss:0.185, val_acc:0.938]
Epoch [61/120    avg_loss:0.242, val_acc:0.946]
Epoch [62/120    avg_loss:0.195, val_acc:0.964]
Epoch [63/120    avg_loss:0.160, val_acc:0.966]
Epoch [64/120    avg_loss:0.103, val_acc:0.974]
Epoch [65/120    avg_loss:0.142, val_acc:0.966]
Epoch [66/120    avg_loss:0.131, val_acc:0.958]
Epoch [67/120    avg_loss:0.134, val_acc:0.978]
Epoch [68/120    avg_loss:0.089, val_acc:0.974]
Epoch [69/120    avg_loss:0.112, val_acc:0.968]
Epoch [70/120    avg_loss:0.115, val_acc:0.966]
Epoch [71/120    avg_loss:0.108, val_acc:0.980]
Epoch [72/120    avg_loss:0.078, val_acc:0.976]
Epoch [73/120    avg_loss:0.074, val_acc:0.976]
Epoch [74/120    avg_loss:0.089, val_acc:0.976]
Epoch [75/120    avg_loss:0.081, val_acc:0.962]
Epoch [76/120    avg_loss:0.088, val_acc:0.974]
Epoch [77/120    avg_loss:0.066, val_acc:0.980]
Epoch [78/120    avg_loss:0.066, val_acc:0.980]
Epoch [79/120    avg_loss:0.101, val_acc:0.974]
Epoch [80/120    avg_loss:0.081, val_acc:0.972]
Epoch [81/120    avg_loss:0.072, val_acc:0.976]
Epoch [82/120    avg_loss:0.068, val_acc:0.982]
Epoch [83/120    avg_loss:0.059, val_acc:0.986]
Epoch [84/120    avg_loss:0.054, val_acc:0.980]
Epoch [85/120    avg_loss:0.060, val_acc:0.978]
Epoch [86/120    avg_loss:0.112, val_acc:0.952]
Epoch [87/120    avg_loss:0.099, val_acc:0.984]
Epoch [88/120    avg_loss:0.057, val_acc:0.982]
Epoch [89/120    avg_loss:0.054, val_acc:0.988]
Epoch [90/120    avg_loss:0.038, val_acc:0.980]
Epoch [91/120    avg_loss:0.057, val_acc:0.978]
Epoch [92/120    avg_loss:0.043, val_acc:0.984]
Epoch [93/120    avg_loss:0.038, val_acc:0.988]
Epoch [94/120    avg_loss:0.048, val_acc:0.954]
Epoch [95/120    avg_loss:0.090, val_acc:0.980]
Epoch [96/120    avg_loss:0.041, val_acc:0.982]
Epoch [97/120    avg_loss:0.026, val_acc:0.986]
Epoch [98/120    avg_loss:0.032, val_acc:0.988]
Epoch [99/120    avg_loss:0.026, val_acc:0.990]
Epoch [100/120    avg_loss:0.045, val_acc:0.978]
Epoch [101/120    avg_loss:0.037, val_acc:0.982]
Epoch [102/120    avg_loss:0.035, val_acc:0.984]
Epoch [103/120    avg_loss:0.028, val_acc:0.988]
Epoch [104/120    avg_loss:0.031, val_acc:0.986]
Epoch [105/120    avg_loss:0.058, val_acc:0.980]
Epoch [106/120    avg_loss:0.031, val_acc:0.982]
Epoch [107/120    avg_loss:0.044, val_acc:0.982]
Epoch [108/120    avg_loss:0.064, val_acc:0.974]
Epoch [109/120    avg_loss:0.053, val_acc:0.982]
Epoch [110/120    avg_loss:0.082, val_acc:0.982]
Epoch [111/120    avg_loss:0.060, val_acc:0.988]
Epoch [112/120    avg_loss:0.024, val_acc:0.978]
Epoch [113/120    avg_loss:0.037, val_acc:0.980]
Epoch [114/120    avg_loss:0.035, val_acc:0.984]
Epoch [115/120    avg_loss:0.020, val_acc:0.984]
Epoch [116/120    avg_loss:0.021, val_acc:0.986]
Epoch [117/120    avg_loss:0.022, val_acc:0.986]
Epoch [118/120    avg_loss:0.024, val_acc:0.986]
Epoch [119/120    avg_loss:0.018, val_acc:0.986]
Epoch [120/120    avg_loss:0.017, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 217  10   0   0   0   2   1   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7   0 199   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  11 442   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.4861407249467

F1 scores:
[       nan 1.         0.98871332 0.97091723 0.8907563  0.87719298
 0.98271605 0.9726776  0.99742931 0.99893276 1.         0.98562092
 0.9877095  1.        ]

Kappa:
0.9831451745159533
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f187a344cf8>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.574, val_acc:0.308]
Epoch [2/120    avg_loss:2.269, val_acc:0.349]
Epoch [3/120    avg_loss:2.070, val_acc:0.581]
Epoch [4/120    avg_loss:1.880, val_acc:0.603]
Epoch [5/120    avg_loss:1.669, val_acc:0.613]
Epoch [6/120    avg_loss:1.471, val_acc:0.722]
Epoch [7/120    avg_loss:1.321, val_acc:0.788]
Epoch [8/120    avg_loss:1.157, val_acc:0.821]
Epoch [9/120    avg_loss:1.037, val_acc:0.837]
Epoch [10/120    avg_loss:0.924, val_acc:0.889]
Epoch [11/120    avg_loss:0.829, val_acc:0.879]
Epoch [12/120    avg_loss:0.762, val_acc:0.861]
Epoch [13/120    avg_loss:0.703, val_acc:0.833]
Epoch [14/120    avg_loss:0.666, val_acc:0.851]
Epoch [15/120    avg_loss:0.595, val_acc:0.901]
Epoch [16/120    avg_loss:0.635, val_acc:0.810]
Epoch [17/120    avg_loss:0.540, val_acc:0.873]
Epoch [18/120    avg_loss:0.521, val_acc:0.899]
Epoch [19/120    avg_loss:0.457, val_acc:0.881]
Epoch [20/120    avg_loss:0.425, val_acc:0.917]
Epoch [21/120    avg_loss:0.417, val_acc:0.901]
Epoch [22/120    avg_loss:0.508, val_acc:0.897]
Epoch [23/120    avg_loss:0.467, val_acc:0.903]
Epoch [24/120    avg_loss:0.402, val_acc:0.915]
Epoch [25/120    avg_loss:0.382, val_acc:0.883]
Epoch [26/120    avg_loss:0.386, val_acc:0.869]
Epoch [27/120    avg_loss:0.370, val_acc:0.913]
Epoch [28/120    avg_loss:0.325, val_acc:0.938]
Epoch [29/120    avg_loss:0.278, val_acc:0.913]
Epoch [30/120    avg_loss:0.295, val_acc:0.907]
Epoch [31/120    avg_loss:0.341, val_acc:0.899]
Epoch [32/120    avg_loss:0.298, val_acc:0.929]
Epoch [33/120    avg_loss:0.270, val_acc:0.952]
Epoch [34/120    avg_loss:0.245, val_acc:0.933]
Epoch [35/120    avg_loss:0.197, val_acc:0.933]
Epoch [36/120    avg_loss:0.258, val_acc:0.933]
Epoch [37/120    avg_loss:0.252, val_acc:0.944]
Epoch [38/120    avg_loss:0.249, val_acc:0.937]
Epoch [39/120    avg_loss:0.210, val_acc:0.946]
Epoch [40/120    avg_loss:0.236, val_acc:0.946]
Epoch [41/120    avg_loss:0.214, val_acc:0.954]
Epoch [42/120    avg_loss:0.254, val_acc:0.929]
Epoch [43/120    avg_loss:0.255, val_acc:0.937]
Epoch [44/120    avg_loss:0.218, val_acc:0.942]
Epoch [45/120    avg_loss:0.213, val_acc:0.938]
Epoch [46/120    avg_loss:0.189, val_acc:0.950]
Epoch [47/120    avg_loss:0.232, val_acc:0.950]
Epoch [48/120    avg_loss:0.196, val_acc:0.952]
Epoch [49/120    avg_loss:0.136, val_acc:0.952]
Epoch [50/120    avg_loss:0.161, val_acc:0.938]
Epoch [51/120    avg_loss:0.181, val_acc:0.935]
Epoch [52/120    avg_loss:0.185, val_acc:0.946]
Epoch [53/120    avg_loss:0.156, val_acc:0.944]
Epoch [54/120    avg_loss:0.131, val_acc:0.954]
Epoch [55/120    avg_loss:0.108, val_acc:0.960]
Epoch [56/120    avg_loss:0.141, val_acc:0.958]
Epoch [57/120    avg_loss:0.166, val_acc:0.958]
Epoch [58/120    avg_loss:0.156, val_acc:0.954]
Epoch [59/120    avg_loss:0.202, val_acc:0.937]
Epoch [60/120    avg_loss:0.162, val_acc:0.929]
Epoch [61/120    avg_loss:0.169, val_acc:0.921]
Epoch [62/120    avg_loss:0.165, val_acc:0.960]
Epoch [63/120    avg_loss:0.149, val_acc:0.948]
Epoch [64/120    avg_loss:0.151, val_acc:0.954]
Epoch [65/120    avg_loss:0.119, val_acc:0.940]
Epoch [66/120    avg_loss:0.102, val_acc:0.966]
Epoch [67/120    avg_loss:0.118, val_acc:0.946]
Epoch [68/120    avg_loss:0.102, val_acc:0.960]
Epoch [69/120    avg_loss:0.090, val_acc:0.966]
Epoch [70/120    avg_loss:0.146, val_acc:0.958]
Epoch [71/120    avg_loss:0.115, val_acc:0.972]
Epoch [72/120    avg_loss:0.116, val_acc:0.964]
Epoch [73/120    avg_loss:0.091, val_acc:0.968]
Epoch [74/120    avg_loss:0.097, val_acc:0.962]
Epoch [75/120    avg_loss:0.076, val_acc:0.960]
Epoch [76/120    avg_loss:0.069, val_acc:0.974]
Epoch [77/120    avg_loss:0.077, val_acc:0.958]
Epoch [78/120    avg_loss:0.090, val_acc:0.960]
Epoch [79/120    avg_loss:0.092, val_acc:0.954]
Epoch [80/120    avg_loss:0.083, val_acc:0.958]
Epoch [81/120    avg_loss:0.094, val_acc:0.972]
Epoch [82/120    avg_loss:0.047, val_acc:0.968]
Epoch [83/120    avg_loss:0.082, val_acc:0.972]
Epoch [84/120    avg_loss:0.123, val_acc:0.937]
Epoch [85/120    avg_loss:0.099, val_acc:0.962]
Epoch [86/120    avg_loss:0.129, val_acc:0.942]
Epoch [87/120    avg_loss:0.100, val_acc:0.952]
Epoch [88/120    avg_loss:0.090, val_acc:0.958]
Epoch [89/120    avg_loss:0.091, val_acc:0.970]
Epoch [90/120    avg_loss:0.049, val_acc:0.970]
Epoch [91/120    avg_loss:0.054, val_acc:0.974]
Epoch [92/120    avg_loss:0.045, val_acc:0.976]
Epoch [93/120    avg_loss:0.039, val_acc:0.976]
Epoch [94/120    avg_loss:0.044, val_acc:0.976]
Epoch [95/120    avg_loss:0.033, val_acc:0.976]
Epoch [96/120    avg_loss:0.047, val_acc:0.976]
Epoch [97/120    avg_loss:0.045, val_acc:0.972]
Epoch [98/120    avg_loss:0.032, val_acc:0.972]
Epoch [99/120    avg_loss:0.035, val_acc:0.972]
Epoch [100/120    avg_loss:0.035, val_acc:0.974]
Epoch [101/120    avg_loss:0.036, val_acc:0.974]
Epoch [102/120    avg_loss:0.039, val_acc:0.974]
Epoch [103/120    avg_loss:0.039, val_acc:0.976]
Epoch [104/120    avg_loss:0.030, val_acc:0.976]
Epoch [105/120    avg_loss:0.030, val_acc:0.976]
Epoch [106/120    avg_loss:0.036, val_acc:0.976]
Epoch [107/120    avg_loss:0.029, val_acc:0.976]
Epoch [108/120    avg_loss:0.031, val_acc:0.976]
Epoch [109/120    avg_loss:0.029, val_acc:0.976]
Epoch [110/120    avg_loss:0.043, val_acc:0.976]
Epoch [111/120    avg_loss:0.039, val_acc:0.978]
Epoch [112/120    avg_loss:0.027, val_acc:0.978]
Epoch [113/120    avg_loss:0.026, val_acc:0.974]
Epoch [114/120    avg_loss:0.031, val_acc:0.974]
Epoch [115/120    avg_loss:0.036, val_acc:0.974]
Epoch [116/120    avg_loss:0.029, val_acc:0.976]
Epoch [117/120    avg_loss:0.025, val_acc:0.974]
Epoch [118/120    avg_loss:0.027, val_acc:0.974]
Epoch [119/120    avg_loss:0.027, val_acc:0.974]
Epoch [120/120    avg_loss:0.022, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 215  11   0   0   0   0   3   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 1.         0.98648649 0.96629213 0.90105263 0.89122807
 0.98771499 0.9726776  1.         0.99680511 1.         1.
 1.         1.        ]

Kappa:
0.9867054205883752
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb28ffda898>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.536, val_acc:0.391]
Epoch [2/120    avg_loss:2.288, val_acc:0.450]
Epoch [3/120    avg_loss:2.121, val_acc:0.544]
Epoch [4/120    avg_loss:1.949, val_acc:0.587]
Epoch [5/120    avg_loss:1.777, val_acc:0.625]
Epoch [6/120    avg_loss:1.598, val_acc:0.641]
Epoch [7/120    avg_loss:1.428, val_acc:0.698]
Epoch [8/120    avg_loss:1.320, val_acc:0.712]
Epoch [9/120    avg_loss:1.140, val_acc:0.744]
Epoch [10/120    avg_loss:1.033, val_acc:0.738]
Epoch [11/120    avg_loss:0.931, val_acc:0.734]
Epoch [12/120    avg_loss:0.871, val_acc:0.796]
Epoch [13/120    avg_loss:0.858, val_acc:0.774]
Epoch [14/120    avg_loss:0.728, val_acc:0.873]
Epoch [15/120    avg_loss:0.672, val_acc:0.853]
Epoch [16/120    avg_loss:0.614, val_acc:0.839]
Epoch [17/120    avg_loss:0.578, val_acc:0.919]
Epoch [18/120    avg_loss:0.570, val_acc:0.917]
Epoch [19/120    avg_loss:0.512, val_acc:0.944]
Epoch [20/120    avg_loss:0.512, val_acc:0.901]
Epoch [21/120    avg_loss:0.481, val_acc:0.921]
Epoch [22/120    avg_loss:0.579, val_acc:0.913]
Epoch [23/120    avg_loss:0.498, val_acc:0.911]
Epoch [24/120    avg_loss:0.452, val_acc:0.905]
Epoch [25/120    avg_loss:0.362, val_acc:0.940]
Epoch [26/120    avg_loss:0.362, val_acc:0.938]
Epoch [27/120    avg_loss:0.349, val_acc:0.946]
Epoch [28/120    avg_loss:0.400, val_acc:0.895]
Epoch [29/120    avg_loss:0.397, val_acc:0.933]
Epoch [30/120    avg_loss:0.307, val_acc:0.956]
Epoch [31/120    avg_loss:0.310, val_acc:0.946]
Epoch [32/120    avg_loss:0.275, val_acc:0.929]
Epoch [33/120    avg_loss:0.294, val_acc:0.970]
Epoch [34/120    avg_loss:0.281, val_acc:0.938]
Epoch [35/120    avg_loss:0.250, val_acc:0.976]
Epoch [36/120    avg_loss:0.224, val_acc:0.966]
Epoch [37/120    avg_loss:0.220, val_acc:0.950]
Epoch [38/120    avg_loss:0.205, val_acc:0.946]
Epoch [39/120    avg_loss:0.212, val_acc:0.958]
Epoch [40/120    avg_loss:0.198, val_acc:0.954]
Epoch [41/120    avg_loss:0.210, val_acc:0.960]
Epoch [42/120    avg_loss:0.213, val_acc:0.978]
Epoch [43/120    avg_loss:0.211, val_acc:0.948]
Epoch [44/120    avg_loss:0.195, val_acc:0.966]
Epoch [45/120    avg_loss:0.215, val_acc:0.940]
Epoch [46/120    avg_loss:0.211, val_acc:0.942]
Epoch [47/120    avg_loss:0.177, val_acc:0.974]
Epoch [48/120    avg_loss:0.162, val_acc:0.968]
Epoch [49/120    avg_loss:0.134, val_acc:0.980]
Epoch [50/120    avg_loss:0.136, val_acc:0.970]
Epoch [51/120    avg_loss:0.136, val_acc:0.964]
Epoch [52/120    avg_loss:0.126, val_acc:0.972]
Epoch [53/120    avg_loss:0.126, val_acc:0.974]
Epoch [54/120    avg_loss:0.129, val_acc:0.976]
Epoch [55/120    avg_loss:0.110, val_acc:0.980]
Epoch [56/120    avg_loss:0.100, val_acc:0.980]
Epoch [57/120    avg_loss:0.121, val_acc:0.960]
Epoch [58/120    avg_loss:0.128, val_acc:0.964]
Epoch [59/120    avg_loss:0.189, val_acc:0.946]
Epoch [60/120    avg_loss:0.218, val_acc:0.976]
Epoch [61/120    avg_loss:0.169, val_acc:0.950]
Epoch [62/120    avg_loss:0.127, val_acc:0.968]
Epoch [63/120    avg_loss:0.121, val_acc:0.972]
Epoch [64/120    avg_loss:0.088, val_acc:0.968]
Epoch [65/120    avg_loss:0.196, val_acc:0.974]
Epoch [66/120    avg_loss:0.113, val_acc:0.974]
Epoch [67/120    avg_loss:0.105, val_acc:0.978]
Epoch [68/120    avg_loss:0.138, val_acc:0.978]
Epoch [69/120    avg_loss:0.089, val_acc:0.976]
Epoch [70/120    avg_loss:0.082, val_acc:0.986]
Epoch [71/120    avg_loss:0.067, val_acc:0.990]
Epoch [72/120    avg_loss:0.067, val_acc:0.990]
Epoch [73/120    avg_loss:0.067, val_acc:0.988]
Epoch [74/120    avg_loss:0.061, val_acc:0.990]
Epoch [75/120    avg_loss:0.069, val_acc:0.988]
Epoch [76/120    avg_loss:0.056, val_acc:0.990]
Epoch [77/120    avg_loss:0.062, val_acc:0.990]
Epoch [78/120    avg_loss:0.063, val_acc:0.988]
Epoch [79/120    avg_loss:0.064, val_acc:0.988]
Epoch [80/120    avg_loss:0.059, val_acc:0.988]
Epoch [81/120    avg_loss:0.061, val_acc:0.990]
Epoch [82/120    avg_loss:0.067, val_acc:0.986]
Epoch [83/120    avg_loss:0.052, val_acc:0.988]
Epoch [84/120    avg_loss:0.052, val_acc:0.986]
Epoch [85/120    avg_loss:0.055, val_acc:0.988]
Epoch [86/120    avg_loss:0.052, val_acc:0.992]
Epoch [87/120    avg_loss:0.046, val_acc:0.994]
Epoch [88/120    avg_loss:0.049, val_acc:0.990]
Epoch [89/120    avg_loss:0.066, val_acc:0.994]
Epoch [90/120    avg_loss:0.052, val_acc:0.992]
Epoch [91/120    avg_loss:0.045, val_acc:0.990]
Epoch [92/120    avg_loss:0.051, val_acc:0.994]
Epoch [93/120    avg_loss:0.051, val_acc:0.990]
Epoch [94/120    avg_loss:0.041, val_acc:0.992]
Epoch [95/120    avg_loss:0.045, val_acc:0.992]
Epoch [96/120    avg_loss:0.048, val_acc:0.990]
Epoch [97/120    avg_loss:0.048, val_acc:0.990]
Epoch [98/120    avg_loss:0.056, val_acc:0.992]
Epoch [99/120    avg_loss:0.037, val_acc:0.994]
Epoch [100/120    avg_loss:0.052, val_acc:0.990]
Epoch [101/120    avg_loss:0.043, val_acc:0.988]
Epoch [102/120    avg_loss:0.049, val_acc:0.992]
Epoch [103/120    avg_loss:0.053, val_acc:0.990]
Epoch [104/120    avg_loss:0.052, val_acc:0.992]
Epoch [105/120    avg_loss:0.041, val_acc:0.990]
Epoch [106/120    avg_loss:0.063, val_acc:0.992]
Epoch [107/120    avg_loss:0.046, val_acc:0.994]
Epoch [108/120    avg_loss:0.041, val_acc:0.986]
Epoch [109/120    avg_loss:0.041, val_acc:0.990]
Epoch [110/120    avg_loss:0.041, val_acc:0.994]
Epoch [111/120    avg_loss:0.051, val_acc:0.992]
Epoch [112/120    avg_loss:0.037, val_acc:0.992]
Epoch [113/120    avg_loss:0.042, val_acc:0.994]
Epoch [114/120    avg_loss:0.037, val_acc:0.990]
Epoch [115/120    avg_loss:0.048, val_acc:0.990]
Epoch [116/120    avg_loss:0.041, val_acc:0.992]
Epoch [117/120    avg_loss:0.037, val_acc:0.992]
Epoch [118/120    avg_loss:0.041, val_acc:0.994]
Epoch [119/120    avg_loss:0.040, val_acc:0.992]
Epoch [120/120    avg_loss:0.042, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   1 227   1   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 1.         0.96247241 0.99343545 0.93512304 0.90604027
 1.         0.90804598 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9888423272992914
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3949202908>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.546, val_acc:0.417]
Epoch [2/120    avg_loss:2.264, val_acc:0.583]
Epoch [3/120    avg_loss:2.093, val_acc:0.567]
Epoch [4/120    avg_loss:1.934, val_acc:0.567]
Epoch [5/120    avg_loss:1.762, val_acc:0.671]
Epoch [6/120    avg_loss:1.574, val_acc:0.659]
Epoch [7/120    avg_loss:1.377, val_acc:0.700]
Epoch [8/120    avg_loss:1.232, val_acc:0.732]
Epoch [9/120    avg_loss:1.105, val_acc:0.738]
Epoch [10/120    avg_loss:0.987, val_acc:0.802]
Epoch [11/120    avg_loss:0.889, val_acc:0.865]
Epoch [12/120    avg_loss:0.861, val_acc:0.853]
Epoch [13/120    avg_loss:0.740, val_acc:0.827]
Epoch [14/120    avg_loss:0.723, val_acc:0.843]
Epoch [15/120    avg_loss:0.665, val_acc:0.863]
Epoch [16/120    avg_loss:0.650, val_acc:0.871]
Epoch [17/120    avg_loss:0.621, val_acc:0.903]
Epoch [18/120    avg_loss:0.650, val_acc:0.869]
Epoch [19/120    avg_loss:0.641, val_acc:0.853]
Epoch [20/120    avg_loss:0.536, val_acc:0.919]
Epoch [21/120    avg_loss:0.494, val_acc:0.883]
Epoch [22/120    avg_loss:0.467, val_acc:0.897]
Epoch [23/120    avg_loss:0.470, val_acc:0.913]
Epoch [24/120    avg_loss:0.451, val_acc:0.919]
Epoch [25/120    avg_loss:0.443, val_acc:0.915]
Epoch [26/120    avg_loss:0.436, val_acc:0.909]
Epoch [27/120    avg_loss:0.418, val_acc:0.895]
Epoch [28/120    avg_loss:0.391, val_acc:0.921]
Epoch [29/120    avg_loss:0.355, val_acc:0.915]
Epoch [30/120    avg_loss:0.405, val_acc:0.925]
Epoch [31/120    avg_loss:0.396, val_acc:0.942]
Epoch [32/120    avg_loss:0.352, val_acc:0.933]
Epoch [33/120    avg_loss:0.356, val_acc:0.937]
Epoch [34/120    avg_loss:0.324, val_acc:0.938]
Epoch [35/120    avg_loss:0.376, val_acc:0.942]
Epoch [36/120    avg_loss:0.343, val_acc:0.956]
Epoch [37/120    avg_loss:0.291, val_acc:0.944]
Epoch [38/120    avg_loss:0.286, val_acc:0.937]
Epoch [39/120    avg_loss:0.363, val_acc:0.950]
Epoch [40/120    avg_loss:0.331, val_acc:0.907]
Epoch [41/120    avg_loss:0.280, val_acc:0.937]
Epoch [42/120    avg_loss:0.339, val_acc:0.935]
Epoch [43/120    avg_loss:0.262, val_acc:0.937]
Epoch [44/120    avg_loss:0.221, val_acc:0.948]
Epoch [45/120    avg_loss:0.243, val_acc:0.956]
Epoch [46/120    avg_loss:0.232, val_acc:0.937]
Epoch [47/120    avg_loss:0.283, val_acc:0.897]
Epoch [48/120    avg_loss:0.437, val_acc:0.935]
Epoch [49/120    avg_loss:0.248, val_acc:0.946]
Epoch [50/120    avg_loss:0.273, val_acc:0.938]
Epoch [51/120    avg_loss:0.257, val_acc:0.935]
Epoch [52/120    avg_loss:0.255, val_acc:0.935]
Epoch [53/120    avg_loss:0.208, val_acc:0.954]
Epoch [54/120    avg_loss:0.219, val_acc:0.948]
Epoch [55/120    avg_loss:0.208, val_acc:0.948]
Epoch [56/120    avg_loss:0.167, val_acc:0.978]
Epoch [57/120    avg_loss:0.168, val_acc:0.978]
Epoch [58/120    avg_loss:0.141, val_acc:0.978]
Epoch [59/120    avg_loss:0.142, val_acc:0.972]
Epoch [60/120    avg_loss:0.131, val_acc:0.980]
Epoch [61/120    avg_loss:0.136, val_acc:0.972]
Epoch [62/120    avg_loss:0.150, val_acc:0.958]
Epoch [63/120    avg_loss:0.128, val_acc:0.976]
Epoch [64/120    avg_loss:0.120, val_acc:0.964]
Epoch [65/120    avg_loss:0.144, val_acc:0.982]
Epoch [66/120    avg_loss:0.204, val_acc:0.972]
Epoch [67/120    avg_loss:0.148, val_acc:0.978]
Epoch [68/120    avg_loss:0.110, val_acc:0.960]
Epoch [69/120    avg_loss:0.180, val_acc:0.964]
Epoch [70/120    avg_loss:0.170, val_acc:0.978]
Epoch [71/120    avg_loss:0.117, val_acc:0.978]
Epoch [72/120    avg_loss:0.079, val_acc:0.980]
Epoch [73/120    avg_loss:0.094, val_acc:0.982]
Epoch [74/120    avg_loss:0.093, val_acc:0.972]
Epoch [75/120    avg_loss:0.113, val_acc:0.982]
Epoch [76/120    avg_loss:0.093, val_acc:0.976]
Epoch [77/120    avg_loss:0.068, val_acc:0.982]
Epoch [78/120    avg_loss:0.083, val_acc:0.974]
Epoch [79/120    avg_loss:0.134, val_acc:0.956]
Epoch [80/120    avg_loss:0.146, val_acc:0.958]
Epoch [81/120    avg_loss:0.079, val_acc:0.982]
Epoch [82/120    avg_loss:0.074, val_acc:0.976]
Epoch [83/120    avg_loss:0.050, val_acc:0.982]
Epoch [84/120    avg_loss:0.070, val_acc:0.978]
Epoch [85/120    avg_loss:0.055, val_acc:0.986]
Epoch [86/120    avg_loss:0.056, val_acc:0.980]
Epoch [87/120    avg_loss:0.072, val_acc:0.982]
Epoch [88/120    avg_loss:0.064, val_acc:0.986]
Epoch [89/120    avg_loss:0.085, val_acc:0.984]
Epoch [90/120    avg_loss:0.079, val_acc:0.996]
Epoch [91/120    avg_loss:0.069, val_acc:0.986]
Epoch [92/120    avg_loss:0.052, val_acc:0.986]
Epoch [93/120    avg_loss:0.059, val_acc:0.968]
Epoch [94/120    avg_loss:0.049, val_acc:0.988]
Epoch [95/120    avg_loss:0.050, val_acc:0.984]
Epoch [96/120    avg_loss:0.041, val_acc:0.986]
Epoch [97/120    avg_loss:0.050, val_acc:0.990]
Epoch [98/120    avg_loss:0.054, val_acc:0.986]
Epoch [99/120    avg_loss:0.085, val_acc:0.980]
Epoch [100/120    avg_loss:0.038, val_acc:0.988]
Epoch [101/120    avg_loss:0.040, val_acc:0.988]
Epoch [102/120    avg_loss:0.034, val_acc:0.982]
Epoch [103/120    avg_loss:0.036, val_acc:0.990]
Epoch [104/120    avg_loss:0.029, val_acc:0.986]
Epoch [105/120    avg_loss:0.028, val_acc:0.986]
Epoch [106/120    avg_loss:0.024, val_acc:0.988]
Epoch [107/120    avg_loss:0.026, val_acc:0.988]
Epoch [108/120    avg_loss:0.020, val_acc:0.986]
Epoch [109/120    avg_loss:0.028, val_acc:0.990]
Epoch [110/120    avg_loss:0.023, val_acc:0.986]
Epoch [111/120    avg_loss:0.020, val_acc:0.986]
Epoch [112/120    avg_loss:0.026, val_acc:0.990]
Epoch [113/120    avg_loss:0.022, val_acc:0.990]
Epoch [114/120    avg_loss:0.019, val_acc:0.988]
Epoch [115/120    avg_loss:0.020, val_acc:0.988]
Epoch [116/120    avg_loss:0.022, val_acc:0.986]
Epoch [117/120    avg_loss:0.023, val_acc:0.988]
Epoch [118/120    avg_loss:0.023, val_acc:0.988]
Epoch [119/120    avg_loss:0.019, val_acc:0.988]
Epoch [120/120    avg_loss:0.024, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   3 208  17   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   4  12 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   1   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 1.         0.98648649 0.94117647 0.91525424 0.90526316
 1.         0.98378378 0.99742931 1.         0.99862826 1.
 0.99889503 1.        ]

Kappa:
0.9874181842845999
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2ad6b3a940>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.582, val_acc:0.391]
Epoch [2/120    avg_loss:2.292, val_acc:0.393]
Epoch [3/120    avg_loss:2.076, val_acc:0.480]
Epoch [4/120    avg_loss:1.881, val_acc:0.581]
Epoch [5/120    avg_loss:1.733, val_acc:0.637]
Epoch [6/120    avg_loss:1.588, val_acc:0.665]
Epoch [7/120    avg_loss:1.462, val_acc:0.623]
Epoch [8/120    avg_loss:1.367, val_acc:0.667]
Epoch [9/120    avg_loss:1.229, val_acc:0.710]
Epoch [10/120    avg_loss:1.130, val_acc:0.815]
Epoch [11/120    avg_loss:1.030, val_acc:0.738]
Epoch [12/120    avg_loss:0.923, val_acc:0.861]
Epoch [13/120    avg_loss:0.845, val_acc:0.835]
Epoch [14/120    avg_loss:0.740, val_acc:0.847]
Epoch [15/120    avg_loss:0.749, val_acc:0.859]
Epoch [16/120    avg_loss:0.733, val_acc:0.825]
Epoch [17/120    avg_loss:0.655, val_acc:0.911]
Epoch [18/120    avg_loss:0.632, val_acc:0.889]
Epoch [19/120    avg_loss:0.630, val_acc:0.889]
Epoch [20/120    avg_loss:0.512, val_acc:0.907]
Epoch [21/120    avg_loss:0.533, val_acc:0.933]
Epoch [22/120    avg_loss:0.492, val_acc:0.921]
Epoch [23/120    avg_loss:0.460, val_acc:0.925]
Epoch [24/120    avg_loss:0.393, val_acc:0.911]
Epoch [25/120    avg_loss:0.378, val_acc:0.913]
Epoch [26/120    avg_loss:0.379, val_acc:0.899]
Epoch [27/120    avg_loss:0.457, val_acc:0.937]
Epoch [28/120    avg_loss:0.370, val_acc:0.927]
Epoch [29/120    avg_loss:0.389, val_acc:0.929]
Epoch [30/120    avg_loss:0.344, val_acc:0.923]
Epoch [31/120    avg_loss:0.333, val_acc:0.921]
Epoch [32/120    avg_loss:0.337, val_acc:0.931]
Epoch [33/120    avg_loss:0.300, val_acc:0.915]
Epoch [34/120    avg_loss:0.316, val_acc:0.911]
Epoch [35/120    avg_loss:0.358, val_acc:0.937]
Epoch [36/120    avg_loss:0.280, val_acc:0.865]
Epoch [37/120    avg_loss:0.340, val_acc:0.915]
Epoch [38/120    avg_loss:0.279, val_acc:0.938]
Epoch [39/120    avg_loss:0.321, val_acc:0.923]
Epoch [40/120    avg_loss:0.325, val_acc:0.946]
Epoch [41/120    avg_loss:0.318, val_acc:0.907]
Epoch [42/120    avg_loss:0.305, val_acc:0.944]
Epoch [43/120    avg_loss:0.239, val_acc:0.907]
Epoch [44/120    avg_loss:0.239, val_acc:0.948]
Epoch [45/120    avg_loss:0.212, val_acc:0.942]
Epoch [46/120    avg_loss:0.242, val_acc:0.952]
Epoch [47/120    avg_loss:0.229, val_acc:0.935]
Epoch [48/120    avg_loss:0.222, val_acc:0.954]
Epoch [49/120    avg_loss:0.209, val_acc:0.901]
Epoch [50/120    avg_loss:0.243, val_acc:0.925]
Epoch [51/120    avg_loss:0.221, val_acc:0.952]
Epoch [52/120    avg_loss:0.204, val_acc:0.950]
Epoch [53/120    avg_loss:0.161, val_acc:0.954]
Epoch [54/120    avg_loss:0.164, val_acc:0.956]
Epoch [55/120    avg_loss:0.183, val_acc:0.956]
Epoch [56/120    avg_loss:0.232, val_acc:0.940]
Epoch [57/120    avg_loss:0.196, val_acc:0.954]
Epoch [58/120    avg_loss:0.206, val_acc:0.940]
Epoch [59/120    avg_loss:0.172, val_acc:0.962]
Epoch [60/120    avg_loss:0.121, val_acc:0.964]
Epoch [61/120    avg_loss:0.126, val_acc:0.960]
Epoch [62/120    avg_loss:0.157, val_acc:0.942]
Epoch [63/120    avg_loss:0.182, val_acc:0.935]
Epoch [64/120    avg_loss:0.130, val_acc:0.962]
Epoch [65/120    avg_loss:0.118, val_acc:0.952]
Epoch [66/120    avg_loss:0.120, val_acc:0.958]
Epoch [67/120    avg_loss:0.152, val_acc:0.938]
Epoch [68/120    avg_loss:0.132, val_acc:0.938]
Epoch [69/120    avg_loss:0.128, val_acc:0.968]
Epoch [70/120    avg_loss:0.095, val_acc:0.958]
Epoch [71/120    avg_loss:0.144, val_acc:0.952]
Epoch [72/120    avg_loss:0.135, val_acc:0.956]
Epoch [73/120    avg_loss:0.103, val_acc:0.954]
Epoch [74/120    avg_loss:0.098, val_acc:0.954]
Epoch [75/120    avg_loss:0.087, val_acc:0.964]
Epoch [76/120    avg_loss:0.102, val_acc:0.970]
Epoch [77/120    avg_loss:0.076, val_acc:0.974]
Epoch [78/120    avg_loss:0.089, val_acc:0.962]
Epoch [79/120    avg_loss:0.141, val_acc:0.942]
Epoch [80/120    avg_loss:0.191, val_acc:0.962]
Epoch [81/120    avg_loss:0.153, val_acc:0.950]
Epoch [82/120    avg_loss:0.133, val_acc:0.954]
Epoch [83/120    avg_loss:0.119, val_acc:0.972]
Epoch [84/120    avg_loss:0.101, val_acc:0.956]
Epoch [85/120    avg_loss:0.095, val_acc:0.958]
Epoch [86/120    avg_loss:0.098, val_acc:0.966]
Epoch [87/120    avg_loss:0.091, val_acc:0.958]
Epoch [88/120    avg_loss:0.084, val_acc:0.962]
Epoch [89/120    avg_loss:0.082, val_acc:0.972]
Epoch [90/120    avg_loss:0.078, val_acc:0.964]
Epoch [91/120    avg_loss:0.065, val_acc:0.972]
Epoch [92/120    avg_loss:0.049, val_acc:0.974]
Epoch [93/120    avg_loss:0.039, val_acc:0.974]
Epoch [94/120    avg_loss:0.041, val_acc:0.974]
Epoch [95/120    avg_loss:0.037, val_acc:0.972]
Epoch [96/120    avg_loss:0.035, val_acc:0.974]
Epoch [97/120    avg_loss:0.042, val_acc:0.976]
Epoch [98/120    avg_loss:0.046, val_acc:0.978]
Epoch [99/120    avg_loss:0.035, val_acc:0.976]
Epoch [100/120    avg_loss:0.046, val_acc:0.980]
Epoch [101/120    avg_loss:0.034, val_acc:0.980]
Epoch [102/120    avg_loss:0.040, val_acc:0.980]
Epoch [103/120    avg_loss:0.051, val_acc:0.976]
Epoch [104/120    avg_loss:0.036, val_acc:0.976]
Epoch [105/120    avg_loss:0.037, val_acc:0.974]
Epoch [106/120    avg_loss:0.040, val_acc:0.976]
Epoch [107/120    avg_loss:0.045, val_acc:0.978]
Epoch [108/120    avg_loss:0.040, val_acc:0.980]
Epoch [109/120    avg_loss:0.036, val_acc:0.980]
Epoch [110/120    avg_loss:0.051, val_acc:0.980]
Epoch [111/120    avg_loss:0.038, val_acc:0.980]
Epoch [112/120    avg_loss:0.051, val_acc:0.982]
Epoch [113/120    avg_loss:0.030, val_acc:0.982]
Epoch [114/120    avg_loss:0.040, val_acc:0.982]
Epoch [115/120    avg_loss:0.041, val_acc:0.980]
Epoch [116/120    avg_loss:0.037, val_acc:0.980]
Epoch [117/120    avg_loss:0.031, val_acc:0.982]
Epoch [118/120    avg_loss:0.031, val_acc:0.980]
Epoch [119/120    avg_loss:0.036, val_acc:0.978]
Epoch [120/120    avg_loss:0.030, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   6 211  13   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   1   0   0   2 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.72068230277185

F1 scores:
[       nan 0.997815   0.95594714 0.9569161  0.92640693 0.92881356
 0.99266504 0.92134831 0.998713   1.         1.         0.9973545
 0.99667774 1.        ]

Kappa:
0.9857557032235694
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:08:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f29efa7d860>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.539, val_acc:0.431]
Epoch [2/120    avg_loss:2.246, val_acc:0.494]
Epoch [3/120    avg_loss:2.011, val_acc:0.554]
Epoch [4/120    avg_loss:1.812, val_acc:0.627]
Epoch [5/120    avg_loss:1.613, val_acc:0.692]
Epoch [6/120    avg_loss:1.437, val_acc:0.700]
Epoch [7/120    avg_loss:1.274, val_acc:0.722]
Epoch [8/120    avg_loss:1.114, val_acc:0.726]
Epoch [9/120    avg_loss:1.012, val_acc:0.750]
Epoch [10/120    avg_loss:0.911, val_acc:0.772]
Epoch [11/120    avg_loss:0.868, val_acc:0.772]
Epoch [12/120    avg_loss:0.803, val_acc:0.796]
Epoch [13/120    avg_loss:0.704, val_acc:0.893]
Epoch [14/120    avg_loss:0.675, val_acc:0.867]
Epoch [15/120    avg_loss:0.619, val_acc:0.815]
Epoch [16/120    avg_loss:0.602, val_acc:0.873]
Epoch [17/120    avg_loss:0.596, val_acc:0.877]
Epoch [18/120    avg_loss:0.581, val_acc:0.901]
Epoch [19/120    avg_loss:0.475, val_acc:0.907]
Epoch [20/120    avg_loss:0.463, val_acc:0.911]
Epoch [21/120    avg_loss:0.449, val_acc:0.907]
Epoch [22/120    avg_loss:0.425, val_acc:0.879]
Epoch [23/120    avg_loss:0.400, val_acc:0.915]
Epoch [24/120    avg_loss:0.412, val_acc:0.921]
Epoch [25/120    avg_loss:0.348, val_acc:0.923]
Epoch [26/120    avg_loss:0.390, val_acc:0.915]
Epoch [27/120    avg_loss:0.341, val_acc:0.913]
Epoch [28/120    avg_loss:0.359, val_acc:0.931]
Epoch [29/120    avg_loss:0.323, val_acc:0.952]
Epoch [30/120    avg_loss:0.313, val_acc:0.925]
Epoch [31/120    avg_loss:0.296, val_acc:0.944]
Epoch [32/120    avg_loss:0.354, val_acc:0.921]
Epoch [33/120    avg_loss:0.298, val_acc:0.942]
Epoch [34/120    avg_loss:0.298, val_acc:0.946]
Epoch [35/120    avg_loss:0.264, val_acc:0.958]
Epoch [36/120    avg_loss:0.352, val_acc:0.940]
Epoch [37/120    avg_loss:0.319, val_acc:0.942]
Epoch [38/120    avg_loss:0.289, val_acc:0.935]
Epoch [39/120    avg_loss:0.248, val_acc:0.948]
Epoch [40/120    avg_loss:0.248, val_acc:0.948]
Epoch [41/120    avg_loss:0.212, val_acc:0.952]
Epoch [42/120    avg_loss:0.234, val_acc:0.913]
Epoch [43/120    avg_loss:0.290, val_acc:0.938]
Epoch [44/120    avg_loss:0.227, val_acc:0.940]
Epoch [45/120    avg_loss:0.244, val_acc:0.954]
Epoch [46/120    avg_loss:0.242, val_acc:0.946]
Epoch [47/120    avg_loss:0.201, val_acc:0.950]
Epoch [48/120    avg_loss:0.170, val_acc:0.944]
Epoch [49/120    avg_loss:0.150, val_acc:0.970]
Epoch [50/120    avg_loss:0.124, val_acc:0.974]
Epoch [51/120    avg_loss:0.114, val_acc:0.976]
Epoch [52/120    avg_loss:0.137, val_acc:0.978]
Epoch [53/120    avg_loss:0.114, val_acc:0.978]
Epoch [54/120    avg_loss:0.127, val_acc:0.978]
Epoch [55/120    avg_loss:0.125, val_acc:0.980]
Epoch [56/120    avg_loss:0.111, val_acc:0.980]
Epoch [57/120    avg_loss:0.121, val_acc:0.978]
Epoch [58/120    avg_loss:0.112, val_acc:0.978]
Epoch [59/120    avg_loss:0.128, val_acc:0.982]
Epoch [60/120    avg_loss:0.121, val_acc:0.982]
Epoch [61/120    avg_loss:0.120, val_acc:0.982]
Epoch [62/120    avg_loss:0.109, val_acc:0.984]
Epoch [63/120    avg_loss:0.114, val_acc:0.980]
Epoch [64/120    avg_loss:0.106, val_acc:0.978]
Epoch [65/120    avg_loss:0.099, val_acc:0.980]
Epoch [66/120    avg_loss:0.103, val_acc:0.984]
Epoch [67/120    avg_loss:0.097, val_acc:0.984]
Epoch [68/120    avg_loss:0.101, val_acc:0.984]
Epoch [69/120    avg_loss:0.091, val_acc:0.982]
Epoch [70/120    avg_loss:0.123, val_acc:0.978]
Epoch [71/120    avg_loss:0.090, val_acc:0.982]
Epoch [72/120    avg_loss:0.103, val_acc:0.982]
Epoch [73/120    avg_loss:0.096, val_acc:0.984]
Epoch [74/120    avg_loss:0.104, val_acc:0.984]
Epoch [75/120    avg_loss:0.099, val_acc:0.984]
Epoch [76/120    avg_loss:0.099, val_acc:0.982]
Epoch [77/120    avg_loss:0.104, val_acc:0.982]
Epoch [78/120    avg_loss:0.085, val_acc:0.982]
Epoch [79/120    avg_loss:0.092, val_acc:0.984]
Epoch [80/120    avg_loss:0.109, val_acc:0.982]
Epoch [81/120    avg_loss:0.090, val_acc:0.984]
Epoch [82/120    avg_loss:0.078, val_acc:0.984]
Epoch [83/120    avg_loss:0.107, val_acc:0.982]
Epoch [84/120    avg_loss:0.087, val_acc:0.982]
Epoch [85/120    avg_loss:0.085, val_acc:0.984]
Epoch [86/120    avg_loss:0.087, val_acc:0.984]
Epoch [87/120    avg_loss:0.083, val_acc:0.980]
Epoch [88/120    avg_loss:0.100, val_acc:0.982]
Epoch [89/120    avg_loss:0.086, val_acc:0.982]
Epoch [90/120    avg_loss:0.094, val_acc:0.982]
Epoch [91/120    avg_loss:0.084, val_acc:0.982]
Epoch [92/120    avg_loss:0.086, val_acc:0.984]
Epoch [93/120    avg_loss:0.076, val_acc:0.986]
Epoch [94/120    avg_loss:0.081, val_acc:0.984]
Epoch [95/120    avg_loss:0.091, val_acc:0.984]
Epoch [96/120    avg_loss:0.085, val_acc:0.984]
Epoch [97/120    avg_loss:0.083, val_acc:0.982]
Epoch [98/120    avg_loss:0.095, val_acc:0.980]
Epoch [99/120    avg_loss:0.084, val_acc:0.984]
Epoch [100/120    avg_loss:0.071, val_acc:0.980]
Epoch [101/120    avg_loss:0.080, val_acc:0.984]
Epoch [102/120    avg_loss:0.083, val_acc:0.982]
Epoch [103/120    avg_loss:0.089, val_acc:0.984]
Epoch [104/120    avg_loss:0.083, val_acc:0.984]
Epoch [105/120    avg_loss:0.085, val_acc:0.986]
Epoch [106/120    avg_loss:0.095, val_acc:0.986]
Epoch [107/120    avg_loss:0.079, val_acc:0.984]
Epoch [108/120    avg_loss:0.078, val_acc:0.984]
Epoch [109/120    avg_loss:0.086, val_acc:0.978]
Epoch [110/120    avg_loss:0.070, val_acc:0.986]
Epoch [111/120    avg_loss:0.076, val_acc:0.986]
Epoch [112/120    avg_loss:0.088, val_acc:0.982]
Epoch [113/120    avg_loss:0.081, val_acc:0.982]
Epoch [114/120    avg_loss:0.074, val_acc:0.986]
Epoch [115/120    avg_loss:0.074, val_acc:0.986]
Epoch [116/120    avg_loss:0.074, val_acc:0.986]
Epoch [117/120    avg_loss:0.077, val_acc:0.986]
Epoch [118/120    avg_loss:0.079, val_acc:0.986]
Epoch [119/120    avg_loss:0.060, val_acc:0.986]
Epoch [120/120    avg_loss:0.066, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   2 228   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.95522388059702

F1 scores:
[       nan 1.         0.95343681 0.99563319 0.93607306 0.90849673
 1.         0.89265537 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9883682845773252
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f33c98148d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.481, val_acc:0.339]
Epoch [2/120    avg_loss:2.250, val_acc:0.448]
Epoch [3/120    avg_loss:2.109, val_acc:0.548]
Epoch [4/120    avg_loss:1.962, val_acc:0.567]
Epoch [5/120    avg_loss:1.796, val_acc:0.528]
Epoch [6/120    avg_loss:1.625, val_acc:0.591]
Epoch [7/120    avg_loss:1.474, val_acc:0.651]
Epoch [8/120    avg_loss:1.379, val_acc:0.698]
Epoch [9/120    avg_loss:1.283, val_acc:0.714]
Epoch [10/120    avg_loss:1.155, val_acc:0.724]
Epoch [11/120    avg_loss:1.054, val_acc:0.724]
Epoch [12/120    avg_loss:0.981, val_acc:0.780]
Epoch [13/120    avg_loss:0.909, val_acc:0.833]
Epoch [14/120    avg_loss:0.861, val_acc:0.857]
Epoch [15/120    avg_loss:0.786, val_acc:0.869]
Epoch [16/120    avg_loss:0.733, val_acc:0.861]
Epoch [17/120    avg_loss:0.684, val_acc:0.792]
Epoch [18/120    avg_loss:0.665, val_acc:0.845]
Epoch [19/120    avg_loss:0.620, val_acc:0.863]
Epoch [20/120    avg_loss:0.593, val_acc:0.893]
Epoch [21/120    avg_loss:0.511, val_acc:0.923]
Epoch [22/120    avg_loss:0.511, val_acc:0.911]
Epoch [23/120    avg_loss:0.470, val_acc:0.839]
Epoch [24/120    avg_loss:0.442, val_acc:0.935]
Epoch [25/120    avg_loss:0.395, val_acc:0.933]
Epoch [26/120    avg_loss:0.444, val_acc:0.937]
Epoch [27/120    avg_loss:0.398, val_acc:0.938]
Epoch [28/120    avg_loss:0.374, val_acc:0.929]
Epoch [29/120    avg_loss:0.379, val_acc:0.946]
Epoch [30/120    avg_loss:0.400, val_acc:0.899]
Epoch [31/120    avg_loss:0.361, val_acc:0.950]
Epoch [32/120    avg_loss:0.335, val_acc:0.948]
Epoch [33/120    avg_loss:0.293, val_acc:0.946]
Epoch [34/120    avg_loss:0.374, val_acc:0.946]
Epoch [35/120    avg_loss:0.269, val_acc:0.935]
Epoch [36/120    avg_loss:0.284, val_acc:0.899]
Epoch [37/120    avg_loss:0.311, val_acc:0.935]
Epoch [38/120    avg_loss:0.270, val_acc:0.942]
Epoch [39/120    avg_loss:0.248, val_acc:0.938]
Epoch [40/120    avg_loss:0.264, val_acc:0.956]
Epoch [41/120    avg_loss:0.270, val_acc:0.935]
Epoch [42/120    avg_loss:0.311, val_acc:0.925]
Epoch [43/120    avg_loss:0.288, val_acc:0.954]
Epoch [44/120    avg_loss:0.269, val_acc:0.950]
Epoch [45/120    avg_loss:0.225, val_acc:0.966]
Epoch [46/120    avg_loss:0.228, val_acc:0.938]
Epoch [47/120    avg_loss:0.265, val_acc:0.946]
Epoch [48/120    avg_loss:0.254, val_acc:0.909]
Epoch [49/120    avg_loss:0.228, val_acc:0.946]
Epoch [50/120    avg_loss:0.239, val_acc:0.966]
Epoch [51/120    avg_loss:0.228, val_acc:0.937]
Epoch [52/120    avg_loss:0.200, val_acc:0.958]
Epoch [53/120    avg_loss:0.191, val_acc:0.927]
Epoch [54/120    avg_loss:0.196, val_acc:0.946]
Epoch [55/120    avg_loss:0.181, val_acc:0.952]
Epoch [56/120    avg_loss:0.188, val_acc:0.964]
Epoch [57/120    avg_loss:0.191, val_acc:0.956]
Epoch [58/120    avg_loss:0.197, val_acc:0.962]
Epoch [59/120    avg_loss:0.166, val_acc:0.954]
Epoch [60/120    avg_loss:0.148, val_acc:0.942]
Epoch [61/120    avg_loss:0.145, val_acc:0.968]
Epoch [62/120    avg_loss:0.120, val_acc:0.935]
Epoch [63/120    avg_loss:0.186, val_acc:0.978]
Epoch [64/120    avg_loss:0.152, val_acc:0.966]
Epoch [65/120    avg_loss:0.159, val_acc:0.974]
Epoch [66/120    avg_loss:0.111, val_acc:0.960]
Epoch [67/120    avg_loss:0.114, val_acc:0.962]
Epoch [68/120    avg_loss:0.159, val_acc:0.950]
Epoch [69/120    avg_loss:0.129, val_acc:0.978]
Epoch [70/120    avg_loss:0.103, val_acc:0.976]
Epoch [71/120    avg_loss:0.127, val_acc:0.948]
Epoch [72/120    avg_loss:0.126, val_acc:0.962]
Epoch [73/120    avg_loss:0.103, val_acc:0.978]
Epoch [74/120    avg_loss:0.101, val_acc:0.980]
Epoch [75/120    avg_loss:0.138, val_acc:0.976]
Epoch [76/120    avg_loss:0.089, val_acc:0.974]
Epoch [77/120    avg_loss:0.082, val_acc:0.974]
Epoch [78/120    avg_loss:0.138, val_acc:0.958]
Epoch [79/120    avg_loss:0.172, val_acc:0.944]
Epoch [80/120    avg_loss:0.157, val_acc:0.966]
Epoch [81/120    avg_loss:0.137, val_acc:0.970]
Epoch [82/120    avg_loss:0.133, val_acc:0.976]
Epoch [83/120    avg_loss:0.121, val_acc:0.976]
Epoch [84/120    avg_loss:0.076, val_acc:0.978]
Epoch [85/120    avg_loss:0.153, val_acc:0.960]
Epoch [86/120    avg_loss:0.103, val_acc:0.984]
Epoch [87/120    avg_loss:0.104, val_acc:0.986]
Epoch [88/120    avg_loss:0.081, val_acc:0.980]
Epoch [89/120    avg_loss:0.088, val_acc:0.980]
Epoch [90/120    avg_loss:0.070, val_acc:0.984]
Epoch [91/120    avg_loss:0.067, val_acc:0.992]
Epoch [92/120    avg_loss:0.081, val_acc:0.978]
Epoch [93/120    avg_loss:0.076, val_acc:0.986]
Epoch [94/120    avg_loss:0.043, val_acc:0.988]
Epoch [95/120    avg_loss:0.055, val_acc:0.990]
Epoch [96/120    avg_loss:0.061, val_acc:0.978]
Epoch [97/120    avg_loss:0.053, val_acc:0.988]
Epoch [98/120    avg_loss:0.036, val_acc:0.986]
Epoch [99/120    avg_loss:0.048, val_acc:0.986]
Epoch [100/120    avg_loss:0.048, val_acc:0.984]
Epoch [101/120    avg_loss:0.045, val_acc:0.992]
Epoch [102/120    avg_loss:0.034, val_acc:0.990]
Epoch [103/120    avg_loss:0.035, val_acc:0.974]
Epoch [104/120    avg_loss:0.047, val_acc:0.988]
Epoch [105/120    avg_loss:0.030, val_acc:0.988]
Epoch [106/120    avg_loss:0.044, val_acc:0.988]
Epoch [107/120    avg_loss:0.081, val_acc:0.980]
Epoch [108/120    avg_loss:0.069, val_acc:0.992]
Epoch [109/120    avg_loss:0.041, val_acc:0.990]
Epoch [110/120    avg_loss:0.039, val_acc:0.990]
Epoch [111/120    avg_loss:0.044, val_acc:0.984]
Epoch [112/120    avg_loss:0.063, val_acc:0.982]
Epoch [113/120    avg_loss:0.103, val_acc:0.984]
Epoch [114/120    avg_loss:0.109, val_acc:0.970]
Epoch [115/120    avg_loss:0.103, val_acc:0.984]
Epoch [116/120    avg_loss:0.094, val_acc:0.992]
Epoch [117/120    avg_loss:0.052, val_acc:0.986]
Epoch [118/120    avg_loss:0.046, val_acc:0.994]
Epoch [119/120    avg_loss:0.045, val_acc:0.992]
Epoch [120/120    avg_loss:0.045, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 228   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 1.         0.98206278 0.99563319 0.94663573 0.92651757
 1.         0.96132597 0.998713   1.         1.         0.99603699
 0.99667774 1.        ]

Kappa:
0.9916921082670886
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3e2d19e8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.564, val_acc:0.294]
Epoch [2/120    avg_loss:2.299, val_acc:0.383]
Epoch [3/120    avg_loss:2.130, val_acc:0.506]
Epoch [4/120    avg_loss:1.957, val_acc:0.583]
Epoch [5/120    avg_loss:1.804, val_acc:0.637]
Epoch [6/120    avg_loss:1.612, val_acc:0.657]
Epoch [7/120    avg_loss:1.440, val_acc:0.669]
Epoch [8/120    avg_loss:1.274, val_acc:0.698]
Epoch [9/120    avg_loss:1.120, val_acc:0.754]
Epoch [10/120    avg_loss:0.998, val_acc:0.762]
Epoch [11/120    avg_loss:0.938, val_acc:0.859]
Epoch [12/120    avg_loss:0.840, val_acc:0.823]
Epoch [13/120    avg_loss:0.757, val_acc:0.806]
Epoch [14/120    avg_loss:0.665, val_acc:0.927]
Epoch [15/120    avg_loss:0.586, val_acc:0.901]
Epoch [16/120    avg_loss:0.536, val_acc:0.917]
Epoch [17/120    avg_loss:0.509, val_acc:0.915]
Epoch [18/120    avg_loss:0.484, val_acc:0.851]
Epoch [19/120    avg_loss:0.458, val_acc:0.905]
Epoch [20/120    avg_loss:0.437, val_acc:0.927]
Epoch [21/120    avg_loss:0.440, val_acc:0.901]
Epoch [22/120    avg_loss:0.378, val_acc:0.940]
Epoch [23/120    avg_loss:0.345, val_acc:0.927]
Epoch [24/120    avg_loss:0.368, val_acc:0.901]
Epoch [25/120    avg_loss:0.385, val_acc:0.944]
Epoch [26/120    avg_loss:0.351, val_acc:0.940]
Epoch [27/120    avg_loss:0.302, val_acc:0.948]
Epoch [28/120    avg_loss:0.357, val_acc:0.942]
Epoch [29/120    avg_loss:0.377, val_acc:0.950]
Epoch [30/120    avg_loss:0.337, val_acc:0.948]
Epoch [31/120    avg_loss:0.260, val_acc:0.950]
Epoch [32/120    avg_loss:0.202, val_acc:0.958]
Epoch [33/120    avg_loss:0.235, val_acc:0.950]
Epoch [34/120    avg_loss:0.275, val_acc:0.950]
Epoch [35/120    avg_loss:0.269, val_acc:0.946]
Epoch [36/120    avg_loss:0.288, val_acc:0.960]
Epoch [37/120    avg_loss:0.178, val_acc:0.968]
Epoch [38/120    avg_loss:0.224, val_acc:0.960]
Epoch [39/120    avg_loss:0.193, val_acc:0.972]
Epoch [40/120    avg_loss:0.203, val_acc:0.962]
Epoch [41/120    avg_loss:0.162, val_acc:0.968]
Epoch [42/120    avg_loss:0.190, val_acc:0.944]
Epoch [43/120    avg_loss:0.250, val_acc:0.970]
Epoch [44/120    avg_loss:0.212, val_acc:0.962]
Epoch [45/120    avg_loss:0.199, val_acc:0.968]
Epoch [46/120    avg_loss:0.146, val_acc:0.970]
Epoch [47/120    avg_loss:0.143, val_acc:0.972]
Epoch [48/120    avg_loss:0.184, val_acc:0.946]
Epoch [49/120    avg_loss:0.185, val_acc:0.970]
Epoch [50/120    avg_loss:0.134, val_acc:0.970]
Epoch [51/120    avg_loss:0.143, val_acc:0.972]
Epoch [52/120    avg_loss:0.180, val_acc:0.970]
Epoch [53/120    avg_loss:0.144, val_acc:0.970]
Epoch [54/120    avg_loss:0.179, val_acc:0.970]
Epoch [55/120    avg_loss:0.174, val_acc:0.972]
Epoch [56/120    avg_loss:0.138, val_acc:0.966]
Epoch [57/120    avg_loss:0.150, val_acc:0.956]
Epoch [58/120    avg_loss:0.107, val_acc:0.978]
Epoch [59/120    avg_loss:0.141, val_acc:0.952]
Epoch [60/120    avg_loss:0.148, val_acc:0.978]
Epoch [61/120    avg_loss:0.120, val_acc:0.980]
Epoch [62/120    avg_loss:0.105, val_acc:0.962]
Epoch [63/120    avg_loss:0.168, val_acc:0.964]
Epoch [64/120    avg_loss:0.150, val_acc:0.962]
Epoch [65/120    avg_loss:0.241, val_acc:0.958]
Epoch [66/120    avg_loss:0.154, val_acc:0.970]
Epoch [67/120    avg_loss:0.142, val_acc:0.964]
Epoch [68/120    avg_loss:0.160, val_acc:0.976]
Epoch [69/120    avg_loss:0.111, val_acc:0.978]
Epoch [70/120    avg_loss:0.113, val_acc:0.982]
Epoch [71/120    avg_loss:0.098, val_acc:0.978]
Epoch [72/120    avg_loss:0.119, val_acc:0.984]
Epoch [73/120    avg_loss:0.124, val_acc:0.942]
Epoch [74/120    avg_loss:0.097, val_acc:0.984]
Epoch [75/120    avg_loss:0.087, val_acc:0.982]
Epoch [76/120    avg_loss:0.102, val_acc:0.984]
Epoch [77/120    avg_loss:0.091, val_acc:0.984]
Epoch [78/120    avg_loss:0.054, val_acc:0.986]
Epoch [79/120    avg_loss:0.051, val_acc:0.982]
Epoch [80/120    avg_loss:0.050, val_acc:0.986]
Epoch [81/120    avg_loss:0.063, val_acc:0.986]
Epoch [82/120    avg_loss:0.071, val_acc:0.978]
Epoch [83/120    avg_loss:0.095, val_acc:0.978]
Epoch [84/120    avg_loss:0.104, val_acc:0.974]
Epoch [85/120    avg_loss:0.071, val_acc:0.980]
Epoch [86/120    avg_loss:0.068, val_acc:0.988]
Epoch [87/120    avg_loss:0.053, val_acc:0.986]
Epoch [88/120    avg_loss:0.046, val_acc:0.990]
Epoch [89/120    avg_loss:0.050, val_acc:0.988]
Epoch [90/120    avg_loss:0.036, val_acc:0.990]
Epoch [91/120    avg_loss:0.052, val_acc:0.988]
Epoch [92/120    avg_loss:0.065, val_acc:0.984]
Epoch [93/120    avg_loss:0.109, val_acc:0.978]
Epoch [94/120    avg_loss:0.067, val_acc:0.986]
Epoch [95/120    avg_loss:0.051, val_acc:0.990]
Epoch [96/120    avg_loss:0.040, val_acc:0.990]
Epoch [97/120    avg_loss:0.046, val_acc:0.988]
Epoch [98/120    avg_loss:0.046, val_acc:0.974]
Epoch [99/120    avg_loss:0.041, val_acc:0.988]
Epoch [100/120    avg_loss:0.036, val_acc:0.990]
Epoch [101/120    avg_loss:0.054, val_acc:0.992]
Epoch [102/120    avg_loss:0.038, val_acc:0.982]
Epoch [103/120    avg_loss:0.042, val_acc:0.990]
Epoch [104/120    avg_loss:0.036, val_acc:0.988]
Epoch [105/120    avg_loss:0.035, val_acc:0.992]
Epoch [106/120    avg_loss:0.028, val_acc:0.990]
Epoch [107/120    avg_loss:0.020, val_acc:0.990]
Epoch [108/120    avg_loss:0.031, val_acc:0.988]
Epoch [109/120    avg_loss:0.048, val_acc:0.986]
Epoch [110/120    avg_loss:0.052, val_acc:0.988]
Epoch [111/120    avg_loss:0.048, val_acc:0.980]
Epoch [112/120    avg_loss:0.046, val_acc:0.986]
Epoch [113/120    avg_loss:0.044, val_acc:0.994]
Epoch [114/120    avg_loss:0.034, val_acc:0.990]
Epoch [115/120    avg_loss:0.030, val_acc:0.990]
Epoch [116/120    avg_loss:0.050, val_acc:0.986]
Epoch [117/120    avg_loss:0.067, val_acc:0.988]
Epoch [118/120    avg_loss:0.024, val_acc:0.990]
Epoch [119/120    avg_loss:0.022, val_acc:0.988]
Epoch [120/120    avg_loss:0.028, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  19   0   0   0   0  75   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  11 442   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.72068230277185

F1 scores:
[       nan 1.         0.95633188 0.99782135 0.93709328 0.8975265
 1.         0.88757396 1.         1.         1.         0.98562092
 0.9877095  1.        ]

Kappa:
0.9857555972302483
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5397af4898>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.589, val_acc:0.272]
Epoch [2/120    avg_loss:2.346, val_acc:0.371]
Epoch [3/120    avg_loss:2.159, val_acc:0.506]
Epoch [4/120    avg_loss:1.984, val_acc:0.563]
Epoch [5/120    avg_loss:1.807, val_acc:0.639]
Epoch [6/120    avg_loss:1.632, val_acc:0.641]
Epoch [7/120    avg_loss:1.429, val_acc:0.651]
Epoch [8/120    avg_loss:1.275, val_acc:0.655]
Epoch [9/120    avg_loss:1.130, val_acc:0.728]
Epoch [10/120    avg_loss:1.006, val_acc:0.728]
Epoch [11/120    avg_loss:0.922, val_acc:0.786]
Epoch [12/120    avg_loss:0.819, val_acc:0.794]
Epoch [13/120    avg_loss:0.810, val_acc:0.853]
Epoch [14/120    avg_loss:0.736, val_acc:0.851]
Epoch [15/120    avg_loss:0.649, val_acc:0.895]
Epoch [16/120    avg_loss:0.582, val_acc:0.891]
Epoch [17/120    avg_loss:0.561, val_acc:0.901]
Epoch [18/120    avg_loss:0.518, val_acc:0.889]
Epoch [19/120    avg_loss:0.517, val_acc:0.905]
Epoch [20/120    avg_loss:0.524, val_acc:0.905]
Epoch [21/120    avg_loss:0.484, val_acc:0.919]
Epoch [22/120    avg_loss:0.449, val_acc:0.917]
Epoch [23/120    avg_loss:0.455, val_acc:0.937]
Epoch [24/120    avg_loss:0.478, val_acc:0.909]
Epoch [25/120    avg_loss:0.397, val_acc:0.925]
Epoch [26/120    avg_loss:0.388, val_acc:0.917]
Epoch [27/120    avg_loss:0.367, val_acc:0.897]
Epoch [28/120    avg_loss:0.415, val_acc:0.871]
Epoch [29/120    avg_loss:0.419, val_acc:0.913]
Epoch [30/120    avg_loss:0.354, val_acc:0.927]
Epoch [31/120    avg_loss:0.397, val_acc:0.919]
Epoch [32/120    avg_loss:0.368, val_acc:0.921]
Epoch [33/120    avg_loss:0.319, val_acc:0.933]
Epoch [34/120    avg_loss:0.309, val_acc:0.925]
Epoch [35/120    avg_loss:0.266, val_acc:0.925]
Epoch [36/120    avg_loss:0.277, val_acc:0.942]
Epoch [37/120    avg_loss:0.235, val_acc:0.940]
Epoch [38/120    avg_loss:0.280, val_acc:0.935]
Epoch [39/120    avg_loss:0.266, val_acc:0.927]
Epoch [40/120    avg_loss:0.231, val_acc:0.950]
Epoch [41/120    avg_loss:0.249, val_acc:0.960]
Epoch [42/120    avg_loss:0.209, val_acc:0.950]
Epoch [43/120    avg_loss:0.254, val_acc:0.907]
Epoch [44/120    avg_loss:0.279, val_acc:0.942]
Epoch [45/120    avg_loss:0.225, val_acc:0.940]
Epoch [46/120    avg_loss:0.265, val_acc:0.927]
Epoch [47/120    avg_loss:0.234, val_acc:0.940]
Epoch [48/120    avg_loss:0.179, val_acc:0.958]
Epoch [49/120    avg_loss:0.156, val_acc:0.952]
Epoch [50/120    avg_loss:0.221, val_acc:0.952]
Epoch [51/120    avg_loss:0.260, val_acc:0.944]
Epoch [52/120    avg_loss:0.188, val_acc:0.954]
Epoch [53/120    avg_loss:0.287, val_acc:0.950]
Epoch [54/120    avg_loss:0.214, val_acc:0.950]
Epoch [55/120    avg_loss:0.192, val_acc:0.964]
Epoch [56/120    avg_loss:0.140, val_acc:0.958]
Epoch [57/120    avg_loss:0.137, val_acc:0.958]
Epoch [58/120    avg_loss:0.123, val_acc:0.956]
Epoch [59/120    avg_loss:0.151, val_acc:0.956]
Epoch [60/120    avg_loss:0.131, val_acc:0.958]
Epoch [61/120    avg_loss:0.109, val_acc:0.960]
Epoch [62/120    avg_loss:0.114, val_acc:0.962]
Epoch [63/120    avg_loss:0.110, val_acc:0.966]
Epoch [64/120    avg_loss:0.121, val_acc:0.968]
Epoch [65/120    avg_loss:0.119, val_acc:0.964]
Epoch [66/120    avg_loss:0.103, val_acc:0.966]
Epoch [67/120    avg_loss:0.119, val_acc:0.966]
Epoch [68/120    avg_loss:0.118, val_acc:0.966]
Epoch [69/120    avg_loss:0.135, val_acc:0.968]
Epoch [70/120    avg_loss:0.112, val_acc:0.968]
Epoch [71/120    avg_loss:0.100, val_acc:0.968]
Epoch [72/120    avg_loss:0.099, val_acc:0.970]
Epoch [73/120    avg_loss:0.098, val_acc:0.972]
Epoch [74/120    avg_loss:0.108, val_acc:0.968]
Epoch [75/120    avg_loss:0.099, val_acc:0.972]
Epoch [76/120    avg_loss:0.097, val_acc:0.968]
Epoch [77/120    avg_loss:0.100, val_acc:0.966]
Epoch [78/120    avg_loss:0.108, val_acc:0.972]
Epoch [79/120    avg_loss:0.083, val_acc:0.974]
Epoch [80/120    avg_loss:0.113, val_acc:0.968]
Epoch [81/120    avg_loss:0.092, val_acc:0.970]
Epoch [82/120    avg_loss:0.096, val_acc:0.972]
Epoch [83/120    avg_loss:0.107, val_acc:0.974]
Epoch [84/120    avg_loss:0.085, val_acc:0.968]
Epoch [85/120    avg_loss:0.099, val_acc:0.970]
Epoch [86/120    avg_loss:0.081, val_acc:0.970]
Epoch [87/120    avg_loss:0.102, val_acc:0.978]
Epoch [88/120    avg_loss:0.097, val_acc:0.968]
Epoch [89/120    avg_loss:0.104, val_acc:0.974]
Epoch [90/120    avg_loss:0.098, val_acc:0.972]
Epoch [91/120    avg_loss:0.097, val_acc:0.970]
Epoch [92/120    avg_loss:0.091, val_acc:0.974]
Epoch [93/120    avg_loss:0.080, val_acc:0.970]
Epoch [94/120    avg_loss:0.093, val_acc:0.972]
Epoch [95/120    avg_loss:0.093, val_acc:0.974]
Epoch [96/120    avg_loss:0.087, val_acc:0.972]
Epoch [97/120    avg_loss:0.084, val_acc:0.970]
Epoch [98/120    avg_loss:0.096, val_acc:0.976]
Epoch [99/120    avg_loss:0.098, val_acc:0.970]
Epoch [100/120    avg_loss:0.081, val_acc:0.972]
Epoch [101/120    avg_loss:0.090, val_acc:0.970]
Epoch [102/120    avg_loss:0.087, val_acc:0.972]
Epoch [103/120    avg_loss:0.078, val_acc:0.972]
Epoch [104/120    avg_loss:0.080, val_acc:0.976]
Epoch [105/120    avg_loss:0.082, val_acc:0.976]
Epoch [106/120    avg_loss:0.079, val_acc:0.976]
Epoch [107/120    avg_loss:0.080, val_acc:0.976]
Epoch [108/120    avg_loss:0.091, val_acc:0.976]
Epoch [109/120    avg_loss:0.087, val_acc:0.972]
Epoch [110/120    avg_loss:0.078, val_acc:0.972]
Epoch [111/120    avg_loss:0.090, val_acc:0.974]
Epoch [112/120    avg_loss:0.086, val_acc:0.974]
Epoch [113/120    avg_loss:0.082, val_acc:0.974]
Epoch [114/120    avg_loss:0.087, val_acc:0.974]
Epoch [115/120    avg_loss:0.085, val_acc:0.974]
Epoch [116/120    avg_loss:0.074, val_acc:0.974]
Epoch [117/120    avg_loss:0.080, val_acc:0.974]
Epoch [118/120    avg_loss:0.074, val_acc:0.974]
Epoch [119/120    avg_loss:0.082, val_acc:0.974]
Epoch [120/120    avg_loss:0.085, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 219   8   0   0   0   3   0   0   0   0   0]
 [  0   0   0   0 203  24   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.4861407249467

F1 scores:
[       nan 0.99927061 0.97117517 0.97550111 0.89427313 0.86577181
 0.99756691 0.92571429 0.99614891 1.         1.         0.99208443
 0.99334812 1.        ]

Kappa:
0.9831445568241892
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9b5091b898>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.571, val_acc:0.302]
Epoch [2/120    avg_loss:2.293, val_acc:0.458]
Epoch [3/120    avg_loss:2.119, val_acc:0.500]
Epoch [4/120    avg_loss:1.949, val_acc:0.524]
Epoch [5/120    avg_loss:1.781, val_acc:0.581]
Epoch [6/120    avg_loss:1.624, val_acc:0.659]
Epoch [7/120    avg_loss:1.476, val_acc:0.665]
Epoch [8/120    avg_loss:1.329, val_acc:0.704]
Epoch [9/120    avg_loss:1.206, val_acc:0.714]
Epoch [10/120    avg_loss:1.119, val_acc:0.716]
Epoch [11/120    avg_loss:1.044, val_acc:0.756]
Epoch [12/120    avg_loss:0.952, val_acc:0.754]
Epoch [13/120    avg_loss:0.856, val_acc:0.794]
Epoch [14/120    avg_loss:0.826, val_acc:0.764]
Epoch [15/120    avg_loss:0.768, val_acc:0.857]
Epoch [16/120    avg_loss:0.762, val_acc:0.786]
Epoch [17/120    avg_loss:0.655, val_acc:0.871]
Epoch [18/120    avg_loss:0.680, val_acc:0.895]
Epoch [19/120    avg_loss:0.607, val_acc:0.885]
Epoch [20/120    avg_loss:0.509, val_acc:0.877]
Epoch [21/120    avg_loss:0.466, val_acc:0.935]
Epoch [22/120    avg_loss:0.471, val_acc:0.869]
Epoch [23/120    avg_loss:0.510, val_acc:0.859]
Epoch [24/120    avg_loss:0.461, val_acc:0.865]
Epoch [25/120    avg_loss:0.524, val_acc:0.921]
Epoch [26/120    avg_loss:0.458, val_acc:0.901]
Epoch [27/120    avg_loss:0.388, val_acc:0.933]
Epoch [28/120    avg_loss:0.396, val_acc:0.927]
Epoch [29/120    avg_loss:0.341, val_acc:0.940]
Epoch [30/120    avg_loss:0.323, val_acc:0.938]
Epoch [31/120    avg_loss:0.314, val_acc:0.950]
Epoch [32/120    avg_loss:0.302, val_acc:0.966]
Epoch [33/120    avg_loss:0.326, val_acc:0.925]
Epoch [34/120    avg_loss:0.267, val_acc:0.923]
Epoch [35/120    avg_loss:0.271, val_acc:0.940]
Epoch [36/120    avg_loss:0.246, val_acc:0.950]
Epoch [37/120    avg_loss:0.272, val_acc:0.956]
Epoch [38/120    avg_loss:0.217, val_acc:0.968]
Epoch [39/120    avg_loss:0.235, val_acc:0.960]
Epoch [40/120    avg_loss:0.193, val_acc:0.970]
Epoch [41/120    avg_loss:0.179, val_acc:0.960]
Epoch [42/120    avg_loss:0.228, val_acc:0.938]
Epoch [43/120    avg_loss:0.205, val_acc:0.948]
Epoch [44/120    avg_loss:0.186, val_acc:0.954]
Epoch [45/120    avg_loss:0.208, val_acc:0.974]
Epoch [46/120    avg_loss:0.142, val_acc:0.970]
Epoch [47/120    avg_loss:0.158, val_acc:0.970]
Epoch [48/120    avg_loss:0.143, val_acc:0.978]
Epoch [49/120    avg_loss:0.148, val_acc:0.984]
Epoch [50/120    avg_loss:0.105, val_acc:0.980]
Epoch [51/120    avg_loss:0.140, val_acc:0.964]
Epoch [52/120    avg_loss:0.120, val_acc:0.978]
Epoch [53/120    avg_loss:0.160, val_acc:0.950]
Epoch [54/120    avg_loss:0.158, val_acc:0.966]
Epoch [55/120    avg_loss:0.165, val_acc:0.968]
Epoch [56/120    avg_loss:0.124, val_acc:0.980]
Epoch [57/120    avg_loss:0.117, val_acc:0.972]
Epoch [58/120    avg_loss:0.189, val_acc:0.954]
Epoch [59/120    avg_loss:0.212, val_acc:0.968]
Epoch [60/120    avg_loss:0.169, val_acc:0.974]
Epoch [61/120    avg_loss:0.166, val_acc:0.966]
Epoch [62/120    avg_loss:0.114, val_acc:0.974]
Epoch [63/120    avg_loss:0.088, val_acc:0.982]
Epoch [64/120    avg_loss:0.120, val_acc:0.988]
Epoch [65/120    avg_loss:0.080, val_acc:0.986]
Epoch [66/120    avg_loss:0.082, val_acc:0.986]
Epoch [67/120    avg_loss:0.071, val_acc:0.986]
Epoch [68/120    avg_loss:0.069, val_acc:0.988]
Epoch [69/120    avg_loss:0.067, val_acc:0.988]
Epoch [70/120    avg_loss:0.077, val_acc:0.986]
Epoch [71/120    avg_loss:0.068, val_acc:0.988]
Epoch [72/120    avg_loss:0.078, val_acc:0.988]
Epoch [73/120    avg_loss:0.062, val_acc:0.986]
Epoch [74/120    avg_loss:0.075, val_acc:0.986]
Epoch [75/120    avg_loss:0.064, val_acc:0.988]
Epoch [76/120    avg_loss:0.062, val_acc:0.988]
Epoch [77/120    avg_loss:0.063, val_acc:0.988]
Epoch [78/120    avg_loss:0.054, val_acc:0.988]
Epoch [79/120    avg_loss:0.053, val_acc:0.986]
Epoch [80/120    avg_loss:0.067, val_acc:0.988]
Epoch [81/120    avg_loss:0.060, val_acc:0.988]
Epoch [82/120    avg_loss:0.057, val_acc:0.988]
Epoch [83/120    avg_loss:0.058, val_acc:0.988]
Epoch [84/120    avg_loss:0.061, val_acc:0.988]
Epoch [85/120    avg_loss:0.060, val_acc:0.988]
Epoch [86/120    avg_loss:0.062, val_acc:0.986]
Epoch [87/120    avg_loss:0.064, val_acc:0.988]
Epoch [88/120    avg_loss:0.054, val_acc:0.988]
Epoch [89/120    avg_loss:0.051, val_acc:0.988]
Epoch [90/120    avg_loss:0.063, val_acc:0.988]
Epoch [91/120    avg_loss:0.066, val_acc:0.988]
Epoch [92/120    avg_loss:0.052, val_acc:0.990]
Epoch [93/120    avg_loss:0.046, val_acc:0.990]
Epoch [94/120    avg_loss:0.057, val_acc:0.988]
Epoch [95/120    avg_loss:0.072, val_acc:0.990]
Epoch [96/120    avg_loss:0.054, val_acc:0.990]
Epoch [97/120    avg_loss:0.045, val_acc:0.990]
Epoch [98/120    avg_loss:0.051, val_acc:0.990]
Epoch [99/120    avg_loss:0.054, val_acc:0.990]
Epoch [100/120    avg_loss:0.059, val_acc:0.992]
Epoch [101/120    avg_loss:0.048, val_acc:0.988]
Epoch [102/120    avg_loss:0.051, val_acc:0.986]
Epoch [103/120    avg_loss:0.057, val_acc:0.988]
Epoch [104/120    avg_loss:0.047, val_acc:0.988]
Epoch [105/120    avg_loss:0.043, val_acc:0.990]
Epoch [106/120    avg_loss:0.048, val_acc:0.992]
Epoch [107/120    avg_loss:0.043, val_acc:0.990]
Epoch [108/120    avg_loss:0.041, val_acc:0.990]
Epoch [109/120    avg_loss:0.053, val_acc:0.992]
Epoch [110/120    avg_loss:0.056, val_acc:0.990]
Epoch [111/120    avg_loss:0.044, val_acc:0.990]
Epoch [112/120    avg_loss:0.044, val_acc:0.988]
Epoch [113/120    avg_loss:0.052, val_acc:0.990]
Epoch [114/120    avg_loss:0.044, val_acc:0.990]
Epoch [115/120    avg_loss:0.043, val_acc:0.990]
Epoch [116/120    avg_loss:0.047, val_acc:0.992]
Epoch [117/120    avg_loss:0.039, val_acc:0.990]
Epoch [118/120    avg_loss:0.048, val_acc:0.990]
Epoch [119/120    avg_loss:0.044, val_acc:0.990]
Epoch [120/120    avg_loss:0.042, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   2 226   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 1.         0.97321429 0.99122807 0.96551724 0.95035461
 1.         0.94444444 1.         1.         1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9931153970013077
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb384183898>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.573, val_acc:0.377]
Epoch [2/120    avg_loss:2.290, val_acc:0.471]
Epoch [3/120    avg_loss:2.092, val_acc:0.542]
Epoch [4/120    avg_loss:1.932, val_acc:0.619]
Epoch [5/120    avg_loss:1.745, val_acc:0.660]
Epoch [6/120    avg_loss:1.547, val_acc:0.671]
Epoch [7/120    avg_loss:1.388, val_acc:0.704]
Epoch [8/120    avg_loss:1.236, val_acc:0.787]
Epoch [9/120    avg_loss:1.101, val_acc:0.817]
Epoch [10/120    avg_loss:1.024, val_acc:0.798]
Epoch [11/120    avg_loss:0.900, val_acc:0.796]
Epoch [12/120    avg_loss:0.841, val_acc:0.869]
Epoch [13/120    avg_loss:0.764, val_acc:0.792]
Epoch [14/120    avg_loss:0.709, val_acc:0.896]
Epoch [15/120    avg_loss:0.664, val_acc:0.894]
Epoch [16/120    avg_loss:0.609, val_acc:0.904]
Epoch [17/120    avg_loss:0.567, val_acc:0.923]
Epoch [18/120    avg_loss:0.505, val_acc:0.919]
Epoch [19/120    avg_loss:0.470, val_acc:0.817]
Epoch [20/120    avg_loss:0.555, val_acc:0.806]
Epoch [21/120    avg_loss:0.504, val_acc:0.925]
Epoch [22/120    avg_loss:0.465, val_acc:0.908]
Epoch [23/120    avg_loss:0.462, val_acc:0.900]
Epoch [24/120    avg_loss:0.365, val_acc:0.927]
Epoch [25/120    avg_loss:0.359, val_acc:0.942]
Epoch [26/120    avg_loss:0.395, val_acc:0.940]
Epoch [27/120    avg_loss:0.344, val_acc:0.946]
Epoch [28/120    avg_loss:0.296, val_acc:0.950]
Epoch [29/120    avg_loss:0.284, val_acc:0.948]
Epoch [30/120    avg_loss:0.307, val_acc:0.935]
Epoch [31/120    avg_loss:0.281, val_acc:0.963]
Epoch [32/120    avg_loss:0.273, val_acc:0.904]
Epoch [33/120    avg_loss:0.286, val_acc:0.904]
Epoch [34/120    avg_loss:0.298, val_acc:0.940]
Epoch [35/120    avg_loss:0.295, val_acc:0.938]
Epoch [36/120    avg_loss:0.241, val_acc:0.963]
Epoch [37/120    avg_loss:0.213, val_acc:0.940]
Epoch [38/120    avg_loss:0.223, val_acc:0.954]
Epoch [39/120    avg_loss:0.240, val_acc:0.948]
Epoch [40/120    avg_loss:0.242, val_acc:0.952]
Epoch [41/120    avg_loss:0.213, val_acc:0.958]
Epoch [42/120    avg_loss:0.194, val_acc:0.956]
Epoch [43/120    avg_loss:0.178, val_acc:0.950]
Epoch [44/120    avg_loss:0.161, val_acc:0.960]
Epoch [45/120    avg_loss:0.181, val_acc:0.948]
Epoch [46/120    avg_loss:0.193, val_acc:0.944]
Epoch [47/120    avg_loss:0.286, val_acc:0.963]
Epoch [48/120    avg_loss:0.212, val_acc:0.960]
Epoch [49/120    avg_loss:0.175, val_acc:0.971]
Epoch [50/120    avg_loss:0.170, val_acc:0.977]
Epoch [51/120    avg_loss:0.174, val_acc:0.975]
Epoch [52/120    avg_loss:0.154, val_acc:0.977]
Epoch [53/120    avg_loss:0.113, val_acc:0.975]
Epoch [54/120    avg_loss:0.118, val_acc:0.973]
Epoch [55/120    avg_loss:0.142, val_acc:0.969]
Epoch [56/120    avg_loss:0.156, val_acc:0.983]
Epoch [57/120    avg_loss:0.143, val_acc:0.971]
Epoch [58/120    avg_loss:0.150, val_acc:0.971]
Epoch [59/120    avg_loss:0.129, val_acc:0.977]
Epoch [60/120    avg_loss:0.149, val_acc:0.971]
Epoch [61/120    avg_loss:0.151, val_acc:0.938]
Epoch [62/120    avg_loss:0.158, val_acc:0.967]
Epoch [63/120    avg_loss:0.173, val_acc:0.965]
Epoch [64/120    avg_loss:0.152, val_acc:0.975]
Epoch [65/120    avg_loss:0.116, val_acc:0.979]
Epoch [66/120    avg_loss:0.103, val_acc:0.969]
Epoch [67/120    avg_loss:0.103, val_acc:0.977]
Epoch [68/120    avg_loss:0.088, val_acc:0.977]
Epoch [69/120    avg_loss:0.095, val_acc:0.973]
Epoch [70/120    avg_loss:0.099, val_acc:0.983]
Epoch [71/120    avg_loss:0.073, val_acc:0.990]
Epoch [72/120    avg_loss:0.060, val_acc:0.990]
Epoch [73/120    avg_loss:0.056, val_acc:0.985]
Epoch [74/120    avg_loss:0.048, val_acc:0.990]
Epoch [75/120    avg_loss:0.050, val_acc:0.988]
Epoch [76/120    avg_loss:0.061, val_acc:0.992]
Epoch [77/120    avg_loss:0.055, val_acc:0.983]
Epoch [78/120    avg_loss:0.050, val_acc:0.990]
Epoch [79/120    avg_loss:0.052, val_acc:0.990]
Epoch [80/120    avg_loss:0.064, val_acc:0.988]
Epoch [81/120    avg_loss:0.062, val_acc:0.988]
Epoch [82/120    avg_loss:0.054, val_acc:0.990]
Epoch [83/120    avg_loss:0.061, val_acc:0.985]
Epoch [84/120    avg_loss:0.056, val_acc:0.988]
Epoch [85/120    avg_loss:0.049, val_acc:0.990]
Epoch [86/120    avg_loss:0.052, val_acc:0.990]
Epoch [87/120    avg_loss:0.050, val_acc:0.988]
Epoch [88/120    avg_loss:0.049, val_acc:0.988]
Epoch [89/120    avg_loss:0.059, val_acc:0.990]
Epoch [90/120    avg_loss:0.052, val_acc:0.990]
Epoch [91/120    avg_loss:0.046, val_acc:0.990]
Epoch [92/120    avg_loss:0.048, val_acc:0.988]
Epoch [93/120    avg_loss:0.054, val_acc:0.988]
Epoch [94/120    avg_loss:0.043, val_acc:0.990]
Epoch [95/120    avg_loss:0.050, val_acc:0.990]
Epoch [96/120    avg_loss:0.053, val_acc:0.988]
Epoch [97/120    avg_loss:0.048, val_acc:0.990]
Epoch [98/120    avg_loss:0.062, val_acc:0.990]
Epoch [99/120    avg_loss:0.050, val_acc:0.988]
Epoch [100/120    avg_loss:0.045, val_acc:0.988]
Epoch [101/120    avg_loss:0.047, val_acc:0.988]
Epoch [102/120    avg_loss:0.051, val_acc:0.988]
Epoch [103/120    avg_loss:0.051, val_acc:0.988]
Epoch [104/120    avg_loss:0.046, val_acc:0.988]
Epoch [105/120    avg_loss:0.057, val_acc:0.988]
Epoch [106/120    avg_loss:0.049, val_acc:0.988]
Epoch [107/120    avg_loss:0.046, val_acc:0.988]
Epoch [108/120    avg_loss:0.047, val_acc:0.988]
Epoch [109/120    avg_loss:0.062, val_acc:0.988]
Epoch [110/120    avg_loss:0.054, val_acc:0.988]
Epoch [111/120    avg_loss:0.047, val_acc:0.988]
Epoch [112/120    avg_loss:0.052, val_acc:0.988]
Epoch [113/120    avg_loss:0.055, val_acc:0.988]
Epoch [114/120    avg_loss:0.058, val_acc:0.988]
Epoch [115/120    avg_loss:0.047, val_acc:0.988]
Epoch [116/120    avg_loss:0.049, val_acc:0.988]
Epoch [117/120    avg_loss:0.060, val_acc:0.988]
Epoch [118/120    avg_loss:0.048, val_acc:0.988]
Epoch [119/120    avg_loss:0.046, val_acc:0.988]
Epoch [120/120    avg_loss:0.047, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 208  19   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0 204   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.84861407249467

F1 scores:
[       nan 1.         0.97333333 1.         0.90829694 0.86111111
 0.99512195 0.93181818 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9871802944837099
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff5a2738940>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.525, val_acc:0.403]
Epoch [2/120    avg_loss:2.272, val_acc:0.508]
Epoch [3/120    avg_loss:2.075, val_acc:0.565]
Epoch [4/120    avg_loss:1.891, val_acc:0.611]
Epoch [5/120    avg_loss:1.669, val_acc:0.681]
Epoch [6/120    avg_loss:1.455, val_acc:0.690]
Epoch [7/120    avg_loss:1.290, val_acc:0.692]
Epoch [8/120    avg_loss:1.172, val_acc:0.730]
Epoch [9/120    avg_loss:1.020, val_acc:0.825]
Epoch [10/120    avg_loss:0.926, val_acc:0.825]
Epoch [11/120    avg_loss:0.821, val_acc:0.841]
Epoch [12/120    avg_loss:0.806, val_acc:0.851]
Epoch [13/120    avg_loss:0.737, val_acc:0.813]
Epoch [14/120    avg_loss:0.675, val_acc:0.867]
Epoch [15/120    avg_loss:0.609, val_acc:0.879]
Epoch [16/120    avg_loss:0.528, val_acc:0.841]
Epoch [17/120    avg_loss:0.512, val_acc:0.911]
Epoch [18/120    avg_loss:0.554, val_acc:0.897]
Epoch [19/120    avg_loss:0.509, val_acc:0.881]
Epoch [20/120    avg_loss:0.462, val_acc:0.839]
Epoch [21/120    avg_loss:0.476, val_acc:0.865]
Epoch [22/120    avg_loss:0.426, val_acc:0.927]
Epoch [23/120    avg_loss:0.416, val_acc:0.917]
Epoch [24/120    avg_loss:0.431, val_acc:0.929]
Epoch [25/120    avg_loss:0.333, val_acc:0.933]
Epoch [26/120    avg_loss:0.317, val_acc:0.923]
Epoch [27/120    avg_loss:0.323, val_acc:0.933]
Epoch [28/120    avg_loss:0.342, val_acc:0.903]
Epoch [29/120    avg_loss:0.340, val_acc:0.921]
Epoch [30/120    avg_loss:0.319, val_acc:0.944]
Epoch [31/120    avg_loss:0.288, val_acc:0.915]
Epoch [32/120    avg_loss:0.376, val_acc:0.933]
Epoch [33/120    avg_loss:0.336, val_acc:0.942]
Epoch [34/120    avg_loss:0.235, val_acc:0.950]
Epoch [35/120    avg_loss:0.220, val_acc:0.944]
Epoch [36/120    avg_loss:0.271, val_acc:0.921]
Epoch [37/120    avg_loss:0.320, val_acc:0.871]
Epoch [38/120    avg_loss:0.277, val_acc:0.958]
Epoch [39/120    avg_loss:0.280, val_acc:0.942]
Epoch [40/120    avg_loss:0.269, val_acc:0.940]
Epoch [41/120    avg_loss:0.176, val_acc:0.960]
Epoch [42/120    avg_loss:0.140, val_acc:0.962]
Epoch [43/120    avg_loss:0.149, val_acc:0.956]
Epoch [44/120    avg_loss:0.170, val_acc:0.950]
Epoch [45/120    avg_loss:0.191, val_acc:0.964]
Epoch [46/120    avg_loss:0.183, val_acc:0.962]
Epoch [47/120    avg_loss:0.186, val_acc:0.940]
Epoch [48/120    avg_loss:0.171, val_acc:0.962]
Epoch [49/120    avg_loss:0.181, val_acc:0.972]
Epoch [50/120    avg_loss:0.115, val_acc:0.968]
Epoch [51/120    avg_loss:0.156, val_acc:0.968]
Epoch [52/120    avg_loss:0.194, val_acc:0.978]
Epoch [53/120    avg_loss:0.114, val_acc:0.976]
Epoch [54/120    avg_loss:0.147, val_acc:0.968]
Epoch [55/120    avg_loss:0.204, val_acc:0.917]
Epoch [56/120    avg_loss:0.181, val_acc:0.938]
Epoch [57/120    avg_loss:0.133, val_acc:0.958]
Epoch [58/120    avg_loss:0.163, val_acc:0.968]
Epoch [59/120    avg_loss:0.100, val_acc:0.986]
Epoch [60/120    avg_loss:0.143, val_acc:0.972]
Epoch [61/120    avg_loss:0.101, val_acc:0.940]
Epoch [62/120    avg_loss:0.133, val_acc:0.972]
Epoch [63/120    avg_loss:0.096, val_acc:0.976]
Epoch [64/120    avg_loss:0.110, val_acc:0.976]
Epoch [65/120    avg_loss:0.143, val_acc:0.958]
Epoch [66/120    avg_loss:0.170, val_acc:0.966]
Epoch [67/120    avg_loss:0.171, val_acc:0.935]
Epoch [68/120    avg_loss:0.155, val_acc:0.964]
Epoch [69/120    avg_loss:0.112, val_acc:0.950]
Epoch [70/120    avg_loss:0.144, val_acc:0.962]
Epoch [71/120    avg_loss:0.114, val_acc:0.988]
Epoch [72/120    avg_loss:0.094, val_acc:0.966]
Epoch [73/120    avg_loss:0.079, val_acc:0.978]
Epoch [74/120    avg_loss:0.093, val_acc:0.986]
Epoch [75/120    avg_loss:0.081, val_acc:0.966]
Epoch [76/120    avg_loss:0.112, val_acc:0.942]
Epoch [77/120    avg_loss:0.124, val_acc:0.986]
Epoch [78/120    avg_loss:0.075, val_acc:0.980]
Epoch [79/120    avg_loss:0.077, val_acc:0.978]
Epoch [80/120    avg_loss:0.076, val_acc:0.938]
Epoch [81/120    avg_loss:0.117, val_acc:0.935]
Epoch [82/120    avg_loss:0.138, val_acc:0.962]
Epoch [83/120    avg_loss:0.128, val_acc:0.982]
Epoch [84/120    avg_loss:0.079, val_acc:0.990]
Epoch [85/120    avg_loss:0.086, val_acc:0.986]
Epoch [86/120    avg_loss:0.077, val_acc:0.986]
Epoch [87/120    avg_loss:0.083, val_acc:0.960]
Epoch [88/120    avg_loss:0.075, val_acc:0.984]
Epoch [89/120    avg_loss:0.047, val_acc:0.988]
Epoch [90/120    avg_loss:0.056, val_acc:0.960]
Epoch [91/120    avg_loss:0.092, val_acc:0.984]
Epoch [92/120    avg_loss:0.064, val_acc:0.974]
Epoch [93/120    avg_loss:0.074, val_acc:0.976]
Epoch [94/120    avg_loss:0.066, val_acc:0.990]
Epoch [95/120    avg_loss:0.053, val_acc:0.988]
Epoch [96/120    avg_loss:0.044, val_acc:0.986]
Epoch [97/120    avg_loss:0.039, val_acc:0.990]
Epoch [98/120    avg_loss:0.045, val_acc:0.988]
Epoch [99/120    avg_loss:0.058, val_acc:0.980]
Epoch [100/120    avg_loss:0.053, val_acc:0.990]
Epoch [101/120    avg_loss:0.052, val_acc:0.990]
Epoch [102/120    avg_loss:0.044, val_acc:0.982]
Epoch [103/120    avg_loss:0.066, val_acc:0.986]
Epoch [104/120    avg_loss:0.042, val_acc:0.982]
Epoch [105/120    avg_loss:0.069, val_acc:0.988]
Epoch [106/120    avg_loss:0.030, val_acc:0.986]
Epoch [107/120    avg_loss:0.032, val_acc:0.992]
Epoch [108/120    avg_loss:0.036, val_acc:0.990]
Epoch [109/120    avg_loss:0.032, val_acc:0.990]
Epoch [110/120    avg_loss:0.024, val_acc:0.988]
Epoch [111/120    avg_loss:0.028, val_acc:0.984]
Epoch [112/120    avg_loss:0.037, val_acc:0.996]
Epoch [113/120    avg_loss:0.042, val_acc:0.992]
Epoch [114/120    avg_loss:0.028, val_acc:0.990]
Epoch [115/120    avg_loss:0.031, val_acc:0.992]
Epoch [116/120    avg_loss:0.032, val_acc:0.992]
Epoch [117/120    avg_loss:0.044, val_acc:0.984]
Epoch [118/120    avg_loss:0.036, val_acc:0.990]
Epoch [119/120    avg_loss:0.031, val_acc:0.992]
Epoch [120/120    avg_loss:0.027, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 1.         0.98206278 0.99782135 0.95730337 0.93645485
 1.         0.95555556 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9933531510528312
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff43cde2898>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.505, val_acc:0.310]
Epoch [2/120    avg_loss:2.284, val_acc:0.484]
Epoch [3/120    avg_loss:2.139, val_acc:0.605]
Epoch [4/120    avg_loss:2.012, val_acc:0.625]
Epoch [5/120    avg_loss:1.857, val_acc:0.653]
Epoch [6/120    avg_loss:1.686, val_acc:0.661]
Epoch [7/120    avg_loss:1.513, val_acc:0.671]
Epoch [8/120    avg_loss:1.337, val_acc:0.688]
Epoch [9/120    avg_loss:1.194, val_acc:0.730]
Epoch [10/120    avg_loss:1.112, val_acc:0.746]
Epoch [11/120    avg_loss:0.985, val_acc:0.752]
Epoch [12/120    avg_loss:0.877, val_acc:0.815]
Epoch [13/120    avg_loss:0.817, val_acc:0.899]
Epoch [14/120    avg_loss:0.705, val_acc:0.907]
Epoch [15/120    avg_loss:0.629, val_acc:0.899]
Epoch [16/120    avg_loss:0.564, val_acc:0.921]
Epoch [17/120    avg_loss:0.559, val_acc:0.877]
Epoch [18/120    avg_loss:0.533, val_acc:0.903]
Epoch [19/120    avg_loss:0.491, val_acc:0.857]
Epoch [20/120    avg_loss:0.472, val_acc:0.919]
Epoch [21/120    avg_loss:0.417, val_acc:0.944]
Epoch [22/120    avg_loss:0.383, val_acc:0.925]
Epoch [23/120    avg_loss:0.414, val_acc:0.921]
Epoch [24/120    avg_loss:0.371, val_acc:0.927]
Epoch [25/120    avg_loss:0.371, val_acc:0.913]
Epoch [26/120    avg_loss:0.405, val_acc:0.935]
Epoch [27/120    avg_loss:0.347, val_acc:0.942]
Epoch [28/120    avg_loss:0.304, val_acc:0.921]
Epoch [29/120    avg_loss:0.298, val_acc:0.956]
Epoch [30/120    avg_loss:0.294, val_acc:0.940]
Epoch [31/120    avg_loss:0.319, val_acc:0.950]
Epoch [32/120    avg_loss:0.300, val_acc:0.962]
Epoch [33/120    avg_loss:0.265, val_acc:0.956]
Epoch [34/120    avg_loss:0.264, val_acc:0.938]
Epoch [35/120    avg_loss:0.266, val_acc:0.952]
Epoch [36/120    avg_loss:0.247, val_acc:0.958]
Epoch [37/120    avg_loss:0.222, val_acc:0.962]
Epoch [38/120    avg_loss:0.191, val_acc:0.944]
Epoch [39/120    avg_loss:0.259, val_acc:0.956]
Epoch [40/120    avg_loss:0.209, val_acc:0.946]
Epoch [41/120    avg_loss:0.215, val_acc:0.960]
Epoch [42/120    avg_loss:0.208, val_acc:0.960]
Epoch [43/120    avg_loss:0.190, val_acc:0.972]
Epoch [44/120    avg_loss:0.163, val_acc:0.978]
Epoch [45/120    avg_loss:0.174, val_acc:0.956]
Epoch [46/120    avg_loss:0.199, val_acc:0.972]
Epoch [47/120    avg_loss:0.184, val_acc:0.966]
Epoch [48/120    avg_loss:0.163, val_acc:0.976]
Epoch [49/120    avg_loss:0.182, val_acc:0.966]
Epoch [50/120    avg_loss:0.188, val_acc:0.940]
Epoch [51/120    avg_loss:0.189, val_acc:0.974]
Epoch [52/120    avg_loss:0.154, val_acc:0.958]
Epoch [53/120    avg_loss:0.200, val_acc:0.976]
Epoch [54/120    avg_loss:0.140, val_acc:0.970]
Epoch [55/120    avg_loss:0.210, val_acc:0.976]
Epoch [56/120    avg_loss:0.130, val_acc:0.968]
Epoch [57/120    avg_loss:0.121, val_acc:0.966]
Epoch [58/120    avg_loss:0.124, val_acc:0.976]
Epoch [59/120    avg_loss:0.086, val_acc:0.984]
Epoch [60/120    avg_loss:0.079, val_acc:0.978]
Epoch [61/120    avg_loss:0.081, val_acc:0.984]
Epoch [62/120    avg_loss:0.073, val_acc:0.986]
Epoch [63/120    avg_loss:0.071, val_acc:0.984]
Epoch [64/120    avg_loss:0.075, val_acc:0.982]
Epoch [65/120    avg_loss:0.075, val_acc:0.980]
Epoch [66/120    avg_loss:0.081, val_acc:0.986]
Epoch [67/120    avg_loss:0.075, val_acc:0.982]
Epoch [68/120    avg_loss:0.071, val_acc:0.980]
Epoch [69/120    avg_loss:0.075, val_acc:0.986]
Epoch [70/120    avg_loss:0.070, val_acc:0.980]
Epoch [71/120    avg_loss:0.076, val_acc:0.986]
Epoch [72/120    avg_loss:0.073, val_acc:0.978]
Epoch [73/120    avg_loss:0.072, val_acc:0.982]
Epoch [74/120    avg_loss:0.068, val_acc:0.984]
Epoch [75/120    avg_loss:0.074, val_acc:0.986]
Epoch [76/120    avg_loss:0.078, val_acc:0.982]
Epoch [77/120    avg_loss:0.068, val_acc:0.976]
Epoch [78/120    avg_loss:0.061, val_acc:0.986]
Epoch [79/120    avg_loss:0.070, val_acc:0.984]
Epoch [80/120    avg_loss:0.069, val_acc:0.986]
Epoch [81/120    avg_loss:0.073, val_acc:0.986]
Epoch [82/120    avg_loss:0.063, val_acc:0.986]
Epoch [83/120    avg_loss:0.073, val_acc:0.986]
Epoch [84/120    avg_loss:0.062, val_acc:0.984]
Epoch [85/120    avg_loss:0.066, val_acc:0.986]
Epoch [86/120    avg_loss:0.064, val_acc:0.986]
Epoch [87/120    avg_loss:0.055, val_acc:0.986]
Epoch [88/120    avg_loss:0.063, val_acc:0.986]
Epoch [89/120    avg_loss:0.058, val_acc:0.984]
Epoch [90/120    avg_loss:0.062, val_acc:0.982]
Epoch [91/120    avg_loss:0.053, val_acc:0.984]
Epoch [92/120    avg_loss:0.064, val_acc:0.984]
Epoch [93/120    avg_loss:0.064, val_acc:0.986]
Epoch [94/120    avg_loss:0.062, val_acc:0.984]
Epoch [95/120    avg_loss:0.052, val_acc:0.984]
Epoch [96/120    avg_loss:0.059, val_acc:0.984]
Epoch [97/120    avg_loss:0.064, val_acc:0.984]
Epoch [98/120    avg_loss:0.058, val_acc:0.982]
Epoch [99/120    avg_loss:0.048, val_acc:0.980]
Epoch [100/120    avg_loss:0.053, val_acc:0.980]
Epoch [101/120    avg_loss:0.051, val_acc:0.986]
Epoch [102/120    avg_loss:0.058, val_acc:0.980]
Epoch [103/120    avg_loss:0.072, val_acc:0.986]
Epoch [104/120    avg_loss:0.058, val_acc:0.986]
Epoch [105/120    avg_loss:0.055, val_acc:0.984]
Epoch [106/120    avg_loss:0.059, val_acc:0.988]
Epoch [107/120    avg_loss:0.057, val_acc:0.986]
Epoch [108/120    avg_loss:0.064, val_acc:0.986]
Epoch [109/120    avg_loss:0.059, val_acc:0.988]
Epoch [110/120    avg_loss:0.062, val_acc:0.980]
Epoch [111/120    avg_loss:0.057, val_acc:0.980]
Epoch [112/120    avg_loss:0.058, val_acc:0.984]
Epoch [113/120    avg_loss:0.056, val_acc:0.980]
Epoch [114/120    avg_loss:0.050, val_acc:0.986]
Epoch [115/120    avg_loss:0.050, val_acc:0.984]
Epoch [116/120    avg_loss:0.051, val_acc:0.986]
Epoch [117/120    avg_loss:0.051, val_acc:0.990]
Epoch [118/120    avg_loss:0.043, val_acc:0.988]
Epoch [119/120    avg_loss:0.055, val_acc:0.984]
Epoch [120/120    avg_loss:0.062, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 1.         0.97333333 0.99782135 0.93002257 0.90066225
 1.         0.93181818 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9897923324001655
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7ad91cc978>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.593, val_acc:0.345]
Epoch [2/120    avg_loss:2.295, val_acc:0.480]
Epoch [3/120    avg_loss:2.068, val_acc:0.520]
Epoch [4/120    avg_loss:1.851, val_acc:0.625]
Epoch [5/120    avg_loss:1.651, val_acc:0.667]
Epoch [6/120    avg_loss:1.459, val_acc:0.690]
Epoch [7/120    avg_loss:1.299, val_acc:0.687]
Epoch [8/120    avg_loss:1.172, val_acc:0.724]
Epoch [9/120    avg_loss:1.043, val_acc:0.744]
Epoch [10/120    avg_loss:0.969, val_acc:0.762]
Epoch [11/120    avg_loss:0.865, val_acc:0.770]
Epoch [12/120    avg_loss:0.828, val_acc:0.831]
Epoch [13/120    avg_loss:0.757, val_acc:0.883]
Epoch [14/120    avg_loss:0.662, val_acc:0.879]
Epoch [15/120    avg_loss:0.633, val_acc:0.839]
Epoch [16/120    avg_loss:0.602, val_acc:0.895]
Epoch [17/120    avg_loss:0.551, val_acc:0.895]
Epoch [18/120    avg_loss:0.556, val_acc:0.893]
Epoch [19/120    avg_loss:0.474, val_acc:0.905]
Epoch [20/120    avg_loss:0.536, val_acc:0.909]
Epoch [21/120    avg_loss:0.510, val_acc:0.879]
Epoch [22/120    avg_loss:0.440, val_acc:0.937]
Epoch [23/120    avg_loss:0.399, val_acc:0.919]
Epoch [24/120    avg_loss:0.399, val_acc:0.911]
Epoch [25/120    avg_loss:0.395, val_acc:0.937]
Epoch [26/120    avg_loss:0.339, val_acc:0.940]
Epoch [27/120    avg_loss:0.297, val_acc:0.956]
Epoch [28/120    avg_loss:0.311, val_acc:0.927]
Epoch [29/120    avg_loss:0.420, val_acc:0.927]
Epoch [30/120    avg_loss:0.341, val_acc:0.974]
Epoch [31/120    avg_loss:0.283, val_acc:0.956]
Epoch [32/120    avg_loss:0.255, val_acc:0.966]
Epoch [33/120    avg_loss:0.248, val_acc:0.948]
Epoch [34/120    avg_loss:0.221, val_acc:0.956]
Epoch [35/120    avg_loss:0.229, val_acc:0.972]
Epoch [36/120    avg_loss:0.220, val_acc:0.966]
Epoch [37/120    avg_loss:0.217, val_acc:0.946]
Epoch [38/120    avg_loss:0.229, val_acc:0.962]
Epoch [39/120    avg_loss:0.259, val_acc:0.859]
Epoch [40/120    avg_loss:0.265, val_acc:0.946]
Epoch [41/120    avg_loss:0.211, val_acc:0.956]
Epoch [42/120    avg_loss:0.182, val_acc:0.978]
Epoch [43/120    avg_loss:0.183, val_acc:0.968]
Epoch [44/120    avg_loss:0.204, val_acc:0.972]
Epoch [45/120    avg_loss:0.211, val_acc:0.960]
Epoch [46/120    avg_loss:0.162, val_acc:0.970]
Epoch [47/120    avg_loss:0.150, val_acc:0.972]
Epoch [48/120    avg_loss:0.189, val_acc:0.980]
Epoch [49/120    avg_loss:0.190, val_acc:0.982]
Epoch [50/120    avg_loss:0.144, val_acc:0.982]
Epoch [51/120    avg_loss:0.130, val_acc:0.980]
Epoch [52/120    avg_loss:0.169, val_acc:0.982]
Epoch [53/120    avg_loss:0.125, val_acc:0.974]
Epoch [54/120    avg_loss:0.150, val_acc:0.982]
Epoch [55/120    avg_loss:0.144, val_acc:0.990]
Epoch [56/120    avg_loss:0.132, val_acc:0.982]
Epoch [57/120    avg_loss:0.139, val_acc:0.972]
Epoch [58/120    avg_loss:0.165, val_acc:0.950]
Epoch [59/120    avg_loss:0.137, val_acc:0.976]
Epoch [60/120    avg_loss:0.103, val_acc:0.986]
Epoch [61/120    avg_loss:0.145, val_acc:0.972]
Epoch [62/120    avg_loss:0.098, val_acc:0.988]
Epoch [63/120    avg_loss:0.078, val_acc:0.988]
Epoch [64/120    avg_loss:0.077, val_acc:0.986]
Epoch [65/120    avg_loss:0.124, val_acc:0.958]
Epoch [66/120    avg_loss:0.138, val_acc:0.988]
Epoch [67/120    avg_loss:0.085, val_acc:0.984]
Epoch [68/120    avg_loss:0.085, val_acc:0.980]
Epoch [69/120    avg_loss:0.082, val_acc:0.988]
Epoch [70/120    avg_loss:0.046, val_acc:0.990]
Epoch [71/120    avg_loss:0.044, val_acc:0.990]
Epoch [72/120    avg_loss:0.053, val_acc:0.990]
Epoch [73/120    avg_loss:0.047, val_acc:0.992]
Epoch [74/120    avg_loss:0.047, val_acc:0.992]
Epoch [75/120    avg_loss:0.044, val_acc:0.992]
Epoch [76/120    avg_loss:0.049, val_acc:0.992]
Epoch [77/120    avg_loss:0.049, val_acc:0.992]
Epoch [78/120    avg_loss:0.043, val_acc:0.990]
Epoch [79/120    avg_loss:0.046, val_acc:0.990]
Epoch [80/120    avg_loss:0.043, val_acc:0.992]
Epoch [81/120    avg_loss:0.037, val_acc:0.990]
Epoch [82/120    avg_loss:0.051, val_acc:0.992]
Epoch [83/120    avg_loss:0.037, val_acc:0.992]
Epoch [84/120    avg_loss:0.042, val_acc:0.992]
Epoch [85/120    avg_loss:0.039, val_acc:0.994]
Epoch [86/120    avg_loss:0.039, val_acc:0.994]
Epoch [87/120    avg_loss:0.038, val_acc:0.992]
Epoch [88/120    avg_loss:0.044, val_acc:0.992]
Epoch [89/120    avg_loss:0.030, val_acc:0.994]
Epoch [90/120    avg_loss:0.043, val_acc:0.992]
Epoch [91/120    avg_loss:0.033, val_acc:0.992]
Epoch [92/120    avg_loss:0.048, val_acc:0.992]
Epoch [93/120    avg_loss:0.032, val_acc:0.992]
Epoch [94/120    avg_loss:0.033, val_acc:0.992]
Epoch [95/120    avg_loss:0.036, val_acc:0.992]
Epoch [96/120    avg_loss:0.035, val_acc:0.992]
Epoch [97/120    avg_loss:0.048, val_acc:0.992]
Epoch [98/120    avg_loss:0.040, val_acc:0.992]
Epoch [99/120    avg_loss:0.031, val_acc:0.992]
Epoch [100/120    avg_loss:0.040, val_acc:0.992]
Epoch [101/120    avg_loss:0.041, val_acc:0.992]
Epoch [102/120    avg_loss:0.038, val_acc:0.992]
Epoch [103/120    avg_loss:0.037, val_acc:0.992]
Epoch [104/120    avg_loss:0.033, val_acc:0.992]
Epoch [105/120    avg_loss:0.037, val_acc:0.992]
Epoch [106/120    avg_loss:0.035, val_acc:0.992]
Epoch [107/120    avg_loss:0.031, val_acc:0.992]
Epoch [108/120    avg_loss:0.042, val_acc:0.992]
Epoch [109/120    avg_loss:0.035, val_acc:0.992]
Epoch [110/120    avg_loss:0.034, val_acc:0.992]
Epoch [111/120    avg_loss:0.035, val_acc:0.992]
Epoch [112/120    avg_loss:0.034, val_acc:0.992]
Epoch [113/120    avg_loss:0.032, val_acc:0.992]
Epoch [114/120    avg_loss:0.043, val_acc:0.992]
Epoch [115/120    avg_loss:0.028, val_acc:0.992]
Epoch [116/120    avg_loss:0.035, val_acc:0.992]
Epoch [117/120    avg_loss:0.032, val_acc:0.992]
Epoch [118/120    avg_loss:0.034, val_acc:0.992]
Epoch [119/120    avg_loss:0.027, val_acc:0.992]
Epoch [120/120    avg_loss:0.040, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   4   0   0   1   0   0   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 1.         0.98426966 0.98901099 0.91759465 0.88963211
 1.         0.95604396 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.989317840481877
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9ff83e5908>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.517, val_acc:0.351]
Epoch [2/120    avg_loss:2.254, val_acc:0.438]
Epoch [3/120    avg_loss:2.085, val_acc:0.619]
Epoch [4/120    avg_loss:1.928, val_acc:0.593]
Epoch [5/120    avg_loss:1.759, val_acc:0.631]
Epoch [6/120    avg_loss:1.565, val_acc:0.673]
Epoch [7/120    avg_loss:1.390, val_acc:0.698]
Epoch [8/120    avg_loss:1.238, val_acc:0.714]
Epoch [9/120    avg_loss:1.146, val_acc:0.746]
Epoch [10/120    avg_loss:0.997, val_acc:0.792]
Epoch [11/120    avg_loss:0.906, val_acc:0.859]
Epoch [12/120    avg_loss:0.840, val_acc:0.804]
Epoch [13/120    avg_loss:0.795, val_acc:0.859]
Epoch [14/120    avg_loss:0.732, val_acc:0.889]
Epoch [15/120    avg_loss:0.663, val_acc:0.812]
Epoch [16/120    avg_loss:0.666, val_acc:0.883]
Epoch [17/120    avg_loss:0.631, val_acc:0.895]
Epoch [18/120    avg_loss:0.524, val_acc:0.911]
Epoch [19/120    avg_loss:0.501, val_acc:0.927]
Epoch [20/120    avg_loss:0.476, val_acc:0.869]
Epoch [21/120    avg_loss:0.504, val_acc:0.917]
Epoch [22/120    avg_loss:0.423, val_acc:0.893]
Epoch [23/120    avg_loss:0.454, val_acc:0.889]
Epoch [24/120    avg_loss:0.412, val_acc:0.942]
Epoch [25/120    avg_loss:0.333, val_acc:0.931]
Epoch [26/120    avg_loss:0.359, val_acc:0.901]
Epoch [27/120    avg_loss:0.404, val_acc:0.917]
Epoch [28/120    avg_loss:0.528, val_acc:0.948]
Epoch [29/120    avg_loss:0.333, val_acc:0.942]
Epoch [30/120    avg_loss:0.345, val_acc:0.925]
Epoch [31/120    avg_loss:0.289, val_acc:0.944]
Epoch [32/120    avg_loss:0.284, val_acc:0.929]
Epoch [33/120    avg_loss:0.328, val_acc:0.948]
Epoch [34/120    avg_loss:0.220, val_acc:0.958]
Epoch [35/120    avg_loss:0.205, val_acc:0.960]
Epoch [36/120    avg_loss:0.357, val_acc:0.938]
Epoch [37/120    avg_loss:0.284, val_acc:0.952]
Epoch [38/120    avg_loss:0.241, val_acc:0.960]
Epoch [39/120    avg_loss:0.292, val_acc:0.944]
Epoch [40/120    avg_loss:0.224, val_acc:0.937]
Epoch [41/120    avg_loss:0.179, val_acc:0.966]
Epoch [42/120    avg_loss:0.175, val_acc:0.968]
Epoch [43/120    avg_loss:0.149, val_acc:0.966]
Epoch [44/120    avg_loss:0.184, val_acc:0.968]
Epoch [45/120    avg_loss:0.157, val_acc:0.937]
Epoch [46/120    avg_loss:0.194, val_acc:0.962]
Epoch [47/120    avg_loss:0.162, val_acc:0.954]
Epoch [48/120    avg_loss:0.132, val_acc:0.966]
Epoch [49/120    avg_loss:0.142, val_acc:0.974]
Epoch [50/120    avg_loss:0.115, val_acc:0.960]
Epoch [51/120    avg_loss:0.122, val_acc:0.966]
Epoch [52/120    avg_loss:0.118, val_acc:0.976]
Epoch [53/120    avg_loss:0.118, val_acc:0.962]
Epoch [54/120    avg_loss:0.105, val_acc:0.974]
Epoch [55/120    avg_loss:0.100, val_acc:0.978]
Epoch [56/120    avg_loss:0.133, val_acc:0.956]
Epoch [57/120    avg_loss:0.141, val_acc:0.972]
Epoch [58/120    avg_loss:0.152, val_acc:0.917]
Epoch [59/120    avg_loss:0.189, val_acc:0.966]
Epoch [60/120    avg_loss:0.106, val_acc:0.940]
Epoch [61/120    avg_loss:0.192, val_acc:0.913]
Epoch [62/120    avg_loss:0.170, val_acc:0.972]
Epoch [63/120    avg_loss:0.103, val_acc:0.964]
Epoch [64/120    avg_loss:0.105, val_acc:0.976]
Epoch [65/120    avg_loss:0.086, val_acc:0.970]
Epoch [66/120    avg_loss:0.098, val_acc:0.982]
Epoch [67/120    avg_loss:0.094, val_acc:0.984]
Epoch [68/120    avg_loss:0.107, val_acc:0.964]
Epoch [69/120    avg_loss:0.088, val_acc:0.974]
Epoch [70/120    avg_loss:0.100, val_acc:0.970]
Epoch [71/120    avg_loss:0.096, val_acc:0.964]
Epoch [72/120    avg_loss:0.082, val_acc:0.972]
Epoch [73/120    avg_loss:0.073, val_acc:0.978]
Epoch [74/120    avg_loss:0.058, val_acc:0.978]
Epoch [75/120    avg_loss:0.080, val_acc:0.980]
Epoch [76/120    avg_loss:0.082, val_acc:0.980]
Epoch [77/120    avg_loss:0.085, val_acc:0.978]
Epoch [78/120    avg_loss:0.084, val_acc:0.968]
Epoch [79/120    avg_loss:0.049, val_acc:0.988]
Epoch [80/120    avg_loss:0.042, val_acc:0.982]
Epoch [81/120    avg_loss:0.043, val_acc:0.984]
Epoch [82/120    avg_loss:0.036, val_acc:0.988]
Epoch [83/120    avg_loss:0.072, val_acc:0.942]
Epoch [84/120    avg_loss:0.172, val_acc:0.980]
Epoch [85/120    avg_loss:0.096, val_acc:0.968]
Epoch [86/120    avg_loss:0.053, val_acc:0.986]
Epoch [87/120    avg_loss:0.091, val_acc:0.978]
Epoch [88/120    avg_loss:0.075, val_acc:0.966]
Epoch [89/120    avg_loss:0.045, val_acc:0.974]
Epoch [90/120    avg_loss:0.045, val_acc:0.980]
Epoch [91/120    avg_loss:0.045, val_acc:0.984]
Epoch [92/120    avg_loss:0.038, val_acc:0.978]
Epoch [93/120    avg_loss:0.027, val_acc:0.980]
Epoch [94/120    avg_loss:0.035, val_acc:0.982]
Epoch [95/120    avg_loss:0.053, val_acc:0.980]
Epoch [96/120    avg_loss:0.035, val_acc:0.978]
Epoch [97/120    avg_loss:0.027, val_acc:0.984]
Epoch [98/120    avg_loss:0.021, val_acc:0.984]
Epoch [99/120    avg_loss:0.026, val_acc:0.986]
Epoch [100/120    avg_loss:0.022, val_acc:0.984]
Epoch [101/120    avg_loss:0.024, val_acc:0.984]
Epoch [102/120    avg_loss:0.021, val_acc:0.986]
Epoch [103/120    avg_loss:0.021, val_acc:0.988]
Epoch [104/120    avg_loss:0.023, val_acc:0.986]
Epoch [105/120    avg_loss:0.021, val_acc:0.986]
Epoch [106/120    avg_loss:0.020, val_acc:0.986]
Epoch [107/120    avg_loss:0.020, val_acc:0.986]
Epoch [108/120    avg_loss:0.019, val_acc:0.988]
Epoch [109/120    avg_loss:0.020, val_acc:0.988]
Epoch [110/120    avg_loss:0.020, val_acc:0.988]
Epoch [111/120    avg_loss:0.021, val_acc:0.988]
Epoch [112/120    avg_loss:0.020, val_acc:0.988]
Epoch [113/120    avg_loss:0.017, val_acc:0.992]
Epoch [114/120    avg_loss:0.018, val_acc:0.990]
Epoch [115/120    avg_loss:0.016, val_acc:0.988]
Epoch [116/120    avg_loss:0.018, val_acc:0.988]
Epoch [117/120    avg_loss:0.019, val_acc:0.990]
Epoch [118/120    avg_loss:0.021, val_acc:0.990]
Epoch [119/120    avg_loss:0.021, val_acc:0.992]
Epoch [120/120    avg_loss:0.019, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   0   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 1.         0.98206278 0.99563319 0.93665158 0.90728477
 1.         0.95555556 1.         0.9978678  1.         1.
 1.         1.        ]

Kappa:
0.99097928008156
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:21
Validation dataloader:21
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9e7df50908>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.538, val_acc:0.315]
Epoch [2/120    avg_loss:2.294, val_acc:0.409]
Epoch [3/120    avg_loss:2.115, val_acc:0.512]
Epoch [4/120    avg_loss:1.949, val_acc:0.536]
Epoch [5/120    avg_loss:1.772, val_acc:0.617]
Epoch [6/120    avg_loss:1.611, val_acc:0.647]
Epoch [7/120    avg_loss:1.444, val_acc:0.669]
Epoch [8/120    avg_loss:1.286, val_acc:0.677]
Epoch [9/120    avg_loss:1.168, val_acc:0.714]
Epoch [10/120    avg_loss:1.072, val_acc:0.744]
Epoch [11/120    avg_loss:0.992, val_acc:0.748]
Epoch [12/120    avg_loss:0.906, val_acc:0.798]
Epoch [13/120    avg_loss:0.852, val_acc:0.845]
Epoch [14/120    avg_loss:0.802, val_acc:0.837]
Epoch [15/120    avg_loss:0.740, val_acc:0.788]
Epoch [16/120    avg_loss:0.657, val_acc:0.881]
Epoch [17/120    avg_loss:0.607, val_acc:0.895]
Epoch [18/120    avg_loss:0.593, val_acc:0.895]
Epoch [19/120    avg_loss:0.536, val_acc:0.877]
Epoch [20/120    avg_loss:0.490, val_acc:0.899]
Epoch [21/120    avg_loss:0.439, val_acc:0.853]
Epoch [22/120    avg_loss:0.441, val_acc:0.915]
Epoch [23/120    avg_loss:0.406, val_acc:0.929]
Epoch [24/120    avg_loss:0.366, val_acc:0.839]
Epoch [25/120    avg_loss:0.419, val_acc:0.935]
Epoch [26/120    avg_loss:0.383, val_acc:0.853]
Epoch [27/120    avg_loss:0.355, val_acc:0.897]
Epoch [28/120    avg_loss:0.386, val_acc:0.881]
Epoch [29/120    avg_loss:0.347, val_acc:0.931]
Epoch [30/120    avg_loss:0.335, val_acc:0.913]
Epoch [31/120    avg_loss:0.294, val_acc:0.929]
Epoch [32/120    avg_loss:0.290, val_acc:0.899]
Epoch [33/120    avg_loss:0.329, val_acc:0.929]
Epoch [34/120    avg_loss:0.280, val_acc:0.938]
Epoch [35/120    avg_loss:0.258, val_acc:0.937]
Epoch [36/120    avg_loss:0.298, val_acc:0.948]
Epoch [37/120    avg_loss:0.253, val_acc:0.935]
Epoch [38/120    avg_loss:0.244, val_acc:0.938]
Epoch [39/120    avg_loss:0.239, val_acc:0.948]
Epoch [40/120    avg_loss:0.212, val_acc:0.942]
Epoch [41/120    avg_loss:0.213, val_acc:0.950]
Epoch [42/120    avg_loss:0.191, val_acc:0.952]
Epoch [43/120    avg_loss:0.182, val_acc:0.944]
Epoch [44/120    avg_loss:0.181, val_acc:0.942]
Epoch [45/120    avg_loss:0.200, val_acc:0.937]
Epoch [46/120    avg_loss:0.179, val_acc:0.946]
Epoch [47/120    avg_loss:0.263, val_acc:0.960]
Epoch [48/120    avg_loss:0.220, val_acc:0.960]
Epoch [49/120    avg_loss:0.228, val_acc:0.921]
Epoch [50/120    avg_loss:0.227, val_acc:0.942]
Epoch [51/120    avg_loss:0.178, val_acc:0.940]
Epoch [52/120    avg_loss:0.190, val_acc:0.897]
Epoch [53/120    avg_loss:0.173, val_acc:0.956]
Epoch [54/120    avg_loss:0.148, val_acc:0.950]
Epoch [55/120    avg_loss:0.125, val_acc:0.944]
Epoch [56/120    avg_loss:0.155, val_acc:0.964]
Epoch [57/120    avg_loss:0.185, val_acc:0.942]
Epoch [58/120    avg_loss:0.145, val_acc:0.970]
Epoch [59/120    avg_loss:0.150, val_acc:0.980]
Epoch [60/120    avg_loss:0.120, val_acc:0.956]
Epoch [61/120    avg_loss:0.100, val_acc:0.986]
Epoch [62/120    avg_loss:0.114, val_acc:0.966]
Epoch [63/120    avg_loss:0.109, val_acc:0.962]
Epoch [64/120    avg_loss:0.125, val_acc:0.956]
Epoch [65/120    avg_loss:0.134, val_acc:0.958]
Epoch [66/120    avg_loss:0.116, val_acc:0.966]
Epoch [67/120    avg_loss:0.104, val_acc:0.964]
Epoch [68/120    avg_loss:0.148, val_acc:0.976]
Epoch [69/120    avg_loss:0.097, val_acc:0.974]
Epoch [70/120    avg_loss:0.131, val_acc:0.952]
Epoch [71/120    avg_loss:0.136, val_acc:0.960]
Epoch [72/120    avg_loss:0.087, val_acc:0.978]
Epoch [73/120    avg_loss:0.112, val_acc:0.956]
Epoch [74/120    avg_loss:0.160, val_acc:0.958]
Epoch [75/120    avg_loss:0.099, val_acc:0.966]
Epoch [76/120    avg_loss:0.064, val_acc:0.972]
Epoch [77/120    avg_loss:0.063, val_acc:0.978]
Epoch [78/120    avg_loss:0.056, val_acc:0.978]
Epoch [79/120    avg_loss:0.058, val_acc:0.978]
Epoch [80/120    avg_loss:0.048, val_acc:0.980]
Epoch [81/120    avg_loss:0.056, val_acc:0.984]
Epoch [82/120    avg_loss:0.073, val_acc:0.982]
Epoch [83/120    avg_loss:0.051, val_acc:0.982]
Epoch [84/120    avg_loss:0.051, val_acc:0.982]
Epoch [85/120    avg_loss:0.044, val_acc:0.984]
Epoch [86/120    avg_loss:0.050, val_acc:0.982]
Epoch [87/120    avg_loss:0.044, val_acc:0.980]
Epoch [88/120    avg_loss:0.064, val_acc:0.980]
Epoch [89/120    avg_loss:0.056, val_acc:0.980]
Epoch [90/120    avg_loss:0.051, val_acc:0.980]
Epoch [91/120    avg_loss:0.053, val_acc:0.980]
Epoch [92/120    avg_loss:0.052, val_acc:0.980]
Epoch [93/120    avg_loss:0.048, val_acc:0.980]
Epoch [94/120    avg_loss:0.042, val_acc:0.980]
Epoch [95/120    avg_loss:0.051, val_acc:0.980]
Epoch [96/120    avg_loss:0.047, val_acc:0.980]
Epoch [97/120    avg_loss:0.046, val_acc:0.980]
Epoch [98/120    avg_loss:0.044, val_acc:0.980]
Epoch [99/120    avg_loss:0.051, val_acc:0.980]
Epoch [100/120    avg_loss:0.051, val_acc:0.984]
Epoch [101/120    avg_loss:0.061, val_acc:0.982]
Epoch [102/120    avg_loss:0.046, val_acc:0.982]
Epoch [103/120    avg_loss:0.045, val_acc:0.982]
Epoch [104/120    avg_loss:0.055, val_acc:0.982]
Epoch [105/120    avg_loss:0.051, val_acc:0.982]
Epoch [106/120    avg_loss:0.050, val_acc:0.982]
Epoch [107/120    avg_loss:0.051, val_acc:0.982]
Epoch [108/120    avg_loss:0.054, val_acc:0.982]
Epoch [109/120    avg_loss:0.055, val_acc:0.982]
Epoch [110/120    avg_loss:0.044, val_acc:0.982]
Epoch [111/120    avg_loss:0.046, val_acc:0.984]
Epoch [112/120    avg_loss:0.051, val_acc:0.984]
Epoch [113/120    avg_loss:0.047, val_acc:0.984]
Epoch [114/120    avg_loss:0.045, val_acc:0.984]
Epoch [115/120    avg_loss:0.045, val_acc:0.984]
Epoch [116/120    avg_loss:0.047, val_acc:0.984]
Epoch [117/120    avg_loss:0.048, val_acc:0.984]
Epoch [118/120    avg_loss:0.045, val_acc:0.984]
Epoch [119/120    avg_loss:0.049, val_acc:0.984]
Epoch [120/120    avg_loss:0.060, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   1 218   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   9 444   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 0.99927061 0.97104677 0.99122807 0.94170404 0.93069307
 0.99756691 0.93181818 1.         1.         1.         0.98820446
 0.98996656 1.        ]

Kappa:
0.9886055604075619
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff1f5d05940>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.535, val_acc:0.349]
Epoch [2/120    avg_loss:2.265, val_acc:0.518]
Epoch [3/120    avg_loss:2.061, val_acc:0.599]
Epoch [4/120    avg_loss:1.874, val_acc:0.595]
Epoch [5/120    avg_loss:1.683, val_acc:0.593]
Epoch [6/120    avg_loss:1.541, val_acc:0.625]
Epoch [7/120    avg_loss:1.381, val_acc:0.645]
Epoch [8/120    avg_loss:1.248, val_acc:0.690]
Epoch [9/120    avg_loss:1.137, val_acc:0.726]
Epoch [10/120    avg_loss:1.037, val_acc:0.742]
Epoch [11/120    avg_loss:0.937, val_acc:0.762]
Epoch [12/120    avg_loss:0.896, val_acc:0.768]
Epoch [13/120    avg_loss:0.848, val_acc:0.812]
Epoch [14/120    avg_loss:0.760, val_acc:0.804]
Epoch [15/120    avg_loss:0.678, val_acc:0.875]
Epoch [16/120    avg_loss:0.639, val_acc:0.813]
Epoch [17/120    avg_loss:0.635, val_acc:0.873]
Epoch [18/120    avg_loss:0.544, val_acc:0.859]
Epoch [19/120    avg_loss:0.456, val_acc:0.903]
Epoch [20/120    avg_loss:0.485, val_acc:0.891]
Epoch [21/120    avg_loss:0.473, val_acc:0.901]
Epoch [22/120    avg_loss:0.466, val_acc:0.883]
Epoch [23/120    avg_loss:0.372, val_acc:0.925]
Epoch [24/120    avg_loss:0.339, val_acc:0.925]
Epoch [25/120    avg_loss:0.369, val_acc:0.903]
Epoch [26/120    avg_loss:0.381, val_acc:0.931]
Epoch [27/120    avg_loss:0.327, val_acc:0.933]
Epoch [28/120    avg_loss:0.319, val_acc:0.956]
Epoch [29/120    avg_loss:0.362, val_acc:0.937]
Epoch [30/120    avg_loss:0.284, val_acc:0.933]
Epoch [31/120    avg_loss:0.299, val_acc:0.927]
Epoch [32/120    avg_loss:0.285, val_acc:0.958]
Epoch [33/120    avg_loss:0.251, val_acc:0.883]
Epoch [34/120    avg_loss:0.252, val_acc:0.960]
Epoch [35/120    avg_loss:0.304, val_acc:0.950]
Epoch [36/120    avg_loss:0.254, val_acc:0.960]
Epoch [37/120    avg_loss:0.254, val_acc:0.958]
Epoch [38/120    avg_loss:0.188, val_acc:0.958]
Epoch [39/120    avg_loss:0.212, val_acc:0.946]
Epoch [40/120    avg_loss:0.204, val_acc:0.966]
Epoch [41/120    avg_loss:0.174, val_acc:0.948]
Epoch [42/120    avg_loss:0.221, val_acc:0.879]
Epoch [43/120    avg_loss:0.203, val_acc:0.950]
Epoch [44/120    avg_loss:0.154, val_acc:0.942]
Epoch [45/120    avg_loss:0.249, val_acc:0.940]
Epoch [46/120    avg_loss:0.159, val_acc:0.952]
Epoch [47/120    avg_loss:0.149, val_acc:0.964]
Epoch [48/120    avg_loss:0.131, val_acc:0.968]
Epoch [49/120    avg_loss:0.140, val_acc:0.972]
Epoch [50/120    avg_loss:0.122, val_acc:0.960]
Epoch [51/120    avg_loss:0.135, val_acc:0.966]
Epoch [52/120    avg_loss:0.125, val_acc:0.966]
Epoch [53/120    avg_loss:0.152, val_acc:0.938]
Epoch [54/120    avg_loss:0.181, val_acc:0.954]
Epoch [55/120    avg_loss:0.125, val_acc:0.968]
Epoch [56/120    avg_loss:0.116, val_acc:0.974]
Epoch [57/120    avg_loss:0.140, val_acc:0.982]
Epoch [58/120    avg_loss:0.092, val_acc:0.972]
Epoch [59/120    avg_loss:0.127, val_acc:0.972]
Epoch [60/120    avg_loss:0.104, val_acc:0.986]
Epoch [61/120    avg_loss:0.074, val_acc:0.986]
Epoch [62/120    avg_loss:0.063, val_acc:0.990]
Epoch [63/120    avg_loss:0.073, val_acc:0.976]
Epoch [64/120    avg_loss:0.104, val_acc:0.968]
Epoch [65/120    avg_loss:0.083, val_acc:0.956]
Epoch [66/120    avg_loss:0.086, val_acc:0.974]
Epoch [67/120    avg_loss:0.080, val_acc:0.962]
Epoch [68/120    avg_loss:0.124, val_acc:0.964]
Epoch [69/120    avg_loss:0.093, val_acc:0.986]
Epoch [70/120    avg_loss:0.069, val_acc:0.976]
Epoch [71/120    avg_loss:0.052, val_acc:0.984]
Epoch [72/120    avg_loss:0.060, val_acc:0.972]
Epoch [73/120    avg_loss:0.066, val_acc:0.980]
Epoch [74/120    avg_loss:0.073, val_acc:0.982]
Epoch [75/120    avg_loss:0.110, val_acc:0.950]
Epoch [76/120    avg_loss:0.062, val_acc:0.972]
Epoch [77/120    avg_loss:0.058, val_acc:0.978]
Epoch [78/120    avg_loss:0.042, val_acc:0.978]
Epoch [79/120    avg_loss:0.037, val_acc:0.980]
Epoch [80/120    avg_loss:0.050, val_acc:0.982]
Epoch [81/120    avg_loss:0.035, val_acc:0.982]
Epoch [82/120    avg_loss:0.044, val_acc:0.984]
Epoch [83/120    avg_loss:0.041, val_acc:0.986]
Epoch [84/120    avg_loss:0.047, val_acc:0.984]
Epoch [85/120    avg_loss:0.041, val_acc:0.986]
Epoch [86/120    avg_loss:0.035, val_acc:0.986]
Epoch [87/120    avg_loss:0.037, val_acc:0.986]
Epoch [88/120    avg_loss:0.037, val_acc:0.986]
Epoch [89/120    avg_loss:0.038, val_acc:0.986]
Epoch [90/120    avg_loss:0.030, val_acc:0.986]
Epoch [91/120    avg_loss:0.037, val_acc:0.986]
Epoch [92/120    avg_loss:0.028, val_acc:0.986]
Epoch [93/120    avg_loss:0.033, val_acc:0.986]
Epoch [94/120    avg_loss:0.031, val_acc:0.986]
Epoch [95/120    avg_loss:0.029, val_acc:0.986]
Epoch [96/120    avg_loss:0.036, val_acc:0.986]
Epoch [97/120    avg_loss:0.043, val_acc:0.986]
Epoch [98/120    avg_loss:0.031, val_acc:0.986]
Epoch [99/120    avg_loss:0.036, val_acc:0.986]
Epoch [100/120    avg_loss:0.034, val_acc:0.986]
Epoch [101/120    avg_loss:0.038, val_acc:0.986]
Epoch [102/120    avg_loss:0.041, val_acc:0.986]
Epoch [103/120    avg_loss:0.034, val_acc:0.986]
Epoch [104/120    avg_loss:0.030, val_acc:0.986]
Epoch [105/120    avg_loss:0.031, val_acc:0.986]
Epoch [106/120    avg_loss:0.040, val_acc:0.986]
Epoch [107/120    avg_loss:0.035, val_acc:0.986]
Epoch [108/120    avg_loss:0.043, val_acc:0.986]
Epoch [109/120    avg_loss:0.032, val_acc:0.986]
Epoch [110/120    avg_loss:0.033, val_acc:0.986]
Epoch [111/120    avg_loss:0.031, val_acc:0.986]
Epoch [112/120    avg_loss:0.033, val_acc:0.986]
Epoch [113/120    avg_loss:0.035, val_acc:0.986]
Epoch [114/120    avg_loss:0.032, val_acc:0.986]
Epoch [115/120    avg_loss:0.042, val_acc:0.986]
Epoch [116/120    avg_loss:0.036, val_acc:0.986]
Epoch [117/120    avg_loss:0.030, val_acc:0.986]
Epoch [118/120    avg_loss:0.034, val_acc:0.986]
Epoch [119/120    avg_loss:0.030, val_acc:0.986]
Epoch [120/120    avg_loss:0.032, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  16   0   0   0   0   0   0   1   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.99926954 0.98426966 1.         0.95238095 0.93377483
 0.99757869 0.96132597 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9931160639413954
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc66fc04978>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.505, val_acc:0.356]
Epoch [2/120    avg_loss:2.269, val_acc:0.479]
Epoch [3/120    avg_loss:2.074, val_acc:0.575]
Epoch [4/120    avg_loss:1.875, val_acc:0.579]
Epoch [5/120    avg_loss:1.721, val_acc:0.602]
Epoch [6/120    avg_loss:1.572, val_acc:0.660]
Epoch [7/120    avg_loss:1.399, val_acc:0.704]
Epoch [8/120    avg_loss:1.252, val_acc:0.794]
Epoch [9/120    avg_loss:1.123, val_acc:0.790]
Epoch [10/120    avg_loss:1.023, val_acc:0.854]
Epoch [11/120    avg_loss:0.884, val_acc:0.877]
Epoch [12/120    avg_loss:0.823, val_acc:0.860]
Epoch [13/120    avg_loss:0.714, val_acc:0.883]
Epoch [14/120    avg_loss:0.688, val_acc:0.904]
Epoch [15/120    avg_loss:0.624, val_acc:0.898]
Epoch [16/120    avg_loss:0.567, val_acc:0.827]
Epoch [17/120    avg_loss:0.539, val_acc:0.883]
Epoch [18/120    avg_loss:0.539, val_acc:0.894]
Epoch [19/120    avg_loss:0.491, val_acc:0.885]
Epoch [20/120    avg_loss:0.488, val_acc:0.892]
Epoch [21/120    avg_loss:0.475, val_acc:0.912]
Epoch [22/120    avg_loss:0.443, val_acc:0.906]
Epoch [23/120    avg_loss:0.446, val_acc:0.892]
Epoch [24/120    avg_loss:0.355, val_acc:0.917]
Epoch [25/120    avg_loss:0.355, val_acc:0.923]
Epoch [26/120    avg_loss:0.331, val_acc:0.902]
Epoch [27/120    avg_loss:0.342, val_acc:0.935]
Epoch [28/120    avg_loss:0.356, val_acc:0.910]
Epoch [29/120    avg_loss:0.279, val_acc:0.915]
Epoch [30/120    avg_loss:0.356, val_acc:0.854]
Epoch [31/120    avg_loss:0.353, val_acc:0.927]
Epoch [32/120    avg_loss:0.305, val_acc:0.940]
Epoch [33/120    avg_loss:0.264, val_acc:0.931]
Epoch [34/120    avg_loss:0.280, val_acc:0.927]
Epoch [35/120    avg_loss:0.288, val_acc:0.969]
Epoch [36/120    avg_loss:0.263, val_acc:0.938]
Epoch [37/120    avg_loss:0.216, val_acc:0.958]
Epoch [38/120    avg_loss:0.267, val_acc:0.927]
Epoch [39/120    avg_loss:0.257, val_acc:0.935]
Epoch [40/120    avg_loss:0.238, val_acc:0.935]
Epoch [41/120    avg_loss:0.221, val_acc:0.925]
Epoch [42/120    avg_loss:0.257, val_acc:0.923]
Epoch [43/120    avg_loss:0.224, val_acc:0.915]
Epoch [44/120    avg_loss:0.212, val_acc:0.925]
Epoch [45/120    avg_loss:0.203, val_acc:0.952]
Epoch [46/120    avg_loss:0.160, val_acc:0.954]
Epoch [47/120    avg_loss:0.182, val_acc:0.950]
Epoch [48/120    avg_loss:0.184, val_acc:0.875]
Epoch [49/120    avg_loss:0.201, val_acc:0.950]
Epoch [50/120    avg_loss:0.146, val_acc:0.967]
Epoch [51/120    avg_loss:0.131, val_acc:0.965]
Epoch [52/120    avg_loss:0.143, val_acc:0.971]
Epoch [53/120    avg_loss:0.128, val_acc:0.967]
Epoch [54/120    avg_loss:0.136, val_acc:0.973]
Epoch [55/120    avg_loss:0.142, val_acc:0.977]
Epoch [56/120    avg_loss:0.121, val_acc:0.975]
Epoch [57/120    avg_loss:0.120, val_acc:0.973]
Epoch [58/120    avg_loss:0.142, val_acc:0.979]
Epoch [59/120    avg_loss:0.133, val_acc:0.977]
Epoch [60/120    avg_loss:0.127, val_acc:0.981]
Epoch [61/120    avg_loss:0.110, val_acc:0.979]
Epoch [62/120    avg_loss:0.108, val_acc:0.979]
Epoch [63/120    avg_loss:0.111, val_acc:0.977]
Epoch [64/120    avg_loss:0.114, val_acc:0.979]
Epoch [65/120    avg_loss:0.114, val_acc:0.969]
Epoch [66/120    avg_loss:0.123, val_acc:0.975]
Epoch [67/120    avg_loss:0.110, val_acc:0.979]
Epoch [68/120    avg_loss:0.102, val_acc:0.981]
Epoch [69/120    avg_loss:0.117, val_acc:0.979]
Epoch [70/120    avg_loss:0.114, val_acc:0.977]
Epoch [71/120    avg_loss:0.112, val_acc:0.977]
Epoch [72/120    avg_loss:0.107, val_acc:0.977]
Epoch [73/120    avg_loss:0.101, val_acc:0.979]
Epoch [74/120    avg_loss:0.102, val_acc:0.971]
Epoch [75/120    avg_loss:0.120, val_acc:0.983]
Epoch [76/120    avg_loss:0.111, val_acc:0.981]
Epoch [77/120    avg_loss:0.105, val_acc:0.981]
Epoch [78/120    avg_loss:0.097, val_acc:0.981]
Epoch [79/120    avg_loss:0.109, val_acc:0.979]
Epoch [80/120    avg_loss:0.110, val_acc:0.979]
Epoch [81/120    avg_loss:0.112, val_acc:0.983]
Epoch [82/120    avg_loss:0.097, val_acc:0.983]
Epoch [83/120    avg_loss:0.101, val_acc:0.983]
Epoch [84/120    avg_loss:0.092, val_acc:0.981]
Epoch [85/120    avg_loss:0.095, val_acc:0.983]
Epoch [86/120    avg_loss:0.121, val_acc:0.983]
Epoch [87/120    avg_loss:0.102, val_acc:0.985]
Epoch [88/120    avg_loss:0.087, val_acc:0.985]
Epoch [89/120    avg_loss:0.102, val_acc:0.979]
Epoch [90/120    avg_loss:0.095, val_acc:0.985]
Epoch [91/120    avg_loss:0.106, val_acc:0.981]
Epoch [92/120    avg_loss:0.121, val_acc:0.981]
Epoch [93/120    avg_loss:0.107, val_acc:0.981]
Epoch [94/120    avg_loss:0.093, val_acc:0.981]
Epoch [95/120    avg_loss:0.091, val_acc:0.988]
Epoch [96/120    avg_loss:0.107, val_acc:0.983]
Epoch [97/120    avg_loss:0.099, val_acc:0.975]
Epoch [98/120    avg_loss:0.101, val_acc:0.988]
Epoch [99/120    avg_loss:0.097, val_acc:0.981]
Epoch [100/120    avg_loss:0.094, val_acc:0.985]
Epoch [101/120    avg_loss:0.090, val_acc:0.981]
Epoch [102/120    avg_loss:0.075, val_acc:0.985]
Epoch [103/120    avg_loss:0.103, val_acc:0.990]
Epoch [104/120    avg_loss:0.111, val_acc:0.990]
Epoch [105/120    avg_loss:0.107, val_acc:0.988]
Epoch [106/120    avg_loss:0.084, val_acc:0.979]
Epoch [107/120    avg_loss:0.088, val_acc:0.985]
Epoch [108/120    avg_loss:0.100, val_acc:0.983]
Epoch [109/120    avg_loss:0.092, val_acc:0.990]
Epoch [110/120    avg_loss:0.082, val_acc:0.990]
Epoch [111/120    avg_loss:0.080, val_acc:0.990]
Epoch [112/120    avg_loss:0.090, val_acc:0.990]
Epoch [113/120    avg_loss:0.087, val_acc:0.990]
Epoch [114/120    avg_loss:0.089, val_acc:0.988]
Epoch [115/120    avg_loss:0.100, val_acc:0.988]
Epoch [116/120    avg_loss:0.095, val_acc:0.988]
Epoch [117/120    avg_loss:0.089, val_acc:0.990]
Epoch [118/120    avg_loss:0.082, val_acc:0.988]
Epoch [119/120    avg_loss:0.081, val_acc:0.988]
Epoch [120/120    avg_loss:0.081, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   0   0   0   0   1   0]
 [  0   0   0 227   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 202  21   0   0   0   0   0   0   4   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.91257995735607

F1 scores:
[       nan 0.99853801 0.98198198 0.99343545 0.90990991 0.89333333
 0.99273608 0.96132597 1.         1.         1.         0.99867198
 0.99342105 1.        ]

Kappa:
0.9878933478808195
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe4b3eb27f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.533, val_acc:0.446]
Epoch [2/120    avg_loss:2.268, val_acc:0.540]
Epoch [3/120    avg_loss:2.059, val_acc:0.556]
Epoch [4/120    avg_loss:1.817, val_acc:0.573]
Epoch [5/120    avg_loss:1.617, val_acc:0.640]
Epoch [6/120    avg_loss:1.477, val_acc:0.719]
Epoch [7/120    avg_loss:1.328, val_acc:0.729]
Epoch [8/120    avg_loss:1.180, val_acc:0.752]
Epoch [9/120    avg_loss:1.052, val_acc:0.754]
Epoch [10/120    avg_loss:0.956, val_acc:0.773]
Epoch [11/120    avg_loss:0.889, val_acc:0.825]
Epoch [12/120    avg_loss:0.771, val_acc:0.844]
Epoch [13/120    avg_loss:0.778, val_acc:0.831]
Epoch [14/120    avg_loss:0.724, val_acc:0.890]
Epoch [15/120    avg_loss:0.687, val_acc:0.921]
Epoch [16/120    avg_loss:0.612, val_acc:0.900]
Epoch [17/120    avg_loss:0.544, val_acc:0.890]
Epoch [18/120    avg_loss:0.519, val_acc:0.935]
Epoch [19/120    avg_loss:0.450, val_acc:0.923]
Epoch [20/120    avg_loss:0.474, val_acc:0.923]
Epoch [21/120    avg_loss:0.452, val_acc:0.919]
Epoch [22/120    avg_loss:0.382, val_acc:0.925]
Epoch [23/120    avg_loss:0.375, val_acc:0.873]
Epoch [24/120    avg_loss:0.418, val_acc:0.881]
Epoch [25/120    avg_loss:0.346, val_acc:0.931]
Epoch [26/120    avg_loss:0.324, val_acc:0.948]
Epoch [27/120    avg_loss:0.298, val_acc:0.927]
Epoch [28/120    avg_loss:0.316, val_acc:0.933]
Epoch [29/120    avg_loss:0.309, val_acc:0.942]
Epoch [30/120    avg_loss:0.243, val_acc:0.971]
Epoch [31/120    avg_loss:0.276, val_acc:0.917]
Epoch [32/120    avg_loss:0.310, val_acc:0.933]
Epoch [33/120    avg_loss:0.253, val_acc:0.940]
Epoch [34/120    avg_loss:0.226, val_acc:0.967]
Epoch [35/120    avg_loss:0.208, val_acc:0.960]
Epoch [36/120    avg_loss:0.210, val_acc:0.952]
Epoch [37/120    avg_loss:0.235, val_acc:0.954]
Epoch [38/120    avg_loss:0.192, val_acc:0.965]
Epoch [39/120    avg_loss:0.214, val_acc:0.956]
Epoch [40/120    avg_loss:0.232, val_acc:0.931]
Epoch [41/120    avg_loss:0.207, val_acc:0.958]
Epoch [42/120    avg_loss:0.171, val_acc:0.965]
Epoch [43/120    avg_loss:0.215, val_acc:0.969]
Epoch [44/120    avg_loss:0.166, val_acc:0.977]
Epoch [45/120    avg_loss:0.128, val_acc:0.977]
Epoch [46/120    avg_loss:0.140, val_acc:0.981]
Epoch [47/120    avg_loss:0.136, val_acc:0.979]
Epoch [48/120    avg_loss:0.133, val_acc:0.981]
Epoch [49/120    avg_loss:0.134, val_acc:0.973]
Epoch [50/120    avg_loss:0.118, val_acc:0.985]
Epoch [51/120    avg_loss:0.135, val_acc:0.983]
Epoch [52/120    avg_loss:0.130, val_acc:0.983]
Epoch [53/120    avg_loss:0.120, val_acc:0.985]
Epoch [54/120    avg_loss:0.117, val_acc:0.983]
Epoch [55/120    avg_loss:0.122, val_acc:0.983]
Epoch [56/120    avg_loss:0.108, val_acc:0.983]
Epoch [57/120    avg_loss:0.112, val_acc:0.988]
Epoch [58/120    avg_loss:0.104, val_acc:0.985]
Epoch [59/120    avg_loss:0.103, val_acc:0.985]
Epoch [60/120    avg_loss:0.119, val_acc:0.985]
Epoch [61/120    avg_loss:0.109, val_acc:0.985]
Epoch [62/120    avg_loss:0.123, val_acc:0.985]
Epoch [63/120    avg_loss:0.107, val_acc:0.985]
Epoch [64/120    avg_loss:0.112, val_acc:0.983]
Epoch [65/120    avg_loss:0.116, val_acc:0.985]
Epoch [66/120    avg_loss:0.116, val_acc:0.985]
Epoch [67/120    avg_loss:0.117, val_acc:0.988]
Epoch [68/120    avg_loss:0.101, val_acc:0.985]
Epoch [69/120    avg_loss:0.108, val_acc:0.983]
Epoch [70/120    avg_loss:0.092, val_acc:0.985]
Epoch [71/120    avg_loss:0.090, val_acc:0.985]
Epoch [72/120    avg_loss:0.107, val_acc:0.985]
Epoch [73/120    avg_loss:0.095, val_acc:0.990]
Epoch [74/120    avg_loss:0.104, val_acc:0.985]
Epoch [75/120    avg_loss:0.106, val_acc:0.985]
Epoch [76/120    avg_loss:0.100, val_acc:0.983]
Epoch [77/120    avg_loss:0.101, val_acc:0.990]
Epoch [78/120    avg_loss:0.096, val_acc:0.988]
Epoch [79/120    avg_loss:0.094, val_acc:0.990]
Epoch [80/120    avg_loss:0.101, val_acc:0.990]
Epoch [81/120    avg_loss:0.095, val_acc:0.988]
Epoch [82/120    avg_loss:0.100, val_acc:0.990]
Epoch [83/120    avg_loss:0.102, val_acc:0.988]
Epoch [84/120    avg_loss:0.094, val_acc:0.988]
Epoch [85/120    avg_loss:0.110, val_acc:0.985]
Epoch [86/120    avg_loss:0.092, val_acc:0.990]
Epoch [87/120    avg_loss:0.087, val_acc:0.992]
Epoch [88/120    avg_loss:0.106, val_acc:0.988]
Epoch [89/120    avg_loss:0.083, val_acc:0.990]
Epoch [90/120    avg_loss:0.090, val_acc:0.990]
Epoch [91/120    avg_loss:0.087, val_acc:0.990]
Epoch [92/120    avg_loss:0.091, val_acc:0.990]
Epoch [93/120    avg_loss:0.087, val_acc:0.990]
Epoch [94/120    avg_loss:0.085, val_acc:0.990]
Epoch [95/120    avg_loss:0.091, val_acc:0.992]
Epoch [96/120    avg_loss:0.084, val_acc:0.988]
Epoch [97/120    avg_loss:0.092, val_acc:0.992]
Epoch [98/120    avg_loss:0.087, val_acc:0.992]
Epoch [99/120    avg_loss:0.089, val_acc:0.990]
Epoch [100/120    avg_loss:0.089, val_acc:0.992]
Epoch [101/120    avg_loss:0.079, val_acc:0.992]
Epoch [102/120    avg_loss:0.081, val_acc:0.990]
Epoch [103/120    avg_loss:0.101, val_acc:0.994]
Epoch [104/120    avg_loss:0.090, val_acc:0.988]
Epoch [105/120    avg_loss:0.088, val_acc:0.992]
Epoch [106/120    avg_loss:0.088, val_acc:0.990]
Epoch [107/120    avg_loss:0.103, val_acc:0.992]
Epoch [108/120    avg_loss:0.089, val_acc:0.992]
Epoch [109/120    avg_loss:0.079, val_acc:0.992]
Epoch [110/120    avg_loss:0.074, val_acc:0.992]
Epoch [111/120    avg_loss:0.073, val_acc:0.990]
Epoch [112/120    avg_loss:0.088, val_acc:0.992]
Epoch [113/120    avg_loss:0.075, val_acc:0.992]
Epoch [114/120    avg_loss:0.074, val_acc:0.990]
Epoch [115/120    avg_loss:0.084, val_acc:0.992]
Epoch [116/120    avg_loss:0.084, val_acc:0.992]
Epoch [117/120    avg_loss:0.070, val_acc:0.990]
Epoch [118/120    avg_loss:0.079, val_acc:0.990]
Epoch [119/120    avg_loss:0.072, val_acc:0.990]
Epoch [120/120    avg_loss:0.074, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 0.99926954 0.98426966 1.         0.92105263 0.875
 0.99757869 0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9895549199333694
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdf83a7e898>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.542, val_acc:0.325]
Epoch [2/120    avg_loss:2.277, val_acc:0.446]
Epoch [3/120    avg_loss:2.097, val_acc:0.483]
Epoch [4/120    avg_loss:1.922, val_acc:0.517]
Epoch [5/120    avg_loss:1.772, val_acc:0.558]
Epoch [6/120    avg_loss:1.613, val_acc:0.629]
Epoch [7/120    avg_loss:1.433, val_acc:0.679]
Epoch [8/120    avg_loss:1.297, val_acc:0.700]
Epoch [9/120    avg_loss:1.165, val_acc:0.746]
Epoch [10/120    avg_loss:1.071, val_acc:0.752]
Epoch [11/120    avg_loss:1.026, val_acc:0.758]
Epoch [12/120    avg_loss:0.895, val_acc:0.833]
Epoch [13/120    avg_loss:0.828, val_acc:0.785]
Epoch [14/120    avg_loss:0.742, val_acc:0.838]
Epoch [15/120    avg_loss:0.706, val_acc:0.892]
Epoch [16/120    avg_loss:0.597, val_acc:0.875]
Epoch [17/120    avg_loss:0.672, val_acc:0.890]
Epoch [18/120    avg_loss:0.579, val_acc:0.883]
Epoch [19/120    avg_loss:0.513, val_acc:0.923]
Epoch [20/120    avg_loss:0.501, val_acc:0.912]
Epoch [21/120    avg_loss:0.437, val_acc:0.929]
Epoch [22/120    avg_loss:0.458, val_acc:0.908]
Epoch [23/120    avg_loss:0.451, val_acc:0.915]
Epoch [24/120    avg_loss:0.428, val_acc:0.925]
Epoch [25/120    avg_loss:0.383, val_acc:0.898]
Epoch [26/120    avg_loss:0.356, val_acc:0.940]
Epoch [27/120    avg_loss:0.380, val_acc:0.925]
Epoch [28/120    avg_loss:0.352, val_acc:0.912]
Epoch [29/120    avg_loss:0.365, val_acc:0.933]
Epoch [30/120    avg_loss:0.284, val_acc:0.965]
Epoch [31/120    avg_loss:0.321, val_acc:0.958]
Epoch [32/120    avg_loss:0.256, val_acc:0.942]
Epoch [33/120    avg_loss:0.315, val_acc:0.946]
Epoch [34/120    avg_loss:0.250, val_acc:0.933]
Epoch [35/120    avg_loss:0.257, val_acc:0.960]
Epoch [36/120    avg_loss:0.230, val_acc:0.956]
Epoch [37/120    avg_loss:0.277, val_acc:0.912]
Epoch [38/120    avg_loss:0.296, val_acc:0.963]
Epoch [39/120    avg_loss:0.235, val_acc:0.969]
Epoch [40/120    avg_loss:0.211, val_acc:0.975]
Epoch [41/120    avg_loss:0.235, val_acc:0.960]
Epoch [42/120    avg_loss:0.208, val_acc:0.973]
Epoch [43/120    avg_loss:0.201, val_acc:0.923]
Epoch [44/120    avg_loss:0.242, val_acc:0.935]
Epoch [45/120    avg_loss:0.235, val_acc:0.952]
Epoch [46/120    avg_loss:0.261, val_acc:0.967]
Epoch [47/120    avg_loss:0.161, val_acc:0.975]
Epoch [48/120    avg_loss:0.170, val_acc:0.975]
Epoch [49/120    avg_loss:0.139, val_acc:0.969]
Epoch [50/120    avg_loss:0.148, val_acc:0.975]
Epoch [51/120    avg_loss:0.140, val_acc:0.965]
Epoch [52/120    avg_loss:0.270, val_acc:0.965]
Epoch [53/120    avg_loss:0.126, val_acc:0.977]
Epoch [54/120    avg_loss:0.165, val_acc:0.954]
Epoch [55/120    avg_loss:0.224, val_acc:0.960]
Epoch [56/120    avg_loss:0.143, val_acc:0.983]
Epoch [57/120    avg_loss:0.164, val_acc:0.977]
Epoch [58/120    avg_loss:0.120, val_acc:0.981]
Epoch [59/120    avg_loss:0.131, val_acc:0.985]
Epoch [60/120    avg_loss:0.127, val_acc:0.979]
Epoch [61/120    avg_loss:0.133, val_acc:0.952]
Epoch [62/120    avg_loss:0.155, val_acc:0.971]
Epoch [63/120    avg_loss:0.228, val_acc:0.965]
Epoch [64/120    avg_loss:0.168, val_acc:0.973]
Epoch [65/120    avg_loss:0.175, val_acc:0.969]
Epoch [66/120    avg_loss:0.132, val_acc:0.967]
Epoch [67/120    avg_loss:0.132, val_acc:0.979]
Epoch [68/120    avg_loss:0.097, val_acc:0.971]
Epoch [69/120    avg_loss:0.137, val_acc:0.977]
Epoch [70/120    avg_loss:0.166, val_acc:0.929]
Epoch [71/120    avg_loss:0.129, val_acc:0.977]
Epoch [72/120    avg_loss:0.110, val_acc:0.979]
Epoch [73/120    avg_loss:0.078, val_acc:0.981]
Epoch [74/120    avg_loss:0.069, val_acc:0.983]
Epoch [75/120    avg_loss:0.086, val_acc:0.981]
Epoch [76/120    avg_loss:0.080, val_acc:0.983]
Epoch [77/120    avg_loss:0.061, val_acc:0.983]
Epoch [78/120    avg_loss:0.061, val_acc:0.985]
Epoch [79/120    avg_loss:0.064, val_acc:0.981]
Epoch [80/120    avg_loss:0.083, val_acc:0.988]
Epoch [81/120    avg_loss:0.068, val_acc:0.988]
Epoch [82/120    avg_loss:0.068, val_acc:0.985]
Epoch [83/120    avg_loss:0.058, val_acc:0.985]
Epoch [84/120    avg_loss:0.068, val_acc:0.983]
Epoch [85/120    avg_loss:0.070, val_acc:0.985]
Epoch [86/120    avg_loss:0.050, val_acc:0.988]
Epoch [87/120    avg_loss:0.079, val_acc:0.985]
Epoch [88/120    avg_loss:0.068, val_acc:0.985]
Epoch [89/120    avg_loss:0.055, val_acc:0.988]
Epoch [90/120    avg_loss:0.054, val_acc:0.990]
Epoch [91/120    avg_loss:0.072, val_acc:0.990]
Epoch [92/120    avg_loss:0.063, val_acc:0.988]
Epoch [93/120    avg_loss:0.064, val_acc:0.988]
Epoch [94/120    avg_loss:0.058, val_acc:0.988]
Epoch [95/120    avg_loss:0.061, val_acc:0.985]
Epoch [96/120    avg_loss:0.056, val_acc:0.985]
Epoch [97/120    avg_loss:0.061, val_acc:0.985]
Epoch [98/120    avg_loss:0.049, val_acc:0.985]
Epoch [99/120    avg_loss:0.058, val_acc:0.988]
Epoch [100/120    avg_loss:0.052, val_acc:0.985]
Epoch [101/120    avg_loss:0.059, val_acc:0.985]
Epoch [102/120    avg_loss:0.051, val_acc:0.985]
Epoch [103/120    avg_loss:0.052, val_acc:0.985]
Epoch [104/120    avg_loss:0.050, val_acc:0.983]
Epoch [105/120    avg_loss:0.055, val_acc:0.983]
Epoch [106/120    avg_loss:0.045, val_acc:0.985]
Epoch [107/120    avg_loss:0.042, val_acc:0.985]
Epoch [108/120    avg_loss:0.055, val_acc:0.985]
Epoch [109/120    avg_loss:0.057, val_acc:0.985]
Epoch [110/120    avg_loss:0.061, val_acc:0.985]
Epoch [111/120    avg_loss:0.050, val_acc:0.985]
Epoch [112/120    avg_loss:0.046, val_acc:0.985]
Epoch [113/120    avg_loss:0.049, val_acc:0.985]
Epoch [114/120    avg_loss:0.053, val_acc:0.985]
Epoch [115/120    avg_loss:0.051, val_acc:0.985]
Epoch [116/120    avg_loss:0.052, val_acc:0.985]
Epoch [117/120    avg_loss:0.048, val_acc:0.985]
Epoch [118/120    avg_loss:0.049, val_acc:0.985]
Epoch [119/120    avg_loss:0.056, val_acc:0.985]
Epoch [120/120    avg_loss:0.060, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 203  22   0   0   0   0   0   0   2   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 0.99926954 0.99090909 1.         0.90423163 0.86394558
 0.99514563 0.98378378 0.998713   1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9886055690584917
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4303062908>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.564, val_acc:0.337]
Epoch [2/120    avg_loss:2.313, val_acc:0.560]
Epoch [3/120    avg_loss:2.125, val_acc:0.577]
Epoch [4/120    avg_loss:1.946, val_acc:0.593]
Epoch [5/120    avg_loss:1.751, val_acc:0.581]
Epoch [6/120    avg_loss:1.609, val_acc:0.631]
Epoch [7/120    avg_loss:1.450, val_acc:0.687]
Epoch [8/120    avg_loss:1.309, val_acc:0.685]
Epoch [9/120    avg_loss:1.168, val_acc:0.772]
Epoch [10/120    avg_loss:1.083, val_acc:0.800]
Epoch [11/120    avg_loss:0.972, val_acc:0.788]
Epoch [12/120    avg_loss:0.894, val_acc:0.869]
Epoch [13/120    avg_loss:0.819, val_acc:0.798]
Epoch [14/120    avg_loss:0.733, val_acc:0.877]
Epoch [15/120    avg_loss:0.754, val_acc:0.897]
Epoch [16/120    avg_loss:0.712, val_acc:0.887]
Epoch [17/120    avg_loss:0.605, val_acc:0.905]
Epoch [18/120    avg_loss:0.541, val_acc:0.929]
Epoch [19/120    avg_loss:0.525, val_acc:0.942]
Epoch [20/120    avg_loss:0.477, val_acc:0.913]
Epoch [21/120    avg_loss:0.466, val_acc:0.895]
Epoch [22/120    avg_loss:0.465, val_acc:0.889]
Epoch [23/120    avg_loss:0.521, val_acc:0.907]
Epoch [24/120    avg_loss:0.417, val_acc:0.919]
Epoch [25/120    avg_loss:0.401, val_acc:0.931]
Epoch [26/120    avg_loss:0.356, val_acc:0.907]
Epoch [27/120    avg_loss:0.327, val_acc:0.923]
Epoch [28/120    avg_loss:0.372, val_acc:0.950]
Epoch [29/120    avg_loss:0.348, val_acc:0.946]
Epoch [30/120    avg_loss:0.315, val_acc:0.940]
Epoch [31/120    avg_loss:0.333, val_acc:0.913]
Epoch [32/120    avg_loss:0.317, val_acc:0.966]
Epoch [33/120    avg_loss:0.247, val_acc:0.881]
Epoch [34/120    avg_loss:0.262, val_acc:0.954]
Epoch [35/120    avg_loss:0.244, val_acc:0.972]
Epoch [36/120    avg_loss:0.257, val_acc:0.948]
Epoch [37/120    avg_loss:0.315, val_acc:0.937]
Epoch [38/120    avg_loss:0.249, val_acc:0.962]
Epoch [39/120    avg_loss:0.236, val_acc:0.954]
Epoch [40/120    avg_loss:0.215, val_acc:0.952]
Epoch [41/120    avg_loss:0.235, val_acc:0.956]
Epoch [42/120    avg_loss:0.207, val_acc:0.942]
Epoch [43/120    avg_loss:0.226, val_acc:0.942]
Epoch [44/120    avg_loss:0.190, val_acc:0.970]
Epoch [45/120    avg_loss:0.181, val_acc:0.964]
Epoch [46/120    avg_loss:0.201, val_acc:0.935]
Epoch [47/120    avg_loss:0.199, val_acc:0.940]
Epoch [48/120    avg_loss:0.208, val_acc:0.968]
Epoch [49/120    avg_loss:0.148, val_acc:0.968]
Epoch [50/120    avg_loss:0.143, val_acc:0.968]
Epoch [51/120    avg_loss:0.149, val_acc:0.968]
Epoch [52/120    avg_loss:0.130, val_acc:0.970]
Epoch [53/120    avg_loss:0.127, val_acc:0.970]
Epoch [54/120    avg_loss:0.124, val_acc:0.968]
Epoch [55/120    avg_loss:0.133, val_acc:0.966]
Epoch [56/120    avg_loss:0.124, val_acc:0.966]
Epoch [57/120    avg_loss:0.126, val_acc:0.968]
Epoch [58/120    avg_loss:0.122, val_acc:0.970]
Epoch [59/120    avg_loss:0.123, val_acc:0.970]
Epoch [60/120    avg_loss:0.119, val_acc:0.970]
Epoch [61/120    avg_loss:0.125, val_acc:0.970]
Epoch [62/120    avg_loss:0.110, val_acc:0.970]
Epoch [63/120    avg_loss:0.097, val_acc:0.970]
Epoch [64/120    avg_loss:0.111, val_acc:0.970]
Epoch [65/120    avg_loss:0.114, val_acc:0.970]
Epoch [66/120    avg_loss:0.119, val_acc:0.970]
Epoch [67/120    avg_loss:0.118, val_acc:0.970]
Epoch [68/120    avg_loss:0.118, val_acc:0.970]
Epoch [69/120    avg_loss:0.116, val_acc:0.970]
Epoch [70/120    avg_loss:0.110, val_acc:0.970]
Epoch [71/120    avg_loss:0.110, val_acc:0.970]
Epoch [72/120    avg_loss:0.121, val_acc:0.970]
Epoch [73/120    avg_loss:0.101, val_acc:0.970]
Epoch [74/120    avg_loss:0.114, val_acc:0.970]
Epoch [75/120    avg_loss:0.112, val_acc:0.970]
Epoch [76/120    avg_loss:0.120, val_acc:0.970]
Epoch [77/120    avg_loss:0.121, val_acc:0.970]
Epoch [78/120    avg_loss:0.100, val_acc:0.970]
Epoch [79/120    avg_loss:0.111, val_acc:0.970]
Epoch [80/120    avg_loss:0.111, val_acc:0.970]
Epoch [81/120    avg_loss:0.124, val_acc:0.970]
Epoch [82/120    avg_loss:0.130, val_acc:0.970]
Epoch [83/120    avg_loss:0.126, val_acc:0.970]
Epoch [84/120    avg_loss:0.109, val_acc:0.970]
Epoch [85/120    avg_loss:0.118, val_acc:0.970]
Epoch [86/120    avg_loss:0.115, val_acc:0.970]
Epoch [87/120    avg_loss:0.114, val_acc:0.970]
Epoch [88/120    avg_loss:0.118, val_acc:0.970]
Epoch [89/120    avg_loss:0.120, val_acc:0.970]
Epoch [90/120    avg_loss:0.102, val_acc:0.970]
Epoch [91/120    avg_loss:0.100, val_acc:0.970]
Epoch [92/120    avg_loss:0.100, val_acc:0.970]
Epoch [93/120    avg_loss:0.108, val_acc:0.970]
Epoch [94/120    avg_loss:0.102, val_acc:0.970]
Epoch [95/120    avg_loss:0.113, val_acc:0.970]
Epoch [96/120    avg_loss:0.111, val_acc:0.970]
Epoch [97/120    avg_loss:0.123, val_acc:0.970]
Epoch [98/120    avg_loss:0.110, val_acc:0.970]
Epoch [99/120    avg_loss:0.108, val_acc:0.970]
Epoch [100/120    avg_loss:0.113, val_acc:0.970]
Epoch [101/120    avg_loss:0.102, val_acc:0.970]
Epoch [102/120    avg_loss:0.117, val_acc:0.970]
Epoch [103/120    avg_loss:0.112, val_acc:0.970]
Epoch [104/120    avg_loss:0.115, val_acc:0.970]
Epoch [105/120    avg_loss:0.109, val_acc:0.970]
Epoch [106/120    avg_loss:0.104, val_acc:0.970]
Epoch [107/120    avg_loss:0.125, val_acc:0.970]
Epoch [108/120    avg_loss:0.114, val_acc:0.970]
Epoch [109/120    avg_loss:0.121, val_acc:0.970]
Epoch [110/120    avg_loss:0.118, val_acc:0.970]
Epoch [111/120    avg_loss:0.115, val_acc:0.970]
Epoch [112/120    avg_loss:0.108, val_acc:0.970]
Epoch [113/120    avg_loss:0.112, val_acc:0.970]
Epoch [114/120    avg_loss:0.109, val_acc:0.970]
Epoch [115/120    avg_loss:0.113, val_acc:0.970]
Epoch [116/120    avg_loss:0.111, val_acc:0.970]
Epoch [117/120    avg_loss:0.113, val_acc:0.970]
Epoch [118/120    avg_loss:0.121, val_acc:0.970]
Epoch [119/120    avg_loss:0.133, val_acc:0.970]
Epoch [120/120    avg_loss:0.118, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   0   0   0   0   1   0]
 [  0   0   0 229   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0   2 192  28   0   0   0   0   0   0   5   0]
 [  0   0   0   0  25 120   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.42217484008529

F1 scores:
[       nan 0.99854227 0.9753915  0.99349241 0.86486486 0.81911263
 0.99512195 0.93854749 1.         1.         1.         1.
 0.99342105 1.        ]

Kappa:
0.9824309303102343
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe6c95e7978>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.560, val_acc:0.458]
Epoch [2/120    avg_loss:2.343, val_acc:0.460]
Epoch [3/120    avg_loss:2.172, val_acc:0.562]
Epoch [4/120    avg_loss:2.036, val_acc:0.595]
Epoch [5/120    avg_loss:1.865, val_acc:0.595]
Epoch [6/120    avg_loss:1.679, val_acc:0.615]
Epoch [7/120    avg_loss:1.528, val_acc:0.665]
Epoch [8/120    avg_loss:1.337, val_acc:0.700]
Epoch [9/120    avg_loss:1.221, val_acc:0.722]
Epoch [10/120    avg_loss:1.118, val_acc:0.764]
Epoch [11/120    avg_loss:0.996, val_acc:0.823]
Epoch [12/120    avg_loss:0.890, val_acc:0.827]
Epoch [13/120    avg_loss:0.799, val_acc:0.851]
Epoch [14/120    avg_loss:0.732, val_acc:0.845]
Epoch [15/120    avg_loss:0.725, val_acc:0.845]
Epoch [16/120    avg_loss:0.677, val_acc:0.827]
Epoch [17/120    avg_loss:0.600, val_acc:0.891]
Epoch [18/120    avg_loss:0.582, val_acc:0.879]
Epoch [19/120    avg_loss:0.658, val_acc:0.778]
Epoch [20/120    avg_loss:0.578, val_acc:0.919]
Epoch [21/120    avg_loss:0.488, val_acc:0.940]
Epoch [22/120    avg_loss:0.464, val_acc:0.921]
Epoch [23/120    avg_loss:0.435, val_acc:0.925]
Epoch [24/120    avg_loss:0.421, val_acc:0.937]
Epoch [25/120    avg_loss:0.390, val_acc:0.917]
Epoch [26/120    avg_loss:0.421, val_acc:0.921]
Epoch [27/120    avg_loss:0.409, val_acc:0.859]
Epoch [28/120    avg_loss:0.363, val_acc:0.873]
Epoch [29/120    avg_loss:0.426, val_acc:0.911]
Epoch [30/120    avg_loss:0.392, val_acc:0.915]
Epoch [31/120    avg_loss:0.363, val_acc:0.942]
Epoch [32/120    avg_loss:0.324, val_acc:0.937]
Epoch [33/120    avg_loss:0.308, val_acc:0.915]
Epoch [34/120    avg_loss:0.325, val_acc:0.919]
Epoch [35/120    avg_loss:0.301, val_acc:0.946]
Epoch [36/120    avg_loss:0.309, val_acc:0.960]
Epoch [37/120    avg_loss:0.288, val_acc:0.958]
Epoch [38/120    avg_loss:0.268, val_acc:0.911]
Epoch [39/120    avg_loss:0.223, val_acc:0.942]
Epoch [40/120    avg_loss:0.233, val_acc:0.970]
Epoch [41/120    avg_loss:0.222, val_acc:0.944]
Epoch [42/120    avg_loss:0.196, val_acc:0.907]
Epoch [43/120    avg_loss:0.214, val_acc:0.946]
Epoch [44/120    avg_loss:0.202, val_acc:0.931]
Epoch [45/120    avg_loss:0.190, val_acc:0.972]
Epoch [46/120    avg_loss:0.178, val_acc:0.940]
Epoch [47/120    avg_loss:0.199, val_acc:0.962]
Epoch [48/120    avg_loss:0.167, val_acc:0.970]
Epoch [49/120    avg_loss:0.221, val_acc:0.952]
Epoch [50/120    avg_loss:0.171, val_acc:0.968]
Epoch [51/120    avg_loss:0.133, val_acc:0.976]
Epoch [52/120    avg_loss:0.148, val_acc:0.952]
Epoch [53/120    avg_loss:0.139, val_acc:0.972]
Epoch [54/120    avg_loss:0.168, val_acc:0.966]
Epoch [55/120    avg_loss:0.166, val_acc:0.964]
Epoch [56/120    avg_loss:0.168, val_acc:0.952]
Epoch [57/120    avg_loss:0.176, val_acc:0.972]
Epoch [58/120    avg_loss:0.115, val_acc:0.962]
Epoch [59/120    avg_loss:0.115, val_acc:0.980]
Epoch [60/120    avg_loss:0.162, val_acc:0.940]
Epoch [61/120    avg_loss:0.107, val_acc:0.974]
Epoch [62/120    avg_loss:0.116, val_acc:0.972]
Epoch [63/120    avg_loss:0.130, val_acc:0.966]
Epoch [64/120    avg_loss:0.102, val_acc:0.978]
Epoch [65/120    avg_loss:0.089, val_acc:0.972]
Epoch [66/120    avg_loss:0.107, val_acc:0.982]
Epoch [67/120    avg_loss:0.095, val_acc:0.972]
Epoch [68/120    avg_loss:0.115, val_acc:0.980]
Epoch [69/120    avg_loss:0.094, val_acc:0.978]
Epoch [70/120    avg_loss:0.071, val_acc:0.988]
Epoch [71/120    avg_loss:0.092, val_acc:0.986]
Epoch [72/120    avg_loss:0.094, val_acc:0.988]
Epoch [73/120    avg_loss:0.078, val_acc:0.990]
Epoch [74/120    avg_loss:0.070, val_acc:0.992]
Epoch [75/120    avg_loss:0.070, val_acc:0.972]
Epoch [76/120    avg_loss:0.057, val_acc:0.988]
Epoch [77/120    avg_loss:0.078, val_acc:0.982]
Epoch [78/120    avg_loss:0.095, val_acc:0.976]
Epoch [79/120    avg_loss:0.066, val_acc:0.988]
Epoch [80/120    avg_loss:0.052, val_acc:0.992]
Epoch [81/120    avg_loss:0.097, val_acc:0.968]
Epoch [82/120    avg_loss:0.107, val_acc:0.978]
Epoch [83/120    avg_loss:0.088, val_acc:0.968]
Epoch [84/120    avg_loss:0.057, val_acc:0.986]
Epoch [85/120    avg_loss:0.071, val_acc:0.992]
Epoch [86/120    avg_loss:0.046, val_acc:0.986]
Epoch [87/120    avg_loss:0.048, val_acc:0.988]
Epoch [88/120    avg_loss:0.041, val_acc:0.990]
Epoch [89/120    avg_loss:0.039, val_acc:0.980]
Epoch [90/120    avg_loss:0.029, val_acc:0.990]
Epoch [91/120    avg_loss:0.035, val_acc:0.988]
Epoch [92/120    avg_loss:0.039, val_acc:0.990]
Epoch [93/120    avg_loss:0.047, val_acc:0.990]
Epoch [94/120    avg_loss:0.047, val_acc:0.964]
Epoch [95/120    avg_loss:0.040, val_acc:0.980]
Epoch [96/120    avg_loss:0.023, val_acc:1.000]
Epoch [97/120    avg_loss:0.026, val_acc:0.990]
Epoch [98/120    avg_loss:0.024, val_acc:0.996]
Epoch [99/120    avg_loss:0.020, val_acc:0.990]
Epoch [100/120    avg_loss:0.023, val_acc:0.998]
Epoch [101/120    avg_loss:0.048, val_acc:0.972]
Epoch [102/120    avg_loss:0.084, val_acc:0.972]
Epoch [103/120    avg_loss:0.074, val_acc:0.968]
Epoch [104/120    avg_loss:0.066, val_acc:0.980]
Epoch [105/120    avg_loss:0.031, val_acc:0.992]
Epoch [106/120    avg_loss:0.030, val_acc:0.992]
Epoch [107/120    avg_loss:0.027, val_acc:0.998]
Epoch [108/120    avg_loss:0.032, val_acc:0.992]
Epoch [109/120    avg_loss:0.041, val_acc:0.988]
Epoch [110/120    avg_loss:0.032, val_acc:0.988]
Epoch [111/120    avg_loss:0.031, val_acc:0.992]
Epoch [112/120    avg_loss:0.020, val_acc:0.994]
Epoch [113/120    avg_loss:0.018, val_acc:0.994]
Epoch [114/120    avg_loss:0.027, val_acc:0.994]
Epoch [115/120    avg_loss:0.023, val_acc:0.996]
Epoch [116/120    avg_loss:0.017, val_acc:0.994]
Epoch [117/120    avg_loss:0.016, val_acc:0.994]
Epoch [118/120    avg_loss:0.014, val_acc:0.994]
Epoch [119/120    avg_loss:0.022, val_acc:0.994]
Epoch [120/120    avg_loss:0.013, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  11   0   0   0   0   0   0   1   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 0.99926954 0.98426966 0.99782135 0.94091904 0.90909091
 0.99757869 0.96703297 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.991691306763332
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7eb51d69e8>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.520, val_acc:0.406]
Epoch [2/120    avg_loss:2.272, val_acc:0.417]
Epoch [3/120    avg_loss:2.092, val_acc:0.467]
Epoch [4/120    avg_loss:1.918, val_acc:0.523]
Epoch [5/120    avg_loss:1.786, val_acc:0.579]
Epoch [6/120    avg_loss:1.649, val_acc:0.675]
Epoch [7/120    avg_loss:1.484, val_acc:0.683]
Epoch [8/120    avg_loss:1.366, val_acc:0.690]
Epoch [9/120    avg_loss:1.211, val_acc:0.721]
Epoch [10/120    avg_loss:1.123, val_acc:0.721]
Epoch [11/120    avg_loss:1.026, val_acc:0.735]
Epoch [12/120    avg_loss:0.922, val_acc:0.767]
Epoch [13/120    avg_loss:0.856, val_acc:0.783]
Epoch [14/120    avg_loss:0.743, val_acc:0.771]
Epoch [15/120    avg_loss:0.696, val_acc:0.831]
Epoch [16/120    avg_loss:0.661, val_acc:0.860]
Epoch [17/120    avg_loss:0.665, val_acc:0.858]
Epoch [18/120    avg_loss:0.569, val_acc:0.863]
Epoch [19/120    avg_loss:0.522, val_acc:0.835]
Epoch [20/120    avg_loss:0.492, val_acc:0.860]
Epoch [21/120    avg_loss:0.519, val_acc:0.929]
Epoch [22/120    avg_loss:0.544, val_acc:0.896]
Epoch [23/120    avg_loss:0.491, val_acc:0.944]
Epoch [24/120    avg_loss:0.468, val_acc:0.890]
Epoch [25/120    avg_loss:0.419, val_acc:0.919]
Epoch [26/120    avg_loss:0.396, val_acc:0.869]
Epoch [27/120    avg_loss:0.338, val_acc:0.967]
Epoch [28/120    avg_loss:0.314, val_acc:0.967]
Epoch [29/120    avg_loss:0.337, val_acc:0.946]
Epoch [30/120    avg_loss:0.296, val_acc:0.965]
Epoch [31/120    avg_loss:0.295, val_acc:0.954]
Epoch [32/120    avg_loss:0.339, val_acc:0.942]
Epoch [33/120    avg_loss:0.328, val_acc:0.954]
Epoch [34/120    avg_loss:0.234, val_acc:0.954]
Epoch [35/120    avg_loss:0.256, val_acc:0.952]
Epoch [36/120    avg_loss:0.240, val_acc:0.971]
Epoch [37/120    avg_loss:0.227, val_acc:0.944]
Epoch [38/120    avg_loss:0.275, val_acc:0.946]
Epoch [39/120    avg_loss:0.256, val_acc:0.938]
Epoch [40/120    avg_loss:0.204, val_acc:0.971]
Epoch [41/120    avg_loss:0.220, val_acc:0.965]
Epoch [42/120    avg_loss:0.269, val_acc:0.954]
Epoch [43/120    avg_loss:0.173, val_acc:0.967]
Epoch [44/120    avg_loss:0.194, val_acc:0.967]
Epoch [45/120    avg_loss:0.210, val_acc:0.973]
Epoch [46/120    avg_loss:0.201, val_acc:0.977]
Epoch [47/120    avg_loss:0.175, val_acc:0.977]
Epoch [48/120    avg_loss:0.134, val_acc:0.977]
Epoch [49/120    avg_loss:0.122, val_acc:0.965]
Epoch [50/120    avg_loss:0.137, val_acc:0.975]
Epoch [51/120    avg_loss:0.124, val_acc:0.979]
Epoch [52/120    avg_loss:0.096, val_acc:0.977]
Epoch [53/120    avg_loss:0.183, val_acc:0.975]
Epoch [54/120    avg_loss:0.127, val_acc:0.981]
Epoch [55/120    avg_loss:0.118, val_acc:0.983]
Epoch [56/120    avg_loss:0.097, val_acc:0.960]
Epoch [57/120    avg_loss:0.117, val_acc:0.977]
Epoch [58/120    avg_loss:0.115, val_acc:0.952]
Epoch [59/120    avg_loss:0.123, val_acc:0.973]
Epoch [60/120    avg_loss:0.134, val_acc:0.977]
Epoch [61/120    avg_loss:0.131, val_acc:0.981]
Epoch [62/120    avg_loss:0.123, val_acc:0.979]
Epoch [63/120    avg_loss:0.183, val_acc:0.965]
Epoch [64/120    avg_loss:0.183, val_acc:0.963]
Epoch [65/120    avg_loss:0.122, val_acc:0.971]
Epoch [66/120    avg_loss:0.084, val_acc:0.969]
Epoch [67/120    avg_loss:0.097, val_acc:0.979]
Epoch [68/120    avg_loss:0.092, val_acc:0.977]
Epoch [69/120    avg_loss:0.074, val_acc:0.981]
Epoch [70/120    avg_loss:0.074, val_acc:0.981]
Epoch [71/120    avg_loss:0.054, val_acc:0.981]
Epoch [72/120    avg_loss:0.057, val_acc:0.981]
Epoch [73/120    avg_loss:0.067, val_acc:0.983]
Epoch [74/120    avg_loss:0.054, val_acc:0.981]
Epoch [75/120    avg_loss:0.065, val_acc:0.983]
Epoch [76/120    avg_loss:0.064, val_acc:0.983]
Epoch [77/120    avg_loss:0.051, val_acc:0.985]
Epoch [78/120    avg_loss:0.059, val_acc:0.981]
Epoch [79/120    avg_loss:0.051, val_acc:0.981]
Epoch [80/120    avg_loss:0.047, val_acc:0.981]
Epoch [81/120    avg_loss:0.051, val_acc:0.981]
Epoch [82/120    avg_loss:0.061, val_acc:0.981]
Epoch [83/120    avg_loss:0.051, val_acc:0.981]
Epoch [84/120    avg_loss:0.043, val_acc:0.981]
Epoch [85/120    avg_loss:0.045, val_acc:0.981]
Epoch [86/120    avg_loss:0.040, val_acc:0.983]
Epoch [87/120    avg_loss:0.048, val_acc:0.983]
Epoch [88/120    avg_loss:0.046, val_acc:0.983]
Epoch [89/120    avg_loss:0.053, val_acc:0.983]
Epoch [90/120    avg_loss:0.049, val_acc:0.983]
Epoch [91/120    avg_loss:0.042, val_acc:0.983]
Epoch [92/120    avg_loss:0.052, val_acc:0.985]
Epoch [93/120    avg_loss:0.043, val_acc:0.985]
Epoch [94/120    avg_loss:0.046, val_acc:0.985]
Epoch [95/120    avg_loss:0.043, val_acc:0.985]
Epoch [96/120    avg_loss:0.048, val_acc:0.985]
Epoch [97/120    avg_loss:0.043, val_acc:0.985]
Epoch [98/120    avg_loss:0.043, val_acc:0.985]
Epoch [99/120    avg_loss:0.045, val_acc:0.985]
Epoch [100/120    avg_loss:0.038, val_acc:0.985]
Epoch [101/120    avg_loss:0.044, val_acc:0.983]
Epoch [102/120    avg_loss:0.040, val_acc:0.983]
Epoch [103/120    avg_loss:0.046, val_acc:0.983]
Epoch [104/120    avg_loss:0.043, val_acc:0.983]
Epoch [105/120    avg_loss:0.046, val_acc:0.985]
Epoch [106/120    avg_loss:0.047, val_acc:0.985]
Epoch [107/120    avg_loss:0.033, val_acc:0.983]
Epoch [108/120    avg_loss:0.043, val_acc:0.983]
Epoch [109/120    avg_loss:0.039, val_acc:0.983]
Epoch [110/120    avg_loss:0.038, val_acc:0.985]
Epoch [111/120    avg_loss:0.042, val_acc:0.985]
Epoch [112/120    avg_loss:0.048, val_acc:0.985]
Epoch [113/120    avg_loss:0.052, val_acc:0.985]
Epoch [114/120    avg_loss:0.049, val_acc:0.985]
Epoch [115/120    avg_loss:0.048, val_acc:0.985]
Epoch [116/120    avg_loss:0.061, val_acc:0.985]
Epoch [117/120    avg_loss:0.039, val_acc:0.983]
Epoch [118/120    avg_loss:0.042, val_acc:0.983]
Epoch [119/120    avg_loss:0.047, val_acc:0.983]
Epoch [120/120    avg_loss:0.042, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  17   0   0   0   0   0   0   3   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 1.         0.98426966 1.         0.91390728 0.875
 1.         0.96132597 1.         1.         1.         0.99472296
 0.99226519 1.        ]

Kappa:
0.9881300783460385
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9f03d678d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.567, val_acc:0.319]
Epoch [2/120    avg_loss:2.281, val_acc:0.375]
Epoch [3/120    avg_loss:2.082, val_acc:0.531]
Epoch [4/120    avg_loss:1.900, val_acc:0.581]
Epoch [5/120    avg_loss:1.753, val_acc:0.650]
Epoch [6/120    avg_loss:1.607, val_acc:0.677]
Epoch [7/120    avg_loss:1.443, val_acc:0.690]
Epoch [8/120    avg_loss:1.337, val_acc:0.692]
Epoch [9/120    avg_loss:1.202, val_acc:0.723]
Epoch [10/120    avg_loss:1.085, val_acc:0.733]
Epoch [11/120    avg_loss:1.003, val_acc:0.781]
Epoch [12/120    avg_loss:0.907, val_acc:0.756]
Epoch [13/120    avg_loss:0.869, val_acc:0.779]
Epoch [14/120    avg_loss:0.813, val_acc:0.783]
Epoch [15/120    avg_loss:0.758, val_acc:0.783]
Epoch [16/120    avg_loss:0.722, val_acc:0.825]
Epoch [17/120    avg_loss:0.658, val_acc:0.815]
Epoch [18/120    avg_loss:0.643, val_acc:0.856]
Epoch [19/120    avg_loss:0.686, val_acc:0.867]
Epoch [20/120    avg_loss:0.584, val_acc:0.792]
Epoch [21/120    avg_loss:0.529, val_acc:0.883]
Epoch [22/120    avg_loss:0.506, val_acc:0.885]
Epoch [23/120    avg_loss:0.492, val_acc:0.887]
Epoch [24/120    avg_loss:0.534, val_acc:0.881]
Epoch [25/120    avg_loss:0.505, val_acc:0.894]
Epoch [26/120    avg_loss:0.503, val_acc:0.933]
Epoch [27/120    avg_loss:0.420, val_acc:0.910]
Epoch [28/120    avg_loss:0.383, val_acc:0.877]
Epoch [29/120    avg_loss:0.488, val_acc:0.883]
Epoch [30/120    avg_loss:0.394, val_acc:0.929]
Epoch [31/120    avg_loss:0.391, val_acc:0.921]
Epoch [32/120    avg_loss:0.370, val_acc:0.933]
Epoch [33/120    avg_loss:0.332, val_acc:0.942]
Epoch [34/120    avg_loss:0.312, val_acc:0.971]
Epoch [35/120    avg_loss:0.285, val_acc:0.940]
Epoch [36/120    avg_loss:0.266, val_acc:0.965]
Epoch [37/120    avg_loss:0.256, val_acc:0.971]
Epoch [38/120    avg_loss:0.231, val_acc:0.944]
Epoch [39/120    avg_loss:0.239, val_acc:0.940]
Epoch [40/120    avg_loss:0.257, val_acc:0.952]
Epoch [41/120    avg_loss:0.222, val_acc:0.963]
Epoch [42/120    avg_loss:0.232, val_acc:0.954]
Epoch [43/120    avg_loss:0.249, val_acc:0.983]
Epoch [44/120    avg_loss:0.237, val_acc:0.967]
Epoch [45/120    avg_loss:0.189, val_acc:0.979]
Epoch [46/120    avg_loss:0.310, val_acc:0.963]
Epoch [47/120    avg_loss:0.237, val_acc:0.981]
Epoch [48/120    avg_loss:0.206, val_acc:0.971]
Epoch [49/120    avg_loss:0.175, val_acc:0.979]
Epoch [50/120    avg_loss:0.170, val_acc:0.965]
Epoch [51/120    avg_loss:0.192, val_acc:0.977]
Epoch [52/120    avg_loss:0.147, val_acc:0.977]
Epoch [53/120    avg_loss:0.138, val_acc:0.971]
Epoch [54/120    avg_loss:0.178, val_acc:0.981]
Epoch [55/120    avg_loss:0.134, val_acc:0.985]
Epoch [56/120    avg_loss:0.165, val_acc:0.985]
Epoch [57/120    avg_loss:0.119, val_acc:0.983]
Epoch [58/120    avg_loss:0.118, val_acc:0.971]
Epoch [59/120    avg_loss:0.120, val_acc:0.981]
Epoch [60/120    avg_loss:0.162, val_acc:0.985]
Epoch [61/120    avg_loss:0.133, val_acc:0.988]
Epoch [62/120    avg_loss:0.118, val_acc:0.985]
Epoch [63/120    avg_loss:0.108, val_acc:0.973]
Epoch [64/120    avg_loss:0.130, val_acc:0.988]
Epoch [65/120    avg_loss:0.089, val_acc:0.977]
Epoch [66/120    avg_loss:0.088, val_acc:0.992]
Epoch [67/120    avg_loss:0.118, val_acc:0.985]
Epoch [68/120    avg_loss:0.102, val_acc:0.981]
Epoch [69/120    avg_loss:0.117, val_acc:0.983]
Epoch [70/120    avg_loss:0.134, val_acc:0.985]
Epoch [71/120    avg_loss:0.101, val_acc:0.985]
Epoch [72/120    avg_loss:0.084, val_acc:0.992]
Epoch [73/120    avg_loss:0.059, val_acc:0.979]
Epoch [74/120    avg_loss:0.059, val_acc:0.988]
Epoch [75/120    avg_loss:0.079, val_acc:0.985]
Epoch [76/120    avg_loss:0.067, val_acc:0.985]
Epoch [77/120    avg_loss:0.081, val_acc:0.975]
Epoch [78/120    avg_loss:0.083, val_acc:0.992]
Epoch [79/120    avg_loss:0.065, val_acc:0.988]
Epoch [80/120    avg_loss:0.067, val_acc:0.981]
Epoch [81/120    avg_loss:0.117, val_acc:0.992]
Epoch [82/120    avg_loss:0.110, val_acc:0.985]
Epoch [83/120    avg_loss:0.066, val_acc:0.990]
Epoch [84/120    avg_loss:0.070, val_acc:0.988]
Epoch [85/120    avg_loss:0.078, val_acc:0.994]
Epoch [86/120    avg_loss:0.109, val_acc:0.981]
Epoch [87/120    avg_loss:0.089, val_acc:0.983]
Epoch [88/120    avg_loss:0.072, val_acc:0.985]
Epoch [89/120    avg_loss:0.068, val_acc:0.988]
Epoch [90/120    avg_loss:0.064, val_acc:0.979]
Epoch [91/120    avg_loss:0.054, val_acc:0.990]
Epoch [92/120    avg_loss:0.073, val_acc:0.990]
Epoch [93/120    avg_loss:0.087, val_acc:0.969]
Epoch [94/120    avg_loss:0.132, val_acc:0.979]
Epoch [95/120    avg_loss:0.088, val_acc:0.992]
Epoch [96/120    avg_loss:0.064, val_acc:0.988]
Epoch [97/120    avg_loss:0.042, val_acc:0.990]
Epoch [98/120    avg_loss:0.041, val_acc:0.985]
Epoch [99/120    avg_loss:0.042, val_acc:0.992]
Epoch [100/120    avg_loss:0.027, val_acc:0.992]
Epoch [101/120    avg_loss:0.028, val_acc:0.994]
Epoch [102/120    avg_loss:0.035, val_acc:0.994]
Epoch [103/120    avg_loss:0.030, val_acc:0.994]
Epoch [104/120    avg_loss:0.032, val_acc:0.994]
Epoch [105/120    avg_loss:0.035, val_acc:0.992]
Epoch [106/120    avg_loss:0.032, val_acc:0.990]
Epoch [107/120    avg_loss:0.028, val_acc:0.994]
Epoch [108/120    avg_loss:0.022, val_acc:0.994]
Epoch [109/120    avg_loss:0.029, val_acc:0.994]
Epoch [110/120    avg_loss:0.028, val_acc:0.994]
Epoch [111/120    avg_loss:0.032, val_acc:0.994]
Epoch [112/120    avg_loss:0.025, val_acc:0.994]
Epoch [113/120    avg_loss:0.028, val_acc:0.994]
Epoch [114/120    avg_loss:0.024, val_acc:0.994]
Epoch [115/120    avg_loss:0.033, val_acc:0.994]
Epoch [116/120    avg_loss:0.023, val_acc:0.994]
Epoch [117/120    avg_loss:0.029, val_acc:0.994]
Epoch [118/120    avg_loss:0.028, val_acc:0.994]
Epoch [119/120    avg_loss:0.032, val_acc:0.994]
Epoch [120/120    avg_loss:0.027, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  16   0   0   0   0   0   0   2   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 0.99780541 0.98648649 1.         0.92682927 0.89347079
 0.99277108 0.96703297 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9900301388628862
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f50e1b1c898>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.548, val_acc:0.404]
Epoch [2/120    avg_loss:2.290, val_acc:0.529]
Epoch [3/120    avg_loss:2.095, val_acc:0.629]
Epoch [4/120    avg_loss:1.921, val_acc:0.596]
Epoch [5/120    avg_loss:1.783, val_acc:0.646]
Epoch [6/120    avg_loss:1.608, val_acc:0.665]
Epoch [7/120    avg_loss:1.463, val_acc:0.681]
Epoch [8/120    avg_loss:1.310, val_acc:0.704]
Epoch [9/120    avg_loss:1.179, val_acc:0.725]
Epoch [10/120    avg_loss:1.061, val_acc:0.744]
Epoch [11/120    avg_loss:0.970, val_acc:0.750]
Epoch [12/120    avg_loss:0.889, val_acc:0.802]
Epoch [13/120    avg_loss:0.877, val_acc:0.775]
Epoch [14/120    avg_loss:0.746, val_acc:0.854]
Epoch [15/120    avg_loss:0.691, val_acc:0.890]
Epoch [16/120    avg_loss:0.613, val_acc:0.858]
Epoch [17/120    avg_loss:0.580, val_acc:0.927]
Epoch [18/120    avg_loss:0.526, val_acc:0.908]
Epoch [19/120    avg_loss:0.557, val_acc:0.921]
Epoch [20/120    avg_loss:0.538, val_acc:0.927]
Epoch [21/120    avg_loss:0.497, val_acc:0.923]
Epoch [22/120    avg_loss:0.456, val_acc:0.935]
Epoch [23/120    avg_loss:0.397, val_acc:0.942]
Epoch [24/120    avg_loss:0.376, val_acc:0.935]
Epoch [25/120    avg_loss:0.344, val_acc:0.925]
Epoch [26/120    avg_loss:0.385, val_acc:0.952]
Epoch [27/120    avg_loss:0.348, val_acc:0.938]
Epoch [28/120    avg_loss:0.329, val_acc:0.954]
Epoch [29/120    avg_loss:0.298, val_acc:0.929]
Epoch [30/120    avg_loss:0.351, val_acc:0.938]
Epoch [31/120    avg_loss:0.324, val_acc:0.952]
Epoch [32/120    avg_loss:0.263, val_acc:0.948]
Epoch [33/120    avg_loss:0.305, val_acc:0.933]
Epoch [34/120    avg_loss:0.260, val_acc:0.938]
Epoch [35/120    avg_loss:0.275, val_acc:0.965]
Epoch [36/120    avg_loss:0.229, val_acc:0.963]
Epoch [37/120    avg_loss:0.246, val_acc:0.963]
Epoch [38/120    avg_loss:0.227, val_acc:0.944]
Epoch [39/120    avg_loss:0.181, val_acc:0.960]
Epoch [40/120    avg_loss:0.186, val_acc:0.940]
Epoch [41/120    avg_loss:0.210, val_acc:0.954]
Epoch [42/120    avg_loss:0.216, val_acc:0.965]
Epoch [43/120    avg_loss:0.192, val_acc:0.965]
Epoch [44/120    avg_loss:0.161, val_acc:0.963]
Epoch [45/120    avg_loss:0.142, val_acc:0.975]
Epoch [46/120    avg_loss:0.153, val_acc:0.965]
Epoch [47/120    avg_loss:0.183, val_acc:0.965]
Epoch [48/120    avg_loss:0.165, val_acc:0.969]
Epoch [49/120    avg_loss:0.116, val_acc:0.971]
Epoch [50/120    avg_loss:0.151, val_acc:0.954]
Epoch [51/120    avg_loss:0.127, val_acc:0.969]
Epoch [52/120    avg_loss:0.162, val_acc:0.963]
Epoch [53/120    avg_loss:0.120, val_acc:0.975]
Epoch [54/120    avg_loss:0.128, val_acc:0.985]
Epoch [55/120    avg_loss:0.121, val_acc:0.971]
Epoch [56/120    avg_loss:0.119, val_acc:0.979]
Epoch [57/120    avg_loss:0.106, val_acc:0.988]
Epoch [58/120    avg_loss:0.117, val_acc:0.963]
Epoch [59/120    avg_loss:0.107, val_acc:0.971]
Epoch [60/120    avg_loss:0.130, val_acc:0.981]
Epoch [61/120    avg_loss:0.115, val_acc:0.977]
Epoch [62/120    avg_loss:0.115, val_acc:0.981]
Epoch [63/120    avg_loss:0.145, val_acc:0.975]
Epoch [64/120    avg_loss:0.113, val_acc:0.975]
Epoch [65/120    avg_loss:0.097, val_acc:0.981]
Epoch [66/120    avg_loss:0.102, val_acc:0.981]
Epoch [67/120    avg_loss:0.091, val_acc:0.985]
Epoch [68/120    avg_loss:0.102, val_acc:0.977]
Epoch [69/120    avg_loss:0.078, val_acc:0.983]
Epoch [70/120    avg_loss:0.065, val_acc:0.981]
Epoch [71/120    avg_loss:0.049, val_acc:0.981]
Epoch [72/120    avg_loss:0.047, val_acc:0.981]
Epoch [73/120    avg_loss:0.051, val_acc:0.983]
Epoch [74/120    avg_loss:0.039, val_acc:0.981]
Epoch [75/120    avg_loss:0.041, val_acc:0.983]
Epoch [76/120    avg_loss:0.044, val_acc:0.983]
Epoch [77/120    avg_loss:0.049, val_acc:0.983]
Epoch [78/120    avg_loss:0.051, val_acc:0.983]
Epoch [79/120    avg_loss:0.037, val_acc:0.983]
Epoch [80/120    avg_loss:0.039, val_acc:0.983]
Epoch [81/120    avg_loss:0.043, val_acc:0.983]
Epoch [82/120    avg_loss:0.036, val_acc:0.988]
Epoch [83/120    avg_loss:0.038, val_acc:0.985]
Epoch [84/120    avg_loss:0.038, val_acc:0.985]
Epoch [85/120    avg_loss:0.036, val_acc:0.985]
Epoch [86/120    avg_loss:0.037, val_acc:0.985]
Epoch [87/120    avg_loss:0.036, val_acc:0.983]
Epoch [88/120    avg_loss:0.040, val_acc:0.985]
Epoch [89/120    avg_loss:0.037, val_acc:0.981]
Epoch [90/120    avg_loss:0.036, val_acc:0.983]
Epoch [91/120    avg_loss:0.034, val_acc:0.981]
Epoch [92/120    avg_loss:0.032, val_acc:0.981]
Epoch [93/120    avg_loss:0.034, val_acc:0.985]
Epoch [94/120    avg_loss:0.038, val_acc:0.983]
Epoch [95/120    avg_loss:0.043, val_acc:0.985]
Epoch [96/120    avg_loss:0.031, val_acc:0.985]
Epoch [97/120    avg_loss:0.032, val_acc:0.985]
Epoch [98/120    avg_loss:0.033, val_acc:0.985]
Epoch [99/120    avg_loss:0.033, val_acc:0.985]
Epoch [100/120    avg_loss:0.040, val_acc:0.985]
Epoch [101/120    avg_loss:0.036, val_acc:0.981]
Epoch [102/120    avg_loss:0.034, val_acc:0.981]
Epoch [103/120    avg_loss:0.033, val_acc:0.981]
Epoch [104/120    avg_loss:0.030, val_acc:0.981]
Epoch [105/120    avg_loss:0.037, val_acc:0.981]
Epoch [106/120    avg_loss:0.036, val_acc:0.981]
Epoch [107/120    avg_loss:0.033, val_acc:0.981]
Epoch [108/120    avg_loss:0.041, val_acc:0.981]
Epoch [109/120    avg_loss:0.034, val_acc:0.981]
Epoch [110/120    avg_loss:0.035, val_acc:0.981]
Epoch [111/120    avg_loss:0.031, val_acc:0.981]
Epoch [112/120    avg_loss:0.032, val_acc:0.981]
Epoch [113/120    avg_loss:0.029, val_acc:0.981]
Epoch [114/120    avg_loss:0.028, val_acc:0.981]
Epoch [115/120    avg_loss:0.038, val_acc:0.981]
Epoch [116/120    avg_loss:0.034, val_acc:0.981]
Epoch [117/120    avg_loss:0.038, val_acc:0.981]
Epoch [118/120    avg_loss:0.030, val_acc:0.981]
Epoch [119/120    avg_loss:0.031, val_acc:0.981]
Epoch [120/120    avg_loss:0.042, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   6 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 0.99853801 0.98648649 1.         0.9375     0.90540541
 0.99516908 0.96703297 1.         1.         1.         0.99210526
 0.99333333 1.        ]

Kappa:
0.9900305622129257
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f124d6708d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.570, val_acc:0.317]
Epoch [2/120    avg_loss:2.273, val_acc:0.412]
Epoch [3/120    avg_loss:2.126, val_acc:0.510]
Epoch [4/120    avg_loss:1.966, val_acc:0.529]
Epoch [5/120    avg_loss:1.798, val_acc:0.552]
Epoch [6/120    avg_loss:1.623, val_acc:0.606]
Epoch [7/120    avg_loss:1.464, val_acc:0.637]
Epoch [8/120    avg_loss:1.341, val_acc:0.677]
Epoch [9/120    avg_loss:1.242, val_acc:0.704]
Epoch [10/120    avg_loss:1.094, val_acc:0.758]
Epoch [11/120    avg_loss:1.043, val_acc:0.800]
Epoch [12/120    avg_loss:0.946, val_acc:0.858]
Epoch [13/120    avg_loss:0.873, val_acc:0.865]
Epoch [14/120    avg_loss:0.813, val_acc:0.804]
Epoch [15/120    avg_loss:0.767, val_acc:0.840]
Epoch [16/120    avg_loss:0.661, val_acc:0.833]
Epoch [17/120    avg_loss:0.668, val_acc:0.858]
Epoch [18/120    avg_loss:0.675, val_acc:0.910]
Epoch [19/120    avg_loss:0.557, val_acc:0.908]
Epoch [20/120    avg_loss:0.543, val_acc:0.873]
Epoch [21/120    avg_loss:0.449, val_acc:0.919]
Epoch [22/120    avg_loss:0.464, val_acc:0.933]
Epoch [23/120    avg_loss:0.425, val_acc:0.912]
Epoch [24/120    avg_loss:0.364, val_acc:0.940]
Epoch [25/120    avg_loss:0.406, val_acc:0.940]
Epoch [26/120    avg_loss:0.365, val_acc:0.938]
Epoch [27/120    avg_loss:0.311, val_acc:0.933]
Epoch [28/120    avg_loss:0.332, val_acc:0.950]
Epoch [29/120    avg_loss:0.279, val_acc:0.931]
Epoch [30/120    avg_loss:0.348, val_acc:0.912]
Epoch [31/120    avg_loss:0.311, val_acc:0.917]
Epoch [32/120    avg_loss:0.294, val_acc:0.942]
Epoch [33/120    avg_loss:0.260, val_acc:0.942]
Epoch [34/120    avg_loss:0.235, val_acc:0.931]
Epoch [35/120    avg_loss:0.244, val_acc:0.952]
Epoch [36/120    avg_loss:0.226, val_acc:0.952]
Epoch [37/120    avg_loss:0.263, val_acc:0.944]
Epoch [38/120    avg_loss:0.224, val_acc:0.956]
Epoch [39/120    avg_loss:0.219, val_acc:0.956]
Epoch [40/120    avg_loss:0.180, val_acc:0.933]
Epoch [41/120    avg_loss:0.226, val_acc:0.946]
Epoch [42/120    avg_loss:0.186, val_acc:0.948]
Epoch [43/120    avg_loss:0.179, val_acc:0.944]
Epoch [44/120    avg_loss:0.152, val_acc:0.967]
Epoch [45/120    avg_loss:0.164, val_acc:0.960]
Epoch [46/120    avg_loss:0.171, val_acc:0.938]
Epoch [47/120    avg_loss:0.197, val_acc:0.942]
Epoch [48/120    avg_loss:0.195, val_acc:0.969]
Epoch [49/120    avg_loss:0.144, val_acc:0.975]
Epoch [50/120    avg_loss:0.133, val_acc:0.971]
Epoch [51/120    avg_loss:0.126, val_acc:0.971]
Epoch [52/120    avg_loss:0.134, val_acc:0.985]
Epoch [53/120    avg_loss:0.124, val_acc:0.965]
Epoch [54/120    avg_loss:0.147, val_acc:0.952]
Epoch [55/120    avg_loss:0.161, val_acc:0.954]
Epoch [56/120    avg_loss:0.125, val_acc:0.963]
Epoch [57/120    avg_loss:0.116, val_acc:0.967]
Epoch [58/120    avg_loss:0.118, val_acc:0.973]
Epoch [59/120    avg_loss:0.129, val_acc:0.971]
Epoch [60/120    avg_loss:0.107, val_acc:0.935]
Epoch [61/120    avg_loss:0.170, val_acc:0.969]
Epoch [62/120    avg_loss:0.096, val_acc:0.985]
Epoch [63/120    avg_loss:0.078, val_acc:0.983]
Epoch [64/120    avg_loss:0.104, val_acc:0.965]
Epoch [65/120    avg_loss:0.093, val_acc:0.971]
Epoch [66/120    avg_loss:0.094, val_acc:0.971]
Epoch [67/120    avg_loss:0.114, val_acc:0.971]
Epoch [68/120    avg_loss:0.126, val_acc:0.981]
Epoch [69/120    avg_loss:0.129, val_acc:0.975]
Epoch [70/120    avg_loss:0.085, val_acc:0.977]
Epoch [71/120    avg_loss:0.066, val_acc:0.975]
Epoch [72/120    avg_loss:0.092, val_acc:0.979]
Epoch [73/120    avg_loss:0.091, val_acc:0.971]
Epoch [74/120    avg_loss:0.090, val_acc:0.971]
Epoch [75/120    avg_loss:0.060, val_acc:0.977]
Epoch [76/120    avg_loss:0.057, val_acc:0.983]
Epoch [77/120    avg_loss:0.049, val_acc:0.983]
Epoch [78/120    avg_loss:0.044, val_acc:0.983]
Epoch [79/120    avg_loss:0.044, val_acc:0.983]
Epoch [80/120    avg_loss:0.037, val_acc:0.983]
Epoch [81/120    avg_loss:0.035, val_acc:0.983]
Epoch [82/120    avg_loss:0.034, val_acc:0.983]
Epoch [83/120    avg_loss:0.038, val_acc:0.983]
Epoch [84/120    avg_loss:0.038, val_acc:0.983]
Epoch [85/120    avg_loss:0.035, val_acc:0.983]
Epoch [86/120    avg_loss:0.036, val_acc:0.983]
Epoch [87/120    avg_loss:0.038, val_acc:0.983]
Epoch [88/120    avg_loss:0.031, val_acc:0.985]
Epoch [89/120    avg_loss:0.040, val_acc:0.985]
Epoch [90/120    avg_loss:0.032, val_acc:0.985]
Epoch [91/120    avg_loss:0.041, val_acc:0.985]
Epoch [92/120    avg_loss:0.032, val_acc:0.985]
Epoch [93/120    avg_loss:0.031, val_acc:0.985]
Epoch [94/120    avg_loss:0.035, val_acc:0.985]
Epoch [95/120    avg_loss:0.032, val_acc:0.985]
Epoch [96/120    avg_loss:0.034, val_acc:0.985]
Epoch [97/120    avg_loss:0.026, val_acc:0.985]
Epoch [98/120    avg_loss:0.032, val_acc:0.985]
Epoch [99/120    avg_loss:0.036, val_acc:0.985]
Epoch [100/120    avg_loss:0.043, val_acc:0.985]
Epoch [101/120    avg_loss:0.034, val_acc:0.985]
Epoch [102/120    avg_loss:0.035, val_acc:0.985]
Epoch [103/120    avg_loss:0.032, val_acc:0.983]
Epoch [104/120    avg_loss:0.028, val_acc:0.983]
Epoch [105/120    avg_loss:0.029, val_acc:0.985]
Epoch [106/120    avg_loss:0.028, val_acc:0.985]
Epoch [107/120    avg_loss:0.025, val_acc:0.985]
Epoch [108/120    avg_loss:0.039, val_acc:0.985]
Epoch [109/120    avg_loss:0.031, val_acc:0.985]
Epoch [110/120    avg_loss:0.034, val_acc:0.985]
Epoch [111/120    avg_loss:0.029, val_acc:0.985]
Epoch [112/120    avg_loss:0.032, val_acc:0.985]
Epoch [113/120    avg_loss:0.027, val_acc:0.985]
Epoch [114/120    avg_loss:0.034, val_acc:0.985]
Epoch [115/120    avg_loss:0.032, val_acc:0.985]
Epoch [116/120    avg_loss:0.024, val_acc:0.985]
Epoch [117/120    avg_loss:0.029, val_acc:0.985]
Epoch [118/120    avg_loss:0.029, val_acc:0.985]
Epoch [119/120    avg_loss:0.029, val_acc:0.985]
Epoch [120/120    avg_loss:0.033, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 226   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 0.99926954 0.98206278 0.99122807 0.94298246 0.9209622
 0.99757869 0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9916915254470632
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f63f45c6940>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.516, val_acc:0.381]
Epoch [2/120    avg_loss:2.243, val_acc:0.498]
Epoch [3/120    avg_loss:2.088, val_acc:0.540]
Epoch [4/120    avg_loss:1.933, val_acc:0.594]
Epoch [5/120    avg_loss:1.771, val_acc:0.613]
Epoch [6/120    avg_loss:1.618, val_acc:0.656]
Epoch [7/120    avg_loss:1.460, val_acc:0.656]
Epoch [8/120    avg_loss:1.333, val_acc:0.700]
Epoch [9/120    avg_loss:1.178, val_acc:0.706]
Epoch [10/120    avg_loss:1.096, val_acc:0.744]
Epoch [11/120    avg_loss:1.016, val_acc:0.762]
Epoch [12/120    avg_loss:0.911, val_acc:0.746]
Epoch [13/120    avg_loss:0.830, val_acc:0.769]
Epoch [14/120    avg_loss:0.784, val_acc:0.785]
Epoch [15/120    avg_loss:0.698, val_acc:0.769]
Epoch [16/120    avg_loss:0.715, val_acc:0.800]
Epoch [17/120    avg_loss:0.645, val_acc:0.877]
Epoch [18/120    avg_loss:0.580, val_acc:0.879]
Epoch [19/120    avg_loss:0.554, val_acc:0.827]
Epoch [20/120    avg_loss:0.506, val_acc:0.844]
Epoch [21/120    avg_loss:0.453, val_acc:0.869]
Epoch [22/120    avg_loss:0.507, val_acc:0.912]
Epoch [23/120    avg_loss:0.473, val_acc:0.912]
Epoch [24/120    avg_loss:0.390, val_acc:0.915]
Epoch [25/120    avg_loss:0.438, val_acc:0.919]
Epoch [26/120    avg_loss:0.371, val_acc:0.931]
Epoch [27/120    avg_loss:0.299, val_acc:0.942]
Epoch [28/120    avg_loss:0.288, val_acc:0.944]
Epoch [29/120    avg_loss:0.329, val_acc:0.898]
Epoch [30/120    avg_loss:0.380, val_acc:0.935]
Epoch [31/120    avg_loss:0.319, val_acc:0.942]
Epoch [32/120    avg_loss:0.291, val_acc:0.931]
Epoch [33/120    avg_loss:0.267, val_acc:0.894]
Epoch [34/120    avg_loss:0.300, val_acc:0.952]
Epoch [35/120    avg_loss:0.274, val_acc:0.946]
Epoch [36/120    avg_loss:0.255, val_acc:0.938]
Epoch [37/120    avg_loss:0.218, val_acc:0.933]
Epoch [38/120    avg_loss:0.205, val_acc:0.952]
Epoch [39/120    avg_loss:0.235, val_acc:0.877]
Epoch [40/120    avg_loss:0.207, val_acc:0.958]
Epoch [41/120    avg_loss:0.194, val_acc:0.958]
Epoch [42/120    avg_loss:0.224, val_acc:0.946]
Epoch [43/120    avg_loss:0.223, val_acc:0.950]
Epoch [44/120    avg_loss:0.161, val_acc:0.969]
Epoch [45/120    avg_loss:0.170, val_acc:0.946]
Epoch [46/120    avg_loss:0.146, val_acc:0.965]
Epoch [47/120    avg_loss:0.137, val_acc:0.960]
Epoch [48/120    avg_loss:0.182, val_acc:0.950]
Epoch [49/120    avg_loss:0.156, val_acc:0.948]
Epoch [50/120    avg_loss:0.146, val_acc:0.975]
Epoch [51/120    avg_loss:0.143, val_acc:0.944]
Epoch [52/120    avg_loss:0.131, val_acc:0.969]
Epoch [53/120    avg_loss:0.112, val_acc:0.969]
Epoch [54/120    avg_loss:0.114, val_acc:0.965]
Epoch [55/120    avg_loss:0.112, val_acc:0.958]
Epoch [56/120    avg_loss:0.118, val_acc:0.948]
Epoch [57/120    avg_loss:0.176, val_acc:0.933]
Epoch [58/120    avg_loss:0.195, val_acc:0.940]
Epoch [59/120    avg_loss:0.133, val_acc:0.973]
Epoch [60/120    avg_loss:0.103, val_acc:0.975]
Epoch [61/120    avg_loss:0.141, val_acc:0.977]
Epoch [62/120    avg_loss:0.119, val_acc:0.946]
Epoch [63/120    avg_loss:0.206, val_acc:0.942]
Epoch [64/120    avg_loss:0.128, val_acc:0.967]
Epoch [65/120    avg_loss:0.098, val_acc:0.967]
Epoch [66/120    avg_loss:0.096, val_acc:0.954]
Epoch [67/120    avg_loss:0.128, val_acc:0.975]
Epoch [68/120    avg_loss:0.093, val_acc:0.950]
Epoch [69/120    avg_loss:0.151, val_acc:0.954]
Epoch [70/120    avg_loss:0.082, val_acc:0.975]
Epoch [71/120    avg_loss:0.071, val_acc:0.971]
Epoch [72/120    avg_loss:0.091, val_acc:0.977]
Epoch [73/120    avg_loss:0.064, val_acc:0.983]
Epoch [74/120    avg_loss:0.051, val_acc:0.977]
Epoch [75/120    avg_loss:0.058, val_acc:0.981]
Epoch [76/120    avg_loss:0.056, val_acc:0.985]
Epoch [77/120    avg_loss:0.044, val_acc:0.983]
Epoch [78/120    avg_loss:0.068, val_acc:0.985]
Epoch [79/120    avg_loss:0.076, val_acc:0.985]
Epoch [80/120    avg_loss:0.068, val_acc:0.967]
Epoch [81/120    avg_loss:0.100, val_acc:0.973]
Epoch [82/120    avg_loss:0.100, val_acc:0.977]
Epoch [83/120    avg_loss:0.062, val_acc:0.981]
Epoch [84/120    avg_loss:0.065, val_acc:0.985]
Epoch [85/120    avg_loss:0.061, val_acc:0.979]
Epoch [86/120    avg_loss:0.078, val_acc:0.977]
Epoch [87/120    avg_loss:0.058, val_acc:0.988]
Epoch [88/120    avg_loss:0.054, val_acc:0.969]
Epoch [89/120    avg_loss:0.070, val_acc:0.992]
Epoch [90/120    avg_loss:0.104, val_acc:0.981]
Epoch [91/120    avg_loss:0.050, val_acc:0.983]
Epoch [92/120    avg_loss:0.055, val_acc:0.992]
Epoch [93/120    avg_loss:0.037, val_acc:0.988]
Epoch [94/120    avg_loss:0.024, val_acc:0.988]
Epoch [95/120    avg_loss:0.025, val_acc:0.988]
Epoch [96/120    avg_loss:0.029, val_acc:0.992]
Epoch [97/120    avg_loss:0.038, val_acc:0.992]
Epoch [98/120    avg_loss:0.050, val_acc:0.994]
Epoch [99/120    avg_loss:0.037, val_acc:0.956]
Epoch [100/120    avg_loss:0.056, val_acc:0.985]
Epoch [101/120    avg_loss:0.064, val_acc:0.971]
Epoch [102/120    avg_loss:0.081, val_acc:0.973]
Epoch [103/120    avg_loss:0.057, val_acc:0.988]
Epoch [104/120    avg_loss:0.029, val_acc:0.988]
Epoch [105/120    avg_loss:0.036, val_acc:0.981]
Epoch [106/120    avg_loss:0.030, val_acc:0.992]
Epoch [107/120    avg_loss:0.029, val_acc:0.994]
Epoch [108/120    avg_loss:0.024, val_acc:0.990]
Epoch [109/120    avg_loss:0.031, val_acc:0.988]
Epoch [110/120    avg_loss:0.021, val_acc:0.994]
Epoch [111/120    avg_loss:0.025, val_acc:0.990]
Epoch [112/120    avg_loss:0.024, val_acc:0.983]
Epoch [113/120    avg_loss:0.021, val_acc:0.990]
Epoch [114/120    avg_loss:0.017, val_acc:0.985]
Epoch [115/120    avg_loss:0.018, val_acc:0.994]
Epoch [116/120    avg_loss:0.028, val_acc:0.988]
Epoch [117/120    avg_loss:0.019, val_acc:0.988]
Epoch [118/120    avg_loss:0.020, val_acc:0.994]
Epoch [119/120    avg_loss:0.015, val_acc:0.990]
Epoch [120/120    avg_loss:0.024, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   8   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.63752665245202

F1 scores:
[       nan 0.99707174 0.9977221  1.         0.97550111 0.96271186
 0.99038462 0.99465241 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9959649492607425
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f09bc2ae940>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.555, val_acc:0.327]
Epoch [2/120    avg_loss:2.312, val_acc:0.467]
Epoch [3/120    avg_loss:2.132, val_acc:0.569]
Epoch [4/120    avg_loss:1.980, val_acc:0.558]
Epoch [5/120    avg_loss:1.802, val_acc:0.642]
Epoch [6/120    avg_loss:1.643, val_acc:0.685]
Epoch [7/120    avg_loss:1.472, val_acc:0.727]
Epoch [8/120    avg_loss:1.317, val_acc:0.790]
Epoch [9/120    avg_loss:1.192, val_acc:0.754]
Epoch [10/120    avg_loss:1.084, val_acc:0.742]
Epoch [11/120    avg_loss:0.946, val_acc:0.794]
Epoch [12/120    avg_loss:0.836, val_acc:0.810]
Epoch [13/120    avg_loss:0.816, val_acc:0.838]
Epoch [14/120    avg_loss:0.707, val_acc:0.838]
Epoch [15/120    avg_loss:0.674, val_acc:0.819]
Epoch [16/120    avg_loss:0.583, val_acc:0.890]
Epoch [17/120    avg_loss:0.543, val_acc:0.910]
Epoch [18/120    avg_loss:0.552, val_acc:0.908]
Epoch [19/120    avg_loss:0.527, val_acc:0.917]
Epoch [20/120    avg_loss:0.467, val_acc:0.908]
Epoch [21/120    avg_loss:0.460, val_acc:0.871]
Epoch [22/120    avg_loss:0.423, val_acc:0.908]
Epoch [23/120    avg_loss:0.404, val_acc:0.933]
Epoch [24/120    avg_loss:0.367, val_acc:0.885]
Epoch [25/120    avg_loss:0.379, val_acc:0.921]
Epoch [26/120    avg_loss:0.406, val_acc:0.910]
Epoch [27/120    avg_loss:0.391, val_acc:0.935]
Epoch [28/120    avg_loss:0.302, val_acc:0.944]
Epoch [29/120    avg_loss:0.283, val_acc:0.952]
Epoch [30/120    avg_loss:0.236, val_acc:0.954]
Epoch [31/120    avg_loss:0.282, val_acc:0.946]
Epoch [32/120    avg_loss:0.240, val_acc:0.960]
Epoch [33/120    avg_loss:0.202, val_acc:0.954]
Epoch [34/120    avg_loss:0.235, val_acc:0.956]
Epoch [35/120    avg_loss:0.199, val_acc:0.946]
Epoch [36/120    avg_loss:0.169, val_acc:0.971]
Epoch [37/120    avg_loss:0.221, val_acc:0.906]
Epoch [38/120    avg_loss:0.230, val_acc:0.942]
Epoch [39/120    avg_loss:0.230, val_acc:0.946]
Epoch [40/120    avg_loss:0.165, val_acc:0.975]
Epoch [41/120    avg_loss:0.144, val_acc:0.973]
Epoch [42/120    avg_loss:0.150, val_acc:0.971]
Epoch [43/120    avg_loss:0.148, val_acc:0.940]
Epoch [44/120    avg_loss:0.205, val_acc:0.956]
Epoch [45/120    avg_loss:0.192, val_acc:0.971]
Epoch [46/120    avg_loss:0.154, val_acc:0.933]
Epoch [47/120    avg_loss:0.190, val_acc:0.917]
Epoch [48/120    avg_loss:0.185, val_acc:0.927]
Epoch [49/120    avg_loss:0.161, val_acc:0.946]
Epoch [50/120    avg_loss:0.146, val_acc:0.969]
Epoch [51/120    avg_loss:0.171, val_acc:0.960]
Epoch [52/120    avg_loss:0.217, val_acc:0.954]
Epoch [53/120    avg_loss:0.175, val_acc:0.952]
Epoch [54/120    avg_loss:0.138, val_acc:0.971]
Epoch [55/120    avg_loss:0.104, val_acc:0.983]
Epoch [56/120    avg_loss:0.097, val_acc:0.979]
Epoch [57/120    avg_loss:0.105, val_acc:0.983]
Epoch [58/120    avg_loss:0.120, val_acc:0.983]
Epoch [59/120    avg_loss:0.088, val_acc:0.983]
Epoch [60/120    avg_loss:0.096, val_acc:0.983]
Epoch [61/120    avg_loss:0.105, val_acc:0.983]
Epoch [62/120    avg_loss:0.083, val_acc:0.983]
Epoch [63/120    avg_loss:0.082, val_acc:0.983]
Epoch [64/120    avg_loss:0.080, val_acc:0.983]
Epoch [65/120    avg_loss:0.082, val_acc:0.981]
Epoch [66/120    avg_loss:0.076, val_acc:0.981]
Epoch [67/120    avg_loss:0.084, val_acc:0.981]
Epoch [68/120    avg_loss:0.077, val_acc:0.983]
Epoch [69/120    avg_loss:0.070, val_acc:0.981]
Epoch [70/120    avg_loss:0.079, val_acc:0.981]
Epoch [71/120    avg_loss:0.073, val_acc:0.981]
Epoch [72/120    avg_loss:0.073, val_acc:0.981]
Epoch [73/120    avg_loss:0.069, val_acc:0.983]
Epoch [74/120    avg_loss:0.083, val_acc:0.983]
Epoch [75/120    avg_loss:0.070, val_acc:0.983]
Epoch [76/120    avg_loss:0.078, val_acc:0.983]
Epoch [77/120    avg_loss:0.086, val_acc:0.983]
Epoch [78/120    avg_loss:0.073, val_acc:0.983]
Epoch [79/120    avg_loss:0.073, val_acc:0.983]
Epoch [80/120    avg_loss:0.074, val_acc:0.983]
Epoch [81/120    avg_loss:0.077, val_acc:0.983]
Epoch [82/120    avg_loss:0.059, val_acc:0.981]
Epoch [83/120    avg_loss:0.079, val_acc:0.981]
Epoch [84/120    avg_loss:0.070, val_acc:0.981]
Epoch [85/120    avg_loss:0.067, val_acc:0.981]
Epoch [86/120    avg_loss:0.069, val_acc:0.981]
Epoch [87/120    avg_loss:0.079, val_acc:0.983]
Epoch [88/120    avg_loss:0.066, val_acc:0.983]
Epoch [89/120    avg_loss:0.072, val_acc:0.985]
Epoch [90/120    avg_loss:0.067, val_acc:0.983]
Epoch [91/120    avg_loss:0.065, val_acc:0.983]
Epoch [92/120    avg_loss:0.064, val_acc:0.983]
Epoch [93/120    avg_loss:0.070, val_acc:0.983]
Epoch [94/120    avg_loss:0.058, val_acc:0.983]
Epoch [95/120    avg_loss:0.058, val_acc:0.983]
Epoch [96/120    avg_loss:0.063, val_acc:0.983]
Epoch [97/120    avg_loss:0.078, val_acc:0.983]
Epoch [98/120    avg_loss:0.056, val_acc:0.988]
Epoch [99/120    avg_loss:0.060, val_acc:0.983]
Epoch [100/120    avg_loss:0.058, val_acc:0.983]
Epoch [101/120    avg_loss:0.065, val_acc:0.983]
Epoch [102/120    avg_loss:0.068, val_acc:0.985]
Epoch [103/120    avg_loss:0.061, val_acc:0.983]
Epoch [104/120    avg_loss:0.071, val_acc:0.983]
Epoch [105/120    avg_loss:0.059, val_acc:0.983]
Epoch [106/120    avg_loss:0.063, val_acc:0.983]
Epoch [107/120    avg_loss:0.055, val_acc:0.983]
Epoch [108/120    avg_loss:0.058, val_acc:0.988]
Epoch [109/120    avg_loss:0.050, val_acc:0.981]
Epoch [110/120    avg_loss:0.054, val_acc:0.981]
Epoch [111/120    avg_loss:0.065, val_acc:0.983]
Epoch [112/120    avg_loss:0.050, val_acc:0.983]
Epoch [113/120    avg_loss:0.055, val_acc:0.983]
Epoch [114/120    avg_loss:0.059, val_acc:0.985]
Epoch [115/120    avg_loss:0.057, val_acc:0.983]
Epoch [116/120    avg_loss:0.061, val_acc:0.985]
Epoch [117/120    avg_loss:0.053, val_acc:0.983]
Epoch [118/120    avg_loss:0.053, val_acc:0.983]
Epoch [119/120    avg_loss:0.059, val_acc:0.988]
Epoch [120/120    avg_loss:0.061, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   0   7   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 0.99486427 0.98426966 1.         0.94930876 0.92903226
 0.98329356 0.96132597 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9912186826839892
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f6c1b7940>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.511, val_acc:0.323]
Epoch [2/120    avg_loss:2.267, val_acc:0.440]
Epoch [3/120    avg_loss:2.081, val_acc:0.498]
Epoch [4/120    avg_loss:1.913, val_acc:0.554]
Epoch [5/120    avg_loss:1.745, val_acc:0.600]
Epoch [6/120    avg_loss:1.561, val_acc:0.637]
Epoch [7/120    avg_loss:1.415, val_acc:0.681]
Epoch [8/120    avg_loss:1.294, val_acc:0.710]
Epoch [9/120    avg_loss:1.169, val_acc:0.752]
Epoch [10/120    avg_loss:1.084, val_acc:0.748]
Epoch [11/120    avg_loss:1.025, val_acc:0.748]
Epoch [12/120    avg_loss:0.919, val_acc:0.777]
Epoch [13/120    avg_loss:0.854, val_acc:0.787]
Epoch [14/120    avg_loss:0.787, val_acc:0.827]
Epoch [15/120    avg_loss:0.725, val_acc:0.877]
Epoch [16/120    avg_loss:0.659, val_acc:0.854]
Epoch [17/120    avg_loss:0.588, val_acc:0.873]
Epoch [18/120    avg_loss:0.575, val_acc:0.877]
Epoch [19/120    avg_loss:0.524, val_acc:0.912]
Epoch [20/120    avg_loss:0.500, val_acc:0.931]
Epoch [21/120    avg_loss:0.468, val_acc:0.935]
Epoch [22/120    avg_loss:0.437, val_acc:0.919]
Epoch [23/120    avg_loss:0.406, val_acc:0.952]
Epoch [24/120    avg_loss:0.358, val_acc:0.912]
Epoch [25/120    avg_loss:0.406, val_acc:0.854]
Epoch [26/120    avg_loss:0.418, val_acc:0.933]
Epoch [27/120    avg_loss:0.401, val_acc:0.879]
Epoch [28/120    avg_loss:0.344, val_acc:0.931]
Epoch [29/120    avg_loss:0.368, val_acc:0.963]
Epoch [30/120    avg_loss:0.333, val_acc:0.942]
Epoch [31/120    avg_loss:0.318, val_acc:0.958]
Epoch [32/120    avg_loss:0.284, val_acc:0.944]
Epoch [33/120    avg_loss:0.320, val_acc:0.935]
Epoch [34/120    avg_loss:0.315, val_acc:0.977]
Epoch [35/120    avg_loss:0.221, val_acc:0.956]
Epoch [36/120    avg_loss:0.220, val_acc:0.944]
Epoch [37/120    avg_loss:0.214, val_acc:0.944]
Epoch [38/120    avg_loss:0.197, val_acc:0.967]
Epoch [39/120    avg_loss:0.218, val_acc:0.933]
Epoch [40/120    avg_loss:0.235, val_acc:0.952]
Epoch [41/120    avg_loss:0.235, val_acc:0.950]
Epoch [42/120    avg_loss:0.227, val_acc:0.967]
Epoch [43/120    avg_loss:0.227, val_acc:0.963]
Epoch [44/120    avg_loss:0.186, val_acc:0.963]
Epoch [45/120    avg_loss:0.209, val_acc:0.977]
Epoch [46/120    avg_loss:0.142, val_acc:0.981]
Epoch [47/120    avg_loss:0.116, val_acc:0.942]
Epoch [48/120    avg_loss:0.234, val_acc:0.946]
Epoch [49/120    avg_loss:0.190, val_acc:0.969]
Epoch [50/120    avg_loss:0.210, val_acc:0.965]
Epoch [51/120    avg_loss:0.184, val_acc:0.963]
Epoch [52/120    avg_loss:0.142, val_acc:0.971]
Epoch [53/120    avg_loss:0.132, val_acc:0.958]
Epoch [54/120    avg_loss:0.114, val_acc:0.958]
Epoch [55/120    avg_loss:0.105, val_acc:0.979]
Epoch [56/120    avg_loss:0.125, val_acc:0.975]
Epoch [57/120    avg_loss:0.156, val_acc:0.979]
Epoch [58/120    avg_loss:0.135, val_acc:0.979]
Epoch [59/120    avg_loss:0.132, val_acc:0.975]
Epoch [60/120    avg_loss:0.092, val_acc:0.973]
Epoch [61/120    avg_loss:0.087, val_acc:0.988]
Epoch [62/120    avg_loss:0.083, val_acc:0.988]
Epoch [63/120    avg_loss:0.074, val_acc:0.988]
Epoch [64/120    avg_loss:0.065, val_acc:0.985]
Epoch [65/120    avg_loss:0.079, val_acc:0.988]
Epoch [66/120    avg_loss:0.066, val_acc:0.990]
Epoch [67/120    avg_loss:0.077, val_acc:0.988]
Epoch [68/120    avg_loss:0.064, val_acc:0.988]
Epoch [69/120    avg_loss:0.062, val_acc:0.988]
Epoch [70/120    avg_loss:0.054, val_acc:0.988]
Epoch [71/120    avg_loss:0.058, val_acc:0.988]
Epoch [72/120    avg_loss:0.057, val_acc:0.985]
Epoch [73/120    avg_loss:0.061, val_acc:0.988]
Epoch [74/120    avg_loss:0.066, val_acc:0.988]
Epoch [75/120    avg_loss:0.059, val_acc:0.985]
Epoch [76/120    avg_loss:0.060, val_acc:0.988]
Epoch [77/120    avg_loss:0.053, val_acc:0.990]
Epoch [78/120    avg_loss:0.069, val_acc:0.988]
Epoch [79/120    avg_loss:0.062, val_acc:0.988]
Epoch [80/120    avg_loss:0.052, val_acc:0.990]
Epoch [81/120    avg_loss:0.057, val_acc:0.990]
Epoch [82/120    avg_loss:0.058, val_acc:0.990]
Epoch [83/120    avg_loss:0.062, val_acc:0.990]
Epoch [84/120    avg_loss:0.056, val_acc:0.988]
Epoch [85/120    avg_loss:0.063, val_acc:0.990]
Epoch [86/120    avg_loss:0.046, val_acc:0.992]
Epoch [87/120    avg_loss:0.050, val_acc:0.994]
Epoch [88/120    avg_loss:0.048, val_acc:0.992]
Epoch [89/120    avg_loss:0.051, val_acc:0.992]
Epoch [90/120    avg_loss:0.054, val_acc:0.990]
Epoch [91/120    avg_loss:0.066, val_acc:0.992]
Epoch [92/120    avg_loss:0.047, val_acc:0.990]
Epoch [93/120    avg_loss:0.047, val_acc:0.994]
Epoch [94/120    avg_loss:0.047, val_acc:0.992]
Epoch [95/120    avg_loss:0.048, val_acc:0.994]
Epoch [96/120    avg_loss:0.052, val_acc:0.988]
Epoch [97/120    avg_loss:0.046, val_acc:0.992]
Epoch [98/120    avg_loss:0.053, val_acc:0.992]
Epoch [99/120    avg_loss:0.051, val_acc:0.992]
Epoch [100/120    avg_loss:0.041, val_acc:0.994]
Epoch [101/120    avg_loss:0.047, val_acc:0.994]
Epoch [102/120    avg_loss:0.062, val_acc:0.994]
Epoch [103/120    avg_loss:0.055, val_acc:0.994]
Epoch [104/120    avg_loss:0.053, val_acc:0.994]
Epoch [105/120    avg_loss:0.046, val_acc:0.994]
Epoch [106/120    avg_loss:0.044, val_acc:0.994]
Epoch [107/120    avg_loss:0.040, val_acc:0.994]
Epoch [108/120    avg_loss:0.053, val_acc:0.994]
Epoch [109/120    avg_loss:0.045, val_acc:0.994]
Epoch [110/120    avg_loss:0.045, val_acc:0.996]
Epoch [111/120    avg_loss:0.052, val_acc:0.994]
Epoch [112/120    avg_loss:0.047, val_acc:0.992]
Epoch [113/120    avg_loss:0.052, val_acc:0.994]
Epoch [114/120    avg_loss:0.042, val_acc:0.994]
Epoch [115/120    avg_loss:0.047, val_acc:0.994]
Epoch [116/120    avg_loss:0.044, val_acc:0.994]
Epoch [117/120    avg_loss:0.041, val_acc:0.996]
Epoch [118/120    avg_loss:0.044, val_acc:0.994]
Epoch [119/120    avg_loss:0.037, val_acc:0.992]
Epoch [120/120    avg_loss:0.038, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 0.996337   0.98871332 1.         0.9610984  0.94462541
 0.98800959 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9935917269665777
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8121d42908>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.554, val_acc:0.458]
Epoch [2/120    avg_loss:2.310, val_acc:0.460]
Epoch [3/120    avg_loss:2.145, val_acc:0.500]
Epoch [4/120    avg_loss:1.997, val_acc:0.533]
Epoch [5/120    avg_loss:1.874, val_acc:0.535]
Epoch [6/120    avg_loss:1.728, val_acc:0.600]
Epoch [7/120    avg_loss:1.603, val_acc:0.675]
Epoch [8/120    avg_loss:1.470, val_acc:0.700]
Epoch [9/120    avg_loss:1.359, val_acc:0.708]
Epoch [10/120    avg_loss:1.259, val_acc:0.727]
Epoch [11/120    avg_loss:1.120, val_acc:0.748]
Epoch [12/120    avg_loss:1.037, val_acc:0.750]
Epoch [13/120    avg_loss:0.965, val_acc:0.750]
Epoch [14/120    avg_loss:0.869, val_acc:0.773]
Epoch [15/120    avg_loss:0.789, val_acc:0.821]
Epoch [16/120    avg_loss:0.775, val_acc:0.833]
Epoch [17/120    avg_loss:0.729, val_acc:0.856]
Epoch [18/120    avg_loss:0.680, val_acc:0.871]
Epoch [19/120    avg_loss:0.618, val_acc:0.890]
Epoch [20/120    avg_loss:0.580, val_acc:0.860]
Epoch [21/120    avg_loss:0.536, val_acc:0.898]
Epoch [22/120    avg_loss:0.518, val_acc:0.871]
Epoch [23/120    avg_loss:0.481, val_acc:0.885]
Epoch [24/120    avg_loss:0.513, val_acc:0.823]
Epoch [25/120    avg_loss:0.427, val_acc:0.917]
Epoch [26/120    avg_loss:0.413, val_acc:0.927]
Epoch [27/120    avg_loss:0.446, val_acc:0.910]
Epoch [28/120    avg_loss:0.413, val_acc:0.915]
Epoch [29/120    avg_loss:0.426, val_acc:0.925]
Epoch [30/120    avg_loss:0.349, val_acc:0.923]
Epoch [31/120    avg_loss:0.412, val_acc:0.896]
Epoch [32/120    avg_loss:0.434, val_acc:0.935]
Epoch [33/120    avg_loss:0.365, val_acc:0.952]
Epoch [34/120    avg_loss:0.305, val_acc:0.952]
Epoch [35/120    avg_loss:0.309, val_acc:0.950]
Epoch [36/120    avg_loss:0.256, val_acc:0.948]
Epoch [37/120    avg_loss:0.268, val_acc:0.956]
Epoch [38/120    avg_loss:0.273, val_acc:0.975]
Epoch [39/120    avg_loss:0.232, val_acc:0.956]
Epoch [40/120    avg_loss:0.244, val_acc:0.950]
Epoch [41/120    avg_loss:0.244, val_acc:0.944]
Epoch [42/120    avg_loss:0.321, val_acc:0.950]
Epoch [43/120    avg_loss:0.254, val_acc:0.963]
Epoch [44/120    avg_loss:0.215, val_acc:0.963]
Epoch [45/120    avg_loss:0.173, val_acc:0.963]
Epoch [46/120    avg_loss:0.183, val_acc:0.960]
Epoch [47/120    avg_loss:0.181, val_acc:0.950]
Epoch [48/120    avg_loss:0.159, val_acc:0.958]
Epoch [49/120    avg_loss:0.164, val_acc:0.973]
Epoch [50/120    avg_loss:0.176, val_acc:0.960]
Epoch [51/120    avg_loss:0.155, val_acc:0.963]
Epoch [52/120    avg_loss:0.135, val_acc:0.965]
Epoch [53/120    avg_loss:0.116, val_acc:0.975]
Epoch [54/120    avg_loss:0.107, val_acc:0.975]
Epoch [55/120    avg_loss:0.110, val_acc:0.975]
Epoch [56/120    avg_loss:0.095, val_acc:0.975]
Epoch [57/120    avg_loss:0.104, val_acc:0.975]
Epoch [58/120    avg_loss:0.101, val_acc:0.977]
Epoch [59/120    avg_loss:0.099, val_acc:0.977]
Epoch [60/120    avg_loss:0.099, val_acc:0.981]
Epoch [61/120    avg_loss:0.102, val_acc:0.979]
Epoch [62/120    avg_loss:0.090, val_acc:0.975]
Epoch [63/120    avg_loss:0.104, val_acc:0.979]
Epoch [64/120    avg_loss:0.095, val_acc:0.977]
Epoch [65/120    avg_loss:0.097, val_acc:0.981]
Epoch [66/120    avg_loss:0.095, val_acc:0.979]
Epoch [67/120    avg_loss:0.104, val_acc:0.979]
Epoch [68/120    avg_loss:0.087, val_acc:0.981]
Epoch [69/120    avg_loss:0.102, val_acc:0.983]
Epoch [70/120    avg_loss:0.084, val_acc:0.983]
Epoch [71/120    avg_loss:0.105, val_acc:0.983]
Epoch [72/120    avg_loss:0.093, val_acc:0.983]
Epoch [73/120    avg_loss:0.078, val_acc:0.983]
Epoch [74/120    avg_loss:0.092, val_acc:0.983]
Epoch [75/120    avg_loss:0.097, val_acc:0.983]
Epoch [76/120    avg_loss:0.088, val_acc:0.981]
Epoch [77/120    avg_loss:0.089, val_acc:0.983]
Epoch [78/120    avg_loss:0.088, val_acc:0.981]
Epoch [79/120    avg_loss:0.081, val_acc:0.983]
Epoch [80/120    avg_loss:0.085, val_acc:0.981]
Epoch [81/120    avg_loss:0.077, val_acc:0.981]
Epoch [82/120    avg_loss:0.082, val_acc:0.981]
Epoch [83/120    avg_loss:0.071, val_acc:0.981]
Epoch [84/120    avg_loss:0.084, val_acc:0.988]
Epoch [85/120    avg_loss:0.088, val_acc:0.983]
Epoch [86/120    avg_loss:0.071, val_acc:0.988]
Epoch [87/120    avg_loss:0.089, val_acc:0.981]
Epoch [88/120    avg_loss:0.073, val_acc:0.983]
Epoch [89/120    avg_loss:0.086, val_acc:0.988]
Epoch [90/120    avg_loss:0.088, val_acc:0.985]
Epoch [91/120    avg_loss:0.071, val_acc:0.988]
Epoch [92/120    avg_loss:0.080, val_acc:0.988]
Epoch [93/120    avg_loss:0.073, val_acc:0.988]
Epoch [94/120    avg_loss:0.075, val_acc:0.988]
Epoch [95/120    avg_loss:0.078, val_acc:0.990]
Epoch [96/120    avg_loss:0.072, val_acc:0.990]
Epoch [97/120    avg_loss:0.073, val_acc:0.983]
Epoch [98/120    avg_loss:0.069, val_acc:0.985]
Epoch [99/120    avg_loss:0.076, val_acc:0.985]
Epoch [100/120    avg_loss:0.067, val_acc:0.985]
Epoch [101/120    avg_loss:0.068, val_acc:0.988]
Epoch [102/120    avg_loss:0.060, val_acc:0.990]
Epoch [103/120    avg_loss:0.076, val_acc:0.990]
Epoch [104/120    avg_loss:0.077, val_acc:0.983]
Epoch [105/120    avg_loss:0.075, val_acc:0.988]
Epoch [106/120    avg_loss:0.073, val_acc:0.988]
Epoch [107/120    avg_loss:0.066, val_acc:0.985]
Epoch [108/120    avg_loss:0.068, val_acc:0.985]
Epoch [109/120    avg_loss:0.059, val_acc:0.990]
Epoch [110/120    avg_loss:0.064, val_acc:0.988]
Epoch [111/120    avg_loss:0.055, val_acc:0.988]
Epoch [112/120    avg_loss:0.068, val_acc:0.988]
Epoch [113/120    avg_loss:0.062, val_acc:0.981]
Epoch [114/120    avg_loss:0.074, val_acc:0.992]
Epoch [115/120    avg_loss:0.069, val_acc:0.992]
Epoch [116/120    avg_loss:0.062, val_acc:0.985]
Epoch [117/120    avg_loss:0.065, val_acc:0.985]
Epoch [118/120    avg_loss:0.065, val_acc:0.990]
Epoch [119/120    avg_loss:0.062, val_acc:0.988]
Epoch [120/120    avg_loss:0.061, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 675   0   0   0   0  10   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 204  18   0   0   0   0   0   0   5   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 0.99264706 0.98871332 1.         0.92727273 0.909699
 0.97630332 0.9726776  1.         1.         1.         1.
 0.99451153 1.        ]

Kappa:
0.9888452018351498
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1941113940>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.539, val_acc:0.358]
Epoch [2/120    avg_loss:2.248, val_acc:0.502]
Epoch [3/120    avg_loss:2.075, val_acc:0.523]
Epoch [4/120    avg_loss:1.892, val_acc:0.560]
Epoch [5/120    avg_loss:1.730, val_acc:0.615]
Epoch [6/120    avg_loss:1.581, val_acc:0.654]
Epoch [7/120    avg_loss:1.423, val_acc:0.688]
Epoch [8/120    avg_loss:1.293, val_acc:0.729]
Epoch [9/120    avg_loss:1.185, val_acc:0.796]
Epoch [10/120    avg_loss:1.025, val_acc:0.785]
Epoch [11/120    avg_loss:0.868, val_acc:0.806]
Epoch [12/120    avg_loss:0.824, val_acc:0.812]
Epoch [13/120    avg_loss:0.711, val_acc:0.810]
Epoch [14/120    avg_loss:0.683, val_acc:0.860]
Epoch [15/120    avg_loss:0.618, val_acc:0.819]
Epoch [16/120    avg_loss:0.601, val_acc:0.929]
Epoch [17/120    avg_loss:0.534, val_acc:0.923]
Epoch [18/120    avg_loss:0.490, val_acc:0.919]
Epoch [19/120    avg_loss:0.511, val_acc:0.935]
Epoch [20/120    avg_loss:0.529, val_acc:0.952]
Epoch [21/120    avg_loss:0.486, val_acc:0.944]
Epoch [22/120    avg_loss:0.454, val_acc:0.952]
Epoch [23/120    avg_loss:0.415, val_acc:0.956]
Epoch [24/120    avg_loss:0.391, val_acc:0.969]
Epoch [25/120    avg_loss:0.361, val_acc:0.973]
Epoch [26/120    avg_loss:0.325, val_acc:0.923]
Epoch [27/120    avg_loss:0.337, val_acc:0.965]
Epoch [28/120    avg_loss:0.242, val_acc:0.977]
Epoch [29/120    avg_loss:0.256, val_acc:0.963]
Epoch [30/120    avg_loss:0.260, val_acc:0.969]
Epoch [31/120    avg_loss:0.243, val_acc:0.960]
Epoch [32/120    avg_loss:0.210, val_acc:0.979]
Epoch [33/120    avg_loss:0.210, val_acc:0.956]
Epoch [34/120    avg_loss:0.223, val_acc:0.965]
Epoch [35/120    avg_loss:0.231, val_acc:0.965]
Epoch [36/120    avg_loss:0.287, val_acc:0.983]
Epoch [37/120    avg_loss:0.194, val_acc:0.977]
Epoch [38/120    avg_loss:0.166, val_acc:0.981]
Epoch [39/120    avg_loss:0.168, val_acc:0.981]
Epoch [40/120    avg_loss:0.185, val_acc:0.977]
Epoch [41/120    avg_loss:0.156, val_acc:0.981]
Epoch [42/120    avg_loss:0.165, val_acc:0.979]
Epoch [43/120    avg_loss:0.197, val_acc:0.956]
Epoch [44/120    avg_loss:0.228, val_acc:0.912]
Epoch [45/120    avg_loss:0.206, val_acc:0.973]
Epoch [46/120    avg_loss:0.188, val_acc:0.967]
Epoch [47/120    avg_loss:0.168, val_acc:0.992]
Epoch [48/120    avg_loss:0.158, val_acc:0.981]
Epoch [49/120    avg_loss:0.175, val_acc:0.979]
Epoch [50/120    avg_loss:0.199, val_acc:0.950]
Epoch [51/120    avg_loss:0.183, val_acc:0.971]
Epoch [52/120    avg_loss:0.198, val_acc:0.975]
Epoch [53/120    avg_loss:0.193, val_acc:0.979]
Epoch [54/120    avg_loss:0.140, val_acc:0.992]
Epoch [55/120    avg_loss:0.109, val_acc:0.983]
Epoch [56/120    avg_loss:0.110, val_acc:0.996]
Epoch [57/120    avg_loss:0.110, val_acc:0.992]
Epoch [58/120    avg_loss:0.125, val_acc:0.990]
Epoch [59/120    avg_loss:0.129, val_acc:0.988]
Epoch [60/120    avg_loss:0.097, val_acc:0.975]
Epoch [61/120    avg_loss:0.096, val_acc:0.983]
Epoch [62/120    avg_loss:0.109, val_acc:0.990]
Epoch [63/120    avg_loss:0.116, val_acc:0.990]
Epoch [64/120    avg_loss:0.068, val_acc:0.994]
Epoch [65/120    avg_loss:0.079, val_acc:0.977]
Epoch [66/120    avg_loss:0.088, val_acc:0.988]
Epoch [67/120    avg_loss:0.067, val_acc:0.990]
Epoch [68/120    avg_loss:0.099, val_acc:0.985]
Epoch [69/120    avg_loss:0.112, val_acc:0.971]
Epoch [70/120    avg_loss:0.139, val_acc:0.985]
Epoch [71/120    avg_loss:0.087, val_acc:0.992]
Epoch [72/120    avg_loss:0.065, val_acc:0.994]
Epoch [73/120    avg_loss:0.064, val_acc:0.994]
Epoch [74/120    avg_loss:0.046, val_acc:0.994]
Epoch [75/120    avg_loss:0.062, val_acc:0.996]
Epoch [76/120    avg_loss:0.049, val_acc:0.996]
Epoch [77/120    avg_loss:0.050, val_acc:0.996]
Epoch [78/120    avg_loss:0.044, val_acc:0.996]
Epoch [79/120    avg_loss:0.054, val_acc:0.996]
Epoch [80/120    avg_loss:0.047, val_acc:0.996]
Epoch [81/120    avg_loss:0.052, val_acc:0.996]
Epoch [82/120    avg_loss:0.051, val_acc:0.996]
Epoch [83/120    avg_loss:0.046, val_acc:0.996]
Epoch [84/120    avg_loss:0.041, val_acc:0.996]
Epoch [85/120    avg_loss:0.044, val_acc:0.996]
Epoch [86/120    avg_loss:0.046, val_acc:0.996]
Epoch [87/120    avg_loss:0.049, val_acc:0.996]
Epoch [88/120    avg_loss:0.046, val_acc:0.996]
Epoch [89/120    avg_loss:0.049, val_acc:0.996]
Epoch [90/120    avg_loss:0.055, val_acc:0.996]
Epoch [91/120    avg_loss:0.033, val_acc:0.996]
Epoch [92/120    avg_loss:0.038, val_acc:0.996]
Epoch [93/120    avg_loss:0.057, val_acc:0.996]
Epoch [94/120    avg_loss:0.041, val_acc:0.996]
Epoch [95/120    avg_loss:0.049, val_acc:0.996]
Epoch [96/120    avg_loss:0.046, val_acc:0.996]
Epoch [97/120    avg_loss:0.043, val_acc:0.996]
Epoch [98/120    avg_loss:0.043, val_acc:0.996]
Epoch [99/120    avg_loss:0.039, val_acc:0.998]
Epoch [100/120    avg_loss:0.042, val_acc:0.996]
Epoch [101/120    avg_loss:0.041, val_acc:0.998]
Epoch [102/120    avg_loss:0.033, val_acc:0.996]
Epoch [103/120    avg_loss:0.034, val_acc:0.996]
Epoch [104/120    avg_loss:0.032, val_acc:0.996]
Epoch [105/120    avg_loss:0.040, val_acc:0.996]
Epoch [106/120    avg_loss:0.040, val_acc:0.996]
Epoch [107/120    avg_loss:0.043, val_acc:0.996]
Epoch [108/120    avg_loss:0.035, val_acc:0.996]
Epoch [109/120    avg_loss:0.030, val_acc:0.996]
Epoch [110/120    avg_loss:0.041, val_acc:0.996]
Epoch [111/120    avg_loss:0.038, val_acc:0.996]
Epoch [112/120    avg_loss:0.028, val_acc:0.996]
Epoch [113/120    avg_loss:0.041, val_acc:0.996]
Epoch [114/120    avg_loss:0.038, val_acc:0.994]
Epoch [115/120    avg_loss:0.031, val_acc:0.994]
Epoch [116/120    avg_loss:0.036, val_acc:0.994]
Epoch [117/120    avg_loss:0.030, val_acc:0.996]
Epoch [118/120    avg_loss:0.036, val_acc:0.996]
Epoch [119/120    avg_loss:0.032, val_acc:0.996]
Epoch [120/120    avg_loss:0.037, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0   1 214  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 0.99412628 0.9977221  0.99565217 0.95964126 0.94276094
 0.98095238 0.9893617  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9933548038817014
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0907f52940>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.579, val_acc:0.338]
Epoch [2/120    avg_loss:2.305, val_acc:0.502]
Epoch [3/120    avg_loss:2.101, val_acc:0.569]
Epoch [4/120    avg_loss:1.916, val_acc:0.615]
Epoch [5/120    avg_loss:1.702, val_acc:0.652]
Epoch [6/120    avg_loss:1.514, val_acc:0.681]
Epoch [7/120    avg_loss:1.374, val_acc:0.685]
Epoch [8/120    avg_loss:1.217, val_acc:0.729]
Epoch [9/120    avg_loss:1.096, val_acc:0.744]
Epoch [10/120    avg_loss:0.986, val_acc:0.752]
Epoch [11/120    avg_loss:0.893, val_acc:0.756]
Epoch [12/120    avg_loss:0.811, val_acc:0.815]
Epoch [13/120    avg_loss:0.834, val_acc:0.773]
Epoch [14/120    avg_loss:0.716, val_acc:0.806]
Epoch [15/120    avg_loss:0.662, val_acc:0.802]
Epoch [16/120    avg_loss:0.684, val_acc:0.825]
Epoch [17/120    avg_loss:0.551, val_acc:0.881]
Epoch [18/120    avg_loss:0.507, val_acc:0.933]
Epoch [19/120    avg_loss:0.481, val_acc:0.931]
Epoch [20/120    avg_loss:0.420, val_acc:0.923]
Epoch [21/120    avg_loss:0.447, val_acc:0.923]
Epoch [22/120    avg_loss:0.428, val_acc:0.902]
Epoch [23/120    avg_loss:0.372, val_acc:0.931]
Epoch [24/120    avg_loss:0.398, val_acc:0.900]
Epoch [25/120    avg_loss:0.368, val_acc:0.960]
Epoch [26/120    avg_loss:0.383, val_acc:0.950]
Epoch [27/120    avg_loss:0.342, val_acc:0.942]
Epoch [28/120    avg_loss:0.370, val_acc:0.946]
Epoch [29/120    avg_loss:0.335, val_acc:0.938]
Epoch [30/120    avg_loss:0.292, val_acc:0.967]
Epoch [31/120    avg_loss:0.267, val_acc:0.963]
Epoch [32/120    avg_loss:0.247, val_acc:0.971]
Epoch [33/120    avg_loss:0.275, val_acc:0.954]
Epoch [34/120    avg_loss:0.272, val_acc:0.958]
Epoch [35/120    avg_loss:0.277, val_acc:0.954]
Epoch [36/120    avg_loss:0.210, val_acc:0.958]
Epoch [37/120    avg_loss:0.192, val_acc:0.946]
Epoch [38/120    avg_loss:0.193, val_acc:0.967]
Epoch [39/120    avg_loss:0.216, val_acc:0.954]
Epoch [40/120    avg_loss:0.202, val_acc:0.946]
Epoch [41/120    avg_loss:0.232, val_acc:0.960]
Epoch [42/120    avg_loss:0.250, val_acc:0.965]
Epoch [43/120    avg_loss:0.188, val_acc:0.963]
Epoch [44/120    avg_loss:0.163, val_acc:0.973]
Epoch [45/120    avg_loss:0.178, val_acc:0.983]
Epoch [46/120    avg_loss:0.221, val_acc:0.960]
Epoch [47/120    avg_loss:0.164, val_acc:0.979]
Epoch [48/120    avg_loss:0.163, val_acc:0.969]
Epoch [49/120    avg_loss:0.168, val_acc:0.975]
Epoch [50/120    avg_loss:0.143, val_acc:0.977]
Epoch [51/120    avg_loss:0.129, val_acc:0.971]
Epoch [52/120    avg_loss:0.142, val_acc:0.977]
Epoch [53/120    avg_loss:0.147, val_acc:0.979]
Epoch [54/120    avg_loss:0.144, val_acc:0.979]
Epoch [55/120    avg_loss:0.155, val_acc:0.988]
Epoch [56/120    avg_loss:0.116, val_acc:0.977]
Epoch [57/120    avg_loss:0.116, val_acc:0.985]
Epoch [58/120    avg_loss:0.127, val_acc:0.979]
Epoch [59/120    avg_loss:0.105, val_acc:0.985]
Epoch [60/120    avg_loss:0.113, val_acc:0.977]
Epoch [61/120    avg_loss:0.105, val_acc:0.990]
Epoch [62/120    avg_loss:0.084, val_acc:0.983]
Epoch [63/120    avg_loss:0.072, val_acc:0.990]
Epoch [64/120    avg_loss:0.066, val_acc:0.992]
Epoch [65/120    avg_loss:0.077, val_acc:0.988]
Epoch [66/120    avg_loss:0.057, val_acc:0.985]
Epoch [67/120    avg_loss:0.056, val_acc:0.992]
Epoch [68/120    avg_loss:0.093, val_acc:0.975]
Epoch [69/120    avg_loss:0.164, val_acc:0.917]
Epoch [70/120    avg_loss:0.136, val_acc:0.973]
Epoch [71/120    avg_loss:0.080, val_acc:0.983]
Epoch [72/120    avg_loss:0.078, val_acc:0.988]
Epoch [73/120    avg_loss:0.066, val_acc:0.990]
Epoch [74/120    avg_loss:0.056, val_acc:0.990]
Epoch [75/120    avg_loss:0.091, val_acc:0.971]
Epoch [76/120    avg_loss:0.133, val_acc:0.979]
Epoch [77/120    avg_loss:0.101, val_acc:0.988]
Epoch [78/120    avg_loss:0.071, val_acc:0.979]
Epoch [79/120    avg_loss:0.073, val_acc:0.988]
Epoch [80/120    avg_loss:0.049, val_acc:0.992]
Epoch [81/120    avg_loss:0.059, val_acc:0.973]
Epoch [82/120    avg_loss:0.087, val_acc:0.969]
Epoch [83/120    avg_loss:0.073, val_acc:0.990]
Epoch [84/120    avg_loss:0.056, val_acc:0.981]
Epoch [85/120    avg_loss:0.105, val_acc:0.979]
Epoch [86/120    avg_loss:0.079, val_acc:0.990]
Epoch [87/120    avg_loss:0.054, val_acc:0.992]
Epoch [88/120    avg_loss:0.045, val_acc:0.985]
Epoch [89/120    avg_loss:0.031, val_acc:0.990]
Epoch [90/120    avg_loss:0.030, val_acc:0.988]
Epoch [91/120    avg_loss:0.033, val_acc:0.994]
Epoch [92/120    avg_loss:0.045, val_acc:0.992]
Epoch [93/120    avg_loss:0.049, val_acc:0.992]
Epoch [94/120    avg_loss:0.043, val_acc:0.985]
Epoch [95/120    avg_loss:0.060, val_acc:0.977]
Epoch [96/120    avg_loss:0.042, val_acc:0.990]
Epoch [97/120    avg_loss:0.032, val_acc:0.990]
Epoch [98/120    avg_loss:0.043, val_acc:0.992]
Epoch [99/120    avg_loss:0.047, val_acc:0.979]
Epoch [100/120    avg_loss:0.046, val_acc:0.990]
Epoch [101/120    avg_loss:0.029, val_acc:0.990]
Epoch [102/120    avg_loss:0.040, val_acc:0.979]
Epoch [103/120    avg_loss:0.061, val_acc:0.990]
Epoch [104/120    avg_loss:0.053, val_acc:0.985]
Epoch [105/120    avg_loss:0.053, val_acc:0.990]
Epoch [106/120    avg_loss:0.021, val_acc:0.992]
Epoch [107/120    avg_loss:0.021, val_acc:0.992]
Epoch [108/120    avg_loss:0.018, val_acc:0.994]
Epoch [109/120    avg_loss:0.016, val_acc:0.994]
Epoch [110/120    avg_loss:0.019, val_acc:0.992]
Epoch [111/120    avg_loss:0.023, val_acc:0.996]
Epoch [112/120    avg_loss:0.019, val_acc:0.996]
Epoch [113/120    avg_loss:0.018, val_acc:0.996]
Epoch [114/120    avg_loss:0.018, val_acc:0.996]
Epoch [115/120    avg_loss:0.016, val_acc:0.994]
Epoch [116/120    avg_loss:0.017, val_acc:0.994]
Epoch [117/120    avg_loss:0.020, val_acc:0.994]
Epoch [118/120    avg_loss:0.014, val_acc:0.992]
Epoch [119/120    avg_loss:0.019, val_acc:0.994]
Epoch [120/120    avg_loss:0.017, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   0   7   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.61620469083155

F1 scores:
[       nan 0.99486427 1.         1.         0.97592998 0.96167247
 0.98329356 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9957278056543266
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd462207940>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.485, val_acc:0.294]
Epoch [2/120    avg_loss:2.231, val_acc:0.412]
Epoch [3/120    avg_loss:2.053, val_acc:0.504]
Epoch [4/120    avg_loss:1.886, val_acc:0.548]
Epoch [5/120    avg_loss:1.700, val_acc:0.615]
Epoch [6/120    avg_loss:1.540, val_acc:0.644]
Epoch [7/120    avg_loss:1.383, val_acc:0.702]
Epoch [8/120    avg_loss:1.249, val_acc:0.721]
Epoch [9/120    avg_loss:1.173, val_acc:0.744]
Epoch [10/120    avg_loss:1.038, val_acc:0.762]
Epoch [11/120    avg_loss:0.951, val_acc:0.765]
Epoch [12/120    avg_loss:0.842, val_acc:0.873]
Epoch [13/120    avg_loss:0.738, val_acc:0.892]
Epoch [14/120    avg_loss:0.667, val_acc:0.912]
Epoch [15/120    avg_loss:0.645, val_acc:0.910]
Epoch [16/120    avg_loss:0.576, val_acc:0.904]
Epoch [17/120    avg_loss:0.530, val_acc:0.917]
Epoch [18/120    avg_loss:0.497, val_acc:0.908]
Epoch [19/120    avg_loss:0.457, val_acc:0.931]
Epoch [20/120    avg_loss:0.474, val_acc:0.935]
Epoch [21/120    avg_loss:0.394, val_acc:0.935]
Epoch [22/120    avg_loss:0.406, val_acc:0.948]
Epoch [23/120    avg_loss:0.359, val_acc:0.935]
Epoch [24/120    avg_loss:0.371, val_acc:0.940]
Epoch [25/120    avg_loss:0.338, val_acc:0.944]
Epoch [26/120    avg_loss:0.325, val_acc:0.938]
Epoch [27/120    avg_loss:0.365, val_acc:0.952]
Epoch [28/120    avg_loss:0.289, val_acc:0.963]
Epoch [29/120    avg_loss:0.246, val_acc:0.948]
Epoch [30/120    avg_loss:0.239, val_acc:0.960]
Epoch [31/120    avg_loss:0.233, val_acc:0.940]
Epoch [32/120    avg_loss:0.208, val_acc:0.963]
Epoch [33/120    avg_loss:0.226, val_acc:0.958]
Epoch [34/120    avg_loss:0.223, val_acc:0.967]
Epoch [35/120    avg_loss:0.212, val_acc:0.965]
Epoch [36/120    avg_loss:0.281, val_acc:0.940]
Epoch [37/120    avg_loss:0.256, val_acc:0.935]
Epoch [38/120    avg_loss:0.176, val_acc:0.946]
Epoch [39/120    avg_loss:0.216, val_acc:0.938]
Epoch [40/120    avg_loss:0.197, val_acc:0.952]
Epoch [41/120    avg_loss:0.179, val_acc:0.960]
Epoch [42/120    avg_loss:0.141, val_acc:0.965]
Epoch [43/120    avg_loss:0.179, val_acc:0.975]
Epoch [44/120    avg_loss:0.142, val_acc:0.948]
Epoch [45/120    avg_loss:0.155, val_acc:0.954]
Epoch [46/120    avg_loss:0.137, val_acc:0.967]
Epoch [47/120    avg_loss:0.168, val_acc:0.954]
Epoch [48/120    avg_loss:0.164, val_acc:0.952]
Epoch [49/120    avg_loss:0.140, val_acc:0.975]
Epoch [50/120    avg_loss:0.125, val_acc:0.967]
Epoch [51/120    avg_loss:0.117, val_acc:0.971]
Epoch [52/120    avg_loss:0.096, val_acc:0.973]
Epoch [53/120    avg_loss:0.150, val_acc:0.967]
Epoch [54/120    avg_loss:0.132, val_acc:0.965]
Epoch [55/120    avg_loss:0.091, val_acc:0.979]
Epoch [56/120    avg_loss:0.094, val_acc:0.975]
Epoch [57/120    avg_loss:0.081, val_acc:0.983]
Epoch [58/120    avg_loss:0.066, val_acc:0.981]
Epoch [59/120    avg_loss:0.144, val_acc:0.971]
Epoch [60/120    avg_loss:0.162, val_acc:0.973]
Epoch [61/120    avg_loss:0.110, val_acc:0.958]
Epoch [62/120    avg_loss:0.101, val_acc:0.977]
Epoch [63/120    avg_loss:0.090, val_acc:0.981]
Epoch [64/120    avg_loss:0.064, val_acc:0.981]
Epoch [65/120    avg_loss:0.096, val_acc:0.977]
Epoch [66/120    avg_loss:0.147, val_acc:0.958]
Epoch [67/120    avg_loss:0.102, val_acc:0.981]
Epoch [68/120    avg_loss:0.076, val_acc:0.985]
Epoch [69/120    avg_loss:0.079, val_acc:0.985]
Epoch [70/120    avg_loss:0.059, val_acc:0.981]
Epoch [71/120    avg_loss:0.044, val_acc:0.985]
Epoch [72/120    avg_loss:0.041, val_acc:0.988]
Epoch [73/120    avg_loss:0.055, val_acc:0.979]
Epoch [74/120    avg_loss:0.039, val_acc:0.979]
Epoch [75/120    avg_loss:0.052, val_acc:0.985]
Epoch [76/120    avg_loss:0.042, val_acc:0.979]
Epoch [77/120    avg_loss:0.054, val_acc:0.985]
Epoch [78/120    avg_loss:0.043, val_acc:0.992]
Epoch [79/120    avg_loss:0.030, val_acc:0.988]
Epoch [80/120    avg_loss:0.075, val_acc:0.985]
Epoch [81/120    avg_loss:0.050, val_acc:0.988]
Epoch [82/120    avg_loss:0.052, val_acc:0.979]
Epoch [83/120    avg_loss:0.059, val_acc:0.988]
Epoch [84/120    avg_loss:0.059, val_acc:0.985]
Epoch [85/120    avg_loss:0.045, val_acc:0.985]
Epoch [86/120    avg_loss:0.034, val_acc:0.990]
Epoch [87/120    avg_loss:0.026, val_acc:0.996]
Epoch [88/120    avg_loss:0.035, val_acc:0.983]
Epoch [89/120    avg_loss:0.036, val_acc:0.992]
Epoch [90/120    avg_loss:0.045, val_acc:0.990]
Epoch [91/120    avg_loss:0.042, val_acc:0.981]
Epoch [92/120    avg_loss:0.052, val_acc:0.988]
Epoch [93/120    avg_loss:0.076, val_acc:0.977]
Epoch [94/120    avg_loss:0.056, val_acc:0.990]
Epoch [95/120    avg_loss:0.056, val_acc:0.992]
Epoch [96/120    avg_loss:0.040, val_acc:0.969]
Epoch [97/120    avg_loss:0.087, val_acc:0.981]
Epoch [98/120    avg_loss:0.073, val_acc:0.981]
Epoch [99/120    avg_loss:0.043, val_acc:0.988]
Epoch [100/120    avg_loss:0.033, val_acc:0.988]
Epoch [101/120    avg_loss:0.020, val_acc:0.988]
Epoch [102/120    avg_loss:0.027, val_acc:0.992]
Epoch [103/120    avg_loss:0.025, val_acc:0.990]
Epoch [104/120    avg_loss:0.018, val_acc:0.990]
Epoch [105/120    avg_loss:0.018, val_acc:0.990]
Epoch [106/120    avg_loss:0.029, val_acc:0.996]
Epoch [107/120    avg_loss:0.018, val_acc:0.994]
Epoch [108/120    avg_loss:0.019, val_acc:0.994]
Epoch [109/120    avg_loss:0.018, val_acc:0.994]
Epoch [110/120    avg_loss:0.014, val_acc:0.994]
Epoch [111/120    avg_loss:0.018, val_acc:0.994]
Epoch [112/120    avg_loss:0.019, val_acc:0.994]
Epoch [113/120    avg_loss:0.018, val_acc:0.994]
Epoch [114/120    avg_loss:0.018, val_acc:0.992]
Epoch [115/120    avg_loss:0.020, val_acc:0.994]
Epoch [116/120    avg_loss:0.015, val_acc:0.994]
Epoch [117/120    avg_loss:0.014, val_acc:0.996]
Epoch [118/120    avg_loss:0.019, val_acc:0.994]
Epoch [119/120    avg_loss:0.023, val_acc:0.992]
Epoch [120/120    avg_loss:0.014, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  17 436   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99780541 0.9977221  1.         0.97757848 0.96644295
 0.99277108 0.99465241 1.         1.         1.         0.97795071
 0.98087739 1.        ]

Kappa:
0.9926423889286147
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb5dbc9a860>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.544, val_acc:0.406]
Epoch [2/120    avg_loss:2.318, val_acc:0.398]
Epoch [3/120    avg_loss:2.148, val_acc:0.556]
Epoch [4/120    avg_loss:1.971, val_acc:0.629]
Epoch [5/120    avg_loss:1.814, val_acc:0.640]
Epoch [6/120    avg_loss:1.624, val_acc:0.677]
Epoch [7/120    avg_loss:1.465, val_acc:0.673]
Epoch [8/120    avg_loss:1.301, val_acc:0.719]
Epoch [9/120    avg_loss:1.172, val_acc:0.735]
Epoch [10/120    avg_loss:1.043, val_acc:0.760]
Epoch [11/120    avg_loss:0.932, val_acc:0.758]
Epoch [12/120    avg_loss:0.877, val_acc:0.765]
Epoch [13/120    avg_loss:0.797, val_acc:0.775]
Epoch [14/120    avg_loss:0.773, val_acc:0.808]
Epoch [15/120    avg_loss:0.679, val_acc:0.860]
Epoch [16/120    avg_loss:0.640, val_acc:0.846]
Epoch [17/120    avg_loss:0.652, val_acc:0.850]
Epoch [18/120    avg_loss:0.569, val_acc:0.819]
Epoch [19/120    avg_loss:0.569, val_acc:0.846]
Epoch [20/120    avg_loss:0.527, val_acc:0.929]
Epoch [21/120    avg_loss:0.501, val_acc:0.925]
Epoch [22/120    avg_loss:0.633, val_acc:0.825]
Epoch [23/120    avg_loss:0.481, val_acc:0.927]
Epoch [24/120    avg_loss:0.399, val_acc:0.850]
Epoch [25/120    avg_loss:0.411, val_acc:0.902]
Epoch [26/120    avg_loss:0.394, val_acc:0.915]
Epoch [27/120    avg_loss:0.431, val_acc:0.896]
Epoch [28/120    avg_loss:0.390, val_acc:0.933]
Epoch [29/120    avg_loss:0.295, val_acc:0.954]
Epoch [30/120    avg_loss:0.313, val_acc:0.950]
Epoch [31/120    avg_loss:0.320, val_acc:0.944]
Epoch [32/120    avg_loss:0.290, val_acc:0.952]
Epoch [33/120    avg_loss:0.306, val_acc:0.960]
Epoch [34/120    avg_loss:0.306, val_acc:0.956]
Epoch [35/120    avg_loss:0.257, val_acc:0.946]
Epoch [36/120    avg_loss:0.271, val_acc:0.958]
Epoch [37/120    avg_loss:0.192, val_acc:0.963]
Epoch [38/120    avg_loss:0.247, val_acc:0.940]
Epoch [39/120    avg_loss:0.280, val_acc:0.956]
Epoch [40/120    avg_loss:0.226, val_acc:0.977]
Epoch [41/120    avg_loss:0.275, val_acc:0.925]
Epoch [42/120    avg_loss:0.262, val_acc:0.975]
Epoch [43/120    avg_loss:0.220, val_acc:0.927]
Epoch [44/120    avg_loss:0.209, val_acc:0.954]
Epoch [45/120    avg_loss:0.202, val_acc:0.981]
Epoch [46/120    avg_loss:0.177, val_acc:0.971]
Epoch [47/120    avg_loss:0.179, val_acc:0.963]
Epoch [48/120    avg_loss:0.222, val_acc:0.963]
Epoch [49/120    avg_loss:0.211, val_acc:0.979]
Epoch [50/120    avg_loss:0.134, val_acc:0.983]
Epoch [51/120    avg_loss:0.173, val_acc:0.981]
Epoch [52/120    avg_loss:0.157, val_acc:0.971]
Epoch [53/120    avg_loss:0.153, val_acc:0.967]
Epoch [54/120    avg_loss:0.120, val_acc:0.977]
Epoch [55/120    avg_loss:0.156, val_acc:0.946]
Epoch [56/120    avg_loss:0.127, val_acc:0.958]
Epoch [57/120    avg_loss:0.119, val_acc:0.973]
Epoch [58/120    avg_loss:0.190, val_acc:0.956]
Epoch [59/120    avg_loss:0.155, val_acc:0.965]
Epoch [60/120    avg_loss:0.130, val_acc:0.990]
Epoch [61/120    avg_loss:0.110, val_acc:0.988]
Epoch [62/120    avg_loss:0.096, val_acc:0.985]
Epoch [63/120    avg_loss:0.124, val_acc:0.971]
Epoch [64/120    avg_loss:0.120, val_acc:0.977]
Epoch [65/120    avg_loss:0.104, val_acc:0.973]
Epoch [66/120    avg_loss:0.105, val_acc:0.979]
Epoch [67/120    avg_loss:0.087, val_acc:0.988]
Epoch [68/120    avg_loss:0.100, val_acc:0.954]
Epoch [69/120    avg_loss:0.094, val_acc:0.969]
Epoch [70/120    avg_loss:0.108, val_acc:0.985]
Epoch [71/120    avg_loss:0.121, val_acc:0.975]
Epoch [72/120    avg_loss:0.111, val_acc:0.979]
Epoch [73/120    avg_loss:0.069, val_acc:0.985]
Epoch [74/120    avg_loss:0.040, val_acc:0.988]
Epoch [75/120    avg_loss:0.050, val_acc:0.990]
Epoch [76/120    avg_loss:0.046, val_acc:0.992]
Epoch [77/120    avg_loss:0.043, val_acc:0.992]
Epoch [78/120    avg_loss:0.048, val_acc:0.992]
Epoch [79/120    avg_loss:0.037, val_acc:0.992]
Epoch [80/120    avg_loss:0.052, val_acc:0.992]
Epoch [81/120    avg_loss:0.049, val_acc:0.992]
Epoch [82/120    avg_loss:0.039, val_acc:0.994]
Epoch [83/120    avg_loss:0.042, val_acc:0.994]
Epoch [84/120    avg_loss:0.041, val_acc:0.994]
Epoch [85/120    avg_loss:0.041, val_acc:0.992]
Epoch [86/120    avg_loss:0.037, val_acc:0.992]
Epoch [87/120    avg_loss:0.035, val_acc:0.990]
Epoch [88/120    avg_loss:0.040, val_acc:0.992]
Epoch [89/120    avg_loss:0.038, val_acc:0.992]
Epoch [90/120    avg_loss:0.044, val_acc:0.992]
Epoch [91/120    avg_loss:0.033, val_acc:0.994]
Epoch [92/120    avg_loss:0.038, val_acc:0.994]
Epoch [93/120    avg_loss:0.035, val_acc:0.994]
Epoch [94/120    avg_loss:0.040, val_acc:0.994]
Epoch [95/120    avg_loss:0.033, val_acc:0.992]
Epoch [96/120    avg_loss:0.039, val_acc:0.992]
Epoch [97/120    avg_loss:0.034, val_acc:0.994]
Epoch [98/120    avg_loss:0.033, val_acc:0.994]
Epoch [99/120    avg_loss:0.033, val_acc:0.992]
Epoch [100/120    avg_loss:0.037, val_acc:0.992]
Epoch [101/120    avg_loss:0.038, val_acc:0.992]
Epoch [102/120    avg_loss:0.035, val_acc:0.992]
Epoch [103/120    avg_loss:0.033, val_acc:0.994]
Epoch [104/120    avg_loss:0.042, val_acc:0.992]
Epoch [105/120    avg_loss:0.033, val_acc:0.992]
Epoch [106/120    avg_loss:0.034, val_acc:0.992]
Epoch [107/120    avg_loss:0.039, val_acc:0.992]
Epoch [108/120    avg_loss:0.040, val_acc:0.992]
Epoch [109/120    avg_loss:0.031, val_acc:0.992]
Epoch [110/120    avg_loss:0.037, val_acc:0.992]
Epoch [111/120    avg_loss:0.031, val_acc:0.994]
Epoch [112/120    avg_loss:0.028, val_acc:0.994]
Epoch [113/120    avg_loss:0.032, val_acc:0.994]
Epoch [114/120    avg_loss:0.034, val_acc:0.992]
Epoch [115/120    avg_loss:0.025, val_acc:0.992]
Epoch [116/120    avg_loss:0.034, val_acc:0.992]
Epoch [117/120    avg_loss:0.029, val_acc:0.992]
Epoch [118/120    avg_loss:0.026, val_acc:0.994]
Epoch [119/120    avg_loss:0.028, val_acc:0.992]
Epoch [120/120    avg_loss:0.028, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 0.99560117 0.99319728 1.         0.96875    0.9527027
 0.98564593 0.98378378 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.994541052956847
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe225e1a898>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.535, val_acc:0.392]
Epoch [2/120    avg_loss:2.291, val_acc:0.571]
Epoch [3/120    avg_loss:2.113, val_acc:0.623]
Epoch [4/120    avg_loss:1.931, val_acc:0.635]
Epoch [5/120    avg_loss:1.739, val_acc:0.660]
Epoch [6/120    avg_loss:1.556, val_acc:0.677]
Epoch [7/120    avg_loss:1.393, val_acc:0.704]
Epoch [8/120    avg_loss:1.244, val_acc:0.719]
Epoch [9/120    avg_loss:1.122, val_acc:0.758]
Epoch [10/120    avg_loss:1.018, val_acc:0.746]
Epoch [11/120    avg_loss:0.941, val_acc:0.762]
Epoch [12/120    avg_loss:0.863, val_acc:0.790]
Epoch [13/120    avg_loss:0.824, val_acc:0.785]
Epoch [14/120    avg_loss:0.735, val_acc:0.783]
Epoch [15/120    avg_loss:0.659, val_acc:0.812]
Epoch [16/120    avg_loss:0.644, val_acc:0.821]
Epoch [17/120    avg_loss:0.557, val_acc:0.875]
Epoch [18/120    avg_loss:0.566, val_acc:0.863]
Epoch [19/120    avg_loss:0.494, val_acc:0.904]
Epoch [20/120    avg_loss:0.462, val_acc:0.900]
Epoch [21/120    avg_loss:0.422, val_acc:0.898]
Epoch [22/120    avg_loss:0.405, val_acc:0.900]
Epoch [23/120    avg_loss:0.406, val_acc:0.935]
Epoch [24/120    avg_loss:0.427, val_acc:0.940]
Epoch [25/120    avg_loss:0.379, val_acc:0.921]
Epoch [26/120    avg_loss:0.342, val_acc:0.931]
Epoch [27/120    avg_loss:0.323, val_acc:0.954]
Epoch [28/120    avg_loss:0.300, val_acc:0.960]
Epoch [29/120    avg_loss:0.280, val_acc:0.971]
Epoch [30/120    avg_loss:0.282, val_acc:0.933]
Epoch [31/120    avg_loss:0.359, val_acc:0.929]
Epoch [32/120    avg_loss:0.319, val_acc:0.958]
Epoch [33/120    avg_loss:0.311, val_acc:0.960]
Epoch [34/120    avg_loss:0.298, val_acc:0.954]
Epoch [35/120    avg_loss:0.261, val_acc:0.954]
Epoch [36/120    avg_loss:0.229, val_acc:0.967]
Epoch [37/120    avg_loss:0.220, val_acc:0.965]
Epoch [38/120    avg_loss:0.184, val_acc:0.979]
Epoch [39/120    avg_loss:0.215, val_acc:0.975]
Epoch [40/120    avg_loss:0.210, val_acc:0.967]
Epoch [41/120    avg_loss:0.241, val_acc:0.979]
Epoch [42/120    avg_loss:0.177, val_acc:0.973]
Epoch [43/120    avg_loss:0.240, val_acc:0.940]
Epoch [44/120    avg_loss:0.226, val_acc:0.952]
Epoch [45/120    avg_loss:0.175, val_acc:0.942]
Epoch [46/120    avg_loss:0.212, val_acc:0.950]
Epoch [47/120    avg_loss:0.172, val_acc:0.935]
Epoch [48/120    avg_loss:0.170, val_acc:0.983]
Epoch [49/120    avg_loss:0.193, val_acc:0.975]
Epoch [50/120    avg_loss:0.151, val_acc:0.977]
Epoch [51/120    avg_loss:0.164, val_acc:0.969]
Epoch [52/120    avg_loss:0.137, val_acc:0.977]
Epoch [53/120    avg_loss:0.136, val_acc:0.983]
Epoch [54/120    avg_loss:0.103, val_acc:0.992]
Epoch [55/120    avg_loss:0.133, val_acc:0.990]
Epoch [56/120    avg_loss:0.123, val_acc:0.994]
Epoch [57/120    avg_loss:0.125, val_acc:0.994]
Epoch [58/120    avg_loss:0.139, val_acc:0.988]
Epoch [59/120    avg_loss:0.183, val_acc:0.977]
Epoch [60/120    avg_loss:0.157, val_acc:0.983]
Epoch [61/120    avg_loss:0.120, val_acc:0.983]
Epoch [62/120    avg_loss:0.098, val_acc:0.990]
Epoch [63/120    avg_loss:0.099, val_acc:0.988]
Epoch [64/120    avg_loss:0.080, val_acc:0.992]
Epoch [65/120    avg_loss:0.064, val_acc:0.996]
Epoch [66/120    avg_loss:0.069, val_acc:0.992]
Epoch [67/120    avg_loss:0.072, val_acc:0.985]
Epoch [68/120    avg_loss:0.111, val_acc:0.983]
Epoch [69/120    avg_loss:0.069, val_acc:0.994]
Epoch [70/120    avg_loss:0.101, val_acc:0.996]
Epoch [71/120    avg_loss:0.092, val_acc:0.994]
Epoch [72/120    avg_loss:0.079, val_acc:0.981]
Epoch [73/120    avg_loss:0.078, val_acc:0.992]
Epoch [74/120    avg_loss:0.070, val_acc:0.996]
Epoch [75/120    avg_loss:0.062, val_acc:0.990]
Epoch [76/120    avg_loss:0.052, val_acc:0.998]
Epoch [77/120    avg_loss:0.041, val_acc:1.000]
Epoch [78/120    avg_loss:0.042, val_acc:0.950]
Epoch [79/120    avg_loss:0.078, val_acc:0.990]
Epoch [80/120    avg_loss:0.078, val_acc:0.996]
Epoch [81/120    avg_loss:0.090, val_acc:0.998]
Epoch [82/120    avg_loss:0.063, val_acc:0.998]
Epoch [83/120    avg_loss:0.040, val_acc:0.994]
Epoch [84/120    avg_loss:0.056, val_acc:0.998]
Epoch [85/120    avg_loss:0.041, val_acc:0.996]
Epoch [86/120    avg_loss:0.066, val_acc:0.998]
Epoch [87/120    avg_loss:0.045, val_acc:1.000]
Epoch [88/120    avg_loss:0.034, val_acc:1.000]
Epoch [89/120    avg_loss:0.026, val_acc:1.000]
Epoch [90/120    avg_loss:0.033, val_acc:1.000]
Epoch [91/120    avg_loss:0.021, val_acc:1.000]
Epoch [92/120    avg_loss:0.020, val_acc:1.000]
Epoch [93/120    avg_loss:0.019, val_acc:1.000]
Epoch [94/120    avg_loss:0.028, val_acc:1.000]
Epoch [95/120    avg_loss:0.021, val_acc:0.998]
Epoch [96/120    avg_loss:0.020, val_acc:1.000]
Epoch [97/120    avg_loss:0.017, val_acc:1.000]
Epoch [98/120    avg_loss:0.027, val_acc:0.998]
Epoch [99/120    avg_loss:0.023, val_acc:0.998]
Epoch [100/120    avg_loss:0.022, val_acc:1.000]
Epoch [101/120    avg_loss:0.081, val_acc:0.990]
Epoch [102/120    avg_loss:0.055, val_acc:0.996]
Epoch [103/120    avg_loss:0.040, val_acc:0.992]
Epoch [104/120    avg_loss:0.025, val_acc:1.000]
Epoch [105/120    avg_loss:0.028, val_acc:1.000]
Epoch [106/120    avg_loss:0.018, val_acc:1.000]
Epoch [107/120    avg_loss:0.037, val_acc:1.000]
Epoch [108/120    avg_loss:0.033, val_acc:1.000]
Epoch [109/120    avg_loss:0.021, val_acc:1.000]
Epoch [110/120    avg_loss:0.021, val_acc:1.000]
Epoch [111/120    avg_loss:0.023, val_acc:0.998]
Epoch [112/120    avg_loss:0.022, val_acc:1.000]
Epoch [113/120    avg_loss:0.012, val_acc:1.000]
Epoch [114/120    avg_loss:0.009, val_acc:1.000]
Epoch [115/120    avg_loss:0.012, val_acc:1.000]
Epoch [116/120    avg_loss:0.009, val_acc:1.000]
Epoch [117/120    avg_loss:0.011, val_acc:0.998]
Epoch [118/120    avg_loss:0.010, val_acc:1.000]
Epoch [119/120    avg_loss:0.011, val_acc:1.000]
Epoch [120/120    avg_loss:0.010, val_acc:1.000]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.53091684434968

F1 scores:
[       nan 0.99560117 1.         1.         0.96536797 0.94326241
 0.98564593 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9947781942086641
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:20
Validation dataloader:20
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5f0950e860>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.533, val_acc:0.323]
Epoch [2/120    avg_loss:2.243, val_acc:0.396]
Epoch [3/120    avg_loss:2.074, val_acc:0.535]
Epoch [4/120    avg_loss:1.879, val_acc:0.594]
Epoch [5/120    avg_loss:1.699, val_acc:0.602]
Epoch [6/120    avg_loss:1.506, val_acc:0.640]
Epoch [7/120    avg_loss:1.365, val_acc:0.710]
Epoch [8/120    avg_loss:1.246, val_acc:0.710]
Epoch [9/120    avg_loss:1.099, val_acc:0.752]
Epoch [10/120    avg_loss:0.991, val_acc:0.777]
Epoch [11/120    avg_loss:0.886, val_acc:0.802]
Epoch [12/120    avg_loss:0.871, val_acc:0.792]
Epoch [13/120    avg_loss:0.786, val_acc:0.790]
Epoch [14/120    avg_loss:0.708, val_acc:0.808]
Epoch [15/120    avg_loss:0.689, val_acc:0.808]
Epoch [16/120    avg_loss:0.577, val_acc:0.842]
Epoch [17/120    avg_loss:0.587, val_acc:0.879]
Epoch [18/120    avg_loss:0.529, val_acc:0.887]
Epoch [19/120    avg_loss:0.475, val_acc:0.917]
Epoch [20/120    avg_loss:0.491, val_acc:0.912]
Epoch [21/120    avg_loss:0.437, val_acc:0.894]
Epoch [22/120    avg_loss:0.472, val_acc:0.875]
Epoch [23/120    avg_loss:0.444, val_acc:0.869]
Epoch [24/120    avg_loss:0.434, val_acc:0.921]
Epoch [25/120    avg_loss:0.396, val_acc:0.923]
Epoch [26/120    avg_loss:0.414, val_acc:0.906]
Epoch [27/120    avg_loss:0.380, val_acc:0.910]
Epoch [28/120    avg_loss:0.372, val_acc:0.923]
Epoch [29/120    avg_loss:0.332, val_acc:0.944]
Epoch [30/120    avg_loss:0.313, val_acc:0.935]
Epoch [31/120    avg_loss:0.303, val_acc:0.933]
Epoch [32/120    avg_loss:0.269, val_acc:0.946]
Epoch [33/120    avg_loss:0.277, val_acc:0.950]
Epoch [34/120    avg_loss:0.253, val_acc:0.942]
Epoch [35/120    avg_loss:0.231, val_acc:0.927]
Epoch [36/120    avg_loss:0.292, val_acc:0.919]
Epoch [37/120    avg_loss:0.307, val_acc:0.906]
Epoch [38/120    avg_loss:0.275, val_acc:0.954]
Epoch [39/120    avg_loss:0.194, val_acc:0.915]
Epoch [40/120    avg_loss:0.254, val_acc:0.879]
Epoch [41/120    avg_loss:0.232, val_acc:0.935]
Epoch [42/120    avg_loss:0.202, val_acc:0.954]
Epoch [43/120    avg_loss:0.222, val_acc:0.954]
Epoch [44/120    avg_loss:0.203, val_acc:0.956]
Epoch [45/120    avg_loss:0.220, val_acc:0.952]
Epoch [46/120    avg_loss:0.196, val_acc:0.954]
Epoch [47/120    avg_loss:0.158, val_acc:0.971]
Epoch [48/120    avg_loss:0.176, val_acc:0.963]
Epoch [49/120    avg_loss:0.130, val_acc:0.973]
Epoch [50/120    avg_loss:0.107, val_acc:0.960]
Epoch [51/120    avg_loss:0.160, val_acc:0.958]
Epoch [52/120    avg_loss:0.152, val_acc:0.954]
Epoch [53/120    avg_loss:0.150, val_acc:0.965]
Epoch [54/120    avg_loss:0.150, val_acc:0.967]
Epoch [55/120    avg_loss:0.147, val_acc:0.971]
Epoch [56/120    avg_loss:0.098, val_acc:0.983]
Epoch [57/120    avg_loss:0.119, val_acc:0.977]
Epoch [58/120    avg_loss:0.134, val_acc:0.963]
Epoch [59/120    avg_loss:0.117, val_acc:0.969]
Epoch [60/120    avg_loss:0.117, val_acc:0.969]
Epoch [61/120    avg_loss:0.127, val_acc:0.946]
Epoch [62/120    avg_loss:0.164, val_acc:0.948]
Epoch [63/120    avg_loss:0.150, val_acc:0.967]
Epoch [64/120    avg_loss:0.109, val_acc:0.967]
Epoch [65/120    avg_loss:0.117, val_acc:0.977]
Epoch [66/120    avg_loss:0.083, val_acc:0.973]
Epoch [67/120    avg_loss:0.099, val_acc:0.975]
Epoch [68/120    avg_loss:0.093, val_acc:0.977]
Epoch [69/120    avg_loss:0.069, val_acc:0.983]
Epoch [70/120    avg_loss:0.091, val_acc:0.971]
Epoch [71/120    avg_loss:0.097, val_acc:0.963]
Epoch [72/120    avg_loss:0.084, val_acc:0.973]
Epoch [73/120    avg_loss:0.203, val_acc:0.965]
Epoch [74/120    avg_loss:0.118, val_acc:0.975]
Epoch [75/120    avg_loss:0.121, val_acc:0.975]
Epoch [76/120    avg_loss:0.083, val_acc:0.990]
Epoch [77/120    avg_loss:0.062, val_acc:0.983]
Epoch [78/120    avg_loss:0.074, val_acc:0.983]
Epoch [79/120    avg_loss:0.063, val_acc:0.977]
Epoch [80/120    avg_loss:0.066, val_acc:0.990]
Epoch [81/120    avg_loss:0.098, val_acc:0.977]
Epoch [82/120    avg_loss:0.076, val_acc:0.973]
Epoch [83/120    avg_loss:0.060, val_acc:0.994]
Epoch [84/120    avg_loss:0.065, val_acc:0.990]
Epoch [85/120    avg_loss:0.044, val_acc:0.983]
Epoch [86/120    avg_loss:0.071, val_acc:0.990]
Epoch [87/120    avg_loss:0.053, val_acc:0.983]
Epoch [88/120    avg_loss:0.038, val_acc:0.988]
Epoch [89/120    avg_loss:0.033, val_acc:0.996]
Epoch [90/120    avg_loss:0.034, val_acc:0.988]
Epoch [91/120    avg_loss:0.059, val_acc:0.992]
Epoch [92/120    avg_loss:0.077, val_acc:0.988]
Epoch [93/120    avg_loss:0.040, val_acc:0.996]
Epoch [94/120    avg_loss:0.043, val_acc:0.985]
Epoch [95/120    avg_loss:0.036, val_acc:0.992]
Epoch [96/120    avg_loss:0.029, val_acc:0.992]
Epoch [97/120    avg_loss:0.036, val_acc:0.996]
Epoch [98/120    avg_loss:0.108, val_acc:0.956]
Epoch [99/120    avg_loss:0.078, val_acc:0.981]
Epoch [100/120    avg_loss:0.048, val_acc:0.967]
Epoch [101/120    avg_loss:0.036, val_acc:0.988]
Epoch [102/120    avg_loss:0.030, val_acc:0.996]
Epoch [103/120    avg_loss:0.027, val_acc:0.992]
Epoch [104/120    avg_loss:0.033, val_acc:0.996]
Epoch [105/120    avg_loss:0.023, val_acc:0.994]
Epoch [106/120    avg_loss:0.026, val_acc:0.988]
Epoch [107/120    avg_loss:0.047, val_acc:0.992]
Epoch [108/120    avg_loss:0.080, val_acc:0.975]
Epoch [109/120    avg_loss:0.049, val_acc:0.992]
Epoch [110/120    avg_loss:0.039, val_acc:0.996]
Epoch [111/120    avg_loss:0.030, val_acc:0.998]
Epoch [112/120    avg_loss:0.066, val_acc:0.973]
Epoch [113/120    avg_loss:0.041, val_acc:0.994]
Epoch [114/120    avg_loss:0.027, val_acc:0.992]
Epoch [115/120    avg_loss:0.019, val_acc:0.996]
Epoch [116/120    avg_loss:0.014, val_acc:0.992]
Epoch [117/120    avg_loss:0.016, val_acc:0.996]
Epoch [118/120    avg_loss:0.015, val_acc:0.996]
Epoch [119/120    avg_loss:0.019, val_acc:0.990]
Epoch [120/120    avg_loss:0.028, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 226   1   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.82942430703625

F1 scores:
[       nan 0.99560117 0.9977221  1.         0.99779249 0.99656357
 0.98564593 1.         1.         1.         1.         1.
 0.99889503 1.        ]

Kappa:
0.9981012549328405
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8742bc5908>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.580, val_acc:0.275]
Epoch [2/120    avg_loss:2.356, val_acc:0.301]
Epoch [3/120    avg_loss:2.211, val_acc:0.426]
Epoch [4/120    avg_loss:2.075, val_acc:0.531]
Epoch [5/120    avg_loss:1.959, val_acc:0.584]
Epoch [6/120    avg_loss:1.813, val_acc:0.604]
Epoch [7/120    avg_loss:1.686, val_acc:0.625]
Epoch [8/120    avg_loss:1.528, val_acc:0.631]
Epoch [9/120    avg_loss:1.397, val_acc:0.664]
Epoch [10/120    avg_loss:1.276, val_acc:0.676]
Epoch [11/120    avg_loss:1.185, val_acc:0.701]
Epoch [12/120    avg_loss:1.121, val_acc:0.711]
Epoch [13/120    avg_loss:1.013, val_acc:0.750]
Epoch [14/120    avg_loss:0.950, val_acc:0.859]
Epoch [15/120    avg_loss:0.882, val_acc:0.873]
Epoch [16/120    avg_loss:0.850, val_acc:0.793]
Epoch [17/120    avg_loss:0.820, val_acc:0.902]
Epoch [18/120    avg_loss:0.766, val_acc:0.908]
Epoch [19/120    avg_loss:0.739, val_acc:0.918]
Epoch [20/120    avg_loss:0.670, val_acc:0.912]
Epoch [21/120    avg_loss:0.621, val_acc:0.914]
Epoch [22/120    avg_loss:0.561, val_acc:0.914]
Epoch [23/120    avg_loss:0.542, val_acc:0.916]
Epoch [24/120    avg_loss:0.517, val_acc:0.904]
Epoch [25/120    avg_loss:0.551, val_acc:0.906]
Epoch [26/120    avg_loss:0.509, val_acc:0.918]
Epoch [27/120    avg_loss:0.435, val_acc:0.904]
Epoch [28/120    avg_loss:0.511, val_acc:0.934]
Epoch [29/120    avg_loss:0.447, val_acc:0.910]
Epoch [30/120    avg_loss:0.470, val_acc:0.922]
Epoch [31/120    avg_loss:0.424, val_acc:0.904]
Epoch [32/120    avg_loss:0.437, val_acc:0.906]
Epoch [33/120    avg_loss:0.409, val_acc:0.918]
Epoch [34/120    avg_loss:0.372, val_acc:0.945]
Epoch [35/120    avg_loss:0.327, val_acc:0.910]
Epoch [36/120    avg_loss:0.372, val_acc:0.924]
Epoch [37/120    avg_loss:0.365, val_acc:0.932]
Epoch [38/120    avg_loss:0.302, val_acc:0.938]
Epoch [39/120    avg_loss:0.330, val_acc:0.934]
Epoch [40/120    avg_loss:0.296, val_acc:0.945]
Epoch [41/120    avg_loss:0.272, val_acc:0.953]
Epoch [42/120    avg_loss:0.280, val_acc:0.912]
Epoch [43/120    avg_loss:0.299, val_acc:0.928]
Epoch [44/120    avg_loss:0.315, val_acc:0.945]
Epoch [45/120    avg_loss:0.342, val_acc:0.904]
Epoch [46/120    avg_loss:0.308, val_acc:0.943]
Epoch [47/120    avg_loss:0.269, val_acc:0.947]
Epoch [48/120    avg_loss:0.251, val_acc:0.930]
Epoch [49/120    avg_loss:0.326, val_acc:0.930]
Epoch [50/120    avg_loss:0.236, val_acc:0.945]
Epoch [51/120    avg_loss:0.230, val_acc:0.936]
Epoch [52/120    avg_loss:0.209, val_acc:0.932]
Epoch [53/120    avg_loss:0.259, val_acc:0.930]
Epoch [54/120    avg_loss:0.229, val_acc:0.941]
Epoch [55/120    avg_loss:0.206, val_acc:0.959]
Epoch [56/120    avg_loss:0.180, val_acc:0.959]
Epoch [57/120    avg_loss:0.180, val_acc:0.959]
Epoch [58/120    avg_loss:0.155, val_acc:0.961]
Epoch [59/120    avg_loss:0.167, val_acc:0.963]
Epoch [60/120    avg_loss:0.141, val_acc:0.961]
Epoch [61/120    avg_loss:0.166, val_acc:0.963]
Epoch [62/120    avg_loss:0.137, val_acc:0.961]
Epoch [63/120    avg_loss:0.155, val_acc:0.961]
Epoch [64/120    avg_loss:0.144, val_acc:0.963]
Epoch [65/120    avg_loss:0.144, val_acc:0.965]
Epoch [66/120    avg_loss:0.153, val_acc:0.963]
Epoch [67/120    avg_loss:0.160, val_acc:0.961]
Epoch [68/120    avg_loss:0.154, val_acc:0.965]
Epoch [69/120    avg_loss:0.145, val_acc:0.965]
Epoch [70/120    avg_loss:0.162, val_acc:0.965]
Epoch [71/120    avg_loss:0.166, val_acc:0.965]
Epoch [72/120    avg_loss:0.158, val_acc:0.963]
Epoch [73/120    avg_loss:0.156, val_acc:0.967]
Epoch [74/120    avg_loss:0.169, val_acc:0.965]
Epoch [75/120    avg_loss:0.138, val_acc:0.963]
Epoch [76/120    avg_loss:0.145, val_acc:0.955]
Epoch [77/120    avg_loss:0.149, val_acc:0.963]
Epoch [78/120    avg_loss:0.131, val_acc:0.963]
Epoch [79/120    avg_loss:0.136, val_acc:0.965]
Epoch [80/120    avg_loss:0.128, val_acc:0.963]
Epoch [81/120    avg_loss:0.135, val_acc:0.959]
Epoch [82/120    avg_loss:0.141, val_acc:0.963]
Epoch [83/120    avg_loss:0.139, val_acc:0.963]
Epoch [84/120    avg_loss:0.141, val_acc:0.967]
Epoch [85/120    avg_loss:0.133, val_acc:0.959]
Epoch [86/120    avg_loss:0.140, val_acc:0.965]
Epoch [87/120    avg_loss:0.140, val_acc:0.965]
Epoch [88/120    avg_loss:0.124, val_acc:0.957]
Epoch [89/120    avg_loss:0.125, val_acc:0.961]
Epoch [90/120    avg_loss:0.136, val_acc:0.967]
Epoch [91/120    avg_loss:0.136, val_acc:0.967]
Epoch [92/120    avg_loss:0.124, val_acc:0.963]
Epoch [93/120    avg_loss:0.135, val_acc:0.965]
Epoch [94/120    avg_loss:0.135, val_acc:0.969]
Epoch [95/120    avg_loss:0.114, val_acc:0.963]
Epoch [96/120    avg_loss:0.130, val_acc:0.961]
Epoch [97/120    avg_loss:0.140, val_acc:0.965]
Epoch [98/120    avg_loss:0.125, val_acc:0.967]
Epoch [99/120    avg_loss:0.118, val_acc:0.957]
Epoch [100/120    avg_loss:0.124, val_acc:0.961]
Epoch [101/120    avg_loss:0.114, val_acc:0.961]
Epoch [102/120    avg_loss:0.118, val_acc:0.959]
Epoch [103/120    avg_loss:0.125, val_acc:0.963]
Epoch [104/120    avg_loss:0.118, val_acc:0.965]
Epoch [105/120    avg_loss:0.144, val_acc:0.967]
Epoch [106/120    avg_loss:0.118, val_acc:0.971]
Epoch [107/120    avg_loss:0.120, val_acc:0.959]
Epoch [108/120    avg_loss:0.108, val_acc:0.963]
Epoch [109/120    avg_loss:0.093, val_acc:0.967]
Epoch [110/120    avg_loss:0.142, val_acc:0.963]
Epoch [111/120    avg_loss:0.121, val_acc:0.969]
Epoch [112/120    avg_loss:0.106, val_acc:0.959]
Epoch [113/120    avg_loss:0.119, val_acc:0.965]
Epoch [114/120    avg_loss:0.122, val_acc:0.961]
Epoch [115/120    avg_loss:0.129, val_acc:0.965]
Epoch [116/120    avg_loss:0.112, val_acc:0.967]
Epoch [117/120    avg_loss:0.127, val_acc:0.969]
Epoch [118/120    avg_loss:0.129, val_acc:0.965]
Epoch [119/120    avg_loss:0.106, val_acc:0.963]
Epoch [120/120    avg_loss:0.102, val_acc:0.963]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 201   0   0   0   0  18   0   0   0   0   0   0]
 [  0   0   9 205   8   0   0   1   4   3   0   0   0   0]
 [  0   0   2   0 183  42   0   0   0   0   0   0   0   0]
 [  0   0   0   0  25 120   0   0   0   0   0   0   0   0]
 [  0   1   0   0  11   0 194   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 370   7   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
96.90831556503198

F1 scores:
[       nan 0.99927061 0.90337079 0.94252874 0.8061674  0.78175896
 0.97       0.82901554 0.99487179 0.99680511 1.         0.99062918
 0.99233297 1.        ]

Kappa:
0.9655789590739113
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9298883898>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.565, val_acc:0.301]
Epoch [2/120    avg_loss:2.337, val_acc:0.326]
Epoch [3/120    avg_loss:2.165, val_acc:0.406]
Epoch [4/120    avg_loss:1.991, val_acc:0.582]
Epoch [5/120    avg_loss:1.850, val_acc:0.701]
Epoch [6/120    avg_loss:1.710, val_acc:0.717]
Epoch [7/120    avg_loss:1.571, val_acc:0.750]
Epoch [8/120    avg_loss:1.468, val_acc:0.729]
Epoch [9/120    avg_loss:1.345, val_acc:0.775]
Epoch [10/120    avg_loss:1.235, val_acc:0.760]
Epoch [11/120    avg_loss:1.124, val_acc:0.799]
Epoch [12/120    avg_loss:1.085, val_acc:0.688]
Epoch [13/120    avg_loss:1.017, val_acc:0.838]
Epoch [14/120    avg_loss:0.909, val_acc:0.828]
Epoch [15/120    avg_loss:0.850, val_acc:0.850]
Epoch [16/120    avg_loss:0.765, val_acc:0.859]
Epoch [17/120    avg_loss:0.729, val_acc:0.840]
Epoch [18/120    avg_loss:0.699, val_acc:0.867]
Epoch [19/120    avg_loss:0.699, val_acc:0.883]
Epoch [20/120    avg_loss:0.616, val_acc:0.891]
Epoch [21/120    avg_loss:0.604, val_acc:0.896]
Epoch [22/120    avg_loss:0.554, val_acc:0.887]
Epoch [23/120    avg_loss:0.577, val_acc:0.883]
Epoch [24/120    avg_loss:0.519, val_acc:0.902]
Epoch [25/120    avg_loss:0.503, val_acc:0.881]
Epoch [26/120    avg_loss:0.497, val_acc:0.902]
Epoch [27/120    avg_loss:0.479, val_acc:0.918]
Epoch [28/120    avg_loss:0.424, val_acc:0.896]
Epoch [29/120    avg_loss:0.455, val_acc:0.910]
Epoch [30/120    avg_loss:0.451, val_acc:0.891]
Epoch [31/120    avg_loss:0.430, val_acc:0.904]
Epoch [32/120    avg_loss:0.410, val_acc:0.938]
Epoch [33/120    avg_loss:0.419, val_acc:0.904]
Epoch [34/120    avg_loss:0.381, val_acc:0.920]
Epoch [35/120    avg_loss:0.358, val_acc:0.930]
Epoch [36/120    avg_loss:0.336, val_acc:0.904]
Epoch [37/120    avg_loss:0.364, val_acc:0.855]
Epoch [38/120    avg_loss:0.422, val_acc:0.932]
Epoch [39/120    avg_loss:0.372, val_acc:0.908]
Epoch [40/120    avg_loss:0.425, val_acc:0.912]
Epoch [41/120    avg_loss:0.346, val_acc:0.934]
Epoch [42/120    avg_loss:0.311, val_acc:0.904]
Epoch [43/120    avg_loss:0.321, val_acc:0.902]
Epoch [44/120    avg_loss:0.314, val_acc:0.934]
Epoch [45/120    avg_loss:0.246, val_acc:0.941]
Epoch [46/120    avg_loss:0.282, val_acc:0.928]
Epoch [47/120    avg_loss:0.251, val_acc:0.928]
Epoch [48/120    avg_loss:0.307, val_acc:0.943]
Epoch [49/120    avg_loss:0.301, val_acc:0.934]
Epoch [50/120    avg_loss:0.323, val_acc:0.924]
Epoch [51/120    avg_loss:0.234, val_acc:0.926]
Epoch [52/120    avg_loss:0.236, val_acc:0.943]
Epoch [53/120    avg_loss:0.212, val_acc:0.941]
Epoch [54/120    avg_loss:0.248, val_acc:0.936]
Epoch [55/120    avg_loss:0.201, val_acc:0.922]
Epoch [56/120    avg_loss:0.283, val_acc:0.918]
Epoch [57/120    avg_loss:0.214, val_acc:0.947]
Epoch [58/120    avg_loss:0.187, val_acc:0.951]
Epoch [59/120    avg_loss:0.223, val_acc:0.959]
Epoch [60/120    avg_loss:0.169, val_acc:0.957]
Epoch [61/120    avg_loss:0.208, val_acc:0.949]
Epoch [62/120    avg_loss:0.175, val_acc:0.961]
Epoch [63/120    avg_loss:0.176, val_acc:0.947]
Epoch [64/120    avg_loss:0.158, val_acc:0.953]
Epoch [65/120    avg_loss:0.155, val_acc:0.967]
Epoch [66/120    avg_loss:0.164, val_acc:0.945]
Epoch [67/120    avg_loss:0.163, val_acc:0.965]
Epoch [68/120    avg_loss:0.137, val_acc:0.965]
Epoch [69/120    avg_loss:0.172, val_acc:0.957]
Epoch [70/120    avg_loss:0.202, val_acc:0.967]
Epoch [71/120    avg_loss:0.109, val_acc:0.967]
Epoch [72/120    avg_loss:0.153, val_acc:0.928]
Epoch [73/120    avg_loss:0.187, val_acc:0.959]
Epoch [74/120    avg_loss:0.154, val_acc:0.977]
Epoch [75/120    avg_loss:0.121, val_acc:0.977]
Epoch [76/120    avg_loss:0.109, val_acc:0.969]
Epoch [77/120    avg_loss:0.116, val_acc:0.969]
Epoch [78/120    avg_loss:0.091, val_acc:0.953]
Epoch [79/120    avg_loss:0.136, val_acc:0.977]
Epoch [80/120    avg_loss:0.138, val_acc:0.959]
Epoch [81/120    avg_loss:0.118, val_acc:0.951]
Epoch [82/120    avg_loss:0.094, val_acc:0.951]
Epoch [83/120    avg_loss:0.150, val_acc:0.971]
Epoch [84/120    avg_loss:0.129, val_acc:0.969]
Epoch [85/120    avg_loss:0.091, val_acc:0.975]
Epoch [86/120    avg_loss:0.066, val_acc:0.980]
Epoch [87/120    avg_loss:0.065, val_acc:0.977]
Epoch [88/120    avg_loss:0.097, val_acc:0.973]
Epoch [89/120    avg_loss:0.111, val_acc:0.975]
Epoch [90/120    avg_loss:0.074, val_acc:0.973]
Epoch [91/120    avg_loss:0.081, val_acc:0.971]
Epoch [92/120    avg_loss:0.111, val_acc:0.957]
Epoch [93/120    avg_loss:0.175, val_acc:0.957]
Epoch [94/120    avg_loss:0.128, val_acc:0.943]
Epoch [95/120    avg_loss:0.147, val_acc:0.963]
Epoch [96/120    avg_loss:0.105, val_acc:0.980]
Epoch [97/120    avg_loss:0.090, val_acc:0.977]
Epoch [98/120    avg_loss:0.085, val_acc:0.977]
Epoch [99/120    avg_loss:0.076, val_acc:0.979]
Epoch [100/120    avg_loss:0.065, val_acc:0.984]
Epoch [101/120    avg_loss:0.043, val_acc:0.984]
Epoch [102/120    avg_loss:0.045, val_acc:0.980]
Epoch [103/120    avg_loss:0.057, val_acc:0.979]
Epoch [104/120    avg_loss:0.046, val_acc:0.982]
Epoch [105/120    avg_loss:0.056, val_acc:0.984]
Epoch [106/120    avg_loss:0.104, val_acc:0.980]
Epoch [107/120    avg_loss:0.080, val_acc:0.984]
Epoch [108/120    avg_loss:0.064, val_acc:0.977]
Epoch [109/120    avg_loss:0.083, val_acc:0.977]
Epoch [110/120    avg_loss:0.070, val_acc:0.969]
Epoch [111/120    avg_loss:0.070, val_acc:0.979]
Epoch [112/120    avg_loss:0.052, val_acc:0.979]
Epoch [113/120    avg_loss:0.041, val_acc:0.977]
Epoch [114/120    avg_loss:0.053, val_acc:0.959]
Epoch [115/120    avg_loss:0.047, val_acc:0.980]
Epoch [116/120    avg_loss:0.059, val_acc:0.967]
Epoch [117/120    avg_loss:0.054, val_acc:0.979]
Epoch [118/120    avg_loss:0.038, val_acc:0.982]
Epoch [119/120    avg_loss:0.039, val_acc:0.986]
Epoch [120/120    avg_loss:0.042, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 202   0   0   0   0  17   0   0   0   0   0   0]
 [  0   0   3 205  20   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 214   9   0   0   0   0   0   0   4   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0   0 381   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   5   0   5   0 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.2089552238806

F1 scores:
[       nan 1.         0.92873563 0.94252874 0.91257996 0.94158076
 1.         0.89552239 0.98322581 0.99893276 0.99317872 1.
 0.98444444 1.        ]

Kappa:
0.9800624331878199
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdeb2a44898>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.622, val_acc:0.293]
Epoch [2/120    avg_loss:2.351, val_acc:0.377]
Epoch [3/120    avg_loss:2.173, val_acc:0.389]
Epoch [4/120    avg_loss:2.037, val_acc:0.527]
Epoch [5/120    avg_loss:1.902, val_acc:0.637]
Epoch [6/120    avg_loss:1.767, val_acc:0.703]
Epoch [7/120    avg_loss:1.622, val_acc:0.686]
Epoch [8/120    avg_loss:1.477, val_acc:0.732]
Epoch [9/120    avg_loss:1.335, val_acc:0.707]
Epoch [10/120    avg_loss:1.266, val_acc:0.727]
Epoch [11/120    avg_loss:1.161, val_acc:0.686]
Epoch [12/120    avg_loss:1.072, val_acc:0.732]
Epoch [13/120    avg_loss:0.986, val_acc:0.760]
Epoch [14/120    avg_loss:0.946, val_acc:0.793]
Epoch [15/120    avg_loss:0.904, val_acc:0.865]
Epoch [16/120    avg_loss:0.838, val_acc:0.777]
Epoch [17/120    avg_loss:0.776, val_acc:0.846]
Epoch [18/120    avg_loss:0.754, val_acc:0.871]
Epoch [19/120    avg_loss:0.740, val_acc:0.891]
Epoch [20/120    avg_loss:0.668, val_acc:0.895]
Epoch [21/120    avg_loss:0.636, val_acc:0.895]
Epoch [22/120    avg_loss:0.599, val_acc:0.912]
Epoch [23/120    avg_loss:0.541, val_acc:0.914]
Epoch [24/120    avg_loss:0.507, val_acc:0.887]
Epoch [25/120    avg_loss:0.553, val_acc:0.898]
Epoch [26/120    avg_loss:0.554, val_acc:0.914]
Epoch [27/120    avg_loss:0.526, val_acc:0.879]
Epoch [28/120    avg_loss:0.514, val_acc:0.889]
Epoch [29/120    avg_loss:0.481, val_acc:0.908]
Epoch [30/120    avg_loss:0.412, val_acc:0.928]
Epoch [31/120    avg_loss:0.372, val_acc:0.930]
Epoch [32/120    avg_loss:0.378, val_acc:0.906]
Epoch [33/120    avg_loss:0.416, val_acc:0.922]
Epoch [34/120    avg_loss:0.389, val_acc:0.924]
Epoch [35/120    avg_loss:0.343, val_acc:0.920]
Epoch [36/120    avg_loss:0.361, val_acc:0.928]
Epoch [37/120    avg_loss:0.321, val_acc:0.953]
Epoch [38/120    avg_loss:0.331, val_acc:0.916]
Epoch [39/120    avg_loss:0.337, val_acc:0.924]
Epoch [40/120    avg_loss:0.318, val_acc:0.920]
Epoch [41/120    avg_loss:0.350, val_acc:0.926]
Epoch [42/120    avg_loss:0.287, val_acc:0.936]
Epoch [43/120    avg_loss:0.260, val_acc:0.934]
Epoch [44/120    avg_loss:0.318, val_acc:0.930]
Epoch [45/120    avg_loss:0.306, val_acc:0.953]
Epoch [46/120    avg_loss:0.276, val_acc:0.938]
Epoch [47/120    avg_loss:0.260, val_acc:0.934]
Epoch [48/120    avg_loss:0.285, val_acc:0.916]
Epoch [49/120    avg_loss:0.270, val_acc:0.924]
Epoch [50/120    avg_loss:0.277, val_acc:0.943]
Epoch [51/120    avg_loss:0.231, val_acc:0.959]
Epoch [52/120    avg_loss:0.212, val_acc:0.916]
Epoch [53/120    avg_loss:0.266, val_acc:0.953]
Epoch [54/120    avg_loss:0.215, val_acc:0.961]
Epoch [55/120    avg_loss:0.179, val_acc:0.951]
Epoch [56/120    avg_loss:0.209, val_acc:0.947]
Epoch [57/120    avg_loss:0.206, val_acc:0.941]
Epoch [58/120    avg_loss:0.178, val_acc:0.955]
Epoch [59/120    avg_loss:0.207, val_acc:0.963]
Epoch [60/120    avg_loss:0.203, val_acc:0.951]
Epoch [61/120    avg_loss:0.197, val_acc:0.973]
Epoch [62/120    avg_loss:0.199, val_acc:0.953]
Epoch [63/120    avg_loss:0.169, val_acc:0.961]
Epoch [64/120    avg_loss:0.322, val_acc:0.949]
Epoch [65/120    avg_loss:0.255, val_acc:0.930]
Epoch [66/120    avg_loss:0.232, val_acc:0.959]
Epoch [67/120    avg_loss:0.173, val_acc:0.951]
Epoch [68/120    avg_loss:0.169, val_acc:0.945]
Epoch [69/120    avg_loss:0.203, val_acc:0.951]
Epoch [70/120    avg_loss:0.319, val_acc:0.904]
Epoch [71/120    avg_loss:0.219, val_acc:0.945]
Epoch [72/120    avg_loss:0.268, val_acc:0.947]
Epoch [73/120    avg_loss:0.208, val_acc:0.924]
Epoch [74/120    avg_loss:0.195, val_acc:0.951]
Epoch [75/120    avg_loss:0.170, val_acc:0.963]
Epoch [76/120    avg_loss:0.128, val_acc:0.967]
Epoch [77/120    avg_loss:0.122, val_acc:0.975]
Epoch [78/120    avg_loss:0.117, val_acc:0.977]
Epoch [79/120    avg_loss:0.119, val_acc:0.977]
Epoch [80/120    avg_loss:0.118, val_acc:0.979]
Epoch [81/120    avg_loss:0.117, val_acc:0.980]
Epoch [82/120    avg_loss:0.107, val_acc:0.979]
Epoch [83/120    avg_loss:0.103, val_acc:0.982]
Epoch [84/120    avg_loss:0.106, val_acc:0.979]
Epoch [85/120    avg_loss:0.113, val_acc:0.982]
Epoch [86/120    avg_loss:0.111, val_acc:0.984]
Epoch [87/120    avg_loss:0.101, val_acc:0.986]
Epoch [88/120    avg_loss:0.090, val_acc:0.986]
Epoch [89/120    avg_loss:0.114, val_acc:0.990]
Epoch [90/120    avg_loss:0.110, val_acc:0.982]
Epoch [91/120    avg_loss:0.093, val_acc:0.984]
Epoch [92/120    avg_loss:0.095, val_acc:0.990]
Epoch [93/120    avg_loss:0.113, val_acc:0.990]
Epoch [94/120    avg_loss:0.113, val_acc:0.984]
Epoch [95/120    avg_loss:0.087, val_acc:0.988]
Epoch [96/120    avg_loss:0.090, val_acc:0.988]
Epoch [97/120    avg_loss:0.106, val_acc:0.984]
Epoch [98/120    avg_loss:0.102, val_acc:0.984]
Epoch [99/120    avg_loss:0.100, val_acc:0.986]
Epoch [100/120    avg_loss:0.083, val_acc:0.988]
Epoch [101/120    avg_loss:0.096, val_acc:0.988]
Epoch [102/120    avg_loss:0.091, val_acc:0.988]
Epoch [103/120    avg_loss:0.088, val_acc:0.990]
Epoch [104/120    avg_loss:0.089, val_acc:0.988]
Epoch [105/120    avg_loss:0.098, val_acc:0.986]
Epoch [106/120    avg_loss:0.094, val_acc:0.984]
Epoch [107/120    avg_loss:0.092, val_acc:0.988]
Epoch [108/120    avg_loss:0.086, val_acc:0.990]
Epoch [109/120    avg_loss:0.101, val_acc:0.990]
Epoch [110/120    avg_loss:0.094, val_acc:0.990]
Epoch [111/120    avg_loss:0.083, val_acc:0.988]
Epoch [112/120    avg_loss:0.093, val_acc:0.984]
Epoch [113/120    avg_loss:0.093, val_acc:0.988]
Epoch [114/120    avg_loss:0.097, val_acc:0.990]
Epoch [115/120    avg_loss:0.076, val_acc:0.988]
Epoch [116/120    avg_loss:0.083, val_acc:0.988]
Epoch [117/120    avg_loss:0.087, val_acc:0.990]
Epoch [118/120    avg_loss:0.086, val_acc:0.990]
Epoch [119/120    avg_loss:0.083, val_acc:0.988]
Epoch [120/120    avg_loss:0.096, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 203   0   0   0   0  16   0   0   0   0   0   0]
 [  0   0  10 215   5   0   0   0   0   0   0   0   0   0]
 [  0   0   2   0 207  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  22 123   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10   0 196   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.84648187633262

F1 scores:
[       nan 1.         0.90022173 0.96629213 0.87898089 0.86013986
 0.97512438 0.82352941 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9760237473674663
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f54b88498d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.557, val_acc:0.381]
Epoch [2/120    avg_loss:2.326, val_acc:0.406]
Epoch [3/120    avg_loss:2.162, val_acc:0.455]
Epoch [4/120    avg_loss:2.008, val_acc:0.502]
Epoch [5/120    avg_loss:1.881, val_acc:0.604]
Epoch [6/120    avg_loss:1.722, val_acc:0.643]
Epoch [7/120    avg_loss:1.603, val_acc:0.715]
Epoch [8/120    avg_loss:1.456, val_acc:0.744]
Epoch [9/120    avg_loss:1.301, val_acc:0.777]
Epoch [10/120    avg_loss:1.196, val_acc:0.838]
Epoch [11/120    avg_loss:1.081, val_acc:0.855]
Epoch [12/120    avg_loss:0.973, val_acc:0.877]
Epoch [13/120    avg_loss:0.889, val_acc:0.875]
Epoch [14/120    avg_loss:0.824, val_acc:0.883]
Epoch [15/120    avg_loss:0.772, val_acc:0.865]
Epoch [16/120    avg_loss:0.736, val_acc:0.846]
Epoch [17/120    avg_loss:0.689, val_acc:0.889]
Epoch [18/120    avg_loss:0.624, val_acc:0.887]
Epoch [19/120    avg_loss:0.582, val_acc:0.885]
Epoch [20/120    avg_loss:0.562, val_acc:0.896]
Epoch [21/120    avg_loss:0.512, val_acc:0.904]
Epoch [22/120    avg_loss:0.475, val_acc:0.918]
Epoch [23/120    avg_loss:0.466, val_acc:0.896]
Epoch [24/120    avg_loss:0.462, val_acc:0.902]
Epoch [25/120    avg_loss:0.446, val_acc:0.904]
Epoch [26/120    avg_loss:0.407, val_acc:0.904]
Epoch [27/120    avg_loss:0.445, val_acc:0.898]
Epoch [28/120    avg_loss:0.463, val_acc:0.871]
Epoch [29/120    avg_loss:0.415, val_acc:0.930]
Epoch [30/120    avg_loss:0.363, val_acc:0.922]
Epoch [31/120    avg_loss:0.356, val_acc:0.883]
Epoch [32/120    avg_loss:0.387, val_acc:0.916]
Epoch [33/120    avg_loss:0.336, val_acc:0.918]
Epoch [34/120    avg_loss:0.341, val_acc:0.926]
Epoch [35/120    avg_loss:0.374, val_acc:0.930]
Epoch [36/120    avg_loss:0.338, val_acc:0.912]
Epoch [37/120    avg_loss:0.363, val_acc:0.936]
Epoch [38/120    avg_loss:0.316, val_acc:0.920]
Epoch [39/120    avg_loss:0.368, val_acc:0.910]
Epoch [40/120    avg_loss:0.345, val_acc:0.924]
Epoch [41/120    avg_loss:0.282, val_acc:0.922]
Epoch [42/120    avg_loss:0.300, val_acc:0.945]
Epoch [43/120    avg_loss:0.296, val_acc:0.938]
Epoch [44/120    avg_loss:0.237, val_acc:0.939]
Epoch [45/120    avg_loss:0.223, val_acc:0.939]
Epoch [46/120    avg_loss:0.259, val_acc:0.947]
Epoch [47/120    avg_loss:0.225, val_acc:0.916]
Epoch [48/120    avg_loss:0.220, val_acc:0.953]
Epoch [49/120    avg_loss:0.233, val_acc:0.934]
Epoch [50/120    avg_loss:0.204, val_acc:0.957]
Epoch [51/120    avg_loss:0.183, val_acc:0.939]
Epoch [52/120    avg_loss:0.202, val_acc:0.939]
Epoch [53/120    avg_loss:0.187, val_acc:0.938]
Epoch [54/120    avg_loss:0.259, val_acc:0.930]
Epoch [55/120    avg_loss:0.218, val_acc:0.947]
Epoch [56/120    avg_loss:0.217, val_acc:0.949]
Epoch [57/120    avg_loss:0.179, val_acc:0.955]
Epoch [58/120    avg_loss:0.177, val_acc:0.961]
Epoch [59/120    avg_loss:0.169, val_acc:0.955]
Epoch [60/120    avg_loss:0.147, val_acc:0.949]
Epoch [61/120    avg_loss:0.153, val_acc:0.955]
Epoch [62/120    avg_loss:0.142, val_acc:0.965]
Epoch [63/120    avg_loss:0.123, val_acc:0.963]
Epoch [64/120    avg_loss:0.130, val_acc:0.969]
Epoch [65/120    avg_loss:0.119, val_acc:0.967]
Epoch [66/120    avg_loss:0.132, val_acc:0.957]
Epoch [67/120    avg_loss:0.139, val_acc:0.971]
Epoch [68/120    avg_loss:0.118, val_acc:0.959]
Epoch [69/120    avg_loss:0.111, val_acc:0.969]
Epoch [70/120    avg_loss:0.104, val_acc:0.967]
Epoch [71/120    avg_loss:0.124, val_acc:0.967]
Epoch [72/120    avg_loss:0.114, val_acc:0.963]
Epoch [73/120    avg_loss:0.133, val_acc:0.967]
Epoch [74/120    avg_loss:0.125, val_acc:0.961]
Epoch [75/120    avg_loss:0.139, val_acc:0.947]
Epoch [76/120    avg_loss:0.113, val_acc:0.971]
Epoch [77/120    avg_loss:0.123, val_acc:0.936]
Epoch [78/120    avg_loss:0.111, val_acc:0.955]
Epoch [79/120    avg_loss:0.088, val_acc:0.973]
Epoch [80/120    avg_loss:0.085, val_acc:0.979]
Epoch [81/120    avg_loss:0.107, val_acc:0.969]
Epoch [82/120    avg_loss:0.107, val_acc:0.973]
Epoch [83/120    avg_loss:0.113, val_acc:0.973]
Epoch [84/120    avg_loss:0.090, val_acc:0.975]
Epoch [85/120    avg_loss:0.126, val_acc:0.945]
Epoch [86/120    avg_loss:0.118, val_acc:0.973]
Epoch [87/120    avg_loss:0.096, val_acc:0.955]
Epoch [88/120    avg_loss:0.113, val_acc:0.969]
Epoch [89/120    avg_loss:0.089, val_acc:0.963]
Epoch [90/120    avg_loss:0.084, val_acc:0.965]
Epoch [91/120    avg_loss:0.077, val_acc:0.971]
Epoch [92/120    avg_loss:0.074, val_acc:0.975]
Epoch [93/120    avg_loss:0.091, val_acc:0.967]
Epoch [94/120    avg_loss:0.071, val_acc:0.977]
Epoch [95/120    avg_loss:0.056, val_acc:0.979]
Epoch [96/120    avg_loss:0.052, val_acc:0.984]
Epoch [97/120    avg_loss:0.043, val_acc:0.984]
Epoch [98/120    avg_loss:0.051, val_acc:0.980]
Epoch [99/120    avg_loss:0.057, val_acc:0.979]
Epoch [100/120    avg_loss:0.034, val_acc:0.979]
Epoch [101/120    avg_loss:0.038, val_acc:0.980]
Epoch [102/120    avg_loss:0.044, val_acc:0.982]
Epoch [103/120    avg_loss:0.048, val_acc:0.980]
Epoch [104/120    avg_loss:0.046, val_acc:0.980]
Epoch [105/120    avg_loss:0.045, val_acc:0.980]
Epoch [106/120    avg_loss:0.042, val_acc:0.982]
Epoch [107/120    avg_loss:0.042, val_acc:0.979]
Epoch [108/120    avg_loss:0.040, val_acc:0.982]
Epoch [109/120    avg_loss:0.040, val_acc:0.982]
Epoch [110/120    avg_loss:0.040, val_acc:0.982]
Epoch [111/120    avg_loss:0.035, val_acc:0.982]
Epoch [112/120    avg_loss:0.041, val_acc:0.982]
Epoch [113/120    avg_loss:0.058, val_acc:0.982]
Epoch [114/120    avg_loss:0.052, val_acc:0.982]
Epoch [115/120    avg_loss:0.044, val_acc:0.982]
Epoch [116/120    avg_loss:0.034, val_acc:0.982]
Epoch [117/120    avg_loss:0.039, val_acc:0.982]
Epoch [118/120    avg_loss:0.036, val_acc:0.982]
Epoch [119/120    avg_loss:0.041, val_acc:0.982]
Epoch [120/120    avg_loss:0.038, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   1 204   0   0   0   0  14   0   0   0   0   0   0]
 [  0   0   0 209  16   0   0   0   4   1   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.40085287846482

F1 scores:
[       nan 0.99927061 0.93577982 0.95216401 0.91025641 0.9109589
 1.         0.85714286 0.99487179 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9821954906277417
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe357b5f860>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.549, val_acc:0.340]
Epoch [2/120    avg_loss:2.326, val_acc:0.334]
Epoch [3/120    avg_loss:2.178, val_acc:0.451]
Epoch [4/120    avg_loss:2.050, val_acc:0.543]
Epoch [5/120    avg_loss:1.912, val_acc:0.598]
Epoch [6/120    avg_loss:1.762, val_acc:0.648]
Epoch [7/120    avg_loss:1.627, val_acc:0.732]
Epoch [8/120    avg_loss:1.501, val_acc:0.762]
Epoch [9/120    avg_loss:1.366, val_acc:0.750]
Epoch [10/120    avg_loss:1.259, val_acc:0.750]
Epoch [11/120    avg_loss:1.141, val_acc:0.758]
Epoch [12/120    avg_loss:1.048, val_acc:0.781]
Epoch [13/120    avg_loss:0.967, val_acc:0.857]
Epoch [14/120    avg_loss:0.894, val_acc:0.838]
Epoch [15/120    avg_loss:0.821, val_acc:0.855]
Epoch [16/120    avg_loss:0.769, val_acc:0.871]
Epoch [17/120    avg_loss:0.734, val_acc:0.871]
Epoch [18/120    avg_loss:0.680, val_acc:0.875]
Epoch [19/120    avg_loss:0.648, val_acc:0.865]
Epoch [20/120    avg_loss:0.622, val_acc:0.879]
Epoch [21/120    avg_loss:0.567, val_acc:0.873]
Epoch [22/120    avg_loss:0.506, val_acc:0.896]
Epoch [23/120    avg_loss:0.524, val_acc:0.900]
Epoch [24/120    avg_loss:0.490, val_acc:0.895]
Epoch [25/120    avg_loss:0.460, val_acc:0.898]
Epoch [26/120    avg_loss:0.459, val_acc:0.891]
Epoch [27/120    avg_loss:0.463, val_acc:0.873]
Epoch [28/120    avg_loss:0.414, val_acc:0.910]
Epoch [29/120    avg_loss:0.443, val_acc:0.908]
Epoch [30/120    avg_loss:0.414, val_acc:0.918]
Epoch [31/120    avg_loss:0.396, val_acc:0.918]
Epoch [32/120    avg_loss:0.408, val_acc:0.908]
Epoch [33/120    avg_loss:0.389, val_acc:0.910]
Epoch [34/120    avg_loss:0.348, val_acc:0.926]
Epoch [35/120    avg_loss:0.335, val_acc:0.918]
Epoch [36/120    avg_loss:0.349, val_acc:0.910]
Epoch [37/120    avg_loss:0.347, val_acc:0.912]
Epoch [38/120    avg_loss:0.331, val_acc:0.924]
Epoch [39/120    avg_loss:0.296, val_acc:0.900]
Epoch [40/120    avg_loss:0.390, val_acc:0.887]
Epoch [41/120    avg_loss:0.329, val_acc:0.924]
Epoch [42/120    avg_loss:0.287, val_acc:0.941]
Epoch [43/120    avg_loss:0.282, val_acc:0.934]
Epoch [44/120    avg_loss:0.288, val_acc:0.924]
Epoch [45/120    avg_loss:0.343, val_acc:0.936]
Epoch [46/120    avg_loss:0.236, val_acc:0.945]
Epoch [47/120    avg_loss:0.279, val_acc:0.943]
Epoch [48/120    avg_loss:0.276, val_acc:0.895]
Epoch [49/120    avg_loss:0.290, val_acc:0.938]
Epoch [50/120    avg_loss:0.273, val_acc:0.930]
Epoch [51/120    avg_loss:0.225, val_acc:0.938]
Epoch [52/120    avg_loss:0.232, val_acc:0.936]
Epoch [53/120    avg_loss:0.236, val_acc:0.947]
Epoch [54/120    avg_loss:0.220, val_acc:0.936]
Epoch [55/120    avg_loss:0.215, val_acc:0.938]
Epoch [56/120    avg_loss:0.210, val_acc:0.945]
Epoch [57/120    avg_loss:0.217, val_acc:0.951]
Epoch [58/120    avg_loss:0.184, val_acc:0.947]
Epoch [59/120    avg_loss:0.190, val_acc:0.949]
Epoch [60/120    avg_loss:0.151, val_acc:0.967]
Epoch [61/120    avg_loss:0.137, val_acc:0.953]
Epoch [62/120    avg_loss:0.168, val_acc:0.957]
Epoch [63/120    avg_loss:0.179, val_acc:0.902]
Epoch [64/120    avg_loss:0.185, val_acc:0.955]
Epoch [65/120    avg_loss:0.152, val_acc:0.941]
Epoch [66/120    avg_loss:0.150, val_acc:0.932]
Epoch [67/120    avg_loss:0.163, val_acc:0.963]
Epoch [68/120    avg_loss:0.156, val_acc:0.963]
Epoch [69/120    avg_loss:0.171, val_acc:0.908]
Epoch [70/120    avg_loss:0.232, val_acc:0.928]
Epoch [71/120    avg_loss:0.206, val_acc:0.922]
Epoch [72/120    avg_loss:0.174, val_acc:0.947]
Epoch [73/120    avg_loss:0.129, val_acc:0.961]
Epoch [74/120    avg_loss:0.105, val_acc:0.961]
Epoch [75/120    avg_loss:0.090, val_acc:0.971]
Epoch [76/120    avg_loss:0.092, val_acc:0.967]
Epoch [77/120    avg_loss:0.100, val_acc:0.961]
Epoch [78/120    avg_loss:0.089, val_acc:0.967]
Epoch [79/120    avg_loss:0.088, val_acc:0.963]
Epoch [80/120    avg_loss:0.080, val_acc:0.969]
Epoch [81/120    avg_loss:0.092, val_acc:0.965]
Epoch [82/120    avg_loss:0.079, val_acc:0.969]
Epoch [83/120    avg_loss:0.092, val_acc:0.967]
Epoch [84/120    avg_loss:0.079, val_acc:0.973]
Epoch [85/120    avg_loss:0.076, val_acc:0.973]
Epoch [86/120    avg_loss:0.083, val_acc:0.967]
Epoch [87/120    avg_loss:0.094, val_acc:0.969]
Epoch [88/120    avg_loss:0.073, val_acc:0.973]
Epoch [89/120    avg_loss:0.082, val_acc:0.971]
Epoch [90/120    avg_loss:0.071, val_acc:0.969]
Epoch [91/120    avg_loss:0.079, val_acc:0.963]
Epoch [92/120    avg_loss:0.073, val_acc:0.973]
Epoch [93/120    avg_loss:0.071, val_acc:0.971]
Epoch [94/120    avg_loss:0.063, val_acc:0.971]
Epoch [95/120    avg_loss:0.070, val_acc:0.973]
Epoch [96/120    avg_loss:0.081, val_acc:0.969]
Epoch [97/120    avg_loss:0.069, val_acc:0.969]
Epoch [98/120    avg_loss:0.072, val_acc:0.969]
Epoch [99/120    avg_loss:0.065, val_acc:0.969]
Epoch [100/120    avg_loss:0.071, val_acc:0.971]
Epoch [101/120    avg_loss:0.072, val_acc:0.973]
Epoch [102/120    avg_loss:0.064, val_acc:0.971]
Epoch [103/120    avg_loss:0.060, val_acc:0.971]
Epoch [104/120    avg_loss:0.070, val_acc:0.973]
Epoch [105/120    avg_loss:0.073, val_acc:0.973]
Epoch [106/120    avg_loss:0.060, val_acc:0.971]
Epoch [107/120    avg_loss:0.066, val_acc:0.973]
Epoch [108/120    avg_loss:0.074, val_acc:0.967]
Epoch [109/120    avg_loss:0.063, val_acc:0.969]
Epoch [110/120    avg_loss:0.054, val_acc:0.967]
Epoch [111/120    avg_loss:0.058, val_acc:0.967]
Epoch [112/120    avg_loss:0.057, val_acc:0.967]
Epoch [113/120    avg_loss:0.070, val_acc:0.971]
Epoch [114/120    avg_loss:0.057, val_acc:0.975]
Epoch [115/120    avg_loss:0.057, val_acc:0.977]
Epoch [116/120    avg_loss:0.071, val_acc:0.975]
Epoch [117/120    avg_loss:0.058, val_acc:0.973]
Epoch [118/120    avg_loss:0.058, val_acc:0.975]
Epoch [119/120    avg_loss:0.059, val_acc:0.977]
Epoch [120/120    avg_loss:0.061, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 205   0   0   0   0  14   0   0   0   0   0   0]
 [  0   0   3 218   8   0   0   0   1   0   0   0   0   0]
 [  0   0   1   0 211  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  24 121   0   0   0   0   0   0   0   0]
 [  0   4   0   0   1   0 201   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.44349680170575

F1 scores:
[       nan 0.99708879 0.95794393 0.97321429 0.89596603 0.86120996
 0.98771499 0.93069307 0.998713   1.         1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.9826701275456745
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff4b65718d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.593, val_acc:0.369]
Epoch [2/120    avg_loss:2.371, val_acc:0.441]
Epoch [3/120    avg_loss:2.200, val_acc:0.521]
Epoch [4/120    avg_loss:2.052, val_acc:0.592]
Epoch [5/120    avg_loss:1.902, val_acc:0.580]
Epoch [6/120    avg_loss:1.721, val_acc:0.580]
Epoch [7/120    avg_loss:1.567, val_acc:0.674]
Epoch [8/120    avg_loss:1.425, val_acc:0.672]
Epoch [9/120    avg_loss:1.303, val_acc:0.781]
Epoch [10/120    avg_loss:1.175, val_acc:0.746]
Epoch [11/120    avg_loss:1.115, val_acc:0.701]
Epoch [12/120    avg_loss:1.049, val_acc:0.750]
Epoch [13/120    avg_loss:0.964, val_acc:0.811]
Epoch [14/120    avg_loss:0.917, val_acc:0.803]
Epoch [15/120    avg_loss:0.863, val_acc:0.855]
Epoch [16/120    avg_loss:0.818, val_acc:0.861]
Epoch [17/120    avg_loss:0.725, val_acc:0.859]
Epoch [18/120    avg_loss:0.705, val_acc:0.873]
Epoch [19/120    avg_loss:0.653, val_acc:0.910]
Epoch [20/120    avg_loss:0.619, val_acc:0.908]
Epoch [21/120    avg_loss:0.553, val_acc:0.898]
Epoch [22/120    avg_loss:0.535, val_acc:0.893]
Epoch [23/120    avg_loss:0.524, val_acc:0.906]
Epoch [24/120    avg_loss:0.521, val_acc:0.900]
Epoch [25/120    avg_loss:0.495, val_acc:0.898]
Epoch [26/120    avg_loss:0.468, val_acc:0.914]
Epoch [27/120    avg_loss:0.455, val_acc:0.928]
Epoch [28/120    avg_loss:0.410, val_acc:0.904]
Epoch [29/120    avg_loss:0.441, val_acc:0.920]
Epoch [30/120    avg_loss:0.371, val_acc:0.902]
Epoch [31/120    avg_loss:0.357, val_acc:0.910]
Epoch [32/120    avg_loss:0.421, val_acc:0.928]
Epoch [33/120    avg_loss:0.363, val_acc:0.924]
Epoch [34/120    avg_loss:0.343, val_acc:0.912]
Epoch [35/120    avg_loss:0.314, val_acc:0.916]
Epoch [36/120    avg_loss:0.355, val_acc:0.930]
Epoch [37/120    avg_loss:0.354, val_acc:0.924]
Epoch [38/120    avg_loss:0.315, val_acc:0.928]
Epoch [39/120    avg_loss:0.307, val_acc:0.938]
Epoch [40/120    avg_loss:0.321, val_acc:0.938]
Epoch [41/120    avg_loss:0.289, val_acc:0.930]
Epoch [42/120    avg_loss:0.273, val_acc:0.941]
Epoch [43/120    avg_loss:0.255, val_acc:0.924]
Epoch [44/120    avg_loss:0.289, val_acc:0.934]
Epoch [45/120    avg_loss:0.235, val_acc:0.939]
Epoch [46/120    avg_loss:0.246, val_acc:0.885]
Epoch [47/120    avg_loss:0.326, val_acc:0.949]
Epoch [48/120    avg_loss:0.255, val_acc:0.891]
Epoch [49/120    avg_loss:0.248, val_acc:0.938]
Epoch [50/120    avg_loss:0.237, val_acc:0.896]
Epoch [51/120    avg_loss:0.219, val_acc:0.947]
Epoch [52/120    avg_loss:0.238, val_acc:0.943]
Epoch [53/120    avg_loss:0.251, val_acc:0.957]
Epoch [54/120    avg_loss:0.181, val_acc:0.955]
Epoch [55/120    avg_loss:0.165, val_acc:0.951]
Epoch [56/120    avg_loss:0.232, val_acc:0.904]
Epoch [57/120    avg_loss:0.226, val_acc:0.928]
Epoch [58/120    avg_loss:0.235, val_acc:0.951]
Epoch [59/120    avg_loss:0.211, val_acc:0.943]
Epoch [60/120    avg_loss:0.185, val_acc:0.963]
Epoch [61/120    avg_loss:0.196, val_acc:0.951]
Epoch [62/120    avg_loss:0.179, val_acc:0.949]
Epoch [63/120    avg_loss:0.171, val_acc:0.947]
Epoch [64/120    avg_loss:0.120, val_acc:0.965]
Epoch [65/120    avg_loss:0.149, val_acc:0.961]
Epoch [66/120    avg_loss:0.141, val_acc:0.957]
Epoch [67/120    avg_loss:0.157, val_acc:0.961]
Epoch [68/120    avg_loss:0.159, val_acc:0.957]
Epoch [69/120    avg_loss:0.165, val_acc:0.939]
Epoch [70/120    avg_loss:0.170, val_acc:0.924]
Epoch [71/120    avg_loss:0.192, val_acc:0.963]
Epoch [72/120    avg_loss:0.241, val_acc:0.953]
Epoch [73/120    avg_loss:0.185, val_acc:0.963]
Epoch [74/120    avg_loss:0.140, val_acc:0.957]
Epoch [75/120    avg_loss:0.160, val_acc:0.947]
Epoch [76/120    avg_loss:0.116, val_acc:0.959]
Epoch [77/120    avg_loss:0.115, val_acc:0.963]
Epoch [78/120    avg_loss:0.100, val_acc:0.971]
Epoch [79/120    avg_loss:0.081, val_acc:0.969]
Epoch [80/120    avg_loss:0.076, val_acc:0.967]
Epoch [81/120    avg_loss:0.078, val_acc:0.969]
Epoch [82/120    avg_loss:0.069, val_acc:0.973]
Epoch [83/120    avg_loss:0.071, val_acc:0.973]
Epoch [84/120    avg_loss:0.066, val_acc:0.973]
Epoch [85/120    avg_loss:0.070, val_acc:0.971]
Epoch [86/120    avg_loss:0.068, val_acc:0.971]
Epoch [87/120    avg_loss:0.069, val_acc:0.971]
Epoch [88/120    avg_loss:0.082, val_acc:0.973]
Epoch [89/120    avg_loss:0.078, val_acc:0.973]
Epoch [90/120    avg_loss:0.066, val_acc:0.975]
Epoch [91/120    avg_loss:0.067, val_acc:0.975]
Epoch [92/120    avg_loss:0.053, val_acc:0.973]
Epoch [93/120    avg_loss:0.064, val_acc:0.975]
Epoch [94/120    avg_loss:0.055, val_acc:0.975]
Epoch [95/120    avg_loss:0.059, val_acc:0.975]
Epoch [96/120    avg_loss:0.059, val_acc:0.973]
Epoch [97/120    avg_loss:0.062, val_acc:0.975]
Epoch [98/120    avg_loss:0.066, val_acc:0.977]
Epoch [99/120    avg_loss:0.059, val_acc:0.977]
Epoch [100/120    avg_loss:0.056, val_acc:0.979]
Epoch [101/120    avg_loss:0.058, val_acc:0.977]
Epoch [102/120    avg_loss:0.050, val_acc:0.979]
Epoch [103/120    avg_loss:0.064, val_acc:0.975]
Epoch [104/120    avg_loss:0.045, val_acc:0.971]
Epoch [105/120    avg_loss:0.058, val_acc:0.973]
Epoch [106/120    avg_loss:0.054, val_acc:0.973]
Epoch [107/120    avg_loss:0.060, val_acc:0.973]
Epoch [108/120    avg_loss:0.060, val_acc:0.971]
Epoch [109/120    avg_loss:0.067, val_acc:0.975]
Epoch [110/120    avg_loss:0.056, val_acc:0.977]
Epoch [111/120    avg_loss:0.058, val_acc:0.975]
Epoch [112/120    avg_loss:0.052, val_acc:0.979]
Epoch [113/120    avg_loss:0.052, val_acc:0.977]
Epoch [114/120    avg_loss:0.061, val_acc:0.980]
Epoch [115/120    avg_loss:0.048, val_acc:0.977]
Epoch [116/120    avg_loss:0.049, val_acc:0.977]
Epoch [117/120    avg_loss:0.047, val_acc:0.980]
Epoch [118/120    avg_loss:0.046, val_acc:0.979]
Epoch [119/120    avg_loss:0.051, val_acc:0.979]
Epoch [120/120    avg_loss:0.046, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 197   0  11   0   0  11   0   0   0   0   0   0]
 [  0   0   5 204  10  10   0   0   0   1   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0  30 115   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.61194029850746

F1 scores:
[       nan 0.99927061 0.9078341  0.94009217 0.85360825 0.79310345
 0.99756691 0.87096774 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9734120120731169
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff22786c828>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.593, val_acc:0.350]
Epoch [2/120    avg_loss:2.372, val_acc:0.377]
Epoch [3/120    avg_loss:2.236, val_acc:0.551]
Epoch [4/120    avg_loss:2.106, val_acc:0.523]
Epoch [5/120    avg_loss:1.990, val_acc:0.588]
Epoch [6/120    avg_loss:1.834, val_acc:0.627]
Epoch [7/120    avg_loss:1.677, val_acc:0.609]
Epoch [8/120    avg_loss:1.517, val_acc:0.684]
Epoch [9/120    avg_loss:1.375, val_acc:0.668]
Epoch [10/120    avg_loss:1.281, val_acc:0.688]
Epoch [11/120    avg_loss:1.169, val_acc:0.730]
Epoch [12/120    avg_loss:1.079, val_acc:0.801]
Epoch [13/120    avg_loss:0.992, val_acc:0.785]
Epoch [14/120    avg_loss:0.893, val_acc:0.873]
Epoch [15/120    avg_loss:0.799, val_acc:0.859]
Epoch [16/120    avg_loss:0.744, val_acc:0.846]
Epoch [17/120    avg_loss:0.683, val_acc:0.893]
Epoch [18/120    avg_loss:0.623, val_acc:0.900]
Epoch [19/120    avg_loss:0.578, val_acc:0.914]
Epoch [20/120    avg_loss:0.515, val_acc:0.910]
Epoch [21/120    avg_loss:0.496, val_acc:0.895]
Epoch [22/120    avg_loss:0.568, val_acc:0.875]
Epoch [23/120    avg_loss:0.511, val_acc:0.914]
Epoch [24/120    avg_loss:0.489, val_acc:0.904]
Epoch [25/120    avg_loss:0.466, val_acc:0.877]
Epoch [26/120    avg_loss:0.437, val_acc:0.910]
Epoch [27/120    avg_loss:0.382, val_acc:0.924]
Epoch [28/120    avg_loss:0.410, val_acc:0.910]
Epoch [29/120    avg_loss:0.406, val_acc:0.889]
Epoch [30/120    avg_loss:0.418, val_acc:0.891]
Epoch [31/120    avg_loss:0.412, val_acc:0.908]
Epoch [32/120    avg_loss:0.435, val_acc:0.926]
Epoch [33/120    avg_loss:0.416, val_acc:0.852]
Epoch [34/120    avg_loss:0.392, val_acc:0.893]
Epoch [35/120    avg_loss:0.349, val_acc:0.865]
Epoch [36/120    avg_loss:0.347, val_acc:0.930]
Epoch [37/120    avg_loss:0.351, val_acc:0.895]
Epoch [38/120    avg_loss:0.434, val_acc:0.916]
Epoch [39/120    avg_loss:0.395, val_acc:0.920]
Epoch [40/120    avg_loss:0.335, val_acc:0.918]
Epoch [41/120    avg_loss:0.311, val_acc:0.943]
Epoch [42/120    avg_loss:0.275, val_acc:0.898]
Epoch [43/120    avg_loss:0.362, val_acc:0.916]
Epoch [44/120    avg_loss:0.285, val_acc:0.934]
Epoch [45/120    avg_loss:0.330, val_acc:0.924]
Epoch [46/120    avg_loss:0.233, val_acc:0.939]
Epoch [47/120    avg_loss:0.283, val_acc:0.926]
Epoch [48/120    avg_loss:0.251, val_acc:0.949]
Epoch [49/120    avg_loss:0.227, val_acc:0.938]
Epoch [50/120    avg_loss:0.252, val_acc:0.936]
Epoch [51/120    avg_loss:0.249, val_acc:0.918]
Epoch [52/120    avg_loss:0.250, val_acc:0.943]
Epoch [53/120    avg_loss:0.212, val_acc:0.932]
Epoch [54/120    avg_loss:0.236, val_acc:0.955]
Epoch [55/120    avg_loss:0.215, val_acc:0.951]
Epoch [56/120    avg_loss:0.184, val_acc:0.955]
Epoch [57/120    avg_loss:0.207, val_acc:0.959]
Epoch [58/120    avg_loss:0.182, val_acc:0.965]
Epoch [59/120    avg_loss:0.189, val_acc:0.953]
Epoch [60/120    avg_loss:0.160, val_acc:0.949]
Epoch [61/120    avg_loss:0.146, val_acc:0.969]
Epoch [62/120    avg_loss:0.171, val_acc:0.971]
Epoch [63/120    avg_loss:0.136, val_acc:0.953]
Epoch [64/120    avg_loss:0.219, val_acc:0.957]
Epoch [65/120    avg_loss:0.182, val_acc:0.965]
Epoch [66/120    avg_loss:0.199, val_acc:0.947]
Epoch [67/120    avg_loss:0.148, val_acc:0.957]
Epoch [68/120    avg_loss:0.177, val_acc:0.951]
Epoch [69/120    avg_loss:0.167, val_acc:0.951]
Epoch [70/120    avg_loss:0.160, val_acc:0.953]
Epoch [71/120    avg_loss:0.172, val_acc:0.959]
Epoch [72/120    avg_loss:0.122, val_acc:0.973]
Epoch [73/120    avg_loss:0.147, val_acc:0.963]
Epoch [74/120    avg_loss:0.113, val_acc:0.977]
Epoch [75/120    avg_loss:0.127, val_acc:0.957]
Epoch [76/120    avg_loss:0.116, val_acc:0.975]
Epoch [77/120    avg_loss:0.105, val_acc:0.969]
Epoch [78/120    avg_loss:0.088, val_acc:0.973]
Epoch [79/120    avg_loss:0.096, val_acc:0.979]
Epoch [80/120    avg_loss:0.094, val_acc:0.973]
Epoch [81/120    avg_loss:0.104, val_acc:0.965]
Epoch [82/120    avg_loss:0.130, val_acc:0.977]
Epoch [83/120    avg_loss:0.092, val_acc:0.973]
Epoch [84/120    avg_loss:0.119, val_acc:0.967]
Epoch [85/120    avg_loss:0.149, val_acc:0.971]
Epoch [86/120    avg_loss:0.160, val_acc:0.967]
Epoch [87/120    avg_loss:0.118, val_acc:0.971]
Epoch [88/120    avg_loss:0.136, val_acc:0.975]
Epoch [89/120    avg_loss:0.112, val_acc:0.953]
Epoch [90/120    avg_loss:0.204, val_acc:0.967]
Epoch [91/120    avg_loss:0.130, val_acc:0.969]
Epoch [92/120    avg_loss:0.123, val_acc:0.965]
Epoch [93/120    avg_loss:0.122, val_acc:0.971]
Epoch [94/120    avg_loss:0.086, val_acc:0.979]
Epoch [95/120    avg_loss:0.071, val_acc:0.979]
Epoch [96/120    avg_loss:0.060, val_acc:0.977]
Epoch [97/120    avg_loss:0.064, val_acc:0.973]
Epoch [98/120    avg_loss:0.062, val_acc:0.977]
Epoch [99/120    avg_loss:0.070, val_acc:0.975]
Epoch [100/120    avg_loss:0.059, val_acc:0.979]
Epoch [101/120    avg_loss:0.056, val_acc:0.980]
Epoch [102/120    avg_loss:0.072, val_acc:0.975]
Epoch [103/120    avg_loss:0.056, val_acc:0.979]
Epoch [104/120    avg_loss:0.052, val_acc:0.982]
Epoch [105/120    avg_loss:0.055, val_acc:0.982]
Epoch [106/120    avg_loss:0.058, val_acc:0.979]
Epoch [107/120    avg_loss:0.052, val_acc:0.980]
Epoch [108/120    avg_loss:0.052, val_acc:0.980]
Epoch [109/120    avg_loss:0.066, val_acc:0.979]
Epoch [110/120    avg_loss:0.059, val_acc:0.979]
Epoch [111/120    avg_loss:0.049, val_acc:0.979]
Epoch [112/120    avg_loss:0.060, val_acc:0.979]
Epoch [113/120    avg_loss:0.047, val_acc:0.979]
Epoch [114/120    avg_loss:0.052, val_acc:0.979]
Epoch [115/120    avg_loss:0.045, val_acc:0.980]
Epoch [116/120    avg_loss:0.050, val_acc:0.980]
Epoch [117/120    avg_loss:0.059, val_acc:0.979]
Epoch [118/120    avg_loss:0.048, val_acc:0.980]
Epoch [119/120    avg_loss:0.055, val_acc:0.980]
Epoch [120/120    avg_loss:0.044, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 214   0   0   0   0   5   0   0   0   0   0   0]
 [  0   0   0 213   8   0   0   0   8   1   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0  24 121   0   0   0   0   0   0   0   0]
 [  0   9   0   0   0   0 197   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.14498933901919

F1 scores:
[       nan 0.99347353 0.96179775 0.96162528 0.88841202 0.84615385
 0.97766749 0.90607735 0.98979592 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.979340502124582
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f19db9ad8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.583, val_acc:0.387]
Epoch [2/120    avg_loss:2.338, val_acc:0.471]
Epoch [3/120    avg_loss:2.187, val_acc:0.453]
Epoch [4/120    avg_loss:2.057, val_acc:0.480]
Epoch [5/120    avg_loss:1.937, val_acc:0.553]
Epoch [6/120    avg_loss:1.827, val_acc:0.598]
Epoch [7/120    avg_loss:1.690, val_acc:0.652]
Epoch [8/120    avg_loss:1.541, val_acc:0.680]
Epoch [9/120    avg_loss:1.408, val_acc:0.709]
Epoch [10/120    avg_loss:1.288, val_acc:0.814]
Epoch [11/120    avg_loss:1.162, val_acc:0.732]
Epoch [12/120    avg_loss:1.044, val_acc:0.777]
Epoch [13/120    avg_loss:0.945, val_acc:0.867]
Epoch [14/120    avg_loss:0.932, val_acc:0.877]
Epoch [15/120    avg_loss:0.828, val_acc:0.883]
Epoch [16/120    avg_loss:0.783, val_acc:0.889]
Epoch [17/120    avg_loss:0.699, val_acc:0.881]
Epoch [18/120    avg_loss:0.681, val_acc:0.883]
Epoch [19/120    avg_loss:0.667, val_acc:0.891]
Epoch [20/120    avg_loss:0.577, val_acc:0.898]
Epoch [21/120    avg_loss:0.570, val_acc:0.877]
Epoch [22/120    avg_loss:0.503, val_acc:0.857]
Epoch [23/120    avg_loss:0.520, val_acc:0.867]
Epoch [24/120    avg_loss:0.474, val_acc:0.920]
Epoch [25/120    avg_loss:0.503, val_acc:0.891]
Epoch [26/120    avg_loss:0.504, val_acc:0.809]
Epoch [27/120    avg_loss:0.441, val_acc:0.914]
Epoch [28/120    avg_loss:0.466, val_acc:0.906]
Epoch [29/120    avg_loss:0.425, val_acc:0.908]
Epoch [30/120    avg_loss:0.418, val_acc:0.916]
Epoch [31/120    avg_loss:0.536, val_acc:0.859]
Epoch [32/120    avg_loss:0.471, val_acc:0.867]
Epoch [33/120    avg_loss:0.397, val_acc:0.924]
Epoch [34/120    avg_loss:0.426, val_acc:0.924]
Epoch [35/120    avg_loss:0.390, val_acc:0.920]
Epoch [36/120    avg_loss:0.343, val_acc:0.934]
Epoch [37/120    avg_loss:0.391, val_acc:0.932]
Epoch [38/120    avg_loss:0.283, val_acc:0.945]
Epoch [39/120    avg_loss:0.319, val_acc:0.922]
Epoch [40/120    avg_loss:0.302, val_acc:0.928]
Epoch [41/120    avg_loss:0.308, val_acc:0.930]
Epoch [42/120    avg_loss:0.271, val_acc:0.924]
Epoch [43/120    avg_loss:0.324, val_acc:0.928]
Epoch [44/120    avg_loss:0.254, val_acc:0.922]
Epoch [45/120    avg_loss:0.312, val_acc:0.918]
Epoch [46/120    avg_loss:0.249, val_acc:0.941]
Epoch [47/120    avg_loss:0.261, val_acc:0.941]
Epoch [48/120    avg_loss:0.256, val_acc:0.953]
Epoch [49/120    avg_loss:0.260, val_acc:0.902]
Epoch [50/120    avg_loss:0.248, val_acc:0.938]
Epoch [51/120    avg_loss:0.308, val_acc:0.943]
Epoch [52/120    avg_loss:0.232, val_acc:0.943]
Epoch [53/120    avg_loss:0.242, val_acc:0.947]
Epoch [54/120    avg_loss:0.288, val_acc:0.906]
Epoch [55/120    avg_loss:0.285, val_acc:0.945]
Epoch [56/120    avg_loss:0.242, val_acc:0.930]
Epoch [57/120    avg_loss:0.250, val_acc:0.949]
Epoch [58/120    avg_loss:0.203, val_acc:0.949]
Epoch [59/120    avg_loss:0.203, val_acc:0.949]
Epoch [60/120    avg_loss:0.265, val_acc:0.928]
Epoch [61/120    avg_loss:0.232, val_acc:0.953]
Epoch [62/120    avg_loss:0.224, val_acc:0.943]
Epoch [63/120    avg_loss:0.193, val_acc:0.934]
Epoch [64/120    avg_loss:0.229, val_acc:0.938]
Epoch [65/120    avg_loss:0.202, val_acc:0.957]
Epoch [66/120    avg_loss:0.160, val_acc:0.951]
Epoch [67/120    avg_loss:0.194, val_acc:0.947]
Epoch [68/120    avg_loss:0.187, val_acc:0.936]
Epoch [69/120    avg_loss:0.179, val_acc:0.928]
Epoch [70/120    avg_loss:0.189, val_acc:0.945]
Epoch [71/120    avg_loss:0.209, val_acc:0.936]
Epoch [72/120    avg_loss:0.184, val_acc:0.951]
Epoch [73/120    avg_loss:0.200, val_acc:0.949]
Epoch [74/120    avg_loss:0.176, val_acc:0.955]
Epoch [75/120    avg_loss:0.135, val_acc:0.961]
Epoch [76/120    avg_loss:0.138, val_acc:0.955]
Epoch [77/120    avg_loss:0.156, val_acc:0.961]
Epoch [78/120    avg_loss:0.131, val_acc:0.930]
Epoch [79/120    avg_loss:0.118, val_acc:0.943]
Epoch [80/120    avg_loss:0.142, val_acc:0.957]
Epoch [81/120    avg_loss:0.174, val_acc:0.955]
Epoch [82/120    avg_loss:0.113, val_acc:0.969]
Epoch [83/120    avg_loss:0.152, val_acc:0.951]
Epoch [84/120    avg_loss:0.176, val_acc:0.965]
Epoch [85/120    avg_loss:0.118, val_acc:0.973]
Epoch [86/120    avg_loss:0.088, val_acc:0.973]
Epoch [87/120    avg_loss:0.065, val_acc:0.980]
Epoch [88/120    avg_loss:0.109, val_acc:0.936]
Epoch [89/120    avg_loss:0.119, val_acc:0.971]
Epoch [90/120    avg_loss:0.110, val_acc:0.967]
Epoch [91/120    avg_loss:0.114, val_acc:0.961]
Epoch [92/120    avg_loss:0.084, val_acc:0.896]
Epoch [93/120    avg_loss:0.120, val_acc:0.965]
Epoch [94/120    avg_loss:0.125, val_acc:0.951]
Epoch [95/120    avg_loss:0.129, val_acc:0.963]
Epoch [96/120    avg_loss:0.109, val_acc:0.959]
Epoch [97/120    avg_loss:0.080, val_acc:0.969]
Epoch [98/120    avg_loss:0.075, val_acc:0.969]
Epoch [99/120    avg_loss:0.079, val_acc:0.971]
Epoch [100/120    avg_loss:0.064, val_acc:0.965]
Epoch [101/120    avg_loss:0.056, val_acc:0.967]
Epoch [102/120    avg_loss:0.042, val_acc:0.973]
Epoch [103/120    avg_loss:0.045, val_acc:0.979]
Epoch [104/120    avg_loss:0.045, val_acc:0.973]
Epoch [105/120    avg_loss:0.043, val_acc:0.979]
Epoch [106/120    avg_loss:0.043, val_acc:0.979]
Epoch [107/120    avg_loss:0.052, val_acc:0.977]
Epoch [108/120    avg_loss:0.045, val_acc:0.973]
Epoch [109/120    avg_loss:0.044, val_acc:0.977]
Epoch [110/120    avg_loss:0.040, val_acc:0.973]
Epoch [111/120    avg_loss:0.039, val_acc:0.977]
Epoch [112/120    avg_loss:0.041, val_acc:0.977]
Epoch [113/120    avg_loss:0.033, val_acc:0.975]
Epoch [114/120    avg_loss:0.038, val_acc:0.975]
Epoch [115/120    avg_loss:0.039, val_acc:0.975]
Epoch [116/120    avg_loss:0.041, val_acc:0.975]
Epoch [117/120    avg_loss:0.038, val_acc:0.975]
Epoch [118/120    avg_loss:0.033, val_acc:0.975]
Epoch [119/120    avg_loss:0.041, val_acc:0.975]
Epoch [120/120    avg_loss:0.038, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   2   0   0   2   0   0   0   0   2   0]
 [  0   0   0 215  14   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 219   8   0   0   0   0   0   0   0   0]
 [  0   0   0   1  29 115   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   3   0   0   0 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.46481876332622

F1 scores:
[       nan 1.         0.96818182 0.96412556 0.89205703 0.85820896
 1.         0.94505495 0.99614891 0.99893276 1.         0.99734043
 0.99228225 1.        ]

Kappa:
0.9829057717622869
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:09:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f33993d18d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.591, val_acc:0.135]
Epoch [2/120    avg_loss:2.368, val_acc:0.400]
Epoch [3/120    avg_loss:2.201, val_acc:0.465]
Epoch [4/120    avg_loss:2.040, val_acc:0.488]
Epoch [5/120    avg_loss:1.908, val_acc:0.545]
Epoch [6/120    avg_loss:1.793, val_acc:0.592]
Epoch [7/120    avg_loss:1.654, val_acc:0.613]
Epoch [8/120    avg_loss:1.529, val_acc:0.645]
Epoch [9/120    avg_loss:1.401, val_acc:0.670]
Epoch [10/120    avg_loss:1.314, val_acc:0.688]
Epoch [11/120    avg_loss:1.199, val_acc:0.707]
Epoch [12/120    avg_loss:1.114, val_acc:0.688]
Epoch [13/120    avg_loss:1.048, val_acc:0.734]
Epoch [14/120    avg_loss:0.978, val_acc:0.863]
Epoch [15/120    avg_loss:0.937, val_acc:0.781]
Epoch [16/120    avg_loss:0.873, val_acc:0.797]
Epoch [17/120    avg_loss:0.803, val_acc:0.840]
Epoch [18/120    avg_loss:0.748, val_acc:0.887]
Epoch [19/120    avg_loss:0.675, val_acc:0.891]
Epoch [20/120    avg_loss:0.672, val_acc:0.881]
Epoch [21/120    avg_loss:0.621, val_acc:0.889]
Epoch [22/120    avg_loss:0.595, val_acc:0.885]
Epoch [23/120    avg_loss:0.589, val_acc:0.896]
Epoch [24/120    avg_loss:0.534, val_acc:0.914]
Epoch [25/120    avg_loss:0.525, val_acc:0.904]
Epoch [26/120    avg_loss:0.493, val_acc:0.896]
Epoch [27/120    avg_loss:0.519, val_acc:0.893]
Epoch [28/120    avg_loss:0.528, val_acc:0.924]
Epoch [29/120    avg_loss:0.468, val_acc:0.912]
Epoch [30/120    avg_loss:0.419, val_acc:0.912]
Epoch [31/120    avg_loss:0.399, val_acc:0.898]
Epoch [32/120    avg_loss:0.408, val_acc:0.914]
Epoch [33/120    avg_loss:0.346, val_acc:0.922]
Epoch [34/120    avg_loss:0.377, val_acc:0.914]
Epoch [35/120    avg_loss:0.358, val_acc:0.930]
Epoch [36/120    avg_loss:0.364, val_acc:0.889]
Epoch [37/120    avg_loss:0.423, val_acc:0.924]
Epoch [38/120    avg_loss:0.389, val_acc:0.912]
Epoch [39/120    avg_loss:0.367, val_acc:0.943]
Epoch [40/120    avg_loss:0.312, val_acc:0.938]
Epoch [41/120    avg_loss:0.312, val_acc:0.898]
Epoch [42/120    avg_loss:0.363, val_acc:0.910]
Epoch [43/120    avg_loss:0.334, val_acc:0.916]
Epoch [44/120    avg_loss:0.330, val_acc:0.916]
Epoch [45/120    avg_loss:0.342, val_acc:0.936]
Epoch [46/120    avg_loss:0.400, val_acc:0.896]
Epoch [47/120    avg_loss:0.389, val_acc:0.891]
Epoch [48/120    avg_loss:0.334, val_acc:0.893]
Epoch [49/120    avg_loss:0.363, val_acc:0.916]
Epoch [50/120    avg_loss:0.292, val_acc:0.938]
Epoch [51/120    avg_loss:0.240, val_acc:0.912]
Epoch [52/120    avg_loss:0.246, val_acc:0.932]
Epoch [53/120    avg_loss:0.249, val_acc:0.949]
Epoch [54/120    avg_loss:0.220, val_acc:0.945]
Epoch [55/120    avg_loss:0.181, val_acc:0.949]
Epoch [56/120    avg_loss:0.207, val_acc:0.949]
Epoch [57/120    avg_loss:0.206, val_acc:0.949]
Epoch [58/120    avg_loss:0.184, val_acc:0.949]
Epoch [59/120    avg_loss:0.186, val_acc:0.955]
Epoch [60/120    avg_loss:0.201, val_acc:0.953]
Epoch [61/120    avg_loss:0.232, val_acc:0.951]
Epoch [62/120    avg_loss:0.201, val_acc:0.951]
Epoch [63/120    avg_loss:0.185, val_acc:0.955]
Epoch [64/120    avg_loss:0.202, val_acc:0.951]
Epoch [65/120    avg_loss:0.185, val_acc:0.953]
Epoch [66/120    avg_loss:0.174, val_acc:0.951]
Epoch [67/120    avg_loss:0.184, val_acc:0.955]
Epoch [68/120    avg_loss:0.159, val_acc:0.955]
Epoch [69/120    avg_loss:0.178, val_acc:0.961]
Epoch [70/120    avg_loss:0.199, val_acc:0.953]
Epoch [71/120    avg_loss:0.174, val_acc:0.957]
Epoch [72/120    avg_loss:0.166, val_acc:0.957]
Epoch [73/120    avg_loss:0.197, val_acc:0.953]
Epoch [74/120    avg_loss:0.173, val_acc:0.961]
Epoch [75/120    avg_loss:0.169, val_acc:0.961]
Epoch [76/120    avg_loss:0.158, val_acc:0.961]
Epoch [77/120    avg_loss:0.168, val_acc:0.961]
Epoch [78/120    avg_loss:0.162, val_acc:0.963]
Epoch [79/120    avg_loss:0.155, val_acc:0.957]
Epoch [80/120    avg_loss:0.161, val_acc:0.961]
Epoch [81/120    avg_loss:0.156, val_acc:0.955]
Epoch [82/120    avg_loss:0.140, val_acc:0.957]
Epoch [83/120    avg_loss:0.162, val_acc:0.955]
Epoch [84/120    avg_loss:0.149, val_acc:0.963]
Epoch [85/120    avg_loss:0.151, val_acc:0.953]
Epoch [86/120    avg_loss:0.154, val_acc:0.957]
Epoch [87/120    avg_loss:0.153, val_acc:0.959]
Epoch [88/120    avg_loss:0.146, val_acc:0.959]
Epoch [89/120    avg_loss:0.146, val_acc:0.957]
Epoch [90/120    avg_loss:0.143, val_acc:0.961]
Epoch [91/120    avg_loss:0.142, val_acc:0.961]
Epoch [92/120    avg_loss:0.146, val_acc:0.959]
Epoch [93/120    avg_loss:0.135, val_acc:0.961]
Epoch [94/120    avg_loss:0.142, val_acc:0.959]
Epoch [95/120    avg_loss:0.152, val_acc:0.959]
Epoch [96/120    avg_loss:0.142, val_acc:0.959]
Epoch [97/120    avg_loss:0.155, val_acc:0.959]
Epoch [98/120    avg_loss:0.131, val_acc:0.959]
Epoch [99/120    avg_loss:0.162, val_acc:0.959]
Epoch [100/120    avg_loss:0.143, val_acc:0.959]
Epoch [101/120    avg_loss:0.140, val_acc:0.959]
Epoch [102/120    avg_loss:0.127, val_acc:0.959]
Epoch [103/120    avg_loss:0.127, val_acc:0.961]
Epoch [104/120    avg_loss:0.131, val_acc:0.963]
Epoch [105/120    avg_loss:0.139, val_acc:0.961]
Epoch [106/120    avg_loss:0.124, val_acc:0.961]
Epoch [107/120    avg_loss:0.136, val_acc:0.961]
Epoch [108/120    avg_loss:0.126, val_acc:0.961]
Epoch [109/120    avg_loss:0.129, val_acc:0.959]
Epoch [110/120    avg_loss:0.135, val_acc:0.961]
Epoch [111/120    avg_loss:0.131, val_acc:0.961]
Epoch [112/120    avg_loss:0.143, val_acc:0.961]
Epoch [113/120    avg_loss:0.133, val_acc:0.965]
Epoch [114/120    avg_loss:0.128, val_acc:0.961]
Epoch [115/120    avg_loss:0.122, val_acc:0.961]
Epoch [116/120    avg_loss:0.124, val_acc:0.961]
Epoch [117/120    avg_loss:0.127, val_acc:0.961]
Epoch [118/120    avg_loss:0.122, val_acc:0.963]
Epoch [119/120    avg_loss:0.125, val_acc:0.959]
Epoch [120/120    avg_loss:0.123, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 202   0   0   0   0  17   0   0   0   0   0   0]
 [  0   0   0 220   8   0   0   0   1   1   0   0   0   0]
 [  0   0   0   0 199  28   0   0   0   0   0   0   0   0]
 [  0   0   0   0  29 116   0   0   0   0   0   0   0   0]
 [  0   3   0   0   7   5 191   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.50533049040511

F1 scores:
[       nan 0.997815   0.92448513 0.97777778 0.84680851 0.78911565
 0.96221662 0.82539683 0.998713   0.99893276 1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.972223895457815
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:16
Validation dataloader:16
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbd2211d8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.533, val_acc:0.316]
Epoch [2/120    avg_loss:2.312, val_acc:0.383]
Epoch [3/120    avg_loss:2.188, val_acc:0.439]
Epoch [4/120    avg_loss:2.073, val_acc:0.518]
Epoch [5/120    avg_loss:1.953, val_acc:0.533]
Epoch [6/120    avg_loss:1.844, val_acc:0.559]
Epoch [7/120    avg_loss:1.725, val_acc:0.568]
Epoch [8/120    avg_loss:1.602, val_acc:0.613]
Epoch [9/120    avg_loss:1.468, val_acc:0.611]
Epoch [10/120    avg_loss:1.334, val_acc:0.742]
Epoch [11/120    avg_loss:1.229, val_acc:0.768]
Epoch [12/120    avg_loss:1.137, val_acc:0.801]
Epoch [13/120    avg_loss:1.053, val_acc:0.785]
Epoch [14/120    avg_loss:0.957, val_acc:0.852]
Epoch [15/120    avg_loss:0.898, val_acc:0.869]
Epoch [16/120    avg_loss:0.842, val_acc:0.881]
Epoch [17/120    avg_loss:0.769, val_acc:0.875]
Epoch [18/120    avg_loss:0.731, val_acc:0.885]
Epoch [19/120    avg_loss:0.723, val_acc:0.871]
Epoch [20/120    avg_loss:0.641, val_acc:0.906]
Epoch [21/120    avg_loss:0.612, val_acc:0.873]
Epoch [22/120    avg_loss:0.555, val_acc:0.916]
Epoch [23/120    avg_loss:0.571, val_acc:0.885]
Epoch [24/120    avg_loss:0.562, val_acc:0.893]
Epoch [25/120    avg_loss:0.530, val_acc:0.930]
Epoch [26/120    avg_loss:0.471, val_acc:0.936]
Epoch [27/120    avg_loss:0.432, val_acc:0.920]
Epoch [28/120    avg_loss:0.437, val_acc:0.883]
Epoch [29/120    avg_loss:0.412, val_acc:0.895]
Epoch [30/120    avg_loss:0.439, val_acc:0.930]
Epoch [31/120    avg_loss:0.374, val_acc:0.920]
Epoch [32/120    avg_loss:0.372, val_acc:0.951]
Epoch [33/120    avg_loss:0.340, val_acc:0.922]
Epoch [34/120    avg_loss:0.399, val_acc:0.877]
Epoch [35/120    avg_loss:0.423, val_acc:0.855]
Epoch [36/120    avg_loss:0.345, val_acc:0.932]
Epoch [37/120    avg_loss:0.335, val_acc:0.924]
Epoch [38/120    avg_loss:0.378, val_acc:0.891]
Epoch [39/120    avg_loss:0.355, val_acc:0.922]
Epoch [40/120    avg_loss:0.340, val_acc:0.949]
Epoch [41/120    avg_loss:0.284, val_acc:0.936]
Epoch [42/120    avg_loss:0.255, val_acc:0.943]
Epoch [43/120    avg_loss:0.261, val_acc:0.932]
Epoch [44/120    avg_loss:0.332, val_acc:0.912]
Epoch [45/120    avg_loss:0.286, val_acc:0.928]
Epoch [46/120    avg_loss:0.222, val_acc:0.947]
Epoch [47/120    avg_loss:0.210, val_acc:0.949]
Epoch [48/120    avg_loss:0.200, val_acc:0.957]
Epoch [49/120    avg_loss:0.205, val_acc:0.961]
Epoch [50/120    avg_loss:0.194, val_acc:0.961]
Epoch [51/120    avg_loss:0.190, val_acc:0.949]
Epoch [52/120    avg_loss:0.210, val_acc:0.961]
Epoch [53/120    avg_loss:0.183, val_acc:0.955]
Epoch [54/120    avg_loss:0.183, val_acc:0.957]
Epoch [55/120    avg_loss:0.192, val_acc:0.961]
Epoch [56/120    avg_loss:0.211, val_acc:0.961]
Epoch [57/120    avg_loss:0.191, val_acc:0.965]
Epoch [58/120    avg_loss:0.177, val_acc:0.961]
Epoch [59/120    avg_loss:0.167, val_acc:0.961]
Epoch [60/120    avg_loss:0.181, val_acc:0.965]
Epoch [61/120    avg_loss:0.186, val_acc:0.963]
Epoch [62/120    avg_loss:0.167, val_acc:0.961]
Epoch [63/120    avg_loss:0.164, val_acc:0.963]
Epoch [64/120    avg_loss:0.181, val_acc:0.967]
Epoch [65/120    avg_loss:0.159, val_acc:0.969]
Epoch [66/120    avg_loss:0.159, val_acc:0.967]
Epoch [67/120    avg_loss:0.163, val_acc:0.969]
Epoch [68/120    avg_loss:0.154, val_acc:0.969]
Epoch [69/120    avg_loss:0.168, val_acc:0.969]
Epoch [70/120    avg_loss:0.161, val_acc:0.967]
Epoch [71/120    avg_loss:0.177, val_acc:0.967]
Epoch [72/120    avg_loss:0.154, val_acc:0.967]
Epoch [73/120    avg_loss:0.169, val_acc:0.967]
Epoch [74/120    avg_loss:0.159, val_acc:0.967]
Epoch [75/120    avg_loss:0.163, val_acc:0.967]
Epoch [76/120    avg_loss:0.170, val_acc:0.957]
Epoch [77/120    avg_loss:0.147, val_acc:0.965]
Epoch [78/120    avg_loss:0.150, val_acc:0.971]
Epoch [79/120    avg_loss:0.167, val_acc:0.965]
Epoch [80/120    avg_loss:0.149, val_acc:0.969]
Epoch [81/120    avg_loss:0.151, val_acc:0.971]
Epoch [82/120    avg_loss:0.163, val_acc:0.969]
Epoch [83/120    avg_loss:0.147, val_acc:0.971]
Epoch [84/120    avg_loss:0.152, val_acc:0.971]
Epoch [85/120    avg_loss:0.164, val_acc:0.965]
Epoch [86/120    avg_loss:0.160, val_acc:0.969]
Epoch [87/120    avg_loss:0.156, val_acc:0.967]
Epoch [88/120    avg_loss:0.161, val_acc:0.967]
Epoch [89/120    avg_loss:0.136, val_acc:0.973]
Epoch [90/120    avg_loss:0.134, val_acc:0.967]
Epoch [91/120    avg_loss:0.148, val_acc:0.969]
Epoch [92/120    avg_loss:0.155, val_acc:0.967]
Epoch [93/120    avg_loss:0.125, val_acc:0.969]
Epoch [94/120    avg_loss:0.127, val_acc:0.973]
Epoch [95/120    avg_loss:0.134, val_acc:0.965]
Epoch [96/120    avg_loss:0.137, val_acc:0.971]
Epoch [97/120    avg_loss:0.129, val_acc:0.967]
Epoch [98/120    avg_loss:0.123, val_acc:0.975]
Epoch [99/120    avg_loss:0.129, val_acc:0.973]
Epoch [100/120    avg_loss:0.135, val_acc:0.965]
Epoch [101/120    avg_loss:0.140, val_acc:0.973]
Epoch [102/120    avg_loss:0.135, val_acc:0.975]
Epoch [103/120    avg_loss:0.131, val_acc:0.969]
Epoch [104/120    avg_loss:0.158, val_acc:0.969]
Epoch [105/120    avg_loss:0.150, val_acc:0.971]
Epoch [106/120    avg_loss:0.150, val_acc:0.969]
Epoch [107/120    avg_loss:0.119, val_acc:0.967]
Epoch [108/120    avg_loss:0.118, val_acc:0.973]
Epoch [109/120    avg_loss:0.121, val_acc:0.973]
Epoch [110/120    avg_loss:0.137, val_acc:0.973]
Epoch [111/120    avg_loss:0.127, val_acc:0.973]
Epoch [112/120    avg_loss:0.111, val_acc:0.973]
Epoch [113/120    avg_loss:0.133, val_acc:0.973]
Epoch [114/120    avg_loss:0.132, val_acc:0.973]
Epoch [115/120    avg_loss:0.125, val_acc:0.975]
Epoch [116/120    avg_loss:0.127, val_acc:0.969]
Epoch [117/120    avg_loss:0.126, val_acc:0.967]
Epoch [118/120    avg_loss:0.132, val_acc:0.977]
Epoch [119/120    avg_loss:0.108, val_acc:0.971]
Epoch [120/120    avg_loss:0.108, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 199   0   0   0   0  20   0   0   0   0   0   0]
 [  0   0   1 221   4   0   0   0   4   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  33 112   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 371   6   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.88912579957356

F1 scores:
[       nan 1.         0.91494253 0.98004435 0.8907563  0.82352941
 1.         0.8125     0.99487179 1.         1.         0.99197861
 0.99342105 1.        ]

Kappa:
0.9764968080032036
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7705b49898>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.530, val_acc:0.365]
Epoch [2/120    avg_loss:2.336, val_acc:0.456]
Epoch [3/120    avg_loss:2.191, val_acc:0.594]
Epoch [4/120    avg_loss:2.052, val_acc:0.556]
Epoch [5/120    avg_loss:1.921, val_acc:0.581]
Epoch [6/120    avg_loss:1.792, val_acc:0.648]
Epoch [7/120    avg_loss:1.681, val_acc:0.633]
Epoch [8/120    avg_loss:1.564, val_acc:0.708]
Epoch [9/120    avg_loss:1.447, val_acc:0.740]
Epoch [10/120    avg_loss:1.319, val_acc:0.754]
Epoch [11/120    avg_loss:1.257, val_acc:0.713]
Epoch [12/120    avg_loss:1.152, val_acc:0.794]
Epoch [13/120    avg_loss:1.069, val_acc:0.827]
Epoch [14/120    avg_loss:0.997, val_acc:0.863]
Epoch [15/120    avg_loss:0.931, val_acc:0.846]
Epoch [16/120    avg_loss:0.887, val_acc:0.856]
Epoch [17/120    avg_loss:0.824, val_acc:0.858]
Epoch [18/120    avg_loss:0.770, val_acc:0.883]
Epoch [19/120    avg_loss:0.703, val_acc:0.904]
Epoch [20/120    avg_loss:0.653, val_acc:0.904]
Epoch [21/120    avg_loss:0.612, val_acc:0.904]
Epoch [22/120    avg_loss:0.589, val_acc:0.902]
Epoch [23/120    avg_loss:0.571, val_acc:0.908]
Epoch [24/120    avg_loss:0.586, val_acc:0.883]
Epoch [25/120    avg_loss:0.607, val_acc:0.873]
Epoch [26/120    avg_loss:0.572, val_acc:0.912]
Epoch [27/120    avg_loss:0.485, val_acc:0.894]
Epoch [28/120    avg_loss:0.438, val_acc:0.908]
Epoch [29/120    avg_loss:0.482, val_acc:0.919]
Epoch [30/120    avg_loss:0.456, val_acc:0.896]
Epoch [31/120    avg_loss:0.493, val_acc:0.894]
Epoch [32/120    avg_loss:0.429, val_acc:0.931]
Epoch [33/120    avg_loss:0.375, val_acc:0.912]
Epoch [34/120    avg_loss:0.388, val_acc:0.929]
Epoch [35/120    avg_loss:0.361, val_acc:0.935]
Epoch [36/120    avg_loss:0.351, val_acc:0.912]
Epoch [37/120    avg_loss:0.380, val_acc:0.919]
Epoch [38/120    avg_loss:0.343, val_acc:0.950]
Epoch [39/120    avg_loss:0.309, val_acc:0.950]
Epoch [40/120    avg_loss:0.289, val_acc:0.933]
Epoch [41/120    avg_loss:0.310, val_acc:0.942]
Epoch [42/120    avg_loss:0.279, val_acc:0.927]
Epoch [43/120    avg_loss:0.370, val_acc:0.923]
Epoch [44/120    avg_loss:0.363, val_acc:0.952]
Epoch [45/120    avg_loss:0.299, val_acc:0.946]
Epoch [46/120    avg_loss:0.318, val_acc:0.921]
Epoch [47/120    avg_loss:0.316, val_acc:0.954]
Epoch [48/120    avg_loss:0.255, val_acc:0.952]
Epoch [49/120    avg_loss:0.239, val_acc:0.960]
Epoch [50/120    avg_loss:0.278, val_acc:0.950]
Epoch [51/120    avg_loss:0.230, val_acc:0.954]
Epoch [52/120    avg_loss:0.247, val_acc:0.944]
Epoch [53/120    avg_loss:0.175, val_acc:0.954]
Epoch [54/120    avg_loss:0.194, val_acc:0.965]
Epoch [55/120    avg_loss:0.216, val_acc:0.965]
Epoch [56/120    avg_loss:0.245, val_acc:0.935]
Epoch [57/120    avg_loss:0.199, val_acc:0.960]
Epoch [58/120    avg_loss:0.198, val_acc:0.969]
Epoch [59/120    avg_loss:0.224, val_acc:0.950]
Epoch [60/120    avg_loss:0.202, val_acc:0.956]
Epoch [61/120    avg_loss:0.154, val_acc:0.952]
Epoch [62/120    avg_loss:0.149, val_acc:0.963]
Epoch [63/120    avg_loss:0.231, val_acc:0.921]
Epoch [64/120    avg_loss:0.189, val_acc:0.948]
Epoch [65/120    avg_loss:0.200, val_acc:0.975]
Epoch [66/120    avg_loss:0.172, val_acc:0.969]
Epoch [67/120    avg_loss:0.171, val_acc:0.944]
Epoch [68/120    avg_loss:0.156, val_acc:0.965]
Epoch [69/120    avg_loss:0.165, val_acc:0.963]
Epoch [70/120    avg_loss:0.179, val_acc:0.956]
Epoch [71/120    avg_loss:0.201, val_acc:0.950]
Epoch [72/120    avg_loss:0.297, val_acc:0.919]
Epoch [73/120    avg_loss:0.231, val_acc:0.954]
Epoch [74/120    avg_loss:0.237, val_acc:0.948]
Epoch [75/120    avg_loss:0.150, val_acc:0.923]
Epoch [76/120    avg_loss:0.222, val_acc:0.915]
Epoch [77/120    avg_loss:0.168, val_acc:0.960]
Epoch [78/120    avg_loss:0.171, val_acc:0.971]
Epoch [79/120    avg_loss:0.119, val_acc:0.977]
Epoch [80/120    avg_loss:0.112, val_acc:0.979]
Epoch [81/120    avg_loss:0.105, val_acc:0.979]
Epoch [82/120    avg_loss:0.100, val_acc:0.981]
Epoch [83/120    avg_loss:0.093, val_acc:0.981]
Epoch [84/120    avg_loss:0.084, val_acc:0.979]
Epoch [85/120    avg_loss:0.097, val_acc:0.981]
Epoch [86/120    avg_loss:0.087, val_acc:0.981]
Epoch [87/120    avg_loss:0.083, val_acc:0.981]
Epoch [88/120    avg_loss:0.112, val_acc:0.979]
Epoch [89/120    avg_loss:0.087, val_acc:0.981]
Epoch [90/120    avg_loss:0.088, val_acc:0.981]
Epoch [91/120    avg_loss:0.092, val_acc:0.981]
Epoch [92/120    avg_loss:0.078, val_acc:0.979]
Epoch [93/120    avg_loss:0.074, val_acc:0.979]
Epoch [94/120    avg_loss:0.086, val_acc:0.979]
Epoch [95/120    avg_loss:0.070, val_acc:0.977]
Epoch [96/120    avg_loss:0.075, val_acc:0.977]
Epoch [97/120    avg_loss:0.077, val_acc:0.979]
Epoch [98/120    avg_loss:0.077, val_acc:0.981]
Epoch [99/120    avg_loss:0.072, val_acc:0.981]
Epoch [100/120    avg_loss:0.070, val_acc:0.979]
Epoch [101/120    avg_loss:0.065, val_acc:0.979]
Epoch [102/120    avg_loss:0.064, val_acc:0.979]
Epoch [103/120    avg_loss:0.082, val_acc:0.979]
Epoch [104/120    avg_loss:0.072, val_acc:0.981]
Epoch [105/120    avg_loss:0.077, val_acc:0.979]
Epoch [106/120    avg_loss:0.076, val_acc:0.981]
Epoch [107/120    avg_loss:0.067, val_acc:0.979]
Epoch [108/120    avg_loss:0.072, val_acc:0.979]
Epoch [109/120    avg_loss:0.065, val_acc:0.977]
Epoch [110/120    avg_loss:0.063, val_acc:0.979]
Epoch [111/120    avg_loss:0.088, val_acc:0.981]
Epoch [112/120    avg_loss:0.078, val_acc:0.981]
Epoch [113/120    avg_loss:0.070, val_acc:0.981]
Epoch [114/120    avg_loss:0.063, val_acc:0.981]
Epoch [115/120    avg_loss:0.059, val_acc:0.981]
Epoch [116/120    avg_loss:0.074, val_acc:0.981]
Epoch [117/120    avg_loss:0.070, val_acc:0.981]
Epoch [118/120    avg_loss:0.058, val_acc:0.981]
Epoch [119/120    avg_loss:0.058, val_acc:0.981]
Epoch [120/120    avg_loss:0.068, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   0 219   6   0   0   0   5   0   0   0   0   0]
 [  0   0   0   0 199  28   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6   0 200   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.4861407249467

F1 scores:
[       nan 1.         0.96213808 0.97550111 0.89038031 0.8802589
 0.98522167 0.9039548  0.99359795 1.         1.         1.
 1.         1.        ]

Kappa:
0.9831453067507098
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f89cbd13898>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.554, val_acc:0.406]
Epoch [2/120    avg_loss:2.370, val_acc:0.450]
Epoch [3/120    avg_loss:2.241, val_acc:0.502]
Epoch [4/120    avg_loss:2.094, val_acc:0.515]
Epoch [5/120    avg_loss:1.963, val_acc:0.596]
Epoch [6/120    avg_loss:1.826, val_acc:0.667]
Epoch [7/120    avg_loss:1.660, val_acc:0.637]
Epoch [8/120    avg_loss:1.550, val_acc:0.662]
Epoch [9/120    avg_loss:1.389, val_acc:0.671]
Epoch [10/120    avg_loss:1.306, val_acc:0.796]
Epoch [11/120    avg_loss:1.220, val_acc:0.806]
Epoch [12/120    avg_loss:1.130, val_acc:0.810]
Epoch [13/120    avg_loss:1.030, val_acc:0.823]
Epoch [14/120    avg_loss:0.962, val_acc:0.854]
Epoch [15/120    avg_loss:0.965, val_acc:0.865]
Epoch [16/120    avg_loss:0.854, val_acc:0.881]
Epoch [17/120    avg_loss:0.812, val_acc:0.896]
Epoch [18/120    avg_loss:0.758, val_acc:0.912]
Epoch [19/120    avg_loss:0.669, val_acc:0.900]
Epoch [20/120    avg_loss:0.653, val_acc:0.894]
Epoch [21/120    avg_loss:0.640, val_acc:0.894]
Epoch [22/120    avg_loss:0.568, val_acc:0.912]
Epoch [23/120    avg_loss:0.616, val_acc:0.902]
Epoch [24/120    avg_loss:0.559, val_acc:0.900]
Epoch [25/120    avg_loss:0.524, val_acc:0.935]
Epoch [26/120    avg_loss:0.466, val_acc:0.917]
Epoch [27/120    avg_loss:0.456, val_acc:0.923]
Epoch [28/120    avg_loss:0.445, val_acc:0.915]
Epoch [29/120    avg_loss:0.459, val_acc:0.927]
Epoch [30/120    avg_loss:0.442, val_acc:0.935]
Epoch [31/120    avg_loss:0.413, val_acc:0.946]
Epoch [32/120    avg_loss:0.468, val_acc:0.938]
Epoch [33/120    avg_loss:0.357, val_acc:0.952]
Epoch [34/120    avg_loss:0.318, val_acc:0.933]
Epoch [35/120    avg_loss:0.305, val_acc:0.946]
Epoch [36/120    avg_loss:0.322, val_acc:0.894]
Epoch [37/120    avg_loss:0.287, val_acc:0.950]
Epoch [38/120    avg_loss:0.300, val_acc:0.956]
Epoch [39/120    avg_loss:0.253, val_acc:0.952]
Epoch [40/120    avg_loss:0.299, val_acc:0.942]
Epoch [41/120    avg_loss:0.305, val_acc:0.927]
Epoch [42/120    avg_loss:0.351, val_acc:0.942]
Epoch [43/120    avg_loss:0.405, val_acc:0.940]
Epoch [44/120    avg_loss:0.345, val_acc:0.952]
Epoch [45/120    avg_loss:0.298, val_acc:0.921]
Epoch [46/120    avg_loss:0.264, val_acc:0.958]
Epoch [47/120    avg_loss:0.265, val_acc:0.938]
Epoch [48/120    avg_loss:0.261, val_acc:0.960]
Epoch [49/120    avg_loss:0.222, val_acc:0.958]
Epoch [50/120    avg_loss:0.222, val_acc:0.963]
Epoch [51/120    avg_loss:0.219, val_acc:0.965]
Epoch [52/120    avg_loss:0.210, val_acc:0.963]
Epoch [53/120    avg_loss:0.204, val_acc:0.965]
Epoch [54/120    avg_loss:0.222, val_acc:0.960]
Epoch [55/120    avg_loss:0.242, val_acc:0.981]
Epoch [56/120    avg_loss:0.178, val_acc:0.963]
Epoch [57/120    avg_loss:0.258, val_acc:0.938]
Epoch [58/120    avg_loss:0.220, val_acc:0.963]
Epoch [59/120    avg_loss:0.165, val_acc:0.960]
Epoch [60/120    avg_loss:0.197, val_acc:0.952]
Epoch [61/120    avg_loss:0.236, val_acc:0.973]
Epoch [62/120    avg_loss:0.185, val_acc:0.977]
Epoch [63/120    avg_loss:0.189, val_acc:0.963]
Epoch [64/120    avg_loss:0.180, val_acc:0.977]
Epoch [65/120    avg_loss:0.162, val_acc:0.956]
Epoch [66/120    avg_loss:0.182, val_acc:0.956]
Epoch [67/120    avg_loss:0.299, val_acc:0.948]
Epoch [68/120    avg_loss:0.169, val_acc:0.954]
Epoch [69/120    avg_loss:0.165, val_acc:0.973]
Epoch [70/120    avg_loss:0.151, val_acc:0.985]
Epoch [71/120    avg_loss:0.120, val_acc:0.985]
Epoch [72/120    avg_loss:0.111, val_acc:0.985]
Epoch [73/120    avg_loss:0.122, val_acc:0.985]
Epoch [74/120    avg_loss:0.100, val_acc:0.981]
Epoch [75/120    avg_loss:0.103, val_acc:0.985]
Epoch [76/120    avg_loss:0.101, val_acc:0.985]
Epoch [77/120    avg_loss:0.099, val_acc:0.985]
Epoch [78/120    avg_loss:0.089, val_acc:0.985]
Epoch [79/120    avg_loss:0.123, val_acc:0.985]
Epoch [80/120    avg_loss:0.099, val_acc:0.985]
Epoch [81/120    avg_loss:0.097, val_acc:0.988]
Epoch [82/120    avg_loss:0.084, val_acc:0.985]
Epoch [83/120    avg_loss:0.103, val_acc:0.983]
Epoch [84/120    avg_loss:0.099, val_acc:0.983]
Epoch [85/120    avg_loss:0.091, val_acc:0.983]
Epoch [86/120    avg_loss:0.075, val_acc:0.988]
Epoch [87/120    avg_loss:0.085, val_acc:0.988]
Epoch [88/120    avg_loss:0.081, val_acc:0.988]
Epoch [89/120    avg_loss:0.077, val_acc:0.985]
Epoch [90/120    avg_loss:0.093, val_acc:0.985]
Epoch [91/120    avg_loss:0.069, val_acc:0.988]
Epoch [92/120    avg_loss:0.089, val_acc:0.990]
Epoch [93/120    avg_loss:0.079, val_acc:0.985]
Epoch [94/120    avg_loss:0.081, val_acc:0.983]
Epoch [95/120    avg_loss:0.083, val_acc:0.985]
Epoch [96/120    avg_loss:0.087, val_acc:0.988]
Epoch [97/120    avg_loss:0.073, val_acc:0.988]
Epoch [98/120    avg_loss:0.097, val_acc:0.988]
Epoch [99/120    avg_loss:0.082, val_acc:0.988]
Epoch [100/120    avg_loss:0.091, val_acc:0.988]
Epoch [101/120    avg_loss:0.090, val_acc:0.990]
Epoch [102/120    avg_loss:0.080, val_acc:0.990]
Epoch [103/120    avg_loss:0.069, val_acc:0.990]
Epoch [104/120    avg_loss:0.071, val_acc:0.990]
Epoch [105/120    avg_loss:0.073, val_acc:0.990]
Epoch [106/120    avg_loss:0.077, val_acc:0.990]
Epoch [107/120    avg_loss:0.086, val_acc:0.990]
Epoch [108/120    avg_loss:0.082, val_acc:0.990]
Epoch [109/120    avg_loss:0.082, val_acc:0.990]
Epoch [110/120    avg_loss:0.058, val_acc:0.990]
Epoch [111/120    avg_loss:0.066, val_acc:0.990]
Epoch [112/120    avg_loss:0.080, val_acc:0.990]
Epoch [113/120    avg_loss:0.075, val_acc:0.990]
Epoch [114/120    avg_loss:0.089, val_acc:0.990]
Epoch [115/120    avg_loss:0.083, val_acc:0.990]
Epoch [116/120    avg_loss:0.093, val_acc:0.990]
Epoch [117/120    avg_loss:0.070, val_acc:0.990]
Epoch [118/120    avg_loss:0.065, val_acc:0.990]
Epoch [119/120    avg_loss:0.064, val_acc:0.990]
Epoch [120/120    avg_loss:0.066, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 224   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.7633262260128

F1 scores:
[       nan 1.         0.96688742 0.98678414 0.9059081  0.87372014
 0.99756691 0.91954023 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9862308309421707
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3f4fc28898>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.541, val_acc:0.344]
Epoch [2/120    avg_loss:2.348, val_acc:0.389]
Epoch [3/120    avg_loss:2.191, val_acc:0.430]
Epoch [4/120    avg_loss:2.073, val_acc:0.537]
Epoch [5/120    avg_loss:1.941, val_acc:0.559]
Epoch [6/120    avg_loss:1.829, val_acc:0.633]
Epoch [7/120    avg_loss:1.692, val_acc:0.621]
Epoch [8/120    avg_loss:1.573, val_acc:0.691]
Epoch [9/120    avg_loss:1.465, val_acc:0.740]
Epoch [10/120    avg_loss:1.344, val_acc:0.740]
Epoch [11/120    avg_loss:1.265, val_acc:0.682]
Epoch [12/120    avg_loss:1.171, val_acc:0.709]
Epoch [13/120    avg_loss:1.077, val_acc:0.797]
Epoch [14/120    avg_loss:0.997, val_acc:0.746]
Epoch [15/120    avg_loss:0.944, val_acc:0.791]
Epoch [16/120    avg_loss:0.870, val_acc:0.877]
Epoch [17/120    avg_loss:0.777, val_acc:0.895]
Epoch [18/120    avg_loss:0.711, val_acc:0.900]
Epoch [19/120    avg_loss:0.670, val_acc:0.904]
Epoch [20/120    avg_loss:0.662, val_acc:0.867]
Epoch [21/120    avg_loss:0.594, val_acc:0.893]
Epoch [22/120    avg_loss:0.562, val_acc:0.932]
Epoch [23/120    avg_loss:0.539, val_acc:0.918]
Epoch [24/120    avg_loss:0.560, val_acc:0.922]
Epoch [25/120    avg_loss:0.499, val_acc:0.932]
Epoch [26/120    avg_loss:0.469, val_acc:0.914]
Epoch [27/120    avg_loss:0.510, val_acc:0.918]
Epoch [28/120    avg_loss:0.441, val_acc:0.936]
Epoch [29/120    avg_loss:0.390, val_acc:0.945]
Epoch [30/120    avg_loss:0.369, val_acc:0.883]
Epoch [31/120    avg_loss:0.389, val_acc:0.930]
Epoch [32/120    avg_loss:0.377, val_acc:0.932]
Epoch [33/120    avg_loss:0.351, val_acc:0.930]
Epoch [34/120    avg_loss:0.409, val_acc:0.918]
Epoch [35/120    avg_loss:0.337, val_acc:0.904]
Epoch [36/120    avg_loss:0.340, val_acc:0.926]
Epoch [37/120    avg_loss:0.335, val_acc:0.951]
Epoch [38/120    avg_loss:0.375, val_acc:0.951]
Epoch [39/120    avg_loss:0.296, val_acc:0.953]
Epoch [40/120    avg_loss:0.319, val_acc:0.938]
Epoch [41/120    avg_loss:0.274, val_acc:0.963]
Epoch [42/120    avg_loss:0.282, val_acc:0.943]
Epoch [43/120    avg_loss:0.275, val_acc:0.941]
Epoch [44/120    avg_loss:0.271, val_acc:0.926]
Epoch [45/120    avg_loss:0.311, val_acc:0.955]
Epoch [46/120    avg_loss:0.299, val_acc:0.961]
Epoch [47/120    avg_loss:0.265, val_acc:0.951]
Epoch [48/120    avg_loss:0.233, val_acc:0.971]
Epoch [49/120    avg_loss:0.249, val_acc:0.961]
Epoch [50/120    avg_loss:0.251, val_acc:0.955]
Epoch [51/120    avg_loss:0.231, val_acc:0.959]
Epoch [52/120    avg_loss:0.212, val_acc:0.945]
Epoch [53/120    avg_loss:0.231, val_acc:0.959]
Epoch [54/120    avg_loss:0.188, val_acc:0.969]
Epoch [55/120    avg_loss:0.160, val_acc:0.971]
Epoch [56/120    avg_loss:0.156, val_acc:0.963]
Epoch [57/120    avg_loss:0.158, val_acc:0.973]
Epoch [58/120    avg_loss:0.155, val_acc:0.961]
Epoch [59/120    avg_loss:0.157, val_acc:0.969]
Epoch [60/120    avg_loss:0.205, val_acc:0.945]
Epoch [61/120    avg_loss:0.172, val_acc:0.955]
Epoch [62/120    avg_loss:0.174, val_acc:0.961]
Epoch [63/120    avg_loss:0.154, val_acc:0.977]
Epoch [64/120    avg_loss:0.135, val_acc:0.971]
Epoch [65/120    avg_loss:0.179, val_acc:0.967]
Epoch [66/120    avg_loss:0.166, val_acc:0.971]
Epoch [67/120    avg_loss:0.109, val_acc:0.975]
Epoch [68/120    avg_loss:0.144, val_acc:0.977]
Epoch [69/120    avg_loss:0.128, val_acc:0.971]
Epoch [70/120    avg_loss:0.147, val_acc:0.971]
Epoch [71/120    avg_loss:0.136, val_acc:0.973]
Epoch [72/120    avg_loss:0.107, val_acc:0.979]
Epoch [73/120    avg_loss:0.085, val_acc:0.984]
Epoch [74/120    avg_loss:0.100, val_acc:0.971]
Epoch [75/120    avg_loss:0.105, val_acc:0.979]
Epoch [76/120    avg_loss:0.089, val_acc:0.975]
Epoch [77/120    avg_loss:0.099, val_acc:0.979]
Epoch [78/120    avg_loss:0.110, val_acc:0.980]
Epoch [79/120    avg_loss:0.090, val_acc:0.990]
Epoch [80/120    avg_loss:0.110, val_acc:0.979]
Epoch [81/120    avg_loss:0.104, val_acc:0.984]
Epoch [82/120    avg_loss:0.064, val_acc:0.977]
Epoch [83/120    avg_loss:0.102, val_acc:0.982]
Epoch [84/120    avg_loss:0.099, val_acc:0.951]
Epoch [85/120    avg_loss:0.109, val_acc:0.971]
Epoch [86/120    avg_loss:0.077, val_acc:0.988]
Epoch [87/120    avg_loss:0.075, val_acc:0.986]
Epoch [88/120    avg_loss:0.085, val_acc:0.975]
Epoch [89/120    avg_loss:0.082, val_acc:0.975]
Epoch [90/120    avg_loss:0.105, val_acc:0.961]
Epoch [91/120    avg_loss:0.153, val_acc:0.973]
Epoch [92/120    avg_loss:0.122, val_acc:0.951]
Epoch [93/120    avg_loss:0.110, val_acc:0.980]
Epoch [94/120    avg_loss:0.074, val_acc:0.988]
Epoch [95/120    avg_loss:0.069, val_acc:0.992]
Epoch [96/120    avg_loss:0.067, val_acc:0.990]
Epoch [97/120    avg_loss:0.051, val_acc:0.992]
Epoch [98/120    avg_loss:0.063, val_acc:0.992]
Epoch [99/120    avg_loss:0.050, val_acc:0.994]
Epoch [100/120    avg_loss:0.054, val_acc:0.994]
Epoch [101/120    avg_loss:0.053, val_acc:0.994]
Epoch [102/120    avg_loss:0.047, val_acc:0.994]
Epoch [103/120    avg_loss:0.056, val_acc:0.992]
Epoch [104/120    avg_loss:0.047, val_acc:0.992]
Epoch [105/120    avg_loss:0.047, val_acc:0.992]
Epoch [106/120    avg_loss:0.054, val_acc:0.992]
Epoch [107/120    avg_loss:0.040, val_acc:0.992]
Epoch [108/120    avg_loss:0.040, val_acc:0.992]
Epoch [109/120    avg_loss:0.057, val_acc:0.992]
Epoch [110/120    avg_loss:0.037, val_acc:0.990]
Epoch [111/120    avg_loss:0.045, val_acc:0.992]
Epoch [112/120    avg_loss:0.038, val_acc:0.992]
Epoch [113/120    avg_loss:0.046, val_acc:0.992]
Epoch [114/120    avg_loss:0.050, val_acc:0.992]
Epoch [115/120    avg_loss:0.040, val_acc:0.992]
Epoch [116/120    avg_loss:0.043, val_acc:0.992]
Epoch [117/120    avg_loss:0.045, val_acc:0.992]
Epoch [118/120    avg_loss:0.041, val_acc:0.992]
Epoch [119/120    avg_loss:0.043, val_acc:0.992]
Epoch [120/120    avg_loss:0.047, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   9   0   0   0   0   0   0]
 [  0   0   0 217   7   0   0   0   0   6   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.65671641791045

F1 scores:
[       nan 1.         0.95454545 0.97091723 0.91868132 0.89864865
 1.         0.89247312 1.         0.99363057 1.         1.
 1.         1.        ]

Kappa:
0.98504408646155
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efe9842b908>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.583, val_acc:0.306]
Epoch [2/120    avg_loss:2.356, val_acc:0.354]
Epoch [3/120    avg_loss:2.191, val_acc:0.404]
Epoch [4/120    avg_loss:2.066, val_acc:0.440]
Epoch [5/120    avg_loss:1.926, val_acc:0.533]
Epoch [6/120    avg_loss:1.804, val_acc:0.571]
Epoch [7/120    avg_loss:1.690, val_acc:0.604]
Epoch [8/120    avg_loss:1.592, val_acc:0.719]
Epoch [9/120    avg_loss:1.486, val_acc:0.748]
Epoch [10/120    avg_loss:1.389, val_acc:0.744]
Epoch [11/120    avg_loss:1.313, val_acc:0.767]
Epoch [12/120    avg_loss:1.207, val_acc:0.787]
Epoch [13/120    avg_loss:1.120, val_acc:0.800]
Epoch [14/120    avg_loss:1.056, val_acc:0.821]
Epoch [15/120    avg_loss:0.975, val_acc:0.827]
Epoch [16/120    avg_loss:0.886, val_acc:0.867]
Epoch [17/120    avg_loss:0.843, val_acc:0.865]
Epoch [18/120    avg_loss:0.784, val_acc:0.910]
Epoch [19/120    avg_loss:0.734, val_acc:0.898]
Epoch [20/120    avg_loss:0.675, val_acc:0.890]
Epoch [21/120    avg_loss:0.627, val_acc:0.910]
Epoch [22/120    avg_loss:0.577, val_acc:0.908]
Epoch [23/120    avg_loss:0.565, val_acc:0.898]
Epoch [24/120    avg_loss:0.527, val_acc:0.910]
Epoch [25/120    avg_loss:0.554, val_acc:0.908]
Epoch [26/120    avg_loss:0.511, val_acc:0.908]
Epoch [27/120    avg_loss:0.450, val_acc:0.929]
Epoch [28/120    avg_loss:0.435, val_acc:0.929]
Epoch [29/120    avg_loss:0.419, val_acc:0.912]
Epoch [30/120    avg_loss:0.390, val_acc:0.942]
Epoch [31/120    avg_loss:0.417, val_acc:0.917]
Epoch [32/120    avg_loss:0.444, val_acc:0.917]
Epoch [33/120    avg_loss:0.389, val_acc:0.935]
Epoch [34/120    avg_loss:0.344, val_acc:0.935]
Epoch [35/120    avg_loss:0.310, val_acc:0.935]
Epoch [36/120    avg_loss:0.340, val_acc:0.915]
Epoch [37/120    avg_loss:0.429, val_acc:0.946]
Epoch [38/120    avg_loss:0.413, val_acc:0.929]
Epoch [39/120    avg_loss:0.337, val_acc:0.948]
Epoch [40/120    avg_loss:0.347, val_acc:0.906]
Epoch [41/120    avg_loss:0.316, val_acc:0.948]
Epoch [42/120    avg_loss:0.280, val_acc:0.952]
Epoch [43/120    avg_loss:0.246, val_acc:0.948]
Epoch [44/120    avg_loss:0.247, val_acc:0.940]
Epoch [45/120    avg_loss:0.261, val_acc:0.942]
Epoch [46/120    avg_loss:0.222, val_acc:0.940]
Epoch [47/120    avg_loss:0.257, val_acc:0.940]
Epoch [48/120    avg_loss:0.248, val_acc:0.956]
Epoch [49/120    avg_loss:0.219, val_acc:0.946]
Epoch [50/120    avg_loss:0.194, val_acc:0.919]
Epoch [51/120    avg_loss:0.213, val_acc:0.956]
Epoch [52/120    avg_loss:0.225, val_acc:0.950]
Epoch [53/120    avg_loss:0.215, val_acc:0.958]
Epoch [54/120    avg_loss:0.167, val_acc:0.958]
Epoch [55/120    avg_loss:0.235, val_acc:0.933]
Epoch [56/120    avg_loss:0.210, val_acc:0.948]
Epoch [57/120    avg_loss:0.172, val_acc:0.969]
Epoch [58/120    avg_loss:0.194, val_acc:0.933]
Epoch [59/120    avg_loss:0.183, val_acc:0.969]
Epoch [60/120    avg_loss:0.162, val_acc:0.952]
Epoch [61/120    avg_loss:0.173, val_acc:0.958]
Epoch [62/120    avg_loss:0.177, val_acc:0.958]
Epoch [63/120    avg_loss:0.251, val_acc:0.929]
Epoch [64/120    avg_loss:0.214, val_acc:0.958]
Epoch [65/120    avg_loss:0.195, val_acc:0.952]
Epoch [66/120    avg_loss:0.174, val_acc:0.935]
Epoch [67/120    avg_loss:0.147, val_acc:0.956]
Epoch [68/120    avg_loss:0.124, val_acc:0.965]
Epoch [69/120    avg_loss:0.138, val_acc:0.963]
Epoch [70/120    avg_loss:0.126, val_acc:0.956]
Epoch [71/120    avg_loss:0.144, val_acc:0.954]
Epoch [72/120    avg_loss:0.150, val_acc:0.960]
Epoch [73/120    avg_loss:0.128, val_acc:0.977]
Epoch [74/120    avg_loss:0.090, val_acc:0.973]
Epoch [75/120    avg_loss:0.091, val_acc:0.971]
Epoch [76/120    avg_loss:0.099, val_acc:0.971]
Epoch [77/120    avg_loss:0.071, val_acc:0.973]
Epoch [78/120    avg_loss:0.079, val_acc:0.971]
Epoch [79/120    avg_loss:0.104, val_acc:0.971]
Epoch [80/120    avg_loss:0.072, val_acc:0.973]
Epoch [81/120    avg_loss:0.099, val_acc:0.973]
Epoch [82/120    avg_loss:0.071, val_acc:0.971]
Epoch [83/120    avg_loss:0.069, val_acc:0.975]
Epoch [84/120    avg_loss:0.091, val_acc:0.977]
Epoch [85/120    avg_loss:0.076, val_acc:0.979]
Epoch [86/120    avg_loss:0.074, val_acc:0.977]
Epoch [87/120    avg_loss:0.073, val_acc:0.975]
Epoch [88/120    avg_loss:0.077, val_acc:0.977]
Epoch [89/120    avg_loss:0.078, val_acc:0.979]
Epoch [90/120    avg_loss:0.082, val_acc:0.977]
Epoch [91/120    avg_loss:0.067, val_acc:0.975]
Epoch [92/120    avg_loss:0.064, val_acc:0.975]
Epoch [93/120    avg_loss:0.070, val_acc:0.977]
Epoch [94/120    avg_loss:0.067, val_acc:0.975]
Epoch [95/120    avg_loss:0.065, val_acc:0.977]
Epoch [96/120    avg_loss:0.060, val_acc:0.979]
Epoch [97/120    avg_loss:0.068, val_acc:0.979]
Epoch [98/120    avg_loss:0.059, val_acc:0.973]
Epoch [99/120    avg_loss:0.058, val_acc:0.977]
Epoch [100/120    avg_loss:0.061, val_acc:0.979]
Epoch [101/120    avg_loss:0.080, val_acc:0.977]
Epoch [102/120    avg_loss:0.061, val_acc:0.979]
Epoch [103/120    avg_loss:0.065, val_acc:0.977]
Epoch [104/120    avg_loss:0.060, val_acc:0.979]
Epoch [105/120    avg_loss:0.072, val_acc:0.979]
Epoch [106/120    avg_loss:0.062, val_acc:0.977]
Epoch [107/120    avg_loss:0.064, val_acc:0.979]
Epoch [108/120    avg_loss:0.068, val_acc:0.977]
Epoch [109/120    avg_loss:0.061, val_acc:0.979]
Epoch [110/120    avg_loss:0.083, val_acc:0.981]
Epoch [111/120    avg_loss:0.048, val_acc:0.975]
Epoch [112/120    avg_loss:0.057, val_acc:0.979]
Epoch [113/120    avg_loss:0.064, val_acc:0.979]
Epoch [114/120    avg_loss:0.060, val_acc:0.981]
Epoch [115/120    avg_loss:0.059, val_acc:0.979]
Epoch [116/120    avg_loss:0.066, val_acc:0.979]
Epoch [117/120    avg_loss:0.061, val_acc:0.981]
Epoch [118/120    avg_loss:0.049, val_acc:0.979]
Epoch [119/120    avg_loss:0.071, val_acc:0.979]
Epoch [120/120    avg_loss:0.062, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   0 218   7   0   0   0   4   1   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  23 122   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.52878464818762

F1 scores:
[       nan 1.         0.95945946 0.97321429 0.9017094  0.86219081
 1.         0.9010989  0.99487179 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9836188779409748
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdf99955898>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.609, val_acc:0.178]
Epoch [2/120    avg_loss:2.409, val_acc:0.355]
Epoch [3/120    avg_loss:2.244, val_acc:0.496]
Epoch [4/120    avg_loss:2.100, val_acc:0.568]
Epoch [5/120    avg_loss:1.962, val_acc:0.602]
Epoch [6/120    avg_loss:1.829, val_acc:0.635]
Epoch [7/120    avg_loss:1.729, val_acc:0.699]
Epoch [8/120    avg_loss:1.604, val_acc:0.695]
Epoch [9/120    avg_loss:1.507, val_acc:0.697]
Epoch [10/120    avg_loss:1.403, val_acc:0.699]
Epoch [11/120    avg_loss:1.276, val_acc:0.748]
Epoch [12/120    avg_loss:1.167, val_acc:0.768]
Epoch [13/120    avg_loss:1.081, val_acc:0.789]
Epoch [14/120    avg_loss:1.008, val_acc:0.799]
Epoch [15/120    avg_loss:0.978, val_acc:0.854]
Epoch [16/120    avg_loss:0.911, val_acc:0.801]
Epoch [17/120    avg_loss:0.872, val_acc:0.791]
Epoch [18/120    avg_loss:0.831, val_acc:0.818]
Epoch [19/120    avg_loss:0.798, val_acc:0.873]
Epoch [20/120    avg_loss:0.735, val_acc:0.895]
Epoch [21/120    avg_loss:0.701, val_acc:0.885]
Epoch [22/120    avg_loss:0.620, val_acc:0.910]
Epoch [23/120    avg_loss:0.615, val_acc:0.885]
Epoch [24/120    avg_loss:0.601, val_acc:0.898]
Epoch [25/120    avg_loss:0.558, val_acc:0.914]
Epoch [26/120    avg_loss:0.509, val_acc:0.914]
Epoch [27/120    avg_loss:0.566, val_acc:0.904]
Epoch [28/120    avg_loss:0.527, val_acc:0.930]
Epoch [29/120    avg_loss:0.532, val_acc:0.891]
Epoch [30/120    avg_loss:0.457, val_acc:0.918]
Epoch [31/120    avg_loss:0.439, val_acc:0.930]
Epoch [32/120    avg_loss:0.396, val_acc:0.928]
Epoch [33/120    avg_loss:0.349, val_acc:0.928]
Epoch [34/120    avg_loss:0.369, val_acc:0.938]
Epoch [35/120    avg_loss:0.380, val_acc:0.928]
Epoch [36/120    avg_loss:0.378, val_acc:0.918]
Epoch [37/120    avg_loss:0.407, val_acc:0.912]
Epoch [38/120    avg_loss:0.352, val_acc:0.924]
Epoch [39/120    avg_loss:0.325, val_acc:0.912]
Epoch [40/120    avg_loss:0.404, val_acc:0.924]
Epoch [41/120    avg_loss:0.298, val_acc:0.934]
Epoch [42/120    avg_loss:0.306, val_acc:0.902]
Epoch [43/120    avg_loss:0.328, val_acc:0.938]
Epoch [44/120    avg_loss:0.270, val_acc:0.955]
Epoch [45/120    avg_loss:0.278, val_acc:0.953]
Epoch [46/120    avg_loss:0.235, val_acc:0.949]
Epoch [47/120    avg_loss:0.282, val_acc:0.947]
Epoch [48/120    avg_loss:0.273, val_acc:0.947]
Epoch [49/120    avg_loss:0.199, val_acc:0.947]
Epoch [50/120    avg_loss:0.215, val_acc:0.945]
Epoch [51/120    avg_loss:0.261, val_acc:0.943]
Epoch [52/120    avg_loss:0.274, val_acc:0.939]
Epoch [53/120    avg_loss:0.214, val_acc:0.934]
Epoch [54/120    avg_loss:0.187, val_acc:0.963]
Epoch [55/120    avg_loss:0.236, val_acc:0.941]
Epoch [56/120    avg_loss:0.339, val_acc:0.949]
Epoch [57/120    avg_loss:0.209, val_acc:0.943]
Epoch [58/120    avg_loss:0.191, val_acc:0.963]
Epoch [59/120    avg_loss:0.233, val_acc:0.934]
Epoch [60/120    avg_loss:0.203, val_acc:0.941]
Epoch [61/120    avg_loss:0.208, val_acc:0.959]
Epoch [62/120    avg_loss:0.175, val_acc:0.963]
Epoch [63/120    avg_loss:0.160, val_acc:0.965]
Epoch [64/120    avg_loss:0.127, val_acc:0.941]
Epoch [65/120    avg_loss:0.166, val_acc:0.961]
Epoch [66/120    avg_loss:0.158, val_acc:0.943]
Epoch [67/120    avg_loss:0.122, val_acc:0.969]
Epoch [68/120    avg_loss:0.144, val_acc:0.967]
Epoch [69/120    avg_loss:0.138, val_acc:0.939]
Epoch [70/120    avg_loss:0.137, val_acc:0.955]
Epoch [71/120    avg_loss:0.121, val_acc:0.959]
Epoch [72/120    avg_loss:0.207, val_acc:0.945]
Epoch [73/120    avg_loss:0.185, val_acc:0.965]
Epoch [74/120    avg_loss:0.137, val_acc:0.943]
Epoch [75/120    avg_loss:0.137, val_acc:0.961]
Epoch [76/120    avg_loss:0.086, val_acc:0.969]
Epoch [77/120    avg_loss:0.110, val_acc:0.963]
Epoch [78/120    avg_loss:0.102, val_acc:0.975]
Epoch [79/120    avg_loss:0.156, val_acc:0.938]
Epoch [80/120    avg_loss:0.153, val_acc:0.951]
Epoch [81/120    avg_loss:0.168, val_acc:0.953]
Epoch [82/120    avg_loss:0.120, val_acc:0.975]
Epoch [83/120    avg_loss:0.144, val_acc:0.973]
Epoch [84/120    avg_loss:0.105, val_acc:0.975]
Epoch [85/120    avg_loss:0.089, val_acc:0.975]
Epoch [86/120    avg_loss:0.094, val_acc:0.977]
Epoch [87/120    avg_loss:0.068, val_acc:0.969]
Epoch [88/120    avg_loss:0.138, val_acc:0.961]
Epoch [89/120    avg_loss:0.122, val_acc:0.967]
Epoch [90/120    avg_loss:0.091, val_acc:0.971]
Epoch [91/120    avg_loss:0.117, val_acc:0.975]
Epoch [92/120    avg_loss:0.073, val_acc:0.973]
Epoch [93/120    avg_loss:0.076, val_acc:0.965]
Epoch [94/120    avg_loss:0.066, val_acc:0.975]
Epoch [95/120    avg_loss:0.052, val_acc:0.973]
Epoch [96/120    avg_loss:0.068, val_acc:0.971]
Epoch [97/120    avg_loss:0.052, val_acc:0.975]
Epoch [98/120    avg_loss:0.055, val_acc:0.969]
Epoch [99/120    avg_loss:0.046, val_acc:0.980]
Epoch [100/120    avg_loss:0.052, val_acc:0.971]
Epoch [101/120    avg_loss:0.050, val_acc:0.977]
Epoch [102/120    avg_loss:0.043, val_acc:0.973]
Epoch [103/120    avg_loss:0.053, val_acc:0.977]
Epoch [104/120    avg_loss:0.034, val_acc:0.982]
Epoch [105/120    avg_loss:0.039, val_acc:0.984]
Epoch [106/120    avg_loss:0.053, val_acc:0.943]
Epoch [107/120    avg_loss:0.060, val_acc:0.979]
Epoch [108/120    avg_loss:0.115, val_acc:0.951]
Epoch [109/120    avg_loss:0.099, val_acc:0.963]
Epoch [110/120    avg_loss:0.165, val_acc:0.963]
Epoch [111/120    avg_loss:0.096, val_acc:0.975]
Epoch [112/120    avg_loss:0.083, val_acc:0.980]
Epoch [113/120    avg_loss:0.058, val_acc:0.979]
Epoch [114/120    avg_loss:0.063, val_acc:0.941]
Epoch [115/120    avg_loss:0.168, val_acc:0.938]
Epoch [116/120    avg_loss:0.233, val_acc:0.945]
Epoch [117/120    avg_loss:0.130, val_acc:0.963]
Epoch [118/120    avg_loss:0.088, val_acc:0.957]
Epoch [119/120    avg_loss:0.072, val_acc:0.965]
Epoch [120/120    avg_loss:0.058, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 220   9   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 1.         0.97333333 0.97777778 0.90598291 0.88888889
 0.99266504 0.93785311 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9867055861341247
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f672bd00908>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.575, val_acc:0.293]
Epoch [2/120    avg_loss:2.354, val_acc:0.309]
Epoch [3/120    avg_loss:2.201, val_acc:0.387]
Epoch [4/120    avg_loss:2.075, val_acc:0.465]
Epoch [5/120    avg_loss:1.943, val_acc:0.555]
Epoch [6/120    avg_loss:1.834, val_acc:0.578]
Epoch [7/120    avg_loss:1.688, val_acc:0.648]
Epoch [8/120    avg_loss:1.590, val_acc:0.662]
Epoch [9/120    avg_loss:1.477, val_acc:0.680]
Epoch [10/120    avg_loss:1.341, val_acc:0.752]
Epoch [11/120    avg_loss:1.220, val_acc:0.791]
Epoch [12/120    avg_loss:1.150, val_acc:0.799]
Epoch [13/120    avg_loss:1.075, val_acc:0.816]
Epoch [14/120    avg_loss:0.972, val_acc:0.807]
Epoch [15/120    avg_loss:0.943, val_acc:0.869]
Epoch [16/120    avg_loss:0.863, val_acc:0.865]
Epoch [17/120    avg_loss:0.810, val_acc:0.816]
Epoch [18/120    avg_loss:0.770, val_acc:0.879]
Epoch [19/120    avg_loss:0.731, val_acc:0.861]
Epoch [20/120    avg_loss:0.692, val_acc:0.898]
Epoch [21/120    avg_loss:0.654, val_acc:0.879]
Epoch [22/120    avg_loss:0.600, val_acc:0.914]
Epoch [23/120    avg_loss:0.586, val_acc:0.887]
Epoch [24/120    avg_loss:0.538, val_acc:0.938]
Epoch [25/120    avg_loss:0.590, val_acc:0.916]
Epoch [26/120    avg_loss:0.464, val_acc:0.924]
Epoch [27/120    avg_loss:0.448, val_acc:0.895]
Epoch [28/120    avg_loss:0.478, val_acc:0.904]
Epoch [29/120    avg_loss:0.410, val_acc:0.926]
Epoch [30/120    avg_loss:0.376, val_acc:0.934]
Epoch [31/120    avg_loss:0.426, val_acc:0.918]
Epoch [32/120    avg_loss:0.354, val_acc:0.934]
Epoch [33/120    avg_loss:0.341, val_acc:0.908]
Epoch [34/120    avg_loss:0.391, val_acc:0.867]
Epoch [35/120    avg_loss:0.404, val_acc:0.918]
Epoch [36/120    avg_loss:0.345, val_acc:0.938]
Epoch [37/120    avg_loss:0.305, val_acc:0.932]
Epoch [38/120    avg_loss:0.378, val_acc:0.928]
Epoch [39/120    avg_loss:0.369, val_acc:0.912]
Epoch [40/120    avg_loss:0.399, val_acc:0.875]
Epoch [41/120    avg_loss:0.311, val_acc:0.947]
Epoch [42/120    avg_loss:0.323, val_acc:0.846]
Epoch [43/120    avg_loss:0.321, val_acc:0.928]
Epoch [44/120    avg_loss:0.239, val_acc:0.936]
Epoch [45/120    avg_loss:0.297, val_acc:0.932]
Epoch [46/120    avg_loss:0.232, val_acc:0.953]
Epoch [47/120    avg_loss:0.227, val_acc:0.943]
Epoch [48/120    avg_loss:0.243, val_acc:0.936]
Epoch [49/120    avg_loss:0.273, val_acc:0.930]
Epoch [50/120    avg_loss:0.226, val_acc:0.908]
Epoch [51/120    avg_loss:0.242, val_acc:0.947]
Epoch [52/120    avg_loss:0.205, val_acc:0.955]
Epoch [53/120    avg_loss:0.176, val_acc:0.945]
Epoch [54/120    avg_loss:0.229, val_acc:0.922]
Epoch [55/120    avg_loss:0.203, val_acc:0.959]
Epoch [56/120    avg_loss:0.166, val_acc:0.955]
Epoch [57/120    avg_loss:0.182, val_acc:0.857]
Epoch [58/120    avg_loss:0.224, val_acc:0.961]
Epoch [59/120    avg_loss:0.181, val_acc:0.949]
Epoch [60/120    avg_loss:0.153, val_acc:0.965]
Epoch [61/120    avg_loss:0.173, val_acc:0.951]
Epoch [62/120    avg_loss:0.176, val_acc:0.934]
Epoch [63/120    avg_loss:0.207, val_acc:0.939]
Epoch [64/120    avg_loss:0.159, val_acc:0.928]
Epoch [65/120    avg_loss:0.178, val_acc:0.953]
Epoch [66/120    avg_loss:0.211, val_acc:0.951]
Epoch [67/120    avg_loss:0.178, val_acc:0.914]
Epoch [68/120    avg_loss:0.177, val_acc:0.936]
Epoch [69/120    avg_loss:0.175, val_acc:0.955]
Epoch [70/120    avg_loss:0.126, val_acc:0.951]
Epoch [71/120    avg_loss:0.133, val_acc:0.957]
Epoch [72/120    avg_loss:0.111, val_acc:0.957]
Epoch [73/120    avg_loss:0.140, val_acc:0.955]
Epoch [74/120    avg_loss:0.115, val_acc:0.965]
Epoch [75/120    avg_loss:0.075, val_acc:0.965]
Epoch [76/120    avg_loss:0.094, val_acc:0.965]
Epoch [77/120    avg_loss:0.095, val_acc:0.965]
Epoch [78/120    avg_loss:0.090, val_acc:0.969]
Epoch [79/120    avg_loss:0.074, val_acc:0.969]
Epoch [80/120    avg_loss:0.084, val_acc:0.971]
Epoch [81/120    avg_loss:0.085, val_acc:0.975]
Epoch [82/120    avg_loss:0.093, val_acc:0.973]
Epoch [83/120    avg_loss:0.077, val_acc:0.975]
Epoch [84/120    avg_loss:0.076, val_acc:0.973]
Epoch [85/120    avg_loss:0.079, val_acc:0.977]
Epoch [86/120    avg_loss:0.074, val_acc:0.975]
Epoch [87/120    avg_loss:0.064, val_acc:0.975]
Epoch [88/120    avg_loss:0.073, val_acc:0.975]
Epoch [89/120    avg_loss:0.064, val_acc:0.975]
Epoch [90/120    avg_loss:0.067, val_acc:0.977]
Epoch [91/120    avg_loss:0.064, val_acc:0.975]
Epoch [92/120    avg_loss:0.069, val_acc:0.973]
Epoch [93/120    avg_loss:0.063, val_acc:0.977]
Epoch [94/120    avg_loss:0.067, val_acc:0.975]
Epoch [95/120    avg_loss:0.052, val_acc:0.975]
Epoch [96/120    avg_loss:0.097, val_acc:0.975]
Epoch [97/120    avg_loss:0.064, val_acc:0.975]
Epoch [98/120    avg_loss:0.081, val_acc:0.973]
Epoch [99/120    avg_loss:0.063, val_acc:0.969]
Epoch [100/120    avg_loss:0.063, val_acc:0.977]
Epoch [101/120    avg_loss:0.055, val_acc:0.973]
Epoch [102/120    avg_loss:0.061, val_acc:0.975]
Epoch [103/120    avg_loss:0.077, val_acc:0.973]
Epoch [104/120    avg_loss:0.053, val_acc:0.973]
Epoch [105/120    avg_loss:0.055, val_acc:0.977]
Epoch [106/120    avg_loss:0.062, val_acc:0.975]
Epoch [107/120    avg_loss:0.062, val_acc:0.977]
Epoch [108/120    avg_loss:0.060, val_acc:0.977]
Epoch [109/120    avg_loss:0.067, val_acc:0.977]
Epoch [110/120    avg_loss:0.055, val_acc:0.979]
Epoch [111/120    avg_loss:0.061, val_acc:0.977]
Epoch [112/120    avg_loss:0.062, val_acc:0.975]
Epoch [113/120    avg_loss:0.047, val_acc:0.975]
Epoch [114/120    avg_loss:0.062, val_acc:0.977]
Epoch [115/120    avg_loss:0.048, val_acc:0.979]
Epoch [116/120    avg_loss:0.057, val_acc:0.977]
Epoch [117/120    avg_loss:0.060, val_acc:0.977]
Epoch [118/120    avg_loss:0.070, val_acc:0.979]
Epoch [119/120    avg_loss:0.056, val_acc:0.977]
Epoch [120/120    avg_loss:0.047, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 214  16   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.37953091684435

F1 scores:
[       nan 1.         0.96674058 0.96396396 0.86993603 0.84931507
 0.99756691 0.91428571 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9819576999074094
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffa65cd2780>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.550, val_acc:0.377]
Epoch [2/120    avg_loss:2.331, val_acc:0.398]
Epoch [3/120    avg_loss:2.189, val_acc:0.440]
Epoch [4/120    avg_loss:2.050, val_acc:0.550]
Epoch [5/120    avg_loss:1.939, val_acc:0.571]
Epoch [6/120    avg_loss:1.811, val_acc:0.577]
Epoch [7/120    avg_loss:1.697, val_acc:0.610]
Epoch [8/120    avg_loss:1.589, val_acc:0.627]
Epoch [9/120    avg_loss:1.492, val_acc:0.665]
Epoch [10/120    avg_loss:1.355, val_acc:0.696]
Epoch [11/120    avg_loss:1.270, val_acc:0.725]
Epoch [12/120    avg_loss:1.140, val_acc:0.829]
Epoch [13/120    avg_loss:1.053, val_acc:0.800]
Epoch [14/120    avg_loss:1.003, val_acc:0.852]
Epoch [15/120    avg_loss:0.884, val_acc:0.840]
Epoch [16/120    avg_loss:0.798, val_acc:0.875]
Epoch [17/120    avg_loss:0.759, val_acc:0.869]
Epoch [18/120    avg_loss:0.675, val_acc:0.869]
Epoch [19/120    avg_loss:0.626, val_acc:0.894]
Epoch [20/120    avg_loss:0.557, val_acc:0.902]
Epoch [21/120    avg_loss:0.527, val_acc:0.894]
Epoch [22/120    avg_loss:0.579, val_acc:0.894]
Epoch [23/120    avg_loss:0.541, val_acc:0.917]
Epoch [24/120    avg_loss:0.493, val_acc:0.892]
Epoch [25/120    avg_loss:0.565, val_acc:0.900]
Epoch [26/120    avg_loss:0.478, val_acc:0.912]
Epoch [27/120    avg_loss:0.445, val_acc:0.898]
Epoch [28/120    avg_loss:0.437, val_acc:0.910]
Epoch [29/120    avg_loss:0.397, val_acc:0.933]
Epoch [30/120    avg_loss:0.370, val_acc:0.925]
Epoch [31/120    avg_loss:0.338, val_acc:0.925]
Epoch [32/120    avg_loss:0.343, val_acc:0.935]
Epoch [33/120    avg_loss:0.331, val_acc:0.912]
Epoch [34/120    avg_loss:0.334, val_acc:0.935]
Epoch [35/120    avg_loss:0.317, val_acc:0.933]
Epoch [36/120    avg_loss:0.368, val_acc:0.900]
Epoch [37/120    avg_loss:0.366, val_acc:0.938]
Epoch [38/120    avg_loss:0.297, val_acc:0.910]
Epoch [39/120    avg_loss:0.340, val_acc:0.946]
Epoch [40/120    avg_loss:0.283, val_acc:0.933]
Epoch [41/120    avg_loss:0.292, val_acc:0.933]
Epoch [42/120    avg_loss:0.311, val_acc:0.935]
Epoch [43/120    avg_loss:0.282, val_acc:0.935]
Epoch [44/120    avg_loss:0.266, val_acc:0.938]
Epoch [45/120    avg_loss:0.311, val_acc:0.925]
Epoch [46/120    avg_loss:0.275, val_acc:0.927]
Epoch [47/120    avg_loss:0.300, val_acc:0.935]
Epoch [48/120    avg_loss:0.260, val_acc:0.921]
Epoch [49/120    avg_loss:0.251, val_acc:0.965]
Epoch [50/120    avg_loss:0.215, val_acc:0.931]
Epoch [51/120    avg_loss:0.243, val_acc:0.906]
Epoch [52/120    avg_loss:0.273, val_acc:0.935]
Epoch [53/120    avg_loss:0.243, val_acc:0.956]
Epoch [54/120    avg_loss:0.214, val_acc:0.946]
Epoch [55/120    avg_loss:0.205, val_acc:0.952]
Epoch [56/120    avg_loss:0.169, val_acc:0.967]
Epoch [57/120    avg_loss:0.161, val_acc:0.960]
Epoch [58/120    avg_loss:0.171, val_acc:0.950]
Epoch [59/120    avg_loss:0.154, val_acc:0.967]
Epoch [60/120    avg_loss:0.179, val_acc:0.969]
Epoch [61/120    avg_loss:0.160, val_acc:0.965]
Epoch [62/120    avg_loss:0.176, val_acc:0.948]
Epoch [63/120    avg_loss:0.173, val_acc:0.967]
Epoch [64/120    avg_loss:0.167, val_acc:0.950]
Epoch [65/120    avg_loss:0.204, val_acc:0.952]
Epoch [66/120    avg_loss:0.208, val_acc:0.975]
Epoch [67/120    avg_loss:0.194, val_acc:0.952]
Epoch [68/120    avg_loss:0.148, val_acc:0.958]
Epoch [69/120    avg_loss:0.163, val_acc:0.969]
Epoch [70/120    avg_loss:0.161, val_acc:0.975]
Epoch [71/120    avg_loss:0.151, val_acc:0.965]
Epoch [72/120    avg_loss:0.123, val_acc:0.979]
Epoch [73/120    avg_loss:0.152, val_acc:0.969]
Epoch [74/120    avg_loss:0.147, val_acc:0.983]
Epoch [75/120    avg_loss:0.110, val_acc:0.967]
Epoch [76/120    avg_loss:0.133, val_acc:0.967]
Epoch [77/120    avg_loss:0.143, val_acc:0.969]
Epoch [78/120    avg_loss:0.124, val_acc:0.985]
Epoch [79/120    avg_loss:0.104, val_acc:0.971]
Epoch [80/120    avg_loss:0.124, val_acc:0.954]
Epoch [81/120    avg_loss:0.103, val_acc:0.977]
Epoch [82/120    avg_loss:0.134, val_acc:0.965]
Epoch [83/120    avg_loss:0.114, val_acc:0.979]
Epoch [84/120    avg_loss:0.078, val_acc:0.973]
Epoch [85/120    avg_loss:0.094, val_acc:0.977]
Epoch [86/120    avg_loss:0.090, val_acc:0.979]
Epoch [87/120    avg_loss:0.090, val_acc:0.969]
Epoch [88/120    avg_loss:0.150, val_acc:0.975]
Epoch [89/120    avg_loss:0.109, val_acc:0.963]
Epoch [90/120    avg_loss:0.089, val_acc:0.977]
Epoch [91/120    avg_loss:0.081, val_acc:0.981]
Epoch [92/120    avg_loss:0.057, val_acc:0.988]
Epoch [93/120    avg_loss:0.048, val_acc:0.988]
Epoch [94/120    avg_loss:0.053, val_acc:0.988]
Epoch [95/120    avg_loss:0.058, val_acc:0.988]
Epoch [96/120    avg_loss:0.053, val_acc:0.988]
Epoch [97/120    avg_loss:0.053, val_acc:0.988]
Epoch [98/120    avg_loss:0.048, val_acc:0.985]
Epoch [99/120    avg_loss:0.065, val_acc:0.981]
Epoch [100/120    avg_loss:0.065, val_acc:0.979]
Epoch [101/120    avg_loss:0.047, val_acc:0.983]
Epoch [102/120    avg_loss:0.046, val_acc:0.985]
Epoch [103/120    avg_loss:0.047, val_acc:0.983]
Epoch [104/120    avg_loss:0.044, val_acc:0.988]
Epoch [105/120    avg_loss:0.033, val_acc:0.985]
Epoch [106/120    avg_loss:0.049, val_acc:0.988]
Epoch [107/120    avg_loss:0.065, val_acc:0.988]
Epoch [108/120    avg_loss:0.048, val_acc:0.990]
Epoch [109/120    avg_loss:0.043, val_acc:0.992]
Epoch [110/120    avg_loss:0.040, val_acc:0.985]
Epoch [111/120    avg_loss:0.051, val_acc:0.985]
Epoch [112/120    avg_loss:0.045, val_acc:0.985]
Epoch [113/120    avg_loss:0.043, val_acc:0.983]
Epoch [114/120    avg_loss:0.042, val_acc:0.988]
Epoch [115/120    avg_loss:0.044, val_acc:0.992]
Epoch [116/120    avg_loss:0.055, val_acc:0.992]
Epoch [117/120    avg_loss:0.045, val_acc:0.988]
Epoch [118/120    avg_loss:0.046, val_acc:0.983]
Epoch [119/120    avg_loss:0.042, val_acc:0.983]
Epoch [120/120    avg_loss:0.041, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   1 226   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   2   1 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  10 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.93390191897655

F1 scores:
[       nan 1.         0.96818182 0.98689956 0.94520548 0.92508143
 1.         0.93048128 1.         1.         1.         0.98691099
 0.98883929 1.        ]

Kappa:
0.9881321544573021
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1ce43ab908>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.557, val_acc:0.312]
Epoch [2/120    avg_loss:2.340, val_acc:0.412]
Epoch [3/120    avg_loss:2.137, val_acc:0.475]
Epoch [4/120    avg_loss:1.991, val_acc:0.586]
Epoch [5/120    avg_loss:1.840, val_acc:0.680]
Epoch [6/120    avg_loss:1.718, val_acc:0.668]
Epoch [7/120    avg_loss:1.571, val_acc:0.678]
Epoch [8/120    avg_loss:1.469, val_acc:0.699]
Epoch [9/120    avg_loss:1.367, val_acc:0.701]
Epoch [10/120    avg_loss:1.231, val_acc:0.734]
Epoch [11/120    avg_loss:1.143, val_acc:0.758]
Epoch [12/120    avg_loss:1.054, val_acc:0.744]
Epoch [13/120    avg_loss:0.959, val_acc:0.777]
Epoch [14/120    avg_loss:0.922, val_acc:0.836]
Epoch [15/120    avg_loss:0.874, val_acc:0.861]
Epoch [16/120    avg_loss:0.792, val_acc:0.877]
Epoch [17/120    avg_loss:0.688, val_acc:0.814]
Epoch [18/120    avg_loss:0.742, val_acc:0.840]
Epoch [19/120    avg_loss:0.642, val_acc:0.902]
Epoch [20/120    avg_loss:0.575, val_acc:0.912]
Epoch [21/120    avg_loss:0.549, val_acc:0.902]
Epoch [22/120    avg_loss:0.528, val_acc:0.928]
Epoch [23/120    avg_loss:0.472, val_acc:0.945]
Epoch [24/120    avg_loss:0.458, val_acc:0.936]
Epoch [25/120    avg_loss:0.427, val_acc:0.936]
Epoch [26/120    avg_loss:0.443, val_acc:0.912]
Epoch [27/120    avg_loss:0.437, val_acc:0.926]
Epoch [28/120    avg_loss:0.417, val_acc:0.928]
Epoch [29/120    avg_loss:0.361, val_acc:0.947]
Epoch [30/120    avg_loss:0.365, val_acc:0.959]
Epoch [31/120    avg_loss:0.407, val_acc:0.934]
Epoch [32/120    avg_loss:0.407, val_acc:0.916]
Epoch [33/120    avg_loss:0.357, val_acc:0.957]
Epoch [34/120    avg_loss:0.328, val_acc:0.938]
Epoch [35/120    avg_loss:0.343, val_acc:0.924]
Epoch [36/120    avg_loss:0.284, val_acc:0.957]
Epoch [37/120    avg_loss:0.291, val_acc:0.955]
Epoch [38/120    avg_loss:0.265, val_acc:0.963]
Epoch [39/120    avg_loss:0.268, val_acc:0.965]
Epoch [40/120    avg_loss:0.232, val_acc:0.939]
Epoch [41/120    avg_loss:0.251, val_acc:0.945]
Epoch [42/120    avg_loss:0.278, val_acc:0.963]
Epoch [43/120    avg_loss:0.308, val_acc:0.963]
Epoch [44/120    avg_loss:0.327, val_acc:0.963]
Epoch [45/120    avg_loss:0.198, val_acc:0.965]
Epoch [46/120    avg_loss:0.218, val_acc:0.959]
Epoch [47/120    avg_loss:0.225, val_acc:0.961]
Epoch [48/120    avg_loss:0.218, val_acc:0.969]
Epoch [49/120    avg_loss:0.228, val_acc:0.967]
Epoch [50/120    avg_loss:0.281, val_acc:0.967]
Epoch [51/120    avg_loss:0.200, val_acc:0.953]
Epoch [52/120    avg_loss:0.185, val_acc:0.969]
Epoch [53/120    avg_loss:0.163, val_acc:0.957]
Epoch [54/120    avg_loss:0.149, val_acc:0.973]
Epoch [55/120    avg_loss:0.149, val_acc:0.971]
Epoch [56/120    avg_loss:0.155, val_acc:0.969]
Epoch [57/120    avg_loss:0.189, val_acc:0.947]
Epoch [58/120    avg_loss:0.205, val_acc:0.959]
Epoch [59/120    avg_loss:0.163, val_acc:0.963]
Epoch [60/120    avg_loss:0.145, val_acc:0.980]
Epoch [61/120    avg_loss:0.182, val_acc:0.965]
Epoch [62/120    avg_loss:0.218, val_acc:0.965]
Epoch [63/120    avg_loss:0.166, val_acc:0.971]
Epoch [64/120    avg_loss:0.144, val_acc:0.967]
Epoch [65/120    avg_loss:0.168, val_acc:0.967]
Epoch [66/120    avg_loss:0.164, val_acc:0.965]
Epoch [67/120    avg_loss:0.116, val_acc:0.969]
Epoch [68/120    avg_loss:0.125, val_acc:0.969]
Epoch [69/120    avg_loss:0.120, val_acc:0.977]
Epoch [70/120    avg_loss:0.116, val_acc:0.975]
Epoch [71/120    avg_loss:0.098, val_acc:0.982]
Epoch [72/120    avg_loss:0.108, val_acc:0.982]
Epoch [73/120    avg_loss:0.090, val_acc:0.975]
Epoch [74/120    avg_loss:0.125, val_acc:0.973]
Epoch [75/120    avg_loss:0.096, val_acc:0.973]
Epoch [76/120    avg_loss:0.089, val_acc:0.965]
Epoch [77/120    avg_loss:0.100, val_acc:0.971]
Epoch [78/120    avg_loss:0.092, val_acc:0.975]
Epoch [79/120    avg_loss:0.115, val_acc:0.973]
Epoch [80/120    avg_loss:0.114, val_acc:0.982]
Epoch [81/120    avg_loss:0.099, val_acc:0.988]
Epoch [82/120    avg_loss:0.081, val_acc:0.990]
Epoch [83/120    avg_loss:0.069, val_acc:0.982]
Epoch [84/120    avg_loss:0.076, val_acc:0.988]
Epoch [85/120    avg_loss:0.071, val_acc:0.971]
Epoch [86/120    avg_loss:0.072, val_acc:0.982]
Epoch [87/120    avg_loss:0.066, val_acc:0.977]
Epoch [88/120    avg_loss:0.068, val_acc:0.986]
Epoch [89/120    avg_loss:0.064, val_acc:0.982]
Epoch [90/120    avg_loss:0.086, val_acc:0.979]
Epoch [91/120    avg_loss:0.085, val_acc:0.980]
Epoch [92/120    avg_loss:0.075, val_acc:0.984]
Epoch [93/120    avg_loss:0.066, val_acc:0.977]
Epoch [94/120    avg_loss:0.066, val_acc:0.969]
Epoch [95/120    avg_loss:0.079, val_acc:0.979]
Epoch [96/120    avg_loss:0.057, val_acc:0.980]
Epoch [97/120    avg_loss:0.042, val_acc:0.980]
Epoch [98/120    avg_loss:0.037, val_acc:0.980]
Epoch [99/120    avg_loss:0.037, val_acc:0.979]
Epoch [100/120    avg_loss:0.030, val_acc:0.982]
Epoch [101/120    avg_loss:0.036, val_acc:0.980]
Epoch [102/120    avg_loss:0.034, val_acc:0.980]
Epoch [103/120    avg_loss:0.037, val_acc:0.988]
Epoch [104/120    avg_loss:0.029, val_acc:0.984]
Epoch [105/120    avg_loss:0.034, val_acc:0.984]
Epoch [106/120    avg_loss:0.031, val_acc:0.980]
Epoch [107/120    avg_loss:0.025, val_acc:0.980]
Epoch [108/120    avg_loss:0.038, val_acc:0.984]
Epoch [109/120    avg_loss:0.023, val_acc:0.984]
Epoch [110/120    avg_loss:0.028, val_acc:0.984]
Epoch [111/120    avg_loss:0.040, val_acc:0.984]
Epoch [112/120    avg_loss:0.027, val_acc:0.984]
Epoch [113/120    avg_loss:0.029, val_acc:0.984]
Epoch [114/120    avg_loss:0.029, val_acc:0.982]
Epoch [115/120    avg_loss:0.028, val_acc:0.984]
Epoch [116/120    avg_loss:0.029, val_acc:0.984]
Epoch [117/120    avg_loss:0.033, val_acc:0.982]
Epoch [118/120    avg_loss:0.032, val_acc:0.982]
Epoch [119/120    avg_loss:0.032, val_acc:0.982]
Epoch [120/120    avg_loss:0.032, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   7   0   0   0   0   0   0]
 [  0   0   0 214  11   0   0   0   5   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.74200426439232

F1 scores:
[       nan 1.         0.96363636 0.96396396 0.91810345 0.90721649
 1.         0.91397849 0.99359795 1.         1.         1.
 1.         1.        ]

Kappa:
0.9859938384227213
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe3e2d0a940>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.594, val_acc:0.315]
Epoch [2/120    avg_loss:2.376, val_acc:0.346]
Epoch [3/120    avg_loss:2.224, val_acc:0.469]
Epoch [4/120    avg_loss:2.114, val_acc:0.596]
Epoch [5/120    avg_loss:1.995, val_acc:0.619]
Epoch [6/120    avg_loss:1.871, val_acc:0.640]
Epoch [7/120    avg_loss:1.747, val_acc:0.640]
Epoch [8/120    avg_loss:1.592, val_acc:0.644]
Epoch [9/120    avg_loss:1.470, val_acc:0.727]
Epoch [10/120    avg_loss:1.377, val_acc:0.760]
Epoch [11/120    avg_loss:1.244, val_acc:0.754]
Epoch [12/120    avg_loss:1.169, val_acc:0.783]
Epoch [13/120    avg_loss:1.081, val_acc:0.773]
Epoch [14/120    avg_loss:0.982, val_acc:0.829]
Epoch [15/120    avg_loss:0.959, val_acc:0.840]
Epoch [16/120    avg_loss:0.867, val_acc:0.858]
Epoch [17/120    avg_loss:0.823, val_acc:0.867]
Epoch [18/120    avg_loss:0.769, val_acc:0.887]
Epoch [19/120    avg_loss:0.723, val_acc:0.856]
Epoch [20/120    avg_loss:0.671, val_acc:0.908]
Epoch [21/120    avg_loss:0.618, val_acc:0.898]
Epoch [22/120    avg_loss:0.574, val_acc:0.921]
Epoch [23/120    avg_loss:0.521, val_acc:0.927]
Epoch [24/120    avg_loss:0.496, val_acc:0.931]
Epoch [25/120    avg_loss:0.511, val_acc:0.896]
Epoch [26/120    avg_loss:0.487, val_acc:0.894]
Epoch [27/120    avg_loss:0.456, val_acc:0.942]
Epoch [28/120    avg_loss:0.515, val_acc:0.925]
Epoch [29/120    avg_loss:0.417, val_acc:0.908]
Epoch [30/120    avg_loss:0.377, val_acc:0.925]
Epoch [31/120    avg_loss:0.439, val_acc:0.935]
Epoch [32/120    avg_loss:0.367, val_acc:0.912]
Epoch [33/120    avg_loss:0.378, val_acc:0.925]
Epoch [34/120    avg_loss:0.341, val_acc:0.923]
Epoch [35/120    avg_loss:0.327, val_acc:0.940]
Epoch [36/120    avg_loss:0.315, val_acc:0.919]
Epoch [37/120    avg_loss:0.324, val_acc:0.952]
Epoch [38/120    avg_loss:0.285, val_acc:0.946]
Epoch [39/120    avg_loss:0.293, val_acc:0.933]
Epoch [40/120    avg_loss:0.341, val_acc:0.925]
Epoch [41/120    avg_loss:0.302, val_acc:0.954]
Epoch [42/120    avg_loss:0.253, val_acc:0.929]
Epoch [43/120    avg_loss:0.273, val_acc:0.935]
Epoch [44/120    avg_loss:0.252, val_acc:0.960]
Epoch [45/120    avg_loss:0.243, val_acc:0.944]
Epoch [46/120    avg_loss:0.300, val_acc:0.952]
Epoch [47/120    avg_loss:0.285, val_acc:0.969]
Epoch [48/120    avg_loss:0.248, val_acc:0.958]
Epoch [49/120    avg_loss:0.231, val_acc:0.967]
Epoch [50/120    avg_loss:0.196, val_acc:0.948]
Epoch [51/120    avg_loss:0.196, val_acc:0.960]
Epoch [52/120    avg_loss:0.179, val_acc:0.958]
Epoch [53/120    avg_loss:0.192, val_acc:0.965]
Epoch [54/120    avg_loss:0.127, val_acc:0.973]
Epoch [55/120    avg_loss:0.149, val_acc:0.965]
Epoch [56/120    avg_loss:0.219, val_acc:0.948]
Epoch [57/120    avg_loss:0.203, val_acc:0.965]
Epoch [58/120    avg_loss:0.197, val_acc:0.960]
Epoch [59/120    avg_loss:0.173, val_acc:0.977]
Epoch [60/120    avg_loss:0.149, val_acc:0.967]
Epoch [61/120    avg_loss:0.118, val_acc:0.977]
Epoch [62/120    avg_loss:0.172, val_acc:0.946]
Epoch [63/120    avg_loss:0.133, val_acc:0.965]
Epoch [64/120    avg_loss:0.105, val_acc:0.983]
Epoch [65/120    avg_loss:0.096, val_acc:0.981]
Epoch [66/120    avg_loss:0.077, val_acc:0.988]
Epoch [67/120    avg_loss:0.132, val_acc:0.948]
Epoch [68/120    avg_loss:0.103, val_acc:0.981]
Epoch [69/120    avg_loss:0.096, val_acc:0.975]
Epoch [70/120    avg_loss:0.118, val_acc:0.973]
Epoch [71/120    avg_loss:0.111, val_acc:0.971]
Epoch [72/120    avg_loss:0.105, val_acc:0.975]
Epoch [73/120    avg_loss:0.083, val_acc:0.975]
Epoch [74/120    avg_loss:0.088, val_acc:0.967]
Epoch [75/120    avg_loss:0.073, val_acc:0.985]
Epoch [76/120    avg_loss:0.097, val_acc:0.971]
Epoch [77/120    avg_loss:0.087, val_acc:0.990]
Epoch [78/120    avg_loss:0.073, val_acc:0.977]
Epoch [79/120    avg_loss:0.069, val_acc:0.988]
Epoch [80/120    avg_loss:0.050, val_acc:0.985]
Epoch [81/120    avg_loss:0.059, val_acc:0.979]
Epoch [82/120    avg_loss:0.076, val_acc:0.981]
Epoch [83/120    avg_loss:0.110, val_acc:0.981]
Epoch [84/120    avg_loss:0.078, val_acc:0.981]
Epoch [85/120    avg_loss:0.092, val_acc:0.983]
Epoch [86/120    avg_loss:0.093, val_acc:0.981]
Epoch [87/120    avg_loss:0.109, val_acc:0.992]
Epoch [88/120    avg_loss:0.099, val_acc:0.979]
Epoch [89/120    avg_loss:0.062, val_acc:0.992]
Epoch [90/120    avg_loss:0.047, val_acc:0.990]
Epoch [91/120    avg_loss:0.040, val_acc:0.992]
Epoch [92/120    avg_loss:0.054, val_acc:0.988]
Epoch [93/120    avg_loss:0.047, val_acc:0.990]
Epoch [94/120    avg_loss:0.042, val_acc:0.990]
Epoch [95/120    avg_loss:0.036, val_acc:0.985]
Epoch [96/120    avg_loss:0.060, val_acc:0.985]
Epoch [97/120    avg_loss:0.051, val_acc:0.990]
Epoch [98/120    avg_loss:0.032, val_acc:0.985]
Epoch [99/120    avg_loss:0.067, val_acc:0.963]
Epoch [100/120    avg_loss:0.051, val_acc:0.992]
Epoch [101/120    avg_loss:0.042, val_acc:0.985]
Epoch [102/120    avg_loss:0.032, val_acc:0.990]
Epoch [103/120    avg_loss:0.040, val_acc:0.977]
Epoch [104/120    avg_loss:0.134, val_acc:0.969]
Epoch [105/120    avg_loss:0.158, val_acc:0.977]
Epoch [106/120    avg_loss:0.136, val_acc:0.960]
Epoch [107/120    avg_loss:0.100, val_acc:0.973]
Epoch [108/120    avg_loss:0.066, val_acc:0.985]
Epoch [109/120    avg_loss:0.043, val_acc:0.985]
Epoch [110/120    avg_loss:0.049, val_acc:0.985]
Epoch [111/120    avg_loss:0.052, val_acc:0.971]
Epoch [112/120    avg_loss:0.095, val_acc:0.960]
Epoch [113/120    avg_loss:0.057, val_acc:0.981]
Epoch [114/120    avg_loss:0.040, val_acc:0.979]
Epoch [115/120    avg_loss:0.034, val_acc:0.983]
Epoch [116/120    avg_loss:0.036, val_acc:0.988]
Epoch [117/120    avg_loss:0.029, val_acc:0.988]
Epoch [118/120    avg_loss:0.033, val_acc:0.985]
Epoch [119/120    avg_loss:0.030, val_acc:0.985]
Epoch [120/120    avg_loss:0.030, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   1 214   9   0   0   0   3   3   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 1.         0.99316629 0.96396396 0.92505353 0.90909091
 1.         0.9893617  0.99614891 0.99680511 1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9893169942653443
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fddbe8c4908>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.558, val_acc:0.300]
Epoch [2/120    avg_loss:2.378, val_acc:0.396]
Epoch [3/120    avg_loss:2.247, val_acc:0.452]
Epoch [4/120    avg_loss:2.143, val_acc:0.523]
Epoch [5/120    avg_loss:2.019, val_acc:0.571]
Epoch [6/120    avg_loss:1.919, val_acc:0.577]
Epoch [7/120    avg_loss:1.796, val_acc:0.598]
Epoch [8/120    avg_loss:1.694, val_acc:0.604]
Epoch [9/120    avg_loss:1.588, val_acc:0.650]
Epoch [10/120    avg_loss:1.443, val_acc:0.708]
Epoch [11/120    avg_loss:1.332, val_acc:0.698]
Epoch [12/120    avg_loss:1.221, val_acc:0.727]
Epoch [13/120    avg_loss:1.130, val_acc:0.756]
Epoch [14/120    avg_loss:1.033, val_acc:0.815]
Epoch [15/120    avg_loss:0.935, val_acc:0.863]
Epoch [16/120    avg_loss:0.866, val_acc:0.840]
Epoch [17/120    avg_loss:0.798, val_acc:0.867]
Epoch [18/120    avg_loss:0.712, val_acc:0.892]
Epoch [19/120    avg_loss:0.652, val_acc:0.892]
Epoch [20/120    avg_loss:0.634, val_acc:0.917]
Epoch [21/120    avg_loss:0.611, val_acc:0.925]
Epoch [22/120    avg_loss:0.545, val_acc:0.896]
Epoch [23/120    avg_loss:0.546, val_acc:0.908]
Epoch [24/120    avg_loss:0.501, val_acc:0.904]
Epoch [25/120    avg_loss:0.481, val_acc:0.931]
Epoch [26/120    avg_loss:0.444, val_acc:0.935]
Epoch [27/120    avg_loss:0.465, val_acc:0.898]
Epoch [28/120    avg_loss:0.486, val_acc:0.946]
Epoch [29/120    avg_loss:0.394, val_acc:0.925]
Epoch [30/120    avg_loss:0.346, val_acc:0.938]
Epoch [31/120    avg_loss:0.373, val_acc:0.944]
Epoch [32/120    avg_loss:0.361, val_acc:0.919]
Epoch [33/120    avg_loss:0.395, val_acc:0.919]
Epoch [34/120    avg_loss:0.389, val_acc:0.925]
Epoch [35/120    avg_loss:0.360, val_acc:0.927]
Epoch [36/120    avg_loss:0.344, val_acc:0.933]
Epoch [37/120    avg_loss:0.330, val_acc:0.952]
Epoch [38/120    avg_loss:0.324, val_acc:0.933]
Epoch [39/120    avg_loss:0.287, val_acc:0.919]
Epoch [40/120    avg_loss:0.295, val_acc:0.956]
Epoch [41/120    avg_loss:0.278, val_acc:0.954]
Epoch [42/120    avg_loss:0.248, val_acc:0.963]
Epoch [43/120    avg_loss:0.208, val_acc:0.944]
Epoch [44/120    avg_loss:0.205, val_acc:0.958]
Epoch [45/120    avg_loss:0.336, val_acc:0.902]
Epoch [46/120    avg_loss:0.358, val_acc:0.904]
Epoch [47/120    avg_loss:0.269, val_acc:0.948]
Epoch [48/120    avg_loss:0.247, val_acc:0.940]
Epoch [49/120    avg_loss:0.289, val_acc:0.912]
Epoch [50/120    avg_loss:0.215, val_acc:0.946]
Epoch [51/120    avg_loss:0.240, val_acc:0.940]
Epoch [52/120    avg_loss:0.259, val_acc:0.963]
Epoch [53/120    avg_loss:0.270, val_acc:0.931]
Epoch [54/120    avg_loss:0.222, val_acc:0.948]
Epoch [55/120    avg_loss:0.200, val_acc:0.960]
Epoch [56/120    avg_loss:0.231, val_acc:0.965]
Epoch [57/120    avg_loss:0.187, val_acc:0.944]
Epoch [58/120    avg_loss:0.142, val_acc:0.973]
Epoch [59/120    avg_loss:0.174, val_acc:0.948]
Epoch [60/120    avg_loss:0.153, val_acc:0.971]
Epoch [61/120    avg_loss:0.146, val_acc:0.971]
Epoch [62/120    avg_loss:0.162, val_acc:0.969]
Epoch [63/120    avg_loss:0.151, val_acc:0.973]
Epoch [64/120    avg_loss:0.142, val_acc:0.952]
Epoch [65/120    avg_loss:0.128, val_acc:0.983]
Epoch [66/120    avg_loss:0.160, val_acc:0.933]
Epoch [67/120    avg_loss:0.161, val_acc:0.963]
Epoch [68/120    avg_loss:0.163, val_acc:0.967]
Epoch [69/120    avg_loss:0.204, val_acc:0.965]
Epoch [70/120    avg_loss:0.163, val_acc:0.971]
Epoch [71/120    avg_loss:0.153, val_acc:0.969]
Epoch [72/120    avg_loss:0.116, val_acc:0.975]
Epoch [73/120    avg_loss:0.114, val_acc:0.977]
Epoch [74/120    avg_loss:0.094, val_acc:0.977]
Epoch [75/120    avg_loss:0.128, val_acc:0.948]
Epoch [76/120    avg_loss:0.227, val_acc:0.965]
Epoch [77/120    avg_loss:0.136, val_acc:0.969]
Epoch [78/120    avg_loss:0.121, val_acc:0.975]
Epoch [79/120    avg_loss:0.088, val_acc:0.977]
Epoch [80/120    avg_loss:0.076, val_acc:0.977]
Epoch [81/120    avg_loss:0.073, val_acc:0.981]
Epoch [82/120    avg_loss:0.078, val_acc:0.979]
Epoch [83/120    avg_loss:0.095, val_acc:0.979]
Epoch [84/120    avg_loss:0.064, val_acc:0.983]
Epoch [85/120    avg_loss:0.073, val_acc:0.981]
Epoch [86/120    avg_loss:0.092, val_acc:0.979]
Epoch [87/120    avg_loss:0.073, val_acc:0.979]
Epoch [88/120    avg_loss:0.071, val_acc:0.979]
Epoch [89/120    avg_loss:0.067, val_acc:0.981]
Epoch [90/120    avg_loss:0.064, val_acc:0.983]
Epoch [91/120    avg_loss:0.067, val_acc:0.983]
Epoch [92/120    avg_loss:0.070, val_acc:0.983]
Epoch [93/120    avg_loss:0.081, val_acc:0.983]
Epoch [94/120    avg_loss:0.054, val_acc:0.983]
Epoch [95/120    avg_loss:0.061, val_acc:0.983]
Epoch [96/120    avg_loss:0.061, val_acc:0.983]
Epoch [97/120    avg_loss:0.066, val_acc:0.983]
Epoch [98/120    avg_loss:0.059, val_acc:0.983]
Epoch [99/120    avg_loss:0.058, val_acc:0.983]
Epoch [100/120    avg_loss:0.066, val_acc:0.983]
Epoch [101/120    avg_loss:0.057, val_acc:0.983]
Epoch [102/120    avg_loss:0.070, val_acc:0.983]
Epoch [103/120    avg_loss:0.055, val_acc:0.981]
Epoch [104/120    avg_loss:0.055, val_acc:0.983]
Epoch [105/120    avg_loss:0.057, val_acc:0.983]
Epoch [106/120    avg_loss:0.056, val_acc:0.983]
Epoch [107/120    avg_loss:0.069, val_acc:0.983]
Epoch [108/120    avg_loss:0.060, val_acc:0.983]
Epoch [109/120    avg_loss:0.055, val_acc:0.983]
Epoch [110/120    avg_loss:0.053, val_acc:0.983]
Epoch [111/120    avg_loss:0.059, val_acc:0.985]
Epoch [112/120    avg_loss:0.069, val_acc:0.985]
Epoch [113/120    avg_loss:0.061, val_acc:0.983]
Epoch [114/120    avg_loss:0.058, val_acc:0.983]
Epoch [115/120    avg_loss:0.051, val_acc:0.983]
Epoch [116/120    avg_loss:0.059, val_acc:0.983]
Epoch [117/120    avg_loss:0.055, val_acc:0.983]
Epoch [118/120    avg_loss:0.056, val_acc:0.983]
Epoch [119/120    avg_loss:0.056, val_acc:0.985]
Epoch [120/120    avg_loss:0.053, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 219   5   0   0   0   5   0   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7   0 199   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.63539445628997

F1 scores:
[       nan 1.         0.97333333 0.97550111 0.89760349 0.88215488
 0.98271605 0.93785311 0.99359795 1.         1.         1.
 1.         1.        ]

Kappa:
0.9848062703291304
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc339ca68d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.579, val_acc:0.348]
Epoch [2/120    avg_loss:2.317, val_acc:0.467]
Epoch [3/120    avg_loss:2.153, val_acc:0.463]
Epoch [4/120    avg_loss:2.017, val_acc:0.490]
Epoch [5/120    avg_loss:1.883, val_acc:0.523]
Epoch [6/120    avg_loss:1.768, val_acc:0.546]
Epoch [7/120    avg_loss:1.659, val_acc:0.590]
Epoch [8/120    avg_loss:1.548, val_acc:0.637]
Epoch [9/120    avg_loss:1.430, val_acc:0.671]
Epoch [10/120    avg_loss:1.332, val_acc:0.704]
Epoch [11/120    avg_loss:1.262, val_acc:0.721]
Epoch [12/120    avg_loss:1.167, val_acc:0.819]
Epoch [13/120    avg_loss:1.060, val_acc:0.840]
Epoch [14/120    avg_loss:0.981, val_acc:0.846]
Epoch [15/120    avg_loss:0.874, val_acc:0.871]
Epoch [16/120    avg_loss:0.809, val_acc:0.877]
Epoch [17/120    avg_loss:0.754, val_acc:0.894]
Epoch [18/120    avg_loss:0.682, val_acc:0.852]
Epoch [19/120    avg_loss:0.623, val_acc:0.921]
Epoch [20/120    avg_loss:0.564, val_acc:0.898]
Epoch [21/120    avg_loss:0.578, val_acc:0.900]
Epoch [22/120    avg_loss:0.517, val_acc:0.921]
Epoch [23/120    avg_loss:0.507, val_acc:0.927]
Epoch [24/120    avg_loss:0.409, val_acc:0.921]
Epoch [25/120    avg_loss:0.475, val_acc:0.935]
Epoch [26/120    avg_loss:0.413, val_acc:0.950]
Epoch [27/120    avg_loss:0.416, val_acc:0.935]
Epoch [28/120    avg_loss:0.395, val_acc:0.944]
Epoch [29/120    avg_loss:0.347, val_acc:0.935]
Epoch [30/120    avg_loss:0.413, val_acc:0.938]
Epoch [31/120    avg_loss:0.297, val_acc:0.948]
Epoch [32/120    avg_loss:0.340, val_acc:0.927]
Epoch [33/120    avg_loss:0.306, val_acc:0.950]
Epoch [34/120    avg_loss:0.348, val_acc:0.956]
Epoch [35/120    avg_loss:0.289, val_acc:0.935]
Epoch [36/120    avg_loss:0.289, val_acc:0.958]
Epoch [37/120    avg_loss:0.318, val_acc:0.952]
Epoch [38/120    avg_loss:0.288, val_acc:0.921]
Epoch [39/120    avg_loss:0.340, val_acc:0.927]
Epoch [40/120    avg_loss:0.260, val_acc:0.927]
Epoch [41/120    avg_loss:0.228, val_acc:0.944]
Epoch [42/120    avg_loss:0.263, val_acc:0.840]
Epoch [43/120    avg_loss:0.276, val_acc:0.931]
Epoch [44/120    avg_loss:0.256, val_acc:0.942]
Epoch [45/120    avg_loss:0.242, val_acc:0.950]
Epoch [46/120    avg_loss:0.239, val_acc:0.950]
Epoch [47/120    avg_loss:0.230, val_acc:0.958]
Epoch [48/120    avg_loss:0.259, val_acc:0.912]
Epoch [49/120    avg_loss:0.238, val_acc:0.956]
Epoch [50/120    avg_loss:0.235, val_acc:0.965]
Epoch [51/120    avg_loss:0.183, val_acc:0.963]
Epoch [52/120    avg_loss:0.198, val_acc:0.929]
Epoch [53/120    avg_loss:0.202, val_acc:0.960]
Epoch [54/120    avg_loss:0.154, val_acc:0.963]
Epoch [55/120    avg_loss:0.132, val_acc:0.950]
Epoch [56/120    avg_loss:0.136, val_acc:0.969]
Epoch [57/120    avg_loss:0.178, val_acc:0.958]
Epoch [58/120    avg_loss:0.157, val_acc:0.958]
Epoch [59/120    avg_loss:0.139, val_acc:0.967]
Epoch [60/120    avg_loss:0.131, val_acc:0.977]
Epoch [61/120    avg_loss:0.216, val_acc:0.904]
Epoch [62/120    avg_loss:0.257, val_acc:0.938]
Epoch [63/120    avg_loss:0.183, val_acc:0.977]
Epoch [64/120    avg_loss:0.149, val_acc:0.967]
Epoch [65/120    avg_loss:0.194, val_acc:0.952]
Epoch [66/120    avg_loss:0.186, val_acc:0.954]
Epoch [67/120    avg_loss:0.166, val_acc:0.967]
Epoch [68/120    avg_loss:0.146, val_acc:0.958]
Epoch [69/120    avg_loss:0.102, val_acc:0.971]
Epoch [70/120    avg_loss:0.137, val_acc:0.969]
Epoch [71/120    avg_loss:0.181, val_acc:0.960]
Epoch [72/120    avg_loss:0.170, val_acc:0.963]
Epoch [73/120    avg_loss:0.127, val_acc:0.963]
Epoch [74/120    avg_loss:0.154, val_acc:0.967]
Epoch [75/120    avg_loss:0.143, val_acc:0.969]
Epoch [76/120    avg_loss:0.114, val_acc:0.973]
Epoch [77/120    avg_loss:0.079, val_acc:0.975]
Epoch [78/120    avg_loss:0.085, val_acc:0.977]
Epoch [79/120    avg_loss:0.079, val_acc:0.981]
Epoch [80/120    avg_loss:0.091, val_acc:0.981]
Epoch [81/120    avg_loss:0.067, val_acc:0.981]
Epoch [82/120    avg_loss:0.069, val_acc:0.983]
Epoch [83/120    avg_loss:0.065, val_acc:0.981]
Epoch [84/120    avg_loss:0.080, val_acc:0.977]
Epoch [85/120    avg_loss:0.067, val_acc:0.977]
Epoch [86/120    avg_loss:0.080, val_acc:0.983]
Epoch [87/120    avg_loss:0.058, val_acc:0.979]
Epoch [88/120    avg_loss:0.066, val_acc:0.981]
Epoch [89/120    avg_loss:0.073, val_acc:0.981]
Epoch [90/120    avg_loss:0.060, val_acc:0.981]
Epoch [91/120    avg_loss:0.065, val_acc:0.983]
Epoch [92/120    avg_loss:0.057, val_acc:0.981]
Epoch [93/120    avg_loss:0.069, val_acc:0.981]
Epoch [94/120    avg_loss:0.056, val_acc:0.983]
Epoch [95/120    avg_loss:0.074, val_acc:0.977]
Epoch [96/120    avg_loss:0.052, val_acc:0.983]
Epoch [97/120    avg_loss:0.063, val_acc:0.981]
Epoch [98/120    avg_loss:0.058, val_acc:0.983]
Epoch [99/120    avg_loss:0.070, val_acc:0.983]
Epoch [100/120    avg_loss:0.063, val_acc:0.983]
Epoch [101/120    avg_loss:0.063, val_acc:0.983]
Epoch [102/120    avg_loss:0.057, val_acc:0.981]
Epoch [103/120    avg_loss:0.062, val_acc:0.983]
Epoch [104/120    avg_loss:0.062, val_acc:0.983]
Epoch [105/120    avg_loss:0.062, val_acc:0.981]
Epoch [106/120    avg_loss:0.068, val_acc:0.983]
Epoch [107/120    avg_loss:0.050, val_acc:0.981]
Epoch [108/120    avg_loss:0.059, val_acc:0.983]
Epoch [109/120    avg_loss:0.055, val_acc:0.983]
Epoch [110/120    avg_loss:0.054, val_acc:0.985]
Epoch [111/120    avg_loss:0.059, val_acc:0.983]
Epoch [112/120    avg_loss:0.050, val_acc:0.981]
Epoch [113/120    avg_loss:0.062, val_acc:0.981]
Epoch [114/120    avg_loss:0.053, val_acc:0.988]
Epoch [115/120    avg_loss:0.065, val_acc:0.985]
Epoch [116/120    avg_loss:0.056, val_acc:0.979]
Epoch [117/120    avg_loss:0.055, val_acc:0.983]
Epoch [118/120    avg_loss:0.053, val_acc:0.985]
Epoch [119/120    avg_loss:0.051, val_acc:0.983]
Epoch [120/120    avg_loss:0.065, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   2   0   0   0   0   4   0]
 [  0   0   0 227   0   0   0   3   0   0   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  11 442   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.7633262260128

F1 scores:
[       nan 1.         0.97038724 0.99343545 0.92970522 0.89768977
 1.         0.93548387 1.         1.         1.         0.98562092
 0.98331479 1.        ]

Kappa:
0.9862323975544259
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4cd499d860>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.556, val_acc:0.177]
Epoch [2/120    avg_loss:2.350, val_acc:0.367]
Epoch [3/120    avg_loss:2.197, val_acc:0.544]
Epoch [4/120    avg_loss:2.039, val_acc:0.588]
Epoch [5/120    avg_loss:1.917, val_acc:0.633]
Epoch [6/120    avg_loss:1.781, val_acc:0.658]
Epoch [7/120    avg_loss:1.645, val_acc:0.665]
Epoch [8/120    avg_loss:1.486, val_acc:0.679]
Epoch [9/120    avg_loss:1.389, val_acc:0.696]
Epoch [10/120    avg_loss:1.278, val_acc:0.719]
Epoch [11/120    avg_loss:1.196, val_acc:0.717]
Epoch [12/120    avg_loss:1.092, val_acc:0.790]
Epoch [13/120    avg_loss:1.007, val_acc:0.738]
Epoch [14/120    avg_loss:0.923, val_acc:0.890]
Epoch [15/120    avg_loss:0.857, val_acc:0.877]
Epoch [16/120    avg_loss:0.809, val_acc:0.898]
Epoch [17/120    avg_loss:0.840, val_acc:0.904]
Epoch [18/120    avg_loss:0.706, val_acc:0.869]
Epoch [19/120    avg_loss:0.714, val_acc:0.917]
Epoch [20/120    avg_loss:0.648, val_acc:0.917]
Epoch [21/120    avg_loss:0.589, val_acc:0.838]
Epoch [22/120    avg_loss:0.591, val_acc:0.919]
Epoch [23/120    avg_loss:0.504, val_acc:0.900]
Epoch [24/120    avg_loss:0.537, val_acc:0.925]
Epoch [25/120    avg_loss:0.465, val_acc:0.908]
Epoch [26/120    avg_loss:0.511, val_acc:0.896]
Epoch [27/120    avg_loss:0.437, val_acc:0.919]
Epoch [28/120    avg_loss:0.401, val_acc:0.921]
Epoch [29/120    avg_loss:0.416, val_acc:0.938]
Epoch [30/120    avg_loss:0.421, val_acc:0.915]
Epoch [31/120    avg_loss:0.409, val_acc:0.931]
Epoch [32/120    avg_loss:0.442, val_acc:0.925]
Epoch [33/120    avg_loss:0.370, val_acc:0.925]
Epoch [34/120    avg_loss:0.329, val_acc:0.940]
Epoch [35/120    avg_loss:0.322, val_acc:0.971]
Epoch [36/120    avg_loss:0.326, val_acc:0.948]
Epoch [37/120    avg_loss:0.348, val_acc:0.929]
Epoch [38/120    avg_loss:0.386, val_acc:0.944]
Epoch [39/120    avg_loss:0.319, val_acc:0.946]
Epoch [40/120    avg_loss:0.289, val_acc:0.960]
Epoch [41/120    avg_loss:0.287, val_acc:0.948]
Epoch [42/120    avg_loss:0.259, val_acc:0.938]
Epoch [43/120    avg_loss:0.317, val_acc:0.933]
Epoch [44/120    avg_loss:0.246, val_acc:0.952]
Epoch [45/120    avg_loss:0.276, val_acc:0.946]
Epoch [46/120    avg_loss:0.296, val_acc:0.912]
Epoch [47/120    avg_loss:0.286, val_acc:0.944]
Epoch [48/120    avg_loss:0.252, val_acc:0.950]
Epoch [49/120    avg_loss:0.210, val_acc:0.967]
Epoch [50/120    avg_loss:0.180, val_acc:0.969]
Epoch [51/120    avg_loss:0.173, val_acc:0.975]
Epoch [52/120    avg_loss:0.198, val_acc:0.973]
Epoch [53/120    avg_loss:0.206, val_acc:0.975]
Epoch [54/120    avg_loss:0.178, val_acc:0.975]
Epoch [55/120    avg_loss:0.206, val_acc:0.981]
Epoch [56/120    avg_loss:0.168, val_acc:0.983]
Epoch [57/120    avg_loss:0.176, val_acc:0.975]
Epoch [58/120    avg_loss:0.172, val_acc:0.977]
Epoch [59/120    avg_loss:0.175, val_acc:0.979]
Epoch [60/120    avg_loss:0.177, val_acc:0.981]
Epoch [61/120    avg_loss:0.165, val_acc:0.981]
Epoch [62/120    avg_loss:0.167, val_acc:0.981]
Epoch [63/120    avg_loss:0.173, val_acc:0.985]
Epoch [64/120    avg_loss:0.164, val_acc:0.981]
Epoch [65/120    avg_loss:0.172, val_acc:0.981]
Epoch [66/120    avg_loss:0.157, val_acc:0.983]
Epoch [67/120    avg_loss:0.170, val_acc:0.983]
Epoch [68/120    avg_loss:0.177, val_acc:0.979]
Epoch [69/120    avg_loss:0.149, val_acc:0.983]
Epoch [70/120    avg_loss:0.148, val_acc:0.983]
Epoch [71/120    avg_loss:0.143, val_acc:0.981]
Epoch [72/120    avg_loss:0.149, val_acc:0.985]
Epoch [73/120    avg_loss:0.134, val_acc:0.985]
Epoch [74/120    avg_loss:0.133, val_acc:0.985]
Epoch [75/120    avg_loss:0.137, val_acc:0.981]
Epoch [76/120    avg_loss:0.118, val_acc:0.983]
Epoch [77/120    avg_loss:0.134, val_acc:0.983]
Epoch [78/120    avg_loss:0.133, val_acc:0.983]
Epoch [79/120    avg_loss:0.145, val_acc:0.988]
Epoch [80/120    avg_loss:0.136, val_acc:0.985]
Epoch [81/120    avg_loss:0.143, val_acc:0.983]
Epoch [82/120    avg_loss:0.146, val_acc:0.983]
Epoch [83/120    avg_loss:0.147, val_acc:0.990]
Epoch [84/120    avg_loss:0.132, val_acc:0.988]
Epoch [85/120    avg_loss:0.132, val_acc:0.985]
Epoch [86/120    avg_loss:0.132, val_acc:0.983]
Epoch [87/120    avg_loss:0.128, val_acc:0.988]
Epoch [88/120    avg_loss:0.125, val_acc:0.985]
Epoch [89/120    avg_loss:0.132, val_acc:0.979]
Epoch [90/120    avg_loss:0.128, val_acc:0.985]
Epoch [91/120    avg_loss:0.126, val_acc:0.981]
Epoch [92/120    avg_loss:0.118, val_acc:0.988]
Epoch [93/120    avg_loss:0.128, val_acc:0.990]
Epoch [94/120    avg_loss:0.136, val_acc:0.992]
Epoch [95/120    avg_loss:0.140, val_acc:0.990]
Epoch [96/120    avg_loss:0.115, val_acc:0.988]
Epoch [97/120    avg_loss:0.097, val_acc:0.990]
Epoch [98/120    avg_loss:0.111, val_acc:0.992]
Epoch [99/120    avg_loss:0.127, val_acc:0.985]
Epoch [100/120    avg_loss:0.120, val_acc:0.988]
Epoch [101/120    avg_loss:0.124, val_acc:0.988]
Epoch [102/120    avg_loss:0.110, val_acc:0.988]
Epoch [103/120    avg_loss:0.141, val_acc:0.990]
Epoch [104/120    avg_loss:0.112, val_acc:0.992]
Epoch [105/120    avg_loss:0.113, val_acc:0.990]
Epoch [106/120    avg_loss:0.107, val_acc:0.992]
Epoch [107/120    avg_loss:0.098, val_acc:0.990]
Epoch [108/120    avg_loss:0.116, val_acc:0.990]
Epoch [109/120    avg_loss:0.110, val_acc:0.988]
Epoch [110/120    avg_loss:0.105, val_acc:0.990]
Epoch [111/120    avg_loss:0.098, val_acc:0.990]
Epoch [112/120    avg_loss:0.104, val_acc:0.992]
Epoch [113/120    avg_loss:0.113, val_acc:0.990]
Epoch [114/120    avg_loss:0.105, val_acc:0.992]
Epoch [115/120    avg_loss:0.114, val_acc:0.981]
Epoch [116/120    avg_loss:0.115, val_acc:0.992]
Epoch [117/120    avg_loss:0.107, val_acc:0.988]
Epoch [118/120    avg_loss:0.103, val_acc:0.990]
Epoch [119/120    avg_loss:0.110, val_acc:0.994]
Epoch [120/120    avg_loss:0.091, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   9   0   0   0   0   0   0]
 [  0   0   0 226   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 203  24   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.95522388059702

F1 scores:
[       nan 1.         0.95238095 0.99122807 0.93548387 0.92356688
 1.         0.88648649 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.988369253551639
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4fe1689908>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.594, val_acc:0.475]
Epoch [2/120    avg_loss:2.363, val_acc:0.421]
Epoch [3/120    avg_loss:2.207, val_acc:0.540]
Epoch [4/120    avg_loss:2.079, val_acc:0.654]
Epoch [5/120    avg_loss:1.949, val_acc:0.650]
Epoch [6/120    avg_loss:1.811, val_acc:0.656]
Epoch [7/120    avg_loss:1.701, val_acc:0.669]
Epoch [8/120    avg_loss:1.548, val_acc:0.775]
Epoch [9/120    avg_loss:1.388, val_acc:0.823]
Epoch [10/120    avg_loss:1.251, val_acc:0.846]
Epoch [11/120    avg_loss:1.147, val_acc:0.840]
Epoch [12/120    avg_loss:1.035, val_acc:0.850]
Epoch [13/120    avg_loss:0.911, val_acc:0.887]
Epoch [14/120    avg_loss:0.831, val_acc:0.877]
Epoch [15/120    avg_loss:0.757, val_acc:0.910]
Epoch [16/120    avg_loss:0.687, val_acc:0.898]
Epoch [17/120    avg_loss:0.619, val_acc:0.896]
Epoch [18/120    avg_loss:0.596, val_acc:0.902]
Epoch [19/120    avg_loss:0.596, val_acc:0.917]
Epoch [20/120    avg_loss:0.534, val_acc:0.894]
Epoch [21/120    avg_loss:0.523, val_acc:0.921]
Epoch [22/120    avg_loss:0.484, val_acc:0.923]
Epoch [23/120    avg_loss:0.425, val_acc:0.912]
Epoch [24/120    avg_loss:0.409, val_acc:0.938]
Epoch [25/120    avg_loss:0.421, val_acc:0.931]
Epoch [26/120    avg_loss:0.404, val_acc:0.938]
Epoch [27/120    avg_loss:0.414, val_acc:0.956]
Epoch [28/120    avg_loss:0.366, val_acc:0.935]
Epoch [29/120    avg_loss:0.359, val_acc:0.935]
Epoch [30/120    avg_loss:0.319, val_acc:0.906]
Epoch [31/120    avg_loss:0.333, val_acc:0.952]
Epoch [32/120    avg_loss:0.301, val_acc:0.915]
Epoch [33/120    avg_loss:0.302, val_acc:0.946]
Epoch [34/120    avg_loss:0.301, val_acc:0.946]
Epoch [35/120    avg_loss:0.306, val_acc:0.935]
Epoch [36/120    avg_loss:0.288, val_acc:0.929]
Epoch [37/120    avg_loss:0.320, val_acc:0.952]
Epoch [38/120    avg_loss:0.255, val_acc:0.948]
Epoch [39/120    avg_loss:0.245, val_acc:0.963]
Epoch [40/120    avg_loss:0.251, val_acc:0.950]
Epoch [41/120    avg_loss:0.260, val_acc:0.960]
Epoch [42/120    avg_loss:0.202, val_acc:0.956]
Epoch [43/120    avg_loss:0.223, val_acc:0.954]
Epoch [44/120    avg_loss:0.206, val_acc:0.963]
Epoch [45/120    avg_loss:0.245, val_acc:0.948]
Epoch [46/120    avg_loss:0.237, val_acc:0.960]
Epoch [47/120    avg_loss:0.254, val_acc:0.960]
Epoch [48/120    avg_loss:0.200, val_acc:0.921]
Epoch [49/120    avg_loss:0.220, val_acc:0.940]
Epoch [50/120    avg_loss:0.192, val_acc:0.960]
Epoch [51/120    avg_loss:0.148, val_acc:0.954]
Epoch [52/120    avg_loss:0.192, val_acc:0.960]
Epoch [53/120    avg_loss:0.182, val_acc:0.965]
Epoch [54/120    avg_loss:0.163, val_acc:0.954]
Epoch [55/120    avg_loss:0.144, val_acc:0.979]
Epoch [56/120    avg_loss:0.146, val_acc:0.958]
Epoch [57/120    avg_loss:0.167, val_acc:0.981]
Epoch [58/120    avg_loss:0.127, val_acc:0.973]
Epoch [59/120    avg_loss:0.164, val_acc:0.956]
Epoch [60/120    avg_loss:0.140, val_acc:0.969]
Epoch [61/120    avg_loss:0.134, val_acc:0.956]
Epoch [62/120    avg_loss:0.171, val_acc:0.956]
Epoch [63/120    avg_loss:0.164, val_acc:0.975]
Epoch [64/120    avg_loss:0.210, val_acc:0.954]
Epoch [65/120    avg_loss:0.190, val_acc:0.965]
Epoch [66/120    avg_loss:0.159, val_acc:0.971]
Epoch [67/120    avg_loss:0.122, val_acc:0.969]
Epoch [68/120    avg_loss:0.114, val_acc:0.973]
Epoch [69/120    avg_loss:0.121, val_acc:0.971]
Epoch [70/120    avg_loss:0.102, val_acc:0.969]
Epoch [71/120    avg_loss:0.114, val_acc:0.975]
Epoch [72/120    avg_loss:0.079, val_acc:0.975]
Epoch [73/120    avg_loss:0.074, val_acc:0.977]
Epoch [74/120    avg_loss:0.074, val_acc:0.977]
Epoch [75/120    avg_loss:0.077, val_acc:0.975]
Epoch [76/120    avg_loss:0.076, val_acc:0.979]
Epoch [77/120    avg_loss:0.061, val_acc:0.983]
Epoch [78/120    avg_loss:0.082, val_acc:0.981]
Epoch [79/120    avg_loss:0.064, val_acc:0.979]
Epoch [80/120    avg_loss:0.065, val_acc:0.983]
Epoch [81/120    avg_loss:0.077, val_acc:0.977]
Epoch [82/120    avg_loss:0.073, val_acc:0.979]
Epoch [83/120    avg_loss:0.077, val_acc:0.981]
Epoch [84/120    avg_loss:0.067, val_acc:0.979]
Epoch [85/120    avg_loss:0.071, val_acc:0.977]
Epoch [86/120    avg_loss:0.092, val_acc:0.979]
Epoch [87/120    avg_loss:0.076, val_acc:0.979]
Epoch [88/120    avg_loss:0.079, val_acc:0.977]
Epoch [89/120    avg_loss:0.058, val_acc:0.979]
Epoch [90/120    avg_loss:0.073, val_acc:0.979]
Epoch [91/120    avg_loss:0.065, val_acc:0.981]
Epoch [92/120    avg_loss:0.056, val_acc:0.981]
Epoch [93/120    avg_loss:0.076, val_acc:0.979]
Epoch [94/120    avg_loss:0.058, val_acc:0.979]
Epoch [95/120    avg_loss:0.058, val_acc:0.979]
Epoch [96/120    avg_loss:0.052, val_acc:0.979]
Epoch [97/120    avg_loss:0.072, val_acc:0.979]
Epoch [98/120    avg_loss:0.059, val_acc:0.979]
Epoch [99/120    avg_loss:0.063, val_acc:0.981]
Epoch [100/120    avg_loss:0.061, val_acc:0.979]
Epoch [101/120    avg_loss:0.057, val_acc:0.979]
Epoch [102/120    avg_loss:0.055, val_acc:0.979]
Epoch [103/120    avg_loss:0.054, val_acc:0.979]
Epoch [104/120    avg_loss:0.054, val_acc:0.979]
Epoch [105/120    avg_loss:0.079, val_acc:0.979]
Epoch [106/120    avg_loss:0.055, val_acc:0.979]
Epoch [107/120    avg_loss:0.061, val_acc:0.979]
Epoch [108/120    avg_loss:0.062, val_acc:0.979]
Epoch [109/120    avg_loss:0.057, val_acc:0.979]
Epoch [110/120    avg_loss:0.067, val_acc:0.979]
Epoch [111/120    avg_loss:0.051, val_acc:0.979]
Epoch [112/120    avg_loss:0.057, val_acc:0.979]
Epoch [113/120    avg_loss:0.063, val_acc:0.979]
Epoch [114/120    avg_loss:0.061, val_acc:0.979]
Epoch [115/120    avg_loss:0.058, val_acc:0.979]
Epoch [116/120    avg_loss:0.060, val_acc:0.979]
Epoch [117/120    avg_loss:0.061, val_acc:0.979]
Epoch [118/120    avg_loss:0.059, val_acc:0.979]
Epoch [119/120    avg_loss:0.053, val_acc:0.979]
Epoch [120/120    avg_loss:0.061, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   3   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 202  25   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 1.         0.97117517 0.99343545 0.92237443 0.89967638
 1.         0.92571429 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9888430388721625
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fea1112f940>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.579, val_acc:0.356]
Epoch [2/120    avg_loss:2.363, val_acc:0.440]
Epoch [3/120    avg_loss:2.218, val_acc:0.510]
Epoch [4/120    avg_loss:2.074, val_acc:0.552]
Epoch [5/120    avg_loss:1.945, val_acc:0.577]
Epoch [6/120    avg_loss:1.816, val_acc:0.675]
Epoch [7/120    avg_loss:1.704, val_acc:0.619]
Epoch [8/120    avg_loss:1.591, val_acc:0.690]
Epoch [9/120    avg_loss:1.455, val_acc:0.715]
Epoch [10/120    avg_loss:1.335, val_acc:0.829]
Epoch [11/120    avg_loss:1.217, val_acc:0.754]
Epoch [12/120    avg_loss:1.110, val_acc:0.850]
Epoch [13/120    avg_loss:0.996, val_acc:0.854]
Epoch [14/120    avg_loss:0.934, val_acc:0.858]
Epoch [15/120    avg_loss:0.862, val_acc:0.854]
Epoch [16/120    avg_loss:0.840, val_acc:0.904]
Epoch [17/120    avg_loss:0.726, val_acc:0.900]
Epoch [18/120    avg_loss:0.697, val_acc:0.875]
Epoch [19/120    avg_loss:0.727, val_acc:0.904]
Epoch [20/120    avg_loss:0.627, val_acc:0.910]
Epoch [21/120    avg_loss:0.589, val_acc:0.933]
Epoch [22/120    avg_loss:0.573, val_acc:0.919]
Epoch [23/120    avg_loss:0.547, val_acc:0.906]
Epoch [24/120    avg_loss:0.487, val_acc:0.892]
Epoch [25/120    avg_loss:0.475, val_acc:0.921]
Epoch [26/120    avg_loss:0.450, val_acc:0.902]
Epoch [27/120    avg_loss:0.429, val_acc:0.927]
Epoch [28/120    avg_loss:0.519, val_acc:0.927]
Epoch [29/120    avg_loss:0.440, val_acc:0.879]
Epoch [30/120    avg_loss:0.363, val_acc:0.915]
Epoch [31/120    avg_loss:0.333, val_acc:0.927]
Epoch [32/120    avg_loss:0.367, val_acc:0.958]
Epoch [33/120    avg_loss:0.368, val_acc:0.925]
Epoch [34/120    avg_loss:0.338, val_acc:0.935]
Epoch [35/120    avg_loss:0.328, val_acc:0.942]
Epoch [36/120    avg_loss:0.300, val_acc:0.950]
Epoch [37/120    avg_loss:0.307, val_acc:0.942]
Epoch [38/120    avg_loss:0.292, val_acc:0.965]
Epoch [39/120    avg_loss:0.284, val_acc:0.965]
Epoch [40/120    avg_loss:0.232, val_acc:0.950]
Epoch [41/120    avg_loss:0.244, val_acc:0.915]
Epoch [42/120    avg_loss:0.284, val_acc:0.885]
Epoch [43/120    avg_loss:0.234, val_acc:0.956]
Epoch [44/120    avg_loss:0.258, val_acc:0.950]
Epoch [45/120    avg_loss:0.210, val_acc:0.954]
Epoch [46/120    avg_loss:0.216, val_acc:0.969]
Epoch [47/120    avg_loss:0.191, val_acc:0.967]
Epoch [48/120    avg_loss:0.187, val_acc:0.971]
Epoch [49/120    avg_loss:0.175, val_acc:0.973]
Epoch [50/120    avg_loss:0.172, val_acc:0.971]
Epoch [51/120    avg_loss:0.146, val_acc:0.965]
Epoch [52/120    avg_loss:0.200, val_acc:0.950]
Epoch [53/120    avg_loss:0.195, val_acc:0.944]
Epoch [54/120    avg_loss:0.196, val_acc:0.940]
Epoch [55/120    avg_loss:0.183, val_acc:0.960]
Epoch [56/120    avg_loss:0.172, val_acc:0.967]
Epoch [57/120    avg_loss:0.197, val_acc:0.952]
Epoch [58/120    avg_loss:0.131, val_acc:0.975]
Epoch [59/120    avg_loss:0.163, val_acc:0.923]
Epoch [60/120    avg_loss:0.155, val_acc:0.977]
Epoch [61/120    avg_loss:0.173, val_acc:0.971]
Epoch [62/120    avg_loss:0.125, val_acc:0.969]
Epoch [63/120    avg_loss:0.151, val_acc:0.973]
Epoch [64/120    avg_loss:0.232, val_acc:0.956]
Epoch [65/120    avg_loss:0.168, val_acc:0.967]
Epoch [66/120    avg_loss:0.130, val_acc:0.973]
Epoch [67/120    avg_loss:0.130, val_acc:0.979]
Epoch [68/120    avg_loss:0.125, val_acc:0.958]
Epoch [69/120    avg_loss:0.145, val_acc:0.975]
Epoch [70/120    avg_loss:0.105, val_acc:0.963]
Epoch [71/120    avg_loss:0.111, val_acc:0.979]
Epoch [72/120    avg_loss:0.094, val_acc:0.988]
Epoch [73/120    avg_loss:0.108, val_acc:0.988]
Epoch [74/120    avg_loss:0.077, val_acc:0.983]
Epoch [75/120    avg_loss:0.084, val_acc:0.990]
Epoch [76/120    avg_loss:0.108, val_acc:0.973]
Epoch [77/120    avg_loss:0.091, val_acc:0.990]
Epoch [78/120    avg_loss:0.117, val_acc:0.985]
Epoch [79/120    avg_loss:0.105, val_acc:0.981]
Epoch [80/120    avg_loss:0.105, val_acc:0.975]
Epoch [81/120    avg_loss:0.090, val_acc:0.983]
Epoch [82/120    avg_loss:0.113, val_acc:0.956]
Epoch [83/120    avg_loss:0.181, val_acc:0.969]
Epoch [84/120    avg_loss:0.080, val_acc:0.977]
Epoch [85/120    avg_loss:0.083, val_acc:0.960]
Epoch [86/120    avg_loss:0.080, val_acc:0.990]
Epoch [87/120    avg_loss:0.109, val_acc:0.977]
Epoch [88/120    avg_loss:0.089, val_acc:0.985]
Epoch [89/120    avg_loss:0.083, val_acc:0.983]
Epoch [90/120    avg_loss:0.075, val_acc:0.977]
Epoch [91/120    avg_loss:0.059, val_acc:0.981]
Epoch [92/120    avg_loss:0.069, val_acc:0.988]
Epoch [93/120    avg_loss:0.076, val_acc:0.992]
Epoch [94/120    avg_loss:0.080, val_acc:0.992]
Epoch [95/120    avg_loss:0.076, val_acc:0.979]
Epoch [96/120    avg_loss:0.089, val_acc:0.981]
Epoch [97/120    avg_loss:0.120, val_acc:0.938]
Epoch [98/120    avg_loss:0.198, val_acc:0.956]
Epoch [99/120    avg_loss:0.154, val_acc:0.973]
Epoch [100/120    avg_loss:0.115, val_acc:0.983]
Epoch [101/120    avg_loss:0.082, val_acc:0.981]
Epoch [102/120    avg_loss:0.073, val_acc:0.990]
Epoch [103/120    avg_loss:0.063, val_acc:0.994]
Epoch [104/120    avg_loss:0.060, val_acc:0.985]
Epoch [105/120    avg_loss:0.068, val_acc:0.990]
Epoch [106/120    avg_loss:0.046, val_acc:0.992]
Epoch [107/120    avg_loss:0.044, val_acc:0.979]
Epoch [108/120    avg_loss:0.040, val_acc:0.994]
Epoch [109/120    avg_loss:0.043, val_acc:0.977]
Epoch [110/120    avg_loss:0.049, val_acc:0.996]
Epoch [111/120    avg_loss:0.044, val_acc:0.985]
Epoch [112/120    avg_loss:0.048, val_acc:0.979]
Epoch [113/120    avg_loss:0.081, val_acc:0.977]
Epoch [114/120    avg_loss:0.066, val_acc:0.992]
Epoch [115/120    avg_loss:0.047, val_acc:0.990]
Epoch [116/120    avg_loss:0.033, val_acc:0.992]
Epoch [117/120    avg_loss:0.046, val_acc:0.994]
Epoch [118/120    avg_loss:0.071, val_acc:0.975]
Epoch [119/120    avg_loss:0.059, val_acc:0.983]
Epoch [120/120    avg_loss:0.040, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 204  26   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.65671641791045

F1 scores:
[       nan 1.         0.99545455 0.94009217 0.8742268  0.87719298
 1.         0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9850445277964157
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5a23dd8860>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.620, val_acc:0.196]
Epoch [2/120    avg_loss:2.408, val_acc:0.321]
Epoch [3/120    avg_loss:2.236, val_acc:0.435]
Epoch [4/120    avg_loss:2.064, val_acc:0.485]
Epoch [5/120    avg_loss:1.930, val_acc:0.571]
Epoch [6/120    avg_loss:1.783, val_acc:0.581]
Epoch [7/120    avg_loss:1.669, val_acc:0.629]
Epoch [8/120    avg_loss:1.530, val_acc:0.650]
Epoch [9/120    avg_loss:1.407, val_acc:0.679]
Epoch [10/120    avg_loss:1.292, val_acc:0.694]
Epoch [11/120    avg_loss:1.204, val_acc:0.704]
Epoch [12/120    avg_loss:1.138, val_acc:0.727]
Epoch [13/120    avg_loss:1.018, val_acc:0.748]
Epoch [14/120    avg_loss:0.959, val_acc:0.750]
Epoch [15/120    avg_loss:0.917, val_acc:0.767]
Epoch [16/120    avg_loss:0.830, val_acc:0.835]
Epoch [17/120    avg_loss:0.791, val_acc:0.881]
Epoch [18/120    avg_loss:0.721, val_acc:0.858]
Epoch [19/120    avg_loss:0.726, val_acc:0.819]
Epoch [20/120    avg_loss:0.645, val_acc:0.887]
Epoch [21/120    avg_loss:0.650, val_acc:0.894]
Epoch [22/120    avg_loss:0.606, val_acc:0.898]
Epoch [23/120    avg_loss:0.538, val_acc:0.902]
Epoch [24/120    avg_loss:0.529, val_acc:0.923]
Epoch [25/120    avg_loss:0.505, val_acc:0.883]
Epoch [26/120    avg_loss:0.541, val_acc:0.908]
Epoch [27/120    avg_loss:0.536, val_acc:0.902]
Epoch [28/120    avg_loss:0.511, val_acc:0.921]
Epoch [29/120    avg_loss:0.489, val_acc:0.900]
Epoch [30/120    avg_loss:0.436, val_acc:0.915]
Epoch [31/120    avg_loss:0.407, val_acc:0.927]
Epoch [32/120    avg_loss:0.397, val_acc:0.938]
Epoch [33/120    avg_loss:0.424, val_acc:0.846]
Epoch [34/120    avg_loss:0.409, val_acc:0.915]
Epoch [35/120    avg_loss:0.382, val_acc:0.946]
Epoch [36/120    avg_loss:0.355, val_acc:0.938]
Epoch [37/120    avg_loss:0.356, val_acc:0.948]
Epoch [38/120    avg_loss:0.385, val_acc:0.946]
Epoch [39/120    avg_loss:0.333, val_acc:0.935]
Epoch [40/120    avg_loss:0.287, val_acc:0.963]
Epoch [41/120    avg_loss:0.338, val_acc:0.956]
Epoch [42/120    avg_loss:0.337, val_acc:0.952]
Epoch [43/120    avg_loss:0.288, val_acc:0.967]
Epoch [44/120    avg_loss:0.254, val_acc:0.958]
Epoch [45/120    avg_loss:0.231, val_acc:0.952]
Epoch [46/120    avg_loss:0.236, val_acc:0.954]
Epoch [47/120    avg_loss:0.259, val_acc:0.967]
Epoch [48/120    avg_loss:0.233, val_acc:0.952]
Epoch [49/120    avg_loss:0.258, val_acc:0.963]
Epoch [50/120    avg_loss:0.187, val_acc:0.971]
Epoch [51/120    avg_loss:0.260, val_acc:0.946]
Epoch [52/120    avg_loss:0.219, val_acc:0.967]
Epoch [53/120    avg_loss:0.200, val_acc:0.969]
Epoch [54/120    avg_loss:0.299, val_acc:0.908]
Epoch [55/120    avg_loss:0.275, val_acc:0.969]
Epoch [56/120    avg_loss:0.211, val_acc:0.981]
Epoch [57/120    avg_loss:0.151, val_acc:0.956]
Epoch [58/120    avg_loss:0.200, val_acc:0.963]
Epoch [59/120    avg_loss:0.210, val_acc:0.977]
Epoch [60/120    avg_loss:0.164, val_acc:0.971]
Epoch [61/120    avg_loss:0.147, val_acc:0.977]
Epoch [62/120    avg_loss:0.158, val_acc:0.975]
Epoch [63/120    avg_loss:0.147, val_acc:0.979]
Epoch [64/120    avg_loss:0.148, val_acc:0.965]
Epoch [65/120    avg_loss:0.143, val_acc:0.975]
Epoch [66/120    avg_loss:0.151, val_acc:0.973]
Epoch [67/120    avg_loss:0.189, val_acc:0.956]
Epoch [68/120    avg_loss:0.147, val_acc:0.975]
Epoch [69/120    avg_loss:0.122, val_acc:0.979]
Epoch [70/120    avg_loss:0.120, val_acc:0.981]
Epoch [71/120    avg_loss:0.099, val_acc:0.985]
Epoch [72/120    avg_loss:0.104, val_acc:0.983]
Epoch [73/120    avg_loss:0.097, val_acc:0.988]
Epoch [74/120    avg_loss:0.111, val_acc:0.985]
Epoch [75/120    avg_loss:0.105, val_acc:0.985]
Epoch [76/120    avg_loss:0.085, val_acc:0.985]
Epoch [77/120    avg_loss:0.096, val_acc:0.985]
Epoch [78/120    avg_loss:0.091, val_acc:0.985]
Epoch [79/120    avg_loss:0.089, val_acc:0.985]
Epoch [80/120    avg_loss:0.085, val_acc:0.988]
Epoch [81/120    avg_loss:0.081, val_acc:0.985]
Epoch [82/120    avg_loss:0.078, val_acc:0.988]
Epoch [83/120    avg_loss:0.081, val_acc:0.985]
Epoch [84/120    avg_loss:0.076, val_acc:0.983]
Epoch [85/120    avg_loss:0.094, val_acc:0.985]
Epoch [86/120    avg_loss:0.082, val_acc:0.985]
Epoch [87/120    avg_loss:0.083, val_acc:0.988]
Epoch [88/120    avg_loss:0.081, val_acc:0.985]
Epoch [89/120    avg_loss:0.092, val_acc:0.985]
Epoch [90/120    avg_loss:0.089, val_acc:0.988]
Epoch [91/120    avg_loss:0.082, val_acc:0.990]
Epoch [92/120    avg_loss:0.076, val_acc:0.990]
Epoch [93/120    avg_loss:0.090, val_acc:0.988]
Epoch [94/120    avg_loss:0.082, val_acc:0.988]
Epoch [95/120    avg_loss:0.081, val_acc:0.985]
Epoch [96/120    avg_loss:0.064, val_acc:0.983]
Epoch [97/120    avg_loss:0.071, val_acc:0.983]
Epoch [98/120    avg_loss:0.085, val_acc:0.983]
Epoch [99/120    avg_loss:0.085, val_acc:0.990]
Epoch [100/120    avg_loss:0.078, val_acc:0.990]
Epoch [101/120    avg_loss:0.077, val_acc:0.988]
Epoch [102/120    avg_loss:0.084, val_acc:0.990]
Epoch [103/120    avg_loss:0.080, val_acc:0.990]
Epoch [104/120    avg_loss:0.071, val_acc:0.985]
Epoch [105/120    avg_loss:0.066, val_acc:0.983]
Epoch [106/120    avg_loss:0.080, val_acc:0.983]
Epoch [107/120    avg_loss:0.064, val_acc:0.985]
Epoch [108/120    avg_loss:0.081, val_acc:0.990]
Epoch [109/120    avg_loss:0.061, val_acc:0.992]
Epoch [110/120    avg_loss:0.078, val_acc:0.988]
Epoch [111/120    avg_loss:0.065, val_acc:0.988]
Epoch [112/120    avg_loss:0.068, val_acc:0.992]
Epoch [113/120    avg_loss:0.074, val_acc:0.988]
Epoch [114/120    avg_loss:0.069, val_acc:0.988]
Epoch [115/120    avg_loss:0.083, val_acc:0.992]
Epoch [116/120    avg_loss:0.069, val_acc:0.990]
Epoch [117/120    avg_loss:0.081, val_acc:0.992]
Epoch [118/120    avg_loss:0.074, val_acc:0.990]
Epoch [119/120    avg_loss:0.072, val_acc:0.990]
Epoch [120/120    avg_loss:0.094, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   5 223   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 201  26   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  10 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.78464818763327

F1 scores:
[       nan 1.         0.96688742 0.98454746 0.92626728 0.90384615
 1.         0.94382022 1.         1.         1.         0.98691099
 0.98883929 1.        ]

Kappa:
0.9864701986805242
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd8cc2348d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.536, val_acc:0.327]
Epoch [2/120    avg_loss:2.327, val_acc:0.429]
Epoch [3/120    avg_loss:2.162, val_acc:0.540]
Epoch [4/120    avg_loss:2.023, val_acc:0.502]
Epoch [5/120    avg_loss:1.880, val_acc:0.517]
Epoch [6/120    avg_loss:1.763, val_acc:0.600]
Epoch [7/120    avg_loss:1.646, val_acc:0.648]
Epoch [8/120    avg_loss:1.527, val_acc:0.669]
Epoch [9/120    avg_loss:1.411, val_acc:0.679]
Epoch [10/120    avg_loss:1.298, val_acc:0.679]
Epoch [11/120    avg_loss:1.209, val_acc:0.696]
Epoch [12/120    avg_loss:1.101, val_acc:0.723]
Epoch [13/120    avg_loss:1.003, val_acc:0.748]
Epoch [14/120    avg_loss:0.931, val_acc:0.825]
Epoch [15/120    avg_loss:0.908, val_acc:0.798]
Epoch [16/120    avg_loss:0.880, val_acc:0.808]
Epoch [17/120    avg_loss:0.785, val_acc:0.873]
Epoch [18/120    avg_loss:0.740, val_acc:0.873]
Epoch [19/120    avg_loss:0.706, val_acc:0.848]
Epoch [20/120    avg_loss:0.642, val_acc:0.892]
Epoch [21/120    avg_loss:0.626, val_acc:0.904]
Epoch [22/120    avg_loss:0.583, val_acc:0.898]
Epoch [23/120    avg_loss:0.545, val_acc:0.856]
Epoch [24/120    avg_loss:0.557, val_acc:0.829]
Epoch [25/120    avg_loss:0.538, val_acc:0.940]
Epoch [26/120    avg_loss:0.486, val_acc:0.881]
Epoch [27/120    avg_loss:0.461, val_acc:0.931]
Epoch [28/120    avg_loss:0.418, val_acc:0.931]
Epoch [29/120    avg_loss:0.438, val_acc:0.919]
Epoch [30/120    avg_loss:0.412, val_acc:0.902]
Epoch [31/120    avg_loss:0.424, val_acc:0.929]
Epoch [32/120    avg_loss:0.386, val_acc:0.910]
Epoch [33/120    avg_loss:0.397, val_acc:0.890]
Epoch [34/120    avg_loss:0.463, val_acc:0.871]
Epoch [35/120    avg_loss:0.399, val_acc:0.919]
Epoch [36/120    avg_loss:0.331, val_acc:0.942]
Epoch [37/120    avg_loss:0.325, val_acc:0.912]
Epoch [38/120    avg_loss:0.344, val_acc:0.873]
Epoch [39/120    avg_loss:0.289, val_acc:0.948]
Epoch [40/120    avg_loss:0.290, val_acc:0.929]
Epoch [41/120    avg_loss:0.288, val_acc:0.944]
Epoch [42/120    avg_loss:0.335, val_acc:0.902]
Epoch [43/120    avg_loss:0.328, val_acc:0.904]
Epoch [44/120    avg_loss:0.281, val_acc:0.963]
Epoch [45/120    avg_loss:0.279, val_acc:0.971]
Epoch [46/120    avg_loss:0.249, val_acc:0.933]
Epoch [47/120    avg_loss:0.250, val_acc:0.917]
Epoch [48/120    avg_loss:0.239, val_acc:0.896]
Epoch [49/120    avg_loss:0.253, val_acc:0.938]
Epoch [50/120    avg_loss:0.201, val_acc:0.942]
Epoch [51/120    avg_loss:0.248, val_acc:0.965]
Epoch [52/120    avg_loss:0.235, val_acc:0.933]
Epoch [53/120    avg_loss:0.197, val_acc:0.956]
Epoch [54/120    avg_loss:0.202, val_acc:0.935]
Epoch [55/120    avg_loss:0.217, val_acc:0.967]
Epoch [56/120    avg_loss:0.213, val_acc:0.960]
Epoch [57/120    avg_loss:0.179, val_acc:0.965]
Epoch [58/120    avg_loss:0.190, val_acc:0.967]
Epoch [59/120    avg_loss:0.157, val_acc:0.977]
Epoch [60/120    avg_loss:0.165, val_acc:0.977]
Epoch [61/120    avg_loss:0.123, val_acc:0.983]
Epoch [62/120    avg_loss:0.128, val_acc:0.977]
Epoch [63/120    avg_loss:0.120, val_acc:0.985]
Epoch [64/120    avg_loss:0.120, val_acc:0.981]
Epoch [65/120    avg_loss:0.121, val_acc:0.988]
Epoch [66/120    avg_loss:0.124, val_acc:0.983]
Epoch [67/120    avg_loss:0.131, val_acc:0.979]
Epoch [68/120    avg_loss:0.126, val_acc:0.983]
Epoch [69/120    avg_loss:0.127, val_acc:0.977]
Epoch [70/120    avg_loss:0.125, val_acc:0.983]
Epoch [71/120    avg_loss:0.125, val_acc:0.985]
Epoch [72/120    avg_loss:0.123, val_acc:0.981]
Epoch [73/120    avg_loss:0.113, val_acc:0.983]
Epoch [74/120    avg_loss:0.126, val_acc:0.983]
Epoch [75/120    avg_loss:0.127, val_acc:0.979]
Epoch [76/120    avg_loss:0.107, val_acc:0.983]
Epoch [77/120    avg_loss:0.110, val_acc:0.988]
Epoch [78/120    avg_loss:0.112, val_acc:0.981]
Epoch [79/120    avg_loss:0.113, val_acc:0.990]
Epoch [80/120    avg_loss:0.106, val_acc:0.990]
Epoch [81/120    avg_loss:0.117, val_acc:0.988]
Epoch [82/120    avg_loss:0.110, val_acc:0.988]
Epoch [83/120    avg_loss:0.132, val_acc:0.985]
Epoch [84/120    avg_loss:0.116, val_acc:0.990]
Epoch [85/120    avg_loss:0.115, val_acc:0.990]
Epoch [86/120    avg_loss:0.130, val_acc:0.985]
Epoch [87/120    avg_loss:0.117, val_acc:0.983]
Epoch [88/120    avg_loss:0.102, val_acc:0.990]
Epoch [89/120    avg_loss:0.095, val_acc:0.992]
Epoch [90/120    avg_loss:0.114, val_acc:0.992]
Epoch [91/120    avg_loss:0.121, val_acc:0.988]
Epoch [92/120    avg_loss:0.104, val_acc:0.992]
Epoch [93/120    avg_loss:0.120, val_acc:0.990]
Epoch [94/120    avg_loss:0.098, val_acc:0.988]
Epoch [95/120    avg_loss:0.108, val_acc:0.992]
Epoch [96/120    avg_loss:0.108, val_acc:0.985]
Epoch [97/120    avg_loss:0.099, val_acc:0.990]
Epoch [98/120    avg_loss:0.115, val_acc:0.990]
Epoch [99/120    avg_loss:0.098, val_acc:0.985]
Epoch [100/120    avg_loss:0.105, val_acc:0.988]
Epoch [101/120    avg_loss:0.093, val_acc:0.992]
Epoch [102/120    avg_loss:0.093, val_acc:0.985]
Epoch [103/120    avg_loss:0.105, val_acc:0.990]
Epoch [104/120    avg_loss:0.103, val_acc:0.992]
Epoch [105/120    avg_loss:0.102, val_acc:0.988]
Epoch [106/120    avg_loss:0.096, val_acc:0.992]
Epoch [107/120    avg_loss:0.099, val_acc:0.994]
Epoch [108/120    avg_loss:0.101, val_acc:0.990]
Epoch [109/120    avg_loss:0.107, val_acc:0.994]
Epoch [110/120    avg_loss:0.088, val_acc:0.983]
Epoch [111/120    avg_loss:0.101, val_acc:0.990]
Epoch [112/120    avg_loss:0.092, val_acc:0.990]
Epoch [113/120    avg_loss:0.098, val_acc:0.988]
Epoch [114/120    avg_loss:0.102, val_acc:0.988]
Epoch [115/120    avg_loss:0.112, val_acc:0.988]
Epoch [116/120    avg_loss:0.102, val_acc:0.990]
Epoch [117/120    avg_loss:0.126, val_acc:0.988]
Epoch [118/120    avg_loss:0.094, val_acc:0.990]
Epoch [119/120    avg_loss:0.101, val_acc:0.990]
Epoch [120/120    avg_loss:0.098, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   1 225   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 202  25   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0   9   0   0   0   0  85   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.78464818763327

F1 scores:
[       nan 0.99927061 0.97078652 0.98901099 0.90380313 0.87043189
 0.99756691 0.93406593 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9864690488793652
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6910086978>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.579, val_acc:0.152]
Epoch [2/120    avg_loss:2.394, val_acc:0.394]
Epoch [3/120    avg_loss:2.263, val_acc:0.490]
Epoch [4/120    avg_loss:2.097, val_acc:0.537]
Epoch [5/120    avg_loss:1.954, val_acc:0.598]
Epoch [6/120    avg_loss:1.767, val_acc:0.646]
Epoch [7/120    avg_loss:1.633, val_acc:0.681]
Epoch [8/120    avg_loss:1.482, val_acc:0.694]
Epoch [9/120    avg_loss:1.373, val_acc:0.708]
Epoch [10/120    avg_loss:1.292, val_acc:0.729]
Epoch [11/120    avg_loss:1.238, val_acc:0.748]
Epoch [12/120    avg_loss:1.089, val_acc:0.848]
Epoch [13/120    avg_loss:1.074, val_acc:0.858]
Epoch [14/120    avg_loss:0.994, val_acc:0.850]
Epoch [15/120    avg_loss:0.898, val_acc:0.835]
Epoch [16/120    avg_loss:0.856, val_acc:0.896]
Epoch [17/120    avg_loss:0.804, val_acc:0.896]
Epoch [18/120    avg_loss:0.780, val_acc:0.850]
Epoch [19/120    avg_loss:0.754, val_acc:0.890]
Epoch [20/120    avg_loss:0.681, val_acc:0.867]
Epoch [21/120    avg_loss:0.659, val_acc:0.896]
Epoch [22/120    avg_loss:0.632, val_acc:0.871]
Epoch [23/120    avg_loss:0.576, val_acc:0.929]
Epoch [24/120    avg_loss:0.542, val_acc:0.927]
Epoch [25/120    avg_loss:0.491, val_acc:0.919]
Epoch [26/120    avg_loss:0.533, val_acc:0.912]
Epoch [27/120    avg_loss:0.480, val_acc:0.894]
Epoch [28/120    avg_loss:0.493, val_acc:0.894]
Epoch [29/120    avg_loss:0.474, val_acc:0.933]
Epoch [30/120    avg_loss:0.457, val_acc:0.921]
Epoch [31/120    avg_loss:0.379, val_acc:0.935]
Epoch [32/120    avg_loss:0.367, val_acc:0.963]
Epoch [33/120    avg_loss:0.384, val_acc:0.865]
Epoch [34/120    avg_loss:0.434, val_acc:0.940]
Epoch [35/120    avg_loss:0.328, val_acc:0.935]
Epoch [36/120    avg_loss:0.379, val_acc:0.946]
Epoch [37/120    avg_loss:0.405, val_acc:0.944]
Epoch [38/120    avg_loss:0.345, val_acc:0.954]
Epoch [39/120    avg_loss:0.327, val_acc:0.958]
Epoch [40/120    avg_loss:0.293, val_acc:0.938]
Epoch [41/120    avg_loss:0.279, val_acc:0.960]
Epoch [42/120    avg_loss:0.331, val_acc:0.965]
Epoch [43/120    avg_loss:0.261, val_acc:0.931]
Epoch [44/120    avg_loss:0.266, val_acc:0.908]
Epoch [45/120    avg_loss:0.233, val_acc:0.950]
Epoch [46/120    avg_loss:0.235, val_acc:0.946]
Epoch [47/120    avg_loss:0.242, val_acc:0.946]
Epoch [48/120    avg_loss:0.253, val_acc:0.960]
Epoch [49/120    avg_loss:0.201, val_acc:0.954]
Epoch [50/120    avg_loss:0.235, val_acc:0.929]
Epoch [51/120    avg_loss:0.246, val_acc:0.954]
Epoch [52/120    avg_loss:0.221, val_acc:0.965]
Epoch [53/120    avg_loss:0.230, val_acc:0.954]
Epoch [54/120    avg_loss:0.281, val_acc:0.963]
Epoch [55/120    avg_loss:0.188, val_acc:0.977]
Epoch [56/120    avg_loss:0.190, val_acc:0.919]
Epoch [57/120    avg_loss:0.236, val_acc:0.954]
Epoch [58/120    avg_loss:0.199, val_acc:0.948]
Epoch [59/120    avg_loss:0.135, val_acc:0.973]
Epoch [60/120    avg_loss:0.207, val_acc:0.973]
Epoch [61/120    avg_loss:0.184, val_acc:0.965]
Epoch [62/120    avg_loss:0.147, val_acc:0.979]
Epoch [63/120    avg_loss:0.152, val_acc:0.935]
Epoch [64/120    avg_loss:0.174, val_acc:0.969]
Epoch [65/120    avg_loss:0.148, val_acc:0.983]
Epoch [66/120    avg_loss:0.197, val_acc:0.965]
Epoch [67/120    avg_loss:0.172, val_acc:0.975]
Epoch [68/120    avg_loss:0.137, val_acc:0.977]
Epoch [69/120    avg_loss:0.161, val_acc:0.975]
Epoch [70/120    avg_loss:0.126, val_acc:0.979]
Epoch [71/120    avg_loss:0.133, val_acc:0.977]
Epoch [72/120    avg_loss:0.099, val_acc:0.985]
Epoch [73/120    avg_loss:0.079, val_acc:0.985]
Epoch [74/120    avg_loss:0.089, val_acc:0.988]
Epoch [75/120    avg_loss:0.083, val_acc:0.983]
Epoch [76/120    avg_loss:0.078, val_acc:0.992]
Epoch [77/120    avg_loss:0.113, val_acc:0.958]
Epoch [78/120    avg_loss:0.124, val_acc:0.983]
Epoch [79/120    avg_loss:0.105, val_acc:0.969]
Epoch [80/120    avg_loss:0.142, val_acc:0.919]
Epoch [81/120    avg_loss:0.136, val_acc:0.973]
Epoch [82/120    avg_loss:0.133, val_acc:0.985]
Epoch [83/120    avg_loss:0.096, val_acc:0.977]
Epoch [84/120    avg_loss:0.072, val_acc:0.983]
Epoch [85/120    avg_loss:0.084, val_acc:0.981]
Epoch [86/120    avg_loss:0.070, val_acc:0.971]
Epoch [87/120    avg_loss:0.051, val_acc:0.983]
Epoch [88/120    avg_loss:0.087, val_acc:0.979]
Epoch [89/120    avg_loss:0.088, val_acc:0.979]
Epoch [90/120    avg_loss:0.080, val_acc:0.988]
Epoch [91/120    avg_loss:0.056, val_acc:0.990]
Epoch [92/120    avg_loss:0.046, val_acc:0.990]
Epoch [93/120    avg_loss:0.051, val_acc:0.988]
Epoch [94/120    avg_loss:0.039, val_acc:0.988]
Epoch [95/120    avg_loss:0.055, val_acc:0.992]
Epoch [96/120    avg_loss:0.045, val_acc:0.990]
Epoch [97/120    avg_loss:0.045, val_acc:0.990]
Epoch [98/120    avg_loss:0.040, val_acc:0.990]
Epoch [99/120    avg_loss:0.053, val_acc:0.990]
Epoch [100/120    avg_loss:0.035, val_acc:0.988]
Epoch [101/120    avg_loss:0.042, val_acc:0.988]
Epoch [102/120    avg_loss:0.041, val_acc:0.990]
Epoch [103/120    avg_loss:0.038, val_acc:0.990]
Epoch [104/120    avg_loss:0.038, val_acc:0.990]
Epoch [105/120    avg_loss:0.035, val_acc:0.990]
Epoch [106/120    avg_loss:0.038, val_acc:0.992]
Epoch [107/120    avg_loss:0.042, val_acc:0.990]
Epoch [108/120    avg_loss:0.045, val_acc:0.994]
Epoch [109/120    avg_loss:0.040, val_acc:0.990]
Epoch [110/120    avg_loss:0.033, val_acc:0.988]
Epoch [111/120    avg_loss:0.039, val_acc:0.988]
Epoch [112/120    avg_loss:0.038, val_acc:0.990]
Epoch [113/120    avg_loss:0.040, val_acc:0.990]
Epoch [114/120    avg_loss:0.041, val_acc:0.992]
Epoch [115/120    avg_loss:0.034, val_acc:0.992]
Epoch [116/120    avg_loss:0.030, val_acc:0.992]
Epoch [117/120    avg_loss:0.042, val_acc:0.992]
Epoch [118/120    avg_loss:0.035, val_acc:0.994]
Epoch [119/120    avg_loss:0.047, val_acc:0.994]
Epoch [120/120    avg_loss:0.038, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 221   8   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0 204   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.98426966 0.98004435 0.94196429 0.94771242
 0.99512195 0.96132597 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9919291044762899
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1fa7dc3898>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.575, val_acc:0.323]
Epoch [2/120    avg_loss:2.346, val_acc:0.456]
Epoch [3/120    avg_loss:2.181, val_acc:0.481]
Epoch [4/120    avg_loss:2.017, val_acc:0.527]
Epoch [5/120    avg_loss:1.860, val_acc:0.598]
Epoch [6/120    avg_loss:1.736, val_acc:0.648]
Epoch [7/120    avg_loss:1.607, val_acc:0.631]
Epoch [8/120    avg_loss:1.479, val_acc:0.685]
Epoch [9/120    avg_loss:1.362, val_acc:0.710]
Epoch [10/120    avg_loss:1.235, val_acc:0.748]
Epoch [11/120    avg_loss:1.130, val_acc:0.769]
Epoch [12/120    avg_loss:1.009, val_acc:0.798]
Epoch [13/120    avg_loss:0.966, val_acc:0.858]
Epoch [14/120    avg_loss:0.855, val_acc:0.856]
Epoch [15/120    avg_loss:0.822, val_acc:0.881]
Epoch [16/120    avg_loss:0.795, val_acc:0.896]
Epoch [17/120    avg_loss:0.703, val_acc:0.915]
Epoch [18/120    avg_loss:0.657, val_acc:0.900]
Epoch [19/120    avg_loss:0.598, val_acc:0.898]
Epoch [20/120    avg_loss:0.599, val_acc:0.921]
Epoch [21/120    avg_loss:0.514, val_acc:0.829]
Epoch [22/120    avg_loss:0.543, val_acc:0.927]
Epoch [23/120    avg_loss:0.523, val_acc:0.854]
Epoch [24/120    avg_loss:0.515, val_acc:0.925]
Epoch [25/120    avg_loss:0.423, val_acc:0.927]
Epoch [26/120    avg_loss:0.394, val_acc:0.942]
Epoch [27/120    avg_loss:0.391, val_acc:0.925]
Epoch [28/120    avg_loss:0.515, val_acc:0.827]
Epoch [29/120    avg_loss:0.454, val_acc:0.942]
Epoch [30/120    avg_loss:0.347, val_acc:0.950]
Epoch [31/120    avg_loss:0.341, val_acc:0.931]
Epoch [32/120    avg_loss:0.348, val_acc:0.958]
Epoch [33/120    avg_loss:0.304, val_acc:0.948]
Epoch [34/120    avg_loss:0.306, val_acc:0.965]
Epoch [35/120    avg_loss:0.297, val_acc:0.948]
Epoch [36/120    avg_loss:0.325, val_acc:0.965]
Epoch [37/120    avg_loss:0.286, val_acc:0.925]
Epoch [38/120    avg_loss:0.224, val_acc:0.963]
Epoch [39/120    avg_loss:0.228, val_acc:0.946]
Epoch [40/120    avg_loss:0.221, val_acc:0.963]
Epoch [41/120    avg_loss:0.259, val_acc:0.933]
Epoch [42/120    avg_loss:0.237, val_acc:0.963]
Epoch [43/120    avg_loss:0.210, val_acc:0.956]
Epoch [44/120    avg_loss:0.194, val_acc:0.950]
Epoch [45/120    avg_loss:0.183, val_acc:0.971]
Epoch [46/120    avg_loss:0.191, val_acc:0.969]
Epoch [47/120    avg_loss:0.201, val_acc:0.963]
Epoch [48/120    avg_loss:0.176, val_acc:0.969]
Epoch [49/120    avg_loss:0.184, val_acc:0.963]
Epoch [50/120    avg_loss:0.163, val_acc:0.971]
Epoch [51/120    avg_loss:0.138, val_acc:0.971]
Epoch [52/120    avg_loss:0.150, val_acc:0.942]
Epoch [53/120    avg_loss:0.157, val_acc:0.981]
Epoch [54/120    avg_loss:0.157, val_acc:0.971]
Epoch [55/120    avg_loss:0.200, val_acc:0.967]
Epoch [56/120    avg_loss:0.169, val_acc:0.960]
Epoch [57/120    avg_loss:0.233, val_acc:0.948]
Epoch [58/120    avg_loss:0.248, val_acc:0.965]
Epoch [59/120    avg_loss:0.204, val_acc:0.960]
Epoch [60/120    avg_loss:0.186, val_acc:0.967]
Epoch [61/120    avg_loss:0.178, val_acc:0.960]
Epoch [62/120    avg_loss:0.147, val_acc:0.971]
Epoch [63/120    avg_loss:0.152, val_acc:0.973]
Epoch [64/120    avg_loss:0.136, val_acc:0.979]
Epoch [65/120    avg_loss:0.126, val_acc:0.975]
Epoch [66/120    avg_loss:0.107, val_acc:0.948]
Epoch [67/120    avg_loss:0.133, val_acc:0.973]
Epoch [68/120    avg_loss:0.096, val_acc:0.977]
Epoch [69/120    avg_loss:0.085, val_acc:0.975]
Epoch [70/120    avg_loss:0.084, val_acc:0.979]
Epoch [71/120    avg_loss:0.090, val_acc:0.979]
Epoch [72/120    avg_loss:0.101, val_acc:0.979]
Epoch [73/120    avg_loss:0.082, val_acc:0.981]
Epoch [74/120    avg_loss:0.086, val_acc:0.981]
Epoch [75/120    avg_loss:0.077, val_acc:0.979]
Epoch [76/120    avg_loss:0.072, val_acc:0.979]
Epoch [77/120    avg_loss:0.086, val_acc:0.979]
Epoch [78/120    avg_loss:0.075, val_acc:0.979]
Epoch [79/120    avg_loss:0.078, val_acc:0.979]
Epoch [80/120    avg_loss:0.079, val_acc:0.979]
Epoch [81/120    avg_loss:0.074, val_acc:0.979]
Epoch [82/120    avg_loss:0.089, val_acc:0.979]
Epoch [83/120    avg_loss:0.068, val_acc:0.981]
Epoch [84/120    avg_loss:0.069, val_acc:0.981]
Epoch [85/120    avg_loss:0.082, val_acc:0.979]
Epoch [86/120    avg_loss:0.078, val_acc:0.981]
Epoch [87/120    avg_loss:0.067, val_acc:0.983]
Epoch [88/120    avg_loss:0.077, val_acc:0.983]
Epoch [89/120    avg_loss:0.086, val_acc:0.983]
Epoch [90/120    avg_loss:0.075, val_acc:0.981]
Epoch [91/120    avg_loss:0.069, val_acc:0.981]
Epoch [92/120    avg_loss:0.065, val_acc:0.981]
Epoch [93/120    avg_loss:0.073, val_acc:0.983]
Epoch [94/120    avg_loss:0.072, val_acc:0.981]
Epoch [95/120    avg_loss:0.070, val_acc:0.981]
Epoch [96/120    avg_loss:0.067, val_acc:0.981]
Epoch [97/120    avg_loss:0.070, val_acc:0.979]
Epoch [98/120    avg_loss:0.068, val_acc:0.981]
Epoch [99/120    avg_loss:0.072, val_acc:0.981]
Epoch [100/120    avg_loss:0.060, val_acc:0.981]
Epoch [101/120    avg_loss:0.061, val_acc:0.981]
Epoch [102/120    avg_loss:0.062, val_acc:0.981]
Epoch [103/120    avg_loss:0.072, val_acc:0.981]
Epoch [104/120    avg_loss:0.078, val_acc:0.981]
Epoch [105/120    avg_loss:0.067, val_acc:0.981]
Epoch [106/120    avg_loss:0.076, val_acc:0.981]
Epoch [107/120    avg_loss:0.068, val_acc:0.981]
Epoch [108/120    avg_loss:0.062, val_acc:0.981]
Epoch [109/120    avg_loss:0.059, val_acc:0.981]
Epoch [110/120    avg_loss:0.073, val_acc:0.981]
Epoch [111/120    avg_loss:0.059, val_acc:0.981]
Epoch [112/120    avg_loss:0.056, val_acc:0.981]
Epoch [113/120    avg_loss:0.066, val_acc:0.981]
Epoch [114/120    avg_loss:0.061, val_acc:0.981]
Epoch [115/120    avg_loss:0.066, val_acc:0.981]
Epoch [116/120    avg_loss:0.063, val_acc:0.981]
Epoch [117/120    avg_loss:0.071, val_acc:0.981]
Epoch [118/120    avg_loss:0.064, val_acc:0.981]
Epoch [119/120    avg_loss:0.066, val_acc:0.981]
Epoch [120/120    avg_loss:0.060, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   3 226   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 197  30   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.84861407249467

F1 scores:
[       nan 1.         0.97550111 0.99122807 0.91415313 0.88178914
 1.         0.95555556 0.998713   1.         1.         0.99341238
 0.99445061 1.        ]

Kappa:
0.9871821416856683
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fddcf2358d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.614, val_acc:0.323]
Epoch [2/120    avg_loss:2.346, val_acc:0.421]
Epoch [3/120    avg_loss:2.192, val_acc:0.519]
Epoch [4/120    avg_loss:2.047, val_acc:0.552]
Epoch [5/120    avg_loss:1.904, val_acc:0.542]
Epoch [6/120    avg_loss:1.768, val_acc:0.575]
Epoch [7/120    avg_loss:1.612, val_acc:0.608]
Epoch [8/120    avg_loss:1.483, val_acc:0.675]
Epoch [9/120    avg_loss:1.377, val_acc:0.694]
Epoch [10/120    avg_loss:1.245, val_acc:0.760]
Epoch [11/120    avg_loss:1.165, val_acc:0.815]
Epoch [12/120    avg_loss:1.088, val_acc:0.804]
Epoch [13/120    avg_loss:1.017, val_acc:0.815]
Epoch [14/120    avg_loss:0.895, val_acc:0.842]
Epoch [15/120    avg_loss:0.857, val_acc:0.842]
Epoch [16/120    avg_loss:0.801, val_acc:0.883]
Epoch [17/120    avg_loss:0.773, val_acc:0.873]
Epoch [18/120    avg_loss:0.684, val_acc:0.885]
Epoch [19/120    avg_loss:0.648, val_acc:0.877]
Epoch [20/120    avg_loss:0.596, val_acc:0.898]
Epoch [21/120    avg_loss:0.564, val_acc:0.883]
Epoch [22/120    avg_loss:0.568, val_acc:0.925]
Epoch [23/120    avg_loss:0.600, val_acc:0.902]
Epoch [24/120    avg_loss:0.531, val_acc:0.904]
Epoch [25/120    avg_loss:0.482, val_acc:0.906]
Epoch [26/120    avg_loss:0.434, val_acc:0.906]
Epoch [27/120    avg_loss:0.444, val_acc:0.900]
Epoch [28/120    avg_loss:0.438, val_acc:0.938]
Epoch [29/120    avg_loss:0.381, val_acc:0.919]
Epoch [30/120    avg_loss:0.371, val_acc:0.935]
Epoch [31/120    avg_loss:0.333, val_acc:0.929]
Epoch [32/120    avg_loss:0.318, val_acc:0.917]
Epoch [33/120    avg_loss:0.324, val_acc:0.952]
Epoch [34/120    avg_loss:0.353, val_acc:0.912]
Epoch [35/120    avg_loss:0.329, val_acc:0.917]
Epoch [36/120    avg_loss:0.371, val_acc:0.950]
Epoch [37/120    avg_loss:0.306, val_acc:0.940]
Epoch [38/120    avg_loss:0.306, val_acc:0.944]
Epoch [39/120    avg_loss:0.261, val_acc:0.935]
Epoch [40/120    avg_loss:0.276, val_acc:0.946]
Epoch [41/120    avg_loss:0.295, val_acc:0.952]
Epoch [42/120    avg_loss:0.255, val_acc:0.954]
Epoch [43/120    avg_loss:0.267, val_acc:0.944]
Epoch [44/120    avg_loss:0.258, val_acc:0.954]
Epoch [45/120    avg_loss:0.226, val_acc:0.919]
Epoch [46/120    avg_loss:0.286, val_acc:0.940]
Epoch [47/120    avg_loss:0.248, val_acc:0.908]
Epoch [48/120    avg_loss:0.265, val_acc:0.956]
Epoch [49/120    avg_loss:0.216, val_acc:0.952]
Epoch [50/120    avg_loss:0.201, val_acc:0.965]
Epoch [51/120    avg_loss:0.204, val_acc:0.971]
Epoch [52/120    avg_loss:0.201, val_acc:0.958]
Epoch [53/120    avg_loss:0.189, val_acc:0.958]
Epoch [54/120    avg_loss:0.229, val_acc:0.954]
Epoch [55/120    avg_loss:0.257, val_acc:0.952]
Epoch [56/120    avg_loss:0.199, val_acc:0.956]
Epoch [57/120    avg_loss:0.180, val_acc:0.960]
Epoch [58/120    avg_loss:0.155, val_acc:0.975]
Epoch [59/120    avg_loss:0.143, val_acc:0.973]
Epoch [60/120    avg_loss:0.140, val_acc:0.965]
Epoch [61/120    avg_loss:0.172, val_acc:0.969]
Epoch [62/120    avg_loss:0.137, val_acc:0.979]
Epoch [63/120    avg_loss:0.129, val_acc:0.977]
Epoch [64/120    avg_loss:0.165, val_acc:0.965]
Epoch [65/120    avg_loss:0.173, val_acc:0.971]
Epoch [66/120    avg_loss:0.176, val_acc:0.965]
Epoch [67/120    avg_loss:0.181, val_acc:0.958]
Epoch [68/120    avg_loss:0.134, val_acc:0.971]
Epoch [69/120    avg_loss:0.121, val_acc:0.977]
Epoch [70/120    avg_loss:0.122, val_acc:0.983]
Epoch [71/120    avg_loss:0.123, val_acc:0.958]
Epoch [72/120    avg_loss:0.106, val_acc:0.977]
Epoch [73/120    avg_loss:0.168, val_acc:0.965]
Epoch [74/120    avg_loss:0.121, val_acc:0.973]
Epoch [75/120    avg_loss:0.110, val_acc:0.967]
Epoch [76/120    avg_loss:0.152, val_acc:0.979]
Epoch [77/120    avg_loss:0.117, val_acc:0.985]
Epoch [78/120    avg_loss:0.137, val_acc:0.967]
Epoch [79/120    avg_loss:0.115, val_acc:0.979]
Epoch [80/120    avg_loss:0.137, val_acc:0.963]
Epoch [81/120    avg_loss:0.105, val_acc:0.948]
Epoch [82/120    avg_loss:0.126, val_acc:0.975]
Epoch [83/120    avg_loss:0.118, val_acc:0.971]
Epoch [84/120    avg_loss:0.107, val_acc:0.985]
Epoch [85/120    avg_loss:0.117, val_acc:0.975]
Epoch [86/120    avg_loss:0.130, val_acc:0.975]
Epoch [87/120    avg_loss:0.116, val_acc:0.979]
Epoch [88/120    avg_loss:0.079, val_acc:0.975]
Epoch [89/120    avg_loss:0.060, val_acc:0.985]
Epoch [90/120    avg_loss:0.083, val_acc:0.985]
Epoch [91/120    avg_loss:0.070, val_acc:0.981]
Epoch [92/120    avg_loss:0.072, val_acc:0.977]
Epoch [93/120    avg_loss:0.104, val_acc:0.965]
Epoch [94/120    avg_loss:0.098, val_acc:0.981]
Epoch [95/120    avg_loss:0.079, val_acc:0.977]
Epoch [96/120    avg_loss:0.063, val_acc:0.979]
Epoch [97/120    avg_loss:0.066, val_acc:0.990]
Epoch [98/120    avg_loss:0.088, val_acc:0.992]
Epoch [99/120    avg_loss:0.065, val_acc:0.979]
Epoch [100/120    avg_loss:0.060, val_acc:0.990]
Epoch [101/120    avg_loss:0.106, val_acc:0.952]
Epoch [102/120    avg_loss:0.112, val_acc:0.958]
Epoch [103/120    avg_loss:0.139, val_acc:0.988]
Epoch [104/120    avg_loss:0.064, val_acc:0.988]
Epoch [105/120    avg_loss:0.063, val_acc:0.988]
Epoch [106/120    avg_loss:0.057, val_acc:0.988]
Epoch [107/120    avg_loss:0.062, val_acc:0.985]
Epoch [108/120    avg_loss:0.052, val_acc:0.983]
Epoch [109/120    avg_loss:0.064, val_acc:0.979]
Epoch [110/120    avg_loss:0.132, val_acc:0.952]
Epoch [111/120    avg_loss:0.105, val_acc:0.969]
Epoch [112/120    avg_loss:0.068, val_acc:0.988]
Epoch [113/120    avg_loss:0.043, val_acc:0.990]
Epoch [114/120    avg_loss:0.044, val_acc:0.990]
Epoch [115/120    avg_loss:0.039, val_acc:0.992]
Epoch [116/120    avg_loss:0.043, val_acc:0.992]
Epoch [117/120    avg_loss:0.045, val_acc:0.992]
Epoch [118/120    avg_loss:0.052, val_acc:0.992]
Epoch [119/120    avg_loss:0.039, val_acc:0.992]
Epoch [120/120    avg_loss:0.037, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.97550111 0.98678414 0.94407159 0.93729373
 1.         0.93785311 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9914541417574858
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffac05f0940>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.541, val_acc:0.306]
Epoch [2/120    avg_loss:2.332, val_acc:0.346]
Epoch [3/120    avg_loss:2.188, val_acc:0.460]
Epoch [4/120    avg_loss:2.077, val_acc:0.556]
Epoch [5/120    avg_loss:1.977, val_acc:0.596]
Epoch [6/120    avg_loss:1.839, val_acc:0.594]
Epoch [7/120    avg_loss:1.704, val_acc:0.608]
Epoch [8/120    avg_loss:1.575, val_acc:0.631]
Epoch [9/120    avg_loss:1.458, val_acc:0.637]
Epoch [10/120    avg_loss:1.350, val_acc:0.677]
Epoch [11/120    avg_loss:1.264, val_acc:0.688]
Epoch [12/120    avg_loss:1.174, val_acc:0.717]
Epoch [13/120    avg_loss:1.085, val_acc:0.717]
Epoch [14/120    avg_loss:1.069, val_acc:0.742]
Epoch [15/120    avg_loss:0.980, val_acc:0.740]
Epoch [16/120    avg_loss:0.949, val_acc:0.769]
Epoch [17/120    avg_loss:0.883, val_acc:0.781]
Epoch [18/120    avg_loss:0.878, val_acc:0.804]
Epoch [19/120    avg_loss:0.825, val_acc:0.781]
Epoch [20/120    avg_loss:0.810, val_acc:0.838]
Epoch [21/120    avg_loss:0.745, val_acc:0.877]
Epoch [22/120    avg_loss:0.721, val_acc:0.825]
Epoch [23/120    avg_loss:0.723, val_acc:0.825]
Epoch [24/120    avg_loss:0.716, val_acc:0.894]
Epoch [25/120    avg_loss:0.608, val_acc:0.873]
Epoch [26/120    avg_loss:0.579, val_acc:0.863]
Epoch [27/120    avg_loss:0.585, val_acc:0.892]
Epoch [28/120    avg_loss:0.528, val_acc:0.912]
Epoch [29/120    avg_loss:0.477, val_acc:0.933]
Epoch [30/120    avg_loss:0.462, val_acc:0.919]
Epoch [31/120    avg_loss:0.437, val_acc:0.927]
Epoch [32/120    avg_loss:0.418, val_acc:0.944]
Epoch [33/120    avg_loss:0.400, val_acc:0.927]
Epoch [34/120    avg_loss:0.424, val_acc:0.923]
Epoch [35/120    avg_loss:0.406, val_acc:0.929]
Epoch [36/120    avg_loss:0.383, val_acc:0.933]
Epoch [37/120    avg_loss:0.305, val_acc:0.958]
Epoch [38/120    avg_loss:0.340, val_acc:0.931]
Epoch [39/120    avg_loss:0.347, val_acc:0.938]
Epoch [40/120    avg_loss:0.318, val_acc:0.923]
Epoch [41/120    avg_loss:0.296, val_acc:0.946]
Epoch [42/120    avg_loss:0.253, val_acc:0.960]
Epoch [43/120    avg_loss:0.264, val_acc:0.950]
Epoch [44/120    avg_loss:0.282, val_acc:0.954]
Epoch [45/120    avg_loss:0.269, val_acc:0.929]
Epoch [46/120    avg_loss:0.287, val_acc:0.942]
Epoch [47/120    avg_loss:0.229, val_acc:0.931]
Epoch [48/120    avg_loss:0.277, val_acc:0.960]
Epoch [49/120    avg_loss:0.271, val_acc:0.952]
Epoch [50/120    avg_loss:0.229, val_acc:0.969]
Epoch [51/120    avg_loss:0.259, val_acc:0.919]
Epoch [52/120    avg_loss:0.354, val_acc:0.948]
Epoch [53/120    avg_loss:0.309, val_acc:0.940]
Epoch [54/120    avg_loss:0.289, val_acc:0.927]
Epoch [55/120    avg_loss:0.220, val_acc:0.963]
Epoch [56/120    avg_loss:0.216, val_acc:0.944]
Epoch [57/120    avg_loss:0.303, val_acc:0.933]
Epoch [58/120    avg_loss:0.242, val_acc:0.963]
Epoch [59/120    avg_loss:0.223, val_acc:0.954]
Epoch [60/120    avg_loss:0.247, val_acc:0.948]
Epoch [61/120    avg_loss:0.213, val_acc:0.950]
Epoch [62/120    avg_loss:0.258, val_acc:0.940]
Epoch [63/120    avg_loss:0.215, val_acc:0.958]
Epoch [64/120    avg_loss:0.143, val_acc:0.963]
Epoch [65/120    avg_loss:0.146, val_acc:0.967]
Epoch [66/120    avg_loss:0.132, val_acc:0.963]
Epoch [67/120    avg_loss:0.136, val_acc:0.969]
Epoch [68/120    avg_loss:0.126, val_acc:0.969]
Epoch [69/120    avg_loss:0.135, val_acc:0.967]
Epoch [70/120    avg_loss:0.139, val_acc:0.967]
Epoch [71/120    avg_loss:0.136, val_acc:0.969]
Epoch [72/120    avg_loss:0.116, val_acc:0.969]
Epoch [73/120    avg_loss:0.142, val_acc:0.969]
Epoch [74/120    avg_loss:0.124, val_acc:0.967]
Epoch [75/120    avg_loss:0.111, val_acc:0.969]
Epoch [76/120    avg_loss:0.112, val_acc:0.965]
Epoch [77/120    avg_loss:0.112, val_acc:0.973]
Epoch [78/120    avg_loss:0.120, val_acc:0.969]
Epoch [79/120    avg_loss:0.114, val_acc:0.971]
Epoch [80/120    avg_loss:0.132, val_acc:0.973]
Epoch [81/120    avg_loss:0.102, val_acc:0.971]
Epoch [82/120    avg_loss:0.134, val_acc:0.971]
Epoch [83/120    avg_loss:0.119, val_acc:0.971]
Epoch [84/120    avg_loss:0.115, val_acc:0.977]
Epoch [85/120    avg_loss:0.123, val_acc:0.971]
Epoch [86/120    avg_loss:0.120, val_acc:0.977]
Epoch [87/120    avg_loss:0.121, val_acc:0.977]
Epoch [88/120    avg_loss:0.114, val_acc:0.971]
Epoch [89/120    avg_loss:0.116, val_acc:0.971]
Epoch [90/120    avg_loss:0.135, val_acc:0.973]
Epoch [91/120    avg_loss:0.120, val_acc:0.975]
Epoch [92/120    avg_loss:0.119, val_acc:0.975]
Epoch [93/120    avg_loss:0.113, val_acc:0.975]
Epoch [94/120    avg_loss:0.137, val_acc:0.973]
Epoch [95/120    avg_loss:0.107, val_acc:0.977]
Epoch [96/120    avg_loss:0.110, val_acc:0.977]
Epoch [97/120    avg_loss:0.101, val_acc:0.981]
Epoch [98/120    avg_loss:0.113, val_acc:0.981]
Epoch [99/120    avg_loss:0.096, val_acc:0.979]
Epoch [100/120    avg_loss:0.117, val_acc:0.977]
Epoch [101/120    avg_loss:0.107, val_acc:0.975]
Epoch [102/120    avg_loss:0.108, val_acc:0.979]
Epoch [103/120    avg_loss:0.110, val_acc:0.975]
Epoch [104/120    avg_loss:0.102, val_acc:0.981]
Epoch [105/120    avg_loss:0.110, val_acc:0.975]
Epoch [106/120    avg_loss:0.097, val_acc:0.981]
Epoch [107/120    avg_loss:0.091, val_acc:0.981]
Epoch [108/120    avg_loss:0.091, val_acc:0.977]
Epoch [109/120    avg_loss:0.093, val_acc:0.977]
Epoch [110/120    avg_loss:0.094, val_acc:0.975]
Epoch [111/120    avg_loss:0.105, val_acc:0.977]
Epoch [112/120    avg_loss:0.102, val_acc:0.981]
Epoch [113/120    avg_loss:0.095, val_acc:0.977]
Epoch [114/120    avg_loss:0.105, val_acc:0.977]
Epoch [115/120    avg_loss:0.099, val_acc:0.981]
Epoch [116/120    avg_loss:0.088, val_acc:0.981]
Epoch [117/120    avg_loss:0.099, val_acc:0.983]
Epoch [118/120    avg_loss:0.084, val_acc:0.981]
Epoch [119/120    avg_loss:0.087, val_acc:0.983]
Epoch [120/120    avg_loss:0.082, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   0   2   0   0   0   0   0]
 [  0   0   0 209  21   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  24 121   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  15 438   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.05970149253731

F1 scores:
[       nan 1.         0.96875    0.95216401 0.87136929 0.85512367
 1.         0.93181818 0.99742931 1.         1.         0.98049415
 0.98316498 1.        ]

Kappa:
0.9783969656642904
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2fd5d1b860>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.575, val_acc:0.385]
Epoch [2/120    avg_loss:2.349, val_acc:0.410]
Epoch [3/120    avg_loss:2.193, val_acc:0.487]
Epoch [4/120    avg_loss:2.060, val_acc:0.512]
Epoch [5/120    avg_loss:1.928, val_acc:0.550]
Epoch [6/120    avg_loss:1.777, val_acc:0.562]
Epoch [7/120    avg_loss:1.656, val_acc:0.606]
Epoch [8/120    avg_loss:1.555, val_acc:0.602]
Epoch [9/120    avg_loss:1.444, val_acc:0.698]
Epoch [10/120    avg_loss:1.340, val_acc:0.706]
Epoch [11/120    avg_loss:1.251, val_acc:0.735]
Epoch [12/120    avg_loss:1.158, val_acc:0.731]
Epoch [13/120    avg_loss:1.106, val_acc:0.723]
Epoch [14/120    avg_loss:1.022, val_acc:0.781]
Epoch [15/120    avg_loss:0.960, val_acc:0.821]
Epoch [16/120    avg_loss:0.884, val_acc:0.910]
Epoch [17/120    avg_loss:0.837, val_acc:0.842]
Epoch [18/120    avg_loss:0.791, val_acc:0.912]
Epoch [19/120    avg_loss:0.729, val_acc:0.887]
Epoch [20/120    avg_loss:0.682, val_acc:0.908]
Epoch [21/120    avg_loss:0.632, val_acc:0.885]
Epoch [22/120    avg_loss:0.619, val_acc:0.921]
Epoch [23/120    avg_loss:0.576, val_acc:0.915]
Epoch [24/120    avg_loss:0.540, val_acc:0.935]
Epoch [25/120    avg_loss:0.458, val_acc:0.921]
Epoch [26/120    avg_loss:0.423, val_acc:0.935]
Epoch [27/120    avg_loss:0.393, val_acc:0.925]
Epoch [28/120    avg_loss:0.403, val_acc:0.946]
Epoch [29/120    avg_loss:0.403, val_acc:0.919]
Epoch [30/120    avg_loss:0.430, val_acc:0.940]
Epoch [31/120    avg_loss:0.349, val_acc:0.958]
Epoch [32/120    avg_loss:0.316, val_acc:0.948]
Epoch [33/120    avg_loss:0.298, val_acc:0.946]
Epoch [34/120    avg_loss:0.303, val_acc:0.956]
Epoch [35/120    avg_loss:0.323, val_acc:0.946]
Epoch [36/120    avg_loss:0.287, val_acc:0.958]
Epoch [37/120    avg_loss:0.296, val_acc:0.929]
Epoch [38/120    avg_loss:0.286, val_acc:0.975]
Epoch [39/120    avg_loss:0.250, val_acc:0.921]
Epoch [40/120    avg_loss:0.249, val_acc:0.975]
Epoch [41/120    avg_loss:0.247, val_acc:0.948]
Epoch [42/120    avg_loss:0.225, val_acc:0.971]
Epoch [43/120    avg_loss:0.181, val_acc:0.979]
Epoch [44/120    avg_loss:0.210, val_acc:0.956]
Epoch [45/120    avg_loss:0.229, val_acc:0.975]
Epoch [46/120    avg_loss:0.262, val_acc:0.960]
Epoch [47/120    avg_loss:0.213, val_acc:0.975]
Epoch [48/120    avg_loss:0.175, val_acc:0.981]
Epoch [49/120    avg_loss:0.203, val_acc:0.979]
Epoch [50/120    avg_loss:0.164, val_acc:0.967]
Epoch [51/120    avg_loss:0.153, val_acc:0.960]
Epoch [52/120    avg_loss:0.220, val_acc:0.971]
Epoch [53/120    avg_loss:0.187, val_acc:0.985]
Epoch [54/120    avg_loss:0.168, val_acc:0.977]
Epoch [55/120    avg_loss:0.162, val_acc:0.983]
Epoch [56/120    avg_loss:0.128, val_acc:0.983]
Epoch [57/120    avg_loss:0.153, val_acc:0.981]
Epoch [58/120    avg_loss:0.214, val_acc:0.979]
Epoch [59/120    avg_loss:0.168, val_acc:0.992]
Epoch [60/120    avg_loss:0.125, val_acc:0.988]
Epoch [61/120    avg_loss:0.126, val_acc:0.977]
Epoch [62/120    avg_loss:0.116, val_acc:0.981]
Epoch [63/120    avg_loss:0.168, val_acc:0.971]
Epoch [64/120    avg_loss:0.174, val_acc:0.979]
Epoch [65/120    avg_loss:0.126, val_acc:0.967]
Epoch [66/120    avg_loss:0.174, val_acc:0.958]
Epoch [67/120    avg_loss:0.161, val_acc:0.977]
Epoch [68/120    avg_loss:0.159, val_acc:0.967]
Epoch [69/120    avg_loss:0.121, val_acc:0.977]
Epoch [70/120    avg_loss:0.133, val_acc:0.990]
Epoch [71/120    avg_loss:0.105, val_acc:0.969]
Epoch [72/120    avg_loss:0.105, val_acc:0.977]
Epoch [73/120    avg_loss:0.089, val_acc:0.979]
Epoch [74/120    avg_loss:0.088, val_acc:0.985]
Epoch [75/120    avg_loss:0.100, val_acc:0.985]
Epoch [76/120    avg_loss:0.095, val_acc:0.988]
Epoch [77/120    avg_loss:0.064, val_acc:0.988]
Epoch [78/120    avg_loss:0.070, val_acc:0.988]
Epoch [79/120    avg_loss:0.073, val_acc:0.988]
Epoch [80/120    avg_loss:0.069, val_acc:0.988]
Epoch [81/120    avg_loss:0.071, val_acc:0.985]
Epoch [82/120    avg_loss:0.070, val_acc:0.990]
Epoch [83/120    avg_loss:0.073, val_acc:0.988]
Epoch [84/120    avg_loss:0.065, val_acc:0.990]
Epoch [85/120    avg_loss:0.069, val_acc:0.992]
Epoch [86/120    avg_loss:0.074, val_acc:0.988]
Epoch [87/120    avg_loss:0.062, val_acc:0.990]
Epoch [88/120    avg_loss:0.058, val_acc:0.992]
Epoch [89/120    avg_loss:0.061, val_acc:0.990]
Epoch [90/120    avg_loss:0.061, val_acc:0.992]
Epoch [91/120    avg_loss:0.072, val_acc:0.992]
Epoch [92/120    avg_loss:0.060, val_acc:0.988]
Epoch [93/120    avg_loss:0.065, val_acc:0.992]
Epoch [94/120    avg_loss:0.059, val_acc:0.990]
Epoch [95/120    avg_loss:0.065, val_acc:0.992]
Epoch [96/120    avg_loss:0.063, val_acc:0.992]
Epoch [97/120    avg_loss:0.060, val_acc:0.992]
Epoch [98/120    avg_loss:0.057, val_acc:0.988]
Epoch [99/120    avg_loss:0.062, val_acc:0.990]
Epoch [100/120    avg_loss:0.063, val_acc:0.992]
Epoch [101/120    avg_loss:0.058, val_acc:0.990]
Epoch [102/120    avg_loss:0.083, val_acc:0.990]
Epoch [103/120    avg_loss:0.057, val_acc:0.992]
Epoch [104/120    avg_loss:0.064, val_acc:0.988]
Epoch [105/120    avg_loss:0.054, val_acc:0.992]
Epoch [106/120    avg_loss:0.054, val_acc:0.992]
Epoch [107/120    avg_loss:0.051, val_acc:0.992]
Epoch [108/120    avg_loss:0.057, val_acc:0.992]
Epoch [109/120    avg_loss:0.054, val_acc:0.988]
Epoch [110/120    avg_loss:0.049, val_acc:0.988]
Epoch [111/120    avg_loss:0.062, val_acc:0.992]
Epoch [112/120    avg_loss:0.059, val_acc:0.992]
Epoch [113/120    avg_loss:0.059, val_acc:0.992]
Epoch [114/120    avg_loss:0.053, val_acc:0.992]
Epoch [115/120    avg_loss:0.067, val_acc:0.988]
Epoch [116/120    avg_loss:0.059, val_acc:0.992]
Epoch [117/120    avg_loss:0.057, val_acc:0.990]
Epoch [118/120    avg_loss:0.059, val_acc:0.992]
Epoch [119/120    avg_loss:0.055, val_acc:0.992]
Epoch [120/120    avg_loss:0.050, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 0.99927061 0.98426966 0.99782135 0.93886463 0.9020979
 0.99756691 0.95604396 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9912161988418118
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f77c87fb8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.580, val_acc:0.310]
Epoch [2/120    avg_loss:2.369, val_acc:0.429]
Epoch [3/120    avg_loss:2.199, val_acc:0.448]
Epoch [4/120    avg_loss:2.062, val_acc:0.571]
Epoch [5/120    avg_loss:1.932, val_acc:0.627]
Epoch [6/120    avg_loss:1.789, val_acc:0.604]
Epoch [7/120    avg_loss:1.632, val_acc:0.631]
Epoch [8/120    avg_loss:1.491, val_acc:0.677]
Epoch [9/120    avg_loss:1.367, val_acc:0.692]
Epoch [10/120    avg_loss:1.213, val_acc:0.738]
Epoch [11/120    avg_loss:1.084, val_acc:0.754]
Epoch [12/120    avg_loss:1.018, val_acc:0.760]
Epoch [13/120    avg_loss:0.948, val_acc:0.765]
Epoch [14/120    avg_loss:0.862, val_acc:0.775]
Epoch [15/120    avg_loss:0.811, val_acc:0.800]
Epoch [16/120    avg_loss:0.714, val_acc:0.869]
Epoch [17/120    avg_loss:0.684, val_acc:0.819]
Epoch [18/120    avg_loss:0.681, val_acc:0.804]
Epoch [19/120    avg_loss:0.593, val_acc:0.854]
Epoch [20/120    avg_loss:0.584, val_acc:0.783]
Epoch [21/120    avg_loss:0.561, val_acc:0.863]
Epoch [22/120    avg_loss:0.551, val_acc:0.885]
Epoch [23/120    avg_loss:0.531, val_acc:0.890]
Epoch [24/120    avg_loss:0.531, val_acc:0.779]
Epoch [25/120    avg_loss:0.442, val_acc:0.917]
Epoch [26/120    avg_loss:0.418, val_acc:0.923]
Epoch [27/120    avg_loss:0.388, val_acc:0.940]
Epoch [28/120    avg_loss:0.336, val_acc:0.938]
Epoch [29/120    avg_loss:0.329, val_acc:0.944]
Epoch [30/120    avg_loss:0.317, val_acc:0.915]
Epoch [31/120    avg_loss:0.345, val_acc:0.946]
Epoch [32/120    avg_loss:0.304, val_acc:0.925]
Epoch [33/120    avg_loss:0.306, val_acc:0.940]
Epoch [34/120    avg_loss:0.241, val_acc:0.927]
Epoch [35/120    avg_loss:0.303, val_acc:0.944]
Epoch [36/120    avg_loss:0.292, val_acc:0.967]
Epoch [37/120    avg_loss:0.243, val_acc:0.942]
Epoch [38/120    avg_loss:0.247, val_acc:0.954]
Epoch [39/120    avg_loss:0.241, val_acc:0.948]
Epoch [40/120    avg_loss:0.304, val_acc:0.946]
Epoch [41/120    avg_loss:0.246, val_acc:0.967]
Epoch [42/120    avg_loss:0.237, val_acc:0.933]
Epoch [43/120    avg_loss:0.214, val_acc:0.965]
Epoch [44/120    avg_loss:0.267, val_acc:0.952]
Epoch [45/120    avg_loss:0.274, val_acc:0.973]
Epoch [46/120    avg_loss:0.297, val_acc:0.944]
Epoch [47/120    avg_loss:0.234, val_acc:0.967]
Epoch [48/120    avg_loss:0.207, val_acc:0.963]
Epoch [49/120    avg_loss:0.168, val_acc:0.963]
Epoch [50/120    avg_loss:0.192, val_acc:0.973]
Epoch [51/120    avg_loss:0.163, val_acc:0.950]
Epoch [52/120    avg_loss:0.132, val_acc:0.965]
Epoch [53/120    avg_loss:0.150, val_acc:0.940]
Epoch [54/120    avg_loss:0.163, val_acc:0.965]
Epoch [55/120    avg_loss:0.142, val_acc:0.975]
Epoch [56/120    avg_loss:0.172, val_acc:0.971]
Epoch [57/120    avg_loss:0.144, val_acc:0.969]
Epoch [58/120    avg_loss:0.174, val_acc:0.954]
Epoch [59/120    avg_loss:0.112, val_acc:0.977]
Epoch [60/120    avg_loss:0.125, val_acc:0.971]
Epoch [61/120    avg_loss:0.139, val_acc:0.977]
Epoch [62/120    avg_loss:0.165, val_acc:0.971]
Epoch [63/120    avg_loss:0.178, val_acc:0.946]
Epoch [64/120    avg_loss:0.163, val_acc:0.963]
Epoch [65/120    avg_loss:0.131, val_acc:0.969]
Epoch [66/120    avg_loss:0.099, val_acc:0.979]
Epoch [67/120    avg_loss:0.155, val_acc:0.958]
Epoch [68/120    avg_loss:0.153, val_acc:0.956]
Epoch [69/120    avg_loss:0.101, val_acc:0.971]
Epoch [70/120    avg_loss:0.089, val_acc:0.983]
Epoch [71/120    avg_loss:0.096, val_acc:0.973]
Epoch [72/120    avg_loss:0.090, val_acc:0.973]
Epoch [73/120    avg_loss:0.136, val_acc:0.969]
Epoch [74/120    avg_loss:0.100, val_acc:0.983]
Epoch [75/120    avg_loss:0.102, val_acc:0.973]
Epoch [76/120    avg_loss:0.075, val_acc:0.981]
Epoch [77/120    avg_loss:0.080, val_acc:0.977]
Epoch [78/120    avg_loss:0.094, val_acc:0.954]
Epoch [79/120    avg_loss:0.096, val_acc:0.979]
Epoch [80/120    avg_loss:0.105, val_acc:0.965]
Epoch [81/120    avg_loss:0.116, val_acc:0.977]
Epoch [82/120    avg_loss:0.084, val_acc:0.952]
Epoch [83/120    avg_loss:0.076, val_acc:0.977]
Epoch [84/120    avg_loss:0.091, val_acc:0.925]
Epoch [85/120    avg_loss:0.102, val_acc:0.969]
Epoch [86/120    avg_loss:0.111, val_acc:0.979]
Epoch [87/120    avg_loss:0.112, val_acc:0.952]
Epoch [88/120    avg_loss:0.121, val_acc:0.967]
Epoch [89/120    avg_loss:0.080, val_acc:0.981]
Epoch [90/120    avg_loss:0.089, val_acc:0.983]
Epoch [91/120    avg_loss:0.054, val_acc:0.975]
Epoch [92/120    avg_loss:0.049, val_acc:0.981]
Epoch [93/120    avg_loss:0.050, val_acc:0.983]
Epoch [94/120    avg_loss:0.046, val_acc:0.983]
Epoch [95/120    avg_loss:0.041, val_acc:0.983]
Epoch [96/120    avg_loss:0.039, val_acc:0.983]
Epoch [97/120    avg_loss:0.054, val_acc:0.983]
Epoch [98/120    avg_loss:0.043, val_acc:0.985]
Epoch [99/120    avg_loss:0.040, val_acc:0.985]
Epoch [100/120    avg_loss:0.042, val_acc:0.983]
Epoch [101/120    avg_loss:0.041, val_acc:0.985]
Epoch [102/120    avg_loss:0.036, val_acc:0.990]
Epoch [103/120    avg_loss:0.041, val_acc:0.990]
Epoch [104/120    avg_loss:0.039, val_acc:0.988]
Epoch [105/120    avg_loss:0.041, val_acc:0.985]
Epoch [106/120    avg_loss:0.038, val_acc:0.988]
Epoch [107/120    avg_loss:0.039, val_acc:0.988]
Epoch [108/120    avg_loss:0.037, val_acc:0.988]
Epoch [109/120    avg_loss:0.031, val_acc:0.988]
Epoch [110/120    avg_loss:0.031, val_acc:0.988]
Epoch [111/120    avg_loss:0.038, val_acc:0.988]
Epoch [112/120    avg_loss:0.036, val_acc:0.988]
Epoch [113/120    avg_loss:0.035, val_acc:0.988]
Epoch [114/120    avg_loss:0.033, val_acc:0.988]
Epoch [115/120    avg_loss:0.035, val_acc:0.988]
Epoch [116/120    avg_loss:0.037, val_acc:0.988]
Epoch [117/120    avg_loss:0.035, val_acc:0.988]
Epoch [118/120    avg_loss:0.033, val_acc:0.988]
Epoch [119/120    avg_loss:0.035, val_acc:0.988]
Epoch [120/120    avg_loss:0.031, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  14   0   0   0   0   0   0   2   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   1   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 1.         0.98426966 1.         0.92341357 0.88811189
 1.         0.96132597 0.99870968 1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9900292249299141
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdd32093908>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.590, val_acc:0.138]
Epoch [2/120    avg_loss:2.371, val_acc:0.406]
Epoch [3/120    avg_loss:2.227, val_acc:0.494]
Epoch [4/120    avg_loss:2.090, val_acc:0.515]
Epoch [5/120    avg_loss:1.962, val_acc:0.540]
Epoch [6/120    avg_loss:1.824, val_acc:0.558]
Epoch [7/120    avg_loss:1.718, val_acc:0.619]
Epoch [8/120    avg_loss:1.600, val_acc:0.621]
Epoch [9/120    avg_loss:1.489, val_acc:0.650]
Epoch [10/120    avg_loss:1.362, val_acc:0.719]
Epoch [11/120    avg_loss:1.258, val_acc:0.698]
Epoch [12/120    avg_loss:1.184, val_acc:0.727]
Epoch [13/120    avg_loss:1.029, val_acc:0.779]
Epoch [14/120    avg_loss:0.943, val_acc:0.777]
Epoch [15/120    avg_loss:0.904, val_acc:0.869]
Epoch [16/120    avg_loss:0.815, val_acc:0.842]
Epoch [17/120    avg_loss:0.805, val_acc:0.825]
Epoch [18/120    avg_loss:0.684, val_acc:0.802]
Epoch [19/120    avg_loss:0.683, val_acc:0.890]
Epoch [20/120    avg_loss:0.652, val_acc:0.904]
Epoch [21/120    avg_loss:0.568, val_acc:0.917]
Epoch [22/120    avg_loss:0.509, val_acc:0.906]
Epoch [23/120    avg_loss:0.520, val_acc:0.927]
Epoch [24/120    avg_loss:0.464, val_acc:0.923]
Epoch [25/120    avg_loss:0.496, val_acc:0.919]
Epoch [26/120    avg_loss:0.495, val_acc:0.898]
Epoch [27/120    avg_loss:0.504, val_acc:0.925]
Epoch [28/120    avg_loss:0.397, val_acc:0.923]
Epoch [29/120    avg_loss:0.395, val_acc:0.946]
Epoch [30/120    avg_loss:0.399, val_acc:0.933]
Epoch [31/120    avg_loss:0.354, val_acc:0.960]
Epoch [32/120    avg_loss:0.307, val_acc:0.927]
Epoch [33/120    avg_loss:0.284, val_acc:0.938]
Epoch [34/120    avg_loss:0.275, val_acc:0.965]
Epoch [35/120    avg_loss:0.303, val_acc:0.935]
Epoch [36/120    avg_loss:0.297, val_acc:0.940]
Epoch [37/120    avg_loss:0.302, val_acc:0.950]
Epoch [38/120    avg_loss:0.245, val_acc:0.958]
Epoch [39/120    avg_loss:0.250, val_acc:0.965]
Epoch [40/120    avg_loss:0.232, val_acc:0.975]
Epoch [41/120    avg_loss:0.249, val_acc:0.940]
Epoch [42/120    avg_loss:0.225, val_acc:0.979]
Epoch [43/120    avg_loss:0.228, val_acc:0.946]
Epoch [44/120    avg_loss:0.200, val_acc:0.981]
Epoch [45/120    avg_loss:0.192, val_acc:0.958]
Epoch [46/120    avg_loss:0.207, val_acc:0.973]
Epoch [47/120    avg_loss:0.217, val_acc:0.967]
Epoch [48/120    avg_loss:0.207, val_acc:0.958]
Epoch [49/120    avg_loss:0.178, val_acc:0.975]
Epoch [50/120    avg_loss:0.179, val_acc:0.973]
Epoch [51/120    avg_loss:0.173, val_acc:0.973]
Epoch [52/120    avg_loss:0.149, val_acc:0.977]
Epoch [53/120    avg_loss:0.172, val_acc:0.912]
Epoch [54/120    avg_loss:0.186, val_acc:0.963]
Epoch [55/120    avg_loss:0.224, val_acc:0.946]
Epoch [56/120    avg_loss:0.165, val_acc:0.960]
Epoch [57/120    avg_loss:0.147, val_acc:0.985]
Epoch [58/120    avg_loss:0.153, val_acc:0.975]
Epoch [59/120    avg_loss:0.198, val_acc:0.965]
Epoch [60/120    avg_loss:0.166, val_acc:0.971]
Epoch [61/120    avg_loss:0.176, val_acc:0.975]
Epoch [62/120    avg_loss:0.118, val_acc:0.977]
Epoch [63/120    avg_loss:0.091, val_acc:0.992]
Epoch [64/120    avg_loss:0.088, val_acc:0.981]
Epoch [65/120    avg_loss:0.107, val_acc:0.971]
Epoch [66/120    avg_loss:0.109, val_acc:0.956]
Epoch [67/120    avg_loss:0.120, val_acc:0.983]
Epoch [68/120    avg_loss:0.124, val_acc:0.983]
Epoch [69/120    avg_loss:0.123, val_acc:0.985]
Epoch [70/120    avg_loss:0.093, val_acc:0.983]
Epoch [71/120    avg_loss:0.089, val_acc:0.981]
Epoch [72/120    avg_loss:0.073, val_acc:0.985]
Epoch [73/120    avg_loss:0.078, val_acc:0.996]
Epoch [74/120    avg_loss:0.077, val_acc:0.979]
Epoch [75/120    avg_loss:0.088, val_acc:0.990]
Epoch [76/120    avg_loss:0.075, val_acc:0.988]
Epoch [77/120    avg_loss:0.097, val_acc:0.971]
Epoch [78/120    avg_loss:0.133, val_acc:0.967]
Epoch [79/120    avg_loss:0.115, val_acc:0.977]
Epoch [80/120    avg_loss:0.086, val_acc:0.985]
Epoch [81/120    avg_loss:0.099, val_acc:0.988]
Epoch [82/120    avg_loss:0.090, val_acc:0.990]
Epoch [83/120    avg_loss:0.068, val_acc:0.996]
Epoch [84/120    avg_loss:0.059, val_acc:0.996]
Epoch [85/120    avg_loss:0.051, val_acc:0.994]
Epoch [86/120    avg_loss:0.043, val_acc:0.992]
Epoch [87/120    avg_loss:0.057, val_acc:0.994]
Epoch [88/120    avg_loss:0.065, val_acc:0.996]
Epoch [89/120    avg_loss:0.061, val_acc:0.988]
Epoch [90/120    avg_loss:0.052, val_acc:0.990]
Epoch [91/120    avg_loss:0.039, val_acc:0.990]
Epoch [92/120    avg_loss:0.044, val_acc:0.996]
Epoch [93/120    avg_loss:0.035, val_acc:1.000]
Epoch [94/120    avg_loss:0.025, val_acc:1.000]
Epoch [95/120    avg_loss:0.040, val_acc:0.992]
Epoch [96/120    avg_loss:0.038, val_acc:0.996]
Epoch [97/120    avg_loss:0.049, val_acc:0.992]
Epoch [98/120    avg_loss:0.074, val_acc:0.994]
Epoch [99/120    avg_loss:0.064, val_acc:0.994]
Epoch [100/120    avg_loss:0.045, val_acc:0.994]
Epoch [101/120    avg_loss:0.034, val_acc:0.994]
Epoch [102/120    avg_loss:0.038, val_acc:0.996]
Epoch [103/120    avg_loss:0.033, val_acc:0.998]
Epoch [104/120    avg_loss:0.047, val_acc:0.996]
Epoch [105/120    avg_loss:0.050, val_acc:0.985]
Epoch [106/120    avg_loss:0.040, val_acc:0.992]
Epoch [107/120    avg_loss:0.037, val_acc:0.994]
Epoch [108/120    avg_loss:0.022, val_acc:0.996]
Epoch [109/120    avg_loss:0.028, val_acc:0.998]
Epoch [110/120    avg_loss:0.027, val_acc:0.998]
Epoch [111/120    avg_loss:0.025, val_acc:1.000]
Epoch [112/120    avg_loss:0.022, val_acc:1.000]
Epoch [113/120    avg_loss:0.021, val_acc:1.000]
Epoch [114/120    avg_loss:0.021, val_acc:1.000]
Epoch [115/120    avg_loss:0.025, val_acc:1.000]
Epoch [116/120    avg_loss:0.021, val_acc:1.000]
Epoch [117/120    avg_loss:0.022, val_acc:1.000]
Epoch [118/120    avg_loss:0.019, val_acc:1.000]
Epoch [119/120    avg_loss:0.022, val_acc:1.000]
Epoch [120/120    avg_loss:0.025, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 0.99780541 0.9977221  1.         0.93965517 0.9
 0.99277108 0.99465241 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9924039826048772
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7166902898>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.585, val_acc:0.127]
Epoch [2/120    avg_loss:2.358, val_acc:0.346]
Epoch [3/120    avg_loss:2.203, val_acc:0.448]
Epoch [4/120    avg_loss:2.078, val_acc:0.550]
Epoch [5/120    avg_loss:1.943, val_acc:0.583]
Epoch [6/120    avg_loss:1.808, val_acc:0.585]
Epoch [7/120    avg_loss:1.652, val_acc:0.625]
Epoch [8/120    avg_loss:1.527, val_acc:0.635]
Epoch [9/120    avg_loss:1.434, val_acc:0.658]
Epoch [10/120    avg_loss:1.328, val_acc:0.698]
Epoch [11/120    avg_loss:1.234, val_acc:0.685]
Epoch [12/120    avg_loss:1.169, val_acc:0.742]
Epoch [13/120    avg_loss:1.088, val_acc:0.825]
Epoch [14/120    avg_loss:1.000, val_acc:0.881]
Epoch [15/120    avg_loss:0.940, val_acc:0.806]
Epoch [16/120    avg_loss:0.888, val_acc:0.829]
Epoch [17/120    avg_loss:0.813, val_acc:0.865]
Epoch [18/120    avg_loss:0.755, val_acc:0.865]
Epoch [19/120    avg_loss:0.721, val_acc:0.785]
Epoch [20/120    avg_loss:0.715, val_acc:0.887]
Epoch [21/120    avg_loss:0.676, val_acc:0.898]
Epoch [22/120    avg_loss:0.623, val_acc:0.906]
Epoch [23/120    avg_loss:0.597, val_acc:0.892]
Epoch [24/120    avg_loss:0.652, val_acc:0.900]
Epoch [25/120    avg_loss:0.580, val_acc:0.887]
Epoch [26/120    avg_loss:0.522, val_acc:0.904]
Epoch [27/120    avg_loss:0.523, val_acc:0.917]
Epoch [28/120    avg_loss:0.492, val_acc:0.946]
Epoch [29/120    avg_loss:0.450, val_acc:0.915]
Epoch [30/120    avg_loss:0.452, val_acc:0.919]
Epoch [31/120    avg_loss:0.441, val_acc:0.896]
Epoch [32/120    avg_loss:0.524, val_acc:0.929]
Epoch [33/120    avg_loss:0.449, val_acc:0.946]
Epoch [34/120    avg_loss:0.358, val_acc:0.929]
Epoch [35/120    avg_loss:0.438, val_acc:0.902]
Epoch [36/120    avg_loss:0.466, val_acc:0.877]
Epoch [37/120    avg_loss:0.372, val_acc:0.923]
Epoch [38/120    avg_loss:0.355, val_acc:0.929]
Epoch [39/120    avg_loss:0.353, val_acc:0.950]
Epoch [40/120    avg_loss:0.348, val_acc:0.898]
Epoch [41/120    avg_loss:0.344, val_acc:0.931]
Epoch [42/120    avg_loss:0.359, val_acc:0.954]
Epoch [43/120    avg_loss:0.351, val_acc:0.944]
Epoch [44/120    avg_loss:0.279, val_acc:0.965]
Epoch [45/120    avg_loss:0.251, val_acc:0.923]
Epoch [46/120    avg_loss:0.266, val_acc:0.963]
Epoch [47/120    avg_loss:0.228, val_acc:0.948]
Epoch [48/120    avg_loss:0.271, val_acc:0.935]
Epoch [49/120    avg_loss:0.325, val_acc:0.967]
Epoch [50/120    avg_loss:0.270, val_acc:0.958]
Epoch [51/120    avg_loss:0.199, val_acc:0.967]
Epoch [52/120    avg_loss:0.190, val_acc:0.971]
Epoch [53/120    avg_loss:0.178, val_acc:0.971]
Epoch [54/120    avg_loss:0.183, val_acc:0.946]
Epoch [55/120    avg_loss:0.192, val_acc:0.969]
Epoch [56/120    avg_loss:0.180, val_acc:0.973]
Epoch [57/120    avg_loss:0.216, val_acc:0.923]
Epoch [58/120    avg_loss:0.233, val_acc:0.967]
Epoch [59/120    avg_loss:0.231, val_acc:0.948]
Epoch [60/120    avg_loss:0.220, val_acc:0.960]
Epoch [61/120    avg_loss:0.173, val_acc:0.956]
Epoch [62/120    avg_loss:0.175, val_acc:0.967]
Epoch [63/120    avg_loss:0.160, val_acc:0.977]
Epoch [64/120    avg_loss:0.154, val_acc:0.958]
Epoch [65/120    avg_loss:0.155, val_acc:0.965]
Epoch [66/120    avg_loss:0.143, val_acc:0.965]
Epoch [67/120    avg_loss:0.125, val_acc:0.973]
Epoch [68/120    avg_loss:0.147, val_acc:0.969]
Epoch [69/120    avg_loss:0.135, val_acc:0.969]
Epoch [70/120    avg_loss:0.122, val_acc:0.967]
Epoch [71/120    avg_loss:0.162, val_acc:0.977]
Epoch [72/120    avg_loss:0.105, val_acc:0.985]
Epoch [73/120    avg_loss:0.126, val_acc:0.981]
Epoch [74/120    avg_loss:0.117, val_acc:0.965]
Epoch [75/120    avg_loss:0.120, val_acc:0.977]
Epoch [76/120    avg_loss:0.112, val_acc:0.983]
Epoch [77/120    avg_loss:0.113, val_acc:0.981]
Epoch [78/120    avg_loss:0.107, val_acc:0.975]
Epoch [79/120    avg_loss:0.133, val_acc:0.965]
Epoch [80/120    avg_loss:0.103, val_acc:0.985]
Epoch [81/120    avg_loss:0.108, val_acc:0.985]
Epoch [82/120    avg_loss:0.114, val_acc:0.973]
Epoch [83/120    avg_loss:0.115, val_acc:0.948]
Epoch [84/120    avg_loss:0.089, val_acc:0.983]
Epoch [85/120    avg_loss:0.079, val_acc:0.988]
Epoch [86/120    avg_loss:0.107, val_acc:0.958]
Epoch [87/120    avg_loss:0.069, val_acc:0.983]
Epoch [88/120    avg_loss:0.081, val_acc:0.981]
Epoch [89/120    avg_loss:0.067, val_acc:0.988]
Epoch [90/120    avg_loss:0.070, val_acc:0.985]
Epoch [91/120    avg_loss:0.067, val_acc:0.977]
Epoch [92/120    avg_loss:0.060, val_acc:0.985]
Epoch [93/120    avg_loss:0.049, val_acc:0.983]
Epoch [94/120    avg_loss:0.049, val_acc:0.985]
Epoch [95/120    avg_loss:0.055, val_acc:0.985]
Epoch [96/120    avg_loss:0.048, val_acc:0.973]
Epoch [97/120    avg_loss:0.046, val_acc:0.985]
Epoch [98/120    avg_loss:0.041, val_acc:0.994]
Epoch [99/120    avg_loss:0.060, val_acc:0.977]
Epoch [100/120    avg_loss:0.100, val_acc:0.979]
Epoch [101/120    avg_loss:0.064, val_acc:0.990]
Epoch [102/120    avg_loss:0.054, val_acc:0.990]
Epoch [103/120    avg_loss:0.063, val_acc:0.992]
Epoch [104/120    avg_loss:0.048, val_acc:0.994]
Epoch [105/120    avg_loss:0.048, val_acc:0.985]
Epoch [106/120    avg_loss:0.071, val_acc:0.988]
Epoch [107/120    avg_loss:0.051, val_acc:0.983]
Epoch [108/120    avg_loss:0.038, val_acc:0.988]
Epoch [109/120    avg_loss:0.037, val_acc:0.988]
Epoch [110/120    avg_loss:0.050, val_acc:0.990]
Epoch [111/120    avg_loss:0.027, val_acc:0.994]
Epoch [112/120    avg_loss:0.040, val_acc:0.983]
Epoch [113/120    avg_loss:0.038, val_acc:0.983]
Epoch [114/120    avg_loss:0.138, val_acc:0.981]
Epoch [115/120    avg_loss:0.096, val_acc:0.985]
Epoch [116/120    avg_loss:0.079, val_acc:0.990]
Epoch [117/120    avg_loss:0.095, val_acc:0.973]
Epoch [118/120    avg_loss:0.073, val_acc:0.981]
Epoch [119/120    avg_loss:0.075, val_acc:0.985]
Epoch [120/120    avg_loss:0.057, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   6 208  16   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  10   0   0   0   0   0   0   1   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.5501066098081

F1 scores:
[       nan 1.         0.95217391 0.94977169 0.90376569 0.89679715
 1.         0.90697674 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9838557322477344
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcb20cc1908>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.591, val_acc:0.402]
Epoch [2/120    avg_loss:2.383, val_acc:0.504]
Epoch [3/120    avg_loss:2.242, val_acc:0.573]
Epoch [4/120    avg_loss:2.100, val_acc:0.625]
Epoch [5/120    avg_loss:1.939, val_acc:0.585]
Epoch [6/120    avg_loss:1.782, val_acc:0.608]
Epoch [7/120    avg_loss:1.626, val_acc:0.644]
Epoch [8/120    avg_loss:1.499, val_acc:0.673]
Epoch [9/120    avg_loss:1.393, val_acc:0.708]
Epoch [10/120    avg_loss:1.277, val_acc:0.715]
Epoch [11/120    avg_loss:1.183, val_acc:0.742]
Epoch [12/120    avg_loss:1.109, val_acc:0.738]
Epoch [13/120    avg_loss:1.032, val_acc:0.754]
Epoch [14/120    avg_loss:0.941, val_acc:0.742]
Epoch [15/120    avg_loss:0.863, val_acc:0.787]
Epoch [16/120    avg_loss:0.833, val_acc:0.850]
Epoch [17/120    avg_loss:0.756, val_acc:0.894]
Epoch [18/120    avg_loss:0.695, val_acc:0.910]
Epoch [19/120    avg_loss:0.655, val_acc:0.863]
Epoch [20/120    avg_loss:0.568, val_acc:0.912]
Epoch [21/120    avg_loss:0.616, val_acc:0.910]
Epoch [22/120    avg_loss:0.541, val_acc:0.910]
Epoch [23/120    avg_loss:0.473, val_acc:0.902]
Epoch [24/120    avg_loss:0.466, val_acc:0.919]
Epoch [25/120    avg_loss:0.438, val_acc:0.925]
Epoch [26/120    avg_loss:0.403, val_acc:0.948]
Epoch [27/120    avg_loss:0.388, val_acc:0.950]
Epoch [28/120    avg_loss:0.339, val_acc:0.894]
Epoch [29/120    avg_loss:0.332, val_acc:0.948]
Epoch [30/120    avg_loss:0.283, val_acc:0.944]
Epoch [31/120    avg_loss:0.327, val_acc:0.948]
Epoch [32/120    avg_loss:0.310, val_acc:0.950]
Epoch [33/120    avg_loss:0.321, val_acc:0.942]
Epoch [34/120    avg_loss:0.286, val_acc:0.919]
Epoch [35/120    avg_loss:0.309, val_acc:0.929]
Epoch [36/120    avg_loss:0.291, val_acc:0.952]
Epoch [37/120    avg_loss:0.251, val_acc:0.963]
Epoch [38/120    avg_loss:0.272, val_acc:0.973]
Epoch [39/120    avg_loss:0.216, val_acc:0.973]
Epoch [40/120    avg_loss:0.285, val_acc:0.958]
Epoch [41/120    avg_loss:0.277, val_acc:0.948]
Epoch [42/120    avg_loss:0.297, val_acc:0.971]
Epoch [43/120    avg_loss:0.234, val_acc:0.954]
Epoch [44/120    avg_loss:0.202, val_acc:0.954]
Epoch [45/120    avg_loss:0.216, val_acc:0.977]
Epoch [46/120    avg_loss:0.178, val_acc:0.973]
Epoch [47/120    avg_loss:0.173, val_acc:0.963]
Epoch [48/120    avg_loss:0.169, val_acc:0.965]
Epoch [49/120    avg_loss:0.217, val_acc:0.979]
Epoch [50/120    avg_loss:0.182, val_acc:0.985]
Epoch [51/120    avg_loss:0.156, val_acc:0.985]
Epoch [52/120    avg_loss:0.158, val_acc:0.981]
Epoch [53/120    avg_loss:0.150, val_acc:0.963]
Epoch [54/120    avg_loss:0.150, val_acc:0.975]
Epoch [55/120    avg_loss:0.137, val_acc:0.985]
Epoch [56/120    avg_loss:0.113, val_acc:0.985]
Epoch [57/120    avg_loss:0.167, val_acc:0.979]
Epoch [58/120    avg_loss:0.202, val_acc:0.952]
Epoch [59/120    avg_loss:0.141, val_acc:0.946]
Epoch [60/120    avg_loss:0.152, val_acc:0.969]
Epoch [61/120    avg_loss:0.139, val_acc:0.983]
Epoch [62/120    avg_loss:0.123, val_acc:0.967]
Epoch [63/120    avg_loss:0.139, val_acc:0.981]
Epoch [64/120    avg_loss:0.136, val_acc:0.973]
Epoch [65/120    avg_loss:0.112, val_acc:0.983]
Epoch [66/120    avg_loss:0.123, val_acc:0.979]
Epoch [67/120    avg_loss:0.175, val_acc:0.973]
Epoch [68/120    avg_loss:0.119, val_acc:0.983]
Epoch [69/120    avg_loss:0.120, val_acc:0.971]
Epoch [70/120    avg_loss:0.109, val_acc:0.985]
Epoch [71/120    avg_loss:0.080, val_acc:0.985]
Epoch [72/120    avg_loss:0.084, val_acc:0.983]
Epoch [73/120    avg_loss:0.090, val_acc:0.985]
Epoch [74/120    avg_loss:0.072, val_acc:0.983]
Epoch [75/120    avg_loss:0.077, val_acc:0.983]
Epoch [76/120    avg_loss:0.072, val_acc:0.983]
Epoch [77/120    avg_loss:0.071, val_acc:0.983]
Epoch [78/120    avg_loss:0.066, val_acc:0.988]
Epoch [79/120    avg_loss:0.077, val_acc:0.990]
Epoch [80/120    avg_loss:0.072, val_acc:0.985]
Epoch [81/120    avg_loss:0.065, val_acc:0.985]
Epoch [82/120    avg_loss:0.076, val_acc:0.985]
Epoch [83/120    avg_loss:0.062, val_acc:0.985]
Epoch [84/120    avg_loss:0.072, val_acc:0.985]
Epoch [85/120    avg_loss:0.062, val_acc:0.988]
Epoch [86/120    avg_loss:0.074, val_acc:0.985]
Epoch [87/120    avg_loss:0.069, val_acc:0.985]
Epoch [88/120    avg_loss:0.064, val_acc:0.983]
Epoch [89/120    avg_loss:0.068, val_acc:0.983]
Epoch [90/120    avg_loss:0.073, val_acc:0.990]
Epoch [91/120    avg_loss:0.063, val_acc:0.990]
Epoch [92/120    avg_loss:0.063, val_acc:0.985]
Epoch [93/120    avg_loss:0.058, val_acc:0.985]
Epoch [94/120    avg_loss:0.068, val_acc:0.985]
Epoch [95/120    avg_loss:0.061, val_acc:0.985]
Epoch [96/120    avg_loss:0.060, val_acc:0.985]
Epoch [97/120    avg_loss:0.066, val_acc:0.985]
Epoch [98/120    avg_loss:0.063, val_acc:0.985]
Epoch [99/120    avg_loss:0.064, val_acc:0.988]
Epoch [100/120    avg_loss:0.058, val_acc:0.985]
Epoch [101/120    avg_loss:0.065, val_acc:0.985]
Epoch [102/120    avg_loss:0.067, val_acc:0.990]
Epoch [103/120    avg_loss:0.067, val_acc:0.990]
Epoch [104/120    avg_loss:0.069, val_acc:0.990]
Epoch [105/120    avg_loss:0.069, val_acc:0.992]
Epoch [106/120    avg_loss:0.049, val_acc:0.992]
Epoch [107/120    avg_loss:0.054, val_acc:0.992]
Epoch [108/120    avg_loss:0.053, val_acc:0.988]
Epoch [109/120    avg_loss:0.054, val_acc:0.988]
Epoch [110/120    avg_loss:0.048, val_acc:0.985]
Epoch [111/120    avg_loss:0.055, val_acc:0.988]
Epoch [112/120    avg_loss:0.051, val_acc:0.992]
Epoch [113/120    avg_loss:0.047, val_acc:0.992]
Epoch [114/120    avg_loss:0.053, val_acc:0.990]
Epoch [115/120    avg_loss:0.055, val_acc:0.992]
Epoch [116/120    avg_loss:0.056, val_acc:0.992]
Epoch [117/120    avg_loss:0.063, val_acc:0.990]
Epoch [118/120    avg_loss:0.064, val_acc:0.990]
Epoch [119/120    avg_loss:0.059, val_acc:0.990]
Epoch [120/120    avg_loss:0.063, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 203  22   0   0   0   0   0   0   2   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 0.99927061 0.98871332 1.         0.91855204 0.88666667
 0.99756691 0.9726776  1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9900296074799506
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd17ce9c908>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.602, val_acc:0.412]
Epoch [2/120    avg_loss:2.379, val_acc:0.383]
Epoch [3/120    avg_loss:2.222, val_acc:0.460]
Epoch [4/120    avg_loss:2.121, val_acc:0.494]
Epoch [5/120    avg_loss:2.002, val_acc:0.608]
Epoch [6/120    avg_loss:1.894, val_acc:0.606]
Epoch [7/120    avg_loss:1.754, val_acc:0.596]
Epoch [8/120    avg_loss:1.635, val_acc:0.623]
Epoch [9/120    avg_loss:1.511, val_acc:0.667]
Epoch [10/120    avg_loss:1.395, val_acc:0.696]
Epoch [11/120    avg_loss:1.256, val_acc:0.702]
Epoch [12/120    avg_loss:1.160, val_acc:0.746]
Epoch [13/120    avg_loss:1.075, val_acc:0.752]
Epoch [14/120    avg_loss:0.966, val_acc:0.787]
Epoch [15/120    avg_loss:0.878, val_acc:0.800]
Epoch [16/120    avg_loss:0.800, val_acc:0.787]
Epoch [17/120    avg_loss:0.763, val_acc:0.802]
Epoch [18/120    avg_loss:0.720, val_acc:0.840]
Epoch [19/120    avg_loss:0.662, val_acc:0.812]
Epoch [20/120    avg_loss:0.635, val_acc:0.806]
Epoch [21/120    avg_loss:0.608, val_acc:0.808]
Epoch [22/120    avg_loss:0.617, val_acc:0.867]
Epoch [23/120    avg_loss:0.590, val_acc:0.919]
Epoch [24/120    avg_loss:0.514, val_acc:0.879]
Epoch [25/120    avg_loss:0.520, val_acc:0.929]
Epoch [26/120    avg_loss:0.452, val_acc:0.896]
Epoch [27/120    avg_loss:0.447, val_acc:0.929]
Epoch [28/120    avg_loss:0.395, val_acc:0.894]
Epoch [29/120    avg_loss:0.497, val_acc:0.898]
Epoch [30/120    avg_loss:0.481, val_acc:0.915]
Epoch [31/120    avg_loss:0.406, val_acc:0.929]
Epoch [32/120    avg_loss:0.383, val_acc:0.927]
Epoch [33/120    avg_loss:0.334, val_acc:0.940]
Epoch [34/120    avg_loss:0.338, val_acc:0.929]
Epoch [35/120    avg_loss:0.348, val_acc:0.938]
Epoch [36/120    avg_loss:0.279, val_acc:0.927]
Epoch [37/120    avg_loss:0.335, val_acc:0.944]
Epoch [38/120    avg_loss:0.298, val_acc:0.935]
Epoch [39/120    avg_loss:0.241, val_acc:0.950]
Epoch [40/120    avg_loss:0.227, val_acc:0.948]
Epoch [41/120    avg_loss:0.250, val_acc:0.946]
Epoch [42/120    avg_loss:0.237, val_acc:0.952]
Epoch [43/120    avg_loss:0.217, val_acc:0.952]
Epoch [44/120    avg_loss:0.285, val_acc:0.935]
Epoch [45/120    avg_loss:0.252, val_acc:0.942]
Epoch [46/120    avg_loss:0.242, val_acc:0.927]
Epoch [47/120    avg_loss:0.252, val_acc:0.948]
Epoch [48/120    avg_loss:0.194, val_acc:0.960]
Epoch [49/120    avg_loss:0.210, val_acc:0.965]
Epoch [50/120    avg_loss:0.192, val_acc:0.956]
Epoch [51/120    avg_loss:0.225, val_acc:0.954]
Epoch [52/120    avg_loss:0.195, val_acc:0.967]
Epoch [53/120    avg_loss:0.213, val_acc:0.963]
Epoch [54/120    avg_loss:0.211, val_acc:0.954]
Epoch [55/120    avg_loss:0.200, val_acc:0.965]
Epoch [56/120    avg_loss:0.186, val_acc:0.950]
Epoch [57/120    avg_loss:0.175, val_acc:0.954]
Epoch [58/120    avg_loss:0.170, val_acc:0.960]
Epoch [59/120    avg_loss:0.175, val_acc:0.965]
Epoch [60/120    avg_loss:0.150, val_acc:0.965]
Epoch [61/120    avg_loss:0.145, val_acc:0.979]
Epoch [62/120    avg_loss:0.157, val_acc:0.967]
Epoch [63/120    avg_loss:0.147, val_acc:0.958]
Epoch [64/120    avg_loss:0.178, val_acc:0.921]
Epoch [65/120    avg_loss:0.159, val_acc:0.958]
Epoch [66/120    avg_loss:0.146, val_acc:0.956]
Epoch [67/120    avg_loss:0.114, val_acc:0.973]
Epoch [68/120    avg_loss:0.107, val_acc:0.977]
Epoch [69/120    avg_loss:0.104, val_acc:0.965]
Epoch [70/120    avg_loss:0.156, val_acc:0.973]
Epoch [71/120    avg_loss:0.126, val_acc:0.979]
Epoch [72/120    avg_loss:0.167, val_acc:0.965]
Epoch [73/120    avg_loss:0.145, val_acc:0.979]
Epoch [74/120    avg_loss:0.096, val_acc:0.983]
Epoch [75/120    avg_loss:0.114, val_acc:0.952]
Epoch [76/120    avg_loss:0.115, val_acc:0.973]
Epoch [77/120    avg_loss:0.138, val_acc:0.975]
Epoch [78/120    avg_loss:0.126, val_acc:0.973]
Epoch [79/120    avg_loss:0.125, val_acc:0.952]
Epoch [80/120    avg_loss:0.149, val_acc:0.958]
Epoch [81/120    avg_loss:0.120, val_acc:0.958]
Epoch [82/120    avg_loss:0.143, val_acc:0.967]
Epoch [83/120    avg_loss:0.102, val_acc:0.967]
Epoch [84/120    avg_loss:0.134, val_acc:0.971]
Epoch [85/120    avg_loss:0.095, val_acc:0.954]
Epoch [86/120    avg_loss:0.149, val_acc:0.971]
Epoch [87/120    avg_loss:0.127, val_acc:0.958]
Epoch [88/120    avg_loss:0.102, val_acc:0.971]
Epoch [89/120    avg_loss:0.085, val_acc:0.975]
Epoch [90/120    avg_loss:0.063, val_acc:0.975]
Epoch [91/120    avg_loss:0.058, val_acc:0.979]
Epoch [92/120    avg_loss:0.065, val_acc:0.977]
Epoch [93/120    avg_loss:0.067, val_acc:0.977]
Epoch [94/120    avg_loss:0.057, val_acc:0.977]
Epoch [95/120    avg_loss:0.064, val_acc:0.979]
Epoch [96/120    avg_loss:0.054, val_acc:0.979]
Epoch [97/120    avg_loss:0.055, val_acc:0.981]
Epoch [98/120    avg_loss:0.054, val_acc:0.985]
Epoch [99/120    avg_loss:0.053, val_acc:0.983]
Epoch [100/120    avg_loss:0.048, val_acc:0.979]
Epoch [101/120    avg_loss:0.045, val_acc:0.979]
Epoch [102/120    avg_loss:0.055, val_acc:0.979]
Epoch [103/120    avg_loss:0.051, val_acc:0.981]
Epoch [104/120    avg_loss:0.057, val_acc:0.977]
Epoch [105/120    avg_loss:0.048, val_acc:0.979]
Epoch [106/120    avg_loss:0.045, val_acc:0.977]
Epoch [107/120    avg_loss:0.051, val_acc:0.979]
Epoch [108/120    avg_loss:0.055, val_acc:0.979]
Epoch [109/120    avg_loss:0.051, val_acc:0.979]
Epoch [110/120    avg_loss:0.053, val_acc:0.981]
Epoch [111/120    avg_loss:0.049, val_acc:0.981]
Epoch [112/120    avg_loss:0.043, val_acc:0.981]
Epoch [113/120    avg_loss:0.050, val_acc:0.981]
Epoch [114/120    avg_loss:0.038, val_acc:0.981]
Epoch [115/120    avg_loss:0.056, val_acc:0.981]
Epoch [116/120    avg_loss:0.042, val_acc:0.981]
Epoch [117/120    avg_loss:0.050, val_acc:0.981]
Epoch [118/120    avg_loss:0.048, val_acc:0.979]
Epoch [119/120    avg_loss:0.034, val_acc:0.979]
Epoch [120/120    avg_loss:0.047, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 207  16   0   0   0   0   0   0   4   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 1.         0.98648649 1.         0.94520548 0.93377483
 1.         0.96703297 1.         1.         1.         0.98817346
 0.98560354 1.        ]

Kappa:
0.9907420585985023
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3a4f731908>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.579, val_acc:0.319]
Epoch [2/120    avg_loss:2.353, val_acc:0.377]
Epoch [3/120    avg_loss:2.204, val_acc:0.469]
Epoch [4/120    avg_loss:2.088, val_acc:0.515]
Epoch [5/120    avg_loss:1.983, val_acc:0.546]
Epoch [6/120    avg_loss:1.854, val_acc:0.575]
Epoch [7/120    avg_loss:1.750, val_acc:0.604]
Epoch [8/120    avg_loss:1.630, val_acc:0.627]
Epoch [9/120    avg_loss:1.517, val_acc:0.683]
Epoch [10/120    avg_loss:1.407, val_acc:0.688]
Epoch [11/120    avg_loss:1.338, val_acc:0.696]
Epoch [12/120    avg_loss:1.243, val_acc:0.681]
Epoch [13/120    avg_loss:1.157, val_acc:0.729]
Epoch [14/120    avg_loss:1.066, val_acc:0.767]
Epoch [15/120    avg_loss:0.969, val_acc:0.781]
Epoch [16/120    avg_loss:0.911, val_acc:0.833]
Epoch [17/120    avg_loss:0.866, val_acc:0.833]
Epoch [18/120    avg_loss:0.782, val_acc:0.819]
Epoch [19/120    avg_loss:0.748, val_acc:0.923]
Epoch [20/120    avg_loss:0.681, val_acc:0.867]
Epoch [21/120    avg_loss:0.664, val_acc:0.842]
Epoch [22/120    avg_loss:0.657, val_acc:0.910]
Epoch [23/120    avg_loss:0.613, val_acc:0.915]
Epoch [24/120    avg_loss:0.541, val_acc:0.933]
Epoch [25/120    avg_loss:0.537, val_acc:0.850]
Epoch [26/120    avg_loss:0.541, val_acc:0.919]
Epoch [27/120    avg_loss:0.464, val_acc:0.912]
Epoch [28/120    avg_loss:0.515, val_acc:0.917]
Epoch [29/120    avg_loss:0.465, val_acc:0.919]
Epoch [30/120    avg_loss:0.434, val_acc:0.925]
Epoch [31/120    avg_loss:0.423, val_acc:0.940]
Epoch [32/120    avg_loss:0.427, val_acc:0.929]
Epoch [33/120    avg_loss:0.427, val_acc:0.923]
Epoch [34/120    avg_loss:0.363, val_acc:0.960]
Epoch [35/120    avg_loss:0.364, val_acc:0.912]
Epoch [36/120    avg_loss:0.385, val_acc:0.940]
Epoch [37/120    avg_loss:0.321, val_acc:0.933]
Epoch [38/120    avg_loss:0.276, val_acc:0.960]
Epoch [39/120    avg_loss:0.297, val_acc:0.950]
Epoch [40/120    avg_loss:0.313, val_acc:0.950]
Epoch [41/120    avg_loss:0.269, val_acc:0.950]
Epoch [42/120    avg_loss:0.245, val_acc:0.956]
Epoch [43/120    avg_loss:0.284, val_acc:0.946]
Epoch [44/120    avg_loss:0.284, val_acc:0.842]
Epoch [45/120    avg_loss:0.334, val_acc:0.958]
Epoch [46/120    avg_loss:0.255, val_acc:0.946]
Epoch [47/120    avg_loss:0.271, val_acc:0.952]
Epoch [48/120    avg_loss:0.264, val_acc:0.967]
Epoch [49/120    avg_loss:0.261, val_acc:0.944]
Epoch [50/120    avg_loss:0.255, val_acc:0.954]
Epoch [51/120    avg_loss:0.230, val_acc:0.958]
Epoch [52/120    avg_loss:0.188, val_acc:0.958]
Epoch [53/120    avg_loss:0.165, val_acc:0.963]
Epoch [54/120    avg_loss:0.173, val_acc:0.971]
Epoch [55/120    avg_loss:0.204, val_acc:0.956]
Epoch [56/120    avg_loss:0.225, val_acc:0.971]
Epoch [57/120    avg_loss:0.206, val_acc:0.958]
Epoch [58/120    avg_loss:0.294, val_acc:0.940]
Epoch [59/120    avg_loss:0.204, val_acc:0.973]
Epoch [60/120    avg_loss:0.207, val_acc:0.975]
Epoch [61/120    avg_loss:0.229, val_acc:0.954]
Epoch [62/120    avg_loss:0.195, val_acc:0.967]
Epoch [63/120    avg_loss:0.163, val_acc:0.969]
Epoch [64/120    avg_loss:0.154, val_acc:0.971]
Epoch [65/120    avg_loss:0.142, val_acc:0.977]
Epoch [66/120    avg_loss:0.121, val_acc:0.979]
Epoch [67/120    avg_loss:0.108, val_acc:0.981]
Epoch [68/120    avg_loss:0.165, val_acc:0.960]
Epoch [69/120    avg_loss:0.135, val_acc:0.979]
Epoch [70/120    avg_loss:0.117, val_acc:0.975]
Epoch [71/120    avg_loss:0.118, val_acc:0.981]
Epoch [72/120    avg_loss:0.122, val_acc:0.975]
Epoch [73/120    avg_loss:0.111, val_acc:0.983]
Epoch [74/120    avg_loss:0.126, val_acc:0.965]
Epoch [75/120    avg_loss:0.138, val_acc:0.973]
Epoch [76/120    avg_loss:0.192, val_acc:0.956]
Epoch [77/120    avg_loss:0.164, val_acc:0.973]
Epoch [78/120    avg_loss:0.167, val_acc:0.977]
Epoch [79/120    avg_loss:0.124, val_acc:0.981]
Epoch [80/120    avg_loss:0.110, val_acc:0.977]
Epoch [81/120    avg_loss:0.073, val_acc:0.985]
Epoch [82/120    avg_loss:0.068, val_acc:0.990]
Epoch [83/120    avg_loss:0.082, val_acc:0.985]
Epoch [84/120    avg_loss:0.089, val_acc:0.983]
Epoch [85/120    avg_loss:0.096, val_acc:0.988]
Epoch [86/120    avg_loss:0.091, val_acc:0.979]
Epoch [87/120    avg_loss:0.083, val_acc:0.979]
Epoch [88/120    avg_loss:0.069, val_acc:0.983]
Epoch [89/120    avg_loss:0.051, val_acc:0.990]
Epoch [90/120    avg_loss:0.051, val_acc:0.979]
Epoch [91/120    avg_loss:0.068, val_acc:0.975]
Epoch [92/120    avg_loss:0.087, val_acc:0.975]
Epoch [93/120    avg_loss:0.084, val_acc:0.985]
Epoch [94/120    avg_loss:0.094, val_acc:0.981]
Epoch [95/120    avg_loss:0.055, val_acc:0.988]
Epoch [96/120    avg_loss:0.047, val_acc:0.985]
Epoch [97/120    avg_loss:0.059, val_acc:0.990]
Epoch [98/120    avg_loss:0.067, val_acc:0.985]
Epoch [99/120    avg_loss:0.055, val_acc:0.983]
Epoch [100/120    avg_loss:0.067, val_acc:0.985]
Epoch [101/120    avg_loss:0.051, val_acc:0.992]
Epoch [102/120    avg_loss:0.058, val_acc:0.983]
Epoch [103/120    avg_loss:0.070, val_acc:0.979]
Epoch [104/120    avg_loss:0.083, val_acc:0.977]
Epoch [105/120    avg_loss:0.095, val_acc:0.988]
Epoch [106/120    avg_loss:0.063, val_acc:0.992]
Epoch [107/120    avg_loss:0.052, val_acc:0.985]
Epoch [108/120    avg_loss:0.047, val_acc:0.998]
Epoch [109/120    avg_loss:0.066, val_acc:0.973]
Epoch [110/120    avg_loss:0.061, val_acc:0.992]
Epoch [111/120    avg_loss:0.049, val_acc:0.990]
Epoch [112/120    avg_loss:0.037, val_acc:0.994]
Epoch [113/120    avg_loss:0.044, val_acc:0.992]
Epoch [114/120    avg_loss:0.038, val_acc:0.990]
Epoch [115/120    avg_loss:0.031, val_acc:0.996]
Epoch [116/120    avg_loss:0.042, val_acc:0.994]
Epoch [117/120    avg_loss:0.058, val_acc:0.998]
Epoch [118/120    avg_loss:0.041, val_acc:0.996]
Epoch [119/120    avg_loss:0.052, val_acc:0.992]
Epoch [120/120    avg_loss:0.055, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   9   0   0   0   0 197   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 0.99347353 1.         1.         0.9638009  0.94701987
 0.97766749 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9940644641213435
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd1ba585898>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.596, val_acc:0.146]
Epoch [2/120    avg_loss:2.391, val_acc:0.410]
Epoch [3/120    avg_loss:2.230, val_acc:0.463]
Epoch [4/120    avg_loss:2.099, val_acc:0.517]
Epoch [5/120    avg_loss:1.971, val_acc:0.565]
Epoch [6/120    avg_loss:1.839, val_acc:0.598]
Epoch [7/120    avg_loss:1.705, val_acc:0.662]
Epoch [8/120    avg_loss:1.591, val_acc:0.671]
Epoch [9/120    avg_loss:1.486, val_acc:0.671]
Epoch [10/120    avg_loss:1.357, val_acc:0.702]
Epoch [11/120    avg_loss:1.296, val_acc:0.702]
Epoch [12/120    avg_loss:1.190, val_acc:0.742]
Epoch [13/120    avg_loss:1.120, val_acc:0.760]
Epoch [14/120    avg_loss:1.021, val_acc:0.781]
Epoch [15/120    avg_loss:0.950, val_acc:0.808]
Epoch [16/120    avg_loss:0.883, val_acc:0.808]
Epoch [17/120    avg_loss:0.815, val_acc:0.873]
Epoch [18/120    avg_loss:0.812, val_acc:0.817]
Epoch [19/120    avg_loss:0.739, val_acc:0.831]
Epoch [20/120    avg_loss:0.692, val_acc:0.835]
Epoch [21/120    avg_loss:0.670, val_acc:0.858]
Epoch [22/120    avg_loss:0.631, val_acc:0.908]
Epoch [23/120    avg_loss:0.576, val_acc:0.915]
Epoch [24/120    avg_loss:0.528, val_acc:0.919]
Epoch [25/120    avg_loss:0.502, val_acc:0.887]
Epoch [26/120    avg_loss:0.462, val_acc:0.925]
Epoch [27/120    avg_loss:0.401, val_acc:0.956]
Epoch [28/120    avg_loss:0.418, val_acc:0.896]
Epoch [29/120    avg_loss:0.433, val_acc:0.944]
Epoch [30/120    avg_loss:0.367, val_acc:0.935]
Epoch [31/120    avg_loss:0.374, val_acc:0.950]
Epoch [32/120    avg_loss:0.326, val_acc:0.965]
Epoch [33/120    avg_loss:0.326, val_acc:0.960]
Epoch [34/120    avg_loss:0.321, val_acc:0.944]
Epoch [35/120    avg_loss:0.334, val_acc:0.950]
Epoch [36/120    avg_loss:0.293, val_acc:0.950]
Epoch [37/120    avg_loss:0.299, val_acc:0.960]
Epoch [38/120    avg_loss:0.246, val_acc:0.950]
Epoch [39/120    avg_loss:0.249, val_acc:0.940]
Epoch [40/120    avg_loss:0.252, val_acc:0.892]
Epoch [41/120    avg_loss:0.268, val_acc:0.921]
Epoch [42/120    avg_loss:0.285, val_acc:0.971]
Epoch [43/120    avg_loss:0.208, val_acc:0.973]
Epoch [44/120    avg_loss:0.233, val_acc:0.950]
Epoch [45/120    avg_loss:0.215, val_acc:0.958]
Epoch [46/120    avg_loss:0.197, val_acc:0.925]
Epoch [47/120    avg_loss:0.419, val_acc:0.929]
Epoch [48/120    avg_loss:0.295, val_acc:0.960]
Epoch [49/120    avg_loss:0.213, val_acc:0.975]
Epoch [50/120    avg_loss:0.204, val_acc:0.973]
Epoch [51/120    avg_loss:0.161, val_acc:0.977]
Epoch [52/120    avg_loss:0.193, val_acc:0.963]
Epoch [53/120    avg_loss:0.202, val_acc:0.963]
Epoch [54/120    avg_loss:0.188, val_acc:0.969]
Epoch [55/120    avg_loss:0.199, val_acc:0.944]
Epoch [56/120    avg_loss:0.218, val_acc:0.969]
Epoch [57/120    avg_loss:0.188, val_acc:0.977]
Epoch [58/120    avg_loss:0.150, val_acc:0.965]
Epoch [59/120    avg_loss:0.143, val_acc:0.977]
Epoch [60/120    avg_loss:0.112, val_acc:0.981]
Epoch [61/120    avg_loss:0.114, val_acc:0.975]
Epoch [62/120    avg_loss:0.099, val_acc:0.973]
Epoch [63/120    avg_loss:0.165, val_acc:0.967]
Epoch [64/120    avg_loss:0.127, val_acc:0.983]
Epoch [65/120    avg_loss:0.103, val_acc:0.979]
Epoch [66/120    avg_loss:0.123, val_acc:0.979]
Epoch [67/120    avg_loss:0.113, val_acc:0.975]
Epoch [68/120    avg_loss:0.101, val_acc:0.983]
Epoch [69/120    avg_loss:0.112, val_acc:0.973]
Epoch [70/120    avg_loss:0.106, val_acc:0.977]
Epoch [71/120    avg_loss:0.096, val_acc:0.971]
Epoch [72/120    avg_loss:0.116, val_acc:0.969]
Epoch [73/120    avg_loss:0.135, val_acc:0.985]
Epoch [74/120    avg_loss:0.149, val_acc:0.973]
Epoch [75/120    avg_loss:0.127, val_acc:0.965]
Epoch [76/120    avg_loss:0.130, val_acc:0.975]
Epoch [77/120    avg_loss:0.112, val_acc:0.985]
Epoch [78/120    avg_loss:0.057, val_acc:0.985]
Epoch [79/120    avg_loss:0.069, val_acc:0.975]
Epoch [80/120    avg_loss:0.066, val_acc:0.975]
Epoch [81/120    avg_loss:0.065, val_acc:0.988]
Epoch [82/120    avg_loss:0.074, val_acc:0.981]
Epoch [83/120    avg_loss:0.077, val_acc:0.990]
Epoch [84/120    avg_loss:0.066, val_acc:0.979]
Epoch [85/120    avg_loss:0.057, val_acc:0.990]
Epoch [86/120    avg_loss:0.042, val_acc:0.988]
Epoch [87/120    avg_loss:0.051, val_acc:0.990]
Epoch [88/120    avg_loss:0.035, val_acc:0.992]
Epoch [89/120    avg_loss:0.045, val_acc:0.990]
Epoch [90/120    avg_loss:0.064, val_acc:0.990]
Epoch [91/120    avg_loss:0.050, val_acc:0.985]
Epoch [92/120    avg_loss:0.059, val_acc:0.985]
Epoch [93/120    avg_loss:0.062, val_acc:0.988]
Epoch [94/120    avg_loss:0.086, val_acc:0.969]
Epoch [95/120    avg_loss:0.125, val_acc:0.977]
Epoch [96/120    avg_loss:0.185, val_acc:0.988]
Epoch [97/120    avg_loss:0.113, val_acc:0.975]
Epoch [98/120    avg_loss:0.121, val_acc:0.979]
Epoch [99/120    avg_loss:0.064, val_acc:0.990]
Epoch [100/120    avg_loss:0.043, val_acc:0.985]
Epoch [101/120    avg_loss:0.043, val_acc:0.994]
Epoch [102/120    avg_loss:0.033, val_acc:0.992]
Epoch [103/120    avg_loss:0.030, val_acc:0.990]
Epoch [104/120    avg_loss:0.030, val_acc:0.992]
Epoch [105/120    avg_loss:0.024, val_acc:0.990]
Epoch [106/120    avg_loss:0.032, val_acc:0.988]
Epoch [107/120    avg_loss:0.037, val_acc:0.990]
Epoch [108/120    avg_loss:0.023, val_acc:0.990]
Epoch [109/120    avg_loss:0.031, val_acc:0.990]
Epoch [110/120    avg_loss:0.025, val_acc:0.992]
Epoch [111/120    avg_loss:0.025, val_acc:0.988]
Epoch [112/120    avg_loss:0.024, val_acc:0.988]
Epoch [113/120    avg_loss:0.024, val_acc:0.996]
Epoch [114/120    avg_loss:0.026, val_acc:0.994]
Epoch [115/120    avg_loss:0.024, val_acc:0.983]
Epoch [116/120    avg_loss:0.031, val_acc:0.988]
Epoch [117/120    avg_loss:0.035, val_acc:0.992]
Epoch [118/120    avg_loss:0.030, val_acc:0.994]
Epoch [119/120    avg_loss:0.022, val_acc:0.994]
Epoch [120/120    avg_loss:0.017, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99853801 0.99545455 1.         0.94168467 0.90391459
 0.99516908 0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9926411637276535
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2ec4210898>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.565, val_acc:0.210]
Epoch [2/120    avg_loss:2.369, val_acc:0.302]
Epoch [3/120    avg_loss:2.212, val_acc:0.392]
Epoch [4/120    avg_loss:2.072, val_acc:0.458]
Epoch [5/120    avg_loss:1.947, val_acc:0.510]
Epoch [6/120    avg_loss:1.823, val_acc:0.556]
Epoch [7/120    avg_loss:1.686, val_acc:0.617]
Epoch [8/120    avg_loss:1.595, val_acc:0.623]
Epoch [9/120    avg_loss:1.488, val_acc:0.671]
Epoch [10/120    avg_loss:1.375, val_acc:0.708]
Epoch [11/120    avg_loss:1.297, val_acc:0.738]
Epoch [12/120    avg_loss:1.213, val_acc:0.748]
Epoch [13/120    avg_loss:1.117, val_acc:0.756]
Epoch [14/120    avg_loss:1.030, val_acc:0.775]
Epoch [15/120    avg_loss:0.974, val_acc:0.831]
Epoch [16/120    avg_loss:0.885, val_acc:0.787]
Epoch [17/120    avg_loss:0.810, val_acc:0.860]
Epoch [18/120    avg_loss:0.726, val_acc:0.896]
Epoch [19/120    avg_loss:0.693, val_acc:0.890]
Epoch [20/120    avg_loss:0.683, val_acc:0.896]
Epoch [21/120    avg_loss:0.598, val_acc:0.906]
Epoch [22/120    avg_loss:0.554, val_acc:0.900]
Epoch [23/120    avg_loss:0.538, val_acc:0.925]
Epoch [24/120    avg_loss:0.491, val_acc:0.921]
Epoch [25/120    avg_loss:0.462, val_acc:0.912]
Epoch [26/120    avg_loss:0.444, val_acc:0.925]
Epoch [27/120    avg_loss:0.413, val_acc:0.931]
Epoch [28/120    avg_loss:0.358, val_acc:0.869]
Epoch [29/120    avg_loss:0.415, val_acc:0.935]
Epoch [30/120    avg_loss:0.351, val_acc:0.929]
Epoch [31/120    avg_loss:0.351, val_acc:0.946]
Epoch [32/120    avg_loss:0.377, val_acc:0.929]
Epoch [33/120    avg_loss:0.375, val_acc:0.835]
Epoch [34/120    avg_loss:0.369, val_acc:0.894]
Epoch [35/120    avg_loss:0.359, val_acc:0.925]
Epoch [36/120    avg_loss:0.291, val_acc:0.944]
Epoch [37/120    avg_loss:0.277, val_acc:0.938]
Epoch [38/120    avg_loss:0.309, val_acc:0.940]
Epoch [39/120    avg_loss:0.247, val_acc:0.940]
Epoch [40/120    avg_loss:0.319, val_acc:0.952]
Epoch [41/120    avg_loss:0.248, val_acc:0.963]
Epoch [42/120    avg_loss:0.252, val_acc:0.952]
Epoch [43/120    avg_loss:0.251, val_acc:0.965]
Epoch [44/120    avg_loss:0.226, val_acc:0.940]
Epoch [45/120    avg_loss:0.236, val_acc:0.969]
Epoch [46/120    avg_loss:0.226, val_acc:0.938]
Epoch [47/120    avg_loss:0.231, val_acc:0.973]
Epoch [48/120    avg_loss:0.224, val_acc:0.948]
Epoch [49/120    avg_loss:0.217, val_acc:0.973]
Epoch [50/120    avg_loss:0.223, val_acc:0.960]
Epoch [51/120    avg_loss:0.215, val_acc:0.956]
Epoch [52/120    avg_loss:0.188, val_acc:0.958]
Epoch [53/120    avg_loss:0.223, val_acc:0.958]
Epoch [54/120    avg_loss:0.195, val_acc:0.960]
Epoch [55/120    avg_loss:0.202, val_acc:0.958]
Epoch [56/120    avg_loss:0.187, val_acc:0.977]
Epoch [57/120    avg_loss:0.139, val_acc:0.969]
Epoch [58/120    avg_loss:0.151, val_acc:0.981]
Epoch [59/120    avg_loss:0.154, val_acc:0.971]
Epoch [60/120    avg_loss:0.145, val_acc:0.971]
Epoch [61/120    avg_loss:0.129, val_acc:0.977]
Epoch [62/120    avg_loss:0.116, val_acc:0.969]
Epoch [63/120    avg_loss:0.115, val_acc:0.977]
Epoch [64/120    avg_loss:0.114, val_acc:0.963]
Epoch [65/120    avg_loss:0.115, val_acc:0.973]
Epoch [66/120    avg_loss:0.147, val_acc:0.973]
Epoch [67/120    avg_loss:0.135, val_acc:0.985]
Epoch [68/120    avg_loss:0.118, val_acc:0.967]
Epoch [69/120    avg_loss:0.101, val_acc:0.963]
Epoch [70/120    avg_loss:0.131, val_acc:0.981]
Epoch [71/120    avg_loss:0.101, val_acc:0.956]
Epoch [72/120    avg_loss:0.092, val_acc:0.981]
Epoch [73/120    avg_loss:0.092, val_acc:0.975]
Epoch [74/120    avg_loss:0.102, val_acc:0.960]
Epoch [75/120    avg_loss:0.103, val_acc:0.977]
Epoch [76/120    avg_loss:0.127, val_acc:0.975]
Epoch [77/120    avg_loss:0.124, val_acc:0.973]
Epoch [78/120    avg_loss:0.119, val_acc:0.981]
Epoch [79/120    avg_loss:0.080, val_acc:0.979]
Epoch [80/120    avg_loss:0.097, val_acc:0.979]
Epoch [81/120    avg_loss:0.071, val_acc:0.983]
Epoch [82/120    avg_loss:0.058, val_acc:0.985]
Epoch [83/120    avg_loss:0.061, val_acc:0.985]
Epoch [84/120    avg_loss:0.056, val_acc:0.983]
Epoch [85/120    avg_loss:0.060, val_acc:0.983]
Epoch [86/120    avg_loss:0.046, val_acc:0.983]
Epoch [87/120    avg_loss:0.054, val_acc:0.985]
Epoch [88/120    avg_loss:0.045, val_acc:0.983]
Epoch [89/120    avg_loss:0.054, val_acc:0.979]
Epoch [90/120    avg_loss:0.051, val_acc:0.985]
Epoch [91/120    avg_loss:0.042, val_acc:0.988]
Epoch [92/120    avg_loss:0.050, val_acc:0.988]
Epoch [93/120    avg_loss:0.050, val_acc:0.985]
Epoch [94/120    avg_loss:0.040, val_acc:0.985]
Epoch [95/120    avg_loss:0.046, val_acc:0.985]
Epoch [96/120    avg_loss:0.050, val_acc:0.985]
Epoch [97/120    avg_loss:0.046, val_acc:0.988]
Epoch [98/120    avg_loss:0.049, val_acc:0.988]
Epoch [99/120    avg_loss:0.049, val_acc:0.985]
Epoch [100/120    avg_loss:0.048, val_acc:0.985]
Epoch [101/120    avg_loss:0.047, val_acc:0.988]
Epoch [102/120    avg_loss:0.049, val_acc:0.985]
Epoch [103/120    avg_loss:0.040, val_acc:0.985]
Epoch [104/120    avg_loss:0.050, val_acc:0.985]
Epoch [105/120    avg_loss:0.050, val_acc:0.983]
Epoch [106/120    avg_loss:0.038, val_acc:0.985]
Epoch [107/120    avg_loss:0.047, val_acc:0.985]
Epoch [108/120    avg_loss:0.047, val_acc:0.983]
Epoch [109/120    avg_loss:0.043, val_acc:0.988]
Epoch [110/120    avg_loss:0.041, val_acc:0.985]
Epoch [111/120    avg_loss:0.043, val_acc:0.983]
Epoch [112/120    avg_loss:0.041, val_acc:0.981]
Epoch [113/120    avg_loss:0.042, val_acc:0.985]
Epoch [114/120    avg_loss:0.050, val_acc:0.985]
Epoch [115/120    avg_loss:0.037, val_acc:0.985]
Epoch [116/120    avg_loss:0.042, val_acc:0.988]
Epoch [117/120    avg_loss:0.042, val_acc:0.985]
Epoch [118/120    avg_loss:0.046, val_acc:0.983]
Epoch [119/120    avg_loss:0.039, val_acc:0.983]
Epoch [120/120    avg_loss:0.046, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   5   0   0   0   0   0   0 383   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 0.99636364 0.97550111 1.         0.92951542 0.88965517
 1.         0.93785311 0.99351492 1.         1.         1.
 1.         1.        ]

Kappa:
0.9886040405236918
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcbedb078d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.584, val_acc:0.327]
Epoch [2/120    avg_loss:2.364, val_acc:0.406]
Epoch [3/120    avg_loss:2.221, val_acc:0.544]
Epoch [4/120    avg_loss:2.072, val_acc:0.569]
Epoch [5/120    avg_loss:1.920, val_acc:0.569]
Epoch [6/120    avg_loss:1.774, val_acc:0.588]
Epoch [7/120    avg_loss:1.645, val_acc:0.610]
Epoch [8/120    avg_loss:1.526, val_acc:0.656]
Epoch [9/120    avg_loss:1.398, val_acc:0.685]
Epoch [10/120    avg_loss:1.298, val_acc:0.685]
Epoch [11/120    avg_loss:1.220, val_acc:0.721]
Epoch [12/120    avg_loss:1.115, val_acc:0.748]
Epoch [13/120    avg_loss:0.993, val_acc:0.804]
Epoch [14/120    avg_loss:0.905, val_acc:0.825]
Epoch [15/120    avg_loss:0.864, val_acc:0.821]
Epoch [16/120    avg_loss:0.877, val_acc:0.815]
Epoch [17/120    avg_loss:0.783, val_acc:0.883]
Epoch [18/120    avg_loss:0.719, val_acc:0.883]
Epoch [19/120    avg_loss:0.636, val_acc:0.885]
Epoch [20/120    avg_loss:0.604, val_acc:0.873]
Epoch [21/120    avg_loss:0.602, val_acc:0.908]
Epoch [22/120    avg_loss:0.565, val_acc:0.910]
Epoch [23/120    avg_loss:0.498, val_acc:0.906]
Epoch [24/120    avg_loss:0.500, val_acc:0.917]
Epoch [25/120    avg_loss:0.481, val_acc:0.925]
Epoch [26/120    avg_loss:0.478, val_acc:0.935]
Epoch [27/120    avg_loss:0.426, val_acc:0.912]
Epoch [28/120    avg_loss:0.423, val_acc:0.902]
Epoch [29/120    avg_loss:0.467, val_acc:0.923]
Epoch [30/120    avg_loss:0.401, val_acc:0.948]
Epoch [31/120    avg_loss:0.359, val_acc:0.942]
Epoch [32/120    avg_loss:0.343, val_acc:0.946]
Epoch [33/120    avg_loss:0.339, val_acc:0.925]
Epoch [34/120    avg_loss:0.290, val_acc:0.927]
Epoch [35/120    avg_loss:0.274, val_acc:0.954]
Epoch [36/120    avg_loss:0.272, val_acc:0.942]
Epoch [37/120    avg_loss:0.241, val_acc:0.948]
Epoch [38/120    avg_loss:0.203, val_acc:0.965]
Epoch [39/120    avg_loss:0.214, val_acc:0.956]
Epoch [40/120    avg_loss:0.201, val_acc:0.963]
Epoch [41/120    avg_loss:0.231, val_acc:0.967]
Epoch [42/120    avg_loss:0.238, val_acc:0.946]
Epoch [43/120    avg_loss:0.227, val_acc:0.960]
Epoch [44/120    avg_loss:0.203, val_acc:0.965]
Epoch [45/120    avg_loss:0.236, val_acc:0.956]
Epoch [46/120    avg_loss:0.278, val_acc:0.969]
Epoch [47/120    avg_loss:0.201, val_acc:0.973]
Epoch [48/120    avg_loss:0.187, val_acc:0.975]
Epoch [49/120    avg_loss:0.202, val_acc:0.952]
Epoch [50/120    avg_loss:0.225, val_acc:0.956]
Epoch [51/120    avg_loss:0.207, val_acc:0.973]
Epoch [52/120    avg_loss:0.198, val_acc:0.933]
Epoch [53/120    avg_loss:0.247, val_acc:0.963]
Epoch [54/120    avg_loss:0.167, val_acc:0.979]
Epoch [55/120    avg_loss:0.156, val_acc:0.977]
Epoch [56/120    avg_loss:0.115, val_acc:0.983]
Epoch [57/120    avg_loss:0.113, val_acc:0.977]
Epoch [58/120    avg_loss:0.139, val_acc:0.988]
Epoch [59/120    avg_loss:0.124, val_acc:0.981]
Epoch [60/120    avg_loss:0.138, val_acc:0.975]
Epoch [61/120    avg_loss:0.164, val_acc:0.950]
Epoch [62/120    avg_loss:0.153, val_acc:0.963]
Epoch [63/120    avg_loss:0.222, val_acc:0.973]
Epoch [64/120    avg_loss:0.171, val_acc:0.973]
Epoch [65/120    avg_loss:0.101, val_acc:0.988]
Epoch [66/120    avg_loss:0.104, val_acc:0.990]
Epoch [67/120    avg_loss:0.116, val_acc:0.985]
Epoch [68/120    avg_loss:0.148, val_acc:0.979]
Epoch [69/120    avg_loss:0.128, val_acc:0.977]
Epoch [70/120    avg_loss:0.091, val_acc:0.981]
Epoch [71/120    avg_loss:0.077, val_acc:0.979]
Epoch [72/120    avg_loss:0.082, val_acc:0.979]
Epoch [73/120    avg_loss:0.078, val_acc:0.983]
Epoch [74/120    avg_loss:0.066, val_acc:0.985]
Epoch [75/120    avg_loss:0.061, val_acc:0.988]
Epoch [76/120    avg_loss:0.082, val_acc:0.977]
Epoch [77/120    avg_loss:0.069, val_acc:0.990]
Epoch [78/120    avg_loss:0.063, val_acc:0.983]
Epoch [79/120    avg_loss:0.083, val_acc:0.983]
Epoch [80/120    avg_loss:0.108, val_acc:0.977]
Epoch [81/120    avg_loss:0.063, val_acc:0.983]
Epoch [82/120    avg_loss:0.050, val_acc:0.985]
Epoch [83/120    avg_loss:0.099, val_acc:0.977]
Epoch [84/120    avg_loss:0.088, val_acc:0.988]
Epoch [85/120    avg_loss:0.068, val_acc:0.990]
Epoch [86/120    avg_loss:0.068, val_acc:0.990]
Epoch [87/120    avg_loss:0.093, val_acc:0.975]
Epoch [88/120    avg_loss:0.070, val_acc:0.985]
Epoch [89/120    avg_loss:0.069, val_acc:0.979]
Epoch [90/120    avg_loss:0.088, val_acc:0.990]
Epoch [91/120    avg_loss:0.062, val_acc:0.994]
Epoch [92/120    avg_loss:0.054, val_acc:0.990]
Epoch [93/120    avg_loss:0.040, val_acc:0.992]
Epoch [94/120    avg_loss:0.030, val_acc:0.992]
Epoch [95/120    avg_loss:0.034, val_acc:0.992]
Epoch [96/120    avg_loss:0.029, val_acc:0.992]
Epoch [97/120    avg_loss:0.024, val_acc:0.988]
Epoch [98/120    avg_loss:0.037, val_acc:0.985]
Epoch [99/120    avg_loss:0.026, val_acc:0.990]
Epoch [100/120    avg_loss:0.029, val_acc:0.990]
Epoch [101/120    avg_loss:0.028, val_acc:0.977]
Epoch [102/120    avg_loss:0.042, val_acc:0.992]
Epoch [103/120    avg_loss:0.048, val_acc:0.990]
Epoch [104/120    avg_loss:0.035, val_acc:0.990]
Epoch [105/120    avg_loss:0.063, val_acc:0.992]
Epoch [106/120    avg_loss:0.028, val_acc:0.992]
Epoch [107/120    avg_loss:0.025, val_acc:0.994]
Epoch [108/120    avg_loss:0.021, val_acc:0.992]
Epoch [109/120    avg_loss:0.019, val_acc:0.992]
Epoch [110/120    avg_loss:0.017, val_acc:0.992]
Epoch [111/120    avg_loss:0.027, val_acc:0.994]
Epoch [112/120    avg_loss:0.017, val_acc:0.994]
Epoch [113/120    avg_loss:0.028, val_acc:0.994]
Epoch [114/120    avg_loss:0.016, val_acc:0.994]
Epoch [115/120    avg_loss:0.017, val_acc:0.994]
Epoch [116/120    avg_loss:0.016, val_acc:0.994]
Epoch [117/120    avg_loss:0.018, val_acc:0.994]
Epoch [118/120    avg_loss:0.017, val_acc:0.994]
Epoch [119/120    avg_loss:0.021, val_acc:0.994]
Epoch [120/120    avg_loss:0.015, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 0.99780541 0.98426966 1.         0.93913043 0.90140845
 0.99277108 0.96132597 1.         1.         1.         0.9973545
 0.99778761 1.        ]

Kappa:
0.9905048484996952
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7eff01b32898>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.513, val_acc:0.325]
Epoch [2/120    avg_loss:2.320, val_acc:0.481]
Epoch [3/120    avg_loss:2.166, val_acc:0.592]
Epoch [4/120    avg_loss:1.996, val_acc:0.588]
Epoch [5/120    avg_loss:1.841, val_acc:0.596]
Epoch [6/120    avg_loss:1.660, val_acc:0.598]
Epoch [7/120    avg_loss:1.528, val_acc:0.644]
Epoch [8/120    avg_loss:1.397, val_acc:0.677]
Epoch [9/120    avg_loss:1.324, val_acc:0.685]
Epoch [10/120    avg_loss:1.236, val_acc:0.700]
Epoch [11/120    avg_loss:1.145, val_acc:0.713]
Epoch [12/120    avg_loss:1.067, val_acc:0.740]
Epoch [13/120    avg_loss:1.003, val_acc:0.767]
Epoch [14/120    avg_loss:0.982, val_acc:0.773]
Epoch [15/120    avg_loss:0.894, val_acc:0.783]
Epoch [16/120    avg_loss:0.816, val_acc:0.823]
Epoch [17/120    avg_loss:0.770, val_acc:0.829]
Epoch [18/120    avg_loss:0.757, val_acc:0.842]
Epoch [19/120    avg_loss:0.692, val_acc:0.773]
Epoch [20/120    avg_loss:0.637, val_acc:0.827]
Epoch [21/120    avg_loss:0.619, val_acc:0.869]
Epoch [22/120    avg_loss:0.543, val_acc:0.917]
Epoch [23/120    avg_loss:0.492, val_acc:0.938]
Epoch [24/120    avg_loss:0.468, val_acc:0.935]
Epoch [25/120    avg_loss:0.435, val_acc:0.960]
Epoch [26/120    avg_loss:0.457, val_acc:0.948]
Epoch [27/120    avg_loss:0.446, val_acc:0.944]
Epoch [28/120    avg_loss:0.421, val_acc:0.921]
Epoch [29/120    avg_loss:0.404, val_acc:0.940]
Epoch [30/120    avg_loss:0.366, val_acc:0.958]
Epoch [31/120    avg_loss:0.405, val_acc:0.973]
Epoch [32/120    avg_loss:0.318, val_acc:0.963]
Epoch [33/120    avg_loss:0.357, val_acc:0.946]
Epoch [34/120    avg_loss:0.343, val_acc:0.963]
Epoch [35/120    avg_loss:0.300, val_acc:0.973]
Epoch [36/120    avg_loss:0.273, val_acc:0.963]
Epoch [37/120    avg_loss:0.259, val_acc:0.965]
Epoch [38/120    avg_loss:0.273, val_acc:0.988]
Epoch [39/120    avg_loss:0.254, val_acc:0.977]
Epoch [40/120    avg_loss:0.229, val_acc:0.969]
Epoch [41/120    avg_loss:0.221, val_acc:0.963]
Epoch [42/120    avg_loss:0.235, val_acc:0.965]
Epoch [43/120    avg_loss:0.256, val_acc:0.969]
Epoch [44/120    avg_loss:0.229, val_acc:0.963]
Epoch [45/120    avg_loss:0.250, val_acc:0.979]
Epoch [46/120    avg_loss:0.215, val_acc:0.979]
Epoch [47/120    avg_loss:0.258, val_acc:0.965]
Epoch [48/120    avg_loss:0.258, val_acc:0.969]
Epoch [49/120    avg_loss:0.192, val_acc:0.981]
Epoch [50/120    avg_loss:0.164, val_acc:0.985]
Epoch [51/120    avg_loss:0.154, val_acc:0.983]
Epoch [52/120    avg_loss:0.145, val_acc:0.990]
Epoch [53/120    avg_loss:0.132, val_acc:0.988]
Epoch [54/120    avg_loss:0.136, val_acc:0.990]
Epoch [55/120    avg_loss:0.132, val_acc:0.992]
Epoch [56/120    avg_loss:0.127, val_acc:0.992]
Epoch [57/120    avg_loss:0.125, val_acc:0.990]
Epoch [58/120    avg_loss:0.135, val_acc:0.990]
Epoch [59/120    avg_loss:0.127, val_acc:0.992]
Epoch [60/120    avg_loss:0.149, val_acc:0.990]
Epoch [61/120    avg_loss:0.155, val_acc:0.988]
Epoch [62/120    avg_loss:0.136, val_acc:0.992]
Epoch [63/120    avg_loss:0.131, val_acc:0.988]
Epoch [64/120    avg_loss:0.112, val_acc:0.990]
Epoch [65/120    avg_loss:0.116, val_acc:0.990]
Epoch [66/120    avg_loss:0.131, val_acc:0.994]
Epoch [67/120    avg_loss:0.122, val_acc:0.992]
Epoch [68/120    avg_loss:0.127, val_acc:0.992]
Epoch [69/120    avg_loss:0.122, val_acc:0.992]
Epoch [70/120    avg_loss:0.122, val_acc:0.990]
Epoch [71/120    avg_loss:0.116, val_acc:0.988]
Epoch [72/120    avg_loss:0.109, val_acc:0.992]
Epoch [73/120    avg_loss:0.116, val_acc:0.994]
Epoch [74/120    avg_loss:0.104, val_acc:0.990]
Epoch [75/120    avg_loss:0.123, val_acc:0.992]
Epoch [76/120    avg_loss:0.126, val_acc:0.988]
Epoch [77/120    avg_loss:0.114, val_acc:0.992]
Epoch [78/120    avg_loss:0.108, val_acc:0.992]
Epoch [79/120    avg_loss:0.104, val_acc:0.992]
Epoch [80/120    avg_loss:0.101, val_acc:0.994]
Epoch [81/120    avg_loss:0.136, val_acc:0.988]
Epoch [82/120    avg_loss:0.119, val_acc:0.988]
Epoch [83/120    avg_loss:0.116, val_acc:0.990]
Epoch [84/120    avg_loss:0.102, val_acc:0.992]
Epoch [85/120    avg_loss:0.103, val_acc:0.988]
Epoch [86/120    avg_loss:0.117, val_acc:0.990]
Epoch [87/120    avg_loss:0.113, val_acc:0.994]
Epoch [88/120    avg_loss:0.105, val_acc:0.990]
Epoch [89/120    avg_loss:0.113, val_acc:0.992]
Epoch [90/120    avg_loss:0.106, val_acc:0.994]
Epoch [91/120    avg_loss:0.110, val_acc:0.988]
Epoch [92/120    avg_loss:0.107, val_acc:0.992]
Epoch [93/120    avg_loss:0.133, val_acc:0.994]
Epoch [94/120    avg_loss:0.111, val_acc:0.994]
Epoch [95/120    avg_loss:0.104, val_acc:0.994]
Epoch [96/120    avg_loss:0.107, val_acc:0.994]
Epoch [97/120    avg_loss:0.100, val_acc:0.994]
Epoch [98/120    avg_loss:0.103, val_acc:0.992]
Epoch [99/120    avg_loss:0.098, val_acc:0.992]
Epoch [100/120    avg_loss:0.095, val_acc:0.994]
Epoch [101/120    avg_loss:0.095, val_acc:0.994]
Epoch [102/120    avg_loss:0.092, val_acc:0.994]
Epoch [103/120    avg_loss:0.095, val_acc:0.994]
Epoch [104/120    avg_loss:0.103, val_acc:0.992]
Epoch [105/120    avg_loss:0.103, val_acc:0.990]
Epoch [106/120    avg_loss:0.106, val_acc:0.992]
Epoch [107/120    avg_loss:0.097, val_acc:0.994]
Epoch [108/120    avg_loss:0.105, val_acc:0.988]
Epoch [109/120    avg_loss:0.088, val_acc:0.992]
Epoch [110/120    avg_loss:0.106, val_acc:0.992]
Epoch [111/120    avg_loss:0.090, val_acc:0.994]
Epoch [112/120    avg_loss:0.095, val_acc:0.990]
Epoch [113/120    avg_loss:0.085, val_acc:0.990]
Epoch [114/120    avg_loss:0.091, val_acc:0.992]
Epoch [115/120    avg_loss:0.086, val_acc:0.994]
Epoch [116/120    avg_loss:0.088, val_acc:0.994]
Epoch [117/120    avg_loss:0.085, val_acc:0.994]
Epoch [118/120    avg_loss:0.100, val_acc:0.994]
Epoch [119/120    avg_loss:0.086, val_acc:0.992]
Epoch [120/120    avg_loss:0.087, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 0.996337   0.99095023 1.         0.93534483 0.89285714
 0.98800959 0.97826087 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9905052219002017
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f60100a9940>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.590, val_acc:0.181]
Epoch [2/120    avg_loss:2.383, val_acc:0.406]
Epoch [3/120    avg_loss:2.241, val_acc:0.548]
Epoch [4/120    avg_loss:2.111, val_acc:0.660]
Epoch [5/120    avg_loss:2.006, val_acc:0.637]
Epoch [6/120    avg_loss:1.853, val_acc:0.692]
Epoch [7/120    avg_loss:1.715, val_acc:0.713]
Epoch [8/120    avg_loss:1.571, val_acc:0.708]
Epoch [9/120    avg_loss:1.430, val_acc:0.721]
Epoch [10/120    avg_loss:1.277, val_acc:0.744]
Epoch [11/120    avg_loss:1.189, val_acc:0.756]
Epoch [12/120    avg_loss:1.069, val_acc:0.760]
Epoch [13/120    avg_loss:0.946, val_acc:0.787]
Epoch [14/120    avg_loss:0.851, val_acc:0.790]
Epoch [15/120    avg_loss:0.802, val_acc:0.842]
Epoch [16/120    avg_loss:0.687, val_acc:0.852]
Epoch [17/120    avg_loss:0.662, val_acc:0.810]
Epoch [18/120    avg_loss:0.598, val_acc:0.877]
Epoch [19/120    avg_loss:0.606, val_acc:0.860]
Epoch [20/120    avg_loss:0.526, val_acc:0.912]
Epoch [21/120    avg_loss:0.500, val_acc:0.883]
Epoch [22/120    avg_loss:0.509, val_acc:0.873]
Epoch [23/120    avg_loss:0.479, val_acc:0.900]
Epoch [24/120    avg_loss:0.412, val_acc:0.917]
Epoch [25/120    avg_loss:0.385, val_acc:0.946]
Epoch [26/120    avg_loss:0.354, val_acc:0.935]
Epoch [27/120    avg_loss:0.400, val_acc:0.860]
Epoch [28/120    avg_loss:0.350, val_acc:0.935]
Epoch [29/120    avg_loss:0.320, val_acc:0.956]
Epoch [30/120    avg_loss:0.346, val_acc:0.894]
Epoch [31/120    avg_loss:0.331, val_acc:0.940]
Epoch [32/120    avg_loss:0.279, val_acc:0.942]
Epoch [33/120    avg_loss:0.333, val_acc:0.954]
Epoch [34/120    avg_loss:0.342, val_acc:0.929]
Epoch [35/120    avg_loss:0.356, val_acc:0.952]
Epoch [36/120    avg_loss:0.310, val_acc:0.944]
Epoch [37/120    avg_loss:0.287, val_acc:0.933]
Epoch [38/120    avg_loss:0.254, val_acc:0.967]
Epoch [39/120    avg_loss:0.270, val_acc:0.963]
Epoch [40/120    avg_loss:0.216, val_acc:0.965]
Epoch [41/120    avg_loss:0.249, val_acc:0.958]
Epoch [42/120    avg_loss:0.233, val_acc:0.965]
Epoch [43/120    avg_loss:0.207, val_acc:0.946]
Epoch [44/120    avg_loss:0.192, val_acc:0.967]
Epoch [45/120    avg_loss:0.179, val_acc:0.963]
Epoch [46/120    avg_loss:0.219, val_acc:0.973]
Epoch [47/120    avg_loss:0.175, val_acc:0.960]
Epoch [48/120    avg_loss:0.190, val_acc:0.963]
Epoch [49/120    avg_loss:0.163, val_acc:0.965]
Epoch [50/120    avg_loss:0.137, val_acc:0.954]
Epoch [51/120    avg_loss:0.164, val_acc:0.965]
Epoch [52/120    avg_loss:0.164, val_acc:0.967]
Epoch [53/120    avg_loss:0.147, val_acc:0.981]
Epoch [54/120    avg_loss:0.134, val_acc:0.979]
Epoch [55/120    avg_loss:0.146, val_acc:0.981]
Epoch [56/120    avg_loss:0.150, val_acc:0.963]
Epoch [57/120    avg_loss:0.151, val_acc:0.969]
Epoch [58/120    avg_loss:0.130, val_acc:0.981]
Epoch [59/120    avg_loss:0.122, val_acc:0.977]
Epoch [60/120    avg_loss:0.124, val_acc:0.981]
Epoch [61/120    avg_loss:0.141, val_acc:0.973]
Epoch [62/120    avg_loss:0.110, val_acc:0.967]
Epoch [63/120    avg_loss:0.135, val_acc:0.975]
Epoch [64/120    avg_loss:0.115, val_acc:0.977]
Epoch [65/120    avg_loss:0.132, val_acc:0.969]
Epoch [66/120    avg_loss:0.117, val_acc:0.985]
Epoch [67/120    avg_loss:0.123, val_acc:0.979]
Epoch [68/120    avg_loss:0.106, val_acc:0.973]
Epoch [69/120    avg_loss:0.124, val_acc:0.965]
Epoch [70/120    avg_loss:0.125, val_acc:0.983]
Epoch [71/120    avg_loss:0.123, val_acc:0.979]
Epoch [72/120    avg_loss:0.079, val_acc:0.985]
Epoch [73/120    avg_loss:0.083, val_acc:0.973]
Epoch [74/120    avg_loss:0.087, val_acc:0.988]
Epoch [75/120    avg_loss:0.066, val_acc:0.975]
Epoch [76/120    avg_loss:0.084, val_acc:0.990]
Epoch [77/120    avg_loss:0.102, val_acc:0.975]
Epoch [78/120    avg_loss:0.074, val_acc:0.971]
Epoch [79/120    avg_loss:0.150, val_acc:0.983]
Epoch [80/120    avg_loss:0.089, val_acc:0.983]
Epoch [81/120    avg_loss:0.079, val_acc:0.977]
Epoch [82/120    avg_loss:0.071, val_acc:0.985]
Epoch [83/120    avg_loss:0.104, val_acc:0.965]
Epoch [84/120    avg_loss:0.184, val_acc:0.954]
Epoch [85/120    avg_loss:0.100, val_acc:0.973]
Epoch [86/120    avg_loss:0.096, val_acc:0.988]
Epoch [87/120    avg_loss:0.084, val_acc:0.985]
Epoch [88/120    avg_loss:0.061, val_acc:0.988]
Epoch [89/120    avg_loss:0.058, val_acc:0.990]
Epoch [90/120    avg_loss:0.047, val_acc:0.992]
Epoch [91/120    avg_loss:0.045, val_acc:0.994]
Epoch [92/120    avg_loss:0.057, val_acc:0.990]
Epoch [93/120    avg_loss:0.043, val_acc:0.990]
Epoch [94/120    avg_loss:0.058, val_acc:0.985]
Epoch [95/120    avg_loss:0.050, val_acc:0.988]
Epoch [96/120    avg_loss:0.048, val_acc:0.996]
Epoch [97/120    avg_loss:0.049, val_acc:0.990]
Epoch [98/120    avg_loss:0.050, val_acc:0.981]
Epoch [99/120    avg_loss:0.064, val_acc:0.981]
Epoch [100/120    avg_loss:0.053, val_acc:0.992]
Epoch [101/120    avg_loss:0.037, val_acc:0.992]
Epoch [102/120    avg_loss:0.041, val_acc:0.992]
Epoch [103/120    avg_loss:0.037, val_acc:0.990]
Epoch [104/120    avg_loss:0.041, val_acc:0.994]
Epoch [105/120    avg_loss:0.040, val_acc:0.992]
Epoch [106/120    avg_loss:0.034, val_acc:0.996]
Epoch [107/120    avg_loss:0.045, val_acc:0.992]
Epoch [108/120    avg_loss:0.046, val_acc:0.996]
Epoch [109/120    avg_loss:0.082, val_acc:0.985]
Epoch [110/120    avg_loss:0.042, val_acc:0.985]
Epoch [111/120    avg_loss:0.034, val_acc:0.988]
Epoch [112/120    avg_loss:0.041, val_acc:0.992]
Epoch [113/120    avg_loss:0.051, val_acc:0.992]
Epoch [114/120    avg_loss:0.033, val_acc:0.994]
Epoch [115/120    avg_loss:0.028, val_acc:0.994]
Epoch [116/120    avg_loss:0.024, val_acc:0.992]
Epoch [117/120    avg_loss:0.016, val_acc:0.996]
Epoch [118/120    avg_loss:0.018, val_acc:0.994]
Epoch [119/120    avg_loss:0.021, val_acc:0.998]
Epoch [120/120    avg_loss:0.023, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 0.996337   0.99095023 1.         0.96629213 0.94983278
 0.98800959 0.97826087 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.994066231584379
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f16436b3908>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.582, val_acc:0.219]
Epoch [2/120    avg_loss:2.404, val_acc:0.550]
Epoch [3/120    avg_loss:2.278, val_acc:0.560]
Epoch [4/120    avg_loss:2.153, val_acc:0.581]
Epoch [5/120    avg_loss:2.005, val_acc:0.619]
Epoch [6/120    avg_loss:1.845, val_acc:0.629]
Epoch [7/120    avg_loss:1.703, val_acc:0.646]
Epoch [8/120    avg_loss:1.569, val_acc:0.681]
Epoch [9/120    avg_loss:1.414, val_acc:0.706]
Epoch [10/120    avg_loss:1.290, val_acc:0.706]
Epoch [11/120    avg_loss:1.189, val_acc:0.740]
Epoch [12/120    avg_loss:1.081, val_acc:0.752]
Epoch [13/120    avg_loss:0.997, val_acc:0.765]
Epoch [14/120    avg_loss:0.938, val_acc:0.750]
Epoch [15/120    avg_loss:0.855, val_acc:0.767]
Epoch [16/120    avg_loss:0.757, val_acc:0.796]
Epoch [17/120    avg_loss:0.698, val_acc:0.815]
Epoch [18/120    avg_loss:0.667, val_acc:0.815]
Epoch [19/120    avg_loss:0.633, val_acc:0.821]
Epoch [20/120    avg_loss:0.602, val_acc:0.883]
Epoch [21/120    avg_loss:0.562, val_acc:0.923]
Epoch [22/120    avg_loss:0.536, val_acc:0.890]
Epoch [23/120    avg_loss:0.540, val_acc:0.912]
Epoch [24/120    avg_loss:0.463, val_acc:0.956]
Epoch [25/120    avg_loss:0.483, val_acc:0.927]
Epoch [26/120    avg_loss:0.471, val_acc:0.919]
Epoch [27/120    avg_loss:0.383, val_acc:0.929]
Epoch [28/120    avg_loss:0.432, val_acc:0.871]
Epoch [29/120    avg_loss:0.443, val_acc:0.900]
Epoch [30/120    avg_loss:0.439, val_acc:0.946]
Epoch [31/120    avg_loss:0.372, val_acc:0.919]
Epoch [32/120    avg_loss:0.359, val_acc:0.946]
Epoch [33/120    avg_loss:0.375, val_acc:0.950]
Epoch [34/120    avg_loss:0.364, val_acc:0.950]
Epoch [35/120    avg_loss:0.352, val_acc:0.952]
Epoch [36/120    avg_loss:0.356, val_acc:0.956]
Epoch [37/120    avg_loss:0.326, val_acc:0.952]
Epoch [38/120    avg_loss:0.312, val_acc:0.944]
Epoch [39/120    avg_loss:0.255, val_acc:0.967]
Epoch [40/120    avg_loss:0.235, val_acc:0.938]
Epoch [41/120    avg_loss:0.231, val_acc:0.977]
Epoch [42/120    avg_loss:0.240, val_acc:0.981]
Epoch [43/120    avg_loss:0.221, val_acc:0.944]
Epoch [44/120    avg_loss:0.217, val_acc:0.975]
Epoch [45/120    avg_loss:0.242, val_acc:0.950]
Epoch [46/120    avg_loss:0.196, val_acc:0.969]
Epoch [47/120    avg_loss:0.178, val_acc:0.975]
Epoch [48/120    avg_loss:0.219, val_acc:0.979]
Epoch [49/120    avg_loss:0.179, val_acc:0.946]
Epoch [50/120    avg_loss:0.190, val_acc:0.981]
Epoch [51/120    avg_loss:0.150, val_acc:0.977]
Epoch [52/120    avg_loss:0.135, val_acc:0.979]
Epoch [53/120    avg_loss:0.152, val_acc:0.985]
Epoch [54/120    avg_loss:0.144, val_acc:0.979]
Epoch [55/120    avg_loss:0.176, val_acc:0.990]
Epoch [56/120    avg_loss:0.152, val_acc:0.985]
Epoch [57/120    avg_loss:0.159, val_acc:0.977]
Epoch [58/120    avg_loss:0.135, val_acc:0.988]
Epoch [59/120    avg_loss:0.101, val_acc:0.977]
Epoch [60/120    avg_loss:0.145, val_acc:0.983]
Epoch [61/120    avg_loss:0.165, val_acc:0.981]
Epoch [62/120    avg_loss:0.108, val_acc:0.975]
Epoch [63/120    avg_loss:0.098, val_acc:0.985]
Epoch [64/120    avg_loss:0.116, val_acc:0.919]
Epoch [65/120    avg_loss:0.145, val_acc:0.988]
Epoch [66/120    avg_loss:0.101, val_acc:0.981]
Epoch [67/120    avg_loss:0.092, val_acc:0.988]
Epoch [68/120    avg_loss:0.099, val_acc:0.975]
Epoch [69/120    avg_loss:0.085, val_acc:0.990]
Epoch [70/120    avg_loss:0.068, val_acc:0.990]
Epoch [71/120    avg_loss:0.067, val_acc:0.992]
Epoch [72/120    avg_loss:0.085, val_acc:0.990]
Epoch [73/120    avg_loss:0.076, val_acc:0.992]
Epoch [74/120    avg_loss:0.074, val_acc:0.992]
Epoch [75/120    avg_loss:0.060, val_acc:0.992]
Epoch [76/120    avg_loss:0.061, val_acc:0.992]
Epoch [77/120    avg_loss:0.066, val_acc:0.994]
Epoch [78/120    avg_loss:0.061, val_acc:0.992]
Epoch [79/120    avg_loss:0.062, val_acc:0.996]
Epoch [80/120    avg_loss:0.068, val_acc:0.994]
Epoch [81/120    avg_loss:0.059, val_acc:0.996]
Epoch [82/120    avg_loss:0.056, val_acc:0.996]
Epoch [83/120    avg_loss:0.059, val_acc:0.992]
Epoch [84/120    avg_loss:0.078, val_acc:0.992]
Epoch [85/120    avg_loss:0.055, val_acc:0.992]
Epoch [86/120    avg_loss:0.064, val_acc:0.990]
Epoch [87/120    avg_loss:0.077, val_acc:0.996]
Epoch [88/120    avg_loss:0.053, val_acc:0.996]
Epoch [89/120    avg_loss:0.055, val_acc:0.996]
Epoch [90/120    avg_loss:0.060, val_acc:0.996]
Epoch [91/120    avg_loss:0.059, val_acc:0.994]
Epoch [92/120    avg_loss:0.057, val_acc:0.996]
Epoch [93/120    avg_loss:0.056, val_acc:0.994]
Epoch [94/120    avg_loss:0.060, val_acc:0.994]
Epoch [95/120    avg_loss:0.059, val_acc:0.996]
Epoch [96/120    avg_loss:0.057, val_acc:0.996]
Epoch [97/120    avg_loss:0.061, val_acc:0.996]
Epoch [98/120    avg_loss:0.056, val_acc:0.996]
Epoch [99/120    avg_loss:0.055, val_acc:0.994]
Epoch [100/120    avg_loss:0.055, val_acc:0.996]
Epoch [101/120    avg_loss:0.052, val_acc:0.996]
Epoch [102/120    avg_loss:0.052, val_acc:0.996]
Epoch [103/120    avg_loss:0.055, val_acc:0.996]
Epoch [104/120    avg_loss:0.052, val_acc:0.996]
Epoch [105/120    avg_loss:0.049, val_acc:0.996]
Epoch [106/120    avg_loss:0.053, val_acc:0.996]
Epoch [107/120    avg_loss:0.060, val_acc:0.996]
Epoch [108/120    avg_loss:0.049, val_acc:0.996]
Epoch [109/120    avg_loss:0.050, val_acc:0.996]
Epoch [110/120    avg_loss:0.047, val_acc:0.992]
Epoch [111/120    avg_loss:0.053, val_acc:0.994]
Epoch [112/120    avg_loss:0.058, val_acc:0.994]
Epoch [113/120    avg_loss:0.057, val_acc:0.994]
Epoch [114/120    avg_loss:0.047, val_acc:0.992]
Epoch [115/120    avg_loss:0.053, val_acc:0.994]
Epoch [116/120    avg_loss:0.046, val_acc:0.996]
Epoch [117/120    avg_loss:0.047, val_acc:0.996]
Epoch [118/120    avg_loss:0.040, val_acc:0.996]
Epoch [119/120    avg_loss:0.050, val_acc:0.996]
Epoch [120/120    avg_loss:0.050, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  12   0   0   0   0   0   0   1   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 0.99560117 0.99545455 1.         0.94065934 0.90972222
 0.98564593 0.98924731 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.991692588422439
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a91212860>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.590, val_acc:0.323]
Epoch [2/120    avg_loss:2.375, val_acc:0.325]
Epoch [3/120    avg_loss:2.194, val_acc:0.385]
Epoch [4/120    avg_loss:2.031, val_acc:0.419]
Epoch [5/120    avg_loss:1.895, val_acc:0.502]
Epoch [6/120    avg_loss:1.770, val_acc:0.608]
Epoch [7/120    avg_loss:1.659, val_acc:0.648]
Epoch [8/120    avg_loss:1.542, val_acc:0.679]
Epoch [9/120    avg_loss:1.468, val_acc:0.713]
Epoch [10/120    avg_loss:1.344, val_acc:0.725]
Epoch [11/120    avg_loss:1.234, val_acc:0.760]
Epoch [12/120    avg_loss:1.125, val_acc:0.771]
Epoch [13/120    avg_loss:1.021, val_acc:0.765]
Epoch [14/120    avg_loss:0.954, val_acc:0.756]
Epoch [15/120    avg_loss:0.848, val_acc:0.787]
Epoch [16/120    avg_loss:0.802, val_acc:0.783]
Epoch [17/120    avg_loss:0.748, val_acc:0.798]
Epoch [18/120    avg_loss:0.681, val_acc:0.856]
Epoch [19/120    avg_loss:0.632, val_acc:0.827]
Epoch [20/120    avg_loss:0.613, val_acc:0.802]
Epoch [21/120    avg_loss:0.582, val_acc:0.850]
Epoch [22/120    avg_loss:0.536, val_acc:0.892]
Epoch [23/120    avg_loss:0.471, val_acc:0.933]
Epoch [24/120    avg_loss:0.483, val_acc:0.915]
Epoch [25/120    avg_loss:0.512, val_acc:0.915]
Epoch [26/120    avg_loss:0.448, val_acc:0.927]
Epoch [27/120    avg_loss:0.387, val_acc:0.935]
Epoch [28/120    avg_loss:0.362, val_acc:0.940]
Epoch [29/120    avg_loss:0.403, val_acc:0.938]
Epoch [30/120    avg_loss:0.370, val_acc:0.935]
Epoch [31/120    avg_loss:0.322, val_acc:0.963]
Epoch [32/120    avg_loss:0.335, val_acc:0.942]
Epoch [33/120    avg_loss:0.368, val_acc:0.946]
Epoch [34/120    avg_loss:0.311, val_acc:0.956]
Epoch [35/120    avg_loss:0.257, val_acc:0.938]
Epoch [36/120    avg_loss:0.229, val_acc:0.954]
Epoch [37/120    avg_loss:0.235, val_acc:0.925]
Epoch [38/120    avg_loss:0.233, val_acc:0.948]
Epoch [39/120    avg_loss:0.292, val_acc:0.940]
Epoch [40/120    avg_loss:0.260, val_acc:0.963]
Epoch [41/120    avg_loss:0.217, val_acc:0.958]
Epoch [42/120    avg_loss:0.243, val_acc:0.956]
Epoch [43/120    avg_loss:0.281, val_acc:0.894]
Epoch [44/120    avg_loss:0.237, val_acc:0.925]
Epoch [45/120    avg_loss:0.230, val_acc:0.956]
Epoch [46/120    avg_loss:0.257, val_acc:0.950]
Epoch [47/120    avg_loss:0.237, val_acc:0.938]
Epoch [48/120    avg_loss:0.172, val_acc:0.935]
Epoch [49/120    avg_loss:0.168, val_acc:0.958]
Epoch [50/120    avg_loss:0.176, val_acc:0.952]
Epoch [51/120    avg_loss:0.188, val_acc:0.969]
Epoch [52/120    avg_loss:0.190, val_acc:0.960]
Epoch [53/120    avg_loss:0.224, val_acc:0.960]
Epoch [54/120    avg_loss:0.177, val_acc:0.977]
Epoch [55/120    avg_loss:0.165, val_acc:0.971]
Epoch [56/120    avg_loss:0.185, val_acc:0.942]
Epoch [57/120    avg_loss:0.178, val_acc:0.967]
Epoch [58/120    avg_loss:0.156, val_acc:0.973]
Epoch [59/120    avg_loss:0.140, val_acc:0.958]
Epoch [60/120    avg_loss:0.134, val_acc:0.969]
Epoch [61/120    avg_loss:0.168, val_acc:0.973]
Epoch [62/120    avg_loss:0.144, val_acc:0.971]
Epoch [63/120    avg_loss:0.109, val_acc:0.979]
Epoch [64/120    avg_loss:0.095, val_acc:0.969]
Epoch [65/120    avg_loss:0.108, val_acc:0.971]
Epoch [66/120    avg_loss:0.105, val_acc:0.971]
Epoch [67/120    avg_loss:0.137, val_acc:0.946]
Epoch [68/120    avg_loss:0.207, val_acc:0.973]
Epoch [69/120    avg_loss:0.193, val_acc:0.971]
Epoch [70/120    avg_loss:0.119, val_acc:0.969]
Epoch [71/120    avg_loss:0.109, val_acc:0.975]
Epoch [72/120    avg_loss:0.113, val_acc:0.960]
Epoch [73/120    avg_loss:0.101, val_acc:0.977]
Epoch [74/120    avg_loss:0.085, val_acc:0.983]
Epoch [75/120    avg_loss:0.080, val_acc:0.956]
Epoch [76/120    avg_loss:0.142, val_acc:0.969]
Epoch [77/120    avg_loss:0.112, val_acc:0.981]
Epoch [78/120    avg_loss:0.079, val_acc:0.983]
Epoch [79/120    avg_loss:0.081, val_acc:0.985]
Epoch [80/120    avg_loss:0.092, val_acc:0.985]
Epoch [81/120    avg_loss:0.084, val_acc:0.985]
Epoch [82/120    avg_loss:0.100, val_acc:0.977]
Epoch [83/120    avg_loss:0.073, val_acc:0.973]
Epoch [84/120    avg_loss:0.078, val_acc:0.979]
Epoch [85/120    avg_loss:0.065, val_acc:0.985]
Epoch [86/120    avg_loss:0.049, val_acc:0.983]
Epoch [87/120    avg_loss:0.035, val_acc:0.981]
Epoch [88/120    avg_loss:0.046, val_acc:0.975]
Epoch [89/120    avg_loss:0.053, val_acc:0.973]
Epoch [90/120    avg_loss:0.066, val_acc:0.956]
Epoch [91/120    avg_loss:0.065, val_acc:0.969]
Epoch [92/120    avg_loss:0.082, val_acc:0.977]
Epoch [93/120    avg_loss:0.067, val_acc:0.988]
Epoch [94/120    avg_loss:0.063, val_acc:0.981]
Epoch [95/120    avg_loss:0.141, val_acc:0.979]
Epoch [96/120    avg_loss:0.154, val_acc:0.981]
Epoch [97/120    avg_loss:0.088, val_acc:0.981]
Epoch [98/120    avg_loss:0.096, val_acc:0.983]
Epoch [99/120    avg_loss:0.097, val_acc:0.977]
Epoch [100/120    avg_loss:0.114, val_acc:0.967]
Epoch [101/120    avg_loss:0.065, val_acc:0.985]
Epoch [102/120    avg_loss:0.039, val_acc:0.983]
Epoch [103/120    avg_loss:0.056, val_acc:0.990]
Epoch [104/120    avg_loss:0.053, val_acc:0.969]
Epoch [105/120    avg_loss:0.058, val_acc:0.967]
Epoch [106/120    avg_loss:0.067, val_acc:0.971]
Epoch [107/120    avg_loss:0.071, val_acc:0.975]
Epoch [108/120    avg_loss:0.060, val_acc:0.965]
Epoch [109/120    avg_loss:0.053, val_acc:0.988]
Epoch [110/120    avg_loss:0.034, val_acc:0.977]
Epoch [111/120    avg_loss:0.034, val_acc:0.985]
Epoch [112/120    avg_loss:0.049, val_acc:0.994]
Epoch [113/120    avg_loss:0.070, val_acc:0.981]
Epoch [114/120    avg_loss:0.070, val_acc:0.985]
Epoch [115/120    avg_loss:0.065, val_acc:0.975]
Epoch [116/120    avg_loss:0.039, val_acc:0.988]
Epoch [117/120    avg_loss:0.084, val_acc:0.948]
Epoch [118/120    avg_loss:0.097, val_acc:0.975]
Epoch [119/120    avg_loss:0.056, val_acc:0.985]
Epoch [120/120    avg_loss:0.037, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   6   0   0   2   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 0.99780862 0.99545455 0.98230088 0.930131   0.9109589
 0.99516908 0.9787234  0.99870968 1.         1.         0.99472296
 0.99556541 1.        ]

Kappa:
0.9897932013621145
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9cee77d940>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.565, val_acc:0.356]
Epoch [2/120    avg_loss:2.362, val_acc:0.440]
Epoch [3/120    avg_loss:2.203, val_acc:0.542]
Epoch [4/120    avg_loss:2.049, val_acc:0.552]
Epoch [5/120    avg_loss:1.905, val_acc:0.581]
Epoch [6/120    avg_loss:1.793, val_acc:0.608]
Epoch [7/120    avg_loss:1.634, val_acc:0.679]
Epoch [8/120    avg_loss:1.529, val_acc:0.704]
Epoch [9/120    avg_loss:1.393, val_acc:0.706]
Epoch [10/120    avg_loss:1.270, val_acc:0.713]
Epoch [11/120    avg_loss:1.193, val_acc:0.744]
Epoch [12/120    avg_loss:1.110, val_acc:0.731]
Epoch [13/120    avg_loss:1.019, val_acc:0.760]
Epoch [14/120    avg_loss:0.918, val_acc:0.777]
Epoch [15/120    avg_loss:0.847, val_acc:0.783]
Epoch [16/120    avg_loss:0.791, val_acc:0.796]
Epoch [17/120    avg_loss:0.734, val_acc:0.783]
Epoch [18/120    avg_loss:0.699, val_acc:0.796]
Epoch [19/120    avg_loss:0.657, val_acc:0.885]
Epoch [20/120    avg_loss:0.647, val_acc:0.871]
Epoch [21/120    avg_loss:0.607, val_acc:0.831]
Epoch [22/120    avg_loss:0.584, val_acc:0.854]
Epoch [23/120    avg_loss:0.525, val_acc:0.915]
Epoch [24/120    avg_loss:0.460, val_acc:0.950]
Epoch [25/120    avg_loss:0.484, val_acc:0.952]
Epoch [26/120    avg_loss:0.420, val_acc:0.938]
Epoch [27/120    avg_loss:0.373, val_acc:0.948]
Epoch [28/120    avg_loss:0.379, val_acc:0.944]
Epoch [29/120    avg_loss:0.377, val_acc:0.944]
Epoch [30/120    avg_loss:0.339, val_acc:0.915]
Epoch [31/120    avg_loss:0.338, val_acc:0.946]
Epoch [32/120    avg_loss:0.430, val_acc:0.908]
Epoch [33/120    avg_loss:0.350, val_acc:0.952]
Epoch [34/120    avg_loss:0.353, val_acc:0.956]
Epoch [35/120    avg_loss:0.330, val_acc:0.942]
Epoch [36/120    avg_loss:0.346, val_acc:0.963]
Epoch [37/120    avg_loss:0.282, val_acc:0.944]
Epoch [38/120    avg_loss:0.294, val_acc:0.956]
Epoch [39/120    avg_loss:0.237, val_acc:0.944]
Epoch [40/120    avg_loss:0.257, val_acc:0.917]
Epoch [41/120    avg_loss:0.231, val_acc:0.933]
Epoch [42/120    avg_loss:0.231, val_acc:0.960]
Epoch [43/120    avg_loss:0.178, val_acc:0.969]
Epoch [44/120    avg_loss:0.196, val_acc:0.958]
Epoch [45/120    avg_loss:0.191, val_acc:0.954]
Epoch [46/120    avg_loss:0.164, val_acc:0.981]
Epoch [47/120    avg_loss:0.150, val_acc:0.965]
Epoch [48/120    avg_loss:0.161, val_acc:0.977]
Epoch [49/120    avg_loss:0.163, val_acc:0.967]
Epoch [50/120    avg_loss:0.165, val_acc:0.938]
Epoch [51/120    avg_loss:0.336, val_acc:0.944]
Epoch [52/120    avg_loss:0.194, val_acc:0.958]
Epoch [53/120    avg_loss:0.304, val_acc:0.929]
Epoch [54/120    avg_loss:0.242, val_acc:0.967]
Epoch [55/120    avg_loss:0.212, val_acc:0.958]
Epoch [56/120    avg_loss:0.214, val_acc:0.942]
Epoch [57/120    avg_loss:0.148, val_acc:0.979]
Epoch [58/120    avg_loss:0.146, val_acc:0.981]
Epoch [59/120    avg_loss:0.225, val_acc:0.956]
Epoch [60/120    avg_loss:0.235, val_acc:0.940]
Epoch [61/120    avg_loss:0.293, val_acc:0.967]
Epoch [62/120    avg_loss:0.213, val_acc:0.979]
Epoch [63/120    avg_loss:0.127, val_acc:0.983]
Epoch [64/120    avg_loss:0.108, val_acc:0.971]
Epoch [65/120    avg_loss:0.142, val_acc:0.967]
Epoch [66/120    avg_loss:0.120, val_acc:0.985]
Epoch [67/120    avg_loss:0.135, val_acc:0.979]
Epoch [68/120    avg_loss:0.121, val_acc:0.963]
Epoch [69/120    avg_loss:0.119, val_acc:0.990]
Epoch [70/120    avg_loss:0.099, val_acc:0.979]
Epoch [71/120    avg_loss:0.114, val_acc:0.981]
Epoch [72/120    avg_loss:0.114, val_acc:0.988]
Epoch [73/120    avg_loss:0.111, val_acc:0.979]
Epoch [74/120    avg_loss:0.119, val_acc:0.990]
Epoch [75/120    avg_loss:0.074, val_acc:0.983]
Epoch [76/120    avg_loss:0.131, val_acc:0.979]
Epoch [77/120    avg_loss:0.105, val_acc:0.988]
Epoch [78/120    avg_loss:0.104, val_acc:0.977]
Epoch [79/120    avg_loss:0.085, val_acc:0.983]
Epoch [80/120    avg_loss:0.077, val_acc:0.979]
Epoch [81/120    avg_loss:0.100, val_acc:0.979]
Epoch [82/120    avg_loss:0.058, val_acc:0.990]
Epoch [83/120    avg_loss:0.054, val_acc:0.979]
Epoch [84/120    avg_loss:0.044, val_acc:0.996]
Epoch [85/120    avg_loss:0.047, val_acc:0.994]
Epoch [86/120    avg_loss:0.049, val_acc:0.983]
Epoch [87/120    avg_loss:0.058, val_acc:0.983]
Epoch [88/120    avg_loss:0.053, val_acc:0.981]
Epoch [89/120    avg_loss:0.053, val_acc:0.979]
Epoch [90/120    avg_loss:0.049, val_acc:0.985]
Epoch [91/120    avg_loss:0.038, val_acc:0.979]
Epoch [92/120    avg_loss:0.041, val_acc:0.996]
Epoch [93/120    avg_loss:0.037, val_acc:0.981]
Epoch [94/120    avg_loss:0.038, val_acc:0.990]
Epoch [95/120    avg_loss:0.061, val_acc:0.992]
Epoch [96/120    avg_loss:0.041, val_acc:0.981]
Epoch [97/120    avg_loss:0.038, val_acc:0.994]
Epoch [98/120    avg_loss:0.038, val_acc:0.985]
Epoch [99/120    avg_loss:0.035, val_acc:0.990]
Epoch [100/120    avg_loss:0.055, val_acc:0.983]
Epoch [101/120    avg_loss:0.042, val_acc:0.985]
Epoch [102/120    avg_loss:0.032, val_acc:0.990]
Epoch [103/120    avg_loss:0.036, val_acc:0.983]
Epoch [104/120    avg_loss:0.036, val_acc:0.983]
Epoch [105/120    avg_loss:0.050, val_acc:0.988]
Epoch [106/120    avg_loss:0.043, val_acc:0.983]
Epoch [107/120    avg_loss:0.025, val_acc:0.985]
Epoch [108/120    avg_loss:0.030, val_acc:0.988]
Epoch [109/120    avg_loss:0.024, val_acc:0.988]
Epoch [110/120    avg_loss:0.028, val_acc:0.988]
Epoch [111/120    avg_loss:0.021, val_acc:0.988]
Epoch [112/120    avg_loss:0.024, val_acc:0.988]
Epoch [113/120    avg_loss:0.025, val_acc:0.988]
Epoch [114/120    avg_loss:0.038, val_acc:0.983]
Epoch [115/120    avg_loss:0.023, val_acc:0.985]
Epoch [116/120    avg_loss:0.026, val_acc:0.992]
Epoch [117/120    avg_loss:0.019, val_acc:0.990]
Epoch [118/120    avg_loss:0.021, val_acc:0.990]
Epoch [119/120    avg_loss:0.019, val_acc:0.990]
Epoch [120/120    avg_loss:0.016, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 676   0   0   0   0   9   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  11   0   0   0   0   0   0   1   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   1   0   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  13 440   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 0.99265786 0.99095023 1.         0.94298246 0.91289199
 0.97862233 0.97826087 0.99870968 1.         1.         0.98305085
 0.98434004 1.        ]

Kappa:
0.9874213486387001
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8793df5908>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.572, val_acc:0.383]
Epoch [2/120    avg_loss:2.362, val_acc:0.415]
Epoch [3/120    avg_loss:2.217, val_acc:0.542]
Epoch [4/120    avg_loss:2.087, val_acc:0.581]
Epoch [5/120    avg_loss:1.944, val_acc:0.554]
Epoch [6/120    avg_loss:1.821, val_acc:0.540]
Epoch [7/120    avg_loss:1.656, val_acc:0.575]
Epoch [8/120    avg_loss:1.520, val_acc:0.646]
Epoch [9/120    avg_loss:1.400, val_acc:0.683]
Epoch [10/120    avg_loss:1.308, val_acc:0.715]
Epoch [11/120    avg_loss:1.182, val_acc:0.704]
Epoch [12/120    avg_loss:1.080, val_acc:0.746]
Epoch [13/120    avg_loss:0.966, val_acc:0.742]
Epoch [14/120    avg_loss:0.912, val_acc:0.754]
Epoch [15/120    avg_loss:0.825, val_acc:0.815]
Epoch [16/120    avg_loss:0.755, val_acc:0.806]
Epoch [17/120    avg_loss:0.700, val_acc:0.810]
Epoch [18/120    avg_loss:0.631, val_acc:0.850]
Epoch [19/120    avg_loss:0.573, val_acc:0.890]
Epoch [20/120    avg_loss:0.579, val_acc:0.896]
Epoch [21/120    avg_loss:0.563, val_acc:0.877]
Epoch [22/120    avg_loss:0.604, val_acc:0.831]
Epoch [23/120    avg_loss:0.549, val_acc:0.871]
Epoch [24/120    avg_loss:0.485, val_acc:0.844]
Epoch [25/120    avg_loss:0.479, val_acc:0.917]
Epoch [26/120    avg_loss:0.436, val_acc:0.933]
Epoch [27/120    avg_loss:0.407, val_acc:0.919]
Epoch [28/120    avg_loss:0.353, val_acc:0.931]
Epoch [29/120    avg_loss:0.309, val_acc:0.950]
Epoch [30/120    avg_loss:0.311, val_acc:0.931]
Epoch [31/120    avg_loss:0.354, val_acc:0.927]
Epoch [32/120    avg_loss:0.313, val_acc:0.931]
Epoch [33/120    avg_loss:0.309, val_acc:0.946]
Epoch [34/120    avg_loss:0.262, val_acc:0.956]
Epoch [35/120    avg_loss:0.262, val_acc:0.952]
Epoch [36/120    avg_loss:0.245, val_acc:0.954]
Epoch [37/120    avg_loss:0.219, val_acc:0.956]
Epoch [38/120    avg_loss:0.226, val_acc:0.950]
Epoch [39/120    avg_loss:0.247, val_acc:0.933]
Epoch [40/120    avg_loss:0.242, val_acc:0.925]
Epoch [41/120    avg_loss:0.278, val_acc:0.935]
Epoch [42/120    avg_loss:0.228, val_acc:0.950]
Epoch [43/120    avg_loss:0.251, val_acc:0.946]
Epoch [44/120    avg_loss:0.230, val_acc:0.960]
Epoch [45/120    avg_loss:0.202, val_acc:0.960]
Epoch [46/120    avg_loss:0.168, val_acc:0.952]
Epoch [47/120    avg_loss:0.152, val_acc:0.967]
Epoch [48/120    avg_loss:0.127, val_acc:0.958]
Epoch [49/120    avg_loss:0.129, val_acc:0.967]
Epoch [50/120    avg_loss:0.140, val_acc:0.965]
Epoch [51/120    avg_loss:0.123, val_acc:0.960]
Epoch [52/120    avg_loss:0.103, val_acc:0.965]
Epoch [53/120    avg_loss:0.141, val_acc:0.967]
Epoch [54/120    avg_loss:0.119, val_acc:0.969]
Epoch [55/120    avg_loss:0.108, val_acc:0.963]
Epoch [56/120    avg_loss:0.140, val_acc:0.960]
Epoch [57/120    avg_loss:0.145, val_acc:0.977]
Epoch [58/120    avg_loss:0.135, val_acc:0.973]
Epoch [59/120    avg_loss:0.147, val_acc:0.960]
Epoch [60/120    avg_loss:0.165, val_acc:0.960]
Epoch [61/120    avg_loss:0.164, val_acc:0.960]
Epoch [62/120    avg_loss:0.168, val_acc:0.931]
Epoch [63/120    avg_loss:0.179, val_acc:0.958]
Epoch [64/120    avg_loss:0.133, val_acc:0.969]
Epoch [65/120    avg_loss:0.116, val_acc:0.975]
Epoch [66/120    avg_loss:0.106, val_acc:0.965]
Epoch [67/120    avg_loss:0.143, val_acc:0.946]
Epoch [68/120    avg_loss:0.177, val_acc:0.975]
Epoch [69/120    avg_loss:0.114, val_acc:0.975]
Epoch [70/120    avg_loss:0.088, val_acc:0.969]
Epoch [71/120    avg_loss:0.077, val_acc:0.971]
Epoch [72/120    avg_loss:0.071, val_acc:0.975]
Epoch [73/120    avg_loss:0.078, val_acc:0.975]
Epoch [74/120    avg_loss:0.059, val_acc:0.975]
Epoch [75/120    avg_loss:0.062, val_acc:0.977]
Epoch [76/120    avg_loss:0.078, val_acc:0.977]
Epoch [77/120    avg_loss:0.064, val_acc:0.977]
Epoch [78/120    avg_loss:0.072, val_acc:0.981]
Epoch [79/120    avg_loss:0.066, val_acc:0.981]
Epoch [80/120    avg_loss:0.060, val_acc:0.979]
Epoch [81/120    avg_loss:0.053, val_acc:0.981]
Epoch [82/120    avg_loss:0.062, val_acc:0.981]
Epoch [83/120    avg_loss:0.059, val_acc:0.979]
Epoch [84/120    avg_loss:0.060, val_acc:0.985]
Epoch [85/120    avg_loss:0.048, val_acc:0.983]
Epoch [86/120    avg_loss:0.052, val_acc:0.983]
Epoch [87/120    avg_loss:0.060, val_acc:0.983]
Epoch [88/120    avg_loss:0.051, val_acc:0.977]
Epoch [89/120    avg_loss:0.053, val_acc:0.979]
Epoch [90/120    avg_loss:0.062, val_acc:0.985]
Epoch [91/120    avg_loss:0.050, val_acc:0.983]
Epoch [92/120    avg_loss:0.056, val_acc:0.985]
Epoch [93/120    avg_loss:0.053, val_acc:0.983]
Epoch [94/120    avg_loss:0.054, val_acc:0.983]
Epoch [95/120    avg_loss:0.043, val_acc:0.979]
Epoch [96/120    avg_loss:0.055, val_acc:0.983]
Epoch [97/120    avg_loss:0.047, val_acc:0.981]
Epoch [98/120    avg_loss:0.057, val_acc:0.979]
Epoch [99/120    avg_loss:0.058, val_acc:0.983]
Epoch [100/120    avg_loss:0.050, val_acc:0.983]
Epoch [101/120    avg_loss:0.049, val_acc:0.985]
Epoch [102/120    avg_loss:0.047, val_acc:0.985]
Epoch [103/120    avg_loss:0.054, val_acc:0.985]
Epoch [104/120    avg_loss:0.043, val_acc:0.985]
Epoch [105/120    avg_loss:0.049, val_acc:0.985]
Epoch [106/120    avg_loss:0.051, val_acc:0.985]
Epoch [107/120    avg_loss:0.052, val_acc:0.985]
Epoch [108/120    avg_loss:0.048, val_acc:0.985]
Epoch [109/120    avg_loss:0.043, val_acc:0.985]
Epoch [110/120    avg_loss:0.044, val_acc:0.985]
Epoch [111/120    avg_loss:0.047, val_acc:0.985]
Epoch [112/120    avg_loss:0.045, val_acc:0.985]
Epoch [113/120    avg_loss:0.043, val_acc:0.985]
Epoch [114/120    avg_loss:0.038, val_acc:0.985]
Epoch [115/120    avg_loss:0.048, val_acc:0.985]
Epoch [116/120    avg_loss:0.053, val_acc:0.985]
Epoch [117/120    avg_loss:0.042, val_acc:0.985]
Epoch [118/120    avg_loss:0.042, val_acc:0.985]
Epoch [119/120    avg_loss:0.039, val_acc:0.985]
Epoch [120/120    avg_loss:0.041, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 0.99853801 0.98871332 1.         0.95053763 0.91756272
 0.99516908 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9928783512558118
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:10:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb5099e4898>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.517, val_acc:0.181]
Epoch [2/120    avg_loss:2.344, val_acc:0.383]
Epoch [3/120    avg_loss:2.203, val_acc:0.508]
Epoch [4/120    avg_loss:2.069, val_acc:0.558]
Epoch [5/120    avg_loss:1.935, val_acc:0.625]
Epoch [6/120    avg_loss:1.810, val_acc:0.637]
Epoch [7/120    avg_loss:1.673, val_acc:0.671]
Epoch [8/120    avg_loss:1.551, val_acc:0.698]
Epoch [9/120    avg_loss:1.434, val_acc:0.683]
Epoch [10/120    avg_loss:1.333, val_acc:0.733]
Epoch [11/120    avg_loss:1.218, val_acc:0.746]
Epoch [12/120    avg_loss:1.111, val_acc:0.756]
Epoch [13/120    avg_loss:1.035, val_acc:0.783]
Epoch [14/120    avg_loss:0.969, val_acc:0.790]
Epoch [15/120    avg_loss:0.928, val_acc:0.787]
Epoch [16/120    avg_loss:0.827, val_acc:0.810]
Epoch [17/120    avg_loss:0.747, val_acc:0.798]
Epoch [18/120    avg_loss:0.741, val_acc:0.842]
Epoch [19/120    avg_loss:0.694, val_acc:0.800]
Epoch [20/120    avg_loss:0.636, val_acc:0.877]
Epoch [21/120    avg_loss:0.582, val_acc:0.877]
Epoch [22/120    avg_loss:0.554, val_acc:0.900]
Epoch [23/120    avg_loss:0.558, val_acc:0.904]
Epoch [24/120    avg_loss:0.487, val_acc:0.875]
Epoch [25/120    avg_loss:0.475, val_acc:0.856]
Epoch [26/120    avg_loss:0.495, val_acc:0.906]
Epoch [27/120    avg_loss:0.431, val_acc:0.923]
Epoch [28/120    avg_loss:0.450, val_acc:0.938]
Epoch [29/120    avg_loss:0.386, val_acc:0.946]
Epoch [30/120    avg_loss:0.368, val_acc:0.931]
Epoch [31/120    avg_loss:0.346, val_acc:0.948]
Epoch [32/120    avg_loss:0.373, val_acc:0.923]
Epoch [33/120    avg_loss:0.350, val_acc:0.925]
Epoch [34/120    avg_loss:0.303, val_acc:0.960]
Epoch [35/120    avg_loss:0.300, val_acc:0.940]
Epoch [36/120    avg_loss:0.348, val_acc:0.946]
Epoch [37/120    avg_loss:0.295, val_acc:0.929]
Epoch [38/120    avg_loss:0.287, val_acc:0.963]
Epoch [39/120    avg_loss:0.266, val_acc:0.965]
Epoch [40/120    avg_loss:0.272, val_acc:0.944]
Epoch [41/120    avg_loss:0.278, val_acc:0.973]
Epoch [42/120    avg_loss:0.250, val_acc:0.967]
Epoch [43/120    avg_loss:0.245, val_acc:0.963]
Epoch [44/120    avg_loss:0.238, val_acc:0.967]
Epoch [45/120    avg_loss:0.222, val_acc:0.977]
Epoch [46/120    avg_loss:0.220, val_acc:0.967]
Epoch [47/120    avg_loss:0.173, val_acc:0.965]
Epoch [48/120    avg_loss:0.173, val_acc:0.979]
Epoch [49/120    avg_loss:0.222, val_acc:0.963]
Epoch [50/120    avg_loss:0.176, val_acc:0.979]
Epoch [51/120    avg_loss:0.246, val_acc:0.923]
Epoch [52/120    avg_loss:0.234, val_acc:0.944]
Epoch [53/120    avg_loss:0.194, val_acc:0.965]
Epoch [54/120    avg_loss:0.153, val_acc:0.983]
Epoch [55/120    avg_loss:0.167, val_acc:0.977]
Epoch [56/120    avg_loss:0.155, val_acc:0.963]
Epoch [57/120    avg_loss:0.153, val_acc:0.981]
Epoch [58/120    avg_loss:0.147, val_acc:0.983]
Epoch [59/120    avg_loss:0.190, val_acc:0.969]
Epoch [60/120    avg_loss:0.171, val_acc:0.965]
Epoch [61/120    avg_loss:0.161, val_acc:0.981]
Epoch [62/120    avg_loss:0.203, val_acc:0.973]
Epoch [63/120    avg_loss:0.177, val_acc:0.969]
Epoch [64/120    avg_loss:0.132, val_acc:0.969]
Epoch [65/120    avg_loss:0.153, val_acc:0.977]
Epoch [66/120    avg_loss:0.123, val_acc:0.977]
Epoch [67/120    avg_loss:0.108, val_acc:0.992]
Epoch [68/120    avg_loss:0.117, val_acc:0.971]
Epoch [69/120    avg_loss:0.097, val_acc:0.988]
Epoch [70/120    avg_loss:0.103, val_acc:0.977]
Epoch [71/120    avg_loss:0.067, val_acc:0.990]
Epoch [72/120    avg_loss:0.077, val_acc:0.992]
Epoch [73/120    avg_loss:0.090, val_acc:0.967]
Epoch [74/120    avg_loss:0.107, val_acc:0.985]
Epoch [75/120    avg_loss:0.117, val_acc:0.981]
Epoch [76/120    avg_loss:0.114, val_acc:0.983]
Epoch [77/120    avg_loss:0.106, val_acc:0.983]
Epoch [78/120    avg_loss:0.101, val_acc:0.985]
Epoch [79/120    avg_loss:0.067, val_acc:0.988]
Epoch [80/120    avg_loss:0.074, val_acc:0.996]
Epoch [81/120    avg_loss:0.097, val_acc:0.981]
Epoch [82/120    avg_loss:0.079, val_acc:0.990]
Epoch [83/120    avg_loss:0.065, val_acc:0.992]
Epoch [84/120    avg_loss:0.060, val_acc:1.000]
Epoch [85/120    avg_loss:0.090, val_acc:0.992]
Epoch [86/120    avg_loss:0.051, val_acc:0.992]
Epoch [87/120    avg_loss:0.044, val_acc:0.994]
Epoch [88/120    avg_loss:0.055, val_acc:0.992]
Epoch [89/120    avg_loss:0.051, val_acc:0.996]
Epoch [90/120    avg_loss:0.041, val_acc:0.996]
Epoch [91/120    avg_loss:0.081, val_acc:0.967]
Epoch [92/120    avg_loss:0.123, val_acc:0.940]
Epoch [93/120    avg_loss:0.084, val_acc:0.992]
Epoch [94/120    avg_loss:0.077, val_acc:0.992]
Epoch [95/120    avg_loss:0.062, val_acc:0.990]
Epoch [96/120    avg_loss:0.039, val_acc:0.988]
Epoch [97/120    avg_loss:0.038, val_acc:0.990]
Epoch [98/120    avg_loss:0.064, val_acc:0.994]
Epoch [99/120    avg_loss:0.027, val_acc:0.996]
Epoch [100/120    avg_loss:0.028, val_acc:0.998]
Epoch [101/120    avg_loss:0.024, val_acc:0.998]
Epoch [102/120    avg_loss:0.025, val_acc:0.998]
Epoch [103/120    avg_loss:0.023, val_acc:0.998]
Epoch [104/120    avg_loss:0.025, val_acc:0.998]
Epoch [105/120    avg_loss:0.026, val_acc:0.996]
Epoch [106/120    avg_loss:0.021, val_acc:0.996]
Epoch [107/120    avg_loss:0.026, val_acc:0.996]
Epoch [108/120    avg_loss:0.023, val_acc:0.996]
Epoch [109/120    avg_loss:0.026, val_acc:0.996]
Epoch [110/120    avg_loss:0.019, val_acc:0.996]
Epoch [111/120    avg_loss:0.027, val_acc:0.996]
Epoch [112/120    avg_loss:0.023, val_acc:0.996]
Epoch [113/120    avg_loss:0.020, val_acc:0.998]
Epoch [114/120    avg_loss:0.023, val_acc:0.998]
Epoch [115/120    avg_loss:0.020, val_acc:0.998]
Epoch [116/120    avg_loss:0.025, val_acc:0.998]
Epoch [117/120    avg_loss:0.024, val_acc:0.996]
Epoch [118/120    avg_loss:0.021, val_acc:0.998]
Epoch [119/120    avg_loss:0.025, val_acc:0.998]
Epoch [120/120    avg_loss:0.021, val_acc:0.998]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.68017057569296

F1 scores:
[       nan 0.99707174 0.99545455 1.         0.98434004 0.97643098
 0.99038462 0.98924731 1.         1.         1.         0.9973545
 0.99778761 1.        ]

Kappa:
0.9964397092211581
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1b88046908>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.526, val_acc:0.373]
Epoch [2/120    avg_loss:2.330, val_acc:0.396]
Epoch [3/120    avg_loss:2.188, val_acc:0.519]
Epoch [4/120    avg_loss:2.050, val_acc:0.569]
Epoch [5/120    avg_loss:1.894, val_acc:0.585]
Epoch [6/120    avg_loss:1.764, val_acc:0.585]
Epoch [7/120    avg_loss:1.607, val_acc:0.604]
Epoch [8/120    avg_loss:1.488, val_acc:0.698]
Epoch [9/120    avg_loss:1.367, val_acc:0.717]
Epoch [10/120    avg_loss:1.255, val_acc:0.717]
Epoch [11/120    avg_loss:1.161, val_acc:0.729]
Epoch [12/120    avg_loss:1.068, val_acc:0.744]
Epoch [13/120    avg_loss:0.983, val_acc:0.758]
Epoch [14/120    avg_loss:0.930, val_acc:0.783]
Epoch [15/120    avg_loss:0.869, val_acc:0.783]
Epoch [16/120    avg_loss:0.880, val_acc:0.771]
Epoch [17/120    avg_loss:0.822, val_acc:0.792]
Epoch [18/120    avg_loss:0.768, val_acc:0.794]
Epoch [19/120    avg_loss:0.672, val_acc:0.825]
Epoch [20/120    avg_loss:0.624, val_acc:0.831]
Epoch [21/120    avg_loss:0.631, val_acc:0.810]
Epoch [22/120    avg_loss:0.610, val_acc:0.896]
Epoch [23/120    avg_loss:0.556, val_acc:0.915]
Epoch [24/120    avg_loss:0.483, val_acc:0.919]
Epoch [25/120    avg_loss:0.458, val_acc:0.935]
Epoch [26/120    avg_loss:0.433, val_acc:0.915]
Epoch [27/120    avg_loss:0.457, val_acc:0.938]
Epoch [28/120    avg_loss:0.421, val_acc:0.940]
Epoch [29/120    avg_loss:0.413, val_acc:0.883]
Epoch [30/120    avg_loss:0.358, val_acc:0.938]
Epoch [31/120    avg_loss:0.346, val_acc:0.923]
Epoch [32/120    avg_loss:0.380, val_acc:0.950]
Epoch [33/120    avg_loss:0.297, val_acc:0.956]
Epoch [34/120    avg_loss:0.257, val_acc:0.969]
Epoch [35/120    avg_loss:0.256, val_acc:0.954]
Epoch [36/120    avg_loss:0.293, val_acc:0.960]
Epoch [37/120    avg_loss:0.293, val_acc:0.948]
Epoch [38/120    avg_loss:0.284, val_acc:0.956]
Epoch [39/120    avg_loss:0.267, val_acc:0.956]
Epoch [40/120    avg_loss:0.221, val_acc:0.973]
Epoch [41/120    avg_loss:0.227, val_acc:0.950]
Epoch [42/120    avg_loss:0.241, val_acc:0.963]
Epoch [43/120    avg_loss:0.197, val_acc:0.975]
Epoch [44/120    avg_loss:0.181, val_acc:0.954]
Epoch [45/120    avg_loss:0.224, val_acc:0.898]
Epoch [46/120    avg_loss:0.273, val_acc:0.942]
Epoch [47/120    avg_loss:0.257, val_acc:0.948]
Epoch [48/120    avg_loss:0.230, val_acc:0.967]
Epoch [49/120    avg_loss:0.212, val_acc:0.981]
Epoch [50/120    avg_loss:0.190, val_acc:0.977]
Epoch [51/120    avg_loss:0.206, val_acc:0.977]
Epoch [52/120    avg_loss:0.156, val_acc:0.985]
Epoch [53/120    avg_loss:0.140, val_acc:0.985]
Epoch [54/120    avg_loss:0.146, val_acc:0.977]
Epoch [55/120    avg_loss:0.138, val_acc:0.985]
Epoch [56/120    avg_loss:0.131, val_acc:0.981]
Epoch [57/120    avg_loss:0.127, val_acc:0.996]
Epoch [58/120    avg_loss:0.180, val_acc:0.942]
Epoch [59/120    avg_loss:0.158, val_acc:0.988]
Epoch [60/120    avg_loss:0.131, val_acc:0.979]
Epoch [61/120    avg_loss:0.146, val_acc:0.983]
Epoch [62/120    avg_loss:0.141, val_acc:0.990]
Epoch [63/120    avg_loss:0.113, val_acc:0.981]
Epoch [64/120    avg_loss:0.101, val_acc:0.952]
Epoch [65/120    avg_loss:0.105, val_acc:0.981]
Epoch [66/120    avg_loss:0.093, val_acc:0.994]
Epoch [67/120    avg_loss:0.094, val_acc:0.996]
Epoch [68/120    avg_loss:0.113, val_acc:0.992]
Epoch [69/120    avg_loss:0.105, val_acc:0.996]
Epoch [70/120    avg_loss:0.117, val_acc:0.994]
Epoch [71/120    avg_loss:0.078, val_acc:0.979]
Epoch [72/120    avg_loss:0.079, val_acc:0.998]
Epoch [73/120    avg_loss:0.066, val_acc:0.992]
Epoch [74/120    avg_loss:0.102, val_acc:0.963]
Epoch [75/120    avg_loss:0.090, val_acc:0.996]
Epoch [76/120    avg_loss:0.098, val_acc:0.994]
Epoch [77/120    avg_loss:0.064, val_acc:0.992]
Epoch [78/120    avg_loss:0.083, val_acc:0.998]
Epoch [79/120    avg_loss:0.075, val_acc:1.000]
Epoch [80/120    avg_loss:0.082, val_acc:0.994]
Epoch [81/120    avg_loss:0.048, val_acc:0.985]
Epoch [82/120    avg_loss:0.050, val_acc:0.998]
Epoch [83/120    avg_loss:0.051, val_acc:0.990]
Epoch [84/120    avg_loss:0.062, val_acc:0.975]
Epoch [85/120    avg_loss:0.086, val_acc:0.919]
Epoch [86/120    avg_loss:0.081, val_acc:0.996]
Epoch [87/120    avg_loss:0.095, val_acc:0.954]
Epoch [88/120    avg_loss:0.068, val_acc:0.992]
Epoch [89/120    avg_loss:0.062, val_acc:0.994]
Epoch [90/120    avg_loss:0.035, val_acc:0.998]
Epoch [91/120    avg_loss:0.055, val_acc:0.985]
Epoch [92/120    avg_loss:0.067, val_acc:0.994]
Epoch [93/120    avg_loss:0.040, val_acc:1.000]
Epoch [94/120    avg_loss:0.033, val_acc:1.000]
Epoch [95/120    avg_loss:0.027, val_acc:0.998]
Epoch [96/120    avg_loss:0.021, val_acc:0.998]
Epoch [97/120    avg_loss:0.028, val_acc:0.998]
Epoch [98/120    avg_loss:0.029, val_acc:0.996]
Epoch [99/120    avg_loss:0.022, val_acc:0.996]
Epoch [100/120    avg_loss:0.024, val_acc:0.996]
Epoch [101/120    avg_loss:0.029, val_acc:0.996]
Epoch [102/120    avg_loss:0.028, val_acc:0.998]
Epoch [103/120    avg_loss:0.023, val_acc:0.998]
Epoch [104/120    avg_loss:0.026, val_acc:0.996]
Epoch [105/120    avg_loss:0.025, val_acc:0.996]
Epoch [106/120    avg_loss:0.024, val_acc:0.996]
Epoch [107/120    avg_loss:0.028, val_acc:0.996]
Epoch [108/120    avg_loss:0.022, val_acc:0.996]
Epoch [109/120    avg_loss:0.021, val_acc:0.996]
Epoch [110/120    avg_loss:0.023, val_acc:0.996]
Epoch [111/120    avg_loss:0.024, val_acc:0.996]
Epoch [112/120    avg_loss:0.020, val_acc:0.996]
Epoch [113/120    avg_loss:0.023, val_acc:0.996]
Epoch [114/120    avg_loss:0.022, val_acc:0.996]
Epoch [115/120    avg_loss:0.018, val_acc:0.996]
Epoch [116/120    avg_loss:0.021, val_acc:0.996]
Epoch [117/120    avg_loss:0.029, val_acc:0.996]
Epoch [118/120    avg_loss:0.020, val_acc:0.996]
Epoch [119/120    avg_loss:0.022, val_acc:0.996]
Epoch [120/120    avg_loss:0.024, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 0.99560117 0.99095023 1.         0.96659243 0.94915254
 0.98564593 0.97826087 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9940662997505973
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc3583e5908>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.591, val_acc:0.369]
Epoch [2/120    avg_loss:2.380, val_acc:0.346]
Epoch [3/120    avg_loss:2.239, val_acc:0.469]
Epoch [4/120    avg_loss:2.091, val_acc:0.529]
Epoch [5/120    avg_loss:1.967, val_acc:0.560]
Epoch [6/120    avg_loss:1.819, val_acc:0.577]
Epoch [7/120    avg_loss:1.700, val_acc:0.623]
Epoch [8/120    avg_loss:1.585, val_acc:0.665]
Epoch [9/120    avg_loss:1.477, val_acc:0.669]
Epoch [10/120    avg_loss:1.372, val_acc:0.706]
Epoch [11/120    avg_loss:1.239, val_acc:0.702]
Epoch [12/120    avg_loss:1.128, val_acc:0.738]
Epoch [13/120    avg_loss:1.064, val_acc:0.756]
Epoch [14/120    avg_loss:0.967, val_acc:0.767]
Epoch [15/120    avg_loss:0.904, val_acc:0.773]
Epoch [16/120    avg_loss:0.843, val_acc:0.777]
Epoch [17/120    avg_loss:0.784, val_acc:0.773]
Epoch [18/120    avg_loss:0.731, val_acc:0.781]
Epoch [19/120    avg_loss:0.705, val_acc:0.771]
Epoch [20/120    avg_loss:0.648, val_acc:0.790]
Epoch [21/120    avg_loss:0.643, val_acc:0.781]
Epoch [22/120    avg_loss:0.601, val_acc:0.856]
Epoch [23/120    avg_loss:0.618, val_acc:0.792]
Epoch [24/120    avg_loss:0.556, val_acc:0.883]
Epoch [25/120    avg_loss:0.570, val_acc:0.833]
Epoch [26/120    avg_loss:0.492, val_acc:0.910]
Epoch [27/120    avg_loss:0.494, val_acc:0.923]
Epoch [28/120    avg_loss:0.431, val_acc:0.931]
Epoch [29/120    avg_loss:0.421, val_acc:0.940]
Epoch [30/120    avg_loss:0.391, val_acc:0.927]
Epoch [31/120    avg_loss:0.346, val_acc:0.938]
Epoch [32/120    avg_loss:0.358, val_acc:0.935]
Epoch [33/120    avg_loss:0.308, val_acc:0.940]
Epoch [34/120    avg_loss:0.358, val_acc:0.923]
Epoch [35/120    avg_loss:0.360, val_acc:0.917]
Epoch [36/120    avg_loss:0.303, val_acc:0.954]
Epoch [37/120    avg_loss:0.283, val_acc:0.931]
Epoch [38/120    avg_loss:0.293, val_acc:0.944]
Epoch [39/120    avg_loss:0.276, val_acc:0.929]
Epoch [40/120    avg_loss:0.256, val_acc:0.948]
Epoch [41/120    avg_loss:0.291, val_acc:0.956]
Epoch [42/120    avg_loss:0.219, val_acc:0.967]
Epoch [43/120    avg_loss:0.240, val_acc:0.948]
Epoch [44/120    avg_loss:0.226, val_acc:0.969]
Epoch [45/120    avg_loss:0.191, val_acc:0.971]
Epoch [46/120    avg_loss:0.208, val_acc:0.956]
Epoch [47/120    avg_loss:0.212, val_acc:0.948]
Epoch [48/120    avg_loss:0.175, val_acc:0.950]
Epoch [49/120    avg_loss:0.229, val_acc:0.965]
Epoch [50/120    avg_loss:0.163, val_acc:0.979]
Epoch [51/120    avg_loss:0.199, val_acc:0.975]
Epoch [52/120    avg_loss:0.153, val_acc:0.915]
Epoch [53/120    avg_loss:0.219, val_acc:0.963]
Epoch [54/120    avg_loss:0.225, val_acc:0.965]
Epoch [55/120    avg_loss:0.202, val_acc:0.950]
Epoch [56/120    avg_loss:0.167, val_acc:0.940]
Epoch [57/120    avg_loss:0.166, val_acc:0.985]
Epoch [58/120    avg_loss:0.134, val_acc:0.977]
Epoch [59/120    avg_loss:0.126, val_acc:0.971]
Epoch [60/120    avg_loss:0.138, val_acc:0.952]
Epoch [61/120    avg_loss:0.182, val_acc:0.944]
Epoch [62/120    avg_loss:0.202, val_acc:0.958]
Epoch [63/120    avg_loss:0.178, val_acc:0.963]
Epoch [64/120    avg_loss:0.169, val_acc:0.971]
Epoch [65/120    avg_loss:0.122, val_acc:0.952]
Epoch [66/120    avg_loss:0.110, val_acc:0.977]
Epoch [67/120    avg_loss:0.112, val_acc:0.975]
Epoch [68/120    avg_loss:0.133, val_acc:0.985]
Epoch [69/120    avg_loss:0.099, val_acc:0.975]
Epoch [70/120    avg_loss:0.095, val_acc:0.992]
Epoch [71/120    avg_loss:0.092, val_acc:0.979]
Epoch [72/120    avg_loss:0.081, val_acc:0.981]
Epoch [73/120    avg_loss:0.080, val_acc:0.985]
Epoch [74/120    avg_loss:0.126, val_acc:0.960]
Epoch [75/120    avg_loss:0.080, val_acc:0.977]
Epoch [76/120    avg_loss:0.131, val_acc:0.979]
Epoch [77/120    avg_loss:0.112, val_acc:0.981]
Epoch [78/120    avg_loss:0.084, val_acc:0.985]
Epoch [79/120    avg_loss:0.086, val_acc:0.977]
Epoch [80/120    avg_loss:0.083, val_acc:0.985]
Epoch [81/120    avg_loss:0.094, val_acc:0.981]
Epoch [82/120    avg_loss:0.074, val_acc:0.985]
Epoch [83/120    avg_loss:0.071, val_acc:0.998]
Epoch [84/120    avg_loss:0.061, val_acc:0.969]
Epoch [85/120    avg_loss:0.092, val_acc:0.979]
Epoch [86/120    avg_loss:0.096, val_acc:0.979]
Epoch [87/120    avg_loss:0.094, val_acc:0.977]
Epoch [88/120    avg_loss:0.111, val_acc:0.979]
Epoch [89/120    avg_loss:0.082, val_acc:0.954]
Epoch [90/120    avg_loss:0.089, val_acc:0.967]
Epoch [91/120    avg_loss:0.063, val_acc:0.979]
Epoch [92/120    avg_loss:0.053, val_acc:0.990]
Epoch [93/120    avg_loss:0.040, val_acc:0.979]
Epoch [94/120    avg_loss:0.032, val_acc:0.981]
Epoch [95/120    avg_loss:0.034, val_acc:0.988]
Epoch [96/120    avg_loss:0.045, val_acc:0.988]
Epoch [97/120    avg_loss:0.031, val_acc:0.988]
Epoch [98/120    avg_loss:0.033, val_acc:0.990]
Epoch [99/120    avg_loss:0.028, val_acc:0.988]
Epoch [100/120    avg_loss:0.031, val_acc:0.992]
Epoch [101/120    avg_loss:0.029, val_acc:0.992]
Epoch [102/120    avg_loss:0.026, val_acc:0.992]
Epoch [103/120    avg_loss:0.031, val_acc:0.992]
Epoch [104/120    avg_loss:0.028, val_acc:0.992]
Epoch [105/120    avg_loss:0.026, val_acc:0.992]
Epoch [106/120    avg_loss:0.024, val_acc:0.992]
Epoch [107/120    avg_loss:0.031, val_acc:0.992]
Epoch [108/120    avg_loss:0.023, val_acc:0.992]
Epoch [109/120    avg_loss:0.025, val_acc:0.992]
Epoch [110/120    avg_loss:0.022, val_acc:0.992]
Epoch [111/120    avg_loss:0.025, val_acc:0.992]
Epoch [112/120    avg_loss:0.019, val_acc:0.992]
Epoch [113/120    avg_loss:0.029, val_acc:0.992]
Epoch [114/120    avg_loss:0.022, val_acc:0.992]
Epoch [115/120    avg_loss:0.024, val_acc:0.992]
Epoch [116/120    avg_loss:0.028, val_acc:0.992]
Epoch [117/120    avg_loss:0.027, val_acc:0.992]
Epoch [118/120    avg_loss:0.022, val_acc:0.992]
Epoch [119/120    avg_loss:0.021, val_acc:0.992]
Epoch [120/120    avg_loss:0.026, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   7   0   0   0   0   0   0   2   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.59488272921108

F1 scores:
[       nan 0.99707174 1.         1.         0.96674058 0.95532646
 0.99038462 1.         1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9954901052575321
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:15
Validation dataloader:15
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe67be1c8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.556, val_acc:0.388]
Epoch [2/120    avg_loss:2.346, val_acc:0.381]
Epoch [3/120    avg_loss:2.220, val_acc:0.531]
Epoch [4/120    avg_loss:2.111, val_acc:0.565]
Epoch [5/120    avg_loss:1.992, val_acc:0.581]
Epoch [6/120    avg_loss:1.864, val_acc:0.596]
Epoch [7/120    avg_loss:1.735, val_acc:0.598]
Epoch [8/120    avg_loss:1.612, val_acc:0.667]
Epoch [9/120    avg_loss:1.504, val_acc:0.700]
Epoch [10/120    avg_loss:1.381, val_acc:0.708]
Epoch [11/120    avg_loss:1.284, val_acc:0.725]
Epoch [12/120    avg_loss:1.184, val_acc:0.740]
Epoch [13/120    avg_loss:1.121, val_acc:0.748]
Epoch [14/120    avg_loss:1.024, val_acc:0.783]
Epoch [15/120    avg_loss:0.929, val_acc:0.777]
Epoch [16/120    avg_loss:0.876, val_acc:0.785]
Epoch [17/120    avg_loss:0.829, val_acc:0.808]
Epoch [18/120    avg_loss:0.737, val_acc:0.831]
Epoch [19/120    avg_loss:0.694, val_acc:0.821]
Epoch [20/120    avg_loss:0.657, val_acc:0.844]
Epoch [21/120    avg_loss:0.651, val_acc:0.842]
Epoch [22/120    avg_loss:0.634, val_acc:0.819]
Epoch [23/120    avg_loss:0.554, val_acc:0.871]
Epoch [24/120    avg_loss:0.561, val_acc:0.823]
Epoch [25/120    avg_loss:0.559, val_acc:0.869]
Epoch [26/120    avg_loss:0.476, val_acc:0.900]
Epoch [27/120    avg_loss:0.452, val_acc:0.833]
Epoch [28/120    avg_loss:0.435, val_acc:0.817]
Epoch [29/120    avg_loss:0.409, val_acc:0.929]
Epoch [30/120    avg_loss:0.425, val_acc:0.892]
Epoch [31/120    avg_loss:0.395, val_acc:0.942]
Epoch [32/120    avg_loss:0.380, val_acc:0.860]
Epoch [33/120    avg_loss:0.418, val_acc:0.904]
Epoch [34/120    avg_loss:0.396, val_acc:0.927]
Epoch [35/120    avg_loss:0.400, val_acc:0.935]
Epoch [36/120    avg_loss:0.305, val_acc:0.958]
Epoch [37/120    avg_loss:0.354, val_acc:0.950]
Epoch [38/120    avg_loss:0.358, val_acc:0.931]
Epoch [39/120    avg_loss:0.289, val_acc:0.954]
Epoch [40/120    avg_loss:0.216, val_acc:0.944]
Epoch [41/120    avg_loss:0.248, val_acc:0.975]
Epoch [42/120    avg_loss:0.210, val_acc:0.971]
Epoch [43/120    avg_loss:0.281, val_acc:0.967]
Epoch [44/120    avg_loss:0.314, val_acc:0.963]
Epoch [45/120    avg_loss:0.255, val_acc:0.956]
Epoch [46/120    avg_loss:0.235, val_acc:0.965]
Epoch [47/120    avg_loss:0.247, val_acc:0.954]
Epoch [48/120    avg_loss:0.206, val_acc:0.973]
Epoch [49/120    avg_loss:0.176, val_acc:0.977]
Epoch [50/120    avg_loss:0.166, val_acc:0.973]
Epoch [51/120    avg_loss:0.200, val_acc:0.956]
Epoch [52/120    avg_loss:0.204, val_acc:0.952]
Epoch [53/120    avg_loss:0.163, val_acc:0.975]
Epoch [54/120    avg_loss:0.130, val_acc:0.990]
Epoch [55/120    avg_loss:0.139, val_acc:0.963]
Epoch [56/120    avg_loss:0.145, val_acc:0.958]
Epoch [57/120    avg_loss:0.213, val_acc:0.983]
Epoch [58/120    avg_loss:0.142, val_acc:0.988]
Epoch [59/120    avg_loss:0.141, val_acc:0.981]
Epoch [60/120    avg_loss:0.114, val_acc:0.985]
Epoch [61/120    avg_loss:0.134, val_acc:0.977]
Epoch [62/120    avg_loss:0.118, val_acc:0.977]
Epoch [63/120    avg_loss:0.109, val_acc:0.975]
Epoch [64/120    avg_loss:0.108, val_acc:0.983]
Epoch [65/120    avg_loss:0.093, val_acc:0.988]
Epoch [66/120    avg_loss:0.082, val_acc:0.988]
Epoch [67/120    avg_loss:0.067, val_acc:0.990]
Epoch [68/120    avg_loss:0.078, val_acc:0.988]
Epoch [69/120    avg_loss:0.059, val_acc:0.985]
Epoch [70/120    avg_loss:0.107, val_acc:0.983]
Epoch [71/120    avg_loss:0.090, val_acc:0.988]
Epoch [72/120    avg_loss:0.067, val_acc:0.988]
Epoch [73/120    avg_loss:0.059, val_acc:0.990]
Epoch [74/120    avg_loss:0.064, val_acc:0.983]
Epoch [75/120    avg_loss:0.071, val_acc:0.985]
Epoch [76/120    avg_loss:0.091, val_acc:0.983]
Epoch [77/120    avg_loss:0.105, val_acc:0.988]
Epoch [78/120    avg_loss:0.059, val_acc:0.988]
Epoch [79/120    avg_loss:0.057, val_acc:0.985]
Epoch [80/120    avg_loss:0.065, val_acc:0.990]
Epoch [81/120    avg_loss:0.074, val_acc:0.990]
Epoch [82/120    avg_loss:0.054, val_acc:0.990]
Epoch [83/120    avg_loss:0.044, val_acc:0.969]
Epoch [84/120    avg_loss:0.047, val_acc:0.990]
Epoch [85/120    avg_loss:0.057, val_acc:0.981]
Epoch [86/120    avg_loss:0.053, val_acc:0.988]
Epoch [87/120    avg_loss:0.047, val_acc:0.990]
Epoch [88/120    avg_loss:0.044, val_acc:0.988]
Epoch [89/120    avg_loss:0.056, val_acc:0.988]
Epoch [90/120    avg_loss:0.044, val_acc:0.990]
Epoch [91/120    avg_loss:0.041, val_acc:0.988]
Epoch [92/120    avg_loss:0.029, val_acc:0.988]
Epoch [93/120    avg_loss:0.040, val_acc:0.988]
Epoch [94/120    avg_loss:0.038, val_acc:0.988]
Epoch [95/120    avg_loss:0.031, val_acc:0.990]
Epoch [96/120    avg_loss:0.030, val_acc:0.988]
Epoch [97/120    avg_loss:0.028, val_acc:0.988]
Epoch [98/120    avg_loss:0.039, val_acc:0.990]
Epoch [99/120    avg_loss:0.048, val_acc:0.983]
Epoch [100/120    avg_loss:0.050, val_acc:0.983]
Epoch [101/120    avg_loss:0.050, val_acc:0.988]
Epoch [102/120    avg_loss:0.035, val_acc:0.990]
Epoch [103/120    avg_loss:0.047, val_acc:0.985]
Epoch [104/120    avg_loss:0.040, val_acc:0.985]
Epoch [105/120    avg_loss:0.040, val_acc:0.985]
Epoch [106/120    avg_loss:0.027, val_acc:0.988]
Epoch [107/120    avg_loss:0.024, val_acc:0.988]
Epoch [108/120    avg_loss:0.022, val_acc:0.988]
Epoch [109/120    avg_loss:0.029, val_acc:0.988]
Epoch [110/120    avg_loss:0.028, val_acc:0.990]
Epoch [111/120    avg_loss:0.028, val_acc:0.988]
Epoch [112/120    avg_loss:0.017, val_acc:0.990]
Epoch [113/120    avg_loss:0.020, val_acc:0.988]
Epoch [114/120    avg_loss:0.016, val_acc:0.988]
Epoch [115/120    avg_loss:0.013, val_acc:0.988]
Epoch [116/120    avg_loss:0.018, val_acc:0.988]
Epoch [117/120    avg_loss:0.015, val_acc:0.985]
Epoch [118/120    avg_loss:0.023, val_acc:0.985]
Epoch [119/120    avg_loss:0.023, val_acc:0.988]
Epoch [120/120    avg_loss:0.015, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 1.         1.         0.99782135 0.9452954  0.91666667
 1.         1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9940654152698459
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faf7f90b898>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.640, val_acc:0.104]
Epoch [2/120    avg_loss:2.435, val_acc:0.317]
Epoch [3/120    avg_loss:2.313, val_acc:0.383]
Epoch [4/120    avg_loss:2.198, val_acc:0.496]
Epoch [5/120    avg_loss:2.085, val_acc:0.525]
Epoch [6/120    avg_loss:1.972, val_acc:0.629]
Epoch [7/120    avg_loss:1.859, val_acc:0.646]
Epoch [8/120    avg_loss:1.716, val_acc:0.717]
Epoch [9/120    avg_loss:1.614, val_acc:0.708]
Epoch [10/120    avg_loss:1.511, val_acc:0.723]
Epoch [11/120    avg_loss:1.384, val_acc:0.758]
Epoch [12/120    avg_loss:1.254, val_acc:0.771]
Epoch [13/120    avg_loss:1.195, val_acc:0.842]
Epoch [14/120    avg_loss:1.086, val_acc:0.848]
Epoch [15/120    avg_loss:1.002, val_acc:0.852]
Epoch [16/120    avg_loss:0.922, val_acc:0.869]
Epoch [17/120    avg_loss:0.887, val_acc:0.873]
Epoch [18/120    avg_loss:0.807, val_acc:0.883]
Epoch [19/120    avg_loss:0.729, val_acc:0.883]
Epoch [20/120    avg_loss:0.693, val_acc:0.875]
Epoch [21/120    avg_loss:0.658, val_acc:0.887]
Epoch [22/120    avg_loss:0.642, val_acc:0.921]
Epoch [23/120    avg_loss:0.640, val_acc:0.885]
Epoch [24/120    avg_loss:0.595, val_acc:0.896]
Epoch [25/120    avg_loss:0.592, val_acc:0.931]
Epoch [26/120    avg_loss:0.567, val_acc:0.915]
Epoch [27/120    avg_loss:0.481, val_acc:0.925]
Epoch [28/120    avg_loss:0.496, val_acc:0.927]
Epoch [29/120    avg_loss:0.505, val_acc:0.904]
Epoch [30/120    avg_loss:0.441, val_acc:0.919]
Epoch [31/120    avg_loss:0.460, val_acc:0.902]
Epoch [32/120    avg_loss:0.469, val_acc:0.931]
Epoch [33/120    avg_loss:0.405, val_acc:0.931]
Epoch [34/120    avg_loss:0.399, val_acc:0.938]
Epoch [35/120    avg_loss:0.410, val_acc:0.917]
Epoch [36/120    avg_loss:0.397, val_acc:0.938]
Epoch [37/120    avg_loss:0.404, val_acc:0.933]
Epoch [38/120    avg_loss:0.403, val_acc:0.931]
Epoch [39/120    avg_loss:0.372, val_acc:0.931]
Epoch [40/120    avg_loss:0.387, val_acc:0.946]
Epoch [41/120    avg_loss:0.398, val_acc:0.917]
Epoch [42/120    avg_loss:0.364, val_acc:0.935]
Epoch [43/120    avg_loss:0.360, val_acc:0.904]
Epoch [44/120    avg_loss:0.386, val_acc:0.944]
Epoch [45/120    avg_loss:0.337, val_acc:0.952]
Epoch [46/120    avg_loss:0.313, val_acc:0.944]
Epoch [47/120    avg_loss:0.304, val_acc:0.952]
Epoch [48/120    avg_loss:0.283, val_acc:0.952]
Epoch [49/120    avg_loss:0.304, val_acc:0.948]
Epoch [50/120    avg_loss:0.280, val_acc:0.938]
Epoch [51/120    avg_loss:0.275, val_acc:0.958]
Epoch [52/120    avg_loss:0.268, val_acc:0.963]
Epoch [53/120    avg_loss:0.258, val_acc:0.940]
Epoch [54/120    avg_loss:0.285, val_acc:0.958]
Epoch [55/120    avg_loss:0.348, val_acc:0.958]
Epoch [56/120    avg_loss:0.262, val_acc:0.950]
Epoch [57/120    avg_loss:0.260, val_acc:0.942]
Epoch [58/120    avg_loss:0.254, val_acc:0.946]
Epoch [59/120    avg_loss:0.253, val_acc:0.950]
Epoch [60/120    avg_loss:0.236, val_acc:0.935]
Epoch [61/120    avg_loss:0.226, val_acc:0.948]
Epoch [62/120    avg_loss:0.262, val_acc:0.969]
Epoch [63/120    avg_loss:0.216, val_acc:0.977]
Epoch [64/120    avg_loss:0.224, val_acc:0.971]
Epoch [65/120    avg_loss:0.199, val_acc:0.956]
Epoch [66/120    avg_loss:0.241, val_acc:0.969]
Epoch [67/120    avg_loss:0.181, val_acc:0.958]
Epoch [68/120    avg_loss:0.178, val_acc:0.960]
Epoch [69/120    avg_loss:0.188, val_acc:0.969]
Epoch [70/120    avg_loss:0.164, val_acc:0.971]
Epoch [71/120    avg_loss:0.174, val_acc:0.960]
Epoch [72/120    avg_loss:0.198, val_acc:0.971]
Epoch [73/120    avg_loss:0.170, val_acc:0.969]
Epoch [74/120    avg_loss:0.178, val_acc:0.985]
Epoch [75/120    avg_loss:0.170, val_acc:0.950]
Epoch [76/120    avg_loss:0.158, val_acc:0.971]
Epoch [77/120    avg_loss:0.185, val_acc:0.973]
Epoch [78/120    avg_loss:0.195, val_acc:0.965]
Epoch [79/120    avg_loss:0.200, val_acc:0.965]
Epoch [80/120    avg_loss:0.168, val_acc:0.969]
Epoch [81/120    avg_loss:0.176, val_acc:0.960]
Epoch [82/120    avg_loss:0.149, val_acc:0.979]
Epoch [83/120    avg_loss:0.128, val_acc:0.975]
Epoch [84/120    avg_loss:0.114, val_acc:0.963]
Epoch [85/120    avg_loss:0.130, val_acc:0.975]
Epoch [86/120    avg_loss:0.099, val_acc:0.971]
Epoch [87/120    avg_loss:0.086, val_acc:0.985]
Epoch [88/120    avg_loss:0.127, val_acc:0.977]
Epoch [89/120    avg_loss:0.130, val_acc:0.965]
Epoch [90/120    avg_loss:0.146, val_acc:0.963]
Epoch [91/120    avg_loss:0.136, val_acc:0.971]
Epoch [92/120    avg_loss:0.133, val_acc:0.956]
Epoch [93/120    avg_loss:0.098, val_acc:0.967]
Epoch [94/120    avg_loss:0.143, val_acc:0.971]
Epoch [95/120    avg_loss:0.134, val_acc:0.975]
Epoch [96/120    avg_loss:0.124, val_acc:0.965]
Epoch [97/120    avg_loss:0.103, val_acc:0.979]
Epoch [98/120    avg_loss:0.097, val_acc:0.979]
Epoch [99/120    avg_loss:0.091, val_acc:0.977]
Epoch [100/120    avg_loss:0.087, val_acc:0.979]
Epoch [101/120    avg_loss:0.071, val_acc:0.979]
Epoch [102/120    avg_loss:0.069, val_acc:0.983]
Epoch [103/120    avg_loss:0.056, val_acc:0.979]
Epoch [104/120    avg_loss:0.062, val_acc:0.981]
Epoch [105/120    avg_loss:0.057, val_acc:0.977]
Epoch [106/120    avg_loss:0.068, val_acc:0.979]
Epoch [107/120    avg_loss:0.057, val_acc:0.979]
Epoch [108/120    avg_loss:0.050, val_acc:0.979]
Epoch [109/120    avg_loss:0.049, val_acc:0.979]
Epoch [110/120    avg_loss:0.055, val_acc:0.981]
Epoch [111/120    avg_loss:0.065, val_acc:0.979]
Epoch [112/120    avg_loss:0.057, val_acc:0.979]
Epoch [113/120    avg_loss:0.058, val_acc:0.977]
Epoch [114/120    avg_loss:0.064, val_acc:0.977]
Epoch [115/120    avg_loss:0.057, val_acc:0.979]
Epoch [116/120    avg_loss:0.046, val_acc:0.979]
Epoch [117/120    avg_loss:0.050, val_acc:0.979]
Epoch [118/120    avg_loss:0.052, val_acc:0.979]
Epoch [119/120    avg_loss:0.055, val_acc:0.979]
Epoch [120/120    avg_loss:0.053, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 206   0   0   0   0  13   0   0   0   0   0   0]
 [  0   0   0 220   9   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   2   0 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.65671641791045

F1 scores:
[       nan 0.997815   0.9321267  0.97777778 0.93932584 0.94155844
 0.99266504 0.84324324 1.         0.99893276 0.99726027 1.
 0.99667774 1.        ]

Kappa:
0.9850448661607387
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f19d1101898>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.565, val_acc:0.265]
Epoch [2/120    avg_loss:2.415, val_acc:0.306]
Epoch [3/120    avg_loss:2.297, val_acc:0.310]
Epoch [4/120    avg_loss:2.182, val_acc:0.367]
Epoch [5/120    avg_loss:2.069, val_acc:0.483]
Epoch [6/120    avg_loss:1.971, val_acc:0.504]
Epoch [7/120    avg_loss:1.882, val_acc:0.594]
Epoch [8/120    avg_loss:1.799, val_acc:0.581]
Epoch [9/120    avg_loss:1.710, val_acc:0.627]
Epoch [10/120    avg_loss:1.631, val_acc:0.642]
Epoch [11/120    avg_loss:1.554, val_acc:0.656]
Epoch [12/120    avg_loss:1.442, val_acc:0.665]
Epoch [13/120    avg_loss:1.376, val_acc:0.679]
Epoch [14/120    avg_loss:1.277, val_acc:0.704]
Epoch [15/120    avg_loss:1.191, val_acc:0.790]
Epoch [16/120    avg_loss:1.132, val_acc:0.740]
Epoch [17/120    avg_loss:1.061, val_acc:0.829]
Epoch [18/120    avg_loss:1.005, val_acc:0.771]
Epoch [19/120    avg_loss:0.945, val_acc:0.865]
Epoch [20/120    avg_loss:0.852, val_acc:0.883]
Epoch [21/120    avg_loss:0.813, val_acc:0.867]
Epoch [22/120    avg_loss:0.797, val_acc:0.860]
Epoch [23/120    avg_loss:0.749, val_acc:0.925]
Epoch [24/120    avg_loss:0.687, val_acc:0.912]
Epoch [25/120    avg_loss:0.659, val_acc:0.904]
Epoch [26/120    avg_loss:0.590, val_acc:0.912]
Epoch [27/120    avg_loss:0.587, val_acc:0.867]
Epoch [28/120    avg_loss:0.552, val_acc:0.819]
Epoch [29/120    avg_loss:0.526, val_acc:0.929]
Epoch [30/120    avg_loss:0.503, val_acc:0.915]
Epoch [31/120    avg_loss:0.535, val_acc:0.921]
Epoch [32/120    avg_loss:0.481, val_acc:0.923]
Epoch [33/120    avg_loss:0.508, val_acc:0.915]
Epoch [34/120    avg_loss:0.475, val_acc:0.915]
Epoch [35/120    avg_loss:0.440, val_acc:0.921]
Epoch [36/120    avg_loss:0.448, val_acc:0.917]
Epoch [37/120    avg_loss:0.436, val_acc:0.912]
Epoch [38/120    avg_loss:0.428, val_acc:0.915]
Epoch [39/120    avg_loss:0.433, val_acc:0.890]
Epoch [40/120    avg_loss:0.463, val_acc:0.894]
Epoch [41/120    avg_loss:0.395, val_acc:0.927]
Epoch [42/120    avg_loss:0.397, val_acc:0.935]
Epoch [43/120    avg_loss:0.366, val_acc:0.948]
Epoch [44/120    avg_loss:0.307, val_acc:0.940]
Epoch [45/120    avg_loss:0.323, val_acc:0.946]
Epoch [46/120    avg_loss:0.286, val_acc:0.952]
Epoch [47/120    avg_loss:0.300, val_acc:0.940]
Epoch [48/120    avg_loss:0.308, val_acc:0.933]
Epoch [49/120    avg_loss:0.295, val_acc:0.958]
Epoch [50/120    avg_loss:0.334, val_acc:0.954]
Epoch [51/120    avg_loss:0.306, val_acc:0.931]
Epoch [52/120    avg_loss:0.274, val_acc:0.938]
Epoch [53/120    avg_loss:0.272, val_acc:0.946]
Epoch [54/120    avg_loss:0.289, val_acc:0.952]
Epoch [55/120    avg_loss:0.256, val_acc:0.958]
Epoch [56/120    avg_loss:0.287, val_acc:0.954]
Epoch [57/120    avg_loss:0.321, val_acc:0.935]
Epoch [58/120    avg_loss:0.271, val_acc:0.944]
Epoch [59/120    avg_loss:0.235, val_acc:0.963]
Epoch [60/120    avg_loss:0.215, val_acc:0.960]
Epoch [61/120    avg_loss:0.216, val_acc:0.946]
Epoch [62/120    avg_loss:0.219, val_acc:0.952]
Epoch [63/120    avg_loss:0.224, val_acc:0.946]
Epoch [64/120    avg_loss:0.209, val_acc:0.960]
Epoch [65/120    avg_loss:0.240, val_acc:0.969]
Epoch [66/120    avg_loss:0.175, val_acc:0.956]
Epoch [67/120    avg_loss:0.193, val_acc:0.948]
Epoch [68/120    avg_loss:0.201, val_acc:0.965]
Epoch [69/120    avg_loss:0.176, val_acc:0.965]
Epoch [70/120    avg_loss:0.163, val_acc:0.954]
Epoch [71/120    avg_loss:0.180, val_acc:0.958]
Epoch [72/120    avg_loss:0.183, val_acc:0.958]
Epoch [73/120    avg_loss:0.162, val_acc:0.969]
Epoch [74/120    avg_loss:0.173, val_acc:0.952]
Epoch [75/120    avg_loss:0.157, val_acc:0.931]
Epoch [76/120    avg_loss:0.237, val_acc:0.942]
Epoch [77/120    avg_loss:0.181, val_acc:0.954]
Epoch [78/120    avg_loss:0.204, val_acc:0.954]
Epoch [79/120    avg_loss:0.174, val_acc:0.965]
Epoch [80/120    avg_loss:0.143, val_acc:0.958]
Epoch [81/120    avg_loss:0.174, val_acc:0.960]
Epoch [82/120    avg_loss:0.148, val_acc:0.950]
Epoch [83/120    avg_loss:0.131, val_acc:0.973]
Epoch [84/120    avg_loss:0.165, val_acc:0.956]
Epoch [85/120    avg_loss:0.120, val_acc:0.975]
Epoch [86/120    avg_loss:0.117, val_acc:0.977]
Epoch [87/120    avg_loss:0.108, val_acc:0.956]
Epoch [88/120    avg_loss:0.149, val_acc:0.950]
Epoch [89/120    avg_loss:0.185, val_acc:0.942]
Epoch [90/120    avg_loss:0.180, val_acc:0.967]
Epoch [91/120    avg_loss:0.182, val_acc:0.963]
Epoch [92/120    avg_loss:0.161, val_acc:0.981]
Epoch [93/120    avg_loss:0.120, val_acc:0.960]
Epoch [94/120    avg_loss:0.104, val_acc:0.975]
Epoch [95/120    avg_loss:0.115, val_acc:0.971]
Epoch [96/120    avg_loss:0.105, val_acc:0.981]
Epoch [97/120    avg_loss:0.107, val_acc:0.975]
Epoch [98/120    avg_loss:0.127, val_acc:0.971]
Epoch [99/120    avg_loss:0.097, val_acc:0.979]
Epoch [100/120    avg_loss:0.100, val_acc:0.983]
Epoch [101/120    avg_loss:0.082, val_acc:0.965]
Epoch [102/120    avg_loss:0.091, val_acc:0.973]
Epoch [103/120    avg_loss:0.072, val_acc:0.985]
Epoch [104/120    avg_loss:0.068, val_acc:0.985]
Epoch [105/120    avg_loss:0.078, val_acc:0.985]
Epoch [106/120    avg_loss:0.071, val_acc:0.985]
Epoch [107/120    avg_loss:0.074, val_acc:0.973]
Epoch [108/120    avg_loss:0.080, val_acc:0.983]
Epoch [109/120    avg_loss:0.070, val_acc:0.990]
Epoch [110/120    avg_loss:0.056, val_acc:0.985]
Epoch [111/120    avg_loss:0.064, val_acc:0.979]
Epoch [112/120    avg_loss:0.075, val_acc:0.985]
Epoch [113/120    avg_loss:0.096, val_acc:0.929]
Epoch [114/120    avg_loss:0.129, val_acc:0.963]
Epoch [115/120    avg_loss:0.097, val_acc:0.973]
Epoch [116/120    avg_loss:0.084, val_acc:0.973]
Epoch [117/120    avg_loss:0.087, val_acc:0.973]
Epoch [118/120    avg_loss:0.062, val_acc:0.983]
Epoch [119/120    avg_loss:0.042, val_acc:0.981]
Epoch [120/120    avg_loss:0.054, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 192   0   0   0   0  27   0   0   0   0   0   0]
 [  0   0   0 228   1   0   0   0   1   0   0   0   0   0]
 [  0   0   0  41 180   4   1   0   1   0   0   0   0   0]
 [  0   0   0   7   9 124   5   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.6545842217484

F1 scores:
[       nan 0.99853801 0.90995261 0.90118577 0.86330935 0.90842491
 0.98095238 0.83653846 0.99742931 1.         1.         1.
 0.99556541 1.        ]

Kappa:
0.973891562992032
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2d41fc07f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.600, val_acc:0.083]
Epoch [2/120    avg_loss:2.427, val_acc:0.265]
Epoch [3/120    avg_loss:2.289, val_acc:0.388]
Epoch [4/120    avg_loss:2.173, val_acc:0.471]
Epoch [5/120    avg_loss:2.069, val_acc:0.521]
Epoch [6/120    avg_loss:1.965, val_acc:0.533]
Epoch [7/120    avg_loss:1.846, val_acc:0.594]
Epoch [8/120    avg_loss:1.720, val_acc:0.610]
Epoch [9/120    avg_loss:1.617, val_acc:0.608]
Epoch [10/120    avg_loss:1.509, val_acc:0.710]
Epoch [11/120    avg_loss:1.384, val_acc:0.742]
Epoch [12/120    avg_loss:1.291, val_acc:0.762]
Epoch [13/120    avg_loss:1.220, val_acc:0.777]
Epoch [14/120    avg_loss:1.124, val_acc:0.731]
Epoch [15/120    avg_loss:1.083, val_acc:0.787]
Epoch [16/120    avg_loss:1.018, val_acc:0.829]
Epoch [17/120    avg_loss:0.971, val_acc:0.802]
Epoch [18/120    avg_loss:0.908, val_acc:0.835]
Epoch [19/120    avg_loss:0.861, val_acc:0.869]
Epoch [20/120    avg_loss:0.782, val_acc:0.848]
Epoch [21/120    avg_loss:0.738, val_acc:0.875]
Epoch [22/120    avg_loss:0.701, val_acc:0.798]
Epoch [23/120    avg_loss:0.711, val_acc:0.890]
Epoch [24/120    avg_loss:0.696, val_acc:0.890]
Epoch [25/120    avg_loss:0.672, val_acc:0.910]
Epoch [26/120    avg_loss:0.606, val_acc:0.917]
Epoch [27/120    avg_loss:0.575, val_acc:0.902]
Epoch [28/120    avg_loss:0.549, val_acc:0.910]
Epoch [29/120    avg_loss:0.554, val_acc:0.933]
Epoch [30/120    avg_loss:0.588, val_acc:0.846]
Epoch [31/120    avg_loss:0.568, val_acc:0.921]
Epoch [32/120    avg_loss:0.540, val_acc:0.929]
Epoch [33/120    avg_loss:0.451, val_acc:0.923]
Epoch [34/120    avg_loss:0.449, val_acc:0.890]
Epoch [35/120    avg_loss:0.444, val_acc:0.915]
Epoch [36/120    avg_loss:0.430, val_acc:0.944]
Epoch [37/120    avg_loss:0.392, val_acc:0.942]
Epoch [38/120    avg_loss:0.389, val_acc:0.925]
Epoch [39/120    avg_loss:0.411, val_acc:0.933]
Epoch [40/120    avg_loss:0.418, val_acc:0.921]
Epoch [41/120    avg_loss:0.372, val_acc:0.940]
Epoch [42/120    avg_loss:0.376, val_acc:0.948]
Epoch [43/120    avg_loss:0.377, val_acc:0.931]
Epoch [44/120    avg_loss:0.356, val_acc:0.940]
Epoch [45/120    avg_loss:0.406, val_acc:0.921]
Epoch [46/120    avg_loss:0.398, val_acc:0.877]
Epoch [47/120    avg_loss:0.342, val_acc:0.925]
Epoch [48/120    avg_loss:0.324, val_acc:0.938]
Epoch [49/120    avg_loss:0.293, val_acc:0.965]
Epoch [50/120    avg_loss:0.291, val_acc:0.942]
Epoch [51/120    avg_loss:0.315, val_acc:0.942]
Epoch [52/120    avg_loss:0.315, val_acc:0.952]
Epoch [53/120    avg_loss:0.343, val_acc:0.963]
Epoch [54/120    avg_loss:0.273, val_acc:0.960]
Epoch [55/120    avg_loss:0.270, val_acc:0.965]
Epoch [56/120    avg_loss:0.248, val_acc:0.965]
Epoch [57/120    avg_loss:0.277, val_acc:0.931]
Epoch [58/120    avg_loss:0.323, val_acc:0.935]
Epoch [59/120    avg_loss:0.287, val_acc:0.958]
Epoch [60/120    avg_loss:0.262, val_acc:0.965]
Epoch [61/120    avg_loss:0.246, val_acc:0.958]
Epoch [62/120    avg_loss:0.231, val_acc:0.960]
Epoch [63/120    avg_loss:0.235, val_acc:0.956]
Epoch [64/120    avg_loss:0.232, val_acc:0.948]
Epoch [65/120    avg_loss:0.246, val_acc:0.973]
Epoch [66/120    avg_loss:0.199, val_acc:0.950]
Epoch [67/120    avg_loss:0.182, val_acc:0.975]
Epoch [68/120    avg_loss:0.211, val_acc:0.967]
Epoch [69/120    avg_loss:0.229, val_acc:0.942]
Epoch [70/120    avg_loss:0.187, val_acc:0.979]
Epoch [71/120    avg_loss:0.167, val_acc:0.973]
Epoch [72/120    avg_loss:0.165, val_acc:0.981]
Epoch [73/120    avg_loss:0.160, val_acc:0.975]
Epoch [74/120    avg_loss:0.151, val_acc:0.981]
Epoch [75/120    avg_loss:0.143, val_acc:0.956]
Epoch [76/120    avg_loss:0.233, val_acc:0.950]
Epoch [77/120    avg_loss:0.200, val_acc:0.971]
Epoch [78/120    avg_loss:0.162, val_acc:0.948]
Epoch [79/120    avg_loss:0.140, val_acc:0.977]
Epoch [80/120    avg_loss:0.154, val_acc:0.973]
Epoch [81/120    avg_loss:0.187, val_acc:0.965]
Epoch [82/120    avg_loss:0.199, val_acc:0.969]
Epoch [83/120    avg_loss:0.257, val_acc:0.946]
Epoch [84/120    avg_loss:0.208, val_acc:0.960]
Epoch [85/120    avg_loss:0.192, val_acc:0.971]
Epoch [86/120    avg_loss:0.189, val_acc:0.871]
Epoch [87/120    avg_loss:0.230, val_acc:0.963]
Epoch [88/120    avg_loss:0.136, val_acc:0.977]
Epoch [89/120    avg_loss:0.096, val_acc:0.975]
Epoch [90/120    avg_loss:0.101, val_acc:0.977]
Epoch [91/120    avg_loss:0.098, val_acc:0.977]
Epoch [92/120    avg_loss:0.079, val_acc:0.977]
Epoch [93/120    avg_loss:0.081, val_acc:0.985]
Epoch [94/120    avg_loss:0.088, val_acc:0.985]
Epoch [95/120    avg_loss:0.090, val_acc:0.983]
Epoch [96/120    avg_loss:0.084, val_acc:0.983]
Epoch [97/120    avg_loss:0.085, val_acc:0.981]
Epoch [98/120    avg_loss:0.062, val_acc:0.981]
Epoch [99/120    avg_loss:0.087, val_acc:0.985]
Epoch [100/120    avg_loss:0.078, val_acc:0.983]
Epoch [101/120    avg_loss:0.075, val_acc:0.988]
Epoch [102/120    avg_loss:0.071, val_acc:0.983]
Epoch [103/120    avg_loss:0.089, val_acc:0.981]
Epoch [104/120    avg_loss:0.084, val_acc:0.981]
Epoch [105/120    avg_loss:0.084, val_acc:0.983]
Epoch [106/120    avg_loss:0.083, val_acc:0.983]
Epoch [107/120    avg_loss:0.088, val_acc:0.983]
Epoch [108/120    avg_loss:0.066, val_acc:0.985]
Epoch [109/120    avg_loss:0.069, val_acc:0.985]
Epoch [110/120    avg_loss:0.070, val_acc:0.988]
Epoch [111/120    avg_loss:0.075, val_acc:0.983]
Epoch [112/120    avg_loss:0.077, val_acc:0.985]
Epoch [113/120    avg_loss:0.065, val_acc:0.983]
Epoch [114/120    avg_loss:0.064, val_acc:0.985]
Epoch [115/120    avg_loss:0.065, val_acc:0.988]
Epoch [116/120    avg_loss:0.069, val_acc:0.985]
Epoch [117/120    avg_loss:0.061, val_acc:0.985]
Epoch [118/120    avg_loss:0.072, val_acc:0.983]
Epoch [119/120    avg_loss:0.065, val_acc:0.983]
Epoch [120/120    avg_loss:0.067, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   1 218  10   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 201  26   0   0   0   0   0   0   0   0]
 [  0   0   0   4   7 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  21   0   0   0   0  73   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.42217484008529

F1 scores:
[       nan 1.         0.9452954  0.96460177 0.90337079 0.87868852
 1.         0.85882353 1.         0.99893276 1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9824326119885554
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f45f1109908>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.624, val_acc:0.367]
Epoch [2/120    avg_loss:2.401, val_acc:0.385]
Epoch [3/120    avg_loss:2.255, val_acc:0.433]
Epoch [4/120    avg_loss:2.120, val_acc:0.479]
Epoch [5/120    avg_loss:2.036, val_acc:0.573]
Epoch [6/120    avg_loss:1.943, val_acc:0.590]
Epoch [7/120    avg_loss:1.842, val_acc:0.619]
Epoch [8/120    avg_loss:1.733, val_acc:0.613]
Epoch [9/120    avg_loss:1.651, val_acc:0.625]
Epoch [10/120    avg_loss:1.560, val_acc:0.615]
Epoch [11/120    avg_loss:1.455, val_acc:0.652]
Epoch [12/120    avg_loss:1.363, val_acc:0.665]
Epoch [13/120    avg_loss:1.295, val_acc:0.690]
Epoch [14/120    avg_loss:1.224, val_acc:0.762]
Epoch [15/120    avg_loss:1.131, val_acc:0.783]
Epoch [16/120    avg_loss:1.103, val_acc:0.871]
Epoch [17/120    avg_loss:1.019, val_acc:0.825]
Epoch [18/120    avg_loss:0.936, val_acc:0.798]
Epoch [19/120    avg_loss:0.885, val_acc:0.892]
Epoch [20/120    avg_loss:0.825, val_acc:0.865]
Epoch [21/120    avg_loss:0.795, val_acc:0.906]
Epoch [22/120    avg_loss:0.718, val_acc:0.910]
Epoch [23/120    avg_loss:0.690, val_acc:0.865]
Epoch [24/120    avg_loss:0.656, val_acc:0.912]
Epoch [25/120    avg_loss:0.624, val_acc:0.898]
Epoch [26/120    avg_loss:0.631, val_acc:0.919]
Epoch [27/120    avg_loss:0.591, val_acc:0.892]
Epoch [28/120    avg_loss:0.552, val_acc:0.923]
Epoch [29/120    avg_loss:0.522, val_acc:0.917]
Epoch [30/120    avg_loss:0.502, val_acc:0.927]
Epoch [31/120    avg_loss:0.469, val_acc:0.925]
Epoch [32/120    avg_loss:0.479, val_acc:0.908]
Epoch [33/120    avg_loss:0.496, val_acc:0.923]
Epoch [34/120    avg_loss:0.426, val_acc:0.927]
Epoch [35/120    avg_loss:0.398, val_acc:0.921]
Epoch [36/120    avg_loss:0.391, val_acc:0.919]
Epoch [37/120    avg_loss:0.359, val_acc:0.929]
Epoch [38/120    avg_loss:0.381, val_acc:0.923]
Epoch [39/120    avg_loss:0.364, val_acc:0.935]
Epoch [40/120    avg_loss:0.409, val_acc:0.950]
Epoch [41/120    avg_loss:0.457, val_acc:0.940]
Epoch [42/120    avg_loss:0.408, val_acc:0.921]
Epoch [43/120    avg_loss:0.349, val_acc:0.929]
Epoch [44/120    avg_loss:0.322, val_acc:0.948]
Epoch [45/120    avg_loss:0.323, val_acc:0.933]
Epoch [46/120    avg_loss:0.299, val_acc:0.950]
Epoch [47/120    avg_loss:0.265, val_acc:0.929]
Epoch [48/120    avg_loss:0.320, val_acc:0.944]
Epoch [49/120    avg_loss:0.293, val_acc:0.948]
Epoch [50/120    avg_loss:0.253, val_acc:0.956]
Epoch [51/120    avg_loss:0.282, val_acc:0.944]
Epoch [52/120    avg_loss:0.318, val_acc:0.950]
Epoch [53/120    avg_loss:0.263, val_acc:0.948]
Epoch [54/120    avg_loss:0.246, val_acc:0.952]
Epoch [55/120    avg_loss:0.232, val_acc:0.956]
Epoch [56/120    avg_loss:0.245, val_acc:0.954]
Epoch [57/120    avg_loss:0.225, val_acc:0.956]
Epoch [58/120    avg_loss:0.208, val_acc:0.956]
Epoch [59/120    avg_loss:0.249, val_acc:0.933]
Epoch [60/120    avg_loss:0.230, val_acc:0.940]
Epoch [61/120    avg_loss:0.226, val_acc:0.967]
Epoch [62/120    avg_loss:0.195, val_acc:0.963]
Epoch [63/120    avg_loss:0.224, val_acc:0.963]
Epoch [64/120    avg_loss:0.202, val_acc:0.967]
Epoch [65/120    avg_loss:0.205, val_acc:0.965]
Epoch [66/120    avg_loss:0.201, val_acc:0.935]
Epoch [67/120    avg_loss:0.306, val_acc:0.933]
Epoch [68/120    avg_loss:0.263, val_acc:0.954]
Epoch [69/120    avg_loss:0.196, val_acc:0.946]
Epoch [70/120    avg_loss:0.251, val_acc:0.956]
Epoch [71/120    avg_loss:0.227, val_acc:0.946]
Epoch [72/120    avg_loss:0.233, val_acc:0.954]
Epoch [73/120    avg_loss:0.233, val_acc:0.944]
Epoch [74/120    avg_loss:0.250, val_acc:0.946]
Epoch [75/120    avg_loss:0.191, val_acc:0.971]
Epoch [76/120    avg_loss:0.159, val_acc:0.963]
Epoch [77/120    avg_loss:0.141, val_acc:0.950]
Epoch [78/120    avg_loss:0.222, val_acc:0.944]
Epoch [79/120    avg_loss:0.158, val_acc:0.942]
Epoch [80/120    avg_loss:0.204, val_acc:0.904]
Epoch [81/120    avg_loss:0.413, val_acc:0.912]
Epoch [82/120    avg_loss:0.306, val_acc:0.935]
Epoch [83/120    avg_loss:0.206, val_acc:0.954]
Epoch [84/120    avg_loss:0.173, val_acc:0.963]
Epoch [85/120    avg_loss:0.176, val_acc:0.971]
Epoch [86/120    avg_loss:0.112, val_acc:0.967]
Epoch [87/120    avg_loss:0.132, val_acc:0.952]
Epoch [88/120    avg_loss:0.138, val_acc:0.948]
Epoch [89/120    avg_loss:0.179, val_acc:0.938]
Epoch [90/120    avg_loss:0.145, val_acc:0.944]
Epoch [91/120    avg_loss:0.137, val_acc:0.969]
Epoch [92/120    avg_loss:0.121, val_acc:0.960]
Epoch [93/120    avg_loss:0.138, val_acc:0.925]
Epoch [94/120    avg_loss:0.131, val_acc:0.971]
Epoch [95/120    avg_loss:0.149, val_acc:0.973]
Epoch [96/120    avg_loss:0.143, val_acc:0.965]
Epoch [97/120    avg_loss:0.176, val_acc:0.950]
Epoch [98/120    avg_loss:0.214, val_acc:0.948]
Epoch [99/120    avg_loss:0.153, val_acc:0.956]
Epoch [100/120    avg_loss:0.139, val_acc:0.958]
Epoch [101/120    avg_loss:0.120, val_acc:0.960]
Epoch [102/120    avg_loss:0.117, val_acc:0.967]
Epoch [103/120    avg_loss:0.107, val_acc:0.981]
Epoch [104/120    avg_loss:0.080, val_acc:0.960]
Epoch [105/120    avg_loss:0.107, val_acc:0.954]
Epoch [106/120    avg_loss:0.136, val_acc:0.960]
Epoch [107/120    avg_loss:0.126, val_acc:0.963]
Epoch [108/120    avg_loss:0.105, val_acc:0.963]
Epoch [109/120    avg_loss:0.111, val_acc:0.954]
Epoch [110/120    avg_loss:0.145, val_acc:0.940]
Epoch [111/120    avg_loss:0.154, val_acc:0.954]
Epoch [112/120    avg_loss:0.146, val_acc:0.958]
Epoch [113/120    avg_loss:0.116, val_acc:0.973]
Epoch [114/120    avg_loss:0.102, val_acc:0.971]
Epoch [115/120    avg_loss:0.107, val_acc:0.956]
Epoch [116/120    avg_loss:0.106, val_acc:0.967]
Epoch [117/120    avg_loss:0.078, val_acc:0.969]
Epoch [118/120    avg_loss:0.066, val_acc:0.973]
Epoch [119/120    avg_loss:0.067, val_acc:0.973]
Epoch [120/120    avg_loss:0.065, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   0   8   0   0   0   0   0   0]
 [  0   0   0 216   9   0   0   0   5   0   0   0   0   0]
 [  0   0   0   0 207  20   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   2   0   0   0   0 204   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 372   5   0]
 [  0   0   0   0   0   0   0   0   1   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.12366737739872

F1 scores:
[       nan 0.99854227 0.94407159 0.96860987 0.89224138 0.85813149
 0.99512195 0.8603352  0.99232737 1.         1.         0.99332443
 0.99340659 1.        ]

Kappa:
0.979107002550928
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f118aecf7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.577, val_acc:0.165]
Epoch [2/120    avg_loss:2.413, val_acc:0.371]
Epoch [3/120    avg_loss:2.293, val_acc:0.433]
Epoch [4/120    avg_loss:2.180, val_acc:0.444]
Epoch [5/120    avg_loss:2.093, val_acc:0.475]
Epoch [6/120    avg_loss:1.995, val_acc:0.506]
Epoch [7/120    avg_loss:1.899, val_acc:0.554]
Epoch [8/120    avg_loss:1.788, val_acc:0.619]
Epoch [9/120    avg_loss:1.670, val_acc:0.717]
Epoch [10/120    avg_loss:1.571, val_acc:0.752]
Epoch [11/120    avg_loss:1.456, val_acc:0.769]
Epoch [12/120    avg_loss:1.380, val_acc:0.758]
Epoch [13/120    avg_loss:1.285, val_acc:0.785]
Epoch [14/120    avg_loss:1.180, val_acc:0.781]
Epoch [15/120    avg_loss:1.125, val_acc:0.783]
Epoch [16/120    avg_loss:1.026, val_acc:0.802]
Epoch [17/120    avg_loss:0.946, val_acc:0.817]
Epoch [18/120    avg_loss:0.904, val_acc:0.833]
Epoch [19/120    avg_loss:0.884, val_acc:0.835]
Epoch [20/120    avg_loss:0.801, val_acc:0.850]
Epoch [21/120    avg_loss:0.752, val_acc:0.885]
Epoch [22/120    avg_loss:0.732, val_acc:0.871]
Epoch [23/120    avg_loss:0.691, val_acc:0.890]
Epoch [24/120    avg_loss:0.646, val_acc:0.827]
Epoch [25/120    avg_loss:0.645, val_acc:0.887]
Epoch [26/120    avg_loss:0.559, val_acc:0.900]
Epoch [27/120    avg_loss:0.531, val_acc:0.900]
Epoch [28/120    avg_loss:0.501, val_acc:0.908]
Epoch [29/120    avg_loss:0.504, val_acc:0.904]
Epoch [30/120    avg_loss:0.531, val_acc:0.846]
Epoch [31/120    avg_loss:0.515, val_acc:0.894]
Epoch [32/120    avg_loss:0.426, val_acc:0.917]
Epoch [33/120    avg_loss:0.420, val_acc:0.917]
Epoch [34/120    avg_loss:0.411, val_acc:0.921]
Epoch [35/120    avg_loss:0.356, val_acc:0.938]
Epoch [36/120    avg_loss:0.373, val_acc:0.925]
Epoch [37/120    avg_loss:0.391, val_acc:0.931]
Epoch [38/120    avg_loss:0.354, val_acc:0.940]
Epoch [39/120    avg_loss:0.365, val_acc:0.925]
Epoch [40/120    avg_loss:0.368, val_acc:0.929]
Epoch [41/120    avg_loss:0.411, val_acc:0.919]
Epoch [42/120    avg_loss:0.340, val_acc:0.923]
Epoch [43/120    avg_loss:0.357, val_acc:0.942]
Epoch [44/120    avg_loss:0.320, val_acc:0.915]
Epoch [45/120    avg_loss:0.368, val_acc:0.898]
Epoch [46/120    avg_loss:0.393, val_acc:0.917]
Epoch [47/120    avg_loss:0.342, val_acc:0.910]
Epoch [48/120    avg_loss:0.318, val_acc:0.950]
Epoch [49/120    avg_loss:0.259, val_acc:0.950]
Epoch [50/120    avg_loss:0.268, val_acc:0.948]
Epoch [51/120    avg_loss:0.281, val_acc:0.942]
Epoch [52/120    avg_loss:0.247, val_acc:0.958]
Epoch [53/120    avg_loss:0.227, val_acc:0.956]
Epoch [54/120    avg_loss:0.247, val_acc:0.912]
Epoch [55/120    avg_loss:0.230, val_acc:0.956]
Epoch [56/120    avg_loss:0.196, val_acc:0.958]
Epoch [57/120    avg_loss:0.201, val_acc:0.971]
Epoch [58/120    avg_loss:0.242, val_acc:0.946]
Epoch [59/120    avg_loss:0.215, val_acc:0.958]
Epoch [60/120    avg_loss:0.204, val_acc:0.965]
Epoch [61/120    avg_loss:0.184, val_acc:0.942]
Epoch [62/120    avg_loss:0.180, val_acc:0.969]
Epoch [63/120    avg_loss:0.180, val_acc:0.958]
Epoch [64/120    avg_loss:0.171, val_acc:0.973]
Epoch [65/120    avg_loss:0.182, val_acc:0.963]
Epoch [66/120    avg_loss:0.194, val_acc:0.944]
Epoch [67/120    avg_loss:0.172, val_acc:0.963]
Epoch [68/120    avg_loss:0.185, val_acc:0.960]
Epoch [69/120    avg_loss:0.137, val_acc:0.963]
Epoch [70/120    avg_loss:0.161, val_acc:0.965]
Epoch [71/120    avg_loss:0.179, val_acc:0.960]
Epoch [72/120    avg_loss:0.188, val_acc:0.956]
Epoch [73/120    avg_loss:0.209, val_acc:0.958]
Epoch [74/120    avg_loss:0.196, val_acc:0.969]
Epoch [75/120    avg_loss:0.149, val_acc:0.973]
Epoch [76/120    avg_loss:0.169, val_acc:0.969]
Epoch [77/120    avg_loss:0.141, val_acc:0.969]
Epoch [78/120    avg_loss:0.148, val_acc:0.975]
Epoch [79/120    avg_loss:0.130, val_acc:0.973]
Epoch [80/120    avg_loss:0.173, val_acc:0.960]
Epoch [81/120    avg_loss:0.174, val_acc:0.950]
Epoch [82/120    avg_loss:0.175, val_acc:0.954]
Epoch [83/120    avg_loss:0.183, val_acc:0.969]
Epoch [84/120    avg_loss:0.144, val_acc:0.954]
Epoch [85/120    avg_loss:0.147, val_acc:0.967]
Epoch [86/120    avg_loss:0.105, val_acc:0.973]
Epoch [87/120    avg_loss:0.105, val_acc:0.967]
Epoch [88/120    avg_loss:0.110, val_acc:0.958]
Epoch [89/120    avg_loss:0.119, val_acc:0.967]
Epoch [90/120    avg_loss:0.099, val_acc:0.969]
Epoch [91/120    avg_loss:0.083, val_acc:0.977]
Epoch [92/120    avg_loss:0.097, val_acc:0.977]
Epoch [93/120    avg_loss:0.080, val_acc:0.963]
Epoch [94/120    avg_loss:0.099, val_acc:0.960]
Epoch [95/120    avg_loss:0.101, val_acc:0.977]
Epoch [96/120    avg_loss:0.113, val_acc:0.969]
Epoch [97/120    avg_loss:0.125, val_acc:0.965]
Epoch [98/120    avg_loss:0.188, val_acc:0.960]
Epoch [99/120    avg_loss:0.172, val_acc:0.958]
Epoch [100/120    avg_loss:0.140, val_acc:0.954]
Epoch [101/120    avg_loss:0.173, val_acc:0.975]
Epoch [102/120    avg_loss:0.116, val_acc:0.960]
Epoch [103/120    avg_loss:0.113, val_acc:0.975]
Epoch [104/120    avg_loss:0.086, val_acc:0.977]
Epoch [105/120    avg_loss:0.095, val_acc:0.971]
Epoch [106/120    avg_loss:0.076, val_acc:0.977]
Epoch [107/120    avg_loss:0.097, val_acc:0.973]
Epoch [108/120    avg_loss:0.073, val_acc:0.983]
Epoch [109/120    avg_loss:0.099, val_acc:0.973]
Epoch [110/120    avg_loss:0.184, val_acc:0.969]
Epoch [111/120    avg_loss:0.131, val_acc:0.963]
Epoch [112/120    avg_loss:0.116, val_acc:0.975]
Epoch [113/120    avg_loss:0.083, val_acc:0.973]
Epoch [114/120    avg_loss:0.092, val_acc:0.981]
Epoch [115/120    avg_loss:0.074, val_acc:0.988]
Epoch [116/120    avg_loss:0.078, val_acc:0.983]
Epoch [117/120    avg_loss:0.068, val_acc:0.977]
Epoch [118/120    avg_loss:0.055, val_acc:0.977]
Epoch [119/120    avg_loss:0.085, val_acc:0.969]
Epoch [120/120    avg_loss:0.086, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   0   8   0   0   0   0   0   0]
 [  0   0   3 220   0   0   0   0   3   4   0   0   0   0]
 [  0   0   0  23 198   3   0   0   3   0   0   0   0   0]
 [  0   0   0  20   0 125   0   0   0   0   0   0   0   0]
 [  0  11   0   0   0   0 195   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.99573560767591

F1 scores:
[       nan 0.99203476 0.94407159 0.89249493 0.93176471 0.91575092
 0.97256858 0.87912088 0.99232737 0.99574468 1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.9776753296339921
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f49797b4898>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.638, val_acc:0.125]
Epoch [2/120    avg_loss:2.458, val_acc:0.417]
Epoch [3/120    avg_loss:2.324, val_acc:0.446]
Epoch [4/120    avg_loss:2.218, val_acc:0.492]
Epoch [5/120    avg_loss:2.106, val_acc:0.554]
Epoch [6/120    avg_loss:1.996, val_acc:0.600]
Epoch [7/120    avg_loss:1.881, val_acc:0.588]
Epoch [8/120    avg_loss:1.767, val_acc:0.619]
Epoch [9/120    avg_loss:1.641, val_acc:0.633]
Epoch [10/120    avg_loss:1.517, val_acc:0.642]
Epoch [11/120    avg_loss:1.419, val_acc:0.646]
Epoch [12/120    avg_loss:1.332, val_acc:0.673]
Epoch [13/120    avg_loss:1.232, val_acc:0.694]
Epoch [14/120    avg_loss:1.163, val_acc:0.717]
Epoch [15/120    avg_loss:1.111, val_acc:0.706]
Epoch [16/120    avg_loss:1.032, val_acc:0.727]
Epoch [17/120    avg_loss:0.971, val_acc:0.785]
Epoch [18/120    avg_loss:0.889, val_acc:0.833]
Epoch [19/120    avg_loss:0.843, val_acc:0.846]
Epoch [20/120    avg_loss:0.788, val_acc:0.829]
Epoch [21/120    avg_loss:0.744, val_acc:0.846]
Epoch [22/120    avg_loss:0.701, val_acc:0.869]
Epoch [23/120    avg_loss:0.656, val_acc:0.821]
Epoch [24/120    avg_loss:0.703, val_acc:0.863]
Epoch [25/120    avg_loss:0.595, val_acc:0.863]
Epoch [26/120    avg_loss:0.546, val_acc:0.869]
Epoch [27/120    avg_loss:0.546, val_acc:0.860]
Epoch [28/120    avg_loss:0.540, val_acc:0.787]
Epoch [29/120    avg_loss:0.538, val_acc:0.850]
Epoch [30/120    avg_loss:0.484, val_acc:0.873]
Epoch [31/120    avg_loss:0.504, val_acc:0.890]
Epoch [32/120    avg_loss:0.511, val_acc:0.873]
Epoch [33/120    avg_loss:0.441, val_acc:0.896]
Epoch [34/120    avg_loss:0.424, val_acc:0.894]
Epoch [35/120    avg_loss:0.447, val_acc:0.904]
Epoch [36/120    avg_loss:0.436, val_acc:0.898]
Epoch [37/120    avg_loss:0.419, val_acc:0.894]
Epoch [38/120    avg_loss:0.412, val_acc:0.890]
Epoch [39/120    avg_loss:0.402, val_acc:0.879]
Epoch [40/120    avg_loss:0.445, val_acc:0.825]
Epoch [41/120    avg_loss:0.440, val_acc:0.908]
Epoch [42/120    avg_loss:0.398, val_acc:0.890]
Epoch [43/120    avg_loss:0.390, val_acc:0.915]
Epoch [44/120    avg_loss:0.393, val_acc:0.908]
Epoch [45/120    avg_loss:0.331, val_acc:0.929]
Epoch [46/120    avg_loss:0.310, val_acc:0.927]
Epoch [47/120    avg_loss:0.294, val_acc:0.912]
Epoch [48/120    avg_loss:0.322, val_acc:0.927]
Epoch [49/120    avg_loss:0.303, val_acc:0.908]
Epoch [50/120    avg_loss:0.283, val_acc:0.938]
Epoch [51/120    avg_loss:0.290, val_acc:0.952]
Epoch [52/120    avg_loss:0.274, val_acc:0.935]
Epoch [53/120    avg_loss:0.279, val_acc:0.869]
Epoch [54/120    avg_loss:0.270, val_acc:0.938]
Epoch [55/120    avg_loss:0.302, val_acc:0.921]
Epoch [56/120    avg_loss:0.293, val_acc:0.923]
Epoch [57/120    avg_loss:0.253, val_acc:0.940]
Epoch [58/120    avg_loss:0.207, val_acc:0.958]
Epoch [59/120    avg_loss:0.215, val_acc:0.940]
Epoch [60/120    avg_loss:0.231, val_acc:0.931]
Epoch [61/120    avg_loss:0.254, val_acc:0.954]
Epoch [62/120    avg_loss:0.216, val_acc:0.950]
Epoch [63/120    avg_loss:0.307, val_acc:0.906]
Epoch [64/120    avg_loss:0.235, val_acc:0.931]
Epoch [65/120    avg_loss:0.234, val_acc:0.946]
Epoch [66/120    avg_loss:0.194, val_acc:0.954]
Epoch [67/120    avg_loss:0.152, val_acc:0.969]
Epoch [68/120    avg_loss:0.180, val_acc:0.933]
Epoch [69/120    avg_loss:0.187, val_acc:0.944]
Epoch [70/120    avg_loss:0.202, val_acc:0.944]
Epoch [71/120    avg_loss:0.208, val_acc:0.944]
Epoch [72/120    avg_loss:0.214, val_acc:0.967]
Epoch [73/120    avg_loss:0.225, val_acc:0.938]
Epoch [74/120    avg_loss:0.167, val_acc:0.902]
Epoch [75/120    avg_loss:0.171, val_acc:0.960]
Epoch [76/120    avg_loss:0.161, val_acc:0.958]
Epoch [77/120    avg_loss:0.125, val_acc:0.963]
Epoch [78/120    avg_loss:0.116, val_acc:0.963]
Epoch [79/120    avg_loss:0.115, val_acc:0.973]
Epoch [80/120    avg_loss:0.110, val_acc:0.975]
Epoch [81/120    avg_loss:0.150, val_acc:0.958]
Epoch [82/120    avg_loss:0.161, val_acc:0.956]
Epoch [83/120    avg_loss:0.122, val_acc:0.952]
Epoch [84/120    avg_loss:0.120, val_acc:0.956]
Epoch [85/120    avg_loss:0.115, val_acc:0.967]
Epoch [86/120    avg_loss:0.141, val_acc:0.965]
Epoch [87/120    avg_loss:0.104, val_acc:0.958]
Epoch [88/120    avg_loss:0.115, val_acc:0.969]
Epoch [89/120    avg_loss:0.086, val_acc:0.975]
Epoch [90/120    avg_loss:0.070, val_acc:0.971]
Epoch [91/120    avg_loss:0.074, val_acc:0.967]
Epoch [92/120    avg_loss:0.071, val_acc:0.973]
Epoch [93/120    avg_loss:0.095, val_acc:0.960]
Epoch [94/120    avg_loss:0.052, val_acc:0.960]
Epoch [95/120    avg_loss:0.077, val_acc:0.977]
Epoch [96/120    avg_loss:0.085, val_acc:0.973]
Epoch [97/120    avg_loss:0.088, val_acc:0.971]
Epoch [98/120    avg_loss:0.073, val_acc:0.977]
Epoch [99/120    avg_loss:0.070, val_acc:0.975]
Epoch [100/120    avg_loss:0.057, val_acc:0.979]
Epoch [101/120    avg_loss:0.070, val_acc:0.969]
Epoch [102/120    avg_loss:0.065, val_acc:0.960]
Epoch [103/120    avg_loss:0.065, val_acc:0.985]
Epoch [104/120    avg_loss:0.074, val_acc:0.967]
Epoch [105/120    avg_loss:0.092, val_acc:0.977]
Epoch [106/120    avg_loss:0.052, val_acc:0.973]
Epoch [107/120    avg_loss:0.039, val_acc:0.983]
Epoch [108/120    avg_loss:0.084, val_acc:0.979]
Epoch [109/120    avg_loss:0.095, val_acc:0.977]
Epoch [110/120    avg_loss:0.078, val_acc:0.983]
Epoch [111/120    avg_loss:0.052, val_acc:0.975]
Epoch [112/120    avg_loss:0.100, val_acc:0.969]
Epoch [113/120    avg_loss:0.082, val_acc:0.967]
Epoch [114/120    avg_loss:0.075, val_acc:0.981]
Epoch [115/120    avg_loss:0.045, val_acc:0.973]
Epoch [116/120    avg_loss:0.049, val_acc:0.977]
Epoch [117/120    avg_loss:0.036, val_acc:0.981]
Epoch [118/120    avg_loss:0.033, val_acc:0.977]
Epoch [119/120    avg_loss:0.027, val_acc:0.979]
Epoch [120/120    avg_loss:0.034, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   1   1   0   0   0   0   0   0   0   0]
 [  0   0   0 217   7   0   0   0   3   3   0   0   0   0]
 [  0   0   1   0 209  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 363   0   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0 374   3   0]
 [  0   1   0   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.69936034115139

F1 scores:
[       nan 0.99927061 0.96659243 0.97091723 0.91868132 0.9023569
 1.         0.94382022 0.99356499 0.99680511 0.99862448 0.99600533
 0.99449945 1.        ]

Kappa:
0.9855182198399933
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f82b1a2c898>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.543, val_acc:0.302]
Epoch [2/120    avg_loss:2.382, val_acc:0.304]
Epoch [3/120    avg_loss:2.285, val_acc:0.356]
Epoch [4/120    avg_loss:2.199, val_acc:0.415]
Epoch [5/120    avg_loss:2.098, val_acc:0.533]
Epoch [6/120    avg_loss:2.021, val_acc:0.542]
Epoch [7/120    avg_loss:1.934, val_acc:0.565]
Epoch [8/120    avg_loss:1.831, val_acc:0.644]
Epoch [9/120    avg_loss:1.739, val_acc:0.631]
Epoch [10/120    avg_loss:1.635, val_acc:0.640]
Epoch [11/120    avg_loss:1.548, val_acc:0.644]
Epoch [12/120    avg_loss:1.479, val_acc:0.658]
Epoch [13/120    avg_loss:1.386, val_acc:0.719]
Epoch [14/120    avg_loss:1.299, val_acc:0.735]
Epoch [15/120    avg_loss:1.226, val_acc:0.762]
Epoch [16/120    avg_loss:1.139, val_acc:0.781]
Epoch [17/120    avg_loss:1.089, val_acc:0.802]
Epoch [18/120    avg_loss:1.004, val_acc:0.825]
Epoch [19/120    avg_loss:0.939, val_acc:0.838]
Epoch [20/120    avg_loss:0.917, val_acc:0.856]
Epoch [21/120    avg_loss:0.832, val_acc:0.892]
Epoch [22/120    avg_loss:0.750, val_acc:0.871]
Epoch [23/120    avg_loss:0.736, val_acc:0.894]
Epoch [24/120    avg_loss:0.689, val_acc:0.871]
Epoch [25/120    avg_loss:0.651, val_acc:0.887]
Epoch [26/120    avg_loss:0.600, val_acc:0.885]
Epoch [27/120    avg_loss:0.599, val_acc:0.896]
Epoch [28/120    avg_loss:0.563, val_acc:0.900]
Epoch [29/120    avg_loss:0.515, val_acc:0.906]
Epoch [30/120    avg_loss:0.540, val_acc:0.910]
Epoch [31/120    avg_loss:0.495, val_acc:0.887]
Epoch [32/120    avg_loss:0.469, val_acc:0.908]
Epoch [33/120    avg_loss:0.460, val_acc:0.885]
Epoch [34/120    avg_loss:0.446, val_acc:0.881]
Epoch [35/120    avg_loss:0.405, val_acc:0.894]
Epoch [36/120    avg_loss:0.446, val_acc:0.904]
Epoch [37/120    avg_loss:0.466, val_acc:0.921]
Epoch [38/120    avg_loss:0.390, val_acc:0.912]
Epoch [39/120    avg_loss:0.376, val_acc:0.873]
Epoch [40/120    avg_loss:0.396, val_acc:0.915]
Epoch [41/120    avg_loss:0.384, val_acc:0.908]
Epoch [42/120    avg_loss:0.345, val_acc:0.923]
Epoch [43/120    avg_loss:0.323, val_acc:0.929]
Epoch [44/120    avg_loss:0.356, val_acc:0.894]
Epoch [45/120    avg_loss:0.338, val_acc:0.917]
Epoch [46/120    avg_loss:0.345, val_acc:0.931]
Epoch [47/120    avg_loss:0.379, val_acc:0.912]
Epoch [48/120    avg_loss:0.362, val_acc:0.929]
Epoch [49/120    avg_loss:0.312, val_acc:0.894]
Epoch [50/120    avg_loss:0.348, val_acc:0.925]
Epoch [51/120    avg_loss:0.332, val_acc:0.927]
Epoch [52/120    avg_loss:0.300, val_acc:0.919]
Epoch [53/120    avg_loss:0.289, val_acc:0.925]
Epoch [54/120    avg_loss:0.283, val_acc:0.927]
Epoch [55/120    avg_loss:0.277, val_acc:0.927]
Epoch [56/120    avg_loss:0.267, val_acc:0.940]
Epoch [57/120    avg_loss:0.249, val_acc:0.933]
Epoch [58/120    avg_loss:0.258, val_acc:0.940]
Epoch [59/120    avg_loss:0.237, val_acc:0.960]
Epoch [60/120    avg_loss:0.211, val_acc:0.946]
Epoch [61/120    avg_loss:0.203, val_acc:0.940]
Epoch [62/120    avg_loss:0.241, val_acc:0.938]
Epoch [63/120    avg_loss:0.267, val_acc:0.921]
Epoch [64/120    avg_loss:0.224, val_acc:0.948]
Epoch [65/120    avg_loss:0.208, val_acc:0.921]
Epoch [66/120    avg_loss:0.221, val_acc:0.942]
Epoch [67/120    avg_loss:0.219, val_acc:0.944]
Epoch [68/120    avg_loss:0.207, val_acc:0.935]
Epoch [69/120    avg_loss:0.237, val_acc:0.938]
Epoch [70/120    avg_loss:0.167, val_acc:0.948]
Epoch [71/120    avg_loss:0.178, val_acc:0.925]
Epoch [72/120    avg_loss:0.165, val_acc:0.948]
Epoch [73/120    avg_loss:0.159, val_acc:0.952]
Epoch [74/120    avg_loss:0.146, val_acc:0.954]
Epoch [75/120    avg_loss:0.134, val_acc:0.950]
Epoch [76/120    avg_loss:0.129, val_acc:0.948]
Epoch [77/120    avg_loss:0.131, val_acc:0.952]
Epoch [78/120    avg_loss:0.146, val_acc:0.952]
Epoch [79/120    avg_loss:0.127, val_acc:0.954]
Epoch [80/120    avg_loss:0.153, val_acc:0.952]
Epoch [81/120    avg_loss:0.151, val_acc:0.956]
Epoch [82/120    avg_loss:0.143, val_acc:0.954]
Epoch [83/120    avg_loss:0.138, val_acc:0.958]
Epoch [84/120    avg_loss:0.140, val_acc:0.956]
Epoch [85/120    avg_loss:0.145, val_acc:0.956]
Epoch [86/120    avg_loss:0.126, val_acc:0.956]
Epoch [87/120    avg_loss:0.128, val_acc:0.956]
Epoch [88/120    avg_loss:0.132, val_acc:0.956]
Epoch [89/120    avg_loss:0.136, val_acc:0.956]
Epoch [90/120    avg_loss:0.137, val_acc:0.956]
Epoch [91/120    avg_loss:0.131, val_acc:0.956]
Epoch [92/120    avg_loss:0.137, val_acc:0.956]
Epoch [93/120    avg_loss:0.131, val_acc:0.956]
Epoch [94/120    avg_loss:0.128, val_acc:0.956]
Epoch [95/120    avg_loss:0.127, val_acc:0.956]
Epoch [96/120    avg_loss:0.117, val_acc:0.956]
Epoch [97/120    avg_loss:0.129, val_acc:0.956]
Epoch [98/120    avg_loss:0.129, val_acc:0.956]
Epoch [99/120    avg_loss:0.119, val_acc:0.956]
Epoch [100/120    avg_loss:0.131, val_acc:0.956]
Epoch [101/120    avg_loss:0.126, val_acc:0.956]
Epoch [102/120    avg_loss:0.120, val_acc:0.956]
Epoch [103/120    avg_loss:0.136, val_acc:0.956]
Epoch [104/120    avg_loss:0.127, val_acc:0.956]
Epoch [105/120    avg_loss:0.123, val_acc:0.956]
Epoch [106/120    avg_loss:0.142, val_acc:0.956]
Epoch [107/120    avg_loss:0.131, val_acc:0.956]
Epoch [108/120    avg_loss:0.138, val_acc:0.956]
Epoch [109/120    avg_loss:0.125, val_acc:0.956]
Epoch [110/120    avg_loss:0.138, val_acc:0.956]
Epoch [111/120    avg_loss:0.134, val_acc:0.956]
Epoch [112/120    avg_loss:0.121, val_acc:0.956]
Epoch [113/120    avg_loss:0.139, val_acc:0.956]
Epoch [114/120    avg_loss:0.133, val_acc:0.956]
Epoch [115/120    avg_loss:0.119, val_acc:0.956]
Epoch [116/120    avg_loss:0.152, val_acc:0.956]
Epoch [117/120    avg_loss:0.134, val_acc:0.956]
Epoch [118/120    avg_loss:0.110, val_acc:0.956]
Epoch [119/120    avg_loss:0.136, val_acc:0.956]
Epoch [120/120    avg_loss:0.122, val_acc:0.956]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   0 223   6   0   0   0   1   0   0   0   0   0]
 [  0   0   0   5 196  26   0   0   0   0   0   0   0   0]
 [  0   0   0   2  32 111   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8   0 198   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0   0 386   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   5   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.82515991471215

F1 scores:
[       nan 1.         0.96213808 0.96956522 0.8358209  0.78723404
 0.98019802 0.91620112 0.98974359 1.         1.         1.
 0.99445061 1.        ]

Kappa:
0.9757853181521583
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff2fe737898>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.607, val_acc:0.117]
Epoch [2/120    avg_loss:2.442, val_acc:0.292]
Epoch [3/120    avg_loss:2.308, val_acc:0.310]
Epoch [4/120    avg_loss:2.185, val_acc:0.385]
Epoch [5/120    avg_loss:2.084, val_acc:0.473]
Epoch [6/120    avg_loss:1.948, val_acc:0.627]
Epoch [7/120    avg_loss:1.840, val_acc:0.654]
Epoch [8/120    avg_loss:1.741, val_acc:0.654]
Epoch [9/120    avg_loss:1.621, val_acc:0.675]
Epoch [10/120    avg_loss:1.527, val_acc:0.702]
Epoch [11/120    avg_loss:1.437, val_acc:0.685]
Epoch [12/120    avg_loss:1.311, val_acc:0.671]
Epoch [13/120    avg_loss:1.244, val_acc:0.696]
Epoch [14/120    avg_loss:1.140, val_acc:0.769]
Epoch [15/120    avg_loss:1.094, val_acc:0.829]
Epoch [16/120    avg_loss:1.026, val_acc:0.817]
Epoch [17/120    avg_loss:0.956, val_acc:0.852]
Epoch [18/120    avg_loss:0.885, val_acc:0.869]
Epoch [19/120    avg_loss:0.838, val_acc:0.875]
Epoch [20/120    avg_loss:0.774, val_acc:0.898]
Epoch [21/120    avg_loss:0.717, val_acc:0.881]
Epoch [22/120    avg_loss:0.740, val_acc:0.887]
Epoch [23/120    avg_loss:0.651, val_acc:0.892]
Epoch [24/120    avg_loss:0.615, val_acc:0.892]
Epoch [25/120    avg_loss:0.579, val_acc:0.844]
Epoch [26/120    avg_loss:0.557, val_acc:0.881]
Epoch [27/120    avg_loss:0.509, val_acc:0.892]
Epoch [28/120    avg_loss:0.485, val_acc:0.898]
Epoch [29/120    avg_loss:0.464, val_acc:0.906]
Epoch [30/120    avg_loss:0.435, val_acc:0.890]
Epoch [31/120    avg_loss:0.442, val_acc:0.908]
Epoch [32/120    avg_loss:0.475, val_acc:0.908]
Epoch [33/120    avg_loss:0.444, val_acc:0.910]
Epoch [34/120    avg_loss:0.429, val_acc:0.904]
Epoch [35/120    avg_loss:0.393, val_acc:0.927]
Epoch [36/120    avg_loss:0.404, val_acc:0.912]
Epoch [37/120    avg_loss:0.384, val_acc:0.887]
Epoch [38/120    avg_loss:0.377, val_acc:0.927]
Epoch [39/120    avg_loss:0.337, val_acc:0.927]
Epoch [40/120    avg_loss:0.370, val_acc:0.908]
Epoch [41/120    avg_loss:0.347, val_acc:0.931]
Epoch [42/120    avg_loss:0.309, val_acc:0.935]
Epoch [43/120    avg_loss:0.340, val_acc:0.933]
Epoch [44/120    avg_loss:0.366, val_acc:0.915]
Epoch [45/120    avg_loss:0.323, val_acc:0.912]
Epoch [46/120    avg_loss:0.343, val_acc:0.927]
Epoch [47/120    avg_loss:0.294, val_acc:0.915]
Epoch [48/120    avg_loss:0.291, val_acc:0.912]
Epoch [49/120    avg_loss:0.272, val_acc:0.938]
Epoch [50/120    avg_loss:0.278, val_acc:0.929]
Epoch [51/120    avg_loss:0.280, val_acc:0.942]
Epoch [52/120    avg_loss:0.367, val_acc:0.910]
Epoch [53/120    avg_loss:0.331, val_acc:0.927]
Epoch [54/120    avg_loss:0.339, val_acc:0.952]
Epoch [55/120    avg_loss:0.246, val_acc:0.933]
Epoch [56/120    avg_loss:0.241, val_acc:0.948]
Epoch [57/120    avg_loss:0.246, val_acc:0.935]
Epoch [58/120    avg_loss:0.232, val_acc:0.929]
Epoch [59/120    avg_loss:0.325, val_acc:0.940]
Epoch [60/120    avg_loss:0.364, val_acc:0.950]
Epoch [61/120    avg_loss:0.250, val_acc:0.923]
Epoch [62/120    avg_loss:0.245, val_acc:0.948]
Epoch [63/120    avg_loss:0.211, val_acc:0.952]
Epoch [64/120    avg_loss:0.182, val_acc:0.948]
Epoch [65/120    avg_loss:0.189, val_acc:0.958]
Epoch [66/120    avg_loss:0.172, val_acc:0.944]
Epoch [67/120    avg_loss:0.228, val_acc:0.963]
Epoch [68/120    avg_loss:0.187, val_acc:0.948]
Epoch [69/120    avg_loss:0.182, val_acc:0.944]
Epoch [70/120    avg_loss:0.173, val_acc:0.960]
Epoch [71/120    avg_loss:0.176, val_acc:0.948]
Epoch [72/120    avg_loss:0.202, val_acc:0.963]
Epoch [73/120    avg_loss:0.170, val_acc:0.944]
Epoch [74/120    avg_loss:0.182, val_acc:0.963]
Epoch [75/120    avg_loss:0.166, val_acc:0.931]
Epoch [76/120    avg_loss:0.174, val_acc:0.958]
Epoch [77/120    avg_loss:0.194, val_acc:0.971]
Epoch [78/120    avg_loss:0.167, val_acc:0.954]
Epoch [79/120    avg_loss:0.144, val_acc:0.965]
Epoch [80/120    avg_loss:0.143, val_acc:0.973]
Epoch [81/120    avg_loss:0.114, val_acc:0.983]
Epoch [82/120    avg_loss:0.135, val_acc:0.967]
Epoch [83/120    avg_loss:0.198, val_acc:0.967]
Epoch [84/120    avg_loss:0.171, val_acc:0.958]
Epoch [85/120    avg_loss:0.168, val_acc:0.965]
Epoch [86/120    avg_loss:0.122, val_acc:0.977]
Epoch [87/120    avg_loss:0.106, val_acc:0.979]
Epoch [88/120    avg_loss:0.139, val_acc:0.983]
Epoch [89/120    avg_loss:0.115, val_acc:0.971]
Epoch [90/120    avg_loss:0.097, val_acc:0.969]
Epoch [91/120    avg_loss:0.123, val_acc:0.973]
Epoch [92/120    avg_loss:0.130, val_acc:0.981]
Epoch [93/120    avg_loss:0.112, val_acc:0.969]
Epoch [94/120    avg_loss:0.110, val_acc:0.958]
Epoch [95/120    avg_loss:0.136, val_acc:0.977]
Epoch [96/120    avg_loss:0.097, val_acc:0.975]
Epoch [97/120    avg_loss:0.097, val_acc:0.975]
Epoch [98/120    avg_loss:0.096, val_acc:0.952]
Epoch [99/120    avg_loss:0.140, val_acc:0.969]
Epoch [100/120    avg_loss:0.126, val_acc:0.977]
Epoch [101/120    avg_loss:0.117, val_acc:0.952]
Epoch [102/120    avg_loss:0.113, val_acc:0.973]
Epoch [103/120    avg_loss:0.069, val_acc:0.981]
Epoch [104/120    avg_loss:0.059, val_acc:0.983]
Epoch [105/120    avg_loss:0.070, val_acc:0.979]
Epoch [106/120    avg_loss:0.065, val_acc:0.977]
Epoch [107/120    avg_loss:0.061, val_acc:0.979]
Epoch [108/120    avg_loss:0.052, val_acc:0.981]
Epoch [109/120    avg_loss:0.053, val_acc:0.979]
Epoch [110/120    avg_loss:0.058, val_acc:0.983]
Epoch [111/120    avg_loss:0.054, val_acc:0.985]
Epoch [112/120    avg_loss:0.047, val_acc:0.983]
Epoch [113/120    avg_loss:0.053, val_acc:0.985]
Epoch [114/120    avg_loss:0.046, val_acc:0.983]
Epoch [115/120    avg_loss:0.051, val_acc:0.985]
Epoch [116/120    avg_loss:0.060, val_acc:0.985]
Epoch [117/120    avg_loss:0.058, val_acc:0.983]
Epoch [118/120    avg_loss:0.048, val_acc:0.985]
Epoch [119/120    avg_loss:0.072, val_acc:0.985]
Epoch [120/120    avg_loss:0.059, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   4   0   0   0   0   2   0]
 [  0   0   0 219   8   0   0   0   2   1   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0  24 121   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.52878464818762

F1 scores:
[       nan 1.         0.95089286 0.97550111 0.907173   0.8705036
 1.         0.89265537 0.99742931 0.99893276 1.         1.
 0.99669239 1.        ]

Kappa:
0.9836180901520883
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8064bf2898>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.572, val_acc:0.377]
Epoch [2/120    avg_loss:2.381, val_acc:0.396]
Epoch [3/120    avg_loss:2.242, val_acc:0.438]
Epoch [4/120    avg_loss:2.124, val_acc:0.554]
Epoch [5/120    avg_loss:2.019, val_acc:0.665]
Epoch [6/120    avg_loss:1.917, val_acc:0.665]
Epoch [7/120    avg_loss:1.822, val_acc:0.654]
Epoch [8/120    avg_loss:1.730, val_acc:0.731]
Epoch [9/120    avg_loss:1.614, val_acc:0.740]
Epoch [10/120    avg_loss:1.516, val_acc:0.727]
Epoch [11/120    avg_loss:1.467, val_acc:0.754]
Epoch [12/120    avg_loss:1.358, val_acc:0.771]
Epoch [13/120    avg_loss:1.237, val_acc:0.773]
Epoch [14/120    avg_loss:1.194, val_acc:0.796]
Epoch [15/120    avg_loss:1.132, val_acc:0.796]
Epoch [16/120    avg_loss:1.043, val_acc:0.773]
Epoch [17/120    avg_loss:0.990, val_acc:0.790]
Epoch [18/120    avg_loss:0.938, val_acc:0.823]
Epoch [19/120    avg_loss:0.895, val_acc:0.819]
Epoch [20/120    avg_loss:0.859, val_acc:0.885]
Epoch [21/120    avg_loss:0.777, val_acc:0.833]
Epoch [22/120    avg_loss:0.701, val_acc:0.881]
Epoch [23/120    avg_loss:0.682, val_acc:0.890]
Epoch [24/120    avg_loss:0.649, val_acc:0.894]
Epoch [25/120    avg_loss:0.609, val_acc:0.915]
Epoch [26/120    avg_loss:0.609, val_acc:0.910]
Epoch [27/120    avg_loss:0.558, val_acc:0.898]
Epoch [28/120    avg_loss:0.523, val_acc:0.906]
Epoch [29/120    avg_loss:0.498, val_acc:0.915]
Epoch [30/120    avg_loss:0.502, val_acc:0.910]
Epoch [31/120    avg_loss:0.452, val_acc:0.917]
Epoch [32/120    avg_loss:0.460, val_acc:0.904]
Epoch [33/120    avg_loss:0.449, val_acc:0.929]
Epoch [34/120    avg_loss:0.433, val_acc:0.931]
Epoch [35/120    avg_loss:0.435, val_acc:0.904]
Epoch [36/120    avg_loss:0.437, val_acc:0.929]
Epoch [37/120    avg_loss:0.401, val_acc:0.927]
Epoch [38/120    avg_loss:0.420, val_acc:0.931]
Epoch [39/120    avg_loss:0.361, val_acc:0.919]
Epoch [40/120    avg_loss:0.339, val_acc:0.940]
Epoch [41/120    avg_loss:0.322, val_acc:0.940]
Epoch [42/120    avg_loss:0.321, val_acc:0.938]
Epoch [43/120    avg_loss:0.297, val_acc:0.954]
Epoch [44/120    avg_loss:0.294, val_acc:0.944]
Epoch [45/120    avg_loss:0.315, val_acc:0.956]
Epoch [46/120    avg_loss:0.290, val_acc:0.958]
Epoch [47/120    avg_loss:0.300, val_acc:0.933]
Epoch [48/120    avg_loss:0.292, val_acc:0.948]
Epoch [49/120    avg_loss:0.268, val_acc:0.942]
Epoch [50/120    avg_loss:0.294, val_acc:0.940]
Epoch [51/120    avg_loss:0.281, val_acc:0.948]
Epoch [52/120    avg_loss:0.313, val_acc:0.944]
Epoch [53/120    avg_loss:0.250, val_acc:0.935]
Epoch [54/120    avg_loss:0.278, val_acc:0.952]
Epoch [55/120    avg_loss:0.213, val_acc:0.963]
Epoch [56/120    avg_loss:0.201, val_acc:0.946]
Epoch [57/120    avg_loss:0.240, val_acc:0.917]
Epoch [58/120    avg_loss:0.232, val_acc:0.946]
Epoch [59/120    avg_loss:0.291, val_acc:0.946]
Epoch [60/120    avg_loss:0.272, val_acc:0.931]
Epoch [61/120    avg_loss:0.280, val_acc:0.933]
Epoch [62/120    avg_loss:0.255, val_acc:0.958]
Epoch [63/120    avg_loss:0.239, val_acc:0.929]
Epoch [64/120    avg_loss:0.251, val_acc:0.950]
Epoch [65/120    avg_loss:0.203, val_acc:0.958]
Epoch [66/120    avg_loss:0.210, val_acc:0.956]
Epoch [67/120    avg_loss:0.213, val_acc:0.958]
Epoch [68/120    avg_loss:0.206, val_acc:0.956]
Epoch [69/120    avg_loss:0.179, val_acc:0.958]
Epoch [70/120    avg_loss:0.135, val_acc:0.967]
Epoch [71/120    avg_loss:0.141, val_acc:0.967]
Epoch [72/120    avg_loss:0.134, val_acc:0.969]
Epoch [73/120    avg_loss:0.147, val_acc:0.969]
Epoch [74/120    avg_loss:0.140, val_acc:0.971]
Epoch [75/120    avg_loss:0.140, val_acc:0.967]
Epoch [76/120    avg_loss:0.145, val_acc:0.969]
Epoch [77/120    avg_loss:0.121, val_acc:0.967]
Epoch [78/120    avg_loss:0.139, val_acc:0.969]
Epoch [79/120    avg_loss:0.124, val_acc:0.971]
Epoch [80/120    avg_loss:0.129, val_acc:0.971]
Epoch [81/120    avg_loss:0.130, val_acc:0.971]
Epoch [82/120    avg_loss:0.130, val_acc:0.969]
Epoch [83/120    avg_loss:0.131, val_acc:0.967]
Epoch [84/120    avg_loss:0.139, val_acc:0.971]
Epoch [85/120    avg_loss:0.115, val_acc:0.971]
Epoch [86/120    avg_loss:0.123, val_acc:0.971]
Epoch [87/120    avg_loss:0.120, val_acc:0.969]
Epoch [88/120    avg_loss:0.132, val_acc:0.971]
Epoch [89/120    avg_loss:0.135, val_acc:0.971]
Epoch [90/120    avg_loss:0.119, val_acc:0.973]
Epoch [91/120    avg_loss:0.122, val_acc:0.973]
Epoch [92/120    avg_loss:0.136, val_acc:0.967]
Epoch [93/120    avg_loss:0.122, val_acc:0.973]
Epoch [94/120    avg_loss:0.122, val_acc:0.969]
Epoch [95/120    avg_loss:0.110, val_acc:0.971]
Epoch [96/120    avg_loss:0.127, val_acc:0.971]
Epoch [97/120    avg_loss:0.110, val_acc:0.971]
Epoch [98/120    avg_loss:0.126, val_acc:0.971]
Epoch [99/120    avg_loss:0.113, val_acc:0.969]
Epoch [100/120    avg_loss:0.126, val_acc:0.971]
Epoch [101/120    avg_loss:0.120, val_acc:0.971]
Epoch [102/120    avg_loss:0.121, val_acc:0.969]
Epoch [103/120    avg_loss:0.114, val_acc:0.969]
Epoch [104/120    avg_loss:0.101, val_acc:0.971]
Epoch [105/120    avg_loss:0.113, val_acc:0.971]
Epoch [106/120    avg_loss:0.126, val_acc:0.973]
Epoch [107/120    avg_loss:0.120, val_acc:0.969]
Epoch [108/120    avg_loss:0.117, val_acc:0.971]
Epoch [109/120    avg_loss:0.119, val_acc:0.971]
Epoch [110/120    avg_loss:0.110, val_acc:0.969]
Epoch [111/120    avg_loss:0.110, val_acc:0.969]
Epoch [112/120    avg_loss:0.099, val_acc:0.971]
Epoch [113/120    avg_loss:0.095, val_acc:0.971]
Epoch [114/120    avg_loss:0.117, val_acc:0.969]
Epoch [115/120    avg_loss:0.104, val_acc:0.969]
Epoch [116/120    avg_loss:0.114, val_acc:0.967]
Epoch [117/120    avg_loss:0.101, val_acc:0.971]
Epoch [118/120    avg_loss:0.133, val_acc:0.973]
Epoch [119/120    avg_loss:0.099, val_acc:0.975]
Epoch [120/120    avg_loss:0.112, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 202   0   0   0   0  12   0   0   0   0   5   0]
 [  0   0   1 220   4   0   0   0   4   1   0   0   0   0]
 [  0   0   0   0 202  24   0   0   1   0   0   0   0   0]
 [  0   0   0   0  34 111   0   0   0   0   0   0   0   0]
 [  0   7   0   0   0   0 199   0   0   0   0   0   0   0]
 [  0   0  16   0   0   0   0  78   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   3   0   0   0   0   0   1   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.56929637526652

F1 scores:
[       nan 0.99491649 0.91609977 0.97777778 0.86509636 0.79285714
 0.98271605 0.84782609 0.99232737 0.99893276 1.         0.99867198
 0.98898678 1.        ]

Kappa:
0.9729300741681337
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5e2ed9a860>
supervision:full
center_pixel:True
Network :
Number of parameter: 37912==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.615, val_acc:0.283]
Epoch [2/120    avg_loss:2.417, val_acc:0.323]
Epoch [3/120    avg_loss:2.274, val_acc:0.358]
Epoch [4/120    avg_loss:2.166, val_acc:0.417]
Epoch [5/120    avg_loss:2.078, val_acc:0.519]
Epoch [6/120    avg_loss:1.965, val_acc:0.554]
Epoch [7/120    avg_loss:1.860, val_acc:0.592]
Epoch [8/120    avg_loss:1.753, val_acc:0.606]
Epoch [9/120    avg_loss:1.652, val_acc:0.621]
Epoch [10/120    avg_loss:1.549, val_acc:0.648]
Epoch [11/120    avg_loss:1.449, val_acc:0.656]
Epoch [12/120    avg_loss:1.348, val_acc:0.685]
Epoch [13/120    avg_loss:1.273, val_acc:0.685]
Epoch [14/120    avg_loss:1.184, val_acc:0.717]
Epoch [15/120    avg_loss:1.110, val_acc:0.771]
Epoch [16/120    avg_loss:1.047, val_acc:0.806]
Epoch [17/120    avg_loss:1.014, val_acc:0.840]
Epoch [18/120    avg_loss:0.950, val_acc:0.819]
Epoch [19/120    avg_loss:0.887, val_acc:0.829]
Epoch [20/120    avg_loss:0.851, val_acc:0.867]
Epoch [21/120    avg_loss:0.803, val_acc:0.887]
Epoch [22/120    avg_loss:0.756, val_acc:0.896]
Epoch [23/120    avg_loss:0.749, val_acc:0.883]
Epoch [24/120    avg_loss:0.715, val_acc:0.894]
Epoch [25/120    avg_loss:0.626, val_acc:0.921]
Epoch [26/120    avg_loss:0.598, val_acc:0.904]
Epoch [27/120    avg_loss:0.562, val_acc:0.912]
Epoch [28/120    avg_loss:0.513, val_acc:0.894]
Epoch [29/120    avg_loss:0.541, val_acc:0.915]
Epoch [30/120    avg_loss:0.502, val_acc:0.938]
Epoch [31/120    avg_loss:0.461, val_acc:0.885]
Epoch [32/120    avg_loss:0.455, val_acc:0.919]
Epoch [33/120    avg_loss:0.471, val_acc:0.923]
Epoch [34/120    avg_loss:0.458, val_acc:0.921]
Epoch [35/120    avg_loss:0.464, val_acc:0.867]
Epoch [36/120    avg_loss:0.447, val_acc:0.898]
Epoch [37/120    avg_loss:0.423, val_acc:0.938]
Epoch [38/120    avg_loss:0.380, val_acc:0.917]
Epoch [39/120    avg_loss:0.370, val_acc:0.896]
Epoch [40/120    avg_loss:0.427, val_acc:0.946]
Epoch [41/120    avg_loss:0.361, val_acc:0.946]
Epoch [42/120    avg_loss:0.343, val_acc:0.954]
Epoch [43/120    avg_loss:0.307, val_acc:0.923]
Epoch [44/120    avg_loss:0.314, val_acc:0.910]
Epoch [45/120    avg_loss:0.380, val_acc:0.921]
Epoch [46/120    avg_loss:0.380, val_acc:0.950]
Epoch [47/120    avg_loss:0.327, val_acc:0.931]
Epoch [48/120    avg_loss:0.307, val_acc:0.954]
Epoch [49/120    avg_loss:0.290, val_acc:0.952]
Epoch [50/120    avg_loss:0.270, val_acc:0.954]
Epoch [51/120    avg_loss:0.276, val_acc:0.940]
Epoch [52/120    avg_loss:0.290, val_acc:0.933]
Epoch [53/120    avg_loss:0.229, val_acc:0.952]
Epoch [54/120    avg_loss:0.229, val_acc:0.958]
Epoch [55/120    avg_loss:0.215, val_acc:0.948]
Epoch [56/120    avg_loss:0.241, val_acc:0.965]
Epoch [57/120    avg_loss:0.234, val_acc:0.969]
Epoch [58/120    avg_loss:0.200, val_acc:0.958]
Epoch [59/120    avg_loss:0.197, val_acc:0.965]
Epoch [60/120    avg_loss:0.242, val_acc:0.960]
Epoch [61/120    avg_loss:0.237, val_acc:0.944]
Epoch [62/120    avg_loss:0.255, val_acc:0.967]
Epoch [63/120    avg_loss:0.201, val_acc:0.960]
Epoch [64/120    avg_loss:0.207, val_acc:0.975]
Epoch [65/120    avg_loss:0.172, val_acc:0.971]
Epoch [66/120    avg_loss:0.193, val_acc:0.965]
Epoch [67/120    avg_loss:0.224, val_acc:0.969]
Epoch [68/120    avg_loss:0.235, val_acc:0.965]
Epoch [69/120    avg_loss:0.164, val_acc:0.963]
Epoch [70/120    avg_loss:0.170, val_acc:0.983]
Epoch [71/120    avg_loss:0.175, val_acc:0.971]
Epoch [72/120    avg_loss:0.141, val_acc:0.975]
Epoch [73/120    avg_loss:0.170, val_acc:0.975]
Epoch [74/120    avg_loss:0.124, val_acc:0.971]
Epoch [75/120    avg_loss:0.166, val_acc:0.981]
Epoch [76/120    avg_loss:0.166, val_acc:0.971]
Epoch [77/120    avg_loss:0.137, val_acc:0.967]
Epoch [78/120    avg_loss:0.120, val_acc:0.979]
Epoch [79/120    avg_loss:0.116, val_acc:0.973]
Epoch [80/120    avg_loss:0.110, val_acc:0.973]
Epoch [81/120    avg_loss:0.104, val_acc:0.963]
Epoch [82/120    avg_loss:0.154, val_acc:0.977]
Epoch [83/120    avg_loss:0.104, val_acc:0.977]
Epoch [84/120    avg_loss:0.106, val_acc:0.981]
Epoch [85/120    avg_loss:0.081, val_acc:0.977]
Epoch [86/120    avg_loss:0.080, val_acc:0.977]
Epoch [87/120    avg_loss:0.072, val_acc:0.981]
Epoch [88/120    avg_loss:0.083, val_acc:0.983]
Epoch [89/120    avg_loss:0.069, val_acc:0.985]
Epoch [90/120    avg_loss:0.073, val_acc:0.983]
Epoch [91/120    avg_loss:0.072, val_acc:0.983]
Epoch [92/120    avg_loss:0.083, val_acc:0.981]
Epoch [93/120    avg_loss:0.072, val_acc:0.985]
Epoch [94/120    avg_loss:0.084, val_acc:0.981]
Epoch [95/120    avg_loss:0.079, val_acc:0.981]
Epoch [96/120    avg_loss:0.066, val_acc:0.983]
Epoch [97/120    avg_loss:0.066, val_acc:0.985]
Epoch [98/120    avg_loss:0.064, val_acc:0.981]
Epoch [99/120    avg_loss:0.066, val_acc:0.983]
Epoch [100/120    avg_loss:0.068, val_acc:0.985]
Epoch [101/120    avg_loss:0.067, val_acc:0.985]
Epoch [102/120    avg_loss:0.062, val_acc:0.988]
Epoch [103/120    avg_loss:0.073, val_acc:0.988]
Epoch [104/120    avg_loss:0.061, val_acc:0.988]
Epoch [105/120    avg_loss:0.064, val_acc:0.981]
Epoch [106/120    avg_loss:0.066, val_acc:0.990]
Epoch [107/120    avg_loss:0.059, val_acc:0.990]
Epoch [108/120    avg_loss:0.064, val_acc:0.983]
Epoch [109/120    avg_loss:0.058, val_acc:0.977]
Epoch [110/120    avg_loss:0.060, val_acc:0.983]
Epoch [111/120    avg_loss:0.060, val_acc:0.985]
Epoch [112/120    avg_loss:0.066, val_acc:0.983]
Epoch [113/120    avg_loss:0.053, val_acc:0.983]
Epoch [114/120    avg_loss:0.071, val_acc:0.983]
Epoch [115/120    avg_loss:0.060, val_acc:0.985]
Epoch [116/120    avg_loss:0.056, val_acc:0.983]
Epoch [117/120    avg_loss:0.074, val_acc:0.981]
Epoch [118/120    avg_loss:0.055, val_acc:0.988]
Epoch [119/120    avg_loss:0.058, val_acc:0.988]
Epoch [120/120    avg_loss:0.068, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   3   0   0   7   0   0   0   0   0   0]
 [  0   0   0 218   7   0   0   0   5   0   0   0   0   0]
 [  0   0   1   1 214  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   1   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.7633262260128

F1 scores:
[       nan 1.         0.94570136 0.97104677 0.93043478 0.93150685
 1.         0.89010989 0.99232737 1.         1.         1.
 0.99889503 1.        ]

Kappa:
0.9862309633653628
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ccdc57898>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.611, val_acc:0.296]
Epoch [2/120    avg_loss:2.422, val_acc:0.348]
Epoch [3/120    avg_loss:2.302, val_acc:0.392]
Epoch [4/120    avg_loss:2.183, val_acc:0.525]
Epoch [5/120    avg_loss:2.075, val_acc:0.556]
Epoch [6/120    avg_loss:1.964, val_acc:0.596]
Epoch [7/120    avg_loss:1.845, val_acc:0.590]
Epoch [8/120    avg_loss:1.745, val_acc:0.619]
Epoch [9/120    avg_loss:1.629, val_acc:0.633]
Epoch [10/120    avg_loss:1.520, val_acc:0.665]
Epoch [11/120    avg_loss:1.409, val_acc:0.694]
Epoch [12/120    avg_loss:1.316, val_acc:0.681]
Epoch [13/120    avg_loss:1.240, val_acc:0.713]
Epoch [14/120    avg_loss:1.174, val_acc:0.775]
Epoch [15/120    avg_loss:1.082, val_acc:0.765]
Epoch [16/120    avg_loss:1.004, val_acc:0.771]
Epoch [17/120    avg_loss:0.985, val_acc:0.792]
Epoch [18/120    avg_loss:0.880, val_acc:0.838]
Epoch [19/120    avg_loss:0.830, val_acc:0.821]
Epoch [20/120    avg_loss:0.803, val_acc:0.898]
Epoch [21/120    avg_loss:0.729, val_acc:0.887]
Epoch [22/120    avg_loss:0.692, val_acc:0.892]
Epoch [23/120    avg_loss:0.676, val_acc:0.883]
Epoch [24/120    avg_loss:0.634, val_acc:0.858]
Epoch [25/120    avg_loss:0.580, val_acc:0.902]
Epoch [26/120    avg_loss:0.543, val_acc:0.931]
Epoch [27/120    avg_loss:0.520, val_acc:0.923]
Epoch [28/120    avg_loss:0.492, val_acc:0.912]
Epoch [29/120    avg_loss:0.436, val_acc:0.921]
Epoch [30/120    avg_loss:0.430, val_acc:0.925]
Epoch [31/120    avg_loss:0.436, val_acc:0.927]
Epoch [32/120    avg_loss:0.449, val_acc:0.921]
Epoch [33/120    avg_loss:0.426, val_acc:0.917]
Epoch [34/120    avg_loss:0.379, val_acc:0.935]
Epoch [35/120    avg_loss:0.355, val_acc:0.927]
Epoch [36/120    avg_loss:0.357, val_acc:0.931]
Epoch [37/120    avg_loss:0.328, val_acc:0.929]
Epoch [38/120    avg_loss:0.342, val_acc:0.935]
Epoch [39/120    avg_loss:0.377, val_acc:0.902]
Epoch [40/120    avg_loss:0.348, val_acc:0.952]
Epoch [41/120    avg_loss:0.296, val_acc:0.954]
Epoch [42/120    avg_loss:0.251, val_acc:0.956]
Epoch [43/120    avg_loss:0.298, val_acc:0.950]
Epoch [44/120    avg_loss:0.281, val_acc:0.940]
Epoch [45/120    avg_loss:0.226, val_acc:0.938]
Epoch [46/120    avg_loss:0.225, val_acc:0.935]
Epoch [47/120    avg_loss:0.333, val_acc:0.938]
Epoch [48/120    avg_loss:0.244, val_acc:0.944]
Epoch [49/120    avg_loss:0.298, val_acc:0.933]
Epoch [50/120    avg_loss:0.244, val_acc:0.954]
Epoch [51/120    avg_loss:0.225, val_acc:0.956]
Epoch [52/120    avg_loss:0.211, val_acc:0.956]
Epoch [53/120    avg_loss:0.190, val_acc:0.963]
Epoch [54/120    avg_loss:0.182, val_acc:0.960]
Epoch [55/120    avg_loss:0.188, val_acc:0.954]
Epoch [56/120    avg_loss:0.155, val_acc:0.952]
Epoch [57/120    avg_loss:0.185, val_acc:0.965]
Epoch [58/120    avg_loss:0.214, val_acc:0.977]
Epoch [59/120    avg_loss:0.173, val_acc:0.965]
Epoch [60/120    avg_loss:0.151, val_acc:0.971]
Epoch [61/120    avg_loss:0.170, val_acc:0.956]
Epoch [62/120    avg_loss:0.168, val_acc:0.973]
Epoch [63/120    avg_loss:0.153, val_acc:0.960]
Epoch [64/120    avg_loss:0.150, val_acc:0.965]
Epoch [65/120    avg_loss:0.214, val_acc:0.950]
Epoch [66/120    avg_loss:0.213, val_acc:0.958]
Epoch [67/120    avg_loss:0.159, val_acc:0.965]
Epoch [68/120    avg_loss:0.169, val_acc:0.944]
Epoch [69/120    avg_loss:0.156, val_acc:0.956]
Epoch [70/120    avg_loss:0.119, val_acc:0.973]
Epoch [71/120    avg_loss:0.105, val_acc:0.967]
Epoch [72/120    avg_loss:0.094, val_acc:0.971]
Epoch [73/120    avg_loss:0.090, val_acc:0.973]
Epoch [74/120    avg_loss:0.077, val_acc:0.971]
Epoch [75/120    avg_loss:0.073, val_acc:0.973]
Epoch [76/120    avg_loss:0.085, val_acc:0.975]
Epoch [77/120    avg_loss:0.076, val_acc:0.977]
Epoch [78/120    avg_loss:0.067, val_acc:0.979]
Epoch [79/120    avg_loss:0.064, val_acc:0.979]
Epoch [80/120    avg_loss:0.070, val_acc:0.979]
Epoch [81/120    avg_loss:0.078, val_acc:0.979]
Epoch [82/120    avg_loss:0.063, val_acc:0.975]
Epoch [83/120    avg_loss:0.060, val_acc:0.975]
Epoch [84/120    avg_loss:0.067, val_acc:0.975]
Epoch [85/120    avg_loss:0.078, val_acc:0.977]
Epoch [86/120    avg_loss:0.070, val_acc:0.977]
Epoch [87/120    avg_loss:0.060, val_acc:0.977]
Epoch [88/120    avg_loss:0.066, val_acc:0.977]
Epoch [89/120    avg_loss:0.068, val_acc:0.977]
Epoch [90/120    avg_loss:0.075, val_acc:0.979]
Epoch [91/120    avg_loss:0.063, val_acc:0.977]
Epoch [92/120    avg_loss:0.061, val_acc:0.977]
Epoch [93/120    avg_loss:0.075, val_acc:0.979]
Epoch [94/120    avg_loss:0.073, val_acc:0.977]
Epoch [95/120    avg_loss:0.064, val_acc:0.977]
Epoch [96/120    avg_loss:0.076, val_acc:0.979]
Epoch [97/120    avg_loss:0.067, val_acc:0.979]
Epoch [98/120    avg_loss:0.063, val_acc:0.977]
Epoch [99/120    avg_loss:0.073, val_acc:0.977]
Epoch [100/120    avg_loss:0.065, val_acc:0.979]
Epoch [101/120    avg_loss:0.073, val_acc:0.979]
Epoch [102/120    avg_loss:0.064, val_acc:0.979]
Epoch [103/120    avg_loss:0.059, val_acc:0.979]
Epoch [104/120    avg_loss:0.059, val_acc:0.979]
Epoch [105/120    avg_loss:0.072, val_acc:0.979]
Epoch [106/120    avg_loss:0.063, val_acc:0.979]
Epoch [107/120    avg_loss:0.060, val_acc:0.979]
Epoch [108/120    avg_loss:0.063, val_acc:0.979]
Epoch [109/120    avg_loss:0.050, val_acc:0.979]
Epoch [110/120    avg_loss:0.071, val_acc:0.979]
Epoch [111/120    avg_loss:0.062, val_acc:0.979]
Epoch [112/120    avg_loss:0.067, val_acc:0.979]
Epoch [113/120    avg_loss:0.062, val_acc:0.979]
Epoch [114/120    avg_loss:0.070, val_acc:0.979]
Epoch [115/120    avg_loss:0.052, val_acc:0.981]
Epoch [116/120    avg_loss:0.047, val_acc:0.979]
Epoch [117/120    avg_loss:0.055, val_acc:0.979]
Epoch [118/120    avg_loss:0.052, val_acc:0.979]
Epoch [119/120    avg_loss:0.060, val_acc:0.981]
Epoch [120/120    avg_loss:0.057, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 219   9   0   0   0   0   2   0   0   0   0]
 [  0   0   0   0 204  23   0   0   0   0   0   0   0   0]
 [  0   0   0   1   1 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8   0 198   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0   0 383   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.61407249466951

F1 scores:
[       nan 1.         0.95842451 0.97333333 0.90868597 0.91961415
 0.98019802 0.91954023 0.99351492 0.9978678  1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.9845702405822666
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f74d3f95940>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.632, val_acc:0.317]
Epoch [2/120    avg_loss:2.443, val_acc:0.315]
Epoch [3/120    avg_loss:2.320, val_acc:0.350]
Epoch [4/120    avg_loss:2.227, val_acc:0.429]
Epoch [5/120    avg_loss:2.119, val_acc:0.533]
Epoch [6/120    avg_loss:2.029, val_acc:0.537]
Epoch [7/120    avg_loss:1.923, val_acc:0.562]
Epoch [8/120    avg_loss:1.825, val_acc:0.606]
Epoch [9/120    avg_loss:1.708, val_acc:0.648]
Epoch [10/120    avg_loss:1.611, val_acc:0.646]
Epoch [11/120    avg_loss:1.524, val_acc:0.658]
Epoch [12/120    avg_loss:1.414, val_acc:0.665]
Epoch [13/120    avg_loss:1.322, val_acc:0.665]
Epoch [14/120    avg_loss:1.238, val_acc:0.696]
Epoch [15/120    avg_loss:1.134, val_acc:0.704]
Epoch [16/120    avg_loss:1.084, val_acc:0.775]
Epoch [17/120    avg_loss:1.004, val_acc:0.852]
Epoch [18/120    avg_loss:0.953, val_acc:0.854]
Epoch [19/120    avg_loss:0.884, val_acc:0.883]
Epoch [20/120    avg_loss:0.826, val_acc:0.863]
Epoch [21/120    avg_loss:0.789, val_acc:0.863]
Epoch [22/120    avg_loss:0.742, val_acc:0.912]
Epoch [23/120    avg_loss:0.668, val_acc:0.894]
Epoch [24/120    avg_loss:0.612, val_acc:0.892]
Epoch [25/120    avg_loss:0.579, val_acc:0.912]
Epoch [26/120    avg_loss:0.567, val_acc:0.894]
Epoch [27/120    avg_loss:0.509, val_acc:0.925]
Epoch [28/120    avg_loss:0.492, val_acc:0.935]
Epoch [29/120    avg_loss:0.483, val_acc:0.927]
Epoch [30/120    avg_loss:0.464, val_acc:0.902]
Epoch [31/120    avg_loss:0.428, val_acc:0.910]
Epoch [32/120    avg_loss:0.433, val_acc:0.927]
Epoch [33/120    avg_loss:0.381, val_acc:0.915]
Epoch [34/120    avg_loss:0.415, val_acc:0.902]
Epoch [35/120    avg_loss:0.404, val_acc:0.938]
Epoch [36/120    avg_loss:0.387, val_acc:0.929]
Epoch [37/120    avg_loss:0.318, val_acc:0.942]
Epoch [38/120    avg_loss:0.338, val_acc:0.929]
Epoch [39/120    avg_loss:0.331, val_acc:0.917]
Epoch [40/120    avg_loss:0.313, val_acc:0.912]
Epoch [41/120    avg_loss:0.315, val_acc:0.935]
Epoch [42/120    avg_loss:0.308, val_acc:0.948]
Epoch [43/120    avg_loss:0.281, val_acc:0.929]
Epoch [44/120    avg_loss:0.273, val_acc:0.950]
Epoch [45/120    avg_loss:0.284, val_acc:0.940]
Epoch [46/120    avg_loss:0.285, val_acc:0.948]
Epoch [47/120    avg_loss:0.279, val_acc:0.935]
Epoch [48/120    avg_loss:0.248, val_acc:0.948]
Epoch [49/120    avg_loss:0.226, val_acc:0.931]
Epoch [50/120    avg_loss:0.297, val_acc:0.940]
Epoch [51/120    avg_loss:0.344, val_acc:0.956]
Epoch [52/120    avg_loss:0.283, val_acc:0.933]
Epoch [53/120    avg_loss:0.249, val_acc:0.935]
Epoch [54/120    avg_loss:0.249, val_acc:0.925]
Epoch [55/120    avg_loss:0.270, val_acc:0.935]
Epoch [56/120    avg_loss:0.256, val_acc:0.885]
Epoch [57/120    avg_loss:0.298, val_acc:0.938]
Epoch [58/120    avg_loss:0.274, val_acc:0.935]
Epoch [59/120    avg_loss:0.207, val_acc:0.946]
Epoch [60/120    avg_loss:0.184, val_acc:0.938]
Epoch [61/120    avg_loss:0.173, val_acc:0.960]
Epoch [62/120    avg_loss:0.183, val_acc:0.938]
Epoch [63/120    avg_loss:0.166, val_acc:0.938]
Epoch [64/120    avg_loss:0.165, val_acc:0.954]
Epoch [65/120    avg_loss:0.219, val_acc:0.940]
Epoch [66/120    avg_loss:0.195, val_acc:0.950]
Epoch [67/120    avg_loss:0.177, val_acc:0.944]
Epoch [68/120    avg_loss:0.153, val_acc:0.963]
Epoch [69/120    avg_loss:0.160, val_acc:0.969]
Epoch [70/120    avg_loss:0.149, val_acc:0.960]
Epoch [71/120    avg_loss:0.155, val_acc:0.946]
Epoch [72/120    avg_loss:0.193, val_acc:0.902]
Epoch [73/120    avg_loss:0.181, val_acc:0.956]
Epoch [74/120    avg_loss:0.156, val_acc:0.950]
Epoch [75/120    avg_loss:0.162, val_acc:0.956]
Epoch [76/120    avg_loss:0.116, val_acc:0.967]
Epoch [77/120    avg_loss:0.123, val_acc:0.954]
Epoch [78/120    avg_loss:0.132, val_acc:0.948]
Epoch [79/120    avg_loss:0.150, val_acc:0.965]
Epoch [80/120    avg_loss:0.136, val_acc:0.948]
Epoch [81/120    avg_loss:0.133, val_acc:0.933]
Epoch [82/120    avg_loss:0.147, val_acc:0.950]
Epoch [83/120    avg_loss:0.146, val_acc:0.965]
Epoch [84/120    avg_loss:0.105, val_acc:0.965]
Epoch [85/120    avg_loss:0.087, val_acc:0.963]
Epoch [86/120    avg_loss:0.074, val_acc:0.967]
Epoch [87/120    avg_loss:0.081, val_acc:0.969]
Epoch [88/120    avg_loss:0.070, val_acc:0.973]
Epoch [89/120    avg_loss:0.082, val_acc:0.971]
Epoch [90/120    avg_loss:0.080, val_acc:0.971]
Epoch [91/120    avg_loss:0.075, val_acc:0.967]
Epoch [92/120    avg_loss:0.076, val_acc:0.969]
Epoch [93/120    avg_loss:0.075, val_acc:0.969]
Epoch [94/120    avg_loss:0.074, val_acc:0.963]
Epoch [95/120    avg_loss:0.068, val_acc:0.969]
Epoch [96/120    avg_loss:0.068, val_acc:0.971]
Epoch [97/120    avg_loss:0.073, val_acc:0.963]
Epoch [98/120    avg_loss:0.069, val_acc:0.969]
Epoch [99/120    avg_loss:0.064, val_acc:0.967]
Epoch [100/120    avg_loss:0.065, val_acc:0.967]
Epoch [101/120    avg_loss:0.081, val_acc:0.958]
Epoch [102/120    avg_loss:0.072, val_acc:0.958]
Epoch [103/120    avg_loss:0.070, val_acc:0.958]
Epoch [104/120    avg_loss:0.062, val_acc:0.960]
Epoch [105/120    avg_loss:0.069, val_acc:0.965]
Epoch [106/120    avg_loss:0.070, val_acc:0.967]
Epoch [107/120    avg_loss:0.061, val_acc:0.967]
Epoch [108/120    avg_loss:0.068, val_acc:0.967]
Epoch [109/120    avg_loss:0.072, val_acc:0.967]
Epoch [110/120    avg_loss:0.067, val_acc:0.967]
Epoch [111/120    avg_loss:0.070, val_acc:0.967]
Epoch [112/120    avg_loss:0.076, val_acc:0.967]
Epoch [113/120    avg_loss:0.059, val_acc:0.967]
Epoch [114/120    avg_loss:0.068, val_acc:0.967]
Epoch [115/120    avg_loss:0.077, val_acc:0.967]
Epoch [116/120    avg_loss:0.067, val_acc:0.967]
Epoch [117/120    avg_loss:0.058, val_acc:0.967]
Epoch [118/120    avg_loss:0.056, val_acc:0.967]
Epoch [119/120    avg_loss:0.070, val_acc:0.967]
Epoch [120/120    avg_loss:0.077, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   1  21 123   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.44349680170575

F1 scores:
[       nan 1.         0.94570136 0.99565217 0.89519651 0.84827586
 0.99019608 0.86956522 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.982670641565375
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f92d14ee630>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.587, val_acc:0.144]
Epoch [2/120    avg_loss:2.412, val_acc:0.267]
Epoch [3/120    avg_loss:2.287, val_acc:0.412]
Epoch [4/120    avg_loss:2.172, val_acc:0.435]
Epoch [5/120    avg_loss:2.061, val_acc:0.458]
Epoch [6/120    avg_loss:1.965, val_acc:0.506]
Epoch [7/120    avg_loss:1.864, val_acc:0.550]
Epoch [8/120    avg_loss:1.733, val_acc:0.583]
Epoch [9/120    avg_loss:1.613, val_acc:0.623]
Epoch [10/120    avg_loss:1.506, val_acc:0.648]
Epoch [11/120    avg_loss:1.413, val_acc:0.637]
Epoch [12/120    avg_loss:1.303, val_acc:0.675]
Epoch [13/120    avg_loss:1.235, val_acc:0.702]
Epoch [14/120    avg_loss:1.146, val_acc:0.704]
Epoch [15/120    avg_loss:1.108, val_acc:0.696]
Epoch [16/120    avg_loss:1.048, val_acc:0.800]
Epoch [17/120    avg_loss:0.985, val_acc:0.785]
Epoch [18/120    avg_loss:0.926, val_acc:0.748]
Epoch [19/120    avg_loss:0.905, val_acc:0.767]
Epoch [20/120    avg_loss:0.837, val_acc:0.771]
Epoch [21/120    avg_loss:0.809, val_acc:0.783]
Epoch [22/120    avg_loss:0.760, val_acc:0.856]
Epoch [23/120    avg_loss:0.749, val_acc:0.773]
Epoch [24/120    avg_loss:0.732, val_acc:0.879]
Epoch [25/120    avg_loss:0.677, val_acc:0.865]
Epoch [26/120    avg_loss:0.662, val_acc:0.883]
Epoch [27/120    avg_loss:0.698, val_acc:0.894]
Epoch [28/120    avg_loss:0.624, val_acc:0.904]
Epoch [29/120    avg_loss:0.554, val_acc:0.931]
Epoch [30/120    avg_loss:0.560, val_acc:0.898]
Epoch [31/120    avg_loss:0.563, val_acc:0.910]
Epoch [32/120    avg_loss:0.494, val_acc:0.923]
Epoch [33/120    avg_loss:0.515, val_acc:0.908]
Epoch [34/120    avg_loss:0.457, val_acc:0.929]
Epoch [35/120    avg_loss:0.441, val_acc:0.946]
Epoch [36/120    avg_loss:0.424, val_acc:0.942]
Epoch [37/120    avg_loss:0.404, val_acc:0.933]
Epoch [38/120    avg_loss:0.389, val_acc:0.940]
Epoch [39/120    avg_loss:0.396, val_acc:0.958]
Epoch [40/120    avg_loss:0.373, val_acc:0.942]
Epoch [41/120    avg_loss:0.375, val_acc:0.935]
Epoch [42/120    avg_loss:0.408, val_acc:0.950]
Epoch [43/120    avg_loss:0.362, val_acc:0.958]
Epoch [44/120    avg_loss:0.361, val_acc:0.933]
Epoch [45/120    avg_loss:0.378, val_acc:0.854]
Epoch [46/120    avg_loss:0.409, val_acc:0.938]
Epoch [47/120    avg_loss:0.318, val_acc:0.940]
Epoch [48/120    avg_loss:0.360, val_acc:0.948]
Epoch [49/120    avg_loss:0.309, val_acc:0.910]
Epoch [50/120    avg_loss:0.317, val_acc:0.938]
Epoch [51/120    avg_loss:0.299, val_acc:0.952]
Epoch [52/120    avg_loss:0.296, val_acc:0.956]
Epoch [53/120    avg_loss:0.249, val_acc:0.958]
Epoch [54/120    avg_loss:0.265, val_acc:0.965]
Epoch [55/120    avg_loss:0.249, val_acc:0.921]
Epoch [56/120    avg_loss:0.296, val_acc:0.948]
Epoch [57/120    avg_loss:0.279, val_acc:0.963]
Epoch [58/120    avg_loss:0.217, val_acc:0.969]
Epoch [59/120    avg_loss:0.220, val_acc:0.940]
Epoch [60/120    avg_loss:0.203, val_acc:0.965]
Epoch [61/120    avg_loss:0.202, val_acc:0.954]
Epoch [62/120    avg_loss:0.201, val_acc:0.977]
Epoch [63/120    avg_loss:0.189, val_acc:0.956]
Epoch [64/120    avg_loss:0.232, val_acc:0.950]
Epoch [65/120    avg_loss:0.239, val_acc:0.965]
Epoch [66/120    avg_loss:0.204, val_acc:0.975]
Epoch [67/120    avg_loss:0.265, val_acc:0.973]
Epoch [68/120    avg_loss:0.193, val_acc:0.967]
Epoch [69/120    avg_loss:0.169, val_acc:0.965]
Epoch [70/120    avg_loss:0.161, val_acc:0.981]
Epoch [71/120    avg_loss:0.202, val_acc:0.975]
Epoch [72/120    avg_loss:0.200, val_acc:0.958]
Epoch [73/120    avg_loss:0.176, val_acc:0.958]
Epoch [74/120    avg_loss:0.179, val_acc:0.956]
Epoch [75/120    avg_loss:0.143, val_acc:0.981]
Epoch [76/120    avg_loss:0.141, val_acc:0.965]
Epoch [77/120    avg_loss:0.164, val_acc:0.956]
Epoch [78/120    avg_loss:0.147, val_acc:0.981]
Epoch [79/120    avg_loss:0.119, val_acc:0.967]
Epoch [80/120    avg_loss:0.151, val_acc:0.960]
Epoch [81/120    avg_loss:0.123, val_acc:0.973]
Epoch [82/120    avg_loss:0.097, val_acc:0.969]
Epoch [83/120    avg_loss:0.100, val_acc:0.979]
Epoch [84/120    avg_loss:0.103, val_acc:0.973]
Epoch [85/120    avg_loss:0.122, val_acc:0.971]
Epoch [86/120    avg_loss:0.187, val_acc:0.971]
Epoch [87/120    avg_loss:0.171, val_acc:0.956]
Epoch [88/120    avg_loss:0.146, val_acc:0.975]
Epoch [89/120    avg_loss:0.121, val_acc:0.965]
Epoch [90/120    avg_loss:0.143, val_acc:0.950]
Epoch [91/120    avg_loss:0.149, val_acc:0.969]
Epoch [92/120    avg_loss:0.089, val_acc:0.973]
Epoch [93/120    avg_loss:0.108, val_acc:0.979]
Epoch [94/120    avg_loss:0.074, val_acc:0.975]
Epoch [95/120    avg_loss:0.072, val_acc:0.975]
Epoch [96/120    avg_loss:0.073, val_acc:0.977]
Epoch [97/120    avg_loss:0.062, val_acc:0.975]
Epoch [98/120    avg_loss:0.073, val_acc:0.975]
Epoch [99/120    avg_loss:0.074, val_acc:0.979]
Epoch [100/120    avg_loss:0.073, val_acc:0.979]
Epoch [101/120    avg_loss:0.067, val_acc:0.979]
Epoch [102/120    avg_loss:0.070, val_acc:0.979]
Epoch [103/120    avg_loss:0.070, val_acc:0.977]
Epoch [104/120    avg_loss:0.062, val_acc:0.979]
Epoch [105/120    avg_loss:0.058, val_acc:0.979]
Epoch [106/120    avg_loss:0.063, val_acc:0.979]
Epoch [107/120    avg_loss:0.061, val_acc:0.979]
Epoch [108/120    avg_loss:0.058, val_acc:0.979]
Epoch [109/120    avg_loss:0.064, val_acc:0.979]
Epoch [110/120    avg_loss:0.063, val_acc:0.979]
Epoch [111/120    avg_loss:0.064, val_acc:0.979]
Epoch [112/120    avg_loss:0.051, val_acc:0.977]
Epoch [113/120    avg_loss:0.069, val_acc:0.977]
Epoch [114/120    avg_loss:0.061, val_acc:0.977]
Epoch [115/120    avg_loss:0.068, val_acc:0.977]
Epoch [116/120    avg_loss:0.056, val_acc:0.977]
Epoch [117/120    avg_loss:0.060, val_acc:0.977]
Epoch [118/120    avg_loss:0.086, val_acc:0.977]
Epoch [119/120    avg_loss:0.077, val_acc:0.977]
Epoch [120/120    avg_loss:0.057, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 199  26   0   0   0   3   1   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9   0 197   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  14 439   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.71855010660981

F1 scores:
[       nan 1.         0.97117517 0.92773893 0.8436214  0.86006826
 0.97766749 0.93181818 0.99614891 0.99893276 1.         0.98177083
 0.98430493 1.        ]

Kappa:
0.9745988961050661
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fea149828d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.601, val_acc:0.310]
Epoch [2/120    avg_loss:2.385, val_acc:0.388]
Epoch [3/120    avg_loss:2.224, val_acc:0.433]
Epoch [4/120    avg_loss:2.120, val_acc:0.481]
Epoch [5/120    avg_loss:2.016, val_acc:0.515]
Epoch [6/120    avg_loss:1.908, val_acc:0.567]
Epoch [7/120    avg_loss:1.817, val_acc:0.602]
Epoch [8/120    avg_loss:1.703, val_acc:0.617]
Epoch [9/120    avg_loss:1.583, val_acc:0.619]
Epoch [10/120    avg_loss:1.486, val_acc:0.642]
Epoch [11/120    avg_loss:1.407, val_acc:0.673]
Epoch [12/120    avg_loss:1.313, val_acc:0.710]
Epoch [13/120    avg_loss:1.233, val_acc:0.754]
Epoch [14/120    avg_loss:1.151, val_acc:0.729]
Epoch [15/120    avg_loss:1.082, val_acc:0.754]
Epoch [16/120    avg_loss:1.004, val_acc:0.819]
Epoch [17/120    avg_loss:0.948, val_acc:0.850]
Epoch [18/120    avg_loss:0.883, val_acc:0.890]
Epoch [19/120    avg_loss:0.810, val_acc:0.902]
Epoch [20/120    avg_loss:0.736, val_acc:0.915]
Epoch [21/120    avg_loss:0.679, val_acc:0.904]
Epoch [22/120    avg_loss:0.636, val_acc:0.917]
Epoch [23/120    avg_loss:0.597, val_acc:0.900]
Epoch [24/120    avg_loss:0.603, val_acc:0.881]
Epoch [25/120    avg_loss:0.579, val_acc:0.923]
Epoch [26/120    avg_loss:0.545, val_acc:0.929]
Epoch [27/120    avg_loss:0.520, val_acc:0.890]
Epoch [28/120    avg_loss:0.477, val_acc:0.925]
Epoch [29/120    avg_loss:0.423, val_acc:0.919]
Epoch [30/120    avg_loss:0.432, val_acc:0.919]
Epoch [31/120    avg_loss:0.476, val_acc:0.923]
Epoch [32/120    avg_loss:0.431, val_acc:0.925]
Epoch [33/120    avg_loss:0.483, val_acc:0.923]
Epoch [34/120    avg_loss:0.415, val_acc:0.935]
Epoch [35/120    avg_loss:0.395, val_acc:0.923]
Epoch [36/120    avg_loss:0.381, val_acc:0.890]
Epoch [37/120    avg_loss:0.401, val_acc:0.919]
Epoch [38/120    avg_loss:0.348, val_acc:0.935]
Epoch [39/120    avg_loss:0.361, val_acc:0.944]
Epoch [40/120    avg_loss:0.296, val_acc:0.929]
Epoch [41/120    avg_loss:0.324, val_acc:0.925]
Epoch [42/120    avg_loss:0.341, val_acc:0.925]
Epoch [43/120    avg_loss:0.283, val_acc:0.952]
Epoch [44/120    avg_loss:0.247, val_acc:0.954]
Epoch [45/120    avg_loss:0.263, val_acc:0.919]
Epoch [46/120    avg_loss:0.312, val_acc:0.958]
Epoch [47/120    avg_loss:0.262, val_acc:0.965]
Epoch [48/120    avg_loss:0.265, val_acc:0.960]
Epoch [49/120    avg_loss:0.225, val_acc:0.963]
Epoch [50/120    avg_loss:0.204, val_acc:0.969]
Epoch [51/120    avg_loss:0.251, val_acc:0.958]
Epoch [52/120    avg_loss:0.203, val_acc:0.977]
Epoch [53/120    avg_loss:0.210, val_acc:0.950]
Epoch [54/120    avg_loss:0.200, val_acc:0.944]
Epoch [55/120    avg_loss:0.215, val_acc:0.981]
Epoch [56/120    avg_loss:0.195, val_acc:0.946]
Epoch [57/120    avg_loss:0.204, val_acc:0.977]
Epoch [58/120    avg_loss:0.182, val_acc:0.960]
Epoch [59/120    avg_loss:0.186, val_acc:0.944]
Epoch [60/120    avg_loss:0.177, val_acc:0.979]
Epoch [61/120    avg_loss:0.165, val_acc:0.981]
Epoch [62/120    avg_loss:0.137, val_acc:0.967]
Epoch [63/120    avg_loss:0.151, val_acc:0.981]
Epoch [64/120    avg_loss:0.138, val_acc:0.960]
Epoch [65/120    avg_loss:0.187, val_acc:0.973]
Epoch [66/120    avg_loss:0.127, val_acc:0.977]
Epoch [67/120    avg_loss:0.128, val_acc:0.979]
Epoch [68/120    avg_loss:0.138, val_acc:0.967]
Epoch [69/120    avg_loss:0.128, val_acc:0.983]
Epoch [70/120    avg_loss:0.105, val_acc:0.969]
Epoch [71/120    avg_loss:0.117, val_acc:0.979]
Epoch [72/120    avg_loss:0.126, val_acc:0.971]
Epoch [73/120    avg_loss:0.169, val_acc:0.910]
Epoch [74/120    avg_loss:0.169, val_acc:0.975]
Epoch [75/120    avg_loss:0.163, val_acc:0.969]
Epoch [76/120    avg_loss:0.202, val_acc:0.973]
Epoch [77/120    avg_loss:0.137, val_acc:0.981]
Epoch [78/120    avg_loss:0.107, val_acc:0.979]
Epoch [79/120    avg_loss:0.114, val_acc:0.979]
Epoch [80/120    avg_loss:0.117, val_acc:0.971]
Epoch [81/120    avg_loss:0.095, val_acc:0.975]
Epoch [82/120    avg_loss:0.165, val_acc:0.971]
Epoch [83/120    avg_loss:0.136, val_acc:0.988]
Epoch [84/120    avg_loss:0.071, val_acc:0.985]
Epoch [85/120    avg_loss:0.085, val_acc:0.983]
Epoch [86/120    avg_loss:0.074, val_acc:0.985]
Epoch [87/120    avg_loss:0.073, val_acc:0.985]
Epoch [88/120    avg_loss:0.078, val_acc:0.985]
Epoch [89/120    avg_loss:0.064, val_acc:0.985]
Epoch [90/120    avg_loss:0.063, val_acc:0.983]
Epoch [91/120    avg_loss:0.061, val_acc:0.985]
Epoch [92/120    avg_loss:0.059, val_acc:0.985]
Epoch [93/120    avg_loss:0.065, val_acc:0.983]
Epoch [94/120    avg_loss:0.069, val_acc:0.985]
Epoch [95/120    avg_loss:0.058, val_acc:0.985]
Epoch [96/120    avg_loss:0.063, val_acc:0.985]
Epoch [97/120    avg_loss:0.051, val_acc:0.985]
Epoch [98/120    avg_loss:0.064, val_acc:0.985]
Epoch [99/120    avg_loss:0.061, val_acc:0.985]
Epoch [100/120    avg_loss:0.068, val_acc:0.985]
Epoch [101/120    avg_loss:0.067, val_acc:0.985]
Epoch [102/120    avg_loss:0.065, val_acc:0.985]
Epoch [103/120    avg_loss:0.063, val_acc:0.985]
Epoch [104/120    avg_loss:0.060, val_acc:0.983]
Epoch [105/120    avg_loss:0.061, val_acc:0.983]
Epoch [106/120    avg_loss:0.055, val_acc:0.983]
Epoch [107/120    avg_loss:0.052, val_acc:0.983]
Epoch [108/120    avg_loss:0.052, val_acc:0.983]
Epoch [109/120    avg_loss:0.057, val_acc:0.983]
Epoch [110/120    avg_loss:0.061, val_acc:0.983]
Epoch [111/120    avg_loss:0.065, val_acc:0.983]
Epoch [112/120    avg_loss:0.062, val_acc:0.983]
Epoch [113/120    avg_loss:0.059, val_acc:0.983]
Epoch [114/120    avg_loss:0.054, val_acc:0.983]
Epoch [115/120    avg_loss:0.057, val_acc:0.983]
Epoch [116/120    avg_loss:0.058, val_acc:0.983]
Epoch [117/120    avg_loss:0.058, val_acc:0.983]
Epoch [118/120    avg_loss:0.068, val_acc:0.983]
Epoch [119/120    avg_loss:0.057, val_acc:0.983]
Epoch [120/120    avg_loss:0.062, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   1 218   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   4 218   6   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 219   8   0   0   0   0   0   0   0   0]
 [  0   0   0   0  22 123   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 0.99927061 0.96035242 0.97321429 0.92405063 0.89130435
 1.         0.92571429 0.99742931 1.         1.         1.
 1.         1.        ]

Kappa:
0.9867042879025877
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f05cc48d940>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.577, val_acc:0.212]
Epoch [2/120    avg_loss:2.424, val_acc:0.440]
Epoch [3/120    avg_loss:2.281, val_acc:0.531]
Epoch [4/120    avg_loss:2.150, val_acc:0.527]
Epoch [5/120    avg_loss:2.035, val_acc:0.562]
Epoch [6/120    avg_loss:1.913, val_acc:0.604]
Epoch [7/120    avg_loss:1.808, val_acc:0.594]
Epoch [8/120    avg_loss:1.686, val_acc:0.658]
Epoch [9/120    avg_loss:1.556, val_acc:0.635]
Epoch [10/120    avg_loss:1.441, val_acc:0.652]
Epoch [11/120    avg_loss:1.338, val_acc:0.677]
Epoch [12/120    avg_loss:1.267, val_acc:0.702]
Epoch [13/120    avg_loss:1.151, val_acc:0.738]
Epoch [14/120    avg_loss:1.097, val_acc:0.742]
Epoch [15/120    avg_loss:1.020, val_acc:0.769]
Epoch [16/120    avg_loss:0.988, val_acc:0.777]
Epoch [17/120    avg_loss:0.925, val_acc:0.794]
Epoch [18/120    avg_loss:0.877, val_acc:0.792]
Epoch [19/120    avg_loss:0.801, val_acc:0.831]
Epoch [20/120    avg_loss:0.765, val_acc:0.854]
Epoch [21/120    avg_loss:0.674, val_acc:0.904]
Epoch [22/120    avg_loss:0.649, val_acc:0.892]
Epoch [23/120    avg_loss:0.649, val_acc:0.921]
Epoch [24/120    avg_loss:0.603, val_acc:0.921]
Epoch [25/120    avg_loss:0.540, val_acc:0.921]
Epoch [26/120    avg_loss:0.537, val_acc:0.921]
Epoch [27/120    avg_loss:0.526, val_acc:0.915]
Epoch [28/120    avg_loss:0.491, val_acc:0.890]
Epoch [29/120    avg_loss:0.488, val_acc:0.929]
Epoch [30/120    avg_loss:0.477, val_acc:0.929]
Epoch [31/120    avg_loss:0.453, val_acc:0.900]
Epoch [32/120    avg_loss:0.482, val_acc:0.890]
Epoch [33/120    avg_loss:0.463, val_acc:0.923]
Epoch [34/120    avg_loss:0.414, val_acc:0.944]
Epoch [35/120    avg_loss:0.426, val_acc:0.919]
Epoch [36/120    avg_loss:0.420, val_acc:0.915]
Epoch [37/120    avg_loss:0.390, val_acc:0.946]
Epoch [38/120    avg_loss:0.353, val_acc:0.923]
Epoch [39/120    avg_loss:0.395, val_acc:0.935]
Epoch [40/120    avg_loss:0.372, val_acc:0.948]
Epoch [41/120    avg_loss:0.330, val_acc:0.946]
Epoch [42/120    avg_loss:0.305, val_acc:0.946]
Epoch [43/120    avg_loss:0.344, val_acc:0.931]
Epoch [44/120    avg_loss:0.305, val_acc:0.952]
Epoch [45/120    avg_loss:0.325, val_acc:0.944]
Epoch [46/120    avg_loss:0.351, val_acc:0.958]
Epoch [47/120    avg_loss:0.238, val_acc:0.973]
Epoch [48/120    avg_loss:0.227, val_acc:0.956]
Epoch [49/120    avg_loss:0.236, val_acc:0.944]
Epoch [50/120    avg_loss:0.247, val_acc:0.963]
Epoch [51/120    avg_loss:0.240, val_acc:0.944]
Epoch [52/120    avg_loss:0.244, val_acc:0.935]
Epoch [53/120    avg_loss:0.213, val_acc:0.969]
Epoch [54/120    avg_loss:0.181, val_acc:0.971]
Epoch [55/120    avg_loss:0.177, val_acc:0.967]
Epoch [56/120    avg_loss:0.192, val_acc:0.969]
Epoch [57/120    avg_loss:0.229, val_acc:0.938]
Epoch [58/120    avg_loss:0.207, val_acc:0.967]
Epoch [59/120    avg_loss:0.188, val_acc:0.969]
Epoch [60/120    avg_loss:0.188, val_acc:0.963]
Epoch [61/120    avg_loss:0.156, val_acc:0.979]
Epoch [62/120    avg_loss:0.162, val_acc:0.981]
Epoch [63/120    avg_loss:0.143, val_acc:0.979]
Epoch [64/120    avg_loss:0.126, val_acc:0.973]
Epoch [65/120    avg_loss:0.143, val_acc:0.981]
Epoch [66/120    avg_loss:0.131, val_acc:0.981]
Epoch [67/120    avg_loss:0.138, val_acc:0.981]
Epoch [68/120    avg_loss:0.137, val_acc:0.983]
Epoch [69/120    avg_loss:0.139, val_acc:0.979]
Epoch [70/120    avg_loss:0.122, val_acc:0.981]
Epoch [71/120    avg_loss:0.122, val_acc:0.979]
Epoch [72/120    avg_loss:0.124, val_acc:0.983]
Epoch [73/120    avg_loss:0.119, val_acc:0.988]
Epoch [74/120    avg_loss:0.110, val_acc:0.985]
Epoch [75/120    avg_loss:0.110, val_acc:0.988]
Epoch [76/120    avg_loss:0.124, val_acc:0.981]
Epoch [77/120    avg_loss:0.113, val_acc:0.983]
Epoch [78/120    avg_loss:0.103, val_acc:0.985]
Epoch [79/120    avg_loss:0.107, val_acc:0.983]
Epoch [80/120    avg_loss:0.114, val_acc:0.985]
Epoch [81/120    avg_loss:0.120, val_acc:0.985]
Epoch [82/120    avg_loss:0.117, val_acc:0.979]
Epoch [83/120    avg_loss:0.111, val_acc:0.988]
Epoch [84/120    avg_loss:0.114, val_acc:0.983]
Epoch [85/120    avg_loss:0.091, val_acc:0.983]
Epoch [86/120    avg_loss:0.098, val_acc:0.983]
Epoch [87/120    avg_loss:0.105, val_acc:0.981]
Epoch [88/120    avg_loss:0.096, val_acc:0.985]
Epoch [89/120    avg_loss:0.089, val_acc:0.985]
Epoch [90/120    avg_loss:0.112, val_acc:0.985]
Epoch [91/120    avg_loss:0.105, val_acc:0.985]
Epoch [92/120    avg_loss:0.121, val_acc:0.985]
Epoch [93/120    avg_loss:0.097, val_acc:0.983]
Epoch [94/120    avg_loss:0.107, val_acc:0.981]
Epoch [95/120    avg_loss:0.115, val_acc:0.985]
Epoch [96/120    avg_loss:0.119, val_acc:0.983]
Epoch [97/120    avg_loss:0.108, val_acc:0.981]
Epoch [98/120    avg_loss:0.100, val_acc:0.983]
Epoch [99/120    avg_loss:0.098, val_acc:0.983]
Epoch [100/120    avg_loss:0.107, val_acc:0.983]
Epoch [101/120    avg_loss:0.097, val_acc:0.985]
Epoch [102/120    avg_loss:0.103, val_acc:0.990]
Epoch [103/120    avg_loss:0.115, val_acc:0.990]
Epoch [104/120    avg_loss:0.088, val_acc:0.990]
Epoch [105/120    avg_loss:0.120, val_acc:0.988]
Epoch [106/120    avg_loss:0.106, val_acc:0.990]
Epoch [107/120    avg_loss:0.102, val_acc:0.988]
Epoch [108/120    avg_loss:0.099, val_acc:0.988]
Epoch [109/120    avg_loss:0.094, val_acc:0.988]
Epoch [110/120    avg_loss:0.098, val_acc:0.988]
Epoch [111/120    avg_loss:0.095, val_acc:0.988]
Epoch [112/120    avg_loss:0.096, val_acc:0.985]
Epoch [113/120    avg_loss:0.086, val_acc:0.985]
Epoch [114/120    avg_loss:0.107, val_acc:0.988]
Epoch [115/120    avg_loss:0.087, val_acc:0.988]
Epoch [116/120    avg_loss:0.106, val_acc:0.988]
Epoch [117/120    avg_loss:0.096, val_acc:0.988]
Epoch [118/120    avg_loss:0.090, val_acc:0.988]
Epoch [119/120    avg_loss:0.082, val_acc:0.988]
Epoch [120/120    avg_loss:0.100, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   0   8   0   0   0   0   0   0]
 [  0   0   2 221   6   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 201  26   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0   0 387   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.40085287846482

F1 scores:
[       nan 1.         0.94407159 0.98004435 0.89135255 0.875
 0.98771499 0.87912088 0.99870968 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9821966287829822
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feddabb2908>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.609, val_acc:0.142]
Epoch [2/120    avg_loss:2.427, val_acc:0.371]
Epoch [3/120    avg_loss:2.315, val_acc:0.450]
Epoch [4/120    avg_loss:2.215, val_acc:0.487]
Epoch [5/120    avg_loss:2.116, val_acc:0.490]
Epoch [6/120    avg_loss:2.030, val_acc:0.483]
Epoch [7/120    avg_loss:1.922, val_acc:0.487]
Epoch [8/120    avg_loss:1.852, val_acc:0.512]
Epoch [9/120    avg_loss:1.736, val_acc:0.585]
Epoch [10/120    avg_loss:1.627, val_acc:0.646]
Epoch [11/120    avg_loss:1.535, val_acc:0.688]
Epoch [12/120    avg_loss:1.428, val_acc:0.679]
Epoch [13/120    avg_loss:1.344, val_acc:0.708]
Epoch [14/120    avg_loss:1.255, val_acc:0.708]
Epoch [15/120    avg_loss:1.205, val_acc:0.729]
Epoch [16/120    avg_loss:1.123, val_acc:0.752]
Epoch [17/120    avg_loss:1.078, val_acc:0.762]
Epoch [18/120    avg_loss:1.034, val_acc:0.804]
Epoch [19/120    avg_loss:0.961, val_acc:0.823]
Epoch [20/120    avg_loss:0.897, val_acc:0.856]
Epoch [21/120    avg_loss:0.855, val_acc:0.858]
Epoch [22/120    avg_loss:0.806, val_acc:0.852]
Epoch [23/120    avg_loss:0.787, val_acc:0.852]
Epoch [24/120    avg_loss:0.803, val_acc:0.881]
Epoch [25/120    avg_loss:0.700, val_acc:0.873]
Epoch [26/120    avg_loss:0.667, val_acc:0.894]
Epoch [27/120    avg_loss:0.654, val_acc:0.915]
Epoch [28/120    avg_loss:0.602, val_acc:0.919]
Epoch [29/120    avg_loss:0.582, val_acc:0.871]
Epoch [30/120    avg_loss:0.556, val_acc:0.921]
Epoch [31/120    avg_loss:0.530, val_acc:0.927]
Epoch [32/120    avg_loss:0.480, val_acc:0.938]
Epoch [33/120    avg_loss:0.475, val_acc:0.915]
Epoch [34/120    avg_loss:0.531, val_acc:0.881]
Epoch [35/120    avg_loss:0.577, val_acc:0.908]
Epoch [36/120    avg_loss:0.533, val_acc:0.935]
Epoch [37/120    avg_loss:0.453, val_acc:0.944]
Epoch [38/120    avg_loss:0.427, val_acc:0.938]
Epoch [39/120    avg_loss:0.389, val_acc:0.935]
Epoch [40/120    avg_loss:0.369, val_acc:0.950]
Epoch [41/120    avg_loss:0.384, val_acc:0.944]
Epoch [42/120    avg_loss:0.320, val_acc:0.946]
Epoch [43/120    avg_loss:0.329, val_acc:0.952]
Epoch [44/120    avg_loss:0.327, val_acc:0.942]
Epoch [45/120    avg_loss:0.312, val_acc:0.952]
Epoch [46/120    avg_loss:0.267, val_acc:0.952]
Epoch [47/120    avg_loss:0.303, val_acc:0.973]
Epoch [48/120    avg_loss:0.293, val_acc:0.929]
Epoch [49/120    avg_loss:0.301, val_acc:0.942]
Epoch [50/120    avg_loss:0.275, val_acc:0.931]
Epoch [51/120    avg_loss:0.325, val_acc:0.925]
Epoch [52/120    avg_loss:0.304, val_acc:0.965]
Epoch [53/120    avg_loss:0.293, val_acc:0.950]
Epoch [54/120    avg_loss:0.278, val_acc:0.954]
Epoch [55/120    avg_loss:0.254, val_acc:0.954]
Epoch [56/120    avg_loss:0.260, val_acc:0.960]
Epoch [57/120    avg_loss:0.292, val_acc:0.929]
Epoch [58/120    avg_loss:0.292, val_acc:0.958]
Epoch [59/120    avg_loss:0.238, val_acc:0.960]
Epoch [60/120    avg_loss:0.245, val_acc:0.948]
Epoch [61/120    avg_loss:0.221, val_acc:0.958]
Epoch [62/120    avg_loss:0.196, val_acc:0.965]
Epoch [63/120    avg_loss:0.180, val_acc:0.973]
Epoch [64/120    avg_loss:0.181, val_acc:0.985]
Epoch [65/120    avg_loss:0.184, val_acc:0.979]
Epoch [66/120    avg_loss:0.179, val_acc:0.979]
Epoch [67/120    avg_loss:0.165, val_acc:0.981]
Epoch [68/120    avg_loss:0.178, val_acc:0.979]
Epoch [69/120    avg_loss:0.169, val_acc:0.969]
Epoch [70/120    avg_loss:0.169, val_acc:0.977]
Epoch [71/120    avg_loss:0.167, val_acc:0.973]
Epoch [72/120    avg_loss:0.156, val_acc:0.979]
Epoch [73/120    avg_loss:0.159, val_acc:0.977]
Epoch [74/120    avg_loss:0.166, val_acc:0.979]
Epoch [75/120    avg_loss:0.162, val_acc:0.975]
Epoch [76/120    avg_loss:0.166, val_acc:0.977]
Epoch [77/120    avg_loss:0.168, val_acc:0.977]
Epoch [78/120    avg_loss:0.165, val_acc:0.977]
Epoch [79/120    avg_loss:0.162, val_acc:0.977]
Epoch [80/120    avg_loss:0.152, val_acc:0.977]
Epoch [81/120    avg_loss:0.157, val_acc:0.977]
Epoch [82/120    avg_loss:0.140, val_acc:0.977]
Epoch [83/120    avg_loss:0.160, val_acc:0.977]
Epoch [84/120    avg_loss:0.156, val_acc:0.977]
Epoch [85/120    avg_loss:0.153, val_acc:0.977]
Epoch [86/120    avg_loss:0.155, val_acc:0.977]
Epoch [87/120    avg_loss:0.167, val_acc:0.977]
Epoch [88/120    avg_loss:0.166, val_acc:0.977]
Epoch [89/120    avg_loss:0.158, val_acc:0.977]
Epoch [90/120    avg_loss:0.165, val_acc:0.979]
Epoch [91/120    avg_loss:0.152, val_acc:0.979]
Epoch [92/120    avg_loss:0.157, val_acc:0.979]
Epoch [93/120    avg_loss:0.147, val_acc:0.979]
Epoch [94/120    avg_loss:0.166, val_acc:0.979]
Epoch [95/120    avg_loss:0.154, val_acc:0.979]
Epoch [96/120    avg_loss:0.147, val_acc:0.979]
Epoch [97/120    avg_loss:0.153, val_acc:0.979]
Epoch [98/120    avg_loss:0.151, val_acc:0.979]
Epoch [99/120    avg_loss:0.153, val_acc:0.979]
Epoch [100/120    avg_loss:0.150, val_acc:0.979]
Epoch [101/120    avg_loss:0.148, val_acc:0.979]
Epoch [102/120    avg_loss:0.141, val_acc:0.979]
Epoch [103/120    avg_loss:0.165, val_acc:0.979]
Epoch [104/120    avg_loss:0.164, val_acc:0.979]
Epoch [105/120    avg_loss:0.146, val_acc:0.979]
Epoch [106/120    avg_loss:0.153, val_acc:0.979]
Epoch [107/120    avg_loss:0.156, val_acc:0.979]
Epoch [108/120    avg_loss:0.159, val_acc:0.979]
Epoch [109/120    avg_loss:0.164, val_acc:0.979]
Epoch [110/120    avg_loss:0.150, val_acc:0.979]
Epoch [111/120    avg_loss:0.149, val_acc:0.979]
Epoch [112/120    avg_loss:0.153, val_acc:0.979]
Epoch [113/120    avg_loss:0.136, val_acc:0.979]
Epoch [114/120    avg_loss:0.163, val_acc:0.979]
Epoch [115/120    avg_loss:0.156, val_acc:0.979]
Epoch [116/120    avg_loss:0.144, val_acc:0.979]
Epoch [117/120    avg_loss:0.161, val_acc:0.979]
Epoch [118/120    avg_loss:0.155, val_acc:0.979]
Epoch [119/120    avg_loss:0.141, val_acc:0.979]
Epoch [120/120    avg_loss:0.166, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 203   0   0   0   0  16   0   0   0   0   0   0]
 [  0   0   3 213   4   0   0   0  10   0   0   0   0   0]
 [  0   0   0   1 200  26   0   0   0   0   0   0   0   0]
 [  0   0   0   0  29 116   0   0   0   0   0   0   0   0]
 [  0   2   0   0   4   0 200   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.63326226012794

F1 scores:
[       nan 0.99854227 0.92482916 0.95945946 0.86206897 0.80836237
 0.98522167 0.84210526 0.98727735 1.         1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.9736470198033846
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f27f1e338d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.599, val_acc:0.085]
Epoch [2/120    avg_loss:2.438, val_acc:0.319]
Epoch [3/120    avg_loss:2.318, val_acc:0.435]
Epoch [4/120    avg_loss:2.208, val_acc:0.550]
Epoch [5/120    avg_loss:2.111, val_acc:0.588]
Epoch [6/120    avg_loss:2.011, val_acc:0.579]
Epoch [7/120    avg_loss:1.900, val_acc:0.642]
Epoch [8/120    avg_loss:1.777, val_acc:0.652]
Epoch [9/120    avg_loss:1.638, val_acc:0.658]
Epoch [10/120    avg_loss:1.498, val_acc:0.658]
Epoch [11/120    avg_loss:1.416, val_acc:0.665]
Epoch [12/120    avg_loss:1.299, val_acc:0.692]
Epoch [13/120    avg_loss:1.199, val_acc:0.715]
Epoch [14/120    avg_loss:1.122, val_acc:0.700]
Epoch [15/120    avg_loss:1.066, val_acc:0.744]
Epoch [16/120    avg_loss:0.975, val_acc:0.738]
Epoch [17/120    avg_loss:0.925, val_acc:0.754]
Epoch [18/120    avg_loss:0.881, val_acc:0.765]
Epoch [19/120    avg_loss:0.887, val_acc:0.792]
Epoch [20/120    avg_loss:0.821, val_acc:0.783]
Epoch [21/120    avg_loss:0.799, val_acc:0.779]
Epoch [22/120    avg_loss:0.765, val_acc:0.848]
Epoch [23/120    avg_loss:0.700, val_acc:0.844]
Epoch [24/120    avg_loss:0.676, val_acc:0.887]
Epoch [25/120    avg_loss:0.651, val_acc:0.900]
Epoch [26/120    avg_loss:0.613, val_acc:0.906]
Epoch [27/120    avg_loss:0.595, val_acc:0.896]
Epoch [28/120    avg_loss:0.535, val_acc:0.915]
Epoch [29/120    avg_loss:0.566, val_acc:0.896]
Epoch [30/120    avg_loss:0.479, val_acc:0.890]
Epoch [31/120    avg_loss:0.455, val_acc:0.917]
Epoch [32/120    avg_loss:0.445, val_acc:0.923]
Epoch [33/120    avg_loss:0.434, val_acc:0.906]
Epoch [34/120    avg_loss:0.441, val_acc:0.931]
Epoch [35/120    avg_loss:0.398, val_acc:0.933]
Epoch [36/120    avg_loss:0.401, val_acc:0.927]
Epoch [37/120    avg_loss:0.357, val_acc:0.900]
Epoch [38/120    avg_loss:0.363, val_acc:0.940]
Epoch [39/120    avg_loss:0.299, val_acc:0.948]
Epoch [40/120    avg_loss:0.302, val_acc:0.940]
Epoch [41/120    avg_loss:0.280, val_acc:0.954]
Epoch [42/120    avg_loss:0.336, val_acc:0.929]
Epoch [43/120    avg_loss:0.392, val_acc:0.921]
Epoch [44/120    avg_loss:0.327, val_acc:0.948]
Epoch [45/120    avg_loss:0.304, val_acc:0.948]
Epoch [46/120    avg_loss:0.260, val_acc:0.965]
Epoch [47/120    avg_loss:0.295, val_acc:0.942]
Epoch [48/120    avg_loss:0.247, val_acc:0.925]
Epoch [49/120    avg_loss:0.244, val_acc:0.950]
Epoch [50/120    avg_loss:0.254, val_acc:0.956]
Epoch [51/120    avg_loss:0.230, val_acc:0.965]
Epoch [52/120    avg_loss:0.226, val_acc:0.971]
Epoch [53/120    avg_loss:0.209, val_acc:0.948]
Epoch [54/120    avg_loss:0.215, val_acc:0.963]
Epoch [55/120    avg_loss:0.226, val_acc:0.944]
Epoch [56/120    avg_loss:0.203, val_acc:0.963]
Epoch [57/120    avg_loss:0.242, val_acc:0.938]
Epoch [58/120    avg_loss:0.179, val_acc:0.944]
Epoch [59/120    avg_loss:0.193, val_acc:0.965]
Epoch [60/120    avg_loss:0.200, val_acc:0.956]
Epoch [61/120    avg_loss:0.213, val_acc:0.946]
Epoch [62/120    avg_loss:0.182, val_acc:0.935]
Epoch [63/120    avg_loss:0.241, val_acc:0.942]
Epoch [64/120    avg_loss:0.180, val_acc:0.954]
Epoch [65/120    avg_loss:0.164, val_acc:0.975]
Epoch [66/120    avg_loss:0.126, val_acc:0.965]
Epoch [67/120    avg_loss:0.147, val_acc:0.948]
Epoch [68/120    avg_loss:0.129, val_acc:0.971]
Epoch [69/120    avg_loss:0.141, val_acc:0.963]
Epoch [70/120    avg_loss:0.183, val_acc:0.973]
Epoch [71/120    avg_loss:0.145, val_acc:0.969]
Epoch [72/120    avg_loss:0.111, val_acc:0.977]
Epoch [73/120    avg_loss:0.101, val_acc:0.973]
Epoch [74/120    avg_loss:0.125, val_acc:0.963]
Epoch [75/120    avg_loss:0.147, val_acc:0.960]
Epoch [76/120    avg_loss:0.131, val_acc:0.960]
Epoch [77/120    avg_loss:0.128, val_acc:0.969]
Epoch [78/120    avg_loss:0.109, val_acc:0.981]
Epoch [79/120    avg_loss:0.105, val_acc:0.973]
Epoch [80/120    avg_loss:0.090, val_acc:0.981]
Epoch [81/120    avg_loss:0.103, val_acc:0.973]
Epoch [82/120    avg_loss:0.100, val_acc:0.969]
Epoch [83/120    avg_loss:0.134, val_acc:0.969]
Epoch [84/120    avg_loss:0.116, val_acc:0.977]
Epoch [85/120    avg_loss:0.204, val_acc:0.971]
Epoch [86/120    avg_loss:0.202, val_acc:0.971]
Epoch [87/120    avg_loss:0.124, val_acc:0.977]
Epoch [88/120    avg_loss:0.095, val_acc:0.977]
Epoch [89/120    avg_loss:0.135, val_acc:0.967]
Epoch [90/120    avg_loss:0.096, val_acc:0.954]
Epoch [91/120    avg_loss:0.170, val_acc:0.963]
Epoch [92/120    avg_loss:0.193, val_acc:0.973]
Epoch [93/120    avg_loss:0.101, val_acc:0.981]
Epoch [94/120    avg_loss:0.099, val_acc:0.977]
Epoch [95/120    avg_loss:0.078, val_acc:0.975]
Epoch [96/120    avg_loss:0.090, val_acc:0.977]
Epoch [97/120    avg_loss:0.103, val_acc:0.929]
Epoch [98/120    avg_loss:0.120, val_acc:0.956]
Epoch [99/120    avg_loss:0.092, val_acc:0.973]
Epoch [100/120    avg_loss:0.077, val_acc:0.975]
Epoch [101/120    avg_loss:0.078, val_acc:0.979]
Epoch [102/120    avg_loss:0.080, val_acc:0.979]
Epoch [103/120    avg_loss:0.053, val_acc:0.983]
Epoch [104/120    avg_loss:0.058, val_acc:0.967]
Epoch [105/120    avg_loss:0.080, val_acc:0.990]
Epoch [106/120    avg_loss:0.049, val_acc:0.979]
Epoch [107/120    avg_loss:0.074, val_acc:0.973]
Epoch [108/120    avg_loss:0.079, val_acc:0.975]
Epoch [109/120    avg_loss:0.072, val_acc:0.988]
Epoch [110/120    avg_loss:0.057, val_acc:0.985]
Epoch [111/120    avg_loss:0.045, val_acc:0.988]
Epoch [112/120    avg_loss:0.061, val_acc:0.975]
Epoch [113/120    avg_loss:0.066, val_acc:0.977]
Epoch [114/120    avg_loss:0.077, val_acc:0.985]
Epoch [115/120    avg_loss:0.077, val_acc:0.977]
Epoch [116/120    avg_loss:0.147, val_acc:0.960]
Epoch [117/120    avg_loss:0.170, val_acc:0.965]
Epoch [118/120    avg_loss:0.120, val_acc:0.981]
Epoch [119/120    avg_loss:0.059, val_acc:0.983]
Epoch [120/120    avg_loss:0.056, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 216   0   0   0   0   3   0   0   0   0   0   0]
 [  0   0   1 217   4   2   0   0   6   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   3   0   0   3   2 198   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.52878464818762

F1 scores:
[       nan 0.997815   0.96213808 0.97091723 0.92372881 0.88339223
 0.98019802 0.91011236 0.99232737 1.         1.         0.99603699
 0.99667774 1.        ]

Kappa:
0.9836173155573537
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3973a3d8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.602, val_acc:0.256]
Epoch [2/120    avg_loss:2.409, val_acc:0.329]
Epoch [3/120    avg_loss:2.268, val_acc:0.367]
Epoch [4/120    avg_loss:2.186, val_acc:0.465]
Epoch [5/120    avg_loss:2.114, val_acc:0.487]
Epoch [6/120    avg_loss:2.011, val_acc:0.550]
Epoch [7/120    avg_loss:1.929, val_acc:0.569]
Epoch [8/120    avg_loss:1.840, val_acc:0.581]
Epoch [9/120    avg_loss:1.737, val_acc:0.588]
Epoch [10/120    avg_loss:1.649, val_acc:0.617]
Epoch [11/120    avg_loss:1.558, val_acc:0.648]
Epoch [12/120    avg_loss:1.469, val_acc:0.694]
Epoch [13/120    avg_loss:1.375, val_acc:0.696]
Epoch [14/120    avg_loss:1.292, val_acc:0.710]
Epoch [15/120    avg_loss:1.210, val_acc:0.748]
Epoch [16/120    avg_loss:1.142, val_acc:0.762]
Epoch [17/120    avg_loss:1.067, val_acc:0.798]
Epoch [18/120    avg_loss:1.010, val_acc:0.760]
Epoch [19/120    avg_loss:0.950, val_acc:0.850]
Epoch [20/120    avg_loss:0.897, val_acc:0.833]
Epoch [21/120    avg_loss:0.846, val_acc:0.854]
Epoch [22/120    avg_loss:0.798, val_acc:0.875]
Epoch [23/120    avg_loss:0.878, val_acc:0.852]
Epoch [24/120    avg_loss:0.769, val_acc:0.877]
Epoch [25/120    avg_loss:0.682, val_acc:0.879]
Epoch [26/120    avg_loss:0.613, val_acc:0.896]
Epoch [27/120    avg_loss:0.612, val_acc:0.898]
Epoch [28/120    avg_loss:0.590, val_acc:0.912]
Epoch [29/120    avg_loss:0.556, val_acc:0.917]
Epoch [30/120    avg_loss:0.494, val_acc:0.896]
Epoch [31/120    avg_loss:0.477, val_acc:0.885]
Epoch [32/120    avg_loss:0.477, val_acc:0.904]
Epoch [33/120    avg_loss:0.490, val_acc:0.912]
Epoch [34/120    avg_loss:0.442, val_acc:0.869]
Epoch [35/120    avg_loss:0.425, val_acc:0.923]
Epoch [36/120    avg_loss:0.441, val_acc:0.892]
Epoch [37/120    avg_loss:0.458, val_acc:0.912]
Epoch [38/120    avg_loss:0.489, val_acc:0.912]
Epoch [39/120    avg_loss:0.395, val_acc:0.910]
Epoch [40/120    avg_loss:0.455, val_acc:0.925]
Epoch [41/120    avg_loss:0.445, val_acc:0.902]
Epoch [42/120    avg_loss:0.394, val_acc:0.923]
Epoch [43/120    avg_loss:0.360, val_acc:0.929]
Epoch [44/120    avg_loss:0.398, val_acc:0.923]
Epoch [45/120    avg_loss:0.378, val_acc:0.912]
Epoch [46/120    avg_loss:0.370, val_acc:0.923]
Epoch [47/120    avg_loss:0.335, val_acc:0.925]
Epoch [48/120    avg_loss:0.344, val_acc:0.923]
Epoch [49/120    avg_loss:0.323, val_acc:0.950]
Epoch [50/120    avg_loss:0.300, val_acc:0.946]
Epoch [51/120    avg_loss:0.324, val_acc:0.900]
Epoch [52/120    avg_loss:0.294, val_acc:0.944]
Epoch [53/120    avg_loss:0.265, val_acc:0.950]
Epoch [54/120    avg_loss:0.269, val_acc:0.950]
Epoch [55/120    avg_loss:0.237, val_acc:0.952]
Epoch [56/120    avg_loss:0.214, val_acc:0.954]
Epoch [57/120    avg_loss:0.233, val_acc:0.956]
Epoch [58/120    avg_loss:0.222, val_acc:0.940]
Epoch [59/120    avg_loss:0.232, val_acc:0.948]
Epoch [60/120    avg_loss:0.220, val_acc:0.954]
Epoch [61/120    avg_loss:0.222, val_acc:0.952]
Epoch [62/120    avg_loss:0.215, val_acc:0.944]
Epoch [63/120    avg_loss:0.265, val_acc:0.946]
Epoch [64/120    avg_loss:0.261, val_acc:0.952]
Epoch [65/120    avg_loss:0.200, val_acc:0.950]
Epoch [66/120    avg_loss:0.200, val_acc:0.973]
Epoch [67/120    avg_loss:0.218, val_acc:0.946]
Epoch [68/120    avg_loss:0.173, val_acc:0.963]
Epoch [69/120    avg_loss:0.182, val_acc:0.946]
Epoch [70/120    avg_loss:0.220, val_acc:0.965]
Epoch [71/120    avg_loss:0.215, val_acc:0.952]
Epoch [72/120    avg_loss:0.181, val_acc:0.910]
Epoch [73/120    avg_loss:0.231, val_acc:0.958]
Epoch [74/120    avg_loss:0.200, val_acc:0.954]
Epoch [75/120    avg_loss:0.217, val_acc:0.900]
Epoch [76/120    avg_loss:0.261, val_acc:0.950]
Epoch [77/120    avg_loss:0.220, val_acc:0.940]
Epoch [78/120    avg_loss:0.169, val_acc:0.973]
Epoch [79/120    avg_loss:0.170, val_acc:0.963]
Epoch [80/120    avg_loss:0.143, val_acc:0.960]
Epoch [81/120    avg_loss:0.125, val_acc:0.960]
Epoch [82/120    avg_loss:0.144, val_acc:0.975]
Epoch [83/120    avg_loss:0.137, val_acc:0.971]
Epoch [84/120    avg_loss:0.124, val_acc:0.971]
Epoch [85/120    avg_loss:0.109, val_acc:0.973]
Epoch [86/120    avg_loss:0.113, val_acc:0.973]
Epoch [87/120    avg_loss:0.100, val_acc:0.971]
Epoch [88/120    avg_loss:0.122, val_acc:0.973]
Epoch [89/120    avg_loss:0.116, val_acc:0.967]
Epoch [90/120    avg_loss:0.105, val_acc:0.971]
Epoch [91/120    avg_loss:0.109, val_acc:0.967]
Epoch [92/120    avg_loss:0.096, val_acc:0.967]
Epoch [93/120    avg_loss:0.084, val_acc:0.971]
Epoch [94/120    avg_loss:0.090, val_acc:0.946]
Epoch [95/120    avg_loss:0.135, val_acc:0.977]
Epoch [96/120    avg_loss:0.088, val_acc:0.969]
Epoch [97/120    avg_loss:0.125, val_acc:0.981]
Epoch [98/120    avg_loss:0.138, val_acc:0.967]
Epoch [99/120    avg_loss:0.112, val_acc:0.967]
Epoch [100/120    avg_loss:0.110, val_acc:0.954]
Epoch [101/120    avg_loss:0.103, val_acc:0.975]
Epoch [102/120    avg_loss:0.074, val_acc:0.975]
Epoch [103/120    avg_loss:0.090, val_acc:0.975]
Epoch [104/120    avg_loss:0.104, val_acc:0.971]
Epoch [105/120    avg_loss:0.142, val_acc:0.965]
Epoch [106/120    avg_loss:0.085, val_acc:0.938]
Epoch [107/120    avg_loss:0.232, val_acc:0.925]
Epoch [108/120    avg_loss:0.207, val_acc:0.963]
Epoch [109/120    avg_loss:0.131, val_acc:0.971]
Epoch [110/120    avg_loss:0.137, val_acc:0.975]
Epoch [111/120    avg_loss:0.082, val_acc:0.977]
Epoch [112/120    avg_loss:0.069, val_acc:0.981]
Epoch [113/120    avg_loss:0.087, val_acc:0.983]
Epoch [114/120    avg_loss:0.057, val_acc:0.985]
Epoch [115/120    avg_loss:0.056, val_acc:0.985]
Epoch [116/120    avg_loss:0.059, val_acc:0.985]
Epoch [117/120    avg_loss:0.053, val_acc:0.983]
Epoch [118/120    avg_loss:0.049, val_acc:0.983]
Epoch [119/120    avg_loss:0.053, val_acc:0.985]
Epoch [120/120    avg_loss:0.052, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 220   6   0   0   0   3   0   0   0   0   0]
 [  0   0   0   0 202  25   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   1   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 1.         0.97117517 0.97777778 0.91196388 0.8961039
 0.99756691 0.93181818 0.99487179 1.         1.         1.
 0.99889503 1.        ]

Kappa:
0.9867063525706712
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb2c84d7940>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.574, val_acc:0.287]
Epoch [2/120    avg_loss:2.417, val_acc:0.298]
Epoch [3/120    avg_loss:2.287, val_acc:0.508]
Epoch [4/120    avg_loss:2.152, val_acc:0.544]
Epoch [5/120    avg_loss:2.017, val_acc:0.552]
Epoch [6/120    avg_loss:1.881, val_acc:0.577]
Epoch [7/120    avg_loss:1.717, val_acc:0.592]
Epoch [8/120    avg_loss:1.607, val_acc:0.588]
Epoch [9/120    avg_loss:1.466, val_acc:0.650]
Epoch [10/120    avg_loss:1.358, val_acc:0.667]
Epoch [11/120    avg_loss:1.239, val_acc:0.679]
Epoch [12/120    avg_loss:1.162, val_acc:0.731]
Epoch [13/120    avg_loss:1.049, val_acc:0.792]
Epoch [14/120    avg_loss:1.001, val_acc:0.831]
Epoch [15/120    avg_loss:0.930, val_acc:0.858]
Epoch [16/120    avg_loss:0.862, val_acc:0.823]
Epoch [17/120    avg_loss:0.808, val_acc:0.871]
Epoch [18/120    avg_loss:0.737, val_acc:0.879]
Epoch [19/120    avg_loss:0.700, val_acc:0.902]
Epoch [20/120    avg_loss:0.698, val_acc:0.885]
Epoch [21/120    avg_loss:0.641, val_acc:0.887]
Epoch [22/120    avg_loss:0.585, val_acc:0.894]
Epoch [23/120    avg_loss:0.549, val_acc:0.917]
Epoch [24/120    avg_loss:0.518, val_acc:0.902]
Epoch [25/120    avg_loss:0.498, val_acc:0.917]
Epoch [26/120    avg_loss:0.553, val_acc:0.921]
Epoch [27/120    avg_loss:0.586, val_acc:0.917]
Epoch [28/120    avg_loss:0.510, val_acc:0.917]
Epoch [29/120    avg_loss:0.503, val_acc:0.910]
Epoch [30/120    avg_loss:0.470, val_acc:0.921]
Epoch [31/120    avg_loss:0.394, val_acc:0.935]
Epoch [32/120    avg_loss:0.397, val_acc:0.942]
Epoch [33/120    avg_loss:0.372, val_acc:0.927]
Epoch [34/120    avg_loss:0.338, val_acc:0.931]
Epoch [35/120    avg_loss:0.368, val_acc:0.942]
Epoch [36/120    avg_loss:0.361, val_acc:0.935]
Epoch [37/120    avg_loss:0.301, val_acc:0.938]
Epoch [38/120    avg_loss:0.264, val_acc:0.931]
Epoch [39/120    avg_loss:0.274, val_acc:0.948]
Epoch [40/120    avg_loss:0.282, val_acc:0.956]
Epoch [41/120    avg_loss:0.307, val_acc:0.894]
Epoch [42/120    avg_loss:0.287, val_acc:0.917]
Epoch [43/120    avg_loss:0.246, val_acc:0.950]
Epoch [44/120    avg_loss:0.264, val_acc:0.935]
Epoch [45/120    avg_loss:0.264, val_acc:0.946]
Epoch [46/120    avg_loss:0.284, val_acc:0.927]
Epoch [47/120    avg_loss:0.307, val_acc:0.933]
Epoch [48/120    avg_loss:0.244, val_acc:0.960]
Epoch [49/120    avg_loss:0.229, val_acc:0.923]
Epoch [50/120    avg_loss:0.219, val_acc:0.963]
Epoch [51/120    avg_loss:0.249, val_acc:0.944]
Epoch [52/120    avg_loss:0.219, val_acc:0.946]
Epoch [53/120    avg_loss:0.242, val_acc:0.931]
Epoch [54/120    avg_loss:0.226, val_acc:0.960]
Epoch [55/120    avg_loss:0.198, val_acc:0.954]
Epoch [56/120    avg_loss:0.181, val_acc:0.958]
Epoch [57/120    avg_loss:0.193, val_acc:0.954]
Epoch [58/120    avg_loss:0.219, val_acc:0.935]
Epoch [59/120    avg_loss:0.290, val_acc:0.919]
Epoch [60/120    avg_loss:0.198, val_acc:0.942]
Epoch [61/120    avg_loss:0.206, val_acc:0.935]
Epoch [62/120    avg_loss:0.178, val_acc:0.925]
Epoch [63/120    avg_loss:0.207, val_acc:0.958]
Epoch [64/120    avg_loss:0.150, val_acc:0.967]
Epoch [65/120    avg_loss:0.114, val_acc:0.973]
Epoch [66/120    avg_loss:0.124, val_acc:0.971]
Epoch [67/120    avg_loss:0.137, val_acc:0.975]
Epoch [68/120    avg_loss:0.117, val_acc:0.969]
Epoch [69/120    avg_loss:0.110, val_acc:0.971]
Epoch [70/120    avg_loss:0.119, val_acc:0.973]
Epoch [71/120    avg_loss:0.123, val_acc:0.973]
Epoch [72/120    avg_loss:0.111, val_acc:0.971]
Epoch [73/120    avg_loss:0.109, val_acc:0.969]
Epoch [74/120    avg_loss:0.112, val_acc:0.971]
Epoch [75/120    avg_loss:0.109, val_acc:0.973]
Epoch [76/120    avg_loss:0.123, val_acc:0.975]
Epoch [77/120    avg_loss:0.107, val_acc:0.975]
Epoch [78/120    avg_loss:0.113, val_acc:0.971]
Epoch [79/120    avg_loss:0.109, val_acc:0.971]
Epoch [80/120    avg_loss:0.115, val_acc:0.973]
Epoch [81/120    avg_loss:0.108, val_acc:0.975]
Epoch [82/120    avg_loss:0.113, val_acc:0.975]
Epoch [83/120    avg_loss:0.107, val_acc:0.971]
Epoch [84/120    avg_loss:0.124, val_acc:0.971]
Epoch [85/120    avg_loss:0.102, val_acc:0.973]
Epoch [86/120    avg_loss:0.107, val_acc:0.971]
Epoch [87/120    avg_loss:0.101, val_acc:0.973]
Epoch [88/120    avg_loss:0.108, val_acc:0.973]
Epoch [89/120    avg_loss:0.105, val_acc:0.971]
Epoch [90/120    avg_loss:0.098, val_acc:0.975]
Epoch [91/120    avg_loss:0.105, val_acc:0.973]
Epoch [92/120    avg_loss:0.099, val_acc:0.973]
Epoch [93/120    avg_loss:0.087, val_acc:0.975]
Epoch [94/120    avg_loss:0.102, val_acc:0.975]
Epoch [95/120    avg_loss:0.101, val_acc:0.975]
Epoch [96/120    avg_loss:0.097, val_acc:0.977]
Epoch [97/120    avg_loss:0.104, val_acc:0.975]
Epoch [98/120    avg_loss:0.108, val_acc:0.975]
Epoch [99/120    avg_loss:0.077, val_acc:0.971]
Epoch [100/120    avg_loss:0.099, val_acc:0.973]
Epoch [101/120    avg_loss:0.088, val_acc:0.973]
Epoch [102/120    avg_loss:0.106, val_acc:0.977]
Epoch [103/120    avg_loss:0.104, val_acc:0.973]
Epoch [104/120    avg_loss:0.102, val_acc:0.975]
Epoch [105/120    avg_loss:0.092, val_acc:0.973]
Epoch [106/120    avg_loss:0.091, val_acc:0.975]
Epoch [107/120    avg_loss:0.106, val_acc:0.975]
Epoch [108/120    avg_loss:0.095, val_acc:0.975]
Epoch [109/120    avg_loss:0.113, val_acc:0.977]
Epoch [110/120    avg_loss:0.091, val_acc:0.977]
Epoch [111/120    avg_loss:0.115, val_acc:0.977]
Epoch [112/120    avg_loss:0.079, val_acc:0.977]
Epoch [113/120    avg_loss:0.085, val_acc:0.975]
Epoch [114/120    avg_loss:0.084, val_acc:0.973]
Epoch [115/120    avg_loss:0.100, val_acc:0.977]
Epoch [116/120    avg_loss:0.090, val_acc:0.973]
Epoch [117/120    avg_loss:0.099, val_acc:0.973]
Epoch [118/120    avg_loss:0.087, val_acc:0.973]
Epoch [119/120    avg_loss:0.082, val_acc:0.975]
Epoch [120/120    avg_loss:0.087, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0  11 213   2   0   0   0   1   3   0   0   0   0]
 [  0   0   0   0 201  26   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7   0 199   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.37953091684435

F1 scores:
[       nan 1.         0.94600432 0.96162528 0.89932886 0.88235294
 0.98271605 0.91954023 0.998713   0.99680511 1.         0.9973545
 0.99778761 1.        ]

Kappa:
0.9819579337005186
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f46c913c940>
supervision:full
center_pixel:True
Network :
Number of parameter: 41112==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.631, val_acc:0.196]
Epoch [2/120    avg_loss:2.432, val_acc:0.454]
Epoch [3/120    avg_loss:2.279, val_acc:0.458]
Epoch [4/120    avg_loss:2.142, val_acc:0.475]
Epoch [5/120    avg_loss:2.025, val_acc:0.479]
Epoch [6/120    avg_loss:1.910, val_acc:0.579]
Epoch [7/120    avg_loss:1.799, val_acc:0.577]
Epoch [8/120    avg_loss:1.693, val_acc:0.646]
Epoch [9/120    avg_loss:1.582, val_acc:0.698]
Epoch [10/120    avg_loss:1.483, val_acc:0.677]
Epoch [11/120    avg_loss:1.401, val_acc:0.706]
Epoch [12/120    avg_loss:1.328, val_acc:0.723]
Epoch [13/120    avg_loss:1.253, val_acc:0.710]
Epoch [14/120    avg_loss:1.176, val_acc:0.735]
Epoch [15/120    avg_loss:1.133, val_acc:0.800]
Epoch [16/120    avg_loss:1.023, val_acc:0.869]
Epoch [17/120    avg_loss:0.940, val_acc:0.850]
Epoch [18/120    avg_loss:0.888, val_acc:0.846]
Epoch [19/120    avg_loss:0.865, val_acc:0.804]
Epoch [20/120    avg_loss:0.834, val_acc:0.840]
Epoch [21/120    avg_loss:0.763, val_acc:0.902]
Epoch [22/120    avg_loss:0.679, val_acc:0.908]
Epoch [23/120    avg_loss:0.626, val_acc:0.912]
Epoch [24/120    avg_loss:0.614, val_acc:0.902]
Epoch [25/120    avg_loss:0.603, val_acc:0.921]
Epoch [26/120    avg_loss:0.557, val_acc:0.881]
Epoch [27/120    avg_loss:0.534, val_acc:0.894]
Epoch [28/120    avg_loss:0.576, val_acc:0.881]
Epoch [29/120    avg_loss:0.529, val_acc:0.894]
Epoch [30/120    avg_loss:0.499, val_acc:0.933]
Epoch [31/120    avg_loss:0.438, val_acc:0.927]
Epoch [32/120    avg_loss:0.429, val_acc:0.933]
Epoch [33/120    avg_loss:0.368, val_acc:0.952]
Epoch [34/120    avg_loss:0.363, val_acc:0.933]
Epoch [35/120    avg_loss:0.383, val_acc:0.942]
Epoch [36/120    avg_loss:0.357, val_acc:0.935]
Epoch [37/120    avg_loss:0.317, val_acc:0.942]
Epoch [38/120    avg_loss:0.351, val_acc:0.946]
Epoch [39/120    avg_loss:0.346, val_acc:0.906]
Epoch [40/120    avg_loss:0.350, val_acc:0.919]
Epoch [41/120    avg_loss:0.295, val_acc:0.946]
Epoch [42/120    avg_loss:0.268, val_acc:0.919]
Epoch [43/120    avg_loss:0.298, val_acc:0.956]
Epoch [44/120    avg_loss:0.272, val_acc:0.942]
Epoch [45/120    avg_loss:0.293, val_acc:0.960]
Epoch [46/120    avg_loss:0.250, val_acc:0.948]
Epoch [47/120    avg_loss:0.271, val_acc:0.948]
Epoch [48/120    avg_loss:0.272, val_acc:0.948]
Epoch [49/120    avg_loss:0.236, val_acc:0.960]
Epoch [50/120    avg_loss:0.291, val_acc:0.948]
Epoch [51/120    avg_loss:0.251, val_acc:0.952]
Epoch [52/120    avg_loss:0.281, val_acc:0.946]
Epoch [53/120    avg_loss:0.232, val_acc:0.952]
Epoch [54/120    avg_loss:0.209, val_acc:0.969]
Epoch [55/120    avg_loss:0.204, val_acc:0.956]
Epoch [56/120    avg_loss:0.216, val_acc:0.933]
Epoch [57/120    avg_loss:0.202, val_acc:0.965]
Epoch [58/120    avg_loss:0.194, val_acc:0.954]
Epoch [59/120    avg_loss:0.262, val_acc:0.963]
Epoch [60/120    avg_loss:0.214, val_acc:0.908]
Epoch [61/120    avg_loss:0.220, val_acc:0.965]
Epoch [62/120    avg_loss:0.174, val_acc:0.967]
Epoch [63/120    avg_loss:0.176, val_acc:0.950]
Epoch [64/120    avg_loss:0.145, val_acc:0.973]
Epoch [65/120    avg_loss:0.141, val_acc:0.973]
Epoch [66/120    avg_loss:0.124, val_acc:0.971]
Epoch [67/120    avg_loss:0.129, val_acc:0.960]
Epoch [68/120    avg_loss:0.134, val_acc:0.971]
Epoch [69/120    avg_loss:0.158, val_acc:0.950]
Epoch [70/120    avg_loss:0.150, val_acc:0.967]
Epoch [71/120    avg_loss:0.113, val_acc:0.971]
Epoch [72/120    avg_loss:0.111, val_acc:0.967]
Epoch [73/120    avg_loss:0.136, val_acc:0.963]
Epoch [74/120    avg_loss:0.109, val_acc:0.952]
Epoch [75/120    avg_loss:0.125, val_acc:0.971]
Epoch [76/120    avg_loss:0.124, val_acc:0.967]
Epoch [77/120    avg_loss:0.131, val_acc:0.969]
Epoch [78/120    avg_loss:0.128, val_acc:0.971]
Epoch [79/120    avg_loss:0.102, val_acc:0.975]
Epoch [80/120    avg_loss:0.067, val_acc:0.977]
Epoch [81/120    avg_loss:0.081, val_acc:0.981]
Epoch [82/120    avg_loss:0.075, val_acc:0.979]
Epoch [83/120    avg_loss:0.069, val_acc:0.977]
Epoch [84/120    avg_loss:0.063, val_acc:0.977]
Epoch [85/120    avg_loss:0.064, val_acc:0.977]
Epoch [86/120    avg_loss:0.073, val_acc:0.981]
Epoch [87/120    avg_loss:0.069, val_acc:0.981]
Epoch [88/120    avg_loss:0.057, val_acc:0.981]
Epoch [89/120    avg_loss:0.060, val_acc:0.983]
Epoch [90/120    avg_loss:0.056, val_acc:0.981]
Epoch [91/120    avg_loss:0.063, val_acc:0.983]
Epoch [92/120    avg_loss:0.059, val_acc:0.981]
Epoch [93/120    avg_loss:0.058, val_acc:0.983]
Epoch [94/120    avg_loss:0.066, val_acc:0.981]
Epoch [95/120    avg_loss:0.065, val_acc:0.981]
Epoch [96/120    avg_loss:0.057, val_acc:0.983]
Epoch [97/120    avg_loss:0.062, val_acc:0.983]
Epoch [98/120    avg_loss:0.049, val_acc:0.983]
Epoch [99/120    avg_loss:0.055, val_acc:0.983]
Epoch [100/120    avg_loss:0.060, val_acc:0.983]
Epoch [101/120    avg_loss:0.064, val_acc:0.985]
Epoch [102/120    avg_loss:0.065, val_acc:0.985]
Epoch [103/120    avg_loss:0.052, val_acc:0.983]
Epoch [104/120    avg_loss:0.054, val_acc:0.985]
Epoch [105/120    avg_loss:0.052, val_acc:0.985]
Epoch [106/120    avg_loss:0.054, val_acc:0.983]
Epoch [107/120    avg_loss:0.060, val_acc:0.985]
Epoch [108/120    avg_loss:0.051, val_acc:0.985]
Epoch [109/120    avg_loss:0.055, val_acc:0.988]
Epoch [110/120    avg_loss:0.052, val_acc:0.985]
Epoch [111/120    avg_loss:0.051, val_acc:0.985]
Epoch [112/120    avg_loss:0.051, val_acc:0.985]
Epoch [113/120    avg_loss:0.055, val_acc:0.985]
Epoch [114/120    avg_loss:0.047, val_acc:0.985]
Epoch [115/120    avg_loss:0.046, val_acc:0.983]
Epoch [116/120    avg_loss:0.051, val_acc:0.985]
Epoch [117/120    avg_loss:0.053, val_acc:0.985]
Epoch [118/120    avg_loss:0.043, val_acc:0.983]
Epoch [119/120    avg_loss:0.049, val_acc:0.985]
Epoch [120/120    avg_loss:0.048, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  18   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.91257995735607

F1 scores:
[       nan 1.         0.97333333 0.98678414 0.91466083 0.89115646
 0.99756691 0.93181818 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9878928257691123
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3180aec898>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.604, val_acc:0.210]
Epoch [2/120    avg_loss:2.386, val_acc:0.350]
Epoch [3/120    avg_loss:2.274, val_acc:0.398]
Epoch [4/120    avg_loss:2.181, val_acc:0.456]
Epoch [5/120    avg_loss:2.073, val_acc:0.540]
Epoch [6/120    avg_loss:1.977, val_acc:0.550]
Epoch [7/120    avg_loss:1.855, val_acc:0.575]
Epoch [8/120    avg_loss:1.755, val_acc:0.604]
Epoch [9/120    avg_loss:1.642, val_acc:0.600]
Epoch [10/120    avg_loss:1.540, val_acc:0.610]
Epoch [11/120    avg_loss:1.447, val_acc:0.652]
Epoch [12/120    avg_loss:1.329, val_acc:0.662]
Epoch [13/120    avg_loss:1.247, val_acc:0.702]
Epoch [14/120    avg_loss:1.159, val_acc:0.729]
Epoch [15/120    avg_loss:1.091, val_acc:0.754]
Epoch [16/120    avg_loss:1.015, val_acc:0.756]
Epoch [17/120    avg_loss:0.975, val_acc:0.881]
Epoch [18/120    avg_loss:0.884, val_acc:0.858]
Epoch [19/120    avg_loss:0.799, val_acc:0.873]
Epoch [20/120    avg_loss:0.740, val_acc:0.917]
Epoch [21/120    avg_loss:0.697, val_acc:0.883]
Epoch [22/120    avg_loss:0.660, val_acc:0.912]
Epoch [23/120    avg_loss:0.656, val_acc:0.925]
Epoch [24/120    avg_loss:0.602, val_acc:0.908]
Epoch [25/120    avg_loss:0.584, val_acc:0.912]
Epoch [26/120    avg_loss:0.503, val_acc:0.935]
Epoch [27/120    avg_loss:0.481, val_acc:0.915]
Epoch [28/120    avg_loss:0.470, val_acc:0.933]
Epoch [29/120    avg_loss:0.464, val_acc:0.929]
Epoch [30/120    avg_loss:0.430, val_acc:0.933]
Epoch [31/120    avg_loss:0.456, val_acc:0.919]
Epoch [32/120    avg_loss:0.480, val_acc:0.908]
Epoch [33/120    avg_loss:0.437, val_acc:0.921]
Epoch [34/120    avg_loss:0.404, val_acc:0.915]
Epoch [35/120    avg_loss:0.393, val_acc:0.942]
Epoch [36/120    avg_loss:0.346, val_acc:0.944]
Epoch [37/120    avg_loss:0.378, val_acc:0.927]
Epoch [38/120    avg_loss:0.352, val_acc:0.946]
Epoch [39/120    avg_loss:0.280, val_acc:0.923]
Epoch [40/120    avg_loss:0.326, val_acc:0.952]
Epoch [41/120    avg_loss:0.334, val_acc:0.938]
Epoch [42/120    avg_loss:0.324, val_acc:0.892]
Epoch [43/120    avg_loss:0.305, val_acc:0.960]
Epoch [44/120    avg_loss:0.251, val_acc:0.950]
Epoch [45/120    avg_loss:0.280, val_acc:0.958]
Epoch [46/120    avg_loss:0.261, val_acc:0.950]
Epoch [47/120    avg_loss:0.240, val_acc:0.940]
Epoch [48/120    avg_loss:0.257, val_acc:0.952]
Epoch [49/120    avg_loss:0.242, val_acc:0.952]
Epoch [50/120    avg_loss:0.212, val_acc:0.956]
Epoch [51/120    avg_loss:0.215, val_acc:0.967]
Epoch [52/120    avg_loss:0.224, val_acc:0.954]
Epoch [53/120    avg_loss:0.279, val_acc:0.944]
Epoch [54/120    avg_loss:0.238, val_acc:0.967]
Epoch [55/120    avg_loss:0.202, val_acc:0.960]
Epoch [56/120    avg_loss:0.185, val_acc:0.975]
Epoch [57/120    avg_loss:0.189, val_acc:0.971]
Epoch [58/120    avg_loss:0.161, val_acc:0.967]
Epoch [59/120    avg_loss:0.279, val_acc:0.933]
Epoch [60/120    avg_loss:0.225, val_acc:0.954]
Epoch [61/120    avg_loss:0.201, val_acc:0.944]
Epoch [62/120    avg_loss:0.189, val_acc:0.971]
Epoch [63/120    avg_loss:0.141, val_acc:0.977]
Epoch [64/120    avg_loss:0.144, val_acc:0.977]
Epoch [65/120    avg_loss:0.178, val_acc:0.973]
Epoch [66/120    avg_loss:0.136, val_acc:0.973]
Epoch [67/120    avg_loss:0.148, val_acc:0.967]
Epoch [68/120    avg_loss:0.150, val_acc:0.963]
Epoch [69/120    avg_loss:0.162, val_acc:0.979]
Epoch [70/120    avg_loss:0.127, val_acc:0.983]
Epoch [71/120    avg_loss:0.116, val_acc:0.969]
Epoch [72/120    avg_loss:0.133, val_acc:0.988]
Epoch [73/120    avg_loss:0.115, val_acc:0.971]
Epoch [74/120    avg_loss:0.125, val_acc:0.985]
Epoch [75/120    avg_loss:0.107, val_acc:0.983]
Epoch [76/120    avg_loss:0.109, val_acc:0.977]
Epoch [77/120    avg_loss:0.096, val_acc:0.977]
Epoch [78/120    avg_loss:0.112, val_acc:0.983]
Epoch [79/120    avg_loss:0.122, val_acc:0.988]
Epoch [80/120    avg_loss:0.118, val_acc:0.973]
Epoch [81/120    avg_loss:0.105, val_acc:0.985]
Epoch [82/120    avg_loss:0.081, val_acc:0.983]
Epoch [83/120    avg_loss:0.115, val_acc:0.973]
Epoch [84/120    avg_loss:0.120, val_acc:0.958]
Epoch [85/120    avg_loss:0.133, val_acc:0.979]
Epoch [86/120    avg_loss:0.114, val_acc:0.958]
Epoch [87/120    avg_loss:0.195, val_acc:0.975]
Epoch [88/120    avg_loss:0.120, val_acc:0.979]
Epoch [89/120    avg_loss:0.110, val_acc:0.988]
Epoch [90/120    avg_loss:0.085, val_acc:0.985]
Epoch [91/120    avg_loss:0.082, val_acc:0.988]
Epoch [92/120    avg_loss:0.065, val_acc:0.990]
Epoch [93/120    avg_loss:0.066, val_acc:0.990]
Epoch [94/120    avg_loss:0.075, val_acc:0.965]
Epoch [95/120    avg_loss:0.111, val_acc:0.973]
Epoch [96/120    avg_loss:0.072, val_acc:0.992]
Epoch [97/120    avg_loss:0.065, val_acc:0.975]
Epoch [98/120    avg_loss:0.070, val_acc:0.985]
Epoch [99/120    avg_loss:0.060, val_acc:0.988]
Epoch [100/120    avg_loss:0.058, val_acc:0.994]
Epoch [101/120    avg_loss:0.069, val_acc:0.977]
Epoch [102/120    avg_loss:0.112, val_acc:0.973]
Epoch [103/120    avg_loss:0.097, val_acc:0.988]
Epoch [104/120    avg_loss:0.064, val_acc:0.992]
Epoch [105/120    avg_loss:0.069, val_acc:0.985]
Epoch [106/120    avg_loss:0.065, val_acc:0.983]
Epoch [107/120    avg_loss:0.043, val_acc:0.992]
Epoch [108/120    avg_loss:0.050, val_acc:0.994]
Epoch [109/120    avg_loss:0.054, val_acc:0.990]
Epoch [110/120    avg_loss:0.046, val_acc:0.988]
Epoch [111/120    avg_loss:0.044, val_acc:0.992]
Epoch [112/120    avg_loss:0.058, val_acc:0.988]
Epoch [113/120    avg_loss:0.056, val_acc:0.979]
Epoch [114/120    avg_loss:0.044, val_acc:0.985]
Epoch [115/120    avg_loss:0.043, val_acc:0.992]
Epoch [116/120    avg_loss:0.040, val_acc:0.992]
Epoch [117/120    avg_loss:0.030, val_acc:0.990]
Epoch [118/120    avg_loss:0.035, val_acc:0.990]
Epoch [119/120    avg_loss:0.028, val_acc:0.990]
Epoch [120/120    avg_loss:0.041, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.4456289978678

F1 scores:
[       nan 1.         0.9977221  0.98901099 0.94945055 0.94949495
 0.99266504 0.99465241 1.         1.         1.         0.9973545
 0.99778761 1.        ]

Kappa:
0.9938282549211114
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8d60546940>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.585, val_acc:0.375]
Epoch [2/120    avg_loss:2.416, val_acc:0.354]
Epoch [3/120    avg_loss:2.308, val_acc:0.406]
Epoch [4/120    avg_loss:2.198, val_acc:0.438]
Epoch [5/120    avg_loss:2.116, val_acc:0.442]
Epoch [6/120    avg_loss:1.997, val_acc:0.550]
Epoch [7/120    avg_loss:1.888, val_acc:0.565]
Epoch [8/120    avg_loss:1.797, val_acc:0.602]
Epoch [9/120    avg_loss:1.679, val_acc:0.675]
Epoch [10/120    avg_loss:1.580, val_acc:0.698]
Epoch [11/120    avg_loss:1.485, val_acc:0.719]
Epoch [12/120    avg_loss:1.366, val_acc:0.727]
Epoch [13/120    avg_loss:1.251, val_acc:0.729]
Epoch [14/120    avg_loss:1.167, val_acc:0.744]
Epoch [15/120    avg_loss:1.072, val_acc:0.750]
Epoch [16/120    avg_loss:1.022, val_acc:0.760]
Epoch [17/120    avg_loss:0.944, val_acc:0.771]
Epoch [18/120    avg_loss:0.914, val_acc:0.777]
Epoch [19/120    avg_loss:0.868, val_acc:0.769]
Epoch [20/120    avg_loss:0.841, val_acc:0.838]
Epoch [21/120    avg_loss:0.745, val_acc:0.787]
Epoch [22/120    avg_loss:0.673, val_acc:0.892]
Epoch [23/120    avg_loss:0.694, val_acc:0.860]
Epoch [24/120    avg_loss:0.708, val_acc:0.842]
Epoch [25/120    avg_loss:0.638, val_acc:0.848]
Epoch [26/120    avg_loss:0.628, val_acc:0.885]
Epoch [27/120    avg_loss:0.559, val_acc:0.906]
Epoch [28/120    avg_loss:0.669, val_acc:0.831]
Epoch [29/120    avg_loss:0.563, val_acc:0.854]
Epoch [30/120    avg_loss:0.618, val_acc:0.925]
Epoch [31/120    avg_loss:0.626, val_acc:0.806]
Epoch [32/120    avg_loss:0.608, val_acc:0.890]
Epoch [33/120    avg_loss:0.563, val_acc:0.904]
Epoch [34/120    avg_loss:0.496, val_acc:0.915]
Epoch [35/120    avg_loss:0.484, val_acc:0.929]
Epoch [36/120    avg_loss:0.421, val_acc:0.927]
Epoch [37/120    avg_loss:0.418, val_acc:0.894]
Epoch [38/120    avg_loss:0.465, val_acc:0.894]
Epoch [39/120    avg_loss:0.417, val_acc:0.921]
Epoch [40/120    avg_loss:0.405, val_acc:0.896]
Epoch [41/120    avg_loss:0.346, val_acc:0.933]
Epoch [42/120    avg_loss:0.363, val_acc:0.910]
Epoch [43/120    avg_loss:0.386, val_acc:0.923]
Epoch [44/120    avg_loss:0.420, val_acc:0.915]
Epoch [45/120    avg_loss:0.390, val_acc:0.912]
Epoch [46/120    avg_loss:0.327, val_acc:0.917]
Epoch [47/120    avg_loss:0.358, val_acc:0.925]
Epoch [48/120    avg_loss:0.311, val_acc:0.956]
Epoch [49/120    avg_loss:0.277, val_acc:0.956]
Epoch [50/120    avg_loss:0.282, val_acc:0.917]
Epoch [51/120    avg_loss:0.334, val_acc:0.908]
Epoch [52/120    avg_loss:0.277, val_acc:0.940]
Epoch [53/120    avg_loss:0.292, val_acc:0.912]
Epoch [54/120    avg_loss:0.287, val_acc:0.908]
Epoch [55/120    avg_loss:0.278, val_acc:0.921]
Epoch [56/120    avg_loss:0.248, val_acc:0.946]
Epoch [57/120    avg_loss:0.234, val_acc:0.950]
Epoch [58/120    avg_loss:0.263, val_acc:0.931]
Epoch [59/120    avg_loss:0.244, val_acc:0.915]
Epoch [60/120    avg_loss:0.243, val_acc:0.942]
Epoch [61/120    avg_loss:0.235, val_acc:0.940]
Epoch [62/120    avg_loss:0.223, val_acc:0.954]
Epoch [63/120    avg_loss:0.223, val_acc:0.952]
Epoch [64/120    avg_loss:0.178, val_acc:0.942]
Epoch [65/120    avg_loss:0.171, val_acc:0.944]
Epoch [66/120    avg_loss:0.162, val_acc:0.948]
Epoch [67/120    avg_loss:0.162, val_acc:0.952]
Epoch [68/120    avg_loss:0.169, val_acc:0.956]
Epoch [69/120    avg_loss:0.157, val_acc:0.960]
Epoch [70/120    avg_loss:0.166, val_acc:0.950]
Epoch [71/120    avg_loss:0.153, val_acc:0.954]
Epoch [72/120    avg_loss:0.166, val_acc:0.950]
Epoch [73/120    avg_loss:0.205, val_acc:0.956]
Epoch [74/120    avg_loss:0.164, val_acc:0.960]
Epoch [75/120    avg_loss:0.145, val_acc:0.956]
Epoch [76/120    avg_loss:0.139, val_acc:0.946]
Epoch [77/120    avg_loss:0.148, val_acc:0.956]
Epoch [78/120    avg_loss:0.162, val_acc:0.958]
Epoch [79/120    avg_loss:0.156, val_acc:0.958]
Epoch [80/120    avg_loss:0.163, val_acc:0.958]
Epoch [81/120    avg_loss:0.140, val_acc:0.960]
Epoch [82/120    avg_loss:0.159, val_acc:0.958]
Epoch [83/120    avg_loss:0.135, val_acc:0.958]
Epoch [84/120    avg_loss:0.146, val_acc:0.960]
Epoch [85/120    avg_loss:0.166, val_acc:0.950]
Epoch [86/120    avg_loss:0.169, val_acc:0.952]
Epoch [87/120    avg_loss:0.150, val_acc:0.954]
Epoch [88/120    avg_loss:0.155, val_acc:0.960]
Epoch [89/120    avg_loss:0.139, val_acc:0.965]
Epoch [90/120    avg_loss:0.137, val_acc:0.963]
Epoch [91/120    avg_loss:0.142, val_acc:0.960]
Epoch [92/120    avg_loss:0.150, val_acc:0.963]
Epoch [93/120    avg_loss:0.165, val_acc:0.960]
Epoch [94/120    avg_loss:0.146, val_acc:0.954]
Epoch [95/120    avg_loss:0.130, val_acc:0.963]
Epoch [96/120    avg_loss:0.132, val_acc:0.960]
Epoch [97/120    avg_loss:0.159, val_acc:0.954]
Epoch [98/120    avg_loss:0.140, val_acc:0.956]
Epoch [99/120    avg_loss:0.144, val_acc:0.958]
Epoch [100/120    avg_loss:0.137, val_acc:0.960]
Epoch [101/120    avg_loss:0.134, val_acc:0.960]
Epoch [102/120    avg_loss:0.136, val_acc:0.960]
Epoch [103/120    avg_loss:0.134, val_acc:0.960]
Epoch [104/120    avg_loss:0.127, val_acc:0.960]
Epoch [105/120    avg_loss:0.135, val_acc:0.960]
Epoch [106/120    avg_loss:0.119, val_acc:0.960]
Epoch [107/120    avg_loss:0.102, val_acc:0.960]
Epoch [108/120    avg_loss:0.128, val_acc:0.960]
Epoch [109/120    avg_loss:0.136, val_acc:0.960]
Epoch [110/120    avg_loss:0.138, val_acc:0.960]
Epoch [111/120    avg_loss:0.142, val_acc:0.960]
Epoch [112/120    avg_loss:0.148, val_acc:0.960]
Epoch [113/120    avg_loss:0.133, val_acc:0.960]
Epoch [114/120    avg_loss:0.131, val_acc:0.960]
Epoch [115/120    avg_loss:0.128, val_acc:0.960]
Epoch [116/120    avg_loss:0.137, val_acc:0.960]
Epoch [117/120    avg_loss:0.134, val_acc:0.960]
Epoch [118/120    avg_loss:0.128, val_acc:0.960]
Epoch [119/120    avg_loss:0.136, val_acc:0.960]
Epoch [120/120    avg_loss:0.138, val_acc:0.960]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 207   0   0   0   0  12   0   0   0   0   0   0]
 [  0   0   2 228   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 166  61   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   9 444   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.88912579957356

F1 scores:
[       nan 1.         0.94090909 0.99563319 0.83838384 0.81609195
 1.         0.87234043 1.         1.         1.         0.98820446
 0.98996656 1.        ]

Kappa:
0.9765057311018155
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb2d663b8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.573, val_acc:0.317]
Epoch [2/120    avg_loss:2.373, val_acc:0.338]
Epoch [3/120    avg_loss:2.236, val_acc:0.392]
Epoch [4/120    avg_loss:2.135, val_acc:0.460]
Epoch [5/120    avg_loss:2.015, val_acc:0.585]
Epoch [6/120    avg_loss:1.912, val_acc:0.615]
Epoch [7/120    avg_loss:1.791, val_acc:0.627]
Epoch [8/120    avg_loss:1.647, val_acc:0.654]
Epoch [9/120    avg_loss:1.557, val_acc:0.650]
Epoch [10/120    avg_loss:1.443, val_acc:0.671]
Epoch [11/120    avg_loss:1.315, val_acc:0.673]
Epoch [12/120    avg_loss:1.269, val_acc:0.694]
Epoch [13/120    avg_loss:1.133, val_acc:0.721]
Epoch [14/120    avg_loss:1.077, val_acc:0.731]
Epoch [15/120    avg_loss:0.994, val_acc:0.758]
Epoch [16/120    avg_loss:0.917, val_acc:0.771]
Epoch [17/120    avg_loss:0.862, val_acc:0.792]
Epoch [18/120    avg_loss:0.799, val_acc:0.808]
Epoch [19/120    avg_loss:0.796, val_acc:0.802]
Epoch [20/120    avg_loss:0.718, val_acc:0.810]
Epoch [21/120    avg_loss:0.699, val_acc:0.867]
Epoch [22/120    avg_loss:0.675, val_acc:0.835]
Epoch [23/120    avg_loss:0.603, val_acc:0.800]
Epoch [24/120    avg_loss:0.624, val_acc:0.850]
Epoch [25/120    avg_loss:0.551, val_acc:0.840]
Epoch [26/120    avg_loss:0.567, val_acc:0.931]
Epoch [27/120    avg_loss:0.505, val_acc:0.869]
Epoch [28/120    avg_loss:0.508, val_acc:0.919]
Epoch [29/120    avg_loss:0.471, val_acc:0.933]
Epoch [30/120    avg_loss:0.427, val_acc:0.938]
Epoch [31/120    avg_loss:0.415, val_acc:0.929]
Epoch [32/120    avg_loss:0.397, val_acc:0.935]
Epoch [33/120    avg_loss:0.384, val_acc:0.952]
Epoch [34/120    avg_loss:0.385, val_acc:0.950]
Epoch [35/120    avg_loss:0.368, val_acc:0.940]
Epoch [36/120    avg_loss:0.330, val_acc:0.946]
Epoch [37/120    avg_loss:0.356, val_acc:0.846]
Epoch [38/120    avg_loss:0.366, val_acc:0.871]
Epoch [39/120    avg_loss:0.328, val_acc:0.946]
Epoch [40/120    avg_loss:0.358, val_acc:0.958]
Epoch [41/120    avg_loss:0.280, val_acc:0.946]
Epoch [42/120    avg_loss:0.280, val_acc:0.944]
Epoch [43/120    avg_loss:0.284, val_acc:0.954]
Epoch [44/120    avg_loss:0.266, val_acc:0.921]
Epoch [45/120    avg_loss:0.322, val_acc:0.842]
Epoch [46/120    avg_loss:0.334, val_acc:0.935]
Epoch [47/120    avg_loss:0.288, val_acc:0.954]
Epoch [48/120    avg_loss:0.262, val_acc:0.940]
Epoch [49/120    avg_loss:0.263, val_acc:0.952]
Epoch [50/120    avg_loss:0.229, val_acc:0.952]
Epoch [51/120    avg_loss:0.217, val_acc:0.952]
Epoch [52/120    avg_loss:0.221, val_acc:0.929]
Epoch [53/120    avg_loss:0.208, val_acc:0.956]
Epoch [54/120    avg_loss:0.226, val_acc:0.965]
Epoch [55/120    avg_loss:0.185, val_acc:0.969]
Epoch [56/120    avg_loss:0.169, val_acc:0.967]
Epoch [57/120    avg_loss:0.156, val_acc:0.965]
Epoch [58/120    avg_loss:0.189, val_acc:0.969]
Epoch [59/120    avg_loss:0.147, val_acc:0.969]
Epoch [60/120    avg_loss:0.152, val_acc:0.971]
Epoch [61/120    avg_loss:0.158, val_acc:0.965]
Epoch [62/120    avg_loss:0.146, val_acc:0.963]
Epoch [63/120    avg_loss:0.144, val_acc:0.969]
Epoch [64/120    avg_loss:0.146, val_acc:0.967]
Epoch [65/120    avg_loss:0.159, val_acc:0.967]
Epoch [66/120    avg_loss:0.146, val_acc:0.967]
Epoch [67/120    avg_loss:0.152, val_acc:0.960]
Epoch [68/120    avg_loss:0.147, val_acc:0.965]
Epoch [69/120    avg_loss:0.158, val_acc:0.971]
Epoch [70/120    avg_loss:0.153, val_acc:0.965]
Epoch [71/120    avg_loss:0.145, val_acc:0.965]
Epoch [72/120    avg_loss:0.157, val_acc:0.969]
Epoch [73/120    avg_loss:0.140, val_acc:0.965]
Epoch [74/120    avg_loss:0.143, val_acc:0.969]
Epoch [75/120    avg_loss:0.159, val_acc:0.965]
Epoch [76/120    avg_loss:0.144, val_acc:0.969]
Epoch [77/120    avg_loss:0.158, val_acc:0.971]
Epoch [78/120    avg_loss:0.147, val_acc:0.969]
Epoch [79/120    avg_loss:0.145, val_acc:0.969]
Epoch [80/120    avg_loss:0.132, val_acc:0.965]
Epoch [81/120    avg_loss:0.135, val_acc:0.967]
Epoch [82/120    avg_loss:0.139, val_acc:0.971]
Epoch [83/120    avg_loss:0.135, val_acc:0.967]
Epoch [84/120    avg_loss:0.130, val_acc:0.967]
Epoch [85/120    avg_loss:0.163, val_acc:0.971]
Epoch [86/120    avg_loss:0.139, val_acc:0.969]
Epoch [87/120    avg_loss:0.135, val_acc:0.971]
Epoch [88/120    avg_loss:0.137, val_acc:0.969]
Epoch [89/120    avg_loss:0.135, val_acc:0.973]
Epoch [90/120    avg_loss:0.114, val_acc:0.971]
Epoch [91/120    avg_loss:0.151, val_acc:0.973]
Epoch [92/120    avg_loss:0.155, val_acc:0.950]
Epoch [93/120    avg_loss:0.131, val_acc:0.960]
Epoch [94/120    avg_loss:0.119, val_acc:0.967]
Epoch [95/120    avg_loss:0.122, val_acc:0.965]
Epoch [96/120    avg_loss:0.142, val_acc:0.971]
Epoch [97/120    avg_loss:0.146, val_acc:0.973]
Epoch [98/120    avg_loss:0.142, val_acc:0.967]
Epoch [99/120    avg_loss:0.121, val_acc:0.973]
Epoch [100/120    avg_loss:0.123, val_acc:0.971]
Epoch [101/120    avg_loss:0.127, val_acc:0.969]
Epoch [102/120    avg_loss:0.113, val_acc:0.971]
Epoch [103/120    avg_loss:0.123, val_acc:0.971]
Epoch [104/120    avg_loss:0.107, val_acc:0.969]
Epoch [105/120    avg_loss:0.130, val_acc:0.971]
Epoch [106/120    avg_loss:0.116, val_acc:0.971]
Epoch [107/120    avg_loss:0.113, val_acc:0.969]
Epoch [108/120    avg_loss:0.130, val_acc:0.971]
Epoch [109/120    avg_loss:0.110, val_acc:0.973]
Epoch [110/120    avg_loss:0.119, val_acc:0.977]
Epoch [111/120    avg_loss:0.121, val_acc:0.975]
Epoch [112/120    avg_loss:0.122, val_acc:0.975]
Epoch [113/120    avg_loss:0.125, val_acc:0.973]
Epoch [114/120    avg_loss:0.120, val_acc:0.971]
Epoch [115/120    avg_loss:0.112, val_acc:0.977]
Epoch [116/120    avg_loss:0.125, val_acc:0.975]
Epoch [117/120    avg_loss:0.113, val_acc:0.975]
Epoch [118/120    avg_loss:0.111, val_acc:0.975]
Epoch [119/120    avg_loss:0.122, val_acc:0.975]
Epoch [120/120    avg_loss:0.110, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   2 226   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   6 194  27   0   0   0   0   0   0   0   0]
 [  0   0   0   0  28 117   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   9 444   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.16631130063966

F1 scores:
[       nan 1.         0.96902655 0.97835498 0.86031042 0.80968858
 1.         0.93181818 1.         1.         1.         0.98820446
 0.98996656 1.        ]

Kappa:
0.9795842772918185
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe8f58d8898>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.580, val_acc:0.352]
Epoch [2/120    avg_loss:2.397, val_acc:0.377]
Epoch [3/120    avg_loss:2.262, val_acc:0.371]
Epoch [4/120    avg_loss:2.156, val_acc:0.371]
Epoch [5/120    avg_loss:2.036, val_acc:0.429]
Epoch [6/120    avg_loss:1.921, val_acc:0.458]
Epoch [7/120    avg_loss:1.791, val_acc:0.490]
Epoch [8/120    avg_loss:1.699, val_acc:0.527]
Epoch [9/120    avg_loss:1.611, val_acc:0.581]
Epoch [10/120    avg_loss:1.510, val_acc:0.629]
Epoch [11/120    avg_loss:1.397, val_acc:0.681]
Epoch [12/120    avg_loss:1.318, val_acc:0.667]
Epoch [13/120    avg_loss:1.233, val_acc:0.733]
Epoch [14/120    avg_loss:1.143, val_acc:0.769]
Epoch [15/120    avg_loss:1.065, val_acc:0.804]
Epoch [16/120    avg_loss:0.981, val_acc:0.815]
Epoch [17/120    avg_loss:0.894, val_acc:0.819]
Epoch [18/120    avg_loss:0.852, val_acc:0.852]
Epoch [19/120    avg_loss:0.825, val_acc:0.783]
Epoch [20/120    avg_loss:0.767, val_acc:0.887]
Epoch [21/120    avg_loss:0.740, val_acc:0.885]
Epoch [22/120    avg_loss:0.665, val_acc:0.892]
Epoch [23/120    avg_loss:0.661, val_acc:0.885]
Epoch [24/120    avg_loss:0.592, val_acc:0.892]
Epoch [25/120    avg_loss:0.585, val_acc:0.910]
Epoch [26/120    avg_loss:0.539, val_acc:0.915]
Epoch [27/120    avg_loss:0.524, val_acc:0.898]
Epoch [28/120    avg_loss:0.524, val_acc:0.917]
Epoch [29/120    avg_loss:0.512, val_acc:0.910]
Epoch [30/120    avg_loss:0.480, val_acc:0.929]
Epoch [31/120    avg_loss:0.452, val_acc:0.935]
Epoch [32/120    avg_loss:0.434, val_acc:0.923]
Epoch [33/120    avg_loss:0.435, val_acc:0.915]
Epoch [34/120    avg_loss:0.423, val_acc:0.904]
Epoch [35/120    avg_loss:0.418, val_acc:0.900]
Epoch [36/120    avg_loss:0.398, val_acc:0.942]
Epoch [37/120    avg_loss:0.346, val_acc:0.946]
Epoch [38/120    avg_loss:0.363, val_acc:0.931]
Epoch [39/120    avg_loss:0.367, val_acc:0.933]
Epoch [40/120    avg_loss:0.352, val_acc:0.925]
Epoch [41/120    avg_loss:0.300, val_acc:0.938]
Epoch [42/120    avg_loss:0.309, val_acc:0.952]
Epoch [43/120    avg_loss:0.304, val_acc:0.960]
Epoch [44/120    avg_loss:0.292, val_acc:0.938]
Epoch [45/120    avg_loss:0.297, val_acc:0.958]
Epoch [46/120    avg_loss:0.271, val_acc:0.954]
Epoch [47/120    avg_loss:0.278, val_acc:0.952]
Epoch [48/120    avg_loss:0.300, val_acc:0.952]
Epoch [49/120    avg_loss:0.234, val_acc:0.950]
Epoch [50/120    avg_loss:0.252, val_acc:0.952]
Epoch [51/120    avg_loss:0.266, val_acc:0.963]
Epoch [52/120    avg_loss:0.237, val_acc:0.950]
Epoch [53/120    avg_loss:0.212, val_acc:0.960]
Epoch [54/120    avg_loss:0.216, val_acc:0.950]
Epoch [55/120    avg_loss:0.207, val_acc:0.925]
Epoch [56/120    avg_loss:0.276, val_acc:0.944]
Epoch [57/120    avg_loss:0.257, val_acc:0.946]
Epoch [58/120    avg_loss:0.278, val_acc:0.965]
Epoch [59/120    avg_loss:0.186, val_acc:0.946]
Epoch [60/120    avg_loss:0.206, val_acc:0.963]
Epoch [61/120    avg_loss:0.207, val_acc:0.971]
Epoch [62/120    avg_loss:0.182, val_acc:0.960]
Epoch [63/120    avg_loss:0.185, val_acc:0.954]
Epoch [64/120    avg_loss:0.191, val_acc:0.940]
Epoch [65/120    avg_loss:0.214, val_acc:0.963]
Epoch [66/120    avg_loss:0.168, val_acc:0.960]
Epoch [67/120    avg_loss:0.181, val_acc:0.960]
Epoch [68/120    avg_loss:0.196, val_acc:0.960]
Epoch [69/120    avg_loss:0.195, val_acc:0.960]
Epoch [70/120    avg_loss:0.191, val_acc:0.969]
Epoch [71/120    avg_loss:0.146, val_acc:0.963]
Epoch [72/120    avg_loss:0.163, val_acc:0.965]
Epoch [73/120    avg_loss:0.147, val_acc:0.973]
Epoch [74/120    avg_loss:0.149, val_acc:0.977]
Epoch [75/120    avg_loss:0.144, val_acc:0.967]
Epoch [76/120    avg_loss:0.131, val_acc:0.975]
Epoch [77/120    avg_loss:0.138, val_acc:0.967]
Epoch [78/120    avg_loss:0.156, val_acc:0.958]
Epoch [79/120    avg_loss:0.133, val_acc:0.971]
Epoch [80/120    avg_loss:0.143, val_acc:0.969]
Epoch [81/120    avg_loss:0.123, val_acc:0.973]
Epoch [82/120    avg_loss:0.130, val_acc:0.977]
Epoch [83/120    avg_loss:0.103, val_acc:0.969]
Epoch [84/120    avg_loss:0.123, val_acc:0.973]
Epoch [85/120    avg_loss:0.161, val_acc:0.940]
Epoch [86/120    avg_loss:0.152, val_acc:0.963]
Epoch [87/120    avg_loss:0.110, val_acc:0.971]
Epoch [88/120    avg_loss:0.133, val_acc:0.973]
Epoch [89/120    avg_loss:0.116, val_acc:0.983]
Epoch [90/120    avg_loss:0.090, val_acc:0.971]
Epoch [91/120    avg_loss:0.096, val_acc:0.979]
Epoch [92/120    avg_loss:0.100, val_acc:0.965]
Epoch [93/120    avg_loss:0.087, val_acc:0.977]
Epoch [94/120    avg_loss:0.080, val_acc:0.973]
Epoch [95/120    avg_loss:0.096, val_acc:0.975]
Epoch [96/120    avg_loss:0.108, val_acc:0.973]
Epoch [97/120    avg_loss:0.107, val_acc:0.971]
Epoch [98/120    avg_loss:0.095, val_acc:0.971]
Epoch [99/120    avg_loss:0.108, val_acc:0.969]
Epoch [100/120    avg_loss:0.089, val_acc:0.979]
Epoch [101/120    avg_loss:0.093, val_acc:0.979]
Epoch [102/120    avg_loss:0.117, val_acc:0.981]
Epoch [103/120    avg_loss:0.071, val_acc:0.981]
Epoch [104/120    avg_loss:0.059, val_acc:0.977]
Epoch [105/120    avg_loss:0.073, val_acc:0.985]
Epoch [106/120    avg_loss:0.062, val_acc:0.985]
Epoch [107/120    avg_loss:0.063, val_acc:0.983]
Epoch [108/120    avg_loss:0.060, val_acc:0.983]
Epoch [109/120    avg_loss:0.057, val_acc:0.979]
Epoch [110/120    avg_loss:0.052, val_acc:0.983]
Epoch [111/120    avg_loss:0.054, val_acc:0.979]
Epoch [112/120    avg_loss:0.057, val_acc:0.983]
Epoch [113/120    avg_loss:0.059, val_acc:0.983]
Epoch [114/120    avg_loss:0.070, val_acc:0.983]
Epoch [115/120    avg_loss:0.057, val_acc:0.983]
Epoch [116/120    avg_loss:0.054, val_acc:0.983]
Epoch [117/120    avg_loss:0.053, val_acc:0.983]
Epoch [118/120    avg_loss:0.056, val_acc:0.983]
Epoch [119/120    avg_loss:0.048, val_acc:0.983]
Epoch [120/120    avg_loss:0.051, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 202  22   0   0   0   0   0   0   3   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 1.         0.97986577 0.99782135 0.94172494 0.92948718
 1.         0.95555556 1.         1.         1.         0.9986755
 0.99559471 1.        ]

Kappa:
0.9916917386526182
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5ff975c860>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.615, val_acc:0.102]
Epoch [2/120    avg_loss:2.404, val_acc:0.406]
Epoch [3/120    avg_loss:2.280, val_acc:0.421]
Epoch [4/120    avg_loss:2.153, val_acc:0.410]
Epoch [5/120    avg_loss:2.043, val_acc:0.510]
Epoch [6/120    avg_loss:1.923, val_acc:0.592]
Epoch [7/120    avg_loss:1.847, val_acc:0.652]
Epoch [8/120    avg_loss:1.736, val_acc:0.648]
Epoch [9/120    avg_loss:1.653, val_acc:0.692]
Epoch [10/120    avg_loss:1.555, val_acc:0.710]
Epoch [11/120    avg_loss:1.466, val_acc:0.706]
Epoch [12/120    avg_loss:1.401, val_acc:0.708]
Epoch [13/120    avg_loss:1.277, val_acc:0.702]
Epoch [14/120    avg_loss:1.211, val_acc:0.740]
Epoch [15/120    avg_loss:1.137, val_acc:0.725]
Epoch [16/120    avg_loss:1.072, val_acc:0.794]
Epoch [17/120    avg_loss:0.995, val_acc:0.777]
Epoch [18/120    avg_loss:0.906, val_acc:0.848]
Epoch [19/120    avg_loss:0.861, val_acc:0.854]
Epoch [20/120    avg_loss:0.804, val_acc:0.812]
Epoch [21/120    avg_loss:0.778, val_acc:0.825]
Epoch [22/120    avg_loss:0.697, val_acc:0.910]
Epoch [23/120    avg_loss:0.633, val_acc:0.904]
Epoch [24/120    avg_loss:0.599, val_acc:0.921]
Epoch [25/120    avg_loss:0.558, val_acc:0.919]
Epoch [26/120    avg_loss:0.530, val_acc:0.833]
Epoch [27/120    avg_loss:0.527, val_acc:0.906]
Epoch [28/120    avg_loss:0.529, val_acc:0.900]
Epoch [29/120    avg_loss:0.479, val_acc:0.948]
Epoch [30/120    avg_loss:0.490, val_acc:0.938]
Epoch [31/120    avg_loss:0.473, val_acc:0.927]
Epoch [32/120    avg_loss:0.403, val_acc:0.925]
Epoch [33/120    avg_loss:0.438, val_acc:0.940]
Epoch [34/120    avg_loss:0.407, val_acc:0.935]
Epoch [35/120    avg_loss:0.375, val_acc:0.933]
Epoch [36/120    avg_loss:0.360, val_acc:0.952]
Epoch [37/120    avg_loss:0.357, val_acc:0.931]
Epoch [38/120    avg_loss:0.322, val_acc:0.925]
Epoch [39/120    avg_loss:0.335, val_acc:0.965]
Epoch [40/120    avg_loss:0.329, val_acc:0.915]
Epoch [41/120    avg_loss:0.332, val_acc:0.946]
Epoch [42/120    avg_loss:0.323, val_acc:0.904]
Epoch [43/120    avg_loss:0.351, val_acc:0.919]
Epoch [44/120    avg_loss:0.349, val_acc:0.908]
Epoch [45/120    avg_loss:0.323, val_acc:0.946]
Epoch [46/120    avg_loss:0.283, val_acc:0.963]
Epoch [47/120    avg_loss:0.273, val_acc:0.958]
Epoch [48/120    avg_loss:0.306, val_acc:0.904]
Epoch [49/120    avg_loss:0.341, val_acc:0.933]
Epoch [50/120    avg_loss:0.280, val_acc:0.956]
Epoch [51/120    avg_loss:0.321, val_acc:0.929]
Epoch [52/120    avg_loss:0.370, val_acc:0.921]
Epoch [53/120    avg_loss:0.321, val_acc:0.954]
Epoch [54/120    avg_loss:0.256, val_acc:0.952]
Epoch [55/120    avg_loss:0.252, val_acc:0.969]
Epoch [56/120    avg_loss:0.238, val_acc:0.973]
Epoch [57/120    avg_loss:0.223, val_acc:0.969]
Epoch [58/120    avg_loss:0.221, val_acc:0.969]
Epoch [59/120    avg_loss:0.200, val_acc:0.969]
Epoch [60/120    avg_loss:0.206, val_acc:0.971]
Epoch [61/120    avg_loss:0.213, val_acc:0.971]
Epoch [62/120    avg_loss:0.185, val_acc:0.973]
Epoch [63/120    avg_loss:0.219, val_acc:0.971]
Epoch [64/120    avg_loss:0.219, val_acc:0.971]
Epoch [65/120    avg_loss:0.185, val_acc:0.973]
Epoch [66/120    avg_loss:0.179, val_acc:0.975]
Epoch [67/120    avg_loss:0.179, val_acc:0.975]
Epoch [68/120    avg_loss:0.186, val_acc:0.975]
Epoch [69/120    avg_loss:0.179, val_acc:0.973]
Epoch [70/120    avg_loss:0.182, val_acc:0.973]
Epoch [71/120    avg_loss:0.178, val_acc:0.967]
Epoch [72/120    avg_loss:0.183, val_acc:0.975]
Epoch [73/120    avg_loss:0.185, val_acc:0.973]
Epoch [74/120    avg_loss:0.196, val_acc:0.975]
Epoch [75/120    avg_loss:0.170, val_acc:0.977]
Epoch [76/120    avg_loss:0.165, val_acc:0.971]
Epoch [77/120    avg_loss:0.166, val_acc:0.977]
Epoch [78/120    avg_loss:0.175, val_acc:0.977]
Epoch [79/120    avg_loss:0.172, val_acc:0.977]
Epoch [80/120    avg_loss:0.165, val_acc:0.977]
Epoch [81/120    avg_loss:0.166, val_acc:0.975]
Epoch [82/120    avg_loss:0.159, val_acc:0.977]
Epoch [83/120    avg_loss:0.151, val_acc:0.979]
Epoch [84/120    avg_loss:0.154, val_acc:0.975]
Epoch [85/120    avg_loss:0.163, val_acc:0.979]
Epoch [86/120    avg_loss:0.156, val_acc:0.975]
Epoch [87/120    avg_loss:0.162, val_acc:0.977]
Epoch [88/120    avg_loss:0.153, val_acc:0.975]
Epoch [89/120    avg_loss:0.160, val_acc:0.977]
Epoch [90/120    avg_loss:0.157, val_acc:0.977]
Epoch [91/120    avg_loss:0.143, val_acc:0.983]
Epoch [92/120    avg_loss:0.148, val_acc:0.981]
Epoch [93/120    avg_loss:0.155, val_acc:0.981]
Epoch [94/120    avg_loss:0.138, val_acc:0.979]
Epoch [95/120    avg_loss:0.142, val_acc:0.981]
Epoch [96/120    avg_loss:0.134, val_acc:0.983]
Epoch [97/120    avg_loss:0.142, val_acc:0.975]
Epoch [98/120    avg_loss:0.153, val_acc:0.977]
Epoch [99/120    avg_loss:0.136, val_acc:0.981]
Epoch [100/120    avg_loss:0.145, val_acc:0.977]
Epoch [101/120    avg_loss:0.132, val_acc:0.979]
Epoch [102/120    avg_loss:0.140, val_acc:0.979]
Epoch [103/120    avg_loss:0.151, val_acc:0.977]
Epoch [104/120    avg_loss:0.150, val_acc:0.979]
Epoch [105/120    avg_loss:0.138, val_acc:0.979]
Epoch [106/120    avg_loss:0.151, val_acc:0.977]
Epoch [107/120    avg_loss:0.158, val_acc:0.977]
Epoch [108/120    avg_loss:0.140, val_acc:0.979]
Epoch [109/120    avg_loss:0.137, val_acc:0.977]
Epoch [110/120    avg_loss:0.139, val_acc:0.977]
Epoch [111/120    avg_loss:0.126, val_acc:0.979]
Epoch [112/120    avg_loss:0.121, val_acc:0.979]
Epoch [113/120    avg_loss:0.124, val_acc:0.979]
Epoch [114/120    avg_loss:0.150, val_acc:0.979]
Epoch [115/120    avg_loss:0.124, val_acc:0.979]
Epoch [116/120    avg_loss:0.142, val_acc:0.979]
Epoch [117/120    avg_loss:0.132, val_acc:0.979]
Epoch [118/120    avg_loss:0.125, val_acc:0.979]
Epoch [119/120    avg_loss:0.137, val_acc:0.979]
Epoch [120/120    avg_loss:0.133, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   2 228   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 200  27   0   0   0   0   0   0   0   0]
 [  0   0   0   5  18 122   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3   0 203   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.44349680170575

F1 scores:
[       nan 1.         0.95555556 0.98488121 0.89285714 0.82993197
 0.99266504 0.8988764  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9826702959650444
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe8b6c808d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.624, val_acc:0.319]
Epoch [2/120    avg_loss:2.446, val_acc:0.319]
Epoch [3/120    avg_loss:2.315, val_acc:0.371]
Epoch [4/120    avg_loss:2.211, val_acc:0.450]
Epoch [5/120    avg_loss:2.088, val_acc:0.544]
Epoch [6/120    avg_loss:1.971, val_acc:0.610]
Epoch [7/120    avg_loss:1.853, val_acc:0.667]
Epoch [8/120    avg_loss:1.759, val_acc:0.696]
Epoch [9/120    avg_loss:1.647, val_acc:0.700]
Epoch [10/120    avg_loss:1.560, val_acc:0.679]
Epoch [11/120    avg_loss:1.424, val_acc:0.681]
Epoch [12/120    avg_loss:1.342, val_acc:0.694]
Epoch [13/120    avg_loss:1.238, val_acc:0.710]
Epoch [14/120    avg_loss:1.171, val_acc:0.725]
Epoch [15/120    avg_loss:1.106, val_acc:0.727]
Epoch [16/120    avg_loss:1.024, val_acc:0.750]
Epoch [17/120    avg_loss:0.951, val_acc:0.754]
Epoch [18/120    avg_loss:0.885, val_acc:0.779]
Epoch [19/120    avg_loss:0.833, val_acc:0.860]
Epoch [20/120    avg_loss:0.793, val_acc:0.908]
Epoch [21/120    avg_loss:0.744, val_acc:0.773]
Epoch [22/120    avg_loss:0.735, val_acc:0.919]
Epoch [23/120    avg_loss:0.631, val_acc:0.919]
Epoch [24/120    avg_loss:0.578, val_acc:0.915]
Epoch [25/120    avg_loss:0.570, val_acc:0.794]
Epoch [26/120    avg_loss:0.565, val_acc:0.904]
Epoch [27/120    avg_loss:0.596, val_acc:0.819]
Epoch [28/120    avg_loss:0.537, val_acc:0.929]
Epoch [29/120    avg_loss:0.495, val_acc:0.942]
Epoch [30/120    avg_loss:0.432, val_acc:0.946]
Epoch [31/120    avg_loss:0.442, val_acc:0.940]
Epoch [32/120    avg_loss:0.390, val_acc:0.958]
Epoch [33/120    avg_loss:0.398, val_acc:0.933]
Epoch [34/120    avg_loss:0.384, val_acc:0.896]
Epoch [35/120    avg_loss:0.391, val_acc:0.912]
Epoch [36/120    avg_loss:0.420, val_acc:0.906]
Epoch [37/120    avg_loss:0.375, val_acc:0.958]
Epoch [38/120    avg_loss:0.339, val_acc:0.944]
Epoch [39/120    avg_loss:0.326, val_acc:0.965]
Epoch [40/120    avg_loss:0.308, val_acc:0.940]
Epoch [41/120    avg_loss:0.338, val_acc:0.967]
Epoch [42/120    avg_loss:0.283, val_acc:0.948]
Epoch [43/120    avg_loss:0.307, val_acc:0.958]
Epoch [44/120    avg_loss:0.278, val_acc:0.969]
Epoch [45/120    avg_loss:0.278, val_acc:0.960]
Epoch [46/120    avg_loss:0.258, val_acc:0.960]
Epoch [47/120    avg_loss:0.268, val_acc:0.954]
Epoch [48/120    avg_loss:0.242, val_acc:0.960]
Epoch [49/120    avg_loss:0.274, val_acc:0.956]
Epoch [50/120    avg_loss:0.240, val_acc:0.963]
Epoch [51/120    avg_loss:0.234, val_acc:0.967]
Epoch [52/120    avg_loss:0.205, val_acc:0.969]
Epoch [53/120    avg_loss:0.231, val_acc:0.952]
Epoch [54/120    avg_loss:0.230, val_acc:0.958]
Epoch [55/120    avg_loss:0.193, val_acc:0.948]
Epoch [56/120    avg_loss:0.219, val_acc:0.960]
Epoch [57/120    avg_loss:0.212, val_acc:0.965]
Epoch [58/120    avg_loss:0.243, val_acc:0.963]
Epoch [59/120    avg_loss:0.213, val_acc:0.965]
Epoch [60/120    avg_loss:0.293, val_acc:0.952]
Epoch [61/120    avg_loss:0.211, val_acc:0.952]
Epoch [62/120    avg_loss:0.296, val_acc:0.973]
Epoch [63/120    avg_loss:0.229, val_acc:0.960]
Epoch [64/120    avg_loss:0.249, val_acc:0.948]
Epoch [65/120    avg_loss:0.208, val_acc:0.965]
Epoch [66/120    avg_loss:0.250, val_acc:0.950]
Epoch [67/120    avg_loss:0.202, val_acc:0.963]
Epoch [68/120    avg_loss:0.193, val_acc:0.973]
Epoch [69/120    avg_loss:0.166, val_acc:0.969]
Epoch [70/120    avg_loss:0.134, val_acc:0.977]
Epoch [71/120    avg_loss:0.140, val_acc:0.971]
Epoch [72/120    avg_loss:0.214, val_acc:0.979]
Epoch [73/120    avg_loss:0.282, val_acc:0.956]
Epoch [74/120    avg_loss:0.228, val_acc:0.956]
Epoch [75/120    avg_loss:0.170, val_acc:0.975]
Epoch [76/120    avg_loss:0.169, val_acc:0.965]
Epoch [77/120    avg_loss:0.163, val_acc:0.969]
Epoch [78/120    avg_loss:0.189, val_acc:0.967]
Epoch [79/120    avg_loss:0.154, val_acc:0.977]
Epoch [80/120    avg_loss:0.124, val_acc:0.967]
Epoch [81/120    avg_loss:0.140, val_acc:0.979]
Epoch [82/120    avg_loss:0.112, val_acc:0.973]
Epoch [83/120    avg_loss:0.109, val_acc:0.973]
Epoch [84/120    avg_loss:0.138, val_acc:0.973]
Epoch [85/120    avg_loss:0.153, val_acc:0.973]
Epoch [86/120    avg_loss:0.145, val_acc:0.979]
Epoch [87/120    avg_loss:0.112, val_acc:0.971]
Epoch [88/120    avg_loss:0.108, val_acc:0.975]
Epoch [89/120    avg_loss:0.139, val_acc:0.967]
Epoch [90/120    avg_loss:0.100, val_acc:0.971]
Epoch [91/120    avg_loss:0.131, val_acc:0.975]
Epoch [92/120    avg_loss:0.126, val_acc:0.979]
Epoch [93/120    avg_loss:0.129, val_acc:0.981]
Epoch [94/120    avg_loss:0.107, val_acc:0.969]
Epoch [95/120    avg_loss:0.127, val_acc:0.971]
Epoch [96/120    avg_loss:0.082, val_acc:0.983]
Epoch [97/120    avg_loss:0.102, val_acc:0.969]
Epoch [98/120    avg_loss:0.123, val_acc:0.979]
Epoch [99/120    avg_loss:0.130, val_acc:0.975]
Epoch [100/120    avg_loss:0.123, val_acc:0.979]
Epoch [101/120    avg_loss:0.085, val_acc:0.969]
Epoch [102/120    avg_loss:0.087, val_acc:0.981]
Epoch [103/120    avg_loss:0.161, val_acc:0.944]
Epoch [104/120    avg_loss:0.109, val_acc:0.977]
Epoch [105/120    avg_loss:0.105, val_acc:0.983]
Epoch [106/120    avg_loss:0.100, val_acc:0.975]
Epoch [107/120    avg_loss:0.087, val_acc:0.983]
Epoch [108/120    avg_loss:0.075, val_acc:0.981]
Epoch [109/120    avg_loss:0.092, val_acc:0.979]
Epoch [110/120    avg_loss:0.075, val_acc:0.965]
Epoch [111/120    avg_loss:0.082, val_acc:0.977]
Epoch [112/120    avg_loss:0.093, val_acc:0.979]
Epoch [113/120    avg_loss:0.105, val_acc:0.969]
Epoch [114/120    avg_loss:0.146, val_acc:0.969]
Epoch [115/120    avg_loss:0.077, val_acc:0.975]
Epoch [116/120    avg_loss:0.067, val_acc:0.981]
Epoch [117/120    avg_loss:0.059, val_acc:0.979]
Epoch [118/120    avg_loss:0.078, val_acc:0.979]
Epoch [119/120    avg_loss:0.090, val_acc:0.965]
Epoch [120/120    avg_loss:0.101, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 205  22   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 0.99412628 0.97285068 0.99782135 0.94470046 0.92258065
 0.98095238 0.94054054 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9895579156676705
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f618015b940>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.568, val_acc:0.310]
Epoch [2/120    avg_loss:2.382, val_acc:0.329]
Epoch [3/120    avg_loss:2.265, val_acc:0.331]
Epoch [4/120    avg_loss:2.135, val_acc:0.446]
Epoch [5/120    avg_loss:2.001, val_acc:0.504]
Epoch [6/120    avg_loss:1.889, val_acc:0.575]
Epoch [7/120    avg_loss:1.774, val_acc:0.600]
Epoch [8/120    avg_loss:1.669, val_acc:0.600]
Epoch [9/120    avg_loss:1.549, val_acc:0.648]
Epoch [10/120    avg_loss:1.449, val_acc:0.662]
Epoch [11/120    avg_loss:1.336, val_acc:0.692]
Epoch [12/120    avg_loss:1.247, val_acc:0.706]
Epoch [13/120    avg_loss:1.171, val_acc:0.704]
Epoch [14/120    avg_loss:1.072, val_acc:0.806]
Epoch [15/120    avg_loss:0.998, val_acc:0.846]
Epoch [16/120    avg_loss:0.926, val_acc:0.775]
Epoch [17/120    avg_loss:0.839, val_acc:0.877]
Epoch [18/120    avg_loss:0.809, val_acc:0.904]
Epoch [19/120    avg_loss:0.773, val_acc:0.831]
Epoch [20/120    avg_loss:0.708, val_acc:0.896]
Epoch [21/120    avg_loss:0.633, val_acc:0.906]
Epoch [22/120    avg_loss:0.609, val_acc:0.904]
Epoch [23/120    avg_loss:0.590, val_acc:0.933]
Epoch [24/120    avg_loss:0.573, val_acc:0.925]
Epoch [25/120    avg_loss:0.517, val_acc:0.935]
Epoch [26/120    avg_loss:0.479, val_acc:0.927]
Epoch [27/120    avg_loss:0.465, val_acc:0.929]
Epoch [28/120    avg_loss:0.427, val_acc:0.927]
Epoch [29/120    avg_loss:0.411, val_acc:0.940]
Epoch [30/120    avg_loss:0.454, val_acc:0.940]
Epoch [31/120    avg_loss:0.441, val_acc:0.935]
Epoch [32/120    avg_loss:0.500, val_acc:0.921]
Epoch [33/120    avg_loss:0.434, val_acc:0.946]
Epoch [34/120    avg_loss:0.407, val_acc:0.927]
Epoch [35/120    avg_loss:0.440, val_acc:0.906]
Epoch [36/120    avg_loss:0.402, val_acc:0.938]
Epoch [37/120    avg_loss:0.339, val_acc:0.938]
Epoch [38/120    avg_loss:0.321, val_acc:0.925]
Epoch [39/120    avg_loss:0.361, val_acc:0.931]
Epoch [40/120    avg_loss:0.370, val_acc:0.921]
Epoch [41/120    avg_loss:0.358, val_acc:0.942]
Epoch [42/120    avg_loss:0.342, val_acc:0.948]
Epoch [43/120    avg_loss:0.295, val_acc:0.925]
Epoch [44/120    avg_loss:0.396, val_acc:0.948]
Epoch [45/120    avg_loss:0.354, val_acc:0.944]
Epoch [46/120    avg_loss:0.308, val_acc:0.935]
Epoch [47/120    avg_loss:0.282, val_acc:0.952]
Epoch [48/120    avg_loss:0.264, val_acc:0.958]
Epoch [49/120    avg_loss:0.296, val_acc:0.931]
Epoch [50/120    avg_loss:0.282, val_acc:0.944]
Epoch [51/120    avg_loss:0.304, val_acc:0.931]
Epoch [52/120    avg_loss:0.287, val_acc:0.931]
Epoch [53/120    avg_loss:0.285, val_acc:0.954]
Epoch [54/120    avg_loss:0.253, val_acc:0.942]
Epoch [55/120    avg_loss:0.262, val_acc:0.942]
Epoch [56/120    avg_loss:0.217, val_acc:0.969]
Epoch [57/120    avg_loss:0.204, val_acc:0.958]
Epoch [58/120    avg_loss:0.231, val_acc:0.958]
Epoch [59/120    avg_loss:0.222, val_acc:0.948]
Epoch [60/120    avg_loss:0.236, val_acc:0.960]
Epoch [61/120    avg_loss:0.182, val_acc:0.963]
Epoch [62/120    avg_loss:0.204, val_acc:0.960]
Epoch [63/120    avg_loss:0.226, val_acc:0.929]
Epoch [64/120    avg_loss:0.228, val_acc:0.948]
Epoch [65/120    avg_loss:0.203, val_acc:0.958]
Epoch [66/120    avg_loss:0.240, val_acc:0.954]
Epoch [67/120    avg_loss:0.225, val_acc:0.948]
Epoch [68/120    avg_loss:0.183, val_acc:0.963]
Epoch [69/120    avg_loss:0.194, val_acc:0.975]
Epoch [70/120    avg_loss:0.157, val_acc:0.973]
Epoch [71/120    avg_loss:0.138, val_acc:0.975]
Epoch [72/120    avg_loss:0.156, val_acc:0.967]
Epoch [73/120    avg_loss:0.157, val_acc:0.973]
Epoch [74/120    avg_loss:0.159, val_acc:0.971]
Epoch [75/120    avg_loss:0.136, val_acc:0.975]
Epoch [76/120    avg_loss:0.146, val_acc:0.983]
Epoch [77/120    avg_loss:0.130, val_acc:0.979]
Epoch [78/120    avg_loss:0.140, val_acc:0.985]
Epoch [79/120    avg_loss:0.126, val_acc:0.969]
Epoch [80/120    avg_loss:0.133, val_acc:0.975]
Epoch [81/120    avg_loss:0.145, val_acc:0.975]
Epoch [82/120    avg_loss:0.134, val_acc:0.973]
Epoch [83/120    avg_loss:0.144, val_acc:0.971]
Epoch [84/120    avg_loss:0.111, val_acc:0.971]
Epoch [85/120    avg_loss:0.116, val_acc:0.963]
Epoch [86/120    avg_loss:0.134, val_acc:0.983]
Epoch [87/120    avg_loss:0.133, val_acc:0.981]
Epoch [88/120    avg_loss:0.098, val_acc:0.969]
Epoch [89/120    avg_loss:0.110, val_acc:0.981]
Epoch [90/120    avg_loss:0.096, val_acc:0.990]
Epoch [91/120    avg_loss:0.145, val_acc:0.921]
Epoch [92/120    avg_loss:0.126, val_acc:0.981]
Epoch [93/120    avg_loss:0.143, val_acc:0.977]
Epoch [94/120    avg_loss:0.116, val_acc:0.981]
Epoch [95/120    avg_loss:0.096, val_acc:0.983]
Epoch [96/120    avg_loss:0.112, val_acc:0.988]
Epoch [97/120    avg_loss:0.126, val_acc:0.977]
Epoch [98/120    avg_loss:0.136, val_acc:0.975]
Epoch [99/120    avg_loss:0.109, val_acc:0.965]
Epoch [100/120    avg_loss:0.118, val_acc:0.990]
Epoch [101/120    avg_loss:0.075, val_acc:0.992]
Epoch [102/120    avg_loss:0.082, val_acc:0.985]
Epoch [103/120    avg_loss:0.059, val_acc:0.994]
Epoch [104/120    avg_loss:0.055, val_acc:0.994]
Epoch [105/120    avg_loss:0.056, val_acc:0.994]
Epoch [106/120    avg_loss:0.083, val_acc:0.992]
Epoch [107/120    avg_loss:0.072, val_acc:0.988]
Epoch [108/120    avg_loss:0.109, val_acc:0.985]
Epoch [109/120    avg_loss:0.084, val_acc:0.996]
Epoch [110/120    avg_loss:0.081, val_acc:0.994]
Epoch [111/120    avg_loss:0.064, val_acc:0.992]
Epoch [112/120    avg_loss:0.052, val_acc:0.996]
Epoch [113/120    avg_loss:0.067, val_acc:0.992]
Epoch [114/120    avg_loss:0.045, val_acc:0.994]
Epoch [115/120    avg_loss:0.050, val_acc:0.996]
Epoch [116/120    avg_loss:0.038, val_acc:0.996]
Epoch [117/120    avg_loss:0.037, val_acc:0.994]
Epoch [118/120    avg_loss:0.055, val_acc:0.990]
Epoch [119/120    avg_loss:0.054, val_acc:0.998]
Epoch [120/120    avg_loss:0.073, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 671   0   0   0   0   0   0  14   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   8 211   8   0   0   0   0   0   0   0   0]
 [  0   0   0   7  63  75   0   0   0   0   0   0   0   0]
 [  0  13   0   0   7   0 186   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
97.22814498933901

F1 scores:
[       nan 0.98027757 0.98206278 0.96624473 0.83070866 0.65789474
 0.94897959 0.96132597 0.98227848 1.         1.         0.9973545
 0.99778761 1.        ]

Kappa:
0.9691278212004117
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7474bb3898>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.622, val_acc:0.248]
Epoch [2/120    avg_loss:2.440, val_acc:0.319]
Epoch [3/120    avg_loss:2.291, val_acc:0.415]
Epoch [4/120    avg_loss:2.172, val_acc:0.465]
Epoch [5/120    avg_loss:2.073, val_acc:0.504]
Epoch [6/120    avg_loss:1.960, val_acc:0.548]
Epoch [7/120    avg_loss:1.849, val_acc:0.560]
Epoch [8/120    avg_loss:1.743, val_acc:0.613]
Epoch [9/120    avg_loss:1.628, val_acc:0.613]
Epoch [10/120    avg_loss:1.514, val_acc:0.646]
Epoch [11/120    avg_loss:1.399, val_acc:0.681]
Epoch [12/120    avg_loss:1.315, val_acc:0.688]
Epoch [13/120    avg_loss:1.209, val_acc:0.735]
Epoch [14/120    avg_loss:1.132, val_acc:0.717]
Epoch [15/120    avg_loss:1.050, val_acc:0.760]
Epoch [16/120    avg_loss:0.945, val_acc:0.762]
Epoch [17/120    avg_loss:0.901, val_acc:0.808]
Epoch [18/120    avg_loss:0.829, val_acc:0.842]
Epoch [19/120    avg_loss:0.788, val_acc:0.848]
Epoch [20/120    avg_loss:0.743, val_acc:0.879]
Epoch [21/120    avg_loss:0.689, val_acc:0.871]
Epoch [22/120    avg_loss:0.673, val_acc:0.869]
Epoch [23/120    avg_loss:0.690, val_acc:0.867]
Epoch [24/120    avg_loss:0.624, val_acc:0.910]
Epoch [25/120    avg_loss:0.584, val_acc:0.902]
Epoch [26/120    avg_loss:0.541, val_acc:0.921]
Epoch [27/120    avg_loss:0.513, val_acc:0.879]
Epoch [28/120    avg_loss:0.530, val_acc:0.931]
Epoch [29/120    avg_loss:0.445, val_acc:0.917]
Epoch [30/120    avg_loss:0.447, val_acc:0.925]
Epoch [31/120    avg_loss:0.428, val_acc:0.931]
Epoch [32/120    avg_loss:0.401, val_acc:0.958]
Epoch [33/120    avg_loss:0.417, val_acc:0.910]
Epoch [34/120    avg_loss:0.403, val_acc:0.898]
Epoch [35/120    avg_loss:0.397, val_acc:0.921]
Epoch [36/120    avg_loss:0.436, val_acc:0.927]
Epoch [37/120    avg_loss:0.405, val_acc:0.948]
Epoch [38/120    avg_loss:0.357, val_acc:0.948]
Epoch [39/120    avg_loss:0.372, val_acc:0.929]
Epoch [40/120    avg_loss:0.343, val_acc:0.946]
Epoch [41/120    avg_loss:0.332, val_acc:0.938]
Epoch [42/120    avg_loss:0.315, val_acc:0.927]
Epoch [43/120    avg_loss:0.330, val_acc:0.925]
Epoch [44/120    avg_loss:0.306, val_acc:0.952]
Epoch [45/120    avg_loss:0.301, val_acc:0.965]
Epoch [46/120    avg_loss:0.305, val_acc:0.946]
Epoch [47/120    avg_loss:0.260, val_acc:0.946]
Epoch [48/120    avg_loss:0.280, val_acc:0.958]
Epoch [49/120    avg_loss:0.239, val_acc:0.925]
Epoch [50/120    avg_loss:0.290, val_acc:0.919]
Epoch [51/120    avg_loss:0.276, val_acc:0.967]
Epoch [52/120    avg_loss:0.241, val_acc:0.944]
Epoch [53/120    avg_loss:0.351, val_acc:0.965]
Epoch [54/120    avg_loss:0.296, val_acc:0.958]
Epoch [55/120    avg_loss:0.213, val_acc:0.958]
Epoch [56/120    avg_loss:0.228, val_acc:0.965]
Epoch [57/120    avg_loss:0.200, val_acc:0.960]
Epoch [58/120    avg_loss:0.197, val_acc:0.973]
Epoch [59/120    avg_loss:0.207, val_acc:0.965]
Epoch [60/120    avg_loss:0.210, val_acc:0.963]
Epoch [61/120    avg_loss:0.206, val_acc:0.956]
Epoch [62/120    avg_loss:0.216, val_acc:0.967]
Epoch [63/120    avg_loss:0.216, val_acc:0.948]
Epoch [64/120    avg_loss:0.159, val_acc:0.977]
Epoch [65/120    avg_loss:0.222, val_acc:0.948]
Epoch [66/120    avg_loss:0.191, val_acc:0.969]
Epoch [67/120    avg_loss:0.231, val_acc:0.971]
Epoch [68/120    avg_loss:0.163, val_acc:0.981]
Epoch [69/120    avg_loss:0.149, val_acc:0.975]
Epoch [70/120    avg_loss:0.160, val_acc:0.958]
Epoch [71/120    avg_loss:0.199, val_acc:0.973]
Epoch [72/120    avg_loss:0.168, val_acc:0.946]
Epoch [73/120    avg_loss:0.134, val_acc:0.973]
Epoch [74/120    avg_loss:0.109, val_acc:0.967]
Epoch [75/120    avg_loss:0.110, val_acc:0.977]
Epoch [76/120    avg_loss:0.135, val_acc:0.948]
Epoch [77/120    avg_loss:0.172, val_acc:0.977]
Epoch [78/120    avg_loss:0.131, val_acc:0.971]
Epoch [79/120    avg_loss:0.141, val_acc:0.960]
Epoch [80/120    avg_loss:0.169, val_acc:0.979]
Epoch [81/120    avg_loss:0.127, val_acc:0.967]
Epoch [82/120    avg_loss:0.107, val_acc:0.973]
Epoch [83/120    avg_loss:0.091, val_acc:0.977]
Epoch [84/120    avg_loss:0.086, val_acc:0.983]
Epoch [85/120    avg_loss:0.084, val_acc:0.983]
Epoch [86/120    avg_loss:0.080, val_acc:0.983]
Epoch [87/120    avg_loss:0.077, val_acc:0.985]
Epoch [88/120    avg_loss:0.079, val_acc:0.988]
Epoch [89/120    avg_loss:0.077, val_acc:0.990]
Epoch [90/120    avg_loss:0.067, val_acc:0.990]
Epoch [91/120    avg_loss:0.076, val_acc:0.990]
Epoch [92/120    avg_loss:0.077, val_acc:0.990]
Epoch [93/120    avg_loss:0.085, val_acc:0.990]
Epoch [94/120    avg_loss:0.072, val_acc:0.988]
Epoch [95/120    avg_loss:0.069, val_acc:0.988]
Epoch [96/120    avg_loss:0.064, val_acc:0.985]
Epoch [97/120    avg_loss:0.076, val_acc:0.985]
Epoch [98/120    avg_loss:0.065, val_acc:0.988]
Epoch [99/120    avg_loss:0.065, val_acc:0.988]
Epoch [100/120    avg_loss:0.069, val_acc:0.988]
Epoch [101/120    avg_loss:0.061, val_acc:0.988]
Epoch [102/120    avg_loss:0.058, val_acc:0.990]
Epoch [103/120    avg_loss:0.076, val_acc:0.990]
Epoch [104/120    avg_loss:0.083, val_acc:0.990]
Epoch [105/120    avg_loss:0.066, val_acc:0.990]
Epoch [106/120    avg_loss:0.068, val_acc:0.988]
Epoch [107/120    avg_loss:0.060, val_acc:0.990]
Epoch [108/120    avg_loss:0.059, val_acc:0.990]
Epoch [109/120    avg_loss:0.077, val_acc:0.990]
Epoch [110/120    avg_loss:0.061, val_acc:0.990]
Epoch [111/120    avg_loss:0.061, val_acc:0.990]
Epoch [112/120    avg_loss:0.078, val_acc:0.990]
Epoch [113/120    avg_loss:0.058, val_acc:0.990]
Epoch [114/120    avg_loss:0.068, val_acc:0.990]
Epoch [115/120    avg_loss:0.066, val_acc:0.988]
Epoch [116/120    avg_loss:0.052, val_acc:0.988]
Epoch [117/120    avg_loss:0.059, val_acc:0.990]
Epoch [118/120    avg_loss:0.051, val_acc:0.990]
Epoch [119/120    avg_loss:0.062, val_acc:0.990]
Epoch [120/120    avg_loss:0.059, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0 204   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0   0 385   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.96263736 0.99782135 0.9569161  0.9442623
 0.99512195 0.92571429 0.99611902 1.         1.         1.
 1.         1.        ]

Kappa:
0.9914543026629484
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd17be26978>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.590, val_acc:0.269]
Epoch [2/120    avg_loss:2.390, val_acc:0.350]
Epoch [3/120    avg_loss:2.261, val_acc:0.381]
Epoch [4/120    avg_loss:2.167, val_acc:0.431]
Epoch [5/120    avg_loss:2.081, val_acc:0.465]
Epoch [6/120    avg_loss:1.978, val_acc:0.515]
Epoch [7/120    avg_loss:1.866, val_acc:0.544]
Epoch [8/120    avg_loss:1.780, val_acc:0.537]
Epoch [9/120    avg_loss:1.675, val_acc:0.621]
Epoch [10/120    avg_loss:1.594, val_acc:0.648]
Epoch [11/120    avg_loss:1.500, val_acc:0.665]
Epoch [12/120    avg_loss:1.408, val_acc:0.694]
Epoch [13/120    avg_loss:1.324, val_acc:0.706]
Epoch [14/120    avg_loss:1.255, val_acc:0.731]
Epoch [15/120    avg_loss:1.185, val_acc:0.713]
Epoch [16/120    avg_loss:1.088, val_acc:0.800]
Epoch [17/120    avg_loss:1.027, val_acc:0.798]
Epoch [18/120    avg_loss:0.963, val_acc:0.858]
Epoch [19/120    avg_loss:0.921, val_acc:0.860]
Epoch [20/120    avg_loss:0.851, val_acc:0.877]
Epoch [21/120    avg_loss:0.820, val_acc:0.890]
Epoch [22/120    avg_loss:0.758, val_acc:0.902]
Epoch [23/120    avg_loss:0.699, val_acc:0.904]
Epoch [24/120    avg_loss:0.650, val_acc:0.923]
Epoch [25/120    avg_loss:0.621, val_acc:0.900]
Epoch [26/120    avg_loss:0.598, val_acc:0.915]
Epoch [27/120    avg_loss:0.554, val_acc:0.898]
Epoch [28/120    avg_loss:0.548, val_acc:0.921]
Epoch [29/120    avg_loss:0.521, val_acc:0.917]
Epoch [30/120    avg_loss:0.478, val_acc:0.912]
Epoch [31/120    avg_loss:0.447, val_acc:0.938]
Epoch [32/120    avg_loss:0.442, val_acc:0.933]
Epoch [33/120    avg_loss:0.390, val_acc:0.933]
Epoch [34/120    avg_loss:0.366, val_acc:0.925]
Epoch [35/120    avg_loss:0.418, val_acc:0.925]
Epoch [36/120    avg_loss:0.465, val_acc:0.929]
Epoch [37/120    avg_loss:0.407, val_acc:0.833]
Epoch [38/120    avg_loss:0.400, val_acc:0.942]
Epoch [39/120    avg_loss:0.373, val_acc:0.933]
Epoch [40/120    avg_loss:0.340, val_acc:0.933]
Epoch [41/120    avg_loss:0.337, val_acc:0.935]
Epoch [42/120    avg_loss:0.312, val_acc:0.933]
Epoch [43/120    avg_loss:0.311, val_acc:0.950]
Epoch [44/120    avg_loss:0.282, val_acc:0.919]
Epoch [45/120    avg_loss:0.287, val_acc:0.946]
Epoch [46/120    avg_loss:0.293, val_acc:0.948]
Epoch [47/120    avg_loss:0.255, val_acc:0.948]
Epoch [48/120    avg_loss:0.251, val_acc:0.952]
Epoch [49/120    avg_loss:0.242, val_acc:0.948]
Epoch [50/120    avg_loss:0.201, val_acc:0.967]
Epoch [51/120    avg_loss:0.209, val_acc:0.973]
Epoch [52/120    avg_loss:0.247, val_acc:0.958]
Epoch [53/120    avg_loss:0.213, val_acc:0.938]
Epoch [54/120    avg_loss:0.271, val_acc:0.977]
Epoch [55/120    avg_loss:0.231, val_acc:0.952]
Epoch [56/120    avg_loss:0.169, val_acc:0.973]
Epoch [57/120    avg_loss:0.180, val_acc:0.971]
Epoch [58/120    avg_loss:0.181, val_acc:0.983]
Epoch [59/120    avg_loss:0.166, val_acc:0.975]
Epoch [60/120    avg_loss:0.153, val_acc:0.992]
Epoch [61/120    avg_loss:0.124, val_acc:0.983]
Epoch [62/120    avg_loss:0.117, val_acc:0.946]
Epoch [63/120    avg_loss:0.140, val_acc:0.946]
Epoch [64/120    avg_loss:0.159, val_acc:0.990]
Epoch [65/120    avg_loss:0.132, val_acc:0.990]
Epoch [66/120    avg_loss:0.130, val_acc:0.979]
Epoch [67/120    avg_loss:0.135, val_acc:0.965]
Epoch [68/120    avg_loss:0.169, val_acc:0.938]
Epoch [69/120    avg_loss:0.140, val_acc:0.971]
Epoch [70/120    avg_loss:0.151, val_acc:0.981]
Epoch [71/120    avg_loss:0.128, val_acc:0.973]
Epoch [72/120    avg_loss:0.128, val_acc:0.975]
Epoch [73/120    avg_loss:0.110, val_acc:0.990]
Epoch [74/120    avg_loss:0.077, val_acc:0.992]
Epoch [75/120    avg_loss:0.085, val_acc:0.996]
Epoch [76/120    avg_loss:0.080, val_acc:0.998]
Epoch [77/120    avg_loss:0.076, val_acc:0.992]
Epoch [78/120    avg_loss:0.067, val_acc:0.992]
Epoch [79/120    avg_loss:0.072, val_acc:0.996]
Epoch [80/120    avg_loss:0.076, val_acc:0.992]
Epoch [81/120    avg_loss:0.073, val_acc:0.994]
Epoch [82/120    avg_loss:0.063, val_acc:0.996]
Epoch [83/120    avg_loss:0.066, val_acc:0.994]
Epoch [84/120    avg_loss:0.071, val_acc:0.994]
Epoch [85/120    avg_loss:0.064, val_acc:0.996]
Epoch [86/120    avg_loss:0.068, val_acc:0.994]
Epoch [87/120    avg_loss:0.071, val_acc:0.994]
Epoch [88/120    avg_loss:0.071, val_acc:0.992]
Epoch [89/120    avg_loss:0.075, val_acc:0.992]
Epoch [90/120    avg_loss:0.059, val_acc:0.992]
Epoch [91/120    avg_loss:0.063, val_acc:0.994]
Epoch [92/120    avg_loss:0.062, val_acc:0.994]
Epoch [93/120    avg_loss:0.057, val_acc:0.994]
Epoch [94/120    avg_loss:0.069, val_acc:0.994]
Epoch [95/120    avg_loss:0.071, val_acc:0.994]
Epoch [96/120    avg_loss:0.059, val_acc:0.994]
Epoch [97/120    avg_loss:0.066, val_acc:0.994]
Epoch [98/120    avg_loss:0.065, val_acc:0.994]
Epoch [99/120    avg_loss:0.064, val_acc:0.994]
Epoch [100/120    avg_loss:0.061, val_acc:0.994]
Epoch [101/120    avg_loss:0.075, val_acc:0.994]
Epoch [102/120    avg_loss:0.058, val_acc:0.994]
Epoch [103/120    avg_loss:0.056, val_acc:0.994]
Epoch [104/120    avg_loss:0.060, val_acc:0.994]
Epoch [105/120    avg_loss:0.058, val_acc:0.994]
Epoch [106/120    avg_loss:0.056, val_acc:0.994]
Epoch [107/120    avg_loss:0.063, val_acc:0.994]
Epoch [108/120    avg_loss:0.064, val_acc:0.994]
Epoch [109/120    avg_loss:0.061, val_acc:0.994]
Epoch [110/120    avg_loss:0.053, val_acc:0.994]
Epoch [111/120    avg_loss:0.065, val_acc:0.994]
Epoch [112/120    avg_loss:0.058, val_acc:0.994]
Epoch [113/120    avg_loss:0.062, val_acc:0.994]
Epoch [114/120    avg_loss:0.068, val_acc:0.994]
Epoch [115/120    avg_loss:0.060, val_acc:0.994]
Epoch [116/120    avg_loss:0.067, val_acc:0.994]
Epoch [117/120    avg_loss:0.063, val_acc:0.994]
Epoch [118/120    avg_loss:0.060, val_acc:0.994]
Epoch [119/120    avg_loss:0.068, val_acc:0.994]
Epoch [120/120    avg_loss:0.055, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 209   0   0   0   0  10   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 1.         0.95       0.98901099 0.95495495 0.95081967
 1.         0.88172043 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9900304809769096
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcd77aeb898>
supervision:full
center_pixel:True
Network :
Number of parameter: 44952==>0.04M
----------Training process----------
Epoch [1/120    avg_loss:2.609, val_acc:0.252]
Epoch [2/120    avg_loss:2.424, val_acc:0.385]
Epoch [3/120    avg_loss:2.284, val_acc:0.460]
Epoch [4/120    avg_loss:2.158, val_acc:0.490]
Epoch [5/120    avg_loss:2.040, val_acc:0.542]
Epoch [6/120    avg_loss:1.908, val_acc:0.585]
Epoch [7/120    avg_loss:1.791, val_acc:0.606]
Epoch [8/120    avg_loss:1.660, val_acc:0.617]
Epoch [9/120    avg_loss:1.565, val_acc:0.648]
Epoch [10/120    avg_loss:1.463, val_acc:0.671]
Epoch [11/120    avg_loss:1.374, val_acc:0.692]
Epoch [12/120    avg_loss:1.295, val_acc:0.710]
Epoch [13/120    avg_loss:1.211, val_acc:0.717]
Epoch [14/120    avg_loss:1.115, val_acc:0.733]
Epoch [15/120    avg_loss:1.057, val_acc:0.729]
Epoch [16/120    avg_loss:1.028, val_acc:0.740]
Epoch [17/120    avg_loss:0.965, val_acc:0.746]
Epoch [18/120    avg_loss:0.891, val_acc:0.762]
Epoch [19/120    avg_loss:0.835, val_acc:0.779]
Epoch [20/120    avg_loss:0.776, val_acc:0.796]
Epoch [21/120    avg_loss:0.775, val_acc:0.802]
Epoch [22/120    avg_loss:0.725, val_acc:0.858]
Epoch [23/120    avg_loss:0.691, val_acc:0.821]
Epoch [24/120    avg_loss:0.688, val_acc:0.858]
Epoch [25/120    avg_loss:0.682, val_acc:0.827]
Epoch [26/120    avg_loss:0.640, val_acc:0.885]
Epoch [27/120    avg_loss:0.605, val_acc:0.892]
Epoch [28/120    avg_loss:0.590, val_acc:0.838]
Epoch [29/120    avg_loss:0.535, val_acc:0.900]
Epoch [30/120    avg_loss:0.547, val_acc:0.919]
Epoch [31/120    avg_loss:0.519, val_acc:0.915]
Epoch [32/120    avg_loss:0.468, val_acc:0.925]
Epoch [33/120    avg_loss:0.465, val_acc:0.896]
Epoch [34/120    avg_loss:0.465, val_acc:0.898]
Epoch [35/120    avg_loss:0.449, val_acc:0.917]
Epoch [36/120    avg_loss:0.406, val_acc:0.929]
Epoch [37/120    avg_loss:0.386, val_acc:0.931]
Epoch [38/120    avg_loss:0.337, val_acc:0.952]
Epoch [39/120    avg_loss:0.309, val_acc:0.935]
Epoch [40/120    avg_loss:0.323, val_acc:0.942]
Epoch [41/120    avg_loss:0.353, val_acc:0.921]
Epoch [42/120    avg_loss:0.321, val_acc:0.944]
Epoch [43/120    avg_loss:0.332, val_acc:0.933]
Epoch [44/120    avg_loss:0.280, val_acc:0.923]
Epoch [45/120    avg_loss:0.307, val_acc:0.948]
Epoch [46/120    avg_loss:0.267, val_acc:0.952]
Epoch [47/120    avg_loss:0.259, val_acc:0.946]
Epoch [48/120    avg_loss:0.211, val_acc:0.950]
Epoch [49/120    avg_loss:0.272, val_acc:0.933]
Epoch [50/120    avg_loss:0.284, val_acc:0.948]
Epoch [51/120    avg_loss:0.260, val_acc:0.933]
Epoch [52/120    avg_loss:0.258, val_acc:0.954]
Epoch [53/120    avg_loss:0.237, val_acc:0.954]
Epoch [54/120    avg_loss:0.207, val_acc:0.958]
Epoch [55/120    avg_loss:0.229, val_acc:0.942]
Epoch [56/120    avg_loss:0.210, val_acc:0.965]
Epoch [57/120    avg_loss:0.239, val_acc:0.960]
Epoch [58/120    avg_loss:0.205, val_acc:0.954]
Epoch [59/120    avg_loss:0.209, val_acc:0.963]
Epoch [60/120    avg_loss:0.164, val_acc:0.963]
Epoch [61/120    avg_loss:0.176, val_acc:0.954]
Epoch [62/120    avg_loss:0.227, val_acc:0.963]
Epoch [63/120    avg_loss:0.159, val_acc:0.971]
Epoch [64/120    avg_loss:0.147, val_acc:0.965]
Epoch [65/120    avg_loss:0.147, val_acc:0.975]
Epoch [66/120    avg_loss:0.154, val_acc:0.963]
Epoch [67/120    avg_loss:0.176, val_acc:0.917]
Epoch [68/120    avg_loss:0.214, val_acc:0.956]
Epoch [69/120    avg_loss:0.207, val_acc:0.958]
Epoch [70/120    avg_loss:0.182, val_acc:0.965]
Epoch [71/120    avg_loss:0.139, val_acc:0.960]
Epoch [72/120    avg_loss:0.146, val_acc:0.973]
Epoch [73/120    avg_loss:0.165, val_acc:0.956]
Epoch [74/120    avg_loss:0.163, val_acc:0.965]
Epoch [75/120    avg_loss:0.144, val_acc:0.965]
Epoch [76/120    avg_loss:0.157, val_acc:0.965]
Epoch [77/120    avg_loss:0.140, val_acc:0.971]
Epoch [78/120    avg_loss:0.129, val_acc:0.967]
Epoch [79/120    avg_loss:0.120, val_acc:0.977]
Epoch [80/120    avg_loss:0.097, val_acc:0.977]
Epoch [81/120    avg_loss:0.097, val_acc:0.977]
Epoch [82/120    avg_loss:0.101, val_acc:0.979]
Epoch [83/120    avg_loss:0.098, val_acc:0.979]
Epoch [84/120    avg_loss:0.090, val_acc:0.979]
Epoch [85/120    avg_loss:0.093, val_acc:0.977]
Epoch [86/120    avg_loss:0.077, val_acc:0.979]
Epoch [87/120    avg_loss:0.085, val_acc:0.977]
Epoch [88/120    avg_loss:0.074, val_acc:0.977]
Epoch [89/120    avg_loss:0.085, val_acc:0.977]
Epoch [90/120    avg_loss:0.094, val_acc:0.975]
Epoch [91/120    avg_loss:0.080, val_acc:0.977]
Epoch [92/120    avg_loss:0.091, val_acc:0.979]
Epoch [93/120    avg_loss:0.093, val_acc:0.977]
Epoch [94/120    avg_loss:0.074, val_acc:0.977]
Epoch [95/120    avg_loss:0.082, val_acc:0.977]
Epoch [96/120    avg_loss:0.085, val_acc:0.977]
Epoch [97/120    avg_loss:0.078, val_acc:0.977]
Epoch [98/120    avg_loss:0.084, val_acc:0.977]
Epoch [99/120    avg_loss:0.069, val_acc:0.977]
Epoch [100/120    avg_loss:0.085, val_acc:0.977]
Epoch [101/120    avg_loss:0.076, val_acc:0.977]
Epoch [102/120    avg_loss:0.079, val_acc:0.975]
Epoch [103/120    avg_loss:0.078, val_acc:0.975]
Epoch [104/120    avg_loss:0.078, val_acc:0.977]
Epoch [105/120    avg_loss:0.081, val_acc:0.975]
Epoch [106/120    avg_loss:0.080, val_acc:0.975]
Epoch [107/120    avg_loss:0.087, val_acc:0.977]
Epoch [108/120    avg_loss:0.075, val_acc:0.977]
Epoch [109/120    avg_loss:0.069, val_acc:0.977]
Epoch [110/120    avg_loss:0.071, val_acc:0.977]
Epoch [111/120    avg_loss:0.087, val_acc:0.977]
Epoch [112/120    avg_loss:0.067, val_acc:0.977]
Epoch [113/120    avg_loss:0.066, val_acc:0.977]
Epoch [114/120    avg_loss:0.065, val_acc:0.977]
Epoch [115/120    avg_loss:0.057, val_acc:0.977]
Epoch [116/120    avg_loss:0.066, val_acc:0.977]
Epoch [117/120    avg_loss:0.067, val_acc:0.977]
Epoch [118/120    avg_loss:0.066, val_acc:0.977]
Epoch [119/120    avg_loss:0.078, val_acc:0.977]
Epoch [120/120    avg_loss:0.073, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.99786780383795

F1 scores:
[       nan 1.         0.97550111 0.98678414 0.93636364 0.93247588
 0.99756691 0.93785311 1.         1.         1.         0.98950131
 0.99109131 1.        ]

Kappa:
0.9888436091944087
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3ff66248d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.594, val_acc:0.085]
Epoch [2/120    avg_loss:2.435, val_acc:0.321]
Epoch [3/120    avg_loss:2.318, val_acc:0.369]
Epoch [4/120    avg_loss:2.220, val_acc:0.523]
Epoch [5/120    avg_loss:2.144, val_acc:0.579]
Epoch [6/120    avg_loss:2.056, val_acc:0.579]
Epoch [7/120    avg_loss:1.962, val_acc:0.592]
Epoch [8/120    avg_loss:1.877, val_acc:0.627]
Epoch [9/120    avg_loss:1.752, val_acc:0.658]
Epoch [10/120    avg_loss:1.665, val_acc:0.690]
Epoch [11/120    avg_loss:1.560, val_acc:0.702]
Epoch [12/120    avg_loss:1.455, val_acc:0.713]
Epoch [13/120    avg_loss:1.350, val_acc:0.723]
Epoch [14/120    avg_loss:1.231, val_acc:0.748]
Epoch [15/120    avg_loss:1.139, val_acc:0.756]
Epoch [16/120    avg_loss:1.080, val_acc:0.783]
Epoch [17/120    avg_loss:1.022, val_acc:0.781]
Epoch [18/120    avg_loss:0.945, val_acc:0.794]
Epoch [19/120    avg_loss:0.850, val_acc:0.800]
Epoch [20/120    avg_loss:0.810, val_acc:0.856]
Epoch [21/120    avg_loss:0.785, val_acc:0.804]
Epoch [22/120    avg_loss:0.702, val_acc:0.838]
Epoch [23/120    avg_loss:0.674, val_acc:0.894]
Epoch [24/120    avg_loss:0.602, val_acc:0.927]
Epoch [25/120    avg_loss:0.587, val_acc:0.892]
Epoch [26/120    avg_loss:0.576, val_acc:0.912]
Epoch [27/120    avg_loss:0.515, val_acc:0.925]
Epoch [28/120    avg_loss:0.514, val_acc:0.938]
Epoch [29/120    avg_loss:0.429, val_acc:0.946]
Epoch [30/120    avg_loss:0.440, val_acc:0.890]
Epoch [31/120    avg_loss:0.454, val_acc:0.950]
Epoch [32/120    avg_loss:0.408, val_acc:0.929]
Epoch [33/120    avg_loss:0.410, val_acc:0.925]
Epoch [34/120    avg_loss:0.388, val_acc:0.910]
Epoch [35/120    avg_loss:0.361, val_acc:0.950]
Epoch [36/120    avg_loss:0.342, val_acc:0.931]
Epoch [37/120    avg_loss:0.352, val_acc:0.958]
Epoch [38/120    avg_loss:0.335, val_acc:0.931]
Epoch [39/120    avg_loss:0.306, val_acc:0.950]
Epoch [40/120    avg_loss:0.304, val_acc:0.958]
Epoch [41/120    avg_loss:0.249, val_acc:0.958]
Epoch [42/120    avg_loss:0.292, val_acc:0.960]
Epoch [43/120    avg_loss:0.306, val_acc:0.944]
Epoch [44/120    avg_loss:0.269, val_acc:0.973]
Epoch [45/120    avg_loss:0.254, val_acc:0.929]
Epoch [46/120    avg_loss:0.266, val_acc:0.965]
Epoch [47/120    avg_loss:0.292, val_acc:0.946]
Epoch [48/120    avg_loss:0.264, val_acc:0.975]
Epoch [49/120    avg_loss:0.239, val_acc:0.965]
Epoch [50/120    avg_loss:0.199, val_acc:0.979]
Epoch [51/120    avg_loss:0.215, val_acc:0.963]
Epoch [52/120    avg_loss:0.189, val_acc:0.975]
Epoch [53/120    avg_loss:0.196, val_acc:0.958]
Epoch [54/120    avg_loss:0.211, val_acc:0.973]
Epoch [55/120    avg_loss:0.179, val_acc:0.971]
Epoch [56/120    avg_loss:0.213, val_acc:0.933]
Epoch [57/120    avg_loss:0.227, val_acc:0.973]
Epoch [58/120    avg_loss:0.243, val_acc:0.979]
Epoch [59/120    avg_loss:0.193, val_acc:0.975]
Epoch [60/120    avg_loss:0.173, val_acc:0.977]
Epoch [61/120    avg_loss:0.199, val_acc:0.971]
Epoch [62/120    avg_loss:0.183, val_acc:0.969]
Epoch [63/120    avg_loss:0.166, val_acc:0.975]
Epoch [64/120    avg_loss:0.158, val_acc:0.967]
Epoch [65/120    avg_loss:0.181, val_acc:0.965]
Epoch [66/120    avg_loss:0.193, val_acc:0.977]
Epoch [67/120    avg_loss:0.186, val_acc:0.965]
Epoch [68/120    avg_loss:0.189, val_acc:0.969]
Epoch [69/120    avg_loss:0.158, val_acc:0.977]
Epoch [70/120    avg_loss:0.157, val_acc:0.973]
Epoch [71/120    avg_loss:0.151, val_acc:0.973]
Epoch [72/120    avg_loss:0.156, val_acc:0.981]
Epoch [73/120    avg_loss:0.109, val_acc:0.979]
Epoch [74/120    avg_loss:0.115, val_acc:0.981]
Epoch [75/120    avg_loss:0.098, val_acc:0.979]
Epoch [76/120    avg_loss:0.108, val_acc:0.981]
Epoch [77/120    avg_loss:0.094, val_acc:0.977]
Epoch [78/120    avg_loss:0.100, val_acc:0.981]
Epoch [79/120    avg_loss:0.109, val_acc:0.983]
Epoch [80/120    avg_loss:0.090, val_acc:0.981]
Epoch [81/120    avg_loss:0.101, val_acc:0.981]
Epoch [82/120    avg_loss:0.101, val_acc:0.981]
Epoch [83/120    avg_loss:0.091, val_acc:0.981]
Epoch [84/120    avg_loss:0.094, val_acc:0.983]
Epoch [85/120    avg_loss:0.098, val_acc:0.983]
Epoch [86/120    avg_loss:0.090, val_acc:0.981]
Epoch [87/120    avg_loss:0.088, val_acc:0.981]
Epoch [88/120    avg_loss:0.101, val_acc:0.985]
Epoch [89/120    avg_loss:0.092, val_acc:0.983]
Epoch [90/120    avg_loss:0.119, val_acc:0.985]
Epoch [91/120    avg_loss:0.085, val_acc:0.988]
Epoch [92/120    avg_loss:0.092, val_acc:0.985]
Epoch [93/120    avg_loss:0.107, val_acc:0.985]
Epoch [94/120    avg_loss:0.092, val_acc:0.988]
Epoch [95/120    avg_loss:0.080, val_acc:0.983]
Epoch [96/120    avg_loss:0.090, val_acc:0.985]
Epoch [97/120    avg_loss:0.088, val_acc:0.981]
Epoch [98/120    avg_loss:0.079, val_acc:0.983]
Epoch [99/120    avg_loss:0.089, val_acc:0.983]
Epoch [100/120    avg_loss:0.093, val_acc:0.985]
Epoch [101/120    avg_loss:0.088, val_acc:0.983]
Epoch [102/120    avg_loss:0.094, val_acc:0.981]
Epoch [103/120    avg_loss:0.088, val_acc:0.981]
Epoch [104/120    avg_loss:0.082, val_acc:0.983]
Epoch [105/120    avg_loss:0.081, val_acc:0.981]
Epoch [106/120    avg_loss:0.092, val_acc:0.985]
Epoch [107/120    avg_loss:0.083, val_acc:0.985]
Epoch [108/120    avg_loss:0.078, val_acc:0.985]
Epoch [109/120    avg_loss:0.088, val_acc:0.983]
Epoch [110/120    avg_loss:0.073, val_acc:0.983]
Epoch [111/120    avg_loss:0.084, val_acc:0.983]
Epoch [112/120    avg_loss:0.068, val_acc:0.983]
Epoch [113/120    avg_loss:0.088, val_acc:0.983]
Epoch [114/120    avg_loss:0.074, val_acc:0.983]
Epoch [115/120    avg_loss:0.080, val_acc:0.983]
Epoch [116/120    avg_loss:0.074, val_acc:0.985]
Epoch [117/120    avg_loss:0.073, val_acc:0.985]
Epoch [118/120    avg_loss:0.086, val_acc:0.985]
Epoch [119/120    avg_loss:0.076, val_acc:0.985]
Epoch [120/120    avg_loss:0.068, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   6 202  17   0   0   0   0   0   0   2   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 0.99926954 0.98206278 0.98712446 0.92237443 0.91638796
 0.99514563 0.95555556 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9897924558823739
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7292a1f860>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.556, val_acc:0.327]
Epoch [2/120    avg_loss:2.374, val_acc:0.331]
Epoch [3/120    avg_loss:2.253, val_acc:0.427]
Epoch [4/120    avg_loss:2.156, val_acc:0.477]
Epoch [5/120    avg_loss:2.071, val_acc:0.500]
Epoch [6/120    avg_loss:1.986, val_acc:0.573]
Epoch [7/120    avg_loss:1.883, val_acc:0.608]
Epoch [8/120    avg_loss:1.775, val_acc:0.613]
Epoch [9/120    avg_loss:1.651, val_acc:0.644]
Epoch [10/120    avg_loss:1.541, val_acc:0.654]
Epoch [11/120    avg_loss:1.442, val_acc:0.677]
Epoch [12/120    avg_loss:1.336, val_acc:0.688]
Epoch [13/120    avg_loss:1.259, val_acc:0.700]
Epoch [14/120    avg_loss:1.169, val_acc:0.706]
Epoch [15/120    avg_loss:1.085, val_acc:0.713]
Epoch [16/120    avg_loss:1.029, val_acc:0.721]
Epoch [17/120    avg_loss:0.984, val_acc:0.719]
Epoch [18/120    avg_loss:0.924, val_acc:0.729]
Epoch [19/120    avg_loss:0.874, val_acc:0.740]
Epoch [20/120    avg_loss:0.815, val_acc:0.783]
Epoch [21/120    avg_loss:0.778, val_acc:0.808]
Epoch [22/120    avg_loss:0.735, val_acc:0.785]
Epoch [23/120    avg_loss:0.665, val_acc:0.840]
Epoch [24/120    avg_loss:0.655, val_acc:0.902]
Epoch [25/120    avg_loss:0.608, val_acc:0.867]
Epoch [26/120    avg_loss:0.599, val_acc:0.908]
Epoch [27/120    avg_loss:0.556, val_acc:0.921]
Epoch [28/120    avg_loss:0.569, val_acc:0.912]
Epoch [29/120    avg_loss:0.517, val_acc:0.919]
Epoch [30/120    avg_loss:0.480, val_acc:0.942]
Epoch [31/120    avg_loss:0.477, val_acc:0.898]
Epoch [32/120    avg_loss:0.428, val_acc:0.910]
Epoch [33/120    avg_loss:0.393, val_acc:0.890]
Epoch [34/120    avg_loss:0.465, val_acc:0.923]
Epoch [35/120    avg_loss:0.354, val_acc:0.921]
Epoch [36/120    avg_loss:0.340, val_acc:0.942]
Epoch [37/120    avg_loss:0.329, val_acc:0.925]
Epoch [38/120    avg_loss:0.324, val_acc:0.958]
Epoch [39/120    avg_loss:0.369, val_acc:0.912]
Epoch [40/120    avg_loss:0.374, val_acc:0.938]
Epoch [41/120    avg_loss:0.340, val_acc:0.950]
Epoch [42/120    avg_loss:0.363, val_acc:0.933]
Epoch [43/120    avg_loss:0.301, val_acc:0.960]
Epoch [44/120    avg_loss:0.311, val_acc:0.969]
Epoch [45/120    avg_loss:0.278, val_acc:0.940]
Epoch [46/120    avg_loss:0.276, val_acc:0.944]
Epoch [47/120    avg_loss:0.349, val_acc:0.948]
Epoch [48/120    avg_loss:0.295, val_acc:0.944]
Epoch [49/120    avg_loss:0.273, val_acc:0.956]
Epoch [50/120    avg_loss:0.290, val_acc:0.960]
Epoch [51/120    avg_loss:0.202, val_acc:0.963]
Epoch [52/120    avg_loss:0.192, val_acc:0.965]
Epoch [53/120    avg_loss:0.174, val_acc:0.967]
Epoch [54/120    avg_loss:0.191, val_acc:0.965]
Epoch [55/120    avg_loss:0.183, val_acc:0.965]
Epoch [56/120    avg_loss:0.176, val_acc:0.967]
Epoch [57/120    avg_loss:0.184, val_acc:0.956]
Epoch [58/120    avg_loss:0.164, val_acc:0.973]
Epoch [59/120    avg_loss:0.134, val_acc:0.973]
Epoch [60/120    avg_loss:0.137, val_acc:0.975]
Epoch [61/120    avg_loss:0.119, val_acc:0.977]
Epoch [62/120    avg_loss:0.118, val_acc:0.975]
Epoch [63/120    avg_loss:0.128, val_acc:0.979]
Epoch [64/120    avg_loss:0.113, val_acc:0.977]
Epoch [65/120    avg_loss:0.124, val_acc:0.975]
Epoch [66/120    avg_loss:0.108, val_acc:0.977]
Epoch [67/120    avg_loss:0.123, val_acc:0.975]
Epoch [68/120    avg_loss:0.115, val_acc:0.971]
Epoch [69/120    avg_loss:0.117, val_acc:0.977]
Epoch [70/120    avg_loss:0.140, val_acc:0.979]
Epoch [71/120    avg_loss:0.101, val_acc:0.977]
Epoch [72/120    avg_loss:0.107, val_acc:0.977]
Epoch [73/120    avg_loss:0.104, val_acc:0.979]
Epoch [74/120    avg_loss:0.103, val_acc:0.979]
Epoch [75/120    avg_loss:0.119, val_acc:0.977]
Epoch [76/120    avg_loss:0.110, val_acc:0.979]
Epoch [77/120    avg_loss:0.101, val_acc:0.979]
Epoch [78/120    avg_loss:0.130, val_acc:0.977]
Epoch [79/120    avg_loss:0.105, val_acc:0.975]
Epoch [80/120    avg_loss:0.105, val_acc:0.979]
Epoch [81/120    avg_loss:0.094, val_acc:0.975]
Epoch [82/120    avg_loss:0.114, val_acc:0.977]
Epoch [83/120    avg_loss:0.110, val_acc:0.979]
Epoch [84/120    avg_loss:0.091, val_acc:0.979]
Epoch [85/120    avg_loss:0.112, val_acc:0.975]
Epoch [86/120    avg_loss:0.093, val_acc:0.977]
Epoch [87/120    avg_loss:0.096, val_acc:0.975]
Epoch [88/120    avg_loss:0.095, val_acc:0.977]
Epoch [89/120    avg_loss:0.106, val_acc:0.975]
Epoch [90/120    avg_loss:0.104, val_acc:0.977]
Epoch [91/120    avg_loss:0.101, val_acc:0.977]
Epoch [92/120    avg_loss:0.099, val_acc:0.975]
Epoch [93/120    avg_loss:0.108, val_acc:0.979]
Epoch [94/120    avg_loss:0.095, val_acc:0.975]
Epoch [95/120    avg_loss:0.100, val_acc:0.975]
Epoch [96/120    avg_loss:0.092, val_acc:0.977]
Epoch [97/120    avg_loss:0.089, val_acc:0.979]
Epoch [98/120    avg_loss:0.078, val_acc:0.977]
Epoch [99/120    avg_loss:0.101, val_acc:0.975]
Epoch [100/120    avg_loss:0.084, val_acc:0.977]
Epoch [101/120    avg_loss:0.075, val_acc:0.977]
Epoch [102/120    avg_loss:0.093, val_acc:0.979]
Epoch [103/120    avg_loss:0.089, val_acc:0.977]
Epoch [104/120    avg_loss:0.092, val_acc:0.979]
Epoch [105/120    avg_loss:0.079, val_acc:0.981]
Epoch [106/120    avg_loss:0.080, val_acc:0.977]
Epoch [107/120    avg_loss:0.088, val_acc:0.979]
Epoch [108/120    avg_loss:0.089, val_acc:0.977]
Epoch [109/120    avg_loss:0.095, val_acc:0.973]
Epoch [110/120    avg_loss:0.096, val_acc:0.977]
Epoch [111/120    avg_loss:0.092, val_acc:0.975]
Epoch [112/120    avg_loss:0.091, val_acc:0.977]
Epoch [113/120    avg_loss:0.084, val_acc:0.979]
Epoch [114/120    avg_loss:0.102, val_acc:0.975]
Epoch [115/120    avg_loss:0.085, val_acc:0.977]
Epoch [116/120    avg_loss:0.082, val_acc:0.975]
Epoch [117/120    avg_loss:0.075, val_acc:0.975]
Epoch [118/120    avg_loss:0.076, val_acc:0.979]
Epoch [119/120    avg_loss:0.076, val_acc:0.979]
Epoch [120/120    avg_loss:0.072, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99780541 0.98871332 1.         0.94712644 0.92556634
 0.99277108 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9926420575315289
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff628a52978>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.580, val_acc:0.296]
Epoch [2/120    avg_loss:2.374, val_acc:0.323]
Epoch [3/120    avg_loss:2.222, val_acc:0.350]
Epoch [4/120    avg_loss:2.098, val_acc:0.421]
Epoch [5/120    avg_loss:1.987, val_acc:0.485]
Epoch [6/120    avg_loss:1.865, val_acc:0.567]
Epoch [7/120    avg_loss:1.766, val_acc:0.577]
Epoch [8/120    avg_loss:1.695, val_acc:0.644]
Epoch [9/120    avg_loss:1.607, val_acc:0.671]
Epoch [10/120    avg_loss:1.515, val_acc:0.696]
Epoch [11/120    avg_loss:1.450, val_acc:0.713]
Epoch [12/120    avg_loss:1.339, val_acc:0.729]
Epoch [13/120    avg_loss:1.249, val_acc:0.742]
Epoch [14/120    avg_loss:1.178, val_acc:0.729]
Epoch [15/120    avg_loss:1.154, val_acc:0.735]
Epoch [16/120    avg_loss:1.051, val_acc:0.746]
Epoch [17/120    avg_loss:0.972, val_acc:0.756]
Epoch [18/120    avg_loss:0.932, val_acc:0.752]
Epoch [19/120    avg_loss:0.919, val_acc:0.760]
Epoch [20/120    avg_loss:0.828, val_acc:0.760]
Epoch [21/120    avg_loss:0.800, val_acc:0.767]
Epoch [22/120    avg_loss:0.725, val_acc:0.781]
Epoch [23/120    avg_loss:0.726, val_acc:0.785]
Epoch [24/120    avg_loss:0.705, val_acc:0.750]
Epoch [25/120    avg_loss:0.722, val_acc:0.840]
Epoch [26/120    avg_loss:0.628, val_acc:0.798]
Epoch [27/120    avg_loss:0.612, val_acc:0.873]
Epoch [28/120    avg_loss:0.551, val_acc:0.825]
Epoch [29/120    avg_loss:0.561, val_acc:0.863]
Epoch [30/120    avg_loss:0.536, val_acc:0.879]
Epoch [31/120    avg_loss:0.474, val_acc:0.840]
Epoch [32/120    avg_loss:0.439, val_acc:0.887]
Epoch [33/120    avg_loss:0.442, val_acc:0.927]
Epoch [34/120    avg_loss:0.421, val_acc:0.938]
Epoch [35/120    avg_loss:0.478, val_acc:0.894]
Epoch [36/120    avg_loss:0.442, val_acc:0.890]
Epoch [37/120    avg_loss:0.418, val_acc:0.927]
Epoch [38/120    avg_loss:0.351, val_acc:0.931]
Epoch [39/120    avg_loss:0.326, val_acc:0.950]
Epoch [40/120    avg_loss:0.320, val_acc:0.940]
Epoch [41/120    avg_loss:0.309, val_acc:0.950]
Epoch [42/120    avg_loss:0.274, val_acc:0.958]
Epoch [43/120    avg_loss:0.286, val_acc:0.938]
Epoch [44/120    avg_loss:0.284, val_acc:0.948]
Epoch [45/120    avg_loss:0.253, val_acc:0.933]
Epoch [46/120    avg_loss:0.275, val_acc:0.927]
Epoch [47/120    avg_loss:0.284, val_acc:0.933]
Epoch [48/120    avg_loss:0.256, val_acc:0.960]
Epoch [49/120    avg_loss:0.224, val_acc:0.965]
Epoch [50/120    avg_loss:0.222, val_acc:0.956]
Epoch [51/120    avg_loss:0.243, val_acc:0.917]
Epoch [52/120    avg_loss:0.261, val_acc:0.967]
Epoch [53/120    avg_loss:0.211, val_acc:0.960]
Epoch [54/120    avg_loss:0.207, val_acc:0.942]
Epoch [55/120    avg_loss:0.226, val_acc:0.944]
Epoch [56/120    avg_loss:0.184, val_acc:0.956]
Epoch [57/120    avg_loss:0.206, val_acc:0.944]
Epoch [58/120    avg_loss:0.242, val_acc:0.933]
Epoch [59/120    avg_loss:0.233, val_acc:0.967]
Epoch [60/120    avg_loss:0.188, val_acc:0.950]
Epoch [61/120    avg_loss:0.167, val_acc:0.958]
Epoch [62/120    avg_loss:0.172, val_acc:0.965]
Epoch [63/120    avg_loss:0.166, val_acc:0.967]
Epoch [64/120    avg_loss:0.191, val_acc:0.958]
Epoch [65/120    avg_loss:0.158, val_acc:0.971]
Epoch [66/120    avg_loss:0.163, val_acc:0.952]
Epoch [67/120    avg_loss:0.123, val_acc:0.958]
Epoch [68/120    avg_loss:0.121, val_acc:0.960]
Epoch [69/120    avg_loss:0.146, val_acc:0.969]
Epoch [70/120    avg_loss:0.194, val_acc:0.963]
Epoch [71/120    avg_loss:0.137, val_acc:0.956]
Epoch [72/120    avg_loss:0.159, val_acc:0.973]
Epoch [73/120    avg_loss:0.130, val_acc:0.969]
Epoch [74/120    avg_loss:0.142, val_acc:0.969]
Epoch [75/120    avg_loss:0.124, val_acc:0.971]
Epoch [76/120    avg_loss:0.145, val_acc:0.979]
Epoch [77/120    avg_loss:0.123, val_acc:0.963]
Epoch [78/120    avg_loss:0.127, val_acc:0.981]
Epoch [79/120    avg_loss:0.107, val_acc:0.979]
Epoch [80/120    avg_loss:0.100, val_acc:0.963]
Epoch [81/120    avg_loss:0.106, val_acc:0.975]
Epoch [82/120    avg_loss:0.115, val_acc:0.973]
Epoch [83/120    avg_loss:0.079, val_acc:0.983]
Epoch [84/120    avg_loss:0.080, val_acc:0.975]
Epoch [85/120    avg_loss:0.082, val_acc:0.979]
Epoch [86/120    avg_loss:0.106, val_acc:0.969]
Epoch [87/120    avg_loss:0.156, val_acc:0.967]
Epoch [88/120    avg_loss:0.114, val_acc:0.975]
Epoch [89/120    avg_loss:0.086, val_acc:0.977]
Epoch [90/120    avg_loss:0.066, val_acc:0.983]
Epoch [91/120    avg_loss:0.081, val_acc:0.981]
Epoch [92/120    avg_loss:0.088, val_acc:0.954]
Epoch [93/120    avg_loss:0.088, val_acc:0.977]
Epoch [94/120    avg_loss:0.067, val_acc:0.971]
Epoch [95/120    avg_loss:0.120, val_acc:0.977]
Epoch [96/120    avg_loss:0.094, val_acc:0.971]
Epoch [97/120    avg_loss:0.092, val_acc:0.971]
Epoch [98/120    avg_loss:0.103, val_acc:0.977]
Epoch [99/120    avg_loss:0.098, val_acc:0.942]
Epoch [100/120    avg_loss:0.100, val_acc:0.967]
Epoch [101/120    avg_loss:0.087, val_acc:0.977]
Epoch [102/120    avg_loss:0.079, val_acc:0.977]
Epoch [103/120    avg_loss:0.114, val_acc:0.950]
Epoch [104/120    avg_loss:0.104, val_acc:0.971]
Epoch [105/120    avg_loss:0.067, val_acc:0.983]
Epoch [106/120    avg_loss:0.049, val_acc:0.983]
Epoch [107/120    avg_loss:0.056, val_acc:0.983]
Epoch [108/120    avg_loss:0.055, val_acc:0.981]
Epoch [109/120    avg_loss:0.057, val_acc:0.979]
Epoch [110/120    avg_loss:0.037, val_acc:0.981]
Epoch [111/120    avg_loss:0.034, val_acc:0.981]
Epoch [112/120    avg_loss:0.056, val_acc:0.981]
Epoch [113/120    avg_loss:0.053, val_acc:0.981]
Epoch [114/120    avg_loss:0.039, val_acc:0.981]
Epoch [115/120    avg_loss:0.055, val_acc:0.981]
Epoch [116/120    avg_loss:0.033, val_acc:0.983]
Epoch [117/120    avg_loss:0.037, val_acc:0.983]
Epoch [118/120    avg_loss:0.033, val_acc:0.985]
Epoch [119/120    avg_loss:0.050, val_acc:0.985]
Epoch [120/120    avg_loss:0.036, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 0.99926954 0.99095023 1.         0.9321663  0.89198606
 0.99757869 0.97826087 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9914541521386663
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8cccbab908>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.672, val_acc:0.092]
Epoch [2/120    avg_loss:2.482, val_acc:0.344]
Epoch [3/120    avg_loss:2.327, val_acc:0.435]
Epoch [4/120    avg_loss:2.207, val_acc:0.485]
Epoch [5/120    avg_loss:2.107, val_acc:0.537]
Epoch [6/120    avg_loss:2.007, val_acc:0.575]
Epoch [7/120    avg_loss:1.926, val_acc:0.594]
Epoch [8/120    avg_loss:1.813, val_acc:0.594]
Epoch [9/120    avg_loss:1.722, val_acc:0.631]
Epoch [10/120    avg_loss:1.623, val_acc:0.665]
Epoch [11/120    avg_loss:1.498, val_acc:0.696]
Epoch [12/120    avg_loss:1.385, val_acc:0.715]
Epoch [13/120    avg_loss:1.279, val_acc:0.723]
Epoch [14/120    avg_loss:1.194, val_acc:0.735]
Epoch [15/120    avg_loss:1.097, val_acc:0.765]
Epoch [16/120    avg_loss:1.019, val_acc:0.783]
Epoch [17/120    avg_loss:0.940, val_acc:0.785]
Epoch [18/120    avg_loss:0.880, val_acc:0.806]
Epoch [19/120    avg_loss:0.843, val_acc:0.817]
Epoch [20/120    avg_loss:0.753, val_acc:0.812]
Epoch [21/120    avg_loss:0.735, val_acc:0.815]
Epoch [22/120    avg_loss:0.688, val_acc:0.858]
Epoch [23/120    avg_loss:0.627, val_acc:0.806]
Epoch [24/120    avg_loss:0.595, val_acc:0.883]
Epoch [25/120    avg_loss:0.575, val_acc:0.794]
Epoch [26/120    avg_loss:0.555, val_acc:0.917]
Epoch [27/120    avg_loss:0.503, val_acc:0.929]
Epoch [28/120    avg_loss:0.474, val_acc:0.915]
Epoch [29/120    avg_loss:0.423, val_acc:0.935]
Epoch [30/120    avg_loss:0.409, val_acc:0.940]
Epoch [31/120    avg_loss:0.372, val_acc:0.956]
Epoch [32/120    avg_loss:0.381, val_acc:0.933]
Epoch [33/120    avg_loss:0.371, val_acc:0.935]
Epoch [34/120    avg_loss:0.353, val_acc:0.954]
Epoch [35/120    avg_loss:0.317, val_acc:0.946]
Epoch [36/120    avg_loss:0.346, val_acc:0.944]
Epoch [37/120    avg_loss:0.352, val_acc:0.933]
Epoch [38/120    avg_loss:0.318, val_acc:0.954]
Epoch [39/120    avg_loss:0.311, val_acc:0.950]
Epoch [40/120    avg_loss:0.299, val_acc:0.940]
Epoch [41/120    avg_loss:0.286, val_acc:0.960]
Epoch [42/120    avg_loss:0.276, val_acc:0.958]
Epoch [43/120    avg_loss:0.283, val_acc:0.887]
Epoch [44/120    avg_loss:0.281, val_acc:0.915]
Epoch [45/120    avg_loss:0.259, val_acc:0.963]
Epoch [46/120    avg_loss:0.228, val_acc:0.948]
Epoch [47/120    avg_loss:0.271, val_acc:0.917]
Epoch [48/120    avg_loss:0.249, val_acc:0.967]
Epoch [49/120    avg_loss:0.227, val_acc:0.963]
Epoch [50/120    avg_loss:0.216, val_acc:0.967]
Epoch [51/120    avg_loss:0.176, val_acc:0.965]
Epoch [52/120    avg_loss:0.220, val_acc:0.952]
Epoch [53/120    avg_loss:0.168, val_acc:0.969]
Epoch [54/120    avg_loss:0.223, val_acc:0.858]
Epoch [55/120    avg_loss:0.196, val_acc:0.954]
Epoch [56/120    avg_loss:0.163, val_acc:0.979]
Epoch [57/120    avg_loss:0.155, val_acc:0.969]
Epoch [58/120    avg_loss:0.185, val_acc:0.973]
Epoch [59/120    avg_loss:0.140, val_acc:0.981]
Epoch [60/120    avg_loss:0.126, val_acc:0.981]
Epoch [61/120    avg_loss:0.108, val_acc:0.981]
Epoch [62/120    avg_loss:0.171, val_acc:0.954]
Epoch [63/120    avg_loss:0.182, val_acc:0.971]
Epoch [64/120    avg_loss:0.146, val_acc:0.967]
Epoch [65/120    avg_loss:0.141, val_acc:0.979]
Epoch [66/120    avg_loss:0.122, val_acc:0.977]
Epoch [67/120    avg_loss:0.126, val_acc:0.977]
Epoch [68/120    avg_loss:0.102, val_acc:0.977]
Epoch [69/120    avg_loss:0.104, val_acc:0.985]
Epoch [70/120    avg_loss:0.115, val_acc:0.935]
Epoch [71/120    avg_loss:0.115, val_acc:0.975]
Epoch [72/120    avg_loss:0.103, val_acc:0.985]
Epoch [73/120    avg_loss:0.097, val_acc:0.963]
Epoch [74/120    avg_loss:0.118, val_acc:0.983]
Epoch [75/120    avg_loss:0.123, val_acc:0.958]
Epoch [76/120    avg_loss:0.084, val_acc:0.981]
Epoch [77/120    avg_loss:0.085, val_acc:0.988]
Epoch [78/120    avg_loss:0.089, val_acc:0.994]
Epoch [79/120    avg_loss:0.071, val_acc:0.981]
Epoch [80/120    avg_loss:0.070, val_acc:0.985]
Epoch [81/120    avg_loss:0.086, val_acc:0.977]
Epoch [82/120    avg_loss:0.071, val_acc:0.985]
Epoch [83/120    avg_loss:0.060, val_acc:0.981]
Epoch [84/120    avg_loss:0.055, val_acc:0.988]
Epoch [85/120    avg_loss:0.102, val_acc:0.967]
Epoch [86/120    avg_loss:0.100, val_acc:0.985]
Epoch [87/120    avg_loss:0.080, val_acc:0.994]
Epoch [88/120    avg_loss:0.051, val_acc:0.992]
Epoch [89/120    avg_loss:0.078, val_acc:0.983]
Epoch [90/120    avg_loss:0.049, val_acc:0.981]
Epoch [91/120    avg_loss:0.042, val_acc:0.996]
Epoch [92/120    avg_loss:0.048, val_acc:0.985]
Epoch [93/120    avg_loss:0.035, val_acc:0.996]
Epoch [94/120    avg_loss:0.038, val_acc:0.990]
Epoch [95/120    avg_loss:0.037, val_acc:0.996]
Epoch [96/120    avg_loss:0.042, val_acc:0.979]
Epoch [97/120    avg_loss:0.068, val_acc:0.990]
Epoch [98/120    avg_loss:0.034, val_acc:0.994]
Epoch [99/120    avg_loss:0.026, val_acc:0.992]
Epoch [100/120    avg_loss:0.029, val_acc:0.996]
Epoch [101/120    avg_loss:0.030, val_acc:0.988]
Epoch [102/120    avg_loss:0.032, val_acc:0.994]
Epoch [103/120    avg_loss:0.032, val_acc:0.992]
Epoch [104/120    avg_loss:0.044, val_acc:0.990]
Epoch [105/120    avg_loss:0.026, val_acc:0.988]
Epoch [106/120    avg_loss:0.024, val_acc:0.988]
Epoch [107/120    avg_loss:0.027, val_acc:0.998]
Epoch [108/120    avg_loss:0.024, val_acc:0.990]
Epoch [109/120    avg_loss:0.029, val_acc:0.985]
Epoch [110/120    avg_loss:0.032, val_acc:0.988]
Epoch [111/120    avg_loss:0.054, val_acc:0.985]
Epoch [112/120    avg_loss:0.043, val_acc:0.979]
Epoch [113/120    avg_loss:0.053, val_acc:0.996]
Epoch [114/120    avg_loss:0.042, val_acc:0.990]
Epoch [115/120    avg_loss:0.032, val_acc:0.996]
Epoch [116/120    avg_loss:0.030, val_acc:0.990]
Epoch [117/120    avg_loss:0.027, val_acc:0.994]
Epoch [118/120    avg_loss:0.028, val_acc:0.994]
Epoch [119/120    avg_loss:0.026, val_acc:0.990]
Epoch [120/120    avg_loss:0.023, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 220  10   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  11 442   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 0.99926954 1.         0.97777778 0.91404612 0.89208633
 0.99514563 1.         1.         1.         1.         0.98562092
 0.9877095  1.        ]

Kappa:
0.9874189994012421
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbbdcb08908>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.569, val_acc:0.375]
Epoch [2/120    avg_loss:2.393, val_acc:0.329]
Epoch [3/120    avg_loss:2.253, val_acc:0.379]
Epoch [4/120    avg_loss:2.142, val_acc:0.467]
Epoch [5/120    avg_loss:2.043, val_acc:0.529]
Epoch [6/120    avg_loss:1.937, val_acc:0.527]
Epoch [7/120    avg_loss:1.847, val_acc:0.558]
Epoch [8/120    avg_loss:1.724, val_acc:0.575]
Epoch [9/120    avg_loss:1.638, val_acc:0.602]
Epoch [10/120    avg_loss:1.530, val_acc:0.633]
Epoch [11/120    avg_loss:1.439, val_acc:0.646]
Epoch [12/120    avg_loss:1.370, val_acc:0.681]
Epoch [13/120    avg_loss:1.307, val_acc:0.717]
Epoch [14/120    avg_loss:1.224, val_acc:0.713]
Epoch [15/120    avg_loss:1.164, val_acc:0.742]
Epoch [16/120    avg_loss:1.137, val_acc:0.756]
Epoch [17/120    avg_loss:1.056, val_acc:0.765]
Epoch [18/120    avg_loss:0.961, val_acc:0.800]
Epoch [19/120    avg_loss:0.940, val_acc:0.815]
Epoch [20/120    avg_loss:0.869, val_acc:0.835]
Epoch [21/120    avg_loss:0.847, val_acc:0.792]
Epoch [22/120    avg_loss:0.796, val_acc:0.848]
Epoch [23/120    avg_loss:0.738, val_acc:0.829]
Epoch [24/120    avg_loss:0.727, val_acc:0.896]
Epoch [25/120    avg_loss:0.680, val_acc:0.883]
Epoch [26/120    avg_loss:0.616, val_acc:0.894]
Epoch [27/120    avg_loss:0.557, val_acc:0.902]
Epoch [28/120    avg_loss:0.538, val_acc:0.927]
Epoch [29/120    avg_loss:0.534, val_acc:0.829]
Epoch [30/120    avg_loss:0.573, val_acc:0.890]
Epoch [31/120    avg_loss:0.498, val_acc:0.892]
Epoch [32/120    avg_loss:0.523, val_acc:0.892]
Epoch [33/120    avg_loss:0.499, val_acc:0.927]
Epoch [34/120    avg_loss:0.483, val_acc:0.927]
Epoch [35/120    avg_loss:0.487, val_acc:0.908]
Epoch [36/120    avg_loss:0.435, val_acc:0.929]
Epoch [37/120    avg_loss:0.389, val_acc:0.927]
Epoch [38/120    avg_loss:0.371, val_acc:0.954]
Epoch [39/120    avg_loss:0.326, val_acc:0.929]
Epoch [40/120    avg_loss:0.370, val_acc:0.952]
Epoch [41/120    avg_loss:0.302, val_acc:0.944]
Epoch [42/120    avg_loss:0.264, val_acc:0.963]
Epoch [43/120    avg_loss:0.267, val_acc:0.948]
Epoch [44/120    avg_loss:0.253, val_acc:0.923]
Epoch [45/120    avg_loss:0.284, val_acc:0.950]
Epoch [46/120    avg_loss:0.253, val_acc:0.963]
Epoch [47/120    avg_loss:0.288, val_acc:0.908]
Epoch [48/120    avg_loss:0.451, val_acc:0.946]
Epoch [49/120    avg_loss:0.281, val_acc:0.944]
Epoch [50/120    avg_loss:0.252, val_acc:0.956]
Epoch [51/120    avg_loss:0.260, val_acc:0.965]
Epoch [52/120    avg_loss:0.209, val_acc:0.963]
Epoch [53/120    avg_loss:0.210, val_acc:0.969]
Epoch [54/120    avg_loss:0.186, val_acc:0.979]
Epoch [55/120    avg_loss:0.172, val_acc:0.921]
Epoch [56/120    avg_loss:0.228, val_acc:0.969]
Epoch [57/120    avg_loss:0.222, val_acc:0.960]
Epoch [58/120    avg_loss:0.217, val_acc:0.969]
Epoch [59/120    avg_loss:0.188, val_acc:0.967]
Epoch [60/120    avg_loss:0.222, val_acc:0.954]
Epoch [61/120    avg_loss:0.216, val_acc:0.975]
Epoch [62/120    avg_loss:0.187, val_acc:0.956]
Epoch [63/120    avg_loss:0.172, val_acc:0.981]
Epoch [64/120    avg_loss:0.165, val_acc:0.975]
Epoch [65/120    avg_loss:0.129, val_acc:0.973]
Epoch [66/120    avg_loss:0.152, val_acc:0.969]
Epoch [67/120    avg_loss:0.203, val_acc:0.971]
Epoch [68/120    avg_loss:0.178, val_acc:0.973]
Epoch [69/120    avg_loss:0.175, val_acc:0.979]
Epoch [70/120    avg_loss:0.116, val_acc:0.988]
Epoch [71/120    avg_loss:0.115, val_acc:0.990]
Epoch [72/120    avg_loss:0.157, val_acc:0.973]
Epoch [73/120    avg_loss:0.112, val_acc:0.988]
Epoch [74/120    avg_loss:0.083, val_acc:0.992]
Epoch [75/120    avg_loss:0.095, val_acc:0.998]
Epoch [76/120    avg_loss:0.084, val_acc:0.988]
Epoch [77/120    avg_loss:0.110, val_acc:0.992]
Epoch [78/120    avg_loss:0.190, val_acc:0.912]
Epoch [79/120    avg_loss:0.241, val_acc:0.954]
Epoch [80/120    avg_loss:0.197, val_acc:0.965]
Epoch [81/120    avg_loss:0.218, val_acc:0.967]
Epoch [82/120    avg_loss:0.207, val_acc:0.963]
Epoch [83/120    avg_loss:0.168, val_acc:0.969]
Epoch [84/120    avg_loss:0.127, val_acc:0.973]
Epoch [85/120    avg_loss:0.094, val_acc:0.983]
Epoch [86/120    avg_loss:0.104, val_acc:0.981]
Epoch [87/120    avg_loss:0.081, val_acc:0.988]
Epoch [88/120    avg_loss:0.068, val_acc:0.990]
Epoch [89/120    avg_loss:0.056, val_acc:0.992]
Epoch [90/120    avg_loss:0.058, val_acc:0.992]
Epoch [91/120    avg_loss:0.045, val_acc:0.992]
Epoch [92/120    avg_loss:0.057, val_acc:0.992]
Epoch [93/120    avg_loss:0.053, val_acc:0.992]
Epoch [94/120    avg_loss:0.047, val_acc:0.992]
Epoch [95/120    avg_loss:0.057, val_acc:0.992]
Epoch [96/120    avg_loss:0.043, val_acc:0.992]
Epoch [97/120    avg_loss:0.066, val_acc:0.992]
Epoch [98/120    avg_loss:0.049, val_acc:0.992]
Epoch [99/120    avg_loss:0.042, val_acc:0.992]
Epoch [100/120    avg_loss:0.044, val_acc:0.994]
Epoch [101/120    avg_loss:0.050, val_acc:0.992]
Epoch [102/120    avg_loss:0.037, val_acc:0.992]
Epoch [103/120    avg_loss:0.045, val_acc:0.992]
Epoch [104/120    avg_loss:0.055, val_acc:0.992]
Epoch [105/120    avg_loss:0.041, val_acc:0.992]
Epoch [106/120    avg_loss:0.043, val_acc:0.992]
Epoch [107/120    avg_loss:0.044, val_acc:0.992]
Epoch [108/120    avg_loss:0.049, val_acc:0.992]
Epoch [109/120    avg_loss:0.044, val_acc:0.992]
Epoch [110/120    avg_loss:0.047, val_acc:0.992]
Epoch [111/120    avg_loss:0.055, val_acc:0.992]
Epoch [112/120    avg_loss:0.048, val_acc:0.992]
Epoch [113/120    avg_loss:0.047, val_acc:0.992]
Epoch [114/120    avg_loss:0.050, val_acc:0.990]
Epoch [115/120    avg_loss:0.045, val_acc:0.990]
Epoch [116/120    avg_loss:0.045, val_acc:0.990]
Epoch [117/120    avg_loss:0.054, val_acc:0.990]
Epoch [118/120    avg_loss:0.049, val_acc:0.990]
Epoch [119/120    avg_loss:0.042, val_acc:0.992]
Epoch [120/120    avg_loss:0.048, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 1.         0.98198198 1.         0.95280899 0.92976589
 1.         0.96132597 0.998713   1.         1.         1.
 1.         1.        ]

Kappa:
0.9931158311891062
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f55a6fc1898>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.543, val_acc:0.346]
Epoch [2/120    avg_loss:2.331, val_acc:0.392]
Epoch [3/120    avg_loss:2.209, val_acc:0.490]
Epoch [4/120    avg_loss:2.110, val_acc:0.510]
Epoch [5/120    avg_loss:2.025, val_acc:0.533]
Epoch [6/120    avg_loss:1.913, val_acc:0.515]
Epoch [7/120    avg_loss:1.826, val_acc:0.519]
Epoch [8/120    avg_loss:1.718, val_acc:0.544]
Epoch [9/120    avg_loss:1.617, val_acc:0.565]
Epoch [10/120    avg_loss:1.515, val_acc:0.613]
Epoch [11/120    avg_loss:1.414, val_acc:0.617]
Epoch [12/120    avg_loss:1.342, val_acc:0.656]
Epoch [13/120    avg_loss:1.264, val_acc:0.692]
Epoch [14/120    avg_loss:1.191, val_acc:0.742]
Epoch [15/120    avg_loss:1.094, val_acc:0.758]
Epoch [16/120    avg_loss:1.014, val_acc:0.773]
Epoch [17/120    avg_loss:0.948, val_acc:0.792]
Epoch [18/120    avg_loss:0.887, val_acc:0.808]
Epoch [19/120    avg_loss:0.801, val_acc:0.767]
Epoch [20/120    avg_loss:0.763, val_acc:0.817]
Epoch [21/120    avg_loss:0.707, val_acc:0.819]
Epoch [22/120    avg_loss:0.649, val_acc:0.852]
Epoch [23/120    avg_loss:0.647, val_acc:0.831]
Epoch [24/120    avg_loss:0.625, val_acc:0.871]
Epoch [25/120    avg_loss:0.594, val_acc:0.873]
Epoch [26/120    avg_loss:0.540, val_acc:0.823]
Epoch [27/120    avg_loss:0.494, val_acc:0.912]
Epoch [28/120    avg_loss:0.474, val_acc:0.894]
Epoch [29/120    avg_loss:0.468, val_acc:0.898]
Epoch [30/120    avg_loss:0.459, val_acc:0.919]
Epoch [31/120    avg_loss:0.474, val_acc:0.915]
Epoch [32/120    avg_loss:0.419, val_acc:0.933]
Epoch [33/120    avg_loss:0.390, val_acc:0.912]
Epoch [34/120    avg_loss:0.382, val_acc:0.938]
Epoch [35/120    avg_loss:0.354, val_acc:0.925]
Epoch [36/120    avg_loss:0.348, val_acc:0.933]
Epoch [37/120    avg_loss:0.367, val_acc:0.942]
Epoch [38/120    avg_loss:0.336, val_acc:0.912]
Epoch [39/120    avg_loss:0.300, val_acc:0.938]
Epoch [40/120    avg_loss:0.263, val_acc:0.942]
Epoch [41/120    avg_loss:0.331, val_acc:0.921]
Epoch [42/120    avg_loss:0.300, val_acc:0.929]
Epoch [43/120    avg_loss:0.338, val_acc:0.935]
Epoch [44/120    avg_loss:0.303, val_acc:0.917]
Epoch [45/120    avg_loss:0.268, val_acc:0.938]
Epoch [46/120    avg_loss:0.258, val_acc:0.938]
Epoch [47/120    avg_loss:0.229, val_acc:0.940]
Epoch [48/120    avg_loss:0.225, val_acc:0.960]
Epoch [49/120    avg_loss:0.198, val_acc:0.956]
Epoch [50/120    avg_loss:0.183, val_acc:0.956]
Epoch [51/120    avg_loss:0.174, val_acc:0.942]
Epoch [52/120    avg_loss:0.202, val_acc:0.952]
Epoch [53/120    avg_loss:0.208, val_acc:0.950]
Epoch [54/120    avg_loss:0.329, val_acc:0.940]
Epoch [55/120    avg_loss:0.264, val_acc:0.952]
Epoch [56/120    avg_loss:0.245, val_acc:0.944]
Epoch [57/120    avg_loss:0.208, val_acc:0.948]
Epoch [58/120    avg_loss:0.197, val_acc:0.952]
Epoch [59/120    avg_loss:0.170, val_acc:0.952]
Epoch [60/120    avg_loss:0.173, val_acc:0.963]
Epoch [61/120    avg_loss:0.171, val_acc:0.956]
Epoch [62/120    avg_loss:0.178, val_acc:0.952]
Epoch [63/120    avg_loss:0.165, val_acc:0.952]
Epoch [64/120    avg_loss:0.142, val_acc:0.956]
Epoch [65/120    avg_loss:0.117, val_acc:0.958]
Epoch [66/120    avg_loss:0.124, val_acc:0.958]
Epoch [67/120    avg_loss:0.160, val_acc:0.969]
Epoch [68/120    avg_loss:0.177, val_acc:0.971]
Epoch [69/120    avg_loss:0.140, val_acc:0.963]
Epoch [70/120    avg_loss:0.142, val_acc:0.956]
Epoch [71/120    avg_loss:0.149, val_acc:0.956]
Epoch [72/120    avg_loss:0.118, val_acc:0.950]
Epoch [73/120    avg_loss:0.106, val_acc:0.956]
Epoch [74/120    avg_loss:0.134, val_acc:0.956]
Epoch [75/120    avg_loss:0.134, val_acc:0.944]
Epoch [76/120    avg_loss:0.142, val_acc:0.960]
Epoch [77/120    avg_loss:0.139, val_acc:0.969]
Epoch [78/120    avg_loss:0.099, val_acc:0.971]
Epoch [79/120    avg_loss:0.101, val_acc:0.975]
Epoch [80/120    avg_loss:0.089, val_acc:0.969]
Epoch [81/120    avg_loss:0.081, val_acc:0.971]
Epoch [82/120    avg_loss:0.092, val_acc:0.969]
Epoch [83/120    avg_loss:0.162, val_acc:0.975]
Epoch [84/120    avg_loss:0.090, val_acc:0.946]
Epoch [85/120    avg_loss:0.128, val_acc:0.967]
Epoch [86/120    avg_loss:0.107, val_acc:0.977]
Epoch [87/120    avg_loss:0.128, val_acc:0.973]
Epoch [88/120    avg_loss:0.076, val_acc:0.983]
Epoch [89/120    avg_loss:0.076, val_acc:0.965]
Epoch [90/120    avg_loss:0.108, val_acc:0.981]
Epoch [91/120    avg_loss:0.078, val_acc:0.971]
Epoch [92/120    avg_loss:0.077, val_acc:0.975]
Epoch [93/120    avg_loss:0.062, val_acc:0.981]
Epoch [94/120    avg_loss:0.060, val_acc:0.981]
Epoch [95/120    avg_loss:0.049, val_acc:0.969]
Epoch [96/120    avg_loss:0.070, val_acc:0.979]
Epoch [97/120    avg_loss:0.068, val_acc:0.977]
Epoch [98/120    avg_loss:0.061, val_acc:0.981]
Epoch [99/120    avg_loss:0.050, val_acc:0.975]
Epoch [100/120    avg_loss:0.049, val_acc:0.977]
Epoch [101/120    avg_loss:0.049, val_acc:0.975]
Epoch [102/120    avg_loss:0.044, val_acc:0.979]
Epoch [103/120    avg_loss:0.033, val_acc:0.983]
Epoch [104/120    avg_loss:0.033, val_acc:0.981]
Epoch [105/120    avg_loss:0.038, val_acc:0.981]
Epoch [106/120    avg_loss:0.030, val_acc:0.983]
Epoch [107/120    avg_loss:0.036, val_acc:0.983]
Epoch [108/120    avg_loss:0.040, val_acc:0.983]
Epoch [109/120    avg_loss:0.029, val_acc:0.983]
Epoch [110/120    avg_loss:0.030, val_acc:0.985]
Epoch [111/120    avg_loss:0.034, val_acc:0.985]
Epoch [112/120    avg_loss:0.042, val_acc:0.985]
Epoch [113/120    avg_loss:0.038, val_acc:0.985]
Epoch [114/120    avg_loss:0.044, val_acc:0.985]
Epoch [115/120    avg_loss:0.029, val_acc:0.985]
Epoch [116/120    avg_loss:0.037, val_acc:0.985]
Epoch [117/120    avg_loss:0.030, val_acc:0.985]
Epoch [118/120    avg_loss:0.032, val_acc:0.985]
Epoch [119/120    avg_loss:0.029, val_acc:0.983]
Epoch [120/120    avg_loss:0.033, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.55223880597015

F1 scores:
[       nan 1.         0.98426966 1.         0.96929825 0.95138889
 1.         0.96132597 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9950147272821973
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:11:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f604b5158d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.571, val_acc:0.338]
Epoch [2/120    avg_loss:2.388, val_acc:0.379]
Epoch [3/120    avg_loss:2.274, val_acc:0.456]
Epoch [4/120    avg_loss:2.174, val_acc:0.475]
Epoch [5/120    avg_loss:2.080, val_acc:0.523]
Epoch [6/120    avg_loss:1.995, val_acc:0.554]
Epoch [7/120    avg_loss:1.911, val_acc:0.594]
Epoch [8/120    avg_loss:1.805, val_acc:0.619]
Epoch [9/120    avg_loss:1.729, val_acc:0.646]
Epoch [10/120    avg_loss:1.627, val_acc:0.658]
Epoch [11/120    avg_loss:1.540, val_acc:0.671]
Epoch [12/120    avg_loss:1.434, val_acc:0.685]
Epoch [13/120    avg_loss:1.380, val_acc:0.696]
Epoch [14/120    avg_loss:1.273, val_acc:0.704]
Epoch [15/120    avg_loss:1.171, val_acc:0.725]
Epoch [16/120    avg_loss:1.076, val_acc:0.742]
Epoch [17/120    avg_loss:1.047, val_acc:0.750]
Epoch [18/120    avg_loss:0.982, val_acc:0.817]
Epoch [19/120    avg_loss:0.913, val_acc:0.802]
Epoch [20/120    avg_loss:0.848, val_acc:0.787]
Epoch [21/120    avg_loss:0.824, val_acc:0.762]
Epoch [22/120    avg_loss:0.739, val_acc:0.867]
Epoch [23/120    avg_loss:0.692, val_acc:0.904]
Epoch [24/120    avg_loss:0.657, val_acc:0.910]
Epoch [25/120    avg_loss:0.609, val_acc:0.869]
Epoch [26/120    avg_loss:0.595, val_acc:0.929]
Epoch [27/120    avg_loss:0.575, val_acc:0.935]
Epoch [28/120    avg_loss:0.503, val_acc:0.892]
Epoch [29/120    avg_loss:0.502, val_acc:0.900]
Epoch [30/120    avg_loss:0.472, val_acc:0.935]
Epoch [31/120    avg_loss:0.406, val_acc:0.931]
Epoch [32/120    avg_loss:0.416, val_acc:0.965]
Epoch [33/120    avg_loss:0.395, val_acc:0.944]
Epoch [34/120    avg_loss:0.358, val_acc:0.948]
Epoch [35/120    avg_loss:0.392, val_acc:0.952]
Epoch [36/120    avg_loss:0.390, val_acc:0.950]
Epoch [37/120    avg_loss:0.300, val_acc:0.948]
Epoch [38/120    avg_loss:0.316, val_acc:0.950]
Epoch [39/120    avg_loss:0.331, val_acc:0.944]
Epoch [40/120    avg_loss:0.327, val_acc:0.954]
Epoch [41/120    avg_loss:0.261, val_acc:0.948]
Epoch [42/120    avg_loss:0.281, val_acc:0.956]
Epoch [43/120    avg_loss:0.243, val_acc:0.967]
Epoch [44/120    avg_loss:0.246, val_acc:0.946]
Epoch [45/120    avg_loss:0.297, val_acc:0.963]
Epoch [46/120    avg_loss:0.227, val_acc:0.956]
Epoch [47/120    avg_loss:0.217, val_acc:0.958]
Epoch [48/120    avg_loss:0.221, val_acc:0.963]
Epoch [49/120    avg_loss:0.200, val_acc:0.954]
Epoch [50/120    avg_loss:0.229, val_acc:0.969]
Epoch [51/120    avg_loss:0.224, val_acc:0.960]
Epoch [52/120    avg_loss:0.214, val_acc:0.965]
Epoch [53/120    avg_loss:0.211, val_acc:0.960]
Epoch [54/120    avg_loss:0.155, val_acc:0.983]
Epoch [55/120    avg_loss:0.166, val_acc:0.975]
Epoch [56/120    avg_loss:0.220, val_acc:0.971]
Epoch [57/120    avg_loss:0.190, val_acc:0.956]
Epoch [58/120    avg_loss:0.186, val_acc:0.973]
Epoch [59/120    avg_loss:0.163, val_acc:0.965]
Epoch [60/120    avg_loss:0.243, val_acc:0.960]
Epoch [61/120    avg_loss:0.178, val_acc:0.983]
Epoch [62/120    avg_loss:0.187, val_acc:0.971]
Epoch [63/120    avg_loss:0.146, val_acc:0.971]
Epoch [64/120    avg_loss:0.153, val_acc:0.979]
Epoch [65/120    avg_loss:0.109, val_acc:0.985]
Epoch [66/120    avg_loss:0.130, val_acc:0.981]
Epoch [67/120    avg_loss:0.106, val_acc:0.971]
Epoch [68/120    avg_loss:0.126, val_acc:0.983]
Epoch [69/120    avg_loss:0.114, val_acc:0.977]
Epoch [70/120    avg_loss:0.123, val_acc:0.983]
Epoch [71/120    avg_loss:0.088, val_acc:0.983]
Epoch [72/120    avg_loss:0.093, val_acc:0.975]
Epoch [73/120    avg_loss:0.086, val_acc:0.973]
Epoch [74/120    avg_loss:0.128, val_acc:0.985]
Epoch [75/120    avg_loss:0.113, val_acc:0.975]
Epoch [76/120    avg_loss:0.095, val_acc:0.952]
Epoch [77/120    avg_loss:0.144, val_acc:0.975]
Epoch [78/120    avg_loss:0.100, val_acc:0.969]
Epoch [79/120    avg_loss:0.119, val_acc:0.988]
Epoch [80/120    avg_loss:0.075, val_acc:0.988]
Epoch [81/120    avg_loss:0.087, val_acc:0.983]
Epoch [82/120    avg_loss:0.069, val_acc:0.988]
Epoch [83/120    avg_loss:0.080, val_acc:0.975]
Epoch [84/120    avg_loss:0.096, val_acc:0.977]
Epoch [85/120    avg_loss:0.102, val_acc:0.967]
Epoch [86/120    avg_loss:0.121, val_acc:0.990]
Epoch [87/120    avg_loss:0.090, val_acc:0.958]
Epoch [88/120    avg_loss:0.096, val_acc:0.973]
Epoch [89/120    avg_loss:0.087, val_acc:0.965]
Epoch [90/120    avg_loss:0.094, val_acc:0.988]
Epoch [91/120    avg_loss:0.089, val_acc:0.983]
Epoch [92/120    avg_loss:0.088, val_acc:0.983]
Epoch [93/120    avg_loss:0.073, val_acc:0.988]
Epoch [94/120    avg_loss:0.050, val_acc:0.990]
Epoch [95/120    avg_loss:0.061, val_acc:0.990]
Epoch [96/120    avg_loss:0.047, val_acc:0.990]
Epoch [97/120    avg_loss:0.055, val_acc:0.988]
Epoch [98/120    avg_loss:0.077, val_acc:0.965]
Epoch [99/120    avg_loss:0.115, val_acc:0.965]
Epoch [100/120    avg_loss:0.110, val_acc:0.994]
Epoch [101/120    avg_loss:0.053, val_acc:0.990]
Epoch [102/120    avg_loss:0.051, val_acc:0.994]
Epoch [103/120    avg_loss:0.056, val_acc:0.985]
Epoch [104/120    avg_loss:0.052, val_acc:0.990]
Epoch [105/120    avg_loss:0.072, val_acc:0.990]
Epoch [106/120    avg_loss:0.087, val_acc:0.983]
Epoch [107/120    avg_loss:0.059, val_acc:0.992]
Epoch [108/120    avg_loss:0.077, val_acc:0.977]
Epoch [109/120    avg_loss:0.048, val_acc:0.992]
Epoch [110/120    avg_loss:0.055, val_acc:0.990]
Epoch [111/120    avg_loss:0.049, val_acc:0.992]
Epoch [112/120    avg_loss:0.048, val_acc:0.983]
Epoch [113/120    avg_loss:0.066, val_acc:0.988]
Epoch [114/120    avg_loss:0.084, val_acc:0.981]
Epoch [115/120    avg_loss:0.065, val_acc:0.983]
Epoch [116/120    avg_loss:0.040, val_acc:0.988]
Epoch [117/120    avg_loss:0.044, val_acc:0.990]
Epoch [118/120    avg_loss:0.040, val_acc:0.990]
Epoch [119/120    avg_loss:0.028, val_acc:0.990]
Epoch [120/120    avg_loss:0.032, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   0   0   6   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  11   0   0   0   0   0   0   1   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 0.99560117 0.99095023 1.         0.93478261 0.8975265
 0.98564593 0.97826087 1.         1.         1.         0.99472296
 0.9944629  1.        ]

Kappa:
0.9895562087114459
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f42f2b778d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.606, val_acc:0.383]
Epoch [2/120    avg_loss:2.423, val_acc:0.356]
Epoch [3/120    avg_loss:2.282, val_acc:0.446]
Epoch [4/120    avg_loss:2.145, val_acc:0.552]
Epoch [5/120    avg_loss:2.020, val_acc:0.569]
Epoch [6/120    avg_loss:1.894, val_acc:0.562]
Epoch [7/120    avg_loss:1.793, val_acc:0.573]
Epoch [8/120    avg_loss:1.658, val_acc:0.588]
Epoch [9/120    avg_loss:1.568, val_acc:0.629]
Epoch [10/120    avg_loss:1.461, val_acc:0.654]
Epoch [11/120    avg_loss:1.375, val_acc:0.685]
Epoch [12/120    avg_loss:1.303, val_acc:0.717]
Epoch [13/120    avg_loss:1.221, val_acc:0.754]
Epoch [14/120    avg_loss:1.183, val_acc:0.756]
Epoch [15/120    avg_loss:1.063, val_acc:0.773]
Epoch [16/120    avg_loss:1.029, val_acc:0.790]
Epoch [17/120    avg_loss:0.971, val_acc:0.798]
Epoch [18/120    avg_loss:0.916, val_acc:0.785]
Epoch [19/120    avg_loss:0.881, val_acc:0.819]
Epoch [20/120    avg_loss:0.832, val_acc:0.798]
Epoch [21/120    avg_loss:0.787, val_acc:0.831]
Epoch [22/120    avg_loss:0.736, val_acc:0.810]
Epoch [23/120    avg_loss:0.683, val_acc:0.827]
Epoch [24/120    avg_loss:0.675, val_acc:0.856]
Epoch [25/120    avg_loss:0.658, val_acc:0.812]
Epoch [26/120    avg_loss:0.654, val_acc:0.923]
Epoch [27/120    avg_loss:0.603, val_acc:0.925]
Epoch [28/120    avg_loss:0.562, val_acc:0.929]
Epoch [29/120    avg_loss:0.571, val_acc:0.887]
Epoch [30/120    avg_loss:0.520, val_acc:0.890]
Epoch [31/120    avg_loss:0.526, val_acc:0.919]
Epoch [32/120    avg_loss:0.473, val_acc:0.912]
Epoch [33/120    avg_loss:0.476, val_acc:0.940]
Epoch [34/120    avg_loss:0.471, val_acc:0.919]
Epoch [35/120    avg_loss:0.401, val_acc:0.946]
Epoch [36/120    avg_loss:0.419, val_acc:0.946]
Epoch [37/120    avg_loss:0.382, val_acc:0.963]
Epoch [38/120    avg_loss:0.337, val_acc:0.963]
Epoch [39/120    avg_loss:0.353, val_acc:0.948]
Epoch [40/120    avg_loss:0.378, val_acc:0.948]
Epoch [41/120    avg_loss:0.380, val_acc:0.869]
Epoch [42/120    avg_loss:0.439, val_acc:0.929]
Epoch [43/120    avg_loss:0.401, val_acc:0.946]
Epoch [44/120    avg_loss:0.314, val_acc:0.948]
Epoch [45/120    avg_loss:0.326, val_acc:0.960]
Epoch [46/120    avg_loss:0.337, val_acc:0.948]
Epoch [47/120    avg_loss:0.307, val_acc:0.956]
Epoch [48/120    avg_loss:0.283, val_acc:0.952]
Epoch [49/120    avg_loss:0.273, val_acc:0.967]
Epoch [50/120    avg_loss:0.290, val_acc:0.963]
Epoch [51/120    avg_loss:0.322, val_acc:0.958]
Epoch [52/120    avg_loss:0.312, val_acc:0.960]
Epoch [53/120    avg_loss:0.305, val_acc:0.956]
Epoch [54/120    avg_loss:0.285, val_acc:0.956]
Epoch [55/120    avg_loss:0.286, val_acc:0.973]
Epoch [56/120    avg_loss:0.244, val_acc:0.971]
Epoch [57/120    avg_loss:0.230, val_acc:0.967]
Epoch [58/120    avg_loss:0.215, val_acc:0.963]
Epoch [59/120    avg_loss:0.221, val_acc:0.977]
Epoch [60/120    avg_loss:0.182, val_acc:0.979]
Epoch [61/120    avg_loss:0.210, val_acc:0.965]
Epoch [62/120    avg_loss:0.205, val_acc:0.973]
Epoch [63/120    avg_loss:0.203, val_acc:0.979]
Epoch [64/120    avg_loss:0.198, val_acc:0.965]
Epoch [65/120    avg_loss:0.204, val_acc:0.969]
Epoch [66/120    avg_loss:0.231, val_acc:0.950]
Epoch [67/120    avg_loss:0.230, val_acc:0.975]
Epoch [68/120    avg_loss:0.219, val_acc:0.954]
Epoch [69/120    avg_loss:0.190, val_acc:0.971]
Epoch [70/120    avg_loss:0.239, val_acc:0.967]
Epoch [71/120    avg_loss:0.162, val_acc:0.977]
Epoch [72/120    avg_loss:0.148, val_acc:0.956]
Epoch [73/120    avg_loss:0.145, val_acc:0.979]
Epoch [74/120    avg_loss:0.142, val_acc:0.981]
Epoch [75/120    avg_loss:0.128, val_acc:0.977]
Epoch [76/120    avg_loss:0.140, val_acc:0.975]
Epoch [77/120    avg_loss:0.123, val_acc:0.983]
Epoch [78/120    avg_loss:0.158, val_acc:0.979]
Epoch [79/120    avg_loss:0.150, val_acc:0.973]
Epoch [80/120    avg_loss:0.139, val_acc:0.990]
Epoch [81/120    avg_loss:0.136, val_acc:0.975]
Epoch [82/120    avg_loss:0.165, val_acc:0.988]
Epoch [83/120    avg_loss:0.117, val_acc:0.988]
Epoch [84/120    avg_loss:0.104, val_acc:0.967]
Epoch [85/120    avg_loss:0.125, val_acc:0.967]
Epoch [86/120    avg_loss:0.166, val_acc:0.965]
Epoch [87/120    avg_loss:0.121, val_acc:0.977]
Epoch [88/120    avg_loss:0.095, val_acc:0.988]
Epoch [89/120    avg_loss:0.087, val_acc:0.985]
Epoch [90/120    avg_loss:0.099, val_acc:0.985]
Epoch [91/120    avg_loss:0.120, val_acc:0.983]
Epoch [92/120    avg_loss:0.101, val_acc:0.981]
Epoch [93/120    avg_loss:0.076, val_acc:0.988]
Epoch [94/120    avg_loss:0.087, val_acc:0.990]
Epoch [95/120    avg_loss:0.061, val_acc:0.990]
Epoch [96/120    avg_loss:0.065, val_acc:0.990]
Epoch [97/120    avg_loss:0.068, val_acc:0.990]
Epoch [98/120    avg_loss:0.071, val_acc:0.992]
Epoch [99/120    avg_loss:0.054, val_acc:0.992]
Epoch [100/120    avg_loss:0.085, val_acc:0.990]
Epoch [101/120    avg_loss:0.062, val_acc:0.992]
Epoch [102/120    avg_loss:0.065, val_acc:0.990]
Epoch [103/120    avg_loss:0.066, val_acc:0.990]
Epoch [104/120    avg_loss:0.056, val_acc:0.992]
Epoch [105/120    avg_loss:0.073, val_acc:0.990]
Epoch [106/120    avg_loss:0.054, val_acc:0.990]
Epoch [107/120    avg_loss:0.066, val_acc:0.990]
Epoch [108/120    avg_loss:0.055, val_acc:0.990]
Epoch [109/120    avg_loss:0.070, val_acc:0.992]
Epoch [110/120    avg_loss:0.055, val_acc:0.992]
Epoch [111/120    avg_loss:0.058, val_acc:0.990]
Epoch [112/120    avg_loss:0.067, val_acc:0.990]
Epoch [113/120    avg_loss:0.057, val_acc:0.992]
Epoch [114/120    avg_loss:0.064, val_acc:0.992]
Epoch [115/120    avg_loss:0.058, val_acc:0.990]
Epoch [116/120    avg_loss:0.048, val_acc:0.990]
Epoch [117/120    avg_loss:0.050, val_acc:0.988]
Epoch [118/120    avg_loss:0.050, val_acc:0.990]
Epoch [119/120    avg_loss:0.050, val_acc:0.992]
Epoch [120/120    avg_loss:0.053, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 0.99926954 0.97333333 0.99782135 0.94831461 0.92307692
 0.99757869 0.93785311 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9914542040441902
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9ca793d8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.620, val_acc:0.113]
Epoch [2/120    avg_loss:2.432, val_acc:0.304]
Epoch [3/120    avg_loss:2.276, val_acc:0.381]
Epoch [4/120    avg_loss:2.154, val_acc:0.469]
Epoch [5/120    avg_loss:2.044, val_acc:0.550]
Epoch [6/120    avg_loss:1.929, val_acc:0.583]
Epoch [7/120    avg_loss:1.834, val_acc:0.633]
Epoch [8/120    avg_loss:1.732, val_acc:0.671]
Epoch [9/120    avg_loss:1.603, val_acc:0.669]
Epoch [10/120    avg_loss:1.502, val_acc:0.690]
Epoch [11/120    avg_loss:1.414, val_acc:0.715]
Epoch [12/120    avg_loss:1.292, val_acc:0.731]
Epoch [13/120    avg_loss:1.208, val_acc:0.754]
Epoch [14/120    avg_loss:1.115, val_acc:0.792]
Epoch [15/120    avg_loss:1.054, val_acc:0.790]
Epoch [16/120    avg_loss:0.948, val_acc:0.808]
Epoch [17/120    avg_loss:0.898, val_acc:0.794]
Epoch [18/120    avg_loss:0.834, val_acc:0.860]
Epoch [19/120    avg_loss:0.836, val_acc:0.867]
Epoch [20/120    avg_loss:0.739, val_acc:0.838]
Epoch [21/120    avg_loss:0.721, val_acc:0.898]
Epoch [22/120    avg_loss:0.656, val_acc:0.906]
Epoch [23/120    avg_loss:0.618, val_acc:0.919]
Epoch [24/120    avg_loss:0.618, val_acc:0.892]
Epoch [25/120    avg_loss:0.558, val_acc:0.942]
Epoch [26/120    avg_loss:0.515, val_acc:0.890]
Epoch [27/120    avg_loss:0.496, val_acc:0.925]
Epoch [28/120    avg_loss:0.439, val_acc:0.948]
Epoch [29/120    avg_loss:0.409, val_acc:0.956]
Epoch [30/120    avg_loss:0.358, val_acc:0.946]
Epoch [31/120    avg_loss:0.408, val_acc:0.902]
Epoch [32/120    avg_loss:0.396, val_acc:0.948]
Epoch [33/120    avg_loss:0.345, val_acc:0.931]
Epoch [34/120    avg_loss:0.363, val_acc:0.933]
Epoch [35/120    avg_loss:0.343, val_acc:0.963]
Epoch [36/120    avg_loss:0.326, val_acc:0.952]
Epoch [37/120    avg_loss:0.296, val_acc:0.938]
Epoch [38/120    avg_loss:0.283, val_acc:0.965]
Epoch [39/120    avg_loss:0.262, val_acc:0.950]
Epoch [40/120    avg_loss:0.258, val_acc:0.965]
Epoch [41/120    avg_loss:0.271, val_acc:0.960]
Epoch [42/120    avg_loss:0.225, val_acc:0.958]
Epoch [43/120    avg_loss:0.237, val_acc:0.958]
Epoch [44/120    avg_loss:0.263, val_acc:0.971]
Epoch [45/120    avg_loss:0.238, val_acc:0.963]
Epoch [46/120    avg_loss:0.234, val_acc:0.971]
Epoch [47/120    avg_loss:0.210, val_acc:0.954]
Epoch [48/120    avg_loss:0.177, val_acc:0.975]
Epoch [49/120    avg_loss:0.223, val_acc:0.952]
Epoch [50/120    avg_loss:0.253, val_acc:0.975]
Epoch [51/120    avg_loss:0.209, val_acc:0.969]
Epoch [52/120    avg_loss:0.253, val_acc:0.929]
Epoch [53/120    avg_loss:0.240, val_acc:0.971]
Epoch [54/120    avg_loss:0.193, val_acc:0.960]
Epoch [55/120    avg_loss:0.204, val_acc:0.971]
Epoch [56/120    avg_loss:0.161, val_acc:0.969]
Epoch [57/120    avg_loss:0.175, val_acc:0.965]
Epoch [58/120    avg_loss:0.159, val_acc:0.975]
Epoch [59/120    avg_loss:0.146, val_acc:0.977]
Epoch [60/120    avg_loss:0.160, val_acc:0.954]
Epoch [61/120    avg_loss:0.178, val_acc:0.960]
Epoch [62/120    avg_loss:0.147, val_acc:0.981]
Epoch [63/120    avg_loss:0.141, val_acc:0.973]
Epoch [64/120    avg_loss:0.137, val_acc:0.975]
Epoch [65/120    avg_loss:0.153, val_acc:0.958]
Epoch [66/120    avg_loss:0.159, val_acc:0.952]
Epoch [67/120    avg_loss:0.122, val_acc:0.975]
Epoch [68/120    avg_loss:0.151, val_acc:0.960]
Epoch [69/120    avg_loss:0.180, val_acc:0.977]
Epoch [70/120    avg_loss:0.139, val_acc:0.979]
Epoch [71/120    avg_loss:0.145, val_acc:0.958]
Epoch [72/120    avg_loss:0.158, val_acc:0.969]
Epoch [73/120    avg_loss:0.139, val_acc:0.981]
Epoch [74/120    avg_loss:0.124, val_acc:0.985]
Epoch [75/120    avg_loss:0.095, val_acc:0.977]
Epoch [76/120    avg_loss:0.094, val_acc:0.975]
Epoch [77/120    avg_loss:0.137, val_acc:0.963]
Epoch [78/120    avg_loss:0.142, val_acc:0.971]
Epoch [79/120    avg_loss:0.115, val_acc:0.977]
Epoch [80/120    avg_loss:0.106, val_acc:0.990]
Epoch [81/120    avg_loss:0.119, val_acc:0.956]
Epoch [82/120    avg_loss:0.124, val_acc:0.981]
Epoch [83/120    avg_loss:0.089, val_acc:0.983]
Epoch [84/120    avg_loss:0.086, val_acc:0.981]
Epoch [85/120    avg_loss:0.105, val_acc:0.979]
Epoch [86/120    avg_loss:0.082, val_acc:0.977]
Epoch [87/120    avg_loss:0.091, val_acc:0.983]
Epoch [88/120    avg_loss:0.110, val_acc:0.988]
Epoch [89/120    avg_loss:0.074, val_acc:0.985]
Epoch [90/120    avg_loss:0.083, val_acc:0.985]
Epoch [91/120    avg_loss:0.066, val_acc:0.992]
Epoch [92/120    avg_loss:0.076, val_acc:0.985]
Epoch [93/120    avg_loss:0.105, val_acc:0.971]
Epoch [94/120    avg_loss:0.151, val_acc:0.979]
Epoch [95/120    avg_loss:0.096, val_acc:0.979]
Epoch [96/120    avg_loss:0.066, val_acc:0.988]
Epoch [97/120    avg_loss:0.082, val_acc:0.992]
Epoch [98/120    avg_loss:0.065, val_acc:0.992]
Epoch [99/120    avg_loss:0.075, val_acc:0.994]
Epoch [100/120    avg_loss:0.059, val_acc:0.994]
Epoch [101/120    avg_loss:0.058, val_acc:0.981]
Epoch [102/120    avg_loss:0.079, val_acc:0.988]
Epoch [103/120    avg_loss:0.059, val_acc:0.988]
Epoch [104/120    avg_loss:0.058, val_acc:0.992]
Epoch [105/120    avg_loss:0.063, val_acc:0.992]
Epoch [106/120    avg_loss:0.041, val_acc:0.992]
Epoch [107/120    avg_loss:0.047, val_acc:0.988]
Epoch [108/120    avg_loss:0.048, val_acc:0.988]
Epoch [109/120    avg_loss:0.049, val_acc:0.992]
Epoch [110/120    avg_loss:0.035, val_acc:0.992]
Epoch [111/120    avg_loss:0.040, val_acc:0.983]
Epoch [112/120    avg_loss:0.044, val_acc:0.979]
Epoch [113/120    avg_loss:0.059, val_acc:0.992]
Epoch [114/120    avg_loss:0.035, val_acc:0.992]
Epoch [115/120    avg_loss:0.035, val_acc:0.992]
Epoch [116/120    avg_loss:0.038, val_acc:0.990]
Epoch [117/120    avg_loss:0.032, val_acc:0.992]
Epoch [118/120    avg_loss:0.037, val_acc:0.994]
Epoch [119/120    avg_loss:0.024, val_acc:0.994]
Epoch [120/120    avg_loss:0.022, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  15   0   0   0   0   0   0   1   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   1   0 376   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 0.99853801 0.98206278 1.         0.96127563 0.94736842
 0.99516908 0.95555556 1.         0.99893276 1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9933535846888294
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd18ec3b908>
supervision:full
center_pixel:True
Network :
Number of parameter: 49432==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.551, val_acc:0.358]
Epoch [2/120    avg_loss:2.405, val_acc:0.467]
Epoch [3/120    avg_loss:2.295, val_acc:0.475]
Epoch [4/120    avg_loss:2.171, val_acc:0.492]
Epoch [5/120    avg_loss:2.075, val_acc:0.588]
Epoch [6/120    avg_loss:1.972, val_acc:0.585]
Epoch [7/120    avg_loss:1.864, val_acc:0.625]
Epoch [8/120    avg_loss:1.776, val_acc:0.694]
Epoch [9/120    avg_loss:1.682, val_acc:0.690]
Epoch [10/120    avg_loss:1.580, val_acc:0.694]
Epoch [11/120    avg_loss:1.477, val_acc:0.710]
Epoch [12/120    avg_loss:1.372, val_acc:0.719]
Epoch [13/120    avg_loss:1.297, val_acc:0.779]
Epoch [14/120    avg_loss:1.232, val_acc:0.765]
Epoch [15/120    avg_loss:1.143, val_acc:0.767]
Epoch [16/120    avg_loss:1.077, val_acc:0.800]
Epoch [17/120    avg_loss:1.016, val_acc:0.821]
Epoch [18/120    avg_loss:0.966, val_acc:0.831]
Epoch [19/120    avg_loss:0.898, val_acc:0.735]
Epoch [20/120    avg_loss:0.860, val_acc:0.838]
Epoch [21/120    avg_loss:0.806, val_acc:0.792]
Epoch [22/120    avg_loss:0.816, val_acc:0.815]
Epoch [23/120    avg_loss:0.753, val_acc:0.890]
Epoch [24/120    avg_loss:0.692, val_acc:0.885]
Epoch [25/120    avg_loss:0.626, val_acc:0.908]
Epoch [26/120    avg_loss:0.606, val_acc:0.896]
Epoch [27/120    avg_loss:0.573, val_acc:0.877]
Epoch [28/120    avg_loss:0.501, val_acc:0.923]
Epoch [29/120    avg_loss:0.473, val_acc:0.887]
Epoch [30/120    avg_loss:0.530, val_acc:0.900]
Epoch [31/120    avg_loss:0.480, val_acc:0.890]
Epoch [32/120    avg_loss:0.444, val_acc:0.906]
Epoch [33/120    avg_loss:0.496, val_acc:0.921]
Epoch [34/120    avg_loss:0.465, val_acc:0.927]
Epoch [35/120    avg_loss:0.432, val_acc:0.931]
Epoch [36/120    avg_loss:0.371, val_acc:0.921]
Epoch [37/120    avg_loss:0.327, val_acc:0.940]
Epoch [38/120    avg_loss:0.370, val_acc:0.944]
Epoch [39/120    avg_loss:0.338, val_acc:0.912]
Epoch [40/120    avg_loss:0.318, val_acc:0.929]
Epoch [41/120    avg_loss:0.310, val_acc:0.946]
Epoch [42/120    avg_loss:0.277, val_acc:0.927]
Epoch [43/120    avg_loss:0.296, val_acc:0.942]
Epoch [44/120    avg_loss:0.282, val_acc:0.938]
Epoch [45/120    avg_loss:0.302, val_acc:0.944]
Epoch [46/120    avg_loss:0.252, val_acc:0.958]
Epoch [47/120    avg_loss:0.257, val_acc:0.946]
Epoch [48/120    avg_loss:0.266, val_acc:0.950]
Epoch [49/120    avg_loss:0.233, val_acc:0.965]
Epoch [50/120    avg_loss:0.220, val_acc:0.946]
Epoch [51/120    avg_loss:0.209, val_acc:0.942]
Epoch [52/120    avg_loss:0.214, val_acc:0.944]
Epoch [53/120    avg_loss:0.208, val_acc:0.963]
Epoch [54/120    avg_loss:0.171, val_acc:0.950]
Epoch [55/120    avg_loss:0.164, val_acc:0.963]
Epoch [56/120    avg_loss:0.187, val_acc:0.958]
Epoch [57/120    avg_loss:0.175, val_acc:0.965]
Epoch [58/120    avg_loss:0.153, val_acc:0.967]
Epoch [59/120    avg_loss:0.142, val_acc:0.969]
Epoch [60/120    avg_loss:0.156, val_acc:0.973]
Epoch [61/120    avg_loss:0.141, val_acc:0.979]
Epoch [62/120    avg_loss:0.137, val_acc:0.981]
Epoch [63/120    avg_loss:0.141, val_acc:0.946]
Epoch [64/120    avg_loss:0.206, val_acc:0.973]
Epoch [65/120    avg_loss:0.139, val_acc:0.975]
Epoch [66/120    avg_loss:0.153, val_acc:0.975]
Epoch [67/120    avg_loss:0.155, val_acc:0.965]
Epoch [68/120    avg_loss:0.175, val_acc:0.954]
Epoch [69/120    avg_loss:0.147, val_acc:0.963]
Epoch [70/120    avg_loss:0.122, val_acc:0.979]
Epoch [71/120    avg_loss:0.112, val_acc:0.960]
Epoch [72/120    avg_loss:0.127, val_acc:0.975]
Epoch [73/120    avg_loss:0.121, val_acc:0.988]
Epoch [74/120    avg_loss:0.099, val_acc:0.977]
Epoch [75/120    avg_loss:0.105, val_acc:0.985]
Epoch [76/120    avg_loss:0.116, val_acc:0.973]
Epoch [77/120    avg_loss:0.105, val_acc:0.969]
Epoch [78/120    avg_loss:0.111, val_acc:0.975]
Epoch [79/120    avg_loss:0.093, val_acc:0.979]
Epoch [80/120    avg_loss:0.076, val_acc:0.981]
Epoch [81/120    avg_loss:0.080, val_acc:0.979]
Epoch [82/120    avg_loss:0.083, val_acc:0.985]
Epoch [83/120    avg_loss:0.084, val_acc:0.985]
Epoch [84/120    avg_loss:0.080, val_acc:0.988]
Epoch [85/120    avg_loss:0.076, val_acc:0.985]
Epoch [86/120    avg_loss:0.071, val_acc:0.983]
Epoch [87/120    avg_loss:0.062, val_acc:0.979]
Epoch [88/120    avg_loss:0.061, val_acc:0.985]
Epoch [89/120    avg_loss:0.182, val_acc:0.977]
Epoch [90/120    avg_loss:0.097, val_acc:0.969]
Epoch [91/120    avg_loss:0.075, val_acc:0.983]
Epoch [92/120    avg_loss:0.059, val_acc:0.977]
Epoch [93/120    avg_loss:0.077, val_acc:0.975]
Epoch [94/120    avg_loss:0.097, val_acc:0.983]
Epoch [95/120    avg_loss:0.077, val_acc:0.990]
Epoch [96/120    avg_loss:0.074, val_acc:0.979]
Epoch [97/120    avg_loss:0.077, val_acc:0.973]
Epoch [98/120    avg_loss:0.083, val_acc:0.988]
Epoch [99/120    avg_loss:0.075, val_acc:0.985]
Epoch [100/120    avg_loss:0.046, val_acc:0.990]
Epoch [101/120    avg_loss:0.064, val_acc:0.985]
Epoch [102/120    avg_loss:0.075, val_acc:0.981]
Epoch [103/120    avg_loss:0.066, val_acc:0.988]
Epoch [104/120    avg_loss:0.066, val_acc:0.975]
Epoch [105/120    avg_loss:0.088, val_acc:0.979]
Epoch [106/120    avg_loss:0.059, val_acc:0.975]
Epoch [107/120    avg_loss:0.075, val_acc:0.983]
Epoch [108/120    avg_loss:0.048, val_acc:0.981]
Epoch [109/120    avg_loss:0.041, val_acc:0.983]
Epoch [110/120    avg_loss:0.046, val_acc:0.992]
Epoch [111/120    avg_loss:0.066, val_acc:0.954]
Epoch [112/120    avg_loss:0.091, val_acc:0.977]
Epoch [113/120    avg_loss:0.063, val_acc:0.985]
Epoch [114/120    avg_loss:0.046, val_acc:0.979]
Epoch [115/120    avg_loss:0.061, val_acc:0.985]
Epoch [116/120    avg_loss:0.037, val_acc:0.990]
Epoch [117/120    avg_loss:0.038, val_acc:0.992]
Epoch [118/120    avg_loss:0.047, val_acc:0.988]
Epoch [119/120    avg_loss:0.037, val_acc:0.985]
Epoch [120/120    avg_loss:0.026, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   7   1   0   0   0   0   0 380   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 0.99345455 0.98206278 0.99782135 0.93709328 0.90140845
 0.99516908 0.96132597 0.98958333 1.         1.         1.
 1.         1.        ]

Kappa:
0.9890791729085736
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f75299cb940>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.573, val_acc:0.319]
Epoch [2/120    avg_loss:2.389, val_acc:0.317]
Epoch [3/120    avg_loss:2.231, val_acc:0.338]
Epoch [4/120    avg_loss:2.103, val_acc:0.406]
Epoch [5/120    avg_loss:1.969, val_acc:0.533]
Epoch [6/120    avg_loss:1.877, val_acc:0.558]
Epoch [7/120    avg_loss:1.770, val_acc:0.600]
Epoch [8/120    avg_loss:1.676, val_acc:0.619]
Epoch [9/120    avg_loss:1.589, val_acc:0.635]
Epoch [10/120    avg_loss:1.499, val_acc:0.671]
Epoch [11/120    avg_loss:1.405, val_acc:0.667]
Epoch [12/120    avg_loss:1.312, val_acc:0.679]
Epoch [13/120    avg_loss:1.230, val_acc:0.708]
Epoch [14/120    avg_loss:1.148, val_acc:0.742]
Epoch [15/120    avg_loss:1.045, val_acc:0.760]
Epoch [16/120    avg_loss:0.987, val_acc:0.781]
Epoch [17/120    avg_loss:0.918, val_acc:0.767]
Epoch [18/120    avg_loss:0.848, val_acc:0.762]
Epoch [19/120    avg_loss:0.772, val_acc:0.798]
Epoch [20/120    avg_loss:0.722, val_acc:0.804]
Epoch [21/120    avg_loss:0.695, val_acc:0.821]
Epoch [22/120    avg_loss:0.654, val_acc:0.804]
Epoch [23/120    avg_loss:0.662, val_acc:0.794]
Epoch [24/120    avg_loss:0.582, val_acc:0.896]
Epoch [25/120    avg_loss:0.545, val_acc:0.933]
Epoch [26/120    avg_loss:0.526, val_acc:0.908]
Epoch [27/120    avg_loss:0.492, val_acc:0.925]
Epoch [28/120    avg_loss:0.450, val_acc:0.887]
Epoch [29/120    avg_loss:0.442, val_acc:0.885]
Epoch [30/120    avg_loss:0.435, val_acc:0.935]
Epoch [31/120    avg_loss:0.384, val_acc:0.954]
Epoch [32/120    avg_loss:0.401, val_acc:0.923]
Epoch [33/120    avg_loss:0.356, val_acc:0.960]
Epoch [34/120    avg_loss:0.381, val_acc:0.929]
Epoch [35/120    avg_loss:0.356, val_acc:0.917]
Epoch [36/120    avg_loss:0.357, val_acc:0.935]
Epoch [37/120    avg_loss:0.307, val_acc:0.975]
Epoch [38/120    avg_loss:0.295, val_acc:0.981]
Epoch [39/120    avg_loss:0.299, val_acc:0.956]
Epoch [40/120    avg_loss:0.264, val_acc:0.935]
Epoch [41/120    avg_loss:0.285, val_acc:0.896]
Epoch [42/120    avg_loss:0.315, val_acc:0.965]
Epoch [43/120    avg_loss:0.292, val_acc:0.973]
Epoch [44/120    avg_loss:0.288, val_acc:0.965]
Epoch [45/120    avg_loss:0.272, val_acc:0.965]
Epoch [46/120    avg_loss:0.311, val_acc:0.969]
Epoch [47/120    avg_loss:0.284, val_acc:0.965]
Epoch [48/120    avg_loss:0.239, val_acc:0.933]
Epoch [49/120    avg_loss:0.294, val_acc:0.954]
Epoch [50/120    avg_loss:0.245, val_acc:0.971]
Epoch [51/120    avg_loss:0.232, val_acc:0.969]
Epoch [52/120    avg_loss:0.185, val_acc:0.973]
Epoch [53/120    avg_loss:0.177, val_acc:0.975]
Epoch [54/120    avg_loss:0.168, val_acc:0.973]
Epoch [55/120    avg_loss:0.176, val_acc:0.975]
Epoch [56/120    avg_loss:0.176, val_acc:0.977]
Epoch [57/120    avg_loss:0.159, val_acc:0.979]
Epoch [58/120    avg_loss:0.153, val_acc:0.977]
Epoch [59/120    avg_loss:0.162, val_acc:0.983]
Epoch [60/120    avg_loss:0.185, val_acc:0.981]
Epoch [61/120    avg_loss:0.152, val_acc:0.988]
Epoch [62/120    avg_loss:0.160, val_acc:0.985]
Epoch [63/120    avg_loss:0.172, val_acc:0.988]
Epoch [64/120    avg_loss:0.180, val_acc:0.990]
Epoch [65/120    avg_loss:0.143, val_acc:0.983]
Epoch [66/120    avg_loss:0.160, val_acc:0.990]
Epoch [67/120    avg_loss:0.160, val_acc:0.988]
Epoch [68/120    avg_loss:0.163, val_acc:0.985]
Epoch [69/120    avg_loss:0.172, val_acc:0.988]
Epoch [70/120    avg_loss:0.157, val_acc:0.985]
Epoch [71/120    avg_loss:0.156, val_acc:0.990]
Epoch [72/120    avg_loss:0.139, val_acc:0.992]
Epoch [73/120    avg_loss:0.151, val_acc:0.988]
Epoch [74/120    avg_loss:0.145, val_acc:0.994]
Epoch [75/120    avg_loss:0.139, val_acc:0.988]
Epoch [76/120    avg_loss:0.153, val_acc:0.994]
Epoch [77/120    avg_loss:0.140, val_acc:0.990]
Epoch [78/120    avg_loss:0.153, val_acc:0.988]
Epoch [79/120    avg_loss:0.127, val_acc:0.994]
Epoch [80/120    avg_loss:0.139, val_acc:0.988]
Epoch [81/120    avg_loss:0.137, val_acc:0.992]
Epoch [82/120    avg_loss:0.138, val_acc:0.992]
Epoch [83/120    avg_loss:0.141, val_acc:0.992]
Epoch [84/120    avg_loss:0.141, val_acc:0.979]
Epoch [85/120    avg_loss:0.147, val_acc:0.992]
Epoch [86/120    avg_loss:0.147, val_acc:0.992]
Epoch [87/120    avg_loss:0.147, val_acc:0.990]
Epoch [88/120    avg_loss:0.142, val_acc:0.992]
Epoch [89/120    avg_loss:0.144, val_acc:0.992]
Epoch [90/120    avg_loss:0.135, val_acc:0.992]
Epoch [91/120    avg_loss:0.142, val_acc:0.994]
Epoch [92/120    avg_loss:0.143, val_acc:0.994]
Epoch [93/120    avg_loss:0.122, val_acc:0.990]
Epoch [94/120    avg_loss:0.145, val_acc:0.992]
Epoch [95/120    avg_loss:0.118, val_acc:0.992]
Epoch [96/120    avg_loss:0.138, val_acc:0.985]
Epoch [97/120    avg_loss:0.135, val_acc:0.985]
Epoch [98/120    avg_loss:0.138, val_acc:0.988]
Epoch [99/120    avg_loss:0.127, val_acc:0.992]
Epoch [100/120    avg_loss:0.138, val_acc:0.992]
Epoch [101/120    avg_loss:0.120, val_acc:0.985]
Epoch [102/120    avg_loss:0.119, val_acc:0.992]
Epoch [103/120    avg_loss:0.120, val_acc:0.994]
Epoch [104/120    avg_loss:0.128, val_acc:0.990]
Epoch [105/120    avg_loss:0.121, val_acc:0.985]
Epoch [106/120    avg_loss:0.116, val_acc:0.988]
Epoch [107/120    avg_loss:0.123, val_acc:0.990]
Epoch [108/120    avg_loss:0.127, val_acc:0.985]
Epoch [109/120    avg_loss:0.133, val_acc:0.983]
Epoch [110/120    avg_loss:0.137, val_acc:0.988]
Epoch [111/120    avg_loss:0.113, val_acc:0.985]
Epoch [112/120    avg_loss:0.111, val_acc:0.985]
Epoch [113/120    avg_loss:0.119, val_acc:0.985]
Epoch [114/120    avg_loss:0.110, val_acc:0.983]
Epoch [115/120    avg_loss:0.138, val_acc:0.985]
Epoch [116/120    avg_loss:0.116, val_acc:0.985]
Epoch [117/120    avg_loss:0.105, val_acc:0.985]
Epoch [118/120    avg_loss:0.117, val_acc:0.985]
Epoch [119/120    avg_loss:0.100, val_acc:0.985]
Epoch [120/120    avg_loss:0.111, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 679   0   0   1   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 195  26   0   0   0   0   0   0   6   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   6 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8912579957356

F1 scores:
[       nan 0.99560117 0.99095023 1.         0.91334895 0.90384615
 0.98800959 0.97826087 1.         1.         1.         0.99210526
 0.98675497 1.        ]

Kappa:
0.9876581989097738
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9bbd68c940>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.594, val_acc:0.335]
Epoch [2/120    avg_loss:2.377, val_acc:0.454]
Epoch [3/120    avg_loss:2.227, val_acc:0.471]
Epoch [4/120    avg_loss:2.104, val_acc:0.508]
Epoch [5/120    avg_loss:1.973, val_acc:0.542]
Epoch [6/120    avg_loss:1.859, val_acc:0.546]
Epoch [7/120    avg_loss:1.747, val_acc:0.573]
Epoch [8/120    avg_loss:1.649, val_acc:0.617]
Epoch [9/120    avg_loss:1.570, val_acc:0.654]
Epoch [10/120    avg_loss:1.475, val_acc:0.650]
Epoch [11/120    avg_loss:1.376, val_acc:0.683]
Epoch [12/120    avg_loss:1.303, val_acc:0.706]
Epoch [13/120    avg_loss:1.201, val_acc:0.752]
Epoch [14/120    avg_loss:1.114, val_acc:0.762]
Epoch [15/120    avg_loss:1.028, val_acc:0.787]
Epoch [16/120    avg_loss:1.001, val_acc:0.783]
Epoch [17/120    avg_loss:0.927, val_acc:0.760]
Epoch [18/120    avg_loss:0.831, val_acc:0.800]
Epoch [19/120    avg_loss:0.769, val_acc:0.787]
Epoch [20/120    avg_loss:0.724, val_acc:0.802]
Epoch [21/120    avg_loss:0.663, val_acc:0.812]
Epoch [22/120    avg_loss:0.636, val_acc:0.848]
Epoch [23/120    avg_loss:0.576, val_acc:0.825]
Epoch [24/120    avg_loss:0.550, val_acc:0.863]
Epoch [25/120    avg_loss:0.515, val_acc:0.812]
Epoch [26/120    avg_loss:0.535, val_acc:0.827]
Epoch [27/120    avg_loss:0.465, val_acc:0.917]
Epoch [28/120    avg_loss:0.620, val_acc:0.806]
Epoch [29/120    avg_loss:0.594, val_acc:0.933]
Epoch [30/120    avg_loss:0.473, val_acc:0.883]
Epoch [31/120    avg_loss:0.451, val_acc:0.917]
Epoch [32/120    avg_loss:0.449, val_acc:0.948]
Epoch [33/120    avg_loss:0.364, val_acc:0.904]
Epoch [34/120    avg_loss:0.348, val_acc:0.935]
Epoch [35/120    avg_loss:0.356, val_acc:0.963]
Epoch [36/120    avg_loss:0.320, val_acc:0.929]
Epoch [37/120    avg_loss:0.282, val_acc:0.960]
Epoch [38/120    avg_loss:0.252, val_acc:0.942]
Epoch [39/120    avg_loss:0.255, val_acc:0.960]
Epoch [40/120    avg_loss:0.264, val_acc:0.948]
Epoch [41/120    avg_loss:0.247, val_acc:0.971]
Epoch [42/120    avg_loss:0.277, val_acc:0.967]
Epoch [43/120    avg_loss:0.258, val_acc:0.942]
Epoch [44/120    avg_loss:0.253, val_acc:0.946]
Epoch [45/120    avg_loss:0.218, val_acc:0.963]
Epoch [46/120    avg_loss:0.205, val_acc:0.965]
Epoch [47/120    avg_loss:0.246, val_acc:0.952]
Epoch [48/120    avg_loss:0.192, val_acc:0.948]
Epoch [49/120    avg_loss:0.162, val_acc:0.977]
Epoch [50/120    avg_loss:0.167, val_acc:0.967]
Epoch [51/120    avg_loss:0.179, val_acc:0.952]
Epoch [52/120    avg_loss:0.173, val_acc:0.963]
Epoch [53/120    avg_loss:0.193, val_acc:0.965]
Epoch [54/120    avg_loss:0.140, val_acc:0.965]
Epoch [55/120    avg_loss:0.143, val_acc:0.965]
Epoch [56/120    avg_loss:0.122, val_acc:0.965]
Epoch [57/120    avg_loss:0.134, val_acc:0.927]
Epoch [58/120    avg_loss:0.154, val_acc:0.950]
Epoch [59/120    avg_loss:0.157, val_acc:0.975]
Epoch [60/120    avg_loss:0.177, val_acc:0.956]
Epoch [61/120    avg_loss:0.167, val_acc:0.965]
Epoch [62/120    avg_loss:0.138, val_acc:0.971]
Epoch [63/120    avg_loss:0.108, val_acc:0.967]
Epoch [64/120    avg_loss:0.096, val_acc:0.973]
Epoch [65/120    avg_loss:0.094, val_acc:0.973]
Epoch [66/120    avg_loss:0.086, val_acc:0.975]
Epoch [67/120    avg_loss:0.114, val_acc:0.973]
Epoch [68/120    avg_loss:0.092, val_acc:0.973]
Epoch [69/120    avg_loss:0.100, val_acc:0.975]
Epoch [70/120    avg_loss:0.088, val_acc:0.977]
Epoch [71/120    avg_loss:0.083, val_acc:0.977]
Epoch [72/120    avg_loss:0.081, val_acc:0.977]
Epoch [73/120    avg_loss:0.085, val_acc:0.979]
Epoch [74/120    avg_loss:0.084, val_acc:0.979]
Epoch [75/120    avg_loss:0.084, val_acc:0.979]
Epoch [76/120    avg_loss:0.090, val_acc:0.979]
Epoch [77/120    avg_loss:0.079, val_acc:0.979]
Epoch [78/120    avg_loss:0.082, val_acc:0.977]
Epoch [79/120    avg_loss:0.078, val_acc:0.981]
Epoch [80/120    avg_loss:0.086, val_acc:0.981]
Epoch [81/120    avg_loss:0.072, val_acc:0.983]
Epoch [82/120    avg_loss:0.073, val_acc:0.979]
Epoch [83/120    avg_loss:0.101, val_acc:0.981]
Epoch [84/120    avg_loss:0.076, val_acc:0.981]
Epoch [85/120    avg_loss:0.077, val_acc:0.985]
Epoch [86/120    avg_loss:0.075, val_acc:0.981]
Epoch [87/120    avg_loss:0.083, val_acc:0.983]
Epoch [88/120    avg_loss:0.079, val_acc:0.981]
Epoch [89/120    avg_loss:0.072, val_acc:0.981]
Epoch [90/120    avg_loss:0.081, val_acc:0.981]
Epoch [91/120    avg_loss:0.083, val_acc:0.981]
Epoch [92/120    avg_loss:0.074, val_acc:0.983]
Epoch [93/120    avg_loss:0.062, val_acc:0.981]
Epoch [94/120    avg_loss:0.071, val_acc:0.983]
Epoch [95/120    avg_loss:0.074, val_acc:0.981]
Epoch [96/120    avg_loss:0.067, val_acc:0.983]
Epoch [97/120    avg_loss:0.066, val_acc:0.983]
Epoch [98/120    avg_loss:0.078, val_acc:0.981]
Epoch [99/120    avg_loss:0.064, val_acc:0.981]
Epoch [100/120    avg_loss:0.075, val_acc:0.981]
Epoch [101/120    avg_loss:0.076, val_acc:0.981]
Epoch [102/120    avg_loss:0.072, val_acc:0.981]
Epoch [103/120    avg_loss:0.074, val_acc:0.981]
Epoch [104/120    avg_loss:0.065, val_acc:0.981]
Epoch [105/120    avg_loss:0.072, val_acc:0.981]
Epoch [106/120    avg_loss:0.063, val_acc:0.981]
Epoch [107/120    avg_loss:0.065, val_acc:0.981]
Epoch [108/120    avg_loss:0.068, val_acc:0.981]
Epoch [109/120    avg_loss:0.065, val_acc:0.981]
Epoch [110/120    avg_loss:0.068, val_acc:0.981]
Epoch [111/120    avg_loss:0.073, val_acc:0.981]
Epoch [112/120    avg_loss:0.081, val_acc:0.981]
Epoch [113/120    avg_loss:0.068, val_acc:0.981]
Epoch [114/120    avg_loss:0.078, val_acc:0.981]
Epoch [115/120    avg_loss:0.075, val_acc:0.981]
Epoch [116/120    avg_loss:0.069, val_acc:0.981]
Epoch [117/120    avg_loss:0.065, val_acc:0.981]
Epoch [118/120    avg_loss:0.062, val_acc:0.981]
Epoch [119/120    avg_loss:0.061, val_acc:0.981]
Epoch [120/120    avg_loss:0.072, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  19 126   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.14712153518124

F1 scores:
[       nan 0.99854015 0.99095023 1.         0.92576419 0.88111888
 0.99514563 0.97826087 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9905043438577531
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5488db28d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.591, val_acc:0.373]
Epoch [2/120    avg_loss:2.369, val_acc:0.396]
Epoch [3/120    avg_loss:2.239, val_acc:0.485]
Epoch [4/120    avg_loss:2.141, val_acc:0.496]
Epoch [5/120    avg_loss:2.037, val_acc:0.508]
Epoch [6/120    avg_loss:1.930, val_acc:0.560]
Epoch [7/120    avg_loss:1.831, val_acc:0.602]
Epoch [8/120    avg_loss:1.733, val_acc:0.627]
Epoch [9/120    avg_loss:1.629, val_acc:0.646]
Epoch [10/120    avg_loss:1.556, val_acc:0.675]
Epoch [11/120    avg_loss:1.473, val_acc:0.700]
Epoch [12/120    avg_loss:1.375, val_acc:0.721]
Epoch [13/120    avg_loss:1.291, val_acc:0.725]
Epoch [14/120    avg_loss:1.211, val_acc:0.738]
Epoch [15/120    avg_loss:1.136, val_acc:0.744]
Epoch [16/120    avg_loss:1.051, val_acc:0.760]
Epoch [17/120    avg_loss:0.978, val_acc:0.779]
Epoch [18/120    avg_loss:0.930, val_acc:0.815]
Epoch [19/120    avg_loss:0.863, val_acc:0.794]
Epoch [20/120    avg_loss:0.822, val_acc:0.817]
Epoch [21/120    avg_loss:0.784, val_acc:0.806]
Epoch [22/120    avg_loss:0.757, val_acc:0.804]
Epoch [23/120    avg_loss:0.692, val_acc:0.812]
Epoch [24/120    avg_loss:0.660, val_acc:0.833]
Epoch [25/120    avg_loss:0.629, val_acc:0.821]
Epoch [26/120    avg_loss:0.602, val_acc:0.833]
Epoch [27/120    avg_loss:0.572, val_acc:0.879]
Epoch [28/120    avg_loss:0.501, val_acc:0.881]
Epoch [29/120    avg_loss:0.487, val_acc:0.881]
Epoch [30/120    avg_loss:0.470, val_acc:0.902]
Epoch [31/120    avg_loss:0.477, val_acc:0.933]
Epoch [32/120    avg_loss:0.413, val_acc:0.927]
Epoch [33/120    avg_loss:0.400, val_acc:0.904]
Epoch [34/120    avg_loss:0.367, val_acc:0.919]
Epoch [35/120    avg_loss:0.354, val_acc:0.917]
Epoch [36/120    avg_loss:0.335, val_acc:0.942]
Epoch [37/120    avg_loss:0.352, val_acc:0.942]
Epoch [38/120    avg_loss:0.342, val_acc:0.923]
Epoch [39/120    avg_loss:0.335, val_acc:0.944]
Epoch [40/120    avg_loss:0.353, val_acc:0.967]
Epoch [41/120    avg_loss:0.326, val_acc:0.902]
Epoch [42/120    avg_loss:0.319, val_acc:0.958]
Epoch [43/120    avg_loss:0.299, val_acc:0.958]
Epoch [44/120    avg_loss:0.337, val_acc:0.933]
Epoch [45/120    avg_loss:0.296, val_acc:0.950]
Epoch [46/120    avg_loss:0.230, val_acc:0.965]
Epoch [47/120    avg_loss:0.285, val_acc:0.940]
Epoch [48/120    avg_loss:0.273, val_acc:0.938]
Epoch [49/120    avg_loss:0.272, val_acc:0.929]
Epoch [50/120    avg_loss:0.238, val_acc:0.956]
Epoch [51/120    avg_loss:0.225, val_acc:0.967]
Epoch [52/120    avg_loss:0.199, val_acc:0.969]
Epoch [53/120    avg_loss:0.233, val_acc:0.933]
Epoch [54/120    avg_loss:0.208, val_acc:0.969]
Epoch [55/120    avg_loss:0.192, val_acc:0.944]
Epoch [56/120    avg_loss:0.227, val_acc:0.954]
Epoch [57/120    avg_loss:0.211, val_acc:0.965]
Epoch [58/120    avg_loss:0.202, val_acc:0.967]
Epoch [59/120    avg_loss:0.228, val_acc:0.969]
Epoch [60/120    avg_loss:0.225, val_acc:0.965]
Epoch [61/120    avg_loss:0.195, val_acc:0.971]
Epoch [62/120    avg_loss:0.224, val_acc:0.877]
Epoch [63/120    avg_loss:0.198, val_acc:0.979]
Epoch [64/120    avg_loss:0.147, val_acc:0.965]
Epoch [65/120    avg_loss:0.188, val_acc:0.971]
Epoch [66/120    avg_loss:0.137, val_acc:0.975]
Epoch [67/120    avg_loss:0.159, val_acc:0.960]
Epoch [68/120    avg_loss:0.133, val_acc:0.979]
Epoch [69/120    avg_loss:0.138, val_acc:0.977]
Epoch [70/120    avg_loss:0.129, val_acc:0.977]
Epoch [71/120    avg_loss:0.120, val_acc:0.935]
Epoch [72/120    avg_loss:0.117, val_acc:0.981]
Epoch [73/120    avg_loss:0.107, val_acc:0.985]
Epoch [74/120    avg_loss:0.081, val_acc:0.965]
Epoch [75/120    avg_loss:0.131, val_acc:0.981]
Epoch [76/120    avg_loss:0.112, val_acc:0.983]
Epoch [77/120    avg_loss:0.105, val_acc:0.985]
Epoch [78/120    avg_loss:0.111, val_acc:0.985]
Epoch [79/120    avg_loss:0.093, val_acc:0.975]
Epoch [80/120    avg_loss:0.085, val_acc:0.954]
Epoch [81/120    avg_loss:0.106, val_acc:0.983]
Epoch [82/120    avg_loss:0.099, val_acc:0.971]
Epoch [83/120    avg_loss:0.087, val_acc:0.979]
Epoch [84/120    avg_loss:0.093, val_acc:0.990]
Epoch [85/120    avg_loss:0.072, val_acc:0.990]
Epoch [86/120    avg_loss:0.081, val_acc:0.990]
Epoch [87/120    avg_loss:0.089, val_acc:0.988]
Epoch [88/120    avg_loss:0.102, val_acc:0.971]
Epoch [89/120    avg_loss:0.104, val_acc:0.977]
Epoch [90/120    avg_loss:0.082, val_acc:0.985]
Epoch [91/120    avg_loss:0.060, val_acc:0.973]
Epoch [92/120    avg_loss:0.087, val_acc:0.960]
Epoch [93/120    avg_loss:0.129, val_acc:0.973]
Epoch [94/120    avg_loss:0.148, val_acc:0.973]
Epoch [95/120    avg_loss:0.108, val_acc:0.979]
Epoch [96/120    avg_loss:0.113, val_acc:0.958]
Epoch [97/120    avg_loss:0.137, val_acc:0.981]
Epoch [98/120    avg_loss:0.131, val_acc:0.958]
Epoch [99/120    avg_loss:0.121, val_acc:0.994]
Epoch [100/120    avg_loss:0.081, val_acc:0.985]
Epoch [101/120    avg_loss:0.077, val_acc:0.990]
Epoch [102/120    avg_loss:0.069, val_acc:0.985]
Epoch [103/120    avg_loss:0.076, val_acc:0.983]
Epoch [104/120    avg_loss:0.043, val_acc:0.990]
Epoch [105/120    avg_loss:0.053, val_acc:0.998]
Epoch [106/120    avg_loss:0.045, val_acc:0.998]
Epoch [107/120    avg_loss:0.053, val_acc:0.988]
Epoch [108/120    avg_loss:0.052, val_acc:0.992]
Epoch [109/120    avg_loss:0.051, val_acc:0.969]
Epoch [110/120    avg_loss:0.051, val_acc:0.990]
Epoch [111/120    avg_loss:0.055, val_acc:0.990]
Epoch [112/120    avg_loss:0.052, val_acc:0.992]
Epoch [113/120    avg_loss:0.034, val_acc:0.996]
Epoch [114/120    avg_loss:0.044, val_acc:0.992]
Epoch [115/120    avg_loss:0.041, val_acc:1.000]
Epoch [116/120    avg_loss:0.067, val_acc:0.977]
Epoch [117/120    avg_loss:0.058, val_acc:0.954]
Epoch [118/120    avg_loss:0.056, val_acc:0.996]
Epoch [119/120    avg_loss:0.066, val_acc:0.985]
Epoch [120/120    avg_loss:0.044, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0  14 203  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.996337   1.         0.97046414 0.94418605 0.96666667
 0.98800959 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9931170432101006
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd6303778d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.566, val_acc:0.327]
Epoch [2/120    avg_loss:2.363, val_acc:0.375]
Epoch [3/120    avg_loss:2.221, val_acc:0.469]
Epoch [4/120    avg_loss:2.097, val_acc:0.487]
Epoch [5/120    avg_loss:1.992, val_acc:0.512]
Epoch [6/120    avg_loss:1.901, val_acc:0.554]
Epoch [7/120    avg_loss:1.801, val_acc:0.592]
Epoch [8/120    avg_loss:1.708, val_acc:0.642]
Epoch [9/120    avg_loss:1.603, val_acc:0.654]
Epoch [10/120    avg_loss:1.486, val_acc:0.671]
Epoch [11/120    avg_loss:1.357, val_acc:0.725]
Epoch [12/120    avg_loss:1.282, val_acc:0.727]
Epoch [13/120    avg_loss:1.192, val_acc:0.765]
Epoch [14/120    avg_loss:1.074, val_acc:0.746]
Epoch [15/120    avg_loss:0.997, val_acc:0.767]
Epoch [16/120    avg_loss:0.899, val_acc:0.792]
Epoch [17/120    avg_loss:0.855, val_acc:0.773]
Epoch [18/120    avg_loss:0.790, val_acc:0.838]
Epoch [19/120    avg_loss:0.713, val_acc:0.829]
Epoch [20/120    avg_loss:0.723, val_acc:0.865]
Epoch [21/120    avg_loss:0.641, val_acc:0.865]
Epoch [22/120    avg_loss:0.587, val_acc:0.875]
Epoch [23/120    avg_loss:0.543, val_acc:0.890]
Epoch [24/120    avg_loss:0.532, val_acc:0.881]
Epoch [25/120    avg_loss:0.472, val_acc:0.927]
Epoch [26/120    avg_loss:0.476, val_acc:0.908]
Epoch [27/120    avg_loss:0.426, val_acc:0.948]
Epoch [28/120    avg_loss:0.402, val_acc:0.933]
Epoch [29/120    avg_loss:0.412, val_acc:0.944]
Epoch [30/120    avg_loss:0.354, val_acc:0.944]
Epoch [31/120    avg_loss:0.331, val_acc:0.940]
Epoch [32/120    avg_loss:0.447, val_acc:0.894]
Epoch [33/120    avg_loss:0.435, val_acc:0.958]
Epoch [34/120    avg_loss:0.377, val_acc:0.958]
Epoch [35/120    avg_loss:0.349, val_acc:0.948]
Epoch [36/120    avg_loss:0.298, val_acc:0.977]
Epoch [37/120    avg_loss:0.293, val_acc:0.965]
Epoch [38/120    avg_loss:0.279, val_acc:0.929]
Epoch [39/120    avg_loss:0.320, val_acc:0.967]
Epoch [40/120    avg_loss:0.274, val_acc:0.950]
Epoch [41/120    avg_loss:0.286, val_acc:0.942]
Epoch [42/120    avg_loss:0.246, val_acc:0.967]
Epoch [43/120    avg_loss:0.242, val_acc:0.942]
Epoch [44/120    avg_loss:0.230, val_acc:0.971]
Epoch [45/120    avg_loss:0.257, val_acc:0.975]
Epoch [46/120    avg_loss:0.234, val_acc:0.973]
Epoch [47/120    avg_loss:0.223, val_acc:0.973]
Epoch [48/120    avg_loss:0.222, val_acc:0.969]
Epoch [49/120    avg_loss:0.279, val_acc:0.952]
Epoch [50/120    avg_loss:0.247, val_acc:0.988]
Epoch [51/120    avg_loss:0.163, val_acc:0.985]
Epoch [52/120    avg_loss:0.150, val_acc:0.988]
Epoch [53/120    avg_loss:0.156, val_acc:0.981]
Epoch [54/120    avg_loss:0.159, val_acc:0.990]
Epoch [55/120    avg_loss:0.157, val_acc:0.990]
Epoch [56/120    avg_loss:0.156, val_acc:0.990]
Epoch [57/120    avg_loss:0.142, val_acc:0.992]
Epoch [58/120    avg_loss:0.172, val_acc:0.988]
Epoch [59/120    avg_loss:0.142, val_acc:0.992]
Epoch [60/120    avg_loss:0.150, val_acc:0.990]
Epoch [61/120    avg_loss:0.153, val_acc:0.985]
Epoch [62/120    avg_loss:0.147, val_acc:0.988]
Epoch [63/120    avg_loss:0.162, val_acc:0.990]
Epoch [64/120    avg_loss:0.152, val_acc:0.988]
Epoch [65/120    avg_loss:0.149, val_acc:0.988]
Epoch [66/120    avg_loss:0.145, val_acc:0.990]
Epoch [67/120    avg_loss:0.137, val_acc:0.988]
Epoch [68/120    avg_loss:0.132, val_acc:0.990]
Epoch [69/120    avg_loss:0.147, val_acc:0.988]
Epoch [70/120    avg_loss:0.157, val_acc:0.983]
Epoch [71/120    avg_loss:0.136, val_acc:0.988]
Epoch [72/120    avg_loss:0.147, val_acc:0.985]
Epoch [73/120    avg_loss:0.133, val_acc:0.990]
Epoch [74/120    avg_loss:0.146, val_acc:0.990]
Epoch [75/120    avg_loss:0.141, val_acc:0.990]
Epoch [76/120    avg_loss:0.167, val_acc:0.990]
Epoch [77/120    avg_loss:0.132, val_acc:0.990]
Epoch [78/120    avg_loss:0.123, val_acc:0.990]
Epoch [79/120    avg_loss:0.130, val_acc:0.990]
Epoch [80/120    avg_loss:0.137, val_acc:0.990]
Epoch [81/120    avg_loss:0.129, val_acc:0.990]
Epoch [82/120    avg_loss:0.135, val_acc:0.990]
Epoch [83/120    avg_loss:0.127, val_acc:0.990]
Epoch [84/120    avg_loss:0.137, val_acc:0.990]
Epoch [85/120    avg_loss:0.126, val_acc:0.990]
Epoch [86/120    avg_loss:0.129, val_acc:0.990]
Epoch [87/120    avg_loss:0.141, val_acc:0.990]
Epoch [88/120    avg_loss:0.137, val_acc:0.990]
Epoch [89/120    avg_loss:0.145, val_acc:0.990]
Epoch [90/120    avg_loss:0.138, val_acc:0.990]
Epoch [91/120    avg_loss:0.135, val_acc:0.992]
Epoch [92/120    avg_loss:0.135, val_acc:0.990]
Epoch [93/120    avg_loss:0.121, val_acc:0.990]
Epoch [94/120    avg_loss:0.140, val_acc:0.990]
Epoch [95/120    avg_loss:0.125, val_acc:0.990]
Epoch [96/120    avg_loss:0.134, val_acc:0.990]
Epoch [97/120    avg_loss:0.134, val_acc:0.990]
Epoch [98/120    avg_loss:0.161, val_acc:0.992]
Epoch [99/120    avg_loss:0.124, val_acc:0.992]
Epoch [100/120    avg_loss:0.148, val_acc:0.992]
Epoch [101/120    avg_loss:0.151, val_acc:0.992]
Epoch [102/120    avg_loss:0.130, val_acc:0.992]
Epoch [103/120    avg_loss:0.125, val_acc:0.992]
Epoch [104/120    avg_loss:0.135, val_acc:0.992]
Epoch [105/120    avg_loss:0.138, val_acc:0.992]
Epoch [106/120    avg_loss:0.136, val_acc:0.992]
Epoch [107/120    avg_loss:0.125, val_acc:0.992]
Epoch [108/120    avg_loss:0.133, val_acc:0.992]
Epoch [109/120    avg_loss:0.131, val_acc:0.992]
Epoch [110/120    avg_loss:0.134, val_acc:0.992]
Epoch [111/120    avg_loss:0.154, val_acc:0.992]
Epoch [112/120    avg_loss:0.150, val_acc:0.992]
Epoch [113/120    avg_loss:0.131, val_acc:0.992]
Epoch [114/120    avg_loss:0.143, val_acc:0.992]
Epoch [115/120    avg_loss:0.130, val_acc:0.992]
Epoch [116/120    avg_loss:0.142, val_acc:0.992]
Epoch [117/120    avg_loss:0.126, val_acc:0.992]
Epoch [118/120    avg_loss:0.138, val_acc:0.992]
Epoch [119/120    avg_loss:0.132, val_acc:0.992]
Epoch [120/120    avg_loss:0.139, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 189  38   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.95522388059702

F1 scores:
[       nan 0.99634236 0.99545455 1.         0.90214797 0.87384615
 0.98795181 0.98924731 1.         1.         1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9883707414311451
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7cd0267860>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.676, val_acc:0.446]
Epoch [2/120    avg_loss:2.467, val_acc:0.315]
Epoch [3/120    avg_loss:2.326, val_acc:0.512]
Epoch [4/120    avg_loss:2.200, val_acc:0.579]
Epoch [5/120    avg_loss:2.094, val_acc:0.573]
Epoch [6/120    avg_loss:1.977, val_acc:0.590]
Epoch [7/120    avg_loss:1.858, val_acc:0.598]
Epoch [8/120    avg_loss:1.747, val_acc:0.600]
Epoch [9/120    avg_loss:1.652, val_acc:0.615]
Epoch [10/120    avg_loss:1.529, val_acc:0.633]
Epoch [11/120    avg_loss:1.437, val_acc:0.700]
Epoch [12/120    avg_loss:1.333, val_acc:0.715]
Epoch [13/120    avg_loss:1.206, val_acc:0.735]
Epoch [14/120    avg_loss:1.119, val_acc:0.740]
Epoch [15/120    avg_loss:1.039, val_acc:0.754]
Epoch [16/120    avg_loss:0.956, val_acc:0.746]
Epoch [17/120    avg_loss:0.870, val_acc:0.806]
Epoch [18/120    avg_loss:0.831, val_acc:0.790]
Epoch [19/120    avg_loss:0.777, val_acc:0.785]
Epoch [20/120    avg_loss:0.717, val_acc:0.777]
Epoch [21/120    avg_loss:0.640, val_acc:0.879]
Epoch [22/120    avg_loss:0.618, val_acc:0.827]
Epoch [23/120    avg_loss:0.590, val_acc:0.881]
Epoch [24/120    avg_loss:0.553, val_acc:0.902]
Epoch [25/120    avg_loss:0.519, val_acc:0.925]
Epoch [26/120    avg_loss:0.536, val_acc:0.831]
Epoch [27/120    avg_loss:0.481, val_acc:0.908]
Epoch [28/120    avg_loss:0.441, val_acc:0.935]
Epoch [29/120    avg_loss:0.463, val_acc:0.927]
Epoch [30/120    avg_loss:0.382, val_acc:0.944]
Epoch [31/120    avg_loss:0.375, val_acc:0.929]
Epoch [32/120    avg_loss:0.361, val_acc:0.929]
Epoch [33/120    avg_loss:0.340, val_acc:0.938]
Epoch [34/120    avg_loss:0.365, val_acc:0.952]
Epoch [35/120    avg_loss:0.295, val_acc:0.956]
Epoch [36/120    avg_loss:0.299, val_acc:0.956]
Epoch [37/120    avg_loss:0.298, val_acc:0.954]
Epoch [38/120    avg_loss:0.302, val_acc:0.935]
Epoch [39/120    avg_loss:0.267, val_acc:0.950]
Epoch [40/120    avg_loss:0.282, val_acc:0.931]
Epoch [41/120    avg_loss:0.246, val_acc:0.969]
Epoch [42/120    avg_loss:0.253, val_acc:0.958]
Epoch [43/120    avg_loss:0.314, val_acc:0.948]
Epoch [44/120    avg_loss:0.299, val_acc:0.944]
Epoch [45/120    avg_loss:0.286, val_acc:0.883]
Epoch [46/120    avg_loss:0.270, val_acc:0.923]
Epoch [47/120    avg_loss:0.308, val_acc:0.960]
Epoch [48/120    avg_loss:0.256, val_acc:0.969]
Epoch [49/120    avg_loss:0.230, val_acc:0.977]
Epoch [50/120    avg_loss:0.194, val_acc:0.973]
Epoch [51/120    avg_loss:0.189, val_acc:0.963]
Epoch [52/120    avg_loss:0.215, val_acc:0.944]
Epoch [53/120    avg_loss:0.197, val_acc:0.971]
Epoch [54/120    avg_loss:0.206, val_acc:0.965]
Epoch [55/120    avg_loss:0.195, val_acc:0.965]
Epoch [56/120    avg_loss:0.217, val_acc:0.948]
Epoch [57/120    avg_loss:0.236, val_acc:0.956]
Epoch [58/120    avg_loss:0.175, val_acc:0.969]
Epoch [59/120    avg_loss:0.163, val_acc:0.971]
Epoch [60/120    avg_loss:0.147, val_acc:0.983]
Epoch [61/120    avg_loss:0.117, val_acc:0.979]
Epoch [62/120    avg_loss:0.144, val_acc:0.975]
Epoch [63/120    avg_loss:0.139, val_acc:0.981]
Epoch [64/120    avg_loss:0.122, val_acc:0.981]
Epoch [65/120    avg_loss:0.124, val_acc:0.973]
Epoch [66/120    avg_loss:0.125, val_acc:0.979]
Epoch [67/120    avg_loss:0.135, val_acc:0.981]
Epoch [68/120    avg_loss:0.132, val_acc:0.981]
Epoch [69/120    avg_loss:0.104, val_acc:0.979]
Epoch [70/120    avg_loss:0.100, val_acc:0.985]
Epoch [71/120    avg_loss:0.161, val_acc:0.973]
Epoch [72/120    avg_loss:0.117, val_acc:0.985]
Epoch [73/120    avg_loss:0.108, val_acc:0.979]
Epoch [74/120    avg_loss:0.146, val_acc:0.981]
Epoch [75/120    avg_loss:0.127, val_acc:0.971]
Epoch [76/120    avg_loss:0.114, val_acc:0.990]
Epoch [77/120    avg_loss:0.106, val_acc:0.985]
Epoch [78/120    avg_loss:0.090, val_acc:0.988]
Epoch [79/120    avg_loss:0.099, val_acc:0.985]
Epoch [80/120    avg_loss:0.085, val_acc:0.988]
Epoch [81/120    avg_loss:0.059, val_acc:0.992]
Epoch [82/120    avg_loss:0.071, val_acc:0.988]
Epoch [83/120    avg_loss:0.094, val_acc:0.981]
Epoch [84/120    avg_loss:0.088, val_acc:0.985]
Epoch [85/120    avg_loss:0.090, val_acc:0.975]
Epoch [86/120    avg_loss:0.093, val_acc:0.985]
Epoch [87/120    avg_loss:0.100, val_acc:0.983]
Epoch [88/120    avg_loss:0.069, val_acc:0.990]
Epoch [89/120    avg_loss:0.073, val_acc:0.973]
Epoch [90/120    avg_loss:0.082, val_acc:0.992]
Epoch [91/120    avg_loss:0.051, val_acc:0.996]
Epoch [92/120    avg_loss:0.054, val_acc:0.983]
Epoch [93/120    avg_loss:0.044, val_acc:0.992]
Epoch [94/120    avg_loss:0.073, val_acc:0.981]
Epoch [95/120    avg_loss:0.095, val_acc:0.992]
Epoch [96/120    avg_loss:0.075, val_acc:0.988]
Epoch [97/120    avg_loss:0.056, val_acc:0.992]
Epoch [98/120    avg_loss:0.057, val_acc:0.988]
Epoch [99/120    avg_loss:0.056, val_acc:0.990]
Epoch [100/120    avg_loss:0.050, val_acc:0.973]
Epoch [101/120    avg_loss:0.111, val_acc:0.981]
Epoch [102/120    avg_loss:0.075, val_acc:0.992]
Epoch [103/120    avg_loss:0.064, val_acc:0.990]
Epoch [104/120    avg_loss:0.061, val_acc:0.994]
Epoch [105/120    avg_loss:0.044, val_acc:0.994]
Epoch [106/120    avg_loss:0.055, val_acc:0.998]
Epoch [107/120    avg_loss:0.033, val_acc:0.998]
Epoch [108/120    avg_loss:0.033, val_acc:0.998]
Epoch [109/120    avg_loss:0.030, val_acc:0.998]
Epoch [110/120    avg_loss:0.035, val_acc:0.996]
Epoch [111/120    avg_loss:0.034, val_acc:0.996]
Epoch [112/120    avg_loss:0.034, val_acc:0.994]
Epoch [113/120    avg_loss:0.031, val_acc:0.992]
Epoch [114/120    avg_loss:0.030, val_acc:0.994]
Epoch [115/120    avg_loss:0.036, val_acc:0.994]
Epoch [116/120    avg_loss:0.033, val_acc:0.994]
Epoch [117/120    avg_loss:0.032, val_acc:0.998]
Epoch [118/120    avg_loss:0.032, val_acc:0.994]
Epoch [119/120    avg_loss:0.031, val_acc:0.994]
Epoch [120/120    avg_loss:0.029, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 676   0   0   0   0   9   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.55223880597015

F1 scores:
[       nan 0.99338722 0.99545455 1.         0.97777778 0.96598639
 0.97862233 0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9950160966062548
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f12dd805908>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.630, val_acc:0.283]
Epoch [2/120    avg_loss:2.423, val_acc:0.435]
Epoch [3/120    avg_loss:2.265, val_acc:0.525]
Epoch [4/120    avg_loss:2.146, val_acc:0.537]
Epoch [5/120    avg_loss:2.032, val_acc:0.590]
Epoch [6/120    avg_loss:1.893, val_acc:0.602]
Epoch [7/120    avg_loss:1.779, val_acc:0.610]
Epoch [8/120    avg_loss:1.645, val_acc:0.635]
Epoch [9/120    avg_loss:1.521, val_acc:0.656]
Epoch [10/120    avg_loss:1.405, val_acc:0.685]
Epoch [11/120    avg_loss:1.294, val_acc:0.713]
Epoch [12/120    avg_loss:1.215, val_acc:0.706]
Epoch [13/120    avg_loss:1.149, val_acc:0.742]
Epoch [14/120    avg_loss:1.073, val_acc:0.735]
Epoch [15/120    avg_loss:0.971, val_acc:0.746]
Epoch [16/120    avg_loss:0.906, val_acc:0.752]
Epoch [17/120    avg_loss:0.830, val_acc:0.783]
Epoch [18/120    avg_loss:0.789, val_acc:0.760]
Epoch [19/120    avg_loss:0.743, val_acc:0.790]
Epoch [20/120    avg_loss:0.708, val_acc:0.825]
Epoch [21/120    avg_loss:0.663, val_acc:0.877]
Epoch [22/120    avg_loss:0.611, val_acc:0.871]
Epoch [23/120    avg_loss:0.575, val_acc:0.846]
Epoch [24/120    avg_loss:0.578, val_acc:0.831]
Epoch [25/120    avg_loss:0.531, val_acc:0.915]
Epoch [26/120    avg_loss:0.484, val_acc:0.931]
Epoch [27/120    avg_loss:0.458, val_acc:0.921]
Epoch [28/120    avg_loss:0.418, val_acc:0.938]
Epoch [29/120    avg_loss:0.473, val_acc:0.923]
Epoch [30/120    avg_loss:0.415, val_acc:0.954]
Epoch [31/120    avg_loss:0.365, val_acc:0.931]
Epoch [32/120    avg_loss:0.353, val_acc:0.942]
Epoch [33/120    avg_loss:0.329, val_acc:0.946]
Epoch [34/120    avg_loss:0.329, val_acc:0.956]
Epoch [35/120    avg_loss:0.324, val_acc:0.954]
Epoch [36/120    avg_loss:0.320, val_acc:0.946]
Epoch [37/120    avg_loss:0.330, val_acc:0.944]
Epoch [38/120    avg_loss:0.299, val_acc:0.942]
Epoch [39/120    avg_loss:0.307, val_acc:0.946]
Epoch [40/120    avg_loss:0.275, val_acc:0.958]
Epoch [41/120    avg_loss:0.256, val_acc:0.969]
Epoch [42/120    avg_loss:0.298, val_acc:0.931]
Epoch [43/120    avg_loss:0.264, val_acc:0.952]
Epoch [44/120    avg_loss:0.252, val_acc:0.960]
Epoch [45/120    avg_loss:0.286, val_acc:0.946]
Epoch [46/120    avg_loss:0.276, val_acc:0.938]
Epoch [47/120    avg_loss:0.272, val_acc:0.971]
Epoch [48/120    avg_loss:0.208, val_acc:0.969]
Epoch [49/120    avg_loss:0.218, val_acc:0.971]
Epoch [50/120    avg_loss:0.226, val_acc:0.954]
Epoch [51/120    avg_loss:0.214, val_acc:0.956]
Epoch [52/120    avg_loss:0.199, val_acc:0.967]
Epoch [53/120    avg_loss:0.203, val_acc:0.948]
Epoch [54/120    avg_loss:0.238, val_acc:0.965]
Epoch [55/120    avg_loss:0.177, val_acc:0.977]
Epoch [56/120    avg_loss:0.198, val_acc:0.967]
Epoch [57/120    avg_loss:0.215, val_acc:0.956]
Epoch [58/120    avg_loss:0.176, val_acc:0.969]
Epoch [59/120    avg_loss:0.184, val_acc:0.967]
Epoch [60/120    avg_loss:0.160, val_acc:0.981]
Epoch [61/120    avg_loss:0.147, val_acc:0.975]
Epoch [62/120    avg_loss:0.137, val_acc:0.985]
Epoch [63/120    avg_loss:0.131, val_acc:0.977]
Epoch [64/120    avg_loss:0.112, val_acc:0.985]
Epoch [65/120    avg_loss:0.164, val_acc:0.965]
Epoch [66/120    avg_loss:0.167, val_acc:0.958]
Epoch [67/120    avg_loss:0.174, val_acc:0.983]
Epoch [68/120    avg_loss:0.151, val_acc:0.983]
Epoch [69/120    avg_loss:0.153, val_acc:0.975]
Epoch [70/120    avg_loss:0.132, val_acc:0.985]
Epoch [71/120    avg_loss:0.127, val_acc:0.985]
Epoch [72/120    avg_loss:0.127, val_acc:0.983]
Epoch [73/120    avg_loss:0.120, val_acc:0.985]
Epoch [74/120    avg_loss:0.122, val_acc:0.990]
Epoch [75/120    avg_loss:0.123, val_acc:0.981]
Epoch [76/120    avg_loss:0.167, val_acc:0.981]
Epoch [77/120    avg_loss:0.105, val_acc:0.985]
Epoch [78/120    avg_loss:0.097, val_acc:0.992]
Epoch [79/120    avg_loss:0.092, val_acc:0.988]
Epoch [80/120    avg_loss:0.085, val_acc:0.996]
Epoch [81/120    avg_loss:0.103, val_acc:0.977]
Epoch [82/120    avg_loss:0.093, val_acc:0.996]
Epoch [83/120    avg_loss:0.093, val_acc:0.994]
Epoch [84/120    avg_loss:0.140, val_acc:0.965]
Epoch [85/120    avg_loss:0.082, val_acc:0.983]
Epoch [86/120    avg_loss:0.075, val_acc:0.992]
Epoch [87/120    avg_loss:0.071, val_acc:0.992]
Epoch [88/120    avg_loss:0.072, val_acc:0.998]
Epoch [89/120    avg_loss:0.075, val_acc:0.996]
Epoch [90/120    avg_loss:0.066, val_acc:0.994]
Epoch [91/120    avg_loss:0.082, val_acc:0.996]
Epoch [92/120    avg_loss:0.077, val_acc:0.985]
Epoch [93/120    avg_loss:0.084, val_acc:0.990]
Epoch [94/120    avg_loss:0.080, val_acc:0.992]
Epoch [95/120    avg_loss:0.070, val_acc:0.994]
Epoch [96/120    avg_loss:0.061, val_acc:0.998]
Epoch [97/120    avg_loss:0.062, val_acc:0.990]
Epoch [98/120    avg_loss:0.104, val_acc:0.960]
Epoch [99/120    avg_loss:0.137, val_acc:0.956]
Epoch [100/120    avg_loss:0.123, val_acc:0.981]
Epoch [101/120    avg_loss:0.078, val_acc:0.998]
Epoch [102/120    avg_loss:0.068, val_acc:1.000]
Epoch [103/120    avg_loss:0.073, val_acc:0.992]
Epoch [104/120    avg_loss:0.063, val_acc:0.996]
Epoch [105/120    avg_loss:0.061, val_acc:0.994]
Epoch [106/120    avg_loss:0.046, val_acc:0.996]
Epoch [107/120    avg_loss:0.041, val_acc:1.000]
Epoch [108/120    avg_loss:0.042, val_acc:0.998]
Epoch [109/120    avg_loss:0.037, val_acc:0.998]
Epoch [110/120    avg_loss:0.035, val_acc:0.996]
Epoch [111/120    avg_loss:0.065, val_acc:0.994]
Epoch [112/120    avg_loss:0.052, val_acc:1.000]
Epoch [113/120    avg_loss:0.030, val_acc:1.000]
Epoch [114/120    avg_loss:0.023, val_acc:1.000]
Epoch [115/120    avg_loss:0.030, val_acc:0.996]
Epoch [116/120    avg_loss:0.044, val_acc:0.996]
Epoch [117/120    avg_loss:0.032, val_acc:0.994]
Epoch [118/120    avg_loss:0.056, val_acc:0.988]
Epoch [119/120    avg_loss:0.049, val_acc:0.996]
Epoch [120/120    avg_loss:0.043, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   1 214  11   0   0   0   0   0   0   1   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0  61   0   0   0   0 145   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.12366737739872

F1 scores:
[       nan 0.95664336 0.99319728 0.9978308  0.94900222 0.92783505
 0.82386364 0.98378378 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9790794470064023
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2a8a974978>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.628, val_acc:0.150]
Epoch [2/120    avg_loss:2.413, val_acc:0.396]
Epoch [3/120    avg_loss:2.267, val_acc:0.510]
Epoch [4/120    avg_loss:2.156, val_acc:0.594]
Epoch [5/120    avg_loss:2.043, val_acc:0.573]
Epoch [6/120    avg_loss:1.923, val_acc:0.562]
Epoch [7/120    avg_loss:1.820, val_acc:0.562]
Epoch [8/120    avg_loss:1.706, val_acc:0.575]
Epoch [9/120    avg_loss:1.590, val_acc:0.600]
Epoch [10/120    avg_loss:1.491, val_acc:0.610]
Epoch [11/120    avg_loss:1.400, val_acc:0.640]
Epoch [12/120    avg_loss:1.309, val_acc:0.683]
Epoch [13/120    avg_loss:1.223, val_acc:0.733]
Epoch [14/120    avg_loss:1.156, val_acc:0.746]
Epoch [15/120    avg_loss:1.065, val_acc:0.769]
Epoch [16/120    avg_loss:1.045, val_acc:0.779]
Epoch [17/120    avg_loss:0.959, val_acc:0.792]
Epoch [18/120    avg_loss:0.894, val_acc:0.804]
Epoch [19/120    avg_loss:0.822, val_acc:0.846]
Epoch [20/120    avg_loss:0.768, val_acc:0.819]
Epoch [21/120    avg_loss:0.701, val_acc:0.846]
Epoch [22/120    avg_loss:0.686, val_acc:0.860]
Epoch [23/120    avg_loss:0.609, val_acc:0.869]
Epoch [24/120    avg_loss:0.588, val_acc:0.906]
Epoch [25/120    avg_loss:0.553, val_acc:0.908]
Epoch [26/120    avg_loss:0.507, val_acc:0.900]
Epoch [27/120    avg_loss:0.496, val_acc:0.908]
Epoch [28/120    avg_loss:0.472, val_acc:0.871]
Epoch [29/120    avg_loss:0.421, val_acc:0.908]
Epoch [30/120    avg_loss:0.395, val_acc:0.912]
Epoch [31/120    avg_loss:0.408, val_acc:0.875]
Epoch [32/120    avg_loss:0.373, val_acc:0.931]
Epoch [33/120    avg_loss:0.312, val_acc:0.938]
Epoch [34/120    avg_loss:0.307, val_acc:0.917]
Epoch [35/120    avg_loss:0.318, val_acc:0.915]
Epoch [36/120    avg_loss:0.305, val_acc:0.915]
Epoch [37/120    avg_loss:0.322, val_acc:0.925]
Epoch [38/120    avg_loss:0.363, val_acc:0.898]
Epoch [39/120    avg_loss:0.269, val_acc:0.944]
Epoch [40/120    avg_loss:0.261, val_acc:0.931]
Epoch [41/120    avg_loss:0.248, val_acc:0.946]
Epoch [42/120    avg_loss:0.280, val_acc:0.963]
Epoch [43/120    avg_loss:0.296, val_acc:0.944]
Epoch [44/120    avg_loss:0.251, val_acc:0.946]
Epoch [45/120    avg_loss:0.220, val_acc:0.908]
Epoch [46/120    avg_loss:0.251, val_acc:0.902]
Epoch [47/120    avg_loss:0.263, val_acc:0.942]
Epoch [48/120    avg_loss:0.261, val_acc:0.948]
Epoch [49/120    avg_loss:0.242, val_acc:0.958]
Epoch [50/120    avg_loss:0.217, val_acc:0.977]
Epoch [51/120    avg_loss:0.200, val_acc:0.927]
Epoch [52/120    avg_loss:0.184, val_acc:0.956]
Epoch [53/120    avg_loss:0.174, val_acc:0.948]
Epoch [54/120    avg_loss:0.153, val_acc:0.958]
Epoch [55/120    avg_loss:0.131, val_acc:0.963]
Epoch [56/120    avg_loss:0.145, val_acc:0.954]
Epoch [57/120    avg_loss:0.164, val_acc:0.981]
Epoch [58/120    avg_loss:0.140, val_acc:0.975]
Epoch [59/120    avg_loss:0.138, val_acc:0.971]
Epoch [60/120    avg_loss:0.136, val_acc:0.969]
Epoch [61/120    avg_loss:0.168, val_acc:0.969]
Epoch [62/120    avg_loss:0.159, val_acc:0.967]
Epoch [63/120    avg_loss:0.143, val_acc:0.965]
Epoch [64/120    avg_loss:0.121, val_acc:0.975]
Epoch [65/120    avg_loss:0.109, val_acc:0.983]
Epoch [66/120    avg_loss:0.106, val_acc:0.963]
Epoch [67/120    avg_loss:0.127, val_acc:0.979]
Epoch [68/120    avg_loss:0.101, val_acc:0.981]
Epoch [69/120    avg_loss:0.154, val_acc:0.950]
Epoch [70/120    avg_loss:0.100, val_acc:0.985]
Epoch [71/120    avg_loss:0.093, val_acc:0.971]
Epoch [72/120    avg_loss:0.081, val_acc:0.963]
Epoch [73/120    avg_loss:0.111, val_acc:0.969]
Epoch [74/120    avg_loss:0.140, val_acc:0.981]
Epoch [75/120    avg_loss:0.096, val_acc:0.967]
Epoch [76/120    avg_loss:0.076, val_acc:0.983]
Epoch [77/120    avg_loss:0.061, val_acc:0.990]
Epoch [78/120    avg_loss:0.083, val_acc:0.996]
Epoch [79/120    avg_loss:0.075, val_acc:0.981]
Epoch [80/120    avg_loss:0.085, val_acc:0.977]
Epoch [81/120    avg_loss:0.118, val_acc:0.973]
Epoch [82/120    avg_loss:0.101, val_acc:0.979]
Epoch [83/120    avg_loss:0.084, val_acc:0.969]
Epoch [84/120    avg_loss:0.083, val_acc:0.985]
Epoch [85/120    avg_loss:0.050, val_acc:0.988]
Epoch [86/120    avg_loss:0.049, val_acc:0.979]
Epoch [87/120    avg_loss:0.074, val_acc:0.981]
Epoch [88/120    avg_loss:0.077, val_acc:0.981]
Epoch [89/120    avg_loss:0.057, val_acc:0.988]
Epoch [90/120    avg_loss:0.041, val_acc:0.979]
Epoch [91/120    avg_loss:0.042, val_acc:0.983]
Epoch [92/120    avg_loss:0.042, val_acc:0.983]
Epoch [93/120    avg_loss:0.034, val_acc:0.988]
Epoch [94/120    avg_loss:0.032, val_acc:0.990]
Epoch [95/120    avg_loss:0.034, val_acc:0.990]
Epoch [96/120    avg_loss:0.030, val_acc:0.988]
Epoch [97/120    avg_loss:0.028, val_acc:0.988]
Epoch [98/120    avg_loss:0.026, val_acc:0.988]
Epoch [99/120    avg_loss:0.032, val_acc:0.990]
Epoch [100/120    avg_loss:0.026, val_acc:0.988]
Epoch [101/120    avg_loss:0.025, val_acc:0.988]
Epoch [102/120    avg_loss:0.030, val_acc:0.988]
Epoch [103/120    avg_loss:0.039, val_acc:0.990]
Epoch [104/120    avg_loss:0.031, val_acc:0.990]
Epoch [105/120    avg_loss:0.029, val_acc:0.990]
Epoch [106/120    avg_loss:0.032, val_acc:0.990]
Epoch [107/120    avg_loss:0.032, val_acc:0.990]
Epoch [108/120    avg_loss:0.031, val_acc:0.990]
Epoch [109/120    avg_loss:0.026, val_acc:0.990]
Epoch [110/120    avg_loss:0.031, val_acc:0.990]
Epoch [111/120    avg_loss:0.024, val_acc:0.990]
Epoch [112/120    avg_loss:0.029, val_acc:0.990]
Epoch [113/120    avg_loss:0.026, val_acc:0.990]
Epoch [114/120    avg_loss:0.035, val_acc:0.990]
Epoch [115/120    avg_loss:0.028, val_acc:0.990]
Epoch [116/120    avg_loss:0.030, val_acc:0.990]
Epoch [117/120    avg_loss:0.025, val_acc:0.990]
Epoch [118/120    avg_loss:0.025, val_acc:0.990]
Epoch [119/120    avg_loss:0.026, val_acc:0.990]
Epoch [120/120    avg_loss:0.024, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 674   0   0   0   0  11   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   4   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.57356076759062

F1 scores:
[       nan 0.99190581 0.99545455 1.         0.98454746 0.97594502
 0.97399527 0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9952535963495399
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2d0762b898>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.594, val_acc:0.348]
Epoch [2/120    avg_loss:2.411, val_acc:0.392]
Epoch [3/120    avg_loss:2.255, val_acc:0.381]
Epoch [4/120    avg_loss:2.113, val_acc:0.456]
Epoch [5/120    avg_loss:2.005, val_acc:0.529]
Epoch [6/120    avg_loss:1.896, val_acc:0.552]
Epoch [7/120    avg_loss:1.806, val_acc:0.567]
Epoch [8/120    avg_loss:1.750, val_acc:0.562]
Epoch [9/120    avg_loss:1.654, val_acc:0.592]
Epoch [10/120    avg_loss:1.582, val_acc:0.665]
Epoch [11/120    avg_loss:1.478, val_acc:0.708]
Epoch [12/120    avg_loss:1.408, val_acc:0.708]
Epoch [13/120    avg_loss:1.354, val_acc:0.713]
Epoch [14/120    avg_loss:1.255, val_acc:0.708]
Epoch [15/120    avg_loss:1.157, val_acc:0.731]
Epoch [16/120    avg_loss:1.118, val_acc:0.744]
Epoch [17/120    avg_loss:1.024, val_acc:0.756]
Epoch [18/120    avg_loss:0.974, val_acc:0.758]
Epoch [19/120    avg_loss:0.924, val_acc:0.779]
Epoch [20/120    avg_loss:0.816, val_acc:0.781]
Epoch [21/120    avg_loss:0.770, val_acc:0.812]
Epoch [22/120    avg_loss:0.724, val_acc:0.823]
Epoch [23/120    avg_loss:0.730, val_acc:0.831]
Epoch [24/120    avg_loss:0.618, val_acc:0.844]
Epoch [25/120    avg_loss:0.606, val_acc:0.917]
Epoch [26/120    avg_loss:0.531, val_acc:0.883]
Epoch [27/120    avg_loss:0.526, val_acc:0.940]
Epoch [28/120    avg_loss:0.529, val_acc:0.858]
Epoch [29/120    avg_loss:0.521, val_acc:0.906]
Epoch [30/120    avg_loss:0.483, val_acc:0.879]
Epoch [31/120    avg_loss:0.478, val_acc:0.902]
Epoch [32/120    avg_loss:0.452, val_acc:0.950]
Epoch [33/120    avg_loss:0.378, val_acc:0.925]
Epoch [34/120    avg_loss:0.392, val_acc:0.915]
Epoch [35/120    avg_loss:0.395, val_acc:0.933]
Epoch [36/120    avg_loss:0.372, val_acc:0.942]
Epoch [37/120    avg_loss:0.306, val_acc:0.952]
Epoch [38/120    avg_loss:0.325, val_acc:0.923]
Epoch [39/120    avg_loss:0.335, val_acc:0.929]
Epoch [40/120    avg_loss:0.336, val_acc:0.929]
Epoch [41/120    avg_loss:0.327, val_acc:0.950]
Epoch [42/120    avg_loss:0.288, val_acc:0.971]
Epoch [43/120    avg_loss:0.258, val_acc:0.952]
Epoch [44/120    avg_loss:0.228, val_acc:0.960]
Epoch [45/120    avg_loss:0.232, val_acc:0.938]
Epoch [46/120    avg_loss:0.268, val_acc:0.952]
Epoch [47/120    avg_loss:0.204, val_acc:0.969]
Epoch [48/120    avg_loss:0.218, val_acc:0.969]
Epoch [49/120    avg_loss:0.243, val_acc:0.950]
Epoch [50/120    avg_loss:0.212, val_acc:0.971]
Epoch [51/120    avg_loss:0.187, val_acc:0.973]
Epoch [52/120    avg_loss:0.169, val_acc:0.969]
Epoch [53/120    avg_loss:0.194, val_acc:0.954]
Epoch [54/120    avg_loss:0.190, val_acc:0.971]
Epoch [55/120    avg_loss:0.181, val_acc:0.969]
Epoch [56/120    avg_loss:0.176, val_acc:0.981]
Epoch [57/120    avg_loss:0.214, val_acc:0.979]
Epoch [58/120    avg_loss:0.169, val_acc:0.971]
Epoch [59/120    avg_loss:0.188, val_acc:0.983]
Epoch [60/120    avg_loss:0.156, val_acc:0.983]
Epoch [61/120    avg_loss:0.140, val_acc:0.988]
Epoch [62/120    avg_loss:0.148, val_acc:0.931]
Epoch [63/120    avg_loss:0.168, val_acc:0.979]
Epoch [64/120    avg_loss:0.122, val_acc:0.975]
Epoch [65/120    avg_loss:0.113, val_acc:0.985]
Epoch [66/120    avg_loss:0.110, val_acc:0.988]
Epoch [67/120    avg_loss:0.133, val_acc:0.985]
Epoch [68/120    avg_loss:0.098, val_acc:0.990]
Epoch [69/120    avg_loss:0.112, val_acc:0.981]
Epoch [70/120    avg_loss:0.119, val_acc:0.994]
Epoch [71/120    avg_loss:0.146, val_acc:0.981]
Epoch [72/120    avg_loss:0.111, val_acc:0.965]
Epoch [73/120    avg_loss:0.125, val_acc:0.981]
Epoch [74/120    avg_loss:0.104, val_acc:0.992]
Epoch [75/120    avg_loss:0.087, val_acc:0.988]
Epoch [76/120    avg_loss:0.115, val_acc:0.988]
Epoch [77/120    avg_loss:0.087, val_acc:0.996]
Epoch [78/120    avg_loss:0.078, val_acc:0.992]
Epoch [79/120    avg_loss:0.085, val_acc:0.992]
Epoch [80/120    avg_loss:0.090, val_acc:0.983]
Epoch [81/120    avg_loss:0.128, val_acc:0.988]
Epoch [82/120    avg_loss:0.106, val_acc:0.979]
Epoch [83/120    avg_loss:0.160, val_acc:0.983]
Epoch [84/120    avg_loss:0.137, val_acc:0.988]
Epoch [85/120    avg_loss:0.094, val_acc:0.983]
Epoch [86/120    avg_loss:0.132, val_acc:0.994]
Epoch [87/120    avg_loss:0.088, val_acc:0.988]
Epoch [88/120    avg_loss:0.071, val_acc:0.992]
Epoch [89/120    avg_loss:0.063, val_acc:0.996]
Epoch [90/120    avg_loss:0.053, val_acc:0.992]
Epoch [91/120    avg_loss:0.052, val_acc:0.996]
Epoch [92/120    avg_loss:0.054, val_acc:0.998]
Epoch [93/120    avg_loss:0.048, val_acc:0.994]
Epoch [94/120    avg_loss:0.058, val_acc:0.994]
Epoch [95/120    avg_loss:0.056, val_acc:0.996]
Epoch [96/120    avg_loss:0.063, val_acc:0.985]
Epoch [97/120    avg_loss:0.148, val_acc:0.983]
Epoch [98/120    avg_loss:0.100, val_acc:0.977]
Epoch [99/120    avg_loss:0.101, val_acc:0.992]
Epoch [100/120    avg_loss:0.086, val_acc:0.996]
Epoch [101/120    avg_loss:0.062, val_acc:0.994]
Epoch [102/120    avg_loss:0.046, val_acc:0.998]
Epoch [103/120    avg_loss:0.058, val_acc:0.977]
Epoch [104/120    avg_loss:0.074, val_acc:0.988]
Epoch [105/120    avg_loss:0.050, val_acc:0.994]
Epoch [106/120    avg_loss:0.049, val_acc:0.992]
Epoch [107/120    avg_loss:0.044, val_acc:0.985]
Epoch [108/120    avg_loss:0.058, val_acc:0.996]
Epoch [109/120    avg_loss:0.091, val_acc:0.996]
Epoch [110/120    avg_loss:0.101, val_acc:0.960]
Epoch [111/120    avg_loss:0.117, val_acc:0.985]
Epoch [112/120    avg_loss:0.091, val_acc:0.983]
Epoch [113/120    avg_loss:0.050, val_acc:0.992]
Epoch [114/120    avg_loss:0.043, val_acc:0.996]
Epoch [115/120    avg_loss:0.042, val_acc:0.994]
Epoch [116/120    avg_loss:0.042, val_acc:0.994]
Epoch [117/120    avg_loss:0.030, val_acc:0.996]
Epoch [118/120    avg_loss:0.032, val_acc:0.998]
Epoch [119/120    avg_loss:0.023, val_acc:0.996]
Epoch [120/120    avg_loss:0.026, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   3 215   9   0   0   0   0   0   0   0   0]
 [  0   0   0   9   4 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99707174 0.99545455 0.97457627 0.96412556 0.92307692
 0.99038462 0.98924731 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9926416598156178
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f98df6ce8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.607, val_acc:0.154]
Epoch [2/120    avg_loss:2.436, val_acc:0.344]
Epoch [3/120    avg_loss:2.304, val_acc:0.421]
Epoch [4/120    avg_loss:2.185, val_acc:0.552]
Epoch [5/120    avg_loss:2.103, val_acc:0.598]
Epoch [6/120    avg_loss:1.992, val_acc:0.592]
Epoch [7/120    avg_loss:1.886, val_acc:0.577]
Epoch [8/120    avg_loss:1.760, val_acc:0.600]
Epoch [9/120    avg_loss:1.625, val_acc:0.602]
Epoch [10/120    avg_loss:1.519, val_acc:0.625]
Epoch [11/120    avg_loss:1.398, val_acc:0.654]
Epoch [12/120    avg_loss:1.313, val_acc:0.667]
Epoch [13/120    avg_loss:1.216, val_acc:0.692]
Epoch [14/120    avg_loss:1.160, val_acc:0.719]
Epoch [15/120    avg_loss:1.105, val_acc:0.717]
Epoch [16/120    avg_loss:1.029, val_acc:0.727]
Epoch [17/120    avg_loss:0.948, val_acc:0.744]
Epoch [18/120    avg_loss:0.893, val_acc:0.779]
Epoch [19/120    avg_loss:0.835, val_acc:0.779]
Epoch [20/120    avg_loss:0.792, val_acc:0.796]
Epoch [21/120    avg_loss:0.743, val_acc:0.781]
Epoch [22/120    avg_loss:0.708, val_acc:0.860]
Epoch [23/120    avg_loss:0.671, val_acc:0.819]
Epoch [24/120    avg_loss:0.676, val_acc:0.815]
Epoch [25/120    avg_loss:0.590, val_acc:0.917]
Epoch [26/120    avg_loss:0.522, val_acc:0.921]
Epoch [27/120    avg_loss:0.529, val_acc:0.898]
Epoch [28/120    avg_loss:0.531, val_acc:0.906]
Epoch [29/120    avg_loss:0.484, val_acc:0.917]
Epoch [30/120    avg_loss:0.510, val_acc:0.883]
Epoch [31/120    avg_loss:0.503, val_acc:0.890]
Epoch [32/120    avg_loss:0.486, val_acc:0.929]
Epoch [33/120    avg_loss:0.435, val_acc:0.935]
Epoch [34/120    avg_loss:0.405, val_acc:0.904]
Epoch [35/120    avg_loss:0.388, val_acc:0.933]
Epoch [36/120    avg_loss:0.382, val_acc:0.931]
Epoch [37/120    avg_loss:0.386, val_acc:0.927]
Epoch [38/120    avg_loss:0.355, val_acc:0.942]
Epoch [39/120    avg_loss:0.348, val_acc:0.950]
Epoch [40/120    avg_loss:0.318, val_acc:0.894]
Epoch [41/120    avg_loss:0.327, val_acc:0.950]
Epoch [42/120    avg_loss:0.361, val_acc:0.931]
Epoch [43/120    avg_loss:0.336, val_acc:0.948]
Epoch [44/120    avg_loss:0.276, val_acc:0.944]
Epoch [45/120    avg_loss:0.302, val_acc:0.900]
Epoch [46/120    avg_loss:0.270, val_acc:0.960]
Epoch [47/120    avg_loss:0.304, val_acc:0.950]
Epoch [48/120    avg_loss:0.275, val_acc:0.958]
Epoch [49/120    avg_loss:0.259, val_acc:0.956]
Epoch [50/120    avg_loss:0.250, val_acc:0.952]
Epoch [51/120    avg_loss:0.280, val_acc:0.952]
Epoch [52/120    avg_loss:0.300, val_acc:0.940]
Epoch [53/120    avg_loss:0.352, val_acc:0.956]
Epoch [54/120    avg_loss:0.267, val_acc:0.952]
Epoch [55/120    avg_loss:0.257, val_acc:0.935]
Epoch [56/120    avg_loss:0.240, val_acc:0.963]
Epoch [57/120    avg_loss:0.232, val_acc:0.971]
Epoch [58/120    avg_loss:0.185, val_acc:0.985]
Epoch [59/120    avg_loss:0.157, val_acc:0.975]
Epoch [60/120    avg_loss:0.165, val_acc:0.977]
Epoch [61/120    avg_loss:0.146, val_acc:0.977]
Epoch [62/120    avg_loss:0.144, val_acc:0.988]
Epoch [63/120    avg_loss:0.142, val_acc:0.971]
Epoch [64/120    avg_loss:0.191, val_acc:0.954]
Epoch [65/120    avg_loss:0.186, val_acc:0.973]
Epoch [66/120    avg_loss:0.186, val_acc:0.971]
Epoch [67/120    avg_loss:0.203, val_acc:0.944]
Epoch [68/120    avg_loss:0.191, val_acc:0.983]
Epoch [69/120    avg_loss:0.142, val_acc:0.988]
Epoch [70/120    avg_loss:0.132, val_acc:0.971]
Epoch [71/120    avg_loss:0.172, val_acc:0.971]
Epoch [72/120    avg_loss:0.114, val_acc:0.979]
Epoch [73/120    avg_loss:0.113, val_acc:0.979]
Epoch [74/120    avg_loss:0.115, val_acc:0.983]
Epoch [75/120    avg_loss:0.108, val_acc:0.988]
Epoch [76/120    avg_loss:0.100, val_acc:0.990]
Epoch [77/120    avg_loss:0.109, val_acc:0.985]
Epoch [78/120    avg_loss:0.093, val_acc:0.992]
Epoch [79/120    avg_loss:0.107, val_acc:0.985]
Epoch [80/120    avg_loss:0.117, val_acc:0.967]
Epoch [81/120    avg_loss:0.142, val_acc:0.981]
Epoch [82/120    avg_loss:0.109, val_acc:0.990]
Epoch [83/120    avg_loss:0.112, val_acc:0.981]
Epoch [84/120    avg_loss:0.112, val_acc:0.985]
Epoch [85/120    avg_loss:0.097, val_acc:0.985]
Epoch [86/120    avg_loss:0.081, val_acc:0.988]
Epoch [87/120    avg_loss:0.065, val_acc:0.990]
Epoch [88/120    avg_loss:0.080, val_acc:0.979]
Epoch [89/120    avg_loss:0.092, val_acc:0.988]
Epoch [90/120    avg_loss:0.072, val_acc:0.992]
Epoch [91/120    avg_loss:0.126, val_acc:0.979]
Epoch [92/120    avg_loss:0.094, val_acc:0.967]
Epoch [93/120    avg_loss:0.120, val_acc:0.975]
Epoch [94/120    avg_loss:0.084, val_acc:0.985]
Epoch [95/120    avg_loss:0.065, val_acc:0.992]
Epoch [96/120    avg_loss:0.070, val_acc:0.988]
Epoch [97/120    avg_loss:0.068, val_acc:0.990]
Epoch [98/120    avg_loss:0.088, val_acc:0.994]
Epoch [99/120    avg_loss:0.091, val_acc:0.975]
Epoch [100/120    avg_loss:0.061, val_acc:0.994]
Epoch [101/120    avg_loss:0.063, val_acc:0.992]
Epoch [102/120    avg_loss:0.052, val_acc:0.994]
Epoch [103/120    avg_loss:0.057, val_acc:0.985]
Epoch [104/120    avg_loss:0.115, val_acc:0.985]
Epoch [105/120    avg_loss:0.092, val_acc:0.979]
Epoch [106/120    avg_loss:0.079, val_acc:0.983]
Epoch [107/120    avg_loss:0.062, val_acc:0.985]
Epoch [108/120    avg_loss:0.049, val_acc:0.985]
Epoch [109/120    avg_loss:0.056, val_acc:0.988]
Epoch [110/120    avg_loss:0.051, val_acc:0.992]
Epoch [111/120    avg_loss:0.075, val_acc:0.990]
Epoch [112/120    avg_loss:0.063, val_acc:0.992]
Epoch [113/120    avg_loss:0.043, val_acc:0.996]
Epoch [114/120    avg_loss:0.050, val_acc:0.990]
Epoch [115/120    avg_loss:0.058, val_acc:0.992]
Epoch [116/120    avg_loss:0.065, val_acc:0.990]
Epoch [117/120    avg_loss:0.041, val_acc:0.994]
Epoch [118/120    avg_loss:0.034, val_acc:0.996]
Epoch [119/120    avg_loss:0.037, val_acc:0.994]
Epoch [120/120    avg_loss:0.032, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 677   0   0   0   0   8   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 223   2   0   0   0   0   0   0   2   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 0.99412628 0.9977221  1.         0.95913978 0.93862816
 0.98095238 0.99465241 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9933540556025459
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:12:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:12
Validation dataloader:12
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:40
reserve_bands:160
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3f7fafb908>
supervision:full
center_pixel:True
Network :
Number of parameter: 54552==>0.05M
----------Training process----------
Epoch [1/120    avg_loss:2.546, val_acc:0.196]
Epoch [2/120    avg_loss:2.390, val_acc:0.423]
Epoch [3/120    avg_loss:2.267, val_acc:0.456]
Epoch [4/120    avg_loss:2.165, val_acc:0.494]
Epoch [5/120    avg_loss:2.064, val_acc:0.581]
Epoch [6/120    avg_loss:1.970, val_acc:0.633]
Epoch [7/120    avg_loss:1.857, val_acc:0.606]
Epoch [8/120    avg_loss:1.754, val_acc:0.625]
Epoch [9/120    avg_loss:1.653, val_acc:0.637]
Epoch [10/120    avg_loss:1.563, val_acc:0.642]
Epoch [11/120    avg_loss:1.446, val_acc:0.713]
Epoch [12/120    avg_loss:1.355, val_acc:0.727]
Epoch [13/120    avg_loss:1.236, val_acc:0.727]
Epoch [14/120    avg_loss:1.162, val_acc:0.715]
Epoch [15/120    avg_loss:1.074, val_acc:0.767]
Epoch [16/120    avg_loss:0.987, val_acc:0.760]
Epoch [17/120    avg_loss:0.921, val_acc:0.775]
Epoch [18/120    avg_loss:0.885, val_acc:0.779]
Epoch [19/120    avg_loss:0.784, val_acc:0.804]
Epoch [20/120    avg_loss:0.747, val_acc:0.817]
Epoch [21/120    avg_loss:0.757, val_acc:0.796]
Epoch [22/120    avg_loss:0.675, val_acc:0.823]
Epoch [23/120    avg_loss:0.658, val_acc:0.842]
Epoch [24/120    avg_loss:0.587, val_acc:0.865]
Epoch [25/120    avg_loss:0.571, val_acc:0.827]
Epoch [26/120    avg_loss:0.530, val_acc:0.863]
Epoch [27/120    avg_loss:0.516, val_acc:0.877]
Epoch [28/120    avg_loss:0.468, val_acc:0.940]
Epoch [29/120    avg_loss:0.453, val_acc:0.908]
Epoch [30/120    avg_loss:0.406, val_acc:0.956]
Epoch [31/120    avg_loss:0.418, val_acc:0.942]
Epoch [32/120    avg_loss:0.417, val_acc:0.954]
Epoch [33/120    avg_loss:0.340, val_acc:0.952]
Epoch [34/120    avg_loss:0.354, val_acc:0.890]
Epoch [35/120    avg_loss:0.390, val_acc:0.952]
Epoch [36/120    avg_loss:0.373, val_acc:0.875]
Epoch [37/120    avg_loss:0.351, val_acc:0.940]
Epoch [38/120    avg_loss:0.321, val_acc:0.969]
Epoch [39/120    avg_loss:0.268, val_acc:0.956]
Epoch [40/120    avg_loss:0.309, val_acc:0.958]
Epoch [41/120    avg_loss:0.247, val_acc:0.979]
Epoch [42/120    avg_loss:0.236, val_acc:0.975]
Epoch [43/120    avg_loss:0.218, val_acc:0.958]
Epoch [44/120    avg_loss:0.221, val_acc:0.983]
Epoch [45/120    avg_loss:0.228, val_acc:0.977]
Epoch [46/120    avg_loss:0.182, val_acc:0.975]
Epoch [47/120    avg_loss:0.203, val_acc:0.969]
Epoch [48/120    avg_loss:0.201, val_acc:0.958]
Epoch [49/120    avg_loss:0.196, val_acc:0.977]
Epoch [50/120    avg_loss:0.193, val_acc:0.975]
Epoch [51/120    avg_loss:0.200, val_acc:0.975]
Epoch [52/120    avg_loss:0.278, val_acc:0.946]
Epoch [53/120    avg_loss:0.202, val_acc:0.981]
Epoch [54/120    avg_loss:0.166, val_acc:0.983]
Epoch [55/120    avg_loss:0.159, val_acc:0.975]
Epoch [56/120    avg_loss:0.140, val_acc:0.981]
Epoch [57/120    avg_loss:0.151, val_acc:0.985]
Epoch [58/120    avg_loss:0.133, val_acc:0.973]
Epoch [59/120    avg_loss:0.165, val_acc:0.988]
Epoch [60/120    avg_loss:0.169, val_acc:0.988]
Epoch [61/120    avg_loss:0.140, val_acc:0.985]
Epoch [62/120    avg_loss:0.120, val_acc:0.983]
Epoch [63/120    avg_loss:0.122, val_acc:0.981]
Epoch [64/120    avg_loss:0.118, val_acc:0.971]
Epoch [65/120    avg_loss:0.108, val_acc:0.985]
Epoch [66/120    avg_loss:0.097, val_acc:0.988]
Epoch [67/120    avg_loss:0.095, val_acc:0.985]
Epoch [68/120    avg_loss:0.101, val_acc:0.990]
Epoch [69/120    avg_loss:0.083, val_acc:0.985]
Epoch [70/120    avg_loss:0.082, val_acc:0.992]
Epoch [71/120    avg_loss:0.065, val_acc:0.992]
Epoch [72/120    avg_loss:0.100, val_acc:0.994]
Epoch [73/120    avg_loss:0.074, val_acc:0.985]
Epoch [74/120    avg_loss:0.073, val_acc:0.990]
Epoch [75/120    avg_loss:0.079, val_acc:0.985]
Epoch [76/120    avg_loss:0.096, val_acc:0.969]
Epoch [77/120    avg_loss:0.130, val_acc:0.977]
Epoch [78/120    avg_loss:0.124, val_acc:0.983]
Epoch [79/120    avg_loss:0.140, val_acc:0.975]
Epoch [80/120    avg_loss:0.105, val_acc:0.990]
Epoch [81/120    avg_loss:0.077, val_acc:0.969]
Epoch [82/120    avg_loss:0.079, val_acc:0.994]
Epoch [83/120    avg_loss:0.100, val_acc:0.992]
Epoch [84/120    avg_loss:0.100, val_acc:0.990]
Epoch [85/120    avg_loss:0.077, val_acc:0.992]
Epoch [86/120    avg_loss:0.070, val_acc:0.988]
Epoch [87/120    avg_loss:0.104, val_acc:0.990]
Epoch [88/120    avg_loss:0.089, val_acc:0.992]
Epoch [89/120    avg_loss:0.100, val_acc:0.973]
Epoch [90/120    avg_loss:0.092, val_acc:0.990]
Epoch [91/120    avg_loss:0.063, val_acc:0.990]
Epoch [92/120    avg_loss:0.045, val_acc:0.990]
Epoch [93/120    avg_loss:0.040, val_acc:0.994]
Epoch [94/120    avg_loss:0.060, val_acc:0.988]
Epoch [95/120    avg_loss:0.077, val_acc:0.992]
Epoch [96/120    avg_loss:0.047, val_acc:0.990]
Epoch [97/120    avg_loss:0.049, val_acc:0.988]
Epoch [98/120    avg_loss:0.054, val_acc:0.988]
Epoch [99/120    avg_loss:0.053, val_acc:0.990]
Epoch [100/120    avg_loss:0.077, val_acc:0.994]
Epoch [101/120    avg_loss:0.043, val_acc:0.992]
Epoch [102/120    avg_loss:0.052, val_acc:0.990]
Epoch [103/120    avg_loss:0.049, val_acc:0.988]
Epoch [104/120    avg_loss:0.050, val_acc:0.990]
Epoch [105/120    avg_loss:0.075, val_acc:0.988]
Epoch [106/120    avg_loss:0.032, val_acc:0.994]
Epoch [107/120    avg_loss:0.042, val_acc:0.990]
Epoch [108/120    avg_loss:0.029, val_acc:0.994]
Epoch [109/120    avg_loss:0.026, val_acc:0.996]
Epoch [110/120    avg_loss:0.031, val_acc:0.992]
Epoch [111/120    avg_loss:0.038, val_acc:0.992]
Epoch [112/120    avg_loss:0.035, val_acc:0.992]
Epoch [113/120    avg_loss:0.042, val_acc:0.994]
Epoch [114/120    avg_loss:0.022, val_acc:0.994]
Epoch [115/120    avg_loss:0.020, val_acc:0.994]
Epoch [116/120    avg_loss:0.019, val_acc:0.994]
Epoch [117/120    avg_loss:0.019, val_acc:0.994]
Epoch [118/120    avg_loss:0.016, val_acc:0.994]
Epoch [119/120    avg_loss:0.014, val_acc:0.994]
Epoch [120/120    avg_loss:0.014, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 678   0   0   0   0   7   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   3   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.317697228145

F1 scores:
[       nan 0.99486427 0.98871332 1.         0.95726496 0.92753623
 0.98329356 0.9726776  1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9924044008723197
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd7b3aa07b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.026, val_acc:0.576]
Epoch [2/120    avg_loss:1.271, val_acc:0.748]
Epoch [3/120    avg_loss:1.017, val_acc:0.736]
Epoch [4/120    avg_loss:0.874, val_acc:0.832]
Epoch [5/120    avg_loss:0.766, val_acc:0.838]
Epoch [6/120    avg_loss:0.627, val_acc:0.834]
Epoch [7/120    avg_loss:0.666, val_acc:0.818]
Epoch [8/120    avg_loss:0.680, val_acc:0.850]
Epoch [9/120    avg_loss:0.570, val_acc:0.850]
Epoch [10/120    avg_loss:0.541, val_acc:0.785]
Epoch [11/120    avg_loss:0.562, val_acc:0.871]
Epoch [12/120    avg_loss:0.522, val_acc:0.844]
Epoch [13/120    avg_loss:0.433, val_acc:0.900]
Epoch [14/120    avg_loss:0.414, val_acc:0.900]
Epoch [15/120    avg_loss:0.448, val_acc:0.885]
Epoch [16/120    avg_loss:0.382, val_acc:0.889]
Epoch [17/120    avg_loss:0.407, val_acc:0.877]
Epoch [18/120    avg_loss:0.442, val_acc:0.863]
Epoch [19/120    avg_loss:0.356, val_acc:0.881]
Epoch [20/120    avg_loss:0.366, val_acc:0.902]
Epoch [21/120    avg_loss:0.420, val_acc:0.908]
Epoch [22/120    avg_loss:0.374, val_acc:0.881]
Epoch [23/120    avg_loss:0.319, val_acc:0.898]
Epoch [24/120    avg_loss:0.266, val_acc:0.932]
Epoch [25/120    avg_loss:0.302, val_acc:0.934]
Epoch [26/120    avg_loss:0.320, val_acc:0.900]
Epoch [27/120    avg_loss:0.254, val_acc:0.908]
Epoch [28/120    avg_loss:0.303, val_acc:0.945]
Epoch [29/120    avg_loss:0.233, val_acc:0.938]
Epoch [30/120    avg_loss:0.300, val_acc:0.928]
Epoch [31/120    avg_loss:0.257, val_acc:0.928]
Epoch [32/120    avg_loss:0.279, val_acc:0.939]
Epoch [33/120    avg_loss:0.248, val_acc:0.934]
Epoch [34/120    avg_loss:0.210, val_acc:0.930]
Epoch [35/120    avg_loss:0.186, val_acc:0.945]
Epoch [36/120    avg_loss:0.215, val_acc:0.918]
Epoch [37/120    avg_loss:0.228, val_acc:0.963]
Epoch [38/120    avg_loss:0.163, val_acc:0.930]
Epoch [39/120    avg_loss:0.216, val_acc:0.959]
Epoch [40/120    avg_loss:0.191, val_acc:0.930]
Epoch [41/120    avg_loss:0.231, val_acc:0.924]
Epoch [42/120    avg_loss:0.193, val_acc:0.947]
Epoch [43/120    avg_loss:0.185, val_acc:0.967]
Epoch [44/120    avg_loss:0.181, val_acc:0.928]
Epoch [45/120    avg_loss:0.159, val_acc:0.963]
Epoch [46/120    avg_loss:0.182, val_acc:0.945]
Epoch [47/120    avg_loss:0.155, val_acc:0.949]
Epoch [48/120    avg_loss:0.099, val_acc:0.941]
Epoch [49/120    avg_loss:0.107, val_acc:0.971]
Epoch [50/120    avg_loss:0.127, val_acc:0.967]
Epoch [51/120    avg_loss:0.101, val_acc:0.975]
Epoch [52/120    avg_loss:0.138, val_acc:0.979]
Epoch [53/120    avg_loss:0.179, val_acc:0.979]
Epoch [54/120    avg_loss:0.108, val_acc:0.965]
Epoch [55/120    avg_loss:0.158, val_acc:0.963]
Epoch [56/120    avg_loss:0.127, val_acc:0.947]
Epoch [57/120    avg_loss:0.131, val_acc:0.971]
Epoch [58/120    avg_loss:0.090, val_acc:0.973]
Epoch [59/120    avg_loss:0.143, val_acc:0.945]
Epoch [60/120    avg_loss:0.129, val_acc:0.959]
Epoch [61/120    avg_loss:0.084, val_acc:0.961]
Epoch [62/120    avg_loss:0.173, val_acc:0.955]
Epoch [63/120    avg_loss:0.239, val_acc:0.904]
Epoch [64/120    avg_loss:0.238, val_acc:0.945]
Epoch [65/120    avg_loss:0.109, val_acc:0.967]
Epoch [66/120    avg_loss:0.104, val_acc:0.947]
Epoch [67/120    avg_loss:0.107, val_acc:0.973]
Epoch [68/120    avg_loss:0.074, val_acc:0.969]
Epoch [69/120    avg_loss:0.054, val_acc:0.971]
Epoch [70/120    avg_loss:0.054, val_acc:0.971]
Epoch [71/120    avg_loss:0.059, val_acc:0.969]
Epoch [72/120    avg_loss:0.056, val_acc:0.975]
Epoch [73/120    avg_loss:0.060, val_acc:0.973]
Epoch [74/120    avg_loss:0.055, val_acc:0.977]
Epoch [75/120    avg_loss:0.054, val_acc:0.979]
Epoch [76/120    avg_loss:0.035, val_acc:0.977]
Epoch [77/120    avg_loss:0.039, val_acc:0.979]
Epoch [78/120    avg_loss:0.048, val_acc:0.979]
Epoch [79/120    avg_loss:0.051, val_acc:0.979]
Epoch [80/120    avg_loss:0.052, val_acc:0.975]
Epoch [81/120    avg_loss:0.057, val_acc:0.973]
Epoch [82/120    avg_loss:0.035, val_acc:0.973]
Epoch [83/120    avg_loss:0.037, val_acc:0.973]
Epoch [84/120    avg_loss:0.045, val_acc:0.980]
Epoch [85/120    avg_loss:0.045, val_acc:0.975]
Epoch [86/120    avg_loss:0.039, val_acc:0.977]
Epoch [87/120    avg_loss:0.044, val_acc:0.977]
Epoch [88/120    avg_loss:0.043, val_acc:0.977]
Epoch [89/120    avg_loss:0.037, val_acc:0.977]
Epoch [90/120    avg_loss:0.042, val_acc:0.977]
Epoch [91/120    avg_loss:0.051, val_acc:0.975]
Epoch [92/120    avg_loss:0.034, val_acc:0.975]
Epoch [93/120    avg_loss:0.046, val_acc:0.979]
Epoch [94/120    avg_loss:0.034, val_acc:0.975]
Epoch [95/120    avg_loss:0.033, val_acc:0.975]
Epoch [96/120    avg_loss:0.028, val_acc:0.977]
Epoch [97/120    avg_loss:0.036, val_acc:0.975]
Epoch [98/120    avg_loss:0.030, val_acc:0.975]
Epoch [99/120    avg_loss:0.038, val_acc:0.975]
Epoch [100/120    avg_loss:0.034, val_acc:0.975]
Epoch [101/120    avg_loss:0.042, val_acc:0.975]
Epoch [102/120    avg_loss:0.030, val_acc:0.975]
Epoch [103/120    avg_loss:0.031, val_acc:0.975]
Epoch [104/120    avg_loss:0.034, val_acc:0.975]
Epoch [105/120    avg_loss:0.040, val_acc:0.975]
Epoch [106/120    avg_loss:0.039, val_acc:0.975]
Epoch [107/120    avg_loss:0.031, val_acc:0.975]
Epoch [108/120    avg_loss:0.036, val_acc:0.975]
Epoch [109/120    avg_loss:0.032, val_acc:0.975]
Epoch [110/120    avg_loss:0.029, val_acc:0.977]
Epoch [111/120    avg_loss:0.037, val_acc:0.975]
Epoch [112/120    avg_loss:0.024, val_acc:0.975]
Epoch [113/120    avg_loss:0.034, val_acc:0.977]
Epoch [114/120    avg_loss:0.041, val_acc:0.977]
Epoch [115/120    avg_loss:0.045, val_acc:0.977]
Epoch [116/120    avg_loss:0.034, val_acc:0.977]
Epoch [117/120    avg_loss:0.033, val_acc:0.977]
Epoch [118/120    avg_loss:0.038, val_acc:0.977]
Epoch [119/120    avg_loss:0.042, val_acc:0.975]
Epoch [120/120    avg_loss:0.034, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 219   8   0   0   0   2   1   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  13   0   0   0   0  81   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 1.         0.97117517 0.97550111 0.9217759  0.89605735
 1.         0.92571429 0.99742931 0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9874168359262623
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff0302b07f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.023, val_acc:0.662]
Epoch [2/120    avg_loss:1.302, val_acc:0.752]
Epoch [3/120    avg_loss:1.001, val_acc:0.781]
Epoch [4/120    avg_loss:0.906, val_acc:0.812]
Epoch [5/120    avg_loss:0.795, val_acc:0.807]
Epoch [6/120    avg_loss:0.730, val_acc:0.801]
Epoch [7/120    avg_loss:0.626, val_acc:0.756]
Epoch [8/120    avg_loss:0.653, val_acc:0.840]
Epoch [9/120    avg_loss:0.619, val_acc:0.830]
Epoch [10/120    avg_loss:0.549, val_acc:0.873]
Epoch [11/120    avg_loss:0.562, val_acc:0.762]
Epoch [12/120    avg_loss:0.480, val_acc:0.885]
Epoch [13/120    avg_loss:0.462, val_acc:0.887]
Epoch [14/120    avg_loss:0.492, val_acc:0.871]
Epoch [15/120    avg_loss:0.483, val_acc:0.900]
Epoch [16/120    avg_loss:0.441, val_acc:0.871]
Epoch [17/120    avg_loss:0.457, val_acc:0.865]
Epoch [18/120    avg_loss:0.378, val_acc:0.908]
Epoch [19/120    avg_loss:0.363, val_acc:0.898]
Epoch [20/120    avg_loss:0.429, val_acc:0.885]
Epoch [21/120    avg_loss:0.374, val_acc:0.887]
Epoch [22/120    avg_loss:0.348, val_acc:0.883]
Epoch [23/120    avg_loss:0.390, val_acc:0.922]
Epoch [24/120    avg_loss:0.311, val_acc:0.895]
Epoch [25/120    avg_loss:0.367, val_acc:0.912]
Epoch [26/120    avg_loss:0.262, val_acc:0.920]
Epoch [27/120    avg_loss:0.260, val_acc:0.932]
Epoch [28/120    avg_loss:0.307, val_acc:0.938]
Epoch [29/120    avg_loss:0.407, val_acc:0.926]
Epoch [30/120    avg_loss:0.263, val_acc:0.900]
Epoch [31/120    avg_loss:0.322, val_acc:0.924]
Epoch [32/120    avg_loss:0.260, val_acc:0.906]
Epoch [33/120    avg_loss:0.343, val_acc:0.893]
Epoch [34/120    avg_loss:0.206, val_acc:0.938]
Epoch [35/120    avg_loss:0.247, val_acc:0.938]
Epoch [36/120    avg_loss:0.311, val_acc:0.922]
Epoch [37/120    avg_loss:0.276, val_acc:0.906]
Epoch [38/120    avg_loss:0.194, val_acc:0.953]
Epoch [39/120    avg_loss:0.258, val_acc:0.924]
Epoch [40/120    avg_loss:0.254, val_acc:0.926]
Epoch [41/120    avg_loss:0.325, val_acc:0.951]
Epoch [42/120    avg_loss:0.174, val_acc:0.941]
Epoch [43/120    avg_loss:0.168, val_acc:0.939]
Epoch [44/120    avg_loss:0.181, val_acc:0.947]
Epoch [45/120    avg_loss:0.153, val_acc:0.924]
Epoch [46/120    avg_loss:0.157, val_acc:0.941]
Epoch [47/120    avg_loss:0.112, val_acc:0.936]
Epoch [48/120    avg_loss:0.147, val_acc:0.957]
Epoch [49/120    avg_loss:0.126, val_acc:0.943]
Epoch [50/120    avg_loss:0.138, val_acc:0.955]
Epoch [51/120    avg_loss:0.118, val_acc:0.943]
Epoch [52/120    avg_loss:0.144, val_acc:0.939]
Epoch [53/120    avg_loss:0.174, val_acc:0.947]
Epoch [54/120    avg_loss:0.154, val_acc:0.924]
Epoch [55/120    avg_loss:0.246, val_acc:0.947]
Epoch [56/120    avg_loss:0.137, val_acc:0.957]
Epoch [57/120    avg_loss:0.128, val_acc:0.947]
Epoch [58/120    avg_loss:0.086, val_acc:0.957]
Epoch [59/120    avg_loss:0.095, val_acc:0.951]
Epoch [60/120    avg_loss:0.103, val_acc:0.957]
Epoch [61/120    avg_loss:0.167, val_acc:0.922]
Epoch [62/120    avg_loss:0.169, val_acc:0.965]
Epoch [63/120    avg_loss:0.175, val_acc:0.945]
Epoch [64/120    avg_loss:0.106, val_acc:0.961]
Epoch [65/120    avg_loss:0.111, val_acc:0.959]
Epoch [66/120    avg_loss:0.138, val_acc:0.953]
Epoch [67/120    avg_loss:0.091, val_acc:0.957]
Epoch [68/120    avg_loss:0.095, val_acc:0.961]
Epoch [69/120    avg_loss:0.065, val_acc:0.945]
Epoch [70/120    avg_loss:0.112, val_acc:0.961]
Epoch [71/120    avg_loss:0.135, val_acc:0.922]
Epoch [72/120    avg_loss:0.180, val_acc:0.959]
Epoch [73/120    avg_loss:0.200, val_acc:0.906]
Epoch [74/120    avg_loss:0.143, val_acc:0.955]
Epoch [75/120    avg_loss:0.077, val_acc:0.936]
Epoch [76/120    avg_loss:0.115, val_acc:0.955]
Epoch [77/120    avg_loss:0.051, val_acc:0.961]
Epoch [78/120    avg_loss:0.046, val_acc:0.965]
Epoch [79/120    avg_loss:0.057, val_acc:0.965]
Epoch [80/120    avg_loss:0.051, val_acc:0.971]
Epoch [81/120    avg_loss:0.048, val_acc:0.967]
Epoch [82/120    avg_loss:0.036, val_acc:0.971]
Epoch [83/120    avg_loss:0.041, val_acc:0.969]
Epoch [84/120    avg_loss:0.036, val_acc:0.971]
Epoch [85/120    avg_loss:0.030, val_acc:0.973]
Epoch [86/120    avg_loss:0.051, val_acc:0.973]
Epoch [87/120    avg_loss:0.047, val_acc:0.971]
Epoch [88/120    avg_loss:0.039, val_acc:0.971]
Epoch [89/120    avg_loss:0.034, val_acc:0.973]
Epoch [90/120    avg_loss:0.040, val_acc:0.973]
Epoch [91/120    avg_loss:0.040, val_acc:0.973]
Epoch [92/120    avg_loss:0.028, val_acc:0.977]
Epoch [93/120    avg_loss:0.029, val_acc:0.977]
Epoch [94/120    avg_loss:0.028, val_acc:0.971]
Epoch [95/120    avg_loss:0.029, val_acc:0.973]
Epoch [96/120    avg_loss:0.032, val_acc:0.977]
Epoch [97/120    avg_loss:0.040, val_acc:0.973]
Epoch [98/120    avg_loss:0.031, val_acc:0.977]
Epoch [99/120    avg_loss:0.022, val_acc:0.973]
Epoch [100/120    avg_loss:0.037, val_acc:0.975]
Epoch [101/120    avg_loss:0.035, val_acc:0.973]
Epoch [102/120    avg_loss:0.029, val_acc:0.975]
Epoch [103/120    avg_loss:0.035, val_acc:0.975]
Epoch [104/120    avg_loss:0.028, val_acc:0.977]
Epoch [105/120    avg_loss:0.018, val_acc:0.979]
Epoch [106/120    avg_loss:0.035, val_acc:0.975]
Epoch [107/120    avg_loss:0.038, val_acc:0.975]
Epoch [108/120    avg_loss:0.030, val_acc:0.977]
Epoch [109/120    avg_loss:0.029, val_acc:0.971]
Epoch [110/120    avg_loss:0.026, val_acc:0.971]
Epoch [111/120    avg_loss:0.030, val_acc:0.975]
Epoch [112/120    avg_loss:0.030, val_acc:0.973]
Epoch [113/120    avg_loss:0.030, val_acc:0.971]
Epoch [114/120    avg_loss:0.033, val_acc:0.971]
Epoch [115/120    avg_loss:0.031, val_acc:0.973]
Epoch [116/120    avg_loss:0.032, val_acc:0.969]
Epoch [117/120    avg_loss:0.029, val_acc:0.977]
Epoch [118/120    avg_loss:0.022, val_acc:0.975]
Epoch [119/120    avg_loss:0.026, val_acc:0.975]
Epoch [120/120    avg_loss:0.023, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   1   0   0   0   2   1   0   0   0   0]
 [  0   0   0   0 219   6   0   0   0   0   0   0   2   0]
 [  0   0   0   0  23 121   1   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8912579957356

F1 scores:
[       nan 1.         0.96475771 0.99122807 0.93191489 0.88970588
 0.99757869 0.93785311 0.99742931 0.99893276 1.         1.
 0.99224806 1.        ]

Kappa:
0.9876545003492103
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa4f2dde7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:1.981, val_acc:0.562]
Epoch [2/120    avg_loss:1.276, val_acc:0.750]
Epoch [3/120    avg_loss:1.063, val_acc:0.740]
Epoch [4/120    avg_loss:0.843, val_acc:0.762]
Epoch [5/120    avg_loss:0.766, val_acc:0.775]
Epoch [6/120    avg_loss:0.725, val_acc:0.791]
Epoch [7/120    avg_loss:0.656, val_acc:0.824]
Epoch [8/120    avg_loss:0.712, val_acc:0.826]
Epoch [9/120    avg_loss:0.632, val_acc:0.834]
Epoch [10/120    avg_loss:0.642, val_acc:0.834]
Epoch [11/120    avg_loss:0.646, val_acc:0.863]
Epoch [12/120    avg_loss:0.555, val_acc:0.848]
Epoch [13/120    avg_loss:0.448, val_acc:0.857]
Epoch [14/120    avg_loss:0.476, val_acc:0.863]
Epoch [15/120    avg_loss:0.486, val_acc:0.877]
Epoch [16/120    avg_loss:0.527, val_acc:0.877]
Epoch [17/120    avg_loss:0.448, val_acc:0.885]
Epoch [18/120    avg_loss:0.433, val_acc:0.893]
Epoch [19/120    avg_loss:0.409, val_acc:0.887]
Epoch [20/120    avg_loss:0.543, val_acc:0.830]
Epoch [21/120    avg_loss:0.406, val_acc:0.900]
Epoch [22/120    avg_loss:0.403, val_acc:0.908]
Epoch [23/120    avg_loss:0.382, val_acc:0.902]
Epoch [24/120    avg_loss:0.385, val_acc:0.910]
Epoch [25/120    avg_loss:0.349, val_acc:0.895]
Epoch [26/120    avg_loss:0.360, val_acc:0.912]
Epoch [27/120    avg_loss:0.347, val_acc:0.916]
Epoch [28/120    avg_loss:0.348, val_acc:0.900]
Epoch [29/120    avg_loss:0.345, val_acc:0.922]
Epoch [30/120    avg_loss:0.343, val_acc:0.854]
Epoch [31/120    avg_loss:0.385, val_acc:0.822]
Epoch [32/120    avg_loss:0.340, val_acc:0.895]
Epoch [33/120    avg_loss:0.307, val_acc:0.914]
Epoch [34/120    avg_loss:0.267, val_acc:0.926]
Epoch [35/120    avg_loss:0.256, val_acc:0.926]
Epoch [36/120    avg_loss:0.281, val_acc:0.914]
Epoch [37/120    avg_loss:0.305, val_acc:0.910]
Epoch [38/120    avg_loss:0.243, val_acc:0.908]
Epoch [39/120    avg_loss:0.282, val_acc:0.928]
Epoch [40/120    avg_loss:0.245, val_acc:0.932]
Epoch [41/120    avg_loss:0.240, val_acc:0.918]
Epoch [42/120    avg_loss:0.195, val_acc:0.941]
Epoch [43/120    avg_loss:0.227, val_acc:0.945]
Epoch [44/120    avg_loss:0.152, val_acc:0.938]
Epoch [45/120    avg_loss:0.240, val_acc:0.885]
Epoch [46/120    avg_loss:0.273, val_acc:0.900]
Epoch [47/120    avg_loss:0.183, val_acc:0.951]
Epoch [48/120    avg_loss:0.171, val_acc:0.928]
Epoch [49/120    avg_loss:0.146, val_acc:0.957]
Epoch [50/120    avg_loss:0.178, val_acc:0.932]
Epoch [51/120    avg_loss:0.139, val_acc:0.930]
Epoch [52/120    avg_loss:0.163, val_acc:0.926]
Epoch [53/120    avg_loss:0.193, val_acc:0.949]
Epoch [54/120    avg_loss:0.121, val_acc:0.936]
Epoch [55/120    avg_loss:0.162, val_acc:0.955]
Epoch [56/120    avg_loss:0.169, val_acc:0.949]
Epoch [57/120    avg_loss:0.114, val_acc:0.941]
Epoch [58/120    avg_loss:0.227, val_acc:0.934]
Epoch [59/120    avg_loss:0.130, val_acc:0.955]
Epoch [60/120    avg_loss:0.117, val_acc:0.953]
Epoch [61/120    avg_loss:0.132, val_acc:0.951]
Epoch [62/120    avg_loss:0.127, val_acc:0.953]
Epoch [63/120    avg_loss:0.105, val_acc:0.965]
Epoch [64/120    avg_loss:0.059, val_acc:0.965]
Epoch [65/120    avg_loss:0.050, val_acc:0.969]
Epoch [66/120    avg_loss:0.066, val_acc:0.967]
Epoch [67/120    avg_loss:0.063, val_acc:0.967]
Epoch [68/120    avg_loss:0.051, val_acc:0.967]
Epoch [69/120    avg_loss:0.066, val_acc:0.961]
Epoch [70/120    avg_loss:0.059, val_acc:0.965]
Epoch [71/120    avg_loss:0.060, val_acc:0.967]
Epoch [72/120    avg_loss:0.063, val_acc:0.971]
Epoch [73/120    avg_loss:0.051, val_acc:0.967]
Epoch [74/120    avg_loss:0.061, val_acc:0.969]
Epoch [75/120    avg_loss:0.048, val_acc:0.969]
Epoch [76/120    avg_loss:0.054, val_acc:0.967]
Epoch [77/120    avg_loss:0.052, val_acc:0.973]
Epoch [78/120    avg_loss:0.050, val_acc:0.973]
Epoch [79/120    avg_loss:0.048, val_acc:0.973]
Epoch [80/120    avg_loss:0.048, val_acc:0.975]
Epoch [81/120    avg_loss:0.046, val_acc:0.969]
Epoch [82/120    avg_loss:0.049, val_acc:0.975]
Epoch [83/120    avg_loss:0.053, val_acc:0.979]
Epoch [84/120    avg_loss:0.048, val_acc:0.977]
Epoch [85/120    avg_loss:0.048, val_acc:0.977]
Epoch [86/120    avg_loss:0.039, val_acc:0.977]
Epoch [87/120    avg_loss:0.045, val_acc:0.979]
Epoch [88/120    avg_loss:0.046, val_acc:0.977]
Epoch [89/120    avg_loss:0.035, val_acc:0.977]
Epoch [90/120    avg_loss:0.051, val_acc:0.979]
Epoch [91/120    avg_loss:0.035, val_acc:0.979]
Epoch [92/120    avg_loss:0.048, val_acc:0.979]
Epoch [93/120    avg_loss:0.047, val_acc:0.977]
Epoch [94/120    avg_loss:0.029, val_acc:0.982]
Epoch [95/120    avg_loss:0.045, val_acc:0.984]
Epoch [96/120    avg_loss:0.039, val_acc:0.980]
Epoch [97/120    avg_loss:0.050, val_acc:0.980]
Epoch [98/120    avg_loss:0.050, val_acc:0.973]
Epoch [99/120    avg_loss:0.046, val_acc:0.973]
Epoch [100/120    avg_loss:0.041, val_acc:0.977]
Epoch [101/120    avg_loss:0.048, val_acc:0.980]
Epoch [102/120    avg_loss:0.050, val_acc:0.982]
Epoch [103/120    avg_loss:0.033, val_acc:0.982]
Epoch [104/120    avg_loss:0.031, val_acc:0.977]
Epoch [105/120    avg_loss:0.045, val_acc:0.977]
Epoch [106/120    avg_loss:0.036, val_acc:0.980]
Epoch [107/120    avg_loss:0.029, val_acc:0.980]
Epoch [108/120    avg_loss:0.035, val_acc:0.982]
Epoch [109/120    avg_loss:0.044, val_acc:0.982]
Epoch [110/120    avg_loss:0.038, val_acc:0.982]
Epoch [111/120    avg_loss:0.048, val_acc:0.982]
Epoch [112/120    avg_loss:0.032, val_acc:0.982]
Epoch [113/120    avg_loss:0.037, val_acc:0.980]
Epoch [114/120    avg_loss:0.036, val_acc:0.980]
Epoch [115/120    avg_loss:0.032, val_acc:0.980]
Epoch [116/120    avg_loss:0.033, val_acc:0.980]
Epoch [117/120    avg_loss:0.042, val_acc:0.979]
Epoch [118/120    avg_loss:0.036, val_acc:0.980]
Epoch [119/120    avg_loss:0.050, val_acc:0.980]
Epoch [120/120    avg_loss:0.031, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 226   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   6 207  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   1 467   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   3   0   0   0   0   0   0   0   0   0 450   0]
 [  0   0   0   0   0   0   0   0   4   0   0   0   0 830]]

Accuracy:
98.8912579957356

F1 scores:
[       nan 1.         0.95768374 0.97835498 0.93665158 0.94352159
 0.99756691 0.91111111 0.99359795 0.99893048 1.         1.
 0.99667774 0.99759615]

Kappa:
0.9876576892398663
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f667597d828>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.034, val_acc:0.510]
Epoch [2/120    avg_loss:1.380, val_acc:0.758]
Epoch [3/120    avg_loss:0.985, val_acc:0.789]
Epoch [4/120    avg_loss:0.885, val_acc:0.773]
Epoch [5/120    avg_loss:0.786, val_acc:0.846]
Epoch [6/120    avg_loss:0.604, val_acc:0.855]
Epoch [7/120    avg_loss:0.647, val_acc:0.863]
Epoch [8/120    avg_loss:0.609, val_acc:0.840]
Epoch [9/120    avg_loss:0.498, val_acc:0.900]
Epoch [10/120    avg_loss:0.522, val_acc:0.924]
Epoch [11/120    avg_loss:0.499, val_acc:0.900]
Epoch [12/120    avg_loss:0.537, val_acc:0.854]
Epoch [13/120    avg_loss:0.485, val_acc:0.900]
Epoch [14/120    avg_loss:0.454, val_acc:0.881]
Epoch [15/120    avg_loss:0.466, val_acc:0.906]
Epoch [16/120    avg_loss:0.372, val_acc:0.908]
Epoch [17/120    avg_loss:0.444, val_acc:0.895]
Epoch [18/120    avg_loss:0.464, val_acc:0.891]
Epoch [19/120    avg_loss:0.476, val_acc:0.906]
Epoch [20/120    avg_loss:0.386, val_acc:0.881]
Epoch [21/120    avg_loss:0.407, val_acc:0.904]
Epoch [22/120    avg_loss:0.332, val_acc:0.939]
Epoch [23/120    avg_loss:0.311, val_acc:0.930]
Epoch [24/120    avg_loss:0.326, val_acc:0.910]
Epoch [25/120    avg_loss:0.282, val_acc:0.922]
Epoch [26/120    avg_loss:0.277, val_acc:0.936]
Epoch [27/120    avg_loss:0.254, val_acc:0.945]
Epoch [28/120    avg_loss:0.305, val_acc:0.939]
Epoch [29/120    avg_loss:0.263, val_acc:0.918]
Epoch [30/120    avg_loss:0.282, val_acc:0.887]
Epoch [31/120    avg_loss:0.275, val_acc:0.949]
Epoch [32/120    avg_loss:0.253, val_acc:0.945]
Epoch [33/120    avg_loss:0.264, val_acc:0.906]
Epoch [34/120    avg_loss:0.271, val_acc:0.951]
Epoch [35/120    avg_loss:0.269, val_acc:0.934]
Epoch [36/120    avg_loss:0.258, val_acc:0.938]
Epoch [37/120    avg_loss:0.269, val_acc:0.947]
Epoch [38/120    avg_loss:0.186, val_acc:0.934]
Epoch [39/120    avg_loss:0.203, val_acc:0.953]
Epoch [40/120    avg_loss:0.168, val_acc:0.959]
Epoch [41/120    avg_loss:0.191, val_acc:0.910]
Epoch [42/120    avg_loss:0.243, val_acc:0.951]
Epoch [43/120    avg_loss:0.105, val_acc:0.955]
Epoch [44/120    avg_loss:0.196, val_acc:0.953]
Epoch [45/120    avg_loss:0.165, val_acc:0.943]
Epoch [46/120    avg_loss:0.258, val_acc:0.936]
Epoch [47/120    avg_loss:0.193, val_acc:0.924]
Epoch [48/120    avg_loss:0.151, val_acc:0.961]
Epoch [49/120    avg_loss:0.148, val_acc:0.951]
Epoch [50/120    avg_loss:0.182, val_acc:0.959]
Epoch [51/120    avg_loss:0.150, val_acc:0.961]
Epoch [52/120    avg_loss:0.098, val_acc:0.969]
Epoch [53/120    avg_loss:0.122, val_acc:0.953]
Epoch [54/120    avg_loss:0.118, val_acc:0.961]
Epoch [55/120    avg_loss:0.129, val_acc:0.967]
Epoch [56/120    avg_loss:0.081, val_acc:0.971]
Epoch [57/120    avg_loss:0.136, val_acc:0.959]
Epoch [58/120    avg_loss:0.141, val_acc:0.963]
Epoch [59/120    avg_loss:0.101, val_acc:0.961]
Epoch [60/120    avg_loss:0.132, val_acc:0.965]
Epoch [61/120    avg_loss:0.086, val_acc:0.965]
Epoch [62/120    avg_loss:0.098, val_acc:0.965]
Epoch [63/120    avg_loss:0.104, val_acc:0.928]
Epoch [64/120    avg_loss:0.118, val_acc:0.951]
Epoch [65/120    avg_loss:0.070, val_acc:0.967]
Epoch [66/120    avg_loss:0.087, val_acc:0.961]
Epoch [67/120    avg_loss:0.075, val_acc:0.969]
Epoch [68/120    avg_loss:0.048, val_acc:0.969]
Epoch [69/120    avg_loss:0.049, val_acc:0.961]
Epoch [70/120    avg_loss:0.061, val_acc:0.975]
Epoch [71/120    avg_loss:0.036, val_acc:0.977]
Epoch [72/120    avg_loss:0.026, val_acc:0.975]
Epoch [73/120    avg_loss:0.029, val_acc:0.979]
Epoch [74/120    avg_loss:0.029, val_acc:0.979]
Epoch [75/120    avg_loss:0.039, val_acc:0.984]
Epoch [76/120    avg_loss:0.020, val_acc:0.982]
Epoch [77/120    avg_loss:0.027, val_acc:0.980]
Epoch [78/120    avg_loss:0.027, val_acc:0.980]
Epoch [79/120    avg_loss:0.027, val_acc:0.982]
Epoch [80/120    avg_loss:0.027, val_acc:0.982]
Epoch [81/120    avg_loss:0.022, val_acc:0.982]
Epoch [82/120    avg_loss:0.035, val_acc:0.982]
Epoch [83/120    avg_loss:0.020, val_acc:0.982]
Epoch [84/120    avg_loss:0.028, val_acc:0.982]
Epoch [85/120    avg_loss:0.023, val_acc:0.984]
Epoch [86/120    avg_loss:0.028, val_acc:0.984]
Epoch [87/120    avg_loss:0.024, val_acc:0.984]
Epoch [88/120    avg_loss:0.022, val_acc:0.984]
Epoch [89/120    avg_loss:0.018, val_acc:0.984]
Epoch [90/120    avg_loss:0.024, val_acc:0.984]
Epoch [91/120    avg_loss:0.020, val_acc:0.982]
Epoch [92/120    avg_loss:0.022, val_acc:0.984]
Epoch [93/120    avg_loss:0.024, val_acc:0.986]
Epoch [94/120    avg_loss:0.022, val_acc:0.986]
Epoch [95/120    avg_loss:0.028, val_acc:0.984]
Epoch [96/120    avg_loss:0.019, val_acc:0.982]
Epoch [97/120    avg_loss:0.018, val_acc:0.982]
Epoch [98/120    avg_loss:0.014, val_acc:0.980]
Epoch [99/120    avg_loss:0.019, val_acc:0.980]
Epoch [100/120    avg_loss:0.026, val_acc:0.982]
Epoch [101/120    avg_loss:0.021, val_acc:0.982]
Epoch [102/120    avg_loss:0.019, val_acc:0.980]
Epoch [103/120    avg_loss:0.015, val_acc:0.982]
Epoch [104/120    avg_loss:0.013, val_acc:0.980]
Epoch [105/120    avg_loss:0.020, val_acc:0.984]
Epoch [106/120    avg_loss:0.018, val_acc:0.984]
Epoch [107/120    avg_loss:0.019, val_acc:0.984]
Epoch [108/120    avg_loss:0.019, val_acc:0.982]
Epoch [109/120    avg_loss:0.016, val_acc:0.982]
Epoch [110/120    avg_loss:0.018, val_acc:0.982]
Epoch [111/120    avg_loss:0.020, val_acc:0.982]
Epoch [112/120    avg_loss:0.020, val_acc:0.980]
Epoch [113/120    avg_loss:0.020, val_acc:0.982]
Epoch [114/120    avg_loss:0.021, val_acc:0.982]
Epoch [115/120    avg_loss:0.017, val_acc:0.982]
Epoch [116/120    avg_loss:0.020, val_acc:0.982]
Epoch [117/120    avg_loss:0.018, val_acc:0.982]
Epoch [118/120    avg_loss:0.014, val_acc:0.982]
Epoch [119/120    avg_loss:0.016, val_acc:0.982]
Epoch [120/120    avg_loss:0.013, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   1   0   0   0   0   3   0]
 [  0   0   0 217   7   0   0   0   5   1   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  22 123   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   5   0   0   0   0   0   0   0   0   1 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.63539445628997

F1 scores:
[       nan 1.         0.96846847 0.97091723 0.9106383  0.87544484
 1.         0.9673913  0.99359795 0.99893276 1.         0.99734748
 0.98893805 1.        ]

Kappa:
0.9848062203382529
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff5250bf748>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.011, val_acc:0.666]
Epoch [2/120    avg_loss:1.247, val_acc:0.688]
Epoch [3/120    avg_loss:1.033, val_acc:0.682]
Epoch [4/120    avg_loss:0.909, val_acc:0.752]
Epoch [5/120    avg_loss:0.836, val_acc:0.754]
Epoch [6/120    avg_loss:0.730, val_acc:0.795]
Epoch [7/120    avg_loss:0.691, val_acc:0.846]
Epoch [8/120    avg_loss:0.677, val_acc:0.871]
Epoch [9/120    avg_loss:0.643, val_acc:0.893]
Epoch [10/120    avg_loss:0.566, val_acc:0.844]
Epoch [11/120    avg_loss:0.576, val_acc:0.855]
Epoch [12/120    avg_loss:0.562, val_acc:0.793]
Epoch [13/120    avg_loss:0.564, val_acc:0.863]
Epoch [14/120    avg_loss:0.502, val_acc:0.889]
Epoch [15/120    avg_loss:0.425, val_acc:0.900]
Epoch [16/120    avg_loss:0.396, val_acc:0.875]
Epoch [17/120    avg_loss:0.394, val_acc:0.924]
Epoch [18/120    avg_loss:0.363, val_acc:0.900]
Epoch [19/120    avg_loss:0.347, val_acc:0.893]
Epoch [20/120    avg_loss:0.437, val_acc:0.885]
Epoch [21/120    avg_loss:0.410, val_acc:0.895]
Epoch [22/120    avg_loss:0.367, val_acc:0.922]
Epoch [23/120    avg_loss:0.352, val_acc:0.918]
Epoch [24/120    avg_loss:0.332, val_acc:0.895]
Epoch [25/120    avg_loss:0.417, val_acc:0.934]
Epoch [26/120    avg_loss:0.309, val_acc:0.912]
Epoch [27/120    avg_loss:0.325, val_acc:0.912]
Epoch [28/120    avg_loss:0.299, val_acc:0.928]
Epoch [29/120    avg_loss:0.345, val_acc:0.912]
Epoch [30/120    avg_loss:0.288, val_acc:0.928]
Epoch [31/120    avg_loss:0.312, val_acc:0.904]
Epoch [32/120    avg_loss:0.335, val_acc:0.924]
Epoch [33/120    avg_loss:0.308, val_acc:0.951]
Epoch [34/120    avg_loss:0.300, val_acc:0.936]
Epoch [35/120    avg_loss:0.296, val_acc:0.891]
Epoch [36/120    avg_loss:0.295, val_acc:0.938]
Epoch [37/120    avg_loss:0.228, val_acc:0.928]
Epoch [38/120    avg_loss:0.242, val_acc:0.941]
Epoch [39/120    avg_loss:0.207, val_acc:0.947]
Epoch [40/120    avg_loss:0.246, val_acc:0.916]
Epoch [41/120    avg_loss:0.306, val_acc:0.934]
Epoch [42/120    avg_loss:0.273, val_acc:0.947]
Epoch [43/120    avg_loss:0.266, val_acc:0.955]
Epoch [44/120    avg_loss:0.143, val_acc:0.967]
Epoch [45/120    avg_loss:0.171, val_acc:0.969]
Epoch [46/120    avg_loss:0.183, val_acc:0.932]
Epoch [47/120    avg_loss:0.259, val_acc:0.941]
Epoch [48/120    avg_loss:0.202, val_acc:0.943]
Epoch [49/120    avg_loss:0.199, val_acc:0.936]
Epoch [50/120    avg_loss:0.165, val_acc:0.957]
Epoch [51/120    avg_loss:0.124, val_acc:0.943]
Epoch [52/120    avg_loss:0.247, val_acc:0.943]
Epoch [53/120    avg_loss:0.137, val_acc:0.961]
Epoch [54/120    avg_loss:0.152, val_acc:0.945]
Epoch [55/120    avg_loss:0.181, val_acc:0.949]
Epoch [56/120    avg_loss:0.188, val_acc:0.951]
Epoch [57/120    avg_loss:0.185, val_acc:0.957]
Epoch [58/120    avg_loss:0.127, val_acc:0.957]
Epoch [59/120    avg_loss:0.083, val_acc:0.975]
Epoch [60/120    avg_loss:0.090, val_acc:0.977]
Epoch [61/120    avg_loss:0.078, val_acc:0.973]
Epoch [62/120    avg_loss:0.059, val_acc:0.975]
Epoch [63/120    avg_loss:0.076, val_acc:0.975]
Epoch [64/120    avg_loss:0.076, val_acc:0.979]
Epoch [65/120    avg_loss:0.063, val_acc:0.979]
Epoch [66/120    avg_loss:0.077, val_acc:0.979]
Epoch [67/120    avg_loss:0.070, val_acc:0.977]
Epoch [68/120    avg_loss:0.056, val_acc:0.979]
Epoch [69/120    avg_loss:0.064, val_acc:0.979]
Epoch [70/120    avg_loss:0.047, val_acc:0.980]
Epoch [71/120    avg_loss:0.046, val_acc:0.980]
Epoch [72/120    avg_loss:0.068, val_acc:0.982]
Epoch [73/120    avg_loss:0.072, val_acc:0.982]
Epoch [74/120    avg_loss:0.065, val_acc:0.979]
Epoch [75/120    avg_loss:0.059, val_acc:0.980]
Epoch [76/120    avg_loss:0.061, val_acc:0.980]
Epoch [77/120    avg_loss:0.059, val_acc:0.982]
Epoch [78/120    avg_loss:0.048, val_acc:0.982]
Epoch [79/120    avg_loss:0.065, val_acc:0.980]
Epoch [80/120    avg_loss:0.047, val_acc:0.980]
Epoch [81/120    avg_loss:0.060, val_acc:0.984]
Epoch [82/120    avg_loss:0.049, val_acc:0.977]
Epoch [83/120    avg_loss:0.052, val_acc:0.977]
Epoch [84/120    avg_loss:0.093, val_acc:0.979]
Epoch [85/120    avg_loss:0.048, val_acc:0.979]
Epoch [86/120    avg_loss:0.048, val_acc:0.984]
Epoch [87/120    avg_loss:0.042, val_acc:0.982]
Epoch [88/120    avg_loss:0.048, val_acc:0.982]
Epoch [89/120    avg_loss:0.053, val_acc:0.984]
Epoch [90/120    avg_loss:0.051, val_acc:0.986]
Epoch [91/120    avg_loss:0.050, val_acc:0.984]
Epoch [92/120    avg_loss:0.033, val_acc:0.984]
Epoch [93/120    avg_loss:0.043, val_acc:0.980]
Epoch [94/120    avg_loss:0.046, val_acc:0.982]
Epoch [95/120    avg_loss:0.036, val_acc:0.979]
Epoch [96/120    avg_loss:0.057, val_acc:0.982]
Epoch [97/120    avg_loss:0.047, val_acc:0.982]
Epoch [98/120    avg_loss:0.045, val_acc:0.982]
Epoch [99/120    avg_loss:0.043, val_acc:0.984]
Epoch [100/120    avg_loss:0.037, val_acc:0.984]
Epoch [101/120    avg_loss:0.036, val_acc:0.982]
Epoch [102/120    avg_loss:0.049, val_acc:0.984]
Epoch [103/120    avg_loss:0.049, val_acc:0.984]
Epoch [104/120    avg_loss:0.042, val_acc:0.984]
Epoch [105/120    avg_loss:0.049, val_acc:0.984]
Epoch [106/120    avg_loss:0.043, val_acc:0.984]
Epoch [107/120    avg_loss:0.041, val_acc:0.984]
Epoch [108/120    avg_loss:0.044, val_acc:0.984]
Epoch [109/120    avg_loss:0.044, val_acc:0.984]
Epoch [110/120    avg_loss:0.043, val_acc:0.984]
Epoch [111/120    avg_loss:0.038, val_acc:0.984]
Epoch [112/120    avg_loss:0.033, val_acc:0.984]
Epoch [113/120    avg_loss:0.035, val_acc:0.984]
Epoch [114/120    avg_loss:0.052, val_acc:0.984]
Epoch [115/120    avg_loss:0.043, val_acc:0.984]
Epoch [116/120    avg_loss:0.047, val_acc:0.984]
Epoch [117/120    avg_loss:0.043, val_acc:0.984]
Epoch [118/120    avg_loss:0.036, val_acc:0.984]
Epoch [119/120    avg_loss:0.042, val_acc:0.984]
Epoch [120/120    avg_loss:0.044, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 199   0   0   0   0  20   0   0   0   0   0   0]
 [  0   0   2 222   3   0   0   0   3   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   0  28 117   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.23027718550107

F1 scores:
[       nan 1.         0.9255814  0.98230088 0.8974359  0.83870968
 1.         0.87128713 0.99614891 1.         1.         1.
 0.99556541 1.        ]

Kappa:
0.9802986764579281
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4120712710>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.060, val_acc:0.631]
Epoch [2/120    avg_loss:1.315, val_acc:0.695]
Epoch [3/120    avg_loss:1.122, val_acc:0.725]
Epoch [4/120    avg_loss:0.956, val_acc:0.721]
Epoch [5/120    avg_loss:0.877, val_acc:0.785]
Epoch [6/120    avg_loss:0.728, val_acc:0.812]
Epoch [7/120    avg_loss:0.758, val_acc:0.836]
Epoch [8/120    avg_loss:0.718, val_acc:0.842]
Epoch [9/120    avg_loss:0.682, val_acc:0.785]
Epoch [10/120    avg_loss:0.563, val_acc:0.869]
Epoch [11/120    avg_loss:0.539, val_acc:0.867]
Epoch [12/120    avg_loss:0.570, val_acc:0.855]
Epoch [13/120    avg_loss:0.464, val_acc:0.832]
Epoch [14/120    avg_loss:0.495, val_acc:0.865]
Epoch [15/120    avg_loss:0.476, val_acc:0.883]
Epoch [16/120    avg_loss:0.470, val_acc:0.879]
Epoch [17/120    avg_loss:0.419, val_acc:0.895]
Epoch [18/120    avg_loss:0.394, val_acc:0.898]
Epoch [19/120    avg_loss:0.517, val_acc:0.896]
Epoch [20/120    avg_loss:0.453, val_acc:0.877]
Epoch [21/120    avg_loss:0.409, val_acc:0.879]
Epoch [22/120    avg_loss:0.400, val_acc:0.896]
Epoch [23/120    avg_loss:0.344, val_acc:0.906]
Epoch [24/120    avg_loss:0.359, val_acc:0.918]
Epoch [25/120    avg_loss:0.375, val_acc:0.887]
Epoch [26/120    avg_loss:0.354, val_acc:0.922]
Epoch [27/120    avg_loss:0.382, val_acc:0.883]
Epoch [28/120    avg_loss:0.287, val_acc:0.908]
Epoch [29/120    avg_loss:0.308, val_acc:0.938]
Epoch [30/120    avg_loss:0.295, val_acc:0.914]
Epoch [31/120    avg_loss:0.257, val_acc:0.926]
Epoch [32/120    avg_loss:0.301, val_acc:0.910]
Epoch [33/120    avg_loss:0.294, val_acc:0.926]
Epoch [34/120    avg_loss:0.305, val_acc:0.904]
Epoch [35/120    avg_loss:0.287, val_acc:0.914]
Epoch [36/120    avg_loss:0.224, val_acc:0.922]
Epoch [37/120    avg_loss:0.257, val_acc:0.918]
Epoch [38/120    avg_loss:0.239, val_acc:0.930]
Epoch [39/120    avg_loss:0.277, val_acc:0.939]
Epoch [40/120    avg_loss:0.209, val_acc:0.930]
Epoch [41/120    avg_loss:0.191, val_acc:0.932]
Epoch [42/120    avg_loss:0.220, val_acc:0.945]
Epoch [43/120    avg_loss:0.189, val_acc:0.941]
Epoch [44/120    avg_loss:0.198, val_acc:0.926]
Epoch [45/120    avg_loss:0.200, val_acc:0.938]
Epoch [46/120    avg_loss:0.247, val_acc:0.943]
Epoch [47/120    avg_loss:0.244, val_acc:0.938]
Epoch [48/120    avg_loss:0.188, val_acc:0.939]
Epoch [49/120    avg_loss:0.192, val_acc:0.916]
Epoch [50/120    avg_loss:0.197, val_acc:0.924]
Epoch [51/120    avg_loss:0.221, val_acc:0.961]
Epoch [52/120    avg_loss:0.182, val_acc:0.904]
Epoch [53/120    avg_loss:0.197, val_acc:0.939]
Epoch [54/120    avg_loss:0.148, val_acc:0.906]
Epoch [55/120    avg_loss:0.164, val_acc:0.955]
Epoch [56/120    avg_loss:0.146, val_acc:0.941]
Epoch [57/120    avg_loss:0.130, val_acc:0.953]
Epoch [58/120    avg_loss:0.160, val_acc:0.955]
Epoch [59/120    avg_loss:0.125, val_acc:0.953]
Epoch [60/120    avg_loss:0.130, val_acc:0.945]
Epoch [61/120    avg_loss:0.152, val_acc:0.936]
Epoch [62/120    avg_loss:0.209, val_acc:0.963]
Epoch [63/120    avg_loss:0.119, val_acc:0.943]
Epoch [64/120    avg_loss:0.111, val_acc:0.961]
Epoch [65/120    avg_loss:0.108, val_acc:0.957]
Epoch [66/120    avg_loss:0.102, val_acc:0.957]
Epoch [67/120    avg_loss:0.112, val_acc:0.957]
Epoch [68/120    avg_loss:0.126, val_acc:0.957]
Epoch [69/120    avg_loss:0.130, val_acc:0.955]
Epoch [70/120    avg_loss:0.077, val_acc:0.961]
Epoch [71/120    avg_loss:0.099, val_acc:0.934]
Epoch [72/120    avg_loss:0.106, val_acc:0.961]
Epoch [73/120    avg_loss:0.140, val_acc:0.939]
Epoch [74/120    avg_loss:0.141, val_acc:0.961]
Epoch [75/120    avg_loss:0.111, val_acc:0.965]
Epoch [76/120    avg_loss:0.074, val_acc:0.959]
Epoch [77/120    avg_loss:0.085, val_acc:0.951]
Epoch [78/120    avg_loss:0.076, val_acc:0.963]
Epoch [79/120    avg_loss:0.089, val_acc:0.967]
Epoch [80/120    avg_loss:0.157, val_acc:0.963]
Epoch [81/120    avg_loss:0.097, val_acc:0.969]
Epoch [82/120    avg_loss:0.104, val_acc:0.969]
Epoch [83/120    avg_loss:0.094, val_acc:0.959]
Epoch [84/120    avg_loss:0.085, val_acc:0.965]
Epoch [85/120    avg_loss:0.077, val_acc:0.973]
Epoch [86/120    avg_loss:0.113, val_acc:0.965]
Epoch [87/120    avg_loss:0.128, val_acc:0.961]
Epoch [88/120    avg_loss:0.060, val_acc:0.980]
Epoch [89/120    avg_loss:0.056, val_acc:0.951]
Epoch [90/120    avg_loss:0.071, val_acc:0.969]
Epoch [91/120    avg_loss:0.041, val_acc:0.963]
Epoch [92/120    avg_loss:0.047, val_acc:0.967]
Epoch [93/120    avg_loss:0.049, val_acc:0.973]
Epoch [94/120    avg_loss:0.050, val_acc:0.971]
Epoch [95/120    avg_loss:0.048, val_acc:0.967]
Epoch [96/120    avg_loss:0.066, val_acc:0.959]
Epoch [97/120    avg_loss:0.051, val_acc:0.973]
Epoch [98/120    avg_loss:0.042, val_acc:0.967]
Epoch [99/120    avg_loss:0.046, val_acc:0.967]
Epoch [100/120    avg_loss:0.084, val_acc:0.959]
Epoch [101/120    avg_loss:0.050, val_acc:0.967]
Epoch [102/120    avg_loss:0.041, val_acc:0.977]
Epoch [103/120    avg_loss:0.024, val_acc:0.977]
Epoch [104/120    avg_loss:0.026, val_acc:0.975]
Epoch [105/120    avg_loss:0.033, val_acc:0.975]
Epoch [106/120    avg_loss:0.033, val_acc:0.977]
Epoch [107/120    avg_loss:0.024, val_acc:0.975]
Epoch [108/120    avg_loss:0.023, val_acc:0.977]
Epoch [109/120    avg_loss:0.026, val_acc:0.977]
Epoch [110/120    avg_loss:0.023, val_acc:0.979]
Epoch [111/120    avg_loss:0.026, val_acc:0.979]
Epoch [112/120    avg_loss:0.019, val_acc:0.979]
Epoch [113/120    avg_loss:0.017, val_acc:0.979]
Epoch [114/120    avg_loss:0.014, val_acc:0.980]
Epoch [115/120    avg_loss:0.021, val_acc:0.979]
Epoch [116/120    avg_loss:0.019, val_acc:0.979]
Epoch [117/120    avg_loss:0.016, val_acc:0.979]
Epoch [118/120    avg_loss:0.020, val_acc:0.980]
Epoch [119/120    avg_loss:0.018, val_acc:0.980]
Epoch [120/120    avg_loss:0.015, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   1   0   0   0   1   0   0   0   0   0]
 [  0   0   0   1 205  18   0   0   0   0   0   0   3   0]
 [  0   0   0   0   8 132   5   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   1 467   0   0   0   0]
 [  0   0   0   0   2   0   0   0   0   0 362   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   5   0   0   0   0   0   0   0   0   0 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 1.         0.98871332 0.99346405 0.9255079  0.89491525
 0.98800959 1.         0.99742931 0.99893048 0.99724518 0.99867198
 0.99005525 1.        ]

Kappa:
0.9890810759214443
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f88de581710>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:1.976, val_acc:0.525]
Epoch [2/120    avg_loss:1.338, val_acc:0.766]
Epoch [3/120    avg_loss:0.985, val_acc:0.715]
Epoch [4/120    avg_loss:0.840, val_acc:0.777]
Epoch [5/120    avg_loss:0.869, val_acc:0.773]
Epoch [6/120    avg_loss:0.727, val_acc:0.814]
Epoch [7/120    avg_loss:0.674, val_acc:0.826]
Epoch [8/120    avg_loss:0.618, val_acc:0.859]
Epoch [9/120    avg_loss:0.572, val_acc:0.861]
Epoch [10/120    avg_loss:0.546, val_acc:0.859]
Epoch [11/120    avg_loss:0.518, val_acc:0.850]
Epoch [12/120    avg_loss:0.527, val_acc:0.855]
Epoch [13/120    avg_loss:0.527, val_acc:0.877]
Epoch [14/120    avg_loss:0.524, val_acc:0.885]
Epoch [15/120    avg_loss:0.441, val_acc:0.906]
Epoch [16/120    avg_loss:0.479, val_acc:0.889]
Epoch [17/120    avg_loss:0.439, val_acc:0.893]
Epoch [18/120    avg_loss:0.427, val_acc:0.873]
Epoch [19/120    avg_loss:0.548, val_acc:0.898]
Epoch [20/120    avg_loss:0.480, val_acc:0.885]
Epoch [21/120    avg_loss:0.429, val_acc:0.893]
Epoch [22/120    avg_loss:0.444, val_acc:0.910]
Epoch [23/120    avg_loss:0.417, val_acc:0.877]
Epoch [24/120    avg_loss:0.369, val_acc:0.893]
Epoch [25/120    avg_loss:0.358, val_acc:0.938]
Epoch [26/120    avg_loss:0.341, val_acc:0.895]
Epoch [27/120    avg_loss:0.324, val_acc:0.916]
Epoch [28/120    avg_loss:0.295, val_acc:0.947]
Epoch [29/120    avg_loss:0.250, val_acc:0.902]
Epoch [30/120    avg_loss:0.317, val_acc:0.943]
Epoch [31/120    avg_loss:0.257, val_acc:0.920]
Epoch [32/120    avg_loss:0.287, val_acc:0.941]
Epoch [33/120    avg_loss:0.285, val_acc:0.924]
Epoch [34/120    avg_loss:0.259, val_acc:0.938]
Epoch [35/120    avg_loss:0.225, val_acc:0.889]
Epoch [36/120    avg_loss:0.317, val_acc:0.941]
Epoch [37/120    avg_loss:0.244, val_acc:0.941]
Epoch [38/120    avg_loss:0.264, val_acc:0.934]
Epoch [39/120    avg_loss:0.208, val_acc:0.949]
Epoch [40/120    avg_loss:0.196, val_acc:0.945]
Epoch [41/120    avg_loss:0.207, val_acc:0.959]
Epoch [42/120    avg_loss:0.195, val_acc:0.938]
Epoch [43/120    avg_loss:0.309, val_acc:0.891]
Epoch [44/120    avg_loss:0.278, val_acc:0.953]
Epoch [45/120    avg_loss:0.175, val_acc:0.949]
Epoch [46/120    avg_loss:0.157, val_acc:0.955]
Epoch [47/120    avg_loss:0.140, val_acc:0.963]
Epoch [48/120    avg_loss:0.148, val_acc:0.902]
Epoch [49/120    avg_loss:0.228, val_acc:0.943]
Epoch [50/120    avg_loss:0.182, val_acc:0.973]
Epoch [51/120    avg_loss:0.165, val_acc:0.961]
Epoch [52/120    avg_loss:0.180, val_acc:0.930]
Epoch [53/120    avg_loss:0.130, val_acc:0.969]
Epoch [54/120    avg_loss:0.131, val_acc:0.953]
Epoch [55/120    avg_loss:0.158, val_acc:0.961]
Epoch [56/120    avg_loss:0.209, val_acc:0.965]
Epoch [57/120    avg_loss:0.179, val_acc:0.977]
Epoch [58/120    avg_loss:0.183, val_acc:0.953]
Epoch [59/120    avg_loss:0.164, val_acc:0.963]
Epoch [60/120    avg_loss:0.137, val_acc:0.947]
Epoch [61/120    avg_loss:0.122, val_acc:0.959]
Epoch [62/120    avg_loss:0.153, val_acc:0.988]
Epoch [63/120    avg_loss:0.107, val_acc:0.967]
Epoch [64/120    avg_loss:0.122, val_acc:0.969]
Epoch [65/120    avg_loss:0.211, val_acc:0.947]
Epoch [66/120    avg_loss:0.135, val_acc:0.953]
Epoch [67/120    avg_loss:0.119, val_acc:0.969]
Epoch [68/120    avg_loss:0.083, val_acc:0.982]
Epoch [69/120    avg_loss:0.142, val_acc:0.963]
Epoch [70/120    avg_loss:0.153, val_acc:0.943]
Epoch [71/120    avg_loss:0.163, val_acc:0.949]
Epoch [72/120    avg_loss:0.138, val_acc:0.965]
Epoch [73/120    avg_loss:0.087, val_acc:0.977]
Epoch [74/120    avg_loss:0.094, val_acc:0.973]
Epoch [75/120    avg_loss:0.079, val_acc:0.969]
Epoch [76/120    avg_loss:0.049, val_acc:0.982]
Epoch [77/120    avg_loss:0.057, val_acc:0.986]
Epoch [78/120    avg_loss:0.035, val_acc:0.988]
Epoch [79/120    avg_loss:0.042, val_acc:0.986]
Epoch [80/120    avg_loss:0.029, val_acc:0.988]
Epoch [81/120    avg_loss:0.046, val_acc:0.986]
Epoch [82/120    avg_loss:0.042, val_acc:0.984]
Epoch [83/120    avg_loss:0.043, val_acc:0.982]
Epoch [84/120    avg_loss:0.046, val_acc:0.982]
Epoch [85/120    avg_loss:0.044, val_acc:0.986]
Epoch [86/120    avg_loss:0.039, val_acc:0.986]
Epoch [87/120    avg_loss:0.031, val_acc:0.984]
Epoch [88/120    avg_loss:0.038, val_acc:0.986]
Epoch [89/120    avg_loss:0.024, val_acc:0.982]
Epoch [90/120    avg_loss:0.030, val_acc:0.988]
Epoch [91/120    avg_loss:0.033, val_acc:0.982]
Epoch [92/120    avg_loss:0.034, val_acc:0.980]
Epoch [93/120    avg_loss:0.034, val_acc:0.982]
Epoch [94/120    avg_loss:0.035, val_acc:0.984]
Epoch [95/120    avg_loss:0.031, val_acc:0.982]
Epoch [96/120    avg_loss:0.024, val_acc:0.984]
Epoch [97/120    avg_loss:0.025, val_acc:0.986]
Epoch [98/120    avg_loss:0.031, val_acc:0.986]
Epoch [99/120    avg_loss:0.032, val_acc:0.986]
Epoch [100/120    avg_loss:0.031, val_acc:0.988]
Epoch [101/120    avg_loss:0.027, val_acc:0.988]
Epoch [102/120    avg_loss:0.020, val_acc:0.986]
Epoch [103/120    avg_loss:0.039, val_acc:0.984]
Epoch [104/120    avg_loss:0.028, val_acc:0.988]
Epoch [105/120    avg_loss:0.030, val_acc:0.984]
Epoch [106/120    avg_loss:0.034, val_acc:0.988]
Epoch [107/120    avg_loss:0.032, val_acc:0.988]
Epoch [108/120    avg_loss:0.021, val_acc:0.988]
Epoch [109/120    avg_loss:0.025, val_acc:0.988]
Epoch [110/120    avg_loss:0.032, val_acc:0.990]
Epoch [111/120    avg_loss:0.023, val_acc:0.990]
Epoch [112/120    avg_loss:0.033, val_acc:0.990]
Epoch [113/120    avg_loss:0.021, val_acc:0.986]
Epoch [114/120    avg_loss:0.023, val_acc:0.986]
Epoch [115/120    avg_loss:0.026, val_acc:0.988]
Epoch [116/120    avg_loss:0.026, val_acc:0.986]
Epoch [117/120    avg_loss:0.029, val_acc:0.986]
Epoch [118/120    avg_loss:0.027, val_acc:0.986]
Epoch [119/120    avg_loss:0.021, val_acc:0.986]
Epoch [120/120    avg_loss:0.024, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  16   0   0   0   0   0   0   1   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   2 466   0   0   0   0]
 [  0   0   0   0   1   0   0   0   0   0 363   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 1.         0.98866213 0.98901099 0.9375     0.93023256
 1.         0.97849462 0.99742931 0.99785867 0.99862448 0.99867198
 0.99669239 1.        ]

Kappa:
0.9914546551631466
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f896978b390>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:1.968, val_acc:0.508]
Epoch [2/120    avg_loss:1.227, val_acc:0.744]
Epoch [3/120    avg_loss:1.021, val_acc:0.740]
Epoch [4/120    avg_loss:0.941, val_acc:0.803]
Epoch [5/120    avg_loss:0.792, val_acc:0.814]
Epoch [6/120    avg_loss:0.743, val_acc:0.812]
Epoch [7/120    avg_loss:0.697, val_acc:0.852]
Epoch [8/120    avg_loss:0.636, val_acc:0.811]
Epoch [9/120    avg_loss:0.702, val_acc:0.822]
Epoch [10/120    avg_loss:0.554, val_acc:0.867]
Epoch [11/120    avg_loss:0.571, val_acc:0.836]
Epoch [12/120    avg_loss:0.487, val_acc:0.869]
Epoch [13/120    avg_loss:0.494, val_acc:0.855]
Epoch [14/120    avg_loss:0.572, val_acc:0.881]
Epoch [15/120    avg_loss:0.433, val_acc:0.900]
Epoch [16/120    avg_loss:0.405, val_acc:0.867]
Epoch [17/120    avg_loss:0.420, val_acc:0.871]
Epoch [18/120    avg_loss:0.454, val_acc:0.891]
Epoch [19/120    avg_loss:0.360, val_acc:0.893]
Epoch [20/120    avg_loss:0.391, val_acc:0.904]
Epoch [21/120    avg_loss:0.358, val_acc:0.916]
Epoch [22/120    avg_loss:0.370, val_acc:0.900]
Epoch [23/120    avg_loss:0.368, val_acc:0.908]
Epoch [24/120    avg_loss:0.364, val_acc:0.938]
Epoch [25/120    avg_loss:0.300, val_acc:0.947]
Epoch [26/120    avg_loss:0.395, val_acc:0.904]
Epoch [27/120    avg_loss:0.331, val_acc:0.916]
Epoch [28/120    avg_loss:0.267, val_acc:0.945]
Epoch [29/120    avg_loss:0.218, val_acc:0.928]
Epoch [30/120    avg_loss:0.233, val_acc:0.934]
Epoch [31/120    avg_loss:0.198, val_acc:0.936]
Epoch [32/120    avg_loss:0.251, val_acc:0.924]
Epoch [33/120    avg_loss:0.218, val_acc:0.945]
Epoch [34/120    avg_loss:0.299, val_acc:0.924]
Epoch [35/120    avg_loss:0.200, val_acc:0.934]
Epoch [36/120    avg_loss:0.244, val_acc:0.908]
Epoch [37/120    avg_loss:0.283, val_acc:0.922]
Epoch [38/120    avg_loss:0.226, val_acc:0.941]
Epoch [39/120    avg_loss:0.185, val_acc:0.957]
Epoch [40/120    avg_loss:0.156, val_acc:0.959]
Epoch [41/120    avg_loss:0.130, val_acc:0.965]
Epoch [42/120    avg_loss:0.116, val_acc:0.965]
Epoch [43/120    avg_loss:0.102, val_acc:0.971]
Epoch [44/120    avg_loss:0.102, val_acc:0.973]
Epoch [45/120    avg_loss:0.093, val_acc:0.971]
Epoch [46/120    avg_loss:0.079, val_acc:0.969]
Epoch [47/120    avg_loss:0.100, val_acc:0.969]
Epoch [48/120    avg_loss:0.111, val_acc:0.973]
Epoch [49/120    avg_loss:0.110, val_acc:0.969]
Epoch [50/120    avg_loss:0.090, val_acc:0.971]
Epoch [51/120    avg_loss:0.099, val_acc:0.973]
Epoch [52/120    avg_loss:0.086, val_acc:0.973]
Epoch [53/120    avg_loss:0.112, val_acc:0.971]
Epoch [54/120    avg_loss:0.108, val_acc:0.973]
Epoch [55/120    avg_loss:0.085, val_acc:0.971]
Epoch [56/120    avg_loss:0.101, val_acc:0.977]
Epoch [57/120    avg_loss:0.083, val_acc:0.979]
Epoch [58/120    avg_loss:0.084, val_acc:0.975]
Epoch [59/120    avg_loss:0.084, val_acc:0.975]
Epoch [60/120    avg_loss:0.078, val_acc:0.973]
Epoch [61/120    avg_loss:0.078, val_acc:0.979]
Epoch [62/120    avg_loss:0.086, val_acc:0.975]
Epoch [63/120    avg_loss:0.081, val_acc:0.977]
Epoch [64/120    avg_loss:0.076, val_acc:0.971]
Epoch [65/120    avg_loss:0.063, val_acc:0.971]
Epoch [66/120    avg_loss:0.086, val_acc:0.971]
Epoch [67/120    avg_loss:0.089, val_acc:0.977]
Epoch [68/120    avg_loss:0.073, val_acc:0.977]
Epoch [69/120    avg_loss:0.068, val_acc:0.973]
Epoch [70/120    avg_loss:0.090, val_acc:0.977]
Epoch [71/120    avg_loss:0.069, val_acc:0.977]
Epoch [72/120    avg_loss:0.079, val_acc:0.973]
Epoch [73/120    avg_loss:0.069, val_acc:0.973]
Epoch [74/120    avg_loss:0.066, val_acc:0.975]
Epoch [75/120    avg_loss:0.063, val_acc:0.975]
Epoch [76/120    avg_loss:0.061, val_acc:0.975]
Epoch [77/120    avg_loss:0.086, val_acc:0.975]
Epoch [78/120    avg_loss:0.070, val_acc:0.975]
Epoch [79/120    avg_loss:0.066, val_acc:0.975]
Epoch [80/120    avg_loss:0.056, val_acc:0.975]
Epoch [81/120    avg_loss:0.068, val_acc:0.975]
Epoch [82/120    avg_loss:0.069, val_acc:0.975]
Epoch [83/120    avg_loss:0.069, val_acc:0.979]
Epoch [84/120    avg_loss:0.062, val_acc:0.979]
Epoch [85/120    avg_loss:0.075, val_acc:0.979]
Epoch [86/120    avg_loss:0.064, val_acc:0.979]
Epoch [87/120    avg_loss:0.064, val_acc:0.979]
Epoch [88/120    avg_loss:0.073, val_acc:0.979]
Epoch [89/120    avg_loss:0.060, val_acc:0.979]
Epoch [90/120    avg_loss:0.071, val_acc:0.979]
Epoch [91/120    avg_loss:0.076, val_acc:0.979]
Epoch [92/120    avg_loss:0.066, val_acc:0.977]
Epoch [93/120    avg_loss:0.063, val_acc:0.979]
Epoch [94/120    avg_loss:0.061, val_acc:0.979]
Epoch [95/120    avg_loss:0.053, val_acc:0.979]
Epoch [96/120    avg_loss:0.074, val_acc:0.979]
Epoch [97/120    avg_loss:0.056, val_acc:0.979]
Epoch [98/120    avg_loss:0.062, val_acc:0.979]
Epoch [99/120    avg_loss:0.064, val_acc:0.979]
Epoch [100/120    avg_loss:0.063, val_acc:0.979]
Epoch [101/120    avg_loss:0.067, val_acc:0.979]
Epoch [102/120    avg_loss:0.063, val_acc:0.979]
Epoch [103/120    avg_loss:0.066, val_acc:0.979]
Epoch [104/120    avg_loss:0.074, val_acc:0.979]
Epoch [105/120    avg_loss:0.063, val_acc:0.979]
Epoch [106/120    avg_loss:0.066, val_acc:0.979]
Epoch [107/120    avg_loss:0.059, val_acc:0.979]
Epoch [108/120    avg_loss:0.081, val_acc:0.979]
Epoch [109/120    avg_loss:0.072, val_acc:0.979]
Epoch [110/120    avg_loss:0.056, val_acc:0.979]
Epoch [111/120    avg_loss:0.059, val_acc:0.979]
Epoch [112/120    avg_loss:0.063, val_acc:0.979]
Epoch [113/120    avg_loss:0.070, val_acc:0.979]
Epoch [114/120    avg_loss:0.066, val_acc:0.979]
Epoch [115/120    avg_loss:0.059, val_acc:0.979]
Epoch [116/120    avg_loss:0.062, val_acc:0.979]
Epoch [117/120    avg_loss:0.052, val_acc:0.979]
Epoch [118/120    avg_loss:0.059, val_acc:0.977]
Epoch [119/120    avg_loss:0.059, val_acc:0.979]
Epoch [120/120    avg_loss:0.062, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 202   0   0   0   0  17   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   1 220   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0  30 115   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   1   0   0   0   0   0   0   0 363   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   4 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.35820895522387

F1 scores:
[       nan 1.         0.92237443 0.99346405 0.91858038 0.86466165
 1.         0.83157895 1.         1.         0.99862448 0.99472296
 0.99445061 1.        ]

Kappa:
0.9817206731781084
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f86eea7c828>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.026, val_acc:0.670]
Epoch [2/120    avg_loss:1.350, val_acc:0.709]
Epoch [3/120    avg_loss:1.100, val_acc:0.703]
Epoch [4/120    avg_loss:0.931, val_acc:0.766]
Epoch [5/120    avg_loss:0.803, val_acc:0.799]
Epoch [6/120    avg_loss:0.789, val_acc:0.816]
Epoch [7/120    avg_loss:0.673, val_acc:0.848]
Epoch [8/120    avg_loss:0.709, val_acc:0.842]
Epoch [9/120    avg_loss:0.608, val_acc:0.859]
Epoch [10/120    avg_loss:0.561, val_acc:0.838]
Epoch [11/120    avg_loss:0.627, val_acc:0.855]
Epoch [12/120    avg_loss:0.522, val_acc:0.850]
Epoch [13/120    avg_loss:0.519, val_acc:0.883]
Epoch [14/120    avg_loss:0.452, val_acc:0.834]
Epoch [15/120    avg_loss:0.432, val_acc:0.918]
Epoch [16/120    avg_loss:0.478, val_acc:0.889]
Epoch [17/120    avg_loss:0.362, val_acc:0.842]
Epoch [18/120    avg_loss:0.363, val_acc:0.838]
Epoch [19/120    avg_loss:0.415, val_acc:0.873]
Epoch [20/120    avg_loss:0.396, val_acc:0.910]
Epoch [21/120    avg_loss:0.342, val_acc:0.908]
Epoch [22/120    avg_loss:0.341, val_acc:0.916]
Epoch [23/120    avg_loss:0.386, val_acc:0.887]
Epoch [24/120    avg_loss:0.398, val_acc:0.934]
Epoch [25/120    avg_loss:0.311, val_acc:0.920]
Epoch [26/120    avg_loss:0.303, val_acc:0.938]
Epoch [27/120    avg_loss:0.340, val_acc:0.916]
Epoch [28/120    avg_loss:0.259, val_acc:0.930]
Epoch [29/120    avg_loss:0.258, val_acc:0.918]
Epoch [30/120    avg_loss:0.317, val_acc:0.949]
Epoch [31/120    avg_loss:0.346, val_acc:0.914]
Epoch [32/120    avg_loss:0.283, val_acc:0.936]
Epoch [33/120    avg_loss:0.279, val_acc:0.930]
Epoch [34/120    avg_loss:0.216, val_acc:0.955]
Epoch [35/120    avg_loss:0.142, val_acc:0.959]
Epoch [36/120    avg_loss:0.144, val_acc:0.947]
Epoch [37/120    avg_loss:0.193, val_acc:0.936]
Epoch [38/120    avg_loss:0.171, val_acc:0.965]
Epoch [39/120    avg_loss:0.151, val_acc:0.922]
Epoch [40/120    avg_loss:0.181, val_acc:0.975]
Epoch [41/120    avg_loss:0.204, val_acc:0.947]
Epoch [42/120    avg_loss:0.179, val_acc:0.918]
Epoch [43/120    avg_loss:0.233, val_acc:0.961]
Epoch [44/120    avg_loss:0.147, val_acc:0.943]
Epoch [45/120    avg_loss:0.184, val_acc:0.953]
Epoch [46/120    avg_loss:0.179, val_acc:0.961]
Epoch [47/120    avg_loss:0.147, val_acc:0.936]
Epoch [48/120    avg_loss:0.187, val_acc:0.949]
Epoch [49/120    avg_loss:0.149, val_acc:0.951]
Epoch [50/120    avg_loss:0.122, val_acc:0.951]
Epoch [51/120    avg_loss:0.101, val_acc:0.961]
Epoch [52/120    avg_loss:0.247, val_acc:0.928]
Epoch [53/120    avg_loss:0.165, val_acc:0.941]
Epoch [54/120    avg_loss:0.119, val_acc:0.973]
Epoch [55/120    avg_loss:0.081, val_acc:0.975]
Epoch [56/120    avg_loss:0.073, val_acc:0.973]
Epoch [57/120    avg_loss:0.074, val_acc:0.977]
Epoch [58/120    avg_loss:0.069, val_acc:0.979]
Epoch [59/120    avg_loss:0.070, val_acc:0.979]
Epoch [60/120    avg_loss:0.046, val_acc:0.977]
Epoch [61/120    avg_loss:0.055, val_acc:0.980]
Epoch [62/120    avg_loss:0.048, val_acc:0.979]
Epoch [63/120    avg_loss:0.051, val_acc:0.979]
Epoch [64/120    avg_loss:0.049, val_acc:0.979]
Epoch [65/120    avg_loss:0.037, val_acc:0.982]
Epoch [66/120    avg_loss:0.049, val_acc:0.982]
Epoch [67/120    avg_loss:0.043, val_acc:0.979]
Epoch [68/120    avg_loss:0.059, val_acc:0.982]
Epoch [69/120    avg_loss:0.047, val_acc:0.982]
Epoch [70/120    avg_loss:0.049, val_acc:0.980]
Epoch [71/120    avg_loss:0.047, val_acc:0.980]
Epoch [72/120    avg_loss:0.051, val_acc:0.982]
Epoch [73/120    avg_loss:0.047, val_acc:0.980]
Epoch [74/120    avg_loss:0.037, val_acc:0.980]
Epoch [75/120    avg_loss:0.051, val_acc:0.980]
Epoch [76/120    avg_loss:0.046, val_acc:0.980]
Epoch [77/120    avg_loss:0.040, val_acc:0.979]
Epoch [78/120    avg_loss:0.060, val_acc:0.979]
Epoch [79/120    avg_loss:0.043, val_acc:0.980]
Epoch [80/120    avg_loss:0.046, val_acc:0.982]
Epoch [81/120    avg_loss:0.041, val_acc:0.982]
Epoch [82/120    avg_loss:0.041, val_acc:0.982]
Epoch [83/120    avg_loss:0.040, val_acc:0.980]
Epoch [84/120    avg_loss:0.046, val_acc:0.980]
Epoch [85/120    avg_loss:0.042, val_acc:0.980]
Epoch [86/120    avg_loss:0.050, val_acc:0.982]
Epoch [87/120    avg_loss:0.040, val_acc:0.980]
Epoch [88/120    avg_loss:0.033, val_acc:0.980]
Epoch [89/120    avg_loss:0.039, val_acc:0.980]
Epoch [90/120    avg_loss:0.029, val_acc:0.979]
Epoch [91/120    avg_loss:0.043, val_acc:0.979]
Epoch [92/120    avg_loss:0.035, val_acc:0.982]
Epoch [93/120    avg_loss:0.037, val_acc:0.980]
Epoch [94/120    avg_loss:0.035, val_acc:0.980]
Epoch [95/120    avg_loss:0.041, val_acc:0.982]
Epoch [96/120    avg_loss:0.032, val_acc:0.982]
Epoch [97/120    avg_loss:0.039, val_acc:0.984]
Epoch [98/120    avg_loss:0.033, val_acc:0.984]
Epoch [99/120    avg_loss:0.037, val_acc:0.984]
Epoch [100/120    avg_loss:0.032, val_acc:0.982]
Epoch [101/120    avg_loss:0.031, val_acc:0.980]
Epoch [102/120    avg_loss:0.036, val_acc:0.982]
Epoch [103/120    avg_loss:0.028, val_acc:0.984]
Epoch [104/120    avg_loss:0.034, val_acc:0.980]
Epoch [105/120    avg_loss:0.028, val_acc:0.982]
Epoch [106/120    avg_loss:0.028, val_acc:0.980]
Epoch [107/120    avg_loss:0.039, val_acc:0.984]
Epoch [108/120    avg_loss:0.033, val_acc:0.982]
Epoch [109/120    avg_loss:0.031, val_acc:0.982]
Epoch [110/120    avg_loss:0.026, val_acc:0.982]
Epoch [111/120    avg_loss:0.028, val_acc:0.984]
Epoch [112/120    avg_loss:0.028, val_acc:0.984]
Epoch [113/120    avg_loss:0.027, val_acc:0.984]
Epoch [114/120    avg_loss:0.035, val_acc:0.984]
Epoch [115/120    avg_loss:0.043, val_acc:0.986]
Epoch [116/120    avg_loss:0.035, val_acc:0.984]
Epoch [117/120    avg_loss:0.034, val_acc:0.988]
Epoch [118/120    avg_loss:0.028, val_acc:0.986]
Epoch [119/120    avg_loss:0.029, val_acc:0.984]
Epoch [120/120    avg_loss:0.024, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 212   0   0   0   0   6   0   0   0   0   1   0]
 [  0   0   0 226   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   1 467   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   4   0   0   0   0   0   0   0   0   1 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.97654584221748

F1 scores:
[       nan 1.         0.97471264 0.99122807 0.92608696 0.89965398
 0.99756691 0.96907216 0.998713   0.99893048 1.         0.99734748
 0.99224806 1.        ]

Kappa:
0.9886065211572176
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:13:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:64
Validation dataloader:64
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f318b1567b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:1.949, val_acc:0.621]
Epoch [2/120    avg_loss:1.295, val_acc:0.674]
Epoch [3/120    avg_loss:0.994, val_acc:0.752]
Epoch [4/120    avg_loss:0.900, val_acc:0.797]
Epoch [5/120    avg_loss:0.801, val_acc:0.840]
Epoch [6/120    avg_loss:0.715, val_acc:0.807]
Epoch [7/120    avg_loss:0.654, val_acc:0.840]
Epoch [8/120    avg_loss:0.737, val_acc:0.787]
Epoch [9/120    avg_loss:0.605, val_acc:0.869]
Epoch [10/120    avg_loss:0.578, val_acc:0.836]
Epoch [11/120    avg_loss:0.507, val_acc:0.865]
Epoch [12/120    avg_loss:0.562, val_acc:0.883]
Epoch [13/120    avg_loss:0.552, val_acc:0.879]
Epoch [14/120    avg_loss:0.516, val_acc:0.881]
Epoch [15/120    avg_loss:0.407, val_acc:0.910]
Epoch [16/120    avg_loss:0.413, val_acc:0.914]
Epoch [17/120    avg_loss:0.457, val_acc:0.879]
Epoch [18/120    avg_loss:0.365, val_acc:0.896]
Epoch [19/120    avg_loss:0.392, val_acc:0.857]
Epoch [20/120    avg_loss:0.400, val_acc:0.820]
Epoch [21/120    avg_loss:0.408, val_acc:0.904]
Epoch [22/120    avg_loss:0.429, val_acc:0.896]
Epoch [23/120    avg_loss:0.376, val_acc:0.879]
Epoch [24/120    avg_loss:0.401, val_acc:0.873]
Epoch [25/120    avg_loss:0.388, val_acc:0.920]
Epoch [26/120    avg_loss:0.307, val_acc:0.936]
Epoch [27/120    avg_loss:0.291, val_acc:0.918]
Epoch [28/120    avg_loss:0.253, val_acc:0.930]
Epoch [29/120    avg_loss:0.314, val_acc:0.941]
Epoch [30/120    avg_loss:0.269, val_acc:0.914]
Epoch [31/120    avg_loss:0.261, val_acc:0.908]
Epoch [32/120    avg_loss:0.280, val_acc:0.941]
Epoch [33/120    avg_loss:0.301, val_acc:0.945]
Epoch [34/120    avg_loss:0.276, val_acc:0.947]
Epoch [35/120    avg_loss:0.203, val_acc:0.947]
Epoch [36/120    avg_loss:0.224, val_acc:0.955]
Epoch [37/120    avg_loss:0.253, val_acc:0.957]
Epoch [38/120    avg_loss:0.265, val_acc:0.908]
Epoch [39/120    avg_loss:0.222, val_acc:0.967]
Epoch [40/120    avg_loss:0.231, val_acc:0.961]
Epoch [41/120    avg_loss:0.185, val_acc:0.961]
Epoch [42/120    avg_loss:0.281, val_acc:0.959]
Epoch [43/120    avg_loss:0.159, val_acc:0.947]
Epoch [44/120    avg_loss:0.216, val_acc:0.957]
Epoch [45/120    avg_loss:0.185, val_acc:0.951]
Epoch [46/120    avg_loss:0.165, val_acc:0.914]
Epoch [47/120    avg_loss:0.193, val_acc:0.965]
Epoch [48/120    avg_loss:0.137, val_acc:0.971]
Epoch [49/120    avg_loss:0.159, val_acc:0.969]
Epoch [50/120    avg_loss:0.233, val_acc:0.949]
Epoch [51/120    avg_loss:0.183, val_acc:0.951]
Epoch [52/120    avg_loss:0.168, val_acc:0.955]
Epoch [53/120    avg_loss:0.177, val_acc:0.967]
Epoch [54/120    avg_loss:0.123, val_acc:0.982]
Epoch [55/120    avg_loss:0.145, val_acc:0.943]
Epoch [56/120    avg_loss:0.124, val_acc:0.969]
Epoch [57/120    avg_loss:0.123, val_acc:0.955]
Epoch [58/120    avg_loss:0.119, val_acc:0.977]
Epoch [59/120    avg_loss:0.105, val_acc:0.965]
Epoch [60/120    avg_loss:0.143, val_acc:0.959]
Epoch [61/120    avg_loss:0.155, val_acc:0.969]
Epoch [62/120    avg_loss:0.143, val_acc:0.965]
Epoch [63/120    avg_loss:0.120, val_acc:0.975]
Epoch [64/120    avg_loss:0.097, val_acc:0.965]
Epoch [65/120    avg_loss:0.156, val_acc:0.979]
Epoch [66/120    avg_loss:0.108, val_acc:0.963]
Epoch [67/120    avg_loss:0.113, val_acc:0.959]
Epoch [68/120    avg_loss:0.105, val_acc:0.973]
Epoch [69/120    avg_loss:0.055, val_acc:0.973]
Epoch [70/120    avg_loss:0.076, val_acc:0.980]
Epoch [71/120    avg_loss:0.060, val_acc:0.979]
Epoch [72/120    avg_loss:0.045, val_acc:0.982]
Epoch [73/120    avg_loss:0.056, val_acc:0.986]
Epoch [74/120    avg_loss:0.045, val_acc:0.986]
Epoch [75/120    avg_loss:0.051, val_acc:0.988]
Epoch [76/120    avg_loss:0.041, val_acc:0.988]
Epoch [77/120    avg_loss:0.045, val_acc:0.986]
Epoch [78/120    avg_loss:0.049, val_acc:0.984]
Epoch [79/120    avg_loss:0.043, val_acc:0.986]
Epoch [80/120    avg_loss:0.040, val_acc:0.988]
Epoch [81/120    avg_loss:0.043, val_acc:0.988]
Epoch [82/120    avg_loss:0.034, val_acc:0.990]
Epoch [83/120    avg_loss:0.047, val_acc:0.990]
Epoch [84/120    avg_loss:0.039, val_acc:0.990]
Epoch [85/120    avg_loss:0.043, val_acc:0.988]
Epoch [86/120    avg_loss:0.042, val_acc:0.990]
Epoch [87/120    avg_loss:0.049, val_acc:0.988]
Epoch [88/120    avg_loss:0.040, val_acc:0.990]
Epoch [89/120    avg_loss:0.026, val_acc:0.992]
Epoch [90/120    avg_loss:0.033, val_acc:0.990]
Epoch [91/120    avg_loss:0.053, val_acc:0.990]
Epoch [92/120    avg_loss:0.029, val_acc:0.990]
Epoch [93/120    avg_loss:0.041, val_acc:0.992]
Epoch [94/120    avg_loss:0.039, val_acc:0.992]
Epoch [95/120    avg_loss:0.036, val_acc:0.988]
Epoch [96/120    avg_loss:0.032, val_acc:0.988]
Epoch [97/120    avg_loss:0.054, val_acc:0.992]
Epoch [98/120    avg_loss:0.043, val_acc:0.990]
Epoch [99/120    avg_loss:0.036, val_acc:0.990]
Epoch [100/120    avg_loss:0.030, val_acc:0.992]
Epoch [101/120    avg_loss:0.028, val_acc:0.990]
Epoch [102/120    avg_loss:0.032, val_acc:0.992]
Epoch [103/120    avg_loss:0.035, val_acc:0.990]
Epoch [104/120    avg_loss:0.036, val_acc:0.992]
Epoch [105/120    avg_loss:0.035, val_acc:0.992]
Epoch [106/120    avg_loss:0.030, val_acc:0.992]
Epoch [107/120    avg_loss:0.038, val_acc:0.992]
Epoch [108/120    avg_loss:0.025, val_acc:0.990]
Epoch [109/120    avg_loss:0.032, val_acc:0.990]
Epoch [110/120    avg_loss:0.028, val_acc:0.990]
Epoch [111/120    avg_loss:0.020, val_acc:0.990]
Epoch [112/120    avg_loss:0.026, val_acc:0.988]
Epoch [113/120    avg_loss:0.030, val_acc:0.988]
Epoch [114/120    avg_loss:0.039, val_acc:0.988]
Epoch [115/120    avg_loss:0.034, val_acc:0.990]
Epoch [116/120    avg_loss:0.030, val_acc:0.994]
Epoch [117/120    avg_loss:0.028, val_acc:0.994]
Epoch [118/120    avg_loss:0.031, val_acc:0.988]
Epoch [119/120    avg_loss:0.025, val_acc:0.990]
Epoch [120/120    avg_loss:0.027, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  15   0   0   0   0   0   0   2   0]
 [  0   0   0   0   8 136   1   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   2 466   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 1.         0.97104677 0.99782135 0.94170404 0.91891892
 0.99757869 0.92655367 0.99742931 0.99785867 1.         0.99867198
 0.99669967 1.        ]

Kappa:
0.9897919634857315
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f527ff107f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.967, val_acc:0.581]
Epoch [2/120    avg_loss:1.319, val_acc:0.742]
Epoch [3/120    avg_loss:1.008, val_acc:0.732]
Epoch [4/120    avg_loss:0.903, val_acc:0.724]
Epoch [5/120    avg_loss:0.843, val_acc:0.681]
Epoch [6/120    avg_loss:0.827, val_acc:0.790]
Epoch [7/120    avg_loss:0.673, val_acc:0.782]
Epoch [8/120    avg_loss:0.604, val_acc:0.804]
Epoch [9/120    avg_loss:0.585, val_acc:0.831]
Epoch [10/120    avg_loss:0.582, val_acc:0.873]
Epoch [11/120    avg_loss:0.577, val_acc:0.829]
Epoch [12/120    avg_loss:0.593, val_acc:0.861]
Epoch [13/120    avg_loss:0.510, val_acc:0.869]
Epoch [14/120    avg_loss:0.488, val_acc:0.887]
Epoch [15/120    avg_loss:0.456, val_acc:0.869]
Epoch [16/120    avg_loss:0.467, val_acc:0.865]
Epoch [17/120    avg_loss:0.563, val_acc:0.865]
Epoch [18/120    avg_loss:0.489, val_acc:0.891]
Epoch [19/120    avg_loss:0.426, val_acc:0.915]
Epoch [20/120    avg_loss:0.405, val_acc:0.855]
Epoch [21/120    avg_loss:0.426, val_acc:0.917]
Epoch [22/120    avg_loss:0.368, val_acc:0.913]
Epoch [23/120    avg_loss:0.439, val_acc:0.917]
Epoch [24/120    avg_loss:0.347, val_acc:0.921]
Epoch [25/120    avg_loss:0.322, val_acc:0.903]
Epoch [26/120    avg_loss:0.366, val_acc:0.885]
Epoch [27/120    avg_loss:0.310, val_acc:0.927]
Epoch [28/120    avg_loss:0.366, val_acc:0.935]
Epoch [29/120    avg_loss:0.286, val_acc:0.948]
Epoch [30/120    avg_loss:0.307, val_acc:0.946]
Epoch [31/120    avg_loss:0.267, val_acc:0.942]
Epoch [32/120    avg_loss:0.309, val_acc:0.899]
Epoch [33/120    avg_loss:0.293, val_acc:0.909]
Epoch [34/120    avg_loss:0.292, val_acc:0.946]
Epoch [35/120    avg_loss:0.251, val_acc:0.944]
Epoch [36/120    avg_loss:0.170, val_acc:0.958]
Epoch [37/120    avg_loss:0.254, val_acc:0.899]
Epoch [38/120    avg_loss:0.226, val_acc:0.938]
Epoch [39/120    avg_loss:0.232, val_acc:0.931]
Epoch [40/120    avg_loss:0.185, val_acc:0.942]
Epoch [41/120    avg_loss:0.247, val_acc:0.946]
Epoch [42/120    avg_loss:0.220, val_acc:0.948]
Epoch [43/120    avg_loss:0.226, val_acc:0.964]
Epoch [44/120    avg_loss:0.186, val_acc:0.956]
Epoch [45/120    avg_loss:0.174, val_acc:0.958]
Epoch [46/120    avg_loss:0.179, val_acc:0.968]
Epoch [47/120    avg_loss:0.135, val_acc:0.974]
Epoch [48/120    avg_loss:0.158, val_acc:0.952]
Epoch [49/120    avg_loss:0.157, val_acc:0.956]
Epoch [50/120    avg_loss:0.138, val_acc:0.976]
Epoch [51/120    avg_loss:0.151, val_acc:0.964]
Epoch [52/120    avg_loss:0.174, val_acc:0.962]
Epoch [53/120    avg_loss:0.107, val_acc:0.966]
Epoch [54/120    avg_loss:0.126, val_acc:0.986]
Epoch [55/120    avg_loss:0.157, val_acc:0.964]
Epoch [56/120    avg_loss:0.162, val_acc:0.962]
Epoch [57/120    avg_loss:0.177, val_acc:0.960]
Epoch [58/120    avg_loss:0.175, val_acc:0.964]
Epoch [59/120    avg_loss:0.118, val_acc:0.982]
Epoch [60/120    avg_loss:0.096, val_acc:0.972]
Epoch [61/120    avg_loss:0.086, val_acc:0.978]
Epoch [62/120    avg_loss:0.096, val_acc:0.976]
Epoch [63/120    avg_loss:0.104, val_acc:0.962]
Epoch [64/120    avg_loss:0.127, val_acc:0.964]
Epoch [65/120    avg_loss:0.072, val_acc:0.974]
Epoch [66/120    avg_loss:0.073, val_acc:0.946]
Epoch [67/120    avg_loss:0.094, val_acc:0.968]
Epoch [68/120    avg_loss:0.086, val_acc:0.978]
Epoch [69/120    avg_loss:0.040, val_acc:0.984]
Epoch [70/120    avg_loss:0.038, val_acc:0.988]
Epoch [71/120    avg_loss:0.031, val_acc:0.988]
Epoch [72/120    avg_loss:0.032, val_acc:0.990]
Epoch [73/120    avg_loss:0.039, val_acc:0.990]
Epoch [74/120    avg_loss:0.032, val_acc:0.986]
Epoch [75/120    avg_loss:0.036, val_acc:0.986]
Epoch [76/120    avg_loss:0.028, val_acc:0.990]
Epoch [77/120    avg_loss:0.031, val_acc:0.988]
Epoch [78/120    avg_loss:0.027, val_acc:0.988]
Epoch [79/120    avg_loss:0.030, val_acc:0.988]
Epoch [80/120    avg_loss:0.023, val_acc:0.988]
Epoch [81/120    avg_loss:0.025, val_acc:0.988]
Epoch [82/120    avg_loss:0.023, val_acc:0.988]
Epoch [83/120    avg_loss:0.029, val_acc:0.990]
Epoch [84/120    avg_loss:0.019, val_acc:0.990]
Epoch [85/120    avg_loss:0.031, val_acc:0.990]
Epoch [86/120    avg_loss:0.028, val_acc:0.990]
Epoch [87/120    avg_loss:0.033, val_acc:0.992]
Epoch [88/120    avg_loss:0.027, val_acc:0.990]
Epoch [89/120    avg_loss:0.027, val_acc:0.990]
Epoch [90/120    avg_loss:0.036, val_acc:0.990]
Epoch [91/120    avg_loss:0.032, val_acc:0.990]
Epoch [92/120    avg_loss:0.031, val_acc:0.990]
Epoch [93/120    avg_loss:0.026, val_acc:0.990]
Epoch [94/120    avg_loss:0.024, val_acc:0.988]
Epoch [95/120    avg_loss:0.023, val_acc:0.988]
Epoch [96/120    avg_loss:0.024, val_acc:0.988]
Epoch [97/120    avg_loss:0.026, val_acc:0.990]
Epoch [98/120    avg_loss:0.016, val_acc:0.990]
Epoch [99/120    avg_loss:0.018, val_acc:0.990]
Epoch [100/120    avg_loss:0.020, val_acc:0.990]
Epoch [101/120    avg_loss:0.022, val_acc:0.990]
Epoch [102/120    avg_loss:0.031, val_acc:0.990]
Epoch [103/120    avg_loss:0.019, val_acc:0.990]
Epoch [104/120    avg_loss:0.020, val_acc:0.988]
Epoch [105/120    avg_loss:0.023, val_acc:0.988]
Epoch [106/120    avg_loss:0.027, val_acc:0.990]
Epoch [107/120    avg_loss:0.017, val_acc:0.990]
Epoch [108/120    avg_loss:0.017, val_acc:0.990]
Epoch [109/120    avg_loss:0.022, val_acc:0.990]
Epoch [110/120    avg_loss:0.023, val_acc:0.990]
Epoch [111/120    avg_loss:0.021, val_acc:0.990]
Epoch [112/120    avg_loss:0.020, val_acc:0.988]
Epoch [113/120    avg_loss:0.027, val_acc:0.990]
Epoch [114/120    avg_loss:0.021, val_acc:0.990]
Epoch [115/120    avg_loss:0.022, val_acc:0.990]
Epoch [116/120    avg_loss:0.020, val_acc:0.988]
Epoch [117/120    avg_loss:0.021, val_acc:0.988]
Epoch [118/120    avg_loss:0.021, val_acc:0.988]
Epoch [119/120    avg_loss:0.019, val_acc:0.988]
Epoch [120/120    avg_loss:0.016, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   3 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 1.         0.98871332 0.99782135 0.93777778 0.9047619
 1.         0.9726776  0.998713   1.         1.         0.99603699
 0.99667774 1.        ]

Kappa:
0.9912167852583714
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f587de167b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.985, val_acc:0.597]
Epoch [2/120    avg_loss:1.276, val_acc:0.677]
Epoch [3/120    avg_loss:0.989, val_acc:0.764]
Epoch [4/120    avg_loss:0.882, val_acc:0.796]
Epoch [5/120    avg_loss:0.754, val_acc:0.780]
Epoch [6/120    avg_loss:0.738, val_acc:0.800]
Epoch [7/120    avg_loss:0.645, val_acc:0.776]
Epoch [8/120    avg_loss:0.684, val_acc:0.819]
Epoch [9/120    avg_loss:0.604, val_acc:0.881]
Epoch [10/120    avg_loss:0.618, val_acc:0.833]
Epoch [11/120    avg_loss:0.526, val_acc:0.881]
Epoch [12/120    avg_loss:0.453, val_acc:0.857]
Epoch [13/120    avg_loss:0.501, val_acc:0.899]
Epoch [14/120    avg_loss:0.351, val_acc:0.855]
Epoch [15/120    avg_loss:0.570, val_acc:0.893]
Epoch [16/120    avg_loss:0.393, val_acc:0.921]
Epoch [17/120    avg_loss:0.425, val_acc:0.833]
Epoch [18/120    avg_loss:0.416, val_acc:0.871]
Epoch [19/120    avg_loss:0.356, val_acc:0.915]
Epoch [20/120    avg_loss:0.257, val_acc:0.935]
Epoch [21/120    avg_loss:0.222, val_acc:0.931]
Epoch [22/120    avg_loss:0.276, val_acc:0.923]
Epoch [23/120    avg_loss:0.268, val_acc:0.929]
Epoch [24/120    avg_loss:0.299, val_acc:0.899]
Epoch [25/120    avg_loss:0.210, val_acc:0.927]
Epoch [26/120    avg_loss:0.177, val_acc:0.938]
Epoch [27/120    avg_loss:0.313, val_acc:0.925]
Epoch [28/120    avg_loss:0.188, val_acc:0.935]
Epoch [29/120    avg_loss:0.230, val_acc:0.933]
Epoch [30/120    avg_loss:0.166, val_acc:0.950]
Epoch [31/120    avg_loss:0.147, val_acc:0.948]
Epoch [32/120    avg_loss:0.291, val_acc:0.954]
Epoch [33/120    avg_loss:0.232, val_acc:0.937]
Epoch [34/120    avg_loss:0.184, val_acc:0.925]
Epoch [35/120    avg_loss:0.203, val_acc:0.940]
Epoch [36/120    avg_loss:0.245, val_acc:0.960]
Epoch [37/120    avg_loss:0.191, val_acc:0.944]
Epoch [38/120    avg_loss:0.177, val_acc:0.964]
Epoch [39/120    avg_loss:0.119, val_acc:0.956]
Epoch [40/120    avg_loss:0.113, val_acc:0.937]
Epoch [41/120    avg_loss:0.157, val_acc:0.964]
Epoch [42/120    avg_loss:0.065, val_acc:0.962]
Epoch [43/120    avg_loss:0.066, val_acc:0.948]
Epoch [44/120    avg_loss:0.140, val_acc:0.968]
Epoch [45/120    avg_loss:0.122, val_acc:0.972]
Epoch [46/120    avg_loss:0.095, val_acc:0.962]
Epoch [47/120    avg_loss:0.076, val_acc:0.976]
Epoch [48/120    avg_loss:0.085, val_acc:0.946]
Epoch [49/120    avg_loss:0.080, val_acc:0.986]
Epoch [50/120    avg_loss:0.069, val_acc:0.968]
Epoch [51/120    avg_loss:0.070, val_acc:0.964]
Epoch [52/120    avg_loss:0.108, val_acc:0.940]
Epoch [53/120    avg_loss:0.138, val_acc:0.968]
Epoch [54/120    avg_loss:0.073, val_acc:0.974]
Epoch [55/120    avg_loss:0.095, val_acc:0.935]
Epoch [56/120    avg_loss:0.083, val_acc:0.982]
Epoch [57/120    avg_loss:0.031, val_acc:0.976]
Epoch [58/120    avg_loss:0.097, val_acc:0.960]
Epoch [59/120    avg_loss:0.102, val_acc:0.978]
Epoch [60/120    avg_loss:0.027, val_acc:0.974]
Epoch [61/120    avg_loss:0.053, val_acc:0.954]
Epoch [62/120    avg_loss:0.051, val_acc:0.976]
Epoch [63/120    avg_loss:0.028, val_acc:0.984]
Epoch [64/120    avg_loss:0.027, val_acc:0.986]
Epoch [65/120    avg_loss:0.025, val_acc:0.986]
Epoch [66/120    avg_loss:0.018, val_acc:0.988]
Epoch [67/120    avg_loss:0.026, val_acc:0.990]
Epoch [68/120    avg_loss:0.019, val_acc:0.990]
Epoch [69/120    avg_loss:0.017, val_acc:0.994]
Epoch [70/120    avg_loss:0.011, val_acc:0.990]
Epoch [71/120    avg_loss:0.018, val_acc:0.992]
Epoch [72/120    avg_loss:0.014, val_acc:0.992]
Epoch [73/120    avg_loss:0.015, val_acc:0.992]
Epoch [74/120    avg_loss:0.012, val_acc:0.992]
Epoch [75/120    avg_loss:0.016, val_acc:0.990]
Epoch [76/120    avg_loss:0.018, val_acc:0.990]
Epoch [77/120    avg_loss:0.015, val_acc:0.990]
Epoch [78/120    avg_loss:0.016, val_acc:0.990]
Epoch [79/120    avg_loss:0.016, val_acc:0.988]
Epoch [80/120    avg_loss:0.018, val_acc:0.990]
Epoch [81/120    avg_loss:0.015, val_acc:0.992]
Epoch [82/120    avg_loss:0.015, val_acc:0.990]
Epoch [83/120    avg_loss:0.013, val_acc:0.992]
Epoch [84/120    avg_loss:0.013, val_acc:0.990]
Epoch [85/120    avg_loss:0.011, val_acc:0.990]
Epoch [86/120    avg_loss:0.010, val_acc:0.990]
Epoch [87/120    avg_loss:0.011, val_acc:0.990]
Epoch [88/120    avg_loss:0.012, val_acc:0.990]
Epoch [89/120    avg_loss:0.015, val_acc:0.990]
Epoch [90/120    avg_loss:0.017, val_acc:0.992]
Epoch [91/120    avg_loss:0.012, val_acc:0.992]
Epoch [92/120    avg_loss:0.017, val_acc:0.992]
Epoch [93/120    avg_loss:0.014, val_acc:0.992]
Epoch [94/120    avg_loss:0.011, val_acc:0.992]
Epoch [95/120    avg_loss:0.019, val_acc:0.992]
Epoch [96/120    avg_loss:0.015, val_acc:0.992]
Epoch [97/120    avg_loss:0.012, val_acc:0.992]
Epoch [98/120    avg_loss:0.014, val_acc:0.992]
Epoch [99/120    avg_loss:0.015, val_acc:0.992]
Epoch [100/120    avg_loss:0.013, val_acc:0.992]
Epoch [101/120    avg_loss:0.014, val_acc:0.992]
Epoch [102/120    avg_loss:0.012, val_acc:0.992]
Epoch [103/120    avg_loss:0.021, val_acc:0.992]
Epoch [104/120    avg_loss:0.011, val_acc:0.992]
Epoch [105/120    avg_loss:0.011, val_acc:0.992]
Epoch [106/120    avg_loss:0.010, val_acc:0.992]
Epoch [107/120    avg_loss:0.013, val_acc:0.992]
Epoch [108/120    avg_loss:0.015, val_acc:0.992]
Epoch [109/120    avg_loss:0.010, val_acc:0.992]
Epoch [110/120    avg_loss:0.016, val_acc:0.992]
Epoch [111/120    avg_loss:0.012, val_acc:0.992]
Epoch [112/120    avg_loss:0.009, val_acc:0.992]
Epoch [113/120    avg_loss:0.013, val_acc:0.992]
Epoch [114/120    avg_loss:0.012, val_acc:0.992]
Epoch [115/120    avg_loss:0.010, val_acc:0.992]
Epoch [116/120    avg_loss:0.011, val_acc:0.992]
Epoch [117/120    avg_loss:0.009, val_acc:0.992]
Epoch [118/120    avg_loss:0.010, val_acc:0.992]
Epoch [119/120    avg_loss:0.009, val_acc:0.992]
Epoch [120/120    avg_loss:0.013, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 214   0   0   0   0   5   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 361   0   3   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.98845266 0.99563319 0.94666667 0.92567568
 1.         0.97409326 1.         1.         0.99586207 0.9973545
 0.99448732 1.        ]

Kappa:
0.9919294422913622
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffa71253898>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.024, val_acc:0.538]
Epoch [2/120    avg_loss:1.301, val_acc:0.651]
Epoch [3/120    avg_loss:0.989, val_acc:0.790]
Epoch [4/120    avg_loss:0.924, val_acc:0.760]
Epoch [5/120    avg_loss:0.846, val_acc:0.696]
Epoch [6/120    avg_loss:0.764, val_acc:0.841]
Epoch [7/120    avg_loss:0.719, val_acc:0.800]
Epoch [8/120    avg_loss:0.623, val_acc:0.845]
Epoch [9/120    avg_loss:0.593, val_acc:0.869]
Epoch [10/120    avg_loss:0.603, val_acc:0.865]
Epoch [11/120    avg_loss:0.524, val_acc:0.877]
Epoch [12/120    avg_loss:0.501, val_acc:0.845]
Epoch [13/120    avg_loss:0.577, val_acc:0.881]
Epoch [14/120    avg_loss:0.511, val_acc:0.859]
Epoch [15/120    avg_loss:0.466, val_acc:0.887]
Epoch [16/120    avg_loss:0.502, val_acc:0.891]
Epoch [17/120    avg_loss:0.522, val_acc:0.893]
Epoch [18/120    avg_loss:0.447, val_acc:0.891]
Epoch [19/120    avg_loss:0.436, val_acc:0.875]
Epoch [20/120    avg_loss:0.449, val_acc:0.903]
Epoch [21/120    avg_loss:0.377, val_acc:0.893]
Epoch [22/120    avg_loss:0.382, val_acc:0.905]
Epoch [23/120    avg_loss:0.401, val_acc:0.891]
Epoch [24/120    avg_loss:0.320, val_acc:0.897]
Epoch [25/120    avg_loss:0.378, val_acc:0.859]
Epoch [26/120    avg_loss:0.426, val_acc:0.915]
Epoch [27/120    avg_loss:0.304, val_acc:0.883]
Epoch [28/120    avg_loss:0.424, val_acc:0.915]
Epoch [29/120    avg_loss:0.323, val_acc:0.925]
Epoch [30/120    avg_loss:0.320, val_acc:0.913]
Epoch [31/120    avg_loss:0.340, val_acc:0.921]
Epoch [32/120    avg_loss:0.383, val_acc:0.921]
Epoch [33/120    avg_loss:0.286, val_acc:0.927]
Epoch [34/120    avg_loss:0.261, val_acc:0.903]
Epoch [35/120    avg_loss:0.308, val_acc:0.921]
Epoch [36/120    avg_loss:0.270, val_acc:0.937]
Epoch [37/120    avg_loss:0.253, val_acc:0.927]
Epoch [38/120    avg_loss:0.263, val_acc:0.921]
Epoch [39/120    avg_loss:0.292, val_acc:0.917]
Epoch [40/120    avg_loss:0.302, val_acc:0.903]
Epoch [41/120    avg_loss:0.247, val_acc:0.950]
Epoch [42/120    avg_loss:0.245, val_acc:0.923]
Epoch [43/120    avg_loss:0.201, val_acc:0.929]
Epoch [44/120    avg_loss:0.185, val_acc:0.950]
Epoch [45/120    avg_loss:0.298, val_acc:0.925]
Epoch [46/120    avg_loss:0.220, val_acc:0.909]
Epoch [47/120    avg_loss:0.271, val_acc:0.956]
Epoch [48/120    avg_loss:0.217, val_acc:0.942]
Epoch [49/120    avg_loss:0.142, val_acc:0.962]
Epoch [50/120    avg_loss:0.138, val_acc:0.948]
Epoch [51/120    avg_loss:0.166, val_acc:0.952]
Epoch [52/120    avg_loss:0.123, val_acc:0.966]
Epoch [53/120    avg_loss:0.128, val_acc:0.956]
Epoch [54/120    avg_loss:0.293, val_acc:0.923]
Epoch [55/120    avg_loss:0.167, val_acc:0.933]
Epoch [56/120    avg_loss:0.138, val_acc:0.948]
Epoch [57/120    avg_loss:0.140, val_acc:0.962]
Epoch [58/120    avg_loss:0.099, val_acc:0.935]
Epoch [59/120    avg_loss:0.094, val_acc:0.958]
Epoch [60/120    avg_loss:0.291, val_acc:0.935]
Epoch [61/120    avg_loss:0.170, val_acc:0.952]
Epoch [62/120    avg_loss:0.166, val_acc:0.962]
Epoch [63/120    avg_loss:0.086, val_acc:0.964]
Epoch [64/120    avg_loss:0.116, val_acc:0.962]
Epoch [65/120    avg_loss:0.083, val_acc:0.962]
Epoch [66/120    avg_loss:0.062, val_acc:0.966]
Epoch [67/120    avg_loss:0.045, val_acc:0.968]
Epoch [68/120    avg_loss:0.049, val_acc:0.976]
Epoch [69/120    avg_loss:0.048, val_acc:0.974]
Epoch [70/120    avg_loss:0.039, val_acc:0.976]
Epoch [71/120    avg_loss:0.040, val_acc:0.978]
Epoch [72/120    avg_loss:0.049, val_acc:0.976]
Epoch [73/120    avg_loss:0.043, val_acc:0.974]
Epoch [74/120    avg_loss:0.044, val_acc:0.978]
Epoch [75/120    avg_loss:0.026, val_acc:0.976]
Epoch [76/120    avg_loss:0.041, val_acc:0.974]
Epoch [77/120    avg_loss:0.033, val_acc:0.976]
Epoch [78/120    avg_loss:0.032, val_acc:0.976]
Epoch [79/120    avg_loss:0.024, val_acc:0.974]
Epoch [80/120    avg_loss:0.038, val_acc:0.974]
Epoch [81/120    avg_loss:0.046, val_acc:0.974]
Epoch [82/120    avg_loss:0.035, val_acc:0.972]
Epoch [83/120    avg_loss:0.043, val_acc:0.968]
Epoch [84/120    avg_loss:0.038, val_acc:0.974]
Epoch [85/120    avg_loss:0.027, val_acc:0.976]
Epoch [86/120    avg_loss:0.034, val_acc:0.974]
Epoch [87/120    avg_loss:0.029, val_acc:0.976]
Epoch [88/120    avg_loss:0.026, val_acc:0.976]
Epoch [89/120    avg_loss:0.034, val_acc:0.976]
Epoch [90/120    avg_loss:0.028, val_acc:0.976]
Epoch [91/120    avg_loss:0.027, val_acc:0.976]
Epoch [92/120    avg_loss:0.034, val_acc:0.976]
Epoch [93/120    avg_loss:0.028, val_acc:0.976]
Epoch [94/120    avg_loss:0.038, val_acc:0.974]
Epoch [95/120    avg_loss:0.032, val_acc:0.978]
Epoch [96/120    avg_loss:0.036, val_acc:0.976]
Epoch [97/120    avg_loss:0.034, val_acc:0.974]
Epoch [98/120    avg_loss:0.018, val_acc:0.974]
Epoch [99/120    avg_loss:0.039, val_acc:0.974]
Epoch [100/120    avg_loss:0.030, val_acc:0.974]
Epoch [101/120    avg_loss:0.029, val_acc:0.974]
Epoch [102/120    avg_loss:0.034, val_acc:0.972]
Epoch [103/120    avg_loss:0.037, val_acc:0.974]
Epoch [104/120    avg_loss:0.037, val_acc:0.974]
Epoch [105/120    avg_loss:0.029, val_acc:0.974]
Epoch [106/120    avg_loss:0.033, val_acc:0.974]
Epoch [107/120    avg_loss:0.037, val_acc:0.974]
Epoch [108/120    avg_loss:0.038, val_acc:0.974]
Epoch [109/120    avg_loss:0.035, val_acc:0.974]
Epoch [110/120    avg_loss:0.021, val_acc:0.974]
Epoch [111/120    avg_loss:0.025, val_acc:0.974]
Epoch [112/120    avg_loss:0.023, val_acc:0.974]
Epoch [113/120    avg_loss:0.024, val_acc:0.974]
Epoch [114/120    avg_loss:0.028, val_acc:0.974]
Epoch [115/120    avg_loss:0.034, val_acc:0.974]
Epoch [116/120    avg_loss:0.027, val_acc:0.974]
Epoch [117/120    avg_loss:0.035, val_acc:0.974]
Epoch [118/120    avg_loss:0.022, val_acc:0.974]
Epoch [119/120    avg_loss:0.029, val_acc:0.974]
Epoch [120/120    avg_loss:0.035, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 208  19   0   0   0   2   1   0   0   0   0]
 [  0   0   0   3 206  18   0   0   0   0   0   0   0   0]
 [  0   0   0   2   8 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   2 466   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.72068230277185

F1 scores:
[       nan 1.         0.98871332 0.93905192 0.89565217 0.90604027
 1.         0.9726776  0.99487179 0.99679144 1.         1.
 1.         1.        ]

Kappa:
0.9857568813016246
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efdf25907b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.055, val_acc:0.524]
Epoch [2/120    avg_loss:1.369, val_acc:0.665]
Epoch [3/120    avg_loss:1.046, val_acc:0.776]
Epoch [4/120    avg_loss:0.851, val_acc:0.772]
Epoch [5/120    avg_loss:0.718, val_acc:0.859]
Epoch [6/120    avg_loss:0.704, val_acc:0.853]
Epoch [7/120    avg_loss:0.672, val_acc:0.804]
Epoch [8/120    avg_loss:0.636, val_acc:0.827]
Epoch [9/120    avg_loss:0.498, val_acc:0.905]
Epoch [10/120    avg_loss:0.529, val_acc:0.885]
Epoch [11/120    avg_loss:0.510, val_acc:0.895]
Epoch [12/120    avg_loss:0.463, val_acc:0.877]
Epoch [13/120    avg_loss:0.519, val_acc:0.853]
Epoch [14/120    avg_loss:0.522, val_acc:0.909]
Epoch [15/120    avg_loss:0.439, val_acc:0.905]
Epoch [16/120    avg_loss:0.383, val_acc:0.881]
Epoch [17/120    avg_loss:0.400, val_acc:0.913]
Epoch [18/120    avg_loss:0.387, val_acc:0.893]
Epoch [19/120    avg_loss:0.342, val_acc:0.871]
Epoch [20/120    avg_loss:0.404, val_acc:0.927]
Epoch [21/120    avg_loss:0.346, val_acc:0.921]
Epoch [22/120    avg_loss:0.293, val_acc:0.909]
Epoch [23/120    avg_loss:0.325, val_acc:0.897]
Epoch [24/120    avg_loss:0.422, val_acc:0.883]
Epoch [25/120    avg_loss:0.333, val_acc:0.907]
Epoch [26/120    avg_loss:0.308, val_acc:0.897]
Epoch [27/120    avg_loss:0.232, val_acc:0.952]
Epoch [28/120    avg_loss:0.223, val_acc:0.885]
Epoch [29/120    avg_loss:0.330, val_acc:0.952]
Epoch [30/120    avg_loss:0.191, val_acc:0.907]
Epoch [31/120    avg_loss:0.203, val_acc:0.956]
Epoch [32/120    avg_loss:0.306, val_acc:0.927]
Epoch [33/120    avg_loss:0.239, val_acc:0.938]
Epoch [34/120    avg_loss:0.211, val_acc:0.948]
Epoch [35/120    avg_loss:0.217, val_acc:0.962]
Epoch [36/120    avg_loss:0.251, val_acc:0.919]
Epoch [37/120    avg_loss:0.216, val_acc:0.960]
Epoch [38/120    avg_loss:0.291, val_acc:0.964]
Epoch [39/120    avg_loss:0.145, val_acc:0.952]
Epoch [40/120    avg_loss:0.133, val_acc:0.952]
Epoch [41/120    avg_loss:0.106, val_acc:0.970]
Epoch [42/120    avg_loss:0.151, val_acc:0.935]
Epoch [43/120    avg_loss:0.146, val_acc:0.960]
Epoch [44/120    avg_loss:0.147, val_acc:0.954]
Epoch [45/120    avg_loss:0.196, val_acc:0.964]
Epoch [46/120    avg_loss:0.182, val_acc:0.954]
Epoch [47/120    avg_loss:0.128, val_acc:0.940]
Epoch [48/120    avg_loss:0.138, val_acc:0.974]
Epoch [49/120    avg_loss:0.135, val_acc:0.956]
Epoch [50/120    avg_loss:0.131, val_acc:0.966]
Epoch [51/120    avg_loss:0.122, val_acc:0.962]
Epoch [52/120    avg_loss:0.116, val_acc:0.976]
Epoch [53/120    avg_loss:0.158, val_acc:0.958]
Epoch [54/120    avg_loss:0.127, val_acc:0.970]
Epoch [55/120    avg_loss:0.147, val_acc:0.966]
Epoch [56/120    avg_loss:0.143, val_acc:0.976]
Epoch [57/120    avg_loss:0.069, val_acc:0.974]
Epoch [58/120    avg_loss:0.100, val_acc:0.978]
Epoch [59/120    avg_loss:0.102, val_acc:0.964]
Epoch [60/120    avg_loss:0.063, val_acc:0.954]
Epoch [61/120    avg_loss:0.102, val_acc:0.974]
Epoch [62/120    avg_loss:0.060, val_acc:0.966]
Epoch [63/120    avg_loss:0.081, val_acc:0.970]
Epoch [64/120    avg_loss:0.045, val_acc:0.964]
Epoch [65/120    avg_loss:0.039, val_acc:0.976]
Epoch [66/120    avg_loss:0.033, val_acc:0.982]
Epoch [67/120    avg_loss:0.058, val_acc:0.976]
Epoch [68/120    avg_loss:0.071, val_acc:0.968]
Epoch [69/120    avg_loss:0.070, val_acc:0.980]
Epoch [70/120    avg_loss:0.078, val_acc:0.968]
Epoch [71/120    avg_loss:0.052, val_acc:0.980]
Epoch [72/120    avg_loss:0.042, val_acc:0.986]
Epoch [73/120    avg_loss:0.051, val_acc:0.962]
Epoch [74/120    avg_loss:0.060, val_acc:0.974]
Epoch [75/120    avg_loss:0.081, val_acc:0.948]
Epoch [76/120    avg_loss:0.124, val_acc:0.974]
Epoch [77/120    avg_loss:0.123, val_acc:0.972]
Epoch [78/120    avg_loss:0.065, val_acc:0.968]
Epoch [79/120    avg_loss:0.061, val_acc:0.978]
Epoch [80/120    avg_loss:0.060, val_acc:0.976]
Epoch [81/120    avg_loss:0.049, val_acc:0.962]
Epoch [82/120    avg_loss:0.038, val_acc:0.976]
Epoch [83/120    avg_loss:0.049, val_acc:0.964]
Epoch [84/120    avg_loss:0.125, val_acc:0.958]
Epoch [85/120    avg_loss:0.097, val_acc:0.956]
Epoch [86/120    avg_loss:0.073, val_acc:0.976]
Epoch [87/120    avg_loss:0.066, val_acc:0.984]
Epoch [88/120    avg_loss:0.052, val_acc:0.986]
Epoch [89/120    avg_loss:0.026, val_acc:0.984]
Epoch [90/120    avg_loss:0.032, val_acc:0.984]
Epoch [91/120    avg_loss:0.020, val_acc:0.984]
Epoch [92/120    avg_loss:0.018, val_acc:0.984]
Epoch [93/120    avg_loss:0.025, val_acc:0.984]
Epoch [94/120    avg_loss:0.031, val_acc:0.984]
Epoch [95/120    avg_loss:0.021, val_acc:0.984]
Epoch [96/120    avg_loss:0.028, val_acc:0.984]
Epoch [97/120    avg_loss:0.017, val_acc:0.984]
Epoch [98/120    avg_loss:0.018, val_acc:0.984]
Epoch [99/120    avg_loss:0.018, val_acc:0.984]
Epoch [100/120    avg_loss:0.023, val_acc:0.984]
Epoch [101/120    avg_loss:0.023, val_acc:0.984]
Epoch [102/120    avg_loss:0.018, val_acc:0.984]
Epoch [103/120    avg_loss:0.022, val_acc:0.984]
Epoch [104/120    avg_loss:0.018, val_acc:0.984]
Epoch [105/120    avg_loss:0.016, val_acc:0.984]
Epoch [106/120    avg_loss:0.021, val_acc:0.984]
Epoch [107/120    avg_loss:0.017, val_acc:0.984]
Epoch [108/120    avg_loss:0.017, val_acc:0.984]
Epoch [109/120    avg_loss:0.027, val_acc:0.984]
Epoch [110/120    avg_loss:0.039, val_acc:0.984]
Epoch [111/120    avg_loss:0.020, val_acc:0.984]
Epoch [112/120    avg_loss:0.012, val_acc:0.984]
Epoch [113/120    avg_loss:0.022, val_acc:0.984]
Epoch [114/120    avg_loss:0.019, val_acc:0.984]
Epoch [115/120    avg_loss:0.016, val_acc:0.984]
Epoch [116/120    avg_loss:0.015, val_acc:0.984]
Epoch [117/120    avg_loss:0.015, val_acc:0.984]
Epoch [118/120    avg_loss:0.020, val_acc:0.984]
Epoch [119/120    avg_loss:0.015, val_acc:0.984]
Epoch [120/120    avg_loss:0.019, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 225   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.06183368869935

F1 scores:
[       nan 1.         0.98871332 0.98901099 0.91612903 0.87632509
 1.         0.97826087 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9895546603419619
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd48631f7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.045, val_acc:0.587]
Epoch [2/120    avg_loss:1.323, val_acc:0.677]
Epoch [3/120    avg_loss:1.021, val_acc:0.752]
Epoch [4/120    avg_loss:0.862, val_acc:0.792]
Epoch [5/120    avg_loss:0.795, val_acc:0.821]
Epoch [6/120    avg_loss:0.750, val_acc:0.827]
Epoch [7/120    avg_loss:0.669, val_acc:0.794]
Epoch [8/120    avg_loss:0.711, val_acc:0.804]
Epoch [9/120    avg_loss:0.544, val_acc:0.843]
Epoch [10/120    avg_loss:0.595, val_acc:0.853]
Epoch [11/120    avg_loss:0.457, val_acc:0.875]
Epoch [12/120    avg_loss:0.490, val_acc:0.865]
Epoch [13/120    avg_loss:0.487, val_acc:0.869]
Epoch [14/120    avg_loss:0.486, val_acc:0.837]
Epoch [15/120    avg_loss:0.504, val_acc:0.879]
Epoch [16/120    avg_loss:0.424, val_acc:0.879]
Epoch [17/120    avg_loss:0.428, val_acc:0.869]
Epoch [18/120    avg_loss:0.402, val_acc:0.865]
Epoch [19/120    avg_loss:0.391, val_acc:0.887]
Epoch [20/120    avg_loss:0.356, val_acc:0.919]
Epoch [21/120    avg_loss:0.304, val_acc:0.913]
Epoch [22/120    avg_loss:0.369, val_acc:0.917]
Epoch [23/120    avg_loss:0.311, val_acc:0.919]
Epoch [24/120    avg_loss:0.296, val_acc:0.919]
Epoch [25/120    avg_loss:0.266, val_acc:0.938]
Epoch [26/120    avg_loss:0.339, val_acc:0.905]
Epoch [27/120    avg_loss:0.348, val_acc:0.905]
Epoch [28/120    avg_loss:0.225, val_acc:0.897]
Epoch [29/120    avg_loss:0.313, val_acc:0.946]
Epoch [30/120    avg_loss:0.239, val_acc:0.942]
Epoch [31/120    avg_loss:0.260, val_acc:0.937]
Epoch [32/120    avg_loss:0.256, val_acc:0.935]
Epoch [33/120    avg_loss:0.203, val_acc:0.935]
Epoch [34/120    avg_loss:0.179, val_acc:0.933]
Epoch [35/120    avg_loss:0.158, val_acc:0.954]
Epoch [36/120    avg_loss:0.214, val_acc:0.925]
Epoch [37/120    avg_loss:0.197, val_acc:0.919]
Epoch [38/120    avg_loss:0.115, val_acc:0.952]
Epoch [39/120    avg_loss:0.133, val_acc:0.942]
Epoch [40/120    avg_loss:0.150, val_acc:0.940]
Epoch [41/120    avg_loss:0.212, val_acc:0.954]
Epoch [42/120    avg_loss:0.135, val_acc:0.946]
Epoch [43/120    avg_loss:0.194, val_acc:0.938]
Epoch [44/120    avg_loss:0.145, val_acc:0.946]
Epoch [45/120    avg_loss:0.166, val_acc:0.958]
Epoch [46/120    avg_loss:0.165, val_acc:0.966]
Epoch [47/120    avg_loss:0.064, val_acc:0.960]
Epoch [48/120    avg_loss:0.093, val_acc:0.966]
Epoch [49/120    avg_loss:0.124, val_acc:0.968]
Epoch [50/120    avg_loss:0.157, val_acc:0.950]
Epoch [51/120    avg_loss:0.152, val_acc:0.956]
Epoch [52/120    avg_loss:0.178, val_acc:0.950]
Epoch [53/120    avg_loss:0.119, val_acc:0.968]
Epoch [54/120    avg_loss:0.141, val_acc:0.960]
Epoch [55/120    avg_loss:0.081, val_acc:0.964]
Epoch [56/120    avg_loss:0.063, val_acc:0.966]
Epoch [57/120    avg_loss:0.067, val_acc:0.968]
Epoch [58/120    avg_loss:0.089, val_acc:0.954]
Epoch [59/120    avg_loss:0.087, val_acc:0.960]
Epoch [60/120    avg_loss:0.054, val_acc:0.968]
Epoch [61/120    avg_loss:0.102, val_acc:0.948]
Epoch [62/120    avg_loss:0.085, val_acc:0.962]
Epoch [63/120    avg_loss:0.104, val_acc:0.952]
Epoch [64/120    avg_loss:0.177, val_acc:0.950]
Epoch [65/120    avg_loss:0.073, val_acc:0.972]
Epoch [66/120    avg_loss:0.079, val_acc:0.960]
Epoch [67/120    avg_loss:0.057, val_acc:0.978]
Epoch [68/120    avg_loss:0.136, val_acc:0.966]
Epoch [69/120    avg_loss:0.068, val_acc:0.956]
Epoch [70/120    avg_loss:0.099, val_acc:0.964]
Epoch [71/120    avg_loss:0.093, val_acc:0.968]
Epoch [72/120    avg_loss:0.053, val_acc:0.984]
Epoch [73/120    avg_loss:0.031, val_acc:0.982]
Epoch [74/120    avg_loss:0.024, val_acc:0.982]
Epoch [75/120    avg_loss:0.032, val_acc:0.980]
Epoch [76/120    avg_loss:0.038, val_acc:0.990]
Epoch [77/120    avg_loss:0.060, val_acc:0.968]
Epoch [78/120    avg_loss:0.082, val_acc:0.972]
Epoch [79/120    avg_loss:0.077, val_acc:0.940]
Epoch [80/120    avg_loss:0.086, val_acc:0.972]
Epoch [81/120    avg_loss:0.080, val_acc:0.950]
Epoch [82/120    avg_loss:0.132, val_acc:0.946]
Epoch [83/120    avg_loss:0.067, val_acc:0.982]
Epoch [84/120    avg_loss:0.041, val_acc:0.944]
Epoch [85/120    avg_loss:0.139, val_acc:0.966]
Epoch [86/120    avg_loss:0.044, val_acc:0.974]
Epoch [87/120    avg_loss:0.032, val_acc:0.978]
Epoch [88/120    avg_loss:0.017, val_acc:0.988]
Epoch [89/120    avg_loss:0.072, val_acc:0.931]
Epoch [90/120    avg_loss:0.042, val_acc:0.968]
Epoch [91/120    avg_loss:0.036, val_acc:0.978]
Epoch [92/120    avg_loss:0.027, val_acc:0.982]
Epoch [93/120    avg_loss:0.028, val_acc:0.976]
Epoch [94/120    avg_loss:0.019, val_acc:0.978]
Epoch [95/120    avg_loss:0.019, val_acc:0.980]
Epoch [96/120    avg_loss:0.015, val_acc:0.980]
Epoch [97/120    avg_loss:0.015, val_acc:0.980]
Epoch [98/120    avg_loss:0.019, val_acc:0.980]
Epoch [99/120    avg_loss:0.016, val_acc:0.980]
Epoch [100/120    avg_loss:0.023, val_acc:0.982]
Epoch [101/120    avg_loss:0.013, val_acc:0.982]
Epoch [102/120    avg_loss:0.009, val_acc:0.982]
Epoch [103/120    avg_loss:0.019, val_acc:0.982]
Epoch [104/120    avg_loss:0.008, val_acc:0.982]
Epoch [105/120    avg_loss:0.012, val_acc:0.982]
Epoch [106/120    avg_loss:0.010, val_acc:0.982]
Epoch [107/120    avg_loss:0.014, val_acc:0.982]
Epoch [108/120    avg_loss:0.012, val_acc:0.982]
Epoch [109/120    avg_loss:0.029, val_acc:0.982]
Epoch [110/120    avg_loss:0.011, val_acc:0.982]
Epoch [111/120    avg_loss:0.013, val_acc:0.982]
Epoch [112/120    avg_loss:0.025, val_acc:0.982]
Epoch [113/120    avg_loss:0.015, val_acc:0.982]
Epoch [114/120    avg_loss:0.012, val_acc:0.982]
Epoch [115/120    avg_loss:0.016, val_acc:0.982]
Epoch [116/120    avg_loss:0.011, val_acc:0.982]
Epoch [117/120    avg_loss:0.013, val_acc:0.982]
Epoch [118/120    avg_loss:0.019, val_acc:0.982]
Epoch [119/120    avg_loss:0.014, val_acc:0.982]
Epoch [120/120    avg_loss:0.017, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 221   5   0   0   0   3   1   0   0   0   0]
 [  0   0   0   0 216   7   0   0   0   0   0   0   4   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2   0 204   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   9 444   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 1.         0.99541284 0.98004435 0.94736842 0.95532646
 0.99512195 0.98947368 0.99614891 0.99893276 1.         0.98554534
 0.9833887  1.        ]

Kappa:
0.9902669765941261
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f820f57d828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.036, val_acc:0.533]
Epoch [2/120    avg_loss:1.382, val_acc:0.738]
Epoch [3/120    avg_loss:1.044, val_acc:0.707]
Epoch [4/120    avg_loss:0.855, val_acc:0.742]
Epoch [5/120    avg_loss:0.856, val_acc:0.730]
Epoch [6/120    avg_loss:0.799, val_acc:0.764]
Epoch [7/120    avg_loss:0.725, val_acc:0.762]
Epoch [8/120    avg_loss:0.743, val_acc:0.809]
Epoch [9/120    avg_loss:0.618, val_acc:0.852]
Epoch [10/120    avg_loss:0.542, val_acc:0.771]
Epoch [11/120    avg_loss:0.571, val_acc:0.775]
Epoch [12/120    avg_loss:0.613, val_acc:0.871]
Epoch [13/120    avg_loss:0.548, val_acc:0.846]
Epoch [14/120    avg_loss:0.544, val_acc:0.873]
Epoch [15/120    avg_loss:0.425, val_acc:0.902]
Epoch [16/120    avg_loss:0.452, val_acc:0.904]
Epoch [17/120    avg_loss:0.468, val_acc:0.873]
Epoch [18/120    avg_loss:0.332, val_acc:0.889]
Epoch [19/120    avg_loss:0.414, val_acc:0.934]
Epoch [20/120    avg_loss:0.319, val_acc:0.908]
Epoch [21/120    avg_loss:0.392, val_acc:0.906]
Epoch [22/120    avg_loss:0.433, val_acc:0.916]
Epoch [23/120    avg_loss:0.277, val_acc:0.877]
Epoch [24/120    avg_loss:0.272, val_acc:0.918]
Epoch [25/120    avg_loss:0.370, val_acc:0.902]
Epoch [26/120    avg_loss:0.411, val_acc:0.928]
Epoch [27/120    avg_loss:0.257, val_acc:0.939]
Epoch [28/120    avg_loss:0.291, val_acc:0.910]
Epoch [29/120    avg_loss:0.289, val_acc:0.912]
Epoch [30/120    avg_loss:0.312, val_acc:0.941]
Epoch [31/120    avg_loss:0.233, val_acc:0.949]
Epoch [32/120    avg_loss:0.166, val_acc:0.939]
Epoch [33/120    avg_loss:0.283, val_acc:0.938]
Epoch [34/120    avg_loss:0.211, val_acc:0.945]
Epoch [35/120    avg_loss:0.257, val_acc:0.906]
Epoch [36/120    avg_loss:0.243, val_acc:0.934]
Epoch [37/120    avg_loss:0.220, val_acc:0.932]
Epoch [38/120    avg_loss:0.232, val_acc:0.949]
Epoch [39/120    avg_loss:0.184, val_acc:0.959]
Epoch [40/120    avg_loss:0.133, val_acc:0.932]
Epoch [41/120    avg_loss:0.111, val_acc:0.957]
Epoch [42/120    avg_loss:0.162, val_acc:0.920]
Epoch [43/120    avg_loss:0.110, val_acc:0.955]
Epoch [44/120    avg_loss:0.226, val_acc:0.926]
Epoch [45/120    avg_loss:0.196, val_acc:0.902]
Epoch [46/120    avg_loss:0.228, val_acc:0.953]
Epoch [47/120    avg_loss:0.172, val_acc:0.928]
Epoch [48/120    avg_loss:0.128, val_acc:0.951]
Epoch [49/120    avg_loss:0.146, val_acc:0.953]
Epoch [50/120    avg_loss:0.225, val_acc:0.961]
Epoch [51/120    avg_loss:0.156, val_acc:0.955]
Epoch [52/120    avg_loss:0.164, val_acc:0.898]
Epoch [53/120    avg_loss:0.107, val_acc:0.934]
Epoch [54/120    avg_loss:0.182, val_acc:0.955]
Epoch [55/120    avg_loss:0.176, val_acc:0.961]
Epoch [56/120    avg_loss:0.143, val_acc:0.934]
Epoch [57/120    avg_loss:0.106, val_acc:0.967]
Epoch [58/120    avg_loss:0.066, val_acc:0.955]
Epoch [59/120    avg_loss:0.134, val_acc:0.971]
Epoch [60/120    avg_loss:0.145, val_acc:0.963]
Epoch [61/120    avg_loss:0.136, val_acc:0.971]
Epoch [62/120    avg_loss:0.088, val_acc:0.965]
Epoch [63/120    avg_loss:0.083, val_acc:0.975]
Epoch [64/120    avg_loss:0.144, val_acc:0.971]
Epoch [65/120    avg_loss:0.125, val_acc:0.953]
Epoch [66/120    avg_loss:0.145, val_acc:0.973]
Epoch [67/120    avg_loss:0.108, val_acc:0.975]
Epoch [68/120    avg_loss:0.093, val_acc:0.977]
Epoch [69/120    avg_loss:0.071, val_acc:0.973]
Epoch [70/120    avg_loss:0.062, val_acc:0.967]
Epoch [71/120    avg_loss:0.050, val_acc:0.969]
Epoch [72/120    avg_loss:0.100, val_acc:0.965]
Epoch [73/120    avg_loss:0.080, val_acc:0.969]
Epoch [74/120    avg_loss:0.114, val_acc:0.959]
Epoch [75/120    avg_loss:0.103, val_acc:0.961]
Epoch [76/120    avg_loss:0.069, val_acc:0.965]
Epoch [77/120    avg_loss:0.038, val_acc:0.955]
Epoch [78/120    avg_loss:0.059, val_acc:0.975]
Epoch [79/120    avg_loss:0.135, val_acc:0.967]
Epoch [80/120    avg_loss:0.072, val_acc:0.955]
Epoch [81/120    avg_loss:0.075, val_acc:0.969]
Epoch [82/120    avg_loss:0.029, val_acc:0.977]
Epoch [83/120    avg_loss:0.021, val_acc:0.980]
Epoch [84/120    avg_loss:0.029, val_acc:0.979]
Epoch [85/120    avg_loss:0.023, val_acc:0.979]
Epoch [86/120    avg_loss:0.026, val_acc:0.977]
Epoch [87/120    avg_loss:0.031, val_acc:0.980]
Epoch [88/120    avg_loss:0.029, val_acc:0.980]
Epoch [89/120    avg_loss:0.026, val_acc:0.984]
Epoch [90/120    avg_loss:0.023, val_acc:0.984]
Epoch [91/120    avg_loss:0.018, val_acc:0.980]
Epoch [92/120    avg_loss:0.020, val_acc:0.986]
Epoch [93/120    avg_loss:0.030, val_acc:0.988]
Epoch [94/120    avg_loss:0.017, val_acc:0.986]
Epoch [95/120    avg_loss:0.022, val_acc:0.982]
Epoch [96/120    avg_loss:0.015, val_acc:0.984]
Epoch [97/120    avg_loss:0.021, val_acc:0.979]
Epoch [98/120    avg_loss:0.019, val_acc:0.982]
Epoch [99/120    avg_loss:0.018, val_acc:0.982]
Epoch [100/120    avg_loss:0.020, val_acc:0.982]
Epoch [101/120    avg_loss:0.025, val_acc:0.984]
Epoch [102/120    avg_loss:0.011, val_acc:0.986]
Epoch [103/120    avg_loss:0.018, val_acc:0.980]
Epoch [104/120    avg_loss:0.018, val_acc:0.980]
Epoch [105/120    avg_loss:0.019, val_acc:0.980]
Epoch [106/120    avg_loss:0.019, val_acc:0.979]
Epoch [107/120    avg_loss:0.016, val_acc:0.979]
Epoch [108/120    avg_loss:0.018, val_acc:0.980]
Epoch [109/120    avg_loss:0.018, val_acc:0.980]
Epoch [110/120    avg_loss:0.017, val_acc:0.980]
Epoch [111/120    avg_loss:0.017, val_acc:0.980]
Epoch [112/120    avg_loss:0.017, val_acc:0.980]
Epoch [113/120    avg_loss:0.015, val_acc:0.980]
Epoch [114/120    avg_loss:0.012, val_acc:0.980]
Epoch [115/120    avg_loss:0.019, val_acc:0.982]
Epoch [116/120    avg_loss:0.016, val_acc:0.984]
Epoch [117/120    avg_loss:0.018, val_acc:0.980]
Epoch [118/120    avg_loss:0.019, val_acc:0.980]
Epoch [119/120    avg_loss:0.012, val_acc:0.980]
Epoch [120/120    avg_loss:0.017, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 132   4   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 1.         0.98871332 0.99782135 0.95343681 0.91349481
 0.99038462 0.97826087 1.         0.99893276 1.         1.
 0.99889503 1.        ]

Kappa:
0.9926409879206718
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:14:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f97b647f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.051, val_acc:0.602]
Epoch [2/120    avg_loss:1.295, val_acc:0.742]
Epoch [3/120    avg_loss:1.007, val_acc:0.773]
Epoch [4/120    avg_loss:0.873, val_acc:0.787]
Epoch [5/120    avg_loss:0.841, val_acc:0.809]
Epoch [6/120    avg_loss:0.688, val_acc:0.822]
Epoch [7/120    avg_loss:0.705, val_acc:0.852]
Epoch [8/120    avg_loss:0.745, val_acc:0.857]
Epoch [9/120    avg_loss:0.596, val_acc:0.797]
Epoch [10/120    avg_loss:0.627, val_acc:0.867]
Epoch [11/120    avg_loss:0.544, val_acc:0.854]
Epoch [12/120    avg_loss:0.471, val_acc:0.896]
Epoch [13/120    avg_loss:0.467, val_acc:0.873]
Epoch [14/120    avg_loss:0.473, val_acc:0.898]
Epoch [15/120    avg_loss:0.505, val_acc:0.879]
Epoch [16/120    avg_loss:0.438, val_acc:0.869]
Epoch [17/120    avg_loss:0.430, val_acc:0.910]
Epoch [18/120    avg_loss:0.408, val_acc:0.895]
Epoch [19/120    avg_loss:0.423, val_acc:0.900]
Epoch [20/120    avg_loss:0.412, val_acc:0.895]
Epoch [21/120    avg_loss:0.370, val_acc:0.881]
Epoch [22/120    avg_loss:0.397, val_acc:0.908]
Epoch [23/120    avg_loss:0.336, val_acc:0.920]
Epoch [24/120    avg_loss:0.333, val_acc:0.918]
Epoch [25/120    avg_loss:0.367, val_acc:0.926]
Epoch [26/120    avg_loss:0.394, val_acc:0.918]
Epoch [27/120    avg_loss:0.379, val_acc:0.924]
Epoch [28/120    avg_loss:0.256, val_acc:0.904]
Epoch [29/120    avg_loss:0.293, val_acc:0.848]
Epoch [30/120    avg_loss:0.370, val_acc:0.914]
Epoch [31/120    avg_loss:0.266, val_acc:0.930]
Epoch [32/120    avg_loss:0.230, val_acc:0.932]
Epoch [33/120    avg_loss:0.205, val_acc:0.916]
Epoch [34/120    avg_loss:0.271, val_acc:0.865]
Epoch [35/120    avg_loss:0.203, val_acc:0.951]
Epoch [36/120    avg_loss:0.225, val_acc:0.912]
Epoch [37/120    avg_loss:0.236, val_acc:0.908]
Epoch [38/120    avg_loss:0.227, val_acc:0.959]
Epoch [39/120    avg_loss:0.298, val_acc:0.920]
Epoch [40/120    avg_loss:0.311, val_acc:0.928]
Epoch [41/120    avg_loss:0.160, val_acc:0.951]
Epoch [42/120    avg_loss:0.152, val_acc:0.957]
Epoch [43/120    avg_loss:0.175, val_acc:0.936]
Epoch [44/120    avg_loss:0.168, val_acc:0.961]
Epoch [45/120    avg_loss:0.244, val_acc:0.953]
Epoch [46/120    avg_loss:0.255, val_acc:0.947]
Epoch [47/120    avg_loss:0.157, val_acc:0.955]
Epoch [48/120    avg_loss:0.128, val_acc:0.953]
Epoch [49/120    avg_loss:0.161, val_acc:0.957]
Epoch [50/120    avg_loss:0.162, val_acc:0.928]
Epoch [51/120    avg_loss:0.127, val_acc:0.965]
Epoch [52/120    avg_loss:0.131, val_acc:0.934]
Epoch [53/120    avg_loss:0.182, val_acc:0.947]
Epoch [54/120    avg_loss:0.172, val_acc:0.900]
Epoch [55/120    avg_loss:0.206, val_acc:0.965]
Epoch [56/120    avg_loss:0.152, val_acc:0.939]
Epoch [57/120    avg_loss:0.108, val_acc:0.957]
Epoch [58/120    avg_loss:0.141, val_acc:0.967]
Epoch [59/120    avg_loss:0.109, val_acc:0.971]
Epoch [60/120    avg_loss:0.099, val_acc:0.973]
Epoch [61/120    avg_loss:0.131, val_acc:0.953]
Epoch [62/120    avg_loss:0.135, val_acc:0.969]
Epoch [63/120    avg_loss:0.120, val_acc:0.977]
Epoch [64/120    avg_loss:0.065, val_acc:0.926]
Epoch [65/120    avg_loss:0.094, val_acc:0.967]
Epoch [66/120    avg_loss:0.093, val_acc:0.916]
Epoch [67/120    avg_loss:0.113, val_acc:0.967]
Epoch [68/120    avg_loss:0.110, val_acc:0.977]
Epoch [69/120    avg_loss:0.153, val_acc:0.969]
Epoch [70/120    avg_loss:0.086, val_acc:0.979]
Epoch [71/120    avg_loss:0.087, val_acc:0.971]
Epoch [72/120    avg_loss:0.083, val_acc:0.973]
Epoch [73/120    avg_loss:0.121, val_acc:0.969]
Epoch [74/120    avg_loss:0.125, val_acc:0.936]
Epoch [75/120    avg_loss:0.212, val_acc:0.979]
Epoch [76/120    avg_loss:0.092, val_acc:0.977]
Epoch [77/120    avg_loss:0.093, val_acc:0.947]
Epoch [78/120    avg_loss:0.084, val_acc:0.973]
Epoch [79/120    avg_loss:0.053, val_acc:0.975]
Epoch [80/120    avg_loss:0.066, val_acc:0.980]
Epoch [81/120    avg_loss:0.121, val_acc:0.949]
Epoch [82/120    avg_loss:0.055, val_acc:0.977]
Epoch [83/120    avg_loss:0.054, val_acc:0.967]
Epoch [84/120    avg_loss:0.117, val_acc:0.969]
Epoch [85/120    avg_loss:0.066, val_acc:0.982]
Epoch [86/120    avg_loss:0.048, val_acc:0.982]
Epoch [87/120    avg_loss:0.023, val_acc:0.971]
Epoch [88/120    avg_loss:0.093, val_acc:0.980]
Epoch [89/120    avg_loss:0.077, val_acc:0.965]
Epoch [90/120    avg_loss:0.091, val_acc:0.977]
Epoch [91/120    avg_loss:0.054, val_acc:0.982]
Epoch [92/120    avg_loss:0.041, val_acc:0.973]
Epoch [93/120    avg_loss:0.030, val_acc:0.980]
Epoch [94/120    avg_loss:0.054, val_acc:0.969]
Epoch [95/120    avg_loss:0.088, val_acc:0.984]
Epoch [96/120    avg_loss:0.065, val_acc:0.977]
Epoch [97/120    avg_loss:0.046, val_acc:0.980]
Epoch [98/120    avg_loss:0.031, val_acc:0.984]
Epoch [99/120    avg_loss:0.029, val_acc:0.986]
Epoch [100/120    avg_loss:0.019, val_acc:0.986]
Epoch [101/120    avg_loss:0.023, val_acc:0.980]
Epoch [102/120    avg_loss:0.038, val_acc:0.971]
Epoch [103/120    avg_loss:0.015, val_acc:0.988]
Epoch [104/120    avg_loss:0.024, val_acc:0.990]
Epoch [105/120    avg_loss:0.026, val_acc:0.969]
Epoch [106/120    avg_loss:0.028, val_acc:0.990]
Epoch [107/120    avg_loss:0.016, val_acc:0.984]
Epoch [108/120    avg_loss:0.029, val_acc:0.959]
Epoch [109/120    avg_loss:0.047, val_acc:0.947]
Epoch [110/120    avg_loss:0.031, val_acc:0.982]
Epoch [111/120    avg_loss:0.023, val_acc:0.975]
Epoch [112/120    avg_loss:0.024, val_acc:0.984]
Epoch [113/120    avg_loss:0.022, val_acc:0.986]
Epoch [114/120    avg_loss:0.019, val_acc:0.980]
Epoch [115/120    avg_loss:0.019, val_acc:0.980]
Epoch [116/120    avg_loss:0.027, val_acc:0.977]
Epoch [117/120    avg_loss:0.009, val_acc:0.984]
Epoch [118/120    avg_loss:0.011, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 211  14   0   0   0   4   1   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  21 124   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.91257995735607

F1 scores:
[       nan 1.         0.9977221  0.9569161  0.90605428 0.88888889
 1.         1.         0.99487179 0.99893276 1.         1.
 0.99889503 1.        ]

Kappa:
0.9878926290463316
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe6c049c7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.005, val_acc:0.566]
Epoch [2/120    avg_loss:1.338, val_acc:0.699]
Epoch [3/120    avg_loss:1.067, val_acc:0.729]
Epoch [4/120    avg_loss:0.931, val_acc:0.764]
Epoch [5/120    avg_loss:0.765, val_acc:0.803]
Epoch [6/120    avg_loss:0.595, val_acc:0.820]
Epoch [7/120    avg_loss:0.630, val_acc:0.834]
Epoch [8/120    avg_loss:0.591, val_acc:0.840]
Epoch [9/120    avg_loss:0.601, val_acc:0.867]
Epoch [10/120    avg_loss:0.547, val_acc:0.859]
Epoch [11/120    avg_loss:0.539, val_acc:0.863]
Epoch [12/120    avg_loss:0.474, val_acc:0.867]
Epoch [13/120    avg_loss:0.421, val_acc:0.879]
Epoch [14/120    avg_loss:0.458, val_acc:0.891]
Epoch [15/120    avg_loss:0.392, val_acc:0.900]
Epoch [16/120    avg_loss:0.430, val_acc:0.893]
Epoch [17/120    avg_loss:0.382, val_acc:0.857]
Epoch [18/120    avg_loss:0.443, val_acc:0.906]
Epoch [19/120    avg_loss:0.350, val_acc:0.932]
Epoch [20/120    avg_loss:0.267, val_acc:0.898]
Epoch [21/120    avg_loss:0.365, val_acc:0.920]
Epoch [22/120    avg_loss:0.291, val_acc:0.941]
Epoch [23/120    avg_loss:0.443, val_acc:0.924]
Epoch [24/120    avg_loss:0.243, val_acc:0.934]
Epoch [25/120    avg_loss:0.339, val_acc:0.932]
Epoch [26/120    avg_loss:0.245, val_acc:0.932]
Epoch [27/120    avg_loss:0.219, val_acc:0.955]
Epoch [28/120    avg_loss:0.227, val_acc:0.947]
Epoch [29/120    avg_loss:0.190, val_acc:0.930]
Epoch [30/120    avg_loss:0.252, val_acc:0.963]
Epoch [31/120    avg_loss:0.184, val_acc:0.953]
Epoch [32/120    avg_loss:0.234, val_acc:0.959]
Epoch [33/120    avg_loss:0.252, val_acc:0.941]
Epoch [34/120    avg_loss:0.175, val_acc:0.936]
Epoch [35/120    avg_loss:0.120, val_acc:0.953]
Epoch [36/120    avg_loss:0.185, val_acc:0.949]
Epoch [37/120    avg_loss:0.162, val_acc:0.955]
Epoch [38/120    avg_loss:0.123, val_acc:0.957]
Epoch [39/120    avg_loss:0.158, val_acc:0.963]
Epoch [40/120    avg_loss:0.129, val_acc:0.977]
Epoch [41/120    avg_loss:0.112, val_acc:0.965]
Epoch [42/120    avg_loss:0.074, val_acc:0.977]
Epoch [43/120    avg_loss:0.115, val_acc:0.951]
Epoch [44/120    avg_loss:0.100, val_acc:0.957]
Epoch [45/120    avg_loss:0.121, val_acc:0.963]
Epoch [46/120    avg_loss:0.136, val_acc:0.967]
Epoch [47/120    avg_loss:0.125, val_acc:0.977]
Epoch [48/120    avg_loss:0.089, val_acc:0.971]
Epoch [49/120    avg_loss:0.070, val_acc:0.973]
Epoch [50/120    avg_loss:0.096, val_acc:0.979]
Epoch [51/120    avg_loss:0.084, val_acc:0.975]
Epoch [52/120    avg_loss:0.168, val_acc:0.965]
Epoch [53/120    avg_loss:0.133, val_acc:0.973]
Epoch [54/120    avg_loss:0.098, val_acc:0.963]
Epoch [55/120    avg_loss:0.103, val_acc:0.977]
Epoch [56/120    avg_loss:0.085, val_acc:0.973]
Epoch [57/120    avg_loss:0.121, val_acc:0.959]
Epoch [58/120    avg_loss:0.099, val_acc:0.979]
Epoch [59/120    avg_loss:0.112, val_acc:0.971]
Epoch [60/120    avg_loss:0.108, val_acc:0.973]
Epoch [61/120    avg_loss:0.057, val_acc:0.979]
Epoch [62/120    avg_loss:0.060, val_acc:0.975]
Epoch [63/120    avg_loss:0.032, val_acc:0.980]
Epoch [64/120    avg_loss:0.032, val_acc:0.980]
Epoch [65/120    avg_loss:0.041, val_acc:0.982]
Epoch [66/120    avg_loss:0.066, val_acc:0.977]
Epoch [67/120    avg_loss:0.115, val_acc:0.975]
Epoch [68/120    avg_loss:0.051, val_acc:0.977]
Epoch [69/120    avg_loss:0.093, val_acc:0.975]
Epoch [70/120    avg_loss:0.076, val_acc:0.971]
Epoch [71/120    avg_loss:0.043, val_acc:0.990]
Epoch [72/120    avg_loss:0.036, val_acc:0.982]
Epoch [73/120    avg_loss:0.041, val_acc:0.984]
Epoch [74/120    avg_loss:0.056, val_acc:0.984]
Epoch [75/120    avg_loss:0.043, val_acc:0.990]
Epoch [76/120    avg_loss:0.040, val_acc:0.984]
Epoch [77/120    avg_loss:0.052, val_acc:0.975]
Epoch [78/120    avg_loss:0.034, val_acc:0.986]
Epoch [79/120    avg_loss:0.062, val_acc:0.986]
Epoch [80/120    avg_loss:0.045, val_acc:0.967]
Epoch [81/120    avg_loss:0.036, val_acc:0.992]
Epoch [82/120    avg_loss:0.015, val_acc:0.992]
Epoch [83/120    avg_loss:0.018, val_acc:0.992]
Epoch [84/120    avg_loss:0.014, val_acc:0.988]
Epoch [85/120    avg_loss:0.011, val_acc:0.992]
Epoch [86/120    avg_loss:0.013, val_acc:0.996]
Epoch [87/120    avg_loss:0.009, val_acc:0.994]
Epoch [88/120    avg_loss:0.010, val_acc:0.988]
Epoch [89/120    avg_loss:0.010, val_acc:0.992]
Epoch [90/120    avg_loss:0.022, val_acc:0.990]
Epoch [91/120    avg_loss:0.018, val_acc:0.986]
Epoch [92/120    avg_loss:0.048, val_acc:0.980]
Epoch [93/120    avg_loss:0.028, val_acc:0.988]
Epoch [94/120    avg_loss:0.014, val_acc:0.986]
Epoch [95/120    avg_loss:0.035, val_acc:0.977]
Epoch [96/120    avg_loss:0.032, val_acc:0.986]
Epoch [97/120    avg_loss:0.088, val_acc:0.980]
Epoch [98/120    avg_loss:0.031, val_acc:0.984]
Epoch [99/120    avg_loss:0.047, val_acc:0.984]
Epoch [100/120    avg_loss:0.034, val_acc:0.986]
Epoch [101/120    avg_loss:0.023, val_acc:0.988]
Epoch [102/120    avg_loss:0.010, val_acc:0.986]
Epoch [103/120    avg_loss:0.010, val_acc:0.988]
Epoch [104/120    avg_loss:0.011, val_acc:0.988]
Epoch [105/120    avg_loss:0.013, val_acc:0.988]
Epoch [106/120    avg_loss:0.008, val_acc:0.992]
Epoch [107/120    avg_loss:0.011, val_acc:0.992]
Epoch [108/120    avg_loss:0.009, val_acc:0.992]
Epoch [109/120    avg_loss:0.016, val_acc:0.990]
Epoch [110/120    avg_loss:0.007, val_acc:0.992]
Epoch [111/120    avg_loss:0.010, val_acc:0.992]
Epoch [112/120    avg_loss:0.012, val_acc:0.992]
Epoch [113/120    avg_loss:0.009, val_acc:0.992]
Epoch [114/120    avg_loss:0.010, val_acc:0.992]
Epoch [115/120    avg_loss:0.013, val_acc:0.992]
Epoch [116/120    avg_loss:0.011, val_acc:0.992]
Epoch [117/120    avg_loss:0.008, val_acc:0.992]
Epoch [118/120    avg_loss:0.006, val_acc:0.992]
Epoch [119/120    avg_loss:0.006, val_acc:0.992]
Epoch [120/120    avg_loss:0.016, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   5   0   0   0   3   0   0   0   0   0]
 [  0   0   0   0 213  13   0   0   0   0   0   0   1   0]
 [  0   0   0   0   6 138   1   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0  10 443   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.08315565031982

F1 scores:
[       nan 1.         0.99319728 0.98230088 0.94456763 0.93243243
 0.99757869 0.98378378 0.99614891 1.         1.         0.98558322
 0.98663697 1.        ]

Kappa:
0.9897926682234531
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc5712aa828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.029, val_acc:0.653]
Epoch [2/120    avg_loss:1.323, val_acc:0.728]
Epoch [3/120    avg_loss:1.020, val_acc:0.780]
Epoch [4/120    avg_loss:0.874, val_acc:0.802]
Epoch [5/120    avg_loss:0.775, val_acc:0.802]
Epoch [6/120    avg_loss:0.742, val_acc:0.802]
Epoch [7/120    avg_loss:0.666, val_acc:0.833]
Epoch [8/120    avg_loss:0.606, val_acc:0.845]
Epoch [9/120    avg_loss:0.626, val_acc:0.867]
Epoch [10/120    avg_loss:0.659, val_acc:0.893]
Epoch [11/120    avg_loss:0.502, val_acc:0.899]
Epoch [12/120    avg_loss:0.465, val_acc:0.899]
Epoch [13/120    avg_loss:0.478, val_acc:0.871]
Epoch [14/120    avg_loss:0.490, val_acc:0.909]
Epoch [15/120    avg_loss:0.437, val_acc:0.901]
Epoch [16/120    avg_loss:0.419, val_acc:0.921]
Epoch [17/120    avg_loss:0.371, val_acc:0.903]
Epoch [18/120    avg_loss:0.378, val_acc:0.903]
Epoch [19/120    avg_loss:0.374, val_acc:0.903]
Epoch [20/120    avg_loss:0.329, val_acc:0.935]
Epoch [21/120    avg_loss:0.359, val_acc:0.905]
Epoch [22/120    avg_loss:0.339, val_acc:0.905]
Epoch [23/120    avg_loss:0.377, val_acc:0.919]
Epoch [24/120    avg_loss:0.393, val_acc:0.933]
Epoch [25/120    avg_loss:0.329, val_acc:0.950]
Epoch [26/120    avg_loss:0.278, val_acc:0.944]
Epoch [27/120    avg_loss:0.318, val_acc:0.929]
Epoch [28/120    avg_loss:0.249, val_acc:0.942]
Epoch [29/120    avg_loss:0.305, val_acc:0.954]
Epoch [30/120    avg_loss:0.321, val_acc:0.935]
Epoch [31/120    avg_loss:0.375, val_acc:0.935]
Epoch [32/120    avg_loss:0.298, val_acc:0.938]
Epoch [33/120    avg_loss:0.243, val_acc:0.925]
Epoch [34/120    avg_loss:0.241, val_acc:0.933]
Epoch [35/120    avg_loss:0.267, val_acc:0.958]
Epoch [36/120    avg_loss:0.245, val_acc:0.937]
Epoch [37/120    avg_loss:0.208, val_acc:0.970]
Epoch [38/120    avg_loss:0.182, val_acc:0.962]
Epoch [39/120    avg_loss:0.169, val_acc:0.968]
Epoch [40/120    avg_loss:0.192, val_acc:0.954]
Epoch [41/120    avg_loss:0.264, val_acc:0.962]
Epoch [42/120    avg_loss:0.175, val_acc:0.954]
Epoch [43/120    avg_loss:0.168, val_acc:0.960]
Epoch [44/120    avg_loss:0.171, val_acc:0.954]
Epoch [45/120    avg_loss:0.114, val_acc:0.960]
Epoch [46/120    avg_loss:0.226, val_acc:0.952]
Epoch [47/120    avg_loss:0.212, val_acc:0.935]
Epoch [48/120    avg_loss:0.192, val_acc:0.950]
Epoch [49/120    avg_loss:0.187, val_acc:0.966]
Epoch [50/120    avg_loss:0.143, val_acc:0.966]
Epoch [51/120    avg_loss:0.122, val_acc:0.976]
Epoch [52/120    avg_loss:0.075, val_acc:0.978]
Epoch [53/120    avg_loss:0.085, val_acc:0.974]
Epoch [54/120    avg_loss:0.075, val_acc:0.978]
Epoch [55/120    avg_loss:0.072, val_acc:0.980]
Epoch [56/120    avg_loss:0.075, val_acc:0.980]
Epoch [57/120    avg_loss:0.067, val_acc:0.982]
Epoch [58/120    avg_loss:0.071, val_acc:0.984]
Epoch [59/120    avg_loss:0.067, val_acc:0.978]
Epoch [60/120    avg_loss:0.074, val_acc:0.980]
Epoch [61/120    avg_loss:0.087, val_acc:0.982]
Epoch [62/120    avg_loss:0.071, val_acc:0.984]
Epoch [63/120    avg_loss:0.061, val_acc:0.984]
Epoch [64/120    avg_loss:0.064, val_acc:0.984]
Epoch [65/120    avg_loss:0.058, val_acc:0.982]
Epoch [66/120    avg_loss:0.051, val_acc:0.982]
Epoch [67/120    avg_loss:0.063, val_acc:0.986]
Epoch [68/120    avg_loss:0.050, val_acc:0.984]
Epoch [69/120    avg_loss:0.065, val_acc:0.984]
Epoch [70/120    avg_loss:0.065, val_acc:0.986]
Epoch [71/120    avg_loss:0.064, val_acc:0.984]
Epoch [72/120    avg_loss:0.049, val_acc:0.984]
Epoch [73/120    avg_loss:0.058, val_acc:0.984]
Epoch [74/120    avg_loss:0.068, val_acc:0.984]
Epoch [75/120    avg_loss:0.063, val_acc:0.984]
Epoch [76/120    avg_loss:0.052, val_acc:0.984]
Epoch [77/120    avg_loss:0.050, val_acc:0.986]
Epoch [78/120    avg_loss:0.056, val_acc:0.984]
Epoch [79/120    avg_loss:0.057, val_acc:0.984]
Epoch [80/120    avg_loss:0.043, val_acc:0.986]
Epoch [81/120    avg_loss:0.057, val_acc:0.984]
Epoch [82/120    avg_loss:0.051, val_acc:0.984]
Epoch [83/120    avg_loss:0.051, val_acc:0.984]
Epoch [84/120    avg_loss:0.054, val_acc:0.984]
Epoch [85/120    avg_loss:0.053, val_acc:0.986]
Epoch [86/120    avg_loss:0.058, val_acc:0.990]
Epoch [87/120    avg_loss:0.058, val_acc:0.992]
Epoch [88/120    avg_loss:0.047, val_acc:0.992]
Epoch [89/120    avg_loss:0.044, val_acc:0.992]
Epoch [90/120    avg_loss:0.053, val_acc:0.990]
Epoch [91/120    avg_loss:0.053, val_acc:0.988]
Epoch [92/120    avg_loss:0.047, val_acc:0.992]
Epoch [93/120    avg_loss:0.057, val_acc:0.992]
Epoch [94/120    avg_loss:0.051, val_acc:0.990]
Epoch [95/120    avg_loss:0.040, val_acc:0.990]
Epoch [96/120    avg_loss:0.046, val_acc:0.990]
Epoch [97/120    avg_loss:0.047, val_acc:0.990]
Epoch [98/120    avg_loss:0.038, val_acc:0.990]
Epoch [99/120    avg_loss:0.040, val_acc:0.990]
Epoch [100/120    avg_loss:0.047, val_acc:0.990]
Epoch [101/120    avg_loss:0.052, val_acc:0.992]
Epoch [102/120    avg_loss:0.057, val_acc:0.992]
Epoch [103/120    avg_loss:0.037, val_acc:0.986]
Epoch [104/120    avg_loss:0.039, val_acc:0.990]
Epoch [105/120    avg_loss:0.043, val_acc:0.988]
Epoch [106/120    avg_loss:0.043, val_acc:0.986]
Epoch [107/120    avg_loss:0.050, val_acc:0.994]
Epoch [108/120    avg_loss:0.046, val_acc:0.992]
Epoch [109/120    avg_loss:0.048, val_acc:0.992]
Epoch [110/120    avg_loss:0.044, val_acc:0.986]
Epoch [111/120    avg_loss:0.036, val_acc:0.990]
Epoch [112/120    avg_loss:0.035, val_acc:0.984]
Epoch [113/120    avg_loss:0.048, val_acc:0.984]
Epoch [114/120    avg_loss:0.045, val_acc:0.988]
Epoch [115/120    avg_loss:0.030, val_acc:0.988]
Epoch [116/120    avg_loss:0.039, val_acc:0.988]
Epoch [117/120    avg_loss:0.043, val_acc:0.990]
Epoch [118/120    avg_loss:0.056, val_acc:0.986]
Epoch [119/120    avg_loss:0.044, val_acc:0.992]
Epoch [120/120    avg_loss:0.042, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 1.         0.9752809  0.99782135 0.93626374 0.90344828
 1.         0.94505495 1.         1.         1.         0.99734043
 0.99669239 1.        ]

Kappa:
0.9900296433109703
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5a5856b828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.086, val_acc:0.641]
Epoch [2/120    avg_loss:1.411, val_acc:0.656]
Epoch [3/120    avg_loss:1.104, val_acc:0.750]
Epoch [4/120    avg_loss:0.900, val_acc:0.762]
Epoch [5/120    avg_loss:0.833, val_acc:0.793]
Epoch [6/120    avg_loss:0.740, val_acc:0.791]
Epoch [7/120    avg_loss:0.728, val_acc:0.730]
Epoch [8/120    avg_loss:0.678, val_acc:0.820]
Epoch [9/120    avg_loss:0.705, val_acc:0.859]
Epoch [10/120    avg_loss:0.612, val_acc:0.871]
Epoch [11/120    avg_loss:0.534, val_acc:0.885]
Epoch [12/120    avg_loss:0.507, val_acc:0.867]
Epoch [13/120    avg_loss:0.536, val_acc:0.809]
Epoch [14/120    avg_loss:0.519, val_acc:0.889]
Epoch [15/120    avg_loss:0.437, val_acc:0.893]
Epoch [16/120    avg_loss:0.459, val_acc:0.895]
Epoch [17/120    avg_loss:0.375, val_acc:0.898]
Epoch [18/120    avg_loss:0.378, val_acc:0.879]
Epoch [19/120    avg_loss:0.346, val_acc:0.896]
Epoch [20/120    avg_loss:0.325, val_acc:0.900]
Epoch [21/120    avg_loss:0.389, val_acc:0.881]
Epoch [22/120    avg_loss:0.389, val_acc:0.926]
Epoch [23/120    avg_loss:0.416, val_acc:0.941]
Epoch [24/120    avg_loss:0.317, val_acc:0.959]
Epoch [25/120    avg_loss:0.288, val_acc:0.941]
Epoch [26/120    avg_loss:0.262, val_acc:0.943]
Epoch [27/120    avg_loss:0.249, val_acc:0.932]
Epoch [28/120    avg_loss:0.219, val_acc:0.965]
Epoch [29/120    avg_loss:0.305, val_acc:0.936]
Epoch [30/120    avg_loss:0.219, val_acc:0.953]
Epoch [31/120    avg_loss:0.214, val_acc:0.955]
Epoch [32/120    avg_loss:0.234, val_acc:0.947]
Epoch [33/120    avg_loss:0.263, val_acc:0.928]
Epoch [34/120    avg_loss:0.226, val_acc:0.938]
Epoch [35/120    avg_loss:0.191, val_acc:0.949]
Epoch [36/120    avg_loss:0.180, val_acc:0.957]
Epoch [37/120    avg_loss:0.152, val_acc:0.980]
Epoch [38/120    avg_loss:0.195, val_acc:0.959]
Epoch [39/120    avg_loss:0.177, val_acc:0.951]
Epoch [40/120    avg_loss:0.137, val_acc:0.943]
Epoch [41/120    avg_loss:0.219, val_acc:0.953]
Epoch [42/120    avg_loss:0.156, val_acc:0.977]
Epoch [43/120    avg_loss:0.128, val_acc:0.977]
Epoch [44/120    avg_loss:0.159, val_acc:0.955]
Epoch [45/120    avg_loss:0.119, val_acc:0.980]
Epoch [46/120    avg_loss:0.104, val_acc:0.965]
Epoch [47/120    avg_loss:0.098, val_acc:0.959]
Epoch [48/120    avg_loss:0.137, val_acc:0.980]
Epoch [49/120    avg_loss:0.094, val_acc:0.975]
Epoch [50/120    avg_loss:0.082, val_acc:0.969]
Epoch [51/120    avg_loss:0.096, val_acc:0.979]
Epoch [52/120    avg_loss:0.107, val_acc:0.973]
Epoch [53/120    avg_loss:0.096, val_acc:0.957]
Epoch [54/120    avg_loss:0.059, val_acc:0.982]
Epoch [55/120    avg_loss:0.048, val_acc:0.973]
Epoch [56/120    avg_loss:0.071, val_acc:0.971]
Epoch [57/120    avg_loss:0.083, val_acc:0.975]
Epoch [58/120    avg_loss:0.117, val_acc:0.973]
Epoch [59/120    avg_loss:0.103, val_acc:0.951]
Epoch [60/120    avg_loss:0.124, val_acc:0.982]
Epoch [61/120    avg_loss:0.105, val_acc:0.975]
Epoch [62/120    avg_loss:0.076, val_acc:0.971]
Epoch [63/120    avg_loss:0.116, val_acc:0.967]
Epoch [64/120    avg_loss:0.166, val_acc:0.965]
Epoch [65/120    avg_loss:0.054, val_acc:0.945]
Epoch [66/120    avg_loss:0.071, val_acc:0.977]
Epoch [67/120    avg_loss:0.058, val_acc:0.961]
Epoch [68/120    avg_loss:0.095, val_acc:0.980]
Epoch [69/120    avg_loss:0.057, val_acc:0.975]
Epoch [70/120    avg_loss:0.039, val_acc:0.977]
Epoch [71/120    avg_loss:0.079, val_acc:0.980]
Epoch [72/120    avg_loss:0.043, val_acc:0.967]
Epoch [73/120    avg_loss:0.040, val_acc:0.986]
Epoch [74/120    avg_loss:0.026, val_acc:0.986]
Epoch [75/120    avg_loss:0.048, val_acc:0.984]
Epoch [76/120    avg_loss:0.064, val_acc:0.979]
Epoch [77/120    avg_loss:0.058, val_acc:0.953]
Epoch [78/120    avg_loss:0.065, val_acc:0.971]
Epoch [79/120    avg_loss:0.033, val_acc:0.969]
Epoch [80/120    avg_loss:0.052, val_acc:0.988]
Epoch [81/120    avg_loss:0.031, val_acc:0.988]
Epoch [82/120    avg_loss:0.026, val_acc:0.977]
Epoch [83/120    avg_loss:0.033, val_acc:0.984]
Epoch [84/120    avg_loss:0.024, val_acc:0.986]
Epoch [85/120    avg_loss:0.010, val_acc:0.984]
Epoch [86/120    avg_loss:0.022, val_acc:0.979]
Epoch [87/120    avg_loss:0.017, val_acc:0.988]
Epoch [88/120    avg_loss:0.010, val_acc:0.988]
Epoch [89/120    avg_loss:0.011, val_acc:0.992]
Epoch [90/120    avg_loss:0.009, val_acc:0.988]
Epoch [91/120    avg_loss:0.009, val_acc:0.984]
Epoch [92/120    avg_loss:0.018, val_acc:0.988]
Epoch [93/120    avg_loss:0.025, val_acc:0.986]
Epoch [94/120    avg_loss:0.022, val_acc:0.984]
Epoch [95/120    avg_loss:0.010, val_acc:0.990]
Epoch [96/120    avg_loss:0.009, val_acc:0.990]
Epoch [97/120    avg_loss:0.035, val_acc:0.984]
Epoch [98/120    avg_loss:0.021, val_acc:0.980]
Epoch [99/120    avg_loss:0.020, val_acc:0.984]
Epoch [100/120    avg_loss:0.092, val_acc:0.965]
Epoch [101/120    avg_loss:0.057, val_acc:0.992]
Epoch [102/120    avg_loss:0.039, val_acc:0.980]
Epoch [103/120    avg_loss:0.032, val_acc:0.980]
Epoch [104/120    avg_loss:0.028, val_acc:0.986]
Epoch [105/120    avg_loss:0.024, val_acc:0.961]
Epoch [106/120    avg_loss:0.029, val_acc:0.979]
Epoch [107/120    avg_loss:0.031, val_acc:0.959]
Epoch [108/120    avg_loss:0.053, val_acc:0.953]
Epoch [109/120    avg_loss:0.114, val_acc:0.980]
Epoch [110/120    avg_loss:0.017, val_acc:0.980]
Epoch [111/120    avg_loss:0.048, val_acc:0.969]
Epoch [112/120    avg_loss:0.051, val_acc:0.988]
Epoch [113/120    avg_loss:0.047, val_acc:0.951]
Epoch [114/120    avg_loss:0.033, val_acc:0.990]
Epoch [115/120    avg_loss:0.011, val_acc:0.990]
Epoch [116/120    avg_loss:0.014, val_acc:0.990]
Epoch [117/120    avg_loss:0.007, val_acc:0.990]
Epoch [118/120    avg_loss:0.011, val_acc:0.990]
Epoch [119/120    avg_loss:0.011, val_acc:0.990]
Epoch [120/120    avg_loss:0.008, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 226   0   0   0   0   4   0   0   0   0   0]
 [  0   0   0   0 206  15   0   0   0   0   0   0   6   0]
 [  0   0   0   0  17 127   1   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 1.         0.99541284 0.99122807 0.91555556 0.88501742
 0.99757869 0.98947368 0.99487179 1.         1.         1.
 0.99342105 1.        ]

Kappa:
0.9893167730914495
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6637b8c7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.021, val_acc:0.611]
Epoch [2/120    avg_loss:1.312, val_acc:0.712]
Epoch [3/120    avg_loss:0.973, val_acc:0.645]
Epoch [4/120    avg_loss:0.845, val_acc:0.821]
Epoch [5/120    avg_loss:0.747, val_acc:0.835]
Epoch [6/120    avg_loss:0.693, val_acc:0.845]
Epoch [7/120    avg_loss:0.697, val_acc:0.851]
Epoch [8/120    avg_loss:0.630, val_acc:0.905]
Epoch [9/120    avg_loss:0.552, val_acc:0.853]
Epoch [10/120    avg_loss:0.490, val_acc:0.831]
Epoch [11/120    avg_loss:0.484, val_acc:0.889]
Epoch [12/120    avg_loss:0.474, val_acc:0.879]
Epoch [13/120    avg_loss:0.457, val_acc:0.875]
Epoch [14/120    avg_loss:0.338, val_acc:0.891]
Epoch [15/120    avg_loss:0.455, val_acc:0.911]
Epoch [16/120    avg_loss:0.319, val_acc:0.921]
Epoch [17/120    avg_loss:0.466, val_acc:0.877]
Epoch [18/120    avg_loss:0.415, val_acc:0.883]
Epoch [19/120    avg_loss:0.401, val_acc:0.923]
Epoch [20/120    avg_loss:0.272, val_acc:0.893]
Epoch [21/120    avg_loss:0.331, val_acc:0.923]
Epoch [22/120    avg_loss:0.327, val_acc:0.909]
Epoch [23/120    avg_loss:0.281, val_acc:0.956]
Epoch [24/120    avg_loss:0.295, val_acc:0.901]
Epoch [25/120    avg_loss:0.270, val_acc:0.942]
Epoch [26/120    avg_loss:0.364, val_acc:0.923]
Epoch [27/120    avg_loss:0.204, val_acc:0.942]
Epoch [28/120    avg_loss:0.242, val_acc:0.966]
Epoch [29/120    avg_loss:0.255, val_acc:0.948]
Epoch [30/120    avg_loss:0.237, val_acc:0.942]
Epoch [31/120    avg_loss:0.266, val_acc:0.946]
Epoch [32/120    avg_loss:0.262, val_acc:0.931]
Epoch [33/120    avg_loss:0.191, val_acc:0.960]
Epoch [34/120    avg_loss:0.234, val_acc:0.948]
Epoch [35/120    avg_loss:0.194, val_acc:0.962]
Epoch [36/120    avg_loss:0.192, val_acc:0.841]
Epoch [37/120    avg_loss:0.251, val_acc:0.940]
Epoch [38/120    avg_loss:0.168, val_acc:0.935]
Epoch [39/120    avg_loss:0.114, val_acc:0.919]
Epoch [40/120    avg_loss:0.127, val_acc:0.956]
Epoch [41/120    avg_loss:0.154, val_acc:0.921]
Epoch [42/120    avg_loss:0.203, val_acc:0.974]
Epoch [43/120    avg_loss:0.094, val_acc:0.978]
Epoch [44/120    avg_loss:0.094, val_acc:0.982]
Epoch [45/120    avg_loss:0.062, val_acc:0.980]
Epoch [46/120    avg_loss:0.065, val_acc:0.980]
Epoch [47/120    avg_loss:0.069, val_acc:0.982]
Epoch [48/120    avg_loss:0.057, val_acc:0.982]
Epoch [49/120    avg_loss:0.071, val_acc:0.986]
Epoch [50/120    avg_loss:0.061, val_acc:0.982]
Epoch [51/120    avg_loss:0.074, val_acc:0.982]
Epoch [52/120    avg_loss:0.062, val_acc:0.982]
Epoch [53/120    avg_loss:0.069, val_acc:0.984]
Epoch [54/120    avg_loss:0.064, val_acc:0.984]
Epoch [55/120    avg_loss:0.063, val_acc:0.984]
Epoch [56/120    avg_loss:0.051, val_acc:0.986]
Epoch [57/120    avg_loss:0.044, val_acc:0.984]
Epoch [58/120    avg_loss:0.045, val_acc:0.984]
Epoch [59/120    avg_loss:0.058, val_acc:0.982]
Epoch [60/120    avg_loss:0.048, val_acc:0.984]
Epoch [61/120    avg_loss:0.051, val_acc:0.984]
Epoch [62/120    avg_loss:0.062, val_acc:0.984]
Epoch [63/120    avg_loss:0.065, val_acc:0.982]
Epoch [64/120    avg_loss:0.047, val_acc:0.982]
Epoch [65/120    avg_loss:0.038, val_acc:0.984]
Epoch [66/120    avg_loss:0.039, val_acc:0.984]
Epoch [67/120    avg_loss:0.045, val_acc:0.984]
Epoch [68/120    avg_loss:0.058, val_acc:0.982]
Epoch [69/120    avg_loss:0.063, val_acc:0.982]
Epoch [70/120    avg_loss:0.052, val_acc:0.982]
Epoch [71/120    avg_loss:0.049, val_acc:0.982]
Epoch [72/120    avg_loss:0.046, val_acc:0.984]
Epoch [73/120    avg_loss:0.048, val_acc:0.984]
Epoch [74/120    avg_loss:0.035, val_acc:0.984]
Epoch [75/120    avg_loss:0.047, val_acc:0.984]
Epoch [76/120    avg_loss:0.051, val_acc:0.984]
Epoch [77/120    avg_loss:0.042, val_acc:0.984]
Epoch [78/120    avg_loss:0.047, val_acc:0.984]
Epoch [79/120    avg_loss:0.054, val_acc:0.984]
Epoch [80/120    avg_loss:0.038, val_acc:0.984]
Epoch [81/120    avg_loss:0.042, val_acc:0.984]
Epoch [82/120    avg_loss:0.059, val_acc:0.984]
Epoch [83/120    avg_loss:0.057, val_acc:0.984]
Epoch [84/120    avg_loss:0.047, val_acc:0.984]
Epoch [85/120    avg_loss:0.051, val_acc:0.984]
Epoch [86/120    avg_loss:0.058, val_acc:0.984]
Epoch [87/120    avg_loss:0.044, val_acc:0.984]
Epoch [88/120    avg_loss:0.038, val_acc:0.984]
Epoch [89/120    avg_loss:0.046, val_acc:0.984]
Epoch [90/120    avg_loss:0.053, val_acc:0.984]
Epoch [91/120    avg_loss:0.045, val_acc:0.984]
Epoch [92/120    avg_loss:0.049, val_acc:0.984]
Epoch [93/120    avg_loss:0.039, val_acc:0.984]
Epoch [94/120    avg_loss:0.045, val_acc:0.984]
Epoch [95/120    avg_loss:0.049, val_acc:0.984]
Epoch [96/120    avg_loss:0.048, val_acc:0.984]
Epoch [97/120    avg_loss:0.037, val_acc:0.984]
Epoch [98/120    avg_loss:0.052, val_acc:0.984]
Epoch [99/120    avg_loss:0.040, val_acc:0.984]
Epoch [100/120    avg_loss:0.043, val_acc:0.984]
Epoch [101/120    avg_loss:0.027, val_acc:0.984]
Epoch [102/120    avg_loss:0.041, val_acc:0.984]
Epoch [103/120    avg_loss:0.046, val_acc:0.984]
Epoch [104/120    avg_loss:0.040, val_acc:0.984]
Epoch [105/120    avg_loss:0.048, val_acc:0.984]
Epoch [106/120    avg_loss:0.053, val_acc:0.984]
Epoch [107/120    avg_loss:0.042, val_acc:0.984]
Epoch [108/120    avg_loss:0.049, val_acc:0.984]
Epoch [109/120    avg_loss:0.046, val_acc:0.984]
Epoch [110/120    avg_loss:0.033, val_acc:0.984]
Epoch [111/120    avg_loss:0.050, val_acc:0.984]
Epoch [112/120    avg_loss:0.047, val_acc:0.984]
Epoch [113/120    avg_loss:0.055, val_acc:0.984]
Epoch [114/120    avg_loss:0.039, val_acc:0.984]
Epoch [115/120    avg_loss:0.041, val_acc:0.984]
Epoch [116/120    avg_loss:0.045, val_acc:0.984]
Epoch [117/120    avg_loss:0.057, val_acc:0.984]
Epoch [118/120    avg_loss:0.045, val_acc:0.984]
Epoch [119/120    avg_loss:0.044, val_acc:0.984]
Epoch [120/120    avg_loss:0.039, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 206  20   0   0   0   0   0   0   1   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 1.         0.98206278 1.         0.94930876 0.93203883
 1.         0.95555556 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9928786760228817
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7a58d5e7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.064, val_acc:0.619]
Epoch [2/120    avg_loss:1.290, val_acc:0.730]
Epoch [3/120    avg_loss:1.025, val_acc:0.708]
Epoch [4/120    avg_loss:0.826, val_acc:0.692]
Epoch [5/120    avg_loss:0.795, val_acc:0.796]
Epoch [6/120    avg_loss:0.742, val_acc:0.798]
Epoch [7/120    avg_loss:0.681, val_acc:0.821]
Epoch [8/120    avg_loss:0.561, val_acc:0.829]
Epoch [9/120    avg_loss:0.565, val_acc:0.843]
Epoch [10/120    avg_loss:0.560, val_acc:0.871]
Epoch [11/120    avg_loss:0.450, val_acc:0.857]
Epoch [12/120    avg_loss:0.633, val_acc:0.843]
Epoch [13/120    avg_loss:0.510, val_acc:0.849]
Epoch [14/120    avg_loss:0.449, val_acc:0.889]
Epoch [15/120    avg_loss:0.372, val_acc:0.919]
Epoch [16/120    avg_loss:0.345, val_acc:0.869]
Epoch [17/120    avg_loss:0.400, val_acc:0.871]
Epoch [18/120    avg_loss:0.400, val_acc:0.905]
Epoch [19/120    avg_loss:0.293, val_acc:0.891]
Epoch [20/120    avg_loss:0.305, val_acc:0.885]
Epoch [21/120    avg_loss:0.304, val_acc:0.925]
Epoch [22/120    avg_loss:0.227, val_acc:0.919]
Epoch [23/120    avg_loss:0.217, val_acc:0.905]
Epoch [24/120    avg_loss:0.321, val_acc:0.895]
Epoch [25/120    avg_loss:0.331, val_acc:0.885]
Epoch [26/120    avg_loss:0.307, val_acc:0.933]
Epoch [27/120    avg_loss:0.251, val_acc:0.933]
Epoch [28/120    avg_loss:0.210, val_acc:0.946]
Epoch [29/120    avg_loss:0.318, val_acc:0.921]
Epoch [30/120    avg_loss:0.279, val_acc:0.935]
Epoch [31/120    avg_loss:0.160, val_acc:0.960]
Epoch [32/120    avg_loss:0.220, val_acc:0.952]
Epoch [33/120    avg_loss:0.190, val_acc:0.960]
Epoch [34/120    avg_loss:0.161, val_acc:0.970]
Epoch [35/120    avg_loss:0.144, val_acc:0.942]
Epoch [36/120    avg_loss:0.300, val_acc:0.948]
Epoch [37/120    avg_loss:0.167, val_acc:0.970]
Epoch [38/120    avg_loss:0.112, val_acc:0.964]
Epoch [39/120    avg_loss:0.109, val_acc:0.964]
Epoch [40/120    avg_loss:0.091, val_acc:0.968]
Epoch [41/120    avg_loss:0.127, val_acc:0.972]
Epoch [42/120    avg_loss:0.158, val_acc:0.948]
Epoch [43/120    avg_loss:0.090, val_acc:0.972]
Epoch [44/120    avg_loss:0.073, val_acc:0.968]
Epoch [45/120    avg_loss:0.147, val_acc:0.960]
Epoch [46/120    avg_loss:0.083, val_acc:0.958]
Epoch [47/120    avg_loss:0.060, val_acc:0.978]
Epoch [48/120    avg_loss:0.076, val_acc:0.960]
Epoch [49/120    avg_loss:0.120, val_acc:0.958]
Epoch [50/120    avg_loss:0.071, val_acc:0.966]
Epoch [51/120    avg_loss:0.054, val_acc:0.964]
Epoch [52/120    avg_loss:0.145, val_acc:0.956]
Epoch [53/120    avg_loss:0.106, val_acc:0.960]
Epoch [54/120    avg_loss:0.047, val_acc:0.982]
Epoch [55/120    avg_loss:0.047, val_acc:0.982]
Epoch [56/120    avg_loss:0.100, val_acc:0.964]
Epoch [57/120    avg_loss:0.052, val_acc:0.982]
Epoch [58/120    avg_loss:0.062, val_acc:0.976]
Epoch [59/120    avg_loss:0.078, val_acc:0.982]
Epoch [60/120    avg_loss:0.063, val_acc:0.966]
Epoch [61/120    avg_loss:0.030, val_acc:0.974]
Epoch [62/120    avg_loss:0.035, val_acc:0.974]
Epoch [63/120    avg_loss:0.077, val_acc:0.970]
Epoch [64/120    avg_loss:0.118, val_acc:0.974]
Epoch [65/120    avg_loss:0.082, val_acc:0.952]
Epoch [66/120    avg_loss:0.053, val_acc:0.960]
Epoch [67/120    avg_loss:0.039, val_acc:0.986]
Epoch [68/120    avg_loss:0.027, val_acc:0.990]
Epoch [69/120    avg_loss:0.035, val_acc:0.982]
Epoch [70/120    avg_loss:0.031, val_acc:0.974]
Epoch [71/120    avg_loss:0.047, val_acc:0.980]
Epoch [72/120    avg_loss:0.056, val_acc:0.964]
Epoch [73/120    avg_loss:0.065, val_acc:0.984]
Epoch [74/120    avg_loss:0.025, val_acc:0.986]
Epoch [75/120    avg_loss:0.024, val_acc:0.986]
Epoch [76/120    avg_loss:0.027, val_acc:0.980]
Epoch [77/120    avg_loss:0.025, val_acc:0.990]
Epoch [78/120    avg_loss:0.029, val_acc:0.978]
Epoch [79/120    avg_loss:0.014, val_acc:0.988]
Epoch [80/120    avg_loss:0.012, val_acc:0.984]
Epoch [81/120    avg_loss:0.018, val_acc:0.988]
Epoch [82/120    avg_loss:0.012, val_acc:0.986]
Epoch [83/120    avg_loss:0.021, val_acc:0.978]
Epoch [84/120    avg_loss:0.010, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.988]
Epoch [86/120    avg_loss:0.007, val_acc:0.988]
Epoch [87/120    avg_loss:0.005, val_acc:0.988]
Epoch [88/120    avg_loss:0.016, val_acc:0.988]
Epoch [89/120    avg_loss:0.044, val_acc:0.976]
Epoch [90/120    avg_loss:0.081, val_acc:0.982]
Epoch [91/120    avg_loss:0.023, val_acc:0.984]
Epoch [92/120    avg_loss:0.018, val_acc:0.986]
Epoch [93/120    avg_loss:0.018, val_acc:0.986]
Epoch [94/120    avg_loss:0.017, val_acc:0.988]
Epoch [95/120    avg_loss:0.013, val_acc:0.988]
Epoch [96/120    avg_loss:0.010, val_acc:0.988]
Epoch [97/120    avg_loss:0.011, val_acc:0.988]
Epoch [98/120    avg_loss:0.008, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.988]
Epoch [100/120    avg_loss:0.014, val_acc:0.988]
Epoch [101/120    avg_loss:0.018, val_acc:0.988]
Epoch [102/120    avg_loss:0.008, val_acc:0.988]
Epoch [103/120    avg_loss:0.012, val_acc:0.988]
Epoch [104/120    avg_loss:0.009, val_acc:0.988]
Epoch [105/120    avg_loss:0.006, val_acc:0.988]
Epoch [106/120    avg_loss:0.011, val_acc:0.988]
Epoch [107/120    avg_loss:0.013, val_acc:0.988]
Epoch [108/120    avg_loss:0.014, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.009, val_acc:0.988]
Epoch [111/120    avg_loss:0.010, val_acc:0.988]
Epoch [112/120    avg_loss:0.009, val_acc:0.988]
Epoch [113/120    avg_loss:0.010, val_acc:0.988]
Epoch [114/120    avg_loss:0.008, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.007, val_acc:0.988]
Epoch [117/120    avg_loss:0.010, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   0   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 208  16   0   0   0   0   0   0   3   0]
 [  0   0   0   0   5 133   7   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.9977221  0.99563319 0.94545455 0.9047619
 0.98329356 0.99465241 0.99742931 1.         1.         1.
 0.99669967 1.        ]

Kappa:
0.9919287674500037
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7eff002d1828>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.003, val_acc:0.685]
Epoch [2/120    avg_loss:1.321, val_acc:0.675]
Epoch [3/120    avg_loss:1.051, val_acc:0.780]
Epoch [4/120    avg_loss:0.813, val_acc:0.784]
Epoch [5/120    avg_loss:0.759, val_acc:0.706]
Epoch [6/120    avg_loss:0.735, val_acc:0.825]
Epoch [7/120    avg_loss:0.659, val_acc:0.839]
Epoch [8/120    avg_loss:0.688, val_acc:0.766]
Epoch [9/120    avg_loss:0.572, val_acc:0.812]
Epoch [10/120    avg_loss:0.670, val_acc:0.821]
Epoch [11/120    avg_loss:0.504, val_acc:0.837]
Epoch [12/120    avg_loss:0.540, val_acc:0.907]
Epoch [13/120    avg_loss:0.399, val_acc:0.897]
Epoch [14/120    avg_loss:0.380, val_acc:0.919]
Epoch [15/120    avg_loss:0.436, val_acc:0.911]
Epoch [16/120    avg_loss:0.387, val_acc:0.937]
Epoch [17/120    avg_loss:0.339, val_acc:0.909]
Epoch [18/120    avg_loss:0.335, val_acc:0.899]
Epoch [19/120    avg_loss:0.366, val_acc:0.893]
Epoch [20/120    avg_loss:0.298, val_acc:0.942]
Epoch [21/120    avg_loss:0.262, val_acc:0.899]
Epoch [22/120    avg_loss:0.265, val_acc:0.938]
Epoch [23/120    avg_loss:0.287, val_acc:0.944]
Epoch [24/120    avg_loss:0.263, val_acc:0.942]
Epoch [25/120    avg_loss:0.213, val_acc:0.927]
Epoch [26/120    avg_loss:0.233, val_acc:0.956]
Epoch [27/120    avg_loss:0.195, val_acc:0.962]
Epoch [28/120    avg_loss:0.212, val_acc:0.966]
Epoch [29/120    avg_loss:0.186, val_acc:0.948]
Epoch [30/120    avg_loss:0.137, val_acc:0.952]
Epoch [31/120    avg_loss:0.124, val_acc:0.964]
Epoch [32/120    avg_loss:0.208, val_acc:0.972]
Epoch [33/120    avg_loss:0.144, val_acc:0.988]
Epoch [34/120    avg_loss:0.162, val_acc:0.964]
Epoch [35/120    avg_loss:0.141, val_acc:0.982]
Epoch [36/120    avg_loss:0.140, val_acc:0.964]
Epoch [37/120    avg_loss:0.127, val_acc:0.931]
Epoch [38/120    avg_loss:0.083, val_acc:0.986]
Epoch [39/120    avg_loss:0.105, val_acc:0.966]
Epoch [40/120    avg_loss:0.165, val_acc:0.962]
Epoch [41/120    avg_loss:0.147, val_acc:0.966]
Epoch [42/120    avg_loss:0.085, val_acc:0.978]
Epoch [43/120    avg_loss:0.172, val_acc:0.931]
Epoch [44/120    avg_loss:0.132, val_acc:0.980]
Epoch [45/120    avg_loss:0.056, val_acc:0.992]
Epoch [46/120    avg_loss:0.061, val_acc:0.992]
Epoch [47/120    avg_loss:0.036, val_acc:0.976]
Epoch [48/120    avg_loss:0.053, val_acc:0.974]
Epoch [49/120    avg_loss:0.054, val_acc:0.986]
Epoch [50/120    avg_loss:0.040, val_acc:0.958]
Epoch [51/120    avg_loss:0.055, val_acc:0.986]
Epoch [52/120    avg_loss:0.046, val_acc:0.962]
Epoch [53/120    avg_loss:0.055, val_acc:0.994]
Epoch [54/120    avg_loss:0.038, val_acc:0.972]
Epoch [55/120    avg_loss:0.040, val_acc:0.986]
Epoch [56/120    avg_loss:0.041, val_acc:0.990]
Epoch [57/120    avg_loss:0.035, val_acc:0.986]
Epoch [58/120    avg_loss:0.042, val_acc:0.988]
Epoch [59/120    avg_loss:0.027, val_acc:0.992]
Epoch [60/120    avg_loss:0.020, val_acc:0.988]
Epoch [61/120    avg_loss:0.028, val_acc:0.984]
Epoch [62/120    avg_loss:0.135, val_acc:0.976]
Epoch [63/120    avg_loss:0.073, val_acc:0.982]
Epoch [64/120    avg_loss:0.036, val_acc:0.984]
Epoch [65/120    avg_loss:0.022, val_acc:0.986]
Epoch [66/120    avg_loss:0.024, val_acc:0.988]
Epoch [67/120    avg_loss:0.017, val_acc:0.986]
Epoch [68/120    avg_loss:0.015, val_acc:0.986]
Epoch [69/120    avg_loss:0.022, val_acc:0.986]
Epoch [70/120    avg_loss:0.011, val_acc:0.986]
Epoch [71/120    avg_loss:0.013, val_acc:0.988]
Epoch [72/120    avg_loss:0.012, val_acc:0.988]
Epoch [73/120    avg_loss:0.015, val_acc:0.988]
Epoch [74/120    avg_loss:0.009, val_acc:0.988]
Epoch [75/120    avg_loss:0.012, val_acc:0.988]
Epoch [76/120    avg_loss:0.011, val_acc:0.988]
Epoch [77/120    avg_loss:0.009, val_acc:0.988]
Epoch [78/120    avg_loss:0.012, val_acc:0.988]
Epoch [79/120    avg_loss:0.021, val_acc:0.988]
Epoch [80/120    avg_loss:0.013, val_acc:0.988]
Epoch [81/120    avg_loss:0.009, val_acc:0.988]
Epoch [82/120    avg_loss:0.013, val_acc:0.988]
Epoch [83/120    avg_loss:0.009, val_acc:0.988]
Epoch [84/120    avg_loss:0.010, val_acc:0.988]
Epoch [85/120    avg_loss:0.014, val_acc:0.988]
Epoch [86/120    avg_loss:0.009, val_acc:0.988]
Epoch [87/120    avg_loss:0.010, val_acc:0.988]
Epoch [88/120    avg_loss:0.015, val_acc:0.988]
Epoch [89/120    avg_loss:0.013, val_acc:0.988]
Epoch [90/120    avg_loss:0.008, val_acc:0.988]
Epoch [91/120    avg_loss:0.010, val_acc:0.988]
Epoch [92/120    avg_loss:0.011, val_acc:0.988]
Epoch [93/120    avg_loss:0.008, val_acc:0.988]
Epoch [94/120    avg_loss:0.009, val_acc:0.988]
Epoch [95/120    avg_loss:0.009, val_acc:0.988]
Epoch [96/120    avg_loss:0.010, val_acc:0.988]
Epoch [97/120    avg_loss:0.010, val_acc:0.988]
Epoch [98/120    avg_loss:0.010, val_acc:0.988]
Epoch [99/120    avg_loss:0.009, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.988]
Epoch [101/120    avg_loss:0.009, val_acc:0.988]
Epoch [102/120    avg_loss:0.009, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.988]
Epoch [104/120    avg_loss:0.016, val_acc:0.988]
Epoch [105/120    avg_loss:0.009, val_acc:0.988]
Epoch [106/120    avg_loss:0.013, val_acc:0.988]
Epoch [107/120    avg_loss:0.012, val_acc:0.988]
Epoch [108/120    avg_loss:0.009, val_acc:0.988]
Epoch [109/120    avg_loss:0.019, val_acc:0.988]
Epoch [110/120    avg_loss:0.009, val_acc:0.988]
Epoch [111/120    avg_loss:0.012, val_acc:0.988]
Epoch [112/120    avg_loss:0.011, val_acc:0.988]
Epoch [113/120    avg_loss:0.008, val_acc:0.988]
Epoch [114/120    avg_loss:0.011, val_acc:0.988]
Epoch [115/120    avg_loss:0.011, val_acc:0.988]
Epoch [116/120    avg_loss:0.011, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.009, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.012, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   1   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  10   0   0   0   0   0   0   3   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5   0 201   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.99545455 0.99782135 0.93043478 0.91986063
 0.98771499 0.98924731 1.         1.         1.         1.
 0.99669967 1.        ]

Kappa:
0.9919285092484017
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb8240247f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.986, val_acc:0.536]
Epoch [2/120    avg_loss:1.226, val_acc:0.627]
Epoch [3/120    avg_loss:1.028, val_acc:0.808]
Epoch [4/120    avg_loss:0.865, val_acc:0.744]
Epoch [5/120    avg_loss:0.759, val_acc:0.712]
Epoch [6/120    avg_loss:0.795, val_acc:0.728]
Epoch [7/120    avg_loss:0.702, val_acc:0.790]
Epoch [8/120    avg_loss:0.617, val_acc:0.784]
Epoch [9/120    avg_loss:0.704, val_acc:0.841]
Epoch [10/120    avg_loss:0.630, val_acc:0.843]
Epoch [11/120    avg_loss:0.477, val_acc:0.841]
Epoch [12/120    avg_loss:0.608, val_acc:0.837]
Epoch [13/120    avg_loss:0.532, val_acc:0.863]
Epoch [14/120    avg_loss:0.400, val_acc:0.869]
Epoch [15/120    avg_loss:0.509, val_acc:0.873]
Epoch [16/120    avg_loss:0.465, val_acc:0.891]
Epoch [17/120    avg_loss:0.408, val_acc:0.877]
Epoch [18/120    avg_loss:0.483, val_acc:0.895]
Epoch [19/120    avg_loss:0.348, val_acc:0.913]
Epoch [20/120    avg_loss:0.392, val_acc:0.901]
Epoch [21/120    avg_loss:0.348, val_acc:0.901]
Epoch [22/120    avg_loss:0.318, val_acc:0.861]
Epoch [23/120    avg_loss:0.361, val_acc:0.889]
Epoch [24/120    avg_loss:0.347, val_acc:0.933]
Epoch [25/120    avg_loss:0.353, val_acc:0.909]
Epoch [26/120    avg_loss:0.334, val_acc:0.895]
Epoch [27/120    avg_loss:0.325, val_acc:0.881]
Epoch [28/120    avg_loss:0.301, val_acc:0.937]
Epoch [29/120    avg_loss:0.238, val_acc:0.923]
Epoch [30/120    avg_loss:0.297, val_acc:0.895]
Epoch [31/120    avg_loss:0.196, val_acc:0.879]
Epoch [32/120    avg_loss:0.321, val_acc:0.915]
Epoch [33/120    avg_loss:0.335, val_acc:0.899]
Epoch [34/120    avg_loss:0.259, val_acc:0.938]
Epoch [35/120    avg_loss:0.255, val_acc:0.929]
Epoch [36/120    avg_loss:0.174, val_acc:0.938]
Epoch [37/120    avg_loss:0.167, val_acc:0.937]
Epoch [38/120    avg_loss:0.154, val_acc:0.948]
Epoch [39/120    avg_loss:0.174, val_acc:0.919]
Epoch [40/120    avg_loss:0.281, val_acc:0.940]
Epoch [41/120    avg_loss:0.155, val_acc:0.966]
Epoch [42/120    avg_loss:0.146, val_acc:0.954]
Epoch [43/120    avg_loss:0.183, val_acc:0.968]
Epoch [44/120    avg_loss:0.078, val_acc:0.958]
Epoch [45/120    avg_loss:0.111, val_acc:0.966]
Epoch [46/120    avg_loss:0.121, val_acc:0.956]
Epoch [47/120    avg_loss:0.162, val_acc:0.950]
Epoch [48/120    avg_loss:0.134, val_acc:0.970]
Epoch [49/120    avg_loss:0.091, val_acc:0.929]
Epoch [50/120    avg_loss:0.097, val_acc:0.958]
Epoch [51/120    avg_loss:0.071, val_acc:0.962]
Epoch [52/120    avg_loss:0.174, val_acc:0.899]
Epoch [53/120    avg_loss:0.184, val_acc:0.964]
Epoch [54/120    avg_loss:0.114, val_acc:0.970]
Epoch [55/120    avg_loss:0.106, val_acc:0.946]
Epoch [56/120    avg_loss:0.088, val_acc:0.954]
Epoch [57/120    avg_loss:0.132, val_acc:0.960]
Epoch [58/120    avg_loss:0.104, val_acc:0.960]
Epoch [59/120    avg_loss:0.047, val_acc:0.970]
Epoch [60/120    avg_loss:0.048, val_acc:0.976]
Epoch [61/120    avg_loss:0.037, val_acc:0.980]
Epoch [62/120    avg_loss:0.096, val_acc:0.954]
Epoch [63/120    avg_loss:0.079, val_acc:0.976]
Epoch [64/120    avg_loss:0.065, val_acc:0.964]
Epoch [65/120    avg_loss:0.079, val_acc:0.952]
Epoch [66/120    avg_loss:0.041, val_acc:0.978]
Epoch [67/120    avg_loss:0.072, val_acc:0.956]
Epoch [68/120    avg_loss:0.062, val_acc:0.968]
Epoch [69/120    avg_loss:0.032, val_acc:0.958]
Epoch [70/120    avg_loss:0.031, val_acc:0.974]
Epoch [71/120    avg_loss:0.072, val_acc:0.976]
Epoch [72/120    avg_loss:0.035, val_acc:0.972]
Epoch [73/120    avg_loss:0.182, val_acc:0.937]
Epoch [74/120    avg_loss:0.154, val_acc:0.940]
Epoch [75/120    avg_loss:0.087, val_acc:0.962]
Epoch [76/120    avg_loss:0.040, val_acc:0.964]
Epoch [77/120    avg_loss:0.043, val_acc:0.966]
Epoch [78/120    avg_loss:0.042, val_acc:0.976]
Epoch [79/120    avg_loss:0.038, val_acc:0.974]
Epoch [80/120    avg_loss:0.035, val_acc:0.978]
Epoch [81/120    avg_loss:0.038, val_acc:0.980]
Epoch [82/120    avg_loss:0.030, val_acc:0.984]
Epoch [83/120    avg_loss:0.041, val_acc:0.982]
Epoch [84/120    avg_loss:0.032, val_acc:0.984]
Epoch [85/120    avg_loss:0.028, val_acc:0.986]
Epoch [86/120    avg_loss:0.017, val_acc:0.988]
Epoch [87/120    avg_loss:0.035, val_acc:0.988]
Epoch [88/120    avg_loss:0.030, val_acc:0.986]
Epoch [89/120    avg_loss:0.022, val_acc:0.986]
Epoch [90/120    avg_loss:0.019, val_acc:0.988]
Epoch [91/120    avg_loss:0.018, val_acc:0.988]
Epoch [92/120    avg_loss:0.027, val_acc:0.984]
Epoch [93/120    avg_loss:0.020, val_acc:0.984]
Epoch [94/120    avg_loss:0.020, val_acc:0.986]
Epoch [95/120    avg_loss:0.021, val_acc:0.986]
Epoch [96/120    avg_loss:0.019, val_acc:0.984]
Epoch [97/120    avg_loss:0.023, val_acc:0.986]
Epoch [98/120    avg_loss:0.014, val_acc:0.984]
Epoch [99/120    avg_loss:0.016, val_acc:0.984]
Epoch [100/120    avg_loss:0.017, val_acc:0.986]
Epoch [101/120    avg_loss:0.020, val_acc:0.986]
Epoch [102/120    avg_loss:0.016, val_acc:0.986]
Epoch [103/120    avg_loss:0.014, val_acc:0.986]
Epoch [104/120    avg_loss:0.016, val_acc:0.986]
Epoch [105/120    avg_loss:0.017, val_acc:0.986]
Epoch [106/120    avg_loss:0.020, val_acc:0.986]
Epoch [107/120    avg_loss:0.018, val_acc:0.986]
Epoch [108/120    avg_loss:0.026, val_acc:0.986]
Epoch [109/120    avg_loss:0.018, val_acc:0.986]
Epoch [110/120    avg_loss:0.020, val_acc:0.986]
Epoch [111/120    avg_loss:0.025, val_acc:0.986]
Epoch [112/120    avg_loss:0.024, val_acc:0.986]
Epoch [113/120    avg_loss:0.012, val_acc:0.986]
Epoch [114/120    avg_loss:0.014, val_acc:0.986]
Epoch [115/120    avg_loss:0.018, val_acc:0.986]
Epoch [116/120    avg_loss:0.015, val_acc:0.986]
Epoch [117/120    avg_loss:0.018, val_acc:0.986]
Epoch [118/120    avg_loss:0.016, val_acc:0.986]
Epoch [119/120    avg_loss:0.014, val_acc:0.986]
Epoch [120/120    avg_loss:0.015, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  14   0   0   0   0   0   0   2   0]
 [  0   0   0   0   4 131  10   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 1.         0.98206278 1.         0.95475113 0.90344828
 0.97630332 0.95555556 1.         1.         1.         0.99867198
 0.99669967 1.        ]

Kappa:
0.9907415056313268
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:15:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff0a2cab7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:1.982, val_acc:0.629]
Epoch [2/120    avg_loss:1.227, val_acc:0.720]
Epoch [3/120    avg_loss:1.011, val_acc:0.736]
Epoch [4/120    avg_loss:0.934, val_acc:0.766]
Epoch [5/120    avg_loss:0.754, val_acc:0.829]
Epoch [6/120    avg_loss:0.785, val_acc:0.847]
Epoch [7/120    avg_loss:0.656, val_acc:0.786]
Epoch [8/120    avg_loss:0.684, val_acc:0.849]
Epoch [9/120    avg_loss:0.552, val_acc:0.893]
Epoch [10/120    avg_loss:0.583, val_acc:0.871]
Epoch [11/120    avg_loss:0.446, val_acc:0.903]
Epoch [12/120    avg_loss:0.522, val_acc:0.869]
Epoch [13/120    avg_loss:0.437, val_acc:0.899]
Epoch [14/120    avg_loss:0.403, val_acc:0.915]
Epoch [15/120    avg_loss:0.479, val_acc:0.895]
Epoch [16/120    avg_loss:0.370, val_acc:0.849]
Epoch [17/120    avg_loss:0.354, val_acc:0.917]
Epoch [18/120    avg_loss:0.281, val_acc:0.940]
Epoch [19/120    avg_loss:0.356, val_acc:0.909]
Epoch [20/120    avg_loss:0.321, val_acc:0.917]
Epoch [21/120    avg_loss:0.321, val_acc:0.911]
Epoch [22/120    avg_loss:0.355, val_acc:0.938]
Epoch [23/120    avg_loss:0.310, val_acc:0.946]
Epoch [24/120    avg_loss:0.273, val_acc:0.946]
Epoch [25/120    avg_loss:0.244, val_acc:0.919]
Epoch [26/120    avg_loss:0.238, val_acc:0.942]
Epoch [27/120    avg_loss:0.186, val_acc:0.944]
Epoch [28/120    avg_loss:0.202, val_acc:0.935]
Epoch [29/120    avg_loss:0.165, val_acc:0.956]
Epoch [30/120    avg_loss:0.249, val_acc:0.950]
Epoch [31/120    avg_loss:0.231, val_acc:0.931]
Epoch [32/120    avg_loss:0.203, val_acc:0.958]
Epoch [33/120    avg_loss:0.242, val_acc:0.956]
Epoch [34/120    avg_loss:0.258, val_acc:0.933]
Epoch [35/120    avg_loss:0.215, val_acc:0.950]
Epoch [36/120    avg_loss:0.179, val_acc:0.968]
Epoch [37/120    avg_loss:0.121, val_acc:0.956]
Epoch [38/120    avg_loss:0.123, val_acc:0.974]
Epoch [39/120    avg_loss:0.137, val_acc:0.976]
Epoch [40/120    avg_loss:0.099, val_acc:0.962]
Epoch [41/120    avg_loss:0.099, val_acc:0.972]
Epoch [42/120    avg_loss:0.084, val_acc:0.978]
Epoch [43/120    avg_loss:0.097, val_acc:0.970]
Epoch [44/120    avg_loss:0.116, val_acc:0.966]
Epoch [45/120    avg_loss:0.149, val_acc:0.960]
Epoch [46/120    avg_loss:0.088, val_acc:0.966]
Epoch [47/120    avg_loss:0.151, val_acc:0.968]
Epoch [48/120    avg_loss:0.217, val_acc:0.952]
Epoch [49/120    avg_loss:0.182, val_acc:0.958]
Epoch [50/120    avg_loss:0.105, val_acc:0.966]
Epoch [51/120    avg_loss:0.112, val_acc:0.972]
Epoch [52/120    avg_loss:0.093, val_acc:0.956]
Epoch [53/120    avg_loss:0.086, val_acc:0.978]
Epoch [54/120    avg_loss:0.109, val_acc:0.972]
Epoch [55/120    avg_loss:0.105, val_acc:0.982]
Epoch [56/120    avg_loss:0.053, val_acc:0.976]
Epoch [57/120    avg_loss:0.049, val_acc:0.982]
Epoch [58/120    avg_loss:0.073, val_acc:0.978]
Epoch [59/120    avg_loss:0.076, val_acc:0.978]
Epoch [60/120    avg_loss:0.043, val_acc:0.982]
Epoch [61/120    avg_loss:0.053, val_acc:0.984]
Epoch [62/120    avg_loss:0.044, val_acc:0.976]
Epoch [63/120    avg_loss:0.061, val_acc:0.980]
Epoch [64/120    avg_loss:0.057, val_acc:0.982]
Epoch [65/120    avg_loss:0.028, val_acc:0.980]
Epoch [66/120    avg_loss:0.026, val_acc:0.984]
Epoch [67/120    avg_loss:0.026, val_acc:0.982]
Epoch [68/120    avg_loss:0.066, val_acc:0.986]
Epoch [69/120    avg_loss:0.089, val_acc:0.962]
Epoch [70/120    avg_loss:0.056, val_acc:0.984]
Epoch [71/120    avg_loss:0.031, val_acc:0.984]
Epoch [72/120    avg_loss:0.048, val_acc:0.984]
Epoch [73/120    avg_loss:0.044, val_acc:0.980]
Epoch [74/120    avg_loss:0.040, val_acc:0.964]
Epoch [75/120    avg_loss:0.067, val_acc:0.964]
Epoch [76/120    avg_loss:0.037, val_acc:0.978]
Epoch [77/120    avg_loss:0.056, val_acc:0.972]
Epoch [78/120    avg_loss:0.031, val_acc:0.982]
Epoch [79/120    avg_loss:0.015, val_acc:0.984]
Epoch [80/120    avg_loss:0.015, val_acc:0.984]
Epoch [81/120    avg_loss:0.016, val_acc:0.986]
Epoch [82/120    avg_loss:0.014, val_acc:0.988]
Epoch [83/120    avg_loss:0.029, val_acc:0.988]
Epoch [84/120    avg_loss:0.029, val_acc:0.976]
Epoch [85/120    avg_loss:0.049, val_acc:0.988]
Epoch [86/120    avg_loss:0.026, val_acc:0.984]
Epoch [87/120    avg_loss:0.014, val_acc:0.986]
Epoch [88/120    avg_loss:0.017, val_acc:0.986]
Epoch [89/120    avg_loss:0.022, val_acc:0.984]
Epoch [90/120    avg_loss:0.015, val_acc:0.982]
Epoch [91/120    avg_loss:0.015, val_acc:0.974]
Epoch [92/120    avg_loss:0.025, val_acc:0.978]
Epoch [93/120    avg_loss:0.022, val_acc:0.988]
Epoch [94/120    avg_loss:0.013, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.986]
Epoch [96/120    avg_loss:0.017, val_acc:0.990]
Epoch [97/120    avg_loss:0.008, val_acc:0.988]
Epoch [98/120    avg_loss:0.010, val_acc:0.992]
Epoch [99/120    avg_loss:0.008, val_acc:0.986]
Epoch [100/120    avg_loss:0.009, val_acc:0.990]
Epoch [101/120    avg_loss:0.016, val_acc:0.992]
Epoch [102/120    avg_loss:0.039, val_acc:0.976]
Epoch [103/120    avg_loss:0.013, val_acc:0.992]
Epoch [104/120    avg_loss:0.036, val_acc:0.978]
Epoch [105/120    avg_loss:0.020, val_acc:0.982]
Epoch [106/120    avg_loss:0.034, val_acc:0.984]
Epoch [107/120    avg_loss:0.029, val_acc:0.990]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.070, val_acc:0.952]
Epoch [110/120    avg_loss:0.144, val_acc:0.982]
Epoch [111/120    avg_loss:0.055, val_acc:0.982]
Epoch [112/120    avg_loss:0.186, val_acc:0.972]
Epoch [113/120    avg_loss:0.029, val_acc:0.978]
Epoch [114/120    avg_loss:0.012, val_acc:0.988]
Epoch [115/120    avg_loss:0.017, val_acc:0.988]
Epoch [116/120    avg_loss:0.044, val_acc:0.972]
Epoch [117/120    avg_loss:0.071, val_acc:0.984]
Epoch [118/120    avg_loss:0.031, val_acc:0.986]
Epoch [119/120    avg_loss:0.026, val_acc:0.988]
Epoch [120/120    avg_loss:0.014, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216   9   0   0   0   0   0   0   2   0]
 [  0   0   0   0  16 129   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  12   0   0   0   0  82   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 1.         0.97333333 1.         0.94117647 0.91166078
 1.         0.93181818 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9907409394744664
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc4aff9d898>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.051, val_acc:0.575]
Epoch [2/120    avg_loss:1.352, val_acc:0.728]
Epoch [3/120    avg_loss:1.042, val_acc:0.692]
Epoch [4/120    avg_loss:0.853, val_acc:0.786]
Epoch [5/120    avg_loss:0.810, val_acc:0.720]
Epoch [6/120    avg_loss:0.777, val_acc:0.762]
Epoch [7/120    avg_loss:0.753, val_acc:0.821]
Epoch [8/120    avg_loss:0.566, val_acc:0.835]
Epoch [9/120    avg_loss:0.587, val_acc:0.853]
Epoch [10/120    avg_loss:0.578, val_acc:0.859]
Epoch [11/120    avg_loss:0.581, val_acc:0.889]
Epoch [12/120    avg_loss:0.516, val_acc:0.867]
Epoch [13/120    avg_loss:0.493, val_acc:0.889]
Epoch [14/120    avg_loss:0.419, val_acc:0.893]
Epoch [15/120    avg_loss:0.452, val_acc:0.863]
Epoch [16/120    avg_loss:0.424, val_acc:0.903]
Epoch [17/120    avg_loss:0.400, val_acc:0.897]
Epoch [18/120    avg_loss:0.346, val_acc:0.927]
Epoch [19/120    avg_loss:0.347, val_acc:0.911]
Epoch [20/120    avg_loss:0.388, val_acc:0.913]
Epoch [21/120    avg_loss:0.333, val_acc:0.907]
Epoch [22/120    avg_loss:0.371, val_acc:0.907]
Epoch [23/120    avg_loss:0.294, val_acc:0.861]
Epoch [24/120    avg_loss:0.277, val_acc:0.940]
Epoch [25/120    avg_loss:0.301, val_acc:0.958]
Epoch [26/120    avg_loss:0.223, val_acc:0.925]
Epoch [27/120    avg_loss:0.267, val_acc:0.927]
Epoch [28/120    avg_loss:0.267, val_acc:0.950]
Epoch [29/120    avg_loss:0.218, val_acc:0.954]
Epoch [30/120    avg_loss:0.234, val_acc:0.935]
Epoch [31/120    avg_loss:0.289, val_acc:0.921]
Epoch [32/120    avg_loss:0.216, val_acc:0.950]
Epoch [33/120    avg_loss:0.217, val_acc:0.954]
Epoch [34/120    avg_loss:0.166, val_acc:0.964]
Epoch [35/120    avg_loss:0.142, val_acc:0.964]
Epoch [36/120    avg_loss:0.168, val_acc:0.962]
Epoch [37/120    avg_loss:0.142, val_acc:0.938]
Epoch [38/120    avg_loss:0.160, val_acc:0.966]
Epoch [39/120    avg_loss:0.136, val_acc:0.966]
Epoch [40/120    avg_loss:0.162, val_acc:0.970]
Epoch [41/120    avg_loss:0.114, val_acc:0.972]
Epoch [42/120    avg_loss:0.122, val_acc:0.966]
Epoch [43/120    avg_loss:0.139, val_acc:0.964]
Epoch [44/120    avg_loss:0.096, val_acc:0.950]
Epoch [45/120    avg_loss:0.073, val_acc:0.982]
Epoch [46/120    avg_loss:0.100, val_acc:0.972]
Epoch [47/120    avg_loss:0.099, val_acc:0.972]
Epoch [48/120    avg_loss:0.153, val_acc:0.937]
Epoch [49/120    avg_loss:0.243, val_acc:0.952]
Epoch [50/120    avg_loss:0.091, val_acc:0.970]
Epoch [51/120    avg_loss:0.069, val_acc:0.982]
Epoch [52/120    avg_loss:0.057, val_acc:0.982]
Epoch [53/120    avg_loss:0.066, val_acc:0.980]
Epoch [54/120    avg_loss:0.074, val_acc:0.982]
Epoch [55/120    avg_loss:0.087, val_acc:0.970]
Epoch [56/120    avg_loss:0.095, val_acc:0.970]
Epoch [57/120    avg_loss:0.154, val_acc:0.968]
Epoch [58/120    avg_loss:0.038, val_acc:0.974]
Epoch [59/120    avg_loss:0.059, val_acc:0.986]
Epoch [60/120    avg_loss:0.056, val_acc:0.972]
Epoch [61/120    avg_loss:0.074, val_acc:0.982]
Epoch [62/120    avg_loss:0.076, val_acc:0.976]
Epoch [63/120    avg_loss:0.061, val_acc:0.988]
Epoch [64/120    avg_loss:0.021, val_acc:0.982]
Epoch [65/120    avg_loss:0.042, val_acc:0.988]
Epoch [66/120    avg_loss:0.173, val_acc:0.980]
Epoch [67/120    avg_loss:0.114, val_acc:0.978]
Epoch [68/120    avg_loss:0.053, val_acc:0.978]
Epoch [69/120    avg_loss:0.089, val_acc:0.978]
Epoch [70/120    avg_loss:0.053, val_acc:0.982]
Epoch [71/120    avg_loss:0.063, val_acc:0.984]
Epoch [72/120    avg_loss:0.057, val_acc:0.980]
Epoch [73/120    avg_loss:0.041, val_acc:0.984]
Epoch [74/120    avg_loss:0.044, val_acc:0.962]
Epoch [75/120    avg_loss:0.092, val_acc:0.986]
Epoch [76/120    avg_loss:0.025, val_acc:0.976]
Epoch [77/120    avg_loss:0.018, val_acc:0.988]
Epoch [78/120    avg_loss:0.019, val_acc:0.986]
Epoch [79/120    avg_loss:0.078, val_acc:0.938]
Epoch [80/120    avg_loss:0.049, val_acc:0.966]
Epoch [81/120    avg_loss:0.037, val_acc:0.980]
Epoch [82/120    avg_loss:0.048, val_acc:0.974]
Epoch [83/120    avg_loss:0.076, val_acc:0.986]
Epoch [84/120    avg_loss:0.029, val_acc:0.990]
Epoch [85/120    avg_loss:0.025, val_acc:0.986]
Epoch [86/120    avg_loss:0.014, val_acc:0.984]
Epoch [87/120    avg_loss:0.016, val_acc:0.984]
Epoch [88/120    avg_loss:0.031, val_acc:0.984]
Epoch [89/120    avg_loss:0.053, val_acc:0.962]
Epoch [90/120    avg_loss:0.034, val_acc:0.988]
Epoch [91/120    avg_loss:0.032, val_acc:0.986]
Epoch [92/120    avg_loss:0.054, val_acc:0.980]
Epoch [93/120    avg_loss:0.031, val_acc:0.982]
Epoch [94/120    avg_loss:0.019, val_acc:0.990]
Epoch [95/120    avg_loss:0.008, val_acc:0.990]
Epoch [96/120    avg_loss:0.020, val_acc:0.984]
Epoch [97/120    avg_loss:0.015, val_acc:0.986]
Epoch [98/120    avg_loss:0.020, val_acc:0.984]
Epoch [99/120    avg_loss:0.006, val_acc:0.992]
Epoch [100/120    avg_loss:0.009, val_acc:0.988]
Epoch [101/120    avg_loss:0.015, val_acc:0.980]
Epoch [102/120    avg_loss:0.033, val_acc:0.988]
Epoch [103/120    avg_loss:0.016, val_acc:0.982]
Epoch [104/120    avg_loss:0.006, val_acc:0.988]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.020, val_acc:0.984]
Epoch [107/120    avg_loss:0.008, val_acc:0.984]
Epoch [108/120    avg_loss:0.011, val_acc:0.982]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.004, val_acc:0.988]
Epoch [113/120    avg_loss:0.008, val_acc:0.988]
Epoch [114/120    avg_loss:0.004, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.004, val_acc:0.988]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.003, val_acc:0.988]
Epoch [119/120    avg_loss:0.006, val_acc:0.988]
Epoch [120/120    avg_loss:0.004, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 227   0   0   0   0   0   3   0   0   0   0]
 [  0   0   0   0 213  13   0   0   0   0   0   0   1   0]
 [  0   0   0   0   6 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 1.         1.         0.99343545 0.95302013 0.93602694
 0.99756691 1.         1.         0.99680511 1.         0.99341238
 0.99334812 1.        ]

Kappa:
0.9931159353720915
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd106c1c780>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.014, val_acc:0.550]
Epoch [2/120    avg_loss:1.325, val_acc:0.716]
Epoch [3/120    avg_loss:0.961, val_acc:0.714]
Epoch [4/120    avg_loss:0.854, val_acc:0.730]
Epoch [5/120    avg_loss:0.758, val_acc:0.752]
Epoch [6/120    avg_loss:0.753, val_acc:0.812]
Epoch [7/120    avg_loss:0.637, val_acc:0.871]
Epoch [8/120    avg_loss:0.594, val_acc:0.867]
Epoch [9/120    avg_loss:0.520, val_acc:0.812]
Epoch [10/120    avg_loss:0.537, val_acc:0.867]
Epoch [11/120    avg_loss:0.421, val_acc:0.885]
Epoch [12/120    avg_loss:0.512, val_acc:0.879]
Epoch [13/120    avg_loss:0.512, val_acc:0.891]
Epoch [14/120    avg_loss:0.479, val_acc:0.853]
Epoch [15/120    avg_loss:0.426, val_acc:0.899]
Epoch [16/120    avg_loss:0.352, val_acc:0.931]
Epoch [17/120    avg_loss:0.390, val_acc:0.790]
Epoch [18/120    avg_loss:0.398, val_acc:0.873]
Epoch [19/120    avg_loss:0.401, val_acc:0.887]
Epoch [20/120    avg_loss:0.354, val_acc:0.901]
Epoch [21/120    avg_loss:0.359, val_acc:0.917]
Epoch [22/120    avg_loss:0.315, val_acc:0.911]
Epoch [23/120    avg_loss:0.330, val_acc:0.921]
Epoch [24/120    avg_loss:0.340, val_acc:0.931]
Epoch [25/120    avg_loss:0.319, val_acc:0.859]
Epoch [26/120    avg_loss:0.268, val_acc:0.903]
Epoch [27/120    avg_loss:0.242, val_acc:0.923]
Epoch [28/120    avg_loss:0.259, val_acc:0.931]
Epoch [29/120    avg_loss:0.243, val_acc:0.946]
Epoch [30/120    avg_loss:0.382, val_acc:0.909]
Epoch [31/120    avg_loss:0.238, val_acc:0.944]
Epoch [32/120    avg_loss:0.229, val_acc:0.897]
Epoch [33/120    avg_loss:0.337, val_acc:0.944]
Epoch [34/120    avg_loss:0.227, val_acc:0.948]
Epoch [35/120    avg_loss:0.201, val_acc:0.952]
Epoch [36/120    avg_loss:0.168, val_acc:0.958]
Epoch [37/120    avg_loss:0.140, val_acc:0.972]
Epoch [38/120    avg_loss:0.145, val_acc:0.968]
Epoch [39/120    avg_loss:0.158, val_acc:0.952]
Epoch [40/120    avg_loss:0.130, val_acc:0.927]
Epoch [41/120    avg_loss:0.135, val_acc:0.970]
Epoch [42/120    avg_loss:0.124, val_acc:0.978]
Epoch [43/120    avg_loss:0.084, val_acc:0.970]
Epoch [44/120    avg_loss:0.223, val_acc:0.948]
Epoch [45/120    avg_loss:0.137, val_acc:0.964]
Epoch [46/120    avg_loss:0.185, val_acc:0.925]
Epoch [47/120    avg_loss:0.213, val_acc:0.925]
Epoch [48/120    avg_loss:0.112, val_acc:0.964]
Epoch [49/120    avg_loss:0.106, val_acc:0.964]
Epoch [50/120    avg_loss:0.181, val_acc:0.958]
Epoch [51/120    avg_loss:0.138, val_acc:0.974]
Epoch [52/120    avg_loss:0.069, val_acc:0.968]
Epoch [53/120    avg_loss:0.064, val_acc:0.972]
Epoch [54/120    avg_loss:0.140, val_acc:0.962]
Epoch [55/120    avg_loss:0.106, val_acc:0.972]
Epoch [56/120    avg_loss:0.080, val_acc:0.976]
Epoch [57/120    avg_loss:0.047, val_acc:0.980]
Epoch [58/120    avg_loss:0.064, val_acc:0.978]
Epoch [59/120    avg_loss:0.047, val_acc:0.980]
Epoch [60/120    avg_loss:0.048, val_acc:0.982]
Epoch [61/120    avg_loss:0.048, val_acc:0.980]
Epoch [62/120    avg_loss:0.034, val_acc:0.978]
Epoch [63/120    avg_loss:0.048, val_acc:0.980]
Epoch [64/120    avg_loss:0.035, val_acc:0.982]
Epoch [65/120    avg_loss:0.029, val_acc:0.980]
Epoch [66/120    avg_loss:0.032, val_acc:0.980]
Epoch [67/120    avg_loss:0.034, val_acc:0.980]
Epoch [68/120    avg_loss:0.039, val_acc:0.980]
Epoch [69/120    avg_loss:0.040, val_acc:0.982]
Epoch [70/120    avg_loss:0.032, val_acc:0.982]
Epoch [71/120    avg_loss:0.034, val_acc:0.982]
Epoch [72/120    avg_loss:0.031, val_acc:0.984]
Epoch [73/120    avg_loss:0.039, val_acc:0.984]
Epoch [74/120    avg_loss:0.027, val_acc:0.984]
Epoch [75/120    avg_loss:0.031, val_acc:0.982]
Epoch [76/120    avg_loss:0.028, val_acc:0.984]
Epoch [77/120    avg_loss:0.033, val_acc:0.986]
Epoch [78/120    avg_loss:0.031, val_acc:0.988]
Epoch [79/120    avg_loss:0.042, val_acc:0.990]
Epoch [80/120    avg_loss:0.028, val_acc:0.990]
Epoch [81/120    avg_loss:0.027, val_acc:0.988]
Epoch [82/120    avg_loss:0.024, val_acc:0.984]
Epoch [83/120    avg_loss:0.022, val_acc:0.990]
Epoch [84/120    avg_loss:0.023, val_acc:0.990]
Epoch [85/120    avg_loss:0.020, val_acc:0.988]
Epoch [86/120    avg_loss:0.021, val_acc:0.988]
Epoch [87/120    avg_loss:0.021, val_acc:0.988]
Epoch [88/120    avg_loss:0.024, val_acc:0.988]
Epoch [89/120    avg_loss:0.021, val_acc:0.988]
Epoch [90/120    avg_loss:0.018, val_acc:0.990]
Epoch [91/120    avg_loss:0.023, val_acc:0.988]
Epoch [92/120    avg_loss:0.029, val_acc:0.988]
Epoch [93/120    avg_loss:0.024, val_acc:0.990]
Epoch [94/120    avg_loss:0.038, val_acc:0.988]
Epoch [95/120    avg_loss:0.028, val_acc:0.984]
Epoch [96/120    avg_loss:0.028, val_acc:0.984]
Epoch [97/120    avg_loss:0.020, val_acc:0.984]
Epoch [98/120    avg_loss:0.022, val_acc:0.984]
Epoch [99/120    avg_loss:0.019, val_acc:0.986]
Epoch [100/120    avg_loss:0.024, val_acc:0.986]
Epoch [101/120    avg_loss:0.023, val_acc:0.988]
Epoch [102/120    avg_loss:0.029, val_acc:0.988]
Epoch [103/120    avg_loss:0.022, val_acc:0.988]
Epoch [104/120    avg_loss:0.016, val_acc:0.990]
Epoch [105/120    avg_loss:0.019, val_acc:0.988]
Epoch [106/120    avg_loss:0.020, val_acc:0.990]
Epoch [107/120    avg_loss:0.022, val_acc:0.992]
Epoch [108/120    avg_loss:0.034, val_acc:0.990]
Epoch [109/120    avg_loss:0.022, val_acc:0.988]
Epoch [110/120    avg_loss:0.021, val_acc:0.986]
Epoch [111/120    avg_loss:0.021, val_acc:0.990]
Epoch [112/120    avg_loss:0.018, val_acc:0.988]
Epoch [113/120    avg_loss:0.024, val_acc:0.990]
Epoch [114/120    avg_loss:0.020, val_acc:0.990]
Epoch [115/120    avg_loss:0.025, val_acc:0.988]
Epoch [116/120    avg_loss:0.019, val_acc:0.988]
Epoch [117/120    avg_loss:0.020, val_acc:0.990]
Epoch [118/120    avg_loss:0.023, val_acc:0.990]
Epoch [119/120    avg_loss:0.020, val_acc:0.990]
Epoch [120/120    avg_loss:0.024, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 1.         0.99095023 0.99782135 0.93986637 0.90847458
 1.         0.98378378 1.         1.         1.         0.99734748
 0.99779249 1.        ]

Kappa:
0.9921664302305642
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb9ce4df898>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.048, val_acc:0.609]
Epoch [2/120    avg_loss:1.390, val_acc:0.645]
Epoch [3/120    avg_loss:1.043, val_acc:0.696]
Epoch [4/120    avg_loss:0.880, val_acc:0.819]
Epoch [5/120    avg_loss:0.853, val_acc:0.782]
Epoch [6/120    avg_loss:0.785, val_acc:0.847]
Epoch [7/120    avg_loss:0.698, val_acc:0.859]
Epoch [8/120    avg_loss:0.660, val_acc:0.837]
Epoch [9/120    avg_loss:0.582, val_acc:0.857]
Epoch [10/120    avg_loss:0.523, val_acc:0.871]
Epoch [11/120    avg_loss:0.551, val_acc:0.887]
Epoch [12/120    avg_loss:0.485, val_acc:0.889]
Epoch [13/120    avg_loss:0.468, val_acc:0.899]
Epoch [14/120    avg_loss:0.414, val_acc:0.907]
Epoch [15/120    avg_loss:0.400, val_acc:0.917]
Epoch [16/120    avg_loss:0.354, val_acc:0.915]
Epoch [17/120    avg_loss:0.376, val_acc:0.923]
Epoch [18/120    avg_loss:0.358, val_acc:0.875]
Epoch [19/120    avg_loss:0.375, val_acc:0.921]
Epoch [20/120    avg_loss:0.317, val_acc:0.937]
Epoch [21/120    avg_loss:0.252, val_acc:0.929]
Epoch [22/120    avg_loss:0.296, val_acc:0.962]
Epoch [23/120    avg_loss:0.227, val_acc:0.968]
Epoch [24/120    avg_loss:0.193, val_acc:0.944]
Epoch [25/120    avg_loss:0.226, val_acc:0.933]
Epoch [26/120    avg_loss:0.277, val_acc:0.952]
Epoch [27/120    avg_loss:0.235, val_acc:0.960]
Epoch [28/120    avg_loss:0.258, val_acc:0.913]
Epoch [29/120    avg_loss:0.253, val_acc:0.940]
Epoch [30/120    avg_loss:0.214, val_acc:0.950]
Epoch [31/120    avg_loss:0.180, val_acc:0.946]
Epoch [32/120    avg_loss:0.172, val_acc:0.962]
Epoch [33/120    avg_loss:0.176, val_acc:0.960]
Epoch [34/120    avg_loss:0.128, val_acc:0.911]
Epoch [35/120    avg_loss:0.214, val_acc:0.901]
Epoch [36/120    avg_loss:0.200, val_acc:0.964]
Epoch [37/120    avg_loss:0.114, val_acc:0.968]
Epoch [38/120    avg_loss:0.075, val_acc:0.972]
Epoch [39/120    avg_loss:0.087, val_acc:0.968]
Epoch [40/120    avg_loss:0.107, val_acc:0.972]
Epoch [41/120    avg_loss:0.080, val_acc:0.972]
Epoch [42/120    avg_loss:0.075, val_acc:0.972]
Epoch [43/120    avg_loss:0.076, val_acc:0.976]
Epoch [44/120    avg_loss:0.092, val_acc:0.976]
Epoch [45/120    avg_loss:0.058, val_acc:0.976]
Epoch [46/120    avg_loss:0.071, val_acc:0.978]
Epoch [47/120    avg_loss:0.049, val_acc:0.982]
Epoch [48/120    avg_loss:0.072, val_acc:0.978]
Epoch [49/120    avg_loss:0.060, val_acc:0.978]
Epoch [50/120    avg_loss:0.054, val_acc:0.980]
Epoch [51/120    avg_loss:0.065, val_acc:0.978]
Epoch [52/120    avg_loss:0.053, val_acc:0.984]
Epoch [53/120    avg_loss:0.060, val_acc:0.982]
Epoch [54/120    avg_loss:0.046, val_acc:0.982]
Epoch [55/120    avg_loss:0.060, val_acc:0.980]
Epoch [56/120    avg_loss:0.048, val_acc:0.980]
Epoch [57/120    avg_loss:0.049, val_acc:0.976]
Epoch [58/120    avg_loss:0.047, val_acc:0.984]
Epoch [59/120    avg_loss:0.052, val_acc:0.982]
Epoch [60/120    avg_loss:0.054, val_acc:0.978]
Epoch [61/120    avg_loss:0.060, val_acc:0.984]
Epoch [62/120    avg_loss:0.045, val_acc:0.984]
Epoch [63/120    avg_loss:0.042, val_acc:0.988]
Epoch [64/120    avg_loss:0.055, val_acc:0.988]
Epoch [65/120    avg_loss:0.047, val_acc:0.984]
Epoch [66/120    avg_loss:0.051, val_acc:0.988]
Epoch [67/120    avg_loss:0.043, val_acc:0.986]
Epoch [68/120    avg_loss:0.043, val_acc:0.986]
Epoch [69/120    avg_loss:0.041, val_acc:0.986]
Epoch [70/120    avg_loss:0.041, val_acc:0.988]
Epoch [71/120    avg_loss:0.035, val_acc:0.986]
Epoch [72/120    avg_loss:0.040, val_acc:0.986]
Epoch [73/120    avg_loss:0.050, val_acc:0.986]
Epoch [74/120    avg_loss:0.044, val_acc:0.990]
Epoch [75/120    avg_loss:0.033, val_acc:0.988]
Epoch [76/120    avg_loss:0.044, val_acc:0.992]
Epoch [77/120    avg_loss:0.033, val_acc:0.990]
Epoch [78/120    avg_loss:0.037, val_acc:0.990]
Epoch [79/120    avg_loss:0.040, val_acc:0.984]
Epoch [80/120    avg_loss:0.040, val_acc:0.992]
Epoch [81/120    avg_loss:0.038, val_acc:0.992]
Epoch [82/120    avg_loss:0.037, val_acc:0.988]
Epoch [83/120    avg_loss:0.033, val_acc:0.990]
Epoch [84/120    avg_loss:0.034, val_acc:0.988]
Epoch [85/120    avg_loss:0.040, val_acc:0.990]
Epoch [86/120    avg_loss:0.037, val_acc:0.992]
Epoch [87/120    avg_loss:0.032, val_acc:0.992]
Epoch [88/120    avg_loss:0.031, val_acc:0.992]
Epoch [89/120    avg_loss:0.024, val_acc:0.992]
Epoch [90/120    avg_loss:0.027, val_acc:0.992]
Epoch [91/120    avg_loss:0.028, val_acc:0.992]
Epoch [92/120    avg_loss:0.031, val_acc:0.992]
Epoch [93/120    avg_loss:0.030, val_acc:0.992]
Epoch [94/120    avg_loss:0.028, val_acc:0.992]
Epoch [95/120    avg_loss:0.035, val_acc:0.992]
Epoch [96/120    avg_loss:0.027, val_acc:0.992]
Epoch [97/120    avg_loss:0.025, val_acc:0.992]
Epoch [98/120    avg_loss:0.025, val_acc:0.992]
Epoch [99/120    avg_loss:0.031, val_acc:0.988]
Epoch [100/120    avg_loss:0.023, val_acc:0.992]
Epoch [101/120    avg_loss:0.029, val_acc:0.994]
Epoch [102/120    avg_loss:0.026, val_acc:0.992]
Epoch [103/120    avg_loss:0.032, val_acc:0.992]
Epoch [104/120    avg_loss:0.023, val_acc:0.994]
Epoch [105/120    avg_loss:0.029, val_acc:0.994]
Epoch [106/120    avg_loss:0.032, val_acc:0.994]
Epoch [107/120    avg_loss:0.026, val_acc:0.994]
Epoch [108/120    avg_loss:0.022, val_acc:0.992]
Epoch [109/120    avg_loss:0.030, val_acc:0.996]
Epoch [110/120    avg_loss:0.026, val_acc:0.990]
Epoch [111/120    avg_loss:0.025, val_acc:0.992]
Epoch [112/120    avg_loss:0.027, val_acc:0.994]
Epoch [113/120    avg_loss:0.023, val_acc:0.990]
Epoch [114/120    avg_loss:0.029, val_acc:0.992]
Epoch [115/120    avg_loss:0.023, val_acc:0.994]
Epoch [116/120    avg_loss:0.022, val_acc:0.992]
Epoch [117/120    avg_loss:0.022, val_acc:0.996]
Epoch [118/120    avg_loss:0.039, val_acc:0.992]
Epoch [119/120    avg_loss:0.023, val_acc:0.992]
Epoch [120/120    avg_loss:0.024, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 205  19   0   0   0   0   0   0   3   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 1.         0.9977221  0.99782135 0.94688222 0.93506494
 1.         1.         1.         1.         1.         0.99341238
 0.99115044 1.        ]

Kappa:
0.9931163520725014
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4c7f2957b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.003, val_acc:0.573]
Epoch [2/120    avg_loss:1.324, val_acc:0.708]
Epoch [3/120    avg_loss:1.029, val_acc:0.772]
Epoch [4/120    avg_loss:0.944, val_acc:0.786]
Epoch [5/120    avg_loss:0.805, val_acc:0.788]
Epoch [6/120    avg_loss:0.832, val_acc:0.740]
Epoch [7/120    avg_loss:0.668, val_acc:0.776]
Epoch [8/120    avg_loss:0.646, val_acc:0.812]
Epoch [9/120    avg_loss:0.610, val_acc:0.861]
Epoch [10/120    avg_loss:0.659, val_acc:0.881]
Epoch [11/120    avg_loss:0.553, val_acc:0.871]
Epoch [12/120    avg_loss:0.510, val_acc:0.855]
Epoch [13/120    avg_loss:0.522, val_acc:0.863]
Epoch [14/120    avg_loss:0.505, val_acc:0.865]
Epoch [15/120    avg_loss:0.478, val_acc:0.901]
Epoch [16/120    avg_loss:0.440, val_acc:0.915]
Epoch [17/120    avg_loss:0.388, val_acc:0.899]
Epoch [18/120    avg_loss:0.369, val_acc:0.909]
Epoch [19/120    avg_loss:0.411, val_acc:0.885]
Epoch [20/120    avg_loss:0.376, val_acc:0.909]
Epoch [21/120    avg_loss:0.325, val_acc:0.883]
Epoch [22/120    avg_loss:0.388, val_acc:0.895]
Epoch [23/120    avg_loss:0.391, val_acc:0.942]
Epoch [24/120    avg_loss:0.286, val_acc:0.940]
Epoch [25/120    avg_loss:0.325, val_acc:0.950]
Epoch [26/120    avg_loss:0.293, val_acc:0.952]
Epoch [27/120    avg_loss:0.320, val_acc:0.948]
Epoch [28/120    avg_loss:0.304, val_acc:0.933]
Epoch [29/120    avg_loss:0.252, val_acc:0.933]
Epoch [30/120    avg_loss:0.367, val_acc:0.931]
Epoch [31/120    avg_loss:0.271, val_acc:0.944]
Epoch [32/120    avg_loss:0.218, val_acc:0.944]
Epoch [33/120    avg_loss:0.214, val_acc:0.937]
Epoch [34/120    avg_loss:0.223, val_acc:0.938]
Epoch [35/120    avg_loss:0.221, val_acc:0.933]
Epoch [36/120    avg_loss:0.231, val_acc:0.942]
Epoch [37/120    avg_loss:0.190, val_acc:0.950]
Epoch [38/120    avg_loss:0.186, val_acc:0.944]
Epoch [39/120    avg_loss:0.193, val_acc:0.938]
Epoch [40/120    avg_loss:0.125, val_acc:0.972]
Epoch [41/120    avg_loss:0.093, val_acc:0.976]
Epoch [42/120    avg_loss:0.098, val_acc:0.974]
Epoch [43/120    avg_loss:0.091, val_acc:0.974]
Epoch [44/120    avg_loss:0.100, val_acc:0.974]
Epoch [45/120    avg_loss:0.086, val_acc:0.974]
Epoch [46/120    avg_loss:0.088, val_acc:0.976]
Epoch [47/120    avg_loss:0.094, val_acc:0.978]
Epoch [48/120    avg_loss:0.084, val_acc:0.974]
Epoch [49/120    avg_loss:0.080, val_acc:0.980]
Epoch [50/120    avg_loss:0.084, val_acc:0.982]
Epoch [51/120    avg_loss:0.066, val_acc:0.980]
Epoch [52/120    avg_loss:0.070, val_acc:0.984]
Epoch [53/120    avg_loss:0.069, val_acc:0.980]
Epoch [54/120    avg_loss:0.066, val_acc:0.984]
Epoch [55/120    avg_loss:0.058, val_acc:0.984]
Epoch [56/120    avg_loss:0.085, val_acc:0.986]
Epoch [57/120    avg_loss:0.077, val_acc:0.982]
Epoch [58/120    avg_loss:0.057, val_acc:0.978]
Epoch [59/120    avg_loss:0.060, val_acc:0.982]
Epoch [60/120    avg_loss:0.051, val_acc:0.984]
Epoch [61/120    avg_loss:0.061, val_acc:0.982]
Epoch [62/120    avg_loss:0.059, val_acc:0.984]
Epoch [63/120    avg_loss:0.058, val_acc:0.982]
Epoch [64/120    avg_loss:0.065, val_acc:0.984]
Epoch [65/120    avg_loss:0.055, val_acc:0.984]
Epoch [66/120    avg_loss:0.064, val_acc:0.984]
Epoch [67/120    avg_loss:0.053, val_acc:0.984]
Epoch [68/120    avg_loss:0.063, val_acc:0.984]
Epoch [69/120    avg_loss:0.053, val_acc:0.984]
Epoch [70/120    avg_loss:0.058, val_acc:0.984]
Epoch [71/120    avg_loss:0.054, val_acc:0.984]
Epoch [72/120    avg_loss:0.049, val_acc:0.984]
Epoch [73/120    avg_loss:0.050, val_acc:0.984]
Epoch [74/120    avg_loss:0.048, val_acc:0.984]
Epoch [75/120    avg_loss:0.071, val_acc:0.984]
Epoch [76/120    avg_loss:0.041, val_acc:0.984]
Epoch [77/120    avg_loss:0.059, val_acc:0.984]
Epoch [78/120    avg_loss:0.055, val_acc:0.984]
Epoch [79/120    avg_loss:0.055, val_acc:0.984]
Epoch [80/120    avg_loss:0.061, val_acc:0.984]
Epoch [81/120    avg_loss:0.066, val_acc:0.984]
Epoch [82/120    avg_loss:0.050, val_acc:0.984]
Epoch [83/120    avg_loss:0.056, val_acc:0.984]
Epoch [84/120    avg_loss:0.055, val_acc:0.984]
Epoch [85/120    avg_loss:0.061, val_acc:0.984]
Epoch [86/120    avg_loss:0.053, val_acc:0.984]
Epoch [87/120    avg_loss:0.049, val_acc:0.984]
Epoch [88/120    avg_loss:0.054, val_acc:0.984]
Epoch [89/120    avg_loss:0.053, val_acc:0.984]
Epoch [90/120    avg_loss:0.055, val_acc:0.984]
Epoch [91/120    avg_loss:0.063, val_acc:0.984]
Epoch [92/120    avg_loss:0.060, val_acc:0.984]
Epoch [93/120    avg_loss:0.052, val_acc:0.984]
Epoch [94/120    avg_loss:0.049, val_acc:0.984]
Epoch [95/120    avg_loss:0.066, val_acc:0.984]
Epoch [96/120    avg_loss:0.053, val_acc:0.984]
Epoch [97/120    avg_loss:0.055, val_acc:0.984]
Epoch [98/120    avg_loss:0.049, val_acc:0.984]
Epoch [99/120    avg_loss:0.057, val_acc:0.984]
Epoch [100/120    avg_loss:0.060, val_acc:0.984]
Epoch [101/120    avg_loss:0.061, val_acc:0.984]
Epoch [102/120    avg_loss:0.055, val_acc:0.984]
Epoch [103/120    avg_loss:0.057, val_acc:0.984]
Epoch [104/120    avg_loss:0.055, val_acc:0.984]
Epoch [105/120    avg_loss:0.054, val_acc:0.984]
Epoch [106/120    avg_loss:0.051, val_acc:0.984]
Epoch [107/120    avg_loss:0.059, val_acc:0.984]
Epoch [108/120    avg_loss:0.062, val_acc:0.984]
Epoch [109/120    avg_loss:0.049, val_acc:0.984]
Epoch [110/120    avg_loss:0.058, val_acc:0.984]
Epoch [111/120    avg_loss:0.064, val_acc:0.984]
Epoch [112/120    avg_loss:0.059, val_acc:0.984]
Epoch [113/120    avg_loss:0.048, val_acc:0.984]
Epoch [114/120    avg_loss:0.049, val_acc:0.984]
Epoch [115/120    avg_loss:0.052, val_acc:0.984]
Epoch [116/120    avg_loss:0.054, val_acc:0.984]
Epoch [117/120    avg_loss:0.051, val_acc:0.984]
Epoch [118/120    avg_loss:0.050, val_acc:0.984]
Epoch [119/120    avg_loss:0.056, val_acc:0.984]
Epoch [120/120    avg_loss:0.066, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 202  20   0   0   0   0   0   0   5   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.67803837953092

F1 scores:
[       nan 1.         0.97767857 1.         0.90380313 0.86986301
 1.         0.94382022 1.         1.         1.         0.98817346
 0.98451327 1.        ]

Kappa:
0.9852810952428245
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:63
Validation dataloader:63
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6db4d31860>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.035, val_acc:0.665]
Epoch [2/120    avg_loss:1.274, val_acc:0.651]
Epoch [3/120    avg_loss:0.964, val_acc:0.657]
Epoch [4/120    avg_loss:0.936, val_acc:0.758]
Epoch [5/120    avg_loss:0.846, val_acc:0.776]
Epoch [6/120    avg_loss:0.809, val_acc:0.821]
Epoch [7/120    avg_loss:0.660, val_acc:0.837]
Epoch [8/120    avg_loss:0.619, val_acc:0.829]
Epoch [9/120    avg_loss:0.600, val_acc:0.841]
Epoch [10/120    avg_loss:0.542, val_acc:0.871]
Epoch [11/120    avg_loss:0.544, val_acc:0.760]
Epoch [12/120    avg_loss:0.517, val_acc:0.859]
Epoch [13/120    avg_loss:0.481, val_acc:0.853]
Epoch [14/120    avg_loss:0.461, val_acc:0.887]
Epoch [15/120    avg_loss:0.538, val_acc:0.847]
Epoch [16/120    avg_loss:0.422, val_acc:0.895]
Epoch [17/120    avg_loss:0.385, val_acc:0.865]
Epoch [18/120    avg_loss:0.445, val_acc:0.875]
Epoch [19/120    avg_loss:0.370, val_acc:0.885]
Epoch [20/120    avg_loss:0.383, val_acc:0.903]
Epoch [21/120    avg_loss:0.434, val_acc:0.895]
Epoch [22/120    avg_loss:0.320, val_acc:0.897]
Epoch [23/120    avg_loss:0.386, val_acc:0.901]
Epoch [24/120    avg_loss:0.371, val_acc:0.911]
Epoch [25/120    avg_loss:0.294, val_acc:0.931]
Epoch [26/120    avg_loss:0.319, val_acc:0.927]
Epoch [27/120    avg_loss:0.307, val_acc:0.915]
Epoch [28/120    avg_loss:0.305, val_acc:0.964]
Epoch [29/120    avg_loss:0.257, val_acc:0.913]
Epoch [30/120    avg_loss:0.252, val_acc:0.944]
Epoch [31/120    avg_loss:0.208, val_acc:0.942]
Epoch [32/120    avg_loss:0.219, val_acc:0.877]
Epoch [33/120    avg_loss:0.194, val_acc:0.909]
Epoch [34/120    avg_loss:0.243, val_acc:0.940]
Epoch [35/120    avg_loss:0.179, val_acc:0.925]
Epoch [36/120    avg_loss:0.147, val_acc:0.950]
Epoch [37/120    avg_loss:0.175, val_acc:0.935]
Epoch [38/120    avg_loss:0.384, val_acc:0.929]
Epoch [39/120    avg_loss:0.194, val_acc:0.946]
Epoch [40/120    avg_loss:0.238, val_acc:0.909]
Epoch [41/120    avg_loss:0.236, val_acc:0.960]
Epoch [42/120    avg_loss:0.138, val_acc:0.968]
Epoch [43/120    avg_loss:0.112, val_acc:0.972]
Epoch [44/120    avg_loss:0.097, val_acc:0.972]
Epoch [45/120    avg_loss:0.098, val_acc:0.972]
Epoch [46/120    avg_loss:0.094, val_acc:0.972]
Epoch [47/120    avg_loss:0.087, val_acc:0.974]
Epoch [48/120    avg_loss:0.096, val_acc:0.976]
Epoch [49/120    avg_loss:0.079, val_acc:0.978]
Epoch [50/120    avg_loss:0.082, val_acc:0.974]
Epoch [51/120    avg_loss:0.089, val_acc:0.978]
Epoch [52/120    avg_loss:0.064, val_acc:0.974]
Epoch [53/120    avg_loss:0.093, val_acc:0.978]
Epoch [54/120    avg_loss:0.077, val_acc:0.978]
Epoch [55/120    avg_loss:0.068, val_acc:0.980]
Epoch [56/120    avg_loss:0.066, val_acc:0.978]
Epoch [57/120    avg_loss:0.077, val_acc:0.978]
Epoch [58/120    avg_loss:0.059, val_acc:0.976]
Epoch [59/120    avg_loss:0.051, val_acc:0.980]
Epoch [60/120    avg_loss:0.067, val_acc:0.976]
Epoch [61/120    avg_loss:0.069, val_acc:0.978]
Epoch [62/120    avg_loss:0.060, val_acc:0.978]
Epoch [63/120    avg_loss:0.073, val_acc:0.978]
Epoch [64/120    avg_loss:0.060, val_acc:0.976]
Epoch [65/120    avg_loss:0.052, val_acc:0.982]
Epoch [66/120    avg_loss:0.060, val_acc:0.982]
Epoch [67/120    avg_loss:0.069, val_acc:0.976]
Epoch [68/120    avg_loss:0.057, val_acc:0.978]
Epoch [69/120    avg_loss:0.067, val_acc:0.980]
Epoch [70/120    avg_loss:0.071, val_acc:0.980]
Epoch [71/120    avg_loss:0.048, val_acc:0.980]
Epoch [72/120    avg_loss:0.059, val_acc:0.978]
Epoch [73/120    avg_loss:0.060, val_acc:0.980]
Epoch [74/120    avg_loss:0.058, val_acc:0.980]
Epoch [75/120    avg_loss:0.063, val_acc:0.978]
Epoch [76/120    avg_loss:0.039, val_acc:0.978]
Epoch [77/120    avg_loss:0.045, val_acc:0.980]
Epoch [78/120    avg_loss:0.051, val_acc:0.982]
Epoch [79/120    avg_loss:0.070, val_acc:0.984]
Epoch [80/120    avg_loss:0.044, val_acc:0.978]
Epoch [81/120    avg_loss:0.055, val_acc:0.978]
Epoch [82/120    avg_loss:0.047, val_acc:0.978]
Epoch [83/120    avg_loss:0.056, val_acc:0.990]
Epoch [84/120    avg_loss:0.037, val_acc:0.978]
Epoch [85/120    avg_loss:0.045, val_acc:0.978]
Epoch [86/120    avg_loss:0.053, val_acc:0.980]
Epoch [87/120    avg_loss:0.042, val_acc:0.980]
Epoch [88/120    avg_loss:0.040, val_acc:0.984]
Epoch [89/120    avg_loss:0.040, val_acc:0.988]
Epoch [90/120    avg_loss:0.034, val_acc:0.978]
Epoch [91/120    avg_loss:0.035, val_acc:0.980]
Epoch [92/120    avg_loss:0.049, val_acc:0.980]
Epoch [93/120    avg_loss:0.042, val_acc:0.978]
Epoch [94/120    avg_loss:0.039, val_acc:0.980]
Epoch [95/120    avg_loss:0.044, val_acc:0.982]
Epoch [96/120    avg_loss:0.031, val_acc:0.980]
Epoch [97/120    avg_loss:0.030, val_acc:0.980]
Epoch [98/120    avg_loss:0.029, val_acc:0.980]
Epoch [99/120    avg_loss:0.029, val_acc:0.980]
Epoch [100/120    avg_loss:0.032, val_acc:0.980]
Epoch [101/120    avg_loss:0.029, val_acc:0.980]
Epoch [102/120    avg_loss:0.028, val_acc:0.980]
Epoch [103/120    avg_loss:0.032, val_acc:0.978]
Epoch [104/120    avg_loss:0.032, val_acc:0.978]
Epoch [105/120    avg_loss:0.036, val_acc:0.978]
Epoch [106/120    avg_loss:0.042, val_acc:0.978]
Epoch [107/120    avg_loss:0.033, val_acc:0.978]
Epoch [108/120    avg_loss:0.035, val_acc:0.978]
Epoch [109/120    avg_loss:0.026, val_acc:0.978]
Epoch [110/120    avg_loss:0.033, val_acc:0.978]
Epoch [111/120    avg_loss:0.033, val_acc:0.978]
Epoch [112/120    avg_loss:0.030, val_acc:0.976]
Epoch [113/120    avg_loss:0.039, val_acc:0.978]
Epoch [114/120    avg_loss:0.037, val_acc:0.978]
Epoch [115/120    avg_loss:0.041, val_acc:0.978]
Epoch [116/120    avg_loss:0.033, val_acc:0.978]
Epoch [117/120    avg_loss:0.036, val_acc:0.978]
Epoch [118/120    avg_loss:0.036, val_acc:0.976]
Epoch [119/120    avg_loss:0.039, val_acc:0.976]
Epoch [120/120    avg_loss:0.036, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   2   0   0   0   3   0   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0  17 436   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.74200426439232

F1 scores:
[       nan 1.         0.98426966 0.98901099 0.9321663  0.89965398
 1.         0.96132597 0.99614891 1.         1.         0.97662338
 0.97977528 1.        ]

Kappa:
0.9859943460099577
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe6cccaa780>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.067, val_acc:0.579]
Epoch [2/120    avg_loss:1.350, val_acc:0.669]
Epoch [3/120    avg_loss:0.981, val_acc:0.782]
Epoch [4/120    avg_loss:0.821, val_acc:0.754]
Epoch [5/120    avg_loss:0.871, val_acc:0.808]
Epoch [6/120    avg_loss:0.706, val_acc:0.782]
Epoch [7/120    avg_loss:0.685, val_acc:0.772]
Epoch [8/120    avg_loss:0.661, val_acc:0.841]
Epoch [9/120    avg_loss:0.643, val_acc:0.800]
Epoch [10/120    avg_loss:0.616, val_acc:0.827]
Epoch [11/120    avg_loss:0.519, val_acc:0.837]
Epoch [12/120    avg_loss:0.556, val_acc:0.843]
Epoch [13/120    avg_loss:0.520, val_acc:0.845]
Epoch [14/120    avg_loss:0.482, val_acc:0.885]
Epoch [15/120    avg_loss:0.377, val_acc:0.891]
Epoch [16/120    avg_loss:0.432, val_acc:0.938]
Epoch [17/120    avg_loss:0.344, val_acc:0.925]
Epoch [18/120    avg_loss:0.377, val_acc:0.907]
Epoch [19/120    avg_loss:0.401, val_acc:0.893]
Epoch [20/120    avg_loss:0.453, val_acc:0.917]
Epoch [21/120    avg_loss:0.288, val_acc:0.921]
Epoch [22/120    avg_loss:0.304, val_acc:0.873]
Epoch [23/120    avg_loss:0.286, val_acc:0.944]
Epoch [24/120    avg_loss:0.190, val_acc:0.964]
Epoch [25/120    avg_loss:0.219, val_acc:0.956]
Epoch [26/120    avg_loss:0.216, val_acc:0.925]
Epoch [27/120    avg_loss:0.264, val_acc:0.954]
Epoch [28/120    avg_loss:0.209, val_acc:0.948]
Epoch [29/120    avg_loss:0.217, val_acc:0.952]
Epoch [30/120    avg_loss:0.212, val_acc:0.933]
Epoch [31/120    avg_loss:0.284, val_acc:0.935]
Epoch [32/120    avg_loss:0.201, val_acc:0.919]
Epoch [33/120    avg_loss:0.193, val_acc:0.956]
Epoch [34/120    avg_loss:0.184, val_acc:0.968]
Epoch [35/120    avg_loss:0.154, val_acc:0.976]
Epoch [36/120    avg_loss:0.149, val_acc:0.972]
Epoch [37/120    avg_loss:0.138, val_acc:0.980]
Epoch [38/120    avg_loss:0.136, val_acc:0.931]
Epoch [39/120    avg_loss:0.103, val_acc:0.960]
Epoch [40/120    avg_loss:0.116, val_acc:0.954]
Epoch [41/120    avg_loss:0.101, val_acc:0.976]
Epoch [42/120    avg_loss:0.094, val_acc:0.968]
Epoch [43/120    avg_loss:0.138, val_acc:0.952]
Epoch [44/120    avg_loss:0.119, val_acc:0.958]
Epoch [45/120    avg_loss:0.164, val_acc:0.970]
Epoch [46/120    avg_loss:0.107, val_acc:0.982]
Epoch [47/120    avg_loss:0.082, val_acc:0.996]
Epoch [48/120    avg_loss:0.043, val_acc:0.976]
Epoch [49/120    avg_loss:0.064, val_acc:0.992]
Epoch [50/120    avg_loss:0.093, val_acc:0.982]
Epoch [51/120    avg_loss:0.081, val_acc:0.974]
Epoch [52/120    avg_loss:0.056, val_acc:0.996]
Epoch [53/120    avg_loss:0.047, val_acc:0.990]
Epoch [54/120    avg_loss:0.035, val_acc:0.992]
Epoch [55/120    avg_loss:0.078, val_acc:0.984]
Epoch [56/120    avg_loss:0.058, val_acc:0.974]
Epoch [57/120    avg_loss:0.193, val_acc:0.944]
Epoch [58/120    avg_loss:0.074, val_acc:0.956]
Epoch [59/120    avg_loss:0.174, val_acc:0.946]
Epoch [60/120    avg_loss:0.116, val_acc:0.982]
Epoch [61/120    avg_loss:0.050, val_acc:0.992]
Epoch [62/120    avg_loss:0.046, val_acc:0.980]
Epoch [63/120    avg_loss:0.039, val_acc:0.992]
Epoch [64/120    avg_loss:0.051, val_acc:0.990]
Epoch [65/120    avg_loss:0.065, val_acc:0.980]
Epoch [66/120    avg_loss:0.053, val_acc:0.994]
Epoch [67/120    avg_loss:0.034, val_acc:0.998]
Epoch [68/120    avg_loss:0.028, val_acc:1.000]
Epoch [69/120    avg_loss:0.023, val_acc:1.000]
Epoch [70/120    avg_loss:0.019, val_acc:1.000]
Epoch [71/120    avg_loss:0.023, val_acc:1.000]
Epoch [72/120    avg_loss:0.024, val_acc:0.998]
Epoch [73/120    avg_loss:0.022, val_acc:1.000]
Epoch [74/120    avg_loss:0.020, val_acc:1.000]
Epoch [75/120    avg_loss:0.019, val_acc:1.000]
Epoch [76/120    avg_loss:0.016, val_acc:1.000]
Epoch [77/120    avg_loss:0.014, val_acc:1.000]
Epoch [78/120    avg_loss:0.012, val_acc:1.000]
Epoch [79/120    avg_loss:0.016, val_acc:1.000]
Epoch [80/120    avg_loss:0.014, val_acc:0.998]
Epoch [81/120    avg_loss:0.025, val_acc:1.000]
Epoch [82/120    avg_loss:0.014, val_acc:1.000]
Epoch [83/120    avg_loss:0.021, val_acc:1.000]
Epoch [84/120    avg_loss:0.022, val_acc:0.998]
Epoch [85/120    avg_loss:0.021, val_acc:0.998]
Epoch [86/120    avg_loss:0.016, val_acc:1.000]
Epoch [87/120    avg_loss:0.016, val_acc:1.000]
Epoch [88/120    avg_loss:0.013, val_acc:1.000]
Epoch [89/120    avg_loss:0.013, val_acc:1.000]
Epoch [90/120    avg_loss:0.019, val_acc:1.000]
Epoch [91/120    avg_loss:0.013, val_acc:1.000]
Epoch [92/120    avg_loss:0.012, val_acc:1.000]
Epoch [93/120    avg_loss:0.016, val_acc:1.000]
Epoch [94/120    avg_loss:0.012, val_acc:1.000]
Epoch [95/120    avg_loss:0.013, val_acc:1.000]
Epoch [96/120    avg_loss:0.009, val_acc:1.000]
Epoch [97/120    avg_loss:0.022, val_acc:1.000]
Epoch [98/120    avg_loss:0.015, val_acc:1.000]
Epoch [99/120    avg_loss:0.011, val_acc:0.998]
Epoch [100/120    avg_loss:0.019, val_acc:1.000]
Epoch [101/120    avg_loss:0.015, val_acc:1.000]
Epoch [102/120    avg_loss:0.010, val_acc:1.000]
Epoch [103/120    avg_loss:0.015, val_acc:1.000]
Epoch [104/120    avg_loss:0.010, val_acc:1.000]
Epoch [105/120    avg_loss:0.012, val_acc:1.000]
Epoch [106/120    avg_loss:0.013, val_acc:1.000]
Epoch [107/120    avg_loss:0.011, val_acc:1.000]
Epoch [108/120    avg_loss:0.012, val_acc:1.000]
Epoch [109/120    avg_loss:0.014, val_acc:0.998]
Epoch [110/120    avg_loss:0.010, val_acc:0.998]
Epoch [111/120    avg_loss:0.015, val_acc:0.998]
Epoch [112/120    avg_loss:0.009, val_acc:0.998]
Epoch [113/120    avg_loss:0.012, val_acc:0.998]
Epoch [114/120    avg_loss:0.009, val_acc:0.998]
Epoch [115/120    avg_loss:0.009, val_acc:0.998]
Epoch [116/120    avg_loss:0.012, val_acc:0.998]
Epoch [117/120    avg_loss:0.009, val_acc:0.998]
Epoch [118/120    avg_loss:0.012, val_acc:0.998]
Epoch [119/120    avg_loss:0.010, val_acc:0.998]
Epoch [120/120    avg_loss:0.014, val_acc:1.000]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217   7   0   0   0   0   0   0   3   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 0.99707174 0.9977221  1.         0.94967177 0.92957746
 0.99038462 0.99465241 1.         1.         1.         1.
 0.99669967 1.        ]

Kappa:
0.9933535281746988
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:16:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcfd57a67f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.016, val_acc:0.569]
Epoch [2/120    avg_loss:1.368, val_acc:0.639]
Epoch [3/120    avg_loss:1.016, val_acc:0.798]
Epoch [4/120    avg_loss:0.814, val_acc:0.740]
Epoch [5/120    avg_loss:0.743, val_acc:0.782]
Epoch [6/120    avg_loss:0.692, val_acc:0.835]
Epoch [7/120    avg_loss:0.697, val_acc:0.855]
Epoch [8/120    avg_loss:0.580, val_acc:0.815]
Epoch [9/120    avg_loss:0.550, val_acc:0.855]
Epoch [10/120    avg_loss:0.529, val_acc:0.859]
Epoch [11/120    avg_loss:0.496, val_acc:0.851]
Epoch [12/120    avg_loss:0.450, val_acc:0.907]
Epoch [13/120    avg_loss:0.489, val_acc:0.917]
Epoch [14/120    avg_loss:0.383, val_acc:0.919]
Epoch [15/120    avg_loss:0.470, val_acc:0.907]
Epoch [16/120    avg_loss:0.374, val_acc:0.867]
Epoch [17/120    avg_loss:0.433, val_acc:0.853]
Epoch [18/120    avg_loss:0.328, val_acc:0.901]
Epoch [19/120    avg_loss:0.332, val_acc:0.925]
Epoch [20/120    avg_loss:0.343, val_acc:0.919]
Epoch [21/120    avg_loss:0.327, val_acc:0.935]
Epoch [22/120    avg_loss:0.303, val_acc:0.927]
Epoch [23/120    avg_loss:0.262, val_acc:0.938]
Epoch [24/120    avg_loss:0.242, val_acc:0.935]
Epoch [25/120    avg_loss:0.246, val_acc:0.929]
Epoch [26/120    avg_loss:0.234, val_acc:0.935]
Epoch [27/120    avg_loss:0.286, val_acc:0.966]
Epoch [28/120    avg_loss:0.225, val_acc:0.966]
Epoch [29/120    avg_loss:0.212, val_acc:0.921]
Epoch [30/120    avg_loss:0.193, val_acc:0.938]
Epoch [31/120    avg_loss:0.177, val_acc:0.958]
Epoch [32/120    avg_loss:0.278, val_acc:0.962]
Epoch [33/120    avg_loss:0.195, val_acc:0.923]
Epoch [34/120    avg_loss:0.199, val_acc:0.931]
Epoch [35/120    avg_loss:0.154, val_acc:0.960]
Epoch [36/120    avg_loss:0.157, val_acc:0.972]
Epoch [37/120    avg_loss:0.159, val_acc:0.952]
Epoch [38/120    avg_loss:0.132, val_acc:0.931]
Epoch [39/120    avg_loss:0.104, val_acc:0.960]
Epoch [40/120    avg_loss:0.189, val_acc:0.927]
Epoch [41/120    avg_loss:0.191, val_acc:0.944]
Epoch [42/120    avg_loss:0.157, val_acc:0.974]
Epoch [43/120    avg_loss:0.140, val_acc:0.974]
Epoch [44/120    avg_loss:0.084, val_acc:0.976]
Epoch [45/120    avg_loss:0.116, val_acc:0.986]
Epoch [46/120    avg_loss:0.081, val_acc:0.919]
Epoch [47/120    avg_loss:0.070, val_acc:0.984]
Epoch [48/120    avg_loss:0.104, val_acc:0.970]
Epoch [49/120    avg_loss:0.062, val_acc:0.988]
Epoch [50/120    avg_loss:0.082, val_acc:0.976]
Epoch [51/120    avg_loss:0.126, val_acc:0.968]
Epoch [52/120    avg_loss:0.113, val_acc:0.976]
Epoch [53/120    avg_loss:0.094, val_acc:0.966]
Epoch [54/120    avg_loss:0.097, val_acc:0.982]
Epoch [55/120    avg_loss:0.077, val_acc:0.950]
Epoch [56/120    avg_loss:0.070, val_acc:0.944]
Epoch [57/120    avg_loss:0.071, val_acc:0.964]
Epoch [58/120    avg_loss:0.071, val_acc:0.984]
Epoch [59/120    avg_loss:0.058, val_acc:0.986]
Epoch [60/120    avg_loss:0.057, val_acc:0.986]
Epoch [61/120    avg_loss:0.077, val_acc:0.917]
Epoch [62/120    avg_loss:0.112, val_acc:0.970]
Epoch [63/120    avg_loss:0.062, val_acc:0.992]
Epoch [64/120    avg_loss:0.036, val_acc:0.992]
Epoch [65/120    avg_loss:0.025, val_acc:0.992]
Epoch [66/120    avg_loss:0.039, val_acc:0.994]
Epoch [67/120    avg_loss:0.019, val_acc:0.992]
Epoch [68/120    avg_loss:0.037, val_acc:0.992]
Epoch [69/120    avg_loss:0.029, val_acc:0.990]
Epoch [70/120    avg_loss:0.026, val_acc:0.992]
Epoch [71/120    avg_loss:0.030, val_acc:0.990]
Epoch [72/120    avg_loss:0.023, val_acc:0.990]
Epoch [73/120    avg_loss:0.020, val_acc:0.992]
Epoch [74/120    avg_loss:0.020, val_acc:0.992]
Epoch [75/120    avg_loss:0.026, val_acc:0.988]
Epoch [76/120    avg_loss:0.026, val_acc:0.990]
Epoch [77/120    avg_loss:0.017, val_acc:0.988]
Epoch [78/120    avg_loss:0.025, val_acc:0.990]
Epoch [79/120    avg_loss:0.024, val_acc:0.992]
Epoch [80/120    avg_loss:0.024, val_acc:0.992]
Epoch [81/120    avg_loss:0.020, val_acc:0.992]
Epoch [82/120    avg_loss:0.018, val_acc:0.992]
Epoch [83/120    avg_loss:0.019, val_acc:0.990]
Epoch [84/120    avg_loss:0.023, val_acc:0.990]
Epoch [85/120    avg_loss:0.026, val_acc:0.990]
Epoch [86/120    avg_loss:0.022, val_acc:0.990]
Epoch [87/120    avg_loss:0.028, val_acc:0.988]
Epoch [88/120    avg_loss:0.018, val_acc:0.988]
Epoch [89/120    avg_loss:0.024, val_acc:0.988]
Epoch [90/120    avg_loss:0.019, val_acc:0.990]
Epoch [91/120    avg_loss:0.025, val_acc:0.988]
Epoch [92/120    avg_loss:0.020, val_acc:0.988]
Epoch [93/120    avg_loss:0.019, val_acc:0.990]
Epoch [94/120    avg_loss:0.019, val_acc:0.990]
Epoch [95/120    avg_loss:0.018, val_acc:0.990]
Epoch [96/120    avg_loss:0.019, val_acc:0.990]
Epoch [97/120    avg_loss:0.017, val_acc:0.990]
Epoch [98/120    avg_loss:0.027, val_acc:0.990]
Epoch [99/120    avg_loss:0.021, val_acc:0.990]
Epoch [100/120    avg_loss:0.022, val_acc:0.990]
Epoch [101/120    avg_loss:0.015, val_acc:0.990]
Epoch [102/120    avg_loss:0.017, val_acc:0.990]
Epoch [103/120    avg_loss:0.018, val_acc:0.990]
Epoch [104/120    avg_loss:0.023, val_acc:0.990]
Epoch [105/120    avg_loss:0.022, val_acc:0.990]
Epoch [106/120    avg_loss:0.019, val_acc:0.990]
Epoch [107/120    avg_loss:0.016, val_acc:0.990]
Epoch [108/120    avg_loss:0.018, val_acc:0.990]
Epoch [109/120    avg_loss:0.029, val_acc:0.990]
Epoch [110/120    avg_loss:0.019, val_acc:0.990]
Epoch [111/120    avg_loss:0.018, val_acc:0.990]
Epoch [112/120    avg_loss:0.016, val_acc:0.990]
Epoch [113/120    avg_loss:0.013, val_acc:0.990]
Epoch [114/120    avg_loss:0.025, val_acc:0.990]
Epoch [115/120    avg_loss:0.017, val_acc:0.990]
Epoch [116/120    avg_loss:0.024, val_acc:0.990]
Epoch [117/120    avg_loss:0.020, val_acc:0.990]
Epoch [118/120    avg_loss:0.018, val_acc:0.990]
Epoch [119/120    avg_loss:0.021, val_acc:0.990]
Epoch [120/120    avg_loss:0.027, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 211  11   0   0   0   0   0   0   5   0]
 [  0   0   0   0  15 129   1   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.12579957356077

F1 scores:
[       nan 0.99853801 0.98871332 0.99782135 0.93156733 0.90526316
 0.99277108 0.9726776  0.998713   1.         1.         0.99867198
 0.99342105 1.        ]

Kappa:
0.9902666607949763
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4bf99be780>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.034, val_acc:0.657]
Epoch [2/120    avg_loss:1.285, val_acc:0.786]
Epoch [3/120    avg_loss:1.024, val_acc:0.732]
Epoch [4/120    avg_loss:0.845, val_acc:0.833]
Epoch [5/120    avg_loss:0.701, val_acc:0.849]
Epoch [6/120    avg_loss:0.647, val_acc:0.839]
Epoch [7/120    avg_loss:0.620, val_acc:0.887]
Epoch [8/120    avg_loss:0.550, val_acc:0.879]
Epoch [9/120    avg_loss:0.499, val_acc:0.879]
Epoch [10/120    avg_loss:0.492, val_acc:0.885]
Epoch [11/120    avg_loss:0.444, val_acc:0.879]
Epoch [12/120    avg_loss:0.496, val_acc:0.887]
Epoch [13/120    avg_loss:0.384, val_acc:0.877]
Epoch [14/120    avg_loss:0.381, val_acc:0.907]
Epoch [15/120    avg_loss:0.334, val_acc:0.865]
Epoch [16/120    avg_loss:0.339, val_acc:0.891]
Epoch [17/120    avg_loss:0.337, val_acc:0.923]
Epoch [18/120    avg_loss:0.369, val_acc:0.919]
Epoch [19/120    avg_loss:0.343, val_acc:0.917]
Epoch [20/120    avg_loss:0.281, val_acc:0.921]
Epoch [21/120    avg_loss:0.339, val_acc:0.925]
Epoch [22/120    avg_loss:0.278, val_acc:0.933]
Epoch [23/120    avg_loss:0.258, val_acc:0.917]
Epoch [24/120    avg_loss:0.256, val_acc:0.907]
Epoch [25/120    avg_loss:0.225, val_acc:0.909]
Epoch [26/120    avg_loss:0.220, val_acc:0.917]
Epoch [27/120    avg_loss:0.203, val_acc:0.966]
Epoch [28/120    avg_loss:0.182, val_acc:0.938]
Epoch [29/120    avg_loss:0.225, val_acc:0.933]
Epoch [30/120    avg_loss:0.155, val_acc:0.940]
Epoch [31/120    avg_loss:0.193, val_acc:0.962]
Epoch [32/120    avg_loss:0.162, val_acc:0.942]
Epoch [33/120    avg_loss:0.172, val_acc:0.940]
Epoch [34/120    avg_loss:0.137, val_acc:0.964]
Epoch [35/120    avg_loss:0.134, val_acc:0.974]
Epoch [36/120    avg_loss:0.125, val_acc:0.966]
Epoch [37/120    avg_loss:0.136, val_acc:0.925]
Epoch [38/120    avg_loss:0.125, val_acc:0.958]
Epoch [39/120    avg_loss:0.158, val_acc:0.940]
Epoch [40/120    avg_loss:0.153, val_acc:0.954]
Epoch [41/120    avg_loss:0.094, val_acc:0.970]
Epoch [42/120    avg_loss:0.121, val_acc:0.917]
Epoch [43/120    avg_loss:0.137, val_acc:0.956]
Epoch [44/120    avg_loss:0.149, val_acc:0.972]
Epoch [45/120    avg_loss:0.105, val_acc:0.954]
Epoch [46/120    avg_loss:0.072, val_acc:0.972]
Epoch [47/120    avg_loss:0.091, val_acc:0.980]
Epoch [48/120    avg_loss:0.099, val_acc:0.964]
Epoch [49/120    avg_loss:0.132, val_acc:0.962]
Epoch [50/120    avg_loss:0.110, val_acc:0.964]
Epoch [51/120    avg_loss:0.103, val_acc:0.966]
Epoch [52/120    avg_loss:0.118, val_acc:0.978]
Epoch [53/120    avg_loss:0.075, val_acc:0.982]
Epoch [54/120    avg_loss:0.048, val_acc:0.978]
Epoch [55/120    avg_loss:0.042, val_acc:0.976]
Epoch [56/120    avg_loss:0.064, val_acc:0.988]
Epoch [57/120    avg_loss:0.089, val_acc:0.976]
Epoch [58/120    avg_loss:0.064, val_acc:0.980]
Epoch [59/120    avg_loss:0.043, val_acc:0.978]
Epoch [60/120    avg_loss:0.044, val_acc:0.976]
Epoch [61/120    avg_loss:0.044, val_acc:0.984]
Epoch [62/120    avg_loss:0.101, val_acc:0.984]
Epoch [63/120    avg_loss:0.056, val_acc:0.982]
Epoch [64/120    avg_loss:0.061, val_acc:0.966]
Epoch [65/120    avg_loss:0.069, val_acc:0.988]
Epoch [66/120    avg_loss:0.165, val_acc:0.893]
Epoch [67/120    avg_loss:0.197, val_acc:0.946]
Epoch [68/120    avg_loss:0.069, val_acc:0.980]
Epoch [69/120    avg_loss:0.093, val_acc:0.966]
Epoch [70/120    avg_loss:0.059, val_acc:0.980]
Epoch [71/120    avg_loss:0.104, val_acc:0.972]
Epoch [72/120    avg_loss:0.061, val_acc:0.950]
Epoch [73/120    avg_loss:0.085, val_acc:0.982]
Epoch [74/120    avg_loss:0.038, val_acc:0.984]
Epoch [75/120    avg_loss:0.030, val_acc:0.986]
Epoch [76/120    avg_loss:0.024, val_acc:0.990]
Epoch [77/120    avg_loss:0.017, val_acc:0.990]
Epoch [78/120    avg_loss:0.035, val_acc:0.992]
Epoch [79/120    avg_loss:0.029, val_acc:0.986]
Epoch [80/120    avg_loss:0.018, val_acc:0.978]
Epoch [81/120    avg_loss:0.024, val_acc:0.990]
Epoch [82/120    avg_loss:0.009, val_acc:0.992]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.074, val_acc:0.962]
Epoch [85/120    avg_loss:0.066, val_acc:0.978]
Epoch [86/120    avg_loss:0.038, val_acc:0.970]
Epoch [87/120    avg_loss:0.019, val_acc:0.988]
Epoch [88/120    avg_loss:0.020, val_acc:0.986]
Epoch [89/120    avg_loss:0.013, val_acc:0.990]
Epoch [90/120    avg_loss:0.034, val_acc:0.980]
Epoch [91/120    avg_loss:0.021, val_acc:0.984]
Epoch [92/120    avg_loss:0.033, val_acc:0.984]
Epoch [93/120    avg_loss:0.014, val_acc:0.982]
Epoch [94/120    avg_loss:0.012, val_acc:0.988]
Epoch [95/120    avg_loss:0.022, val_acc:0.990]
Epoch [96/120    avg_loss:0.011, val_acc:0.990]
Epoch [97/120    avg_loss:0.012, val_acc:0.994]
Epoch [98/120    avg_loss:0.009, val_acc:0.990]
Epoch [99/120    avg_loss:0.008, val_acc:0.990]
Epoch [100/120    avg_loss:0.014, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.009, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.990]
Epoch [104/120    avg_loss:0.010, val_acc:0.990]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.007, val_acc:0.988]
Epoch [107/120    avg_loss:0.008, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.006, val_acc:0.988]
Epoch [110/120    avg_loss:0.009, val_acc:0.988]
Epoch [111/120    avg_loss:0.010, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.006, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   7   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.48827292110875

F1 scores:
[       nan 1.         1.         0.99563319 0.94827586 0.92198582
 1.         1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9943026576462121
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6b585717b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.040, val_acc:0.602]
Epoch [2/120    avg_loss:1.337, val_acc:0.672]
Epoch [3/120    avg_loss:1.052, val_acc:0.750]
Epoch [4/120    avg_loss:0.943, val_acc:0.732]
Epoch [5/120    avg_loss:0.884, val_acc:0.766]
Epoch [6/120    avg_loss:0.761, val_acc:0.814]
Epoch [7/120    avg_loss:0.727, val_acc:0.824]
Epoch [8/120    avg_loss:0.652, val_acc:0.848]
Epoch [9/120    avg_loss:0.578, val_acc:0.852]
Epoch [10/120    avg_loss:0.580, val_acc:0.848]
Epoch [11/120    avg_loss:0.511, val_acc:0.852]
Epoch [12/120    avg_loss:0.523, val_acc:0.844]
Epoch [13/120    avg_loss:0.496, val_acc:0.879]
Epoch [14/120    avg_loss:0.492, val_acc:0.867]
Epoch [15/120    avg_loss:0.400, val_acc:0.916]
Epoch [16/120    avg_loss:0.406, val_acc:0.902]
Epoch [17/120    avg_loss:0.339, val_acc:0.908]
Epoch [18/120    avg_loss:0.349, val_acc:0.918]
Epoch [19/120    avg_loss:0.305, val_acc:0.895]
Epoch [20/120    avg_loss:0.373, val_acc:0.932]
Epoch [21/120    avg_loss:0.277, val_acc:0.945]
Epoch [22/120    avg_loss:0.243, val_acc:0.918]
Epoch [23/120    avg_loss:0.289, val_acc:0.936]
Epoch [24/120    avg_loss:0.253, val_acc:0.951]
Epoch [25/120    avg_loss:0.197, val_acc:0.947]
Epoch [26/120    avg_loss:0.282, val_acc:0.902]
Epoch [27/120    avg_loss:0.230, val_acc:0.939]
Epoch [28/120    avg_loss:0.203, val_acc:0.906]
Epoch [29/120    avg_loss:0.260, val_acc:0.945]
Epoch [30/120    avg_loss:0.213, val_acc:0.887]
Epoch [31/120    avg_loss:0.174, val_acc:0.926]
Epoch [32/120    avg_loss:0.170, val_acc:0.951]
Epoch [33/120    avg_loss:0.159, val_acc:0.955]
Epoch [34/120    avg_loss:0.135, val_acc:0.957]
Epoch [35/120    avg_loss:0.130, val_acc:0.953]
Epoch [36/120    avg_loss:0.177, val_acc:0.936]
Epoch [37/120    avg_loss:0.160, val_acc:0.959]
Epoch [38/120    avg_loss:0.111, val_acc:0.955]
Epoch [39/120    avg_loss:0.156, val_acc:0.932]
Epoch [40/120    avg_loss:0.201, val_acc:0.932]
Epoch [41/120    avg_loss:0.241, val_acc:0.951]
Epoch [42/120    avg_loss:0.137, val_acc:0.951]
Epoch [43/120    avg_loss:0.114, val_acc:0.965]
Epoch [44/120    avg_loss:0.106, val_acc:0.953]
Epoch [45/120    avg_loss:0.125, val_acc:0.943]
Epoch [46/120    avg_loss:0.111, val_acc:0.963]
Epoch [47/120    avg_loss:0.091, val_acc:0.965]
Epoch [48/120    avg_loss:0.081, val_acc:0.967]
Epoch [49/120    avg_loss:0.089, val_acc:0.973]
Epoch [50/120    avg_loss:0.088, val_acc:0.963]
Epoch [51/120    avg_loss:0.082, val_acc:0.932]
Epoch [52/120    avg_loss:0.068, val_acc:0.986]
Epoch [53/120    avg_loss:0.138, val_acc:0.975]
Epoch [54/120    avg_loss:0.115, val_acc:0.971]
Epoch [55/120    avg_loss:0.064, val_acc:0.980]
Epoch [56/120    avg_loss:0.096, val_acc:0.971]
Epoch [57/120    avg_loss:0.100, val_acc:0.959]
Epoch [58/120    avg_loss:0.074, val_acc:0.965]
Epoch [59/120    avg_loss:0.077, val_acc:0.971]
Epoch [60/120    avg_loss:0.064, val_acc:0.969]
Epoch [61/120    avg_loss:0.077, val_acc:0.975]
Epoch [62/120    avg_loss:0.057, val_acc:0.939]
Epoch [63/120    avg_loss:0.088, val_acc:0.986]
Epoch [64/120    avg_loss:0.042, val_acc:0.986]
Epoch [65/120    avg_loss:0.109, val_acc:0.975]
Epoch [66/120    avg_loss:0.083, val_acc:0.965]
Epoch [67/120    avg_loss:0.050, val_acc:0.988]
Epoch [68/120    avg_loss:0.035, val_acc:0.984]
Epoch [69/120    avg_loss:0.070, val_acc:0.963]
Epoch [70/120    avg_loss:0.143, val_acc:0.980]
Epoch [71/120    avg_loss:0.037, val_acc:0.986]
Epoch [72/120    avg_loss:0.035, val_acc:0.982]
Epoch [73/120    avg_loss:0.043, val_acc:0.982]
Epoch [74/120    avg_loss:0.049, val_acc:0.963]
Epoch [75/120    avg_loss:0.038, val_acc:0.982]
Epoch [76/120    avg_loss:0.033, val_acc:0.986]
Epoch [77/120    avg_loss:0.019, val_acc:0.990]
Epoch [78/120    avg_loss:0.023, val_acc:0.988]
Epoch [79/120    avg_loss:0.019, val_acc:0.986]
Epoch [80/120    avg_loss:0.018, val_acc:0.986]
Epoch [81/120    avg_loss:0.045, val_acc:0.957]
Epoch [82/120    avg_loss:0.033, val_acc:0.984]
Epoch [83/120    avg_loss:0.038, val_acc:0.992]
Epoch [84/120    avg_loss:0.025, val_acc:0.994]
Epoch [85/120    avg_loss:0.014, val_acc:0.988]
Epoch [86/120    avg_loss:0.010, val_acc:0.988]
Epoch [87/120    avg_loss:0.016, val_acc:0.994]
Epoch [88/120    avg_loss:0.012, val_acc:0.988]
Epoch [89/120    avg_loss:0.023, val_acc:0.988]
Epoch [90/120    avg_loss:0.013, val_acc:0.988]
Epoch [91/120    avg_loss:0.013, val_acc:0.992]
Epoch [92/120    avg_loss:0.021, val_acc:0.990]
Epoch [93/120    avg_loss:0.021, val_acc:0.992]
Epoch [94/120    avg_loss:0.023, val_acc:0.984]
Epoch [95/120    avg_loss:0.017, val_acc:0.973]
Epoch [96/120    avg_loss:0.022, val_acc:0.977]
Epoch [97/120    avg_loss:0.006, val_acc:0.990]
Epoch [98/120    avg_loss:0.009, val_acc:0.990]
Epoch [99/120    avg_loss:0.008, val_acc:0.986]
Epoch [100/120    avg_loss:0.007, val_acc:0.990]
Epoch [101/120    avg_loss:0.008, val_acc:0.992]
Epoch [102/120    avg_loss:0.008, val_acc:0.992]
Epoch [103/120    avg_loss:0.011, val_acc:0.992]
Epoch [104/120    avg_loss:0.005, val_acc:0.992]
Epoch [105/120    avg_loss:0.006, val_acc:0.992]
Epoch [106/120    avg_loss:0.010, val_acc:0.994]
Epoch [107/120    avg_loss:0.007, val_acc:0.990]
Epoch [108/120    avg_loss:0.007, val_acc:0.992]
Epoch [109/120    avg_loss:0.009, val_acc:0.990]
Epoch [110/120    avg_loss:0.004, val_acc:0.992]
Epoch [111/120    avg_loss:0.005, val_acc:0.992]
Epoch [112/120    avg_loss:0.014, val_acc:0.994]
Epoch [113/120    avg_loss:0.007, val_acc:0.992]
Epoch [114/120    avg_loss:0.007, val_acc:0.992]
Epoch [115/120    avg_loss:0.007, val_acc:0.992]
Epoch [116/120    avg_loss:0.005, val_acc:0.994]
Epoch [117/120    avg_loss:0.006, val_acc:0.994]
Epoch [118/120    avg_loss:0.008, val_acc:0.992]
Epoch [119/120    avg_loss:0.008, val_acc:0.992]
Epoch [120/120    avg_loss:0.005, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  10   0   0   0   0   0   0   5   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 0.99853801 0.99545455 1.         0.92982456 0.90459364
 0.99516908 0.98924731 1.         1.         1.         1.
 0.99451153 1.        ]

Kappa:
0.991453836799058
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:17:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f79f12af7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.010, val_acc:0.649]
Epoch [2/120    avg_loss:1.325, val_acc:0.732]
Epoch [3/120    avg_loss:1.027, val_acc:0.748]
Epoch [4/120    avg_loss:0.858, val_acc:0.750]
Epoch [5/120    avg_loss:0.787, val_acc:0.819]
Epoch [6/120    avg_loss:0.692, val_acc:0.843]
Epoch [7/120    avg_loss:0.629, val_acc:0.853]
Epoch [8/120    avg_loss:0.617, val_acc:0.875]
Epoch [9/120    avg_loss:0.561, val_acc:0.855]
Epoch [10/120    avg_loss:0.478, val_acc:0.871]
Epoch [11/120    avg_loss:0.535, val_acc:0.859]
Epoch [12/120    avg_loss:0.477, val_acc:0.901]
Epoch [13/120    avg_loss:0.412, val_acc:0.915]
Epoch [14/120    avg_loss:0.322, val_acc:0.903]
Epoch [15/120    avg_loss:0.409, val_acc:0.873]
Epoch [16/120    avg_loss:0.375, val_acc:0.929]
Epoch [17/120    avg_loss:0.362, val_acc:0.903]
Epoch [18/120    avg_loss:0.297, val_acc:0.921]
Epoch [19/120    avg_loss:0.303, val_acc:0.931]
Epoch [20/120    avg_loss:0.331, val_acc:0.933]
Epoch [21/120    avg_loss:0.318, val_acc:0.897]
Epoch [22/120    avg_loss:0.258, val_acc:0.946]
Epoch [23/120    avg_loss:0.249, val_acc:0.923]
Epoch [24/120    avg_loss:0.360, val_acc:0.931]
Epoch [25/120    avg_loss:0.254, val_acc:0.907]
Epoch [26/120    avg_loss:0.334, val_acc:0.929]
Epoch [27/120    avg_loss:0.311, val_acc:0.952]
Epoch [28/120    avg_loss:0.250, val_acc:0.956]
Epoch [29/120    avg_loss:0.190, val_acc:0.952]
Epoch [30/120    avg_loss:0.242, val_acc:0.944]
Epoch [31/120    avg_loss:0.176, val_acc:0.944]
Epoch [32/120    avg_loss:0.177, val_acc:0.921]
Epoch [33/120    avg_loss:0.275, val_acc:0.899]
Epoch [34/120    avg_loss:0.285, val_acc:0.950]
Epoch [35/120    avg_loss:0.191, val_acc:0.948]
Epoch [36/120    avg_loss:0.116, val_acc:0.960]
Epoch [37/120    avg_loss:0.152, val_acc:0.960]
Epoch [38/120    avg_loss:0.120, val_acc:0.964]
Epoch [39/120    avg_loss:0.119, val_acc:0.968]
Epoch [40/120    avg_loss:0.140, val_acc:0.976]
Epoch [41/120    avg_loss:0.101, val_acc:0.976]
Epoch [42/120    avg_loss:0.072, val_acc:0.976]
Epoch [43/120    avg_loss:0.091, val_acc:0.940]
Epoch [44/120    avg_loss:0.140, val_acc:0.962]
Epoch [45/120    avg_loss:0.102, val_acc:0.976]
Epoch [46/120    avg_loss:0.121, val_acc:0.960]
Epoch [47/120    avg_loss:0.104, val_acc:0.980]
Epoch [48/120    avg_loss:0.079, val_acc:0.962]
Epoch [49/120    avg_loss:0.059, val_acc:0.978]
Epoch [50/120    avg_loss:0.055, val_acc:0.986]
Epoch [51/120    avg_loss:0.038, val_acc:0.986]
Epoch [52/120    avg_loss:0.080, val_acc:0.982]
Epoch [53/120    avg_loss:0.117, val_acc:0.974]
Epoch [54/120    avg_loss:0.065, val_acc:0.964]
Epoch [55/120    avg_loss:0.080, val_acc:0.984]
Epoch [56/120    avg_loss:0.061, val_acc:0.980]
Epoch [57/120    avg_loss:0.063, val_acc:0.970]
Epoch [58/120    avg_loss:0.080, val_acc:0.982]
Epoch [59/120    avg_loss:0.047, val_acc:0.978]
Epoch [60/120    avg_loss:0.032, val_acc:0.986]
Epoch [61/120    avg_loss:0.037, val_acc:0.992]
Epoch [62/120    avg_loss:0.023, val_acc:0.990]
Epoch [63/120    avg_loss:0.019, val_acc:0.990]
Epoch [64/120    avg_loss:0.028, val_acc:0.980]
Epoch [65/120    avg_loss:0.020, val_acc:0.982]
Epoch [66/120    avg_loss:0.029, val_acc:0.980]
Epoch [67/120    avg_loss:0.026, val_acc:0.978]
Epoch [68/120    avg_loss:0.037, val_acc:0.984]
Epoch [69/120    avg_loss:0.019, val_acc:0.988]
Epoch [70/120    avg_loss:0.019, val_acc:0.988]
Epoch [71/120    avg_loss:0.021, val_acc:0.990]
Epoch [72/120    avg_loss:0.014, val_acc:0.986]
Epoch [73/120    avg_loss:0.020, val_acc:0.990]
Epoch [74/120    avg_loss:0.013, val_acc:0.988]
Epoch [75/120    avg_loss:0.012, val_acc:0.990]
Epoch [76/120    avg_loss:0.010, val_acc:0.990]
Epoch [77/120    avg_loss:0.011, val_acc:0.990]
Epoch [78/120    avg_loss:0.011, val_acc:0.990]
Epoch [79/120    avg_loss:0.009, val_acc:0.988]
Epoch [80/120    avg_loss:0.010, val_acc:0.990]
Epoch [81/120    avg_loss:0.009, val_acc:0.990]
Epoch [82/120    avg_loss:0.007, val_acc:0.990]
Epoch [83/120    avg_loss:0.012, val_acc:0.990]
Epoch [84/120    avg_loss:0.008, val_acc:0.990]
Epoch [85/120    avg_loss:0.007, val_acc:0.988]
Epoch [86/120    avg_loss:0.009, val_acc:0.988]
Epoch [87/120    avg_loss:0.008, val_acc:0.988]
Epoch [88/120    avg_loss:0.011, val_acc:0.988]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.008, val_acc:0.988]
Epoch [91/120    avg_loss:0.011, val_acc:0.988]
Epoch [92/120    avg_loss:0.008, val_acc:0.988]
Epoch [93/120    avg_loss:0.008, val_acc:0.988]
Epoch [94/120    avg_loss:0.011, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.988]
Epoch [96/120    avg_loss:0.011, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.009, val_acc:0.988]
Epoch [99/120    avg_loss:0.012, val_acc:0.988]
Epoch [100/120    avg_loss:0.010, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.009, val_acc:0.988]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.008, val_acc:0.988]
Epoch [107/120    avg_loss:0.010, val_acc:0.988]
Epoch [108/120    avg_loss:0.010, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.007, val_acc:0.988]
Epoch [113/120    avg_loss:0.011, val_acc:0.988]
Epoch [114/120    avg_loss:0.010, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.012, val_acc:0.988]
Epoch [117/120    avg_loss:0.009, val_acc:0.988]
Epoch [118/120    avg_loss:0.012, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.015, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216   7   0   0   0   0   0   0   4   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 1.         1.         1.         0.93913043 0.91428571
 1.         1.         1.         1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9933527392312806
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff2bb9a47f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.000, val_acc:0.738]
Epoch [2/120    avg_loss:1.273, val_acc:0.746]
Epoch [3/120    avg_loss:0.997, val_acc:0.788]
Epoch [4/120    avg_loss:0.859, val_acc:0.817]
Epoch [5/120    avg_loss:0.858, val_acc:0.790]
Epoch [6/120    avg_loss:0.692, val_acc:0.883]
Epoch [7/120    avg_loss:0.568, val_acc:0.881]
Epoch [8/120    avg_loss:0.475, val_acc:0.863]
Epoch [9/120    avg_loss:0.524, val_acc:0.889]
Epoch [10/120    avg_loss:0.510, val_acc:0.893]
Epoch [11/120    avg_loss:0.467, val_acc:0.919]
Epoch [12/120    avg_loss:0.469, val_acc:0.869]
Epoch [13/120    avg_loss:0.470, val_acc:0.919]
Epoch [14/120    avg_loss:0.358, val_acc:0.942]
Epoch [15/120    avg_loss:0.381, val_acc:0.917]
Epoch [16/120    avg_loss:0.310, val_acc:0.899]
Epoch [17/120    avg_loss:0.355, val_acc:0.940]
Epoch [18/120    avg_loss:0.324, val_acc:0.929]
Epoch [19/120    avg_loss:0.267, val_acc:0.954]
Epoch [20/120    avg_loss:0.261, val_acc:0.931]
Epoch [21/120    avg_loss:0.334, val_acc:0.919]
Epoch [22/120    avg_loss:0.304, val_acc:0.954]
Epoch [23/120    avg_loss:0.262, val_acc:0.948]
Epoch [24/120    avg_loss:0.252, val_acc:0.931]
Epoch [25/120    avg_loss:0.311, val_acc:0.929]
Epoch [26/120    avg_loss:0.258, val_acc:0.964]
Epoch [27/120    avg_loss:0.191, val_acc:0.935]
Epoch [28/120    avg_loss:0.205, val_acc:0.935]
Epoch [29/120    avg_loss:0.267, val_acc:0.923]
Epoch [30/120    avg_loss:0.203, val_acc:0.950]
Epoch [31/120    avg_loss:0.202, val_acc:0.954]
Epoch [32/120    avg_loss:0.185, val_acc:0.962]
Epoch [33/120    avg_loss:0.193, val_acc:0.946]
Epoch [34/120    avg_loss:0.114, val_acc:0.968]
Epoch [35/120    avg_loss:0.159, val_acc:0.938]
Epoch [36/120    avg_loss:0.178, val_acc:0.946]
Epoch [37/120    avg_loss:0.187, val_acc:0.960]
Epoch [38/120    avg_loss:0.115, val_acc:0.980]
Epoch [39/120    avg_loss:0.163, val_acc:0.948]
Epoch [40/120    avg_loss:0.138, val_acc:0.972]
Epoch [41/120    avg_loss:0.126, val_acc:0.964]
Epoch [42/120    avg_loss:0.085, val_acc:0.952]
Epoch [43/120    avg_loss:0.158, val_acc:0.935]
Epoch [44/120    avg_loss:0.196, val_acc:0.954]
Epoch [45/120    avg_loss:0.098, val_acc:0.974]
Epoch [46/120    avg_loss:0.143, val_acc:0.921]
Epoch [47/120    avg_loss:0.106, val_acc:0.970]
Epoch [48/120    avg_loss:0.096, val_acc:0.962]
Epoch [49/120    avg_loss:0.086, val_acc:0.978]
Epoch [50/120    avg_loss:0.100, val_acc:0.970]
Epoch [51/120    avg_loss:0.144, val_acc:0.964]
Epoch [52/120    avg_loss:0.076, val_acc:0.982]
Epoch [53/120    avg_loss:0.053, val_acc:0.986]
Epoch [54/120    avg_loss:0.053, val_acc:0.986]
Epoch [55/120    avg_loss:0.051, val_acc:0.984]
Epoch [56/120    avg_loss:0.051, val_acc:0.986]
Epoch [57/120    avg_loss:0.040, val_acc:0.986]
Epoch [58/120    avg_loss:0.051, val_acc:0.986]
Epoch [59/120    avg_loss:0.046, val_acc:0.986]
Epoch [60/120    avg_loss:0.049, val_acc:0.986]
Epoch [61/120    avg_loss:0.028, val_acc:0.986]
Epoch [62/120    avg_loss:0.046, val_acc:0.986]
Epoch [63/120    avg_loss:0.039, val_acc:0.986]
Epoch [64/120    avg_loss:0.037, val_acc:0.986]
Epoch [65/120    avg_loss:0.035, val_acc:0.986]
Epoch [66/120    avg_loss:0.032, val_acc:0.984]
Epoch [67/120    avg_loss:0.040, val_acc:0.986]
Epoch [68/120    avg_loss:0.038, val_acc:0.986]
Epoch [69/120    avg_loss:0.031, val_acc:0.986]
Epoch [70/120    avg_loss:0.037, val_acc:0.984]
Epoch [71/120    avg_loss:0.040, val_acc:0.986]
Epoch [72/120    avg_loss:0.037, val_acc:0.984]
Epoch [73/120    avg_loss:0.036, val_acc:0.986]
Epoch [74/120    avg_loss:0.025, val_acc:0.986]
Epoch [75/120    avg_loss:0.029, val_acc:0.986]
Epoch [76/120    avg_loss:0.035, val_acc:0.984]
Epoch [77/120    avg_loss:0.035, val_acc:0.986]
Epoch [78/120    avg_loss:0.038, val_acc:0.986]
Epoch [79/120    avg_loss:0.032, val_acc:0.986]
Epoch [80/120    avg_loss:0.040, val_acc:0.986]
Epoch [81/120    avg_loss:0.033, val_acc:0.986]
Epoch [82/120    avg_loss:0.027, val_acc:0.986]
Epoch [83/120    avg_loss:0.030, val_acc:0.986]
Epoch [84/120    avg_loss:0.023, val_acc:0.986]
Epoch [85/120    avg_loss:0.025, val_acc:0.986]
Epoch [86/120    avg_loss:0.027, val_acc:0.986]
Epoch [87/120    avg_loss:0.026, val_acc:0.984]
Epoch [88/120    avg_loss:0.026, val_acc:0.988]
Epoch [89/120    avg_loss:0.024, val_acc:0.990]
Epoch [90/120    avg_loss:0.024, val_acc:0.990]
Epoch [91/120    avg_loss:0.020, val_acc:0.990]
Epoch [92/120    avg_loss:0.030, val_acc:0.988]
Epoch [93/120    avg_loss:0.024, val_acc:0.988]
Epoch [94/120    avg_loss:0.025, val_acc:0.988]
Epoch [95/120    avg_loss:0.017, val_acc:0.990]
Epoch [96/120    avg_loss:0.021, val_acc:0.986]
Epoch [97/120    avg_loss:0.026, val_acc:0.988]
Epoch [98/120    avg_loss:0.027, val_acc:0.988]
Epoch [99/120    avg_loss:0.024, val_acc:0.988]
Epoch [100/120    avg_loss:0.023, val_acc:0.988]
Epoch [101/120    avg_loss:0.040, val_acc:0.990]
Epoch [102/120    avg_loss:0.024, val_acc:0.988]
Epoch [103/120    avg_loss:0.021, val_acc:0.990]
Epoch [104/120    avg_loss:0.032, val_acc:0.990]
Epoch [105/120    avg_loss:0.018, val_acc:0.990]
Epoch [106/120    avg_loss:0.027, val_acc:0.986]
Epoch [107/120    avg_loss:0.022, val_acc:0.986]
Epoch [108/120    avg_loss:0.017, val_acc:0.986]
Epoch [109/120    avg_loss:0.021, val_acc:0.986]
Epoch [110/120    avg_loss:0.021, val_acc:0.988]
Epoch [111/120    avg_loss:0.018, val_acc:0.990]
Epoch [112/120    avg_loss:0.017, val_acc:0.990]
Epoch [113/120    avg_loss:0.022, val_acc:0.990]
Epoch [114/120    avg_loss:0.033, val_acc:0.986]
Epoch [115/120    avg_loss:0.016, val_acc:0.986]
Epoch [116/120    avg_loss:0.019, val_acc:0.988]
Epoch [117/120    avg_loss:0.023, val_acc:0.988]
Epoch [118/120    avg_loss:0.023, val_acc:0.988]
Epoch [119/120    avg_loss:0.017, val_acc:0.988]
Epoch [120/120    avg_loss:0.022, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   4   0   0   0   0   0   0   1   0]
 [  0   0   0   1  21 123   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 0.99853801 0.98426966 0.9978308  0.94468085 0.90441176
 0.99516908 0.96132597 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9914535659950329
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f51eeb4b780>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.992, val_acc:0.587]
Epoch [2/120    avg_loss:1.266, val_acc:0.722]
Epoch [3/120    avg_loss:1.019, val_acc:0.804]
Epoch [4/120    avg_loss:0.843, val_acc:0.782]
Epoch [5/120    avg_loss:0.820, val_acc:0.786]
Epoch [6/120    avg_loss:0.740, val_acc:0.790]
Epoch [7/120    avg_loss:0.577, val_acc:0.812]
Epoch [8/120    avg_loss:0.634, val_acc:0.812]
Epoch [9/120    avg_loss:0.610, val_acc:0.865]
Epoch [10/120    avg_loss:0.588, val_acc:0.871]
Epoch [11/120    avg_loss:0.448, val_acc:0.853]
Epoch [12/120    avg_loss:0.501, val_acc:0.821]
Epoch [13/120    avg_loss:0.517, val_acc:0.897]
Epoch [14/120    avg_loss:0.458, val_acc:0.895]
Epoch [15/120    avg_loss:0.455, val_acc:0.899]
Epoch [16/120    avg_loss:0.445, val_acc:0.919]
Epoch [17/120    avg_loss:0.439, val_acc:0.915]
Epoch [18/120    avg_loss:0.310, val_acc:0.925]
Epoch [19/120    avg_loss:0.309, val_acc:0.905]
Epoch [20/120    avg_loss:0.288, val_acc:0.923]
Epoch [21/120    avg_loss:0.250, val_acc:0.935]
Epoch [22/120    avg_loss:0.307, val_acc:0.940]
Epoch [23/120    avg_loss:0.295, val_acc:0.938]
Epoch [24/120    avg_loss:0.252, val_acc:0.952]
Epoch [25/120    avg_loss:0.221, val_acc:0.927]
Epoch [26/120    avg_loss:0.251, val_acc:0.950]
Epoch [27/120    avg_loss:0.203, val_acc:0.946]
Epoch [28/120    avg_loss:0.169, val_acc:0.968]
Epoch [29/120    avg_loss:0.183, val_acc:0.933]
Epoch [30/120    avg_loss:0.200, val_acc:0.935]
Epoch [31/120    avg_loss:0.291, val_acc:0.921]
Epoch [32/120    avg_loss:0.233, val_acc:0.964]
Epoch [33/120    avg_loss:0.165, val_acc:0.940]
Epoch [34/120    avg_loss:0.159, val_acc:0.950]
Epoch [35/120    avg_loss:0.166, val_acc:0.962]
Epoch [36/120    avg_loss:0.125, val_acc:0.958]
Epoch [37/120    avg_loss:0.115, val_acc:0.952]
Epoch [38/120    avg_loss:0.117, val_acc:0.966]
Epoch [39/120    avg_loss:0.189, val_acc:0.919]
Epoch [40/120    avg_loss:0.250, val_acc:0.935]
Epoch [41/120    avg_loss:0.225, val_acc:0.974]
Epoch [42/120    avg_loss:0.125, val_acc:0.956]
Epoch [43/120    avg_loss:0.143, val_acc:0.976]
Epoch [44/120    avg_loss:0.093, val_acc:0.978]
Epoch [45/120    avg_loss:0.087, val_acc:0.974]
Epoch [46/120    avg_loss:0.106, val_acc:0.962]
Epoch [47/120    avg_loss:0.133, val_acc:0.950]
Epoch [48/120    avg_loss:0.086, val_acc:0.970]
Epoch [49/120    avg_loss:0.068, val_acc:0.966]
Epoch [50/120    avg_loss:0.074, val_acc:0.978]
Epoch [51/120    avg_loss:0.099, val_acc:0.980]
Epoch [52/120    avg_loss:0.074, val_acc:0.976]
Epoch [53/120    avg_loss:0.086, val_acc:0.972]
Epoch [54/120    avg_loss:0.069, val_acc:0.982]
Epoch [55/120    avg_loss:0.070, val_acc:0.974]
Epoch [56/120    avg_loss:0.049, val_acc:0.968]
Epoch [57/120    avg_loss:0.057, val_acc:0.986]
Epoch [58/120    avg_loss:0.033, val_acc:0.978]
Epoch [59/120    avg_loss:0.144, val_acc:0.958]
Epoch [60/120    avg_loss:0.115, val_acc:0.982]
Epoch [61/120    avg_loss:0.121, val_acc:0.964]
Epoch [62/120    avg_loss:0.112, val_acc:0.968]
Epoch [63/120    avg_loss:0.067, val_acc:0.978]
Epoch [64/120    avg_loss:0.040, val_acc:0.986]
Epoch [65/120    avg_loss:0.048, val_acc:0.980]
Epoch [66/120    avg_loss:0.022, val_acc:0.982]
Epoch [67/120    avg_loss:0.030, val_acc:0.984]
Epoch [68/120    avg_loss:0.136, val_acc:0.944]
Epoch [69/120    avg_loss:0.060, val_acc:0.976]
Epoch [70/120    avg_loss:0.046, val_acc:0.984]
Epoch [71/120    avg_loss:0.033, val_acc:0.974]
Epoch [72/120    avg_loss:0.084, val_acc:0.966]
Epoch [73/120    avg_loss:0.071, val_acc:0.984]
Epoch [74/120    avg_loss:0.049, val_acc:0.982]
Epoch [75/120    avg_loss:0.065, val_acc:0.984]
Epoch [76/120    avg_loss:0.043, val_acc:0.980]
Epoch [77/120    avg_loss:0.020, val_acc:0.988]
Epoch [78/120    avg_loss:0.020, val_acc:0.990]
Epoch [79/120    avg_loss:0.018, val_acc:0.984]
Epoch [80/120    avg_loss:0.017, val_acc:0.990]
Epoch [81/120    avg_loss:0.012, val_acc:0.982]
Epoch [82/120    avg_loss:0.023, val_acc:0.984]
Epoch [83/120    avg_loss:0.050, val_acc:0.984]
Epoch [84/120    avg_loss:0.040, val_acc:0.986]
Epoch [85/120    avg_loss:0.020, val_acc:0.988]
Epoch [86/120    avg_loss:0.011, val_acc:0.990]
Epoch [87/120    avg_loss:0.020, val_acc:0.992]
Epoch [88/120    avg_loss:0.017, val_acc:0.990]
Epoch [89/120    avg_loss:0.011, val_acc:0.992]
Epoch [90/120    avg_loss:0.008, val_acc:0.990]
Epoch [91/120    avg_loss:0.009, val_acc:0.990]
Epoch [92/120    avg_loss:0.007, val_acc:0.992]
Epoch [93/120    avg_loss:0.007, val_acc:0.990]
Epoch [94/120    avg_loss:0.008, val_acc:0.988]
Epoch [95/120    avg_loss:0.013, val_acc:0.988]
Epoch [96/120    avg_loss:0.011, val_acc:0.984]
Epoch [97/120    avg_loss:0.007, val_acc:0.990]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.022, val_acc:0.984]
Epoch [100/120    avg_loss:0.013, val_acc:0.990]
Epoch [101/120    avg_loss:0.010, val_acc:0.986]
Epoch [102/120    avg_loss:0.009, val_acc:0.988]
Epoch [103/120    avg_loss:0.010, val_acc:0.990]
Epoch [104/120    avg_loss:0.008, val_acc:0.992]
Epoch [105/120    avg_loss:0.005, val_acc:0.990]
Epoch [106/120    avg_loss:0.010, val_acc:0.992]
Epoch [107/120    avg_loss:0.014, val_acc:0.988]
Epoch [108/120    avg_loss:0.005, val_acc:0.994]
Epoch [109/120    avg_loss:0.006, val_acc:0.992]
Epoch [110/120    avg_loss:0.007, val_acc:0.986]
Epoch [111/120    avg_loss:0.013, val_acc:0.984]
Epoch [112/120    avg_loss:0.007, val_acc:0.990]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.004, val_acc:0.992]
Epoch [115/120    avg_loss:0.007, val_acc:0.990]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.994]
Epoch [120/120    avg_loss:0.010, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 670   0   0   0   0  15   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 0.98892989 1.         1.         0.94759825 0.91608392
 0.96487119 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9907453352133089
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdc2b475780>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.994, val_acc:0.621]
Epoch [2/120    avg_loss:1.282, val_acc:0.694]
Epoch [3/120    avg_loss:1.010, val_acc:0.744]
Epoch [4/120    avg_loss:0.862, val_acc:0.732]
Epoch [5/120    avg_loss:0.708, val_acc:0.810]
Epoch [6/120    avg_loss:0.703, val_acc:0.861]
Epoch [7/120    avg_loss:0.558, val_acc:0.808]
Epoch [8/120    avg_loss:0.659, val_acc:0.829]
Epoch [9/120    avg_loss:0.616, val_acc:0.863]
Epoch [10/120    avg_loss:0.581, val_acc:0.889]
Epoch [11/120    avg_loss:0.482, val_acc:0.897]
Epoch [12/120    avg_loss:0.511, val_acc:0.881]
Epoch [13/120    avg_loss:0.443, val_acc:0.905]
Epoch [14/120    avg_loss:0.386, val_acc:0.897]
Epoch [15/120    avg_loss:0.348, val_acc:0.901]
Epoch [16/120    avg_loss:0.316, val_acc:0.903]
Epoch [17/120    avg_loss:0.377, val_acc:0.919]
Epoch [18/120    avg_loss:0.280, val_acc:0.925]
Epoch [19/120    avg_loss:0.282, val_acc:0.925]
Epoch [20/120    avg_loss:0.273, val_acc:0.931]
Epoch [21/120    avg_loss:0.227, val_acc:0.956]
Epoch [22/120    avg_loss:0.290, val_acc:0.909]
Epoch [23/120    avg_loss:0.273, val_acc:0.893]
Epoch [24/120    avg_loss:0.219, val_acc:0.917]
Epoch [25/120    avg_loss:0.160, val_acc:0.966]
Epoch [26/120    avg_loss:0.184, val_acc:0.940]
Epoch [27/120    avg_loss:0.191, val_acc:0.952]
Epoch [28/120    avg_loss:0.164, val_acc:0.931]
Epoch [29/120    avg_loss:0.175, val_acc:0.935]
Epoch [30/120    avg_loss:0.263, val_acc:0.952]
Epoch [31/120    avg_loss:0.161, val_acc:0.972]
Epoch [32/120    avg_loss:0.134, val_acc:0.964]
Epoch [33/120    avg_loss:0.175, val_acc:0.950]
Epoch [34/120    avg_loss:0.266, val_acc:0.962]
Epoch [35/120    avg_loss:0.160, val_acc:0.958]
Epoch [36/120    avg_loss:0.110, val_acc:0.974]
Epoch [37/120    avg_loss:0.087, val_acc:0.958]
Epoch [38/120    avg_loss:0.155, val_acc:0.944]
Epoch [39/120    avg_loss:0.150, val_acc:0.952]
Epoch [40/120    avg_loss:0.142, val_acc:0.968]
Epoch [41/120    avg_loss:0.172, val_acc:0.964]
Epoch [42/120    avg_loss:0.132, val_acc:0.968]
Epoch [43/120    avg_loss:0.096, val_acc:0.956]
Epoch [44/120    avg_loss:0.179, val_acc:0.968]
Epoch [45/120    avg_loss:0.107, val_acc:0.958]
Epoch [46/120    avg_loss:0.069, val_acc:0.978]
Epoch [47/120    avg_loss:0.095, val_acc:0.982]
Epoch [48/120    avg_loss:0.062, val_acc:0.976]
Epoch [49/120    avg_loss:0.077, val_acc:0.970]
Epoch [50/120    avg_loss:0.066, val_acc:0.974]
Epoch [51/120    avg_loss:0.045, val_acc:0.986]
Epoch [52/120    avg_loss:0.056, val_acc:0.974]
Epoch [53/120    avg_loss:0.086, val_acc:0.980]
Epoch [54/120    avg_loss:0.060, val_acc:0.972]
Epoch [55/120    avg_loss:0.079, val_acc:0.986]
Epoch [56/120    avg_loss:0.133, val_acc:0.970]
Epoch [57/120    avg_loss:0.046, val_acc:0.986]
Epoch [58/120    avg_loss:0.050, val_acc:0.986]
Epoch [59/120    avg_loss:0.054, val_acc:0.972]
Epoch [60/120    avg_loss:0.021, val_acc:0.984]
Epoch [61/120    avg_loss:0.032, val_acc:0.980]
Epoch [62/120    avg_loss:0.096, val_acc:0.962]
Epoch [63/120    avg_loss:0.110, val_acc:0.938]
Epoch [64/120    avg_loss:0.072, val_acc:0.986]
Epoch [65/120    avg_loss:0.040, val_acc:0.982]
Epoch [66/120    avg_loss:0.026, val_acc:0.984]
Epoch [67/120    avg_loss:0.083, val_acc:0.980]
Epoch [68/120    avg_loss:0.025, val_acc:0.988]
Epoch [69/120    avg_loss:0.023, val_acc:0.988]
Epoch [70/120    avg_loss:0.047, val_acc:0.982]
Epoch [71/120    avg_loss:0.015, val_acc:0.992]
Epoch [72/120    avg_loss:0.020, val_acc:0.992]
Epoch [73/120    avg_loss:0.023, val_acc:0.986]
Epoch [74/120    avg_loss:0.018, val_acc:0.984]
Epoch [75/120    avg_loss:0.021, val_acc:0.992]
Epoch [76/120    avg_loss:0.035, val_acc:0.990]
Epoch [77/120    avg_loss:0.074, val_acc:0.980]
Epoch [78/120    avg_loss:0.065, val_acc:0.986]
Epoch [79/120    avg_loss:0.027, val_acc:0.986]
Epoch [80/120    avg_loss:0.028, val_acc:0.986]
Epoch [81/120    avg_loss:0.014, val_acc:0.992]
Epoch [82/120    avg_loss:0.018, val_acc:0.990]
Epoch [83/120    avg_loss:0.014, val_acc:0.986]
Epoch [84/120    avg_loss:0.011, val_acc:0.990]
Epoch [85/120    avg_loss:0.011, val_acc:0.992]
Epoch [86/120    avg_loss:0.026, val_acc:0.992]
Epoch [87/120    avg_loss:0.018, val_acc:0.998]
Epoch [88/120    avg_loss:0.015, val_acc:0.992]
Epoch [89/120    avg_loss:0.010, val_acc:0.994]
Epoch [90/120    avg_loss:0.023, val_acc:0.982]
Epoch [91/120    avg_loss:0.023, val_acc:0.990]
Epoch [92/120    avg_loss:0.060, val_acc:0.986]
Epoch [93/120    avg_loss:0.024, val_acc:0.986]
Epoch [94/120    avg_loss:0.030, val_acc:0.990]
Epoch [95/120    avg_loss:0.012, val_acc:0.992]
Epoch [96/120    avg_loss:0.009, val_acc:0.994]
Epoch [97/120    avg_loss:0.009, val_acc:0.996]
Epoch [98/120    avg_loss:0.012, val_acc:0.994]
Epoch [99/120    avg_loss:0.010, val_acc:0.982]
Epoch [100/120    avg_loss:0.017, val_acc:0.986]
Epoch [101/120    avg_loss:0.011, val_acc:0.992]
Epoch [102/120    avg_loss:0.010, val_acc:0.992]
Epoch [103/120    avg_loss:0.007, val_acc:0.992]
Epoch [104/120    avg_loss:0.009, val_acc:0.992]
Epoch [105/120    avg_loss:0.010, val_acc:0.992]
Epoch [106/120    avg_loss:0.006, val_acc:0.992]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.007, val_acc:0.994]
Epoch [109/120    avg_loss:0.007, val_acc:0.994]
Epoch [110/120    avg_loss:0.006, val_acc:0.994]
Epoch [111/120    avg_loss:0.007, val_acc:0.994]
Epoch [112/120    avg_loss:0.004, val_acc:0.992]
Epoch [113/120    avg_loss:0.005, val_acc:0.994]
Epoch [114/120    avg_loss:0.006, val_acc:0.994]
Epoch [115/120    avg_loss:0.007, val_acc:0.994]
Epoch [116/120    avg_loss:0.006, val_acc:0.994]
Epoch [117/120    avg_loss:0.008, val_acc:0.994]
Epoch [118/120    avg_loss:0.008, val_acc:0.994]
Epoch [119/120    avg_loss:0.006, val_acc:0.992]
Epoch [120/120    avg_loss:0.010, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  12   0   0   0   0   0   0   2   0]
 [  0   0   0   0   7 138   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.48827292110875

F1 scores:
[       nan 1.         0.99319728 0.99782135 0.95302013 0.93559322
 1.         0.98924731 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9943027640553205
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:18:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff0e3e48748>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:1.992, val_acc:0.525]
Epoch [2/120    avg_loss:1.330, val_acc:0.744]
Epoch [3/120    avg_loss:0.961, val_acc:0.672]
Epoch [4/120    avg_loss:0.876, val_acc:0.768]
Epoch [5/120    avg_loss:0.709, val_acc:0.805]
Epoch [6/120    avg_loss:0.643, val_acc:0.783]
Epoch [7/120    avg_loss:0.604, val_acc:0.844]
Epoch [8/120    avg_loss:0.585, val_acc:0.867]
Epoch [9/120    avg_loss:0.517, val_acc:0.879]
Epoch [10/120    avg_loss:0.424, val_acc:0.906]
Epoch [11/120    avg_loss:0.405, val_acc:0.855]
Epoch [12/120    avg_loss:0.390, val_acc:0.885]
Epoch [13/120    avg_loss:0.568, val_acc:0.885]
Epoch [14/120    avg_loss:0.401, val_acc:0.895]
Epoch [15/120    avg_loss:0.411, val_acc:0.930]
Epoch [16/120    avg_loss:0.398, val_acc:0.912]
Epoch [17/120    avg_loss:0.356, val_acc:0.904]
Epoch [18/120    avg_loss:0.390, val_acc:0.918]
Epoch [19/120    avg_loss:0.304, val_acc:0.930]
Epoch [20/120    avg_loss:0.329, val_acc:0.908]
Epoch [21/120    avg_loss:0.259, val_acc:0.939]
Epoch [22/120    avg_loss:0.245, val_acc:0.926]
Epoch [23/120    avg_loss:0.274, val_acc:0.918]
Epoch [24/120    avg_loss:0.193, val_acc:0.951]
Epoch [25/120    avg_loss:0.237, val_acc:0.936]
Epoch [26/120    avg_loss:0.247, val_acc:0.961]
Epoch [27/120    avg_loss:0.191, val_acc:0.939]
Epoch [28/120    avg_loss:0.173, val_acc:0.930]
Epoch [29/120    avg_loss:0.217, val_acc:0.924]
Epoch [30/120    avg_loss:0.226, val_acc:0.949]
Epoch [31/120    avg_loss:0.149, val_acc:0.959]
Epoch [32/120    avg_loss:0.199, val_acc:0.963]
Epoch [33/120    avg_loss:0.151, val_acc:0.969]
Epoch [34/120    avg_loss:0.149, val_acc:0.955]
Epoch [35/120    avg_loss:0.139, val_acc:0.930]
Epoch [36/120    avg_loss:0.179, val_acc:0.973]
Epoch [37/120    avg_loss:0.107, val_acc:0.963]
Epoch [38/120    avg_loss:0.111, val_acc:0.980]
Epoch [39/120    avg_loss:0.113, val_acc:0.967]
Epoch [40/120    avg_loss:0.124, val_acc:0.961]
Epoch [41/120    avg_loss:0.108, val_acc:0.975]
Epoch [42/120    avg_loss:0.098, val_acc:0.975]
Epoch [43/120    avg_loss:0.150, val_acc:0.941]
Epoch [44/120    avg_loss:0.152, val_acc:0.977]
Epoch [45/120    avg_loss:0.085, val_acc:0.973]
Epoch [46/120    avg_loss:0.132, val_acc:0.971]
Epoch [47/120    avg_loss:0.085, val_acc:0.980]
Epoch [48/120    avg_loss:0.092, val_acc:0.961]
Epoch [49/120    avg_loss:0.113, val_acc:0.982]
Epoch [50/120    avg_loss:0.064, val_acc:0.986]
Epoch [51/120    avg_loss:0.105, val_acc:0.975]
Epoch [52/120    avg_loss:0.085, val_acc:0.967]
Epoch [53/120    avg_loss:0.046, val_acc:0.988]
Epoch [54/120    avg_loss:0.066, val_acc:0.990]
Epoch [55/120    avg_loss:0.047, val_acc:0.980]
Epoch [56/120    avg_loss:0.035, val_acc:0.990]
Epoch [57/120    avg_loss:0.054, val_acc:0.990]
Epoch [58/120    avg_loss:0.075, val_acc:0.986]
Epoch [59/120    avg_loss:0.108, val_acc:0.969]
Epoch [60/120    avg_loss:0.089, val_acc:0.982]
Epoch [61/120    avg_loss:0.061, val_acc:0.977]
Epoch [62/120    avg_loss:0.113, val_acc:0.975]
Epoch [63/120    avg_loss:0.099, val_acc:0.982]
Epoch [64/120    avg_loss:0.057, val_acc:0.973]
Epoch [65/120    avg_loss:0.036, val_acc:0.990]
Epoch [66/120    avg_loss:0.032, val_acc:0.988]
Epoch [67/120    avg_loss:0.031, val_acc:0.990]
Epoch [68/120    avg_loss:0.046, val_acc:0.980]
Epoch [69/120    avg_loss:0.023, val_acc:0.982]
Epoch [70/120    avg_loss:0.070, val_acc:0.971]
Epoch [71/120    avg_loss:0.077, val_acc:0.992]
Epoch [72/120    avg_loss:0.043, val_acc:0.982]
Epoch [73/120    avg_loss:0.095, val_acc:0.982]
Epoch [74/120    avg_loss:0.040, val_acc:0.990]
Epoch [75/120    avg_loss:0.021, val_acc:0.988]
Epoch [76/120    avg_loss:0.028, val_acc:0.990]
Epoch [77/120    avg_loss:0.013, val_acc:0.990]
Epoch [78/120    avg_loss:0.014, val_acc:0.994]
Epoch [79/120    avg_loss:0.051, val_acc:0.984]
Epoch [80/120    avg_loss:0.015, val_acc:0.986]
Epoch [81/120    avg_loss:0.018, val_acc:0.982]
Epoch [82/120    avg_loss:0.013, val_acc:0.992]
Epoch [83/120    avg_loss:0.007, val_acc:0.992]
Epoch [84/120    avg_loss:0.013, val_acc:0.990]
Epoch [85/120    avg_loss:0.017, val_acc:0.992]
Epoch [86/120    avg_loss:0.009, val_acc:0.992]
Epoch [87/120    avg_loss:0.008, val_acc:0.996]
Epoch [88/120    avg_loss:0.015, val_acc:0.994]
Epoch [89/120    avg_loss:0.008, val_acc:0.994]
Epoch [90/120    avg_loss:0.011, val_acc:0.996]
Epoch [91/120    avg_loss:0.015, val_acc:0.994]
Epoch [92/120    avg_loss:0.007, val_acc:0.996]
Epoch [93/120    avg_loss:0.005, val_acc:0.994]
Epoch [94/120    avg_loss:0.009, val_acc:0.994]
Epoch [95/120    avg_loss:0.005, val_acc:0.992]
Epoch [96/120    avg_loss:0.005, val_acc:0.994]
Epoch [97/120    avg_loss:0.006, val_acc:0.994]
Epoch [98/120    avg_loss:0.006, val_acc:0.994]
Epoch [99/120    avg_loss:0.006, val_acc:0.992]
Epoch [100/120    avg_loss:0.004, val_acc:0.994]
Epoch [101/120    avg_loss:0.012, val_acc:0.967]
Epoch [102/120    avg_loss:0.037, val_acc:0.988]
Epoch [103/120    avg_loss:0.025, val_acc:0.992]
Epoch [104/120    avg_loss:0.009, val_acc:0.990]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.009, val_acc:0.988]
Epoch [108/120    avg_loss:0.010, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.010, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.006, val_acc:0.990]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.006, val_acc:0.990]
Epoch [115/120    avg_loss:0.008, val_acc:0.990]
Epoch [116/120    avg_loss:0.008, val_acc:0.990]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.006, val_acc:0.988]
Epoch [120/120    avg_loss:0.006, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217   4   0   0   0   0   0   0   6   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.23240938166312

F1 scores:
[       nan 0.99853801 0.99095023 1.         0.93534483 0.91240876
 0.99516908 0.97826087 1.         1.         1.         1.
 0.99342105 1.        ]

Kappa:
0.9914533116139593
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:62
Validation dataloader:62
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a23955780>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.050, val_acc:0.637]
Epoch [2/120    avg_loss:1.345, val_acc:0.744]
Epoch [3/120    avg_loss:0.921, val_acc:0.694]
Epoch [4/120    avg_loss:0.913, val_acc:0.766]
Epoch [5/120    avg_loss:0.780, val_acc:0.760]
Epoch [6/120    avg_loss:0.684, val_acc:0.810]
Epoch [7/120    avg_loss:0.698, val_acc:0.772]
Epoch [8/120    avg_loss:0.661, val_acc:0.831]
Epoch [9/120    avg_loss:0.564, val_acc:0.831]
Epoch [10/120    avg_loss:0.482, val_acc:0.843]
Epoch [11/120    avg_loss:0.476, val_acc:0.817]
Epoch [12/120    avg_loss:0.535, val_acc:0.855]
Epoch [13/120    avg_loss:0.429, val_acc:0.865]
Epoch [14/120    avg_loss:0.393, val_acc:0.881]
Epoch [15/120    avg_loss:0.415, val_acc:0.901]
Epoch [16/120    avg_loss:0.334, val_acc:0.903]
Epoch [17/120    avg_loss:0.253, val_acc:0.897]
Epoch [18/120    avg_loss:0.356, val_acc:0.921]
Epoch [19/120    avg_loss:0.311, val_acc:0.863]
Epoch [20/120    avg_loss:0.329, val_acc:0.907]
Epoch [21/120    avg_loss:0.280, val_acc:0.909]
Epoch [22/120    avg_loss:0.350, val_acc:0.905]
Epoch [23/120    avg_loss:0.390, val_acc:0.935]
Epoch [24/120    avg_loss:0.235, val_acc:0.933]
Epoch [25/120    avg_loss:0.273, val_acc:0.942]
Epoch [26/120    avg_loss:0.207, val_acc:0.940]
Epoch [27/120    avg_loss:0.172, val_acc:0.921]
Epoch [28/120    avg_loss:0.173, val_acc:0.962]
Epoch [29/120    avg_loss:0.140, val_acc:0.954]
Epoch [30/120    avg_loss:0.128, val_acc:0.942]
Epoch [31/120    avg_loss:0.265, val_acc:0.883]
Epoch [32/120    avg_loss:0.156, val_acc:0.954]
Epoch [33/120    avg_loss:0.155, val_acc:0.950]
Epoch [34/120    avg_loss:0.187, val_acc:0.940]
Epoch [35/120    avg_loss:0.140, val_acc:0.935]
Epoch [36/120    avg_loss:0.138, val_acc:0.960]
Epoch [37/120    avg_loss:0.138, val_acc:0.946]
Epoch [38/120    avg_loss:0.148, val_acc:0.956]
Epoch [39/120    avg_loss:0.129, val_acc:0.964]
Epoch [40/120    avg_loss:0.104, val_acc:0.950]
Epoch [41/120    avg_loss:0.136, val_acc:0.958]
Epoch [42/120    avg_loss:0.134, val_acc:0.968]
Epoch [43/120    avg_loss:0.152, val_acc:0.968]
Epoch [44/120    avg_loss:0.069, val_acc:0.974]
Epoch [45/120    avg_loss:0.101, val_acc:0.964]
Epoch [46/120    avg_loss:0.104, val_acc:0.954]
Epoch [47/120    avg_loss:0.113, val_acc:0.974]
Epoch [48/120    avg_loss:0.062, val_acc:0.974]
Epoch [49/120    avg_loss:0.057, val_acc:0.960]
Epoch [50/120    avg_loss:0.077, val_acc:0.968]
Epoch [51/120    avg_loss:0.124, val_acc:0.964]
Epoch [52/120    avg_loss:0.064, val_acc:0.968]
Epoch [53/120    avg_loss:0.038, val_acc:0.972]
Epoch [54/120    avg_loss:0.039, val_acc:0.972]
Epoch [55/120    avg_loss:0.090, val_acc:0.970]
Epoch [56/120    avg_loss:0.094, val_acc:0.980]
Epoch [57/120    avg_loss:0.051, val_acc:0.976]
Epoch [58/120    avg_loss:0.037, val_acc:0.976]
Epoch [59/120    avg_loss:0.023, val_acc:0.978]
Epoch [60/120    avg_loss:0.022, val_acc:0.986]
Epoch [61/120    avg_loss:0.022, val_acc:0.982]
Epoch [62/120    avg_loss:0.018, val_acc:0.988]
Epoch [63/120    avg_loss:0.034, val_acc:0.986]
Epoch [64/120    avg_loss:0.020, val_acc:0.984]
Epoch [65/120    avg_loss:0.041, val_acc:0.988]
Epoch [66/120    avg_loss:0.055, val_acc:0.978]
Epoch [67/120    avg_loss:0.024, val_acc:0.982]
Epoch [68/120    avg_loss:0.024, val_acc:0.978]
Epoch [69/120    avg_loss:0.040, val_acc:0.986]
Epoch [70/120    avg_loss:0.028, val_acc:0.988]
Epoch [71/120    avg_loss:0.023, val_acc:0.986]
Epoch [72/120    avg_loss:0.015, val_acc:0.986]
Epoch [73/120    avg_loss:0.015, val_acc:0.988]
Epoch [74/120    avg_loss:0.012, val_acc:0.990]
Epoch [75/120    avg_loss:0.013, val_acc:0.988]
Epoch [76/120    avg_loss:0.014, val_acc:0.984]
Epoch [77/120    avg_loss:0.010, val_acc:0.984]
Epoch [78/120    avg_loss:0.009, val_acc:0.986]
Epoch [79/120    avg_loss:0.011, val_acc:0.982]
Epoch [80/120    avg_loss:0.010, val_acc:0.984]
Epoch [81/120    avg_loss:0.008, val_acc:0.990]
Epoch [82/120    avg_loss:0.009, val_acc:0.982]
Epoch [83/120    avg_loss:0.014, val_acc:0.990]
Epoch [84/120    avg_loss:0.018, val_acc:0.974]
Epoch [85/120    avg_loss:0.016, val_acc:0.986]
Epoch [86/120    avg_loss:0.020, val_acc:0.972]
Epoch [87/120    avg_loss:0.045, val_acc:0.972]
Epoch [88/120    avg_loss:0.050, val_acc:0.984]
Epoch [89/120    avg_loss:0.032, val_acc:0.984]
Epoch [90/120    avg_loss:0.021, val_acc:0.974]
Epoch [91/120    avg_loss:0.017, val_acc:0.990]
Epoch [92/120    avg_loss:0.034, val_acc:0.972]
Epoch [93/120    avg_loss:0.011, val_acc:0.982]
Epoch [94/120    avg_loss:0.017, val_acc:0.988]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.014, val_acc:0.980]
Epoch [98/120    avg_loss:0.014, val_acc:0.990]
Epoch [99/120    avg_loss:0.008, val_acc:0.988]
Epoch [100/120    avg_loss:0.019, val_acc:0.964]
Epoch [101/120    avg_loss:0.016, val_acc:0.988]
Epoch [102/120    avg_loss:0.009, val_acc:0.990]
Epoch [103/120    avg_loss:0.011, val_acc:0.980]
Epoch [104/120    avg_loss:0.016, val_acc:0.986]
Epoch [105/120    avg_loss:0.012, val_acc:0.986]
Epoch [106/120    avg_loss:0.014, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.005, val_acc:0.990]
Epoch [109/120    avg_loss:0.018, val_acc:0.988]
Epoch [110/120    avg_loss:0.062, val_acc:0.988]
Epoch [111/120    avg_loss:0.080, val_acc:0.970]
Epoch [112/120    avg_loss:0.111, val_acc:0.968]
Epoch [113/120    avg_loss:0.110, val_acc:0.960]
Epoch [114/120    avg_loss:0.078, val_acc:0.968]
Epoch [115/120    avg_loss:0.057, val_acc:0.974]
Epoch [116/120    avg_loss:0.023, val_acc:0.982]
Epoch [117/120    avg_loss:0.019, val_acc:0.982]
Epoch [118/120    avg_loss:0.011, val_acc:0.986]
Epoch [119/120    avg_loss:0.040, val_acc:0.980]
Epoch [120/120    avg_loss:0.018, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   0   0   0   0   4   0   0   0   0   0]
 [  0   0   0   0 213  11   0   0   0   0   0   0   3   0]
 [  0   0   0   0   6 135   4   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.27505330490405

F1 scores:
[       nan 1.         0.98648649 0.99122807 0.95515695 0.92783505
 0.99038462 0.96703297 0.99487179 1.         1.         1.
 0.99669967 1.        ]

Kappa:
0.9919282567504799
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4ed5e19828>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.011, val_acc:0.571]
Epoch [2/120    avg_loss:1.424, val_acc:0.723]
Epoch [3/120    avg_loss:1.099, val_acc:0.694]
Epoch [4/120    avg_loss:0.905, val_acc:0.775]
Epoch [5/120    avg_loss:0.737, val_acc:0.787]
Epoch [6/120    avg_loss:0.751, val_acc:0.829]
Epoch [7/120    avg_loss:0.627, val_acc:0.819]
Epoch [8/120    avg_loss:0.628, val_acc:0.777]
Epoch [9/120    avg_loss:0.566, val_acc:0.842]
Epoch [10/120    avg_loss:0.531, val_acc:0.844]
Epoch [11/120    avg_loss:0.536, val_acc:0.838]
Epoch [12/120    avg_loss:0.563, val_acc:0.865]
Epoch [13/120    avg_loss:0.513, val_acc:0.885]
Epoch [14/120    avg_loss:0.395, val_acc:0.883]
Epoch [15/120    avg_loss:0.379, val_acc:0.892]
Epoch [16/120    avg_loss:0.377, val_acc:0.885]
Epoch [17/120    avg_loss:0.423, val_acc:0.904]
Epoch [18/120    avg_loss:0.409, val_acc:0.879]
Epoch [19/120    avg_loss:0.421, val_acc:0.890]
Epoch [20/120    avg_loss:0.380, val_acc:0.919]
Epoch [21/120    avg_loss:0.254, val_acc:0.910]
Epoch [22/120    avg_loss:0.335, val_acc:0.869]
Epoch [23/120    avg_loss:0.349, val_acc:0.921]
Epoch [24/120    avg_loss:0.289, val_acc:0.946]
Epoch [25/120    avg_loss:0.252, val_acc:0.931]
Epoch [26/120    avg_loss:0.218, val_acc:0.919]
Epoch [27/120    avg_loss:0.176, val_acc:0.935]
Epoch [28/120    avg_loss:0.217, val_acc:0.933]
Epoch [29/120    avg_loss:0.297, val_acc:0.938]
Epoch [30/120    avg_loss:0.171, val_acc:0.944]
Epoch [31/120    avg_loss:0.210, val_acc:0.927]
Epoch [32/120    avg_loss:0.152, val_acc:0.967]
Epoch [33/120    avg_loss:0.164, val_acc:0.946]
Epoch [34/120    avg_loss:0.125, val_acc:0.933]
Epoch [35/120    avg_loss:0.171, val_acc:0.935]
Epoch [36/120    avg_loss:0.251, val_acc:0.931]
Epoch [37/120    avg_loss:0.283, val_acc:0.952]
Epoch [38/120    avg_loss:0.152, val_acc:0.944]
Epoch [39/120    avg_loss:0.091, val_acc:0.963]
Epoch [40/120    avg_loss:0.084, val_acc:0.958]
Epoch [41/120    avg_loss:0.096, val_acc:0.963]
Epoch [42/120    avg_loss:0.144, val_acc:0.950]
Epoch [43/120    avg_loss:0.112, val_acc:0.948]
Epoch [44/120    avg_loss:0.075, val_acc:0.969]
Epoch [45/120    avg_loss:0.078, val_acc:0.973]
Epoch [46/120    avg_loss:0.095, val_acc:0.940]
Epoch [47/120    avg_loss:0.063, val_acc:0.967]
Epoch [48/120    avg_loss:0.053, val_acc:0.956]
Epoch [49/120    avg_loss:0.075, val_acc:0.969]
Epoch [50/120    avg_loss:0.045, val_acc:0.965]
Epoch [51/120    avg_loss:0.050, val_acc:0.954]
Epoch [52/120    avg_loss:0.063, val_acc:0.969]
Epoch [53/120    avg_loss:0.037, val_acc:0.965]
Epoch [54/120    avg_loss:0.059, val_acc:0.977]
Epoch [55/120    avg_loss:0.028, val_acc:0.977]
Epoch [56/120    avg_loss:0.020, val_acc:0.977]
Epoch [57/120    avg_loss:0.028, val_acc:0.975]
Epoch [58/120    avg_loss:0.028, val_acc:0.973]
Epoch [59/120    avg_loss:0.025, val_acc:0.981]
Epoch [60/120    avg_loss:0.019, val_acc:0.975]
Epoch [61/120    avg_loss:0.026, val_acc:0.981]
Epoch [62/120    avg_loss:0.016, val_acc:0.981]
Epoch [63/120    avg_loss:0.023, val_acc:0.983]
Epoch [64/120    avg_loss:0.013, val_acc:0.975]
Epoch [65/120    avg_loss:0.020, val_acc:0.983]
Epoch [66/120    avg_loss:0.013, val_acc:0.985]
Epoch [67/120    avg_loss:0.016, val_acc:0.981]
Epoch [68/120    avg_loss:0.018, val_acc:0.988]
Epoch [69/120    avg_loss:0.037, val_acc:0.967]
Epoch [70/120    avg_loss:0.039, val_acc:0.981]
Epoch [71/120    avg_loss:0.128, val_acc:0.965]
Epoch [72/120    avg_loss:0.045, val_acc:0.975]
Epoch [73/120    avg_loss:0.035, val_acc:0.973]
Epoch [74/120    avg_loss:0.054, val_acc:0.960]
Epoch [75/120    avg_loss:0.033, val_acc:0.969]
Epoch [76/120    avg_loss:0.029, val_acc:0.973]
Epoch [77/120    avg_loss:0.013, val_acc:0.975]
Epoch [78/120    avg_loss:0.020, val_acc:0.973]
Epoch [79/120    avg_loss:0.015, val_acc:0.971]
Epoch [80/120    avg_loss:0.013, val_acc:0.977]
Epoch [81/120    avg_loss:0.007, val_acc:0.981]
Epoch [82/120    avg_loss:0.010, val_acc:0.981]
Epoch [83/120    avg_loss:0.009, val_acc:0.983]
Epoch [84/120    avg_loss:0.010, val_acc:0.983]
Epoch [85/120    avg_loss:0.006, val_acc:0.983]
Epoch [86/120    avg_loss:0.017, val_acc:0.983]
Epoch [87/120    avg_loss:0.006, val_acc:0.983]
Epoch [88/120    avg_loss:0.010, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.988]
Epoch [90/120    avg_loss:0.008, val_acc:0.985]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.019, val_acc:0.990]
Epoch [93/120    avg_loss:0.016, val_acc:0.988]
Epoch [94/120    avg_loss:0.009, val_acc:0.985]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.006, val_acc:0.988]
Epoch [99/120    avg_loss:0.007, val_acc:0.988]
Epoch [100/120    avg_loss:0.008, val_acc:0.985]
Epoch [101/120    avg_loss:0.007, val_acc:0.985]
Epoch [102/120    avg_loss:0.015, val_acc:0.985]
Epoch [103/120    avg_loss:0.012, val_acc:0.985]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.005, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.988]
Epoch [108/120    avg_loss:0.009, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.006, val_acc:0.988]
Epoch [111/120    avg_loss:0.006, val_acc:0.988]
Epoch [112/120    avg_loss:0.011, val_acc:0.988]
Epoch [113/120    avg_loss:0.008, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.011, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   1   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   2   0   0   0   0   0   0   6   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 0.996337   1.         1.         0.93991416 0.91575092
 0.99038462 1.         1.         1.         1.         1.
 0.99342105 1.        ]

Kappa:
0.9921662629080521
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbee4a237f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.037, val_acc:0.639]
Epoch [2/120    avg_loss:1.291, val_acc:0.611]
Epoch [3/120    avg_loss:1.064, val_acc:0.742]
Epoch [4/120    avg_loss:0.889, val_acc:0.754]
Epoch [5/120    avg_loss:0.802, val_acc:0.713]
Epoch [6/120    avg_loss:0.688, val_acc:0.762]
Epoch [7/120    avg_loss:0.734, val_acc:0.811]
Epoch [8/120    avg_loss:0.668, val_acc:0.838]
Epoch [9/120    avg_loss:0.616, val_acc:0.863]
Epoch [10/120    avg_loss:0.565, val_acc:0.816]
Epoch [11/120    avg_loss:0.554, val_acc:0.852]
Epoch [12/120    avg_loss:0.476, val_acc:0.863]
Epoch [13/120    avg_loss:0.430, val_acc:0.848]
Epoch [14/120    avg_loss:0.430, val_acc:0.906]
Epoch [15/120    avg_loss:0.355, val_acc:0.928]
Epoch [16/120    avg_loss:0.304, val_acc:0.867]
Epoch [17/120    avg_loss:0.392, val_acc:0.912]
Epoch [18/120    avg_loss:0.366, val_acc:0.881]
Epoch [19/120    avg_loss:0.275, val_acc:0.924]
Epoch [20/120    avg_loss:0.290, val_acc:0.926]
Epoch [21/120    avg_loss:0.258, val_acc:0.912]
Epoch [22/120    avg_loss:0.305, val_acc:0.912]
Epoch [23/120    avg_loss:0.299, val_acc:0.932]
Epoch [24/120    avg_loss:0.256, val_acc:0.936]
Epoch [25/120    avg_loss:0.260, val_acc:0.912]
Epoch [26/120    avg_loss:0.247, val_acc:0.928]
Epoch [27/120    avg_loss:0.275, val_acc:0.943]
Epoch [28/120    avg_loss:0.229, val_acc:0.949]
Epoch [29/120    avg_loss:0.294, val_acc:0.941]
Epoch [30/120    avg_loss:0.225, val_acc:0.947]
Epoch [31/120    avg_loss:0.231, val_acc:0.951]
Epoch [32/120    avg_loss:0.210, val_acc:0.967]
Epoch [33/120    avg_loss:0.152, val_acc:0.961]
Epoch [34/120    avg_loss:0.106, val_acc:0.955]
Epoch [35/120    avg_loss:0.101, val_acc:0.975]
Epoch [36/120    avg_loss:0.149, val_acc:0.959]
Epoch [37/120    avg_loss:0.137, val_acc:0.959]
Epoch [38/120    avg_loss:0.119, val_acc:0.977]
Epoch [39/120    avg_loss:0.122, val_acc:0.980]
Epoch [40/120    avg_loss:0.106, val_acc:0.943]
Epoch [41/120    avg_loss:0.147, val_acc:0.977]
Epoch [42/120    avg_loss:0.074, val_acc:0.986]
Epoch [43/120    avg_loss:0.059, val_acc:0.982]
Epoch [44/120    avg_loss:0.096, val_acc:0.965]
Epoch [45/120    avg_loss:0.259, val_acc:0.939]
Epoch [46/120    avg_loss:0.163, val_acc:0.963]
Epoch [47/120    avg_loss:0.116, val_acc:0.988]
Epoch [48/120    avg_loss:0.087, val_acc:0.975]
Epoch [49/120    avg_loss:0.082, val_acc:0.963]
Epoch [50/120    avg_loss:0.070, val_acc:0.986]
Epoch [51/120    avg_loss:0.071, val_acc:0.986]
Epoch [52/120    avg_loss:0.057, val_acc:0.990]
Epoch [53/120    avg_loss:0.042, val_acc:0.992]
Epoch [54/120    avg_loss:0.039, val_acc:0.994]
Epoch [55/120    avg_loss:0.040, val_acc:0.992]
Epoch [56/120    avg_loss:0.065, val_acc:0.988]
Epoch [57/120    avg_loss:0.079, val_acc:0.953]
Epoch [58/120    avg_loss:0.057, val_acc:0.971]
Epoch [59/120    avg_loss:0.109, val_acc:0.947]
Epoch [60/120    avg_loss:0.126, val_acc:0.977]
Epoch [61/120    avg_loss:0.066, val_acc:0.980]
Epoch [62/120    avg_loss:0.052, val_acc:0.980]
Epoch [63/120    avg_loss:0.031, val_acc:0.992]
Epoch [64/120    avg_loss:0.069, val_acc:0.988]
Epoch [65/120    avg_loss:0.050, val_acc:0.990]
Epoch [66/120    avg_loss:0.043, val_acc:0.990]
Epoch [67/120    avg_loss:0.016, val_acc:0.992]
Epoch [68/120    avg_loss:0.018, val_acc:0.994]
Epoch [69/120    avg_loss:0.015, val_acc:0.994]
Epoch [70/120    avg_loss:0.023, val_acc:0.994]
Epoch [71/120    avg_loss:0.011, val_acc:0.994]
Epoch [72/120    avg_loss:0.016, val_acc:0.994]
Epoch [73/120    avg_loss:0.013, val_acc:0.994]
Epoch [74/120    avg_loss:0.019, val_acc:0.994]
Epoch [75/120    avg_loss:0.015, val_acc:0.994]
Epoch [76/120    avg_loss:0.014, val_acc:0.994]
Epoch [77/120    avg_loss:0.014, val_acc:0.994]
Epoch [78/120    avg_loss:0.016, val_acc:0.994]
Epoch [79/120    avg_loss:0.021, val_acc:0.992]
Epoch [80/120    avg_loss:0.011, val_acc:0.994]
Epoch [81/120    avg_loss:0.014, val_acc:0.994]
Epoch [82/120    avg_loss:0.020, val_acc:0.994]
Epoch [83/120    avg_loss:0.012, val_acc:0.994]
Epoch [84/120    avg_loss:0.013, val_acc:0.994]
Epoch [85/120    avg_loss:0.011, val_acc:0.994]
Epoch [86/120    avg_loss:0.016, val_acc:0.994]
Epoch [87/120    avg_loss:0.011, val_acc:0.994]
Epoch [88/120    avg_loss:0.013, val_acc:0.994]
Epoch [89/120    avg_loss:0.012, val_acc:0.994]
Epoch [90/120    avg_loss:0.017, val_acc:0.994]
Epoch [91/120    avg_loss:0.012, val_acc:0.994]
Epoch [92/120    avg_loss:0.015, val_acc:0.994]
Epoch [93/120    avg_loss:0.015, val_acc:0.994]
Epoch [94/120    avg_loss:0.012, val_acc:0.994]
Epoch [95/120    avg_loss:0.012, val_acc:0.994]
Epoch [96/120    avg_loss:0.017, val_acc:0.994]
Epoch [97/120    avg_loss:0.009, val_acc:0.994]
Epoch [98/120    avg_loss:0.010, val_acc:0.994]
Epoch [99/120    avg_loss:0.009, val_acc:0.994]
Epoch [100/120    avg_loss:0.010, val_acc:0.994]
Epoch [101/120    avg_loss:0.012, val_acc:0.994]
Epoch [102/120    avg_loss:0.009, val_acc:0.994]
Epoch [103/120    avg_loss:0.016, val_acc:0.994]
Epoch [104/120    avg_loss:0.011, val_acc:0.994]
Epoch [105/120    avg_loss:0.011, val_acc:0.994]
Epoch [106/120    avg_loss:0.012, val_acc:0.994]
Epoch [107/120    avg_loss:0.009, val_acc:0.994]
Epoch [108/120    avg_loss:0.010, val_acc:0.994]
Epoch [109/120    avg_loss:0.012, val_acc:0.994]
Epoch [110/120    avg_loss:0.012, val_acc:0.994]
Epoch [111/120    avg_loss:0.015, val_acc:0.994]
Epoch [112/120    avg_loss:0.014, val_acc:0.994]
Epoch [113/120    avg_loss:0.012, val_acc:0.994]
Epoch [114/120    avg_loss:0.007, val_acc:0.994]
Epoch [115/120    avg_loss:0.010, val_acc:0.994]
Epoch [116/120    avg_loss:0.013, val_acc:0.994]
Epoch [117/120    avg_loss:0.012, val_acc:0.994]
Epoch [118/120    avg_loss:0.018, val_acc:0.994]
Epoch [119/120    avg_loss:0.009, val_acc:0.994]
Epoch [120/120    avg_loss:0.013, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 224   1   0   0   0   0   0   0   2   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 0.99707174 1.         1.         0.95522388 0.93040293
 0.99038462 1.         1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9940654846563043
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f48959ee860>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.085, val_acc:0.617]
Epoch [2/120    avg_loss:1.364, val_acc:0.678]
Epoch [3/120    avg_loss:0.998, val_acc:0.824]
Epoch [4/120    avg_loss:0.879, val_acc:0.822]
Epoch [5/120    avg_loss:0.727, val_acc:0.844]
Epoch [6/120    avg_loss:0.657, val_acc:0.828]
Epoch [7/120    avg_loss:0.659, val_acc:0.838]
Epoch [8/120    avg_loss:0.590, val_acc:0.836]
Epoch [9/120    avg_loss:0.586, val_acc:0.859]
Epoch [10/120    avg_loss:0.512, val_acc:0.875]
Epoch [11/120    avg_loss:0.648, val_acc:0.826]
Epoch [12/120    avg_loss:0.441, val_acc:0.889]
Epoch [13/120    avg_loss:0.446, val_acc:0.855]
Epoch [14/120    avg_loss:0.381, val_acc:0.902]
Epoch [15/120    avg_loss:0.408, val_acc:0.916]
Epoch [16/120    avg_loss:0.408, val_acc:0.871]
Epoch [17/120    avg_loss:0.325, val_acc:0.842]
Epoch [18/120    avg_loss:0.333, val_acc:0.922]
Epoch [19/120    avg_loss:0.368, val_acc:0.895]
Epoch [20/120    avg_loss:0.298, val_acc:0.932]
Epoch [21/120    avg_loss:0.330, val_acc:0.926]
Epoch [22/120    avg_loss:0.367, val_acc:0.906]
Epoch [23/120    avg_loss:0.404, val_acc:0.936]
Epoch [24/120    avg_loss:0.215, val_acc:0.932]
Epoch [25/120    avg_loss:0.299, val_acc:0.916]
Epoch [26/120    avg_loss:0.225, val_acc:0.906]
Epoch [27/120    avg_loss:0.338, val_acc:0.920]
Epoch [28/120    avg_loss:0.310, val_acc:0.922]
Epoch [29/120    avg_loss:0.203, val_acc:0.945]
Epoch [30/120    avg_loss:0.267, val_acc:0.891]
Epoch [31/120    avg_loss:0.216, val_acc:0.955]
Epoch [32/120    avg_loss:0.285, val_acc:0.949]
Epoch [33/120    avg_loss:0.165, val_acc:0.949]
Epoch [34/120    avg_loss:0.192, val_acc:0.947]
Epoch [35/120    avg_loss:0.160, val_acc:0.961]
Epoch [36/120    avg_loss:0.168, val_acc:0.963]
Epoch [37/120    avg_loss:0.137, val_acc:0.936]
Epoch [38/120    avg_loss:0.129, val_acc:0.963]
Epoch [39/120    avg_loss:0.129, val_acc:0.934]
Epoch [40/120    avg_loss:0.174, val_acc:0.957]
Epoch [41/120    avg_loss:0.114, val_acc:0.951]
Epoch [42/120    avg_loss:0.113, val_acc:0.965]
Epoch [43/120    avg_loss:0.149, val_acc:0.957]
Epoch [44/120    avg_loss:0.140, val_acc:0.963]
Epoch [45/120    avg_loss:0.091, val_acc:0.963]
Epoch [46/120    avg_loss:0.110, val_acc:0.973]
Epoch [47/120    avg_loss:0.099, val_acc:0.980]
Epoch [48/120    avg_loss:0.053, val_acc:0.973]
Epoch [49/120    avg_loss:0.060, val_acc:0.959]
Epoch [50/120    avg_loss:0.073, val_acc:0.988]
Epoch [51/120    avg_loss:0.111, val_acc:0.953]
Epoch [52/120    avg_loss:0.082, val_acc:0.969]
Epoch [53/120    avg_loss:0.058, val_acc:0.977]
Epoch [54/120    avg_loss:0.055, val_acc:0.973]
Epoch [55/120    avg_loss:0.059, val_acc:0.971]
Epoch [56/120    avg_loss:0.176, val_acc:0.973]
Epoch [57/120    avg_loss:0.095, val_acc:0.980]
Epoch [58/120    avg_loss:0.091, val_acc:0.959]
Epoch [59/120    avg_loss:0.091, val_acc:0.988]
Epoch [60/120    avg_loss:0.054, val_acc:0.986]
Epoch [61/120    avg_loss:0.043, val_acc:0.988]
Epoch [62/120    avg_loss:0.050, val_acc:0.980]
Epoch [63/120    avg_loss:0.082, val_acc:0.975]
Epoch [64/120    avg_loss:0.098, val_acc:0.969]
Epoch [65/120    avg_loss:0.070, val_acc:0.992]
Epoch [66/120    avg_loss:0.073, val_acc:0.973]
Epoch [67/120    avg_loss:0.095, val_acc:0.988]
Epoch [68/120    avg_loss:0.050, val_acc:0.975]
Epoch [69/120    avg_loss:0.061, val_acc:0.984]
Epoch [70/120    avg_loss:0.073, val_acc:0.955]
Epoch [71/120    avg_loss:0.060, val_acc:0.986]
Epoch [72/120    avg_loss:0.036, val_acc:0.990]
Epoch [73/120    avg_loss:0.018, val_acc:0.994]
Epoch [74/120    avg_loss:0.048, val_acc:0.975]
Epoch [75/120    avg_loss:0.026, val_acc:0.980]
Epoch [76/120    avg_loss:0.032, val_acc:0.992]
Epoch [77/120    avg_loss:0.023, val_acc:0.975]
Epoch [78/120    avg_loss:0.033, val_acc:0.990]
Epoch [79/120    avg_loss:0.022, val_acc:0.994]
Epoch [80/120    avg_loss:0.019, val_acc:0.982]
Epoch [81/120    avg_loss:0.051, val_acc:0.945]
Epoch [82/120    avg_loss:0.089, val_acc:0.977]
Epoch [83/120    avg_loss:0.051, val_acc:0.980]
Epoch [84/120    avg_loss:0.029, val_acc:0.984]
Epoch [85/120    avg_loss:0.025, val_acc:0.990]
Epoch [86/120    avg_loss:0.022, val_acc:0.990]
Epoch [87/120    avg_loss:0.026, val_acc:0.986]
Epoch [88/120    avg_loss:0.040, val_acc:0.988]
Epoch [89/120    avg_loss:0.035, val_acc:0.990]
Epoch [90/120    avg_loss:0.026, val_acc:0.988]
Epoch [91/120    avg_loss:0.013, val_acc:0.992]
Epoch [92/120    avg_loss:0.010, val_acc:0.992]
Epoch [93/120    avg_loss:0.008, val_acc:0.992]
Epoch [94/120    avg_loss:0.010, val_acc:0.992]
Epoch [95/120    avg_loss:0.015, val_acc:0.992]
Epoch [96/120    avg_loss:0.010, val_acc:0.992]
Epoch [97/120    avg_loss:0.010, val_acc:0.992]
Epoch [98/120    avg_loss:0.009, val_acc:0.992]
Epoch [99/120    avg_loss:0.006, val_acc:0.992]
Epoch [100/120    avg_loss:0.010, val_acc:0.992]
Epoch [101/120    avg_loss:0.007, val_acc:0.992]
Epoch [102/120    avg_loss:0.011, val_acc:0.992]
Epoch [103/120    avg_loss:0.016, val_acc:0.992]
Epoch [104/120    avg_loss:0.011, val_acc:0.992]
Epoch [105/120    avg_loss:0.011, val_acc:0.992]
Epoch [106/120    avg_loss:0.010, val_acc:0.992]
Epoch [107/120    avg_loss:0.010, val_acc:0.992]
Epoch [108/120    avg_loss:0.009, val_acc:0.992]
Epoch [109/120    avg_loss:0.010, val_acc:0.992]
Epoch [110/120    avg_loss:0.008, val_acc:0.992]
Epoch [111/120    avg_loss:0.010, val_acc:0.992]
Epoch [112/120    avg_loss:0.011, val_acc:0.992]
Epoch [113/120    avg_loss:0.010, val_acc:0.992]
Epoch [114/120    avg_loss:0.008, val_acc:0.992]
Epoch [115/120    avg_loss:0.007, val_acc:0.992]
Epoch [116/120    avg_loss:0.007, val_acc:0.992]
Epoch [117/120    avg_loss:0.005, val_acc:0.992]
Epoch [118/120    avg_loss:0.008, val_acc:0.992]
Epoch [119/120    avg_loss:0.012, val_acc:0.992]
Epoch [120/120    avg_loss:0.009, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   1   0   0   0   0  93   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   3   0   0   0   0   0   0   0   0   0 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 0.99780541 0.99095023 1.         0.96086957 0.93661972
 0.99277108 0.99465241 1.         1.         1.         1.
 0.99667774 1.        ]

Kappa:
0.9940659207626069
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:19:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0f7f029828>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:1.971, val_acc:0.543]
Epoch [2/120    avg_loss:1.292, val_acc:0.746]
Epoch [3/120    avg_loss:0.973, val_acc:0.695]
Epoch [4/120    avg_loss:0.842, val_acc:0.809]
Epoch [5/120    avg_loss:0.661, val_acc:0.758]
Epoch [6/120    avg_loss:0.628, val_acc:0.838]
Epoch [7/120    avg_loss:0.596, val_acc:0.820]
Epoch [8/120    avg_loss:0.594, val_acc:0.848]
Epoch [9/120    avg_loss:0.464, val_acc:0.840]
Epoch [10/120    avg_loss:0.457, val_acc:0.828]
Epoch [11/120    avg_loss:0.400, val_acc:0.895]
Epoch [12/120    avg_loss:0.379, val_acc:0.902]
Epoch [13/120    avg_loss:0.303, val_acc:0.918]
Epoch [14/120    avg_loss:0.363, val_acc:0.889]
Epoch [15/120    avg_loss:0.311, val_acc:0.914]
Epoch [16/120    avg_loss:0.342, val_acc:0.893]
Epoch [17/120    avg_loss:0.331, val_acc:0.906]
Epoch [18/120    avg_loss:0.309, val_acc:0.873]
Epoch [19/120    avg_loss:0.321, val_acc:0.902]
Epoch [20/120    avg_loss:0.222, val_acc:0.941]
Epoch [21/120    avg_loss:0.239, val_acc:0.910]
Epoch [22/120    avg_loss:0.224, val_acc:0.959]
Epoch [23/120    avg_loss:0.215, val_acc:0.834]
Epoch [24/120    avg_loss:0.250, val_acc:0.934]
Epoch [25/120    avg_loss:0.171, val_acc:0.949]
Epoch [26/120    avg_loss:0.164, val_acc:0.943]
Epoch [27/120    avg_loss:0.273, val_acc:0.934]
Epoch [28/120    avg_loss:0.254, val_acc:0.928]
Epoch [29/120    avg_loss:0.210, val_acc:0.930]
Epoch [30/120    avg_loss:0.202, val_acc:0.959]
Epoch [31/120    avg_loss:0.157, val_acc:0.955]
Epoch [32/120    avg_loss:0.161, val_acc:0.924]
Epoch [33/120    avg_loss:0.145, val_acc:0.961]
Epoch [34/120    avg_loss:0.149, val_acc:0.971]
Epoch [35/120    avg_loss:0.214, val_acc:0.951]
Epoch [36/120    avg_loss:0.136, val_acc:0.949]
Epoch [37/120    avg_loss:0.111, val_acc:0.963]
Epoch [38/120    avg_loss:0.082, val_acc:0.963]
Epoch [39/120    avg_loss:0.078, val_acc:0.955]
Epoch [40/120    avg_loss:0.086, val_acc:0.945]
Epoch [41/120    avg_loss:0.125, val_acc:0.924]
Epoch [42/120    avg_loss:0.217, val_acc:0.939]
Epoch [43/120    avg_loss:0.099, val_acc:0.982]
Epoch [44/120    avg_loss:0.112, val_acc:0.980]
Epoch [45/120    avg_loss:0.093, val_acc:0.967]
Epoch [46/120    avg_loss:0.070, val_acc:0.990]
Epoch [47/120    avg_loss:0.056, val_acc:0.984]
Epoch [48/120    avg_loss:0.049, val_acc:0.982]
Epoch [49/120    avg_loss:0.046, val_acc:0.982]
Epoch [50/120    avg_loss:0.104, val_acc:0.986]
Epoch [51/120    avg_loss:0.065, val_acc:0.992]
Epoch [52/120    avg_loss:0.029, val_acc:0.986]
Epoch [53/120    avg_loss:0.085, val_acc:0.984]
Epoch [54/120    avg_loss:0.062, val_acc:0.984]
Epoch [55/120    avg_loss:0.033, val_acc:0.988]
Epoch [56/120    avg_loss:0.046, val_acc:0.969]
Epoch [57/120    avg_loss:0.034, val_acc:0.984]
Epoch [58/120    avg_loss:0.024, val_acc:0.994]
Epoch [59/120    avg_loss:0.040, val_acc:0.986]
Epoch [60/120    avg_loss:0.030, val_acc:0.988]
Epoch [61/120    avg_loss:0.027, val_acc:0.992]
Epoch [62/120    avg_loss:0.019, val_acc:0.992]
Epoch [63/120    avg_loss:0.022, val_acc:0.990]
Epoch [64/120    avg_loss:0.021, val_acc:0.980]
Epoch [65/120    avg_loss:0.044, val_acc:0.992]
Epoch [66/120    avg_loss:0.081, val_acc:0.986]
Epoch [67/120    avg_loss:0.057, val_acc:0.961]
Epoch [68/120    avg_loss:0.039, val_acc:0.990]
Epoch [69/120    avg_loss:0.046, val_acc:0.969]
Epoch [70/120    avg_loss:0.047, val_acc:0.982]
Epoch [71/120    avg_loss:0.038, val_acc:0.994]
Epoch [72/120    avg_loss:0.020, val_acc:0.982]
Epoch [73/120    avg_loss:0.051, val_acc:0.988]
Epoch [74/120    avg_loss:0.027, val_acc:0.988]
Epoch [75/120    avg_loss:0.020, val_acc:0.992]
Epoch [76/120    avg_loss:0.033, val_acc:0.992]
Epoch [77/120    avg_loss:0.014, val_acc:0.994]
Epoch [78/120    avg_loss:0.008, val_acc:0.994]
Epoch [79/120    avg_loss:0.012, val_acc:0.996]
Epoch [80/120    avg_loss:0.010, val_acc:0.994]
Epoch [81/120    avg_loss:0.010, val_acc:0.994]
Epoch [82/120    avg_loss:0.008, val_acc:0.994]
Epoch [83/120    avg_loss:0.018, val_acc:0.996]
Epoch [84/120    avg_loss:0.010, val_acc:0.994]
Epoch [85/120    avg_loss:0.007, val_acc:0.994]
Epoch [86/120    avg_loss:0.025, val_acc:0.986]
Epoch [87/120    avg_loss:0.069, val_acc:0.982]
Epoch [88/120    avg_loss:0.034, val_acc:0.990]
Epoch [89/120    avg_loss:0.021, val_acc:0.992]
Epoch [90/120    avg_loss:0.016, val_acc:0.992]
Epoch [91/120    avg_loss:0.015, val_acc:0.990]
Epoch [92/120    avg_loss:0.015, val_acc:0.992]
Epoch [93/120    avg_loss:0.011, val_acc:0.990]
Epoch [94/120    avg_loss:0.022, val_acc:0.992]
Epoch [95/120    avg_loss:0.011, val_acc:0.996]
Epoch [96/120    avg_loss:0.052, val_acc:0.967]
Epoch [97/120    avg_loss:0.047, val_acc:0.988]
Epoch [98/120    avg_loss:0.038, val_acc:0.992]
Epoch [99/120    avg_loss:0.022, val_acc:0.992]
Epoch [100/120    avg_loss:0.018, val_acc:0.992]
Epoch [101/120    avg_loss:0.013, val_acc:0.992]
Epoch [102/120    avg_loss:0.010, val_acc:0.998]
Epoch [103/120    avg_loss:0.028, val_acc:0.949]
Epoch [104/120    avg_loss:0.093, val_acc:0.984]
Epoch [105/120    avg_loss:0.107, val_acc:0.984]
Epoch [106/120    avg_loss:0.028, val_acc:0.992]
Epoch [107/120    avg_loss:0.022, val_acc:0.992]
Epoch [108/120    avg_loss:0.008, val_acc:0.996]
Epoch [109/120    avg_loss:0.008, val_acc:0.994]
Epoch [110/120    avg_loss:0.020, val_acc:0.992]
Epoch [111/120    avg_loss:0.014, val_acc:0.996]
Epoch [112/120    avg_loss:0.014, val_acc:0.996]
Epoch [113/120    avg_loss:0.006, val_acc:0.996]
Epoch [114/120    avg_loss:0.011, val_acc:0.996]
Epoch [115/120    avg_loss:0.009, val_acc:0.998]
Epoch [116/120    avg_loss:0.006, val_acc:0.996]
Epoch [117/120    avg_loss:0.006, val_acc:0.998]
Epoch [118/120    avg_loss:0.005, val_acc:0.996]
Epoch [119/120    avg_loss:0.004, val_acc:0.996]
Epoch [120/120    avg_loss:0.004, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   4   0   0   0   0   0   0   4   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.5095948827292

F1 scores:
[       nan 0.99853801 1.         1.         0.95424837 0.93950178
 0.99516908 1.         1.         1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9945400375146547
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f090b7f0860>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.031, val_acc:0.615]
Epoch [2/120    avg_loss:1.244, val_acc:0.762]
Epoch [3/120    avg_loss:1.038, val_acc:0.777]
Epoch [4/120    avg_loss:0.836, val_acc:0.789]
Epoch [5/120    avg_loss:0.831, val_acc:0.797]
Epoch [6/120    avg_loss:0.755, val_acc:0.789]
Epoch [7/120    avg_loss:0.632, val_acc:0.809]
Epoch [8/120    avg_loss:0.571, val_acc:0.840]
Epoch [9/120    avg_loss:0.525, val_acc:0.844]
Epoch [10/120    avg_loss:0.549, val_acc:0.811]
Epoch [11/120    avg_loss:0.472, val_acc:0.820]
Epoch [12/120    avg_loss:0.523, val_acc:0.885]
Epoch [13/120    avg_loss:0.398, val_acc:0.922]
Epoch [14/120    avg_loss:0.405, val_acc:0.889]
Epoch [15/120    avg_loss:0.395, val_acc:0.828]
Epoch [16/120    avg_loss:0.338, val_acc:0.910]
Epoch [17/120    avg_loss:0.339, val_acc:0.912]
Epoch [18/120    avg_loss:0.276, val_acc:0.914]
Epoch [19/120    avg_loss:0.367, val_acc:0.918]
Epoch [20/120    avg_loss:0.287, val_acc:0.936]
Epoch [21/120    avg_loss:0.233, val_acc:0.939]
Epoch [22/120    avg_loss:0.358, val_acc:0.922]
Epoch [23/120    avg_loss:0.295, val_acc:0.949]
Epoch [24/120    avg_loss:0.213, val_acc:0.926]
Epoch [25/120    avg_loss:0.271, val_acc:0.916]
Epoch [26/120    avg_loss:0.239, val_acc:0.914]
Epoch [27/120    avg_loss:0.205, val_acc:0.939]
Epoch [28/120    avg_loss:0.194, val_acc:0.953]
Epoch [29/120    avg_loss:0.189, val_acc:0.936]
Epoch [30/120    avg_loss:0.226, val_acc:0.953]
Epoch [31/120    avg_loss:0.186, val_acc:0.932]
Epoch [32/120    avg_loss:0.182, val_acc:0.945]
Epoch [33/120    avg_loss:0.165, val_acc:0.934]
Epoch [34/120    avg_loss:0.115, val_acc:0.936]
Epoch [35/120    avg_loss:0.136, val_acc:0.939]
Epoch [36/120    avg_loss:0.155, val_acc:0.969]
Epoch [37/120    avg_loss:0.191, val_acc:0.969]
Epoch [38/120    avg_loss:0.204, val_acc:0.953]
Epoch [39/120    avg_loss:0.151, val_acc:0.959]
Epoch [40/120    avg_loss:0.130, val_acc:0.945]
Epoch [41/120    avg_loss:0.106, val_acc:0.967]
Epoch [42/120    avg_loss:0.084, val_acc:0.961]
Epoch [43/120    avg_loss:0.121, val_acc:0.969]
Epoch [44/120    avg_loss:0.122, val_acc:0.957]
Epoch [45/120    avg_loss:0.175, val_acc:0.914]
Epoch [46/120    avg_loss:0.159, val_acc:0.963]
Epoch [47/120    avg_loss:0.119, val_acc:0.967]
Epoch [48/120    avg_loss:0.094, val_acc:0.961]
Epoch [49/120    avg_loss:0.060, val_acc:0.973]
Epoch [50/120    avg_loss:0.063, val_acc:0.975]
Epoch [51/120    avg_loss:0.062, val_acc:0.977]
Epoch [52/120    avg_loss:0.068, val_acc:0.984]
Epoch [53/120    avg_loss:0.082, val_acc:0.961]
Epoch [54/120    avg_loss:0.093, val_acc:0.961]
Epoch [55/120    avg_loss:0.135, val_acc:0.959]
Epoch [56/120    avg_loss:0.118, val_acc:0.969]
Epoch [57/120    avg_loss:0.124, val_acc:0.957]
Epoch [58/120    avg_loss:0.078, val_acc:0.971]
Epoch [59/120    avg_loss:0.087, val_acc:0.951]
Epoch [60/120    avg_loss:0.104, val_acc:0.961]
Epoch [61/120    avg_loss:0.067, val_acc:0.971]
Epoch [62/120    avg_loss:0.035, val_acc:0.982]
Epoch [63/120    avg_loss:0.035, val_acc:0.982]
Epoch [64/120    avg_loss:0.021, val_acc:0.982]
Epoch [65/120    avg_loss:0.036, val_acc:0.980]
Epoch [66/120    avg_loss:0.022, val_acc:0.982]
Epoch [67/120    avg_loss:0.021, val_acc:0.984]
Epoch [68/120    avg_loss:0.024, val_acc:0.980]
Epoch [69/120    avg_loss:0.023, val_acc:0.977]
Epoch [70/120    avg_loss:0.030, val_acc:0.977]
Epoch [71/120    avg_loss:0.022, val_acc:0.975]
Epoch [72/120    avg_loss:0.016, val_acc:0.980]
Epoch [73/120    avg_loss:0.014, val_acc:0.982]
Epoch [74/120    avg_loss:0.019, val_acc:0.982]
Epoch [75/120    avg_loss:0.020, val_acc:0.982]
Epoch [76/120    avg_loss:0.020, val_acc:0.982]
Epoch [77/120    avg_loss:0.030, val_acc:0.980]
Epoch [78/120    avg_loss:0.017, val_acc:0.982]
Epoch [79/120    avg_loss:0.013, val_acc:0.982]
Epoch [80/120    avg_loss:0.016, val_acc:0.982]
Epoch [81/120    avg_loss:0.018, val_acc:0.982]
Epoch [82/120    avg_loss:0.015, val_acc:0.982]
Epoch [83/120    avg_loss:0.015, val_acc:0.982]
Epoch [84/120    avg_loss:0.014, val_acc:0.984]
Epoch [85/120    avg_loss:0.018, val_acc:0.984]
Epoch [86/120    avg_loss:0.026, val_acc:0.984]
Epoch [87/120    avg_loss:0.022, val_acc:0.984]
Epoch [88/120    avg_loss:0.015, val_acc:0.984]
Epoch [89/120    avg_loss:0.020, val_acc:0.982]
Epoch [90/120    avg_loss:0.016, val_acc:0.984]
Epoch [91/120    avg_loss:0.013, val_acc:0.984]
Epoch [92/120    avg_loss:0.016, val_acc:0.984]
Epoch [93/120    avg_loss:0.016, val_acc:0.984]
Epoch [94/120    avg_loss:0.018, val_acc:0.984]
Epoch [95/120    avg_loss:0.011, val_acc:0.984]
Epoch [96/120    avg_loss:0.019, val_acc:0.982]
Epoch [97/120    avg_loss:0.013, val_acc:0.982]
Epoch [98/120    avg_loss:0.015, val_acc:0.982]
Epoch [99/120    avg_loss:0.021, val_acc:0.982]
Epoch [100/120    avg_loss:0.018, val_acc:0.982]
Epoch [101/120    avg_loss:0.014, val_acc:0.982]
Epoch [102/120    avg_loss:0.011, val_acc:0.982]
Epoch [103/120    avg_loss:0.016, val_acc:0.982]
Epoch [104/120    avg_loss:0.011, val_acc:0.982]
Epoch [105/120    avg_loss:0.015, val_acc:0.982]
Epoch [106/120    avg_loss:0.023, val_acc:0.982]
Epoch [107/120    avg_loss:0.017, val_acc:0.982]
Epoch [108/120    avg_loss:0.018, val_acc:0.982]
Epoch [109/120    avg_loss:0.017, val_acc:0.982]
Epoch [110/120    avg_loss:0.016, val_acc:0.982]
Epoch [111/120    avg_loss:0.017, val_acc:0.982]
Epoch [112/120    avg_loss:0.013, val_acc:0.982]
Epoch [113/120    avg_loss:0.018, val_acc:0.982]
Epoch [114/120    avg_loss:0.019, val_acc:0.982]
Epoch [115/120    avg_loss:0.015, val_acc:0.982]
Epoch [116/120    avg_loss:0.011, val_acc:0.982]
Epoch [117/120    avg_loss:0.017, val_acc:0.982]
Epoch [118/120    avg_loss:0.014, val_acc:0.982]
Epoch [119/120    avg_loss:0.016, val_acc:0.982]
Epoch [120/120    avg_loss:0.015, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   1   0   0   0   0   0   0   4   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   2   0   0   0   0   0   0   0   0   0 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 0.99853801 0.98648649 1.         0.95074946 0.93040293
 0.99516908 0.97826087 1.         1.         1.         1.
 0.99339207 1.        ]

Kappa:
0.9926406638476547
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0deb4f27f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.050, val_acc:0.617]
Epoch [2/120    avg_loss:1.269, val_acc:0.738]
Epoch [3/120    avg_loss:0.947, val_acc:0.758]
Epoch [4/120    avg_loss:0.847, val_acc:0.776]
Epoch [5/120    avg_loss:0.736, val_acc:0.792]
Epoch [6/120    avg_loss:0.690, val_acc:0.750]
Epoch [7/120    avg_loss:0.598, val_acc:0.843]
Epoch [8/120    avg_loss:0.552, val_acc:0.837]
Epoch [9/120    avg_loss:0.570, val_acc:0.859]
Epoch [10/120    avg_loss:0.469, val_acc:0.825]
Epoch [11/120    avg_loss:0.506, val_acc:0.877]
Epoch [12/120    avg_loss:0.501, val_acc:0.897]
Epoch [13/120    avg_loss:0.474, val_acc:0.865]
Epoch [14/120    avg_loss:0.475, val_acc:0.909]
Epoch [15/120    avg_loss:0.354, val_acc:0.907]
Epoch [16/120    avg_loss:0.317, val_acc:0.911]
Epoch [17/120    avg_loss:0.318, val_acc:0.907]
Epoch [18/120    avg_loss:0.370, val_acc:0.810]
Epoch [19/120    avg_loss:0.354, val_acc:0.909]
Epoch [20/120    avg_loss:0.290, val_acc:0.931]
Epoch [21/120    avg_loss:0.230, val_acc:0.931]
Epoch [22/120    avg_loss:0.240, val_acc:0.940]
Epoch [23/120    avg_loss:0.295, val_acc:0.942]
Epoch [24/120    avg_loss:0.286, val_acc:0.933]
Epoch [25/120    avg_loss:0.268, val_acc:0.952]
Epoch [26/120    avg_loss:0.233, val_acc:0.907]
Epoch [27/120    avg_loss:0.246, val_acc:0.940]
Epoch [28/120    avg_loss:0.198, val_acc:0.929]
Epoch [29/120    avg_loss:0.181, val_acc:0.952]
Epoch [30/120    avg_loss:0.225, val_acc:0.954]
Epoch [31/120    avg_loss:0.236, val_acc:0.940]
Epoch [32/120    avg_loss:0.170, val_acc:0.960]
Epoch [33/120    avg_loss:0.139, val_acc:0.948]
Epoch [34/120    avg_loss:0.212, val_acc:0.958]
Epoch [35/120    avg_loss:0.131, val_acc:0.946]
Epoch [36/120    avg_loss:0.153, val_acc:0.913]
Epoch [37/120    avg_loss:0.227, val_acc:0.911]
Epoch [38/120    avg_loss:0.271, val_acc:0.958]
Epoch [39/120    avg_loss:0.163, val_acc:0.940]
Epoch [40/120    avg_loss:0.154, val_acc:0.962]
Epoch [41/120    avg_loss:0.131, val_acc:0.976]
Epoch [42/120    avg_loss:0.169, val_acc:0.968]
Epoch [43/120    avg_loss:0.120, val_acc:0.970]
Epoch [44/120    avg_loss:0.126, val_acc:0.966]
Epoch [45/120    avg_loss:0.085, val_acc:0.978]
Epoch [46/120    avg_loss:0.127, val_acc:0.946]
Epoch [47/120    avg_loss:0.103, val_acc:0.978]
Epoch [48/120    avg_loss:0.095, val_acc:0.966]
Epoch [49/120    avg_loss:0.096, val_acc:0.968]
Epoch [50/120    avg_loss:0.095, val_acc:0.960]
Epoch [51/120    avg_loss:0.073, val_acc:0.974]
Epoch [52/120    avg_loss:0.282, val_acc:0.956]
Epoch [53/120    avg_loss:0.080, val_acc:0.966]
Epoch [54/120    avg_loss:0.083, val_acc:0.978]
Epoch [55/120    avg_loss:0.078, val_acc:0.956]
Epoch [56/120    avg_loss:0.068, val_acc:0.964]
Epoch [57/120    avg_loss:0.084, val_acc:0.964]
Epoch [58/120    avg_loss:0.101, val_acc:0.966]
Epoch [59/120    avg_loss:0.053, val_acc:0.972]
Epoch [60/120    avg_loss:0.051, val_acc:0.970]
Epoch [61/120    avg_loss:0.078, val_acc:0.986]
Epoch [62/120    avg_loss:0.062, val_acc:0.978]
Epoch [63/120    avg_loss:0.097, val_acc:0.974]
Epoch [64/120    avg_loss:0.084, val_acc:0.956]
Epoch [65/120    avg_loss:0.054, val_acc:0.974]
Epoch [66/120    avg_loss:0.065, val_acc:0.956]
Epoch [67/120    avg_loss:0.090, val_acc:0.980]
Epoch [68/120    avg_loss:0.046, val_acc:0.978]
Epoch [69/120    avg_loss:0.077, val_acc:0.982]
Epoch [70/120    avg_loss:0.056, val_acc:0.972]
Epoch [71/120    avg_loss:0.034, val_acc:0.982]
Epoch [72/120    avg_loss:0.041, val_acc:0.988]
Epoch [73/120    avg_loss:0.031, val_acc:0.986]
Epoch [74/120    avg_loss:0.028, val_acc:0.984]
Epoch [75/120    avg_loss:0.052, val_acc:0.990]
Epoch [76/120    avg_loss:0.074, val_acc:0.974]
Epoch [77/120    avg_loss:0.041, val_acc:0.988]
Epoch [78/120    avg_loss:0.065, val_acc:0.968]
Epoch [79/120    avg_loss:0.059, val_acc:0.984]
Epoch [80/120    avg_loss:0.045, val_acc:0.990]
Epoch [81/120    avg_loss:0.037, val_acc:0.990]
Epoch [82/120    avg_loss:0.139, val_acc:0.946]
Epoch [83/120    avg_loss:0.073, val_acc:0.984]
Epoch [84/120    avg_loss:0.042, val_acc:0.984]
Epoch [85/120    avg_loss:0.029, val_acc:0.980]
Epoch [86/120    avg_loss:0.042, val_acc:0.986]
Epoch [87/120    avg_loss:0.026, val_acc:0.988]
Epoch [88/120    avg_loss:0.026, val_acc:0.986]
Epoch [89/120    avg_loss:0.023, val_acc:0.990]
Epoch [90/120    avg_loss:0.045, val_acc:0.990]
Epoch [91/120    avg_loss:0.014, val_acc:0.992]
Epoch [92/120    avg_loss:0.052, val_acc:0.984]
Epoch [93/120    avg_loss:0.029, val_acc:0.992]
Epoch [94/120    avg_loss:0.022, val_acc:0.988]
Epoch [95/120    avg_loss:0.023, val_acc:0.978]
Epoch [96/120    avg_loss:0.041, val_acc:0.984]
Epoch [97/120    avg_loss:0.032, val_acc:0.978]
Epoch [98/120    avg_loss:0.018, val_acc:0.992]
Epoch [99/120    avg_loss:0.011, val_acc:0.992]
Epoch [100/120    avg_loss:0.012, val_acc:0.992]
Epoch [101/120    avg_loss:0.025, val_acc:0.984]
Epoch [102/120    avg_loss:0.045, val_acc:0.982]
Epoch [103/120    avg_loss:0.038, val_acc:0.986]
Epoch [104/120    avg_loss:0.031, val_acc:0.976]
Epoch [105/120    avg_loss:0.044, val_acc:0.988]
Epoch [106/120    avg_loss:0.095, val_acc:0.974]
Epoch [107/120    avg_loss:0.037, val_acc:0.994]
Epoch [108/120    avg_loss:0.029, val_acc:0.988]
Epoch [109/120    avg_loss:0.022, val_acc:0.990]
Epoch [110/120    avg_loss:0.018, val_acc:0.996]
Epoch [111/120    avg_loss:0.023, val_acc:0.984]
Epoch [112/120    avg_loss:0.063, val_acc:0.980]
Epoch [113/120    avg_loss:0.035, val_acc:0.990]
Epoch [114/120    avg_loss:0.047, val_acc:0.976]
Epoch [115/120    avg_loss:0.016, val_acc:0.992]
Epoch [116/120    avg_loss:0.010, val_acc:0.992]
Epoch [117/120    avg_loss:0.007, val_acc:0.992]
Epoch [118/120    avg_loss:0.007, val_acc:0.992]
Epoch [119/120    avg_loss:0.029, val_acc:0.988]
Epoch [120/120    avg_loss:0.048, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 225   0   0   0   0   0   0   0   2   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 362   0   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.40298507462687

F1 scores:
[       nan 0.99707174 0.98648649 1.         0.96774194 0.95306859
 0.99038462 0.96703297 1.         1.         0.99724518 0.99867198
 0.99451153 1.        ]

Kappa:
0.9933531153905648
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f88f13d2780>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.042, val_acc:0.561]
Epoch [2/120    avg_loss:1.365, val_acc:0.742]
Epoch [3/120    avg_loss:1.006, val_acc:0.752]
Epoch [4/120    avg_loss:0.825, val_acc:0.793]
Epoch [5/120    avg_loss:0.753, val_acc:0.801]
Epoch [6/120    avg_loss:0.677, val_acc:0.803]
Epoch [7/120    avg_loss:0.623, val_acc:0.871]
Epoch [8/120    avg_loss:0.651, val_acc:0.824]
Epoch [9/120    avg_loss:0.661, val_acc:0.859]
Epoch [10/120    avg_loss:0.538, val_acc:0.832]
Epoch [11/120    avg_loss:0.491, val_acc:0.861]
Epoch [12/120    avg_loss:0.439, val_acc:0.875]
Epoch [13/120    avg_loss:0.418, val_acc:0.902]
Epoch [14/120    avg_loss:0.383, val_acc:0.900]
Epoch [15/120    avg_loss:0.379, val_acc:0.895]
Epoch [16/120    avg_loss:0.329, val_acc:0.949]
Epoch [17/120    avg_loss:0.281, val_acc:0.945]
Epoch [18/120    avg_loss:0.234, val_acc:0.934]
Epoch [19/120    avg_loss:0.202, val_acc:0.936]
Epoch [20/120    avg_loss:0.265, val_acc:0.953]
Epoch [21/120    avg_loss:0.319, val_acc:0.922]
Epoch [22/120    avg_loss:0.237, val_acc:0.939]
Epoch [23/120    avg_loss:0.236, val_acc:0.934]
Epoch [24/120    avg_loss:0.256, val_acc:0.891]
Epoch [25/120    avg_loss:0.246, val_acc:0.955]
Epoch [26/120    avg_loss:0.182, val_acc:0.953]
Epoch [27/120    avg_loss:0.150, val_acc:0.951]
Epoch [28/120    avg_loss:0.190, val_acc:0.969]
Epoch [29/120    avg_loss:0.202, val_acc:0.965]
Epoch [30/120    avg_loss:0.235, val_acc:0.936]
Epoch [31/120    avg_loss:0.219, val_acc:0.961]
Epoch [32/120    avg_loss:0.192, val_acc:0.951]
Epoch [33/120    avg_loss:0.174, val_acc:0.941]
Epoch [34/120    avg_loss:0.137, val_acc:0.955]
Epoch [35/120    avg_loss:0.099, val_acc:0.965]
Epoch [36/120    avg_loss:0.067, val_acc:0.969]
Epoch [37/120    avg_loss:0.102, val_acc:0.920]
Epoch [38/120    avg_loss:0.101, val_acc:0.975]
Epoch [39/120    avg_loss:0.091, val_acc:0.965]
Epoch [40/120    avg_loss:0.100, val_acc:0.953]
Epoch [41/120    avg_loss:0.112, val_acc:0.969]
Epoch [42/120    avg_loss:0.092, val_acc:0.980]
Epoch [43/120    avg_loss:0.095, val_acc:0.965]
Epoch [44/120    avg_loss:0.083, val_acc:0.971]
Epoch [45/120    avg_loss:0.072, val_acc:0.961]
Epoch [46/120    avg_loss:0.106, val_acc:0.980]
Epoch [47/120    avg_loss:0.065, val_acc:0.969]
Epoch [48/120    avg_loss:0.094, val_acc:0.973]
Epoch [49/120    avg_loss:0.094, val_acc:0.973]
Epoch [50/120    avg_loss:0.102, val_acc:0.971]
Epoch [51/120    avg_loss:0.068, val_acc:0.975]
Epoch [52/120    avg_loss:0.051, val_acc:0.982]
Epoch [53/120    avg_loss:0.041, val_acc:0.986]
Epoch [54/120    avg_loss:0.037, val_acc:0.984]
Epoch [55/120    avg_loss:0.052, val_acc:0.965]
Epoch [56/120    avg_loss:0.096, val_acc:0.947]
Epoch [57/120    avg_loss:0.063, val_acc:0.988]
Epoch [58/120    avg_loss:0.066, val_acc:0.984]
Epoch [59/120    avg_loss:0.050, val_acc:0.986]
Epoch [60/120    avg_loss:0.038, val_acc:0.971]
Epoch [61/120    avg_loss:0.030, val_acc:0.980]
Epoch [62/120    avg_loss:0.041, val_acc:0.971]
Epoch [63/120    avg_loss:0.081, val_acc:0.930]
Epoch [64/120    avg_loss:0.075, val_acc:0.975]
Epoch [65/120    avg_loss:0.042, val_acc:0.988]
Epoch [66/120    avg_loss:0.016, val_acc:0.990]
Epoch [67/120    avg_loss:0.027, val_acc:0.988]
Epoch [68/120    avg_loss:0.015, val_acc:0.990]
Epoch [69/120    avg_loss:0.020, val_acc:0.990]
Epoch [70/120    avg_loss:0.013, val_acc:0.990]
Epoch [71/120    avg_loss:0.017, val_acc:0.990]
Epoch [72/120    avg_loss:0.026, val_acc:0.992]
Epoch [73/120    avg_loss:0.024, val_acc:0.992]
Epoch [74/120    avg_loss:0.021, val_acc:0.992]
Epoch [75/120    avg_loss:0.018, val_acc:0.992]
Epoch [76/120    avg_loss:0.014, val_acc:0.992]
Epoch [77/120    avg_loss:0.015, val_acc:0.994]
Epoch [78/120    avg_loss:0.010, val_acc:0.992]
Epoch [79/120    avg_loss:0.009, val_acc:0.992]
Epoch [80/120    avg_loss:0.012, val_acc:0.990]
Epoch [81/120    avg_loss:0.011, val_acc:0.990]
Epoch [82/120    avg_loss:0.007, val_acc:0.992]
Epoch [83/120    avg_loss:0.011, val_acc:0.990]
Epoch [84/120    avg_loss:0.029, val_acc:0.994]
Epoch [85/120    avg_loss:0.024, val_acc:0.984]
Epoch [86/120    avg_loss:0.008, val_acc:0.990]
Epoch [87/120    avg_loss:0.020, val_acc:0.920]
Epoch [88/120    avg_loss:0.033, val_acc:0.986]
Epoch [89/120    avg_loss:0.078, val_acc:0.988]
Epoch [90/120    avg_loss:0.016, val_acc:0.992]
Epoch [91/120    avg_loss:0.014, val_acc:0.990]
Epoch [92/120    avg_loss:0.017, val_acc:0.992]
Epoch [93/120    avg_loss:0.013, val_acc:0.992]
Epoch [94/120    avg_loss:0.008, val_acc:0.990]
Epoch [95/120    avg_loss:0.007, val_acc:0.990]
Epoch [96/120    avg_loss:0.006, val_acc:0.990]
Epoch [97/120    avg_loss:0.006, val_acc:0.994]
Epoch [98/120    avg_loss:0.007, val_acc:0.992]
Epoch [99/120    avg_loss:0.006, val_acc:0.992]
Epoch [100/120    avg_loss:0.025, val_acc:0.982]
Epoch [101/120    avg_loss:0.014, val_acc:0.990]
Epoch [102/120    avg_loss:0.008, val_acc:0.992]
Epoch [103/120    avg_loss:0.012, val_acc:0.990]
Epoch [104/120    avg_loss:0.006, val_acc:0.992]
Epoch [105/120    avg_loss:0.026, val_acc:0.975]
Epoch [106/120    avg_loss:0.034, val_acc:0.984]
Epoch [107/120    avg_loss:0.016, val_acc:0.986]
Epoch [108/120    avg_loss:0.010, val_acc:0.990]
Epoch [109/120    avg_loss:0.010, val_acc:0.996]
Epoch [110/120    avg_loss:0.015, val_acc:0.984]
Epoch [111/120    avg_loss:0.010, val_acc:0.996]
Epoch [112/120    avg_loss:0.013, val_acc:0.992]
Epoch [113/120    avg_loss:0.006, val_acc:0.994]
Epoch [114/120    avg_loss:0.004, val_acc:0.994]
Epoch [115/120    avg_loss:0.005, val_acc:0.994]
Epoch [116/120    avg_loss:0.004, val_acc:0.994]
Epoch [117/120    avg_loss:0.005, val_acc:0.992]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.007, val_acc:0.992]
Epoch [120/120    avg_loss:0.006, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 680   0   0   0   0   5   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  11   0   0   0   0   0   0   1   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.996337   1.         1.         0.94713656 0.92041522
 0.98800959 1.         1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9931166649130636
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2fc8d037f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.024, val_acc:0.537]
Epoch [2/120    avg_loss:1.338, val_acc:0.732]
Epoch [3/120    avg_loss:0.982, val_acc:0.699]
Epoch [4/120    avg_loss:0.837, val_acc:0.764]
Epoch [5/120    avg_loss:0.749, val_acc:0.834]
Epoch [6/120    avg_loss:0.687, val_acc:0.795]
Epoch [7/120    avg_loss:0.621, val_acc:0.871]
Epoch [8/120    avg_loss:0.590, val_acc:0.842]
Epoch [9/120    avg_loss:0.549, val_acc:0.863]
Epoch [10/120    avg_loss:0.509, val_acc:0.852]
Epoch [11/120    avg_loss:0.494, val_acc:0.904]
Epoch [12/120    avg_loss:0.430, val_acc:0.922]
Epoch [13/120    avg_loss:0.412, val_acc:0.906]
Epoch [14/120    avg_loss:0.312, val_acc:0.912]
Epoch [15/120    avg_loss:0.361, val_acc:0.900]
Epoch [16/120    avg_loss:0.353, val_acc:0.900]
Epoch [17/120    avg_loss:0.341, val_acc:0.930]
Epoch [18/120    avg_loss:0.338, val_acc:0.947]
Epoch [19/120    avg_loss:0.301, val_acc:0.914]
Epoch [20/120    avg_loss:0.278, val_acc:0.943]
Epoch [21/120    avg_loss:0.294, val_acc:0.912]
Epoch [22/120    avg_loss:0.231, val_acc:0.920]
Epoch [23/120    avg_loss:0.203, val_acc:0.961]
Epoch [24/120    avg_loss:0.189, val_acc:0.959]
Epoch [25/120    avg_loss:0.163, val_acc:0.924]
Epoch [26/120    avg_loss:0.182, val_acc:0.947]
Epoch [27/120    avg_loss:0.225, val_acc:0.939]
Epoch [28/120    avg_loss:0.174, val_acc:0.900]
Epoch [29/120    avg_loss:0.146, val_acc:0.975]
Epoch [30/120    avg_loss:0.166, val_acc:0.922]
Epoch [31/120    avg_loss:0.140, val_acc:0.957]
Epoch [32/120    avg_loss:0.173, val_acc:0.949]
Epoch [33/120    avg_loss:0.122, val_acc:0.971]
Epoch [34/120    avg_loss:0.095, val_acc:0.947]
Epoch [35/120    avg_loss:0.179, val_acc:0.959]
Epoch [36/120    avg_loss:0.142, val_acc:0.959]
Epoch [37/120    avg_loss:0.084, val_acc:0.977]
Epoch [38/120    avg_loss:0.074, val_acc:0.967]
Epoch [39/120    avg_loss:0.064, val_acc:0.973]
Epoch [40/120    avg_loss:0.062, val_acc:0.980]
Epoch [41/120    avg_loss:0.056, val_acc:0.984]
Epoch [42/120    avg_loss:0.067, val_acc:0.965]
Epoch [43/120    avg_loss:0.070, val_acc:0.977]
Epoch [44/120    avg_loss:0.072, val_acc:0.984]
Epoch [45/120    avg_loss:0.057, val_acc:0.980]
Epoch [46/120    avg_loss:0.049, val_acc:0.977]
Epoch [47/120    avg_loss:0.087, val_acc:0.959]
Epoch [48/120    avg_loss:0.078, val_acc:0.975]
Epoch [49/120    avg_loss:0.052, val_acc:0.975]
Epoch [50/120    avg_loss:0.046, val_acc:0.980]
Epoch [51/120    avg_loss:0.040, val_acc:0.990]
Epoch [52/120    avg_loss:0.046, val_acc:0.951]
Epoch [53/120    avg_loss:0.028, val_acc:0.988]
Epoch [54/120    avg_loss:0.029, val_acc:0.988]
Epoch [55/120    avg_loss:0.029, val_acc:0.975]
Epoch [56/120    avg_loss:0.039, val_acc:0.990]
Epoch [57/120    avg_loss:0.019, val_acc:0.990]
Epoch [58/120    avg_loss:0.016, val_acc:0.988]
Epoch [59/120    avg_loss:0.039, val_acc:0.984]
Epoch [60/120    avg_loss:0.046, val_acc:0.975]
Epoch [61/120    avg_loss:0.018, val_acc:0.990]
Epoch [62/120    avg_loss:0.015, val_acc:0.986]
Epoch [63/120    avg_loss:0.017, val_acc:0.984]
Epoch [64/120    avg_loss:0.042, val_acc:0.975]
Epoch [65/120    avg_loss:0.041, val_acc:0.988]
Epoch [66/120    avg_loss:0.030, val_acc:0.990]
Epoch [67/120    avg_loss:0.032, val_acc:0.961]
Epoch [68/120    avg_loss:0.025, val_acc:0.986]
Epoch [69/120    avg_loss:0.067, val_acc:0.980]
Epoch [70/120    avg_loss:0.029, val_acc:0.980]
Epoch [71/120    avg_loss:0.017, val_acc:0.984]
Epoch [72/120    avg_loss:0.018, val_acc:0.984]
Epoch [73/120    avg_loss:0.038, val_acc:0.990]
Epoch [74/120    avg_loss:0.021, val_acc:0.988]
Epoch [75/120    avg_loss:0.015, val_acc:0.992]
Epoch [76/120    avg_loss:0.013, val_acc:0.984]
Epoch [77/120    avg_loss:0.006, val_acc:0.988]
Epoch [78/120    avg_loss:0.018, val_acc:0.992]
Epoch [79/120    avg_loss:0.009, val_acc:0.990]
Epoch [80/120    avg_loss:0.012, val_acc:0.986]
Epoch [81/120    avg_loss:0.026, val_acc:0.986]
Epoch [82/120    avg_loss:0.058, val_acc:0.990]
Epoch [83/120    avg_loss:0.087, val_acc:0.949]
Epoch [84/120    avg_loss:0.035, val_acc:0.984]
Epoch [85/120    avg_loss:0.056, val_acc:0.928]
Epoch [86/120    avg_loss:0.055, val_acc:0.977]
Epoch [87/120    avg_loss:0.057, val_acc:0.988]
Epoch [88/120    avg_loss:0.017, val_acc:0.990]
Epoch [89/120    avg_loss:0.009, val_acc:0.992]
Epoch [90/120    avg_loss:0.009, val_acc:0.994]
Epoch [91/120    avg_loss:0.010, val_acc:0.990]
Epoch [92/120    avg_loss:0.012, val_acc:0.988]
Epoch [93/120    avg_loss:0.012, val_acc:0.988]
Epoch [94/120    avg_loss:0.040, val_acc:0.973]
Epoch [95/120    avg_loss:0.079, val_acc:0.980]
Epoch [96/120    avg_loss:0.047, val_acc:0.986]
Epoch [97/120    avg_loss:0.052, val_acc:0.988]
Epoch [98/120    avg_loss:0.019, val_acc:0.990]
Epoch [99/120    avg_loss:0.012, val_acc:0.990]
Epoch [100/120    avg_loss:0.007, val_acc:0.990]
Epoch [101/120    avg_loss:0.006, val_acc:0.992]
Epoch [102/120    avg_loss:0.009, val_acc:0.992]
Epoch [103/120    avg_loss:0.022, val_acc:0.984]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.005, val_acc:0.986]
Epoch [106/120    avg_loss:0.010, val_acc:0.988]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.004, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.004, val_acc:0.990]
Epoch [114/120    avg_loss:0.005, val_acc:0.990]
Epoch [115/120    avg_loss:0.010, val_acc:0.992]
Epoch [116/120    avg_loss:0.004, val_acc:0.992]
Epoch [117/120    avg_loss:0.005, val_acc:0.992]
Epoch [118/120    avg_loss:0.004, val_acc:0.992]
Epoch [119/120    avg_loss:0.006, val_acc:0.992]
Epoch [120/120    avg_loss:0.007, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 683   0   0   0   0   2   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   5   0   0   0   0   0   0   3   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.53091684434968

F1 scores:
[       nan 0.99853801 1.         1.         0.95633188 0.93992933
 0.99516908 1.         1.         1.         1.         1.
 0.99669967 1.        ]

Kappa:
0.9947775302810112
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f733b49b828>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.042, val_acc:0.558]
Epoch [2/120    avg_loss:1.307, val_acc:0.663]
Epoch [3/120    avg_loss:0.972, val_acc:0.746]
Epoch [4/120    avg_loss:0.777, val_acc:0.780]
Epoch [5/120    avg_loss:0.701, val_acc:0.758]
Epoch [6/120    avg_loss:0.686, val_acc:0.800]
Epoch [7/120    avg_loss:0.608, val_acc:0.806]
Epoch [8/120    avg_loss:0.602, val_acc:0.845]
Epoch [9/120    avg_loss:0.529, val_acc:0.849]
Epoch [10/120    avg_loss:0.486, val_acc:0.808]
Epoch [11/120    avg_loss:0.496, val_acc:0.839]
Epoch [12/120    avg_loss:0.482, val_acc:0.891]
Epoch [13/120    avg_loss:0.468, val_acc:0.853]
Epoch [14/120    avg_loss:0.422, val_acc:0.875]
Epoch [15/120    avg_loss:0.379, val_acc:0.903]
Epoch [16/120    avg_loss:0.389, val_acc:0.895]
Epoch [17/120    avg_loss:0.290, val_acc:0.887]
Epoch [18/120    avg_loss:0.408, val_acc:0.893]
Epoch [19/120    avg_loss:0.317, val_acc:0.903]
Epoch [20/120    avg_loss:0.352, val_acc:0.893]
Epoch [21/120    avg_loss:0.264, val_acc:0.950]
Epoch [22/120    avg_loss:0.303, val_acc:0.907]
Epoch [23/120    avg_loss:0.223, val_acc:0.917]
Epoch [24/120    avg_loss:0.205, val_acc:0.958]
Epoch [25/120    avg_loss:0.207, val_acc:0.911]
Epoch [26/120    avg_loss:0.187, val_acc:0.952]
Epoch [27/120    avg_loss:0.238, val_acc:0.946]
Epoch [28/120    avg_loss:0.209, val_acc:0.927]
Epoch [29/120    avg_loss:0.213, val_acc:0.923]
Epoch [30/120    avg_loss:0.240, val_acc:0.956]
Epoch [31/120    avg_loss:0.194, val_acc:0.935]
Epoch [32/120    avg_loss:0.139, val_acc:0.964]
Epoch [33/120    avg_loss:0.173, val_acc:0.956]
Epoch [34/120    avg_loss:0.129, val_acc:0.978]
Epoch [35/120    avg_loss:0.103, val_acc:0.964]
Epoch [36/120    avg_loss:0.098, val_acc:0.980]
Epoch [37/120    avg_loss:0.078, val_acc:0.962]
Epoch [38/120    avg_loss:0.086, val_acc:0.974]
Epoch [39/120    avg_loss:0.228, val_acc:0.950]
Epoch [40/120    avg_loss:0.145, val_acc:0.966]
Epoch [41/120    avg_loss:0.128, val_acc:0.968]
Epoch [42/120    avg_loss:0.095, val_acc:0.972]
Epoch [43/120    avg_loss:0.114, val_acc:0.958]
Epoch [44/120    avg_loss:0.091, val_acc:0.972]
Epoch [45/120    avg_loss:0.089, val_acc:0.970]
Epoch [46/120    avg_loss:0.088, val_acc:0.970]
Epoch [47/120    avg_loss:0.100, val_acc:0.978]
Epoch [48/120    avg_loss:0.063, val_acc:0.982]
Epoch [49/120    avg_loss:0.054, val_acc:0.984]
Epoch [50/120    avg_loss:0.069, val_acc:0.976]
Epoch [51/120    avg_loss:0.072, val_acc:0.976]
Epoch [52/120    avg_loss:0.045, val_acc:0.984]
Epoch [53/120    avg_loss:0.064, val_acc:0.982]
Epoch [54/120    avg_loss:0.059, val_acc:0.980]
Epoch [55/120    avg_loss:0.091, val_acc:0.956]
Epoch [56/120    avg_loss:0.073, val_acc:0.972]
Epoch [57/120    avg_loss:0.100, val_acc:0.982]
Epoch [58/120    avg_loss:0.058, val_acc:0.970]
Epoch [59/120    avg_loss:0.040, val_acc:0.970]
Epoch [60/120    avg_loss:0.045, val_acc:0.984]
Epoch [61/120    avg_loss:0.046, val_acc:0.970]
Epoch [62/120    avg_loss:0.103, val_acc:0.952]
Epoch [63/120    avg_loss:0.078, val_acc:0.978]
Epoch [64/120    avg_loss:0.038, val_acc:0.980]
Epoch [65/120    avg_loss:0.035, val_acc:0.980]
Epoch [66/120    avg_loss:0.042, val_acc:0.988]
Epoch [67/120    avg_loss:0.045, val_acc:0.974]
Epoch [68/120    avg_loss:0.026, val_acc:0.986]
Epoch [69/120    avg_loss:0.020, val_acc:0.984]
Epoch [70/120    avg_loss:0.022, val_acc:0.982]
Epoch [71/120    avg_loss:0.033, val_acc:0.988]
Epoch [72/120    avg_loss:0.022, val_acc:0.990]
Epoch [73/120    avg_loss:0.038, val_acc:0.982]
Epoch [74/120    avg_loss:0.028, val_acc:0.984]
Epoch [75/120    avg_loss:0.017, val_acc:0.982]
Epoch [76/120    avg_loss:0.044, val_acc:0.948]
Epoch [77/120    avg_loss:0.048, val_acc:0.980]
Epoch [78/120    avg_loss:0.042, val_acc:0.986]
Epoch [79/120    avg_loss:0.027, val_acc:0.984]
Epoch [80/120    avg_loss:0.041, val_acc:0.996]
Epoch [81/120    avg_loss:0.089, val_acc:0.986]
Epoch [82/120    avg_loss:0.039, val_acc:0.986]
Epoch [83/120    avg_loss:0.013, val_acc:0.986]
Epoch [84/120    avg_loss:0.010, val_acc:0.984]
Epoch [85/120    avg_loss:0.012, val_acc:0.986]
Epoch [86/120    avg_loss:0.024, val_acc:0.972]
Epoch [87/120    avg_loss:0.042, val_acc:0.978]
Epoch [88/120    avg_loss:0.013, val_acc:0.992]
Epoch [89/120    avg_loss:0.010, val_acc:0.984]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.011, val_acc:0.988]
Epoch [92/120    avg_loss:0.011, val_acc:0.990]
Epoch [93/120    avg_loss:0.023, val_acc:0.988]
Epoch [94/120    avg_loss:0.026, val_acc:0.988]
Epoch [95/120    avg_loss:0.009, val_acc:0.988]
Epoch [96/120    avg_loss:0.015, val_acc:0.988]
Epoch [97/120    avg_loss:0.009, val_acc:0.988]
Epoch [98/120    avg_loss:0.012, val_acc:0.990]
Epoch [99/120    avg_loss:0.009, val_acc:0.990]
Epoch [100/120    avg_loss:0.011, val_acc:0.990]
Epoch [101/120    avg_loss:0.019, val_acc:0.990]
Epoch [102/120    avg_loss:0.008, val_acc:0.990]
Epoch [103/120    avg_loss:0.010, val_acc:0.990]
Epoch [104/120    avg_loss:0.008, val_acc:0.992]
Epoch [105/120    avg_loss:0.007, val_acc:0.994]
Epoch [106/120    avg_loss:0.012, val_acc:0.994]
Epoch [107/120    avg_loss:0.011, val_acc:0.994]
Epoch [108/120    avg_loss:0.009, val_acc:0.994]
Epoch [109/120    avg_loss:0.007, val_acc:0.994]
Epoch [110/120    avg_loss:0.009, val_acc:0.994]
Epoch [111/120    avg_loss:0.010, val_acc:0.994]
Epoch [112/120    avg_loss:0.006, val_acc:0.994]
Epoch [113/120    avg_loss:0.006, val_acc:0.994]
Epoch [114/120    avg_loss:0.008, val_acc:0.992]
Epoch [115/120    avg_loss:0.013, val_acc:0.992]
Epoch [116/120    avg_loss:0.011, val_acc:0.992]
Epoch [117/120    avg_loss:0.014, val_acc:0.992]
Epoch [118/120    avg_loss:0.008, val_acc:0.992]
Epoch [119/120    avg_loss:0.010, val_acc:0.992]
Epoch [120/120    avg_loss:0.006, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 681   0   0   0   0   4   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  11   0   0   0   0   0   0   1   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 0.99707174 1.         1.         0.94505495 0.91666667
 0.99038462 1.         1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9931164694781393
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:20:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc9b4740748>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.061, val_acc:0.627]
Epoch [2/120    avg_loss:1.348, val_acc:0.704]
Epoch [3/120    avg_loss:1.020, val_acc:0.740]
Epoch [4/120    avg_loss:0.957, val_acc:0.748]
Epoch [5/120    avg_loss:0.831, val_acc:0.823]
Epoch [6/120    avg_loss:0.743, val_acc:0.821]
Epoch [7/120    avg_loss:0.656, val_acc:0.833]
Epoch [8/120    avg_loss:0.614, val_acc:0.835]
Epoch [9/120    avg_loss:0.575, val_acc:0.798]
Epoch [10/120    avg_loss:0.548, val_acc:0.852]
Epoch [11/120    avg_loss:0.589, val_acc:0.821]
Epoch [12/120    avg_loss:0.558, val_acc:0.877]
Epoch [13/120    avg_loss:0.451, val_acc:0.879]
Epoch [14/120    avg_loss:0.402, val_acc:0.885]
Epoch [15/120    avg_loss:0.395, val_acc:0.896]
Epoch [16/120    avg_loss:0.334, val_acc:0.921]
Epoch [17/120    avg_loss:0.373, val_acc:0.871]
Epoch [18/120    avg_loss:0.459, val_acc:0.921]
Epoch [19/120    avg_loss:0.369, val_acc:0.935]
Epoch [20/120    avg_loss:0.329, val_acc:0.944]
Epoch [21/120    avg_loss:0.326, val_acc:0.929]
Epoch [22/120    avg_loss:0.265, val_acc:0.942]
Epoch [23/120    avg_loss:0.292, val_acc:0.900]
Epoch [24/120    avg_loss:0.295, val_acc:0.900]
Epoch [25/120    avg_loss:0.296, val_acc:0.942]
Epoch [26/120    avg_loss:0.228, val_acc:0.960]
Epoch [27/120    avg_loss:0.285, val_acc:0.967]
Epoch [28/120    avg_loss:0.237, val_acc:0.973]
Epoch [29/120    avg_loss:0.202, val_acc:0.948]
Epoch [30/120    avg_loss:0.150, val_acc:0.948]
Epoch [31/120    avg_loss:0.149, val_acc:0.963]
Epoch [32/120    avg_loss:0.166, val_acc:0.960]
Epoch [33/120    avg_loss:0.189, val_acc:0.948]
Epoch [34/120    avg_loss:0.224, val_acc:0.954]
Epoch [35/120    avg_loss:0.152, val_acc:0.967]
Epoch [36/120    avg_loss:0.103, val_acc:0.956]
Epoch [37/120    avg_loss:0.109, val_acc:0.960]
Epoch [38/120    avg_loss:0.117, val_acc:0.969]
Epoch [39/120    avg_loss:0.085, val_acc:0.975]
Epoch [40/120    avg_loss:0.108, val_acc:0.975]
Epoch [41/120    avg_loss:0.114, val_acc:0.965]
Epoch [42/120    avg_loss:0.168, val_acc:0.948]
Epoch [43/120    avg_loss:0.103, val_acc:0.954]
Epoch [44/120    avg_loss:0.069, val_acc:0.973]
Epoch [45/120    avg_loss:0.083, val_acc:0.958]
Epoch [46/120    avg_loss:0.127, val_acc:0.958]
Epoch [47/120    avg_loss:0.110, val_acc:0.975]
Epoch [48/120    avg_loss:0.089, val_acc:0.969]
Epoch [49/120    avg_loss:0.062, val_acc:0.952]
Epoch [50/120    avg_loss:0.078, val_acc:0.971]
Epoch [51/120    avg_loss:0.071, val_acc:0.979]
Epoch [52/120    avg_loss:0.124, val_acc:0.971]
Epoch [53/120    avg_loss:0.090, val_acc:0.977]
Epoch [54/120    avg_loss:0.049, val_acc:0.975]
Epoch [55/120    avg_loss:0.043, val_acc:0.979]
Epoch [56/120    avg_loss:0.036, val_acc:0.981]
Epoch [57/120    avg_loss:0.018, val_acc:0.977]
Epoch [58/120    avg_loss:0.026, val_acc:0.983]
Epoch [59/120    avg_loss:0.036, val_acc:0.983]
Epoch [60/120    avg_loss:0.038, val_acc:0.985]
Epoch [61/120    avg_loss:0.019, val_acc:0.983]
Epoch [62/120    avg_loss:0.034, val_acc:0.977]
Epoch [63/120    avg_loss:0.027, val_acc:0.969]
Epoch [64/120    avg_loss:0.026, val_acc:0.985]
Epoch [65/120    avg_loss:0.019, val_acc:0.992]
Epoch [66/120    avg_loss:0.055, val_acc:0.985]
Epoch [67/120    avg_loss:0.020, val_acc:0.990]
Epoch [68/120    avg_loss:0.025, val_acc:0.988]
Epoch [69/120    avg_loss:0.050, val_acc:0.969]
Epoch [70/120    avg_loss:0.052, val_acc:0.988]
Epoch [71/120    avg_loss:0.053, val_acc:0.969]
Epoch [72/120    avg_loss:0.039, val_acc:0.942]
Epoch [73/120    avg_loss:0.038, val_acc:0.977]
Epoch [74/120    avg_loss:0.046, val_acc:0.994]
Epoch [75/120    avg_loss:0.017, val_acc:0.988]
Epoch [76/120    avg_loss:0.017, val_acc:0.985]
Epoch [77/120    avg_loss:0.012, val_acc:0.992]
Epoch [78/120    avg_loss:0.010, val_acc:0.992]
Epoch [79/120    avg_loss:0.007, val_acc:0.992]
Epoch [80/120    avg_loss:0.024, val_acc:0.985]
Epoch [81/120    avg_loss:0.018, val_acc:0.988]
Epoch [82/120    avg_loss:0.007, val_acc:0.992]
Epoch [83/120    avg_loss:0.008, val_acc:0.992]
Epoch [84/120    avg_loss:0.013, val_acc:0.979]
Epoch [85/120    avg_loss:0.031, val_acc:0.983]
Epoch [86/120    avg_loss:0.027, val_acc:0.963]
Epoch [87/120    avg_loss:0.040, val_acc:0.994]
Epoch [88/120    avg_loss:0.040, val_acc:0.975]
Epoch [89/120    avg_loss:0.070, val_acc:0.975]
Epoch [90/120    avg_loss:0.022, val_acc:0.990]
Epoch [91/120    avg_loss:0.008, val_acc:0.990]
Epoch [92/120    avg_loss:0.014, val_acc:0.981]
Epoch [93/120    avg_loss:0.011, val_acc:0.988]
Epoch [94/120    avg_loss:0.008, val_acc:0.985]
Epoch [95/120    avg_loss:0.012, val_acc:0.983]
Epoch [96/120    avg_loss:0.024, val_acc:0.992]
Epoch [97/120    avg_loss:0.008, val_acc:0.992]
Epoch [98/120    avg_loss:0.060, val_acc:0.942]
Epoch [99/120    avg_loss:0.102, val_acc:0.981]
Epoch [100/120    avg_loss:0.089, val_acc:0.944]
Epoch [101/120    avg_loss:0.047, val_acc:0.973]
Epoch [102/120    avg_loss:0.026, val_acc:0.983]
Epoch [103/120    avg_loss:0.015, val_acc:0.985]
Epoch [104/120    avg_loss:0.017, val_acc:0.983]
Epoch [105/120    avg_loss:0.015, val_acc:0.983]
Epoch [106/120    avg_loss:0.022, val_acc:0.983]
Epoch [107/120    avg_loss:0.014, val_acc:0.985]
Epoch [108/120    avg_loss:0.016, val_acc:0.985]
Epoch [109/120    avg_loss:0.017, val_acc:0.985]
Epoch [110/120    avg_loss:0.012, val_acc:0.985]
Epoch [111/120    avg_loss:0.014, val_acc:0.985]
Epoch [112/120    avg_loss:0.010, val_acc:0.985]
Epoch [113/120    avg_loss:0.009, val_acc:0.985]
Epoch [114/120    avg_loss:0.011, val_acc:0.985]
Epoch [115/120    avg_loss:0.010, val_acc:0.985]
Epoch [116/120    avg_loss:0.008, val_acc:0.985]
Epoch [117/120    avg_loss:0.015, val_acc:0.985]
Epoch [118/120    avg_loss:0.013, val_acc:0.985]
Epoch [119/120    avg_loss:0.007, val_acc:0.985]
Epoch [120/120    avg_loss:0.009, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 219   8   0   0   0   0   0   0   0   0]
 [  0   0   0   0  12 133   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   3   0   0   0   0  91   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.48827292110875

F1 scores:
[       nan 1.         0.99095023 1.         0.95633188 0.93006993
 1.         0.98378378 1.         1.         1.         1.
 0.99889503 1.        ]

Kappa:
0.9943027098419626
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f793239a7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.256, val_acc:0.568]
Epoch [2/120    avg_loss:1.729, val_acc:0.656]
Epoch [3/120    avg_loss:1.362, val_acc:0.721]
Epoch [4/120    avg_loss:1.049, val_acc:0.809]
Epoch [5/120    avg_loss:0.888, val_acc:0.828]
Epoch [6/120    avg_loss:0.803, val_acc:0.781]
Epoch [7/120    avg_loss:0.686, val_acc:0.871]
Epoch [8/120    avg_loss:0.598, val_acc:0.854]
Epoch [9/120    avg_loss:0.562, val_acc:0.904]
Epoch [10/120    avg_loss:0.540, val_acc:0.871]
Epoch [11/120    avg_loss:0.524, val_acc:0.887]
Epoch [12/120    avg_loss:0.573, val_acc:0.875]
Epoch [13/120    avg_loss:0.457, val_acc:0.912]
Epoch [14/120    avg_loss:0.417, val_acc:0.896]
Epoch [15/120    avg_loss:0.430, val_acc:0.883]
Epoch [16/120    avg_loss:0.468, val_acc:0.893]
Epoch [17/120    avg_loss:0.498, val_acc:0.896]
Epoch [18/120    avg_loss:0.452, val_acc:0.893]
Epoch [19/120    avg_loss:0.381, val_acc:0.896]
Epoch [20/120    avg_loss:0.391, val_acc:0.896]
Epoch [21/120    avg_loss:0.416, val_acc:0.893]
Epoch [22/120    avg_loss:0.440, val_acc:0.922]
Epoch [23/120    avg_loss:0.431, val_acc:0.920]
Epoch [24/120    avg_loss:0.335, val_acc:0.930]
Epoch [25/120    avg_loss:0.312, val_acc:0.898]
Epoch [26/120    avg_loss:0.315, val_acc:0.910]
Epoch [27/120    avg_loss:0.289, val_acc:0.928]
Epoch [28/120    avg_loss:0.275, val_acc:0.926]
Epoch [29/120    avg_loss:0.283, val_acc:0.918]
Epoch [30/120    avg_loss:0.334, val_acc:0.908]
Epoch [31/120    avg_loss:0.329, val_acc:0.898]
Epoch [32/120    avg_loss:0.292, val_acc:0.941]
Epoch [33/120    avg_loss:0.253, val_acc:0.926]
Epoch [34/120    avg_loss:0.240, val_acc:0.896]
Epoch [35/120    avg_loss:0.320, val_acc:0.924]
Epoch [36/120    avg_loss:0.251, val_acc:0.955]
Epoch [37/120    avg_loss:0.254, val_acc:0.910]
Epoch [38/120    avg_loss:0.205, val_acc:0.943]
Epoch [39/120    avg_loss:0.207, val_acc:0.938]
Epoch [40/120    avg_loss:0.187, val_acc:0.953]
Epoch [41/120    avg_loss:0.211, val_acc:0.939]
Epoch [42/120    avg_loss:0.261, val_acc:0.938]
Epoch [43/120    avg_loss:0.161, val_acc:0.939]
Epoch [44/120    avg_loss:0.157, val_acc:0.959]
Epoch [45/120    avg_loss:0.223, val_acc:0.924]
Epoch [46/120    avg_loss:0.250, val_acc:0.955]
Epoch [47/120    avg_loss:0.256, val_acc:0.943]
Epoch [48/120    avg_loss:0.180, val_acc:0.930]
Epoch [49/120    avg_loss:0.193, val_acc:0.955]
Epoch [50/120    avg_loss:0.177, val_acc:0.906]
Epoch [51/120    avg_loss:0.208, val_acc:0.939]
Epoch [52/120    avg_loss:0.232, val_acc:0.943]
Epoch [53/120    avg_loss:0.153, val_acc:0.963]
Epoch [54/120    avg_loss:0.154, val_acc:0.938]
Epoch [55/120    avg_loss:0.140, val_acc:0.953]
Epoch [56/120    avg_loss:0.116, val_acc:0.967]
Epoch [57/120    avg_loss:0.130, val_acc:0.941]
Epoch [58/120    avg_loss:0.138, val_acc:0.969]
Epoch [59/120    avg_loss:0.087, val_acc:0.971]
Epoch [60/120    avg_loss:0.101, val_acc:0.959]
Epoch [61/120    avg_loss:0.109, val_acc:0.965]
Epoch [62/120    avg_loss:0.106, val_acc:0.957]
Epoch [63/120    avg_loss:0.089, val_acc:0.957]
Epoch [64/120    avg_loss:0.170, val_acc:0.936]
Epoch [65/120    avg_loss:0.231, val_acc:0.939]
Epoch [66/120    avg_loss:0.124, val_acc:0.957]
Epoch [67/120    avg_loss:0.113, val_acc:0.961]
Epoch [68/120    avg_loss:0.086, val_acc:0.965]
Epoch [69/120    avg_loss:0.175, val_acc:0.963]
Epoch [70/120    avg_loss:0.195, val_acc:0.949]
Epoch [71/120    avg_loss:0.125, val_acc:0.965]
Epoch [72/120    avg_loss:0.114, val_acc:0.961]
Epoch [73/120    avg_loss:0.073, val_acc:0.963]
Epoch [74/120    avg_loss:0.066, val_acc:0.969]
Epoch [75/120    avg_loss:0.052, val_acc:0.977]
Epoch [76/120    avg_loss:0.055, val_acc:0.977]
Epoch [77/120    avg_loss:0.052, val_acc:0.977]
Epoch [78/120    avg_loss:0.048, val_acc:0.979]
Epoch [79/120    avg_loss:0.048, val_acc:0.980]
Epoch [80/120    avg_loss:0.065, val_acc:0.980]
Epoch [81/120    avg_loss:0.056, val_acc:0.979]
Epoch [82/120    avg_loss:0.032, val_acc:0.980]
Epoch [83/120    avg_loss:0.043, val_acc:0.979]
Epoch [84/120    avg_loss:0.040, val_acc:0.979]
Epoch [85/120    avg_loss:0.046, val_acc:0.979]
Epoch [86/120    avg_loss:0.051, val_acc:0.979]
Epoch [87/120    avg_loss:0.036, val_acc:0.980]
Epoch [88/120    avg_loss:0.049, val_acc:0.980]
Epoch [89/120    avg_loss:0.038, val_acc:0.980]
Epoch [90/120    avg_loss:0.033, val_acc:0.980]
Epoch [91/120    avg_loss:0.037, val_acc:0.980]
Epoch [92/120    avg_loss:0.057, val_acc:0.980]
Epoch [93/120    avg_loss:0.037, val_acc:0.980]
Epoch [94/120    avg_loss:0.045, val_acc:0.980]
Epoch [95/120    avg_loss:0.049, val_acc:0.982]
Epoch [96/120    avg_loss:0.047, val_acc:0.980]
Epoch [97/120    avg_loss:0.044, val_acc:0.979]
Epoch [98/120    avg_loss:0.031, val_acc:0.980]
Epoch [99/120    avg_loss:0.031, val_acc:0.980]
Epoch [100/120    avg_loss:0.035, val_acc:0.980]
Epoch [101/120    avg_loss:0.040, val_acc:0.980]
Epoch [102/120    avg_loss:0.045, val_acc:0.980]
Epoch [103/120    avg_loss:0.039, val_acc:0.982]
Epoch [104/120    avg_loss:0.035, val_acc:0.980]
Epoch [105/120    avg_loss:0.046, val_acc:0.980]
Epoch [106/120    avg_loss:0.050, val_acc:0.977]
Epoch [107/120    avg_loss:0.051, val_acc:0.980]
Epoch [108/120    avg_loss:0.027, val_acc:0.982]
Epoch [109/120    avg_loss:0.035, val_acc:0.980]
Epoch [110/120    avg_loss:0.039, val_acc:0.979]
Epoch [111/120    avg_loss:0.037, val_acc:0.979]
Epoch [112/120    avg_loss:0.040, val_acc:0.979]
Epoch [113/120    avg_loss:0.034, val_acc:0.980]
Epoch [114/120    avg_loss:0.039, val_acc:0.979]
Epoch [115/120    avg_loss:0.032, val_acc:0.980]
Epoch [116/120    avg_loss:0.031, val_acc:0.980]
Epoch [117/120    avg_loss:0.027, val_acc:0.980]
Epoch [118/120    avg_loss:0.034, val_acc:0.980]
Epoch [119/120    avg_loss:0.025, val_acc:0.980]
Epoch [120/120    avg_loss:0.026, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   5   0   0   0   0   3   0   0   0   0]
 [  0   0   0   1 208  18   0   0   0   0   0   0   0   0]
 [  0   0   0   1   6 138   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0   0 384   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.95522388059702

F1 scores:
[       nan 0.99927061 0.96902655 0.97797357 0.93273543 0.91694352
 0.99756691 0.94382022 0.99481865 0.99680511 1.         1.
 1.         1.        ]

Kappa:
0.9883678047398778
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9780ad7748>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.218, val_acc:0.600]
Epoch [2/120    avg_loss:1.607, val_acc:0.686]
Epoch [3/120    avg_loss:1.217, val_acc:0.727]
Epoch [4/120    avg_loss:0.955, val_acc:0.738]
Epoch [5/120    avg_loss:0.833, val_acc:0.828]
Epoch [6/120    avg_loss:0.768, val_acc:0.846]
Epoch [7/120    avg_loss:0.633, val_acc:0.830]
Epoch [8/120    avg_loss:0.683, val_acc:0.854]
Epoch [9/120    avg_loss:0.619, val_acc:0.877]
Epoch [10/120    avg_loss:0.610, val_acc:0.881]
Epoch [11/120    avg_loss:0.539, val_acc:0.750]
Epoch [12/120    avg_loss:0.535, val_acc:0.902]
Epoch [13/120    avg_loss:0.519, val_acc:0.887]
Epoch [14/120    avg_loss:0.494, val_acc:0.902]
Epoch [15/120    avg_loss:0.488, val_acc:0.883]
Epoch [16/120    avg_loss:0.470, val_acc:0.885]
Epoch [17/120    avg_loss:0.409, val_acc:0.891]
Epoch [18/120    avg_loss:0.373, val_acc:0.896]
Epoch [19/120    avg_loss:0.328, val_acc:0.896]
Epoch [20/120    avg_loss:0.433, val_acc:0.896]
Epoch [21/120    avg_loss:0.410, val_acc:0.881]
Epoch [22/120    avg_loss:0.329, val_acc:0.891]
Epoch [23/120    avg_loss:0.316, val_acc:0.902]
Epoch [24/120    avg_loss:0.391, val_acc:0.873]
Epoch [25/120    avg_loss:0.375, val_acc:0.902]
Epoch [26/120    avg_loss:0.329, val_acc:0.943]
Epoch [27/120    avg_loss:0.288, val_acc:0.922]
Epoch [28/120    avg_loss:0.253, val_acc:0.918]
Epoch [29/120    avg_loss:0.302, val_acc:0.916]
Epoch [30/120    avg_loss:0.286, val_acc:0.910]
Epoch [31/120    avg_loss:0.268, val_acc:0.918]
Epoch [32/120    avg_loss:0.240, val_acc:0.938]
Epoch [33/120    avg_loss:0.184, val_acc:0.939]
Epoch [34/120    avg_loss:0.229, val_acc:0.938]
Epoch [35/120    avg_loss:0.235, val_acc:0.947]
Epoch [36/120    avg_loss:0.212, val_acc:0.938]
Epoch [37/120    avg_loss:0.210, val_acc:0.951]
Epoch [38/120    avg_loss:0.178, val_acc:0.955]
Epoch [39/120    avg_loss:0.130, val_acc:0.963]
Epoch [40/120    avg_loss:0.178, val_acc:0.936]
Epoch [41/120    avg_loss:0.171, val_acc:0.916]
Epoch [42/120    avg_loss:0.224, val_acc:0.947]
Epoch [43/120    avg_loss:0.206, val_acc:0.955]
Epoch [44/120    avg_loss:0.167, val_acc:0.961]
Epoch [45/120    avg_loss:0.209, val_acc:0.934]
Epoch [46/120    avg_loss:0.203, val_acc:0.963]
Epoch [47/120    avg_loss:0.178, val_acc:0.943]
Epoch [48/120    avg_loss:0.219, val_acc:0.965]
Epoch [49/120    avg_loss:0.213, val_acc:0.922]
Epoch [50/120    avg_loss:0.216, val_acc:0.936]
Epoch [51/120    avg_loss:0.176, val_acc:0.959]
Epoch [52/120    avg_loss:0.137, val_acc:0.967]
Epoch [53/120    avg_loss:0.105, val_acc:0.951]
Epoch [54/120    avg_loss:0.144, val_acc:0.951]
Epoch [55/120    avg_loss:0.111, val_acc:0.971]
Epoch [56/120    avg_loss:0.108, val_acc:0.959]
Epoch [57/120    avg_loss:0.100, val_acc:0.973]
Epoch [58/120    avg_loss:0.092, val_acc:0.979]
Epoch [59/120    avg_loss:0.083, val_acc:0.977]
Epoch [60/120    avg_loss:0.091, val_acc:0.971]
Epoch [61/120    avg_loss:0.086, val_acc:0.951]
Epoch [62/120    avg_loss:0.106, val_acc:0.967]
Epoch [63/120    avg_loss:0.157, val_acc:0.963]
Epoch [64/120    avg_loss:0.124, val_acc:0.973]
Epoch [65/120    avg_loss:0.097, val_acc:0.975]
Epoch [66/120    avg_loss:0.075, val_acc:0.965]
Epoch [67/120    avg_loss:0.067, val_acc:0.971]
Epoch [68/120    avg_loss:0.058, val_acc:0.975]
Epoch [69/120    avg_loss:0.050, val_acc:0.963]
Epoch [70/120    avg_loss:0.075, val_acc:0.955]
Epoch [71/120    avg_loss:0.068, val_acc:0.963]
Epoch [72/120    avg_loss:0.088, val_acc:0.977]
Epoch [73/120    avg_loss:0.041, val_acc:0.980]
Epoch [74/120    avg_loss:0.038, val_acc:0.979]
Epoch [75/120    avg_loss:0.043, val_acc:0.979]
Epoch [76/120    avg_loss:0.043, val_acc:0.979]
Epoch [77/120    avg_loss:0.038, val_acc:0.980]
Epoch [78/120    avg_loss:0.042, val_acc:0.980]
Epoch [79/120    avg_loss:0.033, val_acc:0.980]
Epoch [80/120    avg_loss:0.038, val_acc:0.982]
Epoch [81/120    avg_loss:0.037, val_acc:0.982]
Epoch [82/120    avg_loss:0.053, val_acc:0.980]
Epoch [83/120    avg_loss:0.036, val_acc:0.982]
Epoch [84/120    avg_loss:0.041, val_acc:0.982]
Epoch [85/120    avg_loss:0.033, val_acc:0.984]
Epoch [86/120    avg_loss:0.036, val_acc:0.982]
Epoch [87/120    avg_loss:0.032, val_acc:0.982]
Epoch [88/120    avg_loss:0.032, val_acc:0.982]
Epoch [89/120    avg_loss:0.023, val_acc:0.982]
Epoch [90/120    avg_loss:0.031, val_acc:0.982]
Epoch [91/120    avg_loss:0.035, val_acc:0.982]
Epoch [92/120    avg_loss:0.034, val_acc:0.980]
Epoch [93/120    avg_loss:0.036, val_acc:0.984]
Epoch [94/120    avg_loss:0.032, val_acc:0.984]
Epoch [95/120    avg_loss:0.023, val_acc:0.980]
Epoch [96/120    avg_loss:0.036, val_acc:0.982]
Epoch [97/120    avg_loss:0.027, val_acc:0.982]
Epoch [98/120    avg_loss:0.036, val_acc:0.984]
Epoch [99/120    avg_loss:0.029, val_acc:0.982]
Epoch [100/120    avg_loss:0.036, val_acc:0.980]
Epoch [101/120    avg_loss:0.030, val_acc:0.984]
Epoch [102/120    avg_loss:0.030, val_acc:0.984]
Epoch [103/120    avg_loss:0.029, val_acc:0.984]
Epoch [104/120    avg_loss:0.026, val_acc:0.982]
Epoch [105/120    avg_loss:0.023, val_acc:0.982]
Epoch [106/120    avg_loss:0.032, val_acc:0.984]
Epoch [107/120    avg_loss:0.027, val_acc:0.984]
Epoch [108/120    avg_loss:0.023, val_acc:0.982]
Epoch [109/120    avg_loss:0.030, val_acc:0.982]
Epoch [110/120    avg_loss:0.026, val_acc:0.982]
Epoch [111/120    avg_loss:0.026, val_acc:0.982]
Epoch [112/120    avg_loss:0.034, val_acc:0.980]
Epoch [113/120    avg_loss:0.032, val_acc:0.982]
Epoch [114/120    avg_loss:0.026, val_acc:0.982]
Epoch [115/120    avg_loss:0.025, val_acc:0.980]
Epoch [116/120    avg_loss:0.026, val_acc:0.980]
Epoch [117/120    avg_loss:0.025, val_acc:0.982]
Epoch [118/120    avg_loss:0.020, val_acc:0.980]
Epoch [119/120    avg_loss:0.030, val_acc:0.986]
Epoch [120/120    avg_loss:0.024, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 207   0   0   0   0  12   0   0   0   0   0   0]
 [  0   0   0 218   8   0   0   0   4   0   0   0   0   0]
 [  0   0   0   2 215  10   0   0   0   0   0   0   0   0]
 [  0   0   0   4  16 125   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  11   0   0   0   0  83   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   4   0   0   0   0   0   0   0   0   0 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.46481876332622

F1 scores:
[       nan 0.99926954 0.93877551 0.96035242 0.92274678 0.89285714
 0.99757869 0.87830688 0.99487179 1.         1.         1.
 0.99556541 1.        ]

Kappa:
0.9829086165717583
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3b2b19e7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.289, val_acc:0.541]
Epoch [2/120    avg_loss:1.660, val_acc:0.627]
Epoch [3/120    avg_loss:1.280, val_acc:0.688]
Epoch [4/120    avg_loss:0.994, val_acc:0.811]
Epoch [5/120    avg_loss:0.825, val_acc:0.854]
Epoch [6/120    avg_loss:0.727, val_acc:0.807]
Epoch [7/120    avg_loss:0.664, val_acc:0.889]
Epoch [8/120    avg_loss:0.595, val_acc:0.875]
Epoch [9/120    avg_loss:0.592, val_acc:0.863]
Epoch [10/120    avg_loss:0.604, val_acc:0.906]
Epoch [11/120    avg_loss:0.516, val_acc:0.887]
Epoch [12/120    avg_loss:0.482, val_acc:0.871]
Epoch [13/120    avg_loss:0.409, val_acc:0.912]
Epoch [14/120    avg_loss:0.521, val_acc:0.908]
Epoch [15/120    avg_loss:0.459, val_acc:0.898]
Epoch [16/120    avg_loss:0.428, val_acc:0.869]
Epoch [17/120    avg_loss:0.368, val_acc:0.898]
Epoch [18/120    avg_loss:0.372, val_acc:0.895]
Epoch [19/120    avg_loss:0.347, val_acc:0.900]
Epoch [20/120    avg_loss:0.386, val_acc:0.902]
Epoch [21/120    avg_loss:0.339, val_acc:0.885]
Epoch [22/120    avg_loss:0.406, val_acc:0.912]
Epoch [23/120    avg_loss:0.298, val_acc:0.914]
Epoch [24/120    avg_loss:0.292, val_acc:0.920]
Epoch [25/120    avg_loss:0.304, val_acc:0.916]
Epoch [26/120    avg_loss:0.306, val_acc:0.924]
Epoch [27/120    avg_loss:0.263, val_acc:0.930]
Epoch [28/120    avg_loss:0.218, val_acc:0.930]
Epoch [29/120    avg_loss:0.245, val_acc:0.945]
Epoch [30/120    avg_loss:0.245, val_acc:0.932]
Epoch [31/120    avg_loss:0.269, val_acc:0.945]
Epoch [32/120    avg_loss:0.188, val_acc:0.912]
Epoch [33/120    avg_loss:0.232, val_acc:0.949]
Epoch [34/120    avg_loss:0.192, val_acc:0.939]
Epoch [35/120    avg_loss:0.171, val_acc:0.941]
Epoch [36/120    avg_loss:0.157, val_acc:0.951]
Epoch [37/120    avg_loss:0.241, val_acc:0.922]
Epoch [38/120    avg_loss:0.234, val_acc:0.943]
Epoch [39/120    avg_loss:0.185, val_acc:0.932]
Epoch [40/120    avg_loss:0.239, val_acc:0.951]
Epoch [41/120    avg_loss:0.161, val_acc:0.943]
Epoch [42/120    avg_loss:0.158, val_acc:0.949]
Epoch [43/120    avg_loss:0.171, val_acc:0.971]
Epoch [44/120    avg_loss:0.127, val_acc:0.943]
Epoch [45/120    avg_loss:0.224, val_acc:0.943]
Epoch [46/120    avg_loss:0.192, val_acc:0.936]
Epoch [47/120    avg_loss:0.126, val_acc:0.951]
Epoch [48/120    avg_loss:0.146, val_acc:0.930]
Epoch [49/120    avg_loss:0.150, val_acc:0.951]
Epoch [50/120    avg_loss:0.172, val_acc:0.953]
Epoch [51/120    avg_loss:0.125, val_acc:0.957]
Epoch [52/120    avg_loss:0.146, val_acc:0.955]
Epoch [53/120    avg_loss:0.102, val_acc:0.969]
Epoch [54/120    avg_loss:0.122, val_acc:0.961]
Epoch [55/120    avg_loss:0.107, val_acc:0.969]
Epoch [56/120    avg_loss:0.114, val_acc:0.959]
Epoch [57/120    avg_loss:0.059, val_acc:0.973]
Epoch [58/120    avg_loss:0.062, val_acc:0.979]
Epoch [59/120    avg_loss:0.054, val_acc:0.980]
Epoch [60/120    avg_loss:0.062, val_acc:0.982]
Epoch [61/120    avg_loss:0.052, val_acc:0.980]
Epoch [62/120    avg_loss:0.076, val_acc:0.977]
Epoch [63/120    avg_loss:0.060, val_acc:0.982]
Epoch [64/120    avg_loss:0.055, val_acc:0.984]
Epoch [65/120    avg_loss:0.053, val_acc:0.982]
Epoch [66/120    avg_loss:0.052, val_acc:0.982]
Epoch [67/120    avg_loss:0.058, val_acc:0.980]
Epoch [68/120    avg_loss:0.051, val_acc:0.979]
Epoch [69/120    avg_loss:0.043, val_acc:0.980]
Epoch [70/120    avg_loss:0.045, val_acc:0.982]
Epoch [71/120    avg_loss:0.050, val_acc:0.982]
Epoch [72/120    avg_loss:0.063, val_acc:0.980]
Epoch [73/120    avg_loss:0.059, val_acc:0.977]
Epoch [74/120    avg_loss:0.049, val_acc:0.982]
Epoch [75/120    avg_loss:0.045, val_acc:0.982]
Epoch [76/120    avg_loss:0.035, val_acc:0.980]
Epoch [77/120    avg_loss:0.040, val_acc:0.982]
Epoch [78/120    avg_loss:0.045, val_acc:0.982]
Epoch [79/120    avg_loss:0.052, val_acc:0.982]
Epoch [80/120    avg_loss:0.045, val_acc:0.982]
Epoch [81/120    avg_loss:0.045, val_acc:0.982]
Epoch [82/120    avg_loss:0.037, val_acc:0.982]
Epoch [83/120    avg_loss:0.056, val_acc:0.982]
Epoch [84/120    avg_loss:0.042, val_acc:0.982]
Epoch [85/120    avg_loss:0.046, val_acc:0.982]
Epoch [86/120    avg_loss:0.045, val_acc:0.982]
Epoch [87/120    avg_loss:0.041, val_acc:0.982]
Epoch [88/120    avg_loss:0.054, val_acc:0.982]
Epoch [89/120    avg_loss:0.047, val_acc:0.982]
Epoch [90/120    avg_loss:0.053, val_acc:0.982]
Epoch [91/120    avg_loss:0.052, val_acc:0.982]
Epoch [92/120    avg_loss:0.053, val_acc:0.982]
Epoch [93/120    avg_loss:0.046, val_acc:0.982]
Epoch [94/120    avg_loss:0.041, val_acc:0.982]
Epoch [95/120    avg_loss:0.047, val_acc:0.982]
Epoch [96/120    avg_loss:0.046, val_acc:0.982]
Epoch [97/120    avg_loss:0.039, val_acc:0.982]
Epoch [98/120    avg_loss:0.039, val_acc:0.982]
Epoch [99/120    avg_loss:0.046, val_acc:0.982]
Epoch [100/120    avg_loss:0.049, val_acc:0.982]
Epoch [101/120    avg_loss:0.054, val_acc:0.982]
Epoch [102/120    avg_loss:0.045, val_acc:0.982]
Epoch [103/120    avg_loss:0.040, val_acc:0.982]
Epoch [104/120    avg_loss:0.040, val_acc:0.982]
Epoch [105/120    avg_loss:0.057, val_acc:0.982]
Epoch [106/120    avg_loss:0.057, val_acc:0.982]
Epoch [107/120    avg_loss:0.044, val_acc:0.982]
Epoch [108/120    avg_loss:0.040, val_acc:0.982]
Epoch [109/120    avg_loss:0.048, val_acc:0.982]
Epoch [110/120    avg_loss:0.044, val_acc:0.982]
Epoch [111/120    avg_loss:0.045, val_acc:0.982]
Epoch [112/120    avg_loss:0.049, val_acc:0.982]
Epoch [113/120    avg_loss:0.040, val_acc:0.982]
Epoch [114/120    avg_loss:0.037, val_acc:0.982]
Epoch [115/120    avg_loss:0.038, val_acc:0.982]
Epoch [116/120    avg_loss:0.040, val_acc:0.982]
Epoch [117/120    avg_loss:0.039, val_acc:0.982]
Epoch [118/120    avg_loss:0.037, val_acc:0.982]
Epoch [119/120    avg_loss:0.038, val_acc:0.982]
Epoch [120/120    avg_loss:0.061, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 211   0   0   0   0   8   0   0   0   0   0   0]
 [  0   0   0 223   4   0   0   0   2   1   0   0   0   0]
 [  0   0   0   2 209  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  23 122   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  15   0   0   0   0  79   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.44349680170575

F1 scores:
[       nan 1.         0.94831461 0.98021978 0.90280778 0.86219081
 1.         0.87292818 0.99742931 0.99893276 1.         0.99734043
 0.99779736 1.        ]

Kappa:
0.9826692687314216
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4d6956d7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.227, val_acc:0.525]
Epoch [2/120    avg_loss:1.676, val_acc:0.693]
Epoch [3/120    avg_loss:1.258, val_acc:0.678]
Epoch [4/120    avg_loss:1.035, val_acc:0.758]
Epoch [5/120    avg_loss:0.881, val_acc:0.793]
Epoch [6/120    avg_loss:0.794, val_acc:0.766]
Epoch [7/120    avg_loss:0.776, val_acc:0.855]
Epoch [8/120    avg_loss:0.618, val_acc:0.871]
Epoch [9/120    avg_loss:0.574, val_acc:0.830]
Epoch [10/120    avg_loss:0.533, val_acc:0.838]
Epoch [11/120    avg_loss:0.606, val_acc:0.871]
Epoch [12/120    avg_loss:0.489, val_acc:0.879]
Epoch [13/120    avg_loss:0.447, val_acc:0.891]
Epoch [14/120    avg_loss:0.468, val_acc:0.828]
Epoch [15/120    avg_loss:0.453, val_acc:0.867]
Epoch [16/120    avg_loss:0.393, val_acc:0.873]
Epoch [17/120    avg_loss:0.388, val_acc:0.879]
Epoch [18/120    avg_loss:0.459, val_acc:0.832]
Epoch [19/120    avg_loss:0.412, val_acc:0.908]
Epoch [20/120    avg_loss:0.352, val_acc:0.914]
Epoch [21/120    avg_loss:0.293, val_acc:0.949]
Epoch [22/120    avg_loss:0.308, val_acc:0.932]
Epoch [23/120    avg_loss:0.288, val_acc:0.910]
Epoch [24/120    avg_loss:0.259, val_acc:0.891]
Epoch [25/120    avg_loss:0.358, val_acc:0.906]
Epoch [26/120    avg_loss:0.328, val_acc:0.924]
Epoch [27/120    avg_loss:0.425, val_acc:0.922]
Epoch [28/120    avg_loss:0.322, val_acc:0.908]
Epoch [29/120    avg_loss:0.227, val_acc:0.947]
Epoch [30/120    avg_loss:0.257, val_acc:0.941]
Epoch [31/120    avg_loss:0.286, val_acc:0.898]
Epoch [32/120    avg_loss:0.278, val_acc:0.939]
Epoch [33/120    avg_loss:0.304, val_acc:0.912]
Epoch [34/120    avg_loss:0.235, val_acc:0.955]
Epoch [35/120    avg_loss:0.253, val_acc:0.928]
Epoch [36/120    avg_loss:0.210, val_acc:0.928]
Epoch [37/120    avg_loss:0.240, val_acc:0.953]
Epoch [38/120    avg_loss:0.156, val_acc:0.969]
Epoch [39/120    avg_loss:0.167, val_acc:0.949]
Epoch [40/120    avg_loss:0.170, val_acc:0.957]
Epoch [41/120    avg_loss:0.168, val_acc:0.959]
Epoch [42/120    avg_loss:0.206, val_acc:0.953]
Epoch [43/120    avg_loss:0.175, val_acc:0.947]
Epoch [44/120    avg_loss:0.158, val_acc:0.953]
Epoch [45/120    avg_loss:0.177, val_acc:0.969]
Epoch [46/120    avg_loss:0.155, val_acc:0.955]
Epoch [47/120    avg_loss:0.144, val_acc:0.955]
Epoch [48/120    avg_loss:0.150, val_acc:0.961]
Epoch [49/120    avg_loss:0.118, val_acc:0.961]
Epoch [50/120    avg_loss:0.176, val_acc:0.943]
Epoch [51/120    avg_loss:0.133, val_acc:0.959]
Epoch [52/120    avg_loss:0.108, val_acc:0.951]
Epoch [53/120    avg_loss:0.121, val_acc:0.959]
Epoch [54/120    avg_loss:0.154, val_acc:0.967]
Epoch [55/120    avg_loss:0.170, val_acc:0.971]
Epoch [56/120    avg_loss:0.096, val_acc:0.961]
Epoch [57/120    avg_loss:0.155, val_acc:0.947]
Epoch [58/120    avg_loss:0.166, val_acc:0.947]
Epoch [59/120    avg_loss:0.115, val_acc:0.949]
Epoch [60/120    avg_loss:0.116, val_acc:0.965]
Epoch [61/120    avg_loss:0.121, val_acc:0.967]
Epoch [62/120    avg_loss:0.149, val_acc:0.963]
Epoch [63/120    avg_loss:0.147, val_acc:0.967]
Epoch [64/120    avg_loss:0.106, val_acc:0.959]
Epoch [65/120    avg_loss:0.095, val_acc:0.963]
Epoch [66/120    avg_loss:0.094, val_acc:0.951]
Epoch [67/120    avg_loss:0.086, val_acc:0.951]
Epoch [68/120    avg_loss:0.151, val_acc:0.959]
Epoch [69/120    avg_loss:0.080, val_acc:0.961]
Epoch [70/120    avg_loss:0.053, val_acc:0.961]
Epoch [71/120    avg_loss:0.061, val_acc:0.967]
Epoch [72/120    avg_loss:0.068, val_acc:0.967]
Epoch [73/120    avg_loss:0.065, val_acc:0.967]
Epoch [74/120    avg_loss:0.064, val_acc:0.965]
Epoch [75/120    avg_loss:0.063, val_acc:0.969]
Epoch [76/120    avg_loss:0.049, val_acc:0.971]
Epoch [77/120    avg_loss:0.049, val_acc:0.971]
Epoch [78/120    avg_loss:0.053, val_acc:0.971]
Epoch [79/120    avg_loss:0.049, val_acc:0.971]
Epoch [80/120    avg_loss:0.055, val_acc:0.971]
Epoch [81/120    avg_loss:0.042, val_acc:0.969]
Epoch [82/120    avg_loss:0.042, val_acc:0.971]
Epoch [83/120    avg_loss:0.053, val_acc:0.971]
Epoch [84/120    avg_loss:0.034, val_acc:0.973]
Epoch [85/120    avg_loss:0.046, val_acc:0.975]
Epoch [86/120    avg_loss:0.034, val_acc:0.973]
Epoch [87/120    avg_loss:0.044, val_acc:0.971]
Epoch [88/120    avg_loss:0.037, val_acc:0.973]
Epoch [89/120    avg_loss:0.054, val_acc:0.975]
Epoch [90/120    avg_loss:0.043, val_acc:0.973]
Epoch [91/120    avg_loss:0.041, val_acc:0.973]
Epoch [92/120    avg_loss:0.038, val_acc:0.969]
Epoch [93/120    avg_loss:0.043, val_acc:0.973]
Epoch [94/120    avg_loss:0.039, val_acc:0.971]
Epoch [95/120    avg_loss:0.049, val_acc:0.975]
Epoch [96/120    avg_loss:0.060, val_acc:0.971]
Epoch [97/120    avg_loss:0.051, val_acc:0.975]
Epoch [98/120    avg_loss:0.030, val_acc:0.977]
Epoch [99/120    avg_loss:0.043, val_acc:0.975]
Epoch [100/120    avg_loss:0.047, val_acc:0.975]
Epoch [101/120    avg_loss:0.028, val_acc:0.969]
Epoch [102/120    avg_loss:0.031, val_acc:0.971]
Epoch [103/120    avg_loss:0.057, val_acc:0.969]
Epoch [104/120    avg_loss:0.045, val_acc:0.973]
Epoch [105/120    avg_loss:0.034, val_acc:0.973]
Epoch [106/120    avg_loss:0.037, val_acc:0.975]
Epoch [107/120    avg_loss:0.037, val_acc:0.973]
Epoch [108/120    avg_loss:0.056, val_acc:0.973]
Epoch [109/120    avg_loss:0.033, val_acc:0.973]
Epoch [110/120    avg_loss:0.034, val_acc:0.975]
Epoch [111/120    avg_loss:0.041, val_acc:0.975]
Epoch [112/120    avg_loss:0.025, val_acc:0.975]
Epoch [113/120    avg_loss:0.033, val_acc:0.975]
Epoch [114/120    avg_loss:0.032, val_acc:0.975]
Epoch [115/120    avg_loss:0.037, val_acc:0.975]
Epoch [116/120    avg_loss:0.032, val_acc:0.975]
Epoch [117/120    avg_loss:0.034, val_acc:0.975]
Epoch [118/120    avg_loss:0.024, val_acc:0.975]
Epoch [119/120    avg_loss:0.026, val_acc:0.975]
Epoch [120/120    avg_loss:0.035, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   0 219   1   0   0   0   9   1   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  33 112   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   4 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.42217484008529

F1 scores:
[       nan 1.         0.96818182 0.97550111 0.90794979 0.83895131
 1.         0.92473118 0.98853503 0.99893276 1.         0.99206349
 0.99336283 1.        ]

Kappa:
0.9824305345221063
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbee55a6470>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.276, val_acc:0.578]
Epoch [2/120    avg_loss:1.606, val_acc:0.645]
Epoch [3/120    avg_loss:1.185, val_acc:0.750]
Epoch [4/120    avg_loss:1.000, val_acc:0.713]
Epoch [5/120    avg_loss:0.837, val_acc:0.834]
Epoch [6/120    avg_loss:0.708, val_acc:0.814]
Epoch [7/120    avg_loss:0.744, val_acc:0.789]
Epoch [8/120    avg_loss:0.663, val_acc:0.814]
Epoch [9/120    avg_loss:0.635, val_acc:0.822]
Epoch [10/120    avg_loss:0.607, val_acc:0.867]
Epoch [11/120    avg_loss:0.606, val_acc:0.879]
Epoch [12/120    avg_loss:0.489, val_acc:0.902]
Epoch [13/120    avg_loss:0.500, val_acc:0.898]
Epoch [14/120    avg_loss:0.433, val_acc:0.898]
Epoch [15/120    avg_loss:0.506, val_acc:0.873]
Epoch [16/120    avg_loss:0.462, val_acc:0.879]
Epoch [17/120    avg_loss:0.404, val_acc:0.902]
Epoch [18/120    avg_loss:0.365, val_acc:0.930]
Epoch [19/120    avg_loss:0.326, val_acc:0.918]
Epoch [20/120    avg_loss:0.376, val_acc:0.920]
Epoch [21/120    avg_loss:0.403, val_acc:0.928]
Epoch [22/120    avg_loss:0.284, val_acc:0.930]
Epoch [23/120    avg_loss:0.281, val_acc:0.930]
Epoch [24/120    avg_loss:0.324, val_acc:0.912]
Epoch [25/120    avg_loss:0.254, val_acc:0.889]
Epoch [26/120    avg_loss:0.283, val_acc:0.934]
Epoch [27/120    avg_loss:0.335, val_acc:0.928]
Epoch [28/120    avg_loss:0.233, val_acc:0.885]
Epoch [29/120    avg_loss:0.216, val_acc:0.924]
Epoch [30/120    avg_loss:0.171, val_acc:0.971]
Epoch [31/120    avg_loss:0.217, val_acc:0.932]
Epoch [32/120    avg_loss:0.194, val_acc:0.934]
Epoch [33/120    avg_loss:0.222, val_acc:0.947]
Epoch [34/120    avg_loss:0.178, val_acc:0.957]
Epoch [35/120    avg_loss:0.195, val_acc:0.955]
Epoch [36/120    avg_loss:0.214, val_acc:0.922]
Epoch [37/120    avg_loss:0.243, val_acc:0.918]
Epoch [38/120    avg_loss:0.224, val_acc:0.898]
Epoch [39/120    avg_loss:0.177, val_acc:0.922]
Epoch [40/120    avg_loss:0.253, val_acc:0.955]
Epoch [41/120    avg_loss:0.133, val_acc:0.955]
Epoch [42/120    avg_loss:0.163, val_acc:0.947]
Epoch [43/120    avg_loss:0.213, val_acc:0.951]
Epoch [44/120    avg_loss:0.121, val_acc:0.971]
Epoch [45/120    avg_loss:0.105, val_acc:0.973]
Epoch [46/120    avg_loss:0.086, val_acc:0.977]
Epoch [47/120    avg_loss:0.087, val_acc:0.984]
Epoch [48/120    avg_loss:0.086, val_acc:0.979]
Epoch [49/120    avg_loss:0.077, val_acc:0.977]
Epoch [50/120    avg_loss:0.072, val_acc:0.977]
Epoch [51/120    avg_loss:0.072, val_acc:0.984]
Epoch [52/120    avg_loss:0.076, val_acc:0.982]
Epoch [53/120    avg_loss:0.062, val_acc:0.980]
Epoch [54/120    avg_loss:0.068, val_acc:0.982]
Epoch [55/120    avg_loss:0.058, val_acc:0.982]
Epoch [56/120    avg_loss:0.072, val_acc:0.984]
Epoch [57/120    avg_loss:0.080, val_acc:0.979]
Epoch [58/120    avg_loss:0.067, val_acc:0.984]
Epoch [59/120    avg_loss:0.067, val_acc:0.982]
Epoch [60/120    avg_loss:0.076, val_acc:0.986]
Epoch [61/120    avg_loss:0.078, val_acc:0.984]
Epoch [62/120    avg_loss:0.068, val_acc:0.984]
Epoch [63/120    avg_loss:0.061, val_acc:0.988]
Epoch [64/120    avg_loss:0.053, val_acc:0.986]
Epoch [65/120    avg_loss:0.052, val_acc:0.986]
Epoch [66/120    avg_loss:0.065, val_acc:0.988]
Epoch [67/120    avg_loss:0.057, val_acc:0.988]
Epoch [68/120    avg_loss:0.065, val_acc:0.986]
Epoch [69/120    avg_loss:0.062, val_acc:0.988]
Epoch [70/120    avg_loss:0.065, val_acc:0.986]
Epoch [71/120    avg_loss:0.060, val_acc:0.988]
Epoch [72/120    avg_loss:0.057, val_acc:0.980]
Epoch [73/120    avg_loss:0.056, val_acc:0.986]
Epoch [74/120    avg_loss:0.065, val_acc:0.990]
Epoch [75/120    avg_loss:0.052, val_acc:0.990]
Epoch [76/120    avg_loss:0.055, val_acc:0.988]
Epoch [77/120    avg_loss:0.054, val_acc:0.988]
Epoch [78/120    avg_loss:0.048, val_acc:0.988]
Epoch [79/120    avg_loss:0.060, val_acc:0.990]
Epoch [80/120    avg_loss:0.047, val_acc:0.988]
Epoch [81/120    avg_loss:0.054, val_acc:0.986]
Epoch [82/120    avg_loss:0.056, val_acc:0.988]
Epoch [83/120    avg_loss:0.047, val_acc:0.988]
Epoch [84/120    avg_loss:0.049, val_acc:0.984]
Epoch [85/120    avg_loss:0.045, val_acc:0.992]
Epoch [86/120    avg_loss:0.038, val_acc:0.990]
Epoch [87/120    avg_loss:0.049, val_acc:0.990]
Epoch [88/120    avg_loss:0.057, val_acc:0.986]
Epoch [89/120    avg_loss:0.042, val_acc:0.992]
Epoch [90/120    avg_loss:0.070, val_acc:0.990]
Epoch [91/120    avg_loss:0.044, val_acc:0.988]
Epoch [92/120    avg_loss:0.047, val_acc:0.988]
Epoch [93/120    avg_loss:0.053, val_acc:0.980]
Epoch [94/120    avg_loss:0.035, val_acc:0.986]
Epoch [95/120    avg_loss:0.048, val_acc:0.990]
Epoch [96/120    avg_loss:0.046, val_acc:0.988]
Epoch [97/120    avg_loss:0.045, val_acc:0.988]
Epoch [98/120    avg_loss:0.044, val_acc:0.988]
Epoch [99/120    avg_loss:0.046, val_acc:0.988]
Epoch [100/120    avg_loss:0.045, val_acc:0.990]
Epoch [101/120    avg_loss:0.044, val_acc:0.988]
Epoch [102/120    avg_loss:0.043, val_acc:0.990]
Epoch [103/120    avg_loss:0.054, val_acc:0.990]
Epoch [104/120    avg_loss:0.045, val_acc:0.990]
Epoch [105/120    avg_loss:0.046, val_acc:0.990]
Epoch [106/120    avg_loss:0.038, val_acc:0.990]
Epoch [107/120    avg_loss:0.047, val_acc:0.990]
Epoch [108/120    avg_loss:0.044, val_acc:0.990]
Epoch [109/120    avg_loss:0.042, val_acc:0.988]
Epoch [110/120    avg_loss:0.048, val_acc:0.988]
Epoch [111/120    avg_loss:0.043, val_acc:0.990]
Epoch [112/120    avg_loss:0.035, val_acc:0.990]
Epoch [113/120    avg_loss:0.040, val_acc:0.988]
Epoch [114/120    avg_loss:0.032, val_acc:0.990]
Epoch [115/120    avg_loss:0.060, val_acc:0.988]
Epoch [116/120    avg_loss:0.041, val_acc:0.988]
Epoch [117/120    avg_loss:0.041, val_acc:0.988]
Epoch [118/120    avg_loss:0.044, val_acc:0.988]
Epoch [119/120    avg_loss:0.031, val_acc:0.988]
Epoch [120/120    avg_loss:0.034, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 228   2   0   0   0   0   0   0   0   0   0]
 [  0   0   0   3 199  25   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  19   0   0   0   0  75   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   1 467   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.80597014925372

F1 scores:
[       nan 1.         0.95842451 0.98915401 0.91916859 0.90322581
 1.         0.88757396 0.998713   0.99893048 1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9867061446523714
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcf46f63828>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.239, val_acc:0.678]
Epoch [2/120    avg_loss:1.653, val_acc:0.605]
Epoch [3/120    avg_loss:1.223, val_acc:0.762]
Epoch [4/120    avg_loss:1.061, val_acc:0.781]
Epoch [5/120    avg_loss:0.856, val_acc:0.820]
Epoch [6/120    avg_loss:0.747, val_acc:0.785]
Epoch [7/120    avg_loss:0.748, val_acc:0.848]
Epoch [8/120    avg_loss:0.609, val_acc:0.857]
Epoch [9/120    avg_loss:0.673, val_acc:0.861]
Epoch [10/120    avg_loss:0.559, val_acc:0.848]
Epoch [11/120    avg_loss:0.576, val_acc:0.869]
Epoch [12/120    avg_loss:0.498, val_acc:0.850]
Epoch [13/120    avg_loss:0.507, val_acc:0.889]
Epoch [14/120    avg_loss:0.472, val_acc:0.854]
Epoch [15/120    avg_loss:0.517, val_acc:0.889]
Epoch [16/120    avg_loss:0.419, val_acc:0.900]
Epoch [17/120    avg_loss:0.443, val_acc:0.881]
Epoch [18/120    avg_loss:0.382, val_acc:0.898]
Epoch [19/120    avg_loss:0.422, val_acc:0.924]
Epoch [20/120    avg_loss:0.400, val_acc:0.908]
Epoch [21/120    avg_loss:0.360, val_acc:0.891]
Epoch [22/120    avg_loss:0.425, val_acc:0.898]
Epoch [23/120    avg_loss:0.427, val_acc:0.814]
Epoch [24/120    avg_loss:0.369, val_acc:0.924]
Epoch [25/120    avg_loss:0.323, val_acc:0.900]
Epoch [26/120    avg_loss:0.410, val_acc:0.803]
Epoch [27/120    avg_loss:0.351, val_acc:0.926]
Epoch [28/120    avg_loss:0.264, val_acc:0.928]
Epoch [29/120    avg_loss:0.307, val_acc:0.938]
Epoch [30/120    avg_loss:0.263, val_acc:0.941]
Epoch [31/120    avg_loss:0.261, val_acc:0.932]
Epoch [32/120    avg_loss:0.235, val_acc:0.945]
Epoch [33/120    avg_loss:0.210, val_acc:0.947]
Epoch [34/120    avg_loss:0.262, val_acc:0.938]
Epoch [35/120    avg_loss:0.209, val_acc:0.936]
Epoch [36/120    avg_loss:0.227, val_acc:0.920]
Epoch [37/120    avg_loss:0.248, val_acc:0.949]
Epoch [38/120    avg_loss:0.199, val_acc:0.938]
Epoch [39/120    avg_loss:0.241, val_acc:0.939]
Epoch [40/120    avg_loss:0.239, val_acc:0.934]
Epoch [41/120    avg_loss:0.217, val_acc:0.967]
Epoch [42/120    avg_loss:0.199, val_acc:0.969]
Epoch [43/120    avg_loss:0.183, val_acc:0.928]
Epoch [44/120    avg_loss:0.202, val_acc:0.934]
Epoch [45/120    avg_loss:0.153, val_acc:0.957]
Epoch [46/120    avg_loss:0.138, val_acc:0.953]
Epoch [47/120    avg_loss:0.176, val_acc:0.965]
Epoch [48/120    avg_loss:0.139, val_acc:0.967]
Epoch [49/120    avg_loss:0.099, val_acc:0.969]
Epoch [50/120    avg_loss:0.107, val_acc:0.973]
Epoch [51/120    avg_loss:0.101, val_acc:0.967]
Epoch [52/120    avg_loss:0.128, val_acc:0.951]
Epoch [53/120    avg_loss:0.125, val_acc:0.955]
Epoch [54/120    avg_loss:0.115, val_acc:0.963]
Epoch [55/120    avg_loss:0.147, val_acc:0.953]
Epoch [56/120    avg_loss:0.246, val_acc:0.932]
Epoch [57/120    avg_loss:0.202, val_acc:0.961]
Epoch [58/120    avg_loss:0.116, val_acc:0.955]
Epoch [59/120    avg_loss:0.150, val_acc:0.957]
Epoch [60/120    avg_loss:0.141, val_acc:0.949]
Epoch [61/120    avg_loss:0.135, val_acc:0.967]
Epoch [62/120    avg_loss:0.117, val_acc:0.973]
Epoch [63/120    avg_loss:0.100, val_acc:0.977]
Epoch [64/120    avg_loss:0.069, val_acc:0.979]
Epoch [65/120    avg_loss:0.067, val_acc:0.975]
Epoch [66/120    avg_loss:0.081, val_acc:0.973]
Epoch [67/120    avg_loss:0.125, val_acc:0.977]
Epoch [68/120    avg_loss:0.147, val_acc:0.924]
Epoch [69/120    avg_loss:0.097, val_acc:0.965]
Epoch [70/120    avg_loss:0.091, val_acc:0.971]
Epoch [71/120    avg_loss:0.066, val_acc:0.975]
Epoch [72/120    avg_loss:0.043, val_acc:0.975]
Epoch [73/120    avg_loss:0.069, val_acc:0.977]
Epoch [74/120    avg_loss:0.063, val_acc:0.980]
Epoch [75/120    avg_loss:0.061, val_acc:0.973]
Epoch [76/120    avg_loss:0.060, val_acc:0.982]
Epoch [77/120    avg_loss:0.064, val_acc:0.965]
Epoch [78/120    avg_loss:0.085, val_acc:0.965]
Epoch [79/120    avg_loss:0.072, val_acc:0.977]
Epoch [80/120    avg_loss:0.103, val_acc:0.975]
Epoch [81/120    avg_loss:0.061, val_acc:0.975]
Epoch [82/120    avg_loss:0.063, val_acc:0.979]
Epoch [83/120    avg_loss:0.040, val_acc:0.980]
Epoch [84/120    avg_loss:0.031, val_acc:0.980]
Epoch [85/120    avg_loss:0.037, val_acc:0.986]
Epoch [86/120    avg_loss:0.053, val_acc:0.979]
Epoch [87/120    avg_loss:0.028, val_acc:0.977]
Epoch [88/120    avg_loss:0.088, val_acc:0.973]
Epoch [89/120    avg_loss:0.087, val_acc:0.969]
Epoch [90/120    avg_loss:0.094, val_acc:0.977]
Epoch [91/120    avg_loss:0.071, val_acc:0.973]
Epoch [92/120    avg_loss:0.060, val_acc:0.965]
Epoch [93/120    avg_loss:0.049, val_acc:0.980]
Epoch [94/120    avg_loss:0.035, val_acc:0.980]
Epoch [95/120    avg_loss:0.055, val_acc:0.959]
Epoch [96/120    avg_loss:0.050, val_acc:0.973]
Epoch [97/120    avg_loss:0.038, val_acc:0.971]
Epoch [98/120    avg_loss:0.028, val_acc:0.979]
Epoch [99/120    avg_loss:0.025, val_acc:0.980]
Epoch [100/120    avg_loss:0.015, val_acc:0.982]
Epoch [101/120    avg_loss:0.017, val_acc:0.982]
Epoch [102/120    avg_loss:0.022, val_acc:0.980]
Epoch [103/120    avg_loss:0.015, val_acc:0.980]
Epoch [104/120    avg_loss:0.011, val_acc:0.980]
Epoch [105/120    avg_loss:0.016, val_acc:0.980]
Epoch [106/120    avg_loss:0.020, val_acc:0.982]
Epoch [107/120    avg_loss:0.025, val_acc:0.982]
Epoch [108/120    avg_loss:0.010, val_acc:0.982]
Epoch [109/120    avg_loss:0.020, val_acc:0.984]
Epoch [110/120    avg_loss:0.015, val_acc:0.986]
Epoch [111/120    avg_loss:0.013, val_acc:0.986]
Epoch [112/120    avg_loss:0.013, val_acc:0.986]
Epoch [113/120    avg_loss:0.015, val_acc:0.984]
Epoch [114/120    avg_loss:0.015, val_acc:0.984]
Epoch [115/120    avg_loss:0.010, val_acc:0.982]
Epoch [116/120    avg_loss:0.009, val_acc:0.984]
Epoch [117/120    avg_loss:0.022, val_acc:0.986]
Epoch [118/120    avg_loss:0.016, val_acc:0.986]
Epoch [119/120    avg_loss:0.017, val_acc:0.986]
Epoch [120/120    avg_loss:0.013, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 215   0   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0 210   8   0   0   0   7   5   0   0   0   0]
 [  0   0   0   0 223   1   0   0   0   0   0   0   3   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   3   0   0   0   0 203   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 363   0   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   1   0   0   0   0   0   0   2 450   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.74200426439232

F1 scores:
[       nan 0.997815   0.98173516 0.95454545 0.93110647 0.92250923
 0.99266504 0.95744681 0.99106003 0.9946865  0.99862448 0.9973545
 0.99228225 1.        ]

Kappa:
0.9859904835938644
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2687099828>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.228, val_acc:0.572]
Epoch [2/120    avg_loss:1.602, val_acc:0.613]
Epoch [3/120    avg_loss:1.220, val_acc:0.703]
Epoch [4/120    avg_loss:0.962, val_acc:0.811]
Epoch [5/120    avg_loss:0.848, val_acc:0.828]
Epoch [6/120    avg_loss:0.743, val_acc:0.844]
Epoch [7/120    avg_loss:0.699, val_acc:0.854]
Epoch [8/120    avg_loss:0.717, val_acc:0.799]
Epoch [9/120    avg_loss:0.699, val_acc:0.865]
Epoch [10/120    avg_loss:0.642, val_acc:0.863]
Epoch [11/120    avg_loss:0.584, val_acc:0.887]
Epoch [12/120    avg_loss:0.497, val_acc:0.895]
Epoch [13/120    avg_loss:0.478, val_acc:0.906]
Epoch [14/120    avg_loss:0.407, val_acc:0.896]
Epoch [15/120    avg_loss:0.469, val_acc:0.842]
Epoch [16/120    avg_loss:0.379, val_acc:0.916]
Epoch [17/120    avg_loss:0.424, val_acc:0.912]
Epoch [18/120    avg_loss:0.342, val_acc:0.916]
Epoch [19/120    avg_loss:0.337, val_acc:0.883]
Epoch [20/120    avg_loss:0.382, val_acc:0.896]
Epoch [21/120    avg_loss:0.381, val_acc:0.889]
Epoch [22/120    avg_loss:0.467, val_acc:0.918]
Epoch [23/120    avg_loss:0.295, val_acc:0.910]
Epoch [24/120    avg_loss:0.313, val_acc:0.939]
Epoch [25/120    avg_loss:0.282, val_acc:0.936]
Epoch [26/120    avg_loss:0.246, val_acc:0.934]
Epoch [27/120    avg_loss:0.257, val_acc:0.902]
Epoch [28/120    avg_loss:0.268, val_acc:0.932]
Epoch [29/120    avg_loss:0.255, val_acc:0.930]
Epoch [30/120    avg_loss:0.250, val_acc:0.949]
Epoch [31/120    avg_loss:0.241, val_acc:0.926]
Epoch [32/120    avg_loss:0.217, val_acc:0.963]
Epoch [33/120    avg_loss:0.183, val_acc:0.961]
Epoch [34/120    avg_loss:0.192, val_acc:0.953]
Epoch [35/120    avg_loss:0.233, val_acc:0.953]
Epoch [36/120    avg_loss:0.227, val_acc:0.914]
Epoch [37/120    avg_loss:0.273, val_acc:0.926]
Epoch [38/120    avg_loss:0.240, val_acc:0.949]
Epoch [39/120    avg_loss:0.181, val_acc:0.977]
Epoch [40/120    avg_loss:0.164, val_acc:0.949]
Epoch [41/120    avg_loss:0.221, val_acc:0.916]
Epoch [42/120    avg_loss:0.206, val_acc:0.963]
Epoch [43/120    avg_loss:0.164, val_acc:0.953]
Epoch [44/120    avg_loss:0.116, val_acc:0.965]
Epoch [45/120    avg_loss:0.116, val_acc:0.975]
Epoch [46/120    avg_loss:0.140, val_acc:0.971]
Epoch [47/120    avg_loss:0.142, val_acc:0.955]
Epoch [48/120    avg_loss:0.159, val_acc:0.943]
Epoch [49/120    avg_loss:0.129, val_acc:0.973]
Epoch [50/120    avg_loss:0.089, val_acc:0.973]
Epoch [51/120    avg_loss:0.100, val_acc:0.963]
Epoch [52/120    avg_loss:0.115, val_acc:0.965]
Epoch [53/120    avg_loss:0.098, val_acc:0.980]
Epoch [54/120    avg_loss:0.075, val_acc:0.977]
Epoch [55/120    avg_loss:0.056, val_acc:0.982]
Epoch [56/120    avg_loss:0.060, val_acc:0.982]
Epoch [57/120    avg_loss:0.068, val_acc:0.982]
Epoch [58/120    avg_loss:0.054, val_acc:0.980]
Epoch [59/120    avg_loss:0.056, val_acc:0.980]
Epoch [60/120    avg_loss:0.059, val_acc:0.980]
Epoch [61/120    avg_loss:0.055, val_acc:0.977]
Epoch [62/120    avg_loss:0.058, val_acc:0.980]
Epoch [63/120    avg_loss:0.060, val_acc:0.982]
Epoch [64/120    avg_loss:0.056, val_acc:0.977]
Epoch [65/120    avg_loss:0.046, val_acc:0.980]
Epoch [66/120    avg_loss:0.054, val_acc:0.979]
Epoch [67/120    avg_loss:0.050, val_acc:0.979]
Epoch [68/120    avg_loss:0.051, val_acc:0.977]
Epoch [69/120    avg_loss:0.068, val_acc:0.977]
Epoch [70/120    avg_loss:0.046, val_acc:0.979]
Epoch [71/120    avg_loss:0.044, val_acc:0.980]
Epoch [72/120    avg_loss:0.059, val_acc:0.979]
Epoch [73/120    avg_loss:0.044, val_acc:0.979]
Epoch [74/120    avg_loss:0.048, val_acc:0.979]
Epoch [75/120    avg_loss:0.049, val_acc:0.977]
Epoch [76/120    avg_loss:0.061, val_acc:0.979]
Epoch [77/120    avg_loss:0.050, val_acc:0.979]
Epoch [78/120    avg_loss:0.055, val_acc:0.979]
Epoch [79/120    avg_loss:0.044, val_acc:0.979]
Epoch [80/120    avg_loss:0.055, val_acc:0.979]
Epoch [81/120    avg_loss:0.041, val_acc:0.979]
Epoch [82/120    avg_loss:0.046, val_acc:0.979]
Epoch [83/120    avg_loss:0.046, val_acc:0.979]
Epoch [84/120    avg_loss:0.041, val_acc:0.979]
Epoch [85/120    avg_loss:0.044, val_acc:0.979]
Epoch [86/120    avg_loss:0.049, val_acc:0.979]
Epoch [87/120    avg_loss:0.045, val_acc:0.979]
Epoch [88/120    avg_loss:0.049, val_acc:0.979]
Epoch [89/120    avg_loss:0.045, val_acc:0.979]
Epoch [90/120    avg_loss:0.036, val_acc:0.979]
Epoch [91/120    avg_loss:0.045, val_acc:0.979]
Epoch [92/120    avg_loss:0.045, val_acc:0.979]
Epoch [93/120    avg_loss:0.055, val_acc:0.979]
Epoch [94/120    avg_loss:0.043, val_acc:0.979]
Epoch [95/120    avg_loss:0.050, val_acc:0.979]
Epoch [96/120    avg_loss:0.067, val_acc:0.979]
Epoch [97/120    avg_loss:0.041, val_acc:0.979]
Epoch [98/120    avg_loss:0.059, val_acc:0.979]
Epoch [99/120    avg_loss:0.044, val_acc:0.979]
Epoch [100/120    avg_loss:0.040, val_acc:0.979]
Epoch [101/120    avg_loss:0.046, val_acc:0.979]
Epoch [102/120    avg_loss:0.055, val_acc:0.979]
Epoch [103/120    avg_loss:0.043, val_acc:0.979]
Epoch [104/120    avg_loss:0.044, val_acc:0.979]
Epoch [105/120    avg_loss:0.047, val_acc:0.979]
Epoch [106/120    avg_loss:0.042, val_acc:0.979]
Epoch [107/120    avg_loss:0.051, val_acc:0.979]
Epoch [108/120    avg_loss:0.039, val_acc:0.979]
Epoch [109/120    avg_loss:0.040, val_acc:0.979]
Epoch [110/120    avg_loss:0.047, val_acc:0.979]
Epoch [111/120    avg_loss:0.043, val_acc:0.979]
Epoch [112/120    avg_loss:0.047, val_acc:0.979]
Epoch [113/120    avg_loss:0.049, val_acc:0.979]
Epoch [114/120    avg_loss:0.040, val_acc:0.979]
Epoch [115/120    avg_loss:0.052, val_acc:0.979]
Epoch [116/120    avg_loss:0.041, val_acc:0.979]
Epoch [117/120    avg_loss:0.045, val_acc:0.979]
Epoch [118/120    avg_loss:0.042, val_acc:0.979]
Epoch [119/120    avg_loss:0.043, val_acc:0.979]
Epoch [120/120    avg_loss:0.043, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 220   3   0   0   0   6   1   0   0   0   0]
 [  0   0   0   0 221   6   0   0   0   0   0   0   0   0]
 [  0   0   0   0  22 122   0   0   1   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  17   0   0   0   0  77   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.78464818763327

F1 scores:
[       nan 1.         0.96263736 0.97777778 0.93446089 0.89377289
 1.         0.9005848  0.99106003 0.99893276 1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9864658778672678
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa4cd7567b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.237, val_acc:0.527]
Epoch [2/120    avg_loss:1.643, val_acc:0.635]
Epoch [3/120    avg_loss:1.278, val_acc:0.783]
Epoch [4/120    avg_loss:0.994, val_acc:0.787]
Epoch [5/120    avg_loss:0.852, val_acc:0.857]
Epoch [6/120    avg_loss:0.657, val_acc:0.816]
Epoch [7/120    avg_loss:0.600, val_acc:0.871]
Epoch [8/120    avg_loss:0.560, val_acc:0.863]
Epoch [9/120    avg_loss:0.595, val_acc:0.871]
Epoch [10/120    avg_loss:0.536, val_acc:0.879]
Epoch [11/120    avg_loss:0.471, val_acc:0.873]
Epoch [12/120    avg_loss:0.471, val_acc:0.916]
Epoch [13/120    avg_loss:0.398, val_acc:0.885]
Epoch [14/120    avg_loss:0.408, val_acc:0.883]
Epoch [15/120    avg_loss:0.419, val_acc:0.898]
Epoch [16/120    avg_loss:0.452, val_acc:0.893]
Epoch [17/120    avg_loss:0.371, val_acc:0.936]
Epoch [18/120    avg_loss:0.383, val_acc:0.891]
Epoch [19/120    avg_loss:0.392, val_acc:0.896]
Epoch [20/120    avg_loss:0.356, val_acc:0.947]
Epoch [21/120    avg_loss:0.399, val_acc:0.920]
Epoch [22/120    avg_loss:0.377, val_acc:0.900]
Epoch [23/120    avg_loss:0.343, val_acc:0.959]
Epoch [24/120    avg_loss:0.313, val_acc:0.936]
Epoch [25/120    avg_loss:0.299, val_acc:0.928]
Epoch [26/120    avg_loss:0.261, val_acc:0.896]
Epoch [27/120    avg_loss:0.285, val_acc:0.900]
Epoch [28/120    avg_loss:0.274, val_acc:0.934]
Epoch [29/120    avg_loss:0.280, val_acc:0.961]
Epoch [30/120    avg_loss:0.229, val_acc:0.963]
Epoch [31/120    avg_loss:0.182, val_acc:0.979]
Epoch [32/120    avg_loss:0.182, val_acc:0.959]
Epoch [33/120    avg_loss:0.185, val_acc:0.967]
Epoch [34/120    avg_loss:0.221, val_acc:0.926]
Epoch [35/120    avg_loss:0.230, val_acc:0.953]
Epoch [36/120    avg_loss:0.200, val_acc:0.965]
Epoch [37/120    avg_loss:0.275, val_acc:0.912]
Epoch [38/120    avg_loss:0.244, val_acc:0.938]
Epoch [39/120    avg_loss:0.236, val_acc:0.963]
Epoch [40/120    avg_loss:0.172, val_acc:0.977]
Epoch [41/120    avg_loss:0.144, val_acc:0.957]
Epoch [42/120    avg_loss:0.224, val_acc:0.941]
Epoch [43/120    avg_loss:0.151, val_acc:0.967]
Epoch [44/120    avg_loss:0.106, val_acc:0.984]
Epoch [45/120    avg_loss:0.103, val_acc:0.984]
Epoch [46/120    avg_loss:0.148, val_acc:0.951]
Epoch [47/120    avg_loss:0.194, val_acc:0.959]
Epoch [48/120    avg_loss:0.135, val_acc:0.977]
Epoch [49/120    avg_loss:0.110, val_acc:0.975]
Epoch [50/120    avg_loss:0.113, val_acc:0.930]
Epoch [51/120    avg_loss:0.320, val_acc:0.936]
Epoch [52/120    avg_loss:0.215, val_acc:0.967]
Epoch [53/120    avg_loss:0.175, val_acc:0.963]
Epoch [54/120    avg_loss:0.220, val_acc:0.979]
Epoch [55/120    avg_loss:0.166, val_acc:0.979]
Epoch [56/120    avg_loss:0.111, val_acc:0.975]
Epoch [57/120    avg_loss:0.153, val_acc:0.967]
Epoch [58/120    avg_loss:0.082, val_acc:0.977]
Epoch [59/120    avg_loss:0.089, val_acc:0.986]
Epoch [60/120    avg_loss:0.066, val_acc:0.988]
Epoch [61/120    avg_loss:0.067, val_acc:0.988]
Epoch [62/120    avg_loss:0.073, val_acc:0.988]
Epoch [63/120    avg_loss:0.060, val_acc:0.988]
Epoch [64/120    avg_loss:0.061, val_acc:0.988]
Epoch [65/120    avg_loss:0.054, val_acc:0.988]
Epoch [66/120    avg_loss:0.053, val_acc:0.988]
Epoch [67/120    avg_loss:0.059, val_acc:0.988]
Epoch [68/120    avg_loss:0.054, val_acc:0.986]
Epoch [69/120    avg_loss:0.049, val_acc:0.988]
Epoch [70/120    avg_loss:0.045, val_acc:0.988]
Epoch [71/120    avg_loss:0.049, val_acc:0.988]
Epoch [72/120    avg_loss:0.060, val_acc:0.988]
Epoch [73/120    avg_loss:0.056, val_acc:0.988]
Epoch [74/120    avg_loss:0.049, val_acc:0.988]
Epoch [75/120    avg_loss:0.050, val_acc:0.988]
Epoch [76/120    avg_loss:0.052, val_acc:0.988]
Epoch [77/120    avg_loss:0.051, val_acc:0.988]
Epoch [78/120    avg_loss:0.044, val_acc:0.988]
Epoch [79/120    avg_loss:0.074, val_acc:0.988]
Epoch [80/120    avg_loss:0.034, val_acc:0.988]
Epoch [81/120    avg_loss:0.032, val_acc:0.988]
Epoch [82/120    avg_loss:0.030, val_acc:0.988]
Epoch [83/120    avg_loss:0.037, val_acc:0.988]
Epoch [84/120    avg_loss:0.043, val_acc:0.988]
Epoch [85/120    avg_loss:0.037, val_acc:0.988]
Epoch [86/120    avg_loss:0.038, val_acc:0.988]
Epoch [87/120    avg_loss:0.039, val_acc:0.988]
Epoch [88/120    avg_loss:0.053, val_acc:0.988]
Epoch [89/120    avg_loss:0.046, val_acc:0.988]
Epoch [90/120    avg_loss:0.035, val_acc:0.988]
Epoch [91/120    avg_loss:0.026, val_acc:0.988]
Epoch [92/120    avg_loss:0.037, val_acc:0.988]
Epoch [93/120    avg_loss:0.043, val_acc:0.988]
Epoch [94/120    avg_loss:0.033, val_acc:0.986]
Epoch [95/120    avg_loss:0.034, val_acc:0.988]
Epoch [96/120    avg_loss:0.040, val_acc:0.988]
Epoch [97/120    avg_loss:0.039, val_acc:0.988]
Epoch [98/120    avg_loss:0.041, val_acc:0.988]
Epoch [99/120    avg_loss:0.048, val_acc:0.988]
Epoch [100/120    avg_loss:0.037, val_acc:0.988]
Epoch [101/120    avg_loss:0.040, val_acc:0.988]
Epoch [102/120    avg_loss:0.035, val_acc:0.988]
Epoch [103/120    avg_loss:0.035, val_acc:0.988]
Epoch [104/120    avg_loss:0.040, val_acc:0.988]
Epoch [105/120    avg_loss:0.040, val_acc:0.988]
Epoch [106/120    avg_loss:0.042, val_acc:0.986]
Epoch [107/120    avg_loss:0.034, val_acc:0.988]
Epoch [108/120    avg_loss:0.039, val_acc:0.986]
Epoch [109/120    avg_loss:0.032, val_acc:0.986]
Epoch [110/120    avg_loss:0.045, val_acc:0.986]
Epoch [111/120    avg_loss:0.036, val_acc:0.984]
Epoch [112/120    avg_loss:0.041, val_acc:0.986]
Epoch [113/120    avg_loss:0.033, val_acc:0.986]
Epoch [114/120    avg_loss:0.031, val_acc:0.986]
Epoch [115/120    avg_loss:0.041, val_acc:0.986]
Epoch [116/120    avg_loss:0.048, val_acc:0.988]
Epoch [117/120    avg_loss:0.034, val_acc:0.988]
Epoch [118/120    avg_loss:0.031, val_acc:0.988]
Epoch [119/120    avg_loss:0.035, val_acc:0.988]
Epoch [120/120    avg_loss:0.026, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 217   0   0   0   0   2   0   0   0   0   0   0]
 [  0   0   0 224   3   0   0   0   1   2   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0  23 122   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   1   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   3   0   0   0   0 831]]

Accuracy:
98.72068230277185

F1 scores:
[       nan 1.         0.97309417 0.98678414 0.91182796 0.86524823
 1.         0.93333333 0.99359795 0.9978678  1.         1.
 0.99889503 0.9981982 ]

Kappa:
0.9857564797387696
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fad60826780>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.251, val_acc:0.645]
Epoch [2/120    avg_loss:1.637, val_acc:0.709]
Epoch [3/120    avg_loss:1.260, val_acc:0.682]
Epoch [4/120    avg_loss:0.990, val_acc:0.803]
Epoch [5/120    avg_loss:0.894, val_acc:0.727]
Epoch [6/120    avg_loss:0.779, val_acc:0.811]
Epoch [7/120    avg_loss:0.661, val_acc:0.824]
Epoch [8/120    avg_loss:0.668, val_acc:0.832]
Epoch [9/120    avg_loss:0.617, val_acc:0.879]
Epoch [10/120    avg_loss:0.575, val_acc:0.865]
Epoch [11/120    avg_loss:0.581, val_acc:0.836]
Epoch [12/120    avg_loss:0.541, val_acc:0.875]
Epoch [13/120    avg_loss:0.493, val_acc:0.873]
Epoch [14/120    avg_loss:0.467, val_acc:0.873]
Epoch [15/120    avg_loss:0.414, val_acc:0.865]
Epoch [16/120    avg_loss:0.423, val_acc:0.908]
Epoch [17/120    avg_loss:0.371, val_acc:0.883]
Epoch [18/120    avg_loss:0.384, val_acc:0.875]
Epoch [19/120    avg_loss:0.398, val_acc:0.891]
Epoch [20/120    avg_loss:0.405, val_acc:0.904]
Epoch [21/120    avg_loss:0.376, val_acc:0.893]
Epoch [22/120    avg_loss:0.401, val_acc:0.891]
Epoch [23/120    avg_loss:0.363, val_acc:0.885]
Epoch [24/120    avg_loss:0.261, val_acc:0.902]
Epoch [25/120    avg_loss:0.338, val_acc:0.916]
Epoch [26/120    avg_loss:0.280, val_acc:0.904]
Epoch [27/120    avg_loss:0.270, val_acc:0.887]
Epoch [28/120    avg_loss:0.288, val_acc:0.902]
Epoch [29/120    avg_loss:0.255, val_acc:0.932]
Epoch [30/120    avg_loss:0.220, val_acc:0.906]
Epoch [31/120    avg_loss:0.278, val_acc:0.904]
Epoch [32/120    avg_loss:0.249, val_acc:0.906]
Epoch [33/120    avg_loss:0.429, val_acc:0.900]
Epoch [34/120    avg_loss:0.283, val_acc:0.920]
Epoch [35/120    avg_loss:0.218, val_acc:0.938]
Epoch [36/120    avg_loss:0.225, val_acc:0.945]
Epoch [37/120    avg_loss:0.154, val_acc:0.928]
Epoch [38/120    avg_loss:0.176, val_acc:0.918]
Epoch [39/120    avg_loss:0.195, val_acc:0.932]
Epoch [40/120    avg_loss:0.221, val_acc:0.936]
Epoch [41/120    avg_loss:0.171, val_acc:0.939]
Epoch [42/120    avg_loss:0.151, val_acc:0.947]
Epoch [43/120    avg_loss:0.148, val_acc:0.939]
Epoch [44/120    avg_loss:0.161, val_acc:0.951]
Epoch [45/120    avg_loss:0.081, val_acc:0.953]
Epoch [46/120    avg_loss:0.153, val_acc:0.939]
Epoch [47/120    avg_loss:0.120, val_acc:0.953]
Epoch [48/120    avg_loss:0.154, val_acc:0.939]
Epoch [49/120    avg_loss:0.179, val_acc:0.947]
Epoch [50/120    avg_loss:0.115, val_acc:0.908]
Epoch [51/120    avg_loss:0.182, val_acc:0.953]
Epoch [52/120    avg_loss:0.130, val_acc:0.943]
Epoch [53/120    avg_loss:0.113, val_acc:0.934]
Epoch [54/120    avg_loss:0.133, val_acc:0.934]
Epoch [55/120    avg_loss:0.135, val_acc:0.951]
Epoch [56/120    avg_loss:0.157, val_acc:0.945]
Epoch [57/120    avg_loss:0.143, val_acc:0.953]
Epoch [58/120    avg_loss:0.154, val_acc:0.939]
Epoch [59/120    avg_loss:0.110, val_acc:0.961]
Epoch [60/120    avg_loss:0.074, val_acc:0.967]
Epoch [61/120    avg_loss:0.076, val_acc:0.945]
Epoch [62/120    avg_loss:0.125, val_acc:0.924]
Epoch [63/120    avg_loss:0.203, val_acc:0.951]
Epoch [64/120    avg_loss:0.115, val_acc:0.943]
Epoch [65/120    avg_loss:0.063, val_acc:0.947]
Epoch [66/120    avg_loss:0.072, val_acc:0.957]
Epoch [67/120    avg_loss:0.162, val_acc:0.943]
Epoch [68/120    avg_loss:0.125, val_acc:0.930]
Epoch [69/120    avg_loss:0.094, val_acc:0.963]
Epoch [70/120    avg_loss:0.092, val_acc:0.959]
Epoch [71/120    avg_loss:0.068, val_acc:0.920]
Epoch [72/120    avg_loss:0.131, val_acc:0.955]
Epoch [73/120    avg_loss:0.058, val_acc:0.963]
Epoch [74/120    avg_loss:0.065, val_acc:0.963]
Epoch [75/120    avg_loss:0.047, val_acc:0.969]
Epoch [76/120    avg_loss:0.040, val_acc:0.971]
Epoch [77/120    avg_loss:0.032, val_acc:0.967]
Epoch [78/120    avg_loss:0.039, val_acc:0.967]
Epoch [79/120    avg_loss:0.026, val_acc:0.969]
Epoch [80/120    avg_loss:0.039, val_acc:0.969]
Epoch [81/120    avg_loss:0.037, val_acc:0.971]
Epoch [82/120    avg_loss:0.042, val_acc:0.967]
Epoch [83/120    avg_loss:0.029, val_acc:0.969]
Epoch [84/120    avg_loss:0.025, val_acc:0.969]
Epoch [85/120    avg_loss:0.035, val_acc:0.971]
Epoch [86/120    avg_loss:0.029, val_acc:0.973]
Epoch [87/120    avg_loss:0.028, val_acc:0.971]
Epoch [88/120    avg_loss:0.038, val_acc:0.971]
Epoch [89/120    avg_loss:0.026, val_acc:0.971]
Epoch [90/120    avg_loss:0.031, val_acc:0.971]
Epoch [91/120    avg_loss:0.027, val_acc:0.971]
Epoch [92/120    avg_loss:0.030, val_acc:0.971]
Epoch [93/120    avg_loss:0.029, val_acc:0.973]
Epoch [94/120    avg_loss:0.029, val_acc:0.973]
Epoch [95/120    avg_loss:0.030, val_acc:0.971]
Epoch [96/120    avg_loss:0.027, val_acc:0.971]
Epoch [97/120    avg_loss:0.023, val_acc:0.971]
Epoch [98/120    avg_loss:0.026, val_acc:0.971]
Epoch [99/120    avg_loss:0.025, val_acc:0.971]
Epoch [100/120    avg_loss:0.026, val_acc:0.971]
Epoch [101/120    avg_loss:0.034, val_acc:0.971]
Epoch [102/120    avg_loss:0.033, val_acc:0.971]
Epoch [103/120    avg_loss:0.023, val_acc:0.973]
Epoch [104/120    avg_loss:0.036, val_acc:0.969]
Epoch [105/120    avg_loss:0.023, val_acc:0.973]
Epoch [106/120    avg_loss:0.021, val_acc:0.973]
Epoch [107/120    avg_loss:0.026, val_acc:0.977]
Epoch [108/120    avg_loss:0.029, val_acc:0.971]
Epoch [109/120    avg_loss:0.031, val_acc:0.971]
Epoch [110/120    avg_loss:0.027, val_acc:0.975]
Epoch [111/120    avg_loss:0.029, val_acc:0.971]
Epoch [112/120    avg_loss:0.023, val_acc:0.975]
Epoch [113/120    avg_loss:0.027, val_acc:0.975]
Epoch [114/120    avg_loss:0.029, val_acc:0.973]
Epoch [115/120    avg_loss:0.033, val_acc:0.975]
Epoch [116/120    avg_loss:0.019, val_acc:0.975]
Epoch [117/120    avg_loss:0.027, val_acc:0.973]
Epoch [118/120    avg_loss:0.031, val_acc:0.973]
Epoch [119/120    avg_loss:0.026, val_acc:0.973]
Epoch [120/120    avg_loss:0.023, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 208   0   0   0   0  11   0   0   0   0   0   0]
 [  0   0   0 217  10   0   0   0   0   3   0   0   0   0]
 [  0   0   0   0 210  16   0   0   0   0   0   0   1   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   1   0   0   0   0 205   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.86993603411514

F1 scores:
[       nan 0.99927061 0.95194508 0.97091723 0.93959732 0.94771242
 0.99756691 0.88888889 1.         0.99680511 1.         0.99867198
 0.99779736 1.        ]

Kappa:
0.987418765066025
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:32
Validation dataloader:32
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe75bd197b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 70407==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.209, val_acc:0.602]
Epoch [2/120    avg_loss:1.605, val_acc:0.668]
Epoch [3/120    avg_loss:1.285, val_acc:0.742]
Epoch [4/120    avg_loss:1.056, val_acc:0.754]
Epoch [5/120    avg_loss:0.857, val_acc:0.818]
Epoch [6/120    avg_loss:0.771, val_acc:0.748]
Epoch [7/120    avg_loss:0.715, val_acc:0.762]
Epoch [8/120    avg_loss:0.633, val_acc:0.787]
Epoch [9/120    avg_loss:0.567, val_acc:0.848]
Epoch [10/120    avg_loss:0.493, val_acc:0.879]
Epoch [11/120    avg_loss:0.491, val_acc:0.859]
Epoch [12/120    avg_loss:0.433, val_acc:0.906]
Epoch [13/120    avg_loss:0.388, val_acc:0.881]
Epoch [14/120    avg_loss:0.510, val_acc:0.811]
Epoch [15/120    avg_loss:0.508, val_acc:0.867]
Epoch [16/120    avg_loss:0.391, val_acc:0.895]
Epoch [17/120    avg_loss:0.372, val_acc:0.924]
Epoch [18/120    avg_loss:0.390, val_acc:0.865]
Epoch [19/120    avg_loss:0.346, val_acc:0.896]
Epoch [20/120    avg_loss:0.343, val_acc:0.928]
Epoch [21/120    avg_loss:0.321, val_acc:0.893]
Epoch [22/120    avg_loss:0.388, val_acc:0.891]
Epoch [23/120    avg_loss:0.328, val_acc:0.920]
Epoch [24/120    avg_loss:0.311, val_acc:0.922]
Epoch [25/120    avg_loss:0.307, val_acc:0.924]
Epoch [26/120    avg_loss:0.299, val_acc:0.941]
Epoch [27/120    avg_loss:0.275, val_acc:0.945]
Epoch [28/120    avg_loss:0.342, val_acc:0.928]
Epoch [29/120    avg_loss:0.346, val_acc:0.953]
Epoch [30/120    avg_loss:0.199, val_acc:0.912]
Epoch [31/120    avg_loss:0.242, val_acc:0.939]
Epoch [32/120    avg_loss:0.226, val_acc:0.951]
Epoch [33/120    avg_loss:0.278, val_acc:0.930]
Epoch [34/120    avg_loss:0.208, val_acc:0.936]
Epoch [35/120    avg_loss:0.197, val_acc:0.943]
Epoch [36/120    avg_loss:0.197, val_acc:0.932]
Epoch [37/120    avg_loss:0.189, val_acc:0.924]
Epoch [38/120    avg_loss:0.226, val_acc:0.918]
Epoch [39/120    avg_loss:0.228, val_acc:0.951]
Epoch [40/120    avg_loss:0.182, val_acc:0.955]
Epoch [41/120    avg_loss:0.164, val_acc:0.932]
Epoch [42/120    avg_loss:0.209, val_acc:0.945]
Epoch [43/120    avg_loss:0.145, val_acc:0.955]
Epoch [44/120    avg_loss:0.149, val_acc:0.961]
Epoch [45/120    avg_loss:0.141, val_acc:0.969]
Epoch [46/120    avg_loss:0.135, val_acc:0.967]
Epoch [47/120    avg_loss:0.084, val_acc:0.965]
Epoch [48/120    avg_loss:0.176, val_acc:0.963]
Epoch [49/120    avg_loss:0.182, val_acc:0.951]
Epoch [50/120    avg_loss:0.163, val_acc:0.957]
Epoch [51/120    avg_loss:0.084, val_acc:0.971]
Epoch [52/120    avg_loss:0.071, val_acc:0.965]
Epoch [53/120    avg_loss:0.167, val_acc:0.955]
Epoch [54/120    avg_loss:0.113, val_acc:0.934]
Epoch [55/120    avg_loss:0.100, val_acc:0.957]
Epoch [56/120    avg_loss:0.133, val_acc:0.924]
Epoch [57/120    avg_loss:0.171, val_acc:0.965]
Epoch [58/120    avg_loss:0.082, val_acc:0.965]
Epoch [59/120    avg_loss:0.142, val_acc:0.965]
Epoch [60/120    avg_loss:0.144, val_acc:0.941]
Epoch [61/120    avg_loss:0.110, val_acc:0.963]
Epoch [62/120    avg_loss:0.105, val_acc:0.967]
Epoch [63/120    avg_loss:0.063, val_acc:0.967]
Epoch [64/120    avg_loss:0.103, val_acc:0.953]
Epoch [65/120    avg_loss:0.097, val_acc:0.967]
Epoch [66/120    avg_loss:0.066, val_acc:0.975]
Epoch [67/120    avg_loss:0.045, val_acc:0.979]
Epoch [68/120    avg_loss:0.047, val_acc:0.979]
Epoch [69/120    avg_loss:0.044, val_acc:0.979]
Epoch [70/120    avg_loss:0.039, val_acc:0.975]
Epoch [71/120    avg_loss:0.040, val_acc:0.975]
Epoch [72/120    avg_loss:0.042, val_acc:0.977]
Epoch [73/120    avg_loss:0.034, val_acc:0.977]
Epoch [74/120    avg_loss:0.037, val_acc:0.977]
Epoch [75/120    avg_loss:0.046, val_acc:0.977]
Epoch [76/120    avg_loss:0.031, val_acc:0.975]
Epoch [77/120    avg_loss:0.051, val_acc:0.977]
Epoch [78/120    avg_loss:0.035, val_acc:0.977]
Epoch [79/120    avg_loss:0.040, val_acc:0.979]
Epoch [80/120    avg_loss:0.040, val_acc:0.979]
Epoch [81/120    avg_loss:0.040, val_acc:0.979]
Epoch [82/120    avg_loss:0.035, val_acc:0.980]
Epoch [83/120    avg_loss:0.034, val_acc:0.980]
Epoch [84/120    avg_loss:0.033, val_acc:0.979]
Epoch [85/120    avg_loss:0.031, val_acc:0.980]
Epoch [86/120    avg_loss:0.029, val_acc:0.979]
Epoch [87/120    avg_loss:0.028, val_acc:0.979]
Epoch [88/120    avg_loss:0.034, val_acc:0.980]
Epoch [89/120    avg_loss:0.033, val_acc:0.979]
Epoch [90/120    avg_loss:0.038, val_acc:0.979]
Epoch [91/120    avg_loss:0.035, val_acc:0.980]
Epoch [92/120    avg_loss:0.029, val_acc:0.980]
Epoch [93/120    avg_loss:0.027, val_acc:0.980]
Epoch [94/120    avg_loss:0.035, val_acc:0.980]
Epoch [95/120    avg_loss:0.037, val_acc:0.980]
Epoch [96/120    avg_loss:0.034, val_acc:0.980]
Epoch [97/120    avg_loss:0.032, val_acc:0.977]
Epoch [98/120    avg_loss:0.031, val_acc:0.982]
Epoch [99/120    avg_loss:0.032, val_acc:0.980]
Epoch [100/120    avg_loss:0.032, val_acc:0.979]
Epoch [101/120    avg_loss:0.032, val_acc:0.980]
Epoch [102/120    avg_loss:0.026, val_acc:0.982]
Epoch [103/120    avg_loss:0.032, val_acc:0.980]
Epoch [104/120    avg_loss:0.031, val_acc:0.980]
Epoch [105/120    avg_loss:0.025, val_acc:0.980]
Epoch [106/120    avg_loss:0.034, val_acc:0.982]
Epoch [107/120    avg_loss:0.026, val_acc:0.982]
Epoch [108/120    avg_loss:0.029, val_acc:0.982]
Epoch [109/120    avg_loss:0.038, val_acc:0.979]
Epoch [110/120    avg_loss:0.032, val_acc:0.979]
Epoch [111/120    avg_loss:0.024, val_acc:0.980]
Epoch [112/120    avg_loss:0.029, val_acc:0.980]
Epoch [113/120    avg_loss:0.026, val_acc:0.980]
Epoch [114/120    avg_loss:0.029, val_acc:0.982]
Epoch [115/120    avg_loss:0.029, val_acc:0.980]
Epoch [116/120    avg_loss:0.028, val_acc:0.979]
Epoch [117/120    avg_loss:0.028, val_acc:0.979]
Epoch [118/120    avg_loss:0.029, val_acc:0.982]
Epoch [119/120    avg_loss:0.031, val_acc:0.980]
Epoch [120/120    avg_loss:0.027, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 210   0   0   0   0   9   0   0   0   0   0   0]
 [  0   0   0 214  13   2   0   0   0   1   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0  20 125   0   0   0   0   0   0   0   0]
 [  0   6   0   0   0   0 200   0   0   0   0   0   0   0]
 [  0   0  14   0   0   0   0  80   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.37953091684435

F1 scores:
[       nan 0.99563953 0.94808126 0.96396396 0.90756303 0.88339223
 0.98522167 0.87431694 1.         0.99893276 1.         1.
 1.         1.        ]

Kappa:
0.9819551131668138
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f355cd4d7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.237, val_acc:0.521]
Epoch [2/120    avg_loss:1.708, val_acc:0.639]
Epoch [3/120    avg_loss:1.410, val_acc:0.637]
Epoch [4/120    avg_loss:1.110, val_acc:0.754]
Epoch [5/120    avg_loss:0.911, val_acc:0.811]
Epoch [6/120    avg_loss:0.871, val_acc:0.846]
Epoch [7/120    avg_loss:0.746, val_acc:0.812]
Epoch [8/120    avg_loss:0.670, val_acc:0.865]
Epoch [9/120    avg_loss:0.602, val_acc:0.859]
Epoch [10/120    avg_loss:0.599, val_acc:0.889]
Epoch [11/120    avg_loss:0.591, val_acc:0.836]
Epoch [12/120    avg_loss:0.576, val_acc:0.891]
Epoch [13/120    avg_loss:0.518, val_acc:0.900]
Epoch [14/120    avg_loss:0.421, val_acc:0.875]
Epoch [15/120    avg_loss:0.482, val_acc:0.898]
Epoch [16/120    avg_loss:0.422, val_acc:0.883]
Epoch [17/120    avg_loss:0.486, val_acc:0.885]
Epoch [18/120    avg_loss:0.418, val_acc:0.908]
Epoch [19/120    avg_loss:0.461, val_acc:0.854]
Epoch [20/120    avg_loss:0.355, val_acc:0.920]
Epoch [21/120    avg_loss:0.396, val_acc:0.914]
Epoch [22/120    avg_loss:0.375, val_acc:0.895]
Epoch [23/120    avg_loss:0.303, val_acc:0.934]
Epoch [24/120    avg_loss:0.290, val_acc:0.938]
Epoch [25/120    avg_loss:0.234, val_acc:0.928]
Epoch [26/120    avg_loss:0.216, val_acc:0.916]
Epoch [27/120    avg_loss:0.237, val_acc:0.955]
Epoch [28/120    avg_loss:0.210, val_acc:0.941]
Epoch [29/120    avg_loss:0.204, val_acc:0.945]
Epoch [30/120    avg_loss:0.190, val_acc:0.951]
Epoch [31/120    avg_loss:0.261, val_acc:0.926]
Epoch [32/120    avg_loss:0.273, val_acc:0.926]
Epoch [33/120    avg_loss:0.175, val_acc:0.951]
Epoch [34/120    avg_loss:0.137, val_acc:0.965]
Epoch [35/120    avg_loss:0.246, val_acc:0.928]
Epoch [36/120    avg_loss:0.193, val_acc:0.965]
Epoch [37/120    avg_loss:0.190, val_acc:0.951]
Epoch [38/120    avg_loss:0.142, val_acc:0.951]
Epoch [39/120    avg_loss:0.172, val_acc:0.961]
Epoch [40/120    avg_loss:0.181, val_acc:0.924]
Epoch [41/120    avg_loss:0.184, val_acc:0.967]
Epoch [42/120    avg_loss:0.113, val_acc:0.975]
Epoch [43/120    avg_loss:0.128, val_acc:0.961]
Epoch [44/120    avg_loss:0.228, val_acc:0.893]
Epoch [45/120    avg_loss:0.260, val_acc:0.934]
Epoch [46/120    avg_loss:0.217, val_acc:0.961]
Epoch [47/120    avg_loss:0.129, val_acc:0.980]
Epoch [48/120    avg_loss:0.090, val_acc:0.975]
Epoch [49/120    avg_loss:0.078, val_acc:0.975]
Epoch [50/120    avg_loss:0.083, val_acc:0.984]
Epoch [51/120    avg_loss:0.077, val_acc:0.971]
Epoch [52/120    avg_loss:0.155, val_acc:0.963]
Epoch [53/120    avg_loss:0.117, val_acc:0.971]
Epoch [54/120    avg_loss:0.117, val_acc:0.984]
Epoch [55/120    avg_loss:0.076, val_acc:0.982]
Epoch [56/120    avg_loss:0.071, val_acc:0.971]
Epoch [57/120    avg_loss:0.099, val_acc:0.977]
Epoch [58/120    avg_loss:0.100, val_acc:0.941]
Epoch [59/120    avg_loss:0.083, val_acc:0.973]
Epoch [60/120    avg_loss:0.041, val_acc:0.988]
Epoch [61/120    avg_loss:0.058, val_acc:0.986]
Epoch [62/120    avg_loss:0.065, val_acc:0.988]
Epoch [63/120    avg_loss:0.049, val_acc:0.986]
Epoch [64/120    avg_loss:0.080, val_acc:0.992]
Epoch [65/120    avg_loss:0.097, val_acc:0.979]
Epoch [66/120    avg_loss:0.117, val_acc:0.973]
Epoch [67/120    avg_loss:0.095, val_acc:0.980]
Epoch [68/120    avg_loss:0.084, val_acc:0.971]
Epoch [69/120    avg_loss:0.204, val_acc:0.928]
Epoch [70/120    avg_loss:0.113, val_acc:0.969]
Epoch [71/120    avg_loss:0.080, val_acc:0.984]
Epoch [72/120    avg_loss:0.071, val_acc:0.986]
Epoch [73/120    avg_loss:0.045, val_acc:0.982]
Epoch [74/120    avg_loss:0.049, val_acc:0.990]
Epoch [75/120    avg_loss:0.052, val_acc:0.980]
Epoch [76/120    avg_loss:0.154, val_acc:0.975]
Epoch [77/120    avg_loss:0.071, val_acc:0.988]
Epoch [78/120    avg_loss:0.047, val_acc:0.990]
Epoch [79/120    avg_loss:0.047, val_acc:0.992]
Epoch [80/120    avg_loss:0.024, val_acc:0.992]
Epoch [81/120    avg_loss:0.026, val_acc:0.992]
Epoch [82/120    avg_loss:0.021, val_acc:0.992]
Epoch [83/120    avg_loss:0.036, val_acc:0.994]
Epoch [84/120    avg_loss:0.032, val_acc:0.992]
Epoch [85/120    avg_loss:0.027, val_acc:0.992]
Epoch [86/120    avg_loss:0.024, val_acc:0.992]
Epoch [87/120    avg_loss:0.024, val_acc:0.992]
Epoch [88/120    avg_loss:0.027, val_acc:0.992]
Epoch [89/120    avg_loss:0.018, val_acc:0.992]
Epoch [90/120    avg_loss:0.017, val_acc:0.992]
Epoch [91/120    avg_loss:0.019, val_acc:0.992]
Epoch [92/120    avg_loss:0.024, val_acc:0.992]
Epoch [93/120    avg_loss:0.017, val_acc:0.988]
Epoch [94/120    avg_loss:0.020, val_acc:0.990]
Epoch [95/120    avg_loss:0.017, val_acc:0.990]
Epoch [96/120    avg_loss:0.020, val_acc:0.990]
Epoch [97/120    avg_loss:0.020, val_acc:0.990]
Epoch [98/120    avg_loss:0.016, val_acc:0.990]
Epoch [99/120    avg_loss:0.020, val_acc:0.990]
Epoch [100/120    avg_loss:0.015, val_acc:0.990]
Epoch [101/120    avg_loss:0.022, val_acc:0.990]
Epoch [102/120    avg_loss:0.024, val_acc:0.990]
Epoch [103/120    avg_loss:0.017, val_acc:0.992]
Epoch [104/120    avg_loss:0.015, val_acc:0.992]
Epoch [105/120    avg_loss:0.019, val_acc:0.992]
Epoch [106/120    avg_loss:0.022, val_acc:0.992]
Epoch [107/120    avg_loss:0.019, val_acc:0.992]
Epoch [108/120    avg_loss:0.017, val_acc:0.992]
Epoch [109/120    avg_loss:0.015, val_acc:0.992]
Epoch [110/120    avg_loss:0.020, val_acc:0.992]
Epoch [111/120    avg_loss:0.018, val_acc:0.992]
Epoch [112/120    avg_loss:0.013, val_acc:0.992]
Epoch [113/120    avg_loss:0.025, val_acc:0.992]
Epoch [114/120    avg_loss:0.015, val_acc:0.992]
Epoch [115/120    avg_loss:0.020, val_acc:0.992]
Epoch [116/120    avg_loss:0.022, val_acc:0.992]
Epoch [117/120    avg_loss:0.013, val_acc:0.992]
Epoch [118/120    avg_loss:0.030, val_acc:0.992]
Epoch [119/120    avg_loss:0.019, val_acc:0.992]
Epoch [120/120    avg_loss:0.013, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 143   1   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   1   0   0   0   0   0   0   0   0   3 449   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 1.         0.98198198 0.98678414 0.96       0.95652174
 0.99757869 0.96174863 1.         1.         1.         0.99603699
 0.99556541 1.        ]

Kappa:
0.9928787776642026
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0c64d507f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.313, val_acc:0.502]
Epoch [2/120    avg_loss:1.724, val_acc:0.645]
Epoch [3/120    avg_loss:1.319, val_acc:0.663]
Epoch [4/120    avg_loss:1.074, val_acc:0.780]
Epoch [5/120    avg_loss:0.950, val_acc:0.736]
Epoch [6/120    avg_loss:0.832, val_acc:0.837]
Epoch [7/120    avg_loss:0.780, val_acc:0.810]
Epoch [8/120    avg_loss:0.672, val_acc:0.891]
Epoch [9/120    avg_loss:0.563, val_acc:0.819]
Epoch [10/120    avg_loss:0.553, val_acc:0.911]
Epoch [11/120    avg_loss:0.487, val_acc:0.879]
Epoch [12/120    avg_loss:0.444, val_acc:0.899]
Epoch [13/120    avg_loss:0.431, val_acc:0.851]
Epoch [14/120    avg_loss:0.471, val_acc:0.883]
Epoch [15/120    avg_loss:0.466, val_acc:0.885]
Epoch [16/120    avg_loss:0.442, val_acc:0.897]
Epoch [17/120    avg_loss:0.449, val_acc:0.911]
Epoch [18/120    avg_loss:0.349, val_acc:0.883]
Epoch [19/120    avg_loss:0.365, val_acc:0.905]
Epoch [20/120    avg_loss:0.350, val_acc:0.891]
Epoch [21/120    avg_loss:0.382, val_acc:0.929]
Epoch [22/120    avg_loss:0.297, val_acc:0.917]
Epoch [23/120    avg_loss:0.346, val_acc:0.909]
Epoch [24/120    avg_loss:0.382, val_acc:0.877]
Epoch [25/120    avg_loss:0.292, val_acc:0.921]
Epoch [26/120    avg_loss:0.215, val_acc:0.929]
Epoch [27/120    avg_loss:0.234, val_acc:0.931]
Epoch [28/120    avg_loss:0.382, val_acc:0.927]
Epoch [29/120    avg_loss:0.320, val_acc:0.946]
Epoch [30/120    avg_loss:0.203, val_acc:0.935]
Epoch [31/120    avg_loss:0.229, val_acc:0.946]
Epoch [32/120    avg_loss:0.248, val_acc:0.929]
Epoch [33/120    avg_loss:0.211, val_acc:0.931]
Epoch [34/120    avg_loss:0.238, val_acc:0.921]
Epoch [35/120    avg_loss:0.239, val_acc:0.929]
Epoch [36/120    avg_loss:0.229, val_acc:0.938]
Epoch [37/120    avg_loss:0.169, val_acc:0.956]
Epoch [38/120    avg_loss:0.170, val_acc:0.935]
Epoch [39/120    avg_loss:0.134, val_acc:0.962]
Epoch [40/120    avg_loss:0.115, val_acc:0.958]
Epoch [41/120    avg_loss:0.088, val_acc:0.942]
Epoch [42/120    avg_loss:0.124, val_acc:0.964]
Epoch [43/120    avg_loss:0.086, val_acc:0.974]
Epoch [44/120    avg_loss:0.101, val_acc:0.968]
Epoch [45/120    avg_loss:0.133, val_acc:0.956]
Epoch [46/120    avg_loss:0.160, val_acc:0.940]
Epoch [47/120    avg_loss:0.242, val_acc:0.946]
Epoch [48/120    avg_loss:0.199, val_acc:0.942]
Epoch [49/120    avg_loss:0.114, val_acc:0.968]
Epoch [50/120    avg_loss:0.097, val_acc:0.976]
Epoch [51/120    avg_loss:0.107, val_acc:0.978]
Epoch [52/120    avg_loss:0.119, val_acc:0.974]
Epoch [53/120    avg_loss:0.092, val_acc:0.933]
Epoch [54/120    avg_loss:0.142, val_acc:0.962]
Epoch [55/120    avg_loss:0.121, val_acc:0.974]
Epoch [56/120    avg_loss:0.081, val_acc:0.980]
Epoch [57/120    avg_loss:0.059, val_acc:0.968]
Epoch [58/120    avg_loss:0.065, val_acc:0.968]
Epoch [59/120    avg_loss:0.098, val_acc:0.968]
Epoch [60/120    avg_loss:0.054, val_acc:0.986]
Epoch [61/120    avg_loss:0.043, val_acc:0.982]
Epoch [62/120    avg_loss:0.086, val_acc:0.974]
Epoch [63/120    avg_loss:0.083, val_acc:0.964]
Epoch [64/120    avg_loss:0.141, val_acc:0.933]
Epoch [65/120    avg_loss:0.086, val_acc:0.962]
Epoch [66/120    avg_loss:0.056, val_acc:0.972]
Epoch [67/120    avg_loss:0.135, val_acc:0.978]
Epoch [68/120    avg_loss:0.127, val_acc:0.960]
Epoch [69/120    avg_loss:0.068, val_acc:0.970]
Epoch [70/120    avg_loss:0.084, val_acc:0.988]
Epoch [71/120    avg_loss:0.045, val_acc:0.982]
Epoch [72/120    avg_loss:0.038, val_acc:0.980]
Epoch [73/120    avg_loss:0.049, val_acc:0.933]
Epoch [74/120    avg_loss:0.070, val_acc:0.984]
Epoch [75/120    avg_loss:0.038, val_acc:0.980]
Epoch [76/120    avg_loss:0.038, val_acc:0.992]
Epoch [77/120    avg_loss:0.039, val_acc:0.980]
Epoch [78/120    avg_loss:0.034, val_acc:0.986]
Epoch [79/120    avg_loss:0.024, val_acc:0.986]
Epoch [80/120    avg_loss:0.027, val_acc:0.986]
Epoch [81/120    avg_loss:0.026, val_acc:0.986]
Epoch [82/120    avg_loss:0.024, val_acc:0.980]
Epoch [83/120    avg_loss:0.054, val_acc:0.974]
Epoch [84/120    avg_loss:0.034, val_acc:0.982]
Epoch [85/120    avg_loss:0.038, val_acc:0.980]
Epoch [86/120    avg_loss:0.016, val_acc:0.986]
Epoch [87/120    avg_loss:0.033, val_acc:0.978]
Epoch [88/120    avg_loss:0.030, val_acc:0.988]
Epoch [89/120    avg_loss:0.015, val_acc:0.984]
Epoch [90/120    avg_loss:0.013, val_acc:0.990]
Epoch [91/120    avg_loss:0.008, val_acc:0.988]
Epoch [92/120    avg_loss:0.017, val_acc:0.988]
Epoch [93/120    avg_loss:0.012, val_acc:0.986]
Epoch [94/120    avg_loss:0.011, val_acc:0.986]
Epoch [95/120    avg_loss:0.010, val_acc:0.986]
Epoch [96/120    avg_loss:0.015, val_acc:0.986]
Epoch [97/120    avg_loss:0.010, val_acc:0.988]
Epoch [98/120    avg_loss:0.012, val_acc:0.986]
Epoch [99/120    avg_loss:0.010, val_acc:0.986]
Epoch [100/120    avg_loss:0.012, val_acc:0.988]
Epoch [101/120    avg_loss:0.008, val_acc:0.986]
Epoch [102/120    avg_loss:0.010, val_acc:0.986]
Epoch [103/120    avg_loss:0.010, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.986]
Epoch [106/120    avg_loss:0.012, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.007, val_acc:0.986]
Epoch [109/120    avg_loss:0.007, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.009, val_acc:0.986]
Epoch [112/120    avg_loss:0.009, val_acc:0.986]
Epoch [113/120    avg_loss:0.010, val_acc:0.986]
Epoch [114/120    avg_loss:0.011, val_acc:0.986]
Epoch [115/120    avg_loss:0.008, val_acc:0.986]
Epoch [116/120    avg_loss:0.007, val_acc:0.986]
Epoch [117/120    avg_loss:0.011, val_acc:0.986]
Epoch [118/120    avg_loss:0.008, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 227   0   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.59488272921108

F1 scores:
[       nan 1.         0.9977221  0.99343545 0.96363636 0.94736842
 1.         1.         0.99742931 1.         1.         1.
 1.         1.        ]

Kappa:
0.9954899447937053
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0c88779860>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.268, val_acc:0.555]
Epoch [2/120    avg_loss:1.698, val_acc:0.664]
Epoch [3/120    avg_loss:1.238, val_acc:0.715]
Epoch [4/120    avg_loss:1.026, val_acc:0.818]
Epoch [5/120    avg_loss:0.833, val_acc:0.881]
Epoch [6/120    avg_loss:0.828, val_acc:0.869]
Epoch [7/120    avg_loss:0.646, val_acc:0.887]
Epoch [8/120    avg_loss:0.590, val_acc:0.879]
Epoch [9/120    avg_loss:0.613, val_acc:0.869]
Epoch [10/120    avg_loss:0.478, val_acc:0.865]
Epoch [11/120    avg_loss:0.476, val_acc:0.887]
Epoch [12/120    avg_loss:0.445, val_acc:0.904]
Epoch [13/120    avg_loss:0.375, val_acc:0.893]
Epoch [14/120    avg_loss:0.417, val_acc:0.898]
Epoch [15/120    avg_loss:0.419, val_acc:0.887]
Epoch [16/120    avg_loss:0.502, val_acc:0.889]
Epoch [17/120    avg_loss:0.436, val_acc:0.889]
Epoch [18/120    avg_loss:0.424, val_acc:0.883]
Epoch [19/120    avg_loss:0.382, val_acc:0.916]
Epoch [20/120    avg_loss:0.333, val_acc:0.916]
Epoch [21/120    avg_loss:0.358, val_acc:0.910]
Epoch [22/120    avg_loss:0.341, val_acc:0.877]
Epoch [23/120    avg_loss:0.357, val_acc:0.902]
Epoch [24/120    avg_loss:0.375, val_acc:0.902]
Epoch [25/120    avg_loss:0.343, val_acc:0.893]
Epoch [26/120    avg_loss:0.353, val_acc:0.918]
Epoch [27/120    avg_loss:0.304, val_acc:0.920]
Epoch [28/120    avg_loss:0.298, val_acc:0.920]
Epoch [29/120    avg_loss:0.274, val_acc:0.930]
Epoch [30/120    avg_loss:0.260, val_acc:0.928]
Epoch [31/120    avg_loss:0.367, val_acc:0.908]
Epoch [32/120    avg_loss:0.375, val_acc:0.926]
Epoch [33/120    avg_loss:0.275, val_acc:0.930]
Epoch [34/120    avg_loss:0.202, val_acc:0.928]
Epoch [35/120    avg_loss:0.192, val_acc:0.926]
Epoch [36/120    avg_loss:0.237, val_acc:0.941]
Epoch [37/120    avg_loss:0.211, val_acc:0.939]
Epoch [38/120    avg_loss:0.310, val_acc:0.912]
Epoch [39/120    avg_loss:0.217, val_acc:0.932]
Epoch [40/120    avg_loss:0.202, val_acc:0.949]
Epoch [41/120    avg_loss:0.179, val_acc:0.930]
Epoch [42/120    avg_loss:0.234, val_acc:0.928]
Epoch [43/120    avg_loss:0.171, val_acc:0.945]
Epoch [44/120    avg_loss:0.164, val_acc:0.953]
Epoch [45/120    avg_loss:0.182, val_acc:0.963]
Epoch [46/120    avg_loss:0.155, val_acc:0.910]
Epoch [47/120    avg_loss:0.196, val_acc:0.926]
Epoch [48/120    avg_loss:0.120, val_acc:0.957]
Epoch [49/120    avg_loss:0.156, val_acc:0.938]
Epoch [50/120    avg_loss:0.157, val_acc:0.941]
Epoch [51/120    avg_loss:0.131, val_acc:0.926]
Epoch [52/120    avg_loss:0.141, val_acc:0.938]
Epoch [53/120    avg_loss:0.139, val_acc:0.953]
Epoch [54/120    avg_loss:0.133, val_acc:0.941]
Epoch [55/120    avg_loss:0.086, val_acc:0.961]
Epoch [56/120    avg_loss:0.089, val_acc:0.967]
Epoch [57/120    avg_loss:0.092, val_acc:0.969]
Epoch [58/120    avg_loss:0.079, val_acc:0.961]
Epoch [59/120    avg_loss:0.105, val_acc:0.971]
Epoch [60/120    avg_loss:0.097, val_acc:0.965]
Epoch [61/120    avg_loss:0.087, val_acc:0.955]
Epoch [62/120    avg_loss:0.073, val_acc:0.963]
Epoch [63/120    avg_loss:0.073, val_acc:0.965]
Epoch [64/120    avg_loss:0.058, val_acc:0.967]
Epoch [65/120    avg_loss:0.094, val_acc:0.959]
Epoch [66/120    avg_loss:0.060, val_acc:0.955]
Epoch [67/120    avg_loss:0.061, val_acc:0.959]
Epoch [68/120    avg_loss:0.076, val_acc:0.943]
Epoch [69/120    avg_loss:0.068, val_acc:0.975]
Epoch [70/120    avg_loss:0.073, val_acc:0.969]
Epoch [71/120    avg_loss:0.076, val_acc:0.945]
Epoch [72/120    avg_loss:0.061, val_acc:0.977]
Epoch [73/120    avg_loss:0.068, val_acc:0.971]
Epoch [74/120    avg_loss:0.070, val_acc:0.973]
Epoch [75/120    avg_loss:0.070, val_acc:0.959]
Epoch [76/120    avg_loss:0.052, val_acc:0.963]
Epoch [77/120    avg_loss:0.044, val_acc:0.963]
Epoch [78/120    avg_loss:0.078, val_acc:0.953]
Epoch [79/120    avg_loss:0.127, val_acc:0.975]
Epoch [80/120    avg_loss:0.058, val_acc:0.979]
Epoch [81/120    avg_loss:0.039, val_acc:0.961]
Epoch [82/120    avg_loss:0.048, val_acc:0.971]
Epoch [83/120    avg_loss:0.059, val_acc:0.977]
Epoch [84/120    avg_loss:0.048, val_acc:0.975]
Epoch [85/120    avg_loss:0.030, val_acc:0.969]
Epoch [86/120    avg_loss:0.034, val_acc:0.975]
Epoch [87/120    avg_loss:0.061, val_acc:0.971]
Epoch [88/120    avg_loss:0.074, val_acc:0.967]
Epoch [89/120    avg_loss:0.033, val_acc:0.957]
Epoch [90/120    avg_loss:0.062, val_acc:0.975]
Epoch [91/120    avg_loss:0.056, val_acc:0.955]
Epoch [92/120    avg_loss:0.076, val_acc:0.914]
Epoch [93/120    avg_loss:0.041, val_acc:0.975]
Epoch [94/120    avg_loss:0.024, val_acc:0.975]
Epoch [95/120    avg_loss:0.021, val_acc:0.973]
Epoch [96/120    avg_loss:0.018, val_acc:0.975]
Epoch [97/120    avg_loss:0.017, val_acc:0.973]
Epoch [98/120    avg_loss:0.018, val_acc:0.975]
Epoch [99/120    avg_loss:0.015, val_acc:0.971]
Epoch [100/120    avg_loss:0.015, val_acc:0.973]
Epoch [101/120    avg_loss:0.018, val_acc:0.973]
Epoch [102/120    avg_loss:0.018, val_acc:0.973]
Epoch [103/120    avg_loss:0.016, val_acc:0.973]
Epoch [104/120    avg_loss:0.014, val_acc:0.971]
Epoch [105/120    avg_loss:0.015, val_acc:0.971]
Epoch [106/120    avg_loss:0.015, val_acc:0.971]
Epoch [107/120    avg_loss:0.014, val_acc:0.971]
Epoch [108/120    avg_loss:0.013, val_acc:0.971]
Epoch [109/120    avg_loss:0.012, val_acc:0.971]
Epoch [110/120    avg_loss:0.016, val_acc:0.971]
Epoch [111/120    avg_loss:0.014, val_acc:0.971]
Epoch [112/120    avg_loss:0.015, val_acc:0.971]
Epoch [113/120    avg_loss:0.017, val_acc:0.971]
Epoch [114/120    avg_loss:0.017, val_acc:0.971]
Epoch [115/120    avg_loss:0.017, val_acc:0.973]
Epoch [116/120    avg_loss:0.016, val_acc:0.971]
Epoch [117/120    avg_loss:0.016, val_acc:0.971]
Epoch [118/120    avg_loss:0.017, val_acc:0.971]
Epoch [119/120    avg_loss:0.010, val_acc:0.971]
Epoch [120/120    avg_loss:0.013, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 224   3   0   0   0   2   1   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   2  15 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   8 445   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.91257995735607

F1 scores:
[       nan 1.         0.98642534 0.98245614 0.930131   0.89198606
 1.         0.9673913  0.99742931 0.99893276 1.         0.98950131
 0.99109131 1.        ]

Kappa:
0.9878931235989133
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f696db07860>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.233, val_acc:0.492]
Epoch [2/120    avg_loss:1.657, val_acc:0.619]
Epoch [3/120    avg_loss:1.292, val_acc:0.641]
Epoch [4/120    avg_loss:1.075, val_acc:0.768]
Epoch [5/120    avg_loss:0.945, val_acc:0.768]
Epoch [6/120    avg_loss:0.831, val_acc:0.756]
Epoch [7/120    avg_loss:0.773, val_acc:0.793]
Epoch [8/120    avg_loss:0.671, val_acc:0.824]
Epoch [9/120    avg_loss:0.642, val_acc:0.816]
Epoch [10/120    avg_loss:0.678, val_acc:0.758]
Epoch [11/120    avg_loss:0.544, val_acc:0.855]
Epoch [12/120    avg_loss:0.541, val_acc:0.820]
Epoch [13/120    avg_loss:0.599, val_acc:0.881]
Epoch [14/120    avg_loss:0.480, val_acc:0.840]
Epoch [15/120    avg_loss:0.508, val_acc:0.881]
Epoch [16/120    avg_loss:0.481, val_acc:0.895]
Epoch [17/120    avg_loss:0.472, val_acc:0.914]
Epoch [18/120    avg_loss:0.368, val_acc:0.910]
Epoch [19/120    avg_loss:0.407, val_acc:0.879]
Epoch [20/120    avg_loss:0.361, val_acc:0.904]
Epoch [21/120    avg_loss:0.408, val_acc:0.910]
Epoch [22/120    avg_loss:0.378, val_acc:0.926]
Epoch [23/120    avg_loss:0.354, val_acc:0.918]
Epoch [24/120    avg_loss:0.314, val_acc:0.938]
Epoch [25/120    avg_loss:0.267, val_acc:0.947]
Epoch [26/120    avg_loss:0.277, val_acc:0.926]
Epoch [27/120    avg_loss:0.284, val_acc:0.955]
Epoch [28/120    avg_loss:0.257, val_acc:0.939]
Epoch [29/120    avg_loss:0.369, val_acc:0.924]
Epoch [30/120    avg_loss:0.394, val_acc:0.863]
Epoch [31/120    avg_loss:0.348, val_acc:0.938]
Epoch [32/120    avg_loss:0.285, val_acc:0.949]
Epoch [33/120    avg_loss:0.268, val_acc:0.938]
Epoch [34/120    avg_loss:0.243, val_acc:0.939]
Epoch [35/120    avg_loss:0.257, val_acc:0.963]
Epoch [36/120    avg_loss:0.233, val_acc:0.928]
Epoch [37/120    avg_loss:0.203, val_acc:0.934]
Epoch [38/120    avg_loss:0.263, val_acc:0.932]
Epoch [39/120    avg_loss:0.211, val_acc:0.965]
Epoch [40/120    avg_loss:0.175, val_acc:0.949]
Epoch [41/120    avg_loss:0.179, val_acc:0.932]
Epoch [42/120    avg_loss:0.155, val_acc:0.949]
Epoch [43/120    avg_loss:0.257, val_acc:0.916]
Epoch [44/120    avg_loss:0.274, val_acc:0.959]
Epoch [45/120    avg_loss:0.172, val_acc:0.963]
Epoch [46/120    avg_loss:0.158, val_acc:0.934]
Epoch [47/120    avg_loss:0.168, val_acc:0.957]
Epoch [48/120    avg_loss:0.123, val_acc:0.955]
Epoch [49/120    avg_loss:0.182, val_acc:0.955]
Epoch [50/120    avg_loss:0.195, val_acc:0.957]
Epoch [51/120    avg_loss:0.153, val_acc:0.951]
Epoch [52/120    avg_loss:0.137, val_acc:0.969]
Epoch [53/120    avg_loss:0.189, val_acc:0.951]
Epoch [54/120    avg_loss:0.133, val_acc:0.957]
Epoch [55/120    avg_loss:0.151, val_acc:0.938]
Epoch [56/120    avg_loss:0.145, val_acc:0.980]
Epoch [57/120    avg_loss:0.077, val_acc:0.982]
Epoch [58/120    avg_loss:0.072, val_acc:0.967]
Epoch [59/120    avg_loss:0.103, val_acc:0.986]
Epoch [60/120    avg_loss:0.299, val_acc:0.904]
Epoch [61/120    avg_loss:0.135, val_acc:0.957]
Epoch [62/120    avg_loss:0.114, val_acc:0.951]
Epoch [63/120    avg_loss:0.116, val_acc:0.965]
Epoch [64/120    avg_loss:0.074, val_acc:0.967]
Epoch [65/120    avg_loss:0.059, val_acc:0.969]
Epoch [66/120    avg_loss:0.199, val_acc:0.953]
Epoch [67/120    avg_loss:0.147, val_acc:0.949]
Epoch [68/120    avg_loss:0.131, val_acc:0.928]
Epoch [69/120    avg_loss:0.104, val_acc:0.965]
Epoch [70/120    avg_loss:0.122, val_acc:0.967]
Epoch [71/120    avg_loss:0.141, val_acc:0.959]
Epoch [72/120    avg_loss:0.077, val_acc:0.973]
Epoch [73/120    avg_loss:0.059, val_acc:0.975]
Epoch [74/120    avg_loss:0.063, val_acc:0.979]
Epoch [75/120    avg_loss:0.048, val_acc:0.977]
Epoch [76/120    avg_loss:0.046, val_acc:0.979]
Epoch [77/120    avg_loss:0.034, val_acc:0.980]
Epoch [78/120    avg_loss:0.041, val_acc:0.982]
Epoch [79/120    avg_loss:0.037, val_acc:0.980]
Epoch [80/120    avg_loss:0.043, val_acc:0.980]
Epoch [81/120    avg_loss:0.047, val_acc:0.982]
Epoch [82/120    avg_loss:0.041, val_acc:0.982]
Epoch [83/120    avg_loss:0.034, val_acc:0.984]
Epoch [84/120    avg_loss:0.041, val_acc:0.984]
Epoch [85/120    avg_loss:0.045, val_acc:0.982]
Epoch [86/120    avg_loss:0.039, val_acc:0.982]
Epoch [87/120    avg_loss:0.045, val_acc:0.982]
Epoch [88/120    avg_loss:0.044, val_acc:0.982]
Epoch [89/120    avg_loss:0.030, val_acc:0.982]
Epoch [90/120    avg_loss:0.033, val_acc:0.982]
Epoch [91/120    avg_loss:0.032, val_acc:0.982]
Epoch [92/120    avg_loss:0.029, val_acc:0.982]
Epoch [93/120    avg_loss:0.032, val_acc:0.982]
Epoch [94/120    avg_loss:0.035, val_acc:0.982]
Epoch [95/120    avg_loss:0.033, val_acc:0.982]
Epoch [96/120    avg_loss:0.037, val_acc:0.982]
Epoch [97/120    avg_loss:0.030, val_acc:0.982]
Epoch [98/120    avg_loss:0.042, val_acc:0.982]
Epoch [99/120    avg_loss:0.040, val_acc:0.982]
Epoch [100/120    avg_loss:0.039, val_acc:0.982]
Epoch [101/120    avg_loss:0.031, val_acc:0.982]
Epoch [102/120    avg_loss:0.034, val_acc:0.982]
Epoch [103/120    avg_loss:0.031, val_acc:0.982]
Epoch [104/120    avg_loss:0.037, val_acc:0.982]
Epoch [105/120    avg_loss:0.034, val_acc:0.982]
Epoch [106/120    avg_loss:0.034, val_acc:0.982]
Epoch [107/120    avg_loss:0.048, val_acc:0.982]
Epoch [108/120    avg_loss:0.037, val_acc:0.982]
Epoch [109/120    avg_loss:0.036, val_acc:0.982]
Epoch [110/120    avg_loss:0.033, val_acc:0.982]
Epoch [111/120    avg_loss:0.036, val_acc:0.982]
Epoch [112/120    avg_loss:0.030, val_acc:0.982]
Epoch [113/120    avg_loss:0.038, val_acc:0.982]
Epoch [114/120    avg_loss:0.045, val_acc:0.982]
Epoch [115/120    avg_loss:0.039, val_acc:0.982]
Epoch [116/120    avg_loss:0.035, val_acc:0.982]
Epoch [117/120    avg_loss:0.028, val_acc:0.982]
Epoch [118/120    avg_loss:0.035, val_acc:0.982]
Epoch [119/120    avg_loss:0.025, val_acc:0.982]
Epoch [120/120    avg_loss:0.039, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 209  21   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 210  17   0   0   0   0   0   0   0   0]
 [  0   0   0   1   8 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
98.8272921108742

F1 scores:
[       nan 1.         0.98206278 0.95       0.90128755 0.91275168
 1.         0.95555556 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.986943832310925
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:21:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa8ad95c860>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.264, val_acc:0.627]
Epoch [2/120    avg_loss:1.676, val_acc:0.694]
Epoch [3/120    avg_loss:1.267, val_acc:0.806]
Epoch [4/120    avg_loss:0.982, val_acc:0.859]
Epoch [5/120    avg_loss:0.862, val_acc:0.847]
Epoch [6/120    avg_loss:0.718, val_acc:0.849]
Epoch [7/120    avg_loss:0.645, val_acc:0.823]
Epoch [8/120    avg_loss:0.643, val_acc:0.839]
Epoch [9/120    avg_loss:0.589, val_acc:0.903]
Epoch [10/120    avg_loss:0.505, val_acc:0.879]
Epoch [11/120    avg_loss:0.531, val_acc:0.921]
Epoch [12/120    avg_loss:0.492, val_acc:0.885]
Epoch [13/120    avg_loss:0.434, val_acc:0.903]
Epoch [14/120    avg_loss:0.382, val_acc:0.925]
Epoch [15/120    avg_loss:0.339, val_acc:0.903]
Epoch [16/120    avg_loss:0.312, val_acc:0.927]
Epoch [17/120    avg_loss:0.358, val_acc:0.931]
Epoch [18/120    avg_loss:0.277, val_acc:0.942]
Epoch [19/120    avg_loss:0.368, val_acc:0.927]
Epoch [20/120    avg_loss:0.288, val_acc:0.907]
Epoch [21/120    avg_loss:0.297, val_acc:0.944]
Epoch [22/120    avg_loss:0.276, val_acc:0.948]
Epoch [23/120    avg_loss:0.229, val_acc:0.946]
Epoch [24/120    avg_loss:0.263, val_acc:0.925]
Epoch [25/120    avg_loss:0.295, val_acc:0.935]
Epoch [26/120    avg_loss:0.255, val_acc:0.940]
Epoch [27/120    avg_loss:0.211, val_acc:0.968]
Epoch [28/120    avg_loss:0.194, val_acc:0.966]
Epoch [29/120    avg_loss:0.211, val_acc:0.946]
Epoch [30/120    avg_loss:0.276, val_acc:0.940]
Epoch [31/120    avg_loss:0.215, val_acc:0.966]
Epoch [32/120    avg_loss:0.194, val_acc:0.958]
Epoch [33/120    avg_loss:0.192, val_acc:0.954]
Epoch [34/120    avg_loss:0.166, val_acc:0.968]
Epoch [35/120    avg_loss:0.165, val_acc:0.952]
Epoch [36/120    avg_loss:0.160, val_acc:0.966]
Epoch [37/120    avg_loss:0.163, val_acc:0.956]
Epoch [38/120    avg_loss:0.127, val_acc:0.968]
Epoch [39/120    avg_loss:0.178, val_acc:0.964]
Epoch [40/120    avg_loss:0.147, val_acc:0.980]
Epoch [41/120    avg_loss:0.147, val_acc:0.974]
Epoch [42/120    avg_loss:0.108, val_acc:0.962]
Epoch [43/120    avg_loss:0.129, val_acc:0.968]
Epoch [44/120    avg_loss:0.104, val_acc:0.972]
Epoch [45/120    avg_loss:0.100, val_acc:0.970]
Epoch [46/120    avg_loss:0.084, val_acc:0.974]
Epoch [47/120    avg_loss:0.083, val_acc:0.982]
Epoch [48/120    avg_loss:0.069, val_acc:0.972]
Epoch [49/120    avg_loss:0.079, val_acc:0.966]
Epoch [50/120    avg_loss:0.164, val_acc:0.938]
Epoch [51/120    avg_loss:0.176, val_acc:0.970]
Epoch [52/120    avg_loss:0.163, val_acc:0.931]
Epoch [53/120    avg_loss:0.136, val_acc:0.968]
Epoch [54/120    avg_loss:0.103, val_acc:0.984]
Epoch [55/120    avg_loss:0.062, val_acc:0.980]
Epoch [56/120    avg_loss:0.131, val_acc:0.982]
Epoch [57/120    avg_loss:0.089, val_acc:0.980]
Epoch [58/120    avg_loss:0.067, val_acc:0.980]
Epoch [59/120    avg_loss:0.075, val_acc:0.929]
Epoch [60/120    avg_loss:0.079, val_acc:0.978]
Epoch [61/120    avg_loss:0.069, val_acc:0.978]
Epoch [62/120    avg_loss:0.042, val_acc:0.978]
Epoch [63/120    avg_loss:0.065, val_acc:0.970]
Epoch [64/120    avg_loss:0.045, val_acc:0.980]
Epoch [65/120    avg_loss:0.052, val_acc:0.974]
Epoch [66/120    avg_loss:0.050, val_acc:0.982]
Epoch [67/120    avg_loss:0.089, val_acc:0.964]
Epoch [68/120    avg_loss:0.059, val_acc:0.976]
Epoch [69/120    avg_loss:0.038, val_acc:0.984]
Epoch [70/120    avg_loss:0.030, val_acc:0.982]
Epoch [71/120    avg_loss:0.020, val_acc:0.980]
Epoch [72/120    avg_loss:0.029, val_acc:0.982]
Epoch [73/120    avg_loss:0.029, val_acc:0.980]
Epoch [74/120    avg_loss:0.026, val_acc:0.982]
Epoch [75/120    avg_loss:0.024, val_acc:0.984]
Epoch [76/120    avg_loss:0.026, val_acc:0.984]
Epoch [77/120    avg_loss:0.026, val_acc:0.984]
Epoch [78/120    avg_loss:0.018, val_acc:0.982]
Epoch [79/120    avg_loss:0.023, val_acc:0.982]
Epoch [80/120    avg_loss:0.024, val_acc:0.982]
Epoch [81/120    avg_loss:0.022, val_acc:0.984]
Epoch [82/120    avg_loss:0.030, val_acc:0.984]
Epoch [83/120    avg_loss:0.021, val_acc:0.984]
Epoch [84/120    avg_loss:0.018, val_acc:0.984]
Epoch [85/120    avg_loss:0.021, val_acc:0.984]
Epoch [86/120    avg_loss:0.019, val_acc:0.984]
Epoch [87/120    avg_loss:0.023, val_acc:0.984]
Epoch [88/120    avg_loss:0.017, val_acc:0.986]
Epoch [89/120    avg_loss:0.021, val_acc:0.986]
Epoch [90/120    avg_loss:0.017, val_acc:0.986]
Epoch [91/120    avg_loss:0.019, val_acc:0.986]
Epoch [92/120    avg_loss:0.022, val_acc:0.986]
Epoch [93/120    avg_loss:0.021, val_acc:0.988]
Epoch [94/120    avg_loss:0.014, val_acc:0.986]
Epoch [95/120    avg_loss:0.013, val_acc:0.986]
Epoch [96/120    avg_loss:0.019, val_acc:0.984]
Epoch [97/120    avg_loss:0.017, val_acc:0.986]
Epoch [98/120    avg_loss:0.017, val_acc:0.986]
Epoch [99/120    avg_loss:0.015, val_acc:0.986]
Epoch [100/120    avg_loss:0.013, val_acc:0.984]
Epoch [101/120    avg_loss:0.019, val_acc:0.984]
Epoch [102/120    avg_loss:0.019, val_acc:0.986]
Epoch [103/120    avg_loss:0.017, val_acc:0.986]
Epoch [104/120    avg_loss:0.015, val_acc:0.986]
Epoch [105/120    avg_loss:0.016, val_acc:0.986]
Epoch [106/120    avg_loss:0.015, val_acc:0.984]
Epoch [107/120    avg_loss:0.013, val_acc:0.984]
Epoch [108/120    avg_loss:0.019, val_acc:0.984]
Epoch [109/120    avg_loss:0.019, val_acc:0.984]
Epoch [110/120    avg_loss:0.012, val_acc:0.984]
Epoch [111/120    avg_loss:0.015, val_acc:0.984]
Epoch [112/120    avg_loss:0.019, val_acc:0.984]
Epoch [113/120    avg_loss:0.014, val_acc:0.984]
Epoch [114/120    avg_loss:0.013, val_acc:0.984]
Epoch [115/120    avg_loss:0.019, val_acc:0.984]
Epoch [116/120    avg_loss:0.015, val_acc:0.984]
Epoch [117/120    avg_loss:0.016, val_acc:0.984]
Epoch [118/120    avg_loss:0.017, val_acc:0.986]
Epoch [119/120    avg_loss:0.015, val_acc:0.984]
Epoch [120/120    avg_loss:0.014, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 1.         0.98648649 0.98678414 0.94222222 0.93333333
 1.         0.96703297 1.         1.         1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9921664770161489
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff0c29147f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.313, val_acc:0.567]
Epoch [2/120    avg_loss:1.730, val_acc:0.696]
Epoch [3/120    avg_loss:1.352, val_acc:0.720]
Epoch [4/120    avg_loss:1.038, val_acc:0.756]
Epoch [5/120    avg_loss:0.863, val_acc:0.837]
Epoch [6/120    avg_loss:0.814, val_acc:0.823]
Epoch [7/120    avg_loss:0.742, val_acc:0.774]
Epoch [8/120    avg_loss:0.663, val_acc:0.867]
Epoch [9/120    avg_loss:0.620, val_acc:0.869]
Epoch [10/120    avg_loss:0.592, val_acc:0.883]
Epoch [11/120    avg_loss:0.608, val_acc:0.829]
Epoch [12/120    avg_loss:0.540, val_acc:0.891]
Epoch [13/120    avg_loss:0.417, val_acc:0.887]
Epoch [14/120    avg_loss:0.489, val_acc:0.919]
Epoch [15/120    avg_loss:0.526, val_acc:0.873]
Epoch [16/120    avg_loss:0.402, val_acc:0.897]
Epoch [17/120    avg_loss:0.388, val_acc:0.929]
Epoch [18/120    avg_loss:0.396, val_acc:0.877]
Epoch [19/120    avg_loss:0.341, val_acc:0.923]
Epoch [20/120    avg_loss:0.432, val_acc:0.923]
Epoch [21/120    avg_loss:0.384, val_acc:0.907]
Epoch [22/120    avg_loss:0.392, val_acc:0.929]
Epoch [23/120    avg_loss:0.342, val_acc:0.869]
Epoch [24/120    avg_loss:0.368, val_acc:0.942]
Epoch [25/120    avg_loss:0.285, val_acc:0.942]
Epoch [26/120    avg_loss:0.326, val_acc:0.946]
Epoch [27/120    avg_loss:0.318, val_acc:0.923]
Epoch [28/120    avg_loss:0.293, val_acc:0.917]
Epoch [29/120    avg_loss:0.297, val_acc:0.938]
Epoch [30/120    avg_loss:0.249, val_acc:0.958]
Epoch [31/120    avg_loss:0.270, val_acc:0.950]
Epoch [32/120    avg_loss:0.227, val_acc:0.954]
Epoch [33/120    avg_loss:0.235, val_acc:0.940]
Epoch [34/120    avg_loss:0.187, val_acc:0.964]
Epoch [35/120    avg_loss:0.188, val_acc:0.954]
Epoch [36/120    avg_loss:0.267, val_acc:0.935]
Epoch [37/120    avg_loss:0.146, val_acc:0.954]
Epoch [38/120    avg_loss:0.186, val_acc:0.935]
Epoch [39/120    avg_loss:0.188, val_acc:0.950]
Epoch [40/120    avg_loss:0.186, val_acc:0.946]
Epoch [41/120    avg_loss:0.208, val_acc:0.956]
Epoch [42/120    avg_loss:0.208, val_acc:0.927]
Epoch [43/120    avg_loss:0.108, val_acc:0.978]
Epoch [44/120    avg_loss:0.199, val_acc:0.954]
Epoch [45/120    avg_loss:0.225, val_acc:0.970]
Epoch [46/120    avg_loss:0.121, val_acc:0.958]
Epoch [47/120    avg_loss:0.138, val_acc:0.966]
Epoch [48/120    avg_loss:0.169, val_acc:0.954]
Epoch [49/120    avg_loss:0.116, val_acc:0.970]
Epoch [50/120    avg_loss:0.134, val_acc:0.944]
Epoch [51/120    avg_loss:0.141, val_acc:0.968]
Epoch [52/120    avg_loss:0.217, val_acc:0.958]
Epoch [53/120    avg_loss:0.157, val_acc:0.972]
Epoch [54/120    avg_loss:0.097, val_acc:0.970]
Epoch [55/120    avg_loss:0.084, val_acc:0.962]
Epoch [56/120    avg_loss:0.099, val_acc:0.978]
Epoch [57/120    avg_loss:0.098, val_acc:0.974]
Epoch [58/120    avg_loss:0.091, val_acc:0.978]
Epoch [59/120    avg_loss:0.116, val_acc:0.974]
Epoch [60/120    avg_loss:0.156, val_acc:0.960]
Epoch [61/120    avg_loss:0.246, val_acc:0.952]
Epoch [62/120    avg_loss:0.181, val_acc:0.944]
Epoch [63/120    avg_loss:0.088, val_acc:0.976]
Epoch [64/120    avg_loss:0.111, val_acc:0.972]
Epoch [65/120    avg_loss:0.098, val_acc:0.970]
Epoch [66/120    avg_loss:0.061, val_acc:0.976]
Epoch [67/120    avg_loss:0.055, val_acc:0.984]
Epoch [68/120    avg_loss:0.055, val_acc:0.986]
Epoch [69/120    avg_loss:0.042, val_acc:0.986]
Epoch [70/120    avg_loss:0.047, val_acc:0.990]
Epoch [71/120    avg_loss:0.077, val_acc:0.966]
Epoch [72/120    avg_loss:0.084, val_acc:0.970]
Epoch [73/120    avg_loss:0.042, val_acc:0.974]
Epoch [74/120    avg_loss:0.105, val_acc:0.968]
Epoch [75/120    avg_loss:0.054, val_acc:0.978]
Epoch [76/120    avg_loss:0.043, val_acc:0.986]
Epoch [77/120    avg_loss:0.046, val_acc:0.986]
Epoch [78/120    avg_loss:0.040, val_acc:0.986]
Epoch [79/120    avg_loss:0.029, val_acc:0.984]
Epoch [80/120    avg_loss:0.068, val_acc:0.968]
Epoch [81/120    avg_loss:0.076, val_acc:0.982]
Epoch [82/120    avg_loss:0.042, val_acc:0.986]
Epoch [83/120    avg_loss:0.076, val_acc:0.980]
Epoch [84/120    avg_loss:0.045, val_acc:0.990]
Epoch [85/120    avg_loss:0.040, val_acc:0.990]
Epoch [86/120    avg_loss:0.021, val_acc:0.994]
Epoch [87/120    avg_loss:0.020, val_acc:0.992]
Epoch [88/120    avg_loss:0.016, val_acc:0.992]
Epoch [89/120    avg_loss:0.025, val_acc:0.992]
Epoch [90/120    avg_loss:0.022, val_acc:0.992]
Epoch [91/120    avg_loss:0.018, val_acc:0.992]
Epoch [92/120    avg_loss:0.025, val_acc:0.994]
Epoch [93/120    avg_loss:0.023, val_acc:0.992]
Epoch [94/120    avg_loss:0.026, val_acc:0.992]
Epoch [95/120    avg_loss:0.021, val_acc:0.990]
Epoch [96/120    avg_loss:0.015, val_acc:0.992]
Epoch [97/120    avg_loss:0.025, val_acc:0.992]
Epoch [98/120    avg_loss:0.017, val_acc:0.992]
Epoch [99/120    avg_loss:0.023, val_acc:0.992]
Epoch [100/120    avg_loss:0.019, val_acc:0.990]
Epoch [101/120    avg_loss:0.014, val_acc:0.992]
Epoch [102/120    avg_loss:0.018, val_acc:0.992]
Epoch [103/120    avg_loss:0.014, val_acc:0.992]
Epoch [104/120    avg_loss:0.014, val_acc:0.992]
Epoch [105/120    avg_loss:0.012, val_acc:0.992]
Epoch [106/120    avg_loss:0.013, val_acc:0.992]
Epoch [107/120    avg_loss:0.014, val_acc:0.992]
Epoch [108/120    avg_loss:0.014, val_acc:0.992]
Epoch [109/120    avg_loss:0.018, val_acc:0.992]
Epoch [110/120    avg_loss:0.017, val_acc:0.992]
Epoch [111/120    avg_loss:0.012, val_acc:0.992]
Epoch [112/120    avg_loss:0.014, val_acc:0.992]
Epoch [113/120    avg_loss:0.014, val_acc:0.992]
Epoch [114/120    avg_loss:0.023, val_acc:0.992]
Epoch [115/120    avg_loss:0.020, val_acc:0.992]
Epoch [116/120    avg_loss:0.016, val_acc:0.992]
Epoch [117/120    avg_loss:0.014, val_acc:0.992]
Epoch [118/120    avg_loss:0.017, val_acc:0.992]
Epoch [119/120    avg_loss:0.016, val_acc:0.992]
Epoch [120/120    avg_loss:0.014, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 215   8   0   0   0   3   3   0   0   0   0]
 [  0   0   0   0 213  14   0   0   0   0   0   0   0   0]
 [  0   0   0   0  11 134   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.0405117270789

F1 scores:
[       nan 1.         0.98871332 0.96629213 0.92608696 0.91467577
 0.99756691 0.97826087 0.99614891 0.99680511 1.         1.
 1.         1.        ]

Kappa:
0.9893170624002002
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2181e7e828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.238, val_acc:0.508]
Epoch [2/120    avg_loss:1.671, val_acc:0.641]
Epoch [3/120    avg_loss:1.367, val_acc:0.738]
Epoch [4/120    avg_loss:1.065, val_acc:0.734]
Epoch [5/120    avg_loss:0.960, val_acc:0.808]
Epoch [6/120    avg_loss:0.827, val_acc:0.766]
Epoch [7/120    avg_loss:0.761, val_acc:0.782]
Epoch [8/120    avg_loss:0.705, val_acc:0.784]
Epoch [9/120    avg_loss:0.627, val_acc:0.831]
Epoch [10/120    avg_loss:0.577, val_acc:0.849]
Epoch [11/120    avg_loss:0.612, val_acc:0.847]
Epoch [12/120    avg_loss:0.524, val_acc:0.883]
Epoch [13/120    avg_loss:0.478, val_acc:0.823]
Epoch [14/120    avg_loss:0.490, val_acc:0.867]
Epoch [15/120    avg_loss:0.414, val_acc:0.885]
Epoch [16/120    avg_loss:0.490, val_acc:0.873]
Epoch [17/120    avg_loss:0.401, val_acc:0.909]
Epoch [18/120    avg_loss:0.479, val_acc:0.847]
Epoch [19/120    avg_loss:0.419, val_acc:0.867]
Epoch [20/120    avg_loss:0.375, val_acc:0.849]
Epoch [21/120    avg_loss:0.444, val_acc:0.909]
Epoch [22/120    avg_loss:0.325, val_acc:0.929]
Epoch [23/120    avg_loss:0.305, val_acc:0.909]
Epoch [24/120    avg_loss:0.298, val_acc:0.942]
Epoch [25/120    avg_loss:0.294, val_acc:0.917]
Epoch [26/120    avg_loss:0.331, val_acc:0.919]
Epoch [27/120    avg_loss:0.278, val_acc:0.946]
Epoch [28/120    avg_loss:0.257, val_acc:0.944]
Epoch [29/120    avg_loss:0.285, val_acc:0.929]
Epoch [30/120    avg_loss:0.257, val_acc:0.946]
Epoch [31/120    avg_loss:0.258, val_acc:0.942]
Epoch [32/120    avg_loss:0.249, val_acc:0.931]
Epoch [33/120    avg_loss:0.201, val_acc:0.960]
Epoch [34/120    avg_loss:0.190, val_acc:0.954]
Epoch [35/120    avg_loss:0.200, val_acc:0.915]
Epoch [36/120    avg_loss:0.252, val_acc:0.935]
Epoch [37/120    avg_loss:0.273, val_acc:0.885]
Epoch [38/120    avg_loss:0.242, val_acc:0.952]
Epoch [39/120    avg_loss:0.185, val_acc:0.948]
Epoch [40/120    avg_loss:0.150, val_acc:0.952]
Epoch [41/120    avg_loss:0.154, val_acc:0.931]
Epoch [42/120    avg_loss:0.173, val_acc:0.950]
Epoch [43/120    avg_loss:0.135, val_acc:0.948]
Epoch [44/120    avg_loss:0.127, val_acc:0.958]
Epoch [45/120    avg_loss:0.117, val_acc:0.958]
Epoch [46/120    avg_loss:0.174, val_acc:0.958]
Epoch [47/120    avg_loss:0.108, val_acc:0.960]
Epoch [48/120    avg_loss:0.094, val_acc:0.962]
Epoch [49/120    avg_loss:0.093, val_acc:0.962]
Epoch [50/120    avg_loss:0.113, val_acc:0.962]
Epoch [51/120    avg_loss:0.089, val_acc:0.964]
Epoch [52/120    avg_loss:0.076, val_acc:0.962]
Epoch [53/120    avg_loss:0.077, val_acc:0.964]
Epoch [54/120    avg_loss:0.080, val_acc:0.966]
Epoch [55/120    avg_loss:0.075, val_acc:0.968]
Epoch [56/120    avg_loss:0.063, val_acc:0.964]
Epoch [57/120    avg_loss:0.079, val_acc:0.966]
Epoch [58/120    avg_loss:0.060, val_acc:0.962]
Epoch [59/120    avg_loss:0.053, val_acc:0.964]
Epoch [60/120    avg_loss:0.068, val_acc:0.966]
Epoch [61/120    avg_loss:0.077, val_acc:0.966]
Epoch [62/120    avg_loss:0.058, val_acc:0.970]
Epoch [63/120    avg_loss:0.069, val_acc:0.974]
Epoch [64/120    avg_loss:0.052, val_acc:0.974]
Epoch [65/120    avg_loss:0.062, val_acc:0.972]
Epoch [66/120    avg_loss:0.064, val_acc:0.970]
Epoch [67/120    avg_loss:0.072, val_acc:0.968]
Epoch [68/120    avg_loss:0.046, val_acc:0.970]
Epoch [69/120    avg_loss:0.058, val_acc:0.970]
Epoch [70/120    avg_loss:0.070, val_acc:0.972]
Epoch [71/120    avg_loss:0.054, val_acc:0.974]
Epoch [72/120    avg_loss:0.044, val_acc:0.970]
Epoch [73/120    avg_loss:0.054, val_acc:0.972]
Epoch [74/120    avg_loss:0.071, val_acc:0.974]
Epoch [75/120    avg_loss:0.056, val_acc:0.974]
Epoch [76/120    avg_loss:0.042, val_acc:0.972]
Epoch [77/120    avg_loss:0.068, val_acc:0.972]
Epoch [78/120    avg_loss:0.044, val_acc:0.970]
Epoch [79/120    avg_loss:0.043, val_acc:0.972]
Epoch [80/120    avg_loss:0.038, val_acc:0.974]
Epoch [81/120    avg_loss:0.041, val_acc:0.974]
Epoch [82/120    avg_loss:0.048, val_acc:0.972]
Epoch [83/120    avg_loss:0.044, val_acc:0.968]
Epoch [84/120    avg_loss:0.045, val_acc:0.972]
Epoch [85/120    avg_loss:0.047, val_acc:0.970]
Epoch [86/120    avg_loss:0.059, val_acc:0.972]
Epoch [87/120    avg_loss:0.046, val_acc:0.972]
Epoch [88/120    avg_loss:0.045, val_acc:0.974]
Epoch [89/120    avg_loss:0.039, val_acc:0.974]
Epoch [90/120    avg_loss:0.056, val_acc:0.970]
Epoch [91/120    avg_loss:0.046, val_acc:0.970]
Epoch [92/120    avg_loss:0.046, val_acc:0.970]
Epoch [93/120    avg_loss:0.050, val_acc:0.970]
Epoch [94/120    avg_loss:0.048, val_acc:0.976]
Epoch [95/120    avg_loss:0.036, val_acc:0.976]
Epoch [96/120    avg_loss:0.040, val_acc:0.974]
Epoch [97/120    avg_loss:0.037, val_acc:0.972]
Epoch [98/120    avg_loss:0.046, val_acc:0.978]
Epoch [99/120    avg_loss:0.045, val_acc:0.972]
Epoch [100/120    avg_loss:0.047, val_acc:0.976]
Epoch [101/120    avg_loss:0.034, val_acc:0.974]
Epoch [102/120    avg_loss:0.037, val_acc:0.978]
Epoch [103/120    avg_loss:0.034, val_acc:0.980]
Epoch [104/120    avg_loss:0.042, val_acc:0.978]
Epoch [105/120    avg_loss:0.036, val_acc:0.978]
Epoch [106/120    avg_loss:0.075, val_acc:0.978]
Epoch [107/120    avg_loss:0.040, val_acc:0.974]
Epoch [108/120    avg_loss:0.041, val_acc:0.976]
Epoch [109/120    avg_loss:0.037, val_acc:0.974]
Epoch [110/120    avg_loss:0.045, val_acc:0.978]
Epoch [111/120    avg_loss:0.033, val_acc:0.974]
Epoch [112/120    avg_loss:0.044, val_acc:0.978]
Epoch [113/120    avg_loss:0.037, val_acc:0.972]
Epoch [114/120    avg_loss:0.042, val_acc:0.980]
Epoch [115/120    avg_loss:0.035, val_acc:0.976]
Epoch [116/120    avg_loss:0.031, val_acc:0.976]
Epoch [117/120    avg_loss:0.040, val_acc:0.980]
Epoch [118/120    avg_loss:0.033, val_acc:0.978]
Epoch [119/120    avg_loss:0.039, val_acc:0.982]
Epoch [120/120    avg_loss:0.029, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 206  21   0   0   0   0   0   0   0   0]
 [  0   0   0   0   8 137   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1   0 205   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 1.         1.         0.98678414 0.91964286 0.90429043
 0.99756691 1.         1.         1.         1.         0.99602649
 0.99668508 1.        ]

Kappa:
0.9907426569470384
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb0f78ce828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.244, val_acc:0.510]
Epoch [2/120    avg_loss:1.647, val_acc:0.637]
Epoch [3/120    avg_loss:1.273, val_acc:0.728]
Epoch [4/120    avg_loss:1.049, val_acc:0.790]
Epoch [5/120    avg_loss:0.924, val_acc:0.792]
Epoch [6/120    avg_loss:0.867, val_acc:0.817]
Epoch [7/120    avg_loss:0.783, val_acc:0.835]
Epoch [8/120    avg_loss:0.648, val_acc:0.899]
Epoch [9/120    avg_loss:0.654, val_acc:0.837]
Epoch [10/120    avg_loss:0.579, val_acc:0.863]
Epoch [11/120    avg_loss:0.544, val_acc:0.893]
Epoch [12/120    avg_loss:0.545, val_acc:0.895]
Epoch [13/120    avg_loss:0.486, val_acc:0.917]
Epoch [14/120    avg_loss:0.448, val_acc:0.881]
Epoch [15/120    avg_loss:0.454, val_acc:0.925]
Epoch [16/120    avg_loss:0.397, val_acc:0.877]
Epoch [17/120    avg_loss:0.333, val_acc:0.899]
Epoch [18/120    avg_loss:0.364, val_acc:0.905]
Epoch [19/120    avg_loss:0.399, val_acc:0.915]
Epoch [20/120    avg_loss:0.407, val_acc:0.935]
Epoch [21/120    avg_loss:0.310, val_acc:0.887]
Epoch [22/120    avg_loss:0.311, val_acc:0.927]
Epoch [23/120    avg_loss:0.265, val_acc:0.927]
Epoch [24/120    avg_loss:0.264, val_acc:0.933]
Epoch [25/120    avg_loss:0.219, val_acc:0.944]
Epoch [26/120    avg_loss:0.308, val_acc:0.919]
Epoch [27/120    avg_loss:0.254, val_acc:0.923]
Epoch [28/120    avg_loss:0.273, val_acc:0.925]
Epoch [29/120    avg_loss:0.203, val_acc:0.952]
Epoch [30/120    avg_loss:0.231, val_acc:0.942]
Epoch [31/120    avg_loss:0.318, val_acc:0.929]
Epoch [32/120    avg_loss:0.229, val_acc:0.954]
Epoch [33/120    avg_loss:0.169, val_acc:0.946]
Epoch [34/120    avg_loss:0.145, val_acc:0.950]
Epoch [35/120    avg_loss:0.160, val_acc:0.935]
Epoch [36/120    avg_loss:0.194, val_acc:0.956]
Epoch [37/120    avg_loss:0.184, val_acc:0.923]
Epoch [38/120    avg_loss:0.306, val_acc:0.915]
Epoch [39/120    avg_loss:0.187, val_acc:0.962]
Epoch [40/120    avg_loss:0.168, val_acc:0.940]
Epoch [41/120    avg_loss:0.183, val_acc:0.946]
Epoch [42/120    avg_loss:0.154, val_acc:0.950]
Epoch [43/120    avg_loss:0.145, val_acc:0.970]
Epoch [44/120    avg_loss:0.111, val_acc:0.960]
Epoch [45/120    avg_loss:0.121, val_acc:0.958]
Epoch [46/120    avg_loss:0.152, val_acc:0.940]
Epoch [47/120    avg_loss:0.133, val_acc:0.972]
Epoch [48/120    avg_loss:0.086, val_acc:0.931]
Epoch [49/120    avg_loss:0.110, val_acc:0.966]
Epoch [50/120    avg_loss:0.159, val_acc:0.972]
Epoch [51/120    avg_loss:0.102, val_acc:0.968]
Epoch [52/120    avg_loss:0.154, val_acc:0.954]
Epoch [53/120    avg_loss:0.131, val_acc:0.958]
Epoch [54/120    avg_loss:0.099, val_acc:0.960]
Epoch [55/120    avg_loss:0.130, val_acc:0.946]
Epoch [56/120    avg_loss:0.079, val_acc:0.966]
Epoch [57/120    avg_loss:0.086, val_acc:0.940]
Epoch [58/120    avg_loss:0.088, val_acc:0.976]
Epoch [59/120    avg_loss:0.108, val_acc:0.946]
Epoch [60/120    avg_loss:0.056, val_acc:0.966]
Epoch [61/120    avg_loss:0.066, val_acc:0.972]
Epoch [62/120    avg_loss:0.156, val_acc:0.917]
Epoch [63/120    avg_loss:0.186, val_acc:0.954]
Epoch [64/120    avg_loss:0.089, val_acc:0.960]
Epoch [65/120    avg_loss:0.072, val_acc:0.970]
Epoch [66/120    avg_loss:0.090, val_acc:0.956]
Epoch [67/120    avg_loss:0.064, val_acc:0.966]
Epoch [68/120    avg_loss:0.089, val_acc:0.956]
Epoch [69/120    avg_loss:0.080, val_acc:0.950]
Epoch [70/120    avg_loss:0.091, val_acc:0.964]
Epoch [71/120    avg_loss:0.051, val_acc:0.964]
Epoch [72/120    avg_loss:0.040, val_acc:0.972]
Epoch [73/120    avg_loss:0.042, val_acc:0.974]
Epoch [74/120    avg_loss:0.031, val_acc:0.974]
Epoch [75/120    avg_loss:0.029, val_acc:0.974]
Epoch [76/120    avg_loss:0.028, val_acc:0.978]
Epoch [77/120    avg_loss:0.022, val_acc:0.978]
Epoch [78/120    avg_loss:0.023, val_acc:0.978]
Epoch [79/120    avg_loss:0.033, val_acc:0.978]
Epoch [80/120    avg_loss:0.022, val_acc:0.978]
Epoch [81/120    avg_loss:0.021, val_acc:0.978]
Epoch [82/120    avg_loss:0.026, val_acc:0.978]
Epoch [83/120    avg_loss:0.031, val_acc:0.978]
Epoch [84/120    avg_loss:0.022, val_acc:0.978]
Epoch [85/120    avg_loss:0.021, val_acc:0.980]
Epoch [86/120    avg_loss:0.024, val_acc:0.980]
Epoch [87/120    avg_loss:0.019, val_acc:0.980]
Epoch [88/120    avg_loss:0.021, val_acc:0.980]
Epoch [89/120    avg_loss:0.028, val_acc:0.980]
Epoch [90/120    avg_loss:0.022, val_acc:0.980]
Epoch [91/120    avg_loss:0.024, val_acc:0.980]
Epoch [92/120    avg_loss:0.022, val_acc:0.980]
Epoch [93/120    avg_loss:0.028, val_acc:0.978]
Epoch [94/120    avg_loss:0.023, val_acc:0.978]
Epoch [95/120    avg_loss:0.020, val_acc:0.978]
Epoch [96/120    avg_loss:0.025, val_acc:0.980]
Epoch [97/120    avg_loss:0.023, val_acc:0.978]
Epoch [98/120    avg_loss:0.027, val_acc:0.980]
Epoch [99/120    avg_loss:0.017, val_acc:0.980]
Epoch [100/120    avg_loss:0.014, val_acc:0.978]
Epoch [101/120    avg_loss:0.018, val_acc:0.978]
Epoch [102/120    avg_loss:0.015, val_acc:0.978]
Epoch [103/120    avg_loss:0.018, val_acc:0.980]
Epoch [104/120    avg_loss:0.016, val_acc:0.980]
Epoch [105/120    avg_loss:0.020, val_acc:0.980]
Epoch [106/120    avg_loss:0.019, val_acc:0.980]
Epoch [107/120    avg_loss:0.020, val_acc:0.982]
Epoch [108/120    avg_loss:0.020, val_acc:0.980]
Epoch [109/120    avg_loss:0.023, val_acc:0.986]
Epoch [110/120    avg_loss:0.019, val_acc:0.984]
Epoch [111/120    avg_loss:0.024, val_acc:0.978]
Epoch [112/120    avg_loss:0.017, val_acc:0.980]
Epoch [113/120    avg_loss:0.031, val_acc:0.982]
Epoch [114/120    avg_loss:0.016, val_acc:0.978]
Epoch [115/120    avg_loss:0.024, val_acc:0.980]
Epoch [116/120    avg_loss:0.018, val_acc:0.980]
Epoch [117/120    avg_loss:0.022, val_acc:0.980]
Epoch [118/120    avg_loss:0.015, val_acc:0.980]
Epoch [119/120    avg_loss:0.014, val_acc:0.980]
Epoch [120/120    avg_loss:0.016, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 218   0   0   0   0   1   0   0   0   0   0   0]
 [  0   0   0 223   7   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  10   0   0   0   0   0   0   4   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.36034115138592

F1 scores:
[       nan 1.         0.98866213 0.98454746 0.94456763 0.9527027
 1.         0.97297297 1.         1.         1.         1.
 0.9956044  1.        ]

Kappa:
0.9928782802428612
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc73533f8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.260, val_acc:0.520]
Epoch [2/120    avg_loss:1.653, val_acc:0.679]
Epoch [3/120    avg_loss:1.305, val_acc:0.661]
Epoch [4/120    avg_loss:1.051, val_acc:0.770]
Epoch [5/120    avg_loss:0.892, val_acc:0.742]
Epoch [6/120    avg_loss:0.824, val_acc:0.823]
Epoch [7/120    avg_loss:0.719, val_acc:0.843]
Epoch [8/120    avg_loss:0.598, val_acc:0.780]
Epoch [9/120    avg_loss:0.575, val_acc:0.857]
Epoch [10/120    avg_loss:0.496, val_acc:0.877]
Epoch [11/120    avg_loss:0.483, val_acc:0.911]
Epoch [12/120    avg_loss:0.507, val_acc:0.907]
Epoch [13/120    avg_loss:0.529, val_acc:0.907]
Epoch [14/120    avg_loss:0.404, val_acc:0.935]
Epoch [15/120    avg_loss:0.382, val_acc:0.925]
Epoch [16/120    avg_loss:0.458, val_acc:0.917]
Epoch [17/120    avg_loss:0.406, val_acc:0.929]
Epoch [18/120    avg_loss:0.359, val_acc:0.907]
Epoch [19/120    avg_loss:0.351, val_acc:0.919]
Epoch [20/120    avg_loss:0.427, val_acc:0.911]
Epoch [21/120    avg_loss:0.329, val_acc:0.923]
Epoch [22/120    avg_loss:0.394, val_acc:0.942]
Epoch [23/120    avg_loss:0.326, val_acc:0.944]
Epoch [24/120    avg_loss:0.276, val_acc:0.946]
Epoch [25/120    avg_loss:0.255, val_acc:0.960]
Epoch [26/120    avg_loss:0.262, val_acc:0.950]
Epoch [27/120    avg_loss:0.249, val_acc:0.933]
Epoch [28/120    avg_loss:0.289, val_acc:0.948]
Epoch [29/120    avg_loss:0.220, val_acc:0.958]
Epoch [30/120    avg_loss:0.271, val_acc:0.948]
Epoch [31/120    avg_loss:0.260, val_acc:0.964]
Epoch [32/120    avg_loss:0.242, val_acc:0.966]
Epoch [33/120    avg_loss:0.213, val_acc:0.966]
Epoch [34/120    avg_loss:0.206, val_acc:0.958]
Epoch [35/120    avg_loss:0.225, val_acc:0.956]
Epoch [36/120    avg_loss:0.215, val_acc:0.958]
Epoch [37/120    avg_loss:0.227, val_acc:0.940]
Epoch [38/120    avg_loss:0.197, val_acc:0.966]
Epoch [39/120    avg_loss:0.132, val_acc:0.970]
Epoch [40/120    avg_loss:0.150, val_acc:0.976]
Epoch [41/120    avg_loss:0.274, val_acc:0.905]
Epoch [42/120    avg_loss:0.301, val_acc:0.962]
Epoch [43/120    avg_loss:0.209, val_acc:0.956]
Epoch [44/120    avg_loss:0.177, val_acc:0.970]
Epoch [45/120    avg_loss:0.129, val_acc:0.970]
Epoch [46/120    avg_loss:0.133, val_acc:0.960]
Epoch [47/120    avg_loss:0.123, val_acc:0.960]
Epoch [48/120    avg_loss:0.166, val_acc:0.980]
Epoch [49/120    avg_loss:0.178, val_acc:0.970]
Epoch [50/120    avg_loss:0.103, val_acc:0.962]
Epoch [51/120    avg_loss:0.235, val_acc:0.960]
Epoch [52/120    avg_loss:0.149, val_acc:0.982]
Epoch [53/120    avg_loss:0.101, val_acc:0.976]
Epoch [54/120    avg_loss:0.116, val_acc:0.984]
Epoch [55/120    avg_loss:0.135, val_acc:0.974]
Epoch [56/120    avg_loss:0.086, val_acc:0.970]
Epoch [57/120    avg_loss:0.124, val_acc:0.970]
Epoch [58/120    avg_loss:0.113, val_acc:0.976]
Epoch [59/120    avg_loss:0.140, val_acc:0.980]
Epoch [60/120    avg_loss:0.143, val_acc:0.966]
Epoch [61/120    avg_loss:0.090, val_acc:0.988]
Epoch [62/120    avg_loss:0.110, val_acc:0.972]
Epoch [63/120    avg_loss:0.101, val_acc:0.980]
Epoch [64/120    avg_loss:0.167, val_acc:0.980]
Epoch [65/120    avg_loss:0.137, val_acc:0.948]
Epoch [66/120    avg_loss:0.142, val_acc:0.980]
Epoch [67/120    avg_loss:0.077, val_acc:0.978]
Epoch [68/120    avg_loss:0.073, val_acc:0.982]
Epoch [69/120    avg_loss:0.088, val_acc:0.968]
Epoch [70/120    avg_loss:0.068, val_acc:0.966]
Epoch [71/120    avg_loss:0.119, val_acc:0.988]
Epoch [72/120    avg_loss:0.066, val_acc:0.990]
Epoch [73/120    avg_loss:0.048, val_acc:0.982]
Epoch [74/120    avg_loss:0.068, val_acc:0.986]
Epoch [75/120    avg_loss:0.120, val_acc:0.964]
Epoch [76/120    avg_loss:0.104, val_acc:0.980]
Epoch [77/120    avg_loss:0.056, val_acc:0.992]
Epoch [78/120    avg_loss:0.061, val_acc:0.992]
Epoch [79/120    avg_loss:0.060, val_acc:0.990]
Epoch [80/120    avg_loss:0.073, val_acc:0.982]
Epoch [81/120    avg_loss:0.051, val_acc:0.986]
Epoch [82/120    avg_loss:0.072, val_acc:0.988]
Epoch [83/120    avg_loss:0.033, val_acc:0.988]
Epoch [84/120    avg_loss:0.034, val_acc:0.990]
Epoch [85/120    avg_loss:0.037, val_acc:0.992]
Epoch [86/120    avg_loss:0.032, val_acc:0.994]
Epoch [87/120    avg_loss:0.029, val_acc:0.992]
Epoch [88/120    avg_loss:0.025, val_acc:0.994]
Epoch [89/120    avg_loss:0.027, val_acc:0.992]
Epoch [90/120    avg_loss:0.031, val_acc:0.988]
Epoch [91/120    avg_loss:0.131, val_acc:0.980]
Epoch [92/120    avg_loss:0.058, val_acc:0.986]
Epoch [93/120    avg_loss:0.122, val_acc:0.984]
Epoch [94/120    avg_loss:0.052, val_acc:0.988]
Epoch [95/120    avg_loss:0.057, val_acc:0.988]
Epoch [96/120    avg_loss:0.050, val_acc:0.988]
Epoch [97/120    avg_loss:0.027, val_acc:0.988]
Epoch [98/120    avg_loss:0.026, val_acc:0.990]
Epoch [99/120    avg_loss:0.033, val_acc:0.986]
Epoch [100/120    avg_loss:0.043, val_acc:0.978]
Epoch [101/120    avg_loss:0.063, val_acc:0.992]
Epoch [102/120    avg_loss:0.028, val_acc:0.990]
Epoch [103/120    avg_loss:0.017, val_acc:0.992]
Epoch [104/120    avg_loss:0.019, val_acc:0.992]
Epoch [105/120    avg_loss:0.024, val_acc:0.992]
Epoch [106/120    avg_loss:0.022, val_acc:0.992]
Epoch [107/120    avg_loss:0.022, val_acc:0.992]
Epoch [108/120    avg_loss:0.015, val_acc:0.992]
Epoch [109/120    avg_loss:0.020, val_acc:0.992]
Epoch [110/120    avg_loss:0.010, val_acc:0.992]
Epoch [111/120    avg_loss:0.019, val_acc:0.992]
Epoch [112/120    avg_loss:0.019, val_acc:0.992]
Epoch [113/120    avg_loss:0.018, val_acc:0.992]
Epoch [114/120    avg_loss:0.014, val_acc:0.992]
Epoch [115/120    avg_loss:0.014, val_acc:0.992]
Epoch [116/120    avg_loss:0.022, val_acc:0.992]
Epoch [117/120    avg_loss:0.015, val_acc:0.992]
Epoch [118/120    avg_loss:0.015, val_acc:0.992]
Epoch [119/120    avg_loss:0.022, val_acc:0.992]
Epoch [120/120    avg_loss:0.014, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 225   5   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0  15 130   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4   0 202   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 1.         0.97767857 0.98901099 0.92274678 0.90592334
 0.99019608 0.94382022 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9890795880574003
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4673e4f7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 76807==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.238, val_acc:0.490]
Epoch [2/120    avg_loss:1.619, val_acc:0.641]
Epoch [3/120    avg_loss:1.228, val_acc:0.704]
Epoch [4/120    avg_loss:0.986, val_acc:0.730]
Epoch [5/120    avg_loss:0.883, val_acc:0.732]
Epoch [6/120    avg_loss:0.742, val_acc:0.855]
Epoch [7/120    avg_loss:0.646, val_acc:0.859]
Epoch [8/120    avg_loss:0.661, val_acc:0.869]
Epoch [9/120    avg_loss:0.549, val_acc:0.879]
Epoch [10/120    avg_loss:0.586, val_acc:0.867]
Epoch [11/120    avg_loss:0.461, val_acc:0.893]
Epoch [12/120    avg_loss:0.439, val_acc:0.899]
Epoch [13/120    avg_loss:0.425, val_acc:0.911]
Epoch [14/120    avg_loss:0.425, val_acc:0.909]
Epoch [15/120    avg_loss:0.425, val_acc:0.873]
Epoch [16/120    avg_loss:0.450, val_acc:0.903]
Epoch [17/120    avg_loss:0.399, val_acc:0.857]
Epoch [18/120    avg_loss:0.435, val_acc:0.905]
Epoch [19/120    avg_loss:0.345, val_acc:0.899]
Epoch [20/120    avg_loss:0.387, val_acc:0.929]
Epoch [21/120    avg_loss:0.325, val_acc:0.925]
Epoch [22/120    avg_loss:0.275, val_acc:0.867]
Epoch [23/120    avg_loss:0.291, val_acc:0.915]
Epoch [24/120    avg_loss:0.351, val_acc:0.921]
Epoch [25/120    avg_loss:0.322, val_acc:0.942]
Epoch [26/120    avg_loss:0.342, val_acc:0.925]
Epoch [27/120    avg_loss:0.247, val_acc:0.921]
Epoch [28/120    avg_loss:0.243, val_acc:0.944]
Epoch [29/120    avg_loss:0.201, val_acc:0.948]
Epoch [30/120    avg_loss:0.202, val_acc:0.954]
Epoch [31/120    avg_loss:0.215, val_acc:0.907]
Epoch [32/120    avg_loss:0.277, val_acc:0.931]
Epoch [33/120    avg_loss:0.301, val_acc:0.938]
Epoch [34/120    avg_loss:0.216, val_acc:0.950]
Epoch [35/120    avg_loss:0.267, val_acc:0.954]
Epoch [36/120    avg_loss:0.224, val_acc:0.929]
Epoch [37/120    avg_loss:0.278, val_acc:0.944]
Epoch [38/120    avg_loss:0.219, val_acc:0.960]
Epoch [39/120    avg_loss:0.138, val_acc:0.968]
Epoch [40/120    avg_loss:0.148, val_acc:0.958]
Epoch [41/120    avg_loss:0.188, val_acc:0.923]
Epoch [42/120    avg_loss:0.202, val_acc:0.935]
Epoch [43/120    avg_loss:0.185, val_acc:0.950]
Epoch [44/120    avg_loss:0.095, val_acc:0.970]
Epoch [45/120    avg_loss:0.097, val_acc:0.962]
Epoch [46/120    avg_loss:0.141, val_acc:0.968]
Epoch [47/120    avg_loss:0.127, val_acc:0.980]
Epoch [48/120    avg_loss:0.071, val_acc:0.978]
Epoch [49/120    avg_loss:0.088, val_acc:0.972]
Epoch [50/120    avg_loss:0.146, val_acc:0.942]
Epoch [51/120    avg_loss:0.181, val_acc:0.964]
Epoch [52/120    avg_loss:0.121, val_acc:0.980]
Epoch [53/120    avg_loss:0.104, val_acc:0.950]
Epoch [54/120    avg_loss:0.108, val_acc:0.970]
Epoch [55/120    avg_loss:0.136, val_acc:0.958]
Epoch [56/120    avg_loss:0.076, val_acc:0.962]
Epoch [57/120    avg_loss:0.088, val_acc:0.978]
Epoch [58/120    avg_loss:0.096, val_acc:0.978]
Epoch [59/120    avg_loss:0.058, val_acc:0.990]
Epoch [60/120    avg_loss:0.040, val_acc:0.990]
Epoch [61/120    avg_loss:0.068, val_acc:0.972]
Epoch [62/120    avg_loss:0.121, val_acc:0.966]
Epoch [63/120    avg_loss:0.091, val_acc:0.978]
Epoch [64/120    avg_loss:0.102, val_acc:0.976]
Epoch [65/120    avg_loss:0.098, val_acc:0.984]
Epoch [66/120    avg_loss:0.061, val_acc:0.978]
Epoch [67/120    avg_loss:0.056, val_acc:0.982]
Epoch [68/120    avg_loss:0.046, val_acc:0.978]
Epoch [69/120    avg_loss:0.059, val_acc:0.964]
Epoch [70/120    avg_loss:0.042, val_acc:0.974]
Epoch [71/120    avg_loss:0.088, val_acc:0.964]
Epoch [72/120    avg_loss:0.110, val_acc:0.968]
Epoch [73/120    avg_loss:0.123, val_acc:0.970]
Epoch [74/120    avg_loss:0.055, val_acc:0.976]
Epoch [75/120    avg_loss:0.053, val_acc:0.980]
Epoch [76/120    avg_loss:0.055, val_acc:0.984]
Epoch [77/120    avg_loss:0.041, val_acc:0.986]
Epoch [78/120    avg_loss:0.036, val_acc:0.986]
Epoch [79/120    avg_loss:0.025, val_acc:0.988]
Epoch [80/120    avg_loss:0.031, val_acc:0.986]
Epoch [81/120    avg_loss:0.033, val_acc:0.988]
Epoch [82/120    avg_loss:0.032, val_acc:0.988]
Epoch [83/120    avg_loss:0.025, val_acc:0.990]
Epoch [84/120    avg_loss:0.026, val_acc:0.988]
Epoch [85/120    avg_loss:0.022, val_acc:0.988]
Epoch [86/120    avg_loss:0.021, val_acc:0.988]
Epoch [87/120    avg_loss:0.032, val_acc:0.990]
Epoch [88/120    avg_loss:0.035, val_acc:0.988]
Epoch [89/120    avg_loss:0.030, val_acc:0.990]
Epoch [90/120    avg_loss:0.027, val_acc:0.990]
Epoch [91/120    avg_loss:0.018, val_acc:0.990]
Epoch [92/120    avg_loss:0.026, val_acc:0.990]
Epoch [93/120    avg_loss:0.026, val_acc:0.992]
Epoch [94/120    avg_loss:0.034, val_acc:0.992]
Epoch [95/120    avg_loss:0.018, val_acc:0.992]
Epoch [96/120    avg_loss:0.023, val_acc:0.992]
Epoch [97/120    avg_loss:0.020, val_acc:0.992]
Epoch [98/120    avg_loss:0.028, val_acc:0.990]
Epoch [99/120    avg_loss:0.017, val_acc:0.990]
Epoch [100/120    avg_loss:0.019, val_acc:0.990]
Epoch [101/120    avg_loss:0.020, val_acc:0.992]
Epoch [102/120    avg_loss:0.016, val_acc:0.992]
Epoch [103/120    avg_loss:0.018, val_acc:0.992]
Epoch [104/120    avg_loss:0.018, val_acc:0.992]
Epoch [105/120    avg_loss:0.022, val_acc:0.992]
Epoch [106/120    avg_loss:0.026, val_acc:0.992]
Epoch [107/120    avg_loss:0.023, val_acc:0.992]
Epoch [108/120    avg_loss:0.023, val_acc:0.992]
Epoch [109/120    avg_loss:0.021, val_acc:0.990]
Epoch [110/120    avg_loss:0.017, val_acc:0.992]
Epoch [111/120    avg_loss:0.019, val_acc:0.992]
Epoch [112/120    avg_loss:0.015, val_acc:0.994]
Epoch [113/120    avg_loss:0.015, val_acc:0.994]
Epoch [114/120    avg_loss:0.016, val_acc:0.992]
Epoch [115/120    avg_loss:0.016, val_acc:0.992]
Epoch [116/120    avg_loss:0.013, val_acc:0.992]
Epoch [117/120    avg_loss:0.018, val_acc:0.992]
Epoch [118/120    avg_loss:0.014, val_acc:0.994]
Epoch [119/120    avg_loss:0.017, val_acc:0.992]
Epoch [120/120    avg_loss:0.017, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 213   0   0   0   0   6   0   0   0   0   0   0]
 [  0   0   2 224   4   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   6   0 200   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 1.         0.98156682 0.98678414 0.93598234 0.93687708
 0.98522167 0.96907216 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9912175796189947
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8cf2fe4828>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.239, val_acc:0.611]
Epoch [2/120    avg_loss:1.638, val_acc:0.641]
Epoch [3/120    avg_loss:1.280, val_acc:0.766]
Epoch [4/120    avg_loss:1.064, val_acc:0.716]
Epoch [5/120    avg_loss:0.969, val_acc:0.746]
Epoch [6/120    avg_loss:0.799, val_acc:0.812]
Epoch [7/120    avg_loss:0.714, val_acc:0.827]
Epoch [8/120    avg_loss:0.640, val_acc:0.829]
Epoch [9/120    avg_loss:0.646, val_acc:0.833]
Epoch [10/120    avg_loss:0.611, val_acc:0.845]
Epoch [11/120    avg_loss:0.537, val_acc:0.847]
Epoch [12/120    avg_loss:0.570, val_acc:0.851]
Epoch [13/120    avg_loss:0.567, val_acc:0.857]
Epoch [14/120    avg_loss:0.491, val_acc:0.845]
Epoch [15/120    avg_loss:0.539, val_acc:0.766]
Epoch [16/120    avg_loss:0.471, val_acc:0.883]
Epoch [17/120    avg_loss:0.471, val_acc:0.873]
Epoch [18/120    avg_loss:0.449, val_acc:0.907]
Epoch [19/120    avg_loss:0.391, val_acc:0.897]
Epoch [20/120    avg_loss:0.462, val_acc:0.841]
Epoch [21/120    avg_loss:0.337, val_acc:0.893]
Epoch [22/120    avg_loss:0.384, val_acc:0.889]
Epoch [23/120    avg_loss:0.350, val_acc:0.905]
Epoch [24/120    avg_loss:0.350, val_acc:0.927]
Epoch [25/120    avg_loss:0.310, val_acc:0.921]
Epoch [26/120    avg_loss:0.341, val_acc:0.940]
Epoch [27/120    avg_loss:0.264, val_acc:0.923]
Epoch [28/120    avg_loss:0.232, val_acc:0.907]
Epoch [29/120    avg_loss:0.372, val_acc:0.921]
Epoch [30/120    avg_loss:0.282, val_acc:0.927]
Epoch [31/120    avg_loss:0.264, val_acc:0.946]
Epoch [32/120    avg_loss:0.270, val_acc:0.917]
Epoch [33/120    avg_loss:0.319, val_acc:0.867]
Epoch [34/120    avg_loss:0.295, val_acc:0.929]
Epoch [35/120    avg_loss:0.202, val_acc:0.933]
Epoch [36/120    avg_loss:0.218, val_acc:0.927]
Epoch [37/120    avg_loss:0.214, val_acc:0.935]
Epoch [38/120    avg_loss:0.182, val_acc:0.946]
Epoch [39/120    avg_loss:0.203, val_acc:0.919]
Epoch [40/120    avg_loss:0.181, val_acc:0.966]
Epoch [41/120    avg_loss:0.229, val_acc:0.962]
Epoch [42/120    avg_loss:0.141, val_acc:0.935]
Epoch [43/120    avg_loss:0.189, val_acc:0.940]
Epoch [44/120    avg_loss:0.161, val_acc:0.952]
Epoch [45/120    avg_loss:0.113, val_acc:0.966]
Epoch [46/120    avg_loss:0.139, val_acc:0.966]
Epoch [47/120    avg_loss:0.158, val_acc:0.964]
Epoch [48/120    avg_loss:0.111, val_acc:0.946]
Epoch [49/120    avg_loss:0.190, val_acc:0.952]
Epoch [50/120    avg_loss:0.156, val_acc:0.958]
Epoch [51/120    avg_loss:0.124, val_acc:0.966]
Epoch [52/120    avg_loss:0.139, val_acc:0.962]
Epoch [53/120    avg_loss:0.089, val_acc:0.948]
Epoch [54/120    avg_loss:0.135, val_acc:0.966]
Epoch [55/120    avg_loss:0.152, val_acc:0.970]
Epoch [56/120    avg_loss:0.164, val_acc:0.956]
Epoch [57/120    avg_loss:0.163, val_acc:0.956]
Epoch [58/120    avg_loss:0.112, val_acc:0.966]
Epoch [59/120    avg_loss:0.101, val_acc:0.980]
Epoch [60/120    avg_loss:0.094, val_acc:0.972]
Epoch [61/120    avg_loss:0.085, val_acc:0.978]
Epoch [62/120    avg_loss:0.084, val_acc:0.978]
Epoch [63/120    avg_loss:0.075, val_acc:0.968]
Epoch [64/120    avg_loss:0.067, val_acc:0.982]
Epoch [65/120    avg_loss:0.063, val_acc:0.978]
Epoch [66/120    avg_loss:0.068, val_acc:0.968]
Epoch [67/120    avg_loss:0.066, val_acc:0.980]
Epoch [68/120    avg_loss:0.084, val_acc:0.968]
Epoch [69/120    avg_loss:0.069, val_acc:0.970]
Epoch [70/120    avg_loss:0.097, val_acc:0.970]
Epoch [71/120    avg_loss:0.075, val_acc:0.978]
Epoch [72/120    avg_loss:0.078, val_acc:0.986]
Epoch [73/120    avg_loss:0.073, val_acc:0.986]
Epoch [74/120    avg_loss:0.053, val_acc:0.980]
Epoch [75/120    avg_loss:0.076, val_acc:0.935]
Epoch [76/120    avg_loss:0.097, val_acc:0.976]
Epoch [77/120    avg_loss:0.038, val_acc:0.978]
Epoch [78/120    avg_loss:0.031, val_acc:0.978]
Epoch [79/120    avg_loss:0.060, val_acc:0.970]
Epoch [80/120    avg_loss:0.064, val_acc:0.976]
Epoch [81/120    avg_loss:0.057, val_acc:0.980]
Epoch [82/120    avg_loss:0.060, val_acc:0.980]
Epoch [83/120    avg_loss:0.049, val_acc:0.980]
Epoch [84/120    avg_loss:0.023, val_acc:0.982]
Epoch [85/120    avg_loss:0.063, val_acc:0.980]
Epoch [86/120    avg_loss:0.120, val_acc:0.974]
Epoch [87/120    avg_loss:0.036, val_acc:0.984]
Epoch [88/120    avg_loss:0.040, val_acc:0.984]
Epoch [89/120    avg_loss:0.025, val_acc:0.986]
Epoch [90/120    avg_loss:0.028, val_acc:0.988]
Epoch [91/120    avg_loss:0.023, val_acc:0.988]
Epoch [92/120    avg_loss:0.028, val_acc:0.988]
Epoch [93/120    avg_loss:0.020, val_acc:0.986]
Epoch [94/120    avg_loss:0.026, val_acc:0.986]
Epoch [95/120    avg_loss:0.018, val_acc:0.986]
Epoch [96/120    avg_loss:0.023, val_acc:0.988]
Epoch [97/120    avg_loss:0.024, val_acc:0.988]
Epoch [98/120    avg_loss:0.020, val_acc:0.988]
Epoch [99/120    avg_loss:0.023, val_acc:0.988]
Epoch [100/120    avg_loss:0.025, val_acc:0.986]
Epoch [101/120    avg_loss:0.018, val_acc:0.986]
Epoch [102/120    avg_loss:0.018, val_acc:0.988]
Epoch [103/120    avg_loss:0.018, val_acc:0.988]
Epoch [104/120    avg_loss:0.017, val_acc:0.988]
Epoch [105/120    avg_loss:0.019, val_acc:0.988]
Epoch [106/120    avg_loss:0.016, val_acc:0.988]
Epoch [107/120    avg_loss:0.015, val_acc:0.988]
Epoch [108/120    avg_loss:0.014, val_acc:0.988]
Epoch [109/120    avg_loss:0.019, val_acc:0.988]
Epoch [110/120    avg_loss:0.016, val_acc:0.988]
Epoch [111/120    avg_loss:0.022, val_acc:0.988]
Epoch [112/120    avg_loss:0.015, val_acc:0.988]
Epoch [113/120    avg_loss:0.013, val_acc:0.988]
Epoch [114/120    avg_loss:0.015, val_acc:0.988]
Epoch [115/120    avg_loss:0.018, val_acc:0.988]
Epoch [116/120    avg_loss:0.016, val_acc:0.990]
Epoch [117/120    avg_loss:0.017, val_acc:0.990]
Epoch [118/120    avg_loss:0.014, val_acc:0.988]
Epoch [119/120    avg_loss:0.012, val_acc:0.988]
Epoch [120/120    avg_loss:0.018, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 222   7   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  13 132   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 1.         0.98426966 0.98230088 0.92841649 0.91034483
 1.         0.96132597 1.         0.99893276 1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9900294661720105
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0c5f68a7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.237, val_acc:0.518]
Epoch [2/120    avg_loss:1.605, val_acc:0.645]
Epoch [3/120    avg_loss:1.206, val_acc:0.742]
Epoch [4/120    avg_loss:0.950, val_acc:0.764]
Epoch [5/120    avg_loss:0.839, val_acc:0.817]
Epoch [6/120    avg_loss:0.763, val_acc:0.853]
Epoch [7/120    avg_loss:0.655, val_acc:0.869]
Epoch [8/120    avg_loss:0.533, val_acc:0.863]
Epoch [9/120    avg_loss:0.585, val_acc:0.897]
Epoch [10/120    avg_loss:0.513, val_acc:0.893]
Epoch [11/120    avg_loss:0.433, val_acc:0.899]
Epoch [12/120    avg_loss:0.415, val_acc:0.897]
Epoch [13/120    avg_loss:0.497, val_acc:0.895]
Epoch [14/120    avg_loss:0.429, val_acc:0.891]
Epoch [15/120    avg_loss:0.430, val_acc:0.903]
Epoch [16/120    avg_loss:0.408, val_acc:0.903]
Epoch [17/120    avg_loss:0.458, val_acc:0.913]
Epoch [18/120    avg_loss:0.332, val_acc:0.938]
Epoch [19/120    avg_loss:0.311, val_acc:0.903]
Epoch [20/120    avg_loss:0.342, val_acc:0.905]
Epoch [21/120    avg_loss:0.336, val_acc:0.925]
Epoch [22/120    avg_loss:0.318, val_acc:0.931]
Epoch [23/120    avg_loss:0.337, val_acc:0.933]
Epoch [24/120    avg_loss:0.228, val_acc:0.938]
Epoch [25/120    avg_loss:0.258, val_acc:0.923]
Epoch [26/120    avg_loss:0.217, val_acc:0.952]
Epoch [27/120    avg_loss:0.220, val_acc:0.938]
Epoch [28/120    avg_loss:0.307, val_acc:0.938]
Epoch [29/120    avg_loss:0.256, val_acc:0.927]
Epoch [30/120    avg_loss:0.223, val_acc:0.940]
Epoch [31/120    avg_loss:0.186, val_acc:0.919]
Epoch [32/120    avg_loss:0.174, val_acc:0.952]
Epoch [33/120    avg_loss:0.204, val_acc:0.944]
Epoch [34/120    avg_loss:0.172, val_acc:0.968]
Epoch [35/120    avg_loss:0.150, val_acc:0.952]
Epoch [36/120    avg_loss:0.186, val_acc:0.962]
Epoch [37/120    avg_loss:0.196, val_acc:0.946]
Epoch [38/120    avg_loss:0.210, val_acc:0.964]
Epoch [39/120    avg_loss:0.149, val_acc:0.962]
Epoch [40/120    avg_loss:0.109, val_acc:0.968]
Epoch [41/120    avg_loss:0.080, val_acc:0.966]
Epoch [42/120    avg_loss:0.108, val_acc:0.950]
Epoch [43/120    avg_loss:0.101, val_acc:0.966]
Epoch [44/120    avg_loss:0.223, val_acc:0.956]
Epoch [45/120    avg_loss:0.149, val_acc:0.958]
Epoch [46/120    avg_loss:0.146, val_acc:0.970]
Epoch [47/120    avg_loss:0.114, val_acc:0.956]
Epoch [48/120    avg_loss:0.190, val_acc:0.972]
Epoch [49/120    avg_loss:0.062, val_acc:0.982]
Epoch [50/120    avg_loss:0.081, val_acc:0.974]
Epoch [51/120    avg_loss:0.097, val_acc:0.974]
Epoch [52/120    avg_loss:0.057, val_acc:0.968]
Epoch [53/120    avg_loss:0.074, val_acc:0.974]
Epoch [54/120    avg_loss:0.048, val_acc:0.966]
Epoch [55/120    avg_loss:0.047, val_acc:0.978]
Epoch [56/120    avg_loss:0.059, val_acc:0.964]
Epoch [57/120    avg_loss:0.092, val_acc:0.976]
Epoch [58/120    avg_loss:0.073, val_acc:0.964]
Epoch [59/120    avg_loss:0.080, val_acc:0.972]
Epoch [60/120    avg_loss:0.085, val_acc:0.944]
Epoch [61/120    avg_loss:0.104, val_acc:0.962]
Epoch [62/120    avg_loss:0.077, val_acc:0.970]
Epoch [63/120    avg_loss:0.040, val_acc:0.978]
Epoch [64/120    avg_loss:0.028, val_acc:0.978]
Epoch [65/120    avg_loss:0.029, val_acc:0.980]
Epoch [66/120    avg_loss:0.039, val_acc:0.978]
Epoch [67/120    avg_loss:0.024, val_acc:0.982]
Epoch [68/120    avg_loss:0.025, val_acc:0.984]
Epoch [69/120    avg_loss:0.030, val_acc:0.984]
Epoch [70/120    avg_loss:0.032, val_acc:0.984]
Epoch [71/120    avg_loss:0.026, val_acc:0.984]
Epoch [72/120    avg_loss:0.018, val_acc:0.984]
Epoch [73/120    avg_loss:0.024, val_acc:0.984]
Epoch [74/120    avg_loss:0.029, val_acc:0.984]
Epoch [75/120    avg_loss:0.024, val_acc:0.984]
Epoch [76/120    avg_loss:0.030, val_acc:0.986]
Epoch [77/120    avg_loss:0.023, val_acc:0.990]
Epoch [78/120    avg_loss:0.021, val_acc:0.984]
Epoch [79/120    avg_loss:0.027, val_acc:0.978]
Epoch [80/120    avg_loss:0.026, val_acc:0.982]
Epoch [81/120    avg_loss:0.028, val_acc:0.982]
Epoch [82/120    avg_loss:0.026, val_acc:0.984]
Epoch [83/120    avg_loss:0.025, val_acc:0.988]
Epoch [84/120    avg_loss:0.028, val_acc:0.986]
Epoch [85/120    avg_loss:0.022, val_acc:0.986]
Epoch [86/120    avg_loss:0.017, val_acc:0.988]
Epoch [87/120    avg_loss:0.017, val_acc:0.986]
Epoch [88/120    avg_loss:0.025, val_acc:0.984]
Epoch [89/120    avg_loss:0.020, val_acc:0.984]
Epoch [90/120    avg_loss:0.027, val_acc:0.986]
Epoch [91/120    avg_loss:0.025, val_acc:0.986]
Epoch [92/120    avg_loss:0.025, val_acc:0.986]
Epoch [93/120    avg_loss:0.020, val_acc:0.986]
Epoch [94/120    avg_loss:0.020, val_acc:0.988]
Epoch [95/120    avg_loss:0.018, val_acc:0.988]
Epoch [96/120    avg_loss:0.020, val_acc:0.988]
Epoch [97/120    avg_loss:0.024, val_acc:0.988]
Epoch [98/120    avg_loss:0.020, val_acc:0.988]
Epoch [99/120    avg_loss:0.019, val_acc:0.988]
Epoch [100/120    avg_loss:0.024, val_acc:0.988]
Epoch [101/120    avg_loss:0.020, val_acc:0.988]
Epoch [102/120    avg_loss:0.020, val_acc:0.988]
Epoch [103/120    avg_loss:0.020, val_acc:0.988]
Epoch [104/120    avg_loss:0.021, val_acc:0.988]
Epoch [105/120    avg_loss:0.024, val_acc:0.988]
Epoch [106/120    avg_loss:0.015, val_acc:0.988]
Epoch [107/120    avg_loss:0.018, val_acc:0.988]
Epoch [108/120    avg_loss:0.020, val_acc:0.988]
Epoch [109/120    avg_loss:0.020, val_acc:0.988]
Epoch [110/120    avg_loss:0.022, val_acc:0.988]
Epoch [111/120    avg_loss:0.019, val_acc:0.988]
Epoch [112/120    avg_loss:0.021, val_acc:0.988]
Epoch [113/120    avg_loss:0.019, val_acc:0.988]
Epoch [114/120    avg_loss:0.022, val_acc:0.988]
Epoch [115/120    avg_loss:0.023, val_acc:0.988]
Epoch [116/120    avg_loss:0.015, val_acc:0.988]
Epoch [117/120    avg_loss:0.024, val_acc:0.988]
Epoch [118/120    avg_loss:0.016, val_acc:0.988]
Epoch [119/120    avg_loss:0.019, val_acc:0.988]
Epoch [120/120    avg_loss:0.023, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0   3 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 1.         0.98426966 1.         0.9569161  0.93729373
 1.         0.96132597 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9935907380267947
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f611c9e67f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.268, val_acc:0.514]
Epoch [2/120    avg_loss:1.679, val_acc:0.655]
Epoch [3/120    avg_loss:1.295, val_acc:0.665]
Epoch [4/120    avg_loss:1.039, val_acc:0.736]
Epoch [5/120    avg_loss:0.893, val_acc:0.802]
Epoch [6/120    avg_loss:0.838, val_acc:0.760]
Epoch [7/120    avg_loss:0.748, val_acc:0.841]
Epoch [8/120    avg_loss:0.653, val_acc:0.802]
Epoch [9/120    avg_loss:0.608, val_acc:0.841]
Epoch [10/120    avg_loss:0.517, val_acc:0.847]
Epoch [11/120    avg_loss:0.536, val_acc:0.869]
Epoch [12/120    avg_loss:0.468, val_acc:0.899]
Epoch [13/120    avg_loss:0.437, val_acc:0.911]
Epoch [14/120    avg_loss:0.374, val_acc:0.899]
Epoch [15/120    avg_loss:0.419, val_acc:0.909]
Epoch [16/120    avg_loss:0.345, val_acc:0.905]
Epoch [17/120    avg_loss:0.310, val_acc:0.925]
Epoch [18/120    avg_loss:0.344, val_acc:0.938]
Epoch [19/120    avg_loss:0.402, val_acc:0.895]
Epoch [20/120    avg_loss:0.358, val_acc:0.938]
Epoch [21/120    avg_loss:0.357, val_acc:0.933]
Epoch [22/120    avg_loss:0.240, val_acc:0.946]
Epoch [23/120    avg_loss:0.242, val_acc:0.935]
Epoch [24/120    avg_loss:0.321, val_acc:0.931]
Epoch [25/120    avg_loss:0.293, val_acc:0.881]
Epoch [26/120    avg_loss:0.301, val_acc:0.927]
Epoch [27/120    avg_loss:0.291, val_acc:0.950]
Epoch [28/120    avg_loss:0.297, val_acc:0.933]
Epoch [29/120    avg_loss:0.279, val_acc:0.935]
Epoch [30/120    avg_loss:0.240, val_acc:0.940]
Epoch [31/120    avg_loss:0.275, val_acc:0.958]
Epoch [32/120    avg_loss:0.174, val_acc:0.954]
Epoch [33/120    avg_loss:0.159, val_acc:0.968]
Epoch [34/120    avg_loss:0.183, val_acc:0.968]
Epoch [35/120    avg_loss:0.193, val_acc:0.933]
Epoch [36/120    avg_loss:0.183, val_acc:0.968]
Epoch [37/120    avg_loss:0.158, val_acc:0.970]
Epoch [38/120    avg_loss:0.152, val_acc:0.954]
Epoch [39/120    avg_loss:0.197, val_acc:0.948]
Epoch [40/120    avg_loss:0.177, val_acc:0.966]
Epoch [41/120    avg_loss:0.149, val_acc:0.968]
Epoch [42/120    avg_loss:0.226, val_acc:0.956]
Epoch [43/120    avg_loss:0.209, val_acc:0.958]
Epoch [44/120    avg_loss:0.165, val_acc:0.966]
Epoch [45/120    avg_loss:0.139, val_acc:0.970]
Epoch [46/120    avg_loss:0.168, val_acc:0.976]
Epoch [47/120    avg_loss:0.146, val_acc:0.980]
Epoch [48/120    avg_loss:0.118, val_acc:0.980]
Epoch [49/120    avg_loss:0.108, val_acc:0.966]
Epoch [50/120    avg_loss:0.131, val_acc:0.956]
Epoch [51/120    avg_loss:0.148, val_acc:0.962]
Epoch [52/120    avg_loss:0.110, val_acc:0.972]
Epoch [53/120    avg_loss:0.147, val_acc:0.960]
Epoch [54/120    avg_loss:0.126, val_acc:0.974]
Epoch [55/120    avg_loss:0.143, val_acc:0.933]
Epoch [56/120    avg_loss:0.133, val_acc:0.978]
Epoch [57/120    avg_loss:0.083, val_acc:0.976]
Epoch [58/120    avg_loss:0.108, val_acc:0.980]
Epoch [59/120    avg_loss:0.064, val_acc:0.990]
Epoch [60/120    avg_loss:0.064, val_acc:0.976]
Epoch [61/120    avg_loss:0.072, val_acc:0.982]
Epoch [62/120    avg_loss:0.073, val_acc:0.978]
Epoch [63/120    avg_loss:0.108, val_acc:0.950]
Epoch [64/120    avg_loss:0.225, val_acc:0.968]
Epoch [65/120    avg_loss:0.094, val_acc:0.972]
Epoch [66/120    avg_loss:0.086, val_acc:0.968]
Epoch [67/120    avg_loss:0.061, val_acc:0.972]
Epoch [68/120    avg_loss:0.065, val_acc:0.974]
Epoch [69/120    avg_loss:0.023, val_acc:0.988]
Epoch [70/120    avg_loss:0.034, val_acc:0.976]
Epoch [71/120    avg_loss:0.087, val_acc:0.978]
Epoch [72/120    avg_loss:0.058, val_acc:0.980]
Epoch [73/120    avg_loss:0.058, val_acc:0.988]
Epoch [74/120    avg_loss:0.037, val_acc:0.990]
Epoch [75/120    avg_loss:0.033, val_acc:0.990]
Epoch [76/120    avg_loss:0.041, val_acc:0.990]
Epoch [77/120    avg_loss:0.023, val_acc:0.990]
Epoch [78/120    avg_loss:0.037, val_acc:0.990]
Epoch [79/120    avg_loss:0.029, val_acc:0.990]
Epoch [80/120    avg_loss:0.020, val_acc:0.990]
Epoch [81/120    avg_loss:0.018, val_acc:0.990]
Epoch [82/120    avg_loss:0.024, val_acc:0.990]
Epoch [83/120    avg_loss:0.022, val_acc:0.988]
Epoch [84/120    avg_loss:0.031, val_acc:0.990]
Epoch [85/120    avg_loss:0.032, val_acc:0.990]
Epoch [86/120    avg_loss:0.022, val_acc:0.990]
Epoch [87/120    avg_loss:0.021, val_acc:0.990]
Epoch [88/120    avg_loss:0.022, val_acc:0.990]
Epoch [89/120    avg_loss:0.017, val_acc:0.990]
Epoch [90/120    avg_loss:0.023, val_acc:0.992]
Epoch [91/120    avg_loss:0.024, val_acc:0.992]
Epoch [92/120    avg_loss:0.025, val_acc:0.990]
Epoch [93/120    avg_loss:0.028, val_acc:0.990]
Epoch [94/120    avg_loss:0.032, val_acc:0.990]
Epoch [95/120    avg_loss:0.022, val_acc:0.990]
Epoch [96/120    avg_loss:0.022, val_acc:0.990]
Epoch [97/120    avg_loss:0.022, val_acc:0.990]
Epoch [98/120    avg_loss:0.019, val_acc:0.992]
Epoch [99/120    avg_loss:0.029, val_acc:0.992]
Epoch [100/120    avg_loss:0.016, val_acc:0.990]
Epoch [101/120    avg_loss:0.025, val_acc:0.990]
Epoch [102/120    avg_loss:0.018, val_acc:0.992]
Epoch [103/120    avg_loss:0.027, val_acc:0.992]
Epoch [104/120    avg_loss:0.015, val_acc:0.994]
Epoch [105/120    avg_loss:0.014, val_acc:0.994]
Epoch [106/120    avg_loss:0.018, val_acc:0.994]
Epoch [107/120    avg_loss:0.021, val_acc:0.990]
Epoch [108/120    avg_loss:0.018, val_acc:0.992]
Epoch [109/120    avg_loss:0.019, val_acc:0.994]
Epoch [110/120    avg_loss:0.020, val_acc:0.994]
Epoch [111/120    avg_loss:0.015, val_acc:0.994]
Epoch [112/120    avg_loss:0.016, val_acc:0.994]
Epoch [113/120    avg_loss:0.018, val_acc:0.990]
Epoch [114/120    avg_loss:0.015, val_acc:0.990]
Epoch [115/120    avg_loss:0.019, val_acc:0.994]
Epoch [116/120    avg_loss:0.019, val_acc:0.994]
Epoch [117/120    avg_loss:0.018, val_acc:0.994]
Epoch [118/120    avg_loss:0.024, val_acc:0.992]
Epoch [119/120    avg_loss:0.017, val_acc:0.994]
Epoch [120/120    avg_loss:0.018, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 224   6   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 214  13   0   0   0   0   0   0   0   0]
 [  0   0   0   0  18 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.01918976545842

F1 scores:
[       nan 1.         0.98206278 0.98678414 0.92043011 0.89122807
 1.         0.95555556 1.         1.         1.         0.9986755
 0.99889503 1.        ]

Kappa:
0.9890797256963156
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0a9cc3e7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.339, val_acc:0.577]
Epoch [2/120    avg_loss:1.713, val_acc:0.681]
Epoch [3/120    avg_loss:1.293, val_acc:0.722]
Epoch [4/120    avg_loss:1.000, val_acc:0.829]
Epoch [5/120    avg_loss:0.874, val_acc:0.774]
Epoch [6/120    avg_loss:0.758, val_acc:0.837]
Epoch [7/120    avg_loss:0.644, val_acc:0.788]
Epoch [8/120    avg_loss:0.644, val_acc:0.877]
Epoch [9/120    avg_loss:0.587, val_acc:0.835]
Epoch [10/120    avg_loss:0.523, val_acc:0.893]
Epoch [11/120    avg_loss:0.514, val_acc:0.901]
Epoch [12/120    avg_loss:0.471, val_acc:0.907]
Epoch [13/120    avg_loss:0.403, val_acc:0.909]
Epoch [14/120    avg_loss:0.498, val_acc:0.893]
Epoch [15/120    avg_loss:0.383, val_acc:0.911]
Epoch [16/120    avg_loss:0.398, val_acc:0.923]
Epoch [17/120    avg_loss:0.354, val_acc:0.948]
Epoch [18/120    avg_loss:0.330, val_acc:0.960]
Epoch [19/120    avg_loss:0.292, val_acc:0.921]
Epoch [20/120    avg_loss:0.350, val_acc:0.946]
Epoch [21/120    avg_loss:0.263, val_acc:0.962]
Epoch [22/120    avg_loss:0.241, val_acc:0.956]
Epoch [23/120    avg_loss:0.205, val_acc:0.940]
Epoch [24/120    avg_loss:0.306, val_acc:0.933]
Epoch [25/120    avg_loss:0.267, val_acc:0.942]
Epoch [26/120    avg_loss:0.254, val_acc:0.968]
Epoch [27/120    avg_loss:0.211, val_acc:0.954]
Epoch [28/120    avg_loss:0.312, val_acc:0.938]
Epoch [29/120    avg_loss:0.252, val_acc:0.952]
Epoch [30/120    avg_loss:0.189, val_acc:0.964]
Epoch [31/120    avg_loss:0.157, val_acc:0.972]
Epoch [32/120    avg_loss:0.160, val_acc:0.956]
Epoch [33/120    avg_loss:0.138, val_acc:0.962]
Epoch [34/120    avg_loss:0.186, val_acc:0.968]
Epoch [35/120    avg_loss:0.135, val_acc:0.964]
Epoch [36/120    avg_loss:0.121, val_acc:0.923]
Epoch [37/120    avg_loss:0.192, val_acc:0.940]
Epoch [38/120    avg_loss:0.153, val_acc:0.964]
Epoch [39/120    avg_loss:0.144, val_acc:0.976]
Epoch [40/120    avg_loss:0.144, val_acc:0.956]
Epoch [41/120    avg_loss:0.144, val_acc:0.980]
Epoch [42/120    avg_loss:0.153, val_acc:0.970]
Epoch [43/120    avg_loss:0.098, val_acc:0.964]
Epoch [44/120    avg_loss:0.108, val_acc:0.978]
Epoch [45/120    avg_loss:0.096, val_acc:0.968]
Epoch [46/120    avg_loss:0.112, val_acc:0.968]
Epoch [47/120    avg_loss:0.161, val_acc:0.962]
Epoch [48/120    avg_loss:0.113, val_acc:0.958]
Epoch [49/120    avg_loss:0.100, val_acc:0.978]
Epoch [50/120    avg_loss:0.060, val_acc:0.984]
Epoch [51/120    avg_loss:0.063, val_acc:0.974]
Epoch [52/120    avg_loss:0.085, val_acc:0.960]
Epoch [53/120    avg_loss:0.136, val_acc:0.938]
Epoch [54/120    avg_loss:0.111, val_acc:0.964]
Epoch [55/120    avg_loss:0.100, val_acc:0.976]
Epoch [56/120    avg_loss:0.074, val_acc:0.976]
Epoch [57/120    avg_loss:0.118, val_acc:0.968]
Epoch [58/120    avg_loss:0.070, val_acc:0.978]
Epoch [59/120    avg_loss:0.066, val_acc:0.978]
Epoch [60/120    avg_loss:0.046, val_acc:0.982]
Epoch [61/120    avg_loss:0.092, val_acc:0.962]
Epoch [62/120    avg_loss:0.073, val_acc:0.976]
Epoch [63/120    avg_loss:0.083, val_acc:0.956]
Epoch [64/120    avg_loss:0.105, val_acc:0.976]
Epoch [65/120    avg_loss:0.038, val_acc:0.978]
Epoch [66/120    avg_loss:0.043, val_acc:0.984]
Epoch [67/120    avg_loss:0.040, val_acc:0.984]
Epoch [68/120    avg_loss:0.045, val_acc:0.980]
Epoch [69/120    avg_loss:0.029, val_acc:0.982]
Epoch [70/120    avg_loss:0.029, val_acc:0.982]
Epoch [71/120    avg_loss:0.040, val_acc:0.982]
Epoch [72/120    avg_loss:0.029, val_acc:0.982]
Epoch [73/120    avg_loss:0.029, val_acc:0.982]
Epoch [74/120    avg_loss:0.032, val_acc:0.982]
Epoch [75/120    avg_loss:0.032, val_acc:0.982]
Epoch [76/120    avg_loss:0.025, val_acc:0.982]
Epoch [77/120    avg_loss:0.025, val_acc:0.982]
Epoch [78/120    avg_loss:0.025, val_acc:0.982]
Epoch [79/120    avg_loss:0.022, val_acc:0.982]
Epoch [80/120    avg_loss:0.026, val_acc:0.982]
Epoch [81/120    avg_loss:0.025, val_acc:0.982]
Epoch [82/120    avg_loss:0.022, val_acc:0.982]
Epoch [83/120    avg_loss:0.039, val_acc:0.982]
Epoch [84/120    avg_loss:0.025, val_acc:0.982]
Epoch [85/120    avg_loss:0.026, val_acc:0.984]
Epoch [86/120    avg_loss:0.031, val_acc:0.984]
Epoch [87/120    avg_loss:0.030, val_acc:0.984]
Epoch [88/120    avg_loss:0.021, val_acc:0.984]
Epoch [89/120    avg_loss:0.025, val_acc:0.984]
Epoch [90/120    avg_loss:0.033, val_acc:0.984]
Epoch [91/120    avg_loss:0.027, val_acc:0.984]
Epoch [92/120    avg_loss:0.021, val_acc:0.984]
Epoch [93/120    avg_loss:0.022, val_acc:0.984]
Epoch [94/120    avg_loss:0.019, val_acc:0.984]
Epoch [95/120    avg_loss:0.022, val_acc:0.984]
Epoch [96/120    avg_loss:0.019, val_acc:0.984]
Epoch [97/120    avg_loss:0.029, val_acc:0.984]
Epoch [98/120    avg_loss:0.024, val_acc:0.984]
Epoch [99/120    avg_loss:0.018, val_acc:0.984]
Epoch [100/120    avg_loss:0.019, val_acc:0.984]
Epoch [101/120    avg_loss:0.031, val_acc:0.984]
Epoch [102/120    avg_loss:0.027, val_acc:0.984]
Epoch [103/120    avg_loss:0.024, val_acc:0.984]
Epoch [104/120    avg_loss:0.023, val_acc:0.984]
Epoch [105/120    avg_loss:0.036, val_acc:0.984]
Epoch [106/120    avg_loss:0.019, val_acc:0.984]
Epoch [107/120    avg_loss:0.022, val_acc:0.984]
Epoch [108/120    avg_loss:0.023, val_acc:0.984]
Epoch [109/120    avg_loss:0.025, val_acc:0.984]
Epoch [110/120    avg_loss:0.018, val_acc:0.984]
Epoch [111/120    avg_loss:0.028, val_acc:0.984]
Epoch [112/120    avg_loss:0.026, val_acc:0.984]
Epoch [113/120    avg_loss:0.025, val_acc:0.984]
Epoch [114/120    avg_loss:0.019, val_acc:0.984]
Epoch [115/120    avg_loss:0.022, val_acc:0.984]
Epoch [116/120    avg_loss:0.029, val_acc:0.984]
Epoch [117/120    avg_loss:0.032, val_acc:0.984]
Epoch [118/120    avg_loss:0.029, val_acc:0.984]
Epoch [119/120    avg_loss:0.022, val_acc:0.984]
Epoch [120/120    avg_loss:0.023, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   6 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1044776119403

F1 scores:
[       nan 1.         0.98206278 1.         0.94196429 0.91216216
 1.         0.95555556 1.         1.         1.         0.98944591
 0.99113082 1.        ]

Kappa:
0.9900298759540024
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2f15b24828>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.254, val_acc:0.579]
Epoch [2/120    avg_loss:1.623, val_acc:0.607]
Epoch [3/120    avg_loss:1.285, val_acc:0.744]
Epoch [4/120    avg_loss:1.052, val_acc:0.738]
Epoch [5/120    avg_loss:0.847, val_acc:0.812]
Epoch [6/120    avg_loss:0.827, val_acc:0.837]
Epoch [7/120    avg_loss:0.729, val_acc:0.819]
Epoch [8/120    avg_loss:0.660, val_acc:0.784]
Epoch [9/120    avg_loss:0.650, val_acc:0.839]
Epoch [10/120    avg_loss:0.564, val_acc:0.806]
Epoch [11/120    avg_loss:0.655, val_acc:0.853]
Epoch [12/120    avg_loss:0.523, val_acc:0.839]
Epoch [13/120    avg_loss:0.480, val_acc:0.895]
Epoch [14/120    avg_loss:0.514, val_acc:0.893]
Epoch [15/120    avg_loss:0.491, val_acc:0.865]
Epoch [16/120    avg_loss:0.473, val_acc:0.887]
Epoch [17/120    avg_loss:0.405, val_acc:0.891]
Epoch [18/120    avg_loss:0.392, val_acc:0.913]
Epoch [19/120    avg_loss:0.336, val_acc:0.935]
Epoch [20/120    avg_loss:0.304, val_acc:0.919]
Epoch [21/120    avg_loss:0.305, val_acc:0.915]
Epoch [22/120    avg_loss:0.340, val_acc:0.925]
Epoch [23/120    avg_loss:0.229, val_acc:0.954]
Epoch [24/120    avg_loss:0.256, val_acc:0.923]
Epoch [25/120    avg_loss:0.337, val_acc:0.962]
Epoch [26/120    avg_loss:0.308, val_acc:0.960]
Epoch [27/120    avg_loss:0.291, val_acc:0.921]
Epoch [28/120    avg_loss:0.285, val_acc:0.944]
Epoch [29/120    avg_loss:0.208, val_acc:0.944]
Epoch [30/120    avg_loss:0.197, val_acc:0.978]
Epoch [31/120    avg_loss:0.152, val_acc:0.966]
Epoch [32/120    avg_loss:0.150, val_acc:0.950]
Epoch [33/120    avg_loss:0.184, val_acc:0.984]
Epoch [34/120    avg_loss:0.155, val_acc:0.978]
Epoch [35/120    avg_loss:0.151, val_acc:0.966]
Epoch [36/120    avg_loss:0.187, val_acc:0.984]
Epoch [37/120    avg_loss:0.187, val_acc:0.956]
Epoch [38/120    avg_loss:0.148, val_acc:0.968]
Epoch [39/120    avg_loss:0.124, val_acc:0.978]
Epoch [40/120    avg_loss:0.090, val_acc:0.976]
Epoch [41/120    avg_loss:0.105, val_acc:0.946]
Epoch [42/120    avg_loss:0.117, val_acc:0.974]
Epoch [43/120    avg_loss:0.074, val_acc:0.978]
Epoch [44/120    avg_loss:0.097, val_acc:0.968]
Epoch [45/120    avg_loss:0.111, val_acc:0.901]
Epoch [46/120    avg_loss:0.104, val_acc:0.974]
Epoch [47/120    avg_loss:0.085, val_acc:0.972]
Epoch [48/120    avg_loss:0.121, val_acc:0.964]
Epoch [49/120    avg_loss:0.067, val_acc:0.976]
Epoch [50/120    avg_loss:0.069, val_acc:0.988]
Epoch [51/120    avg_loss:0.063, val_acc:0.990]
Epoch [52/120    avg_loss:0.037, val_acc:0.990]
Epoch [53/120    avg_loss:0.038, val_acc:0.986]
Epoch [54/120    avg_loss:0.049, val_acc:0.986]
Epoch [55/120    avg_loss:0.031, val_acc:0.984]
Epoch [56/120    avg_loss:0.038, val_acc:0.986]
Epoch [57/120    avg_loss:0.038, val_acc:0.986]
Epoch [58/120    avg_loss:0.043, val_acc:0.986]
Epoch [59/120    avg_loss:0.037, val_acc:0.986]
Epoch [60/120    avg_loss:0.039, val_acc:0.988]
Epoch [61/120    avg_loss:0.031, val_acc:0.990]
Epoch [62/120    avg_loss:0.036, val_acc:0.992]
Epoch [63/120    avg_loss:0.038, val_acc:0.992]
Epoch [64/120    avg_loss:0.034, val_acc:0.990]
Epoch [65/120    avg_loss:0.034, val_acc:0.990]
Epoch [66/120    avg_loss:0.032, val_acc:0.990]
Epoch [67/120    avg_loss:0.052, val_acc:0.988]
Epoch [68/120    avg_loss:0.036, val_acc:0.986]
Epoch [69/120    avg_loss:0.032, val_acc:0.986]
Epoch [70/120    avg_loss:0.022, val_acc:0.988]
Epoch [71/120    avg_loss:0.030, val_acc:0.988]
Epoch [72/120    avg_loss:0.026, val_acc:0.990]
Epoch [73/120    avg_loss:0.032, val_acc:0.990]
Epoch [74/120    avg_loss:0.027, val_acc:0.990]
Epoch [75/120    avg_loss:0.033, val_acc:0.994]
Epoch [76/120    avg_loss:0.033, val_acc:0.990]
Epoch [77/120    avg_loss:0.025, val_acc:0.990]
Epoch [78/120    avg_loss:0.041, val_acc:0.990]
Epoch [79/120    avg_loss:0.031, val_acc:0.990]
Epoch [80/120    avg_loss:0.022, val_acc:0.992]
Epoch [81/120    avg_loss:0.021, val_acc:0.990]
Epoch [82/120    avg_loss:0.022, val_acc:0.992]
Epoch [83/120    avg_loss:0.029, val_acc:0.992]
Epoch [84/120    avg_loss:0.026, val_acc:0.992]
Epoch [85/120    avg_loss:0.021, val_acc:0.992]
Epoch [86/120    avg_loss:0.019, val_acc:0.992]
Epoch [87/120    avg_loss:0.026, val_acc:0.992]
Epoch [88/120    avg_loss:0.020, val_acc:0.990]
Epoch [89/120    avg_loss:0.023, val_acc:0.990]
Epoch [90/120    avg_loss:0.018, val_acc:0.990]
Epoch [91/120    avg_loss:0.025, val_acc:0.990]
Epoch [92/120    avg_loss:0.018, val_acc:0.992]
Epoch [93/120    avg_loss:0.021, val_acc:0.992]
Epoch [94/120    avg_loss:0.031, val_acc:0.992]
Epoch [95/120    avg_loss:0.030, val_acc:0.992]
Epoch [96/120    avg_loss:0.022, val_acc:0.992]
Epoch [97/120    avg_loss:0.021, val_acc:0.992]
Epoch [98/120    avg_loss:0.026, val_acc:0.992]
Epoch [99/120    avg_loss:0.021, val_acc:0.992]
Epoch [100/120    avg_loss:0.022, val_acc:0.992]
Epoch [101/120    avg_loss:0.024, val_acc:0.992]
Epoch [102/120    avg_loss:0.017, val_acc:0.992]
Epoch [103/120    avg_loss:0.020, val_acc:0.992]
Epoch [104/120    avg_loss:0.024, val_acc:0.992]
Epoch [105/120    avg_loss:0.016, val_acc:0.992]
Epoch [106/120    avg_loss:0.019, val_acc:0.992]
Epoch [107/120    avg_loss:0.029, val_acc:0.992]
Epoch [108/120    avg_loss:0.022, val_acc:0.992]
Epoch [109/120    avg_loss:0.025, val_acc:0.992]
Epoch [110/120    avg_loss:0.025, val_acc:0.992]
Epoch [111/120    avg_loss:0.031, val_acc:0.992]
Epoch [112/120    avg_loss:0.021, val_acc:0.992]
Epoch [113/120    avg_loss:0.024, val_acc:0.992]
Epoch [114/120    avg_loss:0.025, val_acc:0.992]
Epoch [115/120    avg_loss:0.023, val_acc:0.992]
Epoch [116/120    avg_loss:0.023, val_acc:0.992]
Epoch [117/120    avg_loss:0.022, val_acc:0.992]
Epoch [118/120    avg_loss:0.033, val_acc:0.992]
Epoch [119/120    avg_loss:0.017, val_acc:0.992]
Epoch [120/120    avg_loss:0.028, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 229   0   0   0   0   1   0   0   0   0   0]
 [  0   0   0   0 212  13   0   0   0   0   0   0   2   0]
 [  0   0   0   0   4 140   1   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   5   0   0   0   0  89   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   7 446   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.29637526652452

F1 scores:
[       nan 1.         0.98871332 0.99782135 0.95711061 0.93959732
 0.99757869 0.9726776  0.998713   1.         1.         0.99080158
 0.9900111  1.        ]

Kappa:
0.992166401683153
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff6a946a7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.273, val_acc:0.609]
Epoch [2/120    avg_loss:1.678, val_acc:0.704]
Epoch [3/120    avg_loss:1.344, val_acc:0.692]
Epoch [4/120    avg_loss:1.053, val_acc:0.675]
Epoch [5/120    avg_loss:0.867, val_acc:0.806]
Epoch [6/120    avg_loss:0.811, val_acc:0.790]
Epoch [7/120    avg_loss:0.738, val_acc:0.845]
Epoch [8/120    avg_loss:0.611, val_acc:0.819]
Epoch [9/120    avg_loss:0.560, val_acc:0.845]
Epoch [10/120    avg_loss:0.537, val_acc:0.847]
Epoch [11/120    avg_loss:0.525, val_acc:0.873]
Epoch [12/120    avg_loss:0.537, val_acc:0.863]
Epoch [13/120    avg_loss:0.448, val_acc:0.893]
Epoch [14/120    avg_loss:0.450, val_acc:0.871]
Epoch [15/120    avg_loss:0.446, val_acc:0.889]
Epoch [16/120    avg_loss:0.449, val_acc:0.875]
Epoch [17/120    avg_loss:0.392, val_acc:0.891]
Epoch [18/120    avg_loss:0.340, val_acc:0.895]
Epoch [19/120    avg_loss:0.321, val_acc:0.899]
Epoch [20/120    avg_loss:0.362, val_acc:0.925]
Epoch [21/120    avg_loss:0.353, val_acc:0.905]
Epoch [22/120    avg_loss:0.327, val_acc:0.893]
Epoch [23/120    avg_loss:0.282, val_acc:0.940]
Epoch [24/120    avg_loss:0.293, val_acc:0.929]
Epoch [25/120    avg_loss:0.258, val_acc:0.950]
Epoch [26/120    avg_loss:0.265, val_acc:0.933]
Epoch [27/120    avg_loss:0.307, val_acc:0.929]
Epoch [28/120    avg_loss:0.232, val_acc:0.942]
Epoch [29/120    avg_loss:0.238, val_acc:0.950]
Epoch [30/120    avg_loss:0.189, val_acc:0.962]
Epoch [31/120    avg_loss:0.197, val_acc:0.966]
Epoch [32/120    avg_loss:0.207, val_acc:0.917]
Epoch [33/120    avg_loss:0.215, val_acc:0.938]
Epoch [34/120    avg_loss:0.157, val_acc:0.962]
Epoch [35/120    avg_loss:0.138, val_acc:0.952]
Epoch [36/120    avg_loss:0.214, val_acc:0.942]
Epoch [37/120    avg_loss:0.219, val_acc:0.972]
Epoch [38/120    avg_loss:0.159, val_acc:0.956]
Epoch [39/120    avg_loss:0.132, val_acc:0.974]
Epoch [40/120    avg_loss:0.121, val_acc:0.970]
Epoch [41/120    avg_loss:0.102, val_acc:0.970]
Epoch [42/120    avg_loss:0.129, val_acc:0.972]
Epoch [43/120    avg_loss:0.096, val_acc:0.966]
Epoch [44/120    avg_loss:0.105, val_acc:0.974]
Epoch [45/120    avg_loss:0.119, val_acc:0.968]
Epoch [46/120    avg_loss:0.210, val_acc:0.950]
Epoch [47/120    avg_loss:0.222, val_acc:0.954]
Epoch [48/120    avg_loss:0.139, val_acc:0.962]
Epoch [49/120    avg_loss:0.106, val_acc:0.976]
Epoch [50/120    avg_loss:0.105, val_acc:0.966]
Epoch [51/120    avg_loss:0.072, val_acc:0.982]
Epoch [52/120    avg_loss:0.097, val_acc:0.982]
Epoch [53/120    avg_loss:0.063, val_acc:0.966]
Epoch [54/120    avg_loss:0.067, val_acc:0.966]
Epoch [55/120    avg_loss:0.152, val_acc:0.942]
Epoch [56/120    avg_loss:0.074, val_acc:0.974]
Epoch [57/120    avg_loss:0.032, val_acc:0.988]
Epoch [58/120    avg_loss:0.048, val_acc:0.976]
Epoch [59/120    avg_loss:0.031, val_acc:0.984]
Epoch [60/120    avg_loss:0.043, val_acc:0.978]
Epoch [61/120    avg_loss:0.060, val_acc:0.972]
Epoch [62/120    avg_loss:0.062, val_acc:0.982]
Epoch [63/120    avg_loss:0.043, val_acc:0.974]
Epoch [64/120    avg_loss:0.035, val_acc:0.982]
Epoch [65/120    avg_loss:0.032, val_acc:0.982]
Epoch [66/120    avg_loss:0.032, val_acc:0.984]
Epoch [67/120    avg_loss:0.037, val_acc:0.984]
Epoch [68/120    avg_loss:0.052, val_acc:0.980]
Epoch [69/120    avg_loss:0.111, val_acc:0.974]
Epoch [70/120    avg_loss:0.074, val_acc:0.976]
Epoch [71/120    avg_loss:0.041, val_acc:0.980]
Epoch [72/120    avg_loss:0.030, val_acc:0.982]
Epoch [73/120    avg_loss:0.026, val_acc:0.988]
Epoch [74/120    avg_loss:0.023, val_acc:0.984]
Epoch [75/120    avg_loss:0.021, val_acc:0.984]
Epoch [76/120    avg_loss:0.018, val_acc:0.986]
Epoch [77/120    avg_loss:0.023, val_acc:0.988]
Epoch [78/120    avg_loss:0.025, val_acc:0.988]
Epoch [79/120    avg_loss:0.018, val_acc:0.990]
Epoch [80/120    avg_loss:0.021, val_acc:0.994]
Epoch [81/120    avg_loss:0.015, val_acc:0.994]
Epoch [82/120    avg_loss:0.023, val_acc:0.990]
Epoch [83/120    avg_loss:0.019, val_acc:0.988]
Epoch [84/120    avg_loss:0.018, val_acc:0.992]
Epoch [85/120    avg_loss:0.017, val_acc:0.992]
Epoch [86/120    avg_loss:0.015, val_acc:0.990]
Epoch [87/120    avg_loss:0.015, val_acc:0.992]
Epoch [88/120    avg_loss:0.014, val_acc:0.990]
Epoch [89/120    avg_loss:0.018, val_acc:0.994]
Epoch [90/120    avg_loss:0.013, val_acc:0.994]
Epoch [91/120    avg_loss:0.014, val_acc:0.994]
Epoch [92/120    avg_loss:0.013, val_acc:0.994]
Epoch [93/120    avg_loss:0.017, val_acc:0.990]
Epoch [94/120    avg_loss:0.017, val_acc:0.990]
Epoch [95/120    avg_loss:0.013, val_acc:0.992]
Epoch [96/120    avg_loss:0.014, val_acc:0.992]
Epoch [97/120    avg_loss:0.015, val_acc:0.990]
Epoch [98/120    avg_loss:0.012, val_acc:0.990]
Epoch [99/120    avg_loss:0.014, val_acc:0.990]
Epoch [100/120    avg_loss:0.011, val_acc:0.990]
Epoch [101/120    avg_loss:0.012, val_acc:0.990]
Epoch [102/120    avg_loss:0.015, val_acc:0.988]
Epoch [103/120    avg_loss:0.015, val_acc:0.990]
Epoch [104/120    avg_loss:0.021, val_acc:0.990]
Epoch [105/120    avg_loss:0.011, val_acc:0.988]
Epoch [106/120    avg_loss:0.014, val_acc:0.988]
Epoch [107/120    avg_loss:0.016, val_acc:0.988]
Epoch [108/120    avg_loss:0.014, val_acc:0.988]
Epoch [109/120    avg_loss:0.011, val_acc:0.988]
Epoch [110/120    avg_loss:0.010, val_acc:0.988]
Epoch [111/120    avg_loss:0.013, val_acc:0.988]
Epoch [112/120    avg_loss:0.011, val_acc:0.988]
Epoch [113/120    avg_loss:0.015, val_acc:0.990]
Epoch [114/120    avg_loss:0.012, val_acc:0.990]
Epoch [115/120    avg_loss:0.010, val_acc:0.990]
Epoch [116/120    avg_loss:0.019, val_acc:0.990]
Epoch [117/120    avg_loss:0.012, val_acc:0.990]
Epoch [118/120    avg_loss:0.022, val_acc:0.988]
Epoch [119/120    avg_loss:0.016, val_acc:0.988]
Epoch [120/120    avg_loss:0.012, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 215  15   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 212  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   5 448   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.18976545842217

F1 scores:
[       nan 1.         1.         0.96629213 0.92982456 0.94389439
 1.         1.         1.         1.         1.         0.99208443
 0.99334812 1.        ]

Kappa:
0.9909801507137762
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f92ac885860>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.271, val_acc:0.607]
Epoch [2/120    avg_loss:1.694, val_acc:0.665]
Epoch [3/120    avg_loss:1.234, val_acc:0.772]
Epoch [4/120    avg_loss:1.015, val_acc:0.694]
Epoch [5/120    avg_loss:0.838, val_acc:0.831]
Epoch [6/120    avg_loss:0.768, val_acc:0.728]
Epoch [7/120    avg_loss:0.708, val_acc:0.845]
Epoch [8/120    avg_loss:0.707, val_acc:0.804]
Epoch [9/120    avg_loss:0.612, val_acc:0.823]
Epoch [10/120    avg_loss:0.603, val_acc:0.871]
Epoch [11/120    avg_loss:0.566, val_acc:0.877]
Epoch [12/120    avg_loss:0.566, val_acc:0.786]
Epoch [13/120    avg_loss:0.537, val_acc:0.788]
Epoch [14/120    avg_loss:0.479, val_acc:0.887]
Epoch [15/120    avg_loss:0.443, val_acc:0.897]
Epoch [16/120    avg_loss:0.436, val_acc:0.865]
Epoch [17/120    avg_loss:0.478, val_acc:0.869]
Epoch [18/120    avg_loss:0.429, val_acc:0.901]
Epoch [19/120    avg_loss:0.305, val_acc:0.909]
Epoch [20/120    avg_loss:0.312, val_acc:0.921]
Epoch [21/120    avg_loss:0.359, val_acc:0.875]
Epoch [22/120    avg_loss:0.357, val_acc:0.927]
Epoch [23/120    avg_loss:0.283, val_acc:0.942]
Epoch [24/120    avg_loss:0.251, val_acc:0.942]
Epoch [25/120    avg_loss:0.231, val_acc:0.929]
Epoch [26/120    avg_loss:0.277, val_acc:0.897]
Epoch [27/120    avg_loss:0.381, val_acc:0.921]
Epoch [28/120    avg_loss:0.229, val_acc:0.946]
Epoch [29/120    avg_loss:0.231, val_acc:0.925]
Epoch [30/120    avg_loss:0.252, val_acc:0.935]
Epoch [31/120    avg_loss:0.202, val_acc:0.940]
Epoch [32/120    avg_loss:0.213, val_acc:0.942]
Epoch [33/120    avg_loss:0.226, val_acc:0.952]
Epoch [34/120    avg_loss:0.140, val_acc:0.948]
Epoch [35/120    avg_loss:0.153, val_acc:0.946]
Epoch [36/120    avg_loss:0.130, val_acc:0.954]
Epoch [37/120    avg_loss:0.167, val_acc:0.946]
Epoch [38/120    avg_loss:0.152, val_acc:0.950]
Epoch [39/120    avg_loss:0.160, val_acc:0.952]
Epoch [40/120    avg_loss:0.140, val_acc:0.948]
Epoch [41/120    avg_loss:0.106, val_acc:0.954]
Epoch [42/120    avg_loss:0.140, val_acc:0.933]
Epoch [43/120    avg_loss:0.164, val_acc:0.925]
Epoch [44/120    avg_loss:0.132, val_acc:0.946]
Epoch [45/120    avg_loss:0.163, val_acc:0.958]
Epoch [46/120    avg_loss:0.138, val_acc:0.956]
Epoch [47/120    avg_loss:0.130, val_acc:0.958]
Epoch [48/120    avg_loss:0.086, val_acc:0.966]
Epoch [49/120    avg_loss:0.114, val_acc:0.976]
Epoch [50/120    avg_loss:0.102, val_acc:0.962]
Epoch [51/120    avg_loss:0.123, val_acc:0.954]
Epoch [52/120    avg_loss:0.118, val_acc:0.962]
Epoch [53/120    avg_loss:0.130, val_acc:0.954]
Epoch [54/120    avg_loss:0.056, val_acc:0.956]
Epoch [55/120    avg_loss:0.079, val_acc:0.954]
Epoch [56/120    avg_loss:0.054, val_acc:0.968]
Epoch [57/120    avg_loss:0.060, val_acc:0.970]
Epoch [58/120    avg_loss:0.099, val_acc:0.956]
Epoch [59/120    avg_loss:0.072, val_acc:0.954]
Epoch [60/120    avg_loss:0.053, val_acc:0.972]
Epoch [61/120    avg_loss:0.037, val_acc:0.972]
Epoch [62/120    avg_loss:0.040, val_acc:0.958]
Epoch [63/120    avg_loss:0.058, val_acc:0.972]
Epoch [64/120    avg_loss:0.052, val_acc:0.972]
Epoch [65/120    avg_loss:0.028, val_acc:0.970]
Epoch [66/120    avg_loss:0.029, val_acc:0.968]
Epoch [67/120    avg_loss:0.034, val_acc:0.968]
Epoch [68/120    avg_loss:0.031, val_acc:0.970]
Epoch [69/120    avg_loss:0.019, val_acc:0.968]
Epoch [70/120    avg_loss:0.026, val_acc:0.970]
Epoch [71/120    avg_loss:0.020, val_acc:0.970]
Epoch [72/120    avg_loss:0.027, val_acc:0.972]
Epoch [73/120    avg_loss:0.026, val_acc:0.972]
Epoch [74/120    avg_loss:0.020, val_acc:0.972]
Epoch [75/120    avg_loss:0.019, val_acc:0.972]
Epoch [76/120    avg_loss:0.018, val_acc:0.972]
Epoch [77/120    avg_loss:0.024, val_acc:0.972]
Epoch [78/120    avg_loss:0.021, val_acc:0.972]
Epoch [79/120    avg_loss:0.025, val_acc:0.972]
Epoch [80/120    avg_loss:0.029, val_acc:0.972]
Epoch [81/120    avg_loss:0.022, val_acc:0.972]
Epoch [82/120    avg_loss:0.020, val_acc:0.972]
Epoch [83/120    avg_loss:0.021, val_acc:0.972]
Epoch [84/120    avg_loss:0.020, val_acc:0.972]
Epoch [85/120    avg_loss:0.022, val_acc:0.972]
Epoch [86/120    avg_loss:0.027, val_acc:0.972]
Epoch [87/120    avg_loss:0.021, val_acc:0.972]
Epoch [88/120    avg_loss:0.019, val_acc:0.972]
Epoch [89/120    avg_loss:0.026, val_acc:0.972]
Epoch [90/120    avg_loss:0.027, val_acc:0.972]
Epoch [91/120    avg_loss:0.017, val_acc:0.972]
Epoch [92/120    avg_loss:0.022, val_acc:0.972]
Epoch [93/120    avg_loss:0.020, val_acc:0.972]
Epoch [94/120    avg_loss:0.032, val_acc:0.972]
Epoch [95/120    avg_loss:0.021, val_acc:0.972]
Epoch [96/120    avg_loss:0.021, val_acc:0.972]
Epoch [97/120    avg_loss:0.028, val_acc:0.972]
Epoch [98/120    avg_loss:0.019, val_acc:0.972]
Epoch [99/120    avg_loss:0.021, val_acc:0.972]
Epoch [100/120    avg_loss:0.028, val_acc:0.972]
Epoch [101/120    avg_loss:0.029, val_acc:0.972]
Epoch [102/120    avg_loss:0.019, val_acc:0.972]
Epoch [103/120    avg_loss:0.021, val_acc:0.972]
Epoch [104/120    avg_loss:0.024, val_acc:0.972]
Epoch [105/120    avg_loss:0.023, val_acc:0.972]
Epoch [106/120    avg_loss:0.022, val_acc:0.972]
Epoch [107/120    avg_loss:0.020, val_acc:0.972]
Epoch [108/120    avg_loss:0.017, val_acc:0.972]
Epoch [109/120    avg_loss:0.024, val_acc:0.972]
Epoch [110/120    avg_loss:0.021, val_acc:0.972]
Epoch [111/120    avg_loss:0.020, val_acc:0.972]
Epoch [112/120    avg_loss:0.019, val_acc:0.972]
Epoch [113/120    avg_loss:0.021, val_acc:0.972]
Epoch [114/120    avg_loss:0.018, val_acc:0.972]
Epoch [115/120    avg_loss:0.018, val_acc:0.972]
Epoch [116/120    avg_loss:0.021, val_acc:0.972]
Epoch [117/120    avg_loss:0.028, val_acc:0.972]
Epoch [118/120    avg_loss:0.017, val_acc:0.972]
Epoch [119/120    avg_loss:0.024, val_acc:0.972]
Epoch [120/120    avg_loss:0.022, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  16   0   0   0   0   0   0   0   0]
 [  0   0   0   0  10 135   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0  10   0   0   0   0  84   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.21108742004265

F1 scores:
[       nan 1.         0.97767857 1.         0.94196429 0.91216216
 1.         0.94382022 1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9912165154008858
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb588f7e748>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.224, val_acc:0.558]
Epoch [2/120    avg_loss:1.595, val_acc:0.728]
Epoch [3/120    avg_loss:1.201, val_acc:0.792]
Epoch [4/120    avg_loss:0.919, val_acc:0.778]
Epoch [5/120    avg_loss:0.837, val_acc:0.794]
Epoch [6/120    avg_loss:0.774, val_acc:0.754]
Epoch [7/120    avg_loss:0.694, val_acc:0.849]
Epoch [8/120    avg_loss:0.609, val_acc:0.845]
Epoch [9/120    avg_loss:0.525, val_acc:0.863]
Epoch [10/120    avg_loss:0.553, val_acc:0.885]
Epoch [11/120    avg_loss:0.522, val_acc:0.877]
Epoch [12/120    avg_loss:0.484, val_acc:0.881]
Epoch [13/120    avg_loss:0.506, val_acc:0.863]
Epoch [14/120    avg_loss:0.410, val_acc:0.907]
Epoch [15/120    avg_loss:0.397, val_acc:0.891]
Epoch [16/120    avg_loss:0.411, val_acc:0.903]
Epoch [17/120    avg_loss:0.402, val_acc:0.917]
Epoch [18/120    avg_loss:0.373, val_acc:0.883]
Epoch [19/120    avg_loss:0.347, val_acc:0.915]
Epoch [20/120    avg_loss:0.307, val_acc:0.935]
Epoch [21/120    avg_loss:0.301, val_acc:0.942]
Epoch [22/120    avg_loss:0.330, val_acc:0.913]
Epoch [23/120    avg_loss:0.280, val_acc:0.929]
Epoch [24/120    avg_loss:0.242, val_acc:0.946]
Epoch [25/120    avg_loss:0.238, val_acc:0.938]
Epoch [26/120    avg_loss:0.181, val_acc:0.927]
Epoch [27/120    avg_loss:0.226, val_acc:0.938]
Epoch [28/120    avg_loss:0.203, val_acc:0.929]
Epoch [29/120    avg_loss:0.247, val_acc:0.921]
Epoch [30/120    avg_loss:0.266, val_acc:0.944]
Epoch [31/120    avg_loss:0.198, val_acc:0.954]
Epoch [32/120    avg_loss:0.215, val_acc:0.927]
Epoch [33/120    avg_loss:0.216, val_acc:0.933]
Epoch [34/120    avg_loss:0.162, val_acc:0.952]
Epoch [35/120    avg_loss:0.240, val_acc:0.948]
Epoch [36/120    avg_loss:0.326, val_acc:0.929]
Epoch [37/120    avg_loss:0.249, val_acc:0.954]
Epoch [38/120    avg_loss:0.175, val_acc:0.907]
Epoch [39/120    avg_loss:0.193, val_acc:0.954]
Epoch [40/120    avg_loss:0.142, val_acc:0.966]
Epoch [41/120    avg_loss:0.143, val_acc:0.968]
Epoch [42/120    avg_loss:0.126, val_acc:0.962]
Epoch [43/120    avg_loss:0.142, val_acc:0.976]
Epoch [44/120    avg_loss:0.110, val_acc:0.938]
Epoch [45/120    avg_loss:0.139, val_acc:0.964]
Epoch [46/120    avg_loss:0.149, val_acc:0.958]
Epoch [47/120    avg_loss:0.105, val_acc:0.972]
Epoch [48/120    avg_loss:0.101, val_acc:0.968]
Epoch [49/120    avg_loss:0.093, val_acc:0.960]
Epoch [50/120    avg_loss:0.117, val_acc:0.974]
Epoch [51/120    avg_loss:0.103, val_acc:0.972]
Epoch [52/120    avg_loss:0.090, val_acc:0.982]
Epoch [53/120    avg_loss:0.084, val_acc:0.962]
Epoch [54/120    avg_loss:0.104, val_acc:0.966]
Epoch [55/120    avg_loss:0.078, val_acc:0.978]
Epoch [56/120    avg_loss:0.065, val_acc:0.970]
Epoch [57/120    avg_loss:0.055, val_acc:0.972]
Epoch [58/120    avg_loss:0.077, val_acc:0.980]
Epoch [59/120    avg_loss:0.052, val_acc:0.984]
Epoch [60/120    avg_loss:0.060, val_acc:0.978]
Epoch [61/120    avg_loss:0.049, val_acc:0.972]
Epoch [62/120    avg_loss:0.038, val_acc:0.982]
Epoch [63/120    avg_loss:0.055, val_acc:0.976]
Epoch [64/120    avg_loss:0.080, val_acc:0.976]
Epoch [65/120    avg_loss:0.067, val_acc:0.982]
Epoch [66/120    avg_loss:0.065, val_acc:0.978]
Epoch [67/120    avg_loss:0.078, val_acc:0.992]
Epoch [68/120    avg_loss:0.049, val_acc:0.982]
Epoch [69/120    avg_loss:0.030, val_acc:0.982]
Epoch [70/120    avg_loss:0.050, val_acc:0.982]
Epoch [71/120    avg_loss:0.038, val_acc:0.980]
Epoch [72/120    avg_loss:0.023, val_acc:0.988]
Epoch [73/120    avg_loss:0.028, val_acc:0.990]
Epoch [74/120    avg_loss:0.033, val_acc:0.986]
Epoch [75/120    avg_loss:0.044, val_acc:0.974]
Epoch [76/120    avg_loss:0.060, val_acc:0.970]
Epoch [77/120    avg_loss:0.059, val_acc:0.972]
Epoch [78/120    avg_loss:0.039, val_acc:0.972]
Epoch [79/120    avg_loss:0.043, val_acc:0.986]
Epoch [80/120    avg_loss:0.051, val_acc:0.988]
Epoch [81/120    avg_loss:0.027, val_acc:0.990]
Epoch [82/120    avg_loss:0.014, val_acc:0.990]
Epoch [83/120    avg_loss:0.016, val_acc:0.990]
Epoch [84/120    avg_loss:0.022, val_acc:0.988]
Epoch [85/120    avg_loss:0.015, val_acc:0.988]
Epoch [86/120    avg_loss:0.016, val_acc:0.990]
Epoch [87/120    avg_loss:0.012, val_acc:0.990]
Epoch [88/120    avg_loss:0.015, val_acc:0.990]
Epoch [89/120    avg_loss:0.014, val_acc:0.990]
Epoch [90/120    avg_loss:0.016, val_acc:0.990]
Epoch [91/120    avg_loss:0.011, val_acc:0.990]
Epoch [92/120    avg_loss:0.020, val_acc:0.990]
Epoch [93/120    avg_loss:0.013, val_acc:0.990]
Epoch [94/120    avg_loss:0.013, val_acc:0.990]
Epoch [95/120    avg_loss:0.009, val_acc:0.990]
Epoch [96/120    avg_loss:0.013, val_acc:0.990]
Epoch [97/120    avg_loss:0.011, val_acc:0.990]
Epoch [98/120    avg_loss:0.014, val_acc:0.990]
Epoch [99/120    avg_loss:0.013, val_acc:0.990]
Epoch [100/120    avg_loss:0.012, val_acc:0.990]
Epoch [101/120    avg_loss:0.011, val_acc:0.990]
Epoch [102/120    avg_loss:0.010, val_acc:0.990]
Epoch [103/120    avg_loss:0.014, val_acc:0.990]
Epoch [104/120    avg_loss:0.022, val_acc:0.990]
Epoch [105/120    avg_loss:0.009, val_acc:0.990]
Epoch [106/120    avg_loss:0.014, val_acc:0.990]
Epoch [107/120    avg_loss:0.009, val_acc:0.990]
Epoch [108/120    avg_loss:0.009, val_acc:0.990]
Epoch [109/120    avg_loss:0.011, val_acc:0.990]
Epoch [110/120    avg_loss:0.014, val_acc:0.990]
Epoch [111/120    avg_loss:0.013, val_acc:0.990]
Epoch [112/120    avg_loss:0.013, val_acc:0.990]
Epoch [113/120    avg_loss:0.020, val_acc:0.990]
Epoch [114/120    avg_loss:0.016, val_acc:0.990]
Epoch [115/120    avg_loss:0.011, val_acc:0.990]
Epoch [116/120    avg_loss:0.012, val_acc:0.990]
Epoch [117/120    avg_loss:0.011, val_acc:0.990]
Epoch [118/120    avg_loss:0.012, val_acc:0.990]
Epoch [119/120    avg_loss:0.011, val_acc:0.990]
Epoch [120/120    avg_loss:0.016, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 229   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 217  10   0   0   0   0   0   0   0   0]
 [  0   0   0   0  17 128   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 1.         0.9977221  0.99782135 0.94143167 0.90459364
 1.         1.         1.         1.         1.         0.99867198
 0.99889746 1.        ]

Kappa:
0.9931157151555144
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:22:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f522e0fe828>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.258, val_acc:0.615]
Epoch [2/120    avg_loss:1.610, val_acc:0.665]
Epoch [3/120    avg_loss:1.232, val_acc:0.778]
Epoch [4/120    avg_loss:0.979, val_acc:0.752]
Epoch [5/120    avg_loss:0.898, val_acc:0.748]
Epoch [6/120    avg_loss:0.719, val_acc:0.792]
Epoch [7/120    avg_loss:0.692, val_acc:0.839]
Epoch [8/120    avg_loss:0.648, val_acc:0.857]
Epoch [9/120    avg_loss:0.594, val_acc:0.829]
Epoch [10/120    avg_loss:0.532, val_acc:0.887]
Epoch [11/120    avg_loss:0.512, val_acc:0.859]
Epoch [12/120    avg_loss:0.613, val_acc:0.847]
Epoch [13/120    avg_loss:0.558, val_acc:0.905]
Epoch [14/120    avg_loss:0.444, val_acc:0.893]
Epoch [15/120    avg_loss:0.396, val_acc:0.893]
Epoch [16/120    avg_loss:0.463, val_acc:0.851]
Epoch [17/120    avg_loss:0.377, val_acc:0.869]
Epoch [18/120    avg_loss:0.363, val_acc:0.909]
Epoch [19/120    avg_loss:0.394, val_acc:0.873]
Epoch [20/120    avg_loss:0.359, val_acc:0.933]
Epoch [21/120    avg_loss:0.321, val_acc:0.923]
Epoch [22/120    avg_loss:0.291, val_acc:0.873]
Epoch [23/120    avg_loss:0.315, val_acc:0.907]
Epoch [24/120    avg_loss:0.343, val_acc:0.875]
Epoch [25/120    avg_loss:0.358, val_acc:0.946]
Epoch [26/120    avg_loss:0.305, val_acc:0.921]
Epoch [27/120    avg_loss:0.369, val_acc:0.901]
Epoch [28/120    avg_loss:0.264, val_acc:0.935]
Epoch [29/120    avg_loss:0.210, val_acc:0.942]
Epoch [30/120    avg_loss:0.246, val_acc:0.933]
Epoch [31/120    avg_loss:0.262, val_acc:0.931]
Epoch [32/120    avg_loss:0.249, val_acc:0.940]
Epoch [33/120    avg_loss:0.217, val_acc:0.952]
Epoch [34/120    avg_loss:0.197, val_acc:0.952]
Epoch [35/120    avg_loss:0.219, val_acc:0.935]
Epoch [36/120    avg_loss:0.190, val_acc:0.942]
Epoch [37/120    avg_loss:0.163, val_acc:0.958]
Epoch [38/120    avg_loss:0.215, val_acc:0.933]
Epoch [39/120    avg_loss:0.186, val_acc:0.956]
Epoch [40/120    avg_loss:0.198, val_acc:0.968]
Epoch [41/120    avg_loss:0.204, val_acc:0.944]
Epoch [42/120    avg_loss:0.144, val_acc:0.948]
Epoch [43/120    avg_loss:0.147, val_acc:0.921]
Epoch [44/120    avg_loss:0.148, val_acc:0.964]
Epoch [45/120    avg_loss:0.165, val_acc:0.948]
Epoch [46/120    avg_loss:0.124, val_acc:0.968]
Epoch [47/120    avg_loss:0.122, val_acc:0.952]
Epoch [48/120    avg_loss:0.107, val_acc:0.935]
Epoch [49/120    avg_loss:0.136, val_acc:0.968]
Epoch [50/120    avg_loss:0.097, val_acc:0.974]
Epoch [51/120    avg_loss:0.092, val_acc:0.976]
Epoch [52/120    avg_loss:0.094, val_acc:0.966]
Epoch [53/120    avg_loss:0.079, val_acc:0.974]
Epoch [54/120    avg_loss:0.092, val_acc:0.976]
Epoch [55/120    avg_loss:0.059, val_acc:0.976]
Epoch [56/120    avg_loss:0.055, val_acc:0.960]
Epoch [57/120    avg_loss:0.055, val_acc:0.980]
Epoch [58/120    avg_loss:0.034, val_acc:0.978]
Epoch [59/120    avg_loss:0.066, val_acc:0.980]
Epoch [60/120    avg_loss:0.041, val_acc:0.978]
Epoch [61/120    avg_loss:0.029, val_acc:0.976]
Epoch [62/120    avg_loss:0.052, val_acc:0.962]
Epoch [63/120    avg_loss:0.150, val_acc:0.972]
Epoch [64/120    avg_loss:0.121, val_acc:0.948]
Epoch [65/120    avg_loss:0.123, val_acc:0.960]
Epoch [66/120    avg_loss:0.088, val_acc:0.974]
Epoch [67/120    avg_loss:0.040, val_acc:0.974]
Epoch [68/120    avg_loss:0.032, val_acc:0.978]
Epoch [69/120    avg_loss:0.048, val_acc:0.978]
Epoch [70/120    avg_loss:0.043, val_acc:0.970]
Epoch [71/120    avg_loss:0.039, val_acc:0.966]
Epoch [72/120    avg_loss:0.034, val_acc:0.984]
Epoch [73/120    avg_loss:0.041, val_acc:0.972]
Epoch [74/120    avg_loss:0.026, val_acc:0.980]
Epoch [75/120    avg_loss:0.020, val_acc:0.978]
Epoch [76/120    avg_loss:0.012, val_acc:0.980]
Epoch [77/120    avg_loss:0.015, val_acc:0.984]
Epoch [78/120    avg_loss:0.018, val_acc:0.986]
Epoch [79/120    avg_loss:0.027, val_acc:0.976]
Epoch [80/120    avg_loss:0.046, val_acc:0.976]
Epoch [81/120    avg_loss:0.032, val_acc:0.980]
Epoch [82/120    avg_loss:0.017, val_acc:0.982]
Epoch [83/120    avg_loss:0.014, val_acc:0.982]
Epoch [84/120    avg_loss:0.011, val_acc:0.982]
Epoch [85/120    avg_loss:0.011, val_acc:0.982]
Epoch [86/120    avg_loss:0.012, val_acc:0.974]
Epoch [87/120    avg_loss:0.055, val_acc:0.978]
Epoch [88/120    avg_loss:0.015, val_acc:0.976]
Epoch [89/120    avg_loss:0.033, val_acc:0.980]
Epoch [90/120    avg_loss:0.016, val_acc:0.982]
Epoch [91/120    avg_loss:0.012, val_acc:0.986]
Epoch [92/120    avg_loss:0.014, val_acc:0.978]
Epoch [93/120    avg_loss:0.009, val_acc:0.984]
Epoch [94/120    avg_loss:0.007, val_acc:0.982]
Epoch [95/120    avg_loss:0.015, val_acc:0.954]
Epoch [96/120    avg_loss:0.022, val_acc:0.982]
Epoch [97/120    avg_loss:0.009, val_acc:0.984]
Epoch [98/120    avg_loss:0.012, val_acc:0.978]
Epoch [99/120    avg_loss:0.009, val_acc:0.984]
Epoch [100/120    avg_loss:0.006, val_acc:0.984]
Epoch [101/120    avg_loss:0.012, val_acc:0.980]
Epoch [102/120    avg_loss:0.014, val_acc:0.982]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.982]
Epoch [105/120    avg_loss:0.005, val_acc:0.982]
Epoch [106/120    avg_loss:0.017, val_acc:0.984]
Epoch [107/120    avg_loss:0.004, val_acc:0.984]
Epoch [108/120    avg_loss:0.005, val_acc:0.984]
Epoch [109/120    avg_loss:0.005, val_acc:0.984]
Epoch [110/120    avg_loss:0.004, val_acc:0.984]
Epoch [111/120    avg_loss:0.005, val_acc:0.984]
Epoch [112/120    avg_loss:0.005, val_acc:0.984]
Epoch [113/120    avg_loss:0.005, val_acc:0.984]
Epoch [114/120    avg_loss:0.004, val_acc:0.984]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.006, val_acc:0.984]
Epoch [117/120    avg_loss:0.004, val_acc:0.984]
Epoch [118/120    avg_loss:0.003, val_acc:0.984]
Epoch [119/120    avg_loss:0.005, val_acc:0.984]
Epoch [120/120    avg_loss:0.004, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 215  14   0   0   0   0   1   0   0   0   0]
 [  0   0   0   0 214  12   0   0   0   0   0   0   1   0]
 [  0   0   0   3   0 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.33901918976545

F1 scores:
[       nan 1.         1.         0.95982143 0.94065934 0.94983278
 1.         1.         1.         0.99893276 1.         1.
 0.99889746 1.        ]

Kappa:
0.9926412892457191
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f259a8ee7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 84487==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.278, val_acc:0.579]
Epoch [2/120    avg_loss:1.688, val_acc:0.627]
Epoch [3/120    avg_loss:1.270, val_acc:0.647]
Epoch [4/120    avg_loss:1.039, val_acc:0.808]
Epoch [5/120    avg_loss:0.866, val_acc:0.752]
Epoch [6/120    avg_loss:0.714, val_acc:0.823]
Epoch [7/120    avg_loss:0.706, val_acc:0.794]
Epoch [8/120    avg_loss:0.650, val_acc:0.891]
Epoch [9/120    avg_loss:0.592, val_acc:0.879]
Epoch [10/120    avg_loss:0.503, val_acc:0.865]
Epoch [11/120    avg_loss:0.503, val_acc:0.873]
Epoch [12/120    avg_loss:0.463, val_acc:0.899]
Epoch [13/120    avg_loss:0.438, val_acc:0.893]
Epoch [14/120    avg_loss:0.413, val_acc:0.923]
Epoch [15/120    avg_loss:0.415, val_acc:0.885]
Epoch [16/120    avg_loss:0.447, val_acc:0.899]
Epoch [17/120    avg_loss:0.358, val_acc:0.917]
Epoch [18/120    avg_loss:0.317, val_acc:0.919]
Epoch [19/120    avg_loss:0.283, val_acc:0.946]
Epoch [20/120    avg_loss:0.265, val_acc:0.887]
Epoch [21/120    avg_loss:0.392, val_acc:0.917]
Epoch [22/120    avg_loss:0.320, val_acc:0.938]
Epoch [23/120    avg_loss:0.287, val_acc:0.942]
Epoch [24/120    avg_loss:0.215, val_acc:0.915]
Epoch [25/120    avg_loss:0.268, val_acc:0.911]
Epoch [26/120    avg_loss:0.242, val_acc:0.946]
Epoch [27/120    avg_loss:0.238, val_acc:0.925]
Epoch [28/120    avg_loss:0.276, val_acc:0.899]
Epoch [29/120    avg_loss:0.349, val_acc:0.921]
Epoch [30/120    avg_loss:0.235, val_acc:0.954]
Epoch [31/120    avg_loss:0.226, val_acc:0.899]
Epoch [32/120    avg_loss:0.258, val_acc:0.946]
Epoch [33/120    avg_loss:0.190, val_acc:0.929]
Epoch [34/120    avg_loss:0.186, val_acc:0.958]
Epoch [35/120    avg_loss:0.166, val_acc:0.948]
Epoch [36/120    avg_loss:0.156, val_acc:0.944]
Epoch [37/120    avg_loss:0.153, val_acc:0.968]
Epoch [38/120    avg_loss:0.144, val_acc:0.942]
Epoch [39/120    avg_loss:0.142, val_acc:0.956]
Epoch [40/120    avg_loss:0.134, val_acc:0.960]
Epoch [41/120    avg_loss:0.106, val_acc:0.962]
Epoch [42/120    avg_loss:0.106, val_acc:0.960]
Epoch [43/120    avg_loss:0.094, val_acc:0.980]
Epoch [44/120    avg_loss:0.060, val_acc:0.976]
Epoch [45/120    avg_loss:0.159, val_acc:0.917]
Epoch [46/120    avg_loss:0.207, val_acc:0.935]
Epoch [47/120    avg_loss:0.122, val_acc:0.970]
Epoch [48/120    avg_loss:0.088, val_acc:0.980]
Epoch [49/120    avg_loss:0.079, val_acc:0.972]
Epoch [50/120    avg_loss:0.073, val_acc:0.986]
Epoch [51/120    avg_loss:0.058, val_acc:0.978]
Epoch [52/120    avg_loss:0.051, val_acc:0.968]
Epoch [53/120    avg_loss:0.056, val_acc:0.974]
Epoch [54/120    avg_loss:0.078, val_acc:0.978]
Epoch [55/120    avg_loss:0.047, val_acc:0.978]
Epoch [56/120    avg_loss:0.035, val_acc:0.990]
Epoch [57/120    avg_loss:0.037, val_acc:0.978]
Epoch [58/120    avg_loss:0.045, val_acc:0.978]
Epoch [59/120    avg_loss:0.046, val_acc:0.984]
Epoch [60/120    avg_loss:0.037, val_acc:0.986]
Epoch [61/120    avg_loss:0.055, val_acc:0.974]
Epoch [62/120    avg_loss:0.058, val_acc:0.972]
Epoch [63/120    avg_loss:0.099, val_acc:0.958]
Epoch [64/120    avg_loss:0.091, val_acc:0.946]
Epoch [65/120    avg_loss:0.163, val_acc:0.935]
Epoch [66/120    avg_loss:0.189, val_acc:0.958]
Epoch [67/120    avg_loss:0.065, val_acc:0.980]
Epoch [68/120    avg_loss:0.036, val_acc:0.990]
Epoch [69/120    avg_loss:0.019, val_acc:0.988]
Epoch [70/120    avg_loss:0.022, val_acc:0.988]
Epoch [71/120    avg_loss:0.035, val_acc:0.988]
Epoch [72/120    avg_loss:0.022, val_acc:0.986]
Epoch [73/120    avg_loss:0.029, val_acc:0.962]
Epoch [74/120    avg_loss:0.076, val_acc:0.954]
Epoch [75/120    avg_loss:0.045, val_acc:0.978]
Epoch [76/120    avg_loss:0.079, val_acc:0.966]
Epoch [77/120    avg_loss:0.058, val_acc:0.988]
Epoch [78/120    avg_loss:0.024, val_acc:0.986]
Epoch [79/120    avg_loss:0.027, val_acc:0.974]
Epoch [80/120    avg_loss:0.026, val_acc:0.984]
Epoch [81/120    avg_loss:0.021, val_acc:0.958]
Epoch [82/120    avg_loss:0.034, val_acc:0.976]
Epoch [83/120    avg_loss:0.017, val_acc:0.978]
Epoch [84/120    avg_loss:0.019, val_acc:0.980]
Epoch [85/120    avg_loss:0.014, val_acc:0.988]
Epoch [86/120    avg_loss:0.013, val_acc:0.988]
Epoch [87/120    avg_loss:0.016, val_acc:0.988]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.015, val_acc:0.990]
Epoch [90/120    avg_loss:0.010, val_acc:0.988]
Epoch [91/120    avg_loss:0.007, val_acc:0.988]
Epoch [92/120    avg_loss:0.011, val_acc:0.988]
Epoch [93/120    avg_loss:0.011, val_acc:0.988]
Epoch [94/120    avg_loss:0.009, val_acc:0.990]
Epoch [95/120    avg_loss:0.008, val_acc:0.990]
Epoch [96/120    avg_loss:0.013, val_acc:0.990]
Epoch [97/120    avg_loss:0.010, val_acc:0.992]
Epoch [98/120    avg_loss:0.006, val_acc:0.992]
Epoch [99/120    avg_loss:0.012, val_acc:0.990]
Epoch [100/120    avg_loss:0.009, val_acc:0.988]
Epoch [101/120    avg_loss:0.009, val_acc:0.988]
Epoch [102/120    avg_loss:0.016, val_acc:0.990]
Epoch [103/120    avg_loss:0.010, val_acc:0.990]
Epoch [104/120    avg_loss:0.008, val_acc:0.990]
Epoch [105/120    avg_loss:0.023, val_acc:0.992]
Epoch [106/120    avg_loss:0.009, val_acc:0.992]
Epoch [107/120    avg_loss:0.017, val_acc:0.994]
Epoch [108/120    avg_loss:0.009, val_acc:0.994]
Epoch [109/120    avg_loss:0.008, val_acc:0.990]
Epoch [110/120    avg_loss:0.008, val_acc:0.990]
Epoch [111/120    avg_loss:0.016, val_acc:0.988]
Epoch [112/120    avg_loss:0.011, val_acc:0.990]
Epoch [113/120    avg_loss:0.009, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.992]
Epoch [115/120    avg_loss:0.008, val_acc:0.990]
Epoch [116/120    avg_loss:0.007, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.009, val_acc:0.988]
Epoch [119/120    avg_loss:0.016, val_acc:0.990]
Epoch [120/120    avg_loss:0.009, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   1 227   0   0   0   0   2   0   0   0   0   0]
 [  0   0   0   0 215  12   0   0   0   0   0   0   0   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.48827292110875

F1 scores:
[       nan 1.         0.9977221  0.99343545 0.95343681 0.92832765
 1.         1.         0.99742931 1.         1.         1.
 1.         1.        ]

Kappa:
0.9943028280718936
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff10702e7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.289, val_acc:0.599]
Epoch [2/120    avg_loss:1.594, val_acc:0.671]
Epoch [3/120    avg_loss:1.233, val_acc:0.736]
Epoch [4/120    avg_loss:0.990, val_acc:0.792]
Epoch [5/120    avg_loss:0.894, val_acc:0.790]
Epoch [6/120    avg_loss:0.802, val_acc:0.843]
Epoch [7/120    avg_loss:0.736, val_acc:0.843]
Epoch [8/120    avg_loss:0.691, val_acc:0.863]
Epoch [9/120    avg_loss:0.605, val_acc:0.901]
Epoch [10/120    avg_loss:0.576, val_acc:0.887]
Epoch [11/120    avg_loss:0.509, val_acc:0.929]
Epoch [12/120    avg_loss:0.459, val_acc:0.891]
Epoch [13/120    avg_loss:0.432, val_acc:0.935]
Epoch [14/120    avg_loss:0.385, val_acc:0.909]
Epoch [15/120    avg_loss:0.356, val_acc:0.931]
Epoch [16/120    avg_loss:0.350, val_acc:0.879]
Epoch [17/120    avg_loss:0.331, val_acc:0.857]
Epoch [18/120    avg_loss:0.349, val_acc:0.938]
Epoch [19/120    avg_loss:0.391, val_acc:0.885]
Epoch [20/120    avg_loss:0.330, val_acc:0.950]
Epoch [21/120    avg_loss:0.298, val_acc:0.946]
Epoch [22/120    avg_loss:0.273, val_acc:0.921]
Epoch [23/120    avg_loss:0.295, val_acc:0.935]
Epoch [24/120    avg_loss:0.330, val_acc:0.883]
Epoch [25/120    avg_loss:0.285, val_acc:0.938]
Epoch [26/120    avg_loss:0.256, val_acc:0.917]
Epoch [27/120    avg_loss:0.272, val_acc:0.954]
Epoch [28/120    avg_loss:0.220, val_acc:0.958]
Epoch [29/120    avg_loss:0.196, val_acc:0.966]
Epoch [30/120    avg_loss:0.143, val_acc:0.954]
Epoch [31/120    avg_loss:0.178, val_acc:0.960]
Epoch [32/120    avg_loss:0.210, val_acc:0.976]
Epoch [33/120    avg_loss:0.166, val_acc:0.962]
Epoch [34/120    avg_loss:0.150, val_acc:0.968]
Epoch [35/120    avg_loss:0.122, val_acc:0.976]
Epoch [36/120    avg_loss:0.120, val_acc:0.966]
Epoch [37/120    avg_loss:0.132, val_acc:0.970]
Epoch [38/120    avg_loss:0.098, val_acc:0.978]
Epoch [39/120    avg_loss:0.129, val_acc:0.986]
Epoch [40/120    avg_loss:0.105, val_acc:0.974]
Epoch [41/120    avg_loss:0.129, val_acc:0.972]
Epoch [42/120    avg_loss:0.121, val_acc:0.976]
Epoch [43/120    avg_loss:0.065, val_acc:0.978]
Epoch [44/120    avg_loss:0.080, val_acc:0.982]
Epoch [45/120    avg_loss:0.076, val_acc:0.968]
Epoch [46/120    avg_loss:0.168, val_acc:0.974]
Epoch [47/120    avg_loss:0.109, val_acc:0.992]
Epoch [48/120    avg_loss:0.082, val_acc:0.976]
Epoch [49/120    avg_loss:0.075, val_acc:0.986]
Epoch [50/120    avg_loss:0.065, val_acc:0.976]
Epoch [51/120    avg_loss:0.066, val_acc:0.978]
Epoch [52/120    avg_loss:0.078, val_acc:0.970]
Epoch [53/120    avg_loss:0.076, val_acc:0.988]
Epoch [54/120    avg_loss:0.063, val_acc:0.980]
Epoch [55/120    avg_loss:0.050, val_acc:0.982]
Epoch [56/120    avg_loss:0.054, val_acc:0.976]
Epoch [57/120    avg_loss:0.070, val_acc:0.990]
Epoch [58/120    avg_loss:0.044, val_acc:0.998]
Epoch [59/120    avg_loss:0.038, val_acc:0.994]
Epoch [60/120    avg_loss:0.051, val_acc:0.996]
Epoch [61/120    avg_loss:0.050, val_acc:0.984]
Epoch [62/120    avg_loss:0.058, val_acc:0.996]
Epoch [63/120    avg_loss:0.036, val_acc:0.992]
Epoch [64/120    avg_loss:0.039, val_acc:0.982]
Epoch [65/120    avg_loss:0.029, val_acc:0.992]
Epoch [66/120    avg_loss:0.036, val_acc:0.990]
Epoch [67/120    avg_loss:0.039, val_acc:0.990]
Epoch [68/120    avg_loss:0.035, val_acc:0.994]
Epoch [69/120    avg_loss:0.018, val_acc:0.992]
Epoch [70/120    avg_loss:0.020, val_acc:0.992]
Epoch [71/120    avg_loss:0.014, val_acc:0.994]
Epoch [72/120    avg_loss:0.015, val_acc:0.994]
Epoch [73/120    avg_loss:0.021, val_acc:0.996]
Epoch [74/120    avg_loss:0.012, val_acc:0.996]
Epoch [75/120    avg_loss:0.014, val_acc:0.996]
Epoch [76/120    avg_loss:0.014, val_acc:0.996]
Epoch [77/120    avg_loss:0.010, val_acc:0.996]
Epoch [78/120    avg_loss:0.017, val_acc:0.996]
Epoch [79/120    avg_loss:0.012, val_acc:0.996]
Epoch [80/120    avg_loss:0.010, val_acc:0.996]
Epoch [81/120    avg_loss:0.023, val_acc:0.996]
Epoch [82/120    avg_loss:0.010, val_acc:0.996]
Epoch [83/120    avg_loss:0.012, val_acc:0.996]
Epoch [84/120    avg_loss:0.011, val_acc:0.996]
Epoch [85/120    avg_loss:0.009, val_acc:0.996]
Epoch [86/120    avg_loss:0.020, val_acc:0.996]
Epoch [87/120    avg_loss:0.013, val_acc:0.996]
Epoch [88/120    avg_loss:0.013, val_acc:0.996]
Epoch [89/120    avg_loss:0.016, val_acc:0.996]
Epoch [90/120    avg_loss:0.010, val_acc:0.996]
Epoch [91/120    avg_loss:0.010, val_acc:0.996]
Epoch [92/120    avg_loss:0.010, val_acc:0.996]
Epoch [93/120    avg_loss:0.012, val_acc:0.996]
Epoch [94/120    avg_loss:0.015, val_acc:0.996]
Epoch [95/120    avg_loss:0.011, val_acc:0.996]
Epoch [96/120    avg_loss:0.011, val_acc:0.996]
Epoch [97/120    avg_loss:0.009, val_acc:0.996]
Epoch [98/120    avg_loss:0.012, val_acc:0.996]
Epoch [99/120    avg_loss:0.011, val_acc:0.996]
Epoch [100/120    avg_loss:0.009, val_acc:0.996]
Epoch [101/120    avg_loss:0.009, val_acc:0.996]
Epoch [102/120    avg_loss:0.010, val_acc:0.996]
Epoch [103/120    avg_loss:0.015, val_acc:0.996]
Epoch [104/120    avg_loss:0.017, val_acc:0.996]
Epoch [105/120    avg_loss:0.010, val_acc:0.996]
Epoch [106/120    avg_loss:0.011, val_acc:0.996]
Epoch [107/120    avg_loss:0.013, val_acc:0.996]
Epoch [108/120    avg_loss:0.010, val_acc:0.996]
Epoch [109/120    avg_loss:0.013, val_acc:0.996]
Epoch [110/120    avg_loss:0.015, val_acc:0.996]
Epoch [111/120    avg_loss:0.009, val_acc:0.996]
Epoch [112/120    avg_loss:0.012, val_acc:0.996]
Epoch [113/120    avg_loss:0.011, val_acc:0.996]
Epoch [114/120    avg_loss:0.011, val_acc:0.996]
Epoch [115/120    avg_loss:0.014, val_acc:0.996]
Epoch [116/120    avg_loss:0.011, val_acc:0.996]
Epoch [117/120    avg_loss:0.010, val_acc:0.996]
Epoch [118/120    avg_loss:0.012, val_acc:0.996]
Epoch [119/120    avg_loss:0.010, val_acc:0.996]
Epoch [120/120    avg_loss:0.013, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 211  13   0   0   0   0   0   0   3   0]
 [  0   0   0   0   4 141   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   2 451   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.53091684434968

F1 scores:
[       nan 1.         1.         1.         0.95475113 0.94314381
 1.         1.         1.         1.         1.         0.9973545
 0.99448732 1.        ]

Kappa:
0.9947776640316089
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5448eb97b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.258, val_acc:0.583]
Epoch [2/120    avg_loss:1.648, val_acc:0.671]
Epoch [3/120    avg_loss:1.258, val_acc:0.752]
Epoch [4/120    avg_loss:1.025, val_acc:0.714]
Epoch [5/120    avg_loss:0.849, val_acc:0.817]
Epoch [6/120    avg_loss:0.657, val_acc:0.851]
Epoch [7/120    avg_loss:0.644, val_acc:0.835]
Epoch [8/120    avg_loss:0.558, val_acc:0.881]
Epoch [9/120    avg_loss:0.590, val_acc:0.879]
Epoch [10/120    avg_loss:0.513, val_acc:0.899]
Epoch [11/120    avg_loss:0.409, val_acc:0.871]
Epoch [12/120    avg_loss:0.374, val_acc:0.913]
Epoch [13/120    avg_loss:0.428, val_acc:0.927]
Epoch [14/120    avg_loss:0.354, val_acc:0.911]
Epoch [15/120    avg_loss:0.383, val_acc:0.913]
Epoch [16/120    avg_loss:0.312, val_acc:0.933]
Epoch [17/120    avg_loss:0.318, val_acc:0.917]
Epoch [18/120    avg_loss:0.283, val_acc:0.921]
Epoch [19/120    avg_loss:0.255, val_acc:0.909]
Epoch [20/120    avg_loss:0.245, val_acc:0.933]
Epoch [21/120    avg_loss:0.248, val_acc:0.948]
Epoch [22/120    avg_loss:0.309, val_acc:0.893]
Epoch [23/120    avg_loss:0.231, val_acc:0.954]
Epoch [24/120    avg_loss:0.239, val_acc:0.968]
Epoch [25/120    avg_loss:0.217, val_acc:0.923]
Epoch [26/120    avg_loss:0.237, val_acc:0.948]
Epoch [27/120    avg_loss:0.160, val_acc:0.938]
Epoch [28/120    avg_loss:0.173, val_acc:0.919]
Epoch [29/120    avg_loss:0.167, val_acc:0.958]
Epoch [30/120    avg_loss:0.278, val_acc:0.929]
Epoch [31/120    avg_loss:0.234, val_acc:0.942]
Epoch [32/120    avg_loss:0.201, val_acc:0.915]
Epoch [33/120    avg_loss:0.170, val_acc:0.972]
Epoch [34/120    avg_loss:0.142, val_acc:0.956]
Epoch [35/120    avg_loss:0.178, val_acc:0.950]
Epoch [36/120    avg_loss:0.128, val_acc:0.972]
Epoch [37/120    avg_loss:0.094, val_acc:0.940]
Epoch [38/120    avg_loss:0.143, val_acc:0.966]
Epoch [39/120    avg_loss:0.099, val_acc:0.978]
Epoch [40/120    avg_loss:0.077, val_acc:0.978]
Epoch [41/120    avg_loss:0.067, val_acc:0.976]
Epoch [42/120    avg_loss:0.088, val_acc:0.960]
Epoch [43/120    avg_loss:0.092, val_acc:0.970]
Epoch [44/120    avg_loss:0.065, val_acc:0.938]
Epoch [45/120    avg_loss:0.155, val_acc:0.960]
Epoch [46/120    avg_loss:0.109, val_acc:0.962]
Epoch [47/120    avg_loss:0.102, val_acc:0.970]
Epoch [48/120    avg_loss:0.065, val_acc:0.986]
Epoch [49/120    avg_loss:0.059, val_acc:0.978]
Epoch [50/120    avg_loss:0.082, val_acc:0.980]
Epoch [51/120    avg_loss:0.056, val_acc:0.980]
Epoch [52/120    avg_loss:0.047, val_acc:0.990]
Epoch [53/120    avg_loss:0.044, val_acc:0.974]
Epoch [54/120    avg_loss:0.047, val_acc:0.982]
Epoch [55/120    avg_loss:0.047, val_acc:0.966]
Epoch [56/120    avg_loss:0.050, val_acc:0.978]
Epoch [57/120    avg_loss:0.037, val_acc:0.976]
Epoch [58/120    avg_loss:0.052, val_acc:0.986]
Epoch [59/120    avg_loss:0.087, val_acc:0.940]
Epoch [60/120    avg_loss:0.066, val_acc:0.970]
Epoch [61/120    avg_loss:0.068, val_acc:0.980]
Epoch [62/120    avg_loss:0.053, val_acc:0.976]
Epoch [63/120    avg_loss:0.050, val_acc:0.982]
Epoch [64/120    avg_loss:0.030, val_acc:0.974]
Epoch [65/120    avg_loss:0.029, val_acc:0.984]
Epoch [66/120    avg_loss:0.028, val_acc:0.988]
Epoch [67/120    avg_loss:0.023, val_acc:0.984]
Epoch [68/120    avg_loss:0.016, val_acc:0.984]
Epoch [69/120    avg_loss:0.016, val_acc:0.984]
Epoch [70/120    avg_loss:0.014, val_acc:0.982]
Epoch [71/120    avg_loss:0.022, val_acc:0.982]
Epoch [72/120    avg_loss:0.020, val_acc:0.988]
Epoch [73/120    avg_loss:0.026, val_acc:0.984]
Epoch [74/120    avg_loss:0.024, val_acc:0.982]
Epoch [75/120    avg_loss:0.016, val_acc:0.986]
Epoch [76/120    avg_loss:0.016, val_acc:0.982]
Epoch [77/120    avg_loss:0.015, val_acc:0.986]
Epoch [78/120    avg_loss:0.015, val_acc:0.982]
Epoch [79/120    avg_loss:0.019, val_acc:0.982]
Epoch [80/120    avg_loss:0.018, val_acc:0.982]
Epoch [81/120    avg_loss:0.019, val_acc:0.982]
Epoch [82/120    avg_loss:0.017, val_acc:0.982]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.014, val_acc:0.982]
Epoch [85/120    avg_loss:0.016, val_acc:0.982]
Epoch [86/120    avg_loss:0.014, val_acc:0.982]
Epoch [87/120    avg_loss:0.013, val_acc:0.982]
Epoch [88/120    avg_loss:0.020, val_acc:0.982]
Epoch [89/120    avg_loss:0.012, val_acc:0.982]
Epoch [90/120    avg_loss:0.014, val_acc:0.982]
Epoch [91/120    avg_loss:0.017, val_acc:0.980]
Epoch [92/120    avg_loss:0.013, val_acc:0.980]
Epoch [93/120    avg_loss:0.017, val_acc:0.980]
Epoch [94/120    avg_loss:0.018, val_acc:0.980]
Epoch [95/120    avg_loss:0.018, val_acc:0.980]
Epoch [96/120    avg_loss:0.015, val_acc:0.980]
Epoch [97/120    avg_loss:0.013, val_acc:0.980]
Epoch [98/120    avg_loss:0.013, val_acc:0.980]
Epoch [99/120    avg_loss:0.015, val_acc:0.980]
Epoch [100/120    avg_loss:0.014, val_acc:0.980]
Epoch [101/120    avg_loss:0.013, val_acc:0.980]
Epoch [102/120    avg_loss:0.015, val_acc:0.980]
Epoch [103/120    avg_loss:0.016, val_acc:0.980]
Epoch [104/120    avg_loss:0.011, val_acc:0.980]
Epoch [105/120    avg_loss:0.017, val_acc:0.980]
Epoch [106/120    avg_loss:0.014, val_acc:0.980]
Epoch [107/120    avg_loss:0.017, val_acc:0.980]
Epoch [108/120    avg_loss:0.019, val_acc:0.980]
Epoch [109/120    avg_loss:0.015, val_acc:0.980]
Epoch [110/120    avg_loss:0.016, val_acc:0.980]
Epoch [111/120    avg_loss:0.020, val_acc:0.980]
Epoch [112/120    avg_loss:0.012, val_acc:0.980]
Epoch [113/120    avg_loss:0.013, val_acc:0.980]
Epoch [114/120    avg_loss:0.016, val_acc:0.980]
Epoch [115/120    avg_loss:0.018, val_acc:0.980]
Epoch [116/120    avg_loss:0.014, val_acc:0.980]
Epoch [117/120    avg_loss:0.014, val_acc:0.980]
Epoch [118/120    avg_loss:0.019, val_acc:0.980]
Epoch [119/120    avg_loss:0.013, val_acc:0.980]
Epoch [120/120    avg_loss:0.014, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 222   5   0   0   0   0   0   0   0   0]
 [  0   0   0   2  16 127   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 375   2   0]
 [  0   0   0   0   0   0   0   0   0   0   0   6 447   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.1684434968017

F1 scores:
[       nan 1.         0.98206278 0.995671   0.95483871 0.91696751
 1.         0.95555556 1.         1.         1.         0.98944591
 0.99113082 1.        ]

Kappa:
0.9907412947366151
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fca4bcc2860>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.254, val_acc:0.597]
Epoch [2/120    avg_loss:1.679, val_acc:0.655]
Epoch [3/120    avg_loss:1.273, val_acc:0.736]
Epoch [4/120    avg_loss:1.019, val_acc:0.750]
Epoch [5/120    avg_loss:0.898, val_acc:0.780]
Epoch [6/120    avg_loss:0.783, val_acc:0.806]
Epoch [7/120    avg_loss:0.684, val_acc:0.875]
Epoch [8/120    avg_loss:0.638, val_acc:0.859]
Epoch [9/120    avg_loss:0.546, val_acc:0.833]
Epoch [10/120    avg_loss:0.496, val_acc:0.881]
Epoch [11/120    avg_loss:0.437, val_acc:0.921]
Epoch [12/120    avg_loss:0.420, val_acc:0.917]
Epoch [13/120    avg_loss:0.409, val_acc:0.877]
Epoch [14/120    avg_loss:0.504, val_acc:0.893]
Epoch [15/120    avg_loss:0.377, val_acc:0.869]
Epoch [16/120    avg_loss:0.380, val_acc:0.895]
Epoch [17/120    avg_loss:0.358, val_acc:0.907]
Epoch [18/120    avg_loss:0.293, val_acc:0.942]
Epoch [19/120    avg_loss:0.291, val_acc:0.919]
Epoch [20/120    avg_loss:0.264, val_acc:0.946]
Epoch [21/120    avg_loss:0.254, val_acc:0.948]
Epoch [22/120    avg_loss:0.303, val_acc:0.925]
Epoch [23/120    avg_loss:0.228, val_acc:0.940]
Epoch [24/120    avg_loss:0.230, val_acc:0.950]
Epoch [25/120    avg_loss:0.224, val_acc:0.956]
Epoch [26/120    avg_loss:0.209, val_acc:0.903]
Epoch [27/120    avg_loss:0.235, val_acc:0.960]
Epoch [28/120    avg_loss:0.173, val_acc:0.970]
Epoch [29/120    avg_loss:0.148, val_acc:0.944]
Epoch [30/120    avg_loss:0.122, val_acc:0.970]
Epoch [31/120    avg_loss:0.197, val_acc:0.958]
Epoch [32/120    avg_loss:0.189, val_acc:0.958]
Epoch [33/120    avg_loss:0.119, val_acc:0.935]
Epoch [34/120    avg_loss:0.105, val_acc:0.962]
Epoch [35/120    avg_loss:0.285, val_acc:0.952]
Epoch [36/120    avg_loss:0.149, val_acc:0.923]
Epoch [37/120    avg_loss:0.175, val_acc:0.952]
Epoch [38/120    avg_loss:0.175, val_acc:0.958]
Epoch [39/120    avg_loss:0.161, val_acc:0.972]
Epoch [40/120    avg_loss:0.156, val_acc:0.970]
Epoch [41/120    avg_loss:0.089, val_acc:0.974]
Epoch [42/120    avg_loss:0.070, val_acc:0.972]
Epoch [43/120    avg_loss:0.099, val_acc:0.968]
Epoch [44/120    avg_loss:0.067, val_acc:0.974]
Epoch [45/120    avg_loss:0.078, val_acc:0.978]
Epoch [46/120    avg_loss:0.076, val_acc:0.978]
Epoch [47/120    avg_loss:0.057, val_acc:0.978]
Epoch [48/120    avg_loss:0.047, val_acc:0.990]
Epoch [49/120    avg_loss:0.067, val_acc:0.964]
Epoch [50/120    avg_loss:0.075, val_acc:0.982]
Epoch [51/120    avg_loss:0.039, val_acc:0.982]
Epoch [52/120    avg_loss:0.050, val_acc:0.980]
Epoch [53/120    avg_loss:0.065, val_acc:0.980]
Epoch [54/120    avg_loss:0.054, val_acc:0.990]
Epoch [55/120    avg_loss:0.055, val_acc:0.974]
Epoch [56/120    avg_loss:0.054, val_acc:0.982]
Epoch [57/120    avg_loss:0.096, val_acc:0.974]
Epoch [58/120    avg_loss:0.068, val_acc:0.986]
Epoch [59/120    avg_loss:0.055, val_acc:0.982]
Epoch [60/120    avg_loss:0.075, val_acc:0.986]
Epoch [61/120    avg_loss:0.044, val_acc:0.982]
Epoch [62/120    avg_loss:0.027, val_acc:0.988]
Epoch [63/120    avg_loss:0.035, val_acc:0.986]
Epoch [64/120    avg_loss:0.024, val_acc:0.996]
Epoch [65/120    avg_loss:0.025, val_acc:0.988]
Epoch [66/120    avg_loss:0.029, val_acc:0.984]
Epoch [67/120    avg_loss:0.022, val_acc:0.990]
Epoch [68/120    avg_loss:0.026, val_acc:0.990]
Epoch [69/120    avg_loss:0.030, val_acc:0.984]
Epoch [70/120    avg_loss:0.029, val_acc:0.990]
Epoch [71/120    avg_loss:0.026, val_acc:0.986]
Epoch [72/120    avg_loss:0.015, val_acc:0.994]
Epoch [73/120    avg_loss:0.016, val_acc:0.994]
Epoch [74/120    avg_loss:0.012, val_acc:0.994]
Epoch [75/120    avg_loss:0.011, val_acc:0.988]
Epoch [76/120    avg_loss:0.009, val_acc:0.990]
Epoch [77/120    avg_loss:0.012, val_acc:0.990]
Epoch [78/120    avg_loss:0.017, val_acc:0.990]
Epoch [79/120    avg_loss:0.008, val_acc:0.992]
Epoch [80/120    avg_loss:0.010, val_acc:0.992]
Epoch [81/120    avg_loss:0.008, val_acc:0.990]
Epoch [82/120    avg_loss:0.012, val_acc:0.992]
Epoch [83/120    avg_loss:0.008, val_acc:0.990]
Epoch [84/120    avg_loss:0.008, val_acc:0.992]
Epoch [85/120    avg_loss:0.008, val_acc:0.990]
Epoch [86/120    avg_loss:0.008, val_acc:0.990]
Epoch [87/120    avg_loss:0.009, val_acc:0.992]
Epoch [88/120    avg_loss:0.010, val_acc:0.994]
Epoch [89/120    avg_loss:0.009, val_acc:0.994]
Epoch [90/120    avg_loss:0.007, val_acc:0.994]
Epoch [91/120    avg_loss:0.008, val_acc:0.994]
Epoch [92/120    avg_loss:0.007, val_acc:0.994]
Epoch [93/120    avg_loss:0.008, val_acc:0.994]
Epoch [94/120    avg_loss:0.007, val_acc:0.994]
Epoch [95/120    avg_loss:0.006, val_acc:0.994]
Epoch [96/120    avg_loss:0.009, val_acc:0.994]
Epoch [97/120    avg_loss:0.010, val_acc:0.994]
Epoch [98/120    avg_loss:0.006, val_acc:0.994]
Epoch [99/120    avg_loss:0.007, val_acc:0.994]
Epoch [100/120    avg_loss:0.010, val_acc:0.994]
Epoch [101/120    avg_loss:0.007, val_acc:0.994]
Epoch [102/120    avg_loss:0.007, val_acc:0.994]
Epoch [103/120    avg_loss:0.008, val_acc:0.994]
Epoch [104/120    avg_loss:0.008, val_acc:0.994]
Epoch [105/120    avg_loss:0.008, val_acc:0.994]
Epoch [106/120    avg_loss:0.009, val_acc:0.994]
Epoch [107/120    avg_loss:0.008, val_acc:0.994]
Epoch [108/120    avg_loss:0.006, val_acc:0.994]
Epoch [109/120    avg_loss:0.006, val_acc:0.994]
Epoch [110/120    avg_loss:0.008, val_acc:0.994]
Epoch [111/120    avg_loss:0.006, val_acc:0.994]
Epoch [112/120    avg_loss:0.006, val_acc:0.994]
Epoch [113/120    avg_loss:0.007, val_acc:0.994]
Epoch [114/120    avg_loss:0.008, val_acc:0.994]
Epoch [115/120    avg_loss:0.005, val_acc:0.994]
Epoch [116/120    avg_loss:0.010, val_acc:0.994]
Epoch [117/120    avg_loss:0.009, val_acc:0.994]
Epoch [118/120    avg_loss:0.006, val_acc:0.994]
Epoch [119/120    avg_loss:0.006, val_acc:0.994]
Epoch [120/120    avg_loss:0.008, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  11   0   0   0   0   0   0   0   0]
 [  0   0   0   0   2 143   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   4   0   0   0   0  90   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.63752665245202

F1 scores:
[       nan 1.         0.99095023 1.         0.97078652 0.95652174
 1.         0.97826087 1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9959645638803515
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f573126b860>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.298, val_acc:0.554]
Epoch [2/120    avg_loss:1.659, val_acc:0.645]
Epoch [3/120    avg_loss:1.254, val_acc:0.800]
Epoch [4/120    avg_loss:1.018, val_acc:0.819]
Epoch [5/120    avg_loss:0.868, val_acc:0.817]
Epoch [6/120    avg_loss:0.735, val_acc:0.825]
Epoch [7/120    avg_loss:0.684, val_acc:0.839]
Epoch [8/120    avg_loss:0.641, val_acc:0.921]
Epoch [9/120    avg_loss:0.489, val_acc:0.895]
Epoch [10/120    avg_loss:0.475, val_acc:0.887]
Epoch [11/120    avg_loss:0.422, val_acc:0.885]
Epoch [12/120    avg_loss:0.373, val_acc:0.944]
Epoch [13/120    avg_loss:0.328, val_acc:0.873]
Epoch [14/120    avg_loss:0.336, val_acc:0.935]
Epoch [15/120    avg_loss:0.279, val_acc:0.948]
Epoch [16/120    avg_loss:0.290, val_acc:0.938]
Epoch [17/120    avg_loss:0.254, val_acc:0.938]
Epoch [18/120    avg_loss:0.326, val_acc:0.935]
Epoch [19/120    avg_loss:0.303, val_acc:0.915]
Epoch [20/120    avg_loss:0.318, val_acc:0.915]
Epoch [21/120    avg_loss:0.275, val_acc:0.950]
Epoch [22/120    avg_loss:0.300, val_acc:0.919]
Epoch [23/120    avg_loss:0.274, val_acc:0.942]
Epoch [24/120    avg_loss:0.213, val_acc:0.970]
Epoch [25/120    avg_loss:0.188, val_acc:0.972]
Epoch [26/120    avg_loss:0.225, val_acc:0.962]
Epoch [27/120    avg_loss:0.184, val_acc:0.970]
Epoch [28/120    avg_loss:0.147, val_acc:0.962]
Epoch [29/120    avg_loss:0.149, val_acc:0.954]
Epoch [30/120    avg_loss:0.136, val_acc:0.966]
Epoch [31/120    avg_loss:0.146, val_acc:0.974]
Epoch [32/120    avg_loss:0.175, val_acc:0.964]
Epoch [33/120    avg_loss:0.274, val_acc:0.913]
Epoch [34/120    avg_loss:0.160, val_acc:0.931]
Epoch [35/120    avg_loss:0.164, val_acc:0.960]
Epoch [36/120    avg_loss:0.175, val_acc:0.974]
Epoch [37/120    avg_loss:0.170, val_acc:0.972]
Epoch [38/120    avg_loss:0.121, val_acc:0.970]
Epoch [39/120    avg_loss:0.101, val_acc:0.966]
Epoch [40/120    avg_loss:0.082, val_acc:0.976]
Epoch [41/120    avg_loss:0.088, val_acc:0.980]
Epoch [42/120    avg_loss:0.092, val_acc:0.931]
Epoch [43/120    avg_loss:0.108, val_acc:0.972]
Epoch [44/120    avg_loss:0.120, val_acc:0.919]
Epoch [45/120    avg_loss:0.142, val_acc:0.958]
Epoch [46/120    avg_loss:0.081, val_acc:0.972]
Epoch [47/120    avg_loss:0.095, val_acc:0.982]
Epoch [48/120    avg_loss:0.099, val_acc:0.982]
Epoch [49/120    avg_loss:0.095, val_acc:0.952]
Epoch [50/120    avg_loss:0.050, val_acc:0.982]
Epoch [51/120    avg_loss:0.049, val_acc:0.986]
Epoch [52/120    avg_loss:0.059, val_acc:0.974]
Epoch [53/120    avg_loss:0.045, val_acc:0.982]
Epoch [54/120    avg_loss:0.037, val_acc:0.990]
Epoch [55/120    avg_loss:0.049, val_acc:0.980]
Epoch [56/120    avg_loss:0.096, val_acc:0.960]
Epoch [57/120    avg_loss:0.126, val_acc:0.968]
Epoch [58/120    avg_loss:0.069, val_acc:0.990]
Epoch [59/120    avg_loss:0.035, val_acc:0.976]
Epoch [60/120    avg_loss:0.042, val_acc:0.984]
Epoch [61/120    avg_loss:0.070, val_acc:0.986]
Epoch [62/120    avg_loss:0.068, val_acc:0.972]
Epoch [63/120    avg_loss:0.104, val_acc:0.960]
Epoch [64/120    avg_loss:0.075, val_acc:0.966]
Epoch [65/120    avg_loss:0.070, val_acc:0.984]
Epoch [66/120    avg_loss:0.049, val_acc:0.990]
Epoch [67/120    avg_loss:0.035, val_acc:0.966]
Epoch [68/120    avg_loss:0.038, val_acc:0.980]
Epoch [69/120    avg_loss:0.038, val_acc:0.986]
Epoch [70/120    avg_loss:0.046, val_acc:0.982]
Epoch [71/120    avg_loss:0.030, val_acc:0.986]
Epoch [72/120    avg_loss:0.036, val_acc:0.978]
Epoch [73/120    avg_loss:0.029, val_acc:0.988]
Epoch [74/120    avg_loss:0.042, val_acc:0.988]
Epoch [75/120    avg_loss:0.038, val_acc:0.980]
Epoch [76/120    avg_loss:0.043, val_acc:0.980]
Epoch [77/120    avg_loss:0.025, val_acc:0.988]
Epoch [78/120    avg_loss:0.016, val_acc:0.992]
Epoch [79/120    avg_loss:0.015, val_acc:0.992]
Epoch [80/120    avg_loss:0.016, val_acc:0.980]
Epoch [81/120    avg_loss:0.022, val_acc:0.988]
Epoch [82/120    avg_loss:0.009, val_acc:0.992]
Epoch [83/120    avg_loss:0.016, val_acc:0.994]
Epoch [84/120    avg_loss:0.020, val_acc:0.994]
Epoch [85/120    avg_loss:0.031, val_acc:0.990]
Epoch [86/120    avg_loss:0.041, val_acc:0.986]
Epoch [87/120    avg_loss:0.026, val_acc:0.972]
Epoch [88/120    avg_loss:0.023, val_acc:0.990]
Epoch [89/120    avg_loss:0.011, val_acc:0.992]
Epoch [90/120    avg_loss:0.008, val_acc:0.992]
Epoch [91/120    avg_loss:0.010, val_acc:0.994]
Epoch [92/120    avg_loss:0.009, val_acc:0.996]
Epoch [93/120    avg_loss:0.010, val_acc:0.994]
Epoch [94/120    avg_loss:0.006, val_acc:0.994]
Epoch [95/120    avg_loss:0.006, val_acc:0.996]
Epoch [96/120    avg_loss:0.008, val_acc:0.998]
Epoch [97/120    avg_loss:0.010, val_acc:0.994]
Epoch [98/120    avg_loss:0.012, val_acc:0.996]
Epoch [99/120    avg_loss:0.015, val_acc:0.992]
Epoch [100/120    avg_loss:0.042, val_acc:0.966]
Epoch [101/120    avg_loss:0.031, val_acc:0.994]
Epoch [102/120    avg_loss:0.014, val_acc:0.982]
Epoch [103/120    avg_loss:0.016, val_acc:0.986]
Epoch [104/120    avg_loss:0.081, val_acc:0.980]
Epoch [105/120    avg_loss:0.036, val_acc:0.988]
Epoch [106/120    avg_loss:0.035, val_acc:0.994]
Epoch [107/120    avg_loss:0.012, val_acc:0.996]
Epoch [108/120    avg_loss:0.009, val_acc:0.996]
Epoch [109/120    avg_loss:0.009, val_acc:0.994]
Epoch [110/120    avg_loss:0.011, val_acc:0.994]
Epoch [111/120    avg_loss:0.006, val_acc:0.994]
Epoch [112/120    avg_loss:0.006, val_acc:0.994]
Epoch [113/120    avg_loss:0.009, val_acc:0.994]
Epoch [114/120    avg_loss:0.008, val_acc:0.994]
Epoch [115/120    avg_loss:0.009, val_acc:0.996]
Epoch [116/120    avg_loss:0.008, val_acc:0.996]
Epoch [117/120    avg_loss:0.007, val_acc:0.996]
Epoch [118/120    avg_loss:0.005, val_acc:0.994]
Epoch [119/120    avg_loss:0.005, val_acc:0.996]
Epoch [120/120    avg_loss:0.006, val_acc:0.996]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 682   0   0   0   0   3   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 218   9   0   0   0   0   0   0   0   0]
 [  0   0   0   0   1 144   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.7228144989339

F1 scores:
[       nan 0.99780541 1.         1.         0.97757848 0.96644295
 0.99277108 1.         1.         1.         1.         1.
 1.         1.        ]

Kappa:
0.9969143679669592
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd02b4d4860>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.225, val_acc:0.502]
Epoch [2/120    avg_loss:1.604, val_acc:0.720]
Epoch [3/120    avg_loss:1.257, val_acc:0.724]
Epoch [4/120    avg_loss:1.011, val_acc:0.746]
Epoch [5/120    avg_loss:0.876, val_acc:0.796]
Epoch [6/120    avg_loss:0.737, val_acc:0.863]
Epoch [7/120    avg_loss:0.694, val_acc:0.887]
Epoch [8/120    avg_loss:0.542, val_acc:0.887]
Epoch [9/120    avg_loss:0.495, val_acc:0.909]
Epoch [10/120    avg_loss:0.527, val_acc:0.897]
Epoch [11/120    avg_loss:0.516, val_acc:0.877]
Epoch [12/120    avg_loss:0.414, val_acc:0.913]
Epoch [13/120    avg_loss:0.398, val_acc:0.871]
Epoch [14/120    avg_loss:0.428, val_acc:0.879]
Epoch [15/120    avg_loss:0.368, val_acc:0.929]
Epoch [16/120    avg_loss:0.443, val_acc:0.915]
Epoch [17/120    avg_loss:0.414, val_acc:0.917]
Epoch [18/120    avg_loss:0.330, val_acc:0.921]
Epoch [19/120    avg_loss:0.286, val_acc:0.950]
Epoch [20/120    avg_loss:0.257, val_acc:0.927]
Epoch [21/120    avg_loss:0.241, val_acc:0.946]
Epoch [22/120    avg_loss:0.368, val_acc:0.919]
Epoch [23/120    avg_loss:0.304, val_acc:0.946]
Epoch [24/120    avg_loss:0.245, val_acc:0.962]
Epoch [25/120    avg_loss:0.236, val_acc:0.940]
Epoch [26/120    avg_loss:0.219, val_acc:0.948]
Epoch [27/120    avg_loss:0.197, val_acc:0.968]
Epoch [28/120    avg_loss:0.227, val_acc:0.970]
Epoch [29/120    avg_loss:0.174, val_acc:0.960]
Epoch [30/120    avg_loss:0.213, val_acc:0.935]
Epoch [31/120    avg_loss:0.212, val_acc:0.964]
Epoch [32/120    avg_loss:0.207, val_acc:0.962]
Epoch [33/120    avg_loss:0.156, val_acc:0.958]
Epoch [34/120    avg_loss:0.120, val_acc:0.974]
Epoch [35/120    avg_loss:0.106, val_acc:0.974]
Epoch [36/120    avg_loss:0.078, val_acc:0.976]
Epoch [37/120    avg_loss:0.085, val_acc:0.976]
Epoch [38/120    avg_loss:0.134, val_acc:0.970]
Epoch [39/120    avg_loss:0.097, val_acc:0.980]
Epoch [40/120    avg_loss:0.141, val_acc:0.966]
Epoch [41/120    avg_loss:0.133, val_acc:0.976]
Epoch [42/120    avg_loss:0.107, val_acc:0.984]
Epoch [43/120    avg_loss:0.100, val_acc:0.990]
Epoch [44/120    avg_loss:0.084, val_acc:0.982]
Epoch [45/120    avg_loss:0.081, val_acc:0.984]
Epoch [46/120    avg_loss:0.090, val_acc:0.972]
Epoch [47/120    avg_loss:0.085, val_acc:0.978]
Epoch [48/120    avg_loss:0.074, val_acc:0.984]
Epoch [49/120    avg_loss:0.096, val_acc:0.982]
Epoch [50/120    avg_loss:0.053, val_acc:0.988]
Epoch [51/120    avg_loss:0.098, val_acc:0.950]
Epoch [52/120    avg_loss:0.073, val_acc:0.992]
Epoch [53/120    avg_loss:0.030, val_acc:0.990]
Epoch [54/120    avg_loss:0.043, val_acc:0.996]
Epoch [55/120    avg_loss:0.036, val_acc:0.988]
Epoch [56/120    avg_loss:0.041, val_acc:0.990]
Epoch [57/120    avg_loss:0.041, val_acc:0.990]
Epoch [58/120    avg_loss:0.021, val_acc:0.992]
Epoch [59/120    avg_loss:0.037, val_acc:0.988]
Epoch [60/120    avg_loss:0.028, val_acc:0.990]
Epoch [61/120    avg_loss:0.019, val_acc:0.994]
Epoch [62/120    avg_loss:0.049, val_acc:0.988]
Epoch [63/120    avg_loss:0.030, val_acc:0.988]
Epoch [64/120    avg_loss:0.036, val_acc:0.990]
Epoch [65/120    avg_loss:0.063, val_acc:0.958]
Epoch [66/120    avg_loss:0.048, val_acc:0.990]
Epoch [67/120    avg_loss:0.044, val_acc:0.994]
Epoch [68/120    avg_loss:0.041, val_acc:0.994]
Epoch [69/120    avg_loss:0.016, val_acc:0.994]
Epoch [70/120    avg_loss:0.019, val_acc:0.992]
Epoch [71/120    avg_loss:0.017, val_acc:0.990]
Epoch [72/120    avg_loss:0.023, val_acc:0.990]
Epoch [73/120    avg_loss:0.019, val_acc:0.990]
Epoch [74/120    avg_loss:0.019, val_acc:0.990]
Epoch [75/120    avg_loss:0.013, val_acc:0.990]
Epoch [76/120    avg_loss:0.018, val_acc:0.990]
Epoch [77/120    avg_loss:0.015, val_acc:0.994]
Epoch [78/120    avg_loss:0.017, val_acc:0.994]
Epoch [79/120    avg_loss:0.015, val_acc:0.992]
Epoch [80/120    avg_loss:0.015, val_acc:0.992]
Epoch [81/120    avg_loss:0.015, val_acc:0.992]
Epoch [82/120    avg_loss:0.012, val_acc:0.992]
Epoch [83/120    avg_loss:0.014, val_acc:0.992]
Epoch [84/120    avg_loss:0.016, val_acc:0.992]
Epoch [85/120    avg_loss:0.013, val_acc:0.992]
Epoch [86/120    avg_loss:0.015, val_acc:0.992]
Epoch [87/120    avg_loss:0.013, val_acc:0.992]
Epoch [88/120    avg_loss:0.013, val_acc:0.992]
Epoch [89/120    avg_loss:0.015, val_acc:0.992]
Epoch [90/120    avg_loss:0.016, val_acc:0.992]
Epoch [91/120    avg_loss:0.019, val_acc:0.992]
Epoch [92/120    avg_loss:0.016, val_acc:0.992]
Epoch [93/120    avg_loss:0.015, val_acc:0.992]
Epoch [94/120    avg_loss:0.019, val_acc:0.992]
Epoch [95/120    avg_loss:0.014, val_acc:0.992]
Epoch [96/120    avg_loss:0.014, val_acc:0.992]
Epoch [97/120    avg_loss:0.015, val_acc:0.992]
Epoch [98/120    avg_loss:0.017, val_acc:0.992]
Epoch [99/120    avg_loss:0.014, val_acc:0.992]
Epoch [100/120    avg_loss:0.014, val_acc:0.992]
Epoch [101/120    avg_loss:0.020, val_acc:0.992]
Epoch [102/120    avg_loss:0.015, val_acc:0.992]
Epoch [103/120    avg_loss:0.016, val_acc:0.992]
Epoch [104/120    avg_loss:0.011, val_acc:0.992]
Epoch [105/120    avg_loss:0.011, val_acc:0.992]
Epoch [106/120    avg_loss:0.015, val_acc:0.992]
Epoch [107/120    avg_loss:0.014, val_acc:0.992]
Epoch [108/120    avg_loss:0.015, val_acc:0.992]
Epoch [109/120    avg_loss:0.017, val_acc:0.992]
Epoch [110/120    avg_loss:0.011, val_acc:0.992]
Epoch [111/120    avg_loss:0.019, val_acc:0.992]
Epoch [112/120    avg_loss:0.014, val_acc:0.992]
Epoch [113/120    avg_loss:0.017, val_acc:0.992]
Epoch [114/120    avg_loss:0.017, val_acc:0.992]
Epoch [115/120    avg_loss:0.016, val_acc:0.992]
Epoch [116/120    avg_loss:0.010, val_acc:0.992]
Epoch [117/120    avg_loss:0.016, val_acc:0.992]
Epoch [118/120    avg_loss:0.016, val_acc:0.992]
Epoch [119/120    avg_loss:0.016, val_acc:0.992]
Epoch [120/120    avg_loss:0.018, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 209  13   0   0   0   0   0   0   5   0]
 [  0   0   0   0   0 145   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   0  88   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.46695095948827

F1 scores:
[       nan 0.99926954 0.98648649 1.         0.9587156  0.95709571
 0.99757869 0.96703297 1.         1.         1.         1.
 0.99451153 1.        ]

Kappa:
0.9940653630036501
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1b7a9a3828>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.277, val_acc:0.611]
Epoch [2/120    avg_loss:1.705, val_acc:0.633]
Epoch [3/120    avg_loss:1.341, val_acc:0.688]
Epoch [4/120    avg_loss:1.109, val_acc:0.782]
Epoch [5/120    avg_loss:0.851, val_acc:0.772]
Epoch [6/120    avg_loss:0.755, val_acc:0.780]
Epoch [7/120    avg_loss:0.711, val_acc:0.831]
Epoch [8/120    avg_loss:0.608, val_acc:0.808]
Epoch [9/120    avg_loss:0.594, val_acc:0.774]
Epoch [10/120    avg_loss:0.596, val_acc:0.847]
Epoch [11/120    avg_loss:0.585, val_acc:0.812]
Epoch [12/120    avg_loss:0.448, val_acc:0.899]
Epoch [13/120    avg_loss:0.409, val_acc:0.907]
Epoch [14/120    avg_loss:0.374, val_acc:0.911]
Epoch [15/120    avg_loss:0.422, val_acc:0.903]
Epoch [16/120    avg_loss:0.369, val_acc:0.917]
Epoch [17/120    avg_loss:0.318, val_acc:0.935]
Epoch [18/120    avg_loss:0.308, val_acc:0.931]
Epoch [19/120    avg_loss:0.292, val_acc:0.911]
Epoch [20/120    avg_loss:0.224, val_acc:0.938]
Epoch [21/120    avg_loss:0.240, val_acc:0.927]
Epoch [22/120    avg_loss:0.227, val_acc:0.954]
Epoch [23/120    avg_loss:0.265, val_acc:0.933]
Epoch [24/120    avg_loss:0.245, val_acc:0.938]
Epoch [25/120    avg_loss:0.221, val_acc:0.952]
Epoch [26/120    avg_loss:0.201, val_acc:0.948]
Epoch [27/120    avg_loss:0.179, val_acc:0.946]
Epoch [28/120    avg_loss:0.170, val_acc:0.933]
Epoch [29/120    avg_loss:0.191, val_acc:0.956]
Epoch [30/120    avg_loss:0.210, val_acc:0.942]
Epoch [31/120    avg_loss:0.144, val_acc:0.974]
Epoch [32/120    avg_loss:0.146, val_acc:0.964]
Epoch [33/120    avg_loss:0.131, val_acc:0.966]
Epoch [34/120    avg_loss:0.163, val_acc:0.968]
Epoch [35/120    avg_loss:0.137, val_acc:0.972]
Epoch [36/120    avg_loss:0.199, val_acc:0.946]
Epoch [37/120    avg_loss:0.142, val_acc:0.954]
Epoch [38/120    avg_loss:0.135, val_acc:0.950]
Epoch [39/120    avg_loss:0.139, val_acc:0.966]
Epoch [40/120    avg_loss:0.129, val_acc:0.974]
Epoch [41/120    avg_loss:0.130, val_acc:0.952]
Epoch [42/120    avg_loss:0.152, val_acc:0.966]
Epoch [43/120    avg_loss:0.097, val_acc:0.976]
Epoch [44/120    avg_loss:0.112, val_acc:0.929]
Epoch [45/120    avg_loss:0.125, val_acc:0.966]
Epoch [46/120    avg_loss:0.089, val_acc:0.980]
Epoch [47/120    avg_loss:0.059, val_acc:0.982]
Epoch [48/120    avg_loss:0.065, val_acc:0.978]
Epoch [49/120    avg_loss:0.056, val_acc:0.984]
Epoch [50/120    avg_loss:0.036, val_acc:0.984]
Epoch [51/120    avg_loss:0.040, val_acc:0.994]
Epoch [52/120    avg_loss:0.034, val_acc:0.986]
Epoch [53/120    avg_loss:0.038, val_acc:0.968]
Epoch [54/120    avg_loss:0.075, val_acc:0.986]
Epoch [55/120    avg_loss:0.059, val_acc:0.990]
Epoch [56/120    avg_loss:0.036, val_acc:0.986]
Epoch [57/120    avg_loss:0.037, val_acc:0.990]
Epoch [58/120    avg_loss:0.034, val_acc:0.990]
Epoch [59/120    avg_loss:0.025, val_acc:0.986]
Epoch [60/120    avg_loss:0.023, val_acc:0.992]
Epoch [61/120    avg_loss:0.029, val_acc:0.984]
Epoch [62/120    avg_loss:0.025, val_acc:0.984]
Epoch [63/120    avg_loss:0.035, val_acc:0.994]
Epoch [64/120    avg_loss:0.027, val_acc:0.992]
Epoch [65/120    avg_loss:0.031, val_acc:0.990]
Epoch [66/120    avg_loss:0.037, val_acc:0.986]
Epoch [67/120    avg_loss:0.029, val_acc:0.990]
Epoch [68/120    avg_loss:0.021, val_acc:0.992]
Epoch [69/120    avg_loss:0.041, val_acc:0.988]
Epoch [70/120    avg_loss:0.029, val_acc:0.988]
Epoch [71/120    avg_loss:0.064, val_acc:0.980]
Epoch [72/120    avg_loss:0.048, val_acc:0.984]
Epoch [73/120    avg_loss:0.032, val_acc:0.984]
Epoch [74/120    avg_loss:0.028, val_acc:0.984]
Epoch [75/120    avg_loss:0.025, val_acc:0.994]
Epoch [76/120    avg_loss:0.022, val_acc:0.994]
Epoch [77/120    avg_loss:0.017, val_acc:0.996]
Epoch [78/120    avg_loss:0.014, val_acc:0.988]
Epoch [79/120    avg_loss:0.029, val_acc:0.990]
Epoch [80/120    avg_loss:0.026, val_acc:0.992]
Epoch [81/120    avg_loss:0.018, val_acc:0.976]
Epoch [82/120    avg_loss:0.025, val_acc:0.994]
Epoch [83/120    avg_loss:0.012, val_acc:0.996]
Epoch [84/120    avg_loss:0.013, val_acc:0.998]
Epoch [85/120    avg_loss:0.014, val_acc:0.996]
Epoch [86/120    avg_loss:0.011, val_acc:0.990]
Epoch [87/120    avg_loss:0.018, val_acc:0.996]
Epoch [88/120    avg_loss:0.011, val_acc:0.996]
Epoch [89/120    avg_loss:0.010, val_acc:0.996]
Epoch [90/120    avg_loss:0.010, val_acc:0.996]
Epoch [91/120    avg_loss:0.012, val_acc:0.998]
Epoch [92/120    avg_loss:0.008, val_acc:0.996]
Epoch [93/120    avg_loss:0.007, val_acc:0.998]
Epoch [94/120    avg_loss:0.006, val_acc:0.996]
Epoch [95/120    avg_loss:0.016, val_acc:0.998]
Epoch [96/120    avg_loss:0.021, val_acc:0.996]
Epoch [97/120    avg_loss:0.021, val_acc:0.994]
Epoch [98/120    avg_loss:0.011, val_acc:0.998]
Epoch [99/120    avg_loss:0.011, val_acc:0.986]
Epoch [100/120    avg_loss:0.011, val_acc:0.998]
Epoch [101/120    avg_loss:0.014, val_acc:0.990]
Epoch [102/120    avg_loss:0.014, val_acc:0.998]
Epoch [103/120    avg_loss:0.038, val_acc:0.986]
Epoch [104/120    avg_loss:0.035, val_acc:0.994]
Epoch [105/120    avg_loss:0.011, val_acc:0.994]
Epoch [106/120    avg_loss:0.007, val_acc:0.996]
Epoch [107/120    avg_loss:0.005, val_acc:0.998]
Epoch [108/120    avg_loss:0.009, val_acc:0.998]
Epoch [109/120    avg_loss:0.008, val_acc:0.998]
Epoch [110/120    avg_loss:0.008, val_acc:0.998]
Epoch [111/120    avg_loss:0.004, val_acc:0.998]
Epoch [112/120    avg_loss:0.004, val_acc:0.998]
Epoch [113/120    avg_loss:0.020, val_acc:0.990]
Epoch [114/120    avg_loss:0.030, val_acc:0.994]
Epoch [115/120    avg_loss:0.014, val_acc:0.996]
Epoch [116/120    avg_loss:0.008, val_acc:0.998]
Epoch [117/120    avg_loss:0.005, val_acc:0.998]
Epoch [118/120    avg_loss:0.009, val_acc:0.988]
Epoch [119/120    avg_loss:0.020, val_acc:0.994]
Epoch [120/120    avg_loss:0.011, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   1 216  10   0   0   0   0   0   0   0   0]
 [  0   0   0   1   2 142   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0  13 440   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.42430703624733

F1 scores:
[       nan 1.         1.         0.995671   0.97078652 0.95622896
 1.         1.         1.         1.         1.         0.98305085
 0.98544233 1.        ]

Kappa:
0.9935912054515901
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4c2458b7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.231, val_acc:0.607]
Epoch [2/120    avg_loss:1.648, val_acc:0.675]
Epoch [3/120    avg_loss:1.218, val_acc:0.706]
Epoch [4/120    avg_loss:0.968, val_acc:0.716]
Epoch [5/120    avg_loss:0.850, val_acc:0.788]
Epoch [6/120    avg_loss:0.750, val_acc:0.841]
Epoch [7/120    avg_loss:0.661, val_acc:0.800]
Epoch [8/120    avg_loss:0.591, val_acc:0.859]
Epoch [9/120    avg_loss:0.607, val_acc:0.845]
Epoch [10/120    avg_loss:0.567, val_acc:0.845]
Epoch [11/120    avg_loss:0.490, val_acc:0.911]
Epoch [12/120    avg_loss:0.423, val_acc:0.899]
Epoch [13/120    avg_loss:0.372, val_acc:0.938]
Epoch [14/120    avg_loss:0.349, val_acc:0.869]
Epoch [15/120    avg_loss:0.410, val_acc:0.917]
Epoch [16/120    avg_loss:0.342, val_acc:0.911]
Epoch [17/120    avg_loss:0.338, val_acc:0.940]
Epoch [18/120    avg_loss:0.307, val_acc:0.891]
Epoch [19/120    avg_loss:0.328, val_acc:0.909]
Epoch [20/120    avg_loss:0.309, val_acc:0.938]
Epoch [21/120    avg_loss:0.273, val_acc:0.942]
Epoch [22/120    avg_loss:0.239, val_acc:0.921]
Epoch [23/120    avg_loss:0.265, val_acc:0.952]
Epoch [24/120    avg_loss:0.249, val_acc:0.956]
Epoch [25/120    avg_loss:0.222, val_acc:0.944]
Epoch [26/120    avg_loss:0.263, val_acc:0.958]
Epoch [27/120    avg_loss:0.242, val_acc:0.942]
Epoch [28/120    avg_loss:0.161, val_acc:0.968]
Epoch [29/120    avg_loss:0.149, val_acc:0.950]
Epoch [30/120    avg_loss:0.128, val_acc:0.970]
Epoch [31/120    avg_loss:0.201, val_acc:0.952]
Epoch [32/120    avg_loss:0.198, val_acc:0.917]
Epoch [33/120    avg_loss:0.208, val_acc:0.964]
Epoch [34/120    avg_loss:0.157, val_acc:0.974]
Epoch [35/120    avg_loss:0.192, val_acc:0.938]
Epoch [36/120    avg_loss:0.147, val_acc:0.972]
Epoch [37/120    avg_loss:0.145, val_acc:0.938]
Epoch [38/120    avg_loss:0.129, val_acc:0.976]
Epoch [39/120    avg_loss:0.158, val_acc:0.958]
Epoch [40/120    avg_loss:0.132, val_acc:0.970]
Epoch [41/120    avg_loss:0.156, val_acc:0.962]
Epoch [42/120    avg_loss:0.125, val_acc:0.984]
Epoch [43/120    avg_loss:0.081, val_acc:0.982]
Epoch [44/120    avg_loss:0.096, val_acc:0.980]
Epoch [45/120    avg_loss:0.141, val_acc:0.978]
Epoch [46/120    avg_loss:0.085, val_acc:0.978]
Epoch [47/120    avg_loss:0.099, val_acc:0.984]
Epoch [48/120    avg_loss:0.116, val_acc:0.968]
Epoch [49/120    avg_loss:0.096, val_acc:0.990]
Epoch [50/120    avg_loss:0.058, val_acc:0.982]
Epoch [51/120    avg_loss:0.066, val_acc:0.980]
Epoch [52/120    avg_loss:0.050, val_acc:0.988]
Epoch [53/120    avg_loss:0.087, val_acc:0.974]
Epoch [54/120    avg_loss:0.061, val_acc:0.978]
Epoch [55/120    avg_loss:0.035, val_acc:0.994]
Epoch [56/120    avg_loss:0.045, val_acc:0.990]
Epoch [57/120    avg_loss:0.047, val_acc:0.994]
Epoch [58/120    avg_loss:0.037, val_acc:0.990]
Epoch [59/120    avg_loss:0.039, val_acc:0.988]
Epoch [60/120    avg_loss:0.074, val_acc:0.986]
Epoch [61/120    avg_loss:0.088, val_acc:0.988]
Epoch [62/120    avg_loss:0.057, val_acc:0.964]
Epoch [63/120    avg_loss:0.068, val_acc:0.978]
Epoch [64/120    avg_loss:0.033, val_acc:0.978]
Epoch [65/120    avg_loss:0.057, val_acc:0.980]
Epoch [66/120    avg_loss:0.043, val_acc:0.982]
Epoch [67/120    avg_loss:0.050, val_acc:0.984]
Epoch [68/120    avg_loss:0.053, val_acc:0.982]
Epoch [69/120    avg_loss:0.023, val_acc:0.988]
Epoch [70/120    avg_loss:0.031, val_acc:0.994]
Epoch [71/120    avg_loss:0.017, val_acc:0.994]
Epoch [72/120    avg_loss:0.032, val_acc:0.980]
Epoch [73/120    avg_loss:0.037, val_acc:0.986]
Epoch [74/120    avg_loss:0.036, val_acc:0.988]
Epoch [75/120    avg_loss:0.019, val_acc:0.994]
Epoch [76/120    avg_loss:0.013, val_acc:0.996]
Epoch [77/120    avg_loss:0.019, val_acc:0.992]
Epoch [78/120    avg_loss:0.014, val_acc:0.994]
Epoch [79/120    avg_loss:0.034, val_acc:0.994]
Epoch [80/120    avg_loss:0.022, val_acc:0.994]
Epoch [81/120    avg_loss:0.011, val_acc:0.994]
Epoch [82/120    avg_loss:0.015, val_acc:0.994]
Epoch [83/120    avg_loss:0.014, val_acc:0.988]
Epoch [84/120    avg_loss:0.020, val_acc:0.996]
Epoch [85/120    avg_loss:0.059, val_acc:0.952]
Epoch [86/120    avg_loss:0.026, val_acc:0.990]
Epoch [87/120    avg_loss:0.014, val_acc:0.996]
Epoch [88/120    avg_loss:0.010, val_acc:0.996]
Epoch [89/120    avg_loss:0.009, val_acc:0.994]
Epoch [90/120    avg_loss:0.010, val_acc:0.996]
Epoch [91/120    avg_loss:0.007, val_acc:0.996]
Epoch [92/120    avg_loss:0.010, val_acc:0.994]
Epoch [93/120    avg_loss:0.007, val_acc:0.996]
Epoch [94/120    avg_loss:0.007, val_acc:0.996]
Epoch [95/120    avg_loss:0.007, val_acc:0.996]
Epoch [96/120    avg_loss:0.006, val_acc:0.994]
Epoch [97/120    avg_loss:0.004, val_acc:0.996]
Epoch [98/120    avg_loss:0.007, val_acc:0.996]
Epoch [99/120    avg_loss:0.007, val_acc:0.992]
Epoch [100/120    avg_loss:0.018, val_acc:0.996]
Epoch [101/120    avg_loss:0.016, val_acc:0.988]
Epoch [102/120    avg_loss:0.011, val_acc:0.996]
Epoch [103/120    avg_loss:0.009, val_acc:0.994]
Epoch [104/120    avg_loss:0.011, val_acc:0.994]
Epoch [105/120    avg_loss:0.006, val_acc:0.992]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.006, val_acc:0.992]
Epoch [109/120    avg_loss:0.005, val_acc:0.994]
Epoch [110/120    avg_loss:0.006, val_acc:0.990]
Epoch [111/120    avg_loss:0.008, val_acc:0.992]
Epoch [112/120    avg_loss:0.007, val_acc:0.996]
Epoch [113/120    avg_loss:0.013, val_acc:0.984]
Epoch [114/120    avg_loss:0.011, val_acc:0.996]
Epoch [115/120    avg_loss:0.006, val_acc:0.994]
Epoch [116/120    avg_loss:0.007, val_acc:0.996]
Epoch [117/120    avg_loss:0.006, val_acc:0.996]
Epoch [118/120    avg_loss:0.005, val_acc:0.994]
Epoch [119/120    avg_loss:0.006, val_acc:0.996]
Epoch [120/120    avg_loss:0.119, val_acc:0.909]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 226   0   0   0   4   0   0   0   0   0   0]
 [  0   0   0  69 155   0   0   0   0   0   0   0   3   0]
 [  0   0   0  11 119  15   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 145  61   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   1   0   0   0  54   0 321   1   0]
 [  0   0   1   0   0   0   0   0   0   0   0   0 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
93.09168443496802

F1 scores:
[       nan 1.         0.9977221  0.84328358 0.61876248 0.1863354
 0.82621083 0.743083   1.         0.94545455 1.         0.91977077
 0.99449945 1.        ]

Kappa:
0.9230544247807327
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe84bd41898>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.230, val_acc:0.540]
Epoch [2/120    avg_loss:1.648, val_acc:0.643]
Epoch [3/120    avg_loss:1.224, val_acc:0.728]
Epoch [4/120    avg_loss:1.031, val_acc:0.776]
Epoch [5/120    avg_loss:0.853, val_acc:0.778]
Epoch [6/120    avg_loss:0.852, val_acc:0.730]
Epoch [7/120    avg_loss:0.743, val_acc:0.831]
Epoch [8/120    avg_loss:0.625, val_acc:0.867]
Epoch [9/120    avg_loss:0.567, val_acc:0.883]
Epoch [10/120    avg_loss:0.484, val_acc:0.887]
Epoch [11/120    avg_loss:0.476, val_acc:0.881]
Epoch [12/120    avg_loss:0.433, val_acc:0.869]
Epoch [13/120    avg_loss:0.384, val_acc:0.899]
Epoch [14/120    avg_loss:0.369, val_acc:0.881]
Epoch [15/120    avg_loss:0.378, val_acc:0.847]
Epoch [16/120    avg_loss:0.392, val_acc:0.931]
Epoch [17/120    avg_loss:0.421, val_acc:0.913]
Epoch [18/120    avg_loss:0.299, val_acc:0.946]
Epoch [19/120    avg_loss:0.251, val_acc:0.923]
Epoch [20/120    avg_loss:0.271, val_acc:0.944]
Epoch [21/120    avg_loss:0.288, val_acc:0.925]
Epoch [22/120    avg_loss:0.226, val_acc:0.919]
Epoch [23/120    avg_loss:0.234, val_acc:0.946]
Epoch [24/120    avg_loss:0.269, val_acc:0.942]
Epoch [25/120    avg_loss:0.209, val_acc:0.933]
Epoch [26/120    avg_loss:0.235, val_acc:0.944]
Epoch [27/120    avg_loss:0.255, val_acc:0.944]
Epoch [28/120    avg_loss:0.207, val_acc:0.929]
Epoch [29/120    avg_loss:0.253, val_acc:0.923]
Epoch [30/120    avg_loss:0.209, val_acc:0.944]
Epoch [31/120    avg_loss:0.199, val_acc:0.933]
Epoch [32/120    avg_loss:0.135, val_acc:0.974]
Epoch [33/120    avg_loss:0.135, val_acc:0.944]
Epoch [34/120    avg_loss:0.180, val_acc:0.970]
Epoch [35/120    avg_loss:0.098, val_acc:0.970]
Epoch [36/120    avg_loss:0.114, val_acc:0.974]
Epoch [37/120    avg_loss:0.144, val_acc:0.948]
Epoch [38/120    avg_loss:0.197, val_acc:0.958]
Epoch [39/120    avg_loss:0.151, val_acc:0.958]
Epoch [40/120    avg_loss:0.095, val_acc:0.950]
Epoch [41/120    avg_loss:0.124, val_acc:0.974]
Epoch [42/120    avg_loss:0.127, val_acc:0.978]
Epoch [43/120    avg_loss:0.135, val_acc:0.968]
Epoch [44/120    avg_loss:0.119, val_acc:0.972]
Epoch [45/120    avg_loss:0.071, val_acc:0.976]
Epoch [46/120    avg_loss:0.050, val_acc:0.980]
Epoch [47/120    avg_loss:0.079, val_acc:0.972]
Epoch [48/120    avg_loss:0.080, val_acc:0.974]
Epoch [49/120    avg_loss:0.099, val_acc:0.974]
Epoch [50/120    avg_loss:0.106, val_acc:0.962]
Epoch [51/120    avg_loss:0.133, val_acc:0.982]
Epoch [52/120    avg_loss:0.103, val_acc:0.976]
Epoch [53/120    avg_loss:0.068, val_acc:0.982]
Epoch [54/120    avg_loss:0.080, val_acc:0.970]
Epoch [55/120    avg_loss:0.078, val_acc:0.966]
Epoch [56/120    avg_loss:0.045, val_acc:0.982]
Epoch [57/120    avg_loss:0.045, val_acc:0.980]
Epoch [58/120    avg_loss:0.045, val_acc:0.972]
Epoch [59/120    avg_loss:0.061, val_acc:0.962]
Epoch [60/120    avg_loss:0.078, val_acc:0.968]
Epoch [61/120    avg_loss:0.042, val_acc:0.982]
Epoch [62/120    avg_loss:0.029, val_acc:0.988]
Epoch [63/120    avg_loss:0.037, val_acc:0.982]
Epoch [64/120    avg_loss:0.041, val_acc:0.968]
Epoch [65/120    avg_loss:0.056, val_acc:0.984]
Epoch [66/120    avg_loss:0.027, val_acc:0.984]
Epoch [67/120    avg_loss:0.033, val_acc:0.984]
Epoch [68/120    avg_loss:0.024, val_acc:0.984]
Epoch [69/120    avg_loss:0.018, val_acc:0.974]
Epoch [70/120    avg_loss:0.027, val_acc:0.990]
Epoch [71/120    avg_loss:0.040, val_acc:0.984]
Epoch [72/120    avg_loss:0.041, val_acc:0.976]
Epoch [73/120    avg_loss:0.025, val_acc:0.978]
Epoch [74/120    avg_loss:0.064, val_acc:0.960]
Epoch [75/120    avg_loss:0.047, val_acc:0.978]
Epoch [76/120    avg_loss:0.072, val_acc:0.984]
Epoch [77/120    avg_loss:0.063, val_acc:0.972]
Epoch [78/120    avg_loss:0.047, val_acc:0.986]
Epoch [79/120    avg_loss:0.092, val_acc:0.960]
Epoch [80/120    avg_loss:0.043, val_acc:0.984]
Epoch [81/120    avg_loss:0.023, val_acc:0.984]
Epoch [82/120    avg_loss:0.018, val_acc:0.984]
Epoch [83/120    avg_loss:0.011, val_acc:0.986]
Epoch [84/120    avg_loss:0.012, val_acc:0.990]
Epoch [85/120    avg_loss:0.016, val_acc:0.990]
Epoch [86/120    avg_loss:0.011, val_acc:0.990]
Epoch [87/120    avg_loss:0.012, val_acc:0.990]
Epoch [88/120    avg_loss:0.011, val_acc:0.990]
Epoch [89/120    avg_loss:0.013, val_acc:0.988]
Epoch [90/120    avg_loss:0.018, val_acc:0.986]
Epoch [91/120    avg_loss:0.014, val_acc:0.988]
Epoch [92/120    avg_loss:0.006, val_acc:0.988]
Epoch [93/120    avg_loss:0.009, val_acc:0.988]
Epoch [94/120    avg_loss:0.011, val_acc:0.992]
Epoch [95/120    avg_loss:0.011, val_acc:0.990]
Epoch [96/120    avg_loss:0.014, val_acc:0.992]
Epoch [97/120    avg_loss:0.010, val_acc:0.992]
Epoch [98/120    avg_loss:0.016, val_acc:0.992]
Epoch [99/120    avg_loss:0.009, val_acc:0.992]
Epoch [100/120    avg_loss:0.010, val_acc:0.992]
Epoch [101/120    avg_loss:0.009, val_acc:0.992]
Epoch [102/120    avg_loss:0.012, val_acc:0.992]
Epoch [103/120    avg_loss:0.010, val_acc:0.992]
Epoch [104/120    avg_loss:0.011, val_acc:0.992]
Epoch [105/120    avg_loss:0.008, val_acc:0.992]
Epoch [106/120    avg_loss:0.010, val_acc:0.992]
Epoch [107/120    avg_loss:0.009, val_acc:0.992]
Epoch [108/120    avg_loss:0.008, val_acc:0.992]
Epoch [109/120    avg_loss:0.011, val_acc:0.992]
Epoch [110/120    avg_loss:0.011, val_acc:0.992]
Epoch [111/120    avg_loss:0.009, val_acc:0.992]
Epoch [112/120    avg_loss:0.010, val_acc:0.992]
Epoch [113/120    avg_loss:0.011, val_acc:0.992]
Epoch [114/120    avg_loss:0.009, val_acc:0.990]
Epoch [115/120    avg_loss:0.010, val_acc:0.990]
Epoch [116/120    avg_loss:0.012, val_acc:0.990]
Epoch [117/120    avg_loss:0.011, val_acc:0.992]
Epoch [118/120    avg_loss:0.010, val_acc:0.992]
Epoch [119/120    avg_loss:0.009, val_acc:0.992]
Epoch [120/120    avg_loss:0.008, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  10   0   0   0   0   0   0   1   0]
 [  0   0   0   0   5 140   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0  94   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 376   1   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.63752665245202

F1 scores:
[       nan 1.         1.         1.         0.96428571 0.94915254
 1.         1.         1.         1.         1.         0.99867198
 0.99779736 1.        ]

Kappa:
0.9959645373278525
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff628e06828>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.275, val_acc:0.544]
Epoch [2/120    avg_loss:1.725, val_acc:0.615]
Epoch [3/120    avg_loss:1.298, val_acc:0.673]
Epoch [4/120    avg_loss:1.038, val_acc:0.748]
Epoch [5/120    avg_loss:0.844, val_acc:0.837]
Epoch [6/120    avg_loss:0.685, val_acc:0.847]
Epoch [7/120    avg_loss:0.661, val_acc:0.885]
Epoch [8/120    avg_loss:0.537, val_acc:0.825]
Epoch [9/120    avg_loss:0.588, val_acc:0.893]
Epoch [10/120    avg_loss:0.465, val_acc:0.903]
Epoch [11/120    avg_loss:0.414, val_acc:0.893]
Epoch [12/120    avg_loss:0.430, val_acc:0.891]
Epoch [13/120    avg_loss:0.366, val_acc:0.893]
Epoch [14/120    avg_loss:0.364, val_acc:0.909]
Epoch [15/120    avg_loss:0.354, val_acc:0.905]
Epoch [16/120    avg_loss:0.326, val_acc:0.931]
Epoch [17/120    avg_loss:0.318, val_acc:0.891]
Epoch [18/120    avg_loss:0.308, val_acc:0.940]
Epoch [19/120    avg_loss:0.262, val_acc:0.919]
Epoch [20/120    avg_loss:0.253, val_acc:0.938]
Epoch [21/120    avg_loss:0.261, val_acc:0.942]
Epoch [22/120    avg_loss:0.248, val_acc:0.897]
Epoch [23/120    avg_loss:0.291, val_acc:0.931]
Epoch [24/120    avg_loss:0.193, val_acc:0.946]
Epoch [25/120    avg_loss:0.201, val_acc:0.956]
Epoch [26/120    avg_loss:0.230, val_acc:0.861]
Epoch [27/120    avg_loss:0.277, val_acc:0.935]
Epoch [28/120    avg_loss:0.199, val_acc:0.958]
Epoch [29/120    avg_loss:0.183, val_acc:0.942]
Epoch [30/120    avg_loss:0.165, val_acc:0.927]
Epoch [31/120    avg_loss:0.220, val_acc:0.960]
Epoch [32/120    avg_loss:0.241, val_acc:0.948]
Epoch [33/120    avg_loss:0.175, val_acc:0.946]
Epoch [34/120    avg_loss:0.190, val_acc:0.950]
Epoch [35/120    avg_loss:0.183, val_acc:0.964]
Epoch [36/120    avg_loss:0.134, val_acc:0.933]
Epoch [37/120    avg_loss:0.187, val_acc:0.946]
Epoch [38/120    avg_loss:0.122, val_acc:0.978]
Epoch [39/120    avg_loss:0.138, val_acc:0.972]
Epoch [40/120    avg_loss:0.126, val_acc:0.942]
Epoch [41/120    avg_loss:0.131, val_acc:0.954]
Epoch [42/120    avg_loss:0.166, val_acc:0.966]
Epoch [43/120    avg_loss:0.108, val_acc:0.960]
Epoch [44/120    avg_loss:0.146, val_acc:0.958]
Epoch [45/120    avg_loss:0.115, val_acc:0.982]
Epoch [46/120    avg_loss:0.096, val_acc:0.966]
Epoch [47/120    avg_loss:0.102, val_acc:0.970]
Epoch [48/120    avg_loss:0.130, val_acc:0.976]
Epoch [49/120    avg_loss:0.114, val_acc:0.974]
Epoch [50/120    avg_loss:0.091, val_acc:0.982]
Epoch [51/120    avg_loss:0.115, val_acc:0.982]
Epoch [52/120    avg_loss:0.141, val_acc:0.978]
Epoch [53/120    avg_loss:0.085, val_acc:0.976]
Epoch [54/120    avg_loss:0.059, val_acc:0.992]
Epoch [55/120    avg_loss:0.058, val_acc:0.984]
Epoch [56/120    avg_loss:0.055, val_acc:0.966]
Epoch [57/120    avg_loss:0.057, val_acc:0.980]
Epoch [58/120    avg_loss:0.044, val_acc:0.968]
Epoch [59/120    avg_loss:0.044, val_acc:0.992]
Epoch [60/120    avg_loss:0.042, val_acc:0.978]
Epoch [61/120    avg_loss:0.050, val_acc:0.990]
Epoch [62/120    avg_loss:0.032, val_acc:0.984]
Epoch [63/120    avg_loss:0.036, val_acc:0.980]
Epoch [64/120    avg_loss:0.032, val_acc:0.974]
Epoch [65/120    avg_loss:0.033, val_acc:0.994]
Epoch [66/120    avg_loss:0.042, val_acc:0.980]
Epoch [67/120    avg_loss:0.036, val_acc:0.990]
Epoch [68/120    avg_loss:0.027, val_acc:0.994]
Epoch [69/120    avg_loss:0.023, val_acc:0.996]
Epoch [70/120    avg_loss:0.027, val_acc:0.988]
Epoch [71/120    avg_loss:0.022, val_acc:0.988]
Epoch [72/120    avg_loss:0.025, val_acc:0.990]
Epoch [73/120    avg_loss:0.040, val_acc:0.982]
Epoch [74/120    avg_loss:0.107, val_acc:0.980]
Epoch [75/120    avg_loss:0.044, val_acc:0.986]
Epoch [76/120    avg_loss:0.026, val_acc:0.982]
Epoch [77/120    avg_loss:0.019, val_acc:0.982]
Epoch [78/120    avg_loss:0.041, val_acc:0.980]
Epoch [79/120    avg_loss:0.031, val_acc:0.998]
Epoch [80/120    avg_loss:0.022, val_acc:0.984]
Epoch [81/120    avg_loss:0.031, val_acc:0.982]
Epoch [82/120    avg_loss:0.031, val_acc:0.986]
Epoch [83/120    avg_loss:0.015, val_acc:0.992]
Epoch [84/120    avg_loss:0.023, val_acc:0.984]
Epoch [85/120    avg_loss:0.027, val_acc:0.996]
Epoch [86/120    avg_loss:0.015, val_acc:0.992]
Epoch [87/120    avg_loss:0.011, val_acc:0.996]
Epoch [88/120    avg_loss:0.011, val_acc:0.984]
Epoch [89/120    avg_loss:0.018, val_acc:0.992]
Epoch [90/120    avg_loss:0.018, val_acc:0.978]
Epoch [91/120    avg_loss:0.017, val_acc:0.986]
Epoch [92/120    avg_loss:0.010, val_acc:0.992]
Epoch [93/120    avg_loss:0.011, val_acc:0.992]
Epoch [94/120    avg_loss:0.012, val_acc:0.992]
Epoch [95/120    avg_loss:0.008, val_acc:0.992]
Epoch [96/120    avg_loss:0.008, val_acc:0.992]
Epoch [97/120    avg_loss:0.007, val_acc:0.992]
Epoch [98/120    avg_loss:0.010, val_acc:0.992]
Epoch [99/120    avg_loss:0.007, val_acc:0.992]
Epoch [100/120    avg_loss:0.011, val_acc:0.994]
Epoch [101/120    avg_loss:0.011, val_acc:0.992]
Epoch [102/120    avg_loss:0.008, val_acc:0.992]
Epoch [103/120    avg_loss:0.008, val_acc:0.992]
Epoch [104/120    avg_loss:0.017, val_acc:0.990]
Epoch [105/120    avg_loss:0.012, val_acc:0.990]
Epoch [106/120    avg_loss:0.008, val_acc:0.990]
Epoch [107/120    avg_loss:0.007, val_acc:0.990]
Epoch [108/120    avg_loss:0.007, val_acc:0.990]
Epoch [109/120    avg_loss:0.007, val_acc:0.992]
Epoch [110/120    avg_loss:0.008, val_acc:0.992]
Epoch [111/120    avg_loss:0.008, val_acc:0.994]
Epoch [112/120    avg_loss:0.008, val_acc:0.992]
Epoch [113/120    avg_loss:0.006, val_acc:0.992]
Epoch [114/120    avg_loss:0.006, val_acc:0.994]
Epoch [115/120    avg_loss:0.007, val_acc:0.994]
Epoch [116/120    avg_loss:0.009, val_acc:0.994]
Epoch [117/120    avg_loss:0.006, val_acc:0.994]
Epoch [118/120    avg_loss:0.007, val_acc:0.994]
Epoch [119/120    avg_loss:0.012, val_acc:0.994]
Epoch [120/120    avg_loss:0.009, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 216  10   0   0   0   0   0   0   1   0]
 [  0   0   0   0   9 136   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   8   0   0   0   0  86   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   1 452   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.3816631130064

F1 scores:
[       nan 1.         0.98206278 1.         0.95575221 0.9347079
 1.         0.95555556 1.         1.         1.         0.9986755
 0.99779249 1.        ]

Kappa:
0.9931155656650316
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:31
Validation dataloader:31
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f483f3e5828>
supervision:full
center_pixel:True
Network :
Number of parameter: 93447==>0.09M
----------Training process----------
Epoch [1/120    avg_loss:2.295, val_acc:0.512]
Epoch [2/120    avg_loss:1.781, val_acc:0.579]
Epoch [3/120    avg_loss:1.408, val_acc:0.671]
Epoch [4/120    avg_loss:1.116, val_acc:0.712]
Epoch [5/120    avg_loss:0.950, val_acc:0.806]
Epoch [6/120    avg_loss:0.809, val_acc:0.821]
Epoch [7/120    avg_loss:0.710, val_acc:0.804]
Epoch [8/120    avg_loss:0.657, val_acc:0.808]
Epoch [9/120    avg_loss:0.617, val_acc:0.853]
Epoch [10/120    avg_loss:0.547, val_acc:0.891]
Epoch [11/120    avg_loss:0.504, val_acc:0.879]
Epoch [12/120    avg_loss:0.496, val_acc:0.885]
Epoch [13/120    avg_loss:0.470, val_acc:0.865]
Epoch [14/120    avg_loss:0.391, val_acc:0.895]
Epoch [15/120    avg_loss:0.424, val_acc:0.909]
Epoch [16/120    avg_loss:0.325, val_acc:0.903]
Epoch [17/120    avg_loss:0.312, val_acc:0.911]
Epoch [18/120    avg_loss:0.353, val_acc:0.952]
Epoch [19/120    avg_loss:0.276, val_acc:0.915]
Epoch [20/120    avg_loss:0.272, val_acc:0.921]
Epoch [21/120    avg_loss:0.234, val_acc:0.940]
Epoch [22/120    avg_loss:0.244, val_acc:0.921]
Epoch [23/120    avg_loss:0.247, val_acc:0.901]
Epoch [24/120    avg_loss:0.298, val_acc:0.954]
Epoch [25/120    avg_loss:0.280, val_acc:0.938]
Epoch [26/120    avg_loss:0.192, val_acc:0.968]
Epoch [27/120    avg_loss:0.178, val_acc:0.966]
Epoch [28/120    avg_loss:0.144, val_acc:0.968]
Epoch [29/120    avg_loss:0.206, val_acc:0.960]
Epoch [30/120    avg_loss:0.238, val_acc:0.929]
Epoch [31/120    avg_loss:0.161, val_acc:0.927]
Epoch [32/120    avg_loss:0.198, val_acc:0.972]
Epoch [33/120    avg_loss:0.174, val_acc:0.964]
Epoch [34/120    avg_loss:0.124, val_acc:0.968]
Epoch [35/120    avg_loss:0.134, val_acc:0.962]
Epoch [36/120    avg_loss:0.154, val_acc:0.958]
Epoch [37/120    avg_loss:0.156, val_acc:0.919]
Epoch [38/120    avg_loss:0.149, val_acc:0.931]
Epoch [39/120    avg_loss:0.194, val_acc:0.946]
Epoch [40/120    avg_loss:0.112, val_acc:0.976]
Epoch [41/120    avg_loss:0.112, val_acc:0.976]
Epoch [42/120    avg_loss:0.111, val_acc:0.978]
Epoch [43/120    avg_loss:0.111, val_acc:0.978]
Epoch [44/120    avg_loss:0.088, val_acc:0.986]
Epoch [45/120    avg_loss:0.066, val_acc:0.982]
Epoch [46/120    avg_loss:0.083, val_acc:0.974]
Epoch [47/120    avg_loss:0.051, val_acc:0.982]
Epoch [48/120    avg_loss:0.055, val_acc:0.964]
Epoch [49/120    avg_loss:0.084, val_acc:0.982]
Epoch [50/120    avg_loss:0.065, val_acc:0.980]
Epoch [51/120    avg_loss:0.065, val_acc:0.982]
Epoch [52/120    avg_loss:0.093, val_acc:0.982]
Epoch [53/120    avg_loss:0.099, val_acc:0.976]
Epoch [54/120    avg_loss:0.086, val_acc:0.974]
Epoch [55/120    avg_loss:0.047, val_acc:0.990]
Epoch [56/120    avg_loss:0.034, val_acc:0.988]
Epoch [57/120    avg_loss:0.034, val_acc:0.986]
Epoch [58/120    avg_loss:0.041, val_acc:0.986]
Epoch [59/120    avg_loss:0.030, val_acc:0.974]
Epoch [60/120    avg_loss:0.034, val_acc:0.982]
Epoch [61/120    avg_loss:0.025, val_acc:0.984]
Epoch [62/120    avg_loss:0.019, val_acc:0.990]
Epoch [63/120    avg_loss:0.040, val_acc:0.954]
Epoch [64/120    avg_loss:0.065, val_acc:0.978]
Epoch [65/120    avg_loss:0.041, val_acc:0.988]
Epoch [66/120    avg_loss:0.027, val_acc:0.984]
Epoch [67/120    avg_loss:0.049, val_acc:0.978]
Epoch [68/120    avg_loss:0.032, val_acc:0.988]
Epoch [69/120    avg_loss:0.048, val_acc:0.986]
Epoch [70/120    avg_loss:0.045, val_acc:0.917]
Epoch [71/120    avg_loss:0.062, val_acc:0.938]
Epoch [72/120    avg_loss:0.038, val_acc:0.970]
Epoch [73/120    avg_loss:0.022, val_acc:0.986]
Epoch [74/120    avg_loss:0.018, val_acc:0.986]
Epoch [75/120    avg_loss:0.027, val_acc:0.988]
Epoch [76/120    avg_loss:0.021, val_acc:0.990]
Epoch [77/120    avg_loss:0.020, val_acc:0.992]
Epoch [78/120    avg_loss:0.015, val_acc:0.986]
Epoch [79/120    avg_loss:0.017, val_acc:0.986]
Epoch [80/120    avg_loss:0.010, val_acc:0.988]
Epoch [81/120    avg_loss:0.014, val_acc:0.986]
Epoch [82/120    avg_loss:0.015, val_acc:0.992]
Epoch [83/120    avg_loss:0.012, val_acc:0.990]
Epoch [84/120    avg_loss:0.016, val_acc:0.994]
Epoch [85/120    avg_loss:0.016, val_acc:0.992]
Epoch [86/120    avg_loss:0.011, val_acc:0.992]
Epoch [87/120    avg_loss:0.014, val_acc:0.992]
Epoch [88/120    avg_loss:0.011, val_acc:0.992]
Epoch [89/120    avg_loss:0.013, val_acc:0.992]
Epoch [90/120    avg_loss:0.011, val_acc:0.992]
Epoch [91/120    avg_loss:0.010, val_acc:0.992]
Epoch [92/120    avg_loss:0.009, val_acc:0.992]
Epoch [93/120    avg_loss:0.009, val_acc:0.992]
Epoch [94/120    avg_loss:0.011, val_acc:0.992]
Epoch [95/120    avg_loss:0.011, val_acc:0.992]
Epoch [96/120    avg_loss:0.010, val_acc:0.992]
Epoch [97/120    avg_loss:0.011, val_acc:0.992]
Epoch [98/120    avg_loss:0.008, val_acc:0.992]
Epoch [99/120    avg_loss:0.012, val_acc:0.992]
Epoch [100/120    avg_loss:0.014, val_acc:0.992]
Epoch [101/120    avg_loss:0.015, val_acc:0.992]
Epoch [102/120    avg_loss:0.007, val_acc:0.992]
Epoch [103/120    avg_loss:0.010, val_acc:0.992]
Epoch [104/120    avg_loss:0.010, val_acc:0.992]
Epoch [105/120    avg_loss:0.010, val_acc:0.992]
Epoch [106/120    avg_loss:0.010, val_acc:0.992]
Epoch [107/120    avg_loss:0.016, val_acc:0.992]
Epoch [108/120    avg_loss:0.009, val_acc:0.992]
Epoch [109/120    avg_loss:0.013, val_acc:0.992]
Epoch [110/120    avg_loss:0.010, val_acc:0.992]
Epoch [111/120    avg_loss:0.014, val_acc:0.992]
Epoch [112/120    avg_loss:0.010, val_acc:0.992]
Epoch [113/120    avg_loss:0.012, val_acc:0.992]
Epoch [114/120    avg_loss:0.012, val_acc:0.992]
Epoch [115/120    avg_loss:0.009, val_acc:0.992]
Epoch [116/120    avg_loss:0.011, val_acc:0.992]
Epoch [117/120    avg_loss:0.008, val_acc:0.992]
Epoch [118/120    avg_loss:0.015, val_acc:0.992]
Epoch [119/120    avg_loss:0.010, val_acc:0.992]
Epoch [120/120    avg_loss:0.013, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 685   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 213  12   0   0   0   0   0   0   2   0]
 [  0   0   0   0  14 131   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   7   0   0   0   0  87   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.25373134328358

F1 scores:
[       nan 1.         0.98426966 1.         0.93832599 0.90972222
 1.         0.96132597 1.         1.         1.         1.
 0.99779736 1.        ]

Kappa:
0.9916910220366851
creating ./logs/logs-2022-01-18KSC.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-18:23:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/KSC/0.10/train_gt.npy)
521 samples selected for training(over 5211)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/KSC/0.10/test_gt.npy)
4690 samples selected for training(over 5211)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
521 samples selected for validation(over 5211)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:KSC
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:80
reserve_bands:160
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:14
n_bands:160
ignored_labels:[0]
device:cuda:0
weights:tensor([0.0000, 0.5263, 1.6667, 1.5385, 1.6000, 2.5000, 1.7391, 3.6364, 0.9302,
        0.7692, 1.0000, 0.9524, 0.8000, 0.4301], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa718a118d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103687==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.239, val_acc:0.548]
Epoch [2/120    avg_loss:1.626, val_acc:0.660]
Epoch [3/120    avg_loss:1.244, val_acc:0.713]
Epoch [4/120    avg_loss:1.025, val_acc:0.754]
Epoch [5/120    avg_loss:0.840, val_acc:0.825]
Epoch [6/120    avg_loss:0.742, val_acc:0.810]
Epoch [7/120    avg_loss:0.695, val_acc:0.833]
Epoch [8/120    avg_loss:0.600, val_acc:0.844]
Epoch [9/120    avg_loss:0.537, val_acc:0.867]
Epoch [10/120    avg_loss:0.446, val_acc:0.823]
Epoch [11/120    avg_loss:0.488, val_acc:0.863]
Epoch [12/120    avg_loss:0.493, val_acc:0.885]
Epoch [13/120    avg_loss:0.487, val_acc:0.873]
Epoch [14/120    avg_loss:0.415, val_acc:0.892]
Epoch [15/120    avg_loss:0.368, val_acc:0.887]
Epoch [16/120    avg_loss:0.494, val_acc:0.863]
Epoch [17/120    avg_loss:0.396, val_acc:0.915]
Epoch [18/120    avg_loss:0.336, val_acc:0.931]
Epoch [19/120    avg_loss:0.338, val_acc:0.906]
Epoch [20/120    avg_loss:0.318, val_acc:0.915]
Epoch [21/120    avg_loss:0.251, val_acc:0.938]
Epoch [22/120    avg_loss:0.333, val_acc:0.919]
Epoch [23/120    avg_loss:0.276, val_acc:0.954]
Epoch [24/120    avg_loss:0.245, val_acc:0.935]
Epoch [25/120    avg_loss:0.262, val_acc:0.910]
Epoch [26/120    avg_loss:0.202, val_acc:0.938]
Epoch [27/120    avg_loss:0.153, val_acc:0.950]
Epoch [28/120    avg_loss:0.166, val_acc:0.942]
Epoch [29/120    avg_loss:0.292, val_acc:0.950]
Epoch [30/120    avg_loss:0.184, val_acc:0.950]
Epoch [31/120    avg_loss:0.211, val_acc:0.956]
Epoch [32/120    avg_loss:0.213, val_acc:0.971]
Epoch [33/120    avg_loss:0.143, val_acc:0.958]
Epoch [34/120    avg_loss:0.140, val_acc:0.952]
Epoch [35/120    avg_loss:0.172, val_acc:0.969]
Epoch [36/120    avg_loss:0.174, val_acc:0.948]
Epoch [37/120    avg_loss:0.166, val_acc:0.967]
Epoch [38/120    avg_loss:0.158, val_acc:0.958]
Epoch [39/120    avg_loss:0.144, val_acc:0.952]
Epoch [40/120    avg_loss:0.127, val_acc:0.950]
Epoch [41/120    avg_loss:0.129, val_acc:0.975]
Epoch [42/120    avg_loss:0.095, val_acc:0.971]
Epoch [43/120    avg_loss:0.127, val_acc:0.952]
Epoch [44/120    avg_loss:0.086, val_acc:0.967]
Epoch [45/120    avg_loss:0.080, val_acc:0.969]
Epoch [46/120    avg_loss:0.151, val_acc:0.971]
Epoch [47/120    avg_loss:0.114, val_acc:0.973]
Epoch [48/120    avg_loss:0.127, val_acc:0.965]
Epoch [49/120    avg_loss:0.107, val_acc:0.954]
Epoch [50/120    avg_loss:0.115, val_acc:0.973]
Epoch [51/120    avg_loss:0.065, val_acc:0.975]
Epoch [52/120    avg_loss:0.047, val_acc:0.981]
Epoch [53/120    avg_loss:0.046, val_acc:0.969]
Epoch [54/120    avg_loss:0.046, val_acc:0.975]
Epoch [55/120    avg_loss:0.047, val_acc:0.979]
Epoch [56/120    avg_loss:0.063, val_acc:0.977]
Epoch [57/120    avg_loss:0.043, val_acc:0.988]
Epoch [58/120    avg_loss:0.044, val_acc:0.977]
Epoch [59/120    avg_loss:0.083, val_acc:0.981]
Epoch [60/120    avg_loss:0.062, val_acc:0.977]
Epoch [61/120    avg_loss:0.066, val_acc:0.973]
Epoch [62/120    avg_loss:0.103, val_acc:0.973]
Epoch [63/120    avg_loss:0.112, val_acc:0.969]
Epoch [64/120    avg_loss:0.084, val_acc:0.963]
Epoch [65/120    avg_loss:0.081, val_acc:0.977]
Epoch [66/120    avg_loss:0.046, val_acc:0.975]
Epoch [67/120    avg_loss:0.049, val_acc:0.969]
Epoch [68/120    avg_loss:0.052, val_acc:0.979]
Epoch [69/120    avg_loss:0.066, val_acc:0.969]
Epoch [70/120    avg_loss:0.068, val_acc:0.975]
Epoch [71/120    avg_loss:0.054, val_acc:0.985]
Epoch [72/120    avg_loss:0.032, val_acc:0.983]
Epoch [73/120    avg_loss:0.018, val_acc:0.983]
Epoch [74/120    avg_loss:0.019, val_acc:0.983]
Epoch [75/120    avg_loss:0.016, val_acc:0.983]
Epoch [76/120    avg_loss:0.025, val_acc:0.983]
Epoch [77/120    avg_loss:0.018, val_acc:0.983]
Epoch [78/120    avg_loss:0.025, val_acc:0.981]
Epoch [79/120    avg_loss:0.019, val_acc:0.983]
Epoch [80/120    avg_loss:0.019, val_acc:0.983]
Epoch [81/120    avg_loss:0.018, val_acc:0.983]
Epoch [82/120    avg_loss:0.023, val_acc:0.983]
Epoch [83/120    avg_loss:0.016, val_acc:0.985]
Epoch [84/120    avg_loss:0.013, val_acc:0.985]
Epoch [85/120    avg_loss:0.018, val_acc:0.985]
Epoch [86/120    avg_loss:0.014, val_acc:0.985]
Epoch [87/120    avg_loss:0.019, val_acc:0.985]
Epoch [88/120    avg_loss:0.014, val_acc:0.985]
Epoch [89/120    avg_loss:0.018, val_acc:0.985]
Epoch [90/120    avg_loss:0.014, val_acc:0.985]
Epoch [91/120    avg_loss:0.016, val_acc:0.985]
Epoch [92/120    avg_loss:0.019, val_acc:0.985]
Epoch [93/120    avg_loss:0.025, val_acc:0.985]
Epoch [94/120    avg_loss:0.020, val_acc:0.985]
Epoch [95/120    avg_loss:0.023, val_acc:0.985]
Epoch [96/120    avg_loss:0.019, val_acc:0.985]
Epoch [97/120    avg_loss:0.020, val_acc:0.985]
Epoch [98/120    avg_loss:0.016, val_acc:0.985]
Epoch [99/120    avg_loss:0.015, val_acc:0.985]
Epoch [100/120    avg_loss:0.019, val_acc:0.985]
Epoch [101/120    avg_loss:0.023, val_acc:0.985]
Epoch [102/120    avg_loss:0.014, val_acc:0.985]
Epoch [103/120    avg_loss:0.016, val_acc:0.985]
Epoch [104/120    avg_loss:0.019, val_acc:0.985]
Epoch [105/120    avg_loss:0.017, val_acc:0.985]
Epoch [106/120    avg_loss:0.015, val_acc:0.985]
Epoch [107/120    avg_loss:0.015, val_acc:0.985]
Epoch [108/120    avg_loss:0.016, val_acc:0.985]
Epoch [109/120    avg_loss:0.014, val_acc:0.985]
Epoch [110/120    avg_loss:0.018, val_acc:0.985]
Epoch [111/120    avg_loss:0.018, val_acc:0.985]
Epoch [112/120    avg_loss:0.013, val_acc:0.985]
Epoch [113/120    avg_loss:0.018, val_acc:0.985]
Epoch [114/120    avg_loss:0.017, val_acc:0.985]
Epoch [115/120    avg_loss:0.016, val_acc:0.985]
Epoch [116/120    avg_loss:0.015, val_acc:0.985]
Epoch [117/120    avg_loss:0.019, val_acc:0.985]
Epoch [118/120    avg_loss:0.017, val_acc:0.985]
Epoch [119/120    avg_loss:0.028, val_acc:0.985]
Epoch [120/120    avg_loss:0.021, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0 684   0   0   0   0   1   0   0   0   0   0   0   0]
 [  0   0 219   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0 230   0   0   0   0   0   0   0   0   0   0]
 [  0   0   0   0 220   6   0   0   0   0   0   0   1   0]
 [  0   0   0   1   5 139   0   0   0   0   0   0   0   0]
 [  0   0   0   0   0   0 206   0   0   0   0   0   0   0]
 [  0   0   2   0   0   0   0  92   0   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0 388   0   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0 468   0   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0 364   0   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0 377   0   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0 453   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0 834]]

Accuracy:
99.6588486140725

F1 scores:
[       nan 0.99926954 0.99545455 0.9978308  0.97345133 0.95862069
 0.99757869 0.98924731 1.         1.         1.         1.
 0.99889746 1.        ]

Kappa:
0.9962018967233767
