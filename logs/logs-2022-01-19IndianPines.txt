creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faa581f4860>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.295, val_acc:0.504]
Epoch [2/120    avg_loss:1.782, val_acc:0.554]
Epoch [3/120    avg_loss:1.526, val_acc:0.612]
Epoch [4/120    avg_loss:1.298, val_acc:0.667]
Epoch [5/120    avg_loss:1.107, val_acc:0.669]
Epoch [6/120    avg_loss:0.930, val_acc:0.726]
Epoch [7/120    avg_loss:0.830, val_acc:0.756]
Epoch [8/120    avg_loss:0.696, val_acc:0.767]
Epoch [9/120    avg_loss:0.602, val_acc:0.790]
Epoch [10/120    avg_loss:0.588, val_acc:0.812]
Epoch [11/120    avg_loss:0.550, val_acc:0.822]
Epoch [12/120    avg_loss:0.529, val_acc:0.825]
Epoch [13/120    avg_loss:0.522, val_acc:0.860]
Epoch [14/120    avg_loss:0.410, val_acc:0.828]
Epoch [15/120    avg_loss:0.403, val_acc:0.779]
Epoch [16/120    avg_loss:0.429, val_acc:0.859]
Epoch [17/120    avg_loss:0.339, val_acc:0.869]
Epoch [18/120    avg_loss:0.259, val_acc:0.860]
Epoch [19/120    avg_loss:0.334, val_acc:0.904]
Epoch [20/120    avg_loss:0.235, val_acc:0.893]
Epoch [21/120    avg_loss:0.221, val_acc:0.921]
Epoch [22/120    avg_loss:0.179, val_acc:0.900]
Epoch [23/120    avg_loss:0.216, val_acc:0.879]
Epoch [24/120    avg_loss:0.278, val_acc:0.892]
Epoch [25/120    avg_loss:0.256, val_acc:0.865]
Epoch [26/120    avg_loss:0.299, val_acc:0.867]
Epoch [27/120    avg_loss:0.252, val_acc:0.911]
Epoch [28/120    avg_loss:0.193, val_acc:0.906]
Epoch [29/120    avg_loss:0.176, val_acc:0.927]
Epoch [30/120    avg_loss:0.153, val_acc:0.908]
Epoch [31/120    avg_loss:0.235, val_acc:0.898]
Epoch [32/120    avg_loss:0.211, val_acc:0.914]
Epoch [33/120    avg_loss:0.121, val_acc:0.909]
Epoch [34/120    avg_loss:0.127, val_acc:0.929]
Epoch [35/120    avg_loss:0.121, val_acc:0.922]
Epoch [36/120    avg_loss:0.109, val_acc:0.942]
Epoch [37/120    avg_loss:0.102, val_acc:0.939]
Epoch [38/120    avg_loss:0.088, val_acc:0.933]
Epoch [39/120    avg_loss:0.110, val_acc:0.940]
Epoch [40/120    avg_loss:0.130, val_acc:0.933]
Epoch [41/120    avg_loss:0.112, val_acc:0.940]
Epoch [42/120    avg_loss:0.117, val_acc:0.949]
Epoch [43/120    avg_loss:0.074, val_acc:0.949]
Epoch [44/120    avg_loss:0.078, val_acc:0.933]
Epoch [45/120    avg_loss:0.102, val_acc:0.934]
Epoch [46/120    avg_loss:0.116, val_acc:0.952]
Epoch [47/120    avg_loss:0.086, val_acc:0.946]
Epoch [48/120    avg_loss:0.051, val_acc:0.949]
Epoch [49/120    avg_loss:0.049, val_acc:0.955]
Epoch [50/120    avg_loss:0.053, val_acc:0.955]
Epoch [51/120    avg_loss:0.078, val_acc:0.939]
Epoch [52/120    avg_loss:0.114, val_acc:0.928]
Epoch [53/120    avg_loss:0.086, val_acc:0.952]
Epoch [54/120    avg_loss:0.082, val_acc:0.929]
Epoch [55/120    avg_loss:0.070, val_acc:0.960]
Epoch [56/120    avg_loss:0.094, val_acc:0.956]
Epoch [57/120    avg_loss:0.094, val_acc:0.938]
Epoch [58/120    avg_loss:0.050, val_acc:0.950]
Epoch [59/120    avg_loss:0.047, val_acc:0.962]
Epoch [60/120    avg_loss:0.035, val_acc:0.968]
Epoch [61/120    avg_loss:0.045, val_acc:0.962]
Epoch [62/120    avg_loss:0.051, val_acc:0.954]
Epoch [63/120    avg_loss:0.118, val_acc:0.932]
Epoch [64/120    avg_loss:0.077, val_acc:0.958]
Epoch [65/120    avg_loss:0.060, val_acc:0.964]
Epoch [66/120    avg_loss:0.050, val_acc:0.968]
Epoch [67/120    avg_loss:0.070, val_acc:0.956]
Epoch [68/120    avg_loss:0.040, val_acc:0.967]
Epoch [69/120    avg_loss:0.053, val_acc:0.968]
Epoch [70/120    avg_loss:0.046, val_acc:0.963]
Epoch [71/120    avg_loss:0.028, val_acc:0.973]
Epoch [72/120    avg_loss:0.038, val_acc:0.972]
Epoch [73/120    avg_loss:0.030, val_acc:0.969]
Epoch [74/120    avg_loss:0.030, val_acc:0.966]
Epoch [75/120    avg_loss:0.042, val_acc:0.972]
Epoch [76/120    avg_loss:0.040, val_acc:0.973]
Epoch [77/120    avg_loss:0.041, val_acc:0.978]
Epoch [78/120    avg_loss:0.027, val_acc:0.971]
Epoch [79/120    avg_loss:0.021, val_acc:0.970]
Epoch [80/120    avg_loss:0.020, val_acc:0.981]
Epoch [81/120    avg_loss:0.029, val_acc:0.969]
Epoch [82/120    avg_loss:0.023, val_acc:0.972]
Epoch [83/120    avg_loss:0.023, val_acc:0.976]
Epoch [84/120    avg_loss:0.137, val_acc:0.916]
Epoch [85/120    avg_loss:0.103, val_acc:0.943]
Epoch [86/120    avg_loss:0.058, val_acc:0.959]
Epoch [87/120    avg_loss:0.077, val_acc:0.968]
Epoch [88/120    avg_loss:0.085, val_acc:0.966]
Epoch [89/120    avg_loss:0.033, val_acc:0.966]
Epoch [90/120    avg_loss:0.029, val_acc:0.967]
Epoch [91/120    avg_loss:0.031, val_acc:0.972]
Epoch [92/120    avg_loss:0.029, val_acc:0.976]
Epoch [93/120    avg_loss:0.026, val_acc:0.974]
Epoch [94/120    avg_loss:0.019, val_acc:0.975]
Epoch [95/120    avg_loss:0.018, val_acc:0.975]
Epoch [96/120    avg_loss:0.015, val_acc:0.975]
Epoch [97/120    avg_loss:0.016, val_acc:0.975]
Epoch [98/120    avg_loss:0.016, val_acc:0.974]
Epoch [99/120    avg_loss:0.016, val_acc:0.975]
Epoch [100/120    avg_loss:0.014, val_acc:0.975]
Epoch [101/120    avg_loss:0.014, val_acc:0.975]
Epoch [102/120    avg_loss:0.013, val_acc:0.974]
Epoch [103/120    avg_loss:0.016, val_acc:0.976]
Epoch [104/120    avg_loss:0.010, val_acc:0.976]
Epoch [105/120    avg_loss:0.015, val_acc:0.976]
Epoch [106/120    avg_loss:0.011, val_acc:0.976]
Epoch [107/120    avg_loss:0.010, val_acc:0.976]
Epoch [108/120    avg_loss:0.016, val_acc:0.976]
Epoch [109/120    avg_loss:0.009, val_acc:0.976]
Epoch [110/120    avg_loss:0.012, val_acc:0.976]
Epoch [111/120    avg_loss:0.014, val_acc:0.976]
Epoch [112/120    avg_loss:0.013, val_acc:0.976]
Epoch [113/120    avg_loss:0.014, val_acc:0.976]
Epoch [114/120    avg_loss:0.011, val_acc:0.976]
Epoch [115/120    avg_loss:0.010, val_acc:0.976]
Epoch [116/120    avg_loss:0.021, val_acc:0.976]
Epoch [117/120    avg_loss:0.015, val_acc:0.976]
Epoch [118/120    avg_loss:0.012, val_acc:0.976]
Epoch [119/120    avg_loss:0.013, val_acc:0.976]
Epoch [120/120    avg_loss:0.011, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1264    0    0    0    3    0    0    0    9    9    0    0
     0    0    0]
 [   0    0    0  707    1    7    0    0    0    8    3    0   17    0
     0    4    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  426    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    4    0    0    0    0  845   18    0    0
     1    2    0]
 [   0    0   10    0    0    0    1    0    0    0   17 2174    6    2
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    1    0    1  525    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    3    0    1    0    0    0
  1128    4    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    37  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.88617886178862

F1 scores:
[       nan 0.9382716  0.98595944 0.97248968 0.99765808 0.97968397
 0.99469295 1.         0.99185099 0.8        0.96406161 0.98527079
 0.96863469 0.99462366 0.97831743 0.92631579 0.97674419]

Kappa:
0.9759034919632695
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9e821c87f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.307, val_acc:0.427]
Epoch [2/120    avg_loss:1.775, val_acc:0.590]
Epoch [3/120    avg_loss:1.451, val_acc:0.678]
Epoch [4/120    avg_loss:1.161, val_acc:0.668]
Epoch [5/120    avg_loss:1.009, val_acc:0.717]
Epoch [6/120    avg_loss:0.834, val_acc:0.757]
Epoch [7/120    avg_loss:0.696, val_acc:0.799]
Epoch [8/120    avg_loss:0.599, val_acc:0.798]
Epoch [9/120    avg_loss:0.642, val_acc:0.820]
Epoch [10/120    avg_loss:0.568, val_acc:0.842]
Epoch [11/120    avg_loss:0.510, val_acc:0.838]
Epoch [12/120    avg_loss:0.470, val_acc:0.852]
Epoch [13/120    avg_loss:0.405, val_acc:0.877]
Epoch [14/120    avg_loss:0.332, val_acc:0.883]
Epoch [15/120    avg_loss:0.336, val_acc:0.856]
Epoch [16/120    avg_loss:0.401, val_acc:0.885]
Epoch [17/120    avg_loss:0.323, val_acc:0.835]
Epoch [18/120    avg_loss:0.300, val_acc:0.925]
Epoch [19/120    avg_loss:0.246, val_acc:0.846]
Epoch [20/120    avg_loss:0.295, val_acc:0.873]
Epoch [21/120    avg_loss:0.269, val_acc:0.910]
Epoch [22/120    avg_loss:0.256, val_acc:0.927]
Epoch [23/120    avg_loss:0.150, val_acc:0.933]
Epoch [24/120    avg_loss:0.200, val_acc:0.944]
Epoch [25/120    avg_loss:0.197, val_acc:0.938]
Epoch [26/120    avg_loss:0.183, val_acc:0.927]
Epoch [27/120    avg_loss:0.136, val_acc:0.943]
Epoch [28/120    avg_loss:0.192, val_acc:0.943]
Epoch [29/120    avg_loss:0.135, val_acc:0.925]
Epoch [30/120    avg_loss:0.123, val_acc:0.961]
Epoch [31/120    avg_loss:0.082, val_acc:0.952]
Epoch [32/120    avg_loss:0.094, val_acc:0.969]
Epoch [33/120    avg_loss:0.109, val_acc:0.949]
Epoch [34/120    avg_loss:0.097, val_acc:0.964]
Epoch [35/120    avg_loss:0.090, val_acc:0.961]
Epoch [36/120    avg_loss:0.091, val_acc:0.963]
Epoch [37/120    avg_loss:0.075, val_acc:0.952]
Epoch [38/120    avg_loss:0.078, val_acc:0.962]
Epoch [39/120    avg_loss:0.064, val_acc:0.962]
Epoch [40/120    avg_loss:0.054, val_acc:0.970]
Epoch [41/120    avg_loss:0.047, val_acc:0.965]
Epoch [42/120    avg_loss:0.093, val_acc:0.965]
Epoch [43/120    avg_loss:0.100, val_acc:0.949]
Epoch [44/120    avg_loss:0.071, val_acc:0.969]
Epoch [45/120    avg_loss:0.040, val_acc:0.977]
Epoch [46/120    avg_loss:0.044, val_acc:0.976]
Epoch [47/120    avg_loss:0.057, val_acc:0.972]
Epoch [48/120    avg_loss:0.035, val_acc:0.967]
Epoch [49/120    avg_loss:0.053, val_acc:0.960]
Epoch [50/120    avg_loss:0.081, val_acc:0.938]
Epoch [51/120    avg_loss:0.070, val_acc:0.966]
Epoch [52/120    avg_loss:0.096, val_acc:0.931]
Epoch [53/120    avg_loss:0.096, val_acc:0.961]
Epoch [54/120    avg_loss:0.072, val_acc:0.969]
Epoch [55/120    avg_loss:0.049, val_acc:0.972]
Epoch [56/120    avg_loss:0.055, val_acc:0.969]
Epoch [57/120    avg_loss:0.042, val_acc:0.981]
Epoch [58/120    avg_loss:0.026, val_acc:0.976]
Epoch [59/120    avg_loss:0.031, val_acc:0.982]
Epoch [60/120    avg_loss:0.045, val_acc:0.961]
Epoch [61/120    avg_loss:0.078, val_acc:0.960]
Epoch [62/120    avg_loss:0.050, val_acc:0.971]
Epoch [63/120    avg_loss:0.032, val_acc:0.976]
Epoch [64/120    avg_loss:0.024, val_acc:0.982]
Epoch [65/120    avg_loss:0.020, val_acc:0.984]
Epoch [66/120    avg_loss:0.016, val_acc:0.982]
Epoch [67/120    avg_loss:0.016, val_acc:0.978]
Epoch [68/120    avg_loss:0.017, val_acc:0.984]
Epoch [69/120    avg_loss:0.015, val_acc:0.977]
Epoch [70/120    avg_loss:0.022, val_acc:0.985]
Epoch [71/120    avg_loss:0.014, val_acc:0.984]
Epoch [72/120    avg_loss:0.013, val_acc:0.984]
Epoch [73/120    avg_loss:0.013, val_acc:0.984]
Epoch [74/120    avg_loss:0.010, val_acc:0.984]
Epoch [75/120    avg_loss:0.009, val_acc:0.975]
Epoch [76/120    avg_loss:0.010, val_acc:0.982]
Epoch [77/120    avg_loss:0.013, val_acc:0.979]
Epoch [78/120    avg_loss:0.011, val_acc:0.986]
Epoch [79/120    avg_loss:0.006, val_acc:0.988]
Epoch [80/120    avg_loss:0.014, val_acc:0.980]
Epoch [81/120    avg_loss:0.009, val_acc:0.986]
Epoch [82/120    avg_loss:0.010, val_acc:0.964]
Epoch [83/120    avg_loss:0.008, val_acc:0.986]
Epoch [84/120    avg_loss:0.009, val_acc:0.976]
Epoch [85/120    avg_loss:0.017, val_acc:0.985]
Epoch [86/120    avg_loss:0.006, val_acc:0.988]
Epoch [87/120    avg_loss:0.007, val_acc:0.987]
Epoch [88/120    avg_loss:0.008, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.985]
Epoch [90/120    avg_loss:0.005, val_acc:0.990]
Epoch [91/120    avg_loss:0.012, val_acc:0.979]
Epoch [92/120    avg_loss:0.023, val_acc:0.975]
Epoch [93/120    avg_loss:0.015, val_acc:0.979]
Epoch [94/120    avg_loss:0.017, val_acc:0.986]
Epoch [95/120    avg_loss:0.006, val_acc:0.982]
Epoch [96/120    avg_loss:0.007, val_acc:0.989]
Epoch [97/120    avg_loss:0.006, val_acc:0.988]
Epoch [98/120    avg_loss:0.016, val_acc:0.986]
Epoch [99/120    avg_loss:0.006, val_acc:0.982]
Epoch [100/120    avg_loss:0.012, val_acc:0.985]
Epoch [101/120    avg_loss:0.006, val_acc:0.980]
Epoch [102/120    avg_loss:0.014, val_acc:0.983]
Epoch [103/120    avg_loss:0.009, val_acc:0.986]
Epoch [104/120    avg_loss:0.009, val_acc:0.985]
Epoch [105/120    avg_loss:0.008, val_acc:0.987]
Epoch [106/120    avg_loss:0.005, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.985]
Epoch [111/120    avg_loss:0.004, val_acc:0.985]
Epoch [112/120    avg_loss:0.004, val_acc:0.985]
Epoch [113/120    avg_loss:0.004, val_acc:0.987]
Epoch [114/120    avg_loss:0.005, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.987]
Epoch [117/120    avg_loss:0.004, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.987]
Epoch [120/120    avg_loss:0.003, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1267    7    0    0    0    0    0    0    3    8    0    0
     0    0    0]
 [   0    0    0  716    0   11    0    0    0    2    1    0   12    4
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  853    9    1    0
     1    5    0]
 [   0    0    2    0    0    0    1    0    0    0    9 2195    1    2
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    2  531    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    46  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
98.34146341463415

F1 scores:
[       nan 0.98765432 0.98984375 0.97348742 1.         0.98405467
 0.99620349 0.98039216 0.99767442 0.88888889 0.97877223 0.9920904
 0.97790055 0.98404255 0.97806452 0.91551459 0.96341463]

Kappa:
0.9810837449752828
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:00:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3c01af08d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.358, val_acc:0.512]
Epoch [2/120    avg_loss:1.823, val_acc:0.543]
Epoch [3/120    avg_loss:1.447, val_acc:0.618]
Epoch [4/120    avg_loss:1.257, val_acc:0.630]
Epoch [5/120    avg_loss:1.063, val_acc:0.688]
Epoch [6/120    avg_loss:0.957, val_acc:0.733]
Epoch [7/120    avg_loss:0.828, val_acc:0.704]
Epoch [8/120    avg_loss:0.806, val_acc:0.723]
Epoch [9/120    avg_loss:0.664, val_acc:0.709]
Epoch [10/120    avg_loss:0.636, val_acc:0.792]
Epoch [11/120    avg_loss:0.607, val_acc:0.811]
Epoch [12/120    avg_loss:0.481, val_acc:0.764]
Epoch [13/120    avg_loss:0.490, val_acc:0.830]
Epoch [14/120    avg_loss:0.476, val_acc:0.873]
Epoch [15/120    avg_loss:0.448, val_acc:0.849]
Epoch [16/120    avg_loss:0.484, val_acc:0.866]
Epoch [17/120    avg_loss:0.352, val_acc:0.883]
Epoch [18/120    avg_loss:0.325, val_acc:0.889]
Epoch [19/120    avg_loss:0.315, val_acc:0.899]
Epoch [20/120    avg_loss:0.292, val_acc:0.864]
Epoch [21/120    avg_loss:0.296, val_acc:0.878]
Epoch [22/120    avg_loss:0.237, val_acc:0.914]
Epoch [23/120    avg_loss:0.204, val_acc:0.891]
Epoch [24/120    avg_loss:0.268, val_acc:0.896]
Epoch [25/120    avg_loss:0.222, val_acc:0.911]
Epoch [26/120    avg_loss:0.178, val_acc:0.920]
Epoch [27/120    avg_loss:0.181, val_acc:0.909]
Epoch [28/120    avg_loss:0.150, val_acc:0.919]
Epoch [29/120    avg_loss:0.149, val_acc:0.939]
Epoch [30/120    avg_loss:0.160, val_acc:0.930]
Epoch [31/120    avg_loss:0.154, val_acc:0.937]
Epoch [32/120    avg_loss:0.121, val_acc:0.942]
Epoch [33/120    avg_loss:0.150, val_acc:0.908]
Epoch [34/120    avg_loss:0.276, val_acc:0.908]
Epoch [35/120    avg_loss:0.139, val_acc:0.928]
Epoch [36/120    avg_loss:0.125, val_acc:0.942]
Epoch [37/120    avg_loss:0.123, val_acc:0.934]
Epoch [38/120    avg_loss:0.130, val_acc:0.937]
Epoch [39/120    avg_loss:0.082, val_acc:0.952]
Epoch [40/120    avg_loss:0.091, val_acc:0.928]
Epoch [41/120    avg_loss:0.079, val_acc:0.959]
Epoch [42/120    avg_loss:0.140, val_acc:0.954]
Epoch [43/120    avg_loss:0.100, val_acc:0.945]
Epoch [44/120    avg_loss:0.058, val_acc:0.960]
Epoch [45/120    avg_loss:0.058, val_acc:0.970]
Epoch [46/120    avg_loss:0.062, val_acc:0.953]
Epoch [47/120    avg_loss:0.053, val_acc:0.966]
Epoch [48/120    avg_loss:0.066, val_acc:0.942]
Epoch [49/120    avg_loss:0.055, val_acc:0.958]
Epoch [50/120    avg_loss:0.049, val_acc:0.970]
Epoch [51/120    avg_loss:0.059, val_acc:0.971]
Epoch [52/120    avg_loss:0.053, val_acc:0.967]
Epoch [53/120    avg_loss:0.038, val_acc:0.971]
Epoch [54/120    avg_loss:0.044, val_acc:0.973]
Epoch [55/120    avg_loss:0.039, val_acc:0.968]
Epoch [56/120    avg_loss:0.045, val_acc:0.975]
Epoch [57/120    avg_loss:0.048, val_acc:0.971]
Epoch [58/120    avg_loss:0.037, val_acc:0.974]
Epoch [59/120    avg_loss:0.027, val_acc:0.951]
Epoch [60/120    avg_loss:0.046, val_acc:0.959]
Epoch [61/120    avg_loss:0.030, val_acc:0.974]
Epoch [62/120    avg_loss:0.030, val_acc:0.966]
Epoch [63/120    avg_loss:0.085, val_acc:0.950]
Epoch [64/120    avg_loss:0.078, val_acc:0.967]
Epoch [65/120    avg_loss:0.078, val_acc:0.909]
Epoch [66/120    avg_loss:0.148, val_acc:0.945]
Epoch [67/120    avg_loss:0.158, val_acc:0.905]
Epoch [68/120    avg_loss:0.064, val_acc:0.966]
Epoch [69/120    avg_loss:0.070, val_acc:0.960]
Epoch [70/120    avg_loss:0.040, val_acc:0.970]
Epoch [71/120    avg_loss:0.034, val_acc:0.976]
Epoch [72/120    avg_loss:0.026, val_acc:0.974]
Epoch [73/120    avg_loss:0.036, val_acc:0.975]
Epoch [74/120    avg_loss:0.032, val_acc:0.972]
Epoch [75/120    avg_loss:0.025, val_acc:0.975]
Epoch [76/120    avg_loss:0.021, val_acc:0.975]
Epoch [77/120    avg_loss:0.021, val_acc:0.975]
Epoch [78/120    avg_loss:0.029, val_acc:0.978]
Epoch [79/120    avg_loss:0.024, val_acc:0.978]
Epoch [80/120    avg_loss:0.026, val_acc:0.977]
Epoch [81/120    avg_loss:0.019, val_acc:0.976]
Epoch [82/120    avg_loss:0.027, val_acc:0.977]
Epoch [83/120    avg_loss:0.020, val_acc:0.975]
Epoch [84/120    avg_loss:0.021, val_acc:0.975]
Epoch [85/120    avg_loss:0.026, val_acc:0.976]
Epoch [86/120    avg_loss:0.018, val_acc:0.976]
Epoch [87/120    avg_loss:0.015, val_acc:0.976]
Epoch [88/120    avg_loss:0.014, val_acc:0.977]
Epoch [89/120    avg_loss:0.017, val_acc:0.975]
Epoch [90/120    avg_loss:0.014, val_acc:0.976]
Epoch [91/120    avg_loss:0.016, val_acc:0.976]
Epoch [92/120    avg_loss:0.018, val_acc:0.975]
Epoch [93/120    avg_loss:0.016, val_acc:0.975]
Epoch [94/120    avg_loss:0.016, val_acc:0.976]
Epoch [95/120    avg_loss:0.017, val_acc:0.976]
Epoch [96/120    avg_loss:0.018, val_acc:0.976]
Epoch [97/120    avg_loss:0.018, val_acc:0.976]
Epoch [98/120    avg_loss:0.014, val_acc:0.976]
Epoch [99/120    avg_loss:0.016, val_acc:0.976]
Epoch [100/120    avg_loss:0.017, val_acc:0.976]
Epoch [101/120    avg_loss:0.013, val_acc:0.976]
Epoch [102/120    avg_loss:0.013, val_acc:0.976]
Epoch [103/120    avg_loss:0.016, val_acc:0.976]
Epoch [104/120    avg_loss:0.021, val_acc:0.976]
Epoch [105/120    avg_loss:0.018, val_acc:0.976]
Epoch [106/120    avg_loss:0.014, val_acc:0.976]
Epoch [107/120    avg_loss:0.012, val_acc:0.976]
Epoch [108/120    avg_loss:0.014, val_acc:0.976]
Epoch [109/120    avg_loss:0.013, val_acc:0.976]
Epoch [110/120    avg_loss:0.013, val_acc:0.976]
Epoch [111/120    avg_loss:0.014, val_acc:0.976]
Epoch [112/120    avg_loss:0.018, val_acc:0.976]
Epoch [113/120    avg_loss:0.017, val_acc:0.976]
Epoch [114/120    avg_loss:0.013, val_acc:0.976]
Epoch [115/120    avg_loss:0.018, val_acc:0.976]
Epoch [116/120    avg_loss:0.016, val_acc:0.976]
Epoch [117/120    avg_loss:0.018, val_acc:0.976]
Epoch [118/120    avg_loss:0.015, val_acc:0.976]
Epoch [119/120    avg_loss:0.013, val_acc:0.976]
Epoch [120/120    avg_loss:0.017, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    2    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1256    6    0    0    2    0    0    0    5   16    0    0
     0    0    0]
 [   0    0    0  721    0    4    0    0    0    4    0    0   13    5
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    5    0    0    0    0  832   30    1    0
     0    2    0]
 [   0    0   10    0    0    0    5    0    0    0   13 2175    1    1
     5    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    6  526    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    0    3    0    0
  1129    3    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    13  334    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.07046070460704

F1 scores:
[       nan 0.94871795 0.9820172  0.97630332 0.99764706 0.98177677
 0.99317665 0.98039216 0.99883856 0.7804878  0.96352056 0.97972973
 0.97769517 0.98404255 0.98688811 0.97376093 0.98224852]

Kappa:
0.9779964979585003
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f417d7827f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.329, val_acc:0.444]
Epoch [2/120    avg_loss:1.807, val_acc:0.581]
Epoch [3/120    avg_loss:1.440, val_acc:0.591]
Epoch [4/120    avg_loss:1.264, val_acc:0.605]
Epoch [5/120    avg_loss:1.025, val_acc:0.689]
Epoch [6/120    avg_loss:0.911, val_acc:0.725]
Epoch [7/120    avg_loss:0.780, val_acc:0.745]
Epoch [8/120    avg_loss:0.747, val_acc:0.688]
Epoch [9/120    avg_loss:0.670, val_acc:0.778]
Epoch [10/120    avg_loss:0.666, val_acc:0.806]
Epoch [11/120    avg_loss:0.586, val_acc:0.834]
Epoch [12/120    avg_loss:0.576, val_acc:0.825]
Epoch [13/120    avg_loss:0.468, val_acc:0.837]
Epoch [14/120    avg_loss:0.440, val_acc:0.838]
Epoch [15/120    avg_loss:0.409, val_acc:0.836]
Epoch [16/120    avg_loss:0.354, val_acc:0.871]
Epoch [17/120    avg_loss:0.293, val_acc:0.891]
Epoch [18/120    avg_loss:0.304, val_acc:0.882]
Epoch [19/120    avg_loss:0.284, val_acc:0.882]
Epoch [20/120    avg_loss:0.338, val_acc:0.844]
Epoch [21/120    avg_loss:0.331, val_acc:0.872]
Epoch [22/120    avg_loss:0.276, val_acc:0.841]
Epoch [23/120    avg_loss:0.249, val_acc:0.881]
Epoch [24/120    avg_loss:0.266, val_acc:0.889]
Epoch [25/120    avg_loss:0.327, val_acc:0.858]
Epoch [26/120    avg_loss:0.194, val_acc:0.921]
Epoch [27/120    avg_loss:0.248, val_acc:0.895]
Epoch [28/120    avg_loss:0.191, val_acc:0.918]
Epoch [29/120    avg_loss:0.158, val_acc:0.890]
Epoch [30/120    avg_loss:0.158, val_acc:0.914]
Epoch [31/120    avg_loss:0.116, val_acc:0.923]
Epoch [32/120    avg_loss:0.101, val_acc:0.931]
Epoch [33/120    avg_loss:0.115, val_acc:0.935]
Epoch [34/120    avg_loss:0.132, val_acc:0.891]
Epoch [35/120    avg_loss:0.093, val_acc:0.934]
Epoch [36/120    avg_loss:0.097, val_acc:0.947]
Epoch [37/120    avg_loss:0.095, val_acc:0.946]
Epoch [38/120    avg_loss:0.069, val_acc:0.944]
Epoch [39/120    avg_loss:0.160, val_acc:0.945]
Epoch [40/120    avg_loss:0.067, val_acc:0.953]
Epoch [41/120    avg_loss:0.063, val_acc:0.954]
Epoch [42/120    avg_loss:0.074, val_acc:0.951]
Epoch [43/120    avg_loss:0.084, val_acc:0.927]
Epoch [44/120    avg_loss:0.073, val_acc:0.951]
Epoch [45/120    avg_loss:0.062, val_acc:0.948]
Epoch [46/120    avg_loss:0.066, val_acc:0.934]
Epoch [47/120    avg_loss:0.046, val_acc:0.899]
Epoch [48/120    avg_loss:0.050, val_acc:0.961]
Epoch [49/120    avg_loss:0.043, val_acc:0.966]
Epoch [50/120    avg_loss:0.047, val_acc:0.969]
Epoch [51/120    avg_loss:0.030, val_acc:0.969]
Epoch [52/120    avg_loss:0.033, val_acc:0.948]
Epoch [53/120    avg_loss:0.032, val_acc:0.966]
Epoch [54/120    avg_loss:0.033, val_acc:0.974]
Epoch [55/120    avg_loss:0.034, val_acc:0.970]
Epoch [56/120    avg_loss:0.032, val_acc:0.963]
Epoch [57/120    avg_loss:0.041, val_acc:0.960]
Epoch [58/120    avg_loss:0.042, val_acc:0.966]
Epoch [59/120    avg_loss:0.036, val_acc:0.970]
Epoch [60/120    avg_loss:0.052, val_acc:0.956]
Epoch [61/120    avg_loss:0.250, val_acc:0.948]
Epoch [62/120    avg_loss:0.130, val_acc:0.942]
Epoch [63/120    avg_loss:0.047, val_acc:0.956]
Epoch [64/120    avg_loss:0.054, val_acc:0.967]
Epoch [65/120    avg_loss:0.073, val_acc:0.968]
Epoch [66/120    avg_loss:0.043, val_acc:0.968]
Epoch [67/120    avg_loss:0.047, val_acc:0.968]
Epoch [68/120    avg_loss:0.027, val_acc:0.978]
Epoch [69/120    avg_loss:0.021, val_acc:0.979]
Epoch [70/120    avg_loss:0.023, val_acc:0.977]
Epoch [71/120    avg_loss:0.033, val_acc:0.977]
Epoch [72/120    avg_loss:0.017, val_acc:0.977]
Epoch [73/120    avg_loss:0.023, val_acc:0.978]
Epoch [74/120    avg_loss:0.016, val_acc:0.978]
Epoch [75/120    avg_loss:0.018, val_acc:0.981]
Epoch [76/120    avg_loss:0.018, val_acc:0.979]
Epoch [77/120    avg_loss:0.017, val_acc:0.980]
Epoch [78/120    avg_loss:0.016, val_acc:0.980]
Epoch [79/120    avg_loss:0.018, val_acc:0.982]
Epoch [80/120    avg_loss:0.018, val_acc:0.983]
Epoch [81/120    avg_loss:0.016, val_acc:0.982]
Epoch [82/120    avg_loss:0.017, val_acc:0.977]
Epoch [83/120    avg_loss:0.017, val_acc:0.982]
Epoch [84/120    avg_loss:0.019, val_acc:0.983]
Epoch [85/120    avg_loss:0.014, val_acc:0.983]
Epoch [86/120    avg_loss:0.014, val_acc:0.982]
Epoch [87/120    avg_loss:0.013, val_acc:0.987]
Epoch [88/120    avg_loss:0.016, val_acc:0.985]
Epoch [89/120    avg_loss:0.016, val_acc:0.984]
Epoch [90/120    avg_loss:0.014, val_acc:0.985]
Epoch [91/120    avg_loss:0.012, val_acc:0.985]
Epoch [92/120    avg_loss:0.015, val_acc:0.984]
Epoch [93/120    avg_loss:0.015, val_acc:0.983]
Epoch [94/120    avg_loss:0.014, val_acc:0.985]
Epoch [95/120    avg_loss:0.013, val_acc:0.986]
Epoch [96/120    avg_loss:0.013, val_acc:0.984]
Epoch [97/120    avg_loss:0.012, val_acc:0.983]
Epoch [98/120    avg_loss:0.013, val_acc:0.983]
Epoch [99/120    avg_loss:0.016, val_acc:0.986]
Epoch [100/120    avg_loss:0.017, val_acc:0.986]
Epoch [101/120    avg_loss:0.012, val_acc:0.986]
Epoch [102/120    avg_loss:0.013, val_acc:0.986]
Epoch [103/120    avg_loss:0.011, val_acc:0.986]
Epoch [104/120    avg_loss:0.010, val_acc:0.986]
Epoch [105/120    avg_loss:0.014, val_acc:0.986]
Epoch [106/120    avg_loss:0.014, val_acc:0.986]
Epoch [107/120    avg_loss:0.015, val_acc:0.986]
Epoch [108/120    avg_loss:0.013, val_acc:0.986]
Epoch [109/120    avg_loss:0.015, val_acc:0.985]
Epoch [110/120    avg_loss:0.013, val_acc:0.985]
Epoch [111/120    avg_loss:0.013, val_acc:0.985]
Epoch [112/120    avg_loss:0.012, val_acc:0.986]
Epoch [113/120    avg_loss:0.011, val_acc:0.986]
Epoch [114/120    avg_loss:0.018, val_acc:0.986]
Epoch [115/120    avg_loss:0.011, val_acc:0.986]
Epoch [116/120    avg_loss:0.013, val_acc:0.986]
Epoch [117/120    avg_loss:0.017, val_acc:0.986]
Epoch [118/120    avg_loss:0.016, val_acc:0.985]
Epoch [119/120    avg_loss:0.013, val_acc:0.985]
Epoch [120/120    avg_loss:0.013, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1269    0    0    0    1    0    0    1    7    4    1    0
     0    2    0]
 [   0    0    0  709    0   12    0    0    0    4    3    0   10    2
     0    7    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    3    1    0    0    0  846   10    1    0
     3    9    0]
 [   0    0   11    0    0    1    3    0    1    0    3 2189    0    2
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    1    5  521    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    1    0    1    0    0    0
  1134    1    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    31  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.00542005420054

F1 scores:
[       nan 0.93506494 0.98870277 0.9739011  1.         0.97412823
 0.98121713 1.         0.99767981 0.85714286 0.97185526 0.99072188
 0.9729225  0.98930481 0.98139334 0.89955022 0.95808383]

Kappa:
0.9772574344400395
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff983c3e7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.343, val_acc:0.501]
Epoch [2/120    avg_loss:1.799, val_acc:0.595]
Epoch [3/120    avg_loss:1.491, val_acc:0.615]
Epoch [4/120    avg_loss:1.296, val_acc:0.666]
Epoch [5/120    avg_loss:1.116, val_acc:0.682]
Epoch [6/120    avg_loss:0.910, val_acc:0.618]
Epoch [7/120    avg_loss:0.884, val_acc:0.774]
Epoch [8/120    avg_loss:0.744, val_acc:0.776]
Epoch [9/120    avg_loss:0.657, val_acc:0.775]
Epoch [10/120    avg_loss:0.632, val_acc:0.817]
Epoch [11/120    avg_loss:0.602, val_acc:0.831]
Epoch [12/120    avg_loss:0.522, val_acc:0.809]
Epoch [13/120    avg_loss:0.554, val_acc:0.798]
Epoch [14/120    avg_loss:0.481, val_acc:0.843]
Epoch [15/120    avg_loss:0.395, val_acc:0.857]
Epoch [16/120    avg_loss:0.369, val_acc:0.881]
Epoch [17/120    avg_loss:0.337, val_acc:0.828]
Epoch [18/120    avg_loss:0.315, val_acc:0.887]
Epoch [19/120    avg_loss:0.343, val_acc:0.844]
Epoch [20/120    avg_loss:0.288, val_acc:0.880]
Epoch [21/120    avg_loss:0.373, val_acc:0.860]
Epoch [22/120    avg_loss:0.294, val_acc:0.894]
Epoch [23/120    avg_loss:0.213, val_acc:0.898]
Epoch [24/120    avg_loss:0.310, val_acc:0.867]
Epoch [25/120    avg_loss:0.238, val_acc:0.901]
Epoch [26/120    avg_loss:0.299, val_acc:0.864]
Epoch [27/120    avg_loss:0.233, val_acc:0.912]
Epoch [28/120    avg_loss:0.178, val_acc:0.896]
Epoch [29/120    avg_loss:0.176, val_acc:0.933]
Epoch [30/120    avg_loss:0.135, val_acc:0.939]
Epoch [31/120    avg_loss:0.106, val_acc:0.930]
Epoch [32/120    avg_loss:0.107, val_acc:0.914]
Epoch [33/120    avg_loss:0.126, val_acc:0.935]
Epoch [34/120    avg_loss:0.136, val_acc:0.942]
Epoch [35/120    avg_loss:0.094, val_acc:0.949]
Epoch [36/120    avg_loss:0.127, val_acc:0.913]
Epoch [37/120    avg_loss:0.148, val_acc:0.916]
Epoch [38/120    avg_loss:0.106, val_acc:0.944]
Epoch [39/120    avg_loss:0.076, val_acc:0.910]
Epoch [40/120    avg_loss:0.077, val_acc:0.952]
Epoch [41/120    avg_loss:0.076, val_acc:0.941]
Epoch [42/120    avg_loss:0.199, val_acc:0.922]
Epoch [43/120    avg_loss:0.104, val_acc:0.936]
Epoch [44/120    avg_loss:0.102, val_acc:0.957]
Epoch [45/120    avg_loss:0.323, val_acc:0.912]
Epoch [46/120    avg_loss:0.097, val_acc:0.956]
Epoch [47/120    avg_loss:0.070, val_acc:0.939]
Epoch [48/120    avg_loss:0.054, val_acc:0.958]
Epoch [49/120    avg_loss:0.069, val_acc:0.927]
Epoch [50/120    avg_loss:0.123, val_acc:0.932]
Epoch [51/120    avg_loss:0.101, val_acc:0.947]
Epoch [52/120    avg_loss:0.065, val_acc:0.959]
Epoch [53/120    avg_loss:0.050, val_acc:0.960]
Epoch [54/120    avg_loss:0.044, val_acc:0.959]
Epoch [55/120    avg_loss:0.054, val_acc:0.961]
Epoch [56/120    avg_loss:0.046, val_acc:0.954]
Epoch [57/120    avg_loss:0.043, val_acc:0.957]
Epoch [58/120    avg_loss:0.112, val_acc:0.952]
Epoch [59/120    avg_loss:0.080, val_acc:0.932]
Epoch [60/120    avg_loss:0.058, val_acc:0.967]
Epoch [61/120    avg_loss:0.075, val_acc:0.921]
Epoch [62/120    avg_loss:0.089, val_acc:0.947]
Epoch [63/120    avg_loss:0.040, val_acc:0.961]
Epoch [64/120    avg_loss:0.048, val_acc:0.964]
Epoch [65/120    avg_loss:0.030, val_acc:0.971]
Epoch [66/120    avg_loss:0.031, val_acc:0.971]
Epoch [67/120    avg_loss:0.035, val_acc:0.960]
Epoch [68/120    avg_loss:0.051, val_acc:0.935]
Epoch [69/120    avg_loss:0.077, val_acc:0.958]
Epoch [70/120    avg_loss:0.035, val_acc:0.966]
Epoch [71/120    avg_loss:0.021, val_acc:0.972]
Epoch [72/120    avg_loss:0.030, val_acc:0.972]
Epoch [73/120    avg_loss:0.072, val_acc:0.950]
Epoch [74/120    avg_loss:0.071, val_acc:0.949]
Epoch [75/120    avg_loss:0.036, val_acc:0.960]
Epoch [76/120    avg_loss:0.028, val_acc:0.971]
Epoch [77/120    avg_loss:0.074, val_acc:0.951]
Epoch [78/120    avg_loss:0.075, val_acc:0.931]
Epoch [79/120    avg_loss:0.064, val_acc:0.958]
Epoch [80/120    avg_loss:0.035, val_acc:0.961]
Epoch [81/120    avg_loss:0.029, val_acc:0.962]
Epoch [82/120    avg_loss:0.025, val_acc:0.965]
Epoch [83/120    avg_loss:0.032, val_acc:0.966]
Epoch [84/120    avg_loss:0.041, val_acc:0.966]
Epoch [85/120    avg_loss:0.026, val_acc:0.976]
Epoch [86/120    avg_loss:0.021, val_acc:0.978]
Epoch [87/120    avg_loss:0.016, val_acc:0.980]
Epoch [88/120    avg_loss:0.033, val_acc:0.968]
Epoch [89/120    avg_loss:0.028, val_acc:0.976]
Epoch [90/120    avg_loss:0.016, val_acc:0.975]
Epoch [91/120    avg_loss:0.016, val_acc:0.976]
Epoch [92/120    avg_loss:0.025, val_acc:0.971]
Epoch [93/120    avg_loss:0.030, val_acc:0.960]
Epoch [94/120    avg_loss:0.062, val_acc:0.960]
Epoch [95/120    avg_loss:0.047, val_acc:0.952]
Epoch [96/120    avg_loss:0.066, val_acc:0.946]
Epoch [97/120    avg_loss:0.058, val_acc:0.972]
Epoch [98/120    avg_loss:0.016, val_acc:0.977]
Epoch [99/120    avg_loss:0.015, val_acc:0.977]
Epoch [100/120    avg_loss:0.015, val_acc:0.974]
Epoch [101/120    avg_loss:0.011, val_acc:0.982]
Epoch [102/120    avg_loss:0.008, val_acc:0.981]
Epoch [103/120    avg_loss:0.008, val_acc:0.980]
Epoch [104/120    avg_loss:0.007, val_acc:0.981]
Epoch [105/120    avg_loss:0.007, val_acc:0.980]
Epoch [106/120    avg_loss:0.010, val_acc:0.980]
Epoch [107/120    avg_loss:0.007, val_acc:0.980]
Epoch [108/120    avg_loss:0.007, val_acc:0.980]
Epoch [109/120    avg_loss:0.008, val_acc:0.981]
Epoch [110/120    avg_loss:0.010, val_acc:0.982]
Epoch [111/120    avg_loss:0.006, val_acc:0.982]
Epoch [112/120    avg_loss:0.011, val_acc:0.981]
Epoch [113/120    avg_loss:0.008, val_acc:0.979]
Epoch [114/120    avg_loss:0.007, val_acc:0.980]
Epoch [115/120    avg_loss:0.011, val_acc:0.981]
Epoch [116/120    avg_loss:0.010, val_acc:0.980]
Epoch [117/120    avg_loss:0.009, val_acc:0.980]
Epoch [118/120    avg_loss:0.008, val_acc:0.981]
Epoch [119/120    avg_loss:0.008, val_acc:0.981]
Epoch [120/120    avg_loss:0.010, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1249    0    3    0    0    0    0    2    9   17    1    2
     0    2    0]
 [   0    0    0  702   16    0    0    0    0    3    1    1   24    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    0    0    1    1    0    0    0    0  860    0    5    0
     1    7    0]
 [   0    0    1    1    0    4    1    0    0    0   15 2187    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1  532    0
     0    0    1]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    2    0    0    2    0    1    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
    22  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.20054200542005

F1 scores:
[       nan 0.94871795 0.98540434 0.96827586 0.95515695 0.98742857
 0.99394856 1.         0.99651568 0.82926829 0.97450425 0.99048913
 0.96727273 0.99191375 0.98694517 0.94362018 0.98809524]

Kappa:
0.9794923471039876
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5231060b00>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.333, val_acc:0.482]
Epoch [2/120    avg_loss:1.798, val_acc:0.586]
Epoch [3/120    avg_loss:1.534, val_acc:0.593]
Epoch [4/120    avg_loss:1.338, val_acc:0.622]
Epoch [5/120    avg_loss:1.104, val_acc:0.627]
Epoch [6/120    avg_loss:0.988, val_acc:0.727]
Epoch [7/120    avg_loss:0.901, val_acc:0.735]
Epoch [8/120    avg_loss:0.709, val_acc:0.740]
Epoch [9/120    avg_loss:0.722, val_acc:0.770]
Epoch [10/120    avg_loss:0.751, val_acc:0.755]
Epoch [11/120    avg_loss:0.628, val_acc:0.775]
Epoch [12/120    avg_loss:0.562, val_acc:0.839]
Epoch [13/120    avg_loss:0.439, val_acc:0.791]
Epoch [14/120    avg_loss:0.443, val_acc:0.758]
Epoch [15/120    avg_loss:0.425, val_acc:0.881]
Epoch [16/120    avg_loss:0.388, val_acc:0.887]
Epoch [17/120    avg_loss:0.410, val_acc:0.841]
Epoch [18/120    avg_loss:0.390, val_acc:0.859]
Epoch [19/120    avg_loss:0.344, val_acc:0.892]
Epoch [20/120    avg_loss:0.302, val_acc:0.868]
Epoch [21/120    avg_loss:0.287, val_acc:0.892]
Epoch [22/120    avg_loss:0.254, val_acc:0.876]
Epoch [23/120    avg_loss:0.207, val_acc:0.901]
Epoch [24/120    avg_loss:0.232, val_acc:0.914]
Epoch [25/120    avg_loss:0.196, val_acc:0.921]
Epoch [26/120    avg_loss:0.247, val_acc:0.912]
Epoch [27/120    avg_loss:0.193, val_acc:0.907]
Epoch [28/120    avg_loss:0.229, val_acc:0.924]
Epoch [29/120    avg_loss:0.161, val_acc:0.932]
Epoch [30/120    avg_loss:0.182, val_acc:0.922]
Epoch [31/120    avg_loss:0.138, val_acc:0.879]
Epoch [32/120    avg_loss:0.150, val_acc:0.931]
Epoch [33/120    avg_loss:0.140, val_acc:0.924]
Epoch [34/120    avg_loss:0.120, val_acc:0.957]
Epoch [35/120    avg_loss:0.146, val_acc:0.897]
Epoch [36/120    avg_loss:0.138, val_acc:0.947]
Epoch [37/120    avg_loss:0.145, val_acc:0.912]
Epoch [38/120    avg_loss:0.130, val_acc:0.904]
Epoch [39/120    avg_loss:0.165, val_acc:0.925]
Epoch [40/120    avg_loss:0.129, val_acc:0.938]
Epoch [41/120    avg_loss:0.084, val_acc:0.947]
Epoch [42/120    avg_loss:0.084, val_acc:0.953]
Epoch [43/120    avg_loss:0.141, val_acc:0.948]
Epoch [44/120    avg_loss:0.114, val_acc:0.885]
Epoch [45/120    avg_loss:0.162, val_acc:0.933]
Epoch [46/120    avg_loss:0.078, val_acc:0.951]
Epoch [47/120    avg_loss:0.059, val_acc:0.956]
Epoch [48/120    avg_loss:0.059, val_acc:0.963]
Epoch [49/120    avg_loss:0.039, val_acc:0.961]
Epoch [50/120    avg_loss:0.032, val_acc:0.963]
Epoch [51/120    avg_loss:0.038, val_acc:0.963]
Epoch [52/120    avg_loss:0.037, val_acc:0.965]
Epoch [53/120    avg_loss:0.039, val_acc:0.964]
Epoch [54/120    avg_loss:0.034, val_acc:0.963]
Epoch [55/120    avg_loss:0.031, val_acc:0.965]
Epoch [56/120    avg_loss:0.034, val_acc:0.966]
Epoch [57/120    avg_loss:0.030, val_acc:0.963]
Epoch [58/120    avg_loss:0.026, val_acc:0.967]
Epoch [59/120    avg_loss:0.031, val_acc:0.967]
Epoch [60/120    avg_loss:0.036, val_acc:0.968]
Epoch [61/120    avg_loss:0.031, val_acc:0.971]
Epoch [62/120    avg_loss:0.033, val_acc:0.970]
Epoch [63/120    avg_loss:0.030, val_acc:0.970]
Epoch [64/120    avg_loss:0.032, val_acc:0.970]
Epoch [65/120    avg_loss:0.031, val_acc:0.972]
Epoch [66/120    avg_loss:0.027, val_acc:0.969]
Epoch [67/120    avg_loss:0.023, val_acc:0.970]
Epoch [68/120    avg_loss:0.028, val_acc:0.970]
Epoch [69/120    avg_loss:0.024, val_acc:0.970]
Epoch [70/120    avg_loss:0.034, val_acc:0.972]
Epoch [71/120    avg_loss:0.029, val_acc:0.974]
Epoch [72/120    avg_loss:0.028, val_acc:0.971]
Epoch [73/120    avg_loss:0.026, val_acc:0.971]
Epoch [74/120    avg_loss:0.029, val_acc:0.971]
Epoch [75/120    avg_loss:0.021, val_acc:0.972]
Epoch [76/120    avg_loss:0.026, val_acc:0.974]
Epoch [77/120    avg_loss:0.027, val_acc:0.974]
Epoch [78/120    avg_loss:0.024, val_acc:0.972]
Epoch [79/120    avg_loss:0.026, val_acc:0.975]
Epoch [80/120    avg_loss:0.022, val_acc:0.972]
Epoch [81/120    avg_loss:0.025, val_acc:0.975]
Epoch [82/120    avg_loss:0.027, val_acc:0.971]
Epoch [83/120    avg_loss:0.030, val_acc:0.972]
Epoch [84/120    avg_loss:0.022, val_acc:0.972]
Epoch [85/120    avg_loss:0.026, val_acc:0.975]
Epoch [86/120    avg_loss:0.026, val_acc:0.977]
Epoch [87/120    avg_loss:0.024, val_acc:0.976]
Epoch [88/120    avg_loss:0.021, val_acc:0.968]
Epoch [89/120    avg_loss:0.025, val_acc:0.974]
Epoch [90/120    avg_loss:0.025, val_acc:0.972]
Epoch [91/120    avg_loss:0.021, val_acc:0.975]
Epoch [92/120    avg_loss:0.024, val_acc:0.977]
Epoch [93/120    avg_loss:0.019, val_acc:0.974]
Epoch [94/120    avg_loss:0.023, val_acc:0.974]
Epoch [95/120    avg_loss:0.025, val_acc:0.972]
Epoch [96/120    avg_loss:0.021, val_acc:0.971]
Epoch [97/120    avg_loss:0.018, val_acc:0.974]
Epoch [98/120    avg_loss:0.025, val_acc:0.978]
Epoch [99/120    avg_loss:0.019, val_acc:0.978]
Epoch [100/120    avg_loss:0.022, val_acc:0.977]
Epoch [101/120    avg_loss:0.018, val_acc:0.975]
Epoch [102/120    avg_loss:0.019, val_acc:0.976]
Epoch [103/120    avg_loss:0.017, val_acc:0.974]
Epoch [104/120    avg_loss:0.020, val_acc:0.976]
Epoch [105/120    avg_loss:0.016, val_acc:0.977]
Epoch [106/120    avg_loss:0.019, val_acc:0.977]
Epoch [107/120    avg_loss:0.020, val_acc:0.980]
Epoch [108/120    avg_loss:0.021, val_acc:0.976]
Epoch [109/120    avg_loss:0.020, val_acc:0.980]
Epoch [110/120    avg_loss:0.018, val_acc:0.975]
Epoch [111/120    avg_loss:0.020, val_acc:0.977]
Epoch [112/120    avg_loss:0.018, val_acc:0.978]
Epoch [113/120    avg_loss:0.018, val_acc:0.978]
Epoch [114/120    avg_loss:0.019, val_acc:0.976]
Epoch [115/120    avg_loss:0.022, val_acc:0.974]
Epoch [116/120    avg_loss:0.029, val_acc:0.980]
Epoch [117/120    avg_loss:0.017, val_acc:0.978]
Epoch [118/120    avg_loss:0.015, val_acc:0.977]
Epoch [119/120    avg_loss:0.017, val_acc:0.976]
Epoch [120/120    avg_loss:0.020, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    4    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0   15 1242    0    0    0    9    0    0    0    6   10    1    0
     0    2    0]
 [   0    0    0  696    3   15    0    0    0    8    1   10   13    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    1    0    0    0    1    0    0
     8    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   13    0    0    6    0    0    0    0  839    9    0    0
     2    6    0]
 [   0    1   10    0    0    0    0    0    0    0    6 2191    0    0
     2    0    0]
 [   0    0    0    0    0    6    0    0    0    0    0    7  518    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    1    0    0    0
  1125   11    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    22  323    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.55013550135502

F1 scores:
[       nan 0.77419355 0.97259201 0.96465696 0.99300699 0.95828636
 0.98787879 0.98039216 0.99651568 0.81818182 0.97050318 0.9873817
 0.96913003 0.99730458 0.9765625  0.93759071 0.97619048]

Kappa:
0.9720676842942666
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f5d2ee710>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.364, val_acc:0.482]
Epoch [2/120    avg_loss:1.865, val_acc:0.560]
Epoch [3/120    avg_loss:1.633, val_acc:0.588]
Epoch [4/120    avg_loss:1.428, val_acc:0.646]
Epoch [5/120    avg_loss:1.202, val_acc:0.730]
Epoch [6/120    avg_loss:1.070, val_acc:0.689]
Epoch [7/120    avg_loss:0.912, val_acc:0.738]
Epoch [8/120    avg_loss:0.773, val_acc:0.745]
Epoch [9/120    avg_loss:0.797, val_acc:0.725]
Epoch [10/120    avg_loss:0.698, val_acc:0.751]
Epoch [11/120    avg_loss:0.554, val_acc:0.829]
Epoch [12/120    avg_loss:0.639, val_acc:0.769]
Epoch [13/120    avg_loss:0.519, val_acc:0.828]
Epoch [14/120    avg_loss:0.477, val_acc:0.879]
Epoch [15/120    avg_loss:0.514, val_acc:0.819]
Epoch [16/120    avg_loss:0.396, val_acc:0.834]
Epoch [17/120    avg_loss:0.399, val_acc:0.828]
Epoch [18/120    avg_loss:0.348, val_acc:0.871]
Epoch [19/120    avg_loss:0.309, val_acc:0.880]
Epoch [20/120    avg_loss:0.266, val_acc:0.882]
Epoch [21/120    avg_loss:0.389, val_acc:0.850]
Epoch [22/120    avg_loss:0.245, val_acc:0.868]
Epoch [23/120    avg_loss:0.249, val_acc:0.902]
Epoch [24/120    avg_loss:0.295, val_acc:0.877]
Epoch [25/120    avg_loss:0.213, val_acc:0.896]
Epoch [26/120    avg_loss:0.595, val_acc:0.877]
Epoch [27/120    avg_loss:0.302, val_acc:0.882]
Epoch [28/120    avg_loss:0.290, val_acc:0.871]
Epoch [29/120    avg_loss:0.266, val_acc:0.881]
Epoch [30/120    avg_loss:0.204, val_acc:0.874]
Epoch [31/120    avg_loss:0.176, val_acc:0.885]
Epoch [32/120    avg_loss:0.142, val_acc:0.929]
Epoch [33/120    avg_loss:0.203, val_acc:0.918]
Epoch [34/120    avg_loss:0.182, val_acc:0.915]
Epoch [35/120    avg_loss:0.211, val_acc:0.907]
Epoch [36/120    avg_loss:0.128, val_acc:0.906]
Epoch [37/120    avg_loss:0.141, val_acc:0.925]
Epoch [38/120    avg_loss:0.126, val_acc:0.917]
Epoch [39/120    avg_loss:0.096, val_acc:0.942]
Epoch [40/120    avg_loss:0.100, val_acc:0.927]
Epoch [41/120    avg_loss:0.115, val_acc:0.926]
Epoch [42/120    avg_loss:0.137, val_acc:0.935]
Epoch [43/120    avg_loss:0.084, val_acc:0.936]
Epoch [44/120    avg_loss:0.119, val_acc:0.913]
Epoch [45/120    avg_loss:0.085, val_acc:0.940]
Epoch [46/120    avg_loss:0.070, val_acc:0.951]
Epoch [47/120    avg_loss:0.070, val_acc:0.963]
Epoch [48/120    avg_loss:0.052, val_acc:0.955]
Epoch [49/120    avg_loss:0.076, val_acc:0.952]
Epoch [50/120    avg_loss:0.083, val_acc:0.941]
Epoch [51/120    avg_loss:0.122, val_acc:0.949]
Epoch [52/120    avg_loss:0.089, val_acc:0.967]
Epoch [53/120    avg_loss:0.082, val_acc:0.946]
Epoch [54/120    avg_loss:0.076, val_acc:0.960]
Epoch [55/120    avg_loss:0.051, val_acc:0.959]
Epoch [56/120    avg_loss:0.035, val_acc:0.965]
Epoch [57/120    avg_loss:0.033, val_acc:0.968]
Epoch [58/120    avg_loss:0.087, val_acc:0.942]
Epoch [59/120    avg_loss:0.098, val_acc:0.924]
Epoch [60/120    avg_loss:0.065, val_acc:0.951]
Epoch [61/120    avg_loss:0.056, val_acc:0.960]
Epoch [62/120    avg_loss:0.040, val_acc:0.965]
Epoch [63/120    avg_loss:0.075, val_acc:0.951]
Epoch [64/120    avg_loss:0.051, val_acc:0.976]
Epoch [65/120    avg_loss:0.045, val_acc:0.965]
Epoch [66/120    avg_loss:0.037, val_acc:0.958]
Epoch [67/120    avg_loss:0.036, val_acc:0.968]
Epoch [68/120    avg_loss:0.030, val_acc:0.962]
Epoch [69/120    avg_loss:0.050, val_acc:0.958]
Epoch [70/120    avg_loss:0.077, val_acc:0.944]
Epoch [71/120    avg_loss:0.078, val_acc:0.956]
Epoch [72/120    avg_loss:0.046, val_acc:0.976]
Epoch [73/120    avg_loss:0.047, val_acc:0.954]
Epoch [74/120    avg_loss:0.050, val_acc:0.966]
Epoch [75/120    avg_loss:0.037, val_acc:0.971]
Epoch [76/120    avg_loss:0.053, val_acc:0.966]
Epoch [77/120    avg_loss:0.083, val_acc:0.952]
Epoch [78/120    avg_loss:0.079, val_acc:0.941]
Epoch [79/120    avg_loss:0.037, val_acc:0.980]
Epoch [80/120    avg_loss:0.026, val_acc:0.979]
Epoch [81/120    avg_loss:0.038, val_acc:0.978]
Epoch [82/120    avg_loss:0.023, val_acc:0.971]
Epoch [83/120    avg_loss:0.023, val_acc:0.975]
Epoch [84/120    avg_loss:0.032, val_acc:0.976]
Epoch [85/120    avg_loss:0.035, val_acc:0.981]
Epoch [86/120    avg_loss:0.019, val_acc:0.975]
Epoch [87/120    avg_loss:0.014, val_acc:0.984]
Epoch [88/120    avg_loss:0.022, val_acc:0.974]
Epoch [89/120    avg_loss:0.017, val_acc:0.974]
Epoch [90/120    avg_loss:0.017, val_acc:0.976]
Epoch [91/120    avg_loss:0.035, val_acc:0.971]
Epoch [92/120    avg_loss:0.029, val_acc:0.976]
Epoch [93/120    avg_loss:0.077, val_acc:0.959]
Epoch [94/120    avg_loss:0.030, val_acc:0.975]
Epoch [95/120    avg_loss:0.045, val_acc:0.972]
Epoch [96/120    avg_loss:0.039, val_acc:0.980]
Epoch [97/120    avg_loss:0.030, val_acc:0.969]
Epoch [98/120    avg_loss:0.012, val_acc:0.976]
Epoch [99/120    avg_loss:0.016, val_acc:0.983]
Epoch [100/120    avg_loss:0.017, val_acc:0.980]
Epoch [101/120    avg_loss:0.011, val_acc:0.980]
Epoch [102/120    avg_loss:0.011, val_acc:0.981]
Epoch [103/120    avg_loss:0.007, val_acc:0.981]
Epoch [104/120    avg_loss:0.009, val_acc:0.981]
Epoch [105/120    avg_loss:0.012, val_acc:0.981]
Epoch [106/120    avg_loss:0.009, val_acc:0.982]
Epoch [107/120    avg_loss:0.012, val_acc:0.983]
Epoch [108/120    avg_loss:0.013, val_acc:0.984]
Epoch [109/120    avg_loss:0.008, val_acc:0.982]
Epoch [110/120    avg_loss:0.008, val_acc:0.982]
Epoch [111/120    avg_loss:0.010, val_acc:0.983]
Epoch [112/120    avg_loss:0.007, val_acc:0.982]
Epoch [113/120    avg_loss:0.007, val_acc:0.982]
Epoch [114/120    avg_loss:0.011, val_acc:0.983]
Epoch [115/120    avg_loss:0.009, val_acc:0.983]
Epoch [116/120    avg_loss:0.011, val_acc:0.984]
Epoch [117/120    avg_loss:0.013, val_acc:0.984]
Epoch [118/120    avg_loss:0.008, val_acc:0.983]
Epoch [119/120    avg_loss:0.007, val_acc:0.984]
Epoch [120/120    avg_loss:0.009, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1267    0    0    0    0    0    0    0    5   13    0    0
     0    0    0]
 [   0    0    0  719    0    5    0    0    0    5    2    0   15    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    2    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    1    0    0    0  853   14    0    0
     1    3    0]
 [   0    0    6    1    0    0    0    0    0    0    1 2198    2    2
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0    0    0  527    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    45  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.39566395663957

F1 scores:
[       nan 0.96202532 0.98752923 0.98023177 1.         0.99313501
 0.99160946 0.98039216 0.99883856 0.87804878 0.98045977 0.99075952
 0.97773655 0.9919571  0.97722389 0.92141757 0.98823529]

Kappa:
0.981696861661293
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2821c95828>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.345, val_acc:0.530]
Epoch [2/120    avg_loss:1.822, val_acc:0.591]
Epoch [3/120    avg_loss:1.510, val_acc:0.650]
Epoch [4/120    avg_loss:1.284, val_acc:0.706]
Epoch [5/120    avg_loss:1.048, val_acc:0.699]
Epoch [6/120    avg_loss:0.907, val_acc:0.720]
Epoch [7/120    avg_loss:0.791, val_acc:0.771]
Epoch [8/120    avg_loss:0.716, val_acc:0.773]
Epoch [9/120    avg_loss:0.704, val_acc:0.756]
Epoch [10/120    avg_loss:0.596, val_acc:0.776]
Epoch [11/120    avg_loss:0.615, val_acc:0.822]
Epoch [12/120    avg_loss:0.514, val_acc:0.803]
Epoch [13/120    avg_loss:0.441, val_acc:0.812]
Epoch [14/120    avg_loss:0.473, val_acc:0.836]
Epoch [15/120    avg_loss:0.449, val_acc:0.878]
Epoch [16/120    avg_loss:0.455, val_acc:0.869]
Epoch [17/120    avg_loss:0.358, val_acc:0.795]
Epoch [18/120    avg_loss:0.327, val_acc:0.860]
Epoch [19/120    avg_loss:0.249, val_acc:0.919]
Epoch [20/120    avg_loss:0.258, val_acc:0.897]
Epoch [21/120    avg_loss:0.232, val_acc:0.875]
Epoch [22/120    avg_loss:0.251, val_acc:0.868]
Epoch [23/120    avg_loss:0.246, val_acc:0.917]
Epoch [24/120    avg_loss:0.215, val_acc:0.926]
Epoch [25/120    avg_loss:0.222, val_acc:0.876]
Epoch [26/120    avg_loss:0.198, val_acc:0.903]
Epoch [27/120    avg_loss:0.159, val_acc:0.925]
Epoch [28/120    avg_loss:0.196, val_acc:0.928]
Epoch [29/120    avg_loss:0.173, val_acc:0.889]
Epoch [30/120    avg_loss:0.188, val_acc:0.923]
Epoch [31/120    avg_loss:0.146, val_acc:0.921]
Epoch [32/120    avg_loss:0.129, val_acc:0.944]
Epoch [33/120    avg_loss:0.155, val_acc:0.917]
Epoch [34/120    avg_loss:0.147, val_acc:0.910]
Epoch [35/120    avg_loss:0.133, val_acc:0.938]
Epoch [36/120    avg_loss:0.114, val_acc:0.944]
Epoch [37/120    avg_loss:0.102, val_acc:0.951]
Epoch [38/120    avg_loss:0.101, val_acc:0.926]
Epoch [39/120    avg_loss:0.086, val_acc:0.945]
Epoch [40/120    avg_loss:0.126, val_acc:0.939]
Epoch [41/120    avg_loss:0.089, val_acc:0.947]
Epoch [42/120    avg_loss:0.077, val_acc:0.942]
Epoch [43/120    avg_loss:0.057, val_acc:0.963]
Epoch [44/120    avg_loss:0.060, val_acc:0.960]
Epoch [45/120    avg_loss:0.082, val_acc:0.942]
Epoch [46/120    avg_loss:0.059, val_acc:0.951]
Epoch [47/120    avg_loss:0.119, val_acc:0.943]
Epoch [48/120    avg_loss:0.056, val_acc:0.924]
Epoch [49/120    avg_loss:0.121, val_acc:0.919]
Epoch [50/120    avg_loss:0.171, val_acc:0.928]
Epoch [51/120    avg_loss:0.084, val_acc:0.960]
Epoch [52/120    avg_loss:0.066, val_acc:0.944]
Epoch [53/120    avg_loss:0.066, val_acc:0.944]
Epoch [54/120    avg_loss:0.058, val_acc:0.953]
Epoch [55/120    avg_loss:0.056, val_acc:0.958]
Epoch [56/120    avg_loss:0.061, val_acc:0.961]
Epoch [57/120    avg_loss:0.033, val_acc:0.962]
Epoch [58/120    avg_loss:0.023, val_acc:0.966]
Epoch [59/120    avg_loss:0.023, val_acc:0.970]
Epoch [60/120    avg_loss:0.031, val_acc:0.964]
Epoch [61/120    avg_loss:0.028, val_acc:0.968]
Epoch [62/120    avg_loss:0.019, val_acc:0.967]
Epoch [63/120    avg_loss:0.023, val_acc:0.970]
Epoch [64/120    avg_loss:0.026, val_acc:0.968]
Epoch [65/120    avg_loss:0.024, val_acc:0.973]
Epoch [66/120    avg_loss:0.024, val_acc:0.973]
Epoch [67/120    avg_loss:0.022, val_acc:0.971]
Epoch [68/120    avg_loss:0.018, val_acc:0.971]
Epoch [69/120    avg_loss:0.024, val_acc:0.971]
Epoch [70/120    avg_loss:0.021, val_acc:0.972]
Epoch [71/120    avg_loss:0.022, val_acc:0.973]
Epoch [72/120    avg_loss:0.021, val_acc:0.973]
Epoch [73/120    avg_loss:0.023, val_acc:0.973]
Epoch [74/120    avg_loss:0.025, val_acc:0.974]
Epoch [75/120    avg_loss:0.019, val_acc:0.974]
Epoch [76/120    avg_loss:0.024, val_acc:0.974]
Epoch [77/120    avg_loss:0.019, val_acc:0.976]
Epoch [78/120    avg_loss:0.020, val_acc:0.974]
Epoch [79/120    avg_loss:0.021, val_acc:0.973]
Epoch [80/120    avg_loss:0.016, val_acc:0.972]
Epoch [81/120    avg_loss:0.018, val_acc:0.973]
Epoch [82/120    avg_loss:0.018, val_acc:0.976]
Epoch [83/120    avg_loss:0.018, val_acc:0.974]
Epoch [84/120    avg_loss:0.019, val_acc:0.975]
Epoch [85/120    avg_loss:0.015, val_acc:0.975]
Epoch [86/120    avg_loss:0.020, val_acc:0.976]
Epoch [87/120    avg_loss:0.017, val_acc:0.977]
Epoch [88/120    avg_loss:0.015, val_acc:0.976]
Epoch [89/120    avg_loss:0.015, val_acc:0.974]
Epoch [90/120    avg_loss:0.015, val_acc:0.975]
Epoch [91/120    avg_loss:0.016, val_acc:0.977]
Epoch [92/120    avg_loss:0.015, val_acc:0.976]
Epoch [93/120    avg_loss:0.014, val_acc:0.974]
Epoch [94/120    avg_loss:0.015, val_acc:0.973]
Epoch [95/120    avg_loss:0.014, val_acc:0.975]
Epoch [96/120    avg_loss:0.017, val_acc:0.975]
Epoch [97/120    avg_loss:0.017, val_acc:0.979]
Epoch [98/120    avg_loss:0.016, val_acc:0.977]
Epoch [99/120    avg_loss:0.016, val_acc:0.975]
Epoch [100/120    avg_loss:0.017, val_acc:0.976]
Epoch [101/120    avg_loss:0.017, val_acc:0.973]
Epoch [102/120    avg_loss:0.013, val_acc:0.975]
Epoch [103/120    avg_loss:0.015, val_acc:0.974]
Epoch [104/120    avg_loss:0.018, val_acc:0.975]
Epoch [105/120    avg_loss:0.015, val_acc:0.972]
Epoch [106/120    avg_loss:0.023, val_acc:0.973]
Epoch [107/120    avg_loss:0.014, val_acc:0.975]
Epoch [108/120    avg_loss:0.013, val_acc:0.974]
Epoch [109/120    avg_loss:0.014, val_acc:0.974]
Epoch [110/120    avg_loss:0.016, val_acc:0.976]
Epoch [111/120    avg_loss:0.017, val_acc:0.976]
Epoch [112/120    avg_loss:0.015, val_acc:0.976]
Epoch [113/120    avg_loss:0.013, val_acc:0.976]
Epoch [114/120    avg_loss:0.012, val_acc:0.976]
Epoch [115/120    avg_loss:0.013, val_acc:0.975]
Epoch [116/120    avg_loss:0.015, val_acc:0.975]
Epoch [117/120    avg_loss:0.013, val_acc:0.975]
Epoch [118/120    avg_loss:0.014, val_acc:0.975]
Epoch [119/120    avg_loss:0.013, val_acc:0.975]
Epoch [120/120    avg_loss:0.011, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    2    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1260    2    1    0    1    0    0    0    7   14    0    0
     0    0    0]
 [   0    0    0  714    0   14    0    0    0    3    2    0   11    3
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    2    0    0    0    0  852    5    1    0
     3    7    0]
 [   0    2   12    0    0    0    4    0    0    0   19 2167    4    0
     2    0    0]
 [   0    0    0    0    0    5    0    0    0    0    0    0  527    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    27  320    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.1029810298103

F1 scores:
[       nan 0.925      0.98283931 0.97540984 0.99530516 0.97297297
 0.99315589 1.         0.99883856 0.92307692 0.96818182 0.985
 0.97773655 0.9919571  0.98310957 0.9495549  0.98224852]

Kappa:
0.978378730287613
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:01:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f26027b8828>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.328, val_acc:0.375]
Epoch [2/120    avg_loss:1.799, val_acc:0.609]
Epoch [3/120    avg_loss:1.498, val_acc:0.658]
Epoch [4/120    avg_loss:1.250, val_acc:0.690]
Epoch [5/120    avg_loss:1.093, val_acc:0.724]
Epoch [6/120    avg_loss:0.978, val_acc:0.629]
Epoch [7/120    avg_loss:0.770, val_acc:0.744]
Epoch [8/120    avg_loss:0.747, val_acc:0.789]
Epoch [9/120    avg_loss:0.697, val_acc:0.811]
Epoch [10/120    avg_loss:0.610, val_acc:0.808]
Epoch [11/120    avg_loss:0.511, val_acc:0.817]
Epoch [12/120    avg_loss:0.509, val_acc:0.854]
Epoch [13/120    avg_loss:0.497, val_acc:0.819]
Epoch [14/120    avg_loss:0.445, val_acc:0.893]
Epoch [15/120    avg_loss:0.383, val_acc:0.836]
Epoch [16/120    avg_loss:0.315, val_acc:0.889]
Epoch [17/120    avg_loss:0.367, val_acc:0.878]
Epoch [18/120    avg_loss:0.350, val_acc:0.908]
Epoch [19/120    avg_loss:0.254, val_acc:0.907]
Epoch [20/120    avg_loss:0.225, val_acc:0.900]
Epoch [21/120    avg_loss:0.217, val_acc:0.925]
Epoch [22/120    avg_loss:0.202, val_acc:0.906]
Epoch [23/120    avg_loss:0.219, val_acc:0.929]
Epoch [24/120    avg_loss:0.245, val_acc:0.926]
Epoch [25/120    avg_loss:0.148, val_acc:0.936]
Epoch [26/120    avg_loss:0.210, val_acc:0.933]
Epoch [27/120    avg_loss:0.222, val_acc:0.932]
Epoch [28/120    avg_loss:0.192, val_acc:0.933]
Epoch [29/120    avg_loss:0.151, val_acc:0.948]
Epoch [30/120    avg_loss:0.153, val_acc:0.950]
Epoch [31/120    avg_loss:0.111, val_acc:0.952]
Epoch [32/120    avg_loss:0.117, val_acc:0.953]
Epoch [33/120    avg_loss:0.115, val_acc:0.871]
Epoch [34/120    avg_loss:0.163, val_acc:0.947]
Epoch [35/120    avg_loss:0.108, val_acc:0.950]
Epoch [36/120    avg_loss:0.111, val_acc:0.939]
Epoch [37/120    avg_loss:0.099, val_acc:0.930]
Epoch [38/120    avg_loss:0.137, val_acc:0.946]
Epoch [39/120    avg_loss:0.097, val_acc:0.952]
Epoch [40/120    avg_loss:0.072, val_acc:0.961]
Epoch [41/120    avg_loss:0.060, val_acc:0.960]
Epoch [42/120    avg_loss:0.054, val_acc:0.966]
Epoch [43/120    avg_loss:0.053, val_acc:0.961]
Epoch [44/120    avg_loss:0.037, val_acc:0.971]
Epoch [45/120    avg_loss:0.074, val_acc:0.927]
Epoch [46/120    avg_loss:0.085, val_acc:0.936]
Epoch [47/120    avg_loss:0.093, val_acc:0.961]
Epoch [48/120    avg_loss:0.091, val_acc:0.966]
Epoch [49/120    avg_loss:0.120, val_acc:0.957]
Epoch [50/120    avg_loss:0.045, val_acc:0.963]
Epoch [51/120    avg_loss:0.044, val_acc:0.969]
Epoch [52/120    avg_loss:0.050, val_acc:0.966]
Epoch [53/120    avg_loss:0.049, val_acc:0.976]
Epoch [54/120    avg_loss:0.050, val_acc:0.952]
Epoch [55/120    avg_loss:0.076, val_acc:0.954]
Epoch [56/120    avg_loss:0.088, val_acc:0.960]
Epoch [57/120    avg_loss:0.057, val_acc:0.974]
Epoch [58/120    avg_loss:0.046, val_acc:0.975]
Epoch [59/120    avg_loss:0.041, val_acc:0.964]
Epoch [60/120    avg_loss:0.106, val_acc:0.934]
Epoch [61/120    avg_loss:0.083, val_acc:0.961]
Epoch [62/120    avg_loss:0.054, val_acc:0.976]
Epoch [63/120    avg_loss:0.076, val_acc:0.932]
Epoch [64/120    avg_loss:0.067, val_acc:0.959]
Epoch [65/120    avg_loss:0.046, val_acc:0.974]
Epoch [66/120    avg_loss:0.034, val_acc:0.966]
Epoch [67/120    avg_loss:0.040, val_acc:0.964]
Epoch [68/120    avg_loss:0.046, val_acc:0.965]
Epoch [69/120    avg_loss:0.059, val_acc:0.967]
Epoch [70/120    avg_loss:0.040, val_acc:0.976]
Epoch [71/120    avg_loss:0.037, val_acc:0.977]
Epoch [72/120    avg_loss:0.026, val_acc:0.981]
Epoch [73/120    avg_loss:0.044, val_acc:0.964]
Epoch [74/120    avg_loss:0.032, val_acc:0.971]
Epoch [75/120    avg_loss:0.037, val_acc:0.966]
Epoch [76/120    avg_loss:0.065, val_acc:0.979]
Epoch [77/120    avg_loss:0.022, val_acc:0.983]
Epoch [78/120    avg_loss:0.114, val_acc:0.922]
Epoch [79/120    avg_loss:0.102, val_acc:0.971]
Epoch [80/120    avg_loss:0.048, val_acc:0.958]
Epoch [81/120    avg_loss:0.042, val_acc:0.974]
Epoch [82/120    avg_loss:0.021, val_acc:0.977]
Epoch [83/120    avg_loss:0.019, val_acc:0.979]
Epoch [84/120    avg_loss:0.015, val_acc:0.986]
Epoch [85/120    avg_loss:0.018, val_acc:0.988]
Epoch [86/120    avg_loss:0.014, val_acc:0.984]
Epoch [87/120    avg_loss:0.013, val_acc:0.986]
Epoch [88/120    avg_loss:0.015, val_acc:0.983]
Epoch [89/120    avg_loss:0.015, val_acc:0.977]
Epoch [90/120    avg_loss:0.018, val_acc:0.988]
Epoch [91/120    avg_loss:0.014, val_acc:0.993]
Epoch [92/120    avg_loss:0.030, val_acc:0.967]
Epoch [93/120    avg_loss:0.034, val_acc:0.985]
Epoch [94/120    avg_loss:0.019, val_acc:0.982]
Epoch [95/120    avg_loss:0.028, val_acc:0.982]
Epoch [96/120    avg_loss:0.016, val_acc:0.994]
Epoch [97/120    avg_loss:0.013, val_acc:0.988]
Epoch [98/120    avg_loss:0.015, val_acc:0.983]
Epoch [99/120    avg_loss:0.032, val_acc:0.988]
Epoch [100/120    avg_loss:0.012, val_acc:0.989]
Epoch [101/120    avg_loss:0.013, val_acc:0.990]
Epoch [102/120    avg_loss:0.011, val_acc:0.988]
Epoch [103/120    avg_loss:0.040, val_acc:0.985]
Epoch [104/120    avg_loss:0.013, val_acc:0.989]
Epoch [105/120    avg_loss:0.016, val_acc:0.988]
Epoch [106/120    avg_loss:0.022, val_acc:0.983]
Epoch [107/120    avg_loss:0.016, val_acc:0.974]
Epoch [108/120    avg_loss:0.010, val_acc:0.985]
Epoch [109/120    avg_loss:0.028, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.985]
Epoch [111/120    avg_loss:0.014, val_acc:0.986]
Epoch [112/120    avg_loss:0.008, val_acc:0.985]
Epoch [113/120    avg_loss:0.008, val_acc:0.987]
Epoch [114/120    avg_loss:0.012, val_acc:0.986]
Epoch [115/120    avg_loss:0.008, val_acc:0.986]
Epoch [116/120    avg_loss:0.010, val_acc:0.987]
Epoch [117/120    avg_loss:0.006, val_acc:0.987]
Epoch [118/120    avg_loss:0.006, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1271    0    0    1    2    0    0    0    6    5    0    0
     0    0    0]
 [   0    0    0  691    4   15    0    0    0    4    2    0   27    2
     0    2    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    5    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    3    3    0    0    0  864    1    0    0
     0    3    0]
 [   0    0    5    0    0    1    1    0    0    0    3 2197    0    3
     0    0    0]
 [   0    0    0    0    3    0    0    0    0    0    5    0  524    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    1    0    0    0
  1136    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    78  269    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.8319783197832

F1 scores:
[       nan 0.94871795 0.9921936  0.95972222 0.97911833 0.97175141
 0.99545455 0.90909091 1.         0.87804878 0.98181818 0.99569454
 0.96235078 0.98666667 0.96557586 0.86495177 0.96385542]

Kappa:
0.9752737540521726
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7bb74327b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.354, val_acc:0.518]
Epoch [2/120    avg_loss:1.812, val_acc:0.584]
Epoch [3/120    avg_loss:1.580, val_acc:0.603]
Epoch [4/120    avg_loss:1.352, val_acc:0.677]
Epoch [5/120    avg_loss:1.113, val_acc:0.727]
Epoch [6/120    avg_loss:0.946, val_acc:0.751]
Epoch [7/120    avg_loss:0.814, val_acc:0.745]
Epoch [8/120    avg_loss:0.710, val_acc:0.774]
Epoch [9/120    avg_loss:0.680, val_acc:0.768]
Epoch [10/120    avg_loss:0.562, val_acc:0.809]
Epoch [11/120    avg_loss:0.579, val_acc:0.820]
Epoch [12/120    avg_loss:0.509, val_acc:0.818]
Epoch [13/120    avg_loss:0.480, val_acc:0.829]
Epoch [14/120    avg_loss:0.426, val_acc:0.828]
Epoch [15/120    avg_loss:0.342, val_acc:0.858]
Epoch [16/120    avg_loss:0.360, val_acc:0.879]
Epoch [17/120    avg_loss:0.360, val_acc:0.796]
Epoch [18/120    avg_loss:0.418, val_acc:0.814]
Epoch [19/120    avg_loss:0.334, val_acc:0.875]
Epoch [20/120    avg_loss:0.300, val_acc:0.852]
Epoch [21/120    avg_loss:0.309, val_acc:0.857]
Epoch [22/120    avg_loss:0.269, val_acc:0.867]
Epoch [23/120    avg_loss:0.264, val_acc:0.909]
Epoch [24/120    avg_loss:0.243, val_acc:0.860]
Epoch [25/120    avg_loss:0.231, val_acc:0.877]
Epoch [26/120    avg_loss:0.263, val_acc:0.834]
Epoch [27/120    avg_loss:0.187, val_acc:0.925]
Epoch [28/120    avg_loss:0.189, val_acc:0.915]
Epoch [29/120    avg_loss:0.220, val_acc:0.928]
Epoch [30/120    avg_loss:0.199, val_acc:0.904]
Epoch [31/120    avg_loss:0.139, val_acc:0.920]
Epoch [32/120    avg_loss:0.129, val_acc:0.924]
Epoch [33/120    avg_loss:0.124, val_acc:0.942]
Epoch [34/120    avg_loss:0.145, val_acc:0.941]
Epoch [35/120    avg_loss:0.191, val_acc:0.923]
Epoch [36/120    avg_loss:0.129, val_acc:0.926]
Epoch [37/120    avg_loss:0.105, val_acc:0.943]
Epoch [38/120    avg_loss:0.114, val_acc:0.940]
Epoch [39/120    avg_loss:0.115, val_acc:0.934]
Epoch [40/120    avg_loss:0.099, val_acc:0.939]
Epoch [41/120    avg_loss:0.112, val_acc:0.947]
Epoch [42/120    avg_loss:0.109, val_acc:0.947]
Epoch [43/120    avg_loss:0.141, val_acc:0.885]
Epoch [44/120    avg_loss:0.117, val_acc:0.946]
Epoch [45/120    avg_loss:0.100, val_acc:0.935]
Epoch [46/120    avg_loss:0.094, val_acc:0.946]
Epoch [47/120    avg_loss:0.070, val_acc:0.954]
Epoch [48/120    avg_loss:0.095, val_acc:0.932]
Epoch [49/120    avg_loss:0.121, val_acc:0.943]
Epoch [50/120    avg_loss:0.090, val_acc:0.941]
Epoch [51/120    avg_loss:0.075, val_acc:0.949]
Epoch [52/120    avg_loss:0.055, val_acc:0.964]
Epoch [53/120    avg_loss:0.074, val_acc:0.942]
Epoch [54/120    avg_loss:0.093, val_acc:0.943]
Epoch [55/120    avg_loss:0.120, val_acc:0.953]
Epoch [56/120    avg_loss:0.060, val_acc:0.954]
Epoch [57/120    avg_loss:0.051, val_acc:0.958]
Epoch [58/120    avg_loss:0.039, val_acc:0.951]
Epoch [59/120    avg_loss:0.043, val_acc:0.964]
Epoch [60/120    avg_loss:0.065, val_acc:0.951]
Epoch [61/120    avg_loss:0.045, val_acc:0.964]
Epoch [62/120    avg_loss:0.059, val_acc:0.955]
Epoch [63/120    avg_loss:0.040, val_acc:0.960]
Epoch [64/120    avg_loss:0.108, val_acc:0.949]
Epoch [65/120    avg_loss:0.043, val_acc:0.966]
Epoch [66/120    avg_loss:0.026, val_acc:0.971]
Epoch [67/120    avg_loss:0.039, val_acc:0.975]
Epoch [68/120    avg_loss:0.044, val_acc:0.961]
Epoch [69/120    avg_loss:0.040, val_acc:0.968]
Epoch [70/120    avg_loss:0.050, val_acc:0.960]
Epoch [71/120    avg_loss:0.034, val_acc:0.958]
Epoch [72/120    avg_loss:0.042, val_acc:0.972]
Epoch [73/120    avg_loss:0.054, val_acc:0.974]
Epoch [74/120    avg_loss:0.033, val_acc:0.970]
Epoch [75/120    avg_loss:0.036, val_acc:0.966]
Epoch [76/120    avg_loss:0.049, val_acc:0.970]
Epoch [77/120    avg_loss:0.040, val_acc:0.955]
Epoch [78/120    avg_loss:0.030, val_acc:0.975]
Epoch [79/120    avg_loss:0.023, val_acc:0.971]
Epoch [80/120    avg_loss:0.033, val_acc:0.971]
Epoch [81/120    avg_loss:0.028, val_acc:0.972]
Epoch [82/120    avg_loss:0.019, val_acc:0.972]
Epoch [83/120    avg_loss:0.018, val_acc:0.968]
Epoch [84/120    avg_loss:0.031, val_acc:0.971]
Epoch [85/120    avg_loss:0.037, val_acc:0.953]
Epoch [86/120    avg_loss:0.057, val_acc:0.955]
Epoch [87/120    avg_loss:0.046, val_acc:0.967]
Epoch [88/120    avg_loss:0.034, val_acc:0.948]
Epoch [89/120    avg_loss:0.056, val_acc:0.957]
Epoch [90/120    avg_loss:0.034, val_acc:0.977]
Epoch [91/120    avg_loss:0.061, val_acc:0.964]
Epoch [92/120    avg_loss:0.031, val_acc:0.975]
Epoch [93/120    avg_loss:0.025, val_acc:0.966]
Epoch [94/120    avg_loss:0.028, val_acc:0.968]
Epoch [95/120    avg_loss:0.024, val_acc:0.982]
Epoch [96/120    avg_loss:0.047, val_acc:0.972]
Epoch [97/120    avg_loss:0.024, val_acc:0.974]
Epoch [98/120    avg_loss:0.013, val_acc:0.982]
Epoch [99/120    avg_loss:0.017, val_acc:0.975]
Epoch [100/120    avg_loss:0.021, val_acc:0.971]
Epoch [101/120    avg_loss:0.025, val_acc:0.975]
Epoch [102/120    avg_loss:0.074, val_acc:0.927]
Epoch [103/120    avg_loss:0.097, val_acc:0.951]
Epoch [104/120    avg_loss:0.065, val_acc:0.970]
Epoch [105/120    avg_loss:0.038, val_acc:0.949]
Epoch [106/120    avg_loss:0.041, val_acc:0.942]
Epoch [107/120    avg_loss:0.087, val_acc:0.925]
Epoch [108/120    avg_loss:0.104, val_acc:0.964]
Epoch [109/120    avg_loss:0.156, val_acc:0.930]
Epoch [110/120    avg_loss:0.090, val_acc:0.959]
Epoch [111/120    avg_loss:0.046, val_acc:0.970]
Epoch [112/120    avg_loss:0.025, val_acc:0.974]
Epoch [113/120    avg_loss:0.023, val_acc:0.975]
Epoch [114/120    avg_loss:0.030, val_acc:0.976]
Epoch [115/120    avg_loss:0.019, val_acc:0.977]
Epoch [116/120    avg_loss:0.019, val_acc:0.979]
Epoch [117/120    avg_loss:0.019, val_acc:0.978]
Epoch [118/120    avg_loss:0.021, val_acc:0.978]
Epoch [119/120    avg_loss:0.019, val_acc:0.982]
Epoch [120/120    avg_loss:0.015, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    3 1258    0    0    0    0    0    0    2   10   11    1    0
     0    0    0]
 [   0    0    0  713    0   18    0    0    0    7    2    0    1    3
     3    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   14   31    0    7    0    0    0    0  803   18    0    0
     0    2    0]
 [   0    0    6    0    0    1    2    0    0    0    8 2187    6    0
     0    0    0]
 [   0    0    3   14    2    7    0    0    0    0    3    8  487    0
     1    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   15    0    0    1    0    0    0    0
    98  233    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.49864498644986

F1 scores:
[       nan 0.95238095 0.98051442 0.94750831 0.9953271  0.96230599
 0.98646617 1.         0.99883856 0.76595745 0.94248826 0.9864682
 0.94655005 0.9919571  0.95542473 0.80068729 0.94915254]

Kappa:
0.9600413014170659
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f24c631c7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.312, val_acc:0.430]
Epoch [2/120    avg_loss:1.835, val_acc:0.596]
Epoch [3/120    avg_loss:1.544, val_acc:0.623]
Epoch [4/120    avg_loss:1.244, val_acc:0.659]
Epoch [5/120    avg_loss:0.998, val_acc:0.709]
Epoch [6/120    avg_loss:0.915, val_acc:0.708]
Epoch [7/120    avg_loss:0.848, val_acc:0.779]
Epoch [8/120    avg_loss:0.709, val_acc:0.768]
Epoch [9/120    avg_loss:0.700, val_acc:0.714]
Epoch [10/120    avg_loss:0.692, val_acc:0.764]
Epoch [11/120    avg_loss:0.597, val_acc:0.823]
Epoch [12/120    avg_loss:0.569, val_acc:0.842]
Epoch [13/120    avg_loss:0.489, val_acc:0.775]
Epoch [14/120    avg_loss:0.471, val_acc:0.857]
Epoch [15/120    avg_loss:0.441, val_acc:0.758]
Epoch [16/120    avg_loss:0.454, val_acc:0.844]
Epoch [17/120    avg_loss:0.369, val_acc:0.851]
Epoch [18/120    avg_loss:0.377, val_acc:0.888]
Epoch [19/120    avg_loss:0.267, val_acc:0.853]
Epoch [20/120    avg_loss:0.285, val_acc:0.879]
Epoch [21/120    avg_loss:0.306, val_acc:0.855]
Epoch [22/120    avg_loss:0.480, val_acc:0.829]
Epoch [23/120    avg_loss:0.270, val_acc:0.904]
Epoch [24/120    avg_loss:0.219, val_acc:0.862]
Epoch [25/120    avg_loss:0.273, val_acc:0.874]
Epoch [26/120    avg_loss:0.242, val_acc:0.901]
Epoch [27/120    avg_loss:0.192, val_acc:0.904]
Epoch [28/120    avg_loss:0.216, val_acc:0.927]
Epoch [29/120    avg_loss:0.170, val_acc:0.910]
Epoch [30/120    avg_loss:0.154, val_acc:0.933]
Epoch [31/120    avg_loss:0.178, val_acc:0.867]
Epoch [32/120    avg_loss:0.174, val_acc:0.924]
Epoch [33/120    avg_loss:0.171, val_acc:0.912]
Epoch [34/120    avg_loss:0.160, val_acc:0.906]
Epoch [35/120    avg_loss:0.156, val_acc:0.934]
Epoch [36/120    avg_loss:0.147, val_acc:0.912]
Epoch [37/120    avg_loss:0.146, val_acc:0.939]
Epoch [38/120    avg_loss:0.191, val_acc:0.901]
Epoch [39/120    avg_loss:0.167, val_acc:0.916]
Epoch [40/120    avg_loss:0.186, val_acc:0.920]
Epoch [41/120    avg_loss:0.127, val_acc:0.933]
Epoch [42/120    avg_loss:0.106, val_acc:0.941]
Epoch [43/120    avg_loss:0.139, val_acc:0.923]
Epoch [44/120    avg_loss:0.095, val_acc:0.938]
Epoch [45/120    avg_loss:0.073, val_acc:0.939]
Epoch [46/120    avg_loss:0.094, val_acc:0.938]
Epoch [47/120    avg_loss:0.069, val_acc:0.952]
Epoch [48/120    avg_loss:0.135, val_acc:0.874]
Epoch [49/120    avg_loss:0.106, val_acc:0.933]
Epoch [50/120    avg_loss:0.127, val_acc:0.886]
Epoch [51/120    avg_loss:0.152, val_acc:0.931]
Epoch [52/120    avg_loss:0.120, val_acc:0.930]
Epoch [53/120    avg_loss:0.064, val_acc:0.934]
Epoch [54/120    avg_loss:0.052, val_acc:0.955]
Epoch [55/120    avg_loss:0.058, val_acc:0.943]
Epoch [56/120    avg_loss:0.075, val_acc:0.956]
Epoch [57/120    avg_loss:0.088, val_acc:0.948]
Epoch [58/120    avg_loss:0.074, val_acc:0.951]
Epoch [59/120    avg_loss:0.050, val_acc:0.961]
Epoch [60/120    avg_loss:0.048, val_acc:0.958]
Epoch [61/120    avg_loss:0.052, val_acc:0.957]
Epoch [62/120    avg_loss:0.053, val_acc:0.956]
Epoch [63/120    avg_loss:0.041, val_acc:0.963]
Epoch [64/120    avg_loss:0.052, val_acc:0.964]
Epoch [65/120    avg_loss:0.038, val_acc:0.959]
Epoch [66/120    avg_loss:0.042, val_acc:0.952]
Epoch [67/120    avg_loss:0.089, val_acc:0.955]
Epoch [68/120    avg_loss:0.052, val_acc:0.954]
Epoch [69/120    avg_loss:0.048, val_acc:0.959]
Epoch [70/120    avg_loss:0.078, val_acc:0.944]
Epoch [71/120    avg_loss:0.059, val_acc:0.951]
Epoch [72/120    avg_loss:0.053, val_acc:0.947]
Epoch [73/120    avg_loss:0.029, val_acc:0.962]
Epoch [74/120    avg_loss:0.048, val_acc:0.968]
Epoch [75/120    avg_loss:0.041, val_acc:0.967]
Epoch [76/120    avg_loss:0.043, val_acc:0.959]
Epoch [77/120    avg_loss:0.041, val_acc:0.965]
Epoch [78/120    avg_loss:0.046, val_acc:0.948]
Epoch [79/120    avg_loss:0.039, val_acc:0.953]
Epoch [80/120    avg_loss:0.043, val_acc:0.964]
Epoch [81/120    avg_loss:0.047, val_acc:0.948]
Epoch [82/120    avg_loss:0.081, val_acc:0.957]
Epoch [83/120    avg_loss:0.033, val_acc:0.974]
Epoch [84/120    avg_loss:0.085, val_acc:0.966]
Epoch [85/120    avg_loss:0.033, val_acc:0.944]
Epoch [86/120    avg_loss:0.049, val_acc:0.964]
Epoch [87/120    avg_loss:0.051, val_acc:0.970]
Epoch [88/120    avg_loss:0.125, val_acc:0.919]
Epoch [89/120    avg_loss:0.108, val_acc:0.951]
Epoch [90/120    avg_loss:0.038, val_acc:0.925]
Epoch [91/120    avg_loss:0.043, val_acc:0.963]
Epoch [92/120    avg_loss:0.033, val_acc:0.970]
Epoch [93/120    avg_loss:0.032, val_acc:0.973]
Epoch [94/120    avg_loss:0.024, val_acc:0.974]
Epoch [95/120    avg_loss:0.029, val_acc:0.973]
Epoch [96/120    avg_loss:0.019, val_acc:0.977]
Epoch [97/120    avg_loss:0.018, val_acc:0.979]
Epoch [98/120    avg_loss:0.023, val_acc:0.942]
Epoch [99/120    avg_loss:0.028, val_acc:0.973]
Epoch [100/120    avg_loss:0.016, val_acc:0.964]
Epoch [101/120    avg_loss:0.011, val_acc:0.975]
Epoch [102/120    avg_loss:0.012, val_acc:0.978]
Epoch [103/120    avg_loss:0.011, val_acc:0.979]
Epoch [104/120    avg_loss:0.023, val_acc:0.943]
Epoch [105/120    avg_loss:0.078, val_acc:0.966]
Epoch [106/120    avg_loss:0.038, val_acc:0.971]
Epoch [107/120    avg_loss:0.029, val_acc:0.968]
Epoch [108/120    avg_loss:0.015, val_acc:0.971]
Epoch [109/120    avg_loss:0.010, val_acc:0.974]
Epoch [110/120    avg_loss:0.008, val_acc:0.978]
Epoch [111/120    avg_loss:0.012, val_acc:0.968]
Epoch [112/120    avg_loss:0.021, val_acc:0.974]
Epoch [113/120    avg_loss:0.014, val_acc:0.964]
Epoch [114/120    avg_loss:0.024, val_acc:0.953]
Epoch [115/120    avg_loss:0.024, val_acc:0.970]
Epoch [116/120    avg_loss:0.019, val_acc:0.978]
Epoch [117/120    avg_loss:0.011, val_acc:0.979]
Epoch [118/120    avg_loss:0.013, val_acc:0.979]
Epoch [119/120    avg_loss:0.009, val_acc:0.979]
Epoch [120/120    avg_loss:0.008, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1265    7    0    0    1    0    0    1    5    5    0    0
     0    1    0]
 [   0    0    0  713    0   26    0    0    0    1    1    0    5    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5   17    0    8    0    0    0    0  824   14    3    0
     0    4    0]
 [   0    0    9    0    0    0    1    0    0    0   13 2185    0    2
     0    0    0]
 [   0    0    0    6    0   10    0    0    0    0    4    9  502    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   117  230    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.83468834688347

F1 scores:
[       nan 0.96202532 0.98673947 0.95704698 1.         0.95071194
 0.99466056 1.         0.99883856 0.92307692 0.95425594 0.98779385
 0.96076555 0.9919571  0.94824708 0.79037801 0.97647059]

Kappa:
0.963874295099815
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f942772e7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.377, val_acc:0.504]
Epoch [2/120    avg_loss:1.832, val_acc:0.601]
Epoch [3/120    avg_loss:1.530, val_acc:0.601]
Epoch [4/120    avg_loss:1.295, val_acc:0.670]
Epoch [5/120    avg_loss:1.131, val_acc:0.700]
Epoch [6/120    avg_loss:1.050, val_acc:0.687]
Epoch [7/120    avg_loss:0.840, val_acc:0.752]
Epoch [8/120    avg_loss:0.742, val_acc:0.742]
Epoch [9/120    avg_loss:0.745, val_acc:0.734]
Epoch [10/120    avg_loss:0.590, val_acc:0.829]
Epoch [11/120    avg_loss:0.595, val_acc:0.725]
Epoch [12/120    avg_loss:0.536, val_acc:0.820]
Epoch [13/120    avg_loss:0.521, val_acc:0.784]
Epoch [14/120    avg_loss:0.470, val_acc:0.824]
Epoch [15/120    avg_loss:0.388, val_acc:0.849]
Epoch [16/120    avg_loss:0.417, val_acc:0.878]
Epoch [17/120    avg_loss:0.381, val_acc:0.882]
Epoch [18/120    avg_loss:0.366, val_acc:0.876]
Epoch [19/120    avg_loss:0.308, val_acc:0.884]
Epoch [20/120    avg_loss:0.305, val_acc:0.899]
Epoch [21/120    avg_loss:0.343, val_acc:0.862]
Epoch [22/120    avg_loss:0.282, val_acc:0.905]
Epoch [23/120    avg_loss:0.252, val_acc:0.924]
Epoch [24/120    avg_loss:0.185, val_acc:0.911]
Epoch [25/120    avg_loss:0.226, val_acc:0.895]
Epoch [26/120    avg_loss:0.238, val_acc:0.914]
Epoch [27/120    avg_loss:0.255, val_acc:0.922]
Epoch [28/120    avg_loss:0.200, val_acc:0.927]
Epoch [29/120    avg_loss:0.140, val_acc:0.932]
Epoch [30/120    avg_loss:0.197, val_acc:0.876]
Epoch [31/120    avg_loss:0.186, val_acc:0.941]
Epoch [32/120    avg_loss:0.217, val_acc:0.892]
Epoch [33/120    avg_loss:0.169, val_acc:0.935]
Epoch [34/120    avg_loss:0.123, val_acc:0.945]
Epoch [35/120    avg_loss:0.130, val_acc:0.898]
Epoch [36/120    avg_loss:0.139, val_acc:0.946]
Epoch [37/120    avg_loss:0.171, val_acc:0.933]
Epoch [38/120    avg_loss:0.106, val_acc:0.928]
Epoch [39/120    avg_loss:0.096, val_acc:0.950]
Epoch [40/120    avg_loss:0.074, val_acc:0.937]
Epoch [41/120    avg_loss:0.082, val_acc:0.946]
Epoch [42/120    avg_loss:0.109, val_acc:0.957]
Epoch [43/120    avg_loss:0.131, val_acc:0.932]
Epoch [44/120    avg_loss:0.118, val_acc:0.923]
Epoch [45/120    avg_loss:0.098, val_acc:0.950]
Epoch [46/120    avg_loss:0.065, val_acc:0.961]
Epoch [47/120    avg_loss:0.068, val_acc:0.960]
Epoch [48/120    avg_loss:0.085, val_acc:0.955]
Epoch [49/120    avg_loss:0.083, val_acc:0.948]
Epoch [50/120    avg_loss:0.054, val_acc:0.961]
Epoch [51/120    avg_loss:0.051, val_acc:0.960]
Epoch [52/120    avg_loss:0.056, val_acc:0.955]
Epoch [53/120    avg_loss:0.050, val_acc:0.962]
Epoch [54/120    avg_loss:0.061, val_acc:0.949]
Epoch [55/120    avg_loss:0.050, val_acc:0.962]
Epoch [56/120    avg_loss:0.056, val_acc:0.942]
Epoch [57/120    avg_loss:0.081, val_acc:0.951]
Epoch [58/120    avg_loss:0.062, val_acc:0.952]
Epoch [59/120    avg_loss:0.209, val_acc:0.878]
Epoch [60/120    avg_loss:0.385, val_acc:0.898]
Epoch [61/120    avg_loss:0.189, val_acc:0.942]
Epoch [62/120    avg_loss:0.091, val_acc:0.958]
Epoch [63/120    avg_loss:0.068, val_acc:0.967]
Epoch [64/120    avg_loss:0.060, val_acc:0.940]
Epoch [65/120    avg_loss:0.070, val_acc:0.957]
Epoch [66/120    avg_loss:0.048, val_acc:0.964]
Epoch [67/120    avg_loss:0.040, val_acc:0.966]
Epoch [68/120    avg_loss:0.038, val_acc:0.961]
Epoch [69/120    avg_loss:0.050, val_acc:0.966]
Epoch [70/120    avg_loss:0.032, val_acc:0.960]
Epoch [71/120    avg_loss:0.114, val_acc:0.952]
Epoch [72/120    avg_loss:0.048, val_acc:0.957]
Epoch [73/120    avg_loss:0.066, val_acc:0.959]
Epoch [74/120    avg_loss:0.044, val_acc:0.975]
Epoch [75/120    avg_loss:0.027, val_acc:0.972]
Epoch [76/120    avg_loss:0.035, val_acc:0.963]
Epoch [77/120    avg_loss:0.029, val_acc:0.971]
Epoch [78/120    avg_loss:0.035, val_acc:0.973]
Epoch [79/120    avg_loss:0.021, val_acc:0.972]
Epoch [80/120    avg_loss:0.014, val_acc:0.967]
Epoch [81/120    avg_loss:0.024, val_acc:0.964]
Epoch [82/120    avg_loss:0.022, val_acc:0.974]
Epoch [83/120    avg_loss:0.015, val_acc:0.982]
Epoch [84/120    avg_loss:0.018, val_acc:0.976]
Epoch [85/120    avg_loss:0.023, val_acc:0.964]
Epoch [86/120    avg_loss:0.032, val_acc:0.971]
Epoch [87/120    avg_loss:0.030, val_acc:0.963]
Epoch [88/120    avg_loss:0.026, val_acc:0.976]
Epoch [89/120    avg_loss:0.020, val_acc:0.975]
Epoch [90/120    avg_loss:0.020, val_acc:0.983]
Epoch [91/120    avg_loss:0.015, val_acc:0.978]
Epoch [92/120    avg_loss:0.018, val_acc:0.976]
Epoch [93/120    avg_loss:0.012, val_acc:0.978]
Epoch [94/120    avg_loss:0.009, val_acc:0.979]
Epoch [95/120    avg_loss:0.012, val_acc:0.973]
Epoch [96/120    avg_loss:0.011, val_acc:0.984]
Epoch [97/120    avg_loss:0.015, val_acc:0.978]
Epoch [98/120    avg_loss:0.010, val_acc:0.977]
Epoch [99/120    avg_loss:0.008, val_acc:0.976]
Epoch [100/120    avg_loss:0.015, val_acc:0.975]
Epoch [101/120    avg_loss:0.055, val_acc:0.965]
Epoch [102/120    avg_loss:0.030, val_acc:0.972]
Epoch [103/120    avg_loss:0.018, val_acc:0.978]
Epoch [104/120    avg_loss:0.016, val_acc:0.979]
Epoch [105/120    avg_loss:0.013, val_acc:0.979]
Epoch [106/120    avg_loss:0.013, val_acc:0.982]
Epoch [107/120    avg_loss:0.012, val_acc:0.978]
Epoch [108/120    avg_loss:0.011, val_acc:0.975]
Epoch [109/120    avg_loss:0.012, val_acc:0.980]
Epoch [110/120    avg_loss:0.007, val_acc:0.980]
Epoch [111/120    avg_loss:0.008, val_acc:0.980]
Epoch [112/120    avg_loss:0.006, val_acc:0.980]
Epoch [113/120    avg_loss:0.008, val_acc:0.980]
Epoch [114/120    avg_loss:0.008, val_acc:0.980]
Epoch [115/120    avg_loss:0.006, val_acc:0.982]
Epoch [116/120    avg_loss:0.006, val_acc:0.980]
Epoch [117/120    avg_loss:0.006, val_acc:0.982]
Epoch [118/120    avg_loss:0.007, val_acc:0.982]
Epoch [119/120    avg_loss:0.006, val_acc:0.980]
Epoch [120/120    avg_loss:0.006, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1255    2    0    0    0    0    0    0    7   18    2    0
     0    1    0]
 [   0    0    0  721    0   17    0    0    0    2    1    3    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    1   23    0    3    0    0    0    0  824   14    6    0
     0    4    0]
 [   0    0    0    0    0    0    3    0    0    0   19 2188    0    0
     0    0    0]
 [   0    0    0    9    5    2    0    0    0    0    0   20  495    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    3    1    1    0    0    0
  1130    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    70  276    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.26829268292683

F1 scores:
[       nan 1.         0.98780008 0.96005326 0.98839907 0.97098214
 0.99620349 1.         0.99652375 0.89473684 0.95425594 0.98248765
 0.94918504 1.         0.96622488 0.87758347 0.97619048]

Kappa:
0.9688255800658794
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9ba37337b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.347, val_acc:0.395]
Epoch [2/120    avg_loss:1.845, val_acc:0.575]
Epoch [3/120    avg_loss:1.608, val_acc:0.659]
Epoch [4/120    avg_loss:1.299, val_acc:0.650]
Epoch [5/120    avg_loss:1.095, val_acc:0.678]
Epoch [6/120    avg_loss:0.959, val_acc:0.696]
Epoch [7/120    avg_loss:0.851, val_acc:0.755]
Epoch [8/120    avg_loss:0.749, val_acc:0.771]
Epoch [9/120    avg_loss:0.652, val_acc:0.746]
Epoch [10/120    avg_loss:0.659, val_acc:0.750]
Epoch [11/120    avg_loss:0.563, val_acc:0.807]
Epoch [12/120    avg_loss:0.477, val_acc:0.799]
Epoch [13/120    avg_loss:0.486, val_acc:0.841]
Epoch [14/120    avg_loss:0.435, val_acc:0.854]
Epoch [15/120    avg_loss:0.414, val_acc:0.793]
Epoch [16/120    avg_loss:0.359, val_acc:0.875]
Epoch [17/120    avg_loss:0.529, val_acc:0.842]
Epoch [18/120    avg_loss:0.335, val_acc:0.887]
Epoch [19/120    avg_loss:0.402, val_acc:0.860]
Epoch [20/120    avg_loss:0.368, val_acc:0.885]
Epoch [21/120    avg_loss:0.356, val_acc:0.910]
Epoch [22/120    avg_loss:0.278, val_acc:0.826]
Epoch [23/120    avg_loss:0.273, val_acc:0.912]
Epoch [24/120    avg_loss:0.277, val_acc:0.893]
Epoch [25/120    avg_loss:0.202, val_acc:0.904]
Epoch [26/120    avg_loss:0.139, val_acc:0.913]
Epoch [27/120    avg_loss:0.179, val_acc:0.939]
Epoch [28/120    avg_loss:0.165, val_acc:0.935]
Epoch [29/120    avg_loss:0.212, val_acc:0.895]
Epoch [30/120    avg_loss:0.164, val_acc:0.937]
Epoch [31/120    avg_loss:0.154, val_acc:0.954]
Epoch [32/120    avg_loss:0.129, val_acc:0.939]
Epoch [33/120    avg_loss:0.110, val_acc:0.924]
Epoch [34/120    avg_loss:0.117, val_acc:0.943]
Epoch [35/120    avg_loss:0.121, val_acc:0.945]
Epoch [36/120    avg_loss:0.114, val_acc:0.900]
Epoch [37/120    avg_loss:0.148, val_acc:0.914]
Epoch [38/120    avg_loss:0.158, val_acc:0.946]
Epoch [39/120    avg_loss:0.109, val_acc:0.948]
Epoch [40/120    avg_loss:0.081, val_acc:0.958]
Epoch [41/120    avg_loss:0.093, val_acc:0.954]
Epoch [42/120    avg_loss:0.121, val_acc:0.932]
Epoch [43/120    avg_loss:0.118, val_acc:0.945]
Epoch [44/120    avg_loss:0.103, val_acc:0.949]
Epoch [45/120    avg_loss:0.081, val_acc:0.963]
Epoch [46/120    avg_loss:0.076, val_acc:0.959]
Epoch [47/120    avg_loss:0.079, val_acc:0.960]
Epoch [48/120    avg_loss:0.076, val_acc:0.959]
Epoch [49/120    avg_loss:0.072, val_acc:0.965]
Epoch [50/120    avg_loss:0.101, val_acc:0.950]
Epoch [51/120    avg_loss:0.175, val_acc:0.921]
Epoch [52/120    avg_loss:0.171, val_acc:0.935]
Epoch [53/120    avg_loss:0.101, val_acc:0.942]
Epoch [54/120    avg_loss:0.111, val_acc:0.955]
Epoch [55/120    avg_loss:0.090, val_acc:0.943]
Epoch [56/120    avg_loss:0.052, val_acc:0.963]
Epoch [57/120    avg_loss:0.041, val_acc:0.946]
Epoch [58/120    avg_loss:0.089, val_acc:0.948]
Epoch [59/120    avg_loss:0.414, val_acc:0.807]
Epoch [60/120    avg_loss:0.421, val_acc:0.882]
Epoch [61/120    avg_loss:0.131, val_acc:0.943]
Epoch [62/120    avg_loss:0.087, val_acc:0.953]
Epoch [63/120    avg_loss:0.058, val_acc:0.958]
Epoch [64/120    avg_loss:0.061, val_acc:0.959]
Epoch [65/120    avg_loss:0.055, val_acc:0.964]
Epoch [66/120    avg_loss:0.050, val_acc:0.963]
Epoch [67/120    avg_loss:0.051, val_acc:0.965]
Epoch [68/120    avg_loss:0.044, val_acc:0.968]
Epoch [69/120    avg_loss:0.039, val_acc:0.965]
Epoch [70/120    avg_loss:0.045, val_acc:0.966]
Epoch [71/120    avg_loss:0.047, val_acc:0.966]
Epoch [72/120    avg_loss:0.042, val_acc:0.965]
Epoch [73/120    avg_loss:0.041, val_acc:0.971]
Epoch [74/120    avg_loss:0.034, val_acc:0.973]
Epoch [75/120    avg_loss:0.041, val_acc:0.970]
Epoch [76/120    avg_loss:0.040, val_acc:0.972]
Epoch [77/120    avg_loss:0.031, val_acc:0.972]
Epoch [78/120    avg_loss:0.032, val_acc:0.974]
Epoch [79/120    avg_loss:0.033, val_acc:0.971]
Epoch [80/120    avg_loss:0.038, val_acc:0.967]
Epoch [81/120    avg_loss:0.041, val_acc:0.974]
Epoch [82/120    avg_loss:0.031, val_acc:0.970]
Epoch [83/120    avg_loss:0.034, val_acc:0.974]
Epoch [84/120    avg_loss:0.037, val_acc:0.972]
Epoch [85/120    avg_loss:0.030, val_acc:0.977]
Epoch [86/120    avg_loss:0.031, val_acc:0.973]
Epoch [87/120    avg_loss:0.037, val_acc:0.975]
Epoch [88/120    avg_loss:0.029, val_acc:0.975]
Epoch [89/120    avg_loss:0.031, val_acc:0.975]
Epoch [90/120    avg_loss:0.031, val_acc:0.974]
Epoch [91/120    avg_loss:0.030, val_acc:0.976]
Epoch [92/120    avg_loss:0.029, val_acc:0.976]
Epoch [93/120    avg_loss:0.030, val_acc:0.975]
Epoch [94/120    avg_loss:0.027, val_acc:0.976]
Epoch [95/120    avg_loss:0.029, val_acc:0.978]
Epoch [96/120    avg_loss:0.032, val_acc:0.976]
Epoch [97/120    avg_loss:0.030, val_acc:0.977]
Epoch [98/120    avg_loss:0.031, val_acc:0.976]
Epoch [99/120    avg_loss:0.030, val_acc:0.974]
Epoch [100/120    avg_loss:0.030, val_acc:0.976]
Epoch [101/120    avg_loss:0.030, val_acc:0.975]
Epoch [102/120    avg_loss:0.028, val_acc:0.974]
Epoch [103/120    avg_loss:0.028, val_acc:0.974]
Epoch [104/120    avg_loss:0.025, val_acc:0.974]
Epoch [105/120    avg_loss:0.024, val_acc:0.975]
Epoch [106/120    avg_loss:0.023, val_acc:0.976]
Epoch [107/120    avg_loss:0.028, val_acc:0.976]
Epoch [108/120    avg_loss:0.025, val_acc:0.976]
Epoch [109/120    avg_loss:0.021, val_acc:0.976]
Epoch [110/120    avg_loss:0.023, val_acc:0.976]
Epoch [111/120    avg_loss:0.024, val_acc:0.976]
Epoch [112/120    avg_loss:0.021, val_acc:0.976]
Epoch [113/120    avg_loss:0.024, val_acc:0.976]
Epoch [114/120    avg_loss:0.025, val_acc:0.976]
Epoch [115/120    avg_loss:0.021, val_acc:0.975]
Epoch [116/120    avg_loss:0.029, val_acc:0.976]
Epoch [117/120    avg_loss:0.025, val_acc:0.976]
Epoch [118/120    avg_loss:0.023, val_acc:0.975]
Epoch [119/120    avg_loss:0.023, val_acc:0.975]
Epoch [120/120    avg_loss:0.024, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1250    2    0    0    0    0    0    0    5   28    0    0
     0    0    0]
 [   0    0    0  707    0   19    0    0    0    2    2    0   15    2
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    2    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   13   32    0    4    0    0    0    0  811    9    0    0
     0    6    0]
 [   0    0   14    0    0    1    3    0    0    0   17 2170    0    2
     3    0    0]
 [   0    0    0    1    4    4    0    0    0    0    0    9  510    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    1    0    0
  1137    0    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
   112  231    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.36856368563686

F1 scores:
[       nan 0.94871795 0.97580016 0.94899329 0.98834499 0.96651786
 0.98934551 0.98039216 1.         0.94736842 0.94577259 0.97968397
 0.9631728  0.98930481 0.9490818  0.79109589 0.96551724]

Kappa:
0.9585523950100295
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8340ae9780>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.391, val_acc:0.469]
Epoch [2/120    avg_loss:1.843, val_acc:0.540]
Epoch [3/120    avg_loss:1.599, val_acc:0.588]
Epoch [4/120    avg_loss:1.441, val_acc:0.670]
Epoch [5/120    avg_loss:1.117, val_acc:0.746]
Epoch [6/120    avg_loss:0.974, val_acc:0.779]
Epoch [7/120    avg_loss:0.887, val_acc:0.793]
Epoch [8/120    avg_loss:0.717, val_acc:0.825]
Epoch [9/120    avg_loss:0.674, val_acc:0.769]
Epoch [10/120    avg_loss:0.620, val_acc:0.800]
Epoch [11/120    avg_loss:0.581, val_acc:0.813]
Epoch [12/120    avg_loss:0.604, val_acc:0.845]
Epoch [13/120    avg_loss:0.552, val_acc:0.847]
Epoch [14/120    avg_loss:0.437, val_acc:0.872]
Epoch [15/120    avg_loss:0.381, val_acc:0.869]
Epoch [16/120    avg_loss:0.426, val_acc:0.805]
Epoch [17/120    avg_loss:0.341, val_acc:0.884]
Epoch [18/120    avg_loss:0.409, val_acc:0.873]
Epoch [19/120    avg_loss:0.344, val_acc:0.881]
Epoch [20/120    avg_loss:0.598, val_acc:0.903]
Epoch [21/120    avg_loss:0.290, val_acc:0.903]
Epoch [22/120    avg_loss:0.317, val_acc:0.848]
Epoch [23/120    avg_loss:0.217, val_acc:0.906]
Epoch [24/120    avg_loss:0.207, val_acc:0.902]
Epoch [25/120    avg_loss:0.201, val_acc:0.907]
Epoch [26/120    avg_loss:0.155, val_acc:0.937]
Epoch [27/120    avg_loss:0.167, val_acc:0.929]
Epoch [28/120    avg_loss:0.191, val_acc:0.917]
Epoch [29/120    avg_loss:0.207, val_acc:0.913]
Epoch [30/120    avg_loss:0.260, val_acc:0.869]
Epoch [31/120    avg_loss:0.248, val_acc:0.916]
Epoch [32/120    avg_loss:0.138, val_acc:0.929]
Epoch [33/120    avg_loss:0.113, val_acc:0.940]
Epoch [34/120    avg_loss:0.115, val_acc:0.942]
Epoch [35/120    avg_loss:0.084, val_acc:0.936]
Epoch [36/120    avg_loss:0.175, val_acc:0.909]
Epoch [37/120    avg_loss:0.151, val_acc:0.930]
Epoch [38/120    avg_loss:0.120, val_acc:0.948]
Epoch [39/120    avg_loss:0.091, val_acc:0.941]
Epoch [40/120    avg_loss:0.097, val_acc:0.938]
Epoch [41/120    avg_loss:0.088, val_acc:0.959]
Epoch [42/120    avg_loss:0.105, val_acc:0.939]
Epoch [43/120    avg_loss:0.112, val_acc:0.940]
Epoch [44/120    avg_loss:0.118, val_acc:0.942]
Epoch [45/120    avg_loss:0.076, val_acc:0.948]
Epoch [46/120    avg_loss:0.073, val_acc:0.945]
Epoch [47/120    avg_loss:0.051, val_acc:0.959]
Epoch [48/120    avg_loss:0.114, val_acc:0.939]
Epoch [49/120    avg_loss:0.108, val_acc:0.952]
Epoch [50/120    avg_loss:0.064, val_acc:0.959]
Epoch [51/120    avg_loss:0.076, val_acc:0.965]
Epoch [52/120    avg_loss:0.072, val_acc:0.944]
Epoch [53/120    avg_loss:0.068, val_acc:0.884]
Epoch [54/120    avg_loss:0.052, val_acc:0.956]
Epoch [55/120    avg_loss:0.059, val_acc:0.944]
Epoch [56/120    avg_loss:0.047, val_acc:0.975]
Epoch [57/120    avg_loss:0.056, val_acc:0.958]
Epoch [58/120    avg_loss:0.044, val_acc:0.967]
Epoch [59/120    avg_loss:0.038, val_acc:0.968]
Epoch [60/120    avg_loss:0.029, val_acc:0.963]
Epoch [61/120    avg_loss:0.029, val_acc:0.967]
Epoch [62/120    avg_loss:0.024, val_acc:0.961]
Epoch [63/120    avg_loss:0.037, val_acc:0.975]
Epoch [64/120    avg_loss:0.038, val_acc:0.965]
Epoch [65/120    avg_loss:0.475, val_acc:0.819]
Epoch [66/120    avg_loss:0.168, val_acc:0.930]
Epoch [67/120    avg_loss:0.189, val_acc:0.930]
Epoch [68/120    avg_loss:0.203, val_acc:0.836]
Epoch [69/120    avg_loss:0.242, val_acc:0.909]
Epoch [70/120    avg_loss:0.112, val_acc:0.950]
Epoch [71/120    avg_loss:0.121, val_acc:0.954]
Epoch [72/120    avg_loss:0.060, val_acc:0.965]
Epoch [73/120    avg_loss:0.096, val_acc:0.951]
Epoch [74/120    avg_loss:0.055, val_acc:0.957]
Epoch [75/120    avg_loss:0.052, val_acc:0.960]
Epoch [76/120    avg_loss:0.040, val_acc:0.971]
Epoch [77/120    avg_loss:0.028, val_acc:0.970]
Epoch [78/120    avg_loss:0.021, val_acc:0.978]
Epoch [79/120    avg_loss:0.030, val_acc:0.973]
Epoch [80/120    avg_loss:0.019, val_acc:0.978]
Epoch [81/120    avg_loss:0.017, val_acc:0.977]
Epoch [82/120    avg_loss:0.022, val_acc:0.976]
Epoch [83/120    avg_loss:0.017, val_acc:0.975]
Epoch [84/120    avg_loss:0.020, val_acc:0.975]
Epoch [85/120    avg_loss:0.017, val_acc:0.976]
Epoch [86/120    avg_loss:0.018, val_acc:0.973]
Epoch [87/120    avg_loss:0.017, val_acc:0.975]
Epoch [88/120    avg_loss:0.017, val_acc:0.973]
Epoch [89/120    avg_loss:0.020, val_acc:0.975]
Epoch [90/120    avg_loss:0.018, val_acc:0.973]
Epoch [91/120    avg_loss:0.019, val_acc:0.976]
Epoch [92/120    avg_loss:0.027, val_acc:0.975]
Epoch [93/120    avg_loss:0.018, val_acc:0.970]
Epoch [94/120    avg_loss:0.016, val_acc:0.971]
Epoch [95/120    avg_loss:0.019, val_acc:0.971]
Epoch [96/120    avg_loss:0.017, val_acc:0.972]
Epoch [97/120    avg_loss:0.015, val_acc:0.972]
Epoch [98/120    avg_loss:0.017, val_acc:0.972]
Epoch [99/120    avg_loss:0.016, val_acc:0.972]
Epoch [100/120    avg_loss:0.020, val_acc:0.972]
Epoch [101/120    avg_loss:0.015, val_acc:0.972]
Epoch [102/120    avg_loss:0.016, val_acc:0.972]
Epoch [103/120    avg_loss:0.016, val_acc:0.972]
Epoch [104/120    avg_loss:0.014, val_acc:0.972]
Epoch [105/120    avg_loss:0.017, val_acc:0.972]
Epoch [106/120    avg_loss:0.014, val_acc:0.972]
Epoch [107/120    avg_loss:0.021, val_acc:0.972]
Epoch [108/120    avg_loss:0.019, val_acc:0.972]
Epoch [109/120    avg_loss:0.015, val_acc:0.972]
Epoch [110/120    avg_loss:0.017, val_acc:0.972]
Epoch [111/120    avg_loss:0.014, val_acc:0.972]
Epoch [112/120    avg_loss:0.019, val_acc:0.972]
Epoch [113/120    avg_loss:0.016, val_acc:0.972]
Epoch [114/120    avg_loss:0.016, val_acc:0.972]
Epoch [115/120    avg_loss:0.015, val_acc:0.972]
Epoch [116/120    avg_loss:0.017, val_acc:0.972]
Epoch [117/120    avg_loss:0.017, val_acc:0.972]
Epoch [118/120    avg_loss:0.022, val_acc:0.972]
Epoch [119/120    avg_loss:0.017, val_acc:0.972]
Epoch [120/120    avg_loss:0.020, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1261    7    0    0    2    0    0    0    2   12    1    0
     0    0    0]
 [   0    0    0  719    3   10    0    0    0    6    3    0    3    2
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1   33    0    3    0    0    0    0  822   12    0    0
     3    1    0]
 [   0    0    5    0    0    1    7    0    0    0   11 2181    0    1
     4    0    0]
 [   0    0    0   16   30    4    0    0    0    0    9    0  472    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    0    0    0
  1136    0    0]
 [   0    0    1    0    0    0    0    0    0    0    0    0    0    0
    38  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.3550135501355

F1 scores:
[       nan 0.98765432 0.98785742 0.94480946 0.92810458 0.97627119
 0.99319728 1.         1.         0.8        0.95249131 0.98799547
 0.93280632 0.9919571  0.97888841 0.93902439 0.9704142 ]

Kappa:
0.9698475260564247
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa2d42d3828>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.352, val_acc:0.530]
Epoch [2/120    avg_loss:1.790, val_acc:0.560]
Epoch [3/120    avg_loss:1.503, val_acc:0.585]
Epoch [4/120    avg_loss:1.379, val_acc:0.636]
Epoch [5/120    avg_loss:1.112, val_acc:0.690]
Epoch [6/120    avg_loss:0.994, val_acc:0.676]
Epoch [7/120    avg_loss:0.910, val_acc:0.714]
Epoch [8/120    avg_loss:0.746, val_acc:0.722]
Epoch [9/120    avg_loss:0.799, val_acc:0.722]
Epoch [10/120    avg_loss:0.670, val_acc:0.771]
Epoch [11/120    avg_loss:0.583, val_acc:0.732]
Epoch [12/120    avg_loss:0.603, val_acc:0.821]
Epoch [13/120    avg_loss:0.537, val_acc:0.807]
Epoch [14/120    avg_loss:0.470, val_acc:0.830]
Epoch [15/120    avg_loss:0.399, val_acc:0.845]
Epoch [16/120    avg_loss:0.447, val_acc:0.847]
Epoch [17/120    avg_loss:0.439, val_acc:0.811]
Epoch [18/120    avg_loss:0.358, val_acc:0.887]
Epoch [19/120    avg_loss:0.284, val_acc:0.871]
Epoch [20/120    avg_loss:0.303, val_acc:0.898]
Epoch [21/120    avg_loss:0.419, val_acc:0.812]
Epoch [22/120    avg_loss:0.371, val_acc:0.860]
Epoch [23/120    avg_loss:0.283, val_acc:0.890]
Epoch [24/120    avg_loss:0.266, val_acc:0.889]
Epoch [25/120    avg_loss:0.286, val_acc:0.857]
Epoch [26/120    avg_loss:0.317, val_acc:0.877]
Epoch [27/120    avg_loss:0.237, val_acc:0.892]
Epoch [28/120    avg_loss:0.194, val_acc:0.913]
Epoch [29/120    avg_loss:0.177, val_acc:0.864]
Epoch [30/120    avg_loss:0.254, val_acc:0.899]
Epoch [31/120    avg_loss:0.208, val_acc:0.921]
Epoch [32/120    avg_loss:0.154, val_acc:0.925]
Epoch [33/120    avg_loss:0.155, val_acc:0.855]
Epoch [34/120    avg_loss:0.203, val_acc:0.901]
Epoch [35/120    avg_loss:0.221, val_acc:0.872]
Epoch [36/120    avg_loss:0.126, val_acc:0.923]
Epoch [37/120    avg_loss:0.118, val_acc:0.928]
Epoch [38/120    avg_loss:0.170, val_acc:0.910]
Epoch [39/120    avg_loss:0.129, val_acc:0.924]
Epoch [40/120    avg_loss:0.130, val_acc:0.930]
Epoch [41/120    avg_loss:0.103, val_acc:0.930]
Epoch [42/120    avg_loss:0.133, val_acc:0.888]
Epoch [43/120    avg_loss:0.123, val_acc:0.924]
Epoch [44/120    avg_loss:0.103, val_acc:0.933]
Epoch [45/120    avg_loss:0.121, val_acc:0.930]
Epoch [46/120    avg_loss:0.121, val_acc:0.921]
Epoch [47/120    avg_loss:0.105, val_acc:0.927]
Epoch [48/120    avg_loss:0.127, val_acc:0.920]
Epoch [49/120    avg_loss:0.137, val_acc:0.933]
Epoch [50/120    avg_loss:0.089, val_acc:0.930]
Epoch [51/120    avg_loss:0.087, val_acc:0.916]
Epoch [52/120    avg_loss:0.062, val_acc:0.941]
Epoch [53/120    avg_loss:0.063, val_acc:0.948]
Epoch [54/120    avg_loss:0.076, val_acc:0.943]
Epoch [55/120    avg_loss:0.063, val_acc:0.941]
Epoch [56/120    avg_loss:0.045, val_acc:0.942]
Epoch [57/120    avg_loss:0.051, val_acc:0.940]
Epoch [58/120    avg_loss:0.036, val_acc:0.949]
Epoch [59/120    avg_loss:0.063, val_acc:0.933]
Epoch [60/120    avg_loss:0.072, val_acc:0.940]
Epoch [61/120    avg_loss:0.076, val_acc:0.942]
Epoch [62/120    avg_loss:0.051, val_acc:0.943]
Epoch [63/120    avg_loss:0.071, val_acc:0.950]
Epoch [64/120    avg_loss:0.059, val_acc:0.953]
Epoch [65/120    avg_loss:0.073, val_acc:0.934]
Epoch [66/120    avg_loss:0.089, val_acc:0.949]
Epoch [67/120    avg_loss:0.063, val_acc:0.954]
Epoch [68/120    avg_loss:0.050, val_acc:0.951]
Epoch [69/120    avg_loss:0.033, val_acc:0.961]
Epoch [70/120    avg_loss:0.041, val_acc:0.963]
Epoch [71/120    avg_loss:0.057, val_acc:0.955]
Epoch [72/120    avg_loss:0.029, val_acc:0.958]
Epoch [73/120    avg_loss:0.050, val_acc:0.948]
Epoch [74/120    avg_loss:0.046, val_acc:0.962]
Epoch [75/120    avg_loss:0.076, val_acc:0.873]
Epoch [76/120    avg_loss:0.076, val_acc:0.946]
Epoch [77/120    avg_loss:0.042, val_acc:0.959]
Epoch [78/120    avg_loss:0.037, val_acc:0.955]
Epoch [79/120    avg_loss:0.061, val_acc:0.958]
Epoch [80/120    avg_loss:0.029, val_acc:0.957]
Epoch [81/120    avg_loss:0.033, val_acc:0.955]
Epoch [82/120    avg_loss:0.039, val_acc:0.933]
Epoch [83/120    avg_loss:0.046, val_acc:0.954]
Epoch [84/120    avg_loss:0.040, val_acc:0.962]
Epoch [85/120    avg_loss:0.033, val_acc:0.960]
Epoch [86/120    avg_loss:0.019, val_acc:0.961]
Epoch [87/120    avg_loss:0.018, val_acc:0.963]
Epoch [88/120    avg_loss:0.016, val_acc:0.965]
Epoch [89/120    avg_loss:0.017, val_acc:0.964]
Epoch [90/120    avg_loss:0.016, val_acc:0.963]
Epoch [91/120    avg_loss:0.017, val_acc:0.962]
Epoch [92/120    avg_loss:0.014, val_acc:0.965]
Epoch [93/120    avg_loss:0.023, val_acc:0.966]
Epoch [94/120    avg_loss:0.013, val_acc:0.967]
Epoch [95/120    avg_loss:0.013, val_acc:0.966]
Epoch [96/120    avg_loss:0.012, val_acc:0.966]
Epoch [97/120    avg_loss:0.014, val_acc:0.965]
Epoch [98/120    avg_loss:0.013, val_acc:0.965]
Epoch [99/120    avg_loss:0.011, val_acc:0.965]
Epoch [100/120    avg_loss:0.011, val_acc:0.967]
Epoch [101/120    avg_loss:0.011, val_acc:0.965]
Epoch [102/120    avg_loss:0.012, val_acc:0.965]
Epoch [103/120    avg_loss:0.015, val_acc:0.967]
Epoch [104/120    avg_loss:0.012, val_acc:0.968]
Epoch [105/120    avg_loss:0.011, val_acc:0.966]
Epoch [106/120    avg_loss:0.010, val_acc:0.967]
Epoch [107/120    avg_loss:0.012, val_acc:0.968]
Epoch [108/120    avg_loss:0.009, val_acc:0.968]
Epoch [109/120    avg_loss:0.011, val_acc:0.970]
Epoch [110/120    avg_loss:0.014, val_acc:0.968]
Epoch [111/120    avg_loss:0.013, val_acc:0.968]
Epoch [112/120    avg_loss:0.012, val_acc:0.966]
Epoch [113/120    avg_loss:0.015, val_acc:0.967]
Epoch [114/120    avg_loss:0.008, val_acc:0.970]
Epoch [115/120    avg_loss:0.014, val_acc:0.970]
Epoch [116/120    avg_loss:0.011, val_acc:0.970]
Epoch [117/120    avg_loss:0.010, val_acc:0.970]
Epoch [118/120    avg_loss:0.010, val_acc:0.970]
Epoch [119/120    avg_loss:0.010, val_acc:0.970]
Epoch [120/120    avg_loss:0.010, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1261    3    0    2    0    0    0    1    4    8    2    0
     0    4    0]
 [   0    0    0  720    0   17    0    0    0    3    1    0    4    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   13    0    0    2    0
     0    0    0]
 [   0    0    2   22    0    8    3    0    0    0  824   16    0    0
     0    0    0]
 [   0    0    3    0    0    1    3    0    0    0   10 2191    0    2
     0    0    0]
 [   0    0    0   13    6   13    0    0    0    0    2   15  483    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    3    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    46  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.4308943089431

F1 scores:
[       nan 0.93506494 0.98863191 0.95490716 0.98611111 0.95060373
 0.99393939 1.         1.         0.7027027  0.95591647 0.9867147
 0.94243902 0.98930481 0.97800776 0.92165899 0.98823529]

Kappa:
0.9706932565408833
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:02:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8c270557b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.326, val_acc:0.455]
Epoch [2/120    avg_loss:1.856, val_acc:0.572]
Epoch [3/120    avg_loss:1.575, val_acc:0.642]
Epoch [4/120    avg_loss:1.321, val_acc:0.634]
Epoch [5/120    avg_loss:1.107, val_acc:0.727]
Epoch [6/120    avg_loss:0.927, val_acc:0.762]
Epoch [7/120    avg_loss:0.828, val_acc:0.765]
Epoch [8/120    avg_loss:0.709, val_acc:0.797]
Epoch [9/120    avg_loss:0.721, val_acc:0.795]
Epoch [10/120    avg_loss:0.607, val_acc:0.786]
Epoch [11/120    avg_loss:0.659, val_acc:0.816]
Epoch [12/120    avg_loss:0.668, val_acc:0.835]
Epoch [13/120    avg_loss:0.612, val_acc:0.848]
Epoch [14/120    avg_loss:0.462, val_acc:0.841]
Epoch [15/120    avg_loss:0.360, val_acc:0.845]
Epoch [16/120    avg_loss:0.407, val_acc:0.797]
Epoch [17/120    avg_loss:0.383, val_acc:0.877]
Epoch [18/120    avg_loss:0.377, val_acc:0.889]
Epoch [19/120    avg_loss:0.420, val_acc:0.913]
Epoch [20/120    avg_loss:0.319, val_acc:0.893]
Epoch [21/120    avg_loss:0.295, val_acc:0.889]
Epoch [22/120    avg_loss:0.274, val_acc:0.913]
Epoch [23/120    avg_loss:0.215, val_acc:0.915]
Epoch [24/120    avg_loss:0.188, val_acc:0.911]
Epoch [25/120    avg_loss:0.216, val_acc:0.920]
Epoch [26/120    avg_loss:0.278, val_acc:0.884]
Epoch [27/120    avg_loss:0.223, val_acc:0.891]
Epoch [28/120    avg_loss:0.211, val_acc:0.918]
Epoch [29/120    avg_loss:0.157, val_acc:0.949]
Epoch [30/120    avg_loss:0.172, val_acc:0.931]
Epoch [31/120    avg_loss:0.191, val_acc:0.922]
Epoch [32/120    avg_loss:0.117, val_acc:0.940]
Epoch [33/120    avg_loss:0.152, val_acc:0.929]
Epoch [34/120    avg_loss:0.143, val_acc:0.940]
Epoch [35/120    avg_loss:0.121, val_acc:0.943]
Epoch [36/120    avg_loss:0.103, val_acc:0.935]
Epoch [37/120    avg_loss:0.097, val_acc:0.947]
Epoch [38/120    avg_loss:0.127, val_acc:0.947]
Epoch [39/120    avg_loss:0.126, val_acc:0.903]
Epoch [40/120    avg_loss:0.119, val_acc:0.938]
Epoch [41/120    avg_loss:0.095, val_acc:0.944]
Epoch [42/120    avg_loss:0.070, val_acc:0.969]
Epoch [43/120    avg_loss:0.083, val_acc:0.949]
Epoch [44/120    avg_loss:0.067, val_acc:0.971]
Epoch [45/120    avg_loss:0.086, val_acc:0.966]
Epoch [46/120    avg_loss:0.084, val_acc:0.956]
Epoch [47/120    avg_loss:0.078, val_acc:0.963]
Epoch [48/120    avg_loss:0.066, val_acc:0.956]
Epoch [49/120    avg_loss:0.064, val_acc:0.963]
Epoch [50/120    avg_loss:0.099, val_acc:0.957]
Epoch [51/120    avg_loss:0.050, val_acc:0.969]
Epoch [52/120    avg_loss:0.034, val_acc:0.969]
Epoch [53/120    avg_loss:0.067, val_acc:0.952]
Epoch [54/120    avg_loss:0.044, val_acc:0.962]
Epoch [55/120    avg_loss:0.052, val_acc:0.957]
Epoch [56/120    avg_loss:0.044, val_acc:0.971]
Epoch [57/120    avg_loss:0.057, val_acc:0.953]
Epoch [58/120    avg_loss:0.101, val_acc:0.944]
Epoch [59/120    avg_loss:0.067, val_acc:0.948]
Epoch [60/120    avg_loss:0.042, val_acc:0.963]
Epoch [61/120    avg_loss:0.116, val_acc:0.956]
Epoch [62/120    avg_loss:0.091, val_acc:0.963]
Epoch [63/120    avg_loss:0.047, val_acc:0.971]
Epoch [64/120    avg_loss:0.041, val_acc:0.972]
Epoch [65/120    avg_loss:0.030, val_acc:0.974]
Epoch [66/120    avg_loss:0.036, val_acc:0.975]
Epoch [67/120    avg_loss:0.030, val_acc:0.974]
Epoch [68/120    avg_loss:0.019, val_acc:0.971]
Epoch [69/120    avg_loss:0.019, val_acc:0.973]
Epoch [70/120    avg_loss:0.021, val_acc:0.982]
Epoch [71/120    avg_loss:0.016, val_acc:0.976]
Epoch [72/120    avg_loss:0.024, val_acc:0.972]
Epoch [73/120    avg_loss:0.028, val_acc:0.977]
Epoch [74/120    avg_loss:0.026, val_acc:0.973]
Epoch [75/120    avg_loss:0.021, val_acc:0.981]
Epoch [76/120    avg_loss:0.033, val_acc:0.975]
Epoch [77/120    avg_loss:0.064, val_acc:0.950]
Epoch [78/120    avg_loss:0.060, val_acc:0.972]
Epoch [79/120    avg_loss:0.032, val_acc:0.982]
Epoch [80/120    avg_loss:0.024, val_acc:0.983]
Epoch [81/120    avg_loss:0.015, val_acc:0.983]
Epoch [82/120    avg_loss:0.017, val_acc:0.977]
Epoch [83/120    avg_loss:0.020, val_acc:0.980]
Epoch [84/120    avg_loss:0.015, val_acc:0.982]
Epoch [85/120    avg_loss:0.020, val_acc:0.963]
Epoch [86/120    avg_loss:0.017, val_acc:0.971]
Epoch [87/120    avg_loss:0.029, val_acc:0.976]
Epoch [88/120    avg_loss:0.017, val_acc:0.981]
Epoch [89/120    avg_loss:0.018, val_acc:0.976]
Epoch [90/120    avg_loss:0.019, val_acc:0.976]
Epoch [91/120    avg_loss:0.022, val_acc:0.981]
Epoch [92/120    avg_loss:0.020, val_acc:0.982]
Epoch [93/120    avg_loss:0.020, val_acc:0.971]
Epoch [94/120    avg_loss:0.020, val_acc:0.970]
Epoch [95/120    avg_loss:0.022, val_acc:0.980]
Epoch [96/120    avg_loss:0.014, val_acc:0.982]
Epoch [97/120    avg_loss:0.010, val_acc:0.985]
Epoch [98/120    avg_loss:0.010, val_acc:0.982]
Epoch [99/120    avg_loss:0.007, val_acc:0.984]
Epoch [100/120    avg_loss:0.007, val_acc:0.984]
Epoch [101/120    avg_loss:0.007, val_acc:0.984]
Epoch [102/120    avg_loss:0.007, val_acc:0.984]
Epoch [103/120    avg_loss:0.008, val_acc:0.985]
Epoch [104/120    avg_loss:0.009, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.984]
Epoch [106/120    avg_loss:0.008, val_acc:0.985]
Epoch [107/120    avg_loss:0.006, val_acc:0.985]
Epoch [108/120    avg_loss:0.006, val_acc:0.985]
Epoch [109/120    avg_loss:0.007, val_acc:0.985]
Epoch [110/120    avg_loss:0.007, val_acc:0.984]
Epoch [111/120    avg_loss:0.008, val_acc:0.985]
Epoch [112/120    avg_loss:0.008, val_acc:0.985]
Epoch [113/120    avg_loss:0.007, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.009, val_acc:0.987]
Epoch [116/120    avg_loss:0.008, val_acc:0.987]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.006, val_acc:0.985]
Epoch [119/120    avg_loss:0.006, val_acc:0.985]
Epoch [120/120    avg_loss:0.006, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1263    5    1    0    1    0    0    0    7    8    0    0
     0    0    0]
 [   0    0    0  730    0   13    0    0    0    2    1    0    0    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   11    9    0    1    0    0    0    0  812   36    0    0
     2    4    0]
 [   0    0    3    0    0    0    2    0    1    0    8 2194    0    1
     1    0    0]
 [   0    0    0   12    6    0    0    0    0    0    0    2  511    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1134    3    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    49  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.70189701897019

F1 scores:
[       nan 0.98765432 0.98594848 0.97139055 0.98383372 0.97954545
 0.98944193 1.         0.99767981 0.85714286 0.95249267 0.98584588
 0.97705545 0.99462366 0.97548387 0.89719626 0.97647059]

Kappa:
0.973776876740339
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fca8f8247f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.333, val_acc:0.553]
Epoch [2/120    avg_loss:1.859, val_acc:0.566]
Epoch [3/120    avg_loss:1.622, val_acc:0.575]
Epoch [4/120    avg_loss:1.406, val_acc:0.657]
Epoch [5/120    avg_loss:1.169, val_acc:0.711]
Epoch [6/120    avg_loss:0.956, val_acc:0.732]
Epoch [7/120    avg_loss:0.835, val_acc:0.717]
Epoch [8/120    avg_loss:0.762, val_acc:0.747]
Epoch [9/120    avg_loss:0.689, val_acc:0.795]
Epoch [10/120    avg_loss:0.559, val_acc:0.815]
Epoch [11/120    avg_loss:0.605, val_acc:0.814]
Epoch [12/120    avg_loss:0.515, val_acc:0.851]
Epoch [13/120    avg_loss:0.530, val_acc:0.744]
Epoch [14/120    avg_loss:0.551, val_acc:0.829]
Epoch [15/120    avg_loss:0.425, val_acc:0.836]
Epoch [16/120    avg_loss:0.392, val_acc:0.841]
Epoch [17/120    avg_loss:0.325, val_acc:0.881]
Epoch [18/120    avg_loss:0.391, val_acc:0.863]
Epoch [19/120    avg_loss:0.302, val_acc:0.887]
Epoch [20/120    avg_loss:0.303, val_acc:0.884]
Epoch [21/120    avg_loss:0.273, val_acc:0.857]
Epoch [22/120    avg_loss:0.275, val_acc:0.894]
Epoch [23/120    avg_loss:0.194, val_acc:0.917]
Epoch [24/120    avg_loss:0.228, val_acc:0.885]
Epoch [25/120    avg_loss:0.172, val_acc:0.920]
Epoch [26/120    avg_loss:0.219, val_acc:0.885]
Epoch [27/120    avg_loss:0.195, val_acc:0.916]
Epoch [28/120    avg_loss:0.178, val_acc:0.906]
Epoch [29/120    avg_loss:0.309, val_acc:0.878]
Epoch [30/120    avg_loss:0.181, val_acc:0.913]
Epoch [31/120    avg_loss:0.191, val_acc:0.905]
Epoch [32/120    avg_loss:0.148, val_acc:0.925]
Epoch [33/120    avg_loss:0.135, val_acc:0.939]
Epoch [34/120    avg_loss:0.117, val_acc:0.905]
Epoch [35/120    avg_loss:0.100, val_acc:0.946]
Epoch [36/120    avg_loss:0.162, val_acc:0.870]
Epoch [37/120    avg_loss:0.143, val_acc:0.920]
Epoch [38/120    avg_loss:0.136, val_acc:0.927]
Epoch [39/120    avg_loss:0.101, val_acc:0.925]
Epoch [40/120    avg_loss:0.100, val_acc:0.913]
Epoch [41/120    avg_loss:0.075, val_acc:0.931]
Epoch [42/120    avg_loss:0.117, val_acc:0.920]
Epoch [43/120    avg_loss:0.095, val_acc:0.931]
Epoch [44/120    avg_loss:0.119, val_acc:0.943]
Epoch [45/120    avg_loss:0.104, val_acc:0.928]
Epoch [46/120    avg_loss:0.132, val_acc:0.921]
Epoch [47/120    avg_loss:0.116, val_acc:0.928]
Epoch [48/120    avg_loss:0.098, val_acc:0.941]
Epoch [49/120    avg_loss:0.059, val_acc:0.950]
Epoch [50/120    avg_loss:0.049, val_acc:0.955]
Epoch [51/120    avg_loss:0.048, val_acc:0.957]
Epoch [52/120    avg_loss:0.049, val_acc:0.955]
Epoch [53/120    avg_loss:0.041, val_acc:0.955]
Epoch [54/120    avg_loss:0.034, val_acc:0.954]
Epoch [55/120    avg_loss:0.038, val_acc:0.956]
Epoch [56/120    avg_loss:0.036, val_acc:0.956]
Epoch [57/120    avg_loss:0.037, val_acc:0.955]
Epoch [58/120    avg_loss:0.039, val_acc:0.956]
Epoch [59/120    avg_loss:0.039, val_acc:0.958]
Epoch [60/120    avg_loss:0.036, val_acc:0.957]
Epoch [61/120    avg_loss:0.032, val_acc:0.957]
Epoch [62/120    avg_loss:0.037, val_acc:0.957]
Epoch [63/120    avg_loss:0.031, val_acc:0.956]
Epoch [64/120    avg_loss:0.025, val_acc:0.958]
Epoch [65/120    avg_loss:0.035, val_acc:0.955]
Epoch [66/120    avg_loss:0.030, val_acc:0.956]
Epoch [67/120    avg_loss:0.028, val_acc:0.955]
Epoch [68/120    avg_loss:0.033, val_acc:0.957]
Epoch [69/120    avg_loss:0.026, val_acc:0.955]
Epoch [70/120    avg_loss:0.027, val_acc:0.958]
Epoch [71/120    avg_loss:0.028, val_acc:0.957]
Epoch [72/120    avg_loss:0.034, val_acc:0.960]
Epoch [73/120    avg_loss:0.027, val_acc:0.958]
Epoch [74/120    avg_loss:0.023, val_acc:0.959]
Epoch [75/120    avg_loss:0.027, val_acc:0.957]
Epoch [76/120    avg_loss:0.032, val_acc:0.960]
Epoch [77/120    avg_loss:0.026, val_acc:0.957]
Epoch [78/120    avg_loss:0.029, val_acc:0.960]
Epoch [79/120    avg_loss:0.025, val_acc:0.958]
Epoch [80/120    avg_loss:0.022, val_acc:0.960]
Epoch [81/120    avg_loss:0.025, val_acc:0.959]
Epoch [82/120    avg_loss:0.032, val_acc:0.955]
Epoch [83/120    avg_loss:0.027, val_acc:0.959]
Epoch [84/120    avg_loss:0.030, val_acc:0.959]
Epoch [85/120    avg_loss:0.030, val_acc:0.958]
Epoch [86/120    avg_loss:0.023, val_acc:0.961]
Epoch [87/120    avg_loss:0.033, val_acc:0.962]
Epoch [88/120    avg_loss:0.025, val_acc:0.961]
Epoch [89/120    avg_loss:0.022, val_acc:0.960]
Epoch [90/120    avg_loss:0.029, val_acc:0.962]
Epoch [91/120    avg_loss:0.026, val_acc:0.960]
Epoch [92/120    avg_loss:0.022, val_acc:0.961]
Epoch [93/120    avg_loss:0.020, val_acc:0.960]
Epoch [94/120    avg_loss:0.027, val_acc:0.962]
Epoch [95/120    avg_loss:0.025, val_acc:0.959]
Epoch [96/120    avg_loss:0.028, val_acc:0.958]
Epoch [97/120    avg_loss:0.026, val_acc:0.957]
Epoch [98/120    avg_loss:0.026, val_acc:0.957]
Epoch [99/120    avg_loss:0.025, val_acc:0.958]
Epoch [100/120    avg_loss:0.021, val_acc:0.956]
Epoch [101/120    avg_loss:0.023, val_acc:0.959]
Epoch [102/120    avg_loss:0.028, val_acc:0.959]
Epoch [103/120    avg_loss:0.025, val_acc:0.960]
Epoch [104/120    avg_loss:0.018, val_acc:0.960]
Epoch [105/120    avg_loss:0.023, val_acc:0.959]
Epoch [106/120    avg_loss:0.021, val_acc:0.960]
Epoch [107/120    avg_loss:0.023, val_acc:0.962]
Epoch [108/120    avg_loss:0.021, val_acc:0.961]
Epoch [109/120    avg_loss:0.024, val_acc:0.960]
Epoch [110/120    avg_loss:0.024, val_acc:0.961]
Epoch [111/120    avg_loss:0.018, val_acc:0.963]
Epoch [112/120    avg_loss:0.018, val_acc:0.961]
Epoch [113/120    avg_loss:0.021, val_acc:0.962]
Epoch [114/120    avg_loss:0.018, val_acc:0.962]
Epoch [115/120    avg_loss:0.018, val_acc:0.962]
Epoch [116/120    avg_loss:0.024, val_acc:0.962]
Epoch [117/120    avg_loss:0.019, val_acc:0.961]
Epoch [118/120    avg_loss:0.024, val_acc:0.959]
Epoch [119/120    avg_loss:0.024, val_acc:0.962]
Epoch [120/120    avg_loss:0.022, val_acc:0.962]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1257    1    5    0    2    0    0    1    6   12    0    0
     0    1    0]
 [   0    0    0  707    0   12    0    0    0   15    3    2    2    6
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8   42    1    6    0    0    0    0  792   22    3    0
     1    0    0]
 [   0    0   14    0    0    5    2    0    0    0    5 2177    3    4
     0    0    0]
 [   0    0    4   15    4    4    0    0    0    0    0   10  493    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    3    0    3    0    0    0
  1133    0    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    79  249    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.30352303523036

F1 scores:
[       nan 0.94871795 0.97897196 0.93518519 0.97706422 0.96412556
 0.98203593 1.         0.99535963 0.66666667 0.93838863 0.9819576
 0.95081967 0.97368421 0.96220807 0.83417085 0.97076023]

Kappa:
0.9578261086287951
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2a538cc7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.358, val_acc:0.412]
Epoch [2/120    avg_loss:1.854, val_acc:0.583]
Epoch [3/120    avg_loss:1.548, val_acc:0.641]
Epoch [4/120    avg_loss:1.254, val_acc:0.679]
Epoch [5/120    avg_loss:1.129, val_acc:0.672]
Epoch [6/120    avg_loss:0.908, val_acc:0.715]
Epoch [7/120    avg_loss:0.882, val_acc:0.713]
Epoch [8/120    avg_loss:0.776, val_acc:0.779]
Epoch [9/120    avg_loss:0.682, val_acc:0.792]
Epoch [10/120    avg_loss:0.614, val_acc:0.754]
Epoch [11/120    avg_loss:0.671, val_acc:0.770]
Epoch [12/120    avg_loss:0.515, val_acc:0.818]
Epoch [13/120    avg_loss:0.492, val_acc:0.833]
Epoch [14/120    avg_loss:0.453, val_acc:0.788]
Epoch [15/120    avg_loss:0.426, val_acc:0.839]
Epoch [16/120    avg_loss:0.384, val_acc:0.862]
Epoch [17/120    avg_loss:0.440, val_acc:0.789]
Epoch [18/120    avg_loss:0.378, val_acc:0.846]
Epoch [19/120    avg_loss:0.362, val_acc:0.822]
Epoch [20/120    avg_loss:0.394, val_acc:0.837]
Epoch [21/120    avg_loss:0.281, val_acc:0.876]
Epoch [22/120    avg_loss:0.326, val_acc:0.857]
Epoch [23/120    avg_loss:0.301, val_acc:0.876]
Epoch [24/120    avg_loss:0.241, val_acc:0.879]
Epoch [25/120    avg_loss:0.224, val_acc:0.910]
Epoch [26/120    avg_loss:0.211, val_acc:0.886]
Epoch [27/120    avg_loss:0.175, val_acc:0.918]
Epoch [28/120    avg_loss:0.207, val_acc:0.848]
Epoch [29/120    avg_loss:0.232, val_acc:0.901]
Epoch [30/120    avg_loss:0.203, val_acc:0.909]
Epoch [31/120    avg_loss:0.154, val_acc:0.928]
Epoch [32/120    avg_loss:0.104, val_acc:0.914]
Epoch [33/120    avg_loss:0.178, val_acc:0.893]
Epoch [34/120    avg_loss:0.232, val_acc:0.938]
Epoch [35/120    avg_loss:0.154, val_acc:0.855]
Epoch [36/120    avg_loss:0.216, val_acc:0.930]
Epoch [37/120    avg_loss:0.134, val_acc:0.941]
Epoch [38/120    avg_loss:0.107, val_acc:0.927]
Epoch [39/120    avg_loss:0.132, val_acc:0.918]
Epoch [40/120    avg_loss:0.106, val_acc:0.921]
Epoch [41/120    avg_loss:0.218, val_acc:0.852]
Epoch [42/120    avg_loss:0.114, val_acc:0.925]
Epoch [43/120    avg_loss:0.096, val_acc:0.943]
Epoch [44/120    avg_loss:0.073, val_acc:0.943]
Epoch [45/120    avg_loss:0.085, val_acc:0.935]
Epoch [46/120    avg_loss:0.074, val_acc:0.931]
Epoch [47/120    avg_loss:0.076, val_acc:0.936]
Epoch [48/120    avg_loss:0.045, val_acc:0.951]
Epoch [49/120    avg_loss:0.048, val_acc:0.956]
Epoch [50/120    avg_loss:0.056, val_acc:0.942]
Epoch [51/120    avg_loss:0.161, val_acc:0.925]
Epoch [52/120    avg_loss:0.100, val_acc:0.941]
Epoch [53/120    avg_loss:0.052, val_acc:0.951]
Epoch [54/120    avg_loss:0.067, val_acc:0.950]
Epoch [55/120    avg_loss:0.038, val_acc:0.956]
Epoch [56/120    avg_loss:0.042, val_acc:0.924]
Epoch [57/120    avg_loss:0.068, val_acc:0.930]
Epoch [58/120    avg_loss:0.057, val_acc:0.958]
Epoch [59/120    avg_loss:0.056, val_acc:0.951]
Epoch [60/120    avg_loss:0.245, val_acc:0.914]
Epoch [61/120    avg_loss:0.152, val_acc:0.931]
Epoch [62/120    avg_loss:0.073, val_acc:0.957]
Epoch [63/120    avg_loss:0.043, val_acc:0.962]
Epoch [64/120    avg_loss:0.045, val_acc:0.954]
Epoch [65/120    avg_loss:0.039, val_acc:0.965]
Epoch [66/120    avg_loss:0.043, val_acc:0.957]
Epoch [67/120    avg_loss:0.047, val_acc:0.963]
Epoch [68/120    avg_loss:0.048, val_acc:0.962]
Epoch [69/120    avg_loss:0.038, val_acc:0.963]
Epoch [70/120    avg_loss:0.032, val_acc:0.944]
Epoch [71/120    avg_loss:0.027, val_acc:0.968]
Epoch [72/120    avg_loss:0.028, val_acc:0.965]
Epoch [73/120    avg_loss:0.053, val_acc:0.955]
Epoch [74/120    avg_loss:0.131, val_acc:0.946]
Epoch [75/120    avg_loss:0.079, val_acc:0.941]
Epoch [76/120    avg_loss:0.075, val_acc:0.967]
Epoch [77/120    avg_loss:0.064, val_acc:0.953]
Epoch [78/120    avg_loss:0.046, val_acc:0.962]
Epoch [79/120    avg_loss:0.032, val_acc:0.967]
Epoch [80/120    avg_loss:0.025, val_acc:0.971]
Epoch [81/120    avg_loss:0.028, val_acc:0.957]
Epoch [82/120    avg_loss:0.023, val_acc:0.966]
Epoch [83/120    avg_loss:0.021, val_acc:0.968]
Epoch [84/120    avg_loss:0.061, val_acc:0.943]
Epoch [85/120    avg_loss:0.024, val_acc:0.967]
Epoch [86/120    avg_loss:0.020, val_acc:0.974]
Epoch [87/120    avg_loss:0.017, val_acc:0.974]
Epoch [88/120    avg_loss:0.026, val_acc:0.965]
Epoch [89/120    avg_loss:0.039, val_acc:0.927]
Epoch [90/120    avg_loss:0.043, val_acc:0.954]
Epoch [91/120    avg_loss:0.031, val_acc:0.969]
Epoch [92/120    avg_loss:0.017, val_acc:0.977]
Epoch [93/120    avg_loss:0.063, val_acc:0.958]
Epoch [94/120    avg_loss:0.038, val_acc:0.961]
Epoch [95/120    avg_loss:0.020, val_acc:0.975]
Epoch [96/120    avg_loss:0.014, val_acc:0.968]
Epoch [97/120    avg_loss:0.017, val_acc:0.967]
Epoch [98/120    avg_loss:0.019, val_acc:0.976]
Epoch [99/120    avg_loss:0.015, val_acc:0.974]
Epoch [100/120    avg_loss:0.013, val_acc:0.977]
Epoch [101/120    avg_loss:0.019, val_acc:0.974]
Epoch [102/120    avg_loss:0.019, val_acc:0.976]
Epoch [103/120    avg_loss:0.030, val_acc:0.958]
Epoch [104/120    avg_loss:0.072, val_acc:0.966]
Epoch [105/120    avg_loss:0.025, val_acc:0.969]
Epoch [106/120    avg_loss:0.020, val_acc:0.973]
Epoch [107/120    avg_loss:0.013, val_acc:0.971]
Epoch [108/120    avg_loss:0.014, val_acc:0.976]
Epoch [109/120    avg_loss:0.014, val_acc:0.978]
Epoch [110/120    avg_loss:0.133, val_acc:0.938]
Epoch [111/120    avg_loss:0.042, val_acc:0.941]
Epoch [112/120    avg_loss:0.034, val_acc:0.971]
Epoch [113/120    avg_loss:0.016, val_acc:0.976]
Epoch [114/120    avg_loss:0.011, val_acc:0.978]
Epoch [115/120    avg_loss:0.015, val_acc:0.971]
Epoch [116/120    avg_loss:0.015, val_acc:0.979]
Epoch [117/120    avg_loss:0.009, val_acc:0.981]
Epoch [118/120    avg_loss:0.009, val_acc:0.982]
Epoch [119/120    avg_loss:0.009, val_acc:0.982]
Epoch [120/120    avg_loss:0.009, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    1    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1272    4    0    0    0    0    0    0    4    4    1    0
     0    0    0]
 [   0    0    0  718    0   11    0    0    0    0    1    0   16    1
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    8   33    0    0    0    1    0    0  818    2   13    0
     0    0    0]
 [   0    0   23    0    0    0    3    0    1    0    5 2176    0    2
     0    0    0]
 [   0    0    0    8    0    0    0    0    0    0    0    8  517    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1130    8    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    52  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.60433604336043

F1 scores:
[       nan 0.94871795 0.98261877 0.950364   0.99764706 0.98751419
 0.9969651  0.98039216 0.99767981 0.97142857 0.95896835 0.98909091
 0.95299539 0.9919571  0.97371822 0.90600924 0.97590361]

Kappa:
0.9726853733636043
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8810aff7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.353, val_acc:0.552]
Epoch [2/120    avg_loss:1.836, val_acc:0.584]
Epoch [3/120    avg_loss:1.540, val_acc:0.627]
Epoch [4/120    avg_loss:1.257, val_acc:0.705]
Epoch [5/120    avg_loss:1.097, val_acc:0.702]
Epoch [6/120    avg_loss:0.921, val_acc:0.742]
Epoch [7/120    avg_loss:0.845, val_acc:0.727]
Epoch [8/120    avg_loss:0.810, val_acc:0.773]
Epoch [9/120    avg_loss:0.681, val_acc:0.761]
Epoch [10/120    avg_loss:0.588, val_acc:0.817]
Epoch [11/120    avg_loss:0.615, val_acc:0.824]
Epoch [12/120    avg_loss:0.552, val_acc:0.819]
Epoch [13/120    avg_loss:0.470, val_acc:0.762]
Epoch [14/120    avg_loss:0.392, val_acc:0.839]
Epoch [15/120    avg_loss:0.337, val_acc:0.858]
Epoch [16/120    avg_loss:0.339, val_acc:0.857]
Epoch [17/120    avg_loss:0.364, val_acc:0.860]
Epoch [18/120    avg_loss:0.315, val_acc:0.846]
Epoch [19/120    avg_loss:0.289, val_acc:0.886]
Epoch [20/120    avg_loss:0.287, val_acc:0.847]
Epoch [21/120    avg_loss:0.278, val_acc:0.854]
Epoch [22/120    avg_loss:0.257, val_acc:0.839]
Epoch [23/120    avg_loss:0.274, val_acc:0.885]
Epoch [24/120    avg_loss:0.310, val_acc:0.845]
Epoch [25/120    avg_loss:0.225, val_acc:0.883]
Epoch [26/120    avg_loss:0.225, val_acc:0.888]
Epoch [27/120    avg_loss:0.183, val_acc:0.882]
Epoch [28/120    avg_loss:0.210, val_acc:0.892]
Epoch [29/120    avg_loss:0.229, val_acc:0.908]
Epoch [30/120    avg_loss:0.229, val_acc:0.903]
Epoch [31/120    avg_loss:0.156, val_acc:0.916]
Epoch [32/120    avg_loss:0.152, val_acc:0.901]
Epoch [33/120    avg_loss:0.171, val_acc:0.924]
Epoch [34/120    avg_loss:0.165, val_acc:0.902]
Epoch [35/120    avg_loss:0.247, val_acc:0.900]
Epoch [36/120    avg_loss:0.207, val_acc:0.900]
Epoch [37/120    avg_loss:0.160, val_acc:0.930]
Epoch [38/120    avg_loss:0.179, val_acc:0.931]
Epoch [39/120    avg_loss:0.124, val_acc:0.896]
Epoch [40/120    avg_loss:0.120, val_acc:0.936]
Epoch [41/120    avg_loss:0.085, val_acc:0.948]
Epoch [42/120    avg_loss:0.085, val_acc:0.943]
Epoch [43/120    avg_loss:0.067, val_acc:0.958]
Epoch [44/120    avg_loss:0.077, val_acc:0.944]
Epoch [45/120    avg_loss:0.130, val_acc:0.911]
Epoch [46/120    avg_loss:0.108, val_acc:0.932]
Epoch [47/120    avg_loss:0.073, val_acc:0.929]
Epoch [48/120    avg_loss:0.097, val_acc:0.943]
Epoch [49/120    avg_loss:0.111, val_acc:0.933]
Epoch [50/120    avg_loss:0.113, val_acc:0.938]
Epoch [51/120    avg_loss:0.084, val_acc:0.940]
Epoch [52/120    avg_loss:0.074, val_acc:0.904]
Epoch [53/120    avg_loss:0.073, val_acc:0.926]
Epoch [54/120    avg_loss:0.049, val_acc:0.954]
Epoch [55/120    avg_loss:0.066, val_acc:0.954]
Epoch [56/120    avg_loss:0.058, val_acc:0.949]
Epoch [57/120    avg_loss:0.045, val_acc:0.962]
Epoch [58/120    avg_loss:0.030, val_acc:0.964]
Epoch [59/120    avg_loss:0.031, val_acc:0.962]
Epoch [60/120    avg_loss:0.027, val_acc:0.962]
Epoch [61/120    avg_loss:0.027, val_acc:0.961]
Epoch [62/120    avg_loss:0.030, val_acc:0.962]
Epoch [63/120    avg_loss:0.028, val_acc:0.961]
Epoch [64/120    avg_loss:0.026, val_acc:0.960]
Epoch [65/120    avg_loss:0.025, val_acc:0.959]
Epoch [66/120    avg_loss:0.023, val_acc:0.959]
Epoch [67/120    avg_loss:0.027, val_acc:0.961]
Epoch [68/120    avg_loss:0.027, val_acc:0.961]
Epoch [69/120    avg_loss:0.020, val_acc:0.962]
Epoch [70/120    avg_loss:0.024, val_acc:0.961]
Epoch [71/120    avg_loss:0.024, val_acc:0.961]
Epoch [72/120    avg_loss:0.026, val_acc:0.961]
Epoch [73/120    avg_loss:0.024, val_acc:0.962]
Epoch [74/120    avg_loss:0.030, val_acc:0.962]
Epoch [75/120    avg_loss:0.024, val_acc:0.962]
Epoch [76/120    avg_loss:0.023, val_acc:0.962]
Epoch [77/120    avg_loss:0.024, val_acc:0.963]
Epoch [78/120    avg_loss:0.026, val_acc:0.963]
Epoch [79/120    avg_loss:0.023, val_acc:0.963]
Epoch [80/120    avg_loss:0.023, val_acc:0.963]
Epoch [81/120    avg_loss:0.024, val_acc:0.962]
Epoch [82/120    avg_loss:0.026, val_acc:0.962]
Epoch [83/120    avg_loss:0.031, val_acc:0.962]
Epoch [84/120    avg_loss:0.026, val_acc:0.962]
Epoch [85/120    avg_loss:0.025, val_acc:0.962]
Epoch [86/120    avg_loss:0.022, val_acc:0.962]
Epoch [87/120    avg_loss:0.024, val_acc:0.962]
Epoch [88/120    avg_loss:0.024, val_acc:0.962]
Epoch [89/120    avg_loss:0.024, val_acc:0.962]
Epoch [90/120    avg_loss:0.024, val_acc:0.962]
Epoch [91/120    avg_loss:0.025, val_acc:0.962]
Epoch [92/120    avg_loss:0.021, val_acc:0.962]
Epoch [93/120    avg_loss:0.025, val_acc:0.962]
Epoch [94/120    avg_loss:0.024, val_acc:0.962]
Epoch [95/120    avg_loss:0.026, val_acc:0.962]
Epoch [96/120    avg_loss:0.032, val_acc:0.962]
Epoch [97/120    avg_loss:0.020, val_acc:0.962]
Epoch [98/120    avg_loss:0.026, val_acc:0.962]
Epoch [99/120    avg_loss:0.025, val_acc:0.962]
Epoch [100/120    avg_loss:0.022, val_acc:0.962]
Epoch [101/120    avg_loss:0.026, val_acc:0.962]
Epoch [102/120    avg_loss:0.024, val_acc:0.962]
Epoch [103/120    avg_loss:0.025, val_acc:0.962]
Epoch [104/120    avg_loss:0.021, val_acc:0.962]
Epoch [105/120    avg_loss:0.026, val_acc:0.962]
Epoch [106/120    avg_loss:0.020, val_acc:0.962]
Epoch [107/120    avg_loss:0.023, val_acc:0.962]
Epoch [108/120    avg_loss:0.019, val_acc:0.962]
Epoch [109/120    avg_loss:0.023, val_acc:0.962]
Epoch [110/120    avg_loss:0.020, val_acc:0.962]
Epoch [111/120    avg_loss:0.024, val_acc:0.962]
Epoch [112/120    avg_loss:0.031, val_acc:0.962]
Epoch [113/120    avg_loss:0.023, val_acc:0.962]
Epoch [114/120    avg_loss:0.024, val_acc:0.962]
Epoch [115/120    avg_loss:0.025, val_acc:0.962]
Epoch [116/120    avg_loss:0.028, val_acc:0.962]
Epoch [117/120    avg_loss:0.024, val_acc:0.962]
Epoch [118/120    avg_loss:0.021, val_acc:0.962]
Epoch [119/120    avg_loss:0.021, val_acc:0.962]
Epoch [120/120    avg_loss:0.024, val_acc:0.962]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1252    0    0    0    0    0    0    2    6   24    0    0
     0    1    0]
 [   0    0    0  693   14   17    0    0    0    2    2    0   15    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    2    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    5    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   16    8    0    5    0    0    0    0  809   33    0    0
     0    4    0]
 [   0    0    3    0    0    0    2    0    0    0   10 2187    3    4
     1    0    0]
 [   0    0    0   19   16   15    0    0    0    0    0    6  475    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1132    6    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    87  255    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.16260162601625

F1 scores:
[       nan 0.94871795 0.97965571 0.94478528 0.93421053 0.95575221
 0.98934551 0.96153846 0.9953271  0.87804878 0.94786175 0.97961926
 0.9196515  0.97883598 0.95891571 0.8319739  0.9704142 ]

Kappa:
0.9561959466311261
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8845bdb7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.366, val_acc:0.495]
Epoch [2/120    avg_loss:1.918, val_acc:0.572]
Epoch [3/120    avg_loss:1.645, val_acc:0.626]
Epoch [4/120    avg_loss:1.425, val_acc:0.666]
Epoch [5/120    avg_loss:1.310, val_acc:0.681]
Epoch [6/120    avg_loss:1.095, val_acc:0.718]
Epoch [7/120    avg_loss:0.976, val_acc:0.709]
Epoch [8/120    avg_loss:0.841, val_acc:0.794]
Epoch [9/120    avg_loss:0.799, val_acc:0.794]
Epoch [10/120    avg_loss:0.698, val_acc:0.766]
Epoch [11/120    avg_loss:0.633, val_acc:0.817]
Epoch [12/120    avg_loss:0.589, val_acc:0.744]
Epoch [13/120    avg_loss:0.615, val_acc:0.831]
Epoch [14/120    avg_loss:0.505, val_acc:0.839]
Epoch [15/120    avg_loss:0.465, val_acc:0.834]
Epoch [16/120    avg_loss:0.488, val_acc:0.858]
Epoch [17/120    avg_loss:0.466, val_acc:0.836]
Epoch [18/120    avg_loss:0.484, val_acc:0.823]
Epoch [19/120    avg_loss:0.339, val_acc:0.901]
Epoch [20/120    avg_loss:0.331, val_acc:0.874]
Epoch [21/120    avg_loss:0.317, val_acc:0.890]
Epoch [22/120    avg_loss:0.306, val_acc:0.894]
Epoch [23/120    avg_loss:0.297, val_acc:0.908]
Epoch [24/120    avg_loss:0.256, val_acc:0.913]
Epoch [25/120    avg_loss:0.256, val_acc:0.909]
Epoch [26/120    avg_loss:0.220, val_acc:0.930]
Epoch [27/120    avg_loss:0.264, val_acc:0.877]
Epoch [28/120    avg_loss:0.227, val_acc:0.896]
Epoch [29/120    avg_loss:0.229, val_acc:0.942]
Epoch [30/120    avg_loss:0.227, val_acc:0.919]
Epoch [31/120    avg_loss:0.162, val_acc:0.933]
Epoch [32/120    avg_loss:0.168, val_acc:0.938]
Epoch [33/120    avg_loss:0.153, val_acc:0.917]
Epoch [34/120    avg_loss:0.206, val_acc:0.920]
Epoch [35/120    avg_loss:0.190, val_acc:0.937]
Epoch [36/120    avg_loss:0.139, val_acc:0.944]
Epoch [37/120    avg_loss:0.129, val_acc:0.916]
Epoch [38/120    avg_loss:0.109, val_acc:0.960]
Epoch [39/120    avg_loss:0.147, val_acc:0.942]
Epoch [40/120    avg_loss:0.101, val_acc:0.951]
Epoch [41/120    avg_loss:0.096, val_acc:0.945]
Epoch [42/120    avg_loss:0.091, val_acc:0.937]
Epoch [43/120    avg_loss:0.168, val_acc:0.947]
Epoch [44/120    avg_loss:0.107, val_acc:0.960]
Epoch [45/120    avg_loss:0.068, val_acc:0.961]
Epoch [46/120    avg_loss:0.128, val_acc:0.961]
Epoch [47/120    avg_loss:0.133, val_acc:0.938]
Epoch [48/120    avg_loss:0.090, val_acc:0.954]
Epoch [49/120    avg_loss:0.087, val_acc:0.933]
Epoch [50/120    avg_loss:0.122, val_acc:0.960]
Epoch [51/120    avg_loss:0.088, val_acc:0.956]
Epoch [52/120    avg_loss:0.078, val_acc:0.960]
Epoch [53/120    avg_loss:0.060, val_acc:0.968]
Epoch [54/120    avg_loss:0.064, val_acc:0.971]
Epoch [55/120    avg_loss:0.063, val_acc:0.956]
Epoch [56/120    avg_loss:0.060, val_acc:0.960]
Epoch [57/120    avg_loss:0.089, val_acc:0.971]
Epoch [58/120    avg_loss:0.066, val_acc:0.964]
Epoch [59/120    avg_loss:0.066, val_acc:0.972]
Epoch [60/120    avg_loss:0.099, val_acc:0.943]
Epoch [61/120    avg_loss:0.072, val_acc:0.970]
Epoch [62/120    avg_loss:0.054, val_acc:0.974]
Epoch [63/120    avg_loss:0.030, val_acc:0.976]
Epoch [64/120    avg_loss:0.073, val_acc:0.964]
Epoch [65/120    avg_loss:0.068, val_acc:0.971]
Epoch [66/120    avg_loss:0.044, val_acc:0.972]
Epoch [67/120    avg_loss:0.033, val_acc:0.982]
Epoch [68/120    avg_loss:0.039, val_acc:0.975]
Epoch [69/120    avg_loss:0.030, val_acc:0.983]
Epoch [70/120    avg_loss:0.023, val_acc:0.981]
Epoch [71/120    avg_loss:0.045, val_acc:0.961]
Epoch [72/120    avg_loss:0.027, val_acc:0.982]
Epoch [73/120    avg_loss:0.038, val_acc:0.981]
Epoch [74/120    avg_loss:0.033, val_acc:0.985]
Epoch [75/120    avg_loss:0.037, val_acc:0.977]
Epoch [76/120    avg_loss:0.093, val_acc:0.953]
Epoch [77/120    avg_loss:0.107, val_acc:0.956]
Epoch [78/120    avg_loss:0.036, val_acc:0.974]
Epoch [79/120    avg_loss:0.035, val_acc:0.972]
Epoch [80/120    avg_loss:0.054, val_acc:0.962]
Epoch [81/120    avg_loss:0.028, val_acc:0.977]
Epoch [82/120    avg_loss:0.017, val_acc:0.985]
Epoch [83/120    avg_loss:0.038, val_acc:0.978]
Epoch [84/120    avg_loss:0.113, val_acc:0.901]
Epoch [85/120    avg_loss:0.393, val_acc:0.914]
Epoch [86/120    avg_loss:0.106, val_acc:0.956]
Epoch [87/120    avg_loss:0.081, val_acc:0.968]
Epoch [88/120    avg_loss:0.057, val_acc:0.974]
Epoch [89/120    avg_loss:0.035, val_acc:0.974]
Epoch [90/120    avg_loss:0.027, val_acc:0.979]
Epoch [91/120    avg_loss:0.045, val_acc:0.972]
Epoch [92/120    avg_loss:0.034, val_acc:0.984]
Epoch [93/120    avg_loss:0.022, val_acc:0.989]
Epoch [94/120    avg_loss:0.018, val_acc:0.989]
Epoch [95/120    avg_loss:0.031, val_acc:0.975]
Epoch [96/120    avg_loss:0.039, val_acc:0.979]
Epoch [97/120    avg_loss:0.032, val_acc:0.979]
Epoch [98/120    avg_loss:0.017, val_acc:0.984]
Epoch [99/120    avg_loss:0.029, val_acc:0.976]
Epoch [100/120    avg_loss:0.022, val_acc:0.984]
Epoch [101/120    avg_loss:0.014, val_acc:0.987]
Epoch [102/120    avg_loss:0.012, val_acc:0.994]
Epoch [103/120    avg_loss:0.018, val_acc:0.979]
Epoch [104/120    avg_loss:0.031, val_acc:0.974]
Epoch [105/120    avg_loss:0.065, val_acc:0.968]
Epoch [106/120    avg_loss:0.020, val_acc:0.987]
Epoch [107/120    avg_loss:0.024, val_acc:0.983]
Epoch [108/120    avg_loss:0.035, val_acc:0.975]
Epoch [109/120    avg_loss:0.018, val_acc:0.987]
Epoch [110/120    avg_loss:0.010, val_acc:0.989]
Epoch [111/120    avg_loss:0.012, val_acc:0.990]
Epoch [112/120    avg_loss:0.011, val_acc:0.987]
Epoch [113/120    avg_loss:0.009, val_acc:0.989]
Epoch [114/120    avg_loss:0.009, val_acc:0.989]
Epoch [115/120    avg_loss:0.007, val_acc:0.987]
Epoch [116/120    avg_loss:0.008, val_acc:0.990]
Epoch [117/120    avg_loss:0.006, val_acc:0.991]
Epoch [118/120    avg_loss:0.006, val_acc:0.991]
Epoch [119/120    avg_loss:0.006, val_acc:0.991]
Epoch [120/120    avg_loss:0.004, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1235   14    0    0    3    0    0    0    8   17    5    0
     0    3    0]
 [   0    0    1  733    0   11    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1   57    0    0    0    0    0    1  797   17    0    0
     0    2    0]
 [   0    0    7    0    0    0    7    0    0    0    4 2191    0    1
     0    0    0]
 [   0    0    0   11    3    2    0    0    0    0   12   21  480    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    3    0    3    0    0    0
  1133    0    0]
 [   0    0    0    0    0    0    7    0    0    2    0    0    0    0
   105  233    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.31436314363144

F1 scores:
[       nan 1.         0.97667062 0.93854033 0.99300699 0.97949886
 0.98722765 0.98039216 0.99652375 0.8372093  0.93819894 0.98339318
 0.9421001  0.99730458 0.95250105 0.7965812  0.97109827]

Kappa:
0.9579232171759238
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8bf26af7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.328, val_acc:0.501]
Epoch [2/120    avg_loss:1.812, val_acc:0.587]
Epoch [3/120    avg_loss:1.601, val_acc:0.640]
Epoch [4/120    avg_loss:1.286, val_acc:0.668]
Epoch [5/120    avg_loss:1.118, val_acc:0.715]
Epoch [6/120    avg_loss:0.991, val_acc:0.743]
Epoch [7/120    avg_loss:0.819, val_acc:0.759]
Epoch [8/120    avg_loss:0.815, val_acc:0.773]
Epoch [9/120    avg_loss:0.705, val_acc:0.788]
Epoch [10/120    avg_loss:0.593, val_acc:0.793]
Epoch [11/120    avg_loss:0.585, val_acc:0.805]
Epoch [12/120    avg_loss:0.544, val_acc:0.827]
Epoch [13/120    avg_loss:0.468, val_acc:0.863]
Epoch [14/120    avg_loss:0.495, val_acc:0.848]
Epoch [15/120    avg_loss:0.452, val_acc:0.866]
Epoch [16/120    avg_loss:0.433, val_acc:0.858]
Epoch [17/120    avg_loss:0.408, val_acc:0.860]
Epoch [18/120    avg_loss:0.375, val_acc:0.885]
Epoch [19/120    avg_loss:0.426, val_acc:0.881]
Epoch [20/120    avg_loss:0.357, val_acc:0.890]
Epoch [21/120    avg_loss:0.335, val_acc:0.882]
Epoch [22/120    avg_loss:0.383, val_acc:0.844]
Epoch [23/120    avg_loss:0.359, val_acc:0.899]
Epoch [24/120    avg_loss:0.286, val_acc:0.912]
Epoch [25/120    avg_loss:0.260, val_acc:0.916]
Epoch [26/120    avg_loss:0.262, val_acc:0.917]
Epoch [27/120    avg_loss:0.207, val_acc:0.930]
Epoch [28/120    avg_loss:0.231, val_acc:0.917]
Epoch [29/120    avg_loss:0.194, val_acc:0.930]
Epoch [30/120    avg_loss:0.172, val_acc:0.908]
Epoch [31/120    avg_loss:0.137, val_acc:0.927]
Epoch [32/120    avg_loss:0.198, val_acc:0.916]
Epoch [33/120    avg_loss:0.161, val_acc:0.927]
Epoch [34/120    avg_loss:0.291, val_acc:0.883]
Epoch [35/120    avg_loss:0.280, val_acc:0.915]
Epoch [36/120    avg_loss:0.187, val_acc:0.896]
Epoch [37/120    avg_loss:0.152, val_acc:0.936]
Epoch [38/120    avg_loss:0.142, val_acc:0.942]
Epoch [39/120    avg_loss:0.146, val_acc:0.938]
Epoch [40/120    avg_loss:0.187, val_acc:0.928]
Epoch [41/120    avg_loss:0.138, val_acc:0.933]
Epoch [42/120    avg_loss:0.115, val_acc:0.957]
Epoch [43/120    avg_loss:0.109, val_acc:0.944]
Epoch [44/120    avg_loss:0.145, val_acc:0.937]
Epoch [45/120    avg_loss:0.099, val_acc:0.945]
Epoch [46/120    avg_loss:0.133, val_acc:0.942]
Epoch [47/120    avg_loss:0.110, val_acc:0.934]
Epoch [48/120    avg_loss:0.120, val_acc:0.944]
Epoch [49/120    avg_loss:0.133, val_acc:0.951]
Epoch [50/120    avg_loss:0.093, val_acc:0.935]
Epoch [51/120    avg_loss:0.107, val_acc:0.923]
Epoch [52/120    avg_loss:0.137, val_acc:0.930]
Epoch [53/120    avg_loss:0.150, val_acc:0.941]
Epoch [54/120    avg_loss:0.087, val_acc:0.952]
Epoch [55/120    avg_loss:0.058, val_acc:0.963]
Epoch [56/120    avg_loss:0.057, val_acc:0.959]
Epoch [57/120    avg_loss:0.084, val_acc:0.958]
Epoch [58/120    avg_loss:0.132, val_acc:0.926]
Epoch [59/120    avg_loss:0.156, val_acc:0.940]
Epoch [60/120    avg_loss:0.099, val_acc:0.951]
Epoch [61/120    avg_loss:0.066, val_acc:0.960]
Epoch [62/120    avg_loss:0.079, val_acc:0.961]
Epoch [63/120    avg_loss:0.058, val_acc:0.967]
Epoch [64/120    avg_loss:0.086, val_acc:0.958]
Epoch [65/120    avg_loss:0.046, val_acc:0.958]
Epoch [66/120    avg_loss:0.055, val_acc:0.959]
Epoch [67/120    avg_loss:0.063, val_acc:0.956]
Epoch [68/120    avg_loss:0.093, val_acc:0.961]
Epoch [69/120    avg_loss:0.079, val_acc:0.949]
Epoch [70/120    avg_loss:0.101, val_acc:0.966]
Epoch [71/120    avg_loss:0.048, val_acc:0.962]
Epoch [72/120    avg_loss:0.077, val_acc:0.960]
Epoch [73/120    avg_loss:0.058, val_acc:0.968]
Epoch [74/120    avg_loss:0.048, val_acc:0.949]
Epoch [75/120    avg_loss:0.048, val_acc:0.962]
Epoch [76/120    avg_loss:0.040, val_acc:0.957]
Epoch [77/120    avg_loss:0.054, val_acc:0.965]
Epoch [78/120    avg_loss:0.048, val_acc:0.970]
Epoch [79/120    avg_loss:0.053, val_acc:0.961]
Epoch [80/120    avg_loss:0.053, val_acc:0.961]
Epoch [81/120    avg_loss:0.068, val_acc:0.958]
Epoch [82/120    avg_loss:0.040, val_acc:0.965]
Epoch [83/120    avg_loss:0.043, val_acc:0.968]
Epoch [84/120    avg_loss:0.040, val_acc:0.959]
Epoch [85/120    avg_loss:0.086, val_acc:0.972]
Epoch [86/120    avg_loss:0.037, val_acc:0.970]
Epoch [87/120    avg_loss:0.054, val_acc:0.948]
Epoch [88/120    avg_loss:0.043, val_acc:0.968]
Epoch [89/120    avg_loss:0.029, val_acc:0.963]
Epoch [90/120    avg_loss:0.026, val_acc:0.965]
Epoch [91/120    avg_loss:0.026, val_acc:0.972]
Epoch [92/120    avg_loss:0.045, val_acc:0.973]
Epoch [93/120    avg_loss:0.037, val_acc:0.973]
Epoch [94/120    avg_loss:0.036, val_acc:0.967]
Epoch [95/120    avg_loss:0.032, val_acc:0.968]
Epoch [96/120    avg_loss:0.038, val_acc:0.952]
Epoch [97/120    avg_loss:0.035, val_acc:0.971]
Epoch [98/120    avg_loss:0.022, val_acc:0.969]
Epoch [99/120    avg_loss:0.019, val_acc:0.980]
Epoch [100/120    avg_loss:0.024, val_acc:0.970]
Epoch [101/120    avg_loss:0.021, val_acc:0.971]
Epoch [102/120    avg_loss:0.024, val_acc:0.973]
Epoch [103/120    avg_loss:0.026, val_acc:0.980]
Epoch [104/120    avg_loss:0.026, val_acc:0.983]
Epoch [105/120    avg_loss:0.019, val_acc:0.963]
Epoch [106/120    avg_loss:0.023, val_acc:0.958]
Epoch [107/120    avg_loss:0.032, val_acc:0.969]
Epoch [108/120    avg_loss:0.035, val_acc:0.957]
Epoch [109/120    avg_loss:0.033, val_acc:0.979]
Epoch [110/120    avg_loss:0.020, val_acc:0.969]
Epoch [111/120    avg_loss:0.017, val_acc:0.979]
Epoch [112/120    avg_loss:0.015, val_acc:0.982]
Epoch [113/120    avg_loss:0.016, val_acc:0.979]
Epoch [114/120    avg_loss:0.029, val_acc:0.971]
Epoch [115/120    avg_loss:0.013, val_acc:0.968]
Epoch [116/120    avg_loss:0.013, val_acc:0.977]
Epoch [117/120    avg_loss:0.013, val_acc:0.986]
Epoch [118/120    avg_loss:0.016, val_acc:0.980]
Epoch [119/120    avg_loss:0.011, val_acc:0.982]
Epoch [120/120    avg_loss:0.011, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    3    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1243    4    0    0    2    0    0    0    3   31    1    0
     0    1    0]
 [   0    0    1  730    0    7    0    0    0    5    0    0    0    3
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    3    0    3    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    1   79    0    7    0    0    0    0  765   18    4    0
     0    1    0]
 [   0    0    2    0    0    0    9    0    0    0   10 2186    0    2
     1    0    0]
 [   0    0    0   12   22    2    0    0    0    0    0    0  495    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    2    0    0
  1134    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   139  208    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.68563685636856

F1 scores:
[       nan 0.92682927 0.98067061 0.92639594 0.95089286 0.9738339
 0.98864497 0.94339623 0.99649942 0.7        0.92391304 0.98247191
 0.95652174 0.98666667 0.93874172 0.74685817 0.97647059]

Kappa:
0.9507427236060059
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:03:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc2db56c828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.373, val_acc:0.443]
Epoch [2/120    avg_loss:1.861, val_acc:0.516]
Epoch [3/120    avg_loss:1.644, val_acc:0.608]
Epoch [4/120    avg_loss:1.422, val_acc:0.650]
Epoch [5/120    avg_loss:1.180, val_acc:0.703]
Epoch [6/120    avg_loss:0.979, val_acc:0.675]
Epoch [7/120    avg_loss:0.865, val_acc:0.770]
Epoch [8/120    avg_loss:0.810, val_acc:0.712]
Epoch [9/120    avg_loss:0.779, val_acc:0.792]
Epoch [10/120    avg_loss:0.710, val_acc:0.781]
Epoch [11/120    avg_loss:0.630, val_acc:0.810]
Epoch [12/120    avg_loss:0.535, val_acc:0.840]
Epoch [13/120    avg_loss:0.515, val_acc:0.818]
Epoch [14/120    avg_loss:0.431, val_acc:0.829]
Epoch [15/120    avg_loss:0.432, val_acc:0.820]
Epoch [16/120    avg_loss:0.442, val_acc:0.835]
Epoch [17/120    avg_loss:0.386, val_acc:0.874]
Epoch [18/120    avg_loss:0.346, val_acc:0.873]
Epoch [19/120    avg_loss:0.339, val_acc:0.890]
Epoch [20/120    avg_loss:0.336, val_acc:0.893]
Epoch [21/120    avg_loss:0.339, val_acc:0.844]
Epoch [22/120    avg_loss:0.398, val_acc:0.882]
Epoch [23/120    avg_loss:0.315, val_acc:0.878]
Epoch [24/120    avg_loss:0.326, val_acc:0.906]
Epoch [25/120    avg_loss:0.275, val_acc:0.885]
Epoch [26/120    avg_loss:0.257, val_acc:0.915]
Epoch [27/120    avg_loss:0.239, val_acc:0.920]
Epoch [28/120    avg_loss:0.243, val_acc:0.885]
Epoch [29/120    avg_loss:0.251, val_acc:0.924]
Epoch [30/120    avg_loss:0.180, val_acc:0.903]
Epoch [31/120    avg_loss:0.193, val_acc:0.929]
Epoch [32/120    avg_loss:0.197, val_acc:0.903]
Epoch [33/120    avg_loss:0.189, val_acc:0.898]
Epoch [34/120    avg_loss:0.151, val_acc:0.908]
Epoch [35/120    avg_loss:0.174, val_acc:0.925]
Epoch [36/120    avg_loss:0.118, val_acc:0.901]
Epoch [37/120    avg_loss:0.146, val_acc:0.905]
Epoch [38/120    avg_loss:0.127, val_acc:0.926]
Epoch [39/120    avg_loss:0.143, val_acc:0.939]
Epoch [40/120    avg_loss:0.132, val_acc:0.907]
Epoch [41/120    avg_loss:0.130, val_acc:0.927]
Epoch [42/120    avg_loss:0.104, val_acc:0.936]
Epoch [43/120    avg_loss:0.107, val_acc:0.929]
Epoch [44/120    avg_loss:0.110, val_acc:0.939]
Epoch [45/120    avg_loss:0.064, val_acc:0.950]
Epoch [46/120    avg_loss:0.161, val_acc:0.925]
Epoch [47/120    avg_loss:0.137, val_acc:0.942]
Epoch [48/120    avg_loss:0.101, val_acc:0.948]
Epoch [49/120    avg_loss:0.077, val_acc:0.946]
Epoch [50/120    avg_loss:0.131, val_acc:0.924]
Epoch [51/120    avg_loss:0.126, val_acc:0.932]
Epoch [52/120    avg_loss:0.118, val_acc:0.922]
Epoch [53/120    avg_loss:0.127, val_acc:0.945]
Epoch [54/120    avg_loss:0.100, val_acc:0.951]
Epoch [55/120    avg_loss:0.098, val_acc:0.955]
Epoch [56/120    avg_loss:0.123, val_acc:0.946]
Epoch [57/120    avg_loss:0.113, val_acc:0.921]
Epoch [58/120    avg_loss:0.092, val_acc:0.940]
Epoch [59/120    avg_loss:0.081, val_acc:0.953]
Epoch [60/120    avg_loss:0.064, val_acc:0.944]
Epoch [61/120    avg_loss:0.046, val_acc:0.954]
Epoch [62/120    avg_loss:0.043, val_acc:0.949]
Epoch [63/120    avg_loss:0.069, val_acc:0.960]
Epoch [64/120    avg_loss:0.049, val_acc:0.951]
Epoch [65/120    avg_loss:0.099, val_acc:0.925]
Epoch [66/120    avg_loss:0.064, val_acc:0.961]
Epoch [67/120    avg_loss:0.049, val_acc:0.950]
Epoch [68/120    avg_loss:0.045, val_acc:0.962]
Epoch [69/120    avg_loss:0.056, val_acc:0.967]
Epoch [70/120    avg_loss:0.048, val_acc:0.970]
Epoch [71/120    avg_loss:0.040, val_acc:0.965]
Epoch [72/120    avg_loss:0.057, val_acc:0.963]
Epoch [73/120    avg_loss:0.045, val_acc:0.942]
Epoch [74/120    avg_loss:0.064, val_acc:0.911]
Epoch [75/120    avg_loss:0.314, val_acc:0.868]
Epoch [76/120    avg_loss:0.262, val_acc:0.946]
Epoch [77/120    avg_loss:0.081, val_acc:0.960]
Epoch [78/120    avg_loss:0.073, val_acc:0.962]
Epoch [79/120    avg_loss:0.062, val_acc:0.956]
Epoch [80/120    avg_loss:0.037, val_acc:0.951]
Epoch [81/120    avg_loss:0.096, val_acc:0.932]
Epoch [82/120    avg_loss:0.148, val_acc:0.939]
Epoch [83/120    avg_loss:0.084, val_acc:0.946]
Epoch [84/120    avg_loss:0.063, val_acc:0.962]
Epoch [85/120    avg_loss:0.038, val_acc:0.963]
Epoch [86/120    avg_loss:0.037, val_acc:0.968]
Epoch [87/120    avg_loss:0.034, val_acc:0.969]
Epoch [88/120    avg_loss:0.026, val_acc:0.969]
Epoch [89/120    avg_loss:0.021, val_acc:0.971]
Epoch [90/120    avg_loss:0.030, val_acc:0.970]
Epoch [91/120    avg_loss:0.026, val_acc:0.968]
Epoch [92/120    avg_loss:0.030, val_acc:0.974]
Epoch [93/120    avg_loss:0.025, val_acc:0.973]
Epoch [94/120    avg_loss:0.032, val_acc:0.973]
Epoch [95/120    avg_loss:0.019, val_acc:0.973]
Epoch [96/120    avg_loss:0.021, val_acc:0.972]
Epoch [97/120    avg_loss:0.025, val_acc:0.972]
Epoch [98/120    avg_loss:0.031, val_acc:0.969]
Epoch [99/120    avg_loss:0.024, val_acc:0.970]
Epoch [100/120    avg_loss:0.026, val_acc:0.971]
Epoch [101/120    avg_loss:0.021, val_acc:0.972]
Epoch [102/120    avg_loss:0.020, val_acc:0.972]
Epoch [103/120    avg_loss:0.020, val_acc:0.974]
Epoch [104/120    avg_loss:0.019, val_acc:0.975]
Epoch [105/120    avg_loss:0.019, val_acc:0.972]
Epoch [106/120    avg_loss:0.021, val_acc:0.973]
Epoch [107/120    avg_loss:0.024, val_acc:0.974]
Epoch [108/120    avg_loss:0.019, val_acc:0.973]
Epoch [109/120    avg_loss:0.019, val_acc:0.973]
Epoch [110/120    avg_loss:0.020, val_acc:0.973]
Epoch [111/120    avg_loss:0.021, val_acc:0.973]
Epoch [112/120    avg_loss:0.018, val_acc:0.973]
Epoch [113/120    avg_loss:0.017, val_acc:0.973]
Epoch [114/120    avg_loss:0.018, val_acc:0.972]
Epoch [115/120    avg_loss:0.018, val_acc:0.972]
Epoch [116/120    avg_loss:0.026, val_acc:0.973]
Epoch [117/120    avg_loss:0.017, val_acc:0.972]
Epoch [118/120    avg_loss:0.018, val_acc:0.973]
Epoch [119/120    avg_loss:0.022, val_acc:0.973]
Epoch [120/120    avg_loss:0.017, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1254    0    0    0    0    0    0    0   19   10    2    0
     0    0    0]
 [   0    0    1  721    0   21    0    0    0    2    0    0    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   15    0    0    1    0
     0    0    0]
 [   0    0    6   86    0    2    0    0    0    0  764   10    0    0
     0    7    0]
 [   0    0   10    0    0    4    4    0    0    0   14 2176    0    2
     0    0    0]
 [   0    0    1    2    3    5    0    0    0    0    4    4  511    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    2    0    0    0    0    0    0    2    0    1    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    87  260    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.42276422764228

F1 scores:
[       nan 0.91358025 0.98083692 0.92554557 0.99300699 0.96337403
 0.99467681 1.         0.99651568 0.83333333 0.90898275 0.9861772
 0.97240723 0.99462366 0.96101695 0.84690554 0.97076023]

Kappa:
0.9592011731085012
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f007d570828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.354, val_acc:0.581]
Epoch [2/120    avg_loss:1.864, val_acc:0.565]
Epoch [3/120    avg_loss:1.603, val_acc:0.608]
Epoch [4/120    avg_loss:1.344, val_acc:0.677]
Epoch [5/120    avg_loss:1.117, val_acc:0.737]
Epoch [6/120    avg_loss:0.994, val_acc:0.736]
Epoch [7/120    avg_loss:0.857, val_acc:0.758]
Epoch [8/120    avg_loss:0.793, val_acc:0.754]
Epoch [9/120    avg_loss:0.763, val_acc:0.792]
Epoch [10/120    avg_loss:0.632, val_acc:0.803]
Epoch [11/120    avg_loss:0.611, val_acc:0.806]
Epoch [12/120    avg_loss:0.509, val_acc:0.821]
Epoch [13/120    avg_loss:0.493, val_acc:0.843]
Epoch [14/120    avg_loss:0.438, val_acc:0.858]
Epoch [15/120    avg_loss:0.478, val_acc:0.804]
Epoch [16/120    avg_loss:0.476, val_acc:0.876]
Epoch [17/120    avg_loss:0.398, val_acc:0.855]
Epoch [18/120    avg_loss:0.374, val_acc:0.875]
Epoch [19/120    avg_loss:0.394, val_acc:0.881]
Epoch [20/120    avg_loss:0.488, val_acc:0.867]
Epoch [21/120    avg_loss:0.274, val_acc:0.916]
Epoch [22/120    avg_loss:0.280, val_acc:0.907]
Epoch [23/120    avg_loss:0.544, val_acc:0.785]
Epoch [24/120    avg_loss:0.399, val_acc:0.866]
Epoch [25/120    avg_loss:0.292, val_acc:0.904]
Epoch [26/120    avg_loss:0.235, val_acc:0.915]
Epoch [27/120    avg_loss:0.281, val_acc:0.877]
Epoch [28/120    avg_loss:0.260, val_acc:0.873]
Epoch [29/120    avg_loss:0.230, val_acc:0.925]
Epoch [30/120    avg_loss:0.255, val_acc:0.919]
Epoch [31/120    avg_loss:0.183, val_acc:0.931]
Epoch [32/120    avg_loss:0.184, val_acc:0.930]
Epoch [33/120    avg_loss:0.144, val_acc:0.942]
Epoch [34/120    avg_loss:0.150, val_acc:0.937]
Epoch [35/120    avg_loss:0.148, val_acc:0.929]
Epoch [36/120    avg_loss:0.168, val_acc:0.904]
Epoch [37/120    avg_loss:0.196, val_acc:0.935]
Epoch [38/120    avg_loss:0.176, val_acc:0.940]
Epoch [39/120    avg_loss:0.168, val_acc:0.940]
Epoch [40/120    avg_loss:0.129, val_acc:0.925]
Epoch [41/120    avg_loss:0.180, val_acc:0.928]
Epoch [42/120    avg_loss:0.176, val_acc:0.947]
Epoch [43/120    avg_loss:0.141, val_acc:0.950]
Epoch [44/120    avg_loss:0.133, val_acc:0.947]
Epoch [45/120    avg_loss:0.139, val_acc:0.919]
Epoch [46/120    avg_loss:0.132, val_acc:0.955]
Epoch [47/120    avg_loss:0.097, val_acc:0.935]
Epoch [48/120    avg_loss:0.107, val_acc:0.947]
Epoch [49/120    avg_loss:0.071, val_acc:0.965]
Epoch [50/120    avg_loss:0.092, val_acc:0.954]
Epoch [51/120    avg_loss:0.057, val_acc:0.955]
Epoch [52/120    avg_loss:0.083, val_acc:0.955]
Epoch [53/120    avg_loss:0.069, val_acc:0.947]
Epoch [54/120    avg_loss:0.060, val_acc:0.965]
Epoch [55/120    avg_loss:0.103, val_acc:0.939]
Epoch [56/120    avg_loss:0.073, val_acc:0.948]
Epoch [57/120    avg_loss:0.066, val_acc:0.949]
Epoch [58/120    avg_loss:0.067, val_acc:0.959]
Epoch [59/120    avg_loss:0.069, val_acc:0.968]
Epoch [60/120    avg_loss:0.086, val_acc:0.965]
Epoch [61/120    avg_loss:0.102, val_acc:0.956]
Epoch [62/120    avg_loss:0.068, val_acc:0.960]
Epoch [63/120    avg_loss:0.096, val_acc:0.945]
Epoch [64/120    avg_loss:0.154, val_acc:0.939]
Epoch [65/120    avg_loss:0.107, val_acc:0.893]
Epoch [66/120    avg_loss:0.076, val_acc:0.946]
Epoch [67/120    avg_loss:0.056, val_acc:0.957]
Epoch [68/120    avg_loss:0.052, val_acc:0.955]
Epoch [69/120    avg_loss:0.080, val_acc:0.963]
Epoch [70/120    avg_loss:0.074, val_acc:0.956]
Epoch [71/120    avg_loss:0.043, val_acc:0.972]
Epoch [72/120    avg_loss:0.057, val_acc:0.967]
Epoch [73/120    avg_loss:0.055, val_acc:0.965]
Epoch [74/120    avg_loss:0.050, val_acc:0.966]
Epoch [75/120    avg_loss:0.052, val_acc:0.963]
Epoch [76/120    avg_loss:0.089, val_acc:0.956]
Epoch [77/120    avg_loss:0.041, val_acc:0.970]
Epoch [78/120    avg_loss:0.037, val_acc:0.970]
Epoch [79/120    avg_loss:0.036, val_acc:0.966]
Epoch [80/120    avg_loss:0.040, val_acc:0.967]
Epoch [81/120    avg_loss:0.044, val_acc:0.946]
Epoch [82/120    avg_loss:0.025, val_acc:0.968]
Epoch [83/120    avg_loss:0.027, val_acc:0.979]
Epoch [84/120    avg_loss:0.038, val_acc:0.960]
Epoch [85/120    avg_loss:0.049, val_acc:0.952]
Epoch [86/120    avg_loss:0.044, val_acc:0.977]
Epoch [87/120    avg_loss:0.061, val_acc:0.966]
Epoch [88/120    avg_loss:0.067, val_acc:0.965]
Epoch [89/120    avg_loss:0.030, val_acc:0.972]
Epoch [90/120    avg_loss:0.031, val_acc:0.978]
Epoch [91/120    avg_loss:0.043, val_acc:0.982]
Epoch [92/120    avg_loss:0.033, val_acc:0.965]
Epoch [93/120    avg_loss:0.024, val_acc:0.967]
Epoch [94/120    avg_loss:0.143, val_acc:0.935]
Epoch [95/120    avg_loss:0.131, val_acc:0.956]
Epoch [96/120    avg_loss:0.057, val_acc:0.952]
Epoch [97/120    avg_loss:0.057, val_acc:0.971]
Epoch [98/120    avg_loss:0.068, val_acc:0.975]
Epoch [99/120    avg_loss:0.032, val_acc:0.963]
Epoch [100/120    avg_loss:0.019, val_acc:0.976]
Epoch [101/120    avg_loss:0.032, val_acc:0.970]
Epoch [102/120    avg_loss:0.035, val_acc:0.978]
Epoch [103/120    avg_loss:0.043, val_acc:0.969]
Epoch [104/120    avg_loss:0.028, val_acc:0.978]
Epoch [105/120    avg_loss:0.019, val_acc:0.980]
Epoch [106/120    avg_loss:0.015, val_acc:0.982]
Epoch [107/120    avg_loss:0.013, val_acc:0.982]
Epoch [108/120    avg_loss:0.012, val_acc:0.983]
Epoch [109/120    avg_loss:0.011, val_acc:0.983]
Epoch [110/120    avg_loss:0.013, val_acc:0.981]
Epoch [111/120    avg_loss:0.013, val_acc:0.982]
Epoch [112/120    avg_loss:0.011, val_acc:0.983]
Epoch [113/120    avg_loss:0.013, val_acc:0.983]
Epoch [114/120    avg_loss:0.011, val_acc:0.982]
Epoch [115/120    avg_loss:0.010, val_acc:0.983]
Epoch [116/120    avg_loss:0.010, val_acc:0.983]
Epoch [117/120    avg_loss:0.010, val_acc:0.985]
Epoch [118/120    avg_loss:0.013, val_acc:0.985]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.009, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1252    0    0    1    1    0    0    0   15   15    1    0
     0    0    0]
 [   0    0    1  733    0    8    0    0    0    3    0    0    0    2
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   14   90    0    6    0    0    0    0  755    1    0    0
     0    9    0]
 [   0    0    2    0    0    1    4    0    0    0   11 2188    1    3
     0    0    0]
 [   0    0    0   24    0    8    0    0    0    0    9    0  489    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    1    0    3    0    0    0
  1133    0    0]
 [   0    0    0    0    0    0   20    0    0    2    0    0    0    0
    94  231    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.05420054200542

F1 scores:
[       nan 0.975      0.98042287 0.91912226 0.99764706 0.9675252
 0.98056801 1.         0.99883856 0.79069767 0.90419162 0.99116648
 0.95321637 0.98666667 0.95773457 0.78571429 0.98245614]

Kappa:
0.9549889348300794
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8a38db9828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.364, val_acc:0.506]
Epoch [2/120    avg_loss:1.846, val_acc:0.506]
Epoch [3/120    avg_loss:1.624, val_acc:0.617]
Epoch [4/120    avg_loss:1.356, val_acc:0.648]
Epoch [5/120    avg_loss:1.218, val_acc:0.614]
Epoch [6/120    avg_loss:1.104, val_acc:0.722]
Epoch [7/120    avg_loss:0.920, val_acc:0.728]
Epoch [8/120    avg_loss:0.827, val_acc:0.747]
Epoch [9/120    avg_loss:0.750, val_acc:0.800]
Epoch [10/120    avg_loss:0.721, val_acc:0.740]
Epoch [11/120    avg_loss:0.641, val_acc:0.826]
Epoch [12/120    avg_loss:0.557, val_acc:0.814]
Epoch [13/120    avg_loss:0.582, val_acc:0.801]
Epoch [14/120    avg_loss:0.502, val_acc:0.847]
Epoch [15/120    avg_loss:0.473, val_acc:0.831]
Epoch [16/120    avg_loss:0.466, val_acc:0.834]
Epoch [17/120    avg_loss:0.425, val_acc:0.882]
Epoch [18/120    avg_loss:0.397, val_acc:0.864]
Epoch [19/120    avg_loss:0.380, val_acc:0.869]
Epoch [20/120    avg_loss:0.320, val_acc:0.863]
Epoch [21/120    avg_loss:0.365, val_acc:0.858]
Epoch [22/120    avg_loss:0.398, val_acc:0.871]
Epoch [23/120    avg_loss:0.342, val_acc:0.877]
Epoch [24/120    avg_loss:0.332, val_acc:0.893]
Epoch [25/120    avg_loss:0.307, val_acc:0.916]
Epoch [26/120    avg_loss:0.340, val_acc:0.899]
Epoch [27/120    avg_loss:0.265, val_acc:0.908]
Epoch [28/120    avg_loss:0.292, val_acc:0.893]
Epoch [29/120    avg_loss:0.279, val_acc:0.896]
Epoch [30/120    avg_loss:0.230, val_acc:0.882]
Epoch [31/120    avg_loss:0.220, val_acc:0.896]
Epoch [32/120    avg_loss:0.323, val_acc:0.878]
Epoch [33/120    avg_loss:0.272, val_acc:0.924]
Epoch [34/120    avg_loss:0.161, val_acc:0.933]
Epoch [35/120    avg_loss:0.158, val_acc:0.923]
Epoch [36/120    avg_loss:0.136, val_acc:0.950]
Epoch [37/120    avg_loss:0.182, val_acc:0.935]
Epoch [38/120    avg_loss:0.158, val_acc:0.927]
Epoch [39/120    avg_loss:0.140, val_acc:0.919]
Epoch [40/120    avg_loss:0.170, val_acc:0.876]
Epoch [41/120    avg_loss:0.146, val_acc:0.917]
Epoch [42/120    avg_loss:0.145, val_acc:0.869]
Epoch [43/120    avg_loss:0.199, val_acc:0.866]
Epoch [44/120    avg_loss:0.219, val_acc:0.945]
Epoch [45/120    avg_loss:0.163, val_acc:0.944]
Epoch [46/120    avg_loss:0.136, val_acc:0.928]
Epoch [47/120    avg_loss:0.155, val_acc:0.937]
Epoch [48/120    avg_loss:0.085, val_acc:0.935]
Epoch [49/120    avg_loss:0.090, val_acc:0.942]
Epoch [50/120    avg_loss:0.075, val_acc:0.963]
Epoch [51/120    avg_loss:0.064, val_acc:0.963]
Epoch [52/120    avg_loss:0.056, val_acc:0.967]
Epoch [53/120    avg_loss:0.055, val_acc:0.968]
Epoch [54/120    avg_loss:0.065, val_acc:0.965]
Epoch [55/120    avg_loss:0.053, val_acc:0.967]
Epoch [56/120    avg_loss:0.054, val_acc:0.966]
Epoch [57/120    avg_loss:0.053, val_acc:0.963]
Epoch [58/120    avg_loss:0.042, val_acc:0.967]
Epoch [59/120    avg_loss:0.047, val_acc:0.967]
Epoch [60/120    avg_loss:0.043, val_acc:0.965]
Epoch [61/120    avg_loss:0.064, val_acc:0.970]
Epoch [62/120    avg_loss:0.048, val_acc:0.966]
Epoch [63/120    avg_loss:0.047, val_acc:0.965]
Epoch [64/120    avg_loss:0.044, val_acc:0.966]
Epoch [65/120    avg_loss:0.048, val_acc:0.966]
Epoch [66/120    avg_loss:0.046, val_acc:0.961]
Epoch [67/120    avg_loss:0.049, val_acc:0.962]
Epoch [68/120    avg_loss:0.044, val_acc:0.962]
Epoch [69/120    avg_loss:0.041, val_acc:0.967]
Epoch [70/120    avg_loss:0.045, val_acc:0.967]
Epoch [71/120    avg_loss:0.042, val_acc:0.966]
Epoch [72/120    avg_loss:0.047, val_acc:0.968]
Epoch [73/120    avg_loss:0.036, val_acc:0.970]
Epoch [74/120    avg_loss:0.048, val_acc:0.971]
Epoch [75/120    avg_loss:0.038, val_acc:0.969]
Epoch [76/120    avg_loss:0.043, val_acc:0.970]
Epoch [77/120    avg_loss:0.038, val_acc:0.972]
Epoch [78/120    avg_loss:0.039, val_acc:0.971]
Epoch [79/120    avg_loss:0.051, val_acc:0.973]
Epoch [80/120    avg_loss:0.036, val_acc:0.972]
Epoch [81/120    avg_loss:0.040, val_acc:0.969]
Epoch [82/120    avg_loss:0.041, val_acc:0.971]
Epoch [83/120    avg_loss:0.034, val_acc:0.972]
Epoch [84/120    avg_loss:0.033, val_acc:0.972]
Epoch [85/120    avg_loss:0.042, val_acc:0.973]
Epoch [86/120    avg_loss:0.039, val_acc:0.972]
Epoch [87/120    avg_loss:0.042, val_acc:0.975]
Epoch [88/120    avg_loss:0.035, val_acc:0.972]
Epoch [89/120    avg_loss:0.035, val_acc:0.970]
Epoch [90/120    avg_loss:0.039, val_acc:0.971]
Epoch [91/120    avg_loss:0.042, val_acc:0.973]
Epoch [92/120    avg_loss:0.036, val_acc:0.969]
Epoch [93/120    avg_loss:0.040, val_acc:0.969]
Epoch [94/120    avg_loss:0.041, val_acc:0.970]
Epoch [95/120    avg_loss:0.032, val_acc:0.970]
Epoch [96/120    avg_loss:0.036, val_acc:0.967]
Epoch [97/120    avg_loss:0.031, val_acc:0.968]
Epoch [98/120    avg_loss:0.034, val_acc:0.976]
Epoch [99/120    avg_loss:0.044, val_acc:0.972]
Epoch [100/120    avg_loss:0.033, val_acc:0.976]
Epoch [101/120    avg_loss:0.030, val_acc:0.965]
Epoch [102/120    avg_loss:0.041, val_acc:0.966]
Epoch [103/120    avg_loss:0.033, val_acc:0.976]
Epoch [104/120    avg_loss:0.035, val_acc:0.978]
Epoch [105/120    avg_loss:0.036, val_acc:0.969]
Epoch [106/120    avg_loss:0.030, val_acc:0.973]
Epoch [107/120    avg_loss:0.030, val_acc:0.981]
Epoch [108/120    avg_loss:0.036, val_acc:0.982]
Epoch [109/120    avg_loss:0.033, val_acc:0.979]
Epoch [110/120    avg_loss:0.035, val_acc:0.977]
Epoch [111/120    avg_loss:0.034, val_acc:0.975]
Epoch [112/120    avg_loss:0.047, val_acc:0.972]
Epoch [113/120    avg_loss:0.041, val_acc:0.972]
Epoch [114/120    avg_loss:0.034, val_acc:0.975]
Epoch [115/120    avg_loss:0.030, val_acc:0.975]
Epoch [116/120    avg_loss:0.030, val_acc:0.977]
Epoch [117/120    avg_loss:0.032, val_acc:0.975]
Epoch [118/120    avg_loss:0.031, val_acc:0.971]
Epoch [119/120    avg_loss:0.035, val_acc:0.978]
Epoch [120/120    avg_loss:0.036, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1254    0    0    0    1    0    0    1    6   12    5    0
     0    6    0]
 [   0    0    0  714    4   17    0    0    0   10    0    0    0    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    2    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   14    0    0    1    0
     0    0    0]
 [   0    0   35   89    0    3    0    0    0    0  738    0    0    0
     1    9    0]
 [   0    0   11    0    0    2   21    0    0    0   27 2147    0    2
     0    0    0]
 [   0    0    0   24    0    8    0    0    0    0    4   13  479    0
     0    4    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    3    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0    6    0    0    2    0    0    1    0
   131  207    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.79674796747967

F1 scores:
[       nan 0.98765432 0.96983759 0.9055168  0.99069767 0.95856663
 0.97684839 0.96153846 0.99767981 0.59574468 0.89292196 0.97969427
 0.93829579 0.98930481 0.94107884 0.72251309 0.98224852]

Kappa:
0.94065566166029
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3505961860>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.357, val_acc:0.516]
Epoch [2/120    avg_loss:1.906, val_acc:0.551]
Epoch [3/120    avg_loss:1.676, val_acc:0.646]
Epoch [4/120    avg_loss:1.367, val_acc:0.691]
Epoch [5/120    avg_loss:1.163, val_acc:0.715]
Epoch [6/120    avg_loss:1.012, val_acc:0.768]
Epoch [7/120    avg_loss:0.860, val_acc:0.703]
Epoch [8/120    avg_loss:0.853, val_acc:0.776]
Epoch [9/120    avg_loss:0.738, val_acc:0.732]
Epoch [10/120    avg_loss:0.662, val_acc:0.751]
Epoch [11/120    avg_loss:0.598, val_acc:0.816]
Epoch [12/120    avg_loss:0.521, val_acc:0.865]
Epoch [13/120    avg_loss:0.490, val_acc:0.860]
Epoch [14/120    avg_loss:0.441, val_acc:0.841]
Epoch [15/120    avg_loss:0.399, val_acc:0.868]
Epoch [16/120    avg_loss:0.393, val_acc:0.837]
Epoch [17/120    avg_loss:0.476, val_acc:0.847]
Epoch [18/120    avg_loss:0.421, val_acc:0.875]
Epoch [19/120    avg_loss:0.329, val_acc:0.887]
Epoch [20/120    avg_loss:0.314, val_acc:0.883]
Epoch [21/120    avg_loss:0.290, val_acc:0.910]
Epoch [22/120    avg_loss:0.296, val_acc:0.891]
Epoch [23/120    avg_loss:0.278, val_acc:0.856]
Epoch [24/120    avg_loss:0.262, val_acc:0.883]
Epoch [25/120    avg_loss:0.216, val_acc:0.901]
Epoch [26/120    avg_loss:0.263, val_acc:0.918]
Epoch [27/120    avg_loss:0.235, val_acc:0.920]
Epoch [28/120    avg_loss:0.197, val_acc:0.917]
Epoch [29/120    avg_loss:0.201, val_acc:0.931]
Epoch [30/120    avg_loss:0.194, val_acc:0.925]
Epoch [31/120    avg_loss:0.342, val_acc:0.909]
Epoch [32/120    avg_loss:0.245, val_acc:0.921]
Epoch [33/120    avg_loss:0.228, val_acc:0.919]
Epoch [34/120    avg_loss:0.199, val_acc:0.926]
Epoch [35/120    avg_loss:0.141, val_acc:0.923]
Epoch [36/120    avg_loss:0.187, val_acc:0.936]
Epoch [37/120    avg_loss:0.122, val_acc:0.950]
Epoch [38/120    avg_loss:0.167, val_acc:0.936]
Epoch [39/120    avg_loss:0.115, val_acc:0.958]
Epoch [40/120    avg_loss:0.133, val_acc:0.948]
Epoch [41/120    avg_loss:0.131, val_acc:0.927]
Epoch [42/120    avg_loss:0.155, val_acc:0.925]
Epoch [43/120    avg_loss:0.120, val_acc:0.956]
Epoch [44/120    avg_loss:0.093, val_acc:0.944]
Epoch [45/120    avg_loss:0.074, val_acc:0.941]
Epoch [46/120    avg_loss:0.089, val_acc:0.940]
Epoch [47/120    avg_loss:0.088, val_acc:0.952]
Epoch [48/120    avg_loss:0.109, val_acc:0.936]
Epoch [49/120    avg_loss:0.119, val_acc:0.944]
Epoch [50/120    avg_loss:0.081, val_acc:0.970]
Epoch [51/120    avg_loss:0.070, val_acc:0.963]
Epoch [52/120    avg_loss:0.056, val_acc:0.971]
Epoch [53/120    avg_loss:0.052, val_acc:0.954]
Epoch [54/120    avg_loss:0.090, val_acc:0.964]
Epoch [55/120    avg_loss:0.073, val_acc:0.955]
Epoch [56/120    avg_loss:0.056, val_acc:0.958]
Epoch [57/120    avg_loss:0.063, val_acc:0.934]
Epoch [58/120    avg_loss:0.052, val_acc:0.965]
Epoch [59/120    avg_loss:0.058, val_acc:0.953]
Epoch [60/120    avg_loss:0.059, val_acc:0.967]
Epoch [61/120    avg_loss:0.052, val_acc:0.954]
Epoch [62/120    avg_loss:0.050, val_acc:0.967]
Epoch [63/120    avg_loss:0.057, val_acc:0.955]
Epoch [64/120    avg_loss:0.086, val_acc:0.958]
Epoch [65/120    avg_loss:0.043, val_acc:0.971]
Epoch [66/120    avg_loss:0.034, val_acc:0.976]
Epoch [67/120    avg_loss:0.030, val_acc:0.972]
Epoch [68/120    avg_loss:0.051, val_acc:0.964]
Epoch [69/120    avg_loss:0.050, val_acc:0.966]
Epoch [70/120    avg_loss:0.056, val_acc:0.963]
Epoch [71/120    avg_loss:0.044, val_acc:0.976]
Epoch [72/120    avg_loss:0.028, val_acc:0.976]
Epoch [73/120    avg_loss:0.041, val_acc:0.966]
Epoch [74/120    avg_loss:0.036, val_acc:0.966]
Epoch [75/120    avg_loss:0.036, val_acc:0.973]
Epoch [76/120    avg_loss:0.043, val_acc:0.974]
Epoch [77/120    avg_loss:0.030, val_acc:0.976]
Epoch [78/120    avg_loss:0.039, val_acc:0.968]
Epoch [79/120    avg_loss:0.025, val_acc:0.976]
Epoch [80/120    avg_loss:0.025, val_acc:0.979]
Epoch [81/120    avg_loss:0.038, val_acc:0.976]
Epoch [82/120    avg_loss:0.088, val_acc:0.922]
Epoch [83/120    avg_loss:0.182, val_acc:0.946]
Epoch [84/120    avg_loss:0.073, val_acc:0.956]
Epoch [85/120    avg_loss:0.033, val_acc:0.966]
Epoch [86/120    avg_loss:0.051, val_acc:0.972]
Epoch [87/120    avg_loss:0.033, val_acc:0.982]
Epoch [88/120    avg_loss:0.027, val_acc:0.974]
Epoch [89/120    avg_loss:0.027, val_acc:0.989]
Epoch [90/120    avg_loss:0.019, val_acc:0.977]
Epoch [91/120    avg_loss:0.018, val_acc:0.974]
Epoch [92/120    avg_loss:0.020, val_acc:0.983]
Epoch [93/120    avg_loss:0.017, val_acc:0.983]
Epoch [94/120    avg_loss:0.022, val_acc:0.979]
Epoch [95/120    avg_loss:0.018, val_acc:0.981]
Epoch [96/120    avg_loss:0.028, val_acc:0.971]
Epoch [97/120    avg_loss:0.048, val_acc:0.940]
Epoch [98/120    avg_loss:0.066, val_acc:0.970]
Epoch [99/120    avg_loss:0.028, val_acc:0.962]
Epoch [100/120    avg_loss:0.034, val_acc:0.977]
Epoch [101/120    avg_loss:0.016, val_acc:0.982]
Epoch [102/120    avg_loss:0.017, val_acc:0.981]
Epoch [103/120    avg_loss:0.016, val_acc:0.984]
Epoch [104/120    avg_loss:0.011, val_acc:0.984]
Epoch [105/120    avg_loss:0.013, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.013, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.984]
Epoch [109/120    avg_loss:0.009, val_acc:0.983]
Epoch [110/120    avg_loss:0.009, val_acc:0.984]
Epoch [111/120    avg_loss:0.010, val_acc:0.985]
Epoch [112/120    avg_loss:0.011, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.985]
Epoch [114/120    avg_loss:0.011, val_acc:0.985]
Epoch [115/120    avg_loss:0.009, val_acc:0.986]
Epoch [116/120    avg_loss:0.009, val_acc:0.986]
Epoch [117/120    avg_loss:0.010, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.009, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1258    6    0    0    2    0    0    0    4   10    3    0
     0    2    0]
 [   0    0    1  722    0   11    0    0    0    3    0    0    3    7
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    3    0    3    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3   51    0    1    1    0    0    0  797   16    2    0
     0    4    0]
 [   0    0    3    0    0    0    3    0    0    0   18 2177    8    1
     0    0    0]
 [   0    0    0   22    0    2    0    0    0    0    1    9  497    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    3    0    1    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0   28    0    0    0    0    0    0    0
    78  241    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.50948509485094

F1 scores:
[       nan 0.98765432 0.98666667 0.93281654 0.99764706 0.97371429
 0.97477745 0.94339623 0.99652375 0.85714286 0.93930466 0.98439973
 0.94847328 0.97883598 0.96390658 0.81008403 0.98224852]

Kappa:
0.9601810306314705
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc59b4c0780>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.370, val_acc:0.407]
Epoch [2/120    avg_loss:1.907, val_acc:0.536]
Epoch [3/120    avg_loss:1.635, val_acc:0.613]
Epoch [4/120    avg_loss:1.407, val_acc:0.656]
Epoch [5/120    avg_loss:1.231, val_acc:0.588]
Epoch [6/120    avg_loss:1.040, val_acc:0.718]
Epoch [7/120    avg_loss:0.864, val_acc:0.719]
Epoch [8/120    avg_loss:0.877, val_acc:0.625]
Epoch [9/120    avg_loss:0.860, val_acc:0.778]
Epoch [10/120    avg_loss:0.717, val_acc:0.741]
Epoch [11/120    avg_loss:0.644, val_acc:0.807]
Epoch [12/120    avg_loss:0.621, val_acc:0.814]
Epoch [13/120    avg_loss:0.621, val_acc:0.762]
Epoch [14/120    avg_loss:0.542, val_acc:0.852]
Epoch [15/120    avg_loss:0.511, val_acc:0.835]
Epoch [16/120    avg_loss:0.547, val_acc:0.827]
Epoch [17/120    avg_loss:0.422, val_acc:0.870]
Epoch [18/120    avg_loss:0.476, val_acc:0.845]
Epoch [19/120    avg_loss:0.400, val_acc:0.858]
Epoch [20/120    avg_loss:0.362, val_acc:0.882]
Epoch [21/120    avg_loss:0.330, val_acc:0.867]
Epoch [22/120    avg_loss:0.333, val_acc:0.909]
Epoch [23/120    avg_loss:0.349, val_acc:0.872]
Epoch [24/120    avg_loss:0.313, val_acc:0.898]
Epoch [25/120    avg_loss:0.261, val_acc:0.922]
Epoch [26/120    avg_loss:0.234, val_acc:0.930]
Epoch [27/120    avg_loss:0.207, val_acc:0.932]
Epoch [28/120    avg_loss:0.273, val_acc:0.915]
Epoch [29/120    avg_loss:0.237, val_acc:0.920]
Epoch [30/120    avg_loss:0.263, val_acc:0.922]
Epoch [31/120    avg_loss:0.190, val_acc:0.934]
Epoch [32/120    avg_loss:0.181, val_acc:0.926]
Epoch [33/120    avg_loss:0.194, val_acc:0.926]
Epoch [34/120    avg_loss:0.237, val_acc:0.942]
Epoch [35/120    avg_loss:0.174, val_acc:0.926]
Epoch [36/120    avg_loss:0.171, val_acc:0.935]
Epoch [37/120    avg_loss:0.154, val_acc:0.939]
Epoch [38/120    avg_loss:0.166, val_acc:0.943]
Epoch [39/120    avg_loss:0.122, val_acc:0.952]
Epoch [40/120    avg_loss:0.159, val_acc:0.940]
Epoch [41/120    avg_loss:0.121, val_acc:0.945]
Epoch [42/120    avg_loss:0.123, val_acc:0.950]
Epoch [43/120    avg_loss:0.112, val_acc:0.950]
Epoch [44/120    avg_loss:0.126, val_acc:0.942]
Epoch [45/120    avg_loss:0.187, val_acc:0.926]
Epoch [46/120    avg_loss:0.130, val_acc:0.943]
Epoch [47/120    avg_loss:0.094, val_acc:0.952]
Epoch [48/120    avg_loss:0.098, val_acc:0.939]
Epoch [49/120    avg_loss:0.080, val_acc:0.958]
Epoch [50/120    avg_loss:0.083, val_acc:0.959]
Epoch [51/120    avg_loss:0.140, val_acc:0.945]
Epoch [52/120    avg_loss:0.109, val_acc:0.957]
Epoch [53/120    avg_loss:0.092, val_acc:0.953]
Epoch [54/120    avg_loss:0.111, val_acc:0.955]
Epoch [55/120    avg_loss:0.116, val_acc:0.951]
Epoch [56/120    avg_loss:0.086, val_acc:0.963]
Epoch [57/120    avg_loss:0.066, val_acc:0.960]
Epoch [58/120    avg_loss:0.151, val_acc:0.951]
Epoch [59/120    avg_loss:0.084, val_acc:0.961]
Epoch [60/120    avg_loss:0.065, val_acc:0.953]
Epoch [61/120    avg_loss:0.079, val_acc:0.960]
Epoch [62/120    avg_loss:0.085, val_acc:0.963]
Epoch [63/120    avg_loss:0.060, val_acc:0.968]
Epoch [64/120    avg_loss:0.046, val_acc:0.967]
Epoch [65/120    avg_loss:0.053, val_acc:0.959]
Epoch [66/120    avg_loss:0.084, val_acc:0.957]
Epoch [67/120    avg_loss:0.093, val_acc:0.918]
Epoch [68/120    avg_loss:0.161, val_acc:0.887]
Epoch [69/120    avg_loss:0.236, val_acc:0.920]
Epoch [70/120    avg_loss:0.106, val_acc:0.966]
Epoch [71/120    avg_loss:0.104, val_acc:0.944]
Epoch [72/120    avg_loss:0.069, val_acc:0.968]
Epoch [73/120    avg_loss:0.070, val_acc:0.964]
Epoch [74/120    avg_loss:0.036, val_acc:0.964]
Epoch [75/120    avg_loss:0.041, val_acc:0.973]
Epoch [76/120    avg_loss:0.034, val_acc:0.973]
Epoch [77/120    avg_loss:0.057, val_acc:0.964]
Epoch [78/120    avg_loss:0.047, val_acc:0.976]
Epoch [79/120    avg_loss:0.037, val_acc:0.972]
Epoch [80/120    avg_loss:0.075, val_acc:0.948]
Epoch [81/120    avg_loss:0.063, val_acc:0.951]
Epoch [82/120    avg_loss:0.048, val_acc:0.981]
Epoch [83/120    avg_loss:0.031, val_acc:0.965]
Epoch [84/120    avg_loss:0.035, val_acc:0.978]
Epoch [85/120    avg_loss:0.028, val_acc:0.977]
Epoch [86/120    avg_loss:0.019, val_acc:0.974]
Epoch [87/120    avg_loss:0.026, val_acc:0.976]
Epoch [88/120    avg_loss:0.048, val_acc:0.974]
Epoch [89/120    avg_loss:0.027, val_acc:0.980]
Epoch [90/120    avg_loss:0.028, val_acc:0.974]
Epoch [91/120    avg_loss:0.027, val_acc:0.972]
Epoch [92/120    avg_loss:0.026, val_acc:0.970]
Epoch [93/120    avg_loss:0.023, val_acc:0.974]
Epoch [94/120    avg_loss:0.018, val_acc:0.981]
Epoch [95/120    avg_loss:0.024, val_acc:0.977]
Epoch [96/120    avg_loss:0.047, val_acc:0.959]
Epoch [97/120    avg_loss:0.089, val_acc:0.963]
Epoch [98/120    avg_loss:0.046, val_acc:0.963]
Epoch [99/120    avg_loss:0.071, val_acc:0.941]
Epoch [100/120    avg_loss:0.054, val_acc:0.966]
Epoch [101/120    avg_loss:0.028, val_acc:0.969]
Epoch [102/120    avg_loss:0.019, val_acc:0.981]
Epoch [103/120    avg_loss:0.015, val_acc:0.980]
Epoch [104/120    avg_loss:0.021, val_acc:0.977]
Epoch [105/120    avg_loss:0.022, val_acc:0.976]
Epoch [106/120    avg_loss:0.022, val_acc:0.969]
Epoch [107/120    avg_loss:0.016, val_acc:0.983]
Epoch [108/120    avg_loss:0.019, val_acc:0.976]
Epoch [109/120    avg_loss:0.024, val_acc:0.978]
Epoch [110/120    avg_loss:0.016, val_acc:0.977]
Epoch [111/120    avg_loss:0.018, val_acc:0.982]
Epoch [112/120    avg_loss:0.012, val_acc:0.981]
Epoch [113/120    avg_loss:0.033, val_acc:0.982]
Epoch [114/120    avg_loss:0.012, val_acc:0.984]
Epoch [115/120    avg_loss:0.010, val_acc:0.980]
Epoch [116/120    avg_loss:0.013, val_acc:0.980]
Epoch [117/120    avg_loss:0.010, val_acc:0.977]
Epoch [118/120    avg_loss:0.013, val_acc:0.978]
Epoch [119/120    avg_loss:0.012, val_acc:0.984]
Epoch [120/120    avg_loss:0.012, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1259    0    0    1    0    0    0    0    0   15    0    0
     0   10    0]
 [   0    0    0  742    1    0    0    0    0    2    0    0    2    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    7    0    0    0    0    0    0  423    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   14    0    0    3    0
     0    0    0]
 [   0    0   11   69    0    1    0    0    0    0  755   28    0    0
     0   11    0]
 [   0    0    7    0    0    0    3    0    1    0    4 2194    0    1
     0    0    0]
 [   0    0    0    3    6    3    0    0    3    0    3    1  513    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    0    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   129  218    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.32520325203252

F1 scores:
[       nan 0.89655172 0.98244245 0.95006402 0.98148148 0.98858447
 0.9969651  0.98039216 0.98601399 0.8        0.92185592 0.98651079
 0.97343454 0.99730458 0.94465252 0.7440273  0.97619048]

Kappa:
0.9580426231484614
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f97d382c7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.304, val_acc:0.534]
Epoch [2/120    avg_loss:1.845, val_acc:0.560]
Epoch [3/120    avg_loss:1.603, val_acc:0.618]
Epoch [4/120    avg_loss:1.353, val_acc:0.654]
Epoch [5/120    avg_loss:1.102, val_acc:0.635]
Epoch [6/120    avg_loss:0.965, val_acc:0.712]
Epoch [7/120    avg_loss:0.860, val_acc:0.731]
Epoch [8/120    avg_loss:0.715, val_acc:0.741]
Epoch [9/120    avg_loss:0.703, val_acc:0.766]
Epoch [10/120    avg_loss:0.748, val_acc:0.765]
Epoch [11/120    avg_loss:0.599, val_acc:0.823]
Epoch [12/120    avg_loss:0.557, val_acc:0.803]
Epoch [13/120    avg_loss:0.491, val_acc:0.842]
Epoch [14/120    avg_loss:0.456, val_acc:0.832]
Epoch [15/120    avg_loss:0.430, val_acc:0.839]
Epoch [16/120    avg_loss:0.363, val_acc:0.841]
Epoch [17/120    avg_loss:0.379, val_acc:0.856]
Epoch [18/120    avg_loss:0.379, val_acc:0.851]
Epoch [19/120    avg_loss:0.311, val_acc:0.867]
Epoch [20/120    avg_loss:0.327, val_acc:0.837]
Epoch [21/120    avg_loss:0.340, val_acc:0.878]
Epoch [22/120    avg_loss:0.317, val_acc:0.916]
Epoch [23/120    avg_loss:0.224, val_acc:0.876]
Epoch [24/120    avg_loss:0.320, val_acc:0.893]
Epoch [25/120    avg_loss:0.330, val_acc:0.903]
Epoch [26/120    avg_loss:0.241, val_acc:0.887]
Epoch [27/120    avg_loss:0.229, val_acc:0.913]
Epoch [28/120    avg_loss:0.199, val_acc:0.908]
Epoch [29/120    avg_loss:0.242, val_acc:0.923]
Epoch [30/120    avg_loss:0.200, val_acc:0.902]
Epoch [31/120    avg_loss:0.203, val_acc:0.938]
Epoch [32/120    avg_loss:0.153, val_acc:0.911]
Epoch [33/120    avg_loss:0.217, val_acc:0.870]
Epoch [34/120    avg_loss:0.226, val_acc:0.932]
Epoch [35/120    avg_loss:0.171, val_acc:0.908]
Epoch [36/120    avg_loss:0.393, val_acc:0.712]
Epoch [37/120    avg_loss:0.335, val_acc:0.910]
Epoch [38/120    avg_loss:0.189, val_acc:0.937]
Epoch [39/120    avg_loss:0.207, val_acc:0.929]
Epoch [40/120    avg_loss:0.129, val_acc:0.937]
Epoch [41/120    avg_loss:0.166, val_acc:0.913]
Epoch [42/120    avg_loss:0.147, val_acc:0.921]
Epoch [43/120    avg_loss:0.113, val_acc:0.956]
Epoch [44/120    avg_loss:0.182, val_acc:0.927]
Epoch [45/120    avg_loss:0.115, val_acc:0.938]
Epoch [46/120    avg_loss:0.139, val_acc:0.940]
Epoch [47/120    avg_loss:0.132, val_acc:0.947]
Epoch [48/120    avg_loss:0.172, val_acc:0.941]
Epoch [49/120    avg_loss:0.120, val_acc:0.955]
Epoch [50/120    avg_loss:0.143, val_acc:0.948]
Epoch [51/120    avg_loss:0.136, val_acc:0.930]
Epoch [52/120    avg_loss:0.159, val_acc:0.941]
Epoch [53/120    avg_loss:0.123, val_acc:0.946]
Epoch [54/120    avg_loss:0.111, val_acc:0.945]
Epoch [55/120    avg_loss:0.086, val_acc:0.967]
Epoch [56/120    avg_loss:0.065, val_acc:0.958]
Epoch [57/120    avg_loss:0.066, val_acc:0.965]
Epoch [58/120    avg_loss:0.059, val_acc:0.963]
Epoch [59/120    avg_loss:0.073, val_acc:0.965]
Epoch [60/120    avg_loss:0.089, val_acc:0.963]
Epoch [61/120    avg_loss:0.072, val_acc:0.963]
Epoch [62/120    avg_loss:0.061, val_acc:0.968]
Epoch [63/120    avg_loss:0.056, val_acc:0.965]
Epoch [64/120    avg_loss:0.074, val_acc:0.964]
Epoch [65/120    avg_loss:0.082, val_acc:0.956]
Epoch [66/120    avg_loss:0.159, val_acc:0.962]
Epoch [67/120    avg_loss:0.060, val_acc:0.968]
Epoch [68/120    avg_loss:0.078, val_acc:0.956]
Epoch [69/120    avg_loss:0.093, val_acc:0.927]
Epoch [70/120    avg_loss:0.072, val_acc:0.957]
Epoch [71/120    avg_loss:0.071, val_acc:0.961]
Epoch [72/120    avg_loss:0.049, val_acc:0.971]
Epoch [73/120    avg_loss:0.062, val_acc:0.967]
Epoch [74/120    avg_loss:0.066, val_acc:0.963]
Epoch [75/120    avg_loss:0.044, val_acc:0.953]
Epoch [76/120    avg_loss:0.049, val_acc:0.965]
Epoch [77/120    avg_loss:0.029, val_acc:0.976]
Epoch [78/120    avg_loss:0.040, val_acc:0.965]
Epoch [79/120    avg_loss:0.036, val_acc:0.961]
Epoch [80/120    avg_loss:0.047, val_acc:0.977]
Epoch [81/120    avg_loss:0.035, val_acc:0.972]
Epoch [82/120    avg_loss:0.067, val_acc:0.963]
Epoch [83/120    avg_loss:0.052, val_acc:0.962]
Epoch [84/120    avg_loss:0.126, val_acc:0.961]
Epoch [85/120    avg_loss:0.062, val_acc:0.963]
Epoch [86/120    avg_loss:0.036, val_acc:0.968]
Epoch [87/120    avg_loss:0.035, val_acc:0.959]
Epoch [88/120    avg_loss:0.025, val_acc:0.972]
Epoch [89/120    avg_loss:0.025, val_acc:0.972]
Epoch [90/120    avg_loss:0.025, val_acc:0.973]
Epoch [91/120    avg_loss:0.034, val_acc:0.977]
Epoch [92/120    avg_loss:0.025, val_acc:0.980]
Epoch [93/120    avg_loss:0.022, val_acc:0.984]
Epoch [94/120    avg_loss:0.026, val_acc:0.977]
Epoch [95/120    avg_loss:0.024, val_acc:0.975]
Epoch [96/120    avg_loss:0.022, val_acc:0.979]
Epoch [97/120    avg_loss:0.023, val_acc:0.972]
Epoch [98/120    avg_loss:0.027, val_acc:0.958]
Epoch [99/120    avg_loss:0.027, val_acc:0.975]
Epoch [100/120    avg_loss:0.028, val_acc:0.968]
Epoch [101/120    avg_loss:0.030, val_acc:0.975]
Epoch [102/120    avg_loss:0.026, val_acc:0.965]
Epoch [103/120    avg_loss:0.020, val_acc:0.977]
Epoch [104/120    avg_loss:0.019, val_acc:0.988]
Epoch [105/120    avg_loss:0.021, val_acc:0.971]
Epoch [106/120    avg_loss:0.021, val_acc:0.981]
Epoch [107/120    avg_loss:0.015, val_acc:0.972]
Epoch [108/120    avg_loss:0.012, val_acc:0.976]
Epoch [109/120    avg_loss:0.016, val_acc:0.976]
Epoch [110/120    avg_loss:0.019, val_acc:0.985]
Epoch [111/120    avg_loss:0.014, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.986]
Epoch [113/120    avg_loss:0.011, val_acc:0.977]
Epoch [114/120    avg_loss:0.021, val_acc:0.977]
Epoch [115/120    avg_loss:0.013, val_acc:0.988]
Epoch [116/120    avg_loss:0.010, val_acc:0.983]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.019, val_acc:0.974]
Epoch [119/120    avg_loss:0.013, val_acc:0.979]
Epoch [120/120    avg_loss:0.019, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1226    1    0    0    1    0    0    0   33   22    1    0
     0    1    0]
 [   0    0    1  721    0   11    0    0    0    2    0    0   11    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0   11   90    0   29    0    0    0    0  715    8   10    0
     0   12    0]
 [   0    0    3    0    0    8    1    0    0    0    2 2184   10    2
     0    0    0]
 [   0    0    0    8    0   13    0    0    0    0    0    3  508    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
   141  203    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    7    0
     0    0   77]]

Accuracy:
94.95934959349593

F1 scores:
[       nan 0.98765432 0.97070467 0.92022974 1.         0.92572659
 0.99389313 1.         0.99883856 0.81081081 0.87891825 0.98644986
 0.93726937 0.9919571  0.93734542 0.72113677 0.95061728]

Kappa:
0.9424833099192678
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fde150687b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.411, val_acc:0.418]
Epoch [2/120    avg_loss:1.892, val_acc:0.587]
Epoch [3/120    avg_loss:1.649, val_acc:0.627]
Epoch [4/120    avg_loss:1.392, val_acc:0.659]
Epoch [5/120    avg_loss:1.145, val_acc:0.684]
Epoch [6/120    avg_loss:1.020, val_acc:0.723]
Epoch [7/120    avg_loss:0.930, val_acc:0.726]
Epoch [8/120    avg_loss:0.855, val_acc:0.768]
Epoch [9/120    avg_loss:0.785, val_acc:0.749]
Epoch [10/120    avg_loss:0.724, val_acc:0.776]
Epoch [11/120    avg_loss:0.643, val_acc:0.820]
Epoch [12/120    avg_loss:0.573, val_acc:0.819]
Epoch [13/120    avg_loss:0.598, val_acc:0.818]
Epoch [14/120    avg_loss:0.537, val_acc:0.850]
Epoch [15/120    avg_loss:0.573, val_acc:0.828]
Epoch [16/120    avg_loss:0.469, val_acc:0.857]
Epoch [17/120    avg_loss:0.429, val_acc:0.884]
Epoch [18/120    avg_loss:0.378, val_acc:0.825]
Epoch [19/120    avg_loss:0.430, val_acc:0.831]
Epoch [20/120    avg_loss:0.345, val_acc:0.858]
Epoch [21/120    avg_loss:0.341, val_acc:0.841]
Epoch [22/120    avg_loss:0.353, val_acc:0.894]
Epoch [23/120    avg_loss:0.324, val_acc:0.846]
Epoch [24/120    avg_loss:0.256, val_acc:0.923]
Epoch [25/120    avg_loss:0.312, val_acc:0.904]
Epoch [26/120    avg_loss:0.255, val_acc:0.895]
Epoch [27/120    avg_loss:0.300, val_acc:0.912]
Epoch [28/120    avg_loss:0.276, val_acc:0.895]
Epoch [29/120    avg_loss:0.200, val_acc:0.927]
Epoch [30/120    avg_loss:0.209, val_acc:0.905]
Epoch [31/120    avg_loss:0.219, val_acc:0.936]
Epoch [32/120    avg_loss:0.188, val_acc:0.895]
Epoch [33/120    avg_loss:0.240, val_acc:0.946]
Epoch [34/120    avg_loss:0.129, val_acc:0.929]
Epoch [35/120    avg_loss:0.187, val_acc:0.899]
Epoch [36/120    avg_loss:0.240, val_acc:0.882]
Epoch [37/120    avg_loss:0.215, val_acc:0.883]
Epoch [38/120    avg_loss:0.156, val_acc:0.936]
Epoch [39/120    avg_loss:0.127, val_acc:0.937]
Epoch [40/120    avg_loss:0.157, val_acc:0.914]
Epoch [41/120    avg_loss:0.171, val_acc:0.937]
Epoch [42/120    avg_loss:0.138, val_acc:0.944]
Epoch [43/120    avg_loss:0.124, val_acc:0.941]
Epoch [44/120    avg_loss:0.104, val_acc:0.934]
Epoch [45/120    avg_loss:0.127, val_acc:0.932]
Epoch [46/120    avg_loss:0.119, val_acc:0.946]
Epoch [47/120    avg_loss:0.140, val_acc:0.922]
Epoch [48/120    avg_loss:0.114, val_acc:0.943]
Epoch [49/120    avg_loss:0.090, val_acc:0.931]
Epoch [50/120    avg_loss:0.140, val_acc:0.948]
Epoch [51/120    avg_loss:0.133, val_acc:0.932]
Epoch [52/120    avg_loss:0.092, val_acc:0.940]
Epoch [53/120    avg_loss:0.098, val_acc:0.946]
Epoch [54/120    avg_loss:0.119, val_acc:0.926]
Epoch [55/120    avg_loss:0.178, val_acc:0.939]
Epoch [56/120    avg_loss:0.108, val_acc:0.959]
Epoch [57/120    avg_loss:0.087, val_acc:0.950]
Epoch [58/120    avg_loss:0.066, val_acc:0.973]
Epoch [59/120    avg_loss:0.114, val_acc:0.937]
Epoch [60/120    avg_loss:0.114, val_acc:0.921]
Epoch [61/120    avg_loss:0.068, val_acc:0.965]
Epoch [62/120    avg_loss:0.058, val_acc:0.964]
Epoch [63/120    avg_loss:0.060, val_acc:0.968]
Epoch [64/120    avg_loss:0.055, val_acc:0.971]
Epoch [65/120    avg_loss:0.055, val_acc:0.972]
Epoch [66/120    avg_loss:0.041, val_acc:0.959]
Epoch [67/120    avg_loss:0.085, val_acc:0.943]
Epoch [68/120    avg_loss:0.051, val_acc:0.955]
Epoch [69/120    avg_loss:0.049, val_acc:0.968]
Epoch [70/120    avg_loss:0.042, val_acc:0.947]
Epoch [71/120    avg_loss:0.040, val_acc:0.967]
Epoch [72/120    avg_loss:0.041, val_acc:0.972]
Epoch [73/120    avg_loss:0.037, val_acc:0.970]
Epoch [74/120    avg_loss:0.024, val_acc:0.970]
Epoch [75/120    avg_loss:0.021, val_acc:0.973]
Epoch [76/120    avg_loss:0.023, val_acc:0.974]
Epoch [77/120    avg_loss:0.021, val_acc:0.972]
Epoch [78/120    avg_loss:0.028, val_acc:0.975]
Epoch [79/120    avg_loss:0.021, val_acc:0.973]
Epoch [80/120    avg_loss:0.026, val_acc:0.976]
Epoch [81/120    avg_loss:0.028, val_acc:0.970]
Epoch [82/120    avg_loss:0.021, val_acc:0.972]
Epoch [83/120    avg_loss:0.020, val_acc:0.971]
Epoch [84/120    avg_loss:0.016, val_acc:0.974]
Epoch [85/120    avg_loss:0.020, val_acc:0.973]
Epoch [86/120    avg_loss:0.022, val_acc:0.975]
Epoch [87/120    avg_loss:0.016, val_acc:0.976]
Epoch [88/120    avg_loss:0.023, val_acc:0.976]
Epoch [89/120    avg_loss:0.020, val_acc:0.972]
Epoch [90/120    avg_loss:0.021, val_acc:0.973]
Epoch [91/120    avg_loss:0.021, val_acc:0.974]
Epoch [92/120    avg_loss:0.016, val_acc:0.972]
Epoch [93/120    avg_loss:0.019, val_acc:0.974]
Epoch [94/120    avg_loss:0.020, val_acc:0.975]
Epoch [95/120    avg_loss:0.017, val_acc:0.975]
Epoch [96/120    avg_loss:0.020, val_acc:0.977]
Epoch [97/120    avg_loss:0.019, val_acc:0.980]
Epoch [98/120    avg_loss:0.018, val_acc:0.977]
Epoch [99/120    avg_loss:0.019, val_acc:0.980]
Epoch [100/120    avg_loss:0.019, val_acc:0.977]
Epoch [101/120    avg_loss:0.016, val_acc:0.979]
Epoch [102/120    avg_loss:0.015, val_acc:0.980]
Epoch [103/120    avg_loss:0.019, val_acc:0.975]
Epoch [104/120    avg_loss:0.018, val_acc:0.977]
Epoch [105/120    avg_loss:0.021, val_acc:0.976]
Epoch [106/120    avg_loss:0.018, val_acc:0.977]
Epoch [107/120    avg_loss:0.016, val_acc:0.979]
Epoch [108/120    avg_loss:0.015, val_acc:0.974]
Epoch [109/120    avg_loss:0.019, val_acc:0.977]
Epoch [110/120    avg_loss:0.017, val_acc:0.975]
Epoch [111/120    avg_loss:0.017, val_acc:0.980]
Epoch [112/120    avg_loss:0.022, val_acc:0.975]
Epoch [113/120    avg_loss:0.016, val_acc:0.979]
Epoch [114/120    avg_loss:0.016, val_acc:0.976]
Epoch [115/120    avg_loss:0.014, val_acc:0.975]
Epoch [116/120    avg_loss:0.021, val_acc:0.974]
Epoch [117/120    avg_loss:0.017, val_acc:0.979]
Epoch [118/120    avg_loss:0.018, val_acc:0.975]
Epoch [119/120    avg_loss:0.014, val_acc:0.977]
Epoch [120/120    avg_loss:0.013, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1244    2    0    3    0    0    0    0   16   18    1    0
     1    0    0]
 [   0    0    1  722    2   13    0    0    0    3    0    0    6    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    3    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   12   90    0    4    0    0    0    0  749   16    0    0
     0    4    0]
 [   0    0    4    0    0    1   11    0    0    0   14 2178    0    2
     0    0    0]
 [   0    0    0   13    6    4    0    0    0    0    9    3  496    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    2    1    0    0
  1128    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
   149  195    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.27371273712737

F1 scores:
[       nan 0.975      0.97721917 0.9168254  0.97921478 0.95758929
 0.98869631 1.         0.99883856 0.85714286 0.89862028 0.98396205
 0.95568401 0.99462366 0.9322314  0.71428571 0.97647059]

Kappa:
0.9460515577621079
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff93472b780>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.354, val_acc:0.440]
Epoch [2/120    avg_loss:1.912, val_acc:0.551]
Epoch [3/120    avg_loss:1.697, val_acc:0.597]
Epoch [4/120    avg_loss:1.542, val_acc:0.635]
Epoch [5/120    avg_loss:1.326, val_acc:0.683]
Epoch [6/120    avg_loss:1.065, val_acc:0.715]
Epoch [7/120    avg_loss:0.884, val_acc:0.701]
Epoch [8/120    avg_loss:0.827, val_acc:0.732]
Epoch [9/120    avg_loss:0.716, val_acc:0.763]
Epoch [10/120    avg_loss:0.687, val_acc:0.806]
Epoch [11/120    avg_loss:0.598, val_acc:0.799]
Epoch [12/120    avg_loss:0.583, val_acc:0.822]
Epoch [13/120    avg_loss:0.544, val_acc:0.833]
Epoch [14/120    avg_loss:0.462, val_acc:0.835]
Epoch [15/120    avg_loss:0.465, val_acc:0.848]
Epoch [16/120    avg_loss:0.409, val_acc:0.873]
Epoch [17/120    avg_loss:0.381, val_acc:0.846]
Epoch [18/120    avg_loss:0.302, val_acc:0.886]
Epoch [19/120    avg_loss:0.333, val_acc:0.866]
Epoch [20/120    avg_loss:0.369, val_acc:0.890]
Epoch [21/120    avg_loss:0.311, val_acc:0.892]
Epoch [22/120    avg_loss:0.275, val_acc:0.867]
Epoch [23/120    avg_loss:0.305, val_acc:0.885]
Epoch [24/120    avg_loss:0.303, val_acc:0.889]
Epoch [25/120    avg_loss:0.210, val_acc:0.882]
Epoch [26/120    avg_loss:0.201, val_acc:0.916]
Epoch [27/120    avg_loss:0.237, val_acc:0.882]
Epoch [28/120    avg_loss:0.204, val_acc:0.935]
Epoch [29/120    avg_loss:0.187, val_acc:0.916]
Epoch [30/120    avg_loss:0.174, val_acc:0.924]
Epoch [31/120    avg_loss:0.179, val_acc:0.929]
Epoch [32/120    avg_loss:0.177, val_acc:0.914]
Epoch [33/120    avg_loss:0.173, val_acc:0.918]
Epoch [34/120    avg_loss:0.201, val_acc:0.918]
Epoch [35/120    avg_loss:0.161, val_acc:0.927]
Epoch [36/120    avg_loss:0.109, val_acc:0.937]
Epoch [37/120    avg_loss:0.230, val_acc:0.836]
Epoch [38/120    avg_loss:0.181, val_acc:0.903]
Epoch [39/120    avg_loss:0.126, val_acc:0.930]
Epoch [40/120    avg_loss:0.149, val_acc:0.902]
Epoch [41/120    avg_loss:0.187, val_acc:0.909]
Epoch [42/120    avg_loss:0.167, val_acc:0.907]
Epoch [43/120    avg_loss:0.185, val_acc:0.903]
Epoch [44/120    avg_loss:0.129, val_acc:0.938]
Epoch [45/120    avg_loss:0.149, val_acc:0.941]
Epoch [46/120    avg_loss:0.133, val_acc:0.917]
Epoch [47/120    avg_loss:0.111, val_acc:0.952]
Epoch [48/120    avg_loss:0.075, val_acc:0.940]
Epoch [49/120    avg_loss:0.114, val_acc:0.956]
Epoch [50/120    avg_loss:0.095, val_acc:0.927]
Epoch [51/120    avg_loss:0.098, val_acc:0.940]
Epoch [52/120    avg_loss:0.069, val_acc:0.950]
Epoch [53/120    avg_loss:0.064, val_acc:0.939]
Epoch [54/120    avg_loss:0.075, val_acc:0.944]
Epoch [55/120    avg_loss:0.076, val_acc:0.956]
Epoch [56/120    avg_loss:0.051, val_acc:0.970]
Epoch [57/120    avg_loss:0.054, val_acc:0.962]
Epoch [58/120    avg_loss:0.062, val_acc:0.939]
Epoch [59/120    avg_loss:0.058, val_acc:0.927]
Epoch [60/120    avg_loss:0.068, val_acc:0.952]
Epoch [61/120    avg_loss:0.076, val_acc:0.938]
Epoch [62/120    avg_loss:0.070, val_acc:0.940]
Epoch [63/120    avg_loss:0.088, val_acc:0.949]
Epoch [64/120    avg_loss:0.049, val_acc:0.965]
Epoch [65/120    avg_loss:0.046, val_acc:0.963]
Epoch [66/120    avg_loss:0.031, val_acc:0.961]
Epoch [67/120    avg_loss:0.038, val_acc:0.950]
Epoch [68/120    avg_loss:0.061, val_acc:0.981]
Epoch [69/120    avg_loss:0.037, val_acc:0.968]
Epoch [70/120    avg_loss:0.049, val_acc:0.948]
Epoch [71/120    avg_loss:0.073, val_acc:0.940]
Epoch [72/120    avg_loss:0.049, val_acc:0.968]
Epoch [73/120    avg_loss:0.040, val_acc:0.963]
Epoch [74/120    avg_loss:0.028, val_acc:0.975]
Epoch [75/120    avg_loss:0.030, val_acc:0.954]
Epoch [76/120    avg_loss:0.059, val_acc:0.956]
Epoch [77/120    avg_loss:0.035, val_acc:0.966]
Epoch [78/120    avg_loss:0.021, val_acc:0.968]
Epoch [79/120    avg_loss:0.027, val_acc:0.965]
Epoch [80/120    avg_loss:0.042, val_acc:0.969]
Epoch [81/120    avg_loss:0.036, val_acc:0.968]
Epoch [82/120    avg_loss:0.024, val_acc:0.970]
Epoch [83/120    avg_loss:0.019, val_acc:0.970]
Epoch [84/120    avg_loss:0.022, val_acc:0.977]
Epoch [85/120    avg_loss:0.026, val_acc:0.978]
Epoch [86/120    avg_loss:0.019, val_acc:0.979]
Epoch [87/120    avg_loss:0.016, val_acc:0.976]
Epoch [88/120    avg_loss:0.011, val_acc:0.977]
Epoch [89/120    avg_loss:0.014, val_acc:0.977]
Epoch [90/120    avg_loss:0.012, val_acc:0.976]
Epoch [91/120    avg_loss:0.016, val_acc:0.979]
Epoch [92/120    avg_loss:0.017, val_acc:0.977]
Epoch [93/120    avg_loss:0.019, val_acc:0.977]
Epoch [94/120    avg_loss:0.019, val_acc:0.977]
Epoch [95/120    avg_loss:0.014, val_acc:0.977]
Epoch [96/120    avg_loss:0.012, val_acc:0.977]
Epoch [97/120    avg_loss:0.011, val_acc:0.977]
Epoch [98/120    avg_loss:0.014, val_acc:0.977]
Epoch [99/120    avg_loss:0.013, val_acc:0.977]
Epoch [100/120    avg_loss:0.015, val_acc:0.977]
Epoch [101/120    avg_loss:0.012, val_acc:0.977]
Epoch [102/120    avg_loss:0.015, val_acc:0.977]
Epoch [103/120    avg_loss:0.018, val_acc:0.977]
Epoch [104/120    avg_loss:0.015, val_acc:0.977]
Epoch [105/120    avg_loss:0.011, val_acc:0.977]
Epoch [106/120    avg_loss:0.011, val_acc:0.977]
Epoch [107/120    avg_loss:0.019, val_acc:0.977]
Epoch [108/120    avg_loss:0.015, val_acc:0.977]
Epoch [109/120    avg_loss:0.018, val_acc:0.977]
Epoch [110/120    avg_loss:0.015, val_acc:0.977]
Epoch [111/120    avg_loss:0.015, val_acc:0.977]
Epoch [112/120    avg_loss:0.015, val_acc:0.977]
Epoch [113/120    avg_loss:0.014, val_acc:0.977]
Epoch [114/120    avg_loss:0.012, val_acc:0.977]
Epoch [115/120    avg_loss:0.014, val_acc:0.977]
Epoch [116/120    avg_loss:0.011, val_acc:0.977]
Epoch [117/120    avg_loss:0.013, val_acc:0.977]
Epoch [118/120    avg_loss:0.017, val_acc:0.977]
Epoch [119/120    avg_loss:0.013, val_acc:0.977]
Epoch [120/120    avg_loss:0.012, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1250    0    2    0    0    0    0    1    7   24    0    0
     0    1    0]
 [   0    0    2  714    0   14    0    0    0    5    1    0    4    4
     1    2    0]
 [   0    0    0    0  212    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   16    0    0    1    0
     0    0    0]
 [   0    0    8   85    0    7    5    0    0    0  753    2    3    0
     2   10    0]
 [   0    0    4    0    0    4    8    0    0    0    5 2181    0    3
     5    0    0]
 [   0    0    3    7    0    9    0    0    0    0    1   21  488    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
   161  185    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.24119241192412

F1 scores:
[       nan 0.975      0.97962382 0.91951062 0.99297424 0.95902547
 0.98869631 1.         0.99883856 0.76190476 0.91550152 0.98243243
 0.94573643 0.98143236 0.93006135 0.67889908 0.95906433]

Kappa:
0.9456596122215504
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9eb5747860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.539, val_acc:0.419]
Epoch [2/120    avg_loss:2.022, val_acc:0.506]
Epoch [3/120    avg_loss:1.746, val_acc:0.495]
Epoch [4/120    avg_loss:1.524, val_acc:0.561]
Epoch [5/120    avg_loss:1.359, val_acc:0.617]
Epoch [6/120    avg_loss:1.133, val_acc:0.666]
Epoch [7/120    avg_loss:0.995, val_acc:0.665]
Epoch [8/120    avg_loss:0.914, val_acc:0.766]
Epoch [9/120    avg_loss:0.801, val_acc:0.776]
Epoch [10/120    avg_loss:0.701, val_acc:0.768]
Epoch [11/120    avg_loss:0.579, val_acc:0.812]
Epoch [12/120    avg_loss:0.505, val_acc:0.847]
Epoch [13/120    avg_loss:0.445, val_acc:0.856]
Epoch [14/120    avg_loss:0.450, val_acc:0.856]
Epoch [15/120    avg_loss:0.469, val_acc:0.830]
Epoch [16/120    avg_loss:0.450, val_acc:0.849]
Epoch [17/120    avg_loss:0.405, val_acc:0.875]
Epoch [18/120    avg_loss:0.272, val_acc:0.902]
Epoch [19/120    avg_loss:0.230, val_acc:0.884]
Epoch [20/120    avg_loss:0.209, val_acc:0.907]
Epoch [21/120    avg_loss:0.210, val_acc:0.870]
Epoch [22/120    avg_loss:0.339, val_acc:0.878]
Epoch [23/120    avg_loss:0.235, val_acc:0.894]
Epoch [24/120    avg_loss:0.214, val_acc:0.885]
Epoch [25/120    avg_loss:0.178, val_acc:0.920]
Epoch [26/120    avg_loss:0.142, val_acc:0.925]
Epoch [27/120    avg_loss:0.122, val_acc:0.932]
Epoch [28/120    avg_loss:0.163, val_acc:0.922]
Epoch [29/120    avg_loss:0.139, val_acc:0.926]
Epoch [30/120    avg_loss:0.112, val_acc:0.940]
Epoch [31/120    avg_loss:0.100, val_acc:0.939]
Epoch [32/120    avg_loss:0.091, val_acc:0.947]
Epoch [33/120    avg_loss:0.073, val_acc:0.947]
Epoch [34/120    avg_loss:0.062, val_acc:0.953]
Epoch [35/120    avg_loss:0.063, val_acc:0.954]
Epoch [36/120    avg_loss:0.081, val_acc:0.921]
Epoch [37/120    avg_loss:0.074, val_acc:0.920]
Epoch [38/120    avg_loss:0.063, val_acc:0.947]
Epoch [39/120    avg_loss:0.062, val_acc:0.953]
Epoch [40/120    avg_loss:0.062, val_acc:0.955]
Epoch [41/120    avg_loss:0.060, val_acc:0.922]
Epoch [42/120    avg_loss:0.355, val_acc:0.628]
Epoch [43/120    avg_loss:0.554, val_acc:0.869]
Epoch [44/120    avg_loss:0.220, val_acc:0.916]
Epoch [45/120    avg_loss:0.155, val_acc:0.897]
Epoch [46/120    avg_loss:0.139, val_acc:0.940]
Epoch [47/120    avg_loss:0.117, val_acc:0.901]
Epoch [48/120    avg_loss:0.083, val_acc:0.945]
Epoch [49/120    avg_loss:0.071, val_acc:0.952]
Epoch [50/120    avg_loss:0.062, val_acc:0.918]
Epoch [51/120    avg_loss:0.056, val_acc:0.948]
Epoch [52/120    avg_loss:0.099, val_acc:0.831]
Epoch [53/120    avg_loss:0.120, val_acc:0.925]
Epoch [54/120    avg_loss:0.067, val_acc:0.955]
Epoch [55/120    avg_loss:0.047, val_acc:0.958]
Epoch [56/120    avg_loss:0.044, val_acc:0.959]
Epoch [57/120    avg_loss:0.034, val_acc:0.960]
Epoch [58/120    avg_loss:0.035, val_acc:0.961]
Epoch [59/120    avg_loss:0.034, val_acc:0.961]
Epoch [60/120    avg_loss:0.041, val_acc:0.963]
Epoch [61/120    avg_loss:0.035, val_acc:0.961]
Epoch [62/120    avg_loss:0.029, val_acc:0.963]
Epoch [63/120    avg_loss:0.037, val_acc:0.963]
Epoch [64/120    avg_loss:0.035, val_acc:0.960]
Epoch [65/120    avg_loss:0.042, val_acc:0.963]
Epoch [66/120    avg_loss:0.033, val_acc:0.965]
Epoch [67/120    avg_loss:0.030, val_acc:0.966]
Epoch [68/120    avg_loss:0.030, val_acc:0.966]
Epoch [69/120    avg_loss:0.028, val_acc:0.966]
Epoch [70/120    avg_loss:0.030, val_acc:0.967]
Epoch [71/120    avg_loss:0.026, val_acc:0.969]
Epoch [72/120    avg_loss:0.029, val_acc:0.967]
Epoch [73/120    avg_loss:0.025, val_acc:0.965]
Epoch [74/120    avg_loss:0.026, val_acc:0.966]
Epoch [75/120    avg_loss:0.027, val_acc:0.969]
Epoch [76/120    avg_loss:0.030, val_acc:0.965]
Epoch [77/120    avg_loss:0.029, val_acc:0.969]
Epoch [78/120    avg_loss:0.022, val_acc:0.969]
Epoch [79/120    avg_loss:0.026, val_acc:0.966]
Epoch [80/120    avg_loss:0.031, val_acc:0.968]
Epoch [81/120    avg_loss:0.021, val_acc:0.968]
Epoch [82/120    avg_loss:0.028, val_acc:0.969]
Epoch [83/120    avg_loss:0.029, val_acc:0.971]
Epoch [84/120    avg_loss:0.019, val_acc:0.972]
Epoch [85/120    avg_loss:0.025, val_acc:0.970]
Epoch [86/120    avg_loss:0.026, val_acc:0.967]
Epoch [87/120    avg_loss:0.024, val_acc:0.970]
Epoch [88/120    avg_loss:0.019, val_acc:0.966]
Epoch [89/120    avg_loss:0.023, val_acc:0.969]
Epoch [90/120    avg_loss:0.020, val_acc:0.970]
Epoch [91/120    avg_loss:0.019, val_acc:0.971]
Epoch [92/120    avg_loss:0.020, val_acc:0.968]
Epoch [93/120    avg_loss:0.018, val_acc:0.968]
Epoch [94/120    avg_loss:0.019, val_acc:0.966]
Epoch [95/120    avg_loss:0.019, val_acc:0.968]
Epoch [96/120    avg_loss:0.021, val_acc:0.967]
Epoch [97/120    avg_loss:0.017, val_acc:0.969]
Epoch [98/120    avg_loss:0.020, val_acc:0.969]
Epoch [99/120    avg_loss:0.015, val_acc:0.969]
Epoch [100/120    avg_loss:0.018, val_acc:0.969]
Epoch [101/120    avg_loss:0.021, val_acc:0.969]
Epoch [102/120    avg_loss:0.020, val_acc:0.971]
Epoch [103/120    avg_loss:0.019, val_acc:0.971]
Epoch [104/120    avg_loss:0.018, val_acc:0.970]
Epoch [105/120    avg_loss:0.017, val_acc:0.970]
Epoch [106/120    avg_loss:0.016, val_acc:0.970]
Epoch [107/120    avg_loss:0.020, val_acc:0.970]
Epoch [108/120    avg_loss:0.019, val_acc:0.970]
Epoch [109/120    avg_loss:0.020, val_acc:0.970]
Epoch [110/120    avg_loss:0.023, val_acc:0.970]
Epoch [111/120    avg_loss:0.017, val_acc:0.970]
Epoch [112/120    avg_loss:0.020, val_acc:0.970]
Epoch [113/120    avg_loss:0.019, val_acc:0.970]
Epoch [114/120    avg_loss:0.019, val_acc:0.970]
Epoch [115/120    avg_loss:0.017, val_acc:0.970]
Epoch [116/120    avg_loss:0.016, val_acc:0.970]
Epoch [117/120    avg_loss:0.021, val_acc:0.970]
Epoch [118/120    avg_loss:0.017, val_acc:0.970]
Epoch [119/120    avg_loss:0.019, val_acc:0.970]
Epoch [120/120    avg_loss:0.019, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1264    1    3    0    1    0    0    0    0   16    0    0
     0    0    0]
 [   0    0    0  728    4    4    0    0    0    1    0    7    2    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    3    0    0    0    0  836   31    0    0
     0    1    0]
 [   0    0   19    0    0    0    1    0    0    0   19 2150   19    1
     0    1    0]
 [   0    0    2    4    0    0    0    0    0    0    0    0  524    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    2    0    0    0    0    0    0
  1127   10    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    72  275    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.36585365853658

F1 scores:
[       nan 0.975      0.98174757 0.98378378 0.98383372 0.98623853
 0.99695586 0.96153846 0.99883586 0.97297297 0.96591566 0.97395243
 0.97037037 0.99462366 0.9616041  0.86477987 0.98823529]

Kappa:
0.9699553780720975
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:04:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f800fedb710>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.539, val_acc:0.494]
Epoch [2/120    avg_loss:2.074, val_acc:0.508]
Epoch [3/120    avg_loss:1.788, val_acc:0.610]
Epoch [4/120    avg_loss:1.455, val_acc:0.617]
Epoch [5/120    avg_loss:1.259, val_acc:0.634]
Epoch [6/120    avg_loss:1.096, val_acc:0.689]
Epoch [7/120    avg_loss:0.938, val_acc:0.706]
Epoch [8/120    avg_loss:0.797, val_acc:0.770]
Epoch [9/120    avg_loss:0.822, val_acc:0.753]
Epoch [10/120    avg_loss:0.741, val_acc:0.717]
Epoch [11/120    avg_loss:0.629, val_acc:0.778]
Epoch [12/120    avg_loss:0.561, val_acc:0.782]
Epoch [13/120    avg_loss:0.529, val_acc:0.789]
Epoch [14/120    avg_loss:0.468, val_acc:0.832]
Epoch [15/120    avg_loss:0.358, val_acc:0.861]
Epoch [16/120    avg_loss:0.350, val_acc:0.866]
Epoch [17/120    avg_loss:0.306, val_acc:0.818]
Epoch [18/120    avg_loss:0.317, val_acc:0.852]
Epoch [19/120    avg_loss:0.329, val_acc:0.878]
Epoch [20/120    avg_loss:0.265, val_acc:0.889]
Epoch [21/120    avg_loss:0.367, val_acc:0.856]
Epoch [22/120    avg_loss:0.317, val_acc:0.874]
Epoch [23/120    avg_loss:0.209, val_acc:0.884]
Epoch [24/120    avg_loss:0.216, val_acc:0.894]
Epoch [25/120    avg_loss:0.158, val_acc:0.887]
Epoch [26/120    avg_loss:0.144, val_acc:0.905]
Epoch [27/120    avg_loss:0.133, val_acc:0.921]
Epoch [28/120    avg_loss:0.177, val_acc:0.915]
Epoch [29/120    avg_loss:0.156, val_acc:0.912]
Epoch [30/120    avg_loss:0.143, val_acc:0.920]
Epoch [31/120    avg_loss:0.107, val_acc:0.938]
Epoch [32/120    avg_loss:0.124, val_acc:0.933]
Epoch [33/120    avg_loss:0.147, val_acc:0.893]
Epoch [34/120    avg_loss:0.191, val_acc:0.934]
Epoch [35/120    avg_loss:0.093, val_acc:0.944]
Epoch [36/120    avg_loss:0.082, val_acc:0.950]
Epoch [37/120    avg_loss:0.069, val_acc:0.956]
Epoch [38/120    avg_loss:0.076, val_acc:0.946]
Epoch [39/120    avg_loss:0.064, val_acc:0.965]
Epoch [40/120    avg_loss:0.047, val_acc:0.964]
Epoch [41/120    avg_loss:0.056, val_acc:0.949]
Epoch [42/120    avg_loss:0.047, val_acc:0.968]
Epoch [43/120    avg_loss:0.038, val_acc:0.970]
Epoch [44/120    avg_loss:0.031, val_acc:0.968]
Epoch [45/120    avg_loss:0.041, val_acc:0.966]
Epoch [46/120    avg_loss:0.044, val_acc:0.957]
Epoch [47/120    avg_loss:0.034, val_acc:0.965]
Epoch [48/120    avg_loss:0.041, val_acc:0.968]
Epoch [49/120    avg_loss:0.063, val_acc:0.971]
Epoch [50/120    avg_loss:0.050, val_acc:0.971]
Epoch [51/120    avg_loss:0.046, val_acc:0.978]
Epoch [52/120    avg_loss:0.057, val_acc:0.894]
Epoch [53/120    avg_loss:0.065, val_acc:0.968]
Epoch [54/120    avg_loss:0.045, val_acc:0.957]
Epoch [55/120    avg_loss:0.026, val_acc:0.976]
Epoch [56/120    avg_loss:0.020, val_acc:0.974]
Epoch [57/120    avg_loss:0.027, val_acc:0.968]
Epoch [58/120    avg_loss:0.022, val_acc:0.973]
Epoch [59/120    avg_loss:0.019, val_acc:0.972]
Epoch [60/120    avg_loss:0.019, val_acc:0.974]
Epoch [61/120    avg_loss:0.016, val_acc:0.976]
Epoch [62/120    avg_loss:0.045, val_acc:0.956]
Epoch [63/120    avg_loss:0.037, val_acc:0.975]
Epoch [64/120    avg_loss:0.074, val_acc:0.952]
Epoch [65/120    avg_loss:0.041, val_acc:0.969]
Epoch [66/120    avg_loss:0.023, val_acc:0.974]
Epoch [67/120    avg_loss:0.020, val_acc:0.975]
Epoch [68/120    avg_loss:0.018, val_acc:0.976]
Epoch [69/120    avg_loss:0.015, val_acc:0.979]
Epoch [70/120    avg_loss:0.015, val_acc:0.980]
Epoch [71/120    avg_loss:0.018, val_acc:0.979]
Epoch [72/120    avg_loss:0.015, val_acc:0.980]
Epoch [73/120    avg_loss:0.014, val_acc:0.979]
Epoch [74/120    avg_loss:0.014, val_acc:0.978]
Epoch [75/120    avg_loss:0.015, val_acc:0.979]
Epoch [76/120    avg_loss:0.021, val_acc:0.978]
Epoch [77/120    avg_loss:0.014, val_acc:0.981]
Epoch [78/120    avg_loss:0.012, val_acc:0.981]
Epoch [79/120    avg_loss:0.014, val_acc:0.980]
Epoch [80/120    avg_loss:0.013, val_acc:0.981]
Epoch [81/120    avg_loss:0.013, val_acc:0.982]
Epoch [82/120    avg_loss:0.011, val_acc:0.982]
Epoch [83/120    avg_loss:0.014, val_acc:0.982]
Epoch [84/120    avg_loss:0.012, val_acc:0.982]
Epoch [85/120    avg_loss:0.011, val_acc:0.981]
Epoch [86/120    avg_loss:0.014, val_acc:0.980]
Epoch [87/120    avg_loss:0.010, val_acc:0.981]
Epoch [88/120    avg_loss:0.013, val_acc:0.982]
Epoch [89/120    avg_loss:0.011, val_acc:0.982]
Epoch [90/120    avg_loss:0.011, val_acc:0.981]
Epoch [91/120    avg_loss:0.014, val_acc:0.980]
Epoch [92/120    avg_loss:0.013, val_acc:0.982]
Epoch [93/120    avg_loss:0.009, val_acc:0.981]
Epoch [94/120    avg_loss:0.011, val_acc:0.983]
Epoch [95/120    avg_loss:0.010, val_acc:0.983]
Epoch [96/120    avg_loss:0.011, val_acc:0.982]
Epoch [97/120    avg_loss:0.018, val_acc:0.981]
Epoch [98/120    avg_loss:0.013, val_acc:0.980]
Epoch [99/120    avg_loss:0.012, val_acc:0.981]
Epoch [100/120    avg_loss:0.010, val_acc:0.982]
Epoch [101/120    avg_loss:0.009, val_acc:0.981]
Epoch [102/120    avg_loss:0.009, val_acc:0.983]
Epoch [103/120    avg_loss:0.010, val_acc:0.982]
Epoch [104/120    avg_loss:0.010, val_acc:0.982]
Epoch [105/120    avg_loss:0.011, val_acc:0.984]
Epoch [106/120    avg_loss:0.012, val_acc:0.981]
Epoch [107/120    avg_loss:0.011, val_acc:0.982]
Epoch [108/120    avg_loss:0.009, val_acc:0.983]
Epoch [109/120    avg_loss:0.008, val_acc:0.983]
Epoch [110/120    avg_loss:0.009, val_acc:0.982]
Epoch [111/120    avg_loss:0.010, val_acc:0.983]
Epoch [112/120    avg_loss:0.009, val_acc:0.983]
Epoch [113/120    avg_loss:0.008, val_acc:0.982]
Epoch [114/120    avg_loss:0.010, val_acc:0.983]
Epoch [115/120    avg_loss:0.008, val_acc:0.983]
Epoch [116/120    avg_loss:0.009, val_acc:0.982]
Epoch [117/120    avg_loss:0.009, val_acc:0.982]
Epoch [118/120    avg_loss:0.011, val_acc:0.985]
Epoch [119/120    avg_loss:0.013, val_acc:0.984]
Epoch [120/120    avg_loss:0.009, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    1    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1243   16   11    0    1    0    0    1    1   10    2    0
     0    0    0]
 [   0    0    0  731    0    0    0    0    0    1    1    9    5    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    0    0    1    0    0
     6    1    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    2    0    1    0    0    0    1  837   26    1    0
     0    0    0]
 [   0    0   12    1    0    0    0    0    0    4   12 2155   26    0
     0    0    0]
 [   0    0    0    6    0    0    0    0    0    0    1    0  526    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1128   10    0]
 [   0    0    0    0    0    3    3    0    0    0    0    0    0    0
    71  270    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.1490514905149

F1 scores:
[       nan 0.98765432 0.97605026 0.97207447 0.97247706 0.98387097
 0.99467681 1.         1.         0.8372093  0.96931094 0.97621744
 0.95985401 0.99728997 0.96245734 0.85987261 0.98203593]

Kappa:
0.9674897617890994
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f03481b97f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.501, val_acc:0.494]
Epoch [2/120    avg_loss:2.038, val_acc:0.564]
Epoch [3/120    avg_loss:1.778, val_acc:0.564]
Epoch [4/120    avg_loss:1.591, val_acc:0.590]
Epoch [5/120    avg_loss:1.428, val_acc:0.628]
Epoch [6/120    avg_loss:1.215, val_acc:0.655]
Epoch [7/120    avg_loss:1.079, val_acc:0.614]
Epoch [8/120    avg_loss:0.886, val_acc:0.718]
Epoch [9/120    avg_loss:0.827, val_acc:0.758]
Epoch [10/120    avg_loss:0.770, val_acc:0.764]
Epoch [11/120    avg_loss:0.610, val_acc:0.835]
Epoch [12/120    avg_loss:0.540, val_acc:0.819]
Epoch [13/120    avg_loss:0.458, val_acc:0.848]
Epoch [14/120    avg_loss:0.432, val_acc:0.830]
Epoch [15/120    avg_loss:0.401, val_acc:0.880]
Epoch [16/120    avg_loss:0.364, val_acc:0.860]
Epoch [17/120    avg_loss:0.279, val_acc:0.826]
Epoch [18/120    avg_loss:0.231, val_acc:0.886]
Epoch [19/120    avg_loss:0.238, val_acc:0.844]
Epoch [20/120    avg_loss:0.217, val_acc:0.904]
Epoch [21/120    avg_loss:0.249, val_acc:0.910]
Epoch [22/120    avg_loss:0.287, val_acc:0.866]
Epoch [23/120    avg_loss:0.207, val_acc:0.909]
Epoch [24/120    avg_loss:0.142, val_acc:0.903]
Epoch [25/120    avg_loss:0.129, val_acc:0.917]
Epoch [26/120    avg_loss:0.145, val_acc:0.911]
Epoch [27/120    avg_loss:0.199, val_acc:0.901]
Epoch [28/120    avg_loss:0.184, val_acc:0.922]
Epoch [29/120    avg_loss:0.175, val_acc:0.900]
Epoch [30/120    avg_loss:0.151, val_acc:0.916]
Epoch [31/120    avg_loss:0.130, val_acc:0.926]
Epoch [32/120    avg_loss:0.094, val_acc:0.948]
Epoch [33/120    avg_loss:0.091, val_acc:0.942]
Epoch [34/120    avg_loss:0.088, val_acc:0.909]
Epoch [35/120    avg_loss:0.113, val_acc:0.928]
Epoch [36/120    avg_loss:0.088, val_acc:0.940]
Epoch [37/120    avg_loss:0.090, val_acc:0.914]
Epoch [38/120    avg_loss:0.087, val_acc:0.940]
Epoch [39/120    avg_loss:0.066, val_acc:0.959]
Epoch [40/120    avg_loss:0.052, val_acc:0.959]
Epoch [41/120    avg_loss:0.048, val_acc:0.964]
Epoch [42/120    avg_loss:0.060, val_acc:0.940]
Epoch [43/120    avg_loss:0.064, val_acc:0.950]
Epoch [44/120    avg_loss:0.077, val_acc:0.964]
Epoch [45/120    avg_loss:0.055, val_acc:0.951]
Epoch [46/120    avg_loss:0.063, val_acc:0.955]
Epoch [47/120    avg_loss:0.066, val_acc:0.947]
Epoch [48/120    avg_loss:0.097, val_acc:0.954]
Epoch [49/120    avg_loss:0.042, val_acc:0.948]
Epoch [50/120    avg_loss:0.046, val_acc:0.956]
Epoch [51/120    avg_loss:0.036, val_acc:0.968]
Epoch [52/120    avg_loss:0.043, val_acc:0.958]
Epoch [53/120    avg_loss:0.033, val_acc:0.973]
Epoch [54/120    avg_loss:0.033, val_acc:0.968]
Epoch [55/120    avg_loss:0.028, val_acc:0.974]
Epoch [56/120    avg_loss:0.023, val_acc:0.967]
Epoch [57/120    avg_loss:0.026, val_acc:0.964]
Epoch [58/120    avg_loss:0.020, val_acc:0.973]
Epoch [59/120    avg_loss:0.017, val_acc:0.973]
Epoch [60/120    avg_loss:0.017, val_acc:0.964]
Epoch [61/120    avg_loss:0.023, val_acc:0.954]
Epoch [62/120    avg_loss:0.032, val_acc:0.970]
Epoch [63/120    avg_loss:0.026, val_acc:0.974]
Epoch [64/120    avg_loss:0.019, val_acc:0.973]
Epoch [65/120    avg_loss:0.020, val_acc:0.969]
Epoch [66/120    avg_loss:0.014, val_acc:0.976]
Epoch [67/120    avg_loss:0.018, val_acc:0.968]
Epoch [68/120    avg_loss:0.013, val_acc:0.973]
Epoch [69/120    avg_loss:0.012, val_acc:0.974]
Epoch [70/120    avg_loss:0.013, val_acc:0.974]
Epoch [71/120    avg_loss:0.012, val_acc:0.973]
Epoch [72/120    avg_loss:0.015, val_acc:0.972]
Epoch [73/120    avg_loss:0.030, val_acc:0.965]
Epoch [74/120    avg_loss:0.056, val_acc:0.969]
Epoch [75/120    avg_loss:0.039, val_acc:0.945]
Epoch [76/120    avg_loss:0.056, val_acc:0.949]
Epoch [77/120    avg_loss:0.039, val_acc:0.947]
Epoch [78/120    avg_loss:0.044, val_acc:0.963]
Epoch [79/120    avg_loss:0.022, val_acc:0.968]
Epoch [80/120    avg_loss:0.016, val_acc:0.971]
Epoch [81/120    avg_loss:0.011, val_acc:0.973]
Epoch [82/120    avg_loss:0.010, val_acc:0.971]
Epoch [83/120    avg_loss:0.011, val_acc:0.972]
Epoch [84/120    avg_loss:0.011, val_acc:0.974]
Epoch [85/120    avg_loss:0.011, val_acc:0.973]
Epoch [86/120    avg_loss:0.013, val_acc:0.976]
Epoch [87/120    avg_loss:0.012, val_acc:0.975]
Epoch [88/120    avg_loss:0.010, val_acc:0.974]
Epoch [89/120    avg_loss:0.010, val_acc:0.975]
Epoch [90/120    avg_loss:0.010, val_acc:0.976]
Epoch [91/120    avg_loss:0.011, val_acc:0.975]
Epoch [92/120    avg_loss:0.009, val_acc:0.976]
Epoch [93/120    avg_loss:0.008, val_acc:0.976]
Epoch [94/120    avg_loss:0.009, val_acc:0.975]
Epoch [95/120    avg_loss:0.011, val_acc:0.975]
Epoch [96/120    avg_loss:0.011, val_acc:0.976]
Epoch [97/120    avg_loss:0.009, val_acc:0.977]
Epoch [98/120    avg_loss:0.008, val_acc:0.979]
Epoch [99/120    avg_loss:0.009, val_acc:0.977]
Epoch [100/120    avg_loss:0.008, val_acc:0.977]
Epoch [101/120    avg_loss:0.010, val_acc:0.976]
Epoch [102/120    avg_loss:0.010, val_acc:0.976]
Epoch [103/120    avg_loss:0.008, val_acc:0.976]
Epoch [104/120    avg_loss:0.009, val_acc:0.976]
Epoch [105/120    avg_loss:0.007, val_acc:0.976]
Epoch [106/120    avg_loss:0.012, val_acc:0.976]
Epoch [107/120    avg_loss:0.009, val_acc:0.976]
Epoch [108/120    avg_loss:0.010, val_acc:0.976]
Epoch [109/120    avg_loss:0.010, val_acc:0.975]
Epoch [110/120    avg_loss:0.007, val_acc:0.977]
Epoch [111/120    avg_loss:0.008, val_acc:0.975]
Epoch [112/120    avg_loss:0.006, val_acc:0.975]
Epoch [113/120    avg_loss:0.007, val_acc:0.975]
Epoch [114/120    avg_loss:0.007, val_acc:0.975]
Epoch [115/120    avg_loss:0.009, val_acc:0.976]
Epoch [116/120    avg_loss:0.008, val_acc:0.976]
Epoch [117/120    avg_loss:0.007, val_acc:0.976]
Epoch [118/120    avg_loss:0.009, val_acc:0.975]
Epoch [119/120    avg_loss:0.008, val_acc:0.976]
Epoch [120/120    avg_loss:0.009, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1250   13    0    0    0    0    0    2    0   20    0    0
     0    0    0]
 [   0    0    0  743    0    0    0    0    0    0    0    0    3    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    3    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    0  846   18    3    0
     0    1    0]
 [   0    0   25    1    0    1    0    0    0    1    7 2164    8    0
     0    1    2]
 [   0    0    0    2    1    0    0    0    0    0    3    0  523    0
     0    4    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1125   14    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
   103  242    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.27913279132791

F1 scores:
[       nan 1.         0.97389949 0.98671979 0.99765808 0.99307159
 0.99695586 0.94339623 1.         0.92307692 0.97746967 0.98073873
 0.97665733 0.99730458 0.94936709 0.79474548 0.98245614]

Kappa:
0.9689592882761847
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffa0e8b2748>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.558, val_acc:0.490]
Epoch [2/120    avg_loss:2.062, val_acc:0.564]
Epoch [3/120    avg_loss:1.776, val_acc:0.587]
Epoch [4/120    avg_loss:1.539, val_acc:0.623]
Epoch [5/120    avg_loss:1.354, val_acc:0.674]
Epoch [6/120    avg_loss:1.185, val_acc:0.692]
Epoch [7/120    avg_loss:1.017, val_acc:0.701]
Epoch [8/120    avg_loss:0.897, val_acc:0.773]
Epoch [9/120    avg_loss:0.742, val_acc:0.796]
Epoch [10/120    avg_loss:0.708, val_acc:0.815]
Epoch [11/120    avg_loss:0.606, val_acc:0.812]
Epoch [12/120    avg_loss:0.508, val_acc:0.810]
Epoch [13/120    avg_loss:0.476, val_acc:0.869]
Epoch [14/120    avg_loss:0.471, val_acc:0.859]
Epoch [15/120    avg_loss:0.385, val_acc:0.862]
Epoch [16/120    avg_loss:0.356, val_acc:0.861]
Epoch [17/120    avg_loss:0.406, val_acc:0.842]
Epoch [18/120    avg_loss:0.321, val_acc:0.858]
Epoch [19/120    avg_loss:0.301, val_acc:0.901]
Epoch [20/120    avg_loss:0.222, val_acc:0.913]
Epoch [21/120    avg_loss:0.239, val_acc:0.920]
Epoch [22/120    avg_loss:0.221, val_acc:0.904]
Epoch [23/120    avg_loss:0.240, val_acc:0.847]
Epoch [24/120    avg_loss:0.194, val_acc:0.918]
Epoch [25/120    avg_loss:0.204, val_acc:0.940]
Epoch [26/120    avg_loss:0.131, val_acc:0.939]
Epoch [27/120    avg_loss:0.137, val_acc:0.922]
Epoch [28/120    avg_loss:0.205, val_acc:0.925]
Epoch [29/120    avg_loss:0.129, val_acc:0.935]
Epoch [30/120    avg_loss:0.210, val_acc:0.899]
Epoch [31/120    avg_loss:0.158, val_acc:0.927]
Epoch [32/120    avg_loss:0.099, val_acc:0.947]
Epoch [33/120    avg_loss:0.133, val_acc:0.916]
Epoch [34/120    avg_loss:0.122, val_acc:0.900]
Epoch [35/120    avg_loss:0.102, val_acc:0.944]
Epoch [36/120    avg_loss:0.079, val_acc:0.936]
Epoch [37/120    avg_loss:0.096, val_acc:0.953]
Epoch [38/120    avg_loss:0.069, val_acc:0.944]
Epoch [39/120    avg_loss:0.066, val_acc:0.947]
Epoch [40/120    avg_loss:0.067, val_acc:0.934]
Epoch [41/120    avg_loss:0.071, val_acc:0.952]
Epoch [42/120    avg_loss:0.059, val_acc:0.944]
Epoch [43/120    avg_loss:0.061, val_acc:0.960]
Epoch [44/120    avg_loss:0.054, val_acc:0.946]
Epoch [45/120    avg_loss:0.066, val_acc:0.936]
Epoch [46/120    avg_loss:0.166, val_acc:0.922]
Epoch [47/120    avg_loss:0.104, val_acc:0.927]
Epoch [48/120    avg_loss:0.073, val_acc:0.936]
Epoch [49/120    avg_loss:0.075, val_acc:0.960]
Epoch [50/120    avg_loss:0.074, val_acc:0.953]
Epoch [51/120    avg_loss:0.048, val_acc:0.965]
Epoch [52/120    avg_loss:0.035, val_acc:0.965]
Epoch [53/120    avg_loss:0.030, val_acc:0.972]
Epoch [54/120    avg_loss:0.037, val_acc:0.966]
Epoch [55/120    avg_loss:0.029, val_acc:0.968]
Epoch [56/120    avg_loss:0.038, val_acc:0.964]
Epoch [57/120    avg_loss:0.052, val_acc:0.955]
Epoch [58/120    avg_loss:0.029, val_acc:0.964]
Epoch [59/120    avg_loss:0.028, val_acc:0.967]
Epoch [60/120    avg_loss:0.025, val_acc:0.966]
Epoch [61/120    avg_loss:0.030, val_acc:0.971]
Epoch [62/120    avg_loss:0.042, val_acc:0.952]
Epoch [63/120    avg_loss:0.059, val_acc:0.961]
Epoch [64/120    avg_loss:0.066, val_acc:0.963]
Epoch [65/120    avg_loss:0.032, val_acc:0.971]
Epoch [66/120    avg_loss:0.032, val_acc:0.962]
Epoch [67/120    avg_loss:0.023, val_acc:0.970]
Epoch [68/120    avg_loss:0.016, val_acc:0.970]
Epoch [69/120    avg_loss:0.014, val_acc:0.970]
Epoch [70/120    avg_loss:0.021, val_acc:0.971]
Epoch [71/120    avg_loss:0.012, val_acc:0.971]
Epoch [72/120    avg_loss:0.014, val_acc:0.971]
Epoch [73/120    avg_loss:0.013, val_acc:0.970]
Epoch [74/120    avg_loss:0.013, val_acc:0.971]
Epoch [75/120    avg_loss:0.014, val_acc:0.971]
Epoch [76/120    avg_loss:0.013, val_acc:0.972]
Epoch [77/120    avg_loss:0.017, val_acc:0.972]
Epoch [78/120    avg_loss:0.013, val_acc:0.971]
Epoch [79/120    avg_loss:0.013, val_acc:0.973]
Epoch [80/120    avg_loss:0.014, val_acc:0.973]
Epoch [81/120    avg_loss:0.012, val_acc:0.974]
Epoch [82/120    avg_loss:0.010, val_acc:0.975]
Epoch [83/120    avg_loss:0.011, val_acc:0.974]
Epoch [84/120    avg_loss:0.010, val_acc:0.975]
Epoch [85/120    avg_loss:0.013, val_acc:0.976]
Epoch [86/120    avg_loss:0.009, val_acc:0.974]
Epoch [87/120    avg_loss:0.010, val_acc:0.974]
Epoch [88/120    avg_loss:0.011, val_acc:0.974]
Epoch [89/120    avg_loss:0.012, val_acc:0.974]
Epoch [90/120    avg_loss:0.010, val_acc:0.978]
Epoch [91/120    avg_loss:0.012, val_acc:0.977]
Epoch [92/120    avg_loss:0.012, val_acc:0.974]
Epoch [93/120    avg_loss:0.013, val_acc:0.974]
Epoch [94/120    avg_loss:0.011, val_acc:0.975]
Epoch [95/120    avg_loss:0.010, val_acc:0.976]
Epoch [96/120    avg_loss:0.012, val_acc:0.975]
Epoch [97/120    avg_loss:0.011, val_acc:0.976]
Epoch [98/120    avg_loss:0.011, val_acc:0.974]
Epoch [99/120    avg_loss:0.012, val_acc:0.973]
Epoch [100/120    avg_loss:0.011, val_acc:0.973]
Epoch [101/120    avg_loss:0.009, val_acc:0.973]
Epoch [102/120    avg_loss:0.008, val_acc:0.975]
Epoch [103/120    avg_loss:0.009, val_acc:0.977]
Epoch [104/120    avg_loss:0.013, val_acc:0.977]
Epoch [105/120    avg_loss:0.009, val_acc:0.977]
Epoch [106/120    avg_loss:0.008, val_acc:0.977]
Epoch [107/120    avg_loss:0.010, val_acc:0.977]
Epoch [108/120    avg_loss:0.008, val_acc:0.977]
Epoch [109/120    avg_loss:0.008, val_acc:0.977]
Epoch [110/120    avg_loss:0.010, val_acc:0.977]
Epoch [111/120    avg_loss:0.010, val_acc:0.977]
Epoch [112/120    avg_loss:0.011, val_acc:0.977]
Epoch [113/120    avg_loss:0.009, val_acc:0.977]
Epoch [114/120    avg_loss:0.009, val_acc:0.977]
Epoch [115/120    avg_loss:0.010, val_acc:0.976]
Epoch [116/120    avg_loss:0.007, val_acc:0.976]
Epoch [117/120    avg_loss:0.009, val_acc:0.976]
Epoch [118/120    avg_loss:0.009, val_acc:0.976]
Epoch [119/120    avg_loss:0.009, val_acc:0.976]
Epoch [120/120    avg_loss:0.013, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1254    8    5    3    0    0    0    0    4   11    0    0
     0    0    0]
 [   0    0    0  733    1    0    0    0    0    1    0    3    4    2
     0    3    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    1    2    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    1    0    0    0    0  844   22    0    0
     0    2    0]
 [   0    0    4    0    0    2    0    0    0    0   24 2172    8    0
     0    0    0]
 [   0    0    0    2    1    0    0    0    0    0    0    3  524    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1122   17    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    86  261    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.42005420054201

F1 scores:
[       nan 0.975      0.98352941 0.98323273 0.98148148 0.98737084
 0.99771516 0.96153846 1.         0.94736842 0.96567506 0.98258313
 0.97852474 0.99462366 0.95489362 0.82594937 0.98224852]

Kappa:
0.9705720316926975
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc17af927b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.507, val_acc:0.503]
Epoch [2/120    avg_loss:2.021, val_acc:0.588]
Epoch [3/120    avg_loss:1.776, val_acc:0.640]
Epoch [4/120    avg_loss:1.615, val_acc:0.664]
Epoch [5/120    avg_loss:1.347, val_acc:0.694]
Epoch [6/120    avg_loss:1.169, val_acc:0.717]
Epoch [7/120    avg_loss:0.980, val_acc:0.777]
Epoch [8/120    avg_loss:0.918, val_acc:0.774]
Epoch [9/120    avg_loss:0.877, val_acc:0.797]
Epoch [10/120    avg_loss:0.791, val_acc:0.806]
Epoch [11/120    avg_loss:0.626, val_acc:0.783]
Epoch [12/120    avg_loss:0.516, val_acc:0.852]
Epoch [13/120    avg_loss:0.422, val_acc:0.880]
Epoch [14/120    avg_loss:0.367, val_acc:0.857]
Epoch [15/120    avg_loss:0.437, val_acc:0.867]
Epoch [16/120    avg_loss:0.397, val_acc:0.849]
Epoch [17/120    avg_loss:0.358, val_acc:0.814]
Epoch [18/120    avg_loss:0.278, val_acc:0.890]
Epoch [19/120    avg_loss:0.294, val_acc:0.874]
Epoch [20/120    avg_loss:0.229, val_acc:0.906]
Epoch [21/120    avg_loss:0.185, val_acc:0.929]
Epoch [22/120    avg_loss:0.198, val_acc:0.934]
Epoch [23/120    avg_loss:0.170, val_acc:0.900]
Epoch [24/120    avg_loss:0.149, val_acc:0.938]
Epoch [25/120    avg_loss:0.134, val_acc:0.946]
Epoch [26/120    avg_loss:0.112, val_acc:0.890]
Epoch [27/120    avg_loss:0.124, val_acc:0.942]
Epoch [28/120    avg_loss:0.102, val_acc:0.945]
Epoch [29/120    avg_loss:0.130, val_acc:0.908]
Epoch [30/120    avg_loss:0.101, val_acc:0.953]
Epoch [31/120    avg_loss:0.110, val_acc:0.945]
Epoch [32/120    avg_loss:0.100, val_acc:0.955]
Epoch [33/120    avg_loss:0.076, val_acc:0.959]
Epoch [34/120    avg_loss:0.081, val_acc:0.955]
Epoch [35/120    avg_loss:0.090, val_acc:0.930]
Epoch [36/120    avg_loss:0.089, val_acc:0.939]
Epoch [37/120    avg_loss:0.072, val_acc:0.928]
Epoch [38/120    avg_loss:0.077, val_acc:0.916]
Epoch [39/120    avg_loss:0.153, val_acc:0.944]
Epoch [40/120    avg_loss:0.089, val_acc:0.965]
Epoch [41/120    avg_loss:0.076, val_acc:0.957]
Epoch [42/120    avg_loss:0.070, val_acc:0.941]
Epoch [43/120    avg_loss:0.072, val_acc:0.956]
Epoch [44/120    avg_loss:0.060, val_acc:0.965]
Epoch [45/120    avg_loss:0.051, val_acc:0.947]
Epoch [46/120    avg_loss:0.049, val_acc:0.970]
Epoch [47/120    avg_loss:0.037, val_acc:0.963]
Epoch [48/120    avg_loss:0.036, val_acc:0.972]
Epoch [49/120    avg_loss:0.025, val_acc:0.973]
Epoch [50/120    avg_loss:0.037, val_acc:0.963]
Epoch [51/120    avg_loss:0.054, val_acc:0.971]
Epoch [52/120    avg_loss:0.046, val_acc:0.973]
Epoch [53/120    avg_loss:0.036, val_acc:0.976]
Epoch [54/120    avg_loss:0.030, val_acc:0.971]
Epoch [55/120    avg_loss:0.033, val_acc:0.975]
Epoch [56/120    avg_loss:0.029, val_acc:0.970]
Epoch [57/120    avg_loss:0.023, val_acc:0.977]
Epoch [58/120    avg_loss:0.030, val_acc:0.977]
Epoch [59/120    avg_loss:0.021, val_acc:0.954]
Epoch [60/120    avg_loss:0.038, val_acc:0.974]
Epoch [61/120    avg_loss:0.030, val_acc:0.975]
Epoch [62/120    avg_loss:0.049, val_acc:0.960]
Epoch [63/120    avg_loss:0.096, val_acc:0.960]
Epoch [64/120    avg_loss:0.049, val_acc:0.976]
Epoch [65/120    avg_loss:0.031, val_acc:0.969]
Epoch [66/120    avg_loss:0.018, val_acc:0.979]
Epoch [67/120    avg_loss:0.016, val_acc:0.983]
Epoch [68/120    avg_loss:0.018, val_acc:0.984]
Epoch [69/120    avg_loss:0.018, val_acc:0.978]
Epoch [70/120    avg_loss:0.060, val_acc:0.968]
Epoch [71/120    avg_loss:0.057, val_acc:0.966]
Epoch [72/120    avg_loss:0.028, val_acc:0.976]
Epoch [73/120    avg_loss:0.020, val_acc:0.973]
Epoch [74/120    avg_loss:0.036, val_acc:0.978]
Epoch [75/120    avg_loss:0.019, val_acc:0.973]
Epoch [76/120    avg_loss:0.028, val_acc:0.968]
Epoch [77/120    avg_loss:0.025, val_acc:0.980]
Epoch [78/120    avg_loss:0.030, val_acc:0.978]
Epoch [79/120    avg_loss:0.030, val_acc:0.977]
Epoch [80/120    avg_loss:0.018, val_acc:0.982]
Epoch [81/120    avg_loss:0.008, val_acc:0.982]
Epoch [82/120    avg_loss:0.009, val_acc:0.983]
Epoch [83/120    avg_loss:0.008, val_acc:0.983]
Epoch [84/120    avg_loss:0.008, val_acc:0.983]
Epoch [85/120    avg_loss:0.015, val_acc:0.985]
Epoch [86/120    avg_loss:0.007, val_acc:0.985]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.007, val_acc:0.984]
Epoch [89/120    avg_loss:0.007, val_acc:0.983]
Epoch [90/120    avg_loss:0.007, val_acc:0.984]
Epoch [91/120    avg_loss:0.010, val_acc:0.983]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.007, val_acc:0.986]
Epoch [94/120    avg_loss:0.006, val_acc:0.985]
Epoch [95/120    avg_loss:0.006, val_acc:0.985]
Epoch [96/120    avg_loss:0.006, val_acc:0.984]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.005, val_acc:0.984]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.005, val_acc:0.983]
Epoch [101/120    avg_loss:0.006, val_acc:0.985]
Epoch [102/120    avg_loss:0.006, val_acc:0.984]
Epoch [103/120    avg_loss:0.005, val_acc:0.983]
Epoch [104/120    avg_loss:0.005, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.984]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.006, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.005, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.984]
Epoch [112/120    avg_loss:0.005, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.005, val_acc:0.984]
Epoch [115/120    avg_loss:0.006, val_acc:0.984]
Epoch [116/120    avg_loss:0.006, val_acc:0.984]
Epoch [117/120    avg_loss:0.006, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1259    6    2    1    0    0    0    0    7    8    2    0
     0    0    0]
 [   0    0    0  738    0    0    0    0    0    0    1    4    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  836   35    0    0
     0    0    0]
 [   0    0   12    4    0    0    0    0    0    3    5 2167   14    0
     0    5    0]
 [   0    0    1    5    0    0    0    0    0    0    0    0  525    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    95  252    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.4308943089431

F1 scores:
[       nan 0.975      0.98320968 0.984      0.9953271  0.99885189
 0.99923839 1.         1.         0.92307692 0.96871379 0.97965642
 0.97312326 1.         0.95383312 0.81685575 0.98245614]

Kappa:
0.9706883129407186
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb91853d7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.489, val_acc:0.416]
Epoch [2/120    avg_loss:2.045, val_acc:0.509]
Epoch [3/120    avg_loss:1.774, val_acc:0.533]
Epoch [4/120    avg_loss:1.594, val_acc:0.565]
Epoch [5/120    avg_loss:1.411, val_acc:0.648]
Epoch [6/120    avg_loss:1.215, val_acc:0.651]
Epoch [7/120    avg_loss:1.085, val_acc:0.654]
Epoch [8/120    avg_loss:0.916, val_acc:0.728]
Epoch [9/120    avg_loss:0.754, val_acc:0.733]
Epoch [10/120    avg_loss:0.819, val_acc:0.668]
Epoch [11/120    avg_loss:0.757, val_acc:0.713]
Epoch [12/120    avg_loss:0.584, val_acc:0.770]
Epoch [13/120    avg_loss:0.438, val_acc:0.820]
Epoch [14/120    avg_loss:0.460, val_acc:0.817]
Epoch [15/120    avg_loss:0.378, val_acc:0.840]
Epoch [16/120    avg_loss:0.315, val_acc:0.849]
Epoch [17/120    avg_loss:0.547, val_acc:0.809]
Epoch [18/120    avg_loss:0.391, val_acc:0.821]
Epoch [19/120    avg_loss:0.336, val_acc:0.856]
Epoch [20/120    avg_loss:0.259, val_acc:0.843]
Epoch [21/120    avg_loss:0.251, val_acc:0.797]
Epoch [22/120    avg_loss:0.243, val_acc:0.869]
Epoch [23/120    avg_loss:0.190, val_acc:0.867]
Epoch [24/120    avg_loss:0.225, val_acc:0.845]
Epoch [25/120    avg_loss:0.201, val_acc:0.870]
Epoch [26/120    avg_loss:0.154, val_acc:0.930]
Epoch [27/120    avg_loss:0.112, val_acc:0.898]
Epoch [28/120    avg_loss:0.195, val_acc:0.908]
Epoch [29/120    avg_loss:0.435, val_acc:0.847]
Epoch [30/120    avg_loss:0.382, val_acc:0.825]
Epoch [31/120    avg_loss:0.288, val_acc:0.886]
Epoch [32/120    avg_loss:0.172, val_acc:0.896]
Epoch [33/120    avg_loss:0.100, val_acc:0.915]
Epoch [34/120    avg_loss:0.125, val_acc:0.889]
Epoch [35/120    avg_loss:0.137, val_acc:0.936]
Epoch [36/120    avg_loss:0.115, val_acc:0.927]
Epoch [37/120    avg_loss:0.092, val_acc:0.928]
Epoch [38/120    avg_loss:0.084, val_acc:0.935]
Epoch [39/120    avg_loss:0.104, val_acc:0.874]
Epoch [40/120    avg_loss:0.089, val_acc:0.911]
Epoch [41/120    avg_loss:0.081, val_acc:0.939]
Epoch [42/120    avg_loss:0.062, val_acc:0.942]
Epoch [43/120    avg_loss:0.069, val_acc:0.956]
Epoch [44/120    avg_loss:0.055, val_acc:0.960]
Epoch [45/120    avg_loss:0.088, val_acc:0.931]
Epoch [46/120    avg_loss:0.078, val_acc:0.951]
Epoch [47/120    avg_loss:0.112, val_acc:0.942]
Epoch [48/120    avg_loss:0.058, val_acc:0.947]
Epoch [49/120    avg_loss:0.049, val_acc:0.958]
Epoch [50/120    avg_loss:0.056, val_acc:0.955]
Epoch [51/120    avg_loss:0.044, val_acc:0.971]
Epoch [52/120    avg_loss:0.045, val_acc:0.967]
Epoch [53/120    avg_loss:0.031, val_acc:0.967]
Epoch [54/120    avg_loss:0.042, val_acc:0.942]
Epoch [55/120    avg_loss:0.030, val_acc:0.966]
Epoch [56/120    avg_loss:0.025, val_acc:0.966]
Epoch [57/120    avg_loss:0.025, val_acc:0.971]
Epoch [58/120    avg_loss:0.022, val_acc:0.977]
Epoch [59/120    avg_loss:0.026, val_acc:0.967]
Epoch [60/120    avg_loss:0.042, val_acc:0.969]
Epoch [61/120    avg_loss:0.029, val_acc:0.967]
Epoch [62/120    avg_loss:0.019, val_acc:0.966]
Epoch [63/120    avg_loss:0.033, val_acc:0.958]
Epoch [64/120    avg_loss:0.027, val_acc:0.968]
Epoch [65/120    avg_loss:0.041, val_acc:0.967]
Epoch [66/120    avg_loss:0.026, val_acc:0.961]
Epoch [67/120    avg_loss:0.027, val_acc:0.972]
Epoch [68/120    avg_loss:0.020, val_acc:0.974]
Epoch [69/120    avg_loss:0.016, val_acc:0.978]
Epoch [70/120    avg_loss:0.018, val_acc:0.976]
Epoch [71/120    avg_loss:0.017, val_acc:0.976]
Epoch [72/120    avg_loss:0.015, val_acc:0.977]
Epoch [73/120    avg_loss:0.018, val_acc:0.976]
Epoch [74/120    avg_loss:0.020, val_acc:0.965]
Epoch [75/120    avg_loss:0.012, val_acc:0.975]
Epoch [76/120    avg_loss:0.016, val_acc:0.958]
Epoch [77/120    avg_loss:0.016, val_acc:0.976]
Epoch [78/120    avg_loss:0.018, val_acc:0.976]
Epoch [79/120    avg_loss:0.013, val_acc:0.981]
Epoch [80/120    avg_loss:0.014, val_acc:0.980]
Epoch [81/120    avg_loss:0.010, val_acc:0.977]
Epoch [82/120    avg_loss:0.009, val_acc:0.981]
Epoch [83/120    avg_loss:0.007, val_acc:0.979]
Epoch [84/120    avg_loss:0.008, val_acc:0.982]
Epoch [85/120    avg_loss:0.010, val_acc:0.977]
Epoch [86/120    avg_loss:0.012, val_acc:0.976]
Epoch [87/120    avg_loss:0.013, val_acc:0.974]
Epoch [88/120    avg_loss:0.006, val_acc:0.981]
Epoch [89/120    avg_loss:0.015, val_acc:0.971]
Epoch [90/120    avg_loss:0.022, val_acc:0.971]
Epoch [91/120    avg_loss:0.025, val_acc:0.971]
Epoch [92/120    avg_loss:0.014, val_acc:0.973]
Epoch [93/120    avg_loss:0.016, val_acc:0.970]
Epoch [94/120    avg_loss:0.010, val_acc:0.978]
Epoch [95/120    avg_loss:0.009, val_acc:0.972]
Epoch [96/120    avg_loss:0.008, val_acc:0.975]
Epoch [97/120    avg_loss:0.011, val_acc:0.964]
Epoch [98/120    avg_loss:0.011, val_acc:0.973]
Epoch [99/120    avg_loss:0.006, val_acc:0.974]
Epoch [100/120    avg_loss:0.018, val_acc:0.976]
Epoch [101/120    avg_loss:0.007, val_acc:0.980]
Epoch [102/120    avg_loss:0.008, val_acc:0.979]
Epoch [103/120    avg_loss:0.005, val_acc:0.979]
Epoch [104/120    avg_loss:0.005, val_acc:0.979]
Epoch [105/120    avg_loss:0.006, val_acc:0.980]
Epoch [106/120    avg_loss:0.004, val_acc:0.980]
Epoch [107/120    avg_loss:0.008, val_acc:0.980]
Epoch [108/120    avg_loss:0.005, val_acc:0.979]
Epoch [109/120    avg_loss:0.005, val_acc:0.979]
Epoch [110/120    avg_loss:0.005, val_acc:0.979]
Epoch [111/120    avg_loss:0.006, val_acc:0.979]
Epoch [112/120    avg_loss:0.005, val_acc:0.979]
Epoch [113/120    avg_loss:0.006, val_acc:0.979]
Epoch [114/120    avg_loss:0.005, val_acc:0.979]
Epoch [115/120    avg_loss:0.005, val_acc:0.979]
Epoch [116/120    avg_loss:0.005, val_acc:0.979]
Epoch [117/120    avg_loss:0.004, val_acc:0.979]
Epoch [118/120    avg_loss:0.005, val_acc:0.979]
Epoch [119/120    avg_loss:0.004, val_acc:0.979]
Epoch [120/120    avg_loss:0.004, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1266    6    0    1    0    0    0    0    2    8    1    0
     0    1    0]
 [   0    0    0  732    6    0    0    0    0    2    0    0    7    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    1    0    0    0    0  848   19    0    0
     0    1    0]
 [   0    0   22    5    0    1    0    0    0    2    4 2158   14    0
     0    3    1]
 [   0    0    0    1    5    0    0    0    0    0    0    0  526    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1117   22    0]
 [   0    0    0    0    0    1    3    0    0    0    0    0    0    0
    92  251    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.31165311653116

F1 scores:
[       nan 0.97619048 0.98177588 0.98189135 0.97482838 0.98964327
 0.99696049 0.98039216 0.997669   0.85714286 0.98091382 0.98202503
 0.97137581 1.         0.95023394 0.80191693 0.98224852]

Kappa:
0.9693479731886542
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f93d1290710>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.535, val_acc:0.493]
Epoch [2/120    avg_loss:2.028, val_acc:0.522]
Epoch [3/120    avg_loss:1.772, val_acc:0.577]
Epoch [4/120    avg_loss:1.609, val_acc:0.595]
Epoch [5/120    avg_loss:1.419, val_acc:0.624]
Epoch [6/120    avg_loss:1.189, val_acc:0.641]
Epoch [7/120    avg_loss:1.005, val_acc:0.743]
Epoch [8/120    avg_loss:0.845, val_acc:0.689]
Epoch [9/120    avg_loss:0.760, val_acc:0.744]
Epoch [10/120    avg_loss:0.677, val_acc:0.723]
Epoch [11/120    avg_loss:0.593, val_acc:0.749]
Epoch [12/120    avg_loss:0.547, val_acc:0.784]
Epoch [13/120    avg_loss:0.506, val_acc:0.815]
Epoch [14/120    avg_loss:0.478, val_acc:0.778]
Epoch [15/120    avg_loss:0.409, val_acc:0.835]
Epoch [16/120    avg_loss:0.360, val_acc:0.866]
Epoch [17/120    avg_loss:0.293, val_acc:0.782]
Epoch [18/120    avg_loss:0.339, val_acc:0.859]
Epoch [19/120    avg_loss:0.263, val_acc:0.868]
Epoch [20/120    avg_loss:0.194, val_acc:0.902]
Epoch [21/120    avg_loss:0.186, val_acc:0.895]
Epoch [22/120    avg_loss:0.165, val_acc:0.914]
Epoch [23/120    avg_loss:0.140, val_acc:0.906]
Epoch [24/120    avg_loss:0.139, val_acc:0.919]
Epoch [25/120    avg_loss:0.130, val_acc:0.870]
Epoch [26/120    avg_loss:0.214, val_acc:0.894]
Epoch [27/120    avg_loss:0.189, val_acc:0.887]
Epoch [28/120    avg_loss:0.160, val_acc:0.926]
Epoch [29/120    avg_loss:0.102, val_acc:0.925]
Epoch [30/120    avg_loss:0.103, val_acc:0.927]
Epoch [31/120    avg_loss:0.078, val_acc:0.933]
Epoch [32/120    avg_loss:0.083, val_acc:0.934]
Epoch [33/120    avg_loss:0.074, val_acc:0.939]
Epoch [34/120    avg_loss:0.075, val_acc:0.936]
Epoch [35/120    avg_loss:0.099, val_acc:0.932]
Epoch [36/120    avg_loss:0.100, val_acc:0.942]
Epoch [37/120    avg_loss:0.065, val_acc:0.943]
Epoch [38/120    avg_loss:0.107, val_acc:0.921]
Epoch [39/120    avg_loss:0.095, val_acc:0.936]
Epoch [40/120    avg_loss:0.051, val_acc:0.923]
Epoch [41/120    avg_loss:0.077, val_acc:0.929]
Epoch [42/120    avg_loss:0.098, val_acc:0.926]
Epoch [43/120    avg_loss:0.073, val_acc:0.946]
Epoch [44/120    avg_loss:0.286, val_acc:0.829]
Epoch [45/120    avg_loss:0.292, val_acc:0.893]
Epoch [46/120    avg_loss:0.147, val_acc:0.923]
Epoch [47/120    avg_loss:0.137, val_acc:0.895]
Epoch [48/120    avg_loss:0.097, val_acc:0.942]
Epoch [49/120    avg_loss:0.110, val_acc:0.953]
Epoch [50/120    avg_loss:0.048, val_acc:0.955]
Epoch [51/120    avg_loss:0.042, val_acc:0.959]
Epoch [52/120    avg_loss:0.038, val_acc:0.960]
Epoch [53/120    avg_loss:0.032, val_acc:0.965]
Epoch [54/120    avg_loss:0.027, val_acc:0.966]
Epoch [55/120    avg_loss:0.031, val_acc:0.961]
Epoch [56/120    avg_loss:0.022, val_acc:0.968]
Epoch [57/120    avg_loss:0.020, val_acc:0.973]
Epoch [58/120    avg_loss:0.019, val_acc:0.972]
Epoch [59/120    avg_loss:0.019, val_acc:0.965]
Epoch [60/120    avg_loss:0.023, val_acc:0.961]
Epoch [61/120    avg_loss:0.018, val_acc:0.969]
Epoch [62/120    avg_loss:0.018, val_acc:0.968]
Epoch [63/120    avg_loss:0.019, val_acc:0.959]
Epoch [64/120    avg_loss:0.019, val_acc:0.965]
Epoch [65/120    avg_loss:0.043, val_acc:0.958]
Epoch [66/120    avg_loss:0.043, val_acc:0.963]
Epoch [67/120    avg_loss:0.022, val_acc:0.966]
Epoch [68/120    avg_loss:0.030, val_acc:0.971]
Epoch [69/120    avg_loss:0.021, val_acc:0.956]
Epoch [70/120    avg_loss:0.018, val_acc:0.970]
Epoch [71/120    avg_loss:0.013, val_acc:0.971]
Epoch [72/120    avg_loss:0.013, val_acc:0.973]
Epoch [73/120    avg_loss:0.010, val_acc:0.975]
Epoch [74/120    avg_loss:0.012, val_acc:0.974]
Epoch [75/120    avg_loss:0.012, val_acc:0.975]
Epoch [76/120    avg_loss:0.010, val_acc:0.975]
Epoch [77/120    avg_loss:0.009, val_acc:0.976]
Epoch [78/120    avg_loss:0.011, val_acc:0.977]
Epoch [79/120    avg_loss:0.008, val_acc:0.976]
Epoch [80/120    avg_loss:0.008, val_acc:0.976]
Epoch [81/120    avg_loss:0.010, val_acc:0.977]
Epoch [82/120    avg_loss:0.009, val_acc:0.978]
Epoch [83/120    avg_loss:0.009, val_acc:0.977]
Epoch [84/120    avg_loss:0.008, val_acc:0.976]
Epoch [85/120    avg_loss:0.007, val_acc:0.976]
Epoch [86/120    avg_loss:0.008, val_acc:0.978]
Epoch [87/120    avg_loss:0.009, val_acc:0.979]
Epoch [88/120    avg_loss:0.008, val_acc:0.976]
Epoch [89/120    avg_loss:0.009, val_acc:0.976]
Epoch [90/120    avg_loss:0.009, val_acc:0.976]
Epoch [91/120    avg_loss:0.010, val_acc:0.977]
Epoch [92/120    avg_loss:0.008, val_acc:0.976]
Epoch [93/120    avg_loss:0.007, val_acc:0.976]
Epoch [94/120    avg_loss:0.008, val_acc:0.976]
Epoch [95/120    avg_loss:0.008, val_acc:0.976]
Epoch [96/120    avg_loss:0.008, val_acc:0.979]
Epoch [97/120    avg_loss:0.007, val_acc:0.976]
Epoch [98/120    avg_loss:0.009, val_acc:0.975]
Epoch [99/120    avg_loss:0.008, val_acc:0.975]
Epoch [100/120    avg_loss:0.007, val_acc:0.976]
Epoch [101/120    avg_loss:0.008, val_acc:0.977]
Epoch [102/120    avg_loss:0.008, val_acc:0.976]
Epoch [103/120    avg_loss:0.010, val_acc:0.975]
Epoch [104/120    avg_loss:0.006, val_acc:0.975]
Epoch [105/120    avg_loss:0.008, val_acc:0.975]
Epoch [106/120    avg_loss:0.008, val_acc:0.976]
Epoch [107/120    avg_loss:0.009, val_acc:0.975]
Epoch [108/120    avg_loss:0.008, val_acc:0.974]
Epoch [109/120    avg_loss:0.008, val_acc:0.975]
Epoch [110/120    avg_loss:0.010, val_acc:0.975]
Epoch [111/120    avg_loss:0.006, val_acc:0.975]
Epoch [112/120    avg_loss:0.006, val_acc:0.975]
Epoch [113/120    avg_loss:0.008, val_acc:0.975]
Epoch [114/120    avg_loss:0.008, val_acc:0.974]
Epoch [115/120    avg_loss:0.009, val_acc:0.974]
Epoch [116/120    avg_loss:0.007, val_acc:0.975]
Epoch [117/120    avg_loss:0.008, val_acc:0.975]
Epoch [118/120    avg_loss:0.008, val_acc:0.976]
Epoch [119/120    avg_loss:0.009, val_acc:0.976]
Epoch [120/120    avg_loss:0.008, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1261    5    0    0    0    0    0    1    6   10    2    0
     0    0    0]
 [   0    0    0  734    1    0    0    0    0    1    2    6    3    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    0    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   12    0    0    0    0    0    0    0  840   22    0    0
     0    1    0]
 [   0    0   14    0    0    0    0    0    0    0   32 2163    0    0
     0    1    0]
 [   0    0    1    0    0    0    0    0    0    0    2    4  521    0
     0    4    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    90  253    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.19241192411924

F1 scores:
[       nan 0.93975904 0.98017878 0.9872226  0.99530516 0.99071926
 0.99469295 1.         0.99649942 0.88888889 0.95508812 0.97984145
 0.98209237 1.         0.95173582 0.81481481 0.98224852]

Kappa:
0.9679648967859015
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f81960a2780>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.539, val_acc:0.519]
Epoch [2/120    avg_loss:2.000, val_acc:0.603]
Epoch [3/120    avg_loss:1.707, val_acc:0.582]
Epoch [4/120    avg_loss:1.492, val_acc:0.667]
Epoch [5/120    avg_loss:1.281, val_acc:0.626]
Epoch [6/120    avg_loss:1.070, val_acc:0.727]
Epoch [7/120    avg_loss:0.928, val_acc:0.714]
Epoch [8/120    avg_loss:0.781, val_acc:0.724]
Epoch [9/120    avg_loss:0.675, val_acc:0.756]
Epoch [10/120    avg_loss:0.720, val_acc:0.738]
Epoch [11/120    avg_loss:0.635, val_acc:0.782]
Epoch [12/120    avg_loss:0.508, val_acc:0.826]
Epoch [13/120    avg_loss:0.472, val_acc:0.846]
Epoch [14/120    avg_loss:0.482, val_acc:0.810]
Epoch [15/120    avg_loss:0.396, val_acc:0.882]
Epoch [16/120    avg_loss:0.341, val_acc:0.898]
Epoch [17/120    avg_loss:0.295, val_acc:0.850]
Epoch [18/120    avg_loss:0.320, val_acc:0.855]
Epoch [19/120    avg_loss:0.360, val_acc:0.918]
Epoch [20/120    avg_loss:0.213, val_acc:0.879]
Epoch [21/120    avg_loss:0.242, val_acc:0.854]
Epoch [22/120    avg_loss:0.174, val_acc:0.922]
Epoch [23/120    avg_loss:0.180, val_acc:0.910]
Epoch [24/120    avg_loss:0.174, val_acc:0.932]
Epoch [25/120    avg_loss:0.139, val_acc:0.933]
Epoch [26/120    avg_loss:0.112, val_acc:0.946]
Epoch [27/120    avg_loss:0.119, val_acc:0.956]
Epoch [28/120    avg_loss:0.108, val_acc:0.954]
Epoch [29/120    avg_loss:0.120, val_acc:0.927]
Epoch [30/120    avg_loss:0.197, val_acc:0.905]
Epoch [31/120    avg_loss:0.170, val_acc:0.928]
Epoch [32/120    avg_loss:0.103, val_acc:0.936]
Epoch [33/120    avg_loss:0.111, val_acc:0.953]
Epoch [34/120    avg_loss:0.081, val_acc:0.960]
Epoch [35/120    avg_loss:0.078, val_acc:0.926]
Epoch [36/120    avg_loss:0.082, val_acc:0.942]
Epoch [37/120    avg_loss:0.058, val_acc:0.965]
Epoch [38/120    avg_loss:0.052, val_acc:0.949]
Epoch [39/120    avg_loss:0.065, val_acc:0.959]
Epoch [40/120    avg_loss:0.054, val_acc:0.968]
Epoch [41/120    avg_loss:0.059, val_acc:0.940]
Epoch [42/120    avg_loss:0.071, val_acc:0.957]
Epoch [43/120    avg_loss:0.048, val_acc:0.975]
Epoch [44/120    avg_loss:0.065, val_acc:0.948]
Epoch [45/120    avg_loss:0.088, val_acc:0.964]
Epoch [46/120    avg_loss:0.074, val_acc:0.955]
Epoch [47/120    avg_loss:0.073, val_acc:0.964]
Epoch [48/120    avg_loss:0.043, val_acc:0.960]
Epoch [49/120    avg_loss:0.047, val_acc:0.960]
Epoch [50/120    avg_loss:0.043, val_acc:0.967]
Epoch [51/120    avg_loss:0.034, val_acc:0.968]
Epoch [52/120    avg_loss:0.023, val_acc:0.971]
Epoch [53/120    avg_loss:0.025, val_acc:0.975]
Epoch [54/120    avg_loss:0.033, val_acc:0.967]
Epoch [55/120    avg_loss:0.021, val_acc:0.977]
Epoch [56/120    avg_loss:0.023, val_acc:0.965]
Epoch [57/120    avg_loss:0.023, val_acc:0.963]
Epoch [58/120    avg_loss:0.035, val_acc:0.966]
Epoch [59/120    avg_loss:0.047, val_acc:0.961]
Epoch [60/120    avg_loss:0.030, val_acc:0.968]
Epoch [61/120    avg_loss:0.033, val_acc:0.973]
Epoch [62/120    avg_loss:0.034, val_acc:0.965]
Epoch [63/120    avg_loss:0.021, val_acc:0.977]
Epoch [64/120    avg_loss:0.025, val_acc:0.974]
Epoch [65/120    avg_loss:0.021, val_acc:0.969]
Epoch [66/120    avg_loss:0.019, val_acc:0.967]
Epoch [67/120    avg_loss:0.020, val_acc:0.978]
Epoch [68/120    avg_loss:0.020, val_acc:0.975]
Epoch [69/120    avg_loss:0.016, val_acc:0.981]
Epoch [70/120    avg_loss:0.020, val_acc:0.960]
Epoch [71/120    avg_loss:0.019, val_acc:0.979]
Epoch [72/120    avg_loss:0.015, val_acc:0.976]
Epoch [73/120    avg_loss:0.017, val_acc:0.976]
Epoch [74/120    avg_loss:0.015, val_acc:0.979]
Epoch [75/120    avg_loss:0.025, val_acc:0.972]
Epoch [76/120    avg_loss:0.038, val_acc:0.951]
Epoch [77/120    avg_loss:0.043, val_acc:0.936]
Epoch [78/120    avg_loss:0.161, val_acc:0.911]
Epoch [79/120    avg_loss:0.508, val_acc:0.850]
Epoch [80/120    avg_loss:0.216, val_acc:0.893]
Epoch [81/120    avg_loss:0.114, val_acc:0.948]
Epoch [82/120    avg_loss:0.081, val_acc:0.953]
Epoch [83/120    avg_loss:0.053, val_acc:0.970]
Epoch [84/120    avg_loss:0.041, val_acc:0.969]
Epoch [85/120    avg_loss:0.037, val_acc:0.965]
Epoch [86/120    avg_loss:0.034, val_acc:0.968]
Epoch [87/120    avg_loss:0.032, val_acc:0.972]
Epoch [88/120    avg_loss:0.039, val_acc:0.969]
Epoch [89/120    avg_loss:0.028, val_acc:0.971]
Epoch [90/120    avg_loss:0.026, val_acc:0.971]
Epoch [91/120    avg_loss:0.028, val_acc:0.970]
Epoch [92/120    avg_loss:0.028, val_acc:0.971]
Epoch [93/120    avg_loss:0.024, val_acc:0.972]
Epoch [94/120    avg_loss:0.026, val_acc:0.970]
Epoch [95/120    avg_loss:0.024, val_acc:0.970]
Epoch [96/120    avg_loss:0.028, val_acc:0.970]
Epoch [97/120    avg_loss:0.034, val_acc:0.970]
Epoch [98/120    avg_loss:0.027, val_acc:0.970]
Epoch [99/120    avg_loss:0.024, val_acc:0.970]
Epoch [100/120    avg_loss:0.023, val_acc:0.972]
Epoch [101/120    avg_loss:0.025, val_acc:0.973]
Epoch [102/120    avg_loss:0.027, val_acc:0.973]
Epoch [103/120    avg_loss:0.021, val_acc:0.973]
Epoch [104/120    avg_loss:0.027, val_acc:0.973]
Epoch [105/120    avg_loss:0.023, val_acc:0.973]
Epoch [106/120    avg_loss:0.024, val_acc:0.973]
Epoch [107/120    avg_loss:0.029, val_acc:0.973]
Epoch [108/120    avg_loss:0.024, val_acc:0.973]
Epoch [109/120    avg_loss:0.023, val_acc:0.973]
Epoch [110/120    avg_loss:0.023, val_acc:0.973]
Epoch [111/120    avg_loss:0.023, val_acc:0.973]
Epoch [112/120    avg_loss:0.023, val_acc:0.973]
Epoch [113/120    avg_loss:0.025, val_acc:0.973]
Epoch [114/120    avg_loss:0.025, val_acc:0.973]
Epoch [115/120    avg_loss:0.024, val_acc:0.973]
Epoch [116/120    avg_loss:0.025, val_acc:0.973]
Epoch [117/120    avg_loss:0.021, val_acc:0.973]
Epoch [118/120    avg_loss:0.021, val_acc:0.973]
Epoch [119/120    avg_loss:0.023, val_acc:0.973]
Epoch [120/120    avg_loss:0.023, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1264    8    0    0    2    0    0    1    1    8    1    0
     0    0    0]
 [   0    0    1  730    5    0    0    0    0    1    1    2    5    0
     0    2    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    1    4    0    0    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    3    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    1    0    0    0    0  839   30    0    0
     0    1    0]
 [   0    0   21    0    0    0    0    0    0    1   31 2138   14    0
     0    4    1]
 [   0    0    0    5    0    0    0    0    0    0    0    0  525    0
     0    2    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1118   21    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    57  277    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.10569105691057

F1 scores:
[       nan 0.98765432 0.98136646 0.97986577 0.98839907 0.98372093
 0.98415094 0.92592593 1.         0.92307692 0.96050372 0.97381007
 0.97132285 0.99728997 0.96254843 0.8470948  0.9704142 ]

Kappa:
0.9670083150980462
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa87a3ba860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.559, val_acc:0.445]
Epoch [2/120    avg_loss:2.095, val_acc:0.541]
Epoch [3/120    avg_loss:1.766, val_acc:0.558]
Epoch [4/120    avg_loss:1.543, val_acc:0.621]
Epoch [5/120    avg_loss:1.327, val_acc:0.680]
Epoch [6/120    avg_loss:1.105, val_acc:0.665]
Epoch [7/120    avg_loss:1.068, val_acc:0.711]
Epoch [8/120    avg_loss:0.831, val_acc:0.740]
Epoch [9/120    avg_loss:0.778, val_acc:0.712]
Epoch [10/120    avg_loss:0.690, val_acc:0.762]
Epoch [11/120    avg_loss:0.591, val_acc:0.766]
Epoch [12/120    avg_loss:0.519, val_acc:0.800]
Epoch [13/120    avg_loss:0.452, val_acc:0.796]
Epoch [14/120    avg_loss:0.467, val_acc:0.789]
Epoch [15/120    avg_loss:0.395, val_acc:0.857]
Epoch [16/120    avg_loss:0.315, val_acc:0.831]
Epoch [17/120    avg_loss:0.259, val_acc:0.888]
Epoch [18/120    avg_loss:0.231, val_acc:0.893]
Epoch [19/120    avg_loss:0.209, val_acc:0.907]
Epoch [20/120    avg_loss:0.231, val_acc:0.857]
Epoch [21/120    avg_loss:0.238, val_acc:0.906]
Epoch [22/120    avg_loss:0.194, val_acc:0.898]
Epoch [23/120    avg_loss:0.243, val_acc:0.884]
Epoch [24/120    avg_loss:0.231, val_acc:0.909]
Epoch [25/120    avg_loss:0.200, val_acc:0.912]
Epoch [26/120    avg_loss:0.220, val_acc:0.907]
Epoch [27/120    avg_loss:0.153, val_acc:0.932]
Epoch [28/120    avg_loss:0.106, val_acc:0.939]
Epoch [29/120    avg_loss:0.108, val_acc:0.930]
Epoch [30/120    avg_loss:0.121, val_acc:0.895]
Epoch [31/120    avg_loss:0.294, val_acc:0.881]
Epoch [32/120    avg_loss:0.220, val_acc:0.916]
Epoch [33/120    avg_loss:0.139, val_acc:0.939]
Epoch [34/120    avg_loss:0.107, val_acc:0.907]
Epoch [35/120    avg_loss:0.175, val_acc:0.902]
Epoch [36/120    avg_loss:0.110, val_acc:0.934]
Epoch [37/120    avg_loss:0.089, val_acc:0.948]
Epoch [38/120    avg_loss:0.086, val_acc:0.938]
Epoch [39/120    avg_loss:0.083, val_acc:0.947]
Epoch [40/120    avg_loss:0.060, val_acc:0.944]
Epoch [41/120    avg_loss:0.054, val_acc:0.966]
Epoch [42/120    avg_loss:0.053, val_acc:0.955]
Epoch [43/120    avg_loss:0.043, val_acc:0.955]
Epoch [44/120    avg_loss:0.036, val_acc:0.953]
Epoch [45/120    avg_loss:0.057, val_acc:0.956]
Epoch [46/120    avg_loss:0.046, val_acc:0.957]
Epoch [47/120    avg_loss:0.039, val_acc:0.972]
Epoch [48/120    avg_loss:0.042, val_acc:0.965]
Epoch [49/120    avg_loss:0.048, val_acc:0.971]
Epoch [50/120    avg_loss:0.047, val_acc:0.962]
Epoch [51/120    avg_loss:0.028, val_acc:0.968]
Epoch [52/120    avg_loss:0.034, val_acc:0.960]
Epoch [53/120    avg_loss:0.026, val_acc:0.969]
Epoch [54/120    avg_loss:0.022, val_acc:0.969]
Epoch [55/120    avg_loss:0.029, val_acc:0.965]
Epoch [56/120    avg_loss:0.023, val_acc:0.965]
Epoch [57/120    avg_loss:0.024, val_acc:0.970]
Epoch [58/120    avg_loss:0.026, val_acc:0.970]
Epoch [59/120    avg_loss:0.025, val_acc:0.963]
Epoch [60/120    avg_loss:0.029, val_acc:0.969]
Epoch [61/120    avg_loss:0.016, val_acc:0.972]
Epoch [62/120    avg_loss:0.013, val_acc:0.972]
Epoch [63/120    avg_loss:0.012, val_acc:0.973]
Epoch [64/120    avg_loss:0.011, val_acc:0.974]
Epoch [65/120    avg_loss:0.011, val_acc:0.975]
Epoch [66/120    avg_loss:0.017, val_acc:0.973]
Epoch [67/120    avg_loss:0.012, val_acc:0.974]
Epoch [68/120    avg_loss:0.013, val_acc:0.976]
Epoch [69/120    avg_loss:0.016, val_acc:0.976]
Epoch [70/120    avg_loss:0.011, val_acc:0.977]
Epoch [71/120    avg_loss:0.015, val_acc:0.975]
Epoch [72/120    avg_loss:0.015, val_acc:0.972]
Epoch [73/120    avg_loss:0.016, val_acc:0.978]
Epoch [74/120    avg_loss:0.013, val_acc:0.978]
Epoch [75/120    avg_loss:0.012, val_acc:0.978]
Epoch [76/120    avg_loss:0.013, val_acc:0.978]
Epoch [77/120    avg_loss:0.012, val_acc:0.978]
Epoch [78/120    avg_loss:0.012, val_acc:0.980]
Epoch [79/120    avg_loss:0.011, val_acc:0.981]
Epoch [80/120    avg_loss:0.012, val_acc:0.978]
Epoch [81/120    avg_loss:0.012, val_acc:0.978]
Epoch [82/120    avg_loss:0.009, val_acc:0.977]
Epoch [83/120    avg_loss:0.010, val_acc:0.977]
Epoch [84/120    avg_loss:0.012, val_acc:0.977]
Epoch [85/120    avg_loss:0.012, val_acc:0.975]
Epoch [86/120    avg_loss:0.011, val_acc:0.975]
Epoch [87/120    avg_loss:0.011, val_acc:0.976]
Epoch [88/120    avg_loss:0.013, val_acc:0.973]
Epoch [89/120    avg_loss:0.011, val_acc:0.975]
Epoch [90/120    avg_loss:0.009, val_acc:0.975]
Epoch [91/120    avg_loss:0.011, val_acc:0.978]
Epoch [92/120    avg_loss:0.011, val_acc:0.978]
Epoch [93/120    avg_loss:0.011, val_acc:0.978]
Epoch [94/120    avg_loss:0.009, val_acc:0.978]
Epoch [95/120    avg_loss:0.011, val_acc:0.978]
Epoch [96/120    avg_loss:0.009, val_acc:0.978]
Epoch [97/120    avg_loss:0.010, val_acc:0.978]
Epoch [98/120    avg_loss:0.012, val_acc:0.978]
Epoch [99/120    avg_loss:0.009, val_acc:0.978]
Epoch [100/120    avg_loss:0.011, val_acc:0.978]
Epoch [101/120    avg_loss:0.011, val_acc:0.978]
Epoch [102/120    avg_loss:0.008, val_acc:0.978]
Epoch [103/120    avg_loss:0.010, val_acc:0.978]
Epoch [104/120    avg_loss:0.010, val_acc:0.978]
Epoch [105/120    avg_loss:0.009, val_acc:0.978]
Epoch [106/120    avg_loss:0.012, val_acc:0.978]
Epoch [107/120    avg_loss:0.013, val_acc:0.978]
Epoch [108/120    avg_loss:0.011, val_acc:0.978]
Epoch [109/120    avg_loss:0.008, val_acc:0.978]
Epoch [110/120    avg_loss:0.015, val_acc:0.978]
Epoch [111/120    avg_loss:0.009, val_acc:0.978]
Epoch [112/120    avg_loss:0.008, val_acc:0.978]
Epoch [113/120    avg_loss:0.012, val_acc:0.978]
Epoch [114/120    avg_loss:0.010, val_acc:0.978]
Epoch [115/120    avg_loss:0.009, val_acc:0.978]
Epoch [116/120    avg_loss:0.011, val_acc:0.978]
Epoch [117/120    avg_loss:0.012, val_acc:0.978]
Epoch [118/120    avg_loss:0.010, val_acc:0.978]
Epoch [119/120    avg_loss:0.011, val_acc:0.978]
Epoch [120/120    avg_loss:0.009, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1258    4    1    0    0    0    0    0    2   15    5    0
     0    0    0]
 [   0    0    0  729    0    2    0    0    0    3    0    8    5    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    2    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    1    0    0    0    0    0    0  848   23    0    0
     0    0    0]
 [   0    0   31    3    0    0    0    0    0    0   10 2164    0    0
     1    1    0]
 [   0    0    0    1    0    0    0    0    0    0    0    1  527    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
  1129    8    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    92  246    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.31165311653116

F1 scores:
[       nan 0.94871795 0.97632906 0.98115747 0.99530516 0.9954023
 0.99017385 0.96153846 0.99883586 0.92307692 0.97527315 0.97874265
 0.98137803 1.         0.95596952 0.81322314 0.97619048]

Kappa:
0.9693211271523553
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8bdcb1a828>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.520, val_acc:0.522]
Epoch [2/120    avg_loss:1.955, val_acc:0.602]
Epoch [3/120    avg_loss:1.680, val_acc:0.597]
Epoch [4/120    avg_loss:1.452, val_acc:0.679]
Epoch [5/120    avg_loss:1.270, val_acc:0.662]
Epoch [6/120    avg_loss:1.105, val_acc:0.697]
Epoch [7/120    avg_loss:1.031, val_acc:0.711]
Epoch [8/120    avg_loss:0.818, val_acc:0.781]
Epoch [9/120    avg_loss:0.688, val_acc:0.798]
Epoch [10/120    avg_loss:0.631, val_acc:0.796]
Epoch [11/120    avg_loss:0.520, val_acc:0.784]
Epoch [12/120    avg_loss:0.532, val_acc:0.816]
Epoch [13/120    avg_loss:0.517, val_acc:0.852]
Epoch [14/120    avg_loss:0.422, val_acc:0.866]
Epoch [15/120    avg_loss:0.323, val_acc:0.848]
Epoch [16/120    avg_loss:0.362, val_acc:0.863]
Epoch [17/120    avg_loss:0.398, val_acc:0.865]
Epoch [18/120    avg_loss:0.306, val_acc:0.827]
Epoch [19/120    avg_loss:0.288, val_acc:0.886]
Epoch [20/120    avg_loss:0.312, val_acc:0.888]
Epoch [21/120    avg_loss:0.230, val_acc:0.897]
Epoch [22/120    avg_loss:0.224, val_acc:0.900]
Epoch [23/120    avg_loss:0.165, val_acc:0.930]
Epoch [24/120    avg_loss:0.144, val_acc:0.929]
Epoch [25/120    avg_loss:0.114, val_acc:0.939]
Epoch [26/120    avg_loss:0.145, val_acc:0.913]
Epoch [27/120    avg_loss:0.251, val_acc:0.848]
Epoch [28/120    avg_loss:0.201, val_acc:0.912]
Epoch [29/120    avg_loss:0.156, val_acc:0.928]
Epoch [30/120    avg_loss:0.134, val_acc:0.918]
Epoch [31/120    avg_loss:0.118, val_acc:0.930]
Epoch [32/120    avg_loss:0.094, val_acc:0.946]
Epoch [33/120    avg_loss:0.070, val_acc:0.952]
Epoch [34/120    avg_loss:0.089, val_acc:0.922]
Epoch [35/120    avg_loss:0.092, val_acc:0.931]
Epoch [36/120    avg_loss:0.069, val_acc:0.935]
Epoch [37/120    avg_loss:0.065, val_acc:0.947]
Epoch [38/120    avg_loss:0.072, val_acc:0.932]
Epoch [39/120    avg_loss:0.057, val_acc:0.945]
Epoch [40/120    avg_loss:0.067, val_acc:0.945]
Epoch [41/120    avg_loss:0.074, val_acc:0.933]
Epoch [42/120    avg_loss:0.072, val_acc:0.918]
Epoch [43/120    avg_loss:0.064, val_acc:0.935]
Epoch [44/120    avg_loss:0.060, val_acc:0.944]
Epoch [45/120    avg_loss:0.058, val_acc:0.945]
Epoch [46/120    avg_loss:0.053, val_acc:0.951]
Epoch [47/120    avg_loss:0.034, val_acc:0.955]
Epoch [48/120    avg_loss:0.031, val_acc:0.955]
Epoch [49/120    avg_loss:0.029, val_acc:0.956]
Epoch [50/120    avg_loss:0.025, val_acc:0.954]
Epoch [51/120    avg_loss:0.023, val_acc:0.953]
Epoch [52/120    avg_loss:0.023, val_acc:0.952]
Epoch [53/120    avg_loss:0.030, val_acc:0.950]
Epoch [54/120    avg_loss:0.024, val_acc:0.954]
Epoch [55/120    avg_loss:0.026, val_acc:0.957]
Epoch [56/120    avg_loss:0.023, val_acc:0.954]
Epoch [57/120    avg_loss:0.025, val_acc:0.957]
Epoch [58/120    avg_loss:0.022, val_acc:0.956]
Epoch [59/120    avg_loss:0.019, val_acc:0.956]
Epoch [60/120    avg_loss:0.026, val_acc:0.954]
Epoch [61/120    avg_loss:0.024, val_acc:0.957]
Epoch [62/120    avg_loss:0.022, val_acc:0.954]
Epoch [63/120    avg_loss:0.019, val_acc:0.953]
Epoch [64/120    avg_loss:0.020, val_acc:0.956]
Epoch [65/120    avg_loss:0.029, val_acc:0.954]
Epoch [66/120    avg_loss:0.026, val_acc:0.959]
Epoch [67/120    avg_loss:0.021, val_acc:0.960]
Epoch [68/120    avg_loss:0.019, val_acc:0.957]
Epoch [69/120    avg_loss:0.017, val_acc:0.957]
Epoch [70/120    avg_loss:0.019, val_acc:0.958]
Epoch [71/120    avg_loss:0.017, val_acc:0.957]
Epoch [72/120    avg_loss:0.027, val_acc:0.957]
Epoch [73/120    avg_loss:0.020, val_acc:0.960]
Epoch [74/120    avg_loss:0.017, val_acc:0.959]
Epoch [75/120    avg_loss:0.019, val_acc:0.955]
Epoch [76/120    avg_loss:0.017, val_acc:0.958]
Epoch [77/120    avg_loss:0.018, val_acc:0.963]
Epoch [78/120    avg_loss:0.019, val_acc:0.965]
Epoch [79/120    avg_loss:0.018, val_acc:0.962]
Epoch [80/120    avg_loss:0.019, val_acc:0.960]
Epoch [81/120    avg_loss:0.023, val_acc:0.956]
Epoch [82/120    avg_loss:0.018, val_acc:0.959]
Epoch [83/120    avg_loss:0.017, val_acc:0.961]
Epoch [84/120    avg_loss:0.018, val_acc:0.956]
Epoch [85/120    avg_loss:0.018, val_acc:0.958]
Epoch [86/120    avg_loss:0.019, val_acc:0.961]
Epoch [87/120    avg_loss:0.018, val_acc:0.956]
Epoch [88/120    avg_loss:0.016, val_acc:0.957]
Epoch [89/120    avg_loss:0.018, val_acc:0.956]
Epoch [90/120    avg_loss:0.014, val_acc:0.958]
Epoch [91/120    avg_loss:0.017, val_acc:0.959]
Epoch [92/120    avg_loss:0.015, val_acc:0.960]
Epoch [93/120    avg_loss:0.015, val_acc:0.960]
Epoch [94/120    avg_loss:0.017, val_acc:0.960]
Epoch [95/120    avg_loss:0.017, val_acc:0.960]
Epoch [96/120    avg_loss:0.015, val_acc:0.959]
Epoch [97/120    avg_loss:0.014, val_acc:0.959]
Epoch [98/120    avg_loss:0.016, val_acc:0.960]
Epoch [99/120    avg_loss:0.014, val_acc:0.959]
Epoch [100/120    avg_loss:0.017, val_acc:0.960]
Epoch [101/120    avg_loss:0.013, val_acc:0.960]
Epoch [102/120    avg_loss:0.016, val_acc:0.960]
Epoch [103/120    avg_loss:0.014, val_acc:0.960]
Epoch [104/120    avg_loss:0.017, val_acc:0.960]
Epoch [105/120    avg_loss:0.016, val_acc:0.960]
Epoch [106/120    avg_loss:0.014, val_acc:0.960]
Epoch [107/120    avg_loss:0.015, val_acc:0.960]
Epoch [108/120    avg_loss:0.018, val_acc:0.960]
Epoch [109/120    avg_loss:0.015, val_acc:0.960]
Epoch [110/120    avg_loss:0.014, val_acc:0.960]
Epoch [111/120    avg_loss:0.014, val_acc:0.960]
Epoch [112/120    avg_loss:0.017, val_acc:0.960]
Epoch [113/120    avg_loss:0.015, val_acc:0.960]
Epoch [114/120    avg_loss:0.014, val_acc:0.960]
Epoch [115/120    avg_loss:0.019, val_acc:0.960]
Epoch [116/120    avg_loss:0.016, val_acc:0.960]
Epoch [117/120    avg_loss:0.017, val_acc:0.960]
Epoch [118/120    avg_loss:0.014, val_acc:0.960]
Epoch [119/120    avg_loss:0.016, val_acc:0.960]
Epoch [120/120    avg_loss:0.016, val_acc:0.960]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1253    0    0    3    0    0    0    0    4   25    0    0
     0    0    0]
 [   0    0    0  723    0    0    0    0    0    1    0    5   16    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0  858   15    0    0
     0    0    0]
 [   0    0   13    0    0    0    0    0    0    1   41 2139   16    0
     0    0    0]
 [   0    0    0    7    1    0    0    0    0    0    0    0  525    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1123   16    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    94  253    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.07317073170732

F1 scores:
[       nan 0.975      0.98082192 0.97901151 0.99765808 0.99656357
 0.99771167 1.         1.         0.94736842 0.96512936 0.97315742
 0.96065874 0.99462366 0.95290624 0.82142857 0.98203593]

Kappa:
0.9666245413864263
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f32d135a860>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.508, val_acc:0.532]
Epoch [2/120    avg_loss:1.996, val_acc:0.596]
Epoch [3/120    avg_loss:1.752, val_acc:0.598]
Epoch [4/120    avg_loss:1.564, val_acc:0.605]
Epoch [5/120    avg_loss:1.409, val_acc:0.664]
Epoch [6/120    avg_loss:1.272, val_acc:0.693]
Epoch [7/120    avg_loss:1.038, val_acc:0.692]
Epoch [8/120    avg_loss:0.913, val_acc:0.706]
Epoch [9/120    avg_loss:0.794, val_acc:0.762]
Epoch [10/120    avg_loss:0.684, val_acc:0.772]
Epoch [11/120    avg_loss:0.647, val_acc:0.779]
Epoch [12/120    avg_loss:0.560, val_acc:0.817]
Epoch [13/120    avg_loss:0.507, val_acc:0.818]
Epoch [14/120    avg_loss:0.530, val_acc:0.817]
Epoch [15/120    avg_loss:0.508, val_acc:0.817]
Epoch [16/120    avg_loss:0.461, val_acc:0.827]
Epoch [17/120    avg_loss:0.368, val_acc:0.858]
Epoch [18/120    avg_loss:0.332, val_acc:0.828]
Epoch [19/120    avg_loss:0.359, val_acc:0.831]
Epoch [20/120    avg_loss:0.304, val_acc:0.881]
Epoch [21/120    avg_loss:0.254, val_acc:0.868]
Epoch [22/120    avg_loss:0.306, val_acc:0.884]
Epoch [23/120    avg_loss:0.241, val_acc:0.879]
Epoch [24/120    avg_loss:0.246, val_acc:0.905]
Epoch [25/120    avg_loss:0.232, val_acc:0.901]
Epoch [26/120    avg_loss:0.197, val_acc:0.920]
Epoch [27/120    avg_loss:0.179, val_acc:0.897]
Epoch [28/120    avg_loss:0.218, val_acc:0.867]
Epoch [29/120    avg_loss:0.385, val_acc:0.833]
Epoch [30/120    avg_loss:0.265, val_acc:0.918]
Epoch [31/120    avg_loss:0.217, val_acc:0.914]
Epoch [32/120    avg_loss:0.169, val_acc:0.922]
Epoch [33/120    avg_loss:0.151, val_acc:0.915]
Epoch [34/120    avg_loss:0.338, val_acc:0.896]
Epoch [35/120    avg_loss:0.199, val_acc:0.926]
Epoch [36/120    avg_loss:0.125, val_acc:0.925]
Epoch [37/120    avg_loss:0.107, val_acc:0.916]
Epoch [38/120    avg_loss:0.108, val_acc:0.927]
Epoch [39/120    avg_loss:0.103, val_acc:0.915]
Epoch [40/120    avg_loss:0.091, val_acc:0.947]
Epoch [41/120    avg_loss:0.079, val_acc:0.947]
Epoch [42/120    avg_loss:0.083, val_acc:0.953]
Epoch [43/120    avg_loss:0.073, val_acc:0.934]
Epoch [44/120    avg_loss:0.072, val_acc:0.929]
Epoch [45/120    avg_loss:0.071, val_acc:0.955]
Epoch [46/120    avg_loss:0.089, val_acc:0.956]
Epoch [47/120    avg_loss:0.087, val_acc:0.950]
Epoch [48/120    avg_loss:0.081, val_acc:0.948]
Epoch [49/120    avg_loss:0.065, val_acc:0.945]
Epoch [50/120    avg_loss:0.068, val_acc:0.944]
Epoch [51/120    avg_loss:0.113, val_acc:0.932]
Epoch [52/120    avg_loss:0.056, val_acc:0.946]
Epoch [53/120    avg_loss:0.153, val_acc:0.914]
Epoch [54/120    avg_loss:0.074, val_acc:0.948]
Epoch [55/120    avg_loss:0.050, val_acc:0.956]
Epoch [56/120    avg_loss:0.041, val_acc:0.961]
Epoch [57/120    avg_loss:0.048, val_acc:0.954]
Epoch [58/120    avg_loss:0.092, val_acc:0.936]
Epoch [59/120    avg_loss:0.055, val_acc:0.956]
Epoch [60/120    avg_loss:0.044, val_acc:0.952]
Epoch [61/120    avg_loss:0.043, val_acc:0.938]
Epoch [62/120    avg_loss:0.060, val_acc:0.956]
Epoch [63/120    avg_loss:0.033, val_acc:0.960]
Epoch [64/120    avg_loss:0.038, val_acc:0.959]
Epoch [65/120    avg_loss:0.042, val_acc:0.961]
Epoch [66/120    avg_loss:0.032, val_acc:0.967]
Epoch [67/120    avg_loss:0.028, val_acc:0.969]
Epoch [68/120    avg_loss:0.038, val_acc:0.968]
Epoch [69/120    avg_loss:0.029, val_acc:0.956]
Epoch [70/120    avg_loss:0.029, val_acc:0.952]
Epoch [71/120    avg_loss:0.052, val_acc:0.965]
Epoch [72/120    avg_loss:0.035, val_acc:0.965]
Epoch [73/120    avg_loss:0.054, val_acc:0.960]
Epoch [74/120    avg_loss:0.021, val_acc:0.976]
Epoch [75/120    avg_loss:0.035, val_acc:0.958]
Epoch [76/120    avg_loss:0.027, val_acc:0.967]
Epoch [77/120    avg_loss:0.020, val_acc:0.970]
Epoch [78/120    avg_loss:0.029, val_acc:0.970]
Epoch [79/120    avg_loss:0.015, val_acc:0.970]
Epoch [80/120    avg_loss:0.015, val_acc:0.974]
Epoch [81/120    avg_loss:0.026, val_acc:0.964]
Epoch [82/120    avg_loss:0.027, val_acc:0.974]
Epoch [83/120    avg_loss:0.028, val_acc:0.958]
Epoch [84/120    avg_loss:0.020, val_acc:0.973]
Epoch [85/120    avg_loss:0.019, val_acc:0.977]
Epoch [86/120    avg_loss:0.016, val_acc:0.973]
Epoch [87/120    avg_loss:0.010, val_acc:0.975]
Epoch [88/120    avg_loss:0.021, val_acc:0.969]
Epoch [89/120    avg_loss:0.016, val_acc:0.974]
Epoch [90/120    avg_loss:0.013, val_acc:0.973]
Epoch [91/120    avg_loss:0.011, val_acc:0.976]
Epoch [92/120    avg_loss:0.012, val_acc:0.978]
Epoch [93/120    avg_loss:0.017, val_acc:0.969]
Epoch [94/120    avg_loss:0.012, val_acc:0.975]
Epoch [95/120    avg_loss:0.008, val_acc:0.980]
Epoch [96/120    avg_loss:0.012, val_acc:0.977]
Epoch [97/120    avg_loss:0.014, val_acc:0.980]
Epoch [98/120    avg_loss:0.013, val_acc:0.976]
Epoch [99/120    avg_loss:0.019, val_acc:0.974]
Epoch [100/120    avg_loss:0.016, val_acc:0.971]
Epoch [101/120    avg_loss:0.034, val_acc:0.973]
Epoch [102/120    avg_loss:0.015, val_acc:0.975]
Epoch [103/120    avg_loss:0.015, val_acc:0.976]
Epoch [104/120    avg_loss:0.013, val_acc:0.976]
Epoch [105/120    avg_loss:0.015, val_acc:0.973]
Epoch [106/120    avg_loss:0.008, val_acc:0.974]
Epoch [107/120    avg_loss:0.009, val_acc:0.976]
Epoch [108/120    avg_loss:0.012, val_acc:0.976]
Epoch [109/120    avg_loss:0.010, val_acc:0.979]
Epoch [110/120    avg_loss:0.008, val_acc:0.977]
Epoch [111/120    avg_loss:0.006, val_acc:0.978]
Epoch [112/120    avg_loss:0.007, val_acc:0.979]
Epoch [113/120    avg_loss:0.005, val_acc:0.979]
Epoch [114/120    avg_loss:0.007, val_acc:0.980]
Epoch [115/120    avg_loss:0.005, val_acc:0.978]
Epoch [116/120    avg_loss:0.007, val_acc:0.980]
Epoch [117/120    avg_loss:0.007, val_acc:0.981]
Epoch [118/120    avg_loss:0.006, val_acc:0.982]
Epoch [119/120    avg_loss:0.005, val_acc:0.982]
Epoch [120/120    avg_loss:0.005, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1272    0    0    0    0    0    0    0    6    7    0    0
     0    0    0]
 [   0    0    1  727    0    0    1    0    0    1    3   10    3    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    1    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    1    0    0    0    0  851   19    0    0
     0    3    0]
 [   0    0    4    0    0    1    0    0    0    0    7 2192    2    0
     3    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    8  520    0
     0    3    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    1    0    0    0    0    0    0    0    0    0    0    0
    66  280    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.08130081300813

F1 scores:
[       nan 0.975      0.99219969 0.98643148 1.         0.99076212
 0.99619193 1.         1.         0.97297297 0.97591743 0.98583315
 0.98113208 0.99730458 0.96375267 0.87091757 0.97647059]

Kappa:
0.9781004408278356
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f449f4a1898>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.522, val_acc:0.460]
Epoch [2/120    avg_loss:2.098, val_acc:0.497]
Epoch [3/120    avg_loss:1.826, val_acc:0.571]
Epoch [4/120    avg_loss:1.619, val_acc:0.624]
Epoch [5/120    avg_loss:1.453, val_acc:0.618]
Epoch [6/120    avg_loss:1.259, val_acc:0.663]
Epoch [7/120    avg_loss:1.077, val_acc:0.680]
Epoch [8/120    avg_loss:0.948, val_acc:0.751]
Epoch [9/120    avg_loss:0.826, val_acc:0.755]
Epoch [10/120    avg_loss:0.684, val_acc:0.786]
Epoch [11/120    avg_loss:0.645, val_acc:0.785]
Epoch [12/120    avg_loss:0.619, val_acc:0.780]
Epoch [13/120    avg_loss:0.508, val_acc:0.834]
Epoch [14/120    avg_loss:0.504, val_acc:0.804]
Epoch [15/120    avg_loss:0.503, val_acc:0.840]
Epoch [16/120    avg_loss:0.394, val_acc:0.806]
Epoch [17/120    avg_loss:0.408, val_acc:0.839]
Epoch [18/120    avg_loss:0.343, val_acc:0.825]
Epoch [19/120    avg_loss:0.353, val_acc:0.874]
Epoch [20/120    avg_loss:0.276, val_acc:0.897]
Epoch [21/120    avg_loss:0.208, val_acc:0.906]
Epoch [22/120    avg_loss:0.245, val_acc:0.914]
Epoch [23/120    avg_loss:0.200, val_acc:0.912]
Epoch [24/120    avg_loss:0.200, val_acc:0.908]
Epoch [25/120    avg_loss:0.212, val_acc:0.919]
Epoch [26/120    avg_loss:0.220, val_acc:0.900]
Epoch [27/120    avg_loss:0.190, val_acc:0.917]
Epoch [28/120    avg_loss:0.127, val_acc:0.914]
Epoch [29/120    avg_loss:0.120, val_acc:0.947]
Epoch [30/120    avg_loss:0.114, val_acc:0.933]
Epoch [31/120    avg_loss:0.090, val_acc:0.950]
Epoch [32/120    avg_loss:0.084, val_acc:0.948]
Epoch [33/120    avg_loss:0.103, val_acc:0.951]
Epoch [34/120    avg_loss:0.119, val_acc:0.953]
Epoch [35/120    avg_loss:0.075, val_acc:0.958]
Epoch [36/120    avg_loss:0.084, val_acc:0.956]
Epoch [37/120    avg_loss:0.074, val_acc:0.957]
Epoch [38/120    avg_loss:0.083, val_acc:0.948]
Epoch [39/120    avg_loss:0.102, val_acc:0.931]
Epoch [40/120    avg_loss:0.094, val_acc:0.954]
Epoch [41/120    avg_loss:0.053, val_acc:0.958]
Epoch [42/120    avg_loss:0.087, val_acc:0.950]
Epoch [43/120    avg_loss:0.066, val_acc:0.957]
Epoch [44/120    avg_loss:0.048, val_acc:0.964]
Epoch [45/120    avg_loss:0.042, val_acc:0.966]
Epoch [46/120    avg_loss:0.074, val_acc:0.934]
Epoch [47/120    avg_loss:0.060, val_acc:0.966]
Epoch [48/120    avg_loss:0.054, val_acc:0.939]
Epoch [49/120    avg_loss:0.052, val_acc:0.958]
Epoch [50/120    avg_loss:0.056, val_acc:0.953]
Epoch [51/120    avg_loss:0.037, val_acc:0.970]
Epoch [52/120    avg_loss:0.039, val_acc:0.970]
Epoch [53/120    avg_loss:0.046, val_acc:0.967]
Epoch [54/120    avg_loss:0.037, val_acc:0.963]
Epoch [55/120    avg_loss:0.049, val_acc:0.944]
Epoch [56/120    avg_loss:0.040, val_acc:0.966]
Epoch [57/120    avg_loss:0.036, val_acc:0.945]
Epoch [58/120    avg_loss:0.072, val_acc:0.972]
Epoch [59/120    avg_loss:0.060, val_acc:0.967]
Epoch [60/120    avg_loss:0.038, val_acc:0.965]
Epoch [61/120    avg_loss:0.044, val_acc:0.960]
Epoch [62/120    avg_loss:0.034, val_acc:0.967]
Epoch [63/120    avg_loss:0.044, val_acc:0.944]
Epoch [64/120    avg_loss:0.038, val_acc:0.975]
Epoch [65/120    avg_loss:0.026, val_acc:0.977]
Epoch [66/120    avg_loss:0.028, val_acc:0.972]
Epoch [67/120    avg_loss:0.063, val_acc:0.943]
Epoch [68/120    avg_loss:0.052, val_acc:0.974]
Epoch [69/120    avg_loss:0.035, val_acc:0.925]
Epoch [70/120    avg_loss:0.031, val_acc:0.974]
Epoch [71/120    avg_loss:0.027, val_acc:0.974]
Epoch [72/120    avg_loss:0.026, val_acc:0.978]
Epoch [73/120    avg_loss:0.019, val_acc:0.975]
Epoch [74/120    avg_loss:0.013, val_acc:0.976]
Epoch [75/120    avg_loss:0.052, val_acc:0.974]
Epoch [76/120    avg_loss:0.054, val_acc:0.969]
Epoch [77/120    avg_loss:0.026, val_acc:0.965]
Epoch [78/120    avg_loss:0.020, val_acc:0.976]
Epoch [79/120    avg_loss:0.016, val_acc:0.971]
Epoch [80/120    avg_loss:0.023, val_acc:0.977]
Epoch [81/120    avg_loss:0.025, val_acc:0.967]
Epoch [82/120    avg_loss:0.024, val_acc:0.971]
Epoch [83/120    avg_loss:0.036, val_acc:0.970]
Epoch [84/120    avg_loss:0.019, val_acc:0.977]
Epoch [85/120    avg_loss:0.016, val_acc:0.980]
Epoch [86/120    avg_loss:0.016, val_acc:0.980]
Epoch [87/120    avg_loss:0.013, val_acc:0.977]
Epoch [88/120    avg_loss:0.016, val_acc:0.977]
Epoch [89/120    avg_loss:0.014, val_acc:0.975]
Epoch [90/120    avg_loss:0.013, val_acc:0.981]
Epoch [91/120    avg_loss:0.009, val_acc:0.980]
Epoch [92/120    avg_loss:0.023, val_acc:0.977]
Epoch [93/120    avg_loss:0.024, val_acc:0.978]
Epoch [94/120    avg_loss:0.034, val_acc:0.972]
Epoch [95/120    avg_loss:0.025, val_acc:0.972]
Epoch [96/120    avg_loss:0.013, val_acc:0.977]
Epoch [97/120    avg_loss:0.011, val_acc:0.984]
Epoch [98/120    avg_loss:0.011, val_acc:0.980]
Epoch [99/120    avg_loss:0.016, val_acc:0.978]
Epoch [100/120    avg_loss:0.017, val_acc:0.975]
Epoch [101/120    avg_loss:0.037, val_acc:0.976]
Epoch [102/120    avg_loss:0.018, val_acc:0.979]
Epoch [103/120    avg_loss:0.012, val_acc:0.984]
Epoch [104/120    avg_loss:0.014, val_acc:0.976]
Epoch [105/120    avg_loss:0.010, val_acc:0.977]
Epoch [106/120    avg_loss:0.010, val_acc:0.975]
Epoch [107/120    avg_loss:0.008, val_acc:0.979]
Epoch [108/120    avg_loss:0.008, val_acc:0.976]
Epoch [109/120    avg_loss:0.010, val_acc:0.981]
Epoch [110/120    avg_loss:0.007, val_acc:0.980]
Epoch [111/120    avg_loss:0.010, val_acc:0.979]
Epoch [112/120    avg_loss:0.010, val_acc:0.976]
Epoch [113/120    avg_loss:0.009, val_acc:0.976]
Epoch [114/120    avg_loss:0.007, val_acc:0.983]
Epoch [115/120    avg_loss:0.011, val_acc:0.979]
Epoch [116/120    avg_loss:0.011, val_acc:0.985]
Epoch [117/120    avg_loss:0.010, val_acc:0.984]
Epoch [118/120    avg_loss:0.010, val_acc:0.977]
Epoch [119/120    avg_loss:0.008, val_acc:0.983]
Epoch [120/120    avg_loss:0.005, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1268    1    5    0    0    0    0    0    3    8    0    0
     0    0    0]
 [   0    0    0  743    0    0    0    0    0    0    1    1    0    1
     1    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  428    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    1    0    1    0    0    0    0  860    8    1    0
     0    0    0]
 [   0    0    5    1    0    0    4    0    0    2   10 2169   18    1
     0    0    0]
 [   0    0    0    5    0    1    0    0    0    0    5    0  518    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1128   10    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    68  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.01626016260163

F1 scores:
[       nan 0.96296296 0.98985168 0.99066667 0.98368298 0.99655568
 0.99544765 1.         0.997669   0.92307692 0.97949886 0.98680619
 0.96372093 0.99462366 0.96534018 0.87323944 0.97005988]

Kappa:
0.9773831383775736
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7fae038828>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.549, val_acc:0.484]
Epoch [2/120    avg_loss:2.049, val_acc:0.475]
Epoch [3/120    avg_loss:1.811, val_acc:0.571]
Epoch [4/120    avg_loss:1.630, val_acc:0.581]
Epoch [5/120    avg_loss:1.455, val_acc:0.648]
Epoch [6/120    avg_loss:1.313, val_acc:0.635]
Epoch [7/120    avg_loss:1.214, val_acc:0.652]
Epoch [8/120    avg_loss:0.993, val_acc:0.750]
Epoch [9/120    avg_loss:0.816, val_acc:0.736]
Epoch [10/120    avg_loss:0.739, val_acc:0.770]
Epoch [11/120    avg_loss:0.718, val_acc:0.751]
Epoch [12/120    avg_loss:0.622, val_acc:0.797]
Epoch [13/120    avg_loss:0.665, val_acc:0.779]
Epoch [14/120    avg_loss:0.458, val_acc:0.838]
Epoch [15/120    avg_loss:0.435, val_acc:0.795]
Epoch [16/120    avg_loss:0.385, val_acc:0.881]
Epoch [17/120    avg_loss:0.334, val_acc:0.900]
Epoch [18/120    avg_loss:0.480, val_acc:0.835]
Epoch [19/120    avg_loss:0.332, val_acc:0.890]
Epoch [20/120    avg_loss:0.313, val_acc:0.882]
Epoch [21/120    avg_loss:0.363, val_acc:0.880]
Epoch [22/120    avg_loss:0.277, val_acc:0.896]
Epoch [23/120    avg_loss:0.197, val_acc:0.910]
Epoch [24/120    avg_loss:0.222, val_acc:0.898]
Epoch [25/120    avg_loss:0.181, val_acc:0.898]
Epoch [26/120    avg_loss:0.171, val_acc:0.917]
Epoch [27/120    avg_loss:0.213, val_acc:0.820]
Epoch [28/120    avg_loss:0.207, val_acc:0.917]
Epoch [29/120    avg_loss:0.148, val_acc:0.901]
Epoch [30/120    avg_loss:0.151, val_acc:0.909]
Epoch [31/120    avg_loss:0.111, val_acc:0.943]
Epoch [32/120    avg_loss:0.137, val_acc:0.919]
Epoch [33/120    avg_loss:0.131, val_acc:0.936]
Epoch [34/120    avg_loss:0.163, val_acc:0.898]
Epoch [35/120    avg_loss:0.180, val_acc:0.922]
Epoch [36/120    avg_loss:0.105, val_acc:0.941]
Epoch [37/120    avg_loss:0.126, val_acc:0.938]
Epoch [38/120    avg_loss:0.088, val_acc:0.947]
Epoch [39/120    avg_loss:0.090, val_acc:0.950]
Epoch [40/120    avg_loss:0.073, val_acc:0.938]
Epoch [41/120    avg_loss:0.056, val_acc:0.956]
Epoch [42/120    avg_loss:0.066, val_acc:0.957]
Epoch [43/120    avg_loss:0.054, val_acc:0.961]
Epoch [44/120    avg_loss:0.050, val_acc:0.963]
Epoch [45/120    avg_loss:0.046, val_acc:0.964]
Epoch [46/120    avg_loss:0.045, val_acc:0.954]
Epoch [47/120    avg_loss:0.057, val_acc:0.967]
Epoch [48/120    avg_loss:0.054, val_acc:0.967]
Epoch [49/120    avg_loss:0.040, val_acc:0.966]
Epoch [50/120    avg_loss:0.047, val_acc:0.959]
Epoch [51/120    avg_loss:0.046, val_acc:0.959]
Epoch [52/120    avg_loss:0.044, val_acc:0.959]
Epoch [53/120    avg_loss:0.054, val_acc:0.953]
Epoch [54/120    avg_loss:0.098, val_acc:0.948]
Epoch [55/120    avg_loss:0.060, val_acc:0.968]
Epoch [56/120    avg_loss:0.070, val_acc:0.952]
Epoch [57/120    avg_loss:0.085, val_acc:0.956]
Epoch [58/120    avg_loss:0.051, val_acc:0.973]
Epoch [59/120    avg_loss:0.028, val_acc:0.975]
Epoch [60/120    avg_loss:0.043, val_acc:0.965]
Epoch [61/120    avg_loss:0.043, val_acc:0.964]
Epoch [62/120    avg_loss:0.027, val_acc:0.971]
Epoch [63/120    avg_loss:0.038, val_acc:0.958]
Epoch [64/120    avg_loss:0.034, val_acc:0.973]
Epoch [65/120    avg_loss:0.028, val_acc:0.973]
Epoch [66/120    avg_loss:0.044, val_acc:0.971]
Epoch [67/120    avg_loss:0.022, val_acc:0.973]
Epoch [68/120    avg_loss:0.024, val_acc:0.969]
Epoch [69/120    avg_loss:0.034, val_acc:0.958]
Epoch [70/120    avg_loss:0.049, val_acc:0.966]
Epoch [71/120    avg_loss:0.043, val_acc:0.971]
Epoch [72/120    avg_loss:0.027, val_acc:0.978]
Epoch [73/120    avg_loss:0.025, val_acc:0.975]
Epoch [74/120    avg_loss:0.022, val_acc:0.979]
Epoch [75/120    avg_loss:0.017, val_acc:0.972]
Epoch [76/120    avg_loss:0.022, val_acc:0.976]
Epoch [77/120    avg_loss:0.014, val_acc:0.984]
Epoch [78/120    avg_loss:0.026, val_acc:0.976]
Epoch [79/120    avg_loss:0.019, val_acc:0.977]
Epoch [80/120    avg_loss:0.016, val_acc:0.983]
Epoch [81/120    avg_loss:0.017, val_acc:0.974]
Epoch [82/120    avg_loss:0.011, val_acc:0.978]
Epoch [83/120    avg_loss:0.013, val_acc:0.983]
Epoch [84/120    avg_loss:0.013, val_acc:0.971]
Epoch [85/120    avg_loss:0.063, val_acc:0.971]
Epoch [86/120    avg_loss:0.024, val_acc:0.980]
Epoch [87/120    avg_loss:0.017, val_acc:0.977]
Epoch [88/120    avg_loss:0.011, val_acc:0.984]
Epoch [89/120    avg_loss:0.013, val_acc:0.982]
Epoch [90/120    avg_loss:0.017, val_acc:0.976]
Epoch [91/120    avg_loss:0.017, val_acc:0.975]
Epoch [92/120    avg_loss:0.011, val_acc:0.977]
Epoch [93/120    avg_loss:0.008, val_acc:0.981]
Epoch [94/120    avg_loss:0.010, val_acc:0.985]
Epoch [95/120    avg_loss:0.007, val_acc:0.981]
Epoch [96/120    avg_loss:0.008, val_acc:0.981]
Epoch [97/120    avg_loss:0.007, val_acc:0.985]
Epoch [98/120    avg_loss:0.006, val_acc:0.983]
Epoch [99/120    avg_loss:0.006, val_acc:0.982]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.012, val_acc:0.984]
Epoch [102/120    avg_loss:0.015, val_acc:0.972]
Epoch [103/120    avg_loss:0.009, val_acc:0.985]
Epoch [104/120    avg_loss:0.009, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.980]
Epoch [106/120    avg_loss:0.008, val_acc:0.984]
Epoch [107/120    avg_loss:0.007, val_acc:0.984]
Epoch [108/120    avg_loss:0.007, val_acc:0.983]
Epoch [109/120    avg_loss:0.011, val_acc:0.984]
Epoch [110/120    avg_loss:0.014, val_acc:0.972]
Epoch [111/120    avg_loss:0.032, val_acc:0.976]
Epoch [112/120    avg_loss:0.018, val_acc:0.976]
Epoch [113/120    avg_loss:0.011, val_acc:0.980]
Epoch [114/120    avg_loss:0.013, val_acc:0.985]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.020, val_acc:0.974]
Epoch [118/120    avg_loss:0.009, val_acc:0.976]
Epoch [119/120    avg_loss:0.007, val_acc:0.977]
Epoch [120/120    avg_loss:0.011, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1269    1    1    0    4    0    0    4    4    2    0    0
     0    0    0]
 [   0    0    0  717    0    0    0    0    0    7    1    6   16    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    6    0    0    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   12    0    0    0    0    0    0    2  841   16    0    0
     2    2    0]
 [   0    0   21    0    0    3    2    0    0    1   14 2155   14    0
     0    0    0]
 [   0    0    0    4    0    1    0    0    0    0    0    1  526    0
     0    1    1]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0  183
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    0    9    0    0    3    0    0    0    0
    70  265    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.23577235772358

F1 scores:
[       nan 0.92857143 0.98068006 0.9755102  0.99765808 0.97916667
 0.98871332 0.89285714 0.9953271  0.65384615 0.96889401 0.98177677
 0.96425298 0.99456522 0.96337308 0.85072231 0.98809524]

Kappa:
0.9684853309539622
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efcba287860>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.517, val_acc:0.445]
Epoch [2/120    avg_loss:2.026, val_acc:0.511]
Epoch [3/120    avg_loss:1.845, val_acc:0.588]
Epoch [4/120    avg_loss:1.604, val_acc:0.618]
Epoch [5/120    avg_loss:1.412, val_acc:0.631]
Epoch [6/120    avg_loss:1.244, val_acc:0.656]
Epoch [7/120    avg_loss:1.136, val_acc:0.664]
Epoch [8/120    avg_loss:1.086, val_acc:0.705]
Epoch [9/120    avg_loss:0.888, val_acc:0.706]
Epoch [10/120    avg_loss:0.868, val_acc:0.701]
Epoch [11/120    avg_loss:0.810, val_acc:0.710]
Epoch [12/120    avg_loss:0.658, val_acc:0.773]
Epoch [13/120    avg_loss:0.663, val_acc:0.754]
Epoch [14/120    avg_loss:0.577, val_acc:0.805]
Epoch [15/120    avg_loss:0.550, val_acc:0.812]
Epoch [16/120    avg_loss:0.434, val_acc:0.817]
Epoch [17/120    avg_loss:0.408, val_acc:0.819]
Epoch [18/120    avg_loss:0.425, val_acc:0.830]
Epoch [19/120    avg_loss:0.386, val_acc:0.810]
Epoch [20/120    avg_loss:0.415, val_acc:0.844]
Epoch [21/120    avg_loss:0.307, val_acc:0.845]
Epoch [22/120    avg_loss:0.298, val_acc:0.874]
Epoch [23/120    avg_loss:0.241, val_acc:0.869]
Epoch [24/120    avg_loss:0.238, val_acc:0.881]
Epoch [25/120    avg_loss:0.267, val_acc:0.880]
Epoch [26/120    avg_loss:0.227, val_acc:0.899]
Epoch [27/120    avg_loss:0.224, val_acc:0.881]
Epoch [28/120    avg_loss:0.190, val_acc:0.915]
Epoch [29/120    avg_loss:0.204, val_acc:0.897]
Epoch [30/120    avg_loss:0.165, val_acc:0.871]
Epoch [31/120    avg_loss:0.219, val_acc:0.897]
Epoch [32/120    avg_loss:0.166, val_acc:0.917]
Epoch [33/120    avg_loss:0.139, val_acc:0.925]
Epoch [34/120    avg_loss:0.141, val_acc:0.915]
Epoch [35/120    avg_loss:0.153, val_acc:0.917]
Epoch [36/120    avg_loss:0.147, val_acc:0.926]
Epoch [37/120    avg_loss:0.147, val_acc:0.926]
Epoch [38/120    avg_loss:0.222, val_acc:0.881]
Epoch [39/120    avg_loss:0.144, val_acc:0.909]
Epoch [40/120    avg_loss:0.109, val_acc:0.942]
Epoch [41/120    avg_loss:0.118, val_acc:0.919]
Epoch [42/120    avg_loss:0.180, val_acc:0.920]
Epoch [43/120    avg_loss:0.105, val_acc:0.931]
Epoch [44/120    avg_loss:0.069, val_acc:0.914]
Epoch [45/120    avg_loss:0.094, val_acc:0.944]
Epoch [46/120    avg_loss:0.074, val_acc:0.941]
Epoch [47/120    avg_loss:0.056, val_acc:0.954]
Epoch [48/120    avg_loss:0.054, val_acc:0.953]
Epoch [49/120    avg_loss:0.083, val_acc:0.925]
Epoch [50/120    avg_loss:0.081, val_acc:0.927]
Epoch [51/120    avg_loss:0.167, val_acc:0.923]
Epoch [52/120    avg_loss:0.131, val_acc:0.933]
Epoch [53/120    avg_loss:0.140, val_acc:0.906]
Epoch [54/120    avg_loss:0.088, val_acc:0.946]
Epoch [55/120    avg_loss:0.088, val_acc:0.946]
Epoch [56/120    avg_loss:0.252, val_acc:0.904]
Epoch [57/120    avg_loss:0.107, val_acc:0.934]
Epoch [58/120    avg_loss:0.071, val_acc:0.950]
Epoch [59/120    avg_loss:0.051, val_acc:0.951]
Epoch [60/120    avg_loss:0.063, val_acc:0.946]
Epoch [61/120    avg_loss:0.073, val_acc:0.964]
Epoch [62/120    avg_loss:0.048, val_acc:0.959]
Epoch [63/120    avg_loss:0.042, val_acc:0.959]
Epoch [64/120    avg_loss:0.032, val_acc:0.955]
Epoch [65/120    avg_loss:0.047, val_acc:0.958]
Epoch [66/120    avg_loss:0.027, val_acc:0.957]
Epoch [67/120    avg_loss:0.033, val_acc:0.958]
Epoch [68/120    avg_loss:0.033, val_acc:0.960]
Epoch [69/120    avg_loss:0.026, val_acc:0.966]
Epoch [70/120    avg_loss:0.035, val_acc:0.965]
Epoch [71/120    avg_loss:0.034, val_acc:0.966]
Epoch [72/120    avg_loss:0.034, val_acc:0.964]
Epoch [73/120    avg_loss:0.028, val_acc:0.965]
Epoch [74/120    avg_loss:0.029, val_acc:0.964]
Epoch [75/120    avg_loss:0.029, val_acc:0.969]
Epoch [76/120    avg_loss:0.032, val_acc:0.966]
Epoch [77/120    avg_loss:0.029, val_acc:0.961]
Epoch [78/120    avg_loss:0.025, val_acc:0.965]
Epoch [79/120    avg_loss:0.033, val_acc:0.963]
Epoch [80/120    avg_loss:0.025, val_acc:0.968]
Epoch [81/120    avg_loss:0.028, val_acc:0.968]
Epoch [82/120    avg_loss:0.025, val_acc:0.969]
Epoch [83/120    avg_loss:0.023, val_acc:0.965]
Epoch [84/120    avg_loss:0.030, val_acc:0.965]
Epoch [85/120    avg_loss:0.022, val_acc:0.965]
Epoch [86/120    avg_loss:0.026, val_acc:0.969]
Epoch [87/120    avg_loss:0.022, val_acc:0.968]
Epoch [88/120    avg_loss:0.025, val_acc:0.967]
Epoch [89/120    avg_loss:0.023, val_acc:0.970]
Epoch [90/120    avg_loss:0.023, val_acc:0.972]
Epoch [91/120    avg_loss:0.025, val_acc:0.967]
Epoch [92/120    avg_loss:0.018, val_acc:0.967]
Epoch [93/120    avg_loss:0.049, val_acc:0.967]
Epoch [94/120    avg_loss:0.019, val_acc:0.968]
Epoch [95/120    avg_loss:0.026, val_acc:0.964]
Epoch [96/120    avg_loss:0.024, val_acc:0.963]
Epoch [97/120    avg_loss:0.026, val_acc:0.964]
Epoch [98/120    avg_loss:0.022, val_acc:0.963]
Epoch [99/120    avg_loss:0.022, val_acc:0.963]
Epoch [100/120    avg_loss:0.026, val_acc:0.966]
Epoch [101/120    avg_loss:0.021, val_acc:0.970]
Epoch [102/120    avg_loss:0.023, val_acc:0.968]
Epoch [103/120    avg_loss:0.019, val_acc:0.969]
Epoch [104/120    avg_loss:0.023, val_acc:0.969]
Epoch [105/120    avg_loss:0.022, val_acc:0.968]
Epoch [106/120    avg_loss:0.025, val_acc:0.968]
Epoch [107/120    avg_loss:0.021, val_acc:0.968]
Epoch [108/120    avg_loss:0.019, val_acc:0.969]
Epoch [109/120    avg_loss:0.022, val_acc:0.968]
Epoch [110/120    avg_loss:0.018, val_acc:0.968]
Epoch [111/120    avg_loss:0.021, val_acc:0.969]
Epoch [112/120    avg_loss:0.020, val_acc:0.967]
Epoch [113/120    avg_loss:0.022, val_acc:0.966]
Epoch [114/120    avg_loss:0.020, val_acc:0.967]
Epoch [115/120    avg_loss:0.019, val_acc:0.967]
Epoch [116/120    avg_loss:0.023, val_acc:0.967]
Epoch [117/120    avg_loss:0.021, val_acc:0.967]
Epoch [118/120    avg_loss:0.021, val_acc:0.967]
Epoch [119/120    avg_loss:0.022, val_acc:0.967]
Epoch [120/120    avg_loss:0.018, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1259    2    2    0    2    0    0    0   10    9    0    1
     0    0    0]
 [   0    0    1  720    4    6    1    0    0    5    4    4    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    3    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    3    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    3    4    0    0    0  841   17    0    0
     0    2    0]
 [   0    0   30    0    0    1    1    0    0    3    8 2158    1    8
     0    0    0]
 [   0    0    0    6    0    0    0    0    0    0    0    4  520    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0   10
  1110   19    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    11  334    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.7560975609756

F1 scores:
[       nan 0.975      0.97483546 0.97627119 0.98611111 0.98401826
 0.98786039 0.94339623 1.         0.81818182 0.96666667 0.97979569
 0.98391675 0.95115681 0.97969991 0.95156695 0.98823529]

Kappa:
0.9744331683160363
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9d2c7877f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.521, val_acc:0.506]
Epoch [2/120    avg_loss:2.056, val_acc:0.557]
Epoch [3/120    avg_loss:1.770, val_acc:0.562]
Epoch [4/120    avg_loss:1.583, val_acc:0.590]
Epoch [5/120    avg_loss:1.460, val_acc:0.561]
Epoch [6/120    avg_loss:1.266, val_acc:0.618]
Epoch [7/120    avg_loss:1.135, val_acc:0.605]
Epoch [8/120    avg_loss:1.089, val_acc:0.691]
Epoch [9/120    avg_loss:0.938, val_acc:0.727]
Epoch [10/120    avg_loss:0.847, val_acc:0.743]
Epoch [11/120    avg_loss:0.772, val_acc:0.788]
Epoch [12/120    avg_loss:0.583, val_acc:0.762]
Epoch [13/120    avg_loss:0.615, val_acc:0.836]
Epoch [14/120    avg_loss:0.501, val_acc:0.824]
Epoch [15/120    avg_loss:0.595, val_acc:0.806]
Epoch [16/120    avg_loss:0.489, val_acc:0.847]
Epoch [17/120    avg_loss:0.454, val_acc:0.851]
Epoch [18/120    avg_loss:0.350, val_acc:0.845]
Epoch [19/120    avg_loss:0.355, val_acc:0.849]
Epoch [20/120    avg_loss:0.357, val_acc:0.875]
Epoch [21/120    avg_loss:0.308, val_acc:0.903]
Epoch [22/120    avg_loss:0.250, val_acc:0.889]
Epoch [23/120    avg_loss:0.267, val_acc:0.879]
Epoch [24/120    avg_loss:0.299, val_acc:0.880]
Epoch [25/120    avg_loss:0.208, val_acc:0.899]
Epoch [26/120    avg_loss:0.173, val_acc:0.918]
Epoch [27/120    avg_loss:0.171, val_acc:0.919]
Epoch [28/120    avg_loss:0.149, val_acc:0.926]
Epoch [29/120    avg_loss:0.118, val_acc:0.936]
Epoch [30/120    avg_loss:0.127, val_acc:0.952]
Epoch [31/120    avg_loss:0.134, val_acc:0.935]
Epoch [32/120    avg_loss:0.118, val_acc:0.928]
Epoch [33/120    avg_loss:0.145, val_acc:0.942]
Epoch [34/120    avg_loss:0.105, val_acc:0.957]
Epoch [35/120    avg_loss:0.166, val_acc:0.935]
Epoch [36/120    avg_loss:0.118, val_acc:0.933]
Epoch [37/120    avg_loss:0.122, val_acc:0.940]
Epoch [38/120    avg_loss:0.075, val_acc:0.950]
Epoch [39/120    avg_loss:0.067, val_acc:0.936]
Epoch [40/120    avg_loss:0.094, val_acc:0.951]
Epoch [41/120    avg_loss:0.080, val_acc:0.932]
Epoch [42/120    avg_loss:0.102, val_acc:0.934]
Epoch [43/120    avg_loss:0.099, val_acc:0.926]
Epoch [44/120    avg_loss:0.081, val_acc:0.947]
Epoch [45/120    avg_loss:0.057, val_acc:0.965]
Epoch [46/120    avg_loss:0.068, val_acc:0.965]
Epoch [47/120    avg_loss:0.110, val_acc:0.887]
Epoch [48/120    avg_loss:0.120, val_acc:0.905]
Epoch [49/120    avg_loss:0.088, val_acc:0.945]
Epoch [50/120    avg_loss:0.068, val_acc:0.953]
Epoch [51/120    avg_loss:0.052, val_acc:0.965]
Epoch [52/120    avg_loss:0.047, val_acc:0.964]
Epoch [53/120    avg_loss:0.051, val_acc:0.970]
Epoch [54/120    avg_loss:0.035, val_acc:0.970]
Epoch [55/120    avg_loss:0.038, val_acc:0.964]
Epoch [56/120    avg_loss:0.065, val_acc:0.933]
Epoch [57/120    avg_loss:0.062, val_acc:0.957]
Epoch [58/120    avg_loss:0.057, val_acc:0.970]
Epoch [59/120    avg_loss:0.034, val_acc:0.970]
Epoch [60/120    avg_loss:0.041, val_acc:0.967]
Epoch [61/120    avg_loss:0.037, val_acc:0.959]
Epoch [62/120    avg_loss:0.031, val_acc:0.976]
Epoch [63/120    avg_loss:0.023, val_acc:0.979]
Epoch [64/120    avg_loss:0.030, val_acc:0.969]
Epoch [65/120    avg_loss:0.026, val_acc:0.975]
Epoch [66/120    avg_loss:0.026, val_acc:0.960]
Epoch [67/120    avg_loss:0.022, val_acc:0.975]
Epoch [68/120    avg_loss:0.028, val_acc:0.966]
Epoch [69/120    avg_loss:0.032, val_acc:0.952]
Epoch [70/120    avg_loss:0.025, val_acc:0.974]
Epoch [71/120    avg_loss:0.025, val_acc:0.976]
Epoch [72/120    avg_loss:0.023, val_acc:0.976]
Epoch [73/120    avg_loss:0.015, val_acc:0.980]
Epoch [74/120    avg_loss:0.014, val_acc:0.976]
Epoch [75/120    avg_loss:0.018, val_acc:0.974]
Epoch [76/120    avg_loss:0.017, val_acc:0.979]
Epoch [77/120    avg_loss:0.013, val_acc:0.976]
Epoch [78/120    avg_loss:0.015, val_acc:0.981]
Epoch [79/120    avg_loss:0.013, val_acc:0.976]
Epoch [80/120    avg_loss:0.039, val_acc:0.968]
Epoch [81/120    avg_loss:0.019, val_acc:0.971]
Epoch [82/120    avg_loss:0.016, val_acc:0.983]
Epoch [83/120    avg_loss:0.030, val_acc:0.959]
Epoch [84/120    avg_loss:0.060, val_acc:0.958]
Epoch [85/120    avg_loss:0.058, val_acc:0.974]
Epoch [86/120    avg_loss:0.031, val_acc:0.972]
Epoch [87/120    avg_loss:0.019, val_acc:0.978]
Epoch [88/120    avg_loss:0.015, val_acc:0.978]
Epoch [89/120    avg_loss:0.012, val_acc:0.980]
Epoch [90/120    avg_loss:0.011, val_acc:0.982]
Epoch [91/120    avg_loss:0.011, val_acc:0.982]
Epoch [92/120    avg_loss:0.018, val_acc:0.980]
Epoch [93/120    avg_loss:0.030, val_acc:0.969]
Epoch [94/120    avg_loss:0.014, val_acc:0.975]
Epoch [95/120    avg_loss:0.009, val_acc:0.983]
Epoch [96/120    avg_loss:0.009, val_acc:0.977]
Epoch [97/120    avg_loss:0.008, val_acc:0.980]
Epoch [98/120    avg_loss:0.010, val_acc:0.981]
Epoch [99/120    avg_loss:0.009, val_acc:0.986]
Epoch [100/120    avg_loss:0.011, val_acc:0.980]
Epoch [101/120    avg_loss:0.010, val_acc:0.983]
Epoch [102/120    avg_loss:0.010, val_acc:0.981]
Epoch [103/120    avg_loss:0.020, val_acc:0.971]
Epoch [104/120    avg_loss:0.084, val_acc:0.960]
Epoch [105/120    avg_loss:0.032, val_acc:0.960]
Epoch [106/120    avg_loss:0.032, val_acc:0.971]
Epoch [107/120    avg_loss:0.140, val_acc:0.916]
Epoch [108/120    avg_loss:0.145, val_acc:0.934]
Epoch [109/120    avg_loss:0.059, val_acc:0.965]
Epoch [110/120    avg_loss:0.065, val_acc:0.961]
Epoch [111/120    avg_loss:0.079, val_acc:0.976]
Epoch [112/120    avg_loss:0.047, val_acc:0.951]
Epoch [113/120    avg_loss:0.058, val_acc:0.976]
Epoch [114/120    avg_loss:0.020, val_acc:0.980]
Epoch [115/120    avg_loss:0.018, val_acc:0.983]
Epoch [116/120    avg_loss:0.020, val_acc:0.983]
Epoch [117/120    avg_loss:0.015, val_acc:0.984]
Epoch [118/120    avg_loss:0.014, val_acc:0.985]
Epoch [119/120    avg_loss:0.013, val_acc:0.985]
Epoch [120/120    avg_loss:0.014, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    4 1245    1    0    1    0    0    0    2    8   21    0    0
     0    3    0]
 [   0    0    0  722    7   16    0    0    0    0    0    0    1    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0  858   14    0    0
     0    2    0]
 [   0    0    2    0    0    1    2    0    0    0   21 2178    5    0
     1    0    0]
 [   0    0    0    7    0    1    0    0    0    0    2    0  521    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    0    0    0
  1125   12    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    89  257    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.4308943089431

F1 scores:
[       nan 0.95348837 0.98341232 0.97765741 0.98383372 0.97637795
 0.99467681 0.98039216 0.99883586 0.94736842 0.97223796 0.98462929
 0.98024459 1.         0.95500849 0.82504013 0.98224852]

Kappa:
0.9706971970245377
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe996015828>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.564, val_acc:0.479]
Epoch [2/120    avg_loss:2.082, val_acc:0.572]
Epoch [3/120    avg_loss:1.726, val_acc:0.614]
Epoch [4/120    avg_loss:1.518, val_acc:0.638]
Epoch [5/120    avg_loss:1.330, val_acc:0.665]
Epoch [6/120    avg_loss:1.088, val_acc:0.700]
Epoch [7/120    avg_loss:1.004, val_acc:0.671]
Epoch [8/120    avg_loss:0.925, val_acc:0.763]
Epoch [9/120    avg_loss:0.872, val_acc:0.721]
Epoch [10/120    avg_loss:0.739, val_acc:0.724]
Epoch [11/120    avg_loss:0.684, val_acc:0.794]
Epoch [12/120    avg_loss:0.635, val_acc:0.727]
Epoch [13/120    avg_loss:0.605, val_acc:0.800]
Epoch [14/120    avg_loss:0.511, val_acc:0.814]
Epoch [15/120    avg_loss:0.407, val_acc:0.841]
Epoch [16/120    avg_loss:0.377, val_acc:0.881]
Epoch [17/120    avg_loss:0.335, val_acc:0.863]
Epoch [18/120    avg_loss:0.341, val_acc:0.899]
Epoch [19/120    avg_loss:0.288, val_acc:0.897]
Epoch [20/120    avg_loss:0.306, val_acc:0.885]
Epoch [21/120    avg_loss:0.324, val_acc:0.889]
Epoch [22/120    avg_loss:0.286, val_acc:0.906]
Epoch [23/120    avg_loss:0.222, val_acc:0.910]
Epoch [24/120    avg_loss:0.168, val_acc:0.927]
Epoch [25/120    avg_loss:0.239, val_acc:0.898]
Epoch [26/120    avg_loss:0.196, val_acc:0.923]
Epoch [27/120    avg_loss:0.202, val_acc:0.909]
Epoch [28/120    avg_loss:0.150, val_acc:0.933]
Epoch [29/120    avg_loss:0.142, val_acc:0.930]
Epoch [30/120    avg_loss:0.202, val_acc:0.931]
Epoch [31/120    avg_loss:0.139, val_acc:0.931]
Epoch [32/120    avg_loss:0.184, val_acc:0.913]
Epoch [33/120    avg_loss:0.143, val_acc:0.949]
Epoch [34/120    avg_loss:0.090, val_acc:0.948]
Epoch [35/120    avg_loss:0.094, val_acc:0.953]
Epoch [36/120    avg_loss:0.096, val_acc:0.944]
Epoch [37/120    avg_loss:0.067, val_acc:0.963]
Epoch [38/120    avg_loss:0.098, val_acc:0.945]
Epoch [39/120    avg_loss:0.075, val_acc:0.948]
Epoch [40/120    avg_loss:0.065, val_acc:0.966]
Epoch [41/120    avg_loss:0.052, val_acc:0.968]
Epoch [42/120    avg_loss:0.057, val_acc:0.974]
Epoch [43/120    avg_loss:0.066, val_acc:0.952]
Epoch [44/120    avg_loss:0.084, val_acc:0.959]
Epoch [45/120    avg_loss:0.057, val_acc:0.961]
Epoch [46/120    avg_loss:0.064, val_acc:0.965]
Epoch [47/120    avg_loss:0.070, val_acc:0.971]
Epoch [48/120    avg_loss:0.052, val_acc:0.980]
Epoch [49/120    avg_loss:0.063, val_acc:0.978]
Epoch [50/120    avg_loss:0.046, val_acc:0.972]
Epoch [51/120    avg_loss:0.061, val_acc:0.971]
Epoch [52/120    avg_loss:0.041, val_acc:0.969]
Epoch [53/120    avg_loss:0.032, val_acc:0.972]
Epoch [54/120    avg_loss:0.039, val_acc:0.974]
Epoch [55/120    avg_loss:0.041, val_acc:0.977]
Epoch [56/120    avg_loss:0.061, val_acc:0.971]
Epoch [57/120    avg_loss:0.032, val_acc:0.977]
Epoch [58/120    avg_loss:0.033, val_acc:0.977]
Epoch [59/120    avg_loss:0.050, val_acc:0.978]
Epoch [60/120    avg_loss:0.036, val_acc:0.979]
Epoch [61/120    avg_loss:0.022, val_acc:0.979]
Epoch [62/120    avg_loss:0.019, val_acc:0.987]
Epoch [63/120    avg_loss:0.016, val_acc:0.987]
Epoch [64/120    avg_loss:0.016, val_acc:0.986]
Epoch [65/120    avg_loss:0.017, val_acc:0.988]
Epoch [66/120    avg_loss:0.018, val_acc:0.986]
Epoch [67/120    avg_loss:0.016, val_acc:0.985]
Epoch [68/120    avg_loss:0.015, val_acc:0.987]
Epoch [69/120    avg_loss:0.016, val_acc:0.988]
Epoch [70/120    avg_loss:0.018, val_acc:0.985]
Epoch [71/120    avg_loss:0.014, val_acc:0.988]
Epoch [72/120    avg_loss:0.016, val_acc:0.988]
Epoch [73/120    avg_loss:0.015, val_acc:0.985]
Epoch [74/120    avg_loss:0.013, val_acc:0.985]
Epoch [75/120    avg_loss:0.016, val_acc:0.987]
Epoch [76/120    avg_loss:0.014, val_acc:0.987]
Epoch [77/120    avg_loss:0.013, val_acc:0.987]
Epoch [78/120    avg_loss:0.016, val_acc:0.985]
Epoch [79/120    avg_loss:0.015, val_acc:0.986]
Epoch [80/120    avg_loss:0.015, val_acc:0.987]
Epoch [81/120    avg_loss:0.019, val_acc:0.988]
Epoch [82/120    avg_loss:0.014, val_acc:0.987]
Epoch [83/120    avg_loss:0.012, val_acc:0.988]
Epoch [84/120    avg_loss:0.013, val_acc:0.988]
Epoch [85/120    avg_loss:0.013, val_acc:0.987]
Epoch [86/120    avg_loss:0.015, val_acc:0.988]
Epoch [87/120    avg_loss:0.012, val_acc:0.987]
Epoch [88/120    avg_loss:0.014, val_acc:0.988]
Epoch [89/120    avg_loss:0.017, val_acc:0.989]
Epoch [90/120    avg_loss:0.014, val_acc:0.990]
Epoch [91/120    avg_loss:0.016, val_acc:0.989]
Epoch [92/120    avg_loss:0.011, val_acc:0.989]
Epoch [93/120    avg_loss:0.012, val_acc:0.989]
Epoch [94/120    avg_loss:0.012, val_acc:0.990]
Epoch [95/120    avg_loss:0.012, val_acc:0.992]
Epoch [96/120    avg_loss:0.011, val_acc:0.990]
Epoch [97/120    avg_loss:0.011, val_acc:0.990]
Epoch [98/120    avg_loss:0.014, val_acc:0.992]
Epoch [99/120    avg_loss:0.013, val_acc:0.992]
Epoch [100/120    avg_loss:0.015, val_acc:0.992]
Epoch [101/120    avg_loss:0.029, val_acc:0.989]
Epoch [102/120    avg_loss:0.014, val_acc:0.990]
Epoch [103/120    avg_loss:0.013, val_acc:0.992]
Epoch [104/120    avg_loss:0.013, val_acc:0.992]
Epoch [105/120    avg_loss:0.011, val_acc:0.988]
Epoch [106/120    avg_loss:0.011, val_acc:0.989]
Epoch [107/120    avg_loss:0.010, val_acc:0.990]
Epoch [108/120    avg_loss:0.011, val_acc:0.992]
Epoch [109/120    avg_loss:0.010, val_acc:0.992]
Epoch [110/120    avg_loss:0.011, val_acc:0.993]
Epoch [111/120    avg_loss:0.010, val_acc:0.992]
Epoch [112/120    avg_loss:0.012, val_acc:0.992]
Epoch [113/120    avg_loss:0.012, val_acc:0.992]
Epoch [114/120    avg_loss:0.012, val_acc:0.990]
Epoch [115/120    avg_loss:0.012, val_acc:0.989]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.010, val_acc:0.989]
Epoch [118/120    avg_loss:0.012, val_acc:0.992]
Epoch [119/120    avg_loss:0.012, val_acc:0.990]
Epoch [120/120    avg_loss:0.009, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   34    0    0    0    0    0    0    0    0    7    0    0    0
     0    0    0]
 [   0    0 1267    1    0    0    0    0    0    5    2   10    0    0
     0    0    0]
 [   0    0    0  721    4    1    0    0    0    8    3    3    5    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    2    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    1    0    0    0    1  857   12    0    0
     0    1    0]
 [   0    0    4    0    0    1    0    0    0    2   15 2178   10    0
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    4    1  525    0
     0    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1131    7    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    71  264    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.70189701897019

F1 scores:
[       nan 0.90666667 0.99023056 0.98162015 0.99069767 0.99084668
 0.98789713 0.96153846 0.997669   0.67924528 0.97220647 0.98663647
 0.97402597 0.99191375 0.96501706 0.85298869 0.97619048]

Kappa:
0.9737892698575397
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcf118f0828>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.586, val_acc:0.535]
Epoch [2/120    avg_loss:2.071, val_acc:0.536]
Epoch [3/120    avg_loss:1.806, val_acc:0.581]
Epoch [4/120    avg_loss:1.565, val_acc:0.633]
Epoch [5/120    avg_loss:1.303, val_acc:0.635]
Epoch [6/120    avg_loss:1.103, val_acc:0.702]
Epoch [7/120    avg_loss:0.969, val_acc:0.739]
Epoch [8/120    avg_loss:0.822, val_acc:0.781]
Epoch [9/120    avg_loss:0.685, val_acc:0.799]
Epoch [10/120    avg_loss:0.676, val_acc:0.758]
Epoch [11/120    avg_loss:0.566, val_acc:0.705]
Epoch [12/120    avg_loss:0.594, val_acc:0.846]
Epoch [13/120    avg_loss:0.428, val_acc:0.831]
Epoch [14/120    avg_loss:0.393, val_acc:0.856]
Epoch [15/120    avg_loss:0.392, val_acc:0.821]
Epoch [16/120    avg_loss:0.383, val_acc:0.855]
Epoch [17/120    avg_loss:0.369, val_acc:0.863]
Epoch [18/120    avg_loss:0.343, val_acc:0.848]
Epoch [19/120    avg_loss:0.340, val_acc:0.861]
Epoch [20/120    avg_loss:0.327, val_acc:0.918]
Epoch [21/120    avg_loss:0.219, val_acc:0.912]
Epoch [22/120    avg_loss:0.212, val_acc:0.902]
Epoch [23/120    avg_loss:0.283, val_acc:0.880]
Epoch [24/120    avg_loss:0.238, val_acc:0.889]
Epoch [25/120    avg_loss:0.155, val_acc:0.903]
Epoch [26/120    avg_loss:0.151, val_acc:0.908]
Epoch [27/120    avg_loss:0.135, val_acc:0.921]
Epoch [28/120    avg_loss:0.134, val_acc:0.932]
Epoch [29/120    avg_loss:0.135, val_acc:0.931]
Epoch [30/120    avg_loss:0.113, val_acc:0.927]
Epoch [31/120    avg_loss:0.107, val_acc:0.942]
Epoch [32/120    avg_loss:0.124, val_acc:0.904]
Epoch [33/120    avg_loss:0.124, val_acc:0.929]
Epoch [34/120    avg_loss:0.093, val_acc:0.928]
Epoch [35/120    avg_loss:0.078, val_acc:0.918]
Epoch [36/120    avg_loss:0.094, val_acc:0.944]
Epoch [37/120    avg_loss:0.136, val_acc:0.896]
Epoch [38/120    avg_loss:0.121, val_acc:0.922]
Epoch [39/120    avg_loss:0.123, val_acc:0.933]
Epoch [40/120    avg_loss:0.092, val_acc:0.943]
Epoch [41/120    avg_loss:0.088, val_acc:0.945]
Epoch [42/120    avg_loss:0.099, val_acc:0.929]
Epoch [43/120    avg_loss:0.081, val_acc:0.950]
Epoch [44/120    avg_loss:0.071, val_acc:0.964]
Epoch [45/120    avg_loss:0.056, val_acc:0.964]
Epoch [46/120    avg_loss:0.046, val_acc:0.969]
Epoch [47/120    avg_loss:0.043, val_acc:0.961]
Epoch [48/120    avg_loss:0.065, val_acc:0.966]
Epoch [49/120    avg_loss:0.040, val_acc:0.970]
Epoch [50/120    avg_loss:0.053, val_acc:0.966]
Epoch [51/120    avg_loss:0.036, val_acc:0.972]
Epoch [52/120    avg_loss:0.046, val_acc:0.965]
Epoch [53/120    avg_loss:0.035, val_acc:0.971]
Epoch [54/120    avg_loss:0.033, val_acc:0.971]
Epoch [55/120    avg_loss:0.032, val_acc:0.973]
Epoch [56/120    avg_loss:0.028, val_acc:0.976]
Epoch [57/120    avg_loss:0.032, val_acc:0.971]
Epoch [58/120    avg_loss:0.021, val_acc:0.972]
Epoch [59/120    avg_loss:0.021, val_acc:0.975]
Epoch [60/120    avg_loss:0.023, val_acc:0.975]
Epoch [61/120    avg_loss:0.037, val_acc:0.963]
Epoch [62/120    avg_loss:0.026, val_acc:0.971]
Epoch [63/120    avg_loss:0.027, val_acc:0.976]
Epoch [64/120    avg_loss:0.019, val_acc:0.975]
Epoch [65/120    avg_loss:0.021, val_acc:0.974]
Epoch [66/120    avg_loss:0.022, val_acc:0.978]
Epoch [67/120    avg_loss:0.013, val_acc:0.976]
Epoch [68/120    avg_loss:0.019, val_acc:0.972]
Epoch [69/120    avg_loss:0.015, val_acc:0.974]
Epoch [70/120    avg_loss:0.044, val_acc:0.979]
Epoch [71/120    avg_loss:0.021, val_acc:0.980]
Epoch [72/120    avg_loss:0.018, val_acc:0.968]
Epoch [73/120    avg_loss:0.022, val_acc:0.976]
Epoch [74/120    avg_loss:0.062, val_acc:0.951]
Epoch [75/120    avg_loss:0.032, val_acc:0.968]
Epoch [76/120    avg_loss:0.034, val_acc:0.973]
Epoch [77/120    avg_loss:0.030, val_acc:0.967]
Epoch [78/120    avg_loss:0.057, val_acc:0.968]
Epoch [79/120    avg_loss:0.031, val_acc:0.973]
Epoch [80/120    avg_loss:0.014, val_acc:0.981]
Epoch [81/120    avg_loss:0.015, val_acc:0.980]
Epoch [82/120    avg_loss:0.015, val_acc:0.983]
Epoch [83/120    avg_loss:0.041, val_acc:0.968]
Epoch [84/120    avg_loss:0.027, val_acc:0.952]
Epoch [85/120    avg_loss:0.185, val_acc:0.903]
Epoch [86/120    avg_loss:0.367, val_acc:0.887]
Epoch [87/120    avg_loss:0.098, val_acc:0.947]
Epoch [88/120    avg_loss:0.077, val_acc:0.936]
Epoch [89/120    avg_loss:0.050, val_acc:0.959]
Epoch [90/120    avg_loss:0.030, val_acc:0.970]
Epoch [91/120    avg_loss:0.031, val_acc:0.963]
Epoch [92/120    avg_loss:0.072, val_acc:0.963]
Epoch [93/120    avg_loss:0.032, val_acc:0.957]
Epoch [94/120    avg_loss:0.038, val_acc:0.960]
Epoch [95/120    avg_loss:0.047, val_acc:0.959]
Epoch [96/120    avg_loss:0.037, val_acc:0.971]
Epoch [97/120    avg_loss:0.032, val_acc:0.975]
Epoch [98/120    avg_loss:0.021, val_acc:0.978]
Epoch [99/120    avg_loss:0.018, val_acc:0.977]
Epoch [100/120    avg_loss:0.015, val_acc:0.978]
Epoch [101/120    avg_loss:0.018, val_acc:0.978]
Epoch [102/120    avg_loss:0.016, val_acc:0.977]
Epoch [103/120    avg_loss:0.015, val_acc:0.976]
Epoch [104/120    avg_loss:0.018, val_acc:0.975]
Epoch [105/120    avg_loss:0.020, val_acc:0.974]
Epoch [106/120    avg_loss:0.013, val_acc:0.974]
Epoch [107/120    avg_loss:0.011, val_acc:0.975]
Epoch [108/120    avg_loss:0.016, val_acc:0.977]
Epoch [109/120    avg_loss:0.017, val_acc:0.977]
Epoch [110/120    avg_loss:0.013, val_acc:0.977]
Epoch [111/120    avg_loss:0.011, val_acc:0.977]
Epoch [112/120    avg_loss:0.012, val_acc:0.977]
Epoch [113/120    avg_loss:0.013, val_acc:0.977]
Epoch [114/120    avg_loss:0.011, val_acc:0.978]
Epoch [115/120    avg_loss:0.015, val_acc:0.978]
Epoch [116/120    avg_loss:0.013, val_acc:0.978]
Epoch [117/120    avg_loss:0.013, val_acc:0.978]
Epoch [118/120    avg_loss:0.014, val_acc:0.977]
Epoch [119/120    avg_loss:0.017, val_acc:0.977]
Epoch [120/120    avg_loss:0.010, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1258    2    0    0    0    0    0    2    5   18    0    0
     0    0    0]
 [   0    0    2  714    0    4    0    0    0    4    0    7   16    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    3    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    2  849   20    0    0
     0    4    0]
 [   0    0    9    0    0    0    0    0    0    8   13 2170    1    0
     0    0    9]
 [   0    0    3    1    0    0    0    0    0    0    1    9  516    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
    58  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
97.52845528455285

F1 scores:
[       nan 0.98765432 0.98396558 0.97474403 0.99764706 0.98500577
 0.99923839 0.94339623 1.         0.67924528 0.97362385 0.97880018
 0.96178938 1.         0.96873662 0.89026275 0.9017341 ]

Kappa:
0.9718080996523965
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fefeaba7828>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.536, val_acc:0.344]
Epoch [2/120    avg_loss:2.068, val_acc:0.541]
Epoch [3/120    avg_loss:1.827, val_acc:0.579]
Epoch [4/120    avg_loss:1.571, val_acc:0.646]
Epoch [5/120    avg_loss:1.407, val_acc:0.668]
Epoch [6/120    avg_loss:1.199, val_acc:0.691]
Epoch [7/120    avg_loss:1.008, val_acc:0.722]
Epoch [8/120    avg_loss:0.922, val_acc:0.783]
Epoch [9/120    avg_loss:0.827, val_acc:0.731]
Epoch [10/120    avg_loss:0.738, val_acc:0.782]
Epoch [11/120    avg_loss:0.610, val_acc:0.797]
Epoch [12/120    avg_loss:0.486, val_acc:0.861]
Epoch [13/120    avg_loss:0.462, val_acc:0.855]
Epoch [14/120    avg_loss:0.389, val_acc:0.870]
Epoch [15/120    avg_loss:0.353, val_acc:0.876]
Epoch [16/120    avg_loss:0.318, val_acc:0.864]
Epoch [17/120    avg_loss:0.289, val_acc:0.889]
Epoch [18/120    avg_loss:0.263, val_acc:0.895]
Epoch [19/120    avg_loss:0.188, val_acc:0.914]
Epoch [20/120    avg_loss:0.244, val_acc:0.890]
Epoch [21/120    avg_loss:0.174, val_acc:0.917]
Epoch [22/120    avg_loss:0.175, val_acc:0.895]
Epoch [23/120    avg_loss:0.196, val_acc:0.893]
Epoch [24/120    avg_loss:0.185, val_acc:0.900]
Epoch [25/120    avg_loss:0.155, val_acc:0.935]
Epoch [26/120    avg_loss:0.114, val_acc:0.946]
Epoch [27/120    avg_loss:0.126, val_acc:0.940]
Epoch [28/120    avg_loss:0.180, val_acc:0.917]
Epoch [29/120    avg_loss:0.145, val_acc:0.919]
Epoch [30/120    avg_loss:0.126, val_acc:0.933]
Epoch [31/120    avg_loss:0.082, val_acc:0.953]
Epoch [32/120    avg_loss:0.079, val_acc:0.948]
Epoch [33/120    avg_loss:0.090, val_acc:0.948]
Epoch [34/120    avg_loss:0.167, val_acc:0.908]
Epoch [35/120    avg_loss:0.172, val_acc:0.923]
Epoch [36/120    avg_loss:0.098, val_acc:0.938]
Epoch [37/120    avg_loss:0.116, val_acc:0.934]
Epoch [38/120    avg_loss:0.072, val_acc:0.954]
Epoch [39/120    avg_loss:0.067, val_acc:0.945]
Epoch [40/120    avg_loss:0.058, val_acc:0.954]
Epoch [41/120    avg_loss:0.069, val_acc:0.957]
Epoch [42/120    avg_loss:0.055, val_acc:0.939]
Epoch [43/120    avg_loss:0.047, val_acc:0.966]
Epoch [44/120    avg_loss:0.055, val_acc:0.960]
Epoch [45/120    avg_loss:0.104, val_acc:0.946]
Epoch [46/120    avg_loss:0.088, val_acc:0.948]
Epoch [47/120    avg_loss:0.064, val_acc:0.963]
Epoch [48/120    avg_loss:0.051, val_acc:0.957]
Epoch [49/120    avg_loss:0.040, val_acc:0.970]
Epoch [50/120    avg_loss:0.121, val_acc:0.954]
Epoch [51/120    avg_loss:0.051, val_acc:0.966]
Epoch [52/120    avg_loss:0.058, val_acc:0.930]
Epoch [53/120    avg_loss:0.080, val_acc:0.943]
Epoch [54/120    avg_loss:0.058, val_acc:0.969]
Epoch [55/120    avg_loss:0.030, val_acc:0.963]
Epoch [56/120    avg_loss:0.046, val_acc:0.964]
Epoch [57/120    avg_loss:0.035, val_acc:0.977]
Epoch [58/120    avg_loss:0.026, val_acc:0.977]
Epoch [59/120    avg_loss:0.053, val_acc:0.961]
Epoch [60/120    avg_loss:0.031, val_acc:0.972]
Epoch [61/120    avg_loss:0.027, val_acc:0.960]
Epoch [62/120    avg_loss:0.033, val_acc:0.964]
Epoch [63/120    avg_loss:0.020, val_acc:0.977]
Epoch [64/120    avg_loss:0.032, val_acc:0.969]
Epoch [65/120    avg_loss:0.026, val_acc:0.974]
Epoch [66/120    avg_loss:0.030, val_acc:0.969]
Epoch [67/120    avg_loss:0.019, val_acc:0.976]
Epoch [68/120    avg_loss:0.014, val_acc:0.979]
Epoch [69/120    avg_loss:0.015, val_acc:0.976]
Epoch [70/120    avg_loss:0.015, val_acc:0.974]
Epoch [71/120    avg_loss:0.017, val_acc:0.976]
Epoch [72/120    avg_loss:0.013, val_acc:0.977]
Epoch [73/120    avg_loss:0.018, val_acc:0.974]
Epoch [74/120    avg_loss:0.014, val_acc:0.976]
Epoch [75/120    avg_loss:0.018, val_acc:0.965]
Epoch [76/120    avg_loss:0.012, val_acc:0.968]
Epoch [77/120    avg_loss:0.018, val_acc:0.981]
Epoch [78/120    avg_loss:0.035, val_acc:0.964]
Epoch [79/120    avg_loss:0.022, val_acc:0.972]
Epoch [80/120    avg_loss:0.024, val_acc:0.968]
Epoch [81/120    avg_loss:0.012, val_acc:0.983]
Epoch [82/120    avg_loss:0.020, val_acc:0.978]
Epoch [83/120    avg_loss:0.011, val_acc:0.980]
Epoch [84/120    avg_loss:0.009, val_acc:0.974]
Epoch [85/120    avg_loss:0.012, val_acc:0.972]
Epoch [86/120    avg_loss:0.013, val_acc:0.976]
Epoch [87/120    avg_loss:0.019, val_acc:0.976]
Epoch [88/120    avg_loss:0.017, val_acc:0.956]
Epoch [89/120    avg_loss:0.020, val_acc:0.956]
Epoch [90/120    avg_loss:0.023, val_acc:0.977]
Epoch [91/120    avg_loss:0.014, val_acc:0.981]
Epoch [92/120    avg_loss:0.008, val_acc:0.983]
Epoch [93/120    avg_loss:0.011, val_acc:0.979]
Epoch [94/120    avg_loss:0.011, val_acc:0.985]
Epoch [95/120    avg_loss:0.013, val_acc:0.980]
Epoch [96/120    avg_loss:0.009, val_acc:0.982]
Epoch [97/120    avg_loss:0.019, val_acc:0.974]
Epoch [98/120    avg_loss:0.014, val_acc:0.977]
Epoch [99/120    avg_loss:0.023, val_acc:0.970]
Epoch [100/120    avg_loss:0.014, val_acc:0.980]
Epoch [101/120    avg_loss:0.013, val_acc:0.974]
Epoch [102/120    avg_loss:0.009, val_acc:0.981]
Epoch [103/120    avg_loss:0.014, val_acc:0.979]
Epoch [104/120    avg_loss:0.011, val_acc:0.974]
Epoch [105/120    avg_loss:0.008, val_acc:0.982]
Epoch [106/120    avg_loss:0.011, val_acc:0.982]
Epoch [107/120    avg_loss:0.010, val_acc:0.982]
Epoch [108/120    avg_loss:0.011, val_acc:0.982]
Epoch [109/120    avg_loss:0.006, val_acc:0.982]
Epoch [110/120    avg_loss:0.006, val_acc:0.983]
Epoch [111/120    avg_loss:0.006, val_acc:0.983]
Epoch [112/120    avg_loss:0.013, val_acc:0.984]
Epoch [113/120    avg_loss:0.005, val_acc:0.983]
Epoch [114/120    avg_loss:0.010, val_acc:0.982]
Epoch [115/120    avg_loss:0.006, val_acc:0.982]
Epoch [116/120    avg_loss:0.004, val_acc:0.983]
Epoch [117/120    avg_loss:0.005, val_acc:0.983]
Epoch [118/120    avg_loss:0.005, val_acc:0.983]
Epoch [119/120    avg_loss:0.003, val_acc:0.984]
Epoch [120/120    avg_loss:0.005, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1260    3    0    0    1    0    0    0    6    9    5    0
     0    1    0]
 [   0    0    0  723    0    2    0    0    0    5    0    1   14    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    1    0    0    0    0    0    3  847   21    0    0
     0    2    0]
 [   0    0    9    0    0    2    0    1    0    3    2 2183    8    2
     0    0    0]
 [   0    0    0    6    2    0    0    0    0    0    0    7  517    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1123   15    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    63  282    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.68021680216802

F1 scores:
[       nan 0.975      0.98630137 0.97702703 0.9953271  0.99196326
 0.9946687  0.98039216 1.         0.75       0.97749567 0.98488608
 0.95652174 0.98930481 0.9639485  0.87171561 0.97590361]

Kappa:
0.9735399873966623
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5093b637b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.536, val_acc:0.371]
Epoch [2/120    avg_loss:2.043, val_acc:0.567]
Epoch [3/120    avg_loss:1.846, val_acc:0.443]
Epoch [4/120    avg_loss:1.615, val_acc:0.616]
Epoch [5/120    avg_loss:1.484, val_acc:0.629]
Epoch [6/120    avg_loss:1.276, val_acc:0.659]
Epoch [7/120    avg_loss:1.158, val_acc:0.714]
Epoch [8/120    avg_loss:0.954, val_acc:0.728]
Epoch [9/120    avg_loss:0.857, val_acc:0.804]
Epoch [10/120    avg_loss:0.698, val_acc:0.767]
Epoch [11/120    avg_loss:0.763, val_acc:0.702]
Epoch [12/120    avg_loss:0.761, val_acc:0.766]
Epoch [13/120    avg_loss:0.608, val_acc:0.828]
Epoch [14/120    avg_loss:0.541, val_acc:0.790]
Epoch [15/120    avg_loss:0.475, val_acc:0.834]
Epoch [16/120    avg_loss:0.433, val_acc:0.831]
Epoch [17/120    avg_loss:0.360, val_acc:0.832]
Epoch [18/120    avg_loss:0.300, val_acc:0.866]
Epoch [19/120    avg_loss:0.299, val_acc:0.898]
Epoch [20/120    avg_loss:0.424, val_acc:0.842]
Epoch [21/120    avg_loss:0.350, val_acc:0.878]
Epoch [22/120    avg_loss:0.285, val_acc:0.896]
Epoch [23/120    avg_loss:0.287, val_acc:0.910]
Epoch [24/120    avg_loss:0.234, val_acc:0.901]
Epoch [25/120    avg_loss:0.210, val_acc:0.908]
Epoch [26/120    avg_loss:0.178, val_acc:0.926]
Epoch [27/120    avg_loss:0.158, val_acc:0.903]
Epoch [28/120    avg_loss:0.143, val_acc:0.919]
Epoch [29/120    avg_loss:0.136, val_acc:0.926]
Epoch [30/120    avg_loss:0.103, val_acc:0.932]
Epoch [31/120    avg_loss:0.162, val_acc:0.949]
Epoch [32/120    avg_loss:0.104, val_acc:0.943]
Epoch [33/120    avg_loss:0.176, val_acc:0.938]
Epoch [34/120    avg_loss:0.128, val_acc:0.938]
Epoch [35/120    avg_loss:0.094, val_acc:0.938]
Epoch [36/120    avg_loss:0.082, val_acc:0.925]
Epoch [37/120    avg_loss:0.098, val_acc:0.943]
Epoch [38/120    avg_loss:0.088, val_acc:0.934]
Epoch [39/120    avg_loss:0.101, val_acc:0.943]
Epoch [40/120    avg_loss:0.091, val_acc:0.944]
Epoch [41/120    avg_loss:0.068, val_acc:0.955]
Epoch [42/120    avg_loss:0.071, val_acc:0.957]
Epoch [43/120    avg_loss:0.080, val_acc:0.950]
Epoch [44/120    avg_loss:0.062, val_acc:0.956]
Epoch [45/120    avg_loss:0.059, val_acc:0.958]
Epoch [46/120    avg_loss:0.085, val_acc:0.959]
Epoch [47/120    avg_loss:0.087, val_acc:0.957]
Epoch [48/120    avg_loss:0.103, val_acc:0.950]
Epoch [49/120    avg_loss:0.108, val_acc:0.926]
Epoch [50/120    avg_loss:0.072, val_acc:0.949]
Epoch [51/120    avg_loss:0.062, val_acc:0.953]
Epoch [52/120    avg_loss:0.059, val_acc:0.950]
Epoch [53/120    avg_loss:0.056, val_acc:0.950]
Epoch [54/120    avg_loss:0.047, val_acc:0.965]
Epoch [55/120    avg_loss:0.043, val_acc:0.965]
Epoch [56/120    avg_loss:0.041, val_acc:0.961]
Epoch [57/120    avg_loss:0.039, val_acc:0.965]
Epoch [58/120    avg_loss:0.075, val_acc:0.942]
Epoch [59/120    avg_loss:0.047, val_acc:0.969]
Epoch [60/120    avg_loss:0.053, val_acc:0.965]
Epoch [61/120    avg_loss:0.041, val_acc:0.968]
Epoch [62/120    avg_loss:0.053, val_acc:0.975]
Epoch [63/120    avg_loss:0.030, val_acc:0.970]
Epoch [64/120    avg_loss:0.026, val_acc:0.968]
Epoch [65/120    avg_loss:0.046, val_acc:0.965]
Epoch [66/120    avg_loss:0.025, val_acc:0.969]
Epoch [67/120    avg_loss:0.020, val_acc:0.976]
Epoch [68/120    avg_loss:0.023, val_acc:0.972]
Epoch [69/120    avg_loss:0.019, val_acc:0.974]
Epoch [70/120    avg_loss:0.031, val_acc:0.973]
Epoch [71/120    avg_loss:0.023, val_acc:0.974]
Epoch [72/120    avg_loss:0.036, val_acc:0.963]
Epoch [73/120    avg_loss:0.025, val_acc:0.973]
Epoch [74/120    avg_loss:0.028, val_acc:0.972]
Epoch [75/120    avg_loss:0.027, val_acc:0.961]
Epoch [76/120    avg_loss:0.029, val_acc:0.966]
Epoch [77/120    avg_loss:0.019, val_acc:0.979]
Epoch [78/120    avg_loss:0.017, val_acc:0.971]
Epoch [79/120    avg_loss:0.037, val_acc:0.961]
Epoch [80/120    avg_loss:0.026, val_acc:0.970]
Epoch [81/120    avg_loss:0.016, val_acc:0.973]
Epoch [82/120    avg_loss:0.036, val_acc:0.978]
Epoch [83/120    avg_loss:0.032, val_acc:0.966]
Epoch [84/120    avg_loss:0.027, val_acc:0.976]
Epoch [85/120    avg_loss:0.028, val_acc:0.975]
Epoch [86/120    avg_loss:0.015, val_acc:0.971]
Epoch [87/120    avg_loss:0.016, val_acc:0.974]
Epoch [88/120    avg_loss:0.044, val_acc:0.963]
Epoch [89/120    avg_loss:0.024, val_acc:0.977]
Epoch [90/120    avg_loss:0.016, val_acc:0.977]
Epoch [91/120    avg_loss:0.011, val_acc:0.977]
Epoch [92/120    avg_loss:0.011, val_acc:0.979]
Epoch [93/120    avg_loss:0.010, val_acc:0.978]
Epoch [94/120    avg_loss:0.010, val_acc:0.980]
Epoch [95/120    avg_loss:0.012, val_acc:0.981]
Epoch [96/120    avg_loss:0.012, val_acc:0.978]
Epoch [97/120    avg_loss:0.009, val_acc:0.978]
Epoch [98/120    avg_loss:0.006, val_acc:0.977]
Epoch [99/120    avg_loss:0.010, val_acc:0.978]
Epoch [100/120    avg_loss:0.012, val_acc:0.981]
Epoch [101/120    avg_loss:0.010, val_acc:0.982]
Epoch [102/120    avg_loss:0.008, val_acc:0.981]
Epoch [103/120    avg_loss:0.010, val_acc:0.982]
Epoch [104/120    avg_loss:0.010, val_acc:0.984]
Epoch [105/120    avg_loss:0.009, val_acc:0.984]
Epoch [106/120    avg_loss:0.008, val_acc:0.984]
Epoch [107/120    avg_loss:0.008, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.983]
Epoch [109/120    avg_loss:0.010, val_acc:0.982]
Epoch [110/120    avg_loss:0.007, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.984]
Epoch [112/120    avg_loss:0.009, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.983]
Epoch [114/120    avg_loss:0.009, val_acc:0.983]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.007, val_acc:0.984]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.010, val_acc:0.983]
Epoch [119/120    avg_loss:0.008, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1262    2    0    0    0    0    0    1    5   15    0    0
     0    0    0]
 [   0    0    0  734    0    1    0    0    0    1    2    1    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    1    0    0    0    0  860   11    0    0
     0    2    0]
 [   0    0    5    0    0    0    0    0    0    2    1 2192   10    0
     0    0    0]
 [   0    0    0    1    3    0    0    0    0    0    0    0  528    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
  1133    4    0]
 [   0    0    0    0    0    1    3    0    0    0    0    0    0    0
    77  266    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.17886178861788

F1 scores:
[       nan 0.98765432 0.98864081 0.98921833 0.99300699 0.99313501
 0.99543379 1.         1.         0.9        0.98623853 0.98916968
 0.97687327 1.         0.96384517 0.85945073 0.98809524]

Kappa:
0.9792211057361614
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f598dcc1860>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.505, val_acc:0.440]
Epoch [2/120    avg_loss:2.000, val_acc:0.501]
Epoch [3/120    avg_loss:1.812, val_acc:0.572]
Epoch [4/120    avg_loss:1.611, val_acc:0.643]
Epoch [5/120    avg_loss:1.441, val_acc:0.630]
Epoch [6/120    avg_loss:1.345, val_acc:0.677]
Epoch [7/120    avg_loss:1.088, val_acc:0.700]
Epoch [8/120    avg_loss:1.074, val_acc:0.694]
Epoch [9/120    avg_loss:0.917, val_acc:0.740]
Epoch [10/120    avg_loss:0.803, val_acc:0.766]
Epoch [11/120    avg_loss:0.684, val_acc:0.735]
Epoch [12/120    avg_loss:0.588, val_acc:0.784]
Epoch [13/120    avg_loss:0.639, val_acc:0.779]
Epoch [14/120    avg_loss:0.563, val_acc:0.791]
Epoch [15/120    avg_loss:0.482, val_acc:0.826]
Epoch [16/120    avg_loss:0.511, val_acc:0.819]
Epoch [17/120    avg_loss:0.412, val_acc:0.822]
Epoch [18/120    avg_loss:0.360, val_acc:0.878]
Epoch [19/120    avg_loss:0.282, val_acc:0.885]
Epoch [20/120    avg_loss:0.280, val_acc:0.865]
Epoch [21/120    avg_loss:0.264, val_acc:0.895]
Epoch [22/120    avg_loss:0.188, val_acc:0.911]
Epoch [23/120    avg_loss:0.208, val_acc:0.908]
Epoch [24/120    avg_loss:0.220, val_acc:0.889]
Epoch [25/120    avg_loss:0.218, val_acc:0.908]
Epoch [26/120    avg_loss:0.199, val_acc:0.887]
Epoch [27/120    avg_loss:0.206, val_acc:0.896]
Epoch [28/120    avg_loss:0.223, val_acc:0.906]
Epoch [29/120    avg_loss:0.182, val_acc:0.912]
Epoch [30/120    avg_loss:0.164, val_acc:0.923]
Epoch [31/120    avg_loss:0.187, val_acc:0.906]
Epoch [32/120    avg_loss:0.141, val_acc:0.896]
Epoch [33/120    avg_loss:0.102, val_acc:0.941]
Epoch [34/120    avg_loss:0.115, val_acc:0.946]
Epoch [35/120    avg_loss:0.164, val_acc:0.903]
Epoch [36/120    avg_loss:0.124, val_acc:0.938]
Epoch [37/120    avg_loss:0.088, val_acc:0.952]
Epoch [38/120    avg_loss:0.098, val_acc:0.943]
Epoch [39/120    avg_loss:0.138, val_acc:0.948]
Epoch [40/120    avg_loss:0.088, val_acc:0.932]
Epoch [41/120    avg_loss:0.079, val_acc:0.951]
Epoch [42/120    avg_loss:0.082, val_acc:0.952]
Epoch [43/120    avg_loss:0.059, val_acc:0.946]
Epoch [44/120    avg_loss:0.058, val_acc:0.961]
Epoch [45/120    avg_loss:0.040, val_acc:0.956]
Epoch [46/120    avg_loss:0.049, val_acc:0.957]
Epoch [47/120    avg_loss:0.074, val_acc:0.949]
Epoch [48/120    avg_loss:0.058, val_acc:0.936]
Epoch [49/120    avg_loss:0.044, val_acc:0.954]
Epoch [50/120    avg_loss:0.057, val_acc:0.942]
Epoch [51/120    avg_loss:0.034, val_acc:0.968]
Epoch [52/120    avg_loss:0.048, val_acc:0.969]
Epoch [53/120    avg_loss:0.049, val_acc:0.950]
Epoch [54/120    avg_loss:0.060, val_acc:0.956]
Epoch [55/120    avg_loss:0.032, val_acc:0.973]
Epoch [56/120    avg_loss:0.031, val_acc:0.960]
Epoch [57/120    avg_loss:0.036, val_acc:0.968]
Epoch [58/120    avg_loss:0.028, val_acc:0.971]
Epoch [59/120    avg_loss:0.026, val_acc:0.972]
Epoch [60/120    avg_loss:0.023, val_acc:0.966]
Epoch [61/120    avg_loss:0.021, val_acc:0.966]
Epoch [62/120    avg_loss:0.021, val_acc:0.972]
Epoch [63/120    avg_loss:0.024, val_acc:0.971]
Epoch [64/120    avg_loss:0.023, val_acc:0.968]
Epoch [65/120    avg_loss:0.022, val_acc:0.972]
Epoch [66/120    avg_loss:0.052, val_acc:0.963]
Epoch [67/120    avg_loss:0.058, val_acc:0.947]
Epoch [68/120    avg_loss:0.085, val_acc:0.905]
Epoch [69/120    avg_loss:0.062, val_acc:0.971]
Epoch [70/120    avg_loss:0.032, val_acc:0.968]
Epoch [71/120    avg_loss:0.027, val_acc:0.972]
Epoch [72/120    avg_loss:0.023, val_acc:0.975]
Epoch [73/120    avg_loss:0.024, val_acc:0.975]
Epoch [74/120    avg_loss:0.017, val_acc:0.975]
Epoch [75/120    avg_loss:0.018, val_acc:0.977]
Epoch [76/120    avg_loss:0.017, val_acc:0.978]
Epoch [77/120    avg_loss:0.018, val_acc:0.978]
Epoch [78/120    avg_loss:0.013, val_acc:0.978]
Epoch [79/120    avg_loss:0.015, val_acc:0.979]
Epoch [80/120    avg_loss:0.015, val_acc:0.981]
Epoch [81/120    avg_loss:0.017, val_acc:0.980]
Epoch [82/120    avg_loss:0.016, val_acc:0.980]
Epoch [83/120    avg_loss:0.018, val_acc:0.978]
Epoch [84/120    avg_loss:0.015, val_acc:0.981]
Epoch [85/120    avg_loss:0.018, val_acc:0.982]
Epoch [86/120    avg_loss:0.025, val_acc:0.980]
Epoch [87/120    avg_loss:0.016, val_acc:0.982]
Epoch [88/120    avg_loss:0.012, val_acc:0.980]
Epoch [89/120    avg_loss:0.016, val_acc:0.981]
Epoch [90/120    avg_loss:0.010, val_acc:0.981]
Epoch [91/120    avg_loss:0.015, val_acc:0.980]
Epoch [92/120    avg_loss:0.015, val_acc:0.980]
Epoch [93/120    avg_loss:0.011, val_acc:0.981]
Epoch [94/120    avg_loss:0.012, val_acc:0.980]
Epoch [95/120    avg_loss:0.013, val_acc:0.981]
Epoch [96/120    avg_loss:0.013, val_acc:0.982]
Epoch [97/120    avg_loss:0.012, val_acc:0.980]
Epoch [98/120    avg_loss:0.013, val_acc:0.982]
Epoch [99/120    avg_loss:0.012, val_acc:0.982]
Epoch [100/120    avg_loss:0.011, val_acc:0.981]
Epoch [101/120    avg_loss:0.011, val_acc:0.979]
Epoch [102/120    avg_loss:0.011, val_acc:0.978]
Epoch [103/120    avg_loss:0.011, val_acc:0.979]
Epoch [104/120    avg_loss:0.014, val_acc:0.980]
Epoch [105/120    avg_loss:0.013, val_acc:0.982]
Epoch [106/120    avg_loss:0.010, val_acc:0.981]
Epoch [107/120    avg_loss:0.012, val_acc:0.981]
Epoch [108/120    avg_loss:0.011, val_acc:0.980]
Epoch [109/120    avg_loss:0.014, val_acc:0.982]
Epoch [110/120    avg_loss:0.011, val_acc:0.981]
Epoch [111/120    avg_loss:0.014, val_acc:0.981]
Epoch [112/120    avg_loss:0.009, val_acc:0.982]
Epoch [113/120    avg_loss:0.011, val_acc:0.981]
Epoch [114/120    avg_loss:0.009, val_acc:0.982]
Epoch [115/120    avg_loss:0.010, val_acc:0.982]
Epoch [116/120    avg_loss:0.008, val_acc:0.982]
Epoch [117/120    avg_loss:0.010, val_acc:0.982]
Epoch [118/120    avg_loss:0.013, val_acc:0.982]
Epoch [119/120    avg_loss:0.011, val_acc:0.982]
Epoch [120/120    avg_loss:0.010, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1257    1    0    0    0    0    0    1    7   10    9    0
     0    0    0]
 [   0    0    0  720    7    4    0    0    0    7    1    2    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    2    0    0    0    0  857   11    0    0
     4    0    0]
 [   0    0   14    0    0    6    0    0    0    0   13 2174    2    1
     0    0    0]
 [   0    0    0    6    0    0    0    0    0    0    0    4  521    0
     2    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1125   13    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    62  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.79945799457994

F1 scores:
[       nan 0.96385542 0.98318342 0.97693351 0.98383372 0.98639456
 0.99847561 1.         0.99650757 0.81818182 0.97719498 0.98571752
 0.97201493 0.99730458 0.96401028 0.88372093 0.99408284]

Kappa:
0.9749097044789448
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:05:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe8990f1828>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.488, val_acc:0.383]
Epoch [2/120    avg_loss:2.029, val_acc:0.543]
Epoch [3/120    avg_loss:1.810, val_acc:0.553]
Epoch [4/120    avg_loss:1.562, val_acc:0.606]
Epoch [5/120    avg_loss:1.424, val_acc:0.644]
Epoch [6/120    avg_loss:1.196, val_acc:0.680]
Epoch [7/120    avg_loss:0.996, val_acc:0.686]
Epoch [8/120    avg_loss:0.860, val_acc:0.740]
Epoch [9/120    avg_loss:0.751, val_acc:0.754]
Epoch [10/120    avg_loss:0.725, val_acc:0.703]
Epoch [11/120    avg_loss:0.704, val_acc:0.765]
Epoch [12/120    avg_loss:0.588, val_acc:0.809]
Epoch [13/120    avg_loss:0.542, val_acc:0.775]
Epoch [14/120    avg_loss:0.463, val_acc:0.818]
Epoch [15/120    avg_loss:0.391, val_acc:0.839]
Epoch [16/120    avg_loss:0.404, val_acc:0.802]
Epoch [17/120    avg_loss:0.464, val_acc:0.839]
Epoch [18/120    avg_loss:0.347, val_acc:0.846]
Epoch [19/120    avg_loss:0.391, val_acc:0.885]
Epoch [20/120    avg_loss:0.349, val_acc:0.877]
Epoch [21/120    avg_loss:0.277, val_acc:0.902]
Epoch [22/120    avg_loss:0.242, val_acc:0.902]
Epoch [23/120    avg_loss:0.212, val_acc:0.917]
Epoch [24/120    avg_loss:0.215, val_acc:0.893]
Epoch [25/120    avg_loss:0.172, val_acc:0.918]
Epoch [26/120    avg_loss:0.199, val_acc:0.878]
Epoch [27/120    avg_loss:0.244, val_acc:0.904]
Epoch [28/120    avg_loss:0.178, val_acc:0.892]
Epoch [29/120    avg_loss:0.234, val_acc:0.873]
Epoch [30/120    avg_loss:0.199, val_acc:0.920]
Epoch [31/120    avg_loss:0.146, val_acc:0.926]
Epoch [32/120    avg_loss:0.161, val_acc:0.922]
Epoch [33/120    avg_loss:0.108, val_acc:0.929]
Epoch [34/120    avg_loss:0.139, val_acc:0.952]
Epoch [35/120    avg_loss:0.111, val_acc:0.945]
Epoch [36/120    avg_loss:0.097, val_acc:0.945]
Epoch [37/120    avg_loss:0.118, val_acc:0.919]
Epoch [38/120    avg_loss:0.113, val_acc:0.933]
Epoch [39/120    avg_loss:0.178, val_acc:0.933]
Epoch [40/120    avg_loss:0.146, val_acc:0.914]
Epoch [41/120    avg_loss:0.121, val_acc:0.944]
Epoch [42/120    avg_loss:0.106, val_acc:0.943]
Epoch [43/120    avg_loss:0.070, val_acc:0.947]
Epoch [44/120    avg_loss:0.103, val_acc:0.925]
Epoch [45/120    avg_loss:0.085, val_acc:0.954]
Epoch [46/120    avg_loss:0.076, val_acc:0.929]
Epoch [47/120    avg_loss:0.066, val_acc:0.944]
Epoch [48/120    avg_loss:0.065, val_acc:0.961]
Epoch [49/120    avg_loss:0.050, val_acc:0.961]
Epoch [50/120    avg_loss:0.077, val_acc:0.959]
Epoch [51/120    avg_loss:0.057, val_acc:0.959]
Epoch [52/120    avg_loss:0.058, val_acc:0.970]
Epoch [53/120    avg_loss:0.061, val_acc:0.944]
Epoch [54/120    avg_loss:0.066, val_acc:0.962]
Epoch [55/120    avg_loss:0.054, val_acc:0.957]
Epoch [56/120    avg_loss:0.083, val_acc:0.961]
Epoch [57/120    avg_loss:0.064, val_acc:0.966]
Epoch [58/120    avg_loss:0.056, val_acc:0.963]
Epoch [59/120    avg_loss:0.060, val_acc:0.946]
Epoch [60/120    avg_loss:0.067, val_acc:0.968]
Epoch [61/120    avg_loss:0.039, val_acc:0.970]
Epoch [62/120    avg_loss:0.037, val_acc:0.967]
Epoch [63/120    avg_loss:0.042, val_acc:0.945]
Epoch [64/120    avg_loss:0.053, val_acc:0.969]
Epoch [65/120    avg_loss:0.051, val_acc:0.956]
Epoch [66/120    avg_loss:0.056, val_acc:0.964]
Epoch [67/120    avg_loss:0.040, val_acc:0.970]
Epoch [68/120    avg_loss:0.035, val_acc:0.960]
Epoch [69/120    avg_loss:0.037, val_acc:0.966]
Epoch [70/120    avg_loss:0.033, val_acc:0.964]
Epoch [71/120    avg_loss:0.044, val_acc:0.967]
Epoch [72/120    avg_loss:0.030, val_acc:0.969]
Epoch [73/120    avg_loss:0.029, val_acc:0.963]
Epoch [74/120    avg_loss:0.154, val_acc:0.933]
Epoch [75/120    avg_loss:0.091, val_acc:0.948]
Epoch [76/120    avg_loss:0.048, val_acc:0.957]
Epoch [77/120    avg_loss:0.062, val_acc:0.971]
Epoch [78/120    avg_loss:0.027, val_acc:0.977]
Epoch [79/120    avg_loss:0.021, val_acc:0.982]
Epoch [80/120    avg_loss:0.027, val_acc:0.974]
Epoch [81/120    avg_loss:0.030, val_acc:0.966]
Epoch [82/120    avg_loss:0.067, val_acc:0.970]
Epoch [83/120    avg_loss:0.041, val_acc:0.969]
Epoch [84/120    avg_loss:0.021, val_acc:0.977]
Epoch [85/120    avg_loss:0.019, val_acc:0.980]
Epoch [86/120    avg_loss:0.026, val_acc:0.977]
Epoch [87/120    avg_loss:0.022, val_acc:0.980]
Epoch [88/120    avg_loss:0.014, val_acc:0.981]
Epoch [89/120    avg_loss:0.045, val_acc:0.953]
Epoch [90/120    avg_loss:0.086, val_acc:0.952]
Epoch [91/120    avg_loss:0.059, val_acc:0.958]
Epoch [92/120    avg_loss:0.063, val_acc:0.954]
Epoch [93/120    avg_loss:0.033, val_acc:0.966]
Epoch [94/120    avg_loss:0.031, val_acc:0.962]
Epoch [95/120    avg_loss:0.020, val_acc:0.970]
Epoch [96/120    avg_loss:0.028, val_acc:0.968]
Epoch [97/120    avg_loss:0.024, val_acc:0.968]
Epoch [98/120    avg_loss:0.021, val_acc:0.971]
Epoch [99/120    avg_loss:0.023, val_acc:0.972]
Epoch [100/120    avg_loss:0.021, val_acc:0.971]
Epoch [101/120    avg_loss:0.026, val_acc:0.973]
Epoch [102/120    avg_loss:0.017, val_acc:0.971]
Epoch [103/120    avg_loss:0.016, val_acc:0.974]
Epoch [104/120    avg_loss:0.019, val_acc:0.975]
Epoch [105/120    avg_loss:0.016, val_acc:0.976]
Epoch [106/120    avg_loss:0.016, val_acc:0.976]
Epoch [107/120    avg_loss:0.014, val_acc:0.976]
Epoch [108/120    avg_loss:0.015, val_acc:0.976]
Epoch [109/120    avg_loss:0.014, val_acc:0.976]
Epoch [110/120    avg_loss:0.016, val_acc:0.976]
Epoch [111/120    avg_loss:0.016, val_acc:0.976]
Epoch [112/120    avg_loss:0.023, val_acc:0.976]
Epoch [113/120    avg_loss:0.017, val_acc:0.976]
Epoch [114/120    avg_loss:0.014, val_acc:0.976]
Epoch [115/120    avg_loss:0.016, val_acc:0.976]
Epoch [116/120    avg_loss:0.014, val_acc:0.976]
Epoch [117/120    avg_loss:0.016, val_acc:0.976]
Epoch [118/120    avg_loss:0.016, val_acc:0.977]
Epoch [119/120    avg_loss:0.018, val_acc:0.977]
Epoch [120/120    avg_loss:0.017, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1255    2    0    1    0    0    0    4    5   16    2    0
     0    0    0]
 [   0    0    0  694    4   27    0    0    0    9    3    1    6    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    0    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    6    0    0    0    0  853    7    0    0
     0    3    0]
 [   0    0    7    0    0    0    1    0    1    0   10 2187    2    2
     0    0    0]
 [   0    0    0    2    0    8    0    0    0    0    2   11  508    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1130    5    0]
 [   0    0    0    0    0    0   28    0    0    0    0    0    0    0
    11  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.66937669376694

F1 scores:
[       nan 1.         0.98315707 0.96055363 0.99069767 0.94585635
 0.97382199 1.         0.99767981 0.73469388 0.9743004  0.98669073
 0.96577947 0.98666667 0.98603839 0.92911011 0.98245614]

Kappa:
0.9734283293028975
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5ecf81a860>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.526, val_acc:0.502]
Epoch [2/120    avg_loss:2.085, val_acc:0.543]
Epoch [3/120    avg_loss:1.870, val_acc:0.570]
Epoch [4/120    avg_loss:1.625, val_acc:0.620]
Epoch [5/120    avg_loss:1.479, val_acc:0.675]
Epoch [6/120    avg_loss:1.265, val_acc:0.683]
Epoch [7/120    avg_loss:1.108, val_acc:0.689]
Epoch [8/120    avg_loss:0.977, val_acc:0.730]
Epoch [9/120    avg_loss:0.867, val_acc:0.736]
Epoch [10/120    avg_loss:0.765, val_acc:0.781]
Epoch [11/120    avg_loss:0.632, val_acc:0.789]
Epoch [12/120    avg_loss:0.609, val_acc:0.827]
Epoch [13/120    avg_loss:0.641, val_acc:0.767]
Epoch [14/120    avg_loss:0.545, val_acc:0.767]
Epoch [15/120    avg_loss:0.459, val_acc:0.817]
Epoch [16/120    avg_loss:0.440, val_acc:0.835]
Epoch [17/120    avg_loss:0.373, val_acc:0.859]
Epoch [18/120    avg_loss:0.348, val_acc:0.844]
Epoch [19/120    avg_loss:0.324, val_acc:0.835]
Epoch [20/120    avg_loss:0.331, val_acc:0.840]
Epoch [21/120    avg_loss:0.313, val_acc:0.870]
Epoch [22/120    avg_loss:0.268, val_acc:0.888]
Epoch [23/120    avg_loss:0.226, val_acc:0.910]
Epoch [24/120    avg_loss:0.250, val_acc:0.875]
Epoch [25/120    avg_loss:0.226, val_acc:0.888]
Epoch [26/120    avg_loss:0.206, val_acc:0.891]
Epoch [27/120    avg_loss:0.190, val_acc:0.897]
Epoch [28/120    avg_loss:0.288, val_acc:0.911]
Epoch [29/120    avg_loss:0.250, val_acc:0.867]
Epoch [30/120    avg_loss:0.210, val_acc:0.891]
Epoch [31/120    avg_loss:0.190, val_acc:0.913]
Epoch [32/120    avg_loss:0.137, val_acc:0.935]
Epoch [33/120    avg_loss:0.146, val_acc:0.916]
Epoch [34/120    avg_loss:0.117, val_acc:0.927]
Epoch [35/120    avg_loss:0.128, val_acc:0.890]
Epoch [36/120    avg_loss:0.136, val_acc:0.936]
Epoch [37/120    avg_loss:0.159, val_acc:0.912]
Epoch [38/120    avg_loss:0.149, val_acc:0.927]
Epoch [39/120    avg_loss:0.107, val_acc:0.940]
Epoch [40/120    avg_loss:0.089, val_acc:0.936]
Epoch [41/120    avg_loss:0.097, val_acc:0.940]
Epoch [42/120    avg_loss:0.077, val_acc:0.944]
Epoch [43/120    avg_loss:0.075, val_acc:0.944]
Epoch [44/120    avg_loss:0.066, val_acc:0.945]
Epoch [45/120    avg_loss:0.065, val_acc:0.939]
Epoch [46/120    avg_loss:0.061, val_acc:0.958]
Epoch [47/120    avg_loss:0.069, val_acc:0.943]
Epoch [48/120    avg_loss:0.069, val_acc:0.952]
Epoch [49/120    avg_loss:0.069, val_acc:0.956]
Epoch [50/120    avg_loss:0.069, val_acc:0.958]
Epoch [51/120    avg_loss:0.078, val_acc:0.946]
Epoch [52/120    avg_loss:0.046, val_acc:0.967]
Epoch [53/120    avg_loss:0.046, val_acc:0.967]
Epoch [54/120    avg_loss:0.050, val_acc:0.960]
Epoch [55/120    avg_loss:0.072, val_acc:0.961]
Epoch [56/120    avg_loss:0.087, val_acc:0.942]
Epoch [57/120    avg_loss:0.077, val_acc:0.939]
Epoch [58/120    avg_loss:0.072, val_acc:0.967]
Epoch [59/120    avg_loss:0.053, val_acc:0.959]
Epoch [60/120    avg_loss:0.049, val_acc:0.946]
Epoch [61/120    avg_loss:0.048, val_acc:0.967]
Epoch [62/120    avg_loss:0.036, val_acc:0.970]
Epoch [63/120    avg_loss:0.033, val_acc:0.961]
Epoch [64/120    avg_loss:0.065, val_acc:0.944]
Epoch [65/120    avg_loss:0.084, val_acc:0.910]
Epoch [66/120    avg_loss:0.056, val_acc:0.969]
Epoch [67/120    avg_loss:0.063, val_acc:0.950]
Epoch [68/120    avg_loss:0.048, val_acc:0.972]
Epoch [69/120    avg_loss:0.036, val_acc:0.970]
Epoch [70/120    avg_loss:0.031, val_acc:0.974]
Epoch [71/120    avg_loss:0.029, val_acc:0.969]
Epoch [72/120    avg_loss:0.046, val_acc:0.964]
Epoch [73/120    avg_loss:0.038, val_acc:0.976]
Epoch [74/120    avg_loss:0.022, val_acc:0.974]
Epoch [75/120    avg_loss:0.021, val_acc:0.981]
Epoch [76/120    avg_loss:0.026, val_acc:0.975]
Epoch [77/120    avg_loss:0.044, val_acc:0.968]
Epoch [78/120    avg_loss:0.031, val_acc:0.971]
Epoch [79/120    avg_loss:0.022, val_acc:0.971]
Epoch [80/120    avg_loss:0.023, val_acc:0.952]
Epoch [81/120    avg_loss:0.038, val_acc:0.963]
Epoch [82/120    avg_loss:0.041, val_acc:0.962]
Epoch [83/120    avg_loss:0.031, val_acc:0.964]
Epoch [84/120    avg_loss:0.023, val_acc:0.979]
Epoch [85/120    avg_loss:0.042, val_acc:0.942]
Epoch [86/120    avg_loss:0.026, val_acc:0.970]
Epoch [87/120    avg_loss:0.032, val_acc:0.963]
Epoch [88/120    avg_loss:0.065, val_acc:0.943]
Epoch [89/120    avg_loss:0.045, val_acc:0.966]
Epoch [90/120    avg_loss:0.022, val_acc:0.970]
Epoch [91/120    avg_loss:0.024, val_acc:0.975]
Epoch [92/120    avg_loss:0.016, val_acc:0.976]
Epoch [93/120    avg_loss:0.017, val_acc:0.976]
Epoch [94/120    avg_loss:0.024, val_acc:0.976]
Epoch [95/120    avg_loss:0.016, val_acc:0.978]
Epoch [96/120    avg_loss:0.015, val_acc:0.978]
Epoch [97/120    avg_loss:0.017, val_acc:0.977]
Epoch [98/120    avg_loss:0.016, val_acc:0.980]
Epoch [99/120    avg_loss:0.014, val_acc:0.981]
Epoch [100/120    avg_loss:0.016, val_acc:0.982]
Epoch [101/120    avg_loss:0.014, val_acc:0.981]
Epoch [102/120    avg_loss:0.012, val_acc:0.981]
Epoch [103/120    avg_loss:0.013, val_acc:0.982]
Epoch [104/120    avg_loss:0.010, val_acc:0.983]
Epoch [105/120    avg_loss:0.012, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.985]
Epoch [107/120    avg_loss:0.013, val_acc:0.981]
Epoch [108/120    avg_loss:0.010, val_acc:0.982]
Epoch [109/120    avg_loss:0.011, val_acc:0.983]
Epoch [110/120    avg_loss:0.011, val_acc:0.984]
Epoch [111/120    avg_loss:0.009, val_acc:0.984]
Epoch [112/120    avg_loss:0.013, val_acc:0.983]
Epoch [113/120    avg_loss:0.010, val_acc:0.983]
Epoch [114/120    avg_loss:0.010, val_acc:0.982]
Epoch [115/120    avg_loss:0.010, val_acc:0.983]
Epoch [116/120    avg_loss:0.011, val_acc:0.985]
Epoch [117/120    avg_loss:0.010, val_acc:0.985]
Epoch [118/120    avg_loss:0.011, val_acc:0.984]
Epoch [119/120    avg_loss:0.011, val_acc:0.981]
Epoch [120/120    avg_loss:0.010, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1265    0    0    0    0    0    0    0    6   14    0    0
     0    0    0]
 [   0    0    1  717    3    3    1    0    0    3    0    0   18    1
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    3    2    0    0    0  857   11    0    0
     0    1    0]
 [   0    0   11    0    0    0    0    0    0    0   23 2176    0    0
     0    0    0]
 [   0    0    0    0    1    0    0    0    0    0    1   18  512    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    4    0    0    0
  1133    0    0]
 [   0    0    0    0    0    0   42    0    0    0    0    0    0    0
    48  257    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.47425474254743

F1 scores:
[       nan 0.98765432 0.98712446 0.9795082  0.98834499 0.98737084
 0.96688742 0.98039216 0.99417928 0.87804878 0.97000566 0.98261459
 0.95700935 0.99730458 0.97546276 0.84958678 0.97005988]

Kappa:
0.971182906350415
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f67dd430828>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.512, val_acc:0.547]
Epoch [2/120    avg_loss:2.071, val_acc:0.560]
Epoch [3/120    avg_loss:1.808, val_acc:0.602]
Epoch [4/120    avg_loss:1.610, val_acc:0.646]
Epoch [5/120    avg_loss:1.416, val_acc:0.646]
Epoch [6/120    avg_loss:1.238, val_acc:0.697]
Epoch [7/120    avg_loss:1.046, val_acc:0.736]
Epoch [8/120    avg_loss:0.903, val_acc:0.769]
Epoch [9/120    avg_loss:0.774, val_acc:0.805]
Epoch [10/120    avg_loss:0.759, val_acc:0.822]
Epoch [11/120    avg_loss:0.594, val_acc:0.799]
Epoch [12/120    avg_loss:0.554, val_acc:0.819]
Epoch [13/120    avg_loss:0.516, val_acc:0.822]
Epoch [14/120    avg_loss:0.470, val_acc:0.824]
Epoch [15/120    avg_loss:0.427, val_acc:0.865]
Epoch [16/120    avg_loss:0.412, val_acc:0.895]
Epoch [17/120    avg_loss:0.343, val_acc:0.900]
Epoch [18/120    avg_loss:0.328, val_acc:0.878]
Epoch [19/120    avg_loss:0.314, val_acc:0.876]
Epoch [20/120    avg_loss:0.225, val_acc:0.909]
Epoch [21/120    avg_loss:0.212, val_acc:0.835]
Epoch [22/120    avg_loss:0.270, val_acc:0.895]
Epoch [23/120    avg_loss:0.226, val_acc:0.892]
Epoch [24/120    avg_loss:0.236, val_acc:0.910]
Epoch [25/120    avg_loss:0.205, val_acc:0.918]
Epoch [26/120    avg_loss:0.208, val_acc:0.924]
Epoch [27/120    avg_loss:0.176, val_acc:0.915]
Epoch [28/120    avg_loss:0.143, val_acc:0.934]
Epoch [29/120    avg_loss:0.149, val_acc:0.933]
Epoch [30/120    avg_loss:0.130, val_acc:0.936]
Epoch [31/120    avg_loss:0.146, val_acc:0.939]
Epoch [32/120    avg_loss:0.122, val_acc:0.925]
Epoch [33/120    avg_loss:0.186, val_acc:0.900]
Epoch [34/120    avg_loss:0.166, val_acc:0.944]
Epoch [35/120    avg_loss:0.120, val_acc:0.936]
Epoch [36/120    avg_loss:0.114, val_acc:0.942]
Epoch [37/120    avg_loss:0.095, val_acc:0.943]
Epoch [38/120    avg_loss:0.075, val_acc:0.959]
Epoch [39/120    avg_loss:0.083, val_acc:0.944]
Epoch [40/120    avg_loss:0.110, val_acc:0.931]
Epoch [41/120    avg_loss:0.091, val_acc:0.948]
Epoch [42/120    avg_loss:0.074, val_acc:0.939]
Epoch [43/120    avg_loss:0.066, val_acc:0.956]
Epoch [44/120    avg_loss:0.083, val_acc:0.958]
Epoch [45/120    avg_loss:0.064, val_acc:0.959]
Epoch [46/120    avg_loss:0.065, val_acc:0.954]
Epoch [47/120    avg_loss:0.057, val_acc:0.947]
Epoch [48/120    avg_loss:0.080, val_acc:0.952]
Epoch [49/120    avg_loss:0.098, val_acc:0.929]
Epoch [50/120    avg_loss:0.057, val_acc:0.953]
Epoch [51/120    avg_loss:0.054, val_acc:0.967]
Epoch [52/120    avg_loss:0.041, val_acc:0.961]
Epoch [53/120    avg_loss:0.059, val_acc:0.949]
Epoch [54/120    avg_loss:0.047, val_acc:0.949]
Epoch [55/120    avg_loss:0.061, val_acc:0.949]
Epoch [56/120    avg_loss:0.061, val_acc:0.954]
Epoch [57/120    avg_loss:0.062, val_acc:0.960]
Epoch [58/120    avg_loss:0.080, val_acc:0.962]
Epoch [59/120    avg_loss:0.046, val_acc:0.958]
Epoch [60/120    avg_loss:0.042, val_acc:0.967]
Epoch [61/120    avg_loss:0.040, val_acc:0.967]
Epoch [62/120    avg_loss:0.051, val_acc:0.960]
Epoch [63/120    avg_loss:0.056, val_acc:0.964]
Epoch [64/120    avg_loss:0.030, val_acc:0.967]
Epoch [65/120    avg_loss:0.032, val_acc:0.961]
Epoch [66/120    avg_loss:0.029, val_acc:0.959]
Epoch [67/120    avg_loss:0.035, val_acc:0.958]
Epoch [68/120    avg_loss:0.055, val_acc:0.948]
Epoch [69/120    avg_loss:0.254, val_acc:0.907]
Epoch [70/120    avg_loss:0.072, val_acc:0.949]
Epoch [71/120    avg_loss:0.076, val_acc:0.961]
Epoch [72/120    avg_loss:0.063, val_acc:0.960]
Epoch [73/120    avg_loss:0.061, val_acc:0.947]
Epoch [74/120    avg_loss:0.065, val_acc:0.948]
Epoch [75/120    avg_loss:0.056, val_acc:0.965]
Epoch [76/120    avg_loss:0.033, val_acc:0.962]
Epoch [77/120    avg_loss:0.029, val_acc:0.963]
Epoch [78/120    avg_loss:0.025, val_acc:0.967]
Epoch [79/120    avg_loss:0.020, val_acc:0.966]
Epoch [80/120    avg_loss:0.018, val_acc:0.967]
Epoch [81/120    avg_loss:0.021, val_acc:0.968]
Epoch [82/120    avg_loss:0.022, val_acc:0.968]
Epoch [83/120    avg_loss:0.018, val_acc:0.968]
Epoch [84/120    avg_loss:0.016, val_acc:0.968]
Epoch [85/120    avg_loss:0.021, val_acc:0.967]
Epoch [86/120    avg_loss:0.016, val_acc:0.968]
Epoch [87/120    avg_loss:0.019, val_acc:0.970]
Epoch [88/120    avg_loss:0.016, val_acc:0.969]
Epoch [89/120    avg_loss:0.018, val_acc:0.967]
Epoch [90/120    avg_loss:0.017, val_acc:0.967]
Epoch [91/120    avg_loss:0.017, val_acc:0.969]
Epoch [92/120    avg_loss:0.015, val_acc:0.969]
Epoch [93/120    avg_loss:0.014, val_acc:0.968]
Epoch [94/120    avg_loss:0.015, val_acc:0.967]
Epoch [95/120    avg_loss:0.019, val_acc:0.968]
Epoch [96/120    avg_loss:0.014, val_acc:0.967]
Epoch [97/120    avg_loss:0.013, val_acc:0.969]
Epoch [98/120    avg_loss:0.014, val_acc:0.971]
Epoch [99/120    avg_loss:0.013, val_acc:0.969]
Epoch [100/120    avg_loss:0.014, val_acc:0.969]
Epoch [101/120    avg_loss:0.015, val_acc:0.969]
Epoch [102/120    avg_loss:0.014, val_acc:0.971]
Epoch [103/120    avg_loss:0.015, val_acc:0.969]
Epoch [104/120    avg_loss:0.013, val_acc:0.969]
Epoch [105/120    avg_loss:0.018, val_acc:0.970]
Epoch [106/120    avg_loss:0.015, val_acc:0.971]
Epoch [107/120    avg_loss:0.013, val_acc:0.969]
Epoch [108/120    avg_loss:0.013, val_acc:0.969]
Epoch [109/120    avg_loss:0.013, val_acc:0.969]
Epoch [110/120    avg_loss:0.014, val_acc:0.968]
Epoch [111/120    avg_loss:0.011, val_acc:0.969]
Epoch [112/120    avg_loss:0.013, val_acc:0.969]
Epoch [113/120    avg_loss:0.015, val_acc:0.968]
Epoch [114/120    avg_loss:0.014, val_acc:0.970]
Epoch [115/120    avg_loss:0.013, val_acc:0.969]
Epoch [116/120    avg_loss:0.013, val_acc:0.969]
Epoch [117/120    avg_loss:0.013, val_acc:0.970]
Epoch [118/120    avg_loss:0.012, val_acc:0.970]
Epoch [119/120    avg_loss:0.013, val_acc:0.970]
Epoch [120/120    avg_loss:0.010, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1253    0    0    1    1    0    0    2    9   15    4    0
     0    0    0]
 [   0    0    0  713    1    5    0    0    0    7    4    0   12    5
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    4    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   13    0    0    3    0
     0    0    0]
 [   0    0   10    0    0    4    0    0    0    0  845    8    4    0
     1    3    0]
 [   0    0   10    0    0    0    1    0    2    0    4 2189    2    2
     0    0    0]
 [   0    0    0    2    0    1    0    0    0    0    3    6  520    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    1    0    0    0
  1132    4    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    54  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.72357723577235

F1 scores:
[       nan 0.96202532 0.97967162 0.97404372 0.99765808 0.98521047
 0.99389313 0.98039216 0.99537037 0.65       0.9690367  0.98781588
 0.96207216 0.98143236 0.97167382 0.9057187  0.98203593]

Kappa:
0.9740335967526482
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f876aca0748>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.542, val_acc:0.461]
Epoch [2/120    avg_loss:2.033, val_acc:0.583]
Epoch [3/120    avg_loss:1.769, val_acc:0.617]
Epoch [4/120    avg_loss:1.610, val_acc:0.611]
Epoch [5/120    avg_loss:1.382, val_acc:0.691]
Epoch [6/120    avg_loss:1.166, val_acc:0.734]
Epoch [7/120    avg_loss:0.999, val_acc:0.712]
Epoch [8/120    avg_loss:0.914, val_acc:0.761]
Epoch [9/120    avg_loss:0.806, val_acc:0.768]
Epoch [10/120    avg_loss:0.644, val_acc:0.821]
Epoch [11/120    avg_loss:0.618, val_acc:0.671]
Epoch [12/120    avg_loss:0.672, val_acc:0.764]
Epoch [13/120    avg_loss:0.569, val_acc:0.800]
Epoch [14/120    avg_loss:0.467, val_acc:0.816]
Epoch [15/120    avg_loss:0.465, val_acc:0.849]
Epoch [16/120    avg_loss:0.514, val_acc:0.824]
Epoch [17/120    avg_loss:0.348, val_acc:0.859]
Epoch [18/120    avg_loss:0.330, val_acc:0.869]
Epoch [19/120    avg_loss:0.283, val_acc:0.896]
Epoch [20/120    avg_loss:0.264, val_acc:0.864]
Epoch [21/120    avg_loss:0.273, val_acc:0.905]
Epoch [22/120    avg_loss:0.220, val_acc:0.903]
Epoch [23/120    avg_loss:0.222, val_acc:0.899]
Epoch [24/120    avg_loss:0.259, val_acc:0.834]
Epoch [25/120    avg_loss:0.188, val_acc:0.911]
Epoch [26/120    avg_loss:0.182, val_acc:0.929]
Epoch [27/120    avg_loss:0.160, val_acc:0.927]
Epoch [28/120    avg_loss:0.192, val_acc:0.932]
Epoch [29/120    avg_loss:0.259, val_acc:0.867]
Epoch [30/120    avg_loss:0.233, val_acc:0.915]
Epoch [31/120    avg_loss:0.184, val_acc:0.911]
Epoch [32/120    avg_loss:0.161, val_acc:0.926]
Epoch [33/120    avg_loss:0.121, val_acc:0.940]
Epoch [34/120    avg_loss:0.122, val_acc:0.931]
Epoch [35/120    avg_loss:0.124, val_acc:0.930]
Epoch [36/120    avg_loss:0.115, val_acc:0.949]
Epoch [37/120    avg_loss:0.095, val_acc:0.947]
Epoch [38/120    avg_loss:0.079, val_acc:0.962]
Epoch [39/120    avg_loss:0.106, val_acc:0.953]
Epoch [40/120    avg_loss:0.078, val_acc:0.942]
Epoch [41/120    avg_loss:0.199, val_acc:0.918]
Epoch [42/120    avg_loss:0.124, val_acc:0.956]
Epoch [43/120    avg_loss:0.111, val_acc:0.940]
Epoch [44/120    avg_loss:0.076, val_acc:0.957]
Epoch [45/120    avg_loss:0.090, val_acc:0.947]
Epoch [46/120    avg_loss:0.060, val_acc:0.950]
Epoch [47/120    avg_loss:0.057, val_acc:0.957]
Epoch [48/120    avg_loss:0.071, val_acc:0.947]
Epoch [49/120    avg_loss:0.063, val_acc:0.969]
Epoch [50/120    avg_loss:0.041, val_acc:0.963]
Epoch [51/120    avg_loss:0.048, val_acc:0.962]
Epoch [52/120    avg_loss:0.053, val_acc:0.963]
Epoch [53/120    avg_loss:0.052, val_acc:0.957]
Epoch [54/120    avg_loss:0.063, val_acc:0.961]
Epoch [55/120    avg_loss:0.048, val_acc:0.963]
Epoch [56/120    avg_loss:0.080, val_acc:0.962]
Epoch [57/120    avg_loss:0.050, val_acc:0.963]
Epoch [58/120    avg_loss:0.048, val_acc:0.968]
Epoch [59/120    avg_loss:0.038, val_acc:0.974]
Epoch [60/120    avg_loss:0.024, val_acc:0.979]
Epoch [61/120    avg_loss:0.024, val_acc:0.978]
Epoch [62/120    avg_loss:0.036, val_acc:0.954]
Epoch [63/120    avg_loss:0.034, val_acc:0.979]
Epoch [64/120    avg_loss:0.040, val_acc:0.976]
Epoch [65/120    avg_loss:0.031, val_acc:0.971]
Epoch [66/120    avg_loss:0.038, val_acc:0.960]
Epoch [67/120    avg_loss:0.046, val_acc:0.978]
Epoch [68/120    avg_loss:0.033, val_acc:0.979]
Epoch [69/120    avg_loss:0.038, val_acc:0.970]
Epoch [70/120    avg_loss:0.038, val_acc:0.962]
Epoch [71/120    avg_loss:0.036, val_acc:0.980]
Epoch [72/120    avg_loss:0.025, val_acc:0.983]
Epoch [73/120    avg_loss:0.027, val_acc:0.974]
Epoch [74/120    avg_loss:0.021, val_acc:0.986]
Epoch [75/120    avg_loss:0.024, val_acc:0.985]
Epoch [76/120    avg_loss:0.018, val_acc:0.986]
Epoch [77/120    avg_loss:0.027, val_acc:0.989]
Epoch [78/120    avg_loss:0.018, val_acc:0.990]
Epoch [79/120    avg_loss:0.019, val_acc:0.968]
Epoch [80/120    avg_loss:0.043, val_acc:0.985]
Epoch [81/120    avg_loss:0.029, val_acc:0.976]
Epoch [82/120    avg_loss:0.028, val_acc:0.983]
Epoch [83/120    avg_loss:0.033, val_acc:0.984]
Epoch [84/120    avg_loss:0.033, val_acc:0.981]
Epoch [85/120    avg_loss:0.025, val_acc:0.980]
Epoch [86/120    avg_loss:0.034, val_acc:0.958]
Epoch [87/120    avg_loss:0.024, val_acc:0.982]
Epoch [88/120    avg_loss:0.015, val_acc:0.982]
Epoch [89/120    avg_loss:0.016, val_acc:0.976]
Epoch [90/120    avg_loss:0.033, val_acc:0.985]
Epoch [91/120    avg_loss:0.018, val_acc:0.990]
Epoch [92/120    avg_loss:0.014, val_acc:0.987]
Epoch [93/120    avg_loss:0.013, val_acc:0.993]
Epoch [94/120    avg_loss:0.012, val_acc:0.989]
Epoch [95/120    avg_loss:0.017, val_acc:0.982]
Epoch [96/120    avg_loss:0.032, val_acc:0.985]
Epoch [97/120    avg_loss:0.027, val_acc:0.929]
Epoch [98/120    avg_loss:0.026, val_acc:0.990]
Epoch [99/120    avg_loss:0.021, val_acc:0.989]
Epoch [100/120    avg_loss:0.023, val_acc:0.983]
Epoch [101/120    avg_loss:0.019, val_acc:0.992]
Epoch [102/120    avg_loss:0.014, val_acc:0.989]
Epoch [103/120    avg_loss:0.023, val_acc:0.990]
Epoch [104/120    avg_loss:0.013, val_acc:0.979]
Epoch [105/120    avg_loss:0.012, val_acc:0.988]
Epoch [106/120    avg_loss:0.015, val_acc:0.989]
Epoch [107/120    avg_loss:0.008, val_acc:0.992]
Epoch [108/120    avg_loss:0.008, val_acc:0.992]
Epoch [109/120    avg_loss:0.008, val_acc:0.992]
Epoch [110/120    avg_loss:0.008, val_acc:0.994]
Epoch [111/120    avg_loss:0.006, val_acc:0.993]
Epoch [112/120    avg_loss:0.008, val_acc:0.993]
Epoch [113/120    avg_loss:0.008, val_acc:0.993]
Epoch [114/120    avg_loss:0.007, val_acc:0.994]
Epoch [115/120    avg_loss:0.007, val_acc:0.994]
Epoch [116/120    avg_loss:0.007, val_acc:0.994]
Epoch [117/120    avg_loss:0.013, val_acc:0.990]
Epoch [118/120    avg_loss:0.008, val_acc:0.992]
Epoch [119/120    avg_loss:0.008, val_acc:0.993]
Epoch [120/120    avg_loss:0.007, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1262    0    0    0    1    0    0    3    7   11    1    0
     0    0    0]
 [   0    0    0  729    0    4    0    0    0    2    1    0   10    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    6    0    0    0    0  861    4    2    0
     0    1    0]
 [   0    0   12    0    0    0    0    0    0    0   16 2180    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  530    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    2    3    0    2
  1125    4    0]
 [   0    0    0    0    0    0   27    0    0    0    0    0    0    0
    24  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.25474254742548

F1 scores:
[       nan 0.96202532 0.9859375  0.9864682  1.         0.98412698
 0.97837435 1.         1.         0.8        0.97563739 0.98888637
 0.9823911  0.9919571  0.98339161 0.91358025 0.97674419]

Kappa:
0.9801053818349496
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc7768c0860>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.568, val_acc:0.496]
Epoch [2/120    avg_loss:2.074, val_acc:0.512]
Epoch [3/120    avg_loss:1.836, val_acc:0.574]
Epoch [4/120    avg_loss:1.623, val_acc:0.593]
Epoch [5/120    avg_loss:1.466, val_acc:0.658]
Epoch [6/120    avg_loss:1.252, val_acc:0.676]
Epoch [7/120    avg_loss:1.127, val_acc:0.694]
Epoch [8/120    avg_loss:0.923, val_acc:0.744]
Epoch [9/120    avg_loss:0.888, val_acc:0.704]
Epoch [10/120    avg_loss:0.761, val_acc:0.805]
Epoch [11/120    avg_loss:0.690, val_acc:0.795]
Epoch [12/120    avg_loss:0.588, val_acc:0.791]
Epoch [13/120    avg_loss:0.625, val_acc:0.774]
Epoch [14/120    avg_loss:0.558, val_acc:0.815]
Epoch [15/120    avg_loss:0.476, val_acc:0.865]
Epoch [16/120    avg_loss:0.423, val_acc:0.820]
Epoch [17/120    avg_loss:0.440, val_acc:0.839]
Epoch [18/120    avg_loss:0.342, val_acc:0.867]
Epoch [19/120    avg_loss:0.368, val_acc:0.874]
Epoch [20/120    avg_loss:0.293, val_acc:0.895]
Epoch [21/120    avg_loss:0.251, val_acc:0.912]
Epoch [22/120    avg_loss:0.250, val_acc:0.875]
Epoch [23/120    avg_loss:0.336, val_acc:0.899]
Epoch [24/120    avg_loss:0.301, val_acc:0.880]
Epoch [25/120    avg_loss:0.262, val_acc:0.898]
Epoch [26/120    avg_loss:0.220, val_acc:0.911]
Epoch [27/120    avg_loss:0.177, val_acc:0.921]
Epoch [28/120    avg_loss:0.156, val_acc:0.932]
Epoch [29/120    avg_loss:0.145, val_acc:0.932]
Epoch [30/120    avg_loss:0.159, val_acc:0.923]
Epoch [31/120    avg_loss:0.112, val_acc:0.935]
Epoch [32/120    avg_loss:0.101, val_acc:0.942]
Epoch [33/120    avg_loss:0.097, val_acc:0.932]
Epoch [34/120    avg_loss:0.121, val_acc:0.936]
Epoch [35/120    avg_loss:0.152, val_acc:0.900]
Epoch [36/120    avg_loss:0.166, val_acc:0.933]
Epoch [37/120    avg_loss:0.163, val_acc:0.918]
Epoch [38/120    avg_loss:0.109, val_acc:0.942]
Epoch [39/120    avg_loss:0.094, val_acc:0.948]
Epoch [40/120    avg_loss:0.068, val_acc:0.971]
Epoch [41/120    avg_loss:0.073, val_acc:0.960]
Epoch [42/120    avg_loss:0.061, val_acc:0.946]
Epoch [43/120    avg_loss:0.063, val_acc:0.955]
Epoch [44/120    avg_loss:0.074, val_acc:0.905]
Epoch [45/120    avg_loss:0.073, val_acc:0.958]
Epoch [46/120    avg_loss:0.074, val_acc:0.959]
Epoch [47/120    avg_loss:0.077, val_acc:0.890]
Epoch [48/120    avg_loss:0.082, val_acc:0.958]
Epoch [49/120    avg_loss:0.163, val_acc:0.922]
Epoch [50/120    avg_loss:0.146, val_acc:0.934]
Epoch [51/120    avg_loss:0.095, val_acc:0.945]
Epoch [52/120    avg_loss:0.093, val_acc:0.967]
Epoch [53/120    avg_loss:0.341, val_acc:0.434]
Epoch [54/120    avg_loss:1.205, val_acc:0.693]
Epoch [55/120    avg_loss:0.718, val_acc:0.787]
Epoch [56/120    avg_loss:0.534, val_acc:0.835]
Epoch [57/120    avg_loss:0.417, val_acc:0.860]
Epoch [58/120    avg_loss:0.430, val_acc:0.870]
Epoch [59/120    avg_loss:0.344, val_acc:0.893]
Epoch [60/120    avg_loss:0.337, val_acc:0.902]
Epoch [61/120    avg_loss:1.651, val_acc:0.121]
Epoch [62/120    avg_loss:2.749, val_acc:0.157]
Epoch [63/120    avg_loss:2.462, val_acc:0.214]
Epoch [64/120    avg_loss:2.292, val_acc:0.295]
Epoch [65/120    avg_loss:2.185, val_acc:0.347]
Epoch [66/120    avg_loss:2.163, val_acc:0.378]
Epoch [67/120    avg_loss:2.051, val_acc:0.378]
Epoch [68/120    avg_loss:2.040, val_acc:0.381]
Epoch [69/120    avg_loss:2.047, val_acc:0.384]
Epoch [70/120    avg_loss:2.098, val_acc:0.384]
Epoch [71/120    avg_loss:2.011, val_acc:0.386]
Epoch [72/120    avg_loss:2.004, val_acc:0.385]
Epoch [73/120    avg_loss:2.074, val_acc:0.389]
Epoch [74/120    avg_loss:2.004, val_acc:0.391]
Epoch [75/120    avg_loss:2.006, val_acc:0.401]
Epoch [76/120    avg_loss:1.961, val_acc:0.401]
Epoch [77/120    avg_loss:2.036, val_acc:0.402]
Epoch [78/120    avg_loss:2.006, val_acc:0.404]
Epoch [79/120    avg_loss:1.987, val_acc:0.403]
Epoch [80/120    avg_loss:1.985, val_acc:0.404]
Epoch [81/120    avg_loss:1.984, val_acc:0.405]
Epoch [82/120    avg_loss:1.984, val_acc:0.407]
Epoch [83/120    avg_loss:1.985, val_acc:0.407]
Epoch [84/120    avg_loss:1.951, val_acc:0.406]
Epoch [85/120    avg_loss:2.001, val_acc:0.407]
Epoch [86/120    avg_loss:1.992, val_acc:0.408]
Epoch [87/120    avg_loss:1.972, val_acc:0.407]
Epoch [88/120    avg_loss:2.007, val_acc:0.409]
Epoch [89/120    avg_loss:1.960, val_acc:0.411]
Epoch [90/120    avg_loss:1.969, val_acc:0.411]
Epoch [91/120    avg_loss:1.983, val_acc:0.412]
Epoch [92/120    avg_loss:1.948, val_acc:0.413]
Epoch [93/120    avg_loss:1.949, val_acc:0.413]
Epoch [94/120    avg_loss:1.940, val_acc:0.413]
Epoch [95/120    avg_loss:1.977, val_acc:0.413]
Epoch [96/120    avg_loss:1.966, val_acc:0.413]
Epoch [97/120    avg_loss:1.992, val_acc:0.413]
Epoch [98/120    avg_loss:1.964, val_acc:0.413]
Epoch [99/120    avg_loss:2.015, val_acc:0.413]
Epoch [100/120    avg_loss:1.968, val_acc:0.413]
Epoch [101/120    avg_loss:1.993, val_acc:0.413]
Epoch [102/120    avg_loss:1.967, val_acc:0.413]
Epoch [103/120    avg_loss:1.951, val_acc:0.413]
Epoch [104/120    avg_loss:1.995, val_acc:0.413]
Epoch [105/120    avg_loss:1.961, val_acc:0.413]
Epoch [106/120    avg_loss:1.989, val_acc:0.413]
Epoch [107/120    avg_loss:1.990, val_acc:0.413]
Epoch [108/120    avg_loss:2.000, val_acc:0.413]
Epoch [109/120    avg_loss:1.979, val_acc:0.413]
Epoch [110/120    avg_loss:1.980, val_acc:0.413]
Epoch [111/120    avg_loss:1.946, val_acc:0.413]
Epoch [112/120    avg_loss:1.998, val_acc:0.413]
Epoch [113/120    avg_loss:1.968, val_acc:0.413]
Epoch [114/120    avg_loss:1.986, val_acc:0.413]
Epoch [115/120    avg_loss:1.990, val_acc:0.413]
Epoch [116/120    avg_loss:1.975, val_acc:0.413]
Epoch [117/120    avg_loss:1.941, val_acc:0.413]
Epoch [118/120    avg_loss:1.958, val_acc:0.413]
Epoch [119/120    avg_loss:1.934, val_acc:0.413]
Epoch [120/120    avg_loss:1.953, val_acc:0.413]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]
 [  0   0   6   0   0   0   2   0  30   0   1   0   2   0   0   0   0]
 [  0   0 297  57   6  17  25   0 261   0 266 312  25  11   8   0   0]
 [  0  17 145  83  23  17  42   0   0   0 124 224  38  25   9   0   0]
 [  0   0  81   4   3   0   4   0   5   0  29  78   0   4   5   0   0]
 [  0   0   6   0   0 150  36   0  33   0  11   0   2   0 197   0   0]
 [  0   0  10  21   0   0 567   0   0   0   0  45   0   3  11   0   0]
 [  0   4   0   0   0   0  14   0   7   0   0   0   0   0   0   0   0]
 [  0   0   3   5   0   0   0   0 417   0   0   1   4   0   0   0   0]
 [  0   0   0   0   2   0  14   0   0   0   0   0   0   2   0   0   0]
 [  0   0 153  32  21  28  53   0   0   0 353 167  34  12  14   0   8]
 [  0   0 923  84   9  71 116   0   4   0   2 633 286  19  16   0  47]
 [  0   0  75  15   5   6  10   0   6   0  83  92 162   3  11   0  66]
 [  0   0   0   0   1   0   1   0   0   0   0   0   0 181   2   0   0]
 [  0   2   0   0   1   9  37   0 115   0   0  22  19  91 828  15   0]
 [  0   0   1  37   0   0  69   0   0   0   0   6  31  52  51 100   0]
 [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  84]]

Accuracy:
41.82113821138211

F1 scores:
[       nan 0.         0.19899497 0.15299539 0.02112676 0.40927694
 0.68852459 0.         0.63761468 0.         0.40481651 0.33403694
 0.28496042 0.61564626 0.72282846 0.43290043 0.58131488]

Kappa:
0.34398740099439273
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3e0ba8b780>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.547, val_acc:0.516]
Epoch [2/120    avg_loss:2.075, val_acc:0.551]
Epoch [3/120    avg_loss:1.821, val_acc:0.598]
Epoch [4/120    avg_loss:1.675, val_acc:0.627]
Epoch [5/120    avg_loss:1.414, val_acc:0.657]
Epoch [6/120    avg_loss:1.205, val_acc:0.667]
Epoch [7/120    avg_loss:1.025, val_acc:0.673]
Epoch [8/120    avg_loss:0.891, val_acc:0.762]
Epoch [9/120    avg_loss:0.848, val_acc:0.781]
Epoch [10/120    avg_loss:0.729, val_acc:0.783]
Epoch [11/120    avg_loss:0.595, val_acc:0.815]
Epoch [12/120    avg_loss:0.514, val_acc:0.851]
Epoch [13/120    avg_loss:0.549, val_acc:0.846]
Epoch [14/120    avg_loss:0.461, val_acc:0.850]
Epoch [15/120    avg_loss:0.416, val_acc:0.838]
Epoch [16/120    avg_loss:0.430, val_acc:0.872]
Epoch [17/120    avg_loss:0.387, val_acc:0.830]
Epoch [18/120    avg_loss:0.392, val_acc:0.890]
Epoch [19/120    avg_loss:0.377, val_acc:0.901]
Epoch [20/120    avg_loss:0.318, val_acc:0.887]
Epoch [21/120    avg_loss:0.314, val_acc:0.883]
Epoch [22/120    avg_loss:0.282, val_acc:0.915]
Epoch [23/120    avg_loss:0.222, val_acc:0.912]
Epoch [24/120    avg_loss:0.204, val_acc:0.865]
Epoch [25/120    avg_loss:0.253, val_acc:0.909]
Epoch [26/120    avg_loss:0.181, val_acc:0.883]
Epoch [27/120    avg_loss:0.232, val_acc:0.893]
Epoch [28/120    avg_loss:0.154, val_acc:0.906]
Epoch [29/120    avg_loss:0.159, val_acc:0.923]
Epoch [30/120    avg_loss:0.127, val_acc:0.941]
Epoch [31/120    avg_loss:0.128, val_acc:0.935]
Epoch [32/120    avg_loss:0.140, val_acc:0.931]
Epoch [33/120    avg_loss:0.147, val_acc:0.953]
Epoch [34/120    avg_loss:0.118, val_acc:0.943]
Epoch [35/120    avg_loss:0.109, val_acc:0.941]
Epoch [36/120    avg_loss:0.102, val_acc:0.939]
Epoch [37/120    avg_loss:0.088, val_acc:0.940]
Epoch [38/120    avg_loss:0.103, val_acc:0.931]
Epoch [39/120    avg_loss:0.102, val_acc:0.941]
Epoch [40/120    avg_loss:0.113, val_acc:0.913]
Epoch [41/120    avg_loss:0.339, val_acc:0.914]
Epoch [42/120    avg_loss:0.184, val_acc:0.906]
Epoch [43/120    avg_loss:0.137, val_acc:0.919]
Epoch [44/120    avg_loss:0.208, val_acc:0.907]
Epoch [45/120    avg_loss:0.112, val_acc:0.932]
Epoch [46/120    avg_loss:0.101, val_acc:0.923]
Epoch [47/120    avg_loss:0.079, val_acc:0.935]
Epoch [48/120    avg_loss:0.063, val_acc:0.946]
Epoch [49/120    avg_loss:0.055, val_acc:0.945]
Epoch [50/120    avg_loss:0.049, val_acc:0.956]
Epoch [51/120    avg_loss:0.062, val_acc:0.954]
Epoch [52/120    avg_loss:0.052, val_acc:0.955]
Epoch [53/120    avg_loss:0.056, val_acc:0.954]
Epoch [54/120    avg_loss:0.055, val_acc:0.952]
Epoch [55/120    avg_loss:0.044, val_acc:0.957]
Epoch [56/120    avg_loss:0.045, val_acc:0.955]
Epoch [57/120    avg_loss:0.054, val_acc:0.957]
Epoch [58/120    avg_loss:0.048, val_acc:0.955]
Epoch [59/120    avg_loss:0.037, val_acc:0.956]
Epoch [60/120    avg_loss:0.046, val_acc:0.956]
Epoch [61/120    avg_loss:0.041, val_acc:0.961]
Epoch [62/120    avg_loss:0.044, val_acc:0.958]
Epoch [63/120    avg_loss:0.052, val_acc:0.957]
Epoch [64/120    avg_loss:0.039, val_acc:0.957]
Epoch [65/120    avg_loss:0.045, val_acc:0.954]
Epoch [66/120    avg_loss:0.044, val_acc:0.956]
Epoch [67/120    avg_loss:0.046, val_acc:0.956]
Epoch [68/120    avg_loss:0.048, val_acc:0.960]
Epoch [69/120    avg_loss:0.038, val_acc:0.958]
Epoch [70/120    avg_loss:0.039, val_acc:0.960]
Epoch [71/120    avg_loss:0.045, val_acc:0.963]
Epoch [72/120    avg_loss:0.033, val_acc:0.963]
Epoch [73/120    avg_loss:0.039, val_acc:0.963]
Epoch [74/120    avg_loss:0.040, val_acc:0.962]
Epoch [75/120    avg_loss:0.034, val_acc:0.962]
Epoch [76/120    avg_loss:0.032, val_acc:0.961]
Epoch [77/120    avg_loss:0.032, val_acc:0.964]
Epoch [78/120    avg_loss:0.039, val_acc:0.963]
Epoch [79/120    avg_loss:0.034, val_acc:0.963]
Epoch [80/120    avg_loss:0.033, val_acc:0.962]
Epoch [81/120    avg_loss:0.031, val_acc:0.963]
Epoch [82/120    avg_loss:0.034, val_acc:0.959]
Epoch [83/120    avg_loss:0.033, val_acc:0.963]
Epoch [84/120    avg_loss:0.050, val_acc:0.961]
Epoch [85/120    avg_loss:0.030, val_acc:0.961]
Epoch [86/120    avg_loss:0.035, val_acc:0.961]
Epoch [87/120    avg_loss:0.048, val_acc:0.960]
Epoch [88/120    avg_loss:0.034, val_acc:0.962]
Epoch [89/120    avg_loss:0.030, val_acc:0.963]
Epoch [90/120    avg_loss:0.035, val_acc:0.963]
Epoch [91/120    avg_loss:0.034, val_acc:0.963]
Epoch [92/120    avg_loss:0.037, val_acc:0.966]
Epoch [93/120    avg_loss:0.031, val_acc:0.966]
Epoch [94/120    avg_loss:0.027, val_acc:0.966]
Epoch [95/120    avg_loss:0.041, val_acc:0.967]
Epoch [96/120    avg_loss:0.029, val_acc:0.966]
Epoch [97/120    avg_loss:0.028, val_acc:0.966]
Epoch [98/120    avg_loss:0.033, val_acc:0.968]
Epoch [99/120    avg_loss:0.032, val_acc:0.968]
Epoch [100/120    avg_loss:0.032, val_acc:0.968]
Epoch [101/120    avg_loss:0.031, val_acc:0.968]
Epoch [102/120    avg_loss:0.037, val_acc:0.967]
Epoch [103/120    avg_loss:0.030, val_acc:0.967]
Epoch [104/120    avg_loss:0.030, val_acc:0.966]
Epoch [105/120    avg_loss:0.027, val_acc:0.967]
Epoch [106/120    avg_loss:0.035, val_acc:0.967]
Epoch [107/120    avg_loss:0.030, val_acc:0.967]
Epoch [108/120    avg_loss:0.033, val_acc:0.968]
Epoch [109/120    avg_loss:0.036, val_acc:0.967]
Epoch [110/120    avg_loss:0.031, val_acc:0.967]
Epoch [111/120    avg_loss:0.031, val_acc:0.968]
Epoch [112/120    avg_loss:0.032, val_acc:0.968]
Epoch [113/120    avg_loss:0.032, val_acc:0.968]
Epoch [114/120    avg_loss:0.029, val_acc:0.968]
Epoch [115/120    avg_loss:0.027, val_acc:0.968]
Epoch [116/120    avg_loss:0.033, val_acc:0.968]
Epoch [117/120    avg_loss:0.030, val_acc:0.968]
Epoch [118/120    avg_loss:0.027, val_acc:0.968]
Epoch [119/120    avg_loss:0.038, val_acc:0.968]
Epoch [120/120    avg_loss:0.029, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1255    1    0    0    2    0    0    0    8   19    0    0
     0    0    0]
 [   0    0    1  673    0   20    0    0    0    5    1    0   44    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    3    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   13    0    0   12    5    0    0    0  830   14    0    0
     0    1    0]
 [   0    0   15    6    0    2    2    0    0    0   33 2144    4    2
     0    2    0]
 [   0    0   12    6    1    5    0    0    0    1    2    0  500    0
     1    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    0    0    0
  1134    3    0]
 [   0    0    0    0    0    0    7    0    0    4    0    0    0    0
    51  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.40108401084011

F1 scores:
[       nan 0.93506494 0.97249128 0.93863319 0.99765808 0.95027624
 0.98568199 0.94339623 1.         0.75555556 0.94586895 0.97676538
 0.92250923 0.98666667 0.97464547 0.89341693 0.95348837]

Kappa:
0.9589814076503566
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8282ac97f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.529, val_acc:0.464]
Epoch [2/120    avg_loss:2.119, val_acc:0.528]
Epoch [3/120    avg_loss:1.847, val_acc:0.577]
Epoch [4/120    avg_loss:1.619, val_acc:0.594]
Epoch [5/120    avg_loss:1.455, val_acc:0.606]
Epoch [6/120    avg_loss:1.222, val_acc:0.588]
Epoch [7/120    avg_loss:1.087, val_acc:0.690]
Epoch [8/120    avg_loss:0.938, val_acc:0.641]
Epoch [9/120    avg_loss:0.843, val_acc:0.707]
Epoch [10/120    avg_loss:0.879, val_acc:0.730]
Epoch [11/120    avg_loss:0.804, val_acc:0.760]
Epoch [12/120    avg_loss:0.670, val_acc:0.779]
Epoch [13/120    avg_loss:0.633, val_acc:0.717]
Epoch [14/120    avg_loss:0.603, val_acc:0.788]
Epoch [15/120    avg_loss:0.490, val_acc:0.809]
Epoch [16/120    avg_loss:0.481, val_acc:0.807]
Epoch [17/120    avg_loss:0.532, val_acc:0.745]
Epoch [18/120    avg_loss:0.457, val_acc:0.844]
Epoch [19/120    avg_loss:0.454, val_acc:0.833]
Epoch [20/120    avg_loss:0.394, val_acc:0.843]
Epoch [21/120    avg_loss:0.326, val_acc:0.873]
Epoch [22/120    avg_loss:0.329, val_acc:0.863]
Epoch [23/120    avg_loss:0.309, val_acc:0.904]
Epoch [24/120    avg_loss:0.234, val_acc:0.897]
Epoch [25/120    avg_loss:0.250, val_acc:0.885]
Epoch [26/120    avg_loss:0.271, val_acc:0.886]
Epoch [27/120    avg_loss:0.262, val_acc:0.911]
Epoch [28/120    avg_loss:0.246, val_acc:0.912]
Epoch [29/120    avg_loss:0.205, val_acc:0.911]
Epoch [30/120    avg_loss:0.223, val_acc:0.839]
Epoch [31/120    avg_loss:0.177, val_acc:0.877]
Epoch [32/120    avg_loss:0.170, val_acc:0.930]
Epoch [33/120    avg_loss:0.155, val_acc:0.917]
Epoch [34/120    avg_loss:0.149, val_acc:0.914]
Epoch [35/120    avg_loss:0.127, val_acc:0.932]
Epoch [36/120    avg_loss:0.165, val_acc:0.869]
Epoch [37/120    avg_loss:0.161, val_acc:0.914]
Epoch [38/120    avg_loss:0.178, val_acc:0.927]
Epoch [39/120    avg_loss:0.105, val_acc:0.940]
Epoch [40/120    avg_loss:0.089, val_acc:0.918]
Epoch [41/120    avg_loss:0.082, val_acc:0.956]
Epoch [42/120    avg_loss:0.085, val_acc:0.939]
Epoch [43/120    avg_loss:0.079, val_acc:0.960]
Epoch [44/120    avg_loss:0.104, val_acc:0.917]
Epoch [45/120    avg_loss:0.075, val_acc:0.952]
Epoch [46/120    avg_loss:0.109, val_acc:0.938]
Epoch [47/120    avg_loss:0.106, val_acc:0.951]
Epoch [48/120    avg_loss:0.109, val_acc:0.954]
Epoch [49/120    avg_loss:0.085, val_acc:0.936]
Epoch [50/120    avg_loss:0.076, val_acc:0.954]
Epoch [51/120    avg_loss:0.061, val_acc:0.944]
Epoch [52/120    avg_loss:0.074, val_acc:0.960]
Epoch [53/120    avg_loss:0.075, val_acc:0.958]
Epoch [54/120    avg_loss:0.077, val_acc:0.969]
Epoch [55/120    avg_loss:0.069, val_acc:0.961]
Epoch [56/120    avg_loss:0.059, val_acc:0.970]
Epoch [57/120    avg_loss:0.060, val_acc:0.963]
Epoch [58/120    avg_loss:0.066, val_acc:0.954]
Epoch [59/120    avg_loss:0.045, val_acc:0.959]
Epoch [60/120    avg_loss:0.039, val_acc:0.967]
Epoch [61/120    avg_loss:0.046, val_acc:0.970]
Epoch [62/120    avg_loss:0.041, val_acc:0.965]
Epoch [63/120    avg_loss:0.053, val_acc:0.960]
Epoch [64/120    avg_loss:0.062, val_acc:0.954]
Epoch [65/120    avg_loss:0.038, val_acc:0.972]
Epoch [66/120    avg_loss:0.060, val_acc:0.960]
Epoch [67/120    avg_loss:0.073, val_acc:0.963]
Epoch [68/120    avg_loss:0.047, val_acc:0.970]
Epoch [69/120    avg_loss:0.037, val_acc:0.971]
Epoch [70/120    avg_loss:0.036, val_acc:0.968]
Epoch [71/120    avg_loss:0.036, val_acc:0.978]
Epoch [72/120    avg_loss:0.039, val_acc:0.958]
Epoch [73/120    avg_loss:0.038, val_acc:0.968]
Epoch [74/120    avg_loss:0.043, val_acc:0.967]
Epoch [75/120    avg_loss:0.037, val_acc:0.977]
Epoch [76/120    avg_loss:0.029, val_acc:0.974]
Epoch [77/120    avg_loss:0.032, val_acc:0.974]
Epoch [78/120    avg_loss:0.026, val_acc:0.971]
Epoch [79/120    avg_loss:0.054, val_acc:0.966]
Epoch [80/120    avg_loss:0.045, val_acc:0.977]
Epoch [81/120    avg_loss:0.034, val_acc:0.976]
Epoch [82/120    avg_loss:0.032, val_acc:0.976]
Epoch [83/120    avg_loss:0.036, val_acc:0.979]
Epoch [84/120    avg_loss:0.030, val_acc:0.978]
Epoch [85/120    avg_loss:0.026, val_acc:0.984]
Epoch [86/120    avg_loss:0.021, val_acc:0.979]
Epoch [87/120    avg_loss:0.028, val_acc:0.965]
Epoch [88/120    avg_loss:0.022, val_acc:0.980]
Epoch [89/120    avg_loss:0.026, val_acc:0.971]
Epoch [90/120    avg_loss:0.036, val_acc:0.974]
Epoch [91/120    avg_loss:0.031, val_acc:0.979]
Epoch [92/120    avg_loss:0.018, val_acc:0.983]
Epoch [93/120    avg_loss:0.025, val_acc:0.978]
Epoch [94/120    avg_loss:0.032, val_acc:0.980]
Epoch [95/120    avg_loss:0.021, val_acc:0.969]
Epoch [96/120    avg_loss:0.018, val_acc:0.983]
Epoch [97/120    avg_loss:0.011, val_acc:0.986]
Epoch [98/120    avg_loss:0.028, val_acc:0.954]
Epoch [99/120    avg_loss:0.065, val_acc:0.963]
Epoch [100/120    avg_loss:0.025, val_acc:0.979]
Epoch [101/120    avg_loss:0.019, val_acc:0.984]
Epoch [102/120    avg_loss:0.013, val_acc:0.982]
Epoch [103/120    avg_loss:0.011, val_acc:0.985]
Epoch [104/120    avg_loss:0.014, val_acc:0.978]
Epoch [105/120    avg_loss:0.017, val_acc:0.979]
Epoch [106/120    avg_loss:0.021, val_acc:0.981]
Epoch [107/120    avg_loss:0.024, val_acc:0.978]
Epoch [108/120    avg_loss:0.018, val_acc:0.979]
Epoch [109/120    avg_loss:0.014, val_acc:0.982]
Epoch [110/120    avg_loss:0.011, val_acc:0.983]
Epoch [111/120    avg_loss:0.011, val_acc:0.984]
Epoch [112/120    avg_loss:0.007, val_acc:0.985]
Epoch [113/120    avg_loss:0.009, val_acc:0.984]
Epoch [114/120    avg_loss:0.009, val_acc:0.983]
Epoch [115/120    avg_loss:0.013, val_acc:0.982]
Epoch [116/120    avg_loss:0.007, val_acc:0.982]
Epoch [117/120    avg_loss:0.009, val_acc:0.982]
Epoch [118/120    avg_loss:0.008, val_acc:0.982]
Epoch [119/120    avg_loss:0.007, val_acc:0.984]
Epoch [120/120    avg_loss:0.008, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    2    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1262    1    0    0    0    0    0    0    6   16    0    0
     0    0    0]
 [   0    0    0  699    0    6    0    0    0    7    1    0   32    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    7    0    0    0    1  848    8    0    0
     0    9    0]
 [   0    0    6    0    0    0    1    0    0    0   13 2184    6    0
     0    0    0]
 [   0    0    1    0    0    4    0    0    0    0    0    5  519    0
     2    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    4    0    0    0
  1128    7    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    39  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.85365853658537

F1 scores:
[       nan 0.94871795 0.98670837 0.96480331 1.         0.97853107
 0.99847793 0.98039216 1.         0.74418605 0.96969697 0.98734177
 0.95142071 0.99462366 0.97746967 0.91803279 0.98245614]

Kappa:
0.975528271735957
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5ecdb05898>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.534, val_acc:0.413]
Epoch [2/120    avg_loss:2.089, val_acc:0.543]
Epoch [3/120    avg_loss:1.844, val_acc:0.564]
Epoch [4/120    avg_loss:1.647, val_acc:0.558]
Epoch [5/120    avg_loss:1.474, val_acc:0.582]
Epoch [6/120    avg_loss:1.316, val_acc:0.668]
Epoch [7/120    avg_loss:1.163, val_acc:0.653]
Epoch [8/120    avg_loss:1.048, val_acc:0.726]
Epoch [9/120    avg_loss:0.854, val_acc:0.762]
Epoch [10/120    avg_loss:0.777, val_acc:0.763]
Epoch [11/120    avg_loss:0.770, val_acc:0.766]
Epoch [12/120    avg_loss:0.598, val_acc:0.820]
Epoch [13/120    avg_loss:0.645, val_acc:0.753]
Epoch [14/120    avg_loss:0.525, val_acc:0.825]
Epoch [15/120    avg_loss:0.457, val_acc:0.834]
Epoch [16/120    avg_loss:0.527, val_acc:0.805]
Epoch [17/120    avg_loss:0.390, val_acc:0.858]
Epoch [18/120    avg_loss:0.353, val_acc:0.847]
Epoch [19/120    avg_loss:0.344, val_acc:0.889]
Epoch [20/120    avg_loss:0.298, val_acc:0.883]
Epoch [21/120    avg_loss:0.263, val_acc:0.858]
Epoch [22/120    avg_loss:0.230, val_acc:0.878]
Epoch [23/120    avg_loss:0.238, val_acc:0.897]
Epoch [24/120    avg_loss:0.249, val_acc:0.883]
Epoch [25/120    avg_loss:0.296, val_acc:0.898]
Epoch [26/120    avg_loss:0.222, val_acc:0.905]
Epoch [27/120    avg_loss:0.189, val_acc:0.905]
Epoch [28/120    avg_loss:0.160, val_acc:0.933]
Epoch [29/120    avg_loss:0.196, val_acc:0.906]
Epoch [30/120    avg_loss:0.153, val_acc:0.941]
Epoch [31/120    avg_loss:0.145, val_acc:0.926]
Epoch [32/120    avg_loss:0.126, val_acc:0.929]
Epoch [33/120    avg_loss:0.172, val_acc:0.914]
Epoch [34/120    avg_loss:0.162, val_acc:0.926]
Epoch [35/120    avg_loss:0.108, val_acc:0.940]
Epoch [36/120    avg_loss:0.105, val_acc:0.931]
Epoch [37/120    avg_loss:0.100, val_acc:0.950]
Epoch [38/120    avg_loss:0.086, val_acc:0.923]
Epoch [39/120    avg_loss:0.089, val_acc:0.941]
Epoch [40/120    avg_loss:0.096, val_acc:0.922]
Epoch [41/120    avg_loss:0.102, val_acc:0.943]
Epoch [42/120    avg_loss:0.160, val_acc:0.939]
Epoch [43/120    avg_loss:0.097, val_acc:0.946]
Epoch [44/120    avg_loss:0.078, val_acc:0.936]
Epoch [45/120    avg_loss:0.095, val_acc:0.956]
Epoch [46/120    avg_loss:0.063, val_acc:0.962]
Epoch [47/120    avg_loss:0.067, val_acc:0.951]
Epoch [48/120    avg_loss:0.095, val_acc:0.941]
Epoch [49/120    avg_loss:0.080, val_acc:0.952]
Epoch [50/120    avg_loss:0.063, val_acc:0.960]
Epoch [51/120    avg_loss:0.051, val_acc:0.954]
Epoch [52/120    avg_loss:0.041, val_acc:0.956]
Epoch [53/120    avg_loss:0.047, val_acc:0.969]
Epoch [54/120    avg_loss:0.041, val_acc:0.951]
Epoch [55/120    avg_loss:0.061, val_acc:0.910]
Epoch [56/120    avg_loss:0.062, val_acc:0.966]
Epoch [57/120    avg_loss:0.055, val_acc:0.958]
Epoch [58/120    avg_loss:0.039, val_acc:0.970]
Epoch [59/120    avg_loss:0.064, val_acc:0.952]
Epoch [60/120    avg_loss:0.072, val_acc:0.957]
Epoch [61/120    avg_loss:0.053, val_acc:0.959]
Epoch [62/120    avg_loss:0.051, val_acc:0.959]
Epoch [63/120    avg_loss:0.035, val_acc:0.968]
Epoch [64/120    avg_loss:0.033, val_acc:0.974]
Epoch [65/120    avg_loss:0.027, val_acc:0.967]
Epoch [66/120    avg_loss:0.035, val_acc:0.969]
Epoch [67/120    avg_loss:0.023, val_acc:0.964]
Epoch [68/120    avg_loss:0.039, val_acc:0.961]
Epoch [69/120    avg_loss:0.035, val_acc:0.970]
Epoch [70/120    avg_loss:0.039, val_acc:0.963]
Epoch [71/120    avg_loss:0.052, val_acc:0.966]
Epoch [72/120    avg_loss:0.041, val_acc:0.969]
Epoch [73/120    avg_loss:0.039, val_acc:0.961]
Epoch [74/120    avg_loss:0.032, val_acc:0.963]
Epoch [75/120    avg_loss:0.038, val_acc:0.949]
Epoch [76/120    avg_loss:0.039, val_acc:0.954]
Epoch [77/120    avg_loss:0.027, val_acc:0.977]
Epoch [78/120    avg_loss:0.026, val_acc:0.954]
Epoch [79/120    avg_loss:0.018, val_acc:0.980]
Epoch [80/120    avg_loss:0.014, val_acc:0.977]
Epoch [81/120    avg_loss:0.015, val_acc:0.974]
Epoch [82/120    avg_loss:0.019, val_acc:0.974]
Epoch [83/120    avg_loss:0.025, val_acc:0.976]
Epoch [84/120    avg_loss:0.032, val_acc:0.964]
Epoch [85/120    avg_loss:0.019, val_acc:0.972]
Epoch [86/120    avg_loss:0.017, val_acc:0.972]
Epoch [87/120    avg_loss:0.024, val_acc:0.969]
Epoch [88/120    avg_loss:0.020, val_acc:0.976]
Epoch [89/120    avg_loss:0.033, val_acc:0.978]
Epoch [90/120    avg_loss:0.014, val_acc:0.980]
Epoch [91/120    avg_loss:0.014, val_acc:0.980]
Epoch [92/120    avg_loss:0.016, val_acc:0.983]
Epoch [93/120    avg_loss:0.014, val_acc:0.980]
Epoch [94/120    avg_loss:0.013, val_acc:0.981]
Epoch [95/120    avg_loss:0.027, val_acc:0.967]
Epoch [96/120    avg_loss:0.018, val_acc:0.977]
Epoch [97/120    avg_loss:0.019, val_acc:0.976]
Epoch [98/120    avg_loss:0.016, val_acc:0.975]
Epoch [99/120    avg_loss:0.022, val_acc:0.979]
Epoch [100/120    avg_loss:0.022, val_acc:0.971]
Epoch [101/120    avg_loss:0.015, val_acc:0.979]
Epoch [102/120    avg_loss:0.013, val_acc:0.969]
Epoch [103/120    avg_loss:0.037, val_acc:0.983]
Epoch [104/120    avg_loss:0.022, val_acc:0.980]
Epoch [105/120    avg_loss:0.011, val_acc:0.980]
Epoch [106/120    avg_loss:0.016, val_acc:0.968]
Epoch [107/120    avg_loss:0.015, val_acc:0.984]
Epoch [108/120    avg_loss:0.019, val_acc:0.975]
Epoch [109/120    avg_loss:0.016, val_acc:0.982]
Epoch [110/120    avg_loss:0.015, val_acc:0.981]
Epoch [111/120    avg_loss:0.017, val_acc:0.968]
Epoch [112/120    avg_loss:0.016, val_acc:0.970]
Epoch [113/120    avg_loss:0.014, val_acc:0.979]
Epoch [114/120    avg_loss:0.015, val_acc:0.975]
Epoch [115/120    avg_loss:0.018, val_acc:0.976]
Epoch [116/120    avg_loss:0.011, val_acc:0.977]
Epoch [117/120    avg_loss:0.011, val_acc:0.982]
Epoch [118/120    avg_loss:0.011, val_acc:0.983]
Epoch [119/120    avg_loss:0.008, val_acc:0.979]
Epoch [120/120    avg_loss:0.008, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1252    0    0    0    2    0    0    0    7   23    1    0
     0    0    0]
 [   0    0    0  731    0    3    0    0    0    4    0    0    4    2
     3    0    0]
 [   0    0    0    0  212    0    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    2    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    2    1    0    0    0  865    1    0    0
     0    4    0]
 [   0    0    4    0    0    0    1    0    0    0   11 2192    0    2
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    9    4  516    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    0    0    0    0
  1131    1    0]
 [   0    0    0    0    0    1   31    0    0    0    0    0    0    0
     9  306    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.36314363143632

F1 scores:
[       nan 1.         0.98466378 0.98917456 0.99764706 0.97949886
 0.97185185 1.         1.         0.87804878 0.97906055 0.98939291
 0.97819905 0.98930481 0.98993435 0.92586989 0.98823529]

Kappa:
0.981335842012691
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f5566b7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.526, val_acc:0.487]
Epoch [2/120    avg_loss:2.059, val_acc:0.525]
Epoch [3/120    avg_loss:1.796, val_acc:0.593]
Epoch [4/120    avg_loss:1.581, val_acc:0.651]
Epoch [5/120    avg_loss:1.377, val_acc:0.692]
Epoch [6/120    avg_loss:1.159, val_acc:0.673]
Epoch [7/120    avg_loss:1.019, val_acc:0.654]
Epoch [8/120    avg_loss:1.008, val_acc:0.697]
Epoch [9/120    avg_loss:0.778, val_acc:0.681]
Epoch [10/120    avg_loss:0.747, val_acc:0.726]
Epoch [11/120    avg_loss:0.642, val_acc:0.776]
Epoch [12/120    avg_loss:0.601, val_acc:0.772]
Epoch [13/120    avg_loss:0.503, val_acc:0.800]
Epoch [14/120    avg_loss:0.545, val_acc:0.808]
Epoch [15/120    avg_loss:0.469, val_acc:0.797]
Epoch [16/120    avg_loss:0.369, val_acc:0.825]
Epoch [17/120    avg_loss:0.323, val_acc:0.843]
Epoch [18/120    avg_loss:0.385, val_acc:0.865]
Epoch [19/120    avg_loss:0.416, val_acc:0.845]
Epoch [20/120    avg_loss:0.323, val_acc:0.876]
Epoch [21/120    avg_loss:0.268, val_acc:0.866]
Epoch [22/120    avg_loss:0.256, val_acc:0.869]
Epoch [23/120    avg_loss:0.265, val_acc:0.895]
Epoch [24/120    avg_loss:0.251, val_acc:0.878]
Epoch [25/120    avg_loss:0.216, val_acc:0.903]
Epoch [26/120    avg_loss:0.183, val_acc:0.874]
Epoch [27/120    avg_loss:0.218, val_acc:0.895]
Epoch [28/120    avg_loss:0.155, val_acc:0.920]
Epoch [29/120    avg_loss:0.158, val_acc:0.932]
Epoch [30/120    avg_loss:0.154, val_acc:0.915]
Epoch [31/120    avg_loss:0.151, val_acc:0.928]
Epoch [32/120    avg_loss:0.140, val_acc:0.917]
Epoch [33/120    avg_loss:0.135, val_acc:0.941]
Epoch [34/120    avg_loss:0.159, val_acc:0.923]
Epoch [35/120    avg_loss:0.128, val_acc:0.885]
Epoch [36/120    avg_loss:0.111, val_acc:0.933]
Epoch [37/120    avg_loss:0.120, val_acc:0.911]
Epoch [38/120    avg_loss:0.080, val_acc:0.935]
Epoch [39/120    avg_loss:0.072, val_acc:0.955]
Epoch [40/120    avg_loss:0.113, val_acc:0.907]
Epoch [41/120    avg_loss:0.149, val_acc:0.941]
Epoch [42/120    avg_loss:0.172, val_acc:0.927]
Epoch [43/120    avg_loss:0.157, val_acc:0.941]
Epoch [44/120    avg_loss:0.089, val_acc:0.941]
Epoch [45/120    avg_loss:0.078, val_acc:0.956]
Epoch [46/120    avg_loss:0.067, val_acc:0.913]
Epoch [47/120    avg_loss:0.083, val_acc:0.957]
Epoch [48/120    avg_loss:0.062, val_acc:0.948]
Epoch [49/120    avg_loss:0.073, val_acc:0.952]
Epoch [50/120    avg_loss:0.068, val_acc:0.945]
Epoch [51/120    avg_loss:0.051, val_acc:0.935]
Epoch [52/120    avg_loss:0.057, val_acc:0.945]
Epoch [53/120    avg_loss:0.083, val_acc:0.940]
Epoch [54/120    avg_loss:0.058, val_acc:0.955]
Epoch [55/120    avg_loss:0.044, val_acc:0.959]
Epoch [56/120    avg_loss:0.045, val_acc:0.964]
Epoch [57/120    avg_loss:0.036, val_acc:0.959]
Epoch [58/120    avg_loss:0.034, val_acc:0.946]
Epoch [59/120    avg_loss:0.055, val_acc:0.966]
Epoch [60/120    avg_loss:0.047, val_acc:0.967]
Epoch [61/120    avg_loss:0.038, val_acc:0.968]
Epoch [62/120    avg_loss:0.038, val_acc:0.950]
Epoch [63/120    avg_loss:0.067, val_acc:0.963]
Epoch [64/120    avg_loss:0.069, val_acc:0.949]
Epoch [65/120    avg_loss:0.054, val_acc:0.974]
Epoch [66/120    avg_loss:0.045, val_acc:0.959]
Epoch [67/120    avg_loss:0.041, val_acc:0.968]
Epoch [68/120    avg_loss:0.040, val_acc:0.958]
Epoch [69/120    avg_loss:0.038, val_acc:0.968]
Epoch [70/120    avg_loss:0.042, val_acc:0.977]
Epoch [71/120    avg_loss:0.019, val_acc:0.974]
Epoch [72/120    avg_loss:0.025, val_acc:0.974]
Epoch [73/120    avg_loss:0.028, val_acc:0.961]
Epoch [74/120    avg_loss:0.024, val_acc:0.972]
Epoch [75/120    avg_loss:0.025, val_acc:0.975]
Epoch [76/120    avg_loss:0.032, val_acc:0.978]
Epoch [77/120    avg_loss:0.020, val_acc:0.972]
Epoch [78/120    avg_loss:0.052, val_acc:0.966]
Epoch [79/120    avg_loss:0.050, val_acc:0.967]
Epoch [80/120    avg_loss:0.056, val_acc:0.948]
Epoch [81/120    avg_loss:0.068, val_acc:0.963]
Epoch [82/120    avg_loss:0.029, val_acc:0.969]
Epoch [83/120    avg_loss:0.037, val_acc:0.978]
Epoch [84/120    avg_loss:0.028, val_acc:0.976]
Epoch [85/120    avg_loss:0.034, val_acc:0.972]
Epoch [86/120    avg_loss:0.024, val_acc:0.976]
Epoch [87/120    avg_loss:0.026, val_acc:0.978]
Epoch [88/120    avg_loss:0.016, val_acc:0.976]
Epoch [89/120    avg_loss:0.023, val_acc:0.980]
Epoch [90/120    avg_loss:0.017, val_acc:0.981]
Epoch [91/120    avg_loss:0.052, val_acc:0.934]
Epoch [92/120    avg_loss:0.215, val_acc:0.953]
Epoch [93/120    avg_loss:0.144, val_acc:0.964]
Epoch [94/120    avg_loss:0.068, val_acc:0.968]
Epoch [95/120    avg_loss:0.037, val_acc:0.968]
Epoch [96/120    avg_loss:0.030, val_acc:0.957]
Epoch [97/120    avg_loss:0.055, val_acc:0.953]
Epoch [98/120    avg_loss:0.046, val_acc:0.969]
Epoch [99/120    avg_loss:0.036, val_acc:0.974]
Epoch [100/120    avg_loss:0.016, val_acc:0.981]
Epoch [101/120    avg_loss:0.016, val_acc:0.971]
Epoch [102/120    avg_loss:0.019, val_acc:0.981]
Epoch [103/120    avg_loss:0.015, val_acc:0.981]
Epoch [104/120    avg_loss:0.016, val_acc:0.975]
Epoch [105/120    avg_loss:0.014, val_acc:0.969]
Epoch [106/120    avg_loss:0.011, val_acc:0.980]
Epoch [107/120    avg_loss:0.010, val_acc:0.978]
Epoch [108/120    avg_loss:0.017, val_acc:0.978]
Epoch [109/120    avg_loss:0.025, val_acc:0.962]
Epoch [110/120    avg_loss:0.022, val_acc:0.961]
Epoch [111/120    avg_loss:0.029, val_acc:0.963]
Epoch [112/120    avg_loss:0.019, val_acc:0.981]
Epoch [113/120    avg_loss:0.013, val_acc:0.981]
Epoch [114/120    avg_loss:0.015, val_acc:0.976]
Epoch [115/120    avg_loss:0.013, val_acc:0.974]
Epoch [116/120    avg_loss:0.010, val_acc:0.982]
Epoch [117/120    avg_loss:0.008, val_acc:0.982]
Epoch [118/120    avg_loss:0.010, val_acc:0.984]
Epoch [119/120    avg_loss:0.007, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1249    5    0    0    0    0    0    0    9   22    0    0
     0    0    0]
 [   0    0    0  711    9   15    0    0    0    1    2    6    3    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    1    0    0    0    0  868    2    0    0
     0    1    0]
 [   0    0    0    0    0    1    1    0    1    0   16 2190    1    0
     0    0    0]
 [   0    0    0    0    2    0    0    0    0    0   11   11  504    0
     0    1    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    1    9    0    0    0    0    0    0    0
    10  327    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.25474254742548

F1 scores:
[       nan 1.         0.98462751 0.96998636 0.97011494 0.97395243
 0.99092284 1.         0.99767981 0.94444444 0.9747333  0.98626435
 0.96644295 1.         0.99215344 0.96745562 0.96511628]

Kappa:
0.9800963832365374
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f38dfb177f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.531, val_acc:0.449]
Epoch [2/120    avg_loss:2.085, val_acc:0.519]
Epoch [3/120    avg_loss:1.838, val_acc:0.581]
Epoch [4/120    avg_loss:1.692, val_acc:0.624]
Epoch [5/120    avg_loss:1.532, val_acc:0.602]
Epoch [6/120    avg_loss:1.257, val_acc:0.682]
Epoch [7/120    avg_loss:1.033, val_acc:0.708]
Epoch [8/120    avg_loss:0.873, val_acc:0.730]
Epoch [9/120    avg_loss:0.878, val_acc:0.698]
Epoch [10/120    avg_loss:0.739, val_acc:0.755]
Epoch [11/120    avg_loss:0.616, val_acc:0.787]
Epoch [12/120    avg_loss:0.539, val_acc:0.825]
Epoch [13/120    avg_loss:0.511, val_acc:0.748]
Epoch [14/120    avg_loss:0.461, val_acc:0.841]
Epoch [15/120    avg_loss:0.465, val_acc:0.832]
Epoch [16/120    avg_loss:0.451, val_acc:0.820]
Epoch [17/120    avg_loss:0.370, val_acc:0.837]
Epoch [18/120    avg_loss:0.381, val_acc:0.846]
Epoch [19/120    avg_loss:0.306, val_acc:0.887]
Epoch [20/120    avg_loss:0.303, val_acc:0.899]
Epoch [21/120    avg_loss:0.319, val_acc:0.876]
Epoch [22/120    avg_loss:0.232, val_acc:0.883]
Epoch [23/120    avg_loss:0.245, val_acc:0.891]
Epoch [24/120    avg_loss:0.195, val_acc:0.913]
Epoch [25/120    avg_loss:0.215, val_acc:0.900]
Epoch [26/120    avg_loss:0.212, val_acc:0.915]
Epoch [27/120    avg_loss:0.232, val_acc:0.909]
Epoch [28/120    avg_loss:0.171, val_acc:0.920]
Epoch [29/120    avg_loss:0.167, val_acc:0.862]
Epoch [30/120    avg_loss:0.184, val_acc:0.930]
Epoch [31/120    avg_loss:0.182, val_acc:0.930]
Epoch [32/120    avg_loss:0.139, val_acc:0.921]
Epoch [33/120    avg_loss:0.127, val_acc:0.944]
Epoch [34/120    avg_loss:0.109, val_acc:0.939]
Epoch [35/120    avg_loss:0.120, val_acc:0.935]
Epoch [36/120    avg_loss:0.127, val_acc:0.927]
Epoch [37/120    avg_loss:0.127, val_acc:0.950]
Epoch [38/120    avg_loss:0.099, val_acc:0.942]
Epoch [39/120    avg_loss:0.105, val_acc:0.957]
Epoch [40/120    avg_loss:0.113, val_acc:0.955]
Epoch [41/120    avg_loss:0.071, val_acc:0.915]
Epoch [42/120    avg_loss:0.073, val_acc:0.955]
Epoch [43/120    avg_loss:0.126, val_acc:0.927]
Epoch [44/120    avg_loss:0.095, val_acc:0.953]
Epoch [45/120    avg_loss:0.064, val_acc:0.939]
Epoch [46/120    avg_loss:0.053, val_acc:0.948]
Epoch [47/120    avg_loss:0.075, val_acc:0.956]
Epoch [48/120    avg_loss:0.054, val_acc:0.969]
Epoch [49/120    avg_loss:0.075, val_acc:0.957]
Epoch [50/120    avg_loss:0.070, val_acc:0.967]
Epoch [51/120    avg_loss:0.067, val_acc:0.962]
Epoch [52/120    avg_loss:0.062, val_acc:0.953]
Epoch [53/120    avg_loss:0.072, val_acc:0.934]
Epoch [54/120    avg_loss:0.059, val_acc:0.966]
Epoch [55/120    avg_loss:0.051, val_acc:0.974]
Epoch [56/120    avg_loss:0.056, val_acc:0.968]
Epoch [57/120    avg_loss:0.045, val_acc:0.971]
Epoch [58/120    avg_loss:0.040, val_acc:0.964]
Epoch [59/120    avg_loss:0.029, val_acc:0.971]
Epoch [60/120    avg_loss:0.041, val_acc:0.969]
Epoch [61/120    avg_loss:0.044, val_acc:0.970]
Epoch [62/120    avg_loss:0.047, val_acc:0.972]
Epoch [63/120    avg_loss:0.029, val_acc:0.962]
Epoch [64/120    avg_loss:0.047, val_acc:0.970]
Epoch [65/120    avg_loss:0.037, val_acc:0.960]
Epoch [66/120    avg_loss:0.056, val_acc:0.967]
Epoch [67/120    avg_loss:0.061, val_acc:0.923]
Epoch [68/120    avg_loss:0.067, val_acc:0.933]
Epoch [69/120    avg_loss:0.076, val_acc:0.964]
Epoch [70/120    avg_loss:0.038, val_acc:0.978]
Epoch [71/120    avg_loss:0.027, val_acc:0.980]
Epoch [72/120    avg_loss:0.025, val_acc:0.981]
Epoch [73/120    avg_loss:0.025, val_acc:0.978]
Epoch [74/120    avg_loss:0.019, val_acc:0.982]
Epoch [75/120    avg_loss:0.025, val_acc:0.978]
Epoch [76/120    avg_loss:0.022, val_acc:0.981]
Epoch [77/120    avg_loss:0.019, val_acc:0.981]
Epoch [78/120    avg_loss:0.019, val_acc:0.981]
Epoch [79/120    avg_loss:0.020, val_acc:0.980]
Epoch [80/120    avg_loss:0.016, val_acc:0.978]
Epoch [81/120    avg_loss:0.016, val_acc:0.981]
Epoch [82/120    avg_loss:0.017, val_acc:0.981]
Epoch [83/120    avg_loss:0.019, val_acc:0.981]
Epoch [84/120    avg_loss:0.020, val_acc:0.982]
Epoch [85/120    avg_loss:0.018, val_acc:0.981]
Epoch [86/120    avg_loss:0.014, val_acc:0.981]
Epoch [87/120    avg_loss:0.023, val_acc:0.980]
Epoch [88/120    avg_loss:0.015, val_acc:0.981]
Epoch [89/120    avg_loss:0.014, val_acc:0.983]
Epoch [90/120    avg_loss:0.017, val_acc:0.982]
Epoch [91/120    avg_loss:0.016, val_acc:0.982]
Epoch [92/120    avg_loss:0.018, val_acc:0.982]
Epoch [93/120    avg_loss:0.015, val_acc:0.983]
Epoch [94/120    avg_loss:0.013, val_acc:0.980]
Epoch [95/120    avg_loss:0.015, val_acc:0.981]
Epoch [96/120    avg_loss:0.011, val_acc:0.980]
Epoch [97/120    avg_loss:0.019, val_acc:0.981]
Epoch [98/120    avg_loss:0.014, val_acc:0.980]
Epoch [99/120    avg_loss:0.013, val_acc:0.980]
Epoch [100/120    avg_loss:0.013, val_acc:0.981]
Epoch [101/120    avg_loss:0.015, val_acc:0.982]
Epoch [102/120    avg_loss:0.014, val_acc:0.980]
Epoch [103/120    avg_loss:0.013, val_acc:0.981]
Epoch [104/120    avg_loss:0.013, val_acc:0.981]
Epoch [105/120    avg_loss:0.014, val_acc:0.981]
Epoch [106/120    avg_loss:0.013, val_acc:0.982]
Epoch [107/120    avg_loss:0.013, val_acc:0.981]
Epoch [108/120    avg_loss:0.012, val_acc:0.981]
Epoch [109/120    avg_loss:0.013, val_acc:0.981]
Epoch [110/120    avg_loss:0.012, val_acc:0.981]
Epoch [111/120    avg_loss:0.011, val_acc:0.981]
Epoch [112/120    avg_loss:0.013, val_acc:0.981]
Epoch [113/120    avg_loss:0.011, val_acc:0.981]
Epoch [114/120    avg_loss:0.015, val_acc:0.981]
Epoch [115/120    avg_loss:0.012, val_acc:0.981]
Epoch [116/120    avg_loss:0.013, val_acc:0.981]
Epoch [117/120    avg_loss:0.012, val_acc:0.981]
Epoch [118/120    avg_loss:0.014, val_acc:0.981]
Epoch [119/120    avg_loss:0.012, val_acc:0.982]
Epoch [120/120    avg_loss:0.014, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1252    0    0    0    1    0    0    1   13   12    6    0
     0    0    0]
 [   0    0    0  697    2    5    0    0    0   16    2    0   23    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    3    0    2    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    3    0    0    0    0  847   18    0    0
     1    1    0]
 [   0    0    5    0    0    0    1    0    0    0   12 2184    6    2
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0    7  524    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    3    1    0    0
  1127    6    0]
 [   0    0    0    0    0    0    0    0    0    2    0    0    0    0
    59  286    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.42005420054201

F1 scores:
[       nan 0.93506494 0.98311739 0.96337249 0.9953271  0.97926267
 0.99771863 0.94339623 0.99767981 0.60714286 0.96414343 0.98533724
 0.95795247 0.98930481 0.96696697 0.89375    0.98809524]

Kappa:
0.9705753374890383
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f75ccbba828>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.549, val_acc:0.483]
Epoch [2/120    avg_loss:2.080, val_acc:0.550]
Epoch [3/120    avg_loss:1.903, val_acc:0.584]
Epoch [4/120    avg_loss:1.685, val_acc:0.573]
Epoch [5/120    avg_loss:1.519, val_acc:0.627]
Epoch [6/120    avg_loss:1.291, val_acc:0.685]
Epoch [7/120    avg_loss:1.159, val_acc:0.701]
Epoch [8/120    avg_loss:1.052, val_acc:0.734]
Epoch [9/120    avg_loss:0.894, val_acc:0.761]
Epoch [10/120    avg_loss:0.842, val_acc:0.794]
Epoch [11/120    avg_loss:0.744, val_acc:0.753]
Epoch [12/120    avg_loss:0.752, val_acc:0.835]
Epoch [13/120    avg_loss:0.630, val_acc:0.828]
Epoch [14/120    avg_loss:0.605, val_acc:0.786]
Epoch [15/120    avg_loss:0.616, val_acc:0.833]
Epoch [16/120    avg_loss:0.442, val_acc:0.893]
Epoch [17/120    avg_loss:0.469, val_acc:0.878]
Epoch [18/120    avg_loss:0.402, val_acc:0.885]
Epoch [19/120    avg_loss:0.453, val_acc:0.843]
Epoch [20/120    avg_loss:0.360, val_acc:0.871]
Epoch [21/120    avg_loss:0.327, val_acc:0.912]
Epoch [22/120    avg_loss:0.278, val_acc:0.881]
Epoch [23/120    avg_loss:0.294, val_acc:0.912]
Epoch [24/120    avg_loss:0.303, val_acc:0.908]
Epoch [25/120    avg_loss:0.301, val_acc:0.931]
Epoch [26/120    avg_loss:0.254, val_acc:0.894]
Epoch [27/120    avg_loss:0.227, val_acc:0.929]
Epoch [28/120    avg_loss:0.268, val_acc:0.926]
Epoch [29/120    avg_loss:0.241, val_acc:0.905]
Epoch [30/120    avg_loss:0.217, val_acc:0.941]
Epoch [31/120    avg_loss:0.175, val_acc:0.939]
Epoch [32/120    avg_loss:0.168, val_acc:0.942]
Epoch [33/120    avg_loss:0.159, val_acc:0.948]
Epoch [34/120    avg_loss:0.125, val_acc:0.942]
Epoch [35/120    avg_loss:0.121, val_acc:0.936]
Epoch [36/120    avg_loss:0.098, val_acc:0.936]
Epoch [37/120    avg_loss:0.128, val_acc:0.953]
Epoch [38/120    avg_loss:0.116, val_acc:0.952]
Epoch [39/120    avg_loss:0.127, val_acc:0.931]
Epoch [40/120    avg_loss:0.130, val_acc:0.950]
Epoch [41/120    avg_loss:0.111, val_acc:0.963]
Epoch [42/120    avg_loss:0.241, val_acc:0.933]
Epoch [43/120    avg_loss:0.231, val_acc:0.885]
Epoch [44/120    avg_loss:0.238, val_acc:0.919]
Epoch [45/120    avg_loss:0.122, val_acc:0.949]
Epoch [46/120    avg_loss:0.103, val_acc:0.960]
Epoch [47/120    avg_loss:0.091, val_acc:0.945]
Epoch [48/120    avg_loss:0.094, val_acc:0.949]
Epoch [49/120    avg_loss:0.106, val_acc:0.948]
Epoch [50/120    avg_loss:0.080, val_acc:0.967]
Epoch [51/120    avg_loss:0.061, val_acc:0.967]
Epoch [52/120    avg_loss:0.064, val_acc:0.964]
Epoch [53/120    avg_loss:0.064, val_acc:0.956]
Epoch [54/120    avg_loss:0.066, val_acc:0.952]
Epoch [55/120    avg_loss:0.075, val_acc:0.944]
Epoch [56/120    avg_loss:0.064, val_acc:0.949]
Epoch [57/120    avg_loss:0.059, val_acc:0.961]
Epoch [58/120    avg_loss:0.051, val_acc:0.963]
Epoch [59/120    avg_loss:0.100, val_acc:0.939]
Epoch [60/120    avg_loss:0.139, val_acc:0.963]
Epoch [61/120    avg_loss:0.060, val_acc:0.965]
Epoch [62/120    avg_loss:0.057, val_acc:0.958]
Epoch [63/120    avg_loss:0.062, val_acc:0.971]
Epoch [64/120    avg_loss:0.039, val_acc:0.967]
Epoch [65/120    avg_loss:0.039, val_acc:0.973]
Epoch [66/120    avg_loss:0.035, val_acc:0.969]
Epoch [67/120    avg_loss:0.058, val_acc:0.959]
Epoch [68/120    avg_loss:0.043, val_acc:0.967]
Epoch [69/120    avg_loss:0.030, val_acc:0.973]
Epoch [70/120    avg_loss:0.033, val_acc:0.968]
Epoch [71/120    avg_loss:0.028, val_acc:0.972]
Epoch [72/120    avg_loss:0.036, val_acc:0.975]
Epoch [73/120    avg_loss:0.048, val_acc:0.970]
Epoch [74/120    avg_loss:0.030, val_acc:0.975]
Epoch [75/120    avg_loss:0.035, val_acc:0.978]
Epoch [76/120    avg_loss:0.049, val_acc:0.963]
Epoch [77/120    avg_loss:0.038, val_acc:0.972]
Epoch [78/120    avg_loss:0.039, val_acc:0.970]
Epoch [79/120    avg_loss:0.046, val_acc:0.963]
Epoch [80/120    avg_loss:0.058, val_acc:0.974]
Epoch [81/120    avg_loss:0.139, val_acc:0.927]
Epoch [82/120    avg_loss:0.119, val_acc:0.963]
Epoch [83/120    avg_loss:0.055, val_acc:0.970]
Epoch [84/120    avg_loss:0.028, val_acc:0.974]
Epoch [85/120    avg_loss:0.033, val_acc:0.961]
Epoch [86/120    avg_loss:0.043, val_acc:0.975]
Epoch [87/120    avg_loss:0.053, val_acc:0.971]
Epoch [88/120    avg_loss:0.026, val_acc:0.970]
Epoch [89/120    avg_loss:0.026, val_acc:0.974]
Epoch [90/120    avg_loss:0.022, val_acc:0.980]
Epoch [91/120    avg_loss:0.019, val_acc:0.977]
Epoch [92/120    avg_loss:0.018, val_acc:0.979]
Epoch [93/120    avg_loss:0.019, val_acc:0.979]
Epoch [94/120    avg_loss:0.014, val_acc:0.979]
Epoch [95/120    avg_loss:0.017, val_acc:0.979]
Epoch [96/120    avg_loss:0.016, val_acc:0.980]
Epoch [97/120    avg_loss:0.016, val_acc:0.979]
Epoch [98/120    avg_loss:0.016, val_acc:0.981]
Epoch [99/120    avg_loss:0.016, val_acc:0.980]
Epoch [100/120    avg_loss:0.016, val_acc:0.980]
Epoch [101/120    avg_loss:0.017, val_acc:0.980]
Epoch [102/120    avg_loss:0.012, val_acc:0.979]
Epoch [103/120    avg_loss:0.013, val_acc:0.980]
Epoch [104/120    avg_loss:0.015, val_acc:0.980]
Epoch [105/120    avg_loss:0.012, val_acc:0.981]
Epoch [106/120    avg_loss:0.013, val_acc:0.981]
Epoch [107/120    avg_loss:0.018, val_acc:0.981]
Epoch [108/120    avg_loss:0.013, val_acc:0.980]
Epoch [109/120    avg_loss:0.014, val_acc:0.981]
Epoch [110/120    avg_loss:0.015, val_acc:0.981]
Epoch [111/120    avg_loss:0.014, val_acc:0.983]
Epoch [112/120    avg_loss:0.016, val_acc:0.981]
Epoch [113/120    avg_loss:0.012, val_acc:0.982]
Epoch [114/120    avg_loss:0.013, val_acc:0.981]
Epoch [115/120    avg_loss:0.013, val_acc:0.982]
Epoch [116/120    avg_loss:0.012, val_acc:0.982]
Epoch [117/120    avg_loss:0.012, val_acc:0.979]
Epoch [118/120    avg_loss:0.013, val_acc:0.981]
Epoch [119/120    avg_loss:0.014, val_acc:0.981]
Epoch [120/120    avg_loss:0.016, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1248    0    8    0    0    0    0    1   13    9    5    0
     0    1    0]
 [   0    0    0  715    0   15    0    0    0    6    7    0    0    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    2   14    0    5    0    0    0    0  843    8    0    0
     0    3    0]
 [   0    0    4    0    0    0    4    0    2    0   20 2175    0    2
     0    3    0]
 [   0    0    0   10   13    1    0    0    0    0    0   12  495    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    1    1    0    1    1    0    0
  1135    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
   105  241    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.76964769647697

F1 scores:
[       nan 0.94871795 0.9830642  0.96037609 0.95302013 0.96949153
 0.99620925 0.98039216 0.99652375 0.65217391 0.95632445 0.98527746
 0.95744681 0.98404255 0.95418243 0.81008403 0.98245614]

Kappa:
0.9631531509615023
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbfa23538d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.547, val_acc:0.474]
Epoch [2/120    avg_loss:2.071, val_acc:0.497]
Epoch [3/120    avg_loss:1.830, val_acc:0.600]
Epoch [4/120    avg_loss:1.641, val_acc:0.652]
Epoch [5/120    avg_loss:1.448, val_acc:0.656]
Epoch [6/120    avg_loss:1.270, val_acc:0.676]
Epoch [7/120    avg_loss:1.089, val_acc:0.720]
Epoch [8/120    avg_loss:0.919, val_acc:0.744]
Epoch [9/120    avg_loss:0.844, val_acc:0.790]
Epoch [10/120    avg_loss:0.726, val_acc:0.799]
Epoch [11/120    avg_loss:0.749, val_acc:0.808]
Epoch [12/120    avg_loss:0.612, val_acc:0.820]
Epoch [13/120    avg_loss:0.607, val_acc:0.808]
Epoch [14/120    avg_loss:0.585, val_acc:0.840]
Epoch [15/120    avg_loss:0.498, val_acc:0.825]
Epoch [16/120    avg_loss:0.503, val_acc:0.840]
Epoch [17/120    avg_loss:0.474, val_acc:0.807]
Epoch [18/120    avg_loss:0.388, val_acc:0.886]
Epoch [19/120    avg_loss:0.298, val_acc:0.885]
Epoch [20/120    avg_loss:0.322, val_acc:0.885]
Epoch [21/120    avg_loss:0.308, val_acc:0.908]
Epoch [22/120    avg_loss:0.267, val_acc:0.903]
Epoch [23/120    avg_loss:0.260, val_acc:0.883]
Epoch [24/120    avg_loss:0.236, val_acc:0.921]
Epoch [25/120    avg_loss:0.187, val_acc:0.897]
Epoch [26/120    avg_loss:0.231, val_acc:0.905]
Epoch [27/120    avg_loss:0.208, val_acc:0.920]
Epoch [28/120    avg_loss:0.179, val_acc:0.923]
Epoch [29/120    avg_loss:0.146, val_acc:0.936]
Epoch [30/120    avg_loss:0.202, val_acc:0.908]
Epoch [31/120    avg_loss:0.216, val_acc:0.917]
Epoch [32/120    avg_loss:0.221, val_acc:0.916]
Epoch [33/120    avg_loss:0.164, val_acc:0.927]
Epoch [34/120    avg_loss:0.153, val_acc:0.925]
Epoch [35/120    avg_loss:0.135, val_acc:0.948]
Epoch [36/120    avg_loss:0.207, val_acc:0.936]
Epoch [37/120    avg_loss:0.168, val_acc:0.940]
Epoch [38/120    avg_loss:0.154, val_acc:0.941]
Epoch [39/120    avg_loss:0.130, val_acc:0.931]
Epoch [40/120    avg_loss:0.137, val_acc:0.938]
Epoch [41/120    avg_loss:0.110, val_acc:0.941]
Epoch [42/120    avg_loss:0.091, val_acc:0.925]
Epoch [43/120    avg_loss:0.101, val_acc:0.943]
Epoch [44/120    avg_loss:0.111, val_acc:0.943]
Epoch [45/120    avg_loss:0.076, val_acc:0.958]
Epoch [46/120    avg_loss:0.071, val_acc:0.951]
Epoch [47/120    avg_loss:0.053, val_acc:0.950]
Epoch [48/120    avg_loss:0.063, val_acc:0.956]
Epoch [49/120    avg_loss:0.056, val_acc:0.953]
Epoch [50/120    avg_loss:0.056, val_acc:0.968]
Epoch [51/120    avg_loss:0.071, val_acc:0.946]
Epoch [52/120    avg_loss:0.088, val_acc:0.961]
Epoch [53/120    avg_loss:0.049, val_acc:0.971]
Epoch [54/120    avg_loss:0.040, val_acc:0.964]
Epoch [55/120    avg_loss:0.101, val_acc:0.953]
Epoch [56/120    avg_loss:0.047, val_acc:0.965]
Epoch [57/120    avg_loss:0.069, val_acc:0.944]
Epoch [58/120    avg_loss:0.052, val_acc:0.972]
Epoch [59/120    avg_loss:0.059, val_acc:0.959]
Epoch [60/120    avg_loss:0.062, val_acc:0.970]
Epoch [61/120    avg_loss:0.040, val_acc:0.963]
Epoch [62/120    avg_loss:0.062, val_acc:0.942]
Epoch [63/120    avg_loss:0.051, val_acc:0.971]
Epoch [64/120    avg_loss:0.039, val_acc:0.932]
Epoch [65/120    avg_loss:0.044, val_acc:0.970]
Epoch [66/120    avg_loss:0.049, val_acc:0.956]
Epoch [67/120    avg_loss:0.048, val_acc:0.949]
Epoch [68/120    avg_loss:0.087, val_acc:0.956]
Epoch [69/120    avg_loss:0.093, val_acc:0.958]
Epoch [70/120    avg_loss:0.050, val_acc:0.961]
Epoch [71/120    avg_loss:0.041, val_acc:0.958]
Epoch [72/120    avg_loss:0.051, val_acc:0.967]
Epoch [73/120    avg_loss:0.028, val_acc:0.973]
Epoch [74/120    avg_loss:0.025, val_acc:0.974]
Epoch [75/120    avg_loss:0.023, val_acc:0.973]
Epoch [76/120    avg_loss:0.023, val_acc:0.974]
Epoch [77/120    avg_loss:0.024, val_acc:0.971]
Epoch [78/120    avg_loss:0.021, val_acc:0.975]
Epoch [79/120    avg_loss:0.024, val_acc:0.979]
Epoch [80/120    avg_loss:0.021, val_acc:0.980]
Epoch [81/120    avg_loss:0.020, val_acc:0.975]
Epoch [82/120    avg_loss:0.023, val_acc:0.975]
Epoch [83/120    avg_loss:0.020, val_acc:0.978]
Epoch [84/120    avg_loss:0.021, val_acc:0.973]
Epoch [85/120    avg_loss:0.017, val_acc:0.973]
Epoch [86/120    avg_loss:0.018, val_acc:0.973]
Epoch [87/120    avg_loss:0.024, val_acc:0.974]
Epoch [88/120    avg_loss:0.017, val_acc:0.978]
Epoch [89/120    avg_loss:0.020, val_acc:0.978]
Epoch [90/120    avg_loss:0.018, val_acc:0.975]
Epoch [91/120    avg_loss:0.020, val_acc:0.978]
Epoch [92/120    avg_loss:0.020, val_acc:0.979]
Epoch [93/120    avg_loss:0.018, val_acc:0.977]
Epoch [94/120    avg_loss:0.016, val_acc:0.977]
Epoch [95/120    avg_loss:0.016, val_acc:0.977]
Epoch [96/120    avg_loss:0.018, val_acc:0.975]
Epoch [97/120    avg_loss:0.023, val_acc:0.977]
Epoch [98/120    avg_loss:0.021, val_acc:0.978]
Epoch [99/120    avg_loss:0.018, val_acc:0.978]
Epoch [100/120    avg_loss:0.017, val_acc:0.978]
Epoch [101/120    avg_loss:0.019, val_acc:0.977]
Epoch [102/120    avg_loss:0.020, val_acc:0.978]
Epoch [103/120    avg_loss:0.018, val_acc:0.978]
Epoch [104/120    avg_loss:0.017, val_acc:0.978]
Epoch [105/120    avg_loss:0.021, val_acc:0.978]
Epoch [106/120    avg_loss:0.017, val_acc:0.978]
Epoch [107/120    avg_loss:0.017, val_acc:0.978]
Epoch [108/120    avg_loss:0.014, val_acc:0.978]
Epoch [109/120    avg_loss:0.019, val_acc:0.978]
Epoch [110/120    avg_loss:0.020, val_acc:0.978]
Epoch [111/120    avg_loss:0.019, val_acc:0.978]
Epoch [112/120    avg_loss:0.017, val_acc:0.978]
Epoch [113/120    avg_loss:0.014, val_acc:0.978]
Epoch [114/120    avg_loss:0.016, val_acc:0.978]
Epoch [115/120    avg_loss:0.018, val_acc:0.978]
Epoch [116/120    avg_loss:0.026, val_acc:0.978]
Epoch [117/120    avg_loss:0.020, val_acc:0.978]
Epoch [118/120    avg_loss:0.017, val_acc:0.978]
Epoch [119/120    avg_loss:0.018, val_acc:0.978]
Epoch [120/120    avg_loss:0.016, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1257    0    0    0    0    0    0    1    6   16    3    0
     0    2    0]
 [   0    0    1  731    0    3    0    0    0    5    1    0    3    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   30   47    0    5    0    0    0    0  776    9    0    0
     0    8    0]
 [   0    0    4    0    0    1    2    0    3    0   17 2171   10    2
     0    0    0]
 [   0    0    0    2    6    3    0    0    0    0    0    5  515    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    2    0    3    1    0    0
  1131    0    0]
 [   0    0    0    0    0    0    8    0    0    3    0    0    0    0
   105  231    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.34688346883469

F1 scores:
[       nan 0.975      0.97555297 0.95743287 0.98611111 0.97605473
 0.99015897 0.98039216 0.99421965 0.70588235 0.92380952 0.98346546
 0.96713615 0.98666667 0.95242105 0.78571429 0.98245614]

Kappa:
0.9583255242355008
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:06:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa2405f77f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.572, val_acc:0.466]
Epoch [2/120    avg_loss:2.129, val_acc:0.534]
Epoch [3/120    avg_loss:1.865, val_acc:0.575]
Epoch [4/120    avg_loss:1.677, val_acc:0.630]
Epoch [5/120    avg_loss:1.458, val_acc:0.627]
Epoch [6/120    avg_loss:1.274, val_acc:0.684]
Epoch [7/120    avg_loss:1.044, val_acc:0.708]
Epoch [8/120    avg_loss:0.898, val_acc:0.740]
Epoch [9/120    avg_loss:0.882, val_acc:0.760]
Epoch [10/120    avg_loss:0.740, val_acc:0.760]
Epoch [11/120    avg_loss:0.745, val_acc:0.799]
Epoch [12/120    avg_loss:0.622, val_acc:0.757]
Epoch [13/120    avg_loss:0.644, val_acc:0.838]
Epoch [14/120    avg_loss:0.508, val_acc:0.819]
Epoch [15/120    avg_loss:0.479, val_acc:0.827]
Epoch [16/120    avg_loss:0.419, val_acc:0.840]
Epoch [17/120    avg_loss:0.380, val_acc:0.845]
Epoch [18/120    avg_loss:0.535, val_acc:0.819]
Epoch [19/120    avg_loss:0.405, val_acc:0.874]
Epoch [20/120    avg_loss:0.393, val_acc:0.810]
Epoch [21/120    avg_loss:0.490, val_acc:0.820]
Epoch [22/120    avg_loss:0.327, val_acc:0.866]
Epoch [23/120    avg_loss:0.337, val_acc:0.840]
Epoch [24/120    avg_loss:0.260, val_acc:0.885]
Epoch [25/120    avg_loss:0.296, val_acc:0.902]
Epoch [26/120    avg_loss:0.219, val_acc:0.882]
Epoch [27/120    avg_loss:0.205, val_acc:0.906]
Epoch [28/120    avg_loss:0.233, val_acc:0.884]
Epoch [29/120    avg_loss:0.187, val_acc:0.912]
Epoch [30/120    avg_loss:0.158, val_acc:0.894]
Epoch [31/120    avg_loss:0.224, val_acc:0.891]
Epoch [32/120    avg_loss:0.185, val_acc:0.910]
Epoch [33/120    avg_loss:0.141, val_acc:0.932]
Epoch [34/120    avg_loss:0.173, val_acc:0.887]
Epoch [35/120    avg_loss:0.192, val_acc:0.918]
Epoch [36/120    avg_loss:0.162, val_acc:0.919]
Epoch [37/120    avg_loss:0.120, val_acc:0.925]
Epoch [38/120    avg_loss:0.121, val_acc:0.925]
Epoch [39/120    avg_loss:0.104, val_acc:0.919]
Epoch [40/120    avg_loss:0.095, val_acc:0.948]
Epoch [41/120    avg_loss:0.103, val_acc:0.928]
Epoch [42/120    avg_loss:0.105, val_acc:0.920]
Epoch [43/120    avg_loss:0.092, val_acc:0.941]
Epoch [44/120    avg_loss:0.084, val_acc:0.945]
Epoch [45/120    avg_loss:0.073, val_acc:0.935]
Epoch [46/120    avg_loss:0.092, val_acc:0.894]
Epoch [47/120    avg_loss:0.123, val_acc:0.945]
Epoch [48/120    avg_loss:0.075, val_acc:0.943]
Epoch [49/120    avg_loss:0.063, val_acc:0.952]
Epoch [50/120    avg_loss:0.069, val_acc:0.936]
Epoch [51/120    avg_loss:0.057, val_acc:0.954]
Epoch [52/120    avg_loss:0.053, val_acc:0.958]
Epoch [53/120    avg_loss:0.059, val_acc:0.939]
Epoch [54/120    avg_loss:0.070, val_acc:0.953]
Epoch [55/120    avg_loss:0.062, val_acc:0.953]
Epoch [56/120    avg_loss:0.060, val_acc:0.948]
Epoch [57/120    avg_loss:0.048, val_acc:0.954]
Epoch [58/120    avg_loss:0.113, val_acc:0.923]
Epoch [59/120    avg_loss:0.098, val_acc:0.955]
Epoch [60/120    avg_loss:0.061, val_acc:0.958]
Epoch [61/120    avg_loss:0.057, val_acc:0.954]
Epoch [62/120    avg_loss:0.087, val_acc:0.912]
Epoch [63/120    avg_loss:0.097, val_acc:0.938]
Epoch [64/120    avg_loss:0.072, val_acc:0.947]
Epoch [65/120    avg_loss:0.088, val_acc:0.929]
Epoch [66/120    avg_loss:0.096, val_acc:0.945]
Epoch [67/120    avg_loss:0.087, val_acc:0.941]
Epoch [68/120    avg_loss:0.065, val_acc:0.942]
Epoch [69/120    avg_loss:0.063, val_acc:0.948]
Epoch [70/120    avg_loss:0.056, val_acc:0.953]
Epoch [71/120    avg_loss:0.068, val_acc:0.947]
Epoch [72/120    avg_loss:0.064, val_acc:0.957]
Epoch [73/120    avg_loss:0.053, val_acc:0.962]
Epoch [74/120    avg_loss:0.035, val_acc:0.961]
Epoch [75/120    avg_loss:0.032, val_acc:0.963]
Epoch [76/120    avg_loss:0.031, val_acc:0.956]
Epoch [77/120    avg_loss:0.046, val_acc:0.961]
Epoch [78/120    avg_loss:0.047, val_acc:0.967]
Epoch [79/120    avg_loss:0.034, val_acc:0.965]
Epoch [80/120    avg_loss:0.032, val_acc:0.957]
Epoch [81/120    avg_loss:0.024, val_acc:0.962]
Epoch [82/120    avg_loss:0.033, val_acc:0.959]
Epoch [83/120    avg_loss:0.030, val_acc:0.958]
Epoch [84/120    avg_loss:0.026, val_acc:0.961]
Epoch [85/120    avg_loss:0.024, val_acc:0.968]
Epoch [86/120    avg_loss:0.019, val_acc:0.959]
Epoch [87/120    avg_loss:0.023, val_acc:0.965]
Epoch [88/120    avg_loss:0.020, val_acc:0.967]
Epoch [89/120    avg_loss:0.021, val_acc:0.962]
Epoch [90/120    avg_loss:0.017, val_acc:0.968]
Epoch [91/120    avg_loss:0.019, val_acc:0.967]
Epoch [92/120    avg_loss:0.019, val_acc:0.968]
Epoch [93/120    avg_loss:0.016, val_acc:0.964]
Epoch [94/120    avg_loss:0.023, val_acc:0.967]
Epoch [95/120    avg_loss:0.022, val_acc:0.963]
Epoch [96/120    avg_loss:0.015, val_acc:0.973]
Epoch [97/120    avg_loss:0.015, val_acc:0.970]
Epoch [98/120    avg_loss:0.016, val_acc:0.962]
Epoch [99/120    avg_loss:0.015, val_acc:0.967]
Epoch [100/120    avg_loss:0.012, val_acc:0.969]
Epoch [101/120    avg_loss:0.013, val_acc:0.968]
Epoch [102/120    avg_loss:0.015, val_acc:0.975]
Epoch [103/120    avg_loss:0.013, val_acc:0.973]
Epoch [104/120    avg_loss:0.011, val_acc:0.969]
Epoch [105/120    avg_loss:0.010, val_acc:0.969]
Epoch [106/120    avg_loss:0.014, val_acc:0.968]
Epoch [107/120    avg_loss:0.016, val_acc:0.974]
Epoch [108/120    avg_loss:0.012, val_acc:0.976]
Epoch [109/120    avg_loss:0.013, val_acc:0.975]
Epoch [110/120    avg_loss:0.009, val_acc:0.975]
Epoch [111/120    avg_loss:0.011, val_acc:0.975]
Epoch [112/120    avg_loss:0.045, val_acc:0.957]
Epoch [113/120    avg_loss:0.029, val_acc:0.951]
Epoch [114/120    avg_loss:0.030, val_acc:0.974]
Epoch [115/120    avg_loss:0.020, val_acc:0.967]
Epoch [116/120    avg_loss:0.013, val_acc:0.973]
Epoch [117/120    avg_loss:0.014, val_acc:0.971]
Epoch [118/120    avg_loss:0.013, val_acc:0.970]
Epoch [119/120    avg_loss:0.024, val_acc:0.945]
Epoch [120/120    avg_loss:0.026, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1270    2    1    0    5    0    0    0    3    4    0    0
     0    0    0]
 [   0    0    0  727    5    4    0    0    0    7    0    0    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    1    0    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   46   45    0    6    3    0    0    0  753   10    0    0
     3    9    0]
 [   0    0   65    3    0    1   20    0    0    0   26 2094    0    0
     1    0    0]
 [   0    0   19   20    7    5    0    0    0    0    0   10  464    0
     2    1    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    0    0    0    0
  1132    0    0]
 [   0    0    0    0    0    0   40    0    0    0    0    0    0    0
    70  237    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.90514905149051

F1 scores:
[       nan 0.98765432 0.94599628 0.94049159 0.97038724 0.96723164
 0.95010846 1.         0.99883856 0.68085106 0.90832328 0.9676525
 0.9261477  1.         0.96463571 0.7979798  0.96551724]

Kappa:
0.9419396141991646
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6b71adf860>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.541, val_acc:0.468]
Epoch [2/120    avg_loss:2.092, val_acc:0.514]
Epoch [3/120    avg_loss:1.848, val_acc:0.555]
Epoch [4/120    avg_loss:1.663, val_acc:0.592]
Epoch [5/120    avg_loss:1.471, val_acc:0.645]
Epoch [6/120    avg_loss:1.305, val_acc:0.688]
Epoch [7/120    avg_loss:1.098, val_acc:0.706]
Epoch [8/120    avg_loss:1.013, val_acc:0.758]
Epoch [9/120    avg_loss:0.857, val_acc:0.737]
Epoch [10/120    avg_loss:0.755, val_acc:0.740]
Epoch [11/120    avg_loss:0.731, val_acc:0.708]
Epoch [12/120    avg_loss:0.671, val_acc:0.768]
Epoch [13/120    avg_loss:0.633, val_acc:0.762]
Epoch [14/120    avg_loss:0.639, val_acc:0.795]
Epoch [15/120    avg_loss:0.538, val_acc:0.775]
Epoch [16/120    avg_loss:0.620, val_acc:0.768]
Epoch [17/120    avg_loss:0.485, val_acc:0.810]
Epoch [18/120    avg_loss:0.575, val_acc:0.789]
Epoch [19/120    avg_loss:0.411, val_acc:0.850]
Epoch [20/120    avg_loss:0.375, val_acc:0.846]
Epoch [21/120    avg_loss:0.422, val_acc:0.836]
Epoch [22/120    avg_loss:0.388, val_acc:0.851]
Epoch [23/120    avg_loss:0.342, val_acc:0.838]
Epoch [24/120    avg_loss:0.284, val_acc:0.895]
Epoch [25/120    avg_loss:0.317, val_acc:0.823]
Epoch [26/120    avg_loss:0.408, val_acc:0.878]
Epoch [27/120    avg_loss:0.350, val_acc:0.882]
Epoch [28/120    avg_loss:0.286, val_acc:0.880]
Epoch [29/120    avg_loss:0.216, val_acc:0.874]
Epoch [30/120    avg_loss:0.236, val_acc:0.876]
Epoch [31/120    avg_loss:0.245, val_acc:0.905]
Epoch [32/120    avg_loss:0.217, val_acc:0.877]
Epoch [33/120    avg_loss:0.212, val_acc:0.907]
Epoch [34/120    avg_loss:0.192, val_acc:0.878]
Epoch [35/120    avg_loss:0.204, val_acc:0.911]
Epoch [36/120    avg_loss:0.150, val_acc:0.918]
Epoch [37/120    avg_loss:0.153, val_acc:0.925]
Epoch [38/120    avg_loss:0.135, val_acc:0.909]
Epoch [39/120    avg_loss:0.208, val_acc:0.904]
Epoch [40/120    avg_loss:0.182, val_acc:0.919]
Epoch [41/120    avg_loss:0.169, val_acc:0.921]
Epoch [42/120    avg_loss:0.133, val_acc:0.928]
Epoch [43/120    avg_loss:0.138, val_acc:0.924]
Epoch [44/120    avg_loss:0.107, val_acc:0.934]
Epoch [45/120    avg_loss:0.104, val_acc:0.921]
Epoch [46/120    avg_loss:0.122, val_acc:0.916]
Epoch [47/120    avg_loss:0.160, val_acc:0.923]
Epoch [48/120    avg_loss:0.123, val_acc:0.910]
Epoch [49/120    avg_loss:0.114, val_acc:0.934]
Epoch [50/120    avg_loss:0.106, val_acc:0.919]
Epoch [51/120    avg_loss:0.116, val_acc:0.927]
Epoch [52/120    avg_loss:0.087, val_acc:0.931]
Epoch [53/120    avg_loss:0.131, val_acc:0.928]
Epoch [54/120    avg_loss:0.302, val_acc:0.912]
Epoch [55/120    avg_loss:0.131, val_acc:0.921]
Epoch [56/120    avg_loss:0.098, val_acc:0.935]
Epoch [57/120    avg_loss:0.076, val_acc:0.931]
Epoch [58/120    avg_loss:0.097, val_acc:0.925]
Epoch [59/120    avg_loss:0.077, val_acc:0.934]
Epoch [60/120    avg_loss:0.070, val_acc:0.932]
Epoch [61/120    avg_loss:0.071, val_acc:0.945]
Epoch [62/120    avg_loss:0.092, val_acc:0.925]
Epoch [63/120    avg_loss:0.103, val_acc:0.918]
Epoch [64/120    avg_loss:0.079, val_acc:0.929]
Epoch [65/120    avg_loss:0.066, val_acc:0.942]
Epoch [66/120    avg_loss:0.055, val_acc:0.953]
Epoch [67/120    avg_loss:0.052, val_acc:0.945]
Epoch [68/120    avg_loss:0.084, val_acc:0.940]
Epoch [69/120    avg_loss:0.057, val_acc:0.947]
Epoch [70/120    avg_loss:0.058, val_acc:0.932]
Epoch [71/120    avg_loss:0.068, val_acc:0.948]
Epoch [72/120    avg_loss:0.052, val_acc:0.929]
Epoch [73/120    avg_loss:0.043, val_acc:0.951]
Epoch [74/120    avg_loss:0.043, val_acc:0.956]
Epoch [75/120    avg_loss:0.054, val_acc:0.950]
Epoch [76/120    avg_loss:0.042, val_acc:0.953]
Epoch [77/120    avg_loss:0.040, val_acc:0.956]
Epoch [78/120    avg_loss:0.032, val_acc:0.947]
Epoch [79/120    avg_loss:0.050, val_acc:0.958]
Epoch [80/120    avg_loss:0.061, val_acc:0.955]
Epoch [81/120    avg_loss:0.057, val_acc:0.955]
Epoch [82/120    avg_loss:0.044, val_acc:0.955]
Epoch [83/120    avg_loss:0.052, val_acc:0.934]
Epoch [84/120    avg_loss:0.036, val_acc:0.954]
Epoch [85/120    avg_loss:0.031, val_acc:0.945]
Epoch [86/120    avg_loss:0.050, val_acc:0.951]
Epoch [87/120    avg_loss:0.039, val_acc:0.952]
Epoch [88/120    avg_loss:0.030, val_acc:0.956]
Epoch [89/120    avg_loss:0.046, val_acc:0.957]
Epoch [90/120    avg_loss:0.033, val_acc:0.956]
Epoch [91/120    avg_loss:0.024, val_acc:0.959]
Epoch [92/120    avg_loss:0.040, val_acc:0.967]
Epoch [93/120    avg_loss:0.033, val_acc:0.967]
Epoch [94/120    avg_loss:0.035, val_acc:0.956]
Epoch [95/120    avg_loss:0.030, val_acc:0.954]
Epoch [96/120    avg_loss:0.027, val_acc:0.967]
Epoch [97/120    avg_loss:0.028, val_acc:0.967]
Epoch [98/120    avg_loss:0.020, val_acc:0.970]
Epoch [99/120    avg_loss:0.019, val_acc:0.973]
Epoch [100/120    avg_loss:0.019, val_acc:0.966]
Epoch [101/120    avg_loss:0.023, val_acc:0.961]
Epoch [102/120    avg_loss:0.025, val_acc:0.969]
Epoch [103/120    avg_loss:0.022, val_acc:0.974]
Epoch [104/120    avg_loss:0.026, val_acc:0.965]
Epoch [105/120    avg_loss:0.020, val_acc:0.970]
Epoch [106/120    avg_loss:0.021, val_acc:0.965]
Epoch [107/120    avg_loss:0.021, val_acc:0.967]
Epoch [108/120    avg_loss:0.016, val_acc:0.969]
Epoch [109/120    avg_loss:0.019, val_acc:0.971]
Epoch [110/120    avg_loss:0.021, val_acc:0.973]
Epoch [111/120    avg_loss:0.037, val_acc:0.974]
Epoch [112/120    avg_loss:0.034, val_acc:0.959]
Epoch [113/120    avg_loss:0.038, val_acc:0.861]
Epoch [114/120    avg_loss:0.108, val_acc:0.955]
Epoch [115/120    avg_loss:0.048, val_acc:0.954]
Epoch [116/120    avg_loss:0.031, val_acc:0.954]
Epoch [117/120    avg_loss:0.031, val_acc:0.951]
Epoch [118/120    avg_loss:0.029, val_acc:0.958]
Epoch [119/120    avg_loss:0.021, val_acc:0.969]
Epoch [120/120    avg_loss:0.019, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1246    2    0    0    0    0    0    1    7   10   17    0
     0    2    0]
 [   0    0    0  687    8   26    0    0    0   15    2    0    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3   31    0    6    0    0    0    0  814   13    0    0
     3    5    0]
 [   0    0   13    0    0    0    4    0    4    0   14 2171    2    2
     0    0    0]
 [   0    0    2   14    6    8    0    0    0    1    1   16  473    0
     0    6    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    2    0    0   16    0    0    1    0    3    3    0    0
  1114    0    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    65  276    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.03252032520325

F1 scores:
[       nan 0.93506494 0.97687181 0.92775152 0.96818182 0.93837838
 0.99244713 1.         0.99188876 0.66666667 0.94596165 0.98168664
 0.91049086 0.99462366 0.95993106 0.86792453 0.94797688]

Kappa:
0.9547694501880276
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f642c39b860>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.590, val_acc:0.548]
Epoch [2/120    avg_loss:2.096, val_acc:0.553]
Epoch [3/120    avg_loss:1.878, val_acc:0.610]
Epoch [4/120    avg_loss:1.674, val_acc:0.661]
Epoch [5/120    avg_loss:1.469, val_acc:0.696]
Epoch [6/120    avg_loss:1.285, val_acc:0.683]
Epoch [7/120    avg_loss:1.094, val_acc:0.723]
Epoch [8/120    avg_loss:0.962, val_acc:0.763]
Epoch [9/120    avg_loss:0.865, val_acc:0.782]
Epoch [10/120    avg_loss:0.726, val_acc:0.806]
Epoch [11/120    avg_loss:0.688, val_acc:0.820]
Epoch [12/120    avg_loss:0.630, val_acc:0.780]
Epoch [13/120    avg_loss:0.614, val_acc:0.816]
Epoch [14/120    avg_loss:0.542, val_acc:0.852]
Epoch [15/120    avg_loss:0.542, val_acc:0.805]
Epoch [16/120    avg_loss:0.581, val_acc:0.828]
Epoch [17/120    avg_loss:0.470, val_acc:0.843]
Epoch [18/120    avg_loss:0.414, val_acc:0.859]
Epoch [19/120    avg_loss:0.348, val_acc:0.859]
Epoch [20/120    avg_loss:0.359, val_acc:0.889]
Epoch [21/120    avg_loss:0.293, val_acc:0.897]
Epoch [22/120    avg_loss:0.304, val_acc:0.864]
Epoch [23/120    avg_loss:0.282, val_acc:0.884]
Epoch [24/120    avg_loss:0.250, val_acc:0.895]
Epoch [25/120    avg_loss:0.253, val_acc:0.876]
Epoch [26/120    avg_loss:0.265, val_acc:0.890]
Epoch [27/120    avg_loss:0.259, val_acc:0.908]
Epoch [28/120    avg_loss:0.172, val_acc:0.928]
Epoch [29/120    avg_loss:0.197, val_acc:0.911]
Epoch [30/120    avg_loss:0.186, val_acc:0.919]
Epoch [31/120    avg_loss:0.206, val_acc:0.923]
Epoch [32/120    avg_loss:0.148, val_acc:0.921]
Epoch [33/120    avg_loss:0.157, val_acc:0.906]
Epoch [34/120    avg_loss:0.169, val_acc:0.925]
Epoch [35/120    avg_loss:0.167, val_acc:0.934]
Epoch [36/120    avg_loss:0.172, val_acc:0.933]
Epoch [37/120    avg_loss:0.157, val_acc:0.938]
Epoch [38/120    avg_loss:0.112, val_acc:0.940]
Epoch [39/120    avg_loss:0.112, val_acc:0.927]
Epoch [40/120    avg_loss:0.097, val_acc:0.947]
Epoch [41/120    avg_loss:0.101, val_acc:0.939]
Epoch [42/120    avg_loss:0.099, val_acc:0.953]
Epoch [43/120    avg_loss:0.101, val_acc:0.943]
Epoch [44/120    avg_loss:0.131, val_acc:0.928]
Epoch [45/120    avg_loss:0.143, val_acc:0.944]
Epoch [46/120    avg_loss:0.123, val_acc:0.942]
Epoch [47/120    avg_loss:0.097, val_acc:0.946]
Epoch [48/120    avg_loss:0.109, val_acc:0.942]
Epoch [49/120    avg_loss:0.071, val_acc:0.946]
Epoch [50/120    avg_loss:0.094, val_acc:0.940]
Epoch [51/120    avg_loss:0.098, val_acc:0.935]
Epoch [52/120    avg_loss:0.087, val_acc:0.954]
Epoch [53/120    avg_loss:0.076, val_acc:0.946]
Epoch [54/120    avg_loss:0.059, val_acc:0.954]
Epoch [55/120    avg_loss:0.054, val_acc:0.958]
Epoch [56/120    avg_loss:0.057, val_acc:0.954]
Epoch [57/120    avg_loss:0.056, val_acc:0.938]
Epoch [58/120    avg_loss:0.206, val_acc:0.936]
Epoch [59/120    avg_loss:0.119, val_acc:0.931]
Epoch [60/120    avg_loss:0.101, val_acc:0.951]
Epoch [61/120    avg_loss:0.088, val_acc:0.959]
Epoch [62/120    avg_loss:0.052, val_acc:0.964]
Epoch [63/120    avg_loss:0.051, val_acc:0.953]
Epoch [64/120    avg_loss:0.036, val_acc:0.965]
Epoch [65/120    avg_loss:0.046, val_acc:0.958]
Epoch [66/120    avg_loss:0.056, val_acc:0.951]
Epoch [67/120    avg_loss:0.039, val_acc:0.968]
Epoch [68/120    avg_loss:0.041, val_acc:0.963]
Epoch [69/120    avg_loss:0.045, val_acc:0.946]
Epoch [70/120    avg_loss:0.097, val_acc:0.942]
Epoch [71/120    avg_loss:0.080, val_acc:0.955]
Epoch [72/120    avg_loss:0.143, val_acc:0.945]
Epoch [73/120    avg_loss:0.076, val_acc:0.957]
Epoch [74/120    avg_loss:0.074, val_acc:0.957]
Epoch [75/120    avg_loss:0.036, val_acc:0.954]
Epoch [76/120    avg_loss:0.044, val_acc:0.969]
Epoch [77/120    avg_loss:0.056, val_acc:0.939]
Epoch [78/120    avg_loss:0.062, val_acc:0.964]
Epoch [79/120    avg_loss:0.034, val_acc:0.953]
Epoch [80/120    avg_loss:0.054, val_acc:0.964]
Epoch [81/120    avg_loss:0.048, val_acc:0.935]
Epoch [82/120    avg_loss:0.032, val_acc:0.977]
Epoch [83/120    avg_loss:0.027, val_acc:0.967]
Epoch [84/120    avg_loss:0.032, val_acc:0.961]
Epoch [85/120    avg_loss:0.039, val_acc:0.958]
Epoch [86/120    avg_loss:0.023, val_acc:0.970]
Epoch [87/120    avg_loss:0.018, val_acc:0.978]
Epoch [88/120    avg_loss:0.023, val_acc:0.979]
Epoch [89/120    avg_loss:0.017, val_acc:0.980]
Epoch [90/120    avg_loss:0.018, val_acc:0.975]
Epoch [91/120    avg_loss:0.025, val_acc:0.970]
Epoch [92/120    avg_loss:0.023, val_acc:0.971]
Epoch [93/120    avg_loss:0.020, val_acc:0.982]
Epoch [94/120    avg_loss:0.018, val_acc:0.976]
Epoch [95/120    avg_loss:0.017, val_acc:0.980]
Epoch [96/120    avg_loss:0.012, val_acc:0.978]
Epoch [97/120    avg_loss:0.020, val_acc:0.967]
Epoch [98/120    avg_loss:0.032, val_acc:0.964]
Epoch [99/120    avg_loss:0.025, val_acc:0.980]
Epoch [100/120    avg_loss:0.012, val_acc:0.987]
Epoch [101/120    avg_loss:0.012, val_acc:0.976]
Epoch [102/120    avg_loss:0.014, val_acc:0.977]
Epoch [103/120    avg_loss:0.011, val_acc:0.986]
Epoch [104/120    avg_loss:0.010, val_acc:0.982]
Epoch [105/120    avg_loss:0.019, val_acc:0.973]
Epoch [106/120    avg_loss:0.016, val_acc:0.986]
Epoch [107/120    avg_loss:0.012, val_acc:0.979]
Epoch [108/120    avg_loss:0.017, val_acc:0.980]
Epoch [109/120    avg_loss:0.008, val_acc:0.979]
Epoch [110/120    avg_loss:0.010, val_acc:0.971]
Epoch [111/120    avg_loss:0.020, val_acc:0.966]
Epoch [112/120    avg_loss:0.015, val_acc:0.975]
Epoch [113/120    avg_loss:0.012, val_acc:0.976]
Epoch [114/120    avg_loss:0.010, val_acc:0.979]
Epoch [115/120    avg_loss:0.008, val_acc:0.978]
Epoch [116/120    avg_loss:0.011, val_acc:0.980]
Epoch [117/120    avg_loss:0.010, val_acc:0.982]
Epoch [118/120    avg_loss:0.009, val_acc:0.982]
Epoch [119/120    avg_loss:0.009, val_acc:0.985]
Epoch [120/120    avg_loss:0.006, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1253    0    0    2    0    0    0    0    8   20    1    0
     0    1    0]
 [   0    0    0  707   14   12    0    0    0    1    0    0   12    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   14    0    0    4    0
     0    0    0]
 [   0    0    1   22    0    4    0    0    0    0  831   10    0    0
     0    7    0]
 [   0    0    4    0    0    0    1    0    0    0   15 2188    0    2
     0    0    0]
 [   0    0    3   26    3    0    0    0    0    0    1    3  497    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    69  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.19241192411924

F1 scores:
[       nan 0.98765432 0.98428908 0.94141145 0.96162528 0.97627119
 0.99771516 1.         0.99883856 0.77777778 0.95847751 0.98714189
 0.94756911 0.99462366 0.96843003 0.87835703 0.98809524]

Kappa:
0.967972078083746
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1771f6c828>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.578, val_acc:0.485]
Epoch [2/120    avg_loss:2.133, val_acc:0.514]
Epoch [3/120    avg_loss:1.873, val_acc:0.578]
Epoch [4/120    avg_loss:1.616, val_acc:0.595]
Epoch [5/120    avg_loss:1.410, val_acc:0.600]
Epoch [6/120    avg_loss:1.264, val_acc:0.651]
Epoch [7/120    avg_loss:1.016, val_acc:0.719]
Epoch [8/120    avg_loss:0.982, val_acc:0.701]
Epoch [9/120    avg_loss:0.794, val_acc:0.766]
Epoch [10/120    avg_loss:0.733, val_acc:0.724]
Epoch [11/120    avg_loss:0.734, val_acc:0.773]
Epoch [12/120    avg_loss:0.630, val_acc:0.788]
Epoch [13/120    avg_loss:0.608, val_acc:0.780]
Epoch [14/120    avg_loss:0.612, val_acc:0.791]
Epoch [15/120    avg_loss:0.509, val_acc:0.772]
Epoch [16/120    avg_loss:0.531, val_acc:0.768]
Epoch [17/120    avg_loss:0.459, val_acc:0.826]
Epoch [18/120    avg_loss:0.402, val_acc:0.808]
Epoch [19/120    avg_loss:0.333, val_acc:0.828]
Epoch [20/120    avg_loss:0.407, val_acc:0.842]
Epoch [21/120    avg_loss:0.334, val_acc:0.870]
Epoch [22/120    avg_loss:0.276, val_acc:0.888]
Epoch [23/120    avg_loss:0.288, val_acc:0.895]
Epoch [24/120    avg_loss:0.274, val_acc:0.867]
Epoch [25/120    avg_loss:0.268, val_acc:0.874]
Epoch [26/120    avg_loss:0.244, val_acc:0.896]
Epoch [27/120    avg_loss:0.243, val_acc:0.883]
Epoch [28/120    avg_loss:0.213, val_acc:0.879]
Epoch [29/120    avg_loss:0.244, val_acc:0.851]
Epoch [30/120    avg_loss:0.214, val_acc:0.901]
Epoch [31/120    avg_loss:0.174, val_acc:0.875]
Epoch [32/120    avg_loss:0.243, val_acc:0.897]
Epoch [33/120    avg_loss:0.181, val_acc:0.928]
Epoch [34/120    avg_loss:0.159, val_acc:0.931]
Epoch [35/120    avg_loss:0.125, val_acc:0.913]
Epoch [36/120    avg_loss:0.115, val_acc:0.936]
Epoch [37/120    avg_loss:0.111, val_acc:0.917]
Epoch [38/120    avg_loss:0.114, val_acc:0.913]
Epoch [39/120    avg_loss:0.145, val_acc:0.932]
Epoch [40/120    avg_loss:0.152, val_acc:0.931]
Epoch [41/120    avg_loss:0.126, val_acc:0.912]
Epoch [42/120    avg_loss:0.152, val_acc:0.940]
Epoch [43/120    avg_loss:0.158, val_acc:0.913]
Epoch [44/120    avg_loss:0.137, val_acc:0.931]
Epoch [45/120    avg_loss:0.105, val_acc:0.938]
Epoch [46/120    avg_loss:0.084, val_acc:0.942]
Epoch [47/120    avg_loss:0.074, val_acc:0.944]
Epoch [48/120    avg_loss:0.090, val_acc:0.932]
Epoch [49/120    avg_loss:0.093, val_acc:0.938]
Epoch [50/120    avg_loss:0.076, val_acc:0.947]
Epoch [51/120    avg_loss:0.068, val_acc:0.942]
Epoch [52/120    avg_loss:0.094, val_acc:0.938]
Epoch [53/120    avg_loss:0.088, val_acc:0.931]
Epoch [54/120    avg_loss:0.067, val_acc:0.941]
Epoch [55/120    avg_loss:0.088, val_acc:0.948]
Epoch [56/120    avg_loss:0.077, val_acc:0.959]
Epoch [57/120    avg_loss:0.066, val_acc:0.954]
Epoch [58/120    avg_loss:0.070, val_acc:0.954]
Epoch [59/120    avg_loss:0.080, val_acc:0.956]
Epoch [60/120    avg_loss:0.052, val_acc:0.958]
Epoch [61/120    avg_loss:0.044, val_acc:0.956]
Epoch [62/120    avg_loss:0.055, val_acc:0.952]
Epoch [63/120    avg_loss:0.051, val_acc:0.952]
Epoch [64/120    avg_loss:0.058, val_acc:0.951]
Epoch [65/120    avg_loss:0.034, val_acc:0.923]
Epoch [66/120    avg_loss:0.046, val_acc:0.946]
Epoch [67/120    avg_loss:0.050, val_acc:0.956]
Epoch [68/120    avg_loss:0.038, val_acc:0.945]
Epoch [69/120    avg_loss:0.064, val_acc:0.945]
Epoch [70/120    avg_loss:0.043, val_acc:0.951]
Epoch [71/120    avg_loss:0.033, val_acc:0.957]
Epoch [72/120    avg_loss:0.042, val_acc:0.959]
Epoch [73/120    avg_loss:0.027, val_acc:0.958]
Epoch [74/120    avg_loss:0.028, val_acc:0.963]
Epoch [75/120    avg_loss:0.024, val_acc:0.964]
Epoch [76/120    avg_loss:0.027, val_acc:0.967]
Epoch [77/120    avg_loss:0.022, val_acc:0.967]
Epoch [78/120    avg_loss:0.029, val_acc:0.967]
Epoch [79/120    avg_loss:0.027, val_acc:0.964]
Epoch [80/120    avg_loss:0.019, val_acc:0.964]
Epoch [81/120    avg_loss:0.025, val_acc:0.967]
Epoch [82/120    avg_loss:0.020, val_acc:0.967]
Epoch [83/120    avg_loss:0.023, val_acc:0.970]
Epoch [84/120    avg_loss:0.028, val_acc:0.970]
Epoch [85/120    avg_loss:0.024, val_acc:0.967]
Epoch [86/120    avg_loss:0.022, val_acc:0.967]
Epoch [87/120    avg_loss:0.021, val_acc:0.969]
Epoch [88/120    avg_loss:0.025, val_acc:0.969]
Epoch [89/120    avg_loss:0.024, val_acc:0.968]
Epoch [90/120    avg_loss:0.021, val_acc:0.966]
Epoch [91/120    avg_loss:0.021, val_acc:0.969]
Epoch [92/120    avg_loss:0.026, val_acc:0.967]
Epoch [93/120    avg_loss:0.019, val_acc:0.967]
Epoch [94/120    avg_loss:0.018, val_acc:0.965]
Epoch [95/120    avg_loss:0.020, val_acc:0.964]
Epoch [96/120    avg_loss:0.020, val_acc:0.964]
Epoch [97/120    avg_loss:0.021, val_acc:0.964]
Epoch [98/120    avg_loss:0.024, val_acc:0.964]
Epoch [99/120    avg_loss:0.018, val_acc:0.965]
Epoch [100/120    avg_loss:0.020, val_acc:0.965]
Epoch [101/120    avg_loss:0.020, val_acc:0.965]
Epoch [102/120    avg_loss:0.021, val_acc:0.965]
Epoch [103/120    avg_loss:0.022, val_acc:0.965]
Epoch [104/120    avg_loss:0.018, val_acc:0.965]
Epoch [105/120    avg_loss:0.017, val_acc:0.965]
Epoch [106/120    avg_loss:0.018, val_acc:0.965]
Epoch [107/120    avg_loss:0.022, val_acc:0.965]
Epoch [108/120    avg_loss:0.018, val_acc:0.965]
Epoch [109/120    avg_loss:0.020, val_acc:0.965]
Epoch [110/120    avg_loss:0.022, val_acc:0.965]
Epoch [111/120    avg_loss:0.018, val_acc:0.965]
Epoch [112/120    avg_loss:0.019, val_acc:0.965]
Epoch [113/120    avg_loss:0.017, val_acc:0.965]
Epoch [114/120    avg_loss:0.018, val_acc:0.965]
Epoch [115/120    avg_loss:0.018, val_acc:0.965]
Epoch [116/120    avg_loss:0.017, val_acc:0.965]
Epoch [117/120    avg_loss:0.018, val_acc:0.965]
Epoch [118/120    avg_loss:0.017, val_acc:0.965]
Epoch [119/120    avg_loss:0.018, val_acc:0.965]
Epoch [120/120    avg_loss:0.020, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1233    7    0    0    0    0    0    1   15   28    1    0
     0    0    0]
 [   0    0    0  722    0   11    0    0    0   12    0    0    1    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   16   41    0    6    0    0    0    0  794   12    1    0
     2    3    0]
 [   0    0    4    0    0    0    9    0    0    0   17 2177    2    1
     0    0    0]
 [   0    0    0   10    3    2    0    0    0    0    0   16  502    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    3    0    3    0    0    0
  1132    0    0]
 [   0    0    0    0    0    1    1    0    0    2    0    0    0    0
    93  250    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.36856368563686

F1 scores:
[       nan 0.975      0.97163121 0.94440811 0.99300699 0.9752809
 0.99168556 1.         0.99652375 0.64       0.93083236 0.97996849
 0.96260786 0.99462366 0.956485   0.83333333 0.98203593]

Kappa:
0.9585596019629905
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feaa21a9828>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.542, val_acc:0.405]
Epoch [2/120    avg_loss:2.080, val_acc:0.499]
Epoch [3/120    avg_loss:1.890, val_acc:0.544]
Epoch [4/120    avg_loss:1.671, val_acc:0.604]
Epoch [5/120    avg_loss:1.541, val_acc:0.602]
Epoch [6/120    avg_loss:1.407, val_acc:0.666]
Epoch [7/120    avg_loss:1.232, val_acc:0.641]
Epoch [8/120    avg_loss:1.026, val_acc:0.714]
Epoch [9/120    avg_loss:0.986, val_acc:0.750]
Epoch [10/120    avg_loss:0.807, val_acc:0.759]
Epoch [11/120    avg_loss:0.722, val_acc:0.775]
Epoch [12/120    avg_loss:0.593, val_acc:0.797]
Epoch [13/120    avg_loss:0.606, val_acc:0.840]
Epoch [14/120    avg_loss:0.539, val_acc:0.814]
Epoch [15/120    avg_loss:0.574, val_acc:0.845]
Epoch [16/120    avg_loss:0.759, val_acc:0.780]
Epoch [17/120    avg_loss:0.491, val_acc:0.839]
Epoch [18/120    avg_loss:0.398, val_acc:0.866]
Epoch [19/120    avg_loss:0.361, val_acc:0.872]
Epoch [20/120    avg_loss:0.350, val_acc:0.867]
Epoch [21/120    avg_loss:0.309, val_acc:0.893]
Epoch [22/120    avg_loss:0.339, val_acc:0.888]
Epoch [23/120    avg_loss:0.307, val_acc:0.873]
Epoch [24/120    avg_loss:0.292, val_acc:0.899]
Epoch [25/120    avg_loss:0.257, val_acc:0.861]
Epoch [26/120    avg_loss:0.274, val_acc:0.865]
Epoch [27/120    avg_loss:0.232, val_acc:0.914]
Epoch [28/120    avg_loss:0.274, val_acc:0.894]
Epoch [29/120    avg_loss:0.225, val_acc:0.911]
Epoch [30/120    avg_loss:0.202, val_acc:0.925]
Epoch [31/120    avg_loss:0.152, val_acc:0.914]
Epoch [32/120    avg_loss:0.138, val_acc:0.920]
Epoch [33/120    avg_loss:0.173, val_acc:0.934]
Epoch [34/120    avg_loss:0.142, val_acc:0.933]
Epoch [35/120    avg_loss:0.130, val_acc:0.935]
Epoch [36/120    avg_loss:0.125, val_acc:0.936]
Epoch [37/120    avg_loss:0.112, val_acc:0.928]
Epoch [38/120    avg_loss:0.153, val_acc:0.925]
Epoch [39/120    avg_loss:0.134, val_acc:0.931]
Epoch [40/120    avg_loss:0.128, val_acc:0.936]
Epoch [41/120    avg_loss:0.104, val_acc:0.947]
Epoch [42/120    avg_loss:0.092, val_acc:0.951]
Epoch [43/120    avg_loss:0.086, val_acc:0.958]
Epoch [44/120    avg_loss:0.104, val_acc:0.938]
Epoch [45/120    avg_loss:0.092, val_acc:0.945]
Epoch [46/120    avg_loss:0.070, val_acc:0.947]
Epoch [47/120    avg_loss:0.076, val_acc:0.948]
Epoch [48/120    avg_loss:0.092, val_acc:0.943]
Epoch [49/120    avg_loss:0.077, val_acc:0.955]
Epoch [50/120    avg_loss:0.060, val_acc:0.955]
Epoch [51/120    avg_loss:0.064, val_acc:0.951]
Epoch [52/120    avg_loss:0.088, val_acc:0.944]
Epoch [53/120    avg_loss:0.096, val_acc:0.958]
Epoch [54/120    avg_loss:0.057, val_acc:0.954]
Epoch [55/120    avg_loss:0.059, val_acc:0.954]
Epoch [56/120    avg_loss:0.065, val_acc:0.958]
Epoch [57/120    avg_loss:0.061, val_acc:0.953]
Epoch [58/120    avg_loss:0.052, val_acc:0.970]
Epoch [59/120    avg_loss:0.051, val_acc:0.957]
Epoch [60/120    avg_loss:0.067, val_acc:0.956]
Epoch [61/120    avg_loss:0.051, val_acc:0.939]
Epoch [62/120    avg_loss:0.068, val_acc:0.953]
Epoch [63/120    avg_loss:0.062, val_acc:0.959]
Epoch [64/120    avg_loss:0.047, val_acc:0.959]
Epoch [65/120    avg_loss:0.050, val_acc:0.959]
Epoch [66/120    avg_loss:0.084, val_acc:0.967]
Epoch [67/120    avg_loss:0.060, val_acc:0.969]
Epoch [68/120    avg_loss:0.071, val_acc:0.971]
Epoch [69/120    avg_loss:0.173, val_acc:0.895]
Epoch [70/120    avg_loss:0.186, val_acc:0.916]
Epoch [71/120    avg_loss:0.239, val_acc:0.941]
Epoch [72/120    avg_loss:0.120, val_acc:0.940]
Epoch [73/120    avg_loss:0.098, val_acc:0.957]
Epoch [74/120    avg_loss:0.076, val_acc:0.964]
Epoch [75/120    avg_loss:0.062, val_acc:0.956]
Epoch [76/120    avg_loss:0.068, val_acc:0.966]
Epoch [77/120    avg_loss:0.044, val_acc:0.967]
Epoch [78/120    avg_loss:0.080, val_acc:0.948]
Epoch [79/120    avg_loss:0.045, val_acc:0.970]
Epoch [80/120    avg_loss:0.032, val_acc:0.970]
Epoch [81/120    avg_loss:0.031, val_acc:0.964]
Epoch [82/120    avg_loss:0.024, val_acc:0.968]
Epoch [83/120    avg_loss:0.022, val_acc:0.974]
Epoch [84/120    avg_loss:0.021, val_acc:0.975]
Epoch [85/120    avg_loss:0.021, val_acc:0.975]
Epoch [86/120    avg_loss:0.022, val_acc:0.975]
Epoch [87/120    avg_loss:0.020, val_acc:0.975]
Epoch [88/120    avg_loss:0.019, val_acc:0.974]
Epoch [89/120    avg_loss:0.020, val_acc:0.973]
Epoch [90/120    avg_loss:0.016, val_acc:0.973]
Epoch [91/120    avg_loss:0.019, val_acc:0.974]
Epoch [92/120    avg_loss:0.017, val_acc:0.973]
Epoch [93/120    avg_loss:0.021, val_acc:0.974]
Epoch [94/120    avg_loss:0.022, val_acc:0.975]
Epoch [95/120    avg_loss:0.017, val_acc:0.975]
Epoch [96/120    avg_loss:0.018, val_acc:0.974]
Epoch [97/120    avg_loss:0.016, val_acc:0.974]
Epoch [98/120    avg_loss:0.019, val_acc:0.974]
Epoch [99/120    avg_loss:0.026, val_acc:0.975]
Epoch [100/120    avg_loss:0.021, val_acc:0.976]
Epoch [101/120    avg_loss:0.020, val_acc:0.976]
Epoch [102/120    avg_loss:0.019, val_acc:0.975]
Epoch [103/120    avg_loss:0.017, val_acc:0.977]
Epoch [104/120    avg_loss:0.019, val_acc:0.975]
Epoch [105/120    avg_loss:0.022, val_acc:0.975]
Epoch [106/120    avg_loss:0.018, val_acc:0.977]
Epoch [107/120    avg_loss:0.015, val_acc:0.976]
Epoch [108/120    avg_loss:0.017, val_acc:0.976]
Epoch [109/120    avg_loss:0.021, val_acc:0.977]
Epoch [110/120    avg_loss:0.019, val_acc:0.976]
Epoch [111/120    avg_loss:0.017, val_acc:0.976]
Epoch [112/120    avg_loss:0.017, val_acc:0.976]
Epoch [113/120    avg_loss:0.019, val_acc:0.978]
Epoch [114/120    avg_loss:0.017, val_acc:0.976]
Epoch [115/120    avg_loss:0.017, val_acc:0.975]
Epoch [116/120    avg_loss:0.019, val_acc:0.976]
Epoch [117/120    avg_loss:0.015, val_acc:0.976]
Epoch [118/120    avg_loss:0.016, val_acc:0.976]
Epoch [119/120    avg_loss:0.017, val_acc:0.978]
Epoch [120/120    avg_loss:0.016, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1256    4    0    0    0    0    0    0    4   21    0    0
     0    0    0]
 [   0    0    1  706    0   24    0    0    0    3    1    0    9    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    1    4   23    3    8    0    0    0    0  822    8    0    0
     0    6    0]
 [   0    0    8    0    0    1    3    0    0    0   18 2177    0    3
     0    0    0]
 [   0    0    0    4    1   10    0    0    0    0    7    6  504    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    2    0    3    0    0    0
  1133    0    0]
 [   0    0    0    0    0    0   17    0    0    1    0    0    0    0
    49  280    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.06233062330624

F1 scores:
[       nan 0.95       0.98355521 0.95148248 0.99069767 0.94840834
 0.98271976 1.         0.99767981 0.7804878  0.94864397 0.98439973
 0.96       0.98404255 0.97546276 0.88467615 0.98224852]

Kappa:
0.9665006677756665
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f237b555860>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.574, val_acc:0.431]
Epoch [2/120    avg_loss:2.102, val_acc:0.546]
Epoch [3/120    avg_loss:1.809, val_acc:0.587]
Epoch [4/120    avg_loss:1.630, val_acc:0.615]
Epoch [5/120    avg_loss:1.500, val_acc:0.599]
Epoch [6/120    avg_loss:1.332, val_acc:0.695]
Epoch [7/120    avg_loss:1.109, val_acc:0.707]
Epoch [8/120    avg_loss:0.935, val_acc:0.731]
Epoch [9/120    avg_loss:0.797, val_acc:0.783]
Epoch [10/120    avg_loss:0.783, val_acc:0.792]
Epoch [11/120    avg_loss:0.622, val_acc:0.797]
Epoch [12/120    avg_loss:0.636, val_acc:0.811]
Epoch [13/120    avg_loss:0.539, val_acc:0.795]
Epoch [14/120    avg_loss:0.617, val_acc:0.814]
Epoch [15/120    avg_loss:0.443, val_acc:0.853]
Epoch [16/120    avg_loss:0.382, val_acc:0.841]
Epoch [17/120    avg_loss:0.378, val_acc:0.859]
Epoch [18/120    avg_loss:0.364, val_acc:0.843]
Epoch [19/120    avg_loss:0.315, val_acc:0.871]
Epoch [20/120    avg_loss:0.301, val_acc:0.859]
Epoch [21/120    avg_loss:0.373, val_acc:0.860]
Epoch [22/120    avg_loss:0.273, val_acc:0.894]
Epoch [23/120    avg_loss:0.293, val_acc:0.887]
Epoch [24/120    avg_loss:0.350, val_acc:0.795]
Epoch [25/120    avg_loss:0.447, val_acc:0.863]
Epoch [26/120    avg_loss:0.416, val_acc:0.829]
Epoch [27/120    avg_loss:0.339, val_acc:0.888]
Epoch [28/120    avg_loss:0.249, val_acc:0.919]
Epoch [29/120    avg_loss:0.231, val_acc:0.913]
Epoch [30/120    avg_loss:0.216, val_acc:0.907]
Epoch [31/120    avg_loss:0.188, val_acc:0.931]
Epoch [32/120    avg_loss:0.171, val_acc:0.916]
Epoch [33/120    avg_loss:0.174, val_acc:0.928]
Epoch [34/120    avg_loss:0.136, val_acc:0.945]
Epoch [35/120    avg_loss:0.123, val_acc:0.944]
Epoch [36/120    avg_loss:0.148, val_acc:0.936]
Epoch [37/120    avg_loss:0.115, val_acc:0.943]
Epoch [38/120    avg_loss:0.161, val_acc:0.929]
Epoch [39/120    avg_loss:0.157, val_acc:0.931]
Epoch [40/120    avg_loss:0.150, val_acc:0.928]
Epoch [41/120    avg_loss:0.116, val_acc:0.940]
Epoch [42/120    avg_loss:0.105, val_acc:0.930]
Epoch [43/120    avg_loss:0.105, val_acc:0.961]
Epoch [44/120    avg_loss:0.078, val_acc:0.951]
Epoch [45/120    avg_loss:0.081, val_acc:0.932]
Epoch [46/120    avg_loss:0.090, val_acc:0.941]
Epoch [47/120    avg_loss:0.080, val_acc:0.928]
Epoch [48/120    avg_loss:0.127, val_acc:0.932]
Epoch [49/120    avg_loss:0.106, val_acc:0.947]
Epoch [50/120    avg_loss:0.076, val_acc:0.946]
Epoch [51/120    avg_loss:0.091, val_acc:0.946]
Epoch [52/120    avg_loss:0.070, val_acc:0.952]
Epoch [53/120    avg_loss:0.065, val_acc:0.956]
Epoch [54/120    avg_loss:0.058, val_acc:0.951]
Epoch [55/120    avg_loss:0.063, val_acc:0.950]
Epoch [56/120    avg_loss:0.050, val_acc:0.956]
Epoch [57/120    avg_loss:0.039, val_acc:0.958]
Epoch [58/120    avg_loss:0.037, val_acc:0.959]
Epoch [59/120    avg_loss:0.033, val_acc:0.962]
Epoch [60/120    avg_loss:0.030, val_acc:0.966]
Epoch [61/120    avg_loss:0.030, val_acc:0.969]
Epoch [62/120    avg_loss:0.031, val_acc:0.967]
Epoch [63/120    avg_loss:0.036, val_acc:0.966]
Epoch [64/120    avg_loss:0.033, val_acc:0.967]
Epoch [65/120    avg_loss:0.033, val_acc:0.969]
Epoch [66/120    avg_loss:0.039, val_acc:0.968]
Epoch [67/120    avg_loss:0.030, val_acc:0.969]
Epoch [68/120    avg_loss:0.031, val_acc:0.968]
Epoch [69/120    avg_loss:0.034, val_acc:0.971]
Epoch [70/120    avg_loss:0.035, val_acc:0.969]
Epoch [71/120    avg_loss:0.034, val_acc:0.969]
Epoch [72/120    avg_loss:0.028, val_acc:0.970]
Epoch [73/120    avg_loss:0.033, val_acc:0.965]
Epoch [74/120    avg_loss:0.032, val_acc:0.970]
Epoch [75/120    avg_loss:0.029, val_acc:0.966]
Epoch [76/120    avg_loss:0.037, val_acc:0.964]
Epoch [77/120    avg_loss:0.026, val_acc:0.967]
Epoch [78/120    avg_loss:0.029, val_acc:0.967]
Epoch [79/120    avg_loss:0.030, val_acc:0.970]
Epoch [80/120    avg_loss:0.026, val_acc:0.970]
Epoch [81/120    avg_loss:0.027, val_acc:0.970]
Epoch [82/120    avg_loss:0.027, val_acc:0.970]
Epoch [83/120    avg_loss:0.027, val_acc:0.969]
Epoch [84/120    avg_loss:0.025, val_acc:0.970]
Epoch [85/120    avg_loss:0.026, val_acc:0.971]
Epoch [86/120    avg_loss:0.031, val_acc:0.971]
Epoch [87/120    avg_loss:0.025, val_acc:0.971]
Epoch [88/120    avg_loss:0.025, val_acc:0.970]
Epoch [89/120    avg_loss:0.027, val_acc:0.970]
Epoch [90/120    avg_loss:0.029, val_acc:0.970]
Epoch [91/120    avg_loss:0.024, val_acc:0.970]
Epoch [92/120    avg_loss:0.026, val_acc:0.971]
Epoch [93/120    avg_loss:0.028, val_acc:0.971]
Epoch [94/120    avg_loss:0.024, val_acc:0.971]
Epoch [95/120    avg_loss:0.026, val_acc:0.971]
Epoch [96/120    avg_loss:0.024, val_acc:0.971]
Epoch [97/120    avg_loss:0.026, val_acc:0.971]
Epoch [98/120    avg_loss:0.025, val_acc:0.971]
Epoch [99/120    avg_loss:0.027, val_acc:0.971]
Epoch [100/120    avg_loss:0.027, val_acc:0.971]
Epoch [101/120    avg_loss:0.024, val_acc:0.971]
Epoch [102/120    avg_loss:0.028, val_acc:0.971]
Epoch [103/120    avg_loss:0.025, val_acc:0.971]
Epoch [104/120    avg_loss:0.025, val_acc:0.971]
Epoch [105/120    avg_loss:0.027, val_acc:0.971]
Epoch [106/120    avg_loss:0.027, val_acc:0.971]
Epoch [107/120    avg_loss:0.023, val_acc:0.971]
Epoch [108/120    avg_loss:0.024, val_acc:0.971]
Epoch [109/120    avg_loss:0.024, val_acc:0.971]
Epoch [110/120    avg_loss:0.027, val_acc:0.971]
Epoch [111/120    avg_loss:0.031, val_acc:0.971]
Epoch [112/120    avg_loss:0.024, val_acc:0.971]
Epoch [113/120    avg_loss:0.020, val_acc:0.971]
Epoch [114/120    avg_loss:0.027, val_acc:0.971]
Epoch [115/120    avg_loss:0.023, val_acc:0.971]
Epoch [116/120    avg_loss:0.032, val_acc:0.973]
Epoch [117/120    avg_loss:0.030, val_acc:0.973]
Epoch [118/120    avg_loss:0.024, val_acc:0.973]
Epoch [119/120    avg_loss:0.024, val_acc:0.973]
Epoch [120/120    avg_loss:0.025, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1221    0    6    0    1    0    0    1    8   36    3    0
     1    8    0]
 [   0    0    1  699    1   26    0    0    0    4    0    2    9    5
     0    0    0]
 [   0    0    0    0  212    0    1    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    3    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7   20    0    5    0    0    0    0  831    5    0    0
     0    7    0]
 [   0    0    8    0    0    2    4    0    1    0   12 2157    2   24
     0    0    0]
 [   0    0    0   16    8   10    0    0    0    3   10    8  476    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    3    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   142  205    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.41463414634147

F1 scores:
[       nan 0.975      0.96827914 0.94331984 0.96363636 0.95290252
 0.99163498 1.         0.99652375 0.81818182 0.95462378 0.97579733
 0.92878049 0.9273183  0.93796526 0.72310406 0.97647059]

Kappa:
0.9476908860899211
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe7aab51828>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.603, val_acc:0.446]
Epoch [2/120    avg_loss:2.114, val_acc:0.524]
Epoch [3/120    avg_loss:1.875, val_acc:0.543]
Epoch [4/120    avg_loss:1.708, val_acc:0.599]
Epoch [5/120    avg_loss:1.493, val_acc:0.656]
Epoch [6/120    avg_loss:1.265, val_acc:0.655]
Epoch [7/120    avg_loss:1.065, val_acc:0.724]
Epoch [8/120    avg_loss:0.933, val_acc:0.715]
Epoch [9/120    avg_loss:0.832, val_acc:0.717]
Epoch [10/120    avg_loss:0.822, val_acc:0.730]
Epoch [11/120    avg_loss:0.728, val_acc:0.761]
Epoch [12/120    avg_loss:0.602, val_acc:0.799]
Epoch [13/120    avg_loss:0.608, val_acc:0.798]
Epoch [14/120    avg_loss:0.616, val_acc:0.823]
Epoch [15/120    avg_loss:0.557, val_acc:0.802]
Epoch [16/120    avg_loss:0.513, val_acc:0.799]
Epoch [17/120    avg_loss:0.375, val_acc:0.875]
Epoch [18/120    avg_loss:0.348, val_acc:0.870]
Epoch [19/120    avg_loss:0.341, val_acc:0.868]
Epoch [20/120    avg_loss:0.339, val_acc:0.886]
Epoch [21/120    avg_loss:0.340, val_acc:0.896]
Epoch [22/120    avg_loss:0.281, val_acc:0.911]
Epoch [23/120    avg_loss:0.246, val_acc:0.890]
Epoch [24/120    avg_loss:0.250, val_acc:0.912]
Epoch [25/120    avg_loss:0.226, val_acc:0.922]
Epoch [26/120    avg_loss:0.204, val_acc:0.876]
Epoch [27/120    avg_loss:0.255, val_acc:0.906]
Epoch [28/120    avg_loss:0.167, val_acc:0.934]
Epoch [29/120    avg_loss:0.160, val_acc:0.929]
Epoch [30/120    avg_loss:0.164, val_acc:0.935]
Epoch [31/120    avg_loss:0.145, val_acc:0.931]
Epoch [32/120    avg_loss:0.149, val_acc:0.934]
Epoch [33/120    avg_loss:0.134, val_acc:0.900]
Epoch [34/120    avg_loss:0.130, val_acc:0.948]
Epoch [35/120    avg_loss:0.387, val_acc:0.895]
Epoch [36/120    avg_loss:0.262, val_acc:0.885]
Epoch [37/120    avg_loss:0.191, val_acc:0.916]
Epoch [38/120    avg_loss:0.248, val_acc:0.920]
Epoch [39/120    avg_loss:0.168, val_acc:0.929]
Epoch [40/120    avg_loss:0.142, val_acc:0.929]
Epoch [41/120    avg_loss:0.121, val_acc:0.936]
Epoch [42/120    avg_loss:0.122, val_acc:0.959]
Epoch [43/120    avg_loss:0.102, val_acc:0.951]
Epoch [44/120    avg_loss:0.088, val_acc:0.962]
Epoch [45/120    avg_loss:0.195, val_acc:0.914]
Epoch [46/120    avg_loss:0.238, val_acc:0.936]
Epoch [47/120    avg_loss:0.179, val_acc:0.947]
Epoch [48/120    avg_loss:0.109, val_acc:0.956]
Epoch [49/120    avg_loss:0.086, val_acc:0.961]
Epoch [50/120    avg_loss:0.070, val_acc:0.961]
Epoch [51/120    avg_loss:0.067, val_acc:0.959]
Epoch [52/120    avg_loss:0.059, val_acc:0.968]
Epoch [53/120    avg_loss:0.069, val_acc:0.957]
Epoch [54/120    avg_loss:0.070, val_acc:0.956]
Epoch [55/120    avg_loss:0.071, val_acc:0.952]
Epoch [56/120    avg_loss:0.084, val_acc:0.962]
Epoch [57/120    avg_loss:0.065, val_acc:0.963]
Epoch [58/120    avg_loss:0.050, val_acc:0.963]
Epoch [59/120    avg_loss:0.060, val_acc:0.964]
Epoch [60/120    avg_loss:0.048, val_acc:0.969]
Epoch [61/120    avg_loss:0.037, val_acc:0.969]
Epoch [62/120    avg_loss:0.066, val_acc:0.966]
Epoch [63/120    avg_loss:0.060, val_acc:0.964]
Epoch [64/120    avg_loss:0.038, val_acc:0.974]
Epoch [65/120    avg_loss:0.038, val_acc:0.979]
Epoch [66/120    avg_loss:0.039, val_acc:0.977]
Epoch [67/120    avg_loss:0.042, val_acc:0.974]
Epoch [68/120    avg_loss:0.036, val_acc:0.973]
Epoch [69/120    avg_loss:0.035, val_acc:0.967]
Epoch [70/120    avg_loss:0.034, val_acc:0.978]
Epoch [71/120    avg_loss:0.025, val_acc:0.980]
Epoch [72/120    avg_loss:0.035, val_acc:0.980]
Epoch [73/120    avg_loss:0.026, val_acc:0.974]
Epoch [74/120    avg_loss:0.065, val_acc:0.955]
Epoch [75/120    avg_loss:0.035, val_acc:0.976]
Epoch [76/120    avg_loss:0.066, val_acc:0.963]
Epoch [77/120    avg_loss:0.043, val_acc:0.973]
Epoch [78/120    avg_loss:0.096, val_acc:0.969]
Epoch [79/120    avg_loss:0.141, val_acc:0.950]
Epoch [80/120    avg_loss:0.059, val_acc:0.965]
Epoch [81/120    avg_loss:0.044, val_acc:0.976]
Epoch [82/120    avg_loss:0.033, val_acc:0.971]
Epoch [83/120    avg_loss:0.027, val_acc:0.979]
Epoch [84/120    avg_loss:0.030, val_acc:0.977]
Epoch [85/120    avg_loss:0.024, val_acc:0.977]
Epoch [86/120    avg_loss:0.023, val_acc:0.980]
Epoch [87/120    avg_loss:0.021, val_acc:0.978]
Epoch [88/120    avg_loss:0.017, val_acc:0.976]
Epoch [89/120    avg_loss:0.017, val_acc:0.978]
Epoch [90/120    avg_loss:0.017, val_acc:0.982]
Epoch [91/120    avg_loss:0.018, val_acc:0.980]
Epoch [92/120    avg_loss:0.020, val_acc:0.985]
Epoch [93/120    avg_loss:0.017, val_acc:0.984]
Epoch [94/120    avg_loss:0.016, val_acc:0.984]
Epoch [95/120    avg_loss:0.015, val_acc:0.982]
Epoch [96/120    avg_loss:0.018, val_acc:0.979]
Epoch [97/120    avg_loss:0.016, val_acc:0.981]
Epoch [98/120    avg_loss:0.013, val_acc:0.982]
Epoch [99/120    avg_loss:0.017, val_acc:0.985]
Epoch [100/120    avg_loss:0.014, val_acc:0.985]
Epoch [101/120    avg_loss:0.015, val_acc:0.985]
Epoch [102/120    avg_loss:0.013, val_acc:0.982]
Epoch [103/120    avg_loss:0.012, val_acc:0.982]
Epoch [104/120    avg_loss:0.013, val_acc:0.984]
Epoch [105/120    avg_loss:0.015, val_acc:0.984]
Epoch [106/120    avg_loss:0.015, val_acc:0.984]
Epoch [107/120    avg_loss:0.015, val_acc:0.984]
Epoch [108/120    avg_loss:0.015, val_acc:0.982]
Epoch [109/120    avg_loss:0.015, val_acc:0.981]
Epoch [110/120    avg_loss:0.014, val_acc:0.981]
Epoch [111/120    avg_loss:0.017, val_acc:0.982]
Epoch [112/120    avg_loss:0.012, val_acc:0.984]
Epoch [113/120    avg_loss:0.015, val_acc:0.984]
Epoch [114/120    avg_loss:0.016, val_acc:0.982]
Epoch [115/120    avg_loss:0.014, val_acc:0.982]
Epoch [116/120    avg_loss:0.014, val_acc:0.982]
Epoch [117/120    avg_loss:0.012, val_acc:0.982]
Epoch [118/120    avg_loss:0.014, val_acc:0.982]
Epoch [119/120    avg_loss:0.012, val_acc:0.982]
Epoch [120/120    avg_loss:0.014, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1253    1    0    0    1    0    0    0   10   17    3    0
     0    0    0]
 [   0    0    1  715    0    7    0    0    0    4    2    0   10    3
     1    4    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    2    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    1   27    0    6    0    0    0    0  823   10    0    0
     0    8    0]
 [   0    0    3    0    0    0    3    0    0    0   14 2186    2    2
     0    0    0]
 [   0    0    0   21    7    8    0    0    0    0    2    6  487    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    1    0    1    0    0    0
  1129    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
    64  282    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.00813008130082

F1 scores:
[       nan 0.94871795 0.98545026 0.9451421  0.98383372 0.96089385
 0.99620349 0.96153846 0.99883856 0.8        0.95089544 0.98690745
 0.93834297 0.98666667 0.96743787 0.8798752  0.9704142 ]

Kappa:
0.965876215189913
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f60f6151898>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.556, val_acc:0.341]
Epoch [2/120    avg_loss:2.105, val_acc:0.533]
Epoch [3/120    avg_loss:1.874, val_acc:0.565]
Epoch [4/120    avg_loss:1.727, val_acc:0.584]
Epoch [5/120    avg_loss:1.510, val_acc:0.611]
Epoch [6/120    avg_loss:1.309, val_acc:0.664]
Epoch [7/120    avg_loss:1.198, val_acc:0.681]
Epoch [8/120    avg_loss:1.113, val_acc:0.679]
Epoch [9/120    avg_loss:0.867, val_acc:0.719]
Epoch [10/120    avg_loss:0.818, val_acc:0.774]
Epoch [11/120    avg_loss:0.839, val_acc:0.803]
Epoch [12/120    avg_loss:0.696, val_acc:0.792]
Epoch [13/120    avg_loss:0.582, val_acc:0.794]
Epoch [14/120    avg_loss:0.514, val_acc:0.806]
Epoch [15/120    avg_loss:0.533, val_acc:0.827]
Epoch [16/120    avg_loss:0.462, val_acc:0.843]
Epoch [17/120    avg_loss:0.481, val_acc:0.851]
Epoch [18/120    avg_loss:0.511, val_acc:0.834]
Epoch [19/120    avg_loss:0.393, val_acc:0.866]
Epoch [20/120    avg_loss:0.324, val_acc:0.871]
Epoch [21/120    avg_loss:0.312, val_acc:0.841]
Epoch [22/120    avg_loss:0.312, val_acc:0.878]
Epoch [23/120    avg_loss:0.280, val_acc:0.866]
Epoch [24/120    avg_loss:0.281, val_acc:0.871]
Epoch [25/120    avg_loss:0.239, val_acc:0.882]
Epoch [26/120    avg_loss:0.226, val_acc:0.882]
Epoch [27/120    avg_loss:0.262, val_acc:0.890]
Epoch [28/120    avg_loss:0.206, val_acc:0.885]
Epoch [29/120    avg_loss:0.202, val_acc:0.907]
Epoch [30/120    avg_loss:0.178, val_acc:0.897]
Epoch [31/120    avg_loss:0.213, val_acc:0.871]
Epoch [32/120    avg_loss:0.255, val_acc:0.907]
Epoch [33/120    avg_loss:0.207, val_acc:0.898]
Epoch [34/120    avg_loss:0.155, val_acc:0.906]
Epoch [35/120    avg_loss:0.192, val_acc:0.868]
Epoch [36/120    avg_loss:0.157, val_acc:0.907]
Epoch [37/120    avg_loss:0.148, val_acc:0.921]
Epoch [38/120    avg_loss:0.147, val_acc:0.922]
Epoch [39/120    avg_loss:0.144, val_acc:0.933]
Epoch [40/120    avg_loss:0.183, val_acc:0.898]
Epoch [41/120    avg_loss:0.138, val_acc:0.895]
Epoch [42/120    avg_loss:0.149, val_acc:0.925]
Epoch [43/120    avg_loss:0.098, val_acc:0.929]
Epoch [44/120    avg_loss:0.130, val_acc:0.919]
Epoch [45/120    avg_loss:0.118, val_acc:0.947]
Epoch [46/120    avg_loss:0.095, val_acc:0.931]
Epoch [47/120    avg_loss:0.098, val_acc:0.927]
Epoch [48/120    avg_loss:0.104, val_acc:0.938]
Epoch [49/120    avg_loss:0.087, val_acc:0.936]
Epoch [50/120    avg_loss:0.083, val_acc:0.927]
Epoch [51/120    avg_loss:0.064, val_acc:0.946]
Epoch [52/120    avg_loss:0.068, val_acc:0.927]
Epoch [53/120    avg_loss:0.056, val_acc:0.950]
Epoch [54/120    avg_loss:0.052, val_acc:0.948]
Epoch [55/120    avg_loss:0.054, val_acc:0.950]
Epoch [56/120    avg_loss:0.037, val_acc:0.950]
Epoch [57/120    avg_loss:0.063, val_acc:0.947]
Epoch [58/120    avg_loss:0.053, val_acc:0.939]
Epoch [59/120    avg_loss:0.071, val_acc:0.944]
Epoch [60/120    avg_loss:0.062, val_acc:0.955]
Epoch [61/120    avg_loss:0.058, val_acc:0.943]
Epoch [62/120    avg_loss:0.038, val_acc:0.952]
Epoch [63/120    avg_loss:0.048, val_acc:0.941]
Epoch [64/120    avg_loss:0.048, val_acc:0.924]
Epoch [65/120    avg_loss:0.046, val_acc:0.966]
Epoch [66/120    avg_loss:0.087, val_acc:0.955]
Epoch [67/120    avg_loss:0.056, val_acc:0.959]
Epoch [68/120    avg_loss:0.041, val_acc:0.940]
Epoch [69/120    avg_loss:0.051, val_acc:0.957]
Epoch [70/120    avg_loss:0.036, val_acc:0.962]
Epoch [71/120    avg_loss:0.042, val_acc:0.950]
Epoch [72/120    avg_loss:0.037, val_acc:0.958]
Epoch [73/120    avg_loss:0.031, val_acc:0.950]
Epoch [74/120    avg_loss:0.030, val_acc:0.955]
Epoch [75/120    avg_loss:0.077, val_acc:0.917]
Epoch [76/120    avg_loss:0.090, val_acc:0.951]
Epoch [77/120    avg_loss:0.051, val_acc:0.957]
Epoch [78/120    avg_loss:0.056, val_acc:0.948]
Epoch [79/120    avg_loss:0.046, val_acc:0.963]
Epoch [80/120    avg_loss:0.033, val_acc:0.963]
Epoch [81/120    avg_loss:0.032, val_acc:0.968]
Epoch [82/120    avg_loss:0.033, val_acc:0.969]
Epoch [83/120    avg_loss:0.025, val_acc:0.973]
Epoch [84/120    avg_loss:0.019, val_acc:0.971]
Epoch [85/120    avg_loss:0.021, val_acc:0.973]
Epoch [86/120    avg_loss:0.023, val_acc:0.975]
Epoch [87/120    avg_loss:0.019, val_acc:0.974]
Epoch [88/120    avg_loss:0.020, val_acc:0.975]
Epoch [89/120    avg_loss:0.018, val_acc:0.973]
Epoch [90/120    avg_loss:0.021, val_acc:0.976]
Epoch [91/120    avg_loss:0.023, val_acc:0.976]
Epoch [92/120    avg_loss:0.019, val_acc:0.976]
Epoch [93/120    avg_loss:0.021, val_acc:0.974]
Epoch [94/120    avg_loss:0.018, val_acc:0.974]
Epoch [95/120    avg_loss:0.019, val_acc:0.975]
Epoch [96/120    avg_loss:0.017, val_acc:0.976]
Epoch [97/120    avg_loss:0.015, val_acc:0.974]
Epoch [98/120    avg_loss:0.020, val_acc:0.974]
Epoch [99/120    avg_loss:0.020, val_acc:0.975]
Epoch [100/120    avg_loss:0.015, val_acc:0.977]
Epoch [101/120    avg_loss:0.015, val_acc:0.977]
Epoch [102/120    avg_loss:0.013, val_acc:0.976]
Epoch [103/120    avg_loss:0.018, val_acc:0.977]
Epoch [104/120    avg_loss:0.016, val_acc:0.978]
Epoch [105/120    avg_loss:0.016, val_acc:0.977]
Epoch [106/120    avg_loss:0.014, val_acc:0.977]
Epoch [107/120    avg_loss:0.014, val_acc:0.978]
Epoch [108/120    avg_loss:0.015, val_acc:0.979]
Epoch [109/120    avg_loss:0.015, val_acc:0.977]
Epoch [110/120    avg_loss:0.017, val_acc:0.975]
Epoch [111/120    avg_loss:0.016, val_acc:0.977]
Epoch [112/120    avg_loss:0.013, val_acc:0.975]
Epoch [113/120    avg_loss:0.016, val_acc:0.977]
Epoch [114/120    avg_loss:0.014, val_acc:0.976]
Epoch [115/120    avg_loss:0.014, val_acc:0.977]
Epoch [116/120    avg_loss:0.014, val_acc:0.979]
Epoch [117/120    avg_loss:0.016, val_acc:0.977]
Epoch [118/120    avg_loss:0.014, val_acc:0.979]
Epoch [119/120    avg_loss:0.013, val_acc:0.979]
Epoch [120/120    avg_loss:0.015, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1256    1    0    0    0    0    0    1    6   16    3    0
     0    2    0]
 [   0    0    0  699    4   20    0    0    0    5    0    2   13    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   12   50    0    4    0    0    0    0  791    9    0    0
     3    6    0]
 [   0    0    5    0    0    0    4    0    1    0   33 2165    0    2
     0    0    0]
 [   0    0    0    4    4    6    0    0    0    0    2    8  508    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    3    0    0    0    0    0
  1133    0    0]
 [   0    0    0    0    0    0    9    0    0    3    0    0    0    0
   106  229    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.09756097560975

F1 scores:
[       nan 1.         0.9820172  0.93137908 0.98156682 0.96115427
 0.99020347 1.         0.99537037 0.75555556 0.92677211 0.98185941
 0.95849057 0.98404255 0.95130143 0.78424658 0.98224852]

Kappa:
0.9554890308003844
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff9a69c97f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.595, val_acc:0.454]
Epoch [2/120    avg_loss:2.137, val_acc:0.521]
Epoch [3/120    avg_loss:1.896, val_acc:0.579]
Epoch [4/120    avg_loss:1.736, val_acc:0.576]
Epoch [5/120    avg_loss:1.601, val_acc:0.618]
Epoch [6/120    avg_loss:1.470, val_acc:0.646]
Epoch [7/120    avg_loss:1.341, val_acc:0.647]
Epoch [8/120    avg_loss:1.172, val_acc:0.719]
Epoch [9/120    avg_loss:1.094, val_acc:0.733]
Epoch [10/120    avg_loss:0.898, val_acc:0.761]
Epoch [11/120    avg_loss:0.801, val_acc:0.756]
Epoch [12/120    avg_loss:0.735, val_acc:0.770]
Epoch [13/120    avg_loss:0.651, val_acc:0.792]
Epoch [14/120    avg_loss:0.667, val_acc:0.804]
Epoch [15/120    avg_loss:0.653, val_acc:0.817]
Epoch [16/120    avg_loss:0.623, val_acc:0.805]
Epoch [17/120    avg_loss:0.473, val_acc:0.836]
Epoch [18/120    avg_loss:0.477, val_acc:0.866]
Epoch [19/120    avg_loss:0.433, val_acc:0.860]
Epoch [20/120    avg_loss:0.414, val_acc:0.867]
Epoch [21/120    avg_loss:0.387, val_acc:0.862]
Epoch [22/120    avg_loss:0.352, val_acc:0.860]
Epoch [23/120    avg_loss:0.320, val_acc:0.890]
Epoch [24/120    avg_loss:0.339, val_acc:0.894]
Epoch [25/120    avg_loss:0.312, val_acc:0.895]
Epoch [26/120    avg_loss:0.283, val_acc:0.904]
Epoch [27/120    avg_loss:0.368, val_acc:0.881]
Epoch [28/120    avg_loss:0.227, val_acc:0.914]
Epoch [29/120    avg_loss:0.253, val_acc:0.922]
Epoch [30/120    avg_loss:0.199, val_acc:0.927]
Epoch [31/120    avg_loss:0.199, val_acc:0.897]
Epoch [32/120    avg_loss:0.246, val_acc:0.901]
Epoch [33/120    avg_loss:0.218, val_acc:0.897]
Epoch [34/120    avg_loss:0.238, val_acc:0.904]
Epoch [35/120    avg_loss:0.191, val_acc:0.919]
Epoch [36/120    avg_loss:0.199, val_acc:0.905]
Epoch [37/120    avg_loss:0.156, val_acc:0.948]
Epoch [38/120    avg_loss:0.141, val_acc:0.930]
Epoch [39/120    avg_loss:0.202, val_acc:0.921]
Epoch [40/120    avg_loss:0.176, val_acc:0.922]
Epoch [41/120    avg_loss:0.163, val_acc:0.930]
Epoch [42/120    avg_loss:0.132, val_acc:0.916]
Epoch [43/120    avg_loss:0.160, val_acc:0.923]
Epoch [44/120    avg_loss:0.110, val_acc:0.926]
Epoch [45/120    avg_loss:0.097, val_acc:0.941]
Epoch [46/120    avg_loss:0.165, val_acc:0.911]
Epoch [47/120    avg_loss:0.116, val_acc:0.941]
Epoch [48/120    avg_loss:0.102, val_acc:0.935]
Epoch [49/120    avg_loss:0.144, val_acc:0.929]
Epoch [50/120    avg_loss:0.113, val_acc:0.941]
Epoch [51/120    avg_loss:0.078, val_acc:0.943]
Epoch [52/120    avg_loss:0.079, val_acc:0.945]
Epoch [53/120    avg_loss:0.071, val_acc:0.948]
Epoch [54/120    avg_loss:0.069, val_acc:0.949]
Epoch [55/120    avg_loss:0.057, val_acc:0.950]
Epoch [56/120    avg_loss:0.057, val_acc:0.950]
Epoch [57/120    avg_loss:0.054, val_acc:0.948]
Epoch [58/120    avg_loss:0.055, val_acc:0.944]
Epoch [59/120    avg_loss:0.052, val_acc:0.943]
Epoch [60/120    avg_loss:0.048, val_acc:0.949]
Epoch [61/120    avg_loss:0.052, val_acc:0.949]
Epoch [62/120    avg_loss:0.050, val_acc:0.950]
Epoch [63/120    avg_loss:0.050, val_acc:0.953]
Epoch [64/120    avg_loss:0.060, val_acc:0.952]
Epoch [65/120    avg_loss:0.050, val_acc:0.951]
Epoch [66/120    avg_loss:0.049, val_acc:0.952]
Epoch [67/120    avg_loss:0.055, val_acc:0.955]
Epoch [68/120    avg_loss:0.050, val_acc:0.952]
Epoch [69/120    avg_loss:0.059, val_acc:0.951]
Epoch [70/120    avg_loss:0.059, val_acc:0.951]
Epoch [71/120    avg_loss:0.042, val_acc:0.953]
Epoch [72/120    avg_loss:0.043, val_acc:0.954]
Epoch [73/120    avg_loss:0.045, val_acc:0.951]
Epoch [74/120    avg_loss:0.058, val_acc:0.948]
Epoch [75/120    avg_loss:0.044, val_acc:0.949]
Epoch [76/120    avg_loss:0.042, val_acc:0.952]
Epoch [77/120    avg_loss:0.044, val_acc:0.952]
Epoch [78/120    avg_loss:0.041, val_acc:0.955]
Epoch [79/120    avg_loss:0.047, val_acc:0.951]
Epoch [80/120    avg_loss:0.048, val_acc:0.951]
Epoch [81/120    avg_loss:0.043, val_acc:0.951]
Epoch [82/120    avg_loss:0.049, val_acc:0.950]
Epoch [83/120    avg_loss:0.043, val_acc:0.950]
Epoch [84/120    avg_loss:0.042, val_acc:0.954]
Epoch [85/120    avg_loss:0.051, val_acc:0.953]
Epoch [86/120    avg_loss:0.043, val_acc:0.952]
Epoch [87/120    avg_loss:0.043, val_acc:0.953]
Epoch [88/120    avg_loss:0.036, val_acc:0.953]
Epoch [89/120    avg_loss:0.047, val_acc:0.953]
Epoch [90/120    avg_loss:0.038, val_acc:0.955]
Epoch [91/120    avg_loss:0.043, val_acc:0.950]
Epoch [92/120    avg_loss:0.038, val_acc:0.946]
Epoch [93/120    avg_loss:0.039, val_acc:0.952]
Epoch [94/120    avg_loss:0.036, val_acc:0.959]
Epoch [95/120    avg_loss:0.038, val_acc:0.953]
Epoch [96/120    avg_loss:0.042, val_acc:0.953]
Epoch [97/120    avg_loss:0.039, val_acc:0.952]
Epoch [98/120    avg_loss:0.038, val_acc:0.955]
Epoch [99/120    avg_loss:0.036, val_acc:0.953]
Epoch [100/120    avg_loss:0.035, val_acc:0.956]
Epoch [101/120    avg_loss:0.038, val_acc:0.956]
Epoch [102/120    avg_loss:0.035, val_acc:0.953]
Epoch [103/120    avg_loss:0.034, val_acc:0.955]
Epoch [104/120    avg_loss:0.039, val_acc:0.954]
Epoch [105/120    avg_loss:0.033, val_acc:0.954]
Epoch [106/120    avg_loss:0.038, val_acc:0.951]
Epoch [107/120    avg_loss:0.042, val_acc:0.952]
Epoch [108/120    avg_loss:0.033, val_acc:0.953]
Epoch [109/120    avg_loss:0.039, val_acc:0.953]
Epoch [110/120    avg_loss:0.035, val_acc:0.951]
Epoch [111/120    avg_loss:0.030, val_acc:0.951]
Epoch [112/120    avg_loss:0.038, val_acc:0.950]
Epoch [113/120    avg_loss:0.036, val_acc:0.952]
Epoch [114/120    avg_loss:0.035, val_acc:0.952]
Epoch [115/120    avg_loss:0.036, val_acc:0.952]
Epoch [116/120    avg_loss:0.036, val_acc:0.952]
Epoch [117/120    avg_loss:0.032, val_acc:0.951]
Epoch [118/120    avg_loss:0.037, val_acc:0.952]
Epoch [119/120    avg_loss:0.034, val_acc:0.952]
Epoch [120/120    avg_loss:0.035, val_acc:0.952]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1226    0    1    0    1    0    0    3   10   42    2    0
     0    0    0]
 [   0    0   11  680    0   26    0    0    0    7    0    1   20    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  420    0    0    0    5    0    0    0    0
    10    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    4    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    1    0    0   14    0    0    1    0
     0    0    0]
 [   0    0   33   89    0    5    0    0    0    0  733    1    2    0
     2   10    0]
 [   0    0    9    0    0    0   16    0    0    0   21 2160    1    2
     1    0    0]
 [   0    0    5    4    2    4    0    0    0    0   12    1  498    0
     0    0    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    3    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0    2    0    0    4    0    0    0    0
   149  192    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
94.09214092140921

F1 scores:
[       nan 0.975      0.95445699 0.8935611  0.99300699 0.94382022
 0.97965335 1.         0.99767981 0.54901961 0.8852657  0.97759674
 0.93873704 0.98930481 0.93027071 0.69945355 0.93641618]

Kappa:
0.9325696604143654
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f16a3a86860>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.538, val_acc:0.477]
Epoch [2/120    avg_loss:2.141, val_acc:0.548]
Epoch [3/120    avg_loss:1.884, val_acc:0.561]
Epoch [4/120    avg_loss:1.741, val_acc:0.625]
Epoch [5/120    avg_loss:1.491, val_acc:0.625]
Epoch [6/120    avg_loss:1.273, val_acc:0.690]
Epoch [7/120    avg_loss:1.104, val_acc:0.660]
Epoch [8/120    avg_loss:0.948, val_acc:0.709]
Epoch [9/120    avg_loss:0.851, val_acc:0.727]
Epoch [10/120    avg_loss:0.810, val_acc:0.766]
Epoch [11/120    avg_loss:0.749, val_acc:0.782]
Epoch [12/120    avg_loss:0.693, val_acc:0.743]
Epoch [13/120    avg_loss:0.635, val_acc:0.794]
Epoch [14/120    avg_loss:0.536, val_acc:0.818]
Epoch [15/120    avg_loss:0.455, val_acc:0.847]
Epoch [16/120    avg_loss:0.532, val_acc:0.822]
Epoch [17/120    avg_loss:0.446, val_acc:0.844]
Epoch [18/120    avg_loss:0.485, val_acc:0.863]
Epoch [19/120    avg_loss:0.545, val_acc:0.786]
Epoch [20/120    avg_loss:0.389, val_acc:0.882]
Epoch [21/120    avg_loss:0.389, val_acc:0.860]
Epoch [22/120    avg_loss:0.391, val_acc:0.868]
Epoch [23/120    avg_loss:0.369, val_acc:0.894]
Epoch [24/120    avg_loss:0.287, val_acc:0.885]
Epoch [25/120    avg_loss:0.351, val_acc:0.889]
Epoch [26/120    avg_loss:0.296, val_acc:0.890]
Epoch [27/120    avg_loss:0.278, val_acc:0.916]
Epoch [28/120    avg_loss:0.241, val_acc:0.919]
Epoch [29/120    avg_loss:0.232, val_acc:0.905]
Epoch [30/120    avg_loss:0.216, val_acc:0.881]
Epoch [31/120    avg_loss:0.284, val_acc:0.898]
Epoch [32/120    avg_loss:0.215, val_acc:0.925]
Epoch [33/120    avg_loss:0.212, val_acc:0.892]
Epoch [34/120    avg_loss:0.267, val_acc:0.918]
Epoch [35/120    avg_loss:0.200, val_acc:0.903]
Epoch [36/120    avg_loss:0.219, val_acc:0.902]
Epoch [37/120    avg_loss:0.202, val_acc:0.911]
Epoch [38/120    avg_loss:0.163, val_acc:0.931]
Epoch [39/120    avg_loss:0.136, val_acc:0.936]
Epoch [40/120    avg_loss:0.167, val_acc:0.922]
Epoch [41/120    avg_loss:0.242, val_acc:0.901]
Epoch [42/120    avg_loss:0.162, val_acc:0.934]
Epoch [43/120    avg_loss:0.132, val_acc:0.934]
Epoch [44/120    avg_loss:0.155, val_acc:0.934]
Epoch [45/120    avg_loss:0.136, val_acc:0.940]
Epoch [46/120    avg_loss:0.154, val_acc:0.925]
Epoch [47/120    avg_loss:0.165, val_acc:0.935]
Epoch [48/120    avg_loss:0.139, val_acc:0.933]
Epoch [49/120    avg_loss:0.138, val_acc:0.940]
Epoch [50/120    avg_loss:0.123, val_acc:0.926]
Epoch [51/120    avg_loss:0.110, val_acc:0.939]
Epoch [52/120    avg_loss:0.110, val_acc:0.955]
Epoch [53/120    avg_loss:0.094, val_acc:0.926]
Epoch [54/120    avg_loss:0.084, val_acc:0.935]
Epoch [55/120    avg_loss:0.105, val_acc:0.941]
Epoch [56/120    avg_loss:0.174, val_acc:0.936]
Epoch [57/120    avg_loss:0.160, val_acc:0.916]
Epoch [58/120    avg_loss:0.149, val_acc:0.951]
Epoch [59/120    avg_loss:0.118, val_acc:0.949]
Epoch [60/120    avg_loss:0.088, val_acc:0.951]
Epoch [61/120    avg_loss:0.091, val_acc:0.943]
Epoch [62/120    avg_loss:0.097, val_acc:0.948]
Epoch [63/120    avg_loss:0.075, val_acc:0.949]
Epoch [64/120    avg_loss:0.097, val_acc:0.950]
Epoch [65/120    avg_loss:0.080, val_acc:0.951]
Epoch [66/120    avg_loss:0.060, val_acc:0.957]
Epoch [67/120    avg_loss:0.042, val_acc:0.958]
Epoch [68/120    avg_loss:0.047, val_acc:0.955]
Epoch [69/120    avg_loss:0.044, val_acc:0.955]
Epoch [70/120    avg_loss:0.044, val_acc:0.956]
Epoch [71/120    avg_loss:0.044, val_acc:0.957]
Epoch [72/120    avg_loss:0.038, val_acc:0.958]
Epoch [73/120    avg_loss:0.034, val_acc:0.957]
Epoch [74/120    avg_loss:0.050, val_acc:0.958]
Epoch [75/120    avg_loss:0.052, val_acc:0.956]
Epoch [76/120    avg_loss:0.033, val_acc:0.956]
Epoch [77/120    avg_loss:0.035, val_acc:0.956]
Epoch [78/120    avg_loss:0.044, val_acc:0.957]
Epoch [79/120    avg_loss:0.034, val_acc:0.956]
Epoch [80/120    avg_loss:0.047, val_acc:0.958]
Epoch [81/120    avg_loss:0.037, val_acc:0.957]
Epoch [82/120    avg_loss:0.031, val_acc:0.957]
Epoch [83/120    avg_loss:0.035, val_acc:0.960]
Epoch [84/120    avg_loss:0.041, val_acc:0.961]
Epoch [85/120    avg_loss:0.034, val_acc:0.959]
Epoch [86/120    avg_loss:0.040, val_acc:0.960]
Epoch [87/120    avg_loss:0.046, val_acc:0.959]
Epoch [88/120    avg_loss:0.040, val_acc:0.959]
Epoch [89/120    avg_loss:0.041, val_acc:0.960]
Epoch [90/120    avg_loss:0.036, val_acc:0.964]
Epoch [91/120    avg_loss:0.037, val_acc:0.959]
Epoch [92/120    avg_loss:0.035, val_acc:0.960]
Epoch [93/120    avg_loss:0.032, val_acc:0.958]
Epoch [94/120    avg_loss:0.033, val_acc:0.963]
Epoch [95/120    avg_loss:0.036, val_acc:0.964]
Epoch [96/120    avg_loss:0.034, val_acc:0.965]
Epoch [97/120    avg_loss:0.032, val_acc:0.966]
Epoch [98/120    avg_loss:0.034, val_acc:0.966]
Epoch [99/120    avg_loss:0.035, val_acc:0.964]
Epoch [100/120    avg_loss:0.033, val_acc:0.963]
Epoch [101/120    avg_loss:0.029, val_acc:0.966]
Epoch [102/120    avg_loss:0.037, val_acc:0.966]
Epoch [103/120    avg_loss:0.026, val_acc:0.960]
Epoch [104/120    avg_loss:0.032, val_acc:0.963]
Epoch [105/120    avg_loss:0.033, val_acc:0.966]
Epoch [106/120    avg_loss:0.031, val_acc:0.965]
Epoch [107/120    avg_loss:0.032, val_acc:0.963]
Epoch [108/120    avg_loss:0.029, val_acc:0.963]
Epoch [109/120    avg_loss:0.030, val_acc:0.959]
Epoch [110/120    avg_loss:0.026, val_acc:0.963]
Epoch [111/120    avg_loss:0.029, val_acc:0.965]
Epoch [112/120    avg_loss:0.036, val_acc:0.965]
Epoch [113/120    avg_loss:0.032, val_acc:0.964]
Epoch [114/120    avg_loss:0.033, val_acc:0.966]
Epoch [115/120    avg_loss:0.026, val_acc:0.964]
Epoch [116/120    avg_loss:0.030, val_acc:0.963]
Epoch [117/120    avg_loss:0.030, val_acc:0.961]
Epoch [118/120    avg_loss:0.027, val_acc:0.964]
Epoch [119/120    avg_loss:0.030, val_acc:0.968]
Epoch [120/120    avg_loss:0.027, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1189    1    0    0   13    0    0    0   24   46    4    0
     0    8    0]
 [   0    0    2  648   17   12    0    0    0   12    0    0   55    1
     0    0    0]
 [   0    0    0    0  212    0    1    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    4    0    4    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   10   84    0    5    5    0    0    0  757    3    2    0
     0    9    0]
 [   0    0    1    0    0    0   31    0    0    0    3 2166    5    4
     0    0    0]
 [   0    0    0    6    5    6    0    0    0    0    9    3  496    0
     0    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    1    0    1    1    0    0
  1128    0    0]
 [   0    0    0    0    0    0   35    0    0    6    0    0    0    0
    79  227    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.11382113821138

F1 scores:
[       nan 0.98765432 0.95617209 0.87155346 0.94854586 0.95045045
 0.93924232 0.92592593 0.99883856 0.57142857 0.90658683 0.97809889
 0.90428441 0.98666667 0.95959166 0.76818951 0.94915254]

Kappa:
0.9329177611733425
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f37f82c8828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.526, val_acc:0.375]
Epoch [2/120    avg_loss:2.145, val_acc:0.490]
Epoch [3/120    avg_loss:1.916, val_acc:0.569]
Epoch [4/120    avg_loss:1.748, val_acc:0.607]
Epoch [5/120    avg_loss:1.509, val_acc:0.640]
Epoch [6/120    avg_loss:1.257, val_acc:0.652]
Epoch [7/120    avg_loss:1.061, val_acc:0.659]
Epoch [8/120    avg_loss:0.987, val_acc:0.708]
Epoch [9/120    avg_loss:0.885, val_acc:0.731]
Epoch [10/120    avg_loss:0.762, val_acc:0.793]
Epoch [11/120    avg_loss:0.697, val_acc:0.803]
Epoch [12/120    avg_loss:0.692, val_acc:0.844]
Epoch [13/120    avg_loss:0.527, val_acc:0.830]
Epoch [14/120    avg_loss:0.482, val_acc:0.843]
Epoch [15/120    avg_loss:0.481, val_acc:0.845]
Epoch [16/120    avg_loss:0.465, val_acc:0.884]
Epoch [17/120    avg_loss:0.422, val_acc:0.882]
Epoch [18/120    avg_loss:0.442, val_acc:0.851]
Epoch [19/120    avg_loss:0.463, val_acc:0.858]
Epoch [20/120    avg_loss:0.518, val_acc:0.867]
Epoch [21/120    avg_loss:0.380, val_acc:0.898]
Epoch [22/120    avg_loss:0.269, val_acc:0.931]
Epoch [23/120    avg_loss:0.252, val_acc:0.912]
Epoch [24/120    avg_loss:0.281, val_acc:0.878]
Epoch [25/120    avg_loss:0.310, val_acc:0.895]
Epoch [26/120    avg_loss:0.237, val_acc:0.919]
Epoch [27/120    avg_loss:0.221, val_acc:0.890]
Epoch [28/120    avg_loss:0.244, val_acc:0.912]
Epoch [29/120    avg_loss:0.234, val_acc:0.947]
Epoch [30/120    avg_loss:0.188, val_acc:0.933]
Epoch [31/120    avg_loss:0.175, val_acc:0.932]
Epoch [32/120    avg_loss:0.161, val_acc:0.940]
Epoch [33/120    avg_loss:0.174, val_acc:0.920]
Epoch [34/120    avg_loss:0.155, val_acc:0.938]
Epoch [35/120    avg_loss:0.142, val_acc:0.944]
Epoch [36/120    avg_loss:0.152, val_acc:0.914]
Epoch [37/120    avg_loss:0.153, val_acc:0.936]
Epoch [38/120    avg_loss:0.124, val_acc:0.949]
Epoch [39/120    avg_loss:0.133, val_acc:0.935]
Epoch [40/120    avg_loss:0.156, val_acc:0.931]
Epoch [41/120    avg_loss:0.116, val_acc:0.949]
Epoch [42/120    avg_loss:0.121, val_acc:0.941]
Epoch [43/120    avg_loss:0.099, val_acc:0.952]
Epoch [44/120    avg_loss:0.094, val_acc:0.953]
Epoch [45/120    avg_loss:0.118, val_acc:0.920]
Epoch [46/120    avg_loss:0.144, val_acc:0.949]
Epoch [47/120    avg_loss:0.094, val_acc:0.958]
Epoch [48/120    avg_loss:0.107, val_acc:0.961]
Epoch [49/120    avg_loss:0.076, val_acc:0.961]
Epoch [50/120    avg_loss:0.101, val_acc:0.943]
Epoch [51/120    avg_loss:0.083, val_acc:0.957]
Epoch [52/120    avg_loss:0.070, val_acc:0.960]
Epoch [53/120    avg_loss:0.079, val_acc:0.956]
Epoch [54/120    avg_loss:0.086, val_acc:0.949]
Epoch [55/120    avg_loss:0.096, val_acc:0.932]
Epoch [56/120    avg_loss:0.091, val_acc:0.955]
Epoch [57/120    avg_loss:0.076, val_acc:0.947]
Epoch [58/120    avg_loss:0.094, val_acc:0.968]
Epoch [59/120    avg_loss:0.067, val_acc:0.976]
Epoch [60/120    avg_loss:0.062, val_acc:0.970]
Epoch [61/120    avg_loss:0.071, val_acc:0.966]
Epoch [62/120    avg_loss:0.058, val_acc:0.966]
Epoch [63/120    avg_loss:0.049, val_acc:0.966]
Epoch [64/120    avg_loss:0.049, val_acc:0.965]
Epoch [65/120    avg_loss:0.046, val_acc:0.970]
Epoch [66/120    avg_loss:0.041, val_acc:0.969]
Epoch [67/120    avg_loss:0.050, val_acc:0.969]
Epoch [68/120    avg_loss:0.047, val_acc:0.972]
Epoch [69/120    avg_loss:0.047, val_acc:0.968]
Epoch [70/120    avg_loss:0.038, val_acc:0.975]
Epoch [71/120    avg_loss:0.043, val_acc:0.975]
Epoch [72/120    avg_loss:0.037, val_acc:0.973]
Epoch [73/120    avg_loss:0.039, val_acc:0.978]
Epoch [74/120    avg_loss:0.029, val_acc:0.980]
Epoch [75/120    avg_loss:0.030, val_acc:0.981]
Epoch [76/120    avg_loss:0.024, val_acc:0.980]
Epoch [77/120    avg_loss:0.021, val_acc:0.978]
Epoch [78/120    avg_loss:0.022, val_acc:0.976]
Epoch [79/120    avg_loss:0.022, val_acc:0.977]
Epoch [80/120    avg_loss:0.021, val_acc:0.980]
Epoch [81/120    avg_loss:0.026, val_acc:0.977]
Epoch [82/120    avg_loss:0.024, val_acc:0.975]
Epoch [83/120    avg_loss:0.022, val_acc:0.978]
Epoch [84/120    avg_loss:0.023, val_acc:0.977]
Epoch [85/120    avg_loss:0.021, val_acc:0.978]
Epoch [86/120    avg_loss:0.024, val_acc:0.978]
Epoch [87/120    avg_loss:0.027, val_acc:0.980]
Epoch [88/120    avg_loss:0.019, val_acc:0.978]
Epoch [89/120    avg_loss:0.022, val_acc:0.978]
Epoch [90/120    avg_loss:0.021, val_acc:0.980]
Epoch [91/120    avg_loss:0.020, val_acc:0.980]
Epoch [92/120    avg_loss:0.022, val_acc:0.980]
Epoch [93/120    avg_loss:0.019, val_acc:0.980]
Epoch [94/120    avg_loss:0.020, val_acc:0.980]
Epoch [95/120    avg_loss:0.024, val_acc:0.980]
Epoch [96/120    avg_loss:0.019, val_acc:0.980]
Epoch [97/120    avg_loss:0.019, val_acc:0.980]
Epoch [98/120    avg_loss:0.023, val_acc:0.980]
Epoch [99/120    avg_loss:0.022, val_acc:0.980]
Epoch [100/120    avg_loss:0.022, val_acc:0.980]
Epoch [101/120    avg_loss:0.018, val_acc:0.980]
Epoch [102/120    avg_loss:0.021, val_acc:0.980]
Epoch [103/120    avg_loss:0.019, val_acc:0.980]
Epoch [104/120    avg_loss:0.020, val_acc:0.980]
Epoch [105/120    avg_loss:0.020, val_acc:0.980]
Epoch [106/120    avg_loss:0.019, val_acc:0.980]
Epoch [107/120    avg_loss:0.020, val_acc:0.980]
Epoch [108/120    avg_loss:0.024, val_acc:0.980]
Epoch [109/120    avg_loss:0.028, val_acc:0.980]
Epoch [110/120    avg_loss:0.025, val_acc:0.980]
Epoch [111/120    avg_loss:0.021, val_acc:0.980]
Epoch [112/120    avg_loss:0.016, val_acc:0.980]
Epoch [113/120    avg_loss:0.023, val_acc:0.980]
Epoch [114/120    avg_loss:0.019, val_acc:0.980]
Epoch [115/120    avg_loss:0.018, val_acc:0.980]
Epoch [116/120    avg_loss:0.022, val_acc:0.980]
Epoch [117/120    avg_loss:0.020, val_acc:0.980]
Epoch [118/120    avg_loss:0.022, val_acc:0.980]
Epoch [119/120    avg_loss:0.020, val_acc:0.980]
Epoch [120/120    avg_loss:0.021, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1245    1    0    0    2    0    0    3    7   25    2    0
     0    0    0]
 [   0    0    1  722    0    0    0    0    0    5    0    0   15    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    1    0    4    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0   30   86    0    6    0    0    0    0  742    3    1    0
     0    7    0]
 [   0    0   10    0    0    0    2    0    3    0   13 2181    0    1
     0    0    0]
 [   0    0    0    3    0    7    0    0    0    0    4    8  507    0
     0    0    5]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0    5    0    0    7    0    0    0    0
   114  221    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.64227642276423

F1 scores:
[       nan 0.975      0.96849475 0.92623477 1.         0.96900115
 0.9924357  0.98039216 0.99537037 0.57692308 0.9026764  0.98509485
 0.95480226 0.98395722 0.94829024 0.76869565 0.97109827]

Kappa:
0.950267521695072
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa7e21767f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.514, val_acc:0.387]
Epoch [2/120    avg_loss:2.138, val_acc:0.523]
Epoch [3/120    avg_loss:1.920, val_acc:0.564]
Epoch [4/120    avg_loss:1.767, val_acc:0.577]
Epoch [5/120    avg_loss:1.616, val_acc:0.626]
Epoch [6/120    avg_loss:1.470, val_acc:0.661]
Epoch [7/120    avg_loss:1.287, val_acc:0.686]
Epoch [8/120    avg_loss:1.153, val_acc:0.672]
Epoch [9/120    avg_loss:1.018, val_acc:0.742]
Epoch [10/120    avg_loss:0.909, val_acc:0.739]
Epoch [11/120    avg_loss:0.849, val_acc:0.757]
Epoch [12/120    avg_loss:0.777, val_acc:0.742]
Epoch [13/120    avg_loss:0.695, val_acc:0.782]
Epoch [14/120    avg_loss:0.600, val_acc:0.831]
Epoch [15/120    avg_loss:0.591, val_acc:0.812]
Epoch [16/120    avg_loss:0.574, val_acc:0.843]
Epoch [17/120    avg_loss:0.513, val_acc:0.840]
Epoch [18/120    avg_loss:0.473, val_acc:0.865]
Epoch [19/120    avg_loss:0.455, val_acc:0.838]
Epoch [20/120    avg_loss:0.391, val_acc:0.884]
Epoch [21/120    avg_loss:0.503, val_acc:0.860]
Epoch [22/120    avg_loss:0.401, val_acc:0.863]
Epoch [23/120    avg_loss:0.352, val_acc:0.850]
Epoch [24/120    avg_loss:0.405, val_acc:0.852]
Epoch [25/120    avg_loss:0.362, val_acc:0.869]
Epoch [26/120    avg_loss:0.335, val_acc:0.876]
Epoch [27/120    avg_loss:0.276, val_acc:0.865]
Epoch [28/120    avg_loss:0.324, val_acc:0.894]
Epoch [29/120    avg_loss:0.311, val_acc:0.883]
Epoch [30/120    avg_loss:0.292, val_acc:0.905]
Epoch [31/120    avg_loss:0.262, val_acc:0.917]
Epoch [32/120    avg_loss:0.228, val_acc:0.926]
Epoch [33/120    avg_loss:0.251, val_acc:0.946]
Epoch [34/120    avg_loss:0.256, val_acc:0.898]
Epoch [35/120    avg_loss:0.172, val_acc:0.930]
Epoch [36/120    avg_loss:0.182, val_acc:0.932]
Epoch [37/120    avg_loss:0.179, val_acc:0.920]
Epoch [38/120    avg_loss:0.237, val_acc:0.936]
Epoch [39/120    avg_loss:0.169, val_acc:0.935]
Epoch [40/120    avg_loss:0.165, val_acc:0.940]
Epoch [41/120    avg_loss:0.186, val_acc:0.933]
Epoch [42/120    avg_loss:0.131, val_acc:0.940]
Epoch [43/120    avg_loss:0.339, val_acc:0.881]
Epoch [44/120    avg_loss:0.254, val_acc:0.920]
Epoch [45/120    avg_loss:0.168, val_acc:0.923]
Epoch [46/120    avg_loss:0.171, val_acc:0.927]
Epoch [47/120    avg_loss:0.119, val_acc:0.934]
Epoch [48/120    avg_loss:0.091, val_acc:0.941]
Epoch [49/120    avg_loss:0.086, val_acc:0.943]
Epoch [50/120    avg_loss:0.089, val_acc:0.944]
Epoch [51/120    avg_loss:0.087, val_acc:0.946]
Epoch [52/120    avg_loss:0.092, val_acc:0.950]
Epoch [53/120    avg_loss:0.092, val_acc:0.949]
Epoch [54/120    avg_loss:0.082, val_acc:0.950]
Epoch [55/120    avg_loss:0.066, val_acc:0.951]
Epoch [56/120    avg_loss:0.075, val_acc:0.948]
Epoch [57/120    avg_loss:0.078, val_acc:0.951]
Epoch [58/120    avg_loss:0.071, val_acc:0.949]
Epoch [59/120    avg_loss:0.070, val_acc:0.951]
Epoch [60/120    avg_loss:0.068, val_acc:0.950]
Epoch [61/120    avg_loss:0.071, val_acc:0.950]
Epoch [62/120    avg_loss:0.076, val_acc:0.949]
Epoch [63/120    avg_loss:0.076, val_acc:0.953]
Epoch [64/120    avg_loss:0.068, val_acc:0.950]
Epoch [65/120    avg_loss:0.063, val_acc:0.952]
Epoch [66/120    avg_loss:0.068, val_acc:0.953]
Epoch [67/120    avg_loss:0.068, val_acc:0.958]
Epoch [68/120    avg_loss:0.063, val_acc:0.955]
Epoch [69/120    avg_loss:0.059, val_acc:0.955]
Epoch [70/120    avg_loss:0.071, val_acc:0.954]
Epoch [71/120    avg_loss:0.076, val_acc:0.955]
Epoch [72/120    avg_loss:0.060, val_acc:0.956]
Epoch [73/120    avg_loss:0.056, val_acc:0.954]
Epoch [74/120    avg_loss:0.059, val_acc:0.960]
Epoch [75/120    avg_loss:0.063, val_acc:0.961]
Epoch [76/120    avg_loss:0.053, val_acc:0.955]
Epoch [77/120    avg_loss:0.057, val_acc:0.960]
Epoch [78/120    avg_loss:0.057, val_acc:0.958]
Epoch [79/120    avg_loss:0.062, val_acc:0.955]
Epoch [80/120    avg_loss:0.063, val_acc:0.960]
Epoch [81/120    avg_loss:0.066, val_acc:0.963]
Epoch [82/120    avg_loss:0.051, val_acc:0.962]
Epoch [83/120    avg_loss:0.052, val_acc:0.959]
Epoch [84/120    avg_loss:0.063, val_acc:0.960]
Epoch [85/120    avg_loss:0.059, val_acc:0.961]
Epoch [86/120    avg_loss:0.050, val_acc:0.955]
Epoch [87/120    avg_loss:0.050, val_acc:0.960]
Epoch [88/120    avg_loss:0.052, val_acc:0.958]
Epoch [89/120    avg_loss:0.059, val_acc:0.958]
Epoch [90/120    avg_loss:0.050, val_acc:0.960]
Epoch [91/120    avg_loss:0.050, val_acc:0.958]
Epoch [92/120    avg_loss:0.051, val_acc:0.956]
Epoch [93/120    avg_loss:0.054, val_acc:0.963]
Epoch [94/120    avg_loss:0.049, val_acc:0.963]
Epoch [95/120    avg_loss:0.054, val_acc:0.961]
Epoch [96/120    avg_loss:0.047, val_acc:0.965]
Epoch [97/120    avg_loss:0.048, val_acc:0.961]
Epoch [98/120    avg_loss:0.053, val_acc:0.962]
Epoch [99/120    avg_loss:0.044, val_acc:0.963]
Epoch [100/120    avg_loss:0.053, val_acc:0.967]
Epoch [101/120    avg_loss:0.055, val_acc:0.967]
Epoch [102/120    avg_loss:0.046, val_acc:0.964]
Epoch [103/120    avg_loss:0.045, val_acc:0.963]
Epoch [104/120    avg_loss:0.048, val_acc:0.968]
Epoch [105/120    avg_loss:0.045, val_acc:0.963]
Epoch [106/120    avg_loss:0.048, val_acc:0.961]
Epoch [107/120    avg_loss:0.047, val_acc:0.965]
Epoch [108/120    avg_loss:0.048, val_acc:0.962]
Epoch [109/120    avg_loss:0.049, val_acc:0.964]
Epoch [110/120    avg_loss:0.049, val_acc:0.965]
Epoch [111/120    avg_loss:0.045, val_acc:0.962]
Epoch [112/120    avg_loss:0.039, val_acc:0.962]
Epoch [113/120    avg_loss:0.049, val_acc:0.967]
Epoch [114/120    avg_loss:0.047, val_acc:0.968]
Epoch [115/120    avg_loss:0.041, val_acc:0.968]
Epoch [116/120    avg_loss:0.039, val_acc:0.962]
Epoch [117/120    avg_loss:0.045, val_acc:0.961]
Epoch [118/120    avg_loss:0.042, val_acc:0.961]
Epoch [119/120    avg_loss:0.045, val_acc:0.963]
Epoch [120/120    avg_loss:0.043, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    4 1200   14    1    0    5    0    0    1   14   37    4    0
     0    5    0]
 [   0    0    1  712    0   13    0    0    0    9    1    0   10    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    2    0    2    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   41   89    0    7    0    0    0    1  714   10    2    0
     0   11    0]
 [   0    0   18    0    0    3    7    0    5    0   10 2152   11    3
     1    0    0]
 [   0    3    0   39    0    0    0    0    0    0   15    0  466    0
     0    0   11]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    2    0    2    1    0    0
  1127    0    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
   120  223    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
93.91869918699187

F1 scores:
[       nan 0.87058824 0.94302554 0.88833437 0.99765808 0.9573991
 0.98491704 0.96153846 0.99192618 0.68085106 0.8733945  0.97552131
 0.90749757 0.98930481 0.94191392 0.76109215 0.93854749]

Kappa:
0.9306441228455081
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcf0f6e7860>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.523, val_acc:0.472]
Epoch [2/120    avg_loss:2.062, val_acc:0.503]
Epoch [3/120    avg_loss:1.851, val_acc:0.561]
Epoch [4/120    avg_loss:1.667, val_acc:0.570]
Epoch [5/120    avg_loss:1.481, val_acc:0.625]
Epoch [6/120    avg_loss:1.330, val_acc:0.649]
Epoch [7/120    avg_loss:1.142, val_acc:0.688]
Epoch [8/120    avg_loss:1.008, val_acc:0.715]
Epoch [9/120    avg_loss:0.852, val_acc:0.757]
Epoch [10/120    avg_loss:0.845, val_acc:0.730]
Epoch [11/120    avg_loss:0.778, val_acc:0.805]
Epoch [12/120    avg_loss:0.702, val_acc:0.758]
Epoch [13/120    avg_loss:0.709, val_acc:0.799]
Epoch [14/120    avg_loss:0.596, val_acc:0.800]
Epoch [15/120    avg_loss:0.564, val_acc:0.800]
Epoch [16/120    avg_loss:0.536, val_acc:0.807]
Epoch [17/120    avg_loss:0.563, val_acc:0.830]
Epoch [18/120    avg_loss:0.471, val_acc:0.853]
Epoch [19/120    avg_loss:0.392, val_acc:0.883]
Epoch [20/120    avg_loss:0.363, val_acc:0.831]
Epoch [21/120    avg_loss:0.384, val_acc:0.860]
Epoch [22/120    avg_loss:0.357, val_acc:0.889]
Epoch [23/120    avg_loss:0.281, val_acc:0.885]
Epoch [24/120    avg_loss:0.323, val_acc:0.901]
Epoch [25/120    avg_loss:0.285, val_acc:0.886]
Epoch [26/120    avg_loss:0.231, val_acc:0.922]
Epoch [27/120    avg_loss:0.227, val_acc:0.926]
Epoch [28/120    avg_loss:0.217, val_acc:0.919]
Epoch [29/120    avg_loss:0.322, val_acc:0.865]
Epoch [30/120    avg_loss:0.248, val_acc:0.890]
Epoch [31/120    avg_loss:0.229, val_acc:0.910]
Epoch [32/120    avg_loss:0.217, val_acc:0.927]
Epoch [33/120    avg_loss:0.277, val_acc:0.928]
Epoch [34/120    avg_loss:0.411, val_acc:0.850]
Epoch [35/120    avg_loss:0.317, val_acc:0.935]
Epoch [36/120    avg_loss:0.206, val_acc:0.914]
Epoch [37/120    avg_loss:0.232, val_acc:0.920]
Epoch [38/120    avg_loss:0.160, val_acc:0.919]
Epoch [39/120    avg_loss:0.176, val_acc:0.939]
Epoch [40/120    avg_loss:0.137, val_acc:0.936]
Epoch [41/120    avg_loss:0.125, val_acc:0.935]
Epoch [42/120    avg_loss:0.133, val_acc:0.944]
Epoch [43/120    avg_loss:0.152, val_acc:0.924]
Epoch [44/120    avg_loss:0.134, val_acc:0.963]
Epoch [45/120    avg_loss:0.125, val_acc:0.951]
Epoch [46/120    avg_loss:0.116, val_acc:0.944]
Epoch [47/120    avg_loss:0.118, val_acc:0.955]
Epoch [48/120    avg_loss:0.122, val_acc:0.948]
Epoch [49/120    avg_loss:0.104, val_acc:0.950]
Epoch [50/120    avg_loss:0.097, val_acc:0.963]
Epoch [51/120    avg_loss:0.096, val_acc:0.948]
Epoch [52/120    avg_loss:0.090, val_acc:0.957]
Epoch [53/120    avg_loss:0.076, val_acc:0.949]
Epoch [54/120    avg_loss:0.124, val_acc:0.965]
Epoch [55/120    avg_loss:0.077, val_acc:0.959]
Epoch [56/120    avg_loss:0.060, val_acc:0.959]
Epoch [57/120    avg_loss:0.081, val_acc:0.960]
Epoch [58/120    avg_loss:0.109, val_acc:0.945]
Epoch [59/120    avg_loss:0.095, val_acc:0.944]
Epoch [60/120    avg_loss:0.105, val_acc:0.952]
Epoch [61/120    avg_loss:0.110, val_acc:0.956]
Epoch [62/120    avg_loss:0.082, val_acc:0.957]
Epoch [63/120    avg_loss:0.065, val_acc:0.966]
Epoch [64/120    avg_loss:0.075, val_acc:0.957]
Epoch [65/120    avg_loss:0.082, val_acc:0.957]
Epoch [66/120    avg_loss:0.056, val_acc:0.961]
Epoch [67/120    avg_loss:0.065, val_acc:0.966]
Epoch [68/120    avg_loss:0.043, val_acc:0.958]
Epoch [69/120    avg_loss:0.063, val_acc:0.951]
Epoch [70/120    avg_loss:0.053, val_acc:0.964]
Epoch [71/120    avg_loss:0.061, val_acc:0.966]
Epoch [72/120    avg_loss:0.046, val_acc:0.972]
Epoch [73/120    avg_loss:0.048, val_acc:0.967]
Epoch [74/120    avg_loss:0.075, val_acc:0.967]
Epoch [75/120    avg_loss:0.050, val_acc:0.966]
Epoch [76/120    avg_loss:0.049, val_acc:0.974]
Epoch [77/120    avg_loss:0.036, val_acc:0.969]
Epoch [78/120    avg_loss:0.047, val_acc:0.975]
Epoch [79/120    avg_loss:0.044, val_acc:0.970]
Epoch [80/120    avg_loss:0.050, val_acc:0.964]
Epoch [81/120    avg_loss:0.065, val_acc:0.956]
Epoch [82/120    avg_loss:0.090, val_acc:0.957]
Epoch [83/120    avg_loss:0.069, val_acc:0.973]
Epoch [84/120    avg_loss:0.046, val_acc:0.974]
Epoch [85/120    avg_loss:0.052, val_acc:0.961]
Epoch [86/120    avg_loss:0.055, val_acc:0.973]
Epoch [87/120    avg_loss:0.047, val_acc:0.965]
Epoch [88/120    avg_loss:0.045, val_acc:0.976]
Epoch [89/120    avg_loss:0.030, val_acc:0.970]
Epoch [90/120    avg_loss:0.035, val_acc:0.975]
Epoch [91/120    avg_loss:0.023, val_acc:0.972]
Epoch [92/120    avg_loss:0.024, val_acc:0.981]
Epoch [93/120    avg_loss:0.032, val_acc:0.983]
Epoch [94/120    avg_loss:0.039, val_acc:0.959]
Epoch [95/120    avg_loss:0.055, val_acc:0.953]
Epoch [96/120    avg_loss:0.055, val_acc:0.963]
Epoch [97/120    avg_loss:0.044, val_acc:0.972]
Epoch [98/120    avg_loss:0.050, val_acc:0.952]
Epoch [99/120    avg_loss:0.042, val_acc:0.981]
Epoch [100/120    avg_loss:0.038, val_acc:0.981]
Epoch [101/120    avg_loss:0.027, val_acc:0.980]
Epoch [102/120    avg_loss:0.027, val_acc:0.968]
Epoch [103/120    avg_loss:0.031, val_acc:0.973]
Epoch [104/120    avg_loss:0.026, val_acc:0.982]
Epoch [105/120    avg_loss:0.027, val_acc:0.985]
Epoch [106/120    avg_loss:0.026, val_acc:0.985]
Epoch [107/120    avg_loss:0.022, val_acc:0.981]
Epoch [108/120    avg_loss:0.033, val_acc:0.978]
Epoch [109/120    avg_loss:0.065, val_acc:0.972]
Epoch [110/120    avg_loss:0.034, val_acc:0.980]
Epoch [111/120    avg_loss:0.043, val_acc:0.965]
Epoch [112/120    avg_loss:0.039, val_acc:0.976]
Epoch [113/120    avg_loss:0.027, val_acc:0.982]
Epoch [114/120    avg_loss:0.021, val_acc:0.984]
Epoch [115/120    avg_loss:0.020, val_acc:0.988]
Epoch [116/120    avg_loss:0.025, val_acc:0.977]
Epoch [117/120    avg_loss:0.027, val_acc:0.972]
Epoch [118/120    avg_loss:0.024, val_acc:0.981]
Epoch [119/120    avg_loss:0.022, val_acc:0.980]
Epoch [120/120    avg_loss:0.020, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1259    1    0    0    2    0    0    0   11    9    3    0
     0    0    0]
 [   0    0    0  733    2   10    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    0    0    1    0    0    0    0
    11    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   15    0    0    1    0
     0    0    0]
 [   0    0    6   87    0    3    0    0    0    0  763    9    0    0
     0    7    0]
 [   0    0    8    0    0    4    6    0    0    0    7 2182    0    3
     0    0    0]
 [   0    0    0   12    4    1    0    0    0    0   11    0  504    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    3    0    2    2    0    0
  1132    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
   138  206    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.91327913279133

F1 scores:
[       nan 0.975      0.98436278 0.92667509 0.98611111 0.96575342
 0.99017385 1.         0.99652375 0.83333333 0.91322561 0.98867241
 0.96737044 0.9919571  0.93553719 0.73571429 0.98823529]

Kappa:
0.9533570448293944
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7966adf7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.603, val_acc:0.345]
Epoch [2/120    avg_loss:2.129, val_acc:0.558]
Epoch [3/120    avg_loss:1.904, val_acc:0.588]
Epoch [4/120    avg_loss:1.715, val_acc:0.598]
Epoch [5/120    avg_loss:1.548, val_acc:0.598]
Epoch [6/120    avg_loss:1.341, val_acc:0.658]
Epoch [7/120    avg_loss:1.209, val_acc:0.651]
Epoch [8/120    avg_loss:1.030, val_acc:0.716]
Epoch [9/120    avg_loss:0.963, val_acc:0.720]
Epoch [10/120    avg_loss:0.810, val_acc:0.764]
Epoch [11/120    avg_loss:0.727, val_acc:0.761]
Epoch [12/120    avg_loss:0.704, val_acc:0.794]
Epoch [13/120    avg_loss:0.659, val_acc:0.753]
Epoch [14/120    avg_loss:0.662, val_acc:0.765]
Epoch [15/120    avg_loss:0.603, val_acc:0.793]
Epoch [16/120    avg_loss:0.543, val_acc:0.768]
Epoch [17/120    avg_loss:0.496, val_acc:0.817]
Epoch [18/120    avg_loss:0.463, val_acc:0.794]
Epoch [19/120    avg_loss:0.444, val_acc:0.852]
Epoch [20/120    avg_loss:0.474, val_acc:0.794]
Epoch [21/120    avg_loss:0.401, val_acc:0.860]
Epoch [22/120    avg_loss:0.319, val_acc:0.855]
Epoch [23/120    avg_loss:0.346, val_acc:0.883]
Epoch [24/120    avg_loss:0.288, val_acc:0.885]
Epoch [25/120    avg_loss:0.339, val_acc:0.852]
Epoch [26/120    avg_loss:0.365, val_acc:0.848]
Epoch [27/120    avg_loss:0.343, val_acc:0.868]
Epoch [28/120    avg_loss:0.292, val_acc:0.895]
Epoch [29/120    avg_loss:0.216, val_acc:0.910]
Epoch [30/120    avg_loss:0.220, val_acc:0.905]
Epoch [31/120    avg_loss:0.225, val_acc:0.891]
Epoch [32/120    avg_loss:0.234, val_acc:0.911]
Epoch [33/120    avg_loss:0.241, val_acc:0.906]
Epoch [34/120    avg_loss:0.270, val_acc:0.890]
Epoch [35/120    avg_loss:0.246, val_acc:0.890]
Epoch [36/120    avg_loss:0.179, val_acc:0.910]
Epoch [37/120    avg_loss:0.153, val_acc:0.912]
Epoch [38/120    avg_loss:0.149, val_acc:0.918]
Epoch [39/120    avg_loss:0.144, val_acc:0.924]
Epoch [40/120    avg_loss:0.139, val_acc:0.931]
Epoch [41/120    avg_loss:0.156, val_acc:0.928]
Epoch [42/120    avg_loss:0.146, val_acc:0.910]
Epoch [43/120    avg_loss:0.149, val_acc:0.902]
Epoch [44/120    avg_loss:0.119, val_acc:0.936]
Epoch [45/120    avg_loss:0.140, val_acc:0.925]
Epoch [46/120    avg_loss:0.152, val_acc:0.935]
Epoch [47/120    avg_loss:0.131, val_acc:0.941]
Epoch [48/120    avg_loss:0.108, val_acc:0.936]
Epoch [49/120    avg_loss:0.100, val_acc:0.934]
Epoch [50/120    avg_loss:0.095, val_acc:0.935]
Epoch [51/120    avg_loss:0.097, val_acc:0.924]
Epoch [52/120    avg_loss:0.108, val_acc:0.939]
Epoch [53/120    avg_loss:0.095, val_acc:0.941]
Epoch [54/120    avg_loss:0.135, val_acc:0.924]
Epoch [55/120    avg_loss:0.122, val_acc:0.945]
Epoch [56/120    avg_loss:0.091, val_acc:0.938]
Epoch [57/120    avg_loss:0.097, val_acc:0.956]
Epoch [58/120    avg_loss:0.087, val_acc:0.956]
Epoch [59/120    avg_loss:0.067, val_acc:0.952]
Epoch [60/120    avg_loss:0.083, val_acc:0.932]
Epoch [61/120    avg_loss:0.062, val_acc:0.951]
Epoch [62/120    avg_loss:0.060, val_acc:0.957]
Epoch [63/120    avg_loss:0.058, val_acc:0.955]
Epoch [64/120    avg_loss:0.047, val_acc:0.948]
Epoch [65/120    avg_loss:0.053, val_acc:0.957]
Epoch [66/120    avg_loss:0.048, val_acc:0.948]
Epoch [67/120    avg_loss:0.051, val_acc:0.953]
Epoch [68/120    avg_loss:0.067, val_acc:0.945]
Epoch [69/120    avg_loss:0.089, val_acc:0.941]
Epoch [70/120    avg_loss:0.076, val_acc:0.952]
Epoch [71/120    avg_loss:0.057, val_acc:0.941]
Epoch [72/120    avg_loss:0.056, val_acc:0.951]
Epoch [73/120    avg_loss:0.037, val_acc:0.960]
Epoch [74/120    avg_loss:0.038, val_acc:0.967]
Epoch [75/120    avg_loss:0.045, val_acc:0.968]
Epoch [76/120    avg_loss:0.060, val_acc:0.959]
Epoch [77/120    avg_loss:0.053, val_acc:0.957]
Epoch [78/120    avg_loss:0.043, val_acc:0.967]
Epoch [79/120    avg_loss:0.052, val_acc:0.939]
Epoch [80/120    avg_loss:0.063, val_acc:0.945]
Epoch [81/120    avg_loss:0.062, val_acc:0.961]
Epoch [82/120    avg_loss:0.076, val_acc:0.936]
Epoch [83/120    avg_loss:0.103, val_acc:0.957]
Epoch [84/120    avg_loss:0.039, val_acc:0.950]
Epoch [85/120    avg_loss:0.055, val_acc:0.945]
Epoch [86/120    avg_loss:0.042, val_acc:0.960]
Epoch [87/120    avg_loss:0.036, val_acc:0.951]
Epoch [88/120    avg_loss:0.040, val_acc:0.966]
Epoch [89/120    avg_loss:0.042, val_acc:0.967]
Epoch [90/120    avg_loss:0.029, val_acc:0.969]
Epoch [91/120    avg_loss:0.025, val_acc:0.975]
Epoch [92/120    avg_loss:0.018, val_acc:0.975]
Epoch [93/120    avg_loss:0.022, val_acc:0.973]
Epoch [94/120    avg_loss:0.020, val_acc:0.973]
Epoch [95/120    avg_loss:0.021, val_acc:0.975]
Epoch [96/120    avg_loss:0.022, val_acc:0.977]
Epoch [97/120    avg_loss:0.025, val_acc:0.965]
Epoch [98/120    avg_loss:0.020, val_acc:0.968]
Epoch [99/120    avg_loss:0.022, val_acc:0.976]
Epoch [100/120    avg_loss:0.021, val_acc:0.972]
Epoch [101/120    avg_loss:0.018, val_acc:0.973]
Epoch [102/120    avg_loss:0.021, val_acc:0.973]
Epoch [103/120    avg_loss:0.017, val_acc:0.976]
Epoch [104/120    avg_loss:0.019, val_acc:0.973]
Epoch [105/120    avg_loss:0.018, val_acc:0.974]
Epoch [106/120    avg_loss:0.018, val_acc:0.976]
Epoch [107/120    avg_loss:0.017, val_acc:0.975]
Epoch [108/120    avg_loss:0.020, val_acc:0.973]
Epoch [109/120    avg_loss:0.017, val_acc:0.975]
Epoch [110/120    avg_loss:0.017, val_acc:0.974]
Epoch [111/120    avg_loss:0.015, val_acc:0.974]
Epoch [112/120    avg_loss:0.017, val_acc:0.975]
Epoch [113/120    avg_loss:0.018, val_acc:0.974]
Epoch [114/120    avg_loss:0.022, val_acc:0.974]
Epoch [115/120    avg_loss:0.015, val_acc:0.974]
Epoch [116/120    avg_loss:0.016, val_acc:0.974]
Epoch [117/120    avg_loss:0.016, val_acc:0.975]
Epoch [118/120    avg_loss:0.022, val_acc:0.974]
Epoch [119/120    avg_loss:0.018, val_acc:0.974]
Epoch [120/120    avg_loss:0.018, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    3    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    2 1252    7    0    0    0    0    0    1    2   18    3    0
     0    0    0]
 [   0    0    1  712    2   18    0    0    0    6    0    0    5    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   28   90    0    7    0    0    0    0  740    4    0    0
     0    6    0]
 [   0    0   20    0    0    0   11    0    0    0   10 2164    1    0
     4    0    0]
 [   0    0    1    5    7    9    0    0    0    0    3   20  486    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    1    0    2    0    0    0
  1126    0    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
   157  188    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.76422764227642

F1 scores:
[       nan 0.9382716  0.96679537 0.91223575 0.97931034 0.94609461
 0.98791541 1.         0.99883856 0.79069767 0.90686275 0.97985058
 0.94186047 0.9919571  0.92598684 0.69500924 0.9704142 ]

Kappa:
0.940230919180299
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:07:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff4ea876898>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.537, val_acc:0.515]
Epoch [2/120    avg_loss:2.113, val_acc:0.541]
Epoch [3/120    avg_loss:1.920, val_acc:0.557]
Epoch [4/120    avg_loss:1.708, val_acc:0.624]
Epoch [5/120    avg_loss:1.532, val_acc:0.613]
Epoch [6/120    avg_loss:1.413, val_acc:0.625]
Epoch [7/120    avg_loss:1.193, val_acc:0.644]
Epoch [8/120    avg_loss:1.048, val_acc:0.645]
Epoch [9/120    avg_loss:0.996, val_acc:0.678]
Epoch [10/120    avg_loss:0.881, val_acc:0.765]
Epoch [11/120    avg_loss:0.819, val_acc:0.741]
Epoch [12/120    avg_loss:0.736, val_acc:0.819]
Epoch [13/120    avg_loss:0.593, val_acc:0.818]
Epoch [14/120    avg_loss:0.599, val_acc:0.781]
Epoch [15/120    avg_loss:0.622, val_acc:0.830]
Epoch [16/120    avg_loss:0.532, val_acc:0.848]
Epoch [17/120    avg_loss:0.497, val_acc:0.840]
Epoch [18/120    avg_loss:0.457, val_acc:0.839]
Epoch [19/120    avg_loss:0.398, val_acc:0.852]
Epoch [20/120    avg_loss:0.391, val_acc:0.865]
Epoch [21/120    avg_loss:0.392, val_acc:0.859]
Epoch [22/120    avg_loss:0.335, val_acc:0.870]
Epoch [23/120    avg_loss:0.415, val_acc:0.869]
Epoch [24/120    avg_loss:0.353, val_acc:0.870]
Epoch [25/120    avg_loss:0.306, val_acc:0.881]
Epoch [26/120    avg_loss:0.368, val_acc:0.869]
Epoch [27/120    avg_loss:0.297, val_acc:0.894]
Epoch [28/120    avg_loss:0.326, val_acc:0.859]
Epoch [29/120    avg_loss:0.296, val_acc:0.895]
Epoch [30/120    avg_loss:0.235, val_acc:0.890]
Epoch [31/120    avg_loss:0.256, val_acc:0.903]
Epoch [32/120    avg_loss:0.209, val_acc:0.923]
Epoch [33/120    avg_loss:0.208, val_acc:0.915]
Epoch [34/120    avg_loss:0.192, val_acc:0.930]
Epoch [35/120    avg_loss:0.186, val_acc:0.909]
Epoch [36/120    avg_loss:0.183, val_acc:0.927]
Epoch [37/120    avg_loss:0.168, val_acc:0.932]
Epoch [38/120    avg_loss:0.136, val_acc:0.910]
Epoch [39/120    avg_loss:0.160, val_acc:0.944]
Epoch [40/120    avg_loss:0.150, val_acc:0.920]
Epoch [41/120    avg_loss:0.124, val_acc:0.932]
Epoch [42/120    avg_loss:0.114, val_acc:0.927]
Epoch [43/120    avg_loss:0.149, val_acc:0.926]
Epoch [44/120    avg_loss:0.169, val_acc:0.933]
Epoch [45/120    avg_loss:0.174, val_acc:0.905]
Epoch [46/120    avg_loss:0.140, val_acc:0.942]
Epoch [47/120    avg_loss:0.137, val_acc:0.938]
Epoch [48/120    avg_loss:0.208, val_acc:0.928]
Epoch [49/120    avg_loss:0.133, val_acc:0.907]
Epoch [50/120    avg_loss:0.095, val_acc:0.945]
Epoch [51/120    avg_loss:0.101, val_acc:0.933]
Epoch [52/120    avg_loss:0.105, val_acc:0.950]
Epoch [53/120    avg_loss:0.089, val_acc:0.941]
Epoch [54/120    avg_loss:0.097, val_acc:0.941]
Epoch [55/120    avg_loss:0.090, val_acc:0.947]
Epoch [56/120    avg_loss:0.092, val_acc:0.942]
Epoch [57/120    avg_loss:0.071, val_acc:0.963]
Epoch [58/120    avg_loss:0.067, val_acc:0.951]
Epoch [59/120    avg_loss:0.080, val_acc:0.947]
Epoch [60/120    avg_loss:0.090, val_acc:0.939]
Epoch [61/120    avg_loss:0.114, val_acc:0.933]
Epoch [62/120    avg_loss:0.103, val_acc:0.939]
Epoch [63/120    avg_loss:0.091, val_acc:0.951]
Epoch [64/120    avg_loss:0.109, val_acc:0.956]
Epoch [65/120    avg_loss:0.091, val_acc:0.942]
Epoch [66/120    avg_loss:0.094, val_acc:0.963]
Epoch [67/120    avg_loss:0.076, val_acc:0.952]
Epoch [68/120    avg_loss:0.081, val_acc:0.948]
Epoch [69/120    avg_loss:0.081, val_acc:0.965]
Epoch [70/120    avg_loss:0.070, val_acc:0.940]
Epoch [71/120    avg_loss:0.074, val_acc:0.964]
Epoch [72/120    avg_loss:0.057, val_acc:0.966]
Epoch [73/120    avg_loss:0.068, val_acc:0.943]
Epoch [74/120    avg_loss:0.048, val_acc:0.968]
Epoch [75/120    avg_loss:0.051, val_acc:0.968]
Epoch [76/120    avg_loss:0.037, val_acc:0.981]
Epoch [77/120    avg_loss:0.047, val_acc:0.963]
Epoch [78/120    avg_loss:0.044, val_acc:0.967]
Epoch [79/120    avg_loss:0.041, val_acc:0.972]
Epoch [80/120    avg_loss:0.045, val_acc:0.978]
Epoch [81/120    avg_loss:0.040, val_acc:0.966]
Epoch [82/120    avg_loss:0.046, val_acc:0.969]
Epoch [83/120    avg_loss:0.057, val_acc:0.974]
Epoch [84/120    avg_loss:0.038, val_acc:0.973]
Epoch [85/120    avg_loss:0.029, val_acc:0.961]
Epoch [86/120    avg_loss:0.035, val_acc:0.978]
Epoch [87/120    avg_loss:0.038, val_acc:0.976]
Epoch [88/120    avg_loss:0.027, val_acc:0.975]
Epoch [89/120    avg_loss:0.057, val_acc:0.967]
Epoch [90/120    avg_loss:0.036, val_acc:0.975]
Epoch [91/120    avg_loss:0.025, val_acc:0.977]
Epoch [92/120    avg_loss:0.022, val_acc:0.977]
Epoch [93/120    avg_loss:0.020, val_acc:0.980]
Epoch [94/120    avg_loss:0.016, val_acc:0.978]
Epoch [95/120    avg_loss:0.019, val_acc:0.978]
Epoch [96/120    avg_loss:0.019, val_acc:0.981]
Epoch [97/120    avg_loss:0.019, val_acc:0.981]
Epoch [98/120    avg_loss:0.018, val_acc:0.982]
Epoch [99/120    avg_loss:0.017, val_acc:0.980]
Epoch [100/120    avg_loss:0.022, val_acc:0.983]
Epoch [101/120    avg_loss:0.018, val_acc:0.984]
Epoch [102/120    avg_loss:0.014, val_acc:0.985]
Epoch [103/120    avg_loss:0.014, val_acc:0.983]
Epoch [104/120    avg_loss:0.018, val_acc:0.985]
Epoch [105/120    avg_loss:0.015, val_acc:0.984]
Epoch [106/120    avg_loss:0.017, val_acc:0.983]
Epoch [107/120    avg_loss:0.016, val_acc:0.985]
Epoch [108/120    avg_loss:0.020, val_acc:0.984]
Epoch [109/120    avg_loss:0.015, val_acc:0.985]
Epoch [110/120    avg_loss:0.015, val_acc:0.984]
Epoch [111/120    avg_loss:0.014, val_acc:0.983]
Epoch [112/120    avg_loss:0.016, val_acc:0.984]
Epoch [113/120    avg_loss:0.018, val_acc:0.985]
Epoch [114/120    avg_loss:0.016, val_acc:0.985]
Epoch [115/120    avg_loss:0.015, val_acc:0.985]
Epoch [116/120    avg_loss:0.017, val_acc:0.985]
Epoch [117/120    avg_loss:0.016, val_acc:0.985]
Epoch [118/120    avg_loss:0.016, val_acc:0.988]
Epoch [119/120    avg_loss:0.017, val_acc:0.988]
Epoch [120/120    avg_loss:0.013, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1248    2    0    3    1    0    0    0   17   12    2    0
     0    0    0]
 [   0    0    4  721    0   12    0    0    0    4    0    0    5    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    1    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   15    0    0    2    0
     0    0    0]
 [   0    0    9   76    0    8    0    0    0    0  769   11    0    0
     0    2    0]
 [   0    0   14    0    0    2    8    0    3    0   13 2168    1    1
     0    0    0]
 [   0    0    1    3    7   15    0    0    0    0    1    1  503    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    3    0    1    2    0    0
  1125    0    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
    70  260    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.10840108401084

F1 scores:
[       nan 0.975      0.97461929 0.93032258 0.98383372 0.93846154
 0.97907324 1.         0.99307159 0.78947368 0.91656734 0.98411257
 0.95992366 0.99462366 0.96112772 0.85385878 0.97647059]

Kappa:
0.955628886303435
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6dacbc2748>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.581, val_acc:0.467]
Epoch [2/120    avg_loss:2.133, val_acc:0.570]
Epoch [3/120    avg_loss:1.899, val_acc:0.585]
Epoch [4/120    avg_loss:1.679, val_acc:0.621]
Epoch [5/120    avg_loss:1.513, val_acc:0.607]
Epoch [6/120    avg_loss:1.367, val_acc:0.675]
Epoch [7/120    avg_loss:1.132, val_acc:0.705]
Epoch [8/120    avg_loss:1.047, val_acc:0.728]
Epoch [9/120    avg_loss:0.920, val_acc:0.734]
Epoch [10/120    avg_loss:0.859, val_acc:0.724]
Epoch [11/120    avg_loss:0.686, val_acc:0.779]
Epoch [12/120    avg_loss:0.723, val_acc:0.757]
Epoch [13/120    avg_loss:0.719, val_acc:0.761]
Epoch [14/120    avg_loss:0.637, val_acc:0.786]
Epoch [15/120    avg_loss:0.569, val_acc:0.786]
Epoch [16/120    avg_loss:0.540, val_acc:0.721]
Epoch [17/120    avg_loss:0.605, val_acc:0.796]
Epoch [18/120    avg_loss:0.497, val_acc:0.835]
Epoch [19/120    avg_loss:0.455, val_acc:0.818]
Epoch [20/120    avg_loss:0.340, val_acc:0.883]
Epoch [21/120    avg_loss:0.350, val_acc:0.863]
Epoch [22/120    avg_loss:0.292, val_acc:0.866]
Epoch [23/120    avg_loss:0.318, val_acc:0.843]
Epoch [24/120    avg_loss:0.357, val_acc:0.884]
Epoch [25/120    avg_loss:0.373, val_acc:0.834]
Epoch [26/120    avg_loss:0.271, val_acc:0.891]
Epoch [27/120    avg_loss:0.288, val_acc:0.891]
Epoch [28/120    avg_loss:0.241, val_acc:0.890]
Epoch [29/120    avg_loss:0.246, val_acc:0.908]
Epoch [30/120    avg_loss:0.259, val_acc:0.874]
Epoch [31/120    avg_loss:0.251, val_acc:0.881]
Epoch [32/120    avg_loss:0.260, val_acc:0.896]
Epoch [33/120    avg_loss:0.192, val_acc:0.916]
Epoch [34/120    avg_loss:0.194, val_acc:0.869]
Epoch [35/120    avg_loss:0.270, val_acc:0.853]
Epoch [36/120    avg_loss:0.237, val_acc:0.900]
Epoch [37/120    avg_loss:0.192, val_acc:0.916]
Epoch [38/120    avg_loss:0.205, val_acc:0.902]
Epoch [39/120    avg_loss:0.160, val_acc:0.931]
Epoch [40/120    avg_loss:0.199, val_acc:0.892]
Epoch [41/120    avg_loss:0.193, val_acc:0.910]
Epoch [42/120    avg_loss:0.168, val_acc:0.934]
Epoch [43/120    avg_loss:0.143, val_acc:0.924]
Epoch [44/120    avg_loss:0.145, val_acc:0.930]
Epoch [45/120    avg_loss:0.124, val_acc:0.932]
Epoch [46/120    avg_loss:0.190, val_acc:0.919]
Epoch [47/120    avg_loss:0.187, val_acc:0.925]
Epoch [48/120    avg_loss:0.138, val_acc:0.924]
Epoch [49/120    avg_loss:0.102, val_acc:0.934]
Epoch [50/120    avg_loss:0.100, val_acc:0.942]
Epoch [51/120    avg_loss:0.115, val_acc:0.948]
Epoch [52/120    avg_loss:0.117, val_acc:0.946]
Epoch [53/120    avg_loss:0.112, val_acc:0.942]
Epoch [54/120    avg_loss:0.082, val_acc:0.948]
Epoch [55/120    avg_loss:0.078, val_acc:0.948]
Epoch [56/120    avg_loss:0.094, val_acc:0.921]
Epoch [57/120    avg_loss:0.083, val_acc:0.942]
Epoch [58/120    avg_loss:0.087, val_acc:0.940]
Epoch [59/120    avg_loss:0.091, val_acc:0.953]
Epoch [60/120    avg_loss:0.056, val_acc:0.955]
Epoch [61/120    avg_loss:0.060, val_acc:0.949]
Epoch [62/120    avg_loss:0.067, val_acc:0.939]
Epoch [63/120    avg_loss:0.086, val_acc:0.952]
Epoch [64/120    avg_loss:0.061, val_acc:0.946]
Epoch [65/120    avg_loss:0.060, val_acc:0.953]
Epoch [66/120    avg_loss:0.064, val_acc:0.944]
Epoch [67/120    avg_loss:0.061, val_acc:0.946]
Epoch [68/120    avg_loss:0.053, val_acc:0.960]
Epoch [69/120    avg_loss:0.050, val_acc:0.933]
Epoch [70/120    avg_loss:0.065, val_acc:0.950]
Epoch [71/120    avg_loss:0.048, val_acc:0.951]
Epoch [72/120    avg_loss:0.040, val_acc:0.949]
Epoch [73/120    avg_loss:0.051, val_acc:0.940]
Epoch [74/120    avg_loss:0.043, val_acc:0.942]
Epoch [75/120    avg_loss:0.051, val_acc:0.959]
Epoch [76/120    avg_loss:0.034, val_acc:0.962]
Epoch [77/120    avg_loss:0.050, val_acc:0.958]
Epoch [78/120    avg_loss:0.039, val_acc:0.964]
Epoch [79/120    avg_loss:0.038, val_acc:0.965]
Epoch [80/120    avg_loss:0.035, val_acc:0.945]
Epoch [81/120    avg_loss:0.038, val_acc:0.962]
Epoch [82/120    avg_loss:0.031, val_acc:0.963]
Epoch [83/120    avg_loss:0.032, val_acc:0.970]
Epoch [84/120    avg_loss:0.039, val_acc:0.963]
Epoch [85/120    avg_loss:0.052, val_acc:0.956]
Epoch [86/120    avg_loss:0.037, val_acc:0.954]
Epoch [87/120    avg_loss:0.034, val_acc:0.956]
Epoch [88/120    avg_loss:0.069, val_acc:0.938]
Epoch [89/120    avg_loss:0.075, val_acc:0.963]
Epoch [90/120    avg_loss:0.091, val_acc:0.946]
Epoch [91/120    avg_loss:0.073, val_acc:0.952]
Epoch [92/120    avg_loss:0.056, val_acc:0.955]
Epoch [93/120    avg_loss:0.045, val_acc:0.953]
Epoch [94/120    avg_loss:0.068, val_acc:0.946]
Epoch [95/120    avg_loss:0.051, val_acc:0.951]
Epoch [96/120    avg_loss:0.043, val_acc:0.958]
Epoch [97/120    avg_loss:0.026, val_acc:0.958]
Epoch [98/120    avg_loss:0.023, val_acc:0.961]
Epoch [99/120    avg_loss:0.020, val_acc:0.961]
Epoch [100/120    avg_loss:0.028, val_acc:0.964]
Epoch [101/120    avg_loss:0.019, val_acc:0.964]
Epoch [102/120    avg_loss:0.016, val_acc:0.964]
Epoch [103/120    avg_loss:0.018, val_acc:0.965]
Epoch [104/120    avg_loss:0.025, val_acc:0.967]
Epoch [105/120    avg_loss:0.016, val_acc:0.967]
Epoch [106/120    avg_loss:0.017, val_acc:0.967]
Epoch [107/120    avg_loss:0.017, val_acc:0.967]
Epoch [108/120    avg_loss:0.014, val_acc:0.969]
Epoch [109/120    avg_loss:0.014, val_acc:0.968]
Epoch [110/120    avg_loss:0.016, val_acc:0.967]
Epoch [111/120    avg_loss:0.016, val_acc:0.967]
Epoch [112/120    avg_loss:0.018, val_acc:0.967]
Epoch [113/120    avg_loss:0.018, val_acc:0.967]
Epoch [114/120    avg_loss:0.018, val_acc:0.967]
Epoch [115/120    avg_loss:0.015, val_acc:0.967]
Epoch [116/120    avg_loss:0.014, val_acc:0.967]
Epoch [117/120    avg_loss:0.016, val_acc:0.967]
Epoch [118/120    avg_loss:0.017, val_acc:0.967]
Epoch [119/120    avg_loss:0.017, val_acc:0.967]
Epoch [120/120    avg_loss:0.019, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    1 1242    3    4    0    3    0    0    0    4   22    6    0
     0    0    0]
 [   0    0    1  717    0    3    0    0    0    2    0    0   24    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    2    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   10    0    0    8    0
     0    0    0]
 [   0    0   20   90    0    7    0    0    0    0  752    0    0    0
     0    6    0]
 [   0    0   35    0    0    0    8    0    0    0   16 2149    2    0
     0    0    0]
 [   0    0    1    3    1    3    0    0    0    0    5    0  517    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    1    0    0
  1136    0    0]
 [   0    0    0    0    0    1   21    0    0    0    0    0    0    0
    84  241    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.60975609756098

F1 scores:
[       nan 0.92682927 0.96130031 0.91923077 0.98839907 0.98185941
 0.97164179 0.98039216 0.99650757 0.64516129 0.90821256 0.98038321
 0.94688645 1.         0.96148963 0.81144781 0.97076023]

Kappa:
0.9499448614740243
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faf0804c7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.599, val_acc:0.394]
Epoch [2/120    avg_loss:2.182, val_acc:0.490]
Epoch [3/120    avg_loss:1.947, val_acc:0.519]
Epoch [4/120    avg_loss:1.735, val_acc:0.593]
Epoch [5/120    avg_loss:1.635, val_acc:0.621]
Epoch [6/120    avg_loss:1.413, val_acc:0.615]
Epoch [7/120    avg_loss:1.246, val_acc:0.686]
Epoch [8/120    avg_loss:1.051, val_acc:0.699]
Epoch [9/120    avg_loss:0.906, val_acc:0.717]
Epoch [10/120    avg_loss:0.812, val_acc:0.767]
Epoch [11/120    avg_loss:0.750, val_acc:0.790]
Epoch [12/120    avg_loss:0.654, val_acc:0.800]
Epoch [13/120    avg_loss:0.661, val_acc:0.761]
Epoch [14/120    avg_loss:0.586, val_acc:0.817]
Epoch [15/120    avg_loss:0.589, val_acc:0.779]
Epoch [16/120    avg_loss:0.489, val_acc:0.834]
Epoch [17/120    avg_loss:0.488, val_acc:0.820]
Epoch [18/120    avg_loss:0.635, val_acc:0.818]
Epoch [19/120    avg_loss:0.542, val_acc:0.834]
Epoch [20/120    avg_loss:0.439, val_acc:0.835]
Epoch [21/120    avg_loss:0.428, val_acc:0.828]
Epoch [22/120    avg_loss:0.433, val_acc:0.845]
Epoch [23/120    avg_loss:0.328, val_acc:0.859]
Epoch [24/120    avg_loss:0.371, val_acc:0.874]
Epoch [25/120    avg_loss:0.268, val_acc:0.892]
Epoch [26/120    avg_loss:0.259, val_acc:0.898]
Epoch [27/120    avg_loss:0.323, val_acc:0.871]
Epoch [28/120    avg_loss:0.313, val_acc:0.883]
Epoch [29/120    avg_loss:0.235, val_acc:0.877]
Epoch [30/120    avg_loss:0.241, val_acc:0.907]
Epoch [31/120    avg_loss:0.227, val_acc:0.904]
Epoch [32/120    avg_loss:0.188, val_acc:0.884]
Epoch [33/120    avg_loss:0.218, val_acc:0.888]
Epoch [34/120    avg_loss:0.176, val_acc:0.916]
Epoch [35/120    avg_loss:0.187, val_acc:0.926]
Epoch [36/120    avg_loss:0.181, val_acc:0.926]
Epoch [37/120    avg_loss:0.180, val_acc:0.913]
Epoch [38/120    avg_loss:0.174, val_acc:0.921]
Epoch [39/120    avg_loss:0.241, val_acc:0.915]
Epoch [40/120    avg_loss:0.182, val_acc:0.910]
Epoch [41/120    avg_loss:0.175, val_acc:0.936]
Epoch [42/120    avg_loss:0.141, val_acc:0.924]
Epoch [43/120    avg_loss:0.120, val_acc:0.939]
Epoch [44/120    avg_loss:0.133, val_acc:0.924]
Epoch [45/120    avg_loss:0.112, val_acc:0.920]
Epoch [46/120    avg_loss:0.124, val_acc:0.950]
Epoch [47/120    avg_loss:0.137, val_acc:0.895]
Epoch [48/120    avg_loss:0.137, val_acc:0.943]
Epoch [49/120    avg_loss:0.112, val_acc:0.931]
Epoch [50/120    avg_loss:0.104, val_acc:0.933]
Epoch [51/120    avg_loss:0.090, val_acc:0.942]
Epoch [52/120    avg_loss:0.138, val_acc:0.945]
Epoch [53/120    avg_loss:0.105, val_acc:0.948]
Epoch [54/120    avg_loss:0.100, val_acc:0.962]
Epoch [55/120    avg_loss:0.138, val_acc:0.945]
Epoch [56/120    avg_loss:0.118, val_acc:0.946]
Epoch [57/120    avg_loss:0.120, val_acc:0.939]
Epoch [58/120    avg_loss:0.098, val_acc:0.943]
Epoch [59/120    avg_loss:0.118, val_acc:0.940]
Epoch [60/120    avg_loss:0.091, val_acc:0.948]
Epoch [61/120    avg_loss:0.071, val_acc:0.953]
Epoch [62/120    avg_loss:0.078, val_acc:0.933]
Epoch [63/120    avg_loss:0.082, val_acc:0.941]
Epoch [64/120    avg_loss:0.061, val_acc:0.952]
Epoch [65/120    avg_loss:0.057, val_acc:0.961]
Epoch [66/120    avg_loss:0.103, val_acc:0.946]
Epoch [67/120    avg_loss:0.058, val_acc:0.953]
Epoch [68/120    avg_loss:0.041, val_acc:0.960]
Epoch [69/120    avg_loss:0.043, val_acc:0.965]
Epoch [70/120    avg_loss:0.042, val_acc:0.967]
Epoch [71/120    avg_loss:0.039, val_acc:0.967]
Epoch [72/120    avg_loss:0.037, val_acc:0.970]
Epoch [73/120    avg_loss:0.039, val_acc:0.967]
Epoch [74/120    avg_loss:0.039, val_acc:0.969]
Epoch [75/120    avg_loss:0.029, val_acc:0.967]
Epoch [76/120    avg_loss:0.038, val_acc:0.968]
Epoch [77/120    avg_loss:0.031, val_acc:0.968]
Epoch [78/120    avg_loss:0.032, val_acc:0.970]
Epoch [79/120    avg_loss:0.032, val_acc:0.969]
Epoch [80/120    avg_loss:0.034, val_acc:0.972]
Epoch [81/120    avg_loss:0.035, val_acc:0.967]
Epoch [82/120    avg_loss:0.031, val_acc:0.967]
Epoch [83/120    avg_loss:0.028, val_acc:0.969]
Epoch [84/120    avg_loss:0.035, val_acc:0.969]
Epoch [85/120    avg_loss:0.030, val_acc:0.969]
Epoch [86/120    avg_loss:0.030, val_acc:0.972]
Epoch [87/120    avg_loss:0.028, val_acc:0.971]
Epoch [88/120    avg_loss:0.031, val_acc:0.969]
Epoch [89/120    avg_loss:0.029, val_acc:0.970]
Epoch [90/120    avg_loss:0.029, val_acc:0.972]
Epoch [91/120    avg_loss:0.030, val_acc:0.970]
Epoch [92/120    avg_loss:0.039, val_acc:0.975]
Epoch [93/120    avg_loss:0.028, val_acc:0.973]
Epoch [94/120    avg_loss:0.031, val_acc:0.974]
Epoch [95/120    avg_loss:0.032, val_acc:0.972]
Epoch [96/120    avg_loss:0.034, val_acc:0.973]
Epoch [97/120    avg_loss:0.030, val_acc:0.969]
Epoch [98/120    avg_loss:0.031, val_acc:0.972]
Epoch [99/120    avg_loss:0.025, val_acc:0.974]
Epoch [100/120    avg_loss:0.030, val_acc:0.971]
Epoch [101/120    avg_loss:0.022, val_acc:0.973]
Epoch [102/120    avg_loss:0.030, val_acc:0.971]
Epoch [103/120    avg_loss:0.026, val_acc:0.969]
Epoch [104/120    avg_loss:0.027, val_acc:0.970]
Epoch [105/120    avg_loss:0.025, val_acc:0.971]
Epoch [106/120    avg_loss:0.022, val_acc:0.971]
Epoch [107/120    avg_loss:0.025, val_acc:0.970]
Epoch [108/120    avg_loss:0.026, val_acc:0.970]
Epoch [109/120    avg_loss:0.029, val_acc:0.970]
Epoch [110/120    avg_loss:0.028, val_acc:0.970]
Epoch [111/120    avg_loss:0.024, val_acc:0.972]
Epoch [112/120    avg_loss:0.023, val_acc:0.970]
Epoch [113/120    avg_loss:0.025, val_acc:0.971]
Epoch [114/120    avg_loss:0.027, val_acc:0.971]
Epoch [115/120    avg_loss:0.027, val_acc:0.970]
Epoch [116/120    avg_loss:0.027, val_acc:0.968]
Epoch [117/120    avg_loss:0.029, val_acc:0.969]
Epoch [118/120    avg_loss:0.025, val_acc:0.969]
Epoch [119/120    avg_loss:0.027, val_acc:0.969]
Epoch [120/120    avg_loss:0.034, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1255    3    0    0    3    0    0    3    6   13    2    0
     0    0    0]
 [   0    0    4  732    0    2    0    0    0    5    0    0    3    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0   25   90    0    7    0    0    0    0  750    0    0    0
     0    3    0]
 [   0    0   32    0    0    3    3    0    1    0   37 2130    0    2
     2    0    0]
 [   0    0    2   27   13    0    0    0    0    0    3    7  478    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    3    0    4    0    0    0
  1131    0    0]
 [   0    0    0    0    0    0    3    0    0    3    0    0    0    0
    91  250    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.43631436314364

F1 scores:
[       nan 0.975      0.96427199 0.91328759 0.97038724 0.98412698
 0.9924357  1.         0.99537037 0.63636364 0.89445438 0.97684017
 0.93817468 0.9919571  0.95725772 0.83333333 0.96470588]

Kappa:
0.947981447509872
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0a67c59898>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.593, val_acc:0.491]
Epoch [2/120    avg_loss:2.133, val_acc:0.542]
Epoch [3/120    avg_loss:1.897, val_acc:0.577]
Epoch [4/120    avg_loss:1.752, val_acc:0.603]
Epoch [5/120    avg_loss:1.513, val_acc:0.609]
Epoch [6/120    avg_loss:1.340, val_acc:0.629]
Epoch [7/120    avg_loss:1.184, val_acc:0.694]
Epoch [8/120    avg_loss:1.007, val_acc:0.706]
Epoch [9/120    avg_loss:0.875, val_acc:0.647]
Epoch [10/120    avg_loss:0.799, val_acc:0.759]
Epoch [11/120    avg_loss:0.762, val_acc:0.752]
Epoch [12/120    avg_loss:0.682, val_acc:0.765]
Epoch [13/120    avg_loss:0.610, val_acc:0.785]
Epoch [14/120    avg_loss:0.515, val_acc:0.791]
Epoch [15/120    avg_loss:0.471, val_acc:0.846]
Epoch [16/120    avg_loss:0.590, val_acc:0.826]
Epoch [17/120    avg_loss:0.518, val_acc:0.836]
Epoch [18/120    avg_loss:0.396, val_acc:0.830]
Epoch [19/120    avg_loss:0.349, val_acc:0.837]
Epoch [20/120    avg_loss:0.332, val_acc:0.859]
Epoch [21/120    avg_loss:0.365, val_acc:0.827]
Epoch [22/120    avg_loss:0.383, val_acc:0.852]
Epoch [23/120    avg_loss:0.315, val_acc:0.888]
Epoch [24/120    avg_loss:0.334, val_acc:0.879]
Epoch [25/120    avg_loss:0.294, val_acc:0.856]
Epoch [26/120    avg_loss:0.263, val_acc:0.856]
Epoch [27/120    avg_loss:0.303, val_acc:0.886]
Epoch [28/120    avg_loss:0.327, val_acc:0.874]
Epoch [29/120    avg_loss:0.226, val_acc:0.888]
Epoch [30/120    avg_loss:0.263, val_acc:0.907]
Epoch [31/120    avg_loss:0.154, val_acc:0.917]
Epoch [32/120    avg_loss:0.182, val_acc:0.923]
Epoch [33/120    avg_loss:0.178, val_acc:0.905]
Epoch [34/120    avg_loss:0.168, val_acc:0.923]
Epoch [35/120    avg_loss:0.168, val_acc:0.917]
Epoch [36/120    avg_loss:0.241, val_acc:0.884]
Epoch [37/120    avg_loss:0.180, val_acc:0.913]
Epoch [38/120    avg_loss:0.130, val_acc:0.929]
Epoch [39/120    avg_loss:0.122, val_acc:0.925]
Epoch [40/120    avg_loss:0.152, val_acc:0.897]
Epoch [41/120    avg_loss:0.135, val_acc:0.935]
Epoch [42/120    avg_loss:0.113, val_acc:0.927]
Epoch [43/120    avg_loss:0.126, val_acc:0.917]
Epoch [44/120    avg_loss:0.153, val_acc:0.921]
Epoch [45/120    avg_loss:0.117, val_acc:0.931]
Epoch [46/120    avg_loss:0.117, val_acc:0.939]
Epoch [47/120    avg_loss:0.080, val_acc:0.932]
Epoch [48/120    avg_loss:0.090, val_acc:0.926]
Epoch [49/120    avg_loss:0.107, val_acc:0.939]
Epoch [50/120    avg_loss:0.093, val_acc:0.949]
Epoch [51/120    avg_loss:0.100, val_acc:0.935]
Epoch [52/120    avg_loss:0.154, val_acc:0.933]
Epoch [53/120    avg_loss:0.141, val_acc:0.929]
Epoch [54/120    avg_loss:0.106, val_acc:0.935]
Epoch [55/120    avg_loss:0.073, val_acc:0.955]
Epoch [56/120    avg_loss:0.065, val_acc:0.959]
Epoch [57/120    avg_loss:0.060, val_acc:0.948]
Epoch [58/120    avg_loss:0.064, val_acc:0.951]
Epoch [59/120    avg_loss:0.078, val_acc:0.944]
Epoch [60/120    avg_loss:0.077, val_acc:0.935]
Epoch [61/120    avg_loss:0.087, val_acc:0.941]
Epoch [62/120    avg_loss:0.079, val_acc:0.952]
Epoch [63/120    avg_loss:0.059, val_acc:0.956]
Epoch [64/120    avg_loss:0.064, val_acc:0.941]
Epoch [65/120    avg_loss:0.060, val_acc:0.954]
Epoch [66/120    avg_loss:0.255, val_acc:0.913]
Epoch [67/120    avg_loss:0.108, val_acc:0.905]
Epoch [68/120    avg_loss:0.098, val_acc:0.930]
Epoch [69/120    avg_loss:0.094, val_acc:0.932]
Epoch [70/120    avg_loss:0.097, val_acc:0.946]
Epoch [71/120    avg_loss:0.064, val_acc:0.958]
Epoch [72/120    avg_loss:0.062, val_acc:0.960]
Epoch [73/120    avg_loss:0.046, val_acc:0.958]
Epoch [74/120    avg_loss:0.060, val_acc:0.960]
Epoch [75/120    avg_loss:0.051, val_acc:0.964]
Epoch [76/120    avg_loss:0.043, val_acc:0.962]
Epoch [77/120    avg_loss:0.041, val_acc:0.965]
Epoch [78/120    avg_loss:0.037, val_acc:0.963]
Epoch [79/120    avg_loss:0.051, val_acc:0.961]
Epoch [80/120    avg_loss:0.037, val_acc:0.964]
Epoch [81/120    avg_loss:0.035, val_acc:0.964]
Epoch [82/120    avg_loss:0.038, val_acc:0.964]
Epoch [83/120    avg_loss:0.041, val_acc:0.967]
Epoch [84/120    avg_loss:0.033, val_acc:0.968]
Epoch [85/120    avg_loss:0.035, val_acc:0.967]
Epoch [86/120    avg_loss:0.035, val_acc:0.968]
Epoch [87/120    avg_loss:0.032, val_acc:0.968]
Epoch [88/120    avg_loss:0.034, val_acc:0.970]
Epoch [89/120    avg_loss:0.033, val_acc:0.967]
Epoch [90/120    avg_loss:0.036, val_acc:0.965]
Epoch [91/120    avg_loss:0.032, val_acc:0.968]
Epoch [92/120    avg_loss:0.031, val_acc:0.968]
Epoch [93/120    avg_loss:0.028, val_acc:0.965]
Epoch [94/120    avg_loss:0.031, val_acc:0.968]
Epoch [95/120    avg_loss:0.038, val_acc:0.967]
Epoch [96/120    avg_loss:0.029, val_acc:0.967]
Epoch [97/120    avg_loss:0.025, val_acc:0.965]
Epoch [98/120    avg_loss:0.028, val_acc:0.964]
Epoch [99/120    avg_loss:0.031, val_acc:0.965]
Epoch [100/120    avg_loss:0.033, val_acc:0.964]
Epoch [101/120    avg_loss:0.032, val_acc:0.967]
Epoch [102/120    avg_loss:0.033, val_acc:0.967]
Epoch [103/120    avg_loss:0.029, val_acc:0.967]
Epoch [104/120    avg_loss:0.028, val_acc:0.967]
Epoch [105/120    avg_loss:0.032, val_acc:0.967]
Epoch [106/120    avg_loss:0.029, val_acc:0.967]
Epoch [107/120    avg_loss:0.024, val_acc:0.967]
Epoch [108/120    avg_loss:0.035, val_acc:0.967]
Epoch [109/120    avg_loss:0.033, val_acc:0.968]
Epoch [110/120    avg_loss:0.026, val_acc:0.967]
Epoch [111/120    avg_loss:0.028, val_acc:0.968]
Epoch [112/120    avg_loss:0.027, val_acc:0.968]
Epoch [113/120    avg_loss:0.029, val_acc:0.968]
Epoch [114/120    avg_loss:0.028, val_acc:0.967]
Epoch [115/120    avg_loss:0.032, val_acc:0.967]
Epoch [116/120    avg_loss:0.026, val_acc:0.967]
Epoch [117/120    avg_loss:0.031, val_acc:0.967]
Epoch [118/120    avg_loss:0.029, val_acc:0.967]
Epoch [119/120    avg_loss:0.024, val_acc:0.967]
Epoch [120/120    avg_loss:0.031, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1257    3    0    0    4    0    0    0    4   15    1    0
     0    1    0]
 [   0    0    1  702    0   18    0    0    0   17    0    0    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    3    0    4    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    1    0    0   13    0    1    2    0
     0    0    0]
 [   0    0   22   86    0    6    2    0    0    0  745    9    5    0
     0    0    0]
 [   0    0   23    0    0    2    6    0    3    0   16 2153    1    6
     0    0    0]
 [   0    0    9   16    6    9    0    0    0    0    7   10  466    0
     0    0   11]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    3    0    4    2    0    0
  1130    0    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    74  261    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.17615176151762

F1 scores:
[       nan 0.975      0.96804005 0.90289389 0.98611111 0.94854586
 0.97904192 0.94339623 0.99307159 0.5        0.90139141 0.97796957
 0.91462218 0.98404255 0.9629314  0.85714286 0.93258427]

Kappa:
0.9449951063884182
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa56fa097b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.661, val_acc:0.380]
Epoch [2/120    avg_loss:2.183, val_acc:0.510]
Epoch [3/120    avg_loss:1.915, val_acc:0.503]
Epoch [4/120    avg_loss:1.715, val_acc:0.562]
Epoch [5/120    avg_loss:1.539, val_acc:0.611]
Epoch [6/120    avg_loss:1.380, val_acc:0.649]
Epoch [7/120    avg_loss:1.188, val_acc:0.658]
Epoch [8/120    avg_loss:1.051, val_acc:0.684]
Epoch [9/120    avg_loss:0.897, val_acc:0.722]
Epoch [10/120    avg_loss:0.879, val_acc:0.748]
Epoch [11/120    avg_loss:0.757, val_acc:0.778]
Epoch [12/120    avg_loss:0.623, val_acc:0.758]
Epoch [13/120    avg_loss:0.594, val_acc:0.818]
Epoch [14/120    avg_loss:0.509, val_acc:0.818]
Epoch [15/120    avg_loss:0.429, val_acc:0.802]
Epoch [16/120    avg_loss:0.441, val_acc:0.839]
Epoch [17/120    avg_loss:0.417, val_acc:0.817]
Epoch [18/120    avg_loss:0.334, val_acc:0.860]
Epoch [19/120    avg_loss:0.325, val_acc:0.884]
Epoch [20/120    avg_loss:0.305, val_acc:0.895]
Epoch [21/120    avg_loss:0.267, val_acc:0.895]
Epoch [22/120    avg_loss:0.244, val_acc:0.874]
Epoch [23/120    avg_loss:0.263, val_acc:0.865]
Epoch [24/120    avg_loss:0.227, val_acc:0.905]
Epoch [25/120    avg_loss:0.186, val_acc:0.904]
Epoch [26/120    avg_loss:0.172, val_acc:0.908]
Epoch [27/120    avg_loss:0.149, val_acc:0.927]
Epoch [28/120    avg_loss:0.117, val_acc:0.916]
Epoch [29/120    avg_loss:0.118, val_acc:0.934]
Epoch [30/120    avg_loss:0.095, val_acc:0.935]
Epoch [31/120    avg_loss:0.084, val_acc:0.925]
Epoch [32/120    avg_loss:0.090, val_acc:0.939]
Epoch [33/120    avg_loss:0.112, val_acc:0.929]
Epoch [34/120    avg_loss:0.092, val_acc:0.912]
Epoch [35/120    avg_loss:0.093, val_acc:0.928]
Epoch [36/120    avg_loss:0.082, val_acc:0.941]
Epoch [37/120    avg_loss:0.066, val_acc:0.952]
Epoch [38/120    avg_loss:0.062, val_acc:0.953]
Epoch [39/120    avg_loss:0.069, val_acc:0.960]
Epoch [40/120    avg_loss:0.062, val_acc:0.955]
Epoch [41/120    avg_loss:0.055, val_acc:0.934]
Epoch [42/120    avg_loss:0.072, val_acc:0.965]
Epoch [43/120    avg_loss:0.055, val_acc:0.964]
Epoch [44/120    avg_loss:0.048, val_acc:0.964]
Epoch [45/120    avg_loss:0.069, val_acc:0.940]
Epoch [46/120    avg_loss:0.069, val_acc:0.948]
Epoch [47/120    avg_loss:0.068, val_acc:0.934]
Epoch [48/120    avg_loss:0.057, val_acc:0.956]
Epoch [49/120    avg_loss:0.044, val_acc:0.956]
Epoch [50/120    avg_loss:0.044, val_acc:0.939]
Epoch [51/120    avg_loss:0.042, val_acc:0.948]
Epoch [52/120    avg_loss:0.049, val_acc:0.950]
Epoch [53/120    avg_loss:0.039, val_acc:0.961]
Epoch [54/120    avg_loss:0.034, val_acc:0.960]
Epoch [55/120    avg_loss:0.031, val_acc:0.970]
Epoch [56/120    avg_loss:0.035, val_acc:0.968]
Epoch [57/120    avg_loss:0.028, val_acc:0.958]
Epoch [58/120    avg_loss:0.029, val_acc:0.968]
Epoch [59/120    avg_loss:0.027, val_acc:0.968]
Epoch [60/120    avg_loss:0.022, val_acc:0.977]
Epoch [61/120    avg_loss:0.024, val_acc:0.949]
Epoch [62/120    avg_loss:0.040, val_acc:0.964]
Epoch [63/120    avg_loss:0.044, val_acc:0.944]
Epoch [64/120    avg_loss:0.034, val_acc:0.958]
Epoch [65/120    avg_loss:0.029, val_acc:0.966]
Epoch [66/120    avg_loss:0.023, val_acc:0.964]
Epoch [67/120    avg_loss:0.037, val_acc:0.943]
Epoch [68/120    avg_loss:0.047, val_acc:0.957]
Epoch [69/120    avg_loss:0.025, val_acc:0.966]
Epoch [70/120    avg_loss:0.026, val_acc:0.959]
Epoch [71/120    avg_loss:0.020, val_acc:0.972]
Epoch [72/120    avg_loss:0.016, val_acc:0.968]
Epoch [73/120    avg_loss:0.024, val_acc:0.967]
Epoch [74/120    avg_loss:0.016, val_acc:0.973]
Epoch [75/120    avg_loss:0.012, val_acc:0.974]
Epoch [76/120    avg_loss:0.014, val_acc:0.975]
Epoch [77/120    avg_loss:0.011, val_acc:0.976]
Epoch [78/120    avg_loss:0.012, val_acc:0.977]
Epoch [79/120    avg_loss:0.012, val_acc:0.978]
Epoch [80/120    avg_loss:0.011, val_acc:0.978]
Epoch [81/120    avg_loss:0.010, val_acc:0.977]
Epoch [82/120    avg_loss:0.011, val_acc:0.977]
Epoch [83/120    avg_loss:0.010, val_acc:0.976]
Epoch [84/120    avg_loss:0.011, val_acc:0.977]
Epoch [85/120    avg_loss:0.010, val_acc:0.977]
Epoch [86/120    avg_loss:0.008, val_acc:0.978]
Epoch [87/120    avg_loss:0.009, val_acc:0.978]
Epoch [88/120    avg_loss:0.011, val_acc:0.978]
Epoch [89/120    avg_loss:0.011, val_acc:0.977]
Epoch [90/120    avg_loss:0.009, val_acc:0.977]
Epoch [91/120    avg_loss:0.009, val_acc:0.978]
Epoch [92/120    avg_loss:0.009, val_acc:0.977]
Epoch [93/120    avg_loss:0.008, val_acc:0.978]
Epoch [94/120    avg_loss:0.011, val_acc:0.977]
Epoch [95/120    avg_loss:0.009, val_acc:0.978]
Epoch [96/120    avg_loss:0.009, val_acc:0.978]
Epoch [97/120    avg_loss:0.009, val_acc:0.977]
Epoch [98/120    avg_loss:0.011, val_acc:0.980]
Epoch [99/120    avg_loss:0.009, val_acc:0.978]
Epoch [100/120    avg_loss:0.008, val_acc:0.978]
Epoch [101/120    avg_loss:0.007, val_acc:0.978]
Epoch [102/120    avg_loss:0.010, val_acc:0.979]
Epoch [103/120    avg_loss:0.009, val_acc:0.978]
Epoch [104/120    avg_loss:0.012, val_acc:0.981]
Epoch [105/120    avg_loss:0.007, val_acc:0.979]
Epoch [106/120    avg_loss:0.013, val_acc:0.978]
Epoch [107/120    avg_loss:0.008, val_acc:0.979]
Epoch [108/120    avg_loss:0.009, val_acc:0.978]
Epoch [109/120    avg_loss:0.009, val_acc:0.979]
Epoch [110/120    avg_loss:0.008, val_acc:0.978]
Epoch [111/120    avg_loss:0.008, val_acc:0.980]
Epoch [112/120    avg_loss:0.010, val_acc:0.977]
Epoch [113/120    avg_loss:0.008, val_acc:0.979]
Epoch [114/120    avg_loss:0.010, val_acc:0.980]
Epoch [115/120    avg_loss:0.009, val_acc:0.978]
Epoch [116/120    avg_loss:0.008, val_acc:0.979]
Epoch [117/120    avg_loss:0.007, val_acc:0.979]
Epoch [118/120    avg_loss:0.008, val_acc:0.979]
Epoch [119/120    avg_loss:0.008, val_acc:0.979]
Epoch [120/120    avg_loss:0.008, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1256    2    7    0    0    0    0    0    4   14    0    2
     0    0    0]
 [   0    0    0  720    3    0    0    0    0    1    1   17    2    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    2    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    0    0    0    0    0  843   23    0    0
     0    0    0]
 [   0    2    4    0    0    1    0    0    0    0   26 2154   22    0
     0    1    0]
 [   0    0    0    1    0    0    0    0    0    0    0    1  527    0
     2    2    1]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1125   14    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    62  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.4308943089431

F1 scores:
[       nan 0.95121951 0.98355521 0.97959184 0.97706422 0.99885189
 0.99695586 1.         0.99883586 0.91428571 0.96232877 0.97466063
 0.96875    0.98395722 0.96607986 0.87827427 0.98203593]

Kappa:
0.9707023307896705
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa0c235a860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.641, val_acc:0.420]
Epoch [2/120    avg_loss:2.222, val_acc:0.377]
Epoch [3/120    avg_loss:2.017, val_acc:0.463]
Epoch [4/120    avg_loss:1.840, val_acc:0.532]
Epoch [5/120    avg_loss:1.647, val_acc:0.608]
Epoch [6/120    avg_loss:1.477, val_acc:0.557]
Epoch [7/120    avg_loss:1.338, val_acc:0.616]
Epoch [8/120    avg_loss:1.187, val_acc:0.619]
Epoch [9/120    avg_loss:1.052, val_acc:0.698]
Epoch [10/120    avg_loss:0.977, val_acc:0.694]
Epoch [11/120    avg_loss:0.880, val_acc:0.726]
Epoch [12/120    avg_loss:0.752, val_acc:0.749]
Epoch [13/120    avg_loss:0.647, val_acc:0.753]
Epoch [14/120    avg_loss:0.601, val_acc:0.821]
Epoch [15/120    avg_loss:0.577, val_acc:0.759]
Epoch [16/120    avg_loss:0.589, val_acc:0.782]
Epoch [17/120    avg_loss:0.464, val_acc:0.803]
Epoch [18/120    avg_loss:0.428, val_acc:0.852]
Epoch [19/120    avg_loss:0.379, val_acc:0.809]
Epoch [20/120    avg_loss:0.347, val_acc:0.849]
Epoch [21/120    avg_loss:0.255, val_acc:0.881]
Epoch [22/120    avg_loss:0.281, val_acc:0.878]
Epoch [23/120    avg_loss:0.216, val_acc:0.897]
Epoch [24/120    avg_loss:0.251, val_acc:0.904]
Epoch [25/120    avg_loss:0.199, val_acc:0.892]
Epoch [26/120    avg_loss:0.190, val_acc:0.901]
Epoch [27/120    avg_loss:0.334, val_acc:0.873]
Epoch [28/120    avg_loss:0.329, val_acc:0.897]
Epoch [29/120    avg_loss:0.223, val_acc:0.902]
Epoch [30/120    avg_loss:0.164, val_acc:0.931]
Epoch [31/120    avg_loss:0.156, val_acc:0.918]
Epoch [32/120    avg_loss:0.134, val_acc:0.915]
Epoch [33/120    avg_loss:0.165, val_acc:0.874]
Epoch [34/120    avg_loss:0.126, val_acc:0.941]
Epoch [35/120    avg_loss:0.128, val_acc:0.902]
Epoch [36/120    avg_loss:0.107, val_acc:0.930]
Epoch [37/120    avg_loss:0.124, val_acc:0.935]
Epoch [38/120    avg_loss:0.088, val_acc:0.941]
Epoch [39/120    avg_loss:0.073, val_acc:0.947]
Epoch [40/120    avg_loss:0.058, val_acc:0.955]
Epoch [41/120    avg_loss:0.062, val_acc:0.933]
Epoch [42/120    avg_loss:0.059, val_acc:0.955]
Epoch [43/120    avg_loss:0.065, val_acc:0.965]
Epoch [44/120    avg_loss:0.065, val_acc:0.951]
Epoch [45/120    avg_loss:0.053, val_acc:0.943]
Epoch [46/120    avg_loss:0.059, val_acc:0.944]
Epoch [47/120    avg_loss:0.052, val_acc:0.951]
Epoch [48/120    avg_loss:0.053, val_acc:0.951]
Epoch [49/120    avg_loss:0.055, val_acc:0.955]
Epoch [50/120    avg_loss:0.060, val_acc:0.953]
Epoch [51/120    avg_loss:0.048, val_acc:0.952]
Epoch [52/120    avg_loss:0.044, val_acc:0.955]
Epoch [53/120    avg_loss:0.061, val_acc:0.954]
Epoch [54/120    avg_loss:0.049, val_acc:0.966]
Epoch [55/120    avg_loss:0.052, val_acc:0.954]
Epoch [56/120    avg_loss:0.052, val_acc:0.950]
Epoch [57/120    avg_loss:0.040, val_acc:0.954]
Epoch [58/120    avg_loss:0.050, val_acc:0.968]
Epoch [59/120    avg_loss:0.065, val_acc:0.949]
Epoch [60/120    avg_loss:0.044, val_acc:0.959]
Epoch [61/120    avg_loss:0.051, val_acc:0.957]
Epoch [62/120    avg_loss:0.048, val_acc:0.955]
Epoch [63/120    avg_loss:0.047, val_acc:0.959]
Epoch [64/120    avg_loss:0.025, val_acc:0.967]
Epoch [65/120    avg_loss:0.023, val_acc:0.973]
Epoch [66/120    avg_loss:0.026, val_acc:0.968]
Epoch [67/120    avg_loss:0.031, val_acc:0.969]
Epoch [68/120    avg_loss:0.029, val_acc:0.969]
Epoch [69/120    avg_loss:0.058, val_acc:0.960]
Epoch [70/120    avg_loss:0.037, val_acc:0.968]
Epoch [71/120    avg_loss:0.023, val_acc:0.965]
Epoch [72/120    avg_loss:0.032, val_acc:0.968]
Epoch [73/120    avg_loss:0.017, val_acc:0.976]
Epoch [74/120    avg_loss:0.025, val_acc:0.969]
Epoch [75/120    avg_loss:0.026, val_acc:0.967]
Epoch [76/120    avg_loss:0.019, val_acc:0.973]
Epoch [77/120    avg_loss:0.023, val_acc:0.971]
Epoch [78/120    avg_loss:0.019, val_acc:0.973]
Epoch [79/120    avg_loss:0.020, val_acc:0.966]
Epoch [80/120    avg_loss:0.017, val_acc:0.972]
Epoch [81/120    avg_loss:0.011, val_acc:0.973]
Epoch [82/120    avg_loss:0.015, val_acc:0.972]
Epoch [83/120    avg_loss:0.012, val_acc:0.973]
Epoch [84/120    avg_loss:0.011, val_acc:0.976]
Epoch [85/120    avg_loss:0.012, val_acc:0.973]
Epoch [86/120    avg_loss:0.018, val_acc:0.968]
Epoch [87/120    avg_loss:0.014, val_acc:0.971]
Epoch [88/120    avg_loss:0.016, val_acc:0.974]
Epoch [89/120    avg_loss:0.012, val_acc:0.971]
Epoch [90/120    avg_loss:0.009, val_acc:0.972]
Epoch [91/120    avg_loss:0.010, val_acc:0.974]
Epoch [92/120    avg_loss:0.011, val_acc:0.971]
Epoch [93/120    avg_loss:0.009, val_acc:0.975]
Epoch [94/120    avg_loss:0.009, val_acc:0.971]
Epoch [95/120    avg_loss:0.009, val_acc:0.970]
Epoch [96/120    avg_loss:0.009, val_acc:0.969]
Epoch [97/120    avg_loss:0.015, val_acc:0.968]
Epoch [98/120    avg_loss:0.010, val_acc:0.971]
Epoch [99/120    avg_loss:0.007, val_acc:0.973]
Epoch [100/120    avg_loss:0.010, val_acc:0.974]
Epoch [101/120    avg_loss:0.007, val_acc:0.974]
Epoch [102/120    avg_loss:0.006, val_acc:0.975]
Epoch [103/120    avg_loss:0.006, val_acc:0.975]
Epoch [104/120    avg_loss:0.006, val_acc:0.975]
Epoch [105/120    avg_loss:0.007, val_acc:0.976]
Epoch [106/120    avg_loss:0.008, val_acc:0.975]
Epoch [107/120    avg_loss:0.006, val_acc:0.975]
Epoch [108/120    avg_loss:0.005, val_acc:0.974]
Epoch [109/120    avg_loss:0.005, val_acc:0.974]
Epoch [110/120    avg_loss:0.005, val_acc:0.973]
Epoch [111/120    avg_loss:0.006, val_acc:0.974]
Epoch [112/120    avg_loss:0.005, val_acc:0.974]
Epoch [113/120    avg_loss:0.005, val_acc:0.975]
Epoch [114/120    avg_loss:0.008, val_acc:0.974]
Epoch [115/120    avg_loss:0.006, val_acc:0.975]
Epoch [116/120    avg_loss:0.006, val_acc:0.977]
Epoch [117/120    avg_loss:0.005, val_acc:0.976]
Epoch [118/120    avg_loss:0.005, val_acc:0.975]
Epoch [119/120    avg_loss:0.005, val_acc:0.975]
Epoch [120/120    avg_loss:0.005, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1261    5    0    0    0    0    0    0    1   18    0    0
     0    0    0]
 [   0    0    1  726    0    1    0    0    0    1    0   14    4    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  849   20    0    0
     0    1    0]
 [   0    0    2    0    0    0    0    0    0    4    7 2174   23    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0    9  522    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1123   16    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
   102  244    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.23577235772358

F1 scores:
[       nan 0.975      0.98747063 0.98108108 0.99764706 0.99192618
 0.99694656 0.98039216 0.99883586 0.87804878 0.97867435 0.97751799
 0.95955882 0.99728997 0.94807936 0.80263158 0.97590361]

Kappa:
0.9684441493739242
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd16e9b17f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.658, val_acc:0.438]
Epoch [2/120    avg_loss:2.220, val_acc:0.451]
Epoch [3/120    avg_loss:1.991, val_acc:0.495]
Epoch [4/120    avg_loss:1.830, val_acc:0.544]
Epoch [5/120    avg_loss:1.666, val_acc:0.556]
Epoch [6/120    avg_loss:1.500, val_acc:0.611]
Epoch [7/120    avg_loss:1.391, val_acc:0.613]
Epoch [8/120    avg_loss:1.210, val_acc:0.617]
Epoch [9/120    avg_loss:1.148, val_acc:0.676]
Epoch [10/120    avg_loss:0.955, val_acc:0.685]
Epoch [11/120    avg_loss:0.869, val_acc:0.701]
Epoch [12/120    avg_loss:0.804, val_acc:0.760]
Epoch [13/120    avg_loss:0.680, val_acc:0.772]
Epoch [14/120    avg_loss:0.595, val_acc:0.783]
Epoch [15/120    avg_loss:0.525, val_acc:0.794]
Epoch [16/120    avg_loss:0.494, val_acc:0.775]
Epoch [17/120    avg_loss:0.513, val_acc:0.691]
Epoch [18/120    avg_loss:0.481, val_acc:0.814]
Epoch [19/120    avg_loss:0.451, val_acc:0.808]
Epoch [20/120    avg_loss:0.419, val_acc:0.806]
Epoch [21/120    avg_loss:0.347, val_acc:0.789]
Epoch [22/120    avg_loss:0.322, val_acc:0.853]
Epoch [23/120    avg_loss:0.254, val_acc:0.841]
Epoch [24/120    avg_loss:0.357, val_acc:0.856]
Epoch [25/120    avg_loss:0.275, val_acc:0.855]
Epoch [26/120    avg_loss:0.216, val_acc:0.859]
Epoch [27/120    avg_loss:0.206, val_acc:0.868]
Epoch [28/120    avg_loss:0.208, val_acc:0.871]
Epoch [29/120    avg_loss:0.193, val_acc:0.881]
Epoch [30/120    avg_loss:0.160, val_acc:0.879]
Epoch [31/120    avg_loss:0.151, val_acc:0.917]
Epoch [32/120    avg_loss:0.145, val_acc:0.905]
Epoch [33/120    avg_loss:0.134, val_acc:0.892]
Epoch [34/120    avg_loss:0.141, val_acc:0.915]
Epoch [35/120    avg_loss:0.100, val_acc:0.927]
Epoch [36/120    avg_loss:0.099, val_acc:0.926]
Epoch [37/120    avg_loss:0.102, val_acc:0.923]
Epoch [38/120    avg_loss:0.083, val_acc:0.919]
Epoch [39/120    avg_loss:0.124, val_acc:0.911]
Epoch [40/120    avg_loss:0.095, val_acc:0.935]
Epoch [41/120    avg_loss:0.069, val_acc:0.946]
Epoch [42/120    avg_loss:0.065, val_acc:0.944]
Epoch [43/120    avg_loss:0.060, val_acc:0.927]
Epoch [44/120    avg_loss:0.125, val_acc:0.793]
Epoch [45/120    avg_loss:0.187, val_acc:0.929]
Epoch [46/120    avg_loss:0.085, val_acc:0.941]
Epoch [47/120    avg_loss:0.077, val_acc:0.896]
Epoch [48/120    avg_loss:0.080, val_acc:0.915]
Epoch [49/120    avg_loss:0.072, val_acc:0.923]
Epoch [50/120    avg_loss:0.047, val_acc:0.934]
Epoch [51/120    avg_loss:0.046, val_acc:0.940]
Epoch [52/120    avg_loss:0.050, val_acc:0.935]
Epoch [53/120    avg_loss:0.048, val_acc:0.948]
Epoch [54/120    avg_loss:0.039, val_acc:0.946]
Epoch [55/120    avg_loss:0.028, val_acc:0.948]
Epoch [56/120    avg_loss:0.031, val_acc:0.954]
Epoch [57/120    avg_loss:0.033, val_acc:0.951]
Epoch [58/120    avg_loss:0.086, val_acc:0.927]
Epoch [59/120    avg_loss:0.073, val_acc:0.936]
Epoch [60/120    avg_loss:0.057, val_acc:0.953]
Epoch [61/120    avg_loss:0.043, val_acc:0.938]
Epoch [62/120    avg_loss:0.036, val_acc:0.948]
Epoch [63/120    avg_loss:0.032, val_acc:0.956]
Epoch [64/120    avg_loss:0.025, val_acc:0.961]
Epoch [65/120    avg_loss:0.033, val_acc:0.945]
Epoch [66/120    avg_loss:0.051, val_acc:0.953]
Epoch [67/120    avg_loss:0.028, val_acc:0.948]
Epoch [68/120    avg_loss:0.027, val_acc:0.949]
Epoch [69/120    avg_loss:0.022, val_acc:0.964]
Epoch [70/120    avg_loss:0.021, val_acc:0.955]
Epoch [71/120    avg_loss:0.023, val_acc:0.963]
Epoch [72/120    avg_loss:0.031, val_acc:0.948]
Epoch [73/120    avg_loss:0.042, val_acc:0.953]
Epoch [74/120    avg_loss:0.028, val_acc:0.967]
Epoch [75/120    avg_loss:0.020, val_acc:0.965]
Epoch [76/120    avg_loss:0.023, val_acc:0.953]
Epoch [77/120    avg_loss:0.028, val_acc:0.945]
Epoch [78/120    avg_loss:0.032, val_acc:0.951]
Epoch [79/120    avg_loss:0.018, val_acc:0.967]
Epoch [80/120    avg_loss:0.018, val_acc:0.961]
Epoch [81/120    avg_loss:0.014, val_acc:0.959]
Epoch [82/120    avg_loss:0.017, val_acc:0.959]
Epoch [83/120    avg_loss:0.015, val_acc:0.951]
Epoch [84/120    avg_loss:0.015, val_acc:0.959]
Epoch [85/120    avg_loss:0.018, val_acc:0.954]
Epoch [86/120    avg_loss:0.014, val_acc:0.960]
Epoch [87/120    avg_loss:0.012, val_acc:0.959]
Epoch [88/120    avg_loss:0.012, val_acc:0.958]
Epoch [89/120    avg_loss:0.009, val_acc:0.959]
Epoch [90/120    avg_loss:0.015, val_acc:0.966]
Epoch [91/120    avg_loss:0.022, val_acc:0.964]
Epoch [92/120    avg_loss:0.018, val_acc:0.946]
Epoch [93/120    avg_loss:0.017, val_acc:0.966]
Epoch [94/120    avg_loss:0.009, val_acc:0.969]
Epoch [95/120    avg_loss:0.010, val_acc:0.968]
Epoch [96/120    avg_loss:0.009, val_acc:0.966]
Epoch [97/120    avg_loss:0.008, val_acc:0.966]
Epoch [98/120    avg_loss:0.009, val_acc:0.968]
Epoch [99/120    avg_loss:0.009, val_acc:0.967]
Epoch [100/120    avg_loss:0.007, val_acc:0.968]
Epoch [101/120    avg_loss:0.008, val_acc:0.969]
Epoch [102/120    avg_loss:0.008, val_acc:0.969]
Epoch [103/120    avg_loss:0.007, val_acc:0.965]
Epoch [104/120    avg_loss:0.007, val_acc:0.968]
Epoch [105/120    avg_loss:0.008, val_acc:0.971]
Epoch [106/120    avg_loss:0.007, val_acc:0.972]
Epoch [107/120    avg_loss:0.006, val_acc:0.969]
Epoch [108/120    avg_loss:0.012, val_acc:0.970]
Epoch [109/120    avg_loss:0.007, val_acc:0.969]
Epoch [110/120    avg_loss:0.006, val_acc:0.967]
Epoch [111/120    avg_loss:0.008, val_acc:0.969]
Epoch [112/120    avg_loss:0.009, val_acc:0.968]
Epoch [113/120    avg_loss:0.007, val_acc:0.964]
Epoch [114/120    avg_loss:0.007, val_acc:0.968]
Epoch [115/120    avg_loss:0.008, val_acc:0.969]
Epoch [116/120    avg_loss:0.010, val_acc:0.969]
Epoch [117/120    avg_loss:0.008, val_acc:0.966]
Epoch [118/120    avg_loss:0.006, val_acc:0.970]
Epoch [119/120    avg_loss:0.006, val_acc:0.967]
Epoch [120/120    avg_loss:0.006, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    3    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1249    0    4    0    0    0    0    0    1   30    1    0
     0    0    0]
 [   0    0    0  707   15    1    2    0    0    1    1   12    7    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    3    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0  852   20    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    4   12 2176   14    0
     0    1    0]
 [   0    0    0    1    0    0    0    0    0    0    0    6  526    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    2    0    0
  1120   17    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    37  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.72357723577235

F1 scores:
[       nan 0.96202532 0.98346457 0.97182131 0.95730337 0.98964327
 0.99771863 0.94339623 1.         0.85       0.97874785 0.97578475
 0.9704797  0.99728997 0.97518502 0.91715976 0.98203593]

Kappa:
0.9740325181101154
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3adc332780>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.636, val_acc:0.459]
Epoch [2/120    avg_loss:2.232, val_acc:0.476]
Epoch [3/120    avg_loss:1.970, val_acc:0.529]
Epoch [4/120    avg_loss:1.739, val_acc:0.548]
Epoch [5/120    avg_loss:1.592, val_acc:0.594]
Epoch [6/120    avg_loss:1.406, val_acc:0.608]
Epoch [7/120    avg_loss:1.240, val_acc:0.606]
Epoch [8/120    avg_loss:1.104, val_acc:0.641]
Epoch [9/120    avg_loss:1.048, val_acc:0.689]
Epoch [10/120    avg_loss:0.865, val_acc:0.753]
Epoch [11/120    avg_loss:0.768, val_acc:0.754]
Epoch [12/120    avg_loss:0.662, val_acc:0.805]
Epoch [13/120    avg_loss:0.648, val_acc:0.783]
Epoch [14/120    avg_loss:0.535, val_acc:0.812]
Epoch [15/120    avg_loss:0.542, val_acc:0.811]
Epoch [16/120    avg_loss:0.497, val_acc:0.824]
Epoch [17/120    avg_loss:0.431, val_acc:0.818]
Epoch [18/120    avg_loss:0.386, val_acc:0.841]
Epoch [19/120    avg_loss:0.341, val_acc:0.865]
Epoch [20/120    avg_loss:0.278, val_acc:0.874]
Epoch [21/120    avg_loss:0.226, val_acc:0.894]
Epoch [22/120    avg_loss:0.209, val_acc:0.873]
Epoch [23/120    avg_loss:0.217, val_acc:0.900]
Epoch [24/120    avg_loss:0.184, val_acc:0.884]
Epoch [25/120    avg_loss:0.155, val_acc:0.887]
Epoch [26/120    avg_loss:0.158, val_acc:0.881]
Epoch [27/120    avg_loss:0.146, val_acc:0.903]
Epoch [28/120    avg_loss:0.121, val_acc:0.932]
Epoch [29/120    avg_loss:0.148, val_acc:0.905]
Epoch [30/120    avg_loss:0.161, val_acc:0.891]
Epoch [31/120    avg_loss:0.180, val_acc:0.910]
Epoch [32/120    avg_loss:0.132, val_acc:0.930]
Epoch [33/120    avg_loss:0.123, val_acc:0.904]
Epoch [34/120    avg_loss:0.113, val_acc:0.926]
Epoch [35/120    avg_loss:0.139, val_acc:0.841]
Epoch [36/120    avg_loss:0.301, val_acc:0.877]
Epoch [37/120    avg_loss:0.160, val_acc:0.907]
Epoch [38/120    avg_loss:0.143, val_acc:0.925]
Epoch [39/120    avg_loss:0.128, val_acc:0.933]
Epoch [40/120    avg_loss:0.093, val_acc:0.933]
Epoch [41/120    avg_loss:0.080, val_acc:0.938]
Epoch [42/120    avg_loss:0.080, val_acc:0.925]
Epoch [43/120    avg_loss:0.078, val_acc:0.936]
Epoch [44/120    avg_loss:0.081, val_acc:0.923]
Epoch [45/120    avg_loss:0.065, val_acc:0.941]
Epoch [46/120    avg_loss:0.048, val_acc:0.938]
Epoch [47/120    avg_loss:0.059, val_acc:0.936]
Epoch [48/120    avg_loss:0.064, val_acc:0.949]
Epoch [49/120    avg_loss:0.064, val_acc:0.942]
Epoch [50/120    avg_loss:0.109, val_acc:0.951]
Epoch [51/120    avg_loss:0.084, val_acc:0.934]
Epoch [52/120    avg_loss:0.077, val_acc:0.955]
Epoch [53/120    avg_loss:0.045, val_acc:0.956]
Epoch [54/120    avg_loss:0.040, val_acc:0.956]
Epoch [55/120    avg_loss:0.043, val_acc:0.951]
Epoch [56/120    avg_loss:0.044, val_acc:0.950]
Epoch [57/120    avg_loss:0.048, val_acc:0.950]
Epoch [58/120    avg_loss:0.040, val_acc:0.964]
Epoch [59/120    avg_loss:0.038, val_acc:0.961]
Epoch [60/120    avg_loss:0.056, val_acc:0.960]
Epoch [61/120    avg_loss:0.041, val_acc:0.961]
Epoch [62/120    avg_loss:0.039, val_acc:0.958]
Epoch [63/120    avg_loss:0.032, val_acc:0.957]
Epoch [64/120    avg_loss:0.043, val_acc:0.950]
Epoch [65/120    avg_loss:0.025, val_acc:0.969]
Epoch [66/120    avg_loss:0.040, val_acc:0.930]
Epoch [67/120    avg_loss:0.029, val_acc:0.964]
Epoch [68/120    avg_loss:0.025, val_acc:0.968]
Epoch [69/120    avg_loss:0.052, val_acc:0.957]
Epoch [70/120    avg_loss:0.032, val_acc:0.975]
Epoch [71/120    avg_loss:0.027, val_acc:0.968]
Epoch [72/120    avg_loss:0.019, val_acc:0.968]
Epoch [73/120    avg_loss:0.016, val_acc:0.965]
Epoch [74/120    avg_loss:0.017, val_acc:0.975]
Epoch [75/120    avg_loss:0.031, val_acc:0.964]
Epoch [76/120    avg_loss:0.028, val_acc:0.969]
Epoch [77/120    avg_loss:0.024, val_acc:0.976]
Epoch [78/120    avg_loss:0.023, val_acc:0.978]
Epoch [79/120    avg_loss:0.013, val_acc:0.976]
Epoch [80/120    avg_loss:0.010, val_acc:0.972]
Epoch [81/120    avg_loss:0.041, val_acc:0.946]
Epoch [82/120    avg_loss:0.062, val_acc:0.934]
Epoch [83/120    avg_loss:0.048, val_acc:0.965]
Epoch [84/120    avg_loss:0.038, val_acc:0.968]
Epoch [85/120    avg_loss:0.031, val_acc:0.976]
Epoch [86/120    avg_loss:0.019, val_acc:0.978]
Epoch [87/120    avg_loss:0.011, val_acc:0.979]
Epoch [88/120    avg_loss:0.013, val_acc:0.969]
Epoch [89/120    avg_loss:0.015, val_acc:0.981]
Epoch [90/120    avg_loss:0.016, val_acc:0.983]
Epoch [91/120    avg_loss:0.019, val_acc:0.975]
Epoch [92/120    avg_loss:0.018, val_acc:0.975]
Epoch [93/120    avg_loss:0.010, val_acc:0.978]
Epoch [94/120    avg_loss:0.008, val_acc:0.977]
Epoch [95/120    avg_loss:0.015, val_acc:0.960]
Epoch [96/120    avg_loss:0.032, val_acc:0.966]
Epoch [97/120    avg_loss:0.032, val_acc:0.953]
Epoch [98/120    avg_loss:0.028, val_acc:0.968]
Epoch [99/120    avg_loss:0.014, val_acc:0.977]
Epoch [100/120    avg_loss:0.014, val_acc:0.981]
Epoch [101/120    avg_loss:0.012, val_acc:0.975]
Epoch [102/120    avg_loss:0.013, val_acc:0.979]
Epoch [103/120    avg_loss:0.013, val_acc:0.973]
Epoch [104/120    avg_loss:0.009, val_acc:0.976]
Epoch [105/120    avg_loss:0.009, val_acc:0.978]
Epoch [106/120    avg_loss:0.008, val_acc:0.978]
Epoch [107/120    avg_loss:0.008, val_acc:0.978]
Epoch [108/120    avg_loss:0.006, val_acc:0.978]
Epoch [109/120    avg_loss:0.005, val_acc:0.978]
Epoch [110/120    avg_loss:0.007, val_acc:0.979]
Epoch [111/120    avg_loss:0.006, val_acc:0.979]
Epoch [112/120    avg_loss:0.009, val_acc:0.979]
Epoch [113/120    avg_loss:0.006, val_acc:0.980]
Epoch [114/120    avg_loss:0.005, val_acc:0.980]
Epoch [115/120    avg_loss:0.005, val_acc:0.980]
Epoch [116/120    avg_loss:0.007, val_acc:0.980]
Epoch [117/120    avg_loss:0.005, val_acc:0.980]
Epoch [118/120    avg_loss:0.005, val_acc:0.980]
Epoch [119/120    avg_loss:0.005, val_acc:0.980]
Epoch [120/120    avg_loss:0.006, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1271    2    2    0    0    0    0    0    1    9    0    0
     0    0    0]
 [   0    0    0  733    0    0    0    0    0    1    0   11    2    0
     0    0    0]
 [   0    0    0    1  211    0    1    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    0  834   33    1    0
     0    0    0]
 [   0    0   15    0    0    0    0    0    0    0    4 2175   12    0
     0    4    0]
 [   0    0    0   10    0    0    0    0    0    0    5    1  514    0
     0    3    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    78  264    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.51761517615176

F1 scores:
[       nan 0.94871795 0.98603569 0.98191561 0.99061033 0.99884925
 0.99469295 0.98039216 0.997669   0.97297297 0.96807893 0.97995044
 0.96344892 1.         0.96207925 0.84076433 0.98203593]

Kappa:
0.9716663558608868
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f8cca6828>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.594, val_acc:0.319]
Epoch [2/120    avg_loss:2.181, val_acc:0.423]
Epoch [3/120    avg_loss:1.956, val_acc:0.567]
Epoch [4/120    avg_loss:1.777, val_acc:0.561]
Epoch [5/120    avg_loss:1.611, val_acc:0.623]
Epoch [6/120    avg_loss:1.448, val_acc:0.602]
Epoch [7/120    avg_loss:1.333, val_acc:0.669]
Epoch [8/120    avg_loss:1.184, val_acc:0.666]
Epoch [9/120    avg_loss:0.996, val_acc:0.726]
Epoch [10/120    avg_loss:0.822, val_acc:0.787]
Epoch [11/120    avg_loss:0.784, val_acc:0.752]
Epoch [12/120    avg_loss:0.633, val_acc:0.836]
Epoch [13/120    avg_loss:0.565, val_acc:0.783]
Epoch [14/120    avg_loss:0.624, val_acc:0.807]
Epoch [15/120    avg_loss:0.542, val_acc:0.789]
Epoch [16/120    avg_loss:0.419, val_acc:0.858]
Epoch [17/120    avg_loss:0.449, val_acc:0.847]
Epoch [18/120    avg_loss:0.365, val_acc:0.845]
Epoch [19/120    avg_loss:0.332, val_acc:0.870]
Epoch [20/120    avg_loss:0.279, val_acc:0.853]
Epoch [21/120    avg_loss:0.254, val_acc:0.863]
Epoch [22/120    avg_loss:0.228, val_acc:0.867]
Epoch [23/120    avg_loss:0.234, val_acc:0.872]
Epoch [24/120    avg_loss:0.198, val_acc:0.919]
Epoch [25/120    avg_loss:0.210, val_acc:0.822]
Epoch [26/120    avg_loss:0.241, val_acc:0.898]
Epoch [27/120    avg_loss:0.214, val_acc:0.893]
Epoch [28/120    avg_loss:0.213, val_acc:0.878]
Epoch [29/120    avg_loss:0.152, val_acc:0.926]
Epoch [30/120    avg_loss:0.133, val_acc:0.927]
Epoch [31/120    avg_loss:0.144, val_acc:0.894]
Epoch [32/120    avg_loss:0.133, val_acc:0.907]
Epoch [33/120    avg_loss:0.102, val_acc:0.949]
Epoch [34/120    avg_loss:0.111, val_acc:0.880]
Epoch [35/120    avg_loss:0.108, val_acc:0.945]
Epoch [36/120    avg_loss:0.095, val_acc:0.945]
Epoch [37/120    avg_loss:0.086, val_acc:0.948]
Epoch [38/120    avg_loss:0.076, val_acc:0.942]
Epoch [39/120    avg_loss:0.087, val_acc:0.925]
Epoch [40/120    avg_loss:0.096, val_acc:0.943]
Epoch [41/120    avg_loss:0.059, val_acc:0.955]
Epoch [42/120    avg_loss:0.054, val_acc:0.952]
Epoch [43/120    avg_loss:0.045, val_acc:0.956]
Epoch [44/120    avg_loss:0.053, val_acc:0.953]
Epoch [45/120    avg_loss:0.067, val_acc:0.960]
Epoch [46/120    avg_loss:0.050, val_acc:0.938]
Epoch [47/120    avg_loss:0.048, val_acc:0.934]
Epoch [48/120    avg_loss:0.057, val_acc:0.961]
Epoch [49/120    avg_loss:0.058, val_acc:0.951]
Epoch [50/120    avg_loss:0.036, val_acc:0.960]
Epoch [51/120    avg_loss:0.033, val_acc:0.967]
Epoch [52/120    avg_loss:0.031, val_acc:0.958]
Epoch [53/120    avg_loss:0.035, val_acc:0.970]
Epoch [54/120    avg_loss:0.040, val_acc:0.959]
Epoch [55/120    avg_loss:0.038, val_acc:0.972]
Epoch [56/120    avg_loss:0.030, val_acc:0.973]
Epoch [57/120    avg_loss:0.022, val_acc:0.974]
Epoch [58/120    avg_loss:0.021, val_acc:0.974]
Epoch [59/120    avg_loss:0.047, val_acc:0.959]
Epoch [60/120    avg_loss:0.082, val_acc:0.947]
Epoch [61/120    avg_loss:0.068, val_acc:0.963]
Epoch [62/120    avg_loss:0.060, val_acc:0.968]
Epoch [63/120    avg_loss:0.044, val_acc:0.977]
Epoch [64/120    avg_loss:0.032, val_acc:0.952]
Epoch [65/120    avg_loss:0.041, val_acc:0.973]
Epoch [66/120    avg_loss:0.031, val_acc:0.973]
Epoch [67/120    avg_loss:0.020, val_acc:0.974]
Epoch [68/120    avg_loss:0.020, val_acc:0.970]
Epoch [69/120    avg_loss:0.033, val_acc:0.963]
Epoch [70/120    avg_loss:0.024, val_acc:0.972]
Epoch [71/120    avg_loss:0.020, val_acc:0.976]
Epoch [72/120    avg_loss:0.021, val_acc:0.974]
Epoch [73/120    avg_loss:0.017, val_acc:0.981]
Epoch [74/120    avg_loss:0.012, val_acc:0.983]
Epoch [75/120    avg_loss:0.010, val_acc:0.979]
Epoch [76/120    avg_loss:0.033, val_acc:0.945]
Epoch [77/120    avg_loss:0.062, val_acc:0.939]
Epoch [78/120    avg_loss:0.068, val_acc:0.963]
Epoch [79/120    avg_loss:0.057, val_acc:0.960]
Epoch [80/120    avg_loss:0.028, val_acc:0.958]
Epoch [81/120    avg_loss:0.028, val_acc:0.977]
Epoch [82/120    avg_loss:0.022, val_acc:0.977]
Epoch [83/120    avg_loss:0.015, val_acc:0.985]
Epoch [84/120    avg_loss:0.020, val_acc:0.965]
Epoch [85/120    avg_loss:0.020, val_acc:0.982]
Epoch [86/120    avg_loss:0.015, val_acc:0.975]
Epoch [87/120    avg_loss:0.014, val_acc:0.978]
Epoch [88/120    avg_loss:0.019, val_acc:0.975]
Epoch [89/120    avg_loss:0.014, val_acc:0.977]
Epoch [90/120    avg_loss:0.013, val_acc:0.977]
Epoch [91/120    avg_loss:0.007, val_acc:0.983]
Epoch [92/120    avg_loss:0.007, val_acc:0.981]
Epoch [93/120    avg_loss:0.011, val_acc:0.979]
Epoch [94/120    avg_loss:0.011, val_acc:0.976]
Epoch [95/120    avg_loss:0.009, val_acc:0.983]
Epoch [96/120    avg_loss:0.016, val_acc:0.974]
Epoch [97/120    avg_loss:0.017, val_acc:0.980]
Epoch [98/120    avg_loss:0.011, val_acc:0.984]
Epoch [99/120    avg_loss:0.011, val_acc:0.985]
Epoch [100/120    avg_loss:0.007, val_acc:0.985]
Epoch [101/120    avg_loss:0.007, val_acc:0.986]
Epoch [102/120    avg_loss:0.008, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.985]
Epoch [105/120    avg_loss:0.007, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.985]
Epoch [108/120    avg_loss:0.006, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.006, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.984]
Epoch [113/120    avg_loss:0.008, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.984]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.006, val_acc:0.983]
Epoch [117/120    avg_loss:0.005, val_acc:0.983]
Epoch [118/120    avg_loss:0.008, val_acc:0.983]
Epoch [119/120    avg_loss:0.007, val_acc:0.983]
Epoch [120/120    avg_loss:0.005, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1256    2    0    0    0    0    0    0    6   20    1    0
     0    0    0]
 [   0    0    0  712    2    0    0    0    0    0    2   15   16    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    1    0    0    1    1    0    0    0  846   25    0    0
     0    1    0]
 [   0    0    4    3    0    0    0    0    0    0   30 2168    5    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0    0    0    8  518    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    81  261    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.21409214092141

F1 scores:
[       nan 0.93506494 0.98664572 0.96870748 0.99061033 0.99654776
 0.99469295 1.         1.         0.94444444 0.95918367 0.97482014
 0.96461825 1.         0.96129307 0.84193548 0.98823529]

Kappa:
0.9682002698736895
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f642b6727f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.614, val_acc:0.490]
Epoch [2/120    avg_loss:2.167, val_acc:0.512]
Epoch [3/120    avg_loss:1.926, val_acc:0.566]
Epoch [4/120    avg_loss:1.703, val_acc:0.564]
Epoch [5/120    avg_loss:1.550, val_acc:0.618]
Epoch [6/120    avg_loss:1.382, val_acc:0.635]
Epoch [7/120    avg_loss:1.299, val_acc:0.681]
Epoch [8/120    avg_loss:1.165, val_acc:0.692]
Epoch [9/120    avg_loss:1.035, val_acc:0.717]
Epoch [10/120    avg_loss:0.911, val_acc:0.739]
Epoch [11/120    avg_loss:0.834, val_acc:0.759]
Epoch [12/120    avg_loss:0.720, val_acc:0.721]
Epoch [13/120    avg_loss:0.622, val_acc:0.800]
Epoch [14/120    avg_loss:0.496, val_acc:0.800]
Epoch [15/120    avg_loss:0.516, val_acc:0.768]
Epoch [16/120    avg_loss:0.513, val_acc:0.800]
Epoch [17/120    avg_loss:0.433, val_acc:0.823]
Epoch [18/120    avg_loss:0.355, val_acc:0.834]
Epoch [19/120    avg_loss:0.307, val_acc:0.834]
Epoch [20/120    avg_loss:0.273, val_acc:0.855]
Epoch [21/120    avg_loss:0.240, val_acc:0.868]
Epoch [22/120    avg_loss:0.271, val_acc:0.893]
Epoch [23/120    avg_loss:0.212, val_acc:0.904]
Epoch [24/120    avg_loss:0.232, val_acc:0.895]
Epoch [25/120    avg_loss:0.176, val_acc:0.908]
Epoch [26/120    avg_loss:0.199, val_acc:0.915]
Epoch [27/120    avg_loss:0.143, val_acc:0.931]
Epoch [28/120    avg_loss:0.120, val_acc:0.908]
Epoch [29/120    avg_loss:0.131, val_acc:0.908]
Epoch [30/120    avg_loss:0.124, val_acc:0.909]
Epoch [31/120    avg_loss:0.151, val_acc:0.924]
Epoch [32/120    avg_loss:0.133, val_acc:0.912]
Epoch [33/120    avg_loss:0.152, val_acc:0.926]
Epoch [34/120    avg_loss:0.144, val_acc:0.935]
Epoch [35/120    avg_loss:0.105, val_acc:0.952]
Epoch [36/120    avg_loss:0.089, val_acc:0.943]
Epoch [37/120    avg_loss:0.098, val_acc:0.949]
Epoch [38/120    avg_loss:0.072, val_acc:0.954]
Epoch [39/120    avg_loss:0.070, val_acc:0.944]
Epoch [40/120    avg_loss:0.067, val_acc:0.959]
Epoch [41/120    avg_loss:0.066, val_acc:0.951]
Epoch [42/120    avg_loss:0.066, val_acc:0.941]
Epoch [43/120    avg_loss:0.062, val_acc:0.951]
Epoch [44/120    avg_loss:0.056, val_acc:0.954]
Epoch [45/120    avg_loss:0.043, val_acc:0.972]
Epoch [46/120    avg_loss:0.044, val_acc:0.966]
Epoch [47/120    avg_loss:0.043, val_acc:0.965]
Epoch [48/120    avg_loss:0.042, val_acc:0.956]
Epoch [49/120    avg_loss:0.044, val_acc:0.963]
Epoch [50/120    avg_loss:0.050, val_acc:0.969]
Epoch [51/120    avg_loss:0.040, val_acc:0.977]
Epoch [52/120    avg_loss:0.029, val_acc:0.973]
Epoch [53/120    avg_loss:0.039, val_acc:0.965]
Epoch [54/120    avg_loss:0.047, val_acc:0.961]
Epoch [55/120    avg_loss:0.029, val_acc:0.976]
Epoch [56/120    avg_loss:0.018, val_acc:0.978]
Epoch [57/120    avg_loss:0.023, val_acc:0.978]
Epoch [58/120    avg_loss:0.048, val_acc:0.967]
Epoch [59/120    avg_loss:0.074, val_acc:0.923]
Epoch [60/120    avg_loss:0.055, val_acc:0.956]
Epoch [61/120    avg_loss:0.033, val_acc:0.964]
Epoch [62/120    avg_loss:0.039, val_acc:0.959]
Epoch [63/120    avg_loss:0.033, val_acc:0.973]
Epoch [64/120    avg_loss:0.032, val_acc:0.971]
Epoch [65/120    avg_loss:0.025, val_acc:0.971]
Epoch [66/120    avg_loss:0.016, val_acc:0.981]
Epoch [67/120    avg_loss:0.020, val_acc:0.966]
Epoch [68/120    avg_loss:0.025, val_acc:0.973]
Epoch [69/120    avg_loss:0.022, val_acc:0.974]
Epoch [70/120    avg_loss:0.019, val_acc:0.976]
Epoch [71/120    avg_loss:0.012, val_acc:0.974]
Epoch [72/120    avg_loss:0.012, val_acc:0.979]
Epoch [73/120    avg_loss:0.014, val_acc:0.974]
Epoch [74/120    avg_loss:0.017, val_acc:0.979]
Epoch [75/120    avg_loss:0.017, val_acc:0.973]
Epoch [76/120    avg_loss:0.016, val_acc:0.976]
Epoch [77/120    avg_loss:0.015, val_acc:0.976]
Epoch [78/120    avg_loss:0.010, val_acc:0.980]
Epoch [79/120    avg_loss:0.012, val_acc:0.979]
Epoch [80/120    avg_loss:0.012, val_acc:0.981]
Epoch [81/120    avg_loss:0.008, val_acc:0.981]
Epoch [82/120    avg_loss:0.008, val_acc:0.982]
Epoch [83/120    avg_loss:0.010, val_acc:0.981]
Epoch [84/120    avg_loss:0.009, val_acc:0.982]
Epoch [85/120    avg_loss:0.007, val_acc:0.982]
Epoch [86/120    avg_loss:0.007, val_acc:0.983]
Epoch [87/120    avg_loss:0.006, val_acc:0.983]
Epoch [88/120    avg_loss:0.008, val_acc:0.983]
Epoch [89/120    avg_loss:0.006, val_acc:0.983]
Epoch [90/120    avg_loss:0.006, val_acc:0.983]
Epoch [91/120    avg_loss:0.006, val_acc:0.983]
Epoch [92/120    avg_loss:0.008, val_acc:0.982]
Epoch [93/120    avg_loss:0.006, val_acc:0.983]
Epoch [94/120    avg_loss:0.006, val_acc:0.982]
Epoch [95/120    avg_loss:0.008, val_acc:0.982]
Epoch [96/120    avg_loss:0.010, val_acc:0.983]
Epoch [97/120    avg_loss:0.009, val_acc:0.983]
Epoch [98/120    avg_loss:0.007, val_acc:0.983]
Epoch [99/120    avg_loss:0.006, val_acc:0.983]
Epoch [100/120    avg_loss:0.006, val_acc:0.982]
Epoch [101/120    avg_loss:0.006, val_acc:0.983]
Epoch [102/120    avg_loss:0.008, val_acc:0.983]
Epoch [103/120    avg_loss:0.005, val_acc:0.984]
Epoch [104/120    avg_loss:0.007, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.984]
Epoch [106/120    avg_loss:0.006, val_acc:0.983]
Epoch [107/120    avg_loss:0.008, val_acc:0.984]
Epoch [108/120    avg_loss:0.005, val_acc:0.984]
Epoch [109/120    avg_loss:0.006, val_acc:0.983]
Epoch [110/120    avg_loss:0.005, val_acc:0.983]
Epoch [111/120    avg_loss:0.008, val_acc:0.982]
Epoch [112/120    avg_loss:0.008, val_acc:0.983]
Epoch [113/120    avg_loss:0.007, val_acc:0.982]
Epoch [114/120    avg_loss:0.006, val_acc:0.983]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.006, val_acc:0.983]
Epoch [117/120    avg_loss:0.005, val_acc:0.982]
Epoch [118/120    avg_loss:0.005, val_acc:0.982]
Epoch [119/120    avg_loss:0.005, val_acc:0.982]
Epoch [120/120    avg_loss:0.005, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1250    7    0    1    0    0    0    0    2   22    3    0
     0    0    0]
 [   0    0    0  733    1    0    0    0    0    0    4    6    2    0
     0    1    0]
 [   0    0    0    4  209    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    4    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  839   29    0    0
     0    1    0]
 [   0    0   10    2    0    0    0    0    0    0    3 2175   16    1
     0    3    0]
 [   0    0    0    5    0    0    0    0    0    0    0    1  523    0
     1    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    71  276    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.47425474254743

F1 scores:
[       nan 0.975      0.97962382 0.97863818 0.98817967 0.99307159
 0.99620349 1.         1.         0.875      0.97331787 0.97862767
 0.96941613 0.99459459 0.96157131 0.85847589 0.98224852]

Kappa:
0.9711760397066019
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f31647da748>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.628, val_acc:0.374]
Epoch [2/120    avg_loss:2.206, val_acc:0.448]
Epoch [3/120    avg_loss:1.955, val_acc:0.511]
Epoch [4/120    avg_loss:1.755, val_acc:0.509]
Epoch [5/120    avg_loss:1.613, val_acc:0.473]
Epoch [6/120    avg_loss:1.455, val_acc:0.572]
Epoch [7/120    avg_loss:1.312, val_acc:0.613]
Epoch [8/120    avg_loss:1.188, val_acc:0.629]
Epoch [9/120    avg_loss:1.046, val_acc:0.651]
Epoch [10/120    avg_loss:0.853, val_acc:0.744]
Epoch [11/120    avg_loss:0.767, val_acc:0.738]
Epoch [12/120    avg_loss:0.665, val_acc:0.792]
Epoch [13/120    avg_loss:0.592, val_acc:0.769]
Epoch [14/120    avg_loss:0.519, val_acc:0.711]
Epoch [15/120    avg_loss:0.542, val_acc:0.815]
Epoch [16/120    avg_loss:0.495, val_acc:0.816]
Epoch [17/120    avg_loss:0.390, val_acc:0.793]
Epoch [18/120    avg_loss:0.497, val_acc:0.793]
Epoch [19/120    avg_loss:0.420, val_acc:0.846]
Epoch [20/120    avg_loss:0.397, val_acc:0.851]
Epoch [21/120    avg_loss:0.348, val_acc:0.858]
Epoch [22/120    avg_loss:0.262, val_acc:0.871]
Epoch [23/120    avg_loss:0.265, val_acc:0.876]
Epoch [24/120    avg_loss:0.229, val_acc:0.873]
Epoch [25/120    avg_loss:0.218, val_acc:0.895]
Epoch [26/120    avg_loss:0.225, val_acc:0.875]
Epoch [27/120    avg_loss:0.160, val_acc:0.895]
Epoch [28/120    avg_loss:0.234, val_acc:0.875]
Epoch [29/120    avg_loss:0.171, val_acc:0.907]
Epoch [30/120    avg_loss:0.183, val_acc:0.916]
Epoch [31/120    avg_loss:0.158, val_acc:0.924]
Epoch [32/120    avg_loss:0.134, val_acc:0.906]
Epoch [33/120    avg_loss:0.136, val_acc:0.909]
Epoch [34/120    avg_loss:0.195, val_acc:0.900]
Epoch [35/120    avg_loss:0.123, val_acc:0.928]
Epoch [36/120    avg_loss:0.104, val_acc:0.923]
Epoch [37/120    avg_loss:0.106, val_acc:0.933]
Epoch [38/120    avg_loss:0.094, val_acc:0.931]
Epoch [39/120    avg_loss:0.076, val_acc:0.933]
Epoch [40/120    avg_loss:0.066, val_acc:0.947]
Epoch [41/120    avg_loss:0.064, val_acc:0.947]
Epoch [42/120    avg_loss:0.061, val_acc:0.944]
Epoch [43/120    avg_loss:0.081, val_acc:0.933]
Epoch [44/120    avg_loss:0.091, val_acc:0.932]
Epoch [45/120    avg_loss:0.072, val_acc:0.903]
Epoch [46/120    avg_loss:0.054, val_acc:0.952]
Epoch [47/120    avg_loss:0.052, val_acc:0.955]
Epoch [48/120    avg_loss:0.044, val_acc:0.952]
Epoch [49/120    avg_loss:0.046, val_acc:0.947]
Epoch [50/120    avg_loss:0.038, val_acc:0.956]
Epoch [51/120    avg_loss:0.046, val_acc:0.952]
Epoch [52/120    avg_loss:0.041, val_acc:0.960]
Epoch [53/120    avg_loss:0.038, val_acc:0.960]
Epoch [54/120    avg_loss:0.040, val_acc:0.957]
Epoch [55/120    avg_loss:0.032, val_acc:0.959]
Epoch [56/120    avg_loss:0.032, val_acc:0.954]
Epoch [57/120    avg_loss:0.036, val_acc:0.961]
Epoch [58/120    avg_loss:0.026, val_acc:0.968]
Epoch [59/120    avg_loss:0.029, val_acc:0.950]
Epoch [60/120    avg_loss:0.039, val_acc:0.947]
Epoch [61/120    avg_loss:0.044, val_acc:0.964]
Epoch [62/120    avg_loss:0.034, val_acc:0.966]
Epoch [63/120    avg_loss:0.054, val_acc:0.951]
Epoch [64/120    avg_loss:0.079, val_acc:0.955]
Epoch [65/120    avg_loss:0.054, val_acc:0.939]
Epoch [66/120    avg_loss:0.064, val_acc:0.944]
Epoch [67/120    avg_loss:0.042, val_acc:0.959]
Epoch [68/120    avg_loss:0.029, val_acc:0.960]
Epoch [69/120    avg_loss:0.042, val_acc:0.968]
Epoch [70/120    avg_loss:0.038, val_acc:0.955]
Epoch [71/120    avg_loss:0.031, val_acc:0.955]
Epoch [72/120    avg_loss:0.022, val_acc:0.941]
Epoch [73/120    avg_loss:0.018, val_acc:0.969]
Epoch [74/120    avg_loss:0.019, val_acc:0.955]
Epoch [75/120    avg_loss:0.020, val_acc:0.971]
Epoch [76/120    avg_loss:0.018, val_acc:0.965]
Epoch [77/120    avg_loss:0.026, val_acc:0.966]
Epoch [78/120    avg_loss:0.019, val_acc:0.956]
Epoch [79/120    avg_loss:0.017, val_acc:0.959]
Epoch [80/120    avg_loss:0.022, val_acc:0.966]
Epoch [81/120    avg_loss:0.016, val_acc:0.972]
Epoch [82/120    avg_loss:0.020, val_acc:0.975]
Epoch [83/120    avg_loss:0.016, val_acc:0.974]
Epoch [84/120    avg_loss:0.014, val_acc:0.965]
Epoch [85/120    avg_loss:0.015, val_acc:0.974]
Epoch [86/120    avg_loss:0.013, val_acc:0.975]
Epoch [87/120    avg_loss:0.011, val_acc:0.961]
Epoch [88/120    avg_loss:0.013, val_acc:0.973]
Epoch [89/120    avg_loss:0.019, val_acc:0.966]
Epoch [90/120    avg_loss:0.013, val_acc:0.974]
Epoch [91/120    avg_loss:0.009, val_acc:0.974]
Epoch [92/120    avg_loss:0.009, val_acc:0.974]
Epoch [93/120    avg_loss:0.013, val_acc:0.931]
Epoch [94/120    avg_loss:0.014, val_acc:0.972]
Epoch [95/120    avg_loss:0.030, val_acc:0.974]
Epoch [96/120    avg_loss:0.026, val_acc:0.952]
Epoch [97/120    avg_loss:0.022, val_acc:0.965]
Epoch [98/120    avg_loss:0.014, val_acc:0.972]
Epoch [99/120    avg_loss:0.008, val_acc:0.967]
Epoch [100/120    avg_loss:0.013, val_acc:0.974]
Epoch [101/120    avg_loss:0.012, val_acc:0.978]
Epoch [102/120    avg_loss:0.006, val_acc:0.978]
Epoch [103/120    avg_loss:0.008, val_acc:0.976]
Epoch [104/120    avg_loss:0.007, val_acc:0.976]
Epoch [105/120    avg_loss:0.009, val_acc:0.976]
Epoch [106/120    avg_loss:0.007, val_acc:0.978]
Epoch [107/120    avg_loss:0.008, val_acc:0.977]
Epoch [108/120    avg_loss:0.009, val_acc:0.977]
Epoch [109/120    avg_loss:0.006, val_acc:0.979]
Epoch [110/120    avg_loss:0.006, val_acc:0.978]
Epoch [111/120    avg_loss:0.006, val_acc:0.976]
Epoch [112/120    avg_loss:0.008, val_acc:0.977]
Epoch [113/120    avg_loss:0.006, val_acc:0.975]
Epoch [114/120    avg_loss:0.006, val_acc:0.975]
Epoch [115/120    avg_loss:0.009, val_acc:0.975]
Epoch [116/120    avg_loss:0.009, val_acc:0.975]
Epoch [117/120    avg_loss:0.006, val_acc:0.978]
Epoch [118/120    avg_loss:0.008, val_acc:0.977]
Epoch [119/120    avg_loss:0.006, val_acc:0.977]
Epoch [120/120    avg_loss:0.007, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1262    3    7    0    0    0    0    0    5    8    0    0
     0    0    0]
 [   0    0    0  726    7    0    0    0    0    1    5    4    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    2    0    0    0    0  837   27    2    0
     0    1    0]
 [   0    0   13    0    0    0    0    2    0    0   11 2179    5    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    3  530    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
  1124   13    0]
 [   0    0    0    0    0    1    3    0    0    0    0    0    0    0
    45  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.8970189701897

F1 scores:
[       nan 0.96202532 0.98286604 0.98373984 0.96818182 0.98966705
 0.99543379 0.96153846 0.99883586 0.97297297 0.9648415  0.98308143
 0.98421541 1.         0.97189797 0.90440061 0.98203593]

Kappa:
0.9760128713030666
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbc2a1c6828>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.630, val_acc:0.435]
Epoch [2/120    avg_loss:2.192, val_acc:0.439]
Epoch [3/120    avg_loss:1.968, val_acc:0.502]
Epoch [4/120    avg_loss:1.749, val_acc:0.532]
Epoch [5/120    avg_loss:1.578, val_acc:0.579]
Epoch [6/120    avg_loss:1.465, val_acc:0.574]
Epoch [7/120    avg_loss:1.299, val_acc:0.600]
Epoch [8/120    avg_loss:1.139, val_acc:0.692]
Epoch [9/120    avg_loss:1.014, val_acc:0.707]
Epoch [10/120    avg_loss:0.948, val_acc:0.699]
Epoch [11/120    avg_loss:0.826, val_acc:0.777]
Epoch [12/120    avg_loss:0.725, val_acc:0.783]
Epoch [13/120    avg_loss:0.615, val_acc:0.815]
Epoch [14/120    avg_loss:0.507, val_acc:0.795]
Epoch [15/120    avg_loss:0.458, val_acc:0.814]
Epoch [16/120    avg_loss:0.390, val_acc:0.811]
Epoch [17/120    avg_loss:0.379, val_acc:0.842]
Epoch [18/120    avg_loss:0.354, val_acc:0.836]
Epoch [19/120    avg_loss:0.311, val_acc:0.853]
Epoch [20/120    avg_loss:0.286, val_acc:0.852]
Epoch [21/120    avg_loss:0.289, val_acc:0.856]
Epoch [22/120    avg_loss:0.275, val_acc:0.854]
Epoch [23/120    avg_loss:0.217, val_acc:0.908]
Epoch [24/120    avg_loss:0.346, val_acc:0.836]
Epoch [25/120    avg_loss:0.281, val_acc:0.895]
Epoch [26/120    avg_loss:0.174, val_acc:0.916]
Epoch [27/120    avg_loss:0.153, val_acc:0.930]
Epoch [28/120    avg_loss:0.172, val_acc:0.904]
Epoch [29/120    avg_loss:0.166, val_acc:0.919]
Epoch [30/120    avg_loss:0.170, val_acc:0.889]
Epoch [31/120    avg_loss:0.163, val_acc:0.899]
Epoch [32/120    avg_loss:0.113, val_acc:0.921]
Epoch [33/120    avg_loss:0.103, val_acc:0.952]
Epoch [34/120    avg_loss:0.119, val_acc:0.909]
Epoch [35/120    avg_loss:0.120, val_acc:0.938]
Epoch [36/120    avg_loss:0.092, val_acc:0.940]
Epoch [37/120    avg_loss:0.073, val_acc:0.931]
Epoch [38/120    avg_loss:0.060, val_acc:0.953]
Epoch [39/120    avg_loss:0.067, val_acc:0.941]
Epoch [40/120    avg_loss:0.062, val_acc:0.928]
Epoch [41/120    avg_loss:0.085, val_acc:0.952]
Epoch [42/120    avg_loss:0.061, val_acc:0.949]
Epoch [43/120    avg_loss:0.054, val_acc:0.958]
Epoch [44/120    avg_loss:0.051, val_acc:0.939]
Epoch [45/120    avg_loss:0.059, val_acc:0.959]
Epoch [46/120    avg_loss:0.041, val_acc:0.958]
Epoch [47/120    avg_loss:0.040, val_acc:0.964]
Epoch [48/120    avg_loss:0.049, val_acc:0.949]
Epoch [49/120    avg_loss:0.053, val_acc:0.955]
Epoch [50/120    avg_loss:0.040, val_acc:0.947]
Epoch [51/120    avg_loss:0.072, val_acc:0.911]
Epoch [52/120    avg_loss:0.074, val_acc:0.948]
Epoch [53/120    avg_loss:0.054, val_acc:0.945]
Epoch [54/120    avg_loss:0.062, val_acc:0.958]
Epoch [55/120    avg_loss:0.048, val_acc:0.956]
Epoch [56/120    avg_loss:0.055, val_acc:0.944]
Epoch [57/120    avg_loss:0.048, val_acc:0.952]
Epoch [58/120    avg_loss:0.038, val_acc:0.955]
Epoch [59/120    avg_loss:0.047, val_acc:0.945]
Epoch [60/120    avg_loss:0.037, val_acc:0.963]
Epoch [61/120    avg_loss:0.022, val_acc:0.963]
Epoch [62/120    avg_loss:0.024, val_acc:0.964]
Epoch [63/120    avg_loss:0.018, val_acc:0.963]
Epoch [64/120    avg_loss:0.024, val_acc:0.963]
Epoch [65/120    avg_loss:0.016, val_acc:0.961]
Epoch [66/120    avg_loss:0.023, val_acc:0.960]
Epoch [67/120    avg_loss:0.017, val_acc:0.966]
Epoch [68/120    avg_loss:0.018, val_acc:0.965]
Epoch [69/120    avg_loss:0.020, val_acc:0.966]
Epoch [70/120    avg_loss:0.019, val_acc:0.966]
Epoch [71/120    avg_loss:0.019, val_acc:0.965]
Epoch [72/120    avg_loss:0.018, val_acc:0.968]
Epoch [73/120    avg_loss:0.017, val_acc:0.966]
Epoch [74/120    avg_loss:0.016, val_acc:0.965]
Epoch [75/120    avg_loss:0.022, val_acc:0.970]
Epoch [76/120    avg_loss:0.023, val_acc:0.967]
Epoch [77/120    avg_loss:0.016, val_acc:0.967]
Epoch [78/120    avg_loss:0.021, val_acc:0.966]
Epoch [79/120    avg_loss:0.017, val_acc:0.966]
Epoch [80/120    avg_loss:0.022, val_acc:0.965]
Epoch [81/120    avg_loss:0.015, val_acc:0.965]
Epoch [82/120    avg_loss:0.020, val_acc:0.967]
Epoch [83/120    avg_loss:0.016, val_acc:0.969]
Epoch [84/120    avg_loss:0.019, val_acc:0.969]
Epoch [85/120    avg_loss:0.016, val_acc:0.967]
Epoch [86/120    avg_loss:0.016, val_acc:0.967]
Epoch [87/120    avg_loss:0.018, val_acc:0.967]
Epoch [88/120    avg_loss:0.018, val_acc:0.967]
Epoch [89/120    avg_loss:0.017, val_acc:0.967]
Epoch [90/120    avg_loss:0.017, val_acc:0.967]
Epoch [91/120    avg_loss:0.017, val_acc:0.967]
Epoch [92/120    avg_loss:0.019, val_acc:0.967]
Epoch [93/120    avg_loss:0.016, val_acc:0.967]
Epoch [94/120    avg_loss:0.015, val_acc:0.966]
Epoch [95/120    avg_loss:0.016, val_acc:0.965]
Epoch [96/120    avg_loss:0.018, val_acc:0.965]
Epoch [97/120    avg_loss:0.017, val_acc:0.965]
Epoch [98/120    avg_loss:0.015, val_acc:0.965]
Epoch [99/120    avg_loss:0.014, val_acc:0.966]
Epoch [100/120    avg_loss:0.017, val_acc:0.966]
Epoch [101/120    avg_loss:0.021, val_acc:0.966]
Epoch [102/120    avg_loss:0.016, val_acc:0.966]
Epoch [103/120    avg_loss:0.018, val_acc:0.966]
Epoch [104/120    avg_loss:0.013, val_acc:0.966]
Epoch [105/120    avg_loss:0.015, val_acc:0.966]
Epoch [106/120    avg_loss:0.019, val_acc:0.966]
Epoch [107/120    avg_loss:0.016, val_acc:0.966]
Epoch [108/120    avg_loss:0.015, val_acc:0.966]
Epoch [109/120    avg_loss:0.016, val_acc:0.966]
Epoch [110/120    avg_loss:0.016, val_acc:0.966]
Epoch [111/120    avg_loss:0.016, val_acc:0.966]
Epoch [112/120    avg_loss:0.016, val_acc:0.966]
Epoch [113/120    avg_loss:0.017, val_acc:0.966]
Epoch [114/120    avg_loss:0.014, val_acc:0.966]
Epoch [115/120    avg_loss:0.016, val_acc:0.966]
Epoch [116/120    avg_loss:0.019, val_acc:0.966]
Epoch [117/120    avg_loss:0.014, val_acc:0.966]
Epoch [118/120    avg_loss:0.014, val_acc:0.966]
Epoch [119/120    avg_loss:0.018, val_acc:0.966]
Epoch [120/120    avg_loss:0.018, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1248    3    0    2    0    0    0    0    4   26    2    0
     0    0    0]
 [   0    0    0  725    0    0    0    0    0    1    5   13    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    3    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  648    0    0    0    0    8    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    4    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    1    0    0    0    0  843   25    0    0
     0    0    0]
 [   0    0   14    0    0    0    1    0    0    1   24 2158   12    0
     0    0    0]
 [   0    0    2    2    0    0    0    0    0    0    0    3  524    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1125   14    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    75  269    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.07317073170732

F1 scores:
[       nan 0.975      0.97690802 0.9817197  1.         0.98728324
 0.98705255 0.94339623 0.99883586 0.82352941 0.96177981 0.97141571
 0.97307335 1.         0.95948827 0.85126582 0.98809524]

Kappa:
0.9665966816352961
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f98458780>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.658, val_acc:0.445]
Epoch [2/120    avg_loss:2.189, val_acc:0.475]
Epoch [3/120    avg_loss:1.973, val_acc:0.549]
Epoch [4/120    avg_loss:1.758, val_acc:0.554]
Epoch [5/120    avg_loss:1.585, val_acc:0.555]
Epoch [6/120    avg_loss:1.474, val_acc:0.603]
Epoch [7/120    avg_loss:1.390, val_acc:0.629]
Epoch [8/120    avg_loss:1.297, val_acc:0.626]
Epoch [9/120    avg_loss:1.271, val_acc:0.623]
Epoch [10/120    avg_loss:1.139, val_acc:0.704]
Epoch [11/120    avg_loss:1.007, val_acc:0.722]
Epoch [12/120    avg_loss:0.926, val_acc:0.725]
Epoch [13/120    avg_loss:0.736, val_acc:0.748]
Epoch [14/120    avg_loss:0.921, val_acc:0.760]
Epoch [15/120    avg_loss:0.714, val_acc:0.772]
Epoch [16/120    avg_loss:0.675, val_acc:0.800]
Epoch [17/120    avg_loss:0.609, val_acc:0.820]
Epoch [18/120    avg_loss:0.511, val_acc:0.811]
Epoch [19/120    avg_loss:0.416, val_acc:0.803]
Epoch [20/120    avg_loss:0.437, val_acc:0.860]
Epoch [21/120    avg_loss:0.348, val_acc:0.886]
Epoch [22/120    avg_loss:0.306, val_acc:0.900]
Epoch [23/120    avg_loss:0.303, val_acc:0.872]
Epoch [24/120    avg_loss:0.326, val_acc:0.878]
Epoch [25/120    avg_loss:0.250, val_acc:0.872]
Epoch [26/120    avg_loss:0.200, val_acc:0.899]
Epoch [27/120    avg_loss:0.218, val_acc:0.892]
Epoch [28/120    avg_loss:0.234, val_acc:0.885]
Epoch [29/120    avg_loss:0.340, val_acc:0.868]
Epoch [30/120    avg_loss:0.207, val_acc:0.905]
Epoch [31/120    avg_loss:0.182, val_acc:0.877]
Epoch [32/120    avg_loss:0.182, val_acc:0.916]
Epoch [33/120    avg_loss:0.144, val_acc:0.931]
Epoch [34/120    avg_loss:0.130, val_acc:0.920]
Epoch [35/120    avg_loss:0.167, val_acc:0.932]
Epoch [36/120    avg_loss:0.116, val_acc:0.936]
Epoch [37/120    avg_loss:0.104, val_acc:0.947]
Epoch [38/120    avg_loss:0.095, val_acc:0.938]
Epoch [39/120    avg_loss:0.139, val_acc:0.933]
Epoch [40/120    avg_loss:0.114, val_acc:0.948]
Epoch [41/120    avg_loss:0.109, val_acc:0.931]
Epoch [42/120    avg_loss:0.092, val_acc:0.955]
Epoch [43/120    avg_loss:0.092, val_acc:0.946]
Epoch [44/120    avg_loss:0.073, val_acc:0.954]
Epoch [45/120    avg_loss:0.075, val_acc:0.956]
Epoch [46/120    avg_loss:0.071, val_acc:0.955]
Epoch [47/120    avg_loss:0.070, val_acc:0.959]
Epoch [48/120    avg_loss:0.072, val_acc:0.968]
Epoch [49/120    avg_loss:0.063, val_acc:0.958]
Epoch [50/120    avg_loss:0.074, val_acc:0.957]
Epoch [51/120    avg_loss:0.058, val_acc:0.960]
Epoch [52/120    avg_loss:0.045, val_acc:0.971]
Epoch [53/120    avg_loss:0.064, val_acc:0.952]
Epoch [54/120    avg_loss:0.092, val_acc:0.931]
Epoch [55/120    avg_loss:0.077, val_acc:0.944]
Epoch [56/120    avg_loss:0.063, val_acc:0.947]
Epoch [57/120    avg_loss:0.063, val_acc:0.960]
Epoch [58/120    avg_loss:0.045, val_acc:0.963]
Epoch [59/120    avg_loss:0.052, val_acc:0.958]
Epoch [60/120    avg_loss:0.033, val_acc:0.969]
Epoch [61/120    avg_loss:0.043, val_acc:0.964]
Epoch [62/120    avg_loss:0.046, val_acc:0.971]
Epoch [63/120    avg_loss:0.043, val_acc:0.974]
Epoch [64/120    avg_loss:0.027, val_acc:0.977]
Epoch [65/120    avg_loss:0.030, val_acc:0.971]
Epoch [66/120    avg_loss:0.034, val_acc:0.967]
Epoch [67/120    avg_loss:0.030, val_acc:0.974]
Epoch [68/120    avg_loss:0.019, val_acc:0.980]
Epoch [69/120    avg_loss:0.027, val_acc:0.966]
Epoch [70/120    avg_loss:0.030, val_acc:0.971]
Epoch [71/120    avg_loss:0.027, val_acc:0.970]
Epoch [72/120    avg_loss:0.018, val_acc:0.979]
Epoch [73/120    avg_loss:0.018, val_acc:0.975]
Epoch [74/120    avg_loss:0.015, val_acc:0.980]
Epoch [75/120    avg_loss:0.019, val_acc:0.973]
Epoch [76/120    avg_loss:0.015, val_acc:0.982]
Epoch [77/120    avg_loss:0.015, val_acc:0.977]
Epoch [78/120    avg_loss:0.021, val_acc:0.970]
Epoch [79/120    avg_loss:0.021, val_acc:0.977]
Epoch [80/120    avg_loss:0.021, val_acc:0.965]
Epoch [81/120    avg_loss:0.041, val_acc:0.963]
Epoch [82/120    avg_loss:0.043, val_acc:0.974]
Epoch [83/120    avg_loss:0.017, val_acc:0.972]
Epoch [84/120    avg_loss:0.016, val_acc:0.979]
Epoch [85/120    avg_loss:0.016, val_acc:0.974]
Epoch [86/120    avg_loss:0.015, val_acc:0.972]
Epoch [87/120    avg_loss:0.013, val_acc:0.978]
Epoch [88/120    avg_loss:0.012, val_acc:0.980]
Epoch [89/120    avg_loss:0.016, val_acc:0.976]
Epoch [90/120    avg_loss:0.013, val_acc:0.979]
Epoch [91/120    avg_loss:0.012, val_acc:0.981]
Epoch [92/120    avg_loss:0.012, val_acc:0.980]
Epoch [93/120    avg_loss:0.009, val_acc:0.979]
Epoch [94/120    avg_loss:0.010, val_acc:0.981]
Epoch [95/120    avg_loss:0.009, val_acc:0.980]
Epoch [96/120    avg_loss:0.006, val_acc:0.980]
Epoch [97/120    avg_loss:0.008, val_acc:0.980]
Epoch [98/120    avg_loss:0.008, val_acc:0.980]
Epoch [99/120    avg_loss:0.008, val_acc:0.982]
Epoch [100/120    avg_loss:0.009, val_acc:0.983]
Epoch [101/120    avg_loss:0.012, val_acc:0.982]
Epoch [102/120    avg_loss:0.008, val_acc:0.981]
Epoch [103/120    avg_loss:0.007, val_acc:0.980]
Epoch [104/120    avg_loss:0.009, val_acc:0.980]
Epoch [105/120    avg_loss:0.009, val_acc:0.981]
Epoch [106/120    avg_loss:0.007, val_acc:0.981]
Epoch [107/120    avg_loss:0.007, val_acc:0.982]
Epoch [108/120    avg_loss:0.009, val_acc:0.982]
Epoch [109/120    avg_loss:0.007, val_acc:0.982]
Epoch [110/120    avg_loss:0.008, val_acc:0.982]
Epoch [111/120    avg_loss:0.008, val_acc:0.982]
Epoch [112/120    avg_loss:0.008, val_acc:0.983]
Epoch [113/120    avg_loss:0.008, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.984]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.983]
Epoch [117/120    avg_loss:0.007, val_acc:0.982]
Epoch [118/120    avg_loss:0.011, val_acc:0.984]
Epoch [119/120    avg_loss:0.012, val_acc:0.984]
Epoch [120/120    avg_loss:0.007, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1260    1    3    0    0    0    0    1    7   12    1    0
     0    0    0]
 [   0    0    0  725    1    0    0    0    0    1    1    9    9    0
     1    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  653    0    0    2    1    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  849   19    0    0
     0    2    0]
 [   0    0   11    0    0    0    0    0    0    0   14 2173   11    1
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    0    0  529    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1125   13    0]
 [   0    0    1    0    0    0    2    0    0    0    0    0    0    0
    89  255    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.4308943089431

F1 scores:
[       nan 0.975      0.98360656 0.98105548 0.98598131 0.99538106
 0.99542683 1.         0.99883856 0.9        0.97084048 0.9823689
 0.97242647 0.99730458 0.95419847 0.82658023 0.96385542]

Kappa:
0.9706868407756976
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe6fb566780>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.629, val_acc:0.270]
Epoch [2/120    avg_loss:2.244, val_acc:0.465]
Epoch [3/120    avg_loss:1.987, val_acc:0.572]
Epoch [4/120    avg_loss:1.811, val_acc:0.567]
Epoch [5/120    avg_loss:1.669, val_acc:0.596]
Epoch [6/120    avg_loss:1.518, val_acc:0.620]
Epoch [7/120    avg_loss:1.354, val_acc:0.629]
Epoch [8/120    avg_loss:1.196, val_acc:0.650]
Epoch [9/120    avg_loss:1.054, val_acc:0.672]
Epoch [10/120    avg_loss:0.905, val_acc:0.701]
Epoch [11/120    avg_loss:0.860, val_acc:0.766]
Epoch [12/120    avg_loss:0.694, val_acc:0.816]
Epoch [13/120    avg_loss:0.621, val_acc:0.790]
Epoch [14/120    avg_loss:0.658, val_acc:0.800]
Epoch [15/120    avg_loss:0.501, val_acc:0.826]
Epoch [16/120    avg_loss:0.445, val_acc:0.843]
Epoch [17/120    avg_loss:0.428, val_acc:0.841]
Epoch [18/120    avg_loss:0.371, val_acc:0.842]
Epoch [19/120    avg_loss:0.361, val_acc:0.790]
Epoch [20/120    avg_loss:0.321, val_acc:0.871]
Epoch [21/120    avg_loss:0.274, val_acc:0.892]
Epoch [22/120    avg_loss:0.305, val_acc:0.863]
Epoch [23/120    avg_loss:0.373, val_acc:0.871]
Epoch [24/120    avg_loss:0.296, val_acc:0.797]
Epoch [25/120    avg_loss:0.247, val_acc:0.903]
Epoch [26/120    avg_loss:0.199, val_acc:0.899]
Epoch [27/120    avg_loss:0.171, val_acc:0.914]
Epoch [28/120    avg_loss:0.231, val_acc:0.876]
Epoch [29/120    avg_loss:0.240, val_acc:0.908]
Epoch [30/120    avg_loss:0.164, val_acc:0.906]
Epoch [31/120    avg_loss:0.169, val_acc:0.904]
Epoch [32/120    avg_loss:0.145, val_acc:0.918]
Epoch [33/120    avg_loss:0.120, val_acc:0.915]
Epoch [34/120    avg_loss:0.114, val_acc:0.918]
Epoch [35/120    avg_loss:0.104, val_acc:0.927]
Epoch [36/120    avg_loss:0.101, val_acc:0.932]
Epoch [37/120    avg_loss:0.101, val_acc:0.932]
Epoch [38/120    avg_loss:0.093, val_acc:0.947]
Epoch [39/120    avg_loss:0.074, val_acc:0.951]
Epoch [40/120    avg_loss:0.095, val_acc:0.905]
Epoch [41/120    avg_loss:0.125, val_acc:0.948]
Epoch [42/120    avg_loss:0.106, val_acc:0.924]
Epoch [43/120    avg_loss:0.085, val_acc:0.943]
Epoch [44/120    avg_loss:0.054, val_acc:0.951]
Epoch [45/120    avg_loss:0.075, val_acc:0.926]
Epoch [46/120    avg_loss:0.061, val_acc:0.945]
Epoch [47/120    avg_loss:0.066, val_acc:0.941]
Epoch [48/120    avg_loss:0.068, val_acc:0.933]
Epoch [49/120    avg_loss:0.049, val_acc:0.960]
Epoch [50/120    avg_loss:0.059, val_acc:0.938]
Epoch [51/120    avg_loss:0.069, val_acc:0.924]
Epoch [52/120    avg_loss:0.086, val_acc:0.942]
Epoch [53/120    avg_loss:0.068, val_acc:0.961]
Epoch [54/120    avg_loss:0.038, val_acc:0.967]
Epoch [55/120    avg_loss:0.044, val_acc:0.956]
Epoch [56/120    avg_loss:0.045, val_acc:0.967]
Epoch [57/120    avg_loss:0.036, val_acc:0.971]
Epoch [58/120    avg_loss:0.032, val_acc:0.972]
Epoch [59/120    avg_loss:0.045, val_acc:0.965]
Epoch [60/120    avg_loss:0.032, val_acc:0.964]
Epoch [61/120    avg_loss:0.040, val_acc:0.955]
Epoch [62/120    avg_loss:0.035, val_acc:0.969]
Epoch [63/120    avg_loss:0.024, val_acc:0.971]
Epoch [64/120    avg_loss:0.028, val_acc:0.967]
Epoch [65/120    avg_loss:0.028, val_acc:0.963]
Epoch [66/120    avg_loss:0.028, val_acc:0.972]
Epoch [67/120    avg_loss:0.029, val_acc:0.970]
Epoch [68/120    avg_loss:0.017, val_acc:0.974]
Epoch [69/120    avg_loss:0.018, val_acc:0.970]
Epoch [70/120    avg_loss:0.019, val_acc:0.971]
Epoch [71/120    avg_loss:0.019, val_acc:0.966]
Epoch [72/120    avg_loss:0.015, val_acc:0.975]
Epoch [73/120    avg_loss:0.023, val_acc:0.964]
Epoch [74/120    avg_loss:0.018, val_acc:0.970]
Epoch [75/120    avg_loss:0.021, val_acc:0.972]
Epoch [76/120    avg_loss:0.027, val_acc:0.948]
Epoch [77/120    avg_loss:0.030, val_acc:0.971]
Epoch [78/120    avg_loss:0.021, val_acc:0.971]
Epoch [79/120    avg_loss:0.012, val_acc:0.975]
Epoch [80/120    avg_loss:0.022, val_acc:0.967]
Epoch [81/120    avg_loss:0.015, val_acc:0.978]
Epoch [82/120    avg_loss:0.017, val_acc:0.975]
Epoch [83/120    avg_loss:0.013, val_acc:0.978]
Epoch [84/120    avg_loss:0.010, val_acc:0.973]
Epoch [85/120    avg_loss:0.012, val_acc:0.974]
Epoch [86/120    avg_loss:0.016, val_acc:0.969]
Epoch [87/120    avg_loss:0.009, val_acc:0.977]
Epoch [88/120    avg_loss:0.008, val_acc:0.973]
Epoch [89/120    avg_loss:0.009, val_acc:0.972]
Epoch [90/120    avg_loss:0.011, val_acc:0.971]
Epoch [91/120    avg_loss:0.009, val_acc:0.975]
Epoch [92/120    avg_loss:0.007, val_acc:0.972]
Epoch [93/120    avg_loss:0.006, val_acc:0.978]
Epoch [94/120    avg_loss:0.009, val_acc:0.972]
Epoch [95/120    avg_loss:0.013, val_acc:0.976]
Epoch [96/120    avg_loss:0.011, val_acc:0.970]
Epoch [97/120    avg_loss:0.014, val_acc:0.974]
Epoch [98/120    avg_loss:0.009, val_acc:0.976]
Epoch [99/120    avg_loss:0.009, val_acc:0.979]
Epoch [100/120    avg_loss:0.010, val_acc:0.970]
Epoch [101/120    avg_loss:0.013, val_acc:0.975]
Epoch [102/120    avg_loss:0.010, val_acc:0.970]
Epoch [103/120    avg_loss:0.011, val_acc:0.977]
Epoch [104/120    avg_loss:0.016, val_acc:0.965]
Epoch [105/120    avg_loss:0.025, val_acc:0.970]
Epoch [106/120    avg_loss:0.055, val_acc:0.885]
Epoch [107/120    avg_loss:0.068, val_acc:0.964]
Epoch [108/120    avg_loss:0.063, val_acc:0.960]
Epoch [109/120    avg_loss:0.089, val_acc:0.956]
Epoch [110/120    avg_loss:0.047, val_acc:0.960]
Epoch [111/120    avg_loss:0.036, val_acc:0.975]
Epoch [112/120    avg_loss:0.032, val_acc:0.972]
Epoch [113/120    avg_loss:0.013, val_acc:0.972]
Epoch [114/120    avg_loss:0.012, val_acc:0.973]
Epoch [115/120    avg_loss:0.010, val_acc:0.976]
Epoch [116/120    avg_loss:0.009, val_acc:0.975]
Epoch [117/120    avg_loss:0.010, val_acc:0.977]
Epoch [118/120    avg_loss:0.009, val_acc:0.978]
Epoch [119/120    avg_loss:0.008, val_acc:0.976]
Epoch [120/120    avg_loss:0.009, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1249    7    3    6    0    0    0    0    1   19    0    0
     0    0    0]
 [   0    0    1  721    4    6    0    0    0    1    0    8    6    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    0    0
     0    1    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  848   20    1    0
     0    0    0]
 [   0    0   12    4    0    1    0    0    0    2    5 2167   19    0
     0    0    0]
 [   0    0    2    2    0    0    0    0    0    0    0    8  521    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1126   12    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    57  277    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    7    0
     0    0   77]]

Accuracy:
97.37669376693766

F1 scores:
[       nan 1.         0.9776908  0.97300945 0.98148148 0.98065984
 0.98944193 1.         0.99883586 0.9        0.98091382 0.97788809
 0.95772059 1.         0.96776966 0.86970173 0.95061728]

Kappa:
0.9700763594688085
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f29de8a27f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.590, val_acc:0.324]
Epoch [2/120    avg_loss:2.166, val_acc:0.542]
Epoch [3/120    avg_loss:1.971, val_acc:0.576]
Epoch [4/120    avg_loss:1.819, val_acc:0.600]
Epoch [5/120    avg_loss:1.715, val_acc:0.589]
Epoch [6/120    avg_loss:1.610, val_acc:0.583]
Epoch [7/120    avg_loss:1.432, val_acc:0.653]
Epoch [8/120    avg_loss:1.313, val_acc:0.665]
Epoch [9/120    avg_loss:1.175, val_acc:0.710]
Epoch [10/120    avg_loss:1.010, val_acc:0.717]
Epoch [11/120    avg_loss:0.858, val_acc:0.706]
Epoch [12/120    avg_loss:0.847, val_acc:0.738]
Epoch [13/120    avg_loss:0.727, val_acc:0.767]
Epoch [14/120    avg_loss:0.600, val_acc:0.771]
Epoch [15/120    avg_loss:0.533, val_acc:0.798]
Epoch [16/120    avg_loss:0.478, val_acc:0.823]
Epoch [17/120    avg_loss:0.492, val_acc:0.838]
Epoch [18/120    avg_loss:0.399, val_acc:0.841]
Epoch [19/120    avg_loss:0.352, val_acc:0.850]
Epoch [20/120    avg_loss:0.287, val_acc:0.897]
Epoch [21/120    avg_loss:0.273, val_acc:0.877]
Epoch [22/120    avg_loss:0.307, val_acc:0.909]
Epoch [23/120    avg_loss:0.226, val_acc:0.902]
Epoch [24/120    avg_loss:0.250, val_acc:0.917]
Epoch [25/120    avg_loss:0.266, val_acc:0.893]
Epoch [26/120    avg_loss:0.240, val_acc:0.926]
Epoch [27/120    avg_loss:0.164, val_acc:0.929]
Epoch [28/120    avg_loss:0.161, val_acc:0.911]
Epoch [29/120    avg_loss:0.143, val_acc:0.947]
Epoch [30/120    avg_loss:0.147, val_acc:0.937]
Epoch [31/120    avg_loss:0.164, val_acc:0.933]
Epoch [32/120    avg_loss:0.140, val_acc:0.933]
Epoch [33/120    avg_loss:0.103, val_acc:0.935]
Epoch [34/120    avg_loss:0.093, val_acc:0.949]
Epoch [35/120    avg_loss:0.093, val_acc:0.952]
Epoch [36/120    avg_loss:0.087, val_acc:0.953]
Epoch [37/120    avg_loss:0.094, val_acc:0.957]
Epoch [38/120    avg_loss:0.077, val_acc:0.946]
Epoch [39/120    avg_loss:0.092, val_acc:0.920]
Epoch [40/120    avg_loss:0.108, val_acc:0.960]
Epoch [41/120    avg_loss:0.073, val_acc:0.954]
Epoch [42/120    avg_loss:0.075, val_acc:0.952]
Epoch [43/120    avg_loss:0.073, val_acc:0.939]
Epoch [44/120    avg_loss:0.092, val_acc:0.947]
Epoch [45/120    avg_loss:0.131, val_acc:0.962]
Epoch [46/120    avg_loss:0.074, val_acc:0.952]
Epoch [47/120    avg_loss:0.068, val_acc:0.959]
Epoch [48/120    avg_loss:0.073, val_acc:0.969]
Epoch [49/120    avg_loss:0.089, val_acc:0.949]
Epoch [50/120    avg_loss:0.144, val_acc:0.956]
Epoch [51/120    avg_loss:0.076, val_acc:0.965]
Epoch [52/120    avg_loss:0.060, val_acc:0.969]
Epoch [53/120    avg_loss:0.049, val_acc:0.962]
Epoch [54/120    avg_loss:0.042, val_acc:0.964]
Epoch [55/120    avg_loss:0.056, val_acc:0.936]
Epoch [56/120    avg_loss:0.080, val_acc:0.967]
Epoch [57/120    avg_loss:0.044, val_acc:0.971]
Epoch [58/120    avg_loss:0.047, val_acc:0.966]
Epoch [59/120    avg_loss:0.055, val_acc:0.966]
Epoch [60/120    avg_loss:0.047, val_acc:0.968]
Epoch [61/120    avg_loss:0.043, val_acc:0.971]
Epoch [62/120    avg_loss:0.028, val_acc:0.967]
Epoch [63/120    avg_loss:0.047, val_acc:0.956]
Epoch [64/120    avg_loss:0.061, val_acc:0.967]
Epoch [65/120    avg_loss:0.041, val_acc:0.962]
Epoch [66/120    avg_loss:0.033, val_acc:0.964]
Epoch [67/120    avg_loss:0.028, val_acc:0.972]
Epoch [68/120    avg_loss:0.028, val_acc:0.959]
Epoch [69/120    avg_loss:0.023, val_acc:0.979]
Epoch [70/120    avg_loss:0.024, val_acc:0.976]
Epoch [71/120    avg_loss:0.047, val_acc:0.967]
Epoch [72/120    avg_loss:0.024, val_acc:0.972]
Epoch [73/120    avg_loss:0.027, val_acc:0.979]
Epoch [74/120    avg_loss:0.027, val_acc:0.974]
Epoch [75/120    avg_loss:0.025, val_acc:0.969]
Epoch [76/120    avg_loss:0.022, val_acc:0.972]
Epoch [77/120    avg_loss:0.019, val_acc:0.969]
Epoch [78/120    avg_loss:0.019, val_acc:0.973]
Epoch [79/120    avg_loss:0.019, val_acc:0.978]
Epoch [80/120    avg_loss:0.020, val_acc:0.979]
Epoch [81/120    avg_loss:0.022, val_acc:0.979]
Epoch [82/120    avg_loss:0.020, val_acc:0.978]
Epoch [83/120    avg_loss:0.018, val_acc:0.975]
Epoch [84/120    avg_loss:0.012, val_acc:0.982]
Epoch [85/120    avg_loss:0.014, val_acc:0.982]
Epoch [86/120    avg_loss:0.018, val_acc:0.978]
Epoch [87/120    avg_loss:0.015, val_acc:0.976]
Epoch [88/120    avg_loss:0.017, val_acc:0.979]
Epoch [89/120    avg_loss:0.057, val_acc:0.962]
Epoch [90/120    avg_loss:0.026, val_acc:0.973]
Epoch [91/120    avg_loss:0.028, val_acc:0.966]
Epoch [92/120    avg_loss:0.017, val_acc:0.978]
Epoch [93/120    avg_loss:0.016, val_acc:0.973]
Epoch [94/120    avg_loss:0.016, val_acc:0.978]
Epoch [95/120    avg_loss:0.010, val_acc:0.979]
Epoch [96/120    avg_loss:0.018, val_acc:0.965]
Epoch [97/120    avg_loss:0.050, val_acc:0.951]
Epoch [98/120    avg_loss:0.057, val_acc:0.964]
Epoch [99/120    avg_loss:0.034, val_acc:0.975]
Epoch [100/120    avg_loss:0.024, val_acc:0.975]
Epoch [101/120    avg_loss:0.023, val_acc:0.978]
Epoch [102/120    avg_loss:0.016, val_acc:0.981]
Epoch [103/120    avg_loss:0.015, val_acc:0.981]
Epoch [104/120    avg_loss:0.016, val_acc:0.980]
Epoch [105/120    avg_loss:0.016, val_acc:0.979]
Epoch [106/120    avg_loss:0.016, val_acc:0.980]
Epoch [107/120    avg_loss:0.014, val_acc:0.978]
Epoch [108/120    avg_loss:0.015, val_acc:0.979]
Epoch [109/120    avg_loss:0.018, val_acc:0.978]
Epoch [110/120    avg_loss:0.014, val_acc:0.982]
Epoch [111/120    avg_loss:0.014, val_acc:0.980]
Epoch [112/120    avg_loss:0.012, val_acc:0.980]
Epoch [113/120    avg_loss:0.012, val_acc:0.980]
Epoch [114/120    avg_loss:0.011, val_acc:0.979]
Epoch [115/120    avg_loss:0.010, val_acc:0.980]
Epoch [116/120    avg_loss:0.011, val_acc:0.980]
Epoch [117/120    avg_loss:0.008, val_acc:0.980]
Epoch [118/120    avg_loss:0.012, val_acc:0.980]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.009, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1262    2    0    0    0    0    0    0    4   17    0    0
     0    0    0]
 [   0    0    0  733    2    0    0    0    0    1    3    0    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    1    7    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    1    0    0    0  867    4    0    0
     0    1    0]
 [   0    0    7    1    0    0    3    0    1    0   12 2171   14    1
     0    0    0]
 [   0    0    2    4    1    0    0    0    0    0    6    1  516    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1137    2    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
    36  304    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.28726287262873

F1 scores:
[       nan 1.         0.98670837 0.98587761 0.99300699 0.99071926
 0.98942598 0.87719298 0.99883856 0.97297297 0.98132428 0.98614581
 0.96178938 0.99730458 0.98271392 0.92824427 0.97647059]

Kappa:
0.9804754644684128
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f194a3c17f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.616, val_acc:0.373]
Epoch [2/120    avg_loss:2.224, val_acc:0.482]
Epoch [3/120    avg_loss:2.015, val_acc:0.545]
Epoch [4/120    avg_loss:1.818, val_acc:0.571]
Epoch [5/120    avg_loss:1.656, val_acc:0.607]
Epoch [6/120    avg_loss:1.449, val_acc:0.634]
Epoch [7/120    avg_loss:1.259, val_acc:0.658]
Epoch [8/120    avg_loss:1.071, val_acc:0.698]
Epoch [9/120    avg_loss:0.908, val_acc:0.745]
Epoch [10/120    avg_loss:0.804, val_acc:0.749]
Epoch [11/120    avg_loss:0.721, val_acc:0.781]
Epoch [12/120    avg_loss:0.622, val_acc:0.812]
Epoch [13/120    avg_loss:0.541, val_acc:0.768]
Epoch [14/120    avg_loss:0.475, val_acc:0.784]
Epoch [15/120    avg_loss:0.442, val_acc:0.827]
Epoch [16/120    avg_loss:0.459, val_acc:0.810]
Epoch [17/120    avg_loss:0.432, val_acc:0.830]
Epoch [18/120    avg_loss:0.348, val_acc:0.860]
Epoch [19/120    avg_loss:0.336, val_acc:0.814]
Epoch [20/120    avg_loss:0.313, val_acc:0.844]
Epoch [21/120    avg_loss:0.331, val_acc:0.837]
Epoch [22/120    avg_loss:0.306, val_acc:0.851]
Epoch [23/120    avg_loss:0.238, val_acc:0.885]
Epoch [24/120    avg_loss:0.214, val_acc:0.890]
Epoch [25/120    avg_loss:0.204, val_acc:0.850]
Epoch [26/120    avg_loss:0.204, val_acc:0.895]
Epoch [27/120    avg_loss:0.159, val_acc:0.896]
Epoch [28/120    avg_loss:0.145, val_acc:0.903]
Epoch [29/120    avg_loss:0.127, val_acc:0.918]
Epoch [30/120    avg_loss:0.138, val_acc:0.887]
Epoch [31/120    avg_loss:0.161, val_acc:0.900]
Epoch [32/120    avg_loss:0.133, val_acc:0.910]
Epoch [33/120    avg_loss:0.125, val_acc:0.913]
Epoch [34/120    avg_loss:0.112, val_acc:0.924]
Epoch [35/120    avg_loss:0.113, val_acc:0.926]
Epoch [36/120    avg_loss:0.109, val_acc:0.901]
Epoch [37/120    avg_loss:0.096, val_acc:0.913]
Epoch [38/120    avg_loss:0.090, val_acc:0.947]
Epoch [39/120    avg_loss:0.107, val_acc:0.932]
Epoch [40/120    avg_loss:0.140, val_acc:0.918]
Epoch [41/120    avg_loss:0.123, val_acc:0.900]
Epoch [42/120    avg_loss:0.117, val_acc:0.929]
Epoch [43/120    avg_loss:0.123, val_acc:0.913]
Epoch [44/120    avg_loss:0.106, val_acc:0.916]
Epoch [45/120    avg_loss:0.093, val_acc:0.938]
Epoch [46/120    avg_loss:0.066, val_acc:0.954]
Epoch [47/120    avg_loss:0.056, val_acc:0.949]
Epoch [48/120    avg_loss:0.058, val_acc:0.951]
Epoch [49/120    avg_loss:0.079, val_acc:0.943]
Epoch [50/120    avg_loss:0.058, val_acc:0.952]
Epoch [51/120    avg_loss:0.065, val_acc:0.937]
Epoch [52/120    avg_loss:0.081, val_acc:0.938]
Epoch [53/120    avg_loss:0.069, val_acc:0.955]
Epoch [54/120    avg_loss:0.053, val_acc:0.948]
Epoch [55/120    avg_loss:0.070, val_acc:0.923]
Epoch [56/120    avg_loss:0.066, val_acc:0.944]
Epoch [57/120    avg_loss:0.048, val_acc:0.948]
Epoch [58/120    avg_loss:0.084, val_acc:0.935]
Epoch [59/120    avg_loss:0.090, val_acc:0.936]
Epoch [60/120    avg_loss:0.057, val_acc:0.951]
Epoch [61/120    avg_loss:0.055, val_acc:0.948]
Epoch [62/120    avg_loss:0.045, val_acc:0.953]
Epoch [63/120    avg_loss:0.042, val_acc:0.958]
Epoch [64/120    avg_loss:0.051, val_acc:0.948]
Epoch [65/120    avg_loss:0.047, val_acc:0.959]
Epoch [66/120    avg_loss:0.044, val_acc:0.952]
Epoch [67/120    avg_loss:0.044, val_acc:0.959]
Epoch [68/120    avg_loss:0.045, val_acc:0.957]
Epoch [69/120    avg_loss:0.047, val_acc:0.965]
Epoch [70/120    avg_loss:0.025, val_acc:0.975]
Epoch [71/120    avg_loss:0.029, val_acc:0.967]
Epoch [72/120    avg_loss:0.019, val_acc:0.968]
Epoch [73/120    avg_loss:0.061, val_acc:0.952]
Epoch [74/120    avg_loss:0.050, val_acc:0.952]
Epoch [75/120    avg_loss:0.035, val_acc:0.966]
Epoch [76/120    avg_loss:0.038, val_acc:0.957]
Epoch [77/120    avg_loss:0.038, val_acc:0.964]
Epoch [78/120    avg_loss:0.031, val_acc:0.968]
Epoch [79/120    avg_loss:0.029, val_acc:0.968]
Epoch [80/120    avg_loss:0.023, val_acc:0.967]
Epoch [81/120    avg_loss:0.027, val_acc:0.951]
Epoch [82/120    avg_loss:0.030, val_acc:0.963]
Epoch [83/120    avg_loss:0.020, val_acc:0.970]
Epoch [84/120    avg_loss:0.015, val_acc:0.971]
Epoch [85/120    avg_loss:0.019, val_acc:0.971]
Epoch [86/120    avg_loss:0.013, val_acc:0.972]
Epoch [87/120    avg_loss:0.012, val_acc:0.973]
Epoch [88/120    avg_loss:0.011, val_acc:0.973]
Epoch [89/120    avg_loss:0.012, val_acc:0.973]
Epoch [90/120    avg_loss:0.015, val_acc:0.975]
Epoch [91/120    avg_loss:0.010, val_acc:0.976]
Epoch [92/120    avg_loss:0.011, val_acc:0.975]
Epoch [93/120    avg_loss:0.013, val_acc:0.974]
Epoch [94/120    avg_loss:0.014, val_acc:0.976]
Epoch [95/120    avg_loss:0.010, val_acc:0.975]
Epoch [96/120    avg_loss:0.013, val_acc:0.979]
Epoch [97/120    avg_loss:0.012, val_acc:0.975]
Epoch [98/120    avg_loss:0.011, val_acc:0.976]
Epoch [99/120    avg_loss:0.009, val_acc:0.976]
Epoch [100/120    avg_loss:0.012, val_acc:0.976]
Epoch [101/120    avg_loss:0.010, val_acc:0.976]
Epoch [102/120    avg_loss:0.012, val_acc:0.976]
Epoch [103/120    avg_loss:0.010, val_acc:0.976]
Epoch [104/120    avg_loss:0.010, val_acc:0.976]
Epoch [105/120    avg_loss:0.009, val_acc:0.975]
Epoch [106/120    avg_loss:0.010, val_acc:0.976]
Epoch [107/120    avg_loss:0.013, val_acc:0.975]
Epoch [108/120    avg_loss:0.010, val_acc:0.978]
Epoch [109/120    avg_loss:0.012, val_acc:0.978]
Epoch [110/120    avg_loss:0.010, val_acc:0.978]
Epoch [111/120    avg_loss:0.010, val_acc:0.978]
Epoch [112/120    avg_loss:0.010, val_acc:0.978]
Epoch [113/120    avg_loss:0.009, val_acc:0.978]
Epoch [114/120    avg_loss:0.010, val_acc:0.978]
Epoch [115/120    avg_loss:0.008, val_acc:0.978]
Epoch [116/120    avg_loss:0.011, val_acc:0.978]
Epoch [117/120    avg_loss:0.010, val_acc:0.978]
Epoch [118/120    avg_loss:0.008, val_acc:0.978]
Epoch [119/120    avg_loss:0.010, val_acc:0.978]
Epoch [120/120    avg_loss:0.013, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1255    1    0    0    1    0    0    1    3   24    0    0
     0    0    0]
 [   0    0    0  724    0    0    0    0    0    4    1    5   12    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    3    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    5    0    0    2    0    0    0    1  848   17    1    0
     0    1    0]
 [   0    0    4    0    0    1    1    0    0    3   17 2182    0    1
     0    1    0]
 [   0    0    0    4    0    3    0    0    0    0    3    4  518    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1132    7    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    29  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.07046070460704

F1 scores:
[       nan 0.93506494 0.98469988 0.98102981 1.         0.98853211
 0.99619193 1.         0.99649942 0.77272727 0.96638177 0.98221922
 0.97003745 0.99730458 0.98136107 0.94222222 0.98203593]

Kappa:
0.9779896536024136
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7bf95c0828>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.616, val_acc:0.456]
Epoch [2/120    avg_loss:2.222, val_acc:0.495]
Epoch [3/120    avg_loss:1.955, val_acc:0.512]
Epoch [4/120    avg_loss:1.758, val_acc:0.575]
Epoch [5/120    avg_loss:1.585, val_acc:0.553]
Epoch [6/120    avg_loss:1.419, val_acc:0.665]
Epoch [7/120    avg_loss:1.214, val_acc:0.675]
Epoch [8/120    avg_loss:1.012, val_acc:0.686]
Epoch [9/120    avg_loss:0.961, val_acc:0.657]
Epoch [10/120    avg_loss:0.919, val_acc:0.724]
Epoch [11/120    avg_loss:0.788, val_acc:0.747]
Epoch [12/120    avg_loss:0.711, val_acc:0.730]
Epoch [13/120    avg_loss:0.630, val_acc:0.791]
Epoch [14/120    avg_loss:0.547, val_acc:0.788]
Epoch [15/120    avg_loss:0.501, val_acc:0.829]
Epoch [16/120    avg_loss:0.514, val_acc:0.792]
Epoch [17/120    avg_loss:0.512, val_acc:0.830]
Epoch [18/120    avg_loss:0.423, val_acc:0.850]
Epoch [19/120    avg_loss:0.338, val_acc:0.859]
Epoch [20/120    avg_loss:0.315, val_acc:0.863]
Epoch [21/120    avg_loss:0.301, val_acc:0.850]
Epoch [22/120    avg_loss:0.366, val_acc:0.850]
Epoch [23/120    avg_loss:0.313, val_acc:0.864]
Epoch [24/120    avg_loss:0.280, val_acc:0.856]
Epoch [25/120    avg_loss:0.261, val_acc:0.888]
Epoch [26/120    avg_loss:0.218, val_acc:0.910]
Epoch [27/120    avg_loss:0.208, val_acc:0.863]
Epoch [28/120    avg_loss:0.168, val_acc:0.934]
Epoch [29/120    avg_loss:0.166, val_acc:0.893]
Epoch [30/120    avg_loss:0.162, val_acc:0.908]
Epoch [31/120    avg_loss:0.165, val_acc:0.901]
Epoch [32/120    avg_loss:0.128, val_acc:0.904]
Epoch [33/120    avg_loss:0.119, val_acc:0.916]
Epoch [34/120    avg_loss:0.128, val_acc:0.941]
Epoch [35/120    avg_loss:0.119, val_acc:0.943]
Epoch [36/120    avg_loss:0.093, val_acc:0.946]
Epoch [37/120    avg_loss:0.091, val_acc:0.947]
Epoch [38/120    avg_loss:0.075, val_acc:0.948]
Epoch [39/120    avg_loss:0.088, val_acc:0.947]
Epoch [40/120    avg_loss:0.104, val_acc:0.937]
Epoch [41/120    avg_loss:0.070, val_acc:0.958]
Epoch [42/120    avg_loss:0.076, val_acc:0.955]
Epoch [43/120    avg_loss:0.066, val_acc:0.947]
Epoch [44/120    avg_loss:0.061, val_acc:0.955]
Epoch [45/120    avg_loss:0.064, val_acc:0.956]
Epoch [46/120    avg_loss:0.065, val_acc:0.951]
Epoch [47/120    avg_loss:0.150, val_acc:0.854]
Epoch [48/120    avg_loss:0.144, val_acc:0.934]
Epoch [49/120    avg_loss:0.081, val_acc:0.956]
Epoch [50/120    avg_loss:0.080, val_acc:0.958]
Epoch [51/120    avg_loss:0.058, val_acc:0.939]
Epoch [52/120    avg_loss:0.052, val_acc:0.943]
Epoch [53/120    avg_loss:0.048, val_acc:0.966]
Epoch [54/120    avg_loss:0.048, val_acc:0.929]
Epoch [55/120    avg_loss:0.048, val_acc:0.963]
Epoch [56/120    avg_loss:0.051, val_acc:0.952]
Epoch [57/120    avg_loss:0.170, val_acc:0.922]
Epoch [58/120    avg_loss:0.100, val_acc:0.932]
Epoch [59/120    avg_loss:0.076, val_acc:0.938]
Epoch [60/120    avg_loss:0.060, val_acc:0.942]
Epoch [61/120    avg_loss:0.053, val_acc:0.941]
Epoch [62/120    avg_loss:0.080, val_acc:0.947]
Epoch [63/120    avg_loss:0.046, val_acc:0.965]
Epoch [64/120    avg_loss:0.034, val_acc:0.966]
Epoch [65/120    avg_loss:0.029, val_acc:0.964]
Epoch [66/120    avg_loss:0.025, val_acc:0.962]
Epoch [67/120    avg_loss:0.030, val_acc:0.972]
Epoch [68/120    avg_loss:0.037, val_acc:0.966]
Epoch [69/120    avg_loss:0.033, val_acc:0.967]
Epoch [70/120    avg_loss:0.024, val_acc:0.971]
Epoch [71/120    avg_loss:0.024, val_acc:0.968]
Epoch [72/120    avg_loss:0.025, val_acc:0.960]
Epoch [73/120    avg_loss:0.023, val_acc:0.980]
Epoch [74/120    avg_loss:0.021, val_acc:0.980]
Epoch [75/120    avg_loss:0.018, val_acc:0.974]
Epoch [76/120    avg_loss:0.019, val_acc:0.979]
Epoch [77/120    avg_loss:0.015, val_acc:0.974]
Epoch [78/120    avg_loss:0.017, val_acc:0.972]
Epoch [79/120    avg_loss:0.022, val_acc:0.970]
Epoch [80/120    avg_loss:0.029, val_acc:0.965]
Epoch [81/120    avg_loss:0.031, val_acc:0.963]
Epoch [82/120    avg_loss:0.029, val_acc:0.968]
Epoch [83/120    avg_loss:0.017, val_acc:0.974]
Epoch [84/120    avg_loss:0.014, val_acc:0.978]
Epoch [85/120    avg_loss:0.014, val_acc:0.971]
Epoch [86/120    avg_loss:0.018, val_acc:0.974]
Epoch [87/120    avg_loss:0.012, val_acc:0.970]
Epoch [88/120    avg_loss:0.011, val_acc:0.976]
Epoch [89/120    avg_loss:0.013, val_acc:0.981]
Epoch [90/120    avg_loss:0.010, val_acc:0.981]
Epoch [91/120    avg_loss:0.009, val_acc:0.981]
Epoch [92/120    avg_loss:0.011, val_acc:0.980]
Epoch [93/120    avg_loss:0.008, val_acc:0.982]
Epoch [94/120    avg_loss:0.011, val_acc:0.979]
Epoch [95/120    avg_loss:0.010, val_acc:0.978]
Epoch [96/120    avg_loss:0.010, val_acc:0.979]
Epoch [97/120    avg_loss:0.009, val_acc:0.978]
Epoch [98/120    avg_loss:0.009, val_acc:0.978]
Epoch [99/120    avg_loss:0.009, val_acc:0.978]
Epoch [100/120    avg_loss:0.010, val_acc:0.979]
Epoch [101/120    avg_loss:0.009, val_acc:0.980]
Epoch [102/120    avg_loss:0.007, val_acc:0.976]
Epoch [103/120    avg_loss:0.008, val_acc:0.978]
Epoch [104/120    avg_loss:0.009, val_acc:0.980]
Epoch [105/120    avg_loss:0.009, val_acc:0.981]
Epoch [106/120    avg_loss:0.009, val_acc:0.981]
Epoch [107/120    avg_loss:0.008, val_acc:0.981]
Epoch [108/120    avg_loss:0.007, val_acc:0.981]
Epoch [109/120    avg_loss:0.011, val_acc:0.981]
Epoch [110/120    avg_loss:0.007, val_acc:0.981]
Epoch [111/120    avg_loss:0.007, val_acc:0.981]
Epoch [112/120    avg_loss:0.007, val_acc:0.981]
Epoch [113/120    avg_loss:0.008, val_acc:0.981]
Epoch [114/120    avg_loss:0.008, val_acc:0.981]
Epoch [115/120    avg_loss:0.009, val_acc:0.981]
Epoch [116/120    avg_loss:0.006, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.981]
Epoch [118/120    avg_loss:0.008, val_acc:0.982]
Epoch [119/120    avg_loss:0.008, val_acc:0.981]
Epoch [120/120    avg_loss:0.012, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1259    1    3    0    3    0    0    0    7   10    0    0
     0    2    0]
 [   0    0    0  721    1    3    0    0    0    3    1    1   13    2
     2    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    3    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    1  854   16    1    0
     0    1    0]
 [   0    0    8    0    0    1    0    0    0    0   15 2167   13    0
     0    6    0]
 [   0    0    0    8    1    0    0    0    0    0    3    1  518    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    0    8    0    0    3    0    0    0    0
    29  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.94037940379404

F1 scores:
[       nan 0.97560976 0.98667712 0.97630332 0.98839907 0.98853211
 0.99017385 1.         0.99883586 0.7826087  0.97266515 0.98365865
 0.95925926 0.99462366 0.981755   0.91233284 0.98224852]

Kappa:
0.9765269489241407
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f68ea69f7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.593, val_acc:0.450]
Epoch [2/120    avg_loss:2.201, val_acc:0.485]
Epoch [3/120    avg_loss:2.008, val_acc:0.529]
Epoch [4/120    avg_loss:1.813, val_acc:0.551]
Epoch [5/120    avg_loss:1.672, val_acc:0.567]
Epoch [6/120    avg_loss:1.504, val_acc:0.625]
Epoch [7/120    avg_loss:1.375, val_acc:0.593]
Epoch [8/120    avg_loss:1.220, val_acc:0.657]
Epoch [9/120    avg_loss:1.081, val_acc:0.671]
Epoch [10/120    avg_loss:0.948, val_acc:0.731]
Epoch [11/120    avg_loss:0.878, val_acc:0.724]
Epoch [12/120    avg_loss:0.733, val_acc:0.736]
Epoch [13/120    avg_loss:0.694, val_acc:0.764]
Epoch [14/120    avg_loss:0.621, val_acc:0.776]
Epoch [15/120    avg_loss:0.570, val_acc:0.737]
Epoch [16/120    avg_loss:0.549, val_acc:0.822]
Epoch [17/120    avg_loss:0.451, val_acc:0.818]
Epoch [18/120    avg_loss:0.548, val_acc:0.819]
Epoch [19/120    avg_loss:0.412, val_acc:0.816]
Epoch [20/120    avg_loss:0.441, val_acc:0.855]
Epoch [21/120    avg_loss:0.391, val_acc:0.837]
Epoch [22/120    avg_loss:0.355, val_acc:0.878]
Epoch [23/120    avg_loss:0.267, val_acc:0.879]
Epoch [24/120    avg_loss:0.246, val_acc:0.868]
Epoch [25/120    avg_loss:0.229, val_acc:0.868]
Epoch [26/120    avg_loss:0.225, val_acc:0.876]
Epoch [27/120    avg_loss:0.177, val_acc:0.912]
Epoch [28/120    avg_loss:0.173, val_acc:0.916]
Epoch [29/120    avg_loss:0.178, val_acc:0.839]
Epoch [30/120    avg_loss:0.208, val_acc:0.905]
Epoch [31/120    avg_loss:0.141, val_acc:0.903]
Epoch [32/120    avg_loss:0.133, val_acc:0.851]
Epoch [33/120    avg_loss:0.161, val_acc:0.891]
Epoch [34/120    avg_loss:0.135, val_acc:0.926]
Epoch [35/120    avg_loss:0.098, val_acc:0.919]
Epoch [36/120    avg_loss:0.100, val_acc:0.917]
Epoch [37/120    avg_loss:0.120, val_acc:0.928]
Epoch [38/120    avg_loss:0.100, val_acc:0.926]
Epoch [39/120    avg_loss:0.102, val_acc:0.933]
Epoch [40/120    avg_loss:0.086, val_acc:0.940]
Epoch [41/120    avg_loss:0.079, val_acc:0.940]
Epoch [42/120    avg_loss:0.084, val_acc:0.929]
Epoch [43/120    avg_loss:0.068, val_acc:0.937]
Epoch [44/120    avg_loss:0.083, val_acc:0.944]
Epoch [45/120    avg_loss:0.067, val_acc:0.944]
Epoch [46/120    avg_loss:0.078, val_acc:0.955]
Epoch [47/120    avg_loss:0.095, val_acc:0.923]
Epoch [48/120    avg_loss:0.115, val_acc:0.942]
Epoch [49/120    avg_loss:0.078, val_acc:0.938]
Epoch [50/120    avg_loss:0.069, val_acc:0.944]
Epoch [51/120    avg_loss:0.102, val_acc:0.918]
Epoch [52/120    avg_loss:0.093, val_acc:0.943]
Epoch [53/120    avg_loss:0.094, val_acc:0.940]
Epoch [54/120    avg_loss:0.119, val_acc:0.937]
Epoch [55/120    avg_loss:0.182, val_acc:0.911]
Epoch [56/120    avg_loss:0.119, val_acc:0.933]
Epoch [57/120    avg_loss:0.083, val_acc:0.936]
Epoch [58/120    avg_loss:0.059, val_acc:0.938]
Epoch [59/120    avg_loss:0.075, val_acc:0.934]
Epoch [60/120    avg_loss:0.054, val_acc:0.953]
Epoch [61/120    avg_loss:0.043, val_acc:0.954]
Epoch [62/120    avg_loss:0.035, val_acc:0.957]
Epoch [63/120    avg_loss:0.033, val_acc:0.957]
Epoch [64/120    avg_loss:0.038, val_acc:0.957]
Epoch [65/120    avg_loss:0.034, val_acc:0.958]
Epoch [66/120    avg_loss:0.029, val_acc:0.959]
Epoch [67/120    avg_loss:0.036, val_acc:0.962]
Epoch [68/120    avg_loss:0.028, val_acc:0.962]
Epoch [69/120    avg_loss:0.037, val_acc:0.960]
Epoch [70/120    avg_loss:0.037, val_acc:0.960]
Epoch [71/120    avg_loss:0.030, val_acc:0.957]
Epoch [72/120    avg_loss:0.031, val_acc:0.962]
Epoch [73/120    avg_loss:0.036, val_acc:0.962]
Epoch [74/120    avg_loss:0.027, val_acc:0.962]
Epoch [75/120    avg_loss:0.027, val_acc:0.958]
Epoch [76/120    avg_loss:0.029, val_acc:0.963]
Epoch [77/120    avg_loss:0.029, val_acc:0.963]
Epoch [78/120    avg_loss:0.024, val_acc:0.963]
Epoch [79/120    avg_loss:0.026, val_acc:0.963]
Epoch [80/120    avg_loss:0.021, val_acc:0.962]
Epoch [81/120    avg_loss:0.024, val_acc:0.962]
Epoch [82/120    avg_loss:0.028, val_acc:0.963]
Epoch [83/120    avg_loss:0.028, val_acc:0.964]
Epoch [84/120    avg_loss:0.028, val_acc:0.965]
Epoch [85/120    avg_loss:0.027, val_acc:0.968]
Epoch [86/120    avg_loss:0.025, val_acc:0.967]
Epoch [87/120    avg_loss:0.026, val_acc:0.966]
Epoch [88/120    avg_loss:0.022, val_acc:0.967]
Epoch [89/120    avg_loss:0.029, val_acc:0.968]
Epoch [90/120    avg_loss:0.025, val_acc:0.966]
Epoch [91/120    avg_loss:0.023, val_acc:0.968]
Epoch [92/120    avg_loss:0.024, val_acc:0.966]
Epoch [93/120    avg_loss:0.020, val_acc:0.967]
Epoch [94/120    avg_loss:0.024, val_acc:0.969]
Epoch [95/120    avg_loss:0.023, val_acc:0.965]
Epoch [96/120    avg_loss:0.020, val_acc:0.967]
Epoch [97/120    avg_loss:0.023, val_acc:0.967]
Epoch [98/120    avg_loss:0.020, val_acc:0.967]
Epoch [99/120    avg_loss:0.022, val_acc:0.968]
Epoch [100/120    avg_loss:0.020, val_acc:0.970]
Epoch [101/120    avg_loss:0.029, val_acc:0.969]
Epoch [102/120    avg_loss:0.021, val_acc:0.968]
Epoch [103/120    avg_loss:0.023, val_acc:0.967]
Epoch [104/120    avg_loss:0.017, val_acc:0.966]
Epoch [105/120    avg_loss:0.022, val_acc:0.966]
Epoch [106/120    avg_loss:0.026, val_acc:0.966]
Epoch [107/120    avg_loss:0.021, val_acc:0.965]
Epoch [108/120    avg_loss:0.018, val_acc:0.966]
Epoch [109/120    avg_loss:0.022, val_acc:0.968]
Epoch [110/120    avg_loss:0.020, val_acc:0.967]
Epoch [111/120    avg_loss:0.019, val_acc:0.968]
Epoch [112/120    avg_loss:0.020, val_acc:0.967]
Epoch [113/120    avg_loss:0.020, val_acc:0.968]
Epoch [114/120    avg_loss:0.018, val_acc:0.967]
Epoch [115/120    avg_loss:0.017, val_acc:0.967]
Epoch [116/120    avg_loss:0.020, val_acc:0.969]
Epoch [117/120    avg_loss:0.018, val_acc:0.969]
Epoch [118/120    avg_loss:0.018, val_acc:0.971]
Epoch [119/120    avg_loss:0.019, val_acc:0.971]
Epoch [120/120    avg_loss:0.019, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1246    0    1    0    4    0    0    0    8   24    2    0
     0    0    0]
 [   0    0    0  712    0    0    0    0    0    2    4    7   17    0
     0    5    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  424    0    0    0    6    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    2    1    0    0    0  830   35    0    0
     2    3    0]
 [   0    0    5    0    0    0    1    0    0    0   31 2170    0    2
     1    0    0]
 [   0    0    0    4    0    1    0    0    0    0    4    3  518    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1123   15    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    66  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.0189701897019

F1 scores:
[       nan 0.975      0.98187549 0.97334245 0.99765808 0.99078341
 0.99319728 0.98039216 0.99181287 0.9        0.94640821 0.97550011
 0.96014829 0.99462366 0.96188437 0.85802469 0.97619048]

Kappa:
0.9659854651157723
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f790750a7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.600, val_acc:0.508]
Epoch [2/120    avg_loss:2.211, val_acc:0.555]
Epoch [3/120    avg_loss:1.964, val_acc:0.531]
Epoch [4/120    avg_loss:1.736, val_acc:0.562]
Epoch [5/120    avg_loss:1.556, val_acc:0.611]
Epoch [6/120    avg_loss:1.379, val_acc:0.646]
Epoch [7/120    avg_loss:1.271, val_acc:0.644]
Epoch [8/120    avg_loss:1.123, val_acc:0.676]
Epoch [9/120    avg_loss:0.991, val_acc:0.695]
Epoch [10/120    avg_loss:0.906, val_acc:0.739]
Epoch [11/120    avg_loss:0.769, val_acc:0.805]
Epoch [12/120    avg_loss:0.749, val_acc:0.802]
Epoch [13/120    avg_loss:0.611, val_acc:0.806]
Epoch [14/120    avg_loss:0.569, val_acc:0.755]
Epoch [15/120    avg_loss:0.562, val_acc:0.851]
Epoch [16/120    avg_loss:0.470, val_acc:0.852]
Epoch [17/120    avg_loss:0.447, val_acc:0.870]
Epoch [18/120    avg_loss:0.373, val_acc:0.873]
Epoch [19/120    avg_loss:0.333, val_acc:0.857]
Epoch [20/120    avg_loss:0.396, val_acc:0.879]
Epoch [21/120    avg_loss:0.302, val_acc:0.894]
Epoch [22/120    avg_loss:0.240, val_acc:0.918]
Epoch [23/120    avg_loss:0.211, val_acc:0.905]
Epoch [24/120    avg_loss:0.221, val_acc:0.916]
Epoch [25/120    avg_loss:0.169, val_acc:0.927]
Epoch [26/120    avg_loss:0.160, val_acc:0.919]
Epoch [27/120    avg_loss:0.233, val_acc:0.903]
Epoch [28/120    avg_loss:0.167, val_acc:0.927]
Epoch [29/120    avg_loss:0.146, val_acc:0.932]
Epoch [30/120    avg_loss:0.116, val_acc:0.934]
Epoch [31/120    avg_loss:0.126, val_acc:0.930]
Epoch [32/120    avg_loss:0.112, val_acc:0.941]
Epoch [33/120    avg_loss:0.103, val_acc:0.930]
Epoch [34/120    avg_loss:0.106, val_acc:0.909]
Epoch [35/120    avg_loss:0.137, val_acc:0.933]
Epoch [36/120    avg_loss:0.093, val_acc:0.958]
Epoch [37/120    avg_loss:0.097, val_acc:0.940]
Epoch [38/120    avg_loss:0.085, val_acc:0.951]
Epoch [39/120    avg_loss:0.071, val_acc:0.943]
Epoch [40/120    avg_loss:0.072, val_acc:0.954]
Epoch [41/120    avg_loss:0.093, val_acc:0.926]
Epoch [42/120    avg_loss:0.073, val_acc:0.954]
Epoch [43/120    avg_loss:0.083, val_acc:0.934]
Epoch [44/120    avg_loss:0.086, val_acc:0.957]
Epoch [45/120    avg_loss:0.060, val_acc:0.957]
Epoch [46/120    avg_loss:0.056, val_acc:0.947]
Epoch [47/120    avg_loss:0.062, val_acc:0.959]
Epoch [48/120    avg_loss:0.054, val_acc:0.964]
Epoch [49/120    avg_loss:0.055, val_acc:0.955]
Epoch [50/120    avg_loss:0.073, val_acc:0.940]
Epoch [51/120    avg_loss:0.088, val_acc:0.952]
Epoch [52/120    avg_loss:0.061, val_acc:0.945]
Epoch [53/120    avg_loss:0.082, val_acc:0.952]
Epoch [54/120    avg_loss:0.079, val_acc:0.908]
Epoch [55/120    avg_loss:0.082, val_acc:0.952]
Epoch [56/120    avg_loss:0.053, val_acc:0.957]
Epoch [57/120    avg_loss:0.044, val_acc:0.963]
Epoch [58/120    avg_loss:0.033, val_acc:0.946]
Epoch [59/120    avg_loss:0.042, val_acc:0.955]
Epoch [60/120    avg_loss:0.048, val_acc:0.955]
Epoch [61/120    avg_loss:0.046, val_acc:0.961]
Epoch [62/120    avg_loss:0.032, val_acc:0.963]
Epoch [63/120    avg_loss:0.025, val_acc:0.967]
Epoch [64/120    avg_loss:0.030, val_acc:0.973]
Epoch [65/120    avg_loss:0.025, val_acc:0.970]
Epoch [66/120    avg_loss:0.018, val_acc:0.967]
Epoch [67/120    avg_loss:0.021, val_acc:0.971]
Epoch [68/120    avg_loss:0.026, val_acc:0.972]
Epoch [69/120    avg_loss:0.021, val_acc:0.971]
Epoch [70/120    avg_loss:0.018, val_acc:0.971]
Epoch [71/120    avg_loss:0.025, val_acc:0.971]
Epoch [72/120    avg_loss:0.021, val_acc:0.972]
Epoch [73/120    avg_loss:0.021, val_acc:0.973]
Epoch [74/120    avg_loss:0.016, val_acc:0.970]
Epoch [75/120    avg_loss:0.020, val_acc:0.972]
Epoch [76/120    avg_loss:0.016, val_acc:0.972]
Epoch [77/120    avg_loss:0.017, val_acc:0.973]
Epoch [78/120    avg_loss:0.018, val_acc:0.972]
Epoch [79/120    avg_loss:0.018, val_acc:0.974]
Epoch [80/120    avg_loss:0.020, val_acc:0.973]
Epoch [81/120    avg_loss:0.020, val_acc:0.970]
Epoch [82/120    avg_loss:0.015, val_acc:0.972]
Epoch [83/120    avg_loss:0.019, val_acc:0.973]
Epoch [84/120    avg_loss:0.017, val_acc:0.973]
Epoch [85/120    avg_loss:0.017, val_acc:0.972]
Epoch [86/120    avg_loss:0.017, val_acc:0.975]
Epoch [87/120    avg_loss:0.015, val_acc:0.974]
Epoch [88/120    avg_loss:0.015, val_acc:0.974]
Epoch [89/120    avg_loss:0.017, val_acc:0.972]
Epoch [90/120    avg_loss:0.017, val_acc:0.972]
Epoch [91/120    avg_loss:0.013, val_acc:0.973]
Epoch [92/120    avg_loss:0.015, val_acc:0.973]
Epoch [93/120    avg_loss:0.014, val_acc:0.974]
Epoch [94/120    avg_loss:0.014, val_acc:0.974]
Epoch [95/120    avg_loss:0.013, val_acc:0.974]
Epoch [96/120    avg_loss:0.015, val_acc:0.974]
Epoch [97/120    avg_loss:0.015, val_acc:0.973]
Epoch [98/120    avg_loss:0.020, val_acc:0.970]
Epoch [99/120    avg_loss:0.014, val_acc:0.972]
Epoch [100/120    avg_loss:0.015, val_acc:0.972]
Epoch [101/120    avg_loss:0.014, val_acc:0.972]
Epoch [102/120    avg_loss:0.014, val_acc:0.972]
Epoch [103/120    avg_loss:0.015, val_acc:0.972]
Epoch [104/120    avg_loss:0.014, val_acc:0.972]
Epoch [105/120    avg_loss:0.014, val_acc:0.972]
Epoch [106/120    avg_loss:0.016, val_acc:0.972]
Epoch [107/120    avg_loss:0.018, val_acc:0.972]
Epoch [108/120    avg_loss:0.016, val_acc:0.972]
Epoch [109/120    avg_loss:0.014, val_acc:0.972]
Epoch [110/120    avg_loss:0.015, val_acc:0.972]
Epoch [111/120    avg_loss:0.016, val_acc:0.972]
Epoch [112/120    avg_loss:0.013, val_acc:0.972]
Epoch [113/120    avg_loss:0.014, val_acc:0.972]
Epoch [114/120    avg_loss:0.018, val_acc:0.972]
Epoch [115/120    avg_loss:0.013, val_acc:0.972]
Epoch [116/120    avg_loss:0.015, val_acc:0.972]
Epoch [117/120    avg_loss:0.014, val_acc:0.972]
Epoch [118/120    avg_loss:0.014, val_acc:0.972]
Epoch [119/120    avg_loss:0.016, val_acc:0.972]
Epoch [120/120    avg_loss:0.015, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1263    1    0    0    0    0    0    3    2   16    0    0
     0    0    0]
 [   0    0    0  733    2    0    0    0    0    6    2    0    3    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    5    0    1    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    4    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    2    0    0    0    1  851   14    0    0
     0    4    0]
 [   0    0   11    0    0    0    0    0    0    1   15 2169    5    0
     9    0    0]
 [   0    0    0    0    1    0    0    0    0    0    0    0  529    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    0    2    0    0    2    0    0    0    0
    43  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.09214092140921

F1 scores:
[       nan 1.         0.98594848 0.98987171 0.99300699 0.98842593
 0.99466056 0.90909091 1.         0.72       0.97535817 0.98278206
 0.98786181 0.99730458 0.97243755 0.90497738 0.98823529]

Kappa:
0.97824863839973
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb3fc7d2828>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.594, val_acc:0.421]
Epoch [2/120    avg_loss:2.178, val_acc:0.471]
Epoch [3/120    avg_loss:1.969, val_acc:0.560]
Epoch [4/120    avg_loss:1.781, val_acc:0.608]
Epoch [5/120    avg_loss:1.581, val_acc:0.641]
Epoch [6/120    avg_loss:1.449, val_acc:0.684]
Epoch [7/120    avg_loss:1.307, val_acc:0.713]
Epoch [8/120    avg_loss:1.127, val_acc:0.666]
Epoch [9/120    avg_loss:0.985, val_acc:0.771]
Epoch [10/120    avg_loss:0.877, val_acc:0.745]
Epoch [11/120    avg_loss:0.798, val_acc:0.731]
Epoch [12/120    avg_loss:0.685, val_acc:0.759]
Epoch [13/120    avg_loss:0.640, val_acc:0.808]
Epoch [14/120    avg_loss:0.557, val_acc:0.815]
Epoch [15/120    avg_loss:0.505, val_acc:0.823]
Epoch [16/120    avg_loss:0.442, val_acc:0.824]
Epoch [17/120    avg_loss:0.452, val_acc:0.852]
Epoch [18/120    avg_loss:0.393, val_acc:0.817]
Epoch [19/120    avg_loss:0.368, val_acc:0.853]
Epoch [20/120    avg_loss:0.272, val_acc:0.839]
Epoch [21/120    avg_loss:0.279, val_acc:0.858]
Epoch [22/120    avg_loss:0.335, val_acc:0.799]
Epoch [23/120    avg_loss:0.280, val_acc:0.893]
Epoch [24/120    avg_loss:0.223, val_acc:0.881]
Epoch [25/120    avg_loss:0.229, val_acc:0.895]
Epoch [26/120    avg_loss:0.205, val_acc:0.885]
Epoch [27/120    avg_loss:0.214, val_acc:0.908]
Epoch [28/120    avg_loss:0.160, val_acc:0.891]
Epoch [29/120    avg_loss:0.153, val_acc:0.914]
Epoch [30/120    avg_loss:0.144, val_acc:0.916]
Epoch [31/120    avg_loss:0.144, val_acc:0.938]
Epoch [32/120    avg_loss:0.231, val_acc:0.908]
Epoch [33/120    avg_loss:0.157, val_acc:0.923]
Epoch [34/120    avg_loss:0.124, val_acc:0.935]
Epoch [35/120    avg_loss:0.112, val_acc:0.922]
Epoch [36/120    avg_loss:0.102, val_acc:0.928]
Epoch [37/120    avg_loss:0.124, val_acc:0.930]
Epoch [38/120    avg_loss:0.104, val_acc:0.927]
Epoch [39/120    avg_loss:0.113, val_acc:0.951]
Epoch [40/120    avg_loss:0.109, val_acc:0.960]
Epoch [41/120    avg_loss:0.089, val_acc:0.926]
Epoch [42/120    avg_loss:0.073, val_acc:0.932]
Epoch [43/120    avg_loss:0.058, val_acc:0.946]
Epoch [44/120    avg_loss:0.062, val_acc:0.950]
Epoch [45/120    avg_loss:0.053, val_acc:0.957]
Epoch [46/120    avg_loss:0.050, val_acc:0.958]
Epoch [47/120    avg_loss:0.045, val_acc:0.959]
Epoch [48/120    avg_loss:0.053, val_acc:0.958]
Epoch [49/120    avg_loss:0.044, val_acc:0.951]
Epoch [50/120    avg_loss:0.051, val_acc:0.951]
Epoch [51/120    avg_loss:0.077, val_acc:0.925]
Epoch [52/120    avg_loss:0.152, val_acc:0.924]
Epoch [53/120    avg_loss:0.081, val_acc:0.949]
Epoch [54/120    avg_loss:0.057, val_acc:0.957]
Epoch [55/120    avg_loss:0.036, val_acc:0.965]
Epoch [56/120    avg_loss:0.041, val_acc:0.963]
Epoch [57/120    avg_loss:0.045, val_acc:0.963]
Epoch [58/120    avg_loss:0.037, val_acc:0.966]
Epoch [59/120    avg_loss:0.027, val_acc:0.965]
Epoch [60/120    avg_loss:0.036, val_acc:0.967]
Epoch [61/120    avg_loss:0.030, val_acc:0.964]
Epoch [62/120    avg_loss:0.033, val_acc:0.966]
Epoch [63/120    avg_loss:0.026, val_acc:0.964]
Epoch [64/120    avg_loss:0.035, val_acc:0.968]
Epoch [65/120    avg_loss:0.033, val_acc:0.967]
Epoch [66/120    avg_loss:0.031, val_acc:0.965]
Epoch [67/120    avg_loss:0.028, val_acc:0.967]
Epoch [68/120    avg_loss:0.026, val_acc:0.967]
Epoch [69/120    avg_loss:0.024, val_acc:0.969]
Epoch [70/120    avg_loss:0.028, val_acc:0.967]
Epoch [71/120    avg_loss:0.029, val_acc:0.966]
Epoch [72/120    avg_loss:0.029, val_acc:0.969]
Epoch [73/120    avg_loss:0.027, val_acc:0.968]
Epoch [74/120    avg_loss:0.032, val_acc:0.967]
Epoch [75/120    avg_loss:0.038, val_acc:0.969]
Epoch [76/120    avg_loss:0.028, val_acc:0.970]
Epoch [77/120    avg_loss:0.029, val_acc:0.968]
Epoch [78/120    avg_loss:0.024, val_acc:0.964]
Epoch [79/120    avg_loss:0.023, val_acc:0.970]
Epoch [80/120    avg_loss:0.022, val_acc:0.971]
Epoch [81/120    avg_loss:0.023, val_acc:0.971]
Epoch [82/120    avg_loss:0.025, val_acc:0.970]
Epoch [83/120    avg_loss:0.025, val_acc:0.968]
Epoch [84/120    avg_loss:0.028, val_acc:0.971]
Epoch [85/120    avg_loss:0.025, val_acc:0.969]
Epoch [86/120    avg_loss:0.029, val_acc:0.970]
Epoch [87/120    avg_loss:0.026, val_acc:0.974]
Epoch [88/120    avg_loss:0.022, val_acc:0.967]
Epoch [89/120    avg_loss:0.025, val_acc:0.967]
Epoch [90/120    avg_loss:0.022, val_acc:0.973]
Epoch [91/120    avg_loss:0.020, val_acc:0.971]
Epoch [92/120    avg_loss:0.026, val_acc:0.969]
Epoch [93/120    avg_loss:0.026, val_acc:0.971]
Epoch [94/120    avg_loss:0.023, val_acc:0.971]
Epoch [95/120    avg_loss:0.021, val_acc:0.965]
Epoch [96/120    avg_loss:0.022, val_acc:0.970]
Epoch [97/120    avg_loss:0.019, val_acc:0.972]
Epoch [98/120    avg_loss:0.019, val_acc:0.972]
Epoch [99/120    avg_loss:0.022, val_acc:0.977]
Epoch [100/120    avg_loss:0.020, val_acc:0.971]
Epoch [101/120    avg_loss:0.022, val_acc:0.973]
Epoch [102/120    avg_loss:0.021, val_acc:0.974]
Epoch [103/120    avg_loss:0.022, val_acc:0.970]
Epoch [104/120    avg_loss:0.023, val_acc:0.970]
Epoch [105/120    avg_loss:0.023, val_acc:0.970]
Epoch [106/120    avg_loss:0.021, val_acc:0.969]
Epoch [107/120    avg_loss:0.026, val_acc:0.975]
Epoch [108/120    avg_loss:0.024, val_acc:0.976]
Epoch [109/120    avg_loss:0.018, val_acc:0.977]
Epoch [110/120    avg_loss:0.023, val_acc:0.971]
Epoch [111/120    avg_loss:0.018, val_acc:0.973]
Epoch [112/120    avg_loss:0.021, val_acc:0.974]
Epoch [113/120    avg_loss:0.024, val_acc:0.976]
Epoch [114/120    avg_loss:0.019, val_acc:0.972]
Epoch [115/120    avg_loss:0.018, val_acc:0.976]
Epoch [116/120    avg_loss:0.019, val_acc:0.974]
Epoch [117/120    avg_loss:0.023, val_acc:0.974]
Epoch [118/120    avg_loss:0.019, val_acc:0.976]
Epoch [119/120    avg_loss:0.025, val_acc:0.978]
Epoch [120/120    avg_loss:0.022, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1266    0    6    0    0    0    0    1    5    7    0    0
     0    0    0]
 [   0    0    0  730    0    0    0    0    0    5    1    0    9    2
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  428    0    5    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    5    0    0    2    0    0    0    1  839   24    2    0
     0    2    0]
 [   0    0   17    0    0    1    0    0    0    0   34 2154    0    1
     0    3    0]
 [   0    0    2    6    0    2    0    0    0    0    3    2  515    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1132    6    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    49  289    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.50677506775068

F1 scores:
[       nan 0.93975904 0.98330097 0.9844909  0.9837587  0.98617512
 0.99167298 0.90909091 0.99649942 0.80952381 0.95340909 0.97953615
 0.96804511 0.9919571  0.97460181 0.89060092 0.97619048]

Kappa:
0.9715764285421767
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7e87340860>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.580, val_acc:0.325]
Epoch [2/120    avg_loss:2.170, val_acc:0.526]
Epoch [3/120    avg_loss:1.963, val_acc:0.571]
Epoch [4/120    avg_loss:1.748, val_acc:0.583]
Epoch [5/120    avg_loss:1.563, val_acc:0.568]
Epoch [6/120    avg_loss:1.468, val_acc:0.591]
Epoch [7/120    avg_loss:1.230, val_acc:0.642]
Epoch [8/120    avg_loss:1.063, val_acc:0.677]
Epoch [9/120    avg_loss:0.920, val_acc:0.696]
Epoch [10/120    avg_loss:0.828, val_acc:0.691]
Epoch [11/120    avg_loss:0.810, val_acc:0.712]
Epoch [12/120    avg_loss:0.699, val_acc:0.781]
Epoch [13/120    avg_loss:0.664, val_acc:0.772]
Epoch [14/120    avg_loss:0.524, val_acc:0.772]
Epoch [15/120    avg_loss:0.478, val_acc:0.794]
Epoch [16/120    avg_loss:0.484, val_acc:0.827]
Epoch [17/120    avg_loss:0.411, val_acc:0.842]
Epoch [18/120    avg_loss:0.671, val_acc:0.732]
Epoch [19/120    avg_loss:0.802, val_acc:0.765]
Epoch [20/120    avg_loss:0.605, val_acc:0.725]
Epoch [21/120    avg_loss:0.407, val_acc:0.816]
Epoch [22/120    avg_loss:0.394, val_acc:0.841]
Epoch [23/120    avg_loss:0.321, val_acc:0.848]
Epoch [24/120    avg_loss:0.275, val_acc:0.849]
Epoch [25/120    avg_loss:0.335, val_acc:0.812]
Epoch [26/120    avg_loss:0.279, val_acc:0.869]
Epoch [27/120    avg_loss:0.200, val_acc:0.868]
Epoch [28/120    avg_loss:0.198, val_acc:0.901]
Epoch [29/120    avg_loss:0.158, val_acc:0.893]
Epoch [30/120    avg_loss:0.176, val_acc:0.854]
Epoch [31/120    avg_loss:0.196, val_acc:0.895]
Epoch [32/120    avg_loss:0.230, val_acc:0.889]
Epoch [33/120    avg_loss:0.223, val_acc:0.903]
Epoch [34/120    avg_loss:0.147, val_acc:0.904]
Epoch [35/120    avg_loss:0.144, val_acc:0.911]
Epoch [36/120    avg_loss:0.124, val_acc:0.901]
Epoch [37/120    avg_loss:0.124, val_acc:0.889]
Epoch [38/120    avg_loss:0.127, val_acc:0.903]
Epoch [39/120    avg_loss:0.107, val_acc:0.900]
Epoch [40/120    avg_loss:0.113, val_acc:0.904]
Epoch [41/120    avg_loss:0.108, val_acc:0.916]
Epoch [42/120    avg_loss:0.109, val_acc:0.925]
Epoch [43/120    avg_loss:0.098, val_acc:0.931]
Epoch [44/120    avg_loss:0.081, val_acc:0.941]
Epoch [45/120    avg_loss:0.067, val_acc:0.936]
Epoch [46/120    avg_loss:0.067, val_acc:0.936]
Epoch [47/120    avg_loss:0.059, val_acc:0.940]
Epoch [48/120    avg_loss:0.064, val_acc:0.941]
Epoch [49/120    avg_loss:0.072, val_acc:0.932]
Epoch [50/120    avg_loss:0.057, val_acc:0.946]
Epoch [51/120    avg_loss:0.061, val_acc:0.941]
Epoch [52/120    avg_loss:0.073, val_acc:0.939]
Epoch [53/120    avg_loss:0.063, val_acc:0.950]
Epoch [54/120    avg_loss:0.058, val_acc:0.946]
Epoch [55/120    avg_loss:0.060, val_acc:0.955]
Epoch [56/120    avg_loss:0.053, val_acc:0.953]
Epoch [57/120    avg_loss:0.045, val_acc:0.956]
Epoch [58/120    avg_loss:0.046, val_acc:0.959]
Epoch [59/120    avg_loss:0.040, val_acc:0.956]
Epoch [60/120    avg_loss:0.047, val_acc:0.938]
Epoch [61/120    avg_loss:0.046, val_acc:0.948]
Epoch [62/120    avg_loss:0.044, val_acc:0.948]
Epoch [63/120    avg_loss:0.044, val_acc:0.959]
Epoch [64/120    avg_loss:0.046, val_acc:0.957]
Epoch [65/120    avg_loss:0.026, val_acc:0.962]
Epoch [66/120    avg_loss:0.036, val_acc:0.947]
Epoch [67/120    avg_loss:0.052, val_acc:0.940]
Epoch [68/120    avg_loss:0.055, val_acc:0.944]
Epoch [69/120    avg_loss:0.038, val_acc:0.958]
Epoch [70/120    avg_loss:0.129, val_acc:0.929]
Epoch [71/120    avg_loss:0.067, val_acc:0.948]
Epoch [72/120    avg_loss:0.056, val_acc:0.949]
Epoch [73/120    avg_loss:0.036, val_acc:0.938]
Epoch [74/120    avg_loss:0.037, val_acc:0.957]
Epoch [75/120    avg_loss:0.034, val_acc:0.947]
Epoch [76/120    avg_loss:0.027, val_acc:0.955]
Epoch [77/120    avg_loss:0.028, val_acc:0.965]
Epoch [78/120    avg_loss:0.021, val_acc:0.970]
Epoch [79/120    avg_loss:0.026, val_acc:0.966]
Epoch [80/120    avg_loss:0.031, val_acc:0.951]
Epoch [81/120    avg_loss:0.033, val_acc:0.967]
Epoch [82/120    avg_loss:0.020, val_acc:0.971]
Epoch [83/120    avg_loss:0.016, val_acc:0.971]
Epoch [84/120    avg_loss:0.027, val_acc:0.967]
Epoch [85/120    avg_loss:0.018, val_acc:0.969]
Epoch [86/120    avg_loss:0.015, val_acc:0.966]
Epoch [87/120    avg_loss:0.020, val_acc:0.971]
Epoch [88/120    avg_loss:0.017, val_acc:0.973]
Epoch [89/120    avg_loss:0.015, val_acc:0.978]
Epoch [90/120    avg_loss:0.019, val_acc:0.959]
Epoch [91/120    avg_loss:0.020, val_acc:0.968]
Epoch [92/120    avg_loss:0.020, val_acc:0.965]
Epoch [93/120    avg_loss:0.019, val_acc:0.970]
Epoch [94/120    avg_loss:0.020, val_acc:0.965]
Epoch [95/120    avg_loss:0.015, val_acc:0.971]
Epoch [96/120    avg_loss:0.014, val_acc:0.974]
Epoch [97/120    avg_loss:0.014, val_acc:0.972]
Epoch [98/120    avg_loss:0.012, val_acc:0.971]
Epoch [99/120    avg_loss:0.010, val_acc:0.975]
Epoch [100/120    avg_loss:0.013, val_acc:0.971]
Epoch [101/120    avg_loss:0.011, val_acc:0.975]
Epoch [102/120    avg_loss:0.012, val_acc:0.971]
Epoch [103/120    avg_loss:0.009, val_acc:0.973]
Epoch [104/120    avg_loss:0.012, val_acc:0.979]
Epoch [105/120    avg_loss:0.007, val_acc:0.976]
Epoch [106/120    avg_loss:0.008, val_acc:0.975]
Epoch [107/120    avg_loss:0.008, val_acc:0.975]
Epoch [108/120    avg_loss:0.005, val_acc:0.975]
Epoch [109/120    avg_loss:0.006, val_acc:0.975]
Epoch [110/120    avg_loss:0.005, val_acc:0.975]
Epoch [111/120    avg_loss:0.009, val_acc:0.979]
Epoch [112/120    avg_loss:0.007, val_acc:0.976]
Epoch [113/120    avg_loss:0.007, val_acc:0.978]
Epoch [114/120    avg_loss:0.007, val_acc:0.976]
Epoch [115/120    avg_loss:0.006, val_acc:0.975]
Epoch [116/120    avg_loss:0.008, val_acc:0.978]
Epoch [117/120    avg_loss:0.005, val_acc:0.979]
Epoch [118/120    avg_loss:0.006, val_acc:0.979]
Epoch [119/120    avg_loss:0.008, val_acc:0.980]
Epoch [120/120    avg_loss:0.008, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1254    1    1    3    1    0    0    1    7   16    1    0
     0    0    0]
 [   0    0    0  729    2    0    0    0    0    6    1    3    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  862   12    0    0
     0    1    0]
 [   0    0    9    0    0    0    0    0    0    1   17 2180    1    2
     0    0    0]
 [   0    0    0    1    0    2    0    0    0    0    0    4  525    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    1    0    0    0    0    0    0
  1130    1    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    55  280    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
97.97289972899729

F1 scores:
[       nan 1.         0.98430141 0.9864682  0.99300699 0.98639456
 0.98715042 0.98039216 1.         0.81818182 0.9784336  0.98464318
 0.97856477 0.99462366 0.97204301 0.89030207 0.95121951]

Kappa:
0.9768788145365486
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4de242b908>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.633, val_acc:0.329]
Epoch [2/120    avg_loss:2.214, val_acc:0.401]
Epoch [3/120    avg_loss:1.990, val_acc:0.526]
Epoch [4/120    avg_loss:1.831, val_acc:0.578]
Epoch [5/120    avg_loss:1.655, val_acc:0.619]
Epoch [6/120    avg_loss:1.500, val_acc:0.658]
Epoch [7/120    avg_loss:1.366, val_acc:0.641]
Epoch [8/120    avg_loss:1.209, val_acc:0.705]
Epoch [9/120    avg_loss:1.058, val_acc:0.726]
Epoch [10/120    avg_loss:0.934, val_acc:0.729]
Epoch [11/120    avg_loss:0.863, val_acc:0.748]
Epoch [12/120    avg_loss:0.690, val_acc:0.795]
Epoch [13/120    avg_loss:0.681, val_acc:0.771]
Epoch [14/120    avg_loss:0.600, val_acc:0.814]
Epoch [15/120    avg_loss:0.557, val_acc:0.804]
Epoch [16/120    avg_loss:0.642, val_acc:0.807]
Epoch [17/120    avg_loss:0.524, val_acc:0.840]
Epoch [18/120    avg_loss:0.402, val_acc:0.840]
Epoch [19/120    avg_loss:0.322, val_acc:0.874]
Epoch [20/120    avg_loss:0.271, val_acc:0.828]
Epoch [21/120    avg_loss:0.279, val_acc:0.884]
Epoch [22/120    avg_loss:0.258, val_acc:0.874]
Epoch [23/120    avg_loss:0.224, val_acc:0.902]
Epoch [24/120    avg_loss:0.179, val_acc:0.905]
Epoch [25/120    avg_loss:0.140, val_acc:0.903]
Epoch [26/120    avg_loss:0.269, val_acc:0.860]
Epoch [27/120    avg_loss:0.319, val_acc:0.910]
Epoch [28/120    avg_loss:0.169, val_acc:0.894]
Epoch [29/120    avg_loss:0.201, val_acc:0.869]
Epoch [30/120    avg_loss:0.241, val_acc:0.904]
Epoch [31/120    avg_loss:0.173, val_acc:0.923]
Epoch [32/120    avg_loss:0.128, val_acc:0.929]
Epoch [33/120    avg_loss:0.121, val_acc:0.936]
Epoch [34/120    avg_loss:0.102, val_acc:0.940]
Epoch [35/120    avg_loss:0.105, val_acc:0.896]
Epoch [36/120    avg_loss:0.086, val_acc:0.928]
Epoch [37/120    avg_loss:0.077, val_acc:0.948]
Epoch [38/120    avg_loss:0.072, val_acc:0.947]
Epoch [39/120    avg_loss:0.063, val_acc:0.957]
Epoch [40/120    avg_loss:0.052, val_acc:0.956]
Epoch [41/120    avg_loss:0.086, val_acc:0.931]
Epoch [42/120    avg_loss:0.075, val_acc:0.964]
Epoch [43/120    avg_loss:0.081, val_acc:0.936]
Epoch [44/120    avg_loss:0.063, val_acc:0.958]
Epoch [45/120    avg_loss:0.064, val_acc:0.959]
Epoch [46/120    avg_loss:0.047, val_acc:0.962]
Epoch [47/120    avg_loss:0.040, val_acc:0.967]
Epoch [48/120    avg_loss:0.053, val_acc:0.965]
Epoch [49/120    avg_loss:0.052, val_acc:0.946]
Epoch [50/120    avg_loss:0.043, val_acc:0.948]
Epoch [51/120    avg_loss:0.040, val_acc:0.962]
Epoch [52/120    avg_loss:0.037, val_acc:0.957]
Epoch [53/120    avg_loss:0.052, val_acc:0.942]
Epoch [54/120    avg_loss:0.046, val_acc:0.956]
Epoch [55/120    avg_loss:0.044, val_acc:0.963]
Epoch [56/120    avg_loss:0.041, val_acc:0.964]
Epoch [57/120    avg_loss:0.031, val_acc:0.965]
Epoch [58/120    avg_loss:0.029, val_acc:0.967]
Epoch [59/120    avg_loss:0.042, val_acc:0.969]
Epoch [60/120    avg_loss:0.022, val_acc:0.974]
Epoch [61/120    avg_loss:0.034, val_acc:0.966]
Epoch [62/120    avg_loss:0.025, val_acc:0.971]
Epoch [63/120    avg_loss:0.024, val_acc:0.970]
Epoch [64/120    avg_loss:0.027, val_acc:0.973]
Epoch [65/120    avg_loss:0.041, val_acc:0.942]
Epoch [66/120    avg_loss:0.033, val_acc:0.973]
Epoch [67/120    avg_loss:0.022, val_acc:0.971]
Epoch [68/120    avg_loss:0.028, val_acc:0.975]
Epoch [69/120    avg_loss:0.024, val_acc:0.975]
Epoch [70/120    avg_loss:0.022, val_acc:0.971]
Epoch [71/120    avg_loss:0.279, val_acc:0.910]
Epoch [72/120    avg_loss:0.130, val_acc:0.947]
Epoch [73/120    avg_loss:0.072, val_acc:0.956]
Epoch [74/120    avg_loss:0.064, val_acc:0.955]
Epoch [75/120    avg_loss:0.060, val_acc:0.964]
Epoch [76/120    avg_loss:0.043, val_acc:0.962]
Epoch [77/120    avg_loss:0.036, val_acc:0.959]
Epoch [78/120    avg_loss:0.046, val_acc:0.947]
Epoch [79/120    avg_loss:0.042, val_acc:0.968]
Epoch [80/120    avg_loss:0.031, val_acc:0.963]
Epoch [81/120    avg_loss:0.023, val_acc:0.969]
Epoch [82/120    avg_loss:0.023, val_acc:0.976]
Epoch [83/120    avg_loss:0.018, val_acc:0.972]
Epoch [84/120    avg_loss:0.027, val_acc:0.973]
Epoch [85/120    avg_loss:0.018, val_acc:0.975]
Epoch [86/120    avg_loss:0.014, val_acc:0.979]
Epoch [87/120    avg_loss:0.026, val_acc:0.967]
Epoch [88/120    avg_loss:0.015, val_acc:0.980]
Epoch [89/120    avg_loss:0.022, val_acc:0.975]
Epoch [90/120    avg_loss:0.022, val_acc:0.972]
Epoch [91/120    avg_loss:0.013, val_acc:0.982]
Epoch [92/120    avg_loss:0.011, val_acc:0.981]
Epoch [93/120    avg_loss:0.014, val_acc:0.979]
Epoch [94/120    avg_loss:0.015, val_acc:0.974]
Epoch [95/120    avg_loss:0.018, val_acc:0.974]
Epoch [96/120    avg_loss:0.015, val_acc:0.981]
Epoch [97/120    avg_loss:0.009, val_acc:0.984]
Epoch [98/120    avg_loss:0.015, val_acc:0.971]
Epoch [99/120    avg_loss:0.009, val_acc:0.981]
Epoch [100/120    avg_loss:0.013, val_acc:0.979]
Epoch [101/120    avg_loss:0.008, val_acc:0.987]
Epoch [102/120    avg_loss:0.008, val_acc:0.988]
Epoch [103/120    avg_loss:0.023, val_acc:0.972]
Epoch [104/120    avg_loss:0.373, val_acc:0.756]
Epoch [105/120    avg_loss:0.460, val_acc:0.893]
Epoch [106/120    avg_loss:0.149, val_acc:0.940]
Epoch [107/120    avg_loss:0.070, val_acc:0.955]
Epoch [108/120    avg_loss:0.059, val_acc:0.958]
Epoch [109/120    avg_loss:0.053, val_acc:0.969]
Epoch [110/120    avg_loss:0.046, val_acc:0.955]
Epoch [111/120    avg_loss:0.055, val_acc:0.958]
Epoch [112/120    avg_loss:0.032, val_acc:0.966]
Epoch [113/120    avg_loss:0.030, val_acc:0.975]
Epoch [114/120    avg_loss:0.027, val_acc:0.970]
Epoch [115/120    avg_loss:0.023, val_acc:0.976]
Epoch [116/120    avg_loss:0.016, val_acc:0.978]
Epoch [117/120    avg_loss:0.021, val_acc:0.981]
Epoch [118/120    avg_loss:0.018, val_acc:0.979]
Epoch [119/120    avg_loss:0.017, val_acc:0.981]
Epoch [120/120    avg_loss:0.012, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1262    2    2    0    1    0    0    0    4   12    2    0
     0    0    0]
 [   0    0    0  724    0    0    1    0    0    7    3    3    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    4    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    0    0
     0    1    0]
 [   0    0    3    0    0    1    0    0    0    0  854   15    0    0
     0    2    0]
 [   0    0   12    0    0    0    0    0    0    3   22 2158   11    1
     3    0    0]
 [   0    0    0    1    2    0    0    0    0    0    0    3  524    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1138    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    25  322    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.17886178861788

F1 scores:
[       nan 0.975      0.98516784 0.98236092 0.99069767 0.99307159
 0.99695586 0.92592593 1.         0.73913043 0.97045455 0.98068621
 0.96768236 0.99730458 0.98656264 0.95407407 0.97005988]

Kappa:
0.9792442115383141
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5e7a6757f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.664, val_acc:0.397]
Epoch [2/120    avg_loss:2.228, val_acc:0.526]
Epoch [3/120    avg_loss:1.945, val_acc:0.571]
Epoch [4/120    avg_loss:1.762, val_acc:0.590]
Epoch [5/120    avg_loss:1.595, val_acc:0.606]
Epoch [6/120    avg_loss:1.449, val_acc:0.654]
Epoch [7/120    avg_loss:1.244, val_acc:0.686]
Epoch [8/120    avg_loss:1.108, val_acc:0.708]
Epoch [9/120    avg_loss:1.011, val_acc:0.726]
Epoch [10/120    avg_loss:0.883, val_acc:0.733]
Epoch [11/120    avg_loss:0.748, val_acc:0.734]
Epoch [12/120    avg_loss:0.757, val_acc:0.765]
Epoch [13/120    avg_loss:0.613, val_acc:0.776]
Epoch [14/120    avg_loss:0.506, val_acc:0.792]
Epoch [15/120    avg_loss:0.498, val_acc:0.807]
Epoch [16/120    avg_loss:0.413, val_acc:0.830]
Epoch [17/120    avg_loss:0.453, val_acc:0.823]
Epoch [18/120    avg_loss:0.482, val_acc:0.833]
Epoch [19/120    avg_loss:0.373, val_acc:0.850]
Epoch [20/120    avg_loss:0.313, val_acc:0.873]
Epoch [21/120    avg_loss:0.283, val_acc:0.863]
Epoch [22/120    avg_loss:0.244, val_acc:0.859]
Epoch [23/120    avg_loss:0.245, val_acc:0.880]
Epoch [24/120    avg_loss:0.198, val_acc:0.894]
Epoch [25/120    avg_loss:0.279, val_acc:0.874]
Epoch [26/120    avg_loss:0.217, val_acc:0.895]
Epoch [27/120    avg_loss:0.201, val_acc:0.910]
Epoch [28/120    avg_loss:0.180, val_acc:0.905]
Epoch [29/120    avg_loss:0.196, val_acc:0.861]
Epoch [30/120    avg_loss:0.170, val_acc:0.899]
Epoch [31/120    avg_loss:0.159, val_acc:0.915]
Epoch [32/120    avg_loss:0.144, val_acc:0.895]
Epoch [33/120    avg_loss:0.148, val_acc:0.885]
Epoch [34/120    avg_loss:0.138, val_acc:0.903]
Epoch [35/120    avg_loss:0.148, val_acc:0.864]
Epoch [36/120    avg_loss:0.169, val_acc:0.928]
Epoch [37/120    avg_loss:0.120, val_acc:0.935]
Epoch [38/120    avg_loss:0.108, val_acc:0.904]
Epoch [39/120    avg_loss:0.192, val_acc:0.885]
Epoch [40/120    avg_loss:0.315, val_acc:0.891]
Epoch [41/120    avg_loss:0.152, val_acc:0.935]
Epoch [42/120    avg_loss:0.096, val_acc:0.940]
Epoch [43/120    avg_loss:0.126, val_acc:0.923]
Epoch [44/120    avg_loss:0.106, val_acc:0.943]
Epoch [45/120    avg_loss:0.068, val_acc:0.946]
Epoch [46/120    avg_loss:0.051, val_acc:0.952]
Epoch [47/120    avg_loss:0.057, val_acc:0.918]
Epoch [48/120    avg_loss:0.073, val_acc:0.946]
Epoch [49/120    avg_loss:0.066, val_acc:0.948]
Epoch [50/120    avg_loss:0.090, val_acc:0.934]
Epoch [51/120    avg_loss:0.084, val_acc:0.939]
Epoch [52/120    avg_loss:0.090, val_acc:0.957]
Epoch [53/120    avg_loss:0.062, val_acc:0.940]
Epoch [54/120    avg_loss:0.072, val_acc:0.957]
Epoch [55/120    avg_loss:0.076, val_acc:0.938]
Epoch [56/120    avg_loss:0.052, val_acc:0.959]
Epoch [57/120    avg_loss:0.050, val_acc:0.948]
Epoch [58/120    avg_loss:0.038, val_acc:0.966]
Epoch [59/120    avg_loss:0.036, val_acc:0.962]
Epoch [60/120    avg_loss:0.037, val_acc:0.960]
Epoch [61/120    avg_loss:0.043, val_acc:0.964]
Epoch [62/120    avg_loss:0.041, val_acc:0.965]
Epoch [63/120    avg_loss:0.037, val_acc:0.968]
Epoch [64/120    avg_loss:0.077, val_acc:0.926]
Epoch [65/120    avg_loss:0.085, val_acc:0.958]
Epoch [66/120    avg_loss:0.045, val_acc:0.958]
Epoch [67/120    avg_loss:0.041, val_acc:0.958]
Epoch [68/120    avg_loss:0.033, val_acc:0.965]
Epoch [69/120    avg_loss:0.038, val_acc:0.940]
Epoch [70/120    avg_loss:0.028, val_acc:0.965]
Epoch [71/120    avg_loss:0.025, val_acc:0.972]
Epoch [72/120    avg_loss:0.028, val_acc:0.969]
Epoch [73/120    avg_loss:0.019, val_acc:0.973]
Epoch [74/120    avg_loss:0.020, val_acc:0.971]
Epoch [75/120    avg_loss:0.023, val_acc:0.978]
Epoch [76/120    avg_loss:0.025, val_acc:0.953]
Epoch [77/120    avg_loss:0.023, val_acc:0.963]
Epoch [78/120    avg_loss:0.028, val_acc:0.973]
Epoch [79/120    avg_loss:0.027, val_acc:0.975]
Epoch [80/120    avg_loss:0.022, val_acc:0.963]
Epoch [81/120    avg_loss:0.018, val_acc:0.971]
Epoch [82/120    avg_loss:0.022, val_acc:0.972]
Epoch [83/120    avg_loss:0.016, val_acc:0.981]
Epoch [84/120    avg_loss:0.012, val_acc:0.972]
Epoch [85/120    avg_loss:0.022, val_acc:0.976]
Epoch [86/120    avg_loss:0.032, val_acc:0.973]
Epoch [87/120    avg_loss:0.021, val_acc:0.974]
Epoch [88/120    avg_loss:0.016, val_acc:0.970]
Epoch [89/120    avg_loss:0.017, val_acc:0.965]
Epoch [90/120    avg_loss:0.013, val_acc:0.971]
Epoch [91/120    avg_loss:0.014, val_acc:0.972]
Epoch [92/120    avg_loss:0.012, val_acc:0.982]
Epoch [93/120    avg_loss:0.012, val_acc:0.979]
Epoch [94/120    avg_loss:0.011, val_acc:0.984]
Epoch [95/120    avg_loss:0.011, val_acc:0.980]
Epoch [96/120    avg_loss:0.011, val_acc:0.983]
Epoch [97/120    avg_loss:0.008, val_acc:0.981]
Epoch [98/120    avg_loss:0.012, val_acc:0.985]
Epoch [99/120    avg_loss:0.009, val_acc:0.979]
Epoch [100/120    avg_loss:0.009, val_acc:0.974]
Epoch [101/120    avg_loss:0.018, val_acc:0.976]
Epoch [102/120    avg_loss:0.014, val_acc:0.982]
Epoch [103/120    avg_loss:0.010, val_acc:0.973]
Epoch [104/120    avg_loss:0.010, val_acc:0.985]
Epoch [105/120    avg_loss:0.008, val_acc:0.983]
Epoch [106/120    avg_loss:0.007, val_acc:0.982]
Epoch [107/120    avg_loss:0.009, val_acc:0.976]
Epoch [108/120    avg_loss:0.009, val_acc:0.970]
Epoch [109/120    avg_loss:0.013, val_acc:0.976]
Epoch [110/120    avg_loss:0.011, val_acc:0.976]
Epoch [111/120    avg_loss:0.013, val_acc:0.979]
Epoch [112/120    avg_loss:0.009, val_acc:0.964]
Epoch [113/120    avg_loss:0.015, val_acc:0.979]
Epoch [114/120    avg_loss:0.013, val_acc:0.979]
Epoch [115/120    avg_loss:0.009, val_acc:0.978]
Epoch [116/120    avg_loss:0.008, val_acc:0.980]
Epoch [117/120    avg_loss:0.006, val_acc:0.979]
Epoch [118/120    avg_loss:0.007, val_acc:0.980]
Epoch [119/120    avg_loss:0.006, val_acc:0.979]
Epoch [120/120    avg_loss:0.006, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1248    0    0    0    0    0    0    1    9   23    4    0
     0    0    0]
 [   0    0    1  729    0    0    0    0    0    7    4    1    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    7    0    0    0    0    0    0  422    0    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    3    5    0    0    0  846   15    0    0
     0    1    0]
 [   0    0    6    0    0    2    1    0    0    0   11 2190    0    0
     0    0    0]
 [   0    0    0   10    0    0    0    0    0    0    5    1  517    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    4    0    0    0    0
  1133    2    0]
 [   0    0    0    0    0    1    1    0    0    0    0    0    0    0
    21  324    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.15718157181571

F1 scores:
[       nan 0.88372093 0.98074656 0.98049765 1.         0.98737084
 0.99393939 1.         0.99061033 0.72340426 0.96520251 0.98648649
 0.97271872 1.         0.98564593 0.96142433 0.98203593]

Kappa:
0.978983159067661
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f19199867b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.647, val_acc:0.325]
Epoch [2/120    avg_loss:2.246, val_acc:0.357]
Epoch [3/120    avg_loss:2.031, val_acc:0.544]
Epoch [4/120    avg_loss:1.815, val_acc:0.578]
Epoch [5/120    avg_loss:1.627, val_acc:0.621]
Epoch [6/120    avg_loss:1.486, val_acc:0.683]
Epoch [7/120    avg_loss:1.307, val_acc:0.672]
Epoch [8/120    avg_loss:1.129, val_acc:0.707]
Epoch [9/120    avg_loss:0.980, val_acc:0.730]
Epoch [10/120    avg_loss:0.869, val_acc:0.769]
Epoch [11/120    avg_loss:0.802, val_acc:0.781]
Epoch [12/120    avg_loss:0.650, val_acc:0.815]
Epoch [13/120    avg_loss:0.546, val_acc:0.809]
Epoch [14/120    avg_loss:0.440, val_acc:0.817]
Epoch [15/120    avg_loss:0.541, val_acc:0.789]
Epoch [16/120    avg_loss:0.423, val_acc:0.844]
Epoch [17/120    avg_loss:0.361, val_acc:0.883]
Epoch [18/120    avg_loss:0.352, val_acc:0.880]
Epoch [19/120    avg_loss:0.257, val_acc:0.885]
Epoch [20/120    avg_loss:0.278, val_acc:0.881]
Epoch [21/120    avg_loss:0.280, val_acc:0.873]
Epoch [22/120    avg_loss:0.262, val_acc:0.900]
Epoch [23/120    avg_loss:0.279, val_acc:0.853]
Epoch [24/120    avg_loss:0.203, val_acc:0.921]
Epoch [25/120    avg_loss:0.162, val_acc:0.933]
Epoch [26/120    avg_loss:0.155, val_acc:0.918]
Epoch [27/120    avg_loss:0.155, val_acc:0.935]
Epoch [28/120    avg_loss:0.155, val_acc:0.891]
Epoch [29/120    avg_loss:0.159, val_acc:0.933]
Epoch [30/120    avg_loss:0.162, val_acc:0.929]
Epoch [31/120    avg_loss:0.130, val_acc:0.926]
Epoch [32/120    avg_loss:0.182, val_acc:0.927]
Epoch [33/120    avg_loss:0.108, val_acc:0.947]
Epoch [34/120    avg_loss:0.094, val_acc:0.957]
Epoch [35/120    avg_loss:0.107, val_acc:0.942]
Epoch [36/120    avg_loss:0.085, val_acc:0.933]
Epoch [37/120    avg_loss:0.088, val_acc:0.947]
Epoch [38/120    avg_loss:0.093, val_acc:0.952]
Epoch [39/120    avg_loss:0.071, val_acc:0.969]
Epoch [40/120    avg_loss:0.055, val_acc:0.966]
Epoch [41/120    avg_loss:0.054, val_acc:0.960]
Epoch [42/120    avg_loss:0.082, val_acc:0.954]
Epoch [43/120    avg_loss:0.066, val_acc:0.944]
Epoch [44/120    avg_loss:0.123, val_acc:0.889]
Epoch [45/120    avg_loss:0.130, val_acc:0.949]
Epoch [46/120    avg_loss:0.091, val_acc:0.948]
Epoch [47/120    avg_loss:0.089, val_acc:0.960]
Epoch [48/120    avg_loss:0.062, val_acc:0.961]
Epoch [49/120    avg_loss:0.067, val_acc:0.965]
Epoch [50/120    avg_loss:0.075, val_acc:0.953]
Epoch [51/120    avg_loss:0.050, val_acc:0.966]
Epoch [52/120    avg_loss:0.047, val_acc:0.961]
Epoch [53/120    avg_loss:0.043, val_acc:0.972]
Epoch [54/120    avg_loss:0.035, val_acc:0.972]
Epoch [55/120    avg_loss:0.033, val_acc:0.970]
Epoch [56/120    avg_loss:0.031, val_acc:0.971]
Epoch [57/120    avg_loss:0.031, val_acc:0.971]
Epoch [58/120    avg_loss:0.028, val_acc:0.971]
Epoch [59/120    avg_loss:0.027, val_acc:0.971]
Epoch [60/120    avg_loss:0.030, val_acc:0.971]
Epoch [61/120    avg_loss:0.028, val_acc:0.974]
Epoch [62/120    avg_loss:0.025, val_acc:0.973]
Epoch [63/120    avg_loss:0.027, val_acc:0.974]
Epoch [64/120    avg_loss:0.027, val_acc:0.973]
Epoch [65/120    avg_loss:0.020, val_acc:0.972]
Epoch [66/120    avg_loss:0.023, val_acc:0.973]
Epoch [67/120    avg_loss:0.027, val_acc:0.973]
Epoch [68/120    avg_loss:0.027, val_acc:0.973]
Epoch [69/120    avg_loss:0.027, val_acc:0.973]
Epoch [70/120    avg_loss:0.031, val_acc:0.973]
Epoch [71/120    avg_loss:0.026, val_acc:0.973]
Epoch [72/120    avg_loss:0.026, val_acc:0.973]
Epoch [73/120    avg_loss:0.023, val_acc:0.973]
Epoch [74/120    avg_loss:0.024, val_acc:0.973]
Epoch [75/120    avg_loss:0.024, val_acc:0.973]
Epoch [76/120    avg_loss:0.023, val_acc:0.974]
Epoch [77/120    avg_loss:0.021, val_acc:0.973]
Epoch [78/120    avg_loss:0.026, val_acc:0.973]
Epoch [79/120    avg_loss:0.023, val_acc:0.971]
Epoch [80/120    avg_loss:0.026, val_acc:0.972]
Epoch [81/120    avg_loss:0.026, val_acc:0.973]
Epoch [82/120    avg_loss:0.025, val_acc:0.974]
Epoch [83/120    avg_loss:0.022, val_acc:0.974]
Epoch [84/120    avg_loss:0.021, val_acc:0.973]
Epoch [85/120    avg_loss:0.021, val_acc:0.973]
Epoch [86/120    avg_loss:0.024, val_acc:0.975]
Epoch [87/120    avg_loss:0.023, val_acc:0.972]
Epoch [88/120    avg_loss:0.026, val_acc:0.972]
Epoch [89/120    avg_loss:0.021, val_acc:0.972]
Epoch [90/120    avg_loss:0.019, val_acc:0.972]
Epoch [91/120    avg_loss:0.021, val_acc:0.972]
Epoch [92/120    avg_loss:0.018, val_acc:0.973]
Epoch [93/120    avg_loss:0.020, val_acc:0.974]
Epoch [94/120    avg_loss:0.021, val_acc:0.975]
Epoch [95/120    avg_loss:0.019, val_acc:0.975]
Epoch [96/120    avg_loss:0.018, val_acc:0.976]
Epoch [97/120    avg_loss:0.020, val_acc:0.974]
Epoch [98/120    avg_loss:0.022, val_acc:0.975]
Epoch [99/120    avg_loss:0.020, val_acc:0.975]
Epoch [100/120    avg_loss:0.019, val_acc:0.977]
Epoch [101/120    avg_loss:0.020, val_acc:0.977]
Epoch [102/120    avg_loss:0.019, val_acc:0.977]
Epoch [103/120    avg_loss:0.019, val_acc:0.977]
Epoch [104/120    avg_loss:0.018, val_acc:0.976]
Epoch [105/120    avg_loss:0.019, val_acc:0.977]
Epoch [106/120    avg_loss:0.020, val_acc:0.972]
Epoch [107/120    avg_loss:0.021, val_acc:0.974]
Epoch [108/120    avg_loss:0.020, val_acc:0.975]
Epoch [109/120    avg_loss:0.017, val_acc:0.976]
Epoch [110/120    avg_loss:0.022, val_acc:0.977]
Epoch [111/120    avg_loss:0.019, val_acc:0.976]
Epoch [112/120    avg_loss:0.017, val_acc:0.975]
Epoch [113/120    avg_loss:0.020, val_acc:0.976]
Epoch [114/120    avg_loss:0.020, val_acc:0.978]
Epoch [115/120    avg_loss:0.020, val_acc:0.978]
Epoch [116/120    avg_loss:0.021, val_acc:0.978]
Epoch [117/120    avg_loss:0.019, val_acc:0.975]
Epoch [118/120    avg_loss:0.018, val_acc:0.977]
Epoch [119/120    avg_loss:0.020, val_acc:0.976]
Epoch [120/120    avg_loss:0.019, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1237   11    0    0    2    0    0    3   10   19    2    0
     0    1    0]
 [   0    0    0  705    2    0    0    0    0   15    1    0   16    8
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    4    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    2    1    0    0    4  845   17    0    0
     0    4    0]
 [   0    0    4    0    2    0    3    0    0    7    9 2162   21    1
     0    1    0]
 [   0    0    0    3    0    0    0    0    0    0    0    1  529    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1129    9    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    28  319    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.59349593495935

F1 scores:
[       nan 0.95121951 0.97863924 0.96114519 0.99069767 0.99192618
 0.99545455 0.92592593 0.997669   0.52307692 0.96959266 0.98072125
 0.95920218 0.9762533  0.98344948 0.93685756 0.98809524]

Kappa:
0.9725875310671637
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4bc06b4860>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.613, val_acc:0.326]
Epoch [2/120    avg_loss:2.213, val_acc:0.487]
Epoch [3/120    avg_loss:1.988, val_acc:0.510]
Epoch [4/120    avg_loss:1.801, val_acc:0.577]
Epoch [5/120    avg_loss:1.686, val_acc:0.568]
Epoch [6/120    avg_loss:1.585, val_acc:0.596]
Epoch [7/120    avg_loss:1.407, val_acc:0.648]
Epoch [8/120    avg_loss:1.290, val_acc:0.639]
Epoch [9/120    avg_loss:1.170, val_acc:0.684]
Epoch [10/120    avg_loss:1.033, val_acc:0.704]
Epoch [11/120    avg_loss:0.892, val_acc:0.760]
Epoch [12/120    avg_loss:0.836, val_acc:0.737]
Epoch [13/120    avg_loss:0.684, val_acc:0.792]
Epoch [14/120    avg_loss:0.663, val_acc:0.805]
Epoch [15/120    avg_loss:0.573, val_acc:0.834]
Epoch [16/120    avg_loss:0.642, val_acc:0.743]
Epoch [17/120    avg_loss:0.651, val_acc:0.789]
Epoch [18/120    avg_loss:0.500, val_acc:0.789]
Epoch [19/120    avg_loss:0.444, val_acc:0.834]
Epoch [20/120    avg_loss:0.454, val_acc:0.800]
Epoch [21/120    avg_loss:0.413, val_acc:0.856]
Epoch [22/120    avg_loss:0.429, val_acc:0.838]
Epoch [23/120    avg_loss:0.339, val_acc:0.860]
Epoch [24/120    avg_loss:0.267, val_acc:0.846]
Epoch [25/120    avg_loss:0.330, val_acc:0.863]
Epoch [26/120    avg_loss:0.288, val_acc:0.865]
Epoch [27/120    avg_loss:0.235, val_acc:0.900]
Epoch [28/120    avg_loss:0.226, val_acc:0.889]
Epoch [29/120    avg_loss:0.218, val_acc:0.884]
Epoch [30/120    avg_loss:0.226, val_acc:0.887]
Epoch [31/120    avg_loss:0.185, val_acc:0.897]
Epoch [32/120    avg_loss:0.169, val_acc:0.896]
Epoch [33/120    avg_loss:0.162, val_acc:0.891]
Epoch [34/120    avg_loss:0.212, val_acc:0.891]
Epoch [35/120    avg_loss:0.155, val_acc:0.924]
Epoch [36/120    avg_loss:0.145, val_acc:0.945]
Epoch [37/120    avg_loss:0.101, val_acc:0.917]
Epoch [38/120    avg_loss:0.111, val_acc:0.923]
Epoch [39/120    avg_loss:0.094, val_acc:0.934]
Epoch [40/120    avg_loss:0.101, val_acc:0.927]
Epoch [41/120    avg_loss:0.099, val_acc:0.907]
Epoch [42/120    avg_loss:0.103, val_acc:0.944]
Epoch [43/120    avg_loss:0.077, val_acc:0.938]
Epoch [44/120    avg_loss:0.073, val_acc:0.947]
Epoch [45/120    avg_loss:0.081, val_acc:0.938]
Epoch [46/120    avg_loss:0.076, val_acc:0.957]
Epoch [47/120    avg_loss:0.070, val_acc:0.938]
Epoch [48/120    avg_loss:0.096, val_acc:0.936]
Epoch [49/120    avg_loss:0.089, val_acc:0.933]
Epoch [50/120    avg_loss:0.076, val_acc:0.933]
Epoch [51/120    avg_loss:0.077, val_acc:0.946]
Epoch [52/120    avg_loss:0.071, val_acc:0.934]
Epoch [53/120    avg_loss:0.076, val_acc:0.930]
Epoch [54/120    avg_loss:0.087, val_acc:0.929]
Epoch [55/120    avg_loss:0.074, val_acc:0.946]
Epoch [56/120    avg_loss:0.057, val_acc:0.951]
Epoch [57/120    avg_loss:0.072, val_acc:0.945]
Epoch [58/120    avg_loss:0.088, val_acc:0.944]
Epoch [59/120    avg_loss:0.070, val_acc:0.941]
Epoch [60/120    avg_loss:0.047, val_acc:0.957]
Epoch [61/120    avg_loss:0.054, val_acc:0.963]
Epoch [62/120    avg_loss:0.039, val_acc:0.966]
Epoch [63/120    avg_loss:0.036, val_acc:0.967]
Epoch [64/120    avg_loss:0.037, val_acc:0.967]
Epoch [65/120    avg_loss:0.040, val_acc:0.964]
Epoch [66/120    avg_loss:0.035, val_acc:0.965]
Epoch [67/120    avg_loss:0.035, val_acc:0.965]
Epoch [68/120    avg_loss:0.033, val_acc:0.966]
Epoch [69/120    avg_loss:0.030, val_acc:0.967]
Epoch [70/120    avg_loss:0.035, val_acc:0.965]
Epoch [71/120    avg_loss:0.036, val_acc:0.968]
Epoch [72/120    avg_loss:0.029, val_acc:0.968]
Epoch [73/120    avg_loss:0.028, val_acc:0.966]
Epoch [74/120    avg_loss:0.030, val_acc:0.964]
Epoch [75/120    avg_loss:0.029, val_acc:0.966]
Epoch [76/120    avg_loss:0.034, val_acc:0.965]
Epoch [77/120    avg_loss:0.039, val_acc:0.963]
Epoch [78/120    avg_loss:0.028, val_acc:0.966]
Epoch [79/120    avg_loss:0.026, val_acc:0.967]
Epoch [80/120    avg_loss:0.028, val_acc:0.966]
Epoch [81/120    avg_loss:0.029, val_acc:0.967]
Epoch [82/120    avg_loss:0.031, val_acc:0.966]
Epoch [83/120    avg_loss:0.032, val_acc:0.966]
Epoch [84/120    avg_loss:0.024, val_acc:0.966]
Epoch [85/120    avg_loss:0.032, val_acc:0.967]
Epoch [86/120    avg_loss:0.028, val_acc:0.968]
Epoch [87/120    avg_loss:0.027, val_acc:0.967]
Epoch [88/120    avg_loss:0.021, val_acc:0.968]
Epoch [89/120    avg_loss:0.024, val_acc:0.968]
Epoch [90/120    avg_loss:0.026, val_acc:0.968]
Epoch [91/120    avg_loss:0.029, val_acc:0.968]
Epoch [92/120    avg_loss:0.032, val_acc:0.968]
Epoch [93/120    avg_loss:0.028, val_acc:0.967]
Epoch [94/120    avg_loss:0.024, val_acc:0.969]
Epoch [95/120    avg_loss:0.022, val_acc:0.969]
Epoch [96/120    avg_loss:0.023, val_acc:0.968]
Epoch [97/120    avg_loss:0.025, val_acc:0.967]
Epoch [98/120    avg_loss:0.031, val_acc:0.968]
Epoch [99/120    avg_loss:0.024, val_acc:0.967]
Epoch [100/120    avg_loss:0.026, val_acc:0.968]
Epoch [101/120    avg_loss:0.026, val_acc:0.968]
Epoch [102/120    avg_loss:0.027, val_acc:0.968]
Epoch [103/120    avg_loss:0.024, val_acc:0.968]
Epoch [104/120    avg_loss:0.023, val_acc:0.968]
Epoch [105/120    avg_loss:0.024, val_acc:0.966]
Epoch [106/120    avg_loss:0.029, val_acc:0.967]
Epoch [107/120    avg_loss:0.028, val_acc:0.967]
Epoch [108/120    avg_loss:0.028, val_acc:0.967]
Epoch [109/120    avg_loss:0.030, val_acc:0.967]
Epoch [110/120    avg_loss:0.021, val_acc:0.967]
Epoch [111/120    avg_loss:0.021, val_acc:0.967]
Epoch [112/120    avg_loss:0.024, val_acc:0.968]
Epoch [113/120    avg_loss:0.025, val_acc:0.968]
Epoch [114/120    avg_loss:0.029, val_acc:0.968]
Epoch [115/120    avg_loss:0.022, val_acc:0.968]
Epoch [116/120    avg_loss:0.029, val_acc:0.968]
Epoch [117/120    avg_loss:0.023, val_acc:0.968]
Epoch [118/120    avg_loss:0.025, val_acc:0.968]
Epoch [119/120    avg_loss:0.030, val_acc:0.968]
Epoch [120/120    avg_loss:0.024, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1245    0    0    0    0    0    0    2    9   23    4    0
     0    2    0]
 [   0    0    2  680    0   14    1    0    0   16    2    3   26    0
     0    3    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    4    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    5    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   11    0    0    3    0    0    0    0  836   20    2    0
     0    3    0]
 [   0    0    8    0    0    1    1    0    1    0   28 2167    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    6   11  507    0
     0    4    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    3    0    0    0
  1129    5    0]
 [   0    0    0    0    0    0    6    0    0    3    0    0    0    0
    37  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.82384823848238

F1 scores:
[       nan 0.90243902 0.97608781 0.95304835 1.         0.97162316
 0.99012908 1.         0.99185099 0.56666667 0.94838344 0.97634602
 0.9406308  1.         0.97833622 0.90526316 0.96551724]

Kappa:
0.9637814118207138
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8e606837f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.675, val_acc:0.309]
Epoch [2/120    avg_loss:2.239, val_acc:0.534]
Epoch [3/120    avg_loss:2.010, val_acc:0.596]
Epoch [4/120    avg_loss:1.814, val_acc:0.618]
Epoch [5/120    avg_loss:1.695, val_acc:0.626]
Epoch [6/120    avg_loss:1.545, val_acc:0.655]
Epoch [7/120    avg_loss:1.342, val_acc:0.672]
Epoch [8/120    avg_loss:1.182, val_acc:0.697]
Epoch [9/120    avg_loss:1.022, val_acc:0.685]
Epoch [10/120    avg_loss:0.916, val_acc:0.702]
Epoch [11/120    avg_loss:0.862, val_acc:0.712]
Epoch [12/120    avg_loss:0.835, val_acc:0.801]
Epoch [13/120    avg_loss:0.723, val_acc:0.780]
Epoch [14/120    avg_loss:0.657, val_acc:0.798]
Epoch [15/120    avg_loss:0.707, val_acc:0.809]
Epoch [16/120    avg_loss:0.584, val_acc:0.840]
Epoch [17/120    avg_loss:0.576, val_acc:0.766]
Epoch [18/120    avg_loss:0.482, val_acc:0.872]
Epoch [19/120    avg_loss:0.372, val_acc:0.862]
Epoch [20/120    avg_loss:0.429, val_acc:0.876]
Epoch [21/120    avg_loss:0.345, val_acc:0.875]
Epoch [22/120    avg_loss:0.364, val_acc:0.896]
Epoch [23/120    avg_loss:0.279, val_acc:0.874]
Epoch [24/120    avg_loss:0.287, val_acc:0.900]
Epoch [25/120    avg_loss:0.215, val_acc:0.915]
Epoch [26/120    avg_loss:0.216, val_acc:0.893]
Epoch [27/120    avg_loss:0.257, val_acc:0.911]
Epoch [28/120    avg_loss:0.197, val_acc:0.908]
Epoch [29/120    avg_loss:0.216, val_acc:0.894]
Epoch [30/120    avg_loss:0.203, val_acc:0.903]
Epoch [31/120    avg_loss:0.210, val_acc:0.895]
Epoch [32/120    avg_loss:0.170, val_acc:0.870]
Epoch [33/120    avg_loss:0.121, val_acc:0.944]
Epoch [34/120    avg_loss:0.134, val_acc:0.915]
Epoch [35/120    avg_loss:0.162, val_acc:0.919]
Epoch [36/120    avg_loss:0.148, val_acc:0.940]
Epoch [37/120    avg_loss:0.137, val_acc:0.944]
Epoch [38/120    avg_loss:0.125, val_acc:0.956]
Epoch [39/120    avg_loss:0.100, val_acc:0.955]
Epoch [40/120    avg_loss:0.102, val_acc:0.921]
Epoch [41/120    avg_loss:0.100, val_acc:0.939]
Epoch [42/120    avg_loss:0.079, val_acc:0.957]
Epoch [43/120    avg_loss:0.116, val_acc:0.932]
Epoch [44/120    avg_loss:0.087, val_acc:0.936]
Epoch [45/120    avg_loss:0.067, val_acc:0.950]
Epoch [46/120    avg_loss:0.062, val_acc:0.966]
Epoch [47/120    avg_loss:0.073, val_acc:0.952]
Epoch [48/120    avg_loss:0.090, val_acc:0.918]
Epoch [49/120    avg_loss:0.100, val_acc:0.928]
Epoch [50/120    avg_loss:0.083, val_acc:0.951]
Epoch [51/120    avg_loss:0.067, val_acc:0.955]
Epoch [52/120    avg_loss:0.073, val_acc:0.948]
Epoch [53/120    avg_loss:0.058, val_acc:0.958]
Epoch [54/120    avg_loss:0.045, val_acc:0.962]
Epoch [55/120    avg_loss:0.038, val_acc:0.948]
Epoch [56/120    avg_loss:0.079, val_acc:0.954]
Epoch [57/120    avg_loss:0.055, val_acc:0.958]
Epoch [58/120    avg_loss:0.059, val_acc:0.949]
Epoch [59/120    avg_loss:0.052, val_acc:0.963]
Epoch [60/120    avg_loss:0.045, val_acc:0.970]
Epoch [61/120    avg_loss:0.038, val_acc:0.973]
Epoch [62/120    avg_loss:0.033, val_acc:0.976]
Epoch [63/120    avg_loss:0.036, val_acc:0.974]
Epoch [64/120    avg_loss:0.031, val_acc:0.978]
Epoch [65/120    avg_loss:0.035, val_acc:0.981]
Epoch [66/120    avg_loss:0.029, val_acc:0.976]
Epoch [67/120    avg_loss:0.027, val_acc:0.978]
Epoch [68/120    avg_loss:0.025, val_acc:0.978]
Epoch [69/120    avg_loss:0.025, val_acc:0.976]
Epoch [70/120    avg_loss:0.024, val_acc:0.979]
Epoch [71/120    avg_loss:0.026, val_acc:0.981]
Epoch [72/120    avg_loss:0.028, val_acc:0.978]
Epoch [73/120    avg_loss:0.025, val_acc:0.980]
Epoch [74/120    avg_loss:0.023, val_acc:0.976]
Epoch [75/120    avg_loss:0.024, val_acc:0.976]
Epoch [76/120    avg_loss:0.027, val_acc:0.978]
Epoch [77/120    avg_loss:0.025, val_acc:0.974]
Epoch [78/120    avg_loss:0.028, val_acc:0.981]
Epoch [79/120    avg_loss:0.025, val_acc:0.983]
Epoch [80/120    avg_loss:0.025, val_acc:0.982]
Epoch [81/120    avg_loss:0.024, val_acc:0.984]
Epoch [82/120    avg_loss:0.024, val_acc:0.976]
Epoch [83/120    avg_loss:0.033, val_acc:0.982]
Epoch [84/120    avg_loss:0.021, val_acc:0.979]
Epoch [85/120    avg_loss:0.021, val_acc:0.976]
Epoch [86/120    avg_loss:0.025, val_acc:0.978]
Epoch [87/120    avg_loss:0.027, val_acc:0.975]
Epoch [88/120    avg_loss:0.021, val_acc:0.978]
Epoch [89/120    avg_loss:0.025, val_acc:0.980]
Epoch [90/120    avg_loss:0.023, val_acc:0.980]
Epoch [91/120    avg_loss:0.024, val_acc:0.978]
Epoch [92/120    avg_loss:0.023, val_acc:0.985]
Epoch [93/120    avg_loss:0.026, val_acc:0.984]
Epoch [94/120    avg_loss:0.027, val_acc:0.982]
Epoch [95/120    avg_loss:0.029, val_acc:0.978]
Epoch [96/120    avg_loss:0.026, val_acc:0.983]
Epoch [97/120    avg_loss:0.024, val_acc:0.982]
Epoch [98/120    avg_loss:0.025, val_acc:0.980]
Epoch [99/120    avg_loss:0.027, val_acc:0.980]
Epoch [100/120    avg_loss:0.024, val_acc:0.981]
Epoch [101/120    avg_loss:0.019, val_acc:0.982]
Epoch [102/120    avg_loss:0.023, val_acc:0.981]
Epoch [103/120    avg_loss:0.023, val_acc:0.980]
Epoch [104/120    avg_loss:0.021, val_acc:0.982]
Epoch [105/120    avg_loss:0.023, val_acc:0.980]
Epoch [106/120    avg_loss:0.024, val_acc:0.980]
Epoch [107/120    avg_loss:0.019, val_acc:0.981]
Epoch [108/120    avg_loss:0.023, val_acc:0.982]
Epoch [109/120    avg_loss:0.025, val_acc:0.981]
Epoch [110/120    avg_loss:0.023, val_acc:0.983]
Epoch [111/120    avg_loss:0.021, val_acc:0.983]
Epoch [112/120    avg_loss:0.021, val_acc:0.982]
Epoch [113/120    avg_loss:0.021, val_acc:0.980]
Epoch [114/120    avg_loss:0.021, val_acc:0.981]
Epoch [115/120    avg_loss:0.023, val_acc:0.983]
Epoch [116/120    avg_loss:0.023, val_acc:0.982]
Epoch [117/120    avg_loss:0.027, val_acc:0.982]
Epoch [118/120    avg_loss:0.026, val_acc:0.982]
Epoch [119/120    avg_loss:0.018, val_acc:0.982]
Epoch [120/120    avg_loss:0.019, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1249    0    0    0    1    0    0    3    9   19    4    0
     0    0    0]
 [   0    0    0  705    4   15    3    0    0    5    1    5    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  427    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   13    0    0    4    5    0    0    0  844    7    1    0
     0    1    0]
 [   0    0   14    1    0    0    2    2    0    1    7 2174    7    2
     0    0    0]
 [   0    0    0    1    2    3    0    0    0    0   15    0  506    0
     1    4    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    1    0    2    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0   43    0    0    0    0    0    0    0
    11  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.36585365853658

F1 scores:
[       nan 0.97560976 0.97540023 0.96973865 0.98611111 0.96969697
 0.96052632 0.94339623 0.995338   0.73913043 0.96237172 0.98482446
 0.94845361 0.99462366 0.99256018 0.90852713 0.97005988]

Kappa:
0.9699735641991095
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:08:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9981599898>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.600, val_acc:0.402]
Epoch [2/120    avg_loss:2.189, val_acc:0.541]
Epoch [3/120    avg_loss:1.979, val_acc:0.574]
Epoch [4/120    avg_loss:1.748, val_acc:0.600]
Epoch [5/120    avg_loss:1.553, val_acc:0.612]
Epoch [6/120    avg_loss:1.413, val_acc:0.643]
Epoch [7/120    avg_loss:1.199, val_acc:0.674]
Epoch [8/120    avg_loss:1.117, val_acc:0.671]
Epoch [9/120    avg_loss:0.926, val_acc:0.708]
Epoch [10/120    avg_loss:0.828, val_acc:0.756]
Epoch [11/120    avg_loss:0.775, val_acc:0.778]
Epoch [12/120    avg_loss:0.767, val_acc:0.724]
Epoch [13/120    avg_loss:0.651, val_acc:0.825]
Epoch [14/120    avg_loss:0.571, val_acc:0.721]
Epoch [15/120    avg_loss:0.593, val_acc:0.821]
Epoch [16/120    avg_loss:0.478, val_acc:0.829]
Epoch [17/120    avg_loss:0.516, val_acc:0.821]
Epoch [18/120    avg_loss:0.579, val_acc:0.806]
Epoch [19/120    avg_loss:0.463, val_acc:0.828]
Epoch [20/120    avg_loss:0.402, val_acc:0.863]
Epoch [21/120    avg_loss:0.353, val_acc:0.839]
Epoch [22/120    avg_loss:0.362, val_acc:0.877]
Epoch [23/120    avg_loss:0.309, val_acc:0.863]
Epoch [24/120    avg_loss:0.315, val_acc:0.885]
Epoch [25/120    avg_loss:0.269, val_acc:0.897]
Epoch [26/120    avg_loss:0.316, val_acc:0.856]
Epoch [27/120    avg_loss:0.291, val_acc:0.907]
Epoch [28/120    avg_loss:0.208, val_acc:0.918]
Epoch [29/120    avg_loss:0.277, val_acc:0.878]
Epoch [30/120    avg_loss:0.273, val_acc:0.901]
Epoch [31/120    avg_loss:0.209, val_acc:0.887]
Epoch [32/120    avg_loss:0.204, val_acc:0.902]
Epoch [33/120    avg_loss:0.197, val_acc:0.911]
Epoch [34/120    avg_loss:0.178, val_acc:0.906]
Epoch [35/120    avg_loss:0.152, val_acc:0.919]
Epoch [36/120    avg_loss:0.139, val_acc:0.927]
Epoch [37/120    avg_loss:0.115, val_acc:0.924]
Epoch [38/120    avg_loss:0.179, val_acc:0.904]
Epoch [39/120    avg_loss:0.214, val_acc:0.896]
Epoch [40/120    avg_loss:0.181, val_acc:0.929]
Epoch [41/120    avg_loss:0.141, val_acc:0.927]
Epoch [42/120    avg_loss:0.113, val_acc:0.929]
Epoch [43/120    avg_loss:0.131, val_acc:0.920]
Epoch [44/120    avg_loss:0.142, val_acc:0.929]
Epoch [45/120    avg_loss:0.105, val_acc:0.951]
Epoch [46/120    avg_loss:0.095, val_acc:0.949]
Epoch [47/120    avg_loss:0.082, val_acc:0.952]
Epoch [48/120    avg_loss:0.081, val_acc:0.943]
Epoch [49/120    avg_loss:0.064, val_acc:0.953]
Epoch [50/120    avg_loss:0.065, val_acc:0.958]
Epoch [51/120    avg_loss:0.078, val_acc:0.959]
Epoch [52/120    avg_loss:0.083, val_acc:0.954]
Epoch [53/120    avg_loss:0.083, val_acc:0.958]
Epoch [54/120    avg_loss:0.067, val_acc:0.959]
Epoch [55/120    avg_loss:0.066, val_acc:0.942]
Epoch [56/120    avg_loss:0.063, val_acc:0.956]
Epoch [57/120    avg_loss:0.061, val_acc:0.960]
Epoch [58/120    avg_loss:0.068, val_acc:0.953]
Epoch [59/120    avg_loss:0.063, val_acc:0.932]
Epoch [60/120    avg_loss:0.059, val_acc:0.949]
Epoch [61/120    avg_loss:0.055, val_acc:0.968]
Epoch [62/120    avg_loss:0.070, val_acc:0.967]
Epoch [63/120    avg_loss:0.057, val_acc:0.963]
Epoch [64/120    avg_loss:0.054, val_acc:0.948]
Epoch [65/120    avg_loss:0.049, val_acc:0.966]
Epoch [66/120    avg_loss:0.049, val_acc:0.963]
Epoch [67/120    avg_loss:0.045, val_acc:0.954]
Epoch [68/120    avg_loss:0.066, val_acc:0.959]
Epoch [69/120    avg_loss:0.038, val_acc:0.964]
Epoch [70/120    avg_loss:0.041, val_acc:0.966]
Epoch [71/120    avg_loss:0.031, val_acc:0.963]
Epoch [72/120    avg_loss:0.036, val_acc:0.971]
Epoch [73/120    avg_loss:0.041, val_acc:0.955]
Epoch [74/120    avg_loss:0.053, val_acc:0.962]
Epoch [75/120    avg_loss:0.055, val_acc:0.960]
Epoch [76/120    avg_loss:0.035, val_acc:0.966]
Epoch [77/120    avg_loss:0.033, val_acc:0.971]
Epoch [78/120    avg_loss:0.032, val_acc:0.974]
Epoch [79/120    avg_loss:0.030, val_acc:0.973]
Epoch [80/120    avg_loss:0.025, val_acc:0.979]
Epoch [81/120    avg_loss:0.047, val_acc:0.962]
Epoch [82/120    avg_loss:0.037, val_acc:0.974]
Epoch [83/120    avg_loss:0.055, val_acc:0.960]
Epoch [84/120    avg_loss:0.048, val_acc:0.972]
Epoch [85/120    avg_loss:0.035, val_acc:0.960]
Epoch [86/120    avg_loss:0.024, val_acc:0.968]
Epoch [87/120    avg_loss:0.034, val_acc:0.965]
Epoch [88/120    avg_loss:0.025, val_acc:0.973]
Epoch [89/120    avg_loss:0.018, val_acc:0.975]
Epoch [90/120    avg_loss:0.019, val_acc:0.980]
Epoch [91/120    avg_loss:0.016, val_acc:0.976]
Epoch [92/120    avg_loss:0.025, val_acc:0.980]
Epoch [93/120    avg_loss:0.022, val_acc:0.982]
Epoch [94/120    avg_loss:0.025, val_acc:0.971]
Epoch [95/120    avg_loss:0.037, val_acc:0.955]
Epoch [96/120    avg_loss:0.048, val_acc:0.960]
Epoch [97/120    avg_loss:0.031, val_acc:0.969]
Epoch [98/120    avg_loss:0.022, val_acc:0.970]
Epoch [99/120    avg_loss:0.682, val_acc:0.442]
Epoch [100/120    avg_loss:1.554, val_acc:0.629]
Epoch [101/120    avg_loss:1.159, val_acc:0.685]
Epoch [102/120    avg_loss:0.895, val_acc:0.765]
Epoch [103/120    avg_loss:0.732, val_acc:0.771]
Epoch [104/120    avg_loss:0.624, val_acc:0.811]
Epoch [105/120    avg_loss:0.451, val_acc:0.840]
Epoch [106/120    avg_loss:0.406, val_acc:0.864]
Epoch [107/120    avg_loss:0.290, val_acc:0.888]
Epoch [108/120    avg_loss:0.268, val_acc:0.897]
Epoch [109/120    avg_loss:0.264, val_acc:0.904]
Epoch [110/120    avg_loss:0.230, val_acc:0.897]
Epoch [111/120    avg_loss:0.220, val_acc:0.899]
Epoch [112/120    avg_loss:0.232, val_acc:0.904]
Epoch [113/120    avg_loss:0.264, val_acc:0.904]
Epoch [114/120    avg_loss:0.221, val_acc:0.909]
Epoch [115/120    avg_loss:0.219, val_acc:0.908]
Epoch [116/120    avg_loss:0.209, val_acc:0.906]
Epoch [117/120    avg_loss:0.200, val_acc:0.915]
Epoch [118/120    avg_loss:0.180, val_acc:0.905]
Epoch [119/120    avg_loss:0.221, val_acc:0.905]
Epoch [120/120    avg_loss:0.192, val_acc:0.907]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1187   15    3    0   13    0    0    0   17   33    1    0
     7    9    0]
 [   0    0    1  609   42   10    0    0    0   14    0    0   48    8
     0   15    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  392    0   23    0    5    1    0    0    2
    10    2    0]
 [   0    0    0    0    0    0  631    0    0    1    0   15    0    0
    10    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    0    0
     0    4    0]
 [   0    0    0    0    5    0    4    0    0    8    0    0    1    0
     0    0    0]
 [   0    0   24    0    0   11    8    0    0    1  747   41    7    0
     0   36    0]
 [   0    1   20   10    6    9    9    2    0    0   33 2084    3    3
     0   30    0]
 [   0    0    5    4   25    2    2    0    0    0   14    0  465    0
     0    9    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    2    1   20    0    0   16    1    1    0    1
  1046   51    0]
 [   0    0    0    0    0    0   33    0    0    2    0    0    0    0
    31  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
91.2520325203252

F1 scores:
[       nan 0.93670886 0.94131642 0.87942238 0.83693517 0.91162791
 0.91648511 0.66666667 0.9953271  0.24615385 0.88297872 0.95072993
 0.87653157 0.96354167 0.93267945 0.71683673 0.94252874]

Kappa:
0.9005698281799296
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9fbcf6d828>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.602, val_acc:0.297]
Epoch [2/120    avg_loss:2.252, val_acc:0.515]
Epoch [3/120    avg_loss:2.060, val_acc:0.552]
Epoch [4/120    avg_loss:1.887, val_acc:0.595]
Epoch [5/120    avg_loss:1.671, val_acc:0.639]
Epoch [6/120    avg_loss:1.581, val_acc:0.638]
Epoch [7/120    avg_loss:1.358, val_acc:0.657]
Epoch [8/120    avg_loss:1.187, val_acc:0.714]
Epoch [9/120    avg_loss:0.978, val_acc:0.706]
Epoch [10/120    avg_loss:0.867, val_acc:0.718]
Epoch [11/120    avg_loss:0.759, val_acc:0.728]
Epoch [12/120    avg_loss:0.698, val_acc:0.750]
Epoch [13/120    avg_loss:0.658, val_acc:0.752]
Epoch [14/120    avg_loss:0.615, val_acc:0.811]
Epoch [15/120    avg_loss:0.512, val_acc:0.796]
Epoch [16/120    avg_loss:0.451, val_acc:0.829]
Epoch [17/120    avg_loss:0.400, val_acc:0.843]
Epoch [18/120    avg_loss:0.390, val_acc:0.881]
Epoch [19/120    avg_loss:0.375, val_acc:0.832]
Epoch [20/120    avg_loss:0.397, val_acc:0.878]
Epoch [21/120    avg_loss:0.373, val_acc:0.817]
Epoch [22/120    avg_loss:0.293, val_acc:0.900]
Epoch [23/120    avg_loss:0.270, val_acc:0.864]
Epoch [24/120    avg_loss:0.240, val_acc:0.912]
Epoch [25/120    avg_loss:0.191, val_acc:0.894]
Epoch [26/120    avg_loss:0.186, val_acc:0.892]
Epoch [27/120    avg_loss:0.168, val_acc:0.919]
Epoch [28/120    avg_loss:0.186, val_acc:0.909]
Epoch [29/120    avg_loss:0.204, val_acc:0.908]
Epoch [30/120    avg_loss:0.172, val_acc:0.863]
Epoch [31/120    avg_loss:0.192, val_acc:0.910]
Epoch [32/120    avg_loss:0.188, val_acc:0.915]
Epoch [33/120    avg_loss:0.164, val_acc:0.917]
Epoch [34/120    avg_loss:0.133, val_acc:0.928]
Epoch [35/120    avg_loss:0.145, val_acc:0.918]
Epoch [36/120    avg_loss:0.112, val_acc:0.949]
Epoch [37/120    avg_loss:0.125, val_acc:0.927]
Epoch [38/120    avg_loss:0.105, val_acc:0.939]
Epoch [39/120    avg_loss:0.116, val_acc:0.929]
Epoch [40/120    avg_loss:0.106, val_acc:0.948]
Epoch [41/120    avg_loss:0.087, val_acc:0.950]
Epoch [42/120    avg_loss:0.080, val_acc:0.951]
Epoch [43/120    avg_loss:0.058, val_acc:0.957]
Epoch [44/120    avg_loss:0.076, val_acc:0.955]
Epoch [45/120    avg_loss:0.085, val_acc:0.956]
Epoch [46/120    avg_loss:0.068, val_acc:0.962]
Epoch [47/120    avg_loss:0.056, val_acc:0.959]
Epoch [48/120    avg_loss:0.061, val_acc:0.957]
Epoch [49/120    avg_loss:0.060, val_acc:0.953]
Epoch [50/120    avg_loss:0.051, val_acc:0.966]
Epoch [51/120    avg_loss:0.057, val_acc:0.960]
Epoch [52/120    avg_loss:0.059, val_acc:0.973]
Epoch [53/120    avg_loss:0.055, val_acc:0.966]
Epoch [54/120    avg_loss:0.049, val_acc:0.968]
Epoch [55/120    avg_loss:0.043, val_acc:0.965]
Epoch [56/120    avg_loss:0.095, val_acc:0.951]
Epoch [57/120    avg_loss:0.060, val_acc:0.955]
Epoch [58/120    avg_loss:0.047, val_acc:0.960]
Epoch [59/120    avg_loss:0.045, val_acc:0.966]
Epoch [60/120    avg_loss:0.038, val_acc:0.967]
Epoch [61/120    avg_loss:0.028, val_acc:0.957]
Epoch [62/120    avg_loss:0.038, val_acc:0.964]
Epoch [63/120    avg_loss:0.035, val_acc:0.972]
Epoch [64/120    avg_loss:0.038, val_acc:0.967]
Epoch [65/120    avg_loss:0.051, val_acc:0.958]
Epoch [66/120    avg_loss:0.036, val_acc:0.967]
Epoch [67/120    avg_loss:0.023, val_acc:0.965]
Epoch [68/120    avg_loss:0.030, val_acc:0.968]
Epoch [69/120    avg_loss:0.021, val_acc:0.968]
Epoch [70/120    avg_loss:0.021, val_acc:0.967]
Epoch [71/120    avg_loss:0.021, val_acc:0.969]
Epoch [72/120    avg_loss:0.025, val_acc:0.970]
Epoch [73/120    avg_loss:0.019, val_acc:0.970]
Epoch [74/120    avg_loss:0.024, val_acc:0.974]
Epoch [75/120    avg_loss:0.018, val_acc:0.974]
Epoch [76/120    avg_loss:0.016, val_acc:0.971]
Epoch [77/120    avg_loss:0.016, val_acc:0.972]
Epoch [78/120    avg_loss:0.017, val_acc:0.971]
Epoch [79/120    avg_loss:0.020, val_acc:0.972]
Epoch [80/120    avg_loss:0.018, val_acc:0.972]
Epoch [81/120    avg_loss:0.017, val_acc:0.972]
Epoch [82/120    avg_loss:0.019, val_acc:0.973]
Epoch [83/120    avg_loss:0.015, val_acc:0.973]
Epoch [84/120    avg_loss:0.017, val_acc:0.972]
Epoch [85/120    avg_loss:0.014, val_acc:0.971]
Epoch [86/120    avg_loss:0.023, val_acc:0.973]
Epoch [87/120    avg_loss:0.017, val_acc:0.974]
Epoch [88/120    avg_loss:0.018, val_acc:0.974]
Epoch [89/120    avg_loss:0.022, val_acc:0.975]
Epoch [90/120    avg_loss:0.017, val_acc:0.972]
Epoch [91/120    avg_loss:0.017, val_acc:0.971]
Epoch [92/120    avg_loss:0.016, val_acc:0.974]
Epoch [93/120    avg_loss:0.016, val_acc:0.976]
Epoch [94/120    avg_loss:0.014, val_acc:0.976]
Epoch [95/120    avg_loss:0.017, val_acc:0.975]
Epoch [96/120    avg_loss:0.014, val_acc:0.973]
Epoch [97/120    avg_loss:0.014, val_acc:0.971]
Epoch [98/120    avg_loss:0.016, val_acc:0.976]
Epoch [99/120    avg_loss:0.016, val_acc:0.976]
Epoch [100/120    avg_loss:0.014, val_acc:0.972]
Epoch [101/120    avg_loss:0.014, val_acc:0.974]
Epoch [102/120    avg_loss:0.016, val_acc:0.973]
Epoch [103/120    avg_loss:0.014, val_acc:0.973]
Epoch [104/120    avg_loss:0.015, val_acc:0.972]
Epoch [105/120    avg_loss:0.015, val_acc:0.974]
Epoch [106/120    avg_loss:0.014, val_acc:0.975]
Epoch [107/120    avg_loss:0.019, val_acc:0.972]
Epoch [108/120    avg_loss:0.015, val_acc:0.973]
Epoch [109/120    avg_loss:0.013, val_acc:0.973]
Epoch [110/120    avg_loss:0.015, val_acc:0.972]
Epoch [111/120    avg_loss:0.013, val_acc:0.971]
Epoch [112/120    avg_loss:0.017, val_acc:0.974]
Epoch [113/120    avg_loss:0.017, val_acc:0.972]
Epoch [114/120    avg_loss:0.012, val_acc:0.972]
Epoch [115/120    avg_loss:0.016, val_acc:0.972]
Epoch [116/120    avg_loss:0.013, val_acc:0.972]
Epoch [117/120    avg_loss:0.011, val_acc:0.972]
Epoch [118/120    avg_loss:0.013, val_acc:0.972]
Epoch [119/120    avg_loss:0.015, val_acc:0.972]
Epoch [120/120    avg_loss:0.019, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1259    3    0    0    1    0    0    1    6   12    3    0
     0    0    0]
 [   0    0    2  719    1    0    0    0    0    3    5    1   13    3
     0    0    0]
 [   0    0    0    3  210    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    5    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   11    0    0    9    1    0    0    0  823   24    0    0
     0    7    0]
 [   0    0   11    0    0    1    2    0    1    0   22 2163    7    2
     0    0    1]
 [   0    0    7    6    3    8    0    0    0    0    6    4  498    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    1    2    0    0
  1127    3    0]
 [   0    0    0    0    0    0   25    0    0    1    0    0    0    0
    13  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.22493224932249

F1 scores:
[       nan 0.975      0.97786408 0.97227857 0.98360656 0.96505073
 0.97764531 0.90909091 0.99883856 0.80952381 0.94597701 0.97939778
 0.9422895  0.98666667 0.98903028 0.92631579 0.9704142 ]

Kappa:
0.9683648289663286
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb49488860>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.642, val_acc:0.452]
Epoch [2/120    avg_loss:2.259, val_acc:0.481]
Epoch [3/120    avg_loss:1.996, val_acc:0.519]
Epoch [4/120    avg_loss:1.842, val_acc:0.557]
Epoch [5/120    avg_loss:1.671, val_acc:0.561]
Epoch [6/120    avg_loss:1.525, val_acc:0.616]
Epoch [7/120    avg_loss:1.368, val_acc:0.660]
Epoch [8/120    avg_loss:1.231, val_acc:0.651]
Epoch [9/120    avg_loss:1.073, val_acc:0.691]
Epoch [10/120    avg_loss:0.964, val_acc:0.669]
Epoch [11/120    avg_loss:0.828, val_acc:0.734]
Epoch [12/120    avg_loss:0.821, val_acc:0.756]
Epoch [13/120    avg_loss:0.757, val_acc:0.759]
Epoch [14/120    avg_loss:0.641, val_acc:0.756]
Epoch [15/120    avg_loss:0.601, val_acc:0.788]
Epoch [16/120    avg_loss:0.545, val_acc:0.822]
Epoch [17/120    avg_loss:0.509, val_acc:0.827]
Epoch [18/120    avg_loss:0.418, val_acc:0.828]
Epoch [19/120    avg_loss:0.414, val_acc:0.859]
Epoch [20/120    avg_loss:0.442, val_acc:0.824]
Epoch [21/120    avg_loss:0.400, val_acc:0.841]
Epoch [22/120    avg_loss:0.410, val_acc:0.845]
Epoch [23/120    avg_loss:0.336, val_acc:0.845]
Epoch [24/120    avg_loss:0.271, val_acc:0.892]
Epoch [25/120    avg_loss:0.261, val_acc:0.900]
Epoch [26/120    avg_loss:0.260, val_acc:0.889]
Epoch [27/120    avg_loss:0.225, val_acc:0.897]
Epoch [28/120    avg_loss:0.239, val_acc:0.916]
Epoch [29/120    avg_loss:0.223, val_acc:0.915]
Epoch [30/120    avg_loss:0.169, val_acc:0.878]
Epoch [31/120    avg_loss:0.157, val_acc:0.932]
Epoch [32/120    avg_loss:0.149, val_acc:0.929]
Epoch [33/120    avg_loss:0.129, val_acc:0.932]
Epoch [34/120    avg_loss:0.117, val_acc:0.938]
Epoch [35/120    avg_loss:0.115, val_acc:0.942]
Epoch [36/120    avg_loss:0.120, val_acc:0.947]
Epoch [37/120    avg_loss:0.104, val_acc:0.950]
Epoch [38/120    avg_loss:0.091, val_acc:0.924]
Epoch [39/120    avg_loss:0.080, val_acc:0.953]
Epoch [40/120    avg_loss:0.074, val_acc:0.962]
Epoch [41/120    avg_loss:0.085, val_acc:0.958]
Epoch [42/120    avg_loss:0.079, val_acc:0.957]
Epoch [43/120    avg_loss:0.092, val_acc:0.910]
Epoch [44/120    avg_loss:0.164, val_acc:0.928]
Epoch [45/120    avg_loss:0.142, val_acc:0.933]
Epoch [46/120    avg_loss:0.112, val_acc:0.944]
Epoch [47/120    avg_loss:0.086, val_acc:0.956]
Epoch [48/120    avg_loss:0.078, val_acc:0.959]
Epoch [49/120    avg_loss:0.081, val_acc:0.957]
Epoch [50/120    avg_loss:0.055, val_acc:0.970]
Epoch [51/120    avg_loss:0.052, val_acc:0.972]
Epoch [52/120    avg_loss:0.066, val_acc:0.951]
Epoch [53/120    avg_loss:0.057, val_acc:0.957]
Epoch [54/120    avg_loss:0.059, val_acc:0.965]
Epoch [55/120    avg_loss:0.050, val_acc:0.969]
Epoch [56/120    avg_loss:0.069, val_acc:0.968]
Epoch [57/120    avg_loss:0.052, val_acc:0.967]
Epoch [58/120    avg_loss:0.052, val_acc:0.973]
Epoch [59/120    avg_loss:0.037, val_acc:0.966]
Epoch [60/120    avg_loss:0.052, val_acc:0.969]
Epoch [61/120    avg_loss:0.049, val_acc:0.959]
Epoch [62/120    avg_loss:0.041, val_acc:0.969]
Epoch [63/120    avg_loss:0.032, val_acc:0.960]
Epoch [64/120    avg_loss:0.034, val_acc:0.971]
Epoch [65/120    avg_loss:0.055, val_acc:0.970]
Epoch [66/120    avg_loss:0.047, val_acc:0.972]
Epoch [67/120    avg_loss:0.039, val_acc:0.975]
Epoch [68/120    avg_loss:0.044, val_acc:0.971]
Epoch [69/120    avg_loss:0.052, val_acc:0.962]
Epoch [70/120    avg_loss:0.035, val_acc:0.968]
Epoch [71/120    avg_loss:0.036, val_acc:0.967]
Epoch [72/120    avg_loss:0.052, val_acc:0.968]
Epoch [73/120    avg_loss:0.042, val_acc:0.975]
Epoch [74/120    avg_loss:0.042, val_acc:0.972]
Epoch [75/120    avg_loss:0.031, val_acc:0.974]
Epoch [76/120    avg_loss:0.034, val_acc:0.963]
Epoch [77/120    avg_loss:0.049, val_acc:0.968]
Epoch [78/120    avg_loss:0.065, val_acc:0.959]
Epoch [79/120    avg_loss:0.215, val_acc:0.893]
Epoch [80/120    avg_loss:0.188, val_acc:0.925]
Epoch [81/120    avg_loss:0.124, val_acc:0.953]
Epoch [82/120    avg_loss:0.065, val_acc:0.968]
Epoch [83/120    avg_loss:0.058, val_acc:0.954]
Epoch [84/120    avg_loss:0.050, val_acc:0.962]
Epoch [85/120    avg_loss:0.048, val_acc:0.967]
Epoch [86/120    avg_loss:0.057, val_acc:0.970]
Epoch [87/120    avg_loss:0.035, val_acc:0.975]
Epoch [88/120    avg_loss:0.032, val_acc:0.975]
Epoch [89/120    avg_loss:0.031, val_acc:0.976]
Epoch [90/120    avg_loss:0.023, val_acc:0.976]
Epoch [91/120    avg_loss:0.029, val_acc:0.976]
Epoch [92/120    avg_loss:0.024, val_acc:0.973]
Epoch [93/120    avg_loss:0.022, val_acc:0.978]
Epoch [94/120    avg_loss:0.024, val_acc:0.980]
Epoch [95/120    avg_loss:0.028, val_acc:0.979]
Epoch [96/120    avg_loss:0.020, val_acc:0.980]
Epoch [97/120    avg_loss:0.023, val_acc:0.980]
Epoch [98/120    avg_loss:0.024, val_acc:0.978]
Epoch [99/120    avg_loss:0.022, val_acc:0.979]
Epoch [100/120    avg_loss:0.020, val_acc:0.980]
Epoch [101/120    avg_loss:0.022, val_acc:0.975]
Epoch [102/120    avg_loss:0.021, val_acc:0.976]
Epoch [103/120    avg_loss:0.022, val_acc:0.976]
Epoch [104/120    avg_loss:0.021, val_acc:0.976]
Epoch [105/120    avg_loss:0.020, val_acc:0.978]
Epoch [106/120    avg_loss:0.023, val_acc:0.980]
Epoch [107/120    avg_loss:0.018, val_acc:0.978]
Epoch [108/120    avg_loss:0.019, val_acc:0.978]
Epoch [109/120    avg_loss:0.020, val_acc:0.979]
Epoch [110/120    avg_loss:0.023, val_acc:0.981]
Epoch [111/120    avg_loss:0.024, val_acc:0.982]
Epoch [112/120    avg_loss:0.019, val_acc:0.979]
Epoch [113/120    avg_loss:0.026, val_acc:0.979]
Epoch [114/120    avg_loss:0.020, val_acc:0.980]
Epoch [115/120    avg_loss:0.018, val_acc:0.982]
Epoch [116/120    avg_loss:0.020, val_acc:0.980]
Epoch [117/120    avg_loss:0.016, val_acc:0.976]
Epoch [118/120    avg_loss:0.018, val_acc:0.978]
Epoch [119/120    avg_loss:0.020, val_acc:0.976]
Epoch [120/120    avg_loss:0.017, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1244    4    4    0    0    0    0    1    7   24    1    0
     0    0    0]
 [   0    0    0  687    4   15    0    0    0    5    1    0   35    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    2    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    3    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    8    0    0    0    0  849    5    0    0
     1    5    0]
 [   0    0   11    1    0    1    0    0    0    0   15 2175    5    2
     0    0    0]
 [   0    0    0    0    0   14    0    0    0    0    0    0  517    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    0    0    0
  1136    1    0]
 [   0    0    0    0    0    0   10    0    0    2    0    0    0    0
    55  280    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.1490514905149

F1 scores:
[       nan 0.96202532 0.97683549 0.95482974 0.98156682 0.95248619
 0.98786039 0.96153846 1.         0.7826087  0.96973158 0.98483133
 0.94602013 0.99462366 0.97343616 0.88467615 0.97647059]

Kappa:
0.9674966968522781
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f753bd4e828>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.615, val_acc:0.454]
Epoch [2/120    avg_loss:2.241, val_acc:0.435]
Epoch [3/120    avg_loss:1.999, val_acc:0.534]
Epoch [4/120    avg_loss:1.837, val_acc:0.562]
Epoch [5/120    avg_loss:1.689, val_acc:0.599]
Epoch [6/120    avg_loss:1.589, val_acc:0.577]
Epoch [7/120    avg_loss:1.396, val_acc:0.611]
Epoch [8/120    avg_loss:1.281, val_acc:0.650]
Epoch [9/120    avg_loss:1.149, val_acc:0.696]
Epoch [10/120    avg_loss:1.081, val_acc:0.650]
Epoch [11/120    avg_loss:0.907, val_acc:0.704]
Epoch [12/120    avg_loss:0.789, val_acc:0.774]
Epoch [13/120    avg_loss:0.721, val_acc:0.728]
Epoch [14/120    avg_loss:0.811, val_acc:0.747]
Epoch [15/120    avg_loss:0.722, val_acc:0.813]
Epoch [16/120    avg_loss:0.592, val_acc:0.821]
Epoch [17/120    avg_loss:0.532, val_acc:0.793]
Epoch [18/120    avg_loss:0.409, val_acc:0.872]
Epoch [19/120    avg_loss:0.389, val_acc:0.839]
Epoch [20/120    avg_loss:0.381, val_acc:0.841]
Epoch [21/120    avg_loss:0.390, val_acc:0.872]
Epoch [22/120    avg_loss:0.355, val_acc:0.873]
Epoch [23/120    avg_loss:0.309, val_acc:0.868]
Epoch [24/120    avg_loss:0.307, val_acc:0.897]
Epoch [25/120    avg_loss:0.248, val_acc:0.901]
Epoch [26/120    avg_loss:0.233, val_acc:0.892]
Epoch [27/120    avg_loss:0.227, val_acc:0.906]
Epoch [28/120    avg_loss:0.216, val_acc:0.912]
Epoch [29/120    avg_loss:0.176, val_acc:0.919]
Epoch [30/120    avg_loss:0.180, val_acc:0.922]
Epoch [31/120    avg_loss:0.192, val_acc:0.916]
Epoch [32/120    avg_loss:0.171, val_acc:0.917]
Epoch [33/120    avg_loss:0.187, val_acc:0.928]
Epoch [34/120    avg_loss:0.144, val_acc:0.935]
Epoch [35/120    avg_loss:0.153, val_acc:0.919]
Epoch [36/120    avg_loss:0.151, val_acc:0.906]
Epoch [37/120    avg_loss:0.140, val_acc:0.927]
Epoch [38/120    avg_loss:0.118, val_acc:0.937]
Epoch [39/120    avg_loss:0.110, val_acc:0.950]
Epoch [40/120    avg_loss:0.106, val_acc:0.940]
Epoch [41/120    avg_loss:0.092, val_acc:0.943]
Epoch [42/120    avg_loss:0.121, val_acc:0.938]
Epoch [43/120    avg_loss:0.097, val_acc:0.941]
Epoch [44/120    avg_loss:0.113, val_acc:0.948]
Epoch [45/120    avg_loss:0.085, val_acc:0.929]
Epoch [46/120    avg_loss:0.081, val_acc:0.941]
Epoch [47/120    avg_loss:0.117, val_acc:0.951]
Epoch [48/120    avg_loss:0.088, val_acc:0.946]
Epoch [49/120    avg_loss:0.104, val_acc:0.944]
Epoch [50/120    avg_loss:0.127, val_acc:0.942]
Epoch [51/120    avg_loss:0.101, val_acc:0.949]
Epoch [52/120    avg_loss:0.071, val_acc:0.940]
Epoch [53/120    avg_loss:0.069, val_acc:0.954]
Epoch [54/120    avg_loss:0.117, val_acc:0.933]
Epoch [55/120    avg_loss:0.082, val_acc:0.950]
Epoch [56/120    avg_loss:0.075, val_acc:0.958]
Epoch [57/120    avg_loss:0.060, val_acc:0.955]
Epoch [58/120    avg_loss:0.062, val_acc:0.947]
Epoch [59/120    avg_loss:0.066, val_acc:0.954]
Epoch [60/120    avg_loss:0.057, val_acc:0.966]
Epoch [61/120    avg_loss:0.048, val_acc:0.954]
Epoch [62/120    avg_loss:0.052, val_acc:0.963]
Epoch [63/120    avg_loss:0.040, val_acc:0.958]
Epoch [64/120    avg_loss:0.049, val_acc:0.963]
Epoch [65/120    avg_loss:0.032, val_acc:0.958]
Epoch [66/120    avg_loss:0.036, val_acc:0.960]
Epoch [67/120    avg_loss:0.028, val_acc:0.959]
Epoch [68/120    avg_loss:0.041, val_acc:0.973]
Epoch [69/120    avg_loss:0.032, val_acc:0.957]
Epoch [70/120    avg_loss:0.044, val_acc:0.968]
Epoch [71/120    avg_loss:0.032, val_acc:0.967]
Epoch [72/120    avg_loss:0.031, val_acc:0.964]
Epoch [73/120    avg_loss:0.036, val_acc:0.954]
Epoch [74/120    avg_loss:0.030, val_acc:0.958]
Epoch [75/120    avg_loss:0.025, val_acc:0.962]
Epoch [76/120    avg_loss:0.025, val_acc:0.968]
Epoch [77/120    avg_loss:0.031, val_acc:0.960]
Epoch [78/120    avg_loss:0.043, val_acc:0.959]
Epoch [79/120    avg_loss:0.027, val_acc:0.964]
Epoch [80/120    avg_loss:0.025, val_acc:0.967]
Epoch [81/120    avg_loss:0.037, val_acc:0.955]
Epoch [82/120    avg_loss:0.029, val_acc:0.968]
Epoch [83/120    avg_loss:0.016, val_acc:0.972]
Epoch [84/120    avg_loss:0.017, val_acc:0.973]
Epoch [85/120    avg_loss:0.019, val_acc:0.973]
Epoch [86/120    avg_loss:0.014, val_acc:0.973]
Epoch [87/120    avg_loss:0.014, val_acc:0.973]
Epoch [88/120    avg_loss:0.017, val_acc:0.974]
Epoch [89/120    avg_loss:0.016, val_acc:0.971]
Epoch [90/120    avg_loss:0.013, val_acc:0.974]
Epoch [91/120    avg_loss:0.013, val_acc:0.971]
Epoch [92/120    avg_loss:0.015, val_acc:0.973]
Epoch [93/120    avg_loss:0.014, val_acc:0.974]
Epoch [94/120    avg_loss:0.018, val_acc:0.970]
Epoch [95/120    avg_loss:0.014, val_acc:0.972]
Epoch [96/120    avg_loss:0.014, val_acc:0.971]
Epoch [97/120    avg_loss:0.022, val_acc:0.972]
Epoch [98/120    avg_loss:0.014, val_acc:0.974]
Epoch [99/120    avg_loss:0.013, val_acc:0.976]
Epoch [100/120    avg_loss:0.017, val_acc:0.974]
Epoch [101/120    avg_loss:0.014, val_acc:0.975]
Epoch [102/120    avg_loss:0.012, val_acc:0.973]
Epoch [103/120    avg_loss:0.013, val_acc:0.973]
Epoch [104/120    avg_loss:0.012, val_acc:0.974]
Epoch [105/120    avg_loss:0.014, val_acc:0.978]
Epoch [106/120    avg_loss:0.014, val_acc:0.974]
Epoch [107/120    avg_loss:0.014, val_acc:0.973]
Epoch [108/120    avg_loss:0.013, val_acc:0.973]
Epoch [109/120    avg_loss:0.010, val_acc:0.971]
Epoch [110/120    avg_loss:0.014, val_acc:0.970]
Epoch [111/120    avg_loss:0.013, val_acc:0.971]
Epoch [112/120    avg_loss:0.012, val_acc:0.973]
Epoch [113/120    avg_loss:0.010, val_acc:0.973]
Epoch [114/120    avg_loss:0.014, val_acc:0.971]
Epoch [115/120    avg_loss:0.011, val_acc:0.972]
Epoch [116/120    avg_loss:0.012, val_acc:0.973]
Epoch [117/120    avg_loss:0.010, val_acc:0.973]
Epoch [118/120    avg_loss:0.011, val_acc:0.974]
Epoch [119/120    avg_loss:0.010, val_acc:0.974]
Epoch [120/120    avg_loss:0.014, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1259    0    3    0    0    0    0    0    8   12    3    0
     0    0    0]
 [   0    0    4  705    0    1    0    0    0    8    4    3   21    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    6    0    0    0    1  854   10    0    0
     0    0    0]
 [   0    0    8    0    0    0    1    0    0    0    9 2161    2   29
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0   11    7  514    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    0    0    0
  1125   11    0]
 [   0    0    0    0    0    1   45    0    0    3    0    0    0    0
    20  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.289972899729

F1 scores:
[       nan 0.975      0.98359375 0.97107438 0.99300699 0.98858447
 0.96541575 1.         0.99650757 0.72       0.96770538 0.98138056
 0.95361781 0.925      0.98468271 0.87421384 0.98203593]

Kappa:
0.9691178151902399
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd065d77710>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.667, val_acc:0.356]
Epoch [2/120    avg_loss:2.262, val_acc:0.435]
Epoch [3/120    avg_loss:2.040, val_acc:0.521]
Epoch [4/120    avg_loss:1.870, val_acc:0.560]
Epoch [5/120    avg_loss:1.722, val_acc:0.589]
Epoch [6/120    avg_loss:1.610, val_acc:0.616]
Epoch [7/120    avg_loss:1.461, val_acc:0.643]
Epoch [8/120    avg_loss:1.217, val_acc:0.693]
Epoch [9/120    avg_loss:1.153, val_acc:0.696]
Epoch [10/120    avg_loss:1.030, val_acc:0.721]
Epoch [11/120    avg_loss:0.865, val_acc:0.747]
Epoch [12/120    avg_loss:0.802, val_acc:0.683]
Epoch [13/120    avg_loss:0.830, val_acc:0.737]
Epoch [14/120    avg_loss:0.762, val_acc:0.783]
Epoch [15/120    avg_loss:0.644, val_acc:0.803]
Epoch [16/120    avg_loss:0.524, val_acc:0.837]
Epoch [17/120    avg_loss:0.454, val_acc:0.814]
Epoch [18/120    avg_loss:0.465, val_acc:0.863]
Epoch [19/120    avg_loss:0.465, val_acc:0.864]
Epoch [20/120    avg_loss:0.384, val_acc:0.874]
Epoch [21/120    avg_loss:0.363, val_acc:0.856]
Epoch [22/120    avg_loss:0.483, val_acc:0.861]
Epoch [23/120    avg_loss:0.295, val_acc:0.901]
Epoch [24/120    avg_loss:0.262, val_acc:0.887]
Epoch [25/120    avg_loss:0.339, val_acc:0.863]
Epoch [26/120    avg_loss:0.429, val_acc:0.843]
Epoch [27/120    avg_loss:0.293, val_acc:0.888]
Epoch [28/120    avg_loss:0.223, val_acc:0.901]
Epoch [29/120    avg_loss:0.312, val_acc:0.855]
Epoch [30/120    avg_loss:0.238, val_acc:0.909]
Epoch [31/120    avg_loss:0.201, val_acc:0.894]
Epoch [32/120    avg_loss:0.177, val_acc:0.904]
Epoch [33/120    avg_loss:0.149, val_acc:0.926]
Epoch [34/120    avg_loss:0.142, val_acc:0.940]
Epoch [35/120    avg_loss:0.112, val_acc:0.937]
Epoch [36/120    avg_loss:0.148, val_acc:0.932]
Epoch [37/120    avg_loss:0.157, val_acc:0.920]
Epoch [38/120    avg_loss:0.112, val_acc:0.940]
Epoch [39/120    avg_loss:0.117, val_acc:0.941]
Epoch [40/120    avg_loss:0.107, val_acc:0.936]
Epoch [41/120    avg_loss:0.107, val_acc:0.938]
Epoch [42/120    avg_loss:0.082, val_acc:0.949]
Epoch [43/120    avg_loss:0.071, val_acc:0.939]
Epoch [44/120    avg_loss:0.073, val_acc:0.937]
Epoch [45/120    avg_loss:0.075, val_acc:0.935]
Epoch [46/120    avg_loss:0.080, val_acc:0.948]
Epoch [47/120    avg_loss:0.095, val_acc:0.951]
Epoch [48/120    avg_loss:0.082, val_acc:0.955]
Epoch [49/120    avg_loss:0.064, val_acc:0.953]
Epoch [50/120    avg_loss:0.054, val_acc:0.948]
Epoch [51/120    avg_loss:0.061, val_acc:0.940]
Epoch [52/120    avg_loss:0.057, val_acc:0.950]
Epoch [53/120    avg_loss:0.051, val_acc:0.952]
Epoch [54/120    avg_loss:0.050, val_acc:0.956]
Epoch [55/120    avg_loss:0.044, val_acc:0.956]
Epoch [56/120    avg_loss:0.054, val_acc:0.940]
Epoch [57/120    avg_loss:0.056, val_acc:0.958]
Epoch [58/120    avg_loss:0.042, val_acc:0.962]
Epoch [59/120    avg_loss:0.036, val_acc:0.951]
Epoch [60/120    avg_loss:0.055, val_acc:0.936]
Epoch [61/120    avg_loss:0.053, val_acc:0.958]
Epoch [62/120    avg_loss:0.041, val_acc:0.959]
Epoch [63/120    avg_loss:0.052, val_acc:0.964]
Epoch [64/120    avg_loss:0.041, val_acc:0.939]
Epoch [65/120    avg_loss:0.044, val_acc:0.954]
Epoch [66/120    avg_loss:0.059, val_acc:0.956]
Epoch [67/120    avg_loss:0.040, val_acc:0.955]
Epoch [68/120    avg_loss:0.035, val_acc:0.962]
Epoch [69/120    avg_loss:0.032, val_acc:0.968]
Epoch [70/120    avg_loss:0.028, val_acc:0.959]
Epoch [71/120    avg_loss:0.054, val_acc:0.960]
Epoch [72/120    avg_loss:0.043, val_acc:0.963]
Epoch [73/120    avg_loss:0.031, val_acc:0.960]
Epoch [74/120    avg_loss:0.039, val_acc:0.973]
Epoch [75/120    avg_loss:0.027, val_acc:0.963]
Epoch [76/120    avg_loss:0.031, val_acc:0.960]
Epoch [77/120    avg_loss:0.028, val_acc:0.970]
Epoch [78/120    avg_loss:0.028, val_acc:0.967]
Epoch [79/120    avg_loss:0.028, val_acc:0.971]
Epoch [80/120    avg_loss:0.033, val_acc:0.973]
Epoch [81/120    avg_loss:0.020, val_acc:0.968]
Epoch [82/120    avg_loss:0.025, val_acc:0.972]
Epoch [83/120    avg_loss:0.030, val_acc:0.974]
Epoch [84/120    avg_loss:0.034, val_acc:0.968]
Epoch [85/120    avg_loss:0.024, val_acc:0.970]
Epoch [86/120    avg_loss:0.021, val_acc:0.966]
Epoch [87/120    avg_loss:0.055, val_acc:0.944]
Epoch [88/120    avg_loss:0.052, val_acc:0.879]
Epoch [89/120    avg_loss:0.119, val_acc:0.954]
Epoch [90/120    avg_loss:0.044, val_acc:0.972]
Epoch [91/120    avg_loss:0.045, val_acc:0.964]
Epoch [92/120    avg_loss:0.068, val_acc:0.937]
Epoch [93/120    avg_loss:0.069, val_acc:0.957]
Epoch [94/120    avg_loss:0.035, val_acc:0.965]
Epoch [95/120    avg_loss:0.040, val_acc:0.967]
Epoch [96/120    avg_loss:0.037, val_acc:0.972]
Epoch [97/120    avg_loss:0.031, val_acc:0.978]
Epoch [98/120    avg_loss:0.020, val_acc:0.978]
Epoch [99/120    avg_loss:0.023, val_acc:0.975]
Epoch [100/120    avg_loss:0.021, val_acc:0.973]
Epoch [101/120    avg_loss:0.016, val_acc:0.976]
Epoch [102/120    avg_loss:0.024, val_acc:0.975]
Epoch [103/120    avg_loss:0.017, val_acc:0.976]
Epoch [104/120    avg_loss:0.015, val_acc:0.975]
Epoch [105/120    avg_loss:0.017, val_acc:0.976]
Epoch [106/120    avg_loss:0.017, val_acc:0.979]
Epoch [107/120    avg_loss:0.016, val_acc:0.975]
Epoch [108/120    avg_loss:0.015, val_acc:0.975]
Epoch [109/120    avg_loss:0.016, val_acc:0.975]
Epoch [110/120    avg_loss:0.018, val_acc:0.976]
Epoch [111/120    avg_loss:0.015, val_acc:0.976]
Epoch [112/120    avg_loss:0.014, val_acc:0.975]
Epoch [113/120    avg_loss:0.014, val_acc:0.978]
Epoch [114/120    avg_loss:0.014, val_acc:0.978]
Epoch [115/120    avg_loss:0.012, val_acc:0.975]
Epoch [116/120    avg_loss:0.016, val_acc:0.976]
Epoch [117/120    avg_loss:0.012, val_acc:0.978]
Epoch [118/120    avg_loss:0.012, val_acc:0.978]
Epoch [119/120    avg_loss:0.014, val_acc:0.975]
Epoch [120/120    avg_loss:0.012, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1255    0    3    0    1    0    0    2    6   15    3    0
     0    0    0]
 [   0    0    1  717    1    8    0    0    0   13    1    0    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    4    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    3    1    0    0    0  852    6    0    0
     0    4    0]
 [   0    1   13    0    0    0    0    2    0    0   13 2175    4    2
     0    0    0]
 [   0    0    3    0    0    1    0    0    0    0    0   13  512    0
     0    2    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1136    2    0]
 [   0    0    0    0    0    0   36    0    0    3    0    0    0    0
     8  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
97.66937669376694

F1 scores:
[       nan 0.92307692 0.97817615 0.9795082  0.99069767 0.98061574
 0.96731055 0.96153846 0.99649942 0.64285714 0.97204792 0.98349537
 0.9588015  0.99462366 0.99300699 0.91603053 0.94545455]

Kappa:
0.9734275119339562
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0cd0357898>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.651, val_acc:0.469]
Epoch [2/120    avg_loss:2.218, val_acc:0.493]
Epoch [3/120    avg_loss:2.023, val_acc:0.556]
Epoch [4/120    avg_loss:1.835, val_acc:0.627]
Epoch [5/120    avg_loss:1.707, val_acc:0.668]
Epoch [6/120    avg_loss:1.524, val_acc:0.654]
Epoch [7/120    avg_loss:1.364, val_acc:0.689]
Epoch [8/120    avg_loss:1.218, val_acc:0.704]
Epoch [9/120    avg_loss:1.009, val_acc:0.765]
Epoch [10/120    avg_loss:0.964, val_acc:0.764]
Epoch [11/120    avg_loss:0.796, val_acc:0.746]
Epoch [12/120    avg_loss:0.761, val_acc:0.769]
Epoch [13/120    avg_loss:0.689, val_acc:0.825]
Epoch [14/120    avg_loss:0.662, val_acc:0.838]
Epoch [15/120    avg_loss:0.604, val_acc:0.812]
Epoch [16/120    avg_loss:0.542, val_acc:0.829]
Epoch [17/120    avg_loss:0.447, val_acc:0.869]
Epoch [18/120    avg_loss:0.396, val_acc:0.871]
Epoch [19/120    avg_loss:0.465, val_acc:0.868]
Epoch [20/120    avg_loss:0.388, val_acc:0.827]
Epoch [21/120    avg_loss:0.525, val_acc:0.800]
Epoch [22/120    avg_loss:0.496, val_acc:0.834]
Epoch [23/120    avg_loss:0.427, val_acc:0.824]
Epoch [24/120    avg_loss:0.395, val_acc:0.856]
Epoch [25/120    avg_loss:0.366, val_acc:0.865]
Epoch [26/120    avg_loss:0.282, val_acc:0.863]
Epoch [27/120    avg_loss:0.265, val_acc:0.887]
Epoch [28/120    avg_loss:0.230, val_acc:0.901]
Epoch [29/120    avg_loss:0.244, val_acc:0.890]
Epoch [30/120    avg_loss:0.200, val_acc:0.929]
Epoch [31/120    avg_loss:0.226, val_acc:0.920]
Epoch [32/120    avg_loss:0.212, val_acc:0.925]
Epoch [33/120    avg_loss:0.157, val_acc:0.933]
Epoch [34/120    avg_loss:0.168, val_acc:0.943]
Epoch [35/120    avg_loss:0.141, val_acc:0.928]
Epoch [36/120    avg_loss:0.125, val_acc:0.929]
Epoch [37/120    avg_loss:0.138, val_acc:0.947]
Epoch [38/120    avg_loss:0.103, val_acc:0.948]
Epoch [39/120    avg_loss:0.117, val_acc:0.935]
Epoch [40/120    avg_loss:0.119, val_acc:0.943]
Epoch [41/120    avg_loss:0.114, val_acc:0.948]
Epoch [42/120    avg_loss:0.092, val_acc:0.950]
Epoch [43/120    avg_loss:0.092, val_acc:0.949]
Epoch [44/120    avg_loss:0.084, val_acc:0.951]
Epoch [45/120    avg_loss:0.078, val_acc:0.954]
Epoch [46/120    avg_loss:0.091, val_acc:0.957]
Epoch [47/120    avg_loss:0.079, val_acc:0.954]
Epoch [48/120    avg_loss:0.074, val_acc:0.965]
Epoch [49/120    avg_loss:0.084, val_acc:0.958]
Epoch [50/120    avg_loss:0.073, val_acc:0.959]
Epoch [51/120    avg_loss:0.116, val_acc:0.938]
Epoch [52/120    avg_loss:0.102, val_acc:0.956]
Epoch [53/120    avg_loss:0.065, val_acc:0.965]
Epoch [54/120    avg_loss:0.061, val_acc:0.963]
Epoch [55/120    avg_loss:0.060, val_acc:0.970]
Epoch [56/120    avg_loss:0.065, val_acc:0.967]
Epoch [57/120    avg_loss:0.052, val_acc:0.959]
Epoch [58/120    avg_loss:0.051, val_acc:0.970]
Epoch [59/120    avg_loss:0.052, val_acc:0.960]
Epoch [60/120    avg_loss:0.077, val_acc:0.963]
Epoch [61/120    avg_loss:0.058, val_acc:0.957]
Epoch [62/120    avg_loss:0.132, val_acc:0.948]
Epoch [63/120    avg_loss:0.073, val_acc:0.958]
Epoch [64/120    avg_loss:0.123, val_acc:0.949]
Epoch [65/120    avg_loss:0.068, val_acc:0.967]
Epoch [66/120    avg_loss:0.045, val_acc:0.968]
Epoch [67/120    avg_loss:0.049, val_acc:0.972]
Epoch [68/120    avg_loss:0.065, val_acc:0.948]
Epoch [69/120    avg_loss:0.072, val_acc:0.966]
Epoch [70/120    avg_loss:0.046, val_acc:0.971]
Epoch [71/120    avg_loss:0.030, val_acc:0.966]
Epoch [72/120    avg_loss:0.029, val_acc:0.971]
Epoch [73/120    avg_loss:0.032, val_acc:0.973]
Epoch [74/120    avg_loss:0.039, val_acc:0.970]
Epoch [75/120    avg_loss:0.025, val_acc:0.976]
Epoch [76/120    avg_loss:0.031, val_acc:0.971]
Epoch [77/120    avg_loss:0.021, val_acc:0.973]
Epoch [78/120    avg_loss:0.024, val_acc:0.973]
Epoch [79/120    avg_loss:0.017, val_acc:0.979]
Epoch [80/120    avg_loss:0.028, val_acc:0.972]
Epoch [81/120    avg_loss:0.022, val_acc:0.976]
Epoch [82/120    avg_loss:0.019, val_acc:0.973]
Epoch [83/120    avg_loss:0.021, val_acc:0.973]
Epoch [84/120    avg_loss:0.028, val_acc:0.975]
Epoch [85/120    avg_loss:0.037, val_acc:0.969]
Epoch [86/120    avg_loss:0.029, val_acc:0.978]
Epoch [87/120    avg_loss:0.023, val_acc:0.978]
Epoch [88/120    avg_loss:0.026, val_acc:0.973]
Epoch [89/120    avg_loss:0.022, val_acc:0.978]
Epoch [90/120    avg_loss:0.021, val_acc:0.979]
Epoch [91/120    avg_loss:0.023, val_acc:0.976]
Epoch [92/120    avg_loss:0.022, val_acc:0.979]
Epoch [93/120    avg_loss:0.026, val_acc:0.978]
Epoch [94/120    avg_loss:0.024, val_acc:0.976]
Epoch [95/120    avg_loss:0.022, val_acc:0.973]
Epoch [96/120    avg_loss:0.022, val_acc:0.975]
Epoch [97/120    avg_loss:0.025, val_acc:0.973]
Epoch [98/120    avg_loss:0.028, val_acc:0.972]
Epoch [99/120    avg_loss:0.026, val_acc:0.974]
Epoch [100/120    avg_loss:0.162, val_acc:0.951]
Epoch [101/120    avg_loss:0.061, val_acc:0.969]
Epoch [102/120    avg_loss:0.034, val_acc:0.967]
Epoch [103/120    avg_loss:0.028, val_acc:0.968]
Epoch [104/120    avg_loss:0.022, val_acc:0.973]
Epoch [105/120    avg_loss:0.016, val_acc:0.980]
Epoch [106/120    avg_loss:0.018, val_acc:0.970]
Epoch [107/120    avg_loss:0.016, val_acc:0.980]
Epoch [108/120    avg_loss:0.012, val_acc:0.980]
Epoch [109/120    avg_loss:0.023, val_acc:0.974]
Epoch [110/120    avg_loss:0.031, val_acc:0.974]
Epoch [111/120    avg_loss:0.019, val_acc:0.974]
Epoch [112/120    avg_loss:0.016, val_acc:0.978]
Epoch [113/120    avg_loss:0.018, val_acc:0.973]
Epoch [114/120    avg_loss:0.027, val_acc:0.978]
Epoch [115/120    avg_loss:0.016, val_acc:0.981]
Epoch [116/120    avg_loss:0.014, val_acc:0.983]
Epoch [117/120    avg_loss:0.021, val_acc:0.975]
Epoch [118/120    avg_loss:0.016, val_acc:0.976]
Epoch [119/120    avg_loss:0.013, val_acc:0.980]
Epoch [120/120    avg_loss:0.012, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1257    2    0    0    0    0    0    4    9   11    2    0
     0    0    0]
 [   0    0    0  685    0   16    0    0    0    7   12    0   20    4
     3    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   16    0    0    5    0    0    0    0  835   12    0    0
     0    7    0]
 [   0    0   13    0    0    0    2    0    0    2    7 2186    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    8    0  522    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1136    1    0]
 [   0    0    0    0    0    1    0    0    0    3    0    0    0    0
    21  322    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.7560975609756

F1 scores:
[       nan 0.92682927 0.97782964 0.9553696  1.         0.97187852
 0.99771863 1.         0.995338   0.62962963 0.95428571 0.98914027
 0.96577243 0.98930481 0.98782609 0.95125554 0.9704142 ]

Kappa:
0.9744202931068779
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff754532828>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.684, val_acc:0.441]
Epoch [2/120    avg_loss:2.278, val_acc:0.549]
Epoch [3/120    avg_loss:2.039, val_acc:0.533]
Epoch [4/120    avg_loss:1.862, val_acc:0.572]
Epoch [5/120    avg_loss:1.732, val_acc:0.628]
Epoch [6/120    avg_loss:1.577, val_acc:0.642]
Epoch [7/120    avg_loss:1.383, val_acc:0.688]
Epoch [8/120    avg_loss:1.195, val_acc:0.703]
Epoch [9/120    avg_loss:1.096, val_acc:0.710]
Epoch [10/120    avg_loss:0.974, val_acc:0.755]
Epoch [11/120    avg_loss:0.896, val_acc:0.747]
Epoch [12/120    avg_loss:0.884, val_acc:0.745]
Epoch [13/120    avg_loss:0.807, val_acc:0.762]
Epoch [14/120    avg_loss:0.671, val_acc:0.752]
Epoch [15/120    avg_loss:0.593, val_acc:0.813]
Epoch [16/120    avg_loss:0.511, val_acc:0.826]
Epoch [17/120    avg_loss:0.455, val_acc:0.868]
Epoch [18/120    avg_loss:0.427, val_acc:0.847]
Epoch [19/120    avg_loss:0.436, val_acc:0.855]
Epoch [20/120    avg_loss:0.372, val_acc:0.865]
Epoch [21/120    avg_loss:0.305, val_acc:0.864]
Epoch [22/120    avg_loss:0.286, val_acc:0.893]
Epoch [23/120    avg_loss:0.310, val_acc:0.903]
Epoch [24/120    avg_loss:0.273, val_acc:0.891]
Epoch [25/120    avg_loss:0.270, val_acc:0.863]
Epoch [26/120    avg_loss:0.278, val_acc:0.933]
Epoch [27/120    avg_loss:0.210, val_acc:0.925]
Epoch [28/120    avg_loss:0.220, val_acc:0.896]
Epoch [29/120    avg_loss:0.195, val_acc:0.925]
Epoch [30/120    avg_loss:0.192, val_acc:0.924]
Epoch [31/120    avg_loss:0.162, val_acc:0.943]
Epoch [32/120    avg_loss:0.134, val_acc:0.916]
Epoch [33/120    avg_loss:0.139, val_acc:0.937]
Epoch [34/120    avg_loss:0.107, val_acc:0.940]
Epoch [35/120    avg_loss:0.100, val_acc:0.947]
Epoch [36/120    avg_loss:0.105, val_acc:0.936]
Epoch [37/120    avg_loss:0.136, val_acc:0.906]
Epoch [38/120    avg_loss:0.115, val_acc:0.928]
Epoch [39/120    avg_loss:0.101, val_acc:0.946]
Epoch [40/120    avg_loss:0.095, val_acc:0.926]
Epoch [41/120    avg_loss:0.108, val_acc:0.942]
Epoch [42/120    avg_loss:0.113, val_acc:0.948]
Epoch [43/120    avg_loss:0.082, val_acc:0.947]
Epoch [44/120    avg_loss:0.116, val_acc:0.946]
Epoch [45/120    avg_loss:0.076, val_acc:0.944]
Epoch [46/120    avg_loss:0.099, val_acc:0.928]
Epoch [47/120    avg_loss:0.115, val_acc:0.908]
Epoch [48/120    avg_loss:0.113, val_acc:0.933]
Epoch [49/120    avg_loss:0.095, val_acc:0.951]
Epoch [50/120    avg_loss:0.082, val_acc:0.942]
Epoch [51/120    avg_loss:0.076, val_acc:0.919]
Epoch [52/120    avg_loss:0.081, val_acc:0.949]
Epoch [53/120    avg_loss:0.065, val_acc:0.946]
Epoch [54/120    avg_loss:0.069, val_acc:0.955]
Epoch [55/120    avg_loss:0.075, val_acc:0.939]
Epoch [56/120    avg_loss:0.056, val_acc:0.962]
Epoch [57/120    avg_loss:0.043, val_acc:0.965]
Epoch [58/120    avg_loss:0.057, val_acc:0.958]
Epoch [59/120    avg_loss:0.055, val_acc:0.963]
Epoch [60/120    avg_loss:0.044, val_acc:0.973]
Epoch [61/120    avg_loss:0.056, val_acc:0.958]
Epoch [62/120    avg_loss:0.053, val_acc:0.963]
Epoch [63/120    avg_loss:0.040, val_acc:0.954]
Epoch [64/120    avg_loss:0.050, val_acc:0.964]
Epoch [65/120    avg_loss:0.043, val_acc:0.967]
Epoch [66/120    avg_loss:0.053, val_acc:0.964]
Epoch [67/120    avg_loss:0.041, val_acc:0.949]
Epoch [68/120    avg_loss:0.048, val_acc:0.956]
Epoch [69/120    avg_loss:0.033, val_acc:0.969]
Epoch [70/120    avg_loss:0.054, val_acc:0.968]
Epoch [71/120    avg_loss:0.036, val_acc:0.964]
Epoch [72/120    avg_loss:0.032, val_acc:0.966]
Epoch [73/120    avg_loss:0.024, val_acc:0.969]
Epoch [74/120    avg_loss:0.024, val_acc:0.970]
Epoch [75/120    avg_loss:0.017, val_acc:0.969]
Epoch [76/120    avg_loss:0.026, val_acc:0.972]
Epoch [77/120    avg_loss:0.017, val_acc:0.971]
Epoch [78/120    avg_loss:0.015, val_acc:0.973]
Epoch [79/120    avg_loss:0.016, val_acc:0.971]
Epoch [80/120    avg_loss:0.020, val_acc:0.969]
Epoch [81/120    avg_loss:0.016, val_acc:0.970]
Epoch [82/120    avg_loss:0.015, val_acc:0.971]
Epoch [83/120    avg_loss:0.014, val_acc:0.972]
Epoch [84/120    avg_loss:0.017, val_acc:0.972]
Epoch [85/120    avg_loss:0.017, val_acc:0.970]
Epoch [86/120    avg_loss:0.020, val_acc:0.969]
Epoch [87/120    avg_loss:0.016, val_acc:0.968]
Epoch [88/120    avg_loss:0.016, val_acc:0.971]
Epoch [89/120    avg_loss:0.016, val_acc:0.971]
Epoch [90/120    avg_loss:0.018, val_acc:0.972]
Epoch [91/120    avg_loss:0.019, val_acc:0.972]
Epoch [92/120    avg_loss:0.017, val_acc:0.973]
Epoch [93/120    avg_loss:0.024, val_acc:0.971]
Epoch [94/120    avg_loss:0.019, val_acc:0.972]
Epoch [95/120    avg_loss:0.017, val_acc:0.971]
Epoch [96/120    avg_loss:0.018, val_acc:0.972]
Epoch [97/120    avg_loss:0.015, val_acc:0.973]
Epoch [98/120    avg_loss:0.019, val_acc:0.972]
Epoch [99/120    avg_loss:0.014, val_acc:0.973]
Epoch [100/120    avg_loss:0.014, val_acc:0.973]
Epoch [101/120    avg_loss:0.017, val_acc:0.973]
Epoch [102/120    avg_loss:0.014, val_acc:0.973]
Epoch [103/120    avg_loss:0.014, val_acc:0.973]
Epoch [104/120    avg_loss:0.014, val_acc:0.973]
Epoch [105/120    avg_loss:0.015, val_acc:0.973]
Epoch [106/120    avg_loss:0.016, val_acc:0.973]
Epoch [107/120    avg_loss:0.016, val_acc:0.973]
Epoch [108/120    avg_loss:0.016, val_acc:0.973]
Epoch [109/120    avg_loss:0.015, val_acc:0.973]
Epoch [110/120    avg_loss:0.015, val_acc:0.973]
Epoch [111/120    avg_loss:0.016, val_acc:0.973]
Epoch [112/120    avg_loss:0.015, val_acc:0.973]
Epoch [113/120    avg_loss:0.017, val_acc:0.973]
Epoch [114/120    avg_loss:0.014, val_acc:0.972]
Epoch [115/120    avg_loss:0.016, val_acc:0.971]
Epoch [116/120    avg_loss:0.015, val_acc:0.971]
Epoch [117/120    avg_loss:0.013, val_acc:0.971]
Epoch [118/120    avg_loss:0.013, val_acc:0.971]
Epoch [119/120    avg_loss:0.017, val_acc:0.971]
Epoch [120/120    avg_loss:0.015, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1233    5    0    0    1    0    0    0   14   32    0    0
     0    0    0]
 [   0    0    0  716    0    0    0    0    0    8    2    0   21    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  649    0    0    0    0    3    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    3    0    0    7    0    0    0    0  844   17    0    0
     0    4    0]
 [   0    0    3    0    0    0    2    0    0    0   11 2193    0    1
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    6    7  512    0
     1    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
    22  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.7560975609756

F1 scores:
[       nan 0.98765432 0.9770206  0.97547684 1.         0.98070375
 0.97888386 1.         1.         0.73913043 0.96237172 0.98274703
 0.95970009 0.99730458 0.98697917 0.93333333 0.99408284]

Kappa:
0.974400204393503
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efb5c66f828>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.667, val_acc:0.431]
Epoch [2/120    avg_loss:2.230, val_acc:0.471]
Epoch [3/120    avg_loss:2.008, val_acc:0.555]
Epoch [4/120    avg_loss:1.819, val_acc:0.587]
Epoch [5/120    avg_loss:1.614, val_acc:0.617]
Epoch [6/120    avg_loss:1.441, val_acc:0.625]
Epoch [7/120    avg_loss:1.324, val_acc:0.626]
Epoch [8/120    avg_loss:1.178, val_acc:0.680]
Epoch [9/120    avg_loss:0.978, val_acc:0.688]
Epoch [10/120    avg_loss:0.892, val_acc:0.724]
Epoch [11/120    avg_loss:0.810, val_acc:0.741]
Epoch [12/120    avg_loss:0.647, val_acc:0.795]
Epoch [13/120    avg_loss:0.637, val_acc:0.797]
Epoch [14/120    avg_loss:0.559, val_acc:0.821]
Epoch [15/120    avg_loss:0.493, val_acc:0.809]
Epoch [16/120    avg_loss:0.448, val_acc:0.815]
Epoch [17/120    avg_loss:0.382, val_acc:0.876]
Epoch [18/120    avg_loss:0.301, val_acc:0.874]
Epoch [19/120    avg_loss:0.330, val_acc:0.819]
Epoch [20/120    avg_loss:0.367, val_acc:0.818]
Epoch [21/120    avg_loss:0.331, val_acc:0.873]
Epoch [22/120    avg_loss:0.320, val_acc:0.849]
Epoch [23/120    avg_loss:0.291, val_acc:0.893]
Epoch [24/120    avg_loss:0.230, val_acc:0.906]
Epoch [25/120    avg_loss:0.232, val_acc:0.901]
Epoch [26/120    avg_loss:0.205, val_acc:0.900]
Epoch [27/120    avg_loss:0.172, val_acc:0.931]
Epoch [28/120    avg_loss:0.170, val_acc:0.909]
Epoch [29/120    avg_loss:0.178, val_acc:0.918]
Epoch [30/120    avg_loss:0.127, val_acc:0.897]
Epoch [31/120    avg_loss:0.130, val_acc:0.932]
Epoch [32/120    avg_loss:0.111, val_acc:0.948]
Epoch [33/120    avg_loss:0.162, val_acc:0.899]
Epoch [34/120    avg_loss:0.173, val_acc:0.911]
Epoch [35/120    avg_loss:0.148, val_acc:0.916]
Epoch [36/120    avg_loss:0.194, val_acc:0.917]
Epoch [37/120    avg_loss:0.153, val_acc:0.878]
Epoch [38/120    avg_loss:0.150, val_acc:0.925]
Epoch [39/120    avg_loss:0.108, val_acc:0.932]
Epoch [40/120    avg_loss:0.116, val_acc:0.950]
Epoch [41/120    avg_loss:0.089, val_acc:0.910]
Epoch [42/120    avg_loss:0.089, val_acc:0.946]
Epoch [43/120    avg_loss:0.089, val_acc:0.944]
Epoch [44/120    avg_loss:0.081, val_acc:0.955]
Epoch [45/120    avg_loss:0.091, val_acc:0.944]
Epoch [46/120    avg_loss:0.094, val_acc:0.936]
Epoch [47/120    avg_loss:0.105, val_acc:0.922]
Epoch [48/120    avg_loss:0.099, val_acc:0.948]
Epoch [49/120    avg_loss:0.069, val_acc:0.954]
Epoch [50/120    avg_loss:0.076, val_acc:0.942]
Epoch [51/120    avg_loss:0.068, val_acc:0.953]
Epoch [52/120    avg_loss:0.075, val_acc:0.942]
Epoch [53/120    avg_loss:0.089, val_acc:0.947]
Epoch [54/120    avg_loss:0.085, val_acc:0.948]
Epoch [55/120    avg_loss:0.055, val_acc:0.962]
Epoch [56/120    avg_loss:0.057, val_acc:0.955]
Epoch [57/120    avg_loss:0.054, val_acc:0.958]
Epoch [58/120    avg_loss:0.061, val_acc:0.957]
Epoch [59/120    avg_loss:0.050, val_acc:0.956]
Epoch [60/120    avg_loss:0.039, val_acc:0.959]
Epoch [61/120    avg_loss:0.050, val_acc:0.945]
Epoch [62/120    avg_loss:0.065, val_acc:0.958]
Epoch [63/120    avg_loss:0.077, val_acc:0.919]
Epoch [64/120    avg_loss:0.069, val_acc:0.957]
Epoch [65/120    avg_loss:0.045, val_acc:0.957]
Epoch [66/120    avg_loss:0.043, val_acc:0.967]
Epoch [67/120    avg_loss:0.037, val_acc:0.965]
Epoch [68/120    avg_loss:0.079, val_acc:0.950]
Epoch [69/120    avg_loss:0.120, val_acc:0.947]
Epoch [70/120    avg_loss:0.082, val_acc:0.951]
Epoch [71/120    avg_loss:0.047, val_acc:0.955]
Epoch [72/120    avg_loss:0.041, val_acc:0.957]
Epoch [73/120    avg_loss:0.043, val_acc:0.970]
Epoch [74/120    avg_loss:0.029, val_acc:0.966]
Epoch [75/120    avg_loss:0.036, val_acc:0.970]
Epoch [76/120    avg_loss:0.033, val_acc:0.943]
Epoch [77/120    avg_loss:0.031, val_acc:0.967]
Epoch [78/120    avg_loss:0.038, val_acc:0.961]
Epoch [79/120    avg_loss:0.057, val_acc:0.964]
Epoch [80/120    avg_loss:0.039, val_acc:0.963]
Epoch [81/120    avg_loss:0.034, val_acc:0.966]
Epoch [82/120    avg_loss:0.024, val_acc:0.965]
Epoch [83/120    avg_loss:0.026, val_acc:0.971]
Epoch [84/120    avg_loss:0.019, val_acc:0.970]
Epoch [85/120    avg_loss:0.016, val_acc:0.975]
Epoch [86/120    avg_loss:0.022, val_acc:0.967]
Epoch [87/120    avg_loss:0.018, val_acc:0.973]
Epoch [88/120    avg_loss:0.031, val_acc:0.961]
Epoch [89/120    avg_loss:0.023, val_acc:0.970]
Epoch [90/120    avg_loss:0.029, val_acc:0.969]
Epoch [91/120    avg_loss:0.022, val_acc:0.971]
Epoch [92/120    avg_loss:0.014, val_acc:0.977]
Epoch [93/120    avg_loss:0.016, val_acc:0.980]
Epoch [94/120    avg_loss:0.018, val_acc:0.969]
Epoch [95/120    avg_loss:0.022, val_acc:0.978]
Epoch [96/120    avg_loss:0.018, val_acc:0.981]
Epoch [97/120    avg_loss:0.017, val_acc:0.980]
Epoch [98/120    avg_loss:0.011, val_acc:0.979]
Epoch [99/120    avg_loss:0.018, val_acc:0.981]
Epoch [100/120    avg_loss:0.030, val_acc:0.959]
Epoch [101/120    avg_loss:0.022, val_acc:0.978]
Epoch [102/120    avg_loss:0.016, val_acc:0.979]
Epoch [103/120    avg_loss:0.016, val_acc:0.975]
Epoch [104/120    avg_loss:0.016, val_acc:0.984]
Epoch [105/120    avg_loss:0.011, val_acc:0.984]
Epoch [106/120    avg_loss:0.011, val_acc:0.981]
Epoch [107/120    avg_loss:0.012, val_acc:0.977]
Epoch [108/120    avg_loss:0.013, val_acc:0.980]
Epoch [109/120    avg_loss:0.013, val_acc:0.982]
Epoch [110/120    avg_loss:0.014, val_acc:0.978]
Epoch [111/120    avg_loss:0.017, val_acc:0.976]
Epoch [112/120    avg_loss:0.015, val_acc:0.971]
Epoch [113/120    avg_loss:0.010, val_acc:0.977]
Epoch [114/120    avg_loss:0.019, val_acc:0.980]
Epoch [115/120    avg_loss:0.014, val_acc:0.970]
Epoch [116/120    avg_loss:0.011, val_acc:0.977]
Epoch [117/120    avg_loss:0.010, val_acc:0.974]
Epoch [118/120    avg_loss:0.011, val_acc:0.986]
Epoch [119/120    avg_loss:0.012, val_acc:0.985]
Epoch [120/120    avg_loss:0.012, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1266    1    1    0    2    0    0    0    2   11    2    0
     0    0    0]
 [   0    0    1  688    5    0    1    0    0   11    1    0   38    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   66    0    0    1    4    0    0    0  790   10    0    0
     1    3    0]
 [   0    0   26    0    0    2    4    3    7    0   15 2145    5    2
     1    0    0]
 [   0    0   12    0    0    0    0    0    0    1    4    4  510    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    1    0    0    0
  1114   17    0]
 [   0    0    0    0    0    1   28    0    0    0    0    0    0    0
    12  306    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.52032520325203

F1 scores:
[       nan 0.98765432 0.95331325 0.95821727 0.98611111 0.98748578
 0.97117517 0.94339623 0.99078341 0.72340426 0.93546477 0.97945205
 0.93577982 0.98930481 0.98236332 0.90936107 0.98245614]

Kappa:
0.9603530020431447
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdb576e77b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.651, val_acc:0.466]
Epoch [2/120    avg_loss:2.250, val_acc:0.513]
Epoch [3/120    avg_loss:2.050, val_acc:0.558]
Epoch [4/120    avg_loss:1.873, val_acc:0.591]
Epoch [5/120    avg_loss:1.735, val_acc:0.618]
Epoch [6/120    avg_loss:1.605, val_acc:0.637]
Epoch [7/120    avg_loss:1.462, val_acc:0.682]
Epoch [8/120    avg_loss:1.317, val_acc:0.696]
Epoch [9/120    avg_loss:1.161, val_acc:0.703]
Epoch [10/120    avg_loss:1.036, val_acc:0.720]
Epoch [11/120    avg_loss:0.913, val_acc:0.747]
Epoch [12/120    avg_loss:0.847, val_acc:0.716]
Epoch [13/120    avg_loss:0.800, val_acc:0.769]
Epoch [14/120    avg_loss:0.695, val_acc:0.760]
Epoch [15/120    avg_loss:0.637, val_acc:0.797]
Epoch [16/120    avg_loss:0.674, val_acc:0.777]
Epoch [17/120    avg_loss:0.565, val_acc:0.792]
Epoch [18/120    avg_loss:0.624, val_acc:0.794]
Epoch [19/120    avg_loss:0.514, val_acc:0.827]
Epoch [20/120    avg_loss:0.474, val_acc:0.865]
Epoch [21/120    avg_loss:0.434, val_acc:0.850]
Epoch [22/120    avg_loss:0.372, val_acc:0.859]
Epoch [23/120    avg_loss:0.497, val_acc:0.834]
Epoch [24/120    avg_loss:0.383, val_acc:0.859]
Epoch [25/120    avg_loss:0.349, val_acc:0.880]
Epoch [26/120    avg_loss:0.320, val_acc:0.843]
Epoch [27/120    avg_loss:0.299, val_acc:0.879]
Epoch [28/120    avg_loss:0.300, val_acc:0.859]
Epoch [29/120    avg_loss:0.302, val_acc:0.877]
Epoch [30/120    avg_loss:0.292, val_acc:0.871]
Epoch [31/120    avg_loss:0.333, val_acc:0.887]
Epoch [32/120    avg_loss:0.304, val_acc:0.885]
Epoch [33/120    avg_loss:0.262, val_acc:0.900]
Epoch [34/120    avg_loss:0.239, val_acc:0.867]
Epoch [35/120    avg_loss:0.229, val_acc:0.919]
Epoch [36/120    avg_loss:0.212, val_acc:0.911]
Epoch [37/120    avg_loss:0.177, val_acc:0.921]
Epoch [38/120    avg_loss:0.176, val_acc:0.921]
Epoch [39/120    avg_loss:0.139, val_acc:0.952]
Epoch [40/120    avg_loss:0.160, val_acc:0.925]
Epoch [41/120    avg_loss:0.153, val_acc:0.928]
Epoch [42/120    avg_loss:0.134, val_acc:0.930]
Epoch [43/120    avg_loss:0.152, val_acc:0.931]
Epoch [44/120    avg_loss:0.143, val_acc:0.933]
Epoch [45/120    avg_loss:0.139, val_acc:0.922]
Epoch [46/120    avg_loss:0.158, val_acc:0.938]
Epoch [47/120    avg_loss:0.138, val_acc:0.906]
Epoch [48/120    avg_loss:0.116, val_acc:0.941]
Epoch [49/120    avg_loss:0.093, val_acc:0.942]
Epoch [50/120    avg_loss:0.089, val_acc:0.945]
Epoch [51/120    avg_loss:0.079, val_acc:0.955]
Epoch [52/120    avg_loss:0.089, val_acc:0.943]
Epoch [53/120    avg_loss:0.083, val_acc:0.959]
Epoch [54/120    avg_loss:0.077, val_acc:0.957]
Epoch [55/120    avg_loss:0.086, val_acc:0.941]
Epoch [56/120    avg_loss:0.087, val_acc:0.962]
Epoch [57/120    avg_loss:0.073, val_acc:0.956]
Epoch [58/120    avg_loss:0.091, val_acc:0.945]
Epoch [59/120    avg_loss:0.065, val_acc:0.953]
Epoch [60/120    avg_loss:0.057, val_acc:0.953]
Epoch [61/120    avg_loss:0.077, val_acc:0.947]
Epoch [62/120    avg_loss:0.088, val_acc:0.956]
Epoch [63/120    avg_loss:0.076, val_acc:0.931]
Epoch [64/120    avg_loss:0.069, val_acc:0.944]
Epoch [65/120    avg_loss:0.063, val_acc:0.961]
Epoch [66/120    avg_loss:0.049, val_acc:0.963]
Epoch [67/120    avg_loss:0.051, val_acc:0.959]
Epoch [68/120    avg_loss:0.064, val_acc:0.952]
Epoch [69/120    avg_loss:0.060, val_acc:0.952]
Epoch [70/120    avg_loss:0.065, val_acc:0.933]
Epoch [71/120    avg_loss:0.092, val_acc:0.914]
Epoch [72/120    avg_loss:0.100, val_acc:0.944]
Epoch [73/120    avg_loss:0.139, val_acc:0.929]
Epoch [74/120    avg_loss:0.124, val_acc:0.938]
Epoch [75/120    avg_loss:0.095, val_acc:0.946]
Epoch [76/120    avg_loss:0.062, val_acc:0.964]
Epoch [77/120    avg_loss:0.058, val_acc:0.967]
Epoch [78/120    avg_loss:0.066, val_acc:0.957]
Epoch [79/120    avg_loss:0.074, val_acc:0.965]
Epoch [80/120    avg_loss:0.138, val_acc:0.879]
Epoch [81/120    avg_loss:0.221, val_acc:0.919]
Epoch [82/120    avg_loss:0.123, val_acc:0.943]
Epoch [83/120    avg_loss:0.071, val_acc:0.961]
Epoch [84/120    avg_loss:0.059, val_acc:0.959]
Epoch [85/120    avg_loss:0.055, val_acc:0.962]
Epoch [86/120    avg_loss:0.041, val_acc:0.973]
Epoch [87/120    avg_loss:0.027, val_acc:0.974]
Epoch [88/120    avg_loss:0.034, val_acc:0.965]
Epoch [89/120    avg_loss:0.039, val_acc:0.964]
Epoch [90/120    avg_loss:0.044, val_acc:0.966]
Epoch [91/120    avg_loss:0.040, val_acc:0.973]
Epoch [92/120    avg_loss:0.033, val_acc:0.973]
Epoch [93/120    avg_loss:0.024, val_acc:0.977]
Epoch [94/120    avg_loss:0.026, val_acc:0.968]
Epoch [95/120    avg_loss:0.031, val_acc:0.973]
Epoch [96/120    avg_loss:0.042, val_acc:0.975]
Epoch [97/120    avg_loss:0.022, val_acc:0.974]
Epoch [98/120    avg_loss:0.022, val_acc:0.976]
Epoch [99/120    avg_loss:0.021, val_acc:0.973]
Epoch [100/120    avg_loss:0.034, val_acc:0.973]
Epoch [101/120    avg_loss:0.029, val_acc:0.977]
Epoch [102/120    avg_loss:0.019, val_acc:0.978]
Epoch [103/120    avg_loss:0.024, val_acc:0.976]
Epoch [104/120    avg_loss:0.024, val_acc:0.970]
Epoch [105/120    avg_loss:0.022, val_acc:0.957]
Epoch [106/120    avg_loss:0.017, val_acc:0.966]
Epoch [107/120    avg_loss:0.019, val_acc:0.977]
Epoch [108/120    avg_loss:0.026, val_acc:0.975]
Epoch [109/120    avg_loss:0.019, val_acc:0.953]
Epoch [110/120    avg_loss:0.019, val_acc:0.974]
Epoch [111/120    avg_loss:0.021, val_acc:0.964]
Epoch [112/120    avg_loss:0.027, val_acc:0.975]
Epoch [113/120    avg_loss:0.023, val_acc:0.982]
Epoch [114/120    avg_loss:0.011, val_acc:0.981]
Epoch [115/120    avg_loss:0.030, val_acc:0.973]
Epoch [116/120    avg_loss:0.026, val_acc:0.971]
Epoch [117/120    avg_loss:0.018, val_acc:0.976]
Epoch [118/120    avg_loss:0.016, val_acc:0.978]
Epoch [119/120    avg_loss:0.015, val_acc:0.981]
Epoch [120/120    avg_loss:0.014, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1259    0    0    0    5    0    0    4    6    6    4    0
     0    1    0]
 [   0    0    0  680    1   16    0    0    0   13    0    0   31    2
     0    4    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    2    0    0    0    2
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   20   28    0    2    0    0    0    0  811    1    4    0
     1    8    0]
 [   0    0   25    0    0    2    6    0    0    0   32 2143    0    2
     0    0    0]
 [   0    0    0   18    4    5    0    0    0    4    0    0  502    0
     0    0    1]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    1    2    0    0
  1126    2    0]
 [   0    0    0    0    0    0   34    0    0    0    0    0    0    0
    67  246    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
95.81571815718158

F1 scores:
[       nan 0.94871795 0.97257628 0.92328581 0.98839907 0.95884316
 0.96612666 1.         0.99883856 0.56140351 0.93811452 0.98235159
 0.92876966 0.98133333 0.96528075 0.80921053 0.96969697]

Kappa:
0.9523285766958478
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f53d1af2860>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.666, val_acc:0.410]
Epoch [2/120    avg_loss:2.249, val_acc:0.537]
Epoch [3/120    avg_loss:2.006, val_acc:0.550]
Epoch [4/120    avg_loss:1.863, val_acc:0.580]
Epoch [5/120    avg_loss:1.706, val_acc:0.632]
Epoch [6/120    avg_loss:1.527, val_acc:0.647]
Epoch [7/120    avg_loss:1.403, val_acc:0.688]
Epoch [8/120    avg_loss:1.192, val_acc:0.712]
Epoch [9/120    avg_loss:1.074, val_acc:0.730]
Epoch [10/120    avg_loss:0.912, val_acc:0.754]
Epoch [11/120    avg_loss:0.823, val_acc:0.752]
Epoch [12/120    avg_loss:0.705, val_acc:0.804]
Epoch [13/120    avg_loss:0.645, val_acc:0.789]
Epoch [14/120    avg_loss:0.594, val_acc:0.802]
Epoch [15/120    avg_loss:0.579, val_acc:0.810]
Epoch [16/120    avg_loss:0.569, val_acc:0.797]
Epoch [17/120    avg_loss:0.503, val_acc:0.830]
Epoch [18/120    avg_loss:0.448, val_acc:0.807]
Epoch [19/120    avg_loss:0.438, val_acc:0.848]
Epoch [20/120    avg_loss:0.421, val_acc:0.880]
Epoch [21/120    avg_loss:0.328, val_acc:0.883]
Epoch [22/120    avg_loss:0.314, val_acc:0.883]
Epoch [23/120    avg_loss:0.344, val_acc:0.830]
Epoch [24/120    avg_loss:0.298, val_acc:0.884]
Epoch [25/120    avg_loss:0.278, val_acc:0.872]
Epoch [26/120    avg_loss:0.269, val_acc:0.904]
Epoch [27/120    avg_loss:0.288, val_acc:0.910]
Epoch [28/120    avg_loss:0.232, val_acc:0.909]
Epoch [29/120    avg_loss:0.205, val_acc:0.917]
Epoch [30/120    avg_loss:0.210, val_acc:0.901]
Epoch [31/120    avg_loss:0.226, val_acc:0.917]
Epoch [32/120    avg_loss:0.213, val_acc:0.928]
Epoch [33/120    avg_loss:0.169, val_acc:0.912]
Epoch [34/120    avg_loss:0.143, val_acc:0.924]
Epoch [35/120    avg_loss:0.148, val_acc:0.938]
Epoch [36/120    avg_loss:0.161, val_acc:0.901]
Epoch [37/120    avg_loss:0.136, val_acc:0.940]
Epoch [38/120    avg_loss:0.108, val_acc:0.950]
Epoch [39/120    avg_loss:0.114, val_acc:0.942]
Epoch [40/120    avg_loss:0.101, val_acc:0.932]
Epoch [41/120    avg_loss:0.131, val_acc:0.939]
Epoch [42/120    avg_loss:0.110, val_acc:0.938]
Epoch [43/120    avg_loss:0.154, val_acc:0.917]
Epoch [44/120    avg_loss:0.138, val_acc:0.936]
Epoch [45/120    avg_loss:0.160, val_acc:0.925]
Epoch [46/120    avg_loss:0.127, val_acc:0.939]
Epoch [47/120    avg_loss:0.183, val_acc:0.940]
Epoch [48/120    avg_loss:0.143, val_acc:0.921]
Epoch [49/120    avg_loss:0.116, val_acc:0.947]
Epoch [50/120    avg_loss:0.092, val_acc:0.942]
Epoch [51/120    avg_loss:0.065, val_acc:0.946]
Epoch [52/120    avg_loss:0.058, val_acc:0.956]
Epoch [53/120    avg_loss:0.063, val_acc:0.955]
Epoch [54/120    avg_loss:0.049, val_acc:0.954]
Epoch [55/120    avg_loss:0.051, val_acc:0.957]
Epoch [56/120    avg_loss:0.056, val_acc:0.955]
Epoch [57/120    avg_loss:0.055, val_acc:0.955]
Epoch [58/120    avg_loss:0.048, val_acc:0.955]
Epoch [59/120    avg_loss:0.049, val_acc:0.957]
Epoch [60/120    avg_loss:0.046, val_acc:0.955]
Epoch [61/120    avg_loss:0.049, val_acc:0.952]
Epoch [62/120    avg_loss:0.046, val_acc:0.957]
Epoch [63/120    avg_loss:0.051, val_acc:0.956]
Epoch [64/120    avg_loss:0.046, val_acc:0.956]
Epoch [65/120    avg_loss:0.049, val_acc:0.955]
Epoch [66/120    avg_loss:0.043, val_acc:0.952]
Epoch [67/120    avg_loss:0.043, val_acc:0.952]
Epoch [68/120    avg_loss:0.054, val_acc:0.957]
Epoch [69/120    avg_loss:0.041, val_acc:0.956]
Epoch [70/120    avg_loss:0.042, val_acc:0.953]
Epoch [71/120    avg_loss:0.039, val_acc:0.954]
Epoch [72/120    avg_loss:0.039, val_acc:0.956]
Epoch [73/120    avg_loss:0.043, val_acc:0.955]
Epoch [74/120    avg_loss:0.049, val_acc:0.956]
Epoch [75/120    avg_loss:0.039, val_acc:0.953]
Epoch [76/120    avg_loss:0.043, val_acc:0.957]
Epoch [77/120    avg_loss:0.042, val_acc:0.959]
Epoch [78/120    avg_loss:0.037, val_acc:0.957]
Epoch [79/120    avg_loss:0.040, val_acc:0.956]
Epoch [80/120    avg_loss:0.041, val_acc:0.956]
Epoch [81/120    avg_loss:0.046, val_acc:0.957]
Epoch [82/120    avg_loss:0.038, val_acc:0.958]
Epoch [83/120    avg_loss:0.037, val_acc:0.955]
Epoch [84/120    avg_loss:0.044, val_acc:0.957]
Epoch [85/120    avg_loss:0.042, val_acc:0.957]
Epoch [86/120    avg_loss:0.041, val_acc:0.959]
Epoch [87/120    avg_loss:0.045, val_acc:0.958]
Epoch [88/120    avg_loss:0.048, val_acc:0.956]
Epoch [89/120    avg_loss:0.042, val_acc:0.958]
Epoch [90/120    avg_loss:0.040, val_acc:0.957]
Epoch [91/120    avg_loss:0.038, val_acc:0.958]
Epoch [92/120    avg_loss:0.042, val_acc:0.956]
Epoch [93/120    avg_loss:0.039, val_acc:0.954]
Epoch [94/120    avg_loss:0.034, val_acc:0.954]
Epoch [95/120    avg_loss:0.040, val_acc:0.956]
Epoch [96/120    avg_loss:0.043, val_acc:0.958]
Epoch [97/120    avg_loss:0.036, val_acc:0.957]
Epoch [98/120    avg_loss:0.041, val_acc:0.955]
Epoch [99/120    avg_loss:0.035, val_acc:0.954]
Epoch [100/120    avg_loss:0.035, val_acc:0.953]
Epoch [101/120    avg_loss:0.035, val_acc:0.953]
Epoch [102/120    avg_loss:0.039, val_acc:0.953]
Epoch [103/120    avg_loss:0.038, val_acc:0.954]
Epoch [104/120    avg_loss:0.034, val_acc:0.953]
Epoch [105/120    avg_loss:0.035, val_acc:0.956]
Epoch [106/120    avg_loss:0.035, val_acc:0.956]
Epoch [107/120    avg_loss:0.034, val_acc:0.956]
Epoch [108/120    avg_loss:0.038, val_acc:0.957]
Epoch [109/120    avg_loss:0.037, val_acc:0.956]
Epoch [110/120    avg_loss:0.041, val_acc:0.957]
Epoch [111/120    avg_loss:0.036, val_acc:0.957]
Epoch [112/120    avg_loss:0.037, val_acc:0.957]
Epoch [113/120    avg_loss:0.033, val_acc:0.957]
Epoch [114/120    avg_loss:0.033, val_acc:0.957]
Epoch [115/120    avg_loss:0.035, val_acc:0.957]
Epoch [116/120    avg_loss:0.038, val_acc:0.957]
Epoch [117/120    avg_loss:0.033, val_acc:0.957]
Epoch [118/120    avg_loss:0.035, val_acc:0.957]
Epoch [119/120    avg_loss:0.035, val_acc:0.957]
Epoch [120/120    avg_loss:0.037, val_acc:0.957]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    3 1226    4    0    0    4    0    0    0    6   39    2    0
     0    1    0]
 [   0    0    0  696    2   25    0    0    0    2    1    1   17    0
     0    3    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   13    0    4    0    0
     0    0    0]
 [   0    0   46   40    0    5    0    0    0    0  735   41    0    0
     2    6    0]
 [   0    0    5    0    0    1   11    0    0    0   16 2166    6    3
     2    0    0]
 [   0    0    0   36    5    7    0    0    0    0   12    0  466    0
     0    1    7]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    1    0    0   11    0    0    0    0    3    0    0    0
  1122    2    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    53  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.15447154471545

F1 scores:
[       nan 0.93975904 0.95669138 0.91338583 0.98383372 0.93989071
 0.98795181 1.         1.         0.74285714 0.89090909 0.97086508
 0.90838207 0.98924731 0.96682464 0.89908257 0.95402299]

Kappa:
0.9447102331914623
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a9328e828>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.660, val_acc:0.344]
Epoch [2/120    avg_loss:2.273, val_acc:0.416]
Epoch [3/120    avg_loss:1.999, val_acc:0.513]
Epoch [4/120    avg_loss:1.829, val_acc:0.550]
Epoch [5/120    avg_loss:1.722, val_acc:0.594]
Epoch [6/120    avg_loss:1.573, val_acc:0.556]
Epoch [7/120    avg_loss:1.383, val_acc:0.660]
Epoch [8/120    avg_loss:1.190, val_acc:0.660]
Epoch [9/120    avg_loss:1.093, val_acc:0.599]
Epoch [10/120    avg_loss:0.900, val_acc:0.712]
Epoch [11/120    avg_loss:0.787, val_acc:0.717]
Epoch [12/120    avg_loss:0.754, val_acc:0.755]
Epoch [13/120    avg_loss:0.710, val_acc:0.723]
Epoch [14/120    avg_loss:0.772, val_acc:0.746]
Epoch [15/120    avg_loss:0.614, val_acc:0.763]
Epoch [16/120    avg_loss:0.538, val_acc:0.755]
Epoch [17/120    avg_loss:0.479, val_acc:0.797]
Epoch [18/120    avg_loss:0.412, val_acc:0.803]
Epoch [19/120    avg_loss:0.456, val_acc:0.799]
Epoch [20/120    avg_loss:0.496, val_acc:0.795]
Epoch [21/120    avg_loss:0.397, val_acc:0.791]
Epoch [22/120    avg_loss:0.387, val_acc:0.816]
Epoch [23/120    avg_loss:0.317, val_acc:0.831]
Epoch [24/120    avg_loss:0.289, val_acc:0.839]
Epoch [25/120    avg_loss:0.276, val_acc:0.874]
Epoch [26/120    avg_loss:0.267, val_acc:0.846]
Epoch [27/120    avg_loss:0.250, val_acc:0.860]
Epoch [28/120    avg_loss:0.219, val_acc:0.894]
Epoch [29/120    avg_loss:0.209, val_acc:0.872]
Epoch [30/120    avg_loss:0.224, val_acc:0.851]
Epoch [31/120    avg_loss:0.236, val_acc:0.870]
Epoch [32/120    avg_loss:0.187, val_acc:0.861]
Epoch [33/120    avg_loss:0.190, val_acc:0.896]
Epoch [34/120    avg_loss:0.164, val_acc:0.906]
Epoch [35/120    avg_loss:0.172, val_acc:0.874]
Epoch [36/120    avg_loss:0.166, val_acc:0.893]
Epoch [37/120    avg_loss:0.148, val_acc:0.879]
Epoch [38/120    avg_loss:0.178, val_acc:0.891]
Epoch [39/120    avg_loss:0.180, val_acc:0.916]
Epoch [40/120    avg_loss:0.158, val_acc:0.890]
Epoch [41/120    avg_loss:0.164, val_acc:0.914]
Epoch [42/120    avg_loss:0.159, val_acc:0.902]
Epoch [43/120    avg_loss:0.115, val_acc:0.904]
Epoch [44/120    avg_loss:0.142, val_acc:0.894]
Epoch [45/120    avg_loss:0.142, val_acc:0.901]
Epoch [46/120    avg_loss:0.165, val_acc:0.909]
Epoch [47/120    avg_loss:0.147, val_acc:0.920]
Epoch [48/120    avg_loss:0.107, val_acc:0.916]
Epoch [49/120    avg_loss:0.092, val_acc:0.932]
Epoch [50/120    avg_loss:0.132, val_acc:0.919]
Epoch [51/120    avg_loss:0.270, val_acc:0.887]
Epoch [52/120    avg_loss:0.245, val_acc:0.907]
Epoch [53/120    avg_loss:0.170, val_acc:0.905]
Epoch [54/120    avg_loss:0.207, val_acc:0.844]
Epoch [55/120    avg_loss:0.399, val_acc:0.863]
Epoch [56/120    avg_loss:0.265, val_acc:0.883]
Epoch [57/120    avg_loss:0.145, val_acc:0.910]
Epoch [58/120    avg_loss:0.146, val_acc:0.875]
Epoch [59/120    avg_loss:0.133, val_acc:0.923]
Epoch [60/120    avg_loss:0.099, val_acc:0.910]
Epoch [61/120    avg_loss:0.095, val_acc:0.919]
Epoch [62/120    avg_loss:0.075, val_acc:0.916]
Epoch [63/120    avg_loss:0.092, val_acc:0.933]
Epoch [64/120    avg_loss:0.068, val_acc:0.936]
Epoch [65/120    avg_loss:0.064, val_acc:0.935]
Epoch [66/120    avg_loss:0.067, val_acc:0.935]
Epoch [67/120    avg_loss:0.051, val_acc:0.936]
Epoch [68/120    avg_loss:0.058, val_acc:0.939]
Epoch [69/120    avg_loss:0.057, val_acc:0.940]
Epoch [70/120    avg_loss:0.062, val_acc:0.943]
Epoch [71/120    avg_loss:0.055, val_acc:0.940]
Epoch [72/120    avg_loss:0.065, val_acc:0.941]
Epoch [73/120    avg_loss:0.053, val_acc:0.941]
Epoch [74/120    avg_loss:0.057, val_acc:0.940]
Epoch [75/120    avg_loss:0.057, val_acc:0.939]
Epoch [76/120    avg_loss:0.046, val_acc:0.938]
Epoch [77/120    avg_loss:0.065, val_acc:0.948]
Epoch [78/120    avg_loss:0.051, val_acc:0.944]
Epoch [79/120    avg_loss:0.047, val_acc:0.945]
Epoch [80/120    avg_loss:0.055, val_acc:0.946]
Epoch [81/120    avg_loss:0.049, val_acc:0.948]
Epoch [82/120    avg_loss:0.047, val_acc:0.946]
Epoch [83/120    avg_loss:0.042, val_acc:0.948]
Epoch [84/120    avg_loss:0.040, val_acc:0.944]
Epoch [85/120    avg_loss:0.036, val_acc:0.947]
Epoch [86/120    avg_loss:0.043, val_acc:0.946]
Epoch [87/120    avg_loss:0.048, val_acc:0.946]
Epoch [88/120    avg_loss:0.040, val_acc:0.944]
Epoch [89/120    avg_loss:0.040, val_acc:0.947]
Epoch [90/120    avg_loss:0.046, val_acc:0.944]
Epoch [91/120    avg_loss:0.039, val_acc:0.945]
Epoch [92/120    avg_loss:0.047, val_acc:0.951]
Epoch [93/120    avg_loss:0.044, val_acc:0.947]
Epoch [94/120    avg_loss:0.046, val_acc:0.945]
Epoch [95/120    avg_loss:0.038, val_acc:0.946]
Epoch [96/120    avg_loss:0.039, val_acc:0.948]
Epoch [97/120    avg_loss:0.040, val_acc:0.947]
Epoch [98/120    avg_loss:0.043, val_acc:0.952]
Epoch [99/120    avg_loss:0.036, val_acc:0.947]
Epoch [100/120    avg_loss:0.038, val_acc:0.950]
Epoch [101/120    avg_loss:0.038, val_acc:0.946]
Epoch [102/120    avg_loss:0.042, val_acc:0.953]
Epoch [103/120    avg_loss:0.050, val_acc:0.954]
Epoch [104/120    avg_loss:0.041, val_acc:0.948]
Epoch [105/120    avg_loss:0.041, val_acc:0.954]
Epoch [106/120    avg_loss:0.039, val_acc:0.954]
Epoch [107/120    avg_loss:0.034, val_acc:0.954]
Epoch [108/120    avg_loss:0.039, val_acc:0.952]
Epoch [109/120    avg_loss:0.042, val_acc:0.952]
Epoch [110/120    avg_loss:0.043, val_acc:0.953]
Epoch [111/120    avg_loss:0.038, val_acc:0.954]
Epoch [112/120    avg_loss:0.034, val_acc:0.952]
Epoch [113/120    avg_loss:0.039, val_acc:0.952]
Epoch [114/120    avg_loss:0.038, val_acc:0.953]
Epoch [115/120    avg_loss:0.035, val_acc:0.955]
Epoch [116/120    avg_loss:0.036, val_acc:0.955]
Epoch [117/120    avg_loss:0.036, val_acc:0.953]
Epoch [118/120    avg_loss:0.042, val_acc:0.950]
Epoch [119/120    avg_loss:0.034, val_acc:0.955]
Epoch [120/120    avg_loss:0.038, val_acc:0.948]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1226    4    1    0    1    0    0    1   14   34    4    0
     0    0    0]
 [   0    0   10  707    2    4    0    0    0    5    0    0   17    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    3    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   95   28    0   10    0    0    0    0  726    7    6    0
     2    1    0]
 [   0    1   28    0    0    0    4    0    1    0   23 2145    0    3
     5    0    0]
 [   0    0    0    0    5    3    0    0    0    0    0    8  514    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    1    0    0
  1132    4    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    51  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.57723577235772

F1 scores:
[       nan 0.93670886 0.92738275 0.95154778 0.98156682 0.97621744
 0.9924357  1.         0.99767981 0.74418605 0.88374924 0.97367227
 0.95273401 0.98666667 0.97167382 0.90402477 0.97619048]

Kappa:
0.9495579573414549
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbc4786c860>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.673, val_acc:0.462]
Epoch [2/120    avg_loss:2.238, val_acc:0.524]
Epoch [3/120    avg_loss:2.033, val_acc:0.577]
Epoch [4/120    avg_loss:1.897, val_acc:0.636]
Epoch [5/120    avg_loss:1.670, val_acc:0.677]
Epoch [6/120    avg_loss:1.615, val_acc:0.704]
Epoch [7/120    avg_loss:1.406, val_acc:0.704]
Epoch [8/120    avg_loss:1.216, val_acc:0.739]
Epoch [9/120    avg_loss:1.068, val_acc:0.775]
Epoch [10/120    avg_loss:0.973, val_acc:0.772]
Epoch [11/120    avg_loss:0.903, val_acc:0.780]
Epoch [12/120    avg_loss:0.763, val_acc:0.798]
Epoch [13/120    avg_loss:0.713, val_acc:0.827]
Epoch [14/120    avg_loss:0.625, val_acc:0.851]
Epoch [15/120    avg_loss:0.560, val_acc:0.853]
Epoch [16/120    avg_loss:0.547, val_acc:0.818]
Epoch [17/120    avg_loss:0.504, val_acc:0.829]
Epoch [18/120    avg_loss:0.430, val_acc:0.854]
Epoch [19/120    avg_loss:0.413, val_acc:0.857]
Epoch [20/120    avg_loss:0.437, val_acc:0.825]
Epoch [21/120    avg_loss:0.407, val_acc:0.857]
Epoch [22/120    avg_loss:0.477, val_acc:0.871]
Epoch [23/120    avg_loss:0.340, val_acc:0.845]
Epoch [24/120    avg_loss:0.317, val_acc:0.889]
Epoch [25/120    avg_loss:0.268, val_acc:0.876]
Epoch [26/120    avg_loss:0.213, val_acc:0.912]
Epoch [27/120    avg_loss:0.233, val_acc:0.922]
Epoch [28/120    avg_loss:0.231, val_acc:0.897]
Epoch [29/120    avg_loss:0.215, val_acc:0.899]
Epoch [30/120    avg_loss:0.223, val_acc:0.899]
Epoch [31/120    avg_loss:0.241, val_acc:0.882]
Epoch [32/120    avg_loss:0.233, val_acc:0.916]
Epoch [33/120    avg_loss:0.189, val_acc:0.924]
Epoch [34/120    avg_loss:0.170, val_acc:0.917]
Epoch [35/120    avg_loss:0.151, val_acc:0.914]
Epoch [36/120    avg_loss:0.145, val_acc:0.946]
Epoch [37/120    avg_loss:0.131, val_acc:0.919]
Epoch [38/120    avg_loss:0.166, val_acc:0.925]
Epoch [39/120    avg_loss:0.209, val_acc:0.916]
Epoch [40/120    avg_loss:0.193, val_acc:0.909]
Epoch [41/120    avg_loss:0.142, val_acc:0.909]
Epoch [42/120    avg_loss:0.102, val_acc:0.930]
Epoch [43/120    avg_loss:0.104, val_acc:0.932]
Epoch [44/120    avg_loss:0.104, val_acc:0.907]
Epoch [45/120    avg_loss:0.111, val_acc:0.922]
Epoch [46/120    avg_loss:0.103, val_acc:0.920]
Epoch [47/120    avg_loss:0.110, val_acc:0.925]
Epoch [48/120    avg_loss:0.097, val_acc:0.943]
Epoch [49/120    avg_loss:0.082, val_acc:0.938]
Epoch [50/120    avg_loss:0.070, val_acc:0.944]
Epoch [51/120    avg_loss:0.056, val_acc:0.951]
Epoch [52/120    avg_loss:0.053, val_acc:0.951]
Epoch [53/120    avg_loss:0.057, val_acc:0.957]
Epoch [54/120    avg_loss:0.044, val_acc:0.958]
Epoch [55/120    avg_loss:0.051, val_acc:0.958]
Epoch [56/120    avg_loss:0.042, val_acc:0.958]
Epoch [57/120    avg_loss:0.046, val_acc:0.956]
Epoch [58/120    avg_loss:0.046, val_acc:0.954]
Epoch [59/120    avg_loss:0.050, val_acc:0.955]
Epoch [60/120    avg_loss:0.044, val_acc:0.955]
Epoch [61/120    avg_loss:0.039, val_acc:0.957]
Epoch [62/120    avg_loss:0.043, val_acc:0.958]
Epoch [63/120    avg_loss:0.043, val_acc:0.958]
Epoch [64/120    avg_loss:0.041, val_acc:0.959]
Epoch [65/120    avg_loss:0.048, val_acc:0.958]
Epoch [66/120    avg_loss:0.046, val_acc:0.959]
Epoch [67/120    avg_loss:0.035, val_acc:0.961]
Epoch [68/120    avg_loss:0.046, val_acc:0.959]
Epoch [69/120    avg_loss:0.047, val_acc:0.957]
Epoch [70/120    avg_loss:0.041, val_acc:0.959]
Epoch [71/120    avg_loss:0.045, val_acc:0.959]
Epoch [72/120    avg_loss:0.041, val_acc:0.961]
Epoch [73/120    avg_loss:0.036, val_acc:0.961]
Epoch [74/120    avg_loss:0.041, val_acc:0.962]
Epoch [75/120    avg_loss:0.034, val_acc:0.962]
Epoch [76/120    avg_loss:0.043, val_acc:0.962]
Epoch [77/120    avg_loss:0.037, val_acc:0.964]
Epoch [78/120    avg_loss:0.042, val_acc:0.962]
Epoch [79/120    avg_loss:0.037, val_acc:0.963]
Epoch [80/120    avg_loss:0.039, val_acc:0.965]
Epoch [81/120    avg_loss:0.041, val_acc:0.964]
Epoch [82/120    avg_loss:0.042, val_acc:0.962]
Epoch [83/120    avg_loss:0.035, val_acc:0.962]
Epoch [84/120    avg_loss:0.031, val_acc:0.963]
Epoch [85/120    avg_loss:0.036, val_acc:0.963]
Epoch [86/120    avg_loss:0.034, val_acc:0.962]
Epoch [87/120    avg_loss:0.027, val_acc:0.963]
Epoch [88/120    avg_loss:0.046, val_acc:0.963]
Epoch [89/120    avg_loss:0.033, val_acc:0.964]
Epoch [90/120    avg_loss:0.037, val_acc:0.966]
Epoch [91/120    avg_loss:0.034, val_acc:0.964]
Epoch [92/120    avg_loss:0.035, val_acc:0.964]
Epoch [93/120    avg_loss:0.034, val_acc:0.964]
Epoch [94/120    avg_loss:0.039, val_acc:0.964]
Epoch [95/120    avg_loss:0.031, val_acc:0.965]
Epoch [96/120    avg_loss:0.035, val_acc:0.964]
Epoch [97/120    avg_loss:0.040, val_acc:0.965]
Epoch [98/120    avg_loss:0.043, val_acc:0.967]
Epoch [99/120    avg_loss:0.032, val_acc:0.966]
Epoch [100/120    avg_loss:0.036, val_acc:0.964]
Epoch [101/120    avg_loss:0.037, val_acc:0.965]
Epoch [102/120    avg_loss:0.031, val_acc:0.966]
Epoch [103/120    avg_loss:0.037, val_acc:0.967]
Epoch [104/120    avg_loss:0.031, val_acc:0.966]
Epoch [105/120    avg_loss:0.032, val_acc:0.966]
Epoch [106/120    avg_loss:0.032, val_acc:0.963]
Epoch [107/120    avg_loss:0.028, val_acc:0.964]
Epoch [108/120    avg_loss:0.032, val_acc:0.966]
Epoch [109/120    avg_loss:0.032, val_acc:0.965]
Epoch [110/120    avg_loss:0.030, val_acc:0.966]
Epoch [111/120    avg_loss:0.036, val_acc:0.966]
Epoch [112/120    avg_loss:0.029, val_acc:0.966]
Epoch [113/120    avg_loss:0.036, val_acc:0.965]
Epoch [114/120    avg_loss:0.029, val_acc:0.965]
Epoch [115/120    avg_loss:0.032, val_acc:0.965]
Epoch [116/120    avg_loss:0.029, val_acc:0.966]
Epoch [117/120    avg_loss:0.032, val_acc:0.966]
Epoch [118/120    avg_loss:0.035, val_acc:0.966]
Epoch [119/120    avg_loss:0.032, val_acc:0.966]
Epoch [120/120    avg_loss:0.031, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1236    4    0    0    3    0    0    3   12   27    0    0
     0    0    0]
 [   0    0    4  708    1    6    0    0    0   11    1    0    8    3
     0    5    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    0    0    3    0    0    0    0
    10    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   19   44    0    7    0    0    0    0  787    8    0    0
     0   10    0]
 [   0    0   20    0    0    0    6    0    9    0   23 2148    3    1
     0    0    0]
 [   0    0    0   11   25    2    0    0    0    3   13    0  474    0
     0    2    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    4    0    3    1    0    0
  1123    3    0]
 [   0    0    0    0    0    0   30    0    0    2    0    0    0    0
    51  264    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.44715447154472

F1 scores:
[       nan 0.93506494 0.96411856 0.93527081 0.94247788 0.96237172
 0.96888889 1.         0.98510882 0.59649123 0.91564863 0.97702979
 0.92850147 0.98930481 0.96685321 0.83676704 0.97076023]

Kappa:
0.9481125009901747
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7cca335828>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.681, val_acc:0.450]
Epoch [2/120    avg_loss:2.282, val_acc:0.524]
Epoch [3/120    avg_loss:2.031, val_acc:0.549]
Epoch [4/120    avg_loss:1.905, val_acc:0.573]
Epoch [5/120    avg_loss:1.744, val_acc:0.598]
Epoch [6/120    avg_loss:1.562, val_acc:0.640]
Epoch [7/120    avg_loss:1.465, val_acc:0.623]
Epoch [8/120    avg_loss:1.324, val_acc:0.648]
Epoch [9/120    avg_loss:1.158, val_acc:0.727]
Epoch [10/120    avg_loss:0.994, val_acc:0.724]
Epoch [11/120    avg_loss:0.896, val_acc:0.715]
Epoch [12/120    avg_loss:0.819, val_acc:0.758]
Epoch [13/120    avg_loss:0.663, val_acc:0.780]
Epoch [14/120    avg_loss:0.600, val_acc:0.827]
Epoch [15/120    avg_loss:0.599, val_acc:0.816]
Epoch [16/120    avg_loss:0.550, val_acc:0.781]
Epoch [17/120    avg_loss:0.601, val_acc:0.825]
Epoch [18/120    avg_loss:0.519, val_acc:0.849]
Epoch [19/120    avg_loss:0.452, val_acc:0.796]
Epoch [20/120    avg_loss:0.399, val_acc:0.888]
Epoch [21/120    avg_loss:0.310, val_acc:0.868]
Epoch [22/120    avg_loss:0.302, val_acc:0.873]
Epoch [23/120    avg_loss:0.262, val_acc:0.898]
Epoch [24/120    avg_loss:0.236, val_acc:0.890]
Epoch [25/120    avg_loss:0.263, val_acc:0.905]
Epoch [26/120    avg_loss:0.232, val_acc:0.890]
Epoch [27/120    avg_loss:0.254, val_acc:0.884]
Epoch [28/120    avg_loss:0.237, val_acc:0.919]
Epoch [29/120    avg_loss:0.221, val_acc:0.901]
Epoch [30/120    avg_loss:0.194, val_acc:0.917]
Epoch [31/120    avg_loss:0.164, val_acc:0.929]
Epoch [32/120    avg_loss:0.149, val_acc:0.924]
Epoch [33/120    avg_loss:0.147, val_acc:0.939]
Epoch [34/120    avg_loss:0.134, val_acc:0.924]
Epoch [35/120    avg_loss:0.123, val_acc:0.932]
Epoch [36/120    avg_loss:0.128, val_acc:0.938]
Epoch [37/120    avg_loss:0.114, val_acc:0.934]
Epoch [38/120    avg_loss:0.087, val_acc:0.943]
Epoch [39/120    avg_loss:0.084, val_acc:0.933]
Epoch [40/120    avg_loss:0.081, val_acc:0.950]
Epoch [41/120    avg_loss:0.089, val_acc:0.946]
Epoch [42/120    avg_loss:0.097, val_acc:0.932]
Epoch [43/120    avg_loss:0.088, val_acc:0.930]
Epoch [44/120    avg_loss:0.084, val_acc:0.948]
Epoch [45/120    avg_loss:0.077, val_acc:0.941]
Epoch [46/120    avg_loss:0.088, val_acc:0.945]
Epoch [47/120    avg_loss:0.066, val_acc:0.946]
Epoch [48/120    avg_loss:0.099, val_acc:0.940]
Epoch [49/120    avg_loss:0.081, val_acc:0.958]
Epoch [50/120    avg_loss:0.072, val_acc:0.941]
Epoch [51/120    avg_loss:0.071, val_acc:0.943]
Epoch [52/120    avg_loss:0.087, val_acc:0.928]
Epoch [53/120    avg_loss:0.083, val_acc:0.940]
Epoch [54/120    avg_loss:0.068, val_acc:0.952]
Epoch [55/120    avg_loss:0.072, val_acc:0.951]
Epoch [56/120    avg_loss:0.064, val_acc:0.955]
Epoch [57/120    avg_loss:0.053, val_acc:0.958]
Epoch [58/120    avg_loss:0.049, val_acc:0.954]
Epoch [59/120    avg_loss:0.054, val_acc:0.955]
Epoch [60/120    avg_loss:0.048, val_acc:0.942]
Epoch [61/120    avg_loss:0.070, val_acc:0.954]
Epoch [62/120    avg_loss:0.068, val_acc:0.951]
Epoch [63/120    avg_loss:0.054, val_acc:0.963]
Epoch [64/120    avg_loss:0.039, val_acc:0.961]
Epoch [65/120    avg_loss:0.039, val_acc:0.967]
Epoch [66/120    avg_loss:0.037, val_acc:0.962]
Epoch [67/120    avg_loss:0.036, val_acc:0.967]
Epoch [68/120    avg_loss:0.028, val_acc:0.965]
Epoch [69/120    avg_loss:0.027, val_acc:0.963]
Epoch [70/120    avg_loss:0.035, val_acc:0.961]
Epoch [71/120    avg_loss:0.052, val_acc:0.952]
Epoch [72/120    avg_loss:0.047, val_acc:0.961]
Epoch [73/120    avg_loss:0.040, val_acc:0.961]
Epoch [74/120    avg_loss:0.076, val_acc:0.955]
Epoch [75/120    avg_loss:0.215, val_acc:0.930]
Epoch [76/120    avg_loss:0.264, val_acc:0.938]
Epoch [77/120    avg_loss:0.108, val_acc:0.942]
Epoch [78/120    avg_loss:0.069, val_acc:0.950]
Epoch [79/120    avg_loss:0.095, val_acc:0.924]
Epoch [80/120    avg_loss:0.074, val_acc:0.947]
Epoch [81/120    avg_loss:0.057, val_acc:0.958]
Epoch [82/120    avg_loss:0.041, val_acc:0.961]
Epoch [83/120    avg_loss:0.042, val_acc:0.959]
Epoch [84/120    avg_loss:0.040, val_acc:0.961]
Epoch [85/120    avg_loss:0.032, val_acc:0.961]
Epoch [86/120    avg_loss:0.040, val_acc:0.961]
Epoch [87/120    avg_loss:0.041, val_acc:0.961]
Epoch [88/120    avg_loss:0.037, val_acc:0.964]
Epoch [89/120    avg_loss:0.031, val_acc:0.962]
Epoch [90/120    avg_loss:0.026, val_acc:0.961]
Epoch [91/120    avg_loss:0.031, val_acc:0.963]
Epoch [92/120    avg_loss:0.032, val_acc:0.967]
Epoch [93/120    avg_loss:0.029, val_acc:0.965]
Epoch [94/120    avg_loss:0.029, val_acc:0.965]
Epoch [95/120    avg_loss:0.027, val_acc:0.965]
Epoch [96/120    avg_loss:0.028, val_acc:0.968]
Epoch [97/120    avg_loss:0.025, val_acc:0.969]
Epoch [98/120    avg_loss:0.025, val_acc:0.967]
Epoch [99/120    avg_loss:0.028, val_acc:0.966]
Epoch [100/120    avg_loss:0.026, val_acc:0.966]
Epoch [101/120    avg_loss:0.027, val_acc:0.967]
Epoch [102/120    avg_loss:0.026, val_acc:0.965]
Epoch [103/120    avg_loss:0.032, val_acc:0.968]
Epoch [104/120    avg_loss:0.028, val_acc:0.967]
Epoch [105/120    avg_loss:0.031, val_acc:0.967]
Epoch [106/120    avg_loss:0.024, val_acc:0.965]
Epoch [107/120    avg_loss:0.026, val_acc:0.964]
Epoch [108/120    avg_loss:0.028, val_acc:0.964]
Epoch [109/120    avg_loss:0.025, val_acc:0.965]
Epoch [110/120    avg_loss:0.026, val_acc:0.967]
Epoch [111/120    avg_loss:0.023, val_acc:0.967]
Epoch [112/120    avg_loss:0.023, val_acc:0.967]
Epoch [113/120    avg_loss:0.025, val_acc:0.967]
Epoch [114/120    avg_loss:0.023, val_acc:0.967]
Epoch [115/120    avg_loss:0.024, val_acc:0.968]
Epoch [116/120    avg_loss:0.025, val_acc:0.968]
Epoch [117/120    avg_loss:0.022, val_acc:0.967]
Epoch [118/120    avg_loss:0.025, val_acc:0.967]
Epoch [119/120    avg_loss:0.020, val_acc:0.967]
Epoch [120/120    avg_loss:0.026, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    1 1233    3    0    0    1    0    0    4    7   30    6    0
     0    0    0]
 [   0    0    0  735    0    1    0    0    0    7    1    0    1    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    3    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   12   60    0    9    1    0    0    0  765   17    3    0
     0    8    0]
 [   0    0   11    0    0    0   11    0    0    0   10 2176    1    1
     0    0    0]
 [   0    0    1   15    9    3    0    0    0    0    0    4  498    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   19    0    0    2    0    4    1    0    0
  1113    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    92  255    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.9349593495935

F1 scores:
[       nan 0.96296296 0.97010228 0.94170404 0.97931034 0.95642458
 0.98944193 1.         0.99767981 0.69387755 0.91947115 0.98040099
 0.95311005 0.9919571  0.94804089 0.83606557 0.96470588]

Kappa:
0.9536290267179244
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f32be8f97f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.710, val_acc:0.505]
Epoch [2/120    avg_loss:2.273, val_acc:0.472]
Epoch [3/120    avg_loss:2.104, val_acc:0.525]
Epoch [4/120    avg_loss:1.941, val_acc:0.566]
Epoch [5/120    avg_loss:1.742, val_acc:0.628]
Epoch [6/120    avg_loss:1.585, val_acc:0.659]
Epoch [7/120    avg_loss:1.423, val_acc:0.677]
Epoch [8/120    avg_loss:1.336, val_acc:0.720]
Epoch [9/120    avg_loss:1.196, val_acc:0.726]
Epoch [10/120    avg_loss:1.115, val_acc:0.721]
Epoch [11/120    avg_loss:0.915, val_acc:0.796]
Epoch [12/120    avg_loss:0.784, val_acc:0.775]
Epoch [13/120    avg_loss:0.733, val_acc:0.787]
Epoch [14/120    avg_loss:0.693, val_acc:0.796]
Epoch [15/120    avg_loss:0.656, val_acc:0.820]
Epoch [16/120    avg_loss:0.545, val_acc:0.839]
Epoch [17/120    avg_loss:0.436, val_acc:0.860]
Epoch [18/120    avg_loss:0.401, val_acc:0.865]
Epoch [19/120    avg_loss:0.491, val_acc:0.854]
Epoch [20/120    avg_loss:0.362, val_acc:0.855]
Epoch [21/120    avg_loss:0.424, val_acc:0.868]
Epoch [22/120    avg_loss:0.372, val_acc:0.873]
Epoch [23/120    avg_loss:0.300, val_acc:0.905]
Epoch [24/120    avg_loss:0.309, val_acc:0.896]
Epoch [25/120    avg_loss:0.286, val_acc:0.890]
Epoch [26/120    avg_loss:0.244, val_acc:0.893]
Epoch [27/120    avg_loss:0.256, val_acc:0.910]
Epoch [28/120    avg_loss:0.201, val_acc:0.925]
Epoch [29/120    avg_loss:0.189, val_acc:0.912]
Epoch [30/120    avg_loss:0.191, val_acc:0.922]
Epoch [31/120    avg_loss:0.181, val_acc:0.930]
Epoch [32/120    avg_loss:0.136, val_acc:0.927]
Epoch [33/120    avg_loss:0.147, val_acc:0.927]
Epoch [34/120    avg_loss:0.196, val_acc:0.903]
Epoch [35/120    avg_loss:0.172, val_acc:0.918]
Epoch [36/120    avg_loss:0.148, val_acc:0.919]
Epoch [37/120    avg_loss:0.154, val_acc:0.930]
Epoch [38/120    avg_loss:0.116, val_acc:0.937]
Epoch [39/120    avg_loss:0.167, val_acc:0.928]
Epoch [40/120    avg_loss:0.308, val_acc:0.865]
Epoch [41/120    avg_loss:0.276, val_acc:0.894]
Epoch [42/120    avg_loss:0.205, val_acc:0.927]
Epoch [43/120    avg_loss:0.169, val_acc:0.922]
Epoch [44/120    avg_loss:0.174, val_acc:0.898]
Epoch [45/120    avg_loss:0.137, val_acc:0.935]
Epoch [46/120    avg_loss:0.130, val_acc:0.944]
Epoch [47/120    avg_loss:0.120, val_acc:0.947]
Epoch [48/120    avg_loss:0.116, val_acc:0.932]
Epoch [49/120    avg_loss:0.095, val_acc:0.958]
Epoch [50/120    avg_loss:0.101, val_acc:0.928]
Epoch [51/120    avg_loss:0.116, val_acc:0.941]
Epoch [52/120    avg_loss:0.118, val_acc:0.941]
Epoch [53/120    avg_loss:0.143, val_acc:0.946]
Epoch [54/120    avg_loss:0.083, val_acc:0.921]
Epoch [55/120    avg_loss:0.089, val_acc:0.959]
Epoch [56/120    avg_loss:0.073, val_acc:0.935]
Epoch [57/120    avg_loss:0.067, val_acc:0.948]
Epoch [58/120    avg_loss:0.077, val_acc:0.946]
Epoch [59/120    avg_loss:0.101, val_acc:0.955]
Epoch [60/120    avg_loss:0.071, val_acc:0.949]
Epoch [61/120    avg_loss:0.064, val_acc:0.955]
Epoch [62/120    avg_loss:0.049, val_acc:0.964]
Epoch [63/120    avg_loss:0.073, val_acc:0.953]
Epoch [64/120    avg_loss:0.053, val_acc:0.964]
Epoch [65/120    avg_loss:0.040, val_acc:0.970]
Epoch [66/120    avg_loss:0.049, val_acc:0.946]
Epoch [67/120    avg_loss:0.065, val_acc:0.959]
Epoch [68/120    avg_loss:0.053, val_acc:0.955]
Epoch [69/120    avg_loss:0.059, val_acc:0.958]
Epoch [70/120    avg_loss:0.050, val_acc:0.971]
Epoch [71/120    avg_loss:0.061, val_acc:0.962]
Epoch [72/120    avg_loss:0.032, val_acc:0.970]
Epoch [73/120    avg_loss:0.040, val_acc:0.975]
Epoch [74/120    avg_loss:0.054, val_acc:0.964]
Epoch [75/120    avg_loss:0.073, val_acc:0.927]
Epoch [76/120    avg_loss:0.068, val_acc:0.966]
Epoch [77/120    avg_loss:0.052, val_acc:0.964]
Epoch [78/120    avg_loss:0.045, val_acc:0.968]
Epoch [79/120    avg_loss:0.038, val_acc:0.971]
Epoch [80/120    avg_loss:0.037, val_acc:0.971]
Epoch [81/120    avg_loss:0.029, val_acc:0.973]
Epoch [82/120    avg_loss:0.040, val_acc:0.971]
Epoch [83/120    avg_loss:0.037, val_acc:0.964]
Epoch [84/120    avg_loss:0.028, val_acc:0.979]
Epoch [85/120    avg_loss:0.035, val_acc:0.979]
Epoch [86/120    avg_loss:0.030, val_acc:0.977]
Epoch [87/120    avg_loss:0.020, val_acc:0.979]
Epoch [88/120    avg_loss:0.026, val_acc:0.980]
Epoch [89/120    avg_loss:0.028, val_acc:0.970]
Epoch [90/120    avg_loss:0.022, val_acc:0.967]
Epoch [91/120    avg_loss:0.026, val_acc:0.974]
Epoch [92/120    avg_loss:0.019, val_acc:0.980]
Epoch [93/120    avg_loss:0.015, val_acc:0.981]
Epoch [94/120    avg_loss:0.025, val_acc:0.970]
Epoch [95/120    avg_loss:0.022, val_acc:0.981]
Epoch [96/120    avg_loss:0.016, val_acc:0.980]
Epoch [97/120    avg_loss:0.013, val_acc:0.985]
Epoch [98/120    avg_loss:0.025, val_acc:0.972]
Epoch [99/120    avg_loss:0.032, val_acc:0.968]
Epoch [100/120    avg_loss:0.020, val_acc:0.972]
Epoch [101/120    avg_loss:0.019, val_acc:0.984]
Epoch [102/120    avg_loss:0.021, val_acc:0.968]
Epoch [103/120    avg_loss:0.031, val_acc:0.968]
Epoch [104/120    avg_loss:0.035, val_acc:0.959]
Epoch [105/120    avg_loss:0.037, val_acc:0.976]
Epoch [106/120    avg_loss:0.036, val_acc:0.968]
Epoch [107/120    avg_loss:0.026, val_acc:0.973]
Epoch [108/120    avg_loss:0.021, val_acc:0.976]
Epoch [109/120    avg_loss:0.016, val_acc:0.968]
Epoch [110/120    avg_loss:0.018, val_acc:0.968]
Epoch [111/120    avg_loss:0.025, val_acc:0.977]
Epoch [112/120    avg_loss:0.013, val_acc:0.977]
Epoch [113/120    avg_loss:0.014, val_acc:0.979]
Epoch [114/120    avg_loss:0.013, val_acc:0.980]
Epoch [115/120    avg_loss:0.014, val_acc:0.980]
Epoch [116/120    avg_loss:0.013, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.981]
Epoch [118/120    avg_loss:0.011, val_acc:0.982]
Epoch [119/120    avg_loss:0.014, val_acc:0.982]
Epoch [120/120    avg_loss:0.013, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1247    2    0    0    0    0    0    3    7   21    5    0
     0    0    0]
 [   0    0    2  717    0   15    0    0    0    5    1    0    4    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   10   63    0   10    0    0    0    0  768   20    1    0
     0    3    0]
 [   0    0    3    0    0    1    4    0    2    0   17 2182    0    1
     0    0    0]
 [   0    0    0    5    9   14    0    0    0    0    2    7  491    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   27    0    0    4    0    0    0    0
    38  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.46612466124661

F1 scores:
[       nan 0.96202532 0.97919121 0.93481095 0.97931034 0.95259096
 0.97619048 0.98039216 0.99652375 0.66666667 0.91756272 0.98266156
 0.94696239 0.98930481 0.98271392 0.88535032 0.96551724]

Kappa:
0.9596986495955345
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc5f6e55828>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.630, val_acc:0.469]
Epoch [2/120    avg_loss:2.234, val_acc:0.505]
Epoch [3/120    avg_loss:2.011, val_acc:0.533]
Epoch [4/120    avg_loss:1.885, val_acc:0.561]
Epoch [5/120    avg_loss:1.750, val_acc:0.622]
Epoch [6/120    avg_loss:1.627, val_acc:0.662]
Epoch [7/120    avg_loss:1.451, val_acc:0.635]
Epoch [8/120    avg_loss:1.313, val_acc:0.675]
Epoch [9/120    avg_loss:1.167, val_acc:0.732]
Epoch [10/120    avg_loss:1.037, val_acc:0.681]
Epoch [11/120    avg_loss:0.924, val_acc:0.742]
Epoch [12/120    avg_loss:0.901, val_acc:0.706]
Epoch [13/120    avg_loss:0.781, val_acc:0.761]
Epoch [14/120    avg_loss:0.743, val_acc:0.808]
Epoch [15/120    avg_loss:0.734, val_acc:0.797]
Epoch [16/120    avg_loss:0.598, val_acc:0.815]
Epoch [17/120    avg_loss:0.576, val_acc:0.853]
Epoch [18/120    avg_loss:0.453, val_acc:0.860]
Epoch [19/120    avg_loss:0.461, val_acc:0.837]
Epoch [20/120    avg_loss:0.577, val_acc:0.812]
Epoch [21/120    avg_loss:0.424, val_acc:0.866]
Epoch [22/120    avg_loss:0.361, val_acc:0.878]
Epoch [23/120    avg_loss:0.369, val_acc:0.821]
Epoch [24/120    avg_loss:0.356, val_acc:0.864]
Epoch [25/120    avg_loss:0.333, val_acc:0.880]
Epoch [26/120    avg_loss:0.379, val_acc:0.852]
Epoch [27/120    avg_loss:0.291, val_acc:0.897]
Epoch [28/120    avg_loss:0.231, val_acc:0.899]
Epoch [29/120    avg_loss:0.228, val_acc:0.901]
Epoch [30/120    avg_loss:0.223, val_acc:0.918]
Epoch [31/120    avg_loss:0.217, val_acc:0.894]
Epoch [32/120    avg_loss:0.199, val_acc:0.917]
Epoch [33/120    avg_loss:0.199, val_acc:0.919]
Epoch [34/120    avg_loss:0.165, val_acc:0.922]
Epoch [35/120    avg_loss:0.155, val_acc:0.922]
Epoch [36/120    avg_loss:0.166, val_acc:0.928]
Epoch [37/120    avg_loss:0.113, val_acc:0.922]
Epoch [38/120    avg_loss:0.140, val_acc:0.933]
Epoch [39/120    avg_loss:0.127, val_acc:0.922]
Epoch [40/120    avg_loss:0.157, val_acc:0.920]
Epoch [41/120    avg_loss:0.151, val_acc:0.833]
Epoch [42/120    avg_loss:0.263, val_acc:0.919]
Epoch [43/120    avg_loss:0.169, val_acc:0.919]
Epoch [44/120    avg_loss:0.128, val_acc:0.947]
Epoch [45/120    avg_loss:0.124, val_acc:0.938]
Epoch [46/120    avg_loss:0.089, val_acc:0.942]
Epoch [47/120    avg_loss:0.097, val_acc:0.939]
Epoch [48/120    avg_loss:0.109, val_acc:0.935]
Epoch [49/120    avg_loss:0.120, val_acc:0.943]
Epoch [50/120    avg_loss:0.071, val_acc:0.955]
Epoch [51/120    avg_loss:0.086, val_acc:0.941]
Epoch [52/120    avg_loss:0.070, val_acc:0.951]
Epoch [53/120    avg_loss:0.072, val_acc:0.952]
Epoch [54/120    avg_loss:0.062, val_acc:0.954]
Epoch [55/120    avg_loss:0.063, val_acc:0.953]
Epoch [56/120    avg_loss:0.069, val_acc:0.952]
Epoch [57/120    avg_loss:0.056, val_acc:0.957]
Epoch [58/120    avg_loss:0.066, val_acc:0.948]
Epoch [59/120    avg_loss:0.060, val_acc:0.962]
Epoch [60/120    avg_loss:0.060, val_acc:0.958]
Epoch [61/120    avg_loss:0.053, val_acc:0.955]
Epoch [62/120    avg_loss:0.050, val_acc:0.948]
Epoch [63/120    avg_loss:0.072, val_acc:0.946]
Epoch [64/120    avg_loss:0.085, val_acc:0.961]
Epoch [65/120    avg_loss:0.071, val_acc:0.958]
Epoch [66/120    avg_loss:0.084, val_acc:0.948]
Epoch [67/120    avg_loss:0.053, val_acc:0.953]
Epoch [68/120    avg_loss:0.079, val_acc:0.940]
Epoch [69/120    avg_loss:0.077, val_acc:0.944]
Epoch [70/120    avg_loss:0.060, val_acc:0.952]
Epoch [71/120    avg_loss:0.047, val_acc:0.957]
Epoch [72/120    avg_loss:0.038, val_acc:0.959]
Epoch [73/120    avg_loss:0.032, val_acc:0.961]
Epoch [74/120    avg_loss:0.027, val_acc:0.961]
Epoch [75/120    avg_loss:0.027, val_acc:0.966]
Epoch [76/120    avg_loss:0.029, val_acc:0.965]
Epoch [77/120    avg_loss:0.024, val_acc:0.964]
Epoch [78/120    avg_loss:0.034, val_acc:0.966]
Epoch [79/120    avg_loss:0.026, val_acc:0.964]
Epoch [80/120    avg_loss:0.024, val_acc:0.967]
Epoch [81/120    avg_loss:0.022, val_acc:0.967]
Epoch [82/120    avg_loss:0.024, val_acc:0.968]
Epoch [83/120    avg_loss:0.024, val_acc:0.968]
Epoch [84/120    avg_loss:0.023, val_acc:0.967]
Epoch [85/120    avg_loss:0.024, val_acc:0.964]
Epoch [86/120    avg_loss:0.024, val_acc:0.966]
Epoch [87/120    avg_loss:0.024, val_acc:0.967]
Epoch [88/120    avg_loss:0.019, val_acc:0.967]
Epoch [89/120    avg_loss:0.027, val_acc:0.968]
Epoch [90/120    avg_loss:0.021, val_acc:0.969]
Epoch [91/120    avg_loss:0.018, val_acc:0.968]
Epoch [92/120    avg_loss:0.022, val_acc:0.968]
Epoch [93/120    avg_loss:0.023, val_acc:0.968]
Epoch [94/120    avg_loss:0.023, val_acc:0.967]
Epoch [95/120    avg_loss:0.021, val_acc:0.968]
Epoch [96/120    avg_loss:0.018, val_acc:0.966]
Epoch [97/120    avg_loss:0.021, val_acc:0.968]
Epoch [98/120    avg_loss:0.019, val_acc:0.968]
Epoch [99/120    avg_loss:0.020, val_acc:0.968]
Epoch [100/120    avg_loss:0.021, val_acc:0.968]
Epoch [101/120    avg_loss:0.022, val_acc:0.968]
Epoch [102/120    avg_loss:0.019, val_acc:0.968]
Epoch [103/120    avg_loss:0.021, val_acc:0.968]
Epoch [104/120    avg_loss:0.030, val_acc:0.968]
Epoch [105/120    avg_loss:0.024, val_acc:0.968]
Epoch [106/120    avg_loss:0.020, val_acc:0.968]
Epoch [107/120    avg_loss:0.018, val_acc:0.968]
Epoch [108/120    avg_loss:0.019, val_acc:0.968]
Epoch [109/120    avg_loss:0.019, val_acc:0.969]
Epoch [110/120    avg_loss:0.020, val_acc:0.969]
Epoch [111/120    avg_loss:0.019, val_acc:0.969]
Epoch [112/120    avg_loss:0.020, val_acc:0.969]
Epoch [113/120    avg_loss:0.023, val_acc:0.969]
Epoch [114/120    avg_loss:0.018, val_acc:0.969]
Epoch [115/120    avg_loss:0.020, val_acc:0.969]
Epoch [116/120    avg_loss:0.020, val_acc:0.969]
Epoch [117/120    avg_loss:0.019, val_acc:0.968]
Epoch [118/120    avg_loss:0.018, val_acc:0.968]
Epoch [119/120    avg_loss:0.018, val_acc:0.968]
Epoch [120/120    avg_loss:0.019, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    2    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1248    0    1    0    2    0    0    1    6   27    0    0
     0    0    0]
 [   0    0    1  701    0   16    0    0    0   18    4    0    7    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    3    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   13    0    0    1    0
     0    0    0]
 [   0    0   23   60    0    5    1    0    0    0  779    4    0    0
     0    3    0]
 [   0    0   19    0    0    1    5    0    4    0   17 2162    0    1
     1    0    0]
 [   0    0    0    7    4    9    0    0    0    3   12    0  495    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    2    0    0
  1134    0    0]
 [   0    0    0    0    0    0   47    0    0    4    0    0    0    0
    67  229    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.63143631436314

F1 scores:
[       nan 0.94871795 0.9681924  0.92297564 0.98839907 0.96098105
 0.95696572 0.98039216 0.99537037 0.45614035 0.91755006 0.98138901
 0.95467695 0.99730458 0.96881674 0.791019   0.97674419]

Kappa:
0.9501806905184708
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f523c2ac748>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.645, val_acc:0.434]
Epoch [2/120    avg_loss:2.258, val_acc:0.461]
Epoch [3/120    avg_loss:2.029, val_acc:0.532]
Epoch [4/120    avg_loss:1.925, val_acc:0.574]
Epoch [5/120    avg_loss:1.706, val_acc:0.618]
Epoch [6/120    avg_loss:1.609, val_acc:0.609]
Epoch [7/120    avg_loss:1.462, val_acc:0.634]
Epoch [8/120    avg_loss:1.280, val_acc:0.653]
Epoch [9/120    avg_loss:1.201, val_acc:0.686]
Epoch [10/120    avg_loss:1.106, val_acc:0.732]
Epoch [11/120    avg_loss:0.883, val_acc:0.788]
Epoch [12/120    avg_loss:0.839, val_acc:0.778]
Epoch [13/120    avg_loss:0.823, val_acc:0.751]
Epoch [14/120    avg_loss:0.732, val_acc:0.764]
Epoch [15/120    avg_loss:0.721, val_acc:0.821]
Epoch [16/120    avg_loss:0.645, val_acc:0.797]
Epoch [17/120    avg_loss:0.605, val_acc:0.822]
Epoch [18/120    avg_loss:0.581, val_acc:0.842]
Epoch [19/120    avg_loss:0.512, val_acc:0.859]
Epoch [20/120    avg_loss:0.405, val_acc:0.866]
Epoch [21/120    avg_loss:0.396, val_acc:0.864]
Epoch [22/120    avg_loss:0.343, val_acc:0.899]
Epoch [23/120    avg_loss:0.332, val_acc:0.886]
Epoch [24/120    avg_loss:0.374, val_acc:0.859]
Epoch [25/120    avg_loss:0.320, val_acc:0.851]
Epoch [26/120    avg_loss:0.283, val_acc:0.839]
Epoch [27/120    avg_loss:0.275, val_acc:0.883]
Epoch [28/120    avg_loss:0.274, val_acc:0.894]
Epoch [29/120    avg_loss:0.274, val_acc:0.891]
Epoch [30/120    avg_loss:0.231, val_acc:0.894]
Epoch [31/120    avg_loss:0.251, val_acc:0.895]
Epoch [32/120    avg_loss:0.209, val_acc:0.899]
Epoch [33/120    avg_loss:0.210, val_acc:0.912]
Epoch [34/120    avg_loss:0.182, val_acc:0.919]
Epoch [35/120    avg_loss:0.163, val_acc:0.927]
Epoch [36/120    avg_loss:0.143, val_acc:0.919]
Epoch [37/120    avg_loss:0.148, val_acc:0.910]
Epoch [38/120    avg_loss:0.150, val_acc:0.925]
Epoch [39/120    avg_loss:0.115, val_acc:0.916]
Epoch [40/120    avg_loss:0.119, val_acc:0.927]
Epoch [41/120    avg_loss:0.114, val_acc:0.909]
Epoch [42/120    avg_loss:0.117, val_acc:0.925]
Epoch [43/120    avg_loss:0.131, val_acc:0.931]
Epoch [44/120    avg_loss:0.134, val_acc:0.914]
Epoch [45/120    avg_loss:0.243, val_acc:0.845]
Epoch [46/120    avg_loss:0.232, val_acc:0.894]
Epoch [47/120    avg_loss:0.254, val_acc:0.899]
Epoch [48/120    avg_loss:0.169, val_acc:0.926]
Epoch [49/120    avg_loss:0.135, val_acc:0.930]
Epoch [50/120    avg_loss:0.134, val_acc:0.912]
Epoch [51/120    avg_loss:0.133, val_acc:0.927]
Epoch [52/120    avg_loss:0.151, val_acc:0.930]
Epoch [53/120    avg_loss:0.101, val_acc:0.935]
Epoch [54/120    avg_loss:0.114, val_acc:0.927]
Epoch [55/120    avg_loss:0.085, val_acc:0.939]
Epoch [56/120    avg_loss:0.078, val_acc:0.917]
Epoch [57/120    avg_loss:0.106, val_acc:0.917]
Epoch [58/120    avg_loss:0.091, val_acc:0.947]
Epoch [59/120    avg_loss:0.065, val_acc:0.948]
Epoch [60/120    avg_loss:0.070, val_acc:0.949]
Epoch [61/120    avg_loss:0.073, val_acc:0.955]
Epoch [62/120    avg_loss:0.061, val_acc:0.958]
Epoch [63/120    avg_loss:0.050, val_acc:0.955]
Epoch [64/120    avg_loss:0.168, val_acc:0.874]
Epoch [65/120    avg_loss:0.185, val_acc:0.927]
Epoch [66/120    avg_loss:0.126, val_acc:0.918]
Epoch [67/120    avg_loss:0.152, val_acc:0.930]
Epoch [68/120    avg_loss:0.095, val_acc:0.913]
Epoch [69/120    avg_loss:0.085, val_acc:0.937]
Epoch [70/120    avg_loss:0.085, val_acc:0.936]
Epoch [71/120    avg_loss:0.085, val_acc:0.944]
Epoch [72/120    avg_loss:0.092, val_acc:0.940]
Epoch [73/120    avg_loss:0.090, val_acc:0.939]
Epoch [74/120    avg_loss:0.066, val_acc:0.949]
Epoch [75/120    avg_loss:0.057, val_acc:0.955]
Epoch [76/120    avg_loss:0.058, val_acc:0.955]
Epoch [77/120    avg_loss:0.041, val_acc:0.957]
Epoch [78/120    avg_loss:0.042, val_acc:0.958]
Epoch [79/120    avg_loss:0.036, val_acc:0.954]
Epoch [80/120    avg_loss:0.039, val_acc:0.956]
Epoch [81/120    avg_loss:0.031, val_acc:0.956]
Epoch [82/120    avg_loss:0.036, val_acc:0.954]
Epoch [83/120    avg_loss:0.028, val_acc:0.955]
Epoch [84/120    avg_loss:0.029, val_acc:0.956]
Epoch [85/120    avg_loss:0.031, val_acc:0.956]
Epoch [86/120    avg_loss:0.029, val_acc:0.954]
Epoch [87/120    avg_loss:0.028, val_acc:0.954]
Epoch [88/120    avg_loss:0.035, val_acc:0.958]
Epoch [89/120    avg_loss:0.028, val_acc:0.958]
Epoch [90/120    avg_loss:0.032, val_acc:0.959]
Epoch [91/120    avg_loss:0.031, val_acc:0.955]
Epoch [92/120    avg_loss:0.026, val_acc:0.954]
Epoch [93/120    avg_loss:0.028, val_acc:0.959]
Epoch [94/120    avg_loss:0.032, val_acc:0.954]
Epoch [95/120    avg_loss:0.025, val_acc:0.954]
Epoch [96/120    avg_loss:0.025, val_acc:0.956]
Epoch [97/120    avg_loss:0.024, val_acc:0.956]
Epoch [98/120    avg_loss:0.025, val_acc:0.956]
Epoch [99/120    avg_loss:0.029, val_acc:0.955]
Epoch [100/120    avg_loss:0.034, val_acc:0.956]
Epoch [101/120    avg_loss:0.031, val_acc:0.956]
Epoch [102/120    avg_loss:0.025, val_acc:0.959]
Epoch [103/120    avg_loss:0.031, val_acc:0.959]
Epoch [104/120    avg_loss:0.028, val_acc:0.957]
Epoch [105/120    avg_loss:0.026, val_acc:0.957]
Epoch [106/120    avg_loss:0.028, val_acc:0.959]
Epoch [107/120    avg_loss:0.030, val_acc:0.956]
Epoch [108/120    avg_loss:0.030, val_acc:0.961]
Epoch [109/120    avg_loss:0.032, val_acc:0.958]
Epoch [110/120    avg_loss:0.028, val_acc:0.956]
Epoch [111/120    avg_loss:0.022, val_acc:0.958]
Epoch [112/120    avg_loss:0.027, val_acc:0.958]
Epoch [113/120    avg_loss:0.025, val_acc:0.954]
Epoch [114/120    avg_loss:0.022, val_acc:0.954]
Epoch [115/120    avg_loss:0.024, val_acc:0.957]
Epoch [116/120    avg_loss:0.025, val_acc:0.956]
Epoch [117/120    avg_loss:0.030, val_acc:0.956]
Epoch [118/120    avg_loss:0.023, val_acc:0.955]
Epoch [119/120    avg_loss:0.019, val_acc:0.958]
Epoch [120/120    avg_loss:0.019, val_acc:0.954]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1233    0    7    0    7    0    0    3    8   22    2    0
     0    3    0]
 [   0    0    1  704    0    6    0    0    0   16    1    0   14    5
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    2    0    0    6    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   13   68    0    6   12    0    0    0  768    4    0    0
     0    4    0]
 [   0    0    6    1    0    0   17    0    8    0   20 2150    3    4
     0    1    0]
 [   0    0    0   27   10    8    0    0    0    2   12    0  466    0
     0    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   11    0    0    1    0    3    1    0    0
  1123    0    0]
 [   0    0    0    0    0    0   40    0    0    0    0    0    0    0
    40  267    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.16531165311653

F1 scores:
[       nan 0.94871795 0.97163121 0.9083871  0.96162528 0.95280899
 0.94320633 1.         0.98847926 0.5        0.90833826 0.97994531
 0.91372549 0.9762533  0.97440347 0.8585209  0.94915254]

Kappa:
0.9449336163224505
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f46f66797b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.634, val_acc:0.461]
Epoch [2/120    avg_loss:2.259, val_acc:0.550]
Epoch [3/120    avg_loss:2.079, val_acc:0.591]
Epoch [4/120    avg_loss:1.834, val_acc:0.619]
Epoch [5/120    avg_loss:1.730, val_acc:0.608]
Epoch [6/120    avg_loss:1.650, val_acc:0.640]
Epoch [7/120    avg_loss:1.443, val_acc:0.646]
Epoch [8/120    avg_loss:1.319, val_acc:0.686]
Epoch [9/120    avg_loss:1.180, val_acc:0.711]
Epoch [10/120    avg_loss:1.051, val_acc:0.753]
Epoch [11/120    avg_loss:0.902, val_acc:0.742]
Epoch [12/120    avg_loss:0.815, val_acc:0.788]
Epoch [13/120    avg_loss:0.657, val_acc:0.804]
Epoch [14/120    avg_loss:0.627, val_acc:0.797]
Epoch [15/120    avg_loss:0.619, val_acc:0.819]
Epoch [16/120    avg_loss:0.575, val_acc:0.805]
Epoch [17/120    avg_loss:0.495, val_acc:0.842]
Epoch [18/120    avg_loss:0.470, val_acc:0.838]
Epoch [19/120    avg_loss:0.468, val_acc:0.843]
Epoch [20/120    avg_loss:0.390, val_acc:0.880]
Epoch [21/120    avg_loss:0.300, val_acc:0.882]
Epoch [22/120    avg_loss:0.315, val_acc:0.863]
Epoch [23/120    avg_loss:0.262, val_acc:0.893]
Epoch [24/120    avg_loss:0.260, val_acc:0.877]
Epoch [25/120    avg_loss:0.315, val_acc:0.881]
Epoch [26/120    avg_loss:0.261, val_acc:0.901]
Epoch [27/120    avg_loss:0.219, val_acc:0.904]
Epoch [28/120    avg_loss:0.175, val_acc:0.913]
Epoch [29/120    avg_loss:0.214, val_acc:0.902]
Epoch [30/120    avg_loss:0.164, val_acc:0.910]
Epoch [31/120    avg_loss:0.184, val_acc:0.903]
Epoch [32/120    avg_loss:0.158, val_acc:0.926]
Epoch [33/120    avg_loss:0.194, val_acc:0.921]
Epoch [34/120    avg_loss:0.145, val_acc:0.912]
Epoch [35/120    avg_loss:0.145, val_acc:0.930]
Epoch [36/120    avg_loss:0.148, val_acc:0.913]
Epoch [37/120    avg_loss:0.157, val_acc:0.939]
Epoch [38/120    avg_loss:0.107, val_acc:0.935]
Epoch [39/120    avg_loss:0.096, val_acc:0.939]
Epoch [40/120    avg_loss:0.084, val_acc:0.938]
Epoch [41/120    avg_loss:0.083, val_acc:0.943]
Epoch [42/120    avg_loss:0.077, val_acc:0.939]
Epoch [43/120    avg_loss:0.254, val_acc:0.889]
Epoch [44/120    avg_loss:0.294, val_acc:0.880]
Epoch [45/120    avg_loss:0.183, val_acc:0.914]
Epoch [46/120    avg_loss:0.129, val_acc:0.934]
Epoch [47/120    avg_loss:0.312, val_acc:0.880]
Epoch [48/120    avg_loss:0.302, val_acc:0.908]
Epoch [49/120    avg_loss:0.139, val_acc:0.929]
Epoch [50/120    avg_loss:0.098, val_acc:0.931]
Epoch [51/120    avg_loss:0.102, val_acc:0.925]
Epoch [52/120    avg_loss:0.104, val_acc:0.926]
Epoch [53/120    avg_loss:0.102, val_acc:0.935]
Epoch [54/120    avg_loss:0.108, val_acc:0.938]
Epoch [55/120    avg_loss:0.067, val_acc:0.944]
Epoch [56/120    avg_loss:0.064, val_acc:0.949]
Epoch [57/120    avg_loss:0.055, val_acc:0.955]
Epoch [58/120    avg_loss:0.053, val_acc:0.956]
Epoch [59/120    avg_loss:0.055, val_acc:0.952]
Epoch [60/120    avg_loss:0.056, val_acc:0.953]
Epoch [61/120    avg_loss:0.054, val_acc:0.955]
Epoch [62/120    avg_loss:0.052, val_acc:0.950]
Epoch [63/120    avg_loss:0.046, val_acc:0.954]
Epoch [64/120    avg_loss:0.044, val_acc:0.954]
Epoch [65/120    avg_loss:0.048, val_acc:0.955]
Epoch [66/120    avg_loss:0.053, val_acc:0.956]
Epoch [67/120    avg_loss:0.049, val_acc:0.952]
Epoch [68/120    avg_loss:0.046, val_acc:0.952]
Epoch [69/120    avg_loss:0.049, val_acc:0.950]
Epoch [70/120    avg_loss:0.050, val_acc:0.953]
Epoch [71/120    avg_loss:0.040, val_acc:0.952]
Epoch [72/120    avg_loss:0.040, val_acc:0.954]
Epoch [73/120    avg_loss:0.039, val_acc:0.959]
Epoch [74/120    avg_loss:0.039, val_acc:0.957]
Epoch [75/120    avg_loss:0.047, val_acc:0.957]
Epoch [76/120    avg_loss:0.050, val_acc:0.955]
Epoch [77/120    avg_loss:0.044, val_acc:0.958]
Epoch [78/120    avg_loss:0.035, val_acc:0.959]
Epoch [79/120    avg_loss:0.043, val_acc:0.958]
Epoch [80/120    avg_loss:0.039, val_acc:0.955]
Epoch [81/120    avg_loss:0.037, val_acc:0.956]
Epoch [82/120    avg_loss:0.049, val_acc:0.959]
Epoch [83/120    avg_loss:0.037, val_acc:0.958]
Epoch [84/120    avg_loss:0.038, val_acc:0.955]
Epoch [85/120    avg_loss:0.040, val_acc:0.955]
Epoch [86/120    avg_loss:0.038, val_acc:0.959]
Epoch [87/120    avg_loss:0.037, val_acc:0.957]
Epoch [88/120    avg_loss:0.035, val_acc:0.959]
Epoch [89/120    avg_loss:0.043, val_acc:0.958]
Epoch [90/120    avg_loss:0.037, val_acc:0.958]
Epoch [91/120    avg_loss:0.036, val_acc:0.957]
Epoch [92/120    avg_loss:0.032, val_acc:0.961]
Epoch [93/120    avg_loss:0.039, val_acc:0.956]
Epoch [94/120    avg_loss:0.033, val_acc:0.958]
Epoch [95/120    avg_loss:0.039, val_acc:0.958]
Epoch [96/120    avg_loss:0.032, val_acc:0.961]
Epoch [97/120    avg_loss:0.033, val_acc:0.955]
Epoch [98/120    avg_loss:0.039, val_acc:0.959]
Epoch [99/120    avg_loss:0.036, val_acc:0.958]
Epoch [100/120    avg_loss:0.035, val_acc:0.962]
Epoch [101/120    avg_loss:0.035, val_acc:0.961]
Epoch [102/120    avg_loss:0.032, val_acc:0.958]
Epoch [103/120    avg_loss:0.030, val_acc:0.961]
Epoch [104/120    avg_loss:0.034, val_acc:0.959]
Epoch [105/120    avg_loss:0.034, val_acc:0.961]
Epoch [106/120    avg_loss:0.031, val_acc:0.962]
Epoch [107/120    avg_loss:0.032, val_acc:0.963]
Epoch [108/120    avg_loss:0.032, val_acc:0.962]
Epoch [109/120    avg_loss:0.031, val_acc:0.962]
Epoch [110/120    avg_loss:0.030, val_acc:0.959]
Epoch [111/120    avg_loss:0.028, val_acc:0.959]
Epoch [112/120    avg_loss:0.027, val_acc:0.958]
Epoch [113/120    avg_loss:0.029, val_acc:0.961]
Epoch [114/120    avg_loss:0.030, val_acc:0.959]
Epoch [115/120    avg_loss:0.033, val_acc:0.962]
Epoch [116/120    avg_loss:0.029, val_acc:0.961]
Epoch [117/120    avg_loss:0.029, val_acc:0.962]
Epoch [118/120    avg_loss:0.031, val_acc:0.959]
Epoch [119/120    avg_loss:0.032, val_acc:0.961]
Epoch [120/120    avg_loss:0.029, val_acc:0.962]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1244    0    8    0    3    0    0    0    7   23    0    0
     0    0    0]
 [   0    0    1  724    1    1    0    0    0   12    0    0    6    1
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    0    0    9    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    1    0    0   12    0    0    2    0
     0    0    0]
 [   0    0   42   67    0    2    0    0    0    0  753    5    4    0
     0    2    0]
 [   0    0    9    0    0    0    5    0    0    0   27 2168    0    1
     0    0    0]
 [   0    0    0   31    9    0    0    0    0    0   19    1  470    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   18    0    0    0    0    3    0    0    0
  1118    0    0]
 [   0    0    0    0    0    0   12    0    0    7    0    0    0    0
    44  284    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.70731707317073

F1 scores:
[       nan 0.975      0.96396745 0.92111959 0.95945946 0.96245734
 0.98426966 1.         1.         0.4137931  0.89323843 0.98388927
 0.92519685 0.99462366 0.97006508 0.89731438 0.97674419]

Kappa:
0.9510647256643457
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd7b9d4b828>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.672, val_acc:0.491]
Epoch [2/120    avg_loss:2.259, val_acc:0.522]
Epoch [3/120    avg_loss:2.054, val_acc:0.579]
Epoch [4/120    avg_loss:1.867, val_acc:0.611]
Epoch [5/120    avg_loss:1.766, val_acc:0.625]
Epoch [6/120    avg_loss:1.620, val_acc:0.655]
Epoch [7/120    avg_loss:1.453, val_acc:0.677]
Epoch [8/120    avg_loss:1.280, val_acc:0.716]
Epoch [9/120    avg_loss:1.092, val_acc:0.711]
Epoch [10/120    avg_loss:0.983, val_acc:0.743]
Epoch [11/120    avg_loss:0.949, val_acc:0.747]
Epoch [12/120    avg_loss:0.896, val_acc:0.762]
Epoch [13/120    avg_loss:0.770, val_acc:0.791]
Epoch [14/120    avg_loss:0.688, val_acc:0.775]
Epoch [15/120    avg_loss:0.623, val_acc:0.805]
Epoch [16/120    avg_loss:0.609, val_acc:0.826]
Epoch [17/120    avg_loss:0.547, val_acc:0.797]
Epoch [18/120    avg_loss:0.493, val_acc:0.816]
Epoch [19/120    avg_loss:0.422, val_acc:0.857]
Epoch [20/120    avg_loss:0.450, val_acc:0.844]
Epoch [21/120    avg_loss:0.455, val_acc:0.861]
Epoch [22/120    avg_loss:0.371, val_acc:0.843]
Epoch [23/120    avg_loss:0.332, val_acc:0.870]
Epoch [24/120    avg_loss:0.311, val_acc:0.883]
Epoch [25/120    avg_loss:0.273, val_acc:0.870]
Epoch [26/120    avg_loss:0.310, val_acc:0.890]
Epoch [27/120    avg_loss:0.254, val_acc:0.906]
Epoch [28/120    avg_loss:0.232, val_acc:0.896]
Epoch [29/120    avg_loss:0.234, val_acc:0.910]
Epoch [30/120    avg_loss:0.221, val_acc:0.942]
Epoch [31/120    avg_loss:0.176, val_acc:0.910]
Epoch [32/120    avg_loss:0.228, val_acc:0.866]
Epoch [33/120    avg_loss:0.370, val_acc:0.866]
Epoch [34/120    avg_loss:0.333, val_acc:0.906]
Epoch [35/120    avg_loss:0.284, val_acc:0.909]
Epoch [36/120    avg_loss:0.343, val_acc:0.894]
Epoch [37/120    avg_loss:0.264, val_acc:0.857]
Epoch [38/120    avg_loss:0.227, val_acc:0.886]
Epoch [39/120    avg_loss:0.175, val_acc:0.929]
Epoch [40/120    avg_loss:0.198, val_acc:0.916]
Epoch [41/120    avg_loss:0.161, val_acc:0.894]
Epoch [42/120    avg_loss:0.134, val_acc:0.930]
Epoch [43/120    avg_loss:0.123, val_acc:0.928]
Epoch [44/120    avg_loss:0.098, val_acc:0.946]
Epoch [45/120    avg_loss:0.086, val_acc:0.948]
Epoch [46/120    avg_loss:0.086, val_acc:0.948]
Epoch [47/120    avg_loss:0.086, val_acc:0.955]
Epoch [48/120    avg_loss:0.075, val_acc:0.952]
Epoch [49/120    avg_loss:0.080, val_acc:0.953]
Epoch [50/120    avg_loss:0.075, val_acc:0.953]
Epoch [51/120    avg_loss:0.076, val_acc:0.954]
Epoch [52/120    avg_loss:0.081, val_acc:0.955]
Epoch [53/120    avg_loss:0.074, val_acc:0.961]
Epoch [54/120    avg_loss:0.071, val_acc:0.956]
Epoch [55/120    avg_loss:0.069, val_acc:0.957]
Epoch [56/120    avg_loss:0.068, val_acc:0.955]
Epoch [57/120    avg_loss:0.071, val_acc:0.955]
Epoch [58/120    avg_loss:0.064, val_acc:0.955]
Epoch [59/120    avg_loss:0.074, val_acc:0.954]
Epoch [60/120    avg_loss:0.073, val_acc:0.954]
Epoch [61/120    avg_loss:0.066, val_acc:0.951]
Epoch [62/120    avg_loss:0.068, val_acc:0.956]
Epoch [63/120    avg_loss:0.069, val_acc:0.953]
Epoch [64/120    avg_loss:0.065, val_acc:0.954]
Epoch [65/120    avg_loss:0.066, val_acc:0.955]
Epoch [66/120    avg_loss:0.064, val_acc:0.955]
Epoch [67/120    avg_loss:0.072, val_acc:0.954]
Epoch [68/120    avg_loss:0.068, val_acc:0.954]
Epoch [69/120    avg_loss:0.066, val_acc:0.956]
Epoch [70/120    avg_loss:0.056, val_acc:0.956]
Epoch [71/120    avg_loss:0.059, val_acc:0.955]
Epoch [72/120    avg_loss:0.064, val_acc:0.955]
Epoch [73/120    avg_loss:0.068, val_acc:0.955]
Epoch [74/120    avg_loss:0.061, val_acc:0.957]
Epoch [75/120    avg_loss:0.062, val_acc:0.957]
Epoch [76/120    avg_loss:0.061, val_acc:0.956]
Epoch [77/120    avg_loss:0.063, val_acc:0.956]
Epoch [78/120    avg_loss:0.063, val_acc:0.957]
Epoch [79/120    avg_loss:0.058, val_acc:0.957]
Epoch [80/120    avg_loss:0.064, val_acc:0.957]
Epoch [81/120    avg_loss:0.063, val_acc:0.957]
Epoch [82/120    avg_loss:0.058, val_acc:0.957]
Epoch [83/120    avg_loss:0.065, val_acc:0.957]
Epoch [84/120    avg_loss:0.065, val_acc:0.957]
Epoch [85/120    avg_loss:0.058, val_acc:0.957]
Epoch [86/120    avg_loss:0.081, val_acc:0.957]
Epoch [87/120    avg_loss:0.060, val_acc:0.957]
Epoch [88/120    avg_loss:0.057, val_acc:0.957]
Epoch [89/120    avg_loss:0.056, val_acc:0.957]
Epoch [90/120    avg_loss:0.057, val_acc:0.957]
Epoch [91/120    avg_loss:0.060, val_acc:0.957]
Epoch [92/120    avg_loss:0.059, val_acc:0.957]
Epoch [93/120    avg_loss:0.058, val_acc:0.957]
Epoch [94/120    avg_loss:0.056, val_acc:0.957]
Epoch [95/120    avg_loss:0.063, val_acc:0.957]
Epoch [96/120    avg_loss:0.062, val_acc:0.957]
Epoch [97/120    avg_loss:0.058, val_acc:0.957]
Epoch [98/120    avg_loss:0.058, val_acc:0.957]
Epoch [99/120    avg_loss:0.060, val_acc:0.957]
Epoch [100/120    avg_loss:0.062, val_acc:0.957]
Epoch [101/120    avg_loss:0.056, val_acc:0.957]
Epoch [102/120    avg_loss:0.059, val_acc:0.957]
Epoch [103/120    avg_loss:0.057, val_acc:0.957]
Epoch [104/120    avg_loss:0.061, val_acc:0.957]
Epoch [105/120    avg_loss:0.058, val_acc:0.957]
Epoch [106/120    avg_loss:0.055, val_acc:0.957]
Epoch [107/120    avg_loss:0.055, val_acc:0.957]
Epoch [108/120    avg_loss:0.059, val_acc:0.957]
Epoch [109/120    avg_loss:0.056, val_acc:0.957]
Epoch [110/120    avg_loss:0.060, val_acc:0.957]
Epoch [111/120    avg_loss:0.056, val_acc:0.957]
Epoch [112/120    avg_loss:0.065, val_acc:0.957]
Epoch [113/120    avg_loss:0.059, val_acc:0.957]
Epoch [114/120    avg_loss:0.059, val_acc:0.957]
Epoch [115/120    avg_loss:0.059, val_acc:0.957]
Epoch [116/120    avg_loss:0.059, val_acc:0.957]
Epoch [117/120    avg_loss:0.058, val_acc:0.957]
Epoch [118/120    avg_loss:0.062, val_acc:0.957]
Epoch [119/120    avg_loss:0.061, val_acc:0.957]
Epoch [120/120    avg_loss:0.058, val_acc:0.957]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1178    6    1    0    7    0    0    3   13   61    8    0
     0    8    0]
 [   0    0   14  685    2   18    0    0    0    7    1    0   14    3
     0    3    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   56   48    0   10    0    0    0    0  721   31    0    0
     1    8    0]
 [   0    0   15    0    0    3    7    0    0    0    7 2173    0    3
     2    0    0]
 [   0    0    0   18    5    4    0    0    0    0    0    1  503    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   11    0    0    0    0    3    0    0    0
  1125    0    0]
 [   0    0    0    0    0    0   25    0    0    6    0    0    0    0
    37  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.73170731707317

F1 scores:
[       nan 0.98765432 0.92464678 0.91090426 0.98156682 0.94633078
 0.96812454 0.98039216 0.99649942 0.64150943 0.88957434 0.97030587
 0.94548872 0.98404255 0.97613883 0.86511628 0.97647059]

Kappa:
0.9398947993084754
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f72f4a6f898>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.630, val_acc:0.400]
Epoch [2/120    avg_loss:2.256, val_acc:0.511]
Epoch [3/120    avg_loss:2.019, val_acc:0.526]
Epoch [4/120    avg_loss:1.935, val_acc:0.555]
Epoch [5/120    avg_loss:1.748, val_acc:0.602]
Epoch [6/120    avg_loss:1.582, val_acc:0.627]
Epoch [7/120    avg_loss:1.401, val_acc:0.649]
Epoch [8/120    avg_loss:1.278, val_acc:0.646]
Epoch [9/120    avg_loss:1.077, val_acc:0.687]
Epoch [10/120    avg_loss:0.950, val_acc:0.722]
Epoch [11/120    avg_loss:0.885, val_acc:0.727]
Epoch [12/120    avg_loss:0.845, val_acc:0.717]
Epoch [13/120    avg_loss:0.903, val_acc:0.705]
Epoch [14/120    avg_loss:0.748, val_acc:0.777]
Epoch [15/120    avg_loss:0.604, val_acc:0.768]
Epoch [16/120    avg_loss:0.602, val_acc:0.785]
Epoch [17/120    avg_loss:0.562, val_acc:0.806]
Epoch [18/120    avg_loss:0.538, val_acc:0.834]
Epoch [19/120    avg_loss:0.496, val_acc:0.789]
Epoch [20/120    avg_loss:0.443, val_acc:0.811]
Epoch [21/120    avg_loss:0.408, val_acc:0.865]
Epoch [22/120    avg_loss:0.370, val_acc:0.852]
Epoch [23/120    avg_loss:0.350, val_acc:0.868]
Epoch [24/120    avg_loss:0.315, val_acc:0.842]
Epoch [25/120    avg_loss:0.384, val_acc:0.849]
Epoch [26/120    avg_loss:0.357, val_acc:0.868]
Epoch [27/120    avg_loss:0.308, val_acc:0.893]
Epoch [28/120    avg_loss:0.310, val_acc:0.889]
Epoch [29/120    avg_loss:0.274, val_acc:0.823]
Epoch [30/120    avg_loss:0.301, val_acc:0.885]
Epoch [31/120    avg_loss:0.240, val_acc:0.907]
Epoch [32/120    avg_loss:0.220, val_acc:0.889]
Epoch [33/120    avg_loss:0.253, val_acc:0.875]
Epoch [34/120    avg_loss:0.199, val_acc:0.917]
Epoch [35/120    avg_loss:0.233, val_acc:0.913]
Epoch [36/120    avg_loss:0.156, val_acc:0.920]
Epoch [37/120    avg_loss:0.195, val_acc:0.874]
Epoch [38/120    avg_loss:0.209, val_acc:0.889]
Epoch [39/120    avg_loss:0.162, val_acc:0.925]
Epoch [40/120    avg_loss:0.151, val_acc:0.916]
Epoch [41/120    avg_loss:0.170, val_acc:0.907]
Epoch [42/120    avg_loss:0.180, val_acc:0.928]
Epoch [43/120    avg_loss:0.138, val_acc:0.911]
Epoch [44/120    avg_loss:0.123, val_acc:0.936]
Epoch [45/120    avg_loss:0.123, val_acc:0.916]
Epoch [46/120    avg_loss:0.136, val_acc:0.937]
Epoch [47/120    avg_loss:0.110, val_acc:0.947]
Epoch [48/120    avg_loss:0.107, val_acc:0.937]
Epoch [49/120    avg_loss:0.110, val_acc:0.938]
Epoch [50/120    avg_loss:0.098, val_acc:0.935]
Epoch [51/120    avg_loss:0.099, val_acc:0.944]
Epoch [52/120    avg_loss:0.098, val_acc:0.952]
Epoch [53/120    avg_loss:0.149, val_acc:0.883]
Epoch [54/120    avg_loss:0.170, val_acc:0.930]
Epoch [55/120    avg_loss:0.116, val_acc:0.931]
Epoch [56/120    avg_loss:0.088, val_acc:0.939]
Epoch [57/120    avg_loss:0.087, val_acc:0.935]
Epoch [58/120    avg_loss:0.081, val_acc:0.948]
Epoch [59/120    avg_loss:0.084, val_acc:0.938]
Epoch [60/120    avg_loss:0.086, val_acc:0.947]
Epoch [61/120    avg_loss:0.080, val_acc:0.964]
Epoch [62/120    avg_loss:0.064, val_acc:0.941]
Epoch [63/120    avg_loss:0.070, val_acc:0.957]
Epoch [64/120    avg_loss:0.077, val_acc:0.945]
Epoch [65/120    avg_loss:0.066, val_acc:0.956]
Epoch [66/120    avg_loss:0.072, val_acc:0.963]
Epoch [67/120    avg_loss:0.067, val_acc:0.954]
Epoch [68/120    avg_loss:0.059, val_acc:0.955]
Epoch [69/120    avg_loss:0.090, val_acc:0.921]
Epoch [70/120    avg_loss:0.170, val_acc:0.928]
Epoch [71/120    avg_loss:0.086, val_acc:0.943]
Epoch [72/120    avg_loss:0.060, val_acc:0.943]
Epoch [73/120    avg_loss:0.060, val_acc:0.953]
Epoch [74/120    avg_loss:0.073, val_acc:0.953]
Epoch [75/120    avg_loss:0.051, val_acc:0.957]
Epoch [76/120    avg_loss:0.040, val_acc:0.961]
Epoch [77/120    avg_loss:0.042, val_acc:0.961]
Epoch [78/120    avg_loss:0.042, val_acc:0.957]
Epoch [79/120    avg_loss:0.032, val_acc:0.958]
Epoch [80/120    avg_loss:0.032, val_acc:0.958]
Epoch [81/120    avg_loss:0.036, val_acc:0.961]
Epoch [82/120    avg_loss:0.034, val_acc:0.957]
Epoch [83/120    avg_loss:0.035, val_acc:0.959]
Epoch [84/120    avg_loss:0.030, val_acc:0.959]
Epoch [85/120    avg_loss:0.031, val_acc:0.959]
Epoch [86/120    avg_loss:0.032, val_acc:0.961]
Epoch [87/120    avg_loss:0.027, val_acc:0.963]
Epoch [88/120    avg_loss:0.033, val_acc:0.963]
Epoch [89/120    avg_loss:0.029, val_acc:0.962]
Epoch [90/120    avg_loss:0.032, val_acc:0.963]
Epoch [91/120    avg_loss:0.037, val_acc:0.962]
Epoch [92/120    avg_loss:0.038, val_acc:0.962]
Epoch [93/120    avg_loss:0.027, val_acc:0.961]
Epoch [94/120    avg_loss:0.031, val_acc:0.961]
Epoch [95/120    avg_loss:0.033, val_acc:0.962]
Epoch [96/120    avg_loss:0.034, val_acc:0.962]
Epoch [97/120    avg_loss:0.027, val_acc:0.963]
Epoch [98/120    avg_loss:0.028, val_acc:0.963]
Epoch [99/120    avg_loss:0.030, val_acc:0.963]
Epoch [100/120    avg_loss:0.028, val_acc:0.963]
Epoch [101/120    avg_loss:0.029, val_acc:0.963]
Epoch [102/120    avg_loss:0.028, val_acc:0.964]
Epoch [103/120    avg_loss:0.028, val_acc:0.964]
Epoch [104/120    avg_loss:0.031, val_acc:0.964]
Epoch [105/120    avg_loss:0.033, val_acc:0.964]
Epoch [106/120    avg_loss:0.035, val_acc:0.964]
Epoch [107/120    avg_loss:0.035, val_acc:0.964]
Epoch [108/120    avg_loss:0.029, val_acc:0.964]
Epoch [109/120    avg_loss:0.031, val_acc:0.964]
Epoch [110/120    avg_loss:0.029, val_acc:0.964]
Epoch [111/120    avg_loss:0.031, val_acc:0.964]
Epoch [112/120    avg_loss:0.031, val_acc:0.964]
Epoch [113/120    avg_loss:0.034, val_acc:0.963]
Epoch [114/120    avg_loss:0.033, val_acc:0.963]
Epoch [115/120    avg_loss:0.030, val_acc:0.964]
Epoch [116/120    avg_loss:0.028, val_acc:0.963]
Epoch [117/120    avg_loss:0.031, val_acc:0.964]
Epoch [118/120    avg_loss:0.032, val_acc:0.964]
Epoch [119/120    avg_loss:0.034, val_acc:0.964]
Epoch [120/120    avg_loss:0.028, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1232    4    0    0    1    0    0    1    5   40    0    0
     0    2    0]
 [   0    0    2  675    5   13    0    0    0   10    0    0   38    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    3    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   14    0    0    4    0
     0    0    0]
 [   0    0   30   89    0    7    0    0    0    0  727   14    1    0
     0    7    0]
 [   0    5    8    0    0    2   11    0    2    0    7 2171    0    4
     0    0    0]
 [   0    0    0    1    3    8    0    0    0    0   12    1  499    0
     0    0   10]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    1    0    0
  1136    0    0]
 [   0    0    0    0    0    0   37    0    0    2    0    0    0    0
    99  209    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.53658536585365

F1 scores:
[       nan 0.91764706 0.96362925 0.89050132 0.98156682 0.96089385
 0.96100074 1.         0.99652375 0.58333333 0.89257213 0.97770772
 0.9266481  0.97883598 0.95622896 0.73982301 0.93785311]

Kappa:
0.9376637275723358
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa4d9c2e5c0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.685, val_acc:0.493]
Epoch [2/120    avg_loss:2.271, val_acc:0.481]
Epoch [3/120    avg_loss:2.043, val_acc:0.577]
Epoch [4/120    avg_loss:1.916, val_acc:0.591]
Epoch [5/120    avg_loss:1.748, val_acc:0.619]
Epoch [6/120    avg_loss:1.683, val_acc:0.625]
Epoch [7/120    avg_loss:1.504, val_acc:0.643]
Epoch [8/120    avg_loss:1.382, val_acc:0.655]
Epoch [9/120    avg_loss:1.235, val_acc:0.679]
Epoch [10/120    avg_loss:1.045, val_acc:0.738]
Epoch [11/120    avg_loss:0.942, val_acc:0.741]
Epoch [12/120    avg_loss:0.874, val_acc:0.766]
Epoch [13/120    avg_loss:0.718, val_acc:0.782]
Epoch [14/120    avg_loss:0.687, val_acc:0.760]
Epoch [15/120    avg_loss:0.618, val_acc:0.776]
Epoch [16/120    avg_loss:0.611, val_acc:0.830]
Epoch [17/120    avg_loss:0.643, val_acc:0.823]
Epoch [18/120    avg_loss:0.540, val_acc:0.832]
Epoch [19/120    avg_loss:0.594, val_acc:0.716]
Epoch [20/120    avg_loss:0.567, val_acc:0.810]
Epoch [21/120    avg_loss:0.451, val_acc:0.848]
Epoch [22/120    avg_loss:0.445, val_acc:0.875]
Epoch [23/120    avg_loss:0.413, val_acc:0.813]
Epoch [24/120    avg_loss:0.402, val_acc:0.884]
Epoch [25/120    avg_loss:0.298, val_acc:0.886]
Epoch [26/120    avg_loss:0.387, val_acc:0.843]
Epoch [27/120    avg_loss:0.398, val_acc:0.833]
Epoch [28/120    avg_loss:0.358, val_acc:0.870]
Epoch [29/120    avg_loss:0.369, val_acc:0.874]
Epoch [30/120    avg_loss:0.288, val_acc:0.900]
Epoch [31/120    avg_loss:0.261, val_acc:0.907]
Epoch [32/120    avg_loss:0.256, val_acc:0.891]
Epoch [33/120    avg_loss:0.248, val_acc:0.886]
Epoch [34/120    avg_loss:0.269, val_acc:0.907]
Epoch [35/120    avg_loss:0.235, val_acc:0.916]
Epoch [36/120    avg_loss:0.230, val_acc:0.904]
Epoch [37/120    avg_loss:0.210, val_acc:0.900]
Epoch [38/120    avg_loss:0.169, val_acc:0.916]
Epoch [39/120    avg_loss:0.188, val_acc:0.905]
Epoch [40/120    avg_loss:0.212, val_acc:0.911]
Epoch [41/120    avg_loss:0.181, val_acc:0.939]
Epoch [42/120    avg_loss:0.153, val_acc:0.925]
Epoch [43/120    avg_loss:0.239, val_acc:0.857]
Epoch [44/120    avg_loss:0.279, val_acc:0.899]
Epoch [45/120    avg_loss:0.184, val_acc:0.937]
Epoch [46/120    avg_loss:0.184, val_acc:0.934]
Epoch [47/120    avg_loss:0.139, val_acc:0.953]
Epoch [48/120    avg_loss:0.124, val_acc:0.947]
Epoch [49/120    avg_loss:0.167, val_acc:0.926]
Epoch [50/120    avg_loss:0.143, val_acc:0.923]
Epoch [51/120    avg_loss:0.117, val_acc:0.944]
Epoch [52/120    avg_loss:0.134, val_acc:0.947]
Epoch [53/120    avg_loss:0.134, val_acc:0.930]
Epoch [54/120    avg_loss:0.149, val_acc:0.939]
Epoch [55/120    avg_loss:0.183, val_acc:0.914]
Epoch [56/120    avg_loss:0.140, val_acc:0.943]
Epoch [57/120    avg_loss:0.137, val_acc:0.946]
Epoch [58/120    avg_loss:0.123, val_acc:0.939]
Epoch [59/120    avg_loss:0.120, val_acc:0.932]
Epoch [60/120    avg_loss:0.095, val_acc:0.964]
Epoch [61/120    avg_loss:0.102, val_acc:0.957]
Epoch [62/120    avg_loss:0.100, val_acc:0.959]
Epoch [63/120    avg_loss:0.133, val_acc:0.917]
Epoch [64/120    avg_loss:0.103, val_acc:0.944]
Epoch [65/120    avg_loss:0.093, val_acc:0.970]
Epoch [66/120    avg_loss:0.091, val_acc:0.957]
Epoch [67/120    avg_loss:0.091, val_acc:0.958]
Epoch [68/120    avg_loss:0.061, val_acc:0.972]
Epoch [69/120    avg_loss:0.082, val_acc:0.956]
Epoch [70/120    avg_loss:0.074, val_acc:0.955]
Epoch [71/120    avg_loss:0.082, val_acc:0.954]
Epoch [72/120    avg_loss:0.071, val_acc:0.963]
Epoch [73/120    avg_loss:0.052, val_acc:0.973]
Epoch [74/120    avg_loss:0.072, val_acc:0.958]
Epoch [75/120    avg_loss:0.065, val_acc:0.971]
Epoch [76/120    avg_loss:0.072, val_acc:0.961]
Epoch [77/120    avg_loss:0.092, val_acc:0.937]
Epoch [78/120    avg_loss:0.116, val_acc:0.957]
Epoch [79/120    avg_loss:0.101, val_acc:0.963]
Epoch [80/120    avg_loss:0.081, val_acc:0.950]
Epoch [81/120    avg_loss:0.065, val_acc:0.953]
Epoch [82/120    avg_loss:0.049, val_acc:0.968]
Epoch [83/120    avg_loss:0.052, val_acc:0.970]
Epoch [84/120    avg_loss:0.046, val_acc:0.977]
Epoch [85/120    avg_loss:0.041, val_acc:0.981]
Epoch [86/120    avg_loss:0.035, val_acc:0.984]
Epoch [87/120    avg_loss:0.042, val_acc:0.970]
Epoch [88/120    avg_loss:0.040, val_acc:0.988]
Epoch [89/120    avg_loss:0.047, val_acc:0.974]
Epoch [90/120    avg_loss:0.046, val_acc:0.983]
Epoch [91/120    avg_loss:0.033, val_acc:0.968]
Epoch [92/120    avg_loss:0.034, val_acc:0.980]
Epoch [93/120    avg_loss:0.033, val_acc:0.977]
Epoch [94/120    avg_loss:0.040, val_acc:0.972]
Epoch [95/120    avg_loss:0.034, val_acc:0.972]
Epoch [96/120    avg_loss:0.033, val_acc:0.970]
Epoch [97/120    avg_loss:0.039, val_acc:0.981]
Epoch [98/120    avg_loss:0.037, val_acc:0.944]
Epoch [99/120    avg_loss:0.081, val_acc:0.972]
Epoch [100/120    avg_loss:0.048, val_acc:0.974]
Epoch [101/120    avg_loss:0.029, val_acc:0.983]
Epoch [102/120    avg_loss:0.023, val_acc:0.984]
Epoch [103/120    avg_loss:0.020, val_acc:0.983]
Epoch [104/120    avg_loss:0.019, val_acc:0.986]
Epoch [105/120    avg_loss:0.023, val_acc:0.986]
Epoch [106/120    avg_loss:0.020, val_acc:0.986]
Epoch [107/120    avg_loss:0.020, val_acc:0.988]
Epoch [108/120    avg_loss:0.019, val_acc:0.986]
Epoch [109/120    avg_loss:0.018, val_acc:0.986]
Epoch [110/120    avg_loss:0.017, val_acc:0.986]
Epoch [111/120    avg_loss:0.019, val_acc:0.986]
Epoch [112/120    avg_loss:0.019, val_acc:0.986]
Epoch [113/120    avg_loss:0.015, val_acc:0.986]
Epoch [114/120    avg_loss:0.016, val_acc:0.986]
Epoch [115/120    avg_loss:0.019, val_acc:0.986]
Epoch [116/120    avg_loss:0.016, val_acc:0.988]
Epoch [117/120    avg_loss:0.016, val_acc:0.988]
Epoch [118/120    avg_loss:0.015, val_acc:0.986]
Epoch [119/120    avg_loss:0.019, val_acc:0.985]
Epoch [120/120    avg_loss:0.020, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1257    5    0    0    2    0    0    0    2   15    4    0
     0    0    0]
 [   0    0    2  657    1   13    0    0    0   11    0    0   53   10
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    3    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0    8   87    0    6    0    0    0    0  767    2    0    0
     0    5    0]
 [   0    0   13    0    0    3    9    0    7    0   20 2152    1    2
     3    0    0]
 [   0    0    0    0    3   12    0    0    0    0    6    0  508    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    2    0    0    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   15    0    0    7    0    0    0    0
    75  250    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.42547425474255

F1 scores:
[       nan 0.96202532 0.979735   0.87834225 0.99069767 0.95671476
 0.97907324 1.         0.98964327 0.55555556 0.91746411 0.9824241
 0.92112421 0.96858639 0.96516568 0.83056478 0.97109827]

Kappa:
0.9478797273045162
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcaa1ad2860>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.655, val_acc:0.298]
Epoch [2/120    avg_loss:2.303, val_acc:0.521]
Epoch [3/120    avg_loss:2.073, val_acc:0.524]
Epoch [4/120    avg_loss:1.920, val_acc:0.593]
Epoch [5/120    avg_loss:1.772, val_acc:0.592]
Epoch [6/120    avg_loss:1.667, val_acc:0.605]
Epoch [7/120    avg_loss:1.579, val_acc:0.619]
Epoch [8/120    avg_loss:1.402, val_acc:0.649]
Epoch [9/120    avg_loss:1.290, val_acc:0.667]
Epoch [10/120    avg_loss:1.088, val_acc:0.676]
Epoch [11/120    avg_loss:1.024, val_acc:0.708]
Epoch [12/120    avg_loss:0.970, val_acc:0.732]
Epoch [13/120    avg_loss:0.822, val_acc:0.705]
Epoch [14/120    avg_loss:0.840, val_acc:0.717]
Epoch [15/120    avg_loss:0.821, val_acc:0.770]
Epoch [16/120    avg_loss:0.714, val_acc:0.791]
Epoch [17/120    avg_loss:0.708, val_acc:0.819]
Epoch [18/120    avg_loss:0.629, val_acc:0.742]
Epoch [19/120    avg_loss:0.606, val_acc:0.768]
Epoch [20/120    avg_loss:0.610, val_acc:0.807]
Epoch [21/120    avg_loss:0.543, val_acc:0.839]
Epoch [22/120    avg_loss:0.460, val_acc:0.839]
Epoch [23/120    avg_loss:0.423, val_acc:0.846]
Epoch [24/120    avg_loss:0.374, val_acc:0.881]
Epoch [25/120    avg_loss:0.379, val_acc:0.847]
Epoch [26/120    avg_loss:0.391, val_acc:0.852]
Epoch [27/120    avg_loss:0.421, val_acc:0.825]
Epoch [28/120    avg_loss:0.341, val_acc:0.868]
Epoch [29/120    avg_loss:0.329, val_acc:0.841]
Epoch [30/120    avg_loss:0.350, val_acc:0.846]
Epoch [31/120    avg_loss:0.334, val_acc:0.904]
Epoch [32/120    avg_loss:0.289, val_acc:0.907]
Epoch [33/120    avg_loss:0.247, val_acc:0.889]
Epoch [34/120    avg_loss:0.244, val_acc:0.890]
Epoch [35/120    avg_loss:0.275, val_acc:0.895]
Epoch [36/120    avg_loss:0.229, val_acc:0.886]
Epoch [37/120    avg_loss:0.227, val_acc:0.923]
Epoch [38/120    avg_loss:0.216, val_acc:0.925]
Epoch [39/120    avg_loss:0.190, val_acc:0.923]
Epoch [40/120    avg_loss:0.180, val_acc:0.919]
Epoch [41/120    avg_loss:0.167, val_acc:0.905]
Epoch [42/120    avg_loss:0.213, val_acc:0.916]
Epoch [43/120    avg_loss:0.319, val_acc:0.892]
Epoch [44/120    avg_loss:0.197, val_acc:0.912]
Epoch [45/120    avg_loss:0.157, val_acc:0.922]
Epoch [46/120    avg_loss:0.134, val_acc:0.946]
Epoch [47/120    avg_loss:0.127, val_acc:0.932]
Epoch [48/120    avg_loss:0.123, val_acc:0.938]
Epoch [49/120    avg_loss:0.122, val_acc:0.914]
Epoch [50/120    avg_loss:0.132, val_acc:0.945]
Epoch [51/120    avg_loss:0.118, val_acc:0.938]
Epoch [52/120    avg_loss:0.150, val_acc:0.881]
Epoch [53/120    avg_loss:0.177, val_acc:0.899]
Epoch [54/120    avg_loss:0.165, val_acc:0.932]
Epoch [55/120    avg_loss:0.145, val_acc:0.944]
Epoch [56/120    avg_loss:0.114, val_acc:0.941]
Epoch [57/120    avg_loss:0.099, val_acc:0.945]
Epoch [58/120    avg_loss:0.098, val_acc:0.952]
Epoch [59/120    avg_loss:0.078, val_acc:0.958]
Epoch [60/120    avg_loss:0.080, val_acc:0.955]
Epoch [61/120    avg_loss:0.063, val_acc:0.953]
Epoch [62/120    avg_loss:0.082, val_acc:0.949]
Epoch [63/120    avg_loss:0.070, val_acc:0.950]
Epoch [64/120    avg_loss:0.059, val_acc:0.941]
Epoch [65/120    avg_loss:0.071, val_acc:0.952]
Epoch [66/120    avg_loss:0.103, val_acc:0.943]
Epoch [67/120    avg_loss:0.074, val_acc:0.941]
Epoch [68/120    avg_loss:0.079, val_acc:0.965]
Epoch [69/120    avg_loss:0.078, val_acc:0.962]
Epoch [70/120    avg_loss:0.073, val_acc:0.952]
Epoch [71/120    avg_loss:0.098, val_acc:0.963]
Epoch [72/120    avg_loss:0.077, val_acc:0.943]
Epoch [73/120    avg_loss:0.060, val_acc:0.953]
Epoch [74/120    avg_loss:0.056, val_acc:0.965]
Epoch [75/120    avg_loss:0.055, val_acc:0.966]
Epoch [76/120    avg_loss:0.054, val_acc:0.959]
Epoch [77/120    avg_loss:0.050, val_acc:0.955]
Epoch [78/120    avg_loss:0.069, val_acc:0.959]
Epoch [79/120    avg_loss:0.058, val_acc:0.944]
Epoch [80/120    avg_loss:0.056, val_acc:0.972]
Epoch [81/120    avg_loss:0.063, val_acc:0.955]
Epoch [82/120    avg_loss:0.072, val_acc:0.956]
Epoch [83/120    avg_loss:0.056, val_acc:0.967]
Epoch [84/120    avg_loss:0.078, val_acc:0.922]
Epoch [85/120    avg_loss:0.143, val_acc:0.955]
Epoch [86/120    avg_loss:0.069, val_acc:0.945]
Epoch [87/120    avg_loss:0.050, val_acc:0.947]
Epoch [88/120    avg_loss:0.060, val_acc:0.928]
Epoch [89/120    avg_loss:0.058, val_acc:0.957]
Epoch [90/120    avg_loss:0.053, val_acc:0.959]
Epoch [91/120    avg_loss:0.038, val_acc:0.966]
Epoch [92/120    avg_loss:0.034, val_acc:0.974]
Epoch [93/120    avg_loss:0.036, val_acc:0.971]
Epoch [94/120    avg_loss:0.035, val_acc:0.970]
Epoch [95/120    avg_loss:0.028, val_acc:0.972]
Epoch [96/120    avg_loss:0.041, val_acc:0.972]
Epoch [97/120    avg_loss:0.049, val_acc:0.945]
Epoch [98/120    avg_loss:0.065, val_acc:0.964]
Epoch [99/120    avg_loss:0.054, val_acc:0.959]
Epoch [100/120    avg_loss:0.037, val_acc:0.974]
Epoch [101/120    avg_loss:0.034, val_acc:0.976]
Epoch [102/120    avg_loss:0.031, val_acc:0.965]
Epoch [103/120    avg_loss:0.032, val_acc:0.972]
Epoch [104/120    avg_loss:0.032, val_acc:0.972]
Epoch [105/120    avg_loss:0.038, val_acc:0.968]
Epoch [106/120    avg_loss:0.115, val_acc:0.955]
Epoch [107/120    avg_loss:0.076, val_acc:0.961]
Epoch [108/120    avg_loss:0.111, val_acc:0.950]
Epoch [109/120    avg_loss:0.058, val_acc:0.952]
Epoch [110/120    avg_loss:0.051, val_acc:0.948]
Epoch [111/120    avg_loss:0.044, val_acc:0.974]
Epoch [112/120    avg_loss:0.030, val_acc:0.972]
Epoch [113/120    avg_loss:0.031, val_acc:0.974]
Epoch [114/120    avg_loss:0.027, val_acc:0.970]
Epoch [115/120    avg_loss:0.022, val_acc:0.970]
Epoch [116/120    avg_loss:0.024, val_acc:0.975]
Epoch [117/120    avg_loss:0.019, val_acc:0.975]
Epoch [118/120    avg_loss:0.022, val_acc:0.974]
Epoch [119/120    avg_loss:0.018, val_acc:0.974]
Epoch [120/120    avg_loss:0.017, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1234    6    0    0    4    0    0    0    1   36    4    0
     0    0    0]
 [   0    0    0  694    1    5    0    0    0   15    0    0   32    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    5    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   21   89    0    6    0    0    0    0  755    1    2    0
     0    1    0]
 [   0    0   11    0    0    4    3    0    0    0   24 2164    0    4
     0    0    0]
 [   0    0    2    1    2    2    0    0    0    0    2   17  501    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    1    0    4    3    0    0
  1123    0    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
   130  208    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.8509485094851

F1 scores:
[       nan 0.96202532 0.96670584 0.90305791 0.99300699 0.96279594
 0.98568199 0.90909091 0.99883856 0.61538462 0.90745192 0.97609382
 0.93122677 0.98930481 0.93896321 0.74820144 0.95402299]

Kappa:
0.9412404536983224
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:09:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f27ec40d7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.649, val_acc:0.363]
Epoch [2/120    avg_loss:2.245, val_acc:0.497]
Epoch [3/120    avg_loss:2.019, val_acc:0.529]
Epoch [4/120    avg_loss:1.901, val_acc:0.548]
Epoch [5/120    avg_loss:1.762, val_acc:0.577]
Epoch [6/120    avg_loss:1.654, val_acc:0.581]
Epoch [7/120    avg_loss:1.479, val_acc:0.597]
Epoch [8/120    avg_loss:1.419, val_acc:0.615]
Epoch [9/120    avg_loss:1.237, val_acc:0.680]
Epoch [10/120    avg_loss:1.111, val_acc:0.668]
Epoch [11/120    avg_loss:0.961, val_acc:0.707]
Epoch [12/120    avg_loss:0.953, val_acc:0.646]
Epoch [13/120    avg_loss:0.858, val_acc:0.660]
Epoch [14/120    avg_loss:0.861, val_acc:0.736]
Epoch [15/120    avg_loss:0.733, val_acc:0.752]
Epoch [16/120    avg_loss:0.676, val_acc:0.744]
Epoch [17/120    avg_loss:0.610, val_acc:0.801]
Epoch [18/120    avg_loss:0.593, val_acc:0.788]
Epoch [19/120    avg_loss:0.516, val_acc:0.797]
Epoch [20/120    avg_loss:0.567, val_acc:0.816]
Epoch [21/120    avg_loss:0.442, val_acc:0.829]
Epoch [22/120    avg_loss:0.410, val_acc:0.850]
Epoch [23/120    avg_loss:0.449, val_acc:0.830]
Epoch [24/120    avg_loss:0.435, val_acc:0.828]
Epoch [25/120    avg_loss:0.415, val_acc:0.828]
Epoch [26/120    avg_loss:0.303, val_acc:0.875]
Epoch [27/120    avg_loss:0.333, val_acc:0.886]
Epoch [28/120    avg_loss:0.284, val_acc:0.857]
Epoch [29/120    avg_loss:0.301, val_acc:0.815]
Epoch [30/120    avg_loss:0.404, val_acc:0.876]
Epoch [31/120    avg_loss:0.292, val_acc:0.874]
Epoch [32/120    avg_loss:0.295, val_acc:0.918]
Epoch [33/120    avg_loss:0.274, val_acc:0.877]
Epoch [34/120    avg_loss:0.217, val_acc:0.887]
Epoch [35/120    avg_loss:0.219, val_acc:0.895]
Epoch [36/120    avg_loss:0.215, val_acc:0.880]
Epoch [37/120    avg_loss:0.170, val_acc:0.925]
Epoch [38/120    avg_loss:0.258, val_acc:0.899]
Epoch [39/120    avg_loss:0.199, val_acc:0.894]
Epoch [40/120    avg_loss:0.187, val_acc:0.913]
Epoch [41/120    avg_loss:0.171, val_acc:0.892]
Epoch [42/120    avg_loss:0.199, val_acc:0.882]
Epoch [43/120    avg_loss:0.193, val_acc:0.909]
Epoch [44/120    avg_loss:0.167, val_acc:0.920]
Epoch [45/120    avg_loss:0.192, val_acc:0.928]
Epoch [46/120    avg_loss:0.167, val_acc:0.908]
Epoch [47/120    avg_loss:0.157, val_acc:0.887]
Epoch [48/120    avg_loss:0.147, val_acc:0.919]
Epoch [49/120    avg_loss:0.119, val_acc:0.939]
Epoch [50/120    avg_loss:0.143, val_acc:0.914]
Epoch [51/120    avg_loss:0.150, val_acc:0.917]
Epoch [52/120    avg_loss:0.101, val_acc:0.931]
Epoch [53/120    avg_loss:0.123, val_acc:0.873]
Epoch [54/120    avg_loss:0.114, val_acc:0.928]
Epoch [55/120    avg_loss:0.116, val_acc:0.917]
Epoch [56/120    avg_loss:0.117, val_acc:0.930]
Epoch [57/120    avg_loss:0.120, val_acc:0.921]
Epoch [58/120    avg_loss:0.104, val_acc:0.929]
Epoch [59/120    avg_loss:0.148, val_acc:0.912]
Epoch [60/120    avg_loss:0.133, val_acc:0.922]
Epoch [61/120    avg_loss:0.127, val_acc:0.936]
Epoch [62/120    avg_loss:0.087, val_acc:0.939]
Epoch [63/120    avg_loss:0.089, val_acc:0.949]
Epoch [64/120    avg_loss:0.059, val_acc:0.953]
Epoch [65/120    avg_loss:0.060, val_acc:0.944]
Epoch [66/120    avg_loss:0.067, val_acc:0.945]
Epoch [67/120    avg_loss:0.088, val_acc:0.943]
Epoch [68/120    avg_loss:0.078, val_acc:0.914]
Epoch [69/120    avg_loss:0.083, val_acc:0.952]
Epoch [70/120    avg_loss:0.059, val_acc:0.949]
Epoch [71/120    avg_loss:0.095, val_acc:0.949]
Epoch [72/120    avg_loss:0.075, val_acc:0.947]
Epoch [73/120    avg_loss:0.087, val_acc:0.935]
Epoch [74/120    avg_loss:0.079, val_acc:0.948]
Epoch [75/120    avg_loss:0.063, val_acc:0.962]
Epoch [76/120    avg_loss:0.069, val_acc:0.957]
Epoch [77/120    avg_loss:0.067, val_acc:0.946]
Epoch [78/120    avg_loss:0.047, val_acc:0.957]
Epoch [79/120    avg_loss:0.050, val_acc:0.959]
Epoch [80/120    avg_loss:0.059, val_acc:0.949]
Epoch [81/120    avg_loss:0.048, val_acc:0.959]
Epoch [82/120    avg_loss:0.049, val_acc:0.947]
Epoch [83/120    avg_loss:0.036, val_acc:0.965]
Epoch [84/120    avg_loss:0.061, val_acc:0.941]
Epoch [85/120    avg_loss:0.142, val_acc:0.948]
Epoch [86/120    avg_loss:0.093, val_acc:0.956]
Epoch [87/120    avg_loss:0.064, val_acc:0.956]
Epoch [88/120    avg_loss:0.044, val_acc:0.956]
Epoch [89/120    avg_loss:0.049, val_acc:0.954]
Epoch [90/120    avg_loss:0.045, val_acc:0.950]
Epoch [91/120    avg_loss:0.059, val_acc:0.955]
Epoch [92/120    avg_loss:0.068, val_acc:0.950]
Epoch [93/120    avg_loss:0.050, val_acc:0.946]
Epoch [94/120    avg_loss:0.067, val_acc:0.953]
Epoch [95/120    avg_loss:0.115, val_acc:0.922]
Epoch [96/120    avg_loss:0.088, val_acc:0.950]
Epoch [97/120    avg_loss:0.045, val_acc:0.952]
Epoch [98/120    avg_loss:0.032, val_acc:0.957]
Epoch [99/120    avg_loss:0.035, val_acc:0.957]
Epoch [100/120    avg_loss:0.030, val_acc:0.956]
Epoch [101/120    avg_loss:0.039, val_acc:0.959]
Epoch [102/120    avg_loss:0.027, val_acc:0.962]
Epoch [103/120    avg_loss:0.037, val_acc:0.961]
Epoch [104/120    avg_loss:0.024, val_acc:0.961]
Epoch [105/120    avg_loss:0.029, val_acc:0.965]
Epoch [106/120    avg_loss:0.023, val_acc:0.964]
Epoch [107/120    avg_loss:0.024, val_acc:0.966]
Epoch [108/120    avg_loss:0.030, val_acc:0.965]
Epoch [109/120    avg_loss:0.023, val_acc:0.967]
Epoch [110/120    avg_loss:0.026, val_acc:0.967]
Epoch [111/120    avg_loss:0.024, val_acc:0.967]
Epoch [112/120    avg_loss:0.034, val_acc:0.968]
Epoch [113/120    avg_loss:0.028, val_acc:0.968]
Epoch [114/120    avg_loss:0.026, val_acc:0.968]
Epoch [115/120    avg_loss:0.027, val_acc:0.968]
Epoch [116/120    avg_loss:0.025, val_acc:0.968]
Epoch [117/120    avg_loss:0.023, val_acc:0.970]
Epoch [118/120    avg_loss:0.027, val_acc:0.968]
Epoch [119/120    avg_loss:0.020, val_acc:0.970]
Epoch [120/120    avg_loss:0.019, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1247    2    0    1    3    0    0    0    5   27    0    0
     0    0    0]
 [   0    0    1  702    2   25    0    0    0    5    0    0    9    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    1    5    0    3    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   30   90    0    5    0    0    0    0  741    5    0    0
     3    1    0]
 [   0    0   20    0    0    0    7    0    2    0    2 2176    0    2
     1    0    0]
 [   0    0    5   25   11    7    0    0    0    0    8    0  467    0
     0    0   11]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   17    0    0    2    0    0    0    0    0
  1120    0    0]
 [   0    0    0    0    0    0   11    0    0    2    0    0    0    6
   119  209    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.710027100271

F1 scores:
[       nan 0.98765432 0.96367852 0.89655172 0.97038724 0.92778993
 0.98277154 0.90909091 0.99537037 0.7826087  0.90808824 0.98506111
 0.92475248 0.97112861 0.93920335 0.75044883 0.93854749]

Kappa:
0.9396464462598713
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f98ecb28898>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.665, val_acc:0.381]
Epoch [2/120    avg_loss:2.246, val_acc:0.448]
Epoch [3/120    avg_loss:2.052, val_acc:0.539]
Epoch [4/120    avg_loss:1.866, val_acc:0.582]
Epoch [5/120    avg_loss:1.715, val_acc:0.599]
Epoch [6/120    avg_loss:1.612, val_acc:0.611]
Epoch [7/120    avg_loss:1.406, val_acc:0.628]
Epoch [8/120    avg_loss:1.326, val_acc:0.641]
Epoch [9/120    avg_loss:1.215, val_acc:0.669]
Epoch [10/120    avg_loss:1.048, val_acc:0.687]
Epoch [11/120    avg_loss:0.949, val_acc:0.723]
Epoch [12/120    avg_loss:0.869, val_acc:0.731]
Epoch [13/120    avg_loss:0.800, val_acc:0.792]
Epoch [14/120    avg_loss:0.705, val_acc:0.793]
Epoch [15/120    avg_loss:0.629, val_acc:0.802]
Epoch [16/120    avg_loss:0.613, val_acc:0.819]
Epoch [17/120    avg_loss:0.596, val_acc:0.806]
Epoch [18/120    avg_loss:0.533, val_acc:0.833]
Epoch [19/120    avg_loss:0.509, val_acc:0.801]
Epoch [20/120    avg_loss:0.509, val_acc:0.828]
Epoch [21/120    avg_loss:0.431, val_acc:0.861]
Epoch [22/120    avg_loss:0.478, val_acc:0.841]
Epoch [23/120    avg_loss:0.381, val_acc:0.836]
Epoch [24/120    avg_loss:0.449, val_acc:0.773]
Epoch [25/120    avg_loss:0.495, val_acc:0.840]
Epoch [26/120    avg_loss:0.383, val_acc:0.874]
Epoch [27/120    avg_loss:0.302, val_acc:0.894]
Epoch [28/120    avg_loss:0.297, val_acc:0.882]
Epoch [29/120    avg_loss:0.271, val_acc:0.876]
Epoch [30/120    avg_loss:0.263, val_acc:0.887]
Epoch [31/120    avg_loss:0.225, val_acc:0.898]
Epoch [32/120    avg_loss:0.236, val_acc:0.885]
Epoch [33/120    avg_loss:0.295, val_acc:0.889]
Epoch [34/120    avg_loss:0.210, val_acc:0.903]
Epoch [35/120    avg_loss:0.219, val_acc:0.887]
Epoch [36/120    avg_loss:0.211, val_acc:0.909]
Epoch [37/120    avg_loss:0.160, val_acc:0.916]
Epoch [38/120    avg_loss:0.158, val_acc:0.923]
Epoch [39/120    avg_loss:0.170, val_acc:0.917]
Epoch [40/120    avg_loss:0.135, val_acc:0.925]
Epoch [41/120    avg_loss:0.215, val_acc:0.873]
Epoch [42/120    avg_loss:0.230, val_acc:0.907]
Epoch [43/120    avg_loss:0.176, val_acc:0.919]
Epoch [44/120    avg_loss:0.160, val_acc:0.919]
Epoch [45/120    avg_loss:0.297, val_acc:0.884]
Epoch [46/120    avg_loss:0.306, val_acc:0.850]
Epoch [47/120    avg_loss:0.283, val_acc:0.896]
Epoch [48/120    avg_loss:0.178, val_acc:0.914]
Epoch [49/120    avg_loss:0.162, val_acc:0.923]
Epoch [50/120    avg_loss:0.129, val_acc:0.937]
Epoch [51/120    avg_loss:0.113, val_acc:0.925]
Epoch [52/120    avg_loss:0.134, val_acc:0.926]
Epoch [53/120    avg_loss:0.093, val_acc:0.937]
Epoch [54/120    avg_loss:0.173, val_acc:0.935]
Epoch [55/120    avg_loss:0.169, val_acc:0.882]
Epoch [56/120    avg_loss:0.145, val_acc:0.931]
Epoch [57/120    avg_loss:0.111, val_acc:0.932]
Epoch [58/120    avg_loss:0.113, val_acc:0.928]
Epoch [59/120    avg_loss:0.096, val_acc:0.945]
Epoch [60/120    avg_loss:0.084, val_acc:0.954]
Epoch [61/120    avg_loss:0.066, val_acc:0.955]
Epoch [62/120    avg_loss:0.055, val_acc:0.946]
Epoch [63/120    avg_loss:0.066, val_acc:0.936]
Epoch [64/120    avg_loss:0.094, val_acc:0.928]
Epoch [65/120    avg_loss:0.095, val_acc:0.950]
Epoch [66/120    avg_loss:0.070, val_acc:0.950]
Epoch [67/120    avg_loss:0.064, val_acc:0.961]
Epoch [68/120    avg_loss:0.059, val_acc:0.956]
Epoch [69/120    avg_loss:0.060, val_acc:0.954]
Epoch [70/120    avg_loss:0.048, val_acc:0.961]
Epoch [71/120    avg_loss:0.063, val_acc:0.955]
Epoch [72/120    avg_loss:0.054, val_acc:0.961]
Epoch [73/120    avg_loss:0.069, val_acc:0.953]
Epoch [74/120    avg_loss:0.047, val_acc:0.959]
Epoch [75/120    avg_loss:0.054, val_acc:0.956]
Epoch [76/120    avg_loss:0.049, val_acc:0.948]
Epoch [77/120    avg_loss:0.042, val_acc:0.959]
Epoch [78/120    avg_loss:0.039, val_acc:0.961]
Epoch [79/120    avg_loss:0.044, val_acc:0.965]
Epoch [80/120    avg_loss:0.050, val_acc:0.957]
Epoch [81/120    avg_loss:0.043, val_acc:0.964]
Epoch [82/120    avg_loss:0.035, val_acc:0.974]
Epoch [83/120    avg_loss:0.035, val_acc:0.966]
Epoch [84/120    avg_loss:0.031, val_acc:0.966]
Epoch [85/120    avg_loss:0.032, val_acc:0.968]
Epoch [86/120    avg_loss:0.034, val_acc:0.964]
Epoch [87/120    avg_loss:0.036, val_acc:0.970]
Epoch [88/120    avg_loss:0.033, val_acc:0.962]
Epoch [89/120    avg_loss:0.033, val_acc:0.965]
Epoch [90/120    avg_loss:0.033, val_acc:0.971]
Epoch [91/120    avg_loss:0.030, val_acc:0.966]
Epoch [92/120    avg_loss:0.031, val_acc:0.967]
Epoch [93/120    avg_loss:0.031, val_acc:0.972]
Epoch [94/120    avg_loss:0.052, val_acc:0.958]
Epoch [95/120    avg_loss:0.043, val_acc:0.967]
Epoch [96/120    avg_loss:0.041, val_acc:0.970]
Epoch [97/120    avg_loss:0.025, val_acc:0.971]
Epoch [98/120    avg_loss:0.023, val_acc:0.974]
Epoch [99/120    avg_loss:0.023, val_acc:0.973]
Epoch [100/120    avg_loss:0.022, val_acc:0.972]
Epoch [101/120    avg_loss:0.019, val_acc:0.973]
Epoch [102/120    avg_loss:0.020, val_acc:0.975]
Epoch [103/120    avg_loss:0.020, val_acc:0.974]
Epoch [104/120    avg_loss:0.019, val_acc:0.971]
Epoch [105/120    avg_loss:0.021, val_acc:0.973]
Epoch [106/120    avg_loss:0.018, val_acc:0.972]
Epoch [107/120    avg_loss:0.020, val_acc:0.973]
Epoch [108/120    avg_loss:0.019, val_acc:0.976]
Epoch [109/120    avg_loss:0.019, val_acc:0.973]
Epoch [110/120    avg_loss:0.019, val_acc:0.976]
Epoch [111/120    avg_loss:0.019, val_acc:0.974]
Epoch [112/120    avg_loss:0.018, val_acc:0.976]
Epoch [113/120    avg_loss:0.016, val_acc:0.976]
Epoch [114/120    avg_loss:0.018, val_acc:0.976]
Epoch [115/120    avg_loss:0.020, val_acc:0.979]
Epoch [116/120    avg_loss:0.017, val_acc:0.976]
Epoch [117/120    avg_loss:0.018, val_acc:0.976]
Epoch [118/120    avg_loss:0.015, val_acc:0.976]
Epoch [119/120    avg_loss:0.014, val_acc:0.975]
Epoch [120/120    avg_loss:0.015, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1233    6    0    0    7    0    0    0    8   28    3    0
     0    0    0]
 [   0    0    2  730    1    5    0    0    0    3    0    0    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    3    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   14    0    0    4    0
     0    0    0]
 [   0    0   12   74    0    6    0    0    0    0  753   21    0    0
     1    8    0]
 [   0    0    6    0    0    0    6    0    0    0    7 2187    0    2
     2    0    0]
 [   0    0    2   45   11    0    0    0    0    0    7    0  465    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    2    0    0
  1136    0    0]
 [   0    0    0    0    0    0   11    0    0    2    0    0    0    1
   114  219    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.34959349593495

F1 scores:
[       nan 0.96202532 0.97048406 0.9113608  0.97260274 0.98057143
 0.98130142 1.         1.         0.7        0.91107078 0.98314228
 0.91806515 0.9919571  0.94864301 0.7630662  0.97076023]

Kappa:
0.9469061179583116
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7b1f7c5780>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.629, val_acc:0.452]
Epoch [2/120    avg_loss:2.276, val_acc:0.445]
Epoch [3/120    avg_loss:2.007, val_acc:0.530]
Epoch [4/120    avg_loss:1.888, val_acc:0.537]
Epoch [5/120    avg_loss:1.779, val_acc:0.559]
Epoch [6/120    avg_loss:1.626, val_acc:0.607]
Epoch [7/120    avg_loss:1.504, val_acc:0.615]
Epoch [8/120    avg_loss:1.351, val_acc:0.677]
Epoch [9/120    avg_loss:1.174, val_acc:0.660]
Epoch [10/120    avg_loss:1.090, val_acc:0.669]
Epoch [11/120    avg_loss:0.988, val_acc:0.707]
Epoch [12/120    avg_loss:0.935, val_acc:0.723]
Epoch [13/120    avg_loss:0.820, val_acc:0.758]
Epoch [14/120    avg_loss:0.758, val_acc:0.744]
Epoch [15/120    avg_loss:0.714, val_acc:0.770]
Epoch [16/120    avg_loss:0.574, val_acc:0.810]
Epoch [17/120    avg_loss:0.547, val_acc:0.804]
Epoch [18/120    avg_loss:0.501, val_acc:0.815]
Epoch [19/120    avg_loss:0.434, val_acc:0.859]
Epoch [20/120    avg_loss:0.570, val_acc:0.723]
Epoch [21/120    avg_loss:0.515, val_acc:0.836]
Epoch [22/120    avg_loss:0.545, val_acc:0.828]
Epoch [23/120    avg_loss:0.504, val_acc:0.816]
Epoch [24/120    avg_loss:0.365, val_acc:0.865]
Epoch [25/120    avg_loss:0.359, val_acc:0.865]
Epoch [26/120    avg_loss:0.338, val_acc:0.855]
Epoch [27/120    avg_loss:0.335, val_acc:0.855]
Epoch [28/120    avg_loss:0.328, val_acc:0.873]
Epoch [29/120    avg_loss:0.293, val_acc:0.903]
Epoch [30/120    avg_loss:0.290, val_acc:0.883]
Epoch [31/120    avg_loss:0.259, val_acc:0.892]
Epoch [32/120    avg_loss:0.241, val_acc:0.919]
Epoch [33/120    avg_loss:0.201, val_acc:0.914]
Epoch [34/120    avg_loss:0.177, val_acc:0.914]
Epoch [35/120    avg_loss:0.196, val_acc:0.887]
Epoch [36/120    avg_loss:0.183, val_acc:0.927]
Epoch [37/120    avg_loss:0.161, val_acc:0.916]
Epoch [38/120    avg_loss:0.234, val_acc:0.889]
Epoch [39/120    avg_loss:0.255, val_acc:0.892]
Epoch [40/120    avg_loss:0.196, val_acc:0.901]
Epoch [41/120    avg_loss:0.180, val_acc:0.926]
Epoch [42/120    avg_loss:0.166, val_acc:0.931]
Epoch [43/120    avg_loss:0.146, val_acc:0.930]
Epoch [44/120    avg_loss:0.124, val_acc:0.929]
Epoch [45/120    avg_loss:0.184, val_acc:0.919]
Epoch [46/120    avg_loss:0.199, val_acc:0.923]
Epoch [47/120    avg_loss:0.157, val_acc:0.907]
Epoch [48/120    avg_loss:0.111, val_acc:0.929]
Epoch [49/120    avg_loss:0.090, val_acc:0.940]
Epoch [50/120    avg_loss:0.095, val_acc:0.943]
Epoch [51/120    avg_loss:0.081, val_acc:0.946]
Epoch [52/120    avg_loss:0.103, val_acc:0.932]
Epoch [53/120    avg_loss:0.210, val_acc:0.920]
Epoch [54/120    avg_loss:0.201, val_acc:0.905]
Epoch [55/120    avg_loss:0.159, val_acc:0.922]
Epoch [56/120    avg_loss:0.119, val_acc:0.937]
Epoch [57/120    avg_loss:0.099, val_acc:0.939]
Epoch [58/120    avg_loss:0.110, val_acc:0.941]
Epoch [59/120    avg_loss:0.127, val_acc:0.932]
Epoch [60/120    avg_loss:0.088, val_acc:0.949]
Epoch [61/120    avg_loss:0.094, val_acc:0.953]
Epoch [62/120    avg_loss:0.079, val_acc:0.938]
Epoch [63/120    avg_loss:0.079, val_acc:0.949]
Epoch [64/120    avg_loss:0.088, val_acc:0.947]
Epoch [65/120    avg_loss:0.087, val_acc:0.938]
Epoch [66/120    avg_loss:0.103, val_acc:0.944]
Epoch [67/120    avg_loss:0.072, val_acc:0.953]
Epoch [68/120    avg_loss:0.065, val_acc:0.945]
Epoch [69/120    avg_loss:0.098, val_acc:0.950]
Epoch [70/120    avg_loss:0.102, val_acc:0.950]
Epoch [71/120    avg_loss:0.067, val_acc:0.954]
Epoch [72/120    avg_loss:0.056, val_acc:0.954]
Epoch [73/120    avg_loss:0.047, val_acc:0.955]
Epoch [74/120    avg_loss:0.055, val_acc:0.959]
Epoch [75/120    avg_loss:0.064, val_acc:0.952]
Epoch [76/120    avg_loss:0.048, val_acc:0.961]
Epoch [77/120    avg_loss:0.087, val_acc:0.930]
Epoch [78/120    avg_loss:0.085, val_acc:0.956]
Epoch [79/120    avg_loss:0.061, val_acc:0.964]
Epoch [80/120    avg_loss:0.067, val_acc:0.958]
Epoch [81/120    avg_loss:0.043, val_acc:0.956]
Epoch [82/120    avg_loss:0.039, val_acc:0.962]
Epoch [83/120    avg_loss:0.045, val_acc:0.965]
Epoch [84/120    avg_loss:0.092, val_acc:0.964]
Epoch [85/120    avg_loss:0.052, val_acc:0.965]
Epoch [86/120    avg_loss:0.035, val_acc:0.963]
Epoch [87/120    avg_loss:0.039, val_acc:0.950]
Epoch [88/120    avg_loss:0.055, val_acc:0.959]
Epoch [89/120    avg_loss:0.051, val_acc:0.949]
Epoch [90/120    avg_loss:0.048, val_acc:0.952]
Epoch [91/120    avg_loss:0.065, val_acc:0.963]
Epoch [92/120    avg_loss:0.046, val_acc:0.958]
Epoch [93/120    avg_loss:0.035, val_acc:0.967]
Epoch [94/120    avg_loss:0.033, val_acc:0.961]
Epoch [95/120    avg_loss:0.023, val_acc:0.959]
Epoch [96/120    avg_loss:0.027, val_acc:0.966]
Epoch [97/120    avg_loss:0.027, val_acc:0.968]
Epoch [98/120    avg_loss:0.036, val_acc:0.957]
Epoch [99/120    avg_loss:0.033, val_acc:0.962]
Epoch [100/120    avg_loss:0.029, val_acc:0.970]
Epoch [101/120    avg_loss:0.025, val_acc:0.968]
Epoch [102/120    avg_loss:0.032, val_acc:0.956]
Epoch [103/120    avg_loss:0.022, val_acc:0.968]
Epoch [104/120    avg_loss:0.023, val_acc:0.971]
Epoch [105/120    avg_loss:0.020, val_acc:0.976]
Epoch [106/120    avg_loss:0.024, val_acc:0.963]
Epoch [107/120    avg_loss:0.021, val_acc:0.972]
Epoch [108/120    avg_loss:0.018, val_acc:0.971]
Epoch [109/120    avg_loss:0.019, val_acc:0.970]
Epoch [110/120    avg_loss:0.031, val_acc:0.961]
Epoch [111/120    avg_loss:0.038, val_acc:0.965]
Epoch [112/120    avg_loss:0.033, val_acc:0.964]
Epoch [113/120    avg_loss:0.031, val_acc:0.966]
Epoch [114/120    avg_loss:0.025, val_acc:0.959]
Epoch [115/120    avg_loss:0.027, val_acc:0.968]
Epoch [116/120    avg_loss:0.018, val_acc:0.963]
Epoch [117/120    avg_loss:0.024, val_acc:0.971]
Epoch [118/120    avg_loss:0.020, val_acc:0.973]
Epoch [119/120    avg_loss:0.015, val_acc:0.975]
Epoch [120/120    avg_loss:0.012, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1252    6    0    4    5    0    0    2    3   13    0    0
     0    0    0]
 [   0    0    1  711    0   13    0    0    0   15    0    0    5    0
     0    2    0]
 [   0    0    0    0  211    0    2    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    1    0    3    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   12   90    0    7    0    0    0    0  761    0    0    0
     0    5    0]
 [   0    0   14    0    0    3    9    0    0    0   15 2162    0    2
     5    0    0]
 [   0    0    0   15    4    9    0    0    0    1    6    9  479    0
     0    0   11]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    2    0    1    0    2    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0   64    0    0    0    0    0    0    0
    61  222    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.2520325203252

F1 scores:
[       nan 0.93506494 0.97659906 0.90573248 0.98598131 0.94866071
 0.93898062 0.98039216 0.99883856 0.58181818 0.9130174  0.98339777
 0.94013739 0.99462366 0.96716418 0.77083333 0.93854749]

Kappa:
0.9458686182714122
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f37d8c257f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.661, val_acc:0.324]
Epoch [2/120    avg_loss:2.283, val_acc:0.477]
Epoch [3/120    avg_loss:2.049, val_acc:0.546]
Epoch [4/120    avg_loss:1.876, val_acc:0.590]
Epoch [5/120    avg_loss:1.732, val_acc:0.605]
Epoch [6/120    avg_loss:1.565, val_acc:0.620]
Epoch [7/120    avg_loss:1.433, val_acc:0.654]
Epoch [8/120    avg_loss:1.307, val_acc:0.644]
Epoch [9/120    avg_loss:1.183, val_acc:0.698]
Epoch [10/120    avg_loss:1.073, val_acc:0.711]
Epoch [11/120    avg_loss:1.054, val_acc:0.715]
Epoch [12/120    avg_loss:0.884, val_acc:0.741]
Epoch [13/120    avg_loss:0.814, val_acc:0.742]
Epoch [14/120    avg_loss:0.729, val_acc:0.768]
Epoch [15/120    avg_loss:0.677, val_acc:0.774]
Epoch [16/120    avg_loss:0.690, val_acc:0.810]
Epoch [17/120    avg_loss:0.673, val_acc:0.771]
Epoch [18/120    avg_loss:0.578, val_acc:0.824]
Epoch [19/120    avg_loss:0.506, val_acc:0.828]
Epoch [20/120    avg_loss:0.490, val_acc:0.861]
Epoch [21/120    avg_loss:0.475, val_acc:0.843]
Epoch [22/120    avg_loss:0.566, val_acc:0.793]
Epoch [23/120    avg_loss:0.512, val_acc:0.845]
Epoch [24/120    avg_loss:0.403, val_acc:0.861]
Epoch [25/120    avg_loss:0.372, val_acc:0.887]
Epoch [26/120    avg_loss:0.317, val_acc:0.872]
Epoch [27/120    avg_loss:0.304, val_acc:0.901]
Epoch [28/120    avg_loss:0.315, val_acc:0.863]
Epoch [29/120    avg_loss:0.284, val_acc:0.910]
Epoch [30/120    avg_loss:0.266, val_acc:0.880]
Epoch [31/120    avg_loss:0.265, val_acc:0.889]
Epoch [32/120    avg_loss:0.235, val_acc:0.908]
Epoch [33/120    avg_loss:0.222, val_acc:0.892]
Epoch [34/120    avg_loss:0.211, val_acc:0.898]
Epoch [35/120    avg_loss:0.223, val_acc:0.919]
Epoch [36/120    avg_loss:0.198, val_acc:0.922]
Epoch [37/120    avg_loss:0.155, val_acc:0.926]
Epoch [38/120    avg_loss:0.162, val_acc:0.918]
Epoch [39/120    avg_loss:0.152, val_acc:0.920]
Epoch [40/120    avg_loss:0.158, val_acc:0.918]
Epoch [41/120    avg_loss:0.140, val_acc:0.937]
Epoch [42/120    avg_loss:0.158, val_acc:0.904]
Epoch [43/120    avg_loss:0.193, val_acc:0.927]
Epoch [44/120    avg_loss:0.167, val_acc:0.918]
Epoch [45/120    avg_loss:0.158, val_acc:0.927]
Epoch [46/120    avg_loss:0.151, val_acc:0.930]
Epoch [47/120    avg_loss:0.161, val_acc:0.918]
Epoch [48/120    avg_loss:0.189, val_acc:0.925]
Epoch [49/120    avg_loss:0.155, val_acc:0.939]
Epoch [50/120    avg_loss:0.132, val_acc:0.939]
Epoch [51/120    avg_loss:0.116, val_acc:0.941]
Epoch [52/120    avg_loss:0.136, val_acc:0.928]
Epoch [53/120    avg_loss:0.108, val_acc:0.958]
Epoch [54/120    avg_loss:0.110, val_acc:0.931]
Epoch [55/120    avg_loss:0.111, val_acc:0.944]
Epoch [56/120    avg_loss:0.089, val_acc:0.954]
Epoch [57/120    avg_loss:0.079, val_acc:0.961]
Epoch [58/120    avg_loss:0.099, val_acc:0.949]
Epoch [59/120    avg_loss:0.097, val_acc:0.964]
Epoch [60/120    avg_loss:0.067, val_acc:0.959]
Epoch [61/120    avg_loss:0.081, val_acc:0.961]
Epoch [62/120    avg_loss:0.112, val_acc:0.937]
Epoch [63/120    avg_loss:0.103, val_acc:0.954]
Epoch [64/120    avg_loss:0.073, val_acc:0.957]
Epoch [65/120    avg_loss:0.084, val_acc:0.961]
Epoch [66/120    avg_loss:0.094, val_acc:0.938]
Epoch [67/120    avg_loss:0.094, val_acc:0.955]
Epoch [68/120    avg_loss:0.068, val_acc:0.955]
Epoch [69/120    avg_loss:0.064, val_acc:0.956]
Epoch [70/120    avg_loss:0.060, val_acc:0.958]
Epoch [71/120    avg_loss:0.074, val_acc:0.952]
Epoch [72/120    avg_loss:0.068, val_acc:0.957]
Epoch [73/120    avg_loss:0.056, val_acc:0.964]
Epoch [74/120    avg_loss:0.043, val_acc:0.965]
Epoch [75/120    avg_loss:0.041, val_acc:0.965]
Epoch [76/120    avg_loss:0.044, val_acc:0.964]
Epoch [77/120    avg_loss:0.045, val_acc:0.967]
Epoch [78/120    avg_loss:0.036, val_acc:0.967]
Epoch [79/120    avg_loss:0.040, val_acc:0.967]
Epoch [80/120    avg_loss:0.031, val_acc:0.967]
Epoch [81/120    avg_loss:0.041, val_acc:0.966]
Epoch [82/120    avg_loss:0.028, val_acc:0.965]
Epoch [83/120    avg_loss:0.028, val_acc:0.970]
Epoch [84/120    avg_loss:0.036, val_acc:0.970]
Epoch [85/120    avg_loss:0.030, val_acc:0.970]
Epoch [86/120    avg_loss:0.032, val_acc:0.971]
Epoch [87/120    avg_loss:0.031, val_acc:0.970]
Epoch [88/120    avg_loss:0.030, val_acc:0.973]
Epoch [89/120    avg_loss:0.034, val_acc:0.974]
Epoch [90/120    avg_loss:0.029, val_acc:0.973]
Epoch [91/120    avg_loss:0.036, val_acc:0.972]
Epoch [92/120    avg_loss:0.032, val_acc:0.972]
Epoch [93/120    avg_loss:0.028, val_acc:0.974]
Epoch [94/120    avg_loss:0.032, val_acc:0.973]
Epoch [95/120    avg_loss:0.031, val_acc:0.972]
Epoch [96/120    avg_loss:0.032, val_acc:0.973]
Epoch [97/120    avg_loss:0.028, val_acc:0.975]
Epoch [98/120    avg_loss:0.028, val_acc:0.974]
Epoch [99/120    avg_loss:0.032, val_acc:0.971]
Epoch [100/120    avg_loss:0.030, val_acc:0.973]
Epoch [101/120    avg_loss:0.029, val_acc:0.974]
Epoch [102/120    avg_loss:0.029, val_acc:0.975]
Epoch [103/120    avg_loss:0.029, val_acc:0.972]
Epoch [104/120    avg_loss:0.028, val_acc:0.973]
Epoch [105/120    avg_loss:0.024, val_acc:0.975]
Epoch [106/120    avg_loss:0.027, val_acc:0.972]
Epoch [107/120    avg_loss:0.034, val_acc:0.972]
Epoch [108/120    avg_loss:0.032, val_acc:0.974]
Epoch [109/120    avg_loss:0.026, val_acc:0.973]
Epoch [110/120    avg_loss:0.032, val_acc:0.973]
Epoch [111/120    avg_loss:0.027, val_acc:0.975]
Epoch [112/120    avg_loss:0.026, val_acc:0.973]
Epoch [113/120    avg_loss:0.030, val_acc:0.973]
Epoch [114/120    avg_loss:0.034, val_acc:0.975]
Epoch [115/120    avg_loss:0.025, val_acc:0.973]
Epoch [116/120    avg_loss:0.028, val_acc:0.971]
Epoch [117/120    avg_loss:0.026, val_acc:0.973]
Epoch [118/120    avg_loss:0.026, val_acc:0.973]
Epoch [119/120    avg_loss:0.029, val_acc:0.973]
Epoch [120/120    avg_loss:0.027, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1246    0    0    0    7    0    0    1    3   27    0    0
     0    1    0]
 [   0    0    1  707    1   16    0    0    0    5    0    0   14    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    1   85    0    6    0    0    0    3  771    1    0    2
     0    6    0]
 [   0    0   25    0    0    0    6    0    0    0   11 2161    3    4
     0    0    0]
 [   0    0    0    8    7   10    0    0    0    0   11    7  485    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    6    0    0    0    0    0    0    1    0    3    0    0    0
  1129    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    84  263    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.85907859078591

F1 scores:
[       nan 0.91954023 0.9738179  0.91402715 0.98156682 0.96106785
 0.98944193 1.         0.99883856 0.72340426 0.92114695 0.9807125
 0.9344894  0.9762533  0.96003401 0.85251216 0.95953757]

Kappa:
0.9527917005238749
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb727cc87b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.642, val_acc:0.526]
Epoch [2/120    avg_loss:2.258, val_acc:0.571]
Epoch [3/120    avg_loss:2.016, val_acc:0.523]
Epoch [4/120    avg_loss:1.824, val_acc:0.607]
Epoch [5/120    avg_loss:1.695, val_acc:0.614]
Epoch [6/120    avg_loss:1.526, val_acc:0.606]
Epoch [7/120    avg_loss:1.398, val_acc:0.670]
Epoch [8/120    avg_loss:1.251, val_acc:0.685]
Epoch [9/120    avg_loss:1.125, val_acc:0.694]
Epoch [10/120    avg_loss:0.996, val_acc:0.736]
Epoch [11/120    avg_loss:0.890, val_acc:0.714]
Epoch [12/120    avg_loss:0.825, val_acc:0.723]
Epoch [13/120    avg_loss:0.747, val_acc:0.731]
Epoch [14/120    avg_loss:0.761, val_acc:0.771]
Epoch [15/120    avg_loss:0.657, val_acc:0.804]
Epoch [16/120    avg_loss:0.584, val_acc:0.787]
Epoch [17/120    avg_loss:0.544, val_acc:0.813]
Epoch [18/120    avg_loss:0.500, val_acc:0.837]
Epoch [19/120    avg_loss:0.443, val_acc:0.831]
Epoch [20/120    avg_loss:0.376, val_acc:0.849]
Epoch [21/120    avg_loss:0.436, val_acc:0.838]
Epoch [22/120    avg_loss:0.364, val_acc:0.849]
Epoch [23/120    avg_loss:0.384, val_acc:0.845]
Epoch [24/120    avg_loss:0.429, val_acc:0.829]
Epoch [25/120    avg_loss:0.371, val_acc:0.867]
Epoch [26/120    avg_loss:0.300, val_acc:0.868]
Epoch [27/120    avg_loss:0.255, val_acc:0.885]
Epoch [28/120    avg_loss:0.270, val_acc:0.872]
Epoch [29/120    avg_loss:0.248, val_acc:0.889]
Epoch [30/120    avg_loss:0.233, val_acc:0.876]
Epoch [31/120    avg_loss:0.243, val_acc:0.898]
Epoch [32/120    avg_loss:0.226, val_acc:0.893]
Epoch [33/120    avg_loss:0.232, val_acc:0.863]
Epoch [34/120    avg_loss:0.212, val_acc:0.889]
Epoch [35/120    avg_loss:0.199, val_acc:0.917]
Epoch [36/120    avg_loss:0.170, val_acc:0.903]
Epoch [37/120    avg_loss:0.185, val_acc:0.901]
Epoch [38/120    avg_loss:0.174, val_acc:0.919]
Epoch [39/120    avg_loss:0.208, val_acc:0.873]
Epoch [40/120    avg_loss:0.225, val_acc:0.901]
Epoch [41/120    avg_loss:0.185, val_acc:0.908]
Epoch [42/120    avg_loss:0.120, val_acc:0.931]
Epoch [43/120    avg_loss:0.125, val_acc:0.922]
Epoch [44/120    avg_loss:0.132, val_acc:0.913]
Epoch [45/120    avg_loss:0.153, val_acc:0.929]
Epoch [46/120    avg_loss:0.114, val_acc:0.913]
Epoch [47/120    avg_loss:0.122, val_acc:0.922]
Epoch [48/120    avg_loss:0.096, val_acc:0.927]
Epoch [49/120    avg_loss:0.097, val_acc:0.925]
Epoch [50/120    avg_loss:0.082, val_acc:0.941]
Epoch [51/120    avg_loss:0.120, val_acc:0.935]
Epoch [52/120    avg_loss:0.110, val_acc:0.925]
Epoch [53/120    avg_loss:0.124, val_acc:0.930]
Epoch [54/120    avg_loss:0.118, val_acc:0.944]
Epoch [55/120    avg_loss:0.141, val_acc:0.914]
Epoch [56/120    avg_loss:0.106, val_acc:0.926]
Epoch [57/120    avg_loss:0.088, val_acc:0.940]
Epoch [58/120    avg_loss:0.078, val_acc:0.935]
Epoch [59/120    avg_loss:0.070, val_acc:0.949]
Epoch [60/120    avg_loss:0.075, val_acc:0.936]
Epoch [61/120    avg_loss:0.081, val_acc:0.926]
Epoch [62/120    avg_loss:0.068, val_acc:0.937]
Epoch [63/120    avg_loss:0.065, val_acc:0.943]
Epoch [64/120    avg_loss:0.090, val_acc:0.940]
Epoch [65/120    avg_loss:0.066, val_acc:0.949]
Epoch [66/120    avg_loss:0.056, val_acc:0.949]
Epoch [67/120    avg_loss:0.044, val_acc:0.953]
Epoch [68/120    avg_loss:0.049, val_acc:0.941]
Epoch [69/120    avg_loss:0.045, val_acc:0.961]
Epoch [70/120    avg_loss:0.042, val_acc:0.946]
Epoch [71/120    avg_loss:0.053, val_acc:0.943]
Epoch [72/120    avg_loss:0.049, val_acc:0.950]
Epoch [73/120    avg_loss:0.049, val_acc:0.959]
Epoch [74/120    avg_loss:0.064, val_acc:0.947]
Epoch [75/120    avg_loss:0.051, val_acc:0.956]
Epoch [76/120    avg_loss:0.041, val_acc:0.953]
Epoch [77/120    avg_loss:0.035, val_acc:0.955]
Epoch [78/120    avg_loss:0.043, val_acc:0.945]
Epoch [79/120    avg_loss:0.050, val_acc:0.958]
Epoch [80/120    avg_loss:0.040, val_acc:0.947]
Epoch [81/120    avg_loss:0.046, val_acc:0.954]
Epoch [82/120    avg_loss:0.033, val_acc:0.954]
Epoch [83/120    avg_loss:0.028, val_acc:0.950]
Epoch [84/120    avg_loss:0.025, val_acc:0.957]
Epoch [85/120    avg_loss:0.021, val_acc:0.962]
Epoch [86/120    avg_loss:0.024, val_acc:0.959]
Epoch [87/120    avg_loss:0.020, val_acc:0.959]
Epoch [88/120    avg_loss:0.026, val_acc:0.963]
Epoch [89/120    avg_loss:0.021, val_acc:0.961]
Epoch [90/120    avg_loss:0.021, val_acc:0.963]
Epoch [91/120    avg_loss:0.022, val_acc:0.963]
Epoch [92/120    avg_loss:0.021, val_acc:0.962]
Epoch [93/120    avg_loss:0.020, val_acc:0.963]
Epoch [94/120    avg_loss:0.017, val_acc:0.962]
Epoch [95/120    avg_loss:0.022, val_acc:0.962]
Epoch [96/120    avg_loss:0.022, val_acc:0.962]
Epoch [97/120    avg_loss:0.024, val_acc:0.961]
Epoch [98/120    avg_loss:0.026, val_acc:0.963]
Epoch [99/120    avg_loss:0.023, val_acc:0.963]
Epoch [100/120    avg_loss:0.020, val_acc:0.963]
Epoch [101/120    avg_loss:0.020, val_acc:0.964]
Epoch [102/120    avg_loss:0.021, val_acc:0.964]
Epoch [103/120    avg_loss:0.020, val_acc:0.964]
Epoch [104/120    avg_loss:0.018, val_acc:0.964]
Epoch [105/120    avg_loss:0.025, val_acc:0.963]
Epoch [106/120    avg_loss:0.021, val_acc:0.963]
Epoch [107/120    avg_loss:0.021, val_acc:0.963]
Epoch [108/120    avg_loss:0.020, val_acc:0.961]
Epoch [109/120    avg_loss:0.021, val_acc:0.963]
Epoch [110/120    avg_loss:0.021, val_acc:0.965]
Epoch [111/120    avg_loss:0.016, val_acc:0.964]
Epoch [112/120    avg_loss:0.018, val_acc:0.963]
Epoch [113/120    avg_loss:0.017, val_acc:0.962]
Epoch [114/120    avg_loss:0.019, val_acc:0.965]
Epoch [115/120    avg_loss:0.022, val_acc:0.964]
Epoch [116/120    avg_loss:0.020, val_acc:0.963]
Epoch [117/120    avg_loss:0.019, val_acc:0.961]
Epoch [118/120    avg_loss:0.017, val_acc:0.961]
Epoch [119/120    avg_loss:0.022, val_acc:0.959]
Epoch [120/120    avg_loss:0.016, val_acc:0.959]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    1    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1231    2    0    0    7    0    0    0    0   40    5    0
     0    0    0]
 [   0    0    1  677   18    5    0    0    0   14    0    0   27    5
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    3    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   13    0    0    4    0
     0    0    0]
 [   0    0   21   90    0    5    0    0    0    0  749    4    0    0
     0    6    0]
 [   0    0   18    0    0    3   11    0    2    0   18 2154    0    4
     0    0    0]
 [   0    0    2   30    7    5    0    0    0    0   14    0  475    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    3    0    4    0    0    0
  1132    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   136  211    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.28726287262873

F1 scores:
[       nan 0.94871795 0.96209457 0.8752424  0.94456763 0.97162316
 0.98572502 1.         0.99421965 0.54166667 0.90078172 0.97709231
 0.90735435 0.9762533  0.93902945 0.74822695 0.98203593]

Kappa:
0.9348301064579332
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd6f3f2e8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.619, val_acc:0.440]
Epoch [2/120    avg_loss:2.233, val_acc:0.473]
Epoch [3/120    avg_loss:2.068, val_acc:0.487]
Epoch [4/120    avg_loss:1.861, val_acc:0.559]
Epoch [5/120    avg_loss:1.705, val_acc:0.576]
Epoch [6/120    avg_loss:1.585, val_acc:0.591]
Epoch [7/120    avg_loss:1.483, val_acc:0.609]
Epoch [8/120    avg_loss:1.354, val_acc:0.631]
Epoch [9/120    avg_loss:1.213, val_acc:0.707]
Epoch [10/120    avg_loss:1.053, val_acc:0.707]
Epoch [11/120    avg_loss:1.018, val_acc:0.718]
Epoch [12/120    avg_loss:0.915, val_acc:0.697]
Epoch [13/120    avg_loss:0.836, val_acc:0.727]
Epoch [14/120    avg_loss:0.795, val_acc:0.763]
Epoch [15/120    avg_loss:0.721, val_acc:0.767]
Epoch [16/120    avg_loss:0.633, val_acc:0.750]
Epoch [17/120    avg_loss:0.653, val_acc:0.778]
Epoch [18/120    avg_loss:0.620, val_acc:0.797]
Epoch [19/120    avg_loss:0.492, val_acc:0.779]
Epoch [20/120    avg_loss:0.478, val_acc:0.778]
Epoch [21/120    avg_loss:0.464, val_acc:0.821]
Epoch [22/120    avg_loss:0.497, val_acc:0.829]
Epoch [23/120    avg_loss:0.423, val_acc:0.831]
Epoch [24/120    avg_loss:0.395, val_acc:0.859]
Epoch [25/120    avg_loss:0.378, val_acc:0.851]
Epoch [26/120    avg_loss:0.453, val_acc:0.834]
Epoch [27/120    avg_loss:0.321, val_acc:0.800]
Epoch [28/120    avg_loss:0.316, val_acc:0.875]
Epoch [29/120    avg_loss:0.268, val_acc:0.882]
Epoch [30/120    avg_loss:0.269, val_acc:0.863]
Epoch [31/120    avg_loss:0.296, val_acc:0.829]
Epoch [32/120    avg_loss:0.241, val_acc:0.898]
Epoch [33/120    avg_loss:0.278, val_acc:0.891]
Epoch [34/120    avg_loss:0.303, val_acc:0.899]
Epoch [35/120    avg_loss:0.199, val_acc:0.892]
Epoch [36/120    avg_loss:0.216, val_acc:0.882]
Epoch [37/120    avg_loss:0.294, val_acc:0.845]
Epoch [38/120    avg_loss:0.289, val_acc:0.890]
Epoch [39/120    avg_loss:0.199, val_acc:0.891]
Epoch [40/120    avg_loss:0.240, val_acc:0.899]
Epoch [41/120    avg_loss:0.251, val_acc:0.910]
Epoch [42/120    avg_loss:0.170, val_acc:0.926]
Epoch [43/120    avg_loss:0.145, val_acc:0.925]
Epoch [44/120    avg_loss:0.130, val_acc:0.929]
Epoch [45/120    avg_loss:0.124, val_acc:0.938]
Epoch [46/120    avg_loss:0.134, val_acc:0.925]
Epoch [47/120    avg_loss:0.151, val_acc:0.925]
Epoch [48/120    avg_loss:0.120, val_acc:0.941]
Epoch [49/120    avg_loss:0.134, val_acc:0.926]
Epoch [50/120    avg_loss:0.153, val_acc:0.920]
Epoch [51/120    avg_loss:0.121, val_acc:0.920]
Epoch [52/120    avg_loss:0.173, val_acc:0.941]
Epoch [53/120    avg_loss:0.129, val_acc:0.905]
Epoch [54/120    avg_loss:0.118, val_acc:0.941]
Epoch [55/120    avg_loss:0.113, val_acc:0.943]
Epoch [56/120    avg_loss:0.136, val_acc:0.933]
Epoch [57/120    avg_loss:0.079, val_acc:0.950]
Epoch [58/120    avg_loss:0.078, val_acc:0.948]
Epoch [59/120    avg_loss:0.085, val_acc:0.922]
Epoch [60/120    avg_loss:0.084, val_acc:0.900]
Epoch [61/120    avg_loss:0.096, val_acc:0.939]
Epoch [62/120    avg_loss:0.090, val_acc:0.947]
Epoch [63/120    avg_loss:0.107, val_acc:0.941]
Epoch [64/120    avg_loss:0.096, val_acc:0.933]
Epoch [65/120    avg_loss:0.077, val_acc:0.942]
Epoch [66/120    avg_loss:0.078, val_acc:0.942]
Epoch [67/120    avg_loss:0.085, val_acc:0.953]
Epoch [68/120    avg_loss:0.075, val_acc:0.938]
Epoch [69/120    avg_loss:0.061, val_acc:0.958]
Epoch [70/120    avg_loss:0.067, val_acc:0.956]
Epoch [71/120    avg_loss:0.047, val_acc:0.962]
Epoch [72/120    avg_loss:0.059, val_acc:0.951]
Epoch [73/120    avg_loss:0.060, val_acc:0.950]
Epoch [74/120    avg_loss:0.072, val_acc:0.949]
Epoch [75/120    avg_loss:0.093, val_acc:0.942]
Epoch [76/120    avg_loss:0.071, val_acc:0.962]
Epoch [77/120    avg_loss:0.048, val_acc:0.955]
Epoch [78/120    avg_loss:0.048, val_acc:0.962]
Epoch [79/120    avg_loss:0.056, val_acc:0.971]
Epoch [80/120    avg_loss:0.059, val_acc:0.941]
Epoch [81/120    avg_loss:0.057, val_acc:0.943]
Epoch [82/120    avg_loss:0.059, val_acc:0.955]
Epoch [83/120    avg_loss:0.045, val_acc:0.947]
Epoch [84/120    avg_loss:0.079, val_acc:0.917]
Epoch [85/120    avg_loss:0.168, val_acc:0.936]
Epoch [86/120    avg_loss:0.073, val_acc:0.947]
Epoch [87/120    avg_loss:0.092, val_acc:0.938]
Epoch [88/120    avg_loss:0.081, val_acc:0.936]
Epoch [89/120    avg_loss:0.087, val_acc:0.907]
Epoch [90/120    avg_loss:0.126, val_acc:0.917]
Epoch [91/120    avg_loss:0.069, val_acc:0.962]
Epoch [92/120    avg_loss:0.051, val_acc:0.965]
Epoch [93/120    avg_loss:0.044, val_acc:0.968]
Epoch [94/120    avg_loss:0.032, val_acc:0.970]
Epoch [95/120    avg_loss:0.031, val_acc:0.969]
Epoch [96/120    avg_loss:0.035, val_acc:0.968]
Epoch [97/120    avg_loss:0.037, val_acc:0.969]
Epoch [98/120    avg_loss:0.029, val_acc:0.969]
Epoch [99/120    avg_loss:0.036, val_acc:0.966]
Epoch [100/120    avg_loss:0.036, val_acc:0.963]
Epoch [101/120    avg_loss:0.039, val_acc:0.970]
Epoch [102/120    avg_loss:0.030, val_acc:0.972]
Epoch [103/120    avg_loss:0.028, val_acc:0.970]
Epoch [104/120    avg_loss:0.034, val_acc:0.972]
Epoch [105/120    avg_loss:0.032, val_acc:0.973]
Epoch [106/120    avg_loss:0.035, val_acc:0.973]
Epoch [107/120    avg_loss:0.028, val_acc:0.973]
Epoch [108/120    avg_loss:0.027, val_acc:0.976]
Epoch [109/120    avg_loss:0.030, val_acc:0.972]
Epoch [110/120    avg_loss:0.028, val_acc:0.973]
Epoch [111/120    avg_loss:0.027, val_acc:0.976]
Epoch [112/120    avg_loss:0.026, val_acc:0.975]
Epoch [113/120    avg_loss:0.032, val_acc:0.973]
Epoch [114/120    avg_loss:0.030, val_acc:0.975]
Epoch [115/120    avg_loss:0.028, val_acc:0.973]
Epoch [116/120    avg_loss:0.030, val_acc:0.976]
Epoch [117/120    avg_loss:0.022, val_acc:0.978]
Epoch [118/120    avg_loss:0.026, val_acc:0.979]
Epoch [119/120    avg_loss:0.035, val_acc:0.979]
Epoch [120/120    avg_loss:0.030, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    1 1240    1    0    0    0    0    0    0   13   30    0    0
     0    0    0]
 [   0    0    1  711    6    6    0    0    0    3    0    0   18    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    3    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0   36   85    0    7    1    0    0    4  734    0    0    0
     3    5    0]
 [   0    0   17    0    0    0    5    0    0    0   21 2161    1    4
     1    0    0]
 [   0    0    0   23   12    3    0    0    0    0    9    5  473    0
     0    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    2    0    0    0    2    0    0    0    0
   139  204    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.5691056910569

F1 scores:
[       nan 0.96296296 0.96161303 0.90515595 0.95945946 0.97045455
 0.99086758 1.         0.99883856 0.63636364 0.88593844 0.9807125
 0.92023346 0.98404255 0.93531108 0.73381295 0.93714286]

Kappa:
0.938024871366802
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f648b755828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.662, val_acc:0.400]
Epoch [2/120    avg_loss:2.282, val_acc:0.352]
Epoch [3/120    avg_loss:2.033, val_acc:0.528]
Epoch [4/120    avg_loss:1.931, val_acc:0.568]
Epoch [5/120    avg_loss:1.771, val_acc:0.587]
Epoch [6/120    avg_loss:1.598, val_acc:0.648]
Epoch [7/120    avg_loss:1.473, val_acc:0.667]
Epoch [8/120    avg_loss:1.289, val_acc:0.663]
Epoch [9/120    avg_loss:1.129, val_acc:0.718]
Epoch [10/120    avg_loss:0.973, val_acc:0.761]
Epoch [11/120    avg_loss:0.864, val_acc:0.753]
Epoch [12/120    avg_loss:0.828, val_acc:0.774]
Epoch [13/120    avg_loss:0.858, val_acc:0.747]
Epoch [14/120    avg_loss:0.731, val_acc:0.800]
Epoch [15/120    avg_loss:0.662, val_acc:0.770]
Epoch [16/120    avg_loss:0.584, val_acc:0.837]
Epoch [17/120    avg_loss:0.543, val_acc:0.814]
Epoch [18/120    avg_loss:0.465, val_acc:0.863]
Epoch [19/120    avg_loss:0.419, val_acc:0.827]
Epoch [20/120    avg_loss:0.486, val_acc:0.867]
Epoch [21/120    avg_loss:0.469, val_acc:0.873]
Epoch [22/120    avg_loss:0.492, val_acc:0.859]
Epoch [23/120    avg_loss:0.372, val_acc:0.881]
Epoch [24/120    avg_loss:0.291, val_acc:0.892]
Epoch [25/120    avg_loss:0.333, val_acc:0.846]
Epoch [26/120    avg_loss:0.361, val_acc:0.890]
Epoch [27/120    avg_loss:0.323, val_acc:0.902]
Epoch [28/120    avg_loss:0.395, val_acc:0.875]
Epoch [29/120    avg_loss:0.276, val_acc:0.925]
Epoch [30/120    avg_loss:0.264, val_acc:0.936]
Epoch [31/120    avg_loss:0.262, val_acc:0.885]
Epoch [32/120    avg_loss:0.205, val_acc:0.925]
Epoch [33/120    avg_loss:0.232, val_acc:0.919]
Epoch [34/120    avg_loss:0.203, val_acc:0.922]
Epoch [35/120    avg_loss:0.206, val_acc:0.927]
Epoch [36/120    avg_loss:0.235, val_acc:0.916]
Epoch [37/120    avg_loss:0.228, val_acc:0.913]
Epoch [38/120    avg_loss:0.185, val_acc:0.917]
Epoch [39/120    avg_loss:0.139, val_acc:0.934]
Epoch [40/120    avg_loss:0.139, val_acc:0.944]
Epoch [41/120    avg_loss:0.116, val_acc:0.928]
Epoch [42/120    avg_loss:0.143, val_acc:0.950]
Epoch [43/120    avg_loss:0.240, val_acc:0.931]
Epoch [44/120    avg_loss:0.176, val_acc:0.920]
Epoch [45/120    avg_loss:0.180, val_acc:0.931]
Epoch [46/120    avg_loss:0.135, val_acc:0.941]
Epoch [47/120    avg_loss:0.132, val_acc:0.946]
Epoch [48/120    avg_loss:0.107, val_acc:0.945]
Epoch [49/120    avg_loss:0.124, val_acc:0.923]
Epoch [50/120    avg_loss:0.123, val_acc:0.953]
Epoch [51/120    avg_loss:0.121, val_acc:0.945]
Epoch [52/120    avg_loss:0.092, val_acc:0.955]
Epoch [53/120    avg_loss:0.089, val_acc:0.939]
Epoch [54/120    avg_loss:0.086, val_acc:0.957]
Epoch [55/120    avg_loss:0.094, val_acc:0.948]
Epoch [56/120    avg_loss:0.100, val_acc:0.943]
Epoch [57/120    avg_loss:0.081, val_acc:0.955]
Epoch [58/120    avg_loss:0.085, val_acc:0.928]
Epoch [59/120    avg_loss:0.086, val_acc:0.959]
Epoch [60/120    avg_loss:0.061, val_acc:0.963]
Epoch [61/120    avg_loss:0.106, val_acc:0.956]
Epoch [62/120    avg_loss:0.092, val_acc:0.962]
Epoch [63/120    avg_loss:0.089, val_acc:0.941]
Epoch [64/120    avg_loss:0.159, val_acc:0.930]
Epoch [65/120    avg_loss:0.130, val_acc:0.941]
Epoch [66/120    avg_loss:0.076, val_acc:0.950]
Epoch [67/120    avg_loss:0.092, val_acc:0.953]
Epoch [68/120    avg_loss:0.078, val_acc:0.949]
Epoch [69/120    avg_loss:0.056, val_acc:0.959]
Epoch [70/120    avg_loss:0.061, val_acc:0.950]
Epoch [71/120    avg_loss:0.066, val_acc:0.959]
Epoch [72/120    avg_loss:0.077, val_acc:0.948]
Epoch [73/120    avg_loss:0.064, val_acc:0.958]
Epoch [74/120    avg_loss:0.052, val_acc:0.966]
Epoch [75/120    avg_loss:0.047, val_acc:0.966]
Epoch [76/120    avg_loss:0.044, val_acc:0.967]
Epoch [77/120    avg_loss:0.047, val_acc:0.964]
Epoch [78/120    avg_loss:0.037, val_acc:0.965]
Epoch [79/120    avg_loss:0.033, val_acc:0.967]
Epoch [80/120    avg_loss:0.028, val_acc:0.966]
Epoch [81/120    avg_loss:0.039, val_acc:0.966]
Epoch [82/120    avg_loss:0.040, val_acc:0.967]
Epoch [83/120    avg_loss:0.040, val_acc:0.971]
Epoch [84/120    avg_loss:0.035, val_acc:0.971]
Epoch [85/120    avg_loss:0.032, val_acc:0.968]
Epoch [86/120    avg_loss:0.033, val_acc:0.968]
Epoch [87/120    avg_loss:0.031, val_acc:0.967]
Epoch [88/120    avg_loss:0.032, val_acc:0.971]
Epoch [89/120    avg_loss:0.035, val_acc:0.970]
Epoch [90/120    avg_loss:0.034, val_acc:0.970]
Epoch [91/120    avg_loss:0.028, val_acc:0.970]
Epoch [92/120    avg_loss:0.038, val_acc:0.967]
Epoch [93/120    avg_loss:0.034, val_acc:0.971]
Epoch [94/120    avg_loss:0.036, val_acc:0.970]
Epoch [95/120    avg_loss:0.037, val_acc:0.970]
Epoch [96/120    avg_loss:0.032, val_acc:0.971]
Epoch [97/120    avg_loss:0.038, val_acc:0.968]
Epoch [98/120    avg_loss:0.034, val_acc:0.968]
Epoch [99/120    avg_loss:0.033, val_acc:0.970]
Epoch [100/120    avg_loss:0.036, val_acc:0.970]
Epoch [101/120    avg_loss:0.029, val_acc:0.972]
Epoch [102/120    avg_loss:0.036, val_acc:0.971]
Epoch [103/120    avg_loss:0.032, val_acc:0.972]
Epoch [104/120    avg_loss:0.032, val_acc:0.972]
Epoch [105/120    avg_loss:0.028, val_acc:0.973]
Epoch [106/120    avg_loss:0.032, val_acc:0.971]
Epoch [107/120    avg_loss:0.027, val_acc:0.972]
Epoch [108/120    avg_loss:0.030, val_acc:0.972]
Epoch [109/120    avg_loss:0.028, val_acc:0.971]
Epoch [110/120    avg_loss:0.029, val_acc:0.972]
Epoch [111/120    avg_loss:0.029, val_acc:0.972]
Epoch [112/120    avg_loss:0.030, val_acc:0.972]
Epoch [113/120    avg_loss:0.028, val_acc:0.972]
Epoch [114/120    avg_loss:0.025, val_acc:0.972]
Epoch [115/120    avg_loss:0.027, val_acc:0.973]
Epoch [116/120    avg_loss:0.031, val_acc:0.973]
Epoch [117/120    avg_loss:0.029, val_acc:0.974]
Epoch [118/120    avg_loss:0.032, val_acc:0.971]
Epoch [119/120    avg_loss:0.027, val_acc:0.972]
Epoch [120/120    avg_loss:0.029, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1187    2    1    0    5    2    0    0   29   57    2    0
     0    0    0]
 [   0    0    5  689    1    7    0    0    0    8    0    0   34    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    3    0    5    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   15    0    0    2    0
     0    0    0]
 [   0    0    7   87    0    6    0    0    0    0  764    8    2    0
     0    1    0]
 [   0    0    8    0    0    0   10    0    4    0    8 2171    3    2
     4    0    0]
 [   0    0    0   22    5    4    0    0    0    2   15    0  473    0
     0    0   13]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    4    2    0    0
  1131    0    0]
 [   0    0    0    0    0    1   22    0    0    9    0    0    0    0
    81  234    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.51490514905149

F1 scores:
[       nan 0.975      0.95264848 0.89018088 0.98383372 0.96465222
 0.97185185 0.90909091 0.99421965 0.52631579 0.90041249 0.97573034
 0.90095238 0.98666667 0.95928753 0.80412371 0.9281768 ]

Kappa:
0.9374352224987167
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc48b4a77b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.631, val_acc:0.523]
Epoch [2/120    avg_loss:2.295, val_acc:0.560]
Epoch [3/120    avg_loss:2.098, val_acc:0.525]
Epoch [4/120    avg_loss:1.926, val_acc:0.576]
Epoch [5/120    avg_loss:1.811, val_acc:0.576]
Epoch [6/120    avg_loss:1.626, val_acc:0.590]
Epoch [7/120    avg_loss:1.558, val_acc:0.610]
Epoch [8/120    avg_loss:1.442, val_acc:0.621]
Epoch [9/120    avg_loss:1.293, val_acc:0.671]
Epoch [10/120    avg_loss:1.223, val_acc:0.595]
Epoch [11/120    avg_loss:1.148, val_acc:0.708]
Epoch [12/120    avg_loss:0.978, val_acc:0.716]
Epoch [13/120    avg_loss:0.921, val_acc:0.751]
Epoch [14/120    avg_loss:0.766, val_acc:0.789]
Epoch [15/120    avg_loss:0.680, val_acc:0.778]
Epoch [16/120    avg_loss:0.647, val_acc:0.805]
Epoch [17/120    avg_loss:0.524, val_acc:0.845]
Epoch [18/120    avg_loss:0.504, val_acc:0.804]
Epoch [19/120    avg_loss:0.453, val_acc:0.835]
Epoch [20/120    avg_loss:0.458, val_acc:0.860]
Epoch [21/120    avg_loss:0.488, val_acc:0.823]
Epoch [22/120    avg_loss:0.434, val_acc:0.831]
Epoch [23/120    avg_loss:0.323, val_acc:0.857]
Epoch [24/120    avg_loss:0.302, val_acc:0.866]
Epoch [25/120    avg_loss:0.349, val_acc:0.881]
Epoch [26/120    avg_loss:0.270, val_acc:0.877]
Epoch [27/120    avg_loss:0.311, val_acc:0.840]
Epoch [28/120    avg_loss:0.306, val_acc:0.866]
Epoch [29/120    avg_loss:0.269, val_acc:0.906]
Epoch [30/120    avg_loss:0.186, val_acc:0.909]
Epoch [31/120    avg_loss:0.174, val_acc:0.916]
Epoch [32/120    avg_loss:0.135, val_acc:0.933]
Epoch [33/120    avg_loss:0.122, val_acc:0.910]
Epoch [34/120    avg_loss:0.128, val_acc:0.927]
Epoch [35/120    avg_loss:0.113, val_acc:0.929]
Epoch [36/120    avg_loss:0.108, val_acc:0.936]
Epoch [37/120    avg_loss:0.086, val_acc:0.948]
Epoch [38/120    avg_loss:0.094, val_acc:0.941]
Epoch [39/120    avg_loss:0.093, val_acc:0.923]
Epoch [40/120    avg_loss:0.076, val_acc:0.944]
Epoch [41/120    avg_loss:0.088, val_acc:0.958]
Epoch [42/120    avg_loss:0.064, val_acc:0.957]
Epoch [43/120    avg_loss:0.077, val_acc:0.956]
Epoch [44/120    avg_loss:0.081, val_acc:0.952]
Epoch [45/120    avg_loss:0.058, val_acc:0.946]
Epoch [46/120    avg_loss:0.054, val_acc:0.959]
Epoch [47/120    avg_loss:0.055, val_acc:0.956]
Epoch [48/120    avg_loss:0.056, val_acc:0.955]
Epoch [49/120    avg_loss:0.060, val_acc:0.955]
Epoch [50/120    avg_loss:0.062, val_acc:0.957]
Epoch [51/120    avg_loss:0.052, val_acc:0.961]
Epoch [52/120    avg_loss:0.035, val_acc:0.959]
Epoch [53/120    avg_loss:0.046, val_acc:0.953]
Epoch [54/120    avg_loss:0.041, val_acc:0.957]
Epoch [55/120    avg_loss:0.028, val_acc:0.963]
Epoch [56/120    avg_loss:0.037, val_acc:0.965]
Epoch [57/120    avg_loss:0.048, val_acc:0.958]
Epoch [58/120    avg_loss:0.044, val_acc:0.960]
Epoch [59/120    avg_loss:0.037, val_acc:0.951]
Epoch [60/120    avg_loss:0.042, val_acc:0.947]
Epoch [61/120    avg_loss:0.047, val_acc:0.950]
Epoch [62/120    avg_loss:0.034, val_acc:0.960]
Epoch [63/120    avg_loss:0.055, val_acc:0.960]
Epoch [64/120    avg_loss:0.053, val_acc:0.950]
Epoch [65/120    avg_loss:0.038, val_acc:0.966]
Epoch [66/120    avg_loss:0.042, val_acc:0.941]
Epoch [67/120    avg_loss:0.050, val_acc:0.945]
Epoch [68/120    avg_loss:0.042, val_acc:0.956]
Epoch [69/120    avg_loss:0.033, val_acc:0.959]
Epoch [70/120    avg_loss:0.050, val_acc:0.957]
Epoch [71/120    avg_loss:0.052, val_acc:0.961]
Epoch [72/120    avg_loss:0.062, val_acc:0.954]
Epoch [73/120    avg_loss:0.037, val_acc:0.958]
Epoch [74/120    avg_loss:0.027, val_acc:0.963]
Epoch [75/120    avg_loss:0.024, val_acc:0.970]
Epoch [76/120    avg_loss:0.020, val_acc:0.969]
Epoch [77/120    avg_loss:0.018, val_acc:0.966]
Epoch [78/120    avg_loss:0.019, val_acc:0.969]
Epoch [79/120    avg_loss:0.016, val_acc:0.959]
Epoch [80/120    avg_loss:0.017, val_acc:0.964]
Epoch [81/120    avg_loss:0.018, val_acc:0.964]
Epoch [82/120    avg_loss:0.013, val_acc:0.969]
Epoch [83/120    avg_loss:0.017, val_acc:0.968]
Epoch [84/120    avg_loss:0.025, val_acc:0.964]
Epoch [85/120    avg_loss:0.020, val_acc:0.961]
Epoch [86/120    avg_loss:0.012, val_acc:0.964]
Epoch [87/120    avg_loss:0.015, val_acc:0.940]
Epoch [88/120    avg_loss:0.020, val_acc:0.964]
Epoch [89/120    avg_loss:0.014, val_acc:0.961]
Epoch [90/120    avg_loss:0.011, val_acc:0.965]
Epoch [91/120    avg_loss:0.011, val_acc:0.965]
Epoch [92/120    avg_loss:0.011, val_acc:0.963]
Epoch [93/120    avg_loss:0.011, val_acc:0.964]
Epoch [94/120    avg_loss:0.011, val_acc:0.964]
Epoch [95/120    avg_loss:0.013, val_acc:0.964]
Epoch [96/120    avg_loss:0.009, val_acc:0.963]
Epoch [97/120    avg_loss:0.013, val_acc:0.967]
Epoch [98/120    avg_loss:0.009, val_acc:0.965]
Epoch [99/120    avg_loss:0.012, val_acc:0.966]
Epoch [100/120    avg_loss:0.008, val_acc:0.966]
Epoch [101/120    avg_loss:0.009, val_acc:0.965]
Epoch [102/120    avg_loss:0.008, val_acc:0.965]
Epoch [103/120    avg_loss:0.008, val_acc:0.965]
Epoch [104/120    avg_loss:0.009, val_acc:0.965]
Epoch [105/120    avg_loss:0.009, val_acc:0.966]
Epoch [106/120    avg_loss:0.011, val_acc:0.965]
Epoch [107/120    avg_loss:0.008, val_acc:0.965]
Epoch [108/120    avg_loss:0.009, val_acc:0.965]
Epoch [109/120    avg_loss:0.009, val_acc:0.965]
Epoch [110/120    avg_loss:0.008, val_acc:0.965]
Epoch [111/120    avg_loss:0.008, val_acc:0.965]
Epoch [112/120    avg_loss:0.008, val_acc:0.965]
Epoch [113/120    avg_loss:0.008, val_acc:0.965]
Epoch [114/120    avg_loss:0.009, val_acc:0.965]
Epoch [115/120    avg_loss:0.009, val_acc:0.965]
Epoch [116/120    avg_loss:0.009, val_acc:0.965]
Epoch [117/120    avg_loss:0.008, val_acc:0.965]
Epoch [118/120    avg_loss:0.011, val_acc:0.965]
Epoch [119/120    avg_loss:0.009, val_acc:0.965]
Epoch [120/120    avg_loss:0.011, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    4    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1261    4    1    0    0    0    0    0    3   15    1    0
     0    0    0]
 [   0    0    0  722    2    0    0    0    0    1    4   12    6    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    0    1    0    0    0  837   29    0    0
     0    0    0]
 [   0    0   12    0    0    1    1    0    0    0   28 2158   10    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0   10  519    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1127   12    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    63  280    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.3550135501355

F1 scores:
[       nan 0.92307692 0.98285269 0.97831978 0.99065421 0.99428571
 0.9939302  1.         0.99883586 0.97297297 0.9576659  0.97316798
 0.96828358 1.         0.96738197 0.87363495 0.98203593]

Kappa:
0.9698203500324537
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1ad7c98860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.665, val_acc:0.326]
Epoch [2/120    avg_loss:2.270, val_acc:0.460]
Epoch [3/120    avg_loss:2.061, val_acc:0.561]
Epoch [4/120    avg_loss:1.861, val_acc:0.579]
Epoch [5/120    avg_loss:1.696, val_acc:0.590]
Epoch [6/120    avg_loss:1.526, val_acc:0.619]
Epoch [7/120    avg_loss:1.366, val_acc:0.615]
Epoch [8/120    avg_loss:1.241, val_acc:0.629]
Epoch [9/120    avg_loss:1.125, val_acc:0.656]
Epoch [10/120    avg_loss:1.053, val_acc:0.642]
Epoch [11/120    avg_loss:0.986, val_acc:0.690]
Epoch [12/120    avg_loss:0.975, val_acc:0.719]
Epoch [13/120    avg_loss:0.815, val_acc:0.756]
Epoch [14/120    avg_loss:0.695, val_acc:0.738]
Epoch [15/120    avg_loss:0.659, val_acc:0.753]
Epoch [16/120    avg_loss:0.563, val_acc:0.779]
Epoch [17/120    avg_loss:0.473, val_acc:0.795]
Epoch [18/120    avg_loss:0.436, val_acc:0.826]
Epoch [19/120    avg_loss:0.425, val_acc:0.814]
Epoch [20/120    avg_loss:0.423, val_acc:0.844]
Epoch [21/120    avg_loss:0.380, val_acc:0.818]
Epoch [22/120    avg_loss:0.350, val_acc:0.841]
Epoch [23/120    avg_loss:0.325, val_acc:0.867]
Epoch [24/120    avg_loss:0.254, val_acc:0.875]
Epoch [25/120    avg_loss:0.323, val_acc:0.864]
Epoch [26/120    avg_loss:0.253, val_acc:0.873]
Epoch [27/120    avg_loss:0.224, val_acc:0.879]
Epoch [28/120    avg_loss:0.222, val_acc:0.902]
Epoch [29/120    avg_loss:0.187, val_acc:0.891]
Epoch [30/120    avg_loss:0.171, val_acc:0.891]
Epoch [31/120    avg_loss:0.160, val_acc:0.909]
Epoch [32/120    avg_loss:0.187, val_acc:0.896]
Epoch [33/120    avg_loss:0.143, val_acc:0.931]
Epoch [34/120    avg_loss:0.109, val_acc:0.931]
Epoch [35/120    avg_loss:0.109, val_acc:0.923]
Epoch [36/120    avg_loss:0.122, val_acc:0.919]
Epoch [37/120    avg_loss:0.108, val_acc:0.935]
Epoch [38/120    avg_loss:0.121, val_acc:0.918]
Epoch [39/120    avg_loss:0.276, val_acc:0.797]
Epoch [40/120    avg_loss:0.287, val_acc:0.838]
Epoch [41/120    avg_loss:0.309, val_acc:0.886]
Epoch [42/120    avg_loss:0.194, val_acc:0.907]
Epoch [43/120    avg_loss:0.159, val_acc:0.929]
Epoch [44/120    avg_loss:0.160, val_acc:0.930]
Epoch [45/120    avg_loss:0.115, val_acc:0.942]
Epoch [46/120    avg_loss:0.123, val_acc:0.934]
Epoch [47/120    avg_loss:0.086, val_acc:0.944]
Epoch [48/120    avg_loss:0.078, val_acc:0.939]
Epoch [49/120    avg_loss:0.070, val_acc:0.936]
Epoch [50/120    avg_loss:0.071, val_acc:0.931]
Epoch [51/120    avg_loss:0.067, val_acc:0.940]
Epoch [52/120    avg_loss:0.056, val_acc:0.952]
Epoch [53/120    avg_loss:0.062, val_acc:0.946]
Epoch [54/120    avg_loss:0.064, val_acc:0.909]
Epoch [55/120    avg_loss:0.088, val_acc:0.940]
Epoch [56/120    avg_loss:0.068, val_acc:0.945]
Epoch [57/120    avg_loss:0.065, val_acc:0.944]
Epoch [58/120    avg_loss:0.073, val_acc:0.934]
Epoch [59/120    avg_loss:0.052, val_acc:0.961]
Epoch [60/120    avg_loss:0.045, val_acc:0.959]
Epoch [61/120    avg_loss:0.055, val_acc:0.952]
Epoch [62/120    avg_loss:0.040, val_acc:0.961]
Epoch [63/120    avg_loss:0.060, val_acc:0.938]
Epoch [64/120    avg_loss:0.081, val_acc:0.943]
Epoch [65/120    avg_loss:0.049, val_acc:0.959]
Epoch [66/120    avg_loss:0.034, val_acc:0.949]
Epoch [67/120    avg_loss:0.036, val_acc:0.950]
Epoch [68/120    avg_loss:0.064, val_acc:0.935]
Epoch [69/120    avg_loss:0.050, val_acc:0.933]
Epoch [70/120    avg_loss:0.065, val_acc:0.949]
Epoch [71/120    avg_loss:0.034, val_acc:0.959]
Epoch [72/120    avg_loss:0.067, val_acc:0.951]
Epoch [73/120    avg_loss:0.055, val_acc:0.958]
Epoch [74/120    avg_loss:0.039, val_acc:0.951]
Epoch [75/120    avg_loss:0.028, val_acc:0.953]
Epoch [76/120    avg_loss:0.023, val_acc:0.954]
Epoch [77/120    avg_loss:0.019, val_acc:0.958]
Epoch [78/120    avg_loss:0.022, val_acc:0.958]
Epoch [79/120    avg_loss:0.023, val_acc:0.959]
Epoch [80/120    avg_loss:0.018, val_acc:0.959]
Epoch [81/120    avg_loss:0.017, val_acc:0.961]
Epoch [82/120    avg_loss:0.022, val_acc:0.965]
Epoch [83/120    avg_loss:0.016, val_acc:0.964]
Epoch [84/120    avg_loss:0.019, val_acc:0.961]
Epoch [85/120    avg_loss:0.019, val_acc:0.965]
Epoch [86/120    avg_loss:0.020, val_acc:0.966]
Epoch [87/120    avg_loss:0.020, val_acc:0.964]
Epoch [88/120    avg_loss:0.018, val_acc:0.965]
Epoch [89/120    avg_loss:0.018, val_acc:0.964]
Epoch [90/120    avg_loss:0.019, val_acc:0.966]
Epoch [91/120    avg_loss:0.017, val_acc:0.968]
Epoch [92/120    avg_loss:0.016, val_acc:0.970]
Epoch [93/120    avg_loss:0.016, val_acc:0.966]
Epoch [94/120    avg_loss:0.017, val_acc:0.965]
Epoch [95/120    avg_loss:0.017, val_acc:0.968]
Epoch [96/120    avg_loss:0.018, val_acc:0.971]
Epoch [97/120    avg_loss:0.018, val_acc:0.967]
Epoch [98/120    avg_loss:0.015, val_acc:0.968]
Epoch [99/120    avg_loss:0.020, val_acc:0.970]
Epoch [100/120    avg_loss:0.016, val_acc:0.970]
Epoch [101/120    avg_loss:0.017, val_acc:0.970]
Epoch [102/120    avg_loss:0.017, val_acc:0.969]
Epoch [103/120    avg_loss:0.017, val_acc:0.969]
Epoch [104/120    avg_loss:0.016, val_acc:0.968]
Epoch [105/120    avg_loss:0.018, val_acc:0.970]
Epoch [106/120    avg_loss:0.014, val_acc:0.966]
Epoch [107/120    avg_loss:0.015, val_acc:0.967]
Epoch [108/120    avg_loss:0.015, val_acc:0.966]
Epoch [109/120    avg_loss:0.014, val_acc:0.967]
Epoch [110/120    avg_loss:0.016, val_acc:0.967]
Epoch [111/120    avg_loss:0.014, val_acc:0.967]
Epoch [112/120    avg_loss:0.013, val_acc:0.967]
Epoch [113/120    avg_loss:0.015, val_acc:0.968]
Epoch [114/120    avg_loss:0.016, val_acc:0.969]
Epoch [115/120    avg_loss:0.016, val_acc:0.968]
Epoch [116/120    avg_loss:0.017, val_acc:0.968]
Epoch [117/120    avg_loss:0.016, val_acc:0.968]
Epoch [118/120    avg_loss:0.015, val_acc:0.968]
Epoch [119/120    avg_loss:0.017, val_acc:0.968]
Epoch [120/120    avg_loss:0.015, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1245    5    0    1    0    0    0    0    3   31    0    0
     0    0    0]
 [   0    0    0  722    3    0    1    0    0    3    1   10    7    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    5    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  820   50    0    0
     0    1    0]
 [   0    0   16    0    0    0    0    0    0    2   17 2166    9    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    4    4  521    0
     0    3    1]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1123   15    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    75  266    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.88888888888889

F1 scores:
[       nan 0.96202532 0.97647059 0.97765741 0.9882904  0.99885189
 0.99011407 1.         1.         0.87804878 0.95072464 0.96782842
 0.97201493 0.99728997 0.96065013 0.84177215 0.98809524]

Kappa:
0.9644764763852545
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0f628b57f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.701, val_acc:0.302]
Epoch [2/120    avg_loss:2.306, val_acc:0.352]
Epoch [3/120    avg_loss:2.075, val_acc:0.509]
Epoch [4/120    avg_loss:1.885, val_acc:0.549]
Epoch [5/120    avg_loss:1.690, val_acc:0.560]
Epoch [6/120    avg_loss:1.576, val_acc:0.590]
Epoch [7/120    avg_loss:1.491, val_acc:0.617]
Epoch [8/120    avg_loss:1.409, val_acc:0.556]
Epoch [9/120    avg_loss:1.337, val_acc:0.616]
Epoch [10/120    avg_loss:1.167, val_acc:0.653]
Epoch [11/120    avg_loss:1.034, val_acc:0.661]
Epoch [12/120    avg_loss:0.935, val_acc:0.713]
Epoch [13/120    avg_loss:0.791, val_acc:0.731]
Epoch [14/120    avg_loss:0.724, val_acc:0.745]
Epoch [15/120    avg_loss:0.750, val_acc:0.730]
Epoch [16/120    avg_loss:0.707, val_acc:0.777]
Epoch [17/120    avg_loss:0.592, val_acc:0.792]
Epoch [18/120    avg_loss:0.464, val_acc:0.826]
Epoch [19/120    avg_loss:0.470, val_acc:0.846]
Epoch [20/120    avg_loss:0.425, val_acc:0.864]
Epoch [21/120    avg_loss:0.418, val_acc:0.834]
Epoch [22/120    avg_loss:0.390, val_acc:0.854]
Epoch [23/120    avg_loss:0.351, val_acc:0.874]
Epoch [24/120    avg_loss:0.353, val_acc:0.866]
Epoch [25/120    avg_loss:0.293, val_acc:0.872]
Epoch [26/120    avg_loss:0.324, val_acc:0.853]
Epoch [27/120    avg_loss:0.251, val_acc:0.901]
Epoch [28/120    avg_loss:0.214, val_acc:0.908]
Epoch [29/120    avg_loss:0.229, val_acc:0.889]
Epoch [30/120    avg_loss:0.205, val_acc:0.899]
Epoch [31/120    avg_loss:0.195, val_acc:0.855]
Epoch [32/120    avg_loss:0.205, val_acc:0.919]
Epoch [33/120    avg_loss:0.163, val_acc:0.909]
Epoch [34/120    avg_loss:0.138, val_acc:0.935]
Epoch [35/120    avg_loss:0.164, val_acc:0.935]
Epoch [36/120    avg_loss:0.203, val_acc:0.890]
Epoch [37/120    avg_loss:0.185, val_acc:0.927]
Epoch [38/120    avg_loss:0.142, val_acc:0.926]
Epoch [39/120    avg_loss:0.129, val_acc:0.947]
Epoch [40/120    avg_loss:0.123, val_acc:0.931]
Epoch [41/120    avg_loss:0.114, val_acc:0.944]
Epoch [42/120    avg_loss:0.097, val_acc:0.941]
Epoch [43/120    avg_loss:0.087, val_acc:0.948]
Epoch [44/120    avg_loss:0.073, val_acc:0.945]
Epoch [45/120    avg_loss:0.066, val_acc:0.954]
Epoch [46/120    avg_loss:0.067, val_acc:0.954]
Epoch [47/120    avg_loss:0.056, val_acc:0.950]
Epoch [48/120    avg_loss:0.095, val_acc:0.947]
Epoch [49/120    avg_loss:0.081, val_acc:0.936]
Epoch [50/120    avg_loss:0.064, val_acc:0.949]
Epoch [51/120    avg_loss:0.076, val_acc:0.948]
Epoch [52/120    avg_loss:0.067, val_acc:0.960]
Epoch [53/120    avg_loss:0.053, val_acc:0.964]
Epoch [54/120    avg_loss:0.049, val_acc:0.960]
Epoch [55/120    avg_loss:0.048, val_acc:0.942]
Epoch [56/120    avg_loss:0.047, val_acc:0.960]
Epoch [57/120    avg_loss:0.050, val_acc:0.958]
Epoch [58/120    avg_loss:0.047, val_acc:0.943]
Epoch [59/120    avg_loss:0.043, val_acc:0.960]
Epoch [60/120    avg_loss:0.049, val_acc:0.967]
Epoch [61/120    avg_loss:0.033, val_acc:0.973]
Epoch [62/120    avg_loss:0.034, val_acc:0.972]
Epoch [63/120    avg_loss:0.033, val_acc:0.964]
Epoch [64/120    avg_loss:0.034, val_acc:0.931]
Epoch [65/120    avg_loss:0.034, val_acc:0.951]
Epoch [66/120    avg_loss:0.029, val_acc:0.972]
Epoch [67/120    avg_loss:0.026, val_acc:0.971]
Epoch [68/120    avg_loss:0.032, val_acc:0.965]
Epoch [69/120    avg_loss:0.036, val_acc:0.969]
Epoch [70/120    avg_loss:0.041, val_acc:0.955]
Epoch [71/120    avg_loss:0.050, val_acc:0.967]
Epoch [72/120    avg_loss:0.028, val_acc:0.974]
Epoch [73/120    avg_loss:0.046, val_acc:0.965]
Epoch [74/120    avg_loss:0.039, val_acc:0.968]
Epoch [75/120    avg_loss:0.039, val_acc:0.977]
Epoch [76/120    avg_loss:0.023, val_acc:0.977]
Epoch [77/120    avg_loss:0.025, val_acc:0.976]
Epoch [78/120    avg_loss:0.025, val_acc:0.977]
Epoch [79/120    avg_loss:0.018, val_acc:0.973]
Epoch [80/120    avg_loss:0.019, val_acc:0.976]
Epoch [81/120    avg_loss:0.016, val_acc:0.974]
Epoch [82/120    avg_loss:0.012, val_acc:0.975]
Epoch [83/120    avg_loss:0.016, val_acc:0.978]
Epoch [84/120    avg_loss:0.016, val_acc:0.977]
Epoch [85/120    avg_loss:0.017, val_acc:0.976]
Epoch [86/120    avg_loss:0.015, val_acc:0.975]
Epoch [87/120    avg_loss:0.013, val_acc:0.975]
Epoch [88/120    avg_loss:0.014, val_acc:0.973]
Epoch [89/120    avg_loss:0.015, val_acc:0.965]
Epoch [90/120    avg_loss:0.017, val_acc:0.976]
Epoch [91/120    avg_loss:0.019, val_acc:0.966]
Epoch [92/120    avg_loss:0.022, val_acc:0.971]
Epoch [93/120    avg_loss:0.017, val_acc:0.976]
Epoch [94/120    avg_loss:0.013, val_acc:0.971]
Epoch [95/120    avg_loss:0.015, val_acc:0.976]
Epoch [96/120    avg_loss:0.013, val_acc:0.972]
Epoch [97/120    avg_loss:0.010, val_acc:0.978]
Epoch [98/120    avg_loss:0.009, val_acc:0.979]
Epoch [99/120    avg_loss:0.008, val_acc:0.979]
Epoch [100/120    avg_loss:0.008, val_acc:0.979]
Epoch [101/120    avg_loss:0.008, val_acc:0.979]
Epoch [102/120    avg_loss:0.007, val_acc:0.979]
Epoch [103/120    avg_loss:0.008, val_acc:0.980]
Epoch [104/120    avg_loss:0.008, val_acc:0.980]
Epoch [105/120    avg_loss:0.007, val_acc:0.980]
Epoch [106/120    avg_loss:0.006, val_acc:0.982]
Epoch [107/120    avg_loss:0.007, val_acc:0.979]
Epoch [108/120    avg_loss:0.009, val_acc:0.979]
Epoch [109/120    avg_loss:0.007, val_acc:0.979]
Epoch [110/120    avg_loss:0.007, val_acc:0.979]
Epoch [111/120    avg_loss:0.006, val_acc:0.978]
Epoch [112/120    avg_loss:0.007, val_acc:0.977]
Epoch [113/120    avg_loss:0.007, val_acc:0.978]
Epoch [114/120    avg_loss:0.008, val_acc:0.979]
Epoch [115/120    avg_loss:0.006, val_acc:0.979]
Epoch [116/120    avg_loss:0.008, val_acc:0.979]
Epoch [117/120    avg_loss:0.008, val_acc:0.979]
Epoch [118/120    avg_loss:0.007, val_acc:0.979]
Epoch [119/120    avg_loss:0.006, val_acc:0.979]
Epoch [120/120    avg_loss:0.008, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1243    3    5    0    1    0    0    0    6   27    0    0
     0    0    0]
 [   0    0    0  722    7    1    2    0    0    3    4    4    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    1    0    0    0    0    0    0
     9    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    4    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  843   26    1    0
     0    1    0]
 [   0    0   15    0    0    0    0    0    0    4   12 2166   13    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0    0    0    1  520    0
     0    6    2]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1127   11    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    1
    72  270    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.12737127371274

F1 scores:
[       nan 0.98765432 0.97605026 0.97765741 0.97260274 0.98722416
 0.99012908 0.98039216 1.         0.8372093  0.96785304 0.97611537
 0.96834264 0.99459459 0.95996593 0.8503937  0.97619048]

Kappa:
0.967225842031917
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd6ad66c860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.731, val_acc:0.338]
Epoch [2/120    avg_loss:2.306, val_acc:0.453]
Epoch [3/120    avg_loss:2.072, val_acc:0.477]
Epoch [4/120    avg_loss:1.903, val_acc:0.532]
Epoch [5/120    avg_loss:1.799, val_acc:0.667]
Epoch [6/120    avg_loss:1.619, val_acc:0.628]
Epoch [7/120    avg_loss:1.447, val_acc:0.652]
Epoch [8/120    avg_loss:1.251, val_acc:0.695]
Epoch [9/120    avg_loss:1.096, val_acc:0.666]
Epoch [10/120    avg_loss:1.112, val_acc:0.666]
Epoch [11/120    avg_loss:0.960, val_acc:0.699]
Epoch [12/120    avg_loss:0.840, val_acc:0.723]
Epoch [13/120    avg_loss:0.760, val_acc:0.717]
Epoch [14/120    avg_loss:0.670, val_acc:0.750]
Epoch [15/120    avg_loss:0.611, val_acc:0.779]
Epoch [16/120    avg_loss:0.571, val_acc:0.767]
Epoch [17/120    avg_loss:0.549, val_acc:0.789]
Epoch [18/120    avg_loss:0.569, val_acc:0.775]
Epoch [19/120    avg_loss:0.560, val_acc:0.799]
Epoch [20/120    avg_loss:0.467, val_acc:0.852]
Epoch [21/120    avg_loss:0.359, val_acc:0.854]
Epoch [22/120    avg_loss:0.366, val_acc:0.808]
Epoch [23/120    avg_loss:0.370, val_acc:0.830]
Epoch [24/120    avg_loss:0.348, val_acc:0.852]
Epoch [25/120    avg_loss:0.273, val_acc:0.878]
Epoch [26/120    avg_loss:0.245, val_acc:0.887]
Epoch [27/120    avg_loss:0.226, val_acc:0.889]
Epoch [28/120    avg_loss:0.243, val_acc:0.885]
Epoch [29/120    avg_loss:0.224, val_acc:0.902]
Epoch [30/120    avg_loss:0.170, val_acc:0.920]
Epoch [31/120    avg_loss:0.139, val_acc:0.924]
Epoch [32/120    avg_loss:0.139, val_acc:0.938]
Epoch [33/120    avg_loss:0.128, val_acc:0.926]
Epoch [34/120    avg_loss:0.160, val_acc:0.930]
Epoch [35/120    avg_loss:0.146, val_acc:0.941]
Epoch [36/120    avg_loss:0.254, val_acc:0.903]
Epoch [37/120    avg_loss:0.150, val_acc:0.932]
Epoch [38/120    avg_loss:0.150, val_acc:0.934]
Epoch [39/120    avg_loss:0.132, val_acc:0.935]
Epoch [40/120    avg_loss:0.134, val_acc:0.940]
Epoch [41/120    avg_loss:0.139, val_acc:0.934]
Epoch [42/120    avg_loss:0.452, val_acc:0.823]
Epoch [43/120    avg_loss:0.311, val_acc:0.882]
Epoch [44/120    avg_loss:0.169, val_acc:0.883]
Epoch [45/120    avg_loss:0.158, val_acc:0.919]
Epoch [46/120    avg_loss:0.139, val_acc:0.932]
Epoch [47/120    avg_loss:0.107, val_acc:0.950]
Epoch [48/120    avg_loss:0.094, val_acc:0.945]
Epoch [49/120    avg_loss:0.096, val_acc:0.901]
Epoch [50/120    avg_loss:0.121, val_acc:0.940]
Epoch [51/120    avg_loss:0.093, val_acc:0.948]
Epoch [52/120    avg_loss:0.064, val_acc:0.959]
Epoch [53/120    avg_loss:0.070, val_acc:0.960]
Epoch [54/120    avg_loss:0.061, val_acc:0.956]
Epoch [55/120    avg_loss:0.072, val_acc:0.961]
Epoch [56/120    avg_loss:0.057, val_acc:0.952]
Epoch [57/120    avg_loss:0.049, val_acc:0.957]
Epoch [58/120    avg_loss:0.041, val_acc:0.971]
Epoch [59/120    avg_loss:0.037, val_acc:0.965]
Epoch [60/120    avg_loss:0.039, val_acc:0.958]
Epoch [61/120    avg_loss:0.036, val_acc:0.968]
Epoch [62/120    avg_loss:0.034, val_acc:0.966]
Epoch [63/120    avg_loss:0.032, val_acc:0.963]
Epoch [64/120    avg_loss:0.032, val_acc:0.965]
Epoch [65/120    avg_loss:0.037, val_acc:0.963]
Epoch [66/120    avg_loss:0.053, val_acc:0.951]
Epoch [67/120    avg_loss:0.044, val_acc:0.966]
Epoch [68/120    avg_loss:0.035, val_acc:0.969]
Epoch [69/120    avg_loss:0.039, val_acc:0.956]
Epoch [70/120    avg_loss:0.030, val_acc:0.971]
Epoch [71/120    avg_loss:0.037, val_acc:0.961]
Epoch [72/120    avg_loss:0.033, val_acc:0.975]
Epoch [73/120    avg_loss:0.036, val_acc:0.972]
Epoch [74/120    avg_loss:0.026, val_acc:0.970]
Epoch [75/120    avg_loss:0.024, val_acc:0.967]
Epoch [76/120    avg_loss:0.023, val_acc:0.974]
Epoch [77/120    avg_loss:0.017, val_acc:0.976]
Epoch [78/120    avg_loss:0.016, val_acc:0.978]
Epoch [79/120    avg_loss:0.024, val_acc:0.971]
Epoch [80/120    avg_loss:0.025, val_acc:0.968]
Epoch [81/120    avg_loss:0.032, val_acc:0.971]
Epoch [82/120    avg_loss:0.030, val_acc:0.959]
Epoch [83/120    avg_loss:0.029, val_acc:0.968]
Epoch [84/120    avg_loss:0.027, val_acc:0.971]
Epoch [85/120    avg_loss:0.022, val_acc:0.974]
Epoch [86/120    avg_loss:0.023, val_acc:0.973]
Epoch [87/120    avg_loss:0.020, val_acc:0.975]
Epoch [88/120    avg_loss:0.016, val_acc:0.973]
Epoch [89/120    avg_loss:0.020, val_acc:0.976]
Epoch [90/120    avg_loss:0.015, val_acc:0.976]
Epoch [91/120    avg_loss:0.014, val_acc:0.980]
Epoch [92/120    avg_loss:0.015, val_acc:0.979]
Epoch [93/120    avg_loss:0.014, val_acc:0.978]
Epoch [94/120    avg_loss:0.012, val_acc:0.981]
Epoch [95/120    avg_loss:0.012, val_acc:0.981]
Epoch [96/120    avg_loss:0.012, val_acc:0.979]
Epoch [97/120    avg_loss:0.010, val_acc:0.981]
Epoch [98/120    avg_loss:0.011, val_acc:0.982]
Epoch [99/120    avg_loss:0.011, val_acc:0.981]
Epoch [100/120    avg_loss:0.010, val_acc:0.978]
Epoch [101/120    avg_loss:0.010, val_acc:0.980]
Epoch [102/120    avg_loss:0.008, val_acc:0.981]
Epoch [103/120    avg_loss:0.012, val_acc:0.949]
Epoch [104/120    avg_loss:0.066, val_acc:0.950]
Epoch [105/120    avg_loss:0.030, val_acc:0.968]
Epoch [106/120    avg_loss:0.017, val_acc:0.972]
Epoch [107/120    avg_loss:0.017, val_acc:0.976]
Epoch [108/120    avg_loss:0.020, val_acc:0.957]
Epoch [109/120    avg_loss:0.020, val_acc:0.977]
Epoch [110/120    avg_loss:0.021, val_acc:0.976]
Epoch [111/120    avg_loss:0.022, val_acc:0.971]
Epoch [112/120    avg_loss:0.019, val_acc:0.978]
Epoch [113/120    avg_loss:0.014, val_acc:0.980]
Epoch [114/120    avg_loss:0.009, val_acc:0.979]
Epoch [115/120    avg_loss:0.009, val_acc:0.981]
Epoch [116/120    avg_loss:0.008, val_acc:0.982]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.014, val_acc:0.982]
Epoch [119/120    avg_loss:0.008, val_acc:0.982]
Epoch [120/120    avg_loss:0.008, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    1 1253    9    0    0    0    0    0    0    1   21    0    0
     0    0    0]
 [   0    0    0  725    0    2    0    0    0    1    1    8    9    1
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    1    0    0    0  851   19    2    0
     0    0    0]
 [   0    0   10    0    0    1    0    0    0    1   10 2173   15    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0    0    0    0  526    0
     2    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1122   17    0]
 [   0    0    0    0    0    1    4    0    0    0    0    0    0    0
    84  258    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.45257452574526

F1 scores:
[       nan 0.98795181 0.9827451  0.97511769 0.99764706 0.99311927
 0.99468489 1.         1.         0.92307692 0.97928654 0.98015336
 0.96869245 0.99459459 0.95570698 0.82958199 0.99408284]

Kappa:
0.9709349679334416
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f732ef4d7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.665, val_acc:0.407]
Epoch [2/120    avg_loss:2.276, val_acc:0.463]
Epoch [3/120    avg_loss:2.036, val_acc:0.533]
Epoch [4/120    avg_loss:1.876, val_acc:0.569]
Epoch [5/120    avg_loss:1.653, val_acc:0.618]
Epoch [6/120    avg_loss:1.471, val_acc:0.644]
Epoch [7/120    avg_loss:1.296, val_acc:0.637]
Epoch [8/120    avg_loss:1.149, val_acc:0.655]
Epoch [9/120    avg_loss:1.015, val_acc:0.670]
Epoch [10/120    avg_loss:0.964, val_acc:0.689]
Epoch [11/120    avg_loss:0.827, val_acc:0.748]
Epoch [12/120    avg_loss:0.745, val_acc:0.751]
Epoch [13/120    avg_loss:0.643, val_acc:0.768]
Epoch [14/120    avg_loss:0.607, val_acc:0.729]
Epoch [15/120    avg_loss:0.585, val_acc:0.730]
Epoch [16/120    avg_loss:0.563, val_acc:0.805]
Epoch [17/120    avg_loss:0.438, val_acc:0.825]
Epoch [18/120    avg_loss:0.396, val_acc:0.842]
Epoch [19/120    avg_loss:0.355, val_acc:0.854]
Epoch [20/120    avg_loss:0.327, val_acc:0.877]
Epoch [21/120    avg_loss:0.266, val_acc:0.875]
Epoch [22/120    avg_loss:0.285, val_acc:0.855]
Epoch [23/120    avg_loss:0.274, val_acc:0.890]
Epoch [24/120    avg_loss:0.220, val_acc:0.891]
Epoch [25/120    avg_loss:0.196, val_acc:0.915]
Epoch [26/120    avg_loss:0.226, val_acc:0.901]
Epoch [27/120    avg_loss:0.181, val_acc:0.927]
Epoch [28/120    avg_loss:0.144, val_acc:0.929]
Epoch [29/120    avg_loss:0.142, val_acc:0.929]
Epoch [30/120    avg_loss:0.138, val_acc:0.919]
Epoch [31/120    avg_loss:0.141, val_acc:0.927]
Epoch [32/120    avg_loss:0.105, val_acc:0.938]
Epoch [33/120    avg_loss:0.126, val_acc:0.923]
Epoch [34/120    avg_loss:0.116, val_acc:0.925]
Epoch [35/120    avg_loss:0.115, val_acc:0.936]
Epoch [36/120    avg_loss:0.116, val_acc:0.944]
Epoch [37/120    avg_loss:0.111, val_acc:0.943]
Epoch [38/120    avg_loss:0.263, val_acc:0.909]
Epoch [39/120    avg_loss:0.158, val_acc:0.931]
Epoch [40/120    avg_loss:0.124, val_acc:0.952]
Epoch [41/120    avg_loss:0.120, val_acc:0.912]
Epoch [42/120    avg_loss:0.111, val_acc:0.933]
Epoch [43/120    avg_loss:0.252, val_acc:0.883]
Epoch [44/120    avg_loss:0.181, val_acc:0.929]
Epoch [45/120    avg_loss:0.115, val_acc:0.934]
Epoch [46/120    avg_loss:0.110, val_acc:0.950]
Epoch [47/120    avg_loss:0.086, val_acc:0.941]
Epoch [48/120    avg_loss:0.077, val_acc:0.943]
Epoch [49/120    avg_loss:0.088, val_acc:0.947]
Epoch [50/120    avg_loss:0.071, val_acc:0.953]
Epoch [51/120    avg_loss:0.055, val_acc:0.955]
Epoch [52/120    avg_loss:0.057, val_acc:0.955]
Epoch [53/120    avg_loss:0.045, val_acc:0.965]
Epoch [54/120    avg_loss:0.039, val_acc:0.951]
Epoch [55/120    avg_loss:0.048, val_acc:0.946]
Epoch [56/120    avg_loss:0.045, val_acc:0.949]
Epoch [57/120    avg_loss:0.043, val_acc:0.958]
Epoch [58/120    avg_loss:0.043, val_acc:0.942]
Epoch [59/120    avg_loss:0.040, val_acc:0.967]
Epoch [60/120    avg_loss:0.035, val_acc:0.951]
Epoch [61/120    avg_loss:0.055, val_acc:0.954]
Epoch [62/120    avg_loss:0.030, val_acc:0.959]
Epoch [63/120    avg_loss:0.026, val_acc:0.963]
Epoch [64/120    avg_loss:0.033, val_acc:0.965]
Epoch [65/120    avg_loss:0.028, val_acc:0.961]
Epoch [66/120    avg_loss:0.027, val_acc:0.959]
Epoch [67/120    avg_loss:0.053, val_acc:0.958]
Epoch [68/120    avg_loss:0.049, val_acc:0.940]
Epoch [69/120    avg_loss:0.166, val_acc:0.928]
Epoch [70/120    avg_loss:0.088, val_acc:0.941]
Epoch [71/120    avg_loss:0.059, val_acc:0.935]
Epoch [72/120    avg_loss:0.050, val_acc:0.935]
Epoch [73/120    avg_loss:0.037, val_acc:0.958]
Epoch [74/120    avg_loss:0.034, val_acc:0.963]
Epoch [75/120    avg_loss:0.032, val_acc:0.961]
Epoch [76/120    avg_loss:0.027, val_acc:0.961]
Epoch [77/120    avg_loss:0.027, val_acc:0.964]
Epoch [78/120    avg_loss:0.022, val_acc:0.963]
Epoch [79/120    avg_loss:0.025, val_acc:0.965]
Epoch [80/120    avg_loss:0.025, val_acc:0.964]
Epoch [81/120    avg_loss:0.022, val_acc:0.968]
Epoch [82/120    avg_loss:0.027, val_acc:0.967]
Epoch [83/120    avg_loss:0.024, val_acc:0.969]
Epoch [84/120    avg_loss:0.021, val_acc:0.971]
Epoch [85/120    avg_loss:0.019, val_acc:0.971]
Epoch [86/120    avg_loss:0.018, val_acc:0.970]
Epoch [87/120    avg_loss:0.021, val_acc:0.970]
Epoch [88/120    avg_loss:0.020, val_acc:0.969]
Epoch [89/120    avg_loss:0.018, val_acc:0.972]
Epoch [90/120    avg_loss:0.021, val_acc:0.968]
Epoch [91/120    avg_loss:0.019, val_acc:0.970]
Epoch [92/120    avg_loss:0.016, val_acc:0.969]
Epoch [93/120    avg_loss:0.018, val_acc:0.969]
Epoch [94/120    avg_loss:0.017, val_acc:0.970]
Epoch [95/120    avg_loss:0.016, val_acc:0.970]
Epoch [96/120    avg_loss:0.018, val_acc:0.972]
Epoch [97/120    avg_loss:0.020, val_acc:0.969]
Epoch [98/120    avg_loss:0.017, val_acc:0.970]
Epoch [99/120    avg_loss:0.020, val_acc:0.970]
Epoch [100/120    avg_loss:0.016, val_acc:0.971]
Epoch [101/120    avg_loss:0.018, val_acc:0.969]
Epoch [102/120    avg_loss:0.018, val_acc:0.969]
Epoch [103/120    avg_loss:0.021, val_acc:0.969]
Epoch [104/120    avg_loss:0.019, val_acc:0.969]
Epoch [105/120    avg_loss:0.016, val_acc:0.971]
Epoch [106/120    avg_loss:0.018, val_acc:0.969]
Epoch [107/120    avg_loss:0.019, val_acc:0.973]
Epoch [108/120    avg_loss:0.017, val_acc:0.970]
Epoch [109/120    avg_loss:0.016, val_acc:0.969]
Epoch [110/120    avg_loss:0.015, val_acc:0.971]
Epoch [111/120    avg_loss:0.015, val_acc:0.971]
Epoch [112/120    avg_loss:0.018, val_acc:0.970]
Epoch [113/120    avg_loss:0.016, val_acc:0.971]
Epoch [114/120    avg_loss:0.015, val_acc:0.972]
Epoch [115/120    avg_loss:0.020, val_acc:0.970]
Epoch [116/120    avg_loss:0.018, val_acc:0.971]
Epoch [117/120    avg_loss:0.017, val_acc:0.973]
Epoch [118/120    avg_loss:0.021, val_acc:0.971]
Epoch [119/120    avg_loss:0.018, val_acc:0.972]
Epoch [120/120    avg_loss:0.016, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    1    3    0    0    0    0    1    0   13    0    0
     0    0    0]
 [   0    0    0  722    9    0    0    0    0    3    1   10    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    3    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    1    0    0    0    0  826   41    0    0
     0    0    0]
 [   0    0   22    0    0    0    0    0    0    1   27 2143   16    0
     0    1    0]
 [   0    0    0    4    0    0    0    0    0    0    1    9  516    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
    43  303    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.37669376693766

F1 scores:
[       nan 1.         0.98179    0.97964722 0.97260274 0.98842593
 0.99694656 0.94339623 1.         0.87804878 0.95491329 0.9677128
 0.96538821 1.         0.97542044 0.91679274 0.98224852]

Kappa:
0.9700779232560153
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f00bcda8860>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.667, val_acc:0.333]
Epoch [2/120    avg_loss:2.325, val_acc:0.429]
Epoch [3/120    avg_loss:2.129, val_acc:0.507]
Epoch [4/120    avg_loss:1.972, val_acc:0.539]
Epoch [5/120    avg_loss:1.790, val_acc:0.580]
Epoch [6/120    avg_loss:1.681, val_acc:0.588]
Epoch [7/120    avg_loss:1.534, val_acc:0.617]
Epoch [8/120    avg_loss:1.403, val_acc:0.603]
Epoch [9/120    avg_loss:1.256, val_acc:0.636]
Epoch [10/120    avg_loss:1.143, val_acc:0.660]
Epoch [11/120    avg_loss:1.040, val_acc:0.681]
Epoch [12/120    avg_loss:0.903, val_acc:0.724]
Epoch [13/120    avg_loss:0.840, val_acc:0.714]
Epoch [14/120    avg_loss:0.727, val_acc:0.757]
Epoch [15/120    avg_loss:0.598, val_acc:0.794]
Epoch [16/120    avg_loss:0.576, val_acc:0.766]
Epoch [17/120    avg_loss:0.582, val_acc:0.746]
Epoch [18/120    avg_loss:0.584, val_acc:0.783]
Epoch [19/120    avg_loss:0.414, val_acc:0.820]
Epoch [20/120    avg_loss:0.416, val_acc:0.846]
Epoch [21/120    avg_loss:0.369, val_acc:0.831]
Epoch [22/120    avg_loss:0.357, val_acc:0.868]
Epoch [23/120    avg_loss:0.343, val_acc:0.867]
Epoch [24/120    avg_loss:0.329, val_acc:0.856]
Epoch [25/120    avg_loss:0.291, val_acc:0.871]
Epoch [26/120    avg_loss:0.260, val_acc:0.827]
Epoch [27/120    avg_loss:0.244, val_acc:0.875]
Epoch [28/120    avg_loss:0.223, val_acc:0.834]
Epoch [29/120    avg_loss:0.251, val_acc:0.881]
Epoch [30/120    avg_loss:0.187, val_acc:0.884]
Epoch [31/120    avg_loss:0.174, val_acc:0.905]
Epoch [32/120    avg_loss:0.139, val_acc:0.908]
Epoch [33/120    avg_loss:0.145, val_acc:0.919]
Epoch [34/120    avg_loss:0.124, val_acc:0.914]
Epoch [35/120    avg_loss:0.135, val_acc:0.910]
Epoch [36/120    avg_loss:0.129, val_acc:0.912]
Epoch [37/120    avg_loss:0.093, val_acc:0.897]
Epoch [38/120    avg_loss:0.103, val_acc:0.919]
Epoch [39/120    avg_loss:0.111, val_acc:0.920]
Epoch [40/120    avg_loss:0.150, val_acc:0.885]
Epoch [41/120    avg_loss:0.117, val_acc:0.929]
Epoch [42/120    avg_loss:0.086, val_acc:0.940]
Epoch [43/120    avg_loss:0.075, val_acc:0.940]
Epoch [44/120    avg_loss:0.078, val_acc:0.939]
Epoch [45/120    avg_loss:0.070, val_acc:0.945]
Epoch [46/120    avg_loss:0.064, val_acc:0.946]
Epoch [47/120    avg_loss:0.073, val_acc:0.944]
Epoch [48/120    avg_loss:0.083, val_acc:0.939]
Epoch [49/120    avg_loss:0.083, val_acc:0.946]
Epoch [50/120    avg_loss:0.068, val_acc:0.955]
Epoch [51/120    avg_loss:0.072, val_acc:0.938]
Epoch [52/120    avg_loss:0.058, val_acc:0.938]
Epoch [53/120    avg_loss:0.055, val_acc:0.946]
Epoch [54/120    avg_loss:0.067, val_acc:0.946]
Epoch [55/120    avg_loss:0.057, val_acc:0.950]
Epoch [56/120    avg_loss:0.047, val_acc:0.954]
Epoch [57/120    avg_loss:0.046, val_acc:0.943]
Epoch [58/120    avg_loss:0.048, val_acc:0.959]
Epoch [59/120    avg_loss:0.057, val_acc:0.954]
Epoch [60/120    avg_loss:0.053, val_acc:0.959]
Epoch [61/120    avg_loss:0.043, val_acc:0.956]
Epoch [62/120    avg_loss:0.065, val_acc:0.961]
Epoch [63/120    avg_loss:0.046, val_acc:0.961]
Epoch [64/120    avg_loss:0.032, val_acc:0.960]
Epoch [65/120    avg_loss:0.031, val_acc:0.958]
Epoch [66/120    avg_loss:0.025, val_acc:0.961]
Epoch [67/120    avg_loss:0.035, val_acc:0.948]
Epoch [68/120    avg_loss:0.032, val_acc:0.952]
Epoch [69/120    avg_loss:0.031, val_acc:0.960]
Epoch [70/120    avg_loss:0.026, val_acc:0.958]
Epoch [71/120    avg_loss:0.040, val_acc:0.958]
Epoch [72/120    avg_loss:0.045, val_acc:0.952]
Epoch [73/120    avg_loss:0.037, val_acc:0.959]
Epoch [74/120    avg_loss:0.034, val_acc:0.956]
Epoch [75/120    avg_loss:0.029, val_acc:0.973]
Epoch [76/120    avg_loss:0.020, val_acc:0.970]
Epoch [77/120    avg_loss:0.026, val_acc:0.967]
Epoch [78/120    avg_loss:0.023, val_acc:0.968]
Epoch [79/120    avg_loss:0.026, val_acc:0.970]
Epoch [80/120    avg_loss:0.021, val_acc:0.971]
Epoch [81/120    avg_loss:0.017, val_acc:0.973]
Epoch [82/120    avg_loss:0.014, val_acc:0.968]
Epoch [83/120    avg_loss:0.016, val_acc:0.974]
Epoch [84/120    avg_loss:0.021, val_acc:0.972]
Epoch [85/120    avg_loss:0.016, val_acc:0.975]
Epoch [86/120    avg_loss:0.021, val_acc:0.968]
Epoch [87/120    avg_loss:0.019, val_acc:0.958]
Epoch [88/120    avg_loss:0.019, val_acc:0.963]
Epoch [89/120    avg_loss:0.013, val_acc:0.977]
Epoch [90/120    avg_loss:0.017, val_acc:0.946]
Epoch [91/120    avg_loss:0.031, val_acc:0.964]
Epoch [92/120    avg_loss:0.018, val_acc:0.971]
Epoch [93/120    avg_loss:0.022, val_acc:0.979]
Epoch [94/120    avg_loss:0.013, val_acc:0.971]
Epoch [95/120    avg_loss:0.019, val_acc:0.966]
Epoch [96/120    avg_loss:0.025, val_acc:0.972]
Epoch [97/120    avg_loss:0.024, val_acc:0.950]
Epoch [98/120    avg_loss:0.018, val_acc:0.975]
Epoch [99/120    avg_loss:0.035, val_acc:0.969]
Epoch [100/120    avg_loss:0.016, val_acc:0.974]
Epoch [101/120    avg_loss:0.021, val_acc:0.969]
Epoch [102/120    avg_loss:0.013, val_acc:0.973]
Epoch [103/120    avg_loss:0.012, val_acc:0.980]
Epoch [104/120    avg_loss:0.011, val_acc:0.977]
Epoch [105/120    avg_loss:0.007, val_acc:0.982]
Epoch [106/120    avg_loss:0.009, val_acc:0.983]
Epoch [107/120    avg_loss:0.008, val_acc:0.983]
Epoch [108/120    avg_loss:0.009, val_acc:0.980]
Epoch [109/120    avg_loss:0.008, val_acc:0.972]
Epoch [110/120    avg_loss:0.011, val_acc:0.972]
Epoch [111/120    avg_loss:0.096, val_acc:0.895]
Epoch [112/120    avg_loss:0.196, val_acc:0.919]
Epoch [113/120    avg_loss:0.145, val_acc:0.847]
Epoch [114/120    avg_loss:0.180, val_acc:0.944]
Epoch [115/120    avg_loss:0.131, val_acc:0.951]
Epoch [116/120    avg_loss:0.090, val_acc:0.944]
Epoch [117/120    avg_loss:0.072, val_acc:0.953]
Epoch [118/120    avg_loss:0.043, val_acc:0.949]
Epoch [119/120    avg_loss:0.044, val_acc:0.959]
Epoch [120/120    avg_loss:0.030, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    5    2    0    0    0    0    0    0   17    0    0
     0    0    0]
 [   0    0    0  722    0    0    0    0    0    1    1   14    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    1    0    0    0    1  828   39    0    0
     0    1    0]
 [   0    0   17    0    0    0    0    0    0    1   35 2148    9    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0    0    0    1  525    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    0    0    0    0
  1110   22    0]
 [   0    0    0    0    0    0    0    0    0    3    0    0    0    0
    24  320    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.48509485094851

F1 scores:
[       nan 0.95121951 0.98132296 0.97699594 0.9953271  0.98742857
 0.99847561 1.         0.997669   0.85714286 0.95227142 0.96975169
 0.97493036 1.         0.97496706 0.92485549 0.98823529]

Kappa:
0.971326184851091
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdf8f7da828>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.661, val_acc:0.301]
Epoch [2/120    avg_loss:2.301, val_acc:0.328]
Epoch [3/120    avg_loss:2.126, val_acc:0.509]
Epoch [4/120    avg_loss:1.958, val_acc:0.540]
Epoch [5/120    avg_loss:1.808, val_acc:0.535]
Epoch [6/120    avg_loss:1.698, val_acc:0.575]
Epoch [7/120    avg_loss:1.538, val_acc:0.580]
Epoch [8/120    avg_loss:1.370, val_acc:0.605]
Epoch [9/120    avg_loss:1.277, val_acc:0.623]
Epoch [10/120    avg_loss:1.163, val_acc:0.648]
Epoch [11/120    avg_loss:1.066, val_acc:0.700]
Epoch [12/120    avg_loss:0.936, val_acc:0.705]
Epoch [13/120    avg_loss:0.844, val_acc:0.755]
Epoch [14/120    avg_loss:0.747, val_acc:0.777]
Epoch [15/120    avg_loss:0.690, val_acc:0.753]
Epoch [16/120    avg_loss:0.668, val_acc:0.732]
Epoch [17/120    avg_loss:0.618, val_acc:0.769]
Epoch [18/120    avg_loss:0.512, val_acc:0.807]
Epoch [19/120    avg_loss:0.411, val_acc:0.842]
Epoch [20/120    avg_loss:0.382, val_acc:0.807]
Epoch [21/120    avg_loss:0.371, val_acc:0.856]
Epoch [22/120    avg_loss:0.308, val_acc:0.861]
Epoch [23/120    avg_loss:0.322, val_acc:0.866]
Epoch [24/120    avg_loss:0.289, val_acc:0.852]
Epoch [25/120    avg_loss:0.288, val_acc:0.890]
Epoch [26/120    avg_loss:0.201, val_acc:0.914]
Epoch [27/120    avg_loss:0.192, val_acc:0.895]
Epoch [28/120    avg_loss:0.261, val_acc:0.797]
Epoch [29/120    avg_loss:0.252, val_acc:0.886]
Epoch [30/120    avg_loss:0.238, val_acc:0.900]
Epoch [31/120    avg_loss:0.319, val_acc:0.893]
Epoch [32/120    avg_loss:0.264, val_acc:0.884]
Epoch [33/120    avg_loss:0.206, val_acc:0.919]
Epoch [34/120    avg_loss:0.188, val_acc:0.876]
Epoch [35/120    avg_loss:0.158, val_acc:0.926]
Epoch [36/120    avg_loss:0.124, val_acc:0.926]
Epoch [37/120    avg_loss:0.114, val_acc:0.941]
Epoch [38/120    avg_loss:0.109, val_acc:0.923]
Epoch [39/120    avg_loss:0.113, val_acc:0.926]
Epoch [40/120    avg_loss:0.131, val_acc:0.880]
Epoch [41/120    avg_loss:0.108, val_acc:0.941]
Epoch [42/120    avg_loss:0.089, val_acc:0.935]
Epoch [43/120    avg_loss:0.106, val_acc:0.921]
Epoch [44/120    avg_loss:0.078, val_acc:0.952]
Epoch [45/120    avg_loss:0.069, val_acc:0.952]
Epoch [46/120    avg_loss:0.071, val_acc:0.930]
Epoch [47/120    avg_loss:0.073, val_acc:0.956]
Epoch [48/120    avg_loss:0.062, val_acc:0.945]
Epoch [49/120    avg_loss:0.082, val_acc:0.926]
Epoch [50/120    avg_loss:0.067, val_acc:0.949]
Epoch [51/120    avg_loss:0.050, val_acc:0.947]
Epoch [52/120    avg_loss:0.051, val_acc:0.946]
Epoch [53/120    avg_loss:0.049, val_acc:0.953]
Epoch [54/120    avg_loss:0.047, val_acc:0.934]
Epoch [55/120    avg_loss:0.043, val_acc:0.959]
Epoch [56/120    avg_loss:0.039, val_acc:0.957]
Epoch [57/120    avg_loss:0.034, val_acc:0.958]
Epoch [58/120    avg_loss:0.036, val_acc:0.958]
Epoch [59/120    avg_loss:0.033, val_acc:0.969]
Epoch [60/120    avg_loss:0.031, val_acc:0.952]
Epoch [61/120    avg_loss:0.049, val_acc:0.939]
Epoch [62/120    avg_loss:0.050, val_acc:0.930]
Epoch [63/120    avg_loss:0.076, val_acc:0.959]
Epoch [64/120    avg_loss:0.063, val_acc:0.958]
Epoch [65/120    avg_loss:0.042, val_acc:0.958]
Epoch [66/120    avg_loss:0.032, val_acc:0.969]
Epoch [67/120    avg_loss:0.034, val_acc:0.960]
Epoch [68/120    avg_loss:0.031, val_acc:0.965]
Epoch [69/120    avg_loss:0.030, val_acc:0.961]
Epoch [70/120    avg_loss:0.032, val_acc:0.966]
Epoch [71/120    avg_loss:0.026, val_acc:0.952]
Epoch [72/120    avg_loss:0.026, val_acc:0.968]
Epoch [73/120    avg_loss:0.021, val_acc:0.955]
Epoch [74/120    avg_loss:0.021, val_acc:0.964]
Epoch [75/120    avg_loss:0.017, val_acc:0.956]
Epoch [76/120    avg_loss:0.016, val_acc:0.968]
Epoch [77/120    avg_loss:0.015, val_acc:0.969]
Epoch [78/120    avg_loss:0.017, val_acc:0.971]
Epoch [79/120    avg_loss:0.019, val_acc:0.961]
Epoch [80/120    avg_loss:0.014, val_acc:0.959]
Epoch [81/120    avg_loss:0.015, val_acc:0.960]
Epoch [82/120    avg_loss:0.023, val_acc:0.960]
Epoch [83/120    avg_loss:0.012, val_acc:0.971]
Epoch [84/120    avg_loss:0.013, val_acc:0.968]
Epoch [85/120    avg_loss:0.012, val_acc:0.968]
Epoch [86/120    avg_loss:0.012, val_acc:0.974]
Epoch [87/120    avg_loss:0.010, val_acc:0.967]
Epoch [88/120    avg_loss:0.009, val_acc:0.969]
Epoch [89/120    avg_loss:0.009, val_acc:0.971]
Epoch [90/120    avg_loss:0.012, val_acc:0.970]
Epoch [91/120    avg_loss:0.011, val_acc:0.971]
Epoch [92/120    avg_loss:0.012, val_acc:0.961]
Epoch [93/120    avg_loss:0.018, val_acc:0.963]
Epoch [94/120    avg_loss:0.020, val_acc:0.964]
Epoch [95/120    avg_loss:0.011, val_acc:0.968]
Epoch [96/120    avg_loss:0.010, val_acc:0.973]
Epoch [97/120    avg_loss:0.014, val_acc:0.963]
Epoch [98/120    avg_loss:0.011, val_acc:0.969]
Epoch [99/120    avg_loss:0.017, val_acc:0.977]
Epoch [100/120    avg_loss:0.016, val_acc:0.971]
Epoch [101/120    avg_loss:0.009, val_acc:0.975]
Epoch [102/120    avg_loss:0.010, val_acc:0.972]
Epoch [103/120    avg_loss:0.010, val_acc:0.973]
Epoch [104/120    avg_loss:0.009, val_acc:0.973]
Epoch [105/120    avg_loss:0.007, val_acc:0.969]
Epoch [106/120    avg_loss:0.009, val_acc:0.965]
Epoch [107/120    avg_loss:0.011, val_acc:0.975]
Epoch [108/120    avg_loss:0.009, val_acc:0.973]
Epoch [109/120    avg_loss:0.008, val_acc:0.977]
Epoch [110/120    avg_loss:0.008, val_acc:0.980]
Epoch [111/120    avg_loss:0.006, val_acc:0.974]
Epoch [112/120    avg_loss:0.007, val_acc:0.976]
Epoch [113/120    avg_loss:0.006, val_acc:0.972]
Epoch [114/120    avg_loss:0.006, val_acc:0.976]
Epoch [115/120    avg_loss:0.006, val_acc:0.972]
Epoch [116/120    avg_loss:0.005, val_acc:0.980]
Epoch [117/120    avg_loss:0.007, val_acc:0.975]
Epoch [118/120    avg_loss:0.005, val_acc:0.976]
Epoch [119/120    avg_loss:0.005, val_acc:0.976]
Epoch [120/120    avg_loss:0.006, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    1    0    1    0
     0    0    0]
 [   0    0 1253    5    0    1    0    0    0    0    3   22    1    0
     0    0    0]
 [   0    0    3  725    1    0    0    0    0    2    2   11    3    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  854   14    1    0
     0    0    0]
 [   0    0   14    0    0    0    0    0    0    0   11 2171   12    0
     1    1    0]
 [   0    0    2    0    0    0    0    0    0    0    0    3  524    0
     3    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
    68  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.64769647696477

F1 scores:
[       nan 0.975      0.97776044 0.98105548 0.99530516 0.99308756
 0.99847561 1.         1.         0.94736842 0.97823597 0.97947214
 0.97126969 0.99728997 0.96245734 0.87147335 0.97590361]

Kappa:
0.9731588362144966
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3f57e81780>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.700, val_acc:0.425]
Epoch [2/120    avg_loss:2.331, val_acc:0.432]
Epoch [3/120    avg_loss:2.086, val_acc:0.515]
Epoch [4/120    avg_loss:1.909, val_acc:0.526]
Epoch [5/120    avg_loss:1.751, val_acc:0.568]
Epoch [6/120    avg_loss:1.615, val_acc:0.582]
Epoch [7/120    avg_loss:1.530, val_acc:0.581]
Epoch [8/120    avg_loss:1.380, val_acc:0.616]
Epoch [9/120    avg_loss:1.213, val_acc:0.647]
Epoch [10/120    avg_loss:1.063, val_acc:0.657]
Epoch [11/120    avg_loss:1.012, val_acc:0.733]
Epoch [12/120    avg_loss:0.929, val_acc:0.636]
Epoch [13/120    avg_loss:0.822, val_acc:0.668]
Epoch [14/120    avg_loss:0.712, val_acc:0.781]
Epoch [15/120    avg_loss:0.662, val_acc:0.753]
Epoch [16/120    avg_loss:0.665, val_acc:0.790]
Epoch [17/120    avg_loss:0.534, val_acc:0.822]
Epoch [18/120    avg_loss:0.475, val_acc:0.760]
Epoch [19/120    avg_loss:0.434, val_acc:0.836]
Epoch [20/120    avg_loss:0.409, val_acc:0.864]
Epoch [21/120    avg_loss:0.371, val_acc:0.848]
Epoch [22/120    avg_loss:0.330, val_acc:0.869]
Epoch [23/120    avg_loss:0.345, val_acc:0.864]
Epoch [24/120    avg_loss:0.291, val_acc:0.878]
Epoch [25/120    avg_loss:0.261, val_acc:0.905]
Epoch [26/120    avg_loss:0.295, val_acc:0.885]
Epoch [27/120    avg_loss:0.405, val_acc:0.868]
Epoch [28/120    avg_loss:0.295, val_acc:0.876]
Epoch [29/120    avg_loss:0.251, val_acc:0.898]
Epoch [30/120    avg_loss:0.218, val_acc:0.868]
Epoch [31/120    avg_loss:0.191, val_acc:0.918]
Epoch [32/120    avg_loss:0.156, val_acc:0.909]
Epoch [33/120    avg_loss:0.146, val_acc:0.924]
Epoch [34/120    avg_loss:0.120, val_acc:0.924]
Epoch [35/120    avg_loss:0.229, val_acc:0.893]
Epoch [36/120    avg_loss:0.146, val_acc:0.918]
Epoch [37/120    avg_loss:0.126, val_acc:0.926]
Epoch [38/120    avg_loss:0.138, val_acc:0.910]
Epoch [39/120    avg_loss:0.123, val_acc:0.926]
Epoch [40/120    avg_loss:0.104, val_acc:0.945]
Epoch [41/120    avg_loss:0.083, val_acc:0.941]
Epoch [42/120    avg_loss:0.091, val_acc:0.935]
Epoch [43/120    avg_loss:0.089, val_acc:0.932]
Epoch [44/120    avg_loss:0.109, val_acc:0.947]
Epoch [45/120    avg_loss:0.086, val_acc:0.950]
Epoch [46/120    avg_loss:0.082, val_acc:0.957]
Epoch [47/120    avg_loss:0.072, val_acc:0.946]
Epoch [48/120    avg_loss:0.070, val_acc:0.927]
Epoch [49/120    avg_loss:0.085, val_acc:0.952]
Epoch [50/120    avg_loss:0.066, val_acc:0.958]
Epoch [51/120    avg_loss:0.056, val_acc:0.951]
Epoch [52/120    avg_loss:0.054, val_acc:0.957]
Epoch [53/120    avg_loss:0.038, val_acc:0.963]
Epoch [54/120    avg_loss:0.049, val_acc:0.971]
Epoch [55/120    avg_loss:0.045, val_acc:0.966]
Epoch [56/120    avg_loss:0.049, val_acc:0.956]
Epoch [57/120    avg_loss:0.041, val_acc:0.971]
Epoch [58/120    avg_loss:0.044, val_acc:0.967]
Epoch [59/120    avg_loss:0.041, val_acc:0.972]
Epoch [60/120    avg_loss:0.032, val_acc:0.968]
Epoch [61/120    avg_loss:0.033, val_acc:0.971]
Epoch [62/120    avg_loss:0.046, val_acc:0.968]
Epoch [63/120    avg_loss:0.048, val_acc:0.967]
Epoch [64/120    avg_loss:0.038, val_acc:0.954]
Epoch [65/120    avg_loss:0.031, val_acc:0.969]
Epoch [66/120    avg_loss:0.028, val_acc:0.972]
Epoch [67/120    avg_loss:0.033, val_acc:0.967]
Epoch [68/120    avg_loss:0.052, val_acc:0.965]
Epoch [69/120    avg_loss:0.038, val_acc:0.970]
Epoch [70/120    avg_loss:0.040, val_acc:0.954]
Epoch [71/120    avg_loss:0.040, val_acc:0.959]
Epoch [72/120    avg_loss:0.035, val_acc:0.971]
Epoch [73/120    avg_loss:0.024, val_acc:0.972]
Epoch [74/120    avg_loss:0.019, val_acc:0.976]
Epoch [75/120    avg_loss:0.027, val_acc:0.971]
Epoch [76/120    avg_loss:0.021, val_acc:0.974]
Epoch [77/120    avg_loss:0.018, val_acc:0.975]
Epoch [78/120    avg_loss:0.014, val_acc:0.978]
Epoch [79/120    avg_loss:0.015, val_acc:0.976]
Epoch [80/120    avg_loss:0.016, val_acc:0.979]
Epoch [81/120    avg_loss:0.017, val_acc:0.979]
Epoch [82/120    avg_loss:0.036, val_acc:0.954]
Epoch [83/120    avg_loss:0.048, val_acc:0.965]
Epoch [84/120    avg_loss:0.058, val_acc:0.959]
Epoch [85/120    avg_loss:0.046, val_acc:0.965]
Epoch [86/120    avg_loss:0.035, val_acc:0.978]
Epoch [87/120    avg_loss:0.025, val_acc:0.976]
Epoch [88/120    avg_loss:0.022, val_acc:0.979]
Epoch [89/120    avg_loss:0.020, val_acc:0.972]
Epoch [90/120    avg_loss:0.023, val_acc:0.967]
Epoch [91/120    avg_loss:0.022, val_acc:0.965]
Epoch [92/120    avg_loss:0.023, val_acc:0.968]
Epoch [93/120    avg_loss:0.019, val_acc:0.975]
Epoch [94/120    avg_loss:0.014, val_acc:0.983]
Epoch [95/120    avg_loss:0.015, val_acc:0.975]
Epoch [96/120    avg_loss:0.012, val_acc:0.980]
Epoch [97/120    avg_loss:0.011, val_acc:0.977]
Epoch [98/120    avg_loss:0.013, val_acc:0.977]
Epoch [99/120    avg_loss:0.014, val_acc:0.970]
Epoch [100/120    avg_loss:0.012, val_acc:0.976]
Epoch [101/120    avg_loss:0.009, val_acc:0.978]
Epoch [102/120    avg_loss:0.009, val_acc:0.977]
Epoch [103/120    avg_loss:0.011, val_acc:0.977]
Epoch [104/120    avg_loss:0.018, val_acc:0.972]
Epoch [105/120    avg_loss:0.021, val_acc:0.976]
Epoch [106/120    avg_loss:0.016, val_acc:0.973]
Epoch [107/120    avg_loss:0.011, val_acc:0.976]
Epoch [108/120    avg_loss:0.009, val_acc:0.974]
Epoch [109/120    avg_loss:0.010, val_acc:0.976]
Epoch [110/120    avg_loss:0.007, val_acc:0.976]
Epoch [111/120    avg_loss:0.006, val_acc:0.976]
Epoch [112/120    avg_loss:0.007, val_acc:0.978]
Epoch [113/120    avg_loss:0.007, val_acc:0.977]
Epoch [114/120    avg_loss:0.006, val_acc:0.979]
Epoch [115/120    avg_loss:0.007, val_acc:0.979]
Epoch [116/120    avg_loss:0.006, val_acc:0.978]
Epoch [117/120    avg_loss:0.007, val_acc:0.979]
Epoch [118/120    avg_loss:0.006, val_acc:0.980]
Epoch [119/120    avg_loss:0.007, val_acc:0.980]
Epoch [120/120    avg_loss:0.009, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1250   10    3    0    1    0    0    0    1   20    0    0
     0    0    0]
 [   0    0    0  734    1    0    0    0    0    0    0   11    1    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  845   23    1    0
     0    0    0]
 [   0    0   16    0    0    0    0    0    0    0   10 2177    7    0
     0    0    0]
 [   0    0    0    8    0    0    0    0    0    0    0    2  523    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1127   12    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    88  259    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.51761517615176

F1 scores:
[       nan 1.         0.97770825 0.97931955 0.99069767 0.99653979
 0.99695122 1.         1.         1.         0.97631427 0.97952756
 0.98031865 1.         0.95589483 0.8381877  0.98809524]

Kappa:
0.971665115431131
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f222a8a67f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.708, val_acc:0.344]
Epoch [2/120    avg_loss:2.333, val_acc:0.461]
Epoch [3/120    avg_loss:2.137, val_acc:0.432]
Epoch [4/120    avg_loss:1.967, val_acc:0.475]
Epoch [5/120    avg_loss:1.778, val_acc:0.505]
Epoch [6/120    avg_loss:1.643, val_acc:0.518]
Epoch [7/120    avg_loss:1.483, val_acc:0.569]
Epoch [8/120    avg_loss:1.357, val_acc:0.565]
Epoch [9/120    avg_loss:1.228, val_acc:0.667]
Epoch [10/120    avg_loss:1.128, val_acc:0.649]
Epoch [11/120    avg_loss:1.034, val_acc:0.709]
Epoch [12/120    avg_loss:0.927, val_acc:0.743]
Epoch [13/120    avg_loss:0.864, val_acc:0.752]
Epoch [14/120    avg_loss:0.746, val_acc:0.794]
Epoch [15/120    avg_loss:0.615, val_acc:0.805]
Epoch [16/120    avg_loss:0.515, val_acc:0.808]
Epoch [17/120    avg_loss:0.511, val_acc:0.822]
Epoch [18/120    avg_loss:0.412, val_acc:0.845]
Epoch [19/120    avg_loss:0.353, val_acc:0.867]
Epoch [20/120    avg_loss:0.327, val_acc:0.854]
Epoch [21/120    avg_loss:0.289, val_acc:0.852]
Epoch [22/120    avg_loss:0.280, val_acc:0.846]
Epoch [23/120    avg_loss:0.370, val_acc:0.858]
Epoch [24/120    avg_loss:0.286, val_acc:0.850]
Epoch [25/120    avg_loss:0.285, val_acc:0.872]
Epoch [26/120    avg_loss:0.209, val_acc:0.912]
Epoch [27/120    avg_loss:0.199, val_acc:0.886]
Epoch [28/120    avg_loss:0.179, val_acc:0.877]
Epoch [29/120    avg_loss:0.155, val_acc:0.919]
Epoch [30/120    avg_loss:0.189, val_acc:0.884]
Epoch [31/120    avg_loss:0.169, val_acc:0.918]
Epoch [32/120    avg_loss:0.150, val_acc:0.925]
Epoch [33/120    avg_loss:0.160, val_acc:0.910]
Epoch [34/120    avg_loss:0.143, val_acc:0.943]
Epoch [35/120    avg_loss:0.103, val_acc:0.903]
Epoch [36/120    avg_loss:0.140, val_acc:0.895]
Epoch [37/120    avg_loss:0.138, val_acc:0.923]
Epoch [38/120    avg_loss:0.122, val_acc:0.930]
Epoch [39/120    avg_loss:0.087, val_acc:0.939]
Epoch [40/120    avg_loss:0.066, val_acc:0.940]
Epoch [41/120    avg_loss:0.074, val_acc:0.946]
Epoch [42/120    avg_loss:0.068, val_acc:0.947]
Epoch [43/120    avg_loss:0.056, val_acc:0.940]
Epoch [44/120    avg_loss:0.066, val_acc:0.941]
Epoch [45/120    avg_loss:0.058, val_acc:0.926]
Epoch [46/120    avg_loss:0.084, val_acc:0.951]
Epoch [47/120    avg_loss:0.056, val_acc:0.950]
Epoch [48/120    avg_loss:0.062, val_acc:0.936]
Epoch [49/120    avg_loss:0.075, val_acc:0.926]
Epoch [50/120    avg_loss:0.084, val_acc:0.941]
Epoch [51/120    avg_loss:0.056, val_acc:0.947]
Epoch [52/120    avg_loss:0.042, val_acc:0.953]
Epoch [53/120    avg_loss:0.033, val_acc:0.963]
Epoch [54/120    avg_loss:0.035, val_acc:0.958]
Epoch [55/120    avg_loss:0.067, val_acc:0.939]
Epoch [56/120    avg_loss:0.070, val_acc:0.957]
Epoch [57/120    avg_loss:0.043, val_acc:0.954]
Epoch [58/120    avg_loss:0.039, val_acc:0.955]
Epoch [59/120    avg_loss:0.031, val_acc:0.953]
Epoch [60/120    avg_loss:0.029, val_acc:0.958]
Epoch [61/120    avg_loss:0.021, val_acc:0.961]
Epoch [62/120    avg_loss:0.037, val_acc:0.948]
Epoch [63/120    avg_loss:0.029, val_acc:0.960]
Epoch [64/120    avg_loss:0.022, val_acc:0.954]
Epoch [65/120    avg_loss:0.023, val_acc:0.961]
Epoch [66/120    avg_loss:0.016, val_acc:0.956]
Epoch [67/120    avg_loss:0.020, val_acc:0.964]
Epoch [68/120    avg_loss:0.017, val_acc:0.968]
Epoch [69/120    avg_loss:0.017, val_acc:0.965]
Epoch [70/120    avg_loss:0.018, val_acc:0.966]
Epoch [71/120    avg_loss:0.015, val_acc:0.966]
Epoch [72/120    avg_loss:0.013, val_acc:0.967]
Epoch [73/120    avg_loss:0.019, val_acc:0.964]
Epoch [74/120    avg_loss:0.015, val_acc:0.966]
Epoch [75/120    avg_loss:0.015, val_acc:0.965]
Epoch [76/120    avg_loss:0.014, val_acc:0.965]
Epoch [77/120    avg_loss:0.013, val_acc:0.965]
Epoch [78/120    avg_loss:0.017, val_acc:0.961]
Epoch [79/120    avg_loss:0.015, val_acc:0.963]
Epoch [80/120    avg_loss:0.017, val_acc:0.965]
Epoch [81/120    avg_loss:0.014, val_acc:0.963]
Epoch [82/120    avg_loss:0.016, val_acc:0.963]
Epoch [83/120    avg_loss:0.017, val_acc:0.963]
Epoch [84/120    avg_loss:0.017, val_acc:0.963]
Epoch [85/120    avg_loss:0.015, val_acc:0.963]
Epoch [86/120    avg_loss:0.015, val_acc:0.963]
Epoch [87/120    avg_loss:0.015, val_acc:0.963]
Epoch [88/120    avg_loss:0.015, val_acc:0.963]
Epoch [89/120    avg_loss:0.017, val_acc:0.963]
Epoch [90/120    avg_loss:0.016, val_acc:0.963]
Epoch [91/120    avg_loss:0.011, val_acc:0.963]
Epoch [92/120    avg_loss:0.015, val_acc:0.963]
Epoch [93/120    avg_loss:0.014, val_acc:0.963]
Epoch [94/120    avg_loss:0.016, val_acc:0.963]
Epoch [95/120    avg_loss:0.012, val_acc:0.963]
Epoch [96/120    avg_loss:0.018, val_acc:0.963]
Epoch [97/120    avg_loss:0.016, val_acc:0.963]
Epoch [98/120    avg_loss:0.013, val_acc:0.963]
Epoch [99/120    avg_loss:0.013, val_acc:0.963]
Epoch [100/120    avg_loss:0.015, val_acc:0.963]
Epoch [101/120    avg_loss:0.015, val_acc:0.963]
Epoch [102/120    avg_loss:0.014, val_acc:0.963]
Epoch [103/120    avg_loss:0.016, val_acc:0.963]
Epoch [104/120    avg_loss:0.014, val_acc:0.963]
Epoch [105/120    avg_loss:0.015, val_acc:0.963]
Epoch [106/120    avg_loss:0.017, val_acc:0.963]
Epoch [107/120    avg_loss:0.016, val_acc:0.963]
Epoch [108/120    avg_loss:0.012, val_acc:0.963]
Epoch [109/120    avg_loss:0.015, val_acc:0.963]
Epoch [110/120    avg_loss:0.015, val_acc:0.963]
Epoch [111/120    avg_loss:0.016, val_acc:0.963]
Epoch [112/120    avg_loss:0.015, val_acc:0.963]
Epoch [113/120    avg_loss:0.012, val_acc:0.963]
Epoch [114/120    avg_loss:0.014, val_acc:0.963]
Epoch [115/120    avg_loss:0.016, val_acc:0.963]
Epoch [116/120    avg_loss:0.013, val_acc:0.963]
Epoch [117/120    avg_loss:0.012, val_acc:0.963]
Epoch [118/120    avg_loss:0.014, val_acc:0.963]
Epoch [119/120    avg_loss:0.014, val_acc:0.963]
Epoch [120/120    avg_loss:0.014, val_acc:0.963]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    2    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0 1239    1    0    0    0    0    0    1    1   41    2    0
     0    0    0]
 [   0    0    0  710    3    0    0    0    0    2    2   19   10    0
     0    1    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  428    0    2    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   14    0    0    2    0    0    0    0  834   23    0    0
     2    0    0]
 [   0    0   14    0    0    0    0    0    0    3   10 2168   15    0
     0    0    0]
 [   0    0    1    2    0    0    0    0    0    0    0    8  521    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1125   14    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    87  260    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.7479674796748

F1 scores:
[       nan 0.93670886 0.9706228  0.97260274 0.99065421 0.98731257
 0.99543379 0.96153846 0.99883586 0.76923077 0.96864111 0.96980541
 0.95948435 1.         0.95379398 0.83601286 0.98224852]

Kappa:
0.9628658648216041
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb8c840b828>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.681, val_acc:0.344]
Epoch [2/120    avg_loss:2.332, val_acc:0.478]
Epoch [3/120    avg_loss:2.131, val_acc:0.523]
Epoch [4/120    avg_loss:1.982, val_acc:0.518]
Epoch [5/120    avg_loss:1.797, val_acc:0.578]
Epoch [6/120    avg_loss:1.704, val_acc:0.581]
Epoch [7/120    avg_loss:1.593, val_acc:0.600]
Epoch [8/120    avg_loss:1.474, val_acc:0.610]
Epoch [9/120    avg_loss:1.315, val_acc:0.681]
Epoch [10/120    avg_loss:1.214, val_acc:0.655]
Epoch [11/120    avg_loss:1.083, val_acc:0.671]
Epoch [12/120    avg_loss:0.935, val_acc:0.711]
Epoch [13/120    avg_loss:0.902, val_acc:0.723]
Epoch [14/120    avg_loss:0.813, val_acc:0.727]
Epoch [15/120    avg_loss:0.706, val_acc:0.775]
Epoch [16/120    avg_loss:0.598, val_acc:0.796]
Epoch [17/120    avg_loss:0.507, val_acc:0.790]
Epoch [18/120    avg_loss:0.453, val_acc:0.826]
Epoch [19/120    avg_loss:0.391, val_acc:0.861]
Epoch [20/120    avg_loss:0.350, val_acc:0.873]
Epoch [21/120    avg_loss:0.339, val_acc:0.860]
Epoch [22/120    avg_loss:0.264, val_acc:0.895]
Epoch [23/120    avg_loss:0.252, val_acc:0.887]
Epoch [24/120    avg_loss:0.258, val_acc:0.897]
Epoch [25/120    avg_loss:0.246, val_acc:0.895]
Epoch [26/120    avg_loss:0.185, val_acc:0.923]
Epoch [27/120    avg_loss:0.231, val_acc:0.905]
Epoch [28/120    avg_loss:0.190, val_acc:0.912]
Epoch [29/120    avg_loss:0.189, val_acc:0.902]
Epoch [30/120    avg_loss:0.175, val_acc:0.917]
Epoch [31/120    avg_loss:0.183, val_acc:0.889]
Epoch [32/120    avg_loss:0.195, val_acc:0.920]
Epoch [33/120    avg_loss:0.151, val_acc:0.929]
Epoch [34/120    avg_loss:0.150, val_acc:0.923]
Epoch [35/120    avg_loss:0.116, val_acc:0.944]
Epoch [36/120    avg_loss:0.114, val_acc:0.945]
Epoch [37/120    avg_loss:0.091, val_acc:0.946]
Epoch [38/120    avg_loss:0.232, val_acc:0.817]
Epoch [39/120    avg_loss:0.335, val_acc:0.898]
Epoch [40/120    avg_loss:0.160, val_acc:0.932]
Epoch [41/120    avg_loss:0.119, val_acc:0.938]
Epoch [42/120    avg_loss:0.093, val_acc:0.952]
Epoch [43/120    avg_loss:0.082, val_acc:0.954]
Epoch [44/120    avg_loss:0.073, val_acc:0.926]
Epoch [45/120    avg_loss:0.061, val_acc:0.957]
Epoch [46/120    avg_loss:0.057, val_acc:0.943]
Epoch [47/120    avg_loss:0.066, val_acc:0.951]
Epoch [48/120    avg_loss:0.078, val_acc:0.963]
Epoch [49/120    avg_loss:0.066, val_acc:0.955]
Epoch [50/120    avg_loss:0.062, val_acc:0.966]
Epoch [51/120    avg_loss:0.057, val_acc:0.964]
Epoch [52/120    avg_loss:0.047, val_acc:0.970]
Epoch [53/120    avg_loss:0.041, val_acc:0.970]
Epoch [54/120    avg_loss:0.034, val_acc:0.969]
Epoch [55/120    avg_loss:0.046, val_acc:0.953]
Epoch [56/120    avg_loss:0.056, val_acc:0.955]
Epoch [57/120    avg_loss:0.170, val_acc:0.943]
Epoch [58/120    avg_loss:0.189, val_acc:0.939]
Epoch [59/120    avg_loss:0.121, val_acc:0.948]
Epoch [60/120    avg_loss:0.096, val_acc:0.954]
Epoch [61/120    avg_loss:0.130, val_acc:0.927]
Epoch [62/120    avg_loss:0.099, val_acc:0.959]
Epoch [63/120    avg_loss:0.065, val_acc:0.959]
Epoch [64/120    avg_loss:0.051, val_acc:0.963]
Epoch [65/120    avg_loss:0.066, val_acc:0.956]
Epoch [66/120    avg_loss:0.047, val_acc:0.976]
Epoch [67/120    avg_loss:0.044, val_acc:0.969]
Epoch [68/120    avg_loss:0.039, val_acc:0.979]
Epoch [69/120    avg_loss:0.029, val_acc:0.980]
Epoch [70/120    avg_loss:0.029, val_acc:0.976]
Epoch [71/120    avg_loss:0.024, val_acc:0.973]
Epoch [72/120    avg_loss:0.030, val_acc:0.975]
Epoch [73/120    avg_loss:0.021, val_acc:0.977]
Epoch [74/120    avg_loss:0.026, val_acc:0.970]
Epoch [75/120    avg_loss:0.025, val_acc:0.980]
Epoch [76/120    avg_loss:0.019, val_acc:0.981]
Epoch [77/120    avg_loss:0.024, val_acc:0.985]
Epoch [78/120    avg_loss:0.022, val_acc:0.983]
Epoch [79/120    avg_loss:0.024, val_acc:0.977]
Epoch [80/120    avg_loss:0.016, val_acc:0.979]
Epoch [81/120    avg_loss:0.020, val_acc:0.982]
Epoch [82/120    avg_loss:0.019, val_acc:0.981]
Epoch [83/120    avg_loss:0.021, val_acc:0.984]
Epoch [84/120    avg_loss:0.014, val_acc:0.985]
Epoch [85/120    avg_loss:0.013, val_acc:0.981]
Epoch [86/120    avg_loss:0.017, val_acc:0.978]
Epoch [87/120    avg_loss:0.019, val_acc:0.981]
Epoch [88/120    avg_loss:0.017, val_acc:0.968]
Epoch [89/120    avg_loss:0.016, val_acc:0.980]
Epoch [90/120    avg_loss:0.020, val_acc:0.979]
Epoch [91/120    avg_loss:0.017, val_acc:0.979]
Epoch [92/120    avg_loss:0.017, val_acc:0.981]
Epoch [93/120    avg_loss:0.017, val_acc:0.983]
Epoch [94/120    avg_loss:0.013, val_acc:0.981]
Epoch [95/120    avg_loss:0.010, val_acc:0.983]
Epoch [96/120    avg_loss:0.010, val_acc:0.981]
Epoch [97/120    avg_loss:0.010, val_acc:0.986]
Epoch [98/120    avg_loss:0.012, val_acc:0.970]
Epoch [99/120    avg_loss:0.024, val_acc:0.983]
Epoch [100/120    avg_loss:0.018, val_acc:0.975]
Epoch [101/120    avg_loss:0.019, val_acc:0.964]
Epoch [102/120    avg_loss:0.014, val_acc:0.982]
Epoch [103/120    avg_loss:0.014, val_acc:0.985]
Epoch [104/120    avg_loss:0.013, val_acc:0.985]
Epoch [105/120    avg_loss:0.011, val_acc:0.988]
Epoch [106/120    avg_loss:0.012, val_acc:0.988]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.011, val_acc:0.981]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.009, val_acc:0.984]
Epoch [111/120    avg_loss:0.010, val_acc:0.984]
Epoch [112/120    avg_loss:0.008, val_acc:0.980]
Epoch [113/120    avg_loss:0.010, val_acc:0.983]
Epoch [114/120    avg_loss:0.010, val_acc:0.984]
Epoch [115/120    avg_loss:0.007, val_acc:0.983]
Epoch [116/120    avg_loss:0.009, val_acc:0.982]
Epoch [117/120    avg_loss:0.014, val_acc:0.981]
Epoch [118/120    avg_loss:0.014, val_acc:0.985]
Epoch [119/120    avg_loss:0.015, val_acc:0.982]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1241    3    2    0    0    0    0    0    4   34    1    0
     0    0    0]
 [   0    0    0  727    1    0    0    0    0    0    2   13    3    0
     0    1    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    1    0    0  430    3    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0  843   23    6    0
     0    1    0]
 [   0    0    5    0    0    0    0    0    0    0    8 2174   23    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    5    5  523    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1133    6    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    93  249    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.19241192411924

F1 scores:
[       nan 1.         0.97947908 0.98376184 0.99065421 0.99421965
 0.99318698 1.         1.         1.         0.97063903 0.97488789
 0.95612431 0.99728997 0.95732995 0.82450331 0.96969697]

Kappa:
0.9679457488230071
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3e6534a780>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.638, val_acc:0.388]
Epoch [2/120    avg_loss:2.267, val_acc:0.494]
Epoch [3/120    avg_loss:2.051, val_acc:0.570]
Epoch [4/120    avg_loss:1.888, val_acc:0.597]
Epoch [5/120    avg_loss:1.745, val_acc:0.612]
Epoch [6/120    avg_loss:1.605, val_acc:0.648]
Epoch [7/120    avg_loss:1.447, val_acc:0.679]
Epoch [8/120    avg_loss:1.302, val_acc:0.699]
Epoch [9/120    avg_loss:1.166, val_acc:0.699]
Epoch [10/120    avg_loss:1.038, val_acc:0.756]
Epoch [11/120    avg_loss:0.888, val_acc:0.793]
Epoch [12/120    avg_loss:0.867, val_acc:0.760]
Epoch [13/120    avg_loss:0.773, val_acc:0.767]
Epoch [14/120    avg_loss:0.667, val_acc:0.788]
Epoch [15/120    avg_loss:0.597, val_acc:0.805]
Epoch [16/120    avg_loss:0.537, val_acc:0.792]
Epoch [17/120    avg_loss:0.493, val_acc:0.837]
Epoch [18/120    avg_loss:0.472, val_acc:0.790]
Epoch [19/120    avg_loss:0.491, val_acc:0.802]
Epoch [20/120    avg_loss:0.435, val_acc:0.856]
Epoch [21/120    avg_loss:0.408, val_acc:0.851]
Epoch [22/120    avg_loss:0.348, val_acc:0.790]
Epoch [23/120    avg_loss:0.414, val_acc:0.857]
Epoch [24/120    avg_loss:0.326, val_acc:0.899]
Epoch [25/120    avg_loss:0.236, val_acc:0.905]
Epoch [26/120    avg_loss:0.250, val_acc:0.846]
Epoch [27/120    avg_loss:0.335, val_acc:0.873]
Epoch [28/120    avg_loss:0.276, val_acc:0.914]
Epoch [29/120    avg_loss:0.203, val_acc:0.884]
Epoch [30/120    avg_loss:0.186, val_acc:0.906]
Epoch [31/120    avg_loss:0.181, val_acc:0.891]
Epoch [32/120    avg_loss:0.161, val_acc:0.911]
Epoch [33/120    avg_loss:0.145, val_acc:0.930]
Epoch [34/120    avg_loss:0.131, val_acc:0.940]
Epoch [35/120    avg_loss:0.117, val_acc:0.932]
Epoch [36/120    avg_loss:0.100, val_acc:0.945]
Epoch [37/120    avg_loss:0.118, val_acc:0.923]
Epoch [38/120    avg_loss:0.100, val_acc:0.932]
Epoch [39/120    avg_loss:0.112, val_acc:0.942]
Epoch [40/120    avg_loss:0.127, val_acc:0.944]
Epoch [41/120    avg_loss:0.090, val_acc:0.954]
Epoch [42/120    avg_loss:0.098, val_acc:0.933]
Epoch [43/120    avg_loss:0.100, val_acc:0.954]
Epoch [44/120    avg_loss:0.085, val_acc:0.956]
Epoch [45/120    avg_loss:0.078, val_acc:0.959]
Epoch [46/120    avg_loss:0.053, val_acc:0.967]
Epoch [47/120    avg_loss:0.063, val_acc:0.962]
Epoch [48/120    avg_loss:0.065, val_acc:0.938]
Epoch [49/120    avg_loss:0.067, val_acc:0.969]
Epoch [50/120    avg_loss:0.053, val_acc:0.959]
Epoch [51/120    avg_loss:0.079, val_acc:0.958]
Epoch [52/120    avg_loss:0.055, val_acc:0.955]
Epoch [53/120    avg_loss:0.052, val_acc:0.962]
Epoch [54/120    avg_loss:0.039, val_acc:0.953]
Epoch [55/120    avg_loss:0.053, val_acc:0.968]
Epoch [56/120    avg_loss:0.040, val_acc:0.952]
Epoch [57/120    avg_loss:0.048, val_acc:0.957]
Epoch [58/120    avg_loss:0.056, val_acc:0.929]
Epoch [59/120    avg_loss:0.064, val_acc:0.957]
Epoch [60/120    avg_loss:0.048, val_acc:0.953]
Epoch [61/120    avg_loss:0.041, val_acc:0.967]
Epoch [62/120    avg_loss:0.037, val_acc:0.966]
Epoch [63/120    avg_loss:0.028, val_acc:0.970]
Epoch [64/120    avg_loss:0.026, val_acc:0.976]
Epoch [65/120    avg_loss:0.034, val_acc:0.975]
Epoch [66/120    avg_loss:0.024, val_acc:0.977]
Epoch [67/120    avg_loss:0.023, val_acc:0.978]
Epoch [68/120    avg_loss:0.024, val_acc:0.981]
Epoch [69/120    avg_loss:0.019, val_acc:0.981]
Epoch [70/120    avg_loss:0.029, val_acc:0.978]
Epoch [71/120    avg_loss:0.021, val_acc:0.981]
Epoch [72/120    avg_loss:0.020, val_acc:0.980]
Epoch [73/120    avg_loss:0.022, val_acc:0.978]
Epoch [74/120    avg_loss:0.023, val_acc:0.980]
Epoch [75/120    avg_loss:0.023, val_acc:0.981]
Epoch [76/120    avg_loss:0.019, val_acc:0.980]
Epoch [77/120    avg_loss:0.021, val_acc:0.980]
Epoch [78/120    avg_loss:0.025, val_acc:0.973]
Epoch [79/120    avg_loss:0.026, val_acc:0.977]
Epoch [80/120    avg_loss:0.019, val_acc:0.980]
Epoch [81/120    avg_loss:0.020, val_acc:0.981]
Epoch [82/120    avg_loss:0.021, val_acc:0.978]
Epoch [83/120    avg_loss:0.016, val_acc:0.977]
Epoch [84/120    avg_loss:0.020, val_acc:0.976]
Epoch [85/120    avg_loss:0.022, val_acc:0.980]
Epoch [86/120    avg_loss:0.019, val_acc:0.978]
Epoch [87/120    avg_loss:0.019, val_acc:0.976]
Epoch [88/120    avg_loss:0.018, val_acc:0.977]
Epoch [89/120    avg_loss:0.019, val_acc:0.982]
Epoch [90/120    avg_loss:0.017, val_acc:0.982]
Epoch [91/120    avg_loss:0.017, val_acc:0.978]
Epoch [92/120    avg_loss:0.017, val_acc:0.976]
Epoch [93/120    avg_loss:0.013, val_acc:0.978]
Epoch [94/120    avg_loss:0.018, val_acc:0.976]
Epoch [95/120    avg_loss:0.018, val_acc:0.978]
Epoch [96/120    avg_loss:0.014, val_acc:0.977]
Epoch [97/120    avg_loss:0.016, val_acc:0.980]
Epoch [98/120    avg_loss:0.015, val_acc:0.981]
Epoch [99/120    avg_loss:0.017, val_acc:0.978]
Epoch [100/120    avg_loss:0.023, val_acc:0.981]
Epoch [101/120    avg_loss:0.017, val_acc:0.983]
Epoch [102/120    avg_loss:0.016, val_acc:0.982]
Epoch [103/120    avg_loss:0.018, val_acc:0.978]
Epoch [104/120    avg_loss:0.016, val_acc:0.981]
Epoch [105/120    avg_loss:0.013, val_acc:0.980]
Epoch [106/120    avg_loss:0.017, val_acc:0.977]
Epoch [107/120    avg_loss:0.014, val_acc:0.977]
Epoch [108/120    avg_loss:0.014, val_acc:0.977]
Epoch [109/120    avg_loss:0.021, val_acc:0.976]
Epoch [110/120    avg_loss:0.017, val_acc:0.982]
Epoch [111/120    avg_loss:0.019, val_acc:0.981]
Epoch [112/120    avg_loss:0.014, val_acc:0.980]
Epoch [113/120    avg_loss:0.017, val_acc:0.980]
Epoch [114/120    avg_loss:0.015, val_acc:0.981]
Epoch [115/120    avg_loss:0.017, val_acc:0.981]
Epoch [116/120    avg_loss:0.015, val_acc:0.981]
Epoch [117/120    avg_loss:0.015, val_acc:0.981]
Epoch [118/120    avg_loss:0.015, val_acc:0.981]
Epoch [119/120    avg_loss:0.015, val_acc:0.981]
Epoch [120/120    avg_loss:0.015, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    4    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1249    1    1    0    0    0    0    1    5   28    0    0
     0    0    0]
 [   0    0    0  710    8    0    2    0    0    9    3    4    6    0
     0    5    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    5    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    3  852   12    0    0
     0    4    0]
 [   0    0    3    1    0    0    1    0    0    5   16 2173   10    1
     0    0    0]
 [   0    0    0    5    0    0    0    0    0    0    3    3  517    0
     2    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1123   15    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    26  317    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.72357723577235

F1 scores:
[       nan 0.94871795 0.98153242 0.96994536 0.97931034 0.99188876
 0.99470098 0.90909091 0.99883586 0.64285714 0.97094017 0.98103837
 0.96635514 0.99730458 0.98078603 0.92017417 0.9704142 ]

Kappa:
0.9740506864402155
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7368684898>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.686, val_acc:0.286]
Epoch [2/120    avg_loss:2.339, val_acc:0.500]
Epoch [3/120    avg_loss:2.147, val_acc:0.485]
Epoch [4/120    avg_loss:1.966, val_acc:0.545]
Epoch [5/120    avg_loss:1.819, val_acc:0.526]
Epoch [6/120    avg_loss:1.656, val_acc:0.575]
Epoch [7/120    avg_loss:1.523, val_acc:0.633]
Epoch [8/120    avg_loss:1.407, val_acc:0.654]
Epoch [9/120    avg_loss:1.327, val_acc:0.629]
Epoch [10/120    avg_loss:1.175, val_acc:0.727]
Epoch [11/120    avg_loss:1.011, val_acc:0.730]
Epoch [12/120    avg_loss:0.907, val_acc:0.741]
Epoch [13/120    avg_loss:0.929, val_acc:0.767]
Epoch [14/120    avg_loss:0.753, val_acc:0.807]
Epoch [15/120    avg_loss:0.647, val_acc:0.793]
Epoch [16/120    avg_loss:0.628, val_acc:0.801]
Epoch [17/120    avg_loss:0.606, val_acc:0.819]
Epoch [18/120    avg_loss:0.498, val_acc:0.835]
Epoch [19/120    avg_loss:0.524, val_acc:0.820]
Epoch [20/120    avg_loss:0.461, val_acc:0.850]
Epoch [21/120    avg_loss:0.410, val_acc:0.852]
Epoch [22/120    avg_loss:0.411, val_acc:0.844]
Epoch [23/120    avg_loss:0.351, val_acc:0.864]
Epoch [24/120    avg_loss:0.315, val_acc:0.877]
Epoch [25/120    avg_loss:0.283, val_acc:0.889]
Epoch [26/120    avg_loss:0.220, val_acc:0.895]
Epoch [27/120    avg_loss:0.197, val_acc:0.886]
Epoch [28/120    avg_loss:0.221, val_acc:0.899]
Epoch [29/120    avg_loss:0.275, val_acc:0.884]
Epoch [30/120    avg_loss:0.214, val_acc:0.906]
Epoch [31/120    avg_loss:0.195, val_acc:0.912]
Epoch [32/120    avg_loss:0.181, val_acc:0.884]
Epoch [33/120    avg_loss:0.201, val_acc:0.899]
Epoch [34/120    avg_loss:0.161, val_acc:0.884]
Epoch [35/120    avg_loss:0.168, val_acc:0.911]
Epoch [36/120    avg_loss:0.135, val_acc:0.925]
Epoch [37/120    avg_loss:0.136, val_acc:0.930]
Epoch [38/120    avg_loss:0.111, val_acc:0.926]
Epoch [39/120    avg_loss:0.109, val_acc:0.925]
Epoch [40/120    avg_loss:0.117, val_acc:0.934]
Epoch [41/120    avg_loss:0.124, val_acc:0.921]
Epoch [42/120    avg_loss:0.141, val_acc:0.942]
Epoch [43/120    avg_loss:0.112, val_acc:0.929]
Epoch [44/120    avg_loss:0.112, val_acc:0.945]
Epoch [45/120    avg_loss:0.100, val_acc:0.931]
Epoch [46/120    avg_loss:0.074, val_acc:0.955]
Epoch [47/120    avg_loss:0.087, val_acc:0.954]
Epoch [48/120    avg_loss:0.090, val_acc:0.962]
Epoch [49/120    avg_loss:0.071, val_acc:0.949]
Epoch [50/120    avg_loss:0.105, val_acc:0.928]
Epoch [51/120    avg_loss:0.092, val_acc:0.926]
Epoch [52/120    avg_loss:0.095, val_acc:0.941]
Epoch [53/120    avg_loss:0.071, val_acc:0.933]
Epoch [54/120    avg_loss:0.080, val_acc:0.934]
Epoch [55/120    avg_loss:0.064, val_acc:0.953]
Epoch [56/120    avg_loss:0.077, val_acc:0.950]
Epoch [57/120    avg_loss:0.071, val_acc:0.953]
Epoch [58/120    avg_loss:0.050, val_acc:0.966]
Epoch [59/120    avg_loss:0.042, val_acc:0.960]
Epoch [60/120    avg_loss:0.054, val_acc:0.938]
Epoch [61/120    avg_loss:0.054, val_acc:0.948]
Epoch [62/120    avg_loss:0.059, val_acc:0.948]
Epoch [63/120    avg_loss:0.056, val_acc:0.946]
Epoch [64/120    avg_loss:0.037, val_acc:0.959]
Epoch [65/120    avg_loss:0.043, val_acc:0.966]
Epoch [66/120    avg_loss:0.029, val_acc:0.964]
Epoch [67/120    avg_loss:0.033, val_acc:0.967]
Epoch [68/120    avg_loss:0.038, val_acc:0.975]
Epoch [69/120    avg_loss:0.036, val_acc:0.966]
Epoch [70/120    avg_loss:0.042, val_acc:0.973]
Epoch [71/120    avg_loss:0.031, val_acc:0.972]
Epoch [72/120    avg_loss:0.030, val_acc:0.975]
Epoch [73/120    avg_loss:0.029, val_acc:0.974]
Epoch [74/120    avg_loss:0.025, val_acc:0.964]
Epoch [75/120    avg_loss:0.032, val_acc:0.974]
Epoch [76/120    avg_loss:0.031, val_acc:0.971]
Epoch [77/120    avg_loss:0.023, val_acc:0.964]
Epoch [78/120    avg_loss:0.058, val_acc:0.955]
Epoch [79/120    avg_loss:0.043, val_acc:0.966]
Epoch [80/120    avg_loss:0.032, val_acc:0.960]
Epoch [81/120    avg_loss:0.026, val_acc:0.970]
Epoch [82/120    avg_loss:0.025, val_acc:0.972]
Epoch [83/120    avg_loss:0.016, val_acc:0.972]
Epoch [84/120    avg_loss:0.013, val_acc:0.976]
Epoch [85/120    avg_loss:0.016, val_acc:0.972]
Epoch [86/120    avg_loss:0.040, val_acc:0.961]
Epoch [87/120    avg_loss:0.030, val_acc:0.977]
Epoch [88/120    avg_loss:0.023, val_acc:0.971]
Epoch [89/120    avg_loss:0.017, val_acc:0.978]
Epoch [90/120    avg_loss:0.014, val_acc:0.966]
Epoch [91/120    avg_loss:0.021, val_acc:0.956]
Epoch [92/120    avg_loss:0.022, val_acc:0.972]
Epoch [93/120    avg_loss:0.016, val_acc:0.977]
Epoch [94/120    avg_loss:0.016, val_acc:0.975]
Epoch [95/120    avg_loss:0.022, val_acc:0.968]
Epoch [96/120    avg_loss:0.012, val_acc:0.978]
Epoch [97/120    avg_loss:0.011, val_acc:0.984]
Epoch [98/120    avg_loss:0.023, val_acc:0.974]
Epoch [99/120    avg_loss:0.033, val_acc:0.980]
Epoch [100/120    avg_loss:0.021, val_acc:0.985]
Epoch [101/120    avg_loss:0.023, val_acc:0.967]
Epoch [102/120    avg_loss:0.022, val_acc:0.971]
Epoch [103/120    avg_loss:0.026, val_acc:0.974]
Epoch [104/120    avg_loss:0.019, val_acc:0.975]
Epoch [105/120    avg_loss:0.026, val_acc:0.974]
Epoch [106/120    avg_loss:0.027, val_acc:0.969]
Epoch [107/120    avg_loss:0.020, val_acc:0.975]
Epoch [108/120    avg_loss:0.027, val_acc:0.973]
Epoch [109/120    avg_loss:0.027, val_acc:0.967]
Epoch [110/120    avg_loss:0.015, val_acc:0.972]
Epoch [111/120    avg_loss:0.013, val_acc:0.983]
Epoch [112/120    avg_loss:0.011, val_acc:0.983]
Epoch [113/120    avg_loss:0.015, val_acc:0.977]
Epoch [114/120    avg_loss:0.011, val_acc:0.982]
Epoch [115/120    avg_loss:0.015, val_acc:0.987]
Epoch [116/120    avg_loss:0.009, val_acc:0.990]
Epoch [117/120    avg_loss:0.008, val_acc:0.990]
Epoch [118/120    avg_loss:0.009, val_acc:0.990]
Epoch [119/120    avg_loss:0.008, val_acc:0.990]
Epoch [120/120    avg_loss:0.007, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1255    2    0    0    0    0    0    1    0   27    0    0
     0    0    0]
 [   0    0    0  735    0    0    0    0    0    2    0    4    3    3
     0    0    0]
 [   0    0    0    1  210    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    2    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    1    0    0    0  840   23    4    0
     0    3    0]
 [   0    0   10    0    0    0    0    0    0    0   11 2182    6    1
     0    0    0]
 [   0    0    0    8    4    0    0    0    0    0    5    0  516    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    1    0    0    3
  1128    5    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
     9  332    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.2439024390244

F1 scores:
[       nan 0.98795181 0.98277212 0.98459478 0.98360656 0.99421965
 0.99242424 1.         0.99651568 0.85       0.96997691 0.98111511
 0.96810507 0.98143236 0.98990785 0.96652111 0.98809524]

Kappa:
0.979971841168559
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f453b8267b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.702, val_acc:0.338]
Epoch [2/120    avg_loss:2.272, val_acc:0.500]
Epoch [3/120    avg_loss:2.063, val_acc:0.570]
Epoch [4/120    avg_loss:1.883, val_acc:0.581]
Epoch [5/120    avg_loss:1.710, val_acc:0.603]
Epoch [6/120    avg_loss:1.578, val_acc:0.611]
Epoch [7/120    avg_loss:1.429, val_acc:0.621]
Epoch [8/120    avg_loss:1.328, val_acc:0.642]
Epoch [9/120    avg_loss:1.251, val_acc:0.674]
Epoch [10/120    avg_loss:1.142, val_acc:0.701]
Epoch [11/120    avg_loss:1.045, val_acc:0.722]
Epoch [12/120    avg_loss:0.965, val_acc:0.735]
Epoch [13/120    avg_loss:0.869, val_acc:0.759]
Epoch [14/120    avg_loss:0.804, val_acc:0.805]
Epoch [15/120    avg_loss:0.721, val_acc:0.753]
Epoch [16/120    avg_loss:0.686, val_acc:0.823]
Epoch [17/120    avg_loss:0.581, val_acc:0.818]
Epoch [18/120    avg_loss:0.506, val_acc:0.841]
Epoch [19/120    avg_loss:0.479, val_acc:0.852]
Epoch [20/120    avg_loss:0.526, val_acc:0.824]
Epoch [21/120    avg_loss:0.550, val_acc:0.840]
Epoch [22/120    avg_loss:0.428, val_acc:0.887]
Epoch [23/120    avg_loss:0.430, val_acc:0.822]
Epoch [24/120    avg_loss:0.376, val_acc:0.849]
Epoch [25/120    avg_loss:0.307, val_acc:0.889]
Epoch [26/120    avg_loss:0.257, val_acc:0.917]
Epoch [27/120    avg_loss:0.278, val_acc:0.903]
Epoch [28/120    avg_loss:0.209, val_acc:0.917]
Epoch [29/120    avg_loss:0.217, val_acc:0.920]
Epoch [30/120    avg_loss:0.181, val_acc:0.923]
Epoch [31/120    avg_loss:0.181, val_acc:0.923]
Epoch [32/120    avg_loss:0.168, val_acc:0.932]
Epoch [33/120    avg_loss:0.153, val_acc:0.932]
Epoch [34/120    avg_loss:0.139, val_acc:0.938]
Epoch [35/120    avg_loss:0.133, val_acc:0.938]
Epoch [36/120    avg_loss:0.138, val_acc:0.944]
Epoch [37/120    avg_loss:0.131, val_acc:0.944]
Epoch [38/120    avg_loss:0.146, val_acc:0.943]
Epoch [39/120    avg_loss:0.104, val_acc:0.949]
Epoch [40/120    avg_loss:0.119, val_acc:0.939]
Epoch [41/120    avg_loss:0.174, val_acc:0.928]
Epoch [42/120    avg_loss:0.105, val_acc:0.953]
Epoch [43/120    avg_loss:0.136, val_acc:0.938]
Epoch [44/120    avg_loss:0.125, val_acc:0.940]
Epoch [45/120    avg_loss:0.124, val_acc:0.957]
Epoch [46/120    avg_loss:0.088, val_acc:0.958]
Epoch [47/120    avg_loss:0.104, val_acc:0.956]
Epoch [48/120    avg_loss:0.098, val_acc:0.954]
Epoch [49/120    avg_loss:0.074, val_acc:0.963]
Epoch [50/120    avg_loss:0.073, val_acc:0.951]
Epoch [51/120    avg_loss:0.080, val_acc:0.954]
Epoch [52/120    avg_loss:0.107, val_acc:0.954]
Epoch [53/120    avg_loss:0.127, val_acc:0.945]
Epoch [54/120    avg_loss:0.075, val_acc:0.958]
Epoch [55/120    avg_loss:0.060, val_acc:0.960]
Epoch [56/120    avg_loss:0.054, val_acc:0.960]
Epoch [57/120    avg_loss:0.054, val_acc:0.968]
Epoch [58/120    avg_loss:0.042, val_acc:0.976]
Epoch [59/120    avg_loss:0.047, val_acc:0.969]
Epoch [60/120    avg_loss:0.054, val_acc:0.961]
Epoch [61/120    avg_loss:0.071, val_acc:0.968]
Epoch [62/120    avg_loss:0.044, val_acc:0.960]
Epoch [63/120    avg_loss:0.058, val_acc:0.970]
Epoch [64/120    avg_loss:0.039, val_acc:0.970]
Epoch [65/120    avg_loss:0.055, val_acc:0.968]
Epoch [66/120    avg_loss:0.044, val_acc:0.975]
Epoch [67/120    avg_loss:0.037, val_acc:0.971]
Epoch [68/120    avg_loss:0.037, val_acc:0.966]
Epoch [69/120    avg_loss:0.028, val_acc:0.982]
Epoch [70/120    avg_loss:0.032, val_acc:0.970]
Epoch [71/120    avg_loss:0.029, val_acc:0.973]
Epoch [72/120    avg_loss:0.024, val_acc:0.976]
Epoch [73/120    avg_loss:0.023, val_acc:0.977]
Epoch [74/120    avg_loss:0.023, val_acc:0.974]
Epoch [75/120    avg_loss:0.024, val_acc:0.976]
Epoch [76/120    avg_loss:0.029, val_acc:0.975]
Epoch [77/120    avg_loss:0.024, val_acc:0.979]
Epoch [78/120    avg_loss:0.018, val_acc:0.977]
Epoch [79/120    avg_loss:0.023, val_acc:0.981]
Epoch [80/120    avg_loss:0.022, val_acc:0.977]
Epoch [81/120    avg_loss:0.015, val_acc:0.984]
Epoch [82/120    avg_loss:0.019, val_acc:0.977]
Epoch [83/120    avg_loss:0.017, val_acc:0.981]
Epoch [84/120    avg_loss:0.020, val_acc:0.978]
Epoch [85/120    avg_loss:0.024, val_acc:0.982]
Epoch [86/120    avg_loss:0.046, val_acc:0.963]
Epoch [87/120    avg_loss:0.034, val_acc:0.976]
Epoch [88/120    avg_loss:0.031, val_acc:0.966]
Epoch [89/120    avg_loss:0.029, val_acc:0.976]
Epoch [90/120    avg_loss:0.019, val_acc:0.968]
Epoch [91/120    avg_loss:0.016, val_acc:0.980]
Epoch [92/120    avg_loss:0.011, val_acc:0.980]
Epoch [93/120    avg_loss:0.020, val_acc:0.976]
Epoch [94/120    avg_loss:0.021, val_acc:0.977]
Epoch [95/120    avg_loss:0.025, val_acc:0.983]
Epoch [96/120    avg_loss:0.012, val_acc:0.983]
Epoch [97/120    avg_loss:0.010, val_acc:0.982]
Epoch [98/120    avg_loss:0.012, val_acc:0.982]
Epoch [99/120    avg_loss:0.013, val_acc:0.981]
Epoch [100/120    avg_loss:0.010, val_acc:0.981]
Epoch [101/120    avg_loss:0.009, val_acc:0.981]
Epoch [102/120    avg_loss:0.012, val_acc:0.981]
Epoch [103/120    avg_loss:0.012, val_acc:0.981]
Epoch [104/120    avg_loss:0.010, val_acc:0.981]
Epoch [105/120    avg_loss:0.009, val_acc:0.981]
Epoch [106/120    avg_loss:0.010, val_acc:0.983]
Epoch [107/120    avg_loss:0.011, val_acc:0.982]
Epoch [108/120    avg_loss:0.012, val_acc:0.982]
Epoch [109/120    avg_loss:0.011, val_acc:0.983]
Epoch [110/120    avg_loss:0.010, val_acc:0.983]
Epoch [111/120    avg_loss:0.011, val_acc:0.982]
Epoch [112/120    avg_loss:0.011, val_acc:0.982]
Epoch [113/120    avg_loss:0.010, val_acc:0.982]
Epoch [114/120    avg_loss:0.012, val_acc:0.982]
Epoch [115/120    avg_loss:0.009, val_acc:0.983]
Epoch [116/120    avg_loss:0.009, val_acc:0.983]
Epoch [117/120    avg_loss:0.014, val_acc:0.982]
Epoch [118/120    avg_loss:0.009, val_acc:0.982]
Epoch [119/120    avg_loss:0.011, val_acc:0.982]
Epoch [120/120    avg_loss:0.009, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1249    6    1    0    1    0    0    3    6   19    0    0
     0    0    0]
 [   0    0    0  709    3    9    0    0    0    5    2    4   12    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0  844   23    0    0
     4    3    0]
 [   0    0    3    0    0    0    3    0    0    1   13 2176   13    1
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    0    2  526    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1119   20    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    19  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.77777777777777

F1 scores:
[       nan 0.95348837 0.98423956 0.96791809 0.99069767 0.98630137
 0.98945783 1.         0.9953271  0.73913043 0.97011494 0.98128523
 0.96869245 0.98930481 0.98028909 0.9244186  0.98224852]

Kappa:
0.9746690775931711
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5bdb594828>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.664, val_acc:0.415]
Epoch [2/120    avg_loss:2.307, val_acc:0.509]
Epoch [3/120    avg_loss:2.114, val_acc:0.450]
Epoch [4/120    avg_loss:1.886, val_acc:0.574]
Epoch [5/120    avg_loss:1.730, val_acc:0.623]
Epoch [6/120    avg_loss:1.549, val_acc:0.629]
Epoch [7/120    avg_loss:1.412, val_acc:0.643]
Epoch [8/120    avg_loss:1.228, val_acc:0.689]
Epoch [9/120    avg_loss:1.086, val_acc:0.735]
Epoch [10/120    avg_loss:1.015, val_acc:0.749]
Epoch [11/120    avg_loss:0.877, val_acc:0.800]
Epoch [12/120    avg_loss:0.797, val_acc:0.781]
Epoch [13/120    avg_loss:0.703, val_acc:0.770]
Epoch [14/120    avg_loss:0.638, val_acc:0.817]
Epoch [15/120    avg_loss:0.554, val_acc:0.797]
Epoch [16/120    avg_loss:0.504, val_acc:0.832]
Epoch [17/120    avg_loss:0.396, val_acc:0.865]
Epoch [18/120    avg_loss:0.329, val_acc:0.865]
Epoch [19/120    avg_loss:0.339, val_acc:0.866]
Epoch [20/120    avg_loss:0.354, val_acc:0.874]
Epoch [21/120    avg_loss:0.340, val_acc:0.860]
Epoch [22/120    avg_loss:0.417, val_acc:0.860]
Epoch [23/120    avg_loss:0.303, val_acc:0.863]
Epoch [24/120    avg_loss:0.298, val_acc:0.873]
Epoch [25/120    avg_loss:0.193, val_acc:0.908]
Epoch [26/120    avg_loss:0.184, val_acc:0.912]
Epoch [27/120    avg_loss:0.180, val_acc:0.878]
Epoch [28/120    avg_loss:0.169, val_acc:0.909]
Epoch [29/120    avg_loss:0.146, val_acc:0.925]
Epoch [30/120    avg_loss:0.139, val_acc:0.916]
Epoch [31/120    avg_loss:0.205, val_acc:0.919]
Epoch [32/120    avg_loss:0.155, val_acc:0.932]
Epoch [33/120    avg_loss:0.125, val_acc:0.929]
Epoch [34/120    avg_loss:0.130, val_acc:0.923]
Epoch [35/120    avg_loss:0.105, val_acc:0.914]
Epoch [36/120    avg_loss:0.095, val_acc:0.932]
Epoch [37/120    avg_loss:0.115, val_acc:0.938]
Epoch [38/120    avg_loss:0.098, val_acc:0.919]
Epoch [39/120    avg_loss:0.100, val_acc:0.941]
Epoch [40/120    avg_loss:0.088, val_acc:0.917]
Epoch [41/120    avg_loss:0.087, val_acc:0.943]
Epoch [42/120    avg_loss:0.091, val_acc:0.926]
Epoch [43/120    avg_loss:0.101, val_acc:0.944]
Epoch [44/120    avg_loss:0.097, val_acc:0.948]
Epoch [45/120    avg_loss:0.082, val_acc:0.940]
Epoch [46/120    avg_loss:0.076, val_acc:0.948]
Epoch [47/120    avg_loss:0.057, val_acc:0.954]
Epoch [48/120    avg_loss:0.062, val_acc:0.964]
Epoch [49/120    avg_loss:0.047, val_acc:0.960]
Epoch [50/120    avg_loss:0.066, val_acc:0.946]
Epoch [51/120    avg_loss:0.058, val_acc:0.950]
Epoch [52/120    avg_loss:0.052, val_acc:0.956]
Epoch [53/120    avg_loss:0.043, val_acc:0.966]
Epoch [54/120    avg_loss:0.065, val_acc:0.955]
Epoch [55/120    avg_loss:0.041, val_acc:0.966]
Epoch [56/120    avg_loss:0.036, val_acc:0.944]
Epoch [57/120    avg_loss:0.045, val_acc:0.946]
Epoch [58/120    avg_loss:0.050, val_acc:0.944]
Epoch [59/120    avg_loss:0.044, val_acc:0.966]
Epoch [60/120    avg_loss:0.035, val_acc:0.966]
Epoch [61/120    avg_loss:0.035, val_acc:0.963]
Epoch [62/120    avg_loss:0.043, val_acc:0.957]
Epoch [63/120    avg_loss:0.047, val_acc:0.966]
Epoch [64/120    avg_loss:0.031, val_acc:0.968]
Epoch [65/120    avg_loss:0.026, val_acc:0.967]
Epoch [66/120    avg_loss:0.031, val_acc:0.967]
Epoch [67/120    avg_loss:0.032, val_acc:0.969]
Epoch [68/120    avg_loss:0.044, val_acc:0.959]
Epoch [69/120    avg_loss:0.040, val_acc:0.960]
Epoch [70/120    avg_loss:0.042, val_acc:0.963]
Epoch [71/120    avg_loss:0.050, val_acc:0.969]
Epoch [72/120    avg_loss:0.030, val_acc:0.969]
Epoch [73/120    avg_loss:0.025, val_acc:0.977]
Epoch [74/120    avg_loss:0.031, val_acc:0.962]
Epoch [75/120    avg_loss:0.031, val_acc:0.967]
Epoch [76/120    avg_loss:0.030, val_acc:0.971]
Epoch [77/120    avg_loss:0.032, val_acc:0.974]
Epoch [78/120    avg_loss:0.041, val_acc:0.962]
Epoch [79/120    avg_loss:0.024, val_acc:0.974]
Epoch [80/120    avg_loss:0.022, val_acc:0.972]
Epoch [81/120    avg_loss:0.026, val_acc:0.968]
Epoch [82/120    avg_loss:0.016, val_acc:0.972]
Epoch [83/120    avg_loss:0.017, val_acc:0.976]
Epoch [84/120    avg_loss:0.020, val_acc:0.973]
Epoch [85/120    avg_loss:0.013, val_acc:0.974]
Epoch [86/120    avg_loss:0.011, val_acc:0.975]
Epoch [87/120    avg_loss:0.013, val_acc:0.976]
Epoch [88/120    avg_loss:0.009, val_acc:0.976]
Epoch [89/120    avg_loss:0.011, val_acc:0.978]
Epoch [90/120    avg_loss:0.011, val_acc:0.978]
Epoch [91/120    avg_loss:0.010, val_acc:0.978]
Epoch [92/120    avg_loss:0.010, val_acc:0.980]
Epoch [93/120    avg_loss:0.010, val_acc:0.980]
Epoch [94/120    avg_loss:0.010, val_acc:0.981]
Epoch [95/120    avg_loss:0.010, val_acc:0.981]
Epoch [96/120    avg_loss:0.009, val_acc:0.980]
Epoch [97/120    avg_loss:0.009, val_acc:0.978]
Epoch [98/120    avg_loss:0.008, val_acc:0.977]
Epoch [99/120    avg_loss:0.011, val_acc:0.978]
Epoch [100/120    avg_loss:0.009, val_acc:0.980]
Epoch [101/120    avg_loss:0.008, val_acc:0.980]
Epoch [102/120    avg_loss:0.013, val_acc:0.980]
Epoch [103/120    avg_loss:0.009, val_acc:0.978]
Epoch [104/120    avg_loss:0.008, val_acc:0.978]
Epoch [105/120    avg_loss:0.015, val_acc:0.978]
Epoch [106/120    avg_loss:0.010, val_acc:0.981]
Epoch [107/120    avg_loss:0.007, val_acc:0.980]
Epoch [108/120    avg_loss:0.009, val_acc:0.978]
Epoch [109/120    avg_loss:0.013, val_acc:0.981]
Epoch [110/120    avg_loss:0.013, val_acc:0.982]
Epoch [111/120    avg_loss:0.009, val_acc:0.982]
Epoch [112/120    avg_loss:0.008, val_acc:0.978]
Epoch [113/120    avg_loss:0.009, val_acc:0.980]
Epoch [114/120    avg_loss:0.011, val_acc:0.980]
Epoch [115/120    avg_loss:0.008, val_acc:0.981]
Epoch [116/120    avg_loss:0.008, val_acc:0.980]
Epoch [117/120    avg_loss:0.009, val_acc:0.981]
Epoch [118/120    avg_loss:0.008, val_acc:0.980]
Epoch [119/120    avg_loss:0.008, val_acc:0.978]
Epoch [120/120    avg_loss:0.008, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1262    2    2    1    1    0    0    0    4   13    0    0
     0    0    0]
 [   0    0    0  727    2    0    0    0    0    6    1    0    8    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    1    3    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0  857   15    0    0
     0    1    0]
 [   0    0    1    0    0    2    0    0    0    3   18 2180    6    0
     0    0    0]
 [   0    0    0    1    2    0    0    0    0    0   13    1  516    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    19  325    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.38482384823848

F1 scores:
[       nan 1.         0.98980392 0.98442789 0.98611111 0.99194476
 0.99392097 0.94339623 1.         0.8        0.96945701 0.98620222
 0.96719775 0.9919571  0.98777293 0.95447871 0.97590361]

Kappa:
0.9815877540014896
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f460a3c67f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.705, val_acc:0.240]
Epoch [2/120    avg_loss:2.334, val_acc:0.466]
Epoch [3/120    avg_loss:2.143, val_acc:0.489]
Epoch [4/120    avg_loss:1.976, val_acc:0.492]
Epoch [5/120    avg_loss:1.829, val_acc:0.556]
Epoch [6/120    avg_loss:1.678, val_acc:0.573]
Epoch [7/120    avg_loss:1.565, val_acc:0.599]
Epoch [8/120    avg_loss:1.439, val_acc:0.612]
Epoch [9/120    avg_loss:1.348, val_acc:0.647]
Epoch [10/120    avg_loss:1.241, val_acc:0.651]
Epoch [11/120    avg_loss:1.119, val_acc:0.707]
Epoch [12/120    avg_loss:0.959, val_acc:0.748]
Epoch [13/120    avg_loss:0.804, val_acc:0.763]
Epoch [14/120    avg_loss:0.708, val_acc:0.786]
Epoch [15/120    avg_loss:0.661, val_acc:0.777]
Epoch [16/120    avg_loss:0.591, val_acc:0.794]
Epoch [17/120    avg_loss:0.539, val_acc:0.841]
Epoch [18/120    avg_loss:0.447, val_acc:0.844]
Epoch [19/120    avg_loss:0.399, val_acc:0.819]
Epoch [20/120    avg_loss:0.390, val_acc:0.865]
Epoch [21/120    avg_loss:0.312, val_acc:0.866]
Epoch [22/120    avg_loss:0.321, val_acc:0.847]
Epoch [23/120    avg_loss:0.455, val_acc:0.819]
Epoch [24/120    avg_loss:0.337, val_acc:0.862]
Epoch [25/120    avg_loss:0.305, val_acc:0.864]
Epoch [26/120    avg_loss:0.282, val_acc:0.902]
Epoch [27/120    avg_loss:0.257, val_acc:0.891]
Epoch [28/120    avg_loss:0.259, val_acc:0.900]
Epoch [29/120    avg_loss:0.206, val_acc:0.909]
Epoch [30/120    avg_loss:0.198, val_acc:0.906]
Epoch [31/120    avg_loss:0.185, val_acc:0.897]
Epoch [32/120    avg_loss:0.197, val_acc:0.898]
Epoch [33/120    avg_loss:0.176, val_acc:0.904]
Epoch [34/120    avg_loss:0.193, val_acc:0.919]
Epoch [35/120    avg_loss:0.124, val_acc:0.929]
Epoch [36/120    avg_loss:0.100, val_acc:0.931]
Epoch [37/120    avg_loss:0.116, val_acc:0.934]
Epoch [38/120    avg_loss:0.094, val_acc:0.938]
Epoch [39/120    avg_loss:0.109, val_acc:0.926]
Epoch [40/120    avg_loss:0.094, val_acc:0.936]
Epoch [41/120    avg_loss:0.095, val_acc:0.950]
Epoch [42/120    avg_loss:0.100, val_acc:0.949]
Epoch [43/120    avg_loss:0.078, val_acc:0.948]
Epoch [44/120    avg_loss:0.069, val_acc:0.939]
Epoch [45/120    avg_loss:0.080, val_acc:0.931]
Epoch [46/120    avg_loss:0.100, val_acc:0.935]
Epoch [47/120    avg_loss:0.083, val_acc:0.932]
Epoch [48/120    avg_loss:0.067, val_acc:0.952]
Epoch [49/120    avg_loss:0.064, val_acc:0.949]
Epoch [50/120    avg_loss:0.062, val_acc:0.949]
Epoch [51/120    avg_loss:0.055, val_acc:0.957]
Epoch [52/120    avg_loss:0.048, val_acc:0.950]
Epoch [53/120    avg_loss:0.056, val_acc:0.944]
Epoch [54/120    avg_loss:0.053, val_acc:0.959]
Epoch [55/120    avg_loss:0.056, val_acc:0.957]
Epoch [56/120    avg_loss:0.067, val_acc:0.954]
Epoch [57/120    avg_loss:0.083, val_acc:0.956]
Epoch [58/120    avg_loss:0.059, val_acc:0.970]
Epoch [59/120    avg_loss:0.056, val_acc:0.958]
Epoch [60/120    avg_loss:0.059, val_acc:0.967]
Epoch [61/120    avg_loss:0.060, val_acc:0.960]
Epoch [62/120    avg_loss:0.073, val_acc:0.949]
Epoch [63/120    avg_loss:0.050, val_acc:0.954]
Epoch [64/120    avg_loss:0.038, val_acc:0.961]
Epoch [65/120    avg_loss:0.028, val_acc:0.967]
Epoch [66/120    avg_loss:0.031, val_acc:0.966]
Epoch [67/120    avg_loss:0.033, val_acc:0.967]
Epoch [68/120    avg_loss:0.032, val_acc:0.969]
Epoch [69/120    avg_loss:0.031, val_acc:0.968]
Epoch [70/120    avg_loss:0.029, val_acc:0.964]
Epoch [71/120    avg_loss:0.032, val_acc:0.967]
Epoch [72/120    avg_loss:0.026, val_acc:0.971]
Epoch [73/120    avg_loss:0.019, val_acc:0.973]
Epoch [74/120    avg_loss:0.018, val_acc:0.971]
Epoch [75/120    avg_loss:0.020, val_acc:0.972]
Epoch [76/120    avg_loss:0.020, val_acc:0.973]
Epoch [77/120    avg_loss:0.020, val_acc:0.971]
Epoch [78/120    avg_loss:0.019, val_acc:0.970]
Epoch [79/120    avg_loss:0.017, val_acc:0.974]
Epoch [80/120    avg_loss:0.015, val_acc:0.974]
Epoch [81/120    avg_loss:0.019, val_acc:0.974]
Epoch [82/120    avg_loss:0.016, val_acc:0.973]
Epoch [83/120    avg_loss:0.015, val_acc:0.973]
Epoch [84/120    avg_loss:0.017, val_acc:0.975]
Epoch [85/120    avg_loss:0.015, val_acc:0.975]
Epoch [86/120    avg_loss:0.016, val_acc:0.975]
Epoch [87/120    avg_loss:0.015, val_acc:0.975]
Epoch [88/120    avg_loss:0.016, val_acc:0.975]
Epoch [89/120    avg_loss:0.014, val_acc:0.976]
Epoch [90/120    avg_loss:0.015, val_acc:0.975]
Epoch [91/120    avg_loss:0.015, val_acc:0.975]
Epoch [92/120    avg_loss:0.017, val_acc:0.976]
Epoch [93/120    avg_loss:0.016, val_acc:0.973]
Epoch [94/120    avg_loss:0.013, val_acc:0.975]
Epoch [95/120    avg_loss:0.013, val_acc:0.976]
Epoch [96/120    avg_loss:0.017, val_acc:0.975]
Epoch [97/120    avg_loss:0.016, val_acc:0.977]
Epoch [98/120    avg_loss:0.014, val_acc:0.977]
Epoch [99/120    avg_loss:0.018, val_acc:0.974]
Epoch [100/120    avg_loss:0.016, val_acc:0.976]
Epoch [101/120    avg_loss:0.014, val_acc:0.974]
Epoch [102/120    avg_loss:0.017, val_acc:0.977]
Epoch [103/120    avg_loss:0.014, val_acc:0.977]
Epoch [104/120    avg_loss:0.017, val_acc:0.976]
Epoch [105/120    avg_loss:0.014, val_acc:0.976]
Epoch [106/120    avg_loss:0.017, val_acc:0.976]
Epoch [107/120    avg_loss:0.014, val_acc:0.977]
Epoch [108/120    avg_loss:0.015, val_acc:0.978]
Epoch [109/120    avg_loss:0.013, val_acc:0.977]
Epoch [110/120    avg_loss:0.015, val_acc:0.976]
Epoch [111/120    avg_loss:0.017, val_acc:0.976]
Epoch [112/120    avg_loss:0.016, val_acc:0.976]
Epoch [113/120    avg_loss:0.016, val_acc:0.980]
Epoch [114/120    avg_loss:0.014, val_acc:0.980]
Epoch [115/120    avg_loss:0.014, val_acc:0.978]
Epoch [116/120    avg_loss:0.014, val_acc:0.978]
Epoch [117/120    avg_loss:0.016, val_acc:0.977]
Epoch [118/120    avg_loss:0.014, val_acc:0.978]
Epoch [119/120    avg_loss:0.013, val_acc:0.980]
Epoch [120/120    avg_loss:0.016, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1263    0    0    0    3    0    0    0    2   16    0    1
     0    0    0]
 [   0    0    0  722    4    0    0    0    0    3    3    5    9    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   13    0    0    0    0    0    0    1  825   34    0    0
     0    2    0]
 [   0    0    6    0    0    1    1    0    0    0   12 2178   12    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0    0    2    5  518    0
     2    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    1    0    0    0
  1122   14    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    12  325    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.91869918699187

F1 scores:
[       nan 0.975      0.98402805 0.98031229 0.99069767 0.99425947
 0.98716981 1.         1.         0.82926829 0.95818815 0.97909643
 0.96461825 0.99462366 0.98550725 0.94476744 0.98245614]

Kappa:
0.9762620028364172
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f46952147f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.695, val_acc:0.360]
Epoch [2/120    avg_loss:2.322, val_acc:0.335]
Epoch [3/120    avg_loss:2.133, val_acc:0.552]
Epoch [4/120    avg_loss:1.924, val_acc:0.566]
Epoch [5/120    avg_loss:1.791, val_acc:0.576]
Epoch [6/120    avg_loss:1.661, val_acc:0.596]
Epoch [7/120    avg_loss:1.490, val_acc:0.633]
Epoch [8/120    avg_loss:1.342, val_acc:0.644]
Epoch [9/120    avg_loss:1.199, val_acc:0.639]
Epoch [10/120    avg_loss:1.038, val_acc:0.683]
Epoch [11/120    avg_loss:0.946, val_acc:0.715]
Epoch [12/120    avg_loss:0.932, val_acc:0.719]
Epoch [13/120    avg_loss:0.782, val_acc:0.774]
Epoch [14/120    avg_loss:0.776, val_acc:0.751]
Epoch [15/120    avg_loss:0.661, val_acc:0.801]
Epoch [16/120    avg_loss:0.594, val_acc:0.766]
Epoch [17/120    avg_loss:0.577, val_acc:0.825]
Epoch [18/120    avg_loss:0.513, val_acc:0.840]
Epoch [19/120    avg_loss:0.468, val_acc:0.822]
Epoch [20/120    avg_loss:0.398, val_acc:0.841]
Epoch [21/120    avg_loss:0.389, val_acc:0.853]
Epoch [22/120    avg_loss:0.368, val_acc:0.867]
Epoch [23/120    avg_loss:0.366, val_acc:0.879]
Epoch [24/120    avg_loss:0.306, val_acc:0.880]
Epoch [25/120    avg_loss:0.260, val_acc:0.887]
Epoch [26/120    avg_loss:0.258, val_acc:0.914]
Epoch [27/120    avg_loss:0.207, val_acc:0.906]
Epoch [28/120    avg_loss:0.186, val_acc:0.928]
Epoch [29/120    avg_loss:0.190, val_acc:0.915]
Epoch [30/120    avg_loss:0.181, val_acc:0.923]
Epoch [31/120    avg_loss:0.158, val_acc:0.932]
Epoch [32/120    avg_loss:0.147, val_acc:0.939]
Epoch [33/120    avg_loss:0.144, val_acc:0.931]
Epoch [34/120    avg_loss:0.162, val_acc:0.902]
Epoch [35/120    avg_loss:0.117, val_acc:0.945]
Epoch [36/120    avg_loss:0.122, val_acc:0.930]
Epoch [37/120    avg_loss:0.118, val_acc:0.926]
Epoch [38/120    avg_loss:0.107, val_acc:0.943]
Epoch [39/120    avg_loss:0.092, val_acc:0.936]
Epoch [40/120    avg_loss:0.118, val_acc:0.945]
Epoch [41/120    avg_loss:0.099, val_acc:0.947]
Epoch [42/120    avg_loss:0.082, val_acc:0.961]
Epoch [43/120    avg_loss:0.073, val_acc:0.952]
Epoch [44/120    avg_loss:0.072, val_acc:0.944]
Epoch [45/120    avg_loss:0.075, val_acc:0.964]
Epoch [46/120    avg_loss:0.055, val_acc:0.970]
Epoch [47/120    avg_loss:0.092, val_acc:0.857]
Epoch [48/120    avg_loss:0.122, val_acc:0.950]
Epoch [49/120    avg_loss:0.246, val_acc:0.790]
Epoch [50/120    avg_loss:0.626, val_acc:0.874]
Epoch [51/120    avg_loss:0.299, val_acc:0.901]
Epoch [52/120    avg_loss:0.195, val_acc:0.918]
Epoch [53/120    avg_loss:0.151, val_acc:0.940]
Epoch [54/120    avg_loss:0.097, val_acc:0.930]
Epoch [55/120    avg_loss:0.115, val_acc:0.935]
Epoch [56/120    avg_loss:0.113, val_acc:0.938]
Epoch [57/120    avg_loss:0.100, val_acc:0.940]
Epoch [58/120    avg_loss:0.100, val_acc:0.952]
Epoch [59/120    avg_loss:0.067, val_acc:0.958]
Epoch [60/120    avg_loss:0.054, val_acc:0.967]
Epoch [61/120    avg_loss:0.047, val_acc:0.971]
Epoch [62/120    avg_loss:0.040, val_acc:0.970]
Epoch [63/120    avg_loss:0.044, val_acc:0.971]
Epoch [64/120    avg_loss:0.044, val_acc:0.970]
Epoch [65/120    avg_loss:0.040, val_acc:0.969]
Epoch [66/120    avg_loss:0.037, val_acc:0.971]
Epoch [67/120    avg_loss:0.038, val_acc:0.970]
Epoch [68/120    avg_loss:0.033, val_acc:0.970]
Epoch [69/120    avg_loss:0.037, val_acc:0.971]
Epoch [70/120    avg_loss:0.036, val_acc:0.972]
Epoch [71/120    avg_loss:0.041, val_acc:0.971]
Epoch [72/120    avg_loss:0.035, val_acc:0.973]
Epoch [73/120    avg_loss:0.038, val_acc:0.973]
Epoch [74/120    avg_loss:0.032, val_acc:0.974]
Epoch [75/120    avg_loss:0.029, val_acc:0.973]
Epoch [76/120    avg_loss:0.037, val_acc:0.974]
Epoch [77/120    avg_loss:0.035, val_acc:0.974]
Epoch [78/120    avg_loss:0.028, val_acc:0.973]
Epoch [79/120    avg_loss:0.030, val_acc:0.974]
Epoch [80/120    avg_loss:0.031, val_acc:0.976]
Epoch [81/120    avg_loss:0.033, val_acc:0.974]
Epoch [82/120    avg_loss:0.031, val_acc:0.973]
Epoch [83/120    avg_loss:0.028, val_acc:0.974]
Epoch [84/120    avg_loss:0.027, val_acc:0.978]
Epoch [85/120    avg_loss:0.029, val_acc:0.975]
Epoch [86/120    avg_loss:0.028, val_acc:0.978]
Epoch [87/120    avg_loss:0.027, val_acc:0.977]
Epoch [88/120    avg_loss:0.028, val_acc:0.979]
Epoch [89/120    avg_loss:0.028, val_acc:0.977]
Epoch [90/120    avg_loss:0.029, val_acc:0.978]
Epoch [91/120    avg_loss:0.034, val_acc:0.977]
Epoch [92/120    avg_loss:0.028, val_acc:0.977]
Epoch [93/120    avg_loss:0.025, val_acc:0.977]
Epoch [94/120    avg_loss:0.025, val_acc:0.976]
Epoch [95/120    avg_loss:0.029, val_acc:0.980]
Epoch [96/120    avg_loss:0.028, val_acc:0.978]
Epoch [97/120    avg_loss:0.028, val_acc:0.978]
Epoch [98/120    avg_loss:0.025, val_acc:0.978]
Epoch [99/120    avg_loss:0.032, val_acc:0.974]
Epoch [100/120    avg_loss:0.033, val_acc:0.976]
Epoch [101/120    avg_loss:0.030, val_acc:0.979]
Epoch [102/120    avg_loss:0.025, val_acc:0.979]
Epoch [103/120    avg_loss:0.028, val_acc:0.977]
Epoch [104/120    avg_loss:0.028, val_acc:0.978]
Epoch [105/120    avg_loss:0.031, val_acc:0.976]
Epoch [106/120    avg_loss:0.026, val_acc:0.978]
Epoch [107/120    avg_loss:0.030, val_acc:0.977]
Epoch [108/120    avg_loss:0.026, val_acc:0.977]
Epoch [109/120    avg_loss:0.024, val_acc:0.976]
Epoch [110/120    avg_loss:0.022, val_acc:0.976]
Epoch [111/120    avg_loss:0.024, val_acc:0.977]
Epoch [112/120    avg_loss:0.023, val_acc:0.977]
Epoch [113/120    avg_loss:0.024, val_acc:0.977]
Epoch [114/120    avg_loss:0.021, val_acc:0.977]
Epoch [115/120    avg_loss:0.024, val_acc:0.977]
Epoch [116/120    avg_loss:0.023, val_acc:0.977]
Epoch [117/120    avg_loss:0.022, val_acc:0.978]
Epoch [118/120    avg_loss:0.026, val_acc:0.978]
Epoch [119/120    avg_loss:0.027, val_acc:0.978]
Epoch [120/120    avg_loss:0.022, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1237    0    0    0    2    0    0    0   17   29    0    0
     0    0    0]
 [   0    0    0  713    0    8    0    0    0    6    3    4   13    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    1    1    0    1    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   20    0    0    1    0    0    0    0  825   24    0    0
     1    4    0]
 [   0    0   10    0    0    1    1    0    0    0   19 2164   13    2
     0    0    0]
 [   0    0    0    5    2    1    0    0    0    0   14    4  506    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1132    5    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    38  309    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.11653116531166

F1 scores:
[       nan 0.975      0.96943574 0.97337884 0.9953271  0.97823597
 0.99467681 0.98039216 0.99767442 0.8372093  0.93963554 0.97565374
 0.94756554 0.99462366 0.97712559 0.92932331 0.98224852]

Kappa:
0.9671112591984199
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff21df14860>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.691, val_acc:0.239]
Epoch [2/120    avg_loss:2.292, val_acc:0.381]
Epoch [3/120    avg_loss:2.125, val_acc:0.547]
Epoch [4/120    avg_loss:1.918, val_acc:0.574]
Epoch [5/120    avg_loss:1.794, val_acc:0.589]
Epoch [6/120    avg_loss:1.681, val_acc:0.628]
Epoch [7/120    avg_loss:1.521, val_acc:0.613]
Epoch [8/120    avg_loss:1.413, val_acc:0.644]
Epoch [9/120    avg_loss:1.318, val_acc:0.644]
Epoch [10/120    avg_loss:1.147, val_acc:0.718]
Epoch [11/120    avg_loss:1.045, val_acc:0.706]
Epoch [12/120    avg_loss:0.935, val_acc:0.748]
Epoch [13/120    avg_loss:0.853, val_acc:0.764]
Epoch [14/120    avg_loss:0.801, val_acc:0.781]
Epoch [15/120    avg_loss:0.654, val_acc:0.797]
Epoch [16/120    avg_loss:0.584, val_acc:0.843]
Epoch [17/120    avg_loss:0.555, val_acc:0.778]
Epoch [18/120    avg_loss:0.473, val_acc:0.845]
Epoch [19/120    avg_loss:0.429, val_acc:0.838]
Epoch [20/120    avg_loss:0.399, val_acc:0.860]
Epoch [21/120    avg_loss:0.400, val_acc:0.851]
Epoch [22/120    avg_loss:0.380, val_acc:0.824]
Epoch [23/120    avg_loss:0.428, val_acc:0.803]
Epoch [24/120    avg_loss:0.365, val_acc:0.884]
Epoch [25/120    avg_loss:0.390, val_acc:0.836]
Epoch [26/120    avg_loss:0.335, val_acc:0.887]
Epoch [27/120    avg_loss:0.251, val_acc:0.906]
Epoch [28/120    avg_loss:0.205, val_acc:0.913]
Epoch [29/120    avg_loss:0.190, val_acc:0.908]
Epoch [30/120    avg_loss:0.213, val_acc:0.930]
Epoch [31/120    avg_loss:0.175, val_acc:0.904]
Epoch [32/120    avg_loss:0.174, val_acc:0.919]
Epoch [33/120    avg_loss:0.166, val_acc:0.934]
Epoch [34/120    avg_loss:0.128, val_acc:0.925]
Epoch [35/120    avg_loss:0.143, val_acc:0.932]
Epoch [36/120    avg_loss:0.162, val_acc:0.948]
Epoch [37/120    avg_loss:0.128, val_acc:0.940]
Epoch [38/120    avg_loss:0.124, val_acc:0.939]
Epoch [39/120    avg_loss:0.120, val_acc:0.920]
Epoch [40/120    avg_loss:0.105, val_acc:0.932]
Epoch [41/120    avg_loss:0.082, val_acc:0.948]
Epoch [42/120    avg_loss:0.083, val_acc:0.948]
Epoch [43/120    avg_loss:0.090, val_acc:0.954]
Epoch [44/120    avg_loss:0.068, val_acc:0.964]
Epoch [45/120    avg_loss:0.099, val_acc:0.899]
Epoch [46/120    avg_loss:0.132, val_acc:0.946]
Epoch [47/120    avg_loss:0.108, val_acc:0.930]
Epoch [48/120    avg_loss:0.088, val_acc:0.959]
Epoch [49/120    avg_loss:0.067, val_acc:0.957]
Epoch [50/120    avg_loss:0.056, val_acc:0.957]
Epoch [51/120    avg_loss:0.054, val_acc:0.958]
Epoch [52/120    avg_loss:0.050, val_acc:0.954]
Epoch [53/120    avg_loss:0.050, val_acc:0.964]
Epoch [54/120    avg_loss:0.047, val_acc:0.961]
Epoch [55/120    avg_loss:0.050, val_acc:0.963]
Epoch [56/120    avg_loss:0.052, val_acc:0.943]
Epoch [57/120    avg_loss:0.060, val_acc:0.952]
Epoch [58/120    avg_loss:0.060, val_acc:0.962]
Epoch [59/120    avg_loss:0.041, val_acc:0.975]
Epoch [60/120    avg_loss:0.045, val_acc:0.966]
Epoch [61/120    avg_loss:0.039, val_acc:0.971]
Epoch [62/120    avg_loss:0.036, val_acc:0.960]
Epoch [63/120    avg_loss:0.053, val_acc:0.967]
Epoch [64/120    avg_loss:0.043, val_acc:0.959]
Epoch [65/120    avg_loss:0.047, val_acc:0.959]
Epoch [66/120    avg_loss:0.040, val_acc:0.972]
Epoch [67/120    avg_loss:0.028, val_acc:0.970]
Epoch [68/120    avg_loss:0.025, val_acc:0.980]
Epoch [69/120    avg_loss:0.028, val_acc:0.960]
Epoch [70/120    avg_loss:0.030, val_acc:0.971]
Epoch [71/120    avg_loss:0.029, val_acc:0.968]
Epoch [72/120    avg_loss:0.021, val_acc:0.975]
Epoch [73/120    avg_loss:0.021, val_acc:0.972]
Epoch [74/120    avg_loss:0.032, val_acc:0.973]
Epoch [75/120    avg_loss:0.021, val_acc:0.974]
Epoch [76/120    avg_loss:0.019, val_acc:0.972]
Epoch [77/120    avg_loss:0.025, val_acc:0.978]
Epoch [78/120    avg_loss:0.045, val_acc:0.967]
Epoch [79/120    avg_loss:0.036, val_acc:0.963]
Epoch [80/120    avg_loss:0.020, val_acc:0.974]
Epoch [81/120    avg_loss:0.016, val_acc:0.978]
Epoch [82/120    avg_loss:0.019, val_acc:0.981]
Epoch [83/120    avg_loss:0.017, val_acc:0.983]
Epoch [84/120    avg_loss:0.013, val_acc:0.981]
Epoch [85/120    avg_loss:0.015, val_acc:0.981]
Epoch [86/120    avg_loss:0.013, val_acc:0.981]
Epoch [87/120    avg_loss:0.012, val_acc:0.982]
Epoch [88/120    avg_loss:0.014, val_acc:0.982]
Epoch [89/120    avg_loss:0.012, val_acc:0.982]
Epoch [90/120    avg_loss:0.013, val_acc:0.982]
Epoch [91/120    avg_loss:0.014, val_acc:0.983]
Epoch [92/120    avg_loss:0.014, val_acc:0.981]
Epoch [93/120    avg_loss:0.017, val_acc:0.981]
Epoch [94/120    avg_loss:0.014, val_acc:0.982]
Epoch [95/120    avg_loss:0.012, val_acc:0.983]
Epoch [96/120    avg_loss:0.013, val_acc:0.983]
Epoch [97/120    avg_loss:0.012, val_acc:0.982]
Epoch [98/120    avg_loss:0.011, val_acc:0.982]
Epoch [99/120    avg_loss:0.013, val_acc:0.981]
Epoch [100/120    avg_loss:0.015, val_acc:0.983]
Epoch [101/120    avg_loss:0.011, val_acc:0.982]
Epoch [102/120    avg_loss:0.011, val_acc:0.981]
Epoch [103/120    avg_loss:0.013, val_acc:0.981]
Epoch [104/120    avg_loss:0.015, val_acc:0.982]
Epoch [105/120    avg_loss:0.013, val_acc:0.981]
Epoch [106/120    avg_loss:0.013, val_acc:0.982]
Epoch [107/120    avg_loss:0.010, val_acc:0.981]
Epoch [108/120    avg_loss:0.013, val_acc:0.983]
Epoch [109/120    avg_loss:0.014, val_acc:0.983]
Epoch [110/120    avg_loss:0.013, val_acc:0.982]
Epoch [111/120    avg_loss:0.011, val_acc:0.982]
Epoch [112/120    avg_loss:0.010, val_acc:0.982]
Epoch [113/120    avg_loss:0.012, val_acc:0.982]
Epoch [114/120    avg_loss:0.014, val_acc:0.983]
Epoch [115/120    avg_loss:0.015, val_acc:0.982]
Epoch [116/120    avg_loss:0.011, val_acc:0.982]
Epoch [117/120    avg_loss:0.010, val_acc:0.982]
Epoch [118/120    avg_loss:0.010, val_acc:0.982]
Epoch [119/120    avg_loss:0.012, val_acc:0.983]
Epoch [120/120    avg_loss:0.010, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1257    0    0    0    1    0    0    1    4   22    0    0
     0    0    0]
 [   0    0    0  713    2    6    0    0    0    3    2    6   12    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    4    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    2    0    0    0    2  852   15    0    0
     0    3    0]
 [   0    0    7    0    0    0    0    0    0    1    6 2182   14    0
     0    0    0]
 [   0    0    0    8    0    1    0    0    0    0    4    2  516    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
    47  275    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.57181571815718

F1 scores:
[       nan 0.96202532 0.98588235 0.97138965 0.9953271  0.98514286
 0.97757848 0.92592593 0.99883586 0.80952381 0.97594502 0.9831043
 0.95732839 0.9919571  0.97584124 0.86887836 0.97647059]

Kappa:
0.9723009653992893
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f340b7616d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.669, val_acc:0.433]
Epoch [2/120    avg_loss:2.288, val_acc:0.467]
Epoch [3/120    avg_loss:2.044, val_acc:0.556]
Epoch [4/120    avg_loss:1.891, val_acc:0.552]
Epoch [5/120    avg_loss:1.744, val_acc:0.608]
Epoch [6/120    avg_loss:1.574, val_acc:0.586]
Epoch [7/120    avg_loss:1.413, val_acc:0.632]
Epoch [8/120    avg_loss:1.299, val_acc:0.607]
Epoch [9/120    avg_loss:1.203, val_acc:0.635]
Epoch [10/120    avg_loss:1.053, val_acc:0.688]
Epoch [11/120    avg_loss:0.973, val_acc:0.680]
Epoch [12/120    avg_loss:0.926, val_acc:0.668]
Epoch [13/120    avg_loss:0.794, val_acc:0.717]
Epoch [14/120    avg_loss:0.717, val_acc:0.745]
Epoch [15/120    avg_loss:0.615, val_acc:0.762]
Epoch [16/120    avg_loss:0.528, val_acc:0.796]
Epoch [17/120    avg_loss:0.471, val_acc:0.822]
Epoch [18/120    avg_loss:0.434, val_acc:0.832]
Epoch [19/120    avg_loss:0.404, val_acc:0.845]
Epoch [20/120    avg_loss:0.371, val_acc:0.847]
Epoch [21/120    avg_loss:0.309, val_acc:0.850]
Epoch [22/120    avg_loss:0.285, val_acc:0.863]
Epoch [23/120    avg_loss:0.292, val_acc:0.877]
Epoch [24/120    avg_loss:0.268, val_acc:0.871]
Epoch [25/120    avg_loss:0.204, val_acc:0.899]
Epoch [26/120    avg_loss:0.219, val_acc:0.899]
Epoch [27/120    avg_loss:0.183, val_acc:0.900]
Epoch [28/120    avg_loss:0.190, val_acc:0.922]
Epoch [29/120    avg_loss:0.177, val_acc:0.917]
Epoch [30/120    avg_loss:0.239, val_acc:0.904]
Epoch [31/120    avg_loss:0.192, val_acc:0.912]
Epoch [32/120    avg_loss:0.156, val_acc:0.904]
Epoch [33/120    avg_loss:0.160, val_acc:0.934]
Epoch [34/120    avg_loss:0.124, val_acc:0.914]
Epoch [35/120    avg_loss:0.161, val_acc:0.908]
Epoch [36/120    avg_loss:0.110, val_acc:0.941]
Epoch [37/120    avg_loss:0.091, val_acc:0.944]
Epoch [38/120    avg_loss:0.081, val_acc:0.939]
Epoch [39/120    avg_loss:0.080, val_acc:0.939]
Epoch [40/120    avg_loss:0.081, val_acc:0.935]
Epoch [41/120    avg_loss:0.072, val_acc:0.932]
Epoch [42/120    avg_loss:0.083, val_acc:0.947]
Epoch [43/120    avg_loss:0.108, val_acc:0.929]
Epoch [44/120    avg_loss:0.082, val_acc:0.938]
Epoch [45/120    avg_loss:0.062, val_acc:0.941]
Epoch [46/120    avg_loss:0.058, val_acc:0.941]
Epoch [47/120    avg_loss:0.048, val_acc:0.949]
Epoch [48/120    avg_loss:0.047, val_acc:0.943]
Epoch [49/120    avg_loss:0.058, val_acc:0.945]
Epoch [50/120    avg_loss:0.061, val_acc:0.953]
Epoch [51/120    avg_loss:0.057, val_acc:0.923]
Epoch [52/120    avg_loss:0.066, val_acc:0.916]
Epoch [53/120    avg_loss:0.061, val_acc:0.952]
Epoch [54/120    avg_loss:0.059, val_acc:0.955]
Epoch [55/120    avg_loss:0.059, val_acc:0.955]
Epoch [56/120    avg_loss:0.180, val_acc:0.905]
Epoch [57/120    avg_loss:0.151, val_acc:0.941]
Epoch [58/120    avg_loss:0.094, val_acc:0.943]
Epoch [59/120    avg_loss:0.080, val_acc:0.938]
Epoch [60/120    avg_loss:0.068, val_acc:0.949]
Epoch [61/120    avg_loss:0.090, val_acc:0.921]
Epoch [62/120    avg_loss:0.064, val_acc:0.934]
Epoch [63/120    avg_loss:0.055, val_acc:0.947]
Epoch [64/120    avg_loss:0.052, val_acc:0.953]
Epoch [65/120    avg_loss:0.046, val_acc:0.952]
Epoch [66/120    avg_loss:0.043, val_acc:0.945]
Epoch [67/120    avg_loss:0.039, val_acc:0.956]
Epoch [68/120    avg_loss:0.034, val_acc:0.952]
Epoch [69/120    avg_loss:0.039, val_acc:0.927]
Epoch [70/120    avg_loss:0.040, val_acc:0.959]
Epoch [71/120    avg_loss:0.026, val_acc:0.963]
Epoch [72/120    avg_loss:0.034, val_acc:0.968]
Epoch [73/120    avg_loss:0.027, val_acc:0.965]
Epoch [74/120    avg_loss:0.044, val_acc:0.963]
Epoch [75/120    avg_loss:0.028, val_acc:0.956]
Epoch [76/120    avg_loss:0.026, val_acc:0.965]
Epoch [77/120    avg_loss:0.022, val_acc:0.972]
Epoch [78/120    avg_loss:0.024, val_acc:0.967]
Epoch [79/120    avg_loss:0.015, val_acc:0.953]
Epoch [80/120    avg_loss:0.018, val_acc:0.971]
Epoch [81/120    avg_loss:0.019, val_acc:0.963]
Epoch [82/120    avg_loss:0.033, val_acc:0.961]
Epoch [83/120    avg_loss:0.025, val_acc:0.968]
Epoch [84/120    avg_loss:0.021, val_acc:0.970]
Epoch [85/120    avg_loss:0.020, val_acc:0.967]
Epoch [86/120    avg_loss:0.015, val_acc:0.966]
Epoch [87/120    avg_loss:0.053, val_acc:0.940]
Epoch [88/120    avg_loss:0.052, val_acc:0.967]
Epoch [89/120    avg_loss:0.030, val_acc:0.968]
Epoch [90/120    avg_loss:0.035, val_acc:0.951]
Epoch [91/120    avg_loss:0.020, val_acc:0.963]
Epoch [92/120    avg_loss:0.019, val_acc:0.966]
Epoch [93/120    avg_loss:0.012, val_acc:0.966]
Epoch [94/120    avg_loss:0.014, val_acc:0.968]
Epoch [95/120    avg_loss:0.013, val_acc:0.970]
Epoch [96/120    avg_loss:0.014, val_acc:0.970]
Epoch [97/120    avg_loss:0.013, val_acc:0.972]
Epoch [98/120    avg_loss:0.013, val_acc:0.972]
Epoch [99/120    avg_loss:0.012, val_acc:0.971]
Epoch [100/120    avg_loss:0.010, val_acc:0.972]
Epoch [101/120    avg_loss:0.011, val_acc:0.972]
Epoch [102/120    avg_loss:0.010, val_acc:0.972]
Epoch [103/120    avg_loss:0.011, val_acc:0.971]
Epoch [104/120    avg_loss:0.011, val_acc:0.972]
Epoch [105/120    avg_loss:0.010, val_acc:0.972]
Epoch [106/120    avg_loss:0.013, val_acc:0.971]
Epoch [107/120    avg_loss:0.011, val_acc:0.971]
Epoch [108/120    avg_loss:0.011, val_acc:0.971]
Epoch [109/120    avg_loss:0.011, val_acc:0.969]
Epoch [110/120    avg_loss:0.010, val_acc:0.969]
Epoch [111/120    avg_loss:0.011, val_acc:0.972]
Epoch [112/120    avg_loss:0.013, val_acc:0.973]
Epoch [113/120    avg_loss:0.012, val_acc:0.970]
Epoch [114/120    avg_loss:0.008, val_acc:0.970]
Epoch [115/120    avg_loss:0.010, val_acc:0.970]
Epoch [116/120    avg_loss:0.009, val_acc:0.973]
Epoch [117/120    avg_loss:0.012, val_acc:0.972]
Epoch [118/120    avg_loss:0.010, val_acc:0.972]
Epoch [119/120    avg_loss:0.012, val_acc:0.972]
Epoch [120/120    avg_loss:0.009, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1255    1    0    0    0    0    0    3    5   21    0    0
     0    0    0]
 [   0    0    2  708    1   10    0    0    0   11    2    6    5    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    4    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    1    0    0    0    1  851   12    0    0
     0    3    0]
 [   0    0   12    0    0    1    0    3    0    4   15 2168    7    0
     0    0    0]
 [   0    0    0    0    1    0    0    0    0    0    1    1  528    0
     0    2    1]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    1    0    0    0
  1130    4    0]
 [   0    0    0    0    0    0    5    0    0    1    0    0    0    0
    36  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.72357723577235

F1 scores:
[       nan 0.91358025 0.9800859  0.97252747 0.9953271  0.9738933
 0.99392097 0.87719298 0.99649942 0.62068966 0.97035348 0.98121747
 0.98232558 0.99191375 0.97962722 0.92284418 0.98809524]

Kappa:
0.9740492838426174
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f746df77908>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.726, val_acc:0.466]
Epoch [2/120    avg_loss:2.342, val_acc:0.497]
Epoch [3/120    avg_loss:2.124, val_acc:0.543]
Epoch [4/120    avg_loss:1.909, val_acc:0.601]
Epoch [5/120    avg_loss:1.718, val_acc:0.607]
Epoch [6/120    avg_loss:1.547, val_acc:0.624]
Epoch [7/120    avg_loss:1.374, val_acc:0.643]
Epoch [8/120    avg_loss:1.205, val_acc:0.673]
Epoch [9/120    avg_loss:1.101, val_acc:0.647]
Epoch [10/120    avg_loss:1.017, val_acc:0.723]
Epoch [11/120    avg_loss:1.037, val_acc:0.725]
Epoch [12/120    avg_loss:0.900, val_acc:0.775]
Epoch [13/120    avg_loss:0.765, val_acc:0.779]
Epoch [14/120    avg_loss:0.649, val_acc:0.810]
Epoch [15/120    avg_loss:0.641, val_acc:0.766]
Epoch [16/120    avg_loss:0.627, val_acc:0.827]
Epoch [17/120    avg_loss:0.512, val_acc:0.827]
Epoch [18/120    avg_loss:0.459, val_acc:0.812]
Epoch [19/120    avg_loss:0.434, val_acc:0.832]
Epoch [20/120    avg_loss:0.410, val_acc:0.805]
Epoch [21/120    avg_loss:0.476, val_acc:0.851]
Epoch [22/120    avg_loss:0.337, val_acc:0.869]
Epoch [23/120    avg_loss:0.294, val_acc:0.873]
Epoch [24/120    avg_loss:0.296, val_acc:0.885]
Epoch [25/120    avg_loss:0.222, val_acc:0.907]
Epoch [26/120    avg_loss:0.325, val_acc:0.875]
Epoch [27/120    avg_loss:0.230, val_acc:0.887]
Epoch [28/120    avg_loss:0.254, val_acc:0.906]
Epoch [29/120    avg_loss:0.216, val_acc:0.912]
Epoch [30/120    avg_loss:0.206, val_acc:0.907]
Epoch [31/120    avg_loss:0.208, val_acc:0.912]
Epoch [32/120    avg_loss:0.158, val_acc:0.938]
Epoch [33/120    avg_loss:0.164, val_acc:0.920]
Epoch [34/120    avg_loss:0.175, val_acc:0.930]
Epoch [35/120    avg_loss:0.135, val_acc:0.938]
Epoch [36/120    avg_loss:0.115, val_acc:0.957]
Epoch [37/120    avg_loss:0.105, val_acc:0.952]
Epoch [38/120    avg_loss:0.102, val_acc:0.921]
Epoch [39/120    avg_loss:0.109, val_acc:0.926]
Epoch [40/120    avg_loss:0.099, val_acc:0.950]
Epoch [41/120    avg_loss:0.080, val_acc:0.955]
Epoch [42/120    avg_loss:0.074, val_acc:0.955]
Epoch [43/120    avg_loss:0.070, val_acc:0.963]
Epoch [44/120    avg_loss:0.072, val_acc:0.963]
Epoch [45/120    avg_loss:0.069, val_acc:0.946]
Epoch [46/120    avg_loss:0.091, val_acc:0.926]
Epoch [47/120    avg_loss:0.077, val_acc:0.919]
Epoch [48/120    avg_loss:0.087, val_acc:0.947]
Epoch [49/120    avg_loss:0.073, val_acc:0.953]
Epoch [50/120    avg_loss:0.066, val_acc:0.956]
Epoch [51/120    avg_loss:0.090, val_acc:0.908]
Epoch [52/120    avg_loss:0.358, val_acc:0.916]
Epoch [53/120    avg_loss:0.310, val_acc:0.908]
Epoch [54/120    avg_loss:0.172, val_acc:0.944]
Epoch [55/120    avg_loss:0.164, val_acc:0.933]
Epoch [56/120    avg_loss:0.138, val_acc:0.931]
Epoch [57/120    avg_loss:0.131, val_acc:0.940]
Epoch [58/120    avg_loss:0.097, val_acc:0.954]
Epoch [59/120    avg_loss:0.081, val_acc:0.958]
Epoch [60/120    avg_loss:0.061, val_acc:0.960]
Epoch [61/120    avg_loss:0.068, val_acc:0.961]
Epoch [62/120    avg_loss:0.070, val_acc:0.960]
Epoch [63/120    avg_loss:0.056, val_acc:0.964]
Epoch [64/120    avg_loss:0.059, val_acc:0.963]
Epoch [65/120    avg_loss:0.056, val_acc:0.961]
Epoch [66/120    avg_loss:0.057, val_acc:0.968]
Epoch [67/120    avg_loss:0.060, val_acc:0.967]
Epoch [68/120    avg_loss:0.055, val_acc:0.970]
Epoch [69/120    avg_loss:0.051, val_acc:0.966]
Epoch [70/120    avg_loss:0.050, val_acc:0.968]
Epoch [71/120    avg_loss:0.052, val_acc:0.968]
Epoch [72/120    avg_loss:0.044, val_acc:0.968]
Epoch [73/120    avg_loss:0.048, val_acc:0.972]
Epoch [74/120    avg_loss:0.046, val_acc:0.969]
Epoch [75/120    avg_loss:0.047, val_acc:0.967]
Epoch [76/120    avg_loss:0.044, val_acc:0.969]
Epoch [77/120    avg_loss:0.044, val_acc:0.968]
Epoch [78/120    avg_loss:0.041, val_acc:0.969]
Epoch [79/120    avg_loss:0.047, val_acc:0.970]
Epoch [80/120    avg_loss:0.042, val_acc:0.974]
Epoch [81/120    avg_loss:0.046, val_acc:0.969]
Epoch [82/120    avg_loss:0.048, val_acc:0.970]
Epoch [83/120    avg_loss:0.039, val_acc:0.971]
Epoch [84/120    avg_loss:0.042, val_acc:0.969]
Epoch [85/120    avg_loss:0.036, val_acc:0.968]
Epoch [86/120    avg_loss:0.037, val_acc:0.968]
Epoch [87/120    avg_loss:0.042, val_acc:0.969]
Epoch [88/120    avg_loss:0.040, val_acc:0.969]
Epoch [89/120    avg_loss:0.038, val_acc:0.971]
Epoch [90/120    avg_loss:0.034, val_acc:0.972]
Epoch [91/120    avg_loss:0.035, val_acc:0.971]
Epoch [92/120    avg_loss:0.041, val_acc:0.972]
Epoch [93/120    avg_loss:0.037, val_acc:0.972]
Epoch [94/120    avg_loss:0.035, val_acc:0.972]
Epoch [95/120    avg_loss:0.035, val_acc:0.972]
Epoch [96/120    avg_loss:0.038, val_acc:0.971]
Epoch [97/120    avg_loss:0.034, val_acc:0.971]
Epoch [98/120    avg_loss:0.038, val_acc:0.971]
Epoch [99/120    avg_loss:0.038, val_acc:0.972]
Epoch [100/120    avg_loss:0.038, val_acc:0.973]
Epoch [101/120    avg_loss:0.039, val_acc:0.973]
Epoch [102/120    avg_loss:0.038, val_acc:0.972]
Epoch [103/120    avg_loss:0.039, val_acc:0.972]
Epoch [104/120    avg_loss:0.036, val_acc:0.973]
Epoch [105/120    avg_loss:0.037, val_acc:0.972]
Epoch [106/120    avg_loss:0.041, val_acc:0.972]
Epoch [107/120    avg_loss:0.032, val_acc:0.972]
Epoch [108/120    avg_loss:0.035, val_acc:0.972]
Epoch [109/120    avg_loss:0.038, val_acc:0.972]
Epoch [110/120    avg_loss:0.032, val_acc:0.972]
Epoch [111/120    avg_loss:0.037, val_acc:0.972]
Epoch [112/120    avg_loss:0.037, val_acc:0.972]
Epoch [113/120    avg_loss:0.035, val_acc:0.972]
Epoch [114/120    avg_loss:0.038, val_acc:0.972]
Epoch [115/120    avg_loss:0.030, val_acc:0.972]
Epoch [116/120    avg_loss:0.031, val_acc:0.972]
Epoch [117/120    avg_loss:0.035, val_acc:0.972]
Epoch [118/120    avg_loss:0.033, val_acc:0.972]
Epoch [119/120    avg_loss:0.034, val_acc:0.972]
Epoch [120/120    avg_loss:0.036, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1256    6    0    0    2    0    0    1    1   19    0    0
     0    0    0]
 [   0    0    0  724    9    0    0    0    0    4    2    0    3    5
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    2    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  654    0    0    1    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0   14    0    0    3    2    0    0    2  829   24    0    0
     0    1    0]
 [   0    0   14    0    0    1    4    0    0    2   32 2142   13    2
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0    6  525    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1123   16    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
    17  323    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.46341463414635

F1 scores:
[       nan 0.94117647 0.97781238 0.97837838 0.97695853 0.98847926
 0.98642534 0.96153846 0.9953271  0.75555556 0.95287356 0.97297297
 0.97402597 0.98143236 0.98336252 0.94032023 0.97590361]

Kappa:
0.9710960577360614
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5bdd8c6908>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.652, val_acc:0.283]
Epoch [2/120    avg_loss:2.309, val_acc:0.435]
Epoch [3/120    avg_loss:2.129, val_acc:0.508]
Epoch [4/120    avg_loss:1.940, val_acc:0.520]
Epoch [5/120    avg_loss:1.787, val_acc:0.544]
Epoch [6/120    avg_loss:1.635, val_acc:0.534]
Epoch [7/120    avg_loss:1.521, val_acc:0.554]
Epoch [8/120    avg_loss:1.416, val_acc:0.585]
Epoch [9/120    avg_loss:1.272, val_acc:0.661]
Epoch [10/120    avg_loss:1.149, val_acc:0.698]
Epoch [11/120    avg_loss:1.042, val_acc:0.686]
Epoch [12/120    avg_loss:0.892, val_acc:0.738]
Epoch [13/120    avg_loss:0.822, val_acc:0.756]
Epoch [14/120    avg_loss:0.704, val_acc:0.805]
Epoch [15/120    avg_loss:0.596, val_acc:0.793]
Epoch [16/120    avg_loss:0.657, val_acc:0.789]
Epoch [17/120    avg_loss:0.599, val_acc:0.777]
Epoch [18/120    avg_loss:0.610, val_acc:0.811]
Epoch [19/120    avg_loss:0.465, val_acc:0.791]
Epoch [20/120    avg_loss:0.431, val_acc:0.828]
Epoch [21/120    avg_loss:0.446, val_acc:0.798]
Epoch [22/120    avg_loss:0.399, val_acc:0.805]
Epoch [23/120    avg_loss:0.370, val_acc:0.850]
Epoch [24/120    avg_loss:0.308, val_acc:0.841]
Epoch [25/120    avg_loss:0.260, val_acc:0.878]
Epoch [26/120    avg_loss:0.227, val_acc:0.869]
Epoch [27/120    avg_loss:0.268, val_acc:0.880]
Epoch [28/120    avg_loss:0.264, val_acc:0.895]
Epoch [29/120    avg_loss:0.230, val_acc:0.895]
Epoch [30/120    avg_loss:0.182, val_acc:0.889]
Epoch [31/120    avg_loss:0.210, val_acc:0.861]
Epoch [32/120    avg_loss:0.173, val_acc:0.912]
Epoch [33/120    avg_loss:0.171, val_acc:0.935]
Epoch [34/120    avg_loss:0.136, val_acc:0.938]
Epoch [35/120    avg_loss:0.140, val_acc:0.923]
Epoch [36/120    avg_loss:0.139, val_acc:0.940]
Epoch [37/120    avg_loss:0.129, val_acc:0.942]
Epoch [38/120    avg_loss:0.129, val_acc:0.940]
Epoch [39/120    avg_loss:0.114, val_acc:0.945]
Epoch [40/120    avg_loss:0.088, val_acc:0.947]
Epoch [41/120    avg_loss:0.075, val_acc:0.946]
Epoch [42/120    avg_loss:0.084, val_acc:0.952]
Epoch [43/120    avg_loss:0.078, val_acc:0.948]
Epoch [44/120    avg_loss:0.077, val_acc:0.938]
Epoch [45/120    avg_loss:0.082, val_acc:0.960]
Epoch [46/120    avg_loss:0.082, val_acc:0.951]
Epoch [47/120    avg_loss:0.095, val_acc:0.944]
Epoch [48/120    avg_loss:0.084, val_acc:0.953]
Epoch [49/120    avg_loss:0.069, val_acc:0.931]
Epoch [50/120    avg_loss:0.090, val_acc:0.961]
Epoch [51/120    avg_loss:0.088, val_acc:0.943]
Epoch [52/120    avg_loss:0.062, val_acc:0.963]
Epoch [53/120    avg_loss:0.052, val_acc:0.959]
Epoch [54/120    avg_loss:0.045, val_acc:0.964]
Epoch [55/120    avg_loss:0.046, val_acc:0.945]
Epoch [56/120    avg_loss:0.062, val_acc:0.958]
Epoch [57/120    avg_loss:0.050, val_acc:0.955]
Epoch [58/120    avg_loss:0.080, val_acc:0.948]
Epoch [59/120    avg_loss:0.098, val_acc:0.956]
Epoch [60/120    avg_loss:0.057, val_acc:0.960]
Epoch [61/120    avg_loss:0.050, val_acc:0.955]
Epoch [62/120    avg_loss:0.052, val_acc:0.949]
Epoch [63/120    avg_loss:0.044, val_acc:0.968]
Epoch [64/120    avg_loss:0.037, val_acc:0.964]
Epoch [65/120    avg_loss:0.031, val_acc:0.972]
Epoch [66/120    avg_loss:0.045, val_acc:0.950]
Epoch [67/120    avg_loss:0.032, val_acc:0.968]
Epoch [68/120    avg_loss:0.026, val_acc:0.972]
Epoch [69/120    avg_loss:0.034, val_acc:0.973]
Epoch [70/120    avg_loss:0.028, val_acc:0.972]
Epoch [71/120    avg_loss:0.040, val_acc:0.970]
Epoch [72/120    avg_loss:0.030, val_acc:0.971]
Epoch [73/120    avg_loss:0.025, val_acc:0.976]
Epoch [74/120    avg_loss:0.063, val_acc:0.944]
Epoch [75/120    avg_loss:0.069, val_acc:0.955]
Epoch [76/120    avg_loss:0.052, val_acc:0.949]
Epoch [77/120    avg_loss:0.042, val_acc:0.971]
Epoch [78/120    avg_loss:0.031, val_acc:0.972]
Epoch [79/120    avg_loss:0.031, val_acc:0.973]
Epoch [80/120    avg_loss:0.030, val_acc:0.971]
Epoch [81/120    avg_loss:0.023, val_acc:0.964]
Epoch [82/120    avg_loss:0.021, val_acc:0.978]
Epoch [83/120    avg_loss:0.021, val_acc:0.967]
Epoch [84/120    avg_loss:0.021, val_acc:0.972]
Epoch [85/120    avg_loss:0.019, val_acc:0.974]
Epoch [86/120    avg_loss:0.020, val_acc:0.972]
Epoch [87/120    avg_loss:0.022, val_acc:0.979]
Epoch [88/120    avg_loss:0.019, val_acc:0.975]
Epoch [89/120    avg_loss:0.015, val_acc:0.980]
Epoch [90/120    avg_loss:0.020, val_acc:0.966]
Epoch [91/120    avg_loss:0.020, val_acc:0.970]
Epoch [92/120    avg_loss:0.013, val_acc:0.976]
Epoch [93/120    avg_loss:0.010, val_acc:0.981]
Epoch [94/120    avg_loss:0.018, val_acc:0.975]
Epoch [95/120    avg_loss:0.034, val_acc:0.952]
Epoch [96/120    avg_loss:0.027, val_acc:0.979]
Epoch [97/120    avg_loss:0.017, val_acc:0.979]
Epoch [98/120    avg_loss:0.122, val_acc:0.927]
Epoch [99/120    avg_loss:0.175, val_acc:0.932]
Epoch [100/120    avg_loss:0.124, val_acc:0.918]
Epoch [101/120    avg_loss:0.085, val_acc:0.957]
Epoch [102/120    avg_loss:0.075, val_acc:0.952]
Epoch [103/120    avg_loss:0.046, val_acc:0.954]
Epoch [104/120    avg_loss:0.042, val_acc:0.973]
Epoch [105/120    avg_loss:0.034, val_acc:0.971]
Epoch [106/120    avg_loss:0.036, val_acc:0.972]
Epoch [107/120    avg_loss:0.029, val_acc:0.977]
Epoch [108/120    avg_loss:0.017, val_acc:0.977]
Epoch [109/120    avg_loss:0.018, val_acc:0.978]
Epoch [110/120    avg_loss:0.016, val_acc:0.978]
Epoch [111/120    avg_loss:0.016, val_acc:0.977]
Epoch [112/120    avg_loss:0.015, val_acc:0.978]
Epoch [113/120    avg_loss:0.015, val_acc:0.979]
Epoch [114/120    avg_loss:0.014, val_acc:0.978]
Epoch [115/120    avg_loss:0.014, val_acc:0.979]
Epoch [116/120    avg_loss:0.015, val_acc:0.980]
Epoch [117/120    avg_loss:0.014, val_acc:0.977]
Epoch [118/120    avg_loss:0.014, val_acc:0.977]
Epoch [119/120    avg_loss:0.013, val_acc:0.978]
Epoch [120/120    avg_loss:0.014, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1262    0    0    0    0    0    0    2    4   17    0    0
     0    0    0]
 [   0    0    1  722    0    0    0    0    0    8    4    3    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    2    0    0    0    1  857   10    0    0
     0    1    0]
 [   0    0    8    0    0    1    1    0    0    1   17 2178    3    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    9    0  516    0
     0    6    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1134    4    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
     7  327    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.42818428184282

F1 scores:
[       nan 0.975      0.9859375  0.98298162 1.         0.99656357
 0.98869631 1.         0.99883586 0.75       0.96890899 0.9859665
 0.97083725 0.99730458 0.99430075 0.95474453 0.98245614]

Kappa:
0.9820826842680644
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f56346e0828>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.643, val_acc:0.373]
Epoch [2/120    avg_loss:2.284, val_acc:0.468]
Epoch [3/120    avg_loss:2.070, val_acc:0.457]
Epoch [4/120    avg_loss:1.908, val_acc:0.544]
Epoch [5/120    avg_loss:1.778, val_acc:0.588]
Epoch [6/120    avg_loss:1.658, val_acc:0.609]
Epoch [7/120    avg_loss:1.536, val_acc:0.628]
Epoch [8/120    avg_loss:1.435, val_acc:0.622]
Epoch [9/120    avg_loss:1.227, val_acc:0.650]
Epoch [10/120    avg_loss:1.139, val_acc:0.670]
Epoch [11/120    avg_loss:1.123, val_acc:0.680]
Epoch [12/120    avg_loss:0.952, val_acc:0.722]
Epoch [13/120    avg_loss:0.828, val_acc:0.755]
Epoch [14/120    avg_loss:0.706, val_acc:0.750]
Epoch [15/120    avg_loss:0.706, val_acc:0.752]
Epoch [16/120    avg_loss:0.711, val_acc:0.735]
Epoch [17/120    avg_loss:0.669, val_acc:0.772]
Epoch [18/120    avg_loss:0.530, val_acc:0.814]
Epoch [19/120    avg_loss:0.535, val_acc:0.766]
Epoch [20/120    avg_loss:0.489, val_acc:0.832]
Epoch [21/120    avg_loss:0.391, val_acc:0.857]
Epoch [22/120    avg_loss:0.344, val_acc:0.864]
Epoch [23/120    avg_loss:0.312, val_acc:0.864]
Epoch [24/120    avg_loss:0.302, val_acc:0.890]
Epoch [25/120    avg_loss:0.311, val_acc:0.849]
Epoch [26/120    avg_loss:0.297, val_acc:0.887]
Epoch [27/120    avg_loss:0.257, val_acc:0.880]
Epoch [28/120    avg_loss:0.270, val_acc:0.863]
Epoch [29/120    avg_loss:0.225, val_acc:0.899]
Epoch [30/120    avg_loss:0.223, val_acc:0.913]
Epoch [31/120    avg_loss:0.192, val_acc:0.888]
Epoch [32/120    avg_loss:0.222, val_acc:0.902]
Epoch [33/120    avg_loss:0.180, val_acc:0.917]
Epoch [34/120    avg_loss:0.214, val_acc:0.846]
Epoch [35/120    avg_loss:0.321, val_acc:0.894]
Epoch [36/120    avg_loss:0.201, val_acc:0.917]
Epoch [37/120    avg_loss:0.174, val_acc:0.914]
Epoch [38/120    avg_loss:0.159, val_acc:0.930]
Epoch [39/120    avg_loss:0.126, val_acc:0.926]
Epoch [40/120    avg_loss:0.124, val_acc:0.923]
Epoch [41/120    avg_loss:0.127, val_acc:0.930]
Epoch [42/120    avg_loss:0.107, val_acc:0.936]
Epoch [43/120    avg_loss:0.125, val_acc:0.927]
Epoch [44/120    avg_loss:0.122, val_acc:0.928]
Epoch [45/120    avg_loss:0.113, val_acc:0.932]
Epoch [46/120    avg_loss:0.092, val_acc:0.933]
Epoch [47/120    avg_loss:0.118, val_acc:0.941]
Epoch [48/120    avg_loss:0.089, val_acc:0.925]
Epoch [49/120    avg_loss:0.075, val_acc:0.950]
Epoch [50/120    avg_loss:0.073, val_acc:0.939]
Epoch [51/120    avg_loss:0.086, val_acc:0.927]
Epoch [52/120    avg_loss:0.126, val_acc:0.939]
Epoch [53/120    avg_loss:0.128, val_acc:0.938]
Epoch [54/120    avg_loss:0.074, val_acc:0.948]
Epoch [55/120    avg_loss:0.070, val_acc:0.952]
Epoch [56/120    avg_loss:0.068, val_acc:0.945]
Epoch [57/120    avg_loss:0.085, val_acc:0.946]
Epoch [58/120    avg_loss:0.075, val_acc:0.946]
Epoch [59/120    avg_loss:0.066, val_acc:0.963]
Epoch [60/120    avg_loss:0.063, val_acc:0.950]
Epoch [61/120    avg_loss:0.071, val_acc:0.947]
Epoch [62/120    avg_loss:0.076, val_acc:0.942]
Epoch [63/120    avg_loss:0.059, val_acc:0.950]
Epoch [64/120    avg_loss:0.130, val_acc:0.904]
Epoch [65/120    avg_loss:0.124, val_acc:0.932]
Epoch [66/120    avg_loss:0.102, val_acc:0.949]
Epoch [67/120    avg_loss:0.059, val_acc:0.947]
Epoch [68/120    avg_loss:0.046, val_acc:0.959]
Epoch [69/120    avg_loss:0.049, val_acc:0.946]
Epoch [70/120    avg_loss:0.050, val_acc:0.967]
Epoch [71/120    avg_loss:0.043, val_acc:0.962]
Epoch [72/120    avg_loss:0.042, val_acc:0.955]
Epoch [73/120    avg_loss:0.037, val_acc:0.955]
Epoch [74/120    avg_loss:0.047, val_acc:0.961]
Epoch [75/120    avg_loss:0.039, val_acc:0.954]
Epoch [76/120    avg_loss:0.035, val_acc:0.957]
Epoch [77/120    avg_loss:0.054, val_acc:0.961]
Epoch [78/120    avg_loss:0.033, val_acc:0.967]
Epoch [79/120    avg_loss:0.035, val_acc:0.958]
Epoch [80/120    avg_loss:0.032, val_acc:0.960]
Epoch [81/120    avg_loss:0.029, val_acc:0.964]
Epoch [82/120    avg_loss:0.031, val_acc:0.957]
Epoch [83/120    avg_loss:0.026, val_acc:0.962]
Epoch [84/120    avg_loss:0.024, val_acc:0.969]
Epoch [85/120    avg_loss:0.022, val_acc:0.960]
Epoch [86/120    avg_loss:0.052, val_acc:0.939]
Epoch [87/120    avg_loss:0.063, val_acc:0.966]
Epoch [88/120    avg_loss:0.037, val_acc:0.964]
Epoch [89/120    avg_loss:0.030, val_acc:0.964]
Epoch [90/120    avg_loss:0.040, val_acc:0.963]
Epoch [91/120    avg_loss:0.031, val_acc:0.962]
Epoch [92/120    avg_loss:0.021, val_acc:0.974]
Epoch [93/120    avg_loss:0.020, val_acc:0.967]
Epoch [94/120    avg_loss:0.023, val_acc:0.970]
Epoch [95/120    avg_loss:0.028, val_acc:0.970]
Epoch [96/120    avg_loss:0.025, val_acc:0.962]
Epoch [97/120    avg_loss:0.030, val_acc:0.972]
Epoch [98/120    avg_loss:0.028, val_acc:0.967]
Epoch [99/120    avg_loss:0.021, val_acc:0.969]
Epoch [100/120    avg_loss:0.018, val_acc:0.973]
Epoch [101/120    avg_loss:0.020, val_acc:0.970]
Epoch [102/120    avg_loss:0.020, val_acc:0.976]
Epoch [103/120    avg_loss:0.018, val_acc:0.961]
Epoch [104/120    avg_loss:0.026, val_acc:0.957]
Epoch [105/120    avg_loss:0.017, val_acc:0.970]
Epoch [106/120    avg_loss:0.017, val_acc:0.961]
Epoch [107/120    avg_loss:0.017, val_acc:0.966]
Epoch [108/120    avg_loss:0.021, val_acc:0.970]
Epoch [109/120    avg_loss:0.021, val_acc:0.971]
Epoch [110/120    avg_loss:0.016, val_acc:0.972]
Epoch [111/120    avg_loss:0.015, val_acc:0.974]
Epoch [112/120    avg_loss:0.023, val_acc:0.971]
Epoch [113/120    avg_loss:0.020, val_acc:0.972]
Epoch [114/120    avg_loss:0.015, val_acc:0.975]
Epoch [115/120    avg_loss:0.020, val_acc:0.974]
Epoch [116/120    avg_loss:0.011, val_acc:0.975]
Epoch [117/120    avg_loss:0.012, val_acc:0.977]
Epoch [118/120    avg_loss:0.009, val_acc:0.978]
Epoch [119/120    avg_loss:0.011, val_acc:0.978]
Epoch [120/120    avg_loss:0.012, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1253    0    0    1    1    0    0    1    3   25    1    0
     0    0    0]
 [   0    0    0  695    0   11    0    0    0    5    2    0   30    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   10    0    0    3    0    0    0    1  839   14    0    0
     2    6    0]
 [   0    1    5    0    0    0    1    0    4    0    5 2185    8    1
     0    0    0]
 [   0    0    0    0    0    9    0    0    0    0    2    0  516    0
     0    5    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   11    0    0    1    0    1    0    0    0
  1125    1    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
     9  315    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.63685636856368

F1 scores:
[       nan 0.93670886 0.98159029 0.96393897 1.         0.96017699
 0.97904192 1.         0.99421965 0.79069767 0.96938186 0.98534386
 0.94678899 0.98666667 0.98814229 0.9347181  0.98823529]

Kappa:
0.9730609059822068
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2bec4d47f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.738, val_acc:0.256]
Epoch [2/120    avg_loss:2.364, val_acc:0.320]
Epoch [3/120    avg_loss:2.143, val_acc:0.452]
Epoch [4/120    avg_loss:1.965, val_acc:0.489]
Epoch [5/120    avg_loss:1.827, val_acc:0.492]
Epoch [6/120    avg_loss:1.719, val_acc:0.569]
Epoch [7/120    avg_loss:1.556, val_acc:0.598]
Epoch [8/120    avg_loss:1.484, val_acc:0.634]
Epoch [9/120    avg_loss:1.287, val_acc:0.634]
Epoch [10/120    avg_loss:1.192, val_acc:0.627]
Epoch [11/120    avg_loss:1.061, val_acc:0.666]
Epoch [12/120    avg_loss:0.971, val_acc:0.686]
Epoch [13/120    avg_loss:0.935, val_acc:0.692]
Epoch [14/120    avg_loss:0.831, val_acc:0.728]
Epoch [15/120    avg_loss:0.753, val_acc:0.762]
Epoch [16/120    avg_loss:0.661, val_acc:0.762]
Epoch [17/120    avg_loss:0.579, val_acc:0.789]
Epoch [18/120    avg_loss:0.534, val_acc:0.800]
Epoch [19/120    avg_loss:0.473, val_acc:0.781]
Epoch [20/120    avg_loss:0.489, val_acc:0.811]
Epoch [21/120    avg_loss:0.409, val_acc:0.796]
Epoch [22/120    avg_loss:0.499, val_acc:0.838]
Epoch [23/120    avg_loss:0.406, val_acc:0.847]
Epoch [24/120    avg_loss:0.300, val_acc:0.846]
Epoch [25/120    avg_loss:0.294, val_acc:0.869]
Epoch [26/120    avg_loss:0.296, val_acc:0.858]
Epoch [27/120    avg_loss:0.267, val_acc:0.871]
Epoch [28/120    avg_loss:0.251, val_acc:0.889]
Epoch [29/120    avg_loss:0.194, val_acc:0.889]
Epoch [30/120    avg_loss:0.191, val_acc:0.891]
Epoch [31/120    avg_loss:0.224, val_acc:0.908]
Epoch [32/120    avg_loss:0.229, val_acc:0.895]
Epoch [33/120    avg_loss:0.181, val_acc:0.906]
Epoch [34/120    avg_loss:0.165, val_acc:0.928]
Epoch [35/120    avg_loss:0.157, val_acc:0.887]
Epoch [36/120    avg_loss:0.148, val_acc:0.926]
Epoch [37/120    avg_loss:0.123, val_acc:0.913]
Epoch [38/120    avg_loss:0.115, val_acc:0.913]
Epoch [39/120    avg_loss:0.115, val_acc:0.922]
Epoch [40/120    avg_loss:0.103, val_acc:0.926]
Epoch [41/120    avg_loss:0.102, val_acc:0.926]
Epoch [42/120    avg_loss:0.132, val_acc:0.926]
Epoch [43/120    avg_loss:0.098, val_acc:0.919]
Epoch [44/120    avg_loss:0.109, val_acc:0.942]
Epoch [45/120    avg_loss:0.080, val_acc:0.928]
Epoch [46/120    avg_loss:0.090, val_acc:0.948]
Epoch [47/120    avg_loss:0.080, val_acc:0.946]
Epoch [48/120    avg_loss:0.083, val_acc:0.940]
Epoch [49/120    avg_loss:0.078, val_acc:0.939]
Epoch [50/120    avg_loss:0.084, val_acc:0.942]
Epoch [51/120    avg_loss:0.072, val_acc:0.947]
Epoch [52/120    avg_loss:0.076, val_acc:0.908]
Epoch [53/120    avg_loss:0.350, val_acc:0.861]
Epoch [54/120    avg_loss:0.463, val_acc:0.846]
Epoch [55/120    avg_loss:0.263, val_acc:0.867]
Epoch [56/120    avg_loss:0.156, val_acc:0.915]
Epoch [57/120    avg_loss:0.165, val_acc:0.917]
Epoch [58/120    avg_loss:0.123, val_acc:0.904]
Epoch [59/120    avg_loss:0.142, val_acc:0.914]
Epoch [60/120    avg_loss:0.126, val_acc:0.926]
Epoch [61/120    avg_loss:0.105, val_acc:0.931]
Epoch [62/120    avg_loss:0.092, val_acc:0.932]
Epoch [63/120    avg_loss:0.091, val_acc:0.930]
Epoch [64/120    avg_loss:0.080, val_acc:0.939]
Epoch [65/120    avg_loss:0.076, val_acc:0.943]
Epoch [66/120    avg_loss:0.075, val_acc:0.944]
Epoch [67/120    avg_loss:0.068, val_acc:0.945]
Epoch [68/120    avg_loss:0.070, val_acc:0.944]
Epoch [69/120    avg_loss:0.057, val_acc:0.943]
Epoch [70/120    avg_loss:0.061, val_acc:0.945]
Epoch [71/120    avg_loss:0.073, val_acc:0.949]
Epoch [72/120    avg_loss:0.057, val_acc:0.948]
Epoch [73/120    avg_loss:0.059, val_acc:0.950]
Epoch [74/120    avg_loss:0.057, val_acc:0.952]
Epoch [75/120    avg_loss:0.056, val_acc:0.947]
Epoch [76/120    avg_loss:0.057, val_acc:0.946]
Epoch [77/120    avg_loss:0.067, val_acc:0.952]
Epoch [78/120    avg_loss:0.048, val_acc:0.949]
Epoch [79/120    avg_loss:0.049, val_acc:0.948]
Epoch [80/120    avg_loss:0.048, val_acc:0.949]
Epoch [81/120    avg_loss:0.052, val_acc:0.952]
Epoch [82/120    avg_loss:0.055, val_acc:0.949]
Epoch [83/120    avg_loss:0.065, val_acc:0.953]
Epoch [84/120    avg_loss:0.048, val_acc:0.952]
Epoch [85/120    avg_loss:0.051, val_acc:0.954]
Epoch [86/120    avg_loss:0.052, val_acc:0.950]
Epoch [87/120    avg_loss:0.049, val_acc:0.955]
Epoch [88/120    avg_loss:0.040, val_acc:0.949]
Epoch [89/120    avg_loss:0.044, val_acc:0.955]
Epoch [90/120    avg_loss:0.041, val_acc:0.956]
Epoch [91/120    avg_loss:0.048, val_acc:0.954]
Epoch [92/120    avg_loss:0.039, val_acc:0.954]
Epoch [93/120    avg_loss:0.049, val_acc:0.952]
Epoch [94/120    avg_loss:0.045, val_acc:0.956]
Epoch [95/120    avg_loss:0.050, val_acc:0.956]
Epoch [96/120    avg_loss:0.038, val_acc:0.957]
Epoch [97/120    avg_loss:0.038, val_acc:0.955]
Epoch [98/120    avg_loss:0.037, val_acc:0.956]
Epoch [99/120    avg_loss:0.041, val_acc:0.960]
Epoch [100/120    avg_loss:0.042, val_acc:0.954]
Epoch [101/120    avg_loss:0.039, val_acc:0.959]
Epoch [102/120    avg_loss:0.048, val_acc:0.957]
Epoch [103/120    avg_loss:0.040, val_acc:0.961]
Epoch [104/120    avg_loss:0.042, val_acc:0.959]
Epoch [105/120    avg_loss:0.038, val_acc:0.958]
Epoch [106/120    avg_loss:0.042, val_acc:0.958]
Epoch [107/120    avg_loss:0.040, val_acc:0.958]
Epoch [108/120    avg_loss:0.037, val_acc:0.961]
Epoch [109/120    avg_loss:0.040, val_acc:0.961]
Epoch [110/120    avg_loss:0.038, val_acc:0.964]
Epoch [111/120    avg_loss:0.038, val_acc:0.960]
Epoch [112/120    avg_loss:0.034, val_acc:0.959]
Epoch [113/120    avg_loss:0.033, val_acc:0.961]
Epoch [114/120    avg_loss:0.041, val_acc:0.961]
Epoch [115/120    avg_loss:0.039, val_acc:0.957]
Epoch [116/120    avg_loss:0.034, val_acc:0.962]
Epoch [117/120    avg_loss:0.038, val_acc:0.958]
Epoch [118/120    avg_loss:0.039, val_acc:0.962]
Epoch [119/120    avg_loss:0.041, val_acc:0.961]
Epoch [120/120    avg_loss:0.038, val_acc:0.962]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    3    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1235    1    1    0    2    0    0    0   10   36    0    0
     0    0    0]
 [   0    0    0  708    3   10    0    0    0    9    0    1   12    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    4    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  649    0    0    0    0    3    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0    6    0    0    4    5    0    0    5  836   12    0    0
     1    6    0]
 [   0    0    6    1    0    0   11    0    1    4   26 2151    9    0
     1    0    0]
 [   0    0    1    1    0    8    0    0    0    0    1   12  508    0
     0    0    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0   16    0    0    1    0    3    0    0    0
  1119    0    0]
 [   0    0    0    0    0    0   12    0    0    3    0    0    0    0
    27  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.78048780487805

F1 scores:
[       nan 0.94871795 0.97397476 0.97052776 0.99069767 0.94900222
 0.97155689 0.98039216 0.99767981 0.54237288 0.9543379  0.97220339
 0.95488722 0.98659517 0.97558849 0.92705167 0.98245614]

Kappa:
0.9633055860755465
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff08d2be860>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.710, val_acc:0.474]
Epoch [2/120    avg_loss:2.366, val_acc:0.431]
Epoch [3/120    avg_loss:2.142, val_acc:0.527]
Epoch [4/120    avg_loss:1.962, val_acc:0.544]
Epoch [5/120    avg_loss:1.818, val_acc:0.591]
Epoch [6/120    avg_loss:1.694, val_acc:0.593]
Epoch [7/120    avg_loss:1.570, val_acc:0.643]
Epoch [8/120    avg_loss:1.400, val_acc:0.641]
Epoch [9/120    avg_loss:1.260, val_acc:0.653]
Epoch [10/120    avg_loss:1.162, val_acc:0.630]
Epoch [11/120    avg_loss:1.094, val_acc:0.692]
Epoch [12/120    avg_loss:0.964, val_acc:0.742]
Epoch [13/120    avg_loss:0.812, val_acc:0.747]
Epoch [14/120    avg_loss:0.769, val_acc:0.769]
Epoch [15/120    avg_loss:0.682, val_acc:0.768]
Epoch [16/120    avg_loss:0.638, val_acc:0.792]
Epoch [17/120    avg_loss:0.578, val_acc:0.789]
Epoch [18/120    avg_loss:0.502, val_acc:0.834]
Epoch [19/120    avg_loss:0.466, val_acc:0.823]
Epoch [20/120    avg_loss:0.414, val_acc:0.858]
Epoch [21/120    avg_loss:0.422, val_acc:0.832]
Epoch [22/120    avg_loss:0.388, val_acc:0.872]
Epoch [23/120    avg_loss:0.314, val_acc:0.870]
Epoch [24/120    avg_loss:0.304, val_acc:0.863]
Epoch [25/120    avg_loss:0.284, val_acc:0.859]
Epoch [26/120    avg_loss:0.290, val_acc:0.911]
Epoch [27/120    avg_loss:0.263, val_acc:0.872]
Epoch [28/120    avg_loss:0.252, val_acc:0.885]
Epoch [29/120    avg_loss:0.197, val_acc:0.888]
Epoch [30/120    avg_loss:0.259, val_acc:0.894]
Epoch [31/120    avg_loss:0.197, val_acc:0.923]
Epoch [32/120    avg_loss:0.204, val_acc:0.892]
Epoch [33/120    avg_loss:0.209, val_acc:0.920]
Epoch [34/120    avg_loss:0.184, val_acc:0.929]
Epoch [35/120    avg_loss:0.191, val_acc:0.919]
Epoch [36/120    avg_loss:0.202, val_acc:0.909]
Epoch [37/120    avg_loss:0.141, val_acc:0.927]
Epoch [38/120    avg_loss:0.113, val_acc:0.942]
Epoch [39/120    avg_loss:0.121, val_acc:0.932]
Epoch [40/120    avg_loss:0.123, val_acc:0.935]
Epoch [41/120    avg_loss:0.109, val_acc:0.938]
Epoch [42/120    avg_loss:0.100, val_acc:0.934]
Epoch [43/120    avg_loss:0.092, val_acc:0.944]
Epoch [44/120    avg_loss:0.118, val_acc:0.938]
Epoch [45/120    avg_loss:0.113, val_acc:0.935]
Epoch [46/120    avg_loss:0.106, val_acc:0.939]
Epoch [47/120    avg_loss:0.099, val_acc:0.933]
Epoch [48/120    avg_loss:0.074, val_acc:0.952]
Epoch [49/120    avg_loss:0.072, val_acc:0.960]
Epoch [50/120    avg_loss:0.072, val_acc:0.950]
Epoch [51/120    avg_loss:0.074, val_acc:0.950]
Epoch [52/120    avg_loss:0.094, val_acc:0.911]
Epoch [53/120    avg_loss:0.107, val_acc:0.946]
Epoch [54/120    avg_loss:0.082, val_acc:0.946]
Epoch [55/120    avg_loss:0.095, val_acc:0.945]
Epoch [56/120    avg_loss:0.084, val_acc:0.946]
Epoch [57/120    avg_loss:0.054, val_acc:0.963]
Epoch [58/120    avg_loss:0.065, val_acc:0.932]
Epoch [59/120    avg_loss:0.078, val_acc:0.947]
Epoch [60/120    avg_loss:0.059, val_acc:0.954]
Epoch [61/120    avg_loss:0.049, val_acc:0.966]
Epoch [62/120    avg_loss:0.045, val_acc:0.963]
Epoch [63/120    avg_loss:0.046, val_acc:0.959]
Epoch [64/120    avg_loss:0.047, val_acc:0.972]
Epoch [65/120    avg_loss:0.041, val_acc:0.959]
Epoch [66/120    avg_loss:0.045, val_acc:0.962]
Epoch [67/120    avg_loss:0.039, val_acc:0.956]
Epoch [68/120    avg_loss:0.048, val_acc:0.966]
Epoch [69/120    avg_loss:0.050, val_acc:0.961]
Epoch [70/120    avg_loss:0.056, val_acc:0.956]
Epoch [71/120    avg_loss:0.042, val_acc:0.967]
Epoch [72/120    avg_loss:0.029, val_acc:0.970]
Epoch [73/120    avg_loss:0.035, val_acc:0.969]
Epoch [74/120    avg_loss:0.033, val_acc:0.963]
Epoch [75/120    avg_loss:0.043, val_acc:0.964]
Epoch [76/120    avg_loss:0.037, val_acc:0.963]
Epoch [77/120    avg_loss:0.042, val_acc:0.959]
Epoch [78/120    avg_loss:0.035, val_acc:0.972]
Epoch [79/120    avg_loss:0.024, val_acc:0.975]
Epoch [80/120    avg_loss:0.022, val_acc:0.975]
Epoch [81/120    avg_loss:0.022, val_acc:0.977]
Epoch [82/120    avg_loss:0.026, val_acc:0.974]
Epoch [83/120    avg_loss:0.021, val_acc:0.977]
Epoch [84/120    avg_loss:0.021, val_acc:0.976]
Epoch [85/120    avg_loss:0.023, val_acc:0.976]
Epoch [86/120    avg_loss:0.020, val_acc:0.977]
Epoch [87/120    avg_loss:0.022, val_acc:0.976]
Epoch [88/120    avg_loss:0.017, val_acc:0.976]
Epoch [89/120    avg_loss:0.020, val_acc:0.977]
Epoch [90/120    avg_loss:0.017, val_acc:0.976]
Epoch [91/120    avg_loss:0.019, val_acc:0.980]
Epoch [92/120    avg_loss:0.017, val_acc:0.980]
Epoch [93/120    avg_loss:0.019, val_acc:0.981]
Epoch [94/120    avg_loss:0.015, val_acc:0.981]
Epoch [95/120    avg_loss:0.019, val_acc:0.980]
Epoch [96/120    avg_loss:0.020, val_acc:0.983]
Epoch [97/120    avg_loss:0.018, val_acc:0.980]
Epoch [98/120    avg_loss:0.015, val_acc:0.982]
Epoch [99/120    avg_loss:0.018, val_acc:0.981]
Epoch [100/120    avg_loss:0.021, val_acc:0.977]
Epoch [101/120    avg_loss:0.020, val_acc:0.976]
Epoch [102/120    avg_loss:0.019, val_acc:0.980]
Epoch [103/120    avg_loss:0.019, val_acc:0.977]
Epoch [104/120    avg_loss:0.021, val_acc:0.977]
Epoch [105/120    avg_loss:0.021, val_acc:0.980]
Epoch [106/120    avg_loss:0.018, val_acc:0.978]
Epoch [107/120    avg_loss:0.016, val_acc:0.978]
Epoch [108/120    avg_loss:0.017, val_acc:0.980]
Epoch [109/120    avg_loss:0.019, val_acc:0.981]
Epoch [110/120    avg_loss:0.019, val_acc:0.981]
Epoch [111/120    avg_loss:0.013, val_acc:0.981]
Epoch [112/120    avg_loss:0.014, val_acc:0.981]
Epoch [113/120    avg_loss:0.017, val_acc:0.981]
Epoch [114/120    avg_loss:0.018, val_acc:0.981]
Epoch [115/120    avg_loss:0.018, val_acc:0.981]
Epoch [116/120    avg_loss:0.017, val_acc:0.981]
Epoch [117/120    avg_loss:0.018, val_acc:0.981]
Epoch [118/120    avg_loss:0.019, val_acc:0.981]
Epoch [119/120    avg_loss:0.017, val_acc:0.982]
Epoch [120/120    avg_loss:0.017, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1242    0    1    0    2    0    0    0   14   26    0    0
     0    0    0]
 [   0    0    1  704    2   13    0    0    0    5    4    0   18    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    1    0    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   14    0    0    4    0
     0    0    0]
 [   0    0    2    0    0    9    0    0    0    0  855    3    0    0
     2    4    0]
 [   0    0    9    0    0    0    2    1    1    0   16 2176    2    2
     1    0    0]
 [   0    0    0    0    0   10    0    0    0    0    0    7  511    0
     2    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1135    3    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    10  326    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.78861788617886

F1 scores:
[       nan 0.94871795 0.97833793 0.97036527 0.99300699 0.95875139
 0.98568199 0.98039216 0.99883856 0.68292683 0.96664782 0.98350282
 0.95603368 0.99462366 0.99169943 0.95882353 0.97674419]

Kappa:
0.974792985433938
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd878b6d7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.726, val_acc:0.342]
Epoch [2/120    avg_loss:2.334, val_acc:0.456]
Epoch [3/120    avg_loss:2.154, val_acc:0.512]
Epoch [4/120    avg_loss:1.976, val_acc:0.508]
Epoch [5/120    avg_loss:1.888, val_acc:0.556]
Epoch [6/120    avg_loss:1.755, val_acc:0.605]
Epoch [7/120    avg_loss:1.671, val_acc:0.608]
Epoch [8/120    avg_loss:1.503, val_acc:0.638]
Epoch [9/120    avg_loss:1.329, val_acc:0.633]
Epoch [10/120    avg_loss:1.153, val_acc:0.680]
Epoch [11/120    avg_loss:1.057, val_acc:0.692]
Epoch [12/120    avg_loss:0.927, val_acc:0.704]
Epoch [13/120    avg_loss:0.907, val_acc:0.733]
Epoch [14/120    avg_loss:0.795, val_acc:0.721]
Epoch [15/120    avg_loss:0.709, val_acc:0.764]
Epoch [16/120    avg_loss:0.644, val_acc:0.768]
Epoch [17/120    avg_loss:0.817, val_acc:0.779]
Epoch [18/120    avg_loss:0.681, val_acc:0.802]
Epoch [19/120    avg_loss:0.582, val_acc:0.803]
Epoch [20/120    avg_loss:0.472, val_acc:0.831]
Epoch [21/120    avg_loss:0.439, val_acc:0.852]
Epoch [22/120    avg_loss:0.414, val_acc:0.828]
Epoch [23/120    avg_loss:0.435, val_acc:0.825]
Epoch [24/120    avg_loss:0.412, val_acc:0.871]
Epoch [25/120    avg_loss:0.376, val_acc:0.873]
Epoch [26/120    avg_loss:0.277, val_acc:0.900]
Epoch [27/120    avg_loss:0.277, val_acc:0.873]
Epoch [28/120    avg_loss:0.265, val_acc:0.880]
Epoch [29/120    avg_loss:0.257, val_acc:0.884]
Epoch [30/120    avg_loss:0.243, val_acc:0.909]
Epoch [31/120    avg_loss:0.200, val_acc:0.909]
Epoch [32/120    avg_loss:0.202, val_acc:0.904]
Epoch [33/120    avg_loss:0.219, val_acc:0.843]
Epoch [34/120    avg_loss:0.288, val_acc:0.899]
Epoch [35/120    avg_loss:0.203, val_acc:0.918]
Epoch [36/120    avg_loss:0.174, val_acc:0.931]
Epoch [37/120    avg_loss:0.155, val_acc:0.925]
Epoch [38/120    avg_loss:0.165, val_acc:0.912]
Epoch [39/120    avg_loss:0.163, val_acc:0.929]
Epoch [40/120    avg_loss:0.139, val_acc:0.927]
Epoch [41/120    avg_loss:0.123, val_acc:0.929]
Epoch [42/120    avg_loss:0.114, val_acc:0.939]
Epoch [43/120    avg_loss:0.165, val_acc:0.932]
Epoch [44/120    avg_loss:0.119, val_acc:0.931]
Epoch [45/120    avg_loss:0.100, val_acc:0.944]
Epoch [46/120    avg_loss:0.100, val_acc:0.931]
Epoch [47/120    avg_loss:0.108, val_acc:0.940]
Epoch [48/120    avg_loss:0.104, val_acc:0.950]
Epoch [49/120    avg_loss:0.096, val_acc:0.933]
Epoch [50/120    avg_loss:0.112, val_acc:0.927]
Epoch [51/120    avg_loss:0.095, val_acc:0.917]
Epoch [52/120    avg_loss:0.122, val_acc:0.943]
Epoch [53/120    avg_loss:0.178, val_acc:0.841]
Epoch [54/120    avg_loss:0.217, val_acc:0.923]
Epoch [55/120    avg_loss:0.114, val_acc:0.909]
Epoch [56/120    avg_loss:0.126, val_acc:0.941]
Epoch [57/120    avg_loss:0.079, val_acc:0.914]
Epoch [58/120    avg_loss:0.082, val_acc:0.912]
Epoch [59/120    avg_loss:0.071, val_acc:0.947]
Epoch [60/120    avg_loss:0.073, val_acc:0.952]
Epoch [61/120    avg_loss:0.057, val_acc:0.950]
Epoch [62/120    avg_loss:0.056, val_acc:0.956]
Epoch [63/120    avg_loss:0.047, val_acc:0.958]
Epoch [64/120    avg_loss:0.047, val_acc:0.945]
Epoch [65/120    avg_loss:0.060, val_acc:0.954]
Epoch [66/120    avg_loss:0.050, val_acc:0.956]
Epoch [67/120    avg_loss:0.051, val_acc:0.944]
Epoch [68/120    avg_loss:0.061, val_acc:0.958]
Epoch [69/120    avg_loss:0.046, val_acc:0.950]
Epoch [70/120    avg_loss:0.065, val_acc:0.950]
Epoch [71/120    avg_loss:0.056, val_acc:0.959]
Epoch [72/120    avg_loss:0.045, val_acc:0.960]
Epoch [73/120    avg_loss:0.041, val_acc:0.962]
Epoch [74/120    avg_loss:0.036, val_acc:0.957]
Epoch [75/120    avg_loss:0.029, val_acc:0.964]
Epoch [76/120    avg_loss:0.025, val_acc:0.967]
Epoch [77/120    avg_loss:0.020, val_acc:0.966]
Epoch [78/120    avg_loss:0.030, val_acc:0.961]
Epoch [79/120    avg_loss:0.040, val_acc:0.957]
Epoch [80/120    avg_loss:0.035, val_acc:0.964]
Epoch [81/120    avg_loss:0.035, val_acc:0.962]
Epoch [82/120    avg_loss:0.028, val_acc:0.971]
Epoch [83/120    avg_loss:0.027, val_acc:0.959]
Epoch [84/120    avg_loss:0.042, val_acc:0.967]
Epoch [85/120    avg_loss:0.040, val_acc:0.945]
Epoch [86/120    avg_loss:0.039, val_acc:0.967]
Epoch [87/120    avg_loss:0.058, val_acc:0.954]
Epoch [88/120    avg_loss:0.031, val_acc:0.969]
Epoch [89/120    avg_loss:0.025, val_acc:0.966]
Epoch [90/120    avg_loss:0.027, val_acc:0.963]
Epoch [91/120    avg_loss:0.027, val_acc:0.950]
Epoch [92/120    avg_loss:0.022, val_acc:0.966]
Epoch [93/120    avg_loss:0.021, val_acc:0.968]
Epoch [94/120    avg_loss:0.017, val_acc:0.974]
Epoch [95/120    avg_loss:0.017, val_acc:0.971]
Epoch [96/120    avg_loss:0.017, val_acc:0.969]
Epoch [97/120    avg_loss:0.017, val_acc:0.969]
Epoch [98/120    avg_loss:0.022, val_acc:0.964]
Epoch [99/120    avg_loss:0.025, val_acc:0.966]
Epoch [100/120    avg_loss:0.022, val_acc:0.976]
Epoch [101/120    avg_loss:0.022, val_acc:0.966]
Epoch [102/120    avg_loss:0.022, val_acc:0.964]
Epoch [103/120    avg_loss:0.029, val_acc:0.980]
Epoch [104/120    avg_loss:0.018, val_acc:0.978]
Epoch [105/120    avg_loss:0.018, val_acc:0.961]
Epoch [106/120    avg_loss:0.019, val_acc:0.976]
Epoch [107/120    avg_loss:0.023, val_acc:0.970]
Epoch [108/120    avg_loss:0.015, val_acc:0.970]
Epoch [109/120    avg_loss:0.018, val_acc:0.974]
Epoch [110/120    avg_loss:0.016, val_acc:0.975]
Epoch [111/120    avg_loss:0.018, val_acc:0.969]
Epoch [112/120    avg_loss:0.017, val_acc:0.969]
Epoch [113/120    avg_loss:0.015, val_acc:0.975]
Epoch [114/120    avg_loss:0.013, val_acc:0.971]
Epoch [115/120    avg_loss:0.014, val_acc:0.975]
Epoch [116/120    avg_loss:0.027, val_acc:0.967]
Epoch [117/120    avg_loss:0.019, val_acc:0.972]
Epoch [118/120    avg_loss:0.014, val_acc:0.975]
Epoch [119/120    avg_loss:0.011, val_acc:0.976]
Epoch [120/120    avg_loss:0.016, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    2    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1256    1    0    0    4    0    0    1    3   18    2    0
     0    0    0]
 [   0    0    0  688    1   15    0    0    0    9    4    2   28    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    3    0    0    0    0  866    2    1    0
     0    1    0]
 [   0    0   10    0    0    0    4    0    0    0   29 2160    6    1
     0    0    0]
 [   0    0    3    1    0    0    0    0    0    0    2    2  525    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    0    0    1    0    0    0
  1126    2    0]
 [   0    0    0    0    0    1   11    0    0    1    0    0    0    0
    14  320    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.72357723577235

F1 scores:
[       nan 0.9382716  0.9820172  0.95688456 0.99765808 0.96312849
 0.98345865 1.         0.997669   0.70833333 0.97248737 0.98315885
 0.95715588 0.99730458 0.98598949 0.95522388 0.98809524]

Kappa:
0.9740649530315597
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fea6e6de908>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.682, val_acc:0.239]
Epoch [2/120    avg_loss:2.367, val_acc:0.343]
Epoch [3/120    avg_loss:2.170, val_acc:0.519]
Epoch [4/120    avg_loss:2.020, val_acc:0.550]
Epoch [5/120    avg_loss:1.862, val_acc:0.558]
Epoch [6/120    avg_loss:1.679, val_acc:0.596]
Epoch [7/120    avg_loss:1.596, val_acc:0.619]
Epoch [8/120    avg_loss:1.472, val_acc:0.629]
Epoch [9/120    avg_loss:1.361, val_acc:0.609]
Epoch [10/120    avg_loss:1.232, val_acc:0.643]
Epoch [11/120    avg_loss:1.157, val_acc:0.643]
Epoch [12/120    avg_loss:1.026, val_acc:0.689]
Epoch [13/120    avg_loss:1.002, val_acc:0.685]
Epoch [14/120    avg_loss:0.860, val_acc:0.734]
Epoch [15/120    avg_loss:0.786, val_acc:0.745]
Epoch [16/120    avg_loss:0.706, val_acc:0.766]
Epoch [17/120    avg_loss:0.627, val_acc:0.794]
Epoch [18/120    avg_loss:0.577, val_acc:0.792]
Epoch [19/120    avg_loss:0.582, val_acc:0.798]
Epoch [20/120    avg_loss:0.492, val_acc:0.827]
Epoch [21/120    avg_loss:0.446, val_acc:0.848]
Epoch [22/120    avg_loss:0.424, val_acc:0.841]
Epoch [23/120    avg_loss:0.379, val_acc:0.874]
Epoch [24/120    avg_loss:0.331, val_acc:0.862]
Epoch [25/120    avg_loss:0.328, val_acc:0.884]
Epoch [26/120    avg_loss:0.266, val_acc:0.829]
Epoch [27/120    avg_loss:0.286, val_acc:0.888]
Epoch [28/120    avg_loss:0.380, val_acc:0.775]
Epoch [29/120    avg_loss:0.355, val_acc:0.871]
Epoch [30/120    avg_loss:0.327, val_acc:0.846]
Epoch [31/120    avg_loss:0.223, val_acc:0.899]
Epoch [32/120    avg_loss:0.202, val_acc:0.886]
Epoch [33/120    avg_loss:0.186, val_acc:0.900]
Epoch [34/120    avg_loss:0.190, val_acc:0.909]
Epoch [35/120    avg_loss:0.199, val_acc:0.913]
Epoch [36/120    avg_loss:0.172, val_acc:0.918]
Epoch [37/120    avg_loss:0.142, val_acc:0.922]
Epoch [38/120    avg_loss:0.154, val_acc:0.913]
Epoch [39/120    avg_loss:0.161, val_acc:0.930]
Epoch [40/120    avg_loss:0.145, val_acc:0.930]
Epoch [41/120    avg_loss:0.124, val_acc:0.936]
Epoch [42/120    avg_loss:0.130, val_acc:0.943]
Epoch [43/120    avg_loss:0.121, val_acc:0.932]
Epoch [44/120    avg_loss:0.120, val_acc:0.932]
Epoch [45/120    avg_loss:0.118, val_acc:0.934]
Epoch [46/120    avg_loss:0.102, val_acc:0.932]
Epoch [47/120    avg_loss:0.099, val_acc:0.925]
Epoch [48/120    avg_loss:0.092, val_acc:0.920]
Epoch [49/120    avg_loss:0.081, val_acc:0.944]
Epoch [50/120    avg_loss:0.076, val_acc:0.957]
Epoch [51/120    avg_loss:0.084, val_acc:0.947]
Epoch [52/120    avg_loss:0.109, val_acc:0.945]
Epoch [53/120    avg_loss:0.103, val_acc:0.946]
Epoch [54/120    avg_loss:0.071, val_acc:0.960]
Epoch [55/120    avg_loss:0.073, val_acc:0.953]
Epoch [56/120    avg_loss:0.073, val_acc:0.957]
Epoch [57/120    avg_loss:0.066, val_acc:0.948]
Epoch [58/120    avg_loss:0.064, val_acc:0.953]
Epoch [59/120    avg_loss:0.067, val_acc:0.957]
Epoch [60/120    avg_loss:0.063, val_acc:0.952]
Epoch [61/120    avg_loss:0.046, val_acc:0.963]
Epoch [62/120    avg_loss:0.054, val_acc:0.950]
Epoch [63/120    avg_loss:0.084, val_acc:0.944]
Epoch [64/120    avg_loss:0.051, val_acc:0.952]
Epoch [65/120    avg_loss:0.045, val_acc:0.957]
Epoch [66/120    avg_loss:0.054, val_acc:0.961]
Epoch [67/120    avg_loss:0.045, val_acc:0.941]
Epoch [68/120    avg_loss:0.048, val_acc:0.959]
Epoch [69/120    avg_loss:0.057, val_acc:0.961]
Epoch [70/120    avg_loss:0.042, val_acc:0.957]
Epoch [71/120    avg_loss:0.036, val_acc:0.961]
Epoch [72/120    avg_loss:0.027, val_acc:0.964]
Epoch [73/120    avg_loss:0.047, val_acc:0.964]
Epoch [74/120    avg_loss:0.035, val_acc:0.964]
Epoch [75/120    avg_loss:0.038, val_acc:0.962]
Epoch [76/120    avg_loss:0.032, val_acc:0.966]
Epoch [77/120    avg_loss:0.025, val_acc:0.970]
Epoch [78/120    avg_loss:0.027, val_acc:0.966]
Epoch [79/120    avg_loss:0.046, val_acc:0.966]
Epoch [80/120    avg_loss:0.038, val_acc:0.973]
Epoch [81/120    avg_loss:0.032, val_acc:0.972]
Epoch [82/120    avg_loss:0.032, val_acc:0.961]
Epoch [83/120    avg_loss:0.029, val_acc:0.969]
Epoch [84/120    avg_loss:0.025, val_acc:0.964]
Epoch [85/120    avg_loss:0.029, val_acc:0.964]
Epoch [86/120    avg_loss:0.023, val_acc:0.972]
Epoch [87/120    avg_loss:0.024, val_acc:0.973]
Epoch [88/120    avg_loss:0.015, val_acc:0.975]
Epoch [89/120    avg_loss:0.017, val_acc:0.975]
Epoch [90/120    avg_loss:0.016, val_acc:0.969]
Epoch [91/120    avg_loss:0.019, val_acc:0.971]
Epoch [92/120    avg_loss:0.016, val_acc:0.980]
Epoch [93/120    avg_loss:0.013, val_acc:0.973]
Epoch [94/120    avg_loss:0.014, val_acc:0.971]
Epoch [95/120    avg_loss:0.015, val_acc:0.971]
Epoch [96/120    avg_loss:0.018, val_acc:0.973]
Epoch [97/120    avg_loss:0.019, val_acc:0.981]
Epoch [98/120    avg_loss:0.017, val_acc:0.976]
Epoch [99/120    avg_loss:0.018, val_acc:0.969]
Epoch [100/120    avg_loss:0.017, val_acc:0.974]
Epoch [101/120    avg_loss:0.014, val_acc:0.977]
Epoch [102/120    avg_loss:0.016, val_acc:0.976]
Epoch [103/120    avg_loss:0.017, val_acc:0.972]
Epoch [104/120    avg_loss:0.016, val_acc:0.973]
Epoch [105/120    avg_loss:0.024, val_acc:0.952]
Epoch [106/120    avg_loss:0.027, val_acc:0.970]
Epoch [107/120    avg_loss:0.016, val_acc:0.978]
Epoch [108/120    avg_loss:0.020, val_acc:0.975]
Epoch [109/120    avg_loss:0.015, val_acc:0.975]
Epoch [110/120    avg_loss:0.013, val_acc:0.971]
Epoch [111/120    avg_loss:0.012, val_acc:0.975]
Epoch [112/120    avg_loss:0.010, val_acc:0.975]
Epoch [113/120    avg_loss:0.009, val_acc:0.976]
Epoch [114/120    avg_loss:0.010, val_acc:0.978]
Epoch [115/120    avg_loss:0.008, val_acc:0.978]
Epoch [116/120    avg_loss:0.008, val_acc:0.978]
Epoch [117/120    avg_loss:0.008, val_acc:0.978]
Epoch [118/120    avg_loss:0.008, val_acc:0.978]
Epoch [119/120    avg_loss:0.009, val_acc:0.978]
Epoch [120/120    avg_loss:0.008, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1252    2    0    0    1    0    0    1    7   19    1    0
     0    2    0]
 [   0    0    0  725    0    2    0    0    0    5    1    2    7    5
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    0    1
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    7    0    0    5    2    0    0    2  846   12    0    0
     0    1    0]
 [   0    0    6    0    0    0    1    0    0    0    9 2193    0    1
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0   10    3  513    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   19    0    0    1    0    3    0    0    0
  1116    0    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    21  320    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.03794037940379

F1 scores:
[       nan 0.96202532 0.98196078 0.98371777 0.99764706 0.96329255
 0.99092284 1.         0.99883856 0.75555556 0.96465222 0.98783784
 0.97159091 0.98143236 0.98023715 0.95522388 0.98245614]

Kappa:
0.9776274841485053
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc5071347b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.678, val_acc:0.341]
Epoch [2/120    avg_loss:2.322, val_acc:0.453]
Epoch [3/120    avg_loss:2.116, val_acc:0.509]
Epoch [4/120    avg_loss:1.991, val_acc:0.554]
Epoch [5/120    avg_loss:1.817, val_acc:0.611]
Epoch [6/120    avg_loss:1.703, val_acc:0.642]
Epoch [7/120    avg_loss:1.525, val_acc:0.673]
Epoch [8/120    avg_loss:1.385, val_acc:0.683]
Epoch [9/120    avg_loss:1.296, val_acc:0.668]
Epoch [10/120    avg_loss:1.114, val_acc:0.695]
Epoch [11/120    avg_loss:1.007, val_acc:0.714]
Epoch [12/120    avg_loss:0.932, val_acc:0.719]
Epoch [13/120    avg_loss:0.879, val_acc:0.732]
Epoch [14/120    avg_loss:0.759, val_acc:0.797]
Epoch [15/120    avg_loss:0.723, val_acc:0.791]
Epoch [16/120    avg_loss:0.611, val_acc:0.806]
Epoch [17/120    avg_loss:0.605, val_acc:0.805]
Epoch [18/120    avg_loss:0.623, val_acc:0.796]
Epoch [19/120    avg_loss:0.539, val_acc:0.812]
Epoch [20/120    avg_loss:0.488, val_acc:0.852]
Epoch [21/120    avg_loss:0.432, val_acc:0.815]
Epoch [22/120    avg_loss:0.395, val_acc:0.861]
Epoch [23/120    avg_loss:0.401, val_acc:0.839]
Epoch [24/120    avg_loss:0.370, val_acc:0.867]
Epoch [25/120    avg_loss:0.338, val_acc:0.876]
Epoch [26/120    avg_loss:0.326, val_acc:0.901]
Epoch [27/120    avg_loss:0.324, val_acc:0.894]
Epoch [28/120    avg_loss:0.262, val_acc:0.903]
Epoch [29/120    avg_loss:0.233, val_acc:0.912]
Epoch [30/120    avg_loss:0.202, val_acc:0.919]
Epoch [31/120    avg_loss:0.234, val_acc:0.903]
Epoch [32/120    avg_loss:0.241, val_acc:0.899]
Epoch [33/120    avg_loss:0.226, val_acc:0.865]
Epoch [34/120    avg_loss:0.275, val_acc:0.913]
Epoch [35/120    avg_loss:0.218, val_acc:0.889]
Epoch [36/120    avg_loss:0.224, val_acc:0.906]
Epoch [37/120    avg_loss:0.194, val_acc:0.926]
Epoch [38/120    avg_loss:0.160, val_acc:0.914]
Epoch [39/120    avg_loss:0.152, val_acc:0.933]
Epoch [40/120    avg_loss:0.171, val_acc:0.893]
Epoch [41/120    avg_loss:0.164, val_acc:0.927]
Epoch [42/120    avg_loss:0.157, val_acc:0.935]
Epoch [43/120    avg_loss:0.137, val_acc:0.932]
Epoch [44/120    avg_loss:0.123, val_acc:0.918]
Epoch [45/120    avg_loss:0.121, val_acc:0.922]
Epoch [46/120    avg_loss:0.101, val_acc:0.935]
Epoch [47/120    avg_loss:0.098, val_acc:0.955]
Epoch [48/120    avg_loss:0.107, val_acc:0.922]
Epoch [49/120    avg_loss:0.108, val_acc:0.953]
Epoch [50/120    avg_loss:0.103, val_acc:0.936]
Epoch [51/120    avg_loss:0.102, val_acc:0.929]
Epoch [52/120    avg_loss:0.075, val_acc:0.955]
Epoch [53/120    avg_loss:0.077, val_acc:0.932]
Epoch [54/120    avg_loss:0.065, val_acc:0.947]
Epoch [55/120    avg_loss:0.078, val_acc:0.936]
Epoch [56/120    avg_loss:0.078, val_acc:0.947]
Epoch [57/120    avg_loss:0.058, val_acc:0.962]
Epoch [58/120    avg_loss:0.064, val_acc:0.953]
Epoch [59/120    avg_loss:0.056, val_acc:0.958]
Epoch [60/120    avg_loss:0.065, val_acc:0.956]
Epoch [61/120    avg_loss:0.053, val_acc:0.956]
Epoch [62/120    avg_loss:0.051, val_acc:0.961]
Epoch [63/120    avg_loss:0.050, val_acc:0.959]
Epoch [64/120    avg_loss:0.060, val_acc:0.957]
Epoch [65/120    avg_loss:0.086, val_acc:0.946]
Epoch [66/120    avg_loss:0.143, val_acc:0.921]
Epoch [67/120    avg_loss:0.135, val_acc:0.927]
Epoch [68/120    avg_loss:0.077, val_acc:0.950]
Epoch [69/120    avg_loss:0.070, val_acc:0.939]
Epoch [70/120    avg_loss:0.053, val_acc:0.959]
Epoch [71/120    avg_loss:0.042, val_acc:0.964]
Epoch [72/120    avg_loss:0.031, val_acc:0.969]
Epoch [73/120    avg_loss:0.032, val_acc:0.967]
Epoch [74/120    avg_loss:0.028, val_acc:0.967]
Epoch [75/120    avg_loss:0.029, val_acc:0.966]
Epoch [76/120    avg_loss:0.029, val_acc:0.964]
Epoch [77/120    avg_loss:0.033, val_acc:0.966]
Epoch [78/120    avg_loss:0.033, val_acc:0.969]
Epoch [79/120    avg_loss:0.032, val_acc:0.970]
Epoch [80/120    avg_loss:0.028, val_acc:0.969]
Epoch [81/120    avg_loss:0.025, val_acc:0.967]
Epoch [82/120    avg_loss:0.026, val_acc:0.966]
Epoch [83/120    avg_loss:0.026, val_acc:0.966]
Epoch [84/120    avg_loss:0.032, val_acc:0.970]
Epoch [85/120    avg_loss:0.026, val_acc:0.969]
Epoch [86/120    avg_loss:0.025, val_acc:0.970]
Epoch [87/120    avg_loss:0.027, val_acc:0.968]
Epoch [88/120    avg_loss:0.028, val_acc:0.971]
Epoch [89/120    avg_loss:0.024, val_acc:0.969]
Epoch [90/120    avg_loss:0.025, val_acc:0.967]
Epoch [91/120    avg_loss:0.024, val_acc:0.967]
Epoch [92/120    avg_loss:0.030, val_acc:0.967]
Epoch [93/120    avg_loss:0.026, val_acc:0.968]
Epoch [94/120    avg_loss:0.029, val_acc:0.970]
Epoch [95/120    avg_loss:0.024, val_acc:0.971]
Epoch [96/120    avg_loss:0.026, val_acc:0.969]
Epoch [97/120    avg_loss:0.024, val_acc:0.969]
Epoch [98/120    avg_loss:0.025, val_acc:0.970]
Epoch [99/120    avg_loss:0.025, val_acc:0.970]
Epoch [100/120    avg_loss:0.024, val_acc:0.969]
Epoch [101/120    avg_loss:0.029, val_acc:0.971]
Epoch [102/120    avg_loss:0.025, val_acc:0.968]
Epoch [103/120    avg_loss:0.024, val_acc:0.970]
Epoch [104/120    avg_loss:0.021, val_acc:0.968]
Epoch [105/120    avg_loss:0.022, val_acc:0.968]
Epoch [106/120    avg_loss:0.023, val_acc:0.970]
Epoch [107/120    avg_loss:0.021, val_acc:0.970]
Epoch [108/120    avg_loss:0.022, val_acc:0.969]
Epoch [109/120    avg_loss:0.023, val_acc:0.968]
Epoch [110/120    avg_loss:0.023, val_acc:0.968]
Epoch [111/120    avg_loss:0.027, val_acc:0.967]
Epoch [112/120    avg_loss:0.028, val_acc:0.966]
Epoch [113/120    avg_loss:0.022, val_acc:0.968]
Epoch [114/120    avg_loss:0.023, val_acc:0.969]
Epoch [115/120    avg_loss:0.025, val_acc:0.969]
Epoch [116/120    avg_loss:0.021, val_acc:0.969]
Epoch [117/120    avg_loss:0.021, val_acc:0.969]
Epoch [118/120    avg_loss:0.021, val_acc:0.969]
Epoch [119/120    avg_loss:0.021, val_acc:0.969]
Epoch [120/120    avg_loss:0.022, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1252    0    0    0    0    0    0    0    7   25    0    0
     1    0    0]
 [   0    0    1  690    4   10    0    0    0    8    5    0   29    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    4    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    8    0    0    0    0  827   20    0    0
     3    8    0]
 [   0    0    4    0    0    2    2    0    0    0   20 2171    6    3
     2    0    0]
 [   0    0    0    2    0   13    0    0    0    0    0   11  504    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   13    0    0    1    0    3    0    0    0
  1119    3    0]
 [   0    0    0    0    0    0    2    0    0    9    0    0    0    0
    42  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.83468834688347

F1 scores:
[       nan 0.975      0.98157585 0.95899931 0.99069767 0.94285714
 0.99467681 0.92592593 0.99883856 0.66666667 0.95112133 0.97792793
 0.93854749 0.9919571  0.96967071 0.90184049 0.97647059]

Kappa:
0.9639066357644117
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff554859ba8>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.664, val_acc:0.261]
Epoch [2/120    avg_loss:2.336, val_acc:0.388]
Epoch [3/120    avg_loss:2.114, val_acc:0.530]
Epoch [4/120    avg_loss:1.975, val_acc:0.557]
Epoch [5/120    avg_loss:1.823, val_acc:0.573]
Epoch [6/120    avg_loss:1.661, val_acc:0.575]
Epoch [7/120    avg_loss:1.610, val_acc:0.606]
Epoch [8/120    avg_loss:1.436, val_acc:0.630]
Epoch [9/120    avg_loss:1.310, val_acc:0.667]
Epoch [10/120    avg_loss:1.200, val_acc:0.698]
Epoch [11/120    avg_loss:1.059, val_acc:0.710]
Epoch [12/120    avg_loss:0.968, val_acc:0.725]
Epoch [13/120    avg_loss:0.871, val_acc:0.726]
Epoch [14/120    avg_loss:0.754, val_acc:0.730]
Epoch [15/120    avg_loss:0.685, val_acc:0.758]
Epoch [16/120    avg_loss:0.718, val_acc:0.766]
Epoch [17/120    avg_loss:0.658, val_acc:0.752]
Epoch [18/120    avg_loss:0.548, val_acc:0.792]
Epoch [19/120    avg_loss:0.502, val_acc:0.796]
Epoch [20/120    avg_loss:0.478, val_acc:0.786]
Epoch [21/120    avg_loss:0.466, val_acc:0.797]
Epoch [22/120    avg_loss:0.458, val_acc:0.809]
Epoch [23/120    avg_loss:0.415, val_acc:0.829]
Epoch [24/120    avg_loss:0.321, val_acc:0.847]
Epoch [25/120    avg_loss:0.301, val_acc:0.849]
Epoch [26/120    avg_loss:0.278, val_acc:0.841]
Epoch [27/120    avg_loss:0.262, val_acc:0.849]
Epoch [28/120    avg_loss:0.318, val_acc:0.843]
Epoch [29/120    avg_loss:0.267, val_acc:0.845]
Epoch [30/120    avg_loss:0.235, val_acc:0.884]
Epoch [31/120    avg_loss:0.172, val_acc:0.890]
Epoch [32/120    avg_loss:0.188, val_acc:0.888]
Epoch [33/120    avg_loss:0.233, val_acc:0.895]
Epoch [34/120    avg_loss:0.196, val_acc:0.900]
Epoch [35/120    avg_loss:0.172, val_acc:0.891]
Epoch [36/120    avg_loss:0.213, val_acc:0.900]
Epoch [37/120    avg_loss:0.159, val_acc:0.916]
Epoch [38/120    avg_loss:0.124, val_acc:0.934]
Epoch [39/120    avg_loss:0.114, val_acc:0.931]
Epoch [40/120    avg_loss:0.111, val_acc:0.939]
Epoch [41/120    avg_loss:0.118, val_acc:0.916]
Epoch [42/120    avg_loss:0.106, val_acc:0.916]
Epoch [43/120    avg_loss:0.109, val_acc:0.927]
Epoch [44/120    avg_loss:0.089, val_acc:0.928]
Epoch [45/120    avg_loss:0.090, val_acc:0.938]
Epoch [46/120    avg_loss:0.083, val_acc:0.948]
Epoch [47/120    avg_loss:0.105, val_acc:0.941]
Epoch [48/120    avg_loss:0.127, val_acc:0.934]
Epoch [49/120    avg_loss:0.118, val_acc:0.893]
Epoch [50/120    avg_loss:0.145, val_acc:0.938]
Epoch [51/120    avg_loss:0.133, val_acc:0.928]
Epoch [52/120    avg_loss:0.119, val_acc:0.921]
Epoch [53/120    avg_loss:0.073, val_acc:0.943]
Epoch [54/120    avg_loss:0.076, val_acc:0.950]
Epoch [55/120    avg_loss:0.068, val_acc:0.925]
Epoch [56/120    avg_loss:0.089, val_acc:0.939]
Epoch [57/120    avg_loss:0.079, val_acc:0.939]
Epoch [58/120    avg_loss:0.072, val_acc:0.942]
Epoch [59/120    avg_loss:0.076, val_acc:0.935]
Epoch [60/120    avg_loss:0.071, val_acc:0.944]
Epoch [61/120    avg_loss:0.065, val_acc:0.948]
Epoch [62/120    avg_loss:0.065, val_acc:0.957]
Epoch [63/120    avg_loss:0.052, val_acc:0.946]
Epoch [64/120    avg_loss:0.040, val_acc:0.963]
Epoch [65/120    avg_loss:0.036, val_acc:0.961]
Epoch [66/120    avg_loss:0.042, val_acc:0.958]
Epoch [67/120    avg_loss:0.034, val_acc:0.963]
Epoch [68/120    avg_loss:0.046, val_acc:0.958]
Epoch [69/120    avg_loss:0.031, val_acc:0.961]
Epoch [70/120    avg_loss:0.029, val_acc:0.966]
Epoch [71/120    avg_loss:0.031, val_acc:0.968]
Epoch [72/120    avg_loss:0.037, val_acc:0.972]
Epoch [73/120    avg_loss:0.035, val_acc:0.954]
Epoch [74/120    avg_loss:0.032, val_acc:0.953]
Epoch [75/120    avg_loss:0.028, val_acc:0.960]
Epoch [76/120    avg_loss:0.026, val_acc:0.966]
Epoch [77/120    avg_loss:0.038, val_acc:0.942]
Epoch [78/120    avg_loss:0.074, val_acc:0.943]
Epoch [79/120    avg_loss:0.088, val_acc:0.941]
Epoch [80/120    avg_loss:0.069, val_acc:0.954]
Epoch [81/120    avg_loss:0.057, val_acc:0.952]
Epoch [82/120    avg_loss:0.040, val_acc:0.949]
Epoch [83/120    avg_loss:0.036, val_acc:0.959]
Epoch [84/120    avg_loss:0.053, val_acc:0.948]
Epoch [85/120    avg_loss:0.126, val_acc:0.930]
Epoch [86/120    avg_loss:0.078, val_acc:0.955]
Epoch [87/120    avg_loss:0.067, val_acc:0.961]
Epoch [88/120    avg_loss:0.043, val_acc:0.963]
Epoch [89/120    avg_loss:0.040, val_acc:0.962]
Epoch [90/120    avg_loss:0.036, val_acc:0.962]
Epoch [91/120    avg_loss:0.036, val_acc:0.961]
Epoch [92/120    avg_loss:0.030, val_acc:0.963]
Epoch [93/120    avg_loss:0.026, val_acc:0.966]
Epoch [94/120    avg_loss:0.032, val_acc:0.960]
Epoch [95/120    avg_loss:0.028, val_acc:0.967]
Epoch [96/120    avg_loss:0.032, val_acc:0.968]
Epoch [97/120    avg_loss:0.026, val_acc:0.967]
Epoch [98/120    avg_loss:0.027, val_acc:0.967]
Epoch [99/120    avg_loss:0.023, val_acc:0.967]
Epoch [100/120    avg_loss:0.026, val_acc:0.967]
Epoch [101/120    avg_loss:0.021, val_acc:0.967]
Epoch [102/120    avg_loss:0.022, val_acc:0.967]
Epoch [103/120    avg_loss:0.025, val_acc:0.967]
Epoch [104/120    avg_loss:0.023, val_acc:0.967]
Epoch [105/120    avg_loss:0.024, val_acc:0.966]
Epoch [106/120    avg_loss:0.024, val_acc:0.966]
Epoch [107/120    avg_loss:0.028, val_acc:0.968]
Epoch [108/120    avg_loss:0.027, val_acc:0.967]
Epoch [109/120    avg_loss:0.024, val_acc:0.967]
Epoch [110/120    avg_loss:0.022, val_acc:0.968]
Epoch [111/120    avg_loss:0.025, val_acc:0.968]
Epoch [112/120    avg_loss:0.027, val_acc:0.968]
Epoch [113/120    avg_loss:0.022, val_acc:0.968]
Epoch [114/120    avg_loss:0.020, val_acc:0.967]
Epoch [115/120    avg_loss:0.025, val_acc:0.967]
Epoch [116/120    avg_loss:0.021, val_acc:0.968]
Epoch [117/120    avg_loss:0.024, val_acc:0.967]
Epoch [118/120    avg_loss:0.027, val_acc:0.967]
Epoch [119/120    avg_loss:0.028, val_acc:0.968]
Epoch [120/120    avg_loss:0.023, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1250    4    2    0    1    0    0    0    5   15    8    0
     0    0    0]
 [   0    0    0  691   10    5    0    0    0    1    1    3   36    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    1    0    3    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   14    0    2    0    0
     0    0    0]
 [   0    0    5    0    0    7    3    0    0    2  840   10    4    0
     2    2    0]
 [   0    0   14    0    0    1    0    0    0    0   18 2175    0    0
     2    0    0]
 [   0    0    0    1    2    5    0    0    0    0    0    2  522    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   25    0    0    1    0    2    1    0    2
  1108    0    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    64  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.82384823848238

F1 scores:
[       nan 1.         0.9788567  0.95640138 0.96818182 0.94247788
 0.9893617  0.98039216 0.99883856 0.73684211 0.96496267 0.98438561
 0.94394213 0.99462366 0.95311828 0.88712242 0.98203593]

Kappa:
0.9637871013149666
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:10:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8e39ec57f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.650, val_acc:0.278]
Epoch [2/120    avg_loss:2.331, val_acc:0.421]
Epoch [3/120    avg_loss:2.148, val_acc:0.515]
Epoch [4/120    avg_loss:2.030, val_acc:0.540]
Epoch [5/120    avg_loss:1.852, val_acc:0.568]
Epoch [6/120    avg_loss:1.731, val_acc:0.591]
Epoch [7/120    avg_loss:1.641, val_acc:0.613]
Epoch [8/120    avg_loss:1.535, val_acc:0.607]
Epoch [9/120    avg_loss:1.430, val_acc:0.611]
Epoch [10/120    avg_loss:1.321, val_acc:0.666]
Epoch [11/120    avg_loss:1.189, val_acc:0.675]
Epoch [12/120    avg_loss:1.103, val_acc:0.685]
Epoch [13/120    avg_loss:1.021, val_acc:0.728]
Epoch [14/120    avg_loss:0.908, val_acc:0.780]
Epoch [15/120    avg_loss:0.824, val_acc:0.781]
Epoch [16/120    avg_loss:0.682, val_acc:0.786]
Epoch [17/120    avg_loss:0.709, val_acc:0.798]
Epoch [18/120    avg_loss:0.610, val_acc:0.841]
Epoch [19/120    avg_loss:0.543, val_acc:0.833]
Epoch [20/120    avg_loss:0.491, val_acc:0.858]
Epoch [21/120    avg_loss:0.451, val_acc:0.821]
Epoch [22/120    avg_loss:0.427, val_acc:0.859]
Epoch [23/120    avg_loss:0.366, val_acc:0.856]
Epoch [24/120    avg_loss:0.389, val_acc:0.863]
Epoch [25/120    avg_loss:0.342, val_acc:0.828]
Epoch [26/120    avg_loss:0.362, val_acc:0.833]
Epoch [27/120    avg_loss:0.297, val_acc:0.881]
Epoch [28/120    avg_loss:0.290, val_acc:0.862]
Epoch [29/120    avg_loss:0.254, val_acc:0.876]
Epoch [30/120    avg_loss:0.240, val_acc:0.912]
Epoch [31/120    avg_loss:0.274, val_acc:0.870]
Epoch [32/120    avg_loss:0.222, val_acc:0.891]
Epoch [33/120    avg_loss:0.186, val_acc:0.914]
Epoch [34/120    avg_loss:0.229, val_acc:0.870]
Epoch [35/120    avg_loss:0.304, val_acc:0.877]
Epoch [36/120    avg_loss:0.259, val_acc:0.889]
Epoch [37/120    avg_loss:0.185, val_acc:0.934]
Epoch [38/120    avg_loss:0.168, val_acc:0.934]
Epoch [39/120    avg_loss:0.130, val_acc:0.929]
Epoch [40/120    avg_loss:0.116, val_acc:0.932]
Epoch [41/120    avg_loss:0.108, val_acc:0.929]
Epoch [42/120    avg_loss:0.144, val_acc:0.918]
Epoch [43/120    avg_loss:0.118, val_acc:0.930]
Epoch [44/120    avg_loss:0.115, val_acc:0.928]
Epoch [45/120    avg_loss:0.108, val_acc:0.933]
Epoch [46/120    avg_loss:0.095, val_acc:0.941]
Epoch [47/120    avg_loss:0.111, val_acc:0.930]
Epoch [48/120    avg_loss:0.090, val_acc:0.934]
Epoch [49/120    avg_loss:0.071, val_acc:0.956]
Epoch [50/120    avg_loss:0.059, val_acc:0.948]
Epoch [51/120    avg_loss:0.068, val_acc:0.956]
Epoch [52/120    avg_loss:0.081, val_acc:0.952]
Epoch [53/120    avg_loss:0.074, val_acc:0.950]
Epoch [54/120    avg_loss:0.070, val_acc:0.956]
Epoch [55/120    avg_loss:0.080, val_acc:0.936]
Epoch [56/120    avg_loss:0.119, val_acc:0.940]
Epoch [57/120    avg_loss:0.078, val_acc:0.931]
Epoch [58/120    avg_loss:0.080, val_acc:0.942]
Epoch [59/120    avg_loss:0.079, val_acc:0.949]
Epoch [60/120    avg_loss:0.056, val_acc:0.939]
Epoch [61/120    avg_loss:0.051, val_acc:0.956]
Epoch [62/120    avg_loss:0.046, val_acc:0.948]
Epoch [63/120    avg_loss:0.047, val_acc:0.960]
Epoch [64/120    avg_loss:0.052, val_acc:0.952]
Epoch [65/120    avg_loss:0.057, val_acc:0.944]
Epoch [66/120    avg_loss:0.040, val_acc:0.956]
Epoch [67/120    avg_loss:0.031, val_acc:0.964]
Epoch [68/120    avg_loss:0.028, val_acc:0.954]
Epoch [69/120    avg_loss:0.035, val_acc:0.958]
Epoch [70/120    avg_loss:0.048, val_acc:0.957]
Epoch [71/120    avg_loss:0.043, val_acc:0.955]
Epoch [72/120    avg_loss:0.040, val_acc:0.959]
Epoch [73/120    avg_loss:0.056, val_acc:0.954]
Epoch [74/120    avg_loss:0.043, val_acc:0.954]
Epoch [75/120    avg_loss:0.044, val_acc:0.953]
Epoch [76/120    avg_loss:0.051, val_acc:0.953]
Epoch [77/120    avg_loss:0.038, val_acc:0.949]
Epoch [78/120    avg_loss:0.030, val_acc:0.969]
Epoch [79/120    avg_loss:0.024, val_acc:0.964]
Epoch [80/120    avg_loss:0.024, val_acc:0.956]
Epoch [81/120    avg_loss:0.034, val_acc:0.946]
Epoch [82/120    avg_loss:0.037, val_acc:0.950]
Epoch [83/120    avg_loss:0.030, val_acc:0.960]
Epoch [84/120    avg_loss:0.029, val_acc:0.958]
Epoch [85/120    avg_loss:0.023, val_acc:0.954]
Epoch [86/120    avg_loss:0.017, val_acc:0.957]
Epoch [87/120    avg_loss:0.028, val_acc:0.962]
Epoch [88/120    avg_loss:0.026, val_acc:0.962]
Epoch [89/120    avg_loss:0.025, val_acc:0.959]
Epoch [90/120    avg_loss:0.023, val_acc:0.960]
Epoch [91/120    avg_loss:0.020, val_acc:0.969]
Epoch [92/120    avg_loss:0.017, val_acc:0.962]
Epoch [93/120    avg_loss:0.020, val_acc:0.958]
Epoch [94/120    avg_loss:0.020, val_acc:0.966]
Epoch [95/120    avg_loss:0.017, val_acc:0.960]
Epoch [96/120    avg_loss:0.017, val_acc:0.968]
Epoch [97/120    avg_loss:0.015, val_acc:0.969]
Epoch [98/120    avg_loss:0.018, val_acc:0.963]
Epoch [99/120    avg_loss:0.015, val_acc:0.962]
Epoch [100/120    avg_loss:0.017, val_acc:0.968]
Epoch [101/120    avg_loss:0.016, val_acc:0.963]
Epoch [102/120    avg_loss:0.022, val_acc:0.968]
Epoch [103/120    avg_loss:0.015, val_acc:0.971]
Epoch [104/120    avg_loss:0.011, val_acc:0.966]
Epoch [105/120    avg_loss:0.012, val_acc:0.964]
Epoch [106/120    avg_loss:0.031, val_acc:0.945]
Epoch [107/120    avg_loss:0.027, val_acc:0.959]
Epoch [108/120    avg_loss:0.027, val_acc:0.944]
Epoch [109/120    avg_loss:0.069, val_acc:0.954]
Epoch [110/120    avg_loss:0.031, val_acc:0.964]
Epoch [111/120    avg_loss:0.028, val_acc:0.969]
Epoch [112/120    avg_loss:0.023, val_acc:0.960]
Epoch [113/120    avg_loss:0.026, val_acc:0.964]
Epoch [114/120    avg_loss:0.016, val_acc:0.955]
Epoch [115/120    avg_loss:0.014, val_acc:0.969]
Epoch [116/120    avg_loss:0.019, val_acc:0.967]
Epoch [117/120    avg_loss:0.022, val_acc:0.974]
Epoch [118/120    avg_loss:0.012, val_acc:0.972]
Epoch [119/120    avg_loss:0.014, val_acc:0.973]
Epoch [120/120    avg_loss:0.010, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    4    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1252    1    0    0    3    0    0    0    6   22    1    0
     0    0    0]
 [   0    0    2  700    1   12    0    0    0   11    0    1   20    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    4    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0    2    0    0    6    0    0    0    0  856    7    0    0
     0    4    0]
 [   0    0    1    0    0    0    7    0    0    0   19 2179    2    2
     0    0    0]
 [   0    0    0    8    1    1    0    0    0    0    7   10  503    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  184
     0    1    0]
 [   0    0    0    0    0   14    1    0    1    0    1    2    0    0
  1111    9    0]
 [   0    0    0    0    0    0   29    0    0    6    0    0    0    0
    11  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.289972899729

F1 scores:
[       nan 0.93506494 0.98350353 0.96153846 0.9953271  0.95535714
 0.9704579  0.92592593 0.99883856 0.59259259 0.96997167 0.98352516
 0.9472693  0.99191375 0.98231653 0.90936556 0.98245614]

Kappa:
0.9691095294981857
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f36f5d6e7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.655, val_acc:0.348]
Epoch [2/120    avg_loss:2.379, val_acc:0.445]
Epoch [3/120    avg_loss:2.150, val_acc:0.519]
Epoch [4/120    avg_loss:1.996, val_acc:0.566]
Epoch [5/120    avg_loss:1.875, val_acc:0.613]
Epoch [6/120    avg_loss:1.763, val_acc:0.626]
Epoch [7/120    avg_loss:1.615, val_acc:0.639]
Epoch [8/120    avg_loss:1.435, val_acc:0.659]
Epoch [9/120    avg_loss:1.324, val_acc:0.693]
Epoch [10/120    avg_loss:1.139, val_acc:0.709]
Epoch [11/120    avg_loss:1.013, val_acc:0.693]
Epoch [12/120    avg_loss:0.943, val_acc:0.717]
Epoch [13/120    avg_loss:0.827, val_acc:0.721]
Epoch [14/120    avg_loss:0.737, val_acc:0.764]
Epoch [15/120    avg_loss:0.650, val_acc:0.809]
Epoch [16/120    avg_loss:0.619, val_acc:0.802]
Epoch [17/120    avg_loss:0.564, val_acc:0.817]
Epoch [18/120    avg_loss:0.510, val_acc:0.821]
Epoch [19/120    avg_loss:0.482, val_acc:0.814]
Epoch [20/120    avg_loss:0.411, val_acc:0.810]
Epoch [21/120    avg_loss:0.401, val_acc:0.874]
Epoch [22/120    avg_loss:0.348, val_acc:0.867]
Epoch [23/120    avg_loss:0.306, val_acc:0.863]
Epoch [24/120    avg_loss:0.352, val_acc:0.859]
Epoch [25/120    avg_loss:0.333, val_acc:0.886]
Epoch [26/120    avg_loss:0.290, val_acc:0.889]
Epoch [27/120    avg_loss:0.257, val_acc:0.908]
Epoch [28/120    avg_loss:0.233, val_acc:0.891]
Epoch [29/120    avg_loss:0.240, val_acc:0.889]
Epoch [30/120    avg_loss:0.241, val_acc:0.904]
Epoch [31/120    avg_loss:0.237, val_acc:0.898]
Epoch [32/120    avg_loss:0.232, val_acc:0.907]
Epoch [33/120    avg_loss:0.198, val_acc:0.927]
Epoch [34/120    avg_loss:0.167, val_acc:0.918]
Epoch [35/120    avg_loss:0.201, val_acc:0.929]
Epoch [36/120    avg_loss:0.194, val_acc:0.916]
Epoch [37/120    avg_loss:0.172, val_acc:0.934]
Epoch [38/120    avg_loss:0.144, val_acc:0.914]
Epoch [39/120    avg_loss:0.131, val_acc:0.945]
Epoch [40/120    avg_loss:0.128, val_acc:0.932]
Epoch [41/120    avg_loss:0.118, val_acc:0.926]
Epoch [42/120    avg_loss:0.145, val_acc:0.941]
Epoch [43/120    avg_loss:0.111, val_acc:0.934]
Epoch [44/120    avg_loss:0.097, val_acc:0.930]
Epoch [45/120    avg_loss:0.144, val_acc:0.909]
Epoch [46/120    avg_loss:0.171, val_acc:0.898]
Epoch [47/120    avg_loss:0.146, val_acc:0.925]
Epoch [48/120    avg_loss:0.114, val_acc:0.919]
Epoch [49/120    avg_loss:0.111, val_acc:0.954]
Epoch [50/120    avg_loss:0.084, val_acc:0.932]
Epoch [51/120    avg_loss:0.072, val_acc:0.957]
Epoch [52/120    avg_loss:0.077, val_acc:0.960]
Epoch [53/120    avg_loss:0.061, val_acc:0.966]
Epoch [54/120    avg_loss:0.051, val_acc:0.949]
Epoch [55/120    avg_loss:0.054, val_acc:0.968]
Epoch [56/120    avg_loss:0.066, val_acc:0.952]
Epoch [57/120    avg_loss:0.050, val_acc:0.964]
Epoch [58/120    avg_loss:0.058, val_acc:0.945]
Epoch [59/120    avg_loss:0.068, val_acc:0.970]
Epoch [60/120    avg_loss:0.063, val_acc:0.942]
Epoch [61/120    avg_loss:0.061, val_acc:0.963]
Epoch [62/120    avg_loss:0.051, val_acc:0.963]
Epoch [63/120    avg_loss:0.039, val_acc:0.962]
Epoch [64/120    avg_loss:0.034, val_acc:0.967]
Epoch [65/120    avg_loss:0.037, val_acc:0.961]
Epoch [66/120    avg_loss:0.054, val_acc:0.954]
Epoch [67/120    avg_loss:0.060, val_acc:0.964]
Epoch [68/120    avg_loss:0.034, val_acc:0.964]
Epoch [69/120    avg_loss:0.038, val_acc:0.971]
Epoch [70/120    avg_loss:0.043, val_acc:0.973]
Epoch [71/120    avg_loss:0.042, val_acc:0.976]
Epoch [72/120    avg_loss:0.033, val_acc:0.968]
Epoch [73/120    avg_loss:0.032, val_acc:0.966]
Epoch [74/120    avg_loss:0.047, val_acc:0.969]
Epoch [75/120    avg_loss:0.050, val_acc:0.973]
Epoch [76/120    avg_loss:0.059, val_acc:0.953]
Epoch [77/120    avg_loss:0.046, val_acc:0.969]
Epoch [78/120    avg_loss:0.029, val_acc:0.970]
Epoch [79/120    avg_loss:0.028, val_acc:0.969]
Epoch [80/120    avg_loss:0.033, val_acc:0.971]
Epoch [81/120    avg_loss:0.032, val_acc:0.958]
Epoch [82/120    avg_loss:0.048, val_acc:0.966]
Epoch [83/120    avg_loss:0.035, val_acc:0.973]
Epoch [84/120    avg_loss:0.027, val_acc:0.967]
Epoch [85/120    avg_loss:0.031, val_acc:0.974]
Epoch [86/120    avg_loss:0.021, val_acc:0.974]
Epoch [87/120    avg_loss:0.019, val_acc:0.976]
Epoch [88/120    avg_loss:0.020, val_acc:0.974]
Epoch [89/120    avg_loss:0.023, val_acc:0.975]
Epoch [90/120    avg_loss:0.018, val_acc:0.977]
Epoch [91/120    avg_loss:0.016, val_acc:0.976]
Epoch [92/120    avg_loss:0.018, val_acc:0.977]
Epoch [93/120    avg_loss:0.016, val_acc:0.980]
Epoch [94/120    avg_loss:0.018, val_acc:0.980]
Epoch [95/120    avg_loss:0.014, val_acc:0.978]
Epoch [96/120    avg_loss:0.017, val_acc:0.976]
Epoch [97/120    avg_loss:0.015, val_acc:0.976]
Epoch [98/120    avg_loss:0.020, val_acc:0.976]
Epoch [99/120    avg_loss:0.015, val_acc:0.978]
Epoch [100/120    avg_loss:0.017, val_acc:0.978]
Epoch [101/120    avg_loss:0.017, val_acc:0.977]
Epoch [102/120    avg_loss:0.015, val_acc:0.976]
Epoch [103/120    avg_loss:0.021, val_acc:0.975]
Epoch [104/120    avg_loss:0.016, val_acc:0.980]
Epoch [105/120    avg_loss:0.017, val_acc:0.976]
Epoch [106/120    avg_loss:0.015, val_acc:0.976]
Epoch [107/120    avg_loss:0.016, val_acc:0.973]
Epoch [108/120    avg_loss:0.015, val_acc:0.975]
Epoch [109/120    avg_loss:0.017, val_acc:0.977]
Epoch [110/120    avg_loss:0.018, val_acc:0.975]
Epoch [111/120    avg_loss:0.015, val_acc:0.975]
Epoch [112/120    avg_loss:0.019, val_acc:0.976]
Epoch [113/120    avg_loss:0.017, val_acc:0.977]
Epoch [114/120    avg_loss:0.016, val_acc:0.980]
Epoch [115/120    avg_loss:0.015, val_acc:0.978]
Epoch [116/120    avg_loss:0.015, val_acc:0.976]
Epoch [117/120    avg_loss:0.014, val_acc:0.977]
Epoch [118/120    avg_loss:0.013, val_acc:0.978]
Epoch [119/120    avg_loss:0.016, val_acc:0.977]
Epoch [120/120    avg_loss:0.017, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    0    0    0    0    0    0    0    0    6    0    0    0
     0    0    0]
 [   0    0 1264    0    0    0    1    0    0    0    6   13    1    0
     0    0    0]
 [   0    0    0  708    2    2    1    0    0    4    0    0   30    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    2    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   15    0    0    1    0
     0    0    0]
 [   0    0    7    1    0    3    2    0    0    0  842   15    0    0
     0    5    0]
 [   0    0    7    0    0    0    3    0    0    1    4 2183   11    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0   16    6  510    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   15    0    0    4    0    0    0    0
    10  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.91869918699187

F1 scores:
[       nan 0.92105263 0.98634413 0.97119342 0.9953271  0.98850575
 0.98277154 0.96153846 0.99883856 0.66666667 0.96173615 0.98599819
 0.9375     0.99730458 0.99431072 0.94925373 0.98224852]

Kappa:
0.976268775412136
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1343a3c828>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.714, val_acc:0.283]
Epoch [2/120    avg_loss:2.390, val_acc:0.454]
Epoch [3/120    avg_loss:2.163, val_acc:0.506]
Epoch [4/120    avg_loss:1.965, val_acc:0.541]
Epoch [5/120    avg_loss:1.865, val_acc:0.572]
Epoch [6/120    avg_loss:1.753, val_acc:0.607]
Epoch [7/120    avg_loss:1.636, val_acc:0.615]
Epoch [8/120    avg_loss:1.487, val_acc:0.634]
Epoch [9/120    avg_loss:1.337, val_acc:0.656]
Epoch [10/120    avg_loss:1.194, val_acc:0.682]
Epoch [11/120    avg_loss:1.093, val_acc:0.678]
Epoch [12/120    avg_loss:0.983, val_acc:0.705]
Epoch [13/120    avg_loss:0.894, val_acc:0.752]
Epoch [14/120    avg_loss:0.746, val_acc:0.763]
Epoch [15/120    avg_loss:0.688, val_acc:0.744]
Epoch [16/120    avg_loss:0.648, val_acc:0.796]
Epoch [17/120    avg_loss:0.606, val_acc:0.794]
Epoch [18/120    avg_loss:0.558, val_acc:0.818]
Epoch [19/120    avg_loss:0.556, val_acc:0.824]
Epoch [20/120    avg_loss:0.475, val_acc:0.838]
Epoch [21/120    avg_loss:0.404, val_acc:0.866]
Epoch [22/120    avg_loss:0.372, val_acc:0.880]
Epoch [23/120    avg_loss:0.357, val_acc:0.872]
Epoch [24/120    avg_loss:0.360, val_acc:0.829]
Epoch [25/120    avg_loss:0.376, val_acc:0.876]
Epoch [26/120    avg_loss:0.353, val_acc:0.878]
Epoch [27/120    avg_loss:0.322, val_acc:0.879]
Epoch [28/120    avg_loss:0.262, val_acc:0.912]
Epoch [29/120    avg_loss:0.298, val_acc:0.887]
Epoch [30/120    avg_loss:0.291, val_acc:0.888]
Epoch [31/120    avg_loss:0.260, val_acc:0.904]
Epoch [32/120    avg_loss:0.243, val_acc:0.901]
Epoch [33/120    avg_loss:0.207, val_acc:0.923]
Epoch [34/120    avg_loss:0.230, val_acc:0.879]
Epoch [35/120    avg_loss:0.213, val_acc:0.917]
Epoch [36/120    avg_loss:0.206, val_acc:0.922]
Epoch [37/120    avg_loss:0.207, val_acc:0.921]
Epoch [38/120    avg_loss:0.171, val_acc:0.931]
Epoch [39/120    avg_loss:0.161, val_acc:0.926]
Epoch [40/120    avg_loss:0.138, val_acc:0.936]
Epoch [41/120    avg_loss:0.118, val_acc:0.930]
Epoch [42/120    avg_loss:0.137, val_acc:0.945]
Epoch [43/120    avg_loss:0.149, val_acc:0.911]
Epoch [44/120    avg_loss:0.113, val_acc:0.946]
Epoch [45/120    avg_loss:0.103, val_acc:0.928]
Epoch [46/120    avg_loss:0.101, val_acc:0.944]
Epoch [47/120    avg_loss:0.084, val_acc:0.950]
Epoch [48/120    avg_loss:0.089, val_acc:0.957]
Epoch [49/120    avg_loss:0.083, val_acc:0.938]
Epoch [50/120    avg_loss:0.135, val_acc:0.935]
Epoch [51/120    avg_loss:0.133, val_acc:0.933]
Epoch [52/120    avg_loss:0.083, val_acc:0.952]
Epoch [53/120    avg_loss:0.077, val_acc:0.944]
Epoch [54/120    avg_loss:0.093, val_acc:0.945]
Epoch [55/120    avg_loss:0.075, val_acc:0.963]
Epoch [56/120    avg_loss:0.069, val_acc:0.969]
Epoch [57/120    avg_loss:0.059, val_acc:0.970]
Epoch [58/120    avg_loss:0.053, val_acc:0.972]
Epoch [59/120    avg_loss:0.052, val_acc:0.961]
Epoch [60/120    avg_loss:0.044, val_acc:0.971]
Epoch [61/120    avg_loss:0.056, val_acc:0.971]
Epoch [62/120    avg_loss:0.045, val_acc:0.969]
Epoch [63/120    avg_loss:0.059, val_acc:0.956]
Epoch [64/120    avg_loss:0.069, val_acc:0.963]
Epoch [65/120    avg_loss:0.046, val_acc:0.966]
Epoch [66/120    avg_loss:0.053, val_acc:0.970]
Epoch [67/120    avg_loss:0.059, val_acc:0.961]
Epoch [68/120    avg_loss:0.040, val_acc:0.977]
Epoch [69/120    avg_loss:0.043, val_acc:0.978]
Epoch [70/120    avg_loss:0.046, val_acc:0.962]
Epoch [71/120    avg_loss:0.054, val_acc:0.956]
Epoch [72/120    avg_loss:0.049, val_acc:0.974]
Epoch [73/120    avg_loss:0.042, val_acc:0.967]
Epoch [74/120    avg_loss:0.047, val_acc:0.975]
Epoch [75/120    avg_loss:0.032, val_acc:0.973]
Epoch [76/120    avg_loss:0.028, val_acc:0.976]
Epoch [77/120    avg_loss:0.035, val_acc:0.968]
Epoch [78/120    avg_loss:0.058, val_acc:0.960]
Epoch [79/120    avg_loss:0.041, val_acc:0.969]
Epoch [80/120    avg_loss:0.027, val_acc:0.972]
Epoch [81/120    avg_loss:0.029, val_acc:0.977]
Epoch [82/120    avg_loss:0.022, val_acc:0.970]
Epoch [83/120    avg_loss:0.020, val_acc:0.972]
Epoch [84/120    avg_loss:0.018, val_acc:0.976]
Epoch [85/120    avg_loss:0.017, val_acc:0.977]
Epoch [86/120    avg_loss:0.017, val_acc:0.978]
Epoch [87/120    avg_loss:0.016, val_acc:0.980]
Epoch [88/120    avg_loss:0.018, val_acc:0.980]
Epoch [89/120    avg_loss:0.018, val_acc:0.981]
Epoch [90/120    avg_loss:0.016, val_acc:0.981]
Epoch [91/120    avg_loss:0.014, val_acc:0.983]
Epoch [92/120    avg_loss:0.015, val_acc:0.983]
Epoch [93/120    avg_loss:0.013, val_acc:0.983]
Epoch [94/120    avg_loss:0.015, val_acc:0.982]
Epoch [95/120    avg_loss:0.015, val_acc:0.981]
Epoch [96/120    avg_loss:0.017, val_acc:0.981]
Epoch [97/120    avg_loss:0.015, val_acc:0.981]
Epoch [98/120    avg_loss:0.014, val_acc:0.981]
Epoch [99/120    avg_loss:0.014, val_acc:0.982]
Epoch [100/120    avg_loss:0.013, val_acc:0.983]
Epoch [101/120    avg_loss:0.014, val_acc:0.982]
Epoch [102/120    avg_loss:0.012, val_acc:0.983]
Epoch [103/120    avg_loss:0.016, val_acc:0.982]
Epoch [104/120    avg_loss:0.013, val_acc:0.983]
Epoch [105/120    avg_loss:0.013, val_acc:0.982]
Epoch [106/120    avg_loss:0.017, val_acc:0.984]
Epoch [107/120    avg_loss:0.015, val_acc:0.982]
Epoch [108/120    avg_loss:0.015, val_acc:0.981]
Epoch [109/120    avg_loss:0.016, val_acc:0.981]
Epoch [110/120    avg_loss:0.012, val_acc:0.983]
Epoch [111/120    avg_loss:0.020, val_acc:0.984]
Epoch [112/120    avg_loss:0.014, val_acc:0.984]
Epoch [113/120    avg_loss:0.014, val_acc:0.983]
Epoch [114/120    avg_loss:0.012, val_acc:0.983]
Epoch [115/120    avg_loss:0.014, val_acc:0.982]
Epoch [116/120    avg_loss:0.015, val_acc:0.983]
Epoch [117/120    avg_loss:0.013, val_acc:0.982]
Epoch [118/120    avg_loss:0.012, val_acc:0.981]
Epoch [119/120    avg_loss:0.016, val_acc:0.982]
Epoch [120/120    avg_loss:0.012, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1258    0    0    0    0    0    0    0    4   23    0    0
     0    0    0]
 [   0    0    1  709    3    4    0    0    0    4    2    1   23    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    2    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   11    0    0    7    0    0    0    0  841   15    0    0
     0    1    0]
 [   0    0    6    0    0    0    0    0    0    0   18 2185    0    1
     0    0    0]
 [   0    0    6    0    0    2    0    0    0    0    3    9  512    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   13    0    0    0    0    1    3    0    0
  1118    4    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    14  323    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.81029810298104

F1 scores:
[       nan 0.93506494 0.98013245 0.9739011  0.99300699 0.96520763
 0.99244713 0.96153846 1.         0.8372093  0.9616924  0.98290598
 0.95700935 0.99730458 0.98415493 0.95703704 0.98809524]

Kappa:
0.9750258797663067
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3ac3fda7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.668, val_acc:0.427]
Epoch [2/120    avg_loss:2.343, val_acc:0.470]
Epoch [3/120    avg_loss:2.136, val_acc:0.472]
Epoch [4/120    avg_loss:1.986, val_acc:0.546]
Epoch [5/120    avg_loss:1.851, val_acc:0.548]
Epoch [6/120    avg_loss:1.721, val_acc:0.592]
Epoch [7/120    avg_loss:1.617, val_acc:0.634]
Epoch [8/120    avg_loss:1.488, val_acc:0.636]
Epoch [9/120    avg_loss:1.390, val_acc:0.628]
Epoch [10/120    avg_loss:1.214, val_acc:0.680]
Epoch [11/120    avg_loss:1.176, val_acc:0.685]
Epoch [12/120    avg_loss:1.037, val_acc:0.713]
Epoch [13/120    avg_loss:0.969, val_acc:0.691]
Epoch [14/120    avg_loss:0.924, val_acc:0.698]
Epoch [15/120    avg_loss:0.786, val_acc:0.759]
Epoch [16/120    avg_loss:0.728, val_acc:0.768]
Epoch [17/120    avg_loss:0.781, val_acc:0.776]
Epoch [18/120    avg_loss:0.723, val_acc:0.744]
Epoch [19/120    avg_loss:0.626, val_acc:0.765]
Epoch [20/120    avg_loss:0.632, val_acc:0.761]
Epoch [21/120    avg_loss:0.614, val_acc:0.782]
Epoch [22/120    avg_loss:0.503, val_acc:0.800]
Epoch [23/120    avg_loss:0.532, val_acc:0.828]
Epoch [24/120    avg_loss:0.459, val_acc:0.826]
Epoch [25/120    avg_loss:0.367, val_acc:0.836]
Epoch [26/120    avg_loss:0.377, val_acc:0.817]
Epoch [27/120    avg_loss:0.386, val_acc:0.882]
Epoch [28/120    avg_loss:0.361, val_acc:0.868]
Epoch [29/120    avg_loss:0.331, val_acc:0.855]
Epoch [30/120    avg_loss:0.295, val_acc:0.850]
Epoch [31/120    avg_loss:0.267, val_acc:0.879]
Epoch [32/120    avg_loss:0.258, val_acc:0.888]
Epoch [33/120    avg_loss:0.215, val_acc:0.897]
Epoch [34/120    avg_loss:0.225, val_acc:0.894]
Epoch [35/120    avg_loss:0.223, val_acc:0.920]
Epoch [36/120    avg_loss:0.198, val_acc:0.910]
Epoch [37/120    avg_loss:0.198, val_acc:0.911]
Epoch [38/120    avg_loss:0.165, val_acc:0.917]
Epoch [39/120    avg_loss:0.144, val_acc:0.933]
Epoch [40/120    avg_loss:0.170, val_acc:0.907]
Epoch [41/120    avg_loss:0.183, val_acc:0.907]
Epoch [42/120    avg_loss:0.140, val_acc:0.929]
Epoch [43/120    avg_loss:0.138, val_acc:0.939]
Epoch [44/120    avg_loss:0.098, val_acc:0.923]
Epoch [45/120    avg_loss:0.116, val_acc:0.913]
Epoch [46/120    avg_loss:0.127, val_acc:0.925]
Epoch [47/120    avg_loss:0.106, val_acc:0.916]
Epoch [48/120    avg_loss:0.114, val_acc:0.948]
Epoch [49/120    avg_loss:0.138, val_acc:0.939]
Epoch [50/120    avg_loss:0.140, val_acc:0.935]
Epoch [51/120    avg_loss:0.201, val_acc:0.905]
Epoch [52/120    avg_loss:0.153, val_acc:0.920]
Epoch [53/120    avg_loss:0.120, val_acc:0.916]
Epoch [54/120    avg_loss:0.134, val_acc:0.942]
Epoch [55/120    avg_loss:0.097, val_acc:0.939]
Epoch [56/120    avg_loss:0.080, val_acc:0.940]
Epoch [57/120    avg_loss:0.066, val_acc:0.954]
Epoch [58/120    avg_loss:0.089, val_acc:0.949]
Epoch [59/120    avg_loss:0.073, val_acc:0.948]
Epoch [60/120    avg_loss:0.072, val_acc:0.951]
Epoch [61/120    avg_loss:0.066, val_acc:0.956]
Epoch [62/120    avg_loss:0.065, val_acc:0.946]
Epoch [63/120    avg_loss:0.067, val_acc:0.949]
Epoch [64/120    avg_loss:0.060, val_acc:0.951]
Epoch [65/120    avg_loss:0.053, val_acc:0.959]
Epoch [66/120    avg_loss:0.051, val_acc:0.950]
Epoch [67/120    avg_loss:0.082, val_acc:0.932]
Epoch [68/120    avg_loss:0.059, val_acc:0.961]
Epoch [69/120    avg_loss:0.052, val_acc:0.948]
Epoch [70/120    avg_loss:0.049, val_acc:0.958]
Epoch [71/120    avg_loss:0.047, val_acc:0.954]
Epoch [72/120    avg_loss:0.061, val_acc:0.956]
Epoch [73/120    avg_loss:0.070, val_acc:0.955]
Epoch [74/120    avg_loss:0.068, val_acc:0.960]
Epoch [75/120    avg_loss:0.049, val_acc:0.955]
Epoch [76/120    avg_loss:0.039, val_acc:0.960]
Epoch [77/120    avg_loss:0.035, val_acc:0.967]
Epoch [78/120    avg_loss:0.033, val_acc:0.961]
Epoch [79/120    avg_loss:0.034, val_acc:0.963]
Epoch [80/120    avg_loss:0.036, val_acc:0.956]
Epoch [81/120    avg_loss:0.042, val_acc:0.952]
Epoch [82/120    avg_loss:0.034, val_acc:0.962]
Epoch [83/120    avg_loss:0.069, val_acc:0.946]
Epoch [84/120    avg_loss:0.060, val_acc:0.941]
Epoch [85/120    avg_loss:0.036, val_acc:0.962]
Epoch [86/120    avg_loss:0.041, val_acc:0.951]
Epoch [87/120    avg_loss:0.038, val_acc:0.952]
Epoch [88/120    avg_loss:0.027, val_acc:0.961]
Epoch [89/120    avg_loss:0.036, val_acc:0.961]
Epoch [90/120    avg_loss:0.030, val_acc:0.963]
Epoch [91/120    avg_loss:0.023, val_acc:0.972]
Epoch [92/120    avg_loss:0.019, val_acc:0.972]
Epoch [93/120    avg_loss:0.019, val_acc:0.974]
Epoch [94/120    avg_loss:0.019, val_acc:0.974]
Epoch [95/120    avg_loss:0.023, val_acc:0.974]
Epoch [96/120    avg_loss:0.018, val_acc:0.975]
Epoch [97/120    avg_loss:0.016, val_acc:0.975]
Epoch [98/120    avg_loss:0.017, val_acc:0.974]
Epoch [99/120    avg_loss:0.017, val_acc:0.974]
Epoch [100/120    avg_loss:0.014, val_acc:0.973]
Epoch [101/120    avg_loss:0.014, val_acc:0.975]
Epoch [102/120    avg_loss:0.017, val_acc:0.975]
Epoch [103/120    avg_loss:0.020, val_acc:0.977]
Epoch [104/120    avg_loss:0.015, val_acc:0.978]
Epoch [105/120    avg_loss:0.016, val_acc:0.977]
Epoch [106/120    avg_loss:0.016, val_acc:0.977]
Epoch [107/120    avg_loss:0.017, val_acc:0.975]
Epoch [108/120    avg_loss:0.017, val_acc:0.977]
Epoch [109/120    avg_loss:0.018, val_acc:0.977]
Epoch [110/120    avg_loss:0.015, val_acc:0.974]
Epoch [111/120    avg_loss:0.017, val_acc:0.974]
Epoch [112/120    avg_loss:0.016, val_acc:0.975]
Epoch [113/120    avg_loss:0.016, val_acc:0.978]
Epoch [114/120    avg_loss:0.013, val_acc:0.977]
Epoch [115/120    avg_loss:0.014, val_acc:0.975]
Epoch [116/120    avg_loss:0.016, val_acc:0.975]
Epoch [117/120    avg_loss:0.015, val_acc:0.978]
Epoch [118/120    avg_loss:0.014, val_acc:0.975]
Epoch [119/120    avg_loss:0.015, val_acc:0.978]
Epoch [120/120    avg_loss:0.016, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1239    6    4    0    3    0    0    1    4   27    1    0
     0    0    0]
 [   0    0    5  697    2   13    0    0    0   14    0    0   15    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    2    0    4    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   14    0    0    1    0
     0    0    0]
 [   0    0   19   65    0    5    0    0    0    0  772    9    5    0
     0    0    0]
 [   0    0    7    3    0    0    1    0    6    0   18 2170    3    2
     0    0    0]
 [   0    0    0   16    7    0    0    0    0    0    0   15  490    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    4    0    0    0
  1129    0    0]
 [   0    0    0    0    0    0    3    0    0    2    0    0    0    0
    45  297    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.09756097560975

F1 scores:
[       nan 0.96202532 0.96986301 0.90696161 0.97038724 0.96497175
 0.99393939 0.96153846 0.99192618 0.52830189 0.92124105 0.97924188
 0.93333333 0.9919571  0.97537797 0.92236025 0.95953757]

Kappa:
0.9555043294850138
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0aa7f987f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.686, val_acc:0.270]
Epoch [2/120    avg_loss:2.389, val_acc:0.506]
Epoch [3/120    avg_loss:2.158, val_acc:0.588]
Epoch [4/120    avg_loss:1.989, val_acc:0.578]
Epoch [5/120    avg_loss:1.868, val_acc:0.599]
Epoch [6/120    avg_loss:1.784, val_acc:0.609]
Epoch [7/120    avg_loss:1.664, val_acc:0.612]
Epoch [8/120    avg_loss:1.492, val_acc:0.645]
Epoch [9/120    avg_loss:1.409, val_acc:0.671]
Epoch [10/120    avg_loss:1.264, val_acc:0.658]
Epoch [11/120    avg_loss:1.171, val_acc:0.717]
Epoch [12/120    avg_loss:1.012, val_acc:0.728]
Epoch [13/120    avg_loss:0.923, val_acc:0.762]
Epoch [14/120    avg_loss:0.817, val_acc:0.773]
Epoch [15/120    avg_loss:0.800, val_acc:0.773]
Epoch [16/120    avg_loss:0.715, val_acc:0.783]
Epoch [17/120    avg_loss:0.696, val_acc:0.794]
Epoch [18/120    avg_loss:0.642, val_acc:0.782]
Epoch [19/120    avg_loss:0.611, val_acc:0.811]
Epoch [20/120    avg_loss:0.514, val_acc:0.830]
Epoch [21/120    avg_loss:0.478, val_acc:0.853]
Epoch [22/120    avg_loss:0.448, val_acc:0.835]
Epoch [23/120    avg_loss:0.438, val_acc:0.846]
Epoch [24/120    avg_loss:0.441, val_acc:0.838]
Epoch [25/120    avg_loss:0.370, val_acc:0.881]
Epoch [26/120    avg_loss:0.305, val_acc:0.876]
Epoch [27/120    avg_loss:0.297, val_acc:0.866]
Epoch [28/120    avg_loss:0.261, val_acc:0.896]
Epoch [29/120    avg_loss:0.236, val_acc:0.867]
Epoch [30/120    avg_loss:0.260, val_acc:0.897]
Epoch [31/120    avg_loss:0.242, val_acc:0.911]
Epoch [32/120    avg_loss:0.241, val_acc:0.892]
Epoch [33/120    avg_loss:0.208, val_acc:0.895]
Epoch [34/120    avg_loss:0.213, val_acc:0.902]
Epoch [35/120    avg_loss:0.263, val_acc:0.874]
Epoch [36/120    avg_loss:0.195, val_acc:0.911]
Epoch [37/120    avg_loss:0.345, val_acc:0.901]
Epoch [38/120    avg_loss:0.190, val_acc:0.923]
Epoch [39/120    avg_loss:0.182, val_acc:0.922]
Epoch [40/120    avg_loss:0.171, val_acc:0.910]
Epoch [41/120    avg_loss:0.168, val_acc:0.913]
Epoch [42/120    avg_loss:0.163, val_acc:0.948]
Epoch [43/120    avg_loss:0.134, val_acc:0.922]
Epoch [44/120    avg_loss:0.145, val_acc:0.907]
Epoch [45/120    avg_loss:0.125, val_acc:0.936]
Epoch [46/120    avg_loss:0.113, val_acc:0.940]
Epoch [47/120    avg_loss:0.110, val_acc:0.924]
Epoch [48/120    avg_loss:0.102, val_acc:0.924]
Epoch [49/120    avg_loss:0.095, val_acc:0.949]
Epoch [50/120    avg_loss:0.105, val_acc:0.939]
Epoch [51/120    avg_loss:0.104, val_acc:0.920]
Epoch [52/120    avg_loss:0.086, val_acc:0.945]
Epoch [53/120    avg_loss:0.094, val_acc:0.934]
Epoch [54/120    avg_loss:0.104, val_acc:0.939]
Epoch [55/120    avg_loss:0.100, val_acc:0.934]
Epoch [56/120    avg_loss:0.085, val_acc:0.923]
Epoch [57/120    avg_loss:0.088, val_acc:0.944]
Epoch [58/120    avg_loss:0.072, val_acc:0.958]
Epoch [59/120    avg_loss:0.063, val_acc:0.951]
Epoch [60/120    avg_loss:0.068, val_acc:0.951]
Epoch [61/120    avg_loss:0.072, val_acc:0.952]
Epoch [62/120    avg_loss:0.079, val_acc:0.950]
Epoch [63/120    avg_loss:0.074, val_acc:0.950]
Epoch [64/120    avg_loss:0.059, val_acc:0.953]
Epoch [65/120    avg_loss:0.067, val_acc:0.953]
Epoch [66/120    avg_loss:0.055, val_acc:0.962]
Epoch [67/120    avg_loss:0.043, val_acc:0.959]
Epoch [68/120    avg_loss:0.038, val_acc:0.970]
Epoch [69/120    avg_loss:0.048, val_acc:0.962]
Epoch [70/120    avg_loss:0.050, val_acc:0.953]
Epoch [71/120    avg_loss:0.043, val_acc:0.974]
Epoch [72/120    avg_loss:0.038, val_acc:0.960]
Epoch [73/120    avg_loss:0.058, val_acc:0.963]
Epoch [74/120    avg_loss:0.053, val_acc:0.968]
Epoch [75/120    avg_loss:0.059, val_acc:0.959]
Epoch [76/120    avg_loss:0.071, val_acc:0.944]
Epoch [77/120    avg_loss:0.137, val_acc:0.938]
Epoch [78/120    avg_loss:0.104, val_acc:0.946]
Epoch [79/120    avg_loss:0.079, val_acc:0.959]
Epoch [80/120    avg_loss:0.079, val_acc:0.962]
Epoch [81/120    avg_loss:0.058, val_acc:0.964]
Epoch [82/120    avg_loss:0.073, val_acc:0.954]
Epoch [83/120    avg_loss:0.064, val_acc:0.969]
Epoch [84/120    avg_loss:0.049, val_acc:0.961]
Epoch [85/120    avg_loss:0.038, val_acc:0.965]
Epoch [86/120    avg_loss:0.031, val_acc:0.964]
Epoch [87/120    avg_loss:0.026, val_acc:0.970]
Epoch [88/120    avg_loss:0.031, val_acc:0.971]
Epoch [89/120    avg_loss:0.030, val_acc:0.970]
Epoch [90/120    avg_loss:0.030, val_acc:0.972]
Epoch [91/120    avg_loss:0.030, val_acc:0.973]
Epoch [92/120    avg_loss:0.028, val_acc:0.974]
Epoch [93/120    avg_loss:0.034, val_acc:0.973]
Epoch [94/120    avg_loss:0.029, val_acc:0.971]
Epoch [95/120    avg_loss:0.025, val_acc:0.973]
Epoch [96/120    avg_loss:0.024, val_acc:0.975]
Epoch [97/120    avg_loss:0.023, val_acc:0.974]
Epoch [98/120    avg_loss:0.029, val_acc:0.971]
Epoch [99/120    avg_loss:0.025, val_acc:0.973]
Epoch [100/120    avg_loss:0.032, val_acc:0.972]
Epoch [101/120    avg_loss:0.025, val_acc:0.973]
Epoch [102/120    avg_loss:0.022, val_acc:0.974]
Epoch [103/120    avg_loss:0.026, val_acc:0.973]
Epoch [104/120    avg_loss:0.030, val_acc:0.972]
Epoch [105/120    avg_loss:0.024, val_acc:0.974]
Epoch [106/120    avg_loss:0.030, val_acc:0.975]
Epoch [107/120    avg_loss:0.022, val_acc:0.974]
Epoch [108/120    avg_loss:0.032, val_acc:0.977]
Epoch [109/120    avg_loss:0.027, val_acc:0.977]
Epoch [110/120    avg_loss:0.025, val_acc:0.975]
Epoch [111/120    avg_loss:0.024, val_acc:0.980]
Epoch [112/120    avg_loss:0.024, val_acc:0.978]
Epoch [113/120    avg_loss:0.022, val_acc:0.977]
Epoch [114/120    avg_loss:0.022, val_acc:0.981]
Epoch [115/120    avg_loss:0.022, val_acc:0.979]
Epoch [116/120    avg_loss:0.022, val_acc:0.979]
Epoch [117/120    avg_loss:0.022, val_acc:0.977]
Epoch [118/120    avg_loss:0.021, val_acc:0.978]
Epoch [119/120    avg_loss:0.024, val_acc:0.974]
Epoch [120/120    avg_loss:0.022, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1240    4    5    0    1    0    0    1   13   18    3    0
     0    0    0]
 [   0    0    0  697    3   12    0    0    0    9    1    0   25    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    3    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   27   64    0    6    1    0    0    0  767    6    0    0
     0    4    0]
 [   0   12   18    0    0    0    7    0    0    0   15 2136   15    4
     3    0    0]
 [   0    0    1   11    9   10    0    0    0    0    5   16  475    0
     3    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   14    0    0    1    0    3    1    0    0
  1120    0    0]
 [   0    0    0    0    0    1   34    0    0    3    0    0    0    0
    60  249    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
94.87262872628726

F1 scores:
[       nan 0.82222222 0.96460521 0.91469816 0.95927602 0.94597574
 0.96602659 1.         0.99883856 0.64       0.91146762 0.97312073
 0.8979206  0.98930481 0.96219931 0.83       0.95238095]

Kappa:
0.9415704783620856
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc71fd3a828>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.696, val_acc:0.353]
Epoch [2/120    avg_loss:2.364, val_acc:0.358]
Epoch [3/120    avg_loss:2.117, val_acc:0.453]
Epoch [4/120    avg_loss:1.987, val_acc:0.484]
Epoch [5/120    avg_loss:1.829, val_acc:0.554]
Epoch [6/120    avg_loss:1.676, val_acc:0.587]
Epoch [7/120    avg_loss:1.565, val_acc:0.600]
Epoch [8/120    avg_loss:1.445, val_acc:0.652]
Epoch [9/120    avg_loss:1.308, val_acc:0.634]
Epoch [10/120    avg_loss:1.248, val_acc:0.647]
Epoch [11/120    avg_loss:1.125, val_acc:0.680]
Epoch [12/120    avg_loss:1.027, val_acc:0.727]
Epoch [13/120    avg_loss:0.916, val_acc:0.723]
Epoch [14/120    avg_loss:0.840, val_acc:0.738]
Epoch [15/120    avg_loss:0.765, val_acc:0.762]
Epoch [16/120    avg_loss:0.703, val_acc:0.763]
Epoch [17/120    avg_loss:0.619, val_acc:0.789]
Epoch [18/120    avg_loss:0.601, val_acc:0.786]
Epoch [19/120    avg_loss:0.554, val_acc:0.799]
Epoch [20/120    avg_loss:0.506, val_acc:0.795]
Epoch [21/120    avg_loss:0.457, val_acc:0.826]
Epoch [22/120    avg_loss:0.425, val_acc:0.833]
Epoch [23/120    avg_loss:0.419, val_acc:0.789]
Epoch [24/120    avg_loss:0.426, val_acc:0.823]
Epoch [25/120    avg_loss:0.368, val_acc:0.862]
Epoch [26/120    avg_loss:0.348, val_acc:0.850]
Epoch [27/120    avg_loss:0.318, val_acc:0.858]
Epoch [28/120    avg_loss:0.337, val_acc:0.843]
Epoch [29/120    avg_loss:0.503, val_acc:0.837]
Epoch [30/120    avg_loss:0.314, val_acc:0.865]
Epoch [31/120    avg_loss:0.271, val_acc:0.855]
Epoch [32/120    avg_loss:0.253, val_acc:0.856]
Epoch [33/120    avg_loss:0.236, val_acc:0.891]
Epoch [34/120    avg_loss:0.229, val_acc:0.866]
Epoch [35/120    avg_loss:0.192, val_acc:0.886]
Epoch [36/120    avg_loss:0.184, val_acc:0.901]
Epoch [37/120    avg_loss:0.184, val_acc:0.902]
Epoch [38/120    avg_loss:0.204, val_acc:0.850]
Epoch [39/120    avg_loss:0.185, val_acc:0.906]
Epoch [40/120    avg_loss:0.153, val_acc:0.905]
Epoch [41/120    avg_loss:0.133, val_acc:0.902]
Epoch [42/120    avg_loss:0.130, val_acc:0.917]
Epoch [43/120    avg_loss:0.123, val_acc:0.911]
Epoch [44/120    avg_loss:0.127, val_acc:0.925]
Epoch [45/120    avg_loss:0.127, val_acc:0.906]
Epoch [46/120    avg_loss:0.115, val_acc:0.917]
Epoch [47/120    avg_loss:0.109, val_acc:0.926]
Epoch [48/120    avg_loss:0.123, val_acc:0.920]
Epoch [49/120    avg_loss:0.107, val_acc:0.925]
Epoch [50/120    avg_loss:0.100, val_acc:0.927]
Epoch [51/120    avg_loss:0.095, val_acc:0.929]
Epoch [52/120    avg_loss:0.113, val_acc:0.904]
Epoch [53/120    avg_loss:0.101, val_acc:0.935]
Epoch [54/120    avg_loss:0.075, val_acc:0.933]
Epoch [55/120    avg_loss:0.079, val_acc:0.936]
Epoch [56/120    avg_loss:0.073, val_acc:0.938]
Epoch [57/120    avg_loss:0.090, val_acc:0.921]
Epoch [58/120    avg_loss:0.091, val_acc:0.929]
Epoch [59/120    avg_loss:0.076, val_acc:0.924]
Epoch [60/120    avg_loss:0.072, val_acc:0.943]
Epoch [61/120    avg_loss:0.060, val_acc:0.929]
Epoch [62/120    avg_loss:0.070, val_acc:0.934]
Epoch [63/120    avg_loss:0.090, val_acc:0.932]
Epoch [64/120    avg_loss:0.076, val_acc:0.939]
Epoch [65/120    avg_loss:0.065, val_acc:0.925]
Epoch [66/120    avg_loss:0.047, val_acc:0.941]
Epoch [67/120    avg_loss:0.054, val_acc:0.934]
Epoch [68/120    avg_loss:0.056, val_acc:0.940]
Epoch [69/120    avg_loss:0.055, val_acc:0.944]
Epoch [70/120    avg_loss:0.055, val_acc:0.945]
Epoch [71/120    avg_loss:0.045, val_acc:0.943]
Epoch [72/120    avg_loss:0.037, val_acc:0.952]
Epoch [73/120    avg_loss:0.038, val_acc:0.944]
Epoch [74/120    avg_loss:0.052, val_acc:0.949]
Epoch [75/120    avg_loss:0.052, val_acc:0.941]
Epoch [76/120    avg_loss:0.078, val_acc:0.920]
Epoch [77/120    avg_loss:0.075, val_acc:0.939]
Epoch [78/120    avg_loss:0.053, val_acc:0.939]
Epoch [79/120    avg_loss:0.045, val_acc:0.942]
Epoch [80/120    avg_loss:0.055, val_acc:0.942]
Epoch [81/120    avg_loss:0.044, val_acc:0.946]
Epoch [82/120    avg_loss:0.043, val_acc:0.941]
Epoch [83/120    avg_loss:0.043, val_acc:0.946]
Epoch [84/120    avg_loss:0.100, val_acc:0.926]
Epoch [85/120    avg_loss:0.098, val_acc:0.929]
Epoch [86/120    avg_loss:0.065, val_acc:0.945]
Epoch [87/120    avg_loss:0.032, val_acc:0.945]
Epoch [88/120    avg_loss:0.035, val_acc:0.949]
Epoch [89/120    avg_loss:0.034, val_acc:0.952]
Epoch [90/120    avg_loss:0.030, val_acc:0.949]
Epoch [91/120    avg_loss:0.036, val_acc:0.951]
Epoch [92/120    avg_loss:0.030, val_acc:0.953]
Epoch [93/120    avg_loss:0.029, val_acc:0.952]
Epoch [94/120    avg_loss:0.029, val_acc:0.952]
Epoch [95/120    avg_loss:0.028, val_acc:0.954]
Epoch [96/120    avg_loss:0.027, val_acc:0.955]
Epoch [97/120    avg_loss:0.027, val_acc:0.956]
Epoch [98/120    avg_loss:0.026, val_acc:0.960]
Epoch [99/120    avg_loss:0.031, val_acc:0.954]
Epoch [100/120    avg_loss:0.027, val_acc:0.956]
Epoch [101/120    avg_loss:0.026, val_acc:0.959]
Epoch [102/120    avg_loss:0.025, val_acc:0.960]
Epoch [103/120    avg_loss:0.026, val_acc:0.956]
Epoch [104/120    avg_loss:0.029, val_acc:0.959]
Epoch [105/120    avg_loss:0.023, val_acc:0.963]
Epoch [106/120    avg_loss:0.025, val_acc:0.960]
Epoch [107/120    avg_loss:0.027, val_acc:0.963]
Epoch [108/120    avg_loss:0.025, val_acc:0.960]
Epoch [109/120    avg_loss:0.023, val_acc:0.956]
Epoch [110/120    avg_loss:0.028, val_acc:0.959]
Epoch [111/120    avg_loss:0.025, val_acc:0.959]
Epoch [112/120    avg_loss:0.022, val_acc:0.959]
Epoch [113/120    avg_loss:0.024, val_acc:0.960]
Epoch [114/120    avg_loss:0.027, val_acc:0.959]
Epoch [115/120    avg_loss:0.019, val_acc:0.960]
Epoch [116/120    avg_loss:0.024, val_acc:0.964]
Epoch [117/120    avg_loss:0.026, val_acc:0.958]
Epoch [118/120    avg_loss:0.020, val_acc:0.958]
Epoch [119/120    avg_loss:0.029, val_acc:0.961]
Epoch [120/120    avg_loss:0.021, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1239    1    0    1    0    0    0    0   11   33    0    0
     0    0    0]
 [   0    0    1  704    6   16    0    0    0   13    0    1    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    3    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0    9   46    0    9    0    0    0    0  794   13    0    0
     0    4    0]
 [   0    0    8    0    0    0    2    1    0    0   17 2178    0    2
     2    0    0]
 [   0    0    8   39    8   16    0    0    0    1    7    0  448    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   15    0    0    0    0    3    0    0    0
  1121    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    62  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.85907859078591

F1 scores:
[       nan 0.975      0.97176471 0.91607027 0.96818182 0.93159609
 0.99542683 0.98039216 1.         0.62745098 0.92919836 0.9813021
 0.90505051 0.99462366 0.96347228 0.89622642 0.96      ]

Kappa:
0.9527697459071436
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f84be735860>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.652, val_acc:0.365]
Epoch [2/120    avg_loss:2.340, val_acc:0.493]
Epoch [3/120    avg_loss:2.182, val_acc:0.520]
Epoch [4/120    avg_loss:2.024, val_acc:0.586]
Epoch [5/120    avg_loss:1.923, val_acc:0.587]
Epoch [6/120    avg_loss:1.757, val_acc:0.605]
Epoch [7/120    avg_loss:1.592, val_acc:0.596]
Epoch [8/120    avg_loss:1.511, val_acc:0.612]
Epoch [9/120    avg_loss:1.380, val_acc:0.674]
Epoch [10/120    avg_loss:1.193, val_acc:0.684]
Epoch [11/120    avg_loss:1.102, val_acc:0.706]
Epoch [12/120    avg_loss:0.984, val_acc:0.710]
Epoch [13/120    avg_loss:0.939, val_acc:0.759]
Epoch [14/120    avg_loss:0.850, val_acc:0.729]
Epoch [15/120    avg_loss:0.761, val_acc:0.771]
Epoch [16/120    avg_loss:0.690, val_acc:0.792]
Epoch [17/120    avg_loss:0.630, val_acc:0.825]
Epoch [18/120    avg_loss:0.616, val_acc:0.812]
Epoch [19/120    avg_loss:0.521, val_acc:0.815]
Epoch [20/120    avg_loss:0.473, val_acc:0.850]
Epoch [21/120    avg_loss:0.490, val_acc:0.816]
Epoch [22/120    avg_loss:0.480, val_acc:0.805]
Epoch [23/120    avg_loss:0.447, val_acc:0.826]
Epoch [24/120    avg_loss:0.366, val_acc:0.864]
Epoch [25/120    avg_loss:0.412, val_acc:0.846]
Epoch [26/120    avg_loss:0.485, val_acc:0.821]
Epoch [27/120    avg_loss:0.382, val_acc:0.867]
Epoch [28/120    avg_loss:0.333, val_acc:0.867]
Epoch [29/120    avg_loss:0.285, val_acc:0.911]
Epoch [30/120    avg_loss:0.248, val_acc:0.882]
Epoch [31/120    avg_loss:0.246, val_acc:0.910]
Epoch [32/120    avg_loss:0.234, val_acc:0.873]
Epoch [33/120    avg_loss:0.291, val_acc:0.908]
Epoch [34/120    avg_loss:0.197, val_acc:0.924]
Epoch [35/120    avg_loss:0.187, val_acc:0.898]
Epoch [36/120    avg_loss:0.196, val_acc:0.886]
Epoch [37/120    avg_loss:0.268, val_acc:0.927]
Epoch [38/120    avg_loss:0.190, val_acc:0.935]
Epoch [39/120    avg_loss:0.163, val_acc:0.933]
Epoch [40/120    avg_loss:0.168, val_acc:0.913]
Epoch [41/120    avg_loss:0.173, val_acc:0.942]
Epoch [42/120    avg_loss:0.136, val_acc:0.931]
Epoch [43/120    avg_loss:0.139, val_acc:0.936]
Epoch [44/120    avg_loss:0.115, val_acc:0.951]
Epoch [45/120    avg_loss:0.129, val_acc:0.921]
Epoch [46/120    avg_loss:0.111, val_acc:0.945]
Epoch [47/120    avg_loss:0.106, val_acc:0.938]
Epoch [48/120    avg_loss:0.146, val_acc:0.940]
Epoch [49/120    avg_loss:0.125, val_acc:0.923]
Epoch [50/120    avg_loss:0.130, val_acc:0.949]
Epoch [51/120    avg_loss:0.144, val_acc:0.943]
Epoch [52/120    avg_loss:0.118, val_acc:0.950]
Epoch [53/120    avg_loss:0.100, val_acc:0.949]
Epoch [54/120    avg_loss:0.079, val_acc:0.955]
Epoch [55/120    avg_loss:0.077, val_acc:0.959]
Epoch [56/120    avg_loss:0.071, val_acc:0.952]
Epoch [57/120    avg_loss:0.088, val_acc:0.960]
Epoch [58/120    avg_loss:0.087, val_acc:0.950]
Epoch [59/120    avg_loss:0.103, val_acc:0.955]
Epoch [60/120    avg_loss:0.084, val_acc:0.944]
Epoch [61/120    avg_loss:0.082, val_acc:0.949]
Epoch [62/120    avg_loss:0.053, val_acc:0.960]
Epoch [63/120    avg_loss:0.054, val_acc:0.954]
Epoch [64/120    avg_loss:0.061, val_acc:0.956]
Epoch [65/120    avg_loss:0.057, val_acc:0.952]
Epoch [66/120    avg_loss:0.066, val_acc:0.951]
Epoch [67/120    avg_loss:0.072, val_acc:0.954]
Epoch [68/120    avg_loss:0.064, val_acc:0.950]
Epoch [69/120    avg_loss:0.055, val_acc:0.949]
Epoch [70/120    avg_loss:0.052, val_acc:0.965]
Epoch [71/120    avg_loss:0.053, val_acc:0.963]
Epoch [72/120    avg_loss:0.061, val_acc:0.954]
Epoch [73/120    avg_loss:0.051, val_acc:0.952]
Epoch [74/120    avg_loss:0.167, val_acc:0.879]
Epoch [75/120    avg_loss:0.654, val_acc:0.821]
Epoch [76/120    avg_loss:0.305, val_acc:0.903]
Epoch [77/120    avg_loss:0.180, val_acc:0.936]
Epoch [78/120    avg_loss:0.120, val_acc:0.945]
Epoch [79/120    avg_loss:0.108, val_acc:0.931]
Epoch [80/120    avg_loss:0.097, val_acc:0.942]
Epoch [81/120    avg_loss:0.084, val_acc:0.942]
Epoch [82/120    avg_loss:0.078, val_acc:0.945]
Epoch [83/120    avg_loss:0.079, val_acc:0.951]
Epoch [84/120    avg_loss:0.062, val_acc:0.955]
Epoch [85/120    avg_loss:0.057, val_acc:0.961]
Epoch [86/120    avg_loss:0.052, val_acc:0.962]
Epoch [87/120    avg_loss:0.046, val_acc:0.960]
Epoch [88/120    avg_loss:0.044, val_acc:0.960]
Epoch [89/120    avg_loss:0.038, val_acc:0.961]
Epoch [90/120    avg_loss:0.046, val_acc:0.961]
Epoch [91/120    avg_loss:0.047, val_acc:0.960]
Epoch [92/120    avg_loss:0.043, val_acc:0.961]
Epoch [93/120    avg_loss:0.039, val_acc:0.961]
Epoch [94/120    avg_loss:0.042, val_acc:0.962]
Epoch [95/120    avg_loss:0.044, val_acc:0.960]
Epoch [96/120    avg_loss:0.038, val_acc:0.962]
Epoch [97/120    avg_loss:0.040, val_acc:0.962]
Epoch [98/120    avg_loss:0.039, val_acc:0.962]
Epoch [99/120    avg_loss:0.038, val_acc:0.961]
Epoch [100/120    avg_loss:0.044, val_acc:0.961]
Epoch [101/120    avg_loss:0.045, val_acc:0.961]
Epoch [102/120    avg_loss:0.040, val_acc:0.962]
Epoch [103/120    avg_loss:0.032, val_acc:0.961]
Epoch [104/120    avg_loss:0.033, val_acc:0.960]
Epoch [105/120    avg_loss:0.037, val_acc:0.961]
Epoch [106/120    avg_loss:0.038, val_acc:0.961]
Epoch [107/120    avg_loss:0.040, val_acc:0.960]
Epoch [108/120    avg_loss:0.039, val_acc:0.961]
Epoch [109/120    avg_loss:0.042, val_acc:0.961]
Epoch [110/120    avg_loss:0.038, val_acc:0.961]
Epoch [111/120    avg_loss:0.034, val_acc:0.961]
Epoch [112/120    avg_loss:0.044, val_acc:0.961]
Epoch [113/120    avg_loss:0.034, val_acc:0.961]
Epoch [114/120    avg_loss:0.040, val_acc:0.961]
Epoch [115/120    avg_loss:0.038, val_acc:0.961]
Epoch [116/120    avg_loss:0.036, val_acc:0.961]
Epoch [117/120    avg_loss:0.038, val_acc:0.961]
Epoch [118/120    avg_loss:0.039, val_acc:0.961]
Epoch [119/120    avg_loss:0.038, val_acc:0.961]
Epoch [120/120    avg_loss:0.042, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1218    2    0    0    8    0    0    0   10   44    3    0
     0    0    0]
 [   0    0    8  680    2   26    0    0    0   16    0    0    8    7
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    3    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   49   54    0    9    0    0    0    0  731   25    0    0
     0    7    0]
 [   0    0   27    0    0    0    6    0    0    0    9 2163    1    4
     0    0    0]
 [   0    0    2    7   19   11    0    0    0    0    7    0  482    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    3    2    0    0
  1133    0    0]
 [   0    0    0    0    0    0   12    0    0    2    0    0    0    0
    66  267    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.81842818428184

F1 scores:
[       nan 0.94871795 0.94090382 0.91275168 0.95302013 0.94182217
 0.97754491 1.         1.         0.5483871  0.89200732 0.97322835
 0.93683188 0.97112861 0.96920445 0.85990338 0.96551724]

Kappa:
0.9408926411549358
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2487f727f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.766, val_acc:0.441]
Epoch [2/120    avg_loss:2.376, val_acc:0.417]
Epoch [3/120    avg_loss:2.189, val_acc:0.545]
Epoch [4/120    avg_loss:1.979, val_acc:0.529]
Epoch [5/120    avg_loss:1.905, val_acc:0.608]
Epoch [6/120    avg_loss:1.760, val_acc:0.617]
Epoch [7/120    avg_loss:1.622, val_acc:0.629]
Epoch [8/120    avg_loss:1.525, val_acc:0.676]
Epoch [9/120    avg_loss:1.356, val_acc:0.672]
Epoch [10/120    avg_loss:1.209, val_acc:0.710]
Epoch [11/120    avg_loss:1.076, val_acc:0.699]
Epoch [12/120    avg_loss:1.019, val_acc:0.749]
Epoch [13/120    avg_loss:0.899, val_acc:0.772]
Epoch [14/120    avg_loss:0.826, val_acc:0.818]
Epoch [15/120    avg_loss:0.718, val_acc:0.809]
Epoch [16/120    avg_loss:0.754, val_acc:0.808]
Epoch [17/120    avg_loss:0.654, val_acc:0.811]
Epoch [18/120    avg_loss:0.567, val_acc:0.829]
Epoch [19/120    avg_loss:0.593, val_acc:0.811]
Epoch [20/120    avg_loss:0.570, val_acc:0.852]
Epoch [21/120    avg_loss:0.477, val_acc:0.859]
Epoch [22/120    avg_loss:0.370, val_acc:0.866]
Epoch [23/120    avg_loss:0.416, val_acc:0.888]
Epoch [24/120    avg_loss:0.362, val_acc:0.875]
Epoch [25/120    avg_loss:0.337, val_acc:0.863]
Epoch [26/120    avg_loss:0.316, val_acc:0.905]
Epoch [27/120    avg_loss:0.348, val_acc:0.860]
Epoch [28/120    avg_loss:0.370, val_acc:0.871]
Epoch [29/120    avg_loss:0.329, val_acc:0.901]
Epoch [30/120    avg_loss:0.268, val_acc:0.902]
Epoch [31/120    avg_loss:0.267, val_acc:0.906]
Epoch [32/120    avg_loss:0.188, val_acc:0.920]
Epoch [33/120    avg_loss:0.178, val_acc:0.934]
Epoch [34/120    avg_loss:0.180, val_acc:0.946]
Epoch [35/120    avg_loss:0.175, val_acc:0.940]
Epoch [36/120    avg_loss:0.151, val_acc:0.923]
Epoch [37/120    avg_loss:0.136, val_acc:0.934]
Epoch [38/120    avg_loss:0.153, val_acc:0.926]
Epoch [39/120    avg_loss:0.148, val_acc:0.931]
Epoch [40/120    avg_loss:0.192, val_acc:0.932]
Epoch [41/120    avg_loss:0.161, val_acc:0.934]
Epoch [42/120    avg_loss:0.152, val_acc:0.941]
Epoch [43/120    avg_loss:0.133, val_acc:0.940]
Epoch [44/120    avg_loss:0.146, val_acc:0.938]
Epoch [45/120    avg_loss:0.130, val_acc:0.935]
Epoch [46/120    avg_loss:0.115, val_acc:0.949]
Epoch [47/120    avg_loss:0.103, val_acc:0.949]
Epoch [48/120    avg_loss:0.099, val_acc:0.949]
Epoch [49/120    avg_loss:0.090, val_acc:0.946]
Epoch [50/120    avg_loss:0.105, val_acc:0.960]
Epoch [51/120    avg_loss:0.095, val_acc:0.946]
Epoch [52/120    avg_loss:0.074, val_acc:0.955]
Epoch [53/120    avg_loss:0.089, val_acc:0.956]
Epoch [54/120    avg_loss:0.070, val_acc:0.951]
Epoch [55/120    avg_loss:0.080, val_acc:0.953]
Epoch [56/120    avg_loss:0.085, val_acc:0.955]
Epoch [57/120    avg_loss:0.070, val_acc:0.948]
Epoch [58/120    avg_loss:0.067, val_acc:0.961]
Epoch [59/120    avg_loss:0.065, val_acc:0.962]
Epoch [60/120    avg_loss:0.085, val_acc:0.952]
Epoch [61/120    avg_loss:0.068, val_acc:0.951]
Epoch [62/120    avg_loss:0.072, val_acc:0.959]
Epoch [63/120    avg_loss:0.065, val_acc:0.960]
Epoch [64/120    avg_loss:0.059, val_acc:0.959]
Epoch [65/120    avg_loss:0.061, val_acc:0.959]
Epoch [66/120    avg_loss:0.047, val_acc:0.955]
Epoch [67/120    avg_loss:0.060, val_acc:0.959]
Epoch [68/120    avg_loss:0.054, val_acc:0.950]
Epoch [69/120    avg_loss:0.046, val_acc:0.950]
Epoch [70/120    avg_loss:0.038, val_acc:0.962]
Epoch [71/120    avg_loss:0.034, val_acc:0.962]
Epoch [72/120    avg_loss:0.049, val_acc:0.960]
Epoch [73/120    avg_loss:0.041, val_acc:0.971]
Epoch [74/120    avg_loss:0.033, val_acc:0.962]
Epoch [75/120    avg_loss:0.034, val_acc:0.959]
Epoch [76/120    avg_loss:0.038, val_acc:0.962]
Epoch [77/120    avg_loss:0.034, val_acc:0.950]
Epoch [78/120    avg_loss:0.044, val_acc:0.959]
Epoch [79/120    avg_loss:0.031, val_acc:0.958]
Epoch [80/120    avg_loss:0.031, val_acc:0.963]
Epoch [81/120    avg_loss:0.030, val_acc:0.962]
Epoch [82/120    avg_loss:0.047, val_acc:0.968]
Epoch [83/120    avg_loss:0.042, val_acc:0.953]
Epoch [84/120    avg_loss:0.087, val_acc:0.950]
Epoch [85/120    avg_loss:0.046, val_acc:0.960]
Epoch [86/120    avg_loss:0.055, val_acc:0.952]
Epoch [87/120    avg_loss:0.049, val_acc:0.959]
Epoch [88/120    avg_loss:0.043, val_acc:0.962]
Epoch [89/120    avg_loss:0.037, val_acc:0.962]
Epoch [90/120    avg_loss:0.032, val_acc:0.959]
Epoch [91/120    avg_loss:0.032, val_acc:0.959]
Epoch [92/120    avg_loss:0.028, val_acc:0.962]
Epoch [93/120    avg_loss:0.034, val_acc:0.967]
Epoch [94/120    avg_loss:0.028, val_acc:0.964]
Epoch [95/120    avg_loss:0.026, val_acc:0.963]
Epoch [96/120    avg_loss:0.022, val_acc:0.964]
Epoch [97/120    avg_loss:0.027, val_acc:0.965]
Epoch [98/120    avg_loss:0.022, val_acc:0.965]
Epoch [99/120    avg_loss:0.025, val_acc:0.967]
Epoch [100/120    avg_loss:0.022, val_acc:0.968]
Epoch [101/120    avg_loss:0.022, val_acc:0.968]
Epoch [102/120    avg_loss:0.026, val_acc:0.967]
Epoch [103/120    avg_loss:0.026, val_acc:0.967]
Epoch [104/120    avg_loss:0.023, val_acc:0.967]
Epoch [105/120    avg_loss:0.021, val_acc:0.968]
Epoch [106/120    avg_loss:0.023, val_acc:0.968]
Epoch [107/120    avg_loss:0.022, val_acc:0.967]
Epoch [108/120    avg_loss:0.024, val_acc:0.967]
Epoch [109/120    avg_loss:0.022, val_acc:0.967]
Epoch [110/120    avg_loss:0.020, val_acc:0.967]
Epoch [111/120    avg_loss:0.020, val_acc:0.968]
Epoch [112/120    avg_loss:0.020, val_acc:0.968]
Epoch [113/120    avg_loss:0.021, val_acc:0.968]
Epoch [114/120    avg_loss:0.024, val_acc:0.967]
Epoch [115/120    avg_loss:0.024, val_acc:0.967]
Epoch [116/120    avg_loss:0.023, val_acc:0.967]
Epoch [117/120    avg_loss:0.022, val_acc:0.968]
Epoch [118/120    avg_loss:0.020, val_acc:0.967]
Epoch [119/120    avg_loss:0.025, val_acc:0.967]
Epoch [120/120    avg_loss:0.025, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1231    0    0    1    0    0    0    0    8   40    5    0
     0    0    0]
 [   0    0    1  706    0   16    0    0    0   15    0    0    7    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    5    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    8   40    0    5    0    0    0    0  795   16    3    0
     1    7    0]
 [   0    0    5    0    0    0    3    0    6    0    9 2184    2    1
     0    0    0]
 [   0    0    0   24    4    3    0    0    0    1    0    6  492    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   14    0    0    3    0    4    1    0    0
  1117    0    0]
 [   0    0    0    0    0    0   27    0    0    0    0    0    0    0
    42  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.2059620596206

F1 scores:
[       nan 0.96202532 0.97312253 0.93078444 0.99069767 0.94666667
 0.97691735 0.90909091 0.98964327 0.61818182 0.93860685 0.97981157
 0.94072658 0.9919571  0.97172684 0.87974684 0.96470588]

Kappa:
0.956727531044403
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f339a357780>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.714, val_acc:0.281]
Epoch [2/120    avg_loss:2.375, val_acc:0.480]
Epoch [3/120    avg_loss:2.135, val_acc:0.519]
Epoch [4/120    avg_loss:2.035, val_acc:0.547]
Epoch [5/120    avg_loss:1.905, val_acc:0.550]
Epoch [6/120    avg_loss:1.744, val_acc:0.571]
Epoch [7/120    avg_loss:1.617, val_acc:0.605]
Epoch [8/120    avg_loss:1.486, val_acc:0.635]
Epoch [9/120    avg_loss:1.369, val_acc:0.634]
Epoch [10/120    avg_loss:1.233, val_acc:0.653]
Epoch [11/120    avg_loss:1.156, val_acc:0.680]
Epoch [12/120    avg_loss:1.047, val_acc:0.681]
Epoch [13/120    avg_loss:0.994, val_acc:0.621]
Epoch [14/120    avg_loss:0.898, val_acc:0.715]
Epoch [15/120    avg_loss:0.772, val_acc:0.766]
Epoch [16/120    avg_loss:0.748, val_acc:0.746]
Epoch [17/120    avg_loss:0.761, val_acc:0.775]
Epoch [18/120    avg_loss:0.668, val_acc:0.786]
Epoch [19/120    avg_loss:0.595, val_acc:0.812]
Epoch [20/120    avg_loss:0.566, val_acc:0.825]
Epoch [21/120    avg_loss:0.494, val_acc:0.840]
Epoch [22/120    avg_loss:0.511, val_acc:0.850]
Epoch [23/120    avg_loss:0.481, val_acc:0.830]
Epoch [24/120    avg_loss:0.469, val_acc:0.859]
Epoch [25/120    avg_loss:0.485, val_acc:0.815]
Epoch [26/120    avg_loss:0.615, val_acc:0.823]
Epoch [27/120    avg_loss:0.438, val_acc:0.854]
Epoch [28/120    avg_loss:0.372, val_acc:0.833]
Epoch [29/120    avg_loss:0.326, val_acc:0.868]
Epoch [30/120    avg_loss:0.306, val_acc:0.871]
Epoch [31/120    avg_loss:0.321, val_acc:0.877]
Epoch [32/120    avg_loss:0.265, val_acc:0.906]
Epoch [33/120    avg_loss:0.195, val_acc:0.922]
Epoch [34/120    avg_loss:0.234, val_acc:0.892]
Epoch [35/120    avg_loss:0.228, val_acc:0.913]
Epoch [36/120    avg_loss:0.193, val_acc:0.908]
Epoch [37/120    avg_loss:0.183, val_acc:0.916]
Epoch [38/120    avg_loss:0.195, val_acc:0.923]
Epoch [39/120    avg_loss:0.155, val_acc:0.925]
Epoch [40/120    avg_loss:0.138, val_acc:0.929]
Epoch [41/120    avg_loss:0.137, val_acc:0.935]
Epoch [42/120    avg_loss:0.116, val_acc:0.936]
Epoch [43/120    avg_loss:0.119, val_acc:0.948]
Epoch [44/120    avg_loss:0.106, val_acc:0.931]
Epoch [45/120    avg_loss:0.123, val_acc:0.939]
Epoch [46/120    avg_loss:0.113, val_acc:0.944]
Epoch [47/120    avg_loss:0.096, val_acc:0.952]
Epoch [48/120    avg_loss:0.083, val_acc:0.955]
Epoch [49/120    avg_loss:0.097, val_acc:0.934]
Epoch [50/120    avg_loss:0.094, val_acc:0.941]
Epoch [51/120    avg_loss:0.077, val_acc:0.946]
Epoch [52/120    avg_loss:0.086, val_acc:0.955]
Epoch [53/120    avg_loss:0.072, val_acc:0.952]
Epoch [54/120    avg_loss:0.091, val_acc:0.938]
Epoch [55/120    avg_loss:0.100, val_acc:0.945]
Epoch [56/120    avg_loss:0.077, val_acc:0.917]
Epoch [57/120    avg_loss:0.104, val_acc:0.944]
Epoch [58/120    avg_loss:0.118, val_acc:0.948]
Epoch [59/120    avg_loss:0.074, val_acc:0.946]
Epoch [60/120    avg_loss:0.058, val_acc:0.941]
Epoch [61/120    avg_loss:0.057, val_acc:0.951]
Epoch [62/120    avg_loss:0.060, val_acc:0.952]
Epoch [63/120    avg_loss:0.053, val_acc:0.961]
Epoch [64/120    avg_loss:0.045, val_acc:0.961]
Epoch [65/120    avg_loss:0.047, val_acc:0.954]
Epoch [66/120    avg_loss:0.052, val_acc:0.961]
Epoch [67/120    avg_loss:0.051, val_acc:0.951]
Epoch [68/120    avg_loss:0.051, val_acc:0.949]
Epoch [69/120    avg_loss:0.051, val_acc:0.955]
Epoch [70/120    avg_loss:0.039, val_acc:0.952]
Epoch [71/120    avg_loss:0.045, val_acc:0.945]
Epoch [72/120    avg_loss:0.053, val_acc:0.951]
Epoch [73/120    avg_loss:0.042, val_acc:0.960]
Epoch [74/120    avg_loss:0.043, val_acc:0.959]
Epoch [75/120    avg_loss:0.045, val_acc:0.954]
Epoch [76/120    avg_loss:0.042, val_acc:0.958]
Epoch [77/120    avg_loss:0.040, val_acc:0.963]
Epoch [78/120    avg_loss:0.035, val_acc:0.959]
Epoch [79/120    avg_loss:0.031, val_acc:0.960]
Epoch [80/120    avg_loss:0.032, val_acc:0.962]
Epoch [81/120    avg_loss:0.029, val_acc:0.965]
Epoch [82/120    avg_loss:0.025, val_acc:0.960]
Epoch [83/120    avg_loss:0.027, val_acc:0.959]
Epoch [84/120    avg_loss:0.026, val_acc:0.963]
Epoch [85/120    avg_loss:0.035, val_acc:0.951]
Epoch [86/120    avg_loss:0.057, val_acc:0.956]
Epoch [87/120    avg_loss:0.047, val_acc:0.956]
Epoch [88/120    avg_loss:0.034, val_acc:0.964]
Epoch [89/120    avg_loss:0.036, val_acc:0.962]
Epoch [90/120    avg_loss:0.047, val_acc:0.956]
Epoch [91/120    avg_loss:0.047, val_acc:0.960]
Epoch [92/120    avg_loss:0.046, val_acc:0.954]
Epoch [93/120    avg_loss:0.052, val_acc:0.956]
Epoch [94/120    avg_loss:0.033, val_acc:0.968]
Epoch [95/120    avg_loss:0.024, val_acc:0.974]
Epoch [96/120    avg_loss:0.019, val_acc:0.956]
Epoch [97/120    avg_loss:0.025, val_acc:0.973]
Epoch [98/120    avg_loss:0.025, val_acc:0.970]
Epoch [99/120    avg_loss:0.030, val_acc:0.968]
Epoch [100/120    avg_loss:0.019, val_acc:0.971]
Epoch [101/120    avg_loss:0.015, val_acc:0.972]
Epoch [102/120    avg_loss:0.024, val_acc:0.969]
Epoch [103/120    avg_loss:0.029, val_acc:0.973]
Epoch [104/120    avg_loss:0.032, val_acc:0.967]
Epoch [105/120    avg_loss:0.023, val_acc:0.978]
Epoch [106/120    avg_loss:0.021, val_acc:0.971]
Epoch [107/120    avg_loss:0.017, val_acc:0.974]
Epoch [108/120    avg_loss:0.016, val_acc:0.963]
Epoch [109/120    avg_loss:0.019, val_acc:0.967]
Epoch [110/120    avg_loss:0.015, val_acc:0.974]
Epoch [111/120    avg_loss:0.015, val_acc:0.971]
Epoch [112/120    avg_loss:0.017, val_acc:0.975]
Epoch [113/120    avg_loss:0.017, val_acc:0.973]
Epoch [114/120    avg_loss:0.018, val_acc:0.975]
Epoch [115/120    avg_loss:0.015, val_acc:0.974]
Epoch [116/120    avg_loss:0.014, val_acc:0.969]
Epoch [117/120    avg_loss:0.013, val_acc:0.971]
Epoch [118/120    avg_loss:0.016, val_acc:0.980]
Epoch [119/120    avg_loss:0.018, val_acc:0.974]
Epoch [120/120    avg_loss:0.010, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    1 1256    0    0    0    0    0    0    0    3   20    5    0
     0    0    0]
 [   0    0    3  712    1    6    0    0    0   10    0    0   14    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    2    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    9   42    0    8    0    0    0    0  806    8    0    0
     0    2    0]
 [   0    2    8    0    0    0    4    0    0    0   17 2175    4    0
     0    0    0]
 [   0    0    0   23   11    4    0    0    0    1    0    1  487    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    3    0    0    0
  1130    0    0]
 [   0    0    0    0    0    0   34    0    0    3    0    0    0    0
    48  262    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.48780487804878

F1 scores:
[       nan 0.9        0.98086685 0.93193717 0.97260274 0.9696288
 0.97113249 0.96153846 0.99883856 0.58333333 0.94324166 0.98550068
 0.93205742 0.99730458 0.97497843 0.85761047 0.95402299]

Kappa:
0.9599547780666173
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd1f7fd9860>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.616, val_acc:0.426]
Epoch [2/120    avg_loss:2.286, val_acc:0.502]
Epoch [3/120    avg_loss:2.092, val_acc:0.520]
Epoch [4/120    avg_loss:1.948, val_acc:0.577]
Epoch [5/120    avg_loss:1.841, val_acc:0.576]
Epoch [6/120    avg_loss:1.705, val_acc:0.624]
Epoch [7/120    avg_loss:1.580, val_acc:0.635]
Epoch [8/120    avg_loss:1.411, val_acc:0.646]
Epoch [9/120    avg_loss:1.337, val_acc:0.681]
Epoch [10/120    avg_loss:1.250, val_acc:0.696]
Epoch [11/120    avg_loss:1.112, val_acc:0.652]
Epoch [12/120    avg_loss:1.020, val_acc:0.737]
Epoch [13/120    avg_loss:0.911, val_acc:0.783]
Epoch [14/120    avg_loss:0.784, val_acc:0.776]
Epoch [15/120    avg_loss:0.754, val_acc:0.763]
Epoch [16/120    avg_loss:0.652, val_acc:0.798]
Epoch [17/120    avg_loss:0.587, val_acc:0.809]
Epoch [18/120    avg_loss:0.629, val_acc:0.748]
Epoch [19/120    avg_loss:0.621, val_acc:0.817]
Epoch [20/120    avg_loss:0.533, val_acc:0.809]
Epoch [21/120    avg_loss:0.458, val_acc:0.834]
Epoch [22/120    avg_loss:0.429, val_acc:0.808]
Epoch [23/120    avg_loss:0.403, val_acc:0.853]
Epoch [24/120    avg_loss:0.393, val_acc:0.856]
Epoch [25/120    avg_loss:0.494, val_acc:0.820]
Epoch [26/120    avg_loss:0.354, val_acc:0.826]
Epoch [27/120    avg_loss:0.374, val_acc:0.817]
Epoch [28/120    avg_loss:0.394, val_acc:0.869]
Epoch [29/120    avg_loss:0.405, val_acc:0.804]
Epoch [30/120    avg_loss:0.326, val_acc:0.884]
Epoch [31/120    avg_loss:0.274, val_acc:0.863]
Epoch [32/120    avg_loss:0.269, val_acc:0.904]
Epoch [33/120    avg_loss:0.195, val_acc:0.904]
Epoch [34/120    avg_loss:0.221, val_acc:0.905]
Epoch [35/120    avg_loss:0.222, val_acc:0.902]
Epoch [36/120    avg_loss:0.183, val_acc:0.920]
Epoch [37/120    avg_loss:0.186, val_acc:0.907]
Epoch [38/120    avg_loss:0.153, val_acc:0.890]
Epoch [39/120    avg_loss:0.176, val_acc:0.917]
Epoch [40/120    avg_loss:0.206, val_acc:0.904]
Epoch [41/120    avg_loss:0.226, val_acc:0.900]
Epoch [42/120    avg_loss:0.219, val_acc:0.921]
Epoch [43/120    avg_loss:0.145, val_acc:0.923]
Epoch [44/120    avg_loss:0.142, val_acc:0.926]
Epoch [45/120    avg_loss:0.141, val_acc:0.935]
Epoch [46/120    avg_loss:0.207, val_acc:0.897]
Epoch [47/120    avg_loss:0.147, val_acc:0.927]
Epoch [48/120    avg_loss:0.108, val_acc:0.932]
Epoch [49/120    avg_loss:0.120, val_acc:0.919]
Epoch [50/120    avg_loss:0.130, val_acc:0.924]
Epoch [51/120    avg_loss:0.093, val_acc:0.941]
Epoch [52/120    avg_loss:0.075, val_acc:0.951]
Epoch [53/120    avg_loss:0.118, val_acc:0.921]
Epoch [54/120    avg_loss:0.097, val_acc:0.934]
Epoch [55/120    avg_loss:0.087, val_acc:0.936]
Epoch [56/120    avg_loss:0.091, val_acc:0.926]
Epoch [57/120    avg_loss:0.093, val_acc:0.919]
Epoch [58/120    avg_loss:0.107, val_acc:0.921]
Epoch [59/120    avg_loss:0.086, val_acc:0.929]
Epoch [60/120    avg_loss:0.081, val_acc:0.933]
Epoch [61/120    avg_loss:0.069, val_acc:0.941]
Epoch [62/120    avg_loss:0.051, val_acc:0.942]
Epoch [63/120    avg_loss:0.052, val_acc:0.951]
Epoch [64/120    avg_loss:0.057, val_acc:0.951]
Epoch [65/120    avg_loss:0.047, val_acc:0.956]
Epoch [66/120    avg_loss:0.058, val_acc:0.942]
Epoch [67/120    avg_loss:0.054, val_acc:0.953]
Epoch [68/120    avg_loss:0.064, val_acc:0.950]
Epoch [69/120    avg_loss:0.060, val_acc:0.939]
Epoch [70/120    avg_loss:0.075, val_acc:0.946]
Epoch [71/120    avg_loss:0.077, val_acc:0.944]
Epoch [72/120    avg_loss:0.068, val_acc:0.949]
Epoch [73/120    avg_loss:0.082, val_acc:0.944]
Epoch [74/120    avg_loss:0.059, val_acc:0.946]
Epoch [75/120    avg_loss:0.039, val_acc:0.954]
Epoch [76/120    avg_loss:0.036, val_acc:0.958]
Epoch [77/120    avg_loss:0.032, val_acc:0.958]
Epoch [78/120    avg_loss:0.036, val_acc:0.956]
Epoch [79/120    avg_loss:0.044, val_acc:0.951]
Epoch [80/120    avg_loss:0.032, val_acc:0.955]
Epoch [81/120    avg_loss:0.034, val_acc:0.969]
Epoch [82/120    avg_loss:0.039, val_acc:0.961]
Epoch [83/120    avg_loss:0.041, val_acc:0.960]
Epoch [84/120    avg_loss:0.043, val_acc:0.963]
Epoch [85/120    avg_loss:0.040, val_acc:0.955]
Epoch [86/120    avg_loss:0.041, val_acc:0.961]
Epoch [87/120    avg_loss:0.047, val_acc:0.970]
Epoch [88/120    avg_loss:0.038, val_acc:0.953]
Epoch [89/120    avg_loss:0.030, val_acc:0.968]
Epoch [90/120    avg_loss:0.033, val_acc:0.961]
Epoch [91/120    avg_loss:0.034, val_acc:0.961]
Epoch [92/120    avg_loss:0.038, val_acc:0.961]
Epoch [93/120    avg_loss:0.029, val_acc:0.961]
Epoch [94/120    avg_loss:0.022, val_acc:0.958]
Epoch [95/120    avg_loss:0.030, val_acc:0.963]
Epoch [96/120    avg_loss:0.034, val_acc:0.948]
Epoch [97/120    avg_loss:0.032, val_acc:0.956]
Epoch [98/120    avg_loss:0.050, val_acc:0.930]
Epoch [99/120    avg_loss:0.064, val_acc:0.952]
Epoch [100/120    avg_loss:0.047, val_acc:0.961]
Epoch [101/120    avg_loss:0.033, val_acc:0.961]
Epoch [102/120    avg_loss:0.023, val_acc:0.964]
Epoch [103/120    avg_loss:0.021, val_acc:0.965]
Epoch [104/120    avg_loss:0.020, val_acc:0.967]
Epoch [105/120    avg_loss:0.022, val_acc:0.964]
Epoch [106/120    avg_loss:0.018, val_acc:0.961]
Epoch [107/120    avg_loss:0.018, val_acc:0.962]
Epoch [108/120    avg_loss:0.018, val_acc:0.965]
Epoch [109/120    avg_loss:0.014, val_acc:0.964]
Epoch [110/120    avg_loss:0.017, val_acc:0.967]
Epoch [111/120    avg_loss:0.019, val_acc:0.965]
Epoch [112/120    avg_loss:0.014, val_acc:0.967]
Epoch [113/120    avg_loss:0.017, val_acc:0.965]
Epoch [114/120    avg_loss:0.015, val_acc:0.965]
Epoch [115/120    avg_loss:0.017, val_acc:0.965]
Epoch [116/120    avg_loss:0.013, val_acc:0.965]
Epoch [117/120    avg_loss:0.016, val_acc:0.965]
Epoch [118/120    avg_loss:0.013, val_acc:0.965]
Epoch [119/120    avg_loss:0.018, val_acc:0.965]
Epoch [120/120    avg_loss:0.016, val_acc:0.965]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1240    2    0    0    2    0    0    0   11   30    0    0
     0    0    0]
 [   0    0    0  717    0   18    0    0    0    6    1    0    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   20   44    0    5    0    0    0    0  789    9    3    0
     0    5    0]
 [   0    0    6    0    0    0    3    0    0    0   11 2180    9    1
     0    0    0]
 [   0    0    2   17    0    3    0    0    0    0   15   10  482    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1134    1    0]
 [   0    0    0    0    0    0   22    0    0    0    0    1    0    0
    40  284    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.46612466124661

F1 scores:
[       nan 0.96202532 0.97140619 0.93909627 1.         0.96636771
 0.9752809  1.         0.99883856 0.75555556 0.92388759 0.98176086
 0.93140097 0.99730458 0.97842968 0.89167975 0.96511628]

Kappa:
0.9596863185793049
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff6b506f860>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.663, val_acc:0.247]
Epoch [2/120    avg_loss:2.365, val_acc:0.468]
Epoch [3/120    avg_loss:2.139, val_acc:0.491]
Epoch [4/120    avg_loss:2.027, val_acc:0.525]
Epoch [5/120    avg_loss:1.938, val_acc:0.586]
Epoch [6/120    avg_loss:1.788, val_acc:0.627]
Epoch [7/120    avg_loss:1.661, val_acc:0.589]
Epoch [8/120    avg_loss:1.507, val_acc:0.634]
Epoch [9/120    avg_loss:1.339, val_acc:0.716]
Epoch [10/120    avg_loss:1.196, val_acc:0.697]
Epoch [11/120    avg_loss:1.106, val_acc:0.700]
Epoch [12/120    avg_loss:1.067, val_acc:0.763]
Epoch [13/120    avg_loss:0.955, val_acc:0.726]
Epoch [14/120    avg_loss:0.795, val_acc:0.767]
Epoch [15/120    avg_loss:0.774, val_acc:0.776]
Epoch [16/120    avg_loss:0.668, val_acc:0.764]
Epoch [17/120    avg_loss:0.694, val_acc:0.767]
Epoch [18/120    avg_loss:0.741, val_acc:0.808]
Epoch [19/120    avg_loss:0.613, val_acc:0.847]
Epoch [20/120    avg_loss:0.514, val_acc:0.850]
Epoch [21/120    avg_loss:0.491, val_acc:0.841]
Epoch [22/120    avg_loss:0.443, val_acc:0.836]
Epoch [23/120    avg_loss:0.443, val_acc:0.832]
Epoch [24/120    avg_loss:0.393, val_acc:0.865]
Epoch [25/120    avg_loss:0.333, val_acc:0.888]
Epoch [26/120    avg_loss:0.326, val_acc:0.890]
Epoch [27/120    avg_loss:0.296, val_acc:0.891]
Epoch [28/120    avg_loss:0.296, val_acc:0.907]
Epoch [29/120    avg_loss:0.268, val_acc:0.898]
Epoch [30/120    avg_loss:0.262, val_acc:0.900]
Epoch [31/120    avg_loss:0.245, val_acc:0.909]
Epoch [32/120    avg_loss:0.259, val_acc:0.912]
Epoch [33/120    avg_loss:0.232, val_acc:0.906]
Epoch [34/120    avg_loss:0.181, val_acc:0.912]
Epoch [35/120    avg_loss:0.181, val_acc:0.914]
Epoch [36/120    avg_loss:0.165, val_acc:0.913]
Epoch [37/120    avg_loss:0.233, val_acc:0.895]
Epoch [38/120    avg_loss:0.219, val_acc:0.917]
Epoch [39/120    avg_loss:0.171, val_acc:0.907]
Epoch [40/120    avg_loss:0.162, val_acc:0.925]
Epoch [41/120    avg_loss:0.135, val_acc:0.911]
Epoch [42/120    avg_loss:0.168, val_acc:0.916]
Epoch [43/120    avg_loss:0.172, val_acc:0.920]
Epoch [44/120    avg_loss:0.178, val_acc:0.925]
Epoch [45/120    avg_loss:0.130, val_acc:0.931]
Epoch [46/120    avg_loss:0.146, val_acc:0.935]
Epoch [47/120    avg_loss:0.110, val_acc:0.944]
Epoch [48/120    avg_loss:0.103, val_acc:0.934]
Epoch [49/120    avg_loss:0.096, val_acc:0.942]
Epoch [50/120    avg_loss:0.129, val_acc:0.942]
Epoch [51/120    avg_loss:0.132, val_acc:0.935]
Epoch [52/120    avg_loss:0.144, val_acc:0.942]
Epoch [53/120    avg_loss:0.137, val_acc:0.940]
Epoch [54/120    avg_loss:0.091, val_acc:0.941]
Epoch [55/120    avg_loss:0.076, val_acc:0.947]
Epoch [56/120    avg_loss:0.072, val_acc:0.941]
Epoch [57/120    avg_loss:0.105, val_acc:0.932]
Epoch [58/120    avg_loss:0.091, val_acc:0.933]
Epoch [59/120    avg_loss:0.081, val_acc:0.940]
Epoch [60/120    avg_loss:0.086, val_acc:0.950]
Epoch [61/120    avg_loss:0.086, val_acc:0.950]
Epoch [62/120    avg_loss:0.129, val_acc:0.923]
Epoch [63/120    avg_loss:0.119, val_acc:0.923]
Epoch [64/120    avg_loss:0.105, val_acc:0.936]
Epoch [65/120    avg_loss:0.068, val_acc:0.943]
Epoch [66/120    avg_loss:0.084, val_acc:0.949]
Epoch [67/120    avg_loss:0.066, val_acc:0.944]
Epoch [68/120    avg_loss:0.078, val_acc:0.939]
Epoch [69/120    avg_loss:0.077, val_acc:0.955]
Epoch [70/120    avg_loss:0.051, val_acc:0.952]
Epoch [71/120    avg_loss:0.043, val_acc:0.952]
Epoch [72/120    avg_loss:0.046, val_acc:0.955]
Epoch [73/120    avg_loss:0.050, val_acc:0.958]
Epoch [74/120    avg_loss:0.047, val_acc:0.960]
Epoch [75/120    avg_loss:0.047, val_acc:0.954]
Epoch [76/120    avg_loss:0.046, val_acc:0.964]
Epoch [77/120    avg_loss:0.052, val_acc:0.953]
Epoch [78/120    avg_loss:0.036, val_acc:0.959]
Epoch [79/120    avg_loss:0.048, val_acc:0.961]
Epoch [80/120    avg_loss:0.040, val_acc:0.966]
Epoch [81/120    avg_loss:0.036, val_acc:0.953]
Epoch [82/120    avg_loss:0.031, val_acc:0.963]
Epoch [83/120    avg_loss:0.031, val_acc:0.963]
Epoch [84/120    avg_loss:0.035, val_acc:0.964]
Epoch [85/120    avg_loss:0.024, val_acc:0.967]
Epoch [86/120    avg_loss:0.023, val_acc:0.962]
Epoch [87/120    avg_loss:0.025, val_acc:0.971]
Epoch [88/120    avg_loss:0.047, val_acc:0.955]
Epoch [89/120    avg_loss:0.053, val_acc:0.943]
Epoch [90/120    avg_loss:0.058, val_acc:0.947]
Epoch [91/120    avg_loss:0.092, val_acc:0.920]
Epoch [92/120    avg_loss:0.135, val_acc:0.935]
Epoch [93/120    avg_loss:0.159, val_acc:0.941]
Epoch [94/120    avg_loss:0.067, val_acc:0.941]
Epoch [95/120    avg_loss:0.060, val_acc:0.927]
Epoch [96/120    avg_loss:0.100, val_acc:0.940]
Epoch [97/120    avg_loss:0.061, val_acc:0.954]
Epoch [98/120    avg_loss:0.051, val_acc:0.948]
Epoch [99/120    avg_loss:0.052, val_acc:0.950]
Epoch [100/120    avg_loss:0.041, val_acc:0.955]
Epoch [101/120    avg_loss:0.034, val_acc:0.959]
Epoch [102/120    avg_loss:0.027, val_acc:0.968]
Epoch [103/120    avg_loss:0.026, val_acc:0.970]
Epoch [104/120    avg_loss:0.022, val_acc:0.970]
Epoch [105/120    avg_loss:0.024, val_acc:0.970]
Epoch [106/120    avg_loss:0.027, val_acc:0.964]
Epoch [107/120    avg_loss:0.022, val_acc:0.966]
Epoch [108/120    avg_loss:0.027, val_acc:0.967]
Epoch [109/120    avg_loss:0.021, val_acc:0.971]
Epoch [110/120    avg_loss:0.024, val_acc:0.969]
Epoch [111/120    avg_loss:0.022, val_acc:0.971]
Epoch [112/120    avg_loss:0.020, val_acc:0.970]
Epoch [113/120    avg_loss:0.025, val_acc:0.972]
Epoch [114/120    avg_loss:0.022, val_acc:0.970]
Epoch [115/120    avg_loss:0.019, val_acc:0.971]
Epoch [116/120    avg_loss:0.022, val_acc:0.969]
Epoch [117/120    avg_loss:0.023, val_acc:0.968]
Epoch [118/120    avg_loss:0.024, val_acc:0.969]
Epoch [119/120    avg_loss:0.019, val_acc:0.969]
Epoch [120/120    avg_loss:0.017, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1252    1    0    1    1    0    0    2    6   22    0    0
     0    0    0]
 [   0    0    3  681    2   23    0    0    0   18    1    0   16    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    2    0    0    3    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0   12    0    0    1    0
     0    0    0]
 [   0    0   24   67    0   11    0    0    0    0  759    9    0    0
     0    5    0]
 [   0    0   10    0    0    0    3    0    7    0   18 2167    2    2
     1    0    0]
 [   0    0    4   25    9   12    0    0    0    1    2    0  464    0
     2    8    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    2    0    1    0    3    1    0    0
  1131    1    0]
 [   0    0    0    0    0    1    6    0    0    0    0    0    0    0
    38  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.68563685636856

F1 scores:
[       nan 0.975      0.97129558 0.89252949 0.97482838 0.93962678
 0.98793363 1.         0.99078341 0.44444444 0.91116447 0.98276644
 0.91248771 0.98666667 0.97752809 0.91101056 0.96      ]

Kappa:
0.9508289076604775
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa04c0cf7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.718, val_acc:0.372]
Epoch [2/120    avg_loss:2.356, val_acc:0.475]
Epoch [3/120    avg_loss:2.163, val_acc:0.451]
Epoch [4/120    avg_loss:2.040, val_acc:0.479]
Epoch [5/120    avg_loss:1.896, val_acc:0.537]
Epoch [6/120    avg_loss:1.767, val_acc:0.585]
Epoch [7/120    avg_loss:1.676, val_acc:0.613]
Epoch [8/120    avg_loss:1.515, val_acc:0.641]
Epoch [9/120    avg_loss:1.440, val_acc:0.626]
Epoch [10/120    avg_loss:1.315, val_acc:0.670]
Epoch [11/120    avg_loss:1.181, val_acc:0.642]
Epoch [12/120    avg_loss:1.100, val_acc:0.682]
Epoch [13/120    avg_loss:0.993, val_acc:0.701]
Epoch [14/120    avg_loss:0.900, val_acc:0.747]
Epoch [15/120    avg_loss:0.890, val_acc:0.756]
Epoch [16/120    avg_loss:0.799, val_acc:0.737]
Epoch [17/120    avg_loss:0.730, val_acc:0.794]
Epoch [18/120    avg_loss:0.717, val_acc:0.796]
Epoch [19/120    avg_loss:0.674, val_acc:0.747]
Epoch [20/120    avg_loss:0.654, val_acc:0.804]
Epoch [21/120    avg_loss:0.544, val_acc:0.816]
Epoch [22/120    avg_loss:0.479, val_acc:0.796]
Epoch [23/120    avg_loss:0.460, val_acc:0.840]
Epoch [24/120    avg_loss:0.471, val_acc:0.811]
Epoch [25/120    avg_loss:0.464, val_acc:0.807]
Epoch [26/120    avg_loss:0.435, val_acc:0.815]
Epoch [27/120    avg_loss:0.425, val_acc:0.853]
Epoch [28/120    avg_loss:0.365, val_acc:0.878]
Epoch [29/120    avg_loss:0.315, val_acc:0.843]
Epoch [30/120    avg_loss:0.306, val_acc:0.869]
Epoch [31/120    avg_loss:0.277, val_acc:0.855]
Epoch [32/120    avg_loss:0.292, val_acc:0.871]
Epoch [33/120    avg_loss:0.241, val_acc:0.892]
Epoch [34/120    avg_loss:0.233, val_acc:0.872]
Epoch [35/120    avg_loss:0.208, val_acc:0.894]
Epoch [36/120    avg_loss:0.217, val_acc:0.885]
Epoch [37/120    avg_loss:0.256, val_acc:0.885]
Epoch [38/120    avg_loss:0.189, val_acc:0.904]
Epoch [39/120    avg_loss:0.161, val_acc:0.896]
Epoch [40/120    avg_loss:0.159, val_acc:0.908]
Epoch [41/120    avg_loss:0.147, val_acc:0.916]
Epoch [42/120    avg_loss:0.166, val_acc:0.906]
Epoch [43/120    avg_loss:0.171, val_acc:0.919]
Epoch [44/120    avg_loss:0.185, val_acc:0.916]
Epoch [45/120    avg_loss:0.133, val_acc:0.925]
Epoch [46/120    avg_loss:0.123, val_acc:0.923]
Epoch [47/120    avg_loss:0.152, val_acc:0.914]
Epoch [48/120    avg_loss:0.129, val_acc:0.930]
Epoch [49/120    avg_loss:0.119, val_acc:0.904]
Epoch [50/120    avg_loss:0.125, val_acc:0.930]
Epoch [51/120    avg_loss:0.129, val_acc:0.923]
Epoch [52/120    avg_loss:0.093, val_acc:0.930]
Epoch [53/120    avg_loss:0.089, val_acc:0.932]
Epoch [54/120    avg_loss:0.076, val_acc:0.939]
Epoch [55/120    avg_loss:0.109, val_acc:0.906]
Epoch [56/120    avg_loss:0.132, val_acc:0.923]
Epoch [57/120    avg_loss:0.085, val_acc:0.938]
Epoch [58/120    avg_loss:0.075, val_acc:0.935]
Epoch [59/120    avg_loss:0.065, val_acc:0.943]
Epoch [60/120    avg_loss:0.086, val_acc:0.941]
Epoch [61/120    avg_loss:0.098, val_acc:0.913]
Epoch [62/120    avg_loss:0.161, val_acc:0.905]
Epoch [63/120    avg_loss:0.197, val_acc:0.898]
Epoch [64/120    avg_loss:0.142, val_acc:0.912]
Epoch [65/120    avg_loss:0.104, val_acc:0.919]
Epoch [66/120    avg_loss:0.086, val_acc:0.936]
Epoch [67/120    avg_loss:0.109, val_acc:0.941]
Epoch [68/120    avg_loss:0.095, val_acc:0.926]
Epoch [69/120    avg_loss:0.098, val_acc:0.927]
Epoch [70/120    avg_loss:0.072, val_acc:0.941]
Epoch [71/120    avg_loss:0.070, val_acc:0.944]
Epoch [72/120    avg_loss:0.054, val_acc:0.950]
Epoch [73/120    avg_loss:0.046, val_acc:0.951]
Epoch [74/120    avg_loss:0.052, val_acc:0.943]
Epoch [75/120    avg_loss:0.060, val_acc:0.939]
Epoch [76/120    avg_loss:0.052, val_acc:0.944]
Epoch [77/120    avg_loss:0.042, val_acc:0.950]
Epoch [78/120    avg_loss:0.041, val_acc:0.952]
Epoch [79/120    avg_loss:0.043, val_acc:0.950]
Epoch [80/120    avg_loss:0.045, val_acc:0.930]
Epoch [81/120    avg_loss:0.054, val_acc:0.948]
Epoch [82/120    avg_loss:0.044, val_acc:0.952]
Epoch [83/120    avg_loss:0.050, val_acc:0.938]
Epoch [84/120    avg_loss:0.048, val_acc:0.936]
Epoch [85/120    avg_loss:0.033, val_acc:0.945]
Epoch [86/120    avg_loss:0.050, val_acc:0.944]
Epoch [87/120    avg_loss:0.057, val_acc:0.945]
Epoch [88/120    avg_loss:0.055, val_acc:0.948]
Epoch [89/120    avg_loss:0.060, val_acc:0.944]
Epoch [90/120    avg_loss:0.068, val_acc:0.930]
Epoch [91/120    avg_loss:0.076, val_acc:0.943]
Epoch [92/120    avg_loss:0.061, val_acc:0.951]
Epoch [93/120    avg_loss:0.044, val_acc:0.948]
Epoch [94/120    avg_loss:0.040, val_acc:0.959]
Epoch [95/120    avg_loss:0.040, val_acc:0.951]
Epoch [96/120    avg_loss:0.035, val_acc:0.954]
Epoch [97/120    avg_loss:0.024, val_acc:0.954]
Epoch [98/120    avg_loss:0.025, val_acc:0.956]
Epoch [99/120    avg_loss:0.033, val_acc:0.948]
Epoch [100/120    avg_loss:0.040, val_acc:0.949]
Epoch [101/120    avg_loss:0.051, val_acc:0.955]
Epoch [102/120    avg_loss:0.058, val_acc:0.949]
Epoch [103/120    avg_loss:0.034, val_acc:0.963]
Epoch [104/120    avg_loss:0.031, val_acc:0.962]
Epoch [105/120    avg_loss:0.023, val_acc:0.958]
Epoch [106/120    avg_loss:0.031, val_acc:0.961]
Epoch [107/120    avg_loss:0.026, val_acc:0.964]
Epoch [108/120    avg_loss:0.029, val_acc:0.963]
Epoch [109/120    avg_loss:0.027, val_acc:0.951]
Epoch [110/120    avg_loss:0.030, val_acc:0.965]
Epoch [111/120    avg_loss:0.020, val_acc:0.964]
Epoch [112/120    avg_loss:0.032, val_acc:0.952]
Epoch [113/120    avg_loss:0.042, val_acc:0.942]
Epoch [114/120    avg_loss:0.030, val_acc:0.958]
Epoch [115/120    avg_loss:0.025, val_acc:0.953]
Epoch [116/120    avg_loss:0.027, val_acc:0.965]
Epoch [117/120    avg_loss:0.019, val_acc:0.959]
Epoch [118/120    avg_loss:0.020, val_acc:0.963]
Epoch [119/120    avg_loss:0.020, val_acc:0.961]
Epoch [120/120    avg_loss:0.025, val_acc:0.949]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1249    9    0    0    4    0    0    2    9   12    0    0
     0    0    0]
 [   0    0    0  699    3   12    0    0    0   15    0    0   11    6
     1    0    0]
 [   0    0    0    0  211    0    1    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    3    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    8   64    0    4    8    0    0    0  780    5    1    0
     2    3    0]
 [   0    0   41    5    0    2   15    0    0    0   23 2117    2    3
     2    0    0]
 [   0    0    0   26    7    6    0    0    0    0    9    0  482    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   13    0    0    0    0    1    0    0    0
  1125    0    0]
 [   0    0    0    0    0    1   29    0    0    4    0    0    0    0
    72  241    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.03523035230353

F1 scores:
[       nan 0.96202532 0.96709253 0.90193548 0.97235023 0.95005549
 0.95842451 1.         1.         0.57627119 0.91764706 0.97467772
 0.93320426 0.9762533  0.95907928 0.81556684 0.97647059]

Kappa:
0.9434542293032482
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faa1410e828>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.675, val_acc:0.268]
Epoch [2/120    avg_loss:2.339, val_acc:0.462]
Epoch [3/120    avg_loss:2.106, val_acc:0.443]
Epoch [4/120    avg_loss:1.976, val_acc:0.570]
Epoch [5/120    avg_loss:1.808, val_acc:0.528]
Epoch [6/120    avg_loss:1.712, val_acc:0.586]
Epoch [7/120    avg_loss:1.598, val_acc:0.623]
Epoch [8/120    avg_loss:1.550, val_acc:0.592]
Epoch [9/120    avg_loss:1.485, val_acc:0.578]
Epoch [10/120    avg_loss:1.320, val_acc:0.674]
Epoch [11/120    avg_loss:1.218, val_acc:0.671]
Epoch [12/120    avg_loss:1.140, val_acc:0.680]
Epoch [13/120    avg_loss:1.069, val_acc:0.738]
Epoch [14/120    avg_loss:1.011, val_acc:0.713]
Epoch [15/120    avg_loss:0.949, val_acc:0.729]
Epoch [16/120    avg_loss:0.883, val_acc:0.783]
Epoch [17/120    avg_loss:0.735, val_acc:0.817]
Epoch [18/120    avg_loss:0.688, val_acc:0.767]
Epoch [19/120    avg_loss:0.666, val_acc:0.834]
Epoch [20/120    avg_loss:0.567, val_acc:0.849]
Epoch [21/120    avg_loss:0.552, val_acc:0.853]
Epoch [22/120    avg_loss:0.474, val_acc:0.854]
Epoch [23/120    avg_loss:0.437, val_acc:0.846]
Epoch [24/120    avg_loss:0.355, val_acc:0.869]
Epoch [25/120    avg_loss:0.326, val_acc:0.912]
Epoch [26/120    avg_loss:0.349, val_acc:0.844]
Epoch [27/120    avg_loss:0.394, val_acc:0.858]
Epoch [28/120    avg_loss:0.348, val_acc:0.885]
Epoch [29/120    avg_loss:0.382, val_acc:0.865]
Epoch [30/120    avg_loss:0.398, val_acc:0.888]
Epoch [31/120    avg_loss:0.285, val_acc:0.894]
Epoch [32/120    avg_loss:0.246, val_acc:0.897]
Epoch [33/120    avg_loss:0.238, val_acc:0.901]
Epoch [34/120    avg_loss:0.244, val_acc:0.907]
Epoch [35/120    avg_loss:0.204, val_acc:0.907]
Epoch [36/120    avg_loss:0.193, val_acc:0.843]
Epoch [37/120    avg_loss:0.203, val_acc:0.879]
Epoch [38/120    avg_loss:0.192, val_acc:0.916]
Epoch [39/120    avg_loss:0.213, val_acc:0.910]
Epoch [40/120    avg_loss:0.181, val_acc:0.908]
Epoch [41/120    avg_loss:0.150, val_acc:0.898]
Epoch [42/120    avg_loss:0.114, val_acc:0.920]
Epoch [43/120    avg_loss:0.121, val_acc:0.930]
Epoch [44/120    avg_loss:0.128, val_acc:0.910]
Epoch [45/120    avg_loss:0.164, val_acc:0.919]
Epoch [46/120    avg_loss:0.120, val_acc:0.907]
Epoch [47/120    avg_loss:0.123, val_acc:0.925]
Epoch [48/120    avg_loss:0.132, val_acc:0.935]
Epoch [49/120    avg_loss:0.125, val_acc:0.939]
Epoch [50/120    avg_loss:0.136, val_acc:0.911]
Epoch [51/120    avg_loss:0.121, val_acc:0.920]
Epoch [52/120    avg_loss:0.096, val_acc:0.941]
Epoch [53/120    avg_loss:0.096, val_acc:0.932]
Epoch [54/120    avg_loss:0.073, val_acc:0.939]
Epoch [55/120    avg_loss:0.067, val_acc:0.945]
Epoch [56/120    avg_loss:0.071, val_acc:0.945]
Epoch [57/120    avg_loss:0.091, val_acc:0.943]
Epoch [58/120    avg_loss:0.097, val_acc:0.930]
Epoch [59/120    avg_loss:0.074, val_acc:0.941]
Epoch [60/120    avg_loss:0.055, val_acc:0.945]
Epoch [61/120    avg_loss:0.051, val_acc:0.942]
Epoch [62/120    avg_loss:0.067, val_acc:0.935]
Epoch [63/120    avg_loss:0.060, val_acc:0.941]
Epoch [64/120    avg_loss:0.053, val_acc:0.950]
Epoch [65/120    avg_loss:0.048, val_acc:0.956]
Epoch [66/120    avg_loss:0.065, val_acc:0.950]
Epoch [67/120    avg_loss:0.053, val_acc:0.954]
Epoch [68/120    avg_loss:0.044, val_acc:0.949]
Epoch [69/120    avg_loss:0.045, val_acc:0.958]
Epoch [70/120    avg_loss:0.041, val_acc:0.950]
Epoch [71/120    avg_loss:0.037, val_acc:0.954]
Epoch [72/120    avg_loss:0.051, val_acc:0.932]
Epoch [73/120    avg_loss:0.049, val_acc:0.959]
Epoch [74/120    avg_loss:0.057, val_acc:0.950]
Epoch [75/120    avg_loss:0.047, val_acc:0.958]
Epoch [76/120    avg_loss:0.041, val_acc:0.953]
Epoch [77/120    avg_loss:0.045, val_acc:0.955]
Epoch [78/120    avg_loss:0.058, val_acc:0.943]
Epoch [79/120    avg_loss:0.081, val_acc:0.942]
Epoch [80/120    avg_loss:0.040, val_acc:0.945]
Epoch [81/120    avg_loss:0.032, val_acc:0.944]
Epoch [82/120    avg_loss:0.039, val_acc:0.955]
Epoch [83/120    avg_loss:0.041, val_acc:0.959]
Epoch [84/120    avg_loss:0.032, val_acc:0.959]
Epoch [85/120    avg_loss:0.035, val_acc:0.958]
Epoch [86/120    avg_loss:0.027, val_acc:0.961]
Epoch [87/120    avg_loss:0.034, val_acc:0.958]
Epoch [88/120    avg_loss:0.020, val_acc:0.950]
Epoch [89/120    avg_loss:0.032, val_acc:0.950]
Epoch [90/120    avg_loss:0.030, val_acc:0.949]
Epoch [91/120    avg_loss:0.026, val_acc:0.955]
Epoch [92/120    avg_loss:0.024, val_acc:0.964]
Epoch [93/120    avg_loss:0.039, val_acc:0.955]
Epoch [94/120    avg_loss:0.035, val_acc:0.954]
Epoch [95/120    avg_loss:0.032, val_acc:0.949]
Epoch [96/120    avg_loss:0.026, val_acc:0.960]
Epoch [97/120    avg_loss:0.024, val_acc:0.952]
Epoch [98/120    avg_loss:0.029, val_acc:0.960]
Epoch [99/120    avg_loss:0.043, val_acc:0.961]
Epoch [100/120    avg_loss:0.029, val_acc:0.963]
Epoch [101/120    avg_loss:0.024, val_acc:0.961]
Epoch [102/120    avg_loss:0.017, val_acc:0.973]
Epoch [103/120    avg_loss:0.015, val_acc:0.967]
Epoch [104/120    avg_loss:0.015, val_acc:0.974]
Epoch [105/120    avg_loss:0.017, val_acc:0.969]
Epoch [106/120    avg_loss:0.020, val_acc:0.970]
Epoch [107/120    avg_loss:0.016, val_acc:0.969]
Epoch [108/120    avg_loss:0.014, val_acc:0.963]
Epoch [109/120    avg_loss:0.017, val_acc:0.969]
Epoch [110/120    avg_loss:0.013, val_acc:0.971]
Epoch [111/120    avg_loss:0.013, val_acc:0.972]
Epoch [112/120    avg_loss:0.017, val_acc:0.971]
Epoch [113/120    avg_loss:0.045, val_acc:0.951]
Epoch [114/120    avg_loss:0.075, val_acc:0.948]
Epoch [115/120    avg_loss:0.051, val_acc:0.952]
Epoch [116/120    avg_loss:0.027, val_acc:0.965]
Epoch [117/120    avg_loss:0.026, val_acc:0.952]
Epoch [118/120    avg_loss:0.021, val_acc:0.970]
Epoch [119/120    avg_loss:0.018, val_acc:0.971]
Epoch [120/120    avg_loss:0.017, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1228    2    0    0    0    0    0    0   10   39    6    0
     0    0    0]
 [   0    0    1  696   14   10    0    0    0    7    0    7   12    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0   13   62    0    9    0    0    0    0  777    7    7    0
     0    0    0]
 [   0    0   15    0    0    0    1    0    3    0   14 2173    2    2
     0    0    0]
 [   0    0   11   20   12    5    0    0    0    0    0    0  482    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    4    0    0    0    7    0    0    1    0    2    0    0    0
  1124    1    0]
 [   0    0    0    0    0    0   23    0    0    3    0    0    0    0
    66  255    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.53387533875339

F1 scores:
[       nan 0.94117647 0.96200548 0.91159136 0.94013304 0.96329255
 0.98130142 1.         0.99537037 0.68181818 0.92555092 0.97949065
 0.91897045 0.99462366 0.96480687 0.84577114 0.96470588]

Kappa:
0.9490607418386816
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5305ca0860>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.750, val_acc:0.401]
Epoch [2/120    avg_loss:2.410, val_acc:0.432]
Epoch [3/120    avg_loss:2.180, val_acc:0.480]
Epoch [4/120    avg_loss:2.044, val_acc:0.533]
Epoch [5/120    avg_loss:1.918, val_acc:0.587]
Epoch [6/120    avg_loss:1.822, val_acc:0.592]
Epoch [7/120    avg_loss:1.633, val_acc:0.653]
Epoch [8/120    avg_loss:1.426, val_acc:0.628]
Epoch [9/120    avg_loss:1.311, val_acc:0.651]
Epoch [10/120    avg_loss:1.234, val_acc:0.670]
Epoch [11/120    avg_loss:1.040, val_acc:0.730]
Epoch [12/120    avg_loss:0.947, val_acc:0.752]
Epoch [13/120    avg_loss:0.854, val_acc:0.739]
Epoch [14/120    avg_loss:0.955, val_acc:0.718]
Epoch [15/120    avg_loss:0.872, val_acc:0.698]
Epoch [16/120    avg_loss:0.768, val_acc:0.742]
Epoch [17/120    avg_loss:0.713, val_acc:0.776]
Epoch [18/120    avg_loss:0.659, val_acc:0.800]
Epoch [19/120    avg_loss:0.608, val_acc:0.826]
Epoch [20/120    avg_loss:0.505, val_acc:0.798]
Epoch [21/120    avg_loss:0.540, val_acc:0.807]
Epoch [22/120    avg_loss:0.504, val_acc:0.836]
Epoch [23/120    avg_loss:0.496, val_acc:0.816]
Epoch [24/120    avg_loss:0.405, val_acc:0.862]
Epoch [25/120    avg_loss:0.427, val_acc:0.847]
Epoch [26/120    avg_loss:0.367, val_acc:0.878]
Epoch [27/120    avg_loss:0.333, val_acc:0.887]
Epoch [28/120    avg_loss:0.306, val_acc:0.894]
Epoch [29/120    avg_loss:0.261, val_acc:0.904]
Epoch [30/120    avg_loss:0.281, val_acc:0.901]
Epoch [31/120    avg_loss:0.276, val_acc:0.891]
Epoch [32/120    avg_loss:0.285, val_acc:0.897]
Epoch [33/120    avg_loss:0.241, val_acc:0.908]
Epoch [34/120    avg_loss:0.250, val_acc:0.908]
Epoch [35/120    avg_loss:0.198, val_acc:0.924]
Epoch [36/120    avg_loss:0.194, val_acc:0.925]
Epoch [37/120    avg_loss:0.204, val_acc:0.912]
Epoch [38/120    avg_loss:0.244, val_acc:0.904]
Epoch [39/120    avg_loss:0.257, val_acc:0.896]
Epoch [40/120    avg_loss:0.192, val_acc:0.906]
Epoch [41/120    avg_loss:0.165, val_acc:0.939]
Epoch [42/120    avg_loss:0.174, val_acc:0.924]
Epoch [43/120    avg_loss:0.151, val_acc:0.920]
Epoch [44/120    avg_loss:0.149, val_acc:0.932]
Epoch [45/120    avg_loss:0.157, val_acc:0.939]
Epoch [46/120    avg_loss:0.138, val_acc:0.934]
Epoch [47/120    avg_loss:0.155, val_acc:0.932]
Epoch [48/120    avg_loss:0.141, val_acc:0.929]
Epoch [49/120    avg_loss:0.128, val_acc:0.924]
Epoch [50/120    avg_loss:0.122, val_acc:0.943]
Epoch [51/120    avg_loss:0.141, val_acc:0.919]
Epoch [52/120    avg_loss:0.187, val_acc:0.933]
Epoch [53/120    avg_loss:0.139, val_acc:0.919]
Epoch [54/120    avg_loss:0.137, val_acc:0.934]
Epoch [55/120    avg_loss:0.106, val_acc:0.938]
Epoch [56/120    avg_loss:0.126, val_acc:0.916]
Epoch [57/120    avg_loss:0.122, val_acc:0.942]
Epoch [58/120    avg_loss:0.102, val_acc:0.945]
Epoch [59/120    avg_loss:0.098, val_acc:0.951]
Epoch [60/120    avg_loss:0.086, val_acc:0.945]
Epoch [61/120    avg_loss:0.100, val_acc:0.959]
Epoch [62/120    avg_loss:0.094, val_acc:0.953]
Epoch [63/120    avg_loss:0.084, val_acc:0.948]
Epoch [64/120    avg_loss:0.076, val_acc:0.948]
Epoch [65/120    avg_loss:0.063, val_acc:0.959]
Epoch [66/120    avg_loss:0.061, val_acc:0.961]
Epoch [67/120    avg_loss:0.085, val_acc:0.950]
Epoch [68/120    avg_loss:0.085, val_acc:0.940]
Epoch [69/120    avg_loss:0.113, val_acc:0.930]
Epoch [70/120    avg_loss:0.072, val_acc:0.951]
Epoch [71/120    avg_loss:0.096, val_acc:0.944]
Epoch [72/120    avg_loss:0.098, val_acc:0.944]
Epoch [73/120    avg_loss:0.083, val_acc:0.942]
Epoch [74/120    avg_loss:0.075, val_acc:0.954]
Epoch [75/120    avg_loss:0.064, val_acc:0.960]
Epoch [76/120    avg_loss:0.060, val_acc:0.954]
Epoch [77/120    avg_loss:0.056, val_acc:0.956]
Epoch [78/120    avg_loss:0.049, val_acc:0.960]
Epoch [79/120    avg_loss:0.045, val_acc:0.967]
Epoch [80/120    avg_loss:0.047, val_acc:0.961]
Epoch [81/120    avg_loss:0.061, val_acc:0.954]
Epoch [82/120    avg_loss:0.049, val_acc:0.956]
Epoch [83/120    avg_loss:0.061, val_acc:0.958]
Epoch [84/120    avg_loss:0.085, val_acc:0.956]
Epoch [85/120    avg_loss:0.065, val_acc:0.948]
Epoch [86/120    avg_loss:0.067, val_acc:0.955]
Epoch [87/120    avg_loss:0.051, val_acc:0.963]
Epoch [88/120    avg_loss:0.052, val_acc:0.962]
Epoch [89/120    avg_loss:0.063, val_acc:0.968]
Epoch [90/120    avg_loss:0.075, val_acc:0.958]
Epoch [91/120    avg_loss:0.070, val_acc:0.960]
Epoch [92/120    avg_loss:0.051, val_acc:0.959]
Epoch [93/120    avg_loss:0.048, val_acc:0.962]
Epoch [94/120    avg_loss:0.038, val_acc:0.965]
Epoch [95/120    avg_loss:0.044, val_acc:0.960]
Epoch [96/120    avg_loss:0.034, val_acc:0.968]
Epoch [97/120    avg_loss:0.046, val_acc:0.959]
Epoch [98/120    avg_loss:0.031, val_acc:0.968]
Epoch [99/120    avg_loss:0.032, val_acc:0.970]
Epoch [100/120    avg_loss:0.027, val_acc:0.965]
Epoch [101/120    avg_loss:0.033, val_acc:0.971]
Epoch [102/120    avg_loss:0.032, val_acc:0.974]
Epoch [103/120    avg_loss:0.041, val_acc:0.973]
Epoch [104/120    avg_loss:0.026, val_acc:0.964]
Epoch [105/120    avg_loss:0.029, val_acc:0.968]
Epoch [106/120    avg_loss:0.025, val_acc:0.960]
Epoch [107/120    avg_loss:0.041, val_acc:0.959]
Epoch [108/120    avg_loss:0.041, val_acc:0.971]
Epoch [109/120    avg_loss:0.046, val_acc:0.965]
Epoch [110/120    avg_loss:0.032, val_acc:0.964]
Epoch [111/120    avg_loss:0.022, val_acc:0.970]
Epoch [112/120    avg_loss:0.023, val_acc:0.969]
Epoch [113/120    avg_loss:0.029, val_acc:0.972]
Epoch [114/120    avg_loss:0.027, val_acc:0.971]
Epoch [115/120    avg_loss:0.025, val_acc:0.961]
Epoch [116/120    avg_loss:0.027, val_acc:0.968]
Epoch [117/120    avg_loss:0.016, val_acc:0.971]
Epoch [118/120    avg_loss:0.020, val_acc:0.970]
Epoch [119/120    avg_loss:0.019, val_acc:0.971]
Epoch [120/120    avg_loss:0.017, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1233    2    0    0    2    0    0    0    9   39    0    0
     0    0    0]
 [   0    0    1  682    1   14    0    0    0    8    0    0   39    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   13   89    0    5    0    0    0    0  756    6    0    0
     0    6    0]
 [   0    0   16    0    0    1   12    0    3    0    5 2164    3    4
     2    0    0]
 [   0    0    0    5    0    9    0    0    0    0    8    1  505    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    92  255    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.46883468834689

F1 scores:
[       nan 0.94871795 0.9678179  0.89442623 0.99765808 0.96312849
 0.98869631 1.         0.99537037 0.72340426 0.91084337 0.97896404
 0.93259464 0.98404255 0.95861486 0.83881579 0.95953757]

Kappa:
0.9483254140198302
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd3c58f47b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.689, val_acc:0.334]
Epoch [2/120    avg_loss:2.371, val_acc:0.312]
Epoch [3/120    avg_loss:2.140, val_acc:0.395]
Epoch [4/120    avg_loss:1.948, val_acc:0.443]
Epoch [5/120    avg_loss:1.811, val_acc:0.582]
Epoch [6/120    avg_loss:1.651, val_acc:0.583]
Epoch [7/120    avg_loss:1.513, val_acc:0.581]
Epoch [8/120    avg_loss:1.395, val_acc:0.612]
Epoch [9/120    avg_loss:1.262, val_acc:0.635]
Epoch [10/120    avg_loss:1.158, val_acc:0.668]
Epoch [11/120    avg_loss:1.123, val_acc:0.694]
Epoch [12/120    avg_loss:0.986, val_acc:0.670]
Epoch [13/120    avg_loss:0.954, val_acc:0.708]
Epoch [14/120    avg_loss:0.848, val_acc:0.726]
Epoch [15/120    avg_loss:0.748, val_acc:0.713]
Epoch [16/120    avg_loss:0.738, val_acc:0.722]
Epoch [17/120    avg_loss:0.694, val_acc:0.766]
Epoch [18/120    avg_loss:0.674, val_acc:0.753]
Epoch [19/120    avg_loss:0.582, val_acc:0.799]
Epoch [20/120    avg_loss:0.552, val_acc:0.784]
Epoch [21/120    avg_loss:0.553, val_acc:0.771]
Epoch [22/120    avg_loss:0.508, val_acc:0.807]
Epoch [23/120    avg_loss:0.453, val_acc:0.838]
Epoch [24/120    avg_loss:0.449, val_acc:0.823]
Epoch [25/120    avg_loss:0.413, val_acc:0.828]
Epoch [26/120    avg_loss:0.407, val_acc:0.861]
Epoch [27/120    avg_loss:0.381, val_acc:0.822]
Epoch [28/120    avg_loss:0.397, val_acc:0.851]
Epoch [29/120    avg_loss:0.329, val_acc:0.866]
Epoch [30/120    avg_loss:0.308, val_acc:0.861]
Epoch [31/120    avg_loss:0.316, val_acc:0.870]
Epoch [32/120    avg_loss:0.395, val_acc:0.866]
Epoch [33/120    avg_loss:0.343, val_acc:0.826]
Epoch [34/120    avg_loss:0.357, val_acc:0.860]
Epoch [35/120    avg_loss:0.308, val_acc:0.875]
Epoch [36/120    avg_loss:0.287, val_acc:0.883]
Epoch [37/120    avg_loss:0.281, val_acc:0.869]
Epoch [38/120    avg_loss:0.204, val_acc:0.892]
Epoch [39/120    avg_loss:0.215, val_acc:0.921]
Epoch [40/120    avg_loss:0.179, val_acc:0.919]
Epoch [41/120    avg_loss:0.249, val_acc:0.912]
Epoch [42/120    avg_loss:0.281, val_acc:0.895]
Epoch [43/120    avg_loss:0.303, val_acc:0.856]
Epoch [44/120    avg_loss:0.233, val_acc:0.902]
Epoch [45/120    avg_loss:0.188, val_acc:0.913]
Epoch [46/120    avg_loss:0.205, val_acc:0.905]
Epoch [47/120    avg_loss:0.270, val_acc:0.837]
Epoch [48/120    avg_loss:0.216, val_acc:0.919]
Epoch [49/120    avg_loss:0.185, val_acc:0.906]
Epoch [50/120    avg_loss:0.155, val_acc:0.924]
Epoch [51/120    avg_loss:0.139, val_acc:0.934]
Epoch [52/120    avg_loss:0.134, val_acc:0.927]
Epoch [53/120    avg_loss:0.116, val_acc:0.940]
Epoch [54/120    avg_loss:0.108, val_acc:0.932]
Epoch [55/120    avg_loss:0.129, val_acc:0.931]
Epoch [56/120    avg_loss:0.163, val_acc:0.934]
Epoch [57/120    avg_loss:0.148, val_acc:0.941]
Epoch [58/120    avg_loss:0.166, val_acc:0.919]
Epoch [59/120    avg_loss:0.152, val_acc:0.899]
Epoch [60/120    avg_loss:0.137, val_acc:0.929]
Epoch [61/120    avg_loss:0.089, val_acc:0.942]
Epoch [62/120    avg_loss:0.079, val_acc:0.942]
Epoch [63/120    avg_loss:0.082, val_acc:0.946]
Epoch [64/120    avg_loss:0.085, val_acc:0.954]
Epoch [65/120    avg_loss:0.103, val_acc:0.948]
Epoch [66/120    avg_loss:0.087, val_acc:0.955]
Epoch [67/120    avg_loss:0.073, val_acc:0.951]
Epoch [68/120    avg_loss:0.074, val_acc:0.954]
Epoch [69/120    avg_loss:0.075, val_acc:0.941]
Epoch [70/120    avg_loss:0.081, val_acc:0.957]
Epoch [71/120    avg_loss:0.077, val_acc:0.950]
Epoch [72/120    avg_loss:0.067, val_acc:0.958]
Epoch [73/120    avg_loss:0.051, val_acc:0.956]
Epoch [74/120    avg_loss:0.045, val_acc:0.961]
Epoch [75/120    avg_loss:0.058, val_acc:0.938]
Epoch [76/120    avg_loss:0.046, val_acc:0.963]
Epoch [77/120    avg_loss:0.047, val_acc:0.964]
Epoch [78/120    avg_loss:0.080, val_acc:0.955]
Epoch [79/120    avg_loss:0.066, val_acc:0.951]
Epoch [80/120    avg_loss:0.052, val_acc:0.961]
Epoch [81/120    avg_loss:0.044, val_acc:0.947]
Epoch [82/120    avg_loss:0.046, val_acc:0.963]
Epoch [83/120    avg_loss:0.042, val_acc:0.958]
Epoch [84/120    avg_loss:0.065, val_acc:0.956]
Epoch [85/120    avg_loss:0.056, val_acc:0.956]
Epoch [86/120    avg_loss:0.048, val_acc:0.958]
Epoch [87/120    avg_loss:0.067, val_acc:0.948]
Epoch [88/120    avg_loss:0.053, val_acc:0.956]
Epoch [89/120    avg_loss:0.047, val_acc:0.962]
Epoch [90/120    avg_loss:0.041, val_acc:0.962]
Epoch [91/120    avg_loss:0.041, val_acc:0.971]
Epoch [92/120    avg_loss:0.029, val_acc:0.971]
Epoch [93/120    avg_loss:0.025, val_acc:0.973]
Epoch [94/120    avg_loss:0.027, val_acc:0.976]
Epoch [95/120    avg_loss:0.028, val_acc:0.971]
Epoch [96/120    avg_loss:0.025, val_acc:0.973]
Epoch [97/120    avg_loss:0.022, val_acc:0.972]
Epoch [98/120    avg_loss:0.022, val_acc:0.972]
Epoch [99/120    avg_loss:0.026, val_acc:0.972]
Epoch [100/120    avg_loss:0.022, val_acc:0.972]
Epoch [101/120    avg_loss:0.025, val_acc:0.973]
Epoch [102/120    avg_loss:0.028, val_acc:0.973]
Epoch [103/120    avg_loss:0.024, val_acc:0.973]
Epoch [104/120    avg_loss:0.021, val_acc:0.975]
Epoch [105/120    avg_loss:0.023, val_acc:0.975]
Epoch [106/120    avg_loss:0.021, val_acc:0.975]
Epoch [107/120    avg_loss:0.021, val_acc:0.973]
Epoch [108/120    avg_loss:0.026, val_acc:0.975]
Epoch [109/120    avg_loss:0.024, val_acc:0.975]
Epoch [110/120    avg_loss:0.025, val_acc:0.975]
Epoch [111/120    avg_loss:0.022, val_acc:0.975]
Epoch [112/120    avg_loss:0.023, val_acc:0.973]
Epoch [113/120    avg_loss:0.020, val_acc:0.973]
Epoch [114/120    avg_loss:0.021, val_acc:0.973]
Epoch [115/120    avg_loss:0.020, val_acc:0.975]
Epoch [116/120    avg_loss:0.020, val_acc:0.973]
Epoch [117/120    avg_loss:0.023, val_acc:0.972]
Epoch [118/120    avg_loss:0.025, val_acc:0.973]
Epoch [119/120    avg_loss:0.022, val_acc:0.973]
Epoch [120/120    avg_loss:0.027, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1219    1    0    0    2    0    0    0    3   58    1    0
     0    1    0]
 [   0    0    2  704    0    9    0    0    0   16    0    0   16    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    7    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    3    0    0   10    0    0    2    0
     0    0    0]
 [   0    0   32   89    0    4    0    0    0    0  738    3    0    0
     1    8    0]
 [   0    0   11    0    0    0    2    0    0    0    4 2190    0    3
     0    0    0]
 [   0    0    0   17    3    4    0    0    0    0   14   13  477    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   18    0    0    1    0    3    2    0    0
  1115    0    0]
 [   0    0    0    0    0    1   50    0    0    0    0    0    0    0
    92  204    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.4390243902439

F1 scores:
[       nan 0.975      0.95645351 0.90198591 0.99300699 0.95449501
 0.95307918 0.98039216 0.99883856 0.41666667 0.90054912 0.97702431
 0.92531523 0.9919571  0.95014913 0.72857143 0.95953757]

Kappa:
0.9364925914766755
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f21998a7860>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.722, val_acc:0.255]
Epoch [2/120    avg_loss:2.409, val_acc:0.315]
Epoch [3/120    avg_loss:2.232, val_acc:0.387]
Epoch [4/120    avg_loss:2.076, val_acc:0.529]
Epoch [5/120    avg_loss:1.942, val_acc:0.561]
Epoch [6/120    avg_loss:1.866, val_acc:0.595]
Epoch [7/120    avg_loss:1.730, val_acc:0.589]
Epoch [8/120    avg_loss:1.630, val_acc:0.619]
Epoch [9/120    avg_loss:1.529, val_acc:0.600]
Epoch [10/120    avg_loss:1.403, val_acc:0.610]
Epoch [11/120    avg_loss:1.329, val_acc:0.650]
Epoch [12/120    avg_loss:1.190, val_acc:0.657]
Epoch [13/120    avg_loss:1.071, val_acc:0.672]
Epoch [14/120    avg_loss:0.935, val_acc:0.694]
Epoch [15/120    avg_loss:0.839, val_acc:0.726]
Epoch [16/120    avg_loss:0.779, val_acc:0.765]
Epoch [17/120    avg_loss:0.750, val_acc:0.749]
Epoch [18/120    avg_loss:0.752, val_acc:0.762]
Epoch [19/120    avg_loss:0.658, val_acc:0.750]
Epoch [20/120    avg_loss:0.618, val_acc:0.792]
Epoch [21/120    avg_loss:0.564, val_acc:0.814]
Epoch [22/120    avg_loss:0.535, val_acc:0.834]
Epoch [23/120    avg_loss:0.505, val_acc:0.823]
Epoch [24/120    avg_loss:0.479, val_acc:0.837]
Epoch [25/120    avg_loss:0.459, val_acc:0.825]
Epoch [26/120    avg_loss:0.418, val_acc:0.824]
Epoch [27/120    avg_loss:0.380, val_acc:0.858]
Epoch [28/120    avg_loss:0.431, val_acc:0.856]
Epoch [29/120    avg_loss:0.378, val_acc:0.812]
Epoch [30/120    avg_loss:0.300, val_acc:0.863]
Epoch [31/120    avg_loss:0.311, val_acc:0.873]
Epoch [32/120    avg_loss:0.283, val_acc:0.878]
Epoch [33/120    avg_loss:0.292, val_acc:0.880]
Epoch [34/120    avg_loss:0.281, val_acc:0.847]
Epoch [35/120    avg_loss:0.401, val_acc:0.855]
Epoch [36/120    avg_loss:0.289, val_acc:0.903]
Epoch [37/120    avg_loss:0.236, val_acc:0.891]
Epoch [38/120    avg_loss:0.217, val_acc:0.902]
Epoch [39/120    avg_loss:0.198, val_acc:0.896]
Epoch [40/120    avg_loss:0.194, val_acc:0.913]
Epoch [41/120    avg_loss:0.188, val_acc:0.910]
Epoch [42/120    avg_loss:0.202, val_acc:0.910]
Epoch [43/120    avg_loss:0.210, val_acc:0.905]
Epoch [44/120    avg_loss:0.208, val_acc:0.914]
Epoch [45/120    avg_loss:0.155, val_acc:0.929]
Epoch [46/120    avg_loss:0.159, val_acc:0.911]
Epoch [47/120    avg_loss:0.160, val_acc:0.918]
Epoch [48/120    avg_loss:0.159, val_acc:0.935]
Epoch [49/120    avg_loss:0.123, val_acc:0.951]
Epoch [50/120    avg_loss:0.126, val_acc:0.940]
Epoch [51/120    avg_loss:0.110, val_acc:0.943]
Epoch [52/120    avg_loss:0.109, val_acc:0.942]
Epoch [53/120    avg_loss:0.117, val_acc:0.947]
Epoch [54/120    avg_loss:0.116, val_acc:0.955]
Epoch [55/120    avg_loss:0.093, val_acc:0.949]
Epoch [56/120    avg_loss:0.091, val_acc:0.956]
Epoch [57/120    avg_loss:0.120, val_acc:0.948]
Epoch [58/120    avg_loss:0.096, val_acc:0.942]
Epoch [59/120    avg_loss:0.102, val_acc:0.943]
Epoch [60/120    avg_loss:0.094, val_acc:0.957]
Epoch [61/120    avg_loss:0.101, val_acc:0.950]
Epoch [62/120    avg_loss:0.137, val_acc:0.928]
Epoch [63/120    avg_loss:0.109, val_acc:0.954]
Epoch [64/120    avg_loss:0.098, val_acc:0.957]
Epoch [65/120    avg_loss:0.081, val_acc:0.942]
Epoch [66/120    avg_loss:0.092, val_acc:0.965]
Epoch [67/120    avg_loss:0.077, val_acc:0.951]
Epoch [68/120    avg_loss:0.089, val_acc:0.955]
Epoch [69/120    avg_loss:0.089, val_acc:0.955]
Epoch [70/120    avg_loss:0.096, val_acc:0.956]
Epoch [71/120    avg_loss:0.076, val_acc:0.964]
Epoch [72/120    avg_loss:0.065, val_acc:0.968]
Epoch [73/120    avg_loss:0.059, val_acc:0.962]
Epoch [74/120    avg_loss:0.060, val_acc:0.964]
Epoch [75/120    avg_loss:0.051, val_acc:0.958]
Epoch [76/120    avg_loss:0.048, val_acc:0.958]
Epoch [77/120    avg_loss:0.072, val_acc:0.962]
Epoch [78/120    avg_loss:0.071, val_acc:0.971]
Epoch [79/120    avg_loss:0.076, val_acc:0.951]
Epoch [80/120    avg_loss:0.067, val_acc:0.958]
Epoch [81/120    avg_loss:0.050, val_acc:0.972]
Epoch [82/120    avg_loss:0.054, val_acc:0.946]
Epoch [83/120    avg_loss:0.066, val_acc:0.971]
Epoch [84/120    avg_loss:0.049, val_acc:0.968]
Epoch [85/120    avg_loss:0.093, val_acc:0.959]
Epoch [86/120    avg_loss:0.113, val_acc:0.941]
Epoch [87/120    avg_loss:0.126, val_acc:0.935]
Epoch [88/120    avg_loss:0.080, val_acc:0.955]
Epoch [89/120    avg_loss:0.082, val_acc:0.957]
Epoch [90/120    avg_loss:0.060, val_acc:0.969]
Epoch [91/120    avg_loss:0.057, val_acc:0.966]
Epoch [92/120    avg_loss:0.075, val_acc:0.966]
Epoch [93/120    avg_loss:0.059, val_acc:0.964]
Epoch [94/120    avg_loss:0.050, val_acc:0.970]
Epoch [95/120    avg_loss:0.035, val_acc:0.971]
Epoch [96/120    avg_loss:0.036, val_acc:0.972]
Epoch [97/120    avg_loss:0.035, val_acc:0.973]
Epoch [98/120    avg_loss:0.028, val_acc:0.971]
Epoch [99/120    avg_loss:0.032, val_acc:0.975]
Epoch [100/120    avg_loss:0.033, val_acc:0.975]
Epoch [101/120    avg_loss:0.032, val_acc:0.973]
Epoch [102/120    avg_loss:0.030, val_acc:0.976]
Epoch [103/120    avg_loss:0.032, val_acc:0.977]
Epoch [104/120    avg_loss:0.026, val_acc:0.977]
Epoch [105/120    avg_loss:0.026, val_acc:0.978]
Epoch [106/120    avg_loss:0.032, val_acc:0.977]
Epoch [107/120    avg_loss:0.030, val_acc:0.978]
Epoch [108/120    avg_loss:0.032, val_acc:0.975]
Epoch [109/120    avg_loss:0.029, val_acc:0.975]
Epoch [110/120    avg_loss:0.026, val_acc:0.977]
Epoch [111/120    avg_loss:0.030, val_acc:0.977]
Epoch [112/120    avg_loss:0.031, val_acc:0.977]
Epoch [113/120    avg_loss:0.029, val_acc:0.976]
Epoch [114/120    avg_loss:0.031, val_acc:0.976]
Epoch [115/120    avg_loss:0.032, val_acc:0.976]
Epoch [116/120    avg_loss:0.024, val_acc:0.977]
Epoch [117/120    avg_loss:0.028, val_acc:0.978]
Epoch [118/120    avg_loss:0.028, val_acc:0.978]
Epoch [119/120    avg_loss:0.031, val_acc:0.977]
Epoch [120/120    avg_loss:0.024, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1233    4    0    0    2    0    0    0    9   35    2    0
     0    0    0]
 [   0    0    1  717    0    9    0    0    0    9    0    0    9    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    1    0    7    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0   12    0    0    1    0
     0    0    0]
 [   0    0   21   73    0   16    0    0    0    0  751    9    0    0
     2    3    0]
 [   0    0   24    0    0    2    7    0    0    0    3 2172    1    1
     0    0    0]
 [   0    0    0   34    9    2    0    0    0    0   16    1  467    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    1    0    0
  1134    0    0]
 [   0    0    0    0    0    0    1    0    0   12    0    0    0    0
   130  204    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.84010840108401

F1 scores:
[       nan 1.         0.96177847 0.90759494 0.97931034 0.95613048
 0.99244713 0.98039216 0.99883856 0.4137931  0.90645745 0.98102981
 0.92019704 0.9919571  0.94225177 0.73646209 0.96511628]

Kappa:
0.9411083444182403
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f03bb3db828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.722, val_acc:0.378]
Epoch [2/120    avg_loss:2.419, val_acc:0.415]
Epoch [3/120    avg_loss:2.202, val_acc:0.540]
Epoch [4/120    avg_loss:2.046, val_acc:0.544]
Epoch [5/120    avg_loss:1.959, val_acc:0.537]
Epoch [6/120    avg_loss:1.815, val_acc:0.594]
Epoch [7/120    avg_loss:1.733, val_acc:0.596]
Epoch [8/120    avg_loss:1.662, val_acc:0.617]
Epoch [9/120    avg_loss:1.567, val_acc:0.616]
Epoch [10/120    avg_loss:1.480, val_acc:0.656]
Epoch [11/120    avg_loss:1.320, val_acc:0.625]
Epoch [12/120    avg_loss:1.254, val_acc:0.705]
Epoch [13/120    avg_loss:1.183, val_acc:0.690]
Epoch [14/120    avg_loss:0.979, val_acc:0.686]
Epoch [15/120    avg_loss:0.891, val_acc:0.763]
Epoch [16/120    avg_loss:0.825, val_acc:0.734]
Epoch [17/120    avg_loss:0.794, val_acc:0.722]
Epoch [18/120    avg_loss:0.736, val_acc:0.782]
Epoch [19/120    avg_loss:0.739, val_acc:0.767]
Epoch [20/120    avg_loss:0.648, val_acc:0.783]
Epoch [21/120    avg_loss:0.577, val_acc:0.818]
Epoch [22/120    avg_loss:0.562, val_acc:0.817]
Epoch [23/120    avg_loss:0.521, val_acc:0.785]
Epoch [24/120    avg_loss:0.562, val_acc:0.751]
Epoch [25/120    avg_loss:0.453, val_acc:0.842]
Epoch [26/120    avg_loss:0.416, val_acc:0.823]
Epoch [27/120    avg_loss:0.393, val_acc:0.840]
Epoch [28/120    avg_loss:0.384, val_acc:0.829]
Epoch [29/120    avg_loss:0.371, val_acc:0.846]
Epoch [30/120    avg_loss:0.346, val_acc:0.855]
Epoch [31/120    avg_loss:0.332, val_acc:0.853]
Epoch [32/120    avg_loss:0.310, val_acc:0.863]
Epoch [33/120    avg_loss:0.303, val_acc:0.842]
Epoch [34/120    avg_loss:0.293, val_acc:0.904]
Epoch [35/120    avg_loss:0.276, val_acc:0.868]
Epoch [36/120    avg_loss:0.251, val_acc:0.886]
Epoch [37/120    avg_loss:0.279, val_acc:0.885]
Epoch [38/120    avg_loss:0.259, val_acc:0.869]
Epoch [39/120    avg_loss:0.304, val_acc:0.863]
Epoch [40/120    avg_loss:0.326, val_acc:0.882]
Epoch [41/120    avg_loss:0.252, val_acc:0.891]
Epoch [42/120    avg_loss:0.252, val_acc:0.884]
Epoch [43/120    avg_loss:0.245, val_acc:0.830]
Epoch [44/120    avg_loss:0.276, val_acc:0.886]
Epoch [45/120    avg_loss:0.232, val_acc:0.895]
Epoch [46/120    avg_loss:0.192, val_acc:0.902]
Epoch [47/120    avg_loss:0.196, val_acc:0.921]
Epoch [48/120    avg_loss:0.185, val_acc:0.887]
Epoch [49/120    avg_loss:0.153, val_acc:0.913]
Epoch [50/120    avg_loss:0.173, val_acc:0.908]
Epoch [51/120    avg_loss:0.163, val_acc:0.924]
Epoch [52/120    avg_loss:0.153, val_acc:0.932]
Epoch [53/120    avg_loss:0.131, val_acc:0.926]
Epoch [54/120    avg_loss:0.129, val_acc:0.921]
Epoch [55/120    avg_loss:0.129, val_acc:0.914]
Epoch [56/120    avg_loss:0.142, val_acc:0.895]
Epoch [57/120    avg_loss:0.142, val_acc:0.927]
Epoch [58/120    avg_loss:0.123, val_acc:0.929]
Epoch [59/120    avg_loss:0.108, val_acc:0.913]
Epoch [60/120    avg_loss:0.114, val_acc:0.929]
Epoch [61/120    avg_loss:0.126, val_acc:0.940]
Epoch [62/120    avg_loss:0.093, val_acc:0.945]
Epoch [63/120    avg_loss:0.093, val_acc:0.946]
Epoch [64/120    avg_loss:0.134, val_acc:0.933]
Epoch [65/120    avg_loss:0.118, val_acc:0.940]
Epoch [66/120    avg_loss:0.089, val_acc:0.950]
Epoch [67/120    avg_loss:0.084, val_acc:0.935]
Epoch [68/120    avg_loss:0.091, val_acc:0.943]
Epoch [69/120    avg_loss:0.108, val_acc:0.936]
Epoch [70/120    avg_loss:0.111, val_acc:0.940]
Epoch [71/120    avg_loss:0.090, val_acc:0.932]
Epoch [72/120    avg_loss:0.119, val_acc:0.943]
Epoch [73/120    avg_loss:0.112, val_acc:0.938]
Epoch [74/120    avg_loss:0.128, val_acc:0.929]
Epoch [75/120    avg_loss:0.091, val_acc:0.943]
Epoch [76/120    avg_loss:0.065, val_acc:0.954]
Epoch [77/120    avg_loss:0.092, val_acc:0.951]
Epoch [78/120    avg_loss:0.112, val_acc:0.936]
Epoch [79/120    avg_loss:0.165, val_acc:0.922]
Epoch [80/120    avg_loss:0.100, val_acc:0.933]
Epoch [81/120    avg_loss:0.095, val_acc:0.950]
Epoch [82/120    avg_loss:0.071, val_acc:0.949]
Epoch [83/120    avg_loss:0.059, val_acc:0.944]
Epoch [84/120    avg_loss:0.071, val_acc:0.950]
Epoch [85/120    avg_loss:0.057, val_acc:0.962]
Epoch [86/120    avg_loss:0.054, val_acc:0.949]
Epoch [87/120    avg_loss:0.058, val_acc:0.952]
Epoch [88/120    avg_loss:0.051, val_acc:0.965]
Epoch [89/120    avg_loss:0.059, val_acc:0.961]
Epoch [90/120    avg_loss:0.051, val_acc:0.960]
Epoch [91/120    avg_loss:0.054, val_acc:0.962]
Epoch [92/120    avg_loss:0.042, val_acc:0.952]
Epoch [93/120    avg_loss:0.070, val_acc:0.954]
Epoch [94/120    avg_loss:0.058, val_acc:0.965]
Epoch [95/120    avg_loss:0.047, val_acc:0.950]
Epoch [96/120    avg_loss:0.036, val_acc:0.965]
Epoch [97/120    avg_loss:0.037, val_acc:0.960]
Epoch [98/120    avg_loss:0.033, val_acc:0.964]
Epoch [99/120    avg_loss:0.034, val_acc:0.971]
Epoch [100/120    avg_loss:0.045, val_acc:0.960]
Epoch [101/120    avg_loss:0.042, val_acc:0.961]
Epoch [102/120    avg_loss:0.037, val_acc:0.964]
Epoch [103/120    avg_loss:0.032, val_acc:0.973]
Epoch [104/120    avg_loss:0.039, val_acc:0.942]
Epoch [105/120    avg_loss:0.745, val_acc:0.480]
Epoch [106/120    avg_loss:0.971, val_acc:0.789]
Epoch [107/120    avg_loss:0.530, val_acc:0.854]
Epoch [108/120    avg_loss:0.354, val_acc:0.837]
Epoch [109/120    avg_loss:0.255, val_acc:0.911]
Epoch [110/120    avg_loss:0.172, val_acc:0.912]
Epoch [111/120    avg_loss:0.135, val_acc:0.930]
Epoch [112/120    avg_loss:0.119, val_acc:0.935]
Epoch [113/120    avg_loss:0.125, val_acc:0.942]
Epoch [114/120    avg_loss:0.096, val_acc:0.933]
Epoch [115/120    avg_loss:0.087, val_acc:0.936]
Epoch [116/120    avg_loss:0.127, val_acc:0.944]
Epoch [117/120    avg_loss:0.074, val_acc:0.955]
Epoch [118/120    avg_loss:0.059, val_acc:0.960]
Epoch [119/120    avg_loss:0.049, val_acc:0.961]
Epoch [120/120    avg_loss:0.045, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1235    1    1    1    1    0    0    0    5   38    0    0
     3    0    0]
 [   0    0    1  661   24   19    0    0    0    3    0    0   30    7
     2    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  419    0    4    0    2    0    0    0    0
    10    0    0]
 [   0    0    0    0    0    0  648    0    0    0    0    2    0    0
     7    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   13    0    0    3    0
     0    0    0]
 [   0    0   42   84    0    6    0    0    0    0  717   17    1    0
     0    8    0]
 [   0    0    5    0    0    6   16    0    0    0   16 2159    6    1
     1    0    0]
 [   0    0    1   10    1    3    0    0    0    0    7    0  506    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   12    0    0    1    0    4    2    0    0
  1120    0    0]
 [   0    0    0    0    0    0   10    0    0    4    0    0    0    0
   114  219    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
93.98373983739837

F1 scores:
[       nan 0.98765432 0.9614636  0.87723955 0.93777778 0.93007769
 0.97297297 0.92592593 0.99883856 0.65       0.88246154 0.97515808
 0.93530499 0.97883598 0.93489149 0.7630662  0.95348837]

Kappa:
0.9313634078964331
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f05e0f047f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.740, val_acc:0.280]
Epoch [2/120    avg_loss:2.379, val_acc:0.383]
Epoch [3/120    avg_loss:2.178, val_acc:0.516]
Epoch [4/120    avg_loss:2.002, val_acc:0.525]
Epoch [5/120    avg_loss:1.906, val_acc:0.566]
Epoch [6/120    avg_loss:1.812, val_acc:0.543]
Epoch [7/120    avg_loss:1.705, val_acc:0.589]
Epoch [8/120    avg_loss:1.612, val_acc:0.596]
Epoch [9/120    avg_loss:1.476, val_acc:0.618]
Epoch [10/120    avg_loss:1.419, val_acc:0.602]
Epoch [11/120    avg_loss:1.265, val_acc:0.678]
Epoch [12/120    avg_loss:1.142, val_acc:0.688]
Epoch [13/120    avg_loss:1.062, val_acc:0.670]
Epoch [14/120    avg_loss:0.997, val_acc:0.716]
Epoch [15/120    avg_loss:0.853, val_acc:0.734]
Epoch [16/120    avg_loss:0.798, val_acc:0.762]
Epoch [17/120    avg_loss:0.782, val_acc:0.731]
Epoch [18/120    avg_loss:0.731, val_acc:0.745]
Epoch [19/120    avg_loss:0.746, val_acc:0.766]
Epoch [20/120    avg_loss:0.618, val_acc:0.782]
Epoch [21/120    avg_loss:0.614, val_acc:0.797]
Epoch [22/120    avg_loss:0.629, val_acc:0.851]
Epoch [23/120    avg_loss:0.578, val_acc:0.804]
Epoch [24/120    avg_loss:0.530, val_acc:0.834]
Epoch [25/120    avg_loss:0.460, val_acc:0.831]
Epoch [26/120    avg_loss:0.400, val_acc:0.873]
Epoch [27/120    avg_loss:0.408, val_acc:0.854]
Epoch [28/120    avg_loss:0.399, val_acc:0.867]
Epoch [29/120    avg_loss:0.334, val_acc:0.867]
Epoch [30/120    avg_loss:0.305, val_acc:0.897]
Epoch [31/120    avg_loss:0.315, val_acc:0.877]
Epoch [32/120    avg_loss:0.252, val_acc:0.907]
Epoch [33/120    avg_loss:0.222, val_acc:0.904]
Epoch [34/120    avg_loss:0.271, val_acc:0.895]
Epoch [35/120    avg_loss:0.331, val_acc:0.881]
Epoch [36/120    avg_loss:0.261, val_acc:0.912]
Epoch [37/120    avg_loss:0.204, val_acc:0.894]
Epoch [38/120    avg_loss:0.222, val_acc:0.911]
Epoch [39/120    avg_loss:0.197, val_acc:0.921]
Epoch [40/120    avg_loss:0.245, val_acc:0.862]
Epoch [41/120    avg_loss:0.259, val_acc:0.899]
Epoch [42/120    avg_loss:0.173, val_acc:0.916]
Epoch [43/120    avg_loss:0.140, val_acc:0.932]
Epoch [44/120    avg_loss:0.129, val_acc:0.934]
Epoch [45/120    avg_loss:0.137, val_acc:0.920]
Epoch [46/120    avg_loss:0.152, val_acc:0.903]
Epoch [47/120    avg_loss:0.140, val_acc:0.927]
Epoch [48/120    avg_loss:0.137, val_acc:0.931]
Epoch [49/120    avg_loss:0.122, val_acc:0.914]
Epoch [50/120    avg_loss:0.110, val_acc:0.931]
Epoch [51/120    avg_loss:0.137, val_acc:0.914]
Epoch [52/120    avg_loss:0.168, val_acc:0.913]
Epoch [53/120    avg_loss:0.169, val_acc:0.935]
Epoch [54/120    avg_loss:0.135, val_acc:0.935]
Epoch [55/120    avg_loss:0.136, val_acc:0.929]
Epoch [56/120    avg_loss:0.118, val_acc:0.935]
Epoch [57/120    avg_loss:0.116, val_acc:0.931]
Epoch [58/120    avg_loss:0.118, val_acc:0.932]
Epoch [59/120    avg_loss:0.159, val_acc:0.906]
Epoch [60/120    avg_loss:0.220, val_acc:0.904]
Epoch [61/120    avg_loss:0.220, val_acc:0.924]
Epoch [62/120    avg_loss:0.164, val_acc:0.933]
Epoch [63/120    avg_loss:0.126, val_acc:0.904]
Epoch [64/120    avg_loss:0.100, val_acc:0.927]
Epoch [65/120    avg_loss:0.119, val_acc:0.943]
Epoch [66/120    avg_loss:0.111, val_acc:0.943]
Epoch [67/120    avg_loss:0.102, val_acc:0.939]
Epoch [68/120    avg_loss:0.105, val_acc:0.914]
Epoch [69/120    avg_loss:0.148, val_acc:0.942]
Epoch [70/120    avg_loss:0.116, val_acc:0.940]
Epoch [71/120    avg_loss:0.140, val_acc:0.876]
Epoch [72/120    avg_loss:0.140, val_acc:0.933]
Epoch [73/120    avg_loss:0.099, val_acc:0.928]
Epoch [74/120    avg_loss:0.078, val_acc:0.958]
Epoch [75/120    avg_loss:0.074, val_acc:0.946]
Epoch [76/120    avg_loss:0.068, val_acc:0.954]
Epoch [77/120    avg_loss:0.058, val_acc:0.954]
Epoch [78/120    avg_loss:0.063, val_acc:0.946]
Epoch [79/120    avg_loss:0.068, val_acc:0.950]
Epoch [80/120    avg_loss:0.076, val_acc:0.955]
Epoch [81/120    avg_loss:0.071, val_acc:0.949]
Epoch [82/120    avg_loss:0.062, val_acc:0.950]
Epoch [83/120    avg_loss:0.072, val_acc:0.957]
Epoch [84/120    avg_loss:0.055, val_acc:0.957]
Epoch [85/120    avg_loss:0.041, val_acc:0.964]
Epoch [86/120    avg_loss:0.048, val_acc:0.955]
Epoch [87/120    avg_loss:0.040, val_acc:0.957]
Epoch [88/120    avg_loss:0.047, val_acc:0.964]
Epoch [89/120    avg_loss:0.033, val_acc:0.964]
Epoch [90/120    avg_loss:0.033, val_acc:0.965]
Epoch [91/120    avg_loss:0.054, val_acc:0.941]
Epoch [92/120    avg_loss:0.079, val_acc:0.943]
Epoch [93/120    avg_loss:0.070, val_acc:0.939]
Epoch [94/120    avg_loss:0.048, val_acc:0.955]
Epoch [95/120    avg_loss:0.039, val_acc:0.961]
Epoch [96/120    avg_loss:0.041, val_acc:0.969]
Epoch [97/120    avg_loss:0.032, val_acc:0.969]
Epoch [98/120    avg_loss:0.034, val_acc:0.961]
Epoch [99/120    avg_loss:0.032, val_acc:0.971]
Epoch [100/120    avg_loss:0.030, val_acc:0.968]
Epoch [101/120    avg_loss:0.027, val_acc:0.962]
Epoch [102/120    avg_loss:0.032, val_acc:0.968]
Epoch [103/120    avg_loss:0.026, val_acc:0.962]
Epoch [104/120    avg_loss:0.040, val_acc:0.971]
Epoch [105/120    avg_loss:0.031, val_acc:0.971]
Epoch [106/120    avg_loss:0.048, val_acc:0.965]
Epoch [107/120    avg_loss:0.034, val_acc:0.944]
Epoch [108/120    avg_loss:0.042, val_acc:0.962]
Epoch [109/120    avg_loss:0.035, val_acc:0.961]
Epoch [110/120    avg_loss:0.044, val_acc:0.965]
Epoch [111/120    avg_loss:0.036, val_acc:0.959]
Epoch [112/120    avg_loss:0.031, val_acc:0.966]
Epoch [113/120    avg_loss:0.034, val_acc:0.962]
Epoch [114/120    avg_loss:0.030, val_acc:0.968]
Epoch [115/120    avg_loss:0.042, val_acc:0.963]
Epoch [116/120    avg_loss:0.034, val_acc:0.971]
Epoch [117/120    avg_loss:0.027, val_acc:0.968]
Epoch [118/120    avg_loss:0.029, val_acc:0.962]
Epoch [119/120    avg_loss:0.021, val_acc:0.973]
Epoch [120/120    avg_loss:0.024, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    5 1205    7    0    0    0    0    0    0   38   29    0    0
     0    1    0]
 [   0    0    1  679    1   16    0    0    0   16    0    0   34    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    0    0    8    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   13    0    0    4    0
     0    0    0]
 [   0    0   35   90    0    0    0    0    0    0  744    5    0    0
     0    1    0]
 [   0    0   24    0    0    0    0    0    0    0    5 2176    1    3
     1    0    0]
 [   0    0    1    0    7    2    0    0    0    0   16    6  497    0
     0    0    5]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    2    0    0
  1136    0    0]
 [   0    0    0    0    0    0    0    0    0    2    0    0    0    0
   137  208    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
94.36314363143632

F1 scores:
[       nan 0.89156627 0.94472756 0.89107612 0.98156682 0.96465222
 0.99771167 1.         1.         0.45614035 0.88413547 0.98217107
 0.92637465 0.98924731 0.94000827 0.74685817 0.95294118]

Kappa:
0.9356650205700938
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5f1e7c17b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.711, val_acc:0.348]
Epoch [2/120    avg_loss:2.377, val_acc:0.432]
Epoch [3/120    avg_loss:2.162, val_acc:0.485]
Epoch [4/120    avg_loss:2.019, val_acc:0.513]
Epoch [5/120    avg_loss:1.865, val_acc:0.516]
Epoch [6/120    avg_loss:1.834, val_acc:0.531]
Epoch [7/120    avg_loss:1.721, val_acc:0.586]
Epoch [8/120    avg_loss:1.630, val_acc:0.577]
Epoch [9/120    avg_loss:1.511, val_acc:0.605]
Epoch [10/120    avg_loss:1.373, val_acc:0.648]
Epoch [11/120    avg_loss:1.290, val_acc:0.619]
Epoch [12/120    avg_loss:1.182, val_acc:0.699]
Epoch [13/120    avg_loss:1.069, val_acc:0.679]
Epoch [14/120    avg_loss:1.004, val_acc:0.679]
Epoch [15/120    avg_loss:0.898, val_acc:0.706]
Epoch [16/120    avg_loss:0.842, val_acc:0.700]
Epoch [17/120    avg_loss:0.780, val_acc:0.748]
Epoch [18/120    avg_loss:0.809, val_acc:0.722]
Epoch [19/120    avg_loss:0.769, val_acc:0.746]
Epoch [20/120    avg_loss:0.611, val_acc:0.798]
Epoch [21/120    avg_loss:0.575, val_acc:0.786]
Epoch [22/120    avg_loss:0.548, val_acc:0.826]
Epoch [23/120    avg_loss:0.451, val_acc:0.831]
Epoch [24/120    avg_loss:0.482, val_acc:0.830]
Epoch [25/120    avg_loss:0.501, val_acc:0.853]
Epoch [26/120    avg_loss:0.444, val_acc:0.846]
Epoch [27/120    avg_loss:0.444, val_acc:0.828]
Epoch [28/120    avg_loss:0.392, val_acc:0.885]
Epoch [29/120    avg_loss:0.325, val_acc:0.878]
Epoch [30/120    avg_loss:0.324, val_acc:0.830]
Epoch [31/120    avg_loss:0.335, val_acc:0.874]
Epoch [32/120    avg_loss:0.291, val_acc:0.911]
Epoch [33/120    avg_loss:0.248, val_acc:0.904]
Epoch [34/120    avg_loss:0.238, val_acc:0.908]
Epoch [35/120    avg_loss:0.266, val_acc:0.913]
Epoch [36/120    avg_loss:0.235, val_acc:0.890]
Epoch [37/120    avg_loss:0.236, val_acc:0.915]
Epoch [38/120    avg_loss:0.255, val_acc:0.924]
Epoch [39/120    avg_loss:0.224, val_acc:0.905]
Epoch [40/120    avg_loss:0.183, val_acc:0.910]
Epoch [41/120    avg_loss:0.220, val_acc:0.898]
Epoch [42/120    avg_loss:0.221, val_acc:0.910]
Epoch [43/120    avg_loss:0.226, val_acc:0.863]
Epoch [44/120    avg_loss:0.230, val_acc:0.896]
Epoch [45/120    avg_loss:0.176, val_acc:0.921]
Epoch [46/120    avg_loss:0.151, val_acc:0.925]
Epoch [47/120    avg_loss:0.152, val_acc:0.914]
Epoch [48/120    avg_loss:0.236, val_acc:0.900]
Epoch [49/120    avg_loss:0.228, val_acc:0.916]
Epoch [50/120    avg_loss:0.167, val_acc:0.925]
Epoch [51/120    avg_loss:0.175, val_acc:0.932]
Epoch [52/120    avg_loss:0.144, val_acc:0.933]
Epoch [53/120    avg_loss:0.121, val_acc:0.917]
Epoch [54/120    avg_loss:0.140, val_acc:0.926]
Epoch [55/120    avg_loss:0.117, val_acc:0.931]
Epoch [56/120    avg_loss:0.110, val_acc:0.935]
Epoch [57/120    avg_loss:0.085, val_acc:0.950]
Epoch [58/120    avg_loss:0.094, val_acc:0.949]
Epoch [59/120    avg_loss:0.097, val_acc:0.949]
Epoch [60/120    avg_loss:0.092, val_acc:0.944]
Epoch [61/120    avg_loss:0.080, val_acc:0.951]
Epoch [62/120    avg_loss:0.103, val_acc:0.925]
Epoch [63/120    avg_loss:0.101, val_acc:0.935]
Epoch [64/120    avg_loss:0.088, val_acc:0.921]
Epoch [65/120    avg_loss:0.089, val_acc:0.942]
Epoch [66/120    avg_loss:0.080, val_acc:0.949]
Epoch [67/120    avg_loss:0.069, val_acc:0.940]
Epoch [68/120    avg_loss:0.073, val_acc:0.953]
Epoch [69/120    avg_loss:0.054, val_acc:0.959]
Epoch [70/120    avg_loss:0.058, val_acc:0.955]
Epoch [71/120    avg_loss:0.054, val_acc:0.964]
Epoch [72/120    avg_loss:0.071, val_acc:0.951]
Epoch [73/120    avg_loss:0.079, val_acc:0.954]
Epoch [74/120    avg_loss:0.059, val_acc:0.964]
Epoch [75/120    avg_loss:0.054, val_acc:0.964]
Epoch [76/120    avg_loss:0.056, val_acc:0.954]
Epoch [77/120    avg_loss:0.082, val_acc:0.934]
Epoch [78/120    avg_loss:0.066, val_acc:0.952]
Epoch [79/120    avg_loss:0.055, val_acc:0.968]
Epoch [80/120    avg_loss:0.056, val_acc:0.946]
Epoch [81/120    avg_loss:0.058, val_acc:0.952]
Epoch [82/120    avg_loss:0.046, val_acc:0.969]
Epoch [83/120    avg_loss:0.042, val_acc:0.960]
Epoch [84/120    avg_loss:0.039, val_acc:0.962]
Epoch [85/120    avg_loss:0.042, val_acc:0.965]
Epoch [86/120    avg_loss:0.038, val_acc:0.964]
Epoch [87/120    avg_loss:0.038, val_acc:0.960]
Epoch [88/120    avg_loss:0.046, val_acc:0.969]
Epoch [89/120    avg_loss:0.043, val_acc:0.964]
Epoch [90/120    avg_loss:0.058, val_acc:0.964]
Epoch [91/120    avg_loss:0.051, val_acc:0.965]
Epoch [92/120    avg_loss:0.047, val_acc:0.969]
Epoch [93/120    avg_loss:0.055, val_acc:0.962]
Epoch [94/120    avg_loss:0.033, val_acc:0.969]
Epoch [95/120    avg_loss:0.038, val_acc:0.965]
Epoch [96/120    avg_loss:0.037, val_acc:0.969]
Epoch [97/120    avg_loss:0.040, val_acc:0.961]
Epoch [98/120    avg_loss:0.028, val_acc:0.968]
Epoch [99/120    avg_loss:0.035, val_acc:0.968]
Epoch [100/120    avg_loss:0.038, val_acc:0.956]
Epoch [101/120    avg_loss:0.052, val_acc:0.950]
Epoch [102/120    avg_loss:0.074, val_acc:0.940]
Epoch [103/120    avg_loss:0.060, val_acc:0.954]
Epoch [104/120    avg_loss:0.038, val_acc:0.963]
Epoch [105/120    avg_loss:0.035, val_acc:0.972]
Epoch [106/120    avg_loss:0.041, val_acc:0.971]
Epoch [107/120    avg_loss:0.040, val_acc:0.980]
Epoch [108/120    avg_loss:0.029, val_acc:0.973]
Epoch [109/120    avg_loss:0.022, val_acc:0.969]
Epoch [110/120    avg_loss:0.030, val_acc:0.968]
Epoch [111/120    avg_loss:0.036, val_acc:0.972]
Epoch [112/120    avg_loss:0.027, val_acc:0.970]
Epoch [113/120    avg_loss:0.034, val_acc:0.962]
Epoch [114/120    avg_loss:0.030, val_acc:0.971]
Epoch [115/120    avg_loss:0.025, val_acc:0.971]
Epoch [116/120    avg_loss:0.023, val_acc:0.969]
Epoch [117/120    avg_loss:0.021, val_acc:0.979]
Epoch [118/120    avg_loss:0.025, val_acc:0.977]
Epoch [119/120    avg_loss:0.020, val_acc:0.967]
Epoch [120/120    avg_loss:0.037, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    2 1210    9    0    0    2    0    0    0   26   24    9    0
     0    3    0]
 [   0    0    5  720    0    0    0    0    0   14    0    0    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  412    0   10    0    6    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   12    0    0    2    1
     0    0    0]
 [   0    0   20   86    0    7    4    0    0    0  747    4    0    0
     0    7    0]
 [   0    0   11    0    0    4    6    0    6    0   23 2152    4    4
     0    0    0]
 [   0    0    0   12   14   10    0    0    0    0    6    7  480    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    3    2    0    0
  1126    0    0]
 [   0    0    0    0    0    0   26    0    0    4    0    0    0    0
    68  249    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
94.67750677506776

F1 scores:
[       nan 0.925      0.95614382 0.91312619 0.96818182 0.94171429
 0.97037037 0.83333333 0.99192618 0.44444444 0.8871734  0.97795955
 0.92307692 0.98666667 0.96239316 0.82178218 0.95294118]

Kappa:
0.9393425708119635
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd004093898>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.685, val_acc:0.353]
Epoch [2/120    avg_loss:2.355, val_acc:0.461]
Epoch [3/120    avg_loss:2.185, val_acc:0.504]
Epoch [4/120    avg_loss:2.002, val_acc:0.551]
Epoch [5/120    avg_loss:1.879, val_acc:0.590]
Epoch [6/120    avg_loss:1.760, val_acc:0.623]
Epoch [7/120    avg_loss:1.627, val_acc:0.645]
Epoch [8/120    avg_loss:1.484, val_acc:0.637]
Epoch [9/120    avg_loss:1.383, val_acc:0.656]
Epoch [10/120    avg_loss:1.196, val_acc:0.667]
Epoch [11/120    avg_loss:1.109, val_acc:0.657]
Epoch [12/120    avg_loss:1.002, val_acc:0.688]
Epoch [13/120    avg_loss:0.878, val_acc:0.705]
Epoch [14/120    avg_loss:0.800, val_acc:0.725]
Epoch [15/120    avg_loss:0.766, val_acc:0.730]
Epoch [16/120    avg_loss:0.691, val_acc:0.771]
Epoch [17/120    avg_loss:0.612, val_acc:0.757]
Epoch [18/120    avg_loss:0.591, val_acc:0.788]
Epoch [19/120    avg_loss:0.521, val_acc:0.786]
Epoch [20/120    avg_loss:0.569, val_acc:0.796]
Epoch [21/120    avg_loss:0.501, val_acc:0.809]
Epoch [22/120    avg_loss:0.487, val_acc:0.768]
Epoch [23/120    avg_loss:0.452, val_acc:0.806]
Epoch [24/120    avg_loss:0.436, val_acc:0.816]
Epoch [25/120    avg_loss:0.396, val_acc:0.807]
Epoch [26/120    avg_loss:0.356, val_acc:0.840]
Epoch [27/120    avg_loss:0.371, val_acc:0.854]
Epoch [28/120    avg_loss:0.404, val_acc:0.840]
Epoch [29/120    avg_loss:0.366, val_acc:0.871]
Epoch [30/120    avg_loss:0.301, val_acc:0.872]
Epoch [31/120    avg_loss:0.288, val_acc:0.875]
Epoch [32/120    avg_loss:0.251, val_acc:0.898]
Epoch [33/120    avg_loss:0.211, val_acc:0.920]
Epoch [34/120    avg_loss:0.198, val_acc:0.903]
Epoch [35/120    avg_loss:0.244, val_acc:0.854]
Epoch [36/120    avg_loss:0.247, val_acc:0.898]
Epoch [37/120    avg_loss:0.203, val_acc:0.912]
Epoch [38/120    avg_loss:0.194, val_acc:0.895]
Epoch [39/120    avg_loss:0.213, val_acc:0.878]
Epoch [40/120    avg_loss:0.203, val_acc:0.887]
Epoch [41/120    avg_loss:0.168, val_acc:0.907]
Epoch [42/120    avg_loss:0.161, val_acc:0.917]
Epoch [43/120    avg_loss:0.152, val_acc:0.924]
Epoch [44/120    avg_loss:0.142, val_acc:0.934]
Epoch [45/120    avg_loss:0.143, val_acc:0.926]
Epoch [46/120    avg_loss:0.141, val_acc:0.919]
Epoch [47/120    avg_loss:0.127, val_acc:0.914]
Epoch [48/120    avg_loss:0.118, val_acc:0.925]
Epoch [49/120    avg_loss:0.156, val_acc:0.919]
Epoch [50/120    avg_loss:0.162, val_acc:0.927]
Epoch [51/120    avg_loss:0.183, val_acc:0.927]
Epoch [52/120    avg_loss:0.131, val_acc:0.921]
Epoch [53/120    avg_loss:0.150, val_acc:0.924]
Epoch [54/120    avg_loss:0.102, val_acc:0.932]
Epoch [55/120    avg_loss:0.091, val_acc:0.930]
Epoch [56/120    avg_loss:0.095, val_acc:0.946]
Epoch [57/120    avg_loss:0.085, val_acc:0.933]
Epoch [58/120    avg_loss:0.077, val_acc:0.942]
Epoch [59/120    avg_loss:0.075, val_acc:0.943]
Epoch [60/120    avg_loss:0.093, val_acc:0.931]
Epoch [61/120    avg_loss:0.088, val_acc:0.944]
Epoch [62/120    avg_loss:0.112, val_acc:0.927]
Epoch [63/120    avg_loss:0.136, val_acc:0.912]
Epoch [64/120    avg_loss:0.158, val_acc:0.910]
Epoch [65/120    avg_loss:0.134, val_acc:0.926]
Epoch [66/120    avg_loss:0.105, val_acc:0.943]
Epoch [67/120    avg_loss:0.131, val_acc:0.933]
Epoch [68/120    avg_loss:0.102, val_acc:0.930]
Epoch [69/120    avg_loss:0.097, val_acc:0.940]
Epoch [70/120    avg_loss:0.086, val_acc:0.946]
Epoch [71/120    avg_loss:0.068, val_acc:0.953]
Epoch [72/120    avg_loss:0.066, val_acc:0.953]
Epoch [73/120    avg_loss:0.062, val_acc:0.952]
Epoch [74/120    avg_loss:0.053, val_acc:0.949]
Epoch [75/120    avg_loss:0.047, val_acc:0.952]
Epoch [76/120    avg_loss:0.059, val_acc:0.953]
Epoch [77/120    avg_loss:0.055, val_acc:0.953]
Epoch [78/120    avg_loss:0.051, val_acc:0.953]
Epoch [79/120    avg_loss:0.053, val_acc:0.954]
Epoch [80/120    avg_loss:0.050, val_acc:0.954]
Epoch [81/120    avg_loss:0.047, val_acc:0.952]
Epoch [82/120    avg_loss:0.047, val_acc:0.953]
Epoch [83/120    avg_loss:0.049, val_acc:0.959]
Epoch [84/120    avg_loss:0.053, val_acc:0.958]
Epoch [85/120    avg_loss:0.045, val_acc:0.960]
Epoch [86/120    avg_loss:0.050, val_acc:0.956]
Epoch [87/120    avg_loss:0.047, val_acc:0.958]
Epoch [88/120    avg_loss:0.037, val_acc:0.959]
Epoch [89/120    avg_loss:0.040, val_acc:0.959]
Epoch [90/120    avg_loss:0.047, val_acc:0.959]
Epoch [91/120    avg_loss:0.053, val_acc:0.956]
Epoch [92/120    avg_loss:0.044, val_acc:0.956]
Epoch [93/120    avg_loss:0.045, val_acc:0.963]
Epoch [94/120    avg_loss:0.051, val_acc:0.962]
Epoch [95/120    avg_loss:0.043, val_acc:0.962]
Epoch [96/120    avg_loss:0.042, val_acc:0.959]
Epoch [97/120    avg_loss:0.041, val_acc:0.963]
Epoch [98/120    avg_loss:0.043, val_acc:0.959]
Epoch [99/120    avg_loss:0.041, val_acc:0.960]
Epoch [100/120    avg_loss:0.035, val_acc:0.956]
Epoch [101/120    avg_loss:0.038, val_acc:0.960]
Epoch [102/120    avg_loss:0.041, val_acc:0.963]
Epoch [103/120    avg_loss:0.040, val_acc:0.964]
Epoch [104/120    avg_loss:0.037, val_acc:0.962]
Epoch [105/120    avg_loss:0.040, val_acc:0.961]
Epoch [106/120    avg_loss:0.038, val_acc:0.962]
Epoch [107/120    avg_loss:0.049, val_acc:0.963]
Epoch [108/120    avg_loss:0.033, val_acc:0.962]
Epoch [109/120    avg_loss:0.035, val_acc:0.961]
Epoch [110/120    avg_loss:0.037, val_acc:0.964]
Epoch [111/120    avg_loss:0.052, val_acc:0.956]
Epoch [112/120    avg_loss:0.040, val_acc:0.959]
Epoch [113/120    avg_loss:0.039, val_acc:0.960]
Epoch [114/120    avg_loss:0.045, val_acc:0.962]
Epoch [115/120    avg_loss:0.038, val_acc:0.963]
Epoch [116/120    avg_loss:0.039, val_acc:0.963]
Epoch [117/120    avg_loss:0.038, val_acc:0.961]
Epoch [118/120    avg_loss:0.035, val_acc:0.961]
Epoch [119/120    avg_loss:0.041, val_acc:0.961]
Epoch [120/120    avg_loss:0.036, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1248    0    1    0    6    0    0    0    3   20    0    0
     0    7    0]
 [   0    0    5  697    4   10    0    0    0    9    0    0   19    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    1    0    4    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    1    0    0   13    0    0    2    0
     0    0    0]
 [   0    0   62   78    0    1    5    0    0    0  716    7    0    0
     0    6    0]
 [   0    0   52    0    0    0   13    0    0    0    5 2132    2    5
     1    0    0]
 [   0    0    1   24   11    0    0    0    0    0   15    0  475    0
     0    0    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   21    0    0    0    0    1    0    0    0
  1117    0    0]
 [   0    0    0    0    0    0   28    0    0    0    0    0    0    0
   125  194    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
93.74525745257452

F1 scores:
[       nan 0.975      0.94011299 0.9005168  0.9638009  0.95408735
 0.95970696 0.98039216 1.         0.57777778 0.88668731 0.97574371
 0.9196515  0.97883598 0.93629505 0.70036101 0.94857143]

Kappa:
0.9286656255499263
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0f6a38e898>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.662, val_acc:0.307]
Epoch [2/120    avg_loss:2.335, val_acc:0.402]
Epoch [3/120    avg_loss:2.180, val_acc:0.502]
Epoch [4/120    avg_loss:2.046, val_acc:0.547]
Epoch [5/120    avg_loss:1.913, val_acc:0.568]
Epoch [6/120    avg_loss:1.783, val_acc:0.607]
Epoch [7/120    avg_loss:1.690, val_acc:0.653]
Epoch [8/120    avg_loss:1.627, val_acc:0.634]
Epoch [9/120    avg_loss:1.461, val_acc:0.650]
Epoch [10/120    avg_loss:1.348, val_acc:0.664]
Epoch [11/120    avg_loss:1.275, val_acc:0.672]
Epoch [12/120    avg_loss:1.140, val_acc:0.693]
Epoch [13/120    avg_loss:1.037, val_acc:0.677]
Epoch [14/120    avg_loss:0.945, val_acc:0.714]
Epoch [15/120    avg_loss:0.822, val_acc:0.717]
Epoch [16/120    avg_loss:0.856, val_acc:0.693]
Epoch [17/120    avg_loss:0.736, val_acc:0.737]
Epoch [18/120    avg_loss:0.644, val_acc:0.731]
Epoch [19/120    avg_loss:0.674, val_acc:0.769]
Epoch [20/120    avg_loss:0.557, val_acc:0.806]
Epoch [21/120    avg_loss:0.522, val_acc:0.775]
Epoch [22/120    avg_loss:0.577, val_acc:0.790]
Epoch [23/120    avg_loss:0.485, val_acc:0.833]
Epoch [24/120    avg_loss:0.477, val_acc:0.810]
Epoch [25/120    avg_loss:0.405, val_acc:0.816]
Epoch [26/120    avg_loss:0.419, val_acc:0.825]
Epoch [27/120    avg_loss:0.378, val_acc:0.843]
Epoch [28/120    avg_loss:0.328, val_acc:0.839]
Epoch [29/120    avg_loss:0.305, val_acc:0.853]
Epoch [30/120    avg_loss:0.294, val_acc:0.844]
Epoch [31/120    avg_loss:0.291, val_acc:0.877]
Epoch [32/120    avg_loss:0.328, val_acc:0.884]
Epoch [33/120    avg_loss:0.275, val_acc:0.869]
Epoch [34/120    avg_loss:0.238, val_acc:0.887]
Epoch [35/120    avg_loss:0.237, val_acc:0.887]
Epoch [36/120    avg_loss:0.257, val_acc:0.892]
Epoch [37/120    avg_loss:0.283, val_acc:0.865]
Epoch [38/120    avg_loss:0.222, val_acc:0.900]
Epoch [39/120    avg_loss:0.219, val_acc:0.897]
Epoch [40/120    avg_loss:0.186, val_acc:0.904]
Epoch [41/120    avg_loss:0.182, val_acc:0.910]
Epoch [42/120    avg_loss:0.248, val_acc:0.878]
Epoch [43/120    avg_loss:0.350, val_acc:0.886]
Epoch [44/120    avg_loss:0.216, val_acc:0.901]
Epoch [45/120    avg_loss:0.236, val_acc:0.881]
Epoch [46/120    avg_loss:0.257, val_acc:0.884]
Epoch [47/120    avg_loss:0.194, val_acc:0.902]
Epoch [48/120    avg_loss:0.166, val_acc:0.917]
Epoch [49/120    avg_loss:0.191, val_acc:0.922]
Epoch [50/120    avg_loss:0.151, val_acc:0.921]
Epoch [51/120    avg_loss:0.151, val_acc:0.904]
Epoch [52/120    avg_loss:0.197, val_acc:0.914]
Epoch [53/120    avg_loss:0.186, val_acc:0.911]
Epoch [54/120    avg_loss:0.162, val_acc:0.906]
Epoch [55/120    avg_loss:0.164, val_acc:0.910]
Epoch [56/120    avg_loss:0.140, val_acc:0.938]
Epoch [57/120    avg_loss:0.112, val_acc:0.936]
Epoch [58/120    avg_loss:0.105, val_acc:0.940]
Epoch [59/120    avg_loss:0.085, val_acc:0.948]
Epoch [60/120    avg_loss:0.087, val_acc:0.924]
Epoch [61/120    avg_loss:0.096, val_acc:0.925]
Epoch [62/120    avg_loss:0.101, val_acc:0.952]
Epoch [63/120    avg_loss:0.079, val_acc:0.940]
Epoch [64/120    avg_loss:0.080, val_acc:0.949]
Epoch [65/120    avg_loss:0.070, val_acc:0.948]
Epoch [66/120    avg_loss:0.070, val_acc:0.950]
Epoch [67/120    avg_loss:0.093, val_acc:0.930]
Epoch [68/120    avg_loss:0.099, val_acc:0.934]
Epoch [69/120    avg_loss:0.116, val_acc:0.927]
Epoch [70/120    avg_loss:0.144, val_acc:0.922]
Epoch [71/120    avg_loss:0.097, val_acc:0.936]
Epoch [72/120    avg_loss:0.074, val_acc:0.951]
Epoch [73/120    avg_loss:0.076, val_acc:0.940]
Epoch [74/120    avg_loss:0.086, val_acc:0.933]
Epoch [75/120    avg_loss:0.071, val_acc:0.935]
Epoch [76/120    avg_loss:0.062, val_acc:0.954]
Epoch [77/120    avg_loss:0.040, val_acc:0.959]
Epoch [78/120    avg_loss:0.037, val_acc:0.962]
Epoch [79/120    avg_loss:0.044, val_acc:0.961]
Epoch [80/120    avg_loss:0.039, val_acc:0.960]
Epoch [81/120    avg_loss:0.043, val_acc:0.959]
Epoch [82/120    avg_loss:0.046, val_acc:0.959]
Epoch [83/120    avg_loss:0.038, val_acc:0.958]
Epoch [84/120    avg_loss:0.039, val_acc:0.964]
Epoch [85/120    avg_loss:0.040, val_acc:0.965]
Epoch [86/120    avg_loss:0.033, val_acc:0.962]
Epoch [87/120    avg_loss:0.038, val_acc:0.965]
Epoch [88/120    avg_loss:0.035, val_acc:0.962]
Epoch [89/120    avg_loss:0.045, val_acc:0.960]
Epoch [90/120    avg_loss:0.037, val_acc:0.959]
Epoch [91/120    avg_loss:0.039, val_acc:0.958]
Epoch [92/120    avg_loss:0.028, val_acc:0.961]
Epoch [93/120    avg_loss:0.034, val_acc:0.961]
Epoch [94/120    avg_loss:0.038, val_acc:0.962]
Epoch [95/120    avg_loss:0.040, val_acc:0.964]
Epoch [96/120    avg_loss:0.036, val_acc:0.962]
Epoch [97/120    avg_loss:0.035, val_acc:0.960]
Epoch [98/120    avg_loss:0.038, val_acc:0.960]
Epoch [99/120    avg_loss:0.031, val_acc:0.961]
Epoch [100/120    avg_loss:0.030, val_acc:0.963]
Epoch [101/120    avg_loss:0.028, val_acc:0.963]
Epoch [102/120    avg_loss:0.029, val_acc:0.963]
Epoch [103/120    avg_loss:0.034, val_acc:0.963]
Epoch [104/120    avg_loss:0.034, val_acc:0.963]
Epoch [105/120    avg_loss:0.032, val_acc:0.963]
Epoch [106/120    avg_loss:0.034, val_acc:0.963]
Epoch [107/120    avg_loss:0.031, val_acc:0.963]
Epoch [108/120    avg_loss:0.028, val_acc:0.963]
Epoch [109/120    avg_loss:0.032, val_acc:0.963]
Epoch [110/120    avg_loss:0.032, val_acc:0.962]
Epoch [111/120    avg_loss:0.032, val_acc:0.962]
Epoch [112/120    avg_loss:0.028, val_acc:0.962]
Epoch [113/120    avg_loss:0.032, val_acc:0.962]
Epoch [114/120    avg_loss:0.031, val_acc:0.961]
Epoch [115/120    avg_loss:0.030, val_acc:0.961]
Epoch [116/120    avg_loss:0.035, val_acc:0.962]
Epoch [117/120    avg_loss:0.032, val_acc:0.961]
Epoch [118/120    avg_loss:0.033, val_acc:0.961]
Epoch [119/120    avg_loss:0.032, val_acc:0.961]
Epoch [120/120    avg_loss:0.031, val_acc:0.961]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1193    5    2    0    7    0    0    2   10   64    2    0
     0    0    0]
 [   0    0    3  685    4   18    2    0    0   15    0    0   13    7
     0    0    0]
 [   0    0    0    0  211    0    1    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  425    0    3    0    6    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   15    0    0    1    0
     0    0    0]
 [   0    0   13   87    0    7    3    0    0    0  752    8    0    0
     1    4    0]
 [   0    0   58    0    0    2   10    0    0    0   13 2111    0    4
    12    0    0]
 [   0    0    1    3    6    5    0    0    0    1   18    0  493    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    3    3    0    0
  1129    0    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    88  248    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.02710027100271

F1 scores:
[       nan 0.90243902 0.93458676 0.89601046 0.96788991 0.94972067
 0.97325409 0.94339623 0.99416569 0.52631579 0.89791045 0.96020014
 0.94444444 0.97112861 0.95234078 0.82804674 0.96      ]

Kappa:
0.931918290295516
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f812ad81860>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.665, val_acc:0.287]
Epoch [2/120    avg_loss:2.382, val_acc:0.419]
Epoch [3/120    avg_loss:2.172, val_acc:0.522]
Epoch [4/120    avg_loss:2.055, val_acc:0.576]
Epoch [5/120    avg_loss:1.910, val_acc:0.544]
Epoch [6/120    avg_loss:1.839, val_acc:0.593]
Epoch [7/120    avg_loss:1.745, val_acc:0.603]
Epoch [8/120    avg_loss:1.638, val_acc:0.619]
Epoch [9/120    avg_loss:1.509, val_acc:0.615]
Epoch [10/120    avg_loss:1.394, val_acc:0.650]
Epoch [11/120    avg_loss:1.271, val_acc:0.654]
Epoch [12/120    avg_loss:1.148, val_acc:0.686]
Epoch [13/120    avg_loss:1.063, val_acc:0.675]
Epoch [14/120    avg_loss:0.914, val_acc:0.728]
Epoch [15/120    avg_loss:0.928, val_acc:0.735]
Epoch [16/120    avg_loss:0.790, val_acc:0.774]
Epoch [17/120    avg_loss:0.796, val_acc:0.744]
Epoch [18/120    avg_loss:0.680, val_acc:0.763]
Epoch [19/120    avg_loss:0.634, val_acc:0.804]
Epoch [20/120    avg_loss:0.559, val_acc:0.848]
Epoch [21/120    avg_loss:0.498, val_acc:0.810]
Epoch [22/120    avg_loss:0.502, val_acc:0.854]
Epoch [23/120    avg_loss:0.436, val_acc:0.836]
Epoch [24/120    avg_loss:0.459, val_acc:0.858]
Epoch [25/120    avg_loss:0.397, val_acc:0.863]
Epoch [26/120    avg_loss:0.357, val_acc:0.868]
Epoch [27/120    avg_loss:0.401, val_acc:0.837]
Epoch [28/120    avg_loss:0.402, val_acc:0.866]
Epoch [29/120    avg_loss:0.332, val_acc:0.874]
Epoch [30/120    avg_loss:0.299, val_acc:0.889]
Epoch [31/120    avg_loss:0.261, val_acc:0.880]
Epoch [32/120    avg_loss:0.247, val_acc:0.917]
Epoch [33/120    avg_loss:0.216, val_acc:0.896]
Epoch [34/120    avg_loss:0.258, val_acc:0.902]
Epoch [35/120    avg_loss:0.262, val_acc:0.875]
Epoch [36/120    avg_loss:0.255, val_acc:0.882]
Epoch [37/120    avg_loss:0.281, val_acc:0.880]
Epoch [38/120    avg_loss:0.263, val_acc:0.877]
Epoch [39/120    avg_loss:0.295, val_acc:0.868]
Epoch [40/120    avg_loss:0.257, val_acc:0.914]
Epoch [41/120    avg_loss:0.253, val_acc:0.878]
Epoch [42/120    avg_loss:0.221, val_acc:0.859]
Epoch [43/120    avg_loss:0.192, val_acc:0.902]
Epoch [44/120    avg_loss:0.166, val_acc:0.921]
Epoch [45/120    avg_loss:0.177, val_acc:0.894]
Epoch [46/120    avg_loss:0.180, val_acc:0.897]
Epoch [47/120    avg_loss:0.154, val_acc:0.934]
Epoch [48/120    avg_loss:0.171, val_acc:0.918]
Epoch [49/120    avg_loss:0.164, val_acc:0.926]
Epoch [50/120    avg_loss:0.135, val_acc:0.939]
Epoch [51/120    avg_loss:0.123, val_acc:0.951]
Epoch [52/120    avg_loss:0.099, val_acc:0.946]
Epoch [53/120    avg_loss:0.102, val_acc:0.934]
Epoch [54/120    avg_loss:0.098, val_acc:0.934]
Epoch [55/120    avg_loss:0.105, val_acc:0.942]
Epoch [56/120    avg_loss:0.101, val_acc:0.941]
Epoch [57/120    avg_loss:0.086, val_acc:0.940]
Epoch [58/120    avg_loss:0.092, val_acc:0.942]
Epoch [59/120    avg_loss:0.097, val_acc:0.931]
Epoch [60/120    avg_loss:0.084, val_acc:0.950]
Epoch [61/120    avg_loss:0.097, val_acc:0.936]
Epoch [62/120    avg_loss:0.089, val_acc:0.936]
Epoch [63/120    avg_loss:0.081, val_acc:0.939]
Epoch [64/120    avg_loss:0.081, val_acc:0.958]
Epoch [65/120    avg_loss:0.091, val_acc:0.944]
Epoch [66/120    avg_loss:0.099, val_acc:0.940]
Epoch [67/120    avg_loss:0.079, val_acc:0.957]
Epoch [68/120    avg_loss:0.085, val_acc:0.955]
Epoch [69/120    avg_loss:0.078, val_acc:0.946]
Epoch [70/120    avg_loss:0.097, val_acc:0.940]
Epoch [71/120    avg_loss:0.078, val_acc:0.943]
Epoch [72/120    avg_loss:0.060, val_acc:0.953]
Epoch [73/120    avg_loss:0.065, val_acc:0.950]
Epoch [74/120    avg_loss:0.053, val_acc:0.959]
Epoch [75/120    avg_loss:0.082, val_acc:0.943]
Epoch [76/120    avg_loss:0.101, val_acc:0.942]
Epoch [77/120    avg_loss:0.072, val_acc:0.951]
Epoch [78/120    avg_loss:0.064, val_acc:0.948]
Epoch [79/120    avg_loss:0.061, val_acc:0.947]
Epoch [80/120    avg_loss:0.060, val_acc:0.949]
Epoch [81/120    avg_loss:0.079, val_acc:0.942]
Epoch [82/120    avg_loss:0.064, val_acc:0.943]
Epoch [83/120    avg_loss:0.072, val_acc:0.956]
Epoch [84/120    avg_loss:0.062, val_acc:0.957]
Epoch [85/120    avg_loss:0.044, val_acc:0.963]
Epoch [86/120    avg_loss:0.043, val_acc:0.955]
Epoch [87/120    avg_loss:0.044, val_acc:0.956]
Epoch [88/120    avg_loss:0.056, val_acc:0.955]
Epoch [89/120    avg_loss:0.042, val_acc:0.958]
Epoch [90/120    avg_loss:0.040, val_acc:0.956]
Epoch [91/120    avg_loss:0.045, val_acc:0.963]
Epoch [92/120    avg_loss:0.045, val_acc:0.961]
Epoch [93/120    avg_loss:0.058, val_acc:0.944]
Epoch [94/120    avg_loss:0.054, val_acc:0.946]
Epoch [95/120    avg_loss:0.052, val_acc:0.956]
Epoch [96/120    avg_loss:0.040, val_acc:0.963]
Epoch [97/120    avg_loss:0.046, val_acc:0.958]
Epoch [98/120    avg_loss:0.041, val_acc:0.964]
Epoch [99/120    avg_loss:0.040, val_acc:0.968]
Epoch [100/120    avg_loss:0.078, val_acc:0.953]
Epoch [101/120    avg_loss:0.084, val_acc:0.939]
Epoch [102/120    avg_loss:0.117, val_acc:0.942]
Epoch [103/120    avg_loss:0.089, val_acc:0.944]
Epoch [104/120    avg_loss:0.073, val_acc:0.946]
Epoch [105/120    avg_loss:0.223, val_acc:0.906]
Epoch [106/120    avg_loss:0.166, val_acc:0.939]
Epoch [107/120    avg_loss:0.086, val_acc:0.953]
Epoch [108/120    avg_loss:0.068, val_acc:0.944]
Epoch [109/120    avg_loss:0.167, val_acc:0.931]
Epoch [110/120    avg_loss:0.110, val_acc:0.943]
Epoch [111/120    avg_loss:0.063, val_acc:0.953]
Epoch [112/120    avg_loss:0.065, val_acc:0.947]
Epoch [113/120    avg_loss:0.052, val_acc:0.954]
Epoch [114/120    avg_loss:0.040, val_acc:0.959]
Epoch [115/120    avg_loss:0.035, val_acc:0.962]
Epoch [116/120    avg_loss:0.034, val_acc:0.963]
Epoch [117/120    avg_loss:0.031, val_acc:0.962]
Epoch [118/120    avg_loss:0.031, val_acc:0.963]
Epoch [119/120    avg_loss:0.033, val_acc:0.964]
Epoch [120/120    avg_loss:0.031, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    2    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1229    0    0    0    4    0    0    0    2   48    0    0
     0    2    0]
 [   0    0    2  694    1   25    0    0    0    7    1    0   13    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    3    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   21   90    0    3    0    0    0    0  743    8    0    0
     2    8    0]
 [   0    0    9    0    0    0   20    0    4    0    9 2164    0    2
     2    0    0]
 [   0    0    3   29    3    0    0    0    0    0   17    4  470    0
     0    0    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   12    0    0    3    0    3    1    0    0
  1120    0    0]
 [   0    0    0    0    0    0   11    0    0    5    0    0    0    0
   133  198    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.18970189701896

F1 scores:
[       nan 0.96202532 0.96354371 0.88974359 0.99069767 0.94795127
 0.97100372 0.94339623 0.99192618 0.66666667 0.90006057 0.97543385
 0.92156863 0.98404255 0.93411176 0.71351351 0.94252874]

Kappa:
0.9336870835820493
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f41ad0997f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.729, val_acc:0.309]
Epoch [2/120    avg_loss:2.405, val_acc:0.377]
Epoch [3/120    avg_loss:2.211, val_acc:0.488]
Epoch [4/120    avg_loss:2.071, val_acc:0.546]
Epoch [5/120    avg_loss:1.912, val_acc:0.584]
Epoch [6/120    avg_loss:1.786, val_acc:0.618]
Epoch [7/120    avg_loss:1.710, val_acc:0.617]
Epoch [8/120    avg_loss:1.564, val_acc:0.645]
Epoch [9/120    avg_loss:1.482, val_acc:0.648]
Epoch [10/120    avg_loss:1.347, val_acc:0.666]
Epoch [11/120    avg_loss:1.219, val_acc:0.694]
Epoch [12/120    avg_loss:1.104, val_acc:0.690]
Epoch [13/120    avg_loss:1.043, val_acc:0.735]
Epoch [14/120    avg_loss:0.916, val_acc:0.743]
Epoch [15/120    avg_loss:0.809, val_acc:0.766]
Epoch [16/120    avg_loss:0.770, val_acc:0.780]
Epoch [17/120    avg_loss:0.770, val_acc:0.794]
Epoch [18/120    avg_loss:0.700, val_acc:0.819]
Epoch [19/120    avg_loss:0.634, val_acc:0.763]
Epoch [20/120    avg_loss:0.585, val_acc:0.811]
Epoch [21/120    avg_loss:0.504, val_acc:0.850]
Epoch [22/120    avg_loss:0.528, val_acc:0.821]
Epoch [23/120    avg_loss:0.487, val_acc:0.831]
Epoch [24/120    avg_loss:0.411, val_acc:0.861]
Epoch [25/120    avg_loss:0.431, val_acc:0.853]
Epoch [26/120    avg_loss:0.426, val_acc:0.826]
Epoch [27/120    avg_loss:0.402, val_acc:0.843]
Epoch [28/120    avg_loss:0.340, val_acc:0.870]
Epoch [29/120    avg_loss:0.326, val_acc:0.867]
Epoch [30/120    avg_loss:0.304, val_acc:0.866]
Epoch [31/120    avg_loss:0.299, val_acc:0.887]
Epoch [32/120    avg_loss:0.244, val_acc:0.881]
Epoch [33/120    avg_loss:0.239, val_acc:0.900]
Epoch [34/120    avg_loss:0.242, val_acc:0.892]
Epoch [35/120    avg_loss:0.246, val_acc:0.900]
Epoch [36/120    avg_loss:0.258, val_acc:0.890]
Epoch [37/120    avg_loss:0.209, val_acc:0.896]
Epoch [38/120    avg_loss:0.225, val_acc:0.912]
Epoch [39/120    avg_loss:0.186, val_acc:0.919]
Epoch [40/120    avg_loss:0.183, val_acc:0.902]
Epoch [41/120    avg_loss:0.159, val_acc:0.929]
Epoch [42/120    avg_loss:0.171, val_acc:0.896]
Epoch [43/120    avg_loss:0.141, val_acc:0.921]
Epoch [44/120    avg_loss:0.151, val_acc:0.928]
Epoch [45/120    avg_loss:0.147, val_acc:0.935]
Epoch [46/120    avg_loss:0.197, val_acc:0.888]
Epoch [47/120    avg_loss:0.179, val_acc:0.932]
Epoch [48/120    avg_loss:0.164, val_acc:0.907]
Epoch [49/120    avg_loss:0.145, val_acc:0.933]
Epoch [50/120    avg_loss:0.128, val_acc:0.928]
Epoch [51/120    avg_loss:0.204, val_acc:0.890]
Epoch [52/120    avg_loss:0.234, val_acc:0.913]
Epoch [53/120    avg_loss:0.161, val_acc:0.940]
Epoch [54/120    avg_loss:0.133, val_acc:0.932]
Epoch [55/120    avg_loss:0.122, val_acc:0.929]
Epoch [56/120    avg_loss:0.128, val_acc:0.943]
Epoch [57/120    avg_loss:0.113, val_acc:0.936]
Epoch [58/120    avg_loss:0.097, val_acc:0.946]
Epoch [59/120    avg_loss:0.101, val_acc:0.938]
Epoch [60/120    avg_loss:0.122, val_acc:0.933]
Epoch [61/120    avg_loss:0.102, val_acc:0.932]
Epoch [62/120    avg_loss:0.136, val_acc:0.926]
Epoch [63/120    avg_loss:0.122, val_acc:0.942]
Epoch [64/120    avg_loss:0.075, val_acc:0.944]
Epoch [65/120    avg_loss:0.071, val_acc:0.949]
Epoch [66/120    avg_loss:0.081, val_acc:0.949]
Epoch [67/120    avg_loss:0.090, val_acc:0.948]
Epoch [68/120    avg_loss:0.075, val_acc:0.953]
Epoch [69/120    avg_loss:0.073, val_acc:0.935]
Epoch [70/120    avg_loss:0.070, val_acc:0.964]
Epoch [71/120    avg_loss:0.072, val_acc:0.959]
Epoch [72/120    avg_loss:0.066, val_acc:0.955]
Epoch [73/120    avg_loss:0.060, val_acc:0.958]
Epoch [74/120    avg_loss:0.057, val_acc:0.966]
Epoch [75/120    avg_loss:0.076, val_acc:0.956]
Epoch [76/120    avg_loss:0.069, val_acc:0.954]
Epoch [77/120    avg_loss:0.066, val_acc:0.966]
Epoch [78/120    avg_loss:0.057, val_acc:0.964]
Epoch [79/120    avg_loss:0.077, val_acc:0.950]
Epoch [80/120    avg_loss:0.076, val_acc:0.949]
Epoch [81/120    avg_loss:0.095, val_acc:0.942]
Epoch [82/120    avg_loss:0.056, val_acc:0.958]
Epoch [83/120    avg_loss:0.051, val_acc:0.954]
Epoch [84/120    avg_loss:0.045, val_acc:0.964]
Epoch [85/120    avg_loss:0.059, val_acc:0.953]
Epoch [86/120    avg_loss:0.037, val_acc:0.968]
Epoch [87/120    avg_loss:0.032, val_acc:0.971]
Epoch [88/120    avg_loss:0.036, val_acc:0.963]
Epoch [89/120    avg_loss:0.033, val_acc:0.972]
Epoch [90/120    avg_loss:0.034, val_acc:0.973]
Epoch [91/120    avg_loss:0.036, val_acc:0.959]
Epoch [92/120    avg_loss:0.042, val_acc:0.963]
Epoch [93/120    avg_loss:0.048, val_acc:0.961]
Epoch [94/120    avg_loss:0.085, val_acc:0.950]
Epoch [95/120    avg_loss:0.055, val_acc:0.959]
Epoch [96/120    avg_loss:0.038, val_acc:0.969]
Epoch [97/120    avg_loss:0.040, val_acc:0.968]
Epoch [98/120    avg_loss:0.052, val_acc:0.970]
Epoch [99/120    avg_loss:0.041, val_acc:0.969]
Epoch [100/120    avg_loss:0.039, val_acc:0.970]
Epoch [101/120    avg_loss:0.038, val_acc:0.973]
Epoch [102/120    avg_loss:0.033, val_acc:0.973]
Epoch [103/120    avg_loss:0.025, val_acc:0.975]
Epoch [104/120    avg_loss:0.040, val_acc:0.969]
Epoch [105/120    avg_loss:0.028, val_acc:0.977]
Epoch [106/120    avg_loss:0.024, val_acc:0.972]
Epoch [107/120    avg_loss:0.023, val_acc:0.964]
Epoch [108/120    avg_loss:0.017, val_acc:0.970]
Epoch [109/120    avg_loss:0.026, val_acc:0.964]
Epoch [110/120    avg_loss:0.033, val_acc:0.972]
Epoch [111/120    avg_loss:0.025, val_acc:0.980]
Epoch [112/120    avg_loss:0.028, val_acc:0.970]
Epoch [113/120    avg_loss:0.023, val_acc:0.966]
Epoch [114/120    avg_loss:0.017, val_acc:0.977]
Epoch [115/120    avg_loss:0.019, val_acc:0.969]
Epoch [116/120    avg_loss:0.029, val_acc:0.956]
Epoch [117/120    avg_loss:0.035, val_acc:0.973]
Epoch [118/120    avg_loss:0.031, val_acc:0.973]
Epoch [119/120    avg_loss:0.027, val_acc:0.978]
Epoch [120/120    avg_loss:0.021, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1258    3    0    0    2    0    0    0    2   10    0    0
     0   10    0]
 [   0    0    3  702    1   11    0    0    0    8    0    0   22    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    4    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   23   84    0    3    0    0    0    1  745    5    0    0
     3   11    0]
 [   0    0   26    0    0    2   11    0    0    0   10 2156    2    3
     0    0    0]
 [   0    0    0   27    3    6    0    0    0    0   14    0  482    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    1    0    1    0    0    0
  1129    0    0]
 [   0    0    0    0    0    1   43    0    0    3    0    0    0    0
    58  242    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.2520325203252

F1 scores:
[       nan 0.97560976 0.96955684 0.89655172 0.99069767 0.95749441
 0.95836377 0.92592593 0.99767442 0.65217391 0.90412621 0.98402556
 0.92514395 0.9919571  0.96868297 0.79344262 0.97619048]

Kappa:
0.9458846092804637
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1930560780>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.783, val_acc:0.371]
Epoch [2/120    avg_loss:2.438, val_acc:0.441]
Epoch [3/120    avg_loss:2.202, val_acc:0.364]
Epoch [4/120    avg_loss:1.995, val_acc:0.508]
Epoch [5/120    avg_loss:1.874, val_acc:0.533]
Epoch [6/120    avg_loss:1.698, val_acc:0.566]
Epoch [7/120    avg_loss:1.596, val_acc:0.568]
Epoch [8/120    avg_loss:1.431, val_acc:0.569]
Epoch [9/120    avg_loss:1.340, val_acc:0.581]
Epoch [10/120    avg_loss:1.185, val_acc:0.600]
Epoch [11/120    avg_loss:1.227, val_acc:0.629]
Epoch [12/120    avg_loss:1.095, val_acc:0.668]
Epoch [13/120    avg_loss:0.912, val_acc:0.708]
Epoch [14/120    avg_loss:0.847, val_acc:0.738]
Epoch [15/120    avg_loss:0.751, val_acc:0.766]
Epoch [16/120    avg_loss:0.680, val_acc:0.777]
Epoch [17/120    avg_loss:0.608, val_acc:0.764]
Epoch [18/120    avg_loss:0.595, val_acc:0.797]
Epoch [19/120    avg_loss:0.555, val_acc:0.789]
Epoch [20/120    avg_loss:0.546, val_acc:0.807]
Epoch [21/120    avg_loss:0.484, val_acc:0.784]
Epoch [22/120    avg_loss:0.420, val_acc:0.816]
Epoch [23/120    avg_loss:0.355, val_acc:0.875]
Epoch [24/120    avg_loss:0.334, val_acc:0.857]
Epoch [25/120    avg_loss:0.330, val_acc:0.871]
Epoch [26/120    avg_loss:0.287, val_acc:0.894]
Epoch [27/120    avg_loss:0.252, val_acc:0.898]
Epoch [28/120    avg_loss:0.361, val_acc:0.828]
Epoch [29/120    avg_loss:0.366, val_acc:0.893]
Epoch [30/120    avg_loss:0.361, val_acc:0.842]
Epoch [31/120    avg_loss:0.296, val_acc:0.896]
Epoch [32/120    avg_loss:0.264, val_acc:0.903]
Epoch [33/120    avg_loss:0.233, val_acc:0.915]
Epoch [34/120    avg_loss:0.172, val_acc:0.907]
Epoch [35/120    avg_loss:0.138, val_acc:0.947]
Epoch [36/120    avg_loss:0.126, val_acc:0.934]
Epoch [37/120    avg_loss:0.116, val_acc:0.941]
Epoch [38/120    avg_loss:0.118, val_acc:0.935]
Epoch [39/120    avg_loss:0.097, val_acc:0.940]
Epoch [40/120    avg_loss:0.117, val_acc:0.927]
Epoch [41/120    avg_loss:0.111, val_acc:0.919]
Epoch [42/120    avg_loss:0.090, val_acc:0.938]
Epoch [43/120    avg_loss:0.097, val_acc:0.940]
Epoch [44/120    avg_loss:0.085, val_acc:0.946]
Epoch [45/120    avg_loss:0.088, val_acc:0.944]
Epoch [46/120    avg_loss:0.106, val_acc:0.945]
Epoch [47/120    avg_loss:0.099, val_acc:0.942]
Epoch [48/120    avg_loss:0.073, val_acc:0.952]
Epoch [49/120    avg_loss:0.069, val_acc:0.940]
Epoch [50/120    avg_loss:0.071, val_acc:0.952]
Epoch [51/120    avg_loss:0.076, val_acc:0.934]
Epoch [52/120    avg_loss:0.078, val_acc:0.950]
Epoch [53/120    avg_loss:0.066, val_acc:0.948]
Epoch [54/120    avg_loss:0.059, val_acc:0.956]
Epoch [55/120    avg_loss:0.055, val_acc:0.942]
Epoch [56/120    avg_loss:0.069, val_acc:0.946]
Epoch [57/120    avg_loss:0.048, val_acc:0.947]
Epoch [58/120    avg_loss:0.079, val_acc:0.935]
Epoch [59/120    avg_loss:0.066, val_acc:0.947]
Epoch [60/120    avg_loss:0.048, val_acc:0.965]
Epoch [61/120    avg_loss:0.067, val_acc:0.953]
Epoch [62/120    avg_loss:0.048, val_acc:0.957]
Epoch [63/120    avg_loss:0.047, val_acc:0.916]
Epoch [64/120    avg_loss:0.046, val_acc:0.945]
Epoch [65/120    avg_loss:0.037, val_acc:0.968]
Epoch [66/120    avg_loss:0.044, val_acc:0.957]
Epoch [67/120    avg_loss:0.051, val_acc:0.963]
Epoch [68/120    avg_loss:0.039, val_acc:0.968]
Epoch [69/120    avg_loss:0.035, val_acc:0.960]
Epoch [70/120    avg_loss:0.038, val_acc:0.965]
Epoch [71/120    avg_loss:0.038, val_acc:0.966]
Epoch [72/120    avg_loss:0.026, val_acc:0.964]
Epoch [73/120    avg_loss:0.026, val_acc:0.975]
Epoch [74/120    avg_loss:0.025, val_acc:0.972]
Epoch [75/120    avg_loss:0.019, val_acc:0.972]
Epoch [76/120    avg_loss:0.020, val_acc:0.971]
Epoch [77/120    avg_loss:0.023, val_acc:0.969]
Epoch [78/120    avg_loss:0.021, val_acc:0.966]
Epoch [79/120    avg_loss:0.030, val_acc:0.956]
Epoch [80/120    avg_loss:0.034, val_acc:0.958]
Epoch [81/120    avg_loss:0.024, val_acc:0.952]
Epoch [82/120    avg_loss:0.027, val_acc:0.965]
Epoch [83/120    avg_loss:0.016, val_acc:0.973]
Epoch [84/120    avg_loss:0.020, val_acc:0.967]
Epoch [85/120    avg_loss:0.023, val_acc:0.972]
Epoch [86/120    avg_loss:0.018, val_acc:0.975]
Epoch [87/120    avg_loss:0.018, val_acc:0.963]
Epoch [88/120    avg_loss:0.018, val_acc:0.966]
Epoch [89/120    avg_loss:0.023, val_acc:0.967]
Epoch [90/120    avg_loss:0.020, val_acc:0.961]
Epoch [91/120    avg_loss:0.017, val_acc:0.968]
Epoch [92/120    avg_loss:0.021, val_acc:0.967]
Epoch [93/120    avg_loss:0.018, val_acc:0.966]
Epoch [94/120    avg_loss:0.022, val_acc:0.957]
Epoch [95/120    avg_loss:0.024, val_acc:0.970]
Epoch [96/120    avg_loss:0.017, val_acc:0.965]
Epoch [97/120    avg_loss:0.014, val_acc:0.974]
Epoch [98/120    avg_loss:0.012, val_acc:0.974]
Epoch [99/120    avg_loss:0.014, val_acc:0.968]
Epoch [100/120    avg_loss:0.009, val_acc:0.969]
Epoch [101/120    avg_loss:0.012, val_acc:0.970]
Epoch [102/120    avg_loss:0.010, val_acc:0.973]
Epoch [103/120    avg_loss:0.009, val_acc:0.973]
Epoch [104/120    avg_loss:0.010, val_acc:0.975]
Epoch [105/120    avg_loss:0.010, val_acc:0.975]
Epoch [106/120    avg_loss:0.008, val_acc:0.975]
Epoch [107/120    avg_loss:0.010, val_acc:0.975]
Epoch [108/120    avg_loss:0.009, val_acc:0.973]
Epoch [109/120    avg_loss:0.009, val_acc:0.976]
Epoch [110/120    avg_loss:0.008, val_acc:0.975]
Epoch [111/120    avg_loss:0.008, val_acc:0.974]
Epoch [112/120    avg_loss:0.009, val_acc:0.976]
Epoch [113/120    avg_loss:0.008, val_acc:0.974]
Epoch [114/120    avg_loss:0.009, val_acc:0.974]
Epoch [115/120    avg_loss:0.008, val_acc:0.973]
Epoch [116/120    avg_loss:0.010, val_acc:0.972]
Epoch [117/120    avg_loss:0.007, val_acc:0.974]
Epoch [118/120    avg_loss:0.009, val_acc:0.974]
Epoch [119/120    avg_loss:0.011, val_acc:0.972]
Epoch [120/120    avg_loss:0.009, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    1    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1253    6    5    0    0    0    0    0    4   17    0    0
     0    0    0]
 [   0    0    2  726    2    0    0    0    0    2    2   10    3    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  846   22    1    0
     0    2    0]
 [   0    0    7    0    0    1    3    0    0    0   14 2158   24    1
     1    1    0]
 [   0    0    0    7    0    0    0    0    0    0    0    5  517    0
     0    4    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    1
  1127   11    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    58  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.45257452574526

F1 scores:
[       nan 0.98765432 0.98235986 0.97580645 0.97911833 0.99308756
 0.99241275 1.         1.         0.94736842 0.97185526 0.97580827
 0.95829472 0.99462366 0.96696697 0.87692308 0.99408284]

Kappa:
0.9709476554176641
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efd23c51828>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.699, val_acc:0.244]
Epoch [2/120    avg_loss:2.408, val_acc:0.351]
Epoch [3/120    avg_loss:2.205, val_acc:0.439]
Epoch [4/120    avg_loss:2.073, val_acc:0.485]
Epoch [5/120    avg_loss:1.949, val_acc:0.544]
Epoch [6/120    avg_loss:1.762, val_acc:0.573]
Epoch [7/120    avg_loss:1.651, val_acc:0.562]
Epoch [8/120    avg_loss:1.578, val_acc:0.600]
Epoch [9/120    avg_loss:1.411, val_acc:0.641]
Epoch [10/120    avg_loss:1.326, val_acc:0.637]
Epoch [11/120    avg_loss:1.156, val_acc:0.667]
Epoch [12/120    avg_loss:1.061, val_acc:0.685]
Epoch [13/120    avg_loss:0.901, val_acc:0.716]
Epoch [14/120    avg_loss:0.883, val_acc:0.705]
Epoch [15/120    avg_loss:0.801, val_acc:0.777]
Epoch [16/120    avg_loss:0.668, val_acc:0.778]
Epoch [17/120    avg_loss:0.618, val_acc:0.778]
Epoch [18/120    avg_loss:0.556, val_acc:0.827]
Epoch [19/120    avg_loss:0.472, val_acc:0.828]
Epoch [20/120    avg_loss:0.400, val_acc:0.844]
Epoch [21/120    avg_loss:0.396, val_acc:0.854]
Epoch [22/120    avg_loss:0.415, val_acc:0.868]
Epoch [23/120    avg_loss:0.342, val_acc:0.843]
Epoch [24/120    avg_loss:0.295, val_acc:0.883]
Epoch [25/120    avg_loss:0.269, val_acc:0.882]
Epoch [26/120    avg_loss:0.253, val_acc:0.887]
Epoch [27/120    avg_loss:0.238, val_acc:0.875]
Epoch [28/120    avg_loss:0.322, val_acc:0.857]
Epoch [29/120    avg_loss:0.285, val_acc:0.864]
Epoch [30/120    avg_loss:0.199, val_acc:0.912]
Epoch [31/120    avg_loss:0.184, val_acc:0.917]
Epoch [32/120    avg_loss:0.200, val_acc:0.880]
Epoch [33/120    avg_loss:0.189, val_acc:0.926]
Epoch [34/120    avg_loss:0.147, val_acc:0.905]
Epoch [35/120    avg_loss:0.154, val_acc:0.921]
Epoch [36/120    avg_loss:0.154, val_acc:0.931]
Epoch [37/120    avg_loss:0.122, val_acc:0.916]
Epoch [38/120    avg_loss:0.124, val_acc:0.922]
Epoch [39/120    avg_loss:0.111, val_acc:0.936]
Epoch [40/120    avg_loss:0.093, val_acc:0.943]
Epoch [41/120    avg_loss:0.092, val_acc:0.941]
Epoch [42/120    avg_loss:0.085, val_acc:0.959]
Epoch [43/120    avg_loss:0.090, val_acc:0.926]
Epoch [44/120    avg_loss:0.163, val_acc:0.922]
Epoch [45/120    avg_loss:0.114, val_acc:0.939]
Epoch [46/120    avg_loss:0.090, val_acc:0.958]
Epoch [47/120    avg_loss:0.092, val_acc:0.935]
Epoch [48/120    avg_loss:0.076, val_acc:0.955]
Epoch [49/120    avg_loss:0.074, val_acc:0.958]
Epoch [50/120    avg_loss:0.066, val_acc:0.940]
Epoch [51/120    avg_loss:0.078, val_acc:0.944]
Epoch [52/120    avg_loss:0.062, val_acc:0.953]
Epoch [53/120    avg_loss:0.055, val_acc:0.942]
Epoch [54/120    avg_loss:0.067, val_acc:0.950]
Epoch [55/120    avg_loss:0.051, val_acc:0.956]
Epoch [56/120    avg_loss:0.044, val_acc:0.964]
Epoch [57/120    avg_loss:0.036, val_acc:0.967]
Epoch [58/120    avg_loss:0.037, val_acc:0.968]
Epoch [59/120    avg_loss:0.030, val_acc:0.966]
Epoch [60/120    avg_loss:0.029, val_acc:0.967]
Epoch [61/120    avg_loss:0.031, val_acc:0.967]
Epoch [62/120    avg_loss:0.032, val_acc:0.964]
Epoch [63/120    avg_loss:0.037, val_acc:0.967]
Epoch [64/120    avg_loss:0.033, val_acc:0.968]
Epoch [65/120    avg_loss:0.031, val_acc:0.968]
Epoch [66/120    avg_loss:0.030, val_acc:0.970]
Epoch [67/120    avg_loss:0.029, val_acc:0.969]
Epoch [68/120    avg_loss:0.027, val_acc:0.969]
Epoch [69/120    avg_loss:0.031, val_acc:0.967]
Epoch [70/120    avg_loss:0.030, val_acc:0.967]
Epoch [71/120    avg_loss:0.026, val_acc:0.967]
Epoch [72/120    avg_loss:0.025, val_acc:0.967]
Epoch [73/120    avg_loss:0.027, val_acc:0.964]
Epoch [74/120    avg_loss:0.028, val_acc:0.966]
Epoch [75/120    avg_loss:0.028, val_acc:0.967]
Epoch [76/120    avg_loss:0.032, val_acc:0.966]
Epoch [77/120    avg_loss:0.030, val_acc:0.968]
Epoch [78/120    avg_loss:0.026, val_acc:0.968]
Epoch [79/120    avg_loss:0.026, val_acc:0.968]
Epoch [80/120    avg_loss:0.031, val_acc:0.968]
Epoch [81/120    avg_loss:0.029, val_acc:0.968]
Epoch [82/120    avg_loss:0.024, val_acc:0.968]
Epoch [83/120    avg_loss:0.029, val_acc:0.968]
Epoch [84/120    avg_loss:0.027, val_acc:0.969]
Epoch [85/120    avg_loss:0.024, val_acc:0.969]
Epoch [86/120    avg_loss:0.026, val_acc:0.969]
Epoch [87/120    avg_loss:0.028, val_acc:0.969]
Epoch [88/120    avg_loss:0.027, val_acc:0.969]
Epoch [89/120    avg_loss:0.028, val_acc:0.969]
Epoch [90/120    avg_loss:0.027, val_acc:0.969]
Epoch [91/120    avg_loss:0.027, val_acc:0.969]
Epoch [92/120    avg_loss:0.027, val_acc:0.968]
Epoch [93/120    avg_loss:0.028, val_acc:0.968]
Epoch [94/120    avg_loss:0.027, val_acc:0.968]
Epoch [95/120    avg_loss:0.028, val_acc:0.968]
Epoch [96/120    avg_loss:0.031, val_acc:0.968]
Epoch [97/120    avg_loss:0.026, val_acc:0.968]
Epoch [98/120    avg_loss:0.029, val_acc:0.968]
Epoch [99/120    avg_loss:0.027, val_acc:0.968]
Epoch [100/120    avg_loss:0.027, val_acc:0.968]
Epoch [101/120    avg_loss:0.027, val_acc:0.968]
Epoch [102/120    avg_loss:0.026, val_acc:0.968]
Epoch [103/120    avg_loss:0.031, val_acc:0.968]
Epoch [104/120    avg_loss:0.033, val_acc:0.968]
Epoch [105/120    avg_loss:0.026, val_acc:0.968]
Epoch [106/120    avg_loss:0.027, val_acc:0.968]
Epoch [107/120    avg_loss:0.022, val_acc:0.968]
Epoch [108/120    avg_loss:0.029, val_acc:0.968]
Epoch [109/120    avg_loss:0.027, val_acc:0.968]
Epoch [110/120    avg_loss:0.026, val_acc:0.968]
Epoch [111/120    avg_loss:0.028, val_acc:0.968]
Epoch [112/120    avg_loss:0.025, val_acc:0.968]
Epoch [113/120    avg_loss:0.028, val_acc:0.968]
Epoch [114/120    avg_loss:0.031, val_acc:0.968]
Epoch [115/120    avg_loss:0.028, val_acc:0.968]
Epoch [116/120    avg_loss:0.024, val_acc:0.969]
Epoch [117/120    avg_loss:0.028, val_acc:0.969]
Epoch [118/120    avg_loss:0.029, val_acc:0.968]
Epoch [119/120    avg_loss:0.026, val_acc:0.968]
Epoch [120/120    avg_loss:0.030, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1232    9    3    0    1    0    0    0    6   34    0    0
     0    0    0]
 [   0    0    0  709   14    0    0    0    0    5    5   10    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    1    0    0    0  654    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   16    0    0    0    0    0    0    0  850    9    0    0
     0    0    0]
 [   0    0   25    0    0    0    2    0    0    8   38 2118   19    0
     0    0    0]
 [   0    0    1    3    0    0    0    0    0    0    2    7  516    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    4    0    0    0    0
  1119   16    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    49  295    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.6829268292683

F1 scores:
[       nan 0.975      0.96212417 0.96594005 0.96162528 0.99769585
 0.99316629 1.         1.         0.66666667 0.95613048 0.96514012
 0.96178938 0.99728997 0.96925076 0.89258699 0.98823529]

Kappa:
0.9622043913038542
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f45c27b5828>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.694, val_acc:0.442]
Epoch [2/120    avg_loss:2.400, val_acc:0.497]
Epoch [3/120    avg_loss:2.194, val_acc:0.530]
Epoch [4/120    avg_loss:2.004, val_acc:0.558]
Epoch [5/120    avg_loss:1.871, val_acc:0.573]
Epoch [6/120    avg_loss:1.792, val_acc:0.601]
Epoch [7/120    avg_loss:1.660, val_acc:0.644]
Epoch [8/120    avg_loss:1.548, val_acc:0.639]
Epoch [9/120    avg_loss:1.462, val_acc:0.660]
Epoch [10/120    avg_loss:1.338, val_acc:0.684]
Epoch [11/120    avg_loss:1.215, val_acc:0.708]
Epoch [12/120    avg_loss:1.051, val_acc:0.726]
Epoch [13/120    avg_loss:1.035, val_acc:0.725]
Epoch [14/120    avg_loss:0.892, val_acc:0.762]
Epoch [15/120    avg_loss:0.814, val_acc:0.785]
Epoch [16/120    avg_loss:0.749, val_acc:0.791]
Epoch [17/120    avg_loss:0.657, val_acc:0.808]
Epoch [18/120    avg_loss:0.567, val_acc:0.851]
Epoch [19/120    avg_loss:0.529, val_acc:0.798]
Epoch [20/120    avg_loss:0.522, val_acc:0.790]
Epoch [21/120    avg_loss:0.449, val_acc:0.860]
Epoch [22/120    avg_loss:0.405, val_acc:0.841]
Epoch [23/120    avg_loss:0.356, val_acc:0.871]
Epoch [24/120    avg_loss:0.362, val_acc:0.861]
Epoch [25/120    avg_loss:0.363, val_acc:0.847]
Epoch [26/120    avg_loss:0.320, val_acc:0.874]
Epoch [27/120    avg_loss:0.281, val_acc:0.856]
Epoch [28/120    avg_loss:0.272, val_acc:0.904]
Epoch [29/120    avg_loss:0.243, val_acc:0.896]
Epoch [30/120    avg_loss:0.226, val_acc:0.929]
Epoch [31/120    avg_loss:0.194, val_acc:0.922]
Epoch [32/120    avg_loss:0.181, val_acc:0.896]
Epoch [33/120    avg_loss:0.164, val_acc:0.940]
Epoch [34/120    avg_loss:0.164, val_acc:0.928]
Epoch [35/120    avg_loss:0.167, val_acc:0.934]
Epoch [36/120    avg_loss:0.131, val_acc:0.947]
Epoch [37/120    avg_loss:0.117, val_acc:0.932]
Epoch [38/120    avg_loss:0.118, val_acc:0.948]
Epoch [39/120    avg_loss:0.106, val_acc:0.935]
Epoch [40/120    avg_loss:0.104, val_acc:0.948]
Epoch [41/120    avg_loss:0.112, val_acc:0.953]
Epoch [42/120    avg_loss:0.094, val_acc:0.953]
Epoch [43/120    avg_loss:0.110, val_acc:0.961]
Epoch [44/120    avg_loss:0.085, val_acc:0.948]
Epoch [45/120    avg_loss:0.081, val_acc:0.959]
Epoch [46/120    avg_loss:0.081, val_acc:0.959]
Epoch [47/120    avg_loss:0.082, val_acc:0.945]
Epoch [48/120    avg_loss:0.126, val_acc:0.942]
Epoch [49/120    avg_loss:0.100, val_acc:0.957]
Epoch [50/120    avg_loss:0.078, val_acc:0.955]
Epoch [51/120    avg_loss:0.069, val_acc:0.958]
Epoch [52/120    avg_loss:0.057, val_acc:0.959]
Epoch [53/120    avg_loss:0.054, val_acc:0.966]
Epoch [54/120    avg_loss:0.050, val_acc:0.960]
Epoch [55/120    avg_loss:0.047, val_acc:0.957]
Epoch [56/120    avg_loss:0.051, val_acc:0.977]
Epoch [57/120    avg_loss:0.044, val_acc:0.972]
Epoch [58/120    avg_loss:0.041, val_acc:0.968]
Epoch [59/120    avg_loss:0.036, val_acc:0.970]
Epoch [60/120    avg_loss:0.044, val_acc:0.967]
Epoch [61/120    avg_loss:0.039, val_acc:0.972]
Epoch [62/120    avg_loss:0.037, val_acc:0.967]
Epoch [63/120    avg_loss:0.038, val_acc:0.970]
Epoch [64/120    avg_loss:0.033, val_acc:0.973]
Epoch [65/120    avg_loss:0.038, val_acc:0.956]
Epoch [66/120    avg_loss:0.029, val_acc:0.974]
Epoch [67/120    avg_loss:0.024, val_acc:0.976]
Epoch [68/120    avg_loss:0.025, val_acc:0.980]
Epoch [69/120    avg_loss:0.035, val_acc:0.977]
Epoch [70/120    avg_loss:0.031, val_acc:0.966]
Epoch [71/120    avg_loss:0.041, val_acc:0.960]
Epoch [72/120    avg_loss:0.036, val_acc:0.971]
Epoch [73/120    avg_loss:0.038, val_acc:0.972]
Epoch [74/120    avg_loss:0.036, val_acc:0.969]
Epoch [75/120    avg_loss:0.040, val_acc:0.974]
Epoch [76/120    avg_loss:0.039, val_acc:0.961]
Epoch [77/120    avg_loss:0.036, val_acc:0.961]
Epoch [78/120    avg_loss:0.040, val_acc:0.958]
Epoch [79/120    avg_loss:0.041, val_acc:0.971]
Epoch [80/120    avg_loss:0.035, val_acc:0.974]
Epoch [81/120    avg_loss:0.024, val_acc:0.975]
Epoch [82/120    avg_loss:0.024, val_acc:0.978]
Epoch [83/120    avg_loss:0.017, val_acc:0.980]
Epoch [84/120    avg_loss:0.020, val_acc:0.980]
Epoch [85/120    avg_loss:0.015, val_acc:0.981]
Epoch [86/120    avg_loss:0.017, val_acc:0.980]
Epoch [87/120    avg_loss:0.014, val_acc:0.982]
Epoch [88/120    avg_loss:0.014, val_acc:0.981]
Epoch [89/120    avg_loss:0.012, val_acc:0.983]
Epoch [90/120    avg_loss:0.012, val_acc:0.982]
Epoch [91/120    avg_loss:0.013, val_acc:0.982]
Epoch [92/120    avg_loss:0.014, val_acc:0.983]
Epoch [93/120    avg_loss:0.014, val_acc:0.982]
Epoch [94/120    avg_loss:0.015, val_acc:0.983]
Epoch [95/120    avg_loss:0.013, val_acc:0.984]
Epoch [96/120    avg_loss:0.014, val_acc:0.981]
Epoch [97/120    avg_loss:0.011, val_acc:0.984]
Epoch [98/120    avg_loss:0.013, val_acc:0.983]
Epoch [99/120    avg_loss:0.016, val_acc:0.979]
Epoch [100/120    avg_loss:0.013, val_acc:0.983]
Epoch [101/120    avg_loss:0.012, val_acc:0.983]
Epoch [102/120    avg_loss:0.013, val_acc:0.984]
Epoch [103/120    avg_loss:0.014, val_acc:0.983]
Epoch [104/120    avg_loss:0.011, val_acc:0.983]
Epoch [105/120    avg_loss:0.011, val_acc:0.983]
Epoch [106/120    avg_loss:0.011, val_acc:0.984]
Epoch [107/120    avg_loss:0.013, val_acc:0.985]
Epoch [108/120    avg_loss:0.011, val_acc:0.984]
Epoch [109/120    avg_loss:0.012, val_acc:0.985]
Epoch [110/120    avg_loss:0.012, val_acc:0.985]
Epoch [111/120    avg_loss:0.016, val_acc:0.983]
Epoch [112/120    avg_loss:0.011, val_acc:0.983]
Epoch [113/120    avg_loss:0.012, val_acc:0.985]
Epoch [114/120    avg_loss:0.012, val_acc:0.984]
Epoch [115/120    avg_loss:0.012, val_acc:0.984]
Epoch [116/120    avg_loss:0.011, val_acc:0.985]
Epoch [117/120    avg_loss:0.010, val_acc:0.984]
Epoch [118/120    avg_loss:0.011, val_acc:0.984]
Epoch [119/120    avg_loss:0.011, val_acc:0.985]
Epoch [120/120    avg_loss:0.015, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1255    4    5    0    0    0    0    0    1   20    0    0
     0    0    0]
 [   0    0    0  718    7    0    0    0    0    4    1   15    2    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  424    0    3    0    0    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0  842   29    0    0
     0    1    0]
 [   0    0    2    0    0    0    0    0    0    1   17 2158   31    0
     0    1    0]
 [   0    0    0    7    0    0    0    0    0    0    0    6  516    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1128   10    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    80  267    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.07317073170732

F1 scores:
[       nan 1.         0.98624754 0.97289973 0.97025172 0.98719441
 0.99771167 0.94339623 1.         0.82926829 0.97004608 0.97185319
 0.95115207 1.         0.95755518 0.84896661 0.98224852]

Kappa:
0.9666047391223761
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0f9c71a748>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.691, val_acc:0.283]
Epoch [2/120    avg_loss:2.400, val_acc:0.408]
Epoch [3/120    avg_loss:2.205, val_acc:0.430]
Epoch [4/120    avg_loss:2.004, val_acc:0.432]
Epoch [5/120    avg_loss:1.875, val_acc:0.558]
Epoch [6/120    avg_loss:1.750, val_acc:0.569]
Epoch [7/120    avg_loss:1.651, val_acc:0.596]
Epoch [8/120    avg_loss:1.603, val_acc:0.607]
Epoch [9/120    avg_loss:1.465, val_acc:0.607]
Epoch [10/120    avg_loss:1.328, val_acc:0.676]
Epoch [11/120    avg_loss:1.248, val_acc:0.665]
Epoch [12/120    avg_loss:1.123, val_acc:0.699]
Epoch [13/120    avg_loss:0.986, val_acc:0.746]
Epoch [14/120    avg_loss:0.887, val_acc:0.739]
Epoch [15/120    avg_loss:0.853, val_acc:0.741]
Epoch [16/120    avg_loss:0.757, val_acc:0.766]
Epoch [17/120    avg_loss:0.670, val_acc:0.807]
Epoch [18/120    avg_loss:0.573, val_acc:0.794]
Epoch [19/120    avg_loss:0.591, val_acc:0.777]
Epoch [20/120    avg_loss:0.528, val_acc:0.800]
Epoch [21/120    avg_loss:0.472, val_acc:0.829]
Epoch [22/120    avg_loss:0.396, val_acc:0.836]
Epoch [23/120    avg_loss:0.369, val_acc:0.858]
Epoch [24/120    avg_loss:0.339, val_acc:0.858]
Epoch [25/120    avg_loss:0.362, val_acc:0.856]
Epoch [26/120    avg_loss:0.452, val_acc:0.819]
Epoch [27/120    avg_loss:0.334, val_acc:0.860]
Epoch [28/120    avg_loss:0.284, val_acc:0.882]
Epoch [29/120    avg_loss:0.280, val_acc:0.861]
Epoch [30/120    avg_loss:0.261, val_acc:0.886]
Epoch [31/120    avg_loss:0.224, val_acc:0.901]
Epoch [32/120    avg_loss:0.197, val_acc:0.909]
Epoch [33/120    avg_loss:0.183, val_acc:0.923]
Epoch [34/120    avg_loss:0.176, val_acc:0.922]
Epoch [35/120    avg_loss:0.155, val_acc:0.922]
Epoch [36/120    avg_loss:0.146, val_acc:0.922]
Epoch [37/120    avg_loss:0.123, val_acc:0.905]
Epoch [38/120    avg_loss:0.130, val_acc:0.923]
Epoch [39/120    avg_loss:0.137, val_acc:0.931]
Epoch [40/120    avg_loss:0.111, val_acc:0.947]
Epoch [41/120    avg_loss:0.096, val_acc:0.947]
Epoch [42/120    avg_loss:0.085, val_acc:0.924]
Epoch [43/120    avg_loss:0.080, val_acc:0.938]
Epoch [44/120    avg_loss:0.089, val_acc:0.927]
Epoch [45/120    avg_loss:0.100, val_acc:0.947]
Epoch [46/120    avg_loss:0.097, val_acc:0.949]
Epoch [47/120    avg_loss:0.086, val_acc:0.941]
Epoch [48/120    avg_loss:0.076, val_acc:0.943]
Epoch [49/120    avg_loss:0.073, val_acc:0.933]
Epoch [50/120    avg_loss:0.065, val_acc:0.951]
Epoch [51/120    avg_loss:0.092, val_acc:0.959]
Epoch [52/120    avg_loss:0.091, val_acc:0.948]
Epoch [53/120    avg_loss:0.063, val_acc:0.951]
Epoch [54/120    avg_loss:0.061, val_acc:0.960]
Epoch [55/120    avg_loss:0.061, val_acc:0.961]
Epoch [56/120    avg_loss:0.058, val_acc:0.956]
Epoch [57/120    avg_loss:0.044, val_acc:0.968]
Epoch [58/120    avg_loss:0.066, val_acc:0.963]
Epoch [59/120    avg_loss:0.049, val_acc:0.971]
Epoch [60/120    avg_loss:0.045, val_acc:0.963]
Epoch [61/120    avg_loss:0.035, val_acc:0.961]
Epoch [62/120    avg_loss:0.063, val_acc:0.923]
Epoch [63/120    avg_loss:0.085, val_acc:0.928]
Epoch [64/120    avg_loss:0.068, val_acc:0.964]
Epoch [65/120    avg_loss:0.038, val_acc:0.970]
Epoch [66/120    avg_loss:0.038, val_acc:0.968]
Epoch [67/120    avg_loss:0.036, val_acc:0.965]
Epoch [68/120    avg_loss:0.046, val_acc:0.958]
Epoch [69/120    avg_loss:0.043, val_acc:0.961]
Epoch [70/120    avg_loss:0.035, val_acc:0.968]
Epoch [71/120    avg_loss:0.039, val_acc:0.959]
Epoch [72/120    avg_loss:0.037, val_acc:0.956]
Epoch [73/120    avg_loss:0.037, val_acc:0.969]
Epoch [74/120    avg_loss:0.024, val_acc:0.973]
Epoch [75/120    avg_loss:0.022, val_acc:0.975]
Epoch [76/120    avg_loss:0.025, val_acc:0.978]
Epoch [77/120    avg_loss:0.021, val_acc:0.976]
Epoch [78/120    avg_loss:0.019, val_acc:0.975]
Epoch [79/120    avg_loss:0.019, val_acc:0.974]
Epoch [80/120    avg_loss:0.019, val_acc:0.976]
Epoch [81/120    avg_loss:0.018, val_acc:0.976]
Epoch [82/120    avg_loss:0.018, val_acc:0.976]
Epoch [83/120    avg_loss:0.018, val_acc:0.977]
Epoch [84/120    avg_loss:0.016, val_acc:0.976]
Epoch [85/120    avg_loss:0.017, val_acc:0.978]
Epoch [86/120    avg_loss:0.016, val_acc:0.977]
Epoch [87/120    avg_loss:0.014, val_acc:0.977]
Epoch [88/120    avg_loss:0.019, val_acc:0.975]
Epoch [89/120    avg_loss:0.015, val_acc:0.975]
Epoch [90/120    avg_loss:0.014, val_acc:0.975]
Epoch [91/120    avg_loss:0.015, val_acc:0.975]
Epoch [92/120    avg_loss:0.016, val_acc:0.975]
Epoch [93/120    avg_loss:0.016, val_acc:0.974]
Epoch [94/120    avg_loss:0.015, val_acc:0.973]
Epoch [95/120    avg_loss:0.015, val_acc:0.975]
Epoch [96/120    avg_loss:0.015, val_acc:0.974]
Epoch [97/120    avg_loss:0.016, val_acc:0.974]
Epoch [98/120    avg_loss:0.018, val_acc:0.976]
Epoch [99/120    avg_loss:0.015, val_acc:0.975]
Epoch [100/120    avg_loss:0.015, val_acc:0.975]
Epoch [101/120    avg_loss:0.016, val_acc:0.975]
Epoch [102/120    avg_loss:0.016, val_acc:0.975]
Epoch [103/120    avg_loss:0.016, val_acc:0.975]
Epoch [104/120    avg_loss:0.017, val_acc:0.975]
Epoch [105/120    avg_loss:0.017, val_acc:0.975]
Epoch [106/120    avg_loss:0.013, val_acc:0.976]
Epoch [107/120    avg_loss:0.016, val_acc:0.976]
Epoch [108/120    avg_loss:0.018, val_acc:0.975]
Epoch [109/120    avg_loss:0.014, val_acc:0.976]
Epoch [110/120    avg_loss:0.016, val_acc:0.976]
Epoch [111/120    avg_loss:0.014, val_acc:0.975]
Epoch [112/120    avg_loss:0.018, val_acc:0.975]
Epoch [113/120    avg_loss:0.013, val_acc:0.975]
Epoch [114/120    avg_loss:0.018, val_acc:0.975]
Epoch [115/120    avg_loss:0.015, val_acc:0.975]
Epoch [116/120    avg_loss:0.016, val_acc:0.975]
Epoch [117/120    avg_loss:0.014, val_acc:0.975]
Epoch [118/120    avg_loss:0.015, val_acc:0.975]
Epoch [119/120    avg_loss:0.017, val_acc:0.975]
Epoch [120/120    avg_loss:0.012, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    2    0    0    1    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1251    9    1    6    0    0    0    1    0   17    0    0
     0    0    0]
 [   0    0    0  730    3    0    0    0    0    7    2    3    2    0
     0    0    0]
 [   0    0    0    0  212    0    1    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    2    1    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   16    0    1    0    0
     0    0    0]
 [   0    0   11    0    0    1    0    0    0    0  836   25    0    0
     1    1    0]
 [   0    0   27    0    0    4    0    0    0    1   23 2140   13    0
     0    2    0]
 [   0    0    0    4    1    0    0    0    0    0    0    6  518    0
     0    4    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1134    5    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    66  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
97.05149051490515

F1 scores:
[       nan 0.9382716  0.97127329 0.97986577 0.98604651 0.97945205
 0.99317665 0.98039216 0.997669   0.74418605 0.96313364 0.9720645
 0.96551724 1.         0.96757679 0.87284144 0.95705521]

Kappa:
0.9663759614013405
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f16132fe7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.721, val_acc:0.169]
Epoch [2/120    avg_loss:2.367, val_acc:0.323]
Epoch [3/120    avg_loss:2.163, val_acc:0.385]
Epoch [4/120    avg_loss:2.007, val_acc:0.535]
Epoch [5/120    avg_loss:1.883, val_acc:0.546]
Epoch [6/120    avg_loss:1.713, val_acc:0.530]
Epoch [7/120    avg_loss:1.600, val_acc:0.620]
Epoch [8/120    avg_loss:1.502, val_acc:0.599]
Epoch [9/120    avg_loss:1.499, val_acc:0.591]
Epoch [10/120    avg_loss:1.344, val_acc:0.646]
Epoch [11/120    avg_loss:1.190, val_acc:0.632]
Epoch [12/120    avg_loss:1.160, val_acc:0.632]
Epoch [13/120    avg_loss:1.052, val_acc:0.713]
Epoch [14/120    avg_loss:0.896, val_acc:0.743]
Epoch [15/120    avg_loss:0.873, val_acc:0.755]
Epoch [16/120    avg_loss:0.785, val_acc:0.742]
Epoch [17/120    avg_loss:0.703, val_acc:0.752]
Epoch [18/120    avg_loss:0.646, val_acc:0.783]
Epoch [19/120    avg_loss:0.553, val_acc:0.805]
Epoch [20/120    avg_loss:0.528, val_acc:0.799]
Epoch [21/120    avg_loss:0.397, val_acc:0.843]
Epoch [22/120    avg_loss:0.392, val_acc:0.849]
Epoch [23/120    avg_loss:0.338, val_acc:0.861]
Epoch [24/120    avg_loss:0.327, val_acc:0.877]
Epoch [25/120    avg_loss:0.329, val_acc:0.867]
Epoch [26/120    avg_loss:0.272, val_acc:0.895]
Epoch [27/120    avg_loss:0.313, val_acc:0.865]
Epoch [28/120    avg_loss:0.442, val_acc:0.848]
Epoch [29/120    avg_loss:0.304, val_acc:0.858]
Epoch [30/120    avg_loss:0.280, val_acc:0.810]
Epoch [31/120    avg_loss:0.257, val_acc:0.903]
Epoch [32/120    avg_loss:0.274, val_acc:0.842]
Epoch [33/120    avg_loss:0.253, val_acc:0.914]
Epoch [34/120    avg_loss:0.193, val_acc:0.912]
Epoch [35/120    avg_loss:0.211, val_acc:0.873]
Epoch [36/120    avg_loss:0.177, val_acc:0.901]
Epoch [37/120    avg_loss:0.194, val_acc:0.910]
Epoch [38/120    avg_loss:0.159, val_acc:0.943]
Epoch [39/120    avg_loss:0.121, val_acc:0.919]
Epoch [40/120    avg_loss:0.137, val_acc:0.914]
Epoch [41/120    avg_loss:0.154, val_acc:0.942]
Epoch [42/120    avg_loss:0.129, val_acc:0.928]
Epoch [43/120    avg_loss:0.109, val_acc:0.948]
Epoch [44/120    avg_loss:0.082, val_acc:0.953]
Epoch [45/120    avg_loss:0.078, val_acc:0.951]
Epoch [46/120    avg_loss:0.070, val_acc:0.957]
Epoch [47/120    avg_loss:0.076, val_acc:0.965]
Epoch [48/120    avg_loss:0.073, val_acc:0.970]
Epoch [49/120    avg_loss:0.077, val_acc:0.964]
Epoch [50/120    avg_loss:0.075, val_acc:0.960]
Epoch [51/120    avg_loss:0.062, val_acc:0.955]
Epoch [52/120    avg_loss:0.056, val_acc:0.966]
Epoch [53/120    avg_loss:0.064, val_acc:0.968]
Epoch [54/120    avg_loss:0.070, val_acc:0.956]
Epoch [55/120    avg_loss:0.059, val_acc:0.965]
Epoch [56/120    avg_loss:0.057, val_acc:0.975]
Epoch [57/120    avg_loss:0.053, val_acc:0.953]
Epoch [58/120    avg_loss:0.047, val_acc:0.964]
Epoch [59/120    avg_loss:0.046, val_acc:0.965]
Epoch [60/120    avg_loss:0.055, val_acc:0.964]
Epoch [61/120    avg_loss:0.058, val_acc:0.964]
Epoch [62/120    avg_loss:0.044, val_acc:0.946]
Epoch [63/120    avg_loss:0.048, val_acc:0.972]
Epoch [64/120    avg_loss:0.052, val_acc:0.966]
Epoch [65/120    avg_loss:0.052, val_acc:0.970]
Epoch [66/120    avg_loss:0.038, val_acc:0.972]
Epoch [67/120    avg_loss:0.046, val_acc:0.973]
Epoch [68/120    avg_loss:0.050, val_acc:0.961]
Epoch [69/120    avg_loss:0.040, val_acc:0.979]
Epoch [70/120    avg_loss:0.029, val_acc:0.973]
Epoch [71/120    avg_loss:0.040, val_acc:0.976]
Epoch [72/120    avg_loss:0.063, val_acc:0.954]
Epoch [73/120    avg_loss:0.063, val_acc:0.936]
Epoch [74/120    avg_loss:0.056, val_acc:0.963]
Epoch [75/120    avg_loss:0.037, val_acc:0.971]
Epoch [76/120    avg_loss:0.034, val_acc:0.975]
Epoch [77/120    avg_loss:0.028, val_acc:0.970]
Epoch [78/120    avg_loss:0.033, val_acc:0.970]
Epoch [79/120    avg_loss:0.029, val_acc:0.973]
Epoch [80/120    avg_loss:0.029, val_acc:0.974]
Epoch [81/120    avg_loss:0.030, val_acc:0.979]
Epoch [82/120    avg_loss:0.038, val_acc:0.971]
Epoch [83/120    avg_loss:0.026, val_acc:0.982]
Epoch [84/120    avg_loss:0.025, val_acc:0.978]
Epoch [85/120    avg_loss:0.019, val_acc:0.976]
Epoch [86/120    avg_loss:0.025, val_acc:0.974]
Epoch [87/120    avg_loss:0.026, val_acc:0.978]
Epoch [88/120    avg_loss:0.036, val_acc:0.966]
Epoch [89/120    avg_loss:0.041, val_acc:0.969]
Epoch [90/120    avg_loss:0.036, val_acc:0.975]
Epoch [91/120    avg_loss:0.028, val_acc:0.977]
Epoch [92/120    avg_loss:0.020, val_acc:0.985]
Epoch [93/120    avg_loss:0.017, val_acc:0.985]
Epoch [94/120    avg_loss:0.018, val_acc:0.979]
Epoch [95/120    avg_loss:0.014, val_acc:0.980]
Epoch [96/120    avg_loss:0.015, val_acc:0.976]
Epoch [97/120    avg_loss:0.018, val_acc:0.981]
Epoch [98/120    avg_loss:0.011, val_acc:0.986]
Epoch [99/120    avg_loss:0.013, val_acc:0.984]
Epoch [100/120    avg_loss:0.015, val_acc:0.980]
Epoch [101/120    avg_loss:0.015, val_acc:0.978]
Epoch [102/120    avg_loss:0.014, val_acc:0.984]
Epoch [103/120    avg_loss:0.014, val_acc:0.983]
Epoch [104/120    avg_loss:0.014, val_acc:0.982]
Epoch [105/120    avg_loss:0.015, val_acc:0.981]
Epoch [106/120    avg_loss:0.011, val_acc:0.983]
Epoch [107/120    avg_loss:0.010, val_acc:0.988]
Epoch [108/120    avg_loss:0.009, val_acc:0.982]
Epoch [109/120    avg_loss:0.012, val_acc:0.975]
Epoch [110/120    avg_loss:0.013, val_acc:0.975]
Epoch [111/120    avg_loss:0.017, val_acc:0.977]
Epoch [112/120    avg_loss:0.010, val_acc:0.982]
Epoch [113/120    avg_loss:0.009, val_acc:0.983]
Epoch [114/120    avg_loss:0.009, val_acc:0.985]
Epoch [115/120    avg_loss:0.017, val_acc:0.980]
Epoch [116/120    avg_loss:0.016, val_acc:0.985]
Epoch [117/120    avg_loss:0.015, val_acc:0.971]
Epoch [118/120    avg_loss:0.023, val_acc:0.979]
Epoch [119/120    avg_loss:0.016, val_acc:0.986]
Epoch [120/120    avg_loss:0.012, val_acc:0.956]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    0    0    0    0    0    0    2    0    0
     0    0    0]
 [   0    0 1249    5    0    5    0    0    0    0    6   16    4    0
     0    0    0]
 [   0    0    0  694    4    0    0    0    0    2    2   16   28    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    1    1    0    0    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    1    1    0    0    0  839   27    5    0
     0    0    0]
 [   0    0    8    0    0    0    1    0    0    0  216 1968   17    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    2  529    0
     0    1    2]
 [   0    0    0    0    0    1    0    0    0    0    0    4    0  180
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1135    4    0]
 [   0    0    0    0    0    3   11    0    0    0    0    0    0    0
    66  267    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.80758807588076

F1 scores:
[       nan 0.96202532 0.98153242 0.95988935 0.99069767 0.9793578
 0.98640483 0.98039216 0.997669   0.94736842 0.86584107 0.92633561
 0.94464286 0.98630137 0.96719216 0.86268174 0.98224852]

Kappa:
0.9409556641981519
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f428a4997b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.704, val_acc:0.231]
Epoch [2/120    avg_loss:2.419, val_acc:0.366]
Epoch [3/120    avg_loss:2.201, val_acc:0.379]
Epoch [4/120    avg_loss:2.060, val_acc:0.487]
Epoch [5/120    avg_loss:1.896, val_acc:0.537]
Epoch [6/120    avg_loss:1.774, val_acc:0.483]
Epoch [7/120    avg_loss:1.640, val_acc:0.551]
Epoch [8/120    avg_loss:1.531, val_acc:0.585]
Epoch [9/120    avg_loss:1.441, val_acc:0.589]
Epoch [10/120    avg_loss:1.353, val_acc:0.597]
Epoch [11/120    avg_loss:1.216, val_acc:0.662]
Epoch [12/120    avg_loss:1.068, val_acc:0.671]
Epoch [13/120    avg_loss:1.006, val_acc:0.678]
Epoch [14/120    avg_loss:0.860, val_acc:0.725]
Epoch [15/120    avg_loss:0.778, val_acc:0.751]
Epoch [16/120    avg_loss:0.679, val_acc:0.748]
Epoch [17/120    avg_loss:0.675, val_acc:0.781]
Epoch [18/120    avg_loss:0.551, val_acc:0.801]
Epoch [19/120    avg_loss:0.528, val_acc:0.786]
Epoch [20/120    avg_loss:0.446, val_acc:0.797]
Epoch [21/120    avg_loss:0.453, val_acc:0.822]
Epoch [22/120    avg_loss:0.352, val_acc:0.833]
Epoch [23/120    avg_loss:0.312, val_acc:0.861]
Epoch [24/120    avg_loss:0.307, val_acc:0.856]
Epoch [25/120    avg_loss:0.268, val_acc:0.875]
Epoch [26/120    avg_loss:0.255, val_acc:0.879]
Epoch [27/120    avg_loss:0.212, val_acc:0.886]
Epoch [28/120    avg_loss:0.205, val_acc:0.884]
Epoch [29/120    avg_loss:0.221, val_acc:0.897]
Epoch [30/120    avg_loss:0.201, val_acc:0.904]
Epoch [31/120    avg_loss:0.201, val_acc:0.908]
Epoch [32/120    avg_loss:0.172, val_acc:0.920]
Epoch [33/120    avg_loss:0.144, val_acc:0.917]
Epoch [34/120    avg_loss:0.127, val_acc:0.935]
Epoch [35/120    avg_loss:0.118, val_acc:0.936]
Epoch [36/120    avg_loss:0.117, val_acc:0.926]
Epoch [37/120    avg_loss:0.104, val_acc:0.930]
Epoch [38/120    avg_loss:0.102, val_acc:0.936]
Epoch [39/120    avg_loss:0.107, val_acc:0.935]
Epoch [40/120    avg_loss:0.103, val_acc:0.927]
Epoch [41/120    avg_loss:0.097, val_acc:0.936]
Epoch [42/120    avg_loss:0.085, val_acc:0.952]
Epoch [43/120    avg_loss:0.070, val_acc:0.945]
Epoch [44/120    avg_loss:0.063, val_acc:0.946]
Epoch [45/120    avg_loss:0.084, val_acc:0.958]
Epoch [46/120    avg_loss:0.074, val_acc:0.945]
Epoch [47/120    avg_loss:0.069, val_acc:0.949]
Epoch [48/120    avg_loss:0.069, val_acc:0.949]
Epoch [49/120    avg_loss:0.074, val_acc:0.950]
Epoch [50/120    avg_loss:0.064, val_acc:0.948]
Epoch [51/120    avg_loss:0.066, val_acc:0.953]
Epoch [52/120    avg_loss:0.049, val_acc:0.959]
Epoch [53/120    avg_loss:0.049, val_acc:0.954]
Epoch [54/120    avg_loss:0.053, val_acc:0.966]
Epoch [55/120    avg_loss:0.051, val_acc:0.965]
Epoch [56/120    avg_loss:0.043, val_acc:0.964]
Epoch [57/120    avg_loss:0.045, val_acc:0.958]
Epoch [58/120    avg_loss:0.040, val_acc:0.947]
Epoch [59/120    avg_loss:0.056, val_acc:0.942]
Epoch [60/120    avg_loss:0.046, val_acc:0.957]
Epoch [61/120    avg_loss:0.038, val_acc:0.968]
Epoch [62/120    avg_loss:0.038, val_acc:0.959]
Epoch [63/120    avg_loss:0.030, val_acc:0.963]
Epoch [64/120    avg_loss:0.040, val_acc:0.967]
Epoch [65/120    avg_loss:0.035, val_acc:0.966]
Epoch [66/120    avg_loss:0.028, val_acc:0.964]
Epoch [67/120    avg_loss:0.024, val_acc:0.967]
Epoch [68/120    avg_loss:0.022, val_acc:0.969]
Epoch [69/120    avg_loss:0.020, val_acc:0.968]
Epoch [70/120    avg_loss:0.023, val_acc:0.960]
Epoch [71/120    avg_loss:0.028, val_acc:0.969]
Epoch [72/120    avg_loss:0.024, val_acc:0.969]
Epoch [73/120    avg_loss:0.022, val_acc:0.970]
Epoch [74/120    avg_loss:0.019, val_acc:0.973]
Epoch [75/120    avg_loss:0.022, val_acc:0.966]
Epoch [76/120    avg_loss:0.016, val_acc:0.975]
Epoch [77/120    avg_loss:0.016, val_acc:0.970]
Epoch [78/120    avg_loss:0.020, val_acc:0.972]
Epoch [79/120    avg_loss:0.023, val_acc:0.968]
Epoch [80/120    avg_loss:0.029, val_acc:0.966]
Epoch [81/120    avg_loss:0.024, val_acc:0.967]
Epoch [82/120    avg_loss:0.031, val_acc:0.966]
Epoch [83/120    avg_loss:0.025, val_acc:0.966]
Epoch [84/120    avg_loss:0.025, val_acc:0.966]
Epoch [85/120    avg_loss:0.020, val_acc:0.971]
Epoch [86/120    avg_loss:0.020, val_acc:0.966]
Epoch [87/120    avg_loss:0.018, val_acc:0.964]
Epoch [88/120    avg_loss:0.018, val_acc:0.969]
Epoch [89/120    avg_loss:0.015, val_acc:0.972]
Epoch [90/120    avg_loss:0.010, val_acc:0.967]
Epoch [91/120    avg_loss:0.012, val_acc:0.969]
Epoch [92/120    avg_loss:0.010, val_acc:0.971]
Epoch [93/120    avg_loss:0.009, val_acc:0.971]
Epoch [94/120    avg_loss:0.011, val_acc:0.969]
Epoch [95/120    avg_loss:0.011, val_acc:0.969]
Epoch [96/120    avg_loss:0.009, val_acc:0.969]
Epoch [97/120    avg_loss:0.010, val_acc:0.969]
Epoch [98/120    avg_loss:0.008, val_acc:0.970]
Epoch [99/120    avg_loss:0.010, val_acc:0.970]
Epoch [100/120    avg_loss:0.010, val_acc:0.971]
Epoch [101/120    avg_loss:0.012, val_acc:0.970]
Epoch [102/120    avg_loss:0.009, val_acc:0.970]
Epoch [103/120    avg_loss:0.010, val_acc:0.970]
Epoch [104/120    avg_loss:0.011, val_acc:0.970]
Epoch [105/120    avg_loss:0.008, val_acc:0.970]
Epoch [106/120    avg_loss:0.008, val_acc:0.970]
Epoch [107/120    avg_loss:0.009, val_acc:0.970]
Epoch [108/120    avg_loss:0.011, val_acc:0.970]
Epoch [109/120    avg_loss:0.009, val_acc:0.970]
Epoch [110/120    avg_loss:0.009, val_acc:0.970]
Epoch [111/120    avg_loss:0.009, val_acc:0.970]
Epoch [112/120    avg_loss:0.010, val_acc:0.970]
Epoch [113/120    avg_loss:0.009, val_acc:0.970]
Epoch [114/120    avg_loss:0.008, val_acc:0.970]
Epoch [115/120    avg_loss:0.010, val_acc:0.970]
Epoch [116/120    avg_loss:0.007, val_acc:0.970]
Epoch [117/120    avg_loss:0.008, val_acc:0.970]
Epoch [118/120    avg_loss:0.009, val_acc:0.970]
Epoch [119/120    avg_loss:0.010, val_acc:0.970]
Epoch [120/120    avg_loss:0.008, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    1    6    2    1    0    0    0    1   13    0    0
     0    0    0]
 [   0    0    0  721    3    0    1    0    0    4    4    8    2    4
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    5    0    0   13    0    0    0    0
     0    0    0]
 [   0    0   11    0    0    1    0    0    0    0  839   22    0    0
     2    0    0]
 [   0    0   15    0    0    0    0    0    0    0   20 2168    6    1
     0    0    0]
 [   0    0    0    4    2    0    0    0    0    0    0    0  523    0
     0    4    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    1
  1130    8    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    28  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.77777777777777

F1 scores:
[       nan 0.96296296 0.97979798 0.97895451 0.97247706 0.99541284
 0.98274569 1.         0.99883586 0.74285714 0.96492237 0.98055179
 0.97848457 0.98404255 0.98218166 0.9186747  0.97590361]

Kappa:
0.97465904565842
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6ff7fef7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.700, val_acc:0.309]
Epoch [2/120    avg_loss:2.405, val_acc:0.502]
Epoch [3/120    avg_loss:2.179, val_acc:0.497]
Epoch [4/120    avg_loss:2.046, val_acc:0.503]
Epoch [5/120    avg_loss:1.922, val_acc:0.517]
Epoch [6/120    avg_loss:1.800, val_acc:0.527]
Epoch [7/120    avg_loss:1.692, val_acc:0.545]
Epoch [8/120    avg_loss:1.578, val_acc:0.579]
Epoch [9/120    avg_loss:1.468, val_acc:0.617]
Epoch [10/120    avg_loss:1.338, val_acc:0.622]
Epoch [11/120    avg_loss:1.194, val_acc:0.596]
Epoch [12/120    avg_loss:1.097, val_acc:0.647]
Epoch [13/120    avg_loss:0.962, val_acc:0.654]
Epoch [14/120    avg_loss:0.963, val_acc:0.657]
Epoch [15/120    avg_loss:0.897, val_acc:0.692]
Epoch [16/120    avg_loss:0.769, val_acc:0.733]
Epoch [17/120    avg_loss:0.711, val_acc:0.751]
Epoch [18/120    avg_loss:0.578, val_acc:0.742]
Epoch [19/120    avg_loss:0.554, val_acc:0.780]
Epoch [20/120    avg_loss:0.512, val_acc:0.785]
Epoch [21/120    avg_loss:0.497, val_acc:0.829]
Epoch [22/120    avg_loss:0.438, val_acc:0.827]
Epoch [23/120    avg_loss:0.378, val_acc:0.851]
Epoch [24/120    avg_loss:0.338, val_acc:0.864]
Epoch [25/120    avg_loss:0.312, val_acc:0.849]
Epoch [26/120    avg_loss:0.304, val_acc:0.878]
Epoch [27/120    avg_loss:0.264, val_acc:0.898]
Epoch [28/120    avg_loss:0.238, val_acc:0.906]
Epoch [29/120    avg_loss:0.234, val_acc:0.880]
Epoch [30/120    avg_loss:0.220, val_acc:0.897]
Epoch [31/120    avg_loss:0.200, val_acc:0.902]
Epoch [32/120    avg_loss:0.186, val_acc:0.820]
Epoch [33/120    avg_loss:0.268, val_acc:0.904]
Epoch [34/120    avg_loss:0.189, val_acc:0.919]
Epoch [35/120    avg_loss:0.168, val_acc:0.925]
Epoch [36/120    avg_loss:0.146, val_acc:0.917]
Epoch [37/120    avg_loss:0.123, val_acc:0.923]
Epoch [38/120    avg_loss:0.117, val_acc:0.931]
Epoch [39/120    avg_loss:0.109, val_acc:0.933]
Epoch [40/120    avg_loss:0.108, val_acc:0.943]
Epoch [41/120    avg_loss:0.097, val_acc:0.935]
Epoch [42/120    avg_loss:0.116, val_acc:0.927]
Epoch [43/120    avg_loss:0.094, val_acc:0.941]
Epoch [44/120    avg_loss:0.098, val_acc:0.923]
Epoch [45/120    avg_loss:0.096, val_acc:0.943]
Epoch [46/120    avg_loss:0.091, val_acc:0.942]
Epoch [47/120    avg_loss:0.092, val_acc:0.944]
Epoch [48/120    avg_loss:0.088, val_acc:0.903]
Epoch [49/120    avg_loss:0.101, val_acc:0.931]
Epoch [50/120    avg_loss:0.080, val_acc:0.939]
Epoch [51/120    avg_loss:0.135, val_acc:0.928]
Epoch [52/120    avg_loss:0.093, val_acc:0.944]
Epoch [53/120    avg_loss:0.077, val_acc:0.934]
Epoch [54/120    avg_loss:0.064, val_acc:0.944]
Epoch [55/120    avg_loss:0.097, val_acc:0.942]
Epoch [56/120    avg_loss:0.085, val_acc:0.935]
Epoch [57/120    avg_loss:0.079, val_acc:0.941]
Epoch [58/120    avg_loss:0.081, val_acc:0.925]
Epoch [59/120    avg_loss:0.165, val_acc:0.925]
Epoch [60/120    avg_loss:0.118, val_acc:0.944]
Epoch [61/120    avg_loss:0.107, val_acc:0.928]
Epoch [62/120    avg_loss:0.076, val_acc:0.930]
Epoch [63/120    avg_loss:0.060, val_acc:0.949]
Epoch [64/120    avg_loss:0.043, val_acc:0.965]
Epoch [65/120    avg_loss:0.039, val_acc:0.959]
Epoch [66/120    avg_loss:0.047, val_acc:0.964]
Epoch [67/120    avg_loss:0.042, val_acc:0.950]
Epoch [68/120    avg_loss:0.038, val_acc:0.960]
Epoch [69/120    avg_loss:0.040, val_acc:0.958]
Epoch [70/120    avg_loss:0.036, val_acc:0.957]
Epoch [71/120    avg_loss:0.039, val_acc:0.965]
Epoch [72/120    avg_loss:0.030, val_acc:0.952]
Epoch [73/120    avg_loss:0.037, val_acc:0.957]
Epoch [74/120    avg_loss:0.027, val_acc:0.958]
Epoch [75/120    avg_loss:0.023, val_acc:0.960]
Epoch [76/120    avg_loss:0.022, val_acc:0.969]
Epoch [77/120    avg_loss:0.023, val_acc:0.959]
Epoch [78/120    avg_loss:0.019, val_acc:0.961]
Epoch [79/120    avg_loss:0.026, val_acc:0.958]
Epoch [80/120    avg_loss:0.029, val_acc:0.955]
Epoch [81/120    avg_loss:0.064, val_acc:0.934]
Epoch [82/120    avg_loss:0.038, val_acc:0.930]
Epoch [83/120    avg_loss:0.033, val_acc:0.957]
Epoch [84/120    avg_loss:0.031, val_acc:0.964]
Epoch [85/120    avg_loss:0.022, val_acc:0.952]
Epoch [86/120    avg_loss:0.019, val_acc:0.970]
Epoch [87/120    avg_loss:0.017, val_acc:0.970]
Epoch [88/120    avg_loss:0.013, val_acc:0.969]
Epoch [89/120    avg_loss:0.015, val_acc:0.969]
Epoch [90/120    avg_loss:0.020, val_acc:0.961]
Epoch [91/120    avg_loss:0.061, val_acc:0.942]
Epoch [92/120    avg_loss:0.066, val_acc:0.946]
Epoch [93/120    avg_loss:0.032, val_acc:0.946]
Epoch [94/120    avg_loss:0.035, val_acc:0.960]
Epoch [95/120    avg_loss:0.020, val_acc:0.956]
Epoch [96/120    avg_loss:0.021, val_acc:0.966]
Epoch [97/120    avg_loss:0.021, val_acc:0.969]
Epoch [98/120    avg_loss:0.014, val_acc:0.966]
Epoch [99/120    avg_loss:0.013, val_acc:0.966]
Epoch [100/120    avg_loss:0.014, val_acc:0.968]
Epoch [101/120    avg_loss:0.012, val_acc:0.971]
Epoch [102/120    avg_loss:0.010, val_acc:0.974]
Epoch [103/120    avg_loss:0.011, val_acc:0.972]
Epoch [104/120    avg_loss:0.010, val_acc:0.971]
Epoch [105/120    avg_loss:0.008, val_acc:0.970]
Epoch [106/120    avg_loss:0.009, val_acc:0.970]
Epoch [107/120    avg_loss:0.009, val_acc:0.969]
Epoch [108/120    avg_loss:0.009, val_acc:0.969]
Epoch [109/120    avg_loss:0.010, val_acc:0.970]
Epoch [110/120    avg_loss:0.008, val_acc:0.969]
Epoch [111/120    avg_loss:0.010, val_acc:0.969]
Epoch [112/120    avg_loss:0.009, val_acc:0.970]
Epoch [113/120    avg_loss:0.008, val_acc:0.970]
Epoch [114/120    avg_loss:0.009, val_acc:0.969]
Epoch [115/120    avg_loss:0.009, val_acc:0.970]
Epoch [116/120    avg_loss:0.008, val_acc:0.970]
Epoch [117/120    avg_loss:0.009, val_acc:0.970]
Epoch [118/120    avg_loss:0.008, val_acc:0.970]
Epoch [119/120    avg_loss:0.009, val_acc:0.971]
Epoch [120/120    avg_loss:0.007, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    1    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1263    0    0    0    0    0    0    0    0   22    0    0
     0    0    0]
 [   0    0    0  730    2    3    3    0    0    0    1    6    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    4    0    0   14    0    0    0    0
     0    0    0]
 [   0    0   11    0    0    0    2    0    0    0  837   23    2    0
     0    0    0]
 [   0    0   15    3    0    2    0    0    0    0   12 2160   15    0
     0    3    0]
 [   0    0    0    5    0    0    0    0    0    0    0    2  523    0
     1    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    0    0    0    0
  1127    8    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    82  263    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.37669376693766

F1 scores:
[       nan 0.98765432 0.98135198 0.98316498 0.99300699 0.98976109
 0.99093656 1.         1.         0.875      0.97043478 0.97649186
 0.9703154  1.         0.95955726 0.84430177 0.98203593]

Kappa:
0.9700687174624086
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8aad094780>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.732, val_acc:0.281]
Epoch [2/120    avg_loss:2.403, val_acc:0.314]
Epoch [3/120    avg_loss:2.193, val_acc:0.402]
Epoch [4/120    avg_loss:2.029, val_acc:0.464]
Epoch [5/120    avg_loss:1.884, val_acc:0.574]
Epoch [6/120    avg_loss:1.798, val_acc:0.589]
Epoch [7/120    avg_loss:1.658, val_acc:0.617]
Epoch [8/120    avg_loss:1.521, val_acc:0.639]
Epoch [9/120    avg_loss:1.409, val_acc:0.641]
Epoch [10/120    avg_loss:1.278, val_acc:0.659]
Epoch [11/120    avg_loss:1.137, val_acc:0.696]
Epoch [12/120    avg_loss:1.010, val_acc:0.699]
Epoch [13/120    avg_loss:0.927, val_acc:0.726]
Epoch [14/120    avg_loss:0.857, val_acc:0.741]
Epoch [15/120    avg_loss:0.802, val_acc:0.761]
Epoch [16/120    avg_loss:0.704, val_acc:0.815]
Epoch [17/120    avg_loss:0.600, val_acc:0.806]
Epoch [18/120    avg_loss:0.613, val_acc:0.835]
Epoch [19/120    avg_loss:0.485, val_acc:0.847]
Epoch [20/120    avg_loss:0.426, val_acc:0.855]
Epoch [21/120    avg_loss:0.363, val_acc:0.833]
Epoch [22/120    avg_loss:0.385, val_acc:0.874]
Epoch [23/120    avg_loss:0.393, val_acc:0.825]
Epoch [24/120    avg_loss:0.492, val_acc:0.843]
Epoch [25/120    avg_loss:0.425, val_acc:0.851]
Epoch [26/120    avg_loss:0.323, val_acc:0.878]
Epoch [27/120    avg_loss:0.271, val_acc:0.911]
Epoch [28/120    avg_loss:0.228, val_acc:0.904]
Epoch [29/120    avg_loss:0.219, val_acc:0.898]
Epoch [30/120    avg_loss:0.194, val_acc:0.887]
Epoch [31/120    avg_loss:0.183, val_acc:0.912]
Epoch [32/120    avg_loss:0.170, val_acc:0.932]
Epoch [33/120    avg_loss:0.145, val_acc:0.918]
Epoch [34/120    avg_loss:0.144, val_acc:0.916]
Epoch [35/120    avg_loss:0.130, val_acc:0.945]
Epoch [36/120    avg_loss:0.131, val_acc:0.958]
Epoch [37/120    avg_loss:0.123, val_acc:0.948]
Epoch [38/120    avg_loss:0.129, val_acc:0.943]
Epoch [39/120    avg_loss:0.113, val_acc:0.946]
Epoch [40/120    avg_loss:0.106, val_acc:0.944]
Epoch [41/120    avg_loss:0.114, val_acc:0.952]
Epoch [42/120    avg_loss:0.099, val_acc:0.954]
Epoch [43/120    avg_loss:0.088, val_acc:0.949]
Epoch [44/120    avg_loss:0.108, val_acc:0.921]
Epoch [45/120    avg_loss:0.101, val_acc:0.929]
Epoch [46/120    avg_loss:0.108, val_acc:0.915]
Epoch [47/120    avg_loss:0.099, val_acc:0.955]
Epoch [48/120    avg_loss:0.069, val_acc:0.968]
Epoch [49/120    avg_loss:0.064, val_acc:0.968]
Epoch [50/120    avg_loss:0.066, val_acc:0.958]
Epoch [51/120    avg_loss:0.057, val_acc:0.954]
Epoch [52/120    avg_loss:0.068, val_acc:0.977]
Epoch [53/120    avg_loss:0.062, val_acc:0.958]
Epoch [54/120    avg_loss:0.079, val_acc:0.964]
Epoch [55/120    avg_loss:0.055, val_acc:0.970]
Epoch [56/120    avg_loss:0.055, val_acc:0.963]
Epoch [57/120    avg_loss:0.042, val_acc:0.972]
Epoch [58/120    avg_loss:0.052, val_acc:0.975]
Epoch [59/120    avg_loss:0.042, val_acc:0.967]
Epoch [60/120    avg_loss:0.060, val_acc:0.959]
Epoch [61/120    avg_loss:0.059, val_acc:0.967]
Epoch [62/120    avg_loss:0.048, val_acc:0.957]
Epoch [63/120    avg_loss:0.073, val_acc:0.958]
Epoch [64/120    avg_loss:0.130, val_acc:0.957]
Epoch [65/120    avg_loss:0.066, val_acc:0.959]
Epoch [66/120    avg_loss:0.061, val_acc:0.968]
Epoch [67/120    avg_loss:0.049, val_acc:0.974]
Epoch [68/120    avg_loss:0.044, val_acc:0.975]
Epoch [69/120    avg_loss:0.040, val_acc:0.973]
Epoch [70/120    avg_loss:0.039, val_acc:0.977]
Epoch [71/120    avg_loss:0.031, val_acc:0.978]
Epoch [72/120    avg_loss:0.034, val_acc:0.978]
Epoch [73/120    avg_loss:0.032, val_acc:0.976]
Epoch [74/120    avg_loss:0.033, val_acc:0.978]
Epoch [75/120    avg_loss:0.030, val_acc:0.979]
Epoch [76/120    avg_loss:0.033, val_acc:0.979]
Epoch [77/120    avg_loss:0.034, val_acc:0.979]
Epoch [78/120    avg_loss:0.025, val_acc:0.980]
Epoch [79/120    avg_loss:0.027, val_acc:0.980]
Epoch [80/120    avg_loss:0.033, val_acc:0.980]
Epoch [81/120    avg_loss:0.026, val_acc:0.981]
Epoch [82/120    avg_loss:0.028, val_acc:0.983]
Epoch [83/120    avg_loss:0.025, val_acc:0.978]
Epoch [84/120    avg_loss:0.025, val_acc:0.979]
Epoch [85/120    avg_loss:0.023, val_acc:0.979]
Epoch [86/120    avg_loss:0.025, val_acc:0.979]
Epoch [87/120    avg_loss:0.030, val_acc:0.979]
Epoch [88/120    avg_loss:0.022, val_acc:0.981]
Epoch [89/120    avg_loss:0.024, val_acc:0.980]
Epoch [90/120    avg_loss:0.022, val_acc:0.977]
Epoch [91/120    avg_loss:0.027, val_acc:0.976]
Epoch [92/120    avg_loss:0.025, val_acc:0.979]
Epoch [93/120    avg_loss:0.029, val_acc:0.980]
Epoch [94/120    avg_loss:0.023, val_acc:0.981]
Epoch [95/120    avg_loss:0.029, val_acc:0.981]
Epoch [96/120    avg_loss:0.022, val_acc:0.981]
Epoch [97/120    avg_loss:0.023, val_acc:0.980]
Epoch [98/120    avg_loss:0.023, val_acc:0.979]
Epoch [99/120    avg_loss:0.025, val_acc:0.979]
Epoch [100/120    avg_loss:0.023, val_acc:0.980]
Epoch [101/120    avg_loss:0.024, val_acc:0.979]
Epoch [102/120    avg_loss:0.025, val_acc:0.980]
Epoch [103/120    avg_loss:0.026, val_acc:0.980]
Epoch [104/120    avg_loss:0.022, val_acc:0.980]
Epoch [105/120    avg_loss:0.025, val_acc:0.982]
Epoch [106/120    avg_loss:0.026, val_acc:0.982]
Epoch [107/120    avg_loss:0.025, val_acc:0.981]
Epoch [108/120    avg_loss:0.021, val_acc:0.981]
Epoch [109/120    avg_loss:0.022, val_acc:0.981]
Epoch [110/120    avg_loss:0.027, val_acc:0.981]
Epoch [111/120    avg_loss:0.022, val_acc:0.981]
Epoch [112/120    avg_loss:0.019, val_acc:0.981]
Epoch [113/120    avg_loss:0.021, val_acc:0.981]
Epoch [114/120    avg_loss:0.025, val_acc:0.981]
Epoch [115/120    avg_loss:0.021, val_acc:0.981]
Epoch [116/120    avg_loss:0.024, val_acc:0.981]
Epoch [117/120    avg_loss:0.022, val_acc:0.981]
Epoch [118/120    avg_loss:0.022, val_acc:0.981]
Epoch [119/120    avg_loss:0.021, val_acc:0.981]
Epoch [120/120    avg_loss:0.030, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1241    2    2    2    1    0    0    1    4   31    1    0
     0    0    0]
 [   0    0    0  730    3    0    0    0    0    1    2    4    7    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  652    0    0    0    3    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    2    0    2    0    0    0    0  855   12    0    0
     2    0    0]
 [   0    0    9    4    0    0    1    0    0    0   30 2147   14    4
     1    0    0]
 [   0    0    0    1    0    0    0    0    0    0    5    5  520    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  184
     0    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    2
  1131    6    0]
 [   0    0    0    0    0    1    5    0    0    0    0    0    0    0
    75  266    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.1490514905149

F1 scores:
[       nan 0.975      0.97755022 0.98250336 0.98839907 0.98850575
 0.99088146 1.         1.         0.92307692 0.96392334 0.97369615
 0.96385542 0.98133333 0.96132597 0.85530547 0.97590361]

Kappa:
0.9674867926967792
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f28a015d828>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.746, val_acc:0.217]
Epoch [2/120    avg_loss:2.438, val_acc:0.301]
Epoch [3/120    avg_loss:2.229, val_acc:0.386]
Epoch [4/120    avg_loss:2.072, val_acc:0.420]
Epoch [5/120    avg_loss:1.924, val_acc:0.471]
Epoch [6/120    avg_loss:1.818, val_acc:0.497]
Epoch [7/120    avg_loss:1.672, val_acc:0.566]
Epoch [8/120    avg_loss:1.580, val_acc:0.594]
Epoch [9/120    avg_loss:1.456, val_acc:0.542]
Epoch [10/120    avg_loss:1.359, val_acc:0.632]
Epoch [11/120    avg_loss:1.278, val_acc:0.665]
Epoch [12/120    avg_loss:1.108, val_acc:0.685]
Epoch [13/120    avg_loss:1.024, val_acc:0.695]
Epoch [14/120    avg_loss:0.902, val_acc:0.703]
Epoch [15/120    avg_loss:0.755, val_acc:0.769]
Epoch [16/120    avg_loss:0.699, val_acc:0.793]
Epoch [17/120    avg_loss:0.637, val_acc:0.778]
Epoch [18/120    avg_loss:0.625, val_acc:0.775]
Epoch [19/120    avg_loss:0.672, val_acc:0.797]
Epoch [20/120    avg_loss:0.560, val_acc:0.762]
Epoch [21/120    avg_loss:0.493, val_acc:0.843]
Epoch [22/120    avg_loss:0.411, val_acc:0.847]
Epoch [23/120    avg_loss:0.356, val_acc:0.854]
Epoch [24/120    avg_loss:0.331, val_acc:0.858]
Epoch [25/120    avg_loss:0.295, val_acc:0.854]
Epoch [26/120    avg_loss:0.294, val_acc:0.869]
Epoch [27/120    avg_loss:0.305, val_acc:0.879]
Epoch [28/120    avg_loss:0.291, val_acc:0.887]
Epoch [29/120    avg_loss:0.224, val_acc:0.904]
Epoch [30/120    avg_loss:0.220, val_acc:0.876]
Epoch [31/120    avg_loss:0.223, val_acc:0.898]
Epoch [32/120    avg_loss:0.201, val_acc:0.916]
Epoch [33/120    avg_loss:0.189, val_acc:0.901]
Epoch [34/120    avg_loss:0.162, val_acc:0.904]
Epoch [35/120    avg_loss:0.152, val_acc:0.932]
Epoch [36/120    avg_loss:0.127, val_acc:0.939]
Epoch [37/120    avg_loss:0.140, val_acc:0.934]
Epoch [38/120    avg_loss:0.133, val_acc:0.948]
Epoch [39/120    avg_loss:0.118, val_acc:0.943]
Epoch [40/120    avg_loss:0.096, val_acc:0.929]
Epoch [41/120    avg_loss:0.104, val_acc:0.942]
Epoch [42/120    avg_loss:0.092, val_acc:0.927]
Epoch [43/120    avg_loss:0.097, val_acc:0.932]
Epoch [44/120    avg_loss:0.076, val_acc:0.948]
Epoch [45/120    avg_loss:0.078, val_acc:0.955]
Epoch [46/120    avg_loss:0.079, val_acc:0.955]
Epoch [47/120    avg_loss:0.066, val_acc:0.964]
Epoch [48/120    avg_loss:0.075, val_acc:0.963]
Epoch [49/120    avg_loss:0.066, val_acc:0.959]
Epoch [50/120    avg_loss:0.058, val_acc:0.975]
Epoch [51/120    avg_loss:0.053, val_acc:0.964]
Epoch [52/120    avg_loss:0.049, val_acc:0.960]
Epoch [53/120    avg_loss:0.056, val_acc:0.951]
Epoch [54/120    avg_loss:0.078, val_acc:0.953]
Epoch [55/120    avg_loss:0.054, val_acc:0.969]
Epoch [56/120    avg_loss:0.049, val_acc:0.970]
Epoch [57/120    avg_loss:0.037, val_acc:0.971]
Epoch [58/120    avg_loss:0.036, val_acc:0.964]
Epoch [59/120    avg_loss:0.052, val_acc:0.948]
Epoch [60/120    avg_loss:0.053, val_acc:0.964]
Epoch [61/120    avg_loss:0.047, val_acc:0.979]
Epoch [62/120    avg_loss:0.048, val_acc:0.967]
Epoch [63/120    avg_loss:0.040, val_acc:0.970]
Epoch [64/120    avg_loss:0.035, val_acc:0.975]
Epoch [65/120    avg_loss:0.025, val_acc:0.976]
Epoch [66/120    avg_loss:0.028, val_acc:0.979]
Epoch [67/120    avg_loss:0.026, val_acc:0.971]
Epoch [68/120    avg_loss:0.026, val_acc:0.969]
Epoch [69/120    avg_loss:0.034, val_acc:0.971]
Epoch [70/120    avg_loss:0.028, val_acc:0.978]
Epoch [71/120    avg_loss:0.028, val_acc:0.969]
Epoch [72/120    avg_loss:0.026, val_acc:0.970]
Epoch [73/120    avg_loss:0.024, val_acc:0.972]
Epoch [74/120    avg_loss:0.026, val_acc:0.980]
Epoch [75/120    avg_loss:0.022, val_acc:0.973]
Epoch [76/120    avg_loss:0.022, val_acc:0.975]
Epoch [77/120    avg_loss:0.022, val_acc:0.978]
Epoch [78/120    avg_loss:0.026, val_acc:0.960]
Epoch [79/120    avg_loss:0.027, val_acc:0.971]
Epoch [80/120    avg_loss:0.028, val_acc:0.974]
Epoch [81/120    avg_loss:0.021, val_acc:0.978]
Epoch [82/120    avg_loss:0.018, val_acc:0.977]
Epoch [83/120    avg_loss:0.018, val_acc:0.965]
Epoch [84/120    avg_loss:0.028, val_acc:0.981]
Epoch [85/120    avg_loss:0.020, val_acc:0.973]
Epoch [86/120    avg_loss:0.019, val_acc:0.965]
Epoch [87/120    avg_loss:0.025, val_acc:0.963]
Epoch [88/120    avg_loss:0.022, val_acc:0.978]
Epoch [89/120    avg_loss:0.020, val_acc:0.980]
Epoch [90/120    avg_loss:0.023, val_acc:0.979]
Epoch [91/120    avg_loss:0.023, val_acc:0.969]
Epoch [92/120    avg_loss:0.016, val_acc:0.986]
Epoch [93/120    avg_loss:0.018, val_acc:0.980]
Epoch [94/120    avg_loss:0.019, val_acc:0.982]
Epoch [95/120    avg_loss:0.025, val_acc:0.978]
Epoch [96/120    avg_loss:0.032, val_acc:0.973]
Epoch [97/120    avg_loss:0.026, val_acc:0.978]
Epoch [98/120    avg_loss:0.025, val_acc:0.983]
Epoch [99/120    avg_loss:0.022, val_acc:0.979]
Epoch [100/120    avg_loss:0.014, val_acc:0.984]
Epoch [101/120    avg_loss:0.011, val_acc:0.990]
Epoch [102/120    avg_loss:0.009, val_acc:0.989]
Epoch [103/120    avg_loss:0.023, val_acc:0.978]
Epoch [104/120    avg_loss:0.013, val_acc:0.976]
Epoch [105/120    avg_loss:0.014, val_acc:0.980]
Epoch [106/120    avg_loss:0.010, val_acc:0.981]
Epoch [107/120    avg_loss:0.010, val_acc:0.980]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.971]
Epoch [110/120    avg_loss:0.014, val_acc:0.965]
Epoch [111/120    avg_loss:0.012, val_acc:0.981]
Epoch [112/120    avg_loss:0.011, val_acc:0.985]
Epoch [113/120    avg_loss:0.008, val_acc:0.984]
Epoch [114/120    avg_loss:0.010, val_acc:0.984]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.991]
Epoch [117/120    avg_loss:0.007, val_acc:0.991]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.007, val_acc:0.990]
Epoch [120/120    avg_loss:0.006, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1264    2    1    0    0    0    0    0    0   18    0    0
     0    0    0]
 [   0    0    0  719    4    0    0    0    0    8    3   11    2    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    1    0    0    0  857   12    0    0
     0    1    0]
 [   0    0   12    0    0    0    0    0    0    1   27 2155   15    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0    0    0    8  518    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1130    8    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    64  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.58265582655827

F1 scores:
[       nan 0.95238095 0.98519096 0.97623897 0.98604651 0.99884925
 0.99469295 1.         0.99649942 0.8        0.97275823 0.9764386
 0.9682243  1.         0.96788009 0.87598116 0.98224852]

Kappa:
0.9724307131323883
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbda7d6f7f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 55432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.735, val_acc:0.183]
Epoch [2/120    avg_loss:2.363, val_acc:0.333]
Epoch [3/120    avg_loss:2.162, val_acc:0.482]
Epoch [4/120    avg_loss:1.991, val_acc:0.527]
Epoch [5/120    avg_loss:1.858, val_acc:0.507]
Epoch [6/120    avg_loss:1.761, val_acc:0.588]
Epoch [7/120    avg_loss:1.637, val_acc:0.618]
Epoch [8/120    avg_loss:1.603, val_acc:0.594]
Epoch [9/120    avg_loss:1.422, val_acc:0.626]
Epoch [10/120    avg_loss:1.291, val_acc:0.614]
Epoch [11/120    avg_loss:1.273, val_acc:0.635]
Epoch [12/120    avg_loss:1.095, val_acc:0.651]
Epoch [13/120    avg_loss:0.963, val_acc:0.709]
Epoch [14/120    avg_loss:0.909, val_acc:0.745]
Epoch [15/120    avg_loss:0.808, val_acc:0.746]
Epoch [16/120    avg_loss:0.720, val_acc:0.767]
Epoch [17/120    avg_loss:0.716, val_acc:0.770]
Epoch [18/120    avg_loss:0.577, val_acc:0.767]
Epoch [19/120    avg_loss:0.561, val_acc:0.764]
Epoch [20/120    avg_loss:0.566, val_acc:0.773]
Epoch [21/120    avg_loss:0.450, val_acc:0.832]
Epoch [22/120    avg_loss:0.392, val_acc:0.814]
Epoch [23/120    avg_loss:0.365, val_acc:0.865]
Epoch [24/120    avg_loss:0.306, val_acc:0.852]
Epoch [25/120    avg_loss:0.264, val_acc:0.863]
Epoch [26/120    avg_loss:0.245, val_acc:0.887]
Epoch [27/120    avg_loss:0.232, val_acc:0.877]
Epoch [28/120    avg_loss:0.216, val_acc:0.906]
Epoch [29/120    avg_loss:0.190, val_acc:0.903]
Epoch [30/120    avg_loss:0.186, val_acc:0.864]
Epoch [31/120    avg_loss:0.198, val_acc:0.889]
Epoch [32/120    avg_loss:0.198, val_acc:0.925]
Epoch [33/120    avg_loss:0.143, val_acc:0.924]
Epoch [34/120    avg_loss:0.168, val_acc:0.928]
Epoch [35/120    avg_loss:0.131, val_acc:0.923]
Epoch [36/120    avg_loss:0.112, val_acc:0.931]
Epoch [37/120    avg_loss:0.102, val_acc:0.935]
Epoch [38/120    avg_loss:0.089, val_acc:0.930]
Epoch [39/120    avg_loss:0.104, val_acc:0.932]
Epoch [40/120    avg_loss:0.117, val_acc:0.904]
Epoch [41/120    avg_loss:0.103, val_acc:0.935]
Epoch [42/120    avg_loss:0.089, val_acc:0.934]
Epoch [43/120    avg_loss:0.091, val_acc:0.944]
Epoch [44/120    avg_loss:0.081, val_acc:0.946]
Epoch [45/120    avg_loss:0.077, val_acc:0.947]
Epoch [46/120    avg_loss:0.066, val_acc:0.959]
Epoch [47/120    avg_loss:0.065, val_acc:0.960]
Epoch [48/120    avg_loss:0.060, val_acc:0.953]
Epoch [49/120    avg_loss:0.054, val_acc:0.942]
Epoch [50/120    avg_loss:0.046, val_acc:0.964]
Epoch [51/120    avg_loss:0.048, val_acc:0.957]
Epoch [52/120    avg_loss:0.049, val_acc:0.954]
Epoch [53/120    avg_loss:0.062, val_acc:0.940]
Epoch [54/120    avg_loss:0.059, val_acc:0.965]
Epoch [55/120    avg_loss:0.040, val_acc:0.955]
Epoch [56/120    avg_loss:0.062, val_acc:0.948]
Epoch [57/120    avg_loss:0.079, val_acc:0.960]
Epoch [58/120    avg_loss:0.050, val_acc:0.953]
Epoch [59/120    avg_loss:0.040, val_acc:0.966]
Epoch [60/120    avg_loss:0.039, val_acc:0.961]
Epoch [61/120    avg_loss:0.035, val_acc:0.950]
Epoch [62/120    avg_loss:0.036, val_acc:0.958]
Epoch [63/120    avg_loss:0.031, val_acc:0.958]
Epoch [64/120    avg_loss:0.028, val_acc:0.972]
Epoch [65/120    avg_loss:0.025, val_acc:0.967]
Epoch [66/120    avg_loss:0.023, val_acc:0.968]
Epoch [67/120    avg_loss:0.027, val_acc:0.967]
Epoch [68/120    avg_loss:0.027, val_acc:0.961]
Epoch [69/120    avg_loss:0.022, val_acc:0.959]
Epoch [70/120    avg_loss:0.025, val_acc:0.964]
Epoch [71/120    avg_loss:0.025, val_acc:0.966]
Epoch [72/120    avg_loss:0.027, val_acc:0.960]
Epoch [73/120    avg_loss:0.022, val_acc:0.973]
Epoch [74/120    avg_loss:0.024, val_acc:0.964]
Epoch [75/120    avg_loss:0.023, val_acc:0.971]
Epoch [76/120    avg_loss:0.018, val_acc:0.970]
Epoch [77/120    avg_loss:0.017, val_acc:0.969]
Epoch [78/120    avg_loss:0.021, val_acc:0.968]
Epoch [79/120    avg_loss:0.015, val_acc:0.973]
Epoch [80/120    avg_loss:0.013, val_acc:0.973]
Epoch [81/120    avg_loss:0.014, val_acc:0.969]
Epoch [82/120    avg_loss:0.017, val_acc:0.970]
Epoch [83/120    avg_loss:0.012, val_acc:0.972]
Epoch [84/120    avg_loss:0.010, val_acc:0.977]
Epoch [85/120    avg_loss:0.011, val_acc:0.970]
Epoch [86/120    avg_loss:0.012, val_acc:0.972]
Epoch [87/120    avg_loss:0.012, val_acc:0.981]
Epoch [88/120    avg_loss:0.010, val_acc:0.975]
Epoch [89/120    avg_loss:0.011, val_acc:0.974]
Epoch [90/120    avg_loss:0.013, val_acc:0.966]
Epoch [91/120    avg_loss:0.020, val_acc:0.969]
Epoch [92/120    avg_loss:0.037, val_acc:0.958]
Epoch [93/120    avg_loss:0.028, val_acc:0.961]
Epoch [94/120    avg_loss:0.021, val_acc:0.947]
Epoch [95/120    avg_loss:0.023, val_acc:0.952]
Epoch [96/120    avg_loss:0.013, val_acc:0.971]
Epoch [97/120    avg_loss:0.010, val_acc:0.972]
Epoch [98/120    avg_loss:0.012, val_acc:0.971]
Epoch [99/120    avg_loss:0.013, val_acc:0.971]
Epoch [100/120    avg_loss:0.014, val_acc:0.971]
Epoch [101/120    avg_loss:0.010, val_acc:0.971]
Epoch [102/120    avg_loss:0.010, val_acc:0.975]
Epoch [103/120    avg_loss:0.009, val_acc:0.975]
Epoch [104/120    avg_loss:0.007, val_acc:0.974]
Epoch [105/120    avg_loss:0.007, val_acc:0.974]
Epoch [106/120    avg_loss:0.007, val_acc:0.975]
Epoch [107/120    avg_loss:0.006, val_acc:0.975]
Epoch [108/120    avg_loss:0.010, val_acc:0.976]
Epoch [109/120    avg_loss:0.006, val_acc:0.976]
Epoch [110/120    avg_loss:0.006, val_acc:0.976]
Epoch [111/120    avg_loss:0.006, val_acc:0.976]
Epoch [112/120    avg_loss:0.007, val_acc:0.976]
Epoch [113/120    avg_loss:0.008, val_acc:0.975]
Epoch [114/120    avg_loss:0.006, val_acc:0.975]
Epoch [115/120    avg_loss:0.006, val_acc:0.975]
Epoch [116/120    avg_loss:0.007, val_acc:0.975]
Epoch [117/120    avg_loss:0.006, val_acc:0.975]
Epoch [118/120    avg_loss:0.006, val_acc:0.975]
Epoch [119/120    avg_loss:0.007, val_acc:0.975]
Epoch [120/120    avg_loss:0.007, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1246    9    4    0    0    0    0    0    3   23    0    0
     0    0    0]
 [   0    0    0  704    1    0    2    0    0    9    2   25    4    0
     0    0    0]
 [   0    0    0    1  209    0    0    0    0    0    0    0    2    1
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    0  856   11    0    0
     0    1    0]
 [   0    0   19    7    0    0    0    0    0    0   16 2152   11    0
     3    0    2]
 [   0    0    0    3    4    0    0    0    0    0    2    4  513    0
     0    6    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1118   21    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    71  275    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.86720867208672

F1 scores:
[       nan 1.         0.97457959 0.95717199 0.96983759 0.99423299
 0.99316629 1.         1.         0.69767442 0.97605473 0.97221595
 0.96338028 0.99459459 0.95760171 0.84615385 0.97076023]

Kappa:
0.9642664613995622
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ee3337860>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.718, val_acc:0.418]
Epoch [2/120    avg_loss:2.404, val_acc:0.445]
Epoch [3/120    avg_loss:2.241, val_acc:0.466]
Epoch [4/120    avg_loss:2.070, val_acc:0.510]
Epoch [5/120    avg_loss:1.936, val_acc:0.532]
Epoch [6/120    avg_loss:1.840, val_acc:0.542]
Epoch [7/120    avg_loss:1.696, val_acc:0.564]
Epoch [8/120    avg_loss:1.598, val_acc:0.573]
Epoch [9/120    avg_loss:1.474, val_acc:0.595]
Epoch [10/120    avg_loss:1.358, val_acc:0.630]
Epoch [11/120    avg_loss:1.217, val_acc:0.632]
Epoch [12/120    avg_loss:1.093, val_acc:0.654]
Epoch [13/120    avg_loss:1.009, val_acc:0.691]
Epoch [14/120    avg_loss:0.908, val_acc:0.699]
Epoch [15/120    avg_loss:0.836, val_acc:0.741]
Epoch [16/120    avg_loss:0.739, val_acc:0.747]
Epoch [17/120    avg_loss:0.663, val_acc:0.789]
Epoch [18/120    avg_loss:0.610, val_acc:0.740]
Epoch [19/120    avg_loss:0.577, val_acc:0.792]
Epoch [20/120    avg_loss:0.516, val_acc:0.753]
Epoch [21/120    avg_loss:0.524, val_acc:0.829]
Epoch [22/120    avg_loss:0.436, val_acc:0.852]
Epoch [23/120    avg_loss:0.369, val_acc:0.845]
Epoch [24/120    avg_loss:0.362, val_acc:0.841]
Epoch [25/120    avg_loss:0.329, val_acc:0.873]
Epoch [26/120    avg_loss:0.275, val_acc:0.865]
Epoch [27/120    avg_loss:0.257, val_acc:0.905]
Epoch [28/120    avg_loss:0.223, val_acc:0.899]
Epoch [29/120    avg_loss:0.234, val_acc:0.899]
Epoch [30/120    avg_loss:0.231, val_acc:0.829]
Epoch [31/120    avg_loss:0.273, val_acc:0.908]
Epoch [32/120    avg_loss:0.192, val_acc:0.886]
Epoch [33/120    avg_loss:0.179, val_acc:0.874]
Epoch [34/120    avg_loss:0.198, val_acc:0.906]
Epoch [35/120    avg_loss:0.156, val_acc:0.921]
Epoch [36/120    avg_loss:0.152, val_acc:0.916]
Epoch [37/120    avg_loss:0.130, val_acc:0.930]
Epoch [38/120    avg_loss:0.127, val_acc:0.918]
Epoch [39/120    avg_loss:0.124, val_acc:0.931]
Epoch [40/120    avg_loss:0.110, val_acc:0.934]
Epoch [41/120    avg_loss:0.111, val_acc:0.923]
Epoch [42/120    avg_loss:0.101, val_acc:0.932]
Epoch [43/120    avg_loss:0.083, val_acc:0.929]
Epoch [44/120    avg_loss:0.098, val_acc:0.912]
Epoch [45/120    avg_loss:0.095, val_acc:0.915]
Epoch [46/120    avg_loss:0.083, val_acc:0.946]
Epoch [47/120    avg_loss:0.072, val_acc:0.950]
Epoch [48/120    avg_loss:0.072, val_acc:0.943]
Epoch [49/120    avg_loss:0.075, val_acc:0.940]
Epoch [50/120    avg_loss:0.064, val_acc:0.943]
Epoch [51/120    avg_loss:0.059, val_acc:0.950]
Epoch [52/120    avg_loss:0.059, val_acc:0.926]
Epoch [53/120    avg_loss:0.051, val_acc:0.953]
Epoch [54/120    avg_loss:0.043, val_acc:0.946]
Epoch [55/120    avg_loss:0.057, val_acc:0.952]
Epoch [56/120    avg_loss:0.071, val_acc:0.944]
Epoch [57/120    avg_loss:0.056, val_acc:0.949]
Epoch [58/120    avg_loss:0.052, val_acc:0.953]
Epoch [59/120    avg_loss:0.052, val_acc:0.959]
Epoch [60/120    avg_loss:0.053, val_acc:0.950]
Epoch [61/120    avg_loss:0.067, val_acc:0.943]
Epoch [62/120    avg_loss:0.045, val_acc:0.946]
Epoch [63/120    avg_loss:0.041, val_acc:0.960]
Epoch [64/120    avg_loss:0.039, val_acc:0.959]
Epoch [65/120    avg_loss:0.040, val_acc:0.967]
Epoch [66/120    avg_loss:0.036, val_acc:0.955]
Epoch [67/120    avg_loss:0.034, val_acc:0.957]
Epoch [68/120    avg_loss:0.028, val_acc:0.961]
Epoch [69/120    avg_loss:0.033, val_acc:0.957]
Epoch [70/120    avg_loss:0.052, val_acc:0.955]
Epoch [71/120    avg_loss:0.035, val_acc:0.967]
Epoch [72/120    avg_loss:0.032, val_acc:0.964]
Epoch [73/120    avg_loss:0.032, val_acc:0.959]
Epoch [74/120    avg_loss:0.052, val_acc:0.952]
Epoch [75/120    avg_loss:0.039, val_acc:0.954]
Epoch [76/120    avg_loss:0.042, val_acc:0.955]
Epoch [77/120    avg_loss:0.059, val_acc:0.959]
Epoch [78/120    avg_loss:0.037, val_acc:0.959]
Epoch [79/120    avg_loss:0.047, val_acc:0.950]
Epoch [80/120    avg_loss:0.033, val_acc:0.966]
Epoch [81/120    avg_loss:0.032, val_acc:0.967]
Epoch [82/120    avg_loss:0.038, val_acc:0.947]
Epoch [83/120    avg_loss:0.037, val_acc:0.958]
Epoch [84/120    avg_loss:0.029, val_acc:0.971]
Epoch [85/120    avg_loss:0.020, val_acc:0.975]
Epoch [86/120    avg_loss:0.018, val_acc:0.971]
Epoch [87/120    avg_loss:0.017, val_acc:0.970]
Epoch [88/120    avg_loss:0.042, val_acc:0.964]
Epoch [89/120    avg_loss:0.050, val_acc:0.948]
Epoch [90/120    avg_loss:0.033, val_acc:0.955]
Epoch [91/120    avg_loss:0.042, val_acc:0.959]
Epoch [92/120    avg_loss:0.022, val_acc:0.964]
Epoch [93/120    avg_loss:0.020, val_acc:0.968]
Epoch [94/120    avg_loss:0.023, val_acc:0.954]
Epoch [95/120    avg_loss:0.034, val_acc:0.946]
Epoch [96/120    avg_loss:0.028, val_acc:0.961]
Epoch [97/120    avg_loss:0.031, val_acc:0.966]
Epoch [98/120    avg_loss:0.027, val_acc:0.942]
Epoch [99/120    avg_loss:0.030, val_acc:0.965]
Epoch [100/120    avg_loss:0.015, val_acc:0.971]
Epoch [101/120    avg_loss:0.014, val_acc:0.972]
Epoch [102/120    avg_loss:0.013, val_acc:0.969]
Epoch [103/120    avg_loss:0.013, val_acc:0.973]
Epoch [104/120    avg_loss:0.012, val_acc:0.973]
Epoch [105/120    avg_loss:0.012, val_acc:0.974]
Epoch [106/120    avg_loss:0.013, val_acc:0.972]
Epoch [107/120    avg_loss:0.013, val_acc:0.973]
Epoch [108/120    avg_loss:0.010, val_acc:0.973]
Epoch [109/120    avg_loss:0.014, val_acc:0.975]
Epoch [110/120    avg_loss:0.013, val_acc:0.973]
Epoch [111/120    avg_loss:0.012, val_acc:0.974]
Epoch [112/120    avg_loss:0.012, val_acc:0.974]
Epoch [113/120    avg_loss:0.013, val_acc:0.973]
Epoch [114/120    avg_loss:0.008, val_acc:0.974]
Epoch [115/120    avg_loss:0.010, val_acc:0.976]
Epoch [116/120    avg_loss:0.012, val_acc:0.974]
Epoch [117/120    avg_loss:0.015, val_acc:0.975]
Epoch [118/120    avg_loss:0.010, val_acc:0.976]
Epoch [119/120    avg_loss:0.009, val_acc:0.974]
Epoch [120/120    avg_loss:0.011, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1245    0    4    0    1    0    0    0    2   31    0    0
     0    2    0]
 [   0    0    0  712    3    0    0    0    0   10    3    9   10    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    1    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    9    0    0    0    0    0    0  421    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    2    0    0    0  858    9    0    0
     0    2    0]
 [   0    0    1    0    0    0    0    0    0    0    7 2180   20    1
     0    1    0]
 [   0    0    0    5    1    0    0    0    0    2    1    0  521    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    1    0    0
  1129    7    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    16  321    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.91869918699187

F1 scores:
[       nan 0.85057471 0.98224852 0.9726776  0.98156682 0.99308756
 0.98722765 1.         0.98942421 0.66666667 0.98057143 0.98176086
 0.95948435 0.99730458 0.98775153 0.94134897 0.98224852]

Kappa:
0.9762705326477653
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5dceb00860>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.753, val_acc:0.414]
Epoch [2/120    avg_loss:2.413, val_acc:0.506]
Epoch [3/120    avg_loss:2.179, val_acc:0.501]
Epoch [4/120    avg_loss:2.014, val_acc:0.541]
Epoch [5/120    avg_loss:1.872, val_acc:0.525]
Epoch [6/120    avg_loss:1.751, val_acc:0.580]
Epoch [7/120    avg_loss:1.633, val_acc:0.608]
Epoch [8/120    avg_loss:1.538, val_acc:0.602]
Epoch [9/120    avg_loss:1.389, val_acc:0.644]
Epoch [10/120    avg_loss:1.213, val_acc:0.676]
Epoch [11/120    avg_loss:1.181, val_acc:0.684]
Epoch [12/120    avg_loss:1.056, val_acc:0.700]
Epoch [13/120    avg_loss:0.999, val_acc:0.734]
Epoch [14/120    avg_loss:0.837, val_acc:0.778]
Epoch [15/120    avg_loss:0.792, val_acc:0.760]
Epoch [16/120    avg_loss:0.708, val_acc:0.814]
Epoch [17/120    avg_loss:0.605, val_acc:0.815]
Epoch [18/120    avg_loss:0.568, val_acc:0.824]
Epoch [19/120    avg_loss:0.493, val_acc:0.853]
Epoch [20/120    avg_loss:0.465, val_acc:0.849]
Epoch [21/120    avg_loss:0.377, val_acc:0.876]
Epoch [22/120    avg_loss:0.355, val_acc:0.863]
Epoch [23/120    avg_loss:0.306, val_acc:0.849]
Epoch [24/120    avg_loss:0.303, val_acc:0.873]
Epoch [25/120    avg_loss:0.284, val_acc:0.891]
Epoch [26/120    avg_loss:0.260, val_acc:0.897]
Epoch [27/120    avg_loss:0.239, val_acc:0.886]
Epoch [28/120    avg_loss:0.220, val_acc:0.906]
Epoch [29/120    avg_loss:0.177, val_acc:0.925]
Epoch [30/120    avg_loss:0.147, val_acc:0.910]
Epoch [31/120    avg_loss:0.178, val_acc:0.900]
Epoch [32/120    avg_loss:0.174, val_acc:0.915]
Epoch [33/120    avg_loss:0.137, val_acc:0.914]
Epoch [34/120    avg_loss:0.138, val_acc:0.926]
Epoch [35/120    avg_loss:0.112, val_acc:0.926]
Epoch [36/120    avg_loss:0.170, val_acc:0.925]
Epoch [37/120    avg_loss:0.142, val_acc:0.912]
Epoch [38/120    avg_loss:0.118, val_acc:0.934]
Epoch [39/120    avg_loss:0.130, val_acc:0.925]
Epoch [40/120    avg_loss:0.133, val_acc:0.919]
Epoch [41/120    avg_loss:0.145, val_acc:0.910]
Epoch [42/120    avg_loss:0.122, val_acc:0.924]
Epoch [43/120    avg_loss:0.104, val_acc:0.933]
Epoch [44/120    avg_loss:0.082, val_acc:0.941]
Epoch [45/120    avg_loss:0.071, val_acc:0.935]
Epoch [46/120    avg_loss:0.172, val_acc:0.873]
Epoch [47/120    avg_loss:0.114, val_acc:0.931]
Epoch [48/120    avg_loss:0.091, val_acc:0.934]
Epoch [49/120    avg_loss:0.106, val_acc:0.935]
Epoch [50/120    avg_loss:0.091, val_acc:0.943]
Epoch [51/120    avg_loss:0.062, val_acc:0.946]
Epoch [52/120    avg_loss:0.051, val_acc:0.944]
Epoch [53/120    avg_loss:0.054, val_acc:0.941]
Epoch [54/120    avg_loss:0.051, val_acc:0.946]
Epoch [55/120    avg_loss:0.054, val_acc:0.950]
Epoch [56/120    avg_loss:0.046, val_acc:0.950]
Epoch [57/120    avg_loss:0.044, val_acc:0.954]
Epoch [58/120    avg_loss:0.035, val_acc:0.950]
Epoch [59/120    avg_loss:0.040, val_acc:0.958]
Epoch [60/120    avg_loss:0.043, val_acc:0.951]
Epoch [61/120    avg_loss:0.041, val_acc:0.961]
Epoch [62/120    avg_loss:0.041, val_acc:0.953]
Epoch [63/120    avg_loss:0.047, val_acc:0.930]
Epoch [64/120    avg_loss:0.067, val_acc:0.933]
Epoch [65/120    avg_loss:0.084, val_acc:0.945]
Epoch [66/120    avg_loss:0.065, val_acc:0.952]
Epoch [67/120    avg_loss:0.033, val_acc:0.967]
Epoch [68/120    avg_loss:0.032, val_acc:0.966]
Epoch [69/120    avg_loss:0.030, val_acc:0.961]
Epoch [70/120    avg_loss:0.028, val_acc:0.964]
Epoch [71/120    avg_loss:0.028, val_acc:0.960]
Epoch [72/120    avg_loss:0.028, val_acc:0.967]
Epoch [73/120    avg_loss:0.027, val_acc:0.967]
Epoch [74/120    avg_loss:0.037, val_acc:0.964]
Epoch [75/120    avg_loss:0.028, val_acc:0.963]
Epoch [76/120    avg_loss:0.027, val_acc:0.968]
Epoch [77/120    avg_loss:0.029, val_acc:0.956]
Epoch [78/120    avg_loss:0.022, val_acc:0.961]
Epoch [79/120    avg_loss:0.018, val_acc:0.964]
Epoch [80/120    avg_loss:0.021, val_acc:0.969]
Epoch [81/120    avg_loss:0.017, val_acc:0.964]
Epoch [82/120    avg_loss:0.018, val_acc:0.959]
Epoch [83/120    avg_loss:0.018, val_acc:0.967]
Epoch [84/120    avg_loss:0.027, val_acc:0.964]
Epoch [85/120    avg_loss:0.024, val_acc:0.960]
Epoch [86/120    avg_loss:0.020, val_acc:0.967]
Epoch [87/120    avg_loss:0.016, val_acc:0.969]
Epoch [88/120    avg_loss:0.016, val_acc:0.965]
Epoch [89/120    avg_loss:0.016, val_acc:0.965]
Epoch [90/120    avg_loss:0.014, val_acc:0.970]
Epoch [91/120    avg_loss:0.012, val_acc:0.970]
Epoch [92/120    avg_loss:0.012, val_acc:0.974]
Epoch [93/120    avg_loss:0.016, val_acc:0.967]
Epoch [94/120    avg_loss:0.015, val_acc:0.974]
Epoch [95/120    avg_loss:0.011, val_acc:0.971]
Epoch [96/120    avg_loss:0.014, val_acc:0.975]
Epoch [97/120    avg_loss:0.014, val_acc:0.972]
Epoch [98/120    avg_loss:0.010, val_acc:0.971]
Epoch [99/120    avg_loss:0.012, val_acc:0.971]
Epoch [100/120    avg_loss:0.034, val_acc:0.964]
Epoch [101/120    avg_loss:0.019, val_acc:0.975]
Epoch [102/120    avg_loss:0.015, val_acc:0.971]
Epoch [103/120    avg_loss:0.011, val_acc:0.969]
Epoch [104/120    avg_loss:0.009, val_acc:0.974]
Epoch [105/120    avg_loss:0.009, val_acc:0.974]
Epoch [106/120    avg_loss:0.010, val_acc:0.978]
Epoch [107/120    avg_loss:0.009, val_acc:0.975]
Epoch [108/120    avg_loss:0.010, val_acc:0.973]
Epoch [109/120    avg_loss:0.014, val_acc:0.968]
Epoch [110/120    avg_loss:0.010, val_acc:0.967]
Epoch [111/120    avg_loss:0.007, val_acc:0.973]
Epoch [112/120    avg_loss:0.009, val_acc:0.976]
Epoch [113/120    avg_loss:0.007, val_acc:0.973]
Epoch [114/120    avg_loss:0.008, val_acc:0.972]
Epoch [115/120    avg_loss:0.013, val_acc:0.966]
Epoch [116/120    avg_loss:0.012, val_acc:0.970]
Epoch [117/120    avg_loss:0.012, val_acc:0.965]
Epoch [118/120    avg_loss:0.009, val_acc:0.972]
Epoch [119/120    avg_loss:0.008, val_acc:0.979]
Epoch [120/120    avg_loss:0.011, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    3    0    1    0
     0    0    0]
 [   0    0 1240    0    0    0    0    0    0    2   11   30    0    0
     0    2    0]
 [   0    0    0  729    3    2    0    0    0    4    1    1    7    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    5    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    2    0
     0    1    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    4  836   28    2    0
     0    0    0]
 [   0    0    5    0    0    2    0    0    0    3    8 2176   16    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    0    4  524    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    4    3    0    0    0
  1126    6    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    14  317    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.72357723577235

F1 scores:
[       nan 0.94871795 0.97830375 0.98580122 0.99300699 0.98731257
 0.98568199 0.90909091 0.99649942 0.65454545 0.96257916 0.97797753
 0.96146789 1.         0.98728628 0.93925926 0.96969697]

Kappa:
0.9740418612823598
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe850860780>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.776, val_acc:0.325]
Epoch [2/120    avg_loss:2.441, val_acc:0.384]
Epoch [3/120    avg_loss:2.225, val_acc:0.471]
Epoch [4/120    avg_loss:2.041, val_acc:0.478]
Epoch [5/120    avg_loss:1.918, val_acc:0.493]
Epoch [6/120    avg_loss:1.831, val_acc:0.535]
Epoch [7/120    avg_loss:1.671, val_acc:0.553]
Epoch [8/120    avg_loss:1.636, val_acc:0.579]
Epoch [9/120    avg_loss:1.487, val_acc:0.591]
Epoch [10/120    avg_loss:1.377, val_acc:0.608]
Epoch [11/120    avg_loss:1.243, val_acc:0.628]
Epoch [12/120    avg_loss:1.151, val_acc:0.642]
Epoch [13/120    avg_loss:1.016, val_acc:0.671]
Epoch [14/120    avg_loss:0.893, val_acc:0.681]
Epoch [15/120    avg_loss:0.793, val_acc:0.709]
Epoch [16/120    avg_loss:0.712, val_acc:0.748]
Epoch [17/120    avg_loss:0.681, val_acc:0.747]
Epoch [18/120    avg_loss:0.571, val_acc:0.805]
Epoch [19/120    avg_loss:0.568, val_acc:0.814]
Epoch [20/120    avg_loss:0.542, val_acc:0.830]
Epoch [21/120    avg_loss:0.476, val_acc:0.816]
Epoch [22/120    avg_loss:0.426, val_acc:0.821]
Epoch [23/120    avg_loss:0.351, val_acc:0.849]
Epoch [24/120    avg_loss:0.317, val_acc:0.843]
Epoch [25/120    avg_loss:0.325, val_acc:0.855]
Epoch [26/120    avg_loss:0.276, val_acc:0.878]
Epoch [27/120    avg_loss:0.267, val_acc:0.890]
Epoch [28/120    avg_loss:0.257, val_acc:0.869]
Epoch [29/120    avg_loss:0.264, val_acc:0.884]
Epoch [30/120    avg_loss:0.237, val_acc:0.886]
Epoch [31/120    avg_loss:0.282, val_acc:0.791]
Epoch [32/120    avg_loss:0.381, val_acc:0.886]
Epoch [33/120    avg_loss:0.264, val_acc:0.905]
Epoch [34/120    avg_loss:0.166, val_acc:0.909]
Epoch [35/120    avg_loss:0.198, val_acc:0.895]
Epoch [36/120    avg_loss:0.192, val_acc:0.894]
Epoch [37/120    avg_loss:0.162, val_acc:0.916]
Epoch [38/120    avg_loss:0.153, val_acc:0.919]
Epoch [39/120    avg_loss:0.127, val_acc:0.933]
Epoch [40/120    avg_loss:0.123, val_acc:0.925]
Epoch [41/120    avg_loss:0.148, val_acc:0.903]
Epoch [42/120    avg_loss:0.151, val_acc:0.930]
Epoch [43/120    avg_loss:0.122, val_acc:0.931]
Epoch [44/120    avg_loss:0.122, val_acc:0.938]
Epoch [45/120    avg_loss:0.084, val_acc:0.946]
Epoch [46/120    avg_loss:0.074, val_acc:0.934]
Epoch [47/120    avg_loss:0.075, val_acc:0.941]
Epoch [48/120    avg_loss:0.072, val_acc:0.929]
Epoch [49/120    avg_loss:0.093, val_acc:0.930]
Epoch [50/120    avg_loss:0.076, val_acc:0.938]
Epoch [51/120    avg_loss:0.069, val_acc:0.938]
Epoch [52/120    avg_loss:0.073, val_acc:0.932]
Epoch [53/120    avg_loss:0.068, val_acc:0.955]
Epoch [54/120    avg_loss:0.063, val_acc:0.952]
Epoch [55/120    avg_loss:0.077, val_acc:0.953]
Epoch [56/120    avg_loss:0.083, val_acc:0.951]
Epoch [57/120    avg_loss:0.074, val_acc:0.945]
Epoch [58/120    avg_loss:0.074, val_acc:0.929]
Epoch [59/120    avg_loss:0.138, val_acc:0.915]
Epoch [60/120    avg_loss:0.292, val_acc:0.917]
Epoch [61/120    avg_loss:0.166, val_acc:0.903]
Epoch [62/120    avg_loss:0.133, val_acc:0.927]
Epoch [63/120    avg_loss:0.094, val_acc:0.932]
Epoch [64/120    avg_loss:0.082, val_acc:0.940]
Epoch [65/120    avg_loss:0.084, val_acc:0.933]
Epoch [66/120    avg_loss:0.077, val_acc:0.933]
Epoch [67/120    avg_loss:0.062, val_acc:0.955]
Epoch [68/120    avg_loss:0.042, val_acc:0.954]
Epoch [69/120    avg_loss:0.046, val_acc:0.955]
Epoch [70/120    avg_loss:0.041, val_acc:0.957]
Epoch [71/120    avg_loss:0.039, val_acc:0.960]
Epoch [72/120    avg_loss:0.032, val_acc:0.960]
Epoch [73/120    avg_loss:0.038, val_acc:0.961]
Epoch [74/120    avg_loss:0.037, val_acc:0.958]
Epoch [75/120    avg_loss:0.035, val_acc:0.959]
Epoch [76/120    avg_loss:0.041, val_acc:0.963]
Epoch [77/120    avg_loss:0.035, val_acc:0.964]
Epoch [78/120    avg_loss:0.034, val_acc:0.965]
Epoch [79/120    avg_loss:0.046, val_acc:0.966]
Epoch [80/120    avg_loss:0.046, val_acc:0.965]
Epoch [81/120    avg_loss:0.028, val_acc:0.961]
Epoch [82/120    avg_loss:0.033, val_acc:0.963]
Epoch [83/120    avg_loss:0.036, val_acc:0.966]
Epoch [84/120    avg_loss:0.038, val_acc:0.966]
Epoch [85/120    avg_loss:0.034, val_acc:0.966]
Epoch [86/120    avg_loss:0.032, val_acc:0.963]
Epoch [87/120    avg_loss:0.029, val_acc:0.963]
Epoch [88/120    avg_loss:0.029, val_acc:0.963]
Epoch [89/120    avg_loss:0.033, val_acc:0.965]
Epoch [90/120    avg_loss:0.038, val_acc:0.965]
Epoch [91/120    avg_loss:0.035, val_acc:0.965]
Epoch [92/120    avg_loss:0.034, val_acc:0.965]
Epoch [93/120    avg_loss:0.029, val_acc:0.965]
Epoch [94/120    avg_loss:0.034, val_acc:0.966]
Epoch [95/120    avg_loss:0.029, val_acc:0.964]
Epoch [96/120    avg_loss:0.040, val_acc:0.963]
Epoch [97/120    avg_loss:0.027, val_acc:0.966]
Epoch [98/120    avg_loss:0.027, val_acc:0.967]
Epoch [99/120    avg_loss:0.028, val_acc:0.967]
Epoch [100/120    avg_loss:0.030, val_acc:0.966]
Epoch [101/120    avg_loss:0.026, val_acc:0.968]
Epoch [102/120    avg_loss:0.029, val_acc:0.966]
Epoch [103/120    avg_loss:0.027, val_acc:0.965]
Epoch [104/120    avg_loss:0.028, val_acc:0.966]
Epoch [105/120    avg_loss:0.036, val_acc:0.968]
Epoch [106/120    avg_loss:0.027, val_acc:0.964]
Epoch [107/120    avg_loss:0.025, val_acc:0.968]
Epoch [108/120    avg_loss:0.027, val_acc:0.971]
Epoch [109/120    avg_loss:0.028, val_acc:0.971]
Epoch [110/120    avg_loss:0.028, val_acc:0.967]
Epoch [111/120    avg_loss:0.027, val_acc:0.970]
Epoch [112/120    avg_loss:0.026, val_acc:0.966]
Epoch [113/120    avg_loss:0.026, val_acc:0.971]
Epoch [114/120    avg_loss:0.033, val_acc:0.970]
Epoch [115/120    avg_loss:0.033, val_acc:0.968]
Epoch [116/120    avg_loss:0.029, val_acc:0.969]
Epoch [117/120    avg_loss:0.024, val_acc:0.972]
Epoch [118/120    avg_loss:0.030, val_acc:0.969]
Epoch [119/120    avg_loss:0.026, val_acc:0.969]
Epoch [120/120    avg_loss:0.024, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    1    2    0    0
     0    0    0]
 [   0    0 1223    0    1    0    1    0    0    0    9   42    9    0
     0    0    0]
 [   0    0    0  692    7    0    0    0    0    4    3   10   28    3
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    5    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  425    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    7    0    0    2    0    0    0    0  850   13    0    1
     0    2    0]
 [   0    0   17    0    0    2    1    0    1    0   35 2138   14    0
     0    2    0]
 [   0    0    0    3    0    0    0    0    0    0    0    4  523    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    0    0    0
  1130    7    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    26  321    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.91056910569105

F1 scores:
[       nan 0.91566265 0.96603476 0.95844875 0.97685185 0.98265896
 0.99619193 0.90909091 0.99299065 0.85       0.95828636 0.96698327
 0.94234234 0.98930481 0.9826087  0.94273128 0.98224852]

Kappa:
0.9647871162277829
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f50420097f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.682, val_acc:0.374]
Epoch [2/120    avg_loss:2.381, val_acc:0.390]
Epoch [3/120    avg_loss:2.193, val_acc:0.440]
Epoch [4/120    avg_loss:2.022, val_acc:0.490]
Epoch [5/120    avg_loss:1.945, val_acc:0.533]
Epoch [6/120    avg_loss:1.803, val_acc:0.567]
Epoch [7/120    avg_loss:1.678, val_acc:0.557]
Epoch [8/120    avg_loss:1.555, val_acc:0.558]
Epoch [9/120    avg_loss:1.460, val_acc:0.580]
Epoch [10/120    avg_loss:1.376, val_acc:0.613]
Epoch [11/120    avg_loss:1.253, val_acc:0.650]
Epoch [12/120    avg_loss:1.120, val_acc:0.672]
Epoch [13/120    avg_loss:1.008, val_acc:0.676]
Epoch [14/120    avg_loss:0.955, val_acc:0.705]
Epoch [15/120    avg_loss:0.868, val_acc:0.703]
Epoch [16/120    avg_loss:0.782, val_acc:0.747]
Epoch [17/120    avg_loss:0.672, val_acc:0.778]
Epoch [18/120    avg_loss:0.642, val_acc:0.776]
Epoch [19/120    avg_loss:0.615, val_acc:0.808]
Epoch [20/120    avg_loss:0.501, val_acc:0.814]
Epoch [21/120    avg_loss:0.415, val_acc:0.835]
Epoch [22/120    avg_loss:0.406, val_acc:0.852]
Epoch [23/120    avg_loss:0.345, val_acc:0.858]
Epoch [24/120    avg_loss:0.317, val_acc:0.850]
Epoch [25/120    avg_loss:0.279, val_acc:0.865]
Epoch [26/120    avg_loss:0.289, val_acc:0.868]
Epoch [27/120    avg_loss:0.321, val_acc:0.863]
Epoch [28/120    avg_loss:0.354, val_acc:0.853]
Epoch [29/120    avg_loss:0.246, val_acc:0.895]
Epoch [30/120    avg_loss:0.211, val_acc:0.864]
Epoch [31/120    avg_loss:0.212, val_acc:0.888]
Epoch [32/120    avg_loss:0.169, val_acc:0.899]
Epoch [33/120    avg_loss:0.154, val_acc:0.914]
Epoch [34/120    avg_loss:0.138, val_acc:0.913]
Epoch [35/120    avg_loss:0.145, val_acc:0.926]
Epoch [36/120    avg_loss:0.123, val_acc:0.930]
Epoch [37/120    avg_loss:0.111, val_acc:0.938]
Epoch [38/120    avg_loss:0.100, val_acc:0.895]
Epoch [39/120    avg_loss:0.120, val_acc:0.937]
Epoch [40/120    avg_loss:0.096, val_acc:0.934]
Epoch [41/120    avg_loss:0.100, val_acc:0.935]
Epoch [42/120    avg_loss:0.101, val_acc:0.935]
Epoch [43/120    avg_loss:0.095, val_acc:0.927]
Epoch [44/120    avg_loss:0.086, val_acc:0.954]
Epoch [45/120    avg_loss:0.083, val_acc:0.954]
Epoch [46/120    avg_loss:0.101, val_acc:0.949]
Epoch [47/120    avg_loss:0.101, val_acc:0.936]
Epoch [48/120    avg_loss:0.127, val_acc:0.938]
Epoch [49/120    avg_loss:0.116, val_acc:0.934]
Epoch [50/120    avg_loss:0.083, val_acc:0.957]
Epoch [51/120    avg_loss:0.080, val_acc:0.961]
Epoch [52/120    avg_loss:0.072, val_acc:0.955]
Epoch [53/120    avg_loss:0.060, val_acc:0.947]
Epoch [54/120    avg_loss:0.070, val_acc:0.947]
Epoch [55/120    avg_loss:0.070, val_acc:0.952]
Epoch [56/120    avg_loss:0.058, val_acc:0.954]
Epoch [57/120    avg_loss:0.055, val_acc:0.966]
Epoch [58/120    avg_loss:0.049, val_acc:0.954]
Epoch [59/120    avg_loss:0.046, val_acc:0.964]
Epoch [60/120    avg_loss:0.043, val_acc:0.961]
Epoch [61/120    avg_loss:0.046, val_acc:0.963]
Epoch [62/120    avg_loss:0.045, val_acc:0.960]
Epoch [63/120    avg_loss:0.035, val_acc:0.966]
Epoch [64/120    avg_loss:0.038, val_acc:0.968]
Epoch [65/120    avg_loss:0.036, val_acc:0.966]
Epoch [66/120    avg_loss:0.030, val_acc:0.966]
Epoch [67/120    avg_loss:0.036, val_acc:0.971]
Epoch [68/120    avg_loss:0.035, val_acc:0.975]
Epoch [69/120    avg_loss:0.035, val_acc:0.975]
Epoch [70/120    avg_loss:0.033, val_acc:0.972]
Epoch [71/120    avg_loss:0.029, val_acc:0.966]
Epoch [72/120    avg_loss:0.024, val_acc:0.971]
Epoch [73/120    avg_loss:0.037, val_acc:0.977]
Epoch [74/120    avg_loss:0.029, val_acc:0.964]
Epoch [75/120    avg_loss:0.039, val_acc:0.967]
Epoch [76/120    avg_loss:0.039, val_acc:0.960]
Epoch [77/120    avg_loss:0.027, val_acc:0.973]
Epoch [78/120    avg_loss:0.028, val_acc:0.976]
Epoch [79/120    avg_loss:0.033, val_acc:0.968]
Epoch [80/120    avg_loss:0.025, val_acc:0.971]
Epoch [81/120    avg_loss:0.026, val_acc:0.975]
Epoch [82/120    avg_loss:0.016, val_acc:0.983]
Epoch [83/120    avg_loss:0.024, val_acc:0.971]
Epoch [84/120    avg_loss:0.022, val_acc:0.983]
Epoch [85/120    avg_loss:0.020, val_acc:0.970]
Epoch [86/120    avg_loss:0.032, val_acc:0.973]
Epoch [87/120    avg_loss:0.029, val_acc:0.974]
Epoch [88/120    avg_loss:0.044, val_acc:0.971]
Epoch [89/120    avg_loss:0.041, val_acc:0.967]
Epoch [90/120    avg_loss:0.043, val_acc:0.970]
Epoch [91/120    avg_loss:0.043, val_acc:0.966]
Epoch [92/120    avg_loss:0.036, val_acc:0.961]
Epoch [93/120    avg_loss:0.036, val_acc:0.972]
Epoch [94/120    avg_loss:0.024, val_acc:0.974]
Epoch [95/120    avg_loss:0.033, val_acc:0.971]
Epoch [96/120    avg_loss:0.039, val_acc:0.971]
Epoch [97/120    avg_loss:0.021, val_acc:0.976]
Epoch [98/120    avg_loss:0.017, val_acc:0.979]
Epoch [99/120    avg_loss:0.013, val_acc:0.978]
Epoch [100/120    avg_loss:0.011, val_acc:0.980]
Epoch [101/120    avg_loss:0.013, val_acc:0.980]
Epoch [102/120    avg_loss:0.012, val_acc:0.982]
Epoch [103/120    avg_loss:0.012, val_acc:0.979]
Epoch [104/120    avg_loss:0.010, val_acc:0.982]
Epoch [105/120    avg_loss:0.009, val_acc:0.982]
Epoch [106/120    avg_loss:0.011, val_acc:0.983]
Epoch [107/120    avg_loss:0.010, val_acc:0.980]
Epoch [108/120    avg_loss:0.011, val_acc:0.983]
Epoch [109/120    avg_loss:0.011, val_acc:0.982]
Epoch [110/120    avg_loss:0.020, val_acc:0.978]
Epoch [111/120    avg_loss:0.011, val_acc:0.980]
Epoch [112/120    avg_loss:0.011, val_acc:0.984]
Epoch [113/120    avg_loss:0.009, val_acc:0.983]
Epoch [114/120    avg_loss:0.011, val_acc:0.983]
Epoch [115/120    avg_loss:0.010, val_acc:0.983]
Epoch [116/120    avg_loss:0.011, val_acc:0.983]
Epoch [117/120    avg_loss:0.009, val_acc:0.984]
Epoch [118/120    avg_loss:0.010, val_acc:0.983]
Epoch [119/120    avg_loss:0.011, val_acc:0.980]
Epoch [120/120    avg_loss:0.012, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1262    0    1    1    3    0    0    0    2   14    1    0
     0    1    0]
 [   0    0    0  708    1    2    3    0    0    3    2    2   26    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    5    0    0    0    0    0    0  425    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    2    3    0    0    2  855    5    1    0
     0    1    0]
 [   0    0    7    0    0    0    6    0    6    0   15 2157   18    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    2  525    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1121   17    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    19  312    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.71273712737127

F1 scores:
[       nan 0.9047619  0.9859375  0.97252747 0.99297424 0.99313501
 0.97542815 0.98039216 0.98722416 0.87804878 0.9738041  0.98268793
 0.94850949 0.99730458 0.98204117 0.92035398 0.97619048]

Kappa:
0.973943895996458
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3a00345828>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.755, val_acc:0.335]
Epoch [2/120    avg_loss:2.392, val_acc:0.318]
Epoch [3/120    avg_loss:2.183, val_acc:0.342]
Epoch [4/120    avg_loss:2.013, val_acc:0.455]
Epoch [5/120    avg_loss:1.932, val_acc:0.540]
Epoch [6/120    avg_loss:1.812, val_acc:0.515]
Epoch [7/120    avg_loss:1.681, val_acc:0.573]
Epoch [8/120    avg_loss:1.579, val_acc:0.604]
Epoch [9/120    avg_loss:1.481, val_acc:0.605]
Epoch [10/120    avg_loss:1.334, val_acc:0.637]
Epoch [11/120    avg_loss:1.287, val_acc:0.653]
Epoch [12/120    avg_loss:1.125, val_acc:0.648]
Epoch [13/120    avg_loss:1.060, val_acc:0.678]
Epoch [14/120    avg_loss:0.961, val_acc:0.691]
Epoch [15/120    avg_loss:0.915, val_acc:0.723]
Epoch [16/120    avg_loss:0.796, val_acc:0.740]
Epoch [17/120    avg_loss:0.707, val_acc:0.795]
Epoch [18/120    avg_loss:0.691, val_acc:0.749]
Epoch [19/120    avg_loss:0.607, val_acc:0.788]
Epoch [20/120    avg_loss:0.549, val_acc:0.824]
Epoch [21/120    avg_loss:0.526, val_acc:0.805]
Epoch [22/120    avg_loss:0.455, val_acc:0.810]
Epoch [23/120    avg_loss:0.420, val_acc:0.776]
Epoch [24/120    avg_loss:0.430, val_acc:0.802]
Epoch [25/120    avg_loss:0.380, val_acc:0.827]
Epoch [26/120    avg_loss:0.402, val_acc:0.830]
Epoch [27/120    avg_loss:0.359, val_acc:0.833]
Epoch [28/120    avg_loss:0.293, val_acc:0.849]
Epoch [29/120    avg_loss:0.263, val_acc:0.857]
Epoch [30/120    avg_loss:0.223, val_acc:0.841]
Epoch [31/120    avg_loss:0.218, val_acc:0.874]
Epoch [32/120    avg_loss:0.208, val_acc:0.893]
Epoch [33/120    avg_loss:0.293, val_acc:0.850]
Epoch [34/120    avg_loss:0.250, val_acc:0.870]
Epoch [35/120    avg_loss:0.208, val_acc:0.876]
Epoch [36/120    avg_loss:0.194, val_acc:0.909]
Epoch [37/120    avg_loss:0.143, val_acc:0.933]
Epoch [38/120    avg_loss:0.124, val_acc:0.903]
Epoch [39/120    avg_loss:0.161, val_acc:0.923]
Epoch [40/120    avg_loss:0.162, val_acc:0.888]
Epoch [41/120    avg_loss:0.197, val_acc:0.897]
Epoch [42/120    avg_loss:0.197, val_acc:0.903]
Epoch [43/120    avg_loss:0.157, val_acc:0.930]
Epoch [44/120    avg_loss:0.118, val_acc:0.930]
Epoch [45/120    avg_loss:0.096, val_acc:0.943]
Epoch [46/120    avg_loss:0.077, val_acc:0.951]
Epoch [47/120    avg_loss:0.112, val_acc:0.934]
Epoch [48/120    avg_loss:0.135, val_acc:0.926]
Epoch [49/120    avg_loss:0.103, val_acc:0.934]
Epoch [50/120    avg_loss:0.096, val_acc:0.932]
Epoch [51/120    avg_loss:0.090, val_acc:0.952]
Epoch [52/120    avg_loss:0.065, val_acc:0.946]
Epoch [53/120    avg_loss:0.057, val_acc:0.954]
Epoch [54/120    avg_loss:0.075, val_acc:0.949]
Epoch [55/120    avg_loss:0.079, val_acc:0.945]
Epoch [56/120    avg_loss:0.063, val_acc:0.943]
Epoch [57/120    avg_loss:0.077, val_acc:0.933]
Epoch [58/120    avg_loss:0.062, val_acc:0.943]
Epoch [59/120    avg_loss:0.075, val_acc:0.939]
Epoch [60/120    avg_loss:0.077, val_acc:0.954]
Epoch [61/120    avg_loss:0.062, val_acc:0.952]
Epoch [62/120    avg_loss:0.059, val_acc:0.940]
Epoch [63/120    avg_loss:0.053, val_acc:0.952]
Epoch [64/120    avg_loss:0.049, val_acc:0.959]
Epoch [65/120    avg_loss:0.038, val_acc:0.961]
Epoch [66/120    avg_loss:0.035, val_acc:0.957]
Epoch [67/120    avg_loss:0.055, val_acc:0.949]
Epoch [68/120    avg_loss:0.039, val_acc:0.959]
Epoch [69/120    avg_loss:0.044, val_acc:0.963]
Epoch [70/120    avg_loss:0.037, val_acc:0.948]
Epoch [71/120    avg_loss:0.038, val_acc:0.962]
Epoch [72/120    avg_loss:0.056, val_acc:0.952]
Epoch [73/120    avg_loss:0.045, val_acc:0.948]
Epoch [74/120    avg_loss:0.034, val_acc:0.963]
Epoch [75/120    avg_loss:0.031, val_acc:0.966]
Epoch [76/120    avg_loss:0.026, val_acc:0.965]
Epoch [77/120    avg_loss:0.031, val_acc:0.966]
Epoch [78/120    avg_loss:0.035, val_acc:0.963]
Epoch [79/120    avg_loss:0.027, val_acc:0.963]
Epoch [80/120    avg_loss:0.030, val_acc:0.965]
Epoch [81/120    avg_loss:0.024, val_acc:0.968]
Epoch [82/120    avg_loss:0.028, val_acc:0.970]
Epoch [83/120    avg_loss:0.048, val_acc:0.965]
Epoch [84/120    avg_loss:0.030, val_acc:0.960]
Epoch [85/120    avg_loss:0.029, val_acc:0.966]
Epoch [86/120    avg_loss:0.046, val_acc:0.962]
Epoch [87/120    avg_loss:0.027, val_acc:0.959]
Epoch [88/120    avg_loss:0.027, val_acc:0.959]
Epoch [89/120    avg_loss:0.024, val_acc:0.976]
Epoch [90/120    avg_loss:0.020, val_acc:0.974]
Epoch [91/120    avg_loss:0.023, val_acc:0.970]
Epoch [92/120    avg_loss:0.024, val_acc:0.972]
Epoch [93/120    avg_loss:0.015, val_acc:0.973]
Epoch [94/120    avg_loss:0.013, val_acc:0.978]
Epoch [95/120    avg_loss:0.015, val_acc:0.970]
Epoch [96/120    avg_loss:0.016, val_acc:0.974]
Epoch [97/120    avg_loss:0.017, val_acc:0.964]
Epoch [98/120    avg_loss:0.024, val_acc:0.963]
Epoch [99/120    avg_loss:0.025, val_acc:0.967]
Epoch [100/120    avg_loss:0.024, val_acc:0.963]
Epoch [101/120    avg_loss:0.040, val_acc:0.963]
Epoch [102/120    avg_loss:0.029, val_acc:0.965]
Epoch [103/120    avg_loss:0.021, val_acc:0.970]
Epoch [104/120    avg_loss:0.020, val_acc:0.973]
Epoch [105/120    avg_loss:0.017, val_acc:0.964]
Epoch [106/120    avg_loss:0.019, val_acc:0.967]
Epoch [107/120    avg_loss:0.016, val_acc:0.966]
Epoch [108/120    avg_loss:0.014, val_acc:0.973]
Epoch [109/120    avg_loss:0.013, val_acc:0.975]
Epoch [110/120    avg_loss:0.009, val_acc:0.976]
Epoch [111/120    avg_loss:0.012, val_acc:0.978]
Epoch [112/120    avg_loss:0.011, val_acc:0.977]
Epoch [113/120    avg_loss:0.009, val_acc:0.977]
Epoch [114/120    avg_loss:0.009, val_acc:0.978]
Epoch [115/120    avg_loss:0.009, val_acc:0.977]
Epoch [116/120    avg_loss:0.010, val_acc:0.980]
Epoch [117/120    avg_loss:0.010, val_acc:0.980]
Epoch [118/120    avg_loss:0.009, val_acc:0.978]
Epoch [119/120    avg_loss:0.011, val_acc:0.979]
Epoch [120/120    avg_loss:0.009, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1253    2    0    0    1    0    0    0    4   24    0    1
     0    0    0]
 [   0    0    1  725    1    3    0    0    0    7    3    4    3    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    0    1
     0    0    0]
 [   0    0    0    0    0  428    0    3    0    3    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    6    0    0    0    0    0    0  424    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    2    2    0    0    0  848   16    2    0
     0    0    0]
 [   0    0   13    0    0    1    0    0    0    3    5 2183    4    0
     0    1    0]
 [   0    0    0    6    0    0    0    0    0    0    2    8  515    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1122   16    0]
 [   0    0    0    0    0    0   18    0    0    3    0    0    0    0
    43  283    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.48509485094851

F1 scores:
[       nan 0.90697674 0.98005475 0.97972973 0.99530516 0.98504028
 0.98121713 0.94339623 0.99297424 0.69230769 0.97471264 0.98156475
 0.96986817 0.99462366 0.97311362 0.87211094 0.96969697]

Kappa:
0.9713094101287381
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f71d8b30898>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.741, val_acc:0.345]
Epoch [2/120    avg_loss:2.416, val_acc:0.473]
Epoch [3/120    avg_loss:2.199, val_acc:0.497]
Epoch [4/120    avg_loss:2.033, val_acc:0.528]
Epoch [5/120    avg_loss:1.907, val_acc:0.541]
Epoch [6/120    avg_loss:1.797, val_acc:0.567]
Epoch [7/120    avg_loss:1.666, val_acc:0.580]
Epoch [8/120    avg_loss:1.541, val_acc:0.598]
Epoch [9/120    avg_loss:1.425, val_acc:0.622]
Epoch [10/120    avg_loss:1.308, val_acc:0.654]
Epoch [11/120    avg_loss:1.177, val_acc:0.672]
Epoch [12/120    avg_loss:1.048, val_acc:0.697]
Epoch [13/120    avg_loss:0.993, val_acc:0.679]
Epoch [14/120    avg_loss:0.880, val_acc:0.736]
Epoch [15/120    avg_loss:0.742, val_acc:0.773]
Epoch [16/120    avg_loss:0.781, val_acc:0.765]
Epoch [17/120    avg_loss:0.655, val_acc:0.786]
Epoch [18/120    avg_loss:0.570, val_acc:0.792]
Epoch [19/120    avg_loss:0.553, val_acc:0.826]
Epoch [20/120    avg_loss:0.506, val_acc:0.801]
Epoch [21/120    avg_loss:0.442, val_acc:0.800]
Epoch [22/120    avg_loss:0.426, val_acc:0.843]
Epoch [23/120    avg_loss:0.395, val_acc:0.822]
Epoch [24/120    avg_loss:0.365, val_acc:0.833]
Epoch [25/120    avg_loss:0.362, val_acc:0.837]
Epoch [26/120    avg_loss:0.409, val_acc:0.843]
Epoch [27/120    avg_loss:0.292, val_acc:0.880]
Epoch [28/120    avg_loss:0.246, val_acc:0.887]
Epoch [29/120    avg_loss:0.209, val_acc:0.905]
Epoch [30/120    avg_loss:0.226, val_acc:0.905]
Epoch [31/120    avg_loss:0.184, val_acc:0.914]
Epoch [32/120    avg_loss:0.161, val_acc:0.921]
Epoch [33/120    avg_loss:0.162, val_acc:0.927]
Epoch [34/120    avg_loss:0.155, val_acc:0.929]
Epoch [35/120    avg_loss:0.138, val_acc:0.923]
Epoch [36/120    avg_loss:0.124, val_acc:0.929]
Epoch [37/120    avg_loss:0.133, val_acc:0.920]
Epoch [38/120    avg_loss:0.156, val_acc:0.913]
Epoch [39/120    avg_loss:0.139, val_acc:0.920]
Epoch [40/120    avg_loss:0.159, val_acc:0.903]
Epoch [41/120    avg_loss:0.147, val_acc:0.910]
Epoch [42/120    avg_loss:0.164, val_acc:0.899]
Epoch [43/120    avg_loss:0.385, val_acc:0.829]
Epoch [44/120    avg_loss:0.235, val_acc:0.895]
Epoch [45/120    avg_loss:0.161, val_acc:0.918]
Epoch [46/120    avg_loss:0.139, val_acc:0.926]
Epoch [47/120    avg_loss:0.111, val_acc:0.930]
Epoch [48/120    avg_loss:0.088, val_acc:0.952]
Epoch [49/120    avg_loss:0.103, val_acc:0.935]
Epoch [50/120    avg_loss:0.085, val_acc:0.947]
Epoch [51/120    avg_loss:0.063, val_acc:0.949]
Epoch [52/120    avg_loss:0.055, val_acc:0.954]
Epoch [53/120    avg_loss:0.059, val_acc:0.964]
Epoch [54/120    avg_loss:0.061, val_acc:0.952]
Epoch [55/120    avg_loss:0.052, val_acc:0.945]
Epoch [56/120    avg_loss:0.064, val_acc:0.959]
Epoch [57/120    avg_loss:0.053, val_acc:0.959]
Epoch [58/120    avg_loss:0.064, val_acc:0.957]
Epoch [59/120    avg_loss:0.068, val_acc:0.953]
Epoch [60/120    avg_loss:0.059, val_acc:0.958]
Epoch [61/120    avg_loss:0.061, val_acc:0.959]
Epoch [62/120    avg_loss:0.044, val_acc:0.941]
Epoch [63/120    avg_loss:0.064, val_acc:0.937]
Epoch [64/120    avg_loss:0.060, val_acc:0.953]
Epoch [65/120    avg_loss:0.064, val_acc:0.942]
Epoch [66/120    avg_loss:0.042, val_acc:0.962]
Epoch [67/120    avg_loss:0.029, val_acc:0.968]
Epoch [68/120    avg_loss:0.024, val_acc:0.968]
Epoch [69/120    avg_loss:0.026, val_acc:0.968]
Epoch [70/120    avg_loss:0.026, val_acc:0.970]
Epoch [71/120    avg_loss:0.026, val_acc:0.967]
Epoch [72/120    avg_loss:0.027, val_acc:0.970]
Epoch [73/120    avg_loss:0.026, val_acc:0.971]
Epoch [74/120    avg_loss:0.022, val_acc:0.968]
Epoch [75/120    avg_loss:0.025, val_acc:0.967]
Epoch [76/120    avg_loss:0.021, val_acc:0.967]
Epoch [77/120    avg_loss:0.021, val_acc:0.970]
Epoch [78/120    avg_loss:0.023, val_acc:0.971]
Epoch [79/120    avg_loss:0.023, val_acc:0.971]
Epoch [80/120    avg_loss:0.024, val_acc:0.971]
Epoch [81/120    avg_loss:0.024, val_acc:0.970]
Epoch [82/120    avg_loss:0.022, val_acc:0.970]
Epoch [83/120    avg_loss:0.021, val_acc:0.972]
Epoch [84/120    avg_loss:0.023, val_acc:0.968]
Epoch [85/120    avg_loss:0.026, val_acc:0.968]
Epoch [86/120    avg_loss:0.027, val_acc:0.971]
Epoch [87/120    avg_loss:0.023, val_acc:0.973]
Epoch [88/120    avg_loss:0.026, val_acc:0.968]
Epoch [89/120    avg_loss:0.020, val_acc:0.970]
Epoch [90/120    avg_loss:0.023, val_acc:0.970]
Epoch [91/120    avg_loss:0.019, val_acc:0.970]
Epoch [92/120    avg_loss:0.018, val_acc:0.968]
Epoch [93/120    avg_loss:0.019, val_acc:0.973]
Epoch [94/120    avg_loss:0.020, val_acc:0.972]
Epoch [95/120    avg_loss:0.023, val_acc:0.971]
Epoch [96/120    avg_loss:0.023, val_acc:0.971]
Epoch [97/120    avg_loss:0.022, val_acc:0.971]
Epoch [98/120    avg_loss:0.020, val_acc:0.972]
Epoch [99/120    avg_loss:0.022, val_acc:0.971]
Epoch [100/120    avg_loss:0.023, val_acc:0.974]
Epoch [101/120    avg_loss:0.020, val_acc:0.973]
Epoch [102/120    avg_loss:0.020, val_acc:0.971]
Epoch [103/120    avg_loss:0.022, val_acc:0.970]
Epoch [104/120    avg_loss:0.022, val_acc:0.970]
Epoch [105/120    avg_loss:0.022, val_acc:0.971]
Epoch [106/120    avg_loss:0.021, val_acc:0.971]
Epoch [107/120    avg_loss:0.020, val_acc:0.972]
Epoch [108/120    avg_loss:0.023, val_acc:0.975]
Epoch [109/120    avg_loss:0.020, val_acc:0.971]
Epoch [110/120    avg_loss:0.022, val_acc:0.971]
Epoch [111/120    avg_loss:0.021, val_acc:0.971]
Epoch [112/120    avg_loss:0.019, val_acc:0.971]
Epoch [113/120    avg_loss:0.017, val_acc:0.971]
Epoch [114/120    avg_loss:0.018, val_acc:0.972]
Epoch [115/120    avg_loss:0.018, val_acc:0.971]
Epoch [116/120    avg_loss:0.016, val_acc:0.971]
Epoch [117/120    avg_loss:0.019, val_acc:0.971]
Epoch [118/120    avg_loss:0.022, val_acc:0.971]
Epoch [119/120    avg_loss:0.019, val_acc:0.971]
Epoch [120/120    avg_loss:0.020, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1263    1    0    0    1    0    0    0    7   12    1    0
     0    0    0]
 [   0    0    4  697    4    9    1    0    0    9    3   11    8    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    4    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    8    0    0    2    1    0    0    0  840   20    0    0
     0    4    0]
 [   0    0   10    0    0    2    2    0    0    0   20 2163   10    0
     3    0    0]
 [   0    0    0    0    3    0    0    0    0    0    0    3  522    0
     0    1    5]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1125   14    0]
 [   0    0    1    0    0    1   20    0    0    0    0    0    0    0
    12  313    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.50677506775068

F1 scores:
[       nan 0.98765432 0.98249708 0.96470588 0.98383372 0.97727273
 0.97904192 0.92592593 0.9953271  0.75555556 0.96219931 0.97851165
 0.96577243 0.99459459 0.98684211 0.92194404 0.96511628]

Kappa:
0.9715800971713546
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:11:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff2fdd59860>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.662, val_acc:0.243]
Epoch [2/120    avg_loss:2.350, val_acc:0.398]
Epoch [3/120    avg_loss:2.170, val_acc:0.453]
Epoch [4/120    avg_loss:1.987, val_acc:0.511]
Epoch [5/120    avg_loss:1.824, val_acc:0.589]
Epoch [6/120    avg_loss:1.743, val_acc:0.597]
Epoch [7/120    avg_loss:1.597, val_acc:0.643]
Epoch [8/120    avg_loss:1.483, val_acc:0.666]
Epoch [9/120    avg_loss:1.338, val_acc:0.683]
Epoch [10/120    avg_loss:1.149, val_acc:0.711]
Epoch [11/120    avg_loss:1.055, val_acc:0.709]
Epoch [12/120    avg_loss:1.014, val_acc:0.691]
Epoch [13/120    avg_loss:0.934, val_acc:0.714]
Epoch [14/120    avg_loss:0.812, val_acc:0.741]
Epoch [15/120    avg_loss:0.722, val_acc:0.791]
Epoch [16/120    avg_loss:0.697, val_acc:0.808]
Epoch [17/120    avg_loss:0.613, val_acc:0.808]
Epoch [18/120    avg_loss:0.624, val_acc:0.813]
Epoch [19/120    avg_loss:0.522, val_acc:0.808]
Epoch [20/120    avg_loss:0.473, val_acc:0.800]
Epoch [21/120    avg_loss:0.437, val_acc:0.861]
Epoch [22/120    avg_loss:0.420, val_acc:0.834]
Epoch [23/120    avg_loss:0.358, val_acc:0.835]
Epoch [24/120    avg_loss:0.326, val_acc:0.866]
Epoch [25/120    avg_loss:0.278, val_acc:0.887]
Epoch [26/120    avg_loss:0.248, val_acc:0.893]
Epoch [27/120    avg_loss:0.240, val_acc:0.879]
Epoch [28/120    avg_loss:0.211, val_acc:0.891]
Epoch [29/120    avg_loss:0.186, val_acc:0.923]
Epoch [30/120    avg_loss:0.183, val_acc:0.917]
Epoch [31/120    avg_loss:0.165, val_acc:0.916]
Epoch [32/120    avg_loss:0.163, val_acc:0.918]
Epoch [33/120    avg_loss:0.175, val_acc:0.899]
Epoch [34/120    avg_loss:0.170, val_acc:0.914]
Epoch [35/120    avg_loss:0.177, val_acc:0.889]
Epoch [36/120    avg_loss:0.171, val_acc:0.926]
Epoch [37/120    avg_loss:0.124, val_acc:0.928]
Epoch [38/120    avg_loss:0.105, val_acc:0.940]
Epoch [39/120    avg_loss:0.096, val_acc:0.937]
Epoch [40/120    avg_loss:0.103, val_acc:0.936]
Epoch [41/120    avg_loss:0.122, val_acc:0.921]
Epoch [42/120    avg_loss:0.102, val_acc:0.934]
Epoch [43/120    avg_loss:0.099, val_acc:0.943]
Epoch [44/120    avg_loss:0.082, val_acc:0.947]
Epoch [45/120    avg_loss:0.077, val_acc:0.940]
Epoch [46/120    avg_loss:0.068, val_acc:0.952]
Epoch [47/120    avg_loss:0.073, val_acc:0.940]
Epoch [48/120    avg_loss:0.060, val_acc:0.961]
Epoch [49/120    avg_loss:0.057, val_acc:0.948]
Epoch [50/120    avg_loss:0.064, val_acc:0.954]
Epoch [51/120    avg_loss:0.072, val_acc:0.939]
Epoch [52/120    avg_loss:0.070, val_acc:0.938]
Epoch [53/120    avg_loss:0.058, val_acc:0.952]
Epoch [54/120    avg_loss:0.058, val_acc:0.947]
Epoch [55/120    avg_loss:0.070, val_acc:0.951]
Epoch [56/120    avg_loss:0.083, val_acc:0.932]
Epoch [57/120    avg_loss:0.121, val_acc:0.935]
Epoch [58/120    avg_loss:0.071, val_acc:0.934]
Epoch [59/120    avg_loss:0.076, val_acc:0.935]
Epoch [60/120    avg_loss:0.099, val_acc:0.947]
Epoch [61/120    avg_loss:0.088, val_acc:0.912]
Epoch [62/120    avg_loss:0.072, val_acc:0.946]
Epoch [63/120    avg_loss:0.053, val_acc:0.950]
Epoch [64/120    avg_loss:0.039, val_acc:0.953]
Epoch [65/120    avg_loss:0.044, val_acc:0.954]
Epoch [66/120    avg_loss:0.043, val_acc:0.957]
Epoch [67/120    avg_loss:0.037, val_acc:0.958]
Epoch [68/120    avg_loss:0.032, val_acc:0.959]
Epoch [69/120    avg_loss:0.035, val_acc:0.959]
Epoch [70/120    avg_loss:0.029, val_acc:0.962]
Epoch [71/120    avg_loss:0.035, val_acc:0.962]
Epoch [72/120    avg_loss:0.032, val_acc:0.964]
Epoch [73/120    avg_loss:0.035, val_acc:0.966]
Epoch [74/120    avg_loss:0.033, val_acc:0.963]
Epoch [75/120    avg_loss:0.034, val_acc:0.965]
Epoch [76/120    avg_loss:0.031, val_acc:0.964]
Epoch [77/120    avg_loss:0.028, val_acc:0.966]
Epoch [78/120    avg_loss:0.030, val_acc:0.968]
Epoch [79/120    avg_loss:0.027, val_acc:0.970]
Epoch [80/120    avg_loss:0.027, val_acc:0.968]
Epoch [81/120    avg_loss:0.027, val_acc:0.965]
Epoch [82/120    avg_loss:0.023, val_acc:0.967]
Epoch [83/120    avg_loss:0.032, val_acc:0.968]
Epoch [84/120    avg_loss:0.029, val_acc:0.968]
Epoch [85/120    avg_loss:0.026, val_acc:0.967]
Epoch [86/120    avg_loss:0.024, val_acc:0.967]
Epoch [87/120    avg_loss:0.029, val_acc:0.966]
Epoch [88/120    avg_loss:0.025, val_acc:0.967]
Epoch [89/120    avg_loss:0.026, val_acc:0.968]
Epoch [90/120    avg_loss:0.021, val_acc:0.967]
Epoch [91/120    avg_loss:0.027, val_acc:0.967]
Epoch [92/120    avg_loss:0.027, val_acc:0.967]
Epoch [93/120    avg_loss:0.028, val_acc:0.967]
Epoch [94/120    avg_loss:0.029, val_acc:0.967]
Epoch [95/120    avg_loss:0.028, val_acc:0.967]
Epoch [96/120    avg_loss:0.027, val_acc:0.967]
Epoch [97/120    avg_loss:0.023, val_acc:0.967]
Epoch [98/120    avg_loss:0.023, val_acc:0.967]
Epoch [99/120    avg_loss:0.027, val_acc:0.967]
Epoch [100/120    avg_loss:0.028, val_acc:0.967]
Epoch [101/120    avg_loss:0.024, val_acc:0.967]
Epoch [102/120    avg_loss:0.025, val_acc:0.968]
Epoch [103/120    avg_loss:0.025, val_acc:0.968]
Epoch [104/120    avg_loss:0.023, val_acc:0.968]
Epoch [105/120    avg_loss:0.026, val_acc:0.968]
Epoch [106/120    avg_loss:0.026, val_acc:0.968]
Epoch [107/120    avg_loss:0.025, val_acc:0.968]
Epoch [108/120    avg_loss:0.022, val_acc:0.968]
Epoch [109/120    avg_loss:0.027, val_acc:0.968]
Epoch [110/120    avg_loss:0.026, val_acc:0.968]
Epoch [111/120    avg_loss:0.022, val_acc:0.968]
Epoch [112/120    avg_loss:0.024, val_acc:0.968]
Epoch [113/120    avg_loss:0.022, val_acc:0.968]
Epoch [114/120    avg_loss:0.021, val_acc:0.968]
Epoch [115/120    avg_loss:0.026, val_acc:0.968]
Epoch [116/120    avg_loss:0.027, val_acc:0.968]
Epoch [117/120    avg_loss:0.025, val_acc:0.968]
Epoch [118/120    avg_loss:0.024, val_acc:0.968]
Epoch [119/120    avg_loss:0.021, val_acc:0.968]
Epoch [120/120    avg_loss:0.026, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1241    1    1    0    1    0    0    0   10   31    0    0
     0    0    0]
 [   0    0    0  711    4    7    1    0    0    5    6    7    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    4    0    1    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   14    0    0    2    0    0    0    0  841   17    0    0
     0    1    0]
 [   0    0   12    0    0    0    2    0    0    1   39 2139   17    0
     0    0    0]
 [   0    0    0    8    0    2    0    0    0    0    6    2  511    0
     1    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1122   16    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    39  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.93224932249322

F1 scores:
[       nan 0.92682927 0.97257053 0.96932515 0.98839907 0.97940503
 0.99314547 0.92592593 0.99649942 0.8372093  0.94441325 0.97050817
 0.95603368 1.         0.97311362 0.91530461 0.97647059]

Kappa:
0.9650324941609653
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f71dbf96828>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.751, val_acc:0.165]
Epoch [2/120    avg_loss:2.443, val_acc:0.368]
Epoch [3/120    avg_loss:2.248, val_acc:0.453]
Epoch [4/120    avg_loss:2.036, val_acc:0.463]
Epoch [5/120    avg_loss:1.893, val_acc:0.558]
Epoch [6/120    avg_loss:1.796, val_acc:0.567]
Epoch [7/120    avg_loss:1.657, val_acc:0.581]
Epoch [8/120    avg_loss:1.548, val_acc:0.602]
Epoch [9/120    avg_loss:1.465, val_acc:0.610]
Epoch [10/120    avg_loss:1.328, val_acc:0.601]
Epoch [11/120    avg_loss:1.250, val_acc:0.623]
Epoch [12/120    avg_loss:1.147, val_acc:0.668]
Epoch [13/120    avg_loss:0.997, val_acc:0.684]
Epoch [14/120    avg_loss:0.922, val_acc:0.727]
Epoch [15/120    avg_loss:0.882, val_acc:0.726]
Epoch [16/120    avg_loss:0.783, val_acc:0.682]
Epoch [17/120    avg_loss:0.731, val_acc:0.786]
Epoch [18/120    avg_loss:0.630, val_acc:0.785]
Epoch [19/120    avg_loss:0.600, val_acc:0.785]
Epoch [20/120    avg_loss:0.560, val_acc:0.801]
Epoch [21/120    avg_loss:0.534, val_acc:0.808]
Epoch [22/120    avg_loss:0.498, val_acc:0.823]
Epoch [23/120    avg_loss:0.456, val_acc:0.806]
Epoch [24/120    avg_loss:0.385, val_acc:0.822]
Epoch [25/120    avg_loss:0.372, val_acc:0.825]
Epoch [26/120    avg_loss:0.399, val_acc:0.806]
Epoch [27/120    avg_loss:0.430, val_acc:0.841]
Epoch [28/120    avg_loss:0.332, val_acc:0.886]
Epoch [29/120    avg_loss:0.339, val_acc:0.858]
Epoch [30/120    avg_loss:0.268, val_acc:0.865]
Epoch [31/120    avg_loss:0.251, val_acc:0.883]
Epoch [32/120    avg_loss:0.259, val_acc:0.873]
Epoch [33/120    avg_loss:0.234, val_acc:0.885]
Epoch [34/120    avg_loss:0.198, val_acc:0.882]
Epoch [35/120    avg_loss:0.197, val_acc:0.905]
Epoch [36/120    avg_loss:0.183, val_acc:0.875]
Epoch [37/120    avg_loss:0.174, val_acc:0.894]
Epoch [38/120    avg_loss:0.160, val_acc:0.923]
Epoch [39/120    avg_loss:0.139, val_acc:0.922]
Epoch [40/120    avg_loss:0.130, val_acc:0.915]
Epoch [41/120    avg_loss:0.138, val_acc:0.886]
Epoch [42/120    avg_loss:0.129, val_acc:0.922]
Epoch [43/120    avg_loss:0.102, val_acc:0.940]
Epoch [44/120    avg_loss:0.090, val_acc:0.927]
Epoch [45/120    avg_loss:0.104, val_acc:0.927]
Epoch [46/120    avg_loss:0.138, val_acc:0.924]
Epoch [47/120    avg_loss:0.093, val_acc:0.940]
Epoch [48/120    avg_loss:0.100, val_acc:0.945]
Epoch [49/120    avg_loss:0.074, val_acc:0.961]
Epoch [50/120    avg_loss:0.065, val_acc:0.951]
Epoch [51/120    avg_loss:0.077, val_acc:0.949]
Epoch [52/120    avg_loss:0.062, val_acc:0.949]
Epoch [53/120    avg_loss:0.066, val_acc:0.946]
Epoch [54/120    avg_loss:0.061, val_acc:0.945]
Epoch [55/120    avg_loss:0.058, val_acc:0.957]
Epoch [56/120    avg_loss:0.064, val_acc:0.925]
Epoch [57/120    avg_loss:0.062, val_acc:0.957]
Epoch [58/120    avg_loss:0.097, val_acc:0.930]
Epoch [59/120    avg_loss:0.086, val_acc:0.946]
Epoch [60/120    avg_loss:0.067, val_acc:0.951]
Epoch [61/120    avg_loss:0.051, val_acc:0.946]
Epoch [62/120    avg_loss:0.058, val_acc:0.949]
Epoch [63/120    avg_loss:0.055, val_acc:0.953]
Epoch [64/120    avg_loss:0.039, val_acc:0.956]
Epoch [65/120    avg_loss:0.038, val_acc:0.958]
Epoch [66/120    avg_loss:0.038, val_acc:0.959]
Epoch [67/120    avg_loss:0.032, val_acc:0.959]
Epoch [68/120    avg_loss:0.033, val_acc:0.958]
Epoch [69/120    avg_loss:0.033, val_acc:0.958]
Epoch [70/120    avg_loss:0.030, val_acc:0.959]
Epoch [71/120    avg_loss:0.030, val_acc:0.963]
Epoch [72/120    avg_loss:0.025, val_acc:0.961]
Epoch [73/120    avg_loss:0.028, val_acc:0.959]
Epoch [74/120    avg_loss:0.031, val_acc:0.961]
Epoch [75/120    avg_loss:0.027, val_acc:0.961]
Epoch [76/120    avg_loss:0.035, val_acc:0.961]
Epoch [77/120    avg_loss:0.028, val_acc:0.959]
Epoch [78/120    avg_loss:0.031, val_acc:0.961]
Epoch [79/120    avg_loss:0.026, val_acc:0.963]
Epoch [80/120    avg_loss:0.026, val_acc:0.961]
Epoch [81/120    avg_loss:0.026, val_acc:0.963]
Epoch [82/120    avg_loss:0.025, val_acc:0.963]
Epoch [83/120    avg_loss:0.029, val_acc:0.964]
Epoch [84/120    avg_loss:0.027, val_acc:0.961]
Epoch [85/120    avg_loss:0.024, val_acc:0.961]
Epoch [86/120    avg_loss:0.031, val_acc:0.960]
Epoch [87/120    avg_loss:0.023, val_acc:0.963]
Epoch [88/120    avg_loss:0.028, val_acc:0.966]
Epoch [89/120    avg_loss:0.023, val_acc:0.964]
Epoch [90/120    avg_loss:0.026, val_acc:0.964]
Epoch [91/120    avg_loss:0.025, val_acc:0.961]
Epoch [92/120    avg_loss:0.024, val_acc:0.967]
Epoch [93/120    avg_loss:0.025, val_acc:0.964]
Epoch [94/120    avg_loss:0.021, val_acc:0.961]
Epoch [95/120    avg_loss:0.023, val_acc:0.964]
Epoch [96/120    avg_loss:0.023, val_acc:0.960]
Epoch [97/120    avg_loss:0.026, val_acc:0.963]
Epoch [98/120    avg_loss:0.022, val_acc:0.964]
Epoch [99/120    avg_loss:0.028, val_acc:0.965]
Epoch [100/120    avg_loss:0.020, val_acc:0.964]
Epoch [101/120    avg_loss:0.024, val_acc:0.963]
Epoch [102/120    avg_loss:0.026, val_acc:0.965]
Epoch [103/120    avg_loss:0.024, val_acc:0.964]
Epoch [104/120    avg_loss:0.027, val_acc:0.965]
Epoch [105/120    avg_loss:0.023, val_acc:0.959]
Epoch [106/120    avg_loss:0.026, val_acc:0.960]
Epoch [107/120    avg_loss:0.023, val_acc:0.964]
Epoch [108/120    avg_loss:0.020, val_acc:0.965]
Epoch [109/120    avg_loss:0.024, val_acc:0.963]
Epoch [110/120    avg_loss:0.022, val_acc:0.964]
Epoch [111/120    avg_loss:0.023, val_acc:0.963]
Epoch [112/120    avg_loss:0.022, val_acc:0.964]
Epoch [113/120    avg_loss:0.025, val_acc:0.967]
Epoch [114/120    avg_loss:0.023, val_acc:0.967]
Epoch [115/120    avg_loss:0.020, val_acc:0.967]
Epoch [116/120    avg_loss:0.025, val_acc:0.964]
Epoch [117/120    avg_loss:0.020, val_acc:0.964]
Epoch [118/120    avg_loss:0.018, val_acc:0.964]
Epoch [119/120    avg_loss:0.025, val_acc:0.966]
Epoch [120/120    avg_loss:0.030, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1241    2    0    0    2    0    0    3    7   30    0    0
     0    0    0]
 [   0    0    0  711    1    0    1    0    0   15    2    3   11    3
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    1    5    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  426    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    2    4    0    0    0  847   15    2    0
     1    1    0]
 [   0    1    4    1    0    3    2    8    0    1   25 2155    8    2
     0    0    0]
 [   0    0    0    3    0    1    0    0    0    0    0    5  516    3
     3    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    2    0    0    1    0    0    0
  1105   27    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    19  316    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.10569105691057

F1 scores:
[       nan 0.93975904 0.97986577 0.97064846 0.99530516 0.97701149
 0.98124531 0.76923077 0.9953271  0.63157895 0.96304719 0.97533379
 0.96089385 0.97883598 0.97313959 0.9132948  0.98224852]

Kappa:
0.9670269367266183
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fea2fea07f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.693, val_acc:0.349]
Epoch [2/120    avg_loss:2.354, val_acc:0.473]
Epoch [3/120    avg_loss:2.189, val_acc:0.500]
Epoch [4/120    avg_loss:2.012, val_acc:0.518]
Epoch [5/120    avg_loss:1.939, val_acc:0.552]
Epoch [6/120    avg_loss:1.813, val_acc:0.571]
Epoch [7/120    avg_loss:1.692, val_acc:0.586]
Epoch [8/120    avg_loss:1.597, val_acc:0.620]
Epoch [9/120    avg_loss:1.475, val_acc:0.620]
Epoch [10/120    avg_loss:1.385, val_acc:0.633]
Epoch [11/120    avg_loss:1.246, val_acc:0.647]
Epoch [12/120    avg_loss:1.097, val_acc:0.667]
Epoch [13/120    avg_loss:0.966, val_acc:0.670]
Epoch [14/120    avg_loss:0.878, val_acc:0.702]
Epoch [15/120    avg_loss:0.795, val_acc:0.684]
Epoch [16/120    avg_loss:0.745, val_acc:0.753]
Epoch [17/120    avg_loss:0.701, val_acc:0.772]
Epoch [18/120    avg_loss:0.651, val_acc:0.792]
Epoch [19/120    avg_loss:0.554, val_acc:0.797]
Epoch [20/120    avg_loss:0.497, val_acc:0.772]
Epoch [21/120    avg_loss:0.526, val_acc:0.816]
Epoch [22/120    avg_loss:0.437, val_acc:0.830]
Epoch [23/120    avg_loss:0.431, val_acc:0.836]
Epoch [24/120    avg_loss:0.356, val_acc:0.836]
Epoch [25/120    avg_loss:0.363, val_acc:0.864]
Epoch [26/120    avg_loss:0.292, val_acc:0.868]
Epoch [27/120    avg_loss:0.287, val_acc:0.835]
Epoch [28/120    avg_loss:0.266, val_acc:0.872]
Epoch [29/120    avg_loss:0.225, val_acc:0.892]
Epoch [30/120    avg_loss:0.205, val_acc:0.915]
Epoch [31/120    avg_loss:0.174, val_acc:0.912]
Epoch [32/120    avg_loss:0.155, val_acc:0.922]
Epoch [33/120    avg_loss:0.175, val_acc:0.865]
Epoch [34/120    avg_loss:0.184, val_acc:0.883]
Epoch [35/120    avg_loss:0.210, val_acc:0.904]
Epoch [36/120    avg_loss:0.176, val_acc:0.912]
Epoch [37/120    avg_loss:0.138, val_acc:0.929]
Epoch [38/120    avg_loss:0.116, val_acc:0.944]
Epoch [39/120    avg_loss:0.129, val_acc:0.950]
Epoch [40/120    avg_loss:0.116, val_acc:0.940]
Epoch [41/120    avg_loss:0.136, val_acc:0.926]
Epoch [42/120    avg_loss:0.106, val_acc:0.947]
Epoch [43/120    avg_loss:0.114, val_acc:0.929]
Epoch [44/120    avg_loss:0.098, val_acc:0.943]
Epoch [45/120    avg_loss:0.078, val_acc:0.951]
Epoch [46/120    avg_loss:0.077, val_acc:0.946]
Epoch [47/120    avg_loss:0.085, val_acc:0.943]
Epoch [48/120    avg_loss:0.096, val_acc:0.953]
Epoch [49/120    avg_loss:0.066, val_acc:0.955]
Epoch [50/120    avg_loss:0.069, val_acc:0.953]
Epoch [51/120    avg_loss:0.081, val_acc:0.949]
Epoch [52/120    avg_loss:0.063, val_acc:0.948]
Epoch [53/120    avg_loss:0.065, val_acc:0.966]
Epoch [54/120    avg_loss:0.052, val_acc:0.967]
Epoch [55/120    avg_loss:0.058, val_acc:0.964]
Epoch [56/120    avg_loss:0.044, val_acc:0.963]
Epoch [57/120    avg_loss:0.052, val_acc:0.961]
Epoch [58/120    avg_loss:0.040, val_acc:0.971]
Epoch [59/120    avg_loss:0.035, val_acc:0.968]
Epoch [60/120    avg_loss:0.044, val_acc:0.970]
Epoch [61/120    avg_loss:0.044, val_acc:0.949]
Epoch [62/120    avg_loss:0.055, val_acc:0.961]
Epoch [63/120    avg_loss:0.052, val_acc:0.968]
Epoch [64/120    avg_loss:0.051, val_acc:0.978]
Epoch [65/120    avg_loss:0.051, val_acc:0.958]
Epoch [66/120    avg_loss:0.049, val_acc:0.966]
Epoch [67/120    avg_loss:0.048, val_acc:0.973]
Epoch [68/120    avg_loss:0.038, val_acc:0.972]
Epoch [69/120    avg_loss:0.036, val_acc:0.976]
Epoch [70/120    avg_loss:0.038, val_acc:0.968]
Epoch [71/120    avg_loss:0.029, val_acc:0.970]
Epoch [72/120    avg_loss:0.023, val_acc:0.975]
Epoch [73/120    avg_loss:0.023, val_acc:0.978]
Epoch [74/120    avg_loss:0.022, val_acc:0.968]
Epoch [75/120    avg_loss:0.022, val_acc:0.979]
Epoch [76/120    avg_loss:0.020, val_acc:0.972]
Epoch [77/120    avg_loss:0.029, val_acc:0.963]
Epoch [78/120    avg_loss:0.025, val_acc:0.973]
Epoch [79/120    avg_loss:0.020, val_acc:0.979]
Epoch [80/120    avg_loss:0.026, val_acc:0.976]
Epoch [81/120    avg_loss:0.030, val_acc:0.974]
Epoch [82/120    avg_loss:0.038, val_acc:0.964]
Epoch [83/120    avg_loss:0.051, val_acc:0.969]
Epoch [84/120    avg_loss:0.029, val_acc:0.969]
Epoch [85/120    avg_loss:0.027, val_acc:0.977]
Epoch [86/120    avg_loss:0.021, val_acc:0.979]
Epoch [87/120    avg_loss:0.015, val_acc:0.980]
Epoch [88/120    avg_loss:0.023, val_acc:0.966]
Epoch [89/120    avg_loss:0.023, val_acc:0.976]
Epoch [90/120    avg_loss:0.020, val_acc:0.972]
Epoch [91/120    avg_loss:0.016, val_acc:0.977]
Epoch [92/120    avg_loss:0.013, val_acc:0.980]
Epoch [93/120    avg_loss:0.014, val_acc:0.976]
Epoch [94/120    avg_loss:0.012, val_acc:0.976]
Epoch [95/120    avg_loss:0.014, val_acc:0.971]
Epoch [96/120    avg_loss:0.016, val_acc:0.969]
Epoch [97/120    avg_loss:0.021, val_acc:0.969]
Epoch [98/120    avg_loss:0.017, val_acc:0.980]
Epoch [99/120    avg_loss:0.018, val_acc:0.985]
Epoch [100/120    avg_loss:0.017, val_acc:0.976]
Epoch [101/120    avg_loss:0.020, val_acc:0.970]
Epoch [102/120    avg_loss:0.020, val_acc:0.982]
Epoch [103/120    avg_loss:0.046, val_acc:0.960]
Epoch [104/120    avg_loss:0.048, val_acc:0.967]
Epoch [105/120    avg_loss:0.069, val_acc:0.950]
Epoch [106/120    avg_loss:0.039, val_acc:0.977]
Epoch [107/120    avg_loss:0.027, val_acc:0.965]
Epoch [108/120    avg_loss:0.058, val_acc:0.969]
Epoch [109/120    avg_loss:0.036, val_acc:0.972]
Epoch [110/120    avg_loss:0.026, val_acc:0.970]
Epoch [111/120    avg_loss:0.033, val_acc:0.977]
Epoch [112/120    avg_loss:0.027, val_acc:0.974]
Epoch [113/120    avg_loss:0.017, val_acc:0.976]
Epoch [114/120    avg_loss:0.012, val_acc:0.977]
Epoch [115/120    avg_loss:0.012, val_acc:0.977]
Epoch [116/120    avg_loss:0.010, val_acc:0.979]
Epoch [117/120    avg_loss:0.011, val_acc:0.980]
Epoch [118/120    avg_loss:0.014, val_acc:0.982]
Epoch [119/120    avg_loss:0.011, val_acc:0.980]
Epoch [120/120    avg_loss:0.010, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1254    0    0    0    0    0    0    2    3   26    0    0
     0    0    0]
 [   0    0    0  711    1   11    0    0    0    2    2   13    7    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    3    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    7    0    0    0    0    0    0  423    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    2    0    0    0    1  848   15    0    0
     0    1    0]
 [   0    0    6    0    0    0    0    0    0    2    7 2182   11    0
     0    2    0]
 [   0    0    1    1    0    0    0    0    0    0    0    2  526    0
     1    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0    0    0    0    5    0    0    0    0
     6  336    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.03794037940379

F1 scores:
[       nan 0.88372093 0.98198904 0.97464016 0.99765808 0.976
 0.99694656 0.94339623 0.99179367 0.73469388 0.97583429 0.98089458
 0.97317299 1.         0.98726394 0.95726496 0.97005988]

Kappa:
0.977626961341584
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff36bbe97f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 59432==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.735, val_acc:0.166]
Epoch [2/120    avg_loss:2.421, val_acc:0.377]
Epoch [3/120    avg_loss:2.243, val_acc:0.497]
Epoch [4/120    avg_loss:2.075, val_acc:0.496]
Epoch [5/120    avg_loss:1.948, val_acc:0.470]
Epoch [6/120    avg_loss:1.818, val_acc:0.495]
Epoch [7/120    avg_loss:1.693, val_acc:0.600]
Epoch [8/120    avg_loss:1.568, val_acc:0.607]
Epoch [9/120    avg_loss:1.487, val_acc:0.604]
Epoch [10/120    avg_loss:1.347, val_acc:0.634]
Epoch [11/120    avg_loss:1.235, val_acc:0.646]
Epoch [12/120    avg_loss:1.158, val_acc:0.659]
Epoch [13/120    avg_loss:1.073, val_acc:0.703]
Epoch [14/120    avg_loss:0.985, val_acc:0.731]
Epoch [15/120    avg_loss:0.875, val_acc:0.693]
Epoch [16/120    avg_loss:0.785, val_acc:0.774]
Epoch [17/120    avg_loss:0.687, val_acc:0.764]
Epoch [18/120    avg_loss:0.764, val_acc:0.765]
Epoch [19/120    avg_loss:0.617, val_acc:0.794]
Epoch [20/120    avg_loss:0.528, val_acc:0.824]
Epoch [21/120    avg_loss:0.491, val_acc:0.836]
Epoch [22/120    avg_loss:0.392, val_acc:0.874]
Epoch [23/120    avg_loss:0.400, val_acc:0.860]
Epoch [24/120    avg_loss:0.338, val_acc:0.870]
Epoch [25/120    avg_loss:0.349, val_acc:0.874]
Epoch [26/120    avg_loss:0.300, val_acc:0.879]
Epoch [27/120    avg_loss:0.278, val_acc:0.877]
Epoch [28/120    avg_loss:0.270, val_acc:0.894]
Epoch [29/120    avg_loss:0.256, val_acc:0.895]
Epoch [30/120    avg_loss:0.237, val_acc:0.881]
Epoch [31/120    avg_loss:0.238, val_acc:0.892]
Epoch [32/120    avg_loss:0.232, val_acc:0.886]
Epoch [33/120    avg_loss:0.228, val_acc:0.903]
Epoch [34/120    avg_loss:0.227, val_acc:0.879]
Epoch [35/120    avg_loss:0.203, val_acc:0.875]
Epoch [36/120    avg_loss:0.172, val_acc:0.899]
Epoch [37/120    avg_loss:0.155, val_acc:0.910]
Epoch [38/120    avg_loss:0.155, val_acc:0.921]
Epoch [39/120    avg_loss:0.134, val_acc:0.925]
Epoch [40/120    avg_loss:0.150, val_acc:0.902]
Epoch [41/120    avg_loss:0.173, val_acc:0.925]
Epoch [42/120    avg_loss:0.127, val_acc:0.924]
Epoch [43/120    avg_loss:0.113, val_acc:0.924]
Epoch [44/120    avg_loss:0.114, val_acc:0.946]
Epoch [45/120    avg_loss:0.127, val_acc:0.900]
Epoch [46/120    avg_loss:0.111, val_acc:0.929]
Epoch [47/120    avg_loss:0.097, val_acc:0.923]
Epoch [48/120    avg_loss:0.088, val_acc:0.938]
Epoch [49/120    avg_loss:0.117, val_acc:0.909]
Epoch [50/120    avg_loss:0.097, val_acc:0.945]
Epoch [51/120    avg_loss:0.075, val_acc:0.942]
Epoch [52/120    avg_loss:0.089, val_acc:0.904]
Epoch [53/120    avg_loss:0.085, val_acc:0.943]
Epoch [54/120    avg_loss:0.072, val_acc:0.942]
Epoch [55/120    avg_loss:0.103, val_acc:0.932]
Epoch [56/120    avg_loss:0.068, val_acc:0.933]
Epoch [57/120    avg_loss:0.050, val_acc:0.953]
Epoch [58/120    avg_loss:0.049, val_acc:0.953]
Epoch [59/120    avg_loss:0.050, val_acc:0.946]
Epoch [60/120    avg_loss:0.067, val_acc:0.946]
Epoch [61/120    avg_loss:0.060, val_acc:0.950]
Epoch [62/120    avg_loss:0.061, val_acc:0.947]
Epoch [63/120    avg_loss:0.046, val_acc:0.940]
Epoch [64/120    avg_loss:0.048, val_acc:0.935]
Epoch [65/120    avg_loss:0.051, val_acc:0.952]
Epoch [66/120    avg_loss:0.043, val_acc:0.957]
Epoch [67/120    avg_loss:0.044, val_acc:0.952]
Epoch [68/120    avg_loss:0.034, val_acc:0.954]
Epoch [69/120    avg_loss:0.042, val_acc:0.948]
Epoch [70/120    avg_loss:0.039, val_acc:0.964]
Epoch [71/120    avg_loss:0.039, val_acc:0.947]
Epoch [72/120    avg_loss:0.047, val_acc:0.948]
Epoch [73/120    avg_loss:0.035, val_acc:0.966]
Epoch [74/120    avg_loss:0.030, val_acc:0.957]
Epoch [75/120    avg_loss:0.038, val_acc:0.940]
Epoch [76/120    avg_loss:0.048, val_acc:0.948]
Epoch [77/120    avg_loss:0.029, val_acc:0.963]
Epoch [78/120    avg_loss:0.026, val_acc:0.968]
Epoch [79/120    avg_loss:0.041, val_acc:0.939]
Epoch [80/120    avg_loss:0.058, val_acc:0.956]
Epoch [81/120    avg_loss:0.037, val_acc:0.966]
Epoch [82/120    avg_loss:0.041, val_acc:0.952]
Epoch [83/120    avg_loss:0.037, val_acc:0.965]
Epoch [84/120    avg_loss:0.032, val_acc:0.964]
Epoch [85/120    avg_loss:0.032, val_acc:0.948]
Epoch [86/120    avg_loss:0.042, val_acc:0.954]
Epoch [87/120    avg_loss:0.030, val_acc:0.971]
Epoch [88/120    avg_loss:0.025, val_acc:0.970]
Epoch [89/120    avg_loss:0.021, val_acc:0.960]
Epoch [90/120    avg_loss:0.034, val_acc:0.966]
Epoch [91/120    avg_loss:0.025, val_acc:0.967]
Epoch [92/120    avg_loss:0.037, val_acc:0.959]
Epoch [93/120    avg_loss:0.055, val_acc:0.959]
Epoch [94/120    avg_loss:0.027, val_acc:0.958]
Epoch [95/120    avg_loss:0.032, val_acc:0.968]
Epoch [96/120    avg_loss:0.023, val_acc:0.968]
Epoch [97/120    avg_loss:0.015, val_acc:0.969]
Epoch [98/120    avg_loss:0.013, val_acc:0.968]
Epoch [99/120    avg_loss:0.016, val_acc:0.967]
Epoch [100/120    avg_loss:0.011, val_acc:0.970]
Epoch [101/120    avg_loss:0.014, val_acc:0.973]
Epoch [102/120    avg_loss:0.011, val_acc:0.972]
Epoch [103/120    avg_loss:0.011, val_acc:0.973]
Epoch [104/120    avg_loss:0.009, val_acc:0.973]
Epoch [105/120    avg_loss:0.010, val_acc:0.973]
Epoch [106/120    avg_loss:0.010, val_acc:0.972]
Epoch [107/120    avg_loss:0.010, val_acc:0.973]
Epoch [108/120    avg_loss:0.010, val_acc:0.973]
Epoch [109/120    avg_loss:0.010, val_acc:0.973]
Epoch [110/120    avg_loss:0.009, val_acc:0.972]
Epoch [111/120    avg_loss:0.009, val_acc:0.972]
Epoch [112/120    avg_loss:0.011, val_acc:0.974]
Epoch [113/120    avg_loss:0.010, val_acc:0.972]
Epoch [114/120    avg_loss:0.010, val_acc:0.972]
Epoch [115/120    avg_loss:0.013, val_acc:0.973]
Epoch [116/120    avg_loss:0.008, val_acc:0.975]
Epoch [117/120    avg_loss:0.010, val_acc:0.975]
Epoch [118/120    avg_loss:0.008, val_acc:0.975]
Epoch [119/120    avg_loss:0.008, val_acc:0.975]
Epoch [120/120    avg_loss:0.009, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    0    0    0    0    0    0    1    3   20    0    0
     0    0    0]
 [   0    0    1  725    2    0    0    0    0    9    2    6    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    1    1    0    0    0  843   20    0    0
     0    1    0]
 [   0    0   12    0    0    2    1    0    0    2   10 2181    2    0
     0    0    0]
 [   0    0    0   10    1    1    0    0    0    0    5    1  512    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    1    0    0    0
  1125   11    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    18  320    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.02710027100271

F1 scores:
[       nan 0.98765432 0.98170494 0.97774781 0.99300699 0.98853211
 0.99017385 0.98039216 0.99883586 0.70833333 0.96952271 0.98265375
 0.97245964 0.99728997 0.98425197 0.94117647 0.9704142 ]

Kappa:
0.9774992545987318
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1933669828>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.722, val_acc:0.361]
Epoch [2/120    avg_loss:2.411, val_acc:0.470]
Epoch [3/120    avg_loss:2.212, val_acc:0.491]
Epoch [4/120    avg_loss:2.068, val_acc:0.498]
Epoch [5/120    avg_loss:1.963, val_acc:0.547]
Epoch [6/120    avg_loss:1.831, val_acc:0.532]
Epoch [7/120    avg_loss:1.738, val_acc:0.562]
Epoch [8/120    avg_loss:1.602, val_acc:0.617]
Epoch [9/120    avg_loss:1.493, val_acc:0.637]
Epoch [10/120    avg_loss:1.446, val_acc:0.607]
Epoch [11/120    avg_loss:1.375, val_acc:0.642]
Epoch [12/120    avg_loss:1.262, val_acc:0.655]
Epoch [13/120    avg_loss:1.126, val_acc:0.707]
Epoch [14/120    avg_loss:1.050, val_acc:0.711]
Epoch [15/120    avg_loss:0.934, val_acc:0.770]
Epoch [16/120    avg_loss:0.863, val_acc:0.782]
Epoch [17/120    avg_loss:0.750, val_acc:0.794]
Epoch [18/120    avg_loss:0.714, val_acc:0.795]
Epoch [19/120    avg_loss:0.646, val_acc:0.811]
Epoch [20/120    avg_loss:0.561, val_acc:0.803]
Epoch [21/120    avg_loss:0.504, val_acc:0.841]
Epoch [22/120    avg_loss:0.490, val_acc:0.858]
Epoch [23/120    avg_loss:0.514, val_acc:0.867]
Epoch [24/120    avg_loss:0.470, val_acc:0.773]
Epoch [25/120    avg_loss:0.453, val_acc:0.836]
Epoch [26/120    avg_loss:0.446, val_acc:0.831]
Epoch [27/120    avg_loss:0.381, val_acc:0.859]
Epoch [28/120    avg_loss:0.310, val_acc:0.882]
Epoch [29/120    avg_loss:0.275, val_acc:0.902]
Epoch [30/120    avg_loss:0.286, val_acc:0.883]
Epoch [31/120    avg_loss:0.262, val_acc:0.898]
Epoch [32/120    avg_loss:0.220, val_acc:0.905]
Epoch [33/120    avg_loss:0.234, val_acc:0.908]
Epoch [34/120    avg_loss:0.186, val_acc:0.915]
Epoch [35/120    avg_loss:0.182, val_acc:0.918]
Epoch [36/120    avg_loss:0.160, val_acc:0.924]
Epoch [37/120    avg_loss:0.153, val_acc:0.920]
Epoch [38/120    avg_loss:0.173, val_acc:0.924]
Epoch [39/120    avg_loss:0.165, val_acc:0.924]
Epoch [40/120    avg_loss:0.117, val_acc:0.923]
Epoch [41/120    avg_loss:0.119, val_acc:0.932]
Epoch [42/120    avg_loss:0.131, val_acc:0.923]
Epoch [43/120    avg_loss:0.122, val_acc:0.924]
Epoch [44/120    avg_loss:0.130, val_acc:0.927]
Epoch [45/120    avg_loss:0.107, val_acc:0.957]
Epoch [46/120    avg_loss:0.088, val_acc:0.933]
Epoch [47/120    avg_loss:0.098, val_acc:0.943]
Epoch [48/120    avg_loss:0.115, val_acc:0.927]
Epoch [49/120    avg_loss:0.123, val_acc:0.944]
Epoch [50/120    avg_loss:0.098, val_acc:0.948]
Epoch [51/120    avg_loss:0.084, val_acc:0.945]
Epoch [52/120    avg_loss:0.090, val_acc:0.947]
Epoch [53/120    avg_loss:0.089, val_acc:0.957]
Epoch [54/120    avg_loss:0.118, val_acc:0.922]
Epoch [55/120    avg_loss:0.078, val_acc:0.964]
Epoch [56/120    avg_loss:0.082, val_acc:0.956]
Epoch [57/120    avg_loss:0.058, val_acc:0.959]
Epoch [58/120    avg_loss:0.055, val_acc:0.955]
Epoch [59/120    avg_loss:0.071, val_acc:0.956]
Epoch [60/120    avg_loss:0.058, val_acc:0.958]
Epoch [61/120    avg_loss:0.054, val_acc:0.958]
Epoch [62/120    avg_loss:0.071, val_acc:0.915]
Epoch [63/120    avg_loss:0.081, val_acc:0.941]
Epoch [64/120    avg_loss:0.074, val_acc:0.964]
Epoch [65/120    avg_loss:0.062, val_acc:0.940]
Epoch [66/120    avg_loss:0.077, val_acc:0.917]
Epoch [67/120    avg_loss:0.075, val_acc:0.957]
Epoch [68/120    avg_loss:0.056, val_acc:0.947]
Epoch [69/120    avg_loss:0.058, val_acc:0.950]
Epoch [70/120    avg_loss:0.075, val_acc:0.958]
Epoch [71/120    avg_loss:0.055, val_acc:0.961]
Epoch [72/120    avg_loss:0.051, val_acc:0.960]
Epoch [73/120    avg_loss:0.039, val_acc:0.969]
Epoch [74/120    avg_loss:0.046, val_acc:0.957]
Epoch [75/120    avg_loss:0.052, val_acc:0.952]
Epoch [76/120    avg_loss:0.050, val_acc:0.928]
Epoch [77/120    avg_loss:0.047, val_acc:0.972]
Epoch [78/120    avg_loss:0.033, val_acc:0.968]
Epoch [79/120    avg_loss:0.040, val_acc:0.965]
Epoch [80/120    avg_loss:0.036, val_acc:0.967]
Epoch [81/120    avg_loss:0.038, val_acc:0.974]
Epoch [82/120    avg_loss:0.029, val_acc:0.964]
Epoch [83/120    avg_loss:0.029, val_acc:0.963]
Epoch [84/120    avg_loss:0.038, val_acc:0.964]
Epoch [85/120    avg_loss:0.050, val_acc:0.959]
Epoch [86/120    avg_loss:0.041, val_acc:0.947]
Epoch [87/120    avg_loss:0.035, val_acc:0.960]
Epoch [88/120    avg_loss:0.050, val_acc:0.951]
Epoch [89/120    avg_loss:0.049, val_acc:0.959]
Epoch [90/120    avg_loss:0.128, val_acc:0.943]
Epoch [91/120    avg_loss:0.099, val_acc:0.968]
Epoch [92/120    avg_loss:0.074, val_acc:0.950]
Epoch [93/120    avg_loss:0.075, val_acc:0.956]
Epoch [94/120    avg_loss:0.057, val_acc:0.968]
Epoch [95/120    avg_loss:0.052, val_acc:0.973]
Epoch [96/120    avg_loss:0.032, val_acc:0.974]
Epoch [97/120    avg_loss:0.032, val_acc:0.975]
Epoch [98/120    avg_loss:0.029, val_acc:0.976]
Epoch [99/120    avg_loss:0.024, val_acc:0.977]
Epoch [100/120    avg_loss:0.022, val_acc:0.978]
Epoch [101/120    avg_loss:0.029, val_acc:0.976]
Epoch [102/120    avg_loss:0.026, val_acc:0.977]
Epoch [103/120    avg_loss:0.025, val_acc:0.975]
Epoch [104/120    avg_loss:0.020, val_acc:0.977]
Epoch [105/120    avg_loss:0.025, val_acc:0.976]
Epoch [106/120    avg_loss:0.024, val_acc:0.977]
Epoch [107/120    avg_loss:0.023, val_acc:0.977]
Epoch [108/120    avg_loss:0.019, val_acc:0.976]
Epoch [109/120    avg_loss:0.020, val_acc:0.977]
Epoch [110/120    avg_loss:0.020, val_acc:0.974]
Epoch [111/120    avg_loss:0.021, val_acc:0.977]
Epoch [112/120    avg_loss:0.026, val_acc:0.978]
Epoch [113/120    avg_loss:0.021, val_acc:0.978]
Epoch [114/120    avg_loss:0.023, val_acc:0.977]
Epoch [115/120    avg_loss:0.021, val_acc:0.978]
Epoch [116/120    avg_loss:0.019, val_acc:0.978]
Epoch [117/120    avg_loss:0.018, val_acc:0.977]
Epoch [118/120    avg_loss:0.019, val_acc:0.978]
Epoch [119/120    avg_loss:0.022, val_acc:0.977]
Epoch [120/120    avg_loss:0.017, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    1    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1244    2    1    0    6    0    0    5    7   20    0    0
     0    0    0]
 [   0    0    5  657   36    4    0    0    0   17    3    6   17    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    5    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    9    0    0    5    0    0    0    0  840   17    0    0
     0    4    0]
 [   0    0    5    2    0    0    1    1    0    0   17 2175    7    2
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    1   15  512    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    1    0    0    0
  1134    2    0]
 [   0    0    0    0    0    1   13    0    0    3    0    0    0    0
    11  319    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.00813008130082

F1 scores:
[       nan 0.93506494 0.97606905 0.93323864 0.92008639 0.97494305
 0.98195489 0.89285714 0.997669   0.5483871  0.9610984  0.97884788
 0.95344507 0.98930481 0.99125874 0.94940476 0.98224852]

Kappa:
0.9658948676510963
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbea9f157f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.694, val_acc:0.225]
Epoch [2/120    avg_loss:2.371, val_acc:0.402]
Epoch [3/120    avg_loss:2.207, val_acc:0.503]
Epoch [4/120    avg_loss:2.055, val_acc:0.553]
Epoch [5/120    avg_loss:1.966, val_acc:0.560]
Epoch [6/120    avg_loss:1.874, val_acc:0.590]
Epoch [7/120    avg_loss:1.752, val_acc:0.609]
Epoch [8/120    avg_loss:1.607, val_acc:0.613]
Epoch [9/120    avg_loss:1.494, val_acc:0.654]
Epoch [10/120    avg_loss:1.332, val_acc:0.666]
Epoch [11/120    avg_loss:1.201, val_acc:0.683]
Epoch [12/120    avg_loss:1.167, val_acc:0.720]
Epoch [13/120    avg_loss:0.997, val_acc:0.757]
Epoch [14/120    avg_loss:0.900, val_acc:0.754]
Epoch [15/120    avg_loss:0.818, val_acc:0.761]
Epoch [16/120    avg_loss:0.756, val_acc:0.784]
Epoch [17/120    avg_loss:0.708, val_acc:0.809]
Epoch [18/120    avg_loss:0.681, val_acc:0.789]
Epoch [19/120    avg_loss:0.635, val_acc:0.805]
Epoch [20/120    avg_loss:0.588, val_acc:0.791]
Epoch [21/120    avg_loss:0.522, val_acc:0.800]
Epoch [22/120    avg_loss:0.473, val_acc:0.860]
Epoch [23/120    avg_loss:0.424, val_acc:0.858]
Epoch [24/120    avg_loss:0.387, val_acc:0.848]
Epoch [25/120    avg_loss:0.302, val_acc:0.860]
Epoch [26/120    avg_loss:0.330, val_acc:0.873]
Epoch [27/120    avg_loss:0.336, val_acc:0.880]
Epoch [28/120    avg_loss:0.324, val_acc:0.893]
Epoch [29/120    avg_loss:0.279, val_acc:0.900]
Epoch [30/120    avg_loss:0.257, val_acc:0.887]
Epoch [31/120    avg_loss:0.216, val_acc:0.907]
Epoch [32/120    avg_loss:0.178, val_acc:0.920]
Epoch [33/120    avg_loss:0.196, val_acc:0.862]
Epoch [34/120    avg_loss:0.169, val_acc:0.896]
Epoch [35/120    avg_loss:0.186, val_acc:0.912]
Epoch [36/120    avg_loss:0.193, val_acc:0.893]
Epoch [37/120    avg_loss:0.207, val_acc:0.843]
Epoch [38/120    avg_loss:0.219, val_acc:0.885]
Epoch [39/120    avg_loss:0.147, val_acc:0.907]
Epoch [40/120    avg_loss:0.120, val_acc:0.924]
Epoch [41/120    avg_loss:0.135, val_acc:0.941]
Epoch [42/120    avg_loss:0.141, val_acc:0.939]
Epoch [43/120    avg_loss:0.120, val_acc:0.925]
Epoch [44/120    avg_loss:0.088, val_acc:0.934]
Epoch [45/120    avg_loss:0.092, val_acc:0.945]
Epoch [46/120    avg_loss:0.107, val_acc:0.937]
Epoch [47/120    avg_loss:0.081, val_acc:0.922]
Epoch [48/120    avg_loss:0.093, val_acc:0.909]
Epoch [49/120    avg_loss:0.115, val_acc:0.958]
Epoch [50/120    avg_loss:0.107, val_acc:0.882]
Epoch [51/120    avg_loss:0.136, val_acc:0.930]
Epoch [52/120    avg_loss:0.118, val_acc:0.866]
Epoch [53/120    avg_loss:0.172, val_acc:0.925]
Epoch [54/120    avg_loss:0.099, val_acc:0.924]
Epoch [55/120    avg_loss:0.091, val_acc:0.943]
Epoch [56/120    avg_loss:0.086, val_acc:0.941]
Epoch [57/120    avg_loss:0.078, val_acc:0.909]
Epoch [58/120    avg_loss:0.061, val_acc:0.941]
Epoch [59/120    avg_loss:0.064, val_acc:0.945]
Epoch [60/120    avg_loss:0.051, val_acc:0.949]
Epoch [61/120    avg_loss:0.052, val_acc:0.958]
Epoch [62/120    avg_loss:0.051, val_acc:0.941]
Epoch [63/120    avg_loss:0.060, val_acc:0.959]
Epoch [64/120    avg_loss:0.063, val_acc:0.939]
Epoch [65/120    avg_loss:0.043, val_acc:0.965]
Epoch [66/120    avg_loss:0.053, val_acc:0.962]
Epoch [67/120    avg_loss:0.062, val_acc:0.951]
Epoch [68/120    avg_loss:0.033, val_acc:0.949]
Epoch [69/120    avg_loss:0.036, val_acc:0.961]
Epoch [70/120    avg_loss:0.034, val_acc:0.955]
Epoch [71/120    avg_loss:0.033, val_acc:0.958]
Epoch [72/120    avg_loss:0.041, val_acc:0.950]
Epoch [73/120    avg_loss:0.048, val_acc:0.953]
Epoch [74/120    avg_loss:0.060, val_acc:0.955]
Epoch [75/120    avg_loss:0.047, val_acc:0.957]
Epoch [76/120    avg_loss:0.036, val_acc:0.970]
Epoch [77/120    avg_loss:0.033, val_acc:0.965]
Epoch [78/120    avg_loss:0.027, val_acc:0.970]
Epoch [79/120    avg_loss:0.022, val_acc:0.971]
Epoch [80/120    avg_loss:0.025, val_acc:0.965]
Epoch [81/120    avg_loss:0.020, val_acc:0.968]
Epoch [82/120    avg_loss:0.030, val_acc:0.951]
Epoch [83/120    avg_loss:0.029, val_acc:0.971]
Epoch [84/120    avg_loss:0.036, val_acc:0.967]
Epoch [85/120    avg_loss:0.027, val_acc:0.955]
Epoch [86/120    avg_loss:0.023, val_acc:0.972]
Epoch [87/120    avg_loss:0.016, val_acc:0.966]
Epoch [88/120    avg_loss:0.022, val_acc:0.972]
Epoch [89/120    avg_loss:0.030, val_acc:0.972]
Epoch [90/120    avg_loss:0.038, val_acc:0.960]
Epoch [91/120    avg_loss:0.031, val_acc:0.959]
Epoch [92/120    avg_loss:0.035, val_acc:0.945]
Epoch [93/120    avg_loss:0.022, val_acc:0.974]
Epoch [94/120    avg_loss:0.029, val_acc:0.954]
Epoch [95/120    avg_loss:0.036, val_acc:0.967]
Epoch [96/120    avg_loss:0.048, val_acc:0.964]
Epoch [97/120    avg_loss:0.034, val_acc:0.963]
Epoch [98/120    avg_loss:0.022, val_acc:0.955]
Epoch [99/120    avg_loss:0.024, val_acc:0.946]
Epoch [100/120    avg_loss:0.025, val_acc:0.970]
Epoch [101/120    avg_loss:0.019, val_acc:0.948]
Epoch [102/120    avg_loss:0.022, val_acc:0.961]
Epoch [103/120    avg_loss:0.018, val_acc:0.964]
Epoch [104/120    avg_loss:0.014, val_acc:0.966]
Epoch [105/120    avg_loss:0.013, val_acc:0.972]
Epoch [106/120    avg_loss:0.013, val_acc:0.968]
Epoch [107/120    avg_loss:0.010, val_acc:0.967]
Epoch [108/120    avg_loss:0.010, val_acc:0.968]
Epoch [109/120    avg_loss:0.012, val_acc:0.968]
Epoch [110/120    avg_loss:0.011, val_acc:0.970]
Epoch [111/120    avg_loss:0.010, val_acc:0.970]
Epoch [112/120    avg_loss:0.010, val_acc:0.973]
Epoch [113/120    avg_loss:0.009, val_acc:0.970]
Epoch [114/120    avg_loss:0.010, val_acc:0.970]
Epoch [115/120    avg_loss:0.014, val_acc:0.973]
Epoch [116/120    avg_loss:0.010, val_acc:0.972]
Epoch [117/120    avg_loss:0.010, val_acc:0.972]
Epoch [118/120    avg_loss:0.009, val_acc:0.972]
Epoch [119/120    avg_loss:0.009, val_acc:0.973]
Epoch [120/120    avg_loss:0.009, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    1    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1256    0    6    0    0    0    0    0    6   16    1    0
     0    0    0]
 [   0    0    3  670    4   12    0    0    0   14    4    4   36    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  421    2    2    0    6    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    5    0    0    0    0  851    8    0    0
     0    5    0]
 [   0    0    4    0    0    0    1    0    0    0   20 2182    2    1
     0    0    0]
 [   0    0    0    0    0    9    0    0    0    1    8    0  514    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    1    2    0    0
  1131    3    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
     6  330    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.59349593495935

F1 scores:
[       nan 0.94871795 0.98317025 0.94565984 0.97706422 0.95248869
 0.98869631 0.96153846 1.         0.63157895 0.96266968 0.98666064
 0.94572217 0.99730458 0.99210526 0.96350365 0.98823529]

Kappa:
0.9725738592418861
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8aea2927f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.722, val_acc:0.233]
Epoch [2/120    avg_loss:2.389, val_acc:0.390]
Epoch [3/120    avg_loss:2.193, val_acc:0.386]
Epoch [4/120    avg_loss:2.094, val_acc:0.459]
Epoch [5/120    avg_loss:1.979, val_acc:0.509]
Epoch [6/120    avg_loss:1.823, val_acc:0.536]
Epoch [7/120    avg_loss:1.719, val_acc:0.617]
Epoch [8/120    avg_loss:1.611, val_acc:0.641]
Epoch [9/120    avg_loss:1.447, val_acc:0.624]
Epoch [10/120    avg_loss:1.348, val_acc:0.679]
Epoch [11/120    avg_loss:1.306, val_acc:0.675]
Epoch [12/120    avg_loss:1.225, val_acc:0.693]
Epoch [13/120    avg_loss:1.101, val_acc:0.688]
Epoch [14/120    avg_loss:1.057, val_acc:0.742]
Epoch [15/120    avg_loss:1.005, val_acc:0.742]
Epoch [16/120    avg_loss:0.979, val_acc:0.747]
Epoch [17/120    avg_loss:0.889, val_acc:0.776]
Epoch [18/120    avg_loss:0.749, val_acc:0.800]
Epoch [19/120    avg_loss:0.736, val_acc:0.803]
Epoch [20/120    avg_loss:0.631, val_acc:0.823]
Epoch [21/120    avg_loss:0.601, val_acc:0.820]
Epoch [22/120    avg_loss:0.551, val_acc:0.801]
Epoch [23/120    avg_loss:0.502, val_acc:0.826]
Epoch [24/120    avg_loss:0.497, val_acc:0.830]
Epoch [25/120    avg_loss:0.559, val_acc:0.811]
Epoch [26/120    avg_loss:0.438, val_acc:0.866]
Epoch [27/120    avg_loss:0.432, val_acc:0.839]
Epoch [28/120    avg_loss:0.390, val_acc:0.850]
Epoch [29/120    avg_loss:0.358, val_acc:0.864]
Epoch [30/120    avg_loss:0.365, val_acc:0.895]
Epoch [31/120    avg_loss:0.380, val_acc:0.796]
Epoch [32/120    avg_loss:0.358, val_acc:0.884]
Epoch [33/120    avg_loss:0.277, val_acc:0.899]
Epoch [34/120    avg_loss:0.271, val_acc:0.898]
Epoch [35/120    avg_loss:0.236, val_acc:0.908]
Epoch [36/120    avg_loss:0.210, val_acc:0.902]
Epoch [37/120    avg_loss:0.239, val_acc:0.859]
Epoch [38/120    avg_loss:0.222, val_acc:0.907]
Epoch [39/120    avg_loss:0.185, val_acc:0.897]
Epoch [40/120    avg_loss:0.165, val_acc:0.937]
Epoch [41/120    avg_loss:0.148, val_acc:0.914]
Epoch [42/120    avg_loss:0.170, val_acc:0.897]
Epoch [43/120    avg_loss:0.199, val_acc:0.918]
Epoch [44/120    avg_loss:0.146, val_acc:0.929]
Epoch [45/120    avg_loss:0.141, val_acc:0.940]
Epoch [46/120    avg_loss:0.115, val_acc:0.943]
Epoch [47/120    avg_loss:0.132, val_acc:0.934]
Epoch [48/120    avg_loss:0.136, val_acc:0.905]
Epoch [49/120    avg_loss:0.118, val_acc:0.936]
Epoch [50/120    avg_loss:0.108, val_acc:0.933]
Epoch [51/120    avg_loss:0.093, val_acc:0.950]
Epoch [52/120    avg_loss:0.098, val_acc:0.945]
Epoch [53/120    avg_loss:0.085, val_acc:0.954]
Epoch [54/120    avg_loss:0.092, val_acc:0.948]
Epoch [55/120    avg_loss:0.077, val_acc:0.951]
Epoch [56/120    avg_loss:0.080, val_acc:0.948]
Epoch [57/120    avg_loss:0.085, val_acc:0.955]
Epoch [58/120    avg_loss:0.067, val_acc:0.948]
Epoch [59/120    avg_loss:0.080, val_acc:0.960]
Epoch [60/120    avg_loss:0.072, val_acc:0.951]
Epoch [61/120    avg_loss:0.051, val_acc:0.959]
Epoch [62/120    avg_loss:0.049, val_acc:0.954]
Epoch [63/120    avg_loss:0.046, val_acc:0.965]
Epoch [64/120    avg_loss:0.050, val_acc:0.948]
Epoch [65/120    avg_loss:0.055, val_acc:0.966]
Epoch [66/120    avg_loss:0.051, val_acc:0.955]
Epoch [67/120    avg_loss:0.050, val_acc:0.957]
Epoch [68/120    avg_loss:0.054, val_acc:0.967]
Epoch [69/120    avg_loss:0.039, val_acc:0.963]
Epoch [70/120    avg_loss:0.031, val_acc:0.970]
Epoch [71/120    avg_loss:0.033, val_acc:0.961]
Epoch [72/120    avg_loss:0.029, val_acc:0.968]
Epoch [73/120    avg_loss:0.028, val_acc:0.967]
Epoch [74/120    avg_loss:0.031, val_acc:0.964]
Epoch [75/120    avg_loss:0.038, val_acc:0.953]
Epoch [76/120    avg_loss:0.036, val_acc:0.966]
Epoch [77/120    avg_loss:0.047, val_acc:0.960]
Epoch [78/120    avg_loss:0.056, val_acc:0.939]
Epoch [79/120    avg_loss:0.053, val_acc:0.960]
Epoch [80/120    avg_loss:0.058, val_acc:0.933]
Epoch [81/120    avg_loss:0.075, val_acc:0.943]
Epoch [82/120    avg_loss:0.062, val_acc:0.962]
Epoch [83/120    avg_loss:0.043, val_acc:0.963]
Epoch [84/120    avg_loss:0.036, val_acc:0.968]
Epoch [85/120    avg_loss:0.030, val_acc:0.970]
Epoch [86/120    avg_loss:0.025, val_acc:0.971]
Epoch [87/120    avg_loss:0.028, val_acc:0.973]
Epoch [88/120    avg_loss:0.029, val_acc:0.972]
Epoch [89/120    avg_loss:0.023, val_acc:0.974]
Epoch [90/120    avg_loss:0.024, val_acc:0.973]
Epoch [91/120    avg_loss:0.026, val_acc:0.974]
Epoch [92/120    avg_loss:0.026, val_acc:0.975]
Epoch [93/120    avg_loss:0.022, val_acc:0.974]
Epoch [94/120    avg_loss:0.023, val_acc:0.976]
Epoch [95/120    avg_loss:0.022, val_acc:0.978]
Epoch [96/120    avg_loss:0.023, val_acc:0.978]
Epoch [97/120    avg_loss:0.020, val_acc:0.977]
Epoch [98/120    avg_loss:0.021, val_acc:0.978]
Epoch [99/120    avg_loss:0.020, val_acc:0.974]
Epoch [100/120    avg_loss:0.020, val_acc:0.977]
Epoch [101/120    avg_loss:0.023, val_acc:0.979]
Epoch [102/120    avg_loss:0.020, val_acc:0.979]
Epoch [103/120    avg_loss:0.022, val_acc:0.977]
Epoch [104/120    avg_loss:0.022, val_acc:0.980]
Epoch [105/120    avg_loss:0.018, val_acc:0.977]
Epoch [106/120    avg_loss:0.022, val_acc:0.979]
Epoch [107/120    avg_loss:0.019, val_acc:0.979]
Epoch [108/120    avg_loss:0.019, val_acc:0.979]
Epoch [109/120    avg_loss:0.018, val_acc:0.980]
Epoch [110/120    avg_loss:0.021, val_acc:0.982]
Epoch [111/120    avg_loss:0.018, val_acc:0.980]
Epoch [112/120    avg_loss:0.017, val_acc:0.983]
Epoch [113/120    avg_loss:0.027, val_acc:0.980]
Epoch [114/120    avg_loss:0.017, val_acc:0.980]
Epoch [115/120    avg_loss:0.018, val_acc:0.980]
Epoch [116/120    avg_loss:0.019, val_acc:0.979]
Epoch [117/120    avg_loss:0.018, val_acc:0.980]
Epoch [118/120    avg_loss:0.017, val_acc:0.978]
Epoch [119/120    avg_loss:0.016, val_acc:0.979]
Epoch [120/120    avg_loss:0.020, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1259    3    0    0    2    0    0    1    5   13    0    0
     0    2    0]
 [   0    0    0  673    9    4    1    0    0   11    2    0   44    1
     0    2    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    4    0    2    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    4    0    0   11    3    0    0    0  848    3    0    0
     0    6    0]
 [   0    0   15    1    0    0    1    0    2    0   28 2158    3    2
     0    0    0]
 [   0    0    2    0    0    4    0    0    0    0   11    6  504    0
     1    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    3    0    1    0    0    0
  1127    8    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    10  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.0840108401084

F1 scores:
[       nan 0.975      0.98091157 0.94522472 0.97931034 0.96583144
 0.97767857 0.92592593 0.99421965 0.69387755 0.95819209 0.98314351
 0.92732291 0.9919571  0.98773006 0.92488954 0.95953757]

Kappa:
0.9667848737576912
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f16c9008860>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.756, val_acc:0.378]
Epoch [2/120    avg_loss:2.447, val_acc:0.482]
Epoch [3/120    avg_loss:2.238, val_acc:0.486]
Epoch [4/120    avg_loss:2.098, val_acc:0.548]
Epoch [5/120    avg_loss:1.974, val_acc:0.583]
Epoch [6/120    avg_loss:1.845, val_acc:0.567]
Epoch [7/120    avg_loss:1.713, val_acc:0.603]
Epoch [8/120    avg_loss:1.631, val_acc:0.625]
Epoch [9/120    avg_loss:1.521, val_acc:0.634]
Epoch [10/120    avg_loss:1.346, val_acc:0.674]
Epoch [11/120    avg_loss:1.263, val_acc:0.653]
Epoch [12/120    avg_loss:1.110, val_acc:0.711]
Epoch [13/120    avg_loss:0.964, val_acc:0.718]
Epoch [14/120    avg_loss:0.886, val_acc:0.712]
Epoch [15/120    avg_loss:0.824, val_acc:0.757]
Epoch [16/120    avg_loss:0.820, val_acc:0.722]
Epoch [17/120    avg_loss:0.735, val_acc:0.763]
Epoch [18/120    avg_loss:0.703, val_acc:0.778]
Epoch [19/120    avg_loss:0.633, val_acc:0.790]
Epoch [20/120    avg_loss:0.560, val_acc:0.815]
Epoch [21/120    avg_loss:0.542, val_acc:0.814]
Epoch [22/120    avg_loss:0.553, val_acc:0.828]
Epoch [23/120    avg_loss:0.505, val_acc:0.839]
Epoch [24/120    avg_loss:0.519, val_acc:0.809]
Epoch [25/120    avg_loss:0.465, val_acc:0.811]
Epoch [26/120    avg_loss:0.435, val_acc:0.849]
Epoch [27/120    avg_loss:0.345, val_acc:0.863]
Epoch [28/120    avg_loss:0.320, val_acc:0.873]
Epoch [29/120    avg_loss:0.331, val_acc:0.854]
Epoch [30/120    avg_loss:0.336, val_acc:0.859]
Epoch [31/120    avg_loss:0.280, val_acc:0.861]
Epoch [32/120    avg_loss:0.261, val_acc:0.874]
Epoch [33/120    avg_loss:0.232, val_acc:0.888]
Epoch [34/120    avg_loss:0.207, val_acc:0.909]
Epoch [35/120    avg_loss:0.198, val_acc:0.899]
Epoch [36/120    avg_loss:0.211, val_acc:0.877]
Epoch [37/120    avg_loss:0.206, val_acc:0.891]
Epoch [38/120    avg_loss:0.177, val_acc:0.913]
Epoch [39/120    avg_loss:0.164, val_acc:0.911]
Epoch [40/120    avg_loss:0.168, val_acc:0.910]
Epoch [41/120    avg_loss:0.167, val_acc:0.907]
Epoch [42/120    avg_loss:0.153, val_acc:0.932]
Epoch [43/120    avg_loss:0.172, val_acc:0.925]
Epoch [44/120    avg_loss:0.150, val_acc:0.933]
Epoch [45/120    avg_loss:0.142, val_acc:0.920]
Epoch [46/120    avg_loss:0.146, val_acc:0.923]
Epoch [47/120    avg_loss:0.136, val_acc:0.929]
Epoch [48/120    avg_loss:0.119, val_acc:0.936]
Epoch [49/120    avg_loss:0.101, val_acc:0.936]
Epoch [50/120    avg_loss:0.089, val_acc:0.945]
Epoch [51/120    avg_loss:0.099, val_acc:0.946]
Epoch [52/120    avg_loss:0.126, val_acc:0.953]
Epoch [53/120    avg_loss:0.105, val_acc:0.952]
Epoch [54/120    avg_loss:0.090, val_acc:0.948]
Epoch [55/120    avg_loss:0.071, val_acc:0.961]
Epoch [56/120    avg_loss:0.065, val_acc:0.955]
Epoch [57/120    avg_loss:0.055, val_acc:0.958]
Epoch [58/120    avg_loss:0.051, val_acc:0.961]
Epoch [59/120    avg_loss:0.067, val_acc:0.963]
Epoch [60/120    avg_loss:0.075, val_acc:0.940]
Epoch [61/120    avg_loss:0.101, val_acc:0.947]
Epoch [62/120    avg_loss:0.090, val_acc:0.949]
Epoch [63/120    avg_loss:0.143, val_acc:0.940]
Epoch [64/120    avg_loss:0.117, val_acc:0.940]
Epoch [65/120    avg_loss:0.141, val_acc:0.921]
Epoch [66/120    avg_loss:0.118, val_acc:0.938]
Epoch [67/120    avg_loss:0.114, val_acc:0.923]
Epoch [68/120    avg_loss:0.117, val_acc:0.927]
Epoch [69/120    avg_loss:0.153, val_acc:0.933]
Epoch [70/120    avg_loss:0.095, val_acc:0.962]
Epoch [71/120    avg_loss:0.062, val_acc:0.933]
Epoch [72/120    avg_loss:0.052, val_acc:0.954]
Epoch [73/120    avg_loss:0.050, val_acc:0.957]
Epoch [74/120    avg_loss:0.046, val_acc:0.959]
Epoch [75/120    avg_loss:0.040, val_acc:0.961]
Epoch [76/120    avg_loss:0.041, val_acc:0.960]
Epoch [77/120    avg_loss:0.037, val_acc:0.960]
Epoch [78/120    avg_loss:0.040, val_acc:0.961]
Epoch [79/120    avg_loss:0.035, val_acc:0.961]
Epoch [80/120    avg_loss:0.036, val_acc:0.961]
Epoch [81/120    avg_loss:0.042, val_acc:0.962]
Epoch [82/120    avg_loss:0.037, val_acc:0.966]
Epoch [83/120    avg_loss:0.035, val_acc:0.968]
Epoch [84/120    avg_loss:0.033, val_acc:0.967]
Epoch [85/120    avg_loss:0.039, val_acc:0.972]
Epoch [86/120    avg_loss:0.035, val_acc:0.970]
Epoch [87/120    avg_loss:0.032, val_acc:0.966]
Epoch [88/120    avg_loss:0.035, val_acc:0.973]
Epoch [89/120    avg_loss:0.031, val_acc:0.973]
Epoch [90/120    avg_loss:0.029, val_acc:0.971]
Epoch [91/120    avg_loss:0.033, val_acc:0.974]
Epoch [92/120    avg_loss:0.028, val_acc:0.974]
Epoch [93/120    avg_loss:0.032, val_acc:0.976]
Epoch [94/120    avg_loss:0.029, val_acc:0.972]
Epoch [95/120    avg_loss:0.034, val_acc:0.972]
Epoch [96/120    avg_loss:0.036, val_acc:0.974]
Epoch [97/120    avg_loss:0.029, val_acc:0.973]
Epoch [98/120    avg_loss:0.033, val_acc:0.974]
Epoch [99/120    avg_loss:0.032, val_acc:0.973]
Epoch [100/120    avg_loss:0.035, val_acc:0.975]
Epoch [101/120    avg_loss:0.032, val_acc:0.976]
Epoch [102/120    avg_loss:0.028, val_acc:0.974]
Epoch [103/120    avg_loss:0.034, val_acc:0.975]
Epoch [104/120    avg_loss:0.031, val_acc:0.971]
Epoch [105/120    avg_loss:0.028, val_acc:0.972]
Epoch [106/120    avg_loss:0.025, val_acc:0.975]
Epoch [107/120    avg_loss:0.035, val_acc:0.977]
Epoch [108/120    avg_loss:0.027, val_acc:0.975]
Epoch [109/120    avg_loss:0.030, val_acc:0.974]
Epoch [110/120    avg_loss:0.023, val_acc:0.975]
Epoch [111/120    avg_loss:0.024, val_acc:0.975]
Epoch [112/120    avg_loss:0.026, val_acc:0.976]
Epoch [113/120    avg_loss:0.026, val_acc:0.977]
Epoch [114/120    avg_loss:0.023, val_acc:0.977]
Epoch [115/120    avg_loss:0.025, val_acc:0.977]
Epoch [116/120    avg_loss:0.031, val_acc:0.975]
Epoch [117/120    avg_loss:0.030, val_acc:0.980]
Epoch [118/120    avg_loss:0.027, val_acc:0.982]
Epoch [119/120    avg_loss:0.025, val_acc:0.975]
Epoch [120/120    avg_loss:0.032, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    1    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1250    0    1    0    3    0    0    1    7   17    0    0
     0    6    0]
 [   0    0    0  677    0   14    0    0    0   10    7    3   34    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    2    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   23    0    0    4    4    0    0    0  823   14    2    0
     2    3    0]
 [   0    0   11    0    0    0    4    1    1    0   13 2171    7    2
     0    0    0]
 [   0    0    0    0    2    1    0    0    0    0   18    0  510    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0    1    0    0    0
  1131    6    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    11  326    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.17073170731707

F1 scores:
[       nan 0.94871795 0.97238429 0.9508427  0.99300699 0.9740113
 0.98121713 0.94339623 0.99883856 0.73469388 0.94218661 0.98279765
 0.93836247 0.98930481 0.99036778 0.94767442 0.98245614]

Kappa:
0.9677538174907788
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7effec13e828>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.734, val_acc:0.411]
Epoch [2/120    avg_loss:2.412, val_acc:0.379]
Epoch [3/120    avg_loss:2.226, val_acc:0.413]
Epoch [4/120    avg_loss:2.088, val_acc:0.530]
Epoch [5/120    avg_loss:1.921, val_acc:0.564]
Epoch [6/120    avg_loss:1.844, val_acc:0.573]
Epoch [7/120    avg_loss:1.703, val_acc:0.584]
Epoch [8/120    avg_loss:1.597, val_acc:0.633]
Epoch [9/120    avg_loss:1.447, val_acc:0.646]
Epoch [10/120    avg_loss:1.413, val_acc:0.634]
Epoch [11/120    avg_loss:1.384, val_acc:0.635]
Epoch [12/120    avg_loss:1.170, val_acc:0.668]
Epoch [13/120    avg_loss:1.113, val_acc:0.676]
Epoch [14/120    avg_loss:1.066, val_acc:0.636]
Epoch [15/120    avg_loss:0.994, val_acc:0.705]
Epoch [16/120    avg_loss:0.885, val_acc:0.733]
Epoch [17/120    avg_loss:0.830, val_acc:0.723]
Epoch [18/120    avg_loss:0.809, val_acc:0.714]
Epoch [19/120    avg_loss:0.877, val_acc:0.726]
Epoch [20/120    avg_loss:0.688, val_acc:0.757]
Epoch [21/120    avg_loss:0.630, val_acc:0.789]
Epoch [22/120    avg_loss:0.616, val_acc:0.775]
Epoch [23/120    avg_loss:0.549, val_acc:0.771]
Epoch [24/120    avg_loss:0.492, val_acc:0.785]
Epoch [25/120    avg_loss:0.451, val_acc:0.829]
Epoch [26/120    avg_loss:0.460, val_acc:0.797]
Epoch [27/120    avg_loss:0.440, val_acc:0.791]
Epoch [28/120    avg_loss:0.457, val_acc:0.770]
Epoch [29/120    avg_loss:0.428, val_acc:0.845]
Epoch [30/120    avg_loss:0.381, val_acc:0.830]
Epoch [31/120    avg_loss:0.326, val_acc:0.873]
Epoch [32/120    avg_loss:0.304, val_acc:0.890]
Epoch [33/120    avg_loss:0.272, val_acc:0.900]
Epoch [34/120    avg_loss:0.245, val_acc:0.911]
Epoch [35/120    avg_loss:0.259, val_acc:0.849]
Epoch [36/120    avg_loss:0.265, val_acc:0.898]
Epoch [37/120    avg_loss:0.258, val_acc:0.880]
Epoch [38/120    avg_loss:0.253, val_acc:0.903]
Epoch [39/120    avg_loss:0.230, val_acc:0.912]
Epoch [40/120    avg_loss:0.203, val_acc:0.897]
Epoch [41/120    avg_loss:0.226, val_acc:0.914]
Epoch [42/120    avg_loss:0.186, val_acc:0.927]
Epoch [43/120    avg_loss:0.177, val_acc:0.920]
Epoch [44/120    avg_loss:0.166, val_acc:0.928]
Epoch [45/120    avg_loss:0.156, val_acc:0.885]
Epoch [46/120    avg_loss:0.180, val_acc:0.923]
Epoch [47/120    avg_loss:0.131, val_acc:0.923]
Epoch [48/120    avg_loss:0.131, val_acc:0.925]
Epoch [49/120    avg_loss:0.170, val_acc:0.920]
Epoch [50/120    avg_loss:0.130, val_acc:0.945]
Epoch [51/120    avg_loss:0.120, val_acc:0.915]
Epoch [52/120    avg_loss:0.129, val_acc:0.933]
Epoch [53/120    avg_loss:0.140, val_acc:0.939]
Epoch [54/120    avg_loss:0.094, val_acc:0.947]
Epoch [55/120    avg_loss:0.128, val_acc:0.942]
Epoch [56/120    avg_loss:0.102, val_acc:0.930]
Epoch [57/120    avg_loss:0.091, val_acc:0.943]
Epoch [58/120    avg_loss:0.077, val_acc:0.948]
Epoch [59/120    avg_loss:0.075, val_acc:0.951]
Epoch [60/120    avg_loss:0.074, val_acc:0.946]
Epoch [61/120    avg_loss:0.089, val_acc:0.962]
Epoch [62/120    avg_loss:0.058, val_acc:0.955]
Epoch [63/120    avg_loss:0.055, val_acc:0.955]
Epoch [64/120    avg_loss:0.060, val_acc:0.945]
Epoch [65/120    avg_loss:0.062, val_acc:0.912]
Epoch [66/120    avg_loss:0.067, val_acc:0.957]
Epoch [67/120    avg_loss:0.070, val_acc:0.957]
Epoch [68/120    avg_loss:0.057, val_acc:0.962]
Epoch [69/120    avg_loss:0.051, val_acc:0.962]
Epoch [70/120    avg_loss:0.057, val_acc:0.947]
Epoch [71/120    avg_loss:0.068, val_acc:0.957]
Epoch [72/120    avg_loss:0.057, val_acc:0.958]
Epoch [73/120    avg_loss:0.075, val_acc:0.943]
Epoch [74/120    avg_loss:0.118, val_acc:0.909]
Epoch [75/120    avg_loss:0.091, val_acc:0.950]
Epoch [76/120    avg_loss:0.070, val_acc:0.954]
Epoch [77/120    avg_loss:0.063, val_acc:0.955]
Epoch [78/120    avg_loss:0.098, val_acc:0.937]
Epoch [79/120    avg_loss:0.090, val_acc:0.959]
Epoch [80/120    avg_loss:0.075, val_acc:0.945]
Epoch [81/120    avg_loss:0.085, val_acc:0.949]
Epoch [82/120    avg_loss:0.058, val_acc:0.963]
Epoch [83/120    avg_loss:0.049, val_acc:0.960]
Epoch [84/120    avg_loss:0.054, val_acc:0.953]
Epoch [85/120    avg_loss:0.054, val_acc:0.951]
Epoch [86/120    avg_loss:0.074, val_acc:0.960]
Epoch [87/120    avg_loss:0.065, val_acc:0.950]
Epoch [88/120    avg_loss:0.051, val_acc:0.951]
Epoch [89/120    avg_loss:0.040, val_acc:0.963]
Epoch [90/120    avg_loss:0.035, val_acc:0.968]
Epoch [91/120    avg_loss:0.035, val_acc:0.962]
Epoch [92/120    avg_loss:0.037, val_acc:0.968]
Epoch [93/120    avg_loss:0.038, val_acc:0.963]
Epoch [94/120    avg_loss:0.030, val_acc:0.967]
Epoch [95/120    avg_loss:0.052, val_acc:0.945]
Epoch [96/120    avg_loss:0.059, val_acc:0.961]
Epoch [97/120    avg_loss:0.037, val_acc:0.962]
Epoch [98/120    avg_loss:0.030, val_acc:0.971]
Epoch [99/120    avg_loss:0.028, val_acc:0.962]
Epoch [100/120    avg_loss:0.030, val_acc:0.962]
Epoch [101/120    avg_loss:0.029, val_acc:0.967]
Epoch [102/120    avg_loss:0.024, val_acc:0.963]
Epoch [103/120    avg_loss:0.024, val_acc:0.971]
Epoch [104/120    avg_loss:0.023, val_acc:0.970]
Epoch [105/120    avg_loss:0.024, val_acc:0.967]
Epoch [106/120    avg_loss:0.025, val_acc:0.970]
Epoch [107/120    avg_loss:0.023, val_acc:0.959]
Epoch [108/120    avg_loss:0.024, val_acc:0.960]
Epoch [109/120    avg_loss:0.025, val_acc:0.967]
Epoch [110/120    avg_loss:0.019, val_acc:0.972]
Epoch [111/120    avg_loss:0.021, val_acc:0.970]
Epoch [112/120    avg_loss:0.021, val_acc:0.962]
Epoch [113/120    avg_loss:0.026, val_acc:0.963]
Epoch [114/120    avg_loss:0.026, val_acc:0.966]
Epoch [115/120    avg_loss:0.023, val_acc:0.973]
Epoch [116/120    avg_loss:0.020, val_acc:0.971]
Epoch [117/120    avg_loss:0.024, val_acc:0.964]
Epoch [118/120    avg_loss:0.025, val_acc:0.977]
Epoch [119/120    avg_loss:0.015, val_acc:0.977]
Epoch [120/120    avg_loss:0.015, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1244    0    0    0    1    0    0    0   10   21    9    0
     0    0    0]
 [   0    0    2  699    0   20    0    0    0   15    0    1   10    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  419    0    5    0   11    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    6    0    0    0    0  836   21    5    0
     0    5    0]
 [   0    2    0    0    0    0    2    0    0    0    9 2190    5    1
     1    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  531    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    0    0    0
  1128    8    0]
 [   0    0    0    0    0    0   35    0    0    0    0    0    0    0
     2  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.57181571815718

F1 scores:
[       nan 0.95121951 0.98184688 0.96546961 1.         0.95227273
 0.96960712 0.90909091 1.         0.53333333 0.96424452 0.9851552
 0.9689781  0.99730458 0.9938326  0.92399404 0.97619048]

Kappa:
0.9723206536012043
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5cadb217f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.679, val_acc:0.341]
Epoch [2/120    avg_loss:2.401, val_acc:0.325]
Epoch [3/120    avg_loss:2.176, val_acc:0.477]
Epoch [4/120    avg_loss:2.018, val_acc:0.537]
Epoch [5/120    avg_loss:1.906, val_acc:0.591]
Epoch [6/120    avg_loss:1.749, val_acc:0.596]
Epoch [7/120    avg_loss:1.647, val_acc:0.611]
Epoch [8/120    avg_loss:1.508, val_acc:0.630]
Epoch [9/120    avg_loss:1.415, val_acc:0.635]
Epoch [10/120    avg_loss:1.257, val_acc:0.679]
Epoch [11/120    avg_loss:1.127, val_acc:0.690]
Epoch [12/120    avg_loss:1.088, val_acc:0.688]
Epoch [13/120    avg_loss:0.961, val_acc:0.728]
Epoch [14/120    avg_loss:0.857, val_acc:0.752]
Epoch [15/120    avg_loss:0.775, val_acc:0.788]
Epoch [16/120    avg_loss:0.718, val_acc:0.815]
Epoch [17/120    avg_loss:0.667, val_acc:0.793]
Epoch [18/120    avg_loss:0.579, val_acc:0.829]
Epoch [19/120    avg_loss:0.532, val_acc:0.810]
Epoch [20/120    avg_loss:0.486, val_acc:0.867]
Epoch [21/120    avg_loss:0.431, val_acc:0.865]
Epoch [22/120    avg_loss:0.471, val_acc:0.780]
Epoch [23/120    avg_loss:0.387, val_acc:0.870]
Epoch [24/120    avg_loss:0.338, val_acc:0.885]
Epoch [25/120    avg_loss:0.297, val_acc:0.897]
Epoch [26/120    avg_loss:0.304, val_acc:0.898]
Epoch [27/120    avg_loss:0.316, val_acc:0.878]
Epoch [28/120    avg_loss:0.316, val_acc:0.897]
Epoch [29/120    avg_loss:0.285, val_acc:0.892]
Epoch [30/120    avg_loss:0.289, val_acc:0.897]
Epoch [31/120    avg_loss:0.228, val_acc:0.918]
Epoch [32/120    avg_loss:0.199, val_acc:0.903]
Epoch [33/120    avg_loss:0.184, val_acc:0.927]
Epoch [34/120    avg_loss:0.153, val_acc:0.935]
Epoch [35/120    avg_loss:0.243, val_acc:0.902]
Epoch [36/120    avg_loss:0.236, val_acc:0.879]
Epoch [37/120    avg_loss:0.303, val_acc:0.890]
Epoch [38/120    avg_loss:0.223, val_acc:0.887]
Epoch [39/120    avg_loss:0.267, val_acc:0.902]
Epoch [40/120    avg_loss:0.178, val_acc:0.927]
Epoch [41/120    avg_loss:0.139, val_acc:0.932]
Epoch [42/120    avg_loss:0.115, val_acc:0.936]
Epoch [43/120    avg_loss:0.123, val_acc:0.940]
Epoch [44/120    avg_loss:0.096, val_acc:0.939]
Epoch [45/120    avg_loss:0.102, val_acc:0.941]
Epoch [46/120    avg_loss:0.087, val_acc:0.943]
Epoch [47/120    avg_loss:0.120, val_acc:0.937]
Epoch [48/120    avg_loss:0.100, val_acc:0.951]
Epoch [49/120    avg_loss:0.079, val_acc:0.958]
Epoch [50/120    avg_loss:0.064, val_acc:0.959]
Epoch [51/120    avg_loss:0.058, val_acc:0.955]
Epoch [52/120    avg_loss:0.054, val_acc:0.962]
Epoch [53/120    avg_loss:0.057, val_acc:0.963]
Epoch [54/120    avg_loss:0.065, val_acc:0.957]
Epoch [55/120    avg_loss:0.059, val_acc:0.966]
Epoch [56/120    avg_loss:0.061, val_acc:0.951]
Epoch [57/120    avg_loss:0.068, val_acc:0.955]
Epoch [58/120    avg_loss:0.063, val_acc:0.967]
Epoch [59/120    avg_loss:0.063, val_acc:0.955]
Epoch [60/120    avg_loss:0.055, val_acc:0.974]
Epoch [61/120    avg_loss:0.048, val_acc:0.958]
Epoch [62/120    avg_loss:0.047, val_acc:0.970]
Epoch [63/120    avg_loss:0.046, val_acc:0.955]
Epoch [64/120    avg_loss:0.059, val_acc:0.916]
Epoch [65/120    avg_loss:0.046, val_acc:0.972]
Epoch [66/120    avg_loss:0.043, val_acc:0.974]
Epoch [67/120    avg_loss:0.045, val_acc:0.963]
Epoch [68/120    avg_loss:0.053, val_acc:0.961]
Epoch [69/120    avg_loss:0.040, val_acc:0.971]
Epoch [70/120    avg_loss:0.032, val_acc:0.973]
Epoch [71/120    avg_loss:0.047, val_acc:0.958]
Epoch [72/120    avg_loss:0.053, val_acc:0.964]
Epoch [73/120    avg_loss:0.057, val_acc:0.963]
Epoch [74/120    avg_loss:0.043, val_acc:0.974]
Epoch [75/120    avg_loss:0.037, val_acc:0.970]
Epoch [76/120    avg_loss:0.031, val_acc:0.974]
Epoch [77/120    avg_loss:0.034, val_acc:0.974]
Epoch [78/120    avg_loss:0.035, val_acc:0.973]
Epoch [79/120    avg_loss:0.032, val_acc:0.976]
Epoch [80/120    avg_loss:0.023, val_acc:0.975]
Epoch [81/120    avg_loss:0.027, val_acc:0.982]
Epoch [82/120    avg_loss:0.027, val_acc:0.967]
Epoch [83/120    avg_loss:0.032, val_acc:0.973]
Epoch [84/120    avg_loss:0.027, val_acc:0.979]
Epoch [85/120    avg_loss:0.048, val_acc:0.949]
Epoch [86/120    avg_loss:0.040, val_acc:0.959]
Epoch [87/120    avg_loss:0.035, val_acc:0.983]
Epoch [88/120    avg_loss:0.034, val_acc:0.960]
Epoch [89/120    avg_loss:0.031, val_acc:0.975]
Epoch [90/120    avg_loss:0.019, val_acc:0.983]
Epoch [91/120    avg_loss:0.019, val_acc:0.977]
Epoch [92/120    avg_loss:0.026, val_acc:0.977]
Epoch [93/120    avg_loss:0.017, val_acc:0.979]
Epoch [94/120    avg_loss:0.023, val_acc:0.982]
Epoch [95/120    avg_loss:0.026, val_acc:0.980]
Epoch [96/120    avg_loss:0.022, val_acc:0.978]
Epoch [97/120    avg_loss:0.025, val_acc:0.975]
Epoch [98/120    avg_loss:0.026, val_acc:0.978]
Epoch [99/120    avg_loss:0.018, val_acc:0.984]
Epoch [100/120    avg_loss:0.021, val_acc:0.972]
Epoch [101/120    avg_loss:0.021, val_acc:0.978]
Epoch [102/120    avg_loss:0.013, val_acc:0.983]
Epoch [103/120    avg_loss:0.015, val_acc:0.974]
Epoch [104/120    avg_loss:0.014, val_acc:0.983]
Epoch [105/120    avg_loss:0.021, val_acc:0.980]
Epoch [106/120    avg_loss:0.014, val_acc:0.982]
Epoch [107/120    avg_loss:0.025, val_acc:0.972]
Epoch [108/120    avg_loss:0.060, val_acc:0.961]
Epoch [109/120    avg_loss:0.054, val_acc:0.975]
Epoch [110/120    avg_loss:0.039, val_acc:0.980]
Epoch [111/120    avg_loss:0.029, val_acc:0.980]
Epoch [112/120    avg_loss:0.018, val_acc:0.979]
Epoch [113/120    avg_loss:0.015, val_acc:0.985]
Epoch [114/120    avg_loss:0.015, val_acc:0.977]
Epoch [115/120    avg_loss:0.015, val_acc:0.980]
Epoch [116/120    avg_loss:0.014, val_acc:0.979]
Epoch [117/120    avg_loss:0.010, val_acc:0.977]
Epoch [118/120    avg_loss:0.015, val_acc:0.979]
Epoch [119/120    avg_loss:0.010, val_acc:0.980]
Epoch [120/120    avg_loss:0.014, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1241    2    8    0    0    0    0    0    9   25    0    0
     0    0    0]
 [   0    0    0  703    8    8    1    0    0   14    0    2    8    0
     0    3    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   15    0    0    1    0
     0    0    0]
 [   0    0   13    0    0    3    1    0    0    0  848    7    3    0
     0    0    0]
 [   0    0   10    0    0    0    1    0    0    0   16 2179    1    3
     0    0    0]
 [   0    0    1    2    1    4    0    0    0    0    6    8  508    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    2    0    1    0    0    0
  1133    1    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
     6  324    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.6910569105691

F1 scores:
[       nan 0.975      0.97333333 0.96565934 0.96162528 0.97737557
 0.9850075  0.98039216 0.99767981 0.6122449  0.96528173 0.98352516
 0.95939566 0.9919571  0.99473222 0.96       0.95238095]

Kappa:
0.9736796925102794
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7acd4ba828>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.715, val_acc:0.375]
Epoch [2/120    avg_loss:2.405, val_acc:0.493]
Epoch [3/120    avg_loss:2.196, val_acc:0.495]
Epoch [4/120    avg_loss:2.070, val_acc:0.482]
Epoch [5/120    avg_loss:1.954, val_acc:0.536]
Epoch [6/120    avg_loss:1.811, val_acc:0.539]
Epoch [7/120    avg_loss:1.711, val_acc:0.567]
Epoch [8/120    avg_loss:1.566, val_acc:0.630]
Epoch [9/120    avg_loss:1.495, val_acc:0.620]
Epoch [10/120    avg_loss:1.396, val_acc:0.657]
Epoch [11/120    avg_loss:1.302, val_acc:0.638]
Epoch [12/120    avg_loss:1.146, val_acc:0.703]
Epoch [13/120    avg_loss:1.048, val_acc:0.684]
Epoch [14/120    avg_loss:0.974, val_acc:0.730]
Epoch [15/120    avg_loss:0.808, val_acc:0.777]
Epoch [16/120    avg_loss:0.735, val_acc:0.800]
Epoch [17/120    avg_loss:0.711, val_acc:0.805]
Epoch [18/120    avg_loss:0.641, val_acc:0.804]
Epoch [19/120    avg_loss:0.615, val_acc:0.832]
Epoch [20/120    avg_loss:0.519, val_acc:0.839]
Epoch [21/120    avg_loss:0.488, val_acc:0.808]
Epoch [22/120    avg_loss:0.450, val_acc:0.803]
Epoch [23/120    avg_loss:0.407, val_acc:0.888]
Epoch [24/120    avg_loss:0.385, val_acc:0.853]
Epoch [25/120    avg_loss:0.353, val_acc:0.890]
Epoch [26/120    avg_loss:0.280, val_acc:0.866]
Epoch [27/120    avg_loss:0.281, val_acc:0.888]
Epoch [28/120    avg_loss:0.248, val_acc:0.891]
Epoch [29/120    avg_loss:0.330, val_acc:0.828]
Epoch [30/120    avg_loss:0.299, val_acc:0.915]
Epoch [31/120    avg_loss:0.236, val_acc:0.899]
Epoch [32/120    avg_loss:0.216, val_acc:0.929]
Epoch [33/120    avg_loss:0.217, val_acc:0.891]
Epoch [34/120    avg_loss:0.206, val_acc:0.926]
Epoch [35/120    avg_loss:0.183, val_acc:0.923]
Epoch [36/120    avg_loss:0.148, val_acc:0.936]
Epoch [37/120    avg_loss:0.140, val_acc:0.946]
Epoch [38/120    avg_loss:0.113, val_acc:0.947]
Epoch [39/120    avg_loss:0.144, val_acc:0.929]
Epoch [40/120    avg_loss:0.125, val_acc:0.940]
Epoch [41/120    avg_loss:0.117, val_acc:0.943]
Epoch [42/120    avg_loss:0.100, val_acc:0.961]
Epoch [43/120    avg_loss:0.079, val_acc:0.958]
Epoch [44/120    avg_loss:0.091, val_acc:0.945]
Epoch [45/120    avg_loss:0.076, val_acc:0.960]
Epoch [46/120    avg_loss:0.084, val_acc:0.951]
Epoch [47/120    avg_loss:0.107, val_acc:0.928]
Epoch [48/120    avg_loss:0.102, val_acc:0.951]
Epoch [49/120    avg_loss:0.130, val_acc:0.946]
Epoch [50/120    avg_loss:0.154, val_acc:0.953]
Epoch [51/120    avg_loss:0.108, val_acc:0.953]
Epoch [52/120    avg_loss:0.103, val_acc:0.958]
Epoch [53/120    avg_loss:0.090, val_acc:0.961]
Epoch [54/120    avg_loss:0.069, val_acc:0.962]
Epoch [55/120    avg_loss:0.062, val_acc:0.961]
Epoch [56/120    avg_loss:0.061, val_acc:0.970]
Epoch [57/120    avg_loss:0.060, val_acc:0.966]
Epoch [58/120    avg_loss:0.055, val_acc:0.963]
Epoch [59/120    avg_loss:0.058, val_acc:0.962]
Epoch [60/120    avg_loss:0.051, val_acc:0.962]
Epoch [61/120    avg_loss:0.048, val_acc:0.967]
Epoch [62/120    avg_loss:0.047, val_acc:0.966]
Epoch [63/120    avg_loss:0.051, val_acc:0.958]
Epoch [64/120    avg_loss:0.048, val_acc:0.967]
Epoch [65/120    avg_loss:0.044, val_acc:0.975]
Epoch [66/120    avg_loss:0.035, val_acc:0.973]
Epoch [67/120    avg_loss:0.034, val_acc:0.976]
Epoch [68/120    avg_loss:0.032, val_acc:0.966]
Epoch [69/120    avg_loss:0.034, val_acc:0.971]
Epoch [70/120    avg_loss:0.033, val_acc:0.976]
Epoch [71/120    avg_loss:0.070, val_acc:0.953]
Epoch [72/120    avg_loss:0.098, val_acc:0.963]
Epoch [73/120    avg_loss:0.068, val_acc:0.959]
Epoch [74/120    avg_loss:0.078, val_acc:0.958]
Epoch [75/120    avg_loss:0.104, val_acc:0.965]
Epoch [76/120    avg_loss:0.073, val_acc:0.966]
Epoch [77/120    avg_loss:0.070, val_acc:0.948]
Epoch [78/120    avg_loss:0.042, val_acc:0.968]
Epoch [79/120    avg_loss:0.039, val_acc:0.970]
Epoch [80/120    avg_loss:0.035, val_acc:0.970]
Epoch [81/120    avg_loss:0.045, val_acc:0.971]
Epoch [82/120    avg_loss:0.052, val_acc:0.971]
Epoch [83/120    avg_loss:0.033, val_acc:0.974]
Epoch [84/120    avg_loss:0.029, val_acc:0.977]
Epoch [85/120    avg_loss:0.024, val_acc:0.977]
Epoch [86/120    avg_loss:0.024, val_acc:0.978]
Epoch [87/120    avg_loss:0.021, val_acc:0.979]
Epoch [88/120    avg_loss:0.020, val_acc:0.979]
Epoch [89/120    avg_loss:0.023, val_acc:0.980]
Epoch [90/120    avg_loss:0.021, val_acc:0.977]
Epoch [91/120    avg_loss:0.018, val_acc:0.978]
Epoch [92/120    avg_loss:0.018, val_acc:0.980]
Epoch [93/120    avg_loss:0.016, val_acc:0.982]
Epoch [94/120    avg_loss:0.021, val_acc:0.979]
Epoch [95/120    avg_loss:0.018, val_acc:0.982]
Epoch [96/120    avg_loss:0.017, val_acc:0.982]
Epoch [97/120    avg_loss:0.018, val_acc:0.983]
Epoch [98/120    avg_loss:0.017, val_acc:0.986]
Epoch [99/120    avg_loss:0.021, val_acc:0.979]
Epoch [100/120    avg_loss:0.017, val_acc:0.984]
Epoch [101/120    avg_loss:0.019, val_acc:0.982]
Epoch [102/120    avg_loss:0.017, val_acc:0.983]
Epoch [103/120    avg_loss:0.018, val_acc:0.985]
Epoch [104/120    avg_loss:0.017, val_acc:0.984]
Epoch [105/120    avg_loss:0.016, val_acc:0.983]
Epoch [106/120    avg_loss:0.018, val_acc:0.983]
Epoch [107/120    avg_loss:0.017, val_acc:0.982]
Epoch [108/120    avg_loss:0.018, val_acc:0.984]
Epoch [109/120    avg_loss:0.013, val_acc:0.984]
Epoch [110/120    avg_loss:0.017, val_acc:0.984]
Epoch [111/120    avg_loss:0.016, val_acc:0.984]
Epoch [112/120    avg_loss:0.015, val_acc:0.984]
Epoch [113/120    avg_loss:0.016, val_acc:0.984]
Epoch [114/120    avg_loss:0.014, val_acc:0.984]
Epoch [115/120    avg_loss:0.019, val_acc:0.983]
Epoch [116/120    avg_loss:0.016, val_acc:0.984]
Epoch [117/120    avg_loss:0.015, val_acc:0.984]
Epoch [118/120    avg_loss:0.017, val_acc:0.984]
Epoch [119/120    avg_loss:0.015, val_acc:0.984]
Epoch [120/120    avg_loss:0.017, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1256    3    0    0    2    0    0    0    2   22    0    0
     0    0    0]
 [   0    0    0  717    0    1    0    0    0    7    2    1   16    0
     3    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    8    0    0    4    1    0    0    0  846   13    1    0
     0    2    0]
 [   0    1    6    0    0    0    1    0    1    0   10 2191    0    0
     0    0    0]
 [   0    0    4    0    0    2    0    0    0    0   12    8  502    0
     3    0    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    1    0    0    4    0    0    1    0    3    0    0    0
  1125    5    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
     7  323    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.91869918699187

F1 scores:
[       nan 0.88888889 0.98125    0.97750511 1.         0.98293515
 0.98350825 0.98039216 0.99417928 0.80952381 0.96410256 0.98560504
 0.95165877 0.99728997 0.98727512 0.95420975 0.97647059]

Kappa:
0.9762605268854894
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f78cacb37b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.732, val_acc:0.382]
Epoch [2/120    avg_loss:2.383, val_acc:0.450]
Epoch [3/120    avg_loss:2.220, val_acc:0.477]
Epoch [4/120    avg_loss:2.113, val_acc:0.551]
Epoch [5/120    avg_loss:1.987, val_acc:0.563]
Epoch [6/120    avg_loss:1.890, val_acc:0.583]
Epoch [7/120    avg_loss:1.797, val_acc:0.607]
Epoch [8/120    avg_loss:1.656, val_acc:0.650]
Epoch [9/120    avg_loss:1.541, val_acc:0.654]
Epoch [10/120    avg_loss:1.380, val_acc:0.689]
Epoch [11/120    avg_loss:1.237, val_acc:0.693]
Epoch [12/120    avg_loss:1.211, val_acc:0.693]
Epoch [13/120    avg_loss:1.060, val_acc:0.688]
Epoch [14/120    avg_loss:0.965, val_acc:0.713]
Epoch [15/120    avg_loss:0.912, val_acc:0.729]
Epoch [16/120    avg_loss:0.826, val_acc:0.750]
Epoch [17/120    avg_loss:0.688, val_acc:0.776]
Epoch [18/120    avg_loss:0.643, val_acc:0.786]
Epoch [19/120    avg_loss:0.577, val_acc:0.818]
Epoch [20/120    avg_loss:0.503, val_acc:0.799]
Epoch [21/120    avg_loss:0.595, val_acc:0.809]
Epoch [22/120    avg_loss:0.470, val_acc:0.838]
Epoch [23/120    avg_loss:0.470, val_acc:0.792]
Epoch [24/120    avg_loss:0.494, val_acc:0.814]
Epoch [25/120    avg_loss:0.406, val_acc:0.818]
Epoch [26/120    avg_loss:0.407, val_acc:0.854]
Epoch [27/120    avg_loss:0.352, val_acc:0.880]
Epoch [28/120    avg_loss:0.286, val_acc:0.890]
Epoch [29/120    avg_loss:0.281, val_acc:0.879]
Epoch [30/120    avg_loss:0.255, val_acc:0.905]
Epoch [31/120    avg_loss:0.214, val_acc:0.886]
Epoch [32/120    avg_loss:0.217, val_acc:0.905]
Epoch [33/120    avg_loss:0.208, val_acc:0.890]
Epoch [34/120    avg_loss:0.182, val_acc:0.891]
Epoch [35/120    avg_loss:0.240, val_acc:0.932]
Epoch [36/120    avg_loss:0.217, val_acc:0.899]
Epoch [37/120    avg_loss:0.200, val_acc:0.925]
Epoch [38/120    avg_loss:0.187, val_acc:0.912]
Epoch [39/120    avg_loss:0.169, val_acc:0.938]
Epoch [40/120    avg_loss:0.169, val_acc:0.922]
Epoch [41/120    avg_loss:0.204, val_acc:0.910]
Epoch [42/120    avg_loss:0.182, val_acc:0.934]
Epoch [43/120    avg_loss:0.166, val_acc:0.942]
Epoch [44/120    avg_loss:0.136, val_acc:0.950]
Epoch [45/120    avg_loss:0.115, val_acc:0.951]
Epoch [46/120    avg_loss:0.127, val_acc:0.939]
Epoch [47/120    avg_loss:0.115, val_acc:0.951]
Epoch [48/120    avg_loss:0.104, val_acc:0.946]
Epoch [49/120    avg_loss:0.105, val_acc:0.951]
Epoch [50/120    avg_loss:0.135, val_acc:0.927]
Epoch [51/120    avg_loss:0.136, val_acc:0.937]
Epoch [52/120    avg_loss:0.129, val_acc:0.946]
Epoch [53/120    avg_loss:0.119, val_acc:0.949]
Epoch [54/120    avg_loss:0.133, val_acc:0.948]
Epoch [55/120    avg_loss:0.123, val_acc:0.951]
Epoch [56/120    avg_loss:0.088, val_acc:0.955]
Epoch [57/120    avg_loss:0.072, val_acc:0.951]
Epoch [58/120    avg_loss:0.083, val_acc:0.940]
Epoch [59/120    avg_loss:0.074, val_acc:0.962]
Epoch [60/120    avg_loss:0.093, val_acc:0.947]
Epoch [61/120    avg_loss:0.086, val_acc:0.950]
Epoch [62/120    avg_loss:0.073, val_acc:0.943]
Epoch [63/120    avg_loss:0.063, val_acc:0.953]
Epoch [64/120    avg_loss:0.086, val_acc:0.943]
Epoch [65/120    avg_loss:0.071, val_acc:0.961]
Epoch [66/120    avg_loss:0.059, val_acc:0.945]
Epoch [67/120    avg_loss:0.072, val_acc:0.943]
Epoch [68/120    avg_loss:0.062, val_acc:0.957]
Epoch [69/120    avg_loss:0.072, val_acc:0.963]
Epoch [70/120    avg_loss:0.069, val_acc:0.948]
Epoch [71/120    avg_loss:0.072, val_acc:0.955]
Epoch [72/120    avg_loss:0.055, val_acc:0.965]
Epoch [73/120    avg_loss:0.054, val_acc:0.961]
Epoch [74/120    avg_loss:0.057, val_acc:0.958]
Epoch [75/120    avg_loss:0.108, val_acc:0.940]
Epoch [76/120    avg_loss:0.064, val_acc:0.960]
Epoch [77/120    avg_loss:0.049, val_acc:0.962]
Epoch [78/120    avg_loss:0.053, val_acc:0.968]
Epoch [79/120    avg_loss:0.041, val_acc:0.970]
Epoch [80/120    avg_loss:0.043, val_acc:0.959]
Epoch [81/120    avg_loss:0.042, val_acc:0.970]
Epoch [82/120    avg_loss:0.042, val_acc:0.968]
Epoch [83/120    avg_loss:0.032, val_acc:0.972]
Epoch [84/120    avg_loss:0.026, val_acc:0.975]
Epoch [85/120    avg_loss:0.029, val_acc:0.971]
Epoch [86/120    avg_loss:0.031, val_acc:0.970]
Epoch [87/120    avg_loss:0.030, val_acc:0.960]
Epoch [88/120    avg_loss:0.023, val_acc:0.976]
Epoch [89/120    avg_loss:0.024, val_acc:0.974]
Epoch [90/120    avg_loss:0.024, val_acc:0.972]
Epoch [91/120    avg_loss:0.033, val_acc:0.967]
Epoch [92/120    avg_loss:0.031, val_acc:0.968]
Epoch [93/120    avg_loss:0.021, val_acc:0.970]
Epoch [94/120    avg_loss:0.031, val_acc:0.976]
Epoch [95/120    avg_loss:0.032, val_acc:0.951]
Epoch [96/120    avg_loss:0.028, val_acc:0.975]
Epoch [97/120    avg_loss:0.021, val_acc:0.976]
Epoch [98/120    avg_loss:0.016, val_acc:0.970]
Epoch [99/120    avg_loss:0.017, val_acc:0.980]
Epoch [100/120    avg_loss:0.017, val_acc:0.971]
Epoch [101/120    avg_loss:0.020, val_acc:0.979]
Epoch [102/120    avg_loss:0.017, val_acc:0.972]
Epoch [103/120    avg_loss:0.018, val_acc:0.979]
Epoch [104/120    avg_loss:0.026, val_acc:0.975]
Epoch [105/120    avg_loss:0.056, val_acc:0.977]
Epoch [106/120    avg_loss:0.041, val_acc:0.960]
Epoch [107/120    avg_loss:0.039, val_acc:0.977]
Epoch [108/120    avg_loss:0.025, val_acc:0.974]
Epoch [109/120    avg_loss:0.019, val_acc:0.964]
Epoch [110/120    avg_loss:0.054, val_acc:0.960]
Epoch [111/120    avg_loss:0.042, val_acc:0.960]
Epoch [112/120    avg_loss:0.031, val_acc:0.972]
Epoch [113/120    avg_loss:0.016, val_acc:0.974]
Epoch [114/120    avg_loss:0.018, val_acc:0.977]
Epoch [115/120    avg_loss:0.015, val_acc:0.977]
Epoch [116/120    avg_loss:0.018, val_acc:0.976]
Epoch [117/120    avg_loss:0.016, val_acc:0.977]
Epoch [118/120    avg_loss:0.016, val_acc:0.977]
Epoch [119/120    avg_loss:0.016, val_acc:0.976]
Epoch [120/120    avg_loss:0.018, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1255    2    1    0    3    0    0    2    7   12    3    0
     0    0    0]
 [   0    0    0  673   10   25    0    0    0   17    1    1   20    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    0    0    3    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    9    1    0    3    3    0    0    0  852    7    0    0
     0    0    0]
 [   0    4    7    0    0    0    1    1    1    0   12 2183    0    1
     0    0    0]
 [   0    0    0    6    1    2    0    0    0    0    8    8  503    0
     3    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1136    2    0]
 [   0    0    0    0    0    0   26    0    0    0    0    0    0    0
    21  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.26829268292683

F1 scores:
[       nan 0.94117647 0.98161908 0.94125874 0.97260274 0.95505618
 0.97321429 0.98039216 0.99883856 0.59649123 0.97038724 0.98688969
 0.9472693  0.99730458 0.98525585 0.92449923 0.9704142 ]

Kappa:
0.9688582086658093
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc715b097f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.690, val_acc:0.207]
Epoch [2/120    avg_loss:2.387, val_acc:0.395]
Epoch [3/120    avg_loss:2.224, val_acc:0.487]
Epoch [4/120    avg_loss:2.138, val_acc:0.549]
Epoch [5/120    avg_loss:1.964, val_acc:0.583]
Epoch [6/120    avg_loss:1.867, val_acc:0.593]
Epoch [7/120    avg_loss:1.753, val_acc:0.604]
Epoch [8/120    avg_loss:1.630, val_acc:0.625]
Epoch [9/120    avg_loss:1.521, val_acc:0.638]
Epoch [10/120    avg_loss:1.389, val_acc:0.651]
Epoch [11/120    avg_loss:1.307, val_acc:0.657]
Epoch [12/120    avg_loss:1.148, val_acc:0.678]
Epoch [13/120    avg_loss:1.042, val_acc:0.668]
Epoch [14/120    avg_loss:0.946, val_acc:0.725]
Epoch [15/120    avg_loss:0.858, val_acc:0.760]
Epoch [16/120    avg_loss:0.757, val_acc:0.778]
Epoch [17/120    avg_loss:0.691, val_acc:0.762]
Epoch [18/120    avg_loss:0.655, val_acc:0.798]
Epoch [19/120    avg_loss:0.565, val_acc:0.820]
Epoch [20/120    avg_loss:0.556, val_acc:0.821]
Epoch [21/120    avg_loss:0.499, val_acc:0.829]
Epoch [22/120    avg_loss:0.484, val_acc:0.824]
Epoch [23/120    avg_loss:0.428, val_acc:0.843]
Epoch [24/120    avg_loss:0.365, val_acc:0.867]
Epoch [25/120    avg_loss:0.347, val_acc:0.871]
Epoch [26/120    avg_loss:0.314, val_acc:0.873]
Epoch [27/120    avg_loss:0.332, val_acc:0.835]
Epoch [28/120    avg_loss:0.298, val_acc:0.867]
Epoch [29/120    avg_loss:0.290, val_acc:0.882]
Epoch [30/120    avg_loss:0.243, val_acc:0.903]
Epoch [31/120    avg_loss:0.297, val_acc:0.847]
Epoch [32/120    avg_loss:0.272, val_acc:0.861]
Epoch [33/120    avg_loss:0.231, val_acc:0.924]
Epoch [34/120    avg_loss:0.211, val_acc:0.897]
Epoch [35/120    avg_loss:0.215, val_acc:0.889]
Epoch [36/120    avg_loss:0.186, val_acc:0.908]
Epoch [37/120    avg_loss:0.155, val_acc:0.936]
Epoch [38/120    avg_loss:0.150, val_acc:0.940]
Epoch [39/120    avg_loss:0.147, val_acc:0.924]
Epoch [40/120    avg_loss:0.125, val_acc:0.930]
Epoch [41/120    avg_loss:0.118, val_acc:0.935]
Epoch [42/120    avg_loss:0.109, val_acc:0.942]
Epoch [43/120    avg_loss:0.117, val_acc:0.935]
Epoch [44/120    avg_loss:0.126, val_acc:0.938]
Epoch [45/120    avg_loss:0.102, val_acc:0.939]
Epoch [46/120    avg_loss:0.100, val_acc:0.927]
Epoch [47/120    avg_loss:0.088, val_acc:0.936]
Epoch [48/120    avg_loss:0.079, val_acc:0.941]
Epoch [49/120    avg_loss:0.109, val_acc:0.936]
Epoch [50/120    avg_loss:0.135, val_acc:0.923]
Epoch [51/120    avg_loss:0.124, val_acc:0.939]
Epoch [52/120    avg_loss:0.090, val_acc:0.942]
Epoch [53/120    avg_loss:0.083, val_acc:0.943]
Epoch [54/120    avg_loss:0.065, val_acc:0.950]
Epoch [55/120    avg_loss:0.071, val_acc:0.949]
Epoch [56/120    avg_loss:0.065, val_acc:0.950]
Epoch [57/120    avg_loss:0.068, val_acc:0.945]
Epoch [58/120    avg_loss:0.090, val_acc:0.943]
Epoch [59/120    avg_loss:0.084, val_acc:0.946]
Epoch [60/120    avg_loss:0.074, val_acc:0.918]
Epoch [61/120    avg_loss:0.071, val_acc:0.947]
Epoch [62/120    avg_loss:0.089, val_acc:0.933]
Epoch [63/120    avg_loss:0.068, val_acc:0.953]
Epoch [64/120    avg_loss:0.049, val_acc:0.950]
Epoch [65/120    avg_loss:0.044, val_acc:0.935]
Epoch [66/120    avg_loss:0.046, val_acc:0.953]
Epoch [67/120    avg_loss:0.035, val_acc:0.945]
Epoch [68/120    avg_loss:0.043, val_acc:0.950]
Epoch [69/120    avg_loss:0.054, val_acc:0.955]
Epoch [70/120    avg_loss:0.058, val_acc:0.952]
Epoch [71/120    avg_loss:0.040, val_acc:0.961]
Epoch [72/120    avg_loss:0.038, val_acc:0.959]
Epoch [73/120    avg_loss:0.041, val_acc:0.960]
Epoch [74/120    avg_loss:0.036, val_acc:0.953]
Epoch [75/120    avg_loss:0.046, val_acc:0.957]
Epoch [76/120    avg_loss:0.038, val_acc:0.952]
Epoch [77/120    avg_loss:0.051, val_acc:0.953]
Epoch [78/120    avg_loss:0.044, val_acc:0.954]
Epoch [79/120    avg_loss:0.029, val_acc:0.955]
Epoch [80/120    avg_loss:0.030, val_acc:0.950]
Epoch [81/120    avg_loss:0.026, val_acc:0.962]
Epoch [82/120    avg_loss:0.028, val_acc:0.958]
Epoch [83/120    avg_loss:0.025, val_acc:0.964]
Epoch [84/120    avg_loss:0.023, val_acc:0.958]
Epoch [85/120    avg_loss:0.025, val_acc:0.959]
Epoch [86/120    avg_loss:0.022, val_acc:0.952]
Epoch [87/120    avg_loss:0.019, val_acc:0.957]
Epoch [88/120    avg_loss:0.025, val_acc:0.958]
Epoch [89/120    avg_loss:0.032, val_acc:0.962]
Epoch [90/120    avg_loss:0.026, val_acc:0.950]
Epoch [91/120    avg_loss:0.024, val_acc:0.965]
Epoch [92/120    avg_loss:0.027, val_acc:0.954]
Epoch [93/120    avg_loss:0.025, val_acc:0.952]
Epoch [94/120    avg_loss:0.034, val_acc:0.958]
Epoch [95/120    avg_loss:0.023, val_acc:0.949]
Epoch [96/120    avg_loss:0.022, val_acc:0.962]
Epoch [97/120    avg_loss:0.023, val_acc:0.960]
Epoch [98/120    avg_loss:0.020, val_acc:0.960]
Epoch [99/120    avg_loss:0.028, val_acc:0.957]
Epoch [100/120    avg_loss:0.028, val_acc:0.962]
Epoch [101/120    avg_loss:0.027, val_acc:0.930]
Epoch [102/120    avg_loss:0.044, val_acc:0.958]
Epoch [103/120    avg_loss:0.037, val_acc:0.957]
Epoch [104/120    avg_loss:0.021, val_acc:0.957]
Epoch [105/120    avg_loss:0.022, val_acc:0.958]
Epoch [106/120    avg_loss:0.015, val_acc:0.958]
Epoch [107/120    avg_loss:0.014, val_acc:0.960]
Epoch [108/120    avg_loss:0.012, val_acc:0.959]
Epoch [109/120    avg_loss:0.013, val_acc:0.959]
Epoch [110/120    avg_loss:0.011, val_acc:0.958]
Epoch [111/120    avg_loss:0.012, val_acc:0.958]
Epoch [112/120    avg_loss:0.012, val_acc:0.958]
Epoch [113/120    avg_loss:0.013, val_acc:0.957]
Epoch [114/120    avg_loss:0.010, val_acc:0.957]
Epoch [115/120    avg_loss:0.013, val_acc:0.957]
Epoch [116/120    avg_loss:0.011, val_acc:0.957]
Epoch [117/120    avg_loss:0.012, val_acc:0.955]
Epoch [118/120    avg_loss:0.011, val_acc:0.955]
Epoch [119/120    avg_loss:0.011, val_acc:0.955]
Epoch [120/120    avg_loss:0.014, val_acc:0.955]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1248    8    5    0    2    0    0    3    7   12    0    0
     0    0    0]
 [   0    0    9  670    8    1    1    0    0   15   16    0   24    0
     0    3    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    5    0    7    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    4    4    0    0    0  844   11    0    0
     2    0    0]
 [   0    0    5    0    0    0    1    0    0    0   13 2181    8    2
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    7   10  507    0
     3    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1125   12    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    47  290    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.85636856368563

F1 scores:
[       nan 0.975      0.97614392 0.94035088 0.97038724 0.97577855
 0.98419865 0.90909091 0.99767442 0.59016393 0.95637394 0.98531737
 0.94413408 0.99462366 0.97150259 0.88957055 0.98245614]

Kappa:
0.9641574938410576
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc9b7cf47f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 64232==>0.06M
----------Training process----------
Epoch [1/120    avg_loss:2.736, val_acc:0.197]
Epoch [2/120    avg_loss:2.410, val_acc:0.463]
Epoch [3/120    avg_loss:2.185, val_acc:0.455]
Epoch [4/120    avg_loss:2.052, val_acc:0.537]
Epoch [5/120    avg_loss:1.944, val_acc:0.514]
Epoch [6/120    avg_loss:1.774, val_acc:0.528]
Epoch [7/120    avg_loss:1.703, val_acc:0.580]
Epoch [8/120    avg_loss:1.567, val_acc:0.582]
Epoch [9/120    avg_loss:1.443, val_acc:0.620]
Epoch [10/120    avg_loss:1.324, val_acc:0.648]
Epoch [11/120    avg_loss:1.234, val_acc:0.649]
Epoch [12/120    avg_loss:1.092, val_acc:0.668]
Epoch [13/120    avg_loss:1.010, val_acc:0.743]
Epoch [14/120    avg_loss:0.903, val_acc:0.740]
Epoch [15/120    avg_loss:0.797, val_acc:0.759]
Epoch [16/120    avg_loss:0.757, val_acc:0.792]
Epoch [17/120    avg_loss:0.687, val_acc:0.771]
Epoch [18/120    avg_loss:0.610, val_acc:0.825]
Epoch [19/120    avg_loss:0.580, val_acc:0.811]
Epoch [20/120    avg_loss:0.582, val_acc:0.797]
Epoch [21/120    avg_loss:0.488, val_acc:0.823]
Epoch [22/120    avg_loss:0.440, val_acc:0.868]
Epoch [23/120    avg_loss:0.426, val_acc:0.865]
Epoch [24/120    avg_loss:0.399, val_acc:0.872]
Epoch [25/120    avg_loss:0.351, val_acc:0.863]
Epoch [26/120    avg_loss:0.324, val_acc:0.904]
Epoch [27/120    avg_loss:0.282, val_acc:0.889]
Epoch [28/120    avg_loss:0.267, val_acc:0.905]
Epoch [29/120    avg_loss:0.236, val_acc:0.924]
Epoch [30/120    avg_loss:0.254, val_acc:0.901]
Epoch [31/120    avg_loss:0.243, val_acc:0.899]
Epoch [32/120    avg_loss:0.238, val_acc:0.927]
Epoch [33/120    avg_loss:0.207, val_acc:0.914]
Epoch [34/120    avg_loss:0.233, val_acc:0.926]
Epoch [35/120    avg_loss:0.215, val_acc:0.910]
Epoch [36/120    avg_loss:0.196, val_acc:0.895]
Epoch [37/120    avg_loss:0.199, val_acc:0.927]
Epoch [38/120    avg_loss:0.196, val_acc:0.905]
Epoch [39/120    avg_loss:0.260, val_acc:0.883]
Epoch [40/120    avg_loss:0.178, val_acc:0.917]
Epoch [41/120    avg_loss:0.146, val_acc:0.920]
Epoch [42/120    avg_loss:0.123, val_acc:0.941]
Epoch [43/120    avg_loss:0.140, val_acc:0.937]
Epoch [44/120    avg_loss:0.145, val_acc:0.932]
Epoch [45/120    avg_loss:0.112, val_acc:0.927]
Epoch [46/120    avg_loss:0.116, val_acc:0.947]
Epoch [47/120    avg_loss:0.124, val_acc:0.945]
Epoch [48/120    avg_loss:0.113, val_acc:0.947]
Epoch [49/120    avg_loss:0.099, val_acc:0.953]
Epoch [50/120    avg_loss:0.080, val_acc:0.950]
Epoch [51/120    avg_loss:0.099, val_acc:0.949]
Epoch [52/120    avg_loss:0.091, val_acc:0.965]
Epoch [53/120    avg_loss:0.059, val_acc:0.962]
Epoch [54/120    avg_loss:0.060, val_acc:0.964]
Epoch [55/120    avg_loss:0.053, val_acc:0.961]
Epoch [56/120    avg_loss:0.050, val_acc:0.964]
Epoch [57/120    avg_loss:0.052, val_acc:0.964]
Epoch [58/120    avg_loss:0.070, val_acc:0.959]
Epoch [59/120    avg_loss:0.091, val_acc:0.963]
Epoch [60/120    avg_loss:0.063, val_acc:0.960]
Epoch [61/120    avg_loss:0.064, val_acc:0.960]
Epoch [62/120    avg_loss:0.048, val_acc:0.957]
Epoch [63/120    avg_loss:0.050, val_acc:0.963]
Epoch [64/120    avg_loss:0.051, val_acc:0.958]
Epoch [65/120    avg_loss:0.046, val_acc:0.973]
Epoch [66/120    avg_loss:0.040, val_acc:0.973]
Epoch [67/120    avg_loss:0.053, val_acc:0.967]
Epoch [68/120    avg_loss:0.043, val_acc:0.960]
Epoch [69/120    avg_loss:0.073, val_acc:0.968]
Epoch [70/120    avg_loss:0.046, val_acc:0.957]
Epoch [71/120    avg_loss:0.035, val_acc:0.970]
Epoch [72/120    avg_loss:0.041, val_acc:0.967]
Epoch [73/120    avg_loss:0.052, val_acc:0.955]
Epoch [74/120    avg_loss:0.042, val_acc:0.967]
Epoch [75/120    avg_loss:0.081, val_acc:0.953]
Epoch [76/120    avg_loss:0.142, val_acc:0.955]
Epoch [77/120    avg_loss:0.086, val_acc:0.953]
Epoch [78/120    avg_loss:0.069, val_acc:0.946]
Epoch [79/120    avg_loss:0.064, val_acc:0.949]
Epoch [80/120    avg_loss:0.050, val_acc:0.958]
Epoch [81/120    avg_loss:0.047, val_acc:0.968]
Epoch [82/120    avg_loss:0.036, val_acc:0.970]
Epoch [83/120    avg_loss:0.031, val_acc:0.972]
Epoch [84/120    avg_loss:0.038, val_acc:0.974]
Epoch [85/120    avg_loss:0.033, val_acc:0.975]
Epoch [86/120    avg_loss:0.029, val_acc:0.973]
Epoch [87/120    avg_loss:0.029, val_acc:0.976]
Epoch [88/120    avg_loss:0.022, val_acc:0.975]
Epoch [89/120    avg_loss:0.025, val_acc:0.975]
Epoch [90/120    avg_loss:0.027, val_acc:0.977]
Epoch [91/120    avg_loss:0.027, val_acc:0.978]
Epoch [92/120    avg_loss:0.027, val_acc:0.978]
Epoch [93/120    avg_loss:0.030, val_acc:0.976]
Epoch [94/120    avg_loss:0.028, val_acc:0.978]
Epoch [95/120    avg_loss:0.023, val_acc:0.976]
Epoch [96/120    avg_loss:0.021, val_acc:0.976]
Epoch [97/120    avg_loss:0.023, val_acc:0.980]
Epoch [98/120    avg_loss:0.024, val_acc:0.978]
Epoch [99/120    avg_loss:0.026, val_acc:0.978]
Epoch [100/120    avg_loss:0.028, val_acc:0.984]
Epoch [101/120    avg_loss:0.022, val_acc:0.982]
Epoch [102/120    avg_loss:0.028, val_acc:0.980]
Epoch [103/120    avg_loss:0.028, val_acc:0.982]
Epoch [104/120    avg_loss:0.027, val_acc:0.983]
Epoch [105/120    avg_loss:0.025, val_acc:0.985]
Epoch [106/120    avg_loss:0.023, val_acc:0.985]
Epoch [107/120    avg_loss:0.022, val_acc:0.980]
Epoch [108/120    avg_loss:0.021, val_acc:0.980]
Epoch [109/120    avg_loss:0.020, val_acc:0.980]
Epoch [110/120    avg_loss:0.021, val_acc:0.982]
Epoch [111/120    avg_loss:0.021, val_acc:0.982]
Epoch [112/120    avg_loss:0.020, val_acc:0.983]
Epoch [113/120    avg_loss:0.020, val_acc:0.980]
Epoch [114/120    avg_loss:0.019, val_acc:0.982]
Epoch [115/120    avg_loss:0.021, val_acc:0.983]
Epoch [116/120    avg_loss:0.020, val_acc:0.977]
Epoch [117/120    avg_loss:0.021, val_acc:0.985]
Epoch [118/120    avg_loss:0.020, val_acc:0.982]
Epoch [119/120    avg_loss:0.020, val_acc:0.985]
Epoch [120/120    avg_loss:0.018, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1250    4    0    0    2    0    0    0    8   19    0    0
     0    2    0]
 [   0    0    0  697    3   22    0    0    0    6    0    0   19    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    2    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   16    0    0    4    4    0    0    0  835   10    1    0
     0    5    0]
 [   0    0   20    0    0    0    5    0    3    0   10 2163    5    3
     1    0    0]
 [   0    0    0    7    0    4    0    0    0    0   16    9  493    0
     2    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    3    0    0    0
  1126    3    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    13  319    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.1490514905149

F1 scores:
[       nan 0.975      0.97238429 0.9580756  0.99300699 0.9556541
 0.98059701 0.96153846 0.99537037 0.79069767 0.95483133 0.98072999
 0.93459716 0.9919571  0.98728628 0.94378698 0.9704142 ]

Kappa:
0.9675071720132487
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff972fec860>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.746, val_acc:0.203]
Epoch [2/120    avg_loss:2.423, val_acc:0.435]
Epoch [3/120    avg_loss:2.250, val_acc:0.483]
Epoch [4/120    avg_loss:2.115, val_acc:0.527]
Epoch [5/120    avg_loss:1.984, val_acc:0.565]
Epoch [6/120    avg_loss:1.862, val_acc:0.548]
Epoch [7/120    avg_loss:1.776, val_acc:0.542]
Epoch [8/120    avg_loss:1.631, val_acc:0.607]
Epoch [9/120    avg_loss:1.564, val_acc:0.623]
Epoch [10/120    avg_loss:1.416, val_acc:0.639]
Epoch [11/120    avg_loss:1.326, val_acc:0.641]
Epoch [12/120    avg_loss:1.192, val_acc:0.668]
Epoch [13/120    avg_loss:1.168, val_acc:0.670]
Epoch [14/120    avg_loss:1.090, val_acc:0.679]
Epoch [15/120    avg_loss:1.028, val_acc:0.687]
Epoch [16/120    avg_loss:0.912, val_acc:0.727]
Epoch [17/120    avg_loss:0.841, val_acc:0.733]
Epoch [18/120    avg_loss:0.779, val_acc:0.748]
Epoch [19/120    avg_loss:0.727, val_acc:0.786]
Epoch [20/120    avg_loss:0.685, val_acc:0.771]
Epoch [21/120    avg_loss:0.642, val_acc:0.758]
Epoch [22/120    avg_loss:0.621, val_acc:0.803]
Epoch [23/120    avg_loss:0.579, val_acc:0.760]
Epoch [24/120    avg_loss:0.550, val_acc:0.807]
Epoch [25/120    avg_loss:0.540, val_acc:0.780]
Epoch [26/120    avg_loss:0.449, val_acc:0.810]
Epoch [27/120    avg_loss:0.445, val_acc:0.837]
Epoch [28/120    avg_loss:0.424, val_acc:0.841]
Epoch [29/120    avg_loss:0.378, val_acc:0.855]
Epoch [30/120    avg_loss:0.296, val_acc:0.882]
Epoch [31/120    avg_loss:0.368, val_acc:0.848]
Epoch [32/120    avg_loss:0.382, val_acc:0.879]
Epoch [33/120    avg_loss:0.349, val_acc:0.853]
Epoch [34/120    avg_loss:0.342, val_acc:0.891]
Epoch [35/120    avg_loss:0.413, val_acc:0.834]
Epoch [36/120    avg_loss:0.371, val_acc:0.834]
Epoch [37/120    avg_loss:0.261, val_acc:0.903]
Epoch [38/120    avg_loss:0.250, val_acc:0.884]
Epoch [39/120    avg_loss:0.297, val_acc:0.861]
Epoch [40/120    avg_loss:0.270, val_acc:0.887]
Epoch [41/120    avg_loss:0.191, val_acc:0.901]
Epoch [42/120    avg_loss:0.216, val_acc:0.899]
Epoch [43/120    avg_loss:0.209, val_acc:0.917]
Epoch [44/120    avg_loss:0.239, val_acc:0.876]
Epoch [45/120    avg_loss:0.243, val_acc:0.907]
Epoch [46/120    avg_loss:0.172, val_acc:0.930]
Epoch [47/120    avg_loss:0.193, val_acc:0.922]
Epoch [48/120    avg_loss:0.178, val_acc:0.925]
Epoch [49/120    avg_loss:0.155, val_acc:0.920]
Epoch [50/120    avg_loss:0.157, val_acc:0.939]
Epoch [51/120    avg_loss:0.147, val_acc:0.942]
Epoch [52/120    avg_loss:0.154, val_acc:0.917]
Epoch [53/120    avg_loss:0.173, val_acc:0.922]
Epoch [54/120    avg_loss:0.142, val_acc:0.945]
Epoch [55/120    avg_loss:0.149, val_acc:0.933]
Epoch [56/120    avg_loss:0.092, val_acc:0.951]
Epoch [57/120    avg_loss:0.095, val_acc:0.948]
Epoch [58/120    avg_loss:0.106, val_acc:0.953]
Epoch [59/120    avg_loss:0.090, val_acc:0.942]
Epoch [60/120    avg_loss:0.101, val_acc:0.935]
Epoch [61/120    avg_loss:0.089, val_acc:0.953]
Epoch [62/120    avg_loss:0.078, val_acc:0.953]
Epoch [63/120    avg_loss:0.086, val_acc:0.953]
Epoch [64/120    avg_loss:0.104, val_acc:0.942]
Epoch [65/120    avg_loss:0.107, val_acc:0.948]
Epoch [66/120    avg_loss:0.088, val_acc:0.939]
Epoch [67/120    avg_loss:0.099, val_acc:0.940]
Epoch [68/120    avg_loss:0.077, val_acc:0.954]
Epoch [69/120    avg_loss:0.074, val_acc:0.954]
Epoch [70/120    avg_loss:0.057, val_acc:0.951]
Epoch [71/120    avg_loss:0.061, val_acc:0.949]
Epoch [72/120    avg_loss:0.067, val_acc:0.960]
Epoch [73/120    avg_loss:0.063, val_acc:0.953]
Epoch [74/120    avg_loss:0.044, val_acc:0.955]
Epoch [75/120    avg_loss:0.054, val_acc:0.964]
Epoch [76/120    avg_loss:0.049, val_acc:0.966]
Epoch [77/120    avg_loss:0.064, val_acc:0.953]
Epoch [78/120    avg_loss:0.043, val_acc:0.961]
Epoch [79/120    avg_loss:0.094, val_acc:0.934]
Epoch [80/120    avg_loss:0.086, val_acc:0.957]
Epoch [81/120    avg_loss:0.061, val_acc:0.958]
Epoch [82/120    avg_loss:0.046, val_acc:0.959]
Epoch [83/120    avg_loss:0.046, val_acc:0.959]
Epoch [84/120    avg_loss:0.038, val_acc:0.961]
Epoch [85/120    avg_loss:0.038, val_acc:0.962]
Epoch [86/120    avg_loss:0.045, val_acc:0.962]
Epoch [87/120    avg_loss:0.034, val_acc:0.963]
Epoch [88/120    avg_loss:0.034, val_acc:0.966]
Epoch [89/120    avg_loss:0.038, val_acc:0.957]
Epoch [90/120    avg_loss:0.039, val_acc:0.967]
Epoch [91/120    avg_loss:0.048, val_acc:0.954]
Epoch [92/120    avg_loss:0.048, val_acc:0.957]
Epoch [93/120    avg_loss:0.044, val_acc:0.936]
Epoch [94/120    avg_loss:0.036, val_acc:0.963]
Epoch [95/120    avg_loss:0.030, val_acc:0.965]
Epoch [96/120    avg_loss:0.035, val_acc:0.958]
Epoch [97/120    avg_loss:0.035, val_acc:0.953]
Epoch [98/120    avg_loss:0.041, val_acc:0.965]
Epoch [99/120    avg_loss:0.037, val_acc:0.970]
Epoch [100/120    avg_loss:0.045, val_acc:0.955]
Epoch [101/120    avg_loss:0.040, val_acc:0.965]
Epoch [102/120    avg_loss:0.026, val_acc:0.964]
Epoch [103/120    avg_loss:0.027, val_acc:0.971]
Epoch [104/120    avg_loss:0.030, val_acc:0.970]
Epoch [105/120    avg_loss:0.028, val_acc:0.962]
Epoch [106/120    avg_loss:0.032, val_acc:0.953]
Epoch [107/120    avg_loss:0.040, val_acc:0.962]
Epoch [108/120    avg_loss:0.024, val_acc:0.971]
Epoch [109/120    avg_loss:0.023, val_acc:0.970]
Epoch [110/120    avg_loss:0.018, val_acc:0.968]
Epoch [111/120    avg_loss:0.020, val_acc:0.966]
Epoch [112/120    avg_loss:0.021, val_acc:0.970]
Epoch [113/120    avg_loss:0.020, val_acc:0.962]
Epoch [114/120    avg_loss:0.027, val_acc:0.954]
Epoch [115/120    avg_loss:0.026, val_acc:0.972]
Epoch [116/120    avg_loss:0.028, val_acc:0.971]
Epoch [117/120    avg_loss:0.023, val_acc:0.968]
Epoch [118/120    avg_loss:0.021, val_acc:0.971]
Epoch [119/120    avg_loss:0.019, val_acc:0.967]
Epoch [120/120    avg_loss:0.017, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    2    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1251    1    0    0    2    0    0    0    8   21    2    0
     0    0    0]
 [   0    0    0  708    1   14    0    0    0   10    0    0    8    2
     0    4    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    5    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   13    0    0    4    0
     0    0    0]
 [   0    0   27   44    0    6    0    0    0    0  784    2    0    0
     0   12    0]
 [   0    0   13    0    0    0    6    0    6    0   12 2168    0    1
     4    0    0]
 [   0    0    7   20    5    0    0    0    0    0   15   11  474    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    2    0    0    0
  1132    0    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    35  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
96.30352303523036

F1 scores:
[       nan 0.94871795 0.96789168 0.93096647 0.98611111 0.96629213
 0.98277154 1.         0.99307159 0.56521739 0.92343934 0.98255155
 0.92397661 0.9919571  0.98008658 0.90166415 0.96385542]

Kappa:
0.957852401507719
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f12c4046860>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.765, val_acc:0.266]
Epoch [2/120    avg_loss:2.416, val_acc:0.520]
Epoch [3/120    avg_loss:2.225, val_acc:0.494]
Epoch [4/120    avg_loss:2.108, val_acc:0.549]
Epoch [5/120    avg_loss:1.993, val_acc:0.574]
Epoch [6/120    avg_loss:1.878, val_acc:0.581]
Epoch [7/120    avg_loss:1.715, val_acc:0.576]
Epoch [8/120    avg_loss:1.674, val_acc:0.613]
Epoch [9/120    avg_loss:1.579, val_acc:0.613]
Epoch [10/120    avg_loss:1.430, val_acc:0.664]
Epoch [11/120    avg_loss:1.367, val_acc:0.668]
Epoch [12/120    avg_loss:1.243, val_acc:0.676]
Epoch [13/120    avg_loss:1.166, val_acc:0.698]
Epoch [14/120    avg_loss:1.085, val_acc:0.717]
Epoch [15/120    avg_loss:0.985, val_acc:0.715]
Epoch [16/120    avg_loss:0.944, val_acc:0.760]
Epoch [17/120    avg_loss:0.899, val_acc:0.728]
Epoch [18/120    avg_loss:0.856, val_acc:0.781]
Epoch [19/120    avg_loss:0.719, val_acc:0.802]
Epoch [20/120    avg_loss:0.665, val_acc:0.793]
Epoch [21/120    avg_loss:0.684, val_acc:0.772]
Epoch [22/120    avg_loss:0.577, val_acc:0.786]
Epoch [23/120    avg_loss:0.552, val_acc:0.797]
Epoch [24/120    avg_loss:0.514, val_acc:0.823]
Epoch [25/120    avg_loss:0.467, val_acc:0.833]
Epoch [26/120    avg_loss:0.382, val_acc:0.855]
Epoch [27/120    avg_loss:0.350, val_acc:0.833]
Epoch [28/120    avg_loss:0.336, val_acc:0.867]
Epoch [29/120    avg_loss:0.317, val_acc:0.827]
Epoch [30/120    avg_loss:0.386, val_acc:0.836]
Epoch [31/120    avg_loss:0.485, val_acc:0.850]
Epoch [32/120    avg_loss:0.502, val_acc:0.776]
Epoch [33/120    avg_loss:0.435, val_acc:0.811]
Epoch [34/120    avg_loss:0.327, val_acc:0.870]
Epoch [35/120    avg_loss:0.264, val_acc:0.877]
Epoch [36/120    avg_loss:0.241, val_acc:0.895]
Epoch [37/120    avg_loss:0.246, val_acc:0.897]
Epoch [38/120    avg_loss:0.219, val_acc:0.897]
Epoch [39/120    avg_loss:0.210, val_acc:0.892]
Epoch [40/120    avg_loss:0.191, val_acc:0.910]
Epoch [41/120    avg_loss:0.177, val_acc:0.915]
Epoch [42/120    avg_loss:0.163, val_acc:0.908]
Epoch [43/120    avg_loss:0.151, val_acc:0.926]
Epoch [44/120    avg_loss:0.138, val_acc:0.931]
Epoch [45/120    avg_loss:0.143, val_acc:0.892]
Epoch [46/120    avg_loss:0.173, val_acc:0.926]
Epoch [47/120    avg_loss:0.140, val_acc:0.906]
Epoch [48/120    avg_loss:0.182, val_acc:0.917]
Epoch [49/120    avg_loss:0.161, val_acc:0.907]
Epoch [50/120    avg_loss:0.163, val_acc:0.917]
Epoch [51/120    avg_loss:0.150, val_acc:0.914]
Epoch [52/120    avg_loss:0.101, val_acc:0.936]
Epoch [53/120    avg_loss:0.109, val_acc:0.947]
Epoch [54/120    avg_loss:0.117, val_acc:0.936]
Epoch [55/120    avg_loss:0.092, val_acc:0.939]
Epoch [56/120    avg_loss:0.098, val_acc:0.949]
Epoch [57/120    avg_loss:0.073, val_acc:0.947]
Epoch [58/120    avg_loss:0.077, val_acc:0.949]
Epoch [59/120    avg_loss:0.097, val_acc:0.949]
Epoch [60/120    avg_loss:0.085, val_acc:0.925]
Epoch [61/120    avg_loss:0.093, val_acc:0.934]
Epoch [62/120    avg_loss:0.093, val_acc:0.943]
Epoch [63/120    avg_loss:0.107, val_acc:0.938]
Epoch [64/120    avg_loss:0.085, val_acc:0.940]
Epoch [65/120    avg_loss:0.084, val_acc:0.940]
Epoch [66/120    avg_loss:0.071, val_acc:0.955]
Epoch [67/120    avg_loss:0.065, val_acc:0.943]
Epoch [68/120    avg_loss:0.075, val_acc:0.944]
Epoch [69/120    avg_loss:0.052, val_acc:0.957]
Epoch [70/120    avg_loss:0.057, val_acc:0.960]
Epoch [71/120    avg_loss:0.054, val_acc:0.956]
Epoch [72/120    avg_loss:0.057, val_acc:0.930]
Epoch [73/120    avg_loss:0.052, val_acc:0.951]
Epoch [74/120    avg_loss:0.044, val_acc:0.936]
Epoch [75/120    avg_loss:0.057, val_acc:0.953]
Epoch [76/120    avg_loss:0.045, val_acc:0.956]
Epoch [77/120    avg_loss:0.041, val_acc:0.945]
Epoch [78/120    avg_loss:0.065, val_acc:0.943]
Epoch [79/120    avg_loss:0.058, val_acc:0.939]
Epoch [80/120    avg_loss:0.046, val_acc:0.952]
Epoch [81/120    avg_loss:0.056, val_acc:0.950]
Epoch [82/120    avg_loss:0.055, val_acc:0.957]
Epoch [83/120    avg_loss:0.043, val_acc:0.960]
Epoch [84/120    avg_loss:0.043, val_acc:0.961]
Epoch [85/120    avg_loss:0.048, val_acc:0.953]
Epoch [86/120    avg_loss:0.072, val_acc:0.940]
Epoch [87/120    avg_loss:0.040, val_acc:0.955]
Epoch [88/120    avg_loss:0.032, val_acc:0.957]
Epoch [89/120    avg_loss:0.053, val_acc:0.950]
Epoch [90/120    avg_loss:0.037, val_acc:0.952]
Epoch [91/120    avg_loss:0.040, val_acc:0.957]
Epoch [92/120    avg_loss:0.053, val_acc:0.949]
Epoch [93/120    avg_loss:0.087, val_acc:0.931]
Epoch [94/120    avg_loss:0.055, val_acc:0.960]
Epoch [95/120    avg_loss:0.042, val_acc:0.966]
Epoch [96/120    avg_loss:0.036, val_acc:0.945]
Epoch [97/120    avg_loss:0.029, val_acc:0.960]
Epoch [98/120    avg_loss:0.029, val_acc:0.964]
Epoch [99/120    avg_loss:0.027, val_acc:0.963]
Epoch [100/120    avg_loss:0.030, val_acc:0.965]
Epoch [101/120    avg_loss:0.021, val_acc:0.960]
Epoch [102/120    avg_loss:0.023, val_acc:0.965]
Epoch [103/120    avg_loss:0.023, val_acc:0.958]
Epoch [104/120    avg_loss:0.021, val_acc:0.955]
Epoch [105/120    avg_loss:0.022, val_acc:0.963]
Epoch [106/120    avg_loss:0.023, val_acc:0.961]
Epoch [107/120    avg_loss:0.026, val_acc:0.956]
Epoch [108/120    avg_loss:0.030, val_acc:0.957]
Epoch [109/120    avg_loss:0.029, val_acc:0.960]
Epoch [110/120    avg_loss:0.023, val_acc:0.964]
Epoch [111/120    avg_loss:0.023, val_acc:0.967]
Epoch [112/120    avg_loss:0.017, val_acc:0.966]
Epoch [113/120    avg_loss:0.015, val_acc:0.967]
Epoch [114/120    avg_loss:0.016, val_acc:0.967]
Epoch [115/120    avg_loss:0.016, val_acc:0.966]
Epoch [116/120    avg_loss:0.014, val_acc:0.968]
Epoch [117/120    avg_loss:0.017, val_acc:0.966]
Epoch [118/120    avg_loss:0.015, val_acc:0.967]
Epoch [119/120    avg_loss:0.016, val_acc:0.968]
Epoch [120/120    avg_loss:0.018, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    1 1246    3    0    0    2    0    0    0    6   25    0    0
     0    2    0]
 [   0    0    6  697    3    4    0    0    0   18    1    0   14    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   10   40    0    2    0    0    0    0  803   17    0    0
     0    3    0]
 [   0    0   12    0    0    0    2    0    0    0   22 2168    4    2
     0    0    0]
 [   0    0    1   28   11    0    0    0    0    0   17    0  473    0
     0    0    4]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    3    0    0    0
  1129    0    0]
 [   0    0    0    0    0    0   19    0    0    4    0    0    3    0
    37  284    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.22764227642277

F1 scores:
[       nan 0.95       0.9734375  0.92013201 0.96818182 0.98412698
 0.98050975 1.         0.99767442 0.56140351 0.9283237  0.98033009
 0.91755577 0.98133333 0.97960954 0.89308176 0.97674419]

Kappa:
0.9569883921446921
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdb3d8e2828>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.736, val_acc:0.406]
Epoch [2/120    avg_loss:2.465, val_acc:0.416]
Epoch [3/120    avg_loss:2.292, val_acc:0.497]
Epoch [4/120    avg_loss:2.141, val_acc:0.525]
Epoch [5/120    avg_loss:2.017, val_acc:0.548]
Epoch [6/120    avg_loss:1.853, val_acc:0.568]
Epoch [7/120    avg_loss:1.785, val_acc:0.582]
Epoch [8/120    avg_loss:1.610, val_acc:0.593]
Epoch [9/120    avg_loss:1.478, val_acc:0.633]
Epoch [10/120    avg_loss:1.399, val_acc:0.665]
Epoch [11/120    avg_loss:1.304, val_acc:0.672]
Epoch [12/120    avg_loss:1.196, val_acc:0.673]
Epoch [13/120    avg_loss:1.107, val_acc:0.686]
Epoch [14/120    avg_loss:1.001, val_acc:0.680]
Epoch [15/120    avg_loss:0.917, val_acc:0.739]
Epoch [16/120    avg_loss:0.833, val_acc:0.762]
Epoch [17/120    avg_loss:0.753, val_acc:0.764]
Epoch [18/120    avg_loss:0.662, val_acc:0.793]
Epoch [19/120    avg_loss:0.686, val_acc:0.762]
Epoch [20/120    avg_loss:0.635, val_acc:0.811]
Epoch [21/120    avg_loss:0.593, val_acc:0.805]
Epoch [22/120    avg_loss:0.676, val_acc:0.722]
Epoch [23/120    avg_loss:0.593, val_acc:0.814]
Epoch [24/120    avg_loss:0.466, val_acc:0.844]
Epoch [25/120    avg_loss:0.465, val_acc:0.859]
Epoch [26/120    avg_loss:0.456, val_acc:0.852]
Epoch [27/120    avg_loss:0.387, val_acc:0.866]
Epoch [28/120    avg_loss:0.368, val_acc:0.891]
Epoch [29/120    avg_loss:0.332, val_acc:0.882]
Epoch [30/120    avg_loss:0.340, val_acc:0.874]
Epoch [31/120    avg_loss:0.334, val_acc:0.870]
Epoch [32/120    avg_loss:0.314, val_acc:0.886]
Epoch [33/120    avg_loss:0.249, val_acc:0.883]
Epoch [34/120    avg_loss:0.227, val_acc:0.906]
Epoch [35/120    avg_loss:0.227, val_acc:0.909]
Epoch [36/120    avg_loss:0.207, val_acc:0.906]
Epoch [37/120    avg_loss:0.174, val_acc:0.923]
Epoch [38/120    avg_loss:0.195, val_acc:0.911]
Epoch [39/120    avg_loss:0.200, val_acc:0.916]
Epoch [40/120    avg_loss:0.201, val_acc:0.909]
Epoch [41/120    avg_loss:0.184, val_acc:0.932]
Epoch [42/120    avg_loss:0.166, val_acc:0.925]
Epoch [43/120    avg_loss:0.177, val_acc:0.924]
Epoch [44/120    avg_loss:0.138, val_acc:0.944]
Epoch [45/120    avg_loss:0.136, val_acc:0.940]
Epoch [46/120    avg_loss:0.115, val_acc:0.935]
Epoch [47/120    avg_loss:0.116, val_acc:0.907]
Epoch [48/120    avg_loss:0.160, val_acc:0.915]
Epoch [49/120    avg_loss:0.174, val_acc:0.912]
Epoch [50/120    avg_loss:0.196, val_acc:0.920]
Epoch [51/120    avg_loss:0.149, val_acc:0.924]
Epoch [52/120    avg_loss:0.132, val_acc:0.934]
Epoch [53/120    avg_loss:0.098, val_acc:0.942]
Epoch [54/120    avg_loss:0.112, val_acc:0.932]
Epoch [55/120    avg_loss:0.104, val_acc:0.930]
Epoch [56/120    avg_loss:0.096, val_acc:0.939]
Epoch [57/120    avg_loss:0.106, val_acc:0.935]
Epoch [58/120    avg_loss:0.078, val_acc:0.936]
Epoch [59/120    avg_loss:0.068, val_acc:0.950]
Epoch [60/120    avg_loss:0.060, val_acc:0.953]
Epoch [61/120    avg_loss:0.064, val_acc:0.955]
Epoch [62/120    avg_loss:0.069, val_acc:0.959]
Epoch [63/120    avg_loss:0.058, val_acc:0.958]
Epoch [64/120    avg_loss:0.054, val_acc:0.953]
Epoch [65/120    avg_loss:0.054, val_acc:0.955]
Epoch [66/120    avg_loss:0.049, val_acc:0.957]
Epoch [67/120    avg_loss:0.061, val_acc:0.956]
Epoch [68/120    avg_loss:0.060, val_acc:0.956]
Epoch [69/120    avg_loss:0.062, val_acc:0.956]
Epoch [70/120    avg_loss:0.052, val_acc:0.956]
Epoch [71/120    avg_loss:0.056, val_acc:0.953]
Epoch [72/120    avg_loss:0.051, val_acc:0.953]
Epoch [73/120    avg_loss:0.054, val_acc:0.956]
Epoch [74/120    avg_loss:0.046, val_acc:0.961]
Epoch [75/120    avg_loss:0.055, val_acc:0.957]
Epoch [76/120    avg_loss:0.043, val_acc:0.958]
Epoch [77/120    avg_loss:0.051, val_acc:0.957]
Epoch [78/120    avg_loss:0.051, val_acc:0.957]
Epoch [79/120    avg_loss:0.055, val_acc:0.953]
Epoch [80/120    avg_loss:0.045, val_acc:0.957]
Epoch [81/120    avg_loss:0.049, val_acc:0.952]
Epoch [82/120    avg_loss:0.050, val_acc:0.953]
Epoch [83/120    avg_loss:0.044, val_acc:0.953]
Epoch [84/120    avg_loss:0.044, val_acc:0.957]
Epoch [85/120    avg_loss:0.052, val_acc:0.959]
Epoch [86/120    avg_loss:0.048, val_acc:0.965]
Epoch [87/120    avg_loss:0.050, val_acc:0.958]
Epoch [88/120    avg_loss:0.049, val_acc:0.957]
Epoch [89/120    avg_loss:0.051, val_acc:0.958]
Epoch [90/120    avg_loss:0.045, val_acc:0.958]
Epoch [91/120    avg_loss:0.050, val_acc:0.956]
Epoch [92/120    avg_loss:0.048, val_acc:0.957]
Epoch [93/120    avg_loss:0.050, val_acc:0.963]
Epoch [94/120    avg_loss:0.048, val_acc:0.963]
Epoch [95/120    avg_loss:0.041, val_acc:0.957]
Epoch [96/120    avg_loss:0.051, val_acc:0.959]
Epoch [97/120    avg_loss:0.048, val_acc:0.956]
Epoch [98/120    avg_loss:0.039, val_acc:0.959]
Epoch [99/120    avg_loss:0.048, val_acc:0.961]
Epoch [100/120    avg_loss:0.047, val_acc:0.961]
Epoch [101/120    avg_loss:0.043, val_acc:0.961]
Epoch [102/120    avg_loss:0.044, val_acc:0.961]
Epoch [103/120    avg_loss:0.043, val_acc:0.961]
Epoch [104/120    avg_loss:0.045, val_acc:0.961]
Epoch [105/120    avg_loss:0.051, val_acc:0.961]
Epoch [106/120    avg_loss:0.051, val_acc:0.961]
Epoch [107/120    avg_loss:0.042, val_acc:0.961]
Epoch [108/120    avg_loss:0.042, val_acc:0.961]
Epoch [109/120    avg_loss:0.039, val_acc:0.961]
Epoch [110/120    avg_loss:0.044, val_acc:0.963]
Epoch [111/120    avg_loss:0.039, val_acc:0.963]
Epoch [112/120    avg_loss:0.052, val_acc:0.963]
Epoch [113/120    avg_loss:0.038, val_acc:0.963]
Epoch [114/120    avg_loss:0.038, val_acc:0.963]
Epoch [115/120    avg_loss:0.046, val_acc:0.963]
Epoch [116/120    avg_loss:0.049, val_acc:0.963]
Epoch [117/120    avg_loss:0.043, val_acc:0.963]
Epoch [118/120    avg_loss:0.035, val_acc:0.963]
Epoch [119/120    avg_loss:0.042, val_acc:0.963]
Epoch [120/120    avg_loss:0.047, val_acc:0.963]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1231    3    0    0    4    0    0    3    6   31    1    0
     0    6    0]
 [   0    0    1  711    3   11    0    0    0   14    1    0    1    5
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    4    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   46   76    0    7    0    0    0    0  722   13    0    0
     0   11    0]
 [   0    0   27    0    0    3    7    0    0    0   20 2146    0    5
     2    0    0]
 [   0    0    1    3    9   15    0    0    0    0    5    2  489    0
     1    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    1    0    2    1    0    0
  1133    0    0]
 [   0    0    0    0    0    0   13    0    0    4    0    0    0    0
    45  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.28455284552845

F1 scores:
[       nan 0.94871795 0.95021227 0.92337662 0.97260274 0.95005549
 0.97977528 0.98039216 0.99883856 0.56666667 0.88318043 0.97456857
 0.95321637 0.97368421 0.97504303 0.87827427 0.94915254]

Kappa:
0.9462591464411694
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f85d57457b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.714, val_acc:0.415]
Epoch [2/120    avg_loss:2.406, val_acc:0.463]
Epoch [3/120    avg_loss:2.231, val_acc:0.518]
Epoch [4/120    avg_loss:2.095, val_acc:0.510]
Epoch [5/120    avg_loss:1.990, val_acc:0.542]
Epoch [6/120    avg_loss:1.893, val_acc:0.570]
Epoch [7/120    avg_loss:1.782, val_acc:0.591]
Epoch [8/120    avg_loss:1.616, val_acc:0.605]
Epoch [9/120    avg_loss:1.547, val_acc:0.603]
Epoch [10/120    avg_loss:1.437, val_acc:0.637]
Epoch [11/120    avg_loss:1.371, val_acc:0.650]
Epoch [12/120    avg_loss:1.279, val_acc:0.635]
Epoch [13/120    avg_loss:1.163, val_acc:0.709]
Epoch [14/120    avg_loss:1.086, val_acc:0.690]
Epoch [15/120    avg_loss:0.963, val_acc:0.698]
Epoch [16/120    avg_loss:0.897, val_acc:0.766]
Epoch [17/120    avg_loss:0.823, val_acc:0.747]
Epoch [18/120    avg_loss:0.781, val_acc:0.754]
Epoch [19/120    avg_loss:0.717, val_acc:0.797]
Epoch [20/120    avg_loss:0.756, val_acc:0.773]
Epoch [21/120    avg_loss:0.659, val_acc:0.823]
Epoch [22/120    avg_loss:0.576, val_acc:0.818]
Epoch [23/120    avg_loss:0.593, val_acc:0.864]
Epoch [24/120    avg_loss:0.489, val_acc:0.852]
Epoch [25/120    avg_loss:0.473, val_acc:0.852]
Epoch [26/120    avg_loss:0.434, val_acc:0.865]
Epoch [27/120    avg_loss:0.369, val_acc:0.867]
Epoch [28/120    avg_loss:0.436, val_acc:0.829]
Epoch [29/120    avg_loss:0.353, val_acc:0.891]
Epoch [30/120    avg_loss:0.362, val_acc:0.875]
Epoch [31/120    avg_loss:0.293, val_acc:0.902]
Epoch [32/120    avg_loss:0.267, val_acc:0.896]
Epoch [33/120    avg_loss:0.250, val_acc:0.907]
Epoch [34/120    avg_loss:0.266, val_acc:0.873]
Epoch [35/120    avg_loss:0.284, val_acc:0.866]
Epoch [36/120    avg_loss:0.271, val_acc:0.901]
Epoch [37/120    avg_loss:0.254, val_acc:0.907]
Epoch [38/120    avg_loss:0.218, val_acc:0.903]
Epoch [39/120    avg_loss:0.185, val_acc:0.902]
Epoch [40/120    avg_loss:0.197, val_acc:0.905]
Epoch [41/120    avg_loss:0.187, val_acc:0.924]
Epoch [42/120    avg_loss:0.213, val_acc:0.900]
Epoch [43/120    avg_loss:0.190, val_acc:0.924]
Epoch [44/120    avg_loss:0.155, val_acc:0.929]
Epoch [45/120    avg_loss:0.139, val_acc:0.915]
Epoch [46/120    avg_loss:0.126, val_acc:0.940]
Epoch [47/120    avg_loss:0.119, val_acc:0.933]
Epoch [48/120    avg_loss:0.121, val_acc:0.917]
Epoch [49/120    avg_loss:0.134, val_acc:0.934]
Epoch [50/120    avg_loss:0.121, val_acc:0.935]
Epoch [51/120    avg_loss:0.140, val_acc:0.942]
Epoch [52/120    avg_loss:0.117, val_acc:0.928]
Epoch [53/120    avg_loss:0.093, val_acc:0.943]
Epoch [54/120    avg_loss:0.166, val_acc:0.921]
Epoch [55/120    avg_loss:0.129, val_acc:0.934]
Epoch [56/120    avg_loss:0.119, val_acc:0.935]
Epoch [57/120    avg_loss:0.096, val_acc:0.938]
Epoch [58/120    avg_loss:0.104, val_acc:0.942]
Epoch [59/120    avg_loss:0.094, val_acc:0.943]
Epoch [60/120    avg_loss:0.079, val_acc:0.948]
Epoch [61/120    avg_loss:0.076, val_acc:0.952]
Epoch [62/120    avg_loss:0.067, val_acc:0.947]
Epoch [63/120    avg_loss:0.072, val_acc:0.954]
Epoch [64/120    avg_loss:0.072, val_acc:0.953]
Epoch [65/120    avg_loss:0.070, val_acc:0.946]
Epoch [66/120    avg_loss:0.074, val_acc:0.945]
Epoch [67/120    avg_loss:0.088, val_acc:0.951]
Epoch [68/120    avg_loss:0.076, val_acc:0.948]
Epoch [69/120    avg_loss:0.077, val_acc:0.951]
Epoch [70/120    avg_loss:0.062, val_acc:0.952]
Epoch [71/120    avg_loss:0.058, val_acc:0.946]
Epoch [72/120    avg_loss:0.057, val_acc:0.960]
Epoch [73/120    avg_loss:0.050, val_acc:0.955]
Epoch [74/120    avg_loss:0.060, val_acc:0.957]
Epoch [75/120    avg_loss:0.070, val_acc:0.959]
Epoch [76/120    avg_loss:0.051, val_acc:0.943]
Epoch [77/120    avg_loss:0.067, val_acc:0.951]
Epoch [78/120    avg_loss:0.056, val_acc:0.954]
Epoch [79/120    avg_loss:0.052, val_acc:0.946]
Epoch [80/120    avg_loss:0.068, val_acc:0.955]
Epoch [81/120    avg_loss:0.056, val_acc:0.943]
Epoch [82/120    avg_loss:0.074, val_acc:0.946]
Epoch [83/120    avg_loss:0.059, val_acc:0.950]
Epoch [84/120    avg_loss:0.056, val_acc:0.971]
Epoch [85/120    avg_loss:0.039, val_acc:0.963]
Epoch [86/120    avg_loss:0.049, val_acc:0.958]
Epoch [87/120    avg_loss:0.053, val_acc:0.945]
Epoch [88/120    avg_loss:0.054, val_acc:0.953]
Epoch [89/120    avg_loss:0.044, val_acc:0.962]
Epoch [90/120    avg_loss:0.038, val_acc:0.946]
Epoch [91/120    avg_loss:0.056, val_acc:0.963]
Epoch [92/120    avg_loss:0.043, val_acc:0.966]
Epoch [93/120    avg_loss:0.032, val_acc:0.966]
Epoch [94/120    avg_loss:0.055, val_acc:0.939]
Epoch [95/120    avg_loss:0.180, val_acc:0.917]
Epoch [96/120    avg_loss:0.150, val_acc:0.927]
Epoch [97/120    avg_loss:0.102, val_acc:0.928]
Epoch [98/120    avg_loss:0.086, val_acc:0.949]
Epoch [99/120    avg_loss:0.057, val_acc:0.953]
Epoch [100/120    avg_loss:0.052, val_acc:0.960]
Epoch [101/120    avg_loss:0.050, val_acc:0.959]
Epoch [102/120    avg_loss:0.041, val_acc:0.966]
Epoch [103/120    avg_loss:0.035, val_acc:0.964]
Epoch [104/120    avg_loss:0.040, val_acc:0.966]
Epoch [105/120    avg_loss:0.048, val_acc:0.961]
Epoch [106/120    avg_loss:0.045, val_acc:0.960]
Epoch [107/120    avg_loss:0.045, val_acc:0.966]
Epoch [108/120    avg_loss:0.034, val_acc:0.966]
Epoch [109/120    avg_loss:0.038, val_acc:0.967]
Epoch [110/120    avg_loss:0.034, val_acc:0.966]
Epoch [111/120    avg_loss:0.035, val_acc:0.966]
Epoch [112/120    avg_loss:0.035, val_acc:0.966]
Epoch [113/120    avg_loss:0.039, val_acc:0.966]
Epoch [114/120    avg_loss:0.038, val_acc:0.966]
Epoch [115/120    avg_loss:0.037, val_acc:0.968]
Epoch [116/120    avg_loss:0.035, val_acc:0.967]
Epoch [117/120    avg_loss:0.032, val_acc:0.967]
Epoch [118/120    avg_loss:0.032, val_acc:0.967]
Epoch [119/120    avg_loss:0.034, val_acc:0.967]
Epoch [120/120    avg_loss:0.037, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1223    8    4    0    0    0    0    0   19   29    2    0
     0    0    0]
 [   0    0    1  721    0    6    0    0    0    7    1    0   10    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    8    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    5    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   35   49    0    5    0    0    0    0  764    9    2    0
     1   10    0]
 [   0    0   10    0    0    3    5    0    2    0   17 2162    3    4
     4    0    0]
 [   0    0    0   12    1    2    0    0    0    0   12    2  501    0
     0    0    4]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    3    0    0    3    0    1    2    0    0
  1130    0    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    51  290    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.97831978319783

F1 scores:
[       nan 0.975      0.95771339 0.93819128 0.98839907 0.96237172
 0.98787879 0.86206897 0.99421965 0.74418605 0.90360733 0.97850192
 0.94886364 0.98395722 0.97079038 0.89644513 0.96470588]

Kappa:
0.9541471588588343
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f96b64f17f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.746, val_acc:0.352]
Epoch [2/120    avg_loss:2.439, val_acc:0.466]
Epoch [3/120    avg_loss:2.303, val_acc:0.497]
Epoch [4/120    avg_loss:2.158, val_acc:0.515]
Epoch [5/120    avg_loss:2.012, val_acc:0.507]
Epoch [6/120    avg_loss:1.914, val_acc:0.564]
Epoch [7/120    avg_loss:1.834, val_acc:0.608]
Epoch [8/120    avg_loss:1.732, val_acc:0.602]
Epoch [9/120    avg_loss:1.656, val_acc:0.622]
Epoch [10/120    avg_loss:1.495, val_acc:0.605]
Epoch [11/120    avg_loss:1.437, val_acc:0.664]
Epoch [12/120    avg_loss:1.231, val_acc:0.669]
Epoch [13/120    avg_loss:1.159, val_acc:0.641]
Epoch [14/120    avg_loss:1.116, val_acc:0.657]
Epoch [15/120    avg_loss:1.028, val_acc:0.675]
Epoch [16/120    avg_loss:0.886, val_acc:0.716]
Epoch [17/120    avg_loss:0.799, val_acc:0.764]
Epoch [18/120    avg_loss:0.705, val_acc:0.769]
Epoch [19/120    avg_loss:0.682, val_acc:0.803]
Epoch [20/120    avg_loss:0.650, val_acc:0.786]
Epoch [21/120    avg_loss:0.639, val_acc:0.793]
Epoch [22/120    avg_loss:0.676, val_acc:0.790]
Epoch [23/120    avg_loss:0.653, val_acc:0.783]
Epoch [24/120    avg_loss:0.512, val_acc:0.822]
Epoch [25/120    avg_loss:0.472, val_acc:0.817]
Epoch [26/120    avg_loss:0.435, val_acc:0.839]
Epoch [27/120    avg_loss:0.418, val_acc:0.859]
Epoch [28/120    avg_loss:0.366, val_acc:0.863]
Epoch [29/120    avg_loss:0.332, val_acc:0.885]
Epoch [30/120    avg_loss:0.303, val_acc:0.892]
Epoch [31/120    avg_loss:0.277, val_acc:0.890]
Epoch [32/120    avg_loss:0.309, val_acc:0.886]
Epoch [33/120    avg_loss:0.284, val_acc:0.899]
Epoch [34/120    avg_loss:0.262, val_acc:0.895]
Epoch [35/120    avg_loss:0.250, val_acc:0.890]
Epoch [36/120    avg_loss:0.232, val_acc:0.892]
Epoch [37/120    avg_loss:0.253, val_acc:0.891]
Epoch [38/120    avg_loss:0.211, val_acc:0.930]
Epoch [39/120    avg_loss:0.183, val_acc:0.928]
Epoch [40/120    avg_loss:0.192, val_acc:0.919]
Epoch [41/120    avg_loss:0.201, val_acc:0.900]
Epoch [42/120    avg_loss:0.182, val_acc:0.914]
Epoch [43/120    avg_loss:0.167, val_acc:0.908]
Epoch [44/120    avg_loss:0.157, val_acc:0.924]
Epoch [45/120    avg_loss:0.168, val_acc:0.924]
Epoch [46/120    avg_loss:0.132, val_acc:0.923]
Epoch [47/120    avg_loss:0.151, val_acc:0.936]
Epoch [48/120    avg_loss:0.153, val_acc:0.933]
Epoch [49/120    avg_loss:0.194, val_acc:0.932]
Epoch [50/120    avg_loss:0.193, val_acc:0.933]
Epoch [51/120    avg_loss:0.247, val_acc:0.860]
Epoch [52/120    avg_loss:0.295, val_acc:0.916]
Epoch [53/120    avg_loss:0.300, val_acc:0.920]
Epoch [54/120    avg_loss:0.227, val_acc:0.897]
Epoch [55/120    avg_loss:0.187, val_acc:0.924]
Epoch [56/120    avg_loss:0.149, val_acc:0.911]
Epoch [57/120    avg_loss:0.134, val_acc:0.936]
Epoch [58/120    avg_loss:0.130, val_acc:0.932]
Epoch [59/120    avg_loss:0.178, val_acc:0.923]
Epoch [60/120    avg_loss:0.156, val_acc:0.936]
Epoch [61/120    avg_loss:0.119, val_acc:0.922]
Epoch [62/120    avg_loss:0.134, val_acc:0.944]
Epoch [63/120    avg_loss:0.093, val_acc:0.944]
Epoch [64/120    avg_loss:0.077, val_acc:0.953]
Epoch [65/120    avg_loss:0.073, val_acc:0.950]
Epoch [66/120    avg_loss:0.072, val_acc:0.957]
Epoch [67/120    avg_loss:0.071, val_acc:0.949]
Epoch [68/120    avg_loss:0.071, val_acc:0.957]
Epoch [69/120    avg_loss:0.067, val_acc:0.960]
Epoch [70/120    avg_loss:0.075, val_acc:0.953]
Epoch [71/120    avg_loss:0.070, val_acc:0.953]
Epoch [72/120    avg_loss:0.082, val_acc:0.961]
Epoch [73/120    avg_loss:0.068, val_acc:0.952]
Epoch [74/120    avg_loss:0.051, val_acc:0.959]
Epoch [75/120    avg_loss:0.052, val_acc:0.958]
Epoch [76/120    avg_loss:0.046, val_acc:0.956]
Epoch [77/120    avg_loss:0.057, val_acc:0.957]
Epoch [78/120    avg_loss:0.053, val_acc:0.964]
Epoch [79/120    avg_loss:0.045, val_acc:0.960]
Epoch [80/120    avg_loss:0.050, val_acc:0.952]
Epoch [81/120    avg_loss:0.043, val_acc:0.961]
Epoch [82/120    avg_loss:0.055, val_acc:0.959]
Epoch [83/120    avg_loss:0.041, val_acc:0.957]
Epoch [84/120    avg_loss:0.055, val_acc:0.953]
Epoch [85/120    avg_loss:0.042, val_acc:0.958]
Epoch [86/120    avg_loss:0.053, val_acc:0.963]
Epoch [87/120    avg_loss:0.038, val_acc:0.960]
Epoch [88/120    avg_loss:0.047, val_acc:0.968]
Epoch [89/120    avg_loss:0.043, val_acc:0.953]
Epoch [90/120    avg_loss:0.035, val_acc:0.965]
Epoch [91/120    avg_loss:0.035, val_acc:0.964]
Epoch [92/120    avg_loss:0.036, val_acc:0.961]
Epoch [93/120    avg_loss:0.035, val_acc:0.966]
Epoch [94/120    avg_loss:0.029, val_acc:0.970]
Epoch [95/120    avg_loss:0.040, val_acc:0.963]
Epoch [96/120    avg_loss:0.036, val_acc:0.965]
Epoch [97/120    avg_loss:0.033, val_acc:0.961]
Epoch [98/120    avg_loss:0.038, val_acc:0.964]
Epoch [99/120    avg_loss:0.038, val_acc:0.970]
Epoch [100/120    avg_loss:0.028, val_acc:0.968]
Epoch [101/120    avg_loss:0.045, val_acc:0.973]
Epoch [102/120    avg_loss:0.112, val_acc:0.945]
Epoch [103/120    avg_loss:0.092, val_acc:0.936]
Epoch [104/120    avg_loss:0.089, val_acc:0.931]
Epoch [105/120    avg_loss:0.066, val_acc:0.957]
Epoch [106/120    avg_loss:0.059, val_acc:0.959]
Epoch [107/120    avg_loss:0.053, val_acc:0.968]
Epoch [108/120    avg_loss:0.071, val_acc:0.955]
Epoch [109/120    avg_loss:0.057, val_acc:0.958]
Epoch [110/120    avg_loss:0.074, val_acc:0.957]
Epoch [111/120    avg_loss:0.044, val_acc:0.966]
Epoch [112/120    avg_loss:0.033, val_acc:0.969]
Epoch [113/120    avg_loss:0.027, val_acc:0.982]
Epoch [114/120    avg_loss:0.027, val_acc:0.972]
Epoch [115/120    avg_loss:0.034, val_acc:0.966]
Epoch [116/120    avg_loss:0.036, val_acc:0.966]
Epoch [117/120    avg_loss:0.051, val_acc:0.964]
Epoch [118/120    avg_loss:0.033, val_acc:0.974]
Epoch [119/120    avg_loss:0.029, val_acc:0.973]
Epoch [120/120    avg_loss:0.025, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1191   18    0    0    1    0    0    0   30   42    3    0
     0    0    0]
 [   0    0    0  691    0    7    0    0    0    5    0    1   43    0
     0    0    0]
 [   0    0    0    6  207    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    4    0    2    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    5    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    6   71    0    5    0    0    0    0  763   28    2    0
     0    0    0]
 [   0    0    3    0    0    0    3    0    0    0   26 2170    5    3
     0    0    0]
 [   0    0    1    2    2    5    0    0    0    0    1    8  511    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    1    0    2    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0   25    0    0    2    0    0    0    0
    43  277    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.39295392953929

F1 scores:
[       nan 0.93506494 0.95816573 0.90032573 0.98104265 0.97052154
 0.97458894 0.92592593 0.99883856 0.77272727 0.89659224 0.97200448
 0.92909091 0.9919571  0.97927461 0.88782051 0.97076023]

Kappa:
0.9474490000462026
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1405d6f828>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.721, val_acc:0.395]
Epoch [2/120    avg_loss:2.429, val_acc:0.472]
Epoch [3/120    avg_loss:2.240, val_acc:0.509]
Epoch [4/120    avg_loss:2.110, val_acc:0.524]
Epoch [5/120    avg_loss:2.001, val_acc:0.548]
Epoch [6/120    avg_loss:1.877, val_acc:0.567]
Epoch [7/120    avg_loss:1.771, val_acc:0.559]
Epoch [8/120    avg_loss:1.684, val_acc:0.602]
Epoch [9/120    avg_loss:1.596, val_acc:0.618]
Epoch [10/120    avg_loss:1.477, val_acc:0.642]
Epoch [11/120    avg_loss:1.349, val_acc:0.631]
Epoch [12/120    avg_loss:1.226, val_acc:0.619]
Epoch [13/120    avg_loss:1.176, val_acc:0.708]
Epoch [14/120    avg_loss:1.053, val_acc:0.689]
Epoch [15/120    avg_loss:0.974, val_acc:0.718]
Epoch [16/120    avg_loss:0.908, val_acc:0.691]
Epoch [17/120    avg_loss:0.817, val_acc:0.776]
Epoch [18/120    avg_loss:0.750, val_acc:0.785]
Epoch [19/120    avg_loss:0.744, val_acc:0.794]
Epoch [20/120    avg_loss:0.654, val_acc:0.799]
Epoch [21/120    avg_loss:0.609, val_acc:0.795]
Epoch [22/120    avg_loss:0.607, val_acc:0.809]
Epoch [23/120    avg_loss:0.590, val_acc:0.825]
Epoch [24/120    avg_loss:0.511, val_acc:0.826]
Epoch [25/120    avg_loss:0.463, val_acc:0.855]
Epoch [26/120    avg_loss:0.448, val_acc:0.847]
Epoch [27/120    avg_loss:0.434, val_acc:0.828]
Epoch [28/120    avg_loss:0.448, val_acc:0.839]
Epoch [29/120    avg_loss:0.412, val_acc:0.834]
Epoch [30/120    avg_loss:0.324, val_acc:0.864]
Epoch [31/120    avg_loss:0.283, val_acc:0.887]
Epoch [32/120    avg_loss:0.258, val_acc:0.907]
Epoch [33/120    avg_loss:0.286, val_acc:0.890]
Epoch [34/120    avg_loss:0.289, val_acc:0.865]
Epoch [35/120    avg_loss:0.310, val_acc:0.880]
Epoch [36/120    avg_loss:0.291, val_acc:0.882]
Epoch [37/120    avg_loss:0.255, val_acc:0.894]
Epoch [38/120    avg_loss:0.222, val_acc:0.902]
Epoch [39/120    avg_loss:0.214, val_acc:0.886]
Epoch [40/120    avg_loss:0.208, val_acc:0.890]
Epoch [41/120    avg_loss:0.258, val_acc:0.892]
Epoch [42/120    avg_loss:0.216, val_acc:0.905]
Epoch [43/120    avg_loss:0.199, val_acc:0.910]
Epoch [44/120    avg_loss:0.171, val_acc:0.885]
Epoch [45/120    avg_loss:0.185, val_acc:0.932]
Epoch [46/120    avg_loss:0.165, val_acc:0.924]
Epoch [47/120    avg_loss:0.149, val_acc:0.927]
Epoch [48/120    avg_loss:0.221, val_acc:0.906]
Epoch [49/120    avg_loss:0.186, val_acc:0.890]
Epoch [50/120    avg_loss:0.155, val_acc:0.928]
Epoch [51/120    avg_loss:0.141, val_acc:0.919]
Epoch [52/120    avg_loss:0.127, val_acc:0.933]
Epoch [53/120    avg_loss:0.113, val_acc:0.942]
Epoch [54/120    avg_loss:0.111, val_acc:0.939]
Epoch [55/120    avg_loss:0.119, val_acc:0.930]
Epoch [56/120    avg_loss:0.139, val_acc:0.940]
Epoch [57/120    avg_loss:0.092, val_acc:0.941]
Epoch [58/120    avg_loss:0.097, val_acc:0.938]
Epoch [59/120    avg_loss:0.088, val_acc:0.941]
Epoch [60/120    avg_loss:0.078, val_acc:0.934]
Epoch [61/120    avg_loss:0.089, val_acc:0.936]
Epoch [62/120    avg_loss:0.084, val_acc:0.942]
Epoch [63/120    avg_loss:0.067, val_acc:0.947]
Epoch [64/120    avg_loss:0.061, val_acc:0.948]
Epoch [65/120    avg_loss:0.070, val_acc:0.951]
Epoch [66/120    avg_loss:0.071, val_acc:0.944]
Epoch [67/120    avg_loss:0.093, val_acc:0.930]
Epoch [68/120    avg_loss:0.089, val_acc:0.940]
Epoch [69/120    avg_loss:0.067, val_acc:0.953]
Epoch [70/120    avg_loss:0.062, val_acc:0.950]
Epoch [71/120    avg_loss:0.049, val_acc:0.963]
Epoch [72/120    avg_loss:0.051, val_acc:0.957]
Epoch [73/120    avg_loss:0.051, val_acc:0.950]
Epoch [74/120    avg_loss:0.070, val_acc:0.955]
Epoch [75/120    avg_loss:0.050, val_acc:0.955]
Epoch [76/120    avg_loss:0.051, val_acc:0.947]
Epoch [77/120    avg_loss:0.054, val_acc:0.960]
Epoch [78/120    avg_loss:0.037, val_acc:0.960]
Epoch [79/120    avg_loss:0.036, val_acc:0.961]
Epoch [80/120    avg_loss:0.037, val_acc:0.951]
Epoch [81/120    avg_loss:0.031, val_acc:0.960]
Epoch [82/120    avg_loss:0.042, val_acc:0.945]
Epoch [83/120    avg_loss:0.036, val_acc:0.958]
Epoch [84/120    avg_loss:0.039, val_acc:0.959]
Epoch [85/120    avg_loss:0.029, val_acc:0.960]
Epoch [86/120    avg_loss:0.030, val_acc:0.960]
Epoch [87/120    avg_loss:0.023, val_acc:0.960]
Epoch [88/120    avg_loss:0.024, val_acc:0.963]
Epoch [89/120    avg_loss:0.023, val_acc:0.963]
Epoch [90/120    avg_loss:0.026, val_acc:0.965]
Epoch [91/120    avg_loss:0.024, val_acc:0.964]
Epoch [92/120    avg_loss:0.024, val_acc:0.963]
Epoch [93/120    avg_loss:0.025, val_acc:0.966]
Epoch [94/120    avg_loss:0.027, val_acc:0.966]
Epoch [95/120    avg_loss:0.024, val_acc:0.966]
Epoch [96/120    avg_loss:0.026, val_acc:0.966]
Epoch [97/120    avg_loss:0.023, val_acc:0.968]
Epoch [98/120    avg_loss:0.026, val_acc:0.966]
Epoch [99/120    avg_loss:0.022, val_acc:0.966]
Epoch [100/120    avg_loss:0.023, val_acc:0.969]
Epoch [101/120    avg_loss:0.027, val_acc:0.966]
Epoch [102/120    avg_loss:0.023, val_acc:0.968]
Epoch [103/120    avg_loss:0.026, val_acc:0.969]
Epoch [104/120    avg_loss:0.022, val_acc:0.969]
Epoch [105/120    avg_loss:0.023, val_acc:0.969]
Epoch [106/120    avg_loss:0.025, val_acc:0.973]
Epoch [107/120    avg_loss:0.026, val_acc:0.969]
Epoch [108/120    avg_loss:0.021, val_acc:0.966]
Epoch [109/120    avg_loss:0.019, val_acc:0.968]
Epoch [110/120    avg_loss:0.022, val_acc:0.970]
Epoch [111/120    avg_loss:0.023, val_acc:0.970]
Epoch [112/120    avg_loss:0.025, val_acc:0.969]
Epoch [113/120    avg_loss:0.027, val_acc:0.972]
Epoch [114/120    avg_loss:0.026, val_acc:0.967]
Epoch [115/120    avg_loss:0.019, val_acc:0.967]
Epoch [116/120    avg_loss:0.020, val_acc:0.965]
Epoch [117/120    avg_loss:0.023, val_acc:0.968]
Epoch [118/120    avg_loss:0.018, val_acc:0.970]
Epoch [119/120    avg_loss:0.021, val_acc:0.967]
Epoch [120/120    avg_loss:0.023, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1245    0    0    0    8    0    0    0    6   25    1    0
     0    0    0]
 [   0    0    3  706    0   21    0    0    0    8    0    0    7    2
     0    0    0]
 [   0    0    0    0  210    0    3    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    6    0    6    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   31   38    0   10    0    0    0    0  779   17    0    0
     0    0    0]
 [   0    0   20    0    0    0    4    0    0    0   18 2165    0    3
     0    0    0]
 [   0    0    0   23   15    4    1    0    0    0   15    0  466    0
     0    1    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   19    0    0    2    0    0    0    0
    71  255    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.58807588075881

F1 scores:
[       nan 0.96202532 0.96362229 0.9326288  0.95890411 0.94618834
 0.97329377 0.89285714 0.99883856 0.66666667 0.91755006 0.98008148
 0.92277228 0.98666667 0.96804431 0.84577114 0.94318182]

Kappa:
0.9496758961914562
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a6e55d828>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.775, val_acc:0.287]
Epoch [2/120    avg_loss:2.485, val_acc:0.367]
Epoch [3/120    avg_loss:2.246, val_acc:0.411]
Epoch [4/120    avg_loss:2.100, val_acc:0.466]
Epoch [5/120    avg_loss:1.990, val_acc:0.502]
Epoch [6/120    avg_loss:1.888, val_acc:0.538]
Epoch [7/120    avg_loss:1.792, val_acc:0.526]
Epoch [8/120    avg_loss:1.653, val_acc:0.555]
Epoch [9/120    avg_loss:1.615, val_acc:0.607]
Epoch [10/120    avg_loss:1.434, val_acc:0.608]
Epoch [11/120    avg_loss:1.329, val_acc:0.629]
Epoch [12/120    avg_loss:1.189, val_acc:0.664]
Epoch [13/120    avg_loss:1.118, val_acc:0.666]
Epoch [14/120    avg_loss:0.978, val_acc:0.693]
Epoch [15/120    avg_loss:0.918, val_acc:0.737]
Epoch [16/120    avg_loss:0.771, val_acc:0.738]
Epoch [17/120    avg_loss:0.749, val_acc:0.762]
Epoch [18/120    avg_loss:0.724, val_acc:0.753]
Epoch [19/120    avg_loss:0.618, val_acc:0.780]
Epoch [20/120    avg_loss:0.648, val_acc:0.771]
Epoch [21/120    avg_loss:0.580, val_acc:0.785]
Epoch [22/120    avg_loss:0.572, val_acc:0.764]
Epoch [23/120    avg_loss:0.486, val_acc:0.824]
Epoch [24/120    avg_loss:0.424, val_acc:0.836]
Epoch [25/120    avg_loss:0.405, val_acc:0.833]
Epoch [26/120    avg_loss:0.406, val_acc:0.849]
Epoch [27/120    avg_loss:0.386, val_acc:0.863]
Epoch [28/120    avg_loss:0.365, val_acc:0.858]
Epoch [29/120    avg_loss:0.311, val_acc:0.867]
Epoch [30/120    avg_loss:0.295, val_acc:0.886]
Epoch [31/120    avg_loss:0.281, val_acc:0.882]
Epoch [32/120    avg_loss:0.292, val_acc:0.862]
Epoch [33/120    avg_loss:0.274, val_acc:0.875]
Epoch [34/120    avg_loss:0.255, val_acc:0.890]
Epoch [35/120    avg_loss:0.246, val_acc:0.874]
Epoch [36/120    avg_loss:0.205, val_acc:0.900]
Epoch [37/120    avg_loss:0.236, val_acc:0.853]
Epoch [38/120    avg_loss:0.221, val_acc:0.898]
Epoch [39/120    avg_loss:0.181, val_acc:0.900]
Epoch [40/120    avg_loss:0.214, val_acc:0.901]
Epoch [41/120    avg_loss:0.202, val_acc:0.898]
Epoch [42/120    avg_loss:0.200, val_acc:0.889]
Epoch [43/120    avg_loss:0.202, val_acc:0.895]
Epoch [44/120    avg_loss:0.211, val_acc:0.899]
Epoch [45/120    avg_loss:0.168, val_acc:0.911]
Epoch [46/120    avg_loss:0.155, val_acc:0.915]
Epoch [47/120    avg_loss:0.131, val_acc:0.908]
Epoch [48/120    avg_loss:0.127, val_acc:0.917]
Epoch [49/120    avg_loss:0.120, val_acc:0.917]
Epoch [50/120    avg_loss:0.123, val_acc:0.922]
Epoch [51/120    avg_loss:0.109, val_acc:0.925]
Epoch [52/120    avg_loss:0.107, val_acc:0.917]
Epoch [53/120    avg_loss:0.110, val_acc:0.923]
Epoch [54/120    avg_loss:0.115, val_acc:0.935]
Epoch [55/120    avg_loss:0.090, val_acc:0.924]
Epoch [56/120    avg_loss:0.180, val_acc:0.907]
Epoch [57/120    avg_loss:0.151, val_acc:0.921]
Epoch [58/120    avg_loss:0.133, val_acc:0.913]
Epoch [59/120    avg_loss:0.116, val_acc:0.929]
Epoch [60/120    avg_loss:0.111, val_acc:0.929]
Epoch [61/120    avg_loss:0.084, val_acc:0.910]
Epoch [62/120    avg_loss:0.085, val_acc:0.915]
Epoch [63/120    avg_loss:0.079, val_acc:0.932]
Epoch [64/120    avg_loss:0.080, val_acc:0.926]
Epoch [65/120    avg_loss:0.074, val_acc:0.933]
Epoch [66/120    avg_loss:0.059, val_acc:0.929]
Epoch [67/120    avg_loss:0.080, val_acc:0.937]
Epoch [68/120    avg_loss:0.074, val_acc:0.945]
Epoch [69/120    avg_loss:0.064, val_acc:0.939]
Epoch [70/120    avg_loss:0.054, val_acc:0.940]
Epoch [71/120    avg_loss:0.076, val_acc:0.917]
Epoch [72/120    avg_loss:0.069, val_acc:0.918]
Epoch [73/120    avg_loss:0.061, val_acc:0.942]
Epoch [74/120    avg_loss:0.046, val_acc:0.933]
Epoch [75/120    avg_loss:0.052, val_acc:0.950]
Epoch [76/120    avg_loss:0.057, val_acc:0.955]
Epoch [77/120    avg_loss:0.056, val_acc:0.948]
Epoch [78/120    avg_loss:0.055, val_acc:0.940]
Epoch [79/120    avg_loss:0.062, val_acc:0.928]
Epoch [80/120    avg_loss:0.086, val_acc:0.932]
Epoch [81/120    avg_loss:0.052, val_acc:0.943]
Epoch [82/120    avg_loss:0.051, val_acc:0.942]
Epoch [83/120    avg_loss:0.044, val_acc:0.920]
Epoch [84/120    avg_loss:0.048, val_acc:0.955]
Epoch [85/120    avg_loss:0.039, val_acc:0.951]
Epoch [86/120    avg_loss:0.042, val_acc:0.953]
Epoch [87/120    avg_loss:0.038, val_acc:0.952]
Epoch [88/120    avg_loss:0.029, val_acc:0.943]
Epoch [89/120    avg_loss:0.025, val_acc:0.953]
Epoch [90/120    avg_loss:0.036, val_acc:0.948]
Epoch [91/120    avg_loss:0.045, val_acc:0.955]
Epoch [92/120    avg_loss:0.036, val_acc:0.953]
Epoch [93/120    avg_loss:0.042, val_acc:0.943]
Epoch [94/120    avg_loss:0.044, val_acc:0.939]
Epoch [95/120    avg_loss:0.053, val_acc:0.932]
Epoch [96/120    avg_loss:0.053, val_acc:0.951]
Epoch [97/120    avg_loss:0.036, val_acc:0.961]
Epoch [98/120    avg_loss:0.026, val_acc:0.950]
Epoch [99/120    avg_loss:0.028, val_acc:0.954]
Epoch [100/120    avg_loss:0.027, val_acc:0.955]
Epoch [101/120    avg_loss:0.023, val_acc:0.961]
Epoch [102/120    avg_loss:0.031, val_acc:0.942]
Epoch [103/120    avg_loss:0.039, val_acc:0.946]
Epoch [104/120    avg_loss:0.027, val_acc:0.958]
Epoch [105/120    avg_loss:0.025, val_acc:0.963]
Epoch [106/120    avg_loss:0.020, val_acc:0.952]
Epoch [107/120    avg_loss:0.022, val_acc:0.961]
Epoch [108/120    avg_loss:0.029, val_acc:0.953]
Epoch [109/120    avg_loss:0.041, val_acc:0.954]
Epoch [110/120    avg_loss:0.038, val_acc:0.951]
Epoch [111/120    avg_loss:0.033, val_acc:0.958]
Epoch [112/120    avg_loss:0.030, val_acc:0.946]
Epoch [113/120    avg_loss:0.030, val_acc:0.961]
Epoch [114/120    avg_loss:0.027, val_acc:0.964]
Epoch [115/120    avg_loss:0.020, val_acc:0.966]
Epoch [116/120    avg_loss:0.018, val_acc:0.962]
Epoch [117/120    avg_loss:0.019, val_acc:0.962]
Epoch [118/120    avg_loss:0.018, val_acc:0.962]
Epoch [119/120    avg_loss:0.019, val_acc:0.966]
Epoch [120/120    avg_loss:0.012, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1244    0    0    0    1    0    0    2    1   33    4    0
     0    0    0]
 [   0    0    0  691    1   23    1    0    0   22    0    0    7    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    3    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    2    0    0    2    0    0   12    0    0    2    0
     0    0    0]
 [   0    0   17   72    0    5    0    0    0    0  765    6   10    0
     0    0    0]
 [   0    0    4    0    0    0    6    0    0    0   25 2170    4    1
     0    0    0]
 [   0    0    0   18   16    9    0    0    0    0   15    0  473    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    2    0    3    0    0    0
  1131    2    0]
 [   0    0    0    0    0    1   13    0    0    2    0    0    0    0
    48  283    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.70731707317073

F1 scores:
[       nan 0.975      0.97568627 0.90326797 0.96162528 0.95248619
 0.98203593 1.         0.99417928 0.40677966 0.90747331 0.98190045
 0.91224687 0.9919571  0.97542044 0.89556962 0.98245614]

Kappa:
0.9510674937467094
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe1fe5b3860>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.736, val_acc:0.265]
Epoch [2/120    avg_loss:2.428, val_acc:0.438]
Epoch [3/120    avg_loss:2.227, val_acc:0.517]
Epoch [4/120    avg_loss:2.100, val_acc:0.559]
Epoch [5/120    avg_loss:1.982, val_acc:0.589]
Epoch [6/120    avg_loss:1.890, val_acc:0.594]
Epoch [7/120    avg_loss:1.771, val_acc:0.609]
Epoch [8/120    avg_loss:1.688, val_acc:0.623]
Epoch [9/120    avg_loss:1.623, val_acc:0.653]
Epoch [10/120    avg_loss:1.497, val_acc:0.642]
Epoch [11/120    avg_loss:1.404, val_acc:0.668]
Epoch [12/120    avg_loss:1.312, val_acc:0.714]
Epoch [13/120    avg_loss:1.251, val_acc:0.644]
Epoch [14/120    avg_loss:1.216, val_acc:0.689]
Epoch [15/120    avg_loss:1.080, val_acc:0.709]
Epoch [16/120    avg_loss:1.003, val_acc:0.757]
Epoch [17/120    avg_loss:0.874, val_acc:0.775]
Epoch [18/120    avg_loss:0.817, val_acc:0.786]
Epoch [19/120    avg_loss:0.784, val_acc:0.802]
Epoch [20/120    avg_loss:0.689, val_acc:0.811]
Epoch [21/120    avg_loss:0.620, val_acc:0.830]
Epoch [22/120    avg_loss:0.570, val_acc:0.825]
Epoch [23/120    avg_loss:0.540, val_acc:0.850]
Epoch [24/120    avg_loss:0.505, val_acc:0.820]
Epoch [25/120    avg_loss:0.498, val_acc:0.863]
Epoch [26/120    avg_loss:0.443, val_acc:0.876]
Epoch [27/120    avg_loss:0.445, val_acc:0.834]
Epoch [28/120    avg_loss:0.447, val_acc:0.828]
Epoch [29/120    avg_loss:0.376, val_acc:0.876]
Epoch [30/120    avg_loss:0.371, val_acc:0.873]
Epoch [31/120    avg_loss:0.343, val_acc:0.883]
Epoch [32/120    avg_loss:0.326, val_acc:0.858]
Epoch [33/120    avg_loss:0.293, val_acc:0.895]
Epoch [34/120    avg_loss:0.282, val_acc:0.873]
Epoch [35/120    avg_loss:0.280, val_acc:0.909]
Epoch [36/120    avg_loss:0.254, val_acc:0.905]
Epoch [37/120    avg_loss:0.230, val_acc:0.939]
Epoch [38/120    avg_loss:0.214, val_acc:0.907]
Epoch [39/120    avg_loss:0.220, val_acc:0.906]
Epoch [40/120    avg_loss:0.200, val_acc:0.919]
Epoch [41/120    avg_loss:0.211, val_acc:0.910]
Epoch [42/120    avg_loss:0.196, val_acc:0.914]
Epoch [43/120    avg_loss:0.213, val_acc:0.912]
Epoch [44/120    avg_loss:0.169, val_acc:0.926]
Epoch [45/120    avg_loss:0.146, val_acc:0.934]
Epoch [46/120    avg_loss:0.170, val_acc:0.922]
Epoch [47/120    avg_loss:0.193, val_acc:0.901]
Epoch [48/120    avg_loss:0.158, val_acc:0.941]
Epoch [49/120    avg_loss:0.132, val_acc:0.923]
Epoch [50/120    avg_loss:0.130, val_acc:0.901]
Epoch [51/120    avg_loss:0.119, val_acc:0.923]
Epoch [52/120    avg_loss:0.101, val_acc:0.918]
Epoch [53/120    avg_loss:0.125, val_acc:0.924]
Epoch [54/120    avg_loss:0.120, val_acc:0.941]
Epoch [55/120    avg_loss:0.154, val_acc:0.941]
Epoch [56/120    avg_loss:0.162, val_acc:0.930]
Epoch [57/120    avg_loss:0.121, val_acc:0.908]
Epoch [58/120    avg_loss:0.130, val_acc:0.928]
Epoch [59/120    avg_loss:0.111, val_acc:0.936]
Epoch [60/120    avg_loss:0.113, val_acc:0.927]
Epoch [61/120    avg_loss:0.129, val_acc:0.930]
Epoch [62/120    avg_loss:0.095, val_acc:0.940]
Epoch [63/120    avg_loss:0.075, val_acc:0.943]
Epoch [64/120    avg_loss:0.076, val_acc:0.951]
Epoch [65/120    avg_loss:0.070, val_acc:0.945]
Epoch [66/120    avg_loss:0.065, val_acc:0.960]
Epoch [67/120    avg_loss:0.051, val_acc:0.936]
Epoch [68/120    avg_loss:0.058, val_acc:0.950]
Epoch [69/120    avg_loss:0.061, val_acc:0.951]
Epoch [70/120    avg_loss:0.059, val_acc:0.955]
Epoch [71/120    avg_loss:0.046, val_acc:0.963]
Epoch [72/120    avg_loss:0.057, val_acc:0.940]
Epoch [73/120    avg_loss:0.048, val_acc:0.957]
Epoch [74/120    avg_loss:0.051, val_acc:0.956]
Epoch [75/120    avg_loss:0.056, val_acc:0.943]
Epoch [76/120    avg_loss:0.077, val_acc:0.943]
Epoch [77/120    avg_loss:0.123, val_acc:0.944]
Epoch [78/120    avg_loss:0.133, val_acc:0.934]
Epoch [79/120    avg_loss:0.151, val_acc:0.928]
Epoch [80/120    avg_loss:0.105, val_acc:0.952]
Epoch [81/120    avg_loss:0.069, val_acc:0.951]
Epoch [82/120    avg_loss:0.092, val_acc:0.935]
Epoch [83/120    avg_loss:0.075, val_acc:0.944]
Epoch [84/120    avg_loss:0.063, val_acc:0.956]
Epoch [85/120    avg_loss:0.044, val_acc:0.959]
Epoch [86/120    avg_loss:0.038, val_acc:0.964]
Epoch [87/120    avg_loss:0.046, val_acc:0.967]
Epoch [88/120    avg_loss:0.036, val_acc:0.965]
Epoch [89/120    avg_loss:0.034, val_acc:0.965]
Epoch [90/120    avg_loss:0.037, val_acc:0.964]
Epoch [91/120    avg_loss:0.034, val_acc:0.966]
Epoch [92/120    avg_loss:0.036, val_acc:0.966]
Epoch [93/120    avg_loss:0.035, val_acc:0.967]
Epoch [94/120    avg_loss:0.036, val_acc:0.966]
Epoch [95/120    avg_loss:0.035, val_acc:0.967]
Epoch [96/120    avg_loss:0.036, val_acc:0.967]
Epoch [97/120    avg_loss:0.028, val_acc:0.967]
Epoch [98/120    avg_loss:0.039, val_acc:0.967]
Epoch [99/120    avg_loss:0.048, val_acc:0.967]
Epoch [100/120    avg_loss:0.030, val_acc:0.967]
Epoch [101/120    avg_loss:0.033, val_acc:0.968]
Epoch [102/120    avg_loss:0.036, val_acc:0.969]
Epoch [103/120    avg_loss:0.031, val_acc:0.969]
Epoch [104/120    avg_loss:0.032, val_acc:0.968]
Epoch [105/120    avg_loss:0.029, val_acc:0.968]
Epoch [106/120    avg_loss:0.029, val_acc:0.966]
Epoch [107/120    avg_loss:0.031, val_acc:0.967]
Epoch [108/120    avg_loss:0.032, val_acc:0.968]
Epoch [109/120    avg_loss:0.028, val_acc:0.966]
Epoch [110/120    avg_loss:0.028, val_acc:0.966]
Epoch [111/120    avg_loss:0.025, val_acc:0.965]
Epoch [112/120    avg_loss:0.029, val_acc:0.967]
Epoch [113/120    avg_loss:0.028, val_acc:0.967]
Epoch [114/120    avg_loss:0.024, val_acc:0.968]
Epoch [115/120    avg_loss:0.031, val_acc:0.969]
Epoch [116/120    avg_loss:0.027, val_acc:0.967]
Epoch [117/120    avg_loss:0.025, val_acc:0.965]
Epoch [118/120    avg_loss:0.024, val_acc:0.965]
Epoch [119/120    avg_loss:0.028, val_acc:0.966]
Epoch [120/120    avg_loss:0.027, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1239    4    0    0    0    0    0    0   10   26    4    0
     0    2    0]
 [   0    0    1  706    4   10    0    0    0    8    1    0   15    0
     2    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    3    0    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   14    0    0    1    0
     0    0    0]
 [   0    0   42   72    0    4    0    0    0    0  749    5    0    0
     0    3    0]
 [   0    0    5    0    0    0    2    0    2    0    6 2190    3    2
     0    0    0]
 [   0    0    5    0   18    8    0    0    0    0   10    1  482    0
     1    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    1    0    3    0    0    0
  1125    0    0]
 [   0    0    0    0    0    0   28    0    0    4    0    0    0    0
    68  247    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.49051490514906

F1 scores:
[       nan 0.975      0.96158324 0.92167102 0.95089286 0.95642458
 0.97244974 1.         0.99652375 0.60869565 0.90458937 0.98782138
 0.92781521 0.99462366 0.96194955 0.82470785 0.94915254]

Kappa:
0.9485572952708434
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feee16a6780>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.782, val_acc:0.141]
Epoch [2/120    avg_loss:2.480, val_acc:0.443]
Epoch [3/120    avg_loss:2.263, val_acc:0.472]
Epoch [4/120    avg_loss:2.082, val_acc:0.441]
Epoch [5/120    avg_loss:2.020, val_acc:0.527]
Epoch [6/120    avg_loss:1.890, val_acc:0.601]
Epoch [7/120    avg_loss:1.699, val_acc:0.584]
Epoch [8/120    avg_loss:1.624, val_acc:0.640]
Epoch [9/120    avg_loss:1.529, val_acc:0.670]
Epoch [10/120    avg_loss:1.411, val_acc:0.683]
Epoch [11/120    avg_loss:1.280, val_acc:0.693]
Epoch [12/120    avg_loss:1.212, val_acc:0.685]
Epoch [13/120    avg_loss:1.109, val_acc:0.720]
Epoch [14/120    avg_loss:1.019, val_acc:0.753]
Epoch [15/120    avg_loss:0.943, val_acc:0.784]
Epoch [16/120    avg_loss:0.889, val_acc:0.774]
Epoch [17/120    avg_loss:0.851, val_acc:0.761]
Epoch [18/120    avg_loss:0.816, val_acc:0.781]
Epoch [19/120    avg_loss:0.762, val_acc:0.792]
Epoch [20/120    avg_loss:0.688, val_acc:0.824]
Epoch [21/120    avg_loss:0.607, val_acc:0.822]
Epoch [22/120    avg_loss:0.535, val_acc:0.816]
Epoch [23/120    avg_loss:0.517, val_acc:0.836]
Epoch [24/120    avg_loss:0.560, val_acc:0.762]
Epoch [25/120    avg_loss:0.554, val_acc:0.826]
Epoch [26/120    avg_loss:0.578, val_acc:0.831]
Epoch [27/120    avg_loss:0.499, val_acc:0.843]
Epoch [28/120    avg_loss:0.534, val_acc:0.800]
Epoch [29/120    avg_loss:0.440, val_acc:0.856]
Epoch [30/120    avg_loss:0.361, val_acc:0.861]
Epoch [31/120    avg_loss:0.326, val_acc:0.873]
Epoch [32/120    avg_loss:0.384, val_acc:0.859]
Epoch [33/120    avg_loss:0.353, val_acc:0.884]
Epoch [34/120    avg_loss:0.293, val_acc:0.892]
Epoch [35/120    avg_loss:0.308, val_acc:0.878]
Epoch [36/120    avg_loss:0.283, val_acc:0.894]
Epoch [37/120    avg_loss:0.298, val_acc:0.890]
Epoch [38/120    avg_loss:0.257, val_acc:0.917]
Epoch [39/120    avg_loss:0.222, val_acc:0.910]
Epoch [40/120    avg_loss:0.210, val_acc:0.916]
Epoch [41/120    avg_loss:0.239, val_acc:0.912]
Epoch [42/120    avg_loss:0.379, val_acc:0.900]
Epoch [43/120    avg_loss:0.266, val_acc:0.910]
Epoch [44/120    avg_loss:0.217, val_acc:0.897]
Epoch [45/120    avg_loss:0.207, val_acc:0.912]
Epoch [46/120    avg_loss:0.160, val_acc:0.926]
Epoch [47/120    avg_loss:0.158, val_acc:0.936]
Epoch [48/120    avg_loss:0.146, val_acc:0.930]
Epoch [49/120    avg_loss:0.133, val_acc:0.936]
Epoch [50/120    avg_loss:0.106, val_acc:0.943]
Epoch [51/120    avg_loss:0.119, val_acc:0.934]
Epoch [52/120    avg_loss:0.111, val_acc:0.934]
Epoch [53/120    avg_loss:0.107, val_acc:0.936]
Epoch [54/120    avg_loss:0.094, val_acc:0.949]
Epoch [55/120    avg_loss:0.093, val_acc:0.947]
Epoch [56/120    avg_loss:0.102, val_acc:0.941]
Epoch [57/120    avg_loss:0.103, val_acc:0.942]
Epoch [58/120    avg_loss:0.128, val_acc:0.936]
Epoch [59/120    avg_loss:0.124, val_acc:0.927]
Epoch [60/120    avg_loss:0.116, val_acc:0.936]
Epoch [61/120    avg_loss:0.086, val_acc:0.943]
Epoch [62/120    avg_loss:0.078, val_acc:0.947]
Epoch [63/120    avg_loss:0.082, val_acc:0.950]
Epoch [64/120    avg_loss:0.090, val_acc:0.956]
Epoch [65/120    avg_loss:0.071, val_acc:0.956]
Epoch [66/120    avg_loss:0.072, val_acc:0.951]
Epoch [67/120    avg_loss:0.060, val_acc:0.958]
Epoch [68/120    avg_loss:0.060, val_acc:0.964]
Epoch [69/120    avg_loss:0.061, val_acc:0.952]
Epoch [70/120    avg_loss:0.062, val_acc:0.952]
Epoch [71/120    avg_loss:0.064, val_acc:0.952]
Epoch [72/120    avg_loss:0.054, val_acc:0.951]
Epoch [73/120    avg_loss:0.068, val_acc:0.943]
Epoch [74/120    avg_loss:0.051, val_acc:0.967]
Epoch [75/120    avg_loss:0.048, val_acc:0.953]
Epoch [76/120    avg_loss:0.044, val_acc:0.960]
Epoch [77/120    avg_loss:0.050, val_acc:0.958]
Epoch [78/120    avg_loss:0.050, val_acc:0.953]
Epoch [79/120    avg_loss:0.039, val_acc:0.968]
Epoch [80/120    avg_loss:0.043, val_acc:0.947]
Epoch [81/120    avg_loss:0.043, val_acc:0.947]
Epoch [82/120    avg_loss:0.044, val_acc:0.966]
Epoch [83/120    avg_loss:0.050, val_acc:0.944]
Epoch [84/120    avg_loss:0.039, val_acc:0.963]
Epoch [85/120    avg_loss:0.045, val_acc:0.959]
Epoch [86/120    avg_loss:0.043, val_acc:0.960]
Epoch [87/120    avg_loss:0.034, val_acc:0.966]
Epoch [88/120    avg_loss:0.038, val_acc:0.969]
Epoch [89/120    avg_loss:0.041, val_acc:0.958]
Epoch [90/120    avg_loss:0.037, val_acc:0.967]
Epoch [91/120    avg_loss:0.034, val_acc:0.967]
Epoch [92/120    avg_loss:0.029, val_acc:0.972]
Epoch [93/120    avg_loss:0.025, val_acc:0.972]
Epoch [94/120    avg_loss:0.039, val_acc:0.952]
Epoch [95/120    avg_loss:0.045, val_acc:0.942]
Epoch [96/120    avg_loss:0.065, val_acc:0.966]
Epoch [97/120    avg_loss:0.036, val_acc:0.966]
Epoch [98/120    avg_loss:0.026, val_acc:0.973]
Epoch [99/120    avg_loss:0.026, val_acc:0.972]
Epoch [100/120    avg_loss:0.026, val_acc:0.968]
Epoch [101/120    avg_loss:0.032, val_acc:0.963]
Epoch [102/120    avg_loss:0.045, val_acc:0.956]
Epoch [103/120    avg_loss:0.037, val_acc:0.961]
Epoch [104/120    avg_loss:0.037, val_acc:0.965]
Epoch [105/120    avg_loss:0.033, val_acc:0.972]
Epoch [106/120    avg_loss:0.040, val_acc:0.945]
Epoch [107/120    avg_loss:0.061, val_acc:0.960]
Epoch [108/120    avg_loss:0.034, val_acc:0.970]
Epoch [109/120    avg_loss:0.028, val_acc:0.936]
Epoch [110/120    avg_loss:0.036, val_acc:0.942]
Epoch [111/120    avg_loss:0.028, val_acc:0.968]
Epoch [112/120    avg_loss:0.022, val_acc:0.970]
Epoch [113/120    avg_loss:0.020, val_acc:0.974]
Epoch [114/120    avg_loss:0.018, val_acc:0.972]
Epoch [115/120    avg_loss:0.014, val_acc:0.972]
Epoch [116/120    avg_loss:0.019, val_acc:0.974]
Epoch [117/120    avg_loss:0.020, val_acc:0.972]
Epoch [118/120    avg_loss:0.017, val_acc:0.973]
Epoch [119/120    avg_loss:0.016, val_acc:0.975]
Epoch [120/120    avg_loss:0.014, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1232    4    0    0    3    0    0    1   10   35    0    0
     0    0    0]
 [   0    0    2  686    0   26    0    0    0   19    0    0    9    4
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    6   41    0    9    0    0    0    0  798   12    7    0
     1    1    0]
 [   0    0    4    0    0    0    3    0    1    0   19 2179    3    1
     0    0    0]
 [   0    0    4    1   18    5    0    0    0    0   13    6  476    0
     0    1   10]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    3    0    3    0    0    0
  1124    1    0]
 [   0    0    0    0    0    0   35    0    0    0    0    0    0    0
    39  273    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.89159891598916

F1 scores:
[       nan 1.         0.97275957 0.92702703 0.95711061 0.94310722
 0.9689808  0.98039216 0.99186992 0.5862069  0.92898719 0.98086878
 0.92158761 0.98666667 0.97569444 0.87640449 0.94382022]

Kappa:
0.9531587584858742
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff299b40898>
supervision:full
center_pixel:True
Network :
Number of parameter: 69832==>0.07M
----------Training process----------
Epoch [1/120    avg_loss:2.743, val_acc:0.370]
Epoch [2/120    avg_loss:2.403, val_acc:0.464]
Epoch [3/120    avg_loss:2.238, val_acc:0.537]
Epoch [4/120    avg_loss:2.115, val_acc:0.550]
Epoch [5/120    avg_loss:2.010, val_acc:0.576]
Epoch [6/120    avg_loss:1.838, val_acc:0.592]
Epoch [7/120    avg_loss:1.739, val_acc:0.613]
Epoch [8/120    avg_loss:1.648, val_acc:0.606]
Epoch [9/120    avg_loss:1.534, val_acc:0.622]
Epoch [10/120    avg_loss:1.471, val_acc:0.630]
Epoch [11/120    avg_loss:1.281, val_acc:0.664]
Epoch [12/120    avg_loss:1.215, val_acc:0.681]
Epoch [13/120    avg_loss:1.201, val_acc:0.699]
Epoch [14/120    avg_loss:1.056, val_acc:0.705]
Epoch [15/120    avg_loss:0.962, val_acc:0.719]
Epoch [16/120    avg_loss:0.920, val_acc:0.736]
Epoch [17/120    avg_loss:0.847, val_acc:0.753]
Epoch [18/120    avg_loss:0.842, val_acc:0.738]
Epoch [19/120    avg_loss:0.772, val_acc:0.762]
Epoch [20/120    avg_loss:0.719, val_acc:0.791]
Epoch [21/120    avg_loss:0.689, val_acc:0.773]
Epoch [22/120    avg_loss:0.630, val_acc:0.797]
Epoch [23/120    avg_loss:0.550, val_acc:0.817]
Epoch [24/120    avg_loss:0.500, val_acc:0.818]
Epoch [25/120    avg_loss:0.442, val_acc:0.851]
Epoch [26/120    avg_loss:0.453, val_acc:0.842]
Epoch [27/120    avg_loss:0.478, val_acc:0.833]
Epoch [28/120    avg_loss:0.573, val_acc:0.800]
Epoch [29/120    avg_loss:0.424, val_acc:0.823]
Epoch [30/120    avg_loss:0.381, val_acc:0.832]
Epoch [31/120    avg_loss:0.380, val_acc:0.872]
Epoch [32/120    avg_loss:0.325, val_acc:0.870]
Epoch [33/120    avg_loss:0.256, val_acc:0.893]
Epoch [34/120    avg_loss:0.250, val_acc:0.898]
Epoch [35/120    avg_loss:0.260, val_acc:0.869]
Epoch [36/120    avg_loss:0.276, val_acc:0.886]
Epoch [37/120    avg_loss:0.231, val_acc:0.891]
Epoch [38/120    avg_loss:0.200, val_acc:0.903]
Epoch [39/120    avg_loss:0.206, val_acc:0.902]
Epoch [40/120    avg_loss:0.213, val_acc:0.917]
Epoch [41/120    avg_loss:0.250, val_acc:0.903]
Epoch [42/120    avg_loss:0.238, val_acc:0.876]
Epoch [43/120    avg_loss:0.224, val_acc:0.903]
Epoch [44/120    avg_loss:0.195, val_acc:0.905]
Epoch [45/120    avg_loss:0.175, val_acc:0.919]
Epoch [46/120    avg_loss:0.159, val_acc:0.923]
Epoch [47/120    avg_loss:0.141, val_acc:0.922]
Epoch [48/120    avg_loss:0.129, val_acc:0.934]
Epoch [49/120    avg_loss:0.144, val_acc:0.914]
Epoch [50/120    avg_loss:0.152, val_acc:0.916]
Epoch [51/120    avg_loss:0.130, val_acc:0.927]
Epoch [52/120    avg_loss:0.113, val_acc:0.907]
Epoch [53/120    avg_loss:0.129, val_acc:0.936]
Epoch [54/120    avg_loss:0.140, val_acc:0.926]
Epoch [55/120    avg_loss:0.106, val_acc:0.936]
Epoch [56/120    avg_loss:0.099, val_acc:0.931]
Epoch [57/120    avg_loss:0.086, val_acc:0.933]
Epoch [58/120    avg_loss:0.085, val_acc:0.926]
Epoch [59/120    avg_loss:0.104, val_acc:0.940]
Epoch [60/120    avg_loss:0.083, val_acc:0.943]
Epoch [61/120    avg_loss:0.093, val_acc:0.941]
Epoch [62/120    avg_loss:0.109, val_acc:0.931]
Epoch [63/120    avg_loss:0.095, val_acc:0.940]
Epoch [64/120    avg_loss:0.079, val_acc:0.944]
Epoch [65/120    avg_loss:0.074, val_acc:0.935]
Epoch [66/120    avg_loss:0.070, val_acc:0.941]
Epoch [67/120    avg_loss:0.064, val_acc:0.945]
Epoch [68/120    avg_loss:0.068, val_acc:0.934]
Epoch [69/120    avg_loss:0.057, val_acc:0.938]
Epoch [70/120    avg_loss:0.064, val_acc:0.938]
Epoch [71/120    avg_loss:0.070, val_acc:0.940]
Epoch [72/120    avg_loss:0.070, val_acc:0.945]
Epoch [73/120    avg_loss:0.058, val_acc:0.951]
Epoch [74/120    avg_loss:0.058, val_acc:0.935]
Epoch [75/120    avg_loss:0.075, val_acc:0.943]
Epoch [76/120    avg_loss:0.052, val_acc:0.957]
Epoch [77/120    avg_loss:0.052, val_acc:0.953]
Epoch [78/120    avg_loss:0.050, val_acc:0.957]
Epoch [79/120    avg_loss:0.047, val_acc:0.953]
Epoch [80/120    avg_loss:0.043, val_acc:0.955]
Epoch [81/120    avg_loss:0.051, val_acc:0.956]
Epoch [82/120    avg_loss:0.039, val_acc:0.948]
Epoch [83/120    avg_loss:0.043, val_acc:0.958]
Epoch [84/120    avg_loss:0.047, val_acc:0.952]
Epoch [85/120    avg_loss:0.049, val_acc:0.955]
Epoch [86/120    avg_loss:0.046, val_acc:0.958]
Epoch [87/120    avg_loss:0.048, val_acc:0.941]
Epoch [88/120    avg_loss:0.058, val_acc:0.931]
Epoch [89/120    avg_loss:0.134, val_acc:0.944]
Epoch [90/120    avg_loss:0.088, val_acc:0.939]
Epoch [91/120    avg_loss:0.070, val_acc:0.952]
Epoch [92/120    avg_loss:0.075, val_acc:0.943]
Epoch [93/120    avg_loss:0.069, val_acc:0.947]
Epoch [94/120    avg_loss:0.060, val_acc:0.956]
Epoch [95/120    avg_loss:0.055, val_acc:0.957]
Epoch [96/120    avg_loss:0.053, val_acc:0.944]
Epoch [97/120    avg_loss:0.045, val_acc:0.960]
Epoch [98/120    avg_loss:0.040, val_acc:0.958]
Epoch [99/120    avg_loss:0.040, val_acc:0.952]
Epoch [100/120    avg_loss:0.039, val_acc:0.961]
Epoch [101/120    avg_loss:0.040, val_acc:0.955]
Epoch [102/120    avg_loss:0.043, val_acc:0.949]
Epoch [103/120    avg_loss:0.049, val_acc:0.952]
Epoch [104/120    avg_loss:0.041, val_acc:0.956]
Epoch [105/120    avg_loss:0.037, val_acc:0.957]
Epoch [106/120    avg_loss:0.036, val_acc:0.959]
Epoch [107/120    avg_loss:0.033, val_acc:0.957]
Epoch [108/120    avg_loss:0.030, val_acc:0.964]
Epoch [109/120    avg_loss:0.022, val_acc:0.969]
Epoch [110/120    avg_loss:0.029, val_acc:0.960]
Epoch [111/120    avg_loss:0.026, val_acc:0.967]
Epoch [112/120    avg_loss:0.028, val_acc:0.974]
Epoch [113/120    avg_loss:0.022, val_acc:0.965]
Epoch [114/120    avg_loss:0.021, val_acc:0.963]
Epoch [115/120    avg_loss:0.019, val_acc:0.964]
Epoch [116/120    avg_loss:0.020, val_acc:0.969]
Epoch [117/120    avg_loss:0.020, val_acc:0.967]
Epoch [118/120    avg_loss:0.025, val_acc:0.963]
Epoch [119/120    avg_loss:0.024, val_acc:0.961]
Epoch [120/120    avg_loss:0.021, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1253    0    0    0    1    0    0    0    7   24    0    0
     0    0    0]
 [   0    0    3  704    1   13    0    0    0    9    1    8    5    1
     2    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    3    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    3    0    0
     4    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    5   30    0    6    0    0    0    0  823    7    0    0
     1    3    0]
 [   0    0   12    0    0    0    1    0    2    0   18 2173    0    3
     1    0    0]
 [   0    0    0   17    2    1    0    0    0    0   16   12  475    0
     0    0   11]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   15    0    0    2    0    3    1    0    0
  1117    1    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    30  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.55284552845528

F1 scores:
[       nan 0.93506494 0.97967162 0.93804131 0.99300699 0.95089286
 0.97891566 1.         0.99537037 0.66666667 0.9416476  0.97926994
 0.93688363 0.98930481 0.97130435 0.9183359  0.93854749]

Kappa:
0.960681961397756
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7262e4b828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.807, val_acc:0.260]
Epoch [2/120    avg_loss:2.496, val_acc:0.352]
Epoch [3/120    avg_loss:2.271, val_acc:0.492]
Epoch [4/120    avg_loss:2.134, val_acc:0.501]
Epoch [5/120    avg_loss:2.022, val_acc:0.541]
Epoch [6/120    avg_loss:1.895, val_acc:0.577]
Epoch [7/120    avg_loss:1.777, val_acc:0.613]
Epoch [8/120    avg_loss:1.656, val_acc:0.601]
Epoch [9/120    avg_loss:1.584, val_acc:0.614]
Epoch [10/120    avg_loss:1.473, val_acc:0.686]
Epoch [11/120    avg_loss:1.331, val_acc:0.655]
Epoch [12/120    avg_loss:1.204, val_acc:0.726]
Epoch [13/120    avg_loss:1.164, val_acc:0.695]
Epoch [14/120    avg_loss:1.085, val_acc:0.732]
Epoch [15/120    avg_loss:0.968, val_acc:0.736]
Epoch [16/120    avg_loss:0.889, val_acc:0.745]
Epoch [17/120    avg_loss:0.865, val_acc:0.710]
Epoch [18/120    avg_loss:0.823, val_acc:0.732]
Epoch [19/120    avg_loss:0.757, val_acc:0.795]
Epoch [20/120    avg_loss:0.678, val_acc:0.752]
Epoch [21/120    avg_loss:0.574, val_acc:0.822]
Epoch [22/120    avg_loss:0.524, val_acc:0.830]
Epoch [23/120    avg_loss:0.530, val_acc:0.825]
Epoch [24/120    avg_loss:0.456, val_acc:0.844]
Epoch [25/120    avg_loss:0.493, val_acc:0.802]
Epoch [26/120    avg_loss:0.456, val_acc:0.824]
Epoch [27/120    avg_loss:0.441, val_acc:0.845]
Epoch [28/120    avg_loss:0.359, val_acc:0.857]
Epoch [29/120    avg_loss:0.380, val_acc:0.845]
Epoch [30/120    avg_loss:0.328, val_acc:0.892]
Epoch [31/120    avg_loss:0.364, val_acc:0.865]
Epoch [32/120    avg_loss:0.340, val_acc:0.894]
Epoch [33/120    avg_loss:0.340, val_acc:0.901]
Epoch [34/120    avg_loss:0.271, val_acc:0.894]
Epoch [35/120    avg_loss:0.229, val_acc:0.897]
Epoch [36/120    avg_loss:0.248, val_acc:0.901]
Epoch [37/120    avg_loss:0.212, val_acc:0.917]
Epoch [38/120    avg_loss:0.209, val_acc:0.924]
Epoch [39/120    avg_loss:0.209, val_acc:0.909]
Epoch [40/120    avg_loss:0.282, val_acc:0.886]
Epoch [41/120    avg_loss:0.239, val_acc:0.909]
Epoch [42/120    avg_loss:0.222, val_acc:0.912]
Epoch [43/120    avg_loss:0.172, val_acc:0.941]
Epoch [44/120    avg_loss:0.145, val_acc:0.899]
Epoch [45/120    avg_loss:0.187, val_acc:0.931]
Epoch [46/120    avg_loss:0.145, val_acc:0.938]
Epoch [47/120    avg_loss:0.126, val_acc:0.923]
Epoch [48/120    avg_loss:0.136, val_acc:0.936]
Epoch [49/120    avg_loss:0.166, val_acc:0.934]
Epoch [50/120    avg_loss:0.139, val_acc:0.932]
Epoch [51/120    avg_loss:0.120, val_acc:0.944]
Epoch [52/120    avg_loss:0.124, val_acc:0.948]
Epoch [53/120    avg_loss:0.090, val_acc:0.945]
Epoch [54/120    avg_loss:0.094, val_acc:0.934]
Epoch [55/120    avg_loss:0.101, val_acc:0.922]
Epoch [56/120    avg_loss:0.107, val_acc:0.953]
Epoch [57/120    avg_loss:0.091, val_acc:0.948]
Epoch [58/120    avg_loss:0.077, val_acc:0.955]
Epoch [59/120    avg_loss:0.077, val_acc:0.928]
Epoch [60/120    avg_loss:0.109, val_acc:0.922]
Epoch [61/120    avg_loss:0.126, val_acc:0.955]
Epoch [62/120    avg_loss:0.088, val_acc:0.945]
Epoch [63/120    avg_loss:0.094, val_acc:0.947]
Epoch [64/120    avg_loss:0.099, val_acc:0.948]
Epoch [65/120    avg_loss:0.074, val_acc:0.959]
Epoch [66/120    avg_loss:0.077, val_acc:0.952]
Epoch [67/120    avg_loss:0.057, val_acc:0.960]
Epoch [68/120    avg_loss:0.070, val_acc:0.958]
Epoch [69/120    avg_loss:0.067, val_acc:0.958]
Epoch [70/120    avg_loss:0.079, val_acc:0.955]
Epoch [71/120    avg_loss:0.085, val_acc:0.953]
Epoch [72/120    avg_loss:0.099, val_acc:0.953]
Epoch [73/120    avg_loss:0.108, val_acc:0.931]
Epoch [74/120    avg_loss:0.069, val_acc:0.961]
Epoch [75/120    avg_loss:0.055, val_acc:0.966]
Epoch [76/120    avg_loss:0.061, val_acc:0.957]
Epoch [77/120    avg_loss:0.046, val_acc:0.963]
Epoch [78/120    avg_loss:0.052, val_acc:0.960]
Epoch [79/120    avg_loss:0.062, val_acc:0.957]
Epoch [80/120    avg_loss:0.054, val_acc:0.969]
Epoch [81/120    avg_loss:0.091, val_acc:0.924]
Epoch [82/120    avg_loss:0.088, val_acc:0.934]
Epoch [83/120    avg_loss:0.086, val_acc:0.957]
Epoch [84/120    avg_loss:0.069, val_acc:0.961]
Epoch [85/120    avg_loss:0.057, val_acc:0.955]
Epoch [86/120    avg_loss:0.067, val_acc:0.953]
Epoch [87/120    avg_loss:0.062, val_acc:0.970]
Epoch [88/120    avg_loss:0.038, val_acc:0.967]
Epoch [89/120    avg_loss:0.035, val_acc:0.966]
Epoch [90/120    avg_loss:0.036, val_acc:0.970]
Epoch [91/120    avg_loss:0.037, val_acc:0.969]
Epoch [92/120    avg_loss:0.040, val_acc:0.970]
Epoch [93/120    avg_loss:0.062, val_acc:0.966]
Epoch [94/120    avg_loss:0.077, val_acc:0.965]
Epoch [95/120    avg_loss:0.052, val_acc:0.955]
Epoch [96/120    avg_loss:0.035, val_acc:0.969]
Epoch [97/120    avg_loss:0.048, val_acc:0.958]
Epoch [98/120    avg_loss:0.076, val_acc:0.967]
Epoch [99/120    avg_loss:0.045, val_acc:0.967]
Epoch [100/120    avg_loss:0.037, val_acc:0.970]
Epoch [101/120    avg_loss:0.041, val_acc:0.968]
Epoch [102/120    avg_loss:0.035, val_acc:0.969]
Epoch [103/120    avg_loss:0.030, val_acc:0.973]
Epoch [104/120    avg_loss:0.033, val_acc:0.952]
Epoch [105/120    avg_loss:0.042, val_acc:0.968]
Epoch [106/120    avg_loss:0.038, val_acc:0.965]
Epoch [107/120    avg_loss:0.065, val_acc:0.956]
Epoch [108/120    avg_loss:0.061, val_acc:0.959]
Epoch [109/120    avg_loss:0.034, val_acc:0.966]
Epoch [110/120    avg_loss:0.049, val_acc:0.964]
Epoch [111/120    avg_loss:0.028, val_acc:0.969]
Epoch [112/120    avg_loss:0.025, val_acc:0.964]
Epoch [113/120    avg_loss:0.028, val_acc:0.967]
Epoch [114/120    avg_loss:0.028, val_acc:0.972]
Epoch [115/120    avg_loss:0.019, val_acc:0.973]
Epoch [116/120    avg_loss:0.019, val_acc:0.976]
Epoch [117/120    avg_loss:0.030, val_acc:0.964]
Epoch [118/120    avg_loss:0.029, val_acc:0.973]
Epoch [119/120    avg_loss:0.032, val_acc:0.972]
Epoch [120/120    avg_loss:0.036, val_acc:0.960]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1189    0    0    0   13    0    0   15   15   52    0    0
     0    1    0]
 [   0    0    2  656    0    5    3    0    0   60    0    0   12    9
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0  404    0    0    0   10    0    0    0    0
    19    0    0]
 [   0    0    0    0    0    0  645    0    0    0    0    1    0    0
    11    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   33   75    0    1   14    0    0    2  721    5    0    1
     3   20    0]
 [   0    0   19    0    0    4   46    0    3   19   11 2097    0    5
     6    0    0]
 [   0    0    8    3    3    1    0    0    0    7    1    0  488    0
     0   15    8]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0  182
     0    0    0]
 [   0    0    0    0    0   11    0    0    1   30    0    1    0    0
  1096    0    0]
 [   0    0    0    0    0    3   10    0    0   16    0    0    0    0
    74  244    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
92.43360433604336

F1 scores:
[       nan 0.96202532 0.93732755 0.88350168 0.9882904  0.93194925
 0.92939481 1.         0.99537037 0.18461538 0.88738462 0.96060467
 0.94299517 0.95287958 0.93356048 0.77830941 0.94857143]

Kappa:
0.9139485601343146
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0c43834828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.742, val_acc:0.233]
Epoch [2/120    avg_loss:2.447, val_acc:0.408]
Epoch [3/120    avg_loss:2.250, val_acc:0.489]
Epoch [4/120    avg_loss:2.146, val_acc:0.506]
Epoch [5/120    avg_loss:2.010, val_acc:0.558]
Epoch [6/120    avg_loss:1.908, val_acc:0.565]
Epoch [7/120    avg_loss:1.828, val_acc:0.581]
Epoch [8/120    avg_loss:1.689, val_acc:0.608]
Epoch [9/120    avg_loss:1.570, val_acc:0.635]
Epoch [10/120    avg_loss:1.449, val_acc:0.641]
Epoch [11/120    avg_loss:1.315, val_acc:0.647]
Epoch [12/120    avg_loss:1.255, val_acc:0.661]
Epoch [13/120    avg_loss:1.192, val_acc:0.699]
Epoch [14/120    avg_loss:1.056, val_acc:0.714]
Epoch [15/120    avg_loss:0.955, val_acc:0.731]
Epoch [16/120    avg_loss:0.875, val_acc:0.736]
Epoch [17/120    avg_loss:0.820, val_acc:0.790]
Epoch [18/120    avg_loss:0.749, val_acc:0.774]
Epoch [19/120    avg_loss:0.670, val_acc:0.781]
Epoch [20/120    avg_loss:0.617, val_acc:0.819]
Epoch [21/120    avg_loss:0.607, val_acc:0.819]
Epoch [22/120    avg_loss:0.543, val_acc:0.801]
Epoch [23/120    avg_loss:0.526, val_acc:0.857]
Epoch [24/120    avg_loss:0.467, val_acc:0.849]
Epoch [25/120    avg_loss:0.435, val_acc:0.864]
Epoch [26/120    avg_loss:0.418, val_acc:0.832]
Epoch [27/120    avg_loss:0.429, val_acc:0.844]
Epoch [28/120    avg_loss:0.431, val_acc:0.825]
Epoch [29/120    avg_loss:0.382, val_acc:0.866]
Epoch [30/120    avg_loss:0.347, val_acc:0.874]
Epoch [31/120    avg_loss:0.349, val_acc:0.885]
Epoch [32/120    avg_loss:0.328, val_acc:0.847]
Epoch [33/120    avg_loss:0.290, val_acc:0.880]
Epoch [34/120    avg_loss:0.274, val_acc:0.890]
Epoch [35/120    avg_loss:0.254, val_acc:0.903]
Epoch [36/120    avg_loss:0.289, val_acc:0.865]
Epoch [37/120    avg_loss:0.331, val_acc:0.899]
Epoch [38/120    avg_loss:0.238, val_acc:0.930]
Epoch [39/120    avg_loss:0.232, val_acc:0.899]
Epoch [40/120    avg_loss:0.204, val_acc:0.884]
Epoch [41/120    avg_loss:0.198, val_acc:0.912]
Epoch [42/120    avg_loss:0.234, val_acc:0.903]
Epoch [43/120    avg_loss:0.200, val_acc:0.930]
Epoch [44/120    avg_loss:0.158, val_acc:0.939]
Epoch [45/120    avg_loss:0.153, val_acc:0.943]
Epoch [46/120    avg_loss:0.170, val_acc:0.935]
Epoch [47/120    avg_loss:0.151, val_acc:0.934]
Epoch [48/120    avg_loss:0.152, val_acc:0.942]
Epoch [49/120    avg_loss:0.114, val_acc:0.945]
Epoch [50/120    avg_loss:0.115, val_acc:0.943]
Epoch [51/120    avg_loss:0.119, val_acc:0.943]
Epoch [52/120    avg_loss:0.124, val_acc:0.943]
Epoch [53/120    avg_loss:0.127, val_acc:0.931]
Epoch [54/120    avg_loss:0.140, val_acc:0.920]
Epoch [55/120    avg_loss:0.104, val_acc:0.957]
Epoch [56/120    avg_loss:0.114, val_acc:0.948]
Epoch [57/120    avg_loss:0.121, val_acc:0.949]
Epoch [58/120    avg_loss:0.116, val_acc:0.945]
Epoch [59/120    avg_loss:0.101, val_acc:0.957]
Epoch [60/120    avg_loss:0.093, val_acc:0.955]
Epoch [61/120    avg_loss:0.099, val_acc:0.955]
Epoch [62/120    avg_loss:0.089, val_acc:0.959]
Epoch [63/120    avg_loss:0.076, val_acc:0.956]
Epoch [64/120    avg_loss:0.079, val_acc:0.950]
Epoch [65/120    avg_loss:0.084, val_acc:0.936]
Epoch [66/120    avg_loss:0.091, val_acc:0.953]
Epoch [67/120    avg_loss:0.085, val_acc:0.948]
Epoch [68/120    avg_loss:0.083, val_acc:0.960]
Epoch [69/120    avg_loss:0.081, val_acc:0.945]
Epoch [70/120    avg_loss:0.068, val_acc:0.966]
Epoch [71/120    avg_loss:0.053, val_acc:0.963]
Epoch [72/120    avg_loss:0.064, val_acc:0.969]
Epoch [73/120    avg_loss:0.057, val_acc:0.969]
Epoch [74/120    avg_loss:0.060, val_acc:0.964]
Epoch [75/120    avg_loss:0.088, val_acc:0.953]
Epoch [76/120    avg_loss:0.129, val_acc:0.948]
Epoch [77/120    avg_loss:0.085, val_acc:0.956]
Epoch [78/120    avg_loss:0.064, val_acc:0.951]
Epoch [79/120    avg_loss:0.051, val_acc:0.958]
Epoch [80/120    avg_loss:0.051, val_acc:0.953]
Epoch [81/120    avg_loss:0.056, val_acc:0.964]
Epoch [82/120    avg_loss:0.073, val_acc:0.966]
Epoch [83/120    avg_loss:0.058, val_acc:0.969]
Epoch [84/120    avg_loss:0.060, val_acc:0.960]
Epoch [85/120    avg_loss:0.057, val_acc:0.965]
Epoch [86/120    avg_loss:0.053, val_acc:0.967]
Epoch [87/120    avg_loss:0.056, val_acc:0.935]
Epoch [88/120    avg_loss:0.049, val_acc:0.965]
Epoch [89/120    avg_loss:0.054, val_acc:0.957]
Epoch [90/120    avg_loss:0.052, val_acc:0.973]
Epoch [91/120    avg_loss:0.038, val_acc:0.965]
Epoch [92/120    avg_loss:0.043, val_acc:0.968]
Epoch [93/120    avg_loss:0.042, val_acc:0.970]
Epoch [94/120    avg_loss:0.048, val_acc:0.964]
Epoch [95/120    avg_loss:0.056, val_acc:0.957]
Epoch [96/120    avg_loss:0.048, val_acc:0.968]
Epoch [97/120    avg_loss:0.046, val_acc:0.967]
Epoch [98/120    avg_loss:0.055, val_acc:0.965]
Epoch [99/120    avg_loss:0.147, val_acc:0.958]
Epoch [100/120    avg_loss:0.076, val_acc:0.965]
Epoch [101/120    avg_loss:0.048, val_acc:0.967]
Epoch [102/120    avg_loss:0.041, val_acc:0.968]
Epoch [103/120    avg_loss:0.033, val_acc:0.969]
Epoch [104/120    avg_loss:0.030, val_acc:0.974]
Epoch [105/120    avg_loss:0.027, val_acc:0.976]
Epoch [106/120    avg_loss:0.029, val_acc:0.974]
Epoch [107/120    avg_loss:0.026, val_acc:0.973]
Epoch [108/120    avg_loss:0.027, val_acc:0.974]
Epoch [109/120    avg_loss:0.025, val_acc:0.977]
Epoch [110/120    avg_loss:0.026, val_acc:0.975]
Epoch [111/120    avg_loss:0.021, val_acc:0.975]
Epoch [112/120    avg_loss:0.024, val_acc:0.974]
Epoch [113/120    avg_loss:0.023, val_acc:0.975]
Epoch [114/120    avg_loss:0.023, val_acc:0.973]
Epoch [115/120    avg_loss:0.022, val_acc:0.974]
Epoch [116/120    avg_loss:0.019, val_acc:0.976]
Epoch [117/120    avg_loss:0.022, val_acc:0.975]
Epoch [118/120    avg_loss:0.023, val_acc:0.975]
Epoch [119/120    avg_loss:0.025, val_acc:0.974]
Epoch [120/120    avg_loss:0.024, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1240    0    0    0    1    0    0    0    3   38    1    0
     0    2    0]
 [   0    0    4  699    3   18    0    0    0    9    0    0   13    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  421    0    2    0    3    0    0    0    0
     9    0    0]
 [   0    0    0    0    0    0  650    0    0    1    0    6    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   12    0    0    2    0
     0    0    0]
 [   0    0   23   90    0    3    0    0    0    0  751    2    0    0
     0    6    0]
 [   0    0   18    0    0    0   13    0    0    0    7 2168    0    3
     1    0    0]
 [   0    0    0    4    9    8    0    0    0    0   11    6  494    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    4    3    0    0
  1131    0    0]
 [   0    0    1    0    0    0   26    0    0    0    0    0    0    0
   119  201    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.74254742547426

F1 scores:
[       nan 0.975      0.96460521 0.90544041 0.97260274 0.95141243
 0.96510765 0.96153846 0.99650757 0.55813953 0.90865094 0.97811866
 0.94364852 0.98930481 0.94289287 0.72302158 0.98224852]

Kappa:
0.9399859294141842
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe5bf935828>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.725, val_acc:0.383]
Epoch [2/120    avg_loss:2.418, val_acc:0.489]
Epoch [3/120    avg_loss:2.255, val_acc:0.518]
Epoch [4/120    avg_loss:2.071, val_acc:0.532]
Epoch [5/120    avg_loss:1.980, val_acc:0.565]
Epoch [6/120    avg_loss:1.831, val_acc:0.583]
Epoch [7/120    avg_loss:1.663, val_acc:0.591]
Epoch [8/120    avg_loss:1.606, val_acc:0.626]
Epoch [9/120    avg_loss:1.481, val_acc:0.658]
Epoch [10/120    avg_loss:1.400, val_acc:0.675]
Epoch [11/120    avg_loss:1.300, val_acc:0.689]
Epoch [12/120    avg_loss:1.217, val_acc:0.728]
Epoch [13/120    avg_loss:1.116, val_acc:0.726]
Epoch [14/120    avg_loss:1.022, val_acc:0.702]
Epoch [15/120    avg_loss:0.943, val_acc:0.730]
Epoch [16/120    avg_loss:0.877, val_acc:0.776]
Epoch [17/120    avg_loss:0.808, val_acc:0.760]
Epoch [18/120    avg_loss:0.766, val_acc:0.775]
Epoch [19/120    avg_loss:0.698, val_acc:0.805]
Epoch [20/120    avg_loss:0.620, val_acc:0.811]
Epoch [21/120    avg_loss:0.551, val_acc:0.830]
Epoch [22/120    avg_loss:0.543, val_acc:0.812]
Epoch [23/120    avg_loss:0.514, val_acc:0.828]
Epoch [24/120    avg_loss:0.474, val_acc:0.817]
Epoch [25/120    avg_loss:0.440, val_acc:0.844]
Epoch [26/120    avg_loss:0.431, val_acc:0.858]
Epoch [27/120    avg_loss:0.480, val_acc:0.848]
Epoch [28/120    avg_loss:0.383, val_acc:0.857]
Epoch [29/120    avg_loss:0.354, val_acc:0.866]
Epoch [30/120    avg_loss:0.331, val_acc:0.856]
Epoch [31/120    avg_loss:0.316, val_acc:0.875]
Epoch [32/120    avg_loss:0.354, val_acc:0.867]
Epoch [33/120    avg_loss:0.380, val_acc:0.845]
Epoch [34/120    avg_loss:0.349, val_acc:0.876]
Epoch [35/120    avg_loss:0.319, val_acc:0.897]
Epoch [36/120    avg_loss:0.295, val_acc:0.876]
Epoch [37/120    avg_loss:0.289, val_acc:0.859]
Epoch [38/120    avg_loss:0.359, val_acc:0.838]
Epoch [39/120    avg_loss:0.306, val_acc:0.899]
Epoch [40/120    avg_loss:0.269, val_acc:0.909]
Epoch [41/120    avg_loss:0.322, val_acc:0.898]
Epoch [42/120    avg_loss:0.265, val_acc:0.908]
Epoch [43/120    avg_loss:0.240, val_acc:0.906]
Epoch [44/120    avg_loss:0.239, val_acc:0.910]
Epoch [45/120    avg_loss:0.339, val_acc:0.845]
Epoch [46/120    avg_loss:0.319, val_acc:0.893]
Epoch [47/120    avg_loss:0.251, val_acc:0.887]
Epoch [48/120    avg_loss:0.210, val_acc:0.926]
Epoch [49/120    avg_loss:0.168, val_acc:0.925]
Epoch [50/120    avg_loss:0.189, val_acc:0.924]
Epoch [51/120    avg_loss:0.189, val_acc:0.925]
Epoch [52/120    avg_loss:0.169, val_acc:0.941]
Epoch [53/120    avg_loss:0.145, val_acc:0.934]
Epoch [54/120    avg_loss:0.118, val_acc:0.939]
Epoch [55/120    avg_loss:0.116, val_acc:0.944]
Epoch [56/120    avg_loss:0.103, val_acc:0.942]
Epoch [57/120    avg_loss:0.120, val_acc:0.934]
Epoch [58/120    avg_loss:0.108, val_acc:0.928]
Epoch [59/120    avg_loss:0.120, val_acc:0.945]
Epoch [60/120    avg_loss:0.117, val_acc:0.945]
Epoch [61/120    avg_loss:0.121, val_acc:0.936]
Epoch [62/120    avg_loss:0.107, val_acc:0.942]
Epoch [63/120    avg_loss:0.124, val_acc:0.919]
Epoch [64/120    avg_loss:0.157, val_acc:0.920]
Epoch [65/120    avg_loss:0.203, val_acc:0.933]
Epoch [66/120    avg_loss:0.172, val_acc:0.941]
Epoch [67/120    avg_loss:0.149, val_acc:0.933]
Epoch [68/120    avg_loss:0.116, val_acc:0.941]
Epoch [69/120    avg_loss:0.107, val_acc:0.940]
Epoch [70/120    avg_loss:0.097, val_acc:0.938]
Epoch [71/120    avg_loss:0.103, val_acc:0.949]
Epoch [72/120    avg_loss:0.121, val_acc:0.945]
Epoch [73/120    avg_loss:0.123, val_acc:0.950]
Epoch [74/120    avg_loss:0.098, val_acc:0.941]
Epoch [75/120    avg_loss:0.118, val_acc:0.936]
Epoch [76/120    avg_loss:0.081, val_acc:0.952]
Epoch [77/120    avg_loss:0.081, val_acc:0.943]
Epoch [78/120    avg_loss:0.081, val_acc:0.951]
Epoch [79/120    avg_loss:0.062, val_acc:0.952]
Epoch [80/120    avg_loss:0.068, val_acc:0.950]
Epoch [81/120    avg_loss:0.072, val_acc:0.950]
Epoch [82/120    avg_loss:0.080, val_acc:0.953]
Epoch [83/120    avg_loss:0.065, val_acc:0.960]
Epoch [84/120    avg_loss:0.061, val_acc:0.957]
Epoch [85/120    avg_loss:0.086, val_acc:0.950]
Epoch [86/120    avg_loss:0.088, val_acc:0.951]
Epoch [87/120    avg_loss:0.068, val_acc:0.947]
Epoch [88/120    avg_loss:0.077, val_acc:0.955]
Epoch [89/120    avg_loss:0.074, val_acc:0.964]
Epoch [90/120    avg_loss:0.084, val_acc:0.956]
Epoch [91/120    avg_loss:0.064, val_acc:0.957]
Epoch [92/120    avg_loss:0.060, val_acc:0.955]
Epoch [93/120    avg_loss:0.088, val_acc:0.953]
Epoch [94/120    avg_loss:0.121, val_acc:0.935]
Epoch [95/120    avg_loss:0.093, val_acc:0.943]
Epoch [96/120    avg_loss:0.086, val_acc:0.948]
Epoch [97/120    avg_loss:0.062, val_acc:0.948]
Epoch [98/120    avg_loss:0.065, val_acc:0.949]
Epoch [99/120    avg_loss:0.062, val_acc:0.960]
Epoch [100/120    avg_loss:0.057, val_acc:0.964]
Epoch [101/120    avg_loss:0.053, val_acc:0.950]
Epoch [102/120    avg_loss:0.040, val_acc:0.957]
Epoch [103/120    avg_loss:0.050, val_acc:0.964]
Epoch [104/120    avg_loss:0.050, val_acc:0.959]
Epoch [105/120    avg_loss:0.038, val_acc:0.958]
Epoch [106/120    avg_loss:0.043, val_acc:0.961]
Epoch [107/120    avg_loss:0.057, val_acc:0.961]
Epoch [108/120    avg_loss:0.052, val_acc:0.958]
Epoch [109/120    avg_loss:0.055, val_acc:0.955]
Epoch [110/120    avg_loss:0.044, val_acc:0.960]
Epoch [111/120    avg_loss:0.054, val_acc:0.957]
Epoch [112/120    avg_loss:0.045, val_acc:0.964]
Epoch [113/120    avg_loss:0.041, val_acc:0.963]
Epoch [114/120    avg_loss:0.050, val_acc:0.964]
Epoch [115/120    avg_loss:0.045, val_acc:0.963]
Epoch [116/120    avg_loss:0.043, val_acc:0.966]
Epoch [117/120    avg_loss:0.031, val_acc:0.964]
Epoch [118/120    avg_loss:0.028, val_acc:0.961]
Epoch [119/120    avg_loss:0.035, val_acc:0.960]
Epoch [120/120    avg_loss:0.027, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1175   15    8    0    3    0    0    0    4   78    2    0
     0    0    0]
 [   0    0    0  713    1   15    0    0    0   13    0    0    3    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    0    0    5    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   15    0    0    1    0
     0    0    0]
 [   0    0   50   89    0   13    0    0    0    0  720    1    1    0
     1    0    0]
 [   0    0   11    0    0    0    8    0    0    0    0 2188    0    2
     1    0    0]
 [   0    0    3   25   17   11    0    0    0    2   10    0  458    0
     0    0    8]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0  182
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    6    0    0    0
  1132    0    0]
 [   0    0    0    0    0    1   40    0    0    1    0    0    2    0
    80  223    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
93.99457994579946

F1 scores:
[       nan 0.975      0.93032462 0.89629164 0.94247788 0.93777778
 0.96187683 1.         0.99767442 0.55555556 0.89164087 0.97722197
 0.91144279 0.98113208 0.95891571 0.78245614 0.93641618]

Kappa:
0.931444473061507
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa8b2f1a8d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.800, val_acc:0.277]
Epoch [2/120    avg_loss:2.468, val_acc:0.474]
Epoch [3/120    avg_loss:2.284, val_acc:0.472]
Epoch [4/120    avg_loss:2.154, val_acc:0.541]
Epoch [5/120    avg_loss:2.037, val_acc:0.510]
Epoch [6/120    avg_loss:1.951, val_acc:0.580]
Epoch [7/120    avg_loss:1.840, val_acc:0.611]
Epoch [8/120    avg_loss:1.742, val_acc:0.608]
Epoch [9/120    avg_loss:1.638, val_acc:0.634]
Epoch [10/120    avg_loss:1.561, val_acc:0.634]
Epoch [11/120    avg_loss:1.376, val_acc:0.660]
Epoch [12/120    avg_loss:1.229, val_acc:0.680]
Epoch [13/120    avg_loss:1.140, val_acc:0.681]
Epoch [14/120    avg_loss:1.038, val_acc:0.720]
Epoch [15/120    avg_loss:0.931, val_acc:0.723]
Epoch [16/120    avg_loss:0.872, val_acc:0.732]
Epoch [17/120    avg_loss:0.866, val_acc:0.724]
Epoch [18/120    avg_loss:0.792, val_acc:0.764]
Epoch [19/120    avg_loss:0.723, val_acc:0.800]
Epoch [20/120    avg_loss:0.670, val_acc:0.803]
Epoch [21/120    avg_loss:0.610, val_acc:0.818]
Epoch [22/120    avg_loss:0.555, val_acc:0.814]
Epoch [23/120    avg_loss:0.569, val_acc:0.841]
Epoch [24/120    avg_loss:0.560, val_acc:0.811]
Epoch [25/120    avg_loss:0.497, val_acc:0.855]
Epoch [26/120    avg_loss:0.445, val_acc:0.861]
Epoch [27/120    avg_loss:0.388, val_acc:0.869]
Epoch [28/120    avg_loss:0.371, val_acc:0.838]
Epoch [29/120    avg_loss:0.374, val_acc:0.882]
Epoch [30/120    avg_loss:0.307, val_acc:0.897]
Epoch [31/120    avg_loss:0.333, val_acc:0.859]
Epoch [32/120    avg_loss:0.366, val_acc:0.851]
Epoch [33/120    avg_loss:0.325, val_acc:0.901]
Epoch [34/120    avg_loss:0.303, val_acc:0.899]
Epoch [35/120    avg_loss:0.278, val_acc:0.902]
Epoch [36/120    avg_loss:0.239, val_acc:0.911]
Epoch [37/120    avg_loss:0.237, val_acc:0.924]
Epoch [38/120    avg_loss:0.206, val_acc:0.915]
Epoch [39/120    avg_loss:0.180, val_acc:0.931]
Epoch [40/120    avg_loss:0.193, val_acc:0.922]
Epoch [41/120    avg_loss:0.207, val_acc:0.908]
Epoch [42/120    avg_loss:0.235, val_acc:0.920]
Epoch [43/120    avg_loss:0.192, val_acc:0.935]
Epoch [44/120    avg_loss:0.171, val_acc:0.916]
Epoch [45/120    avg_loss:0.202, val_acc:0.925]
Epoch [46/120    avg_loss:0.193, val_acc:0.926]
Epoch [47/120    avg_loss:0.180, val_acc:0.936]
Epoch [48/120    avg_loss:0.167, val_acc:0.909]
Epoch [49/120    avg_loss:0.176, val_acc:0.922]
Epoch [50/120    avg_loss:0.201, val_acc:0.915]
Epoch [51/120    avg_loss:0.155, val_acc:0.943]
Epoch [52/120    avg_loss:0.190, val_acc:0.906]
Epoch [53/120    avg_loss:0.182, val_acc:0.932]
Epoch [54/120    avg_loss:0.136, val_acc:0.941]
Epoch [55/120    avg_loss:0.142, val_acc:0.928]
Epoch [56/120    avg_loss:0.148, val_acc:0.919]
Epoch [57/120    avg_loss:0.142, val_acc:0.939]
Epoch [58/120    avg_loss:0.100, val_acc:0.947]
Epoch [59/120    avg_loss:0.121, val_acc:0.928]
Epoch [60/120    avg_loss:0.112, val_acc:0.944]
Epoch [61/120    avg_loss:0.120, val_acc:0.932]
Epoch [62/120    avg_loss:0.104, val_acc:0.939]
Epoch [63/120    avg_loss:0.139, val_acc:0.943]
Epoch [64/120    avg_loss:0.104, val_acc:0.958]
Epoch [65/120    avg_loss:0.087, val_acc:0.959]
Epoch [66/120    avg_loss:0.094, val_acc:0.941]
Epoch [67/120    avg_loss:0.084, val_acc:0.958]
Epoch [68/120    avg_loss:0.076, val_acc:0.949]
Epoch [69/120    avg_loss:0.083, val_acc:0.965]
Epoch [70/120    avg_loss:0.070, val_acc:0.958]
Epoch [71/120    avg_loss:0.087, val_acc:0.941]
Epoch [72/120    avg_loss:0.074, val_acc:0.969]
Epoch [73/120    avg_loss:0.071, val_acc:0.956]
Epoch [74/120    avg_loss:0.063, val_acc:0.960]
Epoch [75/120    avg_loss:0.056, val_acc:0.952]
Epoch [76/120    avg_loss:0.067, val_acc:0.958]
Epoch [77/120    avg_loss:0.065, val_acc:0.959]
Epoch [78/120    avg_loss:0.062, val_acc:0.968]
Epoch [79/120    avg_loss:0.053, val_acc:0.964]
Epoch [80/120    avg_loss:0.064, val_acc:0.961]
Epoch [81/120    avg_loss:0.078, val_acc:0.952]
Epoch [82/120    avg_loss:0.054, val_acc:0.960]
Epoch [83/120    avg_loss:0.059, val_acc:0.977]
Epoch [84/120    avg_loss:0.057, val_acc:0.961]
Epoch [85/120    avg_loss:0.046, val_acc:0.965]
Epoch [86/120    avg_loss:0.053, val_acc:0.953]
Epoch [87/120    avg_loss:0.062, val_acc:0.961]
Epoch [88/120    avg_loss:0.055, val_acc:0.957]
Epoch [89/120    avg_loss:0.056, val_acc:0.957]
Epoch [90/120    avg_loss:0.047, val_acc:0.966]
Epoch [91/120    avg_loss:0.049, val_acc:0.956]
Epoch [92/120    avg_loss:0.056, val_acc:0.957]
Epoch [93/120    avg_loss:0.083, val_acc:0.955]
Epoch [94/120    avg_loss:0.073, val_acc:0.964]
Epoch [95/120    avg_loss:0.059, val_acc:0.960]
Epoch [96/120    avg_loss:0.050, val_acc:0.967]
Epoch [97/120    avg_loss:0.037, val_acc:0.972]
Epoch [98/120    avg_loss:0.034, val_acc:0.974]
Epoch [99/120    avg_loss:0.033, val_acc:0.978]
Epoch [100/120    avg_loss:0.031, val_acc:0.976]
Epoch [101/120    avg_loss:0.028, val_acc:0.978]
Epoch [102/120    avg_loss:0.030, val_acc:0.978]
Epoch [103/120    avg_loss:0.025, val_acc:0.978]
Epoch [104/120    avg_loss:0.026, val_acc:0.977]
Epoch [105/120    avg_loss:0.023, val_acc:0.975]
Epoch [106/120    avg_loss:0.025, val_acc:0.977]
Epoch [107/120    avg_loss:0.031, val_acc:0.977]
Epoch [108/120    avg_loss:0.022, val_acc:0.976]
Epoch [109/120    avg_loss:0.023, val_acc:0.977]
Epoch [110/120    avg_loss:0.025, val_acc:0.977]
Epoch [111/120    avg_loss:0.024, val_acc:0.978]
Epoch [112/120    avg_loss:0.021, val_acc:0.977]
Epoch [113/120    avg_loss:0.024, val_acc:0.975]
Epoch [114/120    avg_loss:0.020, val_acc:0.977]
Epoch [115/120    avg_loss:0.023, val_acc:0.977]
Epoch [116/120    avg_loss:0.026, val_acc:0.977]
Epoch [117/120    avg_loss:0.027, val_acc:0.977]
Epoch [118/120    avg_loss:0.030, val_acc:0.977]
Epoch [119/120    avg_loss:0.026, val_acc:0.977]
Epoch [120/120    avg_loss:0.023, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1243    7    0    0    3    0    0    0    2   16    0    0
     0   14    0]
 [   0    0    6  716    1    7    0    0    0    6    0    0   10    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    3    0    3    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   14    0    4    0    0
     0    0    0]
 [   0    0   21   89    0    5    0    0    0    0  737   15    0    0
     0    8    0]
 [   0    0   22    0    0    2    4    0    0    0    1 2175    0    3
     3    0    0]
 [   0    0    0   40   12    6    0    0    0    0   12    3  450    0
     0    0   11]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    1   10    0    0    0    0    0    0    3
   101  232    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
94.8509485094851

F1 scores:
[       nan 0.94871795 0.96468762 0.89555972 0.97038724 0.96245734
 0.98493976 0.94339623 0.99883856 0.68292683 0.90208078 0.98282874
 0.90270812 0.98143236 0.95218121 0.77204659 0.92045455]

Kappa:
0.9412425952596065
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f45134c47f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.762, val_acc:0.277]
Epoch [2/120    avg_loss:2.484, val_acc:0.334]
Epoch [3/120    avg_loss:2.292, val_acc:0.440]
Epoch [4/120    avg_loss:2.120, val_acc:0.426]
Epoch [5/120    avg_loss:2.021, val_acc:0.500]
Epoch [6/120    avg_loss:1.910, val_acc:0.518]
Epoch [7/120    avg_loss:1.826, val_acc:0.564]
Epoch [8/120    avg_loss:1.775, val_acc:0.565]
Epoch [9/120    avg_loss:1.668, val_acc:0.600]
Epoch [10/120    avg_loss:1.563, val_acc:0.619]
Epoch [11/120    avg_loss:1.437, val_acc:0.653]
Epoch [12/120    avg_loss:1.343, val_acc:0.670]
Epoch [13/120    avg_loss:1.261, val_acc:0.672]
Epoch [14/120    avg_loss:1.149, val_acc:0.689]
Epoch [15/120    avg_loss:1.020, val_acc:0.698]
Epoch [16/120    avg_loss:0.975, val_acc:0.738]
Epoch [17/120    avg_loss:0.867, val_acc:0.736]
Epoch [18/120    avg_loss:0.860, val_acc:0.723]
Epoch [19/120    avg_loss:0.770, val_acc:0.753]
Epoch [20/120    avg_loss:0.741, val_acc:0.755]
Epoch [21/120    avg_loss:0.643, val_acc:0.786]
Epoch [22/120    avg_loss:0.601, val_acc:0.803]
Epoch [23/120    avg_loss:0.552, val_acc:0.801]
Epoch [24/120    avg_loss:0.488, val_acc:0.810]
Epoch [25/120    avg_loss:0.780, val_acc:0.761]
Epoch [26/120    avg_loss:0.553, val_acc:0.822]
Epoch [27/120    avg_loss:0.478, val_acc:0.783]
Epoch [28/120    avg_loss:0.558, val_acc:0.809]
Epoch [29/120    avg_loss:0.453, val_acc:0.842]
Epoch [30/120    avg_loss:0.369, val_acc:0.851]
Epoch [31/120    avg_loss:0.348, val_acc:0.861]
Epoch [32/120    avg_loss:0.352, val_acc:0.872]
Epoch [33/120    avg_loss:0.336, val_acc:0.861]
Epoch [34/120    avg_loss:0.308, val_acc:0.869]
Epoch [35/120    avg_loss:0.366, val_acc:0.823]
Epoch [36/120    avg_loss:0.302, val_acc:0.847]
Epoch [37/120    avg_loss:0.278, val_acc:0.890]
Epoch [38/120    avg_loss:0.238, val_acc:0.892]
Epoch [39/120    avg_loss:0.234, val_acc:0.895]
Epoch [40/120    avg_loss:0.271, val_acc:0.852]
Epoch [41/120    avg_loss:0.251, val_acc:0.882]
Epoch [42/120    avg_loss:0.210, val_acc:0.903]
Epoch [43/120    avg_loss:0.196, val_acc:0.900]
Epoch [44/120    avg_loss:0.176, val_acc:0.905]
Epoch [45/120    avg_loss:0.236, val_acc:0.898]
Epoch [46/120    avg_loss:0.183, val_acc:0.916]
Epoch [47/120    avg_loss:0.162, val_acc:0.928]
Epoch [48/120    avg_loss:0.137, val_acc:0.926]
Epoch [49/120    avg_loss:0.191, val_acc:0.885]
Epoch [50/120    avg_loss:0.204, val_acc:0.887]
Epoch [51/120    avg_loss:0.199, val_acc:0.880]
Epoch [52/120    avg_loss:0.306, val_acc:0.902]
Epoch [53/120    avg_loss:0.194, val_acc:0.876]
Epoch [54/120    avg_loss:0.173, val_acc:0.908]
Epoch [55/120    avg_loss:0.199, val_acc:0.891]
Epoch [56/120    avg_loss:0.157, val_acc:0.915]
Epoch [57/120    avg_loss:0.143, val_acc:0.910]
Epoch [58/120    avg_loss:0.169, val_acc:0.920]
Epoch [59/120    avg_loss:0.135, val_acc:0.932]
Epoch [60/120    avg_loss:0.106, val_acc:0.934]
Epoch [61/120    avg_loss:0.100, val_acc:0.917]
Epoch [62/120    avg_loss:0.106, val_acc:0.938]
Epoch [63/120    avg_loss:0.114, val_acc:0.927]
Epoch [64/120    avg_loss:0.087, val_acc:0.930]
Epoch [65/120    avg_loss:0.081, val_acc:0.941]
Epoch [66/120    avg_loss:0.079, val_acc:0.917]
Epoch [67/120    avg_loss:0.089, val_acc:0.934]
Epoch [68/120    avg_loss:0.088, val_acc:0.936]
Epoch [69/120    avg_loss:0.085, val_acc:0.934]
Epoch [70/120    avg_loss:0.084, val_acc:0.932]
Epoch [71/120    avg_loss:0.090, val_acc:0.934]
Epoch [72/120    avg_loss:0.092, val_acc:0.942]
Epoch [73/120    avg_loss:0.067, val_acc:0.947]
Epoch [74/120    avg_loss:0.062, val_acc:0.934]
Epoch [75/120    avg_loss:0.064, val_acc:0.943]
Epoch [76/120    avg_loss:0.077, val_acc:0.923]
Epoch [77/120    avg_loss:0.084, val_acc:0.952]
Epoch [78/120    avg_loss:0.069, val_acc:0.947]
Epoch [79/120    avg_loss:0.088, val_acc:0.917]
Epoch [80/120    avg_loss:0.154, val_acc:0.922]
Epoch [81/120    avg_loss:0.104, val_acc:0.922]
Epoch [82/120    avg_loss:0.085, val_acc:0.934]
Epoch [83/120    avg_loss:0.064, val_acc:0.940]
Epoch [84/120    avg_loss:0.075, val_acc:0.933]
Epoch [85/120    avg_loss:0.057, val_acc:0.940]
Epoch [86/120    avg_loss:0.062, val_acc:0.945]
Epoch [87/120    avg_loss:0.060, val_acc:0.952]
Epoch [88/120    avg_loss:0.052, val_acc:0.947]
Epoch [89/120    avg_loss:0.053, val_acc:0.936]
Epoch [90/120    avg_loss:0.062, val_acc:0.955]
Epoch [91/120    avg_loss:0.041, val_acc:0.952]
Epoch [92/120    avg_loss:0.043, val_acc:0.942]
Epoch [93/120    avg_loss:0.045, val_acc:0.959]
Epoch [94/120    avg_loss:0.038, val_acc:0.964]
Epoch [95/120    avg_loss:0.050, val_acc:0.952]
Epoch [96/120    avg_loss:0.057, val_acc:0.951]
Epoch [97/120    avg_loss:0.060, val_acc:0.944]
Epoch [98/120    avg_loss:0.046, val_acc:0.940]
Epoch [99/120    avg_loss:0.061, val_acc:0.949]
Epoch [100/120    avg_loss:0.042, val_acc:0.957]
Epoch [101/120    avg_loss:0.085, val_acc:0.944]
Epoch [102/120    avg_loss:0.082, val_acc:0.939]
Epoch [103/120    avg_loss:0.113, val_acc:0.922]
Epoch [104/120    avg_loss:0.078, val_acc:0.950]
Epoch [105/120    avg_loss:0.083, val_acc:0.918]
Epoch [106/120    avg_loss:0.062, val_acc:0.948]
Epoch [107/120    avg_loss:0.053, val_acc:0.953]
Epoch [108/120    avg_loss:0.048, val_acc:0.964]
Epoch [109/120    avg_loss:0.040, val_acc:0.963]
Epoch [110/120    avg_loss:0.034, val_acc:0.963]
Epoch [111/120    avg_loss:0.032, val_acc:0.961]
Epoch [112/120    avg_loss:0.035, val_acc:0.965]
Epoch [113/120    avg_loss:0.030, val_acc:0.966]
Epoch [114/120    avg_loss:0.031, val_acc:0.965]
Epoch [115/120    avg_loss:0.029, val_acc:0.961]
Epoch [116/120    avg_loss:0.030, val_acc:0.964]
Epoch [117/120    avg_loss:0.026, val_acc:0.966]
Epoch [118/120    avg_loss:0.028, val_acc:0.967]
Epoch [119/120    avg_loss:0.028, val_acc:0.964]
Epoch [120/120    avg_loss:0.027, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    7 1203    0    8    0    4    0    0    0   21   37    0    0
     0    5    0]
 [   0    0    3  686    5    5    0    0    0   16    0    0   30    0
     2    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  413    0    4    0    1    0    0    0    0
    17    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    5    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   27   81    0    4    0    0    0    0  753    9    0    0
     0    1    0]
 [   0    0   10    0    0    0    3    0    3    0   14 2169    5    2
     4    0    0]
 [   0    0    0   11    7    8    0    0    0    0   12    1  490    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    1    0    0
  1137    0    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    1    0
   122  201    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.2439024390244

F1 scores:
[       nan 0.87058824 0.95174051 0.89967213 0.95515695 0.95491329
 0.97386109 0.92592593 0.99652375 0.67924528 0.89642857 0.97879061
 0.92278719 0.99462366 0.93928129 0.72563177 0.95906433]

Kappa:
0.9343167434830137
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd2dfa767b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.710, val_acc:0.338]
Epoch [2/120    avg_loss:2.447, val_acc:0.476]
Epoch [3/120    avg_loss:2.250, val_acc:0.491]
Epoch [4/120    avg_loss:2.106, val_acc:0.543]
Epoch [5/120    avg_loss:1.974, val_acc:0.583]
Epoch [6/120    avg_loss:1.864, val_acc:0.590]
Epoch [7/120    avg_loss:1.760, val_acc:0.599]
Epoch [8/120    avg_loss:1.654, val_acc:0.607]
Epoch [9/120    avg_loss:1.507, val_acc:0.613]
Epoch [10/120    avg_loss:1.421, val_acc:0.635]
Epoch [11/120    avg_loss:1.300, val_acc:0.640]
Epoch [12/120    avg_loss:1.199, val_acc:0.678]
Epoch [13/120    avg_loss:1.052, val_acc:0.684]
Epoch [14/120    avg_loss:0.994, val_acc:0.722]
Epoch [15/120    avg_loss:0.890, val_acc:0.713]
Epoch [16/120    avg_loss:0.833, val_acc:0.744]
Epoch [17/120    avg_loss:0.789, val_acc:0.768]
Epoch [18/120    avg_loss:0.744, val_acc:0.793]
Epoch [19/120    avg_loss:0.741, val_acc:0.762]
Epoch [20/120    avg_loss:0.786, val_acc:0.803]
Epoch [21/120    avg_loss:0.634, val_acc:0.800]
Epoch [22/120    avg_loss:0.560, val_acc:0.844]
Epoch [23/120    avg_loss:0.520, val_acc:0.828]
Epoch [24/120    avg_loss:0.459, val_acc:0.850]
Epoch [25/120    avg_loss:0.398, val_acc:0.861]
Epoch [26/120    avg_loss:0.453, val_acc:0.844]
Epoch [27/120    avg_loss:0.455, val_acc:0.853]
Epoch [28/120    avg_loss:0.403, val_acc:0.836]
Epoch [29/120    avg_loss:0.402, val_acc:0.860]
Epoch [30/120    avg_loss:0.350, val_acc:0.842]
Epoch [31/120    avg_loss:0.293, val_acc:0.887]
Epoch [32/120    avg_loss:0.322, val_acc:0.891]
Epoch [33/120    avg_loss:0.340, val_acc:0.886]
Epoch [34/120    avg_loss:0.284, val_acc:0.885]
Epoch [35/120    avg_loss:0.273, val_acc:0.876]
Epoch [36/120    avg_loss:0.315, val_acc:0.907]
Epoch [37/120    avg_loss:0.260, val_acc:0.908]
Epoch [38/120    avg_loss:0.204, val_acc:0.903]
Epoch [39/120    avg_loss:0.201, val_acc:0.912]
Epoch [40/120    avg_loss:0.220, val_acc:0.908]
Epoch [41/120    avg_loss:0.243, val_acc:0.917]
Epoch [42/120    avg_loss:0.216, val_acc:0.911]
Epoch [43/120    avg_loss:0.193, val_acc:0.928]
Epoch [44/120    avg_loss:0.181, val_acc:0.936]
Epoch [45/120    avg_loss:0.198, val_acc:0.923]
Epoch [46/120    avg_loss:0.155, val_acc:0.943]
Epoch [47/120    avg_loss:0.162, val_acc:0.911]
Epoch [48/120    avg_loss:0.151, val_acc:0.933]
Epoch [49/120    avg_loss:0.139, val_acc:0.939]
Epoch [50/120    avg_loss:0.127, val_acc:0.925]
Epoch [51/120    avg_loss:0.107, val_acc:0.948]
Epoch [52/120    avg_loss:0.116, val_acc:0.942]
Epoch [53/120    avg_loss:0.115, val_acc:0.955]
Epoch [54/120    avg_loss:0.090, val_acc:0.945]
Epoch [55/120    avg_loss:0.094, val_acc:0.947]
Epoch [56/120    avg_loss:0.080, val_acc:0.947]
Epoch [57/120    avg_loss:0.106, val_acc:0.935]
Epoch [58/120    avg_loss:0.080, val_acc:0.945]
Epoch [59/120    avg_loss:0.118, val_acc:0.933]
Epoch [60/120    avg_loss:0.123, val_acc:0.931]
Epoch [61/120    avg_loss:0.094, val_acc:0.947]
Epoch [62/120    avg_loss:0.110, val_acc:0.951]
Epoch [63/120    avg_loss:0.072, val_acc:0.950]
Epoch [64/120    avg_loss:0.076, val_acc:0.945]
Epoch [65/120    avg_loss:0.079, val_acc:0.956]
Epoch [66/120    avg_loss:0.083, val_acc:0.943]
Epoch [67/120    avg_loss:0.098, val_acc:0.941]
Epoch [68/120    avg_loss:0.092, val_acc:0.948]
Epoch [69/120    avg_loss:0.092, val_acc:0.956]
Epoch [70/120    avg_loss:0.081, val_acc:0.935]
Epoch [71/120    avg_loss:0.064, val_acc:0.953]
Epoch [72/120    avg_loss:0.064, val_acc:0.944]
Epoch [73/120    avg_loss:0.076, val_acc:0.953]
Epoch [74/120    avg_loss:0.073, val_acc:0.956]
Epoch [75/120    avg_loss:0.087, val_acc:0.953]
Epoch [76/120    avg_loss:0.068, val_acc:0.963]
Epoch [77/120    avg_loss:0.072, val_acc:0.967]
Epoch [78/120    avg_loss:0.049, val_acc:0.956]
Epoch [79/120    avg_loss:0.048, val_acc:0.953]
Epoch [80/120    avg_loss:0.054, val_acc:0.950]
Epoch [81/120    avg_loss:0.064, val_acc:0.936]
Epoch [82/120    avg_loss:0.425, val_acc:0.848]
Epoch [83/120    avg_loss:0.252, val_acc:0.900]
Epoch [84/120    avg_loss:0.143, val_acc:0.920]
Epoch [85/120    avg_loss:0.100, val_acc:0.936]
Epoch [86/120    avg_loss:0.081, val_acc:0.940]
Epoch [87/120    avg_loss:0.087, val_acc:0.926]
Epoch [88/120    avg_loss:0.107, val_acc:0.926]
Epoch [89/120    avg_loss:0.082, val_acc:0.943]
Epoch [90/120    avg_loss:0.072, val_acc:0.948]
Epoch [91/120    avg_loss:0.049, val_acc:0.948]
Epoch [92/120    avg_loss:0.048, val_acc:0.948]
Epoch [93/120    avg_loss:0.045, val_acc:0.950]
Epoch [94/120    avg_loss:0.042, val_acc:0.950]
Epoch [95/120    avg_loss:0.044, val_acc:0.951]
Epoch [96/120    avg_loss:0.038, val_acc:0.960]
Epoch [97/120    avg_loss:0.034, val_acc:0.961]
Epoch [98/120    avg_loss:0.036, val_acc:0.963]
Epoch [99/120    avg_loss:0.044, val_acc:0.960]
Epoch [100/120    avg_loss:0.040, val_acc:0.961]
Epoch [101/120    avg_loss:0.037, val_acc:0.961]
Epoch [102/120    avg_loss:0.050, val_acc:0.956]
Epoch [103/120    avg_loss:0.047, val_acc:0.957]
Epoch [104/120    avg_loss:0.038, val_acc:0.957]
Epoch [105/120    avg_loss:0.041, val_acc:0.957]
Epoch [106/120    avg_loss:0.036, val_acc:0.958]
Epoch [107/120    avg_loss:0.036, val_acc:0.958]
Epoch [108/120    avg_loss:0.037, val_acc:0.958]
Epoch [109/120    avg_loss:0.033, val_acc:0.958]
Epoch [110/120    avg_loss:0.032, val_acc:0.958]
Epoch [111/120    avg_loss:0.032, val_acc:0.958]
Epoch [112/120    avg_loss:0.036, val_acc:0.958]
Epoch [113/120    avg_loss:0.036, val_acc:0.958]
Epoch [114/120    avg_loss:0.033, val_acc:0.959]
Epoch [115/120    avg_loss:0.034, val_acc:0.959]
Epoch [116/120    avg_loss:0.033, val_acc:0.960]
Epoch [117/120    avg_loss:0.036, val_acc:0.960]
Epoch [118/120    avg_loss:0.034, val_acc:0.960]
Epoch [119/120    avg_loss:0.033, val_acc:0.960]
Epoch [120/120    avg_loss:0.032, val_acc:0.960]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    0    0    0    0    0    0    0    0    6    0    0    0
     0    0    0]
 [   0    4 1164    5    0    0    4    0    0    3    9   90    5    1
     0    0    0]
 [   0    0    0  691    1    7    0    0    0    9    0    0   38    1
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  652    0    0    1    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   29   89    0    2    1    0    0    0  742    3    1    0
     0    8    0]
 [   0    0    9    0    0    0    4    0    5    0    7 2170    0    4
    11    0    0]
 [   0    0    5    2    7    6    0    0    0    0   12    6  479    0
     0    1   16]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    3    0    3    2    0    0
  1131    0    0]
 [   0    0    0    0    0    0    0    0    0    5    0    0    0    0
   102  240    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.1680216802168

F1 scores:
[       nan 0.875      0.93418941 0.8991542  0.97685185 0.97959184
 0.98937785 1.         0.99078341 0.58181818 0.89721886 0.96767001
 0.90548204 0.98404255 0.94922367 0.80536913 0.91304348]

Kappa:
0.9334325026594541
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc529383c18>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.713, val_acc:0.176]
Epoch [2/120    avg_loss:2.411, val_acc:0.323]
Epoch [3/120    avg_loss:2.245, val_acc:0.476]
Epoch [4/120    avg_loss:2.138, val_acc:0.508]
Epoch [5/120    avg_loss:2.027, val_acc:0.525]
Epoch [6/120    avg_loss:1.919, val_acc:0.532]
Epoch [7/120    avg_loss:1.868, val_acc:0.551]
Epoch [8/120    avg_loss:1.759, val_acc:0.583]
Epoch [9/120    avg_loss:1.675, val_acc:0.603]
Epoch [10/120    avg_loss:1.525, val_acc:0.627]
Epoch [11/120    avg_loss:1.459, val_acc:0.662]
Epoch [12/120    avg_loss:1.338, val_acc:0.666]
Epoch [13/120    avg_loss:1.256, val_acc:0.684]
Epoch [14/120    avg_loss:1.138, val_acc:0.731]
Epoch [15/120    avg_loss:1.002, val_acc:0.747]
Epoch [16/120    avg_loss:0.929, val_acc:0.744]
Epoch [17/120    avg_loss:0.854, val_acc:0.751]
Epoch [18/120    avg_loss:0.754, val_acc:0.802]
Epoch [19/120    avg_loss:0.661, val_acc:0.777]
Epoch [20/120    avg_loss:0.661, val_acc:0.784]
Epoch [21/120    avg_loss:0.617, val_acc:0.822]
Epoch [22/120    avg_loss:0.537, val_acc:0.814]
Epoch [23/120    avg_loss:0.550, val_acc:0.823]
Epoch [24/120    avg_loss:0.526, val_acc:0.826]
Epoch [25/120    avg_loss:0.501, val_acc:0.826]
Epoch [26/120    avg_loss:0.419, val_acc:0.831]
Epoch [27/120    avg_loss:0.400, val_acc:0.866]
Epoch [28/120    avg_loss:0.398, val_acc:0.791]
Epoch [29/120    avg_loss:0.365, val_acc:0.857]
Epoch [30/120    avg_loss:0.361, val_acc:0.838]
Epoch [31/120    avg_loss:0.360, val_acc:0.876]
Epoch [32/120    avg_loss:0.363, val_acc:0.826]
Epoch [33/120    avg_loss:0.306, val_acc:0.873]
Epoch [34/120    avg_loss:0.309, val_acc:0.869]
Epoch [35/120    avg_loss:0.302, val_acc:0.856]
Epoch [36/120    avg_loss:0.305, val_acc:0.863]
Epoch [37/120    avg_loss:0.309, val_acc:0.842]
Epoch [38/120    avg_loss:0.290, val_acc:0.869]
Epoch [39/120    avg_loss:0.268, val_acc:0.887]
Epoch [40/120    avg_loss:0.232, val_acc:0.889]
Epoch [41/120    avg_loss:0.189, val_acc:0.895]
Epoch [42/120    avg_loss:0.201, val_acc:0.882]
Epoch [43/120    avg_loss:0.189, val_acc:0.898]
Epoch [44/120    avg_loss:0.186, val_acc:0.894]
Epoch [45/120    avg_loss:0.184, val_acc:0.902]
Epoch [46/120    avg_loss:0.163, val_acc:0.908]
Epoch [47/120    avg_loss:0.170, val_acc:0.912]
Epoch [48/120    avg_loss:0.182, val_acc:0.899]
Epoch [49/120    avg_loss:0.157, val_acc:0.919]
Epoch [50/120    avg_loss:0.124, val_acc:0.908]
Epoch [51/120    avg_loss:0.135, val_acc:0.918]
Epoch [52/120    avg_loss:0.138, val_acc:0.912]
Epoch [53/120    avg_loss:0.119, val_acc:0.910]
Epoch [54/120    avg_loss:0.141, val_acc:0.910]
Epoch [55/120    avg_loss:0.115, val_acc:0.915]
Epoch [56/120    avg_loss:0.139, val_acc:0.909]
Epoch [57/120    avg_loss:0.148, val_acc:0.914]
Epoch [58/120    avg_loss:0.174, val_acc:0.900]
Epoch [59/120    avg_loss:0.178, val_acc:0.914]
Epoch [60/120    avg_loss:0.205, val_acc:0.876]
Epoch [61/120    avg_loss:0.330, val_acc:0.892]
Epoch [62/120    avg_loss:0.164, val_acc:0.909]
Epoch [63/120    avg_loss:0.153, val_acc:0.914]
Epoch [64/120    avg_loss:0.110, val_acc:0.925]
Epoch [65/120    avg_loss:0.107, val_acc:0.926]
Epoch [66/120    avg_loss:0.097, val_acc:0.925]
Epoch [67/120    avg_loss:0.096, val_acc:0.931]
Epoch [68/120    avg_loss:0.095, val_acc:0.927]
Epoch [69/120    avg_loss:0.081, val_acc:0.925]
Epoch [70/120    avg_loss:0.093, val_acc:0.930]
Epoch [71/120    avg_loss:0.096, val_acc:0.927]
Epoch [72/120    avg_loss:0.075, val_acc:0.927]
Epoch [73/120    avg_loss:0.085, val_acc:0.930]
Epoch [74/120    avg_loss:0.084, val_acc:0.932]
Epoch [75/120    avg_loss:0.093, val_acc:0.928]
Epoch [76/120    avg_loss:0.074, val_acc:0.932]
Epoch [77/120    avg_loss:0.082, val_acc:0.932]
Epoch [78/120    avg_loss:0.081, val_acc:0.933]
Epoch [79/120    avg_loss:0.077, val_acc:0.931]
Epoch [80/120    avg_loss:0.081, val_acc:0.932]
Epoch [81/120    avg_loss:0.083, val_acc:0.932]
Epoch [82/120    avg_loss:0.066, val_acc:0.933]
Epoch [83/120    avg_loss:0.078, val_acc:0.934]
Epoch [84/120    avg_loss:0.073, val_acc:0.935]
Epoch [85/120    avg_loss:0.074, val_acc:0.936]
Epoch [86/120    avg_loss:0.077, val_acc:0.933]
Epoch [87/120    avg_loss:0.069, val_acc:0.934]
Epoch [88/120    avg_loss:0.081, val_acc:0.936]
Epoch [89/120    avg_loss:0.073, val_acc:0.934]
Epoch [90/120    avg_loss:0.075, val_acc:0.936]
Epoch [91/120    avg_loss:0.064, val_acc:0.936]
Epoch [92/120    avg_loss:0.061, val_acc:0.939]
Epoch [93/120    avg_loss:0.060, val_acc:0.939]
Epoch [94/120    avg_loss:0.065, val_acc:0.940]
Epoch [95/120    avg_loss:0.067, val_acc:0.940]
Epoch [96/120    avg_loss:0.077, val_acc:0.935]
Epoch [97/120    avg_loss:0.061, val_acc:0.941]
Epoch [98/120    avg_loss:0.070, val_acc:0.940]
Epoch [99/120    avg_loss:0.067, val_acc:0.944]
Epoch [100/120    avg_loss:0.070, val_acc:0.936]
Epoch [101/120    avg_loss:0.067, val_acc:0.941]
Epoch [102/120    avg_loss:0.062, val_acc:0.942]
Epoch [103/120    avg_loss:0.066, val_acc:0.941]
Epoch [104/120    avg_loss:0.065, val_acc:0.936]
Epoch [105/120    avg_loss:0.063, val_acc:0.936]
Epoch [106/120    avg_loss:0.062, val_acc:0.939]
Epoch [107/120    avg_loss:0.061, val_acc:0.936]
Epoch [108/120    avg_loss:0.059, val_acc:0.942]
Epoch [109/120    avg_loss:0.061, val_acc:0.947]
Epoch [110/120    avg_loss:0.059, val_acc:0.944]
Epoch [111/120    avg_loss:0.060, val_acc:0.945]
Epoch [112/120    avg_loss:0.062, val_acc:0.936]
Epoch [113/120    avg_loss:0.057, val_acc:0.941]
Epoch [114/120    avg_loss:0.056, val_acc:0.942]
Epoch [115/120    avg_loss:0.056, val_acc:0.942]
Epoch [116/120    avg_loss:0.064, val_acc:0.944]
Epoch [117/120    avg_loss:0.063, val_acc:0.941]
Epoch [118/120    avg_loss:0.058, val_acc:0.949]
Epoch [119/120    avg_loss:0.061, val_acc:0.944]
Epoch [120/120    avg_loss:0.052, val_acc:0.947]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    3 1174    5    0    0    9    0    0    0   10   72    1    0
     0   11    0]
 [   0    0    5  672    0   10    0    0    0   13    0    0   43    3
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    8    0    4    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    5    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   13    0    0    3    0
     0    0    0]
 [   0    0   39   90    0    7    0    0    0    0  712   18    0    0
     1    8    0]
 [   0    0   18    0    0    2    6    0    5    0    7 2164    1    5
     2    0    0]
 [   0    0    3   13    9    4    0    0    0    1   18    1  479    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   12    0    0    2    0    4    2    0    0
  1119    0    0]
 [   0    0    1    0    0    0   23    0    0    0    0    0    0    0
   106  217    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
93.17073170731707

F1 scores:
[       nan 0.91358025 0.92990099 0.87900589 0.97931034 0.94618834
 0.96807721 0.86206897 0.99192618 0.53061224 0.87361963 0.96758328
 0.90037594 0.97883598 0.94510135 0.74442539 0.94736842]

Kappa:
0.9220559666524667
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efc13c338d0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.755, val_acc:0.305]
Epoch [2/120    avg_loss:2.452, val_acc:0.416]
Epoch [3/120    avg_loss:2.238, val_acc:0.511]
Epoch [4/120    avg_loss:2.150, val_acc:0.540]
Epoch [5/120    avg_loss:2.034, val_acc:0.532]
Epoch [6/120    avg_loss:1.898, val_acc:0.575]
Epoch [7/120    avg_loss:1.849, val_acc:0.578]
Epoch [8/120    avg_loss:1.720, val_acc:0.595]
Epoch [9/120    avg_loss:1.600, val_acc:0.607]
Epoch [10/120    avg_loss:1.481, val_acc:0.618]
Epoch [11/120    avg_loss:1.385, val_acc:0.655]
Epoch [12/120    avg_loss:1.238, val_acc:0.670]
Epoch [13/120    avg_loss:1.208, val_acc:0.677]
Epoch [14/120    avg_loss:1.130, val_acc:0.665]
Epoch [15/120    avg_loss:1.004, val_acc:0.695]
Epoch [16/120    avg_loss:0.950, val_acc:0.723]
Epoch [17/120    avg_loss:0.886, val_acc:0.694]
Epoch [18/120    avg_loss:0.820, val_acc:0.733]
Epoch [19/120    avg_loss:0.820, val_acc:0.734]
Epoch [20/120    avg_loss:0.742, val_acc:0.740]
Epoch [21/120    avg_loss:0.749, val_acc:0.738]
Epoch [22/120    avg_loss:0.642, val_acc:0.790]
Epoch [23/120    avg_loss:0.579, val_acc:0.784]
Epoch [24/120    avg_loss:0.584, val_acc:0.792]
Epoch [25/120    avg_loss:0.566, val_acc:0.795]
Epoch [26/120    avg_loss:0.489, val_acc:0.808]
Epoch [27/120    avg_loss:0.448, val_acc:0.830]
Epoch [28/120    avg_loss:0.455, val_acc:0.844]
Epoch [29/120    avg_loss:0.462, val_acc:0.780]
Epoch [30/120    avg_loss:0.464, val_acc:0.840]
Epoch [31/120    avg_loss:0.433, val_acc:0.822]
Epoch [32/120    avg_loss:0.384, val_acc:0.865]
Epoch [33/120    avg_loss:0.364, val_acc:0.841]
Epoch [34/120    avg_loss:0.337, val_acc:0.878]
Epoch [35/120    avg_loss:0.337, val_acc:0.872]
Epoch [36/120    avg_loss:0.300, val_acc:0.876]
Epoch [37/120    avg_loss:0.293, val_acc:0.883]
Epoch [38/120    avg_loss:0.326, val_acc:0.897]
Epoch [39/120    avg_loss:0.284, val_acc:0.844]
Epoch [40/120    avg_loss:0.274, val_acc:0.893]
Epoch [41/120    avg_loss:0.225, val_acc:0.906]
Epoch [42/120    avg_loss:0.232, val_acc:0.915]
Epoch [43/120    avg_loss:0.182, val_acc:0.918]
Epoch [44/120    avg_loss:0.204, val_acc:0.926]
Epoch [45/120    avg_loss:0.211, val_acc:0.922]
Epoch [46/120    avg_loss:0.179, val_acc:0.927]
Epoch [47/120    avg_loss:0.179, val_acc:0.906]
Epoch [48/120    avg_loss:0.177, val_acc:0.910]
Epoch [49/120    avg_loss:0.175, val_acc:0.918]
Epoch [50/120    avg_loss:0.171, val_acc:0.917]
Epoch [51/120    avg_loss:0.207, val_acc:0.907]
Epoch [52/120    avg_loss:0.196, val_acc:0.891]
Epoch [53/120    avg_loss:0.215, val_acc:0.909]
Epoch [54/120    avg_loss:0.199, val_acc:0.914]
Epoch [55/120    avg_loss:0.158, val_acc:0.923]
Epoch [56/120    avg_loss:0.139, val_acc:0.936]
Epoch [57/120    avg_loss:0.129, val_acc:0.932]
Epoch [58/120    avg_loss:0.126, val_acc:0.936]
Epoch [59/120    avg_loss:0.140, val_acc:0.939]
Epoch [60/120    avg_loss:0.125, val_acc:0.927]
Epoch [61/120    avg_loss:0.108, val_acc:0.932]
Epoch [62/120    avg_loss:0.103, val_acc:0.936]
Epoch [63/120    avg_loss:0.102, val_acc:0.940]
Epoch [64/120    avg_loss:0.114, val_acc:0.922]
Epoch [65/120    avg_loss:0.138, val_acc:0.933]
Epoch [66/120    avg_loss:0.126, val_acc:0.922]
Epoch [67/120    avg_loss:0.114, val_acc:0.939]
Epoch [68/120    avg_loss:0.082, val_acc:0.945]
Epoch [69/120    avg_loss:0.080, val_acc:0.945]
Epoch [70/120    avg_loss:0.096, val_acc:0.944]
Epoch [71/120    avg_loss:0.076, val_acc:0.948]
Epoch [72/120    avg_loss:0.072, val_acc:0.944]
Epoch [73/120    avg_loss:0.066, val_acc:0.948]
Epoch [74/120    avg_loss:0.061, val_acc:0.953]
Epoch [75/120    avg_loss:0.065, val_acc:0.953]
Epoch [76/120    avg_loss:0.078, val_acc:0.942]
Epoch [77/120    avg_loss:0.100, val_acc:0.931]
Epoch [78/120    avg_loss:0.091, val_acc:0.932]
Epoch [79/120    avg_loss:0.092, val_acc:0.938]
Epoch [80/120    avg_loss:0.065, val_acc:0.952]
Epoch [81/120    avg_loss:0.066, val_acc:0.959]
Epoch [82/120    avg_loss:0.057, val_acc:0.953]
Epoch [83/120    avg_loss:0.064, val_acc:0.938]
Epoch [84/120    avg_loss:0.101, val_acc:0.938]
Epoch [85/120    avg_loss:0.089, val_acc:0.942]
Epoch [86/120    avg_loss:0.081, val_acc:0.935]
Epoch [87/120    avg_loss:0.063, val_acc:0.951]
Epoch [88/120    avg_loss:0.052, val_acc:0.952]
Epoch [89/120    avg_loss:0.045, val_acc:0.945]
Epoch [90/120    avg_loss:0.048, val_acc:0.950]
Epoch [91/120    avg_loss:0.044, val_acc:0.947]
Epoch [92/120    avg_loss:0.033, val_acc:0.950]
Epoch [93/120    avg_loss:0.043, val_acc:0.953]
Epoch [94/120    avg_loss:0.032, val_acc:0.952]
Epoch [95/120    avg_loss:0.033, val_acc:0.955]
Epoch [96/120    avg_loss:0.033, val_acc:0.955]
Epoch [97/120    avg_loss:0.030, val_acc:0.955]
Epoch [98/120    avg_loss:0.032, val_acc:0.955]
Epoch [99/120    avg_loss:0.027, val_acc:0.955]
Epoch [100/120    avg_loss:0.031, val_acc:0.953]
Epoch [101/120    avg_loss:0.033, val_acc:0.952]
Epoch [102/120    avg_loss:0.026, val_acc:0.952]
Epoch [103/120    avg_loss:0.027, val_acc:0.951]
Epoch [104/120    avg_loss:0.028, val_acc:0.950]
Epoch [105/120    avg_loss:0.027, val_acc:0.953]
Epoch [106/120    avg_loss:0.024, val_acc:0.953]
Epoch [107/120    avg_loss:0.028, val_acc:0.953]
Epoch [108/120    avg_loss:0.023, val_acc:0.953]
Epoch [109/120    avg_loss:0.023, val_acc:0.953]
Epoch [110/120    avg_loss:0.028, val_acc:0.953]
Epoch [111/120    avg_loss:0.022, val_acc:0.953]
Epoch [112/120    avg_loss:0.025, val_acc:0.953]
Epoch [113/120    avg_loss:0.026, val_acc:0.953]
Epoch [114/120    avg_loss:0.024, val_acc:0.955]
Epoch [115/120    avg_loss:0.023, val_acc:0.955]
Epoch [116/120    avg_loss:0.022, val_acc:0.955]
Epoch [117/120    avg_loss:0.027, val_acc:0.955]
Epoch [118/120    avg_loss:0.025, val_acc:0.956]
Epoch [119/120    avg_loss:0.026, val_acc:0.956]
Epoch [120/120    avg_loss:0.024, val_acc:0.956]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1212    3    0    0    4    0    0    0   10   56    0    0
     0    0    0]
 [   0    0    0  702    2   15    0    0    0   18    0    0   10    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    1    2    0    5    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   23   90    0    4    0    0    0    0  743    8    0    0
     3    4    0]
 [   0    0   28    0    0    0    5    0    1    0    6 2154    4    6
     6    0    0]
 [   0    0    0   30    9    5    0    0    0    1   14    0  464    0
     0    1   10]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    1    2    0    0
  1129    0    0]
 [   0    0    0    0    0    1    5    0    0    6    0    0    0    0
    36  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.08943089430895

F1 scores:
[       nan 0.96202532 0.95133438 0.89142857 0.97482838 0.95163105
 0.98795181 0.96153846 0.99767981 0.47619048 0.89951574 0.97224103
 0.91699605 0.98404255 0.97453604 0.91858679 0.94382022]

Kappa:
0.9440199868442599
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffa3dd187f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.736, val_acc:0.257]
Epoch [2/120    avg_loss:2.393, val_acc:0.407]
Epoch [3/120    avg_loss:2.178, val_acc:0.464]
Epoch [4/120    avg_loss:2.079, val_acc:0.494]
Epoch [5/120    avg_loss:1.996, val_acc:0.531]
Epoch [6/120    avg_loss:1.850, val_acc:0.566]
Epoch [7/120    avg_loss:1.727, val_acc:0.577]
Epoch [8/120    avg_loss:1.618, val_acc:0.590]
Epoch [9/120    avg_loss:1.433, val_acc:0.631]
Epoch [10/120    avg_loss:1.355, val_acc:0.632]
Epoch [11/120    avg_loss:1.262, val_acc:0.647]
Epoch [12/120    avg_loss:1.163, val_acc:0.675]
Epoch [13/120    avg_loss:1.090, val_acc:0.649]
Epoch [14/120    avg_loss:0.975, val_acc:0.698]
Epoch [15/120    avg_loss:0.866, val_acc:0.711]
Epoch [16/120    avg_loss:0.780, val_acc:0.728]
Epoch [17/120    avg_loss:0.784, val_acc:0.741]
Epoch [18/120    avg_loss:0.815, val_acc:0.726]
Epoch [19/120    avg_loss:0.671, val_acc:0.760]
Epoch [20/120    avg_loss:0.635, val_acc:0.758]
Epoch [21/120    avg_loss:0.566, val_acc:0.798]
Epoch [22/120    avg_loss:0.537, val_acc:0.786]
Epoch [23/120    avg_loss:0.486, val_acc:0.782]
Epoch [24/120    avg_loss:0.484, val_acc:0.812]
Epoch [25/120    avg_loss:0.446, val_acc:0.811]
Epoch [26/120    avg_loss:0.508, val_acc:0.818]
Epoch [27/120    avg_loss:0.416, val_acc:0.812]
Epoch [28/120    avg_loss:0.376, val_acc:0.830]
Epoch [29/120    avg_loss:0.379, val_acc:0.797]
Epoch [30/120    avg_loss:0.423, val_acc:0.850]
Epoch [31/120    avg_loss:0.435, val_acc:0.807]
Epoch [32/120    avg_loss:0.377, val_acc:0.828]
Epoch [33/120    avg_loss:0.347, val_acc:0.865]
Epoch [34/120    avg_loss:0.292, val_acc:0.880]
Epoch [35/120    avg_loss:0.252, val_acc:0.883]
Epoch [36/120    avg_loss:0.276, val_acc:0.855]
Epoch [37/120    avg_loss:0.283, val_acc:0.867]
Epoch [38/120    avg_loss:0.234, val_acc:0.881]
Epoch [39/120    avg_loss:0.238, val_acc:0.875]
Epoch [40/120    avg_loss:0.214, val_acc:0.893]
Epoch [41/120    avg_loss:0.184, val_acc:0.889]
Epoch [42/120    avg_loss:0.200, val_acc:0.884]
Epoch [43/120    avg_loss:0.160, val_acc:0.890]
Epoch [44/120    avg_loss:0.147, val_acc:0.889]
Epoch [45/120    avg_loss:0.180, val_acc:0.906]
Epoch [46/120    avg_loss:0.198, val_acc:0.905]
Epoch [47/120    avg_loss:0.149, val_acc:0.900]
Epoch [48/120    avg_loss:0.173, val_acc:0.882]
Epoch [49/120    avg_loss:0.190, val_acc:0.900]
Epoch [50/120    avg_loss:0.146, val_acc:0.910]
Epoch [51/120    avg_loss:0.147, val_acc:0.916]
Epoch [52/120    avg_loss:0.124, val_acc:0.912]
Epoch [53/120    avg_loss:0.117, val_acc:0.885]
Epoch [54/120    avg_loss:0.124, val_acc:0.911]
Epoch [55/120    avg_loss:0.120, val_acc:0.902]
Epoch [56/120    avg_loss:0.157, val_acc:0.902]
Epoch [57/120    avg_loss:0.129, val_acc:0.906]
Epoch [58/120    avg_loss:0.114, val_acc:0.914]
Epoch [59/120    avg_loss:0.117, val_acc:0.917]
Epoch [60/120    avg_loss:0.105, val_acc:0.915]
Epoch [61/120    avg_loss:0.107, val_acc:0.928]
Epoch [62/120    avg_loss:0.092, val_acc:0.919]
Epoch [63/120    avg_loss:0.086, val_acc:0.915]
Epoch [64/120    avg_loss:0.108, val_acc:0.898]
Epoch [65/120    avg_loss:0.120, val_acc:0.924]
Epoch [66/120    avg_loss:0.120, val_acc:0.891]
Epoch [67/120    avg_loss:0.170, val_acc:0.907]
Epoch [68/120    avg_loss:0.121, val_acc:0.911]
Epoch [69/120    avg_loss:0.114, val_acc:0.900]
Epoch [70/120    avg_loss:0.138, val_acc:0.918]
Epoch [71/120    avg_loss:0.095, val_acc:0.920]
Epoch [72/120    avg_loss:0.073, val_acc:0.923]
Epoch [73/120    avg_loss:0.068, val_acc:0.926]
Epoch [74/120    avg_loss:0.094, val_acc:0.930]
Epoch [75/120    avg_loss:0.116, val_acc:0.931]
Epoch [76/120    avg_loss:0.078, val_acc:0.923]
Epoch [77/120    avg_loss:0.078, val_acc:0.941]
Epoch [78/120    avg_loss:0.064, val_acc:0.945]
Epoch [79/120    avg_loss:0.056, val_acc:0.935]
Epoch [80/120    avg_loss:0.070, val_acc:0.942]
Epoch [81/120    avg_loss:0.066, val_acc:0.931]
Epoch [82/120    avg_loss:0.058, val_acc:0.932]
Epoch [83/120    avg_loss:0.069, val_acc:0.939]
Epoch [84/120    avg_loss:0.074, val_acc:0.939]
Epoch [85/120    avg_loss:0.079, val_acc:0.936]
Epoch [86/120    avg_loss:0.082, val_acc:0.928]
Epoch [87/120    avg_loss:0.071, val_acc:0.939]
Epoch [88/120    avg_loss:0.060, val_acc:0.939]
Epoch [89/120    avg_loss:0.054, val_acc:0.931]
Epoch [90/120    avg_loss:0.061, val_acc:0.942]
Epoch [91/120    avg_loss:0.057, val_acc:0.930]
Epoch [92/120    avg_loss:0.044, val_acc:0.952]
Epoch [93/120    avg_loss:0.036, val_acc:0.955]
Epoch [94/120    avg_loss:0.038, val_acc:0.957]
Epoch [95/120    avg_loss:0.032, val_acc:0.960]
Epoch [96/120    avg_loss:0.035, val_acc:0.955]
Epoch [97/120    avg_loss:0.036, val_acc:0.961]
Epoch [98/120    avg_loss:0.030, val_acc:0.963]
Epoch [99/120    avg_loss:0.028, val_acc:0.967]
Epoch [100/120    avg_loss:0.032, val_acc:0.968]
Epoch [101/120    avg_loss:0.035, val_acc:0.965]
Epoch [102/120    avg_loss:0.035, val_acc:0.968]
Epoch [103/120    avg_loss:0.033, val_acc:0.967]
Epoch [104/120    avg_loss:0.034, val_acc:0.964]
Epoch [105/120    avg_loss:0.028, val_acc:0.965]
Epoch [106/120    avg_loss:0.030, val_acc:0.966]
Epoch [107/120    avg_loss:0.025, val_acc:0.967]
Epoch [108/120    avg_loss:0.024, val_acc:0.960]
Epoch [109/120    avg_loss:0.028, val_acc:0.963]
Epoch [110/120    avg_loss:0.025, val_acc:0.968]
Epoch [111/120    avg_loss:0.023, val_acc:0.967]
Epoch [112/120    avg_loss:0.033, val_acc:0.968]
Epoch [113/120    avg_loss:0.026, val_acc:0.969]
Epoch [114/120    avg_loss:0.028, val_acc:0.969]
Epoch [115/120    avg_loss:0.024, val_acc:0.967]
Epoch [116/120    avg_loss:0.025, val_acc:0.968]
Epoch [117/120    avg_loss:0.028, val_acc:0.967]
Epoch [118/120    avg_loss:0.031, val_acc:0.966]
Epoch [119/120    avg_loss:0.027, val_acc:0.968]
Epoch [120/120    avg_loss:0.023, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    1 1234    0    0    0    5    0    0    0    2   41    0    0
     0    2    0]
 [   0    0    2  680    1   18    0    0    0    4    0    0   39    0
     3    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  419    0    2    0    6    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0   11    0    0    2    0
     0    0    0]
 [   0    0   15   90    0    2    0    0    0    0  762    0    0    0
     0    6    0]
 [   0    0   14    0    0    3   14    0    0    0   18 2156    1    4
     0    0    0]
 [   0    0    0    6    7    3    0    0    0    0   17    5  494    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    1    0    0
  1134    0    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    1    0
   134  202    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.51490514905149

F1 scores:
[       nan 0.93670886 0.96784314 0.89005236 0.98156682 0.95227273
 0.97535474 0.96153846 0.99883856 0.56410256 0.90660321 0.97622821
 0.92250233 0.98930481 0.93796526 0.72531418 0.98823529]

Kappa:
0.9374062270093246
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:50
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbcd4ca87f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 76232==>0.08M
----------Training process----------
Epoch [1/120    avg_loss:2.721, val_acc:0.268]
Epoch [2/120    avg_loss:2.440, val_acc:0.384]
Epoch [3/120    avg_loss:2.281, val_acc:0.403]
Epoch [4/120    avg_loss:2.137, val_acc:0.528]
Epoch [5/120    avg_loss:2.056, val_acc:0.542]
Epoch [6/120    avg_loss:1.894, val_acc:0.577]
Epoch [7/120    avg_loss:1.859, val_acc:0.593]
Epoch [8/120    avg_loss:1.747, val_acc:0.610]
Epoch [9/120    avg_loss:1.636, val_acc:0.633]
Epoch [10/120    avg_loss:1.560, val_acc:0.618]
Epoch [11/120    avg_loss:1.406, val_acc:0.643]
Epoch [12/120    avg_loss:1.335, val_acc:0.649]
Epoch [13/120    avg_loss:1.225, val_acc:0.673]
Epoch [14/120    avg_loss:1.105, val_acc:0.685]
Epoch [15/120    avg_loss:1.067, val_acc:0.701]
Epoch [16/120    avg_loss:1.000, val_acc:0.699]
Epoch [17/120    avg_loss:0.924, val_acc:0.716]
Epoch [18/120    avg_loss:0.819, val_acc:0.733]
Epoch [19/120    avg_loss:0.754, val_acc:0.753]
Epoch [20/120    avg_loss:0.682, val_acc:0.761]
Epoch [21/120    avg_loss:0.651, val_acc:0.772]
Epoch [22/120    avg_loss:0.596, val_acc:0.799]
Epoch [23/120    avg_loss:0.548, val_acc:0.797]
Epoch [24/120    avg_loss:0.528, val_acc:0.816]
Epoch [25/120    avg_loss:0.507, val_acc:0.797]
Epoch [26/120    avg_loss:0.497, val_acc:0.816]
Epoch [27/120    avg_loss:0.459, val_acc:0.827]
Epoch [28/120    avg_loss:0.407, val_acc:0.822]
Epoch [29/120    avg_loss:0.374, val_acc:0.851]
Epoch [30/120    avg_loss:0.391, val_acc:0.806]
Epoch [31/120    avg_loss:0.464, val_acc:0.857]
Epoch [32/120    avg_loss:0.367, val_acc:0.850]
Epoch [33/120    avg_loss:0.445, val_acc:0.835]
Epoch [34/120    avg_loss:0.410, val_acc:0.867]
Epoch [35/120    avg_loss:0.385, val_acc:0.835]
Epoch [36/120    avg_loss:0.406, val_acc:0.834]
Epoch [37/120    avg_loss:0.332, val_acc:0.856]
Epoch [38/120    avg_loss:0.285, val_acc:0.867]
Epoch [39/120    avg_loss:0.246, val_acc:0.889]
Epoch [40/120    avg_loss:0.265, val_acc:0.863]
Epoch [41/120    avg_loss:0.295, val_acc:0.881]
Epoch [42/120    avg_loss:0.302, val_acc:0.861]
Epoch [43/120    avg_loss:0.227, val_acc:0.891]
Epoch [44/120    avg_loss:0.249, val_acc:0.885]
Epoch [45/120    avg_loss:0.232, val_acc:0.869]
Epoch [46/120    avg_loss:0.208, val_acc:0.914]
Epoch [47/120    avg_loss:0.167, val_acc:0.897]
Epoch [48/120    avg_loss:0.168, val_acc:0.909]
Epoch [49/120    avg_loss:0.158, val_acc:0.909]
Epoch [50/120    avg_loss:0.178, val_acc:0.906]
Epoch [51/120    avg_loss:0.146, val_acc:0.920]
Epoch [52/120    avg_loss:0.148, val_acc:0.920]
Epoch [53/120    avg_loss:0.148, val_acc:0.912]
Epoch [54/120    avg_loss:0.138, val_acc:0.923]
Epoch [55/120    avg_loss:0.128, val_acc:0.932]
Epoch [56/120    avg_loss:0.106, val_acc:0.924]
Epoch [57/120    avg_loss:0.135, val_acc:0.901]
Epoch [58/120    avg_loss:0.142, val_acc:0.932]
Epoch [59/120    avg_loss:0.104, val_acc:0.934]
Epoch [60/120    avg_loss:0.152, val_acc:0.922]
Epoch [61/120    avg_loss:0.143, val_acc:0.934]
Epoch [62/120    avg_loss:0.114, val_acc:0.928]
Epoch [63/120    avg_loss:0.085, val_acc:0.943]
Epoch [64/120    avg_loss:0.089, val_acc:0.941]
Epoch [65/120    avg_loss:0.121, val_acc:0.925]
Epoch [66/120    avg_loss:0.121, val_acc:0.930]
Epoch [67/120    avg_loss:0.096, val_acc:0.936]
Epoch [68/120    avg_loss:0.099, val_acc:0.943]
Epoch [69/120    avg_loss:0.087, val_acc:0.932]
Epoch [70/120    avg_loss:0.108, val_acc:0.925]
Epoch [71/120    avg_loss:0.088, val_acc:0.949]
Epoch [72/120    avg_loss:0.069, val_acc:0.941]
Epoch [73/120    avg_loss:0.108, val_acc:0.950]
Epoch [74/120    avg_loss:0.104, val_acc:0.950]
Epoch [75/120    avg_loss:0.098, val_acc:0.940]
Epoch [76/120    avg_loss:0.069, val_acc:0.945]
Epoch [77/120    avg_loss:0.056, val_acc:0.942]
Epoch [78/120    avg_loss:0.053, val_acc:0.958]
Epoch [79/120    avg_loss:0.059, val_acc:0.945]
Epoch [80/120    avg_loss:0.063, val_acc:0.933]
Epoch [81/120    avg_loss:0.048, val_acc:0.961]
Epoch [82/120    avg_loss:0.066, val_acc:0.939]
Epoch [83/120    avg_loss:0.050, val_acc:0.952]
Epoch [84/120    avg_loss:0.049, val_acc:0.950]
Epoch [85/120    avg_loss:0.071, val_acc:0.959]
Epoch [86/120    avg_loss:0.050, val_acc:0.948]
Epoch [87/120    avg_loss:0.052, val_acc:0.953]
Epoch [88/120    avg_loss:0.045, val_acc:0.958]
Epoch [89/120    avg_loss:0.108, val_acc:0.927]
Epoch [90/120    avg_loss:0.099, val_acc:0.936]
Epoch [91/120    avg_loss:0.081, val_acc:0.922]
Epoch [92/120    avg_loss:0.082, val_acc:0.939]
Epoch [93/120    avg_loss:0.052, val_acc:0.949]
Epoch [94/120    avg_loss:0.051, val_acc:0.952]
Epoch [95/120    avg_loss:0.061, val_acc:0.957]
Epoch [96/120    avg_loss:0.036, val_acc:0.961]
Epoch [97/120    avg_loss:0.033, val_acc:0.964]
Epoch [98/120    avg_loss:0.041, val_acc:0.964]
Epoch [99/120    avg_loss:0.032, val_acc:0.961]
Epoch [100/120    avg_loss:0.037, val_acc:0.964]
Epoch [101/120    avg_loss:0.027, val_acc:0.965]
Epoch [102/120    avg_loss:0.031, val_acc:0.966]
Epoch [103/120    avg_loss:0.037, val_acc:0.966]
Epoch [104/120    avg_loss:0.031, val_acc:0.964]
Epoch [105/120    avg_loss:0.030, val_acc:0.965]
Epoch [106/120    avg_loss:0.036, val_acc:0.964]
Epoch [107/120    avg_loss:0.033, val_acc:0.965]
Epoch [108/120    avg_loss:0.029, val_acc:0.968]
Epoch [109/120    avg_loss:0.031, val_acc:0.968]
Epoch [110/120    avg_loss:0.034, val_acc:0.969]
Epoch [111/120    avg_loss:0.032, val_acc:0.969]
Epoch [112/120    avg_loss:0.028, val_acc:0.968]
Epoch [113/120    avg_loss:0.030, val_acc:0.969]
Epoch [114/120    avg_loss:0.025, val_acc:0.966]
Epoch [115/120    avg_loss:0.029, val_acc:0.966]
Epoch [116/120    avg_loss:0.024, val_acc:0.965]
Epoch [117/120    avg_loss:0.028, val_acc:0.969]
Epoch [118/120    avg_loss:0.025, val_acc:0.967]
Epoch [119/120    avg_loss:0.030, val_acc:0.966]
Epoch [120/120    avg_loss:0.032, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1225    3    0    0    4    0    0    1    8   40    0    0
     0    2    2]
 [   0    0    3  713    6    3    0    0    0    6    0    0   10    6
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    7    0    0    0    0    0    0  421    0    0    0    2    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   15    0    0    1    0
     0    0    0]
 [   0    0   23   88    0    4    0    0    0    0  741   17    0    0
     1    1    0]
 [   0    0   10    0    0    0   11    0    3    0    5 2179    1    1
     0    0    0]
 [   0    0    7    5    7    8    0    0    0    0    8    0  494    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    2    0    4    1    0    0
  1124    0    0]
 [   0    0    0    0    0    0    0    0    0    3    0    0    0    0
    77  267    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.42547425474255

F1 scores:
[       nan 0.87058824 0.95965531 0.91527599 0.97038724 0.96730552
 0.98566038 1.         0.98364486 0.6122449  0.90091185 0.97976619
 0.94636015 0.98143236 0.95904437 0.86547812 0.94797688]

Kappa:
0.9478117492396477
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f07bfee6668>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.066, val_acc:0.486]
Epoch [2/120    avg_loss:1.499, val_acc:0.585]
Epoch [3/120    avg_loss:1.261, val_acc:0.583]
Epoch [4/120    avg_loss:1.046, val_acc:0.723]
Epoch [5/120    avg_loss:0.887, val_acc:0.659]
Epoch [6/120    avg_loss:0.730, val_acc:0.802]
Epoch [7/120    avg_loss:0.641, val_acc:0.754]
Epoch [8/120    avg_loss:0.603, val_acc:0.831]
Epoch [9/120    avg_loss:0.452, val_acc:0.845]
Epoch [10/120    avg_loss:0.423, val_acc:0.855]
Epoch [11/120    avg_loss:0.399, val_acc:0.814]
Epoch [12/120    avg_loss:0.339, val_acc:0.875]
Epoch [13/120    avg_loss:0.245, val_acc:0.905]
Epoch [14/120    avg_loss:0.294, val_acc:0.876]
Epoch [15/120    avg_loss:0.317, val_acc:0.855]
Epoch [16/120    avg_loss:0.324, val_acc:0.792]
Epoch [17/120    avg_loss:0.304, val_acc:0.886]
Epoch [18/120    avg_loss:0.210, val_acc:0.879]
Epoch [19/120    avg_loss:0.160, val_acc:0.892]
Epoch [20/120    avg_loss:0.116, val_acc:0.941]
Epoch [21/120    avg_loss:0.097, val_acc:0.941]
Epoch [22/120    avg_loss:0.076, val_acc:0.938]
Epoch [23/120    avg_loss:0.197, val_acc:0.938]
Epoch [24/120    avg_loss:0.114, val_acc:0.942]
Epoch [25/120    avg_loss:0.127, val_acc:0.942]
Epoch [26/120    avg_loss:0.090, val_acc:0.938]
Epoch [27/120    avg_loss:0.092, val_acc:0.938]
Epoch [28/120    avg_loss:0.068, val_acc:0.942]
Epoch [29/120    avg_loss:0.083, val_acc:0.950]
Epoch [30/120    avg_loss:0.054, val_acc:0.960]
Epoch [31/120    avg_loss:0.048, val_acc:0.953]
Epoch [32/120    avg_loss:0.039, val_acc:0.957]
Epoch [33/120    avg_loss:0.039, val_acc:0.958]
Epoch [34/120    avg_loss:0.032, val_acc:0.953]
Epoch [35/120    avg_loss:0.038, val_acc:0.919]
Epoch [36/120    avg_loss:0.057, val_acc:0.957]
Epoch [37/120    avg_loss:0.044, val_acc:0.948]
Epoch [38/120    avg_loss:0.029, val_acc:0.960]
Epoch [39/120    avg_loss:0.023, val_acc:0.963]
Epoch [40/120    avg_loss:0.022, val_acc:0.967]
Epoch [41/120    avg_loss:0.019, val_acc:0.955]
Epoch [42/120    avg_loss:0.036, val_acc:0.955]
Epoch [43/120    avg_loss:0.033, val_acc:0.950]
Epoch [44/120    avg_loss:0.023, val_acc:0.966]
Epoch [45/120    avg_loss:0.012, val_acc:0.958]
Epoch [46/120    avg_loss:0.023, val_acc:0.965]
Epoch [47/120    avg_loss:0.011, val_acc:0.966]
Epoch [48/120    avg_loss:0.009, val_acc:0.968]
Epoch [49/120    avg_loss:0.014, val_acc:0.955]
Epoch [50/120    avg_loss:0.011, val_acc:0.939]
Epoch [51/120    avg_loss:0.046, val_acc:0.964]
Epoch [52/120    avg_loss:0.011, val_acc:0.969]
Epoch [53/120    avg_loss:0.022, val_acc:0.966]
Epoch [54/120    avg_loss:0.022, val_acc:0.934]
Epoch [55/120    avg_loss:0.098, val_acc:0.876]
Epoch [56/120    avg_loss:0.050, val_acc:0.946]
Epoch [57/120    avg_loss:0.027, val_acc:0.961]
Epoch [58/120    avg_loss:0.031, val_acc:0.959]
Epoch [59/120    avg_loss:0.010, val_acc:0.968]
Epoch [60/120    avg_loss:0.019, val_acc:0.968]
Epoch [61/120    avg_loss:0.007, val_acc:0.970]
Epoch [62/120    avg_loss:0.020, val_acc:0.961]
Epoch [63/120    avg_loss:0.009, val_acc:0.967]
Epoch [64/120    avg_loss:0.007, val_acc:0.966]
Epoch [65/120    avg_loss:0.015, val_acc:0.966]
Epoch [66/120    avg_loss:0.016, val_acc:0.969]
Epoch [67/120    avg_loss:0.007, val_acc:0.972]
Epoch [68/120    avg_loss:0.005, val_acc:0.973]
Epoch [69/120    avg_loss:0.006, val_acc:0.969]
Epoch [70/120    avg_loss:0.004, val_acc:0.970]
Epoch [71/120    avg_loss:0.006, val_acc:0.974]
Epoch [72/120    avg_loss:0.004, val_acc:0.975]
Epoch [73/120    avg_loss:0.021, val_acc:0.965]
Epoch [74/120    avg_loss:0.010, val_acc:0.970]
Epoch [75/120    avg_loss:0.016, val_acc:0.965]
Epoch [76/120    avg_loss:0.014, val_acc:0.944]
Epoch [77/120    avg_loss:0.017, val_acc:0.974]
Epoch [78/120    avg_loss:0.016, val_acc:0.968]
Epoch [79/120    avg_loss:0.010, val_acc:0.947]
Epoch [80/120    avg_loss:0.043, val_acc:0.970]
Epoch [81/120    avg_loss:0.023, val_acc:0.960]
Epoch [82/120    avg_loss:0.010, val_acc:0.970]
Epoch [83/120    avg_loss:0.006, val_acc:0.973]
Epoch [84/120    avg_loss:0.005, val_acc:0.971]
Epoch [85/120    avg_loss:0.116, val_acc:0.951]
Epoch [86/120    avg_loss:0.041, val_acc:0.961]
Epoch [87/120    avg_loss:0.024, val_acc:0.963]
Epoch [88/120    avg_loss:0.014, val_acc:0.964]
Epoch [89/120    avg_loss:0.015, val_acc:0.964]
Epoch [90/120    avg_loss:0.013, val_acc:0.968]
Epoch [91/120    avg_loss:0.014, val_acc:0.965]
Epoch [92/120    avg_loss:0.010, val_acc:0.965]
Epoch [93/120    avg_loss:0.011, val_acc:0.966]
Epoch [94/120    avg_loss:0.010, val_acc:0.967]
Epoch [95/120    avg_loss:0.009, val_acc:0.967]
Epoch [96/120    avg_loss:0.006, val_acc:0.967]
Epoch [97/120    avg_loss:0.013, val_acc:0.970]
Epoch [98/120    avg_loss:0.006, val_acc:0.971]
Epoch [99/120    avg_loss:0.005, val_acc:0.969]
Epoch [100/120    avg_loss:0.008, val_acc:0.970]
Epoch [101/120    avg_loss:0.008, val_acc:0.970]
Epoch [102/120    avg_loss:0.005, val_acc:0.970]
Epoch [103/120    avg_loss:0.005, val_acc:0.970]
Epoch [104/120    avg_loss:0.007, val_acc:0.970]
Epoch [105/120    avg_loss:0.006, val_acc:0.970]
Epoch [106/120    avg_loss:0.007, val_acc:0.970]
Epoch [107/120    avg_loss:0.006, val_acc:0.970]
Epoch [108/120    avg_loss:0.005, val_acc:0.970]
Epoch [109/120    avg_loss:0.008, val_acc:0.970]
Epoch [110/120    avg_loss:0.007, val_acc:0.970]
Epoch [111/120    avg_loss:0.006, val_acc:0.970]
Epoch [112/120    avg_loss:0.005, val_acc:0.970]
Epoch [113/120    avg_loss:0.006, val_acc:0.970]
Epoch [114/120    avg_loss:0.005, val_acc:0.970]
Epoch [115/120    avg_loss:0.007, val_acc:0.970]
Epoch [116/120    avg_loss:0.005, val_acc:0.970]
Epoch [117/120    avg_loss:0.007, val_acc:0.970]
Epoch [118/120    avg_loss:0.007, val_acc:0.970]
Epoch [119/120    avg_loss:0.007, val_acc:0.970]
Epoch [120/120    avg_loss:0.006, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1249    4    2    1    0    0    0    0   13   15    1    0
     0    0    0]
 [   0    0    0  729    2    0    0    0    0    2    1    9    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  650    0    0    5    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    0    2    0    0    0  849   12    1    0
     2    1    0]
 [   0    0   10    0    0    0    0    0    0    0    6 2166   16   11
     1    0    0]
 [   0    0    0    3    0    0    0    0    0    0    0    0  526    0
     0    1    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   100  247    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.20325203252033

F1 scores:
[       nan 0.95121951 0.97884013 0.98314228 0.99069767 0.99770115
 0.99312452 1.         0.997669   0.8372093  0.97250859 0.98164514
 0.97137581 0.97112861 0.95021097 0.81116585 0.97076023]

Kappa:
0.9681008132513409
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f10ae023710>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.122, val_acc:0.495]
Epoch [2/120    avg_loss:1.481, val_acc:0.629]
Epoch [3/120    avg_loss:1.217, val_acc:0.657]
Epoch [4/120    avg_loss:0.904, val_acc:0.744]
Epoch [5/120    avg_loss:0.806, val_acc:0.756]
Epoch [6/120    avg_loss:0.720, val_acc:0.774]
Epoch [7/120    avg_loss:0.651, val_acc:0.780]
Epoch [8/120    avg_loss:0.622, val_acc:0.819]
Epoch [9/120    avg_loss:0.584, val_acc:0.787]
Epoch [10/120    avg_loss:0.454, val_acc:0.831]
Epoch [11/120    avg_loss:0.520, val_acc:0.846]
Epoch [12/120    avg_loss:0.359, val_acc:0.853]
Epoch [13/120    avg_loss:0.359, val_acc:0.890]
Epoch [14/120    avg_loss:0.341, val_acc:0.864]
Epoch [15/120    avg_loss:0.307, val_acc:0.853]
Epoch [16/120    avg_loss:0.236, val_acc:0.909]
Epoch [17/120    avg_loss:0.263, val_acc:0.853]
Epoch [18/120    avg_loss:0.186, val_acc:0.910]
Epoch [19/120    avg_loss:0.135, val_acc:0.907]
Epoch [20/120    avg_loss:0.103, val_acc:0.917]
Epoch [21/120    avg_loss:0.251, val_acc:0.881]
Epoch [22/120    avg_loss:0.168, val_acc:0.915]
Epoch [23/120    avg_loss:0.123, val_acc:0.927]
Epoch [24/120    avg_loss:0.091, val_acc:0.919]
Epoch [25/120    avg_loss:0.092, val_acc:0.919]
Epoch [26/120    avg_loss:0.071, val_acc:0.931]
Epoch [27/120    avg_loss:0.216, val_acc:0.930]
Epoch [28/120    avg_loss:0.103, val_acc:0.943]
Epoch [29/120    avg_loss:0.085, val_acc:0.940]
Epoch [30/120    avg_loss:0.066, val_acc:0.956]
Epoch [31/120    avg_loss:0.040, val_acc:0.954]
Epoch [32/120    avg_loss:0.047, val_acc:0.953]
Epoch [33/120    avg_loss:0.046, val_acc:0.959]
Epoch [34/120    avg_loss:0.047, val_acc:0.925]
Epoch [35/120    avg_loss:0.048, val_acc:0.954]
Epoch [36/120    avg_loss:0.034, val_acc:0.954]
Epoch [37/120    avg_loss:0.063, val_acc:0.961]
Epoch [38/120    avg_loss:0.053, val_acc:0.948]
Epoch [39/120    avg_loss:0.025, val_acc:0.958]
Epoch [40/120    avg_loss:0.055, val_acc:0.946]
Epoch [41/120    avg_loss:0.044, val_acc:0.961]
Epoch [42/120    avg_loss:0.039, val_acc:0.959]
Epoch [43/120    avg_loss:0.025, val_acc:0.968]
Epoch [44/120    avg_loss:0.022, val_acc:0.963]
Epoch [45/120    avg_loss:0.023, val_acc:0.965]
Epoch [46/120    avg_loss:0.035, val_acc:0.953]
Epoch [47/120    avg_loss:0.023, val_acc:0.947]
Epoch [48/120    avg_loss:0.033, val_acc:0.961]
Epoch [49/120    avg_loss:0.015, val_acc:0.967]
Epoch [50/120    avg_loss:0.011, val_acc:0.971]
Epoch [51/120    avg_loss:0.017, val_acc:0.970]
Epoch [52/120    avg_loss:0.013, val_acc:0.972]
Epoch [53/120    avg_loss:0.011, val_acc:0.973]
Epoch [54/120    avg_loss:0.040, val_acc:0.950]
Epoch [55/120    avg_loss:0.045, val_acc:0.961]
Epoch [56/120    avg_loss:0.021, val_acc:0.968]
Epoch [57/120    avg_loss:0.063, val_acc:0.949]
Epoch [58/120    avg_loss:0.068, val_acc:0.928]
Epoch [59/120    avg_loss:0.034, val_acc:0.965]
Epoch [60/120    avg_loss:0.030, val_acc:0.958]
Epoch [61/120    avg_loss:0.015, val_acc:0.953]
Epoch [62/120    avg_loss:0.021, val_acc:0.966]
Epoch [63/120    avg_loss:0.009, val_acc:0.966]
Epoch [64/120    avg_loss:0.022, val_acc:0.957]
Epoch [65/120    avg_loss:0.029, val_acc:0.963]
Epoch [66/120    avg_loss:0.028, val_acc:0.951]
Epoch [67/120    avg_loss:0.014, val_acc:0.960]
Epoch [68/120    avg_loss:0.016, val_acc:0.967]
Epoch [69/120    avg_loss:0.010, val_acc:0.970]
Epoch [70/120    avg_loss:0.013, val_acc:0.971]
Epoch [71/120    avg_loss:0.009, val_acc:0.967]
Epoch [72/120    avg_loss:0.007, val_acc:0.968]
Epoch [73/120    avg_loss:0.007, val_acc:0.970]
Epoch [74/120    avg_loss:0.006, val_acc:0.972]
Epoch [75/120    avg_loss:0.007, val_acc:0.971]
Epoch [76/120    avg_loss:0.007, val_acc:0.969]
Epoch [77/120    avg_loss:0.006, val_acc:0.968]
Epoch [78/120    avg_loss:0.004, val_acc:0.969]
Epoch [79/120    avg_loss:0.006, val_acc:0.969]
Epoch [80/120    avg_loss:0.005, val_acc:0.969]
Epoch [81/120    avg_loss:0.006, val_acc:0.969]
Epoch [82/120    avg_loss:0.006, val_acc:0.969]
Epoch [83/120    avg_loss:0.005, val_acc:0.969]
Epoch [84/120    avg_loss:0.006, val_acc:0.969]
Epoch [85/120    avg_loss:0.005, val_acc:0.970]
Epoch [86/120    avg_loss:0.005, val_acc:0.970]
Epoch [87/120    avg_loss:0.006, val_acc:0.970]
Epoch [88/120    avg_loss:0.005, val_acc:0.972]
Epoch [89/120    avg_loss:0.005, val_acc:0.972]
Epoch [90/120    avg_loss:0.005, val_acc:0.972]
Epoch [91/120    avg_loss:0.006, val_acc:0.973]
Epoch [92/120    avg_loss:0.005, val_acc:0.973]
Epoch [93/120    avg_loss:0.006, val_acc:0.973]
Epoch [94/120    avg_loss:0.006, val_acc:0.973]
Epoch [95/120    avg_loss:0.005, val_acc:0.973]
Epoch [96/120    avg_loss:0.006, val_acc:0.973]
Epoch [97/120    avg_loss:0.005, val_acc:0.973]
Epoch [98/120    avg_loss:0.004, val_acc:0.973]
Epoch [99/120    avg_loss:0.003, val_acc:0.973]
Epoch [100/120    avg_loss:0.005, val_acc:0.973]
Epoch [101/120    avg_loss:0.004, val_acc:0.973]
Epoch [102/120    avg_loss:0.004, val_acc:0.973]
Epoch [103/120    avg_loss:0.004, val_acc:0.973]
Epoch [104/120    avg_loss:0.014, val_acc:0.972]
Epoch [105/120    avg_loss:0.006, val_acc:0.972]
Epoch [106/120    avg_loss:0.005, val_acc:0.972]
Epoch [107/120    avg_loss:0.004, val_acc:0.972]
Epoch [108/120    avg_loss:0.006, val_acc:0.971]
Epoch [109/120    avg_loss:0.005, val_acc:0.971]
Epoch [110/120    avg_loss:0.005, val_acc:0.971]
Epoch [111/120    avg_loss:0.005, val_acc:0.972]
Epoch [112/120    avg_loss:0.011, val_acc:0.972]
Epoch [113/120    avg_loss:0.005, val_acc:0.972]
Epoch [114/120    avg_loss:0.005, val_acc:0.972]
Epoch [115/120    avg_loss:0.004, val_acc:0.972]
Epoch [116/120    avg_loss:0.004, val_acc:0.972]
Epoch [117/120    avg_loss:0.005, val_acc:0.972]
Epoch [118/120    avg_loss:0.006, val_acc:0.972]
Epoch [119/120    avg_loss:0.006, val_acc:0.972]
Epoch [120/120    avg_loss:0.006, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1259    3    1    0    0    0    0    0    7   10    5    0
     0    0    0]
 [   0    0    0  733    0    0    0    0    0    4    0    1    8    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    0  840   25    2    0
     0    1    0]
 [   0    0   22    0    0    0    0    0    0    1    1 2143   43    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0    0    0    0  528    0
     0    1    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    2    0    0    0    0    0    0
  1119   18    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    81  263    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.21409214092141

F1 scores:
[       nan 1.         0.97862417 0.98521505 0.99765808 0.99769585
 0.99543379 0.96153846 1.         0.87804878 0.97504353 0.97608745
 0.94285714 0.99730458 0.95559351 0.83492063 1.        ]

Kappa:
0.9682424682077078
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:12:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5d23aca668>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.059, val_acc:0.557]
Epoch [2/120    avg_loss:1.512, val_acc:0.629]
Epoch [3/120    avg_loss:1.163, val_acc:0.631]
Epoch [4/120    avg_loss:0.930, val_acc:0.741]
Epoch [5/120    avg_loss:0.842, val_acc:0.773]
Epoch [6/120    avg_loss:0.750, val_acc:0.794]
Epoch [7/120    avg_loss:0.667, val_acc:0.828]
Epoch [8/120    avg_loss:0.624, val_acc:0.796]
Epoch [9/120    avg_loss:0.525, val_acc:0.808]
Epoch [10/120    avg_loss:0.501, val_acc:0.835]
Epoch [11/120    avg_loss:0.433, val_acc:0.889]
Epoch [12/120    avg_loss:0.272, val_acc:0.839]
Epoch [13/120    avg_loss:0.328, val_acc:0.870]
Epoch [14/120    avg_loss:0.320, val_acc:0.861]
Epoch [15/120    avg_loss:0.222, val_acc:0.890]
Epoch [16/120    avg_loss:0.220, val_acc:0.884]
Epoch [17/120    avg_loss:0.246, val_acc:0.872]
Epoch [18/120    avg_loss:0.182, val_acc:0.856]
Epoch [19/120    avg_loss:0.288, val_acc:0.848]
Epoch [20/120    avg_loss:0.208, val_acc:0.933]
Epoch [21/120    avg_loss:0.179, val_acc:0.889]
Epoch [22/120    avg_loss:0.153, val_acc:0.920]
Epoch [23/120    avg_loss:0.139, val_acc:0.934]
Epoch [24/120    avg_loss:0.106, val_acc:0.944]
Epoch [25/120    avg_loss:0.118, val_acc:0.940]
Epoch [26/120    avg_loss:0.115, val_acc:0.929]
Epoch [27/120    avg_loss:0.072, val_acc:0.958]
Epoch [28/120    avg_loss:0.074, val_acc:0.965]
Epoch [29/120    avg_loss:0.051, val_acc:0.957]
Epoch [30/120    avg_loss:0.066, val_acc:0.948]
Epoch [31/120    avg_loss:0.056, val_acc:0.972]
Epoch [32/120    avg_loss:0.045, val_acc:0.955]
Epoch [33/120    avg_loss:0.053, val_acc:0.958]
Epoch [34/120    avg_loss:0.125, val_acc:0.939]
Epoch [35/120    avg_loss:0.162, val_acc:0.946]
Epoch [36/120    avg_loss:0.185, val_acc:0.919]
Epoch [37/120    avg_loss:0.087, val_acc:0.934]
Epoch [38/120    avg_loss:0.096, val_acc:0.943]
Epoch [39/120    avg_loss:0.045, val_acc:0.956]
Epoch [40/120    avg_loss:0.034, val_acc:0.957]
Epoch [41/120    avg_loss:0.029, val_acc:0.964]
Epoch [42/120    avg_loss:0.032, val_acc:0.972]
Epoch [43/120    avg_loss:0.148, val_acc:0.932]
Epoch [44/120    avg_loss:0.099, val_acc:0.941]
Epoch [45/120    avg_loss:0.032, val_acc:0.960]
Epoch [46/120    avg_loss:0.034, val_acc:0.963]
Epoch [47/120    avg_loss:0.043, val_acc:0.972]
Epoch [48/120    avg_loss:0.025, val_acc:0.970]
Epoch [49/120    avg_loss:0.022, val_acc:0.959]
Epoch [50/120    avg_loss:0.041, val_acc:0.956]
Epoch [51/120    avg_loss:0.046, val_acc:0.948]
Epoch [52/120    avg_loss:0.027, val_acc:0.971]
Epoch [53/120    avg_loss:0.012, val_acc:0.976]
Epoch [54/120    avg_loss:0.013, val_acc:0.972]
Epoch [55/120    avg_loss:0.012, val_acc:0.968]
Epoch [56/120    avg_loss:0.009, val_acc:0.975]
Epoch [57/120    avg_loss:0.015, val_acc:0.974]
Epoch [58/120    avg_loss:0.012, val_acc:0.975]
Epoch [59/120    avg_loss:0.011, val_acc:0.975]
Epoch [60/120    avg_loss:0.007, val_acc:0.971]
Epoch [61/120    avg_loss:0.005, val_acc:0.980]
Epoch [62/120    avg_loss:0.006, val_acc:0.977]
Epoch [63/120    avg_loss:0.005, val_acc:0.980]
Epoch [64/120    avg_loss:0.006, val_acc:0.982]
Epoch [65/120    avg_loss:0.009, val_acc:0.971]
Epoch [66/120    avg_loss:0.007, val_acc:0.976]
Epoch [67/120    avg_loss:0.011, val_acc:0.982]
Epoch [68/120    avg_loss:0.007, val_acc:0.982]
Epoch [69/120    avg_loss:0.008, val_acc:0.971]
Epoch [70/120    avg_loss:0.021, val_acc:0.972]
Epoch [71/120    avg_loss:0.013, val_acc:0.972]
Epoch [72/120    avg_loss:0.017, val_acc:0.973]
Epoch [73/120    avg_loss:0.009, val_acc:0.976]
Epoch [74/120    avg_loss:0.004, val_acc:0.981]
Epoch [75/120    avg_loss:0.006, val_acc:0.977]
Epoch [76/120    avg_loss:0.004, val_acc:0.979]
Epoch [77/120    avg_loss:0.007, val_acc:0.975]
Epoch [78/120    avg_loss:0.007, val_acc:0.968]
Epoch [79/120    avg_loss:0.015, val_acc:0.954]
Epoch [80/120    avg_loss:0.009, val_acc:0.965]
Epoch [81/120    avg_loss:0.017, val_acc:0.974]
Epoch [82/120    avg_loss:0.009, val_acc:0.981]
Epoch [83/120    avg_loss:0.006, val_acc:0.979]
Epoch [84/120    avg_loss:0.004, val_acc:0.980]
Epoch [85/120    avg_loss:0.005, val_acc:0.980]
Epoch [86/120    avg_loss:0.006, val_acc:0.979]
Epoch [87/120    avg_loss:0.005, val_acc:0.980]
Epoch [88/120    avg_loss:0.006, val_acc:0.982]
Epoch [89/120    avg_loss:0.004, val_acc:0.982]
Epoch [90/120    avg_loss:0.006, val_acc:0.980]
Epoch [91/120    avg_loss:0.005, val_acc:0.980]
Epoch [92/120    avg_loss:0.004, val_acc:0.980]
Epoch [93/120    avg_loss:0.003, val_acc:0.980]
Epoch [94/120    avg_loss:0.006, val_acc:0.981]
Epoch [95/120    avg_loss:0.005, val_acc:0.979]
Epoch [96/120    avg_loss:0.005, val_acc:0.980]
Epoch [97/120    avg_loss:0.005, val_acc:0.981]
Epoch [98/120    avg_loss:0.003, val_acc:0.982]
Epoch [99/120    avg_loss:0.004, val_acc:0.981]
Epoch [100/120    avg_loss:0.003, val_acc:0.981]
Epoch [101/120    avg_loss:0.003, val_acc:0.981]
Epoch [102/120    avg_loss:0.003, val_acc:0.981]
Epoch [103/120    avg_loss:0.004, val_acc:0.982]
Epoch [104/120    avg_loss:0.005, val_acc:0.980]
Epoch [105/120    avg_loss:0.003, val_acc:0.981]
Epoch [106/120    avg_loss:0.003, val_acc:0.982]
Epoch [107/120    avg_loss:0.002, val_acc:0.982]
Epoch [108/120    avg_loss:0.003, val_acc:0.982]
Epoch [109/120    avg_loss:0.004, val_acc:0.981]
Epoch [110/120    avg_loss:0.003, val_acc:0.980]
Epoch [111/120    avg_loss:0.005, val_acc:0.979]
Epoch [112/120    avg_loss:0.004, val_acc:0.979]
Epoch [113/120    avg_loss:0.012, val_acc:0.980]
Epoch [114/120    avg_loss:0.003, val_acc:0.978]
Epoch [115/120    avg_loss:0.003, val_acc:0.978]
Epoch [116/120    avg_loss:0.002, val_acc:0.979]
Epoch [117/120    avg_loss:0.003, val_acc:0.978]
Epoch [118/120    avg_loss:0.003, val_acc:0.980]
Epoch [119/120    avg_loss:0.006, val_acc:0.980]
Epoch [120/120    avg_loss:0.004, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1265    5    0    0    0    0    0    0    4    8    3    0
     0    0    0]
 [   0    0    0  721    0    0    0    0    0    1    0    2   23    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    1  432    1    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    1  655    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    0  851   16    0    0
     0    1    0]
 [   0    0   17    0    0    0    0    0    0    0   16 2169    6    1
     0    1    0]
 [   0    0    1    2    0    0    0    0    0    0    2    0  528    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    80  261    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.57181571815718

F1 scores:
[       nan 1.         0.98252427 0.97762712 0.99765808 0.99539171
 0.99317665 0.98039216 1.         0.97297297 0.97368421 0.9845665
 0.96438356 0.99730458 0.95945369 0.8352     0.98809524]

Kappa:
0.9723065113819247
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f10af1066d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.055, val_acc:0.562]
Epoch [2/120    avg_loss:1.414, val_acc:0.568]
Epoch [3/120    avg_loss:1.094, val_acc:0.710]
Epoch [4/120    avg_loss:0.994, val_acc:0.702]
Epoch [5/120    avg_loss:0.841, val_acc:0.754]
Epoch [6/120    avg_loss:0.759, val_acc:0.757]
Epoch [7/120    avg_loss:0.664, val_acc:0.835]
Epoch [8/120    avg_loss:0.559, val_acc:0.779]
Epoch [9/120    avg_loss:0.515, val_acc:0.823]
Epoch [10/120    avg_loss:0.431, val_acc:0.850]
Epoch [11/120    avg_loss:0.440, val_acc:0.892]
Epoch [12/120    avg_loss:0.363, val_acc:0.860]
Epoch [13/120    avg_loss:0.293, val_acc:0.923]
Epoch [14/120    avg_loss:0.264, val_acc:0.802]
Epoch [15/120    avg_loss:0.322, val_acc:0.878]
Epoch [16/120    avg_loss:0.195, val_acc:0.917]
Epoch [17/120    avg_loss:0.188, val_acc:0.920]
Epoch [18/120    avg_loss:0.168, val_acc:0.928]
Epoch [19/120    avg_loss:0.246, val_acc:0.911]
Epoch [20/120    avg_loss:0.196, val_acc:0.919]
Epoch [21/120    avg_loss:0.167, val_acc:0.940]
Epoch [22/120    avg_loss:0.133, val_acc:0.949]
Epoch [23/120    avg_loss:0.224, val_acc:0.927]
Epoch [24/120    avg_loss:0.133, val_acc:0.926]
Epoch [25/120    avg_loss:0.106, val_acc:0.949]
Epoch [26/120    avg_loss:0.081, val_acc:0.957]
Epoch [27/120    avg_loss:0.073, val_acc:0.965]
Epoch [28/120    avg_loss:0.086, val_acc:0.961]
Epoch [29/120    avg_loss:0.108, val_acc:0.925]
Epoch [30/120    avg_loss:0.091, val_acc:0.943]
Epoch [31/120    avg_loss:0.116, val_acc:0.940]
Epoch [32/120    avg_loss:0.086, val_acc:0.926]
Epoch [33/120    avg_loss:0.091, val_acc:0.968]
Epoch [34/120    avg_loss:0.047, val_acc:0.904]
Epoch [35/120    avg_loss:0.054, val_acc:0.963]
Epoch [36/120    avg_loss:0.041, val_acc:0.971]
Epoch [37/120    avg_loss:0.027, val_acc:0.969]
Epoch [38/120    avg_loss:0.039, val_acc:0.972]
Epoch [39/120    avg_loss:0.031, val_acc:0.969]
Epoch [40/120    avg_loss:0.037, val_acc:0.974]
Epoch [41/120    avg_loss:0.025, val_acc:0.967]
Epoch [42/120    avg_loss:0.039, val_acc:0.949]
Epoch [43/120    avg_loss:0.051, val_acc:0.953]
Epoch [44/120    avg_loss:0.046, val_acc:0.972]
Epoch [45/120    avg_loss:0.082, val_acc:0.947]
Epoch [46/120    avg_loss:0.070, val_acc:0.960]
Epoch [47/120    avg_loss:0.026, val_acc:0.963]
Epoch [48/120    avg_loss:0.035, val_acc:0.967]
Epoch [49/120    avg_loss:0.061, val_acc:0.969]
Epoch [50/120    avg_loss:0.020, val_acc:0.979]
Epoch [51/120    avg_loss:0.034, val_acc:0.967]
Epoch [52/120    avg_loss:0.028, val_acc:0.971]
Epoch [53/120    avg_loss:0.019, val_acc:0.958]
Epoch [54/120    avg_loss:0.016, val_acc:0.963]
Epoch [55/120    avg_loss:0.038, val_acc:0.971]
Epoch [56/120    avg_loss:0.025, val_acc:0.963]
Epoch [57/120    avg_loss:0.011, val_acc:0.972]
Epoch [58/120    avg_loss:0.026, val_acc:0.969]
Epoch [59/120    avg_loss:0.047, val_acc:0.963]
Epoch [60/120    avg_loss:0.037, val_acc:0.967]
Epoch [61/120    avg_loss:0.049, val_acc:0.948]
Epoch [62/120    avg_loss:0.124, val_acc:0.966]
Epoch [63/120    avg_loss:0.056, val_acc:0.950]
Epoch [64/120    avg_loss:0.027, val_acc:0.966]
Epoch [65/120    avg_loss:0.020, val_acc:0.972]
Epoch [66/120    avg_loss:0.014, val_acc:0.974]
Epoch [67/120    avg_loss:0.021, val_acc:0.974]
Epoch [68/120    avg_loss:0.012, val_acc:0.976]
Epoch [69/120    avg_loss:0.010, val_acc:0.976]
Epoch [70/120    avg_loss:0.011, val_acc:0.977]
Epoch [71/120    avg_loss:0.007, val_acc:0.977]
Epoch [72/120    avg_loss:0.009, val_acc:0.977]
Epoch [73/120    avg_loss:0.010, val_acc:0.977]
Epoch [74/120    avg_loss:0.009, val_acc:0.978]
Epoch [75/120    avg_loss:0.009, val_acc:0.977]
Epoch [76/120    avg_loss:0.008, val_acc:0.977]
Epoch [77/120    avg_loss:0.012, val_acc:0.977]
Epoch [78/120    avg_loss:0.008, val_acc:0.977]
Epoch [79/120    avg_loss:0.008, val_acc:0.977]
Epoch [80/120    avg_loss:0.008, val_acc:0.977]
Epoch [81/120    avg_loss:0.007, val_acc:0.977]
Epoch [82/120    avg_loss:0.010, val_acc:0.977]
Epoch [83/120    avg_loss:0.007, val_acc:0.977]
Epoch [84/120    avg_loss:0.008, val_acc:0.977]
Epoch [85/120    avg_loss:0.008, val_acc:0.977]
Epoch [86/120    avg_loss:0.008, val_acc:0.977]
Epoch [87/120    avg_loss:0.008, val_acc:0.977]
Epoch [88/120    avg_loss:0.007, val_acc:0.977]
Epoch [89/120    avg_loss:0.009, val_acc:0.977]
Epoch [90/120    avg_loss:0.007, val_acc:0.977]
Epoch [91/120    avg_loss:0.007, val_acc:0.977]
Epoch [92/120    avg_loss:0.006, val_acc:0.977]
Epoch [93/120    avg_loss:0.010, val_acc:0.977]
Epoch [94/120    avg_loss:0.008, val_acc:0.977]
Epoch [95/120    avg_loss:0.007, val_acc:0.977]
Epoch [96/120    avg_loss:0.008, val_acc:0.977]
Epoch [97/120    avg_loss:0.009, val_acc:0.977]
Epoch [98/120    avg_loss:0.007, val_acc:0.977]
Epoch [99/120    avg_loss:0.010, val_acc:0.977]
Epoch [100/120    avg_loss:0.006, val_acc:0.977]
Epoch [101/120    avg_loss:0.007, val_acc:0.977]
Epoch [102/120    avg_loss:0.008, val_acc:0.977]
Epoch [103/120    avg_loss:0.008, val_acc:0.977]
Epoch [104/120    avg_loss:0.008, val_acc:0.977]
Epoch [105/120    avg_loss:0.009, val_acc:0.977]
Epoch [106/120    avg_loss:0.008, val_acc:0.977]
Epoch [107/120    avg_loss:0.008, val_acc:0.977]
Epoch [108/120    avg_loss:0.007, val_acc:0.977]
Epoch [109/120    avg_loss:0.007, val_acc:0.977]
Epoch [110/120    avg_loss:0.007, val_acc:0.977]
Epoch [111/120    avg_loss:0.006, val_acc:0.977]
Epoch [112/120    avg_loss:0.008, val_acc:0.977]
Epoch [113/120    avg_loss:0.010, val_acc:0.977]
Epoch [114/120    avg_loss:0.008, val_acc:0.977]
Epoch [115/120    avg_loss:0.007, val_acc:0.977]
Epoch [116/120    avg_loss:0.008, val_acc:0.977]
Epoch [117/120    avg_loss:0.014, val_acc:0.977]
Epoch [118/120    avg_loss:0.008, val_acc:0.977]
Epoch [119/120    avg_loss:0.007, val_acc:0.977]
Epoch [120/120    avg_loss:0.009, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    4    4    0    0    0    0    0   10    5    0    2
     0    0    0]
 [   0    0    0  717    1    0    0    0    0    1    0    6   22    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    4    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    3  653    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    1    0    0    0    0    0    0  857   13    0    0
     1    1    0]
 [   0    0   10    0    0    0    0    0    0    0   20 2174    5    0
     0    1    0]
 [   0    0    2    1    1    0    0    0    0    0    0    5  519    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1125   14    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    75  266    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.51761517615176

F1 scores:
[       nan 0.98765432 0.984375   0.97484704 0.9837587  0.99194476
 0.99240122 0.92592593 1.         0.97297297 0.97275823 0.98527079
 0.96111111 0.99462366 0.96112772 0.84578696 0.96551724]

Kappa:
0.9716896446649542
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6358d6c6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.065, val_acc:0.514]
Epoch [2/120    avg_loss:1.499, val_acc:0.627]
Epoch [3/120    avg_loss:1.241, val_acc:0.670]
Epoch [4/120    avg_loss:1.012, val_acc:0.707]
Epoch [5/120    avg_loss:0.834, val_acc:0.764]
Epoch [6/120    avg_loss:0.751, val_acc:0.745]
Epoch [7/120    avg_loss:0.710, val_acc:0.761]
Epoch [8/120    avg_loss:0.575, val_acc:0.820]
Epoch [9/120    avg_loss:0.554, val_acc:0.869]
Epoch [10/120    avg_loss:0.406, val_acc:0.882]
Epoch [11/120    avg_loss:0.448, val_acc:0.854]
Epoch [12/120    avg_loss:0.370, val_acc:0.887]
Epoch [13/120    avg_loss:0.342, val_acc:0.873]
Epoch [14/120    avg_loss:0.368, val_acc:0.868]
Epoch [15/120    avg_loss:0.297, val_acc:0.887]
Epoch [16/120    avg_loss:0.211, val_acc:0.924]
Epoch [17/120    avg_loss:0.215, val_acc:0.872]
Epoch [18/120    avg_loss:0.358, val_acc:0.916]
Epoch [19/120    avg_loss:0.189, val_acc:0.894]
Epoch [20/120    avg_loss:0.186, val_acc:0.886]
Epoch [21/120    avg_loss:0.169, val_acc:0.924]
Epoch [22/120    avg_loss:0.131, val_acc:0.870]
Epoch [23/120    avg_loss:0.263, val_acc:0.914]
Epoch [24/120    avg_loss:0.235, val_acc:0.915]
Epoch [25/120    avg_loss:0.122, val_acc:0.924]
Epoch [26/120    avg_loss:0.094, val_acc:0.953]
Epoch [27/120    avg_loss:0.089, val_acc:0.936]
Epoch [28/120    avg_loss:0.080, val_acc:0.933]
Epoch [29/120    avg_loss:0.089, val_acc:0.954]
Epoch [30/120    avg_loss:0.104, val_acc:0.914]
Epoch [31/120    avg_loss:0.094, val_acc:0.948]
Epoch [32/120    avg_loss:0.076, val_acc:0.950]
Epoch [33/120    avg_loss:0.074, val_acc:0.953]
Epoch [34/120    avg_loss:0.063, val_acc:0.954]
Epoch [35/120    avg_loss:0.092, val_acc:0.920]
Epoch [36/120    avg_loss:0.099, val_acc:0.949]
Epoch [37/120    avg_loss:0.052, val_acc:0.957]
Epoch [38/120    avg_loss:0.058, val_acc:0.969]
Epoch [39/120    avg_loss:0.028, val_acc:0.965]
Epoch [40/120    avg_loss:0.041, val_acc:0.957]
Epoch [41/120    avg_loss:0.031, val_acc:0.966]
Epoch [42/120    avg_loss:0.042, val_acc:0.942]
Epoch [43/120    avg_loss:0.035, val_acc:0.959]
Epoch [44/120    avg_loss:0.038, val_acc:0.944]
Epoch [45/120    avg_loss:0.047, val_acc:0.952]
Epoch [46/120    avg_loss:0.035, val_acc:0.957]
Epoch [47/120    avg_loss:0.039, val_acc:0.947]
Epoch [48/120    avg_loss:0.025, val_acc:0.967]
Epoch [49/120    avg_loss:0.021, val_acc:0.963]
Epoch [50/120    avg_loss:0.023, val_acc:0.954]
Epoch [51/120    avg_loss:0.016, val_acc:0.971]
Epoch [52/120    avg_loss:0.014, val_acc:0.973]
Epoch [53/120    avg_loss:0.068, val_acc:0.966]
Epoch [54/120    avg_loss:0.023, val_acc:0.972]
Epoch [55/120    avg_loss:0.018, val_acc:0.973]
Epoch [56/120    avg_loss:0.016, val_acc:0.973]
Epoch [57/120    avg_loss:0.021, val_acc:0.978]
Epoch [58/120    avg_loss:0.035, val_acc:0.971]
Epoch [59/120    avg_loss:0.025, val_acc:0.964]
Epoch [60/120    avg_loss:0.102, val_acc:0.952]
Epoch [61/120    avg_loss:0.068, val_acc:0.876]
Epoch [62/120    avg_loss:0.050, val_acc:0.969]
Epoch [63/120    avg_loss:0.019, val_acc:0.969]
Epoch [64/120    avg_loss:0.023, val_acc:0.969]
Epoch [65/120    avg_loss:0.023, val_acc:0.971]
Epoch [66/120    avg_loss:0.022, val_acc:0.970]
Epoch [67/120    avg_loss:0.011, val_acc:0.977]
Epoch [68/120    avg_loss:0.010, val_acc:0.971]
Epoch [69/120    avg_loss:0.015, val_acc:0.966]
Epoch [70/120    avg_loss:0.013, val_acc:0.969]
Epoch [71/120    avg_loss:0.013, val_acc:0.977]
Epoch [72/120    avg_loss:0.013, val_acc:0.976]
Epoch [73/120    avg_loss:0.006, val_acc:0.977]
Epoch [74/120    avg_loss:0.007, val_acc:0.978]
Epoch [75/120    avg_loss:0.006, val_acc:0.978]
Epoch [76/120    avg_loss:0.006, val_acc:0.978]
Epoch [77/120    avg_loss:0.004, val_acc:0.977]
Epoch [78/120    avg_loss:0.006, val_acc:0.978]
Epoch [79/120    avg_loss:0.004, val_acc:0.977]
Epoch [80/120    avg_loss:0.005, val_acc:0.978]
Epoch [81/120    avg_loss:0.004, val_acc:0.977]
Epoch [82/120    avg_loss:0.005, val_acc:0.978]
Epoch [83/120    avg_loss:0.005, val_acc:0.978]
Epoch [84/120    avg_loss:0.004, val_acc:0.977]
Epoch [85/120    avg_loss:0.005, val_acc:0.977]
Epoch [86/120    avg_loss:0.005, val_acc:0.979]
Epoch [87/120    avg_loss:0.004, val_acc:0.979]
Epoch [88/120    avg_loss:0.007, val_acc:0.980]
Epoch [89/120    avg_loss:0.004, val_acc:0.979]
Epoch [90/120    avg_loss:0.005, val_acc:0.978]
Epoch [91/120    avg_loss:0.004, val_acc:0.978]
Epoch [92/120    avg_loss:0.004, val_acc:0.977]
Epoch [93/120    avg_loss:0.004, val_acc:0.980]
Epoch [94/120    avg_loss:0.004, val_acc:0.979]
Epoch [95/120    avg_loss:0.005, val_acc:0.977]
Epoch [96/120    avg_loss:0.003, val_acc:0.977]
Epoch [97/120    avg_loss:0.004, val_acc:0.977]
Epoch [98/120    avg_loss:0.004, val_acc:0.976]
Epoch [99/120    avg_loss:0.005, val_acc:0.976]
Epoch [100/120    avg_loss:0.004, val_acc:0.977]
Epoch [101/120    avg_loss:0.004, val_acc:0.977]
Epoch [102/120    avg_loss:0.006, val_acc:0.978]
Epoch [103/120    avg_loss:0.004, val_acc:0.978]
Epoch [104/120    avg_loss:0.004, val_acc:0.978]
Epoch [105/120    avg_loss:0.004, val_acc:0.979]
Epoch [106/120    avg_loss:0.007, val_acc:0.978]
Epoch [107/120    avg_loss:0.005, val_acc:0.978]
Epoch [108/120    avg_loss:0.004, val_acc:0.978]
Epoch [109/120    avg_loss:0.003, val_acc:0.978]
Epoch [110/120    avg_loss:0.003, val_acc:0.978]
Epoch [111/120    avg_loss:0.004, val_acc:0.978]
Epoch [112/120    avg_loss:0.003, val_acc:0.978]
Epoch [113/120    avg_loss:0.004, val_acc:0.978]
Epoch [114/120    avg_loss:0.003, val_acc:0.978]
Epoch [115/120    avg_loss:0.004, val_acc:0.978]
Epoch [116/120    avg_loss:0.003, val_acc:0.978]
Epoch [117/120    avg_loss:0.005, val_acc:0.978]
Epoch [118/120    avg_loss:0.004, val_acc:0.978]
Epoch [119/120    avg_loss:0.004, val_acc:0.978]
Epoch [120/120    avg_loss:0.003, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1266    7    1    0    0    0    0    0    2    9    0    0
     0    0    0]
 [   0    0    0  727    2    0    0    0    0    1    1    6   10    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    1    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    8    0    0    0    0    0    0  854   13    0    0
     0    0    0]
 [   0    0    9    0    0    0    1    0    0    1    1 2181   14    3
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0    3  525    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1125   14    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    80  259    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.72357723577235

F1 scores:
[       nan 0.975      0.9890625  0.9738781  0.9882904  0.99653979
 0.99092284 1.         1.         0.9        0.98443804 0.98620846
 0.96863469 0.9919571  0.95948827 0.83413849 0.97647059]

Kappa:
0.9740312976985142
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9f6bd3d748>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.072, val_acc:0.592]
Epoch [2/120    avg_loss:1.482, val_acc:0.646]
Epoch [3/120    avg_loss:1.188, val_acc:0.711]
Epoch [4/120    avg_loss:0.935, val_acc:0.663]
Epoch [5/120    avg_loss:0.861, val_acc:0.776]
Epoch [6/120    avg_loss:0.724, val_acc:0.738]
Epoch [7/120    avg_loss:0.685, val_acc:0.718]
Epoch [8/120    avg_loss:0.620, val_acc:0.775]
Epoch [9/120    avg_loss:0.632, val_acc:0.830]
Epoch [10/120    avg_loss:0.467, val_acc:0.822]
Epoch [11/120    avg_loss:0.458, val_acc:0.861]
Epoch [12/120    avg_loss:0.385, val_acc:0.818]
Epoch [13/120    avg_loss:0.444, val_acc:0.852]
Epoch [14/120    avg_loss:0.317, val_acc:0.888]
Epoch [15/120    avg_loss:0.191, val_acc:0.888]
Epoch [16/120    avg_loss:0.346, val_acc:0.896]
Epoch [17/120    avg_loss:0.239, val_acc:0.904]
Epoch [18/120    avg_loss:0.359, val_acc:0.907]
Epoch [19/120    avg_loss:0.223, val_acc:0.914]
Epoch [20/120    avg_loss:0.151, val_acc:0.913]
Epoch [21/120    avg_loss:0.178, val_acc:0.934]
Epoch [22/120    avg_loss:0.190, val_acc:0.909]
Epoch [23/120    avg_loss:0.177, val_acc:0.938]
Epoch [24/120    avg_loss:0.132, val_acc:0.944]
Epoch [25/120    avg_loss:0.102, val_acc:0.942]
Epoch [26/120    avg_loss:0.073, val_acc:0.944]
Epoch [27/120    avg_loss:0.103, val_acc:0.954]
Epoch [28/120    avg_loss:0.090, val_acc:0.947]
Epoch [29/120    avg_loss:0.096, val_acc:0.934]
Epoch [30/120    avg_loss:0.110, val_acc:0.930]
Epoch [31/120    avg_loss:0.111, val_acc:0.947]
Epoch [32/120    avg_loss:0.055, val_acc:0.943]
Epoch [33/120    avg_loss:0.081, val_acc:0.958]
Epoch [34/120    avg_loss:0.043, val_acc:0.957]
Epoch [35/120    avg_loss:0.071, val_acc:0.947]
Epoch [36/120    avg_loss:0.065, val_acc:0.933]
Epoch [37/120    avg_loss:0.052, val_acc:0.960]
Epoch [38/120    avg_loss:0.057, val_acc:0.958]
Epoch [39/120    avg_loss:0.049, val_acc:0.958]
Epoch [40/120    avg_loss:0.029, val_acc:0.965]
Epoch [41/120    avg_loss:0.037, val_acc:0.948]
Epoch [42/120    avg_loss:0.040, val_acc:0.960]
Epoch [43/120    avg_loss:0.030, val_acc:0.963]
Epoch [44/120    avg_loss:0.032, val_acc:0.956]
Epoch [45/120    avg_loss:0.025, val_acc:0.957]
Epoch [46/120    avg_loss:0.031, val_acc:0.963]
Epoch [47/120    avg_loss:0.022, val_acc:0.970]
Epoch [48/120    avg_loss:0.028, val_acc:0.959]
Epoch [49/120    avg_loss:0.063, val_acc:0.967]
Epoch [50/120    avg_loss:0.037, val_acc:0.959]
Epoch [51/120    avg_loss:0.032, val_acc:0.966]
Epoch [52/120    avg_loss:0.010, val_acc:0.973]
Epoch [53/120    avg_loss:0.032, val_acc:0.965]
Epoch [54/120    avg_loss:0.030, val_acc:0.972]
Epoch [55/120    avg_loss:0.022, val_acc:0.970]
Epoch [56/120    avg_loss:0.013, val_acc:0.963]
Epoch [57/120    avg_loss:0.011, val_acc:0.973]
Epoch [58/120    avg_loss:0.029, val_acc:0.911]
Epoch [59/120    avg_loss:0.144, val_acc:0.938]
Epoch [60/120    avg_loss:0.046, val_acc:0.971]
Epoch [61/120    avg_loss:0.024, val_acc:0.965]
Epoch [62/120    avg_loss:0.018, val_acc:0.968]
Epoch [63/120    avg_loss:0.025, val_acc:0.956]
Epoch [64/120    avg_loss:0.022, val_acc:0.971]
Epoch [65/120    avg_loss:0.043, val_acc:0.938]
Epoch [66/120    avg_loss:0.078, val_acc:0.969]
Epoch [67/120    avg_loss:0.021, val_acc:0.967]
Epoch [68/120    avg_loss:0.013, val_acc:0.965]
Epoch [69/120    avg_loss:0.016, val_acc:0.967]
Epoch [70/120    avg_loss:0.039, val_acc:0.961]
Epoch [71/120    avg_loss:0.023, val_acc:0.972]
Epoch [72/120    avg_loss:0.012, val_acc:0.974]
Epoch [73/120    avg_loss:0.008, val_acc:0.974]
Epoch [74/120    avg_loss:0.012, val_acc:0.973]
Epoch [75/120    avg_loss:0.011, val_acc:0.971]
Epoch [76/120    avg_loss:0.012, val_acc:0.971]
Epoch [77/120    avg_loss:0.010, val_acc:0.971]
Epoch [78/120    avg_loss:0.008, val_acc:0.972]
Epoch [79/120    avg_loss:0.010, val_acc:0.975]
Epoch [80/120    avg_loss:0.010, val_acc:0.975]
Epoch [81/120    avg_loss:0.006, val_acc:0.976]
Epoch [82/120    avg_loss:0.008, val_acc:0.976]
Epoch [83/120    avg_loss:0.007, val_acc:0.976]
Epoch [84/120    avg_loss:0.007, val_acc:0.976]
Epoch [85/120    avg_loss:0.008, val_acc:0.973]
Epoch [86/120    avg_loss:0.007, val_acc:0.973]
Epoch [87/120    avg_loss:0.006, val_acc:0.974]
Epoch [88/120    avg_loss:0.007, val_acc:0.975]
Epoch [89/120    avg_loss:0.005, val_acc:0.975]
Epoch [90/120    avg_loss:0.005, val_acc:0.975]
Epoch [91/120    avg_loss:0.006, val_acc:0.975]
Epoch [92/120    avg_loss:0.008, val_acc:0.975]
Epoch [93/120    avg_loss:0.005, val_acc:0.976]
Epoch [94/120    avg_loss:0.011, val_acc:0.975]
Epoch [95/120    avg_loss:0.005, val_acc:0.974]
Epoch [96/120    avg_loss:0.007, val_acc:0.975]
Epoch [97/120    avg_loss:0.005, val_acc:0.975]
Epoch [98/120    avg_loss:0.007, val_acc:0.975]
Epoch [99/120    avg_loss:0.006, val_acc:0.974]
Epoch [100/120    avg_loss:0.007, val_acc:0.975]
Epoch [101/120    avg_loss:0.010, val_acc:0.976]
Epoch [102/120    avg_loss:0.006, val_acc:0.977]
Epoch [103/120    avg_loss:0.007, val_acc:0.977]
Epoch [104/120    avg_loss:0.005, val_acc:0.977]
Epoch [105/120    avg_loss:0.005, val_acc:0.977]
Epoch [106/120    avg_loss:0.007, val_acc:0.974]
Epoch [107/120    avg_loss:0.005, val_acc:0.974]
Epoch [108/120    avg_loss:0.004, val_acc:0.974]
Epoch [109/120    avg_loss:0.005, val_acc:0.975]
Epoch [110/120    avg_loss:0.004, val_acc:0.975]
Epoch [111/120    avg_loss:0.004, val_acc:0.975]
Epoch [112/120    avg_loss:0.004, val_acc:0.975]
Epoch [113/120    avg_loss:0.012, val_acc:0.976]
Epoch [114/120    avg_loss:0.006, val_acc:0.975]
Epoch [115/120    avg_loss:0.004, val_acc:0.976]
Epoch [116/120    avg_loss:0.005, val_acc:0.976]
Epoch [117/120    avg_loss:0.005, val_acc:0.975]
Epoch [118/120    avg_loss:0.004, val_acc:0.977]
Epoch [119/120    avg_loss:0.005, val_acc:0.977]
Epoch [120/120    avg_loss:0.004, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1255   10    8    0    0    0    0    0    1    8    3    0
     0    0    0]
 [   0    0    0  729    7    0    0    0    0    1    0    3    5    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    0    0    0    0    0
     1    1    0]
 [   0    0    0    0    0    0  651    0    0    4    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    1    0    0    1    0    0    0  854   11    0    0
     0    0    0]
 [   0    0   11    1    0    0    0    0    0    0    6 2172   18    0
     0    2    0]
 [   0    0    0    4    0    0    0    0    0    0    0    0  527    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1119   20    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    64  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.71273712737127

F1 scores:
[       nan 1.         0.9808519  0.9772118  0.96598639 0.99653979
 0.99313501 0.98039216 1.         0.87804878 0.98387097 0.9861521
 0.96786042 0.99462366 0.96299484 0.86064319 0.98203593]

Kappa:
0.9739270833559373
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff54da3b668>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.144, val_acc:0.486]
Epoch [2/120    avg_loss:1.563, val_acc:0.632]
Epoch [3/120    avg_loss:1.168, val_acc:0.674]
Epoch [4/120    avg_loss:0.973, val_acc:0.709]
Epoch [5/120    avg_loss:0.909, val_acc:0.752]
Epoch [6/120    avg_loss:0.838, val_acc:0.727]
Epoch [7/120    avg_loss:0.641, val_acc:0.760]
Epoch [8/120    avg_loss:0.654, val_acc:0.800]
Epoch [9/120    avg_loss:0.542, val_acc:0.779]
Epoch [10/120    avg_loss:0.391, val_acc:0.800]
Epoch [11/120    avg_loss:0.419, val_acc:0.777]
Epoch [12/120    avg_loss:0.484, val_acc:0.842]
Epoch [13/120    avg_loss:0.398, val_acc:0.897]
Epoch [14/120    avg_loss:0.262, val_acc:0.848]
Epoch [15/120    avg_loss:0.277, val_acc:0.881]
Epoch [16/120    avg_loss:0.254, val_acc:0.892]
Epoch [17/120    avg_loss:0.245, val_acc:0.857]
Epoch [18/120    avg_loss:0.175, val_acc:0.889]
Epoch [19/120    avg_loss:0.186, val_acc:0.915]
Epoch [20/120    avg_loss:0.237, val_acc:0.934]
Epoch [21/120    avg_loss:0.149, val_acc:0.895]
Epoch [22/120    avg_loss:0.150, val_acc:0.890]
Epoch [23/120    avg_loss:0.107, val_acc:0.926]
Epoch [24/120    avg_loss:0.111, val_acc:0.887]
Epoch [25/120    avg_loss:0.096, val_acc:0.932]
Epoch [26/120    avg_loss:0.075, val_acc:0.950]
Epoch [27/120    avg_loss:0.112, val_acc:0.922]
Epoch [28/120    avg_loss:0.114, val_acc:0.887]
Epoch [29/120    avg_loss:0.124, val_acc:0.948]
Epoch [30/120    avg_loss:0.069, val_acc:0.943]
Epoch [31/120    avg_loss:0.112, val_acc:0.944]
Epoch [32/120    avg_loss:0.070, val_acc:0.947]
Epoch [33/120    avg_loss:0.059, val_acc:0.887]
Epoch [34/120    avg_loss:0.070, val_acc:0.945]
Epoch [35/120    avg_loss:0.065, val_acc:0.952]
Epoch [36/120    avg_loss:0.073, val_acc:0.903]
Epoch [37/120    avg_loss:0.087, val_acc:0.954]
Epoch [38/120    avg_loss:0.083, val_acc:0.938]
Epoch [39/120    avg_loss:0.135, val_acc:0.890]
Epoch [40/120    avg_loss:0.043, val_acc:0.954]
Epoch [41/120    avg_loss:0.038, val_acc:0.957]
Epoch [42/120    avg_loss:0.050, val_acc:0.961]
Epoch [43/120    avg_loss:0.036, val_acc:0.952]
Epoch [44/120    avg_loss:0.019, val_acc:0.965]
Epoch [45/120    avg_loss:0.026, val_acc:0.959]
Epoch [46/120    avg_loss:0.022, val_acc:0.968]
Epoch [47/120    avg_loss:0.024, val_acc:0.955]
Epoch [48/120    avg_loss:0.022, val_acc:0.961]
Epoch [49/120    avg_loss:0.022, val_acc:0.968]
Epoch [50/120    avg_loss:0.020, val_acc:0.964]
Epoch [51/120    avg_loss:0.029, val_acc:0.971]
Epoch [52/120    avg_loss:0.048, val_acc:0.922]
Epoch [53/120    avg_loss:0.040, val_acc:0.969]
Epoch [54/120    avg_loss:0.018, val_acc:0.966]
Epoch [55/120    avg_loss:0.016, val_acc:0.972]
Epoch [56/120    avg_loss:0.017, val_acc:0.969]
Epoch [57/120    avg_loss:0.048, val_acc:0.970]
Epoch [58/120    avg_loss:0.021, val_acc:0.972]
Epoch [59/120    avg_loss:0.013, val_acc:0.975]
Epoch [60/120    avg_loss:0.018, val_acc:0.975]
Epoch [61/120    avg_loss:0.021, val_acc:0.968]
Epoch [62/120    avg_loss:0.039, val_acc:0.953]
Epoch [63/120    avg_loss:0.029, val_acc:0.968]
Epoch [64/120    avg_loss:0.011, val_acc:0.968]
Epoch [65/120    avg_loss:0.018, val_acc:0.975]
Epoch [66/120    avg_loss:0.011, val_acc:0.968]
Epoch [67/120    avg_loss:0.078, val_acc:0.954]
Epoch [68/120    avg_loss:0.121, val_acc:0.954]
Epoch [69/120    avg_loss:0.051, val_acc:0.964]
Epoch [70/120    avg_loss:0.044, val_acc:0.973]
Epoch [71/120    avg_loss:0.052, val_acc:0.899]
Epoch [72/120    avg_loss:0.066, val_acc:0.965]
Epoch [73/120    avg_loss:0.027, val_acc:0.967]
Epoch [74/120    avg_loss:0.011, val_acc:0.970]
Epoch [75/120    avg_loss:0.009, val_acc:0.973]
Epoch [76/120    avg_loss:0.008, val_acc:0.977]
Epoch [77/120    avg_loss:0.006, val_acc:0.975]
Epoch [78/120    avg_loss:0.004, val_acc:0.975]
Epoch [79/120    avg_loss:0.008, val_acc:0.975]
Epoch [80/120    avg_loss:0.008, val_acc:0.978]
Epoch [81/120    avg_loss:0.008, val_acc:0.976]
Epoch [82/120    avg_loss:0.006, val_acc:0.973]
Epoch [83/120    avg_loss:0.005, val_acc:0.975]
Epoch [84/120    avg_loss:0.004, val_acc:0.976]
Epoch [85/120    avg_loss:0.005, val_acc:0.977]
Epoch [86/120    avg_loss:0.005, val_acc:0.977]
Epoch [87/120    avg_loss:0.003, val_acc:0.976]
Epoch [88/120    avg_loss:0.003, val_acc:0.979]
Epoch [89/120    avg_loss:0.006, val_acc:0.977]
Epoch [90/120    avg_loss:0.004, val_acc:0.978]
Epoch [91/120    avg_loss:0.003, val_acc:0.977]
Epoch [92/120    avg_loss:0.003, val_acc:0.976]
Epoch [93/120    avg_loss:0.002, val_acc:0.978]
Epoch [94/120    avg_loss:0.012, val_acc:0.963]
Epoch [95/120    avg_loss:0.011, val_acc:0.973]
Epoch [96/120    avg_loss:0.005, val_acc:0.975]
Epoch [97/120    avg_loss:0.004, val_acc:0.975]
Epoch [98/120    avg_loss:0.005, val_acc:0.976]
Epoch [99/120    avg_loss:0.008, val_acc:0.978]
Epoch [100/120    avg_loss:0.009, val_acc:0.980]
Epoch [101/120    avg_loss:0.004, val_acc:0.978]
Epoch [102/120    avg_loss:0.005, val_acc:0.978]
Epoch [103/120    avg_loss:0.007, val_acc:0.971]
Epoch [104/120    avg_loss:0.003, val_acc:0.977]
Epoch [105/120    avg_loss:0.002, val_acc:0.976]
Epoch [106/120    avg_loss:0.002, val_acc:0.978]
Epoch [107/120    avg_loss:0.002, val_acc:0.976]
Epoch [108/120    avg_loss:0.003, val_acc:0.979]
Epoch [109/120    avg_loss:0.002, val_acc:0.982]
Epoch [110/120    avg_loss:0.002, val_acc:0.979]
Epoch [111/120    avg_loss:0.002, val_acc:0.978]
Epoch [112/120    avg_loss:0.002, val_acc:0.981]
Epoch [113/120    avg_loss:0.002, val_acc:0.982]
Epoch [114/120    avg_loss:0.002, val_acc:0.981]
Epoch [115/120    avg_loss:0.003, val_acc:0.978]
Epoch [116/120    avg_loss:0.002, val_acc:0.982]
Epoch [117/120    avg_loss:0.002, val_acc:0.979]
Epoch [118/120    avg_loss:0.002, val_acc:0.977]
Epoch [119/120    avg_loss:0.001, val_acc:0.978]
Epoch [120/120    avg_loss:0.001, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1270    0    0    0    1    0    0    0    5    9    0    0
     0    0    0]
 [   0    0    0  729    3    0    0    0    0    1    0    3   11    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0  863   10    0    0
     0    1    0]
 [   0    0   10    0    0    0    1    0    3    0   12 2167    5    0
     0    4    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0  530    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1123   16    0]
 [   0    0    0    0    0    1    4    0    0    0    0    0    0    0
    82  260    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.85365853658537

F1 scores:
[       nan 0.98765432 0.9898675  0.98780488 0.99300699 0.99770115
 0.99545455 1.         0.99652375 0.94736842 0.98291572 0.98522391
 0.98057354 1.         0.95819113 0.82670906 0.93258427]

Kappa:
0.9755276205747522
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb9fdded5f8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.070, val_acc:0.487]
Epoch [2/120    avg_loss:1.500, val_acc:0.578]
Epoch [3/120    avg_loss:1.199, val_acc:0.686]
Epoch [4/120    avg_loss:0.978, val_acc:0.724]
Epoch [5/120    avg_loss:0.842, val_acc:0.738]
Epoch [6/120    avg_loss:0.840, val_acc:0.721]
Epoch [7/120    avg_loss:0.606, val_acc:0.740]
Epoch [8/120    avg_loss:0.597, val_acc:0.708]
Epoch [9/120    avg_loss:0.650, val_acc:0.800]
Epoch [10/120    avg_loss:0.536, val_acc:0.778]
Epoch [11/120    avg_loss:0.447, val_acc:0.854]
Epoch [12/120    avg_loss:0.403, val_acc:0.825]
Epoch [13/120    avg_loss:0.482, val_acc:0.841]
Epoch [14/120    avg_loss:0.362, val_acc:0.825]
Epoch [15/120    avg_loss:0.309, val_acc:0.831]
Epoch [16/120    avg_loss:0.292, val_acc:0.886]
Epoch [17/120    avg_loss:0.264, val_acc:0.904]
Epoch [18/120    avg_loss:0.206, val_acc:0.853]
Epoch [19/120    avg_loss:0.240, val_acc:0.873]
Epoch [20/120    avg_loss:0.179, val_acc:0.890]
Epoch [21/120    avg_loss:0.147, val_acc:0.908]
Epoch [22/120    avg_loss:0.305, val_acc:0.842]
Epoch [23/120    avg_loss:0.225, val_acc:0.905]
Epoch [24/120    avg_loss:0.168, val_acc:0.926]
Epoch [25/120    avg_loss:0.152, val_acc:0.924]
Epoch [26/120    avg_loss:0.096, val_acc:0.894]
Epoch [27/120    avg_loss:0.114, val_acc:0.925]
Epoch [28/120    avg_loss:0.104, val_acc:0.911]
Epoch [29/120    avg_loss:0.113, val_acc:0.938]
Epoch [30/120    avg_loss:0.079, val_acc:0.940]
Epoch [31/120    avg_loss:0.082, val_acc:0.947]
Epoch [32/120    avg_loss:0.072, val_acc:0.935]
Epoch [33/120    avg_loss:0.075, val_acc:0.947]
Epoch [34/120    avg_loss:0.058, val_acc:0.936]
Epoch [35/120    avg_loss:0.056, val_acc:0.929]
Epoch [36/120    avg_loss:0.167, val_acc:0.903]
Epoch [37/120    avg_loss:0.061, val_acc:0.949]
Epoch [38/120    avg_loss:0.045, val_acc:0.935]
Epoch [39/120    avg_loss:0.049, val_acc:0.951]
Epoch [40/120    avg_loss:0.054, val_acc:0.944]
Epoch [41/120    avg_loss:0.038, val_acc:0.952]
Epoch [42/120    avg_loss:0.054, val_acc:0.946]
Epoch [43/120    avg_loss:0.043, val_acc:0.934]
Epoch [44/120    avg_loss:0.027, val_acc:0.966]
Epoch [45/120    avg_loss:0.088, val_acc:0.868]
Epoch [46/120    avg_loss:0.121, val_acc:0.910]
Epoch [47/120    avg_loss:0.204, val_acc:0.941]
Epoch [48/120    avg_loss:0.095, val_acc:0.948]
Epoch [49/120    avg_loss:0.036, val_acc:0.955]
Epoch [50/120    avg_loss:0.033, val_acc:0.963]
Epoch [51/120    avg_loss:0.052, val_acc:0.963]
Epoch [52/120    avg_loss:0.038, val_acc:0.960]
Epoch [53/120    avg_loss:0.020, val_acc:0.970]
Epoch [54/120    avg_loss:0.019, val_acc:0.943]
Epoch [55/120    avg_loss:0.036, val_acc:0.957]
Epoch [56/120    avg_loss:0.021, val_acc:0.974]
Epoch [57/120    avg_loss:0.012, val_acc:0.978]
Epoch [58/120    avg_loss:0.010, val_acc:0.973]
Epoch [59/120    avg_loss:0.028, val_acc:0.966]
Epoch [60/120    avg_loss:0.020, val_acc:0.959]
Epoch [61/120    avg_loss:0.047, val_acc:0.954]
Epoch [62/120    avg_loss:0.034, val_acc:0.958]
Epoch [63/120    avg_loss:0.026, val_acc:0.961]
Epoch [64/120    avg_loss:0.017, val_acc:0.973]
Epoch [65/120    avg_loss:0.029, val_acc:0.973]
Epoch [66/120    avg_loss:0.016, val_acc:0.971]
Epoch [67/120    avg_loss:0.012, val_acc:0.970]
Epoch [68/120    avg_loss:0.014, val_acc:0.981]
Epoch [69/120    avg_loss:0.011, val_acc:0.977]
Epoch [70/120    avg_loss:0.007, val_acc:0.979]
Epoch [71/120    avg_loss:0.026, val_acc:0.943]
Epoch [72/120    avg_loss:0.015, val_acc:0.978]
Epoch [73/120    avg_loss:0.009, val_acc:0.977]
Epoch [74/120    avg_loss:0.007, val_acc:0.975]
Epoch [75/120    avg_loss:0.008, val_acc:0.959]
Epoch [76/120    avg_loss:0.008, val_acc:0.975]
Epoch [77/120    avg_loss:0.013, val_acc:0.966]
Epoch [78/120    avg_loss:0.007, val_acc:0.977]
Epoch [79/120    avg_loss:0.013, val_acc:0.972]
Epoch [80/120    avg_loss:0.017, val_acc:0.968]
Epoch [81/120    avg_loss:0.011, val_acc:0.959]
Epoch [82/120    avg_loss:0.016, val_acc:0.972]
Epoch [83/120    avg_loss:0.007, val_acc:0.975]
Epoch [84/120    avg_loss:0.006, val_acc:0.977]
Epoch [85/120    avg_loss:0.007, val_acc:0.978]
Epoch [86/120    avg_loss:0.006, val_acc:0.979]
Epoch [87/120    avg_loss:0.004, val_acc:0.979]
Epoch [88/120    avg_loss:0.005, val_acc:0.980]
Epoch [89/120    avg_loss:0.007, val_acc:0.977]
Epoch [90/120    avg_loss:0.012, val_acc:0.978]
Epoch [91/120    avg_loss:0.004, val_acc:0.977]
Epoch [92/120    avg_loss:0.007, val_acc:0.978]
Epoch [93/120    avg_loss:0.004, val_acc:0.978]
Epoch [94/120    avg_loss:0.003, val_acc:0.980]
Epoch [95/120    avg_loss:0.003, val_acc:0.979]
Epoch [96/120    avg_loss:0.006, val_acc:0.979]
Epoch [97/120    avg_loss:0.003, val_acc:0.979]
Epoch [98/120    avg_loss:0.004, val_acc:0.980]
Epoch [99/120    avg_loss:0.004, val_acc:0.979]
Epoch [100/120    avg_loss:0.003, val_acc:0.980]
Epoch [101/120    avg_loss:0.003, val_acc:0.980]
Epoch [102/120    avg_loss:0.004, val_acc:0.980]
Epoch [103/120    avg_loss:0.003, val_acc:0.980]
Epoch [104/120    avg_loss:0.003, val_acc:0.980]
Epoch [105/120    avg_loss:0.004, val_acc:0.980]
Epoch [106/120    avg_loss:0.004, val_acc:0.980]
Epoch [107/120    avg_loss:0.004, val_acc:0.981]
Epoch [108/120    avg_loss:0.004, val_acc:0.980]
Epoch [109/120    avg_loss:0.004, val_acc:0.980]
Epoch [110/120    avg_loss:0.004, val_acc:0.980]
Epoch [111/120    avg_loss:0.003, val_acc:0.981]
Epoch [112/120    avg_loss:0.003, val_acc:0.980]
Epoch [113/120    avg_loss:0.004, val_acc:0.980]
Epoch [114/120    avg_loss:0.002, val_acc:0.979]
Epoch [115/120    avg_loss:0.003, val_acc:0.980]
Epoch [116/120    avg_loss:0.004, val_acc:0.979]
Epoch [117/120    avg_loss:0.003, val_acc:0.979]
Epoch [118/120    avg_loss:0.004, val_acc:0.979]
Epoch [119/120    avg_loss:0.008, val_acc:0.979]
Epoch [120/120    avg_loss:0.003, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    8    0    0    0    0    0    0    9    7    0    1
     0    0    0]
 [   0    0    0  734    1    0    0    0    0    1    0    0   11    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0  857   16    0    0
     0    0    0]
 [   0    0   11    0    0    0    0    0    0    0   24 2163   12    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0    0  530    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    2    0    0    0    1    0    0    0
  1126    9    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
    70  270    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.8319783197832

F1 scores:
[       nan 0.98795181 0.98591549 0.9852349  0.99765808 0.99656357
 0.9924357  1.         0.99883586 0.97297297 0.97055493 0.98385263
 0.97426471 0.99730458 0.96445396 0.86124402 0.98224852]

Kappa:
0.9752821810328238
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f19858f86a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.074, val_acc:0.478]
Epoch [2/120    avg_loss:1.492, val_acc:0.474]
Epoch [3/120    avg_loss:1.240, val_acc:0.669]
Epoch [4/120    avg_loss:0.983, val_acc:0.686]
Epoch [5/120    avg_loss:0.915, val_acc:0.682]
Epoch [6/120    avg_loss:0.767, val_acc:0.786]
Epoch [7/120    avg_loss:0.732, val_acc:0.753]
Epoch [8/120    avg_loss:0.702, val_acc:0.753]
Epoch [9/120    avg_loss:0.519, val_acc:0.779]
Epoch [10/120    avg_loss:0.520, val_acc:0.841]
Epoch [11/120    avg_loss:0.491, val_acc:0.821]
Epoch [12/120    avg_loss:0.343, val_acc:0.875]
Epoch [13/120    avg_loss:0.356, val_acc:0.863]
Epoch [14/120    avg_loss:0.306, val_acc:0.889]
Epoch [15/120    avg_loss:0.234, val_acc:0.874]
Epoch [16/120    avg_loss:0.232, val_acc:0.927]
Epoch [17/120    avg_loss:0.263, val_acc:0.902]
Epoch [18/120    avg_loss:0.227, val_acc:0.890]
Epoch [19/120    avg_loss:0.168, val_acc:0.879]
Epoch [20/120    avg_loss:0.202, val_acc:0.899]
Epoch [21/120    avg_loss:0.159, val_acc:0.923]
Epoch [22/120    avg_loss:0.141, val_acc:0.934]
Epoch [23/120    avg_loss:0.126, val_acc:0.944]
Epoch [24/120    avg_loss:0.143, val_acc:0.919]
Epoch [25/120    avg_loss:0.100, val_acc:0.943]
Epoch [26/120    avg_loss:0.124, val_acc:0.939]
Epoch [27/120    avg_loss:0.081, val_acc:0.949]
Epoch [28/120    avg_loss:0.110, val_acc:0.932]
Epoch [29/120    avg_loss:0.059, val_acc:0.937]
Epoch [30/120    avg_loss:0.105, val_acc:0.934]
Epoch [31/120    avg_loss:0.090, val_acc:0.937]
Epoch [32/120    avg_loss:0.186, val_acc:0.906]
Epoch [33/120    avg_loss:0.168, val_acc:0.912]
Epoch [34/120    avg_loss:0.099, val_acc:0.947]
Epoch [35/120    avg_loss:0.073, val_acc:0.962]
Epoch [36/120    avg_loss:0.067, val_acc:0.954]
Epoch [37/120    avg_loss:0.047, val_acc:0.952]
Epoch [38/120    avg_loss:0.071, val_acc:0.941]
Epoch [39/120    avg_loss:0.043, val_acc:0.951]
Epoch [40/120    avg_loss:0.043, val_acc:0.963]
Epoch [41/120    avg_loss:0.050, val_acc:0.954]
Epoch [42/120    avg_loss:0.041, val_acc:0.959]
Epoch [43/120    avg_loss:0.049, val_acc:0.954]
Epoch [44/120    avg_loss:0.070, val_acc:0.957]
Epoch [45/120    avg_loss:0.088, val_acc:0.947]
Epoch [46/120    avg_loss:0.097, val_acc:0.916]
Epoch [47/120    avg_loss:0.076, val_acc:0.958]
Epoch [48/120    avg_loss:0.032, val_acc:0.969]
Epoch [49/120    avg_loss:0.022, val_acc:0.964]
Epoch [50/120    avg_loss:0.028, val_acc:0.968]
Epoch [51/120    avg_loss:0.029, val_acc:0.964]
Epoch [52/120    avg_loss:0.021, val_acc:0.963]
Epoch [53/120    avg_loss:0.022, val_acc:0.963]
Epoch [54/120    avg_loss:0.013, val_acc:0.976]
Epoch [55/120    avg_loss:0.018, val_acc:0.970]
Epoch [56/120    avg_loss:0.019, val_acc:0.973]
Epoch [57/120    avg_loss:0.014, val_acc:0.971]
Epoch [58/120    avg_loss:0.011, val_acc:0.974]
Epoch [59/120    avg_loss:0.013, val_acc:0.977]
Epoch [60/120    avg_loss:0.008, val_acc:0.976]
Epoch [61/120    avg_loss:0.006, val_acc:0.979]
Epoch [62/120    avg_loss:0.007, val_acc:0.979]
Epoch [63/120    avg_loss:0.015, val_acc:0.968]
Epoch [64/120    avg_loss:0.014, val_acc:0.976]
Epoch [65/120    avg_loss:0.025, val_acc:0.977]
Epoch [66/120    avg_loss:0.012, val_acc:0.973]
Epoch [67/120    avg_loss:0.016, val_acc:0.970]
Epoch [68/120    avg_loss:0.029, val_acc:0.950]
Epoch [69/120    avg_loss:0.065, val_acc:0.976]
Epoch [70/120    avg_loss:0.039, val_acc:0.973]
Epoch [71/120    avg_loss:0.028, val_acc:0.973]
Epoch [72/120    avg_loss:0.015, val_acc:0.980]
Epoch [73/120    avg_loss:0.011, val_acc:0.978]
Epoch [74/120    avg_loss:0.015, val_acc:0.945]
Epoch [75/120    avg_loss:0.031, val_acc:0.951]
Epoch [76/120    avg_loss:0.137, val_acc:0.943]
Epoch [77/120    avg_loss:0.055, val_acc:0.968]
Epoch [78/120    avg_loss:0.050, val_acc:0.959]
Epoch [79/120    avg_loss:0.026, val_acc:0.957]
Epoch [80/120    avg_loss:0.032, val_acc:0.962]
Epoch [81/120    avg_loss:0.015, val_acc:0.961]
Epoch [82/120    avg_loss:0.012, val_acc:0.973]
Epoch [83/120    avg_loss:0.009, val_acc:0.980]
Epoch [84/120    avg_loss:0.017, val_acc:0.970]
Epoch [85/120    avg_loss:0.012, val_acc:0.978]
Epoch [86/120    avg_loss:0.015, val_acc:0.976]
Epoch [87/120    avg_loss:0.007, val_acc:0.979]
Epoch [88/120    avg_loss:0.005, val_acc:0.983]
Epoch [89/120    avg_loss:0.004, val_acc:0.979]
Epoch [90/120    avg_loss:0.005, val_acc:0.982]
Epoch [91/120    avg_loss:0.004, val_acc:0.980]
Epoch [92/120    avg_loss:0.017, val_acc:0.974]
Epoch [93/120    avg_loss:0.008, val_acc:0.978]
Epoch [94/120    avg_loss:0.005, val_acc:0.986]
Epoch [95/120    avg_loss:0.019, val_acc:0.957]
Epoch [96/120    avg_loss:0.013, val_acc:0.981]
Epoch [97/120    avg_loss:0.005, val_acc:0.982]
Epoch [98/120    avg_loss:0.004, val_acc:0.983]
Epoch [99/120    avg_loss:0.004, val_acc:0.985]
Epoch [100/120    avg_loss:0.002, val_acc:0.985]
Epoch [101/120    avg_loss:0.003, val_acc:0.985]
Epoch [102/120    avg_loss:0.003, val_acc:0.987]
Epoch [103/120    avg_loss:0.002, val_acc:0.986]
Epoch [104/120    avg_loss:0.033, val_acc:0.975]
Epoch [105/120    avg_loss:0.009, val_acc:0.979]
Epoch [106/120    avg_loss:0.006, val_acc:0.976]
Epoch [107/120    avg_loss:0.010, val_acc:0.981]
Epoch [108/120    avg_loss:0.008, val_acc:0.977]
Epoch [109/120    avg_loss:0.003, val_acc:0.982]
Epoch [110/120    avg_loss:0.004, val_acc:0.983]
Epoch [111/120    avg_loss:0.002, val_acc:0.983]
Epoch [112/120    avg_loss:0.003, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.981]
Epoch [114/120    avg_loss:0.018, val_acc:0.976]
Epoch [115/120    avg_loss:0.007, val_acc:0.981]
Epoch [116/120    avg_loss:0.004, val_acc:0.981]
Epoch [117/120    avg_loss:0.004, val_acc:0.980]
Epoch [118/120    avg_loss:0.005, val_acc:0.980]
Epoch [119/120    avg_loss:0.004, val_acc:0.980]
Epoch [120/120    avg_loss:0.005, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1262    0    8    0    0    0    0    0    7    8    0    0
     0    0    0]
 [   0    0    0  722    7    0    0    0    0    1    1    5   11    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    3    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    2    0    0    0  858   13    0    0
     0    0    0]
 [   0    0    9    0    0    0    2    0    0    0    8 2182    8    0
     0    1    0]
 [   0    0    0    2    0    0    0    0    0    0    2    0  528    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1123   16    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    81  258    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.72357723577235

F1 scores:
[       nan 1.         0.98670837 0.98164514 0.96598639 0.99884925
 0.98789713 1.         1.         0.9        0.98001142 0.98733032
 0.97597043 1.         0.95860009 0.82958199 0.98224852]

Kappa:
0.9740352439547153
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4293dc8668>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.052, val_acc:0.499]
Epoch [2/120    avg_loss:1.550, val_acc:0.639]
Epoch [3/120    avg_loss:1.182, val_acc:0.688]
Epoch [4/120    avg_loss:1.017, val_acc:0.706]
Epoch [5/120    avg_loss:0.879, val_acc:0.770]
Epoch [6/120    avg_loss:0.712, val_acc:0.760]
Epoch [7/120    avg_loss:0.729, val_acc:0.801]
Epoch [8/120    avg_loss:0.590, val_acc:0.815]
Epoch [9/120    avg_loss:0.616, val_acc:0.785]
Epoch [10/120    avg_loss:0.484, val_acc:0.843]
Epoch [11/120    avg_loss:0.420, val_acc:0.861]
Epoch [12/120    avg_loss:0.423, val_acc:0.877]
Epoch [13/120    avg_loss:0.297, val_acc:0.858]
Epoch [14/120    avg_loss:0.332, val_acc:0.862]
Epoch [15/120    avg_loss:0.212, val_acc:0.884]
Epoch [16/120    avg_loss:0.368, val_acc:0.866]
Epoch [17/120    avg_loss:0.216, val_acc:0.925]
Epoch [18/120    avg_loss:0.164, val_acc:0.911]
Epoch [19/120    avg_loss:0.225, val_acc:0.933]
Epoch [20/120    avg_loss:0.176, val_acc:0.900]
Epoch [21/120    avg_loss:0.155, val_acc:0.944]
Epoch [22/120    avg_loss:0.179, val_acc:0.913]
Epoch [23/120    avg_loss:0.144, val_acc:0.945]
Epoch [24/120    avg_loss:0.104, val_acc:0.938]
Epoch [25/120    avg_loss:0.113, val_acc:0.941]
Epoch [26/120    avg_loss:0.126, val_acc:0.944]
Epoch [27/120    avg_loss:0.082, val_acc:0.946]
Epoch [28/120    avg_loss:0.068, val_acc:0.966]
Epoch [29/120    avg_loss:0.056, val_acc:0.920]
Epoch [30/120    avg_loss:0.137, val_acc:0.952]
Epoch [31/120    avg_loss:0.069, val_acc:0.961]
Epoch [32/120    avg_loss:0.085, val_acc:0.958]
Epoch [33/120    avg_loss:0.065, val_acc:0.958]
Epoch [34/120    avg_loss:0.048, val_acc:0.948]
Epoch [35/120    avg_loss:0.112, val_acc:0.939]
Epoch [36/120    avg_loss:0.133, val_acc:0.939]
Epoch [37/120    avg_loss:0.096, val_acc:0.955]
Epoch [38/120    avg_loss:0.082, val_acc:0.936]
Epoch [39/120    avg_loss:0.058, val_acc:0.966]
Epoch [40/120    avg_loss:0.035, val_acc:0.962]
Epoch [41/120    avg_loss:0.048, val_acc:0.968]
Epoch [42/120    avg_loss:0.024, val_acc:0.966]
Epoch [43/120    avg_loss:0.034, val_acc:0.963]
Epoch [44/120    avg_loss:0.038, val_acc:0.964]
Epoch [45/120    avg_loss:0.019, val_acc:0.967]
Epoch [46/120    avg_loss:0.032, val_acc:0.959]
Epoch [47/120    avg_loss:0.020, val_acc:0.976]
Epoch [48/120    avg_loss:0.020, val_acc:0.979]
Epoch [49/120    avg_loss:0.018, val_acc:0.960]
Epoch [50/120    avg_loss:0.021, val_acc:0.940]
Epoch [51/120    avg_loss:0.031, val_acc:0.960]
Epoch [52/120    avg_loss:0.049, val_acc:0.946]
Epoch [53/120    avg_loss:0.020, val_acc:0.963]
Epoch [54/120    avg_loss:0.015, val_acc:0.971]
Epoch [55/120    avg_loss:0.012, val_acc:0.968]
Epoch [56/120    avg_loss:0.016, val_acc:0.964]
Epoch [57/120    avg_loss:0.054, val_acc:0.958]
Epoch [58/120    avg_loss:0.024, val_acc:0.958]
Epoch [59/120    avg_loss:0.017, val_acc:0.971]
Epoch [60/120    avg_loss:0.008, val_acc:0.972]
Epoch [61/120    avg_loss:0.012, val_acc:0.965]
Epoch [62/120    avg_loss:0.012, val_acc:0.970]
Epoch [63/120    avg_loss:0.010, val_acc:0.970]
Epoch [64/120    avg_loss:0.009, val_acc:0.972]
Epoch [65/120    avg_loss:0.008, val_acc:0.973]
Epoch [66/120    avg_loss:0.007, val_acc:0.974]
Epoch [67/120    avg_loss:0.007, val_acc:0.975]
Epoch [68/120    avg_loss:0.007, val_acc:0.974]
Epoch [69/120    avg_loss:0.007, val_acc:0.974]
Epoch [70/120    avg_loss:0.009, val_acc:0.974]
Epoch [71/120    avg_loss:0.008, val_acc:0.974]
Epoch [72/120    avg_loss:0.007, val_acc:0.975]
Epoch [73/120    avg_loss:0.005, val_acc:0.976]
Epoch [74/120    avg_loss:0.005, val_acc:0.974]
Epoch [75/120    avg_loss:0.004, val_acc:0.975]
Epoch [76/120    avg_loss:0.007, val_acc:0.975]
Epoch [77/120    avg_loss:0.005, val_acc:0.976]
Epoch [78/120    avg_loss:0.009, val_acc:0.976]
Epoch [79/120    avg_loss:0.005, val_acc:0.976]
Epoch [80/120    avg_loss:0.006, val_acc:0.975]
Epoch [81/120    avg_loss:0.006, val_acc:0.975]
Epoch [82/120    avg_loss:0.007, val_acc:0.975]
Epoch [83/120    avg_loss:0.005, val_acc:0.975]
Epoch [84/120    avg_loss:0.006, val_acc:0.975]
Epoch [85/120    avg_loss:0.005, val_acc:0.975]
Epoch [86/120    avg_loss:0.006, val_acc:0.975]
Epoch [87/120    avg_loss:0.005, val_acc:0.975]
Epoch [88/120    avg_loss:0.005, val_acc:0.975]
Epoch [89/120    avg_loss:0.005, val_acc:0.975]
Epoch [90/120    avg_loss:0.008, val_acc:0.975]
Epoch [91/120    avg_loss:0.012, val_acc:0.975]
Epoch [92/120    avg_loss:0.005, val_acc:0.975]
Epoch [93/120    avg_loss:0.006, val_acc:0.975]
Epoch [94/120    avg_loss:0.006, val_acc:0.975]
Epoch [95/120    avg_loss:0.004, val_acc:0.975]
Epoch [96/120    avg_loss:0.005, val_acc:0.975]
Epoch [97/120    avg_loss:0.005, val_acc:0.975]
Epoch [98/120    avg_loss:0.007, val_acc:0.975]
Epoch [99/120    avg_loss:0.006, val_acc:0.975]
Epoch [100/120    avg_loss:0.005, val_acc:0.975]
Epoch [101/120    avg_loss:0.007, val_acc:0.975]
Epoch [102/120    avg_loss:0.005, val_acc:0.975]
Epoch [103/120    avg_loss:0.004, val_acc:0.975]
Epoch [104/120    avg_loss:0.007, val_acc:0.975]
Epoch [105/120    avg_loss:0.007, val_acc:0.975]
Epoch [106/120    avg_loss:0.006, val_acc:0.975]
Epoch [107/120    avg_loss:0.006, val_acc:0.975]
Epoch [108/120    avg_loss:0.005, val_acc:0.975]
Epoch [109/120    avg_loss:0.005, val_acc:0.975]
Epoch [110/120    avg_loss:0.004, val_acc:0.975]
Epoch [111/120    avg_loss:0.008, val_acc:0.975]
Epoch [112/120    avg_loss:0.007, val_acc:0.975]
Epoch [113/120    avg_loss:0.007, val_acc:0.975]
Epoch [114/120    avg_loss:0.006, val_acc:0.975]
Epoch [115/120    avg_loss:0.006, val_acc:0.975]
Epoch [116/120    avg_loss:0.005, val_acc:0.975]
Epoch [117/120    avg_loss:0.007, val_acc:0.975]
Epoch [118/120    avg_loss:0.006, val_acc:0.975]
Epoch [119/120    avg_loss:0.006, val_acc:0.975]
Epoch [120/120    avg_loss:0.005, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    9    0    0    0    0    0    0    4   12    0    0
     0    0    0]
 [   0    0    0  723    1    2    0    0    0    1    0    3   16    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    1  649    0    0    6    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    1    0    0    0    0  851   22    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    2    2 2188   13    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0    0    0    0  526    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1118   21    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    97  245    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.44173441734418

F1 scores:
[       nan 1.         0.9878479  0.97439353 0.99765808 0.99427262
 0.99008391 1.         1.         0.8        0.98267898 0.98625197
 0.96513761 1.         0.94946921 0.7980456  0.98224852]

Kappa:
0.9708094307139091
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f065a10a748>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.061, val_acc:0.603]
Epoch [2/120    avg_loss:1.449, val_acc:0.663]
Epoch [3/120    avg_loss:1.129, val_acc:0.655]
Epoch [4/120    avg_loss:0.958, val_acc:0.755]
Epoch [5/120    avg_loss:0.885, val_acc:0.771]
Epoch [6/120    avg_loss:0.691, val_acc:0.748]
Epoch [7/120    avg_loss:0.638, val_acc:0.804]
Epoch [8/120    avg_loss:0.635, val_acc:0.800]
Epoch [9/120    avg_loss:0.563, val_acc:0.835]
Epoch [10/120    avg_loss:0.453, val_acc:0.788]
Epoch [11/120    avg_loss:0.460, val_acc:0.857]
Epoch [12/120    avg_loss:0.375, val_acc:0.851]
Epoch [13/120    avg_loss:0.329, val_acc:0.866]
Epoch [14/120    avg_loss:0.372, val_acc:0.808]
Epoch [15/120    avg_loss:0.271, val_acc:0.895]
Epoch [16/120    avg_loss:0.275, val_acc:0.862]
Epoch [17/120    avg_loss:0.384, val_acc:0.805]
Epoch [18/120    avg_loss:0.206, val_acc:0.908]
Epoch [19/120    avg_loss:0.246, val_acc:0.888]
Epoch [20/120    avg_loss:0.175, val_acc:0.927]
Epoch [21/120    avg_loss:0.163, val_acc:0.896]
Epoch [22/120    avg_loss:0.191, val_acc:0.905]
Epoch [23/120    avg_loss:0.157, val_acc:0.930]
Epoch [24/120    avg_loss:0.170, val_acc:0.895]
Epoch [25/120    avg_loss:0.196, val_acc:0.919]
Epoch [26/120    avg_loss:0.159, val_acc:0.939]
Epoch [27/120    avg_loss:0.074, val_acc:0.918]
Epoch [28/120    avg_loss:0.103, val_acc:0.935]
Epoch [29/120    avg_loss:0.107, val_acc:0.938]
Epoch [30/120    avg_loss:0.090, val_acc:0.931]
Epoch [31/120    avg_loss:0.137, val_acc:0.937]
Epoch [32/120    avg_loss:0.112, val_acc:0.936]
Epoch [33/120    avg_loss:0.130, val_acc:0.928]
Epoch [34/120    avg_loss:0.117, val_acc:0.937]
Epoch [35/120    avg_loss:0.078, val_acc:0.942]
Epoch [36/120    avg_loss:0.057, val_acc:0.962]
Epoch [37/120    avg_loss:0.075, val_acc:0.936]
Epoch [38/120    avg_loss:0.088, val_acc:0.927]
Epoch [39/120    avg_loss:0.079, val_acc:0.964]
Epoch [40/120    avg_loss:0.071, val_acc:0.929]
Epoch [41/120    avg_loss:0.096, val_acc:0.955]
Epoch [42/120    avg_loss:0.138, val_acc:0.942]
Epoch [43/120    avg_loss:0.092, val_acc:0.955]
Epoch [44/120    avg_loss:0.042, val_acc:0.961]
Epoch [45/120    avg_loss:0.038, val_acc:0.956]
Epoch [46/120    avg_loss:0.029, val_acc:0.972]
Epoch [47/120    avg_loss:0.034, val_acc:0.965]
Epoch [48/120    avg_loss:0.025, val_acc:0.974]
Epoch [49/120    avg_loss:0.027, val_acc:0.966]
Epoch [50/120    avg_loss:0.037, val_acc:0.972]
Epoch [51/120    avg_loss:0.018, val_acc:0.976]
Epoch [52/120    avg_loss:0.041, val_acc:0.962]
Epoch [53/120    avg_loss:0.028, val_acc:0.968]
Epoch [54/120    avg_loss:0.021, val_acc:0.976]
Epoch [55/120    avg_loss:0.027, val_acc:0.945]
Epoch [56/120    avg_loss:0.016, val_acc:0.971]
Epoch [57/120    avg_loss:0.039, val_acc:0.980]
Epoch [58/120    avg_loss:0.015, val_acc:0.976]
Epoch [59/120    avg_loss:0.011, val_acc:0.985]
Epoch [60/120    avg_loss:0.018, val_acc:0.973]
Epoch [61/120    avg_loss:0.018, val_acc:0.983]
Epoch [62/120    avg_loss:0.015, val_acc:0.985]
Epoch [63/120    avg_loss:0.015, val_acc:0.976]
Epoch [64/120    avg_loss:0.015, val_acc:0.981]
Epoch [65/120    avg_loss:0.016, val_acc:0.980]
Epoch [66/120    avg_loss:0.015, val_acc:0.960]
Epoch [67/120    avg_loss:0.011, val_acc:0.983]
Epoch [68/120    avg_loss:0.010, val_acc:0.976]
Epoch [69/120    avg_loss:0.007, val_acc:0.977]
Epoch [70/120    avg_loss:0.009, val_acc:0.977]
Epoch [71/120    avg_loss:0.007, val_acc:0.983]
Epoch [72/120    avg_loss:0.011, val_acc:0.977]
Epoch [73/120    avg_loss:0.015, val_acc:0.982]
Epoch [74/120    avg_loss:0.010, val_acc:0.975]
Epoch [75/120    avg_loss:0.007, val_acc:0.982]
Epoch [76/120    avg_loss:0.006, val_acc:0.983]
Epoch [77/120    avg_loss:0.006, val_acc:0.980]
Epoch [78/120    avg_loss:0.006, val_acc:0.983]
Epoch [79/120    avg_loss:0.006, val_acc:0.982]
Epoch [80/120    avg_loss:0.006, val_acc:0.981]
Epoch [81/120    avg_loss:0.009, val_acc:0.982]
Epoch [82/120    avg_loss:0.005, val_acc:0.985]
Epoch [83/120    avg_loss:0.003, val_acc:0.985]
Epoch [84/120    avg_loss:0.006, val_acc:0.985]
Epoch [85/120    avg_loss:0.003, val_acc:0.983]
Epoch [86/120    avg_loss:0.004, val_acc:0.983]
Epoch [87/120    avg_loss:0.003, val_acc:0.986]
Epoch [88/120    avg_loss:0.004, val_acc:0.986]
Epoch [89/120    avg_loss:0.004, val_acc:0.986]
Epoch [90/120    avg_loss:0.005, val_acc:0.986]
Epoch [91/120    avg_loss:0.005, val_acc:0.987]
Epoch [92/120    avg_loss:0.004, val_acc:0.983]
Epoch [93/120    avg_loss:0.003, val_acc:0.985]
Epoch [94/120    avg_loss:0.006, val_acc:0.985]
Epoch [95/120    avg_loss:0.006, val_acc:0.987]
Epoch [96/120    avg_loss:0.004, val_acc:0.987]
Epoch [97/120    avg_loss:0.004, val_acc:0.987]
Epoch [98/120    avg_loss:0.003, val_acc:0.986]
Epoch [99/120    avg_loss:0.003, val_acc:0.986]
Epoch [100/120    avg_loss:0.004, val_acc:0.983]
Epoch [101/120    avg_loss:0.005, val_acc:0.983]
Epoch [102/120    avg_loss:0.003, val_acc:0.983]
Epoch [103/120    avg_loss:0.004, val_acc:0.985]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.985]
Epoch [106/120    avg_loss:0.004, val_acc:0.983]
Epoch [107/120    avg_loss:0.004, val_acc:0.986]
Epoch [108/120    avg_loss:0.003, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.986]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.004, val_acc:0.985]
Epoch [112/120    avg_loss:0.004, val_acc:0.985]
Epoch [113/120    avg_loss:0.003, val_acc:0.985]
Epoch [114/120    avg_loss:0.004, val_acc:0.985]
Epoch [115/120    avg_loss:0.004, val_acc:0.985]
Epoch [116/120    avg_loss:0.003, val_acc:0.985]
Epoch [117/120    avg_loss:0.004, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.985]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.006, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    5    0    0    0    0    0    0    6   14    0    0
     0    0    0]
 [   0    0    0  715    0    2    0    0    0    5    1    0   19    5
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0  863    7    0    0
     1    3    0]
 [   0    0    5    0    0    0    0    0    0    0    3 2201    0    0
     1    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0  531    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0    0    0
  1131    6    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    61  273    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.15718157181571

F1 scores:
[       nan 1.         0.98823529 0.97345133 0.99528302 0.99424626
 0.98793363 1.         1.         0.8372093  0.98572244 0.99278304
 0.9797048  0.98666667 0.96915167 0.86804452 0.98823529]

Kappa:
0.9789773947977963
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f225970d748>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.107, val_acc:0.524]
Epoch [2/120    avg_loss:1.419, val_acc:0.621]
Epoch [3/120    avg_loss:1.091, val_acc:0.714]
Epoch [4/120    avg_loss:0.968, val_acc:0.744]
Epoch [5/120    avg_loss:0.852, val_acc:0.770]
Epoch [6/120    avg_loss:0.735, val_acc:0.771]
Epoch [7/120    avg_loss:0.725, val_acc:0.777]
Epoch [8/120    avg_loss:0.669, val_acc:0.831]
Epoch [9/120    avg_loss:0.499, val_acc:0.842]
Epoch [10/120    avg_loss:0.446, val_acc:0.742]
Epoch [11/120    avg_loss:0.456, val_acc:0.871]
Epoch [12/120    avg_loss:0.330, val_acc:0.821]
Epoch [13/120    avg_loss:0.280, val_acc:0.885]
Epoch [14/120    avg_loss:0.262, val_acc:0.893]
Epoch [15/120    avg_loss:0.222, val_acc:0.851]
Epoch [16/120    avg_loss:0.301, val_acc:0.910]
Epoch [17/120    avg_loss:0.188, val_acc:0.909]
Epoch [18/120    avg_loss:0.176, val_acc:0.866]
Epoch [19/120    avg_loss:0.317, val_acc:0.842]
Epoch [20/120    avg_loss:0.198, val_acc:0.935]
Epoch [21/120    avg_loss:0.198, val_acc:0.868]
Epoch [22/120    avg_loss:0.185, val_acc:0.904]
Epoch [23/120    avg_loss:0.151, val_acc:0.929]
Epoch [24/120    avg_loss:0.122, val_acc:0.961]
Epoch [25/120    avg_loss:0.160, val_acc:0.923]
Epoch [26/120    avg_loss:0.130, val_acc:0.934]
Epoch [27/120    avg_loss:0.105, val_acc:0.957]
Epoch [28/120    avg_loss:0.077, val_acc:0.940]
Epoch [29/120    avg_loss:0.112, val_acc:0.952]
Epoch [30/120    avg_loss:0.136, val_acc:0.918]
Epoch [31/120    avg_loss:0.126, val_acc:0.938]
Epoch [32/120    avg_loss:0.103, val_acc:0.935]
Epoch [33/120    avg_loss:0.065, val_acc:0.954]
Epoch [34/120    avg_loss:0.059, val_acc:0.959]
Epoch [35/120    avg_loss:0.045, val_acc:0.957]
Epoch [36/120    avg_loss:0.144, val_acc:0.946]
Epoch [37/120    avg_loss:0.131, val_acc:0.895]
Epoch [38/120    avg_loss:0.103, val_acc:0.959]
Epoch [39/120    avg_loss:0.055, val_acc:0.967]
Epoch [40/120    avg_loss:0.058, val_acc:0.968]
Epoch [41/120    avg_loss:0.037, val_acc:0.971]
Epoch [42/120    avg_loss:0.038, val_acc:0.972]
Epoch [43/120    avg_loss:0.043, val_acc:0.973]
Epoch [44/120    avg_loss:0.034, val_acc:0.975]
Epoch [45/120    avg_loss:0.031, val_acc:0.975]
Epoch [46/120    avg_loss:0.029, val_acc:0.977]
Epoch [47/120    avg_loss:0.032, val_acc:0.977]
Epoch [48/120    avg_loss:0.028, val_acc:0.975]
Epoch [49/120    avg_loss:0.029, val_acc:0.977]
Epoch [50/120    avg_loss:0.031, val_acc:0.976]
Epoch [51/120    avg_loss:0.027, val_acc:0.977]
Epoch [52/120    avg_loss:0.026, val_acc:0.978]
Epoch [53/120    avg_loss:0.020, val_acc:0.977]
Epoch [54/120    avg_loss:0.024, val_acc:0.976]
Epoch [55/120    avg_loss:0.023, val_acc:0.978]
Epoch [56/120    avg_loss:0.027, val_acc:0.979]
Epoch [57/120    avg_loss:0.034, val_acc:0.979]
Epoch [58/120    avg_loss:0.030, val_acc:0.978]
Epoch [59/120    avg_loss:0.022, val_acc:0.978]
Epoch [60/120    avg_loss:0.024, val_acc:0.978]
Epoch [61/120    avg_loss:0.024, val_acc:0.977]
Epoch [62/120    avg_loss:0.020, val_acc:0.978]
Epoch [63/120    avg_loss:0.024, val_acc:0.979]
Epoch [64/120    avg_loss:0.019, val_acc:0.979]
Epoch [65/120    avg_loss:0.021, val_acc:0.979]
Epoch [66/120    avg_loss:0.021, val_acc:0.978]
Epoch [67/120    avg_loss:0.024, val_acc:0.979]
Epoch [68/120    avg_loss:0.018, val_acc:0.980]
Epoch [69/120    avg_loss:0.023, val_acc:0.978]
Epoch [70/120    avg_loss:0.019, val_acc:0.980]
Epoch [71/120    avg_loss:0.022, val_acc:0.981]
Epoch [72/120    avg_loss:0.019, val_acc:0.980]
Epoch [73/120    avg_loss:0.016, val_acc:0.980]
Epoch [74/120    avg_loss:0.016, val_acc:0.981]
Epoch [75/120    avg_loss:0.021, val_acc:0.981]
Epoch [76/120    avg_loss:0.016, val_acc:0.982]
Epoch [77/120    avg_loss:0.018, val_acc:0.979]
Epoch [78/120    avg_loss:0.018, val_acc:0.979]
Epoch [79/120    avg_loss:0.018, val_acc:0.978]
Epoch [80/120    avg_loss:0.014, val_acc:0.979]
Epoch [81/120    avg_loss:0.014, val_acc:0.979]
Epoch [82/120    avg_loss:0.015, val_acc:0.980]
Epoch [83/120    avg_loss:0.017, val_acc:0.980]
Epoch [84/120    avg_loss:0.017, val_acc:0.980]
Epoch [85/120    avg_loss:0.018, val_acc:0.979]
Epoch [86/120    avg_loss:0.021, val_acc:0.979]
Epoch [87/120    avg_loss:0.015, val_acc:0.980]
Epoch [88/120    avg_loss:0.017, val_acc:0.981]
Epoch [89/120    avg_loss:0.013, val_acc:0.979]
Epoch [90/120    avg_loss:0.021, val_acc:0.979]
Epoch [91/120    avg_loss:0.013, val_acc:0.979]
Epoch [92/120    avg_loss:0.009, val_acc:0.980]
Epoch [93/120    avg_loss:0.012, val_acc:0.979]
Epoch [94/120    avg_loss:0.013, val_acc:0.980]
Epoch [95/120    avg_loss:0.013, val_acc:0.980]
Epoch [96/120    avg_loss:0.014, val_acc:0.980]
Epoch [97/120    avg_loss:0.014, val_acc:0.980]
Epoch [98/120    avg_loss:0.018, val_acc:0.980]
Epoch [99/120    avg_loss:0.013, val_acc:0.980]
Epoch [100/120    avg_loss:0.014, val_acc:0.980]
Epoch [101/120    avg_loss:0.013, val_acc:0.980]
Epoch [102/120    avg_loss:0.018, val_acc:0.980]
Epoch [103/120    avg_loss:0.012, val_acc:0.980]
Epoch [104/120    avg_loss:0.012, val_acc:0.980]
Epoch [105/120    avg_loss:0.016, val_acc:0.980]
Epoch [106/120    avg_loss:0.012, val_acc:0.980]
Epoch [107/120    avg_loss:0.015, val_acc:0.980]
Epoch [108/120    avg_loss:0.013, val_acc:0.980]
Epoch [109/120    avg_loss:0.014, val_acc:0.980]
Epoch [110/120    avg_loss:0.017, val_acc:0.980]
Epoch [111/120    avg_loss:0.011, val_acc:0.980]
Epoch [112/120    avg_loss:0.013, val_acc:0.980]
Epoch [113/120    avg_loss:0.013, val_acc:0.980]
Epoch [114/120    avg_loss:0.013, val_acc:0.980]
Epoch [115/120    avg_loss:0.011, val_acc:0.980]
Epoch [116/120    avg_loss:0.012, val_acc:0.980]
Epoch [117/120    avg_loss:0.013, val_acc:0.980]
Epoch [118/120    avg_loss:0.012, val_acc:0.980]
Epoch [119/120    avg_loss:0.013, val_acc:0.980]
Epoch [120/120    avg_loss:0.012, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1266    2    0    3    4    0    0    0    7    2    0    0
     0    1    0]
 [   0    0    0  729    1    0    1    0    0    3    0    2   11    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    1    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    1    4    0    0    0  852   13    0    0
     1    0    0]
 [   0    0   23    0    0    0    3    0    0    0   16 2159    8    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    9    0  523    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1122   16    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    28  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.98373983739837

F1 scores:
[       nan 0.98765432 0.98215671 0.9864682  0.99765808 0.99311927
 0.98132935 1.         0.995338   0.9        0.96818182 0.98449612
 0.96851852 0.99730458 0.97991266 0.91505216 0.98224852]

Kappa:
0.9770248134291262
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8204812710>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.136, val_acc:0.466]
Epoch [2/120    avg_loss:1.475, val_acc:0.679]
Epoch [3/120    avg_loss:1.197, val_acc:0.739]
Epoch [4/120    avg_loss:0.954, val_acc:0.765]
Epoch [5/120    avg_loss:0.878, val_acc:0.753]
Epoch [6/120    avg_loss:0.810, val_acc:0.721]
Epoch [7/120    avg_loss:0.690, val_acc:0.820]
Epoch [8/120    avg_loss:0.597, val_acc:0.800]
Epoch [9/120    avg_loss:0.529, val_acc:0.817]
Epoch [10/120    avg_loss:0.414, val_acc:0.841]
Epoch [11/120    avg_loss:0.446, val_acc:0.833]
Epoch [12/120    avg_loss:0.417, val_acc:0.861]
Epoch [13/120    avg_loss:0.367, val_acc:0.861]
Epoch [14/120    avg_loss:0.319, val_acc:0.801]
Epoch [15/120    avg_loss:0.305, val_acc:0.890]
Epoch [16/120    avg_loss:0.238, val_acc:0.902]
Epoch [17/120    avg_loss:0.279, val_acc:0.879]
Epoch [18/120    avg_loss:0.398, val_acc:0.874]
Epoch [19/120    avg_loss:0.205, val_acc:0.910]
Epoch [20/120    avg_loss:0.237, val_acc:0.878]
Epoch [21/120    avg_loss:0.171, val_acc:0.934]
Epoch [22/120    avg_loss:0.145, val_acc:0.931]
Epoch [23/120    avg_loss:0.150, val_acc:0.917]
Epoch [24/120    avg_loss:0.211, val_acc:0.876]
Epoch [25/120    avg_loss:0.165, val_acc:0.903]
Epoch [26/120    avg_loss:0.120, val_acc:0.939]
Epoch [27/120    avg_loss:0.091, val_acc:0.942]
Epoch [28/120    avg_loss:0.151, val_acc:0.887]
Epoch [29/120    avg_loss:0.148, val_acc:0.949]
Epoch [30/120    avg_loss:0.113, val_acc:0.947]
Epoch [31/120    avg_loss:0.072, val_acc:0.952]
Epoch [32/120    avg_loss:0.105, val_acc:0.926]
Epoch [33/120    avg_loss:0.075, val_acc:0.956]
Epoch [34/120    avg_loss:0.072, val_acc:0.945]
Epoch [35/120    avg_loss:0.097, val_acc:0.949]
Epoch [36/120    avg_loss:0.068, val_acc:0.940]
Epoch [37/120    avg_loss:0.064, val_acc:0.964]
Epoch [38/120    avg_loss:0.042, val_acc:0.977]
Epoch [39/120    avg_loss:0.100, val_acc:0.963]
Epoch [40/120    avg_loss:0.065, val_acc:0.957]
Epoch [41/120    avg_loss:0.050, val_acc:0.921]
Epoch [42/120    avg_loss:0.067, val_acc:0.964]
Epoch [43/120    avg_loss:0.048, val_acc:0.972]
Epoch [44/120    avg_loss:0.038, val_acc:0.971]
Epoch [45/120    avg_loss:0.042, val_acc:0.974]
Epoch [46/120    avg_loss:0.035, val_acc:0.965]
Epoch [47/120    avg_loss:0.035, val_acc:0.970]
Epoch [48/120    avg_loss:0.072, val_acc:0.966]
Epoch [49/120    avg_loss:0.033, val_acc:0.968]
Epoch [50/120    avg_loss:0.028, val_acc:0.975]
Epoch [51/120    avg_loss:0.022, val_acc:0.959]
Epoch [52/120    avg_loss:0.032, val_acc:0.979]
Epoch [53/120    avg_loss:0.018, val_acc:0.981]
Epoch [54/120    avg_loss:0.014, val_acc:0.981]
Epoch [55/120    avg_loss:0.012, val_acc:0.981]
Epoch [56/120    avg_loss:0.012, val_acc:0.980]
Epoch [57/120    avg_loss:0.014, val_acc:0.980]
Epoch [58/120    avg_loss:0.010, val_acc:0.978]
Epoch [59/120    avg_loss:0.010, val_acc:0.978]
Epoch [60/120    avg_loss:0.010, val_acc:0.979]
Epoch [61/120    avg_loss:0.012, val_acc:0.979]
Epoch [62/120    avg_loss:0.015, val_acc:0.980]
Epoch [63/120    avg_loss:0.007, val_acc:0.980]
Epoch [64/120    avg_loss:0.011, val_acc:0.980]
Epoch [65/120    avg_loss:0.010, val_acc:0.980]
Epoch [66/120    avg_loss:0.009, val_acc:0.981]
Epoch [67/120    avg_loss:0.012, val_acc:0.981]
Epoch [68/120    avg_loss:0.010, val_acc:0.981]
Epoch [69/120    avg_loss:0.012, val_acc:0.978]
Epoch [70/120    avg_loss:0.011, val_acc:0.978]
Epoch [71/120    avg_loss:0.012, val_acc:0.979]
Epoch [72/120    avg_loss:0.010, val_acc:0.978]
Epoch [73/120    avg_loss:0.010, val_acc:0.978]
Epoch [74/120    avg_loss:0.009, val_acc:0.979]
Epoch [75/120    avg_loss:0.009, val_acc:0.978]
Epoch [76/120    avg_loss:0.011, val_acc:0.978]
Epoch [77/120    avg_loss:0.013, val_acc:0.977]
Epoch [78/120    avg_loss:0.006, val_acc:0.979]
Epoch [79/120    avg_loss:0.016, val_acc:0.983]
Epoch [80/120    avg_loss:0.019, val_acc:0.981]
Epoch [81/120    avg_loss:0.011, val_acc:0.981]
Epoch [82/120    avg_loss:0.010, val_acc:0.980]
Epoch [83/120    avg_loss:0.010, val_acc:0.980]
Epoch [84/120    avg_loss:0.010, val_acc:0.981]
Epoch [85/120    avg_loss:0.009, val_acc:0.980]
Epoch [86/120    avg_loss:0.008, val_acc:0.980]
Epoch [87/120    avg_loss:0.009, val_acc:0.979]
Epoch [88/120    avg_loss:0.009, val_acc:0.979]
Epoch [89/120    avg_loss:0.010, val_acc:0.979]
Epoch [90/120    avg_loss:0.008, val_acc:0.979]
Epoch [91/120    avg_loss:0.012, val_acc:0.983]
Epoch [92/120    avg_loss:0.007, val_acc:0.979]
Epoch [93/120    avg_loss:0.009, val_acc:0.979]
Epoch [94/120    avg_loss:0.006, val_acc:0.979]
Epoch [95/120    avg_loss:0.008, val_acc:0.979]
Epoch [96/120    avg_loss:0.009, val_acc:0.979]
Epoch [97/120    avg_loss:0.009, val_acc:0.980]
Epoch [98/120    avg_loss:0.007, val_acc:0.979]
Epoch [99/120    avg_loss:0.008, val_acc:0.982]
Epoch [100/120    avg_loss:0.008, val_acc:0.981]
Epoch [101/120    avg_loss:0.008, val_acc:0.983]
Epoch [102/120    avg_loss:0.007, val_acc:0.981]
Epoch [103/120    avg_loss:0.008, val_acc:0.979]
Epoch [104/120    avg_loss:0.006, val_acc:0.979]
Epoch [105/120    avg_loss:0.007, val_acc:0.979]
Epoch [106/120    avg_loss:0.008, val_acc:0.979]
Epoch [107/120    avg_loss:0.007, val_acc:0.981]
Epoch [108/120    avg_loss:0.009, val_acc:0.979]
Epoch [109/120    avg_loss:0.007, val_acc:0.982]
Epoch [110/120    avg_loss:0.009, val_acc:0.984]
Epoch [111/120    avg_loss:0.006, val_acc:0.983]
Epoch [112/120    avg_loss:0.007, val_acc:0.982]
Epoch [113/120    avg_loss:0.007, val_acc:0.982]
Epoch [114/120    avg_loss:0.006, val_acc:0.982]
Epoch [115/120    avg_loss:0.007, val_acc:0.981]
Epoch [116/120    avg_loss:0.007, val_acc:0.982]
Epoch [117/120    avg_loss:0.006, val_acc:0.983]
Epoch [118/120    avg_loss:0.007, val_acc:0.983]
Epoch [119/120    avg_loss:0.007, val_acc:0.982]
Epoch [120/120    avg_loss:0.006, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1262    1    5    0    1    0    0    0    8    6    2    0
     0    0    0]
 [   0    0    0  723    0    0    0    0    0    6    1    1   12    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     1    1    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    1    1    0    0    0  860    5    0    0
     1    1    0]
 [   0    0    4    0    0    0    0    0    0    0    3 2201    0    1
     1    0    0]
 [   0    0    0    0    0    1    0    0    0    0    2    0  527    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    38  304    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.48238482384824

F1 scores:
[       nan 0.96202532 0.98709425 0.98300476 0.98839907 0.99770642
 0.99164768 1.         1.         0.85714286 0.98173516 0.99480226
 0.9795539  0.98666667 0.97746967 0.91291291 0.98224852]

Kappa:
0.982694901833665
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcdd86e4748>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.114, val_acc:0.464]
Epoch [2/120    avg_loss:1.425, val_acc:0.623]
Epoch [3/120    avg_loss:1.137, val_acc:0.703]
Epoch [4/120    avg_loss:0.997, val_acc:0.754]
Epoch [5/120    avg_loss:0.821, val_acc:0.761]
Epoch [6/120    avg_loss:0.758, val_acc:0.796]
Epoch [7/120    avg_loss:0.611, val_acc:0.812]
Epoch [8/120    avg_loss:0.551, val_acc:0.834]
Epoch [9/120    avg_loss:0.498, val_acc:0.777]
Epoch [10/120    avg_loss:0.471, val_acc:0.839]
Epoch [11/120    avg_loss:0.472, val_acc:0.809]
Epoch [12/120    avg_loss:0.370, val_acc:0.877]
Epoch [13/120    avg_loss:0.370, val_acc:0.893]
Epoch [14/120    avg_loss:0.257, val_acc:0.852]
Epoch [15/120    avg_loss:0.253, val_acc:0.877]
Epoch [16/120    avg_loss:0.213, val_acc:0.913]
Epoch [17/120    avg_loss:0.245, val_acc:0.856]
Epoch [18/120    avg_loss:0.276, val_acc:0.903]
Epoch [19/120    avg_loss:0.239, val_acc:0.912]
Epoch [20/120    avg_loss:0.201, val_acc:0.926]
Epoch [21/120    avg_loss:0.143, val_acc:0.937]
Epoch [22/120    avg_loss:0.153, val_acc:0.923]
Epoch [23/120    avg_loss:0.127, val_acc:0.946]
Epoch [24/120    avg_loss:0.178, val_acc:0.913]
Epoch [25/120    avg_loss:0.158, val_acc:0.922]
Epoch [26/120    avg_loss:0.227, val_acc:0.933]
Epoch [27/120    avg_loss:0.161, val_acc:0.931]
Epoch [28/120    avg_loss:0.086, val_acc:0.935]
Epoch [29/120    avg_loss:0.123, val_acc:0.942]
Epoch [30/120    avg_loss:0.101, val_acc:0.961]
Epoch [31/120    avg_loss:0.071, val_acc:0.954]
Epoch [32/120    avg_loss:0.081, val_acc:0.955]
Epoch [33/120    avg_loss:0.109, val_acc:0.963]
Epoch [34/120    avg_loss:0.070, val_acc:0.972]
Epoch [35/120    avg_loss:0.049, val_acc:0.963]
Epoch [36/120    avg_loss:0.051, val_acc:0.960]
Epoch [37/120    avg_loss:0.061, val_acc:0.961]
Epoch [38/120    avg_loss:0.077, val_acc:0.959]
Epoch [39/120    avg_loss:0.081, val_acc:0.949]
Epoch [40/120    avg_loss:0.058, val_acc:0.965]
Epoch [41/120    avg_loss:0.049, val_acc:0.974]
Epoch [42/120    avg_loss:0.073, val_acc:0.955]
Epoch [43/120    avg_loss:0.080, val_acc:0.854]
Epoch [44/120    avg_loss:0.111, val_acc:0.947]
Epoch [45/120    avg_loss:0.061, val_acc:0.926]
Epoch [46/120    avg_loss:0.038, val_acc:0.974]
Epoch [47/120    avg_loss:0.024, val_acc:0.970]
Epoch [48/120    avg_loss:0.056, val_acc:0.978]
Epoch [49/120    avg_loss:0.034, val_acc:0.979]
Epoch [50/120    avg_loss:0.023, val_acc:0.973]
Epoch [51/120    avg_loss:0.029, val_acc:0.980]
Epoch [52/120    avg_loss:0.015, val_acc:0.983]
Epoch [53/120    avg_loss:0.021, val_acc:0.978]
Epoch [54/120    avg_loss:0.026, val_acc:0.977]
Epoch [55/120    avg_loss:0.012, val_acc:0.982]
Epoch [56/120    avg_loss:0.016, val_acc:0.979]
Epoch [57/120    avg_loss:0.013, val_acc:0.974]
Epoch [58/120    avg_loss:0.012, val_acc:0.988]
Epoch [59/120    avg_loss:0.013, val_acc:0.989]
Epoch [60/120    avg_loss:0.009, val_acc:0.988]
Epoch [61/120    avg_loss:0.011, val_acc:0.985]
Epoch [62/120    avg_loss:0.007, val_acc:0.987]
Epoch [63/120    avg_loss:0.014, val_acc:0.987]
Epoch [64/120    avg_loss:0.007, val_acc:0.987]
Epoch [65/120    avg_loss:0.009, val_acc:0.982]
Epoch [66/120    avg_loss:0.009, val_acc:0.993]
Epoch [67/120    avg_loss:0.007, val_acc:0.986]
Epoch [68/120    avg_loss:0.019, val_acc:0.972]
Epoch [69/120    avg_loss:0.018, val_acc:0.979]
Epoch [70/120    avg_loss:0.017, val_acc:0.974]
Epoch [71/120    avg_loss:0.021, val_acc:0.983]
Epoch [72/120    avg_loss:0.016, val_acc:0.985]
Epoch [73/120    avg_loss:0.052, val_acc:0.946]
Epoch [74/120    avg_loss:0.191, val_acc:0.946]
Epoch [75/120    avg_loss:0.125, val_acc:0.962]
Epoch [76/120    avg_loss:0.025, val_acc:0.976]
Epoch [77/120    avg_loss:0.021, val_acc:0.974]
Epoch [78/120    avg_loss:0.016, val_acc:0.978]
Epoch [79/120    avg_loss:0.008, val_acc:0.976]
Epoch [80/120    avg_loss:0.008, val_acc:0.978]
Epoch [81/120    avg_loss:0.014, val_acc:0.983]
Epoch [82/120    avg_loss:0.008, val_acc:0.983]
Epoch [83/120    avg_loss:0.005, val_acc:0.983]
Epoch [84/120    avg_loss:0.007, val_acc:0.983]
Epoch [85/120    avg_loss:0.008, val_acc:0.984]
Epoch [86/120    avg_loss:0.009, val_acc:0.982]
Epoch [87/120    avg_loss:0.012, val_acc:0.982]
Epoch [88/120    avg_loss:0.006, val_acc:0.983]
Epoch [89/120    avg_loss:0.008, val_acc:0.983]
Epoch [90/120    avg_loss:0.008, val_acc:0.983]
Epoch [91/120    avg_loss:0.009, val_acc:0.982]
Epoch [92/120    avg_loss:0.006, val_acc:0.982]
Epoch [93/120    avg_loss:0.006, val_acc:0.982]
Epoch [94/120    avg_loss:0.008, val_acc:0.982]
Epoch [95/120    avg_loss:0.005, val_acc:0.982]
Epoch [96/120    avg_loss:0.009, val_acc:0.982]
Epoch [97/120    avg_loss:0.008, val_acc:0.983]
Epoch [98/120    avg_loss:0.006, val_acc:0.983]
Epoch [99/120    avg_loss:0.005, val_acc:0.982]
Epoch [100/120    avg_loss:0.006, val_acc:0.982]
Epoch [101/120    avg_loss:0.005, val_acc:0.982]
Epoch [102/120    avg_loss:0.006, val_acc:0.982]
Epoch [103/120    avg_loss:0.006, val_acc:0.982]
Epoch [104/120    avg_loss:0.007, val_acc:0.983]
Epoch [105/120    avg_loss:0.005, val_acc:0.983]
Epoch [106/120    avg_loss:0.005, val_acc:0.983]
Epoch [107/120    avg_loss:0.007, val_acc:0.983]
Epoch [108/120    avg_loss:0.006, val_acc:0.983]
Epoch [109/120    avg_loss:0.008, val_acc:0.983]
Epoch [110/120    avg_loss:0.007, val_acc:0.983]
Epoch [111/120    avg_loss:0.008, val_acc:0.983]
Epoch [112/120    avg_loss:0.005, val_acc:0.983]
Epoch [113/120    avg_loss:0.005, val_acc:0.983]
Epoch [114/120    avg_loss:0.007, val_acc:0.983]
Epoch [115/120    avg_loss:0.007, val_acc:0.983]
Epoch [116/120    avg_loss:0.004, val_acc:0.983]
Epoch [117/120    avg_loss:0.008, val_acc:0.983]
Epoch [118/120    avg_loss:0.008, val_acc:0.983]
Epoch [119/120    avg_loss:0.006, val_acc:0.983]
Epoch [120/120    avg_loss:0.004, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1266    3    0    0    0    0    0    3    6    7    0    0
     0    0    0]
 [   0    0    0  728    0    0    0    0    0    8    1    0    4    6
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    2  654    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    2  846   19    0    0
     0    6    0]
 [   0    0    3    0    0    0    1    0    0    1    6 2196    1    0
     0    2    0]
 [   0    0    0    3    0    1    0    0    0    0    1    3  522    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    1    8    0    0    0    0    0    0    0
    49  289    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.11382113821138

F1 scores:
[       nan 0.975      0.99138606 0.98245614 0.99764706 0.99315068
 0.99090909 1.         0.99883586 0.72       0.97409326 0.9903044
 0.98212606 0.98404255 0.97189797 0.87708649 0.97076023]

Kappa:
0.9784887180988687
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efd363c56d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.075, val_acc:0.473]
Epoch [2/120    avg_loss:1.436, val_acc:0.641]
Epoch [3/120    avg_loss:1.184, val_acc:0.700]
Epoch [4/120    avg_loss:0.931, val_acc:0.739]
Epoch [5/120    avg_loss:0.842, val_acc:0.739]
Epoch [6/120    avg_loss:0.752, val_acc:0.750]
Epoch [7/120    avg_loss:0.583, val_acc:0.726]
Epoch [8/120    avg_loss:0.631, val_acc:0.742]
Epoch [9/120    avg_loss:0.491, val_acc:0.773]
Epoch [10/120    avg_loss:0.463, val_acc:0.860]
Epoch [11/120    avg_loss:0.293, val_acc:0.845]
Epoch [12/120    avg_loss:0.396, val_acc:0.887]
Epoch [13/120    avg_loss:0.237, val_acc:0.879]
Epoch [14/120    avg_loss:0.297, val_acc:0.859]
Epoch [15/120    avg_loss:0.277, val_acc:0.815]
Epoch [16/120    avg_loss:0.224, val_acc:0.898]
Epoch [17/120    avg_loss:0.208, val_acc:0.874]
Epoch [18/120    avg_loss:0.340, val_acc:0.693]
Epoch [19/120    avg_loss:0.411, val_acc:0.897]
Epoch [20/120    avg_loss:0.229, val_acc:0.867]
Epoch [21/120    avg_loss:0.257, val_acc:0.909]
Epoch [22/120    avg_loss:0.161, val_acc:0.921]
Epoch [23/120    avg_loss:0.092, val_acc:0.936]
Epoch [24/120    avg_loss:0.127, val_acc:0.932]
Epoch [25/120    avg_loss:0.127, val_acc:0.939]
Epoch [26/120    avg_loss:0.114, val_acc:0.932]
Epoch [27/120    avg_loss:0.125, val_acc:0.941]
Epoch [28/120    avg_loss:0.086, val_acc:0.931]
Epoch [29/120    avg_loss:0.082, val_acc:0.950]
Epoch [30/120    avg_loss:0.059, val_acc:0.942]
Epoch [31/120    avg_loss:0.069, val_acc:0.923]
Epoch [32/120    avg_loss:0.082, val_acc:0.944]
Epoch [33/120    avg_loss:0.080, val_acc:0.919]
Epoch [34/120    avg_loss:0.132, val_acc:0.938]
Epoch [35/120    avg_loss:0.173, val_acc:0.921]
Epoch [36/120    avg_loss:0.114, val_acc:0.910]
Epoch [37/120    avg_loss:0.090, val_acc:0.959]
Epoch [38/120    avg_loss:0.038, val_acc:0.968]
Epoch [39/120    avg_loss:0.033, val_acc:0.961]
Epoch [40/120    avg_loss:0.037, val_acc:0.961]
Epoch [41/120    avg_loss:0.034, val_acc:0.955]
Epoch [42/120    avg_loss:0.038, val_acc:0.968]
Epoch [43/120    avg_loss:0.032, val_acc:0.951]
Epoch [44/120    avg_loss:0.064, val_acc:0.962]
Epoch [45/120    avg_loss:0.058, val_acc:0.949]
Epoch [46/120    avg_loss:0.054, val_acc:0.965]
Epoch [47/120    avg_loss:0.125, val_acc:0.934]
Epoch [48/120    avg_loss:0.132, val_acc:0.941]
Epoch [49/120    avg_loss:0.107, val_acc:0.939]
Epoch [50/120    avg_loss:0.054, val_acc:0.972]
Epoch [51/120    avg_loss:0.037, val_acc:0.963]
Epoch [52/120    avg_loss:0.033, val_acc:0.959]
Epoch [53/120    avg_loss:0.041, val_acc:0.972]
Epoch [54/120    avg_loss:0.033, val_acc:0.966]
Epoch [55/120    avg_loss:0.049, val_acc:0.956]
Epoch [56/120    avg_loss:0.035, val_acc:0.945]
Epoch [57/120    avg_loss:0.034, val_acc:0.963]
Epoch [58/120    avg_loss:0.023, val_acc:0.970]
Epoch [59/120    avg_loss:0.021, val_acc:0.973]
Epoch [60/120    avg_loss:0.022, val_acc:0.978]
Epoch [61/120    avg_loss:0.028, val_acc:0.964]
Epoch [62/120    avg_loss:0.020, val_acc:0.956]
Epoch [63/120    avg_loss:0.021, val_acc:0.968]
Epoch [64/120    avg_loss:0.013, val_acc:0.973]
Epoch [65/120    avg_loss:0.015, val_acc:0.970]
Epoch [66/120    avg_loss:0.012, val_acc:0.973]
Epoch [67/120    avg_loss:0.013, val_acc:0.979]
Epoch [68/120    avg_loss:0.008, val_acc:0.980]
Epoch [69/120    avg_loss:0.020, val_acc:0.976]
Epoch [70/120    avg_loss:0.006, val_acc:0.979]
Epoch [71/120    avg_loss:0.006, val_acc:0.982]
Epoch [72/120    avg_loss:0.006, val_acc:0.978]
Epoch [73/120    avg_loss:0.005, val_acc:0.980]
Epoch [74/120    avg_loss:0.005, val_acc:0.979]
Epoch [75/120    avg_loss:0.004, val_acc:0.980]
Epoch [76/120    avg_loss:0.007, val_acc:0.984]
Epoch [77/120    avg_loss:0.020, val_acc:0.979]
Epoch [78/120    avg_loss:0.008, val_acc:0.968]
Epoch [79/120    avg_loss:0.008, val_acc:0.976]
Epoch [80/120    avg_loss:0.003, val_acc:0.982]
Epoch [81/120    avg_loss:0.005, val_acc:0.982]
Epoch [82/120    avg_loss:0.005, val_acc:0.985]
Epoch [83/120    avg_loss:0.005, val_acc:0.981]
Epoch [84/120    avg_loss:0.003, val_acc:0.984]
Epoch [85/120    avg_loss:0.003, val_acc:0.981]
Epoch [86/120    avg_loss:0.007, val_acc:0.975]
Epoch [87/120    avg_loss:0.007, val_acc:0.980]
Epoch [88/120    avg_loss:0.010, val_acc:0.978]
Epoch [89/120    avg_loss:0.035, val_acc:0.973]
Epoch [90/120    avg_loss:0.023, val_acc:0.964]
Epoch [91/120    avg_loss:0.009, val_acc:0.977]
Epoch [92/120    avg_loss:0.009, val_acc:0.968]
Epoch [93/120    avg_loss:0.034, val_acc:0.950]
Epoch [94/120    avg_loss:0.040, val_acc:0.974]
Epoch [95/120    avg_loss:0.028, val_acc:0.965]
Epoch [96/120    avg_loss:0.019, val_acc:0.976]
Epoch [97/120    avg_loss:0.011, val_acc:0.980]
Epoch [98/120    avg_loss:0.007, val_acc:0.981]
Epoch [99/120    avg_loss:0.011, val_acc:0.978]
Epoch [100/120    avg_loss:0.010, val_acc:0.981]
Epoch [101/120    avg_loss:0.006, val_acc:0.981]
Epoch [102/120    avg_loss:0.007, val_acc:0.981]
Epoch [103/120    avg_loss:0.006, val_acc:0.982]
Epoch [104/120    avg_loss:0.007, val_acc:0.982]
Epoch [105/120    avg_loss:0.004, val_acc:0.981]
Epoch [106/120    avg_loss:0.005, val_acc:0.982]
Epoch [107/120    avg_loss:0.005, val_acc:0.983]
Epoch [108/120    avg_loss:0.004, val_acc:0.983]
Epoch [109/120    avg_loss:0.004, val_acc:0.983]
Epoch [110/120    avg_loss:0.006, val_acc:0.983]
Epoch [111/120    avg_loss:0.004, val_acc:0.982]
Epoch [112/120    avg_loss:0.004, val_acc:0.982]
Epoch [113/120    avg_loss:0.005, val_acc:0.982]
Epoch [114/120    avg_loss:0.005, val_acc:0.982]
Epoch [115/120    avg_loss:0.004, val_acc:0.982]
Epoch [116/120    avg_loss:0.006, val_acc:0.982]
Epoch [117/120    avg_loss:0.004, val_acc:0.983]
Epoch [118/120    avg_loss:0.004, val_acc:0.982]
Epoch [119/120    avg_loss:0.003, val_acc:0.982]
Epoch [120/120    avg_loss:0.005, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    1    0    0    0    0    0    2   14    8    0    0
     0    0    0]
 [   0    0    0  731    0    1    0    0    0    1    0    0   10    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    1    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0  871    0    0    0
     0    1    0]
 [   0    0    4    0    0    0    2    0    0    0   13 2191    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    3    0  524    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1136    3    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    54  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.52574525745257

F1 scores:
[       nan 0.98765432 0.98707403 0.98850575 1.         0.99196326
 0.99771863 1.         1.         0.9        0.98085586 0.99342553
 0.98127341 0.98930481 0.9751073  0.90993789 0.97674419]

Kappa:
0.9831890297927617
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f146e713710>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.102, val_acc:0.496]
Epoch [2/120    avg_loss:1.553, val_acc:0.610]
Epoch [3/120    avg_loss:1.245, val_acc:0.707]
Epoch [4/120    avg_loss:0.995, val_acc:0.700]
Epoch [5/120    avg_loss:0.771, val_acc:0.740]
Epoch [6/120    avg_loss:0.731, val_acc:0.780]
Epoch [7/120    avg_loss:0.686, val_acc:0.806]
Epoch [8/120    avg_loss:0.577, val_acc:0.845]
Epoch [9/120    avg_loss:0.515, val_acc:0.800]
Epoch [10/120    avg_loss:0.568, val_acc:0.842]
Epoch [11/120    avg_loss:0.467, val_acc:0.886]
Epoch [12/120    avg_loss:0.374, val_acc:0.858]
Epoch [13/120    avg_loss:0.298, val_acc:0.889]
Epoch [14/120    avg_loss:0.298, val_acc:0.873]
Epoch [15/120    avg_loss:0.358, val_acc:0.910]
Epoch [16/120    avg_loss:0.311, val_acc:0.863]
Epoch [17/120    avg_loss:0.284, val_acc:0.876]
Epoch [18/120    avg_loss:0.205, val_acc:0.934]
Epoch [19/120    avg_loss:0.261, val_acc:0.913]
Epoch [20/120    avg_loss:0.232, val_acc:0.912]
Epoch [21/120    avg_loss:0.182, val_acc:0.902]
Epoch [22/120    avg_loss:0.155, val_acc:0.901]
Epoch [23/120    avg_loss:0.175, val_acc:0.920]
Epoch [24/120    avg_loss:0.253, val_acc:0.899]
Epoch [25/120    avg_loss:0.234, val_acc:0.896]
Epoch [26/120    avg_loss:0.119, val_acc:0.921]
Epoch [27/120    avg_loss:0.114, val_acc:0.951]
Epoch [28/120    avg_loss:0.109, val_acc:0.941]
Epoch [29/120    avg_loss:0.138, val_acc:0.933]
Epoch [30/120    avg_loss:0.110, val_acc:0.968]
Epoch [31/120    avg_loss:0.067, val_acc:0.957]
Epoch [32/120    avg_loss:0.055, val_acc:0.951]
Epoch [33/120    avg_loss:0.062, val_acc:0.962]
Epoch [34/120    avg_loss:0.067, val_acc:0.926]
Epoch [35/120    avg_loss:0.064, val_acc:0.955]
Epoch [36/120    avg_loss:0.074, val_acc:0.958]
Epoch [37/120    avg_loss:0.101, val_acc:0.929]
Epoch [38/120    avg_loss:0.083, val_acc:0.944]
Epoch [39/120    avg_loss:0.110, val_acc:0.951]
Epoch [40/120    avg_loss:0.175, val_acc:0.964]
Epoch [41/120    avg_loss:0.070, val_acc:0.956]
Epoch [42/120    avg_loss:0.060, val_acc:0.969]
Epoch [43/120    avg_loss:0.061, val_acc:0.961]
Epoch [44/120    avg_loss:0.043, val_acc:0.967]
Epoch [45/120    avg_loss:0.036, val_acc:0.973]
Epoch [46/120    avg_loss:0.043, val_acc:0.966]
Epoch [47/120    avg_loss:0.047, val_acc:0.973]
Epoch [48/120    avg_loss:0.043, val_acc:0.962]
Epoch [49/120    avg_loss:0.027, val_acc:0.969]
Epoch [50/120    avg_loss:0.056, val_acc:0.959]
Epoch [51/120    avg_loss:0.032, val_acc:0.973]
Epoch [52/120    avg_loss:0.025, val_acc:0.957]
Epoch [53/120    avg_loss:0.036, val_acc:0.977]
Epoch [54/120    avg_loss:0.016, val_acc:0.977]
Epoch [55/120    avg_loss:0.043, val_acc:0.968]
Epoch [56/120    avg_loss:0.017, val_acc:0.979]
Epoch [57/120    avg_loss:0.009, val_acc:0.986]
Epoch [58/120    avg_loss:0.028, val_acc:0.972]
Epoch [59/120    avg_loss:0.027, val_acc:0.977]
Epoch [60/120    avg_loss:0.013, val_acc:0.978]
Epoch [61/120    avg_loss:0.018, val_acc:0.978]
Epoch [62/120    avg_loss:0.024, val_acc:0.973]
Epoch [63/120    avg_loss:0.022, val_acc:0.969]
Epoch [64/120    avg_loss:0.015, val_acc:0.983]
Epoch [65/120    avg_loss:0.008, val_acc:0.982]
Epoch [66/120    avg_loss:0.015, val_acc:0.979]
Epoch [67/120    avg_loss:0.006, val_acc:0.978]
Epoch [68/120    avg_loss:0.006, val_acc:0.986]
Epoch [69/120    avg_loss:0.005, val_acc:0.983]
Epoch [70/120    avg_loss:0.006, val_acc:0.985]
Epoch [71/120    avg_loss:0.010, val_acc:0.986]
Epoch [72/120    avg_loss:0.005, val_acc:0.986]
Epoch [73/120    avg_loss:0.004, val_acc:0.978]
Epoch [74/120    avg_loss:0.009, val_acc:0.983]
Epoch [75/120    avg_loss:0.007, val_acc:0.982]
Epoch [76/120    avg_loss:0.010, val_acc:0.959]
Epoch [77/120    avg_loss:0.021, val_acc:0.981]
Epoch [78/120    avg_loss:0.006, val_acc:0.981]
Epoch [79/120    avg_loss:0.007, val_acc:0.977]
Epoch [80/120    avg_loss:0.014, val_acc:0.956]
Epoch [81/120    avg_loss:0.041, val_acc:0.978]
Epoch [82/120    avg_loss:0.013, val_acc:0.974]
Epoch [83/120    avg_loss:0.010, val_acc:0.978]
Epoch [84/120    avg_loss:0.012, val_acc:0.980]
Epoch [85/120    avg_loss:0.005, val_acc:0.982]
Epoch [86/120    avg_loss:0.006, val_acc:0.982]
Epoch [87/120    avg_loss:0.004, val_acc:0.983]
Epoch [88/120    avg_loss:0.004, val_acc:0.983]
Epoch [89/120    avg_loss:0.003, val_acc:0.983]
Epoch [90/120    avg_loss:0.003, val_acc:0.983]
Epoch [91/120    avg_loss:0.004, val_acc:0.983]
Epoch [92/120    avg_loss:0.003, val_acc:0.983]
Epoch [93/120    avg_loss:0.003, val_acc:0.983]
Epoch [94/120    avg_loss:0.003, val_acc:0.983]
Epoch [95/120    avg_loss:0.003, val_acc:0.985]
Epoch [96/120    avg_loss:0.003, val_acc:0.985]
Epoch [97/120    avg_loss:0.003, val_acc:0.985]
Epoch [98/120    avg_loss:0.003, val_acc:0.985]
Epoch [99/120    avg_loss:0.002, val_acc:0.985]
Epoch [100/120    avg_loss:0.005, val_acc:0.985]
Epoch [101/120    avg_loss:0.003, val_acc:0.985]
Epoch [102/120    avg_loss:0.002, val_acc:0.985]
Epoch [103/120    avg_loss:0.003, val_acc:0.985]
Epoch [104/120    avg_loss:0.003, val_acc:0.985]
Epoch [105/120    avg_loss:0.003, val_acc:0.985]
Epoch [106/120    avg_loss:0.003, val_acc:0.985]
Epoch [107/120    avg_loss:0.002, val_acc:0.985]
Epoch [108/120    avg_loss:0.002, val_acc:0.985]
Epoch [109/120    avg_loss:0.007, val_acc:0.985]
Epoch [110/120    avg_loss:0.004, val_acc:0.985]
Epoch [111/120    avg_loss:0.003, val_acc:0.985]
Epoch [112/120    avg_loss:0.003, val_acc:0.985]
Epoch [113/120    avg_loss:0.004, val_acc:0.985]
Epoch [114/120    avg_loss:0.003, val_acc:0.985]
Epoch [115/120    avg_loss:0.003, val_acc:0.985]
Epoch [116/120    avg_loss:0.004, val_acc:0.985]
Epoch [117/120    avg_loss:0.003, val_acc:0.985]
Epoch [118/120    avg_loss:0.003, val_acc:0.985]
Epoch [119/120    avg_loss:0.003, val_acc:0.985]
Epoch [120/120    avg_loss:0.003, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1268    2    0    0    0    0    0    0    4   10    1    0
     0    0    0]
 [   0    0    0  721    3    0    0    0    0    6    0    0   11    6
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    2    0    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  652    0    0    4    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    4    0    0    0  843   19    1    0
     1    5    0]
 [   0    0    3    0    0    4    2    0    0    1    0 2194    4    2
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0  529    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1129    9    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    27  309    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.27642276422765

F1 scores:
[       nan 0.975      0.99101211 0.97961957 0.9882904  0.98964327
 0.98192771 1.         1.         0.75       0.97682503 0.98962562
 0.9787234  0.97883598 0.98259356 0.92101341 0.98224852]

Kappa:
0.980346551355894
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa593ed66d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.048, val_acc:0.565]
Epoch [2/120    avg_loss:1.482, val_acc:0.672]
Epoch [3/120    avg_loss:1.172, val_acc:0.619]
Epoch [4/120    avg_loss:1.005, val_acc:0.752]
Epoch [5/120    avg_loss:0.826, val_acc:0.725]
Epoch [6/120    avg_loss:0.781, val_acc:0.824]
Epoch [7/120    avg_loss:0.688, val_acc:0.826]
Epoch [8/120    avg_loss:0.585, val_acc:0.843]
Epoch [9/120    avg_loss:0.558, val_acc:0.812]
Epoch [10/120    avg_loss:0.505, val_acc:0.861]
Epoch [11/120    avg_loss:0.468, val_acc:0.850]
Epoch [12/120    avg_loss:0.351, val_acc:0.845]
Epoch [13/120    avg_loss:0.345, val_acc:0.873]
Epoch [14/120    avg_loss:0.339, val_acc:0.887]
Epoch [15/120    avg_loss:0.319, val_acc:0.879]
Epoch [16/120    avg_loss:0.323, val_acc:0.880]
Epoch [17/120    avg_loss:0.261, val_acc:0.880]
Epoch [18/120    avg_loss:0.288, val_acc:0.892]
Epoch [19/120    avg_loss:0.162, val_acc:0.922]
Epoch [20/120    avg_loss:0.153, val_acc:0.929]
Epoch [21/120    avg_loss:0.194, val_acc:0.914]
Epoch [22/120    avg_loss:0.199, val_acc:0.901]
Epoch [23/120    avg_loss:0.175, val_acc:0.889]
Epoch [24/120    avg_loss:0.167, val_acc:0.946]
Epoch [25/120    avg_loss:0.139, val_acc:0.931]
Epoch [26/120    avg_loss:0.118, val_acc:0.943]
Epoch [27/120    avg_loss:0.119, val_acc:0.909]
Epoch [28/120    avg_loss:0.098, val_acc:0.935]
Epoch [29/120    avg_loss:0.090, val_acc:0.946]
Epoch [30/120    avg_loss:0.125, val_acc:0.945]
Epoch [31/120    avg_loss:0.106, val_acc:0.926]
Epoch [32/120    avg_loss:0.091, val_acc:0.964]
Epoch [33/120    avg_loss:0.070, val_acc:0.940]
Epoch [34/120    avg_loss:0.159, val_acc:0.952]
Epoch [35/120    avg_loss:0.066, val_acc:0.949]
Epoch [36/120    avg_loss:0.088, val_acc:0.957]
Epoch [37/120    avg_loss:0.098, val_acc:0.968]
Epoch [38/120    avg_loss:0.121, val_acc:0.955]
Epoch [39/120    avg_loss:0.077, val_acc:0.967]
Epoch [40/120    avg_loss:0.038, val_acc:0.952]
Epoch [41/120    avg_loss:0.049, val_acc:0.959]
Epoch [42/120    avg_loss:0.053, val_acc:0.946]
Epoch [43/120    avg_loss:0.051, val_acc:0.966]
Epoch [44/120    avg_loss:0.114, val_acc:0.938]
Epoch [45/120    avg_loss:0.091, val_acc:0.958]
Epoch [46/120    avg_loss:0.057, val_acc:0.976]
Epoch [47/120    avg_loss:0.097, val_acc:0.953]
Epoch [48/120    avg_loss:0.070, val_acc:0.975]
Epoch [49/120    avg_loss:0.042, val_acc:0.976]
Epoch [50/120    avg_loss:0.022, val_acc:0.975]
Epoch [51/120    avg_loss:0.029, val_acc:0.979]
Epoch [52/120    avg_loss:0.028, val_acc:0.970]
Epoch [53/120    avg_loss:0.055, val_acc:0.976]
Epoch [54/120    avg_loss:0.078, val_acc:0.975]
Epoch [55/120    avg_loss:0.025, val_acc:0.976]
Epoch [56/120    avg_loss:0.065, val_acc:0.979]
Epoch [57/120    avg_loss:0.021, val_acc:0.983]
Epoch [58/120    avg_loss:0.027, val_acc:0.971]
Epoch [59/120    avg_loss:0.058, val_acc:0.973]
Epoch [60/120    avg_loss:0.018, val_acc:0.981]
Epoch [61/120    avg_loss:0.021, val_acc:0.974]
Epoch [62/120    avg_loss:0.036, val_acc:0.971]
Epoch [63/120    avg_loss:0.077, val_acc:0.971]
Epoch [64/120    avg_loss:0.032, val_acc:0.961]
Epoch [65/120    avg_loss:0.046, val_acc:0.936]
Epoch [66/120    avg_loss:0.075, val_acc:0.963]
Epoch [67/120    avg_loss:0.035, val_acc:0.977]
Epoch [68/120    avg_loss:0.019, val_acc:0.981]
Epoch [69/120    avg_loss:0.020, val_acc:0.984]
Epoch [70/120    avg_loss:0.012, val_acc:0.984]
Epoch [71/120    avg_loss:0.011, val_acc:0.975]
Epoch [72/120    avg_loss:0.012, val_acc:0.982]
Epoch [73/120    avg_loss:0.014, val_acc:0.980]
Epoch [74/120    avg_loss:0.024, val_acc:0.982]
Epoch [75/120    avg_loss:0.008, val_acc:0.988]
Epoch [76/120    avg_loss:0.010, val_acc:0.984]
Epoch [77/120    avg_loss:0.010, val_acc:0.988]
Epoch [78/120    avg_loss:0.005, val_acc:0.983]
Epoch [79/120    avg_loss:0.004, val_acc:0.991]
Epoch [80/120    avg_loss:0.005, val_acc:0.990]
Epoch [81/120    avg_loss:0.004, val_acc:0.986]
Epoch [82/120    avg_loss:0.010, val_acc:0.972]
Epoch [83/120    avg_loss:0.114, val_acc:0.945]
Epoch [84/120    avg_loss:0.062, val_acc:0.972]
Epoch [85/120    avg_loss:0.024, val_acc:0.970]
Epoch [86/120    avg_loss:0.015, val_acc:0.984]
Epoch [87/120    avg_loss:0.008, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.989]
Epoch [89/120    avg_loss:0.007, val_acc:0.988]
Epoch [90/120    avg_loss:0.006, val_acc:0.986]
Epoch [91/120    avg_loss:0.005, val_acc:0.988]
Epoch [92/120    avg_loss:0.004, val_acc:0.989]
Epoch [93/120    avg_loss:0.004, val_acc:0.988]
Epoch [94/120    avg_loss:0.005, val_acc:0.990]
Epoch [95/120    avg_loss:0.005, val_acc:0.989]
Epoch [96/120    avg_loss:0.004, val_acc:0.989]
Epoch [97/120    avg_loss:0.006, val_acc:0.986]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.004, val_acc:0.988]
Epoch [100/120    avg_loss:0.004, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.003, val_acc:0.986]
Epoch [103/120    avg_loss:0.003, val_acc:0.986]
Epoch [104/120    avg_loss:0.004, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.986]
Epoch [106/120    avg_loss:0.003, val_acc:0.986]
Epoch [107/120    avg_loss:0.004, val_acc:0.986]
Epoch [108/120    avg_loss:0.003, val_acc:0.986]
Epoch [109/120    avg_loss:0.003, val_acc:0.986]
Epoch [110/120    avg_loss:0.004, val_acc:0.986]
Epoch [111/120    avg_loss:0.003, val_acc:0.986]
Epoch [112/120    avg_loss:0.004, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.988]
Epoch [114/120    avg_loss:0.003, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.003, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.988]
Epoch [118/120    avg_loss:0.005, val_acc:0.988]
Epoch [119/120    avg_loss:0.004, val_acc:0.988]
Epoch [120/120    avg_loss:0.003, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1269    2    1    1    0    0    0    1    5    6    0    0
     0    0    0]
 [   0    0    0  733    0    0    0    0    0    7    0    0    3    4
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    1  654    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0  859    9    0    0
     1    5    0]
 [   0    0    7    0    0    0    0    0    0    0    0 2201    1    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0  530    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    1    0    0    0
  1129    7    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    38  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.66666666666667

F1 scores:
[       nan 1.         0.99063232 0.98853675 0.99530516 0.99427262
 0.99090909 1.         0.99883586 0.8        0.98622273 0.99412827
 0.99065421 0.98666667 0.97876029 0.91047041 0.98224852]

Kappa:
0.9847946898193776
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe46147a710>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.069, val_acc:0.576]
Epoch [2/120    avg_loss:1.473, val_acc:0.445]
Epoch [3/120    avg_loss:1.185, val_acc:0.632]
Epoch [4/120    avg_loss:0.978, val_acc:0.726]
Epoch [5/120    avg_loss:0.825, val_acc:0.777]
Epoch [6/120    avg_loss:0.677, val_acc:0.761]
Epoch [7/120    avg_loss:0.650, val_acc:0.784]
Epoch [8/120    avg_loss:0.540, val_acc:0.830]
Epoch [9/120    avg_loss:0.604, val_acc:0.813]
Epoch [10/120    avg_loss:0.509, val_acc:0.843]
Epoch [11/120    avg_loss:0.410, val_acc:0.822]
Epoch [12/120    avg_loss:0.478, val_acc:0.874]
Epoch [13/120    avg_loss:0.310, val_acc:0.894]
Epoch [14/120    avg_loss:0.290, val_acc:0.859]
Epoch [15/120    avg_loss:0.353, val_acc:0.884]
Epoch [16/120    avg_loss:0.300, val_acc:0.884]
Epoch [17/120    avg_loss:0.326, val_acc:0.849]
Epoch [18/120    avg_loss:0.266, val_acc:0.880]
Epoch [19/120    avg_loss:0.310, val_acc:0.882]
Epoch [20/120    avg_loss:0.200, val_acc:0.898]
Epoch [21/120    avg_loss:0.207, val_acc:0.921]
Epoch [22/120    avg_loss:0.298, val_acc:0.881]
Epoch [23/120    avg_loss:0.181, val_acc:0.915]
Epoch [24/120    avg_loss:0.111, val_acc:0.928]
Epoch [25/120    avg_loss:0.129, val_acc:0.902]
Epoch [26/120    avg_loss:0.101, val_acc:0.942]
Epoch [27/120    avg_loss:0.095, val_acc:0.907]
Epoch [28/120    avg_loss:0.132, val_acc:0.932]
Epoch [29/120    avg_loss:0.085, val_acc:0.944]
Epoch [30/120    avg_loss:0.092, val_acc:0.942]
Epoch [31/120    avg_loss:0.154, val_acc:0.939]
Epoch [32/120    avg_loss:0.086, val_acc:0.911]
Epoch [33/120    avg_loss:0.116, val_acc:0.933]
Epoch [34/120    avg_loss:0.063, val_acc:0.949]
Epoch [35/120    avg_loss:0.090, val_acc:0.958]
Epoch [36/120    avg_loss:0.064, val_acc:0.943]
Epoch [37/120    avg_loss:0.082, val_acc:0.884]
Epoch [38/120    avg_loss:0.089, val_acc:0.941]
Epoch [39/120    avg_loss:0.103, val_acc:0.955]
Epoch [40/120    avg_loss:0.047, val_acc:0.968]
Epoch [41/120    avg_loss:0.038, val_acc:0.955]
Epoch [42/120    avg_loss:0.043, val_acc:0.960]
Epoch [43/120    avg_loss:0.052, val_acc:0.962]
Epoch [44/120    avg_loss:0.054, val_acc:0.938]
Epoch [45/120    avg_loss:0.117, val_acc:0.960]
Epoch [46/120    avg_loss:0.049, val_acc:0.963]
Epoch [47/120    avg_loss:0.033, val_acc:0.964]
Epoch [48/120    avg_loss:0.052, val_acc:0.958]
Epoch [49/120    avg_loss:0.074, val_acc:0.956]
Epoch [50/120    avg_loss:0.117, val_acc:0.920]
Epoch [51/120    avg_loss:0.123, val_acc:0.939]
Epoch [52/120    avg_loss:0.066, val_acc:0.954]
Epoch [53/120    avg_loss:0.037, val_acc:0.957]
Epoch [54/120    avg_loss:0.034, val_acc:0.966]
Epoch [55/120    avg_loss:0.027, val_acc:0.971]
Epoch [56/120    avg_loss:0.021, val_acc:0.972]
Epoch [57/120    avg_loss:0.020, val_acc:0.975]
Epoch [58/120    avg_loss:0.020, val_acc:0.977]
Epoch [59/120    avg_loss:0.019, val_acc:0.975]
Epoch [60/120    avg_loss:0.018, val_acc:0.976]
Epoch [61/120    avg_loss:0.015, val_acc:0.975]
Epoch [62/120    avg_loss:0.013, val_acc:0.977]
Epoch [63/120    avg_loss:0.020, val_acc:0.977]
Epoch [64/120    avg_loss:0.013, val_acc:0.978]
Epoch [65/120    avg_loss:0.014, val_acc:0.978]
Epoch [66/120    avg_loss:0.013, val_acc:0.977]
Epoch [67/120    avg_loss:0.012, val_acc:0.977]
Epoch [68/120    avg_loss:0.014, val_acc:0.978]
Epoch [69/120    avg_loss:0.011, val_acc:0.979]
Epoch [70/120    avg_loss:0.014, val_acc:0.980]
Epoch [71/120    avg_loss:0.010, val_acc:0.978]
Epoch [72/120    avg_loss:0.018, val_acc:0.978]
Epoch [73/120    avg_loss:0.014, val_acc:0.978]
Epoch [74/120    avg_loss:0.008, val_acc:0.980]
Epoch [75/120    avg_loss:0.011, val_acc:0.978]
Epoch [76/120    avg_loss:0.013, val_acc:0.980]
Epoch [77/120    avg_loss:0.013, val_acc:0.978]
Epoch [78/120    avg_loss:0.010, val_acc:0.980]
Epoch [79/120    avg_loss:0.012, val_acc:0.980]
Epoch [80/120    avg_loss:0.009, val_acc:0.979]
Epoch [81/120    avg_loss:0.010, val_acc:0.978]
Epoch [82/120    avg_loss:0.012, val_acc:0.979]
Epoch [83/120    avg_loss:0.012, val_acc:0.977]
Epoch [84/120    avg_loss:0.010, val_acc:0.979]
Epoch [85/120    avg_loss:0.014, val_acc:0.979]
Epoch [86/120    avg_loss:0.007, val_acc:0.979]
Epoch [87/120    avg_loss:0.012, val_acc:0.979]
Epoch [88/120    avg_loss:0.009, val_acc:0.978]
Epoch [89/120    avg_loss:0.013, val_acc:0.980]
Epoch [90/120    avg_loss:0.009, val_acc:0.979]
Epoch [91/120    avg_loss:0.011, val_acc:0.977]
Epoch [92/120    avg_loss:0.009, val_acc:0.975]
Epoch [93/120    avg_loss:0.010, val_acc:0.976]
Epoch [94/120    avg_loss:0.009, val_acc:0.977]
Epoch [95/120    avg_loss:0.012, val_acc:0.977]
Epoch [96/120    avg_loss:0.009, val_acc:0.979]
Epoch [97/120    avg_loss:0.011, val_acc:0.976]
Epoch [98/120    avg_loss:0.008, val_acc:0.977]
Epoch [99/120    avg_loss:0.008, val_acc:0.978]
Epoch [100/120    avg_loss:0.008, val_acc:0.977]
Epoch [101/120    avg_loss:0.015, val_acc:0.976]
Epoch [102/120    avg_loss:0.008, val_acc:0.976]
Epoch [103/120    avg_loss:0.008, val_acc:0.976]
Epoch [104/120    avg_loss:0.006, val_acc:0.977]
Epoch [105/120    avg_loss:0.008, val_acc:0.977]
Epoch [106/120    avg_loss:0.007, val_acc:0.977]
Epoch [107/120    avg_loss:0.008, val_acc:0.977]
Epoch [108/120    avg_loss:0.007, val_acc:0.977]
Epoch [109/120    avg_loss:0.013, val_acc:0.978]
Epoch [110/120    avg_loss:0.008, val_acc:0.978]
Epoch [111/120    avg_loss:0.006, val_acc:0.978]
Epoch [112/120    avg_loss:0.007, val_acc:0.978]
Epoch [113/120    avg_loss:0.007, val_acc:0.978]
Epoch [114/120    avg_loss:0.007, val_acc:0.978]
Epoch [115/120    avg_loss:0.010, val_acc:0.978]
Epoch [116/120    avg_loss:0.007, val_acc:0.978]
Epoch [117/120    avg_loss:0.014, val_acc:0.978]
Epoch [118/120    avg_loss:0.010, val_acc:0.978]
Epoch [119/120    avg_loss:0.012, val_acc:0.978]
Epoch [120/120    avg_loss:0.009, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    1    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1238   14    3    0    0    0    0    3   13   10    2    0
     0    2    0]
 [   0    0    0  728    3    0    0    0    0    6    1    0    6    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    1  654    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   14    0    5    5    1    0    0    2  836   10    0    0
     0    2    0]
 [   0    0    3    1    0    0    1    0    0    0    2 2171   30    0
     1    1    0]
 [   0    0    0    0    0    2    0    0    0    0    1    0  528    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1124   12    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    37  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.64769647696477

F1 scores:
[       nan 0.94871795 0.97441952 0.97718121 0.97482838 0.98636364
 0.99241275 1.         0.99883586 0.76595745 0.96591566 0.98614581
 0.95825771 0.9919571  0.97654214 0.91180867 0.97647059]

Kappa:
0.9731988325504777
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:13:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f765d08f6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.110, val_acc:0.481]
Epoch [2/120    avg_loss:1.486, val_acc:0.673]
Epoch [3/120    avg_loss:1.219, val_acc:0.667]
Epoch [4/120    avg_loss:0.938, val_acc:0.660]
Epoch [5/120    avg_loss:0.820, val_acc:0.703]
Epoch [6/120    avg_loss:0.758, val_acc:0.831]
Epoch [7/120    avg_loss:0.697, val_acc:0.832]
Epoch [8/120    avg_loss:0.566, val_acc:0.848]
Epoch [9/120    avg_loss:0.443, val_acc:0.821]
Epoch [10/120    avg_loss:0.482, val_acc:0.867]
Epoch [11/120    avg_loss:0.361, val_acc:0.910]
Epoch [12/120    avg_loss:0.412, val_acc:0.869]
Epoch [13/120    avg_loss:0.283, val_acc:0.911]
Epoch [14/120    avg_loss:0.292, val_acc:0.886]
Epoch [15/120    avg_loss:0.214, val_acc:0.907]
Epoch [16/120    avg_loss:0.236, val_acc:0.935]
Epoch [17/120    avg_loss:0.222, val_acc:0.939]
Epoch [18/120    avg_loss:0.146, val_acc:0.925]
Epoch [19/120    avg_loss:0.154, val_acc:0.936]
Epoch [20/120    avg_loss:0.132, val_acc:0.902]
Epoch [21/120    avg_loss:0.218, val_acc:0.939]
Epoch [22/120    avg_loss:0.119, val_acc:0.949]
Epoch [23/120    avg_loss:0.099, val_acc:0.961]
Epoch [24/120    avg_loss:0.108, val_acc:0.939]
Epoch [25/120    avg_loss:0.088, val_acc:0.965]
Epoch [26/120    avg_loss:0.087, val_acc:0.958]
Epoch [27/120    avg_loss:0.085, val_acc:0.947]
Epoch [28/120    avg_loss:0.171, val_acc:0.953]
Epoch [29/120    avg_loss:0.065, val_acc:0.949]
Epoch [30/120    avg_loss:0.077, val_acc:0.959]
Epoch [31/120    avg_loss:0.061, val_acc:0.961]
Epoch [32/120    avg_loss:0.058, val_acc:0.970]
Epoch [33/120    avg_loss:0.061, val_acc:0.975]
Epoch [34/120    avg_loss:0.056, val_acc:0.945]
Epoch [35/120    avg_loss:0.061, val_acc:0.975]
Epoch [36/120    avg_loss:0.073, val_acc:0.952]
Epoch [37/120    avg_loss:0.053, val_acc:0.975]
Epoch [38/120    avg_loss:0.041, val_acc:0.979]
Epoch [39/120    avg_loss:0.036, val_acc:0.979]
Epoch [40/120    avg_loss:0.050, val_acc:0.972]
Epoch [41/120    avg_loss:0.019, val_acc:0.978]
Epoch [42/120    avg_loss:0.026, val_acc:0.977]
Epoch [43/120    avg_loss:0.152, val_acc:0.935]
Epoch [44/120    avg_loss:0.051, val_acc:0.957]
Epoch [45/120    avg_loss:0.029, val_acc:0.968]
Epoch [46/120    avg_loss:0.048, val_acc:0.960]
Epoch [47/120    avg_loss:0.074, val_acc:0.972]
Epoch [48/120    avg_loss:0.040, val_acc:0.977]
Epoch [49/120    avg_loss:0.046, val_acc:0.968]
Epoch [50/120    avg_loss:0.030, val_acc:0.982]
Epoch [51/120    avg_loss:0.016, val_acc:0.980]
Epoch [52/120    avg_loss:0.013, val_acc:0.984]
Epoch [53/120    avg_loss:0.011, val_acc:0.983]
Epoch [54/120    avg_loss:0.010, val_acc:0.986]
Epoch [55/120    avg_loss:0.013, val_acc:0.980]
Epoch [56/120    avg_loss:0.014, val_acc:0.980]
Epoch [57/120    avg_loss:0.009, val_acc:0.988]
Epoch [58/120    avg_loss:0.009, val_acc:0.991]
Epoch [59/120    avg_loss:0.011, val_acc:0.980]
Epoch [60/120    avg_loss:0.147, val_acc:0.945]
Epoch [61/120    avg_loss:0.061, val_acc:0.971]
Epoch [62/120    avg_loss:0.046, val_acc:0.959]
Epoch [63/120    avg_loss:0.033, val_acc:0.979]
Epoch [64/120    avg_loss:0.017, val_acc:0.986]
Epoch [65/120    avg_loss:0.010, val_acc:0.980]
Epoch [66/120    avg_loss:0.012, val_acc:0.984]
Epoch [67/120    avg_loss:0.022, val_acc:0.974]
Epoch [68/120    avg_loss:0.013, val_acc:0.986]
Epoch [69/120    avg_loss:0.010, val_acc:0.984]
Epoch [70/120    avg_loss:0.043, val_acc:0.954]
Epoch [71/120    avg_loss:0.052, val_acc:0.964]
Epoch [72/120    avg_loss:0.022, val_acc:0.979]
Epoch [73/120    avg_loss:0.013, val_acc:0.982]
Epoch [74/120    avg_loss:0.011, val_acc:0.986]
Epoch [75/120    avg_loss:0.006, val_acc:0.985]
Epoch [76/120    avg_loss:0.009, val_acc:0.985]
Epoch [77/120    avg_loss:0.008, val_acc:0.986]
Epoch [78/120    avg_loss:0.011, val_acc:0.986]
Epoch [79/120    avg_loss:0.008, val_acc:0.987]
Epoch [80/120    avg_loss:0.006, val_acc:0.986]
Epoch [81/120    avg_loss:0.007, val_acc:0.985]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.009, val_acc:0.987]
Epoch [84/120    avg_loss:0.007, val_acc:0.987]
Epoch [85/120    avg_loss:0.006, val_acc:0.987]
Epoch [86/120    avg_loss:0.008, val_acc:0.987]
Epoch [87/120    avg_loss:0.007, val_acc:0.987]
Epoch [88/120    avg_loss:0.007, val_acc:0.987]
Epoch [89/120    avg_loss:0.008, val_acc:0.987]
Epoch [90/120    avg_loss:0.006, val_acc:0.987]
Epoch [91/120    avg_loss:0.007, val_acc:0.987]
Epoch [92/120    avg_loss:0.004, val_acc:0.987]
Epoch [93/120    avg_loss:0.005, val_acc:0.987]
Epoch [94/120    avg_loss:0.006, val_acc:0.987]
Epoch [95/120    avg_loss:0.007, val_acc:0.987]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.007, val_acc:0.987]
Epoch [98/120    avg_loss:0.004, val_acc:0.987]
Epoch [99/120    avg_loss:0.007, val_acc:0.987]
Epoch [100/120    avg_loss:0.007, val_acc:0.987]
Epoch [101/120    avg_loss:0.005, val_acc:0.987]
Epoch [102/120    avg_loss:0.005, val_acc:0.987]
Epoch [103/120    avg_loss:0.006, val_acc:0.987]
Epoch [104/120    avg_loss:0.005, val_acc:0.987]
Epoch [105/120    avg_loss:0.006, val_acc:0.987]
Epoch [106/120    avg_loss:0.005, val_acc:0.987]
Epoch [107/120    avg_loss:0.006, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.987]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.013, val_acc:0.987]
Epoch [111/120    avg_loss:0.004, val_acc:0.987]
Epoch [112/120    avg_loss:0.005, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.010, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.987]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.006, val_acc:0.987]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.007, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1273    0    0    0    0    0    0    1    4    7    0    0
     0    0    0]
 [   0    0    0  719    0    0    0    0    0    3    0    1   24    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    5    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  856   14    0    0
     0    5    0]
 [   0    0   12    0    0    0    1    0    0    0    2 2170   24    1
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0    2    0  526    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1135    2    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    28  316    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.26558265582656

F1 scores:
[       nan 0.975      0.99027616 0.98023177 0.99764706 0.99305556
 0.99314547 0.98039216 0.99416569 0.87804878 0.98277842 0.98479691
 0.94604317 0.99730458 0.98438855 0.94328358 0.97109827]

Kappa:
0.980229162756479
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb7a57b6668>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.010, val_acc:0.485]
Epoch [2/120    avg_loss:1.449, val_acc:0.623]
Epoch [3/120    avg_loss:1.255, val_acc:0.661]
Epoch [4/120    avg_loss:1.010, val_acc:0.692]
Epoch [5/120    avg_loss:0.943, val_acc:0.762]
Epoch [6/120    avg_loss:0.695, val_acc:0.812]
Epoch [7/120    avg_loss:0.643, val_acc:0.803]
Epoch [8/120    avg_loss:0.563, val_acc:0.785]
Epoch [9/120    avg_loss:0.603, val_acc:0.782]
Epoch [10/120    avg_loss:0.489, val_acc:0.824]
Epoch [11/120    avg_loss:0.464, val_acc:0.835]
Epoch [12/120    avg_loss:0.401, val_acc:0.839]
Epoch [13/120    avg_loss:0.454, val_acc:0.839]
Epoch [14/120    avg_loss:0.319, val_acc:0.811]
Epoch [15/120    avg_loss:0.337, val_acc:0.894]
Epoch [16/120    avg_loss:0.302, val_acc:0.810]
Epoch [17/120    avg_loss:0.230, val_acc:0.856]
Epoch [18/120    avg_loss:0.307, val_acc:0.843]
Epoch [19/120    avg_loss:0.287, val_acc:0.892]
Epoch [20/120    avg_loss:0.232, val_acc:0.914]
Epoch [21/120    avg_loss:0.230, val_acc:0.933]
Epoch [22/120    avg_loss:0.171, val_acc:0.917]
Epoch [23/120    avg_loss:0.120, val_acc:0.941]
Epoch [24/120    avg_loss:0.191, val_acc:0.881]
Epoch [25/120    avg_loss:0.157, val_acc:0.925]
Epoch [26/120    avg_loss:0.163, val_acc:0.910]
Epoch [27/120    avg_loss:0.161, val_acc:0.940]
Epoch [28/120    avg_loss:0.144, val_acc:0.928]
Epoch [29/120    avg_loss:0.087, val_acc:0.964]
Epoch [30/120    avg_loss:0.097, val_acc:0.928]
Epoch [31/120    avg_loss:0.083, val_acc:0.940]
Epoch [32/120    avg_loss:0.085, val_acc:0.918]
Epoch [33/120    avg_loss:0.079, val_acc:0.944]
Epoch [34/120    avg_loss:0.055, val_acc:0.951]
Epoch [35/120    avg_loss:0.059, val_acc:0.945]
Epoch [36/120    avg_loss:0.064, val_acc:0.951]
Epoch [37/120    avg_loss:0.091, val_acc:0.944]
Epoch [38/120    avg_loss:0.070, val_acc:0.961]
Epoch [39/120    avg_loss:0.054, val_acc:0.957]
Epoch [40/120    avg_loss:0.136, val_acc:0.929]
Epoch [41/120    avg_loss:0.146, val_acc:0.941]
Epoch [42/120    avg_loss:0.093, val_acc:0.967]
Epoch [43/120    avg_loss:0.072, val_acc:0.956]
Epoch [44/120    avg_loss:0.055, val_acc:0.960]
Epoch [45/120    avg_loss:0.060, val_acc:0.961]
Epoch [46/120    avg_loss:0.050, val_acc:0.966]
Epoch [47/120    avg_loss:0.034, val_acc:0.972]
Epoch [48/120    avg_loss:0.030, val_acc:0.958]
Epoch [49/120    avg_loss:0.045, val_acc:0.968]
Epoch [50/120    avg_loss:0.020, val_acc:0.966]
Epoch [51/120    avg_loss:0.022, val_acc:0.960]
Epoch [52/120    avg_loss:0.016, val_acc:0.972]
Epoch [53/120    avg_loss:0.017, val_acc:0.975]
Epoch [54/120    avg_loss:0.049, val_acc:0.967]
Epoch [55/120    avg_loss:0.067, val_acc:0.946]
Epoch [56/120    avg_loss:0.032, val_acc:0.948]
Epoch [57/120    avg_loss:0.029, val_acc:0.980]
Epoch [58/120    avg_loss:0.023, val_acc:0.949]
Epoch [59/120    avg_loss:0.028, val_acc:0.959]
Epoch [60/120    avg_loss:0.036, val_acc:0.978]
Epoch [61/120    avg_loss:0.023, val_acc:0.953]
Epoch [62/120    avg_loss:0.042, val_acc:0.965]
Epoch [63/120    avg_loss:0.026, val_acc:0.975]
Epoch [64/120    avg_loss:0.043, val_acc:0.971]
Epoch [65/120    avg_loss:0.022, val_acc:0.977]
Epoch [66/120    avg_loss:0.012, val_acc:0.984]
Epoch [67/120    avg_loss:0.012, val_acc:0.980]
Epoch [68/120    avg_loss:0.010, val_acc:0.979]
Epoch [69/120    avg_loss:0.010, val_acc:0.982]
Epoch [70/120    avg_loss:0.012, val_acc:0.978]
Epoch [71/120    avg_loss:0.011, val_acc:0.983]
Epoch [72/120    avg_loss:0.009, val_acc:0.980]
Epoch [73/120    avg_loss:0.013, val_acc:0.977]
Epoch [74/120    avg_loss:0.045, val_acc:0.964]
Epoch [75/120    avg_loss:0.046, val_acc:0.973]
Epoch [76/120    avg_loss:0.013, val_acc:0.976]
Epoch [77/120    avg_loss:0.018, val_acc:0.971]
Epoch [78/120    avg_loss:0.034, val_acc:0.981]
Epoch [79/120    avg_loss:0.036, val_acc:0.965]
Epoch [80/120    avg_loss:0.026, val_acc:0.981]
Epoch [81/120    avg_loss:0.009, val_acc:0.982]
Epoch [82/120    avg_loss:0.007, val_acc:0.982]
Epoch [83/120    avg_loss:0.008, val_acc:0.984]
Epoch [84/120    avg_loss:0.010, val_acc:0.983]
Epoch [85/120    avg_loss:0.009, val_acc:0.981]
Epoch [86/120    avg_loss:0.013, val_acc:0.983]
Epoch [87/120    avg_loss:0.010, val_acc:0.985]
Epoch [88/120    avg_loss:0.006, val_acc:0.985]
Epoch [89/120    avg_loss:0.006, val_acc:0.985]
Epoch [90/120    avg_loss:0.005, val_acc:0.985]
Epoch [91/120    avg_loss:0.006, val_acc:0.985]
Epoch [92/120    avg_loss:0.005, val_acc:0.984]
Epoch [93/120    avg_loss:0.005, val_acc:0.982]
Epoch [94/120    avg_loss:0.005, val_acc:0.984]
Epoch [95/120    avg_loss:0.005, val_acc:0.983]
Epoch [96/120    avg_loss:0.007, val_acc:0.980]
Epoch [97/120    avg_loss:0.006, val_acc:0.982]
Epoch [98/120    avg_loss:0.006, val_acc:0.982]
Epoch [99/120    avg_loss:0.005, val_acc:0.981]
Epoch [100/120    avg_loss:0.006, val_acc:0.984]
Epoch [101/120    avg_loss:0.005, val_acc:0.983]
Epoch [102/120    avg_loss:0.006, val_acc:0.981]
Epoch [103/120    avg_loss:0.005, val_acc:0.982]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.005, val_acc:0.984]
Epoch [106/120    avg_loss:0.005, val_acc:0.984]
Epoch [107/120    avg_loss:0.005, val_acc:0.984]
Epoch [108/120    avg_loss:0.006, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.005, val_acc:0.984]
Epoch [111/120    avg_loss:0.005, val_acc:0.984]
Epoch [112/120    avg_loss:0.005, val_acc:0.984]
Epoch [113/120    avg_loss:0.005, val_acc:0.984]
Epoch [114/120    avg_loss:0.005, val_acc:0.983]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.006, val_acc:0.983]
Epoch [117/120    avg_loss:0.005, val_acc:0.983]
Epoch [118/120    avg_loss:0.005, val_acc:0.983]
Epoch [119/120    avg_loss:0.006, val_acc:0.983]
Epoch [120/120    avg_loss:0.005, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   36    0    0    0    0    0    0    0    0    5    0    0    0
     0    0    0]
 [   0    0 1260    4    0    0    0    0    0    0   12    9    0    0
     0    0    0]
 [   0    0    0  739    0    1    0    0    0    2    1    0    2    1
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0  866    4    0    0
     0    4    0]
 [   0    0    6    0    0    0    1    0    0    0    5 2180   18    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0    0  530    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    70  267    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.08130081300813

F1 scores:
[       nan 0.93506494 0.9878479  0.99061662 0.99764706 0.99654776
 0.98942598 1.         0.997669   0.92307692 0.98185941 0.98978434
 0.97605893 0.99730458 0.9653994  0.85031847 0.98823529]

Kappa:
0.9781201269184729
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc5029736d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.111, val_acc:0.442]
Epoch [2/120    avg_loss:1.472, val_acc:0.579]
Epoch [3/120    avg_loss:1.180, val_acc:0.672]
Epoch [4/120    avg_loss:0.993, val_acc:0.685]
Epoch [5/120    avg_loss:0.828, val_acc:0.744]
Epoch [6/120    avg_loss:0.770, val_acc:0.749]
Epoch [7/120    avg_loss:0.749, val_acc:0.789]
Epoch [8/120    avg_loss:0.592, val_acc:0.791]
Epoch [9/120    avg_loss:0.527, val_acc:0.798]
Epoch [10/120    avg_loss:0.475, val_acc:0.815]
Epoch [11/120    avg_loss:0.656, val_acc:0.831]
Epoch [12/120    avg_loss:0.386, val_acc:0.860]
Epoch [13/120    avg_loss:0.354, val_acc:0.855]
Epoch [14/120    avg_loss:0.389, val_acc:0.852]
Epoch [15/120    avg_loss:0.387, val_acc:0.856]
Epoch [16/120    avg_loss:0.298, val_acc:0.860]
Epoch [17/120    avg_loss:0.280, val_acc:0.919]
Epoch [18/120    avg_loss:0.272, val_acc:0.869]
Epoch [19/120    avg_loss:0.334, val_acc:0.910]
Epoch [20/120    avg_loss:0.225, val_acc:0.910]
Epoch [21/120    avg_loss:0.279, val_acc:0.890]
Epoch [22/120    avg_loss:0.211, val_acc:0.926]
Epoch [23/120    avg_loss:0.232, val_acc:0.914]
Epoch [24/120    avg_loss:0.199, val_acc:0.942]
Epoch [25/120    avg_loss:0.141, val_acc:0.924]
Epoch [26/120    avg_loss:0.138, val_acc:0.932]
Epoch [27/120    avg_loss:0.211, val_acc:0.920]
Epoch [28/120    avg_loss:0.214, val_acc:0.924]
Epoch [29/120    avg_loss:0.164, val_acc:0.923]
Epoch [30/120    avg_loss:0.140, val_acc:0.949]
Epoch [31/120    avg_loss:0.111, val_acc:0.944]
Epoch [32/120    avg_loss:0.101, val_acc:0.947]
Epoch [33/120    avg_loss:0.087, val_acc:0.952]
Epoch [34/120    avg_loss:0.132, val_acc:0.930]
Epoch [35/120    avg_loss:0.103, val_acc:0.943]
Epoch [36/120    avg_loss:0.126, val_acc:0.943]
Epoch [37/120    avg_loss:0.097, val_acc:0.942]
Epoch [38/120    avg_loss:0.072, val_acc:0.966]
Epoch [39/120    avg_loss:0.053, val_acc:0.932]
Epoch [40/120    avg_loss:0.068, val_acc:0.961]
Epoch [41/120    avg_loss:0.066, val_acc:0.955]
Epoch [42/120    avg_loss:0.057, val_acc:0.962]
Epoch [43/120    avg_loss:0.046, val_acc:0.954]
Epoch [44/120    avg_loss:0.066, val_acc:0.958]
Epoch [45/120    avg_loss:0.070, val_acc:0.973]
Epoch [46/120    avg_loss:0.048, val_acc:0.955]
Epoch [47/120    avg_loss:0.082, val_acc:0.951]
Epoch [48/120    avg_loss:0.070, val_acc:0.962]
Epoch [49/120    avg_loss:0.043, val_acc:0.967]
Epoch [50/120    avg_loss:0.036, val_acc:0.975]
Epoch [51/120    avg_loss:0.037, val_acc:0.976]
Epoch [52/120    avg_loss:0.034, val_acc:0.964]
Epoch [53/120    avg_loss:0.056, val_acc:0.974]
Epoch [54/120    avg_loss:0.042, val_acc:0.926]
Epoch [55/120    avg_loss:0.117, val_acc:0.947]
Epoch [56/120    avg_loss:0.095, val_acc:0.956]
Epoch [57/120    avg_loss:0.065, val_acc:0.971]
Epoch [58/120    avg_loss:0.049, val_acc:0.964]
Epoch [59/120    avg_loss:0.038, val_acc:0.977]
Epoch [60/120    avg_loss:0.027, val_acc:0.940]
Epoch [61/120    avg_loss:0.029, val_acc:0.978]
Epoch [62/120    avg_loss:0.021, val_acc:0.973]
Epoch [63/120    avg_loss:0.030, val_acc:0.979]
Epoch [64/120    avg_loss:0.019, val_acc:0.977]
Epoch [65/120    avg_loss:0.024, val_acc:0.961]
Epoch [66/120    avg_loss:0.028, val_acc:0.983]
Epoch [67/120    avg_loss:0.013, val_acc:0.982]
Epoch [68/120    avg_loss:0.016, val_acc:0.971]
Epoch [69/120    avg_loss:0.018, val_acc:0.981]
Epoch [70/120    avg_loss:0.018, val_acc:0.980]
Epoch [71/120    avg_loss:0.019, val_acc:0.975]
Epoch [72/120    avg_loss:0.018, val_acc:0.982]
Epoch [73/120    avg_loss:0.019, val_acc:0.979]
Epoch [74/120    avg_loss:0.010, val_acc:0.976]
Epoch [75/120    avg_loss:0.007, val_acc:0.982]
Epoch [76/120    avg_loss:0.008, val_acc:0.984]
Epoch [77/120    avg_loss:0.008, val_acc:0.979]
Epoch [78/120    avg_loss:0.011, val_acc:0.983]
Epoch [79/120    avg_loss:0.037, val_acc:0.965]
Epoch [80/120    avg_loss:0.084, val_acc:0.947]
Epoch [81/120    avg_loss:0.056, val_acc:0.955]
Epoch [82/120    avg_loss:0.035, val_acc:0.965]
Epoch [83/120    avg_loss:0.029, val_acc:0.959]
Epoch [84/120    avg_loss:0.032, val_acc:0.966]
Epoch [85/120    avg_loss:0.024, val_acc:0.974]
Epoch [86/120    avg_loss:0.019, val_acc:0.973]
Epoch [87/120    avg_loss:0.019, val_acc:0.979]
Epoch [88/120    avg_loss:0.054, val_acc:0.966]
Epoch [89/120    avg_loss:0.088, val_acc:0.972]
Epoch [90/120    avg_loss:0.020, val_acc:0.979]
Epoch [91/120    avg_loss:0.020, val_acc:0.983]
Epoch [92/120    avg_loss:0.019, val_acc:0.984]
Epoch [93/120    avg_loss:0.014, val_acc:0.982]
Epoch [94/120    avg_loss:0.012, val_acc:0.982]
Epoch [95/120    avg_loss:0.011, val_acc:0.982]
Epoch [96/120    avg_loss:0.010, val_acc:0.982]
Epoch [97/120    avg_loss:0.012, val_acc:0.984]
Epoch [98/120    avg_loss:0.011, val_acc:0.983]
Epoch [99/120    avg_loss:0.012, val_acc:0.981]
Epoch [100/120    avg_loss:0.010, val_acc:0.983]
Epoch [101/120    avg_loss:0.010, val_acc:0.983]
Epoch [102/120    avg_loss:0.011, val_acc:0.983]
Epoch [103/120    avg_loss:0.009, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.982]
Epoch [105/120    avg_loss:0.008, val_acc:0.983]
Epoch [106/120    avg_loss:0.008, val_acc:0.983]
Epoch [107/120    avg_loss:0.007, val_acc:0.983]
Epoch [108/120    avg_loss:0.009, val_acc:0.983]
Epoch [109/120    avg_loss:0.011, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.986]
Epoch [111/120    avg_loss:0.007, val_acc:0.985]
Epoch [112/120    avg_loss:0.007, val_acc:0.985]
Epoch [113/120    avg_loss:0.006, val_acc:0.987]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.013, val_acc:0.986]
Epoch [116/120    avg_loss:0.009, val_acc:0.987]
Epoch [117/120    avg_loss:0.007, val_acc:0.985]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.010, val_acc:0.985]
Epoch [120/120    avg_loss:0.009, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1265    0    0    0    2    0    0    0   10    4    0    0
     0    4    0]
 [   0    0    0  709    0   11    0    0    0    3    3    0   18    2
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    7    0    0    0    0  847    3    5    0
     1    7    0]
 [   0    0    5    0    0    0    2    1    0    0    4 2198    0    0
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0  528    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    1    0    1    0    0    0
  1133    2    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
    40  290    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.14634146341463

F1 scores:
[       nan 1.         0.98828125 0.9739011  1.         0.97297297
 0.98426966 0.96153846 0.99883856 0.9        0.97356322 0.99569649
 0.97237569 0.99462366 0.97883369 0.88820827 0.98224852]

Kappa:
0.9788668573632003
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f54944bf710>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.160, val_acc:0.428]
Epoch [2/120    avg_loss:1.539, val_acc:0.675]
Epoch [3/120    avg_loss:1.293, val_acc:0.724]
Epoch [4/120    avg_loss:1.029, val_acc:0.768]
Epoch [5/120    avg_loss:0.845, val_acc:0.718]
Epoch [6/120    avg_loss:0.798, val_acc:0.785]
Epoch [7/120    avg_loss:0.701, val_acc:0.824]
Epoch [8/120    avg_loss:0.609, val_acc:0.816]
Epoch [9/120    avg_loss:0.538, val_acc:0.834]
Epoch [10/120    avg_loss:0.536, val_acc:0.829]
Epoch [11/120    avg_loss:0.447, val_acc:0.885]
Epoch [12/120    avg_loss:0.465, val_acc:0.872]
Epoch [13/120    avg_loss:0.448, val_acc:0.884]
Epoch [14/120    avg_loss:0.441, val_acc:0.903]
Epoch [15/120    avg_loss:0.321, val_acc:0.892]
Epoch [16/120    avg_loss:0.232, val_acc:0.904]
Epoch [17/120    avg_loss:0.232, val_acc:0.926]
Epoch [18/120    avg_loss:0.240, val_acc:0.920]
Epoch [19/120    avg_loss:0.285, val_acc:0.920]
Epoch [20/120    avg_loss:0.149, val_acc:0.928]
Epoch [21/120    avg_loss:0.198, val_acc:0.891]
Epoch [22/120    avg_loss:0.178, val_acc:0.949]
Epoch [23/120    avg_loss:0.131, val_acc:0.924]
Epoch [24/120    avg_loss:0.130, val_acc:0.955]
Epoch [25/120    avg_loss:0.149, val_acc:0.890]
Epoch [26/120    avg_loss:0.143, val_acc:0.949]
Epoch [27/120    avg_loss:0.122, val_acc:0.941]
Epoch [28/120    avg_loss:0.099, val_acc:0.928]
Epoch [29/120    avg_loss:0.085, val_acc:0.952]
Epoch [30/120    avg_loss:0.059, val_acc:0.965]
Epoch [31/120    avg_loss:0.077, val_acc:0.955]
Epoch [32/120    avg_loss:0.107, val_acc:0.941]
Epoch [33/120    avg_loss:0.160, val_acc:0.943]
Epoch [34/120    avg_loss:0.062, val_acc:0.965]
Epoch [35/120    avg_loss:0.081, val_acc:0.946]
Epoch [36/120    avg_loss:0.086, val_acc:0.964]
Epoch [37/120    avg_loss:0.059, val_acc:0.967]
Epoch [38/120    avg_loss:0.046, val_acc:0.963]
Epoch [39/120    avg_loss:0.040, val_acc:0.962]
Epoch [40/120    avg_loss:0.061, val_acc:0.950]
Epoch [41/120    avg_loss:0.082, val_acc:0.962]
Epoch [42/120    avg_loss:0.056, val_acc:0.944]
Epoch [43/120    avg_loss:0.087, val_acc:0.967]
Epoch [44/120    avg_loss:0.139, val_acc:0.952]
Epoch [45/120    avg_loss:0.109, val_acc:0.960]
Epoch [46/120    avg_loss:0.077, val_acc:0.942]
Epoch [47/120    avg_loss:0.051, val_acc:0.974]
Epoch [48/120    avg_loss:0.028, val_acc:0.956]
Epoch [49/120    avg_loss:0.047, val_acc:0.981]
Epoch [50/120    avg_loss:0.052, val_acc:0.974]
Epoch [51/120    avg_loss:0.035, val_acc:0.971]
Epoch [52/120    avg_loss:0.027, val_acc:0.978]
Epoch [53/120    avg_loss:0.022, val_acc:0.984]
Epoch [54/120    avg_loss:0.017, val_acc:0.981]
Epoch [55/120    avg_loss:0.075, val_acc:0.980]
Epoch [56/120    avg_loss:0.049, val_acc:0.983]
Epoch [57/120    avg_loss:0.047, val_acc:0.976]
Epoch [58/120    avg_loss:0.041, val_acc:0.966]
Epoch [59/120    avg_loss:0.036, val_acc:0.974]
Epoch [60/120    avg_loss:0.016, val_acc:0.986]
Epoch [61/120    avg_loss:0.013, val_acc:0.985]
Epoch [62/120    avg_loss:0.010, val_acc:0.984]
Epoch [63/120    avg_loss:0.033, val_acc:0.985]
Epoch [64/120    avg_loss:0.036, val_acc:0.920]
Epoch [65/120    avg_loss:0.036, val_acc:0.982]
Epoch [66/120    avg_loss:0.021, val_acc:0.982]
Epoch [67/120    avg_loss:0.025, val_acc:0.979]
Epoch [68/120    avg_loss:0.017, val_acc:0.981]
Epoch [69/120    avg_loss:0.011, val_acc:0.990]
Epoch [70/120    avg_loss:0.008, val_acc:0.987]
Epoch [71/120    avg_loss:0.006, val_acc:0.989]
Epoch [72/120    avg_loss:0.007, val_acc:0.991]
Epoch [73/120    avg_loss:0.010, val_acc:0.987]
Epoch [74/120    avg_loss:0.012, val_acc:0.985]
Epoch [75/120    avg_loss:0.007, val_acc:0.991]
Epoch [76/120    avg_loss:0.036, val_acc:0.984]
Epoch [77/120    avg_loss:0.021, val_acc:0.957]
Epoch [78/120    avg_loss:0.017, val_acc:0.984]
Epoch [79/120    avg_loss:0.008, val_acc:0.988]
Epoch [80/120    avg_loss:0.009, val_acc:0.989]
Epoch [81/120    avg_loss:0.007, val_acc:0.988]
Epoch [82/120    avg_loss:0.005, val_acc:0.988]
Epoch [83/120    avg_loss:0.004, val_acc:0.990]
Epoch [84/120    avg_loss:0.011, val_acc:0.988]
Epoch [85/120    avg_loss:0.008, val_acc:0.988]
Epoch [86/120    avg_loss:0.005, val_acc:0.989]
Epoch [87/120    avg_loss:0.004, val_acc:0.988]
Epoch [88/120    avg_loss:0.004, val_acc:0.987]
Epoch [89/120    avg_loss:0.004, val_acc:0.986]
Epoch [90/120    avg_loss:0.004, val_acc:0.986]
Epoch [91/120    avg_loss:0.007, val_acc:0.987]
Epoch [92/120    avg_loss:0.004, val_acc:0.990]
Epoch [93/120    avg_loss:0.004, val_acc:0.989]
Epoch [94/120    avg_loss:0.004, val_acc:0.988]
Epoch [95/120    avg_loss:0.003, val_acc:0.989]
Epoch [96/120    avg_loss:0.003, val_acc:0.987]
Epoch [97/120    avg_loss:0.004, val_acc:0.987]
Epoch [98/120    avg_loss:0.002, val_acc:0.987]
Epoch [99/120    avg_loss:0.003, val_acc:0.988]
Epoch [100/120    avg_loss:0.004, val_acc:0.988]
Epoch [101/120    avg_loss:0.003, val_acc:0.989]
Epoch [102/120    avg_loss:0.005, val_acc:0.989]
Epoch [103/120    avg_loss:0.003, val_acc:0.989]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.003, val_acc:0.989]
Epoch [106/120    avg_loss:0.011, val_acc:0.989]
Epoch [107/120    avg_loss:0.004, val_acc:0.989]
Epoch [108/120    avg_loss:0.003, val_acc:0.989]
Epoch [109/120    avg_loss:0.003, val_acc:0.989]
Epoch [110/120    avg_loss:0.003, val_acc:0.989]
Epoch [111/120    avg_loss:0.004, val_acc:0.990]
Epoch [112/120    avg_loss:0.003, val_acc:0.990]
Epoch [113/120    avg_loss:0.004, val_acc:0.991]
Epoch [114/120    avg_loss:0.003, val_acc:0.991]
Epoch [115/120    avg_loss:0.004, val_acc:0.991]
Epoch [116/120    avg_loss:0.003, val_acc:0.991]
Epoch [117/120    avg_loss:0.003, val_acc:0.991]
Epoch [118/120    avg_loss:0.003, val_acc:0.991]
Epoch [119/120    avg_loss:0.003, val_acc:0.991]
Epoch [120/120    avg_loss:0.002, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1268    0    0    2    0    0    0    0    6    8    1    0
     0    0    0]
 [   0    0    0  715    0    9    0    0    0    4    1    0   17    1
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    3    5    0    0    0  855    7    1    0
     2    1    0]
 [   0    0    2    0    0    0    2    0    0    0    3 2202    0    0
     1    0    0]
 [   0    0    0    0    2    3    0    0    0    0    2    0  521    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    0    0    0
  1132    4    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    19  328    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.67750677506776

F1 scores:
[       nan 0.98765432 0.99217527 0.97677596 0.99061033 0.97853107
 0.99393939 1.         0.99883856 0.85714286 0.97994269 0.99457995
 0.97020484 0.99730458 0.98735281 0.96470588 0.96551724]

Kappa:
0.9849223786853007
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0de7b416d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.097, val_acc:0.532]
Epoch [2/120    avg_loss:1.480, val_acc:0.638]
Epoch [3/120    avg_loss:1.218, val_acc:0.662]
Epoch [4/120    avg_loss:1.046, val_acc:0.713]
Epoch [5/120    avg_loss:0.817, val_acc:0.739]
Epoch [6/120    avg_loss:0.748, val_acc:0.778]
Epoch [7/120    avg_loss:0.668, val_acc:0.807]
Epoch [8/120    avg_loss:0.612, val_acc:0.759]
Epoch [9/120    avg_loss:0.539, val_acc:0.801]
Epoch [10/120    avg_loss:0.494, val_acc:0.843]
Epoch [11/120    avg_loss:0.465, val_acc:0.815]
Epoch [12/120    avg_loss:0.330, val_acc:0.803]
Epoch [13/120    avg_loss:0.304, val_acc:0.834]
Epoch [14/120    avg_loss:0.306, val_acc:0.871]
Epoch [15/120    avg_loss:0.330, val_acc:0.872]
Epoch [16/120    avg_loss:0.222, val_acc:0.890]
Epoch [17/120    avg_loss:0.356, val_acc:0.830]
Epoch [18/120    avg_loss:0.475, val_acc:0.902]
Epoch [19/120    avg_loss:0.289, val_acc:0.904]
Epoch [20/120    avg_loss:0.222, val_acc:0.904]
Epoch [21/120    avg_loss:0.206, val_acc:0.905]
Epoch [22/120    avg_loss:0.210, val_acc:0.925]
Epoch [23/120    avg_loss:0.147, val_acc:0.928]
Epoch [24/120    avg_loss:0.128, val_acc:0.945]
Epoch [25/120    avg_loss:0.177, val_acc:0.904]
Epoch [26/120    avg_loss:0.139, val_acc:0.922]
Epoch [27/120    avg_loss:0.099, val_acc:0.940]
Epoch [28/120    avg_loss:0.095, val_acc:0.937]
Epoch [29/120    avg_loss:0.109, val_acc:0.941]
Epoch [30/120    avg_loss:0.128, val_acc:0.953]
Epoch [31/120    avg_loss:0.083, val_acc:0.904]
Epoch [32/120    avg_loss:0.120, val_acc:0.916]
Epoch [33/120    avg_loss:0.110, val_acc:0.949]
Epoch [34/120    avg_loss:0.142, val_acc:0.880]
Epoch [35/120    avg_loss:0.083, val_acc:0.943]
Epoch [36/120    avg_loss:0.103, val_acc:0.928]
Epoch [37/120    avg_loss:0.089, val_acc:0.949]
Epoch [38/120    avg_loss:0.064, val_acc:0.937]
Epoch [39/120    avg_loss:0.092, val_acc:0.934]
Epoch [40/120    avg_loss:0.048, val_acc:0.961]
Epoch [41/120    avg_loss:0.060, val_acc:0.939]
Epoch [42/120    avg_loss:0.042, val_acc:0.958]
Epoch [43/120    avg_loss:0.067, val_acc:0.946]
Epoch [44/120    avg_loss:0.048, val_acc:0.938]
Epoch [45/120    avg_loss:0.059, val_acc:0.954]
Epoch [46/120    avg_loss:0.037, val_acc:0.974]
Epoch [47/120    avg_loss:0.027, val_acc:0.968]
Epoch [48/120    avg_loss:0.036, val_acc:0.964]
Epoch [49/120    avg_loss:0.038, val_acc:0.972]
Epoch [50/120    avg_loss:0.120, val_acc:0.872]
Epoch [51/120    avg_loss:0.111, val_acc:0.958]
Epoch [52/120    avg_loss:0.064, val_acc:0.953]
Epoch [53/120    avg_loss:0.047, val_acc:0.963]
Epoch [54/120    avg_loss:0.048, val_acc:0.960]
Epoch [55/120    avg_loss:0.044, val_acc:0.968]
Epoch [56/120    avg_loss:0.042, val_acc:0.971]
Epoch [57/120    avg_loss:0.065, val_acc:0.835]
Epoch [58/120    avg_loss:0.113, val_acc:0.945]
Epoch [59/120    avg_loss:0.043, val_acc:0.962]
Epoch [60/120    avg_loss:0.025, val_acc:0.968]
Epoch [61/120    avg_loss:0.025, val_acc:0.975]
Epoch [62/120    avg_loss:0.022, val_acc:0.973]
Epoch [63/120    avg_loss:0.019, val_acc:0.976]
Epoch [64/120    avg_loss:0.019, val_acc:0.975]
Epoch [65/120    avg_loss:0.016, val_acc:0.975]
Epoch [66/120    avg_loss:0.017, val_acc:0.978]
Epoch [67/120    avg_loss:0.013, val_acc:0.976]
Epoch [68/120    avg_loss:0.019, val_acc:0.977]
Epoch [69/120    avg_loss:0.015, val_acc:0.976]
Epoch [70/120    avg_loss:0.014, val_acc:0.975]
Epoch [71/120    avg_loss:0.014, val_acc:0.977]
Epoch [72/120    avg_loss:0.018, val_acc:0.975]
Epoch [73/120    avg_loss:0.012, val_acc:0.975]
Epoch [74/120    avg_loss:0.013, val_acc:0.976]
Epoch [75/120    avg_loss:0.013, val_acc:0.976]
Epoch [76/120    avg_loss:0.015, val_acc:0.979]
Epoch [77/120    avg_loss:0.011, val_acc:0.978]
Epoch [78/120    avg_loss:0.012, val_acc:0.977]
Epoch [79/120    avg_loss:0.013, val_acc:0.977]
Epoch [80/120    avg_loss:0.010, val_acc:0.977]
Epoch [81/120    avg_loss:0.013, val_acc:0.978]
Epoch [82/120    avg_loss:0.011, val_acc:0.977]
Epoch [83/120    avg_loss:0.013, val_acc:0.977]
Epoch [84/120    avg_loss:0.014, val_acc:0.977]
Epoch [85/120    avg_loss:0.009, val_acc:0.978]
Epoch [86/120    avg_loss:0.011, val_acc:0.977]
Epoch [87/120    avg_loss:0.010, val_acc:0.977]
Epoch [88/120    avg_loss:0.014, val_acc:0.974]
Epoch [89/120    avg_loss:0.008, val_acc:0.977]
Epoch [90/120    avg_loss:0.009, val_acc:0.977]
Epoch [91/120    avg_loss:0.015, val_acc:0.977]
Epoch [92/120    avg_loss:0.011, val_acc:0.978]
Epoch [93/120    avg_loss:0.012, val_acc:0.978]
Epoch [94/120    avg_loss:0.011, val_acc:0.978]
Epoch [95/120    avg_loss:0.018, val_acc:0.978]
Epoch [96/120    avg_loss:0.013, val_acc:0.978]
Epoch [97/120    avg_loss:0.012, val_acc:0.978]
Epoch [98/120    avg_loss:0.014, val_acc:0.978]
Epoch [99/120    avg_loss:0.009, val_acc:0.978]
Epoch [100/120    avg_loss:0.010, val_acc:0.978]
Epoch [101/120    avg_loss:0.010, val_acc:0.978]
Epoch [102/120    avg_loss:0.011, val_acc:0.978]
Epoch [103/120    avg_loss:0.010, val_acc:0.978]
Epoch [104/120    avg_loss:0.012, val_acc:0.978]
Epoch [105/120    avg_loss:0.008, val_acc:0.978]
Epoch [106/120    avg_loss:0.008, val_acc:0.978]
Epoch [107/120    avg_loss:0.011, val_acc:0.978]
Epoch [108/120    avg_loss:0.010, val_acc:0.978]
Epoch [109/120    avg_loss:0.012, val_acc:0.978]
Epoch [110/120    avg_loss:0.011, val_acc:0.978]
Epoch [111/120    avg_loss:0.011, val_acc:0.978]
Epoch [112/120    avg_loss:0.012, val_acc:0.978]
Epoch [113/120    avg_loss:0.008, val_acc:0.978]
Epoch [114/120    avg_loss:0.011, val_acc:0.978]
Epoch [115/120    avg_loss:0.012, val_acc:0.978]
Epoch [116/120    avg_loss:0.011, val_acc:0.978]
Epoch [117/120    avg_loss:0.010, val_acc:0.978]
Epoch [118/120    avg_loss:0.010, val_acc:0.978]
Epoch [119/120    avg_loss:0.011, val_acc:0.978]
Epoch [120/120    avg_loss:0.009, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1261    1    0    2    0    0    0    0    6    7    2    0
     0    6    0]
 [   0    0    0  713    0   12    0    0    0    5    0    0   12    5
     0    0    0]
 [   0    0    0    4  209    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    6    2    0    0    0  849    4    9    0
     0    4    0]
 [   0    0    8    0    0    0    4    0    0    0    1 2195    0    2
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0  530    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    1    0    0    0
  1124    7    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    15  316    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.28726287262873

F1 scores:
[       nan 0.975      0.98669797 0.97337884 0.99052133 0.96651786
 0.98277154 0.98039216 1.         0.85714286 0.97980381 0.99388725
 0.97516099 0.98143236 0.98683055 0.92941176 0.98245614]

Kappa:
0.9804810909237535
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5dc05f86d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.135, val_acc:0.478]
Epoch [2/120    avg_loss:1.609, val_acc:0.653]
Epoch [3/120    avg_loss:1.169, val_acc:0.683]
Epoch [4/120    avg_loss:0.928, val_acc:0.763]
Epoch [5/120    avg_loss:0.841, val_acc:0.631]
Epoch [6/120    avg_loss:0.821, val_acc:0.796]
Epoch [7/120    avg_loss:0.643, val_acc:0.794]
Epoch [8/120    avg_loss:0.617, val_acc:0.824]
Epoch [9/120    avg_loss:0.571, val_acc:0.843]
Epoch [10/120    avg_loss:0.512, val_acc:0.838]
Epoch [11/120    avg_loss:0.353, val_acc:0.894]
Epoch [12/120    avg_loss:0.359, val_acc:0.892]
Epoch [13/120    avg_loss:0.299, val_acc:0.899]
Epoch [14/120    avg_loss:0.307, val_acc:0.848]
Epoch [15/120    avg_loss:0.313, val_acc:0.877]
Epoch [16/120    avg_loss:0.442, val_acc:0.880]
Epoch [17/120    avg_loss:0.300, val_acc:0.907]
Epoch [18/120    avg_loss:0.324, val_acc:0.906]
Epoch [19/120    avg_loss:0.244, val_acc:0.913]
Epoch [20/120    avg_loss:0.263, val_acc:0.913]
Epoch [21/120    avg_loss:0.231, val_acc:0.936]
Epoch [22/120    avg_loss:0.215, val_acc:0.922]
Epoch [23/120    avg_loss:0.154, val_acc:0.932]
Epoch [24/120    avg_loss:0.130, val_acc:0.906]
Epoch [25/120    avg_loss:0.227, val_acc:0.943]
Epoch [26/120    avg_loss:0.107, val_acc:0.946]
Epoch [27/120    avg_loss:0.138, val_acc:0.921]
Epoch [28/120    avg_loss:0.127, val_acc:0.920]
Epoch [29/120    avg_loss:0.081, val_acc:0.939]
Epoch [30/120    avg_loss:0.103, val_acc:0.921]
Epoch [31/120    avg_loss:0.108, val_acc:0.946]
Epoch [32/120    avg_loss:0.136, val_acc:0.934]
Epoch [33/120    avg_loss:0.205, val_acc:0.937]
Epoch [34/120    avg_loss:0.135, val_acc:0.943]
Epoch [35/120    avg_loss:0.079, val_acc:0.957]
Epoch [36/120    avg_loss:0.100, val_acc:0.926]
Epoch [37/120    avg_loss:0.081, val_acc:0.960]
Epoch [38/120    avg_loss:0.070, val_acc:0.964]
Epoch [39/120    avg_loss:0.072, val_acc:0.965]
Epoch [40/120    avg_loss:0.070, val_acc:0.956]
Epoch [41/120    avg_loss:0.106, val_acc:0.950]
Epoch [42/120    avg_loss:0.139, val_acc:0.906]
Epoch [43/120    avg_loss:0.086, val_acc:0.950]
Epoch [44/120    avg_loss:0.070, val_acc:0.968]
Epoch [45/120    avg_loss:0.045, val_acc:0.970]
Epoch [46/120    avg_loss:0.065, val_acc:0.954]
Epoch [47/120    avg_loss:0.062, val_acc:0.978]
Epoch [48/120    avg_loss:0.033, val_acc:0.975]
Epoch [49/120    avg_loss:0.040, val_acc:0.971]
Epoch [50/120    avg_loss:0.185, val_acc:0.906]
Epoch [51/120    avg_loss:0.133, val_acc:0.943]
Epoch [52/120    avg_loss:0.057, val_acc:0.966]
Epoch [53/120    avg_loss:0.061, val_acc:0.968]
Epoch [54/120    avg_loss:0.051, val_acc:0.952]
Epoch [55/120    avg_loss:0.046, val_acc:0.971]
Epoch [56/120    avg_loss:0.034, val_acc:0.971]
Epoch [57/120    avg_loss:0.036, val_acc:0.974]
Epoch [58/120    avg_loss:0.036, val_acc:0.965]
Epoch [59/120    avg_loss:0.126, val_acc:0.953]
Epoch [60/120    avg_loss:0.054, val_acc:0.975]
Epoch [61/120    avg_loss:0.037, val_acc:0.981]
Epoch [62/120    avg_loss:0.027, val_acc:0.980]
Epoch [63/120    avg_loss:0.019, val_acc:0.980]
Epoch [64/120    avg_loss:0.031, val_acc:0.979]
Epoch [65/120    avg_loss:0.015, val_acc:0.979]
Epoch [66/120    avg_loss:0.022, val_acc:0.981]
Epoch [67/120    avg_loss:0.018, val_acc:0.980]
Epoch [68/120    avg_loss:0.019, val_acc:0.981]
Epoch [69/120    avg_loss:0.020, val_acc:0.980]
Epoch [70/120    avg_loss:0.013, val_acc:0.979]
Epoch [71/120    avg_loss:0.017, val_acc:0.979]
Epoch [72/120    avg_loss:0.015, val_acc:0.979]
Epoch [73/120    avg_loss:0.016, val_acc:0.979]
Epoch [74/120    avg_loss:0.016, val_acc:0.980]
Epoch [75/120    avg_loss:0.016, val_acc:0.980]
Epoch [76/120    avg_loss:0.014, val_acc:0.980]
Epoch [77/120    avg_loss:0.014, val_acc:0.980]
Epoch [78/120    avg_loss:0.014, val_acc:0.980]
Epoch [79/120    avg_loss:0.015, val_acc:0.980]
Epoch [80/120    avg_loss:0.014, val_acc:0.980]
Epoch [81/120    avg_loss:0.013, val_acc:0.980]
Epoch [82/120    avg_loss:0.010, val_acc:0.980]
Epoch [83/120    avg_loss:0.014, val_acc:0.980]
Epoch [84/120    avg_loss:0.012, val_acc:0.980]
Epoch [85/120    avg_loss:0.012, val_acc:0.980]
Epoch [86/120    avg_loss:0.014, val_acc:0.980]
Epoch [87/120    avg_loss:0.013, val_acc:0.980]
Epoch [88/120    avg_loss:0.013, val_acc:0.980]
Epoch [89/120    avg_loss:0.014, val_acc:0.980]
Epoch [90/120    avg_loss:0.009, val_acc:0.980]
Epoch [91/120    avg_loss:0.010, val_acc:0.980]
Epoch [92/120    avg_loss:0.011, val_acc:0.980]
Epoch [93/120    avg_loss:0.017, val_acc:0.980]
Epoch [94/120    avg_loss:0.015, val_acc:0.980]
Epoch [95/120    avg_loss:0.014, val_acc:0.980]
Epoch [96/120    avg_loss:0.010, val_acc:0.980]
Epoch [97/120    avg_loss:0.012, val_acc:0.980]
Epoch [98/120    avg_loss:0.015, val_acc:0.980]
Epoch [99/120    avg_loss:0.011, val_acc:0.980]
Epoch [100/120    avg_loss:0.014, val_acc:0.980]
Epoch [101/120    avg_loss:0.011, val_acc:0.980]
Epoch [102/120    avg_loss:0.012, val_acc:0.980]
Epoch [103/120    avg_loss:0.013, val_acc:0.980]
Epoch [104/120    avg_loss:0.012, val_acc:0.980]
Epoch [105/120    avg_loss:0.011, val_acc:0.980]
Epoch [106/120    avg_loss:0.012, val_acc:0.980]
Epoch [107/120    avg_loss:0.013, val_acc:0.980]
Epoch [108/120    avg_loss:0.012, val_acc:0.980]
Epoch [109/120    avg_loss:0.014, val_acc:0.980]
Epoch [110/120    avg_loss:0.010, val_acc:0.980]
Epoch [111/120    avg_loss:0.016, val_acc:0.980]
Epoch [112/120    avg_loss:0.011, val_acc:0.980]
Epoch [113/120    avg_loss:0.011, val_acc:0.980]
Epoch [114/120    avg_loss:0.011, val_acc:0.980]
Epoch [115/120    avg_loss:0.012, val_acc:0.980]
Epoch [116/120    avg_loss:0.011, val_acc:0.980]
Epoch [117/120    avg_loss:0.014, val_acc:0.980]
Epoch [118/120    avg_loss:0.013, val_acc:0.980]
Epoch [119/120    avg_loss:0.013, val_acc:0.980]
Epoch [120/120    avg_loss:0.012, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1262    0    0    1    3    0    0    0    9    7    1    0
     0    2    0]
 [   0    0    0  695    6    3    0    0    0   13    1    0   26    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   21    0    0    5    3    0    0    0  827    8    2    0
     0    9    0]
 [   0    0    7    0    0    0    5    0    0    0    8 2188    0    1
     1    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  522    0
     0    0   12]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0    0    0
  1135    2    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
    27  313    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.82113821138212

F1 scores:
[       nan 0.98765432 0.98019417 0.96393897 0.98611111 0.98514286
 0.98572502 1.         1.         0.73469388 0.95995357 0.99161568
 0.96221198 0.98930481 0.98396186 0.93016345 0.93333333]

Kappa:
0.9751656942974617
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f65fd5dd7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.103, val_acc:0.454]
Epoch [2/120    avg_loss:1.531, val_acc:0.631]
Epoch [3/120    avg_loss:1.270, val_acc:0.664]
Epoch [4/120    avg_loss:1.110, val_acc:0.713]
Epoch [5/120    avg_loss:0.858, val_acc:0.754]
Epoch [6/120    avg_loss:0.734, val_acc:0.740]
Epoch [7/120    avg_loss:0.647, val_acc:0.742]
Epoch [8/120    avg_loss:0.576, val_acc:0.820]
Epoch [9/120    avg_loss:0.516, val_acc:0.763]
Epoch [10/120    avg_loss:0.524, val_acc:0.836]
Epoch [11/120    avg_loss:0.360, val_acc:0.828]
Epoch [12/120    avg_loss:0.379, val_acc:0.851]
Epoch [13/120    avg_loss:0.341, val_acc:0.893]
Epoch [14/120    avg_loss:0.368, val_acc:0.881]
Epoch [15/120    avg_loss:0.366, val_acc:0.869]
Epoch [16/120    avg_loss:0.256, val_acc:0.893]
Epoch [17/120    avg_loss:0.373, val_acc:0.851]
Epoch [18/120    avg_loss:0.289, val_acc:0.855]
Epoch [19/120    avg_loss:0.330, val_acc:0.891]
Epoch [20/120    avg_loss:0.251, val_acc:0.903]
Epoch [21/120    avg_loss:0.202, val_acc:0.920]
Epoch [22/120    avg_loss:0.242, val_acc:0.907]
Epoch [23/120    avg_loss:0.197, val_acc:0.925]
Epoch [24/120    avg_loss:0.144, val_acc:0.932]
Epoch [25/120    avg_loss:0.170, val_acc:0.946]
Epoch [26/120    avg_loss:0.225, val_acc:0.905]
Epoch [27/120    avg_loss:0.160, val_acc:0.926]
Epoch [28/120    avg_loss:0.177, val_acc:0.898]
Epoch [29/120    avg_loss:0.136, val_acc:0.894]
Epoch [30/120    avg_loss:0.143, val_acc:0.927]
Epoch [31/120    avg_loss:0.087, val_acc:0.946]
Epoch [32/120    avg_loss:0.081, val_acc:0.955]
Epoch [33/120    avg_loss:0.073, val_acc:0.915]
Epoch [34/120    avg_loss:0.106, val_acc:0.948]
Epoch [35/120    avg_loss:0.099, val_acc:0.948]
Epoch [36/120    avg_loss:0.068, val_acc:0.963]
Epoch [37/120    avg_loss:0.054, val_acc:0.948]
Epoch [38/120    avg_loss:0.077, val_acc:0.915]
Epoch [39/120    avg_loss:0.116, val_acc:0.933]
Epoch [40/120    avg_loss:0.076, val_acc:0.963]
Epoch [41/120    avg_loss:0.083, val_acc:0.958]
Epoch [42/120    avg_loss:0.071, val_acc:0.948]
Epoch [43/120    avg_loss:0.040, val_acc:0.971]
Epoch [44/120    avg_loss:0.044, val_acc:0.967]
Epoch [45/120    avg_loss:0.038, val_acc:0.974]
Epoch [46/120    avg_loss:0.045, val_acc:0.968]
Epoch [47/120    avg_loss:0.182, val_acc:0.934]
Epoch [48/120    avg_loss:0.101, val_acc:0.960]
Epoch [49/120    avg_loss:0.051, val_acc:0.966]
Epoch [50/120    avg_loss:0.031, val_acc:0.963]
Epoch [51/120    avg_loss:0.031, val_acc:0.981]
Epoch [52/120    avg_loss:0.028, val_acc:0.977]
Epoch [53/120    avg_loss:0.037, val_acc:0.964]
Epoch [54/120    avg_loss:0.031, val_acc:0.981]
Epoch [55/120    avg_loss:0.021, val_acc:0.981]
Epoch [56/120    avg_loss:0.021, val_acc:0.972]
Epoch [57/120    avg_loss:0.025, val_acc:0.969]
Epoch [58/120    avg_loss:0.035, val_acc:0.969]
Epoch [59/120    avg_loss:0.025, val_acc:0.969]
Epoch [60/120    avg_loss:0.041, val_acc:0.971]
Epoch [61/120    avg_loss:0.016, val_acc:0.975]
Epoch [62/120    avg_loss:0.039, val_acc:0.975]
Epoch [63/120    avg_loss:0.015, val_acc:0.982]
Epoch [64/120    avg_loss:0.012, val_acc:0.983]
Epoch [65/120    avg_loss:0.016, val_acc:0.988]
Epoch [66/120    avg_loss:0.033, val_acc:0.964]
Epoch [67/120    avg_loss:0.032, val_acc:0.974]
Epoch [68/120    avg_loss:0.016, val_acc:0.985]
Epoch [69/120    avg_loss:0.009, val_acc:0.980]
Epoch [70/120    avg_loss:0.018, val_acc:0.976]
Epoch [71/120    avg_loss:0.012, val_acc:0.976]
Epoch [72/120    avg_loss:0.013, val_acc:0.981]
Epoch [73/120    avg_loss:0.015, val_acc:0.978]
Epoch [74/120    avg_loss:0.011, val_acc:0.982]
Epoch [75/120    avg_loss:0.013, val_acc:0.981]
Epoch [76/120    avg_loss:0.016, val_acc:0.960]
Epoch [77/120    avg_loss:0.032, val_acc:0.971]
Epoch [78/120    avg_loss:0.024, val_acc:0.984]
Epoch [79/120    avg_loss:0.012, val_acc:0.988]
Epoch [80/120    avg_loss:0.012, val_acc:0.989]
Epoch [81/120    avg_loss:0.009, val_acc:0.987]
Epoch [82/120    avg_loss:0.013, val_acc:0.988]
Epoch [83/120    avg_loss:0.006, val_acc:0.985]
Epoch [84/120    avg_loss:0.008, val_acc:0.986]
Epoch [85/120    avg_loss:0.012, val_acc:0.982]
Epoch [86/120    avg_loss:0.007, val_acc:0.984]
Epoch [87/120    avg_loss:0.006, val_acc:0.984]
Epoch [88/120    avg_loss:0.007, val_acc:0.985]
Epoch [89/120    avg_loss:0.005, val_acc:0.986]
Epoch [90/120    avg_loss:0.009, val_acc:0.988]
Epoch [91/120    avg_loss:0.005, val_acc:0.987]
Epoch [92/120    avg_loss:0.005, val_acc:0.986]
Epoch [93/120    avg_loss:0.006, val_acc:0.986]
Epoch [94/120    avg_loss:0.004, val_acc:0.986]
Epoch [95/120    avg_loss:0.005, val_acc:0.986]
Epoch [96/120    avg_loss:0.006, val_acc:0.986]
Epoch [97/120    avg_loss:0.009, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.004, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.986]
Epoch [101/120    avg_loss:0.007, val_acc:0.986]
Epoch [102/120    avg_loss:0.004, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.005, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.986]
Epoch [110/120    avg_loss:0.004, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.009, val_acc:0.986]
Epoch [114/120    avg_loss:0.005, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.006, val_acc:0.986]
Epoch [117/120    avg_loss:0.006, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.004, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    1 1260    2    1    0    0    0    0    0    5   10    2    0
     0    4    0]
 [   0    0    0  728    0    6    0    0    0    1    0    0    8    4
     0    0    0]
 [   0    0    0    3  210    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0  857    3    1    0
     1    6    0]
 [   0    0    7    3    0    0    3    0    0    0    3 2194    0    0
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    0    0  523    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    0    0    1    0    0    0
  1124    4    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    25  313    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.44986449864498

F1 scores:
[       nan 0.98795181 0.98746082 0.98179366 0.99056604 0.96875
 0.99018868 1.         1.         0.94736842 0.98449167 0.99343446
 0.97940075 0.98930481 0.98165939 0.92878338 0.96      ]

Kappa:
0.9823316890724435
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f67982396d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.096, val_acc:0.567]
Epoch [2/120    avg_loss:1.411, val_acc:0.669]
Epoch [3/120    avg_loss:1.178, val_acc:0.707]
Epoch [4/120    avg_loss:0.965, val_acc:0.752]
Epoch [5/120    avg_loss:0.796, val_acc:0.751]
Epoch [6/120    avg_loss:0.783, val_acc:0.779]
Epoch [7/120    avg_loss:0.722, val_acc:0.801]
Epoch [8/120    avg_loss:0.577, val_acc:0.781]
Epoch [9/120    avg_loss:0.589, val_acc:0.819]
Epoch [10/120    avg_loss:0.438, val_acc:0.864]
Epoch [11/120    avg_loss:0.458, val_acc:0.848]
Epoch [12/120    avg_loss:0.481, val_acc:0.879]
Epoch [13/120    avg_loss:0.294, val_acc:0.884]
Epoch [14/120    avg_loss:0.326, val_acc:0.874]
Epoch [15/120    avg_loss:0.238, val_acc:0.867]
Epoch [16/120    avg_loss:0.370, val_acc:0.878]
Epoch [17/120    avg_loss:0.255, val_acc:0.907]
Epoch [18/120    avg_loss:0.171, val_acc:0.926]
Epoch [19/120    avg_loss:0.193, val_acc:0.880]
Epoch [20/120    avg_loss:0.224, val_acc:0.920]
Epoch [21/120    avg_loss:0.178, val_acc:0.925]
Epoch [22/120    avg_loss:0.188, val_acc:0.945]
Epoch [23/120    avg_loss:0.240, val_acc:0.916]
Epoch [24/120    avg_loss:0.189, val_acc:0.899]
Epoch [25/120    avg_loss:0.186, val_acc:0.900]
Epoch [26/120    avg_loss:0.159, val_acc:0.898]
Epoch [27/120    avg_loss:0.129, val_acc:0.929]
Epoch [28/120    avg_loss:0.102, val_acc:0.939]
Epoch [29/120    avg_loss:0.130, val_acc:0.945]
Epoch [30/120    avg_loss:0.067, val_acc:0.956]
Epoch [31/120    avg_loss:0.071, val_acc:0.958]
Epoch [32/120    avg_loss:0.094, val_acc:0.948]
Epoch [33/120    avg_loss:0.135, val_acc:0.922]
Epoch [34/120    avg_loss:0.097, val_acc:0.953]
Epoch [35/120    avg_loss:0.102, val_acc:0.929]
Epoch [36/120    avg_loss:0.096, val_acc:0.961]
Epoch [37/120    avg_loss:0.070, val_acc:0.960]
Epoch [38/120    avg_loss:0.057, val_acc:0.964]
Epoch [39/120    avg_loss:0.062, val_acc:0.973]
Epoch [40/120    avg_loss:0.053, val_acc:0.912]
Epoch [41/120    avg_loss:0.096, val_acc:0.941]
Epoch [42/120    avg_loss:0.072, val_acc:0.967]
Epoch [43/120    avg_loss:0.041, val_acc:0.966]
Epoch [44/120    avg_loss:0.080, val_acc:0.964]
Epoch [45/120    avg_loss:0.054, val_acc:0.970]
Epoch [46/120    avg_loss:0.040, val_acc:0.969]
Epoch [47/120    avg_loss:0.037, val_acc:0.969]
Epoch [48/120    avg_loss:0.059, val_acc:0.966]
Epoch [49/120    avg_loss:0.080, val_acc:0.935]
Epoch [50/120    avg_loss:0.146, val_acc:0.964]
Epoch [51/120    avg_loss:0.270, val_acc:0.915]
Epoch [52/120    avg_loss:0.115, val_acc:0.973]
Epoch [53/120    avg_loss:0.045, val_acc:0.949]
Epoch [54/120    avg_loss:0.042, val_acc:0.969]
Epoch [55/120    avg_loss:0.032, val_acc:0.971]
Epoch [56/120    avg_loss:0.024, val_acc:0.973]
Epoch [57/120    avg_loss:0.032, val_acc:0.983]
Epoch [58/120    avg_loss:0.051, val_acc:0.973]
Epoch [59/120    avg_loss:0.082, val_acc:0.943]
Epoch [60/120    avg_loss:0.103, val_acc:0.970]
Epoch [61/120    avg_loss:0.031, val_acc:0.977]
Epoch [62/120    avg_loss:0.026, val_acc:0.975]
Epoch [63/120    avg_loss:0.022, val_acc:0.980]
Epoch [64/120    avg_loss:0.021, val_acc:0.968]
Epoch [65/120    avg_loss:0.015, val_acc:0.982]
Epoch [66/120    avg_loss:0.159, val_acc:0.909]
Epoch [67/120    avg_loss:0.125, val_acc:0.943]
Epoch [68/120    avg_loss:0.050, val_acc:0.970]
Epoch [69/120    avg_loss:0.033, val_acc:0.967]
Epoch [70/120    avg_loss:0.025, val_acc:0.980]
Epoch [71/120    avg_loss:0.016, val_acc:0.981]
Epoch [72/120    avg_loss:0.023, val_acc:0.982]
Epoch [73/120    avg_loss:0.014, val_acc:0.986]
Epoch [74/120    avg_loss:0.012, val_acc:0.984]
Epoch [75/120    avg_loss:0.015, val_acc:0.983]
Epoch [76/120    avg_loss:0.012, val_acc:0.984]
Epoch [77/120    avg_loss:0.010, val_acc:0.984]
Epoch [78/120    avg_loss:0.011, val_acc:0.983]
Epoch [79/120    avg_loss:0.010, val_acc:0.983]
Epoch [80/120    avg_loss:0.013, val_acc:0.981]
Epoch [81/120    avg_loss:0.013, val_acc:0.982]
Epoch [82/120    avg_loss:0.009, val_acc:0.981]
Epoch [83/120    avg_loss:0.011, val_acc:0.983]
Epoch [84/120    avg_loss:0.009, val_acc:0.982]
Epoch [85/120    avg_loss:0.009, val_acc:0.983]
Epoch [86/120    avg_loss:0.012, val_acc:0.981]
Epoch [87/120    avg_loss:0.009, val_acc:0.981]
Epoch [88/120    avg_loss:0.010, val_acc:0.980]
Epoch [89/120    avg_loss:0.008, val_acc:0.980]
Epoch [90/120    avg_loss:0.011, val_acc:0.981]
Epoch [91/120    avg_loss:0.010, val_acc:0.981]
Epoch [92/120    avg_loss:0.010, val_acc:0.981]
Epoch [93/120    avg_loss:0.009, val_acc:0.981]
Epoch [94/120    avg_loss:0.008, val_acc:0.981]
Epoch [95/120    avg_loss:0.009, val_acc:0.981]
Epoch [96/120    avg_loss:0.009, val_acc:0.981]
Epoch [97/120    avg_loss:0.009, val_acc:0.981]
Epoch [98/120    avg_loss:0.011, val_acc:0.981]
Epoch [99/120    avg_loss:0.016, val_acc:0.981]
Epoch [100/120    avg_loss:0.010, val_acc:0.981]
Epoch [101/120    avg_loss:0.011, val_acc:0.981]
Epoch [102/120    avg_loss:0.021, val_acc:0.981]
Epoch [103/120    avg_loss:0.012, val_acc:0.981]
Epoch [104/120    avg_loss:0.009, val_acc:0.981]
Epoch [105/120    avg_loss:0.012, val_acc:0.981]
Epoch [106/120    avg_loss:0.011, val_acc:0.981]
Epoch [107/120    avg_loss:0.012, val_acc:0.981]
Epoch [108/120    avg_loss:0.012, val_acc:0.981]
Epoch [109/120    avg_loss:0.008, val_acc:0.981]
Epoch [110/120    avg_loss:0.014, val_acc:0.981]
Epoch [111/120    avg_loss:0.011, val_acc:0.981]
Epoch [112/120    avg_loss:0.011, val_acc:0.981]
Epoch [113/120    avg_loss:0.009, val_acc:0.981]
Epoch [114/120    avg_loss:0.013, val_acc:0.981]
Epoch [115/120    avg_loss:0.013, val_acc:0.981]
Epoch [116/120    avg_loss:0.009, val_acc:0.981]
Epoch [117/120    avg_loss:0.008, val_acc:0.981]
Epoch [118/120    avg_loss:0.011, val_acc:0.981]
Epoch [119/120    avg_loss:0.009, val_acc:0.981]
Epoch [120/120    avg_loss:0.009, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1259    2    0    0    1    0    0    1    8    5    8    0
     0    1    0]
 [   0    0    0  723    0    7    0    0    0    6    1    0    7    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    4    0    0    0  859    3    0    0
     0    4    0]
 [   0    0   18    0    0    0    5    0    0    0    7 2177    3    0
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    5    0  526    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0    0    0
  1133    4    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    22  312    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.31978319783198

F1 scores:
[       nan 0.96202532 0.98091157 0.98233696 1.         0.98858447
 0.98279731 1.         0.99883586 0.8        0.97613636 0.99067122
 0.97497683 0.9919571  0.98779425 0.93413174 0.97647059]

Kappa:
0.9808526372361238
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1bfe317710>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.062, val_acc:0.529]
Epoch [2/120    avg_loss:1.573, val_acc:0.575]
Epoch [3/120    avg_loss:1.169, val_acc:0.724]
Epoch [4/120    avg_loss:1.065, val_acc:0.719]
Epoch [5/120    avg_loss:0.836, val_acc:0.765]
Epoch [6/120    avg_loss:0.877, val_acc:0.797]
Epoch [7/120    avg_loss:0.698, val_acc:0.769]
Epoch [8/120    avg_loss:0.718, val_acc:0.718]
Epoch [9/120    avg_loss:0.658, val_acc:0.841]
Epoch [10/120    avg_loss:0.490, val_acc:0.842]
Epoch [11/120    avg_loss:0.542, val_acc:0.827]
Epoch [12/120    avg_loss:0.455, val_acc:0.825]
Epoch [13/120    avg_loss:0.371, val_acc:0.789]
Epoch [14/120    avg_loss:0.376, val_acc:0.842]
Epoch [15/120    avg_loss:0.348, val_acc:0.861]
Epoch [16/120    avg_loss:0.299, val_acc:0.891]
Epoch [17/120    avg_loss:0.244, val_acc:0.903]
Epoch [18/120    avg_loss:0.242, val_acc:0.901]
Epoch [19/120    avg_loss:0.261, val_acc:0.850]
Epoch [20/120    avg_loss:0.304, val_acc:0.865]
Epoch [21/120    avg_loss:0.239, val_acc:0.923]
Epoch [22/120    avg_loss:0.184, val_acc:0.849]
Epoch [23/120    avg_loss:0.261, val_acc:0.883]
Epoch [24/120    avg_loss:0.248, val_acc:0.930]
Epoch [25/120    avg_loss:0.184, val_acc:0.918]
Epoch [26/120    avg_loss:0.201, val_acc:0.911]
Epoch [27/120    avg_loss:0.158, val_acc:0.928]
Epoch [28/120    avg_loss:0.160, val_acc:0.930]
Epoch [29/120    avg_loss:0.139, val_acc:0.932]
Epoch [30/120    avg_loss:0.138, val_acc:0.938]
Epoch [31/120    avg_loss:0.071, val_acc:0.950]
Epoch [32/120    avg_loss:0.114, val_acc:0.920]
Epoch [33/120    avg_loss:0.148, val_acc:0.941]
Epoch [34/120    avg_loss:0.090, val_acc:0.932]
Epoch [35/120    avg_loss:0.060, val_acc:0.935]
Epoch [36/120    avg_loss:0.153, val_acc:0.945]
Epoch [37/120    avg_loss:0.100, val_acc:0.949]
Epoch [38/120    avg_loss:0.116, val_acc:0.936]
Epoch [39/120    avg_loss:0.067, val_acc:0.955]
Epoch [40/120    avg_loss:0.111, val_acc:0.958]
Epoch [41/120    avg_loss:0.085, val_acc:0.946]
Epoch [42/120    avg_loss:0.095, val_acc:0.959]
Epoch [43/120    avg_loss:0.172, val_acc:0.906]
Epoch [44/120    avg_loss:0.108, val_acc:0.920]
Epoch [45/120    avg_loss:0.070, val_acc:0.904]
Epoch [46/120    avg_loss:0.088, val_acc:0.960]
Epoch [47/120    avg_loss:0.048, val_acc:0.949]
Epoch [48/120    avg_loss:0.049, val_acc:0.949]
Epoch [49/120    avg_loss:0.043, val_acc:0.952]
Epoch [50/120    avg_loss:0.044, val_acc:0.963]
Epoch [51/120    avg_loss:0.053, val_acc:0.959]
Epoch [52/120    avg_loss:0.067, val_acc:0.948]
Epoch [53/120    avg_loss:0.051, val_acc:0.976]
Epoch [54/120    avg_loss:0.035, val_acc:0.959]
Epoch [55/120    avg_loss:0.078, val_acc:0.960]
Epoch [56/120    avg_loss:0.049, val_acc:0.968]
Epoch [57/120    avg_loss:0.044, val_acc:0.968]
Epoch [58/120    avg_loss:0.044, val_acc:0.961]
Epoch [59/120    avg_loss:0.029, val_acc:0.964]
Epoch [60/120    avg_loss:0.035, val_acc:0.968]
Epoch [61/120    avg_loss:0.042, val_acc:0.962]
Epoch [62/120    avg_loss:0.030, val_acc:0.948]
Epoch [63/120    avg_loss:0.031, val_acc:0.973]
Epoch [64/120    avg_loss:0.020, val_acc:0.973]
Epoch [65/120    avg_loss:0.028, val_acc:0.970]
Epoch [66/120    avg_loss:0.032, val_acc:0.947]
Epoch [67/120    avg_loss:0.040, val_acc:0.972]
Epoch [68/120    avg_loss:0.026, val_acc:0.980]
Epoch [69/120    avg_loss:0.020, val_acc:0.980]
Epoch [70/120    avg_loss:0.013, val_acc:0.981]
Epoch [71/120    avg_loss:0.015, val_acc:0.981]
Epoch [72/120    avg_loss:0.017, val_acc:0.981]
Epoch [73/120    avg_loss:0.010, val_acc:0.980]
Epoch [74/120    avg_loss:0.011, val_acc:0.980]
Epoch [75/120    avg_loss:0.012, val_acc:0.982]
Epoch [76/120    avg_loss:0.016, val_acc:0.981]
Epoch [77/120    avg_loss:0.012, val_acc:0.983]
Epoch [78/120    avg_loss:0.010, val_acc:0.982]
Epoch [79/120    avg_loss:0.014, val_acc:0.984]
Epoch [80/120    avg_loss:0.014, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.984]
Epoch [82/120    avg_loss:0.010, val_acc:0.982]
Epoch [83/120    avg_loss:0.011, val_acc:0.983]
Epoch [84/120    avg_loss:0.011, val_acc:0.983]
Epoch [85/120    avg_loss:0.010, val_acc:0.982]
Epoch [86/120    avg_loss:0.010, val_acc:0.983]
Epoch [87/120    avg_loss:0.008, val_acc:0.984]
Epoch [88/120    avg_loss:0.010, val_acc:0.984]
Epoch [89/120    avg_loss:0.008, val_acc:0.983]
Epoch [90/120    avg_loss:0.008, val_acc:0.983]
Epoch [91/120    avg_loss:0.010, val_acc:0.982]
Epoch [92/120    avg_loss:0.010, val_acc:0.982]
Epoch [93/120    avg_loss:0.007, val_acc:0.982]
Epoch [94/120    avg_loss:0.011, val_acc:0.983]
Epoch [95/120    avg_loss:0.008, val_acc:0.982]
Epoch [96/120    avg_loss:0.011, val_acc:0.983]
Epoch [97/120    avg_loss:0.011, val_acc:0.983]
Epoch [98/120    avg_loss:0.008, val_acc:0.983]
Epoch [99/120    avg_loss:0.024, val_acc:0.981]
Epoch [100/120    avg_loss:0.009, val_acc:0.983]
Epoch [101/120    avg_loss:0.012, val_acc:0.983]
Epoch [102/120    avg_loss:0.009, val_acc:0.983]
Epoch [103/120    avg_loss:0.011, val_acc:0.983]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.008, val_acc:0.983]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.008, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.983]
Epoch [109/120    avg_loss:0.008, val_acc:0.983]
Epoch [110/120    avg_loss:0.008, val_acc:0.983]
Epoch [111/120    avg_loss:0.011, val_acc:0.984]
Epoch [112/120    avg_loss:0.008, val_acc:0.984]
Epoch [113/120    avg_loss:0.008, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.983]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.008, val_acc:0.983]
Epoch [119/120    avg_loss:0.007, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1261    0    0    0    0    0    0    0   10   12    0    0
     0    2    0]
 [   0    0    0  694    0   15    0    0    0    3    0    0   31    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    7    4    0    0    0  853    0    3    0
     0    8    0]
 [   0    0    1    0    0    1    4    0    0    0    8 2194    1    1
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    0    0  521    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    1    0    1    0    0    0
  1130    3    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    47  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.86449864498645

F1 scores:
[       nan 0.94871795 0.99018453 0.96321999 1.         0.96230599
 0.98869631 1.         0.99883856 0.9        0.9743004  0.99343446
 0.95508708 0.98666667 0.97582038 0.89908257 0.95402299]

Kappa:
0.9756562772030462
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc1ee365748>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.080, val_acc:0.575]
Epoch [2/120    avg_loss:1.534, val_acc:0.579]
Epoch [3/120    avg_loss:1.315, val_acc:0.644]
Epoch [4/120    avg_loss:1.044, val_acc:0.750]
Epoch [5/120    avg_loss:0.870, val_acc:0.765]
Epoch [6/120    avg_loss:0.817, val_acc:0.742]
Epoch [7/120    avg_loss:0.678, val_acc:0.770]
Epoch [8/120    avg_loss:0.645, val_acc:0.808]
Epoch [9/120    avg_loss:0.560, val_acc:0.809]
Epoch [10/120    avg_loss:0.465, val_acc:0.833]
Epoch [11/120    avg_loss:0.418, val_acc:0.804]
Epoch [12/120    avg_loss:0.398, val_acc:0.874]
Epoch [13/120    avg_loss:0.401, val_acc:0.863]
Epoch [14/120    avg_loss:0.407, val_acc:0.858]
Epoch [15/120    avg_loss:0.486, val_acc:0.824]
Epoch [16/120    avg_loss:0.445, val_acc:0.845]
Epoch [17/120    avg_loss:0.340, val_acc:0.890]
Epoch [18/120    avg_loss:0.294, val_acc:0.867]
Epoch [19/120    avg_loss:0.368, val_acc:0.869]
Epoch [20/120    avg_loss:0.223, val_acc:0.884]
Epoch [21/120    avg_loss:0.220, val_acc:0.850]
Epoch [22/120    avg_loss:0.217, val_acc:0.914]
Epoch [23/120    avg_loss:0.197, val_acc:0.894]
Epoch [24/120    avg_loss:0.233, val_acc:0.867]
Epoch [25/120    avg_loss:0.214, val_acc:0.921]
Epoch [26/120    avg_loss:0.135, val_acc:0.912]
Epoch [27/120    avg_loss:0.102, val_acc:0.941]
Epoch [28/120    avg_loss:0.098, val_acc:0.883]
Epoch [29/120    avg_loss:0.148, val_acc:0.931]
Epoch [30/120    avg_loss:0.154, val_acc:0.920]
Epoch [31/120    avg_loss:0.094, val_acc:0.950]
Epoch [32/120    avg_loss:0.117, val_acc:0.955]
Epoch [33/120    avg_loss:0.083, val_acc:0.934]
Epoch [34/120    avg_loss:0.074, val_acc:0.943]
Epoch [35/120    avg_loss:0.080, val_acc:0.961]
Epoch [36/120    avg_loss:0.068, val_acc:0.955]
Epoch [37/120    avg_loss:0.074, val_acc:0.945]
Epoch [38/120    avg_loss:0.062, val_acc:0.962]
Epoch [39/120    avg_loss:0.070, val_acc:0.959]
Epoch [40/120    avg_loss:0.063, val_acc:0.943]
Epoch [41/120    avg_loss:0.045, val_acc:0.975]
Epoch [42/120    avg_loss:0.128, val_acc:0.948]
Epoch [43/120    avg_loss:0.068, val_acc:0.935]
Epoch [44/120    avg_loss:0.061, val_acc:0.944]
Epoch [45/120    avg_loss:0.220, val_acc:0.923]
Epoch [46/120    avg_loss:0.124, val_acc:0.952]
Epoch [47/120    avg_loss:0.059, val_acc:0.963]
Epoch [48/120    avg_loss:0.049, val_acc:0.944]
Epoch [49/120    avg_loss:0.058, val_acc:0.963]
Epoch [50/120    avg_loss:0.062, val_acc:0.945]
Epoch [51/120    avg_loss:0.055, val_acc:0.958]
Epoch [52/120    avg_loss:0.056, val_acc:0.961]
Epoch [53/120    avg_loss:0.111, val_acc:0.966]
Epoch [54/120    avg_loss:0.088, val_acc:0.947]
Epoch [55/120    avg_loss:0.058, val_acc:0.967]
Epoch [56/120    avg_loss:0.029, val_acc:0.973]
Epoch [57/120    avg_loss:0.025, val_acc:0.975]
Epoch [58/120    avg_loss:0.027, val_acc:0.973]
Epoch [59/120    avg_loss:0.032, val_acc:0.975]
Epoch [60/120    avg_loss:0.025, val_acc:0.975]
Epoch [61/120    avg_loss:0.025, val_acc:0.976]
Epoch [62/120    avg_loss:0.026, val_acc:0.974]
Epoch [63/120    avg_loss:0.020, val_acc:0.975]
Epoch [64/120    avg_loss:0.019, val_acc:0.977]
Epoch [65/120    avg_loss:0.019, val_acc:0.981]
Epoch [66/120    avg_loss:0.022, val_acc:0.982]
Epoch [67/120    avg_loss:0.023, val_acc:0.980]
Epoch [68/120    avg_loss:0.019, val_acc:0.980]
Epoch [69/120    avg_loss:0.020, val_acc:0.982]
Epoch [70/120    avg_loss:0.023, val_acc:0.981]
Epoch [71/120    avg_loss:0.017, val_acc:0.982]
Epoch [72/120    avg_loss:0.027, val_acc:0.981]
Epoch [73/120    avg_loss:0.014, val_acc:0.983]
Epoch [74/120    avg_loss:0.018, val_acc:0.983]
Epoch [75/120    avg_loss:0.017, val_acc:0.982]
Epoch [76/120    avg_loss:0.015, val_acc:0.984]
Epoch [77/120    avg_loss:0.015, val_acc:0.983]
Epoch [78/120    avg_loss:0.015, val_acc:0.983]
Epoch [79/120    avg_loss:0.018, val_acc:0.982]
Epoch [80/120    avg_loss:0.022, val_acc:0.980]
Epoch [81/120    avg_loss:0.022, val_acc:0.981]
Epoch [82/120    avg_loss:0.019, val_acc:0.981]
Epoch [83/120    avg_loss:0.016, val_acc:0.982]
Epoch [84/120    avg_loss:0.021, val_acc:0.982]
Epoch [85/120    avg_loss:0.014, val_acc:0.983]
Epoch [86/120    avg_loss:0.015, val_acc:0.983]
Epoch [87/120    avg_loss:0.014, val_acc:0.985]
Epoch [88/120    avg_loss:0.020, val_acc:0.983]
Epoch [89/120    avg_loss:0.013, val_acc:0.986]
Epoch [90/120    avg_loss:0.015, val_acc:0.984]
Epoch [91/120    avg_loss:0.012, val_acc:0.984]
Epoch [92/120    avg_loss:0.012, val_acc:0.985]
Epoch [93/120    avg_loss:0.014, val_acc:0.984]
Epoch [94/120    avg_loss:0.016, val_acc:0.984]
Epoch [95/120    avg_loss:0.014, val_acc:0.985]
Epoch [96/120    avg_loss:0.013, val_acc:0.983]
Epoch [97/120    avg_loss:0.018, val_acc:0.981]
Epoch [98/120    avg_loss:0.012, val_acc:0.984]
Epoch [99/120    avg_loss:0.011, val_acc:0.985]
Epoch [100/120    avg_loss:0.011, val_acc:0.985]
Epoch [101/120    avg_loss:0.016, val_acc:0.986]
Epoch [102/120    avg_loss:0.013, val_acc:0.986]
Epoch [103/120    avg_loss:0.011, val_acc:0.986]
Epoch [104/120    avg_loss:0.011, val_acc:0.986]
Epoch [105/120    avg_loss:0.012, val_acc:0.986]
Epoch [106/120    avg_loss:0.014, val_acc:0.986]
Epoch [107/120    avg_loss:0.011, val_acc:0.984]
Epoch [108/120    avg_loss:0.012, val_acc:0.984]
Epoch [109/120    avg_loss:0.014, val_acc:0.984]
Epoch [110/120    avg_loss:0.016, val_acc:0.986]
Epoch [111/120    avg_loss:0.015, val_acc:0.985]
Epoch [112/120    avg_loss:0.010, val_acc:0.984]
Epoch [113/120    avg_loss:0.013, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.985]
Epoch [115/120    avg_loss:0.010, val_acc:0.986]
Epoch [116/120    avg_loss:0.012, val_acc:0.984]
Epoch [117/120    avg_loss:0.012, val_acc:0.984]
Epoch [118/120    avg_loss:0.011, val_acc:0.984]
Epoch [119/120    avg_loss:0.013, val_acc:0.982]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1271    0    0    1    0    0    0    0    5    8    0    0
     0    0    0]
 [   0    0    0  728    2    9    0    0    0    2    1    0    1    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    2    0    9    4    0    0    0  845    2    2    0
     2    4    0]
 [   0    0   16    0    0    4    3    0    0    0    1 2141   44    1
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    1    0  521    0
     1    2    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   11    0    0    1    0    3    0    0    0
  1124    0    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    61  277    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.46341463414635

F1 scores:
[       nan 0.98765432 0.98603569 0.98578199 0.9953271  0.95920617
 0.98644578 1.         0.99883856 0.94736842 0.97631427 0.98143479
 0.94469628 0.98666667 0.96605071 0.87936508 0.95953757]

Kappa:
0.9711058746127874
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fba803506d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.100, val_acc:0.447]
Epoch [2/120    avg_loss:1.443, val_acc:0.687]
Epoch [3/120    avg_loss:1.211, val_acc:0.719]
Epoch [4/120    avg_loss:0.928, val_acc:0.753]
Epoch [5/120    avg_loss:0.861, val_acc:0.709]
Epoch [6/120    avg_loss:0.723, val_acc:0.760]
Epoch [7/120    avg_loss:0.773, val_acc:0.801]
Epoch [8/120    avg_loss:0.576, val_acc:0.775]
Epoch [9/120    avg_loss:0.588, val_acc:0.810]
Epoch [10/120    avg_loss:0.504, val_acc:0.815]
Epoch [11/120    avg_loss:0.438, val_acc:0.874]
Epoch [12/120    avg_loss:0.446, val_acc:0.889]
Epoch [13/120    avg_loss:0.414, val_acc:0.842]
Epoch [14/120    avg_loss:0.323, val_acc:0.893]
Epoch [15/120    avg_loss:0.294, val_acc:0.851]
Epoch [16/120    avg_loss:0.365, val_acc:0.885]
Epoch [17/120    avg_loss:0.265, val_acc:0.876]
Epoch [18/120    avg_loss:0.281, val_acc:0.919]
Epoch [19/120    avg_loss:0.250, val_acc:0.913]
Epoch [20/120    avg_loss:0.212, val_acc:0.917]
Epoch [21/120    avg_loss:0.268, val_acc:0.887]
Epoch [22/120    avg_loss:0.221, val_acc:0.890]
Epoch [23/120    avg_loss:0.152, val_acc:0.937]
Epoch [24/120    avg_loss:0.164, val_acc:0.916]
Epoch [25/120    avg_loss:0.165, val_acc:0.931]
Epoch [26/120    avg_loss:0.133, val_acc:0.946]
Epoch [27/120    avg_loss:0.169, val_acc:0.934]
Epoch [28/120    avg_loss:0.168, val_acc:0.935]
Epoch [29/120    avg_loss:0.129, val_acc:0.948]
Epoch [30/120    avg_loss:0.103, val_acc:0.950]
Epoch [31/120    avg_loss:0.196, val_acc:0.928]
Epoch [32/120    avg_loss:0.113, val_acc:0.950]
Epoch [33/120    avg_loss:0.112, val_acc:0.939]
Epoch [34/120    avg_loss:0.111, val_acc:0.948]
Epoch [35/120    avg_loss:0.119, val_acc:0.952]
Epoch [36/120    avg_loss:0.125, val_acc:0.903]
Epoch [37/120    avg_loss:0.118, val_acc:0.966]
Epoch [38/120    avg_loss:0.091, val_acc:0.952]
Epoch [39/120    avg_loss:0.081, val_acc:0.969]
Epoch [40/120    avg_loss:0.068, val_acc:0.964]
Epoch [41/120    avg_loss:0.062, val_acc:0.956]
Epoch [42/120    avg_loss:0.052, val_acc:0.972]
Epoch [43/120    avg_loss:0.055, val_acc:0.978]
Epoch [44/120    avg_loss:0.047, val_acc:0.970]
Epoch [45/120    avg_loss:0.077, val_acc:0.967]
Epoch [46/120    avg_loss:0.053, val_acc:0.976]
Epoch [47/120    avg_loss:0.054, val_acc:0.974]
Epoch [48/120    avg_loss:0.046, val_acc:0.956]
Epoch [49/120    avg_loss:0.078, val_acc:0.971]
Epoch [50/120    avg_loss:0.087, val_acc:0.969]
Epoch [51/120    avg_loss:0.041, val_acc:0.972]
Epoch [52/120    avg_loss:0.037, val_acc:0.982]
Epoch [53/120    avg_loss:0.042, val_acc:0.981]
Epoch [54/120    avg_loss:0.039, val_acc:0.979]
Epoch [55/120    avg_loss:0.035, val_acc:0.982]
Epoch [56/120    avg_loss:0.034, val_acc:0.964]
Epoch [57/120    avg_loss:0.036, val_acc:0.982]
Epoch [58/120    avg_loss:0.029, val_acc:0.978]
Epoch [59/120    avg_loss:0.025, val_acc:0.984]
Epoch [60/120    avg_loss:0.022, val_acc:0.984]
Epoch [61/120    avg_loss:0.017, val_acc:0.987]
Epoch [62/120    avg_loss:0.013, val_acc:0.982]
Epoch [63/120    avg_loss:0.024, val_acc:0.990]
Epoch [64/120    avg_loss:0.088, val_acc:0.971]
Epoch [65/120    avg_loss:0.046, val_acc:0.981]
Epoch [66/120    avg_loss:0.034, val_acc:0.988]
Epoch [67/120    avg_loss:0.033, val_acc:0.973]
Epoch [68/120    avg_loss:0.049, val_acc:0.966]
Epoch [69/120    avg_loss:0.040, val_acc:0.985]
Epoch [70/120    avg_loss:0.010, val_acc:0.986]
Epoch [71/120    avg_loss:0.012, val_acc:0.989]
Epoch [72/120    avg_loss:0.020, val_acc:0.985]
Epoch [73/120    avg_loss:0.014, val_acc:0.980]
Epoch [74/120    avg_loss:0.012, val_acc:0.987]
Epoch [75/120    avg_loss:0.026, val_acc:0.953]
Epoch [76/120    avg_loss:0.116, val_acc:0.954]
Epoch [77/120    avg_loss:0.056, val_acc:0.979]
Epoch [78/120    avg_loss:0.027, val_acc:0.984]
Epoch [79/120    avg_loss:0.015, val_acc:0.986]
Epoch [80/120    avg_loss:0.019, val_acc:0.986]
Epoch [81/120    avg_loss:0.014, val_acc:0.985]
Epoch [82/120    avg_loss:0.015, val_acc:0.986]
Epoch [83/120    avg_loss:0.014, val_acc:0.987]
Epoch [84/120    avg_loss:0.016, val_acc:0.988]
Epoch [85/120    avg_loss:0.013, val_acc:0.990]
Epoch [86/120    avg_loss:0.014, val_acc:0.993]
Epoch [87/120    avg_loss:0.018, val_acc:0.994]
Epoch [88/120    avg_loss:0.008, val_acc:0.994]
Epoch [89/120    avg_loss:0.012, val_acc:0.996]
Epoch [90/120    avg_loss:0.010, val_acc:0.994]
Epoch [91/120    avg_loss:0.008, val_acc:0.995]
Epoch [92/120    avg_loss:0.012, val_acc:0.995]
Epoch [93/120    avg_loss:0.012, val_acc:0.994]
Epoch [94/120    avg_loss:0.010, val_acc:0.994]
Epoch [95/120    avg_loss:0.009, val_acc:0.995]
Epoch [96/120    avg_loss:0.011, val_acc:0.993]
Epoch [97/120    avg_loss:0.008, val_acc:0.994]
Epoch [98/120    avg_loss:0.008, val_acc:0.995]
Epoch [99/120    avg_loss:0.013, val_acc:0.991]
Epoch [100/120    avg_loss:0.009, val_acc:0.995]
Epoch [101/120    avg_loss:0.007, val_acc:0.995]
Epoch [102/120    avg_loss:0.008, val_acc:0.995]
Epoch [103/120    avg_loss:0.009, val_acc:0.995]
Epoch [104/120    avg_loss:0.008, val_acc:0.995]
Epoch [105/120    avg_loss:0.011, val_acc:0.995]
Epoch [106/120    avg_loss:0.006, val_acc:0.995]
Epoch [107/120    avg_loss:0.012, val_acc:0.995]
Epoch [108/120    avg_loss:0.007, val_acc:0.995]
Epoch [109/120    avg_loss:0.006, val_acc:0.995]
Epoch [110/120    avg_loss:0.008, val_acc:0.995]
Epoch [111/120    avg_loss:0.011, val_acc:0.995]
Epoch [112/120    avg_loss:0.007, val_acc:0.995]
Epoch [113/120    avg_loss:0.010, val_acc:0.995]
Epoch [114/120    avg_loss:0.007, val_acc:0.995]
Epoch [115/120    avg_loss:0.009, val_acc:0.995]
Epoch [116/120    avg_loss:0.009, val_acc:0.995]
Epoch [117/120    avg_loss:0.007, val_acc:0.995]
Epoch [118/120    avg_loss:0.008, val_acc:0.995]
Epoch [119/120    avg_loss:0.010, val_acc:0.995]
Epoch [120/120    avg_loss:0.006, val_acc:0.995]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1270    0    2    0    2    0    0    1    6    4    0    0
     0    0    0]
 [   0    0    0  705    5    7    0    0    0    6    1    1   15    7
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    6   10    0    0    0  847    5    2    0
     0    0    0]
 [   0    0    8    0    0    0    4    0    0    0    6 2136   55    1
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    0    0  525    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    0    0    1    0    2    1    0    0
  1119    7    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    31  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.42005420054201

F1 scores:
[       nan 1.         0.98909657 0.97107438 0.98383372 0.96420582
 0.97691735 1.         0.99883856 0.81818182 0.97524467 0.98026618
 0.92838196 0.97883598 0.97643979 0.92073171 0.98245614]

Kappa:
0.970630597608046
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6775dac710>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.059, val_acc:0.536]
Epoch [2/120    avg_loss:1.517, val_acc:0.589]
Epoch [3/120    avg_loss:1.278, val_acc:0.654]
Epoch [4/120    avg_loss:1.025, val_acc:0.715]
Epoch [5/120    avg_loss:0.830, val_acc:0.746]
Epoch [6/120    avg_loss:0.708, val_acc:0.742]
Epoch [7/120    avg_loss:0.642, val_acc:0.799]
Epoch [8/120    avg_loss:0.619, val_acc:0.779]
Epoch [9/120    avg_loss:0.555, val_acc:0.818]
Epoch [10/120    avg_loss:0.493, val_acc:0.794]
Epoch [11/120    avg_loss:0.457, val_acc:0.820]
Epoch [12/120    avg_loss:0.486, val_acc:0.840]
Epoch [13/120    avg_loss:0.357, val_acc:0.859]
Epoch [14/120    avg_loss:0.381, val_acc:0.824]
Epoch [15/120    avg_loss:0.287, val_acc:0.886]
Epoch [16/120    avg_loss:0.355, val_acc:0.861]
Epoch [17/120    avg_loss:0.319, val_acc:0.829]
Epoch [18/120    avg_loss:0.279, val_acc:0.910]
Epoch [19/120    avg_loss:0.254, val_acc:0.881]
Epoch [20/120    avg_loss:0.183, val_acc:0.883]
Epoch [21/120    avg_loss:0.208, val_acc:0.932]
Epoch [22/120    avg_loss:0.152, val_acc:0.927]
Epoch [23/120    avg_loss:0.174, val_acc:0.919]
Epoch [24/120    avg_loss:0.174, val_acc:0.908]
Epoch [25/120    avg_loss:0.304, val_acc:0.877]
Epoch [26/120    avg_loss:0.168, val_acc:0.931]
Epoch [27/120    avg_loss:0.183, val_acc:0.918]
Epoch [28/120    avg_loss:0.090, val_acc:0.935]
Epoch [29/120    avg_loss:0.203, val_acc:0.914]
Epoch [30/120    avg_loss:0.129, val_acc:0.938]
Epoch [31/120    avg_loss:0.084, val_acc:0.941]
Epoch [32/120    avg_loss:0.126, val_acc:0.917]
Epoch [33/120    avg_loss:0.091, val_acc:0.926]
Epoch [34/120    avg_loss:0.102, val_acc:0.942]
Epoch [35/120    avg_loss:0.123, val_acc:0.953]
Epoch [36/120    avg_loss:0.075, val_acc:0.938]
Epoch [37/120    avg_loss:0.104, val_acc:0.934]
Epoch [38/120    avg_loss:0.093, val_acc:0.947]
Epoch [39/120    avg_loss:0.094, val_acc:0.942]
Epoch [40/120    avg_loss:0.063, val_acc:0.968]
Epoch [41/120    avg_loss:0.051, val_acc:0.965]
Epoch [42/120    avg_loss:0.051, val_acc:0.964]
Epoch [43/120    avg_loss:0.044, val_acc:0.969]
Epoch [44/120    avg_loss:0.048, val_acc:0.950]
Epoch [45/120    avg_loss:0.036, val_acc:0.963]
Epoch [46/120    avg_loss:0.041, val_acc:0.969]
Epoch [47/120    avg_loss:0.170, val_acc:0.906]
Epoch [48/120    avg_loss:0.103, val_acc:0.957]
Epoch [49/120    avg_loss:0.078, val_acc:0.943]
Epoch [50/120    avg_loss:0.070, val_acc:0.930]
Epoch [51/120    avg_loss:0.126, val_acc:0.925]
Epoch [52/120    avg_loss:0.077, val_acc:0.950]
Epoch [53/120    avg_loss:0.033, val_acc:0.957]
Epoch [54/120    avg_loss:0.059, val_acc:0.972]
Epoch [55/120    avg_loss:0.053, val_acc:0.961]
Epoch [56/120    avg_loss:0.040, val_acc:0.923]
Epoch [57/120    avg_loss:0.025, val_acc:0.968]
Epoch [58/120    avg_loss:0.019, val_acc:0.970]
Epoch [59/120    avg_loss:0.028, val_acc:0.976]
Epoch [60/120    avg_loss:0.037, val_acc:0.967]
Epoch [61/120    avg_loss:0.059, val_acc:0.971]
Epoch [62/120    avg_loss:0.038, val_acc:0.964]
Epoch [63/120    avg_loss:0.016, val_acc:0.978]
Epoch [64/120    avg_loss:0.013, val_acc:0.971]
Epoch [65/120    avg_loss:0.014, val_acc:0.983]
Epoch [66/120    avg_loss:0.015, val_acc:0.977]
Epoch [67/120    avg_loss:0.010, val_acc:0.984]
Epoch [68/120    avg_loss:0.012, val_acc:0.981]
Epoch [69/120    avg_loss:0.012, val_acc:0.978]
Epoch [70/120    avg_loss:0.018, val_acc:0.980]
Epoch [71/120    avg_loss:0.021, val_acc:0.974]
Epoch [72/120    avg_loss:0.033, val_acc:0.968]
Epoch [73/120    avg_loss:0.037, val_acc:0.975]
Epoch [74/120    avg_loss:0.017, val_acc:0.979]
Epoch [75/120    avg_loss:0.020, val_acc:0.974]
Epoch [76/120    avg_loss:0.017, val_acc:0.975]
Epoch [77/120    avg_loss:0.013, val_acc:0.976]
Epoch [78/120    avg_loss:0.010, val_acc:0.971]
Epoch [79/120    avg_loss:0.015, val_acc:0.963]
Epoch [80/120    avg_loss:0.011, val_acc:0.979]
Epoch [81/120    avg_loss:0.013, val_acc:0.984]
Epoch [82/120    avg_loss:0.007, val_acc:0.984]
Epoch [83/120    avg_loss:0.007, val_acc:0.985]
Epoch [84/120    avg_loss:0.009, val_acc:0.986]
Epoch [85/120    avg_loss:0.005, val_acc:0.985]
Epoch [86/120    avg_loss:0.007, val_acc:0.986]
Epoch [87/120    avg_loss:0.006, val_acc:0.987]
Epoch [88/120    avg_loss:0.006, val_acc:0.988]
Epoch [89/120    avg_loss:0.005, val_acc:0.987]
Epoch [90/120    avg_loss:0.007, val_acc:0.987]
Epoch [91/120    avg_loss:0.005, val_acc:0.987]
Epoch [92/120    avg_loss:0.005, val_acc:0.988]
Epoch [93/120    avg_loss:0.004, val_acc:0.988]
Epoch [94/120    avg_loss:0.006, val_acc:0.987]
Epoch [95/120    avg_loss:0.006, val_acc:0.987]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.005, val_acc:0.987]
Epoch [98/120    avg_loss:0.005, val_acc:0.986]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.005, val_acc:0.987]
Epoch [103/120    avg_loss:0.006, val_acc:0.987]
Epoch [104/120    avg_loss:0.006, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.004, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.004, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.005, val_acc:0.987]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.005, val_acc:0.987]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.005, val_acc:0.987]
Epoch [120/120    avg_loss:0.006, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1257    1    0    0    0    0    0    0    9    7    5    0
     0    6    0]
 [   0    0    0  700    0   18    0    0    0    2    0    0   23    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    6    0    0    0    0  862    0    0    0
     0    3    0]
 [   0    0    4    0    0    0    2    0    0    0    8 2194    0    1
     1    0    0]
 [   0    0    0    0    1    3    0    0    0    0    0    0  523    0
     0    2    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1132    5    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    20  324    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.31978319783198

F1 scores:
[       nan 0.98765432 0.98588235 0.96551724 0.99765808 0.96644295
 0.99544765 0.98039216 0.995338   0.86486486 0.98177677 0.99456029
 0.96139706 0.98666667 0.98735281 0.94323144 0.97109827]

Kappa:
0.9808521844211002
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f84fba356d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.165, val_acc:0.570]
Epoch [2/120    avg_loss:1.517, val_acc:0.673]
Epoch [3/120    avg_loss:1.314, val_acc:0.650]
Epoch [4/120    avg_loss:1.076, val_acc:0.613]
Epoch [5/120    avg_loss:0.908, val_acc:0.743]
Epoch [6/120    avg_loss:0.729, val_acc:0.771]
Epoch [7/120    avg_loss:0.630, val_acc:0.791]
Epoch [8/120    avg_loss:0.650, val_acc:0.746]
Epoch [9/120    avg_loss:0.620, val_acc:0.812]
Epoch [10/120    avg_loss:0.605, val_acc:0.770]
Epoch [11/120    avg_loss:0.440, val_acc:0.841]
Epoch [12/120    avg_loss:0.515, val_acc:0.700]
Epoch [13/120    avg_loss:0.429, val_acc:0.836]
Epoch [14/120    avg_loss:0.352, val_acc:0.867]
Epoch [15/120    avg_loss:0.354, val_acc:0.839]
Epoch [16/120    avg_loss:0.393, val_acc:0.877]
Epoch [17/120    avg_loss:0.281, val_acc:0.898]
Epoch [18/120    avg_loss:0.319, val_acc:0.898]
Epoch [19/120    avg_loss:0.319, val_acc:0.908]
Epoch [20/120    avg_loss:0.235, val_acc:0.843]
Epoch [21/120    avg_loss:0.307, val_acc:0.885]
Epoch [22/120    avg_loss:0.252, val_acc:0.913]
Epoch [23/120    avg_loss:0.224, val_acc:0.908]
Epoch [24/120    avg_loss:0.177, val_acc:0.904]
Epoch [25/120    avg_loss:0.179, val_acc:0.896]
Epoch [26/120    avg_loss:0.198, val_acc:0.875]
Epoch [27/120    avg_loss:0.248, val_acc:0.893]
Epoch [28/120    avg_loss:0.152, val_acc:0.936]
Epoch [29/120    avg_loss:0.127, val_acc:0.934]
Epoch [30/120    avg_loss:0.156, val_acc:0.934]
Epoch [31/120    avg_loss:0.145, val_acc:0.921]
Epoch [32/120    avg_loss:0.211, val_acc:0.923]
Epoch [33/120    avg_loss:0.116, val_acc:0.929]
Epoch [34/120    avg_loss:0.076, val_acc:0.945]
Epoch [35/120    avg_loss:0.134, val_acc:0.921]
Epoch [36/120    avg_loss:0.133, val_acc:0.947]
Epoch [37/120    avg_loss:0.082, val_acc:0.948]
Epoch [38/120    avg_loss:0.095, val_acc:0.941]
Epoch [39/120    avg_loss:0.081, val_acc:0.954]
Epoch [40/120    avg_loss:0.084, val_acc:0.954]
Epoch [41/120    avg_loss:0.074, val_acc:0.951]
Epoch [42/120    avg_loss:0.072, val_acc:0.955]
Epoch [43/120    avg_loss:0.102, val_acc:0.936]
Epoch [44/120    avg_loss:0.159, val_acc:0.910]
Epoch [45/120    avg_loss:0.093, val_acc:0.956]
Epoch [46/120    avg_loss:0.063, val_acc:0.957]
Epoch [47/120    avg_loss:0.056, val_acc:0.953]
Epoch [48/120    avg_loss:0.067, val_acc:0.945]
Epoch [49/120    avg_loss:0.039, val_acc:0.953]
Epoch [50/120    avg_loss:0.064, val_acc:0.958]
Epoch [51/120    avg_loss:0.061, val_acc:0.953]
Epoch [52/120    avg_loss:0.064, val_acc:0.957]
Epoch [53/120    avg_loss:0.038, val_acc:0.952]
Epoch [54/120    avg_loss:0.036, val_acc:0.955]
Epoch [55/120    avg_loss:0.033, val_acc:0.952]
Epoch [56/120    avg_loss:0.028, val_acc:0.967]
Epoch [57/120    avg_loss:0.029, val_acc:0.964]
Epoch [58/120    avg_loss:0.032, val_acc:0.962]
Epoch [59/120    avg_loss:0.046, val_acc:0.959]
Epoch [60/120    avg_loss:0.041, val_acc:0.966]
Epoch [61/120    avg_loss:0.041, val_acc:0.962]
Epoch [62/120    avg_loss:0.030, val_acc:0.966]
Epoch [63/120    avg_loss:0.023, val_acc:0.973]
Epoch [64/120    avg_loss:0.021, val_acc:0.965]
Epoch [65/120    avg_loss:0.028, val_acc:0.948]
Epoch [66/120    avg_loss:0.037, val_acc:0.963]
Epoch [67/120    avg_loss:0.030, val_acc:0.962]
Epoch [68/120    avg_loss:0.044, val_acc:0.964]
Epoch [69/120    avg_loss:0.031, val_acc:0.963]
Epoch [70/120    avg_loss:0.063, val_acc:0.944]
Epoch [71/120    avg_loss:0.086, val_acc:0.965]
Epoch [72/120    avg_loss:0.041, val_acc:0.968]
Epoch [73/120    avg_loss:0.028, val_acc:0.973]
Epoch [74/120    avg_loss:0.024, val_acc:0.976]
Epoch [75/120    avg_loss:0.017, val_acc:0.966]
Epoch [76/120    avg_loss:0.039, val_acc:0.967]
Epoch [77/120    avg_loss:0.035, val_acc:0.970]
Epoch [78/120    avg_loss:0.017, val_acc:0.976]
Epoch [79/120    avg_loss:0.010, val_acc:0.971]
Epoch [80/120    avg_loss:0.013, val_acc:0.970]
Epoch [81/120    avg_loss:0.009, val_acc:0.979]
Epoch [82/120    avg_loss:0.006, val_acc:0.977]
Epoch [83/120    avg_loss:0.012, val_acc:0.979]
Epoch [84/120    avg_loss:0.005, val_acc:0.977]
Epoch [85/120    avg_loss:0.012, val_acc:0.969]
Epoch [86/120    avg_loss:0.045, val_acc:0.956]
Epoch [87/120    avg_loss:0.018, val_acc:0.967]
Epoch [88/120    avg_loss:0.019, val_acc:0.978]
Epoch [89/120    avg_loss:0.011, val_acc:0.976]
Epoch [90/120    avg_loss:0.010, val_acc:0.971]
Epoch [91/120    avg_loss:0.007, val_acc:0.976]
Epoch [92/120    avg_loss:0.005, val_acc:0.978]
Epoch [93/120    avg_loss:0.008, val_acc:0.977]
Epoch [94/120    avg_loss:0.007, val_acc:0.975]
Epoch [95/120    avg_loss:0.008, val_acc:0.978]
Epoch [96/120    avg_loss:0.017, val_acc:0.970]
Epoch [97/120    avg_loss:0.010, val_acc:0.970]
Epoch [98/120    avg_loss:0.011, val_acc:0.976]
Epoch [99/120    avg_loss:0.007, val_acc:0.977]
Epoch [100/120    avg_loss:0.008, val_acc:0.977]
Epoch [101/120    avg_loss:0.007, val_acc:0.976]
Epoch [102/120    avg_loss:0.006, val_acc:0.978]
Epoch [103/120    avg_loss:0.005, val_acc:0.979]
Epoch [104/120    avg_loss:0.005, val_acc:0.978]
Epoch [105/120    avg_loss:0.005, val_acc:0.978]
Epoch [106/120    avg_loss:0.006, val_acc:0.979]
Epoch [107/120    avg_loss:0.004, val_acc:0.980]
Epoch [108/120    avg_loss:0.004, val_acc:0.980]
Epoch [109/120    avg_loss:0.007, val_acc:0.979]
Epoch [110/120    avg_loss:0.005, val_acc:0.980]
Epoch [111/120    avg_loss:0.006, val_acc:0.979]
Epoch [112/120    avg_loss:0.005, val_acc:0.980]
Epoch [113/120    avg_loss:0.004, val_acc:0.979]
Epoch [114/120    avg_loss:0.004, val_acc:0.980]
Epoch [115/120    avg_loss:0.005, val_acc:0.980]
Epoch [116/120    avg_loss:0.004, val_acc:0.980]
Epoch [117/120    avg_loss:0.005, val_acc:0.978]
Epoch [118/120    avg_loss:0.004, val_acc:0.981]
Epoch [119/120    avg_loss:0.007, val_acc:0.981]
Epoch [120/120    avg_loss:0.005, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1263    5    0    0    1    0    0    0    5    6    3    0
     2    0    0]
 [   0    0    0  720    1    7    0    0    0    3    0    0   14    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1   32    0    5    0    0    0    0  801   28    0    0
     4    4    0]
 [   0    0    7    0    0    0    3    0    0    0    4 2196    0    0
     0    0    0]
 [   0    0    0    4   12    3    0    0    0    0    7    0  507    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1131    4    0]
 [   0    0    0    0    0    0    0    0    0    0    0    4    0    0
    72  271    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.25745257452574

F1 scores:
[       nan 1.         0.98826291 0.95490716 0.97038724 0.97959184
 0.99620349 1.         0.99883856 0.87804878 0.94513274 0.98807649
 0.95750708 0.99462366 0.96296296 0.8658147  0.98809524]

Kappa:
0.9687060946074927
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f74064576d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.135, val_acc:0.530]
Epoch [2/120    avg_loss:1.591, val_acc:0.670]
Epoch [3/120    avg_loss:1.158, val_acc:0.713]
Epoch [4/120    avg_loss:1.029, val_acc:0.692]
Epoch [5/120    avg_loss:0.821, val_acc:0.758]
Epoch [6/120    avg_loss:0.917, val_acc:0.789]
Epoch [7/120    avg_loss:0.797, val_acc:0.747]
Epoch [8/120    avg_loss:0.734, val_acc:0.790]
Epoch [9/120    avg_loss:0.577, val_acc:0.819]
Epoch [10/120    avg_loss:0.573, val_acc:0.804]
Epoch [11/120    avg_loss:0.526, val_acc:0.844]
Epoch [12/120    avg_loss:0.458, val_acc:0.855]
Epoch [13/120    avg_loss:0.461, val_acc:0.802]
Epoch [14/120    avg_loss:0.447, val_acc:0.881]
Epoch [15/120    avg_loss:0.352, val_acc:0.884]
Epoch [16/120    avg_loss:0.319, val_acc:0.886]
Epoch [17/120    avg_loss:0.350, val_acc:0.903]
Epoch [18/120    avg_loss:0.267, val_acc:0.910]
Epoch [19/120    avg_loss:0.316, val_acc:0.881]
Epoch [20/120    avg_loss:0.238, val_acc:0.877]
Epoch [21/120    avg_loss:0.235, val_acc:0.913]
Epoch [22/120    avg_loss:0.234, val_acc:0.895]
Epoch [23/120    avg_loss:0.195, val_acc:0.935]
Epoch [24/120    avg_loss:0.219, val_acc:0.935]
Epoch [25/120    avg_loss:0.134, val_acc:0.944]
Epoch [26/120    avg_loss:0.202, val_acc:0.914]
Epoch [27/120    avg_loss:0.157, val_acc:0.946]
Epoch [28/120    avg_loss:0.133, val_acc:0.947]
Epoch [29/120    avg_loss:0.095, val_acc:0.948]
Epoch [30/120    avg_loss:0.129, val_acc:0.924]
Epoch [31/120    avg_loss:0.136, val_acc:0.957]
Epoch [32/120    avg_loss:0.087, val_acc:0.957]
Epoch [33/120    avg_loss:0.135, val_acc:0.937]
Epoch [34/120    avg_loss:0.147, val_acc:0.947]
Epoch [35/120    avg_loss:0.116, val_acc:0.937]
Epoch [36/120    avg_loss:0.096, val_acc:0.919]
Epoch [37/120    avg_loss:0.079, val_acc:0.969]
Epoch [38/120    avg_loss:0.109, val_acc:0.935]
Epoch [39/120    avg_loss:0.090, val_acc:0.956]
Epoch [40/120    avg_loss:0.123, val_acc:0.958]
Epoch [41/120    avg_loss:0.107, val_acc:0.957]
Epoch [42/120    avg_loss:0.068, val_acc:0.956]
Epoch [43/120    avg_loss:0.111, val_acc:0.963]
Epoch [44/120    avg_loss:0.114, val_acc:0.926]
Epoch [45/120    avg_loss:0.117, val_acc:0.954]
Epoch [46/120    avg_loss:0.063, val_acc:0.962]
Epoch [47/120    avg_loss:0.048, val_acc:0.965]
Epoch [48/120    avg_loss:0.117, val_acc:0.935]
Epoch [49/120    avg_loss:0.079, val_acc:0.960]
Epoch [50/120    avg_loss:0.074, val_acc:0.967]
Epoch [51/120    avg_loss:0.043, val_acc:0.975]
Epoch [52/120    avg_loss:0.035, val_acc:0.979]
Epoch [53/120    avg_loss:0.030, val_acc:0.981]
Epoch [54/120    avg_loss:0.032, val_acc:0.986]
Epoch [55/120    avg_loss:0.027, val_acc:0.982]
Epoch [56/120    avg_loss:0.030, val_acc:0.985]
Epoch [57/120    avg_loss:0.035, val_acc:0.983]
Epoch [58/120    avg_loss:0.029, val_acc:0.982]
Epoch [59/120    avg_loss:0.024, val_acc:0.986]
Epoch [60/120    avg_loss:0.028, val_acc:0.983]
Epoch [61/120    avg_loss:0.025, val_acc:0.983]
Epoch [62/120    avg_loss:0.023, val_acc:0.985]
Epoch [63/120    avg_loss:0.025, val_acc:0.983]
Epoch [64/120    avg_loss:0.028, val_acc:0.985]
Epoch [65/120    avg_loss:0.023, val_acc:0.985]
Epoch [66/120    avg_loss:0.020, val_acc:0.983]
Epoch [67/120    avg_loss:0.029, val_acc:0.985]
Epoch [68/120    avg_loss:0.018, val_acc:0.983]
Epoch [69/120    avg_loss:0.021, val_acc:0.982]
Epoch [70/120    avg_loss:0.019, val_acc:0.982]
Epoch [71/120    avg_loss:0.024, val_acc:0.983]
Epoch [72/120    avg_loss:0.023, val_acc:0.983]
Epoch [73/120    avg_loss:0.020, val_acc:0.983]
Epoch [74/120    avg_loss:0.021, val_acc:0.985]
Epoch [75/120    avg_loss:0.015, val_acc:0.985]
Epoch [76/120    avg_loss:0.020, val_acc:0.985]
Epoch [77/120    avg_loss:0.018, val_acc:0.985]
Epoch [78/120    avg_loss:0.020, val_acc:0.985]
Epoch [79/120    avg_loss:0.019, val_acc:0.985]
Epoch [80/120    avg_loss:0.021, val_acc:0.983]
Epoch [81/120    avg_loss:0.020, val_acc:0.983]
Epoch [82/120    avg_loss:0.020, val_acc:0.983]
Epoch [83/120    avg_loss:0.020, val_acc:0.983]
Epoch [84/120    avg_loss:0.019, val_acc:0.983]
Epoch [85/120    avg_loss:0.018, val_acc:0.983]
Epoch [86/120    avg_loss:0.023, val_acc:0.983]
Epoch [87/120    avg_loss:0.017, val_acc:0.983]
Epoch [88/120    avg_loss:0.022, val_acc:0.983]
Epoch [89/120    avg_loss:0.022, val_acc:0.983]
Epoch [90/120    avg_loss:0.018, val_acc:0.983]
Epoch [91/120    avg_loss:0.020, val_acc:0.983]
Epoch [92/120    avg_loss:0.023, val_acc:0.983]
Epoch [93/120    avg_loss:0.017, val_acc:0.983]
Epoch [94/120    avg_loss:0.019, val_acc:0.983]
Epoch [95/120    avg_loss:0.019, val_acc:0.983]
Epoch [96/120    avg_loss:0.021, val_acc:0.983]
Epoch [97/120    avg_loss:0.017, val_acc:0.983]
Epoch [98/120    avg_loss:0.018, val_acc:0.983]
Epoch [99/120    avg_loss:0.022, val_acc:0.983]
Epoch [100/120    avg_loss:0.018, val_acc:0.983]
Epoch [101/120    avg_loss:0.019, val_acc:0.983]
Epoch [102/120    avg_loss:0.020, val_acc:0.983]
Epoch [103/120    avg_loss:0.021, val_acc:0.983]
Epoch [104/120    avg_loss:0.016, val_acc:0.983]
Epoch [105/120    avg_loss:0.021, val_acc:0.983]
Epoch [106/120    avg_loss:0.022, val_acc:0.983]
Epoch [107/120    avg_loss:0.016, val_acc:0.983]
Epoch [108/120    avg_loss:0.023, val_acc:0.983]
Epoch [109/120    avg_loss:0.016, val_acc:0.983]
Epoch [110/120    avg_loss:0.019, val_acc:0.983]
Epoch [111/120    avg_loss:0.018, val_acc:0.983]
Epoch [112/120    avg_loss:0.031, val_acc:0.983]
Epoch [113/120    avg_loss:0.020, val_acc:0.983]
Epoch [114/120    avg_loss:0.018, val_acc:0.983]
Epoch [115/120    avg_loss:0.023, val_acc:0.983]
Epoch [116/120    avg_loss:0.015, val_acc:0.983]
Epoch [117/120    avg_loss:0.026, val_acc:0.983]
Epoch [118/120    avg_loss:0.019, val_acc:0.983]
Epoch [119/120    avg_loss:0.021, val_acc:0.983]
Epoch [120/120    avg_loss:0.015, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1258    8    0    0    1    0    0    2    8    8    0    0
     0    0    0]
 [   0    0    1  713    0   15    0    0    0    7    5    0    3    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   14   20    0   10    0    0    0    0  809   17    2    0
     0    3    0]
 [   0    0   21    0    0    0    5    0    0    0    4 2166   12    2
     0    0    0]
 [   0    0    0    9    3   13    0    0    0    0    2   15  485    0
     0    1    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1134    4    0]
 [   0    0    0    0    0    0   20    0    0    0    0    0    0    0
    21  306    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.04065040650407

F1 scores:
[       nan 0.975      0.97557193 0.95257181 0.99300699 0.95469613
 0.9798357  1.         1.         0.7826087  0.94841735 0.98075617
 0.93539055 0.98666667 0.98780488 0.92586989 0.95953757]

Kappa:
0.9662630304636693
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:14:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8d22220710>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.122, val_acc:0.541]
Epoch [2/120    avg_loss:1.527, val_acc:0.676]
Epoch [3/120    avg_loss:1.252, val_acc:0.720]
Epoch [4/120    avg_loss:1.088, val_acc:0.777]
Epoch [5/120    avg_loss:0.833, val_acc:0.806]
Epoch [6/120    avg_loss:0.756, val_acc:0.760]
Epoch [7/120    avg_loss:0.700, val_acc:0.774]
Epoch [8/120    avg_loss:0.704, val_acc:0.784]
Epoch [9/120    avg_loss:0.582, val_acc:0.850]
Epoch [10/120    avg_loss:0.443, val_acc:0.873]
Epoch [11/120    avg_loss:0.530, val_acc:0.850]
Epoch [12/120    avg_loss:0.411, val_acc:0.860]
Epoch [13/120    avg_loss:0.348, val_acc:0.860]
Epoch [14/120    avg_loss:0.346, val_acc:0.861]
Epoch [15/120    avg_loss:0.349, val_acc:0.892]
Epoch [16/120    avg_loss:0.303, val_acc:0.884]
Epoch [17/120    avg_loss:0.303, val_acc:0.852]
Epoch [18/120    avg_loss:0.252, val_acc:0.892]
Epoch [19/120    avg_loss:0.244, val_acc:0.916]
Epoch [20/120    avg_loss:0.214, val_acc:0.907]
Epoch [21/120    avg_loss:0.228, val_acc:0.921]
Epoch [22/120    avg_loss:0.160, val_acc:0.902]
Epoch [23/120    avg_loss:0.201, val_acc:0.910]
Epoch [24/120    avg_loss:0.162, val_acc:0.932]
Epoch [25/120    avg_loss:0.213, val_acc:0.916]
Epoch [26/120    avg_loss:0.183, val_acc:0.905]
Epoch [27/120    avg_loss:0.142, val_acc:0.931]
Epoch [28/120    avg_loss:0.172, val_acc:0.900]
Epoch [29/120    avg_loss:0.176, val_acc:0.930]
Epoch [30/120    avg_loss:0.145, val_acc:0.929]
Epoch [31/120    avg_loss:0.114, val_acc:0.950]
Epoch [32/120    avg_loss:0.147, val_acc:0.916]
Epoch [33/120    avg_loss:0.135, val_acc:0.944]
Epoch [34/120    avg_loss:0.094, val_acc:0.944]
Epoch [35/120    avg_loss:0.100, val_acc:0.946]
Epoch [36/120    avg_loss:0.098, val_acc:0.937]
Epoch [37/120    avg_loss:0.072, val_acc:0.946]
Epoch [38/120    avg_loss:0.080, val_acc:0.954]
Epoch [39/120    avg_loss:0.107, val_acc:0.944]
Epoch [40/120    avg_loss:0.080, val_acc:0.968]
Epoch [41/120    avg_loss:0.058, val_acc:0.957]
Epoch [42/120    avg_loss:0.068, val_acc:0.926]
Epoch [43/120    avg_loss:0.069, val_acc:0.966]
Epoch [44/120    avg_loss:0.059, val_acc:0.958]
Epoch [45/120    avg_loss:0.034, val_acc:0.961]
Epoch [46/120    avg_loss:0.045, val_acc:0.962]
Epoch [47/120    avg_loss:0.070, val_acc:0.959]
Epoch [48/120    avg_loss:0.108, val_acc:0.955]
Epoch [49/120    avg_loss:0.123, val_acc:0.946]
Epoch [50/120    avg_loss:0.054, val_acc:0.964]
Epoch [51/120    avg_loss:0.046, val_acc:0.952]
Epoch [52/120    avg_loss:0.055, val_acc:0.965]
Epoch [53/120    avg_loss:0.060, val_acc:0.956]
Epoch [54/120    avg_loss:0.038, val_acc:0.977]
Epoch [55/120    avg_loss:0.020, val_acc:0.980]
Epoch [56/120    avg_loss:0.022, val_acc:0.980]
Epoch [57/120    avg_loss:0.022, val_acc:0.981]
Epoch [58/120    avg_loss:0.018, val_acc:0.982]
Epoch [59/120    avg_loss:0.021, val_acc:0.983]
Epoch [60/120    avg_loss:0.017, val_acc:0.985]
Epoch [61/120    avg_loss:0.018, val_acc:0.986]
Epoch [62/120    avg_loss:0.016, val_acc:0.985]
Epoch [63/120    avg_loss:0.015, val_acc:0.984]
Epoch [64/120    avg_loss:0.016, val_acc:0.984]
Epoch [65/120    avg_loss:0.016, val_acc:0.982]
Epoch [66/120    avg_loss:0.013, val_acc:0.984]
Epoch [67/120    avg_loss:0.014, val_acc:0.984]
Epoch [68/120    avg_loss:0.014, val_acc:0.984]
Epoch [69/120    avg_loss:0.013, val_acc:0.986]
Epoch [70/120    avg_loss:0.015, val_acc:0.985]
Epoch [71/120    avg_loss:0.016, val_acc:0.985]
Epoch [72/120    avg_loss:0.012, val_acc:0.985]
Epoch [73/120    avg_loss:0.017, val_acc:0.983]
Epoch [74/120    avg_loss:0.012, val_acc:0.985]
Epoch [75/120    avg_loss:0.014, val_acc:0.984]
Epoch [76/120    avg_loss:0.013, val_acc:0.984]
Epoch [77/120    avg_loss:0.014, val_acc:0.985]
Epoch [78/120    avg_loss:0.014, val_acc:0.985]
Epoch [79/120    avg_loss:0.014, val_acc:0.984]
Epoch [80/120    avg_loss:0.013, val_acc:0.983]
Epoch [81/120    avg_loss:0.016, val_acc:0.983]
Epoch [82/120    avg_loss:0.012, val_acc:0.983]
Epoch [83/120    avg_loss:0.015, val_acc:0.983]
Epoch [84/120    avg_loss:0.013, val_acc:0.983]
Epoch [85/120    avg_loss:0.014, val_acc:0.983]
Epoch [86/120    avg_loss:0.014, val_acc:0.983]
Epoch [87/120    avg_loss:0.011, val_acc:0.983]
Epoch [88/120    avg_loss:0.012, val_acc:0.983]
Epoch [89/120    avg_loss:0.012, val_acc:0.983]
Epoch [90/120    avg_loss:0.010, val_acc:0.983]
Epoch [91/120    avg_loss:0.013, val_acc:0.983]
Epoch [92/120    avg_loss:0.011, val_acc:0.983]
Epoch [93/120    avg_loss:0.013, val_acc:0.983]
Epoch [94/120    avg_loss:0.012, val_acc:0.983]
Epoch [95/120    avg_loss:0.011, val_acc:0.984]
Epoch [96/120    avg_loss:0.013, val_acc:0.984]
Epoch [97/120    avg_loss:0.014, val_acc:0.984]
Epoch [98/120    avg_loss:0.011, val_acc:0.984]
Epoch [99/120    avg_loss:0.013, val_acc:0.984]
Epoch [100/120    avg_loss:0.012, val_acc:0.984]
Epoch [101/120    avg_loss:0.014, val_acc:0.984]
Epoch [102/120    avg_loss:0.011, val_acc:0.983]
Epoch [103/120    avg_loss:0.011, val_acc:0.984]
Epoch [104/120    avg_loss:0.013, val_acc:0.984]
Epoch [105/120    avg_loss:0.011, val_acc:0.985]
Epoch [106/120    avg_loss:0.012, val_acc:0.985]
Epoch [107/120    avg_loss:0.011, val_acc:0.985]
Epoch [108/120    avg_loss:0.011, val_acc:0.985]
Epoch [109/120    avg_loss:0.014, val_acc:0.985]
Epoch [110/120    avg_loss:0.014, val_acc:0.985]
Epoch [111/120    avg_loss:0.012, val_acc:0.985]
Epoch [112/120    avg_loss:0.010, val_acc:0.985]
Epoch [113/120    avg_loss:0.011, val_acc:0.985]
Epoch [114/120    avg_loss:0.010, val_acc:0.985]
Epoch [115/120    avg_loss:0.019, val_acc:0.985]
Epoch [116/120    avg_loss:0.012, val_acc:0.985]
Epoch [117/120    avg_loss:0.015, val_acc:0.985]
Epoch [118/120    avg_loss:0.011, val_acc:0.985]
Epoch [119/120    avg_loss:0.012, val_acc:0.985]
Epoch [120/120    avg_loss:0.015, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1246   11    0    1    0    0    0    0   10   16    0    0
     0    1    0]
 [   0    0    0  717    0   17    0    0    0    5    1    0    6    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0  655    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    2   32    0    6    0    0    0    0  806   17    5    0
     0    7    0]
 [   0    0   11    0    0    0    5    0    0    0   14 2179    0    1
     0    0    0]
 [   0    0    0    9    9    6    0    0    0    0    0    0  505    0
     1    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0    0    0
  1135    2    0]
 [   0    0    0    0    0    0    8    0    0    0    0    1    0    0
    36  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.22493224932249

F1 scores:
[       nan 0.975      0.97917485 0.94528675 0.97931034 0.96436526
 0.98867925 1.         0.99883586 0.80952381 0.94269006 0.98508137
 0.96007605 0.99462366 0.98225876 0.91654021 0.97076023]

Kappa:
0.9683628371043798
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc07f9c96d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.137, val_acc:0.548]
Epoch [2/120    avg_loss:1.598, val_acc:0.627]
Epoch [3/120    avg_loss:1.183, val_acc:0.693]
Epoch [4/120    avg_loss:0.993, val_acc:0.727]
Epoch [5/120    avg_loss:0.865, val_acc:0.751]
Epoch [6/120    avg_loss:0.808, val_acc:0.783]
Epoch [7/120    avg_loss:0.668, val_acc:0.788]
Epoch [8/120    avg_loss:0.646, val_acc:0.785]
Epoch [9/120    avg_loss:0.584, val_acc:0.783]
Epoch [10/120    avg_loss:0.529, val_acc:0.833]
Epoch [11/120    avg_loss:0.467, val_acc:0.843]
Epoch [12/120    avg_loss:0.477, val_acc:0.799]
Epoch [13/120    avg_loss:0.414, val_acc:0.860]
Epoch [14/120    avg_loss:0.385, val_acc:0.864]
Epoch [15/120    avg_loss:0.402, val_acc:0.871]
Epoch [16/120    avg_loss:0.303, val_acc:0.899]
Epoch [17/120    avg_loss:0.346, val_acc:0.870]
Epoch [18/120    avg_loss:0.263, val_acc:0.919]
Epoch [19/120    avg_loss:0.309, val_acc:0.885]
Epoch [20/120    avg_loss:0.393, val_acc:0.877]
Epoch [21/120    avg_loss:0.239, val_acc:0.925]
Epoch [22/120    avg_loss:0.215, val_acc:0.910]
Epoch [23/120    avg_loss:0.244, val_acc:0.916]
Epoch [24/120    avg_loss:0.234, val_acc:0.912]
Epoch [25/120    avg_loss:0.180, val_acc:0.905]
Epoch [26/120    avg_loss:0.197, val_acc:0.907]
Epoch [27/120    avg_loss:0.254, val_acc:0.906]
Epoch [28/120    avg_loss:0.206, val_acc:0.932]
Epoch [29/120    avg_loss:0.232, val_acc:0.933]
Epoch [30/120    avg_loss:0.157, val_acc:0.934]
Epoch [31/120    avg_loss:0.119, val_acc:0.933]
Epoch [32/120    avg_loss:0.129, val_acc:0.943]
Epoch [33/120    avg_loss:0.127, val_acc:0.932]
Epoch [34/120    avg_loss:0.125, val_acc:0.941]
Epoch [35/120    avg_loss:0.131, val_acc:0.920]
Epoch [36/120    avg_loss:0.136, val_acc:0.935]
Epoch [37/120    avg_loss:0.148, val_acc:0.899]
Epoch [38/120    avg_loss:0.154, val_acc:0.946]
Epoch [39/120    avg_loss:0.159, val_acc:0.928]
Epoch [40/120    avg_loss:0.108, val_acc:0.938]
Epoch [41/120    avg_loss:0.077, val_acc:0.948]
Epoch [42/120    avg_loss:0.071, val_acc:0.954]
Epoch [43/120    avg_loss:0.059, val_acc:0.938]
Epoch [44/120    avg_loss:0.083, val_acc:0.947]
Epoch [45/120    avg_loss:0.135, val_acc:0.944]
Epoch [46/120    avg_loss:0.093, val_acc:0.942]
Epoch [47/120    avg_loss:0.072, val_acc:0.962]
Epoch [48/120    avg_loss:0.037, val_acc:0.939]
Epoch [49/120    avg_loss:0.072, val_acc:0.958]
Epoch [50/120    avg_loss:0.048, val_acc:0.963]
Epoch [51/120    avg_loss:0.064, val_acc:0.967]
Epoch [52/120    avg_loss:0.051, val_acc:0.966]
Epoch [53/120    avg_loss:0.052, val_acc:0.958]
Epoch [54/120    avg_loss:0.053, val_acc:0.947]
Epoch [55/120    avg_loss:0.039, val_acc:0.961]
Epoch [56/120    avg_loss:0.038, val_acc:0.958]
Epoch [57/120    avg_loss:0.178, val_acc:0.948]
Epoch [58/120    avg_loss:0.083, val_acc:0.955]
Epoch [59/120    avg_loss:0.061, val_acc:0.964]
Epoch [60/120    avg_loss:0.033, val_acc:0.967]
Epoch [61/120    avg_loss:0.052, val_acc:0.967]
Epoch [62/120    avg_loss:0.057, val_acc:0.963]
Epoch [63/120    avg_loss:0.090, val_acc:0.958]
Epoch [64/120    avg_loss:0.050, val_acc:0.964]
Epoch [65/120    avg_loss:0.032, val_acc:0.974]
Epoch [66/120    avg_loss:0.060, val_acc:0.970]
Epoch [67/120    avg_loss:0.028, val_acc:0.966]
Epoch [68/120    avg_loss:0.025, val_acc:0.979]
Epoch [69/120    avg_loss:0.020, val_acc:0.968]
Epoch [70/120    avg_loss:0.039, val_acc:0.961]
Epoch [71/120    avg_loss:0.034, val_acc:0.965]
Epoch [72/120    avg_loss:0.033, val_acc:0.970]
Epoch [73/120    avg_loss:0.030, val_acc:0.966]
Epoch [74/120    avg_loss:0.029, val_acc:0.959]
Epoch [75/120    avg_loss:0.074, val_acc:0.945]
Epoch [76/120    avg_loss:0.051, val_acc:0.968]
Epoch [77/120    avg_loss:0.061, val_acc:0.957]
Epoch [78/120    avg_loss:0.067, val_acc:0.979]
Epoch [79/120    avg_loss:0.032, val_acc:0.974]
Epoch [80/120    avg_loss:0.023, val_acc:0.976]
Epoch [81/120    avg_loss:0.019, val_acc:0.970]
Epoch [82/120    avg_loss:0.023, val_acc:0.971]
Epoch [83/120    avg_loss:0.017, val_acc:0.977]
Epoch [84/120    avg_loss:0.013, val_acc:0.974]
Epoch [85/120    avg_loss:0.011, val_acc:0.975]
Epoch [86/120    avg_loss:0.015, val_acc:0.976]
Epoch [87/120    avg_loss:0.009, val_acc:0.977]
Epoch [88/120    avg_loss:0.007, val_acc:0.985]
Epoch [89/120    avg_loss:0.011, val_acc:0.976]
Epoch [90/120    avg_loss:0.014, val_acc:0.981]
Epoch [91/120    avg_loss:0.018, val_acc:0.973]
Epoch [92/120    avg_loss:0.014, val_acc:0.978]
Epoch [93/120    avg_loss:0.008, val_acc:0.978]
Epoch [94/120    avg_loss:0.010, val_acc:0.978]
Epoch [95/120    avg_loss:0.009, val_acc:0.980]
Epoch [96/120    avg_loss:0.007, val_acc:0.982]
Epoch [97/120    avg_loss:0.006, val_acc:0.982]
Epoch [98/120    avg_loss:0.009, val_acc:0.979]
Epoch [99/120    avg_loss:0.008, val_acc:0.976]
Epoch [100/120    avg_loss:0.020, val_acc:0.971]
Epoch [101/120    avg_loss:0.012, val_acc:0.981]
Epoch [102/120    avg_loss:0.012, val_acc:0.982]
Epoch [103/120    avg_loss:0.006, val_acc:0.985]
Epoch [104/120    avg_loss:0.007, val_acc:0.985]
Epoch [105/120    avg_loss:0.005, val_acc:0.984]
Epoch [106/120    avg_loss:0.032, val_acc:0.982]
Epoch [107/120    avg_loss:0.006, val_acc:0.982]
Epoch [108/120    avg_loss:0.005, val_acc:0.982]
Epoch [109/120    avg_loss:0.006, val_acc:0.984]
Epoch [110/120    avg_loss:0.005, val_acc:0.984]
Epoch [111/120    avg_loss:0.006, val_acc:0.985]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.004, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.004, val_acc:0.987]
Epoch [117/120    avg_loss:0.006, val_acc:0.986]
Epoch [118/120    avg_loss:0.004, val_acc:0.985]
Epoch [119/120    avg_loss:0.005, val_acc:0.985]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1276    0    0    0    0    0    0    0    3    5    1    0
     0    0    0]
 [   0    0    0  719    0    8    0    0    0    5    1    0    8    6
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    4    0    0    0    0  851   14    0    0
     0    2    0]
 [   0    0    6    0    0    2    3    0    0    0    4 2181   13    1
     0    0    0]
 [   0    0    0    2   12    5    0    0    0    0   14    2  491    0
     5    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    50  291    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.85365853658537

F1 scores:
[       nan 0.98765432 0.99376947 0.97690217 0.97260274 0.9740113
 0.99319728 0.98039216 0.99883856 0.81818182 0.97257143 0.98866727
 0.9370229  0.98143236 0.9751073  0.909375   0.97647059]

Kappa:
0.9755264335754473
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdafea95748>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.155, val_acc:0.546]
Epoch [2/120    avg_loss:1.495, val_acc:0.629]
Epoch [3/120    avg_loss:1.152, val_acc:0.664]
Epoch [4/120    avg_loss:0.997, val_acc:0.727]
Epoch [5/120    avg_loss:0.832, val_acc:0.763]
Epoch [6/120    avg_loss:0.790, val_acc:0.650]
Epoch [7/120    avg_loss:0.718, val_acc:0.820]
Epoch [8/120    avg_loss:0.555, val_acc:0.763]
Epoch [9/120    avg_loss:0.573, val_acc:0.772]
Epoch [10/120    avg_loss:0.454, val_acc:0.860]
Epoch [11/120    avg_loss:0.413, val_acc:0.852]
Epoch [12/120    avg_loss:0.368, val_acc:0.852]
Epoch [13/120    avg_loss:0.420, val_acc:0.857]
Epoch [14/120    avg_loss:0.432, val_acc:0.836]
Epoch [15/120    avg_loss:0.565, val_acc:0.905]
Epoch [16/120    avg_loss:0.285, val_acc:0.846]
Epoch [17/120    avg_loss:0.352, val_acc:0.873]
Epoch [18/120    avg_loss:0.298, val_acc:0.862]
Epoch [19/120    avg_loss:0.298, val_acc:0.890]
Epoch [20/120    avg_loss:0.216, val_acc:0.878]
Epoch [21/120    avg_loss:0.189, val_acc:0.924]
Epoch [22/120    avg_loss:0.225, val_acc:0.910]
Epoch [23/120    avg_loss:0.194, val_acc:0.922]
Epoch [24/120    avg_loss:0.214, val_acc:0.907]
Epoch [25/120    avg_loss:0.217, val_acc:0.936]
Epoch [26/120    avg_loss:0.129, val_acc:0.944]
Epoch [27/120    avg_loss:0.132, val_acc:0.944]
Epoch [28/120    avg_loss:0.135, val_acc:0.939]
Epoch [29/120    avg_loss:0.112, val_acc:0.946]
Epoch [30/120    avg_loss:0.089, val_acc:0.935]
Epoch [31/120    avg_loss:0.105, val_acc:0.931]
Epoch [32/120    avg_loss:0.135, val_acc:0.943]
Epoch [33/120    avg_loss:0.120, val_acc:0.939]
Epoch [34/120    avg_loss:0.093, val_acc:0.945]
Epoch [35/120    avg_loss:0.085, val_acc:0.953]
Epoch [36/120    avg_loss:0.077, val_acc:0.926]
Epoch [37/120    avg_loss:0.106, val_acc:0.961]
Epoch [38/120    avg_loss:0.089, val_acc:0.951]
Epoch [39/120    avg_loss:0.053, val_acc:0.956]
Epoch [40/120    avg_loss:0.052, val_acc:0.968]
Epoch [41/120    avg_loss:0.048, val_acc:0.965]
Epoch [42/120    avg_loss:0.054, val_acc:0.972]
Epoch [43/120    avg_loss:0.060, val_acc:0.958]
Epoch [44/120    avg_loss:0.095, val_acc:0.952]
Epoch [45/120    avg_loss:0.113, val_acc:0.946]
Epoch [46/120    avg_loss:0.050, val_acc:0.967]
Epoch [47/120    avg_loss:0.049, val_acc:0.962]
Epoch [48/120    avg_loss:0.059, val_acc:0.959]
Epoch [49/120    avg_loss:0.061, val_acc:0.975]
Epoch [50/120    avg_loss:0.030, val_acc:0.980]
Epoch [51/120    avg_loss:0.124, val_acc:0.955]
Epoch [52/120    avg_loss:0.049, val_acc:0.979]
Epoch [53/120    avg_loss:0.057, val_acc:0.951]
Epoch [54/120    avg_loss:0.076, val_acc:0.975]
Epoch [55/120    avg_loss:0.033, val_acc:0.972]
Epoch [56/120    avg_loss:0.047, val_acc:0.983]
Epoch [57/120    avg_loss:0.031, val_acc:0.977]
Epoch [58/120    avg_loss:0.024, val_acc:0.975]
Epoch [59/120    avg_loss:0.024, val_acc:0.979]
Epoch [60/120    avg_loss:0.020, val_acc:0.973]
Epoch [61/120    avg_loss:0.023, val_acc:0.961]
Epoch [62/120    avg_loss:0.021, val_acc:0.974]
Epoch [63/120    avg_loss:0.035, val_acc:0.980]
Epoch [64/120    avg_loss:0.018, val_acc:0.983]
Epoch [65/120    avg_loss:0.022, val_acc:0.988]
Epoch [66/120    avg_loss:0.013, val_acc:0.984]
Epoch [67/120    avg_loss:0.016, val_acc:0.978]
Epoch [68/120    avg_loss:0.018, val_acc:0.983]
Epoch [69/120    avg_loss:0.013, val_acc:0.984]
Epoch [70/120    avg_loss:0.011, val_acc:0.985]
Epoch [71/120    avg_loss:0.016, val_acc:0.978]
Epoch [72/120    avg_loss:0.019, val_acc:0.987]
Epoch [73/120    avg_loss:0.019, val_acc:0.978]
Epoch [74/120    avg_loss:0.011, val_acc:0.983]
Epoch [75/120    avg_loss:0.011, val_acc:0.984]
Epoch [76/120    avg_loss:0.010, val_acc:0.991]
Epoch [77/120    avg_loss:0.008, val_acc:0.989]
Epoch [78/120    avg_loss:0.008, val_acc:0.980]
Epoch [79/120    avg_loss:0.013, val_acc:0.981]
Epoch [80/120    avg_loss:0.012, val_acc:0.985]
Epoch [81/120    avg_loss:0.013, val_acc:0.989]
Epoch [82/120    avg_loss:0.012, val_acc:0.985]
Epoch [83/120    avg_loss:0.016, val_acc:0.962]
Epoch [84/120    avg_loss:0.029, val_acc:0.983]
Epoch [85/120    avg_loss:0.024, val_acc:0.968]
Epoch [86/120    avg_loss:0.025, val_acc:0.979]
Epoch [87/120    avg_loss:0.027, val_acc:0.982]
Epoch [88/120    avg_loss:0.009, val_acc:0.990]
Epoch [89/120    avg_loss:0.008, val_acc:0.990]
Epoch [90/120    avg_loss:0.006, val_acc:0.991]
Epoch [91/120    avg_loss:0.006, val_acc:0.990]
Epoch [92/120    avg_loss:0.005, val_acc:0.990]
Epoch [93/120    avg_loss:0.005, val_acc:0.990]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.005, val_acc:0.990]
Epoch [96/120    avg_loss:0.005, val_acc:0.990]
Epoch [97/120    avg_loss:0.005, val_acc:0.990]
Epoch [98/120    avg_loss:0.005, val_acc:0.991]
Epoch [99/120    avg_loss:0.004, val_acc:0.990]
Epoch [100/120    avg_loss:0.005, val_acc:0.991]
Epoch [101/120    avg_loss:0.007, val_acc:0.991]
Epoch [102/120    avg_loss:0.004, val_acc:0.990]
Epoch [103/120    avg_loss:0.005, val_acc:0.990]
Epoch [104/120    avg_loss:0.005, val_acc:0.991]
Epoch [105/120    avg_loss:0.005, val_acc:0.990]
Epoch [106/120    avg_loss:0.006, val_acc:0.991]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.005, val_acc:0.990]
Epoch [109/120    avg_loss:0.004, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.991]
Epoch [112/120    avg_loss:0.004, val_acc:0.991]
Epoch [113/120    avg_loss:0.004, val_acc:0.991]
Epoch [114/120    avg_loss:0.006, val_acc:0.991]
Epoch [115/120    avg_loss:0.006, val_acc:0.992]
Epoch [116/120    avg_loss:0.004, val_acc:0.993]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.991]
Epoch [119/120    avg_loss:0.004, val_acc:0.993]
Epoch [120/120    avg_loss:0.004, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1258    7    0    0    1    0    0    0    5    7    1    0
     0    6    0]
 [   0    0    0  731    0    9    0    0    0    3    0    0    2    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    2    6    0    4    1    0    0    0  851    7    2    0
     0    2    0]
 [   0    0    1    0    0    0    3    0    0    0    4 2193    7    2
     0    0    0]
 [   0    0    0    2   18    5    0    0    0    0    2    0  498    0
     0    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1136    2    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    41  297    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.08130081300813

F1 scores:
[       nan 0.98765432 0.98782882 0.97923644 0.95945946 0.97857948
 0.98869631 1.         1.         0.87179487 0.97928654 0.9927569
 0.95311005 0.98930481 0.98100173 0.90825688 0.94915254]

Kappa:
0.9781270865134563
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6c7f38a710>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.108, val_acc:0.520]
Epoch [2/120    avg_loss:1.577, val_acc:0.625]
Epoch [3/120    avg_loss:1.248, val_acc:0.626]
Epoch [4/120    avg_loss:1.031, val_acc:0.645]
Epoch [5/120    avg_loss:0.959, val_acc:0.772]
Epoch [6/120    avg_loss:0.816, val_acc:0.728]
Epoch [7/120    avg_loss:0.627, val_acc:0.801]
Epoch [8/120    avg_loss:0.642, val_acc:0.798]
Epoch [9/120    avg_loss:0.561, val_acc:0.808]
Epoch [10/120    avg_loss:0.574, val_acc:0.782]
Epoch [11/120    avg_loss:0.455, val_acc:0.766]
Epoch [12/120    avg_loss:0.455, val_acc:0.845]
Epoch [13/120    avg_loss:0.419, val_acc:0.878]
Epoch [14/120    avg_loss:0.435, val_acc:0.860]
Epoch [15/120    avg_loss:0.341, val_acc:0.886]
Epoch [16/120    avg_loss:0.398, val_acc:0.849]
Epoch [17/120    avg_loss:0.395, val_acc:0.792]
Epoch [18/120    avg_loss:0.321, val_acc:0.874]
Epoch [19/120    avg_loss:0.297, val_acc:0.890]
Epoch [20/120    avg_loss:0.321, val_acc:0.863]
Epoch [21/120    avg_loss:0.237, val_acc:0.904]
Epoch [22/120    avg_loss:0.279, val_acc:0.896]
Epoch [23/120    avg_loss:0.252, val_acc:0.920]
Epoch [24/120    avg_loss:0.157, val_acc:0.940]
Epoch [25/120    avg_loss:0.157, val_acc:0.922]
Epoch [26/120    avg_loss:0.170, val_acc:0.924]
Epoch [27/120    avg_loss:0.184, val_acc:0.928]
Epoch [28/120    avg_loss:0.232, val_acc:0.933]
Epoch [29/120    avg_loss:0.201, val_acc:0.799]
Epoch [30/120    avg_loss:0.224, val_acc:0.928]
Epoch [31/120    avg_loss:0.095, val_acc:0.928]
Epoch [32/120    avg_loss:0.123, val_acc:0.952]
Epoch [33/120    avg_loss:0.096, val_acc:0.934]
Epoch [34/120    avg_loss:0.095, val_acc:0.962]
Epoch [35/120    avg_loss:0.159, val_acc:0.916]
Epoch [36/120    avg_loss:0.116, val_acc:0.942]
Epoch [37/120    avg_loss:0.085, val_acc:0.948]
Epoch [38/120    avg_loss:0.098, val_acc:0.937]
Epoch [39/120    avg_loss:0.093, val_acc:0.957]
Epoch [40/120    avg_loss:0.078, val_acc:0.961]
Epoch [41/120    avg_loss:0.133, val_acc:0.961]
Epoch [42/120    avg_loss:0.061, val_acc:0.957]
Epoch [43/120    avg_loss:0.059, val_acc:0.965]
Epoch [44/120    avg_loss:0.059, val_acc:0.964]
Epoch [45/120    avg_loss:0.058, val_acc:0.975]
Epoch [46/120    avg_loss:0.070, val_acc:0.973]
Epoch [47/120    avg_loss:0.064, val_acc:0.965]
Epoch [48/120    avg_loss:0.073, val_acc:0.954]
Epoch [49/120    avg_loss:0.051, val_acc:0.952]
Epoch [50/120    avg_loss:0.085, val_acc:0.950]
Epoch [51/120    avg_loss:0.041, val_acc:0.968]
Epoch [52/120    avg_loss:0.067, val_acc:0.972]
Epoch [53/120    avg_loss:0.148, val_acc:0.949]
Epoch [54/120    avg_loss:0.070, val_acc:0.962]
Epoch [55/120    avg_loss:0.062, val_acc:0.953]
Epoch [56/120    avg_loss:0.065, val_acc:0.964]
Epoch [57/120    avg_loss:0.048, val_acc:0.974]
Epoch [58/120    avg_loss:0.024, val_acc:0.986]
Epoch [59/120    avg_loss:0.020, val_acc:0.976]
Epoch [60/120    avg_loss:0.021, val_acc:0.976]
Epoch [61/120    avg_loss:0.031, val_acc:0.984]
Epoch [62/120    avg_loss:0.031, val_acc:0.973]
Epoch [63/120    avg_loss:0.039, val_acc:0.975]
Epoch [64/120    avg_loss:0.038, val_acc:0.978]
Epoch [65/120    avg_loss:0.037, val_acc:0.971]
Epoch [66/120    avg_loss:0.047, val_acc:0.967]
Epoch [67/120    avg_loss:0.030, val_acc:0.984]
Epoch [68/120    avg_loss:0.014, val_acc:0.982]
Epoch [69/120    avg_loss:0.019, val_acc:0.978]
Epoch [70/120    avg_loss:0.032, val_acc:0.983]
Epoch [71/120    avg_loss:0.026, val_acc:0.974]
Epoch [72/120    avg_loss:0.011, val_acc:0.976]
Epoch [73/120    avg_loss:0.012, val_acc:0.980]
Epoch [74/120    avg_loss:0.012, val_acc:0.983]
Epoch [75/120    avg_loss:0.010, val_acc:0.984]
Epoch [76/120    avg_loss:0.011, val_acc:0.985]
Epoch [77/120    avg_loss:0.018, val_acc:0.985]
Epoch [78/120    avg_loss:0.009, val_acc:0.986]
Epoch [79/120    avg_loss:0.013, val_acc:0.985]
Epoch [80/120    avg_loss:0.009, val_acc:0.985]
Epoch [81/120    avg_loss:0.009, val_acc:0.986]
Epoch [82/120    avg_loss:0.009, val_acc:0.986]
Epoch [83/120    avg_loss:0.008, val_acc:0.986]
Epoch [84/120    avg_loss:0.009, val_acc:0.986]
Epoch [85/120    avg_loss:0.009, val_acc:0.986]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.007, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.009, val_acc:0.986]
Epoch [90/120    avg_loss:0.010, val_acc:0.986]
Epoch [91/120    avg_loss:0.009, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.008, val_acc:0.986]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.009, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.986]
Epoch [97/120    avg_loss:0.010, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.009, val_acc:0.987]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.010, val_acc:0.986]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.987]
Epoch [105/120    avg_loss:0.007, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.008, val_acc:0.987]
Epoch [108/120    avg_loss:0.006, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.012, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.987]
Epoch [112/120    avg_loss:0.007, val_acc:0.987]
Epoch [113/120    avg_loss:0.008, val_acc:0.987]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.009, val_acc:0.987]
Epoch [116/120    avg_loss:0.010, val_acc:0.987]
Epoch [117/120    avg_loss:0.009, val_acc:0.987]
Epoch [118/120    avg_loss:0.007, val_acc:0.987]
Epoch [119/120    avg_loss:0.007, val_acc:0.987]
Epoch [120/120    avg_loss:0.007, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1266    5    0    0    0    0    0    2    2    4    4    0
     0    2    0]
 [   0    0    0  719    0   19    0    0    0    3    0    0    3    3
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0  655    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    7    0    4    0    0    0    0  817   34    2    0
     2    5    0]
 [   0    0    5    0    0    0    4    0    0    0    9 2191    0    1
     0    0    0]
 [   0    0    0    7   10    1    0    0    0    0    0    8  505    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    53  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.57181571815718

F1 scores:
[       nan 0.98765432 0.9882904  0.96769852 0.97471264 0.96853933
 0.98867925 0.98039216 1.         0.81818182 0.95948326 0.98516187
 0.96282173 0.98930481 0.97600686 0.89201878 0.98224852]

Kappa:
0.9722950234627525
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f70d61cf710>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.104, val_acc:0.602]
Epoch [2/120    avg_loss:1.596, val_acc:0.581]
Epoch [3/120    avg_loss:1.286, val_acc:0.731]
Epoch [4/120    avg_loss:0.938, val_acc:0.786]
Epoch [5/120    avg_loss:0.831, val_acc:0.770]
Epoch [6/120    avg_loss:0.712, val_acc:0.797]
Epoch [7/120    avg_loss:0.629, val_acc:0.782]
Epoch [8/120    avg_loss:0.619, val_acc:0.831]
Epoch [9/120    avg_loss:0.535, val_acc:0.827]
Epoch [10/120    avg_loss:0.542, val_acc:0.836]
Epoch [11/120    avg_loss:0.486, val_acc:0.849]
Epoch [12/120    avg_loss:0.361, val_acc:0.833]
Epoch [13/120    avg_loss:0.339, val_acc:0.899]
Epoch [14/120    avg_loss:0.465, val_acc:0.841]
Epoch [15/120    avg_loss:0.415, val_acc:0.901]
Epoch [16/120    avg_loss:0.291, val_acc:0.793]
Epoch [17/120    avg_loss:0.279, val_acc:0.909]
Epoch [18/120    avg_loss:0.363, val_acc:0.831]
Epoch [19/120    avg_loss:0.405, val_acc:0.913]
Epoch [20/120    avg_loss:0.253, val_acc:0.908]
Epoch [21/120    avg_loss:0.249, val_acc:0.895]
Epoch [22/120    avg_loss:0.215, val_acc:0.925]
Epoch [23/120    avg_loss:0.273, val_acc:0.905]
Epoch [24/120    avg_loss:0.256, val_acc:0.908]
Epoch [25/120    avg_loss:0.186, val_acc:0.925]
Epoch [26/120    avg_loss:0.148, val_acc:0.947]
Epoch [27/120    avg_loss:0.111, val_acc:0.959]
Epoch [28/120    avg_loss:0.155, val_acc:0.891]
Epoch [29/120    avg_loss:0.166, val_acc:0.944]
Epoch [30/120    avg_loss:0.136, val_acc:0.946]
Epoch [31/120    avg_loss:0.237, val_acc:0.906]
Epoch [32/120    avg_loss:0.192, val_acc:0.924]
Epoch [33/120    avg_loss:0.113, val_acc:0.967]
Epoch [34/120    avg_loss:0.100, val_acc:0.951]
Epoch [35/120    avg_loss:0.097, val_acc:0.963]
Epoch [36/120    avg_loss:0.086, val_acc:0.944]
Epoch [37/120    avg_loss:0.103, val_acc:0.933]
Epoch [38/120    avg_loss:0.131, val_acc:0.959]
Epoch [39/120    avg_loss:0.158, val_acc:0.945]
Epoch [40/120    avg_loss:0.095, val_acc:0.962]
Epoch [41/120    avg_loss:0.085, val_acc:0.973]
Epoch [42/120    avg_loss:0.080, val_acc:0.973]
Epoch [43/120    avg_loss:0.046, val_acc:0.968]
Epoch [44/120    avg_loss:0.058, val_acc:0.970]
Epoch [45/120    avg_loss:0.080, val_acc:0.964]
Epoch [46/120    avg_loss:0.077, val_acc:0.964]
Epoch [47/120    avg_loss:0.074, val_acc:0.958]
Epoch [48/120    avg_loss:0.067, val_acc:0.964]
Epoch [49/120    avg_loss:0.070, val_acc:0.955]
Epoch [50/120    avg_loss:0.076, val_acc:0.970]
Epoch [51/120    avg_loss:0.066, val_acc:0.975]
Epoch [52/120    avg_loss:0.044, val_acc:0.977]
Epoch [53/120    avg_loss:0.065, val_acc:0.974]
Epoch [54/120    avg_loss:0.229, val_acc:0.898]
Epoch [55/120    avg_loss:0.095, val_acc:0.969]
Epoch [56/120    avg_loss:0.075, val_acc:0.946]
Epoch [57/120    avg_loss:0.052, val_acc:0.971]
Epoch [58/120    avg_loss:0.036, val_acc:0.977]
Epoch [59/120    avg_loss:0.052, val_acc:0.968]
Epoch [60/120    avg_loss:0.071, val_acc:0.965]
Epoch [61/120    avg_loss:0.038, val_acc:0.976]
Epoch [62/120    avg_loss:0.027, val_acc:0.984]
Epoch [63/120    avg_loss:0.027, val_acc:0.982]
Epoch [64/120    avg_loss:0.028, val_acc:0.975]
Epoch [65/120    avg_loss:0.031, val_acc:0.974]
Epoch [66/120    avg_loss:0.034, val_acc:0.980]
Epoch [67/120    avg_loss:0.054, val_acc:0.964]
Epoch [68/120    avg_loss:0.042, val_acc:0.971]
Epoch [69/120    avg_loss:0.023, val_acc:0.977]
Epoch [70/120    avg_loss:0.024, val_acc:0.976]
Epoch [71/120    avg_loss:0.020, val_acc:0.987]
Epoch [72/120    avg_loss:0.012, val_acc:0.981]
Epoch [73/120    avg_loss:0.010, val_acc:0.981]
Epoch [74/120    avg_loss:0.013, val_acc:0.988]
Epoch [75/120    avg_loss:0.009, val_acc:0.984]
Epoch [76/120    avg_loss:0.009, val_acc:0.987]
Epoch [77/120    avg_loss:0.016, val_acc:0.978]
Epoch [78/120    avg_loss:0.012, val_acc:0.987]
Epoch [79/120    avg_loss:0.012, val_acc:0.982]
Epoch [80/120    avg_loss:0.011, val_acc:0.987]
Epoch [81/120    avg_loss:0.007, val_acc:0.984]
Epoch [82/120    avg_loss:0.030, val_acc:0.961]
Epoch [83/120    avg_loss:0.016, val_acc:0.979]
Epoch [84/120    avg_loss:0.043, val_acc:0.977]
Epoch [85/120    avg_loss:0.016, val_acc:0.986]
Epoch [86/120    avg_loss:0.022, val_acc:0.964]
Epoch [87/120    avg_loss:0.046, val_acc:0.962]
Epoch [88/120    avg_loss:0.024, val_acc:0.982]
Epoch [89/120    avg_loss:0.014, val_acc:0.984]
Epoch [90/120    avg_loss:0.013, val_acc:0.985]
Epoch [91/120    avg_loss:0.008, val_acc:0.985]
Epoch [92/120    avg_loss:0.010, val_acc:0.985]
Epoch [93/120    avg_loss:0.012, val_acc:0.984]
Epoch [94/120    avg_loss:0.007, val_acc:0.984]
Epoch [95/120    avg_loss:0.009, val_acc:0.984]
Epoch [96/120    avg_loss:0.008, val_acc:0.985]
Epoch [97/120    avg_loss:0.007, val_acc:0.985]
Epoch [98/120    avg_loss:0.008, val_acc:0.984]
Epoch [99/120    avg_loss:0.018, val_acc:0.985]
Epoch [100/120    avg_loss:0.006, val_acc:0.986]
Epoch [101/120    avg_loss:0.010, val_acc:0.986]
Epoch [102/120    avg_loss:0.009, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.986]
Epoch [104/120    avg_loss:0.006, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.007, val_acc:0.986]
Epoch [108/120    avg_loss:0.008, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.008, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.008, val_acc:0.987]
Epoch [114/120    avg_loss:0.006, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.987]
Epoch [116/120    avg_loss:0.007, val_acc:0.987]
Epoch [117/120    avg_loss:0.006, val_acc:0.987]
Epoch [118/120    avg_loss:0.009, val_acc:0.987]
Epoch [119/120    avg_loss:0.006, val_acc:0.987]
Epoch [120/120    avg_loss:0.007, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1261   13    0    0    0    0    0    0    7    3    1    0
     0    0    0]
 [   0    0    0  641    9   20    0    0    0    7    0    0   66    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    3    0    0    0    0
     3    0    0]
 [   0    0    2    0    0    0  655    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    6    0    5    3    0    0    0  833   11    3    0
     1   10    0]
 [   0    0   12    0    0    0    4    0    0    0    4 2144   44    2
     0    0    0]
 [   0    0    0   17   18    6    0    0    0    0    0    4  487    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1136    2    0]
 [   0    0    0    0    0    0   28    0    0    0    0    0    0    0
    27  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.17344173441734

F1 scores:
[       nan 0.975      0.98400312 0.9002809  0.94039735 0.95865922
 0.97253155 1.         1.         0.7826087  0.96747967 0.98078683
 0.85814978 0.98404255 0.98525585 0.89708141 0.98823529]

Kappa:
0.956431798661333
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2e9d9d1710>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.110, val_acc:0.531]
Epoch [2/120    avg_loss:1.491, val_acc:0.595]
Epoch [3/120    avg_loss:1.202, val_acc:0.720]
Epoch [4/120    avg_loss:0.939, val_acc:0.669]
Epoch [5/120    avg_loss:0.923, val_acc:0.742]
Epoch [6/120    avg_loss:0.680, val_acc:0.771]
Epoch [7/120    avg_loss:0.724, val_acc:0.792]
Epoch [8/120    avg_loss:0.619, val_acc:0.787]
Epoch [9/120    avg_loss:0.536, val_acc:0.851]
Epoch [10/120    avg_loss:0.503, val_acc:0.836]
Epoch [11/120    avg_loss:0.532, val_acc:0.816]
Epoch [12/120    avg_loss:0.622, val_acc:0.851]
Epoch [13/120    avg_loss:0.434, val_acc:0.882]
Epoch [14/120    avg_loss:0.373, val_acc:0.852]
Epoch [15/120    avg_loss:0.377, val_acc:0.878]
Epoch [16/120    avg_loss:0.320, val_acc:0.900]
Epoch [17/120    avg_loss:0.285, val_acc:0.920]
Epoch [18/120    avg_loss:0.254, val_acc:0.890]
Epoch [19/120    avg_loss:0.315, val_acc:0.874]
Epoch [20/120    avg_loss:0.270, val_acc:0.888]
Epoch [21/120    avg_loss:0.229, val_acc:0.913]
Epoch [22/120    avg_loss:0.257, val_acc:0.899]
Epoch [23/120    avg_loss:0.192, val_acc:0.935]
Epoch [24/120    avg_loss:0.149, val_acc:0.928]
Epoch [25/120    avg_loss:0.149, val_acc:0.920]
Epoch [26/120    avg_loss:0.138, val_acc:0.931]
Epoch [27/120    avg_loss:0.116, val_acc:0.936]
Epoch [28/120    avg_loss:0.112, val_acc:0.954]
Epoch [29/120    avg_loss:0.122, val_acc:0.941]
Epoch [30/120    avg_loss:0.193, val_acc:0.927]
Epoch [31/120    avg_loss:0.106, val_acc:0.936]
Epoch [32/120    avg_loss:0.103, val_acc:0.931]
Epoch [33/120    avg_loss:0.116, val_acc:0.936]
Epoch [34/120    avg_loss:0.118, val_acc:0.956]
Epoch [35/120    avg_loss:0.070, val_acc:0.959]
Epoch [36/120    avg_loss:0.103, val_acc:0.941]
Epoch [37/120    avg_loss:0.079, val_acc:0.959]
Epoch [38/120    avg_loss:0.087, val_acc:0.958]
Epoch [39/120    avg_loss:0.071, val_acc:0.958]
Epoch [40/120    avg_loss:0.057, val_acc:0.969]
Epoch [41/120    avg_loss:0.058, val_acc:0.948]
Epoch [42/120    avg_loss:0.096, val_acc:0.938]
Epoch [43/120    avg_loss:0.222, val_acc:0.696]
Epoch [44/120    avg_loss:0.287, val_acc:0.921]
Epoch [45/120    avg_loss:0.167, val_acc:0.952]
Epoch [46/120    avg_loss:0.075, val_acc:0.945]
Epoch [47/120    avg_loss:0.092, val_acc:0.940]
Epoch [48/120    avg_loss:0.091, val_acc:0.958]
Epoch [49/120    avg_loss:0.059, val_acc:0.964]
Epoch [50/120    avg_loss:0.059, val_acc:0.966]
Epoch [51/120    avg_loss:0.048, val_acc:0.967]
Epoch [52/120    avg_loss:0.039, val_acc:0.977]
Epoch [53/120    avg_loss:0.057, val_acc:0.965]
Epoch [54/120    avg_loss:0.042, val_acc:0.966]
Epoch [55/120    avg_loss:0.082, val_acc:0.955]
Epoch [56/120    avg_loss:0.100, val_acc:0.951]
Epoch [57/120    avg_loss:0.061, val_acc:0.962]
Epoch [58/120    avg_loss:0.044, val_acc:0.965]
Epoch [59/120    avg_loss:0.042, val_acc:0.945]
Epoch [60/120    avg_loss:0.145, val_acc:0.953]
Epoch [61/120    avg_loss:0.085, val_acc:0.962]
Epoch [62/120    avg_loss:0.122, val_acc:0.947]
Epoch [63/120    avg_loss:0.034, val_acc:0.969]
Epoch [64/120    avg_loss:0.022, val_acc:0.970]
Epoch [65/120    avg_loss:0.040, val_acc:0.974]
Epoch [66/120    avg_loss:0.025, val_acc:0.976]
Epoch [67/120    avg_loss:0.016, val_acc:0.977]
Epoch [68/120    avg_loss:0.015, val_acc:0.977]
Epoch [69/120    avg_loss:0.013, val_acc:0.977]
Epoch [70/120    avg_loss:0.015, val_acc:0.977]
Epoch [71/120    avg_loss:0.011, val_acc:0.976]
Epoch [72/120    avg_loss:0.015, val_acc:0.975]
Epoch [73/120    avg_loss:0.016, val_acc:0.977]
Epoch [74/120    avg_loss:0.012, val_acc:0.976]
Epoch [75/120    avg_loss:0.013, val_acc:0.976]
Epoch [76/120    avg_loss:0.014, val_acc:0.976]
Epoch [77/120    avg_loss:0.011, val_acc:0.977]
Epoch [78/120    avg_loss:0.010, val_acc:0.976]
Epoch [79/120    avg_loss:0.013, val_acc:0.976]
Epoch [80/120    avg_loss:0.013, val_acc:0.976]
Epoch [81/120    avg_loss:0.010, val_acc:0.977]
Epoch [82/120    avg_loss:0.014, val_acc:0.978]
Epoch [83/120    avg_loss:0.015, val_acc:0.979]
Epoch [84/120    avg_loss:0.011, val_acc:0.980]
Epoch [85/120    avg_loss:0.013, val_acc:0.979]
Epoch [86/120    avg_loss:0.010, val_acc:0.980]
Epoch [87/120    avg_loss:0.010, val_acc:0.980]
Epoch [88/120    avg_loss:0.013, val_acc:0.980]
Epoch [89/120    avg_loss:0.010, val_acc:0.978]
Epoch [90/120    avg_loss:0.009, val_acc:0.977]
Epoch [91/120    avg_loss:0.011, val_acc:0.977]
Epoch [92/120    avg_loss:0.010, val_acc:0.978]
Epoch [93/120    avg_loss:0.010, val_acc:0.977]
Epoch [94/120    avg_loss:0.010, val_acc:0.977]
Epoch [95/120    avg_loss:0.010, val_acc:0.980]
Epoch [96/120    avg_loss:0.010, val_acc:0.979]
Epoch [97/120    avg_loss:0.010, val_acc:0.978]
Epoch [98/120    avg_loss:0.010, val_acc:0.980]
Epoch [99/120    avg_loss:0.010, val_acc:0.979]
Epoch [100/120    avg_loss:0.009, val_acc:0.979]
Epoch [101/120    avg_loss:0.010, val_acc:0.979]
Epoch [102/120    avg_loss:0.010, val_acc:0.979]
Epoch [103/120    avg_loss:0.007, val_acc:0.979]
Epoch [104/120    avg_loss:0.011, val_acc:0.979]
Epoch [105/120    avg_loss:0.009, val_acc:0.978]
Epoch [106/120    avg_loss:0.009, val_acc:0.979]
Epoch [107/120    avg_loss:0.010, val_acc:0.980]
Epoch [108/120    avg_loss:0.013, val_acc:0.979]
Epoch [109/120    avg_loss:0.010, val_acc:0.980]
Epoch [110/120    avg_loss:0.010, val_acc:0.979]
Epoch [111/120    avg_loss:0.009, val_acc:0.979]
Epoch [112/120    avg_loss:0.011, val_acc:0.979]
Epoch [113/120    avg_loss:0.009, val_acc:0.979]
Epoch [114/120    avg_loss:0.010, val_acc:0.980]
Epoch [115/120    avg_loss:0.011, val_acc:0.981]
Epoch [116/120    avg_loss:0.007, val_acc:0.980]
Epoch [117/120    avg_loss:0.011, val_acc:0.981]
Epoch [118/120    avg_loss:0.008, val_acc:0.981]
Epoch [119/120    avg_loss:0.009, val_acc:0.981]
Epoch [120/120    avg_loss:0.008, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1258    8    0    0    0    0    0    0    7   12    0    0
     0    0    0]
 [   0    0    0  712    0   21    0    0    0    5    1    0    5    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3   16    0    5    0    0    0    0  827   21    1    0
     0    2    0]
 [   0    0    5    0    0    1    8    0    0    0    6 2150   38    2
     0    0    0]
 [   0    0    0   19    5    7    0    0    0    0    6    9  484    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    1    0    0    0
  1129    7    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    45  289    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.83468834688347

F1 scores:
[       nan 1.         0.98627989 0.94806924 0.98839907 0.96017699
 0.98274569 1.         0.99883856 0.85714286 0.95995357 0.9763851
 0.91063029 0.98666667 0.97622136 0.89612403 0.97076023]

Kappa:
0.963925708413471
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff54b314668>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.165, val_acc:0.552]
Epoch [2/120    avg_loss:1.670, val_acc:0.661]
Epoch [3/120    avg_loss:1.253, val_acc:0.692]
Epoch [4/120    avg_loss:1.055, val_acc:0.752]
Epoch [5/120    avg_loss:0.958, val_acc:0.721]
Epoch [6/120    avg_loss:0.915, val_acc:0.772]
Epoch [7/120    avg_loss:0.756, val_acc:0.777]
Epoch [8/120    avg_loss:0.704, val_acc:0.792]
Epoch [9/120    avg_loss:0.676, val_acc:0.785]
Epoch [10/120    avg_loss:0.520, val_acc:0.800]
Epoch [11/120    avg_loss:0.441, val_acc:0.842]
Epoch [12/120    avg_loss:0.454, val_acc:0.840]
Epoch [13/120    avg_loss:0.438, val_acc:0.871]
Epoch [14/120    avg_loss:0.389, val_acc:0.850]
Epoch [15/120    avg_loss:0.329, val_acc:0.894]
Epoch [16/120    avg_loss:0.296, val_acc:0.874]
Epoch [17/120    avg_loss:0.318, val_acc:0.872]
Epoch [18/120    avg_loss:0.272, val_acc:0.904]
Epoch [19/120    avg_loss:0.305, val_acc:0.899]
Epoch [20/120    avg_loss:0.284, val_acc:0.912]
Epoch [21/120    avg_loss:0.265, val_acc:0.875]
Epoch [22/120    avg_loss:0.268, val_acc:0.904]
Epoch [23/120    avg_loss:0.234, val_acc:0.933]
Epoch [24/120    avg_loss:0.180, val_acc:0.924]
Epoch [25/120    avg_loss:0.183, val_acc:0.921]
Epoch [26/120    avg_loss:0.179, val_acc:0.879]
Epoch [27/120    avg_loss:0.175, val_acc:0.922]
Epoch [28/120    avg_loss:0.135, val_acc:0.921]
Epoch [29/120    avg_loss:0.126, val_acc:0.948]
Epoch [30/120    avg_loss:0.283, val_acc:0.917]
Epoch [31/120    avg_loss:0.206, val_acc:0.914]
Epoch [32/120    avg_loss:0.280, val_acc:0.941]
Epoch [33/120    avg_loss:0.178, val_acc:0.940]
Epoch [34/120    avg_loss:0.103, val_acc:0.956]
Epoch [35/120    avg_loss:0.164, val_acc:0.935]
Epoch [36/120    avg_loss:0.135, val_acc:0.946]
Epoch [37/120    avg_loss:0.091, val_acc:0.958]
Epoch [38/120    avg_loss:0.076, val_acc:0.943]
Epoch [39/120    avg_loss:0.083, val_acc:0.952]
Epoch [40/120    avg_loss:0.073, val_acc:0.964]
Epoch [41/120    avg_loss:0.113, val_acc:0.957]
Epoch [42/120    avg_loss:0.061, val_acc:0.964]
Epoch [43/120    avg_loss:0.053, val_acc:0.969]
Epoch [44/120    avg_loss:0.048, val_acc:0.966]
Epoch [45/120    avg_loss:0.076, val_acc:0.930]
Epoch [46/120    avg_loss:0.108, val_acc:0.944]
Epoch [47/120    avg_loss:0.065, val_acc:0.966]
Epoch [48/120    avg_loss:0.067, val_acc:0.930]
Epoch [49/120    avg_loss:0.045, val_acc:0.966]
Epoch [50/120    avg_loss:0.107, val_acc:0.962]
Epoch [51/120    avg_loss:0.059, val_acc:0.977]
Epoch [52/120    avg_loss:0.048, val_acc:0.975]
Epoch [53/120    avg_loss:0.056, val_acc:0.959]
Epoch [54/120    avg_loss:0.060, val_acc:0.964]
Epoch [55/120    avg_loss:0.052, val_acc:0.977]
Epoch [56/120    avg_loss:0.024, val_acc:0.964]
Epoch [57/120    avg_loss:0.034, val_acc:0.970]
Epoch [58/120    avg_loss:0.040, val_acc:0.973]
Epoch [59/120    avg_loss:0.533, val_acc:0.931]
Epoch [60/120    avg_loss:0.152, val_acc:0.956]
Epoch [61/120    avg_loss:0.095, val_acc:0.965]
Epoch [62/120    avg_loss:0.057, val_acc:0.964]
Epoch [63/120    avg_loss:0.065, val_acc:0.970]
Epoch [64/120    avg_loss:0.053, val_acc:0.962]
Epoch [65/120    avg_loss:0.033, val_acc:0.980]
Epoch [66/120    avg_loss:0.024, val_acc:0.985]
Epoch [67/120    avg_loss:0.028, val_acc:0.973]
Epoch [68/120    avg_loss:0.025, val_acc:0.982]
Epoch [69/120    avg_loss:0.041, val_acc:0.970]
Epoch [70/120    avg_loss:0.032, val_acc:0.967]
Epoch [71/120    avg_loss:0.021, val_acc:0.986]
Epoch [72/120    avg_loss:0.021, val_acc:0.980]
Epoch [73/120    avg_loss:0.019, val_acc:0.974]
Epoch [74/120    avg_loss:0.037, val_acc:0.981]
Epoch [75/120    avg_loss:0.015, val_acc:0.985]
Epoch [76/120    avg_loss:0.019, val_acc:0.975]
Epoch [77/120    avg_loss:0.015, val_acc:0.981]
Epoch [78/120    avg_loss:0.011, val_acc:0.984]
Epoch [79/120    avg_loss:0.040, val_acc:0.974]
Epoch [80/120    avg_loss:0.026, val_acc:0.980]
Epoch [81/120    avg_loss:0.025, val_acc:0.967]
Epoch [82/120    avg_loss:0.031, val_acc:0.984]
Epoch [83/120    avg_loss:0.021, val_acc:0.977]
Epoch [84/120    avg_loss:0.022, val_acc:0.981]
Epoch [85/120    avg_loss:0.016, val_acc:0.982]
Epoch [86/120    avg_loss:0.010, val_acc:0.985]
Epoch [87/120    avg_loss:0.010, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.010, val_acc:0.987]
Epoch [90/120    avg_loss:0.008, val_acc:0.987]
Epoch [91/120    avg_loss:0.009, val_acc:0.988]
Epoch [92/120    avg_loss:0.008, val_acc:0.989]
Epoch [93/120    avg_loss:0.008, val_acc:0.990]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.008, val_acc:0.988]
Epoch [96/120    avg_loss:0.007, val_acc:0.989]
Epoch [97/120    avg_loss:0.007, val_acc:0.990]
Epoch [98/120    avg_loss:0.008, val_acc:0.989]
Epoch [99/120    avg_loss:0.009, val_acc:0.986]
Epoch [100/120    avg_loss:0.008, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.988]
Epoch [103/120    avg_loss:0.006, val_acc:0.989]
Epoch [104/120    avg_loss:0.008, val_acc:0.989]
Epoch [105/120    avg_loss:0.006, val_acc:0.989]
Epoch [106/120    avg_loss:0.006, val_acc:0.989]
Epoch [107/120    avg_loss:0.006, val_acc:0.989]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.006, val_acc:0.988]
Epoch [110/120    avg_loss:0.007, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.987]
Epoch [113/120    avg_loss:0.008, val_acc:0.987]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.987]
Epoch [116/120    avg_loss:0.008, val_acc:0.987]
Epoch [117/120    avg_loss:0.006, val_acc:0.987]
Epoch [118/120    avg_loss:0.007, val_acc:0.987]
Epoch [119/120    avg_loss:0.008, val_acc:0.987]
Epoch [120/120    avg_loss:0.007, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    3    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    2    0    0    0    0    0    0    5    7    0    0
     0    4    0]
 [   0    0    0  725    0    6    0    0    0    4    2    0    6    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6   10    0    3    0    0    0    0  826   21    0    0
     0    9    0]
 [   0    0    3    0    0    0    4    0    0    0    4 2197    0    2
     0    0    0]
 [   0    0    0   11   19    5    0    0    0    0    2    0  482    0
     1    2   12]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    2    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    68  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.47425474254743

F1 scores:
[       nan 0.96202532 0.98829953 0.96989967 0.95730337 0.98070375
 0.99620349 0.98039216 0.99767981 0.85714286 0.96270396 0.99053201
 0.94324853 0.98404255 0.96884336 0.87051482 0.93333333]

Kappa:
0.9711874127750422
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdc164516a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.084, val_acc:0.543]
Epoch [2/120    avg_loss:1.540, val_acc:0.634]
Epoch [3/120    avg_loss:1.314, val_acc:0.691]
Epoch [4/120    avg_loss:0.990, val_acc:0.724]
Epoch [5/120    avg_loss:0.939, val_acc:0.750]
Epoch [6/120    avg_loss:0.824, val_acc:0.775]
Epoch [7/120    avg_loss:0.753, val_acc:0.781]
Epoch [8/120    avg_loss:0.676, val_acc:0.786]
Epoch [9/120    avg_loss:0.603, val_acc:0.809]
Epoch [10/120    avg_loss:0.579, val_acc:0.816]
Epoch [11/120    avg_loss:0.478, val_acc:0.782]
Epoch [12/120    avg_loss:0.509, val_acc:0.864]
Epoch [13/120    avg_loss:0.461, val_acc:0.842]
Epoch [14/120    avg_loss:0.355, val_acc:0.894]
Epoch [15/120    avg_loss:0.346, val_acc:0.899]
Epoch [16/120    avg_loss:0.387, val_acc:0.896]
Epoch [17/120    avg_loss:0.375, val_acc:0.866]
Epoch [18/120    avg_loss:0.316, val_acc:0.882]
Epoch [19/120    avg_loss:0.266, val_acc:0.916]
Epoch [20/120    avg_loss:0.310, val_acc:0.895]
Epoch [21/120    avg_loss:0.277, val_acc:0.916]
Epoch [22/120    avg_loss:0.281, val_acc:0.891]
Epoch [23/120    avg_loss:0.219, val_acc:0.896]
Epoch [24/120    avg_loss:0.240, val_acc:0.904]
Epoch [25/120    avg_loss:0.167, val_acc:0.916]
Epoch [26/120    avg_loss:0.187, val_acc:0.938]
Epoch [27/120    avg_loss:0.198, val_acc:0.942]
Epoch [28/120    avg_loss:0.134, val_acc:0.920]
Epoch [29/120    avg_loss:0.149, val_acc:0.933]
Epoch [30/120    avg_loss:0.172, val_acc:0.928]
Epoch [31/120    avg_loss:0.134, val_acc:0.935]
Epoch [32/120    avg_loss:0.113, val_acc:0.935]
Epoch [33/120    avg_loss:0.100, val_acc:0.957]
Epoch [34/120    avg_loss:0.094, val_acc:0.958]
Epoch [35/120    avg_loss:0.080, val_acc:0.962]
Epoch [36/120    avg_loss:0.099, val_acc:0.965]
Epoch [37/120    avg_loss:0.084, val_acc:0.939]
Epoch [38/120    avg_loss:0.097, val_acc:0.950]
Epoch [39/120    avg_loss:0.089, val_acc:0.956]
Epoch [40/120    avg_loss:0.079, val_acc:0.950]
Epoch [41/120    avg_loss:0.076, val_acc:0.951]
Epoch [42/120    avg_loss:0.117, val_acc:0.944]
Epoch [43/120    avg_loss:0.054, val_acc:0.974]
Epoch [44/120    avg_loss:0.089, val_acc:0.958]
Epoch [45/120    avg_loss:0.072, val_acc:0.942]
Epoch [46/120    avg_loss:0.092, val_acc:0.952]
Epoch [47/120    avg_loss:0.089, val_acc:0.963]
Epoch [48/120    avg_loss:0.058, val_acc:0.948]
Epoch [49/120    avg_loss:0.067, val_acc:0.922]
Epoch [50/120    avg_loss:0.128, val_acc:0.961]
Epoch [51/120    avg_loss:0.080, val_acc:0.969]
Epoch [52/120    avg_loss:0.067, val_acc:0.963]
Epoch [53/120    avg_loss:0.030, val_acc:0.974]
Epoch [54/120    avg_loss:0.037, val_acc:0.973]
Epoch [55/120    avg_loss:0.043, val_acc:0.965]
Epoch [56/120    avg_loss:0.094, val_acc:0.934]
Epoch [57/120    avg_loss:0.084, val_acc:0.953]
Epoch [58/120    avg_loss:0.093, val_acc:0.970]
Epoch [59/120    avg_loss:0.044, val_acc:0.961]
Epoch [60/120    avg_loss:0.087, val_acc:0.970]
Epoch [61/120    avg_loss:0.053, val_acc:0.978]
Epoch [62/120    avg_loss:0.029, val_acc:0.980]
Epoch [63/120    avg_loss:0.030, val_acc:0.977]
Epoch [64/120    avg_loss:0.033, val_acc:0.980]
Epoch [65/120    avg_loss:0.047, val_acc:0.956]
Epoch [66/120    avg_loss:0.038, val_acc:0.957]
Epoch [67/120    avg_loss:0.030, val_acc:0.978]
Epoch [68/120    avg_loss:0.031, val_acc:0.984]
Epoch [69/120    avg_loss:0.020, val_acc:0.987]
Epoch [70/120    avg_loss:0.027, val_acc:0.980]
Epoch [71/120    avg_loss:0.021, val_acc:0.976]
Epoch [72/120    avg_loss:0.021, val_acc:0.968]
Epoch [73/120    avg_loss:0.024, val_acc:0.985]
Epoch [74/120    avg_loss:0.052, val_acc:0.968]
Epoch [75/120    avg_loss:0.124, val_acc:0.977]
Epoch [76/120    avg_loss:0.113, val_acc:0.956]
Epoch [77/120    avg_loss:0.050, val_acc:0.976]
Epoch [78/120    avg_loss:0.023, val_acc:0.986]
Epoch [79/120    avg_loss:0.019, val_acc:0.981]
Epoch [80/120    avg_loss:0.015, val_acc:0.981]
Epoch [81/120    avg_loss:0.014, val_acc:0.991]
Epoch [82/120    avg_loss:0.018, val_acc:0.976]
Epoch [83/120    avg_loss:0.014, val_acc:0.986]
Epoch [84/120    avg_loss:0.026, val_acc:0.984]
Epoch [85/120    avg_loss:0.016, val_acc:0.984]
Epoch [86/120    avg_loss:0.012, val_acc:0.986]
Epoch [87/120    avg_loss:0.013, val_acc:0.985]
Epoch [88/120    avg_loss:0.013, val_acc:0.987]
Epoch [89/120    avg_loss:0.010, val_acc:0.975]
Epoch [90/120    avg_loss:0.012, val_acc:0.989]
Epoch [91/120    avg_loss:0.013, val_acc:0.979]
Epoch [92/120    avg_loss:0.014, val_acc:0.989]
Epoch [93/120    avg_loss:0.023, val_acc:0.984]
Epoch [94/120    avg_loss:0.015, val_acc:0.981]
Epoch [95/120    avg_loss:0.011, val_acc:0.986]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.008, val_acc:0.989]
Epoch [99/120    avg_loss:0.006, val_acc:0.990]
Epoch [100/120    avg_loss:0.006, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.990]
Epoch [103/120    avg_loss:0.004, val_acc:0.991]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.990]
Epoch [106/120    avg_loss:0.008, val_acc:0.991]
Epoch [107/120    avg_loss:0.006, val_acc:0.990]
Epoch [108/120    avg_loss:0.006, val_acc:0.990]
Epoch [109/120    avg_loss:0.005, val_acc:0.990]
Epoch [110/120    avg_loss:0.004, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.991]
Epoch [113/120    avg_loss:0.005, val_acc:0.991]
Epoch [114/120    avg_loss:0.013, val_acc:0.989]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.005, val_acc:0.990]
Epoch [117/120    avg_loss:0.008, val_acc:0.991]
Epoch [118/120    avg_loss:0.004, val_acc:0.991]
Epoch [119/120    avg_loss:0.006, val_acc:0.991]
Epoch [120/120    avg_loss:0.005, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1263    3    0    0    1    0    0    0    8    8    2    0
     0    0    0]
 [   0    0    1  688    0    2    0    0    0    3    1    0   50    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2   26    0    5    3    0    0    0  824    7    4    0
     0    4    0]
 [   0    0    3    0    0    0    5    0    0    0    5 2181   15    1
     0    0    0]
 [   0    0    0    9    7    2    0    0    0    0    6   12  495    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    1    0    0    0
  1131    1    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    55  289    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.0840108401084

F1 scores:
[       nan 0.98765432 0.98864971 0.934148   0.98383372 0.98074745
 0.99095023 1.         1.         0.87804878 0.95813953 0.98732458
 0.9        0.9919571  0.97290323 0.90171607 0.98245614]

Kappa:
0.9667539696083836
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:15:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7c59372748>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.141, val_acc:0.499]
Epoch [2/120    avg_loss:1.585, val_acc:0.624]
Epoch [3/120    avg_loss:1.363, val_acc:0.620]
Epoch [4/120    avg_loss:1.089, val_acc:0.684]
Epoch [5/120    avg_loss:0.898, val_acc:0.770]
Epoch [6/120    avg_loss:0.835, val_acc:0.743]
Epoch [7/120    avg_loss:0.751, val_acc:0.766]
Epoch [8/120    avg_loss:0.627, val_acc:0.798]
Epoch [9/120    avg_loss:0.617, val_acc:0.757]
Epoch [10/120    avg_loss:0.618, val_acc:0.829]
Epoch [11/120    avg_loss:0.464, val_acc:0.834]
Epoch [12/120    avg_loss:0.507, val_acc:0.855]
Epoch [13/120    avg_loss:0.455, val_acc:0.861]
Epoch [14/120    avg_loss:0.421, val_acc:0.856]
Epoch [15/120    avg_loss:0.410, val_acc:0.864]
Epoch [16/120    avg_loss:0.458, val_acc:0.875]
Epoch [17/120    avg_loss:0.369, val_acc:0.896]
Epoch [18/120    avg_loss:0.374, val_acc:0.875]
Epoch [19/120    avg_loss:0.331, val_acc:0.868]
Epoch [20/120    avg_loss:0.387, val_acc:0.877]
Epoch [21/120    avg_loss:0.274, val_acc:0.885]
Epoch [22/120    avg_loss:0.266, val_acc:0.912]
Epoch [23/120    avg_loss:0.210, val_acc:0.899]
Epoch [24/120    avg_loss:0.264, val_acc:0.905]
Epoch [25/120    avg_loss:0.209, val_acc:0.896]
Epoch [26/120    avg_loss:0.229, val_acc:0.910]
Epoch [27/120    avg_loss:0.208, val_acc:0.916]
Epoch [28/120    avg_loss:0.204, val_acc:0.913]
Epoch [29/120    avg_loss:0.241, val_acc:0.912]
Epoch [30/120    avg_loss:0.178, val_acc:0.927]
Epoch [31/120    avg_loss:0.229, val_acc:0.925]
Epoch [32/120    avg_loss:0.181, val_acc:0.932]
Epoch [33/120    avg_loss:0.157, val_acc:0.927]
Epoch [34/120    avg_loss:0.192, val_acc:0.943]
Epoch [35/120    avg_loss:0.147, val_acc:0.929]
Epoch [36/120    avg_loss:0.148, val_acc:0.920]
Epoch [37/120    avg_loss:0.123, val_acc:0.929]
Epoch [38/120    avg_loss:0.080, val_acc:0.957]
Epoch [39/120    avg_loss:0.155, val_acc:0.940]
Epoch [40/120    avg_loss:0.121, val_acc:0.926]
Epoch [41/120    avg_loss:0.113, val_acc:0.950]
Epoch [42/120    avg_loss:0.083, val_acc:0.953]
Epoch [43/120    avg_loss:0.104, val_acc:0.955]
Epoch [44/120    avg_loss:0.086, val_acc:0.961]
Epoch [45/120    avg_loss:0.098, val_acc:0.956]
Epoch [46/120    avg_loss:0.135, val_acc:0.921]
Epoch [47/120    avg_loss:0.110, val_acc:0.955]
Epoch [48/120    avg_loss:0.070, val_acc:0.954]
Epoch [49/120    avg_loss:0.213, val_acc:0.917]
Epoch [50/120    avg_loss:0.126, val_acc:0.934]
Epoch [51/120    avg_loss:0.090, val_acc:0.971]
Epoch [52/120    avg_loss:0.083, val_acc:0.964]
Epoch [53/120    avg_loss:0.071, val_acc:0.976]
Epoch [54/120    avg_loss:0.075, val_acc:0.964]
Epoch [55/120    avg_loss:0.053, val_acc:0.970]
Epoch [56/120    avg_loss:0.077, val_acc:0.961]
Epoch [57/120    avg_loss:0.100, val_acc:0.965]
Epoch [58/120    avg_loss:0.065, val_acc:0.971]
Epoch [59/120    avg_loss:0.072, val_acc:0.975]
Epoch [60/120    avg_loss:0.064, val_acc:0.977]
Epoch [61/120    avg_loss:0.061, val_acc:0.953]
Epoch [62/120    avg_loss:0.054, val_acc:0.970]
Epoch [63/120    avg_loss:0.094, val_acc:0.896]
Epoch [64/120    avg_loss:0.039, val_acc:0.981]
Epoch [65/120    avg_loss:0.039, val_acc:0.980]
Epoch [66/120    avg_loss:0.036, val_acc:0.970]
Epoch [67/120    avg_loss:0.029, val_acc:0.977]
Epoch [68/120    avg_loss:0.027, val_acc:0.981]
Epoch [69/120    avg_loss:0.028, val_acc:0.979]
Epoch [70/120    avg_loss:0.033, val_acc:0.980]
Epoch [71/120    avg_loss:0.042, val_acc:0.967]
Epoch [72/120    avg_loss:0.075, val_acc:0.961]
Epoch [73/120    avg_loss:0.045, val_acc:0.975]
Epoch [74/120    avg_loss:0.038, val_acc:0.982]
Epoch [75/120    avg_loss:0.022, val_acc:0.979]
Epoch [76/120    avg_loss:0.033, val_acc:0.980]
Epoch [77/120    avg_loss:0.028, val_acc:0.975]
Epoch [78/120    avg_loss:0.027, val_acc:0.976]
Epoch [79/120    avg_loss:0.024, val_acc:0.985]
Epoch [80/120    avg_loss:0.021, val_acc:0.971]
Epoch [81/120    avg_loss:0.072, val_acc:0.940]
Epoch [82/120    avg_loss:0.043, val_acc:0.984]
Epoch [83/120    avg_loss:0.022, val_acc:0.982]
Epoch [84/120    avg_loss:0.024, val_acc:0.983]
Epoch [85/120    avg_loss:0.020, val_acc:0.967]
Epoch [86/120    avg_loss:0.028, val_acc:0.980]
Epoch [87/120    avg_loss:0.018, val_acc:0.982]
Epoch [88/120    avg_loss:0.020, val_acc:0.962]
Epoch [89/120    avg_loss:0.023, val_acc:0.980]
Epoch [90/120    avg_loss:0.028, val_acc:0.981]
Epoch [91/120    avg_loss:0.033, val_acc:0.975]
Epoch [92/120    avg_loss:0.041, val_acc:0.974]
Epoch [93/120    avg_loss:0.029, val_acc:0.988]
Epoch [94/120    avg_loss:0.015, val_acc:0.989]
Epoch [95/120    avg_loss:0.012, val_acc:0.990]
Epoch [96/120    avg_loss:0.013, val_acc:0.990]
Epoch [97/120    avg_loss:0.013, val_acc:0.992]
Epoch [98/120    avg_loss:0.009, val_acc:0.993]
Epoch [99/120    avg_loss:0.013, val_acc:0.992]
Epoch [100/120    avg_loss:0.011, val_acc:0.993]
Epoch [101/120    avg_loss:0.008, val_acc:0.993]
Epoch [102/120    avg_loss:0.011, val_acc:0.994]
Epoch [103/120    avg_loss:0.009, val_acc:0.992]
Epoch [104/120    avg_loss:0.009, val_acc:0.992]
Epoch [105/120    avg_loss:0.010, val_acc:0.993]
Epoch [106/120    avg_loss:0.011, val_acc:0.992]
Epoch [107/120    avg_loss:0.008, val_acc:0.993]
Epoch [108/120    avg_loss:0.009, val_acc:0.993]
Epoch [109/120    avg_loss:0.009, val_acc:0.991]
Epoch [110/120    avg_loss:0.008, val_acc:0.991]
Epoch [111/120    avg_loss:0.006, val_acc:0.991]
Epoch [112/120    avg_loss:0.007, val_acc:0.993]
Epoch [113/120    avg_loss:0.009, val_acc:0.991]
Epoch [114/120    avg_loss:0.008, val_acc:0.993]
Epoch [115/120    avg_loss:0.007, val_acc:0.991]
Epoch [116/120    avg_loss:0.009, val_acc:0.992]
Epoch [117/120    avg_loss:0.009, val_acc:0.993]
Epoch [118/120    avg_loss:0.009, val_acc:0.993]
Epoch [119/120    avg_loss:0.007, val_acc:0.993]
Epoch [120/120    avg_loss:0.009, val_acc:0.993]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1273    3    0    0    0    0    0    0    1    5    0    0
     0    3    0]
 [   0    0    0  721    0   19    0    0    0    2    0    0    0    5
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    0   88    0    5    0    0    0    0  767    5    5    0
     0    5    0]
 [   0    0    7    0    0    4   12    0    0    0   10 2174    1    2
     0    0    0]
 [   0    0    0   10    2    3    0    0    0    0    3   13  500    0
     0    0    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    9    0    0    1    0    3    1    0    0
  1125    0    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
   106  225    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.0650406504065

F1 scores:
[       nan 0.98765432 0.99220577 0.9173028  0.9953271  0.95038589
 0.97837435 1.         0.99883856 0.83333333 0.92465341 0.98616466
 0.96061479 0.9787234  0.94816688 0.77586207 0.97647059]

Kappa:
0.9551142299569306
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f89ff645748>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.148, val_acc:0.493]
Epoch [2/120    avg_loss:1.587, val_acc:0.622]
Epoch [3/120    avg_loss:1.285, val_acc:0.691]
Epoch [4/120    avg_loss:1.058, val_acc:0.731]
Epoch [5/120    avg_loss:0.893, val_acc:0.703]
Epoch [6/120    avg_loss:0.851, val_acc:0.777]
Epoch [7/120    avg_loss:0.700, val_acc:0.788]
Epoch [8/120    avg_loss:0.747, val_acc:0.806]
Epoch [9/120    avg_loss:0.662, val_acc:0.791]
Epoch [10/120    avg_loss:0.548, val_acc:0.821]
Epoch [11/120    avg_loss:0.562, val_acc:0.830]
Epoch [12/120    avg_loss:0.566, val_acc:0.826]
Epoch [13/120    avg_loss:0.499, val_acc:0.853]
Epoch [14/120    avg_loss:0.420, val_acc:0.875]
Epoch [15/120    avg_loss:0.358, val_acc:0.871]
Epoch [16/120    avg_loss:0.397, val_acc:0.896]
Epoch [17/120    avg_loss:0.387, val_acc:0.901]
Epoch [18/120    avg_loss:0.349, val_acc:0.886]
Epoch [19/120    avg_loss:0.319, val_acc:0.913]
Epoch [20/120    avg_loss:0.362, val_acc:0.863]
Epoch [21/120    avg_loss:0.295, val_acc:0.895]
Epoch [22/120    avg_loss:0.297, val_acc:0.891]
Epoch [23/120    avg_loss:0.286, val_acc:0.915]
Epoch [24/120    avg_loss:0.223, val_acc:0.919]
Epoch [25/120    avg_loss:0.286, val_acc:0.875]
Epoch [26/120    avg_loss:0.276, val_acc:0.924]
Epoch [27/120    avg_loss:0.236, val_acc:0.934]
Epoch [28/120    avg_loss:0.212, val_acc:0.930]
Epoch [29/120    avg_loss:0.159, val_acc:0.955]
Epoch [30/120    avg_loss:0.158, val_acc:0.954]
Epoch [31/120    avg_loss:0.185, val_acc:0.933]
Epoch [32/120    avg_loss:0.355, val_acc:0.917]
Epoch [33/120    avg_loss:0.161, val_acc:0.940]
Epoch [34/120    avg_loss:0.140, val_acc:0.956]
Epoch [35/120    avg_loss:0.194, val_acc:0.955]
Epoch [36/120    avg_loss:0.116, val_acc:0.961]
Epoch [37/120    avg_loss:0.129, val_acc:0.951]
Epoch [38/120    avg_loss:0.136, val_acc:0.950]
Epoch [39/120    avg_loss:0.135, val_acc:0.927]
Epoch [40/120    avg_loss:0.158, val_acc:0.871]
Epoch [41/120    avg_loss:0.219, val_acc:0.939]
Epoch [42/120    avg_loss:0.111, val_acc:0.917]
Epoch [43/120    avg_loss:0.106, val_acc:0.952]
Epoch [44/120    avg_loss:0.137, val_acc:0.968]
Epoch [45/120    avg_loss:0.088, val_acc:0.944]
Epoch [46/120    avg_loss:0.071, val_acc:0.973]
Epoch [47/120    avg_loss:0.069, val_acc:0.968]
Epoch [48/120    avg_loss:0.063, val_acc:0.979]
Epoch [49/120    avg_loss:0.050, val_acc:0.979]
Epoch [50/120    avg_loss:0.058, val_acc:0.961]
Epoch [51/120    avg_loss:0.067, val_acc:0.968]
Epoch [52/120    avg_loss:0.121, val_acc:0.973]
Epoch [53/120    avg_loss:0.044, val_acc:0.974]
Epoch [54/120    avg_loss:0.050, val_acc:0.981]
Epoch [55/120    avg_loss:0.039, val_acc:0.977]
Epoch [56/120    avg_loss:0.039, val_acc:0.979]
Epoch [57/120    avg_loss:0.085, val_acc:0.949]
Epoch [58/120    avg_loss:0.043, val_acc:0.979]
Epoch [59/120    avg_loss:0.041, val_acc:0.969]
Epoch [60/120    avg_loss:0.048, val_acc:0.975]
Epoch [61/120    avg_loss:0.031, val_acc:0.984]
Epoch [62/120    avg_loss:0.041, val_acc:0.984]
Epoch [63/120    avg_loss:0.044, val_acc:0.984]
Epoch [64/120    avg_loss:0.043, val_acc:0.968]
Epoch [65/120    avg_loss:0.065, val_acc:0.936]
Epoch [66/120    avg_loss:0.081, val_acc:0.978]
Epoch [67/120    avg_loss:0.047, val_acc:0.984]
Epoch [68/120    avg_loss:0.031, val_acc:0.981]
Epoch [69/120    avg_loss:0.032, val_acc:0.980]
Epoch [70/120    avg_loss:0.035, val_acc:0.985]
Epoch [71/120    avg_loss:0.036, val_acc:0.981]
Epoch [72/120    avg_loss:0.031, val_acc:0.961]
Epoch [73/120    avg_loss:0.029, val_acc:0.988]
Epoch [74/120    avg_loss:0.056, val_acc:0.979]
Epoch [75/120    avg_loss:0.035, val_acc:0.982]
Epoch [76/120    avg_loss:0.041, val_acc:0.973]
Epoch [77/120    avg_loss:0.055, val_acc:0.973]
Epoch [78/120    avg_loss:0.041, val_acc:0.970]
Epoch [79/120    avg_loss:0.054, val_acc:0.958]
Epoch [80/120    avg_loss:0.079, val_acc:0.980]
Epoch [81/120    avg_loss:0.047, val_acc:0.969]
Epoch [82/120    avg_loss:0.041, val_acc:0.972]
Epoch [83/120    avg_loss:0.040, val_acc:0.984]
Epoch [84/120    avg_loss:0.021, val_acc:0.987]
Epoch [85/120    avg_loss:0.019, val_acc:0.975]
Epoch [86/120    avg_loss:0.028, val_acc:0.979]
Epoch [87/120    avg_loss:0.016, val_acc:0.983]
Epoch [88/120    avg_loss:0.011, val_acc:0.984]
Epoch [89/120    avg_loss:0.015, val_acc:0.987]
Epoch [90/120    avg_loss:0.011, val_acc:0.985]
Epoch [91/120    avg_loss:0.011, val_acc:0.987]
Epoch [92/120    avg_loss:0.010, val_acc:0.987]
Epoch [93/120    avg_loss:0.009, val_acc:0.987]
Epoch [94/120    avg_loss:0.009, val_acc:0.987]
Epoch [95/120    avg_loss:0.009, val_acc:0.987]
Epoch [96/120    avg_loss:0.009, val_acc:0.987]
Epoch [97/120    avg_loss:0.011, val_acc:0.987]
Epoch [98/120    avg_loss:0.008, val_acc:0.987]
Epoch [99/120    avg_loss:0.009, val_acc:0.987]
Epoch [100/120    avg_loss:0.009, val_acc:0.987]
Epoch [101/120    avg_loss:0.008, val_acc:0.987]
Epoch [102/120    avg_loss:0.010, val_acc:0.987]
Epoch [103/120    avg_loss:0.010, val_acc:0.987]
Epoch [104/120    avg_loss:0.013, val_acc:0.987]
Epoch [105/120    avg_loss:0.007, val_acc:0.987]
Epoch [106/120    avg_loss:0.008, val_acc:0.987]
Epoch [107/120    avg_loss:0.008, val_acc:0.987]
Epoch [108/120    avg_loss:0.008, val_acc:0.987]
Epoch [109/120    avg_loss:0.008, val_acc:0.987]
Epoch [110/120    avg_loss:0.008, val_acc:0.987]
Epoch [111/120    avg_loss:0.009, val_acc:0.987]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.008, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.009, val_acc:0.988]
Epoch [118/120    avg_loss:0.008, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.008, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    6    0    0    1    0    0    0    3   11    3    0
     0    1    0]
 [   0    0    0  729    0   14    0    0    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    3    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0   74    0    5    0    0    0    0  766   26    2    0
     0    2    0]
 [   0    0    4    0    0    1   11    0    0    0   10 2182    0    2
     0    0    0]
 [   0    0    0   21    3    3    0    0    0    0    8    8  488    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    70  274    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.65040650406505

F1 scores:
[       nan 0.98765432 0.98823529 0.92454027 0.99300699 0.96730552
 0.98795181 1.         0.99883856 0.8372093  0.9212267  0.98332582
 0.9503408  0.99462366 0.96807152 0.87820513 0.98245614]

Kappa:
0.961784453175111
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f82cca0d710>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.107, val_acc:0.536]
Epoch [2/120    avg_loss:1.528, val_acc:0.598]
Epoch [3/120    avg_loss:1.279, val_acc:0.599]
Epoch [4/120    avg_loss:1.103, val_acc:0.728]
Epoch [5/120    avg_loss:0.968, val_acc:0.688]
Epoch [6/120    avg_loss:0.792, val_acc:0.777]
Epoch [7/120    avg_loss:0.795, val_acc:0.781]
Epoch [8/120    avg_loss:0.730, val_acc:0.793]
Epoch [9/120    avg_loss:0.607, val_acc:0.734]
Epoch [10/120    avg_loss:0.543, val_acc:0.830]
Epoch [11/120    avg_loss:0.511, val_acc:0.838]
Epoch [12/120    avg_loss:0.526, val_acc:0.825]
Epoch [13/120    avg_loss:0.473, val_acc:0.823]
Epoch [14/120    avg_loss:0.425, val_acc:0.818]
Epoch [15/120    avg_loss:0.388, val_acc:0.843]
Epoch [16/120    avg_loss:0.355, val_acc:0.869]
Epoch [17/120    avg_loss:0.385, val_acc:0.861]
Epoch [18/120    avg_loss:0.422, val_acc:0.857]
Epoch [19/120    avg_loss:0.373, val_acc:0.864]
Epoch [20/120    avg_loss:0.351, val_acc:0.870]
Epoch [21/120    avg_loss:0.347, val_acc:0.876]
Epoch [22/120    avg_loss:0.257, val_acc:0.911]
Epoch [23/120    avg_loss:0.237, val_acc:0.885]
Epoch [24/120    avg_loss:0.265, val_acc:0.883]
Epoch [25/120    avg_loss:0.325, val_acc:0.897]
Epoch [26/120    avg_loss:0.286, val_acc:0.885]
Epoch [27/120    avg_loss:0.256, val_acc:0.906]
Epoch [28/120    avg_loss:0.181, val_acc:0.930]
Epoch [29/120    avg_loss:0.215, val_acc:0.922]
Epoch [30/120    avg_loss:0.181, val_acc:0.931]
Epoch [31/120    avg_loss:0.225, val_acc:0.923]
Epoch [32/120    avg_loss:0.153, val_acc:0.943]
Epoch [33/120    avg_loss:0.137, val_acc:0.909]
Epoch [34/120    avg_loss:0.170, val_acc:0.947]
Epoch [35/120    avg_loss:0.154, val_acc:0.947]
Epoch [36/120    avg_loss:0.111, val_acc:0.948]
Epoch [37/120    avg_loss:0.133, val_acc:0.944]
Epoch [38/120    avg_loss:0.140, val_acc:0.910]
Epoch [39/120    avg_loss:0.149, val_acc:0.903]
Epoch [40/120    avg_loss:0.136, val_acc:0.920]
Epoch [41/120    avg_loss:0.148, val_acc:0.942]
Epoch [42/120    avg_loss:0.113, val_acc:0.956]
Epoch [43/120    avg_loss:0.107, val_acc:0.934]
Epoch [44/120    avg_loss:0.106, val_acc:0.950]
Epoch [45/120    avg_loss:0.081, val_acc:0.947]
Epoch [46/120    avg_loss:0.125, val_acc:0.956]
Epoch [47/120    avg_loss:0.089, val_acc:0.920]
Epoch [48/120    avg_loss:0.103, val_acc:0.940]
Epoch [49/120    avg_loss:0.085, val_acc:0.958]
Epoch [50/120    avg_loss:0.070, val_acc:0.959]
Epoch [51/120    avg_loss:0.079, val_acc:0.927]
Epoch [52/120    avg_loss:0.105, val_acc:0.945]
Epoch [53/120    avg_loss:0.075, val_acc:0.969]
Epoch [54/120    avg_loss:0.054, val_acc:0.963]
Epoch [55/120    avg_loss:0.069, val_acc:0.952]
Epoch [56/120    avg_loss:0.053, val_acc:0.960]
Epoch [57/120    avg_loss:0.064, val_acc:0.950]
Epoch [58/120    avg_loss:0.058, val_acc:0.968]
Epoch [59/120    avg_loss:0.073, val_acc:0.943]
Epoch [60/120    avg_loss:0.079, val_acc:0.945]
Epoch [61/120    avg_loss:0.072, val_acc:0.957]
Epoch [62/120    avg_loss:0.062, val_acc:0.868]
Epoch [63/120    avg_loss:0.226, val_acc:0.930]
Epoch [64/120    avg_loss:0.131, val_acc:0.925]
Epoch [65/120    avg_loss:0.139, val_acc:0.943]
Epoch [66/120    avg_loss:0.071, val_acc:0.947]
Epoch [67/120    avg_loss:0.043, val_acc:0.969]
Epoch [68/120    avg_loss:0.035, val_acc:0.973]
Epoch [69/120    avg_loss:0.024, val_acc:0.972]
Epoch [70/120    avg_loss:0.033, val_acc:0.976]
Epoch [71/120    avg_loss:0.029, val_acc:0.978]
Epoch [72/120    avg_loss:0.029, val_acc:0.977]
Epoch [73/120    avg_loss:0.033, val_acc:0.981]
Epoch [74/120    avg_loss:0.022, val_acc:0.980]
Epoch [75/120    avg_loss:0.025, val_acc:0.982]
Epoch [76/120    avg_loss:0.021, val_acc:0.985]
Epoch [77/120    avg_loss:0.024, val_acc:0.984]
Epoch [78/120    avg_loss:0.022, val_acc:0.983]
Epoch [79/120    avg_loss:0.022, val_acc:0.984]
Epoch [80/120    avg_loss:0.022, val_acc:0.985]
Epoch [81/120    avg_loss:0.026, val_acc:0.983]
Epoch [82/120    avg_loss:0.022, val_acc:0.983]
Epoch [83/120    avg_loss:0.024, val_acc:0.988]
Epoch [84/120    avg_loss:0.019, val_acc:0.982]
Epoch [85/120    avg_loss:0.019, val_acc:0.986]
Epoch [86/120    avg_loss:0.023, val_acc:0.988]
Epoch [87/120    avg_loss:0.018, val_acc:0.984]
Epoch [88/120    avg_loss:0.021, val_acc:0.982]
Epoch [89/120    avg_loss:0.019, val_acc:0.986]
Epoch [90/120    avg_loss:0.018, val_acc:0.986]
Epoch [91/120    avg_loss:0.018, val_acc:0.988]
Epoch [92/120    avg_loss:0.019, val_acc:0.985]
Epoch [93/120    avg_loss:0.018, val_acc:0.985]
Epoch [94/120    avg_loss:0.023, val_acc:0.983]
Epoch [95/120    avg_loss:0.020, val_acc:0.986]
Epoch [96/120    avg_loss:0.022, val_acc:0.989]
Epoch [97/120    avg_loss:0.022, val_acc:0.983]
Epoch [98/120    avg_loss:0.020, val_acc:0.984]
Epoch [99/120    avg_loss:0.020, val_acc:0.986]
Epoch [100/120    avg_loss:0.026, val_acc:0.984]
Epoch [101/120    avg_loss:0.019, val_acc:0.983]
Epoch [102/120    avg_loss:0.019, val_acc:0.985]
Epoch [103/120    avg_loss:0.018, val_acc:0.985]
Epoch [104/120    avg_loss:0.015, val_acc:0.986]
Epoch [105/120    avg_loss:0.016, val_acc:0.986]
Epoch [106/120    avg_loss:0.019, val_acc:0.984]
Epoch [107/120    avg_loss:0.018, val_acc:0.986]
Epoch [108/120    avg_loss:0.014, val_acc:0.986]
Epoch [109/120    avg_loss:0.014, val_acc:0.988]
Epoch [110/120    avg_loss:0.020, val_acc:0.988]
Epoch [111/120    avg_loss:0.018, val_acc:0.988]
Epoch [112/120    avg_loss:0.013, val_acc:0.988]
Epoch [113/120    avg_loss:0.015, val_acc:0.988]
Epoch [114/120    avg_loss:0.016, val_acc:0.988]
Epoch [115/120    avg_loss:0.014, val_acc:0.986]
Epoch [116/120    avg_loss:0.015, val_acc:0.988]
Epoch [117/120    avg_loss:0.017, val_acc:0.986]
Epoch [118/120    avg_loss:0.013, val_acc:0.986]
Epoch [119/120    avg_loss:0.017, val_acc:0.986]
Epoch [120/120    avg_loss:0.015, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1242    3    0    0    1    0    0    0   14    8    5    0
     0   12    0]
 [   0    0    0  736    0    3    0    0    0    4    0    0    0    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    1    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3   88    0    4    0    0    0    0  773    0    0    0
     0    7    0]
 [   0    0   13    1    0    4   14    0    0    0   19 2154    0    3
     2    0    0]
 [   0    0    0   20    3    3    0    0    0    0   13    1  475    0
     2    0   17]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    1    0    3    0    0    0
  1132    1    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    64  268    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.02168021680217

F1 scores:
[       nan 0.98765432 0.97641509 0.92288401 0.99300699 0.9738339
 0.97615499 1.         0.99883856 0.87804878 0.91101945 0.98491084
 0.93688363 0.98143236 0.96504689 0.84409449 0.90810811]

Kappa:
0.9546749373378657
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9f8f071780>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.179, val_acc:0.581]
Epoch [2/120    avg_loss:1.615, val_acc:0.570]
Epoch [3/120    avg_loss:1.301, val_acc:0.691]
Epoch [4/120    avg_loss:1.071, val_acc:0.752]
Epoch [5/120    avg_loss:0.914, val_acc:0.735]
Epoch [6/120    avg_loss:0.825, val_acc:0.755]
Epoch [7/120    avg_loss:0.764, val_acc:0.798]
Epoch [8/120    avg_loss:0.616, val_acc:0.802]
Epoch [9/120    avg_loss:0.622, val_acc:0.809]
Epoch [10/120    avg_loss:0.542, val_acc:0.840]
Epoch [11/120    avg_loss:0.566, val_acc:0.859]
Epoch [12/120    avg_loss:0.457, val_acc:0.838]
Epoch [13/120    avg_loss:0.422, val_acc:0.873]
Epoch [14/120    avg_loss:0.389, val_acc:0.881]
Epoch [15/120    avg_loss:0.379, val_acc:0.881]
Epoch [16/120    avg_loss:0.361, val_acc:0.876]
Epoch [17/120    avg_loss:0.314, val_acc:0.902]
Epoch [18/120    avg_loss:0.316, val_acc:0.843]
Epoch [19/120    avg_loss:0.283, val_acc:0.896]
Epoch [20/120    avg_loss:0.311, val_acc:0.894]
Epoch [21/120    avg_loss:0.231, val_acc:0.919]
Epoch [22/120    avg_loss:0.236, val_acc:0.899]
Epoch [23/120    avg_loss:0.215, val_acc:0.930]
Epoch [24/120    avg_loss:0.214, val_acc:0.908]
Epoch [25/120    avg_loss:0.197, val_acc:0.930]
Epoch [26/120    avg_loss:0.194, val_acc:0.907]
Epoch [27/120    avg_loss:0.179, val_acc:0.910]
Epoch [28/120    avg_loss:0.132, val_acc:0.954]
Epoch [29/120    avg_loss:0.130, val_acc:0.928]
Epoch [30/120    avg_loss:0.144, val_acc:0.928]
Epoch [31/120    avg_loss:0.142, val_acc:0.945]
Epoch [32/120    avg_loss:0.146, val_acc:0.945]
Epoch [33/120    avg_loss:0.157, val_acc:0.892]
Epoch [34/120    avg_loss:0.134, val_acc:0.956]
Epoch [35/120    avg_loss:0.100, val_acc:0.943]
Epoch [36/120    avg_loss:0.155, val_acc:0.892]
Epoch [37/120    avg_loss:0.167, val_acc:0.946]
Epoch [38/120    avg_loss:0.111, val_acc:0.937]
Epoch [39/120    avg_loss:0.177, val_acc:0.943]
Epoch [40/120    avg_loss:0.243, val_acc:0.928]
Epoch [41/120    avg_loss:0.192, val_acc:0.952]
Epoch [42/120    avg_loss:0.121, val_acc:0.945]
Epoch [43/120    avg_loss:0.127, val_acc:0.925]
Epoch [44/120    avg_loss:0.108, val_acc:0.954]
Epoch [45/120    avg_loss:0.112, val_acc:0.959]
Epoch [46/120    avg_loss:0.083, val_acc:0.961]
Epoch [47/120    avg_loss:0.149, val_acc:0.948]
Epoch [48/120    avg_loss:0.121, val_acc:0.931]
Epoch [49/120    avg_loss:0.073, val_acc:0.968]
Epoch [50/120    avg_loss:0.057, val_acc:0.966]
Epoch [51/120    avg_loss:0.067, val_acc:0.963]
Epoch [52/120    avg_loss:0.076, val_acc:0.946]
Epoch [53/120    avg_loss:0.081, val_acc:0.975]
Epoch [54/120    avg_loss:0.061, val_acc:0.983]
Epoch [55/120    avg_loss:0.043, val_acc:0.964]
Epoch [56/120    avg_loss:0.057, val_acc:0.954]
Epoch [57/120    avg_loss:0.037, val_acc:0.976]
Epoch [58/120    avg_loss:0.041, val_acc:0.972]
Epoch [59/120    avg_loss:0.048, val_acc:0.977]
Epoch [60/120    avg_loss:0.052, val_acc:0.966]
Epoch [61/120    avg_loss:0.083, val_acc:0.958]
Epoch [62/120    avg_loss:0.044, val_acc:0.984]
Epoch [63/120    avg_loss:0.026, val_acc:0.986]
Epoch [64/120    avg_loss:0.036, val_acc:0.983]
Epoch [65/120    avg_loss:0.021, val_acc:0.984]
Epoch [66/120    avg_loss:0.039, val_acc:0.975]
Epoch [67/120    avg_loss:0.026, val_acc:0.986]
Epoch [68/120    avg_loss:0.023, val_acc:0.989]
Epoch [69/120    avg_loss:0.024, val_acc:0.972]
Epoch [70/120    avg_loss:0.052, val_acc:0.976]
Epoch [71/120    avg_loss:0.050, val_acc:0.977]
Epoch [72/120    avg_loss:0.030, val_acc:0.974]
Epoch [73/120    avg_loss:0.033, val_acc:0.988]
Epoch [74/120    avg_loss:0.020, val_acc:0.981]
Epoch [75/120    avg_loss:0.018, val_acc:0.984]
Epoch [76/120    avg_loss:0.024, val_acc:0.986]
Epoch [77/120    avg_loss:0.017, val_acc:0.985]
Epoch [78/120    avg_loss:0.012, val_acc:0.989]
Epoch [79/120    avg_loss:0.058, val_acc:0.955]
Epoch [80/120    avg_loss:0.039, val_acc:0.976]
Epoch [81/120    avg_loss:0.033, val_acc:0.948]
Epoch [82/120    avg_loss:0.042, val_acc:0.984]
Epoch [83/120    avg_loss:0.034, val_acc:0.984]
Epoch [84/120    avg_loss:0.030, val_acc:0.985]
Epoch [85/120    avg_loss:0.016, val_acc:0.984]
Epoch [86/120    avg_loss:0.014, val_acc:0.988]
Epoch [87/120    avg_loss:0.018, val_acc:0.992]
Epoch [88/120    avg_loss:0.019, val_acc:0.989]
Epoch [89/120    avg_loss:0.012, val_acc:0.991]
Epoch [90/120    avg_loss:0.009, val_acc:0.986]
Epoch [91/120    avg_loss:0.015, val_acc:0.989]
Epoch [92/120    avg_loss:0.012, val_acc:0.989]
Epoch [93/120    avg_loss:0.008, val_acc:0.991]
Epoch [94/120    avg_loss:0.009, val_acc:0.990]
Epoch [95/120    avg_loss:0.008, val_acc:0.990]
Epoch [96/120    avg_loss:0.006, val_acc:0.990]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.014, val_acc:0.986]
Epoch [99/120    avg_loss:0.007, val_acc:0.993]
Epoch [100/120    avg_loss:0.014, val_acc:0.968]
Epoch [101/120    avg_loss:0.018, val_acc:0.993]
Epoch [102/120    avg_loss:0.024, val_acc:0.989]
Epoch [103/120    avg_loss:0.011, val_acc:0.985]
Epoch [104/120    avg_loss:0.008, val_acc:0.990]
Epoch [105/120    avg_loss:0.010, val_acc:0.991]
Epoch [106/120    avg_loss:0.064, val_acc:0.858]
Epoch [107/120    avg_loss:0.078, val_acc:0.985]
Epoch [108/120    avg_loss:0.016, val_acc:0.985]
Epoch [109/120    avg_loss:0.013, val_acc:0.992]
Epoch [110/120    avg_loss:0.007, val_acc:0.985]
Epoch [111/120    avg_loss:0.011, val_acc:0.990]
Epoch [112/120    avg_loss:0.010, val_acc:0.989]
Epoch [113/120    avg_loss:0.009, val_acc:0.985]
Epoch [114/120    avg_loss:0.010, val_acc:0.992]
Epoch [115/120    avg_loss:0.006, val_acc:0.992]
Epoch [116/120    avg_loss:0.007, val_acc:0.991]
Epoch [117/120    avg_loss:0.005, val_acc:0.991]
Epoch [118/120    avg_loss:0.005, val_acc:0.991]
Epoch [119/120    avg_loss:0.005, val_acc:0.991]
Epoch [120/120    avg_loss:0.006, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    5    0    0    0    0    0    0    5   11    1    0
     0    3    0]
 [   0    0    0  728    0    5    0    0    0    5    0    0    7    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    5    0    0    0    0    0    0  425    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0   75    0    4    1    0    0    0  765   27    0    0
     1    2    0]
 [   0    0    3    0    0    0    6    0    0    0    6 2193    0    2
     0    0    0]
 [   0    0    0    2    1    0    0    0    0    0   17   10  501    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    62  284    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.98644986449864

F1 scores:
[       nan 0.93023256 0.98862299 0.93513166 0.99765808 0.98745724
 0.99318698 1.         0.99415205 0.85714286 0.9167166  0.9851752
 0.95977011 0.98930481 0.97223409 0.89308176 0.97647059]

Kappa:
0.9656125605662027
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa5df680710>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.094, val_acc:0.566]
Epoch [2/120    avg_loss:1.574, val_acc:0.572]
Epoch [3/120    avg_loss:1.236, val_acc:0.693]
Epoch [4/120    avg_loss:1.125, val_acc:0.693]
Epoch [5/120    avg_loss:0.906, val_acc:0.714]
Epoch [6/120    avg_loss:0.775, val_acc:0.786]
Epoch [7/120    avg_loss:0.790, val_acc:0.798]
Epoch [8/120    avg_loss:0.715, val_acc:0.802]
Epoch [9/120    avg_loss:0.612, val_acc:0.827]
Epoch [10/120    avg_loss:0.517, val_acc:0.824]
Epoch [11/120    avg_loss:0.579, val_acc:0.856]
Epoch [12/120    avg_loss:0.454, val_acc:0.812]
Epoch [13/120    avg_loss:0.395, val_acc:0.830]
Epoch [14/120    avg_loss:0.465, val_acc:0.822]
Epoch [15/120    avg_loss:0.438, val_acc:0.872]
Epoch [16/120    avg_loss:0.345, val_acc:0.868]
Epoch [17/120    avg_loss:0.306, val_acc:0.869]
Epoch [18/120    avg_loss:0.394, val_acc:0.811]
Epoch [19/120    avg_loss:0.394, val_acc:0.842]
Epoch [20/120    avg_loss:0.375, val_acc:0.863]
Epoch [21/120    avg_loss:0.290, val_acc:0.901]
Epoch [22/120    avg_loss:0.297, val_acc:0.893]
Epoch [23/120    avg_loss:0.243, val_acc:0.900]
Epoch [24/120    avg_loss:0.215, val_acc:0.920]
Epoch [25/120    avg_loss:0.205, val_acc:0.905]
Epoch [26/120    avg_loss:0.234, val_acc:0.897]
Epoch [27/120    avg_loss:0.185, val_acc:0.894]
Epoch [28/120    avg_loss:0.214, val_acc:0.923]
Epoch [29/120    avg_loss:0.306, val_acc:0.874]
Epoch [30/120    avg_loss:0.213, val_acc:0.930]
Epoch [31/120    avg_loss:0.152, val_acc:0.920]
Epoch [32/120    avg_loss:0.205, val_acc:0.907]
Epoch [33/120    avg_loss:0.156, val_acc:0.948]
Epoch [34/120    avg_loss:0.129, val_acc:0.940]
Epoch [35/120    avg_loss:0.111, val_acc:0.939]
Epoch [36/120    avg_loss:0.131, val_acc:0.925]
Epoch [37/120    avg_loss:0.090, val_acc:0.941]
Epoch [38/120    avg_loss:0.134, val_acc:0.920]
Epoch [39/120    avg_loss:0.126, val_acc:0.944]
Epoch [40/120    avg_loss:0.119, val_acc:0.931]
Epoch [41/120    avg_loss:0.120, val_acc:0.932]
Epoch [42/120    avg_loss:0.143, val_acc:0.936]
Epoch [43/120    avg_loss:0.081, val_acc:0.932]
Epoch [44/120    avg_loss:0.078, val_acc:0.959]
Epoch [45/120    avg_loss:0.063, val_acc:0.940]
Epoch [46/120    avg_loss:0.072, val_acc:0.948]
Epoch [47/120    avg_loss:0.056, val_acc:0.949]
Epoch [48/120    avg_loss:0.088, val_acc:0.922]
Epoch [49/120    avg_loss:0.092, val_acc:0.940]
Epoch [50/120    avg_loss:0.220, val_acc:0.916]
Epoch [51/120    avg_loss:0.155, val_acc:0.948]
Epoch [52/120    avg_loss:0.079, val_acc:0.955]
Epoch [53/120    avg_loss:0.097, val_acc:0.950]
Epoch [54/120    avg_loss:0.087, val_acc:0.943]
Epoch [55/120    avg_loss:0.061, val_acc:0.948]
Epoch [56/120    avg_loss:0.066, val_acc:0.943]
Epoch [57/120    avg_loss:0.092, val_acc:0.949]
Epoch [58/120    avg_loss:0.055, val_acc:0.967]
Epoch [59/120    avg_loss:0.037, val_acc:0.970]
Epoch [60/120    avg_loss:0.034, val_acc:0.969]
Epoch [61/120    avg_loss:0.033, val_acc:0.970]
Epoch [62/120    avg_loss:0.029, val_acc:0.976]
Epoch [63/120    avg_loss:0.033, val_acc:0.973]
Epoch [64/120    avg_loss:0.027, val_acc:0.970]
Epoch [65/120    avg_loss:0.027, val_acc:0.973]
Epoch [66/120    avg_loss:0.028, val_acc:0.974]
Epoch [67/120    avg_loss:0.021, val_acc:0.973]
Epoch [68/120    avg_loss:0.022, val_acc:0.975]
Epoch [69/120    avg_loss:0.021, val_acc:0.976]
Epoch [70/120    avg_loss:0.028, val_acc:0.973]
Epoch [71/120    avg_loss:0.025, val_acc:0.975]
Epoch [72/120    avg_loss:0.023, val_acc:0.972]
Epoch [73/120    avg_loss:0.025, val_acc:0.974]
Epoch [74/120    avg_loss:0.023, val_acc:0.976]
Epoch [75/120    avg_loss:0.021, val_acc:0.974]
Epoch [76/120    avg_loss:0.021, val_acc:0.973]
Epoch [77/120    avg_loss:0.018, val_acc:0.973]
Epoch [78/120    avg_loss:0.018, val_acc:0.974]
Epoch [79/120    avg_loss:0.024, val_acc:0.976]
Epoch [80/120    avg_loss:0.025, val_acc:0.969]
Epoch [81/120    avg_loss:0.022, val_acc:0.974]
Epoch [82/120    avg_loss:0.017, val_acc:0.974]
Epoch [83/120    avg_loss:0.019, val_acc:0.973]
Epoch [84/120    avg_loss:0.017, val_acc:0.975]
Epoch [85/120    avg_loss:0.020, val_acc:0.973]
Epoch [86/120    avg_loss:0.018, val_acc:0.973]
Epoch [87/120    avg_loss:0.017, val_acc:0.973]
Epoch [88/120    avg_loss:0.021, val_acc:0.973]
Epoch [89/120    avg_loss:0.016, val_acc:0.973]
Epoch [90/120    avg_loss:0.022, val_acc:0.972]
Epoch [91/120    avg_loss:0.020, val_acc:0.973]
Epoch [92/120    avg_loss:0.022, val_acc:0.972]
Epoch [93/120    avg_loss:0.021, val_acc:0.972]
Epoch [94/120    avg_loss:0.016, val_acc:0.972]
Epoch [95/120    avg_loss:0.017, val_acc:0.972]
Epoch [96/120    avg_loss:0.017, val_acc:0.972]
Epoch [97/120    avg_loss:0.018, val_acc:0.972]
Epoch [98/120    avg_loss:0.017, val_acc:0.972]
Epoch [99/120    avg_loss:0.015, val_acc:0.972]
Epoch [100/120    avg_loss:0.017, val_acc:0.972]
Epoch [101/120    avg_loss:0.013, val_acc:0.972]
Epoch [102/120    avg_loss:0.022, val_acc:0.972]
Epoch [103/120    avg_loss:0.018, val_acc:0.972]
Epoch [104/120    avg_loss:0.019, val_acc:0.972]
Epoch [105/120    avg_loss:0.016, val_acc:0.972]
Epoch [106/120    avg_loss:0.022, val_acc:0.972]
Epoch [107/120    avg_loss:0.017, val_acc:0.972]
Epoch [108/120    avg_loss:0.017, val_acc:0.972]
Epoch [109/120    avg_loss:0.017, val_acc:0.972]
Epoch [110/120    avg_loss:0.016, val_acc:0.972]
Epoch [111/120    avg_loss:0.019, val_acc:0.972]
Epoch [112/120    avg_loss:0.014, val_acc:0.972]
Epoch [113/120    avg_loss:0.017, val_acc:0.972]
Epoch [114/120    avg_loss:0.017, val_acc:0.972]
Epoch [115/120    avg_loss:0.017, val_acc:0.972]
Epoch [116/120    avg_loss:0.019, val_acc:0.972]
Epoch [117/120    avg_loss:0.017, val_acc:0.972]
Epoch [118/120    avg_loss:0.018, val_acc:0.972]
Epoch [119/120    avg_loss:0.021, val_acc:0.972]
Epoch [120/120    avg_loss:0.017, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    8 1244    4    0    0    3    0    0    0   10   14    2    0
     0    0    0]
 [   0    0    1  733    1    7    0    0    0    3    0    0    0    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    1    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   10    0    0    0    0    0    0  419    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   19   89    0    5    0    0    0    1  746    7    0    0
     0    8    0]
 [   0    0    8    0    0    3   14    0    0    0   11 2160   14    0
     0    0    0]
 [   0    0    0    5    0    6    0    0    0    0   10    1  503    0
     0    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    78  256    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.90243902439025

F1 scores:
[       nan 0.80808081 0.97301525 0.92902408 0.99765808 0.96839729
 0.97767857 1.         0.98704358 0.87804878 0.90096618 0.98360656
 0.9544592  0.99462366 0.96308867 0.83797054 0.94915254]

Kappa:
0.9532967964244851
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:16:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1067d5a748>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.145, val_acc:0.560]
Epoch [2/120    avg_loss:1.495, val_acc:0.604]
Epoch [3/120    avg_loss:1.182, val_acc:0.617]
Epoch [4/120    avg_loss:0.998, val_acc:0.697]
Epoch [5/120    avg_loss:0.818, val_acc:0.744]
Epoch [6/120    avg_loss:0.681, val_acc:0.771]
Epoch [7/120    avg_loss:0.719, val_acc:0.773]
Epoch [8/120    avg_loss:0.610, val_acc:0.741]
Epoch [9/120    avg_loss:0.615, val_acc:0.752]
Epoch [10/120    avg_loss:0.618, val_acc:0.814]
Epoch [11/120    avg_loss:0.522, val_acc:0.852]
Epoch [12/120    avg_loss:0.482, val_acc:0.823]
Epoch [13/120    avg_loss:0.500, val_acc:0.821]
Epoch [14/120    avg_loss:0.465, val_acc:0.838]
Epoch [15/120    avg_loss:0.385, val_acc:0.867]
Epoch [16/120    avg_loss:0.378, val_acc:0.815]
Epoch [17/120    avg_loss:0.391, val_acc:0.860]
Epoch [18/120    avg_loss:0.367, val_acc:0.878]
Epoch [19/120    avg_loss:0.311, val_acc:0.857]
Epoch [20/120    avg_loss:0.306, val_acc:0.892]
Epoch [21/120    avg_loss:0.238, val_acc:0.904]
Epoch [22/120    avg_loss:0.189, val_acc:0.928]
Epoch [23/120    avg_loss:0.184, val_acc:0.906]
Epoch [24/120    avg_loss:0.270, val_acc:0.890]
Epoch [25/120    avg_loss:0.242, val_acc:0.892]
Epoch [26/120    avg_loss:0.329, val_acc:0.898]
Epoch [27/120    avg_loss:0.230, val_acc:0.909]
Epoch [28/120    avg_loss:0.201, val_acc:0.905]
Epoch [29/120    avg_loss:0.237, val_acc:0.919]
Epoch [30/120    avg_loss:0.144, val_acc:0.920]
Epoch [31/120    avg_loss:0.127, val_acc:0.934]
Epoch [32/120    avg_loss:0.158, val_acc:0.902]
Epoch [33/120    avg_loss:0.182, val_acc:0.926]
Epoch [34/120    avg_loss:0.164, val_acc:0.937]
Epoch [35/120    avg_loss:0.122, val_acc:0.947]
Epoch [36/120    avg_loss:0.132, val_acc:0.944]
Epoch [37/120    avg_loss:0.141, val_acc:0.944]
Epoch [38/120    avg_loss:0.104, val_acc:0.952]
Epoch [39/120    avg_loss:0.069, val_acc:0.954]
Epoch [40/120    avg_loss:0.104, val_acc:0.945]
Epoch [41/120    avg_loss:0.106, val_acc:0.927]
Epoch [42/120    avg_loss:0.101, val_acc:0.924]
Epoch [43/120    avg_loss:0.103, val_acc:0.947]
Epoch [44/120    avg_loss:0.065, val_acc:0.968]
Epoch [45/120    avg_loss:0.053, val_acc:0.872]
Epoch [46/120    avg_loss:0.116, val_acc:0.961]
Epoch [47/120    avg_loss:0.100, val_acc:0.941]
Epoch [48/120    avg_loss:0.074, val_acc:0.959]
Epoch [49/120    avg_loss:0.073, val_acc:0.945]
Epoch [50/120    avg_loss:0.070, val_acc:0.954]
Epoch [51/120    avg_loss:0.124, val_acc:0.914]
Epoch [52/120    avg_loss:0.140, val_acc:0.956]
Epoch [53/120    avg_loss:0.073, val_acc:0.940]
Epoch [54/120    avg_loss:0.074, val_acc:0.958]
Epoch [55/120    avg_loss:0.053, val_acc:0.955]
Epoch [56/120    avg_loss:0.075, val_acc:0.956]
Epoch [57/120    avg_loss:0.087, val_acc:0.940]
Epoch [58/120    avg_loss:0.047, val_acc:0.967]
Epoch [59/120    avg_loss:0.029, val_acc:0.969]
Epoch [60/120    avg_loss:0.032, val_acc:0.971]
Epoch [61/120    avg_loss:0.028, val_acc:0.972]
Epoch [62/120    avg_loss:0.025, val_acc:0.973]
Epoch [63/120    avg_loss:0.030, val_acc:0.976]
Epoch [64/120    avg_loss:0.021, val_acc:0.976]
Epoch [65/120    avg_loss:0.026, val_acc:0.975]
Epoch [66/120    avg_loss:0.023, val_acc:0.973]
Epoch [67/120    avg_loss:0.022, val_acc:0.975]
Epoch [68/120    avg_loss:0.022, val_acc:0.975]
Epoch [69/120    avg_loss:0.021, val_acc:0.976]
Epoch [70/120    avg_loss:0.024, val_acc:0.975]
Epoch [71/120    avg_loss:0.023, val_acc:0.976]
Epoch [72/120    avg_loss:0.027, val_acc:0.978]
Epoch [73/120    avg_loss:0.022, val_acc:0.979]
Epoch [74/120    avg_loss:0.022, val_acc:0.977]
Epoch [75/120    avg_loss:0.021, val_acc:0.978]
Epoch [76/120    avg_loss:0.024, val_acc:0.973]
Epoch [77/120    avg_loss:0.022, val_acc:0.976]
Epoch [78/120    avg_loss:0.026, val_acc:0.973]
Epoch [79/120    avg_loss:0.020, val_acc:0.975]
Epoch [80/120    avg_loss:0.020, val_acc:0.976]
Epoch [81/120    avg_loss:0.022, val_acc:0.977]
Epoch [82/120    avg_loss:0.017, val_acc:0.978]
Epoch [83/120    avg_loss:0.023, val_acc:0.980]
Epoch [84/120    avg_loss:0.021, val_acc:0.972]
Epoch [85/120    avg_loss:0.022, val_acc:0.973]
Epoch [86/120    avg_loss:0.021, val_acc:0.975]
Epoch [87/120    avg_loss:0.025, val_acc:0.973]
Epoch [88/120    avg_loss:0.023, val_acc:0.976]
Epoch [89/120    avg_loss:0.022, val_acc:0.976]
Epoch [90/120    avg_loss:0.022, val_acc:0.973]
Epoch [91/120    avg_loss:0.022, val_acc:0.975]
Epoch [92/120    avg_loss:0.019, val_acc:0.973]
Epoch [93/120    avg_loss:0.018, val_acc:0.979]
Epoch [94/120    avg_loss:0.019, val_acc:0.977]
Epoch [95/120    avg_loss:0.018, val_acc:0.977]
Epoch [96/120    avg_loss:0.020, val_acc:0.976]
Epoch [97/120    avg_loss:0.018, val_acc:0.976]
Epoch [98/120    avg_loss:0.013, val_acc:0.976]
Epoch [99/120    avg_loss:0.021, val_acc:0.977]
Epoch [100/120    avg_loss:0.015, val_acc:0.976]
Epoch [101/120    avg_loss:0.015, val_acc:0.977]
Epoch [102/120    avg_loss:0.017, val_acc:0.977]
Epoch [103/120    avg_loss:0.016, val_acc:0.977]
Epoch [104/120    avg_loss:0.019, val_acc:0.977]
Epoch [105/120    avg_loss:0.016, val_acc:0.977]
Epoch [106/120    avg_loss:0.015, val_acc:0.977]
Epoch [107/120    avg_loss:0.018, val_acc:0.977]
Epoch [108/120    avg_loss:0.017, val_acc:0.977]
Epoch [109/120    avg_loss:0.015, val_acc:0.977]
Epoch [110/120    avg_loss:0.017, val_acc:0.977]
Epoch [111/120    avg_loss:0.017, val_acc:0.977]
Epoch [112/120    avg_loss:0.017, val_acc:0.977]
Epoch [113/120    avg_loss:0.015, val_acc:0.977]
Epoch [114/120    avg_loss:0.016, val_acc:0.977]
Epoch [115/120    avg_loss:0.015, val_acc:0.977]
Epoch [116/120    avg_loss:0.019, val_acc:0.977]
Epoch [117/120    avg_loss:0.017, val_acc:0.977]
Epoch [118/120    avg_loss:0.016, val_acc:0.977]
Epoch [119/120    avg_loss:0.020, val_acc:0.977]
Epoch [120/120    avg_loss:0.018, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1270    1    0    0    0    0    0    0    6    5    0    0
     0    3    0]
 [   0    0    1  724    0   15    0    0    0    4    0    0    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    5    0    0    0    0    0    0  425    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   18   59    0    4   14    0    0    0  763    9    3    0
     0    5    0]
 [   0    0    9    0    0    1   11    0    0    0    2 2184    1    2
     0    0    0]
 [   0    0    0    4    0   10    0    0    0    0    1    1  509    0
     0    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   27    0    0    0    0    0    0    0
    74  246    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
96.52032520325203

F1 scores:
[       nan 0.93023256 0.98297214 0.94209499 1.         0.96205357
 0.96117216 1.         0.99299065 0.82051282 0.92540934 0.99047619
 0.96676163 0.99462366 0.96598639 0.81863561 0.93103448]

Kappa:
0.9603137513754442
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f270e659710>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.098, val_acc:0.588]
Epoch [2/120    avg_loss:1.557, val_acc:0.618]
Epoch [3/120    avg_loss:1.332, val_acc:0.577]
Epoch [4/120    avg_loss:1.130, val_acc:0.707]
Epoch [5/120    avg_loss:0.936, val_acc:0.769]
Epoch [6/120    avg_loss:0.793, val_acc:0.780]
Epoch [7/120    avg_loss:0.732, val_acc:0.805]
Epoch [8/120    avg_loss:0.721, val_acc:0.837]
Epoch [9/120    avg_loss:0.611, val_acc:0.847]
Epoch [10/120    avg_loss:0.654, val_acc:0.838]
Epoch [11/120    avg_loss:0.504, val_acc:0.827]
Epoch [12/120    avg_loss:0.512, val_acc:0.829]
Epoch [13/120    avg_loss:0.426, val_acc:0.845]
Epoch [14/120    avg_loss:0.412, val_acc:0.891]
Epoch [15/120    avg_loss:0.422, val_acc:0.818]
Epoch [16/120    avg_loss:0.459, val_acc:0.870]
Epoch [17/120    avg_loss:0.344, val_acc:0.900]
Epoch [18/120    avg_loss:0.336, val_acc:0.860]
Epoch [19/120    avg_loss:0.336, val_acc:0.842]
Epoch [20/120    avg_loss:0.444, val_acc:0.806]
Epoch [21/120    avg_loss:0.323, val_acc:0.911]
Epoch [22/120    avg_loss:0.280, val_acc:0.903]
Epoch [23/120    avg_loss:0.344, val_acc:0.905]
Epoch [24/120    avg_loss:0.230, val_acc:0.892]
Epoch [25/120    avg_loss:0.253, val_acc:0.893]
Epoch [26/120    avg_loss:0.229, val_acc:0.932]
Epoch [27/120    avg_loss:0.271, val_acc:0.885]
Epoch [28/120    avg_loss:0.232, val_acc:0.909]
Epoch [29/120    avg_loss:0.190, val_acc:0.945]
Epoch [30/120    avg_loss:0.156, val_acc:0.914]
Epoch [31/120    avg_loss:0.155, val_acc:0.931]
Epoch [32/120    avg_loss:0.193, val_acc:0.930]
Epoch [33/120    avg_loss:0.159, val_acc:0.950]
Epoch [34/120    avg_loss:0.142, val_acc:0.953]
Epoch [35/120    avg_loss:0.126, val_acc:0.941]
Epoch [36/120    avg_loss:0.133, val_acc:0.961]
Epoch [37/120    avg_loss:0.081, val_acc:0.965]
Epoch [38/120    avg_loss:0.114, val_acc:0.950]
Epoch [39/120    avg_loss:0.090, val_acc:0.965]
Epoch [40/120    avg_loss:0.087, val_acc:0.952]
Epoch [41/120    avg_loss:0.092, val_acc:0.953]
Epoch [42/120    avg_loss:0.087, val_acc:0.946]
Epoch [43/120    avg_loss:0.066, val_acc:0.970]
Epoch [44/120    avg_loss:0.069, val_acc:0.948]
Epoch [45/120    avg_loss:0.123, val_acc:0.950]
Epoch [46/120    avg_loss:0.087, val_acc:0.941]
Epoch [47/120    avg_loss:0.113, val_acc:0.959]
Epoch [48/120    avg_loss:0.074, val_acc:0.914]
Epoch [49/120    avg_loss:0.095, val_acc:0.959]
Epoch [50/120    avg_loss:0.086, val_acc:0.956]
Epoch [51/120    avg_loss:0.075, val_acc:0.949]
Epoch [52/120    avg_loss:0.160, val_acc:0.884]
Epoch [53/120    avg_loss:0.177, val_acc:0.954]
Epoch [54/120    avg_loss:0.086, val_acc:0.958]
Epoch [55/120    avg_loss:0.084, val_acc:0.967]
Epoch [56/120    avg_loss:0.066, val_acc:0.972]
Epoch [57/120    avg_loss:0.052, val_acc:0.956]
Epoch [58/120    avg_loss:0.103, val_acc:0.967]
Epoch [59/120    avg_loss:0.092, val_acc:0.972]
Epoch [60/120    avg_loss:0.046, val_acc:0.970]
Epoch [61/120    avg_loss:0.061, val_acc:0.962]
Epoch [62/120    avg_loss:0.049, val_acc:0.971]
Epoch [63/120    avg_loss:0.058, val_acc:0.971]
Epoch [64/120    avg_loss:0.053, val_acc:0.971]
Epoch [65/120    avg_loss:0.037, val_acc:0.968]
Epoch [66/120    avg_loss:0.032, val_acc:0.975]
Epoch [67/120    avg_loss:0.023, val_acc:0.975]
Epoch [68/120    avg_loss:0.026, val_acc:0.983]
Epoch [69/120    avg_loss:0.034, val_acc:0.985]
Epoch [70/120    avg_loss:0.053, val_acc:0.977]
Epoch [71/120    avg_loss:0.076, val_acc:0.976]
Epoch [72/120    avg_loss:0.029, val_acc:0.976]
Epoch [73/120    avg_loss:0.025, val_acc:0.979]
Epoch [74/120    avg_loss:0.023, val_acc:0.981]
Epoch [75/120    avg_loss:0.024, val_acc:0.985]
Epoch [76/120    avg_loss:0.029, val_acc:0.974]
Epoch [77/120    avg_loss:0.024, val_acc:0.964]
Epoch [78/120    avg_loss:0.017, val_acc:0.981]
Epoch [79/120    avg_loss:0.063, val_acc:0.970]
Epoch [80/120    avg_loss:0.081, val_acc:0.959]
Epoch [81/120    avg_loss:0.130, val_acc:0.925]
Epoch [82/120    avg_loss:0.102, val_acc:0.934]
Epoch [83/120    avg_loss:0.083, val_acc:0.970]
Epoch [84/120    avg_loss:0.086, val_acc:0.964]
Epoch [85/120    avg_loss:0.074, val_acc:0.981]
Epoch [86/120    avg_loss:0.034, val_acc:0.982]
Epoch [87/120    avg_loss:0.035, val_acc:0.977]
Epoch [88/120    avg_loss:0.026, val_acc:0.986]
Epoch [89/120    avg_loss:0.048, val_acc:0.972]
Epoch [90/120    avg_loss:0.038, val_acc:0.977]
Epoch [91/120    avg_loss:0.013, val_acc:0.984]
Epoch [92/120    avg_loss:0.021, val_acc:0.984]
Epoch [93/120    avg_loss:0.020, val_acc:0.985]
Epoch [94/120    avg_loss:0.028, val_acc:0.954]
Epoch [95/120    avg_loss:0.044, val_acc:0.963]
Epoch [96/120    avg_loss:0.029, val_acc:0.976]
Epoch [97/120    avg_loss:0.028, val_acc:0.982]
Epoch [98/120    avg_loss:0.025, val_acc:0.977]
Epoch [99/120    avg_loss:0.028, val_acc:0.982]
Epoch [100/120    avg_loss:0.019, val_acc:0.985]
Epoch [101/120    avg_loss:0.014, val_acc:0.991]
Epoch [102/120    avg_loss:0.011, val_acc:0.983]
Epoch [103/120    avg_loss:0.010, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.986]
Epoch [105/120    avg_loss:0.010, val_acc:0.992]
Epoch [106/120    avg_loss:0.015, val_acc:0.981]
Epoch [107/120    avg_loss:0.017, val_acc:0.980]
Epoch [108/120    avg_loss:0.012, val_acc:0.989]
Epoch [109/120    avg_loss:0.011, val_acc:0.986]
Epoch [110/120    avg_loss:0.009, val_acc:0.977]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.010, val_acc:0.990]
Epoch [113/120    avg_loss:0.009, val_acc:0.983]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.014, val_acc:0.984]
Epoch [116/120    avg_loss:0.015, val_acc:0.983]
Epoch [117/120    avg_loss:0.008, val_acc:0.985]
Epoch [118/120    avg_loss:0.010, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1276    0    0    0    0    0    0    0    4    2    0    0
     0    3    0]
 [   0    0    0  734    0    9    0    0    0    2    0    0    1    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   13    0    0    2    0
     0    0    0]
 [   0    0    0   50    0    5    2    0    0    0  769   39    3    0
     0    7    0]
 [   0    0    6    0    0    3    7    0    0    0    2 2189    1    2
     0    0    0]
 [   0    0    0    9    2    6    0    0    0    0    6    6  502    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    0    0    0    0
  1132    0    0]
 [   0    0    0    0    0    1    9    0    0    0    0    0    0    0
    90  247    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.76964769647697

F1 scores:
[       nan 0.98765432 0.99376947 0.95139339 0.9953271  0.96436526
 0.98572502 0.98039216 0.99883856 0.76470588 0.92874396 0.98470535
 0.96168582 0.9919571  0.95850974 0.81788079 0.97647059]

Kappa:
0.9631286937948975
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa22a3546d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.130, val_acc:0.514]
Epoch [2/120    avg_loss:1.542, val_acc:0.593]
Epoch [3/120    avg_loss:1.301, val_acc:0.663]
Epoch [4/120    avg_loss:1.086, val_acc:0.697]
Epoch [5/120    avg_loss:0.911, val_acc:0.771]
Epoch [6/120    avg_loss:0.707, val_acc:0.818]
Epoch [7/120    avg_loss:0.688, val_acc:0.771]
Epoch [8/120    avg_loss:0.659, val_acc:0.784]
Epoch [9/120    avg_loss:0.562, val_acc:0.810]
Epoch [10/120    avg_loss:0.548, val_acc:0.842]
Epoch [11/120    avg_loss:0.589, val_acc:0.845]
Epoch [12/120    avg_loss:0.454, val_acc:0.868]
Epoch [13/120    avg_loss:0.401, val_acc:0.891]
Epoch [14/120    avg_loss:0.467, val_acc:0.876]
Epoch [15/120    avg_loss:0.365, val_acc:0.906]
Epoch [16/120    avg_loss:0.339, val_acc:0.886]
Epoch [17/120    avg_loss:0.360, val_acc:0.905]
Epoch [18/120    avg_loss:0.347, val_acc:0.843]
Epoch [19/120    avg_loss:0.343, val_acc:0.896]
Epoch [20/120    avg_loss:0.314, val_acc:0.911]
Epoch [21/120    avg_loss:0.247, val_acc:0.906]
Epoch [22/120    avg_loss:0.296, val_acc:0.888]
Epoch [23/120    avg_loss:0.263, val_acc:0.940]
Epoch [24/120    avg_loss:0.196, val_acc:0.938]
Epoch [25/120    avg_loss:0.210, val_acc:0.922]
Epoch [26/120    avg_loss:0.164, val_acc:0.934]
Epoch [27/120    avg_loss:0.216, val_acc:0.924]
Epoch [28/120    avg_loss:0.160, val_acc:0.914]
Epoch [29/120    avg_loss:0.181, val_acc:0.940]
Epoch [30/120    avg_loss:0.154, val_acc:0.964]
Epoch [31/120    avg_loss:0.187, val_acc:0.930]
Epoch [32/120    avg_loss:0.142, val_acc:0.950]
Epoch [33/120    avg_loss:0.172, val_acc:0.941]
Epoch [34/120    avg_loss:0.155, val_acc:0.936]
Epoch [35/120    avg_loss:0.163, val_acc:0.923]
Epoch [36/120    avg_loss:0.140, val_acc:0.955]
Epoch [37/120    avg_loss:0.094, val_acc:0.958]
Epoch [38/120    avg_loss:0.105, val_acc:0.961]
Epoch [39/120    avg_loss:0.075, val_acc:0.957]
Epoch [40/120    avg_loss:0.079, val_acc:0.954]
Epoch [41/120    avg_loss:0.065, val_acc:0.962]
Epoch [42/120    avg_loss:0.057, val_acc:0.942]
Epoch [43/120    avg_loss:0.075, val_acc:0.955]
Epoch [44/120    avg_loss:0.060, val_acc:0.974]
Epoch [45/120    avg_loss:0.051, val_acc:0.974]
Epoch [46/120    avg_loss:0.035, val_acc:0.974]
Epoch [47/120    avg_loss:0.040, val_acc:0.980]
Epoch [48/120    avg_loss:0.041, val_acc:0.980]
Epoch [49/120    avg_loss:0.038, val_acc:0.981]
Epoch [50/120    avg_loss:0.038, val_acc:0.981]
Epoch [51/120    avg_loss:0.034, val_acc:0.980]
Epoch [52/120    avg_loss:0.040, val_acc:0.980]
Epoch [53/120    avg_loss:0.032, val_acc:0.979]
Epoch [54/120    avg_loss:0.033, val_acc:0.980]
Epoch [55/120    avg_loss:0.027, val_acc:0.979]
Epoch [56/120    avg_loss:0.036, val_acc:0.979]
Epoch [57/120    avg_loss:0.027, val_acc:0.978]
Epoch [58/120    avg_loss:0.028, val_acc:0.980]
Epoch [59/120    avg_loss:0.028, val_acc:0.979]
Epoch [60/120    avg_loss:0.025, val_acc:0.980]
Epoch [61/120    avg_loss:0.027, val_acc:0.980]
Epoch [62/120    avg_loss:0.029, val_acc:0.981]
Epoch [63/120    avg_loss:0.029, val_acc:0.981]
Epoch [64/120    avg_loss:0.027, val_acc:0.980]
Epoch [65/120    avg_loss:0.025, val_acc:0.984]
Epoch [66/120    avg_loss:0.035, val_acc:0.984]
Epoch [67/120    avg_loss:0.032, val_acc:0.981]
Epoch [68/120    avg_loss:0.022, val_acc:0.981]
Epoch [69/120    avg_loss:0.028, val_acc:0.984]
Epoch [70/120    avg_loss:0.020, val_acc:0.981]
Epoch [71/120    avg_loss:0.029, val_acc:0.980]
Epoch [72/120    avg_loss:0.029, val_acc:0.981]
Epoch [73/120    avg_loss:0.028, val_acc:0.980]
Epoch [74/120    avg_loss:0.024, val_acc:0.981]
Epoch [75/120    avg_loss:0.029, val_acc:0.982]
Epoch [76/120    avg_loss:0.022, val_acc:0.981]
Epoch [77/120    avg_loss:0.027, val_acc:0.984]
Epoch [78/120    avg_loss:0.028, val_acc:0.984]
Epoch [79/120    avg_loss:0.023, val_acc:0.984]
Epoch [80/120    avg_loss:0.022, val_acc:0.982]
Epoch [81/120    avg_loss:0.035, val_acc:0.979]
Epoch [82/120    avg_loss:0.023, val_acc:0.982]
Epoch [83/120    avg_loss:0.021, val_acc:0.984]
Epoch [84/120    avg_loss:0.024, val_acc:0.984]
Epoch [85/120    avg_loss:0.030, val_acc:0.981]
Epoch [86/120    avg_loss:0.027, val_acc:0.981]
Epoch [87/120    avg_loss:0.023, val_acc:0.979]
Epoch [88/120    avg_loss:0.022, val_acc:0.981]
Epoch [89/120    avg_loss:0.024, val_acc:0.982]
Epoch [90/120    avg_loss:0.021, val_acc:0.980]
Epoch [91/120    avg_loss:0.020, val_acc:0.982]
Epoch [92/120    avg_loss:0.022, val_acc:0.981]
Epoch [93/120    avg_loss:0.019, val_acc:0.982]
Epoch [94/120    avg_loss:0.024, val_acc:0.987]
Epoch [95/120    avg_loss:0.024, val_acc:0.984]
Epoch [96/120    avg_loss:0.019, val_acc:0.985]
Epoch [97/120    avg_loss:0.021, val_acc:0.984]
Epoch [98/120    avg_loss:0.020, val_acc:0.987]
Epoch [99/120    avg_loss:0.016, val_acc:0.985]
Epoch [100/120    avg_loss:0.020, val_acc:0.985]
Epoch [101/120    avg_loss:0.019, val_acc:0.985]
Epoch [102/120    avg_loss:0.020, val_acc:0.987]
Epoch [103/120    avg_loss:0.018, val_acc:0.987]
Epoch [104/120    avg_loss:0.026, val_acc:0.984]
Epoch [105/120    avg_loss:0.018, val_acc:0.982]
Epoch [106/120    avg_loss:0.018, val_acc:0.985]
Epoch [107/120    avg_loss:0.018, val_acc:0.981]
Epoch [108/120    avg_loss:0.018, val_acc:0.982]
Epoch [109/120    avg_loss:0.019, val_acc:0.980]
Epoch [110/120    avg_loss:0.019, val_acc:0.986]
Epoch [111/120    avg_loss:0.023, val_acc:0.987]
Epoch [112/120    avg_loss:0.021, val_acc:0.980]
Epoch [113/120    avg_loss:0.018, val_acc:0.984]
Epoch [114/120    avg_loss:0.019, val_acc:0.985]
Epoch [115/120    avg_loss:0.020, val_acc:0.985]
Epoch [116/120    avg_loss:0.018, val_acc:0.986]
Epoch [117/120    avg_loss:0.019, val_acc:0.984]
Epoch [118/120    avg_loss:0.018, val_acc:0.982]
Epoch [119/120    avg_loss:0.018, val_acc:0.981]
Epoch [120/120    avg_loss:0.021, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    5 1255    2    0    0    0    0    0    3    7    5    8    0
     0    0    0]
 [   0    0    1  730    0   10    0    0    0    4    0    0    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   29   88    0    6    0    0    0    0  743    0    3    0
     2    4    0]
 [   0    0   28    0    0    4   11    0    0    0   10 2095   60    2
     0    0    0]
 [   0    0    0   21    0    1    0    0    0    0    0    0  501    0
     0    0   11]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    0    0    0
  1133    3    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    70  269    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.47967479674797

F1 scores:
[       nan 0.91764706 0.96575606 0.91939547 1.         0.97297297
 0.98574644 0.98039216 1.         0.79069767 0.9066504  0.97215777
 0.90351668 0.99462366 0.9663113  0.8635634  0.93854749]

Kappa:
0.9485588033658329
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:17:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa6a99b9710>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.138, val_acc:0.542]
Epoch [2/120    avg_loss:1.555, val_acc:0.671]
Epoch [3/120    avg_loss:1.181, val_acc:0.692]
Epoch [4/120    avg_loss:0.955, val_acc:0.681]
Epoch [5/120    avg_loss:0.835, val_acc:0.767]
Epoch [6/120    avg_loss:0.788, val_acc:0.735]
Epoch [7/120    avg_loss:0.724, val_acc:0.776]
Epoch [8/120    avg_loss:0.622, val_acc:0.789]
Epoch [9/120    avg_loss:0.612, val_acc:0.804]
Epoch [10/120    avg_loss:0.597, val_acc:0.792]
Epoch [11/120    avg_loss:0.613, val_acc:0.827]
Epoch [12/120    avg_loss:0.516, val_acc:0.831]
Epoch [13/120    avg_loss:0.420, val_acc:0.837]
Epoch [14/120    avg_loss:0.526, val_acc:0.843]
Epoch [15/120    avg_loss:0.372, val_acc:0.848]
Epoch [16/120    avg_loss:0.345, val_acc:0.863]
Epoch [17/120    avg_loss:0.355, val_acc:0.872]
Epoch [18/120    avg_loss:0.345, val_acc:0.855]
Epoch [19/120    avg_loss:0.305, val_acc:0.876]
Epoch [20/120    avg_loss:0.305, val_acc:0.896]
Epoch [21/120    avg_loss:0.286, val_acc:0.874]
Epoch [22/120    avg_loss:0.292, val_acc:0.884]
Epoch [23/120    avg_loss:0.227, val_acc:0.904]
Epoch [24/120    avg_loss:0.325, val_acc:0.875]
Epoch [25/120    avg_loss:0.202, val_acc:0.900]
Epoch [26/120    avg_loss:0.224, val_acc:0.866]
Epoch [27/120    avg_loss:0.198, val_acc:0.871]
Epoch [28/120    avg_loss:0.217, val_acc:0.906]
Epoch [29/120    avg_loss:0.179, val_acc:0.901]
Epoch [30/120    avg_loss:0.158, val_acc:0.892]
Epoch [31/120    avg_loss:0.155, val_acc:0.929]
Epoch [32/120    avg_loss:0.137, val_acc:0.914]
Epoch [33/120    avg_loss:0.144, val_acc:0.922]
Epoch [34/120    avg_loss:0.190, val_acc:0.936]
Epoch [35/120    avg_loss:0.118, val_acc:0.916]
Epoch [36/120    avg_loss:0.106, val_acc:0.932]
Epoch [37/120    avg_loss:0.138, val_acc:0.905]
Epoch [38/120    avg_loss:0.125, val_acc:0.948]
Epoch [39/120    avg_loss:0.143, val_acc:0.902]
Epoch [40/120    avg_loss:0.118, val_acc:0.927]
Epoch [41/120    avg_loss:0.137, val_acc:0.934]
Epoch [42/120    avg_loss:0.142, val_acc:0.942]
Epoch [43/120    avg_loss:0.072, val_acc:0.939]
Epoch [44/120    avg_loss:0.091, val_acc:0.933]
Epoch [45/120    avg_loss:0.069, val_acc:0.945]
Epoch [46/120    avg_loss:0.053, val_acc:0.956]
Epoch [47/120    avg_loss:0.062, val_acc:0.945]
Epoch [48/120    avg_loss:0.054, val_acc:0.946]
Epoch [49/120    avg_loss:0.126, val_acc:0.930]
Epoch [50/120    avg_loss:0.212, val_acc:0.946]
Epoch [51/120    avg_loss:0.055, val_acc:0.942]
Epoch [52/120    avg_loss:0.080, val_acc:0.938]
Epoch [53/120    avg_loss:0.105, val_acc:0.950]
Epoch [54/120    avg_loss:0.037, val_acc:0.960]
Epoch [55/120    avg_loss:0.053, val_acc:0.962]
Epoch [56/120    avg_loss:0.060, val_acc:0.939]
Epoch [57/120    avg_loss:0.074, val_acc:0.942]
Epoch [58/120    avg_loss:0.122, val_acc:0.935]
Epoch [59/120    avg_loss:0.111, val_acc:0.933]
Epoch [60/120    avg_loss:0.093, val_acc:0.956]
Epoch [61/120    avg_loss:0.077, val_acc:0.960]
Epoch [62/120    avg_loss:0.042, val_acc:0.965]
Epoch [63/120    avg_loss:0.046, val_acc:0.958]
Epoch [64/120    avg_loss:0.030, val_acc:0.965]
Epoch [65/120    avg_loss:0.042, val_acc:0.958]
Epoch [66/120    avg_loss:0.059, val_acc:0.965]
Epoch [67/120    avg_loss:0.053, val_acc:0.962]
Epoch [68/120    avg_loss:0.051, val_acc:0.964]
Epoch [69/120    avg_loss:0.040, val_acc:0.967]
Epoch [70/120    avg_loss:0.030, val_acc:0.968]
Epoch [71/120    avg_loss:0.036, val_acc:0.959]
Epoch [72/120    avg_loss:0.027, val_acc:0.968]
Epoch [73/120    avg_loss:0.024, val_acc:0.967]
Epoch [74/120    avg_loss:0.022, val_acc:0.962]
Epoch [75/120    avg_loss:0.038, val_acc:0.974]
Epoch [76/120    avg_loss:0.033, val_acc:0.969]
Epoch [77/120    avg_loss:0.029, val_acc:0.939]
Epoch [78/120    avg_loss:0.055, val_acc:0.971]
Epoch [79/120    avg_loss:0.102, val_acc:0.960]
Epoch [80/120    avg_loss:0.026, val_acc:0.965]
Epoch [81/120    avg_loss:0.038, val_acc:0.963]
Epoch [82/120    avg_loss:0.023, val_acc:0.969]
Epoch [83/120    avg_loss:0.028, val_acc:0.952]
Epoch [84/120    avg_loss:0.041, val_acc:0.962]
Epoch [85/120    avg_loss:0.054, val_acc:0.952]
Epoch [86/120    avg_loss:0.052, val_acc:0.959]
Epoch [87/120    avg_loss:0.027, val_acc:0.968]
Epoch [88/120    avg_loss:0.015, val_acc:0.974]
Epoch [89/120    avg_loss:0.034, val_acc:0.960]
Epoch [90/120    avg_loss:0.042, val_acc:0.964]
Epoch [91/120    avg_loss:0.032, val_acc:0.963]
Epoch [92/120    avg_loss:0.109, val_acc:0.954]
Epoch [93/120    avg_loss:0.044, val_acc:0.959]
Epoch [94/120    avg_loss:0.044, val_acc:0.951]
Epoch [95/120    avg_loss:0.041, val_acc:0.970]
Epoch [96/120    avg_loss:0.038, val_acc:0.963]
Epoch [97/120    avg_loss:0.114, val_acc:0.961]
Epoch [98/120    avg_loss:0.039, val_acc:0.973]
Epoch [99/120    avg_loss:0.022, val_acc:0.972]
Epoch [100/120    avg_loss:0.029, val_acc:0.969]
Epoch [101/120    avg_loss:0.026, val_acc:0.970]
Epoch [102/120    avg_loss:0.017, val_acc:0.975]
Epoch [103/120    avg_loss:0.019, val_acc:0.974]
Epoch [104/120    avg_loss:0.010, val_acc:0.975]
Epoch [105/120    avg_loss:0.010, val_acc:0.977]
Epoch [106/120    avg_loss:0.010, val_acc:0.975]
Epoch [107/120    avg_loss:0.010, val_acc:0.974]
Epoch [108/120    avg_loss:0.010, val_acc:0.974]
Epoch [109/120    avg_loss:0.011, val_acc:0.975]
Epoch [110/120    avg_loss:0.009, val_acc:0.977]
Epoch [111/120    avg_loss:0.009, val_acc:0.977]
Epoch [112/120    avg_loss:0.009, val_acc:0.977]
Epoch [113/120    avg_loss:0.008, val_acc:0.977]
Epoch [114/120    avg_loss:0.008, val_acc:0.978]
Epoch [115/120    avg_loss:0.007, val_acc:0.978]
Epoch [116/120    avg_loss:0.008, val_acc:0.978]
Epoch [117/120    avg_loss:0.009, val_acc:0.978]
Epoch [118/120    avg_loss:0.007, val_acc:0.978]
Epoch [119/120    avg_loss:0.008, val_acc:0.977]
Epoch [120/120    avg_loss:0.009, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1271    3    0    0    0    0    0    0    5    6    0    0
     0    0    0]
 [   0    0    1  728    0   13    0    0    0    3    0    0    1    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   15    0    0    1    0
     0    0    0]
 [   0    0   14   78    0    4    0    0    0    0  766    4    2    0
     1    6    0]
 [   0    0    7    0    0    0    5    0    0    0   15 2178    2    3
     0    0    0]
 [   0    0    0   11    0    3    0    0    0    0   12    0  503    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    3    1    0    0
  1128    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
   107  237    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.35772357723577

F1 scores:
[       nan 1.         0.98603569 0.9279796  1.         0.96644295
 0.99242424 1.         1.         0.76923077 0.91408115 0.98977505
 0.96267943 0.98930481 0.94989474 0.80338983 0.95906433]

Kappa:
0.9584540889266931
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efd1c22f668>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.168, val_acc:0.527]
Epoch [2/120    avg_loss:1.557, val_acc:0.656]
Epoch [3/120    avg_loss:1.282, val_acc:0.629]
Epoch [4/120    avg_loss:1.058, val_acc:0.689]
Epoch [5/120    avg_loss:0.965, val_acc:0.691]
Epoch [6/120    avg_loss:0.865, val_acc:0.752]
Epoch [7/120    avg_loss:0.704, val_acc:0.788]
Epoch [8/120    avg_loss:0.698, val_acc:0.768]
Epoch [9/120    avg_loss:0.581, val_acc:0.801]
Epoch [10/120    avg_loss:0.512, val_acc:0.789]
Epoch [11/120    avg_loss:0.497, val_acc:0.835]
Epoch [12/120    avg_loss:0.478, val_acc:0.830]
Epoch [13/120    avg_loss:0.566, val_acc:0.762]
Epoch [14/120    avg_loss:0.423, val_acc:0.868]
Epoch [15/120    avg_loss:0.371, val_acc:0.845]
Epoch [16/120    avg_loss:0.364, val_acc:0.856]
Epoch [17/120    avg_loss:0.353, val_acc:0.838]
Epoch [18/120    avg_loss:0.359, val_acc:0.854]
Epoch [19/120    avg_loss:0.382, val_acc:0.864]
Epoch [20/120    avg_loss:0.371, val_acc:0.845]
Epoch [21/120    avg_loss:0.293, val_acc:0.890]
Epoch [22/120    avg_loss:0.312, val_acc:0.895]
Epoch [23/120    avg_loss:0.302, val_acc:0.891]
Epoch [24/120    avg_loss:0.260, val_acc:0.894]
Epoch [25/120    avg_loss:0.239, val_acc:0.888]
Epoch [26/120    avg_loss:0.186, val_acc:0.917]
Epoch [27/120    avg_loss:0.219, val_acc:0.910]
Epoch [28/120    avg_loss:0.188, val_acc:0.911]
Epoch [29/120    avg_loss:0.173, val_acc:0.910]
Epoch [30/120    avg_loss:0.164, val_acc:0.903]
Epoch [31/120    avg_loss:0.150, val_acc:0.931]
Epoch [32/120    avg_loss:0.135, val_acc:0.941]
Epoch [33/120    avg_loss:0.188, val_acc:0.886]
Epoch [34/120    avg_loss:0.208, val_acc:0.922]
Epoch [35/120    avg_loss:0.185, val_acc:0.868]
Epoch [36/120    avg_loss:0.170, val_acc:0.925]
Epoch [37/120    avg_loss:0.130, val_acc:0.934]
Epoch [38/120    avg_loss:0.075, val_acc:0.940]
Epoch [39/120    avg_loss:0.103, val_acc:0.934]
Epoch [40/120    avg_loss:0.097, val_acc:0.940]
Epoch [41/120    avg_loss:0.065, val_acc:0.941]
Epoch [42/120    avg_loss:0.083, val_acc:0.932]
Epoch [43/120    avg_loss:0.061, val_acc:0.956]
Epoch [44/120    avg_loss:0.101, val_acc:0.914]
Epoch [45/120    avg_loss:0.108, val_acc:0.951]
Epoch [46/120    avg_loss:0.117, val_acc:0.888]
Epoch [47/120    avg_loss:0.178, val_acc:0.950]
Epoch [48/120    avg_loss:0.078, val_acc:0.946]
Epoch [49/120    avg_loss:0.111, val_acc:0.934]
Epoch [50/120    avg_loss:0.095, val_acc:0.945]
Epoch [51/120    avg_loss:0.072, val_acc:0.952]
Epoch [52/120    avg_loss:0.061, val_acc:0.952]
Epoch [53/120    avg_loss:0.074, val_acc:0.924]
Epoch [54/120    avg_loss:0.095, val_acc:0.940]
Epoch [55/120    avg_loss:0.057, val_acc:0.948]
Epoch [56/120    avg_loss:0.086, val_acc:0.907]
Epoch [57/120    avg_loss:0.053, val_acc:0.970]
Epoch [58/120    avg_loss:0.031, val_acc:0.971]
Epoch [59/120    avg_loss:0.030, val_acc:0.974]
Epoch [60/120    avg_loss:0.033, val_acc:0.974]
Epoch [61/120    avg_loss:0.030, val_acc:0.975]
Epoch [62/120    avg_loss:0.034, val_acc:0.973]
Epoch [63/120    avg_loss:0.030, val_acc:0.973]
Epoch [64/120    avg_loss:0.028, val_acc:0.974]
Epoch [65/120    avg_loss:0.032, val_acc:0.975]
Epoch [66/120    avg_loss:0.026, val_acc:0.974]
Epoch [67/120    avg_loss:0.026, val_acc:0.974]
Epoch [68/120    avg_loss:0.021, val_acc:0.972]
Epoch [69/120    avg_loss:0.033, val_acc:0.977]
Epoch [70/120    avg_loss:0.021, val_acc:0.975]
Epoch [71/120    avg_loss:0.024, val_acc:0.974]
Epoch [72/120    avg_loss:0.023, val_acc:0.977]
Epoch [73/120    avg_loss:0.029, val_acc:0.978]
Epoch [74/120    avg_loss:0.023, val_acc:0.978]
Epoch [75/120    avg_loss:0.023, val_acc:0.978]
Epoch [76/120    avg_loss:0.022, val_acc:0.978]
Epoch [77/120    avg_loss:0.025, val_acc:0.979]
Epoch [78/120    avg_loss:0.025, val_acc:0.979]
Epoch [79/120    avg_loss:0.024, val_acc:0.977]
Epoch [80/120    avg_loss:0.023, val_acc:0.980]
Epoch [81/120    avg_loss:0.029, val_acc:0.978]
Epoch [82/120    avg_loss:0.022, val_acc:0.978]
Epoch [83/120    avg_loss:0.019, val_acc:0.978]
Epoch [84/120    avg_loss:0.023, val_acc:0.975]
Epoch [85/120    avg_loss:0.023, val_acc:0.980]
Epoch [86/120    avg_loss:0.021, val_acc:0.980]
Epoch [87/120    avg_loss:0.022, val_acc:0.979]
Epoch [88/120    avg_loss:0.022, val_acc:0.979]
Epoch [89/120    avg_loss:0.018, val_acc:0.978]
Epoch [90/120    avg_loss:0.016, val_acc:0.979]
Epoch [91/120    avg_loss:0.020, val_acc:0.977]
Epoch [92/120    avg_loss:0.017, val_acc:0.978]
Epoch [93/120    avg_loss:0.019, val_acc:0.980]
Epoch [94/120    avg_loss:0.021, val_acc:0.981]
Epoch [95/120    avg_loss:0.024, val_acc:0.979]
Epoch [96/120    avg_loss:0.015, val_acc:0.980]
Epoch [97/120    avg_loss:0.021, val_acc:0.978]
Epoch [98/120    avg_loss:0.018, val_acc:0.979]
Epoch [99/120    avg_loss:0.023, val_acc:0.980]
Epoch [100/120    avg_loss:0.017, val_acc:0.981]
Epoch [101/120    avg_loss:0.021, val_acc:0.978]
Epoch [102/120    avg_loss:0.020, val_acc:0.979]
Epoch [103/120    avg_loss:0.020, val_acc:0.981]
Epoch [104/120    avg_loss:0.018, val_acc:0.980]
Epoch [105/120    avg_loss:0.016, val_acc:0.980]
Epoch [106/120    avg_loss:0.017, val_acc:0.981]
Epoch [107/120    avg_loss:0.026, val_acc:0.981]
Epoch [108/120    avg_loss:0.018, val_acc:0.981]
Epoch [109/120    avg_loss:0.018, val_acc:0.981]
Epoch [110/120    avg_loss:0.015, val_acc:0.979]
Epoch [111/120    avg_loss:0.021, val_acc:0.982]
Epoch [112/120    avg_loss:0.020, val_acc:0.980]
Epoch [113/120    avg_loss:0.018, val_acc:0.980]
Epoch [114/120    avg_loss:0.019, val_acc:0.979]
Epoch [115/120    avg_loss:0.017, val_acc:0.981]
Epoch [116/120    avg_loss:0.019, val_acc:0.981]
Epoch [117/120    avg_loss:0.017, val_acc:0.983]
Epoch [118/120    avg_loss:0.021, val_acc:0.979]
Epoch [119/120    avg_loss:0.019, val_acc:0.980]
Epoch [120/120    avg_loss:0.020, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1245    0    0    0    0    0    0    0    4   21    5    0
     1    9    0]
 [   0    0    1  701    1   28    0    0    0    6    0    0    7    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   11   76    0    8    3    0    0    0  748   17    9    0
     0    3    0]
 [   0    0    1    0    0    0    3    0    0    0    2 2198    4    2
     0    0    0]
 [   0    0    0    4    0    4    0    0    0    0    0    0  517    0
     3    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    1    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    27  317    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
96.8780487804878

F1 scores:
[       nan 0.975      0.978389   0.91753927 0.99765808 0.94945055
 0.99167298 1.         0.99883856 0.8372093  0.91779141 0.98830935
 0.95740741 0.98666667 0.98352125 0.93786982 0.94117647]

Kappa:
0.964404066896988
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1b191c76d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.207, val_acc:0.383]
Epoch [2/120    avg_loss:1.641, val_acc:0.568]
Epoch [3/120    avg_loss:1.356, val_acc:0.599]
Epoch [4/120    avg_loss:1.182, val_acc:0.711]
Epoch [5/120    avg_loss:0.876, val_acc:0.777]
Epoch [6/120    avg_loss:0.726, val_acc:0.808]
Epoch [7/120    avg_loss:0.711, val_acc:0.831]
Epoch [8/120    avg_loss:0.582, val_acc:0.794]
Epoch [9/120    avg_loss:0.531, val_acc:0.782]
Epoch [10/120    avg_loss:0.535, val_acc:0.845]
Epoch [11/120    avg_loss:0.399, val_acc:0.838]
Epoch [12/120    avg_loss:0.371, val_acc:0.844]
Epoch [13/120    avg_loss:0.307, val_acc:0.857]
Epoch [14/120    avg_loss:0.398, val_acc:0.830]
Epoch [15/120    avg_loss:0.328, val_acc:0.881]
Epoch [16/120    avg_loss:0.226, val_acc:0.923]
Epoch [17/120    avg_loss:0.234, val_acc:0.869]
Epoch [18/120    avg_loss:0.213, val_acc:0.929]
Epoch [19/120    avg_loss:0.130, val_acc:0.934]
Epoch [20/120    avg_loss:0.207, val_acc:0.898]
Epoch [21/120    avg_loss:0.142, val_acc:0.938]
Epoch [22/120    avg_loss:0.104, val_acc:0.931]
Epoch [23/120    avg_loss:0.083, val_acc:0.939]
Epoch [24/120    avg_loss:0.097, val_acc:0.949]
Epoch [25/120    avg_loss:0.107, val_acc:0.938]
Epoch [26/120    avg_loss:0.093, val_acc:0.939]
Epoch [27/120    avg_loss:0.101, val_acc:0.915]
Epoch [28/120    avg_loss:0.084, val_acc:0.949]
Epoch [29/120    avg_loss:0.064, val_acc:0.959]
Epoch [30/120    avg_loss:0.043, val_acc:0.957]
Epoch [31/120    avg_loss:0.042, val_acc:0.963]
Epoch [32/120    avg_loss:0.079, val_acc:0.961]
Epoch [33/120    avg_loss:0.043, val_acc:0.960]
Epoch [34/120    avg_loss:0.042, val_acc:0.944]
Epoch [35/120    avg_loss:0.081, val_acc:0.967]
Epoch [36/120    avg_loss:0.033, val_acc:0.961]
Epoch [37/120    avg_loss:0.021, val_acc:0.974]
Epoch [38/120    avg_loss:0.025, val_acc:0.982]
Epoch [39/120    avg_loss:0.027, val_acc:0.965]
Epoch [40/120    avg_loss:0.039, val_acc:0.963]
Epoch [41/120    avg_loss:0.030, val_acc:0.973]
Epoch [42/120    avg_loss:0.023, val_acc:0.984]
Epoch [43/120    avg_loss:0.019, val_acc:0.970]
Epoch [44/120    avg_loss:0.021, val_acc:0.978]
Epoch [45/120    avg_loss:0.016, val_acc:0.982]
Epoch [46/120    avg_loss:0.014, val_acc:0.976]
Epoch [47/120    avg_loss:0.014, val_acc:0.982]
Epoch [48/120    avg_loss:0.018, val_acc:0.982]
Epoch [49/120    avg_loss:0.011, val_acc:0.973]
Epoch [50/120    avg_loss:0.010, val_acc:0.975]
Epoch [51/120    avg_loss:0.026, val_acc:0.970]
Epoch [52/120    avg_loss:0.017, val_acc:0.981]
Epoch [53/120    avg_loss:0.012, val_acc:0.985]
Epoch [54/120    avg_loss:0.011, val_acc:0.978]
Epoch [55/120    avg_loss:0.012, val_acc:0.984]
Epoch [56/120    avg_loss:0.009, val_acc:0.986]
Epoch [57/120    avg_loss:0.010, val_acc:0.984]
Epoch [58/120    avg_loss:0.008, val_acc:0.964]
Epoch [59/120    avg_loss:0.029, val_acc:0.978]
Epoch [60/120    avg_loss:0.021, val_acc:0.968]
Epoch [61/120    avg_loss:0.008, val_acc:0.986]
Epoch [62/120    avg_loss:0.008, val_acc:0.981]
Epoch [63/120    avg_loss:0.008, val_acc:0.974]
Epoch [64/120    avg_loss:0.009, val_acc:0.983]
Epoch [65/120    avg_loss:0.005, val_acc:0.984]
Epoch [66/120    avg_loss:0.009, val_acc:0.960]
Epoch [67/120    avg_loss:0.017, val_acc:0.960]
Epoch [68/120    avg_loss:0.012, val_acc:0.981]
Epoch [69/120    avg_loss:0.011, val_acc:0.975]
Epoch [70/120    avg_loss:0.009, val_acc:0.985]
Epoch [71/120    avg_loss:0.005, val_acc:0.986]
Epoch [72/120    avg_loss:0.006, val_acc:0.981]
Epoch [73/120    avg_loss:0.006, val_acc:0.983]
Epoch [74/120    avg_loss:0.005, val_acc:0.987]
Epoch [75/120    avg_loss:0.010, val_acc:0.975]
Epoch [76/120    avg_loss:0.008, val_acc:0.985]
Epoch [77/120    avg_loss:0.004, val_acc:0.987]
Epoch [78/120    avg_loss:0.004, val_acc:0.987]
Epoch [79/120    avg_loss:0.006, val_acc:0.986]
Epoch [80/120    avg_loss:0.004, val_acc:0.986]
Epoch [81/120    avg_loss:0.005, val_acc:0.987]
Epoch [82/120    avg_loss:0.006, val_acc:0.983]
Epoch [83/120    avg_loss:0.007, val_acc:0.986]
Epoch [84/120    avg_loss:0.003, val_acc:0.982]
Epoch [85/120    avg_loss:0.005, val_acc:0.986]
Epoch [86/120    avg_loss:0.005, val_acc:0.986]
Epoch [87/120    avg_loss:0.004, val_acc:0.988]
Epoch [88/120    avg_loss:0.007, val_acc:0.983]
Epoch [89/120    avg_loss:0.014, val_acc:0.982]
Epoch [90/120    avg_loss:0.010, val_acc:0.983]
Epoch [91/120    avg_loss:0.003, val_acc:0.988]
Epoch [92/120    avg_loss:0.004, val_acc:0.988]
Epoch [93/120    avg_loss:0.008, val_acc:0.980]
Epoch [94/120    avg_loss:0.011, val_acc:0.987]
Epoch [95/120    avg_loss:0.004, val_acc:0.984]
Epoch [96/120    avg_loss:0.003, val_acc:0.989]
Epoch [97/120    avg_loss:0.002, val_acc:0.988]
Epoch [98/120    avg_loss:0.002, val_acc:0.986]
Epoch [99/120    avg_loss:0.003, val_acc:0.989]
Epoch [100/120    avg_loss:0.003, val_acc:0.987]
Epoch [101/120    avg_loss:0.002, val_acc:0.990]
Epoch [102/120    avg_loss:0.002, val_acc:0.990]
Epoch [103/120    avg_loss:0.002, val_acc:0.990]
Epoch [104/120    avg_loss:0.002, val_acc:0.991]
Epoch [105/120    avg_loss:0.002, val_acc:0.991]
Epoch [106/120    avg_loss:0.002, val_acc:0.989]
Epoch [107/120    avg_loss:0.002, val_acc:0.989]
Epoch [108/120    avg_loss:0.002, val_acc:0.989]
Epoch [109/120    avg_loss:0.002, val_acc:0.990]
Epoch [110/120    avg_loss:0.002, val_acc:0.990]
Epoch [111/120    avg_loss:0.002, val_acc:0.988]
Epoch [112/120    avg_loss:0.003, val_acc:0.987]
Epoch [113/120    avg_loss:0.003, val_acc:0.988]
Epoch [114/120    avg_loss:0.002, val_acc:0.988]
Epoch [115/120    avg_loss:0.002, val_acc:0.989]
Epoch [116/120    avg_loss:0.002, val_acc:0.990]
Epoch [117/120    avg_loss:0.002, val_acc:0.989]
Epoch [118/120    avg_loss:0.001, val_acc:0.990]
Epoch [119/120    avg_loss:0.001, val_acc:0.990]
Epoch [120/120    avg_loss:0.002, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1266    7    0    0    0    0    0    0    2   10    0    0
     0    0    0]
 [   0    0    0  741    1    0    0    0    0    0    0    0    5    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    0    0    0    0    0  841   26    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    1 2199    8    1
     0    0    0]
 [   0    0    0    5    0    0    0    0    0    0    0    0  527    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1131    7    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    95  247    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.88617886178862

F1 scores:
[       nan 1.         0.989449   0.98668442 0.99294118 0.99538106
 0.99316629 1.         1.         0.9        0.97847586 0.98920378
 0.98046512 0.99730458 0.95523649 0.82059801 0.98809524]

Kappa:
0.9758692344104053
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb07816c710>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.328, val_acc:0.480]
Epoch [2/120    avg_loss:1.659, val_acc:0.584]
Epoch [3/120    avg_loss:1.405, val_acc:0.656]
Epoch [4/120    avg_loss:1.168, val_acc:0.677]
Epoch [5/120    avg_loss:1.048, val_acc:0.636]
Epoch [6/120    avg_loss:0.945, val_acc:0.690]
Epoch [7/120    avg_loss:0.776, val_acc:0.771]
Epoch [8/120    avg_loss:0.574, val_acc:0.775]
Epoch [9/120    avg_loss:0.478, val_acc:0.846]
Epoch [10/120    avg_loss:0.521, val_acc:0.804]
Epoch [11/120    avg_loss:0.422, val_acc:0.828]
Epoch [12/120    avg_loss:0.323, val_acc:0.811]
Epoch [13/120    avg_loss:0.283, val_acc:0.890]
Epoch [14/120    avg_loss:0.233, val_acc:0.884]
Epoch [15/120    avg_loss:0.236, val_acc:0.898]
Epoch [16/120    avg_loss:0.295, val_acc:0.887]
Epoch [17/120    avg_loss:0.320, val_acc:0.894]
Epoch [18/120    avg_loss:0.208, val_acc:0.899]
Epoch [19/120    avg_loss:0.147, val_acc:0.908]
Epoch [20/120    avg_loss:0.273, val_acc:0.855]
Epoch [21/120    avg_loss:0.152, val_acc:0.914]
Epoch [22/120    avg_loss:0.150, val_acc:0.875]
Epoch [23/120    avg_loss:0.127, val_acc:0.947]
Epoch [24/120    avg_loss:0.104, val_acc:0.936]
Epoch [25/120    avg_loss:0.082, val_acc:0.946]
Epoch [26/120    avg_loss:0.083, val_acc:0.934]
Epoch [27/120    avg_loss:0.086, val_acc:0.949]
Epoch [28/120    avg_loss:0.066, val_acc:0.939]
Epoch [29/120    avg_loss:0.061, val_acc:0.950]
Epoch [30/120    avg_loss:0.058, val_acc:0.947]
Epoch [31/120    avg_loss:0.041, val_acc:0.915]
Epoch [32/120    avg_loss:0.061, val_acc:0.942]
Epoch [33/120    avg_loss:0.060, val_acc:0.925]
Epoch [34/120    avg_loss:0.042, val_acc:0.957]
Epoch [35/120    avg_loss:0.038, val_acc:0.955]
Epoch [36/120    avg_loss:0.026, val_acc:0.960]
Epoch [37/120    avg_loss:0.023, val_acc:0.957]
Epoch [38/120    avg_loss:0.026, val_acc:0.964]
Epoch [39/120    avg_loss:0.054, val_acc:0.936]
Epoch [40/120    avg_loss:0.032, val_acc:0.960]
Epoch [41/120    avg_loss:0.028, val_acc:0.967]
Epoch [42/120    avg_loss:0.027, val_acc:0.954]
Epoch [43/120    avg_loss:0.036, val_acc:0.949]
Epoch [44/120    avg_loss:0.034, val_acc:0.922]
Epoch [45/120    avg_loss:0.063, val_acc:0.951]
Epoch [46/120    avg_loss:0.354, val_acc:0.890]
Epoch [47/120    avg_loss:0.279, val_acc:0.899]
Epoch [48/120    avg_loss:0.215, val_acc:0.916]
Epoch [49/120    avg_loss:0.132, val_acc:0.909]
Epoch [50/120    avg_loss:0.147, val_acc:0.926]
Epoch [51/120    avg_loss:0.103, val_acc:0.933]
Epoch [52/120    avg_loss:0.051, val_acc:0.963]
Epoch [53/120    avg_loss:0.049, val_acc:0.950]
Epoch [54/120    avg_loss:0.057, val_acc:0.946]
Epoch [55/120    avg_loss:0.042, val_acc:0.956]
Epoch [56/120    avg_loss:0.037, val_acc:0.959]
Epoch [57/120    avg_loss:0.024, val_acc:0.961]
Epoch [58/120    avg_loss:0.022, val_acc:0.961]
Epoch [59/120    avg_loss:0.024, val_acc:0.963]
Epoch [60/120    avg_loss:0.022, val_acc:0.961]
Epoch [61/120    avg_loss:0.021, val_acc:0.963]
Epoch [62/120    avg_loss:0.020, val_acc:0.960]
Epoch [63/120    avg_loss:0.018, val_acc:0.961]
Epoch [64/120    avg_loss:0.019, val_acc:0.965]
Epoch [65/120    avg_loss:0.015, val_acc:0.964]
Epoch [66/120    avg_loss:0.015, val_acc:0.966]
Epoch [67/120    avg_loss:0.014, val_acc:0.964]
Epoch [68/120    avg_loss:0.016, val_acc:0.963]
Epoch [69/120    avg_loss:0.015, val_acc:0.963]
Epoch [70/120    avg_loss:0.016, val_acc:0.963]
Epoch [71/120    avg_loss:0.017, val_acc:0.963]
Epoch [72/120    avg_loss:0.015, val_acc:0.963]
Epoch [73/120    avg_loss:0.021, val_acc:0.964]
Epoch [74/120    avg_loss:0.014, val_acc:0.963]
Epoch [75/120    avg_loss:0.015, val_acc:0.965]
Epoch [76/120    avg_loss:0.015, val_acc:0.963]
Epoch [77/120    avg_loss:0.016, val_acc:0.964]
Epoch [78/120    avg_loss:0.016, val_acc:0.961]
Epoch [79/120    avg_loss:0.022, val_acc:0.964]
Epoch [80/120    avg_loss:0.014, val_acc:0.963]
Epoch [81/120    avg_loss:0.022, val_acc:0.963]
Epoch [82/120    avg_loss:0.015, val_acc:0.963]
Epoch [83/120    avg_loss:0.013, val_acc:0.963]
Epoch [84/120    avg_loss:0.020, val_acc:0.963]
Epoch [85/120    avg_loss:0.016, val_acc:0.963]
Epoch [86/120    avg_loss:0.017, val_acc:0.963]
Epoch [87/120    avg_loss:0.016, val_acc:0.963]
Epoch [88/120    avg_loss:0.019, val_acc:0.963]
Epoch [89/120    avg_loss:0.015, val_acc:0.963]
Epoch [90/120    avg_loss:0.013, val_acc:0.963]
Epoch [91/120    avg_loss:0.015, val_acc:0.963]
Epoch [92/120    avg_loss:0.014, val_acc:0.963]
Epoch [93/120    avg_loss:0.015, val_acc:0.963]
Epoch [94/120    avg_loss:0.016, val_acc:0.963]
Epoch [95/120    avg_loss:0.013, val_acc:0.963]
Epoch [96/120    avg_loss:0.015, val_acc:0.963]
Epoch [97/120    avg_loss:0.016, val_acc:0.963]
Epoch [98/120    avg_loss:0.016, val_acc:0.963]
Epoch [99/120    avg_loss:0.017, val_acc:0.963]
Epoch [100/120    avg_loss:0.017, val_acc:0.963]
Epoch [101/120    avg_loss:0.015, val_acc:0.963]
Epoch [102/120    avg_loss:0.016, val_acc:0.963]
Epoch [103/120    avg_loss:0.013, val_acc:0.963]
Epoch [104/120    avg_loss:0.017, val_acc:0.963]
Epoch [105/120    avg_loss:0.014, val_acc:0.963]
Epoch [106/120    avg_loss:0.012, val_acc:0.963]
Epoch [107/120    avg_loss:0.013, val_acc:0.963]
Epoch [108/120    avg_loss:0.015, val_acc:0.963]
Epoch [109/120    avg_loss:0.020, val_acc:0.963]
Epoch [110/120    avg_loss:0.013, val_acc:0.963]
Epoch [111/120    avg_loss:0.017, val_acc:0.963]
Epoch [112/120    avg_loss:0.014, val_acc:0.963]
Epoch [113/120    avg_loss:0.017, val_acc:0.963]
Epoch [114/120    avg_loss:0.016, val_acc:0.963]
Epoch [115/120    avg_loss:0.014, val_acc:0.963]
Epoch [116/120    avg_loss:0.011, val_acc:0.963]
Epoch [117/120    avg_loss:0.016, val_acc:0.963]
Epoch [118/120    avg_loss:0.017, val_acc:0.963]
Epoch [119/120    avg_loss:0.013, val_acc:0.963]
Epoch [120/120    avg_loss:0.015, val_acc:0.963]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    1    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1252    8    0    7    0    0    0    0    6   11    1    0
     0    0    0]
 [   0    0    0  712    3    0    2    0    0    1    1    9   19    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    1    9    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    0    0    0    0    0  839   25    1    0
     0    0    0]
 [   0    2    9    0    0    0    0    0    0    0   22 2147   19   10
     0    1    0]
 [   0    0    0    4    0    0    0    0    0    0    0    2  526    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1120   19    0]
 [   0    0    0    0    0    1    3    0    0    0    0    0    0    0
    84  259    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.72628726287263

F1 scores:
[       nan 0.96385542 0.97965571 0.96804895 0.99300699 0.97459584
 0.99240122 0.84745763 1.         0.97297297 0.96270797 0.97435897
 0.95462795 0.97368421 0.95440988 0.82747604 0.97619048]

Kappa:
0.9626739660871487
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f141eeec710>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.240, val_acc:0.468]
Epoch [2/120    avg_loss:1.660, val_acc:0.626]
Epoch [3/120    avg_loss:1.325, val_acc:0.689]
Epoch [4/120    avg_loss:1.151, val_acc:0.751]
Epoch [5/120    avg_loss:0.959, val_acc:0.704]
Epoch [6/120    avg_loss:0.879, val_acc:0.730]
Epoch [7/120    avg_loss:0.734, val_acc:0.804]
Epoch [8/120    avg_loss:0.517, val_acc:0.768]
Epoch [9/120    avg_loss:0.487, val_acc:0.859]
Epoch [10/120    avg_loss:0.410, val_acc:0.895]
Epoch [11/120    avg_loss:0.305, val_acc:0.893]
Epoch [12/120    avg_loss:0.253, val_acc:0.886]
Epoch [13/120    avg_loss:0.256, val_acc:0.910]
Epoch [14/120    avg_loss:0.189, val_acc:0.915]
Epoch [15/120    avg_loss:0.190, val_acc:0.930]
Epoch [16/120    avg_loss:0.169, val_acc:0.871]
Epoch [17/120    avg_loss:0.487, val_acc:0.871]
Epoch [18/120    avg_loss:0.300, val_acc:0.879]
Epoch [19/120    avg_loss:0.191, val_acc:0.931]
Epoch [20/120    avg_loss:0.147, val_acc:0.951]
Epoch [21/120    avg_loss:0.130, val_acc:0.940]
Epoch [22/120    avg_loss:0.107, val_acc:0.940]
Epoch [23/120    avg_loss:0.094, val_acc:0.959]
Epoch [24/120    avg_loss:0.079, val_acc:0.954]
Epoch [25/120    avg_loss:0.073, val_acc:0.947]
Epoch [26/120    avg_loss:0.068, val_acc:0.949]
Epoch [27/120    avg_loss:0.044, val_acc:0.965]
Epoch [28/120    avg_loss:0.039, val_acc:0.958]
Epoch [29/120    avg_loss:0.053, val_acc:0.961]
Epoch [30/120    avg_loss:0.045, val_acc:0.964]
Epoch [31/120    avg_loss:0.029, val_acc:0.968]
Epoch [32/120    avg_loss:0.028, val_acc:0.954]
Epoch [33/120    avg_loss:0.032, val_acc:0.957]
Epoch [34/120    avg_loss:0.056, val_acc:0.964]
Epoch [35/120    avg_loss:0.098, val_acc:0.954]
Epoch [36/120    avg_loss:0.092, val_acc:0.941]
Epoch [37/120    avg_loss:0.052, val_acc:0.970]
Epoch [38/120    avg_loss:0.046, val_acc:0.966]
Epoch [39/120    avg_loss:0.039, val_acc:0.968]
Epoch [40/120    avg_loss:0.029, val_acc:0.952]
Epoch [41/120    avg_loss:0.027, val_acc:0.972]
Epoch [42/120    avg_loss:0.014, val_acc:0.977]
Epoch [43/120    avg_loss:0.021, val_acc:0.973]
Epoch [44/120    avg_loss:0.023, val_acc:0.966]
Epoch [45/120    avg_loss:0.024, val_acc:0.977]
Epoch [46/120    avg_loss:0.019, val_acc:0.970]
Epoch [47/120    avg_loss:0.017, val_acc:0.978]
Epoch [48/120    avg_loss:0.015, val_acc:0.979]
Epoch [49/120    avg_loss:0.072, val_acc:0.946]
Epoch [50/120    avg_loss:0.038, val_acc:0.973]
Epoch [51/120    avg_loss:0.022, val_acc:0.975]
Epoch [52/120    avg_loss:0.013, val_acc:0.975]
Epoch [53/120    avg_loss:0.012, val_acc:0.979]
Epoch [54/120    avg_loss:0.008, val_acc:0.973]
Epoch [55/120    avg_loss:0.008, val_acc:0.976]
Epoch [56/120    avg_loss:0.013, val_acc:0.972]
Epoch [57/120    avg_loss:0.012, val_acc:0.976]
Epoch [58/120    avg_loss:0.012, val_acc:0.976]
Epoch [59/120    avg_loss:0.022, val_acc:0.968]
Epoch [60/120    avg_loss:0.015, val_acc:0.979]
Epoch [61/120    avg_loss:0.008, val_acc:0.982]
Epoch [62/120    avg_loss:0.037, val_acc:0.964]
Epoch [63/120    avg_loss:0.017, val_acc:0.970]
Epoch [64/120    avg_loss:0.025, val_acc:0.944]
Epoch [65/120    avg_loss:0.050, val_acc:0.972]
Epoch [66/120    avg_loss:0.010, val_acc:0.975]
Epoch [67/120    avg_loss:0.013, val_acc:0.980]
Epoch [68/120    avg_loss:0.007, val_acc:0.980]
Epoch [69/120    avg_loss:0.005, val_acc:0.978]
Epoch [70/120    avg_loss:0.006, val_acc:0.980]
Epoch [71/120    avg_loss:0.010, val_acc:0.978]
Epoch [72/120    avg_loss:0.007, val_acc:0.981]
Epoch [73/120    avg_loss:0.005, val_acc:0.980]
Epoch [74/120    avg_loss:0.006, val_acc:0.982]
Epoch [75/120    avg_loss:0.005, val_acc:0.981]
Epoch [76/120    avg_loss:0.005, val_acc:0.979]
Epoch [77/120    avg_loss:0.007, val_acc:0.973]
Epoch [78/120    avg_loss:0.010, val_acc:0.971]
Epoch [79/120    avg_loss:0.009, val_acc:0.978]
Epoch [80/120    avg_loss:0.007, val_acc:0.976]
Epoch [81/120    avg_loss:0.004, val_acc:0.984]
Epoch [82/120    avg_loss:0.003, val_acc:0.980]
Epoch [83/120    avg_loss:0.003, val_acc:0.982]
Epoch [84/120    avg_loss:0.003, val_acc:0.984]
Epoch [85/120    avg_loss:0.003, val_acc:0.982]
Epoch [86/120    avg_loss:0.004, val_acc:0.982]
Epoch [87/120    avg_loss:0.003, val_acc:0.983]
Epoch [88/120    avg_loss:0.003, val_acc:0.982]
Epoch [89/120    avg_loss:0.005, val_acc:0.982]
Epoch [90/120    avg_loss:0.003, val_acc:0.981]
Epoch [91/120    avg_loss:0.004, val_acc:0.983]
Epoch [92/120    avg_loss:0.005, val_acc:0.982]
Epoch [93/120    avg_loss:0.003, val_acc:0.982]
Epoch [94/120    avg_loss:0.005, val_acc:0.980]
Epoch [95/120    avg_loss:0.003, val_acc:0.984]
Epoch [96/120    avg_loss:0.002, val_acc:0.983]
Epoch [97/120    avg_loss:0.003, val_acc:0.983]
Epoch [98/120    avg_loss:0.003, val_acc:0.984]
Epoch [99/120    avg_loss:0.002, val_acc:0.985]
Epoch [100/120    avg_loss:0.004, val_acc:0.983]
Epoch [101/120    avg_loss:0.002, val_acc:0.983]
Epoch [102/120    avg_loss:0.002, val_acc:0.983]
Epoch [103/120    avg_loss:0.002, val_acc:0.983]
Epoch [104/120    avg_loss:0.002, val_acc:0.982]
Epoch [105/120    avg_loss:0.002, val_acc:0.982]
Epoch [106/120    avg_loss:0.002, val_acc:0.982]
Epoch [107/120    avg_loss:0.002, val_acc:0.981]
Epoch [108/120    avg_loss:0.004, val_acc:0.979]
Epoch [109/120    avg_loss:0.002, val_acc:0.982]
Epoch [110/120    avg_loss:0.004, val_acc:0.984]
Epoch [111/120    avg_loss:0.004, val_acc:0.982]
Epoch [112/120    avg_loss:0.002, val_acc:0.984]
Epoch [113/120    avg_loss:0.003, val_acc:0.983]
Epoch [114/120    avg_loss:0.002, val_acc:0.984]
Epoch [115/120    avg_loss:0.003, val_acc:0.983]
Epoch [116/120    avg_loss:0.002, val_acc:0.983]
Epoch [117/120    avg_loss:0.003, val_acc:0.983]
Epoch [118/120    avg_loss:0.002, val_acc:0.983]
Epoch [119/120    avg_loss:0.001, val_acc:0.983]
Epoch [120/120    avg_loss:0.002, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1265    7    4    0    0    0    0    1    1    7    0    0
     0    0    0]
 [   0    0    0  737    1    0    0    0    0    0    0    2    6    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  865    3    0    0
     2    1    0]
 [   0    0   21    0    0    0    0    0    0    1    7 2156   21    1
     0    3    0]
 [   0    0    0    2    0    0    0    0    0    0    0    0  529    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1132    7    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    61  272    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.94037940379404

F1 scores:
[       nan 0.98765432 0.98214286 0.98727395 0.98839907 0.99769585
 0.98716981 1.         1.         0.92307692 0.98970252 0.98425017
 0.96797804 0.99730458 0.96959315 0.86075949 0.97005988]

Kappa:
0.9765237018260068
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5777392710>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.241, val_acc:0.386]
Epoch [2/120    avg_loss:1.662, val_acc:0.567]
Epoch [3/120    avg_loss:1.307, val_acc:0.682]
Epoch [4/120    avg_loss:1.101, val_acc:0.721]
Epoch [5/120    avg_loss:0.875, val_acc:0.759]
Epoch [6/120    avg_loss:0.752, val_acc:0.719]
Epoch [7/120    avg_loss:0.809, val_acc:0.808]
Epoch [8/120    avg_loss:0.508, val_acc:0.790]
Epoch [9/120    avg_loss:0.570, val_acc:0.779]
Epoch [10/120    avg_loss:0.447, val_acc:0.837]
Epoch [11/120    avg_loss:0.322, val_acc:0.857]
Epoch [12/120    avg_loss:0.429, val_acc:0.807]
Epoch [13/120    avg_loss:0.348, val_acc:0.852]
Epoch [14/120    avg_loss:0.303, val_acc:0.908]
Epoch [15/120    avg_loss:0.239, val_acc:0.870]
Epoch [16/120    avg_loss:0.245, val_acc:0.873]
Epoch [17/120    avg_loss:0.157, val_acc:0.915]
Epoch [18/120    avg_loss:0.332, val_acc:0.844]
Epoch [19/120    avg_loss:0.228, val_acc:0.866]
Epoch [20/120    avg_loss:0.139, val_acc:0.932]
Epoch [21/120    avg_loss:0.092, val_acc:0.947]
Epoch [22/120    avg_loss:0.077, val_acc:0.952]
Epoch [23/120    avg_loss:0.085, val_acc:0.944]
Epoch [24/120    avg_loss:0.056, val_acc:0.959]
Epoch [25/120    avg_loss:0.078, val_acc:0.956]
Epoch [26/120    avg_loss:0.087, val_acc:0.958]
Epoch [27/120    avg_loss:0.071, val_acc:0.953]
Epoch [28/120    avg_loss:0.075, val_acc:0.942]
Epoch [29/120    avg_loss:0.101, val_acc:0.953]
Epoch [30/120    avg_loss:0.061, val_acc:0.962]
Epoch [31/120    avg_loss:0.035, val_acc:0.954]
Epoch [32/120    avg_loss:0.053, val_acc:0.956]
Epoch [33/120    avg_loss:0.041, val_acc:0.938]
Epoch [34/120    avg_loss:0.040, val_acc:0.953]
Epoch [35/120    avg_loss:0.034, val_acc:0.923]
Epoch [36/120    avg_loss:0.045, val_acc:0.969]
Epoch [37/120    avg_loss:0.030, val_acc:0.965]
Epoch [38/120    avg_loss:0.026, val_acc:0.977]
Epoch [39/120    avg_loss:0.028, val_acc:0.969]
Epoch [40/120    avg_loss:0.023, val_acc:0.968]
Epoch [41/120    avg_loss:0.026, val_acc:0.942]
Epoch [42/120    avg_loss:0.036, val_acc:0.964]
Epoch [43/120    avg_loss:0.025, val_acc:0.971]
Epoch [44/120    avg_loss:0.021, val_acc:0.971]
Epoch [45/120    avg_loss:0.033, val_acc:0.943]
Epoch [46/120    avg_loss:0.050, val_acc:0.974]
Epoch [47/120    avg_loss:0.021, val_acc:0.968]
Epoch [48/120    avg_loss:0.014, val_acc:0.983]
Epoch [49/120    avg_loss:0.016, val_acc:0.970]
Epoch [50/120    avg_loss:0.017, val_acc:0.970]
Epoch [51/120    avg_loss:0.018, val_acc:0.957]
Epoch [52/120    avg_loss:0.024, val_acc:0.978]
Epoch [53/120    avg_loss:0.012, val_acc:0.986]
Epoch [54/120    avg_loss:0.008, val_acc:0.984]
Epoch [55/120    avg_loss:0.007, val_acc:0.986]
Epoch [56/120    avg_loss:0.005, val_acc:0.982]
Epoch [57/120    avg_loss:0.006, val_acc:0.983]
Epoch [58/120    avg_loss:0.007, val_acc:0.987]
Epoch [59/120    avg_loss:0.014, val_acc:0.963]
Epoch [60/120    avg_loss:0.061, val_acc:0.938]
Epoch [61/120    avg_loss:0.037, val_acc:0.971]
Epoch [62/120    avg_loss:0.018, val_acc:0.980]
Epoch [63/120    avg_loss:0.024, val_acc:0.980]
Epoch [64/120    avg_loss:0.018, val_acc:0.970]
Epoch [65/120    avg_loss:0.022, val_acc:0.974]
Epoch [66/120    avg_loss:0.025, val_acc:0.973]
Epoch [67/120    avg_loss:0.020, val_acc:0.976]
Epoch [68/120    avg_loss:0.015, val_acc:0.968]
Epoch [69/120    avg_loss:0.010, val_acc:0.984]
Epoch [70/120    avg_loss:0.005, val_acc:0.985]
Epoch [71/120    avg_loss:0.004, val_acc:0.986]
Epoch [72/120    avg_loss:0.004, val_acc:0.988]
Epoch [73/120    avg_loss:0.004, val_acc:0.987]
Epoch [74/120    avg_loss:0.004, val_acc:0.988]
Epoch [75/120    avg_loss:0.004, val_acc:0.988]
Epoch [76/120    avg_loss:0.005, val_acc:0.987]
Epoch [77/120    avg_loss:0.004, val_acc:0.986]
Epoch [78/120    avg_loss:0.004, val_acc:0.986]
Epoch [79/120    avg_loss:0.004, val_acc:0.987]
Epoch [80/120    avg_loss:0.003, val_acc:0.986]
Epoch [81/120    avg_loss:0.003, val_acc:0.986]
Epoch [82/120    avg_loss:0.005, val_acc:0.986]
Epoch [83/120    avg_loss:0.005, val_acc:0.987]
Epoch [84/120    avg_loss:0.003, val_acc:0.987]
Epoch [85/120    avg_loss:0.003, val_acc:0.987]
Epoch [86/120    avg_loss:0.004, val_acc:0.987]
Epoch [87/120    avg_loss:0.004, val_acc:0.987]
Epoch [88/120    avg_loss:0.004, val_acc:0.987]
Epoch [89/120    avg_loss:0.003, val_acc:0.987]
Epoch [90/120    avg_loss:0.004, val_acc:0.987]
Epoch [91/120    avg_loss:0.004, val_acc:0.987]
Epoch [92/120    avg_loss:0.003, val_acc:0.987]
Epoch [93/120    avg_loss:0.003, val_acc:0.987]
Epoch [94/120    avg_loss:0.003, val_acc:0.987]
Epoch [95/120    avg_loss:0.003, val_acc:0.987]
Epoch [96/120    avg_loss:0.003, val_acc:0.987]
Epoch [97/120    avg_loss:0.003, val_acc:0.987]
Epoch [98/120    avg_loss:0.003, val_acc:0.987]
Epoch [99/120    avg_loss:0.002, val_acc:0.987]
Epoch [100/120    avg_loss:0.003, val_acc:0.987]
Epoch [101/120    avg_loss:0.004, val_acc:0.987]
Epoch [102/120    avg_loss:0.003, val_acc:0.987]
Epoch [103/120    avg_loss:0.003, val_acc:0.987]
Epoch [104/120    avg_loss:0.004, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.987]
Epoch [106/120    avg_loss:0.003, val_acc:0.987]
Epoch [107/120    avg_loss:0.004, val_acc:0.987]
Epoch [108/120    avg_loss:0.004, val_acc:0.987]
Epoch [109/120    avg_loss:0.004, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.004, val_acc:0.987]
Epoch [112/120    avg_loss:0.004, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.003, val_acc:0.987]
Epoch [115/120    avg_loss:0.005, val_acc:0.987]
Epoch [116/120    avg_loss:0.003, val_acc:0.987]
Epoch [117/120    avg_loss:0.003, val_acc:0.987]
Epoch [118/120    avg_loss:0.003, val_acc:0.987]
Epoch [119/120    avg_loss:0.003, val_acc:0.987]
Epoch [120/120    avg_loss:0.004, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1262    5    3    2    0    0    0    0    3    8    2    0
     0    0    0]
 [   0    0    0  741    1    0    0    0    0    1    1    0    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    1    0    0    0    0    0    0  863   10    0    0
     0    0    0]
 [   0    0    9    0    0    0    0    0    0    0   10 2182    7    0
     1    1    0]
 [   0    0    2    0    1    0    0    0    0    0    0    0  529    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    91  254    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.01626016260163

F1 scores:
[       nan 0.98765432 0.98632278 0.99196787 0.98839907 0.99308756
 0.99771863 1.         1.         0.97297297 0.98459783 0.98956916
 0.9823584  1.         0.95564005 0.8314239  0.98203593]

Kappa:
0.9773700701810334
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f217979c7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.231, val_acc:0.369]
Epoch [2/120    avg_loss:1.668, val_acc:0.534]
Epoch [3/120    avg_loss:1.426, val_acc:0.578]
Epoch [4/120    avg_loss:1.166, val_acc:0.625]
Epoch [5/120    avg_loss:0.973, val_acc:0.734]
Epoch [6/120    avg_loss:0.889, val_acc:0.774]
Epoch [7/120    avg_loss:0.709, val_acc:0.784]
Epoch [8/120    avg_loss:0.795, val_acc:0.722]
Epoch [9/120    avg_loss:0.695, val_acc:0.776]
Epoch [10/120    avg_loss:0.551, val_acc:0.829]
Epoch [11/120    avg_loss:0.420, val_acc:0.834]
Epoch [12/120    avg_loss:0.353, val_acc:0.892]
Epoch [13/120    avg_loss:0.400, val_acc:0.830]
Epoch [14/120    avg_loss:0.289, val_acc:0.904]
Epoch [15/120    avg_loss:0.239, val_acc:0.897]
Epoch [16/120    avg_loss:0.174, val_acc:0.900]
Epoch [17/120    avg_loss:0.232, val_acc:0.902]
Epoch [18/120    avg_loss:0.192, val_acc:0.879]
Epoch [19/120    avg_loss:0.185, val_acc:0.886]
Epoch [20/120    avg_loss:0.188, val_acc:0.840]
Epoch [21/120    avg_loss:0.152, val_acc:0.929]
Epoch [22/120    avg_loss:0.123, val_acc:0.933]
Epoch [23/120    avg_loss:0.095, val_acc:0.924]
Epoch [24/120    avg_loss:0.102, val_acc:0.929]
Epoch [25/120    avg_loss:0.065, val_acc:0.946]
Epoch [26/120    avg_loss:0.069, val_acc:0.954]
Epoch [27/120    avg_loss:0.063, val_acc:0.947]
Epoch [28/120    avg_loss:0.068, val_acc:0.957]
Epoch [29/120    avg_loss:0.056, val_acc:0.933]
Epoch [30/120    avg_loss:0.057, val_acc:0.959]
Epoch [31/120    avg_loss:0.037, val_acc:0.952]
Epoch [32/120    avg_loss:0.034, val_acc:0.951]
Epoch [33/120    avg_loss:0.031, val_acc:0.965]
Epoch [34/120    avg_loss:0.040, val_acc:0.964]
Epoch [35/120    avg_loss:0.120, val_acc:0.935]
Epoch [36/120    avg_loss:0.107, val_acc:0.934]
Epoch [37/120    avg_loss:0.075, val_acc:0.930]
Epoch [38/120    avg_loss:0.089, val_acc:0.941]
Epoch [39/120    avg_loss:0.036, val_acc:0.950]
Epoch [40/120    avg_loss:0.037, val_acc:0.956]
Epoch [41/120    avg_loss:0.033, val_acc:0.958]
Epoch [42/120    avg_loss:0.024, val_acc:0.967]
Epoch [43/120    avg_loss:0.028, val_acc:0.956]
Epoch [44/120    avg_loss:0.027, val_acc:0.950]
Epoch [45/120    avg_loss:0.043, val_acc:0.961]
Epoch [46/120    avg_loss:0.066, val_acc:0.957]
Epoch [47/120    avg_loss:0.036, val_acc:0.960]
Epoch [48/120    avg_loss:0.062, val_acc:0.941]
Epoch [49/120    avg_loss:0.032, val_acc:0.957]
Epoch [50/120    avg_loss:0.015, val_acc:0.964]
Epoch [51/120    avg_loss:0.163, val_acc:0.941]
Epoch [52/120    avg_loss:0.107, val_acc:0.934]
Epoch [53/120    avg_loss:0.062, val_acc:0.953]
Epoch [54/120    avg_loss:0.024, val_acc:0.958]
Epoch [55/120    avg_loss:0.031, val_acc:0.961]
Epoch [56/120    avg_loss:0.028, val_acc:0.969]
Epoch [57/120    avg_loss:0.013, val_acc:0.970]
Epoch [58/120    avg_loss:0.015, val_acc:0.972]
Epoch [59/120    avg_loss:0.014, val_acc:0.970]
Epoch [60/120    avg_loss:0.013, val_acc:0.970]
Epoch [61/120    avg_loss:0.018, val_acc:0.969]
Epoch [62/120    avg_loss:0.011, val_acc:0.967]
Epoch [63/120    avg_loss:0.012, val_acc:0.971]
Epoch [64/120    avg_loss:0.012, val_acc:0.973]
Epoch [65/120    avg_loss:0.012, val_acc:0.973]
Epoch [66/120    avg_loss:0.012, val_acc:0.974]
Epoch [67/120    avg_loss:0.010, val_acc:0.972]
Epoch [68/120    avg_loss:0.012, val_acc:0.973]
Epoch [69/120    avg_loss:0.011, val_acc:0.975]
Epoch [70/120    avg_loss:0.009, val_acc:0.974]
Epoch [71/120    avg_loss:0.011, val_acc:0.974]
Epoch [72/120    avg_loss:0.010, val_acc:0.974]
Epoch [73/120    avg_loss:0.011, val_acc:0.972]
Epoch [74/120    avg_loss:0.011, val_acc:0.973]
Epoch [75/120    avg_loss:0.012, val_acc:0.974]
Epoch [76/120    avg_loss:0.012, val_acc:0.974]
Epoch [77/120    avg_loss:0.012, val_acc:0.974]
Epoch [78/120    avg_loss:0.012, val_acc:0.973]
Epoch [79/120    avg_loss:0.010, val_acc:0.971]
Epoch [80/120    avg_loss:0.007, val_acc:0.974]
Epoch [81/120    avg_loss:0.009, val_acc:0.976]
Epoch [82/120    avg_loss:0.008, val_acc:0.975]
Epoch [83/120    avg_loss:0.009, val_acc:0.974]
Epoch [84/120    avg_loss:0.011, val_acc:0.973]
Epoch [85/120    avg_loss:0.007, val_acc:0.973]
Epoch [86/120    avg_loss:0.008, val_acc:0.974]
Epoch [87/120    avg_loss:0.008, val_acc:0.974]
Epoch [88/120    avg_loss:0.011, val_acc:0.971]
Epoch [89/120    avg_loss:0.009, val_acc:0.971]
Epoch [90/120    avg_loss:0.009, val_acc:0.971]
Epoch [91/120    avg_loss:0.008, val_acc:0.971]
Epoch [92/120    avg_loss:0.010, val_acc:0.972]
Epoch [93/120    avg_loss:0.010, val_acc:0.972]
Epoch [94/120    avg_loss:0.009, val_acc:0.972]
Epoch [95/120    avg_loss:0.013, val_acc:0.971]
Epoch [96/120    avg_loss:0.008, val_acc:0.971]
Epoch [97/120    avg_loss:0.009, val_acc:0.970]
Epoch [98/120    avg_loss:0.006, val_acc:0.970]
Epoch [99/120    avg_loss:0.017, val_acc:0.971]
Epoch [100/120    avg_loss:0.007, val_acc:0.971]
Epoch [101/120    avg_loss:0.006, val_acc:0.971]
Epoch [102/120    avg_loss:0.009, val_acc:0.971]
Epoch [103/120    avg_loss:0.007, val_acc:0.971]
Epoch [104/120    avg_loss:0.006, val_acc:0.971]
Epoch [105/120    avg_loss:0.011, val_acc:0.971]
Epoch [106/120    avg_loss:0.006, val_acc:0.971]
Epoch [107/120    avg_loss:0.006, val_acc:0.971]
Epoch [108/120    avg_loss:0.008, val_acc:0.971]
Epoch [109/120    avg_loss:0.008, val_acc:0.971]
Epoch [110/120    avg_loss:0.009, val_acc:0.971]
Epoch [111/120    avg_loss:0.006, val_acc:0.971]
Epoch [112/120    avg_loss:0.008, val_acc:0.971]
Epoch [113/120    avg_loss:0.009, val_acc:0.971]
Epoch [114/120    avg_loss:0.007, val_acc:0.971]
Epoch [115/120    avg_loss:0.007, val_acc:0.971]
Epoch [116/120    avg_loss:0.005, val_acc:0.971]
Epoch [117/120    avg_loss:0.006, val_acc:0.971]
Epoch [118/120    avg_loss:0.008, val_acc:0.971]
Epoch [119/120    avg_loss:0.008, val_acc:0.971]
Epoch [120/120    avg_loss:0.009, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    1    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1257    2    0    3    0    0    0    0    9   14    0    0
     0    0    0]
 [   0    0    0  728    3    0    0    0    0    2    1    0   13    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    4    0    0    0    1    0    0
     6    0    0]
 [   0    0    0    0    0    0  652    0    0    2    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  846   22    1    0
     0    1    0]
 [   0    0    9    0    0    0    0    0    0    0   11 2159   30    1
     0    0    0]
 [   0    0    0    1    1    0    0    0    0    0    0    0  529    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1124   13    0]
 [   0    0    0    0    0    1    2    0    0    0    0    0    0    0
    94  250    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.11653116531166

F1 scores:
[       nan 0.93670886 0.98356808 0.98511502 0.99069767 0.98148148
 0.99466056 0.92592593 0.99767442 0.9        0.96907216 0.97958258
 0.95487365 0.99730458 0.95093063 0.81699346 0.98224852]

Kappa:
0.9671137270802405
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9ef0c4b710>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.220, val_acc:0.441]
Epoch [2/120    avg_loss:1.662, val_acc:0.565]
Epoch [3/120    avg_loss:1.380, val_acc:0.589]
Epoch [4/120    avg_loss:1.132, val_acc:0.720]
Epoch [5/120    avg_loss:1.001, val_acc:0.725]
Epoch [6/120    avg_loss:0.821, val_acc:0.743]
Epoch [7/120    avg_loss:0.829, val_acc:0.807]
Epoch [8/120    avg_loss:0.646, val_acc:0.790]
Epoch [9/120    avg_loss:0.544, val_acc:0.801]
Epoch [10/120    avg_loss:0.531, val_acc:0.801]
Epoch [11/120    avg_loss:0.518, val_acc:0.835]
Epoch [12/120    avg_loss:0.404, val_acc:0.855]
Epoch [13/120    avg_loss:0.347, val_acc:0.873]
Epoch [14/120    avg_loss:0.302, val_acc:0.843]
Epoch [15/120    avg_loss:0.352, val_acc:0.880]
Epoch [16/120    avg_loss:0.239, val_acc:0.893]
Epoch [17/120    avg_loss:0.209, val_acc:0.917]
Epoch [18/120    avg_loss:0.190, val_acc:0.888]
Epoch [19/120    avg_loss:0.200, val_acc:0.858]
Epoch [20/120    avg_loss:0.153, val_acc:0.940]
Epoch [21/120    avg_loss:0.122, val_acc:0.925]
Epoch [22/120    avg_loss:0.124, val_acc:0.919]
Epoch [23/120    avg_loss:0.085, val_acc:0.889]
Epoch [24/120    avg_loss:0.124, val_acc:0.949]
Epoch [25/120    avg_loss:0.081, val_acc:0.956]
Epoch [26/120    avg_loss:0.065, val_acc:0.958]
Epoch [27/120    avg_loss:0.051, val_acc:0.958]
Epoch [28/120    avg_loss:0.053, val_acc:0.963]
Epoch [29/120    avg_loss:0.046, val_acc:0.957]
Epoch [30/120    avg_loss:0.043, val_acc:0.960]
Epoch [31/120    avg_loss:0.043, val_acc:0.961]
Epoch [32/120    avg_loss:0.033, val_acc:0.968]
Epoch [33/120    avg_loss:0.038, val_acc:0.965]
Epoch [34/120    avg_loss:0.038, val_acc:0.959]
Epoch [35/120    avg_loss:0.055, val_acc:0.952]
Epoch [36/120    avg_loss:0.080, val_acc:0.948]
Epoch [37/120    avg_loss:0.065, val_acc:0.953]
Epoch [38/120    avg_loss:0.053, val_acc:0.953]
Epoch [39/120    avg_loss:0.036, val_acc:0.950]
Epoch [40/120    avg_loss:0.035, val_acc:0.954]
Epoch [41/120    avg_loss:0.038, val_acc:0.946]
Epoch [42/120    avg_loss:0.042, val_acc:0.960]
Epoch [43/120    avg_loss:0.060, val_acc:0.911]
Epoch [44/120    avg_loss:0.065, val_acc:0.956]
Epoch [45/120    avg_loss:0.032, val_acc:0.968]
Epoch [46/120    avg_loss:0.017, val_acc:0.970]
Epoch [47/120    avg_loss:0.018, val_acc:0.967]
Epoch [48/120    avg_loss:0.017, val_acc:0.968]
Epoch [49/120    avg_loss:0.024, val_acc:0.965]
Epoch [50/120    avg_loss:0.019, val_acc:0.965]
Epoch [51/120    avg_loss:0.015, val_acc:0.974]
Epoch [52/120    avg_loss:0.012, val_acc:0.980]
Epoch [53/120    avg_loss:0.014, val_acc:0.968]
Epoch [54/120    avg_loss:0.014, val_acc:0.974]
Epoch [55/120    avg_loss:0.014, val_acc:0.958]
Epoch [56/120    avg_loss:0.023, val_acc:0.967]
Epoch [57/120    avg_loss:0.009, val_acc:0.975]
Epoch [58/120    avg_loss:0.014, val_acc:0.975]
Epoch [59/120    avg_loss:0.007, val_acc:0.975]
Epoch [60/120    avg_loss:0.008, val_acc:0.975]
Epoch [61/120    avg_loss:0.008, val_acc:0.977]
Epoch [62/120    avg_loss:0.006, val_acc:0.973]
Epoch [63/120    avg_loss:0.006, val_acc:0.964]
Epoch [64/120    avg_loss:0.014, val_acc:0.970]
Epoch [65/120    avg_loss:0.018, val_acc:0.968]
Epoch [66/120    avg_loss:0.007, val_acc:0.968]
Epoch [67/120    avg_loss:0.009, val_acc:0.978]
Epoch [68/120    avg_loss:0.007, val_acc:0.977]
Epoch [69/120    avg_loss:0.005, val_acc:0.977]
Epoch [70/120    avg_loss:0.004, val_acc:0.977]
Epoch [71/120    avg_loss:0.006, val_acc:0.976]
Epoch [72/120    avg_loss:0.004, val_acc:0.978]
Epoch [73/120    avg_loss:0.003, val_acc:0.978]
Epoch [74/120    avg_loss:0.006, val_acc:0.978]
Epoch [75/120    avg_loss:0.005, val_acc:0.978]
Epoch [76/120    avg_loss:0.004, val_acc:0.978]
Epoch [77/120    avg_loss:0.004, val_acc:0.977]
Epoch [78/120    avg_loss:0.004, val_acc:0.980]
Epoch [79/120    avg_loss:0.004, val_acc:0.978]
Epoch [80/120    avg_loss:0.005, val_acc:0.978]
Epoch [81/120    avg_loss:0.004, val_acc:0.980]
Epoch [82/120    avg_loss:0.004, val_acc:0.980]
Epoch [83/120    avg_loss:0.005, val_acc:0.980]
Epoch [84/120    avg_loss:0.004, val_acc:0.980]
Epoch [85/120    avg_loss:0.004, val_acc:0.978]
Epoch [86/120    avg_loss:0.005, val_acc:0.980]
Epoch [87/120    avg_loss:0.005, val_acc:0.980]
Epoch [88/120    avg_loss:0.005, val_acc:0.977]
Epoch [89/120    avg_loss:0.003, val_acc:0.978]
Epoch [90/120    avg_loss:0.004, val_acc:0.978]
Epoch [91/120    avg_loss:0.007, val_acc:0.983]
Epoch [92/120    avg_loss:0.005, val_acc:0.982]
Epoch [93/120    avg_loss:0.006, val_acc:0.982]
Epoch [94/120    avg_loss:0.004, val_acc:0.981]
Epoch [95/120    avg_loss:0.004, val_acc:0.980]
Epoch [96/120    avg_loss:0.004, val_acc:0.978]
Epoch [97/120    avg_loss:0.005, val_acc:0.978]
Epoch [98/120    avg_loss:0.004, val_acc:0.980]
Epoch [99/120    avg_loss:0.004, val_acc:0.981]
Epoch [100/120    avg_loss:0.007, val_acc:0.981]
Epoch [101/120    avg_loss:0.004, val_acc:0.981]
Epoch [102/120    avg_loss:0.004, val_acc:0.980]
Epoch [103/120    avg_loss:0.004, val_acc:0.978]
Epoch [104/120    avg_loss:0.004, val_acc:0.978]
Epoch [105/120    avg_loss:0.005, val_acc:0.980]
Epoch [106/120    avg_loss:0.005, val_acc:0.980]
Epoch [107/120    avg_loss:0.003, val_acc:0.980]
Epoch [108/120    avg_loss:0.005, val_acc:0.980]
Epoch [109/120    avg_loss:0.004, val_acc:0.980]
Epoch [110/120    avg_loss:0.004, val_acc:0.980]
Epoch [111/120    avg_loss:0.003, val_acc:0.980]
Epoch [112/120    avg_loss:0.003, val_acc:0.980]
Epoch [113/120    avg_loss:0.003, val_acc:0.981]
Epoch [114/120    avg_loss:0.005, val_acc:0.981]
Epoch [115/120    avg_loss:0.004, val_acc:0.981]
Epoch [116/120    avg_loss:0.004, val_acc:0.981]
Epoch [117/120    avg_loss:0.003, val_acc:0.981]
Epoch [118/120    avg_loss:0.006, val_acc:0.981]
Epoch [119/120    avg_loss:0.004, val_acc:0.981]
Epoch [120/120    avg_loss:0.005, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1271    3    0    0    1    0    0    0    2    7    1    0
     0    0    0]
 [   0    0    1  729    1    0    1    0    0    0    1    9    5    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    2    0    0    0    1    0    0
     6    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  848   22    0    0
     0    1    0]
 [   0    0    5    0    0    0    0    0    0    0   11 2192    1    0
     0    1    0]
 [   0    0    0    2    0    0    0    0    0    0    0    4  525    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
    75  265    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.96205962059621

F1 scores:
[       nan 0.98765432 0.99026101 0.98380567 0.99530516 0.98954704
 0.9924357  0.96153846 1.         1.         0.97639609 0.98627672
 0.98499062 1.         0.96173469 0.85209003 0.98245614]

Kappa:
0.9767373028368044
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f353c07f710>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.249, val_acc:0.477]
Epoch [2/120    avg_loss:1.598, val_acc:0.656]
Epoch [3/120    avg_loss:1.243, val_acc:0.639]
Epoch [4/120    avg_loss:0.998, val_acc:0.736]
Epoch [5/120    avg_loss:0.985, val_acc:0.743]
Epoch [6/120    avg_loss:0.767, val_acc:0.806]
Epoch [7/120    avg_loss:0.652, val_acc:0.790]
Epoch [8/120    avg_loss:0.519, val_acc:0.835]
Epoch [9/120    avg_loss:0.556, val_acc:0.798]
Epoch [10/120    avg_loss:0.424, val_acc:0.851]
Epoch [11/120    avg_loss:0.352, val_acc:0.895]
Epoch [12/120    avg_loss:0.316, val_acc:0.884]
Epoch [13/120    avg_loss:0.257, val_acc:0.891]
Epoch [14/120    avg_loss:0.176, val_acc:0.910]
Epoch [15/120    avg_loss:0.197, val_acc:0.894]
Epoch [16/120    avg_loss:0.299, val_acc:0.889]
Epoch [17/120    avg_loss:0.176, val_acc:0.905]
Epoch [18/120    avg_loss:0.165, val_acc:0.850]
Epoch [19/120    avg_loss:0.263, val_acc:0.948]
Epoch [20/120    avg_loss:0.132, val_acc:0.914]
Epoch [21/120    avg_loss:0.130, val_acc:0.932]
Epoch [22/120    avg_loss:0.092, val_acc:0.898]
Epoch [23/120    avg_loss:0.108, val_acc:0.940]
Epoch [24/120    avg_loss:0.077, val_acc:0.947]
Epoch [25/120    avg_loss:0.126, val_acc:0.946]
Epoch [26/120    avg_loss:0.085, val_acc:0.960]
Epoch [27/120    avg_loss:0.093, val_acc:0.931]
Epoch [28/120    avg_loss:0.067, val_acc:0.961]
Epoch [29/120    avg_loss:0.044, val_acc:0.949]
Epoch [30/120    avg_loss:0.079, val_acc:0.923]
Epoch [31/120    avg_loss:0.053, val_acc:0.949]
Epoch [32/120    avg_loss:0.044, val_acc:0.947]
Epoch [33/120    avg_loss:0.034, val_acc:0.960]
Epoch [34/120    avg_loss:0.031, val_acc:0.969]
Epoch [35/120    avg_loss:0.031, val_acc:0.965]
Epoch [36/120    avg_loss:0.025, val_acc:0.974]
Epoch [37/120    avg_loss:0.021, val_acc:0.956]
Epoch [38/120    avg_loss:0.037, val_acc:0.970]
Epoch [39/120    avg_loss:0.035, val_acc:0.964]
Epoch [40/120    avg_loss:0.038, val_acc:0.963]
Epoch [41/120    avg_loss:0.036, val_acc:0.972]
Epoch [42/120    avg_loss:0.029, val_acc:0.964]
Epoch [43/120    avg_loss:0.027, val_acc:0.963]
Epoch [44/120    avg_loss:0.017, val_acc:0.970]
Epoch [45/120    avg_loss:0.015, val_acc:0.971]
Epoch [46/120    avg_loss:0.013, val_acc:0.977]
Epoch [47/120    avg_loss:0.017, val_acc:0.976]
Epoch [48/120    avg_loss:0.020, val_acc:0.975]
Epoch [49/120    avg_loss:0.013, val_acc:0.978]
Epoch [50/120    avg_loss:0.011, val_acc:0.974]
Epoch [51/120    avg_loss:0.011, val_acc:0.975]
Epoch [52/120    avg_loss:0.011, val_acc:0.976]
Epoch [53/120    avg_loss:0.014, val_acc:0.983]
Epoch [54/120    avg_loss:0.014, val_acc:0.982]
Epoch [55/120    avg_loss:0.009, val_acc:0.981]
Epoch [56/120    avg_loss:0.047, val_acc:0.960]
Epoch [57/120    avg_loss:0.023, val_acc:0.967]
Epoch [58/120    avg_loss:0.013, val_acc:0.976]
Epoch [59/120    avg_loss:0.009, val_acc:0.973]
Epoch [60/120    avg_loss:0.009, val_acc:0.975]
Epoch [61/120    avg_loss:0.008, val_acc:0.977]
Epoch [62/120    avg_loss:0.009, val_acc:0.976]
Epoch [63/120    avg_loss:0.019, val_acc:0.968]
Epoch [64/120    avg_loss:0.017, val_acc:0.971]
Epoch [65/120    avg_loss:0.012, val_acc:0.982]
Epoch [66/120    avg_loss:0.007, val_acc:0.978]
Epoch [67/120    avg_loss:0.006, val_acc:0.977]
Epoch [68/120    avg_loss:0.007, val_acc:0.980]
Epoch [69/120    avg_loss:0.007, val_acc:0.981]
Epoch [70/120    avg_loss:0.004, val_acc:0.980]
Epoch [71/120    avg_loss:0.006, val_acc:0.982]
Epoch [72/120    avg_loss:0.006, val_acc:0.981]
Epoch [73/120    avg_loss:0.005, val_acc:0.981]
Epoch [74/120    avg_loss:0.005, val_acc:0.981]
Epoch [75/120    avg_loss:0.005, val_acc:0.982]
Epoch [76/120    avg_loss:0.004, val_acc:0.984]
Epoch [77/120    avg_loss:0.005, val_acc:0.982]
Epoch [78/120    avg_loss:0.004, val_acc:0.982]
Epoch [79/120    avg_loss:0.003, val_acc:0.982]
Epoch [80/120    avg_loss:0.005, val_acc:0.982]
Epoch [81/120    avg_loss:0.007, val_acc:0.981]
Epoch [82/120    avg_loss:0.004, val_acc:0.981]
Epoch [83/120    avg_loss:0.004, val_acc:0.982]
Epoch [84/120    avg_loss:0.005, val_acc:0.982]
Epoch [85/120    avg_loss:0.004, val_acc:0.982]
Epoch [86/120    avg_loss:0.006, val_acc:0.981]
Epoch [87/120    avg_loss:0.004, val_acc:0.982]
Epoch [88/120    avg_loss:0.004, val_acc:0.983]
Epoch [89/120    avg_loss:0.004, val_acc:0.982]
Epoch [90/120    avg_loss:0.004, val_acc:0.982]
Epoch [91/120    avg_loss:0.004, val_acc:0.982]
Epoch [92/120    avg_loss:0.003, val_acc:0.982]
Epoch [93/120    avg_loss:0.004, val_acc:0.982]
Epoch [94/120    avg_loss:0.003, val_acc:0.982]
Epoch [95/120    avg_loss:0.004, val_acc:0.982]
Epoch [96/120    avg_loss:0.006, val_acc:0.982]
Epoch [97/120    avg_loss:0.004, val_acc:0.982]
Epoch [98/120    avg_loss:0.004, val_acc:0.982]
Epoch [99/120    avg_loss:0.004, val_acc:0.982]
Epoch [100/120    avg_loss:0.004, val_acc:0.982]
Epoch [101/120    avg_loss:0.005, val_acc:0.982]
Epoch [102/120    avg_loss:0.004, val_acc:0.982]
Epoch [103/120    avg_loss:0.004, val_acc:0.982]
Epoch [104/120    avg_loss:0.005, val_acc:0.982]
Epoch [105/120    avg_loss:0.004, val_acc:0.982]
Epoch [106/120    avg_loss:0.004, val_acc:0.982]
Epoch [107/120    avg_loss:0.004, val_acc:0.982]
Epoch [108/120    avg_loss:0.005, val_acc:0.982]
Epoch [109/120    avg_loss:0.003, val_acc:0.982]
Epoch [110/120    avg_loss:0.003, val_acc:0.982]
Epoch [111/120    avg_loss:0.003, val_acc:0.982]
Epoch [112/120    avg_loss:0.003, val_acc:0.982]
Epoch [113/120    avg_loss:0.003, val_acc:0.982]
Epoch [114/120    avg_loss:0.003, val_acc:0.982]
Epoch [115/120    avg_loss:0.004, val_acc:0.982]
Epoch [116/120    avg_loss:0.005, val_acc:0.982]
Epoch [117/120    avg_loss:0.003, val_acc:0.982]
Epoch [118/120    avg_loss:0.004, val_acc:0.982]
Epoch [119/120    avg_loss:0.004, val_acc:0.982]
Epoch [120/120    avg_loss:0.004, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1272    6    0    0    0    0    0    0    1    6    0    0
     0    0    0]
 [   0    0    0  715   10    0    0    0    0    1    0    9   11    0
     0    1    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    1    0    0    0    0  862    9    0    0
     0    1    0]
 [   0    0    1    3    0    0    0    0    0    0    7 2182   16    0
     0    1    0]
 [   0    0    0    2    1    0    0    0    0    0    2    1  526    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1125   14    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    74  273    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.94037940379404

F1 scores:
[       nan 0.98765432 0.99336197 0.97014925 0.97247706 0.99539171
 1.         1.         0.99883586 0.97297297 0.98683457 0.98800091
 0.96513761 1.         0.96112772 0.85714286 0.97619048]

Kappa:
0.9765094002002521
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3f27ef06d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.305, val_acc:0.455]
Epoch [2/120    avg_loss:1.641, val_acc:0.542]
Epoch [3/120    avg_loss:1.428, val_acc:0.627]
Epoch [4/120    avg_loss:1.208, val_acc:0.694]
Epoch [5/120    avg_loss:0.957, val_acc:0.723]
Epoch [6/120    avg_loss:0.769, val_acc:0.746]
Epoch [7/120    avg_loss:0.720, val_acc:0.752]
Epoch [8/120    avg_loss:0.615, val_acc:0.786]
Epoch [9/120    avg_loss:0.460, val_acc:0.800]
Epoch [10/120    avg_loss:0.488, val_acc:0.796]
Epoch [11/120    avg_loss:0.410, val_acc:0.857]
Epoch [12/120    avg_loss:0.308, val_acc:0.881]
Epoch [13/120    avg_loss:0.235, val_acc:0.892]
Epoch [14/120    avg_loss:0.331, val_acc:0.832]
Epoch [15/120    avg_loss:0.337, val_acc:0.832]
Epoch [16/120    avg_loss:0.339, val_acc:0.907]
Epoch [17/120    avg_loss:0.160, val_acc:0.949]
Epoch [18/120    avg_loss:0.164, val_acc:0.915]
Epoch [19/120    avg_loss:0.129, val_acc:0.895]
Epoch [20/120    avg_loss:0.116, val_acc:0.944]
Epoch [21/120    avg_loss:0.113, val_acc:0.944]
Epoch [22/120    avg_loss:0.079, val_acc:0.949]
Epoch [23/120    avg_loss:0.095, val_acc:0.934]
Epoch [24/120    avg_loss:0.093, val_acc:0.965]
Epoch [25/120    avg_loss:0.072, val_acc:0.957]
Epoch [26/120    avg_loss:0.060, val_acc:0.958]
Epoch [27/120    avg_loss:0.041, val_acc:0.958]
Epoch [28/120    avg_loss:0.037, val_acc:0.967]
Epoch [29/120    avg_loss:0.139, val_acc:0.961]
Epoch [30/120    avg_loss:0.089, val_acc:0.951]
Epoch [31/120    avg_loss:0.053, val_acc:0.971]
Epoch [32/120    avg_loss:0.035, val_acc:0.967]
Epoch [33/120    avg_loss:0.032, val_acc:0.968]
Epoch [34/120    avg_loss:0.026, val_acc:0.972]
Epoch [35/120    avg_loss:0.038, val_acc:0.969]
Epoch [36/120    avg_loss:0.035, val_acc:0.968]
Epoch [37/120    avg_loss:0.034, val_acc:0.960]
Epoch [38/120    avg_loss:0.026, val_acc:0.972]
Epoch [39/120    avg_loss:0.022, val_acc:0.974]
Epoch [40/120    avg_loss:0.041, val_acc:0.954]
Epoch [41/120    avg_loss:0.040, val_acc:0.967]
Epoch [42/120    avg_loss:0.022, val_acc:0.971]
Epoch [43/120    avg_loss:0.022, val_acc:0.954]
Epoch [44/120    avg_loss:0.046, val_acc:0.976]
Epoch [45/120    avg_loss:0.033, val_acc:0.971]
Epoch [46/120    avg_loss:0.017, val_acc:0.972]
Epoch [47/120    avg_loss:0.019, val_acc:0.975]
Epoch [48/120    avg_loss:0.026, val_acc:0.977]
Epoch [49/120    avg_loss:0.025, val_acc:0.969]
Epoch [50/120    avg_loss:0.030, val_acc:0.962]
Epoch [51/120    avg_loss:0.055, val_acc:0.974]
Epoch [52/120    avg_loss:0.015, val_acc:0.972]
Epoch [53/120    avg_loss:0.013, val_acc:0.969]
Epoch [54/120    avg_loss:0.013, val_acc:0.969]
Epoch [55/120    avg_loss:0.023, val_acc:0.975]
Epoch [56/120    avg_loss:0.016, val_acc:0.975]
Epoch [57/120    avg_loss:0.011, val_acc:0.977]
Epoch [58/120    avg_loss:0.010, val_acc:0.978]
Epoch [59/120    avg_loss:0.006, val_acc:0.978]
Epoch [60/120    avg_loss:0.005, val_acc:0.978]
Epoch [61/120    avg_loss:0.006, val_acc:0.975]
Epoch [62/120    avg_loss:0.008, val_acc:0.975]
Epoch [63/120    avg_loss:0.006, val_acc:0.976]
Epoch [64/120    avg_loss:0.006, val_acc:0.980]
Epoch [65/120    avg_loss:0.005, val_acc:0.976]
Epoch [66/120    avg_loss:0.003, val_acc:0.981]
Epoch [67/120    avg_loss:0.005, val_acc:0.981]
Epoch [68/120    avg_loss:0.006, val_acc:0.974]
Epoch [69/120    avg_loss:0.005, val_acc:0.976]
Epoch [70/120    avg_loss:0.004, val_acc:0.976]
Epoch [71/120    avg_loss:0.019, val_acc:0.957]
Epoch [72/120    avg_loss:0.011, val_acc:0.970]
Epoch [73/120    avg_loss:0.011, val_acc:0.971]
Epoch [74/120    avg_loss:0.012, val_acc:0.975]
Epoch [75/120    avg_loss:0.007, val_acc:0.978]
Epoch [76/120    avg_loss:0.005, val_acc:0.975]
Epoch [77/120    avg_loss:0.005, val_acc:0.976]
Epoch [78/120    avg_loss:0.007, val_acc:0.951]
Epoch [79/120    avg_loss:0.029, val_acc:0.970]
Epoch [80/120    avg_loss:0.007, val_acc:0.980]
Epoch [81/120    avg_loss:0.005, val_acc:0.980]
Epoch [82/120    avg_loss:0.003, val_acc:0.978]
Epoch [83/120    avg_loss:0.006, val_acc:0.980]
Epoch [84/120    avg_loss:0.004, val_acc:0.977]
Epoch [85/120    avg_loss:0.005, val_acc:0.978]
Epoch [86/120    avg_loss:0.006, val_acc:0.977]
Epoch [87/120    avg_loss:0.005, val_acc:0.977]
Epoch [88/120    avg_loss:0.005, val_acc:0.976]
Epoch [89/120    avg_loss:0.003, val_acc:0.978]
Epoch [90/120    avg_loss:0.004, val_acc:0.977]
Epoch [91/120    avg_loss:0.004, val_acc:0.977]
Epoch [92/120    avg_loss:0.005, val_acc:0.977]
Epoch [93/120    avg_loss:0.003, val_acc:0.977]
Epoch [94/120    avg_loss:0.003, val_acc:0.977]
Epoch [95/120    avg_loss:0.004, val_acc:0.977]
Epoch [96/120    avg_loss:0.003, val_acc:0.977]
Epoch [97/120    avg_loss:0.003, val_acc:0.977]
Epoch [98/120    avg_loss:0.004, val_acc:0.977]
Epoch [99/120    avg_loss:0.003, val_acc:0.977]
Epoch [100/120    avg_loss:0.007, val_acc:0.977]
Epoch [101/120    avg_loss:0.004, val_acc:0.977]
Epoch [102/120    avg_loss:0.003, val_acc:0.977]
Epoch [103/120    avg_loss:0.004, val_acc:0.977]
Epoch [104/120    avg_loss:0.004, val_acc:0.977]
Epoch [105/120    avg_loss:0.005, val_acc:0.977]
Epoch [106/120    avg_loss:0.003, val_acc:0.977]
Epoch [107/120    avg_loss:0.004, val_acc:0.977]
Epoch [108/120    avg_loss:0.004, val_acc:0.977]
Epoch [109/120    avg_loss:0.004, val_acc:0.977]
Epoch [110/120    avg_loss:0.003, val_acc:0.977]
Epoch [111/120    avg_loss:0.005, val_acc:0.977]
Epoch [112/120    avg_loss:0.003, val_acc:0.977]
Epoch [113/120    avg_loss:0.004, val_acc:0.977]
Epoch [114/120    avg_loss:0.003, val_acc:0.977]
Epoch [115/120    avg_loss:0.003, val_acc:0.977]
Epoch [116/120    avg_loss:0.006, val_acc:0.977]
Epoch [117/120    avg_loss:0.003, val_acc:0.977]
Epoch [118/120    avg_loss:0.010, val_acc:0.977]
Epoch [119/120    avg_loss:0.003, val_acc:0.977]
Epoch [120/120    avg_loss:0.003, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1266    6    0    0    0    0    0    0    6    7    0    0
     0    0    0]
 [   0    0    0  737    0    0    0    0    0    1    0    5    2    1
     1    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    3    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    4    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  857   12    0    0
     0    1    0]
 [   0    0    3    0    0    0    0    0    0    0    7 2185   12    0
     0    3    0]
 [   0    0    0    2    1    0    0    0    0    0    1    3  525    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1123   16    0]
 [   0    0    0    0    0    1    7    0    0    0    0    0    0    0
    86  253    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
97.76693766937669

F1 scores:
[       nan 1.         0.989449   0.98661312 0.99294118 0.99539171
 0.99088146 0.94339623 1.         0.97297297 0.98167239 0.98734749
 0.97402597 0.99730458 0.95574468 0.81612903 0.95757576]

Kappa:
0.9745214173339954
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f79a4546780>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.299, val_acc:0.475]
Epoch [2/120    avg_loss:1.655, val_acc:0.643]
Epoch [3/120    avg_loss:1.278, val_acc:0.703]
Epoch [4/120    avg_loss:1.111, val_acc:0.695]
Epoch [5/120    avg_loss:0.975, val_acc:0.748]
Epoch [6/120    avg_loss:0.815, val_acc:0.698]
Epoch [7/120    avg_loss:0.752, val_acc:0.783]
Epoch [8/120    avg_loss:0.530, val_acc:0.785]
Epoch [9/120    avg_loss:0.610, val_acc:0.800]
Epoch [10/120    avg_loss:0.460, val_acc:0.850]
Epoch [11/120    avg_loss:0.310, val_acc:0.857]
Epoch [12/120    avg_loss:0.515, val_acc:0.749]
Epoch [13/120    avg_loss:0.486, val_acc:0.866]
Epoch [14/120    avg_loss:0.391, val_acc:0.882]
Epoch [15/120    avg_loss:0.261, val_acc:0.839]
Epoch [16/120    avg_loss:0.276, val_acc:0.911]
Epoch [17/120    avg_loss:0.256, val_acc:0.899]
Epoch [18/120    avg_loss:0.167, val_acc:0.912]
Epoch [19/120    avg_loss:0.215, val_acc:0.891]
Epoch [20/120    avg_loss:0.165, val_acc:0.932]
Epoch [21/120    avg_loss:0.119, val_acc:0.936]
Epoch [22/120    avg_loss:0.105, val_acc:0.939]
Epoch [23/120    avg_loss:0.082, val_acc:0.936]
Epoch [24/120    avg_loss:0.102, val_acc:0.946]
Epoch [25/120    avg_loss:0.084, val_acc:0.949]
Epoch [26/120    avg_loss:0.081, val_acc:0.946]
Epoch [27/120    avg_loss:0.055, val_acc:0.957]
Epoch [28/120    avg_loss:0.071, val_acc:0.948]
Epoch [29/120    avg_loss:0.119, val_acc:0.938]
Epoch [30/120    avg_loss:0.077, val_acc:0.954]
Epoch [31/120    avg_loss:0.061, val_acc:0.951]
Epoch [32/120    avg_loss:0.066, val_acc:0.951]
Epoch [33/120    avg_loss:0.031, val_acc:0.959]
Epoch [34/120    avg_loss:0.030, val_acc:0.960]
Epoch [35/120    avg_loss:0.023, val_acc:0.960]
Epoch [36/120    avg_loss:0.032, val_acc:0.958]
Epoch [37/120    avg_loss:0.040, val_acc:0.972]
Epoch [38/120    avg_loss:0.033, val_acc:0.947]
Epoch [39/120    avg_loss:0.031, val_acc:0.963]
Epoch [40/120    avg_loss:0.032, val_acc:0.960]
Epoch [41/120    avg_loss:0.021, val_acc:0.967]
Epoch [42/120    avg_loss:0.023, val_acc:0.968]
Epoch [43/120    avg_loss:0.023, val_acc:0.957]
Epoch [44/120    avg_loss:0.030, val_acc:0.964]
Epoch [45/120    avg_loss:0.018, val_acc:0.966]
Epoch [46/120    avg_loss:0.021, val_acc:0.957]
Epoch [47/120    avg_loss:0.020, val_acc:0.970]
Epoch [48/120    avg_loss:0.014, val_acc:0.956]
Epoch [49/120    avg_loss:0.012, val_acc:0.972]
Epoch [50/120    avg_loss:0.018, val_acc:0.961]
Epoch [51/120    avg_loss:0.012, val_acc:0.966]
Epoch [52/120    avg_loss:0.038, val_acc:0.963]
Epoch [53/120    avg_loss:0.035, val_acc:0.949]
Epoch [54/120    avg_loss:0.156, val_acc:0.908]
Epoch [55/120    avg_loss:0.109, val_acc:0.943]
Epoch [56/120    avg_loss:0.069, val_acc:0.949]
Epoch [57/120    avg_loss:0.058, val_acc:0.959]
Epoch [58/120    avg_loss:0.256, val_acc:0.861]
Epoch [59/120    avg_loss:0.177, val_acc:0.934]
Epoch [60/120    avg_loss:0.094, val_acc:0.950]
Epoch [61/120    avg_loss:0.070, val_acc:0.960]
Epoch [62/120    avg_loss:0.073, val_acc:0.960]
Epoch [63/120    avg_loss:0.031, val_acc:0.967]
Epoch [64/120    avg_loss:0.023, val_acc:0.970]
Epoch [65/120    avg_loss:0.022, val_acc:0.970]
Epoch [66/120    avg_loss:0.021, val_acc:0.977]
Epoch [67/120    avg_loss:0.020, val_acc:0.977]
Epoch [68/120    avg_loss:0.019, val_acc:0.977]
Epoch [69/120    avg_loss:0.014, val_acc:0.979]
Epoch [70/120    avg_loss:0.017, val_acc:0.979]
Epoch [71/120    avg_loss:0.016, val_acc:0.977]
Epoch [72/120    avg_loss:0.016, val_acc:0.979]
Epoch [73/120    avg_loss:0.016, val_acc:0.978]
Epoch [74/120    avg_loss:0.018, val_acc:0.980]
Epoch [75/120    avg_loss:0.013, val_acc:0.981]
Epoch [76/120    avg_loss:0.015, val_acc:0.981]
Epoch [77/120    avg_loss:0.013, val_acc:0.983]
Epoch [78/120    avg_loss:0.013, val_acc:0.981]
Epoch [79/120    avg_loss:0.015, val_acc:0.981]
Epoch [80/120    avg_loss:0.015, val_acc:0.980]
Epoch [81/120    avg_loss:0.014, val_acc:0.980]
Epoch [82/120    avg_loss:0.015, val_acc:0.980]
Epoch [83/120    avg_loss:0.012, val_acc:0.981]
Epoch [84/120    avg_loss:0.016, val_acc:0.981]
Epoch [85/120    avg_loss:0.011, val_acc:0.981]
Epoch [86/120    avg_loss:0.011, val_acc:0.982]
Epoch [87/120    avg_loss:0.013, val_acc:0.978]
Epoch [88/120    avg_loss:0.013, val_acc:0.982]
Epoch [89/120    avg_loss:0.011, val_acc:0.983]
Epoch [90/120    avg_loss:0.013, val_acc:0.983]
Epoch [91/120    avg_loss:0.017, val_acc:0.981]
Epoch [92/120    avg_loss:0.013, val_acc:0.981]
Epoch [93/120    avg_loss:0.011, val_acc:0.981]
Epoch [94/120    avg_loss:0.013, val_acc:0.983]
Epoch [95/120    avg_loss:0.010, val_acc:0.981]
Epoch [96/120    avg_loss:0.008, val_acc:0.981]
Epoch [97/120    avg_loss:0.012, val_acc:0.981]
Epoch [98/120    avg_loss:0.014, val_acc:0.981]
Epoch [99/120    avg_loss:0.013, val_acc:0.980]
Epoch [100/120    avg_loss:0.013, val_acc:0.980]
Epoch [101/120    avg_loss:0.010, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.981]
Epoch [103/120    avg_loss:0.010, val_acc:0.978]
Epoch [104/120    avg_loss:0.012, val_acc:0.979]
Epoch [105/120    avg_loss:0.010, val_acc:0.981]
Epoch [106/120    avg_loss:0.010, val_acc:0.981]
Epoch [107/120    avg_loss:0.010, val_acc:0.981]
Epoch [108/120    avg_loss:0.010, val_acc:0.981]
Epoch [109/120    avg_loss:0.008, val_acc:0.981]
Epoch [110/120    avg_loss:0.009, val_acc:0.981]
Epoch [111/120    avg_loss:0.009, val_acc:0.981]
Epoch [112/120    avg_loss:0.011, val_acc:0.981]
Epoch [113/120    avg_loss:0.010, val_acc:0.981]
Epoch [114/120    avg_loss:0.009, val_acc:0.981]
Epoch [115/120    avg_loss:0.008, val_acc:0.981]
Epoch [116/120    avg_loss:0.013, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.981]
Epoch [118/120    avg_loss:0.012, val_acc:0.981]
Epoch [119/120    avg_loss:0.009, val_acc:0.981]
Epoch [120/120    avg_loss:0.011, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1259   10    1    0    0    0    0    0    3   12    0    0
     0    0    0]
 [   0    0    0  737    3    0    0    0    0    1    0    0    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    4    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  839   29    0    0
     1    1    0]
 [   0    0    8    0    0    0    0    0    0    0   12 2165   13   10
     0    2    0]
 [   0    0    0    5    1    0    0    0    0    0    0    1  523    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
    99  247    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.34417344173441

F1 scores:
[       nan 1.         0.98474775 0.98332221 0.98839907 0.99654776
 0.99618029 1.         1.         0.97297297 0.97050318 0.97919493
 0.97211896 0.97368421 0.95274262 0.8125     0.98245614]

Kappa:
0.9697001559202728
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f29f9d1d6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.236, val_acc:0.453]
Epoch [2/120    avg_loss:1.600, val_acc:0.607]
Epoch [3/120    avg_loss:1.268, val_acc:0.636]
Epoch [4/120    avg_loss:1.108, val_acc:0.653]
Epoch [5/120    avg_loss:0.863, val_acc:0.728]
Epoch [6/120    avg_loss:0.827, val_acc:0.666]
Epoch [7/120    avg_loss:0.722, val_acc:0.791]
Epoch [8/120    avg_loss:0.539, val_acc:0.817]
Epoch [9/120    avg_loss:0.446, val_acc:0.861]
Epoch [10/120    avg_loss:0.371, val_acc:0.848]
Epoch [11/120    avg_loss:0.302, val_acc:0.875]
Epoch [12/120    avg_loss:0.261, val_acc:0.842]
Epoch [13/120    avg_loss:0.239, val_acc:0.831]
Epoch [14/120    avg_loss:0.513, val_acc:0.846]
Epoch [15/120    avg_loss:0.429, val_acc:0.790]
Epoch [16/120    avg_loss:0.293, val_acc:0.871]
Epoch [17/120    avg_loss:0.197, val_acc:0.907]
Epoch [18/120    avg_loss:0.199, val_acc:0.905]
Epoch [19/120    avg_loss:0.147, val_acc:0.915]
Epoch [20/120    avg_loss:0.132, val_acc:0.927]
Epoch [21/120    avg_loss:0.093, val_acc:0.921]
Epoch [22/120    avg_loss:0.124, val_acc:0.922]
Epoch [23/120    avg_loss:0.074, val_acc:0.940]
Epoch [24/120    avg_loss:0.089, val_acc:0.951]
Epoch [25/120    avg_loss:0.085, val_acc:0.912]
Epoch [26/120    avg_loss:0.075, val_acc:0.927]
Epoch [27/120    avg_loss:0.056, val_acc:0.932]
Epoch [28/120    avg_loss:0.079, val_acc:0.951]
Epoch [29/120    avg_loss:0.041, val_acc:0.959]
Epoch [30/120    avg_loss:0.056, val_acc:0.939]
Epoch [31/120    avg_loss:0.044, val_acc:0.960]
Epoch [32/120    avg_loss:0.033, val_acc:0.949]
Epoch [33/120    avg_loss:0.038, val_acc:0.943]
Epoch [34/120    avg_loss:0.082, val_acc:0.943]
Epoch [35/120    avg_loss:0.048, val_acc:0.953]
Epoch [36/120    avg_loss:0.029, val_acc:0.951]
Epoch [37/120    avg_loss:0.134, val_acc:0.939]
Epoch [38/120    avg_loss:0.046, val_acc:0.947]
Epoch [39/120    avg_loss:0.065, val_acc:0.962]
Epoch [40/120    avg_loss:0.035, val_acc:0.932]
Epoch [41/120    avg_loss:0.028, val_acc:0.957]
Epoch [42/120    avg_loss:0.023, val_acc:0.962]
Epoch [43/120    avg_loss:0.018, val_acc:0.968]
Epoch [44/120    avg_loss:0.011, val_acc:0.967]
Epoch [45/120    avg_loss:0.012, val_acc:0.971]
Epoch [46/120    avg_loss:0.027, val_acc:0.952]
Epoch [47/120    avg_loss:0.031, val_acc:0.963]
Epoch [48/120    avg_loss:0.065, val_acc:0.921]
Epoch [49/120    avg_loss:0.056, val_acc:0.948]
Epoch [50/120    avg_loss:0.032, val_acc:0.962]
Epoch [51/120    avg_loss:0.057, val_acc:0.926]
Epoch [52/120    avg_loss:0.024, val_acc:0.970]
Epoch [53/120    avg_loss:0.020, val_acc:0.965]
Epoch [54/120    avg_loss:0.023, val_acc:0.961]
Epoch [55/120    avg_loss:0.018, val_acc:0.959]
Epoch [56/120    avg_loss:0.012, val_acc:0.967]
Epoch [57/120    avg_loss:0.010, val_acc:0.969]
Epoch [58/120    avg_loss:0.008, val_acc:0.971]
Epoch [59/120    avg_loss:0.010, val_acc:0.976]
Epoch [60/120    avg_loss:0.013, val_acc:0.968]
Epoch [61/120    avg_loss:0.018, val_acc:0.943]
Epoch [62/120    avg_loss:0.038, val_acc:0.962]
Epoch [63/120    avg_loss:0.020, val_acc:0.972]
Epoch [64/120    avg_loss:0.009, val_acc:0.970]
Epoch [65/120    avg_loss:0.015, val_acc:0.974]
Epoch [66/120    avg_loss:0.007, val_acc:0.978]
Epoch [67/120    avg_loss:0.005, val_acc:0.953]
Epoch [68/120    avg_loss:0.009, val_acc:0.983]
Epoch [69/120    avg_loss:0.004, val_acc:0.983]
Epoch [70/120    avg_loss:0.007, val_acc:0.980]
Epoch [71/120    avg_loss:0.007, val_acc:0.981]
Epoch [72/120    avg_loss:0.011, val_acc:0.973]
Epoch [73/120    avg_loss:0.006, val_acc:0.978]
Epoch [74/120    avg_loss:0.004, val_acc:0.984]
Epoch [75/120    avg_loss:0.004, val_acc:0.977]
Epoch [76/120    avg_loss:0.004, val_acc:0.983]
Epoch [77/120    avg_loss:0.003, val_acc:0.984]
Epoch [78/120    avg_loss:0.007, val_acc:0.954]
Epoch [79/120    avg_loss:0.017, val_acc:0.969]
Epoch [80/120    avg_loss:0.004, val_acc:0.978]
Epoch [81/120    avg_loss:0.003, val_acc:0.980]
Epoch [82/120    avg_loss:0.003, val_acc:0.981]
Epoch [83/120    avg_loss:0.003, val_acc:0.982]
Epoch [84/120    avg_loss:0.003, val_acc:0.980]
Epoch [85/120    avg_loss:0.002, val_acc:0.978]
Epoch [86/120    avg_loss:0.020, val_acc:0.971]
Epoch [87/120    avg_loss:0.011, val_acc:0.970]
Epoch [88/120    avg_loss:0.012, val_acc:0.973]
Epoch [89/120    avg_loss:0.006, val_acc:0.974]
Epoch [90/120    avg_loss:0.009, val_acc:0.974]
Epoch [91/120    avg_loss:0.006, val_acc:0.976]
Epoch [92/120    avg_loss:0.005, val_acc:0.977]
Epoch [93/120    avg_loss:0.003, val_acc:0.977]
Epoch [94/120    avg_loss:0.006, val_acc:0.977]
Epoch [95/120    avg_loss:0.003, val_acc:0.977]
Epoch [96/120    avg_loss:0.003, val_acc:0.976]
Epoch [97/120    avg_loss:0.003, val_acc:0.977]
Epoch [98/120    avg_loss:0.004, val_acc:0.976]
Epoch [99/120    avg_loss:0.004, val_acc:0.977]
Epoch [100/120    avg_loss:0.004, val_acc:0.977]
Epoch [101/120    avg_loss:0.002, val_acc:0.977]
Epoch [102/120    avg_loss:0.003, val_acc:0.976]
Epoch [103/120    avg_loss:0.002, val_acc:0.977]
Epoch [104/120    avg_loss:0.002, val_acc:0.977]
Epoch [105/120    avg_loss:0.003, val_acc:0.977]
Epoch [106/120    avg_loss:0.003, val_acc:0.977]
Epoch [107/120    avg_loss:0.002, val_acc:0.977]
Epoch [108/120    avg_loss:0.002, val_acc:0.977]
Epoch [109/120    avg_loss:0.003, val_acc:0.977]
Epoch [110/120    avg_loss:0.003, val_acc:0.977]
Epoch [111/120    avg_loss:0.003, val_acc:0.977]
Epoch [112/120    avg_loss:0.003, val_acc:0.977]
Epoch [113/120    avg_loss:0.003, val_acc:0.977]
Epoch [114/120    avg_loss:0.003, val_acc:0.977]
Epoch [115/120    avg_loss:0.003, val_acc:0.977]
Epoch [116/120    avg_loss:0.003, val_acc:0.977]
Epoch [117/120    avg_loss:0.003, val_acc:0.977]
Epoch [118/120    avg_loss:0.003, val_acc:0.977]
Epoch [119/120    avg_loss:0.004, val_acc:0.977]
Epoch [120/120    avg_loss:0.003, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    5    3    1    0    0    0    0    1    8    7    0
     0    0    0]
 [   0    0    0  721    3    0    0    0    0    1    0    8   14    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0  856   17    0    0
     1    0    0]
 [   0    0    6    0    0    1    0    0    0    1    3 2182   16    0
     0    1    0]
 [   0    0    1    1    0    0    0    0    0    0    0    0  530    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1136    3    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
    95  251    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.74525745257452

F1 scores:
[       nan 1.         0.98707403 0.97829037 0.98611111 0.99078341
 0.99923839 1.         1.         0.92307692 0.98674352 0.98599187
 0.96188748 1.         0.95663158 0.83388704 0.98224852]

Kappa:
0.9742732239393846
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f174a56a6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.249, val_acc:0.521]
Epoch [2/120    avg_loss:1.673, val_acc:0.632]
Epoch [3/120    avg_loss:1.344, val_acc:0.676]
Epoch [4/120    avg_loss:1.072, val_acc:0.744]
Epoch [5/120    avg_loss:0.815, val_acc:0.822]
Epoch [6/120    avg_loss:0.636, val_acc:0.837]
Epoch [7/120    avg_loss:0.617, val_acc:0.825]
Epoch [8/120    avg_loss:0.566, val_acc:0.860]
Epoch [9/120    avg_loss:0.439, val_acc:0.861]
Epoch [10/120    avg_loss:0.308, val_acc:0.888]
Epoch [11/120    avg_loss:0.325, val_acc:0.903]
Epoch [12/120    avg_loss:0.290, val_acc:0.913]
Epoch [13/120    avg_loss:0.232, val_acc:0.926]
Epoch [14/120    avg_loss:0.185, val_acc:0.919]
Epoch [15/120    avg_loss:0.181, val_acc:0.906]
Epoch [16/120    avg_loss:0.147, val_acc:0.936]
Epoch [17/120    avg_loss:0.286, val_acc:0.794]
Epoch [18/120    avg_loss:0.259, val_acc:0.907]
Epoch [19/120    avg_loss:0.167, val_acc:0.939]
Epoch [20/120    avg_loss:0.113, val_acc:0.939]
Epoch [21/120    avg_loss:0.128, val_acc:0.949]
Epoch [22/120    avg_loss:0.123, val_acc:0.941]
Epoch [23/120    avg_loss:0.111, val_acc:0.950]
Epoch [24/120    avg_loss:0.100, val_acc:0.950]
Epoch [25/120    avg_loss:0.081, val_acc:0.933]
Epoch [26/120    avg_loss:0.106, val_acc:0.943]
Epoch [27/120    avg_loss:0.064, val_acc:0.951]
Epoch [28/120    avg_loss:0.059, val_acc:0.926]
Epoch [29/120    avg_loss:0.085, val_acc:0.966]
Epoch [30/120    avg_loss:0.060, val_acc:0.971]
Epoch [31/120    avg_loss:0.049, val_acc:0.975]
Epoch [32/120    avg_loss:0.041, val_acc:0.977]
Epoch [33/120    avg_loss:0.025, val_acc:0.963]
Epoch [34/120    avg_loss:0.047, val_acc:0.957]
Epoch [35/120    avg_loss:0.103, val_acc:0.938]
Epoch [36/120    avg_loss:0.088, val_acc:0.964]
Epoch [37/120    avg_loss:0.072, val_acc:0.944]
Epoch [38/120    avg_loss:0.108, val_acc:0.966]
Epoch [39/120    avg_loss:0.077, val_acc:0.929]
Epoch [40/120    avg_loss:0.047, val_acc:0.961]
Epoch [41/120    avg_loss:0.083, val_acc:0.956]
Epoch [42/120    avg_loss:0.068, val_acc:0.969]
Epoch [43/120    avg_loss:0.044, val_acc:0.963]
Epoch [44/120    avg_loss:0.034, val_acc:0.972]
Epoch [45/120    avg_loss:0.077, val_acc:0.960]
Epoch [46/120    avg_loss:0.036, val_acc:0.965]
Epoch [47/120    avg_loss:0.032, val_acc:0.971]
Epoch [48/120    avg_loss:0.034, val_acc:0.974]
Epoch [49/120    avg_loss:0.028, val_acc:0.977]
Epoch [50/120    avg_loss:0.022, val_acc:0.977]
Epoch [51/120    avg_loss:0.020, val_acc:0.976]
Epoch [52/120    avg_loss:0.015, val_acc:0.976]
Epoch [53/120    avg_loss:0.014, val_acc:0.976]
Epoch [54/120    avg_loss:0.016, val_acc:0.976]
Epoch [55/120    avg_loss:0.020, val_acc:0.977]
Epoch [56/120    avg_loss:0.016, val_acc:0.977]
Epoch [57/120    avg_loss:0.018, val_acc:0.979]
Epoch [58/120    avg_loss:0.016, val_acc:0.979]
Epoch [59/120    avg_loss:0.012, val_acc:0.979]
Epoch [60/120    avg_loss:0.014, val_acc:0.979]
Epoch [61/120    avg_loss:0.013, val_acc:0.979]
Epoch [62/120    avg_loss:0.014, val_acc:0.978]
Epoch [63/120    avg_loss:0.013, val_acc:0.977]
Epoch [64/120    avg_loss:0.018, val_acc:0.979]
Epoch [65/120    avg_loss:0.015, val_acc:0.979]
Epoch [66/120    avg_loss:0.016, val_acc:0.978]
Epoch [67/120    avg_loss:0.012, val_acc:0.980]
Epoch [68/120    avg_loss:0.012, val_acc:0.979]
Epoch [69/120    avg_loss:0.012, val_acc:0.979]
Epoch [70/120    avg_loss:0.013, val_acc:0.980]
Epoch [71/120    avg_loss:0.011, val_acc:0.980]
Epoch [72/120    avg_loss:0.012, val_acc:0.978]
Epoch [73/120    avg_loss:0.011, val_acc:0.980]
Epoch [74/120    avg_loss:0.012, val_acc:0.980]
Epoch [75/120    avg_loss:0.011, val_acc:0.981]
Epoch [76/120    avg_loss:0.012, val_acc:0.981]
Epoch [77/120    avg_loss:0.013, val_acc:0.981]
Epoch [78/120    avg_loss:0.012, val_acc:0.980]
Epoch [79/120    avg_loss:0.011, val_acc:0.979]
Epoch [80/120    avg_loss:0.010, val_acc:0.980]
Epoch [81/120    avg_loss:0.011, val_acc:0.981]
Epoch [82/120    avg_loss:0.009, val_acc:0.981]
Epoch [83/120    avg_loss:0.010, val_acc:0.982]
Epoch [84/120    avg_loss:0.008, val_acc:0.982]
Epoch [85/120    avg_loss:0.008, val_acc:0.982]
Epoch [86/120    avg_loss:0.011, val_acc:0.982]
Epoch [87/120    avg_loss:0.009, val_acc:0.982]
Epoch [88/120    avg_loss:0.011, val_acc:0.982]
Epoch [89/120    avg_loss:0.009, val_acc:0.982]
Epoch [90/120    avg_loss:0.012, val_acc:0.982]
Epoch [91/120    avg_loss:0.010, val_acc:0.982]
Epoch [92/120    avg_loss:0.008, val_acc:0.982]
Epoch [93/120    avg_loss:0.009, val_acc:0.982]
Epoch [94/120    avg_loss:0.009, val_acc:0.981]
Epoch [95/120    avg_loss:0.008, val_acc:0.981]
Epoch [96/120    avg_loss:0.009, val_acc:0.981]
Epoch [97/120    avg_loss:0.009, val_acc:0.981]
Epoch [98/120    avg_loss:0.010, val_acc:0.981]
Epoch [99/120    avg_loss:0.008, val_acc:0.981]
Epoch [100/120    avg_loss:0.009, val_acc:0.982]
Epoch [101/120    avg_loss:0.009, val_acc:0.978]
Epoch [102/120    avg_loss:0.010, val_acc:0.980]
Epoch [103/120    avg_loss:0.009, val_acc:0.982]
Epoch [104/120    avg_loss:0.008, val_acc:0.982]
Epoch [105/120    avg_loss:0.012, val_acc:0.982]
Epoch [106/120    avg_loss:0.008, val_acc:0.981]
Epoch [107/120    avg_loss:0.008, val_acc:0.982]
Epoch [108/120    avg_loss:0.008, val_acc:0.981]
Epoch [109/120    avg_loss:0.009, val_acc:0.981]
Epoch [110/120    avg_loss:0.008, val_acc:0.982]
Epoch [111/120    avg_loss:0.009, val_acc:0.982]
Epoch [112/120    avg_loss:0.008, val_acc:0.981]
Epoch [113/120    avg_loss:0.007, val_acc:0.979]
Epoch [114/120    avg_loss:0.009, val_acc:0.982]
Epoch [115/120    avg_loss:0.011, val_acc:0.981]
Epoch [116/120    avg_loss:0.006, val_acc:0.980]
Epoch [117/120    avg_loss:0.007, val_acc:0.981]
Epoch [118/120    avg_loss:0.008, val_acc:0.981]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.008, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1265    0    0    0    1    0    0    1    2    5   11    0
     0    0    0]
 [   0    0    0  728    0    1    0    0    0    5    1    0   10    2
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    1    3    0    0    0  853    8    0    0
     2    4    0]
 [   0    0   10    0    0    0    2    0    0    0    7 2176   12    0
     0    3    0]
 [   0    0    0    1    0    1    0    0    0    0    0    0  530    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
  1134    3    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    48  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.12466124661246

F1 scores:
[       nan 0.98765432 0.98635478 0.98578199 0.99764706 0.99310345
 0.98422239 0.98039216 0.99883586 0.8372093  0.98158803 0.98886617
 0.96363636 0.99462366 0.97590361 0.89302326 0.97619048]

Kappa:
0.9786217636520887
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:18:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7c865f96d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.228, val_acc:0.528]
Epoch [2/120    avg_loss:1.605, val_acc:0.565]
Epoch [3/120    avg_loss:1.299, val_acc:0.671]
Epoch [4/120    avg_loss:1.089, val_acc:0.718]
Epoch [5/120    avg_loss:0.845, val_acc:0.732]
Epoch [6/120    avg_loss:0.804, val_acc:0.808]
Epoch [7/120    avg_loss:0.676, val_acc:0.799]
Epoch [8/120    avg_loss:0.598, val_acc:0.808]
Epoch [9/120    avg_loss:0.519, val_acc:0.853]
Epoch [10/120    avg_loss:0.435, val_acc:0.835]
Epoch [11/120    avg_loss:0.422, val_acc:0.879]
Epoch [12/120    avg_loss:0.427, val_acc:0.849]
Epoch [13/120    avg_loss:0.357, val_acc:0.848]
Epoch [14/120    avg_loss:0.474, val_acc:0.872]
Epoch [15/120    avg_loss:0.300, val_acc:0.909]
Epoch [16/120    avg_loss:0.317, val_acc:0.878]
Epoch [17/120    avg_loss:0.261, val_acc:0.890]
Epoch [18/120    avg_loss:0.181, val_acc:0.912]
Epoch [19/120    avg_loss:0.202, val_acc:0.910]
Epoch [20/120    avg_loss:0.164, val_acc:0.925]
Epoch [21/120    avg_loss:0.186, val_acc:0.892]
Epoch [22/120    avg_loss:0.162, val_acc:0.922]
Epoch [23/120    avg_loss:0.194, val_acc:0.927]
Epoch [24/120    avg_loss:0.106, val_acc:0.928]
Epoch [25/120    avg_loss:0.127, val_acc:0.943]
Epoch [26/120    avg_loss:0.109, val_acc:0.946]
Epoch [27/120    avg_loss:0.098, val_acc:0.941]
Epoch [28/120    avg_loss:0.086, val_acc:0.955]
Epoch [29/120    avg_loss:0.082, val_acc:0.951]
Epoch [30/120    avg_loss:0.104, val_acc:0.935]
Epoch [31/120    avg_loss:0.088, val_acc:0.957]
Epoch [32/120    avg_loss:0.134, val_acc:0.901]
Epoch [33/120    avg_loss:0.116, val_acc:0.948]
Epoch [34/120    avg_loss:0.095, val_acc:0.943]
Epoch [35/120    avg_loss:0.072, val_acc:0.950]
Epoch [36/120    avg_loss:0.120, val_acc:0.949]
Epoch [37/120    avg_loss:0.105, val_acc:0.929]
Epoch [38/120    avg_loss:0.061, val_acc:0.953]
Epoch [39/120    avg_loss:0.125, val_acc:0.931]
Epoch [40/120    avg_loss:0.097, val_acc:0.943]
Epoch [41/120    avg_loss:0.072, val_acc:0.949]
Epoch [42/120    avg_loss:0.086, val_acc:0.955]
Epoch [43/120    avg_loss:0.063, val_acc:0.956]
Epoch [44/120    avg_loss:0.039, val_acc:0.956]
Epoch [45/120    avg_loss:0.031, val_acc:0.963]
Epoch [46/120    avg_loss:0.028, val_acc:0.969]
Epoch [47/120    avg_loss:0.027, val_acc:0.971]
Epoch [48/120    avg_loss:0.023, val_acc:0.971]
Epoch [49/120    avg_loss:0.024, val_acc:0.972]
Epoch [50/120    avg_loss:0.017, val_acc:0.974]
Epoch [51/120    avg_loss:0.023, val_acc:0.972]
Epoch [52/120    avg_loss:0.019, val_acc:0.974]
Epoch [53/120    avg_loss:0.024, val_acc:0.973]
Epoch [54/120    avg_loss:0.019, val_acc:0.972]
Epoch [55/120    avg_loss:0.020, val_acc:0.972]
Epoch [56/120    avg_loss:0.016, val_acc:0.974]
Epoch [57/120    avg_loss:0.019, val_acc:0.972]
Epoch [58/120    avg_loss:0.019, val_acc:0.972]
Epoch [59/120    avg_loss:0.018, val_acc:0.971]
Epoch [60/120    avg_loss:0.016, val_acc:0.973]
Epoch [61/120    avg_loss:0.019, val_acc:0.972]
Epoch [62/120    avg_loss:0.018, val_acc:0.971]
Epoch [63/120    avg_loss:0.017, val_acc:0.972]
Epoch [64/120    avg_loss:0.018, val_acc:0.973]
Epoch [65/120    avg_loss:0.014, val_acc:0.973]
Epoch [66/120    avg_loss:0.016, val_acc:0.974]
Epoch [67/120    avg_loss:0.017, val_acc:0.974]
Epoch [68/120    avg_loss:0.017, val_acc:0.973]
Epoch [69/120    avg_loss:0.017, val_acc:0.974]
Epoch [70/120    avg_loss:0.014, val_acc:0.974]
Epoch [71/120    avg_loss:0.014, val_acc:0.975]
Epoch [72/120    avg_loss:0.013, val_acc:0.973]
Epoch [73/120    avg_loss:0.018, val_acc:0.970]
Epoch [74/120    avg_loss:0.015, val_acc:0.973]
Epoch [75/120    avg_loss:0.015, val_acc:0.975]
Epoch [76/120    avg_loss:0.015, val_acc:0.977]
Epoch [77/120    avg_loss:0.015, val_acc:0.974]
Epoch [78/120    avg_loss:0.015, val_acc:0.977]
Epoch [79/120    avg_loss:0.012, val_acc:0.976]
Epoch [80/120    avg_loss:0.012, val_acc:0.977]
Epoch [81/120    avg_loss:0.015, val_acc:0.978]
Epoch [82/120    avg_loss:0.014, val_acc:0.977]
Epoch [83/120    avg_loss:0.014, val_acc:0.975]
Epoch [84/120    avg_loss:0.012, val_acc:0.976]
Epoch [85/120    avg_loss:0.012, val_acc:0.978]
Epoch [86/120    avg_loss:0.013, val_acc:0.975]
Epoch [87/120    avg_loss:0.012, val_acc:0.974]
Epoch [88/120    avg_loss:0.017, val_acc:0.974]
Epoch [89/120    avg_loss:0.017, val_acc:0.973]
Epoch [90/120    avg_loss:0.015, val_acc:0.975]
Epoch [91/120    avg_loss:0.014, val_acc:0.975]
Epoch [92/120    avg_loss:0.012, val_acc:0.975]
Epoch [93/120    avg_loss:0.012, val_acc:0.974]
Epoch [94/120    avg_loss:0.011, val_acc:0.973]
Epoch [95/120    avg_loss:0.013, val_acc:0.973]
Epoch [96/120    avg_loss:0.015, val_acc:0.971]
Epoch [97/120    avg_loss:0.013, val_acc:0.970]
Epoch [98/120    avg_loss:0.014, val_acc:0.972]
Epoch [99/120    avg_loss:0.010, val_acc:0.972]
Epoch [100/120    avg_loss:0.014, val_acc:0.973]
Epoch [101/120    avg_loss:0.012, val_acc:0.972]
Epoch [102/120    avg_loss:0.010, val_acc:0.972]
Epoch [103/120    avg_loss:0.012, val_acc:0.972]
Epoch [104/120    avg_loss:0.013, val_acc:0.972]
Epoch [105/120    avg_loss:0.012, val_acc:0.972]
Epoch [106/120    avg_loss:0.012, val_acc:0.972]
Epoch [107/120    avg_loss:0.014, val_acc:0.972]
Epoch [108/120    avg_loss:0.013, val_acc:0.972]
Epoch [109/120    avg_loss:0.014, val_acc:0.972]
Epoch [110/120    avg_loss:0.012, val_acc:0.972]
Epoch [111/120    avg_loss:0.013, val_acc:0.972]
Epoch [112/120    avg_loss:0.012, val_acc:0.972]
Epoch [113/120    avg_loss:0.013, val_acc:0.972]
Epoch [114/120    avg_loss:0.011, val_acc:0.972]
Epoch [115/120    avg_loss:0.012, val_acc:0.972]
Epoch [116/120    avg_loss:0.012, val_acc:0.972]
Epoch [117/120    avg_loss:0.012, val_acc:0.972]
Epoch [118/120    avg_loss:0.013, val_acc:0.972]
Epoch [119/120    avg_loss:0.011, val_acc:0.972]
Epoch [120/120    avg_loss:0.013, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    0    0    0    1    0    0    0   10    8    3    0
     0    2    0]
 [   0    0    0  726    2    0    0    0    0    3    0    0   16    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  422    0    0    0    8    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    2    0    0    0    1  845   25    0    0
     0    1    0]
 [   0    0    0    0    2    0    1    0    0    0    3 2194    9    1
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    0    1  528    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1125   14    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    38  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
98.05962059620596

F1 scores:
[       nan 0.98765432 0.98979592 0.98240866 0.98598131 0.99424626
 0.99018868 0.98039216 0.99061033 0.9        0.97518754 0.98851093
 0.95738894 0.99730458 0.9765625  0.90196078 0.95757576]

Kappa:
0.977869550166903
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1d469f97f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.272, val_acc:0.495]
Epoch [2/120    avg_loss:1.644, val_acc:0.548]
Epoch [3/120    avg_loss:1.373, val_acc:0.682]
Epoch [4/120    avg_loss:1.267, val_acc:0.663]
Epoch [5/120    avg_loss:1.036, val_acc:0.648]
Epoch [6/120    avg_loss:0.855, val_acc:0.776]
Epoch [7/120    avg_loss:0.682, val_acc:0.794]
Epoch [8/120    avg_loss:0.655, val_acc:0.816]
Epoch [9/120    avg_loss:0.538, val_acc:0.799]
Epoch [10/120    avg_loss:0.529, val_acc:0.775]
Epoch [11/120    avg_loss:0.548, val_acc:0.844]
Epoch [12/120    avg_loss:0.510, val_acc:0.858]
Epoch [13/120    avg_loss:0.420, val_acc:0.841]
Epoch [14/120    avg_loss:0.399, val_acc:0.890]
Epoch [15/120    avg_loss:0.243, val_acc:0.905]
Epoch [16/120    avg_loss:0.218, val_acc:0.893]
Epoch [17/120    avg_loss:0.207, val_acc:0.892]
Epoch [18/120    avg_loss:0.196, val_acc:0.927]
Epoch [19/120    avg_loss:0.157, val_acc:0.886]
Epoch [20/120    avg_loss:0.168, val_acc:0.917]
Epoch [21/120    avg_loss:0.151, val_acc:0.929]
Epoch [22/120    avg_loss:0.131, val_acc:0.941]
Epoch [23/120    avg_loss:0.103, val_acc:0.935]
Epoch [24/120    avg_loss:0.127, val_acc:0.926]
Epoch [25/120    avg_loss:0.098, val_acc:0.945]
Epoch [26/120    avg_loss:0.118, val_acc:0.919]
Epoch [27/120    avg_loss:0.091, val_acc:0.939]
Epoch [28/120    avg_loss:0.085, val_acc:0.917]
Epoch [29/120    avg_loss:0.099, val_acc:0.944]
Epoch [30/120    avg_loss:0.086, val_acc:0.936]
Epoch [31/120    avg_loss:0.075, val_acc:0.954]
Epoch [32/120    avg_loss:0.054, val_acc:0.947]
Epoch [33/120    avg_loss:0.066, val_acc:0.912]
Epoch [34/120    avg_loss:0.061, val_acc:0.946]
Epoch [35/120    avg_loss:0.080, val_acc:0.943]
Epoch [36/120    avg_loss:0.064, val_acc:0.951]
Epoch [37/120    avg_loss:0.041, val_acc:0.942]
Epoch [38/120    avg_loss:0.053, val_acc:0.961]
Epoch [39/120    avg_loss:0.045, val_acc:0.957]
Epoch [40/120    avg_loss:0.039, val_acc:0.959]
Epoch [41/120    avg_loss:0.031, val_acc:0.959]
Epoch [42/120    avg_loss:0.028, val_acc:0.965]
Epoch [43/120    avg_loss:0.022, val_acc:0.962]
Epoch [44/120    avg_loss:0.019, val_acc:0.969]
Epoch [45/120    avg_loss:0.019, val_acc:0.962]
Epoch [46/120    avg_loss:0.039, val_acc:0.952]
Epoch [47/120    avg_loss:0.083, val_acc:0.963]
Epoch [48/120    avg_loss:0.034, val_acc:0.944]
Epoch [49/120    avg_loss:0.036, val_acc:0.964]
Epoch [50/120    avg_loss:0.053, val_acc:0.958]
Epoch [51/120    avg_loss:0.036, val_acc:0.967]
Epoch [52/120    avg_loss:0.050, val_acc:0.953]
Epoch [53/120    avg_loss:0.036, val_acc:0.935]
Epoch [54/120    avg_loss:0.048, val_acc:0.970]
Epoch [55/120    avg_loss:0.025, val_acc:0.975]
Epoch [56/120    avg_loss:0.021, val_acc:0.969]
Epoch [57/120    avg_loss:0.019, val_acc:0.962]
Epoch [58/120    avg_loss:0.023, val_acc:0.965]
Epoch [59/120    avg_loss:0.045, val_acc:0.942]
Epoch [60/120    avg_loss:0.036, val_acc:0.957]
Epoch [61/120    avg_loss:0.033, val_acc:0.968]
Epoch [62/120    avg_loss:0.038, val_acc:0.965]
Epoch [63/120    avg_loss:0.025, val_acc:0.961]
Epoch [64/120    avg_loss:0.020, val_acc:0.974]
Epoch [65/120    avg_loss:0.017, val_acc:0.971]
Epoch [66/120    avg_loss:0.012, val_acc:0.971]
Epoch [67/120    avg_loss:0.015, val_acc:0.967]
Epoch [68/120    avg_loss:0.016, val_acc:0.968]
Epoch [69/120    avg_loss:0.013, val_acc:0.969]
Epoch [70/120    avg_loss:0.009, val_acc:0.972]
Epoch [71/120    avg_loss:0.007, val_acc:0.974]
Epoch [72/120    avg_loss:0.008, val_acc:0.975]
Epoch [73/120    avg_loss:0.008, val_acc:0.974]
Epoch [74/120    avg_loss:0.008, val_acc:0.974]
Epoch [75/120    avg_loss:0.010, val_acc:0.972]
Epoch [76/120    avg_loss:0.009, val_acc:0.972]
Epoch [77/120    avg_loss:0.007, val_acc:0.974]
Epoch [78/120    avg_loss:0.007, val_acc:0.974]
Epoch [79/120    avg_loss:0.008, val_acc:0.972]
Epoch [80/120    avg_loss:0.008, val_acc:0.976]
Epoch [81/120    avg_loss:0.008, val_acc:0.975]
Epoch [82/120    avg_loss:0.007, val_acc:0.976]
Epoch [83/120    avg_loss:0.007, val_acc:0.975]
Epoch [84/120    avg_loss:0.007, val_acc:0.976]
Epoch [85/120    avg_loss:0.008, val_acc:0.976]
Epoch [86/120    avg_loss:0.007, val_acc:0.977]
Epoch [87/120    avg_loss:0.006, val_acc:0.976]
Epoch [88/120    avg_loss:0.008, val_acc:0.976]
Epoch [89/120    avg_loss:0.008, val_acc:0.976]
Epoch [90/120    avg_loss:0.007, val_acc:0.975]
Epoch [91/120    avg_loss:0.008, val_acc:0.976]
Epoch [92/120    avg_loss:0.007, val_acc:0.975]
Epoch [93/120    avg_loss:0.006, val_acc:0.976]
Epoch [94/120    avg_loss:0.005, val_acc:0.976]
Epoch [95/120    avg_loss:0.008, val_acc:0.970]
Epoch [96/120    avg_loss:0.007, val_acc:0.974]
Epoch [97/120    avg_loss:0.007, val_acc:0.975]
Epoch [98/120    avg_loss:0.006, val_acc:0.975]
Epoch [99/120    avg_loss:0.006, val_acc:0.975]
Epoch [100/120    avg_loss:0.005, val_acc:0.975]
Epoch [101/120    avg_loss:0.007, val_acc:0.974]
Epoch [102/120    avg_loss:0.009, val_acc:0.974]
Epoch [103/120    avg_loss:0.005, val_acc:0.974]
Epoch [104/120    avg_loss:0.006, val_acc:0.974]
Epoch [105/120    avg_loss:0.005, val_acc:0.974]
Epoch [106/120    avg_loss:0.005, val_acc:0.974]
Epoch [107/120    avg_loss:0.005, val_acc:0.974]
Epoch [108/120    avg_loss:0.008, val_acc:0.974]
Epoch [109/120    avg_loss:0.005, val_acc:0.974]
Epoch [110/120    avg_loss:0.005, val_acc:0.974]
Epoch [111/120    avg_loss:0.005, val_acc:0.974]
Epoch [112/120    avg_loss:0.006, val_acc:0.974]
Epoch [113/120    avg_loss:0.005, val_acc:0.974]
Epoch [114/120    avg_loss:0.005, val_acc:0.974]
Epoch [115/120    avg_loss:0.006, val_acc:0.974]
Epoch [116/120    avg_loss:0.005, val_acc:0.974]
Epoch [117/120    avg_loss:0.005, val_acc:0.974]
Epoch [118/120    avg_loss:0.007, val_acc:0.974]
Epoch [119/120    avg_loss:0.004, val_acc:0.974]
Epoch [120/120    avg_loss:0.005, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1268    3    0    0    0    0    0    3    6    5    0    0
     0    0    0]
 [   0    0    0  727    0    2    0    0    0    3    1    0   12    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    1    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    3    2    0    0    0  862    5    0    0
     0    2    0]
 [   0    0    8    0    0    0    3    0    0    1    1 2193    4    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    4    0  524    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   12    0    0    0    0    0    0    0    0
  1119    8    0]
 [   0    0    0    0    0    1   13    0    0    0    0    0    0    0
    67  266    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.99457994579946

F1 scores:
[       nan 1.         0.98985168 0.98442789 1.         0.97412823
 0.98345865 1.         0.99883586 0.8372093  0.98570612 0.99320652
 0.9739777  0.99462366 0.96216681 0.85393258 0.97647059]

Kappa:
0.9771300715963268
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe5e65cc710>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.253, val_acc:0.570]
Epoch [2/120    avg_loss:1.689, val_acc:0.589]
Epoch [3/120    avg_loss:1.364, val_acc:0.697]
Epoch [4/120    avg_loss:1.118, val_acc:0.708]
Epoch [5/120    avg_loss:0.871, val_acc:0.745]
Epoch [6/120    avg_loss:0.872, val_acc:0.757]
Epoch [7/120    avg_loss:0.676, val_acc:0.827]
Epoch [8/120    avg_loss:0.604, val_acc:0.788]
Epoch [9/120    avg_loss:0.594, val_acc:0.816]
Epoch [10/120    avg_loss:0.408, val_acc:0.912]
Epoch [11/120    avg_loss:0.333, val_acc:0.869]
Epoch [12/120    avg_loss:0.290, val_acc:0.856]
Epoch [13/120    avg_loss:0.277, val_acc:0.906]
Epoch [14/120    avg_loss:0.222, val_acc:0.910]
Epoch [15/120    avg_loss:0.211, val_acc:0.915]
Epoch [16/120    avg_loss:0.189, val_acc:0.906]
Epoch [17/120    avg_loss:0.184, val_acc:0.941]
Epoch [18/120    avg_loss:0.137, val_acc:0.908]
Epoch [19/120    avg_loss:0.106, val_acc:0.950]
Epoch [20/120    avg_loss:0.103, val_acc:0.908]
Epoch [21/120    avg_loss:0.098, val_acc:0.952]
Epoch [22/120    avg_loss:0.108, val_acc:0.958]
Epoch [23/120    avg_loss:0.074, val_acc:0.911]
Epoch [24/120    avg_loss:0.152, val_acc:0.938]
Epoch [25/120    avg_loss:0.106, val_acc:0.959]
Epoch [26/120    avg_loss:0.081, val_acc:0.949]
Epoch [27/120    avg_loss:0.073, val_acc:0.959]
Epoch [28/120    avg_loss:0.053, val_acc:0.963]
Epoch [29/120    avg_loss:0.095, val_acc:0.947]
Epoch [30/120    avg_loss:0.067, val_acc:0.951]
Epoch [31/120    avg_loss:0.054, val_acc:0.965]
Epoch [32/120    avg_loss:0.081, val_acc:0.923]
Epoch [33/120    avg_loss:0.086, val_acc:0.957]
Epoch [34/120    avg_loss:0.103, val_acc:0.961]
Epoch [35/120    avg_loss:0.055, val_acc:0.958]
Epoch [36/120    avg_loss:0.058, val_acc:0.968]
Epoch [37/120    avg_loss:0.043, val_acc:0.972]
Epoch [38/120    avg_loss:0.032, val_acc:0.974]
Epoch [39/120    avg_loss:0.024, val_acc:0.972]
Epoch [40/120    avg_loss:0.023, val_acc:0.977]
Epoch [41/120    avg_loss:0.027, val_acc:0.969]
Epoch [42/120    avg_loss:0.025, val_acc:0.968]
Epoch [43/120    avg_loss:0.029, val_acc:0.970]
Epoch [44/120    avg_loss:0.019, val_acc:0.974]
Epoch [45/120    avg_loss:0.025, val_acc:0.965]
Epoch [46/120    avg_loss:0.017, val_acc:0.982]
Epoch [47/120    avg_loss:0.015, val_acc:0.976]
Epoch [48/120    avg_loss:0.023, val_acc:0.979]
Epoch [49/120    avg_loss:0.015, val_acc:0.972]
Epoch [50/120    avg_loss:0.020, val_acc:0.980]
Epoch [51/120    avg_loss:0.019, val_acc:0.978]
Epoch [52/120    avg_loss:0.015, val_acc:0.978]
Epoch [53/120    avg_loss:0.014, val_acc:0.980]
Epoch [54/120    avg_loss:0.007, val_acc:0.981]
Epoch [55/120    avg_loss:0.012, val_acc:0.975]
Epoch [56/120    avg_loss:0.021, val_acc:0.984]
Epoch [57/120    avg_loss:0.010, val_acc:0.982]
Epoch [58/120    avg_loss:0.006, val_acc:0.986]
Epoch [59/120    avg_loss:0.015, val_acc:0.980]
Epoch [60/120    avg_loss:0.014, val_acc:0.978]
Epoch [61/120    avg_loss:0.013, val_acc:0.980]
Epoch [62/120    avg_loss:0.007, val_acc:0.979]
Epoch [63/120    avg_loss:0.009, val_acc:0.982]
Epoch [64/120    avg_loss:0.005, val_acc:0.982]
Epoch [65/120    avg_loss:0.010, val_acc:0.980]
Epoch [66/120    avg_loss:0.008, val_acc:0.983]
Epoch [67/120    avg_loss:0.006, val_acc:0.975]
Epoch [68/120    avg_loss:0.005, val_acc:0.979]
Epoch [69/120    avg_loss:0.006, val_acc:0.985]
Epoch [70/120    avg_loss:0.004, val_acc:0.980]
Epoch [71/120    avg_loss:0.004, val_acc:0.982]
Epoch [72/120    avg_loss:0.004, val_acc:0.981]
Epoch [73/120    avg_loss:0.005, val_acc:0.982]
Epoch [74/120    avg_loss:0.006, val_acc:0.979]
Epoch [75/120    avg_loss:0.003, val_acc:0.980]
Epoch [76/120    avg_loss:0.004, val_acc:0.981]
Epoch [77/120    avg_loss:0.003, val_acc:0.983]
Epoch [78/120    avg_loss:0.005, val_acc:0.982]
Epoch [79/120    avg_loss:0.003, val_acc:0.983]
Epoch [80/120    avg_loss:0.003, val_acc:0.983]
Epoch [81/120    avg_loss:0.005, val_acc:0.984]
Epoch [82/120    avg_loss:0.004, val_acc:0.982]
Epoch [83/120    avg_loss:0.005, val_acc:0.982]
Epoch [84/120    avg_loss:0.003, val_acc:0.983]
Epoch [85/120    avg_loss:0.003, val_acc:0.983]
Epoch [86/120    avg_loss:0.004, val_acc:0.983]
Epoch [87/120    avg_loss:0.003, val_acc:0.983]
Epoch [88/120    avg_loss:0.004, val_acc:0.983]
Epoch [89/120    avg_loss:0.003, val_acc:0.983]
Epoch [90/120    avg_loss:0.004, val_acc:0.983]
Epoch [91/120    avg_loss:0.004, val_acc:0.983]
Epoch [92/120    avg_loss:0.005, val_acc:0.983]
Epoch [93/120    avg_loss:0.003, val_acc:0.983]
Epoch [94/120    avg_loss:0.004, val_acc:0.983]
Epoch [95/120    avg_loss:0.003, val_acc:0.983]
Epoch [96/120    avg_loss:0.003, val_acc:0.983]
Epoch [97/120    avg_loss:0.003, val_acc:0.983]
Epoch [98/120    avg_loss:0.003, val_acc:0.983]
Epoch [99/120    avg_loss:0.003, val_acc:0.983]
Epoch [100/120    avg_loss:0.003, val_acc:0.983]
Epoch [101/120    avg_loss:0.004, val_acc:0.983]
Epoch [102/120    avg_loss:0.003, val_acc:0.983]
Epoch [103/120    avg_loss:0.003, val_acc:0.983]
Epoch [104/120    avg_loss:0.003, val_acc:0.983]
Epoch [105/120    avg_loss:0.004, val_acc:0.983]
Epoch [106/120    avg_loss:0.004, val_acc:0.983]
Epoch [107/120    avg_loss:0.004, val_acc:0.983]
Epoch [108/120    avg_loss:0.003, val_acc:0.983]
Epoch [109/120    avg_loss:0.005, val_acc:0.983]
Epoch [110/120    avg_loss:0.005, val_acc:0.983]
Epoch [111/120    avg_loss:0.003, val_acc:0.983]
Epoch [112/120    avg_loss:0.004, val_acc:0.983]
Epoch [113/120    avg_loss:0.004, val_acc:0.983]
Epoch [114/120    avg_loss:0.004, val_acc:0.983]
Epoch [115/120    avg_loss:0.004, val_acc:0.983]
Epoch [116/120    avg_loss:0.005, val_acc:0.983]
Epoch [117/120    avg_loss:0.003, val_acc:0.983]
Epoch [118/120    avg_loss:0.005, val_acc:0.983]
Epoch [119/120    avg_loss:0.005, val_acc:0.983]
Epoch [120/120    avg_loss:0.004, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1271    0    0    1    0    0    0    0    7    5    1    0
     0    0    0]
 [   0    0    0  729    2    0    0    0    0    2    1    0   11    2
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    2    4    0    0    0  858    9    0    0
     1    0    0]
 [   0    0    8    0    0    0    3    0    0    0    2 2197    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    2    0  527    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    2    0    0    0    1    0    0    0
  1130    0    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    63  275    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.2439024390244

F1 scores:
[       nan 0.98765432 0.99064692 0.9864682  0.99061033 0.98293515
 0.98496241 0.98039216 0.997669   0.9        0.98281787 0.99344336
 0.97864438 0.99462366 0.96870982 0.88282504 0.98203593]

Kappa:
0.9799674173115589
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efeb95696a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.327, val_acc:0.480]
Epoch [2/120    avg_loss:1.649, val_acc:0.527]
Epoch [3/120    avg_loss:1.386, val_acc:0.647]
Epoch [4/120    avg_loss:1.209, val_acc:0.651]
Epoch [5/120    avg_loss:0.955, val_acc:0.662]
Epoch [6/120    avg_loss:0.970, val_acc:0.700]
Epoch [7/120    avg_loss:0.767, val_acc:0.762]
Epoch [8/120    avg_loss:0.625, val_acc:0.776]
Epoch [9/120    avg_loss:0.537, val_acc:0.849]
Epoch [10/120    avg_loss:0.469, val_acc:0.722]
Epoch [11/120    avg_loss:0.447, val_acc:0.832]
Epoch [12/120    avg_loss:0.463, val_acc:0.812]
Epoch [13/120    avg_loss:0.398, val_acc:0.860]
Epoch [14/120    avg_loss:0.330, val_acc:0.835]
Epoch [15/120    avg_loss:0.299, val_acc:0.904]
Epoch [16/120    avg_loss:0.250, val_acc:0.893]
Epoch [17/120    avg_loss:0.190, val_acc:0.936]
Epoch [18/120    avg_loss:0.175, val_acc:0.933]
Epoch [19/120    avg_loss:0.141, val_acc:0.950]
Epoch [20/120    avg_loss:0.140, val_acc:0.957]
Epoch [21/120    avg_loss:0.103, val_acc:0.925]
Epoch [22/120    avg_loss:0.143, val_acc:0.934]
Epoch [23/120    avg_loss:0.163, val_acc:0.911]
Epoch [24/120    avg_loss:0.112, val_acc:0.954]
Epoch [25/120    avg_loss:0.101, val_acc:0.943]
Epoch [26/120    avg_loss:0.079, val_acc:0.966]
Epoch [27/120    avg_loss:0.080, val_acc:0.971]
Epoch [28/120    avg_loss:0.102, val_acc:0.958]
Epoch [29/120    avg_loss:0.082, val_acc:0.971]
Epoch [30/120    avg_loss:0.062, val_acc:0.970]
Epoch [31/120    avg_loss:0.054, val_acc:0.968]
Epoch [32/120    avg_loss:0.062, val_acc:0.956]
Epoch [33/120    avg_loss:0.054, val_acc:0.973]
Epoch [34/120    avg_loss:0.046, val_acc:0.968]
Epoch [35/120    avg_loss:0.045, val_acc:0.974]
Epoch [36/120    avg_loss:0.070, val_acc:0.933]
Epoch [37/120    avg_loss:0.061, val_acc:0.966]
Epoch [38/120    avg_loss:0.099, val_acc:0.953]
Epoch [39/120    avg_loss:0.072, val_acc:0.930]
Epoch [40/120    avg_loss:0.059, val_acc:0.972]
Epoch [41/120    avg_loss:0.037, val_acc:0.975]
Epoch [42/120    avg_loss:0.046, val_acc:0.975]
Epoch [43/120    avg_loss:0.060, val_acc:0.945]
Epoch [44/120    avg_loss:0.047, val_acc:0.970]
Epoch [45/120    avg_loss:0.036, val_acc:0.970]
Epoch [46/120    avg_loss:0.031, val_acc:0.981]
Epoch [47/120    avg_loss:0.034, val_acc:0.966]
Epoch [48/120    avg_loss:0.023, val_acc:0.980]
Epoch [49/120    avg_loss:0.018, val_acc:0.976]
Epoch [50/120    avg_loss:0.029, val_acc:0.978]
Epoch [51/120    avg_loss:0.018, val_acc:0.976]
Epoch [52/120    avg_loss:0.016, val_acc:0.983]
Epoch [53/120    avg_loss:0.012, val_acc:0.985]
Epoch [54/120    avg_loss:0.010, val_acc:0.984]
Epoch [55/120    avg_loss:0.010, val_acc:0.985]
Epoch [56/120    avg_loss:0.018, val_acc:0.978]
Epoch [57/120    avg_loss:0.016, val_acc:0.979]
Epoch [58/120    avg_loss:0.013, val_acc:0.985]
Epoch [59/120    avg_loss:0.041, val_acc:0.955]
Epoch [60/120    avg_loss:0.134, val_acc:0.971]
Epoch [61/120    avg_loss:0.053, val_acc:0.964]
Epoch [62/120    avg_loss:0.197, val_acc:0.933]
Epoch [63/120    avg_loss:0.391, val_acc:0.887]
Epoch [64/120    avg_loss:0.123, val_acc:0.953]
Epoch [65/120    avg_loss:0.077, val_acc:0.957]
Epoch [66/120    avg_loss:0.073, val_acc:0.971]
Epoch [67/120    avg_loss:0.079, val_acc:0.933]
Epoch [68/120    avg_loss:0.125, val_acc:0.967]
Epoch [69/120    avg_loss:0.057, val_acc:0.970]
Epoch [70/120    avg_loss:0.087, val_acc:0.956]
Epoch [71/120    avg_loss:0.139, val_acc:0.931]
Epoch [72/120    avg_loss:0.074, val_acc:0.955]
Epoch [73/120    avg_loss:0.055, val_acc:0.967]
Epoch [74/120    avg_loss:0.035, val_acc:0.972]
Epoch [75/120    avg_loss:0.042, val_acc:0.976]
Epoch [76/120    avg_loss:0.029, val_acc:0.976]
Epoch [77/120    avg_loss:0.025, val_acc:0.978]
Epoch [78/120    avg_loss:0.022, val_acc:0.978]
Epoch [79/120    avg_loss:0.036, val_acc:0.980]
Epoch [80/120    avg_loss:0.030, val_acc:0.982]
Epoch [81/120    avg_loss:0.020, val_acc:0.981]
Epoch [82/120    avg_loss:0.039, val_acc:0.983]
Epoch [83/120    avg_loss:0.018, val_acc:0.981]
Epoch [84/120    avg_loss:0.021, val_acc:0.981]
Epoch [85/120    avg_loss:0.017, val_acc:0.981]
Epoch [86/120    avg_loss:0.020, val_acc:0.981]
Epoch [87/120    avg_loss:0.021, val_acc:0.981]
Epoch [88/120    avg_loss:0.019, val_acc:0.981]
Epoch [89/120    avg_loss:0.021, val_acc:0.981]
Epoch [90/120    avg_loss:0.018, val_acc:0.981]
Epoch [91/120    avg_loss:0.020, val_acc:0.981]
Epoch [92/120    avg_loss:0.023, val_acc:0.981]
Epoch [93/120    avg_loss:0.019, val_acc:0.981]
Epoch [94/120    avg_loss:0.023, val_acc:0.981]
Epoch [95/120    avg_loss:0.019, val_acc:0.981]
Epoch [96/120    avg_loss:0.020, val_acc:0.981]
Epoch [97/120    avg_loss:0.015, val_acc:0.981]
Epoch [98/120    avg_loss:0.017, val_acc:0.981]
Epoch [99/120    avg_loss:0.021, val_acc:0.981]
Epoch [100/120    avg_loss:0.020, val_acc:0.981]
Epoch [101/120    avg_loss:0.019, val_acc:0.981]
Epoch [102/120    avg_loss:0.016, val_acc:0.982]
Epoch [103/120    avg_loss:0.016, val_acc:0.981]
Epoch [104/120    avg_loss:0.018, val_acc:0.982]
Epoch [105/120    avg_loss:0.020, val_acc:0.982]
Epoch [106/120    avg_loss:0.021, val_acc:0.982]
Epoch [107/120    avg_loss:0.022, val_acc:0.982]
Epoch [108/120    avg_loss:0.015, val_acc:0.982]
Epoch [109/120    avg_loss:0.020, val_acc:0.982]
Epoch [110/120    avg_loss:0.023, val_acc:0.982]
Epoch [111/120    avg_loss:0.015, val_acc:0.982]
Epoch [112/120    avg_loss:0.016, val_acc:0.982]
Epoch [113/120    avg_loss:0.015, val_acc:0.982]
Epoch [114/120    avg_loss:0.020, val_acc:0.982]
Epoch [115/120    avg_loss:0.017, val_acc:0.982]
Epoch [116/120    avg_loss:0.017, val_acc:0.982]
Epoch [117/120    avg_loss:0.018, val_acc:0.982]
Epoch [118/120    avg_loss:0.018, val_acc:0.982]
Epoch [119/120    avg_loss:0.013, val_acc:0.982]
Epoch [120/120    avg_loss:0.019, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1262    2    0    1    0    0    0    0    5   12    0    0
     0    3    0]
 [   0    0    0  710    0    2    0    0    0    2    1    0   29    3
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    7    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    4    2    0    0    0  840   26    0    0
     0    2    0]
 [   0    0    4    0    0    0    2    0    0    0    1 2199    0    1
     0    3    0]
 [   0    0    0    2    0    4    0    0    0    0    0    1  524    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    0    0    0
  1130    6    0]
 [   0    0    0    0    0    0    9    0    0    0    1    0    0    0
    50  287    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.82113821138212

F1 scores:
[       nan 0.98765432 0.98902821 0.97127223 0.99764706 0.97940503
 0.98791541 0.87719298 0.995338   0.94736842 0.97334878 0.98853675
 0.96058662 0.98930481 0.97371822 0.88580247 0.97647059]

Kappa:
0.9751424570573705
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe05582d748>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.205, val_acc:0.605]
Epoch [2/120    avg_loss:1.688, val_acc:0.588]
Epoch [3/120    avg_loss:1.412, val_acc:0.689]
Epoch [4/120    avg_loss:1.039, val_acc:0.766]
Epoch [5/120    avg_loss:0.880, val_acc:0.815]
Epoch [6/120    avg_loss:0.732, val_acc:0.811]
Epoch [7/120    avg_loss:0.608, val_acc:0.844]
Epoch [8/120    avg_loss:0.551, val_acc:0.791]
Epoch [9/120    avg_loss:0.668, val_acc:0.850]
Epoch [10/120    avg_loss:0.433, val_acc:0.869]
Epoch [11/120    avg_loss:0.396, val_acc:0.843]
Epoch [12/120    avg_loss:0.328, val_acc:0.908]
Epoch [13/120    avg_loss:0.300, val_acc:0.877]
Epoch [14/120    avg_loss:0.345, val_acc:0.895]
Epoch [15/120    avg_loss:0.217, val_acc:0.898]
Epoch [16/120    avg_loss:0.198, val_acc:0.906]
Epoch [17/120    avg_loss:0.151, val_acc:0.944]
Epoch [18/120    avg_loss:0.161, val_acc:0.944]
Epoch [19/120    avg_loss:0.103, val_acc:0.952]
Epoch [20/120    avg_loss:0.124, val_acc:0.939]
Epoch [21/120    avg_loss:0.093, val_acc:0.954]
Epoch [22/120    avg_loss:0.084, val_acc:0.947]
Epoch [23/120    avg_loss:0.096, val_acc:0.947]
Epoch [24/120    avg_loss:0.086, val_acc:0.958]
Epoch [25/120    avg_loss:0.076, val_acc:0.965]
Epoch [26/120    avg_loss:0.060, val_acc:0.960]
Epoch [27/120    avg_loss:0.069, val_acc:0.934]
Epoch [28/120    avg_loss:0.090, val_acc:0.936]
Epoch [29/120    avg_loss:0.057, val_acc:0.965]
Epoch [30/120    avg_loss:0.057, val_acc:0.945]
Epoch [31/120    avg_loss:0.059, val_acc:0.955]
Epoch [32/120    avg_loss:0.090, val_acc:0.961]
Epoch [33/120    avg_loss:0.062, val_acc:0.965]
Epoch [34/120    avg_loss:0.058, val_acc:0.967]
Epoch [35/120    avg_loss:0.040, val_acc:0.969]
Epoch [36/120    avg_loss:0.045, val_acc:0.965]
Epoch [37/120    avg_loss:0.039, val_acc:0.977]
Epoch [38/120    avg_loss:0.038, val_acc:0.972]
Epoch [39/120    avg_loss:0.037, val_acc:0.967]
Epoch [40/120    avg_loss:0.027, val_acc:0.975]
Epoch [41/120    avg_loss:0.035, val_acc:0.978]
Epoch [42/120    avg_loss:0.031, val_acc:0.961]
Epoch [43/120    avg_loss:0.037, val_acc:0.972]
Epoch [44/120    avg_loss:0.019, val_acc:0.972]
Epoch [45/120    avg_loss:0.059, val_acc:0.965]
Epoch [46/120    avg_loss:0.030, val_acc:0.964]
Epoch [47/120    avg_loss:0.031, val_acc:0.951]
Epoch [48/120    avg_loss:0.039, val_acc:0.973]
Epoch [49/120    avg_loss:0.034, val_acc:0.967]
Epoch [50/120    avg_loss:0.022, val_acc:0.977]
Epoch [51/120    avg_loss:0.017, val_acc:0.981]
Epoch [52/120    avg_loss:0.037, val_acc:0.976]
Epoch [53/120    avg_loss:0.026, val_acc:0.969]
Epoch [54/120    avg_loss:0.019, val_acc:0.983]
Epoch [55/120    avg_loss:0.026, val_acc:0.984]
Epoch [56/120    avg_loss:0.010, val_acc:0.986]
Epoch [57/120    avg_loss:0.008, val_acc:0.990]
Epoch [58/120    avg_loss:0.008, val_acc:0.981]
Epoch [59/120    avg_loss:0.010, val_acc:0.979]
Epoch [60/120    avg_loss:0.008, val_acc:0.984]
Epoch [61/120    avg_loss:0.010, val_acc:0.982]
Epoch [62/120    avg_loss:0.011, val_acc:0.984]
Epoch [63/120    avg_loss:0.044, val_acc:0.975]
Epoch [64/120    avg_loss:0.021, val_acc:0.977]
Epoch [65/120    avg_loss:0.012, val_acc:0.978]
Epoch [66/120    avg_loss:0.020, val_acc:0.982]
Epoch [67/120    avg_loss:0.016, val_acc:0.983]
Epoch [68/120    avg_loss:0.008, val_acc:0.984]
Epoch [69/120    avg_loss:0.009, val_acc:0.988]
Epoch [70/120    avg_loss:0.005, val_acc:0.984]
Epoch [71/120    avg_loss:0.006, val_acc:0.986]
Epoch [72/120    avg_loss:0.006, val_acc:0.984]
Epoch [73/120    avg_loss:0.004, val_acc:0.984]
Epoch [74/120    avg_loss:0.005, val_acc:0.984]
Epoch [75/120    avg_loss:0.004, val_acc:0.984]
Epoch [76/120    avg_loss:0.004, val_acc:0.985]
Epoch [77/120    avg_loss:0.004, val_acc:0.986]
Epoch [78/120    avg_loss:0.007, val_acc:0.985]
Epoch [79/120    avg_loss:0.003, val_acc:0.989]
Epoch [80/120    avg_loss:0.004, val_acc:0.989]
Epoch [81/120    avg_loss:0.003, val_acc:0.989]
Epoch [82/120    avg_loss:0.003, val_acc:0.989]
Epoch [83/120    avg_loss:0.004, val_acc:0.989]
Epoch [84/120    avg_loss:0.005, val_acc:0.989]
Epoch [85/120    avg_loss:0.004, val_acc:0.989]
Epoch [86/120    avg_loss:0.003, val_acc:0.989]
Epoch [87/120    avg_loss:0.004, val_acc:0.989]
Epoch [88/120    avg_loss:0.003, val_acc:0.989]
Epoch [89/120    avg_loss:0.005, val_acc:0.989]
Epoch [90/120    avg_loss:0.004, val_acc:0.989]
Epoch [91/120    avg_loss:0.003, val_acc:0.989]
Epoch [92/120    avg_loss:0.004, val_acc:0.989]
Epoch [93/120    avg_loss:0.004, val_acc:0.989]
Epoch [94/120    avg_loss:0.004, val_acc:0.988]
Epoch [95/120    avg_loss:0.003, val_acc:0.988]
Epoch [96/120    avg_loss:0.003, val_acc:0.986]
Epoch [97/120    avg_loss:0.003, val_acc:0.986]
Epoch [98/120    avg_loss:0.005, val_acc:0.986]
Epoch [99/120    avg_loss:0.003, val_acc:0.986]
Epoch [100/120    avg_loss:0.004, val_acc:0.986]
Epoch [101/120    avg_loss:0.003, val_acc:0.986]
Epoch [102/120    avg_loss:0.006, val_acc:0.986]
Epoch [103/120    avg_loss:0.003, val_acc:0.986]
Epoch [104/120    avg_loss:0.003, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.986]
Epoch [106/120    avg_loss:0.004, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.004, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.986]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.004, val_acc:0.986]
Epoch [112/120    avg_loss:0.005, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.005, val_acc:0.986]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.004, val_acc:0.986]
Epoch [118/120    avg_loss:0.005, val_acc:0.986]
Epoch [119/120    avg_loss:0.003, val_acc:0.986]
Epoch [120/120    avg_loss:0.004, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1272    1    3    0    0    0    0    0    5    3    1    0
     0    0    0]
 [   0    0    0  718    0    0    0    0    0    4    1    7   15    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    4    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    1    0    0    0    4  853   12    0    0
     1    1    0]
 [   0    0    6    0    0    0    0    0    0    1    1 2193    7    1
     0    1    0]
 [   0    0    0    0    0    1    0    0    0    0    4    0  527    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    32  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.31978319783198

F1 scores:
[       nan 0.98765432 0.99142634 0.97953615 0.99300699 0.98842593
 0.99020347 0.92592593 0.997669   0.8        0.98045977 0.99118644
 0.97053407 0.9919571  0.97916667 0.91238671 0.98823529]

Kappa:
0.9808411743773567
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7faf9e483710>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.288, val_acc:0.457]
Epoch [2/120    avg_loss:1.661, val_acc:0.645]
Epoch [3/120    avg_loss:1.324, val_acc:0.661]
Epoch [4/120    avg_loss:1.021, val_acc:0.681]
Epoch [5/120    avg_loss:0.851, val_acc:0.745]
Epoch [6/120    avg_loss:0.753, val_acc:0.742]
Epoch [7/120    avg_loss:0.662, val_acc:0.768]
Epoch [8/120    avg_loss:0.629, val_acc:0.822]
Epoch [9/120    avg_loss:0.492, val_acc:0.803]
Epoch [10/120    avg_loss:0.451, val_acc:0.863]
Epoch [11/120    avg_loss:0.435, val_acc:0.838]
Epoch [12/120    avg_loss:0.368, val_acc:0.838]
Epoch [13/120    avg_loss:0.336, val_acc:0.889]
Epoch [14/120    avg_loss:0.260, val_acc:0.858]
Epoch [15/120    avg_loss:0.307, val_acc:0.890]
Epoch [16/120    avg_loss:0.228, val_acc:0.896]
Epoch [17/120    avg_loss:0.285, val_acc:0.922]
Epoch [18/120    avg_loss:0.180, val_acc:0.919]
Epoch [19/120    avg_loss:0.137, val_acc:0.931]
Epoch [20/120    avg_loss:0.126, val_acc:0.947]
Epoch [21/120    avg_loss:0.124, val_acc:0.952]
Epoch [22/120    avg_loss:0.108, val_acc:0.957]
Epoch [23/120    avg_loss:0.083, val_acc:0.958]
Epoch [24/120    avg_loss:0.082, val_acc:0.959]
Epoch [25/120    avg_loss:0.065, val_acc:0.953]
Epoch [26/120    avg_loss:0.075, val_acc:0.953]
Epoch [27/120    avg_loss:0.064, val_acc:0.964]
Epoch [28/120    avg_loss:0.055, val_acc:0.929]
Epoch [29/120    avg_loss:0.058, val_acc:0.969]
Epoch [30/120    avg_loss:0.069, val_acc:0.974]
Epoch [31/120    avg_loss:0.064, val_acc:0.958]
Epoch [32/120    avg_loss:0.062, val_acc:0.974]
Epoch [33/120    avg_loss:0.044, val_acc:0.950]
Epoch [34/120    avg_loss:0.049, val_acc:0.936]
Epoch [35/120    avg_loss:0.058, val_acc:0.976]
Epoch [36/120    avg_loss:0.099, val_acc:0.951]
Epoch [37/120    avg_loss:0.105, val_acc:0.941]
Epoch [38/120    avg_loss:0.644, val_acc:0.811]
Epoch [39/120    avg_loss:0.309, val_acc:0.941]
Epoch [40/120    avg_loss:0.151, val_acc:0.939]
Epoch [41/120    avg_loss:0.101, val_acc:0.941]
Epoch [42/120    avg_loss:0.073, val_acc:0.946]
Epoch [43/120    avg_loss:0.072, val_acc:0.966]
Epoch [44/120    avg_loss:0.066, val_acc:0.970]
Epoch [45/120    avg_loss:0.046, val_acc:0.975]
Epoch [46/120    avg_loss:0.035, val_acc:0.981]
Epoch [47/120    avg_loss:0.034, val_acc:0.984]
Epoch [48/120    avg_loss:0.028, val_acc:0.980]
Epoch [49/120    avg_loss:0.019, val_acc:0.982]
Epoch [50/120    avg_loss:0.016, val_acc:0.979]
Epoch [51/120    avg_loss:0.014, val_acc:0.982]
Epoch [52/120    avg_loss:0.020, val_acc:0.977]
Epoch [53/120    avg_loss:0.019, val_acc:0.980]
Epoch [54/120    avg_loss:0.016, val_acc:0.981]
Epoch [55/120    avg_loss:0.015, val_acc:0.982]
Epoch [56/120    avg_loss:0.016, val_acc:0.974]
Epoch [57/120    avg_loss:0.019, val_acc:0.967]
Epoch [58/120    avg_loss:0.025, val_acc:0.980]
Epoch [59/120    avg_loss:0.025, val_acc:0.976]
Epoch [60/120    avg_loss:0.024, val_acc:0.967]
Epoch [61/120    avg_loss:0.016, val_acc:0.982]
Epoch [62/120    avg_loss:0.010, val_acc:0.983]
Epoch [63/120    avg_loss:0.009, val_acc:0.986]
Epoch [64/120    avg_loss:0.009, val_acc:0.987]
Epoch [65/120    avg_loss:0.008, val_acc:0.987]
Epoch [66/120    avg_loss:0.006, val_acc:0.987]
Epoch [67/120    avg_loss:0.010, val_acc:0.985]
Epoch [68/120    avg_loss:0.008, val_acc:0.986]
Epoch [69/120    avg_loss:0.007, val_acc:0.986]
Epoch [70/120    avg_loss:0.008, val_acc:0.986]
Epoch [71/120    avg_loss:0.009, val_acc:0.985]
Epoch [72/120    avg_loss:0.008, val_acc:0.985]
Epoch [73/120    avg_loss:0.006, val_acc:0.987]
Epoch [74/120    avg_loss:0.009, val_acc:0.986]
Epoch [75/120    avg_loss:0.006, val_acc:0.986]
Epoch [76/120    avg_loss:0.006, val_acc:0.987]
Epoch [77/120    avg_loss:0.006, val_acc:0.989]
Epoch [78/120    avg_loss:0.006, val_acc:0.988]
Epoch [79/120    avg_loss:0.006, val_acc:0.988]
Epoch [80/120    avg_loss:0.005, val_acc:0.987]
Epoch [81/120    avg_loss:0.007, val_acc:0.989]
Epoch [82/120    avg_loss:0.007, val_acc:0.988]
Epoch [83/120    avg_loss:0.007, val_acc:0.989]
Epoch [84/120    avg_loss:0.008, val_acc:0.988]
Epoch [85/120    avg_loss:0.007, val_acc:0.988]
Epoch [86/120    avg_loss:0.008, val_acc:0.990]
Epoch [87/120    avg_loss:0.006, val_acc:0.990]
Epoch [88/120    avg_loss:0.006, val_acc:0.988]
Epoch [89/120    avg_loss:0.006, val_acc:0.988]
Epoch [90/120    avg_loss:0.006, val_acc:0.989]
Epoch [91/120    avg_loss:0.007, val_acc:0.988]
Epoch [92/120    avg_loss:0.006, val_acc:0.989]
Epoch [93/120    avg_loss:0.006, val_acc:0.990]
Epoch [94/120    avg_loss:0.005, val_acc:0.989]
Epoch [95/120    avg_loss:0.006, val_acc:0.990]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.006, val_acc:0.989]
Epoch [98/120    avg_loss:0.005, val_acc:0.989]
Epoch [99/120    avg_loss:0.007, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.006, val_acc:0.989]
Epoch [103/120    avg_loss:0.007, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.988]
Epoch [105/120    avg_loss:0.006, val_acc:0.988]
Epoch [106/120    avg_loss:0.004, val_acc:0.988]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.005, val_acc:0.988]
Epoch [109/120    avg_loss:0.006, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.988]
Epoch [111/120    avg_loss:0.004, val_acc:0.988]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.005, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.004, val_acc:0.988]
Epoch [120/120    avg_loss:0.005, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1265    0    0    4    0    0    0    0    8    7    0    0
     0    1    0]
 [   0    0    0  727    1    0    1    0    0    3    0    0   13    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    1    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    2    0    0    0    1  846   13    3    0
     2    3    0]
 [   0    0    5    0    0    0    1    0    0    1    0 2178   24    1
     0    0    0]
 [   0    0    0    2    0    3    0    0    0    0    5    1  522    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1125   12    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    41  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.00542005420054

F1 scores:
[       nan 1.         0.98828125 0.98509485 0.99765808 0.98861048
 0.98722765 1.         0.99883856 0.87804878 0.97521614 0.98797913
 0.95168642 0.9919571  0.97529259 0.89160305 0.98809524]

Kappa:
0.9772638794813625
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbe5dbe06d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.252, val_acc:0.511]
Epoch [2/120    avg_loss:1.706, val_acc:0.508]
Epoch [3/120    avg_loss:1.380, val_acc:0.627]
Epoch [4/120    avg_loss:1.131, val_acc:0.718]
Epoch [5/120    avg_loss:0.902, val_acc:0.740]
Epoch [6/120    avg_loss:0.790, val_acc:0.729]
Epoch [7/120    avg_loss:0.704, val_acc:0.824]
Epoch [8/120    avg_loss:0.672, val_acc:0.824]
Epoch [9/120    avg_loss:0.547, val_acc:0.822]
Epoch [10/120    avg_loss:0.534, val_acc:0.843]
Epoch [11/120    avg_loss:0.541, val_acc:0.814]
Epoch [12/120    avg_loss:0.374, val_acc:0.854]
Epoch [13/120    avg_loss:0.372, val_acc:0.817]
Epoch [14/120    avg_loss:0.318, val_acc:0.900]
Epoch [15/120    avg_loss:0.345, val_acc:0.874]
Epoch [16/120    avg_loss:0.358, val_acc:0.862]
Epoch [17/120    avg_loss:0.212, val_acc:0.906]
Epoch [18/120    avg_loss:0.184, val_acc:0.922]
Epoch [19/120    avg_loss:0.167, val_acc:0.933]
Epoch [20/120    avg_loss:0.131, val_acc:0.929]
Epoch [21/120    avg_loss:0.141, val_acc:0.905]
Epoch [22/120    avg_loss:0.143, val_acc:0.953]
Epoch [23/120    avg_loss:0.170, val_acc:0.934]
Epoch [24/120    avg_loss:0.144, val_acc:0.929]
Epoch [25/120    avg_loss:0.086, val_acc:0.951]
Epoch [26/120    avg_loss:0.085, val_acc:0.947]
Epoch [27/120    avg_loss:0.076, val_acc:0.923]
Epoch [28/120    avg_loss:0.104, val_acc:0.944]
Epoch [29/120    avg_loss:0.088, val_acc:0.934]
Epoch [30/120    avg_loss:0.081, val_acc:0.966]
Epoch [31/120    avg_loss:0.070, val_acc:0.871]
Epoch [32/120    avg_loss:0.117, val_acc:0.963]
Epoch [33/120    avg_loss:0.065, val_acc:0.940]
Epoch [34/120    avg_loss:0.115, val_acc:0.954]
Epoch [35/120    avg_loss:0.069, val_acc:0.974]
Epoch [36/120    avg_loss:0.044, val_acc:0.961]
Epoch [37/120    avg_loss:0.050, val_acc:0.967]
Epoch [38/120    avg_loss:0.029, val_acc:0.967]
Epoch [39/120    avg_loss:0.424, val_acc:0.898]
Epoch [40/120    avg_loss:0.125, val_acc:0.958]
Epoch [41/120    avg_loss:0.079, val_acc:0.969]
Epoch [42/120    avg_loss:0.092, val_acc:0.956]
Epoch [43/120    avg_loss:0.054, val_acc:0.958]
Epoch [44/120    avg_loss:0.059, val_acc:0.960]
Epoch [45/120    avg_loss:0.083, val_acc:0.956]
Epoch [46/120    avg_loss:0.063, val_acc:0.960]
Epoch [47/120    avg_loss:0.050, val_acc:0.959]
Epoch [48/120    avg_loss:0.036, val_acc:0.947]
Epoch [49/120    avg_loss:0.066, val_acc:0.972]
Epoch [50/120    avg_loss:0.029, val_acc:0.970]
Epoch [51/120    avg_loss:0.027, val_acc:0.972]
Epoch [52/120    avg_loss:0.024, val_acc:0.971]
Epoch [53/120    avg_loss:0.021, val_acc:0.974]
Epoch [54/120    avg_loss:0.017, val_acc:0.975]
Epoch [55/120    avg_loss:0.021, val_acc:0.979]
Epoch [56/120    avg_loss:0.021, val_acc:0.980]
Epoch [57/120    avg_loss:0.019, val_acc:0.979]
Epoch [58/120    avg_loss:0.017, val_acc:0.981]
Epoch [59/120    avg_loss:0.016, val_acc:0.980]
Epoch [60/120    avg_loss:0.028, val_acc:0.982]
Epoch [61/120    avg_loss:0.019, val_acc:0.981]
Epoch [62/120    avg_loss:0.016, val_acc:0.982]
Epoch [63/120    avg_loss:0.022, val_acc:0.980]
Epoch [64/120    avg_loss:0.015, val_acc:0.982]
Epoch [65/120    avg_loss:0.021, val_acc:0.985]
Epoch [66/120    avg_loss:0.023, val_acc:0.983]
Epoch [67/120    avg_loss:0.016, val_acc:0.984]
Epoch [68/120    avg_loss:0.013, val_acc:0.984]
Epoch [69/120    avg_loss:0.012, val_acc:0.984]
Epoch [70/120    avg_loss:0.021, val_acc:0.985]
Epoch [71/120    avg_loss:0.019, val_acc:0.985]
Epoch [72/120    avg_loss:0.017, val_acc:0.986]
Epoch [73/120    avg_loss:0.013, val_acc:0.985]
Epoch [74/120    avg_loss:0.015, val_acc:0.986]
Epoch [75/120    avg_loss:0.014, val_acc:0.981]
Epoch [76/120    avg_loss:0.017, val_acc:0.981]
Epoch [77/120    avg_loss:0.014, val_acc:0.982]
Epoch [78/120    avg_loss:0.016, val_acc:0.985]
Epoch [79/120    avg_loss:0.015, val_acc:0.983]
Epoch [80/120    avg_loss:0.012, val_acc:0.984]
Epoch [81/120    avg_loss:0.015, val_acc:0.985]
Epoch [82/120    avg_loss:0.017, val_acc:0.984]
Epoch [83/120    avg_loss:0.012, val_acc:0.989]
Epoch [84/120    avg_loss:0.009, val_acc:0.986]
Epoch [85/120    avg_loss:0.013, val_acc:0.985]
Epoch [86/120    avg_loss:0.013, val_acc:0.984]
Epoch [87/120    avg_loss:0.011, val_acc:0.985]
Epoch [88/120    avg_loss:0.013, val_acc:0.984]
Epoch [89/120    avg_loss:0.015, val_acc:0.985]
Epoch [90/120    avg_loss:0.012, val_acc:0.985]
Epoch [91/120    avg_loss:0.013, val_acc:0.985]
Epoch [92/120    avg_loss:0.014, val_acc:0.986]
Epoch [93/120    avg_loss:0.014, val_acc:0.985]
Epoch [94/120    avg_loss:0.012, val_acc:0.986]
Epoch [95/120    avg_loss:0.011, val_acc:0.985]
Epoch [96/120    avg_loss:0.014, val_acc:0.985]
Epoch [97/120    avg_loss:0.011, val_acc:0.985]
Epoch [98/120    avg_loss:0.014, val_acc:0.985]
Epoch [99/120    avg_loss:0.012, val_acc:0.985]
Epoch [100/120    avg_loss:0.013, val_acc:0.985]
Epoch [101/120    avg_loss:0.012, val_acc:0.985]
Epoch [102/120    avg_loss:0.012, val_acc:0.985]
Epoch [103/120    avg_loss:0.013, val_acc:0.985]
Epoch [104/120    avg_loss:0.011, val_acc:0.985]
Epoch [105/120    avg_loss:0.011, val_acc:0.985]
Epoch [106/120    avg_loss:0.012, val_acc:0.985]
Epoch [107/120    avg_loss:0.012, val_acc:0.984]
Epoch [108/120    avg_loss:0.011, val_acc:0.985]
Epoch [109/120    avg_loss:0.011, val_acc:0.985]
Epoch [110/120    avg_loss:0.013, val_acc:0.985]
Epoch [111/120    avg_loss:0.014, val_acc:0.985]
Epoch [112/120    avg_loss:0.011, val_acc:0.985]
Epoch [113/120    avg_loss:0.013, val_acc:0.985]
Epoch [114/120    avg_loss:0.011, val_acc:0.985]
Epoch [115/120    avg_loss:0.010, val_acc:0.985]
Epoch [116/120    avg_loss:0.015, val_acc:0.985]
Epoch [117/120    avg_loss:0.012, val_acc:0.985]
Epoch [118/120    avg_loss:0.012, val_acc:0.985]
Epoch [119/120    avg_loss:0.011, val_acc:0.985]
Epoch [120/120    avg_loss:0.010, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1254    0    0    0    1    0    0    1    5   12   10    0
     0    2    0]
 [   0    0    0  731    0    0    0    0    0    6    1    6    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    2    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    1    2    0    0    0  856    7    0    0
     0    3    0]
 [   0    0    4    0    0    0    1    0    0    3    2 2194    1    2
     1    2    0]
 [   0    0    0    1    0    1    0    0    0    0    0    0  530    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1132    6    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    32  315    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.51490514905149

F1 scores:
[       nan 0.98765432 0.98391526 0.98850575 1.         0.99308756
 0.99467681 0.96153846 0.99649942 0.76595745 0.98334291 0.99029564
 0.97966728 0.99462366 0.98178664 0.93195266 0.98809524]

Kappa:
0.9830662016284021
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f91f2ba3748>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.311, val_acc:0.478]
Epoch [2/120    avg_loss:1.704, val_acc:0.463]
Epoch [3/120    avg_loss:1.431, val_acc:0.693]
Epoch [4/120    avg_loss:1.120, val_acc:0.743]
Epoch [5/120    avg_loss:0.908, val_acc:0.747]
Epoch [6/120    avg_loss:0.762, val_acc:0.786]
Epoch [7/120    avg_loss:0.663, val_acc:0.722]
Epoch [8/120    avg_loss:0.564, val_acc:0.793]
Epoch [9/120    avg_loss:0.499, val_acc:0.815]
Epoch [10/120    avg_loss:0.414, val_acc:0.871]
Epoch [11/120    avg_loss:0.315, val_acc:0.860]
Epoch [12/120    avg_loss:0.344, val_acc:0.868]
Epoch [13/120    avg_loss:0.405, val_acc:0.879]
Epoch [14/120    avg_loss:0.345, val_acc:0.847]
Epoch [15/120    avg_loss:0.282, val_acc:0.859]
Epoch [16/120    avg_loss:0.257, val_acc:0.895]
Epoch [17/120    avg_loss:0.216, val_acc:0.907]
Epoch [18/120    avg_loss:0.152, val_acc:0.903]
Epoch [19/120    avg_loss:0.176, val_acc:0.925]
Epoch [20/120    avg_loss:0.143, val_acc:0.931]
Epoch [21/120    avg_loss:0.142, val_acc:0.923]
Epoch [22/120    avg_loss:0.124, val_acc:0.946]
Epoch [23/120    avg_loss:0.112, val_acc:0.925]
Epoch [24/120    avg_loss:0.093, val_acc:0.947]
Epoch [25/120    avg_loss:0.086, val_acc:0.945]
Epoch [26/120    avg_loss:0.095, val_acc:0.926]
Epoch [27/120    avg_loss:0.081, val_acc:0.945]
Epoch [28/120    avg_loss:0.092, val_acc:0.942]
Epoch [29/120    avg_loss:0.061, val_acc:0.949]
Epoch [30/120    avg_loss:0.047, val_acc:0.956]
Epoch [31/120    avg_loss:0.041, val_acc:0.964]
Epoch [32/120    avg_loss:0.061, val_acc:0.947]
Epoch [33/120    avg_loss:0.068, val_acc:0.938]
Epoch [34/120    avg_loss:0.066, val_acc:0.943]
Epoch [35/120    avg_loss:0.068, val_acc:0.970]
Epoch [36/120    avg_loss:0.045, val_acc:0.951]
Epoch [37/120    avg_loss:0.041, val_acc:0.965]
Epoch [38/120    avg_loss:0.034, val_acc:0.974]
Epoch [39/120    avg_loss:0.027, val_acc:0.967]
Epoch [40/120    avg_loss:0.034, val_acc:0.971]
Epoch [41/120    avg_loss:0.027, val_acc:0.963]
Epoch [42/120    avg_loss:0.027, val_acc:0.976]
Epoch [43/120    avg_loss:0.042, val_acc:0.962]
Epoch [44/120    avg_loss:0.027, val_acc:0.972]
Epoch [45/120    avg_loss:0.029, val_acc:0.978]
Epoch [46/120    avg_loss:0.037, val_acc:0.974]
Epoch [47/120    avg_loss:0.032, val_acc:0.980]
Epoch [48/120    avg_loss:0.028, val_acc:0.965]
Epoch [49/120    avg_loss:0.023, val_acc:0.975]
Epoch [50/120    avg_loss:0.018, val_acc:0.983]
Epoch [51/120    avg_loss:0.019, val_acc:0.976]
Epoch [52/120    avg_loss:0.022, val_acc:0.974]
Epoch [53/120    avg_loss:0.020, val_acc:0.978]
Epoch [54/120    avg_loss:0.020, val_acc:0.960]
Epoch [55/120    avg_loss:0.027, val_acc:0.976]
Epoch [56/120    avg_loss:0.013, val_acc:0.979]
Epoch [57/120    avg_loss:0.014, val_acc:0.984]
Epoch [58/120    avg_loss:0.012, val_acc:0.977]
Epoch [59/120    avg_loss:0.017, val_acc:0.969]
Epoch [60/120    avg_loss:0.024, val_acc:0.978]
Epoch [61/120    avg_loss:0.015, val_acc:0.982]
Epoch [62/120    avg_loss:0.012, val_acc:0.969]
Epoch [63/120    avg_loss:0.090, val_acc:0.911]
Epoch [64/120    avg_loss:0.142, val_acc:0.958]
Epoch [65/120    avg_loss:0.046, val_acc:0.971]
Epoch [66/120    avg_loss:0.023, val_acc:0.978]
Epoch [67/120    avg_loss:0.020, val_acc:0.972]
Epoch [68/120    avg_loss:0.022, val_acc:0.981]
Epoch [69/120    avg_loss:0.017, val_acc:0.980]
Epoch [70/120    avg_loss:0.014, val_acc:0.979]
Epoch [71/120    avg_loss:0.009, val_acc:0.984]
Epoch [72/120    avg_loss:0.006, val_acc:0.982]
Epoch [73/120    avg_loss:0.006, val_acc:0.983]
Epoch [74/120    avg_loss:0.006, val_acc:0.983]
Epoch [75/120    avg_loss:0.008, val_acc:0.981]
Epoch [76/120    avg_loss:0.006, val_acc:0.980]
Epoch [77/120    avg_loss:0.007, val_acc:0.981]
Epoch [78/120    avg_loss:0.008, val_acc:0.983]
Epoch [79/120    avg_loss:0.007, val_acc:0.986]
Epoch [80/120    avg_loss:0.007, val_acc:0.986]
Epoch [81/120    avg_loss:0.008, val_acc:0.985]
Epoch [82/120    avg_loss:0.007, val_acc:0.985]
Epoch [83/120    avg_loss:0.006, val_acc:0.985]
Epoch [84/120    avg_loss:0.007, val_acc:0.984]
Epoch [85/120    avg_loss:0.006, val_acc:0.984]
Epoch [86/120    avg_loss:0.007, val_acc:0.985]
Epoch [87/120    avg_loss:0.008, val_acc:0.983]
Epoch [88/120    avg_loss:0.006, val_acc:0.984]
Epoch [89/120    avg_loss:0.008, val_acc:0.982]
Epoch [90/120    avg_loss:0.006, val_acc:0.983]
Epoch [91/120    avg_loss:0.005, val_acc:0.985]
Epoch [92/120    avg_loss:0.006, val_acc:0.985]
Epoch [93/120    avg_loss:0.006, val_acc:0.986]
Epoch [94/120    avg_loss:0.005, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.985]
Epoch [96/120    avg_loss:0.005, val_acc:0.985]
Epoch [97/120    avg_loss:0.005, val_acc:0.986]
Epoch [98/120    avg_loss:0.005, val_acc:0.986]
Epoch [99/120    avg_loss:0.006, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.985]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.005, val_acc:0.986]
Epoch [103/120    avg_loss:0.008, val_acc:0.986]
Epoch [104/120    avg_loss:0.006, val_acc:0.986]
Epoch [105/120    avg_loss:0.005, val_acc:0.987]
Epoch [106/120    avg_loss:0.007, val_acc:0.985]
Epoch [107/120    avg_loss:0.007, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.986]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.007, val_acc:0.986]
Epoch [114/120    avg_loss:0.005, val_acc:0.987]
Epoch [115/120    avg_loss:0.004, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.004, val_acc:0.987]
Epoch [118/120    avg_loss:0.006, val_acc:0.987]
Epoch [119/120    avg_loss:0.006, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    0    4    0    2    0    0    0    3   15    0    0
     0    1    0]
 [   0    0    0  726    2    4    0    0    0    3    0    0   10    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0  870    4    0    0
     0    0    0]
 [   0    0    2    0    0    0    2    0    0    0    3 2203    0    0
     0    0    0]
 [   0    0    0    1    0    1    0    0    0    0    6    0  525    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    0    0    0    0
  1118   14    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    64  271    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.13550135501355

F1 scores:
[       nan 1.         0.98939929 0.98507463 0.98611111 0.98636364
 0.98493976 1.         0.99883856 0.9        0.99032442 0.99368516
 0.97856477 0.99462366 0.96296296 0.85624013 0.96969697]

Kappa:
0.978731729336398
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6d59210748>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.254, val_acc:0.436]
Epoch [2/120    avg_loss:1.710, val_acc:0.608]
Epoch [3/120    avg_loss:1.389, val_acc:0.664]
Epoch [4/120    avg_loss:1.138, val_acc:0.730]
Epoch [5/120    avg_loss:0.951, val_acc:0.706]
Epoch [6/120    avg_loss:0.841, val_acc:0.802]
Epoch [7/120    avg_loss:0.676, val_acc:0.756]
Epoch [8/120    avg_loss:0.539, val_acc:0.793]
Epoch [9/120    avg_loss:0.580, val_acc:0.740]
Epoch [10/120    avg_loss:0.504, val_acc:0.808]
Epoch [11/120    avg_loss:0.371, val_acc:0.828]
Epoch [12/120    avg_loss:0.310, val_acc:0.867]
Epoch [13/120    avg_loss:0.424, val_acc:0.851]
Epoch [14/120    avg_loss:0.239, val_acc:0.886]
Epoch [15/120    avg_loss:0.286, val_acc:0.850]
Epoch [16/120    avg_loss:0.217, val_acc:0.844]
Epoch [17/120    avg_loss:0.163, val_acc:0.908]
Epoch [18/120    avg_loss:0.120, val_acc:0.918]
Epoch [19/120    avg_loss:0.129, val_acc:0.924]
Epoch [20/120    avg_loss:0.145, val_acc:0.911]
Epoch [21/120    avg_loss:0.114, val_acc:0.930]
Epoch [22/120    avg_loss:0.144, val_acc:0.918]
Epoch [23/120    avg_loss:0.105, val_acc:0.938]
Epoch [24/120    avg_loss:0.081, val_acc:0.946]
Epoch [25/120    avg_loss:0.076, val_acc:0.929]
Epoch [26/120    avg_loss:0.077, val_acc:0.939]
Epoch [27/120    avg_loss:0.063, val_acc:0.954]
Epoch [28/120    avg_loss:0.070, val_acc:0.942]
Epoch [29/120    avg_loss:0.065, val_acc:0.951]
Epoch [30/120    avg_loss:0.079, val_acc:0.945]
Epoch [31/120    avg_loss:0.084, val_acc:0.950]
Epoch [32/120    avg_loss:0.056, val_acc:0.929]
Epoch [33/120    avg_loss:0.039, val_acc:0.958]
Epoch [34/120    avg_loss:0.044, val_acc:0.961]
Epoch [35/120    avg_loss:0.038, val_acc:0.952]
Epoch [36/120    avg_loss:0.062, val_acc:0.913]
Epoch [37/120    avg_loss:0.071, val_acc:0.950]
Epoch [38/120    avg_loss:0.083, val_acc:0.932]
Epoch [39/120    avg_loss:0.280, val_acc:0.648]
Epoch [40/120    avg_loss:0.673, val_acc:0.766]
Epoch [41/120    avg_loss:0.179, val_acc:0.939]
Epoch [42/120    avg_loss:0.080, val_acc:0.943]
Epoch [43/120    avg_loss:0.084, val_acc:0.931]
Epoch [44/120    avg_loss:0.066, val_acc:0.957]
Epoch [45/120    avg_loss:0.058, val_acc:0.956]
Epoch [46/120    avg_loss:0.046, val_acc:0.952]
Epoch [47/120    avg_loss:0.049, val_acc:0.962]
Epoch [48/120    avg_loss:0.039, val_acc:0.960]
Epoch [49/120    avg_loss:0.048, val_acc:0.953]
Epoch [50/120    avg_loss:0.050, val_acc:0.945]
Epoch [51/120    avg_loss:0.045, val_acc:0.964]
Epoch [52/120    avg_loss:0.041, val_acc:0.964]
Epoch [53/120    avg_loss:0.027, val_acc:0.965]
Epoch [54/120    avg_loss:0.017, val_acc:0.965]
Epoch [55/120    avg_loss:0.017, val_acc:0.974]
Epoch [56/120    avg_loss:0.024, val_acc:0.969]
Epoch [57/120    avg_loss:0.026, val_acc:0.957]
Epoch [58/120    avg_loss:0.019, val_acc:0.969]
Epoch [59/120    avg_loss:0.013, val_acc:0.974]
Epoch [60/120    avg_loss:0.017, val_acc:0.976]
Epoch [61/120    avg_loss:0.020, val_acc:0.975]
Epoch [62/120    avg_loss:0.032, val_acc:0.968]
Epoch [63/120    avg_loss:0.014, val_acc:0.980]
Epoch [64/120    avg_loss:0.013, val_acc:0.977]
Epoch [65/120    avg_loss:0.016, val_acc:0.972]
Epoch [66/120    avg_loss:0.019, val_acc:0.974]
Epoch [67/120    avg_loss:0.022, val_acc:0.967]
Epoch [68/120    avg_loss:0.036, val_acc:0.967]
Epoch [69/120    avg_loss:0.025, val_acc:0.972]
Epoch [70/120    avg_loss:0.027, val_acc:0.975]
Epoch [71/120    avg_loss:0.015, val_acc:0.980]
Epoch [72/120    avg_loss:0.010, val_acc:0.982]
Epoch [73/120    avg_loss:0.007, val_acc:0.979]
Epoch [74/120    avg_loss:0.008, val_acc:0.981]
Epoch [75/120    avg_loss:0.007, val_acc:0.979]
Epoch [76/120    avg_loss:0.012, val_acc:0.974]
Epoch [77/120    avg_loss:0.011, val_acc:0.980]
Epoch [78/120    avg_loss:0.010, val_acc:0.976]
Epoch [79/120    avg_loss:0.009, val_acc:0.979]
Epoch [80/120    avg_loss:0.009, val_acc:0.981]
Epoch [81/120    avg_loss:0.006, val_acc:0.983]
Epoch [82/120    avg_loss:0.009, val_acc:0.979]
Epoch [83/120    avg_loss:0.006, val_acc:0.985]
Epoch [84/120    avg_loss:0.017, val_acc:0.976]
Epoch [85/120    avg_loss:0.039, val_acc:0.970]
Epoch [86/120    avg_loss:0.012, val_acc:0.979]
Epoch [87/120    avg_loss:0.012, val_acc:0.975]
Epoch [88/120    avg_loss:0.006, val_acc:0.983]
Epoch [89/120    avg_loss:0.005, val_acc:0.983]
Epoch [90/120    avg_loss:0.006, val_acc:0.980]
Epoch [91/120    avg_loss:0.006, val_acc:0.981]
Epoch [92/120    avg_loss:0.008, val_acc:0.982]
Epoch [93/120    avg_loss:0.007, val_acc:0.982]
Epoch [94/120    avg_loss:0.006, val_acc:0.978]
Epoch [95/120    avg_loss:0.007, val_acc:0.982]
Epoch [96/120    avg_loss:0.006, val_acc:0.984]
Epoch [97/120    avg_loss:0.005, val_acc:0.982]
Epoch [98/120    avg_loss:0.004, val_acc:0.981]
Epoch [99/120    avg_loss:0.003, val_acc:0.981]
Epoch [100/120    avg_loss:0.003, val_acc:0.980]
Epoch [101/120    avg_loss:0.004, val_acc:0.981]
Epoch [102/120    avg_loss:0.004, val_acc:0.980]
Epoch [103/120    avg_loss:0.005, val_acc:0.982]
Epoch [104/120    avg_loss:0.004, val_acc:0.981]
Epoch [105/120    avg_loss:0.004, val_acc:0.980]
Epoch [106/120    avg_loss:0.003, val_acc:0.980]
Epoch [107/120    avg_loss:0.003, val_acc:0.982]
Epoch [108/120    avg_loss:0.003, val_acc:0.981]
Epoch [109/120    avg_loss:0.003, val_acc:0.982]
Epoch [110/120    avg_loss:0.005, val_acc:0.983]
Epoch [111/120    avg_loss:0.004, val_acc:0.983]
Epoch [112/120    avg_loss:0.004, val_acc:0.983]
Epoch [113/120    avg_loss:0.003, val_acc:0.983]
Epoch [114/120    avg_loss:0.004, val_acc:0.983]
Epoch [115/120    avg_loss:0.005, val_acc:0.983]
Epoch [116/120    avg_loss:0.004, val_acc:0.983]
Epoch [117/120    avg_loss:0.005, val_acc:0.983]
Epoch [118/120    avg_loss:0.003, val_acc:0.983]
Epoch [119/120    avg_loss:0.003, val_acc:0.983]
Epoch [120/120    avg_loss:0.003, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1268    3    2    0    0    0    0    0    3    7    0    0
     0    2    0]
 [   0    0    0  739    0    1    0    0    0    2    0    0    3    1
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    2    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    1    0    0    0    0  857   11    0    0
     3    1    0]
 [   0    0    5    1    0    0    1    0    0    0    1 2197    2    1
     1    1    0]
 [   0    0    0    4    1    3    0    0    0    0    3    0  522    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1136    3    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    28  316    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.81842818428184

F1 scores:
[       nan 0.98765432 0.99023819 0.9892905  0.99300699 0.98966705
 0.9969651  1.         1.         0.9        0.98562392 0.99254574
 0.98212606 0.99462366 0.98440208 0.94328358 0.98203593]

Kappa:
0.9865251324822603
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3677bab710>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.326, val_acc:0.476]
Epoch [2/120    avg_loss:1.765, val_acc:0.575]
Epoch [3/120    avg_loss:1.429, val_acc:0.637]
Epoch [4/120    avg_loss:1.145, val_acc:0.727]
Epoch [5/120    avg_loss:0.902, val_acc:0.739]
Epoch [6/120    avg_loss:0.843, val_acc:0.776]
Epoch [7/120    avg_loss:0.658, val_acc:0.775]
Epoch [8/120    avg_loss:0.560, val_acc:0.815]
Epoch [9/120    avg_loss:0.532, val_acc:0.822]
Epoch [10/120    avg_loss:0.446, val_acc:0.822]
Epoch [11/120    avg_loss:0.466, val_acc:0.860]
Epoch [12/120    avg_loss:0.394, val_acc:0.872]
Epoch [13/120    avg_loss:0.303, val_acc:0.858]
Epoch [14/120    avg_loss:0.277, val_acc:0.862]
Epoch [15/120    avg_loss:0.234, val_acc:0.900]
Epoch [16/120    avg_loss:0.315, val_acc:0.880]
Epoch [17/120    avg_loss:0.192, val_acc:0.896]
Epoch [18/120    avg_loss:0.207, val_acc:0.893]
Epoch [19/120    avg_loss:0.217, val_acc:0.923]
Epoch [20/120    avg_loss:0.180, val_acc:0.914]
Epoch [21/120    avg_loss:0.139, val_acc:0.929]
Epoch [22/120    avg_loss:0.129, val_acc:0.941]
Epoch [23/120    avg_loss:0.117, val_acc:0.938]
Epoch [24/120    avg_loss:0.102, val_acc:0.950]
Epoch [25/120    avg_loss:0.114, val_acc:0.931]
Epoch [26/120    avg_loss:0.104, val_acc:0.935]
Epoch [27/120    avg_loss:0.100, val_acc:0.952]
Epoch [28/120    avg_loss:0.070, val_acc:0.942]
Epoch [29/120    avg_loss:0.059, val_acc:0.946]
Epoch [30/120    avg_loss:0.083, val_acc:0.948]
Epoch [31/120    avg_loss:0.094, val_acc:0.954]
Epoch [32/120    avg_loss:0.074, val_acc:0.957]
Epoch [33/120    avg_loss:0.050, val_acc:0.957]
Epoch [34/120    avg_loss:0.086, val_acc:0.951]
Epoch [35/120    avg_loss:0.047, val_acc:0.959]
Epoch [36/120    avg_loss:0.042, val_acc:0.975]
Epoch [37/120    avg_loss:0.043, val_acc:0.959]
Epoch [38/120    avg_loss:0.044, val_acc:0.977]
Epoch [39/120    avg_loss:0.052, val_acc:0.960]
Epoch [40/120    avg_loss:0.079, val_acc:0.953]
Epoch [41/120    avg_loss:0.061, val_acc:0.967]
Epoch [42/120    avg_loss:0.036, val_acc:0.971]
Epoch [43/120    avg_loss:0.032, val_acc:0.975]
Epoch [44/120    avg_loss:0.042, val_acc:0.960]
Epoch [45/120    avg_loss:0.043, val_acc:0.976]
Epoch [46/120    avg_loss:0.028, val_acc:0.968]
Epoch [47/120    avg_loss:0.026, val_acc:0.978]
Epoch [48/120    avg_loss:0.042, val_acc:0.975]
Epoch [49/120    avg_loss:0.034, val_acc:0.970]
Epoch [50/120    avg_loss:0.031, val_acc:0.974]
Epoch [51/120    avg_loss:0.020, val_acc:0.984]
Epoch [52/120    avg_loss:0.057, val_acc:0.970]
Epoch [53/120    avg_loss:0.046, val_acc:0.960]
Epoch [54/120    avg_loss:0.046, val_acc:0.971]
Epoch [55/120    avg_loss:0.045, val_acc:0.979]
Epoch [56/120    avg_loss:0.030, val_acc:0.971]
Epoch [57/120    avg_loss:0.028, val_acc:0.966]
Epoch [58/120    avg_loss:0.019, val_acc:0.971]
Epoch [59/120    avg_loss:0.025, val_acc:0.971]
Epoch [60/120    avg_loss:0.016, val_acc:0.953]
Epoch [61/120    avg_loss:0.024, val_acc:0.980]
Epoch [62/120    avg_loss:0.018, val_acc:0.970]
Epoch [63/120    avg_loss:0.023, val_acc:0.980]
Epoch [64/120    avg_loss:0.013, val_acc:0.980]
Epoch [65/120    avg_loss:0.009, val_acc:0.983]
Epoch [66/120    avg_loss:0.009, val_acc:0.983]
Epoch [67/120    avg_loss:0.006, val_acc:0.983]
Epoch [68/120    avg_loss:0.007, val_acc:0.984]
Epoch [69/120    avg_loss:0.010, val_acc:0.983]
Epoch [70/120    avg_loss:0.008, val_acc:0.986]
Epoch [71/120    avg_loss:0.007, val_acc:0.986]
Epoch [72/120    avg_loss:0.008, val_acc:0.985]
Epoch [73/120    avg_loss:0.007, val_acc:0.984]
Epoch [74/120    avg_loss:0.006, val_acc:0.985]
Epoch [75/120    avg_loss:0.008, val_acc:0.985]
Epoch [76/120    avg_loss:0.009, val_acc:0.985]
Epoch [77/120    avg_loss:0.006, val_acc:0.983]
Epoch [78/120    avg_loss:0.006, val_acc:0.983]
Epoch [79/120    avg_loss:0.007, val_acc:0.983]
Epoch [80/120    avg_loss:0.007, val_acc:0.985]
Epoch [81/120    avg_loss:0.006, val_acc:0.986]
Epoch [82/120    avg_loss:0.006, val_acc:0.986]
Epoch [83/120    avg_loss:0.007, val_acc:0.986]
Epoch [84/120    avg_loss:0.005, val_acc:0.986]
Epoch [85/120    avg_loss:0.007, val_acc:0.986]
Epoch [86/120    avg_loss:0.006, val_acc:0.985]
Epoch [87/120    avg_loss:0.006, val_acc:0.985]
Epoch [88/120    avg_loss:0.005, val_acc:0.985]
Epoch [89/120    avg_loss:0.007, val_acc:0.985]
Epoch [90/120    avg_loss:0.006, val_acc:0.984]
Epoch [91/120    avg_loss:0.005, val_acc:0.985]
Epoch [92/120    avg_loss:0.006, val_acc:0.985]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.006, val_acc:0.985]
Epoch [95/120    avg_loss:0.006, val_acc:0.984]
Epoch [96/120    avg_loss:0.004, val_acc:0.984]
Epoch [97/120    avg_loss:0.005, val_acc:0.984]
Epoch [98/120    avg_loss:0.005, val_acc:0.986]
Epoch [99/120    avg_loss:0.006, val_acc:0.985]
Epoch [100/120    avg_loss:0.005, val_acc:0.985]
Epoch [101/120    avg_loss:0.004, val_acc:0.985]
Epoch [102/120    avg_loss:0.006, val_acc:0.985]
Epoch [103/120    avg_loss:0.009, val_acc:0.985]
Epoch [104/120    avg_loss:0.004, val_acc:0.986]
Epoch [105/120    avg_loss:0.005, val_acc:0.986]
Epoch [106/120    avg_loss:0.005, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.984]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.007, val_acc:0.985]
Epoch [112/120    avg_loss:0.005, val_acc:0.985]
Epoch [113/120    avg_loss:0.005, val_acc:0.985]
Epoch [114/120    avg_loss:0.005, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.985]
Epoch [116/120    avg_loss:0.006, val_acc:0.985]
Epoch [117/120    avg_loss:0.006, val_acc:0.985]
Epoch [118/120    avg_loss:0.005, val_acc:0.985]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1259    0    0    0    0    0    0    0   10    9    7    0
     0    0    0]
 [   0    0    0  724    0   12    0    0    0    1    1    0    8    1
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    1    0    4    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    1  869    1    0    0
     0    1    0]
 [   0    0    8    0    0    0    3    0    0    0    9 2190    0    0
     0    0    0]
 [   0    0    0    2    4    0    0    0    0    0    4    0  523    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    1    0    0    0
  1134    2    0]
 [   0    0    0    0    0    0   22    0    0    0    0    0    0    0
    43  282    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.18970189701896

F1 scores:
[       nan 1.         0.98667712 0.98236092 0.98834499 0.97155859
 0.97980553 0.98039216 1.         0.85714286 0.98247598 0.99297212
 0.97574627 0.99730458 0.97758621 0.89240506 0.99408284]

Kappa:
0.9793589691611349
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff031356710>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.297, val_acc:0.591]
Epoch [2/120    avg_loss:1.628, val_acc:0.681]
Epoch [3/120    avg_loss:1.275, val_acc:0.727]
Epoch [4/120    avg_loss:0.972, val_acc:0.756]
Epoch [5/120    avg_loss:0.819, val_acc:0.753]
Epoch [6/120    avg_loss:0.847, val_acc:0.762]
Epoch [7/120    avg_loss:0.831, val_acc:0.810]
Epoch [8/120    avg_loss:0.648, val_acc:0.804]
Epoch [9/120    avg_loss:0.529, val_acc:0.849]
Epoch [10/120    avg_loss:0.520, val_acc:0.880]
Epoch [11/120    avg_loss:0.467, val_acc:0.887]
Epoch [12/120    avg_loss:0.363, val_acc:0.818]
Epoch [13/120    avg_loss:0.340, val_acc:0.898]
Epoch [14/120    avg_loss:0.248, val_acc:0.922]
Epoch [15/120    avg_loss:0.245, val_acc:0.908]
Epoch [16/120    avg_loss:0.253, val_acc:0.906]
Epoch [17/120    avg_loss:0.252, val_acc:0.898]
Epoch [18/120    avg_loss:0.263, val_acc:0.913]
Epoch [19/120    avg_loss:0.201, val_acc:0.908]
Epoch [20/120    avg_loss:0.179, val_acc:0.936]
Epoch [21/120    avg_loss:0.137, val_acc:0.923]
Epoch [22/120    avg_loss:0.140, val_acc:0.954]
Epoch [23/120    avg_loss:0.156, val_acc:0.962]
Epoch [24/120    avg_loss:0.117, val_acc:0.950]
Epoch [25/120    avg_loss:0.082, val_acc:0.964]
Epoch [26/120    avg_loss:0.088, val_acc:0.950]
Epoch [27/120    avg_loss:0.118, val_acc:0.919]
Epoch [28/120    avg_loss:0.111, val_acc:0.960]
Epoch [29/120    avg_loss:0.129, val_acc:0.945]
Epoch [30/120    avg_loss:0.098, val_acc:0.961]
Epoch [31/120    avg_loss:0.066, val_acc:0.931]
Epoch [32/120    avg_loss:0.084, val_acc:0.958]
Epoch [33/120    avg_loss:0.065, val_acc:0.938]
Epoch [34/120    avg_loss:0.074, val_acc:0.964]
Epoch [35/120    avg_loss:0.076, val_acc:0.969]
Epoch [36/120    avg_loss:0.066, val_acc:0.964]
Epoch [37/120    avg_loss:0.067, val_acc:0.967]
Epoch [38/120    avg_loss:0.051, val_acc:0.974]
Epoch [39/120    avg_loss:0.048, val_acc:0.967]
Epoch [40/120    avg_loss:0.047, val_acc:0.961]
Epoch [41/120    avg_loss:0.045, val_acc:0.975]
Epoch [42/120    avg_loss:0.054, val_acc:0.973]
Epoch [43/120    avg_loss:0.042, val_acc:0.973]
Epoch [44/120    avg_loss:0.032, val_acc:0.971]
Epoch [45/120    avg_loss:0.034, val_acc:0.976]
Epoch [46/120    avg_loss:0.036, val_acc:0.980]
Epoch [47/120    avg_loss:0.030, val_acc:0.974]
Epoch [48/120    avg_loss:0.036, val_acc:0.971]
Epoch [49/120    avg_loss:0.053, val_acc:0.969]
Epoch [50/120    avg_loss:0.037, val_acc:0.974]
Epoch [51/120    avg_loss:0.053, val_acc:0.964]
Epoch [52/120    avg_loss:0.051, val_acc:0.974]
Epoch [53/120    avg_loss:0.074, val_acc:0.969]
Epoch [54/120    avg_loss:0.055, val_acc:0.982]
Epoch [55/120    avg_loss:0.037, val_acc:0.974]
Epoch [56/120    avg_loss:0.040, val_acc:0.953]
Epoch [57/120    avg_loss:0.022, val_acc:0.972]
Epoch [58/120    avg_loss:0.017, val_acc:0.975]
Epoch [59/120    avg_loss:0.014, val_acc:0.977]
Epoch [60/120    avg_loss:0.019, val_acc:0.983]
Epoch [61/120    avg_loss:0.017, val_acc:0.985]
Epoch [62/120    avg_loss:0.023, val_acc:0.980]
Epoch [63/120    avg_loss:0.017, val_acc:0.983]
Epoch [64/120    avg_loss:0.013, val_acc:0.982]
Epoch [65/120    avg_loss:0.027, val_acc:0.975]
Epoch [66/120    avg_loss:0.028, val_acc:0.983]
Epoch [67/120    avg_loss:0.011, val_acc:0.975]
Epoch [68/120    avg_loss:0.015, val_acc:0.983]
Epoch [69/120    avg_loss:0.012, val_acc:0.985]
Epoch [70/120    avg_loss:0.011, val_acc:0.985]
Epoch [71/120    avg_loss:0.039, val_acc:0.987]
Epoch [72/120    avg_loss:0.024, val_acc:0.986]
Epoch [73/120    avg_loss:0.021, val_acc:0.981]
Epoch [74/120    avg_loss:0.036, val_acc:0.962]
Epoch [75/120    avg_loss:0.060, val_acc:0.978]
Epoch [76/120    avg_loss:0.048, val_acc:0.982]
Epoch [77/120    avg_loss:0.035, val_acc:0.977]
Epoch [78/120    avg_loss:0.028, val_acc:0.971]
Epoch [79/120    avg_loss:0.041, val_acc:0.973]
Epoch [80/120    avg_loss:0.022, val_acc:0.984]
Epoch [81/120    avg_loss:0.014, val_acc:0.977]
Epoch [82/120    avg_loss:0.010, val_acc:0.982]
Epoch [83/120    avg_loss:0.015, val_acc:0.980]
Epoch [84/120    avg_loss:0.012, val_acc:0.984]
Epoch [85/120    avg_loss:0.009, val_acc:0.985]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.008, val_acc:0.985]
Epoch [88/120    avg_loss:0.006, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.988]
Epoch [90/120    avg_loss:0.005, val_acc:0.987]
Epoch [91/120    avg_loss:0.006, val_acc:0.989]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.005, val_acc:0.987]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.005, val_acc:0.987]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.006, val_acc:0.988]
Epoch [98/120    avg_loss:0.006, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.989]
Epoch [100/120    avg_loss:0.005, val_acc:0.989]
Epoch [101/120    avg_loss:0.006, val_acc:0.987]
Epoch [102/120    avg_loss:0.007, val_acc:0.989]
Epoch [103/120    avg_loss:0.006, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.988]
Epoch [106/120    avg_loss:0.009, val_acc:0.988]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.986]
Epoch [109/120    avg_loss:0.006, val_acc:0.985]
Epoch [110/120    avg_loss:0.005, val_acc:0.989]
Epoch [111/120    avg_loss:0.005, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.987]
Epoch [115/120    avg_loss:0.007, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.006, val_acc:0.989]
Epoch [118/120    avg_loss:0.005, val_acc:0.988]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.004, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1269    0    0    0    1    0    0    0    5    5    3    0
     0    2    0]
 [   0    0    0  720    4    3    0    0    0    6    1    0   11    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    7    3    0    0    0  843   10    5    0
     0    5    0]
 [   0    0    3    0    0    0    3    0    0    0    8 2195    0    1
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0  525    0
     1    1    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0    0    0
  1135    2    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
     7  331    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.6449864498645

F1 scores:
[       nan 0.98765432 0.99179367 0.98159509 0.99069767 0.98297389
 0.98644578 0.98039216 1.         0.8372093  0.97175793 0.99298801
 0.97312326 0.9919571  0.99430574 0.9622093  0.97076023]

Kappa:
0.9845557492150202
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb27b5686a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.267, val_acc:0.564]
Epoch [2/120    avg_loss:1.685, val_acc:0.622]
Epoch [3/120    avg_loss:1.467, val_acc:0.643]
Epoch [4/120    avg_loss:1.145, val_acc:0.711]
Epoch [5/120    avg_loss:1.012, val_acc:0.688]
Epoch [6/120    avg_loss:0.819, val_acc:0.737]
Epoch [7/120    avg_loss:0.743, val_acc:0.783]
Epoch [8/120    avg_loss:0.612, val_acc:0.810]
Epoch [9/120    avg_loss:0.508, val_acc:0.841]
Epoch [10/120    avg_loss:0.414, val_acc:0.865]
Epoch [11/120    avg_loss:0.410, val_acc:0.835]
Epoch [12/120    avg_loss:0.383, val_acc:0.849]
Epoch [13/120    avg_loss:0.313, val_acc:0.870]
Epoch [14/120    avg_loss:0.402, val_acc:0.870]
Epoch [15/120    avg_loss:0.377, val_acc:0.864]
Epoch [16/120    avg_loss:0.284, val_acc:0.846]
Epoch [17/120    avg_loss:0.293, val_acc:0.908]
Epoch [18/120    avg_loss:0.196, val_acc:0.902]
Epoch [19/120    avg_loss:0.174, val_acc:0.943]
Epoch [20/120    avg_loss:0.152, val_acc:0.944]
Epoch [21/120    avg_loss:0.163, val_acc:0.923]
Epoch [22/120    avg_loss:0.162, val_acc:0.922]
Epoch [23/120    avg_loss:0.172, val_acc:0.930]
Epoch [24/120    avg_loss:0.155, val_acc:0.865]
Epoch [25/120    avg_loss:0.140, val_acc:0.945]
Epoch [26/120    avg_loss:0.142, val_acc:0.922]
Epoch [27/120    avg_loss:0.116, val_acc:0.943]
Epoch [28/120    avg_loss:0.075, val_acc:0.943]
Epoch [29/120    avg_loss:0.120, val_acc:0.923]
Epoch [30/120    avg_loss:0.085, val_acc:0.959]
Epoch [31/120    avg_loss:0.076, val_acc:0.961]
Epoch [32/120    avg_loss:0.085, val_acc:0.927]
Epoch [33/120    avg_loss:0.080, val_acc:0.907]
Epoch [34/120    avg_loss:0.110, val_acc:0.933]
Epoch [35/120    avg_loss:0.105, val_acc:0.952]
Epoch [36/120    avg_loss:0.063, val_acc:0.968]
Epoch [37/120    avg_loss:0.067, val_acc:0.957]
Epoch [38/120    avg_loss:0.056, val_acc:0.972]
Epoch [39/120    avg_loss:0.040, val_acc:0.964]
Epoch [40/120    avg_loss:0.041, val_acc:0.957]
Epoch [41/120    avg_loss:0.072, val_acc:0.960]
Epoch [42/120    avg_loss:0.045, val_acc:0.970]
Epoch [43/120    avg_loss:0.041, val_acc:0.978]
Epoch [44/120    avg_loss:0.038, val_acc:0.970]
Epoch [45/120    avg_loss:0.051, val_acc:0.964]
Epoch [46/120    avg_loss:0.086, val_acc:0.922]
Epoch [47/120    avg_loss:0.076, val_acc:0.964]
Epoch [48/120    avg_loss:0.064, val_acc:0.950]
Epoch [49/120    avg_loss:0.046, val_acc:0.973]
Epoch [50/120    avg_loss:0.032, val_acc:0.983]
Epoch [51/120    avg_loss:0.025, val_acc:0.974]
Epoch [52/120    avg_loss:0.037, val_acc:0.961]
Epoch [53/120    avg_loss:0.038, val_acc:0.980]
Epoch [54/120    avg_loss:0.026, val_acc:0.983]
Epoch [55/120    avg_loss:0.024, val_acc:0.982]
Epoch [56/120    avg_loss:0.025, val_acc:0.982]
Epoch [57/120    avg_loss:0.026, val_acc:0.985]
Epoch [58/120    avg_loss:0.028, val_acc:0.987]
Epoch [59/120    avg_loss:0.023, val_acc:0.984]
Epoch [60/120    avg_loss:0.027, val_acc:0.986]
Epoch [61/120    avg_loss:0.023, val_acc:0.974]
Epoch [62/120    avg_loss:0.019, val_acc:0.983]
Epoch [63/120    avg_loss:0.031, val_acc:0.986]
Epoch [64/120    avg_loss:0.043, val_acc:0.983]
Epoch [65/120    avg_loss:0.018, val_acc:0.980]
Epoch [66/120    avg_loss:0.021, val_acc:0.975]
Epoch [67/120    avg_loss:0.024, val_acc:0.978]
Epoch [68/120    avg_loss:0.062, val_acc:0.961]
Epoch [69/120    avg_loss:0.039, val_acc:0.975]
Epoch [70/120    avg_loss:0.020, val_acc:0.986]
Epoch [71/120    avg_loss:0.015, val_acc:0.984]
Epoch [72/120    avg_loss:0.009, val_acc:0.986]
Epoch [73/120    avg_loss:0.010, val_acc:0.986]
Epoch [74/120    avg_loss:0.010, val_acc:0.987]
Epoch [75/120    avg_loss:0.011, val_acc:0.987]
Epoch [76/120    avg_loss:0.011, val_acc:0.988]
Epoch [77/120    avg_loss:0.007, val_acc:0.988]
Epoch [78/120    avg_loss:0.007, val_acc:0.987]
Epoch [79/120    avg_loss:0.012, val_acc:0.987]
Epoch [80/120    avg_loss:0.007, val_acc:0.987]
Epoch [81/120    avg_loss:0.008, val_acc:0.990]
Epoch [82/120    avg_loss:0.011, val_acc:0.988]
Epoch [83/120    avg_loss:0.006, val_acc:0.988]
Epoch [84/120    avg_loss:0.007, val_acc:0.988]
Epoch [85/120    avg_loss:0.010, val_acc:0.988]
Epoch [86/120    avg_loss:0.008, val_acc:0.989]
Epoch [87/120    avg_loss:0.007, val_acc:0.989]
Epoch [88/120    avg_loss:0.008, val_acc:0.989]
Epoch [89/120    avg_loss:0.009, val_acc:0.988]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.009, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.008, val_acc:0.987]
Epoch [94/120    avg_loss:0.008, val_acc:0.988]
Epoch [95/120    avg_loss:0.012, val_acc:0.988]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.012, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.008, val_acc:0.988]
Epoch [100/120    avg_loss:0.006, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.010, val_acc:0.988]
Epoch [103/120    avg_loss:0.006, val_acc:0.988]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.007, val_acc:0.988]
Epoch [106/120    avg_loss:0.005, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.007, val_acc:0.988]
Epoch [113/120    avg_loss:0.008, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.988]
Epoch [115/120    avg_loss:0.009, val_acc:0.988]
Epoch [116/120    avg_loss:0.009, val_acc:0.988]
Epoch [117/120    avg_loss:0.009, val_acc:0.988]
Epoch [118/120    avg_loss:0.007, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1257    0    0    0    2    0    0    1    8    6    5    0
     0    6    0]
 [   0    0    0  724    0    9    0    0    0    6    1    0    2    5
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    2    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    8    3    0    0    0  853    7    1    0
     0    3    0]
 [   0    0    6    0    0    0    4    0    0    0    3 2194    2    1
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    0    0  527    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1136    2    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
     7  317    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.48238482384824

F1 scores:
[       nan 1.         0.9866562  0.98302783 0.99528302 0.97303371
 0.97087379 1.         1.         0.8        0.97989661 0.99298484
 0.98412698 0.98404255 0.99344119 0.93925926 0.98823529]

Kappa:
0.9827030560438796
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:19:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd46a83d710>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.357, val_acc:0.485]
Epoch [2/120    avg_loss:1.706, val_acc:0.604]
Epoch [3/120    avg_loss:1.436, val_acc:0.664]
Epoch [4/120    avg_loss:1.118, val_acc:0.698]
Epoch [5/120    avg_loss:0.923, val_acc:0.747]
Epoch [6/120    avg_loss:0.759, val_acc:0.780]
Epoch [7/120    avg_loss:0.832, val_acc:0.806]
Epoch [8/120    avg_loss:0.616, val_acc:0.838]
Epoch [9/120    avg_loss:0.543, val_acc:0.803]
Epoch [10/120    avg_loss:0.492, val_acc:0.807]
Epoch [11/120    avg_loss:0.438, val_acc:0.834]
Epoch [12/120    avg_loss:0.410, val_acc:0.853]
Epoch [13/120    avg_loss:0.321, val_acc:0.886]
Epoch [14/120    avg_loss:0.228, val_acc:0.903]
Epoch [15/120    avg_loss:0.236, val_acc:0.876]
Epoch [16/120    avg_loss:0.209, val_acc:0.860]
Epoch [17/120    avg_loss:0.236, val_acc:0.898]
Epoch [18/120    avg_loss:0.166, val_acc:0.922]
Epoch [19/120    avg_loss:0.162, val_acc:0.914]
Epoch [20/120    avg_loss:0.165, val_acc:0.858]
Epoch [21/120    avg_loss:0.210, val_acc:0.893]
Epoch [22/120    avg_loss:0.132, val_acc:0.942]
Epoch [23/120    avg_loss:0.115, val_acc:0.940]
Epoch [24/120    avg_loss:0.095, val_acc:0.927]
Epoch [25/120    avg_loss:0.109, val_acc:0.930]
Epoch [26/120    avg_loss:0.166, val_acc:0.896]
Epoch [27/120    avg_loss:0.125, val_acc:0.943]
Epoch [28/120    avg_loss:0.112, val_acc:0.950]
Epoch [29/120    avg_loss:0.086, val_acc:0.941]
Epoch [30/120    avg_loss:0.056, val_acc:0.947]
Epoch [31/120    avg_loss:0.074, val_acc:0.947]
Epoch [32/120    avg_loss:0.072, val_acc:0.950]
Epoch [33/120    avg_loss:0.075, val_acc:0.952]
Epoch [34/120    avg_loss:0.048, val_acc:0.963]
Epoch [35/120    avg_loss:0.040, val_acc:0.958]
Epoch [36/120    avg_loss:0.087, val_acc:0.945]
Epoch [37/120    avg_loss:0.116, val_acc:0.942]
Epoch [38/120    avg_loss:0.087, val_acc:0.945]
Epoch [39/120    avg_loss:0.082, val_acc:0.936]
Epoch [40/120    avg_loss:0.077, val_acc:0.952]
Epoch [41/120    avg_loss:0.058, val_acc:0.971]
Epoch [42/120    avg_loss:0.051, val_acc:0.961]
Epoch [43/120    avg_loss:0.037, val_acc:0.969]
Epoch [44/120    avg_loss:0.035, val_acc:0.975]
Epoch [45/120    avg_loss:0.036, val_acc:0.983]
Epoch [46/120    avg_loss:0.028, val_acc:0.974]
Epoch [47/120    avg_loss:0.029, val_acc:0.974]
Epoch [48/120    avg_loss:0.020, val_acc:0.984]
Epoch [49/120    avg_loss:0.061, val_acc:0.943]
Epoch [50/120    avg_loss:0.061, val_acc:0.971]
Epoch [51/120    avg_loss:0.060, val_acc:0.975]
Epoch [52/120    avg_loss:0.041, val_acc:0.978]
Epoch [53/120    avg_loss:0.165, val_acc:0.863]
Epoch [54/120    avg_loss:0.247, val_acc:0.945]
Epoch [55/120    avg_loss:0.078, val_acc:0.966]
Epoch [56/120    avg_loss:0.053, val_acc:0.963]
Epoch [57/120    avg_loss:0.059, val_acc:0.908]
Epoch [58/120    avg_loss:0.054, val_acc:0.966]
Epoch [59/120    avg_loss:0.035, val_acc:0.976]
Epoch [60/120    avg_loss:0.030, val_acc:0.978]
Epoch [61/120    avg_loss:0.048, val_acc:0.970]
Epoch [62/120    avg_loss:0.031, val_acc:0.974]
Epoch [63/120    avg_loss:0.021, val_acc:0.977]
Epoch [64/120    avg_loss:0.018, val_acc:0.979]
Epoch [65/120    avg_loss:0.015, val_acc:0.980]
Epoch [66/120    avg_loss:0.014, val_acc:0.981]
Epoch [67/120    avg_loss:0.015, val_acc:0.980]
Epoch [68/120    avg_loss:0.017, val_acc:0.980]
Epoch [69/120    avg_loss:0.013, val_acc:0.980]
Epoch [70/120    avg_loss:0.014, val_acc:0.981]
Epoch [71/120    avg_loss:0.013, val_acc:0.982]
Epoch [72/120    avg_loss:0.015, val_acc:0.984]
Epoch [73/120    avg_loss:0.014, val_acc:0.982]
Epoch [74/120    avg_loss:0.013, val_acc:0.984]
Epoch [75/120    avg_loss:0.010, val_acc:0.983]
Epoch [76/120    avg_loss:0.012, val_acc:0.985]
Epoch [77/120    avg_loss:0.011, val_acc:0.984]
Epoch [78/120    avg_loss:0.009, val_acc:0.985]
Epoch [79/120    avg_loss:0.011, val_acc:0.985]
Epoch [80/120    avg_loss:0.011, val_acc:0.985]
Epoch [81/120    avg_loss:0.012, val_acc:0.985]
Epoch [82/120    avg_loss:0.015, val_acc:0.986]
Epoch [83/120    avg_loss:0.011, val_acc:0.984]
Epoch [84/120    avg_loss:0.008, val_acc:0.984]
Epoch [85/120    avg_loss:0.011, val_acc:0.985]
Epoch [86/120    avg_loss:0.012, val_acc:0.985]
Epoch [87/120    avg_loss:0.012, val_acc:0.984]
Epoch [88/120    avg_loss:0.009, val_acc:0.985]
Epoch [89/120    avg_loss:0.009, val_acc:0.985]
Epoch [90/120    avg_loss:0.009, val_acc:0.984]
Epoch [91/120    avg_loss:0.010, val_acc:0.985]
Epoch [92/120    avg_loss:0.008, val_acc:0.986]
Epoch [93/120    avg_loss:0.009, val_acc:0.986]
Epoch [94/120    avg_loss:0.012, val_acc:0.985]
Epoch [95/120    avg_loss:0.010, val_acc:0.986]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.011, val_acc:0.987]
Epoch [98/120    avg_loss:0.010, val_acc:0.986]
Epoch [99/120    avg_loss:0.010, val_acc:0.985]
Epoch [100/120    avg_loss:0.009, val_acc:0.987]
Epoch [101/120    avg_loss:0.011, val_acc:0.986]
Epoch [102/120    avg_loss:0.007, val_acc:0.987]
Epoch [103/120    avg_loss:0.009, val_acc:0.987]
Epoch [104/120    avg_loss:0.010, val_acc:0.987]
Epoch [105/120    avg_loss:0.010, val_acc:0.987]
Epoch [106/120    avg_loss:0.009, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.987]
Epoch [108/120    avg_loss:0.010, val_acc:0.987]
Epoch [109/120    avg_loss:0.009, val_acc:0.987]
Epoch [110/120    avg_loss:0.014, val_acc:0.986]
Epoch [111/120    avg_loss:0.009, val_acc:0.988]
Epoch [112/120    avg_loss:0.012, val_acc:0.987]
Epoch [113/120    avg_loss:0.008, val_acc:0.987]
Epoch [114/120    avg_loss:0.011, val_acc:0.987]
Epoch [115/120    avg_loss:0.008, val_acc:0.987]
Epoch [116/120    avg_loss:0.009, val_acc:0.987]
Epoch [117/120    avg_loss:0.010, val_acc:0.988]
Epoch [118/120    avg_loss:0.010, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.008, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1271    0    0    0    1    0    0    1    5    5    2    0
     0    0    0]
 [   0    0    0  730    0    7    0    0    0    3    0    0    5    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0   13    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    6    1    0    0    0  865    2    1    0
     0    0    0]
 [   0    0    8    0    0    0    4    0    0    0    4 2193    0    1
     0    0    0]
 [   0    0    0    0    0   10    0    0    0    0    0    0  520    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    3    0    0    0
  1129    1    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    27  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.5691056910569

F1 scores:
[       nan 0.98765432 0.99141966 0.9851552  1.         0.96544036
 0.98498498 0.98039216 0.99883586 0.72222222 0.98687963 0.99433235
 0.97836312 0.9919571  0.983878   0.93740458 0.97674419]

Kappa:
0.9836879836111991
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f73cfc25748>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.296, val_acc:0.569]
Epoch [2/120    avg_loss:1.677, val_acc:0.588]
Epoch [3/120    avg_loss:1.438, val_acc:0.668]
Epoch [4/120    avg_loss:1.124, val_acc:0.738]
Epoch [5/120    avg_loss:0.994, val_acc:0.764]
Epoch [6/120    avg_loss:0.757, val_acc:0.805]
Epoch [7/120    avg_loss:0.720, val_acc:0.802]
Epoch [8/120    avg_loss:0.519, val_acc:0.798]
Epoch [9/120    avg_loss:0.565, val_acc:0.772]
Epoch [10/120    avg_loss:0.580, val_acc:0.756]
Epoch [11/120    avg_loss:0.470, val_acc:0.815]
Epoch [12/120    avg_loss:0.423, val_acc:0.865]
Epoch [13/120    avg_loss:0.350, val_acc:0.901]
Epoch [14/120    avg_loss:0.307, val_acc:0.894]
Epoch [15/120    avg_loss:0.253, val_acc:0.864]
Epoch [16/120    avg_loss:0.271, val_acc:0.906]
Epoch [17/120    avg_loss:0.205, val_acc:0.905]
Epoch [18/120    avg_loss:0.273, val_acc:0.881]
Epoch [19/120    avg_loss:0.276, val_acc:0.923]
Epoch [20/120    avg_loss:0.194, val_acc:0.928]
Epoch [21/120    avg_loss:0.154, val_acc:0.930]
Epoch [22/120    avg_loss:0.208, val_acc:0.908]
Epoch [23/120    avg_loss:0.240, val_acc:0.933]
Epoch [24/120    avg_loss:0.221, val_acc:0.923]
Epoch [25/120    avg_loss:0.210, val_acc:0.946]
Epoch [26/120    avg_loss:0.123, val_acc:0.948]
Epoch [27/120    avg_loss:0.093, val_acc:0.948]
Epoch [28/120    avg_loss:0.096, val_acc:0.958]
Epoch [29/120    avg_loss:0.089, val_acc:0.963]
Epoch [30/120    avg_loss:0.113, val_acc:0.956]
Epoch [31/120    avg_loss:0.066, val_acc:0.963]
Epoch [32/120    avg_loss:0.052, val_acc:0.973]
Epoch [33/120    avg_loss:0.050, val_acc:0.960]
Epoch [34/120    avg_loss:0.061, val_acc:0.972]
Epoch [35/120    avg_loss:0.051, val_acc:0.971]
Epoch [36/120    avg_loss:0.044, val_acc:0.961]
Epoch [37/120    avg_loss:0.042, val_acc:0.974]
Epoch [38/120    avg_loss:0.032, val_acc:0.975]
Epoch [39/120    avg_loss:0.039, val_acc:0.963]
Epoch [40/120    avg_loss:0.053, val_acc:0.967]
Epoch [41/120    avg_loss:0.052, val_acc:0.949]
Epoch [42/120    avg_loss:0.196, val_acc:0.888]
Epoch [43/120    avg_loss:0.165, val_acc:0.945]
Epoch [44/120    avg_loss:0.095, val_acc:0.960]
Epoch [45/120    avg_loss:0.070, val_acc:0.970]
Epoch [46/120    avg_loss:0.050, val_acc:0.970]
Epoch [47/120    avg_loss:0.048, val_acc:0.962]
Epoch [48/120    avg_loss:0.059, val_acc:0.948]
Epoch [49/120    avg_loss:0.065, val_acc:0.972]
Epoch [50/120    avg_loss:0.055, val_acc:0.967]
Epoch [51/120    avg_loss:0.034, val_acc:0.977]
Epoch [52/120    avg_loss:0.030, val_acc:0.976]
Epoch [53/120    avg_loss:0.030, val_acc:0.978]
Epoch [54/120    avg_loss:0.039, val_acc:0.980]
Epoch [55/120    avg_loss:0.034, val_acc:0.981]
Epoch [56/120    avg_loss:0.030, val_acc:0.980]
Epoch [57/120    avg_loss:0.024, val_acc:0.980]
Epoch [58/120    avg_loss:0.018, val_acc:0.980]
Epoch [59/120    avg_loss:0.022, val_acc:0.985]
Epoch [60/120    avg_loss:0.020, val_acc:0.984]
Epoch [61/120    avg_loss:0.022, val_acc:0.986]
Epoch [62/120    avg_loss:0.017, val_acc:0.991]
Epoch [63/120    avg_loss:0.011, val_acc:0.989]
Epoch [64/120    avg_loss:0.016, val_acc:0.987]
Epoch [65/120    avg_loss:0.013, val_acc:0.984]
Epoch [66/120    avg_loss:0.015, val_acc:0.986]
Epoch [67/120    avg_loss:0.021, val_acc:0.984]
Epoch [68/120    avg_loss:0.013, val_acc:0.982]
Epoch [69/120    avg_loss:0.010, val_acc:0.984]
Epoch [70/120    avg_loss:0.015, val_acc:0.990]
Epoch [71/120    avg_loss:0.010, val_acc:0.987]
Epoch [72/120    avg_loss:0.018, val_acc:0.977]
Epoch [73/120    avg_loss:0.026, val_acc:0.983]
Epoch [74/120    avg_loss:0.035, val_acc:0.984]
Epoch [75/120    avg_loss:0.020, val_acc:0.987]
Epoch [76/120    avg_loss:0.012, val_acc:0.989]
Epoch [77/120    avg_loss:0.010, val_acc:0.988]
Epoch [78/120    avg_loss:0.009, val_acc:0.989]
Epoch [79/120    avg_loss:0.009, val_acc:0.990]
Epoch [80/120    avg_loss:0.010, val_acc:0.990]
Epoch [81/120    avg_loss:0.012, val_acc:0.987]
Epoch [82/120    avg_loss:0.009, val_acc:0.990]
Epoch [83/120    avg_loss:0.010, val_acc:0.989]
Epoch [84/120    avg_loss:0.008, val_acc:0.989]
Epoch [85/120    avg_loss:0.008, val_acc:0.990]
Epoch [86/120    avg_loss:0.009, val_acc:0.991]
Epoch [87/120    avg_loss:0.008, val_acc:0.991]
Epoch [88/120    avg_loss:0.007, val_acc:0.990]
Epoch [89/120    avg_loss:0.008, val_acc:0.990]
Epoch [90/120    avg_loss:0.008, val_acc:0.990]
Epoch [91/120    avg_loss:0.008, val_acc:0.989]
Epoch [92/120    avg_loss:0.007, val_acc:0.989]
Epoch [93/120    avg_loss:0.006, val_acc:0.990]
Epoch [94/120    avg_loss:0.007, val_acc:0.991]
Epoch [95/120    avg_loss:0.008, val_acc:0.991]
Epoch [96/120    avg_loss:0.007, val_acc:0.992]
Epoch [97/120    avg_loss:0.005, val_acc:0.992]
Epoch [98/120    avg_loss:0.009, val_acc:0.992]
Epoch [99/120    avg_loss:0.006, val_acc:0.991]
Epoch [100/120    avg_loss:0.006, val_acc:0.991]
Epoch [101/120    avg_loss:0.007, val_acc:0.992]
Epoch [102/120    avg_loss:0.005, val_acc:0.992]
Epoch [103/120    avg_loss:0.007, val_acc:0.991]
Epoch [104/120    avg_loss:0.006, val_acc:0.991]
Epoch [105/120    avg_loss:0.005, val_acc:0.991]
Epoch [106/120    avg_loss:0.006, val_acc:0.991]
Epoch [107/120    avg_loss:0.006, val_acc:0.992]
Epoch [108/120    avg_loss:0.006, val_acc:0.991]
Epoch [109/120    avg_loss:0.009, val_acc:0.992]
Epoch [110/120    avg_loss:0.007, val_acc:0.992]
Epoch [111/120    avg_loss:0.006, val_acc:0.992]
Epoch [112/120    avg_loss:0.006, val_acc:0.992]
Epoch [113/120    avg_loss:0.006, val_acc:0.992]
Epoch [114/120    avg_loss:0.006, val_acc:0.992]
Epoch [115/120    avg_loss:0.005, val_acc:0.992]
Epoch [116/120    avg_loss:0.004, val_acc:0.992]
Epoch [117/120    avg_loss:0.006, val_acc:0.992]
Epoch [118/120    avg_loss:0.007, val_acc:0.992]
Epoch [119/120    avg_loss:0.007, val_acc:0.991]
Epoch [120/120    avg_loss:0.005, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1266    0    0    0    0    0    0    1    6   11    1    0
     0    0    0]
 [   0    0    0  707    0    4    0    0    0    5    2    0   25    3
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    5    1    0    0    0  860    1    0    0
     1    2    0]
 [   0    0    3    0    0    0    2    0    0    0    5 2198    0    1
     1    0    0]
 [   0    0    0    0    0    4    0    0    0    0    0    3  524    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    69  268    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.02710027100271

F1 scores:
[       nan 0.98765432 0.9890625  0.97248968 1.         0.98181818
 0.98944193 1.         0.99883856 0.8372093  0.9834191  0.99367089
 0.96678967 0.98930481 0.96765957 0.86871961 0.98245614]

Kappa:
0.9774942444742335
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f262ad186a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.287, val_acc:0.426]
Epoch [2/120    avg_loss:1.789, val_acc:0.543]
Epoch [3/120    avg_loss:1.412, val_acc:0.642]
Epoch [4/120    avg_loss:1.162, val_acc:0.728]
Epoch [5/120    avg_loss:1.043, val_acc:0.691]
Epoch [6/120    avg_loss:0.878, val_acc:0.763]
Epoch [7/120    avg_loss:0.737, val_acc:0.809]
Epoch [8/120    avg_loss:0.723, val_acc:0.769]
Epoch [9/120    avg_loss:0.592, val_acc:0.829]
Epoch [10/120    avg_loss:0.448, val_acc:0.852]
Epoch [11/120    avg_loss:0.489, val_acc:0.818]
Epoch [12/120    avg_loss:0.355, val_acc:0.883]
Epoch [13/120    avg_loss:0.297, val_acc:0.899]
Epoch [14/120    avg_loss:0.264, val_acc:0.894]
Epoch [15/120    avg_loss:0.246, val_acc:0.884]
Epoch [16/120    avg_loss:0.228, val_acc:0.916]
Epoch [17/120    avg_loss:0.254, val_acc:0.916]
Epoch [18/120    avg_loss:0.229, val_acc:0.922]
Epoch [19/120    avg_loss:0.178, val_acc:0.916]
Epoch [20/120    avg_loss:0.173, val_acc:0.914]
Epoch [21/120    avg_loss:0.133, val_acc:0.926]
Epoch [22/120    avg_loss:0.143, val_acc:0.927]
Epoch [23/120    avg_loss:0.135, val_acc:0.934]
Epoch [24/120    avg_loss:0.090, val_acc:0.948]
Epoch [25/120    avg_loss:0.101, val_acc:0.932]
Epoch [26/120    avg_loss:0.188, val_acc:0.908]
Epoch [27/120    avg_loss:0.123, val_acc:0.950]
Epoch [28/120    avg_loss:0.250, val_acc:0.804]
Epoch [29/120    avg_loss:0.304, val_acc:0.888]
Epoch [30/120    avg_loss:0.198, val_acc:0.923]
Epoch [31/120    avg_loss:0.136, val_acc:0.944]
Epoch [32/120    avg_loss:0.096, val_acc:0.947]
Epoch [33/120    avg_loss:0.115, val_acc:0.945]
Epoch [34/120    avg_loss:0.098, val_acc:0.949]
Epoch [35/120    avg_loss:0.071, val_acc:0.940]
Epoch [36/120    avg_loss:0.079, val_acc:0.954]
Epoch [37/120    avg_loss:0.110, val_acc:0.947]
Epoch [38/120    avg_loss:0.056, val_acc:0.957]
Epoch [39/120    avg_loss:0.053, val_acc:0.953]
Epoch [40/120    avg_loss:0.042, val_acc:0.957]
Epoch [41/120    avg_loss:0.043, val_acc:0.959]
Epoch [42/120    avg_loss:0.042, val_acc:0.943]
Epoch [43/120    avg_loss:0.066, val_acc:0.950]
Epoch [44/120    avg_loss:0.083, val_acc:0.953]
Epoch [45/120    avg_loss:0.042, val_acc:0.963]
Epoch [46/120    avg_loss:0.037, val_acc:0.962]
Epoch [47/120    avg_loss:0.027, val_acc:0.957]
Epoch [48/120    avg_loss:0.049, val_acc:0.963]
Epoch [49/120    avg_loss:0.037, val_acc:0.964]
Epoch [50/120    avg_loss:0.032, val_acc:0.956]
Epoch [51/120    avg_loss:0.036, val_acc:0.943]
Epoch [52/120    avg_loss:0.029, val_acc:0.962]
Epoch [53/120    avg_loss:0.034, val_acc:0.964]
Epoch [54/120    avg_loss:0.021, val_acc:0.964]
Epoch [55/120    avg_loss:0.018, val_acc:0.970]
Epoch [56/120    avg_loss:0.014, val_acc:0.967]
Epoch [57/120    avg_loss:0.017, val_acc:0.964]
Epoch [58/120    avg_loss:0.032, val_acc:0.968]
Epoch [59/120    avg_loss:0.021, val_acc:0.971]
Epoch [60/120    avg_loss:0.016, val_acc:0.970]
Epoch [61/120    avg_loss:0.016, val_acc:0.974]
Epoch [62/120    avg_loss:0.013, val_acc:0.973]
Epoch [63/120    avg_loss:0.021, val_acc:0.966]
Epoch [64/120    avg_loss:0.022, val_acc:0.969]
Epoch [65/120    avg_loss:0.020, val_acc:0.976]
Epoch [66/120    avg_loss:0.020, val_acc:0.976]
Epoch [67/120    avg_loss:0.015, val_acc:0.972]
Epoch [68/120    avg_loss:0.056, val_acc:0.963]
Epoch [69/120    avg_loss:0.026, val_acc:0.972]
Epoch [70/120    avg_loss:0.032, val_acc:0.971]
Epoch [71/120    avg_loss:0.017, val_acc:0.972]
Epoch [72/120    avg_loss:0.013, val_acc:0.971]
Epoch [73/120    avg_loss:0.016, val_acc:0.962]
Epoch [74/120    avg_loss:0.023, val_acc:0.959]
Epoch [75/120    avg_loss:0.033, val_acc:0.960]
Epoch [76/120    avg_loss:0.016, val_acc:0.971]
Epoch [77/120    avg_loss:0.009, val_acc:0.974]
Epoch [78/120    avg_loss:0.009, val_acc:0.968]
Epoch [79/120    avg_loss:0.012, val_acc:0.978]
Epoch [80/120    avg_loss:0.007, val_acc:0.981]
Epoch [81/120    avg_loss:0.009, val_acc:0.975]
Epoch [82/120    avg_loss:0.010, val_acc:0.975]
Epoch [83/120    avg_loss:0.007, val_acc:0.973]
Epoch [84/120    avg_loss:0.011, val_acc:0.974]
Epoch [85/120    avg_loss:0.011, val_acc:0.972]
Epoch [86/120    avg_loss:0.041, val_acc:0.964]
Epoch [87/120    avg_loss:0.028, val_acc:0.976]
Epoch [88/120    avg_loss:0.082, val_acc:0.955]
Epoch [89/120    avg_loss:0.045, val_acc:0.964]
Epoch [90/120    avg_loss:0.018, val_acc:0.968]
Epoch [91/120    avg_loss:0.016, val_acc:0.966]
Epoch [92/120    avg_loss:0.010, val_acc:0.975]
Epoch [93/120    avg_loss:0.008, val_acc:0.974]
Epoch [94/120    avg_loss:0.006, val_acc:0.976]
Epoch [95/120    avg_loss:0.010, val_acc:0.976]
Epoch [96/120    avg_loss:0.008, val_acc:0.975]
Epoch [97/120    avg_loss:0.007, val_acc:0.974]
Epoch [98/120    avg_loss:0.006, val_acc:0.974]
Epoch [99/120    avg_loss:0.009, val_acc:0.975]
Epoch [100/120    avg_loss:0.006, val_acc:0.977]
Epoch [101/120    avg_loss:0.009, val_acc:0.977]
Epoch [102/120    avg_loss:0.007, val_acc:0.976]
Epoch [103/120    avg_loss:0.004, val_acc:0.976]
Epoch [104/120    avg_loss:0.007, val_acc:0.976]
Epoch [105/120    avg_loss:0.006, val_acc:0.976]
Epoch [106/120    avg_loss:0.006, val_acc:0.976]
Epoch [107/120    avg_loss:0.010, val_acc:0.976]
Epoch [108/120    avg_loss:0.008, val_acc:0.975]
Epoch [109/120    avg_loss:0.007, val_acc:0.975]
Epoch [110/120    avg_loss:0.006, val_acc:0.975]
Epoch [111/120    avg_loss:0.006, val_acc:0.975]
Epoch [112/120    avg_loss:0.005, val_acc:0.975]
Epoch [113/120    avg_loss:0.005, val_acc:0.975]
Epoch [114/120    avg_loss:0.005, val_acc:0.975]
Epoch [115/120    avg_loss:0.006, val_acc:0.975]
Epoch [116/120    avg_loss:0.006, val_acc:0.975]
Epoch [117/120    avg_loss:0.006, val_acc:0.975]
Epoch [118/120    avg_loss:0.004, val_acc:0.975]
Epoch [119/120    avg_loss:0.005, val_acc:0.975]
Epoch [120/120    avg_loss:0.006, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1263    2    0    0    0    0    0    1    9    8    2    0
     0    0    0]
 [   0    0    0  721    0    5    0    0    0   14    1    0    4    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0   10    1    0    0    0  862    0    0    0
     0    1    0]
 [   0    0    3    0    0    0    6    0    0    0    3 2197    0    1
     0    0    0]
 [   0    0    0    0    0    8    0    0    0    0    2    0  521    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    2    0    0    0
  1130    3    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    37  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.31978319783198

F1 scores:
[       nan 1.         0.98981191 0.98095238 1.         0.9675252
 0.98350825 0.98039216 0.99650757 0.67924528 0.98289624 0.99501812
 0.97840376 0.9919571  0.98005204 0.91499227 0.9704142 ]

Kappa:
0.9808462435356112
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fde1420f6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.347, val_acc:0.509]
Epoch [2/120    avg_loss:1.735, val_acc:0.619]
Epoch [3/120    avg_loss:1.413, val_acc:0.672]
Epoch [4/120    avg_loss:1.068, val_acc:0.756]
Epoch [5/120    avg_loss:0.903, val_acc:0.740]
Epoch [6/120    avg_loss:0.784, val_acc:0.741]
Epoch [7/120    avg_loss:0.727, val_acc:0.763]
Epoch [8/120    avg_loss:0.705, val_acc:0.741]
Epoch [9/120    avg_loss:0.493, val_acc:0.786]
Epoch [10/120    avg_loss:0.486, val_acc:0.832]
Epoch [11/120    avg_loss:0.432, val_acc:0.871]
Epoch [12/120    avg_loss:0.368, val_acc:0.891]
Epoch [13/120    avg_loss:0.275, val_acc:0.893]
Epoch [14/120    avg_loss:0.217, val_acc:0.899]
Epoch [15/120    avg_loss:0.218, val_acc:0.904]
Epoch [16/120    avg_loss:0.244, val_acc:0.905]
Epoch [17/120    avg_loss:0.234, val_acc:0.880]
Epoch [18/120    avg_loss:0.185, val_acc:0.898]
Epoch [19/120    avg_loss:0.194, val_acc:0.930]
Epoch [20/120    avg_loss:0.148, val_acc:0.938]
Epoch [21/120    avg_loss:0.108, val_acc:0.936]
Epoch [22/120    avg_loss:0.108, val_acc:0.939]
Epoch [23/120    avg_loss:0.101, val_acc:0.909]
Epoch [24/120    avg_loss:0.151, val_acc:0.945]
Epoch [25/120    avg_loss:0.146, val_acc:0.952]
Epoch [26/120    avg_loss:0.139, val_acc:0.946]
Epoch [27/120    avg_loss:0.096, val_acc:0.959]
Epoch [28/120    avg_loss:0.075, val_acc:0.948]
Epoch [29/120    avg_loss:0.079, val_acc:0.949]
Epoch [30/120    avg_loss:0.184, val_acc:0.923]
Epoch [31/120    avg_loss:0.131, val_acc:0.957]
Epoch [32/120    avg_loss:0.081, val_acc:0.960]
Epoch [33/120    avg_loss:0.063, val_acc:0.957]
Epoch [34/120    avg_loss:0.050, val_acc:0.948]
Epoch [35/120    avg_loss:0.058, val_acc:0.963]
Epoch [36/120    avg_loss:0.051, val_acc:0.971]
Epoch [37/120    avg_loss:0.043, val_acc:0.972]
Epoch [38/120    avg_loss:0.047, val_acc:0.967]
Epoch [39/120    avg_loss:0.056, val_acc:0.966]
Epoch [40/120    avg_loss:0.062, val_acc:0.959]
Epoch [41/120    avg_loss:0.043, val_acc:0.968]
Epoch [42/120    avg_loss:0.064, val_acc:0.961]
Epoch [43/120    avg_loss:0.043, val_acc:0.970]
Epoch [44/120    avg_loss:0.024, val_acc:0.972]
Epoch [45/120    avg_loss:0.026, val_acc:0.969]
Epoch [46/120    avg_loss:0.028, val_acc:0.978]
Epoch [47/120    avg_loss:0.022, val_acc:0.971]
Epoch [48/120    avg_loss:0.049, val_acc:0.974]
Epoch [49/120    avg_loss:0.037, val_acc:0.964]
Epoch [50/120    avg_loss:0.065, val_acc:0.972]
Epoch [51/120    avg_loss:0.039, val_acc:0.966]
Epoch [52/120    avg_loss:0.022, val_acc:0.981]
Epoch [53/120    avg_loss:0.041, val_acc:0.954]
Epoch [54/120    avg_loss:0.028, val_acc:0.982]
Epoch [55/120    avg_loss:0.020, val_acc:0.978]
Epoch [56/120    avg_loss:0.018, val_acc:0.985]
Epoch [57/120    avg_loss:0.017, val_acc:0.971]
Epoch [58/120    avg_loss:0.021, val_acc:0.983]
Epoch [59/120    avg_loss:0.016, val_acc:0.983]
Epoch [60/120    avg_loss:0.017, val_acc:0.977]
Epoch [61/120    avg_loss:0.040, val_acc:0.950]
Epoch [62/120    avg_loss:0.036, val_acc:0.972]
Epoch [63/120    avg_loss:0.018, val_acc:0.975]
Epoch [64/120    avg_loss:0.014, val_acc:0.985]
Epoch [65/120    avg_loss:0.012, val_acc:0.985]
Epoch [66/120    avg_loss:0.014, val_acc:0.982]
Epoch [67/120    avg_loss:0.027, val_acc:0.966]
Epoch [68/120    avg_loss:0.029, val_acc:0.978]
Epoch [69/120    avg_loss:0.023, val_acc:0.978]
Epoch [70/120    avg_loss:0.018, val_acc:0.971]
Epoch [71/120    avg_loss:0.012, val_acc:0.983]
Epoch [72/120    avg_loss:0.018, val_acc:0.987]
Epoch [73/120    avg_loss:0.011, val_acc:0.983]
Epoch [74/120    avg_loss:0.017, val_acc:0.981]
Epoch [75/120    avg_loss:0.008, val_acc:0.981]
Epoch [76/120    avg_loss:0.007, val_acc:0.982]
Epoch [77/120    avg_loss:0.007, val_acc:0.984]
Epoch [78/120    avg_loss:0.007, val_acc:0.980]
Epoch [79/120    avg_loss:0.008, val_acc:0.981]
Epoch [80/120    avg_loss:0.006, val_acc:0.981]
Epoch [81/120    avg_loss:0.049, val_acc:0.962]
Epoch [82/120    avg_loss:0.270, val_acc:0.912]
Epoch [83/120    avg_loss:0.203, val_acc:0.932]
Epoch [84/120    avg_loss:0.096, val_acc:0.962]
Epoch [85/120    avg_loss:0.076, val_acc:0.956]
Epoch [86/120    avg_loss:0.054, val_acc:0.975]
Epoch [87/120    avg_loss:0.027, val_acc:0.975]
Epoch [88/120    avg_loss:0.026, val_acc:0.975]
Epoch [89/120    avg_loss:0.022, val_acc:0.975]
Epoch [90/120    avg_loss:0.025, val_acc:0.978]
Epoch [91/120    avg_loss:0.021, val_acc:0.977]
Epoch [92/120    avg_loss:0.020, val_acc:0.981]
Epoch [93/120    avg_loss:0.018, val_acc:0.982]
Epoch [94/120    avg_loss:0.020, val_acc:0.983]
Epoch [95/120    avg_loss:0.019, val_acc:0.983]
Epoch [96/120    avg_loss:0.020, val_acc:0.983]
Epoch [97/120    avg_loss:0.016, val_acc:0.983]
Epoch [98/120    avg_loss:0.014, val_acc:0.983]
Epoch [99/120    avg_loss:0.018, val_acc:0.983]
Epoch [100/120    avg_loss:0.016, val_acc:0.983]
Epoch [101/120    avg_loss:0.015, val_acc:0.983]
Epoch [102/120    avg_loss:0.017, val_acc:0.983]
Epoch [103/120    avg_loss:0.017, val_acc:0.982]
Epoch [104/120    avg_loss:0.013, val_acc:0.984]
Epoch [105/120    avg_loss:0.015, val_acc:0.983]
Epoch [106/120    avg_loss:0.019, val_acc:0.984]
Epoch [107/120    avg_loss:0.015, val_acc:0.982]
Epoch [108/120    avg_loss:0.014, val_acc:0.984]
Epoch [109/120    avg_loss:0.014, val_acc:0.984]
Epoch [110/120    avg_loss:0.017, val_acc:0.984]
Epoch [111/120    avg_loss:0.021, val_acc:0.984]
Epoch [112/120    avg_loss:0.023, val_acc:0.984]
Epoch [113/120    avg_loss:0.014, val_acc:0.984]
Epoch [114/120    avg_loss:0.017, val_acc:0.984]
Epoch [115/120    avg_loss:0.017, val_acc:0.984]
Epoch [116/120    avg_loss:0.015, val_acc:0.984]
Epoch [117/120    avg_loss:0.013, val_acc:0.984]
Epoch [118/120    avg_loss:0.014, val_acc:0.984]
Epoch [119/120    avg_loss:0.016, val_acc:0.984]
Epoch [120/120    avg_loss:0.015, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1266    1    2    0    0    0    0    1    5    8    1    0
     0    1    0]
 [   0    0    0  713    3    4    0    0    0    6    1    0   17    3
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    3    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    9    0    0    0    0  832   12    4    0
     3    5    0]
 [   0    0    7    0    0    3    1    0    0    0    7 2181   10    1
     0    0    0]
 [   0    0    0    0    1    0    0    0    0    0    1    1  530    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    2    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    69  274    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.65853658536585

F1 scores:
[       nan 0.98765432 0.98559751 0.9753762  0.9837587  0.97853107
 0.99315589 1.         0.997669   0.75       0.9657574  0.98844324
 0.96451319 0.98930481 0.96804431 0.87400319 0.98809524]

Kappa:
0.9732994985837183
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8301cb2780>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.308, val_acc:0.522]
Epoch [2/120    avg_loss:1.679, val_acc:0.629]
Epoch [3/120    avg_loss:1.395, val_acc:0.639]
Epoch [4/120    avg_loss:1.101, val_acc:0.677]
Epoch [5/120    avg_loss:0.960, val_acc:0.694]
Epoch [6/120    avg_loss:0.713, val_acc:0.775]
Epoch [7/120    avg_loss:0.695, val_acc:0.797]
Epoch [8/120    avg_loss:0.562, val_acc:0.826]
Epoch [9/120    avg_loss:0.526, val_acc:0.827]
Epoch [10/120    avg_loss:0.518, val_acc:0.835]
Epoch [11/120    avg_loss:0.449, val_acc:0.797]
Epoch [12/120    avg_loss:0.420, val_acc:0.820]
Epoch [13/120    avg_loss:0.345, val_acc:0.883]
Epoch [14/120    avg_loss:0.261, val_acc:0.894]
Epoch [15/120    avg_loss:0.286, val_acc:0.897]
Epoch [16/120    avg_loss:0.230, val_acc:0.913]
Epoch [17/120    avg_loss:0.204, val_acc:0.914]
Epoch [18/120    avg_loss:0.192, val_acc:0.921]
Epoch [19/120    avg_loss:0.150, val_acc:0.928]
Epoch [20/120    avg_loss:0.159, val_acc:0.916]
Epoch [21/120    avg_loss:0.157, val_acc:0.926]
Epoch [22/120    avg_loss:0.144, val_acc:0.916]
Epoch [23/120    avg_loss:0.152, val_acc:0.923]
Epoch [24/120    avg_loss:0.122, val_acc:0.910]
Epoch [25/120    avg_loss:0.172, val_acc:0.926]
Epoch [26/120    avg_loss:0.145, val_acc:0.915]
Epoch [27/120    avg_loss:0.105, val_acc:0.927]
Epoch [28/120    avg_loss:0.104, val_acc:0.928]
Epoch [29/120    avg_loss:0.114, val_acc:0.925]
Epoch [30/120    avg_loss:0.106, val_acc:0.931]
Epoch [31/120    avg_loss:0.095, val_acc:0.936]
Epoch [32/120    avg_loss:0.092, val_acc:0.953]
Epoch [33/120    avg_loss:0.065, val_acc:0.947]
Epoch [34/120    avg_loss:0.066, val_acc:0.947]
Epoch [35/120    avg_loss:0.071, val_acc:0.950]
Epoch [36/120    avg_loss:0.073, val_acc:0.953]
Epoch [37/120    avg_loss:0.057, val_acc:0.946]
Epoch [38/120    avg_loss:0.056, val_acc:0.952]
Epoch [39/120    avg_loss:0.042, val_acc:0.964]
Epoch [40/120    avg_loss:0.065, val_acc:0.936]
Epoch [41/120    avg_loss:0.073, val_acc:0.957]
Epoch [42/120    avg_loss:0.048, val_acc:0.959]
Epoch [43/120    avg_loss:0.047, val_acc:0.964]
Epoch [44/120    avg_loss:0.048, val_acc:0.957]
Epoch [45/120    avg_loss:0.109, val_acc:0.942]
Epoch [46/120    avg_loss:0.049, val_acc:0.959]
Epoch [47/120    avg_loss:0.061, val_acc:0.964]
Epoch [48/120    avg_loss:0.037, val_acc:0.948]
Epoch [49/120    avg_loss:0.041, val_acc:0.965]
Epoch [50/120    avg_loss:0.028, val_acc:0.970]
Epoch [51/120    avg_loss:0.037, val_acc:0.961]
Epoch [52/120    avg_loss:0.038, val_acc:0.968]
Epoch [53/120    avg_loss:0.029, val_acc:0.974]
Epoch [54/120    avg_loss:0.046, val_acc:0.966]
Epoch [55/120    avg_loss:0.046, val_acc:0.968]
Epoch [56/120    avg_loss:0.035, val_acc:0.971]
Epoch [57/120    avg_loss:0.038, val_acc:0.970]
Epoch [58/120    avg_loss:0.023, val_acc:0.974]
Epoch [59/120    avg_loss:0.021, val_acc:0.967]
Epoch [60/120    avg_loss:0.053, val_acc:0.954]
Epoch [61/120    avg_loss:0.032, val_acc:0.964]
Epoch [62/120    avg_loss:0.022, val_acc:0.975]
Epoch [63/120    avg_loss:0.054, val_acc:0.953]
Epoch [64/120    avg_loss:0.047, val_acc:0.967]
Epoch [65/120    avg_loss:0.015, val_acc:0.971]
Epoch [66/120    avg_loss:0.017, val_acc:0.975]
Epoch [67/120    avg_loss:0.014, val_acc:0.974]
Epoch [68/120    avg_loss:0.018, val_acc:0.974]
Epoch [69/120    avg_loss:0.018, val_acc:0.970]
Epoch [70/120    avg_loss:0.038, val_acc:0.975]
Epoch [71/120    avg_loss:0.019, val_acc:0.971]
Epoch [72/120    avg_loss:0.015, val_acc:0.971]
Epoch [73/120    avg_loss:0.018, val_acc:0.975]
Epoch [74/120    avg_loss:0.019, val_acc:0.967]
Epoch [75/120    avg_loss:0.029, val_acc:0.976]
Epoch [76/120    avg_loss:0.013, val_acc:0.979]
Epoch [77/120    avg_loss:0.025, val_acc:0.966]
Epoch [78/120    avg_loss:0.018, val_acc:0.978]
Epoch [79/120    avg_loss:0.028, val_acc:0.962]
Epoch [80/120    avg_loss:0.025, val_acc:0.967]
Epoch [81/120    avg_loss:0.011, val_acc:0.969]
Epoch [82/120    avg_loss:0.017, val_acc:0.977]
Epoch [83/120    avg_loss:0.016, val_acc:0.976]
Epoch [84/120    avg_loss:0.012, val_acc:0.976]
Epoch [85/120    avg_loss:0.008, val_acc:0.983]
Epoch [86/120    avg_loss:0.010, val_acc:0.976]
Epoch [87/120    avg_loss:0.016, val_acc:0.969]
Epoch [88/120    avg_loss:0.009, val_acc:0.985]
Epoch [89/120    avg_loss:0.007, val_acc:0.979]
Epoch [90/120    avg_loss:0.016, val_acc:0.957]
Epoch [91/120    avg_loss:0.015, val_acc:0.978]
Epoch [92/120    avg_loss:0.007, val_acc:0.980]
Epoch [93/120    avg_loss:0.015, val_acc:0.983]
Epoch [94/120    avg_loss:0.007, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.983]
Epoch [96/120    avg_loss:0.005, val_acc:0.984]
Epoch [97/120    avg_loss:0.006, val_acc:0.982]
Epoch [98/120    avg_loss:0.009, val_acc:0.976]
Epoch [99/120    avg_loss:0.009, val_acc:0.984]
Epoch [100/120    avg_loss:0.009, val_acc:0.982]
Epoch [101/120    avg_loss:0.006, val_acc:0.985]
Epoch [102/120    avg_loss:0.008, val_acc:0.978]
Epoch [103/120    avg_loss:0.007, val_acc:0.982]
Epoch [104/120    avg_loss:0.005, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.983]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.008, val_acc:0.985]
Epoch [108/120    avg_loss:0.004, val_acc:0.986]
Epoch [109/120    avg_loss:0.010, val_acc:0.982]
Epoch [110/120    avg_loss:0.011, val_acc:0.974]
Epoch [111/120    avg_loss:0.012, val_acc:0.978]
Epoch [112/120    avg_loss:0.042, val_acc:0.964]
Epoch [113/120    avg_loss:0.048, val_acc:0.951]
Epoch [114/120    avg_loss:0.039, val_acc:0.977]
Epoch [115/120    avg_loss:0.023, val_acc:0.970]
Epoch [116/120    avg_loss:0.058, val_acc:0.967]
Epoch [117/120    avg_loss:0.039, val_acc:0.967]
Epoch [118/120    avg_loss:0.018, val_acc:0.972]
Epoch [119/120    avg_loss:0.041, val_acc:0.969]
Epoch [120/120    avg_loss:0.018, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1263    0    0    0    0    0    0    1    3   18    0    0
     0    0    0]
 [   0    0    1  671   27    5    0    0    0   10    0    0   25    7
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7   11    0    7    0    0    0    0  812   37    0    0
     0    1    0]
 [   0    0    3    0    0    0    3    0    0    0    5 2187   11    1
     0    0    0]
 [   0    0    2    0    0   13    0    0    0    0    0    0  518    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    48  297    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.13821138211382

F1 scores:
[       nan 0.975      0.98556379 0.93846154 0.9380531  0.96860987
 0.99315589 1.         0.997669   0.76595745 0.95811209 0.98203862
 0.94871795 0.97883598 0.97684391 0.92093023 0.98203593]

Kappa:
0.9673537902485447
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f80e35b2780>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.324, val_acc:0.513]
Epoch [2/120    avg_loss:1.710, val_acc:0.555]
Epoch [3/120    avg_loss:1.513, val_acc:0.582]
Epoch [4/120    avg_loss:1.203, val_acc:0.702]
Epoch [5/120    avg_loss:1.034, val_acc:0.732]
Epoch [6/120    avg_loss:0.828, val_acc:0.758]
Epoch [7/120    avg_loss:0.684, val_acc:0.755]
Epoch [8/120    avg_loss:0.645, val_acc:0.826]
Epoch [9/120    avg_loss:0.509, val_acc:0.820]
Epoch [10/120    avg_loss:0.643, val_acc:0.824]
Epoch [11/120    avg_loss:0.516, val_acc:0.821]
Epoch [12/120    avg_loss:0.366, val_acc:0.833]
Epoch [13/120    avg_loss:0.330, val_acc:0.875]
Epoch [14/120    avg_loss:0.284, val_acc:0.864]
Epoch [15/120    avg_loss:0.285, val_acc:0.871]
Epoch [16/120    avg_loss:0.223, val_acc:0.894]
Epoch [17/120    avg_loss:0.195, val_acc:0.897]
Epoch [18/120    avg_loss:0.222, val_acc:0.898]
Epoch [19/120    avg_loss:0.245, val_acc:0.882]
Epoch [20/120    avg_loss:0.168, val_acc:0.908]
Epoch [21/120    avg_loss:0.132, val_acc:0.926]
Epoch [22/120    avg_loss:0.124, val_acc:0.933]
Epoch [23/120    avg_loss:0.111, val_acc:0.930]
Epoch [24/120    avg_loss:0.085, val_acc:0.941]
Epoch [25/120    avg_loss:0.080, val_acc:0.938]
Epoch [26/120    avg_loss:0.100, val_acc:0.928]
Epoch [27/120    avg_loss:0.107, val_acc:0.931]
Epoch [28/120    avg_loss:0.098, val_acc:0.918]
Epoch [29/120    avg_loss:0.116, val_acc:0.933]
Epoch [30/120    avg_loss:0.169, val_acc:0.926]
Epoch [31/120    avg_loss:0.143, val_acc:0.947]
Epoch [32/120    avg_loss:0.097, val_acc:0.946]
Epoch [33/120    avg_loss:0.084, val_acc:0.942]
Epoch [34/120    avg_loss:0.085, val_acc:0.944]
Epoch [35/120    avg_loss:0.073, val_acc:0.954]
Epoch [36/120    avg_loss:0.052, val_acc:0.966]
Epoch [37/120    avg_loss:0.073, val_acc:0.941]
Epoch [38/120    avg_loss:0.065, val_acc:0.952]
Epoch [39/120    avg_loss:0.043, val_acc:0.951]
Epoch [40/120    avg_loss:0.038, val_acc:0.963]
Epoch [41/120    avg_loss:0.038, val_acc:0.965]
Epoch [42/120    avg_loss:0.044, val_acc:0.962]
Epoch [43/120    avg_loss:0.043, val_acc:0.961]
Epoch [44/120    avg_loss:0.026, val_acc:0.968]
Epoch [45/120    avg_loss:0.032, val_acc:0.962]
Epoch [46/120    avg_loss:0.027, val_acc:0.968]
Epoch [47/120    avg_loss:0.039, val_acc:0.964]
Epoch [48/120    avg_loss:0.024, val_acc:0.961]
Epoch [49/120    avg_loss:0.021, val_acc:0.972]
Epoch [50/120    avg_loss:0.027, val_acc:0.967]
Epoch [51/120    avg_loss:0.027, val_acc:0.975]
Epoch [52/120    avg_loss:0.032, val_acc:0.964]
Epoch [53/120    avg_loss:0.026, val_acc:0.964]
Epoch [54/120    avg_loss:0.016, val_acc:0.966]
Epoch [55/120    avg_loss:0.022, val_acc:0.968]
Epoch [56/120    avg_loss:0.025, val_acc:0.976]
Epoch [57/120    avg_loss:0.059, val_acc:0.930]
Epoch [58/120    avg_loss:0.083, val_acc:0.950]
Epoch [59/120    avg_loss:0.047, val_acc:0.960]
Epoch [60/120    avg_loss:0.042, val_acc:0.927]
Epoch [61/120    avg_loss:0.664, val_acc:0.785]
Epoch [62/120    avg_loss:0.288, val_acc:0.862]
Epoch [63/120    avg_loss:0.158, val_acc:0.921]
Epoch [64/120    avg_loss:0.115, val_acc:0.939]
Epoch [65/120    avg_loss:0.080, val_acc:0.939]
Epoch [66/120    avg_loss:0.087, val_acc:0.944]
Epoch [67/120    avg_loss:0.051, val_acc:0.954]
Epoch [68/120    avg_loss:0.082, val_acc:0.957]
Epoch [69/120    avg_loss:0.044, val_acc:0.961]
Epoch [70/120    avg_loss:0.035, val_acc:0.968]
Epoch [71/120    avg_loss:0.023, val_acc:0.971]
Epoch [72/120    avg_loss:0.022, val_acc:0.975]
Epoch [73/120    avg_loss:0.022, val_acc:0.975]
Epoch [74/120    avg_loss:0.022, val_acc:0.975]
Epoch [75/120    avg_loss:0.021, val_acc:0.974]
Epoch [76/120    avg_loss:0.024, val_acc:0.976]
Epoch [77/120    avg_loss:0.017, val_acc:0.978]
Epoch [78/120    avg_loss:0.019, val_acc:0.976]
Epoch [79/120    avg_loss:0.018, val_acc:0.976]
Epoch [80/120    avg_loss:0.020, val_acc:0.977]
Epoch [81/120    avg_loss:0.017, val_acc:0.979]
Epoch [82/120    avg_loss:0.018, val_acc:0.976]
Epoch [83/120    avg_loss:0.017, val_acc:0.978]
Epoch [84/120    avg_loss:0.019, val_acc:0.978]
Epoch [85/120    avg_loss:0.017, val_acc:0.976]
Epoch [86/120    avg_loss:0.014, val_acc:0.978]
Epoch [87/120    avg_loss:0.013, val_acc:0.978]
Epoch [88/120    avg_loss:0.015, val_acc:0.979]
Epoch [89/120    avg_loss:0.016, val_acc:0.977]
Epoch [90/120    avg_loss:0.017, val_acc:0.980]
Epoch [91/120    avg_loss:0.013, val_acc:0.978]
Epoch [92/120    avg_loss:0.016, val_acc:0.979]
Epoch [93/120    avg_loss:0.022, val_acc:0.978]
Epoch [94/120    avg_loss:0.013, val_acc:0.979]
Epoch [95/120    avg_loss:0.012, val_acc:0.978]
Epoch [96/120    avg_loss:0.013, val_acc:0.979]
Epoch [97/120    avg_loss:0.014, val_acc:0.977]
Epoch [98/120    avg_loss:0.014, val_acc:0.979]
Epoch [99/120    avg_loss:0.016, val_acc:0.979]
Epoch [100/120    avg_loss:0.013, val_acc:0.977]
Epoch [101/120    avg_loss:0.015, val_acc:0.978]
Epoch [102/120    avg_loss:0.013, val_acc:0.980]
Epoch [103/120    avg_loss:0.017, val_acc:0.979]
Epoch [104/120    avg_loss:0.013, val_acc:0.979]
Epoch [105/120    avg_loss:0.013, val_acc:0.978]
Epoch [106/120    avg_loss:0.013, val_acc:0.977]
Epoch [107/120    avg_loss:0.014, val_acc:0.980]
Epoch [108/120    avg_loss:0.013, val_acc:0.980]
Epoch [109/120    avg_loss:0.015, val_acc:0.980]
Epoch [110/120    avg_loss:0.012, val_acc:0.980]
Epoch [111/120    avg_loss:0.014, val_acc:0.980]
Epoch [112/120    avg_loss:0.013, val_acc:0.980]
Epoch [113/120    avg_loss:0.013, val_acc:0.980]
Epoch [114/120    avg_loss:0.011, val_acc:0.980]
Epoch [115/120    avg_loss:0.011, val_acc:0.980]
Epoch [116/120    avg_loss:0.017, val_acc:0.980]
Epoch [117/120    avg_loss:0.012, val_acc:0.980]
Epoch [118/120    avg_loss:0.012, val_acc:0.980]
Epoch [119/120    avg_loss:0.011, val_acc:0.979]
Epoch [120/120    avg_loss:0.013, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1259    0    4    1    2    0    0    0    7   12    0    0
     0    0    0]
 [   0    0    1  707    0   15    0    0    0    9    0    0   11    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    4    2    0    0    0  856    7    0    0
     0    4    0]
 [   0    0    4    0    0    0    4    0    0    0    3 2184   13    2
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0  529    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1122   13    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
     8  323    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.18970189701896

F1 scores:
[       nan 1.         0.9870639  0.97248968 0.99069767 0.97072072
 0.97977528 0.98039216 0.99883856 0.76595745 0.98165138 0.98913043
 0.9706422  0.98404255 0.98854626 0.94032023 0.96428571]

Kappa:
0.9793730991338301
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:20:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efe17b73668>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.287, val_acc:0.504]
Epoch [2/120    avg_loss:1.739, val_acc:0.597]
Epoch [3/120    avg_loss:1.478, val_acc:0.584]
Epoch [4/120    avg_loss:1.273, val_acc:0.678]
Epoch [5/120    avg_loss:1.047, val_acc:0.720]
Epoch [6/120    avg_loss:0.873, val_acc:0.759]
Epoch [7/120    avg_loss:0.719, val_acc:0.787]
Epoch [8/120    avg_loss:0.673, val_acc:0.769]
Epoch [9/120    avg_loss:0.652, val_acc:0.772]
Epoch [10/120    avg_loss:0.577, val_acc:0.849]
Epoch [11/120    avg_loss:0.456, val_acc:0.828]
Epoch [12/120    avg_loss:0.454, val_acc:0.837]
Epoch [13/120    avg_loss:0.430, val_acc:0.893]
Epoch [14/120    avg_loss:0.415, val_acc:0.863]
Epoch [15/120    avg_loss:0.313, val_acc:0.831]
Epoch [16/120    avg_loss:0.301, val_acc:0.901]
Epoch [17/120    avg_loss:0.301, val_acc:0.879]
Epoch [18/120    avg_loss:0.291, val_acc:0.919]
Epoch [19/120    avg_loss:0.220, val_acc:0.923]
Epoch [20/120    avg_loss:0.183, val_acc:0.925]
Epoch [21/120    avg_loss:0.189, val_acc:0.908]
Epoch [22/120    avg_loss:0.183, val_acc:0.918]
Epoch [23/120    avg_loss:0.138, val_acc:0.956]
Epoch [24/120    avg_loss:0.156, val_acc:0.923]
Epoch [25/120    avg_loss:0.125, val_acc:0.949]
Epoch [26/120    avg_loss:0.126, val_acc:0.889]
Epoch [27/120    avg_loss:0.133, val_acc:0.939]
Epoch [28/120    avg_loss:0.091, val_acc:0.955]
Epoch [29/120    avg_loss:0.118, val_acc:0.952]
Epoch [30/120    avg_loss:0.133, val_acc:0.947]
Epoch [31/120    avg_loss:0.091, val_acc:0.961]
Epoch [32/120    avg_loss:0.084, val_acc:0.969]
Epoch [33/120    avg_loss:0.105, val_acc:0.959]
Epoch [34/120    avg_loss:0.084, val_acc:0.966]
Epoch [35/120    avg_loss:0.052, val_acc:0.956]
Epoch [36/120    avg_loss:0.058, val_acc:0.974]
Epoch [37/120    avg_loss:0.071, val_acc:0.968]
Epoch [38/120    avg_loss:0.075, val_acc:0.960]
Epoch [39/120    avg_loss:0.049, val_acc:0.968]
Epoch [40/120    avg_loss:0.052, val_acc:0.968]
Epoch [41/120    avg_loss:0.085, val_acc:0.947]
Epoch [42/120    avg_loss:0.122, val_acc:0.942]
Epoch [43/120    avg_loss:0.080, val_acc:0.955]
Epoch [44/120    avg_loss:0.053, val_acc:0.961]
Epoch [45/120    avg_loss:0.043, val_acc:0.968]
Epoch [46/120    avg_loss:0.080, val_acc:0.944]
Epoch [47/120    avg_loss:0.068, val_acc:0.972]
Epoch [48/120    avg_loss:0.044, val_acc:0.953]
Epoch [49/120    avg_loss:0.076, val_acc:0.957]
Epoch [50/120    avg_loss:0.054, val_acc:0.971]
Epoch [51/120    avg_loss:0.033, val_acc:0.972]
Epoch [52/120    avg_loss:0.028, val_acc:0.976]
Epoch [53/120    avg_loss:0.027, val_acc:0.976]
Epoch [54/120    avg_loss:0.029, val_acc:0.978]
Epoch [55/120    avg_loss:0.022, val_acc:0.977]
Epoch [56/120    avg_loss:0.024, val_acc:0.980]
Epoch [57/120    avg_loss:0.024, val_acc:0.982]
Epoch [58/120    avg_loss:0.019, val_acc:0.982]
Epoch [59/120    avg_loss:0.025, val_acc:0.980]
Epoch [60/120    avg_loss:0.021, val_acc:0.981]
Epoch [61/120    avg_loss:0.019, val_acc:0.984]
Epoch [62/120    avg_loss:0.018, val_acc:0.981]
Epoch [63/120    avg_loss:0.018, val_acc:0.982]
Epoch [64/120    avg_loss:0.018, val_acc:0.982]
Epoch [65/120    avg_loss:0.021, val_acc:0.983]
Epoch [66/120    avg_loss:0.022, val_acc:0.982]
Epoch [67/120    avg_loss:0.017, val_acc:0.983]
Epoch [68/120    avg_loss:0.016, val_acc:0.982]
Epoch [69/120    avg_loss:0.020, val_acc:0.981]
Epoch [70/120    avg_loss:0.016, val_acc:0.981]
Epoch [71/120    avg_loss:0.021, val_acc:0.982]
Epoch [72/120    avg_loss:0.015, val_acc:0.982]
Epoch [73/120    avg_loss:0.015, val_acc:0.982]
Epoch [74/120    avg_loss:0.015, val_acc:0.983]
Epoch [75/120    avg_loss:0.018, val_acc:0.983]
Epoch [76/120    avg_loss:0.017, val_acc:0.983]
Epoch [77/120    avg_loss:0.017, val_acc:0.983]
Epoch [78/120    avg_loss:0.017, val_acc:0.982]
Epoch [79/120    avg_loss:0.015, val_acc:0.982]
Epoch [80/120    avg_loss:0.014, val_acc:0.982]
Epoch [81/120    avg_loss:0.016, val_acc:0.983]
Epoch [82/120    avg_loss:0.015, val_acc:0.983]
Epoch [83/120    avg_loss:0.014, val_acc:0.983]
Epoch [84/120    avg_loss:0.017, val_acc:0.983]
Epoch [85/120    avg_loss:0.019, val_acc:0.984]
Epoch [86/120    avg_loss:0.015, val_acc:0.984]
Epoch [87/120    avg_loss:0.017, val_acc:0.984]
Epoch [88/120    avg_loss:0.020, val_acc:0.984]
Epoch [89/120    avg_loss:0.018, val_acc:0.983]
Epoch [90/120    avg_loss:0.016, val_acc:0.983]
Epoch [91/120    avg_loss:0.019, val_acc:0.983]
Epoch [92/120    avg_loss:0.017, val_acc:0.983]
Epoch [93/120    avg_loss:0.016, val_acc:0.983]
Epoch [94/120    avg_loss:0.017, val_acc:0.983]
Epoch [95/120    avg_loss:0.019, val_acc:0.983]
Epoch [96/120    avg_loss:0.015, val_acc:0.983]
Epoch [97/120    avg_loss:0.019, val_acc:0.983]
Epoch [98/120    avg_loss:0.015, val_acc:0.983]
Epoch [99/120    avg_loss:0.013, val_acc:0.983]
Epoch [100/120    avg_loss:0.016, val_acc:0.983]
Epoch [101/120    avg_loss:0.015, val_acc:0.983]
Epoch [102/120    avg_loss:0.016, val_acc:0.983]
Epoch [103/120    avg_loss:0.017, val_acc:0.983]
Epoch [104/120    avg_loss:0.017, val_acc:0.983]
Epoch [105/120    avg_loss:0.019, val_acc:0.983]
Epoch [106/120    avg_loss:0.021, val_acc:0.983]
Epoch [107/120    avg_loss:0.015, val_acc:0.983]
Epoch [108/120    avg_loss:0.018, val_acc:0.983]
Epoch [109/120    avg_loss:0.016, val_acc:0.983]
Epoch [110/120    avg_loss:0.017, val_acc:0.983]
Epoch [111/120    avg_loss:0.016, val_acc:0.983]
Epoch [112/120    avg_loss:0.014, val_acc:0.983]
Epoch [113/120    avg_loss:0.015, val_acc:0.983]
Epoch [114/120    avg_loss:0.013, val_acc:0.983]
Epoch [115/120    avg_loss:0.016, val_acc:0.983]
Epoch [116/120    avg_loss:0.015, val_acc:0.983]
Epoch [117/120    avg_loss:0.016, val_acc:0.983]
Epoch [118/120    avg_loss:0.016, val_acc:0.983]
Epoch [119/120    avg_loss:0.019, val_acc:0.983]
Epoch [120/120    avg_loss:0.014, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1259    1    0    0    0    0    0    6    7   10    2    0
     0    0    0]
 [   0    0    0  728    2    0    0    0    0    7    1    0    7    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    6    2    0    0    0  851    9    0    0
     2    5    0]
 [   0    0    5    0    0    2    4    0    0    0    2 2196    0    1
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    1  528    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    3    0    0    0
  1126    2    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    33  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.33062330623306

F1 scores:
[       nan 1.         0.98783837 0.98644986 0.9953271  0.97968397
 0.98426966 1.         0.995338   0.72       0.9787234  0.99231812
 0.98324022 0.9919571  0.97913043 0.91437309 0.98823529]

Kappa:
0.9809678745116777
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f67bb2fb710>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.371, val_acc:0.426]
Epoch [2/120    avg_loss:1.754, val_acc:0.600]
Epoch [3/120    avg_loss:1.489, val_acc:0.713]
Epoch [4/120    avg_loss:1.209, val_acc:0.667]
Epoch [5/120    avg_loss:0.941, val_acc:0.769]
Epoch [6/120    avg_loss:0.833, val_acc:0.749]
Epoch [7/120    avg_loss:0.773, val_acc:0.805]
Epoch [8/120    avg_loss:0.628, val_acc:0.800]
Epoch [9/120    avg_loss:0.578, val_acc:0.826]
Epoch [10/120    avg_loss:0.529, val_acc:0.850]
Epoch [11/120    avg_loss:0.429, val_acc:0.809]
Epoch [12/120    avg_loss:0.405, val_acc:0.838]
Epoch [13/120    avg_loss:0.427, val_acc:0.865]
Epoch [14/120    avg_loss:0.396, val_acc:0.853]
Epoch [15/120    avg_loss:0.318, val_acc:0.895]
Epoch [16/120    avg_loss:0.317, val_acc:0.898]
Epoch [17/120    avg_loss:0.262, val_acc:0.911]
Epoch [18/120    avg_loss:0.233, val_acc:0.921]
Epoch [19/120    avg_loss:0.223, val_acc:0.912]
Epoch [20/120    avg_loss:0.181, val_acc:0.907]
Epoch [21/120    avg_loss:0.227, val_acc:0.916]
Epoch [22/120    avg_loss:0.217, val_acc:0.917]
Epoch [23/120    avg_loss:0.215, val_acc:0.932]
Epoch [24/120    avg_loss:0.135, val_acc:0.952]
Epoch [25/120    avg_loss:0.137, val_acc:0.943]
Epoch [26/120    avg_loss:0.104, val_acc:0.935]
Epoch [27/120    avg_loss:0.100, val_acc:0.949]
Epoch [28/120    avg_loss:0.084, val_acc:0.953]
Epoch [29/120    avg_loss:0.071, val_acc:0.949]
Epoch [30/120    avg_loss:0.129, val_acc:0.911]
Epoch [31/120    avg_loss:0.144, val_acc:0.944]
Epoch [32/120    avg_loss:0.083, val_acc:0.951]
Epoch [33/120    avg_loss:0.082, val_acc:0.939]
Epoch [34/120    avg_loss:0.091, val_acc:0.956]
Epoch [35/120    avg_loss:0.084, val_acc:0.958]
Epoch [36/120    avg_loss:0.068, val_acc:0.942]
Epoch [37/120    avg_loss:0.057, val_acc:0.959]
Epoch [38/120    avg_loss:0.087, val_acc:0.953]
Epoch [39/120    avg_loss:0.075, val_acc:0.959]
Epoch [40/120    avg_loss:0.063, val_acc:0.941]
Epoch [41/120    avg_loss:0.126, val_acc:0.920]
Epoch [42/120    avg_loss:0.136, val_acc:0.944]
Epoch [43/120    avg_loss:0.049, val_acc:0.959]
Epoch [44/120    avg_loss:0.058, val_acc:0.959]
Epoch [45/120    avg_loss:0.058, val_acc:0.962]
Epoch [46/120    avg_loss:0.061, val_acc:0.960]
Epoch [47/120    avg_loss:0.057, val_acc:0.959]
Epoch [48/120    avg_loss:0.053, val_acc:0.958]
Epoch [49/120    avg_loss:0.075, val_acc:0.953]
Epoch [50/120    avg_loss:0.049, val_acc:0.972]
Epoch [51/120    avg_loss:0.040, val_acc:0.973]
Epoch [52/120    avg_loss:0.024, val_acc:0.969]
Epoch [53/120    avg_loss:0.027, val_acc:0.975]
Epoch [54/120    avg_loss:0.025, val_acc:0.971]
Epoch [55/120    avg_loss:0.030, val_acc:0.968]
Epoch [56/120    avg_loss:0.024, val_acc:0.978]
Epoch [57/120    avg_loss:0.021, val_acc:0.971]
Epoch [58/120    avg_loss:0.043, val_acc:0.969]
Epoch [59/120    avg_loss:0.038, val_acc:0.963]
Epoch [60/120    avg_loss:0.030, val_acc:0.974]
Epoch [61/120    avg_loss:0.027, val_acc:0.982]
Epoch [62/120    avg_loss:0.031, val_acc:0.970]
Epoch [63/120    avg_loss:0.025, val_acc:0.980]
Epoch [64/120    avg_loss:0.023, val_acc:0.958]
Epoch [65/120    avg_loss:0.037, val_acc:0.979]
Epoch [66/120    avg_loss:0.024, val_acc:0.971]
Epoch [67/120    avg_loss:0.022, val_acc:0.975]
Epoch [68/120    avg_loss:0.021, val_acc:0.977]
Epoch [69/120    avg_loss:0.023, val_acc:0.977]
Epoch [70/120    avg_loss:0.021, val_acc:0.975]
Epoch [71/120    avg_loss:0.030, val_acc:0.982]
Epoch [72/120    avg_loss:0.014, val_acc:0.982]
Epoch [73/120    avg_loss:0.014, val_acc:0.982]
Epoch [74/120    avg_loss:0.021, val_acc:0.982]
Epoch [75/120    avg_loss:0.016, val_acc:0.982]
Epoch [76/120    avg_loss:0.050, val_acc:0.972]
Epoch [77/120    avg_loss:0.033, val_acc:0.979]
Epoch [78/120    avg_loss:0.015, val_acc:0.975]
Epoch [79/120    avg_loss:0.016, val_acc:0.982]
Epoch [80/120    avg_loss:0.010, val_acc:0.982]
Epoch [81/120    avg_loss:0.016, val_acc:0.983]
Epoch [82/120    avg_loss:0.017, val_acc:0.981]
Epoch [83/120    avg_loss:0.019, val_acc:0.983]
Epoch [84/120    avg_loss:0.009, val_acc:0.984]
Epoch [85/120    avg_loss:0.010, val_acc:0.985]
Epoch [86/120    avg_loss:0.008, val_acc:0.988]
Epoch [87/120    avg_loss:0.005, val_acc:0.987]
Epoch [88/120    avg_loss:0.012, val_acc:0.979]
Epoch [89/120    avg_loss:0.008, val_acc:0.982]
Epoch [90/120    avg_loss:0.007, val_acc:0.985]
Epoch [91/120    avg_loss:0.008, val_acc:0.979]
Epoch [92/120    avg_loss:0.008, val_acc:0.984]
Epoch [93/120    avg_loss:0.007, val_acc:0.984]
Epoch [94/120    avg_loss:0.006, val_acc:0.984]
Epoch [95/120    avg_loss:0.008, val_acc:0.988]
Epoch [96/120    avg_loss:0.009, val_acc:0.987]
Epoch [97/120    avg_loss:0.009, val_acc:0.984]
Epoch [98/120    avg_loss:0.007, val_acc:0.981]
Epoch [99/120    avg_loss:0.015, val_acc:0.980]
Epoch [100/120    avg_loss:0.030, val_acc:0.965]
Epoch [101/120    avg_loss:0.031, val_acc:0.979]
Epoch [102/120    avg_loss:0.019, val_acc:0.977]
Epoch [103/120    avg_loss:0.012, val_acc:0.985]
Epoch [104/120    avg_loss:0.014, val_acc:0.983]
Epoch [105/120    avg_loss:0.009, val_acc:0.977]
Epoch [106/120    avg_loss:0.008, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.005, val_acc:0.980]
Epoch [109/120    avg_loss:0.005, val_acc:0.983]
Epoch [110/120    avg_loss:0.005, val_acc:0.988]
Epoch [111/120    avg_loss:0.006, val_acc:0.988]
Epoch [112/120    avg_loss:0.006, val_acc:0.981]
Epoch [113/120    avg_loss:0.007, val_acc:0.983]
Epoch [114/120    avg_loss:0.006, val_acc:0.985]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.006, val_acc:0.989]
Epoch [118/120    avg_loss:0.007, val_acc:0.985]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.005, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1255   10    0    0    1    0    0    0    5    8    3    0
     0    3    0]
 [   0    0    0  731    0    8    0    0    0    4    1    3    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    8    0    4    0    0    0    0  845   11    0    0
     0    7    0]
 [   0    0    4    0    0    0    6    0    0    0   24 2175    0    0
     1    0    0]
 [   0    0    0   14    3    8    0    0    0    0    6   14  487    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    4    0    0    0
  1132    3    0]
 [   0    0    0    0    0    0   24    0    0    0    0    0    0    0
    40  283    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.46341463414635

F1 scores:
[       nan 0.98765432 0.98624754 0.96821192 0.99300699 0.97291196
 0.97695167 1.         1.         0.81818182 0.96022727 0.98394029
 0.95117188 1.         0.97881539 0.88024883 0.99408284]

Kappa:
0.9710737986437294
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4293780710>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.347, val_acc:0.350]
Epoch [2/120    avg_loss:1.693, val_acc:0.622]
Epoch [3/120    avg_loss:1.405, val_acc:0.670]
Epoch [4/120    avg_loss:1.200, val_acc:0.690]
Epoch [5/120    avg_loss:0.922, val_acc:0.768]
Epoch [6/120    avg_loss:0.874, val_acc:0.772]
Epoch [7/120    avg_loss:0.757, val_acc:0.810]
Epoch [8/120    avg_loss:0.615, val_acc:0.807]
Epoch [9/120    avg_loss:0.668, val_acc:0.808]
Epoch [10/120    avg_loss:0.536, val_acc:0.839]
Epoch [11/120    avg_loss:0.485, val_acc:0.861]
Epoch [12/120    avg_loss:0.470, val_acc:0.868]
Epoch [13/120    avg_loss:0.368, val_acc:0.877]
Epoch [14/120    avg_loss:0.364, val_acc:0.883]
Epoch [15/120    avg_loss:0.312, val_acc:0.878]
Epoch [16/120    avg_loss:0.318, val_acc:0.860]
Epoch [17/120    avg_loss:0.299, val_acc:0.891]
Epoch [18/120    avg_loss:0.243, val_acc:0.900]
Epoch [19/120    avg_loss:0.217, val_acc:0.905]
Epoch [20/120    avg_loss:0.324, val_acc:0.877]
Epoch [21/120    avg_loss:0.236, val_acc:0.929]
Epoch [22/120    avg_loss:0.234, val_acc:0.904]
Epoch [23/120    avg_loss:0.259, val_acc:0.906]
Epoch [24/120    avg_loss:0.187, val_acc:0.924]
Epoch [25/120    avg_loss:0.161, val_acc:0.882]
Epoch [26/120    avg_loss:0.198, val_acc:0.924]
Epoch [27/120    avg_loss:0.123, val_acc:0.936]
Epoch [28/120    avg_loss:0.123, val_acc:0.930]
Epoch [29/120    avg_loss:0.137, val_acc:0.941]
Epoch [30/120    avg_loss:0.109, val_acc:0.957]
Epoch [31/120    avg_loss:0.081, val_acc:0.946]
Epoch [32/120    avg_loss:0.117, val_acc:0.930]
Epoch [33/120    avg_loss:0.117, val_acc:0.961]
Epoch [34/120    avg_loss:0.090, val_acc:0.958]
Epoch [35/120    avg_loss:0.088, val_acc:0.945]
Epoch [36/120    avg_loss:0.067, val_acc:0.950]
Epoch [37/120    avg_loss:0.084, val_acc:0.938]
Epoch [38/120    avg_loss:0.069, val_acc:0.956]
Epoch [39/120    avg_loss:0.071, val_acc:0.947]
Epoch [40/120    avg_loss:0.066, val_acc:0.962]
Epoch [41/120    avg_loss:0.073, val_acc:0.956]
Epoch [42/120    avg_loss:0.059, val_acc:0.959]
Epoch [43/120    avg_loss:0.051, val_acc:0.968]
Epoch [44/120    avg_loss:0.047, val_acc:0.956]
Epoch [45/120    avg_loss:0.049, val_acc:0.967]
Epoch [46/120    avg_loss:0.041, val_acc:0.966]
Epoch [47/120    avg_loss:0.052, val_acc:0.966]
Epoch [48/120    avg_loss:0.045, val_acc:0.971]
Epoch [49/120    avg_loss:0.038, val_acc:0.962]
Epoch [50/120    avg_loss:0.083, val_acc:0.967]
Epoch [51/120    avg_loss:0.048, val_acc:0.961]
Epoch [52/120    avg_loss:0.037, val_acc:0.981]
Epoch [53/120    avg_loss:0.032, val_acc:0.971]
Epoch [54/120    avg_loss:0.041, val_acc:0.969]
Epoch [55/120    avg_loss:0.034, val_acc:0.973]
Epoch [56/120    avg_loss:0.091, val_acc:0.969]
Epoch [57/120    avg_loss:0.043, val_acc:0.970]
Epoch [58/120    avg_loss:0.041, val_acc:0.981]
Epoch [59/120    avg_loss:0.032, val_acc:0.969]
Epoch [60/120    avg_loss:0.022, val_acc:0.977]
Epoch [61/120    avg_loss:0.032, val_acc:0.973]
Epoch [62/120    avg_loss:0.031, val_acc:0.967]
Epoch [63/120    avg_loss:0.026, val_acc:0.979]
Epoch [64/120    avg_loss:0.022, val_acc:0.962]
Epoch [65/120    avg_loss:0.035, val_acc:0.980]
Epoch [66/120    avg_loss:0.030, val_acc:0.970]
Epoch [67/120    avg_loss:0.088, val_acc:0.948]
Epoch [68/120    avg_loss:0.058, val_acc:0.959]
Epoch [69/120    avg_loss:0.041, val_acc:0.966]
Epoch [70/120    avg_loss:0.034, val_acc:0.964]
Epoch [71/120    avg_loss:0.076, val_acc:0.975]
Epoch [72/120    avg_loss:0.036, val_acc:0.979]
Epoch [73/120    avg_loss:0.022, val_acc:0.980]
Epoch [74/120    avg_loss:0.022, val_acc:0.980]
Epoch [75/120    avg_loss:0.022, val_acc:0.981]
Epoch [76/120    avg_loss:0.019, val_acc:0.981]
Epoch [77/120    avg_loss:0.024, val_acc:0.982]
Epoch [78/120    avg_loss:0.023, val_acc:0.984]
Epoch [79/120    avg_loss:0.023, val_acc:0.978]
Epoch [80/120    avg_loss:0.020, val_acc:0.980]
Epoch [81/120    avg_loss:0.024, val_acc:0.984]
Epoch [82/120    avg_loss:0.019, val_acc:0.980]
Epoch [83/120    avg_loss:0.019, val_acc:0.984]
Epoch [84/120    avg_loss:0.016, val_acc:0.981]
Epoch [85/120    avg_loss:0.013, val_acc:0.981]
Epoch [86/120    avg_loss:0.015, val_acc:0.986]
Epoch [87/120    avg_loss:0.016, val_acc:0.984]
Epoch [88/120    avg_loss:0.018, val_acc:0.984]
Epoch [89/120    avg_loss:0.015, val_acc:0.982]
Epoch [90/120    avg_loss:0.020, val_acc:0.986]
Epoch [91/120    avg_loss:0.014, val_acc:0.982]
Epoch [92/120    avg_loss:0.014, val_acc:0.981]
Epoch [93/120    avg_loss:0.012, val_acc:0.981]
Epoch [94/120    avg_loss:0.015, val_acc:0.981]
Epoch [95/120    avg_loss:0.015, val_acc:0.982]
Epoch [96/120    avg_loss:0.012, val_acc:0.985]
Epoch [97/120    avg_loss:0.015, val_acc:0.984]
Epoch [98/120    avg_loss:0.013, val_acc:0.985]
Epoch [99/120    avg_loss:0.013, val_acc:0.984]
Epoch [100/120    avg_loss:0.013, val_acc:0.985]
Epoch [101/120    avg_loss:0.015, val_acc:0.985]
Epoch [102/120    avg_loss:0.013, val_acc:0.987]
Epoch [103/120    avg_loss:0.014, val_acc:0.986]
Epoch [104/120    avg_loss:0.013, val_acc:0.985]
Epoch [105/120    avg_loss:0.016, val_acc:0.985]
Epoch [106/120    avg_loss:0.015, val_acc:0.986]
Epoch [107/120    avg_loss:0.010, val_acc:0.985]
Epoch [108/120    avg_loss:0.013, val_acc:0.984]
Epoch [109/120    avg_loss:0.014, val_acc:0.986]
Epoch [110/120    avg_loss:0.011, val_acc:0.987]
Epoch [111/120    avg_loss:0.011, val_acc:0.984]
Epoch [112/120    avg_loss:0.012, val_acc:0.984]
Epoch [113/120    avg_loss:0.013, val_acc:0.986]
Epoch [114/120    avg_loss:0.011, val_acc:0.985]
Epoch [115/120    avg_loss:0.012, val_acc:0.986]
Epoch [116/120    avg_loss:0.009, val_acc:0.985]
Epoch [117/120    avg_loss:0.011, val_acc:0.987]
Epoch [118/120    avg_loss:0.013, val_acc:0.987]
Epoch [119/120    avg_loss:0.012, val_acc:0.987]
Epoch [120/120    avg_loss:0.010, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1260    2    0    1    1    0    0    0    5    7    4    0
     0    5    0]
 [   0    0    0  716    0   18    0    0    0    7    0    0    3    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6   23    0    7    2    0    0    0  822   11    1    0
     0    3    0]
 [   0    0    8    0    0    0    3    0    0    0    5 2192    2    0
     0    0    0]
 [   0    0    0   14   10   12    0    0    0    0    0    6  490    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    3    0    0    0
  1133    1    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    51  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.44173441734418

F1 scores:
[       nan 0.975      0.98475967 0.95339547 0.97706422 0.95469613
 0.99394856 1.         0.99767981 0.7826087  0.96028037 0.99051062
 0.9468599  0.9919571  0.97546276 0.90461538 0.98224852]

Kappa:
0.9708278123785749
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9b024a66d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.317, val_acc:0.508]
Epoch [2/120    avg_loss:1.748, val_acc:0.527]
Epoch [3/120    avg_loss:1.461, val_acc:0.670]
Epoch [4/120    avg_loss:1.198, val_acc:0.684]
Epoch [5/120    avg_loss:0.971, val_acc:0.752]
Epoch [6/120    avg_loss:0.880, val_acc:0.749]
Epoch [7/120    avg_loss:0.740, val_acc:0.756]
Epoch [8/120    avg_loss:0.617, val_acc:0.844]
Epoch [9/120    avg_loss:0.612, val_acc:0.821]
Epoch [10/120    avg_loss:0.575, val_acc:0.833]
Epoch [11/120    avg_loss:0.530, val_acc:0.850]
Epoch [12/120    avg_loss:0.483, val_acc:0.855]
Epoch [13/120    avg_loss:0.330, val_acc:0.859]
Epoch [14/120    avg_loss:0.703, val_acc:0.791]
Epoch [15/120    avg_loss:0.512, val_acc:0.845]
Epoch [16/120    avg_loss:0.453, val_acc:0.871]
Epoch [17/120    avg_loss:0.320, val_acc:0.865]
Epoch [18/120    avg_loss:0.358, val_acc:0.894]
Epoch [19/120    avg_loss:0.259, val_acc:0.877]
Epoch [20/120    avg_loss:0.223, val_acc:0.905]
Epoch [21/120    avg_loss:0.199, val_acc:0.927]
Epoch [22/120    avg_loss:0.201, val_acc:0.913]
Epoch [23/120    avg_loss:0.219, val_acc:0.925]
Epoch [24/120    avg_loss:0.145, val_acc:0.941]
Epoch [25/120    avg_loss:0.150, val_acc:0.947]
Epoch [26/120    avg_loss:0.116, val_acc:0.920]
Epoch [27/120    avg_loss:0.136, val_acc:0.930]
Epoch [28/120    avg_loss:0.124, val_acc:0.930]
Epoch [29/120    avg_loss:0.125, val_acc:0.931]
Epoch [30/120    avg_loss:0.115, val_acc:0.939]
Epoch [31/120    avg_loss:0.108, val_acc:0.940]
Epoch [32/120    avg_loss:0.084, val_acc:0.943]
Epoch [33/120    avg_loss:0.088, val_acc:0.935]
Epoch [34/120    avg_loss:0.106, val_acc:0.942]
Epoch [35/120    avg_loss:0.103, val_acc:0.947]
Epoch [36/120    avg_loss:0.063, val_acc:0.938]
Epoch [37/120    avg_loss:0.063, val_acc:0.948]
Epoch [38/120    avg_loss:0.065, val_acc:0.949]
Epoch [39/120    avg_loss:0.067, val_acc:0.943]
Epoch [40/120    avg_loss:0.059, val_acc:0.961]
Epoch [41/120    avg_loss:0.075, val_acc:0.949]
Epoch [42/120    avg_loss:0.063, val_acc:0.956]
Epoch [43/120    avg_loss:0.082, val_acc:0.956]
Epoch [44/120    avg_loss:0.051, val_acc:0.955]
Epoch [45/120    avg_loss:0.058, val_acc:0.953]
Epoch [46/120    avg_loss:0.060, val_acc:0.948]
Epoch [47/120    avg_loss:0.055, val_acc:0.957]
Epoch [48/120    avg_loss:0.080, val_acc:0.945]
Epoch [49/120    avg_loss:0.073, val_acc:0.939]
Epoch [50/120    avg_loss:0.063, val_acc:0.961]
Epoch [51/120    avg_loss:0.050, val_acc:0.959]
Epoch [52/120    avg_loss:0.036, val_acc:0.966]
Epoch [53/120    avg_loss:0.027, val_acc:0.962]
Epoch [54/120    avg_loss:0.024, val_acc:0.964]
Epoch [55/120    avg_loss:0.031, val_acc:0.946]
Epoch [56/120    avg_loss:0.048, val_acc:0.956]
Epoch [57/120    avg_loss:0.039, val_acc:0.961]
Epoch [58/120    avg_loss:0.037, val_acc:0.963]
Epoch [59/120    avg_loss:0.028, val_acc:0.964]
Epoch [60/120    avg_loss:0.036, val_acc:0.957]
Epoch [61/120    avg_loss:0.031, val_acc:0.971]
Epoch [62/120    avg_loss:0.026, val_acc:0.968]
Epoch [63/120    avg_loss:0.021, val_acc:0.966]
Epoch [64/120    avg_loss:0.022, val_acc:0.964]
Epoch [65/120    avg_loss:0.031, val_acc:0.971]
Epoch [66/120    avg_loss:0.022, val_acc:0.963]
Epoch [67/120    avg_loss:0.024, val_acc:0.967]
Epoch [68/120    avg_loss:0.030, val_acc:0.957]
Epoch [69/120    avg_loss:0.028, val_acc:0.969]
Epoch [70/120    avg_loss:0.019, val_acc:0.947]
Epoch [71/120    avg_loss:0.020, val_acc:0.964]
Epoch [72/120    avg_loss:0.026, val_acc:0.966]
Epoch [73/120    avg_loss:0.024, val_acc:0.971]
Epoch [74/120    avg_loss:0.027, val_acc:0.969]
Epoch [75/120    avg_loss:0.016, val_acc:0.977]
Epoch [76/120    avg_loss:0.019, val_acc:0.975]
Epoch [77/120    avg_loss:0.019, val_acc:0.969]
Epoch [78/120    avg_loss:0.026, val_acc:0.955]
Epoch [79/120    avg_loss:0.021, val_acc:0.969]
Epoch [80/120    avg_loss:0.026, val_acc:0.974]
Epoch [81/120    avg_loss:0.014, val_acc:0.974]
Epoch [82/120    avg_loss:0.016, val_acc:0.970]
Epoch [83/120    avg_loss:0.014, val_acc:0.977]
Epoch [84/120    avg_loss:0.013, val_acc:0.971]
Epoch [85/120    avg_loss:0.012, val_acc:0.978]
Epoch [86/120    avg_loss:0.012, val_acc:0.975]
Epoch [87/120    avg_loss:0.012, val_acc:0.985]
Epoch [88/120    avg_loss:0.008, val_acc:0.984]
Epoch [89/120    avg_loss:0.009, val_acc:0.978]
Epoch [90/120    avg_loss:0.009, val_acc:0.983]
Epoch [91/120    avg_loss:0.010, val_acc:0.986]
Epoch [92/120    avg_loss:0.008, val_acc:0.982]
Epoch [93/120    avg_loss:0.008, val_acc:0.981]
Epoch [94/120    avg_loss:0.006, val_acc:0.977]
Epoch [95/120    avg_loss:0.014, val_acc:0.986]
Epoch [96/120    avg_loss:0.009, val_acc:0.981]
Epoch [97/120    avg_loss:0.016, val_acc:0.968]
Epoch [98/120    avg_loss:0.013, val_acc:0.977]
Epoch [99/120    avg_loss:0.013, val_acc:0.976]
Epoch [100/120    avg_loss:0.008, val_acc:0.978]
Epoch [101/120    avg_loss:0.011, val_acc:0.975]
Epoch [102/120    avg_loss:0.014, val_acc:0.981]
Epoch [103/120    avg_loss:0.019, val_acc:0.981]
Epoch [104/120    avg_loss:0.032, val_acc:0.957]
Epoch [105/120    avg_loss:0.019, val_acc:0.981]
Epoch [106/120    avg_loss:0.032, val_acc:0.967]
Epoch [107/120    avg_loss:0.018, val_acc:0.975]
Epoch [108/120    avg_loss:0.010, val_acc:0.971]
Epoch [109/120    avg_loss:0.007, val_acc:0.973]
Epoch [110/120    avg_loss:0.007, val_acc:0.976]
Epoch [111/120    avg_loss:0.007, val_acc:0.976]
Epoch [112/120    avg_loss:0.006, val_acc:0.976]
Epoch [113/120    avg_loss:0.007, val_acc:0.977]
Epoch [114/120    avg_loss:0.007, val_acc:0.976]
Epoch [115/120    avg_loss:0.006, val_acc:0.976]
Epoch [116/120    avg_loss:0.005, val_acc:0.976]
Epoch [117/120    avg_loss:0.005, val_acc:0.978]
Epoch [118/120    avg_loss:0.005, val_acc:0.978]
Epoch [119/120    avg_loss:0.005, val_acc:0.978]
Epoch [120/120    avg_loss:0.011, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    1 1259    8    0    2    1    0    0    0    5    9    0    0
     0    0    0]
 [   0    0    0  713    0   22    0    0    0    3    2    0    5    1
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    7   13    0    5    4    0    0    0  833   12    0    0
     0    1    0]
 [   0    0    5    0    0    0    5    0    0    0   18 2178    2    2
     0    0    0]
 [   0    0    0   14   18    7    0    0    0    0    8    0  481    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    45  290    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.22493224932249

F1 scores:
[       nan 0.96296296 0.98474775 0.95384615 0.95945946 0.9579646
 0.98277154 1.         0.99883856 0.85       0.95472779 0.9877551
 0.93853659 0.9919571  0.97844828 0.90909091 0.95348837]

Kappa:
0.9683630710552081
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4b56190748>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.357, val_acc:0.491]
Epoch [2/120    avg_loss:1.692, val_acc:0.568]
Epoch [3/120    avg_loss:1.384, val_acc:0.667]
Epoch [4/120    avg_loss:1.062, val_acc:0.656]
Epoch [5/120    avg_loss:0.926, val_acc:0.776]
Epoch [6/120    avg_loss:0.818, val_acc:0.750]
Epoch [7/120    avg_loss:0.738, val_acc:0.728]
Epoch [8/120    avg_loss:0.812, val_acc:0.683]
Epoch [9/120    avg_loss:0.653, val_acc:0.823]
Epoch [10/120    avg_loss:0.474, val_acc:0.833]
Epoch [11/120    avg_loss:0.529, val_acc:0.812]
Epoch [12/120    avg_loss:0.482, val_acc:0.837]
Epoch [13/120    avg_loss:0.370, val_acc:0.864]
Epoch [14/120    avg_loss:0.355, val_acc:0.834]
Epoch [15/120    avg_loss:0.303, val_acc:0.878]
Epoch [16/120    avg_loss:0.340, val_acc:0.873]
Epoch [17/120    avg_loss:0.251, val_acc:0.899]
Epoch [18/120    avg_loss:0.378, val_acc:0.904]
Epoch [19/120    avg_loss:0.274, val_acc:0.900]
Epoch [20/120    avg_loss:0.190, val_acc:0.875]
Epoch [21/120    avg_loss:0.186, val_acc:0.876]
Epoch [22/120    avg_loss:0.177, val_acc:0.918]
Epoch [23/120    avg_loss:0.166, val_acc:0.923]
Epoch [24/120    avg_loss:0.144, val_acc:0.914]
Epoch [25/120    avg_loss:0.118, val_acc:0.945]
Epoch [26/120    avg_loss:0.130, val_acc:0.932]
Epoch [27/120    avg_loss:0.137, val_acc:0.928]
Epoch [28/120    avg_loss:0.134, val_acc:0.939]
Epoch [29/120    avg_loss:0.116, val_acc:0.938]
Epoch [30/120    avg_loss:0.137, val_acc:0.918]
Epoch [31/120    avg_loss:0.125, val_acc:0.939]
Epoch [32/120    avg_loss:0.101, val_acc:0.940]
Epoch [33/120    avg_loss:0.105, val_acc:0.945]
Epoch [34/120    avg_loss:0.079, val_acc:0.939]
Epoch [35/120    avg_loss:0.086, val_acc:0.919]
Epoch [36/120    avg_loss:0.138, val_acc:0.952]
Epoch [37/120    avg_loss:0.073, val_acc:0.947]
Epoch [38/120    avg_loss:0.076, val_acc:0.900]
Epoch [39/120    avg_loss:0.176, val_acc:0.921]
Epoch [40/120    avg_loss:0.088, val_acc:0.952]
Epoch [41/120    avg_loss:0.060, val_acc:0.958]
Epoch [42/120    avg_loss:0.046, val_acc:0.954]
Epoch [43/120    avg_loss:0.056, val_acc:0.936]
Epoch [44/120    avg_loss:0.061, val_acc:0.953]
Epoch [45/120    avg_loss:0.089, val_acc:0.945]
Epoch [46/120    avg_loss:0.062, val_acc:0.944]
Epoch [47/120    avg_loss:0.061, val_acc:0.953]
Epoch [48/120    avg_loss:0.041, val_acc:0.959]
Epoch [49/120    avg_loss:0.063, val_acc:0.956]
Epoch [50/120    avg_loss:0.055, val_acc:0.958]
Epoch [51/120    avg_loss:0.042, val_acc:0.945]
Epoch [52/120    avg_loss:0.045, val_acc:0.967]
Epoch [53/120    avg_loss:0.049, val_acc:0.948]
Epoch [54/120    avg_loss:0.054, val_acc:0.963]
Epoch [55/120    avg_loss:0.031, val_acc:0.978]
Epoch [56/120    avg_loss:0.028, val_acc:0.977]
Epoch [57/120    avg_loss:0.034, val_acc:0.961]
Epoch [58/120    avg_loss:0.037, val_acc:0.971]
Epoch [59/120    avg_loss:0.022, val_acc:0.970]
Epoch [60/120    avg_loss:0.045, val_acc:0.959]
Epoch [61/120    avg_loss:0.047, val_acc:0.966]
Epoch [62/120    avg_loss:0.032, val_acc:0.954]
Epoch [63/120    avg_loss:0.023, val_acc:0.970]
Epoch [64/120    avg_loss:0.033, val_acc:0.967]
Epoch [65/120    avg_loss:0.036, val_acc:0.962]
Epoch [66/120    avg_loss:0.021, val_acc:0.969]
Epoch [67/120    avg_loss:0.023, val_acc:0.974]
Epoch [68/120    avg_loss:0.031, val_acc:0.961]
Epoch [69/120    avg_loss:0.030, val_acc:0.968]
Epoch [70/120    avg_loss:0.022, val_acc:0.973]
Epoch [71/120    avg_loss:0.018, val_acc:0.973]
Epoch [72/120    avg_loss:0.014, val_acc:0.977]
Epoch [73/120    avg_loss:0.014, val_acc:0.976]
Epoch [74/120    avg_loss:0.015, val_acc:0.978]
Epoch [75/120    avg_loss:0.013, val_acc:0.979]
Epoch [76/120    avg_loss:0.012, val_acc:0.977]
Epoch [77/120    avg_loss:0.012, val_acc:0.979]
Epoch [78/120    avg_loss:0.010, val_acc:0.981]
Epoch [79/120    avg_loss:0.012, val_acc:0.979]
Epoch [80/120    avg_loss:0.010, val_acc:0.977]
Epoch [81/120    avg_loss:0.013, val_acc:0.979]
Epoch [82/120    avg_loss:0.013, val_acc:0.978]
Epoch [83/120    avg_loss:0.010, val_acc:0.979]
Epoch [84/120    avg_loss:0.014, val_acc:0.979]
Epoch [85/120    avg_loss:0.011, val_acc:0.979]
Epoch [86/120    avg_loss:0.011, val_acc:0.979]
Epoch [87/120    avg_loss:0.010, val_acc:0.978]
Epoch [88/120    avg_loss:0.012, val_acc:0.977]
Epoch [89/120    avg_loss:0.012, val_acc:0.977]
Epoch [90/120    avg_loss:0.011, val_acc:0.979]
Epoch [91/120    avg_loss:0.012, val_acc:0.979]
Epoch [92/120    avg_loss:0.012, val_acc:0.979]
Epoch [93/120    avg_loss:0.011, val_acc:0.979]
Epoch [94/120    avg_loss:0.011, val_acc:0.979]
Epoch [95/120    avg_loss:0.013, val_acc:0.979]
Epoch [96/120    avg_loss:0.010, val_acc:0.979]
Epoch [97/120    avg_loss:0.009, val_acc:0.979]
Epoch [98/120    avg_loss:0.008, val_acc:0.979]
Epoch [99/120    avg_loss:0.010, val_acc:0.979]
Epoch [100/120    avg_loss:0.010, val_acc:0.979]
Epoch [101/120    avg_loss:0.009, val_acc:0.979]
Epoch [102/120    avg_loss:0.008, val_acc:0.979]
Epoch [103/120    avg_loss:0.010, val_acc:0.979]
Epoch [104/120    avg_loss:0.008, val_acc:0.979]
Epoch [105/120    avg_loss:0.008, val_acc:0.979]
Epoch [106/120    avg_loss:0.011, val_acc:0.979]
Epoch [107/120    avg_loss:0.012, val_acc:0.979]
Epoch [108/120    avg_loss:0.013, val_acc:0.979]
Epoch [109/120    avg_loss:0.009, val_acc:0.979]
Epoch [110/120    avg_loss:0.011, val_acc:0.979]
Epoch [111/120    avg_loss:0.011, val_acc:0.979]
Epoch [112/120    avg_loss:0.013, val_acc:0.979]
Epoch [113/120    avg_loss:0.008, val_acc:0.979]
Epoch [114/120    avg_loss:0.008, val_acc:0.979]
Epoch [115/120    avg_loss:0.010, val_acc:0.979]
Epoch [116/120    avg_loss:0.013, val_acc:0.979]
Epoch [117/120    avg_loss:0.013, val_acc:0.979]
Epoch [118/120    avg_loss:0.009, val_acc:0.979]
Epoch [119/120    avg_loss:0.012, val_acc:0.979]
Epoch [120/120    avg_loss:0.011, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1271    1    0    1    0    0    0    0    4    8    0    0
     0    0    0]
 [   0    0    0  727    2   10    0    0    0    4    0    0    2    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0   12   11    0    7    0    0    0    0  809   28    1    0
     1    6    0]
 [   0    0    9    0    0    0    6    0    0    0    6 2188    0    1
     0    0    0]
 [   0    0    0   17    5   11    0    0    0    0    0    2  495    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    1    0    0    0
  1132    0    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    74  254    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.05149051490515

F1 scores:
[       nan 0.975      0.98641832 0.96483079 0.98383372 0.95777778
 0.98056801 0.98039216 0.99883856 0.71794872 0.95344726 0.98625197
 0.95930233 0.9919571  0.96463571 0.8369028  0.98245614]

Kappa:
0.966352480111328
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc576421748>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.365, val_acc:0.415]
Epoch [2/120    avg_loss:1.773, val_acc:0.583]
Epoch [3/120    avg_loss:1.456, val_acc:0.629]
Epoch [4/120    avg_loss:1.110, val_acc:0.710]
Epoch [5/120    avg_loss:0.916, val_acc:0.730]
Epoch [6/120    avg_loss:0.796, val_acc:0.747]
Epoch [7/120    avg_loss:0.684, val_acc:0.788]
Epoch [8/120    avg_loss:0.712, val_acc:0.833]
Epoch [9/120    avg_loss:0.620, val_acc:0.806]
Epoch [10/120    avg_loss:0.512, val_acc:0.825]
Epoch [11/120    avg_loss:0.503, val_acc:0.856]
Epoch [12/120    avg_loss:0.395, val_acc:0.881]
Epoch [13/120    avg_loss:0.342, val_acc:0.890]
Epoch [14/120    avg_loss:0.352, val_acc:0.878]
Epoch [15/120    avg_loss:0.272, val_acc:0.895]
Epoch [16/120    avg_loss:0.305, val_acc:0.887]
Epoch [17/120    avg_loss:0.232, val_acc:0.909]
Epoch [18/120    avg_loss:0.301, val_acc:0.903]
Epoch [19/120    avg_loss:0.259, val_acc:0.935]
Epoch [20/120    avg_loss:0.231, val_acc:0.862]
Epoch [21/120    avg_loss:0.282, val_acc:0.916]
Epoch [22/120    avg_loss:0.277, val_acc:0.904]
Epoch [23/120    avg_loss:0.211, val_acc:0.934]
Epoch [24/120    avg_loss:0.200, val_acc:0.909]
Epoch [25/120    avg_loss:0.157, val_acc:0.936]
Epoch [26/120    avg_loss:0.123, val_acc:0.935]
Epoch [27/120    avg_loss:0.151, val_acc:0.909]
Epoch [28/120    avg_loss:0.177, val_acc:0.927]
Epoch [29/120    avg_loss:0.100, val_acc:0.949]
Epoch [30/120    avg_loss:0.098, val_acc:0.935]
Epoch [31/120    avg_loss:0.078, val_acc:0.946]
Epoch [32/120    avg_loss:0.096, val_acc:0.918]
Epoch [33/120    avg_loss:0.108, val_acc:0.954]
Epoch [34/120    avg_loss:0.085, val_acc:0.942]
Epoch [35/120    avg_loss:0.073, val_acc:0.950]
Epoch [36/120    avg_loss:0.079, val_acc:0.960]
Epoch [37/120    avg_loss:0.078, val_acc:0.958]
Epoch [38/120    avg_loss:0.074, val_acc:0.950]
Epoch [39/120    avg_loss:0.075, val_acc:0.957]
Epoch [40/120    avg_loss:0.051, val_acc:0.967]
Epoch [41/120    avg_loss:0.089, val_acc:0.933]
Epoch [42/120    avg_loss:0.081, val_acc:0.960]
Epoch [43/120    avg_loss:0.081, val_acc:0.958]
Epoch [44/120    avg_loss:0.098, val_acc:0.956]
Epoch [45/120    avg_loss:0.074, val_acc:0.963]
Epoch [46/120    avg_loss:0.060, val_acc:0.959]
Epoch [47/120    avg_loss:0.045, val_acc:0.961]
Epoch [48/120    avg_loss:0.034, val_acc:0.970]
Epoch [49/120    avg_loss:0.062, val_acc:0.967]
Epoch [50/120    avg_loss:0.036, val_acc:0.972]
Epoch [51/120    avg_loss:0.041, val_acc:0.954]
Epoch [52/120    avg_loss:0.045, val_acc:0.972]
Epoch [53/120    avg_loss:0.037, val_acc:0.977]
Epoch [54/120    avg_loss:0.033, val_acc:0.974]
Epoch [55/120    avg_loss:0.043, val_acc:0.977]
Epoch [56/120    avg_loss:0.041, val_acc:0.967]
Epoch [57/120    avg_loss:0.038, val_acc:0.969]
Epoch [58/120    avg_loss:0.035, val_acc:0.969]
Epoch [59/120    avg_loss:0.046, val_acc:0.964]
Epoch [60/120    avg_loss:0.041, val_acc:0.967]
Epoch [61/120    avg_loss:0.029, val_acc:0.983]
Epoch [62/120    avg_loss:0.038, val_acc:0.972]
Epoch [63/120    avg_loss:0.033, val_acc:0.967]
Epoch [64/120    avg_loss:0.035, val_acc:0.966]
Epoch [65/120    avg_loss:0.040, val_acc:0.972]
Epoch [66/120    avg_loss:0.039, val_acc:0.960]
Epoch [67/120    avg_loss:0.025, val_acc:0.981]
Epoch [68/120    avg_loss:0.021, val_acc:0.981]
Epoch [69/120    avg_loss:0.019, val_acc:0.977]
Epoch [70/120    avg_loss:0.014, val_acc:0.982]
Epoch [71/120    avg_loss:0.028, val_acc:0.980]
Epoch [72/120    avg_loss:0.023, val_acc:0.983]
Epoch [73/120    avg_loss:0.028, val_acc:0.966]
Epoch [74/120    avg_loss:0.031, val_acc:0.978]
Epoch [75/120    avg_loss:0.027, val_acc:0.977]
Epoch [76/120    avg_loss:0.022, val_acc:0.981]
Epoch [77/120    avg_loss:0.012, val_acc:0.980]
Epoch [78/120    avg_loss:0.015, val_acc:0.982]
Epoch [79/120    avg_loss:0.014, val_acc:0.982]
Epoch [80/120    avg_loss:0.017, val_acc:0.983]
Epoch [81/120    avg_loss:0.010, val_acc:0.978]
Epoch [82/120    avg_loss:0.012, val_acc:0.978]
Epoch [83/120    avg_loss:0.019, val_acc:0.981]
Epoch [84/120    avg_loss:0.015, val_acc:0.984]
Epoch [85/120    avg_loss:0.035, val_acc:0.970]
Epoch [86/120    avg_loss:0.024, val_acc:0.974]
Epoch [87/120    avg_loss:0.017, val_acc:0.978]
Epoch [88/120    avg_loss:0.016, val_acc:0.980]
Epoch [89/120    avg_loss:0.010, val_acc:0.985]
Epoch [90/120    avg_loss:0.009, val_acc:0.984]
Epoch [91/120    avg_loss:0.007, val_acc:0.983]
Epoch [92/120    avg_loss:0.012, val_acc:0.977]
Epoch [93/120    avg_loss:0.010, val_acc:0.980]
Epoch [94/120    avg_loss:0.013, val_acc:0.980]
Epoch [95/120    avg_loss:0.009, val_acc:0.985]
Epoch [96/120    avg_loss:0.008, val_acc:0.986]
Epoch [97/120    avg_loss:0.009, val_acc:0.987]
Epoch [98/120    avg_loss:0.013, val_acc:0.980]
Epoch [99/120    avg_loss:0.010, val_acc:0.980]
Epoch [100/120    avg_loss:0.009, val_acc:0.976]
Epoch [101/120    avg_loss:0.006, val_acc:0.986]
Epoch [102/120    avg_loss:0.008, val_acc:0.988]
Epoch [103/120    avg_loss:0.013, val_acc:0.981]
Epoch [104/120    avg_loss:0.019, val_acc:0.967]
Epoch [105/120    avg_loss:0.043, val_acc:0.961]
Epoch [106/120    avg_loss:0.019, val_acc:0.984]
Epoch [107/120    avg_loss:0.025, val_acc:0.973]
Epoch [108/120    avg_loss:0.021, val_acc:0.976]
Epoch [109/120    avg_loss:0.013, val_acc:0.981]
Epoch [110/120    avg_loss:0.011, val_acc:0.982]
Epoch [111/120    avg_loss:0.010, val_acc:0.983]
Epoch [112/120    avg_loss:0.008, val_acc:0.985]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.007, val_acc:0.982]
Epoch [116/120    avg_loss:0.008, val_acc:0.985]
Epoch [117/120    avg_loss:0.007, val_acc:0.984]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.006, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1265    8    0    0    0    0    0    0    5    6    1    0
     0    0    0]
 [   0    0    0  734    0    7    0    0    0    3    1    0    0    2
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    2    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    7    7    0    2    0    0    0    0  856    0    0    0
     0    3    0]
 [   0    0   12    0    0    0    2    0    0    0   11 2181    2    2
     0    0    0]
 [   0    0    1   27    9    4    0    0    0    0    8    1  483    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    3    0    0    0
  1133    0    0]
 [   0    0    0    0    0    0   14    0    0    1    0    0    0    0
    51  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.60433604336043

F1 scores:
[       nan 0.975      0.9844358  0.96010464 0.97459584 0.9738339
 0.98720843 0.98039216 1.         0.71794872 0.9721749  0.99181446
 0.94613124 0.98930481 0.97378599 0.89064976 0.98809524]

Kappa:
0.9726828641213523
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa20d593668>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.396, val_acc:0.394]
Epoch [2/120    avg_loss:1.768, val_acc:0.527]
Epoch [3/120    avg_loss:1.501, val_acc:0.640]
Epoch [4/120    avg_loss:1.214, val_acc:0.713]
Epoch [5/120    avg_loss:1.020, val_acc:0.735]
Epoch [6/120    avg_loss:0.777, val_acc:0.742]
Epoch [7/120    avg_loss:0.738, val_acc:0.750]
Epoch [8/120    avg_loss:0.595, val_acc:0.828]
Epoch [9/120    avg_loss:0.664, val_acc:0.810]
Epoch [10/120    avg_loss:0.512, val_acc:0.814]
Epoch [11/120    avg_loss:0.429, val_acc:0.857]
Epoch [12/120    avg_loss:0.373, val_acc:0.856]
Epoch [13/120    avg_loss:0.355, val_acc:0.824]
Epoch [14/120    avg_loss:0.361, val_acc:0.856]
Epoch [15/120    avg_loss:0.317, val_acc:0.843]
Epoch [16/120    avg_loss:0.351, val_acc:0.856]
Epoch [17/120    avg_loss:0.351, val_acc:0.884]
Epoch [18/120    avg_loss:0.215, val_acc:0.920]
Epoch [19/120    avg_loss:0.347, val_acc:0.885]
Epoch [20/120    avg_loss:0.274, val_acc:0.871]
Epoch [21/120    avg_loss:0.310, val_acc:0.876]
Epoch [22/120    avg_loss:0.234, val_acc:0.886]
Epoch [23/120    avg_loss:0.225, val_acc:0.893]
Epoch [24/120    avg_loss:0.146, val_acc:0.926]
Epoch [25/120    avg_loss:0.134, val_acc:0.926]
Epoch [26/120    avg_loss:0.207, val_acc:0.906]
Epoch [27/120    avg_loss:0.179, val_acc:0.922]
Epoch [28/120    avg_loss:0.116, val_acc:0.940]
Epoch [29/120    avg_loss:0.079, val_acc:0.940]
Epoch [30/120    avg_loss:0.090, val_acc:0.935]
Epoch [31/120    avg_loss:0.094, val_acc:0.930]
Epoch [32/120    avg_loss:0.096, val_acc:0.943]
Epoch [33/120    avg_loss:0.131, val_acc:0.884]
Epoch [34/120    avg_loss:0.098, val_acc:0.942]
Epoch [35/120    avg_loss:0.069, val_acc:0.949]
Epoch [36/120    avg_loss:0.066, val_acc:0.944]
Epoch [37/120    avg_loss:0.156, val_acc:0.921]
Epoch [38/120    avg_loss:0.110, val_acc:0.948]
Epoch [39/120    avg_loss:0.072, val_acc:0.950]
Epoch [40/120    avg_loss:0.051, val_acc:0.952]
Epoch [41/120    avg_loss:0.044, val_acc:0.959]
Epoch [42/120    avg_loss:0.080, val_acc:0.933]
Epoch [43/120    avg_loss:0.114, val_acc:0.941]
Epoch [44/120    avg_loss:0.073, val_acc:0.946]
Epoch [45/120    avg_loss:0.071, val_acc:0.945]
Epoch [46/120    avg_loss:0.058, val_acc:0.948]
Epoch [47/120    avg_loss:0.063, val_acc:0.925]
Epoch [48/120    avg_loss:0.123, val_acc:0.934]
Epoch [49/120    avg_loss:0.099, val_acc:0.948]
Epoch [50/120    avg_loss:0.051, val_acc:0.935]
Epoch [51/120    avg_loss:0.052, val_acc:0.944]
Epoch [52/120    avg_loss:0.044, val_acc:0.962]
Epoch [53/120    avg_loss:0.028, val_acc:0.961]
Epoch [54/120    avg_loss:0.036, val_acc:0.957]
Epoch [55/120    avg_loss:0.040, val_acc:0.959]
Epoch [56/120    avg_loss:0.040, val_acc:0.959]
Epoch [57/120    avg_loss:0.031, val_acc:0.967]
Epoch [58/120    avg_loss:0.032, val_acc:0.968]
Epoch [59/120    avg_loss:0.023, val_acc:0.972]
Epoch [60/120    avg_loss:0.019, val_acc:0.971]
Epoch [61/120    avg_loss:0.028, val_acc:0.968]
Epoch [62/120    avg_loss:0.024, val_acc:0.962]
Epoch [63/120    avg_loss:0.025, val_acc:0.959]
Epoch [64/120    avg_loss:0.021, val_acc:0.957]
Epoch [65/120    avg_loss:0.026, val_acc:0.969]
Epoch [66/120    avg_loss:0.027, val_acc:0.963]
Epoch [67/120    avg_loss:0.035, val_acc:0.961]
Epoch [68/120    avg_loss:0.025, val_acc:0.966]
Epoch [69/120    avg_loss:0.017, val_acc:0.967]
Epoch [70/120    avg_loss:0.023, val_acc:0.962]
Epoch [71/120    avg_loss:0.022, val_acc:0.967]
Epoch [72/120    avg_loss:0.023, val_acc:0.971]
Epoch [73/120    avg_loss:0.015, val_acc:0.970]
Epoch [74/120    avg_loss:0.012, val_acc:0.970]
Epoch [75/120    avg_loss:0.014, val_acc:0.971]
Epoch [76/120    avg_loss:0.013, val_acc:0.972]
Epoch [77/120    avg_loss:0.012, val_acc:0.975]
Epoch [78/120    avg_loss:0.013, val_acc:0.973]
Epoch [79/120    avg_loss:0.011, val_acc:0.972]
Epoch [80/120    avg_loss:0.014, val_acc:0.972]
Epoch [81/120    avg_loss:0.011, val_acc:0.972]
Epoch [82/120    avg_loss:0.014, val_acc:0.972]
Epoch [83/120    avg_loss:0.011, val_acc:0.972]
Epoch [84/120    avg_loss:0.010, val_acc:0.972]
Epoch [85/120    avg_loss:0.010, val_acc:0.973]
Epoch [86/120    avg_loss:0.014, val_acc:0.972]
Epoch [87/120    avg_loss:0.010, val_acc:0.973]
Epoch [88/120    avg_loss:0.013, val_acc:0.974]
Epoch [89/120    avg_loss:0.008, val_acc:0.974]
Epoch [90/120    avg_loss:0.010, val_acc:0.974]
Epoch [91/120    avg_loss:0.009, val_acc:0.974]
Epoch [92/120    avg_loss:0.010, val_acc:0.974]
Epoch [93/120    avg_loss:0.008, val_acc:0.974]
Epoch [94/120    avg_loss:0.010, val_acc:0.973]
Epoch [95/120    avg_loss:0.008, val_acc:0.973]
Epoch [96/120    avg_loss:0.010, val_acc:0.973]
Epoch [97/120    avg_loss:0.010, val_acc:0.973]
Epoch [98/120    avg_loss:0.009, val_acc:0.973]
Epoch [99/120    avg_loss:0.009, val_acc:0.973]
Epoch [100/120    avg_loss:0.009, val_acc:0.973]
Epoch [101/120    avg_loss:0.011, val_acc:0.973]
Epoch [102/120    avg_loss:0.011, val_acc:0.973]
Epoch [103/120    avg_loss:0.011, val_acc:0.973]
Epoch [104/120    avg_loss:0.009, val_acc:0.973]
Epoch [105/120    avg_loss:0.010, val_acc:0.973]
Epoch [106/120    avg_loss:0.009, val_acc:0.973]
Epoch [107/120    avg_loss:0.013, val_acc:0.973]
Epoch [108/120    avg_loss:0.010, val_acc:0.973]
Epoch [109/120    avg_loss:0.008, val_acc:0.973]
Epoch [110/120    avg_loss:0.011, val_acc:0.973]
Epoch [111/120    avg_loss:0.008, val_acc:0.973]
Epoch [112/120    avg_loss:0.010, val_acc:0.973]
Epoch [113/120    avg_loss:0.009, val_acc:0.973]
Epoch [114/120    avg_loss:0.010, val_acc:0.973]
Epoch [115/120    avg_loss:0.009, val_acc:0.973]
Epoch [116/120    avg_loss:0.008, val_acc:0.973]
Epoch [117/120    avg_loss:0.011, val_acc:0.973]
Epoch [118/120    avg_loss:0.009, val_acc:0.973]
Epoch [119/120    avg_loss:0.009, val_acc:0.973]
Epoch [120/120    avg_loss:0.010, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1264    6    0    0    1    0    0    0    6    8    0    0
     0    0    0]
 [   0    0    0  716    2   20    0    0    0    5    1    0    1    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8   33    0    7    0    0    0    0  807   11    0    0
     3    6    0]
 [   0    0   10    0    0    1    5    0    0    0    3 2189    0    2
     0    0    0]
 [   0    0    0   22   13   16    0    0    0    0    1    7  471    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    1    0    1    0    0    0
  1133    0    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    52  282    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.98644986449864

F1 scores:
[       nan 1.         0.98480717 0.93963255 0.96598639 0.9442623
 0.98574644 1.         0.99883856 0.81818182 0.9527745  0.98937853
 0.93638171 0.98930481 0.97378599 0.88818898 0.97674419]

Kappa:
0.9656337048081644
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb17f732780>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.324, val_acc:0.336]
Epoch [2/120    avg_loss:1.798, val_acc:0.621]
Epoch [3/120    avg_loss:1.486, val_acc:0.666]
Epoch [4/120    avg_loss:1.301, val_acc:0.697]
Epoch [5/120    avg_loss:1.105, val_acc:0.620]
Epoch [6/120    avg_loss:0.866, val_acc:0.765]
Epoch [7/120    avg_loss:0.713, val_acc:0.777]
Epoch [8/120    avg_loss:0.651, val_acc:0.785]
Epoch [9/120    avg_loss:0.525, val_acc:0.819]
Epoch [10/120    avg_loss:0.609, val_acc:0.818]
Epoch [11/120    avg_loss:0.556, val_acc:0.848]
Epoch [12/120    avg_loss:0.410, val_acc:0.879]
Epoch [13/120    avg_loss:0.413, val_acc:0.854]
Epoch [14/120    avg_loss:0.297, val_acc:0.864]
Epoch [15/120    avg_loss:0.267, val_acc:0.900]
Epoch [16/120    avg_loss:0.308, val_acc:0.914]
Epoch [17/120    avg_loss:0.257, val_acc:0.901]
Epoch [18/120    avg_loss:0.266, val_acc:0.890]
Epoch [19/120    avg_loss:0.212, val_acc:0.921]
Epoch [20/120    avg_loss:0.368, val_acc:0.890]
Epoch [21/120    avg_loss:0.310, val_acc:0.901]
Epoch [22/120    avg_loss:0.245, val_acc:0.846]
Epoch [23/120    avg_loss:0.288, val_acc:0.882]
Epoch [24/120    avg_loss:0.253, val_acc:0.945]
Epoch [25/120    avg_loss:0.177, val_acc:0.911]
Epoch [26/120    avg_loss:0.162, val_acc:0.947]
Epoch [27/120    avg_loss:0.135, val_acc:0.946]
Epoch [28/120    avg_loss:0.105, val_acc:0.956]
Epoch [29/120    avg_loss:0.103, val_acc:0.963]
Epoch [30/120    avg_loss:0.091, val_acc:0.950]
Epoch [31/120    avg_loss:0.083, val_acc:0.957]
Epoch [32/120    avg_loss:0.103, val_acc:0.944]
Epoch [33/120    avg_loss:0.100, val_acc:0.934]
Epoch [34/120    avg_loss:0.128, val_acc:0.945]
Epoch [35/120    avg_loss:0.129, val_acc:0.929]
Epoch [36/120    avg_loss:0.117, val_acc:0.963]
Epoch [37/120    avg_loss:0.082, val_acc:0.957]
Epoch [38/120    avg_loss:0.058, val_acc:0.965]
Epoch [39/120    avg_loss:0.053, val_acc:0.968]
Epoch [40/120    avg_loss:0.051, val_acc:0.961]
Epoch [41/120    avg_loss:0.060, val_acc:0.962]
Epoch [42/120    avg_loss:0.061, val_acc:0.953]
Epoch [43/120    avg_loss:0.070, val_acc:0.951]
Epoch [44/120    avg_loss:0.063, val_acc:0.975]
Epoch [45/120    avg_loss:0.051, val_acc:0.970]
Epoch [46/120    avg_loss:0.037, val_acc:0.975]
Epoch [47/120    avg_loss:0.055, val_acc:0.968]
Epoch [48/120    avg_loss:0.060, val_acc:0.945]
Epoch [49/120    avg_loss:0.074, val_acc:0.966]
Epoch [50/120    avg_loss:0.045, val_acc:0.955]
Epoch [51/120    avg_loss:0.042, val_acc:0.968]
Epoch [52/120    avg_loss:0.031, val_acc:0.974]
Epoch [53/120    avg_loss:0.041, val_acc:0.967]
Epoch [54/120    avg_loss:0.027, val_acc:0.980]
Epoch [55/120    avg_loss:0.026, val_acc:0.976]
Epoch [56/120    avg_loss:0.027, val_acc:0.968]
Epoch [57/120    avg_loss:0.035, val_acc:0.979]
Epoch [58/120    avg_loss:0.038, val_acc:0.975]
Epoch [59/120    avg_loss:0.030, val_acc:0.974]
Epoch [60/120    avg_loss:0.034, val_acc:0.974]
Epoch [61/120    avg_loss:0.022, val_acc:0.973]
Epoch [62/120    avg_loss:0.021, val_acc:0.976]
Epoch [63/120    avg_loss:0.031, val_acc:0.961]
Epoch [64/120    avg_loss:0.092, val_acc:0.962]
Epoch [65/120    avg_loss:0.087, val_acc:0.963]
Epoch [66/120    avg_loss:0.064, val_acc:0.964]
Epoch [67/120    avg_loss:0.041, val_acc:0.977]
Epoch [68/120    avg_loss:0.028, val_acc:0.981]
Epoch [69/120    avg_loss:0.020, val_acc:0.984]
Epoch [70/120    avg_loss:0.017, val_acc:0.982]
Epoch [71/120    avg_loss:0.017, val_acc:0.985]
Epoch [72/120    avg_loss:0.017, val_acc:0.985]
Epoch [73/120    avg_loss:0.017, val_acc:0.985]
Epoch [74/120    avg_loss:0.020, val_acc:0.985]
Epoch [75/120    avg_loss:0.017, val_acc:0.985]
Epoch [76/120    avg_loss:0.017, val_acc:0.984]
Epoch [77/120    avg_loss:0.022, val_acc:0.986]
Epoch [78/120    avg_loss:0.016, val_acc:0.986]
Epoch [79/120    avg_loss:0.014, val_acc:0.988]
Epoch [80/120    avg_loss:0.015, val_acc:0.987]
Epoch [81/120    avg_loss:0.015, val_acc:0.987]
Epoch [82/120    avg_loss:0.017, val_acc:0.987]
Epoch [83/120    avg_loss:0.016, val_acc:0.986]
Epoch [84/120    avg_loss:0.014, val_acc:0.986]
Epoch [85/120    avg_loss:0.015, val_acc:0.985]
Epoch [86/120    avg_loss:0.013, val_acc:0.985]
Epoch [87/120    avg_loss:0.014, val_acc:0.986]
Epoch [88/120    avg_loss:0.011, val_acc:0.985]
Epoch [89/120    avg_loss:0.014, val_acc:0.985]
Epoch [90/120    avg_loss:0.012, val_acc:0.984]
Epoch [91/120    avg_loss:0.015, val_acc:0.984]
Epoch [92/120    avg_loss:0.014, val_acc:0.985]
Epoch [93/120    avg_loss:0.014, val_acc:0.986]
Epoch [94/120    avg_loss:0.012, val_acc:0.986]
Epoch [95/120    avg_loss:0.013, val_acc:0.985]
Epoch [96/120    avg_loss:0.012, val_acc:0.986]
Epoch [97/120    avg_loss:0.011, val_acc:0.986]
Epoch [98/120    avg_loss:0.012, val_acc:0.986]
Epoch [99/120    avg_loss:0.012, val_acc:0.986]
Epoch [100/120    avg_loss:0.010, val_acc:0.986]
Epoch [101/120    avg_loss:0.011, val_acc:0.986]
Epoch [102/120    avg_loss:0.015, val_acc:0.986]
Epoch [103/120    avg_loss:0.011, val_acc:0.986]
Epoch [104/120    avg_loss:0.013, val_acc:0.986]
Epoch [105/120    avg_loss:0.021, val_acc:0.986]
Epoch [106/120    avg_loss:0.016, val_acc:0.986]
Epoch [107/120    avg_loss:0.015, val_acc:0.986]
Epoch [108/120    avg_loss:0.013, val_acc:0.986]
Epoch [109/120    avg_loss:0.012, val_acc:0.986]
Epoch [110/120    avg_loss:0.010, val_acc:0.986]
Epoch [111/120    avg_loss:0.013, val_acc:0.986]
Epoch [112/120    avg_loss:0.016, val_acc:0.986]
Epoch [113/120    avg_loss:0.012, val_acc:0.986]
Epoch [114/120    avg_loss:0.013, val_acc:0.986]
Epoch [115/120    avg_loss:0.012, val_acc:0.986]
Epoch [116/120    avg_loss:0.012, val_acc:0.986]
Epoch [117/120    avg_loss:0.012, val_acc:0.986]
Epoch [118/120    avg_loss:0.014, val_acc:0.986]
Epoch [119/120    avg_loss:0.014, val_acc:0.986]
Epoch [120/120    avg_loss:0.012, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    2    0    0    0    0    0    0    4    9    1    0
     0    2    0]
 [   0    0    0  725    0   14    0    0    0    3    0    0    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7   27    0    9    0    0    0    0  809   16    4    0
     1    2    0]
 [   0    0   12    0    0    0    7    0    0    0    8 2134   47    2
     0    0    0]
 [   0    0    0   13    6    5    0    0    0    0    1    7  497    0
     1    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    1    0    0    0
  1132    0    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    42  295    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.92140921409214

F1 scores:
[       nan 1.         0.98560871 0.95772787 0.98611111 0.96115427
 0.98493976 1.         0.99883856 0.87804878 0.95288575 0.9750971
 0.912764   0.99462366 0.97712559 0.91331269 0.97076023]

Kappa:
0.9649295648346514
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7842c93780>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.311, val_acc:0.498]
Epoch [2/120    avg_loss:1.806, val_acc:0.525]
Epoch [3/120    avg_loss:1.459, val_acc:0.648]
Epoch [4/120    avg_loss:1.202, val_acc:0.694]
Epoch [5/120    avg_loss:1.106, val_acc:0.743]
Epoch [6/120    avg_loss:0.893, val_acc:0.738]
Epoch [7/120    avg_loss:0.811, val_acc:0.794]
Epoch [8/120    avg_loss:0.677, val_acc:0.814]
Epoch [9/120    avg_loss:0.661, val_acc:0.833]
Epoch [10/120    avg_loss:0.597, val_acc:0.815]
Epoch [11/120    avg_loss:0.471, val_acc:0.816]
Epoch [12/120    avg_loss:0.510, val_acc:0.861]
Epoch [13/120    avg_loss:0.401, val_acc:0.864]
Epoch [14/120    avg_loss:0.341, val_acc:0.867]
Epoch [15/120    avg_loss:0.419, val_acc:0.864]
Epoch [16/120    avg_loss:0.326, val_acc:0.885]
Epoch [17/120    avg_loss:0.314, val_acc:0.900]
Epoch [18/120    avg_loss:0.269, val_acc:0.909]
Epoch [19/120    avg_loss:0.232, val_acc:0.920]
Epoch [20/120    avg_loss:0.229, val_acc:0.927]
Epoch [21/120    avg_loss:0.199, val_acc:0.902]
Epoch [22/120    avg_loss:0.203, val_acc:0.895]
Epoch [23/120    avg_loss:0.194, val_acc:0.917]
Epoch [24/120    avg_loss:0.186, val_acc:0.904]
Epoch [25/120    avg_loss:0.218, val_acc:0.914]
Epoch [26/120    avg_loss:0.210, val_acc:0.923]
Epoch [27/120    avg_loss:0.167, val_acc:0.913]
Epoch [28/120    avg_loss:0.165, val_acc:0.923]
Epoch [29/120    avg_loss:0.110, val_acc:0.940]
Epoch [30/120    avg_loss:0.118, val_acc:0.922]
Epoch [31/120    avg_loss:0.104, val_acc:0.940]
Epoch [32/120    avg_loss:0.143, val_acc:0.904]
Epoch [33/120    avg_loss:0.134, val_acc:0.921]
Epoch [34/120    avg_loss:0.103, val_acc:0.938]
Epoch [35/120    avg_loss:0.105, val_acc:0.945]
Epoch [36/120    avg_loss:0.183, val_acc:0.871]
Epoch [37/120    avg_loss:0.419, val_acc:0.910]
Epoch [38/120    avg_loss:0.274, val_acc:0.897]
Epoch [39/120    avg_loss:0.192, val_acc:0.920]
Epoch [40/120    avg_loss:0.162, val_acc:0.916]
Epoch [41/120    avg_loss:0.117, val_acc:0.934]
Epoch [42/120    avg_loss:0.121, val_acc:0.944]
Epoch [43/120    avg_loss:0.103, val_acc:0.942]
Epoch [44/120    avg_loss:0.078, val_acc:0.951]
Epoch [45/120    avg_loss:0.069, val_acc:0.964]
Epoch [46/120    avg_loss:0.071, val_acc:0.916]
Epoch [47/120    avg_loss:0.065, val_acc:0.962]
Epoch [48/120    avg_loss:0.048, val_acc:0.947]
Epoch [49/120    avg_loss:0.057, val_acc:0.959]
Epoch [50/120    avg_loss:0.058, val_acc:0.958]
Epoch [51/120    avg_loss:0.061, val_acc:0.955]
Epoch [52/120    avg_loss:0.054, val_acc:0.969]
Epoch [53/120    avg_loss:0.042, val_acc:0.956]
Epoch [54/120    avg_loss:0.052, val_acc:0.962]
Epoch [55/120    avg_loss:0.037, val_acc:0.944]
Epoch [56/120    avg_loss:0.058, val_acc:0.952]
Epoch [57/120    avg_loss:0.044, val_acc:0.944]
Epoch [58/120    avg_loss:0.034, val_acc:0.965]
Epoch [59/120    avg_loss:0.032, val_acc:0.975]
Epoch [60/120    avg_loss:0.047, val_acc:0.962]
Epoch [61/120    avg_loss:0.035, val_acc:0.966]
Epoch [62/120    avg_loss:0.034, val_acc:0.962]
Epoch [63/120    avg_loss:0.036, val_acc:0.966]
Epoch [64/120    avg_loss:0.042, val_acc:0.959]
Epoch [65/120    avg_loss:0.070, val_acc:0.954]
Epoch [66/120    avg_loss:0.068, val_acc:0.969]
Epoch [67/120    avg_loss:0.039, val_acc:0.970]
Epoch [68/120    avg_loss:0.036, val_acc:0.964]
Epoch [69/120    avg_loss:0.093, val_acc:0.945]
Epoch [70/120    avg_loss:0.074, val_acc:0.953]
Epoch [71/120    avg_loss:0.037, val_acc:0.964]
Epoch [72/120    avg_loss:0.029, val_acc:0.962]
Epoch [73/120    avg_loss:0.024, val_acc:0.969]
Epoch [74/120    avg_loss:0.017, val_acc:0.968]
Epoch [75/120    avg_loss:0.020, val_acc:0.971]
Epoch [76/120    avg_loss:0.015, val_acc:0.969]
Epoch [77/120    avg_loss:0.014, val_acc:0.969]
Epoch [78/120    avg_loss:0.019, val_acc:0.973]
Epoch [79/120    avg_loss:0.019, val_acc:0.975]
Epoch [80/120    avg_loss:0.015, val_acc:0.973]
Epoch [81/120    avg_loss:0.014, val_acc:0.974]
Epoch [82/120    avg_loss:0.019, val_acc:0.975]
Epoch [83/120    avg_loss:0.014, val_acc:0.976]
Epoch [84/120    avg_loss:0.014, val_acc:0.975]
Epoch [85/120    avg_loss:0.015, val_acc:0.974]
Epoch [86/120    avg_loss:0.019, val_acc:0.976]
Epoch [87/120    avg_loss:0.015, val_acc:0.976]
Epoch [88/120    avg_loss:0.014, val_acc:0.975]
Epoch [89/120    avg_loss:0.014, val_acc:0.975]
Epoch [90/120    avg_loss:0.014, val_acc:0.976]
Epoch [91/120    avg_loss:0.012, val_acc:0.976]
Epoch [92/120    avg_loss:0.015, val_acc:0.977]
Epoch [93/120    avg_loss:0.015, val_acc:0.976]
Epoch [94/120    avg_loss:0.012, val_acc:0.976]
Epoch [95/120    avg_loss:0.012, val_acc:0.975]
Epoch [96/120    avg_loss:0.014, val_acc:0.973]
Epoch [97/120    avg_loss:0.013, val_acc:0.976]
Epoch [98/120    avg_loss:0.014, val_acc:0.976]
Epoch [99/120    avg_loss:0.011, val_acc:0.974]
Epoch [100/120    avg_loss:0.012, val_acc:0.977]
Epoch [101/120    avg_loss:0.012, val_acc:0.977]
Epoch [102/120    avg_loss:0.013, val_acc:0.977]
Epoch [103/120    avg_loss:0.015, val_acc:0.974]
Epoch [104/120    avg_loss:0.011, val_acc:0.976]
Epoch [105/120    avg_loss:0.013, val_acc:0.977]
Epoch [106/120    avg_loss:0.011, val_acc:0.978]
Epoch [107/120    avg_loss:0.014, val_acc:0.977]
Epoch [108/120    avg_loss:0.013, val_acc:0.978]
Epoch [109/120    avg_loss:0.014, val_acc:0.977]
Epoch [110/120    avg_loss:0.012, val_acc:0.975]
Epoch [111/120    avg_loss:0.012, val_acc:0.976]
Epoch [112/120    avg_loss:0.012, val_acc:0.977]
Epoch [113/120    avg_loss:0.014, val_acc:0.978]
Epoch [114/120    avg_loss:0.015, val_acc:0.977]
Epoch [115/120    avg_loss:0.012, val_acc:0.977]
Epoch [116/120    avg_loss:0.015, val_acc:0.977]
Epoch [117/120    avg_loss:0.012, val_acc:0.978]
Epoch [118/120    avg_loss:0.012, val_acc:0.979]
Epoch [119/120    avg_loss:0.012, val_acc:0.977]
Epoch [120/120    avg_loss:0.011, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    2 1256    7    0    0    1    0    0    1    8    8    2    0
     0    0    0]
 [   0    0    1  716    0   20    0    0    0    6    1    0    1    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    3   17    0    7    0    0    0    0  837    9    0    0
     0    2    0]
 [   0    0    5    1    0    0    4    0    0    0   10 2187    1    2
     0    0    0]
 [   0    0    0   13    4   15    0    0    0    0    4   12  480    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    2    0    2    0    0    0
  1128    2    0]
 [   0    0    0    0    0    0   12    0    0    1    0    0    0    0
    45  289    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.27913279132791

F1 scores:
[       nan 0.96385542 0.98509804 0.95403065 0.99069767 0.9441402
 0.98646617 1.         0.99767981 0.77272727 0.96317606 0.98802801
 0.9421001  0.98930481 0.97451404 0.903125   0.96551724]

Kappa:
0.968973784273083
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2ede6a17b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.359, val_acc:0.564]
Epoch [2/120    avg_loss:1.800, val_acc:0.605]
Epoch [3/120    avg_loss:1.490, val_acc:0.649]
Epoch [4/120    avg_loss:1.173, val_acc:0.669]
Epoch [5/120    avg_loss:1.021, val_acc:0.782]
Epoch [6/120    avg_loss:0.787, val_acc:0.809]
Epoch [7/120    avg_loss:0.693, val_acc:0.803]
Epoch [8/120    avg_loss:0.676, val_acc:0.839]
Epoch [9/120    avg_loss:0.562, val_acc:0.810]
Epoch [10/120    avg_loss:0.460, val_acc:0.861]
Epoch [11/120    avg_loss:0.460, val_acc:0.827]
Epoch [12/120    avg_loss:0.375, val_acc:0.877]
Epoch [13/120    avg_loss:0.377, val_acc:0.850]
Epoch [14/120    avg_loss:0.321, val_acc:0.895]
Epoch [15/120    avg_loss:0.237, val_acc:0.899]
Epoch [16/120    avg_loss:0.231, val_acc:0.888]
Epoch [17/120    avg_loss:0.270, val_acc:0.901]
Epoch [18/120    avg_loss:0.292, val_acc:0.878]
Epoch [19/120    avg_loss:0.230, val_acc:0.888]
Epoch [20/120    avg_loss:0.196, val_acc:0.902]
Epoch [21/120    avg_loss:0.256, val_acc:0.886]
Epoch [22/120    avg_loss:0.241, val_acc:0.886]
Epoch [23/120    avg_loss:0.185, val_acc:0.918]
Epoch [24/120    avg_loss:0.146, val_acc:0.898]
Epoch [25/120    avg_loss:0.146, val_acc:0.931]
Epoch [26/120    avg_loss:0.152, val_acc:0.929]
Epoch [27/120    avg_loss:0.135, val_acc:0.945]
Epoch [28/120    avg_loss:0.105, val_acc:0.934]
Epoch [29/120    avg_loss:0.092, val_acc:0.946]
Epoch [30/120    avg_loss:0.098, val_acc:0.923]
Epoch [31/120    avg_loss:0.124, val_acc:0.924]
Epoch [32/120    avg_loss:0.107, val_acc:0.913]
Epoch [33/120    avg_loss:0.123, val_acc:0.947]
Epoch [34/120    avg_loss:0.122, val_acc:0.920]
Epoch [35/120    avg_loss:0.086, val_acc:0.953]
Epoch [36/120    avg_loss:0.071, val_acc:0.957]
Epoch [37/120    avg_loss:0.062, val_acc:0.953]
Epoch [38/120    avg_loss:0.066, val_acc:0.947]
Epoch [39/120    avg_loss:0.078, val_acc:0.957]
Epoch [40/120    avg_loss:0.086, val_acc:0.945]
Epoch [41/120    avg_loss:0.112, val_acc:0.954]
Epoch [42/120    avg_loss:0.073, val_acc:0.952]
Epoch [43/120    avg_loss:0.096, val_acc:0.934]
Epoch [44/120    avg_loss:0.067, val_acc:0.958]
Epoch [45/120    avg_loss:0.056, val_acc:0.955]
Epoch [46/120    avg_loss:0.079, val_acc:0.963]
Epoch [47/120    avg_loss:0.041, val_acc:0.967]
Epoch [48/120    avg_loss:0.041, val_acc:0.961]
Epoch [49/120    avg_loss:0.035, val_acc:0.970]
Epoch [50/120    avg_loss:0.037, val_acc:0.963]
Epoch [51/120    avg_loss:0.056, val_acc:0.957]
Epoch [52/120    avg_loss:0.046, val_acc:0.963]
Epoch [53/120    avg_loss:0.040, val_acc:0.965]
Epoch [54/120    avg_loss:0.032, val_acc:0.970]
Epoch [55/120    avg_loss:0.032, val_acc:0.964]
Epoch [56/120    avg_loss:0.035, val_acc:0.957]
Epoch [57/120    avg_loss:0.027, val_acc:0.970]
Epoch [58/120    avg_loss:0.040, val_acc:0.961]
Epoch [59/120    avg_loss:0.029, val_acc:0.968]
Epoch [60/120    avg_loss:0.020, val_acc:0.974]
Epoch [61/120    avg_loss:0.019, val_acc:0.975]
Epoch [62/120    avg_loss:0.024, val_acc:0.977]
Epoch [63/120    avg_loss:0.029, val_acc:0.918]
Epoch [64/120    avg_loss:0.040, val_acc:0.961]
Epoch [65/120    avg_loss:0.040, val_acc:0.956]
Epoch [66/120    avg_loss:0.047, val_acc:0.964]
Epoch [67/120    avg_loss:0.039, val_acc:0.977]
Epoch [68/120    avg_loss:0.027, val_acc:0.973]
Epoch [69/120    avg_loss:0.026, val_acc:0.977]
Epoch [70/120    avg_loss:0.019, val_acc:0.971]
Epoch [71/120    avg_loss:0.021, val_acc:0.978]
Epoch [72/120    avg_loss:0.030, val_acc:0.969]
Epoch [73/120    avg_loss:0.024, val_acc:0.976]
Epoch [74/120    avg_loss:0.024, val_acc:0.961]
Epoch [75/120    avg_loss:0.105, val_acc:0.942]
Epoch [76/120    avg_loss:0.066, val_acc:0.968]
Epoch [77/120    avg_loss:0.034, val_acc:0.968]
Epoch [78/120    avg_loss:0.037, val_acc:0.978]
Epoch [79/120    avg_loss:0.022, val_acc:0.979]
Epoch [80/120    avg_loss:0.020, val_acc:0.959]
Epoch [81/120    avg_loss:0.015, val_acc:0.975]
Epoch [82/120    avg_loss:0.025, val_acc:0.964]
Epoch [83/120    avg_loss:0.033, val_acc:0.973]
Epoch [84/120    avg_loss:0.023, val_acc:0.967]
Epoch [85/120    avg_loss:0.022, val_acc:0.975]
Epoch [86/120    avg_loss:0.032, val_acc:0.974]
Epoch [87/120    avg_loss:0.030, val_acc:0.979]
Epoch [88/120    avg_loss:0.034, val_acc:0.969]
Epoch [89/120    avg_loss:0.035, val_acc:0.978]
Epoch [90/120    avg_loss:0.029, val_acc:0.971]
Epoch [91/120    avg_loss:0.026, val_acc:0.959]
Epoch [92/120    avg_loss:0.031, val_acc:0.982]
Epoch [93/120    avg_loss:0.015, val_acc:0.979]
Epoch [94/120    avg_loss:0.011, val_acc:0.982]
Epoch [95/120    avg_loss:0.012, val_acc:0.979]
Epoch [96/120    avg_loss:0.010, val_acc:0.981]
Epoch [97/120    avg_loss:0.012, val_acc:0.980]
Epoch [98/120    avg_loss:0.016, val_acc:0.982]
Epoch [99/120    avg_loss:0.013, val_acc:0.978]
Epoch [100/120    avg_loss:0.013, val_acc:0.984]
Epoch [101/120    avg_loss:0.009, val_acc:0.978]
Epoch [102/120    avg_loss:0.006, val_acc:0.979]
Epoch [103/120    avg_loss:0.007, val_acc:0.974]
Epoch [104/120    avg_loss:0.013, val_acc:0.974]
Epoch [105/120    avg_loss:0.008, val_acc:0.977]
Epoch [106/120    avg_loss:0.016, val_acc:0.978]
Epoch [107/120    avg_loss:0.010, val_acc:0.986]
Epoch [108/120    avg_loss:0.010, val_acc:0.981]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.981]
Epoch [113/120    avg_loss:0.006, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.982]
Epoch [115/120    avg_loss:0.005, val_acc:0.985]
Epoch [116/120    avg_loss:0.004, val_acc:0.982]
Epoch [117/120    avg_loss:0.041, val_acc:0.964]
Epoch [118/120    avg_loss:0.035, val_acc:0.973]
Epoch [119/120    avg_loss:0.014, val_acc:0.978]
Epoch [120/120    avg_loss:0.024, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1268    6    0    0    0    0    0    0    1   10    0    0
     0    0    0]
 [   0    0    0  720    1   15    0    0    0    9    0    0    0    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    1    0    6    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    7   28    0    2    0    0    0    0  820   10    0    0
     0    8    0]
 [   0    0   13    0    0    0    3    0    0    0    9 2170    0    2
     0   13    0]
 [   0    0    0   15   12   12    0    0    0    0    4    0  486    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    0    0    0    0
  1134    0    0]
 [   0    0    0    0    0    1   18    0    0    0    0    0    0    0
    29  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.24661246612466

F1 scores:
[       nan 1.         0.9856199  0.9486166  0.97038724 0.94972067
 0.98426966 0.98039216 1.         0.65306122 0.95962551 0.98636364
 0.95107632 0.98930481 0.98394794 0.89655172 0.95906433]

Kappa:
0.9686226615905141
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4aa90c0780>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.349, val_acc:0.499]
Epoch [2/120    avg_loss:1.800, val_acc:0.573]
Epoch [3/120    avg_loss:1.540, val_acc:0.648]
Epoch [4/120    avg_loss:1.307, val_acc:0.669]
Epoch [5/120    avg_loss:1.113, val_acc:0.691]
Epoch [6/120    avg_loss:0.954, val_acc:0.742]
Epoch [7/120    avg_loss:0.818, val_acc:0.788]
Epoch [8/120    avg_loss:0.773, val_acc:0.724]
Epoch [9/120    avg_loss:0.742, val_acc:0.812]
Epoch [10/120    avg_loss:0.525, val_acc:0.841]
Epoch [11/120    avg_loss:0.444, val_acc:0.814]
Epoch [12/120    avg_loss:0.432, val_acc:0.876]
Epoch [13/120    avg_loss:0.357, val_acc:0.828]
Epoch [14/120    avg_loss:0.515, val_acc:0.827]
Epoch [15/120    avg_loss:0.519, val_acc:0.882]
Epoch [16/120    avg_loss:0.312, val_acc:0.896]
Epoch [17/120    avg_loss:0.266, val_acc:0.884]
Epoch [18/120    avg_loss:0.267, val_acc:0.908]
Epoch [19/120    avg_loss:0.199, val_acc:0.913]
Epoch [20/120    avg_loss:0.216, val_acc:0.921]
Epoch [21/120    avg_loss:0.278, val_acc:0.898]
Epoch [22/120    avg_loss:0.181, val_acc:0.929]
Epoch [23/120    avg_loss:0.176, val_acc:0.935]
Epoch [24/120    avg_loss:0.153, val_acc:0.905]
Epoch [25/120    avg_loss:0.150, val_acc:0.923]
Epoch [26/120    avg_loss:0.132, val_acc:0.945]
Epoch [27/120    avg_loss:0.101, val_acc:0.938]
Epoch [28/120    avg_loss:0.103, val_acc:0.944]
Epoch [29/120    avg_loss:0.124, val_acc:0.922]
Epoch [30/120    avg_loss:0.099, val_acc:0.922]
Epoch [31/120    avg_loss:0.184, val_acc:0.927]
Epoch [32/120    avg_loss:0.113, val_acc:0.929]
Epoch [33/120    avg_loss:0.105, val_acc:0.925]
Epoch [34/120    avg_loss:0.122, val_acc:0.934]
Epoch [35/120    avg_loss:0.082, val_acc:0.953]
Epoch [36/120    avg_loss:0.058, val_acc:0.952]
Epoch [37/120    avg_loss:0.073, val_acc:0.952]
Epoch [38/120    avg_loss:0.061, val_acc:0.959]
Epoch [39/120    avg_loss:0.090, val_acc:0.935]
Epoch [40/120    avg_loss:0.123, val_acc:0.956]
Epoch [41/120    avg_loss:0.051, val_acc:0.957]
Epoch [42/120    avg_loss:0.055, val_acc:0.946]
Epoch [43/120    avg_loss:0.071, val_acc:0.957]
Epoch [44/120    avg_loss:0.046, val_acc:0.957]
Epoch [45/120    avg_loss:0.076, val_acc:0.958]
Epoch [46/120    avg_loss:0.056, val_acc:0.957]
Epoch [47/120    avg_loss:0.066, val_acc:0.964]
Epoch [48/120    avg_loss:0.068, val_acc:0.967]
Epoch [49/120    avg_loss:0.063, val_acc:0.947]
Epoch [50/120    avg_loss:0.085, val_acc:0.955]
Epoch [51/120    avg_loss:0.053, val_acc:0.957]
Epoch [52/120    avg_loss:0.034, val_acc:0.965]
Epoch [53/120    avg_loss:0.040, val_acc:0.964]
Epoch [54/120    avg_loss:0.042, val_acc:0.963]
Epoch [55/120    avg_loss:0.037, val_acc:0.968]
Epoch [56/120    avg_loss:0.047, val_acc:0.943]
Epoch [57/120    avg_loss:0.044, val_acc:0.966]
Epoch [58/120    avg_loss:0.028, val_acc:0.969]
Epoch [59/120    avg_loss:0.025, val_acc:0.967]
Epoch [60/120    avg_loss:0.024, val_acc:0.974]
Epoch [61/120    avg_loss:0.022, val_acc:0.970]
Epoch [62/120    avg_loss:0.031, val_acc:0.961]
Epoch [63/120    avg_loss:0.028, val_acc:0.968]
Epoch [64/120    avg_loss:0.026, val_acc:0.973]
Epoch [65/120    avg_loss:0.020, val_acc:0.966]
Epoch [66/120    avg_loss:0.022, val_acc:0.965]
Epoch [67/120    avg_loss:0.030, val_acc:0.974]
Epoch [68/120    avg_loss:0.020, val_acc:0.970]
Epoch [69/120    avg_loss:0.021, val_acc:0.976]
Epoch [70/120    avg_loss:0.016, val_acc:0.969]
Epoch [71/120    avg_loss:0.018, val_acc:0.973]
Epoch [72/120    avg_loss:0.021, val_acc:0.961]
Epoch [73/120    avg_loss:0.030, val_acc:0.968]
Epoch [74/120    avg_loss:0.042, val_acc:0.967]
Epoch [75/120    avg_loss:0.027, val_acc:0.967]
Epoch [76/120    avg_loss:0.018, val_acc:0.966]
Epoch [77/120    avg_loss:0.019, val_acc:0.968]
Epoch [78/120    avg_loss:0.022, val_acc:0.962]
Epoch [79/120    avg_loss:0.014, val_acc:0.970]
Epoch [80/120    avg_loss:0.014, val_acc:0.971]
Epoch [81/120    avg_loss:0.016, val_acc:0.974]
Epoch [82/120    avg_loss:0.008, val_acc:0.968]
Epoch [83/120    avg_loss:0.011, val_acc:0.977]
Epoch [84/120    avg_loss:0.008, val_acc:0.978]
Epoch [85/120    avg_loss:0.009, val_acc:0.976]
Epoch [86/120    avg_loss:0.007, val_acc:0.976]
Epoch [87/120    avg_loss:0.009, val_acc:0.979]
Epoch [88/120    avg_loss:0.006, val_acc:0.979]
Epoch [89/120    avg_loss:0.009, val_acc:0.977]
Epoch [90/120    avg_loss:0.007, val_acc:0.980]
Epoch [91/120    avg_loss:0.010, val_acc:0.978]
Epoch [92/120    avg_loss:0.007, val_acc:0.981]
Epoch [93/120    avg_loss:0.007, val_acc:0.981]
Epoch [94/120    avg_loss:0.009, val_acc:0.979]
Epoch [95/120    avg_loss:0.006, val_acc:0.979]
Epoch [96/120    avg_loss:0.007, val_acc:0.979]
Epoch [97/120    avg_loss:0.010, val_acc:0.981]
Epoch [98/120    avg_loss:0.007, val_acc:0.979]
Epoch [99/120    avg_loss:0.010, val_acc:0.980]
Epoch [100/120    avg_loss:0.008, val_acc:0.982]
Epoch [101/120    avg_loss:0.009, val_acc:0.981]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.980]
Epoch [104/120    avg_loss:0.008, val_acc:0.981]
Epoch [105/120    avg_loss:0.011, val_acc:0.982]
Epoch [106/120    avg_loss:0.005, val_acc:0.982]
Epoch [107/120    avg_loss:0.007, val_acc:0.982]
Epoch [108/120    avg_loss:0.008, val_acc:0.981]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.005, val_acc:0.984]
Epoch [111/120    avg_loss:0.005, val_acc:0.984]
Epoch [112/120    avg_loss:0.008, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.982]
Epoch [114/120    avg_loss:0.008, val_acc:0.981]
Epoch [115/120    avg_loss:0.007, val_acc:0.982]
Epoch [116/120    avg_loss:0.007, val_acc:0.982]
Epoch [117/120    avg_loss:0.005, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1270    2    0    0    0    0    0    0    4    8    1    0
     0    0    0]
 [   0    0    0  719    0   17    0    0    0    6    0    0    2    2
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0  656    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    0   70    0    3    0    0    0    0  789   10    0    0
     0    3    0]
 [   0    0    3    0    0    0    4    0    0    0    4 2189    8    2
     0    0    0]
 [   0    0    0   13   11    8    0    0    0    0    6   13  482    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   115  232    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.43360433604336

F1 scores:
[       nan 0.98765432 0.9921875  0.92475884 0.97482838 0.96420582
 0.99620349 0.98039216 0.99883856 0.68292683 0.94040524 0.98826185
 0.93774319 0.98930481 0.9511074  0.79725086 0.98809524]

Kappa:
0.9592970040293625
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f586cc68748>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.350, val_acc:0.482]
Epoch [2/120    avg_loss:1.788, val_acc:0.608]
Epoch [3/120    avg_loss:1.454, val_acc:0.666]
Epoch [4/120    avg_loss:1.259, val_acc:0.689]
Epoch [5/120    avg_loss:0.987, val_acc:0.714]
Epoch [6/120    avg_loss:0.773, val_acc:0.751]
Epoch [7/120    avg_loss:0.772, val_acc:0.799]
Epoch [8/120    avg_loss:0.642, val_acc:0.792]
Epoch [9/120    avg_loss:0.665, val_acc:0.797]
Epoch [10/120    avg_loss:0.690, val_acc:0.805]
Epoch [11/120    avg_loss:0.489, val_acc:0.847]
Epoch [12/120    avg_loss:0.416, val_acc:0.820]
Epoch [13/120    avg_loss:0.446, val_acc:0.803]
Epoch [14/120    avg_loss:0.431, val_acc:0.857]
Epoch [15/120    avg_loss:0.371, val_acc:0.867]
Epoch [16/120    avg_loss:0.296, val_acc:0.870]
Epoch [17/120    avg_loss:0.288, val_acc:0.886]
Epoch [18/120    avg_loss:0.341, val_acc:0.856]
Epoch [19/120    avg_loss:0.275, val_acc:0.881]
Epoch [20/120    avg_loss:0.259, val_acc:0.893]
Epoch [21/120    avg_loss:0.265, val_acc:0.895]
Epoch [22/120    avg_loss:0.191, val_acc:0.914]
Epoch [23/120    avg_loss:0.226, val_acc:0.900]
Epoch [24/120    avg_loss:0.199, val_acc:0.895]
Epoch [25/120    avg_loss:0.224, val_acc:0.919]
Epoch [26/120    avg_loss:0.193, val_acc:0.908]
Epoch [27/120    avg_loss:0.155, val_acc:0.923]
Epoch [28/120    avg_loss:0.135, val_acc:0.931]
Epoch [29/120    avg_loss:0.146, val_acc:0.928]
Epoch [30/120    avg_loss:0.227, val_acc:0.911]
Epoch [31/120    avg_loss:0.171, val_acc:0.938]
Epoch [32/120    avg_loss:0.136, val_acc:0.922]
Epoch [33/120    avg_loss:0.138, val_acc:0.943]
Epoch [34/120    avg_loss:0.125, val_acc:0.935]
Epoch [35/120    avg_loss:0.150, val_acc:0.911]
Epoch [36/120    avg_loss:0.139, val_acc:0.917]
Epoch [37/120    avg_loss:0.091, val_acc:0.948]
Epoch [38/120    avg_loss:0.105, val_acc:0.927]
Epoch [39/120    avg_loss:0.140, val_acc:0.941]
Epoch [40/120    avg_loss:0.121, val_acc:0.936]
Epoch [41/120    avg_loss:0.116, val_acc:0.945]
Epoch [42/120    avg_loss:0.123, val_acc:0.935]
Epoch [43/120    avg_loss:0.102, val_acc:0.943]
Epoch [44/120    avg_loss:0.079, val_acc:0.930]
Epoch [45/120    avg_loss:0.074, val_acc:0.955]
Epoch [46/120    avg_loss:0.112, val_acc:0.934]
Epoch [47/120    avg_loss:0.078, val_acc:0.958]
Epoch [48/120    avg_loss:0.053, val_acc:0.959]
Epoch [49/120    avg_loss:0.068, val_acc:0.960]
Epoch [50/120    avg_loss:0.062, val_acc:0.935]
Epoch [51/120    avg_loss:0.062, val_acc:0.959]
Epoch [52/120    avg_loss:0.047, val_acc:0.964]
Epoch [53/120    avg_loss:0.057, val_acc:0.964]
Epoch [54/120    avg_loss:0.051, val_acc:0.961]
Epoch [55/120    avg_loss:0.075, val_acc:0.961]
Epoch [56/120    avg_loss:0.053, val_acc:0.967]
Epoch [57/120    avg_loss:0.082, val_acc:0.959]
Epoch [58/120    avg_loss:0.082, val_acc:0.967]
Epoch [59/120    avg_loss:0.074, val_acc:0.958]
Epoch [60/120    avg_loss:0.047, val_acc:0.966]
Epoch [61/120    avg_loss:0.048, val_acc:0.956]
Epoch [62/120    avg_loss:0.067, val_acc:0.944]
Epoch [63/120    avg_loss:0.106, val_acc:0.932]
Epoch [64/120    avg_loss:0.181, val_acc:0.927]
Epoch [65/120    avg_loss:0.103, val_acc:0.941]
Epoch [66/120    avg_loss:0.074, val_acc:0.963]
Epoch [67/120    avg_loss:0.053, val_acc:0.953]
Epoch [68/120    avg_loss:0.042, val_acc:0.972]
Epoch [69/120    avg_loss:0.039, val_acc:0.956]
Epoch [70/120    avg_loss:0.044, val_acc:0.960]
Epoch [71/120    avg_loss:0.039, val_acc:0.965]
Epoch [72/120    avg_loss:0.031, val_acc:0.972]
Epoch [73/120    avg_loss:0.044, val_acc:0.960]
Epoch [74/120    avg_loss:0.056, val_acc:0.964]
Epoch [75/120    avg_loss:0.068, val_acc:0.966]
Epoch [76/120    avg_loss:0.034, val_acc:0.973]
Epoch [77/120    avg_loss:0.046, val_acc:0.966]
Epoch [78/120    avg_loss:0.037, val_acc:0.956]
Epoch [79/120    avg_loss:0.079, val_acc:0.961]
Epoch [80/120    avg_loss:0.050, val_acc:0.968]
Epoch [81/120    avg_loss:0.067, val_acc:0.972]
Epoch [82/120    avg_loss:0.039, val_acc:0.967]
Epoch [83/120    avg_loss:0.032, val_acc:0.973]
Epoch [84/120    avg_loss:0.037, val_acc:0.977]
Epoch [85/120    avg_loss:0.024, val_acc:0.978]
Epoch [86/120    avg_loss:0.019, val_acc:0.981]
Epoch [87/120    avg_loss:0.020, val_acc:0.984]
Epoch [88/120    avg_loss:0.028, val_acc:0.977]
Epoch [89/120    avg_loss:0.041, val_acc:0.975]
Epoch [90/120    avg_loss:0.025, val_acc:0.970]
Epoch [91/120    avg_loss:0.029, val_acc:0.977]
Epoch [92/120    avg_loss:0.017, val_acc:0.977]
Epoch [93/120    avg_loss:0.037, val_acc:0.977]
Epoch [94/120    avg_loss:0.024, val_acc:0.974]
Epoch [95/120    avg_loss:0.027, val_acc:0.978]
Epoch [96/120    avg_loss:0.027, val_acc:0.976]
Epoch [97/120    avg_loss:0.030, val_acc:0.970]
Epoch [98/120    avg_loss:0.025, val_acc:0.963]
Epoch [99/120    avg_loss:0.021, val_acc:0.975]
Epoch [100/120    avg_loss:0.017, val_acc:0.977]
Epoch [101/120    avg_loss:0.014, val_acc:0.982]
Epoch [102/120    avg_loss:0.011, val_acc:0.985]
Epoch [103/120    avg_loss:0.012, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.985]
Epoch [105/120    avg_loss:0.009, val_acc:0.985]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.009, val_acc:0.986]
Epoch [108/120    avg_loss:0.010, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.012, val_acc:0.988]
Epoch [111/120    avg_loss:0.008, val_acc:0.988]
Epoch [112/120    avg_loss:0.011, val_acc:0.988]
Epoch [113/120    avg_loss:0.009, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.010, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.009, val_acc:0.986]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.007, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1264    1    0    0    2    0    0    0    5   11    0    0
     0    2    0]
 [   0    0    0  721    0   18    0    0    0    3    0    0    3    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    3    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   11   90    0    7    0    0    0    0  763    1    0    0
     0    3    0]
 [   0    0    5    0    0    1    5    0    0    0    6 2190    0    3
     0    0    0]
 [   0    0    0   10   10    9    0    0    0    0    8    7  488    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    2    0    0
  1134    1    0]
 [   0    0    0    0    0    1   18    0    0    1    0    0    0    0
    68  259    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.42276422764228

F1 scores:
[       nan 0.98765432 0.98519096 0.91905672 0.97706422 0.94983278
 0.98056801 1.         0.99883856 0.8372093  0.92038601 0.99050204
 0.95126706 0.98666667 0.96592845 0.84640523 0.98809524]

Kappa:
0.9591972763603958
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f52c34776d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.323, val_acc:0.570]
Epoch [2/120    avg_loss:1.747, val_acc:0.600]
Epoch [3/120    avg_loss:1.485, val_acc:0.622]
Epoch [4/120    avg_loss:1.276, val_acc:0.707]
Epoch [5/120    avg_loss:1.055, val_acc:0.735]
Epoch [6/120    avg_loss:0.860, val_acc:0.780]
Epoch [7/120    avg_loss:0.825, val_acc:0.723]
Epoch [8/120    avg_loss:0.683, val_acc:0.773]
Epoch [9/120    avg_loss:0.615, val_acc:0.837]
Epoch [10/120    avg_loss:0.581, val_acc:0.818]
Epoch [11/120    avg_loss:0.591, val_acc:0.825]
Epoch [12/120    avg_loss:0.493, val_acc:0.843]
Epoch [13/120    avg_loss:0.446, val_acc:0.871]
Epoch [14/120    avg_loss:0.367, val_acc:0.826]
Epoch [15/120    avg_loss:0.348, val_acc:0.877]
Epoch [16/120    avg_loss:0.388, val_acc:0.859]
Epoch [17/120    avg_loss:0.359, val_acc:0.871]
Epoch [18/120    avg_loss:0.298, val_acc:0.893]
Epoch [19/120    avg_loss:0.297, val_acc:0.888]
Epoch [20/120    avg_loss:0.216, val_acc:0.893]
Epoch [21/120    avg_loss:0.238, val_acc:0.886]
Epoch [22/120    avg_loss:0.226, val_acc:0.920]
Epoch [23/120    avg_loss:0.262, val_acc:0.896]
Epoch [24/120    avg_loss:0.562, val_acc:0.821]
Epoch [25/120    avg_loss:0.378, val_acc:0.909]
Epoch [26/120    avg_loss:0.252, val_acc:0.899]
Epoch [27/120    avg_loss:0.247, val_acc:0.908]
Epoch [28/120    avg_loss:0.229, val_acc:0.904]
Epoch [29/120    avg_loss:0.211, val_acc:0.930]
Epoch [30/120    avg_loss:0.158, val_acc:0.948]
Epoch [31/120    avg_loss:0.151, val_acc:0.923]
Epoch [32/120    avg_loss:0.158, val_acc:0.939]
Epoch [33/120    avg_loss:0.190, val_acc:0.899]
Epoch [34/120    avg_loss:0.161, val_acc:0.913]
Epoch [35/120    avg_loss:0.172, val_acc:0.912]
Epoch [36/120    avg_loss:0.134, val_acc:0.948]
Epoch [37/120    avg_loss:0.116, val_acc:0.957]
Epoch [38/120    avg_loss:0.120, val_acc:0.931]
Epoch [39/120    avg_loss:0.110, val_acc:0.951]
Epoch [40/120    avg_loss:0.129, val_acc:0.945]
Epoch [41/120    avg_loss:0.102, val_acc:0.954]
Epoch [42/120    avg_loss:0.107, val_acc:0.956]
Epoch [43/120    avg_loss:0.098, val_acc:0.961]
Epoch [44/120    avg_loss:0.069, val_acc:0.969]
Epoch [45/120    avg_loss:0.070, val_acc:0.968]
Epoch [46/120    avg_loss:0.109, val_acc:0.956]
Epoch [47/120    avg_loss:0.097, val_acc:0.957]
Epoch [48/120    avg_loss:0.085, val_acc:0.954]
Epoch [49/120    avg_loss:0.068, val_acc:0.951]
Epoch [50/120    avg_loss:0.053, val_acc:0.963]
Epoch [51/120    avg_loss:0.071, val_acc:0.964]
Epoch [52/120    avg_loss:0.049, val_acc:0.962]
Epoch [53/120    avg_loss:0.078, val_acc:0.956]
Epoch [54/120    avg_loss:0.085, val_acc:0.951]
Epoch [55/120    avg_loss:0.054, val_acc:0.942]
Epoch [56/120    avg_loss:0.069, val_acc:0.975]
Epoch [57/120    avg_loss:0.040, val_acc:0.971]
Epoch [58/120    avg_loss:0.048, val_acc:0.966]
Epoch [59/120    avg_loss:0.048, val_acc:0.966]
Epoch [60/120    avg_loss:0.054, val_acc:0.967]
Epoch [61/120    avg_loss:0.046, val_acc:0.967]
Epoch [62/120    avg_loss:0.040, val_acc:0.973]
Epoch [63/120    avg_loss:0.046, val_acc:0.980]
Epoch [64/120    avg_loss:0.101, val_acc:0.914]
Epoch [65/120    avg_loss:0.132, val_acc:0.959]
Epoch [66/120    avg_loss:0.065, val_acc:0.973]
Epoch [67/120    avg_loss:0.039, val_acc:0.973]
Epoch [68/120    avg_loss:0.046, val_acc:0.964]
Epoch [69/120    avg_loss:0.077, val_acc:0.968]
Epoch [70/120    avg_loss:0.052, val_acc:0.973]
Epoch [71/120    avg_loss:0.051, val_acc:0.973]
Epoch [72/120    avg_loss:0.057, val_acc:0.973]
Epoch [73/120    avg_loss:0.049, val_acc:0.978]
Epoch [74/120    avg_loss:0.038, val_acc:0.978]
Epoch [75/120    avg_loss:0.046, val_acc:0.974]
Epoch [76/120    avg_loss:0.023, val_acc:0.978]
Epoch [77/120    avg_loss:0.033, val_acc:0.985]
Epoch [78/120    avg_loss:0.021, val_acc:0.985]
Epoch [79/120    avg_loss:0.019, val_acc:0.984]
Epoch [80/120    avg_loss:0.018, val_acc:0.985]
Epoch [81/120    avg_loss:0.021, val_acc:0.984]
Epoch [82/120    avg_loss:0.019, val_acc:0.987]
Epoch [83/120    avg_loss:0.014, val_acc:0.989]
Epoch [84/120    avg_loss:0.018, val_acc:0.988]
Epoch [85/120    avg_loss:0.014, val_acc:0.988]
Epoch [86/120    avg_loss:0.021, val_acc:0.988]
Epoch [87/120    avg_loss:0.021, val_acc:0.988]
Epoch [88/120    avg_loss:0.011, val_acc:0.990]
Epoch [89/120    avg_loss:0.018, val_acc:0.987]
Epoch [90/120    avg_loss:0.015, val_acc:0.988]
Epoch [91/120    avg_loss:0.014, val_acc:0.989]
Epoch [92/120    avg_loss:0.020, val_acc:0.989]
Epoch [93/120    avg_loss:0.015, val_acc:0.991]
Epoch [94/120    avg_loss:0.015, val_acc:0.989]
Epoch [95/120    avg_loss:0.013, val_acc:0.990]
Epoch [96/120    avg_loss:0.016, val_acc:0.989]
Epoch [97/120    avg_loss:0.018, val_acc:0.989]
Epoch [98/120    avg_loss:0.013, val_acc:0.989]
Epoch [99/120    avg_loss:0.015, val_acc:0.989]
Epoch [100/120    avg_loss:0.015, val_acc:0.991]
Epoch [101/120    avg_loss:0.017, val_acc:0.990]
Epoch [102/120    avg_loss:0.014, val_acc:0.989]
Epoch [103/120    avg_loss:0.020, val_acc:0.989]
Epoch [104/120    avg_loss:0.014, val_acc:0.990]
Epoch [105/120    avg_loss:0.013, val_acc:0.990]
Epoch [106/120    avg_loss:0.015, val_acc:0.990]
Epoch [107/120    avg_loss:0.016, val_acc:0.990]
Epoch [108/120    avg_loss:0.014, val_acc:0.992]
Epoch [109/120    avg_loss:0.016, val_acc:0.991]
Epoch [110/120    avg_loss:0.017, val_acc:0.992]
Epoch [111/120    avg_loss:0.016, val_acc:0.992]
Epoch [112/120    avg_loss:0.014, val_acc:0.993]
Epoch [113/120    avg_loss:0.014, val_acc:0.992]
Epoch [114/120    avg_loss:0.015, val_acc:0.992]
Epoch [115/120    avg_loss:0.015, val_acc:0.991]
Epoch [116/120    avg_loss:0.015, val_acc:0.991]
Epoch [117/120    avg_loss:0.013, val_acc:0.992]
Epoch [118/120    avg_loss:0.018, val_acc:0.992]
Epoch [119/120    avg_loss:0.013, val_acc:0.989]
Epoch [120/120    avg_loss:0.014, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1268    1    0    0    1    0    0    3    3    7    0    0
     0    2    0]
 [   0    0    3  719    0   15    0    0    0    4    0    0    3    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    5    0    0    0    0    0    0  425    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    1   83    0    6    0    0    0    0  780    1    1    0
     0    3    0]
 [   0    0    7    0    0    0    5    0    0    0   10 2186    0    2
     0    0    0]
 [   0    0    0    1    0    9    0    0    0    0    4    0  513    0
     1    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    1    0    0
  1136    0    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    75  253    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.80216802168022

F1 scores:
[       nan 0.93023256 0.98869396 0.92535393 1.         0.9632107
 0.98132935 1.         0.99299065 0.71428571 0.93245666 0.99250851
 0.97528517 0.98666667 0.96598639 0.83636364 0.95953757]

Kappa:
0.9635365380938413
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f53cda3c780>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.342, val_acc:0.515]
Epoch [2/120    avg_loss:1.795, val_acc:0.595]
Epoch [3/120    avg_loss:1.559, val_acc:0.627]
Epoch [4/120    avg_loss:1.256, val_acc:0.702]
Epoch [5/120    avg_loss:1.043, val_acc:0.717]
Epoch [6/120    avg_loss:0.871, val_acc:0.651]
Epoch [7/120    avg_loss:0.769, val_acc:0.757]
Epoch [8/120    avg_loss:0.759, val_acc:0.712]
Epoch [9/120    avg_loss:0.624, val_acc:0.781]
Epoch [10/120    avg_loss:0.647, val_acc:0.757]
Epoch [11/120    avg_loss:0.551, val_acc:0.842]
Epoch [12/120    avg_loss:0.483, val_acc:0.836]
Epoch [13/120    avg_loss:0.422, val_acc:0.845]
Epoch [14/120    avg_loss:0.387, val_acc:0.886]
Epoch [15/120    avg_loss:0.332, val_acc:0.872]
Epoch [16/120    avg_loss:0.308, val_acc:0.847]
Epoch [17/120    avg_loss:0.336, val_acc:0.896]
Epoch [18/120    avg_loss:0.234, val_acc:0.910]
Epoch [19/120    avg_loss:0.256, val_acc:0.883]
Epoch [20/120    avg_loss:0.222, val_acc:0.893]
Epoch [21/120    avg_loss:0.226, val_acc:0.920]
Epoch [22/120    avg_loss:0.257, val_acc:0.888]
Epoch [23/120    avg_loss:0.275, val_acc:0.858]
Epoch [24/120    avg_loss:0.246, val_acc:0.902]
Epoch [25/120    avg_loss:0.204, val_acc:0.941]
Epoch [26/120    avg_loss:0.215, val_acc:0.923]
Epoch [27/120    avg_loss:0.176, val_acc:0.905]
Epoch [28/120    avg_loss:0.159, val_acc:0.940]
Epoch [29/120    avg_loss:0.174, val_acc:0.926]
Epoch [30/120    avg_loss:0.189, val_acc:0.932]
Epoch [31/120    avg_loss:0.142, val_acc:0.938]
Epoch [32/120    avg_loss:0.153, val_acc:0.915]
Epoch [33/120    avg_loss:0.135, val_acc:0.950]
Epoch [34/120    avg_loss:0.111, val_acc:0.944]
Epoch [35/120    avg_loss:0.139, val_acc:0.945]
Epoch [36/120    avg_loss:0.087, val_acc:0.960]
Epoch [37/120    avg_loss:0.081, val_acc:0.955]
Epoch [38/120    avg_loss:0.077, val_acc:0.938]
Epoch [39/120    avg_loss:0.082, val_acc:0.951]
Epoch [40/120    avg_loss:0.082, val_acc:0.961]
Epoch [41/120    avg_loss:0.070, val_acc:0.977]
Epoch [42/120    avg_loss:0.061, val_acc:0.975]
Epoch [43/120    avg_loss:0.071, val_acc:0.954]
Epoch [44/120    avg_loss:0.080, val_acc:0.963]
Epoch [45/120    avg_loss:0.055, val_acc:0.973]
Epoch [46/120    avg_loss:0.043, val_acc:0.970]
Epoch [47/120    avg_loss:0.047, val_acc:0.962]
Epoch [48/120    avg_loss:0.051, val_acc:0.949]
Epoch [49/120    avg_loss:0.118, val_acc:0.923]
Epoch [50/120    avg_loss:0.083, val_acc:0.959]
Epoch [51/120    avg_loss:0.070, val_acc:0.974]
Epoch [52/120    avg_loss:0.052, val_acc:0.970]
Epoch [53/120    avg_loss:0.055, val_acc:0.974]
Epoch [54/120    avg_loss:0.042, val_acc:0.969]
Epoch [55/120    avg_loss:0.029, val_acc:0.978]
Epoch [56/120    avg_loss:0.030, val_acc:0.980]
Epoch [57/120    avg_loss:0.025, val_acc:0.981]
Epoch [58/120    avg_loss:0.027, val_acc:0.982]
Epoch [59/120    avg_loss:0.024, val_acc:0.982]
Epoch [60/120    avg_loss:0.024, val_acc:0.983]
Epoch [61/120    avg_loss:0.020, val_acc:0.983]
Epoch [62/120    avg_loss:0.025, val_acc:0.983]
Epoch [63/120    avg_loss:0.024, val_acc:0.984]
Epoch [64/120    avg_loss:0.019, val_acc:0.984]
Epoch [65/120    avg_loss:0.024, val_acc:0.985]
Epoch [66/120    avg_loss:0.024, val_acc:0.985]
Epoch [67/120    avg_loss:0.019, val_acc:0.987]
Epoch [68/120    avg_loss:0.021, val_acc:0.987]
Epoch [69/120    avg_loss:0.020, val_acc:0.987]
Epoch [70/120    avg_loss:0.019, val_acc:0.987]
Epoch [71/120    avg_loss:0.021, val_acc:0.985]
Epoch [72/120    avg_loss:0.025, val_acc:0.987]
Epoch [73/120    avg_loss:0.019, val_acc:0.985]
Epoch [74/120    avg_loss:0.018, val_acc:0.985]
Epoch [75/120    avg_loss:0.021, val_acc:0.984]
Epoch [76/120    avg_loss:0.025, val_acc:0.985]
Epoch [77/120    avg_loss:0.015, val_acc:0.987]
Epoch [78/120    avg_loss:0.017, val_acc:0.985]
Epoch [79/120    avg_loss:0.018, val_acc:0.984]
Epoch [80/120    avg_loss:0.016, val_acc:0.984]
Epoch [81/120    avg_loss:0.016, val_acc:0.985]
Epoch [82/120    avg_loss:0.016, val_acc:0.984]
Epoch [83/120    avg_loss:0.023, val_acc:0.984]
Epoch [84/120    avg_loss:0.020, val_acc:0.985]
Epoch [85/120    avg_loss:0.022, val_acc:0.987]
Epoch [86/120    avg_loss:0.019, val_acc:0.987]
Epoch [87/120    avg_loss:0.020, val_acc:0.988]
Epoch [88/120    avg_loss:0.017, val_acc:0.988]
Epoch [89/120    avg_loss:0.019, val_acc:0.988]
Epoch [90/120    avg_loss:0.016, val_acc:0.988]
Epoch [91/120    avg_loss:0.020, val_acc:0.988]
Epoch [92/120    avg_loss:0.016, val_acc:0.989]
Epoch [93/120    avg_loss:0.014, val_acc:0.989]
Epoch [94/120    avg_loss:0.015, val_acc:0.989]
Epoch [95/120    avg_loss:0.016, val_acc:0.988]
Epoch [96/120    avg_loss:0.016, val_acc:0.987]
Epoch [97/120    avg_loss:0.014, val_acc:0.988]
Epoch [98/120    avg_loss:0.020, val_acc:0.988]
Epoch [99/120    avg_loss:0.016, val_acc:0.988]
Epoch [100/120    avg_loss:0.018, val_acc:0.988]
Epoch [101/120    avg_loss:0.017, val_acc:0.988]
Epoch [102/120    avg_loss:0.019, val_acc:0.987]
Epoch [103/120    avg_loss:0.019, val_acc:0.988]
Epoch [104/120    avg_loss:0.020, val_acc:0.990]
Epoch [105/120    avg_loss:0.015, val_acc:0.989]
Epoch [106/120    avg_loss:0.013, val_acc:0.989]
Epoch [107/120    avg_loss:0.018, val_acc:0.987]
Epoch [108/120    avg_loss:0.014, val_acc:0.987]
Epoch [109/120    avg_loss:0.016, val_acc:0.987]
Epoch [110/120    avg_loss:0.017, val_acc:0.985]
Epoch [111/120    avg_loss:0.019, val_acc:0.984]
Epoch [112/120    avg_loss:0.013, val_acc:0.987]
Epoch [113/120    avg_loss:0.017, val_acc:0.988]
Epoch [114/120    avg_loss:0.021, val_acc:0.988]
Epoch [115/120    avg_loss:0.015, val_acc:0.988]
Epoch [116/120    avg_loss:0.013, val_acc:0.987]
Epoch [117/120    avg_loss:0.017, val_acc:0.988]
Epoch [118/120    avg_loss:0.014, val_acc:0.988]
Epoch [119/120    avg_loss:0.015, val_acc:0.988]
Epoch [120/120    avg_loss:0.014, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1272    0    0    0    0    0    0    1    4    8    0    0
     0    0    0]
 [   0    0    1  729    0    9    0    0    0    7    0    0    1    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   12   84    0    5    2    0    0    0  760    8    0    0
     0    4    0]
 [   0    0    9    0    0    1    5    0    0    0    8 2131   54    2
     0    0    0]
 [   0    0    1   23    7    1    0    0    0    0    9    4  485    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
   109  227    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.59891598915989

F1 scores:
[       nan 0.975      0.98566447 0.91929382 0.98148148 0.97616345
 0.98648649 1.         1.         0.72727273 0.91621459 0.97729878
 0.90316574 0.99462366 0.95226131 0.78546713 0.97674419]

Kappa:
0.9498372595525744
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f64b9ffa710>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.343, val_acc:0.498]
Epoch [2/120    avg_loss:1.746, val_acc:0.596]
Epoch [3/120    avg_loss:1.552, val_acc:0.581]
Epoch [4/120    avg_loss:1.302, val_acc:0.684]
Epoch [5/120    avg_loss:1.065, val_acc:0.661]
Epoch [6/120    avg_loss:0.975, val_acc:0.730]
Epoch [7/120    avg_loss:0.794, val_acc:0.739]
Epoch [8/120    avg_loss:0.740, val_acc:0.786]
Epoch [9/120    avg_loss:0.638, val_acc:0.741]
Epoch [10/120    avg_loss:0.599, val_acc:0.722]
Epoch [11/120    avg_loss:0.656, val_acc:0.779]
Epoch [12/120    avg_loss:0.494, val_acc:0.806]
Epoch [13/120    avg_loss:0.482, val_acc:0.758]
Epoch [14/120    avg_loss:0.535, val_acc:0.831]
Epoch [15/120    avg_loss:0.329, val_acc:0.858]
Epoch [16/120    avg_loss:0.352, val_acc:0.856]
Epoch [17/120    avg_loss:0.342, val_acc:0.874]
Epoch [18/120    avg_loss:0.297, val_acc:0.901]
Epoch [19/120    avg_loss:0.329, val_acc:0.869]
Epoch [20/120    avg_loss:0.331, val_acc:0.834]
Epoch [21/120    avg_loss:0.355, val_acc:0.865]
Epoch [22/120    avg_loss:0.224, val_acc:0.901]
Epoch [23/120    avg_loss:0.258, val_acc:0.867]
Epoch [24/120    avg_loss:0.350, val_acc:0.842]
Epoch [25/120    avg_loss:0.212, val_acc:0.883]
Epoch [26/120    avg_loss:0.231, val_acc:0.919]
Epoch [27/120    avg_loss:0.161, val_acc:0.895]
Epoch [28/120    avg_loss:0.173, val_acc:0.917]
Epoch [29/120    avg_loss:0.158, val_acc:0.915]
Epoch [30/120    avg_loss:0.140, val_acc:0.932]
Epoch [31/120    avg_loss:0.160, val_acc:0.924]
Epoch [32/120    avg_loss:0.136, val_acc:0.927]
Epoch [33/120    avg_loss:0.191, val_acc:0.915]
Epoch [34/120    avg_loss:0.122, val_acc:0.942]
Epoch [35/120    avg_loss:0.152, val_acc:0.892]
Epoch [36/120    avg_loss:0.130, val_acc:0.941]
Epoch [37/120    avg_loss:0.121, val_acc:0.944]
Epoch [38/120    avg_loss:0.113, val_acc:0.917]
Epoch [39/120    avg_loss:0.103, val_acc:0.942]
Epoch [40/120    avg_loss:0.070, val_acc:0.954]
Epoch [41/120    avg_loss:0.088, val_acc:0.939]
Epoch [42/120    avg_loss:0.112, val_acc:0.927]
Epoch [43/120    avg_loss:0.095, val_acc:0.949]
Epoch [44/120    avg_loss:0.078, val_acc:0.934]
Epoch [45/120    avg_loss:0.076, val_acc:0.942]
Epoch [46/120    avg_loss:0.060, val_acc:0.954]
Epoch [47/120    avg_loss:0.047, val_acc:0.958]
Epoch [48/120    avg_loss:0.044, val_acc:0.953]
Epoch [49/120    avg_loss:0.047, val_acc:0.955]
Epoch [50/120    avg_loss:0.049, val_acc:0.962]
Epoch [51/120    avg_loss:0.055, val_acc:0.960]
Epoch [52/120    avg_loss:0.146, val_acc:0.921]
Epoch [53/120    avg_loss:0.090, val_acc:0.955]
Epoch [54/120    avg_loss:0.060, val_acc:0.950]
Epoch [55/120    avg_loss:0.053, val_acc:0.945]
Epoch [56/120    avg_loss:0.054, val_acc:0.968]
Epoch [57/120    avg_loss:0.041, val_acc:0.955]
Epoch [58/120    avg_loss:0.052, val_acc:0.943]
Epoch [59/120    avg_loss:0.053, val_acc:0.978]
Epoch [60/120    avg_loss:0.040, val_acc:0.960]
Epoch [61/120    avg_loss:0.043, val_acc:0.950]
Epoch [62/120    avg_loss:0.036, val_acc:0.959]
Epoch [63/120    avg_loss:0.038, val_acc:0.930]
Epoch [64/120    avg_loss:0.079, val_acc:0.949]
Epoch [65/120    avg_loss:0.089, val_acc:0.940]
Epoch [66/120    avg_loss:0.070, val_acc:0.960]
Epoch [67/120    avg_loss:0.060, val_acc:0.969]
Epoch [68/120    avg_loss:0.046, val_acc:0.974]
Epoch [69/120    avg_loss:0.031, val_acc:0.971]
Epoch [70/120    avg_loss:0.033, val_acc:0.975]
Epoch [71/120    avg_loss:0.035, val_acc:0.971]
Epoch [72/120    avg_loss:0.038, val_acc:0.963]
Epoch [73/120    avg_loss:0.031, val_acc:0.969]
Epoch [74/120    avg_loss:0.019, val_acc:0.971]
Epoch [75/120    avg_loss:0.019, val_acc:0.974]
Epoch [76/120    avg_loss:0.019, val_acc:0.977]
Epoch [77/120    avg_loss:0.015, val_acc:0.977]
Epoch [78/120    avg_loss:0.017, val_acc:0.978]
Epoch [79/120    avg_loss:0.013, val_acc:0.979]
Epoch [80/120    avg_loss:0.017, val_acc:0.977]
Epoch [81/120    avg_loss:0.014, val_acc:0.973]
Epoch [82/120    avg_loss:0.016, val_acc:0.975]
Epoch [83/120    avg_loss:0.015, val_acc:0.977]
Epoch [84/120    avg_loss:0.013, val_acc:0.978]
Epoch [85/120    avg_loss:0.016, val_acc:0.977]
Epoch [86/120    avg_loss:0.013, val_acc:0.977]
Epoch [87/120    avg_loss:0.013, val_acc:0.980]
Epoch [88/120    avg_loss:0.012, val_acc:0.978]
Epoch [89/120    avg_loss:0.015, val_acc:0.979]
Epoch [90/120    avg_loss:0.011, val_acc:0.979]
Epoch [91/120    avg_loss:0.014, val_acc:0.980]
Epoch [92/120    avg_loss:0.016, val_acc:0.979]
Epoch [93/120    avg_loss:0.013, val_acc:0.981]
Epoch [94/120    avg_loss:0.013, val_acc:0.981]
Epoch [95/120    avg_loss:0.012, val_acc:0.977]
Epoch [96/120    avg_loss:0.013, val_acc:0.978]
Epoch [97/120    avg_loss:0.013, val_acc:0.981]
Epoch [98/120    avg_loss:0.014, val_acc:0.982]
Epoch [99/120    avg_loss:0.012, val_acc:0.981]
Epoch [100/120    avg_loss:0.016, val_acc:0.979]
Epoch [101/120    avg_loss:0.012, val_acc:0.979]
Epoch [102/120    avg_loss:0.013, val_acc:0.981]
Epoch [103/120    avg_loss:0.013, val_acc:0.982]
Epoch [104/120    avg_loss:0.011, val_acc:0.984]
Epoch [105/120    avg_loss:0.012, val_acc:0.983]
Epoch [106/120    avg_loss:0.012, val_acc:0.979]
Epoch [107/120    avg_loss:0.011, val_acc:0.980]
Epoch [108/120    avg_loss:0.014, val_acc:0.981]
Epoch [109/120    avg_loss:0.011, val_acc:0.984]
Epoch [110/120    avg_loss:0.014, val_acc:0.981]
Epoch [111/120    avg_loss:0.018, val_acc:0.980]
Epoch [112/120    avg_loss:0.012, val_acc:0.981]
Epoch [113/120    avg_loss:0.013, val_acc:0.982]
Epoch [114/120    avg_loss:0.010, val_acc:0.981]
Epoch [115/120    avg_loss:0.011, val_acc:0.982]
Epoch [116/120    avg_loss:0.011, val_acc:0.983]
Epoch [117/120    avg_loss:0.012, val_acc:0.982]
Epoch [118/120    avg_loss:0.010, val_acc:0.983]
Epoch [119/120    avg_loss:0.010, val_acc:0.982]
Epoch [120/120    avg_loss:0.010, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1239    0    0    0    0    0    0    0   29    9    8    0
     0    0    0]
 [   0    0    1  731    0    3    0    0    0    5    0    0    2    5
     0    0    0]
 [   0    0    0    0  210    0    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    0    0    3    0    0    0    0
    10    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   11   85    0    6    0    0    0    0  759    6    1    1
     1    5    0]
 [   0    0    6    0    0    2    7    0    0    0   10 2182    0    2
     1    0    0]
 [   0    1    0    4    3   10    0    0    0    0    2   16  495    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    1    0    0
  1135    0    0]
 [   0    0    0    0    0    0   12    0    0    2    0    0    0    0
    94  239    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.90243902439025

F1 scores:
[       nan 0.96296296 0.97482297 0.93299298 0.98591549 0.96127563
 0.98422239 1.         1.         0.70833333 0.90357143 0.98599187
 0.95100865 0.97883598 0.95378151 0.80879865 0.98245614]

Kappa:
0.9532508054970472
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:21:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feb37de1780>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.329, val_acc:0.499]
Epoch [2/120    avg_loss:1.771, val_acc:0.540]
Epoch [3/120    avg_loss:1.519, val_acc:0.592]
Epoch [4/120    avg_loss:1.231, val_acc:0.678]
Epoch [5/120    avg_loss:1.047, val_acc:0.705]
Epoch [6/120    avg_loss:0.943, val_acc:0.767]
Epoch [7/120    avg_loss:0.724, val_acc:0.818]
Epoch [8/120    avg_loss:0.709, val_acc:0.802]
Epoch [9/120    avg_loss:0.684, val_acc:0.794]
Epoch [10/120    avg_loss:0.578, val_acc:0.823]
Epoch [11/120    avg_loss:0.457, val_acc:0.865]
Epoch [12/120    avg_loss:0.448, val_acc:0.836]
Epoch [13/120    avg_loss:0.412, val_acc:0.886]
Epoch [14/120    avg_loss:0.407, val_acc:0.868]
Epoch [15/120    avg_loss:0.339, val_acc:0.874]
Epoch [16/120    avg_loss:0.349, val_acc:0.878]
Epoch [17/120    avg_loss:0.289, val_acc:0.890]
Epoch [18/120    avg_loss:0.226, val_acc:0.918]
Epoch [19/120    avg_loss:0.319, val_acc:0.881]
Epoch [20/120    avg_loss:0.244, val_acc:0.931]
Epoch [21/120    avg_loss:0.185, val_acc:0.917]
Epoch [22/120    avg_loss:0.210, val_acc:0.897]
Epoch [23/120    avg_loss:0.249, val_acc:0.914]
Epoch [24/120    avg_loss:0.131, val_acc:0.915]
Epoch [25/120    avg_loss:0.204, val_acc:0.907]
Epoch [26/120    avg_loss:0.172, val_acc:0.931]
Epoch [27/120    avg_loss:0.231, val_acc:0.927]
Epoch [28/120    avg_loss:0.210, val_acc:0.919]
Epoch [29/120    avg_loss:0.180, val_acc:0.925]
Epoch [30/120    avg_loss:0.130, val_acc:0.922]
Epoch [31/120    avg_loss:0.114, val_acc:0.944]
Epoch [32/120    avg_loss:0.112, val_acc:0.923]
Epoch [33/120    avg_loss:0.121, val_acc:0.945]
Epoch [34/120    avg_loss:0.311, val_acc:0.857]
Epoch [35/120    avg_loss:0.197, val_acc:0.907]
Epoch [36/120    avg_loss:0.146, val_acc:0.930]
Epoch [37/120    avg_loss:0.142, val_acc:0.930]
Epoch [38/120    avg_loss:0.112, val_acc:0.949]
Epoch [39/120    avg_loss:0.128, val_acc:0.928]
Epoch [40/120    avg_loss:0.113, val_acc:0.932]
Epoch [41/120    avg_loss:0.119, val_acc:0.957]
Epoch [42/120    avg_loss:0.080, val_acc:0.950]
Epoch [43/120    avg_loss:0.065, val_acc:0.957]
Epoch [44/120    avg_loss:0.068, val_acc:0.961]
Epoch [45/120    avg_loss:0.113, val_acc:0.926]
Epoch [46/120    avg_loss:0.083, val_acc:0.957]
Epoch [47/120    avg_loss:0.051, val_acc:0.945]
Epoch [48/120    avg_loss:0.072, val_acc:0.957]
Epoch [49/120    avg_loss:0.064, val_acc:0.956]
Epoch [50/120    avg_loss:0.053, val_acc:0.953]
Epoch [51/120    avg_loss:0.040, val_acc:0.956]
Epoch [52/120    avg_loss:0.056, val_acc:0.956]
Epoch [53/120    avg_loss:0.040, val_acc:0.959]
Epoch [54/120    avg_loss:0.060, val_acc:0.957]
Epoch [55/120    avg_loss:0.056, val_acc:0.956]
Epoch [56/120    avg_loss:0.053, val_acc:0.957]
Epoch [57/120    avg_loss:0.045, val_acc:0.953]
Epoch [58/120    avg_loss:0.034, val_acc:0.963]
Epoch [59/120    avg_loss:0.029, val_acc:0.966]
Epoch [60/120    avg_loss:0.028, val_acc:0.967]
Epoch [61/120    avg_loss:0.023, val_acc:0.967]
Epoch [62/120    avg_loss:0.025, val_acc:0.967]
Epoch [63/120    avg_loss:0.025, val_acc:0.965]
Epoch [64/120    avg_loss:0.019, val_acc:0.968]
Epoch [65/120    avg_loss:0.026, val_acc:0.969]
Epoch [66/120    avg_loss:0.026, val_acc:0.969]
Epoch [67/120    avg_loss:0.021, val_acc:0.969]
Epoch [68/120    avg_loss:0.020, val_acc:0.968]
Epoch [69/120    avg_loss:0.024, val_acc:0.967]
Epoch [70/120    avg_loss:0.023, val_acc:0.970]
Epoch [71/120    avg_loss:0.020, val_acc:0.970]
Epoch [72/120    avg_loss:0.021, val_acc:0.970]
Epoch [73/120    avg_loss:0.019, val_acc:0.969]
Epoch [74/120    avg_loss:0.017, val_acc:0.969]
Epoch [75/120    avg_loss:0.020, val_acc:0.969]
Epoch [76/120    avg_loss:0.018, val_acc:0.968]
Epoch [77/120    avg_loss:0.018, val_acc:0.968]
Epoch [78/120    avg_loss:0.020, val_acc:0.969]
Epoch [79/120    avg_loss:0.021, val_acc:0.969]
Epoch [80/120    avg_loss:0.022, val_acc:0.970]
Epoch [81/120    avg_loss:0.020, val_acc:0.969]
Epoch [82/120    avg_loss:0.018, val_acc:0.970]
Epoch [83/120    avg_loss:0.023, val_acc:0.968]
Epoch [84/120    avg_loss:0.018, val_acc:0.969]
Epoch [85/120    avg_loss:0.019, val_acc:0.973]
Epoch [86/120    avg_loss:0.017, val_acc:0.972]
Epoch [87/120    avg_loss:0.016, val_acc:0.970]
Epoch [88/120    avg_loss:0.018, val_acc:0.969]
Epoch [89/120    avg_loss:0.020, val_acc:0.972]
Epoch [90/120    avg_loss:0.023, val_acc:0.970]
Epoch [91/120    avg_loss:0.017, val_acc:0.970]
Epoch [92/120    avg_loss:0.016, val_acc:0.972]
Epoch [93/120    avg_loss:0.016, val_acc:0.969]
Epoch [94/120    avg_loss:0.017, val_acc:0.970]
Epoch [95/120    avg_loss:0.020, val_acc:0.974]
Epoch [96/120    avg_loss:0.022, val_acc:0.972]
Epoch [97/120    avg_loss:0.015, val_acc:0.972]
Epoch [98/120    avg_loss:0.017, val_acc:0.970]
Epoch [99/120    avg_loss:0.021, val_acc:0.973]
Epoch [100/120    avg_loss:0.022, val_acc:0.973]
Epoch [101/120    avg_loss:0.014, val_acc:0.972]
Epoch [102/120    avg_loss:0.017, val_acc:0.973]
Epoch [103/120    avg_loss:0.019, val_acc:0.972]
Epoch [104/120    avg_loss:0.013, val_acc:0.975]
Epoch [105/120    avg_loss:0.016, val_acc:0.974]
Epoch [106/120    avg_loss:0.017, val_acc:0.972]
Epoch [107/120    avg_loss:0.015, val_acc:0.972]
Epoch [108/120    avg_loss:0.019, val_acc:0.970]
Epoch [109/120    avg_loss:0.016, val_acc:0.969]
Epoch [110/120    avg_loss:0.018, val_acc:0.969]
Epoch [111/120    avg_loss:0.020, val_acc:0.972]
Epoch [112/120    avg_loss:0.016, val_acc:0.969]
Epoch [113/120    avg_loss:0.017, val_acc:0.973]
Epoch [114/120    avg_loss:0.019, val_acc:0.972]
Epoch [115/120    avg_loss:0.014, val_acc:0.973]
Epoch [116/120    avg_loss:0.017, val_acc:0.973]
Epoch [117/120    avg_loss:0.016, val_acc:0.972]
Epoch [118/120    avg_loss:0.015, val_acc:0.973]
Epoch [119/120    avg_loss:0.015, val_acc:0.972]
Epoch [120/120    avg_loss:0.015, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1247    3    0    0    6    0    0    0    8   17    4    0
     0    0    0]
 [   0    0    1  721    0   19    0    0    0    4    0    0    1    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   15   82    0    6    0    0    0    0  757    9    0    0
     1    5    0]
 [   0    0    3    0    0    1    9    0    0    0    3 2192    0    2
     0    0    0]
 [   0    0    0   24   11    1    0    0    0    2   11    0  479    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    1    0    0
  1136    0    0]
 [   0    0    0    0    0    1   13    0    0    1    0    0    0    0
   104  228    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.81571815718158

F1 scores:
[       nan 0.95238095 0.97727273 0.91439442 0.97482838 0.96651786
 0.97837435 1.         0.995338   0.79069767 0.91480363 0.98961625
 0.93921569 0.9919571  0.95422092 0.7862069  0.95953757]

Kappa:
0.9522559305767428
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2266f7a748>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.331, val_acc:0.539]
Epoch [2/120    avg_loss:1.825, val_acc:0.602]
Epoch [3/120    avg_loss:1.519, val_acc:0.641]
Epoch [4/120    avg_loss:1.218, val_acc:0.711]
Epoch [5/120    avg_loss:1.044, val_acc:0.735]
Epoch [6/120    avg_loss:0.885, val_acc:0.741]
Epoch [7/120    avg_loss:0.771, val_acc:0.807]
Epoch [8/120    avg_loss:0.712, val_acc:0.800]
Epoch [9/120    avg_loss:0.610, val_acc:0.786]
Epoch [10/120    avg_loss:0.524, val_acc:0.845]
Epoch [11/120    avg_loss:0.592, val_acc:0.828]
Epoch [12/120    avg_loss:0.612, val_acc:0.852]
Epoch [13/120    avg_loss:0.351, val_acc:0.886]
Epoch [14/120    avg_loss:0.369, val_acc:0.882]
Epoch [15/120    avg_loss:0.357, val_acc:0.873]
Epoch [16/120    avg_loss:0.426, val_acc:0.858]
Epoch [17/120    avg_loss:0.472, val_acc:0.797]
Epoch [18/120    avg_loss:0.371, val_acc:0.847]
Epoch [19/120    avg_loss:0.356, val_acc:0.852]
Epoch [20/120    avg_loss:0.284, val_acc:0.891]
Epoch [21/120    avg_loss:0.277, val_acc:0.889]
Epoch [22/120    avg_loss:0.279, val_acc:0.890]
Epoch [23/120    avg_loss:0.259, val_acc:0.900]
Epoch [24/120    avg_loss:0.227, val_acc:0.909]
Epoch [25/120    avg_loss:0.185, val_acc:0.922]
Epoch [26/120    avg_loss:0.220, val_acc:0.920]
Epoch [27/120    avg_loss:0.199, val_acc:0.914]
Epoch [28/120    avg_loss:0.200, val_acc:0.934]
Epoch [29/120    avg_loss:0.186, val_acc:0.935]
Epoch [30/120    avg_loss:0.227, val_acc:0.901]
Epoch [31/120    avg_loss:0.171, val_acc:0.930]
Epoch [32/120    avg_loss:0.161, val_acc:0.931]
Epoch [33/120    avg_loss:0.122, val_acc:0.942]
Epoch [34/120    avg_loss:0.104, val_acc:0.934]
Epoch [35/120    avg_loss:0.137, val_acc:0.916]
Epoch [36/120    avg_loss:0.185, val_acc:0.924]
Epoch [37/120    avg_loss:0.169, val_acc:0.935]
Epoch [38/120    avg_loss:0.111, val_acc:0.928]
Epoch [39/120    avg_loss:0.125, val_acc:0.938]
Epoch [40/120    avg_loss:0.118, val_acc:0.935]
Epoch [41/120    avg_loss:0.157, val_acc:0.932]
Epoch [42/120    avg_loss:0.127, val_acc:0.953]
Epoch [43/120    avg_loss:0.098, val_acc:0.951]
Epoch [44/120    avg_loss:0.080, val_acc:0.933]
Epoch [45/120    avg_loss:0.077, val_acc:0.952]
Epoch [46/120    avg_loss:0.070, val_acc:0.959]
Epoch [47/120    avg_loss:0.077, val_acc:0.955]
Epoch [48/120    avg_loss:0.113, val_acc:0.956]
Epoch [49/120    avg_loss:0.075, val_acc:0.966]
Epoch [50/120    avg_loss:0.073, val_acc:0.948]
Epoch [51/120    avg_loss:0.084, val_acc:0.957]
Epoch [52/120    avg_loss:0.071, val_acc:0.930]
Epoch [53/120    avg_loss:0.076, val_acc:0.959]
Epoch [54/120    avg_loss:0.075, val_acc:0.957]
Epoch [55/120    avg_loss:0.055, val_acc:0.959]
Epoch [56/120    avg_loss:0.074, val_acc:0.964]
Epoch [57/120    avg_loss:0.038, val_acc:0.967]
Epoch [58/120    avg_loss:0.037, val_acc:0.968]
Epoch [59/120    avg_loss:0.041, val_acc:0.967]
Epoch [60/120    avg_loss:0.090, val_acc:0.928]
Epoch [61/120    avg_loss:0.070, val_acc:0.951]
Epoch [62/120    avg_loss:0.084, val_acc:0.956]
Epoch [63/120    avg_loss:0.074, val_acc:0.960]
Epoch [64/120    avg_loss:0.070, val_acc:0.959]
Epoch [65/120    avg_loss:0.046, val_acc:0.978]
Epoch [66/120    avg_loss:0.071, val_acc:0.965]
Epoch [67/120    avg_loss:0.050, val_acc:0.965]
Epoch [68/120    avg_loss:0.072, val_acc:0.973]
Epoch [69/120    avg_loss:0.058, val_acc:0.935]
Epoch [70/120    avg_loss:0.055, val_acc:0.961]
Epoch [71/120    avg_loss:0.046, val_acc:0.968]
Epoch [72/120    avg_loss:0.051, val_acc:0.964]
Epoch [73/120    avg_loss:0.048, val_acc:0.968]
Epoch [74/120    avg_loss:0.051, val_acc:0.958]
Epoch [75/120    avg_loss:0.055, val_acc:0.973]
Epoch [76/120    avg_loss:0.041, val_acc:0.969]
Epoch [77/120    avg_loss:0.038, val_acc:0.977]
Epoch [78/120    avg_loss:0.049, val_acc:0.959]
Epoch [79/120    avg_loss:0.042, val_acc:0.968]
Epoch [80/120    avg_loss:0.030, val_acc:0.969]
Epoch [81/120    avg_loss:0.023, val_acc:0.975]
Epoch [82/120    avg_loss:0.018, val_acc:0.976]
Epoch [83/120    avg_loss:0.020, val_acc:0.975]
Epoch [84/120    avg_loss:0.028, val_acc:0.975]
Epoch [85/120    avg_loss:0.016, val_acc:0.977]
Epoch [86/120    avg_loss:0.021, val_acc:0.977]
Epoch [87/120    avg_loss:0.017, val_acc:0.976]
Epoch [88/120    avg_loss:0.017, val_acc:0.976]
Epoch [89/120    avg_loss:0.018, val_acc:0.980]
Epoch [90/120    avg_loss:0.017, val_acc:0.981]
Epoch [91/120    avg_loss:0.017, val_acc:0.980]
Epoch [92/120    avg_loss:0.016, val_acc:0.977]
Epoch [93/120    avg_loss:0.015, val_acc:0.981]
Epoch [94/120    avg_loss:0.018, val_acc:0.981]
Epoch [95/120    avg_loss:0.015, val_acc:0.982]
Epoch [96/120    avg_loss:0.017, val_acc:0.982]
Epoch [97/120    avg_loss:0.018, val_acc:0.984]
Epoch [98/120    avg_loss:0.016, val_acc:0.984]
Epoch [99/120    avg_loss:0.017, val_acc:0.985]
Epoch [100/120    avg_loss:0.018, val_acc:0.985]
Epoch [101/120    avg_loss:0.016, val_acc:0.984]
Epoch [102/120    avg_loss:0.017, val_acc:0.983]
Epoch [103/120    avg_loss:0.018, val_acc:0.983]
Epoch [104/120    avg_loss:0.016, val_acc:0.984]
Epoch [105/120    avg_loss:0.014, val_acc:0.982]
Epoch [106/120    avg_loss:0.014, val_acc:0.985]
Epoch [107/120    avg_loss:0.016, val_acc:0.984]
Epoch [108/120    avg_loss:0.016, val_acc:0.985]
Epoch [109/120    avg_loss:0.015, val_acc:0.985]
Epoch [110/120    avg_loss:0.012, val_acc:0.984]
Epoch [111/120    avg_loss:0.015, val_acc:0.983]
Epoch [112/120    avg_loss:0.013, val_acc:0.985]
Epoch [113/120    avg_loss:0.013, val_acc:0.986]
Epoch [114/120    avg_loss:0.018, val_acc:0.983]
Epoch [115/120    avg_loss:0.011, val_acc:0.983]
Epoch [116/120    avg_loss:0.014, val_acc:0.984]
Epoch [117/120    avg_loss:0.014, val_acc:0.986]
Epoch [118/120    avg_loss:0.014, val_acc:0.988]
Epoch [119/120    avg_loss:0.016, val_acc:0.985]
Epoch [120/120    avg_loss:0.011, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    2 1258    0    0    2    2    0    0    1    7   12    1    0
     0    0    0]
 [   0    0    1  712    0   16    0    0    0    5    0    9    1    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   15    0    0    1    0
     0    0    0]
 [   0    0    9   73    0    3    3    0    0    0  768   13    0    0
     0    6    0]
 [   0    0    4    0    0    0    5    0    0    0    7 2192    0    2
     0    0    0]
 [   0    0    0   12    2    4    0    0    0    0   10   16  486    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
   109  226    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.94579945799458

F1 scores:
[       nan 0.91764706 0.98358092 0.92108668 0.9953271  0.9674523
 0.98277154 0.98039216 0.995338   0.75       0.91921005 0.98450483
 0.94921875 0.98666667 0.95178197 0.7806563  0.97076023]

Kappa:
0.9537104679426728
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe8e5443748>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.327, val_acc:0.541]
Epoch [2/120    avg_loss:1.730, val_acc:0.593]
Epoch [3/120    avg_loss:1.485, val_acc:0.662]
Epoch [4/120    avg_loss:1.203, val_acc:0.743]
Epoch [5/120    avg_loss:1.126, val_acc:0.696]
Epoch [6/120    avg_loss:1.035, val_acc:0.718]
Epoch [7/120    avg_loss:0.867, val_acc:0.729]
Epoch [8/120    avg_loss:0.767, val_acc:0.763]
Epoch [9/120    avg_loss:0.705, val_acc:0.783]
Epoch [10/120    avg_loss:0.601, val_acc:0.703]
Epoch [11/120    avg_loss:0.580, val_acc:0.807]
Epoch [12/120    avg_loss:0.524, val_acc:0.851]
Epoch [13/120    avg_loss:0.377, val_acc:0.833]
Epoch [14/120    avg_loss:0.339, val_acc:0.848]
Epoch [15/120    avg_loss:0.323, val_acc:0.867]
Epoch [16/120    avg_loss:0.313, val_acc:0.868]
Epoch [17/120    avg_loss:0.302, val_acc:0.878]
Epoch [18/120    avg_loss:0.318, val_acc:0.871]
Epoch [19/120    avg_loss:0.232, val_acc:0.886]
Epoch [20/120    avg_loss:0.273, val_acc:0.876]
Epoch [21/120    avg_loss:0.235, val_acc:0.904]
Epoch [22/120    avg_loss:0.318, val_acc:0.897]
Epoch [23/120    avg_loss:0.284, val_acc:0.856]
Epoch [24/120    avg_loss:0.250, val_acc:0.911]
Epoch [25/120    avg_loss:0.235, val_acc:0.908]
Epoch [26/120    avg_loss:0.147, val_acc:0.905]
Epoch [27/120    avg_loss:0.213, val_acc:0.907]
Epoch [28/120    avg_loss:0.238, val_acc:0.909]
Epoch [29/120    avg_loss:0.153, val_acc:0.923]
Epoch [30/120    avg_loss:0.176, val_acc:0.902]
Epoch [31/120    avg_loss:0.131, val_acc:0.918]
Epoch [32/120    avg_loss:0.144, val_acc:0.928]
Epoch [33/120    avg_loss:0.150, val_acc:0.930]
Epoch [34/120    avg_loss:0.164, val_acc:0.930]
Epoch [35/120    avg_loss:0.147, val_acc:0.914]
Epoch [36/120    avg_loss:0.134, val_acc:0.930]
Epoch [37/120    avg_loss:0.119, val_acc:0.919]
Epoch [38/120    avg_loss:0.127, val_acc:0.918]
Epoch [39/120    avg_loss:0.100, val_acc:0.933]
Epoch [40/120    avg_loss:0.087, val_acc:0.943]
Epoch [41/120    avg_loss:0.132, val_acc:0.927]
Epoch [42/120    avg_loss:0.089, val_acc:0.942]
Epoch [43/120    avg_loss:0.077, val_acc:0.936]
Epoch [44/120    avg_loss:0.065, val_acc:0.952]
Epoch [45/120    avg_loss:0.109, val_acc:0.946]
Epoch [46/120    avg_loss:0.080, val_acc:0.935]
Epoch [47/120    avg_loss:0.130, val_acc:0.924]
Epoch [48/120    avg_loss:0.085, val_acc:0.946]
Epoch [49/120    avg_loss:0.083, val_acc:0.938]
Epoch [50/120    avg_loss:0.151, val_acc:0.935]
Epoch [51/120    avg_loss:0.091, val_acc:0.948]
Epoch [52/120    avg_loss:0.088, val_acc:0.925]
Epoch [53/120    avg_loss:0.563, val_acc:0.711]
Epoch [54/120    avg_loss:0.472, val_acc:0.837]
Epoch [55/120    avg_loss:0.197, val_acc:0.908]
Epoch [56/120    avg_loss:0.134, val_acc:0.910]
Epoch [57/120    avg_loss:0.128, val_acc:0.921]
Epoch [58/120    avg_loss:0.079, val_acc:0.940]
Epoch [59/120    avg_loss:0.060, val_acc:0.944]
Epoch [60/120    avg_loss:0.062, val_acc:0.951]
Epoch [61/120    avg_loss:0.055, val_acc:0.952]
Epoch [62/120    avg_loss:0.052, val_acc:0.955]
Epoch [63/120    avg_loss:0.052, val_acc:0.951]
Epoch [64/120    avg_loss:0.056, val_acc:0.955]
Epoch [65/120    avg_loss:0.044, val_acc:0.951]
Epoch [66/120    avg_loss:0.056, val_acc:0.955]
Epoch [67/120    avg_loss:0.056, val_acc:0.953]
Epoch [68/120    avg_loss:0.046, val_acc:0.952]
Epoch [69/120    avg_loss:0.047, val_acc:0.954]
Epoch [70/120    avg_loss:0.044, val_acc:0.956]
Epoch [71/120    avg_loss:0.041, val_acc:0.958]
Epoch [72/120    avg_loss:0.045, val_acc:0.955]
Epoch [73/120    avg_loss:0.043, val_acc:0.954]
Epoch [74/120    avg_loss:0.046, val_acc:0.958]
Epoch [75/120    avg_loss:0.042, val_acc:0.959]
Epoch [76/120    avg_loss:0.038, val_acc:0.957]
Epoch [77/120    avg_loss:0.040, val_acc:0.961]
Epoch [78/120    avg_loss:0.037, val_acc:0.962]
Epoch [79/120    avg_loss:0.044, val_acc:0.959]
Epoch [80/120    avg_loss:0.039, val_acc:0.961]
Epoch [81/120    avg_loss:0.037, val_acc:0.963]
Epoch [82/120    avg_loss:0.041, val_acc:0.959]
Epoch [83/120    avg_loss:0.033, val_acc:0.963]
Epoch [84/120    avg_loss:0.036, val_acc:0.963]
Epoch [85/120    avg_loss:0.034, val_acc:0.963]
Epoch [86/120    avg_loss:0.037, val_acc:0.962]
Epoch [87/120    avg_loss:0.039, val_acc:0.963]
Epoch [88/120    avg_loss:0.037, val_acc:0.963]
Epoch [89/120    avg_loss:0.033, val_acc:0.962]
Epoch [90/120    avg_loss:0.029, val_acc:0.966]
Epoch [91/120    avg_loss:0.036, val_acc:0.965]
Epoch [92/120    avg_loss:0.036, val_acc:0.965]
Epoch [93/120    avg_loss:0.035, val_acc:0.966]
Epoch [94/120    avg_loss:0.030, val_acc:0.966]
Epoch [95/120    avg_loss:0.029, val_acc:0.966]
Epoch [96/120    avg_loss:0.034, val_acc:0.964]
Epoch [97/120    avg_loss:0.029, val_acc:0.965]
Epoch [98/120    avg_loss:0.035, val_acc:0.966]
Epoch [99/120    avg_loss:0.028, val_acc:0.967]
Epoch [100/120    avg_loss:0.030, val_acc:0.965]
Epoch [101/120    avg_loss:0.028, val_acc:0.967]
Epoch [102/120    avg_loss:0.028, val_acc:0.967]
Epoch [103/120    avg_loss:0.027, val_acc:0.967]
Epoch [104/120    avg_loss:0.028, val_acc:0.969]
Epoch [105/120    avg_loss:0.030, val_acc:0.970]
Epoch [106/120    avg_loss:0.029, val_acc:0.970]
Epoch [107/120    avg_loss:0.028, val_acc:0.969]
Epoch [108/120    avg_loss:0.028, val_acc:0.969]
Epoch [109/120    avg_loss:0.025, val_acc:0.970]
Epoch [110/120    avg_loss:0.023, val_acc:0.969]
Epoch [111/120    avg_loss:0.027, val_acc:0.970]
Epoch [112/120    avg_loss:0.025, val_acc:0.970]
Epoch [113/120    avg_loss:0.027, val_acc:0.974]
Epoch [114/120    avg_loss:0.027, val_acc:0.974]
Epoch [115/120    avg_loss:0.023, val_acc:0.973]
Epoch [116/120    avg_loss:0.022, val_acc:0.970]
Epoch [117/120    avg_loss:0.025, val_acc:0.970]
Epoch [118/120    avg_loss:0.028, val_acc:0.968]
Epoch [119/120    avg_loss:0.029, val_acc:0.971]
Epoch [120/120    avg_loss:0.026, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1258    0    0    0    0    0    0    1    5    8    7    0
     0    6    0]
 [   0    0    1  728    0    7    0    0    0    6    0    0    2    2
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    6    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0   36   84    0    4    0    0    0    0  736    9    0    0
     0    6    0]
 [   0    0    8    0    0    3    8    0    0    0    8 2144   36    3
     0    0    0]
 [   0    0    0    5    6    3    0    0    0    0   16    1  495    0
     1    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
   112  224    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.2520325203252

F1 scores:
[       nan 0.98765432 0.97217929 0.93094629 0.98611111 0.96921323
 0.98116051 0.89285714 0.99883856 0.68181818 0.89592209 0.98056254
 0.91751622 0.98666667 0.94903926 0.76843911 0.94797688]

Kappa:
0.9458744446254933
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f46d06fd7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.315, val_acc:0.490]
Epoch [2/120    avg_loss:1.817, val_acc:0.608]
Epoch [3/120    avg_loss:1.417, val_acc:0.645]
Epoch [4/120    avg_loss:1.158, val_acc:0.685]
Epoch [5/120    avg_loss:0.988, val_acc:0.765]
Epoch [6/120    avg_loss:0.809, val_acc:0.804]
Epoch [7/120    avg_loss:0.717, val_acc:0.802]
Epoch [8/120    avg_loss:0.628, val_acc:0.769]
Epoch [9/120    avg_loss:0.601, val_acc:0.847]
Epoch [10/120    avg_loss:0.481, val_acc:0.820]
Epoch [11/120    avg_loss:0.511, val_acc:0.846]
Epoch [12/120    avg_loss:0.407, val_acc:0.864]
Epoch [13/120    avg_loss:0.464, val_acc:0.868]
Epoch [14/120    avg_loss:0.362, val_acc:0.854]
Epoch [15/120    avg_loss:0.332, val_acc:0.862]
Epoch [16/120    avg_loss:0.390, val_acc:0.850]
Epoch [17/120    avg_loss:0.348, val_acc:0.858]
Epoch [18/120    avg_loss:0.356, val_acc:0.907]
Epoch [19/120    avg_loss:0.290, val_acc:0.911]
Epoch [20/120    avg_loss:0.262, val_acc:0.893]
Epoch [21/120    avg_loss:0.237, val_acc:0.884]
Epoch [22/120    avg_loss:0.236, val_acc:0.910]
Epoch [23/120    avg_loss:0.206, val_acc:0.911]
Epoch [24/120    avg_loss:0.195, val_acc:0.919]
Epoch [25/120    avg_loss:0.200, val_acc:0.910]
Epoch [26/120    avg_loss:0.215, val_acc:0.846]
Epoch [27/120    avg_loss:0.288, val_acc:0.910]
Epoch [28/120    avg_loss:0.193, val_acc:0.914]
Epoch [29/120    avg_loss:0.141, val_acc:0.902]
Epoch [30/120    avg_loss:0.144, val_acc:0.939]
Epoch [31/120    avg_loss:0.151, val_acc:0.932]
Epoch [32/120    avg_loss:0.236, val_acc:0.884]
Epoch [33/120    avg_loss:0.142, val_acc:0.936]
Epoch [34/120    avg_loss:0.143, val_acc:0.916]
Epoch [35/120    avg_loss:0.133, val_acc:0.934]
Epoch [36/120    avg_loss:0.138, val_acc:0.944]
Epoch [37/120    avg_loss:0.090, val_acc:0.945]
Epoch [38/120    avg_loss:0.129, val_acc:0.927]
Epoch [39/120    avg_loss:0.095, val_acc:0.943]
Epoch [40/120    avg_loss:0.123, val_acc:0.941]
Epoch [41/120    avg_loss:0.101, val_acc:0.943]
Epoch [42/120    avg_loss:0.073, val_acc:0.960]
Epoch [43/120    avg_loss:0.073, val_acc:0.924]
Epoch [44/120    avg_loss:0.083, val_acc:0.949]
Epoch [45/120    avg_loss:0.062, val_acc:0.954]
Epoch [46/120    avg_loss:0.048, val_acc:0.958]
Epoch [47/120    avg_loss:0.070, val_acc:0.961]
Epoch [48/120    avg_loss:0.075, val_acc:0.956]
Epoch [49/120    avg_loss:0.064, val_acc:0.975]
Epoch [50/120    avg_loss:0.069, val_acc:0.941]
Epoch [51/120    avg_loss:0.076, val_acc:0.970]
Epoch [52/120    avg_loss:0.064, val_acc:0.962]
Epoch [53/120    avg_loss:0.056, val_acc:0.968]
Epoch [54/120    avg_loss:0.065, val_acc:0.965]
Epoch [55/120    avg_loss:0.050, val_acc:0.968]
Epoch [56/120    avg_loss:0.047, val_acc:0.965]
Epoch [57/120    avg_loss:0.054, val_acc:0.965]
Epoch [58/120    avg_loss:0.037, val_acc:0.971]
Epoch [59/120    avg_loss:0.040, val_acc:0.970]
Epoch [60/120    avg_loss:0.082, val_acc:0.951]
Epoch [61/120    avg_loss:0.108, val_acc:0.972]
Epoch [62/120    avg_loss:0.095, val_acc:0.898]
Epoch [63/120    avg_loss:0.188, val_acc:0.950]
Epoch [64/120    avg_loss:0.101, val_acc:0.964]
Epoch [65/120    avg_loss:0.054, val_acc:0.964]
Epoch [66/120    avg_loss:0.052, val_acc:0.972]
Epoch [67/120    avg_loss:0.044, val_acc:0.973]
Epoch [68/120    avg_loss:0.041, val_acc:0.978]
Epoch [69/120    avg_loss:0.040, val_acc:0.973]
Epoch [70/120    avg_loss:0.041, val_acc:0.972]
Epoch [71/120    avg_loss:0.034, val_acc:0.975]
Epoch [72/120    avg_loss:0.031, val_acc:0.979]
Epoch [73/120    avg_loss:0.030, val_acc:0.979]
Epoch [74/120    avg_loss:0.027, val_acc:0.979]
Epoch [75/120    avg_loss:0.025, val_acc:0.977]
Epoch [76/120    avg_loss:0.027, val_acc:0.978]
Epoch [77/120    avg_loss:0.025, val_acc:0.977]
Epoch [78/120    avg_loss:0.028, val_acc:0.978]
Epoch [79/120    avg_loss:0.025, val_acc:0.977]
Epoch [80/120    avg_loss:0.030, val_acc:0.980]
Epoch [81/120    avg_loss:0.032, val_acc:0.980]
Epoch [82/120    avg_loss:0.028, val_acc:0.978]
Epoch [83/120    avg_loss:0.018, val_acc:0.978]
Epoch [84/120    avg_loss:0.025, val_acc:0.978]
Epoch [85/120    avg_loss:0.024, val_acc:0.978]
Epoch [86/120    avg_loss:0.021, val_acc:0.980]
Epoch [87/120    avg_loss:0.031, val_acc:0.980]
Epoch [88/120    avg_loss:0.025, val_acc:0.980]
Epoch [89/120    avg_loss:0.025, val_acc:0.981]
Epoch [90/120    avg_loss:0.026, val_acc:0.982]
Epoch [91/120    avg_loss:0.022, val_acc:0.982]
Epoch [92/120    avg_loss:0.022, val_acc:0.980]
Epoch [93/120    avg_loss:0.020, val_acc:0.983]
Epoch [94/120    avg_loss:0.024, val_acc:0.983]
Epoch [95/120    avg_loss:0.018, val_acc:0.983]
Epoch [96/120    avg_loss:0.026, val_acc:0.982]
Epoch [97/120    avg_loss:0.020, val_acc:0.982]
Epoch [98/120    avg_loss:0.019, val_acc:0.982]
Epoch [99/120    avg_loss:0.023, val_acc:0.982]
Epoch [100/120    avg_loss:0.022, val_acc:0.982]
Epoch [101/120    avg_loss:0.020, val_acc:0.983]
Epoch [102/120    avg_loss:0.018, val_acc:0.982]
Epoch [103/120    avg_loss:0.022, val_acc:0.979]
Epoch [104/120    avg_loss:0.018, val_acc:0.979]
Epoch [105/120    avg_loss:0.019, val_acc:0.981]
Epoch [106/120    avg_loss:0.019, val_acc:0.980]
Epoch [107/120    avg_loss:0.019, val_acc:0.980]
Epoch [108/120    avg_loss:0.021, val_acc:0.981]
Epoch [109/120    avg_loss:0.019, val_acc:0.983]
Epoch [110/120    avg_loss:0.019, val_acc:0.983]
Epoch [111/120    avg_loss:0.019, val_acc:0.982]
Epoch [112/120    avg_loss:0.021, val_acc:0.984]
Epoch [113/120    avg_loss:0.017, val_acc:0.985]
Epoch [114/120    avg_loss:0.018, val_acc:0.985]
Epoch [115/120    avg_loss:0.019, val_acc:0.983]
Epoch [116/120    avg_loss:0.020, val_acc:0.982]
Epoch [117/120    avg_loss:0.017, val_acc:0.983]
Epoch [118/120    avg_loss:0.020, val_acc:0.982]
Epoch [119/120    avg_loss:0.018, val_acc:0.983]
Epoch [120/120    avg_loss:0.020, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    1 1257    0    0    0    0    0    0    1   15    7    0    0
     0    4    0]
 [   0    0    0  729    1   10    0    0    0    4    0    0    1    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    5    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   14    0    0    0    0    0    0  416    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   15   84    0   13    0    0    0    0  757    3    0    0
     0    3    0]
 [   0    0   10    0    0    5   14    0    0    0    8 2171    0    2
     0    0    0]
 [   0    0    0   15    0   10    0    0    0    0    5    8  494    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    1    0    3    0    0    0
  1128    5    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    81  255    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.0

F1 scores:
[       nan 0.83333333 0.97935333 0.92571429 0.99765808 0.94911504
 0.98056801 0.90909091 0.98229044 0.85714286 0.90985577 0.98681818
 0.9592233  0.98930481 0.96081772 0.83061889 0.98224852]

Kappa:
0.9543989999790216
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0158270748>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.313, val_acc:0.568]
Epoch [2/120    avg_loss:1.855, val_acc:0.550]
Epoch [3/120    avg_loss:1.581, val_acc:0.672]
Epoch [4/120    avg_loss:1.311, val_acc:0.721]
Epoch [5/120    avg_loss:1.043, val_acc:0.711]
Epoch [6/120    avg_loss:0.911, val_acc:0.693]
Epoch [7/120    avg_loss:0.805, val_acc:0.729]
Epoch [8/120    avg_loss:0.766, val_acc:0.756]
Epoch [9/120    avg_loss:0.699, val_acc:0.782]
Epoch [10/120    avg_loss:0.630, val_acc:0.781]
Epoch [11/120    avg_loss:0.577, val_acc:0.816]
Epoch [12/120    avg_loss:0.463, val_acc:0.834]
Epoch [13/120    avg_loss:0.469, val_acc:0.864]
Epoch [14/120    avg_loss:0.393, val_acc:0.867]
Epoch [15/120    avg_loss:0.496, val_acc:0.838]
Epoch [16/120    avg_loss:0.329, val_acc:0.881]
Epoch [17/120    avg_loss:0.281, val_acc:0.885]
Epoch [18/120    avg_loss:0.382, val_acc:0.882]
Epoch [19/120    avg_loss:0.360, val_acc:0.862]
Epoch [20/120    avg_loss:0.266, val_acc:0.910]
Epoch [21/120    avg_loss:0.203, val_acc:0.925]
Epoch [22/120    avg_loss:0.249, val_acc:0.906]
Epoch [23/120    avg_loss:0.303, val_acc:0.901]
Epoch [24/120    avg_loss:0.241, val_acc:0.913]
Epoch [25/120    avg_loss:0.271, val_acc:0.892]
Epoch [26/120    avg_loss:0.215, val_acc:0.925]
Epoch [27/120    avg_loss:0.313, val_acc:0.927]
Epoch [28/120    avg_loss:0.176, val_acc:0.932]
Epoch [29/120    avg_loss:0.156, val_acc:0.926]
Epoch [30/120    avg_loss:0.176, val_acc:0.929]
Epoch [31/120    avg_loss:0.150, val_acc:0.923]
Epoch [32/120    avg_loss:0.115, val_acc:0.936]
Epoch [33/120    avg_loss:0.131, val_acc:0.927]
Epoch [34/120    avg_loss:0.155, val_acc:0.926]
Epoch [35/120    avg_loss:0.127, val_acc:0.930]
Epoch [36/120    avg_loss:0.122, val_acc:0.951]
Epoch [37/120    avg_loss:0.096, val_acc:0.955]
Epoch [38/120    avg_loss:0.111, val_acc:0.958]
Epoch [39/120    avg_loss:0.082, val_acc:0.953]
Epoch [40/120    avg_loss:0.078, val_acc:0.961]
Epoch [41/120    avg_loss:0.097, val_acc:0.952]
Epoch [42/120    avg_loss:0.062, val_acc:0.949]
Epoch [43/120    avg_loss:0.093, val_acc:0.952]
Epoch [44/120    avg_loss:0.080, val_acc:0.953]
Epoch [45/120    avg_loss:0.103, val_acc:0.951]
Epoch [46/120    avg_loss:0.112, val_acc:0.953]
Epoch [47/120    avg_loss:0.082, val_acc:0.948]
Epoch [48/120    avg_loss:0.062, val_acc:0.954]
Epoch [49/120    avg_loss:0.047, val_acc:0.968]
Epoch [50/120    avg_loss:0.045, val_acc:0.952]
Epoch [51/120    avg_loss:0.046, val_acc:0.965]
Epoch [52/120    avg_loss:0.059, val_acc:0.939]
Epoch [53/120    avg_loss:0.087, val_acc:0.949]
Epoch [54/120    avg_loss:0.063, val_acc:0.952]
Epoch [55/120    avg_loss:0.077, val_acc:0.967]
Epoch [56/120    avg_loss:0.045, val_acc:0.970]
Epoch [57/120    avg_loss:0.048, val_acc:0.956]
Epoch [58/120    avg_loss:0.042, val_acc:0.964]
Epoch [59/120    avg_loss:0.036, val_acc:0.975]
Epoch [60/120    avg_loss:0.031, val_acc:0.967]
Epoch [61/120    avg_loss:0.044, val_acc:0.965]
Epoch [62/120    avg_loss:0.034, val_acc:0.975]
Epoch [63/120    avg_loss:0.049, val_acc:0.961]
Epoch [64/120    avg_loss:0.027, val_acc:0.971]
Epoch [65/120    avg_loss:0.035, val_acc:0.973]
Epoch [66/120    avg_loss:0.035, val_acc:0.962]
Epoch [67/120    avg_loss:0.048, val_acc:0.954]
Epoch [68/120    avg_loss:0.023, val_acc:0.975]
Epoch [69/120    avg_loss:0.021, val_acc:0.961]
Epoch [70/120    avg_loss:0.027, val_acc:0.971]
Epoch [71/120    avg_loss:0.031, val_acc:0.967]
Epoch [72/120    avg_loss:0.038, val_acc:0.972]
Epoch [73/120    avg_loss:0.028, val_acc:0.978]
Epoch [74/120    avg_loss:0.023, val_acc:0.975]
Epoch [75/120    avg_loss:0.015, val_acc:0.972]
Epoch [76/120    avg_loss:0.016, val_acc:0.971]
Epoch [77/120    avg_loss:0.029, val_acc:0.967]
Epoch [78/120    avg_loss:0.026, val_acc:0.962]
Epoch [79/120    avg_loss:0.035, val_acc:0.970]
Epoch [80/120    avg_loss:0.029, val_acc:0.979]
Epoch [81/120    avg_loss:0.021, val_acc:0.969]
Epoch [82/120    avg_loss:0.015, val_acc:0.970]
Epoch [83/120    avg_loss:0.011, val_acc:0.985]
Epoch [84/120    avg_loss:0.013, val_acc:0.974]
Epoch [85/120    avg_loss:0.015, val_acc:0.974]
Epoch [86/120    avg_loss:0.013, val_acc:0.981]
Epoch [87/120    avg_loss:0.012, val_acc:0.980]
Epoch [88/120    avg_loss:0.012, val_acc:0.987]
Epoch [89/120    avg_loss:0.011, val_acc:0.981]
Epoch [90/120    avg_loss:0.012, val_acc:0.978]
Epoch [91/120    avg_loss:0.014, val_acc:0.979]
Epoch [92/120    avg_loss:0.025, val_acc:0.977]
Epoch [93/120    avg_loss:0.011, val_acc:0.977]
Epoch [94/120    avg_loss:0.012, val_acc:0.981]
Epoch [95/120    avg_loss:0.016, val_acc:0.970]
Epoch [96/120    avg_loss:0.018, val_acc:0.983]
Epoch [97/120    avg_loss:0.019, val_acc:0.981]
Epoch [98/120    avg_loss:0.044, val_acc:0.954]
Epoch [99/120    avg_loss:0.112, val_acc:0.917]
Epoch [100/120    avg_loss:0.264, val_acc:0.924]
Epoch [101/120    avg_loss:0.206, val_acc:0.954]
Epoch [102/120    avg_loss:0.077, val_acc:0.968]
Epoch [103/120    avg_loss:0.046, val_acc:0.968]
Epoch [104/120    avg_loss:0.049, val_acc:0.970]
Epoch [105/120    avg_loss:0.046, val_acc:0.969]
Epoch [106/120    avg_loss:0.042, val_acc:0.971]
Epoch [107/120    avg_loss:0.043, val_acc:0.974]
Epoch [108/120    avg_loss:0.031, val_acc:0.973]
Epoch [109/120    avg_loss:0.033, val_acc:0.969]
Epoch [110/120    avg_loss:0.035, val_acc:0.972]
Epoch [111/120    avg_loss:0.026, val_acc:0.971]
Epoch [112/120    avg_loss:0.029, val_acc:0.973]
Epoch [113/120    avg_loss:0.025, val_acc:0.973]
Epoch [114/120    avg_loss:0.033, val_acc:0.974]
Epoch [115/120    avg_loss:0.025, val_acc:0.974]
Epoch [116/120    avg_loss:0.029, val_acc:0.974]
Epoch [117/120    avg_loss:0.028, val_acc:0.974]
Epoch [118/120    avg_loss:0.026, val_acc:0.974]
Epoch [119/120    avg_loss:0.021, val_acc:0.974]
Epoch [120/120    avg_loss:0.021, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    5 1254    9    0    0    0    0    0    0    7   10    0    0
     0    0    0]
 [   0    0    1  723    0   15    0    0    0    4    0    0    0    4
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    1    0    2    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   14   84    0    7    0    0    0    1  755   12    0    0
     0    2    0]
 [   0    0   13    0    0    0    6    0    0    0    1 2186    1    3
     0    0    0]
 [   0    0    0   23    1   10    0    0    0    0    9   14  468    0
     0    0    9]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    1    0   11    0    0    3    0    3    0    0    0
  1121    0    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    7
   111  224    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.3821138211382

F1 scores:
[       nan 0.9047619  0.97701597 0.90943396 0.99530516 0.94143646
 0.98940998 0.98039216 0.99652375 0.75       0.91349062 0.98557259
 0.9332004  0.96083551 0.94320572 0.78184991 0.94915254]

Kappa:
0.9472957595513419
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2fd53b7710>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.312, val_acc:0.467]
Epoch [2/120    avg_loss:1.754, val_acc:0.602]
Epoch [3/120    avg_loss:1.417, val_acc:0.647]
Epoch [4/120    avg_loss:1.126, val_acc:0.713]
Epoch [5/120    avg_loss:0.989, val_acc:0.720]
Epoch [6/120    avg_loss:0.829, val_acc:0.767]
Epoch [7/120    avg_loss:0.711, val_acc:0.777]
Epoch [8/120    avg_loss:0.654, val_acc:0.771]
Epoch [9/120    avg_loss:0.572, val_acc:0.782]
Epoch [10/120    avg_loss:0.546, val_acc:0.835]
Epoch [11/120    avg_loss:0.527, val_acc:0.828]
Epoch [12/120    avg_loss:0.518, val_acc:0.845]
Epoch [13/120    avg_loss:0.456, val_acc:0.848]
Epoch [14/120    avg_loss:0.348, val_acc:0.847]
Epoch [15/120    avg_loss:0.403, val_acc:0.852]
Epoch [16/120    avg_loss:0.360, val_acc:0.863]
Epoch [17/120    avg_loss:0.274, val_acc:0.877]
Epoch [18/120    avg_loss:0.253, val_acc:0.881]
Epoch [19/120    avg_loss:0.255, val_acc:0.898]
Epoch [20/120    avg_loss:0.329, val_acc:0.871]
Epoch [21/120    avg_loss:0.296, val_acc:0.902]
Epoch [22/120    avg_loss:0.275, val_acc:0.886]
Epoch [23/120    avg_loss:0.223, val_acc:0.865]
Epoch [24/120    avg_loss:0.222, val_acc:0.920]
Epoch [25/120    avg_loss:0.176, val_acc:0.887]
Epoch [26/120    avg_loss:0.224, val_acc:0.868]
Epoch [27/120    avg_loss:0.160, val_acc:0.910]
Epoch [28/120    avg_loss:0.149, val_acc:0.924]
Epoch [29/120    avg_loss:0.117, val_acc:0.939]
Epoch [30/120    avg_loss:0.175, val_acc:0.914]
Epoch [31/120    avg_loss:0.161, val_acc:0.926]
Epoch [32/120    avg_loss:0.117, val_acc:0.927]
Epoch [33/120    avg_loss:0.121, val_acc:0.945]
Epoch [34/120    avg_loss:0.110, val_acc:0.939]
Epoch [35/120    avg_loss:0.173, val_acc:0.933]
Epoch [36/120    avg_loss:0.116, val_acc:0.930]
Epoch [37/120    avg_loss:0.104, val_acc:0.933]
Epoch [38/120    avg_loss:0.118, val_acc:0.919]
Epoch [39/120    avg_loss:0.101, val_acc:0.931]
Epoch [40/120    avg_loss:0.103, val_acc:0.948]
Epoch [41/120    avg_loss:0.079, val_acc:0.949]
Epoch [42/120    avg_loss:0.111, val_acc:0.945]
Epoch [43/120    avg_loss:0.088, val_acc:0.930]
Epoch [44/120    avg_loss:0.099, val_acc:0.929]
Epoch [45/120    avg_loss:0.133, val_acc:0.948]
Epoch [46/120    avg_loss:0.087, val_acc:0.948]
Epoch [47/120    avg_loss:0.065, val_acc:0.950]
Epoch [48/120    avg_loss:0.055, val_acc:0.953]
Epoch [49/120    avg_loss:0.063, val_acc:0.952]
Epoch [50/120    avg_loss:0.085, val_acc:0.952]
Epoch [51/120    avg_loss:0.046, val_acc:0.953]
Epoch [52/120    avg_loss:0.049, val_acc:0.956]
Epoch [53/120    avg_loss:0.061, val_acc:0.949]
Epoch [54/120    avg_loss:0.070, val_acc:0.953]
Epoch [55/120    avg_loss:0.060, val_acc:0.967]
Epoch [56/120    avg_loss:0.054, val_acc:0.963]
Epoch [57/120    avg_loss:0.047, val_acc:0.965]
Epoch [58/120    avg_loss:0.050, val_acc:0.958]
Epoch [59/120    avg_loss:0.063, val_acc:0.955]
Epoch [60/120    avg_loss:0.041, val_acc:0.955]
Epoch [61/120    avg_loss:0.123, val_acc:0.939]
Epoch [62/120    avg_loss:0.119, val_acc:0.939]
Epoch [63/120    avg_loss:0.105, val_acc:0.949]
Epoch [64/120    avg_loss:0.078, val_acc:0.951]
Epoch [65/120    avg_loss:0.055, val_acc:0.963]
Epoch [66/120    avg_loss:0.040, val_acc:0.964]
Epoch [67/120    avg_loss:0.055, val_acc:0.963]
Epoch [68/120    avg_loss:0.051, val_acc:0.959]
Epoch [69/120    avg_loss:0.031, val_acc:0.969]
Epoch [70/120    avg_loss:0.024, val_acc:0.972]
Epoch [71/120    avg_loss:0.022, val_acc:0.972]
Epoch [72/120    avg_loss:0.024, val_acc:0.977]
Epoch [73/120    avg_loss:0.029, val_acc:0.975]
Epoch [74/120    avg_loss:0.030, val_acc:0.977]
Epoch [75/120    avg_loss:0.020, val_acc:0.977]
Epoch [76/120    avg_loss:0.024, val_acc:0.975]
Epoch [77/120    avg_loss:0.023, val_acc:0.978]
Epoch [78/120    avg_loss:0.020, val_acc:0.974]
Epoch [79/120    avg_loss:0.019, val_acc:0.978]
Epoch [80/120    avg_loss:0.022, val_acc:0.972]
Epoch [81/120    avg_loss:0.021, val_acc:0.975]
Epoch [82/120    avg_loss:0.021, val_acc:0.974]
Epoch [83/120    avg_loss:0.023, val_acc:0.978]
Epoch [84/120    avg_loss:0.026, val_acc:0.979]
Epoch [85/120    avg_loss:0.020, val_acc:0.975]
Epoch [86/120    avg_loss:0.024, val_acc:0.977]
Epoch [87/120    avg_loss:0.019, val_acc:0.978]
Epoch [88/120    avg_loss:0.020, val_acc:0.980]
Epoch [89/120    avg_loss:0.019, val_acc:0.978]
Epoch [90/120    avg_loss:0.022, val_acc:0.980]
Epoch [91/120    avg_loss:0.019, val_acc:0.978]
Epoch [92/120    avg_loss:0.025, val_acc:0.979]
Epoch [93/120    avg_loss:0.021, val_acc:0.975]
Epoch [94/120    avg_loss:0.019, val_acc:0.975]
Epoch [95/120    avg_loss:0.021, val_acc:0.979]
Epoch [96/120    avg_loss:0.015, val_acc:0.979]
Epoch [97/120    avg_loss:0.019, val_acc:0.975]
Epoch [98/120    avg_loss:0.017, val_acc:0.978]
Epoch [99/120    avg_loss:0.022, val_acc:0.979]
Epoch [100/120    avg_loss:0.019, val_acc:0.977]
Epoch [101/120    avg_loss:0.015, val_acc:0.979]
Epoch [102/120    avg_loss:0.018, val_acc:0.981]
Epoch [103/120    avg_loss:0.015, val_acc:0.979]
Epoch [104/120    avg_loss:0.015, val_acc:0.978]
Epoch [105/120    avg_loss:0.020, val_acc:0.975]
Epoch [106/120    avg_loss:0.019, val_acc:0.977]
Epoch [107/120    avg_loss:0.015, val_acc:0.977]
Epoch [108/120    avg_loss:0.018, val_acc:0.977]
Epoch [109/120    avg_loss:0.018, val_acc:0.977]
Epoch [110/120    avg_loss:0.016, val_acc:0.978]
Epoch [111/120    avg_loss:0.014, val_acc:0.979]
Epoch [112/120    avg_loss:0.015, val_acc:0.978]
Epoch [113/120    avg_loss:0.016, val_acc:0.978]
Epoch [114/120    avg_loss:0.018, val_acc:0.979]
Epoch [115/120    avg_loss:0.017, val_acc:0.978]
Epoch [116/120    avg_loss:0.017, val_acc:0.978]
Epoch [117/120    avg_loss:0.018, val_acc:0.978]
Epoch [118/120    avg_loss:0.016, val_acc:0.977]
Epoch [119/120    avg_loss:0.016, val_acc:0.978]
Epoch [120/120    avg_loss:0.016, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    3 1246    3    0    0    3    0    0    3    3   21    3    0
     0    0    0]
 [   0    0    0  725    0    7    0    0    0    4    0    0    9    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    3    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   24   83    0    8    0    0    0    0  753    1    0    0
     0    6    0]
 [   0    0    4    0    0    0    4    0    2    0   13 2177    7    3
     0    0    0]
 [   0    0    0    4    4   10    0    0    0    0    5    1  503    0
     2    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    2    0    0
  1133    0    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    94  238    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.9349593495935

F1 scores:
[       nan 0.93975904 0.9738179  0.92829706 0.99069767 0.96162528
 0.98353293 1.         0.99652375 0.72727273 0.91051995 0.98685403
 0.95085066 0.98666667 0.95450716 0.80541455 0.97109827]

Kappa:
0.9536372493419455
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1f7102f6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.396, val_acc:0.424]
Epoch [2/120    avg_loss:1.841, val_acc:0.568]
Epoch [3/120    avg_loss:1.546, val_acc:0.592]
Epoch [4/120    avg_loss:1.340, val_acc:0.627]
Epoch [5/120    avg_loss:1.140, val_acc:0.740]
Epoch [6/120    avg_loss:0.913, val_acc:0.669]
Epoch [7/120    avg_loss:0.763, val_acc:0.772]
Epoch [8/120    avg_loss:0.722, val_acc:0.732]
Epoch [9/120    avg_loss:0.527, val_acc:0.806]
Epoch [10/120    avg_loss:0.454, val_acc:0.795]
Epoch [11/120    avg_loss:0.434, val_acc:0.827]
Epoch [12/120    avg_loss:0.410, val_acc:0.783]
Epoch [13/120    avg_loss:0.443, val_acc:0.790]
Epoch [14/120    avg_loss:0.341, val_acc:0.832]
Epoch [15/120    avg_loss:0.266, val_acc:0.868]
Epoch [16/120    avg_loss:0.228, val_acc:0.904]
Epoch [17/120    avg_loss:0.186, val_acc:0.916]
Epoch [18/120    avg_loss:0.169, val_acc:0.900]
Epoch [19/120    avg_loss:0.186, val_acc:0.917]
Epoch [20/120    avg_loss:0.177, val_acc:0.911]
Epoch [21/120    avg_loss:0.135, val_acc:0.938]
Epoch [22/120    avg_loss:0.117, val_acc:0.918]
Epoch [23/120    avg_loss:0.088, val_acc:0.952]
Epoch [24/120    avg_loss:0.096, val_acc:0.945]
Epoch [25/120    avg_loss:0.091, val_acc:0.954]
Epoch [26/120    avg_loss:0.057, val_acc:0.947]
Epoch [27/120    avg_loss:0.063, val_acc:0.939]
Epoch [28/120    avg_loss:0.061, val_acc:0.965]
Epoch [29/120    avg_loss:0.071, val_acc:0.961]
Epoch [30/120    avg_loss:0.065, val_acc:0.944]
Epoch [31/120    avg_loss:0.087, val_acc:0.956]
Epoch [32/120    avg_loss:0.072, val_acc:0.953]
Epoch [33/120    avg_loss:0.061, val_acc:0.964]
Epoch [34/120    avg_loss:0.060, val_acc:0.953]
Epoch [35/120    avg_loss:0.056, val_acc:0.964]
Epoch [36/120    avg_loss:0.055, val_acc:0.971]
Epoch [37/120    avg_loss:0.081, val_acc:0.959]
Epoch [38/120    avg_loss:0.040, val_acc:0.974]
Epoch [39/120    avg_loss:0.053, val_acc:0.977]
Epoch [40/120    avg_loss:0.032, val_acc:0.975]
Epoch [41/120    avg_loss:0.031, val_acc:0.969]
Epoch [42/120    avg_loss:0.029, val_acc:0.977]
Epoch [43/120    avg_loss:0.033, val_acc:0.977]
Epoch [44/120    avg_loss:0.026, val_acc:0.978]
Epoch [45/120    avg_loss:0.020, val_acc:0.974]
Epoch [46/120    avg_loss:0.091, val_acc:0.958]
Epoch [47/120    avg_loss:0.035, val_acc:0.967]
Epoch [48/120    avg_loss:0.037, val_acc:0.971]
Epoch [49/120    avg_loss:0.022, val_acc:0.978]
Epoch [50/120    avg_loss:0.021, val_acc:0.977]
Epoch [51/120    avg_loss:0.025, val_acc:0.972]
Epoch [52/120    avg_loss:0.027, val_acc:0.975]
Epoch [53/120    avg_loss:0.013, val_acc:0.976]
Epoch [54/120    avg_loss:0.020, val_acc:0.974]
Epoch [55/120    avg_loss:0.016, val_acc:0.978]
Epoch [56/120    avg_loss:0.015, val_acc:0.980]
Epoch [57/120    avg_loss:0.009, val_acc:0.986]
Epoch [58/120    avg_loss:0.022, val_acc:0.965]
Epoch [59/120    avg_loss:0.031, val_acc:0.973]
Epoch [60/120    avg_loss:0.044, val_acc:0.950]
Epoch [61/120    avg_loss:0.031, val_acc:0.969]
Epoch [62/120    avg_loss:0.018, val_acc:0.981]
Epoch [63/120    avg_loss:0.009, val_acc:0.980]
Epoch [64/120    avg_loss:0.007, val_acc:0.990]
Epoch [65/120    avg_loss:0.008, val_acc:0.985]
Epoch [66/120    avg_loss:0.008, val_acc:0.986]
Epoch [67/120    avg_loss:0.013, val_acc:0.978]
Epoch [68/120    avg_loss:0.010, val_acc:0.981]
Epoch [69/120    avg_loss:0.007, val_acc:0.976]
Epoch [70/120    avg_loss:0.006, val_acc:0.983]
Epoch [71/120    avg_loss:0.005, val_acc:0.988]
Epoch [72/120    avg_loss:0.004, val_acc:0.986]
Epoch [73/120    avg_loss:0.004, val_acc:0.985]
Epoch [74/120    avg_loss:0.004, val_acc:0.985]
Epoch [75/120    avg_loss:0.004, val_acc:0.982]
Epoch [76/120    avg_loss:0.012, val_acc:0.978]
Epoch [77/120    avg_loss:0.009, val_acc:0.984]
Epoch [78/120    avg_loss:0.006, val_acc:0.985]
Epoch [79/120    avg_loss:0.004, val_acc:0.985]
Epoch [80/120    avg_loss:0.005, val_acc:0.986]
Epoch [81/120    avg_loss:0.004, val_acc:0.986]
Epoch [82/120    avg_loss:0.004, val_acc:0.985]
Epoch [83/120    avg_loss:0.005, val_acc:0.985]
Epoch [84/120    avg_loss:0.004, val_acc:0.985]
Epoch [85/120    avg_loss:0.007, val_acc:0.986]
Epoch [86/120    avg_loss:0.004, val_acc:0.986]
Epoch [87/120    avg_loss:0.004, val_acc:0.988]
Epoch [88/120    avg_loss:0.005, val_acc:0.986]
Epoch [89/120    avg_loss:0.003, val_acc:0.985]
Epoch [90/120    avg_loss:0.003, val_acc:0.985]
Epoch [91/120    avg_loss:0.003, val_acc:0.985]
Epoch [92/120    avg_loss:0.004, val_acc:0.985]
Epoch [93/120    avg_loss:0.004, val_acc:0.985]
Epoch [94/120    avg_loss:0.004, val_acc:0.985]
Epoch [95/120    avg_loss:0.004, val_acc:0.985]
Epoch [96/120    avg_loss:0.004, val_acc:0.985]
Epoch [97/120    avg_loss:0.003, val_acc:0.985]
Epoch [98/120    avg_loss:0.004, val_acc:0.985]
Epoch [99/120    avg_loss:0.003, val_acc:0.985]
Epoch [100/120    avg_loss:0.004, val_acc:0.985]
Epoch [101/120    avg_loss:0.004, val_acc:0.985]
Epoch [102/120    avg_loss:0.005, val_acc:0.985]
Epoch [103/120    avg_loss:0.003, val_acc:0.985]
Epoch [104/120    avg_loss:0.004, val_acc:0.985]
Epoch [105/120    avg_loss:0.004, val_acc:0.985]
Epoch [106/120    avg_loss:0.003, val_acc:0.985]
Epoch [107/120    avg_loss:0.003, val_acc:0.985]
Epoch [108/120    avg_loss:0.003, val_acc:0.985]
Epoch [109/120    avg_loss:0.004, val_acc:0.985]
Epoch [110/120    avg_loss:0.003, val_acc:0.985]
Epoch [111/120    avg_loss:0.004, val_acc:0.985]
Epoch [112/120    avg_loss:0.004, val_acc:0.985]
Epoch [113/120    avg_loss:0.003, val_acc:0.985]
Epoch [114/120    avg_loss:0.004, val_acc:0.985]
Epoch [115/120    avg_loss:0.004, val_acc:0.985]
Epoch [116/120    avg_loss:0.003, val_acc:0.985]
Epoch [117/120    avg_loss:0.004, val_acc:0.985]
Epoch [118/120    avg_loss:0.003, val_acc:0.985]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.006, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1254    6    0    3    0    0    0    2    5   15    0    0
     0    0    0]
 [   0    0    0  727    1    0    0    0    0    2    2    7    6    2
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    1    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0  868    6    0    0
     0    0    0]
 [   0    0   10    0    0    0    0    0    0    3   10 2186    1    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0    0    2    0  524    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1122   17    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    80  267    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.87533875338754

F1 scores:
[       nan 1.         0.98352941 0.97846568 0.99530516 0.99310345
 1.         1.         0.997669   0.81818182 0.98524404 0.9880226
 0.98127341 0.99462366 0.95815542 0.84493671 0.98224852]

Kappa:
0.9757636221686894
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6ca313e6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.436, val_acc:0.422]
Epoch [2/120    avg_loss:1.834, val_acc:0.535]
Epoch [3/120    avg_loss:1.509, val_acc:0.592]
Epoch [4/120    avg_loss:1.370, val_acc:0.604]
Epoch [5/120    avg_loss:1.072, val_acc:0.648]
Epoch [6/120    avg_loss:0.911, val_acc:0.709]
Epoch [7/120    avg_loss:0.849, val_acc:0.772]
Epoch [8/120    avg_loss:0.668, val_acc:0.777]
Epoch [9/120    avg_loss:0.595, val_acc:0.821]
Epoch [10/120    avg_loss:0.722, val_acc:0.756]
Epoch [11/120    avg_loss:0.588, val_acc:0.825]
Epoch [12/120    avg_loss:0.432, val_acc:0.843]
Epoch [13/120    avg_loss:0.345, val_acc:0.873]
Epoch [14/120    avg_loss:0.272, val_acc:0.889]
Epoch [15/120    avg_loss:0.253, val_acc:0.883]
Epoch [16/120    avg_loss:0.376, val_acc:0.889]
Epoch [17/120    avg_loss:0.247, val_acc:0.918]
Epoch [18/120    avg_loss:0.178, val_acc:0.921]
Epoch [19/120    avg_loss:0.159, val_acc:0.885]
Epoch [20/120    avg_loss:0.216, val_acc:0.923]
Epoch [21/120    avg_loss:0.134, val_acc:0.939]
Epoch [22/120    avg_loss:0.187, val_acc:0.889]
Epoch [23/120    avg_loss:0.156, val_acc:0.928]
Epoch [24/120    avg_loss:0.096, val_acc:0.932]
Epoch [25/120    avg_loss:0.080, val_acc:0.950]
Epoch [26/120    avg_loss:0.072, val_acc:0.955]
Epoch [27/120    avg_loss:0.073, val_acc:0.951]
Epoch [28/120    avg_loss:0.056, val_acc:0.956]
Epoch [29/120    avg_loss:0.043, val_acc:0.956]
Epoch [30/120    avg_loss:0.053, val_acc:0.949]
Epoch [31/120    avg_loss:0.069, val_acc:0.955]
Epoch [32/120    avg_loss:0.047, val_acc:0.969]
Epoch [33/120    avg_loss:0.046, val_acc:0.961]
Epoch [34/120    avg_loss:0.036, val_acc:0.973]
Epoch [35/120    avg_loss:0.037, val_acc:0.963]
Epoch [36/120    avg_loss:0.055, val_acc:0.964]
Epoch [37/120    avg_loss:0.037, val_acc:0.955]
Epoch [38/120    avg_loss:0.043, val_acc:0.955]
Epoch [39/120    avg_loss:0.038, val_acc:0.972]
Epoch [40/120    avg_loss:0.025, val_acc:0.970]
Epoch [41/120    avg_loss:0.027, val_acc:0.978]
Epoch [42/120    avg_loss:0.023, val_acc:0.969]
Epoch [43/120    avg_loss:0.027, val_acc:0.959]
Epoch [44/120    avg_loss:0.038, val_acc:0.963]
Epoch [45/120    avg_loss:0.024, val_acc:0.973]
Epoch [46/120    avg_loss:0.020, val_acc:0.972]
Epoch [47/120    avg_loss:0.016, val_acc:0.969]
Epoch [48/120    avg_loss:0.016, val_acc:0.976]
Epoch [49/120    avg_loss:0.013, val_acc:0.972]
Epoch [50/120    avg_loss:0.023, val_acc:0.969]
Epoch [51/120    avg_loss:0.016, val_acc:0.976]
Epoch [52/120    avg_loss:0.010, val_acc:0.981]
Epoch [53/120    avg_loss:0.013, val_acc:0.977]
Epoch [54/120    avg_loss:0.009, val_acc:0.979]
Epoch [55/120    avg_loss:0.010, val_acc:0.980]
Epoch [56/120    avg_loss:0.012, val_acc:0.969]
Epoch [57/120    avg_loss:0.034, val_acc:0.968]
Epoch [58/120    avg_loss:0.062, val_acc:0.934]
Epoch [59/120    avg_loss:0.163, val_acc:0.957]
Epoch [60/120    avg_loss:0.080, val_acc:0.957]
Epoch [61/120    avg_loss:0.039, val_acc:0.970]
Epoch [62/120    avg_loss:0.025, val_acc:0.976]
Epoch [63/120    avg_loss:0.024, val_acc:0.974]
Epoch [64/120    avg_loss:0.016, val_acc:0.973]
Epoch [65/120    avg_loss:0.019, val_acc:0.974]
Epoch [66/120    avg_loss:0.009, val_acc:0.975]
Epoch [67/120    avg_loss:0.012, val_acc:0.981]
Epoch [68/120    avg_loss:0.010, val_acc:0.981]
Epoch [69/120    avg_loss:0.009, val_acc:0.981]
Epoch [70/120    avg_loss:0.008, val_acc:0.980]
Epoch [71/120    avg_loss:0.007, val_acc:0.981]
Epoch [72/120    avg_loss:0.009, val_acc:0.982]
Epoch [73/120    avg_loss:0.009, val_acc:0.981]
Epoch [74/120    avg_loss:0.007, val_acc:0.980]
Epoch [75/120    avg_loss:0.009, val_acc:0.980]
Epoch [76/120    avg_loss:0.006, val_acc:0.982]
Epoch [77/120    avg_loss:0.010, val_acc:0.981]
Epoch [78/120    avg_loss:0.007, val_acc:0.979]
Epoch [79/120    avg_loss:0.007, val_acc:0.979]
Epoch [80/120    avg_loss:0.007, val_acc:0.980]
Epoch [81/120    avg_loss:0.009, val_acc:0.980]
Epoch [82/120    avg_loss:0.007, val_acc:0.978]
Epoch [83/120    avg_loss:0.006, val_acc:0.980]
Epoch [84/120    avg_loss:0.008, val_acc:0.980]
Epoch [85/120    avg_loss:0.008, val_acc:0.979]
Epoch [86/120    avg_loss:0.007, val_acc:0.981]
Epoch [87/120    avg_loss:0.007, val_acc:0.980]
Epoch [88/120    avg_loss:0.006, val_acc:0.979]
Epoch [89/120    avg_loss:0.006, val_acc:0.980]
Epoch [90/120    avg_loss:0.006, val_acc:0.979]
Epoch [91/120    avg_loss:0.007, val_acc:0.980]
Epoch [92/120    avg_loss:0.008, val_acc:0.980]
Epoch [93/120    avg_loss:0.005, val_acc:0.980]
Epoch [94/120    avg_loss:0.006, val_acc:0.980]
Epoch [95/120    avg_loss:0.006, val_acc:0.980]
Epoch [96/120    avg_loss:0.009, val_acc:0.980]
Epoch [97/120    avg_loss:0.006, val_acc:0.980]
Epoch [98/120    avg_loss:0.008, val_acc:0.980]
Epoch [99/120    avg_loss:0.007, val_acc:0.981]
Epoch [100/120    avg_loss:0.007, val_acc:0.981]
Epoch [101/120    avg_loss:0.007, val_acc:0.980]
Epoch [102/120    avg_loss:0.007, val_acc:0.981]
Epoch [103/120    avg_loss:0.007, val_acc:0.981]
Epoch [104/120    avg_loss:0.008, val_acc:0.981]
Epoch [105/120    avg_loss:0.006, val_acc:0.981]
Epoch [106/120    avg_loss:0.008, val_acc:0.981]
Epoch [107/120    avg_loss:0.007, val_acc:0.981]
Epoch [108/120    avg_loss:0.008, val_acc:0.981]
Epoch [109/120    avg_loss:0.013, val_acc:0.981]
Epoch [110/120    avg_loss:0.007, val_acc:0.981]
Epoch [111/120    avg_loss:0.006, val_acc:0.981]
Epoch [112/120    avg_loss:0.006, val_acc:0.981]
Epoch [113/120    avg_loss:0.006, val_acc:0.981]
Epoch [114/120    avg_loss:0.006, val_acc:0.981]
Epoch [115/120    avg_loss:0.007, val_acc:0.981]
Epoch [116/120    avg_loss:0.006, val_acc:0.981]
Epoch [117/120    avg_loss:0.006, val_acc:0.981]
Epoch [118/120    avg_loss:0.008, val_acc:0.981]
Epoch [119/120    avg_loss:0.007, val_acc:0.981]
Epoch [120/120    avg_loss:0.007, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1266    3    2    6    0    0    0    0    1    5    2    0
     0    0    0]
 [   0    0    0  726    0    0    0    0    0    4    0    7    8    2
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    2    0    0    0    0  849   21    0    0
     1    0    0]
 [   0    0   18    0    0    0    0    0    0    1   15 2168    6    1
     1    0    0]
 [   0    0    0    6    3    0    0    0    0    0    0    0  521    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    69  275    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.65853658536585

F1 scores:
[       nan 0.98765432 0.9844479  0.97843666 0.98368298 0.98973774
 0.99619772 1.         1.         0.85714286 0.97586207 0.98277425
 0.97110904 0.9919571  0.9627409  0.86071987 0.97619048]

Kappa:
0.973297270488157
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0d19450710>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.450, val_acc:0.347]
Epoch [2/120    avg_loss:1.849, val_acc:0.575]
Epoch [3/120    avg_loss:1.570, val_acc:0.636]
Epoch [4/120    avg_loss:1.351, val_acc:0.683]
Epoch [5/120    avg_loss:1.113, val_acc:0.636]
Epoch [6/120    avg_loss:0.889, val_acc:0.766]
Epoch [7/120    avg_loss:0.722, val_acc:0.717]
Epoch [8/120    avg_loss:0.658, val_acc:0.789]
Epoch [9/120    avg_loss:0.668, val_acc:0.761]
Epoch [10/120    avg_loss:0.529, val_acc:0.769]
Epoch [11/120    avg_loss:0.453, val_acc:0.848]
Epoch [12/120    avg_loss:0.327, val_acc:0.841]
Epoch [13/120    avg_loss:0.327, val_acc:0.854]
Epoch [14/120    avg_loss:0.311, val_acc:0.869]
Epoch [15/120    avg_loss:0.399, val_acc:0.845]
Epoch [16/120    avg_loss:0.356, val_acc:0.810]
Epoch [17/120    avg_loss:0.245, val_acc:0.896]
Epoch [18/120    avg_loss:0.310, val_acc:0.894]
Epoch [19/120    avg_loss:0.298, val_acc:0.901]
Epoch [20/120    avg_loss:0.178, val_acc:0.912]
Epoch [21/120    avg_loss:0.162, val_acc:0.919]
Epoch [22/120    avg_loss:0.138, val_acc:0.916]
Epoch [23/120    avg_loss:0.120, val_acc:0.917]
Epoch [24/120    avg_loss:0.114, val_acc:0.914]
Epoch [25/120    avg_loss:0.082, val_acc:0.927]
Epoch [26/120    avg_loss:0.066, val_acc:0.952]
Epoch [27/120    avg_loss:0.056, val_acc:0.945]
Epoch [28/120    avg_loss:0.065, val_acc:0.942]
Epoch [29/120    avg_loss:0.058, val_acc:0.922]
Epoch [30/120    avg_loss:0.060, val_acc:0.957]
Epoch [31/120    avg_loss:0.043, val_acc:0.960]
Epoch [32/120    avg_loss:0.041, val_acc:0.956]
Epoch [33/120    avg_loss:0.034, val_acc:0.955]
Epoch [34/120    avg_loss:0.038, val_acc:0.946]
Epoch [35/120    avg_loss:0.038, val_acc:0.966]
Epoch [36/120    avg_loss:0.026, val_acc:0.951]
Epoch [37/120    avg_loss:0.035, val_acc:0.956]
Epoch [38/120    avg_loss:0.037, val_acc:0.944]
Epoch [39/120    avg_loss:0.031, val_acc:0.968]
Epoch [40/120    avg_loss:0.037, val_acc:0.964]
Epoch [41/120    avg_loss:0.038, val_acc:0.940]
Epoch [42/120    avg_loss:0.050, val_acc:0.965]
Epoch [43/120    avg_loss:0.022, val_acc:0.956]
Epoch [44/120    avg_loss:0.022, val_acc:0.969]
Epoch [45/120    avg_loss:0.025, val_acc:0.952]
Epoch [46/120    avg_loss:0.030, val_acc:0.964]
Epoch [47/120    avg_loss:0.027, val_acc:0.961]
Epoch [48/120    avg_loss:0.025, val_acc:0.964]
Epoch [49/120    avg_loss:0.026, val_acc:0.952]
Epoch [50/120    avg_loss:0.025, val_acc:0.961]
Epoch [51/120    avg_loss:0.014, val_acc:0.971]
Epoch [52/120    avg_loss:0.012, val_acc:0.969]
Epoch [53/120    avg_loss:0.026, val_acc:0.944]
Epoch [54/120    avg_loss:0.066, val_acc:0.942]
Epoch [55/120    avg_loss:0.093, val_acc:0.944]
Epoch [56/120    avg_loss:0.112, val_acc:0.947]
Epoch [57/120    avg_loss:0.068, val_acc:0.960]
Epoch [58/120    avg_loss:0.035, val_acc:0.966]
Epoch [59/120    avg_loss:0.019, val_acc:0.970]
Epoch [60/120    avg_loss:0.016, val_acc:0.965]
Epoch [61/120    avg_loss:0.012, val_acc:0.968]
Epoch [62/120    avg_loss:0.084, val_acc:0.921]
Epoch [63/120    avg_loss:0.082, val_acc:0.944]
Epoch [64/120    avg_loss:0.035, val_acc:0.956]
Epoch [65/120    avg_loss:0.032, val_acc:0.967]
Epoch [66/120    avg_loss:0.021, val_acc:0.968]
Epoch [67/120    avg_loss:0.013, val_acc:0.969]
Epoch [68/120    avg_loss:0.015, val_acc:0.970]
Epoch [69/120    avg_loss:0.016, val_acc:0.970]
Epoch [70/120    avg_loss:0.021, val_acc:0.971]
Epoch [71/120    avg_loss:0.011, val_acc:0.972]
Epoch [72/120    avg_loss:0.013, val_acc:0.973]
Epoch [73/120    avg_loss:0.010, val_acc:0.973]
Epoch [74/120    avg_loss:0.015, val_acc:0.972]
Epoch [75/120    avg_loss:0.013, val_acc:0.972]
Epoch [76/120    avg_loss:0.011, val_acc:0.973]
Epoch [77/120    avg_loss:0.011, val_acc:0.972]
Epoch [78/120    avg_loss:0.012, val_acc:0.973]
Epoch [79/120    avg_loss:0.011, val_acc:0.972]
Epoch [80/120    avg_loss:0.009, val_acc:0.972]
Epoch [81/120    avg_loss:0.011, val_acc:0.971]
Epoch [82/120    avg_loss:0.010, val_acc:0.972]
Epoch [83/120    avg_loss:0.010, val_acc:0.971]
Epoch [84/120    avg_loss:0.010, val_acc:0.971]
Epoch [85/120    avg_loss:0.011, val_acc:0.971]
Epoch [86/120    avg_loss:0.011, val_acc:0.972]
Epoch [87/120    avg_loss:0.011, val_acc:0.972]
Epoch [88/120    avg_loss:0.012, val_acc:0.974]
Epoch [89/120    avg_loss:0.009, val_acc:0.974]
Epoch [90/120    avg_loss:0.013, val_acc:0.974]
Epoch [91/120    avg_loss:0.010, val_acc:0.975]
Epoch [92/120    avg_loss:0.009, val_acc:0.975]
Epoch [93/120    avg_loss:0.010, val_acc:0.973]
Epoch [94/120    avg_loss:0.009, val_acc:0.973]
Epoch [95/120    avg_loss:0.010, val_acc:0.974]
Epoch [96/120    avg_loss:0.007, val_acc:0.974]
Epoch [97/120    avg_loss:0.008, val_acc:0.974]
Epoch [98/120    avg_loss:0.010, val_acc:0.974]
Epoch [99/120    avg_loss:0.009, val_acc:0.975]
Epoch [100/120    avg_loss:0.008, val_acc:0.974]
Epoch [101/120    avg_loss:0.007, val_acc:0.974]
Epoch [102/120    avg_loss:0.009, val_acc:0.974]
Epoch [103/120    avg_loss:0.009, val_acc:0.974]
Epoch [104/120    avg_loss:0.008, val_acc:0.973]
Epoch [105/120    avg_loss:0.011, val_acc:0.974]
Epoch [106/120    avg_loss:0.014, val_acc:0.974]
Epoch [107/120    avg_loss:0.009, val_acc:0.975]
Epoch [108/120    avg_loss:0.008, val_acc:0.976]
Epoch [109/120    avg_loss:0.008, val_acc:0.976]
Epoch [110/120    avg_loss:0.008, val_acc:0.974]
Epoch [111/120    avg_loss:0.009, val_acc:0.973]
Epoch [112/120    avg_loss:0.009, val_acc:0.973]
Epoch [113/120    avg_loss:0.008, val_acc:0.974]
Epoch [114/120    avg_loss:0.009, val_acc:0.974]
Epoch [115/120    avg_loss:0.009, val_acc:0.973]
Epoch [116/120    avg_loss:0.009, val_acc:0.972]
Epoch [117/120    avg_loss:0.008, val_acc:0.973]
Epoch [118/120    avg_loss:0.006, val_acc:0.973]
Epoch [119/120    avg_loss:0.006, val_acc:0.973]
Epoch [120/120    avg_loss:0.012, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1273    2    0    0    0    0    0    0    0   10    0    0
     0    0    0]
 [   0    0    0  720    2    9    0    0    0    1    1    7    6    0
     0    1    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  648    0    0    5    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    1    0    0    0    0  837   32    0    0
     0    1    0]
 [   0    0   21    0    0    0    0    0    0    0    6 2157   22    2
     1    1    0]
 [   0    0    0    3    0    1    0    0    0    0    0    0  526    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1123   16    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
    83  263    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.25745257452574

F1 scores:
[       nan 1.         0.98567557 0.97759674 0.99297424 0.98293515
 0.99310345 0.98039216 1.         0.8372093  0.97382199 0.97645994
 0.96513761 0.99462366 0.95615155 0.83359746 0.97619048]

Kappa:
0.9687161386389268
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5df9065780>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.364, val_acc:0.497]
Epoch [2/120    avg_loss:1.884, val_acc:0.522]
Epoch [3/120    avg_loss:1.577, val_acc:0.606]
Epoch [4/120    avg_loss:1.360, val_acc:0.666]
Epoch [5/120    avg_loss:1.139, val_acc:0.700]
Epoch [6/120    avg_loss:0.922, val_acc:0.743]
Epoch [7/120    avg_loss:0.884, val_acc:0.742]
Epoch [8/120    avg_loss:0.679, val_acc:0.770]
Epoch [9/120    avg_loss:0.640, val_acc:0.807]
Epoch [10/120    avg_loss:0.566, val_acc:0.819]
Epoch [11/120    avg_loss:0.408, val_acc:0.866]
Epoch [12/120    avg_loss:0.356, val_acc:0.834]
Epoch [13/120    avg_loss:0.354, val_acc:0.798]
Epoch [14/120    avg_loss:0.439, val_acc:0.854]
Epoch [15/120    avg_loss:0.346, val_acc:0.884]
Epoch [16/120    avg_loss:0.258, val_acc:0.873]
Epoch [17/120    avg_loss:0.219, val_acc:0.918]
Epoch [18/120    avg_loss:0.203, val_acc:0.892]
Epoch [19/120    avg_loss:0.194, val_acc:0.884]
Epoch [20/120    avg_loss:0.218, val_acc:0.896]
Epoch [21/120    avg_loss:0.129, val_acc:0.941]
Epoch [22/120    avg_loss:0.119, val_acc:0.933]
Epoch [23/120    avg_loss:0.096, val_acc:0.915]
Epoch [24/120    avg_loss:0.080, val_acc:0.928]
Epoch [25/120    avg_loss:0.084, val_acc:0.927]
Epoch [26/120    avg_loss:0.175, val_acc:0.852]
Epoch [27/120    avg_loss:0.276, val_acc:0.919]
Epoch [28/120    avg_loss:0.118, val_acc:0.935]
Epoch [29/120    avg_loss:0.082, val_acc:0.945]
Epoch [30/120    avg_loss:0.099, val_acc:0.939]
Epoch [31/120    avg_loss:0.081, val_acc:0.943]
Epoch [32/120    avg_loss:0.077, val_acc:0.932]
Epoch [33/120    avg_loss:0.055, val_acc:0.952]
Epoch [34/120    avg_loss:0.054, val_acc:0.952]
Epoch [35/120    avg_loss:0.072, val_acc:0.943]
Epoch [36/120    avg_loss:0.055, val_acc:0.956]
Epoch [37/120    avg_loss:0.053, val_acc:0.960]
Epoch [38/120    avg_loss:0.046, val_acc:0.956]
Epoch [39/120    avg_loss:0.069, val_acc:0.936]
Epoch [40/120    avg_loss:0.048, val_acc:0.959]
Epoch [41/120    avg_loss:0.042, val_acc:0.957]
Epoch [42/120    avg_loss:0.031, val_acc:0.966]
Epoch [43/120    avg_loss:0.023, val_acc:0.961]
Epoch [44/120    avg_loss:0.021, val_acc:0.971]
Epoch [45/120    avg_loss:0.041, val_acc:0.961]
Epoch [46/120    avg_loss:0.026, val_acc:0.961]
Epoch [47/120    avg_loss:0.028, val_acc:0.961]
Epoch [48/120    avg_loss:0.027, val_acc:0.963]
Epoch [49/120    avg_loss:0.036, val_acc:0.960]
Epoch [50/120    avg_loss:0.026, val_acc:0.953]
Epoch [51/120    avg_loss:0.029, val_acc:0.959]
Epoch [52/120    avg_loss:0.051, val_acc:0.949]
Epoch [53/120    avg_loss:0.036, val_acc:0.960]
Epoch [54/120    avg_loss:0.023, val_acc:0.958]
Epoch [55/120    avg_loss:0.020, val_acc:0.960]
Epoch [56/120    avg_loss:0.015, val_acc:0.968]
Epoch [57/120    avg_loss:0.015, val_acc:0.968]
Epoch [58/120    avg_loss:0.012, val_acc:0.973]
Epoch [59/120    avg_loss:0.010, val_acc:0.974]
Epoch [60/120    avg_loss:0.007, val_acc:0.975]
Epoch [61/120    avg_loss:0.007, val_acc:0.974]
Epoch [62/120    avg_loss:0.011, val_acc:0.974]
Epoch [63/120    avg_loss:0.009, val_acc:0.974]
Epoch [64/120    avg_loss:0.009, val_acc:0.974]
Epoch [65/120    avg_loss:0.009, val_acc:0.971]
Epoch [66/120    avg_loss:0.009, val_acc:0.971]
Epoch [67/120    avg_loss:0.009, val_acc:0.971]
Epoch [68/120    avg_loss:0.011, val_acc:0.974]
Epoch [69/120    avg_loss:0.007, val_acc:0.973]
Epoch [70/120    avg_loss:0.009, val_acc:0.974]
Epoch [71/120    avg_loss:0.011, val_acc:0.971]
Epoch [72/120    avg_loss:0.010, val_acc:0.971]
Epoch [73/120    avg_loss:0.011, val_acc:0.972]
Epoch [74/120    avg_loss:0.012, val_acc:0.973]
Epoch [75/120    avg_loss:0.010, val_acc:0.973]
Epoch [76/120    avg_loss:0.008, val_acc:0.972]
Epoch [77/120    avg_loss:0.008, val_acc:0.972]
Epoch [78/120    avg_loss:0.009, val_acc:0.972]
Epoch [79/120    avg_loss:0.006, val_acc:0.972]
Epoch [80/120    avg_loss:0.008, val_acc:0.972]
Epoch [81/120    avg_loss:0.010, val_acc:0.972]
Epoch [82/120    avg_loss:0.009, val_acc:0.974]
Epoch [83/120    avg_loss:0.009, val_acc:0.975]
Epoch [84/120    avg_loss:0.007, val_acc:0.975]
Epoch [85/120    avg_loss:0.008, val_acc:0.975]
Epoch [86/120    avg_loss:0.009, val_acc:0.975]
Epoch [87/120    avg_loss:0.008, val_acc:0.974]
Epoch [88/120    avg_loss:0.007, val_acc:0.974]
Epoch [89/120    avg_loss:0.007, val_acc:0.974]
Epoch [90/120    avg_loss:0.010, val_acc:0.974]
Epoch [91/120    avg_loss:0.009, val_acc:0.974]
Epoch [92/120    avg_loss:0.009, val_acc:0.974]
Epoch [93/120    avg_loss:0.006, val_acc:0.974]
Epoch [94/120    avg_loss:0.011, val_acc:0.974]
Epoch [95/120    avg_loss:0.008, val_acc:0.974]
Epoch [96/120    avg_loss:0.007, val_acc:0.974]
Epoch [97/120    avg_loss:0.006, val_acc:0.974]
Epoch [98/120    avg_loss:0.009, val_acc:0.974]
Epoch [99/120    avg_loss:0.010, val_acc:0.974]
Epoch [100/120    avg_loss:0.007, val_acc:0.974]
Epoch [101/120    avg_loss:0.006, val_acc:0.974]
Epoch [102/120    avg_loss:0.007, val_acc:0.974]
Epoch [103/120    avg_loss:0.007, val_acc:0.974]
Epoch [104/120    avg_loss:0.007, val_acc:0.974]
Epoch [105/120    avg_loss:0.007, val_acc:0.974]
Epoch [106/120    avg_loss:0.007, val_acc:0.974]
Epoch [107/120    avg_loss:0.008, val_acc:0.974]
Epoch [108/120    avg_loss:0.007, val_acc:0.974]
Epoch [109/120    avg_loss:0.008, val_acc:0.974]
Epoch [110/120    avg_loss:0.008, val_acc:0.974]
Epoch [111/120    avg_loss:0.006, val_acc:0.974]
Epoch [112/120    avg_loss:0.009, val_acc:0.974]
Epoch [113/120    avg_loss:0.011, val_acc:0.974]
Epoch [114/120    avg_loss:0.008, val_acc:0.974]
Epoch [115/120    avg_loss:0.010, val_acc:0.974]
Epoch [116/120    avg_loss:0.010, val_acc:0.974]
Epoch [117/120    avg_loss:0.008, val_acc:0.974]
Epoch [118/120    avg_loss:0.008, val_acc:0.974]
Epoch [119/120    avg_loss:0.008, val_acc:0.974]
Epoch [120/120    avg_loss:0.007, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1248    2   10    2    0    0    0    0    6   17    0    0
     0    0    0]
 [   0    0    0  730    1    0    0    0    0    0    1    7    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  853   17    0    0
     0    0    0]
 [   0    0   17    0    0    0    0    0    0    0    6 2169   18    0
     0    0    0]
 [   0    0    0    5    2    0    0    0    0    0    0    0  525    0
     0    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    81  265    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.52845528455285

F1 scores:
[       nan 0.98765432 0.97652582 0.98382749 0.97038724 0.99310345
 0.99771516 1.         1.         1.         0.97989661 0.98100407
 0.96774194 0.99728997 0.95870583 0.848      0.98823529]

Kappa:
0.97180736202225
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc3d976a668>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.362, val_acc:0.470]
Epoch [2/120    avg_loss:1.859, val_acc:0.522]
Epoch [3/120    avg_loss:1.565, val_acc:0.614]
Epoch [4/120    avg_loss:1.447, val_acc:0.613]
Epoch [5/120    avg_loss:1.188, val_acc:0.735]
Epoch [6/120    avg_loss:0.958, val_acc:0.754]
Epoch [7/120    avg_loss:0.784, val_acc:0.725]
Epoch [8/120    avg_loss:0.792, val_acc:0.805]
Epoch [9/120    avg_loss:0.777, val_acc:0.817]
Epoch [10/120    avg_loss:0.497, val_acc:0.861]
Epoch [11/120    avg_loss:0.417, val_acc:0.873]
Epoch [12/120    avg_loss:0.363, val_acc:0.855]
Epoch [13/120    avg_loss:0.306, val_acc:0.871]
Epoch [14/120    avg_loss:0.630, val_acc:0.798]
Epoch [15/120    avg_loss:0.512, val_acc:0.783]
Epoch [16/120    avg_loss:0.367, val_acc:0.857]
Epoch [17/120    avg_loss:0.308, val_acc:0.897]
Epoch [18/120    avg_loss:0.252, val_acc:0.918]
Epoch [19/120    avg_loss:0.207, val_acc:0.924]
Epoch [20/120    avg_loss:0.149, val_acc:0.928]
Epoch [21/120    avg_loss:0.116, val_acc:0.936]
Epoch [22/120    avg_loss:0.110, val_acc:0.949]
Epoch [23/120    avg_loss:0.090, val_acc:0.944]
Epoch [24/120    avg_loss:0.113, val_acc:0.949]
Epoch [25/120    avg_loss:0.111, val_acc:0.946]
Epoch [26/120    avg_loss:0.110, val_acc:0.959]
Epoch [27/120    avg_loss:0.092, val_acc:0.964]
Epoch [28/120    avg_loss:0.095, val_acc:0.950]
Epoch [29/120    avg_loss:0.111, val_acc:0.957]
Epoch [30/120    avg_loss:0.071, val_acc:0.960]
Epoch [31/120    avg_loss:0.073, val_acc:0.955]
Epoch [32/120    avg_loss:0.045, val_acc:0.966]
Epoch [33/120    avg_loss:0.045, val_acc:0.967]
Epoch [34/120    avg_loss:0.052, val_acc:0.956]
Epoch [35/120    avg_loss:0.038, val_acc:0.976]
Epoch [36/120    avg_loss:0.028, val_acc:0.971]
Epoch [37/120    avg_loss:0.049, val_acc:0.966]
Epoch [38/120    avg_loss:0.054, val_acc:0.961]
Epoch [39/120    avg_loss:0.043, val_acc:0.970]
Epoch [40/120    avg_loss:0.045, val_acc:0.977]
Epoch [41/120    avg_loss:0.033, val_acc:0.974]
Epoch [42/120    avg_loss:0.028, val_acc:0.966]
Epoch [43/120    avg_loss:0.034, val_acc:0.972]
Epoch [44/120    avg_loss:0.049, val_acc:0.949]
Epoch [45/120    avg_loss:0.077, val_acc:0.963]
Epoch [46/120    avg_loss:0.036, val_acc:0.972]
Epoch [47/120    avg_loss:0.037, val_acc:0.974]
Epoch [48/120    avg_loss:0.023, val_acc:0.972]
Epoch [49/120    avg_loss:0.020, val_acc:0.976]
Epoch [50/120    avg_loss:0.026, val_acc:0.980]
Epoch [51/120    avg_loss:0.016, val_acc:0.977]
Epoch [52/120    avg_loss:0.018, val_acc:0.977]
Epoch [53/120    avg_loss:0.015, val_acc:0.982]
Epoch [54/120    avg_loss:0.011, val_acc:0.982]
Epoch [55/120    avg_loss:0.014, val_acc:0.978]
Epoch [56/120    avg_loss:0.015, val_acc:0.978]
Epoch [57/120    avg_loss:0.013, val_acc:0.976]
Epoch [58/120    avg_loss:0.016, val_acc:0.981]
Epoch [59/120    avg_loss:0.018, val_acc:0.982]
Epoch [60/120    avg_loss:0.011, val_acc:0.981]
Epoch [61/120    avg_loss:0.013, val_acc:0.974]
Epoch [62/120    avg_loss:0.013, val_acc:0.980]
Epoch [63/120    avg_loss:0.015, val_acc:0.976]
Epoch [64/120    avg_loss:0.015, val_acc:0.981]
Epoch [65/120    avg_loss:0.027, val_acc:0.960]
Epoch [66/120    avg_loss:0.019, val_acc:0.972]
Epoch [67/120    avg_loss:0.033, val_acc:0.969]
Epoch [68/120    avg_loss:0.021, val_acc:0.982]
Epoch [69/120    avg_loss:0.016, val_acc:0.980]
Epoch [70/120    avg_loss:0.008, val_acc:0.982]
Epoch [71/120    avg_loss:0.009, val_acc:0.983]
Epoch [72/120    avg_loss:0.006, val_acc:0.984]
Epoch [73/120    avg_loss:0.006, val_acc:0.985]
Epoch [74/120    avg_loss:0.005, val_acc:0.988]
Epoch [75/120    avg_loss:0.007, val_acc:0.983]
Epoch [76/120    avg_loss:0.007, val_acc:0.985]
Epoch [77/120    avg_loss:0.014, val_acc:0.961]
Epoch [78/120    avg_loss:0.290, val_acc:0.934]
Epoch [79/120    avg_loss:0.047, val_acc:0.977]
Epoch [80/120    avg_loss:0.061, val_acc:0.976]
Epoch [81/120    avg_loss:0.042, val_acc:0.976]
Epoch [82/120    avg_loss:0.013, val_acc:0.980]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.012, val_acc:0.979]
Epoch [85/120    avg_loss:0.009, val_acc:0.981]
Epoch [86/120    avg_loss:0.020, val_acc:0.980]
Epoch [87/120    avg_loss:0.017, val_acc:0.979]
Epoch [88/120    avg_loss:0.010, val_acc:0.978]
Epoch [89/120    avg_loss:0.009, val_acc:0.980]
Epoch [90/120    avg_loss:0.008, val_acc:0.982]
Epoch [91/120    avg_loss:0.008, val_acc:0.982]
Epoch [92/120    avg_loss:0.008, val_acc:0.982]
Epoch [93/120    avg_loss:0.006, val_acc:0.983]
Epoch [94/120    avg_loss:0.008, val_acc:0.983]
Epoch [95/120    avg_loss:0.006, val_acc:0.983]
Epoch [96/120    avg_loss:0.006, val_acc:0.983]
Epoch [97/120    avg_loss:0.007, val_acc:0.983]
Epoch [98/120    avg_loss:0.006, val_acc:0.983]
Epoch [99/120    avg_loss:0.007, val_acc:0.983]
Epoch [100/120    avg_loss:0.006, val_acc:0.983]
Epoch [101/120    avg_loss:0.006, val_acc:0.983]
Epoch [102/120    avg_loss:0.006, val_acc:0.983]
Epoch [103/120    avg_loss:0.006, val_acc:0.983]
Epoch [104/120    avg_loss:0.006, val_acc:0.983]
Epoch [105/120    avg_loss:0.006, val_acc:0.983]
Epoch [106/120    avg_loss:0.005, val_acc:0.983]
Epoch [107/120    avg_loss:0.005, val_acc:0.983]
Epoch [108/120    avg_loss:0.005, val_acc:0.983]
Epoch [109/120    avg_loss:0.006, val_acc:0.983]
Epoch [110/120    avg_loss:0.006, val_acc:0.983]
Epoch [111/120    avg_loss:0.005, val_acc:0.983]
Epoch [112/120    avg_loss:0.007, val_acc:0.983]
Epoch [113/120    avg_loss:0.006, val_acc:0.983]
Epoch [114/120    avg_loss:0.006, val_acc:0.983]
Epoch [115/120    avg_loss:0.005, val_acc:0.983]
Epoch [116/120    avg_loss:0.005, val_acc:0.983]
Epoch [117/120    avg_loss:0.005, val_acc:0.983]
Epoch [118/120    avg_loss:0.006, val_acc:0.983]
Epoch [119/120    avg_loss:0.004, val_acc:0.983]
Epoch [120/120    avg_loss:0.006, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    1    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1276    3    0    0    0    0    0    1    1    4    0    0
     0    0    0]
 [   0    0    1  730    3    0    0    0    0    2    3    0    3    5
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    2    5    0    0    0  849   15    0    0
     1    0    0]
 [   0    0    7    3    0    0    0    0    0    0    9 2158   14   19
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0    0  530    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1122   17    0]
 [   0    0    0    0    0    1    4    0    0    0    0    0    0    0
    85  257    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.5609756097561

F1 scores:
[       nan 0.98765432 0.99106796 0.98316498 0.99300699 0.99196326
 0.9924357  1.         0.99649942 0.89473684 0.9775475  0.98381582
 0.97966728 0.93908629 0.95448745 0.82636656 0.98224852]

Kappa:
0.9721964106234204
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f61c7542780>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.414, val_acc:0.381]
Epoch [2/120    avg_loss:1.815, val_acc:0.554]
Epoch [3/120    avg_loss:1.533, val_acc:0.576]
Epoch [4/120    avg_loss:1.311, val_acc:0.654]
Epoch [5/120    avg_loss:1.004, val_acc:0.736]
Epoch [6/120    avg_loss:1.060, val_acc:0.719]
Epoch [7/120    avg_loss:0.759, val_acc:0.802]
Epoch [8/120    avg_loss:0.711, val_acc:0.803]
Epoch [9/120    avg_loss:0.631, val_acc:0.833]
Epoch [10/120    avg_loss:0.499, val_acc:0.867]
Epoch [11/120    avg_loss:0.468, val_acc:0.866]
Epoch [12/120    avg_loss:0.350, val_acc:0.870]
Epoch [13/120    avg_loss:0.360, val_acc:0.866]
Epoch [14/120    avg_loss:0.227, val_acc:0.925]
Epoch [15/120    avg_loss:0.202, val_acc:0.903]
Epoch [16/120    avg_loss:0.204, val_acc:0.915]
Epoch [17/120    avg_loss:0.164, val_acc:0.925]
Epoch [18/120    avg_loss:0.141, val_acc:0.929]
Epoch [19/120    avg_loss:0.144, val_acc:0.927]
Epoch [20/120    avg_loss:0.375, val_acc:0.865]
Epoch [21/120    avg_loss:0.205, val_acc:0.931]
Epoch [22/120    avg_loss:0.144, val_acc:0.948]
Epoch [23/120    avg_loss:0.120, val_acc:0.946]
Epoch [24/120    avg_loss:0.101, val_acc:0.953]
Epoch [25/120    avg_loss:0.083, val_acc:0.961]
Epoch [26/120    avg_loss:0.076, val_acc:0.967]
Epoch [27/120    avg_loss:0.092, val_acc:0.952]
Epoch [28/120    avg_loss:0.081, val_acc:0.941]
Epoch [29/120    avg_loss:0.082, val_acc:0.932]
Epoch [30/120    avg_loss:0.068, val_acc:0.955]
Epoch [31/120    avg_loss:0.064, val_acc:0.961]
Epoch [32/120    avg_loss:0.052, val_acc:0.953]
Epoch [33/120    avg_loss:0.042, val_acc:0.966]
Epoch [34/120    avg_loss:0.036, val_acc:0.972]
Epoch [35/120    avg_loss:0.035, val_acc:0.971]
Epoch [36/120    avg_loss:0.035, val_acc:0.960]
Epoch [37/120    avg_loss:0.057, val_acc:0.964]
Epoch [38/120    avg_loss:0.040, val_acc:0.972]
Epoch [39/120    avg_loss:0.039, val_acc:0.977]
Epoch [40/120    avg_loss:0.029, val_acc:0.980]
Epoch [41/120    avg_loss:0.026, val_acc:0.974]
Epoch [42/120    avg_loss:0.055, val_acc:0.965]
Epoch [43/120    avg_loss:0.030, val_acc:0.979]
Epoch [44/120    avg_loss:0.018, val_acc:0.982]
Epoch [45/120    avg_loss:0.014, val_acc:0.978]
Epoch [46/120    avg_loss:0.020, val_acc:0.975]
Epoch [47/120    avg_loss:0.017, val_acc:0.976]
Epoch [48/120    avg_loss:0.013, val_acc:0.978]
Epoch [49/120    avg_loss:0.011, val_acc:0.985]
Epoch [50/120    avg_loss:0.013, val_acc:0.988]
Epoch [51/120    avg_loss:0.015, val_acc:0.979]
Epoch [52/120    avg_loss:0.017, val_acc:0.984]
Epoch [53/120    avg_loss:0.011, val_acc:0.982]
Epoch [54/120    avg_loss:0.015, val_acc:0.973]
Epoch [55/120    avg_loss:0.042, val_acc:0.985]
Epoch [56/120    avg_loss:0.028, val_acc:0.973]
Epoch [57/120    avg_loss:0.021, val_acc:0.977]
Epoch [58/120    avg_loss:0.018, val_acc:0.979]
Epoch [59/120    avg_loss:0.020, val_acc:0.976]
Epoch [60/120    avg_loss:0.022, val_acc:0.980]
Epoch [61/120    avg_loss:0.016, val_acc:0.977]
Epoch [62/120    avg_loss:0.008, val_acc:0.978]
Epoch [63/120    avg_loss:0.008, val_acc:0.984]
Epoch [64/120    avg_loss:0.006, val_acc:0.984]
Epoch [65/120    avg_loss:0.006, val_acc:0.983]
Epoch [66/120    avg_loss:0.007, val_acc:0.984]
Epoch [67/120    avg_loss:0.006, val_acc:0.984]
Epoch [68/120    avg_loss:0.005, val_acc:0.984]
Epoch [69/120    avg_loss:0.005, val_acc:0.985]
Epoch [70/120    avg_loss:0.005, val_acc:0.985]
Epoch [71/120    avg_loss:0.005, val_acc:0.986]
Epoch [72/120    avg_loss:0.004, val_acc:0.986]
Epoch [73/120    avg_loss:0.005, val_acc:0.986]
Epoch [74/120    avg_loss:0.006, val_acc:0.986]
Epoch [75/120    avg_loss:0.005, val_acc:0.984]
Epoch [76/120    avg_loss:0.005, val_acc:0.986]
Epoch [77/120    avg_loss:0.006, val_acc:0.986]
Epoch [78/120    avg_loss:0.006, val_acc:0.986]
Epoch [79/120    avg_loss:0.006, val_acc:0.985]
Epoch [80/120    avg_loss:0.006, val_acc:0.985]
Epoch [81/120    avg_loss:0.006, val_acc:0.985]
Epoch [82/120    avg_loss:0.005, val_acc:0.985]
Epoch [83/120    avg_loss:0.005, val_acc:0.985]
Epoch [84/120    avg_loss:0.005, val_acc:0.985]
Epoch [85/120    avg_loss:0.004, val_acc:0.985]
Epoch [86/120    avg_loss:0.006, val_acc:0.985]
Epoch [87/120    avg_loss:0.004, val_acc:0.985]
Epoch [88/120    avg_loss:0.006, val_acc:0.985]
Epoch [89/120    avg_loss:0.007, val_acc:0.985]
Epoch [90/120    avg_loss:0.006, val_acc:0.985]
Epoch [91/120    avg_loss:0.005, val_acc:0.985]
Epoch [92/120    avg_loss:0.005, val_acc:0.985]
Epoch [93/120    avg_loss:0.004, val_acc:0.985]
Epoch [94/120    avg_loss:0.005, val_acc:0.985]
Epoch [95/120    avg_loss:0.006, val_acc:0.985]
Epoch [96/120    avg_loss:0.005, val_acc:0.985]
Epoch [97/120    avg_loss:0.005, val_acc:0.985]
Epoch [98/120    avg_loss:0.005, val_acc:0.985]
Epoch [99/120    avg_loss:0.004, val_acc:0.985]
Epoch [100/120    avg_loss:0.005, val_acc:0.985]
Epoch [101/120    avg_loss:0.004, val_acc:0.985]
Epoch [102/120    avg_loss:0.006, val_acc:0.985]
Epoch [103/120    avg_loss:0.005, val_acc:0.985]
Epoch [104/120    avg_loss:0.007, val_acc:0.985]
Epoch [105/120    avg_loss:0.005, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.985]
Epoch [107/120    avg_loss:0.005, val_acc:0.985]
Epoch [108/120    avg_loss:0.007, val_acc:0.985]
Epoch [109/120    avg_loss:0.004, val_acc:0.985]
Epoch [110/120    avg_loss:0.006, val_acc:0.985]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.008, val_acc:0.985]
Epoch [113/120    avg_loss:0.004, val_acc:0.985]
Epoch [114/120    avg_loss:0.005, val_acc:0.985]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.004, val_acc:0.985]
Epoch [117/120    avg_loss:0.004, val_acc:0.985]
Epoch [118/120    avg_loss:0.004, val_acc:0.985]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.005, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    1 1266    0    0    2    1    0    0    0    1    9    5    0
     0    0    0]
 [   0    0    0  729    4    0    0    0    0    1    0    2   10    0
     0    1    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  848   22    0    0
     0    0    0]
 [   0    0    9    0    0    0    0    0    0    1    8 2178   10    1
     0    3    0]
 [   0    0    0    6    0    0    0    0    0    0    0    1  524    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1118   20    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    35  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.00542005420054

F1 scores:
[       nan 0.97560976 0.9871345  0.98314228 0.98598131 0.9954023
 0.98718915 1.         1.         0.94736842 0.97864974 0.98485191
 0.96500921 0.99730458 0.97429194 0.89120715 0.97619048]

Kappa:
0.9772577549963503
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2219f596a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.358, val_acc:0.491]
Epoch [2/120    avg_loss:1.863, val_acc:0.575]
Epoch [3/120    avg_loss:1.522, val_acc:0.599]
Epoch [4/120    avg_loss:1.345, val_acc:0.630]
Epoch [5/120    avg_loss:1.138, val_acc:0.713]
Epoch [6/120    avg_loss:0.960, val_acc:0.678]
Epoch [7/120    avg_loss:0.898, val_acc:0.656]
Epoch [8/120    avg_loss:0.763, val_acc:0.801]
Epoch [9/120    avg_loss:0.574, val_acc:0.791]
Epoch [10/120    avg_loss:0.552, val_acc:0.790]
Epoch [11/120    avg_loss:0.508, val_acc:0.775]
Epoch [12/120    avg_loss:0.454, val_acc:0.867]
Epoch [13/120    avg_loss:0.405, val_acc:0.832]
Epoch [14/120    avg_loss:0.342, val_acc:0.819]
Epoch [15/120    avg_loss:0.302, val_acc:0.899]
Epoch [16/120    avg_loss:0.282, val_acc:0.889]
Epoch [17/120    avg_loss:0.206, val_acc:0.901]
Epoch [18/120    avg_loss:0.162, val_acc:0.916]
Epoch [19/120    avg_loss:0.166, val_acc:0.916]
Epoch [20/120    avg_loss:0.168, val_acc:0.877]
Epoch [21/120    avg_loss:0.229, val_acc:0.905]
Epoch [22/120    avg_loss:0.152, val_acc:0.934]
Epoch [23/120    avg_loss:0.132, val_acc:0.890]
Epoch [24/120    avg_loss:0.122, val_acc:0.935]
Epoch [25/120    avg_loss:0.117, val_acc:0.943]
Epoch [26/120    avg_loss:0.084, val_acc:0.955]
Epoch [27/120    avg_loss:0.068, val_acc:0.942]
Epoch [28/120    avg_loss:0.062, val_acc:0.947]
Epoch [29/120    avg_loss:0.045, val_acc:0.951]
Epoch [30/120    avg_loss:0.053, val_acc:0.949]
Epoch [31/120    avg_loss:0.047, val_acc:0.954]
Epoch [32/120    avg_loss:0.040, val_acc:0.965]
Epoch [33/120    avg_loss:0.043, val_acc:0.956]
Epoch [34/120    avg_loss:0.064, val_acc:0.954]
Epoch [35/120    avg_loss:0.050, val_acc:0.948]
Epoch [36/120    avg_loss:0.034, val_acc:0.960]
Epoch [37/120    avg_loss:0.046, val_acc:0.965]
Epoch [38/120    avg_loss:0.041, val_acc:0.950]
Epoch [39/120    avg_loss:0.039, val_acc:0.948]
Epoch [40/120    avg_loss:0.051, val_acc:0.963]
Epoch [41/120    avg_loss:0.093, val_acc:0.966]
Epoch [42/120    avg_loss:0.047, val_acc:0.965]
Epoch [43/120    avg_loss:0.038, val_acc:0.969]
Epoch [44/120    avg_loss:0.030, val_acc:0.973]
Epoch [45/120    avg_loss:0.025, val_acc:0.963]
Epoch [46/120    avg_loss:0.041, val_acc:0.959]
Epoch [47/120    avg_loss:0.033, val_acc:0.974]
Epoch [48/120    avg_loss:0.036, val_acc:0.961]
Epoch [49/120    avg_loss:0.032, val_acc:0.969]
Epoch [50/120    avg_loss:0.023, val_acc:0.979]
Epoch [51/120    avg_loss:0.020, val_acc:0.961]
Epoch [52/120    avg_loss:0.012, val_acc:0.974]
Epoch [53/120    avg_loss:0.014, val_acc:0.979]
Epoch [54/120    avg_loss:0.024, val_acc:0.971]
Epoch [55/120    avg_loss:0.028, val_acc:0.971]
Epoch [56/120    avg_loss:0.015, val_acc:0.979]
Epoch [57/120    avg_loss:0.015, val_acc:0.972]
Epoch [58/120    avg_loss:0.033, val_acc:0.972]
Epoch [59/120    avg_loss:0.014, val_acc:0.974]
Epoch [60/120    avg_loss:0.009, val_acc:0.978]
Epoch [61/120    avg_loss:0.009, val_acc:0.977]
Epoch [62/120    avg_loss:0.006, val_acc:0.979]
Epoch [63/120    avg_loss:0.009, val_acc:0.979]
Epoch [64/120    avg_loss:0.011, val_acc:0.969]
Epoch [65/120    avg_loss:0.013, val_acc:0.975]
Epoch [66/120    avg_loss:0.008, val_acc:0.977]
Epoch [67/120    avg_loss:0.006, val_acc:0.978]
Epoch [68/120    avg_loss:0.007, val_acc:0.977]
Epoch [69/120    avg_loss:0.009, val_acc:0.976]
Epoch [70/120    avg_loss:0.021, val_acc:0.972]
Epoch [71/120    avg_loss:0.011, val_acc:0.976]
Epoch [72/120    avg_loss:0.010, val_acc:0.979]
Epoch [73/120    avg_loss:0.005, val_acc:0.981]
Epoch [74/120    avg_loss:0.008, val_acc:0.975]
Epoch [75/120    avg_loss:0.007, val_acc:0.978]
Epoch [76/120    avg_loss:0.009, val_acc:0.981]
Epoch [77/120    avg_loss:0.023, val_acc:0.971]
Epoch [78/120    avg_loss:0.012, val_acc:0.975]
Epoch [79/120    avg_loss:0.010, val_acc:0.977]
Epoch [80/120    avg_loss:0.007, val_acc:0.978]
Epoch [81/120    avg_loss:0.005, val_acc:0.978]
Epoch [82/120    avg_loss:0.004, val_acc:0.979]
Epoch [83/120    avg_loss:0.008, val_acc:0.971]
Epoch [84/120    avg_loss:0.009, val_acc:0.976]
Epoch [85/120    avg_loss:0.005, val_acc:0.979]
Epoch [86/120    avg_loss:0.008, val_acc:0.978]
Epoch [87/120    avg_loss:0.005, val_acc:0.980]
Epoch [88/120    avg_loss:0.005, val_acc:0.980]
Epoch [89/120    avg_loss:0.012, val_acc:0.959]
Epoch [90/120    avg_loss:0.013, val_acc:0.975]
Epoch [91/120    avg_loss:0.005, val_acc:0.976]
Epoch [92/120    avg_loss:0.004, val_acc:0.978]
Epoch [93/120    avg_loss:0.004, val_acc:0.979]
Epoch [94/120    avg_loss:0.004, val_acc:0.979]
Epoch [95/120    avg_loss:0.005, val_acc:0.979]
Epoch [96/120    avg_loss:0.005, val_acc:0.978]
Epoch [97/120    avg_loss:0.003, val_acc:0.979]
Epoch [98/120    avg_loss:0.004, val_acc:0.979]
Epoch [99/120    avg_loss:0.004, val_acc:0.980]
Epoch [100/120    avg_loss:0.004, val_acc:0.980]
Epoch [101/120    avg_loss:0.003, val_acc:0.982]
Epoch [102/120    avg_loss:0.003, val_acc:0.982]
Epoch [103/120    avg_loss:0.003, val_acc:0.981]
Epoch [104/120    avg_loss:0.004, val_acc:0.980]
Epoch [105/120    avg_loss:0.003, val_acc:0.981]
Epoch [106/120    avg_loss:0.003, val_acc:0.982]
Epoch [107/120    avg_loss:0.003, val_acc:0.982]
Epoch [108/120    avg_loss:0.003, val_acc:0.981]
Epoch [109/120    avg_loss:0.004, val_acc:0.981]
Epoch [110/120    avg_loss:0.003, val_acc:0.981]
Epoch [111/120    avg_loss:0.003, val_acc:0.982]
Epoch [112/120    avg_loss:0.005, val_acc:0.981]
Epoch [113/120    avg_loss:0.003, val_acc:0.981]
Epoch [114/120    avg_loss:0.004, val_acc:0.982]
Epoch [115/120    avg_loss:0.003, val_acc:0.982]
Epoch [116/120    avg_loss:0.003, val_acc:0.982]
Epoch [117/120    avg_loss:0.004, val_acc:0.982]
Epoch [118/120    avg_loss:0.005, val_acc:0.983]
Epoch [119/120    avg_loss:0.004, val_acc:0.982]
Epoch [120/120    avg_loss:0.003, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    3   15    0    0    0    0    2    0    5    0    0
     0    0    0]
 [   0    0    0  723    4    0    2    0    0    7    2    3    3    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    1    0    0    0    0  855   15    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    1    6 2163   36    1
     0    0    0]
 [   0    0    0    1    0    1    0    0    0    0    0    1  527    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    0    0    0    0
  1121   17    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    77  268    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.57181571815718

F1 scores:
[       nan 1.         0.98746082 0.98100407 0.95730337 0.99655568
 0.9969651  1.         0.99883586 0.76595745 0.98388953 0.98385263
 0.9555757  0.98930481 0.95893926 0.84542587 0.97619048]

Kappa:
0.9723259752622825
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f91bda91710>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.434, val_acc:0.446]
Epoch [2/120    avg_loss:1.929, val_acc:0.519]
Epoch [3/120    avg_loss:1.522, val_acc:0.555]
Epoch [4/120    avg_loss:1.335, val_acc:0.656]
Epoch [5/120    avg_loss:1.173, val_acc:0.732]
Epoch [6/120    avg_loss:1.029, val_acc:0.688]
Epoch [7/120    avg_loss:0.919, val_acc:0.758]
Epoch [8/120    avg_loss:0.796, val_acc:0.779]
Epoch [9/120    avg_loss:0.725, val_acc:0.809]
Epoch [10/120    avg_loss:0.645, val_acc:0.734]
Epoch [11/120    avg_loss:0.540, val_acc:0.859]
Epoch [12/120    avg_loss:0.415, val_acc:0.873]
Epoch [13/120    avg_loss:0.335, val_acc:0.882]
Epoch [14/120    avg_loss:0.419, val_acc:0.867]
Epoch [15/120    avg_loss:0.411, val_acc:0.895]
Epoch [16/120    avg_loss:0.224, val_acc:0.908]
Epoch [17/120    avg_loss:0.181, val_acc:0.931]
Epoch [18/120    avg_loss:0.162, val_acc:0.915]
Epoch [19/120    avg_loss:0.166, val_acc:0.930]
Epoch [20/120    avg_loss:0.135, val_acc:0.914]
Epoch [21/120    avg_loss:0.141, val_acc:0.935]
Epoch [22/120    avg_loss:0.126, val_acc:0.934]
Epoch [23/120    avg_loss:0.092, val_acc:0.949]
Epoch [24/120    avg_loss:0.110, val_acc:0.944]
Epoch [25/120    avg_loss:0.075, val_acc:0.955]
Epoch [26/120    avg_loss:0.077, val_acc:0.953]
Epoch [27/120    avg_loss:0.053, val_acc:0.965]
Epoch [28/120    avg_loss:0.064, val_acc:0.964]
Epoch [29/120    avg_loss:0.043, val_acc:0.963]
Epoch [30/120    avg_loss:0.086, val_acc:0.942]
Epoch [31/120    avg_loss:0.155, val_acc:0.916]
Epoch [32/120    avg_loss:0.179, val_acc:0.953]
Epoch [33/120    avg_loss:0.126, val_acc:0.945]
Epoch [34/120    avg_loss:0.071, val_acc:0.966]
Epoch [35/120    avg_loss:0.060, val_acc:0.966]
Epoch [36/120    avg_loss:0.080, val_acc:0.959]
Epoch [37/120    avg_loss:0.050, val_acc:0.970]
Epoch [38/120    avg_loss:0.041, val_acc:0.959]
Epoch [39/120    avg_loss:0.043, val_acc:0.969]
Epoch [40/120    avg_loss:0.047, val_acc:0.963]
Epoch [41/120    avg_loss:0.042, val_acc:0.963]
Epoch [42/120    avg_loss:0.035, val_acc:0.976]
Epoch [43/120    avg_loss:0.028, val_acc:0.973]
Epoch [44/120    avg_loss:0.029, val_acc:0.975]
Epoch [45/120    avg_loss:0.017, val_acc:0.974]
Epoch [46/120    avg_loss:0.019, val_acc:0.977]
Epoch [47/120    avg_loss:0.014, val_acc:0.981]
Epoch [48/120    avg_loss:0.014, val_acc:0.980]
Epoch [49/120    avg_loss:0.013, val_acc:0.980]
Epoch [50/120    avg_loss:0.020, val_acc:0.967]
Epoch [51/120    avg_loss:0.017, val_acc:0.976]
Epoch [52/120    avg_loss:0.014, val_acc:0.981]
Epoch [53/120    avg_loss:0.010, val_acc:0.980]
Epoch [54/120    avg_loss:0.011, val_acc:0.979]
Epoch [55/120    avg_loss:0.010, val_acc:0.983]
Epoch [56/120    avg_loss:0.008, val_acc:0.980]
Epoch [57/120    avg_loss:0.017, val_acc:0.970]
Epoch [58/120    avg_loss:0.018, val_acc:0.978]
Epoch [59/120    avg_loss:0.015, val_acc:0.982]
Epoch [60/120    avg_loss:0.013, val_acc:0.982]
Epoch [61/120    avg_loss:0.008, val_acc:0.981]
Epoch [62/120    avg_loss:0.012, val_acc:0.977]
Epoch [63/120    avg_loss:0.015, val_acc:0.967]
Epoch [64/120    avg_loss:0.014, val_acc:0.980]
Epoch [65/120    avg_loss:0.018, val_acc:0.977]
Epoch [66/120    avg_loss:0.013, val_acc:0.977]
Epoch [67/120    avg_loss:0.006, val_acc:0.981]
Epoch [68/120    avg_loss:0.005, val_acc:0.983]
Epoch [69/120    avg_loss:0.012, val_acc:0.974]
Epoch [70/120    avg_loss:0.007, val_acc:0.978]
Epoch [71/120    avg_loss:0.017, val_acc:0.983]
Epoch [72/120    avg_loss:0.009, val_acc:0.984]
Epoch [73/120    avg_loss:0.006, val_acc:0.983]
Epoch [74/120    avg_loss:0.005, val_acc:0.982]
Epoch [75/120    avg_loss:0.005, val_acc:0.981]
Epoch [76/120    avg_loss:0.010, val_acc:0.979]
Epoch [77/120    avg_loss:0.006, val_acc:0.980]
Epoch [78/120    avg_loss:0.005, val_acc:0.984]
Epoch [79/120    avg_loss:0.004, val_acc:0.982]
Epoch [80/120    avg_loss:0.004, val_acc:0.983]
Epoch [81/120    avg_loss:0.005, val_acc:0.985]
Epoch [82/120    avg_loss:0.003, val_acc:0.985]
Epoch [83/120    avg_loss:0.004, val_acc:0.985]
Epoch [84/120    avg_loss:0.005, val_acc:0.981]
Epoch [85/120    avg_loss:0.009, val_acc:0.979]
Epoch [86/120    avg_loss:0.014, val_acc:0.980]
Epoch [87/120    avg_loss:0.007, val_acc:0.981]
Epoch [88/120    avg_loss:0.006, val_acc:0.983]
Epoch [89/120    avg_loss:0.014, val_acc:0.969]
Epoch [90/120    avg_loss:0.008, val_acc:0.983]
Epoch [91/120    avg_loss:0.004, val_acc:0.983]
Epoch [92/120    avg_loss:0.003, val_acc:0.983]
Epoch [93/120    avg_loss:0.003, val_acc:0.983]
Epoch [94/120    avg_loss:0.004, val_acc:0.983]
Epoch [95/120    avg_loss:0.005, val_acc:0.985]
Epoch [96/120    avg_loss:0.007, val_acc:0.981]
Epoch [97/120    avg_loss:0.004, val_acc:0.983]
Epoch [98/120    avg_loss:0.004, val_acc:0.983]
Epoch [99/120    avg_loss:0.006, val_acc:0.982]
Epoch [100/120    avg_loss:0.003, val_acc:0.984]
Epoch [101/120    avg_loss:0.003, val_acc:0.982]
Epoch [102/120    avg_loss:0.003, val_acc:0.984]
Epoch [103/120    avg_loss:0.005, val_acc:0.982]
Epoch [104/120    avg_loss:0.005, val_acc:0.983]
Epoch [105/120    avg_loss:0.003, val_acc:0.988]
Epoch [106/120    avg_loss:0.002, val_acc:0.986]
Epoch [107/120    avg_loss:0.002, val_acc:0.986]
Epoch [108/120    avg_loss:0.003, val_acc:0.985]
Epoch [109/120    avg_loss:0.002, val_acc:0.983]
Epoch [110/120    avg_loss:0.002, val_acc:0.983]
Epoch [111/120    avg_loss:0.002, val_acc:0.983]
Epoch [112/120    avg_loss:0.003, val_acc:0.984]
Epoch [113/120    avg_loss:0.002, val_acc:0.984]
Epoch [114/120    avg_loss:0.002, val_acc:0.983]
Epoch [115/120    avg_loss:0.002, val_acc:0.984]
Epoch [116/120    avg_loss:0.003, val_acc:0.986]
Epoch [117/120    avg_loss:0.002, val_acc:0.986]
Epoch [118/120    avg_loss:0.004, val_acc:0.985]
Epoch [119/120    avg_loss:0.002, val_acc:0.985]
Epoch [120/120    avg_loss:0.003, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1259    4    1    0    0    0    0    0    2   12    7    0
     0    0    0]
 [   0    0    0  741    2    0    0    0    0    0    0    1    3    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    4    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  861    7    0    0
     1    1    0]
 [   0    0    0    0    0    0    0    0    0    0    5 2192   12    1
     0    0    0]
 [   0    0    0    4    0    0    0    0    0    0    0    1  528    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1135    4    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    84  263    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.14634146341463

F1 scores:
[       nan 0.975      0.98745098 0.98997996 0.99065421 0.99188876
 0.99771167 0.92592593 1.         1.         0.98738532 0.99051062
 0.97416974 0.99730458 0.96104996 0.85528455 0.99408284]

Kappa:
0.9788504436670729
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7a8ae9e748>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.383, val_acc:0.457]
Epoch [2/120    avg_loss:1.802, val_acc:0.441]
Epoch [3/120    avg_loss:1.663, val_acc:0.591]
Epoch [4/120    avg_loss:1.346, val_acc:0.640]
Epoch [5/120    avg_loss:1.154, val_acc:0.758]
Epoch [6/120    avg_loss:0.981, val_acc:0.753]
Epoch [7/120    avg_loss:0.839, val_acc:0.796]
Epoch [8/120    avg_loss:0.580, val_acc:0.844]
Epoch [9/120    avg_loss:0.448, val_acc:0.847]
Epoch [10/120    avg_loss:0.499, val_acc:0.849]
Epoch [11/120    avg_loss:0.450, val_acc:0.865]
Epoch [12/120    avg_loss:0.375, val_acc:0.829]
Epoch [13/120    avg_loss:0.250, val_acc:0.899]
Epoch [14/120    avg_loss:0.231, val_acc:0.924]
Epoch [15/120    avg_loss:0.215, val_acc:0.931]
Epoch [16/120    avg_loss:0.153, val_acc:0.929]
Epoch [17/120    avg_loss:0.140, val_acc:0.925]
Epoch [18/120    avg_loss:0.115, val_acc:0.955]
Epoch [19/120    avg_loss:0.099, val_acc:0.953]
Epoch [20/120    avg_loss:0.100, val_acc:0.927]
Epoch [21/120    avg_loss:0.084, val_acc:0.960]
Epoch [22/120    avg_loss:0.085, val_acc:0.961]
Epoch [23/120    avg_loss:0.080, val_acc:0.961]
Epoch [24/120    avg_loss:0.072, val_acc:0.955]
Epoch [25/120    avg_loss:0.149, val_acc:0.910]
Epoch [26/120    avg_loss:0.251, val_acc:0.940]
Epoch [27/120    avg_loss:0.101, val_acc:0.963]
Epoch [28/120    avg_loss:0.081, val_acc:0.961]
Epoch [29/120    avg_loss:0.076, val_acc:0.952]
Epoch [30/120    avg_loss:0.098, val_acc:0.958]
Epoch [31/120    avg_loss:0.099, val_acc:0.957]
Epoch [32/120    avg_loss:0.069, val_acc:0.954]
Epoch [33/120    avg_loss:0.075, val_acc:0.954]
Epoch [34/120    avg_loss:0.064, val_acc:0.961]
Epoch [35/120    avg_loss:0.059, val_acc:0.965]
Epoch [36/120    avg_loss:0.046, val_acc:0.977]
Epoch [37/120    avg_loss:0.026, val_acc:0.973]
Epoch [38/120    avg_loss:0.024, val_acc:0.973]
Epoch [39/120    avg_loss:0.024, val_acc:0.978]
Epoch [40/120    avg_loss:0.034, val_acc:0.938]
Epoch [41/120    avg_loss:0.032, val_acc:0.971]
Epoch [42/120    avg_loss:0.024, val_acc:0.973]
Epoch [43/120    avg_loss:0.025, val_acc:0.971]
Epoch [44/120    avg_loss:0.025, val_acc:0.969]
Epoch [45/120    avg_loss:0.021, val_acc:0.973]
Epoch [46/120    avg_loss:0.017, val_acc:0.981]
Epoch [47/120    avg_loss:0.017, val_acc:0.974]
Epoch [48/120    avg_loss:0.024, val_acc:0.975]
Epoch [49/120    avg_loss:0.009, val_acc:0.977]
Epoch [50/120    avg_loss:0.013, val_acc:0.977]
Epoch [51/120    avg_loss:0.015, val_acc:0.975]
Epoch [52/120    avg_loss:0.017, val_acc:0.972]
Epoch [53/120    avg_loss:0.013, val_acc:0.974]
Epoch [54/120    avg_loss:0.012, val_acc:0.980]
Epoch [55/120    avg_loss:0.011, val_acc:0.936]
Epoch [56/120    avg_loss:0.018, val_acc:0.971]
Epoch [57/120    avg_loss:0.015, val_acc:0.979]
Epoch [58/120    avg_loss:0.009, val_acc:0.977]
Epoch [59/120    avg_loss:0.006, val_acc:0.979]
Epoch [60/120    avg_loss:0.008, val_acc:0.982]
Epoch [61/120    avg_loss:0.004, val_acc:0.982]
Epoch [62/120    avg_loss:0.008, val_acc:0.983]
Epoch [63/120    avg_loss:0.005, val_acc:0.983]
Epoch [64/120    avg_loss:0.005, val_acc:0.983]
Epoch [65/120    avg_loss:0.006, val_acc:0.983]
Epoch [66/120    avg_loss:0.006, val_acc:0.984]
Epoch [67/120    avg_loss:0.005, val_acc:0.983]
Epoch [68/120    avg_loss:0.004, val_acc:0.983]
Epoch [69/120    avg_loss:0.008, val_acc:0.983]
Epoch [70/120    avg_loss:0.007, val_acc:0.983]
Epoch [71/120    avg_loss:0.006, val_acc:0.984]
Epoch [72/120    avg_loss:0.005, val_acc:0.983]
Epoch [73/120    avg_loss:0.005, val_acc:0.983]
Epoch [74/120    avg_loss:0.005, val_acc:0.982]
Epoch [75/120    avg_loss:0.006, val_acc:0.983]
Epoch [76/120    avg_loss:0.005, val_acc:0.982]
Epoch [77/120    avg_loss:0.005, val_acc:0.984]
Epoch [78/120    avg_loss:0.006, val_acc:0.982]
Epoch [79/120    avg_loss:0.004, val_acc:0.982]
Epoch [80/120    avg_loss:0.004, val_acc:0.982]
Epoch [81/120    avg_loss:0.005, val_acc:0.984]
Epoch [82/120    avg_loss:0.005, val_acc:0.983]
Epoch [83/120    avg_loss:0.004, val_acc:0.983]
Epoch [84/120    avg_loss:0.006, val_acc:0.983]
Epoch [85/120    avg_loss:0.005, val_acc:0.983]
Epoch [86/120    avg_loss:0.004, val_acc:0.983]
Epoch [87/120    avg_loss:0.010, val_acc:0.986]
Epoch [88/120    avg_loss:0.005, val_acc:0.984]
Epoch [89/120    avg_loss:0.005, val_acc:0.983]
Epoch [90/120    avg_loss:0.006, val_acc:0.986]
Epoch [91/120    avg_loss:0.005, val_acc:0.985]
Epoch [92/120    avg_loss:0.005, val_acc:0.984]
Epoch [93/120    avg_loss:0.004, val_acc:0.982]
Epoch [94/120    avg_loss:0.004, val_acc:0.982]
Epoch [95/120    avg_loss:0.004, val_acc:0.982]
Epoch [96/120    avg_loss:0.004, val_acc:0.982]
Epoch [97/120    avg_loss:0.004, val_acc:0.983]
Epoch [98/120    avg_loss:0.004, val_acc:0.982]
Epoch [99/120    avg_loss:0.004, val_acc:0.983]
Epoch [100/120    avg_loss:0.004, val_acc:0.982]
Epoch [101/120    avg_loss:0.003, val_acc:0.983]
Epoch [102/120    avg_loss:0.004, val_acc:0.983]
Epoch [103/120    avg_loss:0.005, val_acc:0.984]
Epoch [104/120    avg_loss:0.004, val_acc:0.984]
Epoch [105/120    avg_loss:0.004, val_acc:0.983]
Epoch [106/120    avg_loss:0.004, val_acc:0.983]
Epoch [107/120    avg_loss:0.004, val_acc:0.983]
Epoch [108/120    avg_loss:0.004, val_acc:0.983]
Epoch [109/120    avg_loss:0.004, val_acc:0.983]
Epoch [110/120    avg_loss:0.004, val_acc:0.983]
Epoch [111/120    avg_loss:0.007, val_acc:0.983]
Epoch [112/120    avg_loss:0.005, val_acc:0.983]
Epoch [113/120    avg_loss:0.004, val_acc:0.983]
Epoch [114/120    avg_loss:0.004, val_acc:0.983]
Epoch [115/120    avg_loss:0.004, val_acc:0.983]
Epoch [116/120    avg_loss:0.004, val_acc:0.983]
Epoch [117/120    avg_loss:0.004, val_acc:0.983]
Epoch [118/120    avg_loss:0.005, val_acc:0.983]
Epoch [119/120    avg_loss:0.006, val_acc:0.983]
Epoch [120/120    avg_loss:0.004, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1264    3    0    0    0    0    0    0    2    6   10    0
     0    0    0]
 [   0    0    0  736    2    0    0    0    0    0    0    4    4    0
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    3    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  860   11    0    0
     0    0    0]
 [   0    0   10    0    0    0    0    0    0    1   15 2170    7    1
     0    6    0]
 [   0    0    0    3    0    0    0    0    0    0    0    0  527    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1134    5    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    74  272    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.02710027100271

F1 scores:
[       nan 1.         0.98634413 0.98791946 0.99297424 0.99421965
 0.99771516 0.94339623 1.         0.97297297 0.98173516 0.98591549
 0.974122   0.99730458 0.96469587 0.86075949 0.98823529]

Kappa:
0.9775020511062225
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1bebe05748>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.443, val_acc:0.464]
Epoch [2/120    avg_loss:1.764, val_acc:0.585]
Epoch [3/120    avg_loss:1.553, val_acc:0.640]
Epoch [4/120    avg_loss:1.174, val_acc:0.639]
Epoch [5/120    avg_loss:0.938, val_acc:0.728]
Epoch [6/120    avg_loss:1.079, val_acc:0.738]
Epoch [7/120    avg_loss:0.819, val_acc:0.810]
Epoch [8/120    avg_loss:0.702, val_acc:0.783]
Epoch [9/120    avg_loss:0.688, val_acc:0.793]
Epoch [10/120    avg_loss:0.461, val_acc:0.846]
Epoch [11/120    avg_loss:0.474, val_acc:0.822]
Epoch [12/120    avg_loss:0.581, val_acc:0.840]
Epoch [13/120    avg_loss:0.343, val_acc:0.874]
Epoch [14/120    avg_loss:0.313, val_acc:0.882]
Epoch [15/120    avg_loss:0.447, val_acc:0.851]
Epoch [16/120    avg_loss:0.289, val_acc:0.905]
Epoch [17/120    avg_loss:0.216, val_acc:0.900]
Epoch [18/120    avg_loss:0.267, val_acc:0.900]
Epoch [19/120    avg_loss:0.221, val_acc:0.902]
Epoch [20/120    avg_loss:0.175, val_acc:0.910]
Epoch [21/120    avg_loss:0.153, val_acc:0.929]
Epoch [22/120    avg_loss:0.148, val_acc:0.931]
Epoch [23/120    avg_loss:0.126, val_acc:0.931]
Epoch [24/120    avg_loss:0.091, val_acc:0.923]
Epoch [25/120    avg_loss:0.103, val_acc:0.927]
Epoch [26/120    avg_loss:0.075, val_acc:0.936]
Epoch [27/120    avg_loss:0.096, val_acc:0.929]
Epoch [28/120    avg_loss:0.120, val_acc:0.925]
Epoch [29/120    avg_loss:0.093, val_acc:0.943]
Epoch [30/120    avg_loss:0.113, val_acc:0.939]
Epoch [31/120    avg_loss:0.081, val_acc:0.930]
Epoch [32/120    avg_loss:0.089, val_acc:0.960]
Epoch [33/120    avg_loss:0.057, val_acc:0.964]
Epoch [34/120    avg_loss:0.059, val_acc:0.947]
Epoch [35/120    avg_loss:0.045, val_acc:0.953]
Epoch [36/120    avg_loss:0.048, val_acc:0.961]
Epoch [37/120    avg_loss:0.042, val_acc:0.960]
Epoch [38/120    avg_loss:0.046, val_acc:0.956]
Epoch [39/120    avg_loss:0.030, val_acc:0.959]
Epoch [40/120    avg_loss:0.040, val_acc:0.964]
Epoch [41/120    avg_loss:0.034, val_acc:0.959]
Epoch [42/120    avg_loss:0.026, val_acc:0.958]
Epoch [43/120    avg_loss:0.017, val_acc:0.970]
Epoch [44/120    avg_loss:0.017, val_acc:0.970]
Epoch [45/120    avg_loss:0.018, val_acc:0.970]
Epoch [46/120    avg_loss:0.018, val_acc:0.965]
Epoch [47/120    avg_loss:0.027, val_acc:0.967]
Epoch [48/120    avg_loss:0.019, val_acc:0.975]
Epoch [49/120    avg_loss:0.024, val_acc:0.968]
Epoch [50/120    avg_loss:0.018, val_acc:0.966]
Epoch [51/120    avg_loss:0.019, val_acc:0.970]
Epoch [52/120    avg_loss:0.016, val_acc:0.972]
Epoch [53/120    avg_loss:0.017, val_acc:0.971]
Epoch [54/120    avg_loss:0.015, val_acc:0.975]
Epoch [55/120    avg_loss:0.012, val_acc:0.960]
Epoch [56/120    avg_loss:0.032, val_acc:0.928]
Epoch [57/120    avg_loss:0.035, val_acc:0.968]
Epoch [58/120    avg_loss:0.016, val_acc:0.969]
Epoch [59/120    avg_loss:0.016, val_acc:0.964]
Epoch [60/120    avg_loss:0.013, val_acc:0.974]
Epoch [61/120    avg_loss:0.013, val_acc:0.974]
Epoch [62/120    avg_loss:0.025, val_acc:0.960]
Epoch [63/120    avg_loss:0.047, val_acc:0.956]
Epoch [64/120    avg_loss:0.024, val_acc:0.961]
Epoch [65/120    avg_loss:0.015, val_acc:0.970]
Epoch [66/120    avg_loss:0.010, val_acc:0.973]
Epoch [67/120    avg_loss:0.013, val_acc:0.969]
Epoch [68/120    avg_loss:0.009, val_acc:0.970]
Epoch [69/120    avg_loss:0.007, val_acc:0.971]
Epoch [70/120    avg_loss:0.007, val_acc:0.971]
Epoch [71/120    avg_loss:0.005, val_acc:0.970]
Epoch [72/120    avg_loss:0.007, val_acc:0.971]
Epoch [73/120    avg_loss:0.006, val_acc:0.971]
Epoch [74/120    avg_loss:0.006, val_acc:0.969]
Epoch [75/120    avg_loss:0.006, val_acc:0.972]
Epoch [76/120    avg_loss:0.006, val_acc:0.972]
Epoch [77/120    avg_loss:0.005, val_acc:0.970]
Epoch [78/120    avg_loss:0.007, val_acc:0.970]
Epoch [79/120    avg_loss:0.007, val_acc:0.971]
Epoch [80/120    avg_loss:0.006, val_acc:0.970]
Epoch [81/120    avg_loss:0.005, val_acc:0.970]
Epoch [82/120    avg_loss:0.006, val_acc:0.970]
Epoch [83/120    avg_loss:0.006, val_acc:0.970]
Epoch [84/120    avg_loss:0.009, val_acc:0.970]
Epoch [85/120    avg_loss:0.005, val_acc:0.971]
Epoch [86/120    avg_loss:0.005, val_acc:0.971]
Epoch [87/120    avg_loss:0.004, val_acc:0.971]
Epoch [88/120    avg_loss:0.005, val_acc:0.971]
Epoch [89/120    avg_loss:0.004, val_acc:0.971]
Epoch [90/120    avg_loss:0.007, val_acc:0.971]
Epoch [91/120    avg_loss:0.006, val_acc:0.971]
Epoch [92/120    avg_loss:0.006, val_acc:0.971]
Epoch [93/120    avg_loss:0.004, val_acc:0.971]
Epoch [94/120    avg_loss:0.005, val_acc:0.971]
Epoch [95/120    avg_loss:0.004, val_acc:0.971]
Epoch [96/120    avg_loss:0.004, val_acc:0.971]
Epoch [97/120    avg_loss:0.005, val_acc:0.971]
Epoch [98/120    avg_loss:0.005, val_acc:0.971]
Epoch [99/120    avg_loss:0.007, val_acc:0.971]
Epoch [100/120    avg_loss:0.004, val_acc:0.971]
Epoch [101/120    avg_loss:0.006, val_acc:0.971]
Epoch [102/120    avg_loss:0.006, val_acc:0.971]
Epoch [103/120    avg_loss:0.004, val_acc:0.971]
Epoch [104/120    avg_loss:0.011, val_acc:0.971]
Epoch [105/120    avg_loss:0.006, val_acc:0.971]
Epoch [106/120    avg_loss:0.005, val_acc:0.971]
Epoch [107/120    avg_loss:0.005, val_acc:0.971]
Epoch [108/120    avg_loss:0.005, val_acc:0.971]
Epoch [109/120    avg_loss:0.004, val_acc:0.971]
Epoch [110/120    avg_loss:0.004, val_acc:0.971]
Epoch [111/120    avg_loss:0.006, val_acc:0.971]
Epoch [112/120    avg_loss:0.004, val_acc:0.971]
Epoch [113/120    avg_loss:0.005, val_acc:0.971]
Epoch [114/120    avg_loss:0.006, val_acc:0.971]
Epoch [115/120    avg_loss:0.005, val_acc:0.971]
Epoch [116/120    avg_loss:0.006, val_acc:0.971]
Epoch [117/120    avg_loss:0.006, val_acc:0.971]
Epoch [118/120    avg_loss:0.006, val_acc:0.971]
Epoch [119/120    avg_loss:0.005, val_acc:0.971]
Epoch [120/120    avg_loss:0.005, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1263    8    0    1    0    0    0    0    6    7    0    0
     0    0    0]
 [   0    0    0  739    0    0    0    0    0    1    0    0    7    0
     0    0    0]
 [   0    0    0    3  210    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    0    0    0    0    0  857    9    0    0
     0    0    0]
 [   0    0   19    0    0    0    0    0    0    0    7 2172   10    0
     0    2    0]
 [   0    0    0    3    0    0    0    0    0    0    3    1  526    0
     0    0    1]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1121   18    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    46  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
98.1029810298103

F1 scores:
[       nan 0.975      0.97982933 0.98467688 0.9929078  0.99424626
 0.99847561 1.         1.         0.94444444 0.9805492  0.98727273
 0.97227357 0.99728997 0.97056277 0.9011976  0.96341463]

Kappa:
0.9783718353850398
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f1342b710>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.430, val_acc:0.528]
Epoch [2/120    avg_loss:1.840, val_acc:0.496]
Epoch [3/120    avg_loss:1.608, val_acc:0.589]
Epoch [4/120    avg_loss:1.354, val_acc:0.664]
Epoch [5/120    avg_loss:1.181, val_acc:0.682]
Epoch [6/120    avg_loss:1.005, val_acc:0.746]
Epoch [7/120    avg_loss:0.796, val_acc:0.786]
Epoch [8/120    avg_loss:0.717, val_acc:0.724]
Epoch [9/120    avg_loss:0.649, val_acc:0.790]
Epoch [10/120    avg_loss:0.595, val_acc:0.830]
Epoch [11/120    avg_loss:0.557, val_acc:0.814]
Epoch [12/120    avg_loss:0.529, val_acc:0.848]
Epoch [13/120    avg_loss:0.418, val_acc:0.856]
Epoch [14/120    avg_loss:0.345, val_acc:0.885]
Epoch [15/120    avg_loss:0.292, val_acc:0.879]
Epoch [16/120    avg_loss:0.229, val_acc:0.903]
Epoch [17/120    avg_loss:0.212, val_acc:0.916]
Epoch [18/120    avg_loss:0.145, val_acc:0.931]
Epoch [19/120    avg_loss:0.123, val_acc:0.927]
Epoch [20/120    avg_loss:0.138, val_acc:0.924]
Epoch [21/120    avg_loss:0.154, val_acc:0.936]
Epoch [22/120    avg_loss:0.103, val_acc:0.943]
Epoch [23/120    avg_loss:0.107, val_acc:0.942]
Epoch [24/120    avg_loss:0.092, val_acc:0.952]
Epoch [25/120    avg_loss:0.101, val_acc:0.939]
Epoch [26/120    avg_loss:0.081, val_acc:0.931]
Epoch [27/120    avg_loss:0.089, val_acc:0.955]
Epoch [28/120    avg_loss:0.075, val_acc:0.948]
Epoch [29/120    avg_loss:0.059, val_acc:0.954]
Epoch [30/120    avg_loss:0.057, val_acc:0.959]
Epoch [31/120    avg_loss:0.059, val_acc:0.959]
Epoch [32/120    avg_loss:0.062, val_acc:0.940]
Epoch [33/120    avg_loss:0.050, val_acc:0.951]
Epoch [34/120    avg_loss:0.060, val_acc:0.951]
Epoch [35/120    avg_loss:0.052, val_acc:0.955]
Epoch [36/120    avg_loss:0.044, val_acc:0.949]
Epoch [37/120    avg_loss:0.071, val_acc:0.961]
Epoch [38/120    avg_loss:0.040, val_acc:0.955]
Epoch [39/120    avg_loss:0.040, val_acc:0.956]
Epoch [40/120    avg_loss:0.036, val_acc:0.961]
Epoch [41/120    avg_loss:0.035, val_acc:0.963]
Epoch [42/120    avg_loss:0.053, val_acc:0.963]
Epoch [43/120    avg_loss:0.094, val_acc:0.922]
Epoch [44/120    avg_loss:0.222, val_acc:0.939]
Epoch [45/120    avg_loss:0.081, val_acc:0.941]
Epoch [46/120    avg_loss:0.059, val_acc:0.955]
Epoch [47/120    avg_loss:0.042, val_acc:0.948]
Epoch [48/120    avg_loss:0.042, val_acc:0.966]
Epoch [49/120    avg_loss:0.035, val_acc:0.959]
Epoch [50/120    avg_loss:0.034, val_acc:0.956]
Epoch [51/120    avg_loss:0.037, val_acc:0.968]
Epoch [52/120    avg_loss:0.062, val_acc:0.945]
Epoch [53/120    avg_loss:0.057, val_acc:0.952]
Epoch [54/120    avg_loss:0.040, val_acc:0.952]
Epoch [55/120    avg_loss:0.032, val_acc:0.968]
Epoch [56/120    avg_loss:0.023, val_acc:0.959]
Epoch [57/120    avg_loss:0.028, val_acc:0.958]
Epoch [58/120    avg_loss:0.026, val_acc:0.965]
Epoch [59/120    avg_loss:0.018, val_acc:0.971]
Epoch [60/120    avg_loss:0.019, val_acc:0.973]
Epoch [61/120    avg_loss:0.016, val_acc:0.969]
Epoch [62/120    avg_loss:0.019, val_acc:0.960]
Epoch [63/120    avg_loss:0.019, val_acc:0.966]
Epoch [64/120    avg_loss:0.017, val_acc:0.971]
Epoch [65/120    avg_loss:0.011, val_acc:0.969]
Epoch [66/120    avg_loss:0.012, val_acc:0.973]
Epoch [67/120    avg_loss:0.010, val_acc:0.975]
Epoch [68/120    avg_loss:0.034, val_acc:0.968]
Epoch [69/120    avg_loss:0.030, val_acc:0.973]
Epoch [70/120    avg_loss:0.025, val_acc:0.976]
Epoch [71/120    avg_loss:0.016, val_acc:0.977]
Epoch [72/120    avg_loss:0.010, val_acc:0.976]
Epoch [73/120    avg_loss:0.016, val_acc:0.968]
Epoch [74/120    avg_loss:0.016, val_acc:0.975]
Epoch [75/120    avg_loss:0.017, val_acc:0.967]
Epoch [76/120    avg_loss:0.008, val_acc:0.973]
Epoch [77/120    avg_loss:0.019, val_acc:0.968]
Epoch [78/120    avg_loss:0.014, val_acc:0.974]
Epoch [79/120    avg_loss:0.015, val_acc:0.971]
Epoch [80/120    avg_loss:0.016, val_acc:0.968]
Epoch [81/120    avg_loss:0.007, val_acc:0.978]
Epoch [82/120    avg_loss:0.006, val_acc:0.979]
Epoch [83/120    avg_loss:0.011, val_acc:0.976]
Epoch [84/120    avg_loss:0.011, val_acc:0.972]
Epoch [85/120    avg_loss:0.007, val_acc:0.973]
Epoch [86/120    avg_loss:0.015, val_acc:0.969]
Epoch [87/120    avg_loss:0.009, val_acc:0.974]
Epoch [88/120    avg_loss:0.009, val_acc:0.979]
Epoch [89/120    avg_loss:0.011, val_acc:0.956]
Epoch [90/120    avg_loss:0.007, val_acc:0.982]
Epoch [91/120    avg_loss:0.005, val_acc:0.983]
Epoch [92/120    avg_loss:0.008, val_acc:0.978]
Epoch [93/120    avg_loss:0.119, val_acc:0.953]
Epoch [94/120    avg_loss:0.096, val_acc:0.941]
Epoch [95/120    avg_loss:0.068, val_acc:0.973]
Epoch [96/120    avg_loss:0.045, val_acc:0.963]
Epoch [97/120    avg_loss:0.028, val_acc:0.968]
Epoch [98/120    avg_loss:0.024, val_acc:0.978]
Epoch [99/120    avg_loss:0.011, val_acc:0.974]
Epoch [100/120    avg_loss:0.010, val_acc:0.978]
Epoch [101/120    avg_loss:0.009, val_acc:0.976]
Epoch [102/120    avg_loss:0.007, val_acc:0.982]
Epoch [103/120    avg_loss:0.006, val_acc:0.979]
Epoch [104/120    avg_loss:0.006, val_acc:0.980]
Epoch [105/120    avg_loss:0.006, val_acc:0.981]
Epoch [106/120    avg_loss:0.005, val_acc:0.982]
Epoch [107/120    avg_loss:0.004, val_acc:0.982]
Epoch [108/120    avg_loss:0.006, val_acc:0.982]
Epoch [109/120    avg_loss:0.005, val_acc:0.981]
Epoch [110/120    avg_loss:0.005, val_acc:0.980]
Epoch [111/120    avg_loss:0.005, val_acc:0.980]
Epoch [112/120    avg_loss:0.005, val_acc:0.982]
Epoch [113/120    avg_loss:0.005, val_acc:0.982]
Epoch [114/120    avg_loss:0.003, val_acc:0.982]
Epoch [115/120    avg_loss:0.006, val_acc:0.982]
Epoch [116/120    avg_loss:0.004, val_acc:0.982]
Epoch [117/120    avg_loss:0.004, val_acc:0.982]
Epoch [118/120    avg_loss:0.004, val_acc:0.982]
Epoch [119/120    avg_loss:0.005, val_acc:0.982]
Epoch [120/120    avg_loss:0.005, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    1    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1277    0    0    0    0    0    0    0    3    4    1    0
     0    0    0]
 [   0    0    0  717    9    8    0    0    0    2    3    0    4    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    1    0    2    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    2  861    7    0    0
     0    1    0]
 [   0    0    3    0    0    0    1    0    0    3    0 2203    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    6    0  522    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0    0
  1133    5    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
    28  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.4390243902439

F1 scores:
[       nan 0.93975904 0.99377432 0.9795082  0.97931034 0.97594502
 0.98343373 0.98039216 0.99649942 0.8        0.98512586 0.99503162
 0.98028169 0.98930481 0.982228   0.92073171 0.96385542]

Kappa:
0.9821978100706549
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5ef079c6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.474, val_acc:0.489]
Epoch [2/120    avg_loss:1.875, val_acc:0.581]
Epoch [3/120    avg_loss:1.540, val_acc:0.651]
Epoch [4/120    avg_loss:1.329, val_acc:0.622]
Epoch [5/120    avg_loss:1.171, val_acc:0.700]
Epoch [6/120    avg_loss:0.961, val_acc:0.707]
Epoch [7/120    avg_loss:0.825, val_acc:0.770]
Epoch [8/120    avg_loss:0.647, val_acc:0.778]
Epoch [9/120    avg_loss:0.523, val_acc:0.836]
Epoch [10/120    avg_loss:0.571, val_acc:0.808]
Epoch [11/120    avg_loss:0.449, val_acc:0.843]
Epoch [12/120    avg_loss:0.349, val_acc:0.874]
Epoch [13/120    avg_loss:0.305, val_acc:0.881]
Epoch [14/120    avg_loss:0.256, val_acc:0.899]
Epoch [15/120    avg_loss:0.399, val_acc:0.865]
Epoch [16/120    avg_loss:0.317, val_acc:0.884]
Epoch [17/120    avg_loss:0.302, val_acc:0.909]
Epoch [18/120    avg_loss:0.236, val_acc:0.921]
Epoch [19/120    avg_loss:0.177, val_acc:0.900]
Epoch [20/120    avg_loss:0.155, val_acc:0.934]
Epoch [21/120    avg_loss:0.098, val_acc:0.921]
Epoch [22/120    avg_loss:0.116, val_acc:0.939]
Epoch [23/120    avg_loss:0.100, val_acc:0.939]
Epoch [24/120    avg_loss:0.081, val_acc:0.952]
Epoch [25/120    avg_loss:0.088, val_acc:0.930]
Epoch [26/120    avg_loss:0.107, val_acc:0.939]
Epoch [27/120    avg_loss:0.105, val_acc:0.951]
Epoch [28/120    avg_loss:0.102, val_acc:0.932]
Epoch [29/120    avg_loss:0.063, val_acc:0.942]
Epoch [30/120    avg_loss:0.062, val_acc:0.952]
Epoch [31/120    avg_loss:0.049, val_acc:0.964]
Epoch [32/120    avg_loss:0.052, val_acc:0.963]
Epoch [33/120    avg_loss:0.052, val_acc:0.959]
Epoch [34/120    avg_loss:0.055, val_acc:0.947]
Epoch [35/120    avg_loss:0.043, val_acc:0.971]
Epoch [36/120    avg_loss:0.050, val_acc:0.938]
Epoch [37/120    avg_loss:0.053, val_acc:0.952]
Epoch [38/120    avg_loss:0.035, val_acc:0.980]
Epoch [39/120    avg_loss:0.028, val_acc:0.973]
Epoch [40/120    avg_loss:0.025, val_acc:0.975]
Epoch [41/120    avg_loss:0.025, val_acc:0.973]
Epoch [42/120    avg_loss:0.023, val_acc:0.974]
Epoch [43/120    avg_loss:0.039, val_acc:0.960]
Epoch [44/120    avg_loss:0.130, val_acc:0.960]
Epoch [45/120    avg_loss:0.076, val_acc:0.954]
Epoch [46/120    avg_loss:0.054, val_acc:0.960]
Epoch [47/120    avg_loss:0.046, val_acc:0.972]
Epoch [48/120    avg_loss:0.050, val_acc:0.971]
Epoch [49/120    avg_loss:0.029, val_acc:0.973]
Epoch [50/120    avg_loss:0.030, val_acc:0.972]
Epoch [51/120    avg_loss:0.026, val_acc:0.969]
Epoch [52/120    avg_loss:0.015, val_acc:0.973]
Epoch [53/120    avg_loss:0.016, val_acc:0.974]
Epoch [54/120    avg_loss:0.016, val_acc:0.975]
Epoch [55/120    avg_loss:0.017, val_acc:0.975]
Epoch [56/120    avg_loss:0.015, val_acc:0.977]
Epoch [57/120    avg_loss:0.014, val_acc:0.977]
Epoch [58/120    avg_loss:0.013, val_acc:0.978]
Epoch [59/120    avg_loss:0.015, val_acc:0.978]
Epoch [60/120    avg_loss:0.013, val_acc:0.978]
Epoch [61/120    avg_loss:0.014, val_acc:0.978]
Epoch [62/120    avg_loss:0.013, val_acc:0.978]
Epoch [63/120    avg_loss:0.013, val_acc:0.979]
Epoch [64/120    avg_loss:0.012, val_acc:0.979]
Epoch [65/120    avg_loss:0.013, val_acc:0.979]
Epoch [66/120    avg_loss:0.014, val_acc:0.979]
Epoch [67/120    avg_loss:0.013, val_acc:0.979]
Epoch [68/120    avg_loss:0.013, val_acc:0.979]
Epoch [69/120    avg_loss:0.012, val_acc:0.979]
Epoch [70/120    avg_loss:0.012, val_acc:0.979]
Epoch [71/120    avg_loss:0.013, val_acc:0.979]
Epoch [72/120    avg_loss:0.012, val_acc:0.979]
Epoch [73/120    avg_loss:0.011, val_acc:0.979]
Epoch [74/120    avg_loss:0.013, val_acc:0.979]
Epoch [75/120    avg_loss:0.013, val_acc:0.979]
Epoch [76/120    avg_loss:0.013, val_acc:0.979]
Epoch [77/120    avg_loss:0.015, val_acc:0.979]
Epoch [78/120    avg_loss:0.012, val_acc:0.979]
Epoch [79/120    avg_loss:0.010, val_acc:0.979]
Epoch [80/120    avg_loss:0.014, val_acc:0.979]
Epoch [81/120    avg_loss:0.013, val_acc:0.979]
Epoch [82/120    avg_loss:0.012, val_acc:0.979]
Epoch [83/120    avg_loss:0.012, val_acc:0.979]
Epoch [84/120    avg_loss:0.013, val_acc:0.979]
Epoch [85/120    avg_loss:0.013, val_acc:0.979]
Epoch [86/120    avg_loss:0.013, val_acc:0.979]
Epoch [87/120    avg_loss:0.013, val_acc:0.979]
Epoch [88/120    avg_loss:0.014, val_acc:0.979]
Epoch [89/120    avg_loss:0.011, val_acc:0.979]
Epoch [90/120    avg_loss:0.014, val_acc:0.979]
Epoch [91/120    avg_loss:0.012, val_acc:0.979]
Epoch [92/120    avg_loss:0.013, val_acc:0.979]
Epoch [93/120    avg_loss:0.014, val_acc:0.979]
Epoch [94/120    avg_loss:0.010, val_acc:0.979]
Epoch [95/120    avg_loss:0.015, val_acc:0.979]
Epoch [96/120    avg_loss:0.012, val_acc:0.979]
Epoch [97/120    avg_loss:0.011, val_acc:0.979]
Epoch [98/120    avg_loss:0.017, val_acc:0.979]
Epoch [99/120    avg_loss:0.014, val_acc:0.979]
Epoch [100/120    avg_loss:0.018, val_acc:0.979]
Epoch [101/120    avg_loss:0.013, val_acc:0.979]
Epoch [102/120    avg_loss:0.012, val_acc:0.979]
Epoch [103/120    avg_loss:0.012, val_acc:0.979]
Epoch [104/120    avg_loss:0.012, val_acc:0.979]
Epoch [105/120    avg_loss:0.014, val_acc:0.979]
Epoch [106/120    avg_loss:0.013, val_acc:0.979]
Epoch [107/120    avg_loss:0.013, val_acc:0.979]
Epoch [108/120    avg_loss:0.010, val_acc:0.979]
Epoch [109/120    avg_loss:0.012, val_acc:0.979]
Epoch [110/120    avg_loss:0.011, val_acc:0.979]
Epoch [111/120    avg_loss:0.014, val_acc:0.979]
Epoch [112/120    avg_loss:0.011, val_acc:0.979]
Epoch [113/120    avg_loss:0.014, val_acc:0.979]
Epoch [114/120    avg_loss:0.011, val_acc:0.979]
Epoch [115/120    avg_loss:0.011, val_acc:0.979]
Epoch [116/120    avg_loss:0.012, val_acc:0.979]
Epoch [117/120    avg_loss:0.011, val_acc:0.979]
Epoch [118/120    avg_loss:0.012, val_acc:0.979]
Epoch [119/120    avg_loss:0.014, val_acc:0.979]
Epoch [120/120    avg_loss:0.012, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1260    0    0    1    0    0    0    2    7    7    8    0
     0    0    0]
 [   0    0    0  719    0    0    1    0    0    3    1    7   16    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    2    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  649    0    0    4    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    2    4    0    0    0  848   15    1    0
     1    1    0]
 [   0    1    2    0    0    0    2    0    0    1    5 2197    0    2
     0    0    0]
 [   0    0    0    5    0    2    0    0    0    0    8    0  514    0
     0    2    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1129    9    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    29  309    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.03794037940379

F1 scores:
[       nan 0.96296296 0.98823529 0.97756628 1.         0.9862069
 0.98184569 0.98039216 0.99649942 0.75       0.97136312 0.99008562
 0.95450325 0.99462366 0.98046027 0.9251497  0.97647059]

Kappa:
0.9776240353901866
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4adf11c710>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.388, val_acc:0.446]
Epoch [2/120    avg_loss:1.837, val_acc:0.545]
Epoch [3/120    avg_loss:1.536, val_acc:0.614]
Epoch [4/120    avg_loss:1.260, val_acc:0.676]
Epoch [5/120    avg_loss:0.990, val_acc:0.678]
Epoch [6/120    avg_loss:0.882, val_acc:0.767]
Epoch [7/120    avg_loss:0.708, val_acc:0.772]
Epoch [8/120    avg_loss:0.751, val_acc:0.778]
Epoch [9/120    avg_loss:0.555, val_acc:0.784]
Epoch [10/120    avg_loss:0.451, val_acc:0.830]
Epoch [11/120    avg_loss:0.331, val_acc:0.833]
Epoch [12/120    avg_loss:0.504, val_acc:0.814]
Epoch [13/120    avg_loss:0.350, val_acc:0.849]
Epoch [14/120    avg_loss:0.279, val_acc:0.861]
Epoch [15/120    avg_loss:0.230, val_acc:0.904]
Epoch [16/120    avg_loss:0.180, val_acc:0.905]
Epoch [17/120    avg_loss:0.160, val_acc:0.911]
Epoch [18/120    avg_loss:0.133, val_acc:0.879]
Epoch [19/120    avg_loss:0.183, val_acc:0.900]
Epoch [20/120    avg_loss:0.153, val_acc:0.916]
Epoch [21/120    avg_loss:0.101, val_acc:0.923]
Epoch [22/120    avg_loss:0.114, val_acc:0.939]
Epoch [23/120    avg_loss:0.101, val_acc:0.928]
Epoch [24/120    avg_loss:0.113, val_acc:0.933]
Epoch [25/120    avg_loss:0.195, val_acc:0.881]
Epoch [26/120    avg_loss:0.163, val_acc:0.920]
Epoch [27/120    avg_loss:0.123, val_acc:0.921]
Epoch [28/120    avg_loss:0.083, val_acc:0.924]
Epoch [29/120    avg_loss:0.104, val_acc:0.942]
Epoch [30/120    avg_loss:0.079, val_acc:0.950]
Epoch [31/120    avg_loss:0.060, val_acc:0.950]
Epoch [32/120    avg_loss:0.059, val_acc:0.955]
Epoch [33/120    avg_loss:0.063, val_acc:0.936]
Epoch [34/120    avg_loss:0.052, val_acc:0.947]
Epoch [35/120    avg_loss:0.054, val_acc:0.955]
Epoch [36/120    avg_loss:0.050, val_acc:0.935]
Epoch [37/120    avg_loss:0.050, val_acc:0.947]
Epoch [38/120    avg_loss:0.042, val_acc:0.959]
Epoch [39/120    avg_loss:0.025, val_acc:0.958]
Epoch [40/120    avg_loss:0.036, val_acc:0.958]
Epoch [41/120    avg_loss:0.036, val_acc:0.965]
Epoch [42/120    avg_loss:0.034, val_acc:0.963]
Epoch [43/120    avg_loss:0.027, val_acc:0.971]
Epoch [44/120    avg_loss:0.024, val_acc:0.973]
Epoch [45/120    avg_loss:0.025, val_acc:0.966]
Epoch [46/120    avg_loss:0.026, val_acc:0.965]
Epoch [47/120    avg_loss:0.027, val_acc:0.967]
Epoch [48/120    avg_loss:0.022, val_acc:0.976]
Epoch [49/120    avg_loss:0.032, val_acc:0.942]
Epoch [50/120    avg_loss:0.035, val_acc:0.949]
Epoch [51/120    avg_loss:0.021, val_acc:0.976]
Epoch [52/120    avg_loss:0.015, val_acc:0.978]
Epoch [53/120    avg_loss:0.021, val_acc:0.975]
Epoch [54/120    avg_loss:0.014, val_acc:0.976]
Epoch [55/120    avg_loss:0.016, val_acc:0.979]
Epoch [56/120    avg_loss:0.016, val_acc:0.973]
Epoch [57/120    avg_loss:0.027, val_acc:0.961]
Epoch [58/120    avg_loss:0.018, val_acc:0.973]
Epoch [59/120    avg_loss:0.014, val_acc:0.977]
Epoch [60/120    avg_loss:0.047, val_acc:0.969]
Epoch [61/120    avg_loss:0.019, val_acc:0.963]
Epoch [62/120    avg_loss:0.021, val_acc:0.976]
Epoch [63/120    avg_loss:0.010, val_acc:0.973]
Epoch [64/120    avg_loss:0.013, val_acc:0.980]
Epoch [65/120    avg_loss:0.017, val_acc:0.975]
Epoch [66/120    avg_loss:0.016, val_acc:0.975]
Epoch [67/120    avg_loss:0.026, val_acc:0.963]
Epoch [68/120    avg_loss:0.062, val_acc:0.971]
Epoch [69/120    avg_loss:0.029, val_acc:0.973]
Epoch [70/120    avg_loss:0.019, val_acc:0.977]
Epoch [71/120    avg_loss:0.026, val_acc:0.970]
Epoch [72/120    avg_loss:0.018, val_acc:0.984]
Epoch [73/120    avg_loss:0.010, val_acc:0.978]
Epoch [74/120    avg_loss:0.012, val_acc:0.981]
Epoch [75/120    avg_loss:0.009, val_acc:0.988]
Epoch [76/120    avg_loss:0.008, val_acc:0.980]
Epoch [77/120    avg_loss:0.008, val_acc:0.971]
Epoch [78/120    avg_loss:0.009, val_acc:0.980]
Epoch [79/120    avg_loss:0.007, val_acc:0.981]
Epoch [80/120    avg_loss:0.006, val_acc:0.982]
Epoch [81/120    avg_loss:0.006, val_acc:0.980]
Epoch [82/120    avg_loss:0.007, val_acc:0.974]
Epoch [83/120    avg_loss:0.016, val_acc:0.975]
Epoch [84/120    avg_loss:0.016, val_acc:0.979]
Epoch [85/120    avg_loss:0.010, val_acc:0.981]
Epoch [86/120    avg_loss:0.008, val_acc:0.977]
Epoch [87/120    avg_loss:0.006, val_acc:0.984]
Epoch [88/120    avg_loss:0.008, val_acc:0.984]
Epoch [89/120    avg_loss:0.004, val_acc:0.984]
Epoch [90/120    avg_loss:0.006, val_acc:0.984]
Epoch [91/120    avg_loss:0.004, val_acc:0.984]
Epoch [92/120    avg_loss:0.005, val_acc:0.984]
Epoch [93/120    avg_loss:0.004, val_acc:0.984]
Epoch [94/120    avg_loss:0.005, val_acc:0.983]
Epoch [95/120    avg_loss:0.005, val_acc:0.983]
Epoch [96/120    avg_loss:0.004, val_acc:0.983]
Epoch [97/120    avg_loss:0.004, val_acc:0.983]
Epoch [98/120    avg_loss:0.003, val_acc:0.983]
Epoch [99/120    avg_loss:0.004, val_acc:0.983]
Epoch [100/120    avg_loss:0.004, val_acc:0.983]
Epoch [101/120    avg_loss:0.004, val_acc:0.984]
Epoch [102/120    avg_loss:0.004, val_acc:0.984]
Epoch [103/120    avg_loss:0.003, val_acc:0.984]
Epoch [104/120    avg_loss:0.005, val_acc:0.984]
Epoch [105/120    avg_loss:0.003, val_acc:0.984]
Epoch [106/120    avg_loss:0.003, val_acc:0.984]
Epoch [107/120    avg_loss:0.004, val_acc:0.984]
Epoch [108/120    avg_loss:0.004, val_acc:0.984]
Epoch [109/120    avg_loss:0.003, val_acc:0.984]
Epoch [110/120    avg_loss:0.005, val_acc:0.984]
Epoch [111/120    avg_loss:0.003, val_acc:0.984]
Epoch [112/120    avg_loss:0.005, val_acc:0.984]
Epoch [113/120    avg_loss:0.004, val_acc:0.984]
Epoch [114/120    avg_loss:0.004, val_acc:0.984]
Epoch [115/120    avg_loss:0.004, val_acc:0.984]
Epoch [116/120    avg_loss:0.004, val_acc:0.984]
Epoch [117/120    avg_loss:0.004, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.984]
Epoch [119/120    avg_loss:0.004, val_acc:0.984]
Epoch [120/120    avg_loss:0.004, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1277    0    0    0    1    0    0    0    5    2    0    0
     0    0    0]
 [   0    0    0  729    1    1    0    0    0    5    1    0    9    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    3    3    0    0    0  865    3    0    0
     0    1    0]
 [   0    0    4    0    0    0    0    0    0    0    4 2196    4    1
     1    0    0]
 [   0    0    0    3    2    3    0    0    0    0    3    0  520    0
     2    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1132    7    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
    29  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.57994579945799

F1 scores:
[       nan 0.98765432 0.99493572 0.98580122 0.99300699 0.99086758
 0.97764531 1.         0.997669   0.85714286 0.98687963 0.99569259
 0.97196262 0.99462366 0.98263889 0.90432099 0.98809524]

Kappa:
0.9838093082098794
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f35a1a14748>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.397, val_acc:0.515]
Epoch [2/120    avg_loss:1.814, val_acc:0.604]
Epoch [3/120    avg_loss:1.492, val_acc:0.636]
Epoch [4/120    avg_loss:1.238, val_acc:0.680]
Epoch [5/120    avg_loss:0.992, val_acc:0.655]
Epoch [6/120    avg_loss:0.916, val_acc:0.769]
Epoch [7/120    avg_loss:0.754, val_acc:0.775]
Epoch [8/120    avg_loss:0.669, val_acc:0.805]
Epoch [9/120    avg_loss:0.636, val_acc:0.829]
Epoch [10/120    avg_loss:0.526, val_acc:0.847]
Epoch [11/120    avg_loss:0.377, val_acc:0.876]
Epoch [12/120    avg_loss:0.299, val_acc:0.874]
Epoch [13/120    avg_loss:0.318, val_acc:0.896]
Epoch [14/120    avg_loss:0.278, val_acc:0.902]
Epoch [15/120    avg_loss:0.257, val_acc:0.894]
Epoch [16/120    avg_loss:0.236, val_acc:0.877]
Epoch [17/120    avg_loss:0.277, val_acc:0.916]
Epoch [18/120    avg_loss:0.204, val_acc:0.918]
Epoch [19/120    avg_loss:0.119, val_acc:0.924]
Epoch [20/120    avg_loss:0.151, val_acc:0.920]
Epoch [21/120    avg_loss:0.102, val_acc:0.942]
Epoch [22/120    avg_loss:0.086, val_acc:0.945]
Epoch [23/120    avg_loss:0.081, val_acc:0.945]
Epoch [24/120    avg_loss:0.102, val_acc:0.938]
Epoch [25/120    avg_loss:0.082, val_acc:0.942]
Epoch [26/120    avg_loss:0.071, val_acc:0.948]
Epoch [27/120    avg_loss:0.064, val_acc:0.961]
Epoch [28/120    avg_loss:0.081, val_acc:0.945]
Epoch [29/120    avg_loss:0.075, val_acc:0.953]
Epoch [30/120    avg_loss:0.052, val_acc:0.961]
Epoch [31/120    avg_loss:0.054, val_acc:0.960]
Epoch [32/120    avg_loss:0.053, val_acc:0.950]
Epoch [33/120    avg_loss:0.050, val_acc:0.964]
Epoch [34/120    avg_loss:0.039, val_acc:0.961]
Epoch [35/120    avg_loss:0.038, val_acc:0.966]
Epoch [36/120    avg_loss:0.041, val_acc:0.965]
Epoch [37/120    avg_loss:0.041, val_acc:0.973]
Epoch [38/120    avg_loss:0.035, val_acc:0.971]
Epoch [39/120    avg_loss:0.029, val_acc:0.972]
Epoch [40/120    avg_loss:0.065, val_acc:0.956]
Epoch [41/120    avg_loss:0.048, val_acc:0.961]
Epoch [42/120    avg_loss:0.037, val_acc:0.966]
Epoch [43/120    avg_loss:0.026, val_acc:0.974]
Epoch [44/120    avg_loss:0.032, val_acc:0.971]
Epoch [45/120    avg_loss:0.022, val_acc:0.982]
Epoch [46/120    avg_loss:0.017, val_acc:0.980]
Epoch [47/120    avg_loss:0.021, val_acc:0.976]
Epoch [48/120    avg_loss:0.023, val_acc:0.972]
Epoch [49/120    avg_loss:0.018, val_acc:0.970]
Epoch [50/120    avg_loss:0.018, val_acc:0.974]
Epoch [51/120    avg_loss:0.019, val_acc:0.972]
Epoch [52/120    avg_loss:0.031, val_acc:0.971]
Epoch [53/120    avg_loss:0.030, val_acc:0.979]
Epoch [54/120    avg_loss:0.033, val_acc:0.966]
Epoch [55/120    avg_loss:0.022, val_acc:0.973]
Epoch [56/120    avg_loss:0.073, val_acc:0.939]
Epoch [57/120    avg_loss:0.083, val_acc:0.974]
Epoch [58/120    avg_loss:0.056, val_acc:0.948]
Epoch [59/120    avg_loss:0.070, val_acc:0.970]
Epoch [60/120    avg_loss:0.022, val_acc:0.976]
Epoch [61/120    avg_loss:0.022, val_acc:0.977]
Epoch [62/120    avg_loss:0.013, val_acc:0.976]
Epoch [63/120    avg_loss:0.013, val_acc:0.976]
Epoch [64/120    avg_loss:0.016, val_acc:0.977]
Epoch [65/120    avg_loss:0.013, val_acc:0.977]
Epoch [66/120    avg_loss:0.013, val_acc:0.977]
Epoch [67/120    avg_loss:0.013, val_acc:0.977]
Epoch [68/120    avg_loss:0.014, val_acc:0.977]
Epoch [69/120    avg_loss:0.014, val_acc:0.978]
Epoch [70/120    avg_loss:0.014, val_acc:0.979]
Epoch [71/120    avg_loss:0.011, val_acc:0.978]
Epoch [72/120    avg_loss:0.012, val_acc:0.979]
Epoch [73/120    avg_loss:0.011, val_acc:0.979]
Epoch [74/120    avg_loss:0.010, val_acc:0.979]
Epoch [75/120    avg_loss:0.011, val_acc:0.979]
Epoch [76/120    avg_loss:0.013, val_acc:0.979]
Epoch [77/120    avg_loss:0.012, val_acc:0.978]
Epoch [78/120    avg_loss:0.010, val_acc:0.979]
Epoch [79/120    avg_loss:0.013, val_acc:0.978]
Epoch [80/120    avg_loss:0.014, val_acc:0.979]
Epoch [81/120    avg_loss:0.013, val_acc:0.979]
Epoch [82/120    avg_loss:0.013, val_acc:0.979]
Epoch [83/120    avg_loss:0.012, val_acc:0.979]
Epoch [84/120    avg_loss:0.012, val_acc:0.979]
Epoch [85/120    avg_loss:0.012, val_acc:0.979]
Epoch [86/120    avg_loss:0.014, val_acc:0.979]
Epoch [87/120    avg_loss:0.012, val_acc:0.979]
Epoch [88/120    avg_loss:0.015, val_acc:0.979]
Epoch [89/120    avg_loss:0.017, val_acc:0.979]
Epoch [90/120    avg_loss:0.014, val_acc:0.979]
Epoch [91/120    avg_loss:0.012, val_acc:0.979]
Epoch [92/120    avg_loss:0.013, val_acc:0.979]
Epoch [93/120    avg_loss:0.011, val_acc:0.979]
Epoch [94/120    avg_loss:0.010, val_acc:0.979]
Epoch [95/120    avg_loss:0.013, val_acc:0.979]
Epoch [96/120    avg_loss:0.012, val_acc:0.979]
Epoch [97/120    avg_loss:0.013, val_acc:0.979]
Epoch [98/120    avg_loss:0.013, val_acc:0.979]
Epoch [99/120    avg_loss:0.013, val_acc:0.979]
Epoch [100/120    avg_loss:0.011, val_acc:0.979]
Epoch [101/120    avg_loss:0.014, val_acc:0.979]
Epoch [102/120    avg_loss:0.011, val_acc:0.979]
Epoch [103/120    avg_loss:0.011, val_acc:0.979]
Epoch [104/120    avg_loss:0.013, val_acc:0.979]
Epoch [105/120    avg_loss:0.010, val_acc:0.979]
Epoch [106/120    avg_loss:0.011, val_acc:0.979]
Epoch [107/120    avg_loss:0.012, val_acc:0.979]
Epoch [108/120    avg_loss:0.011, val_acc:0.979]
Epoch [109/120    avg_loss:0.010, val_acc:0.979]
Epoch [110/120    avg_loss:0.012, val_acc:0.979]
Epoch [111/120    avg_loss:0.011, val_acc:0.979]
Epoch [112/120    avg_loss:0.013, val_acc:0.979]
Epoch [113/120    avg_loss:0.010, val_acc:0.979]
Epoch [114/120    avg_loss:0.011, val_acc:0.979]
Epoch [115/120    avg_loss:0.011, val_acc:0.979]
Epoch [116/120    avg_loss:0.012, val_acc:0.979]
Epoch [117/120    avg_loss:0.015, val_acc:0.979]
Epoch [118/120    avg_loss:0.013, val_acc:0.979]
Epoch [119/120    avg_loss:0.011, val_acc:0.979]
Epoch [120/120    avg_loss:0.013, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1262    1    1    0    3    0    0    0    3   15    0    0
     0    0    0]
 [   0    0    0  717    0    0    0    0    0    7    1    3   19    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    1    6    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   12    0    0    3    0    0    0    0  833   27    0    0
     0    0    0]
 [   0    0    2    0    0    0    1    0    0    0    2 2196    9    0
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    4  525    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1128   10    0]
 [   0    0    0    0    0    1    9    0    0    0    0    0    0    0
    16  321    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.09214092140921

F1 scores:
[       nan 0.93975904 0.98478346 0.97883959 0.99765808 0.98034682
 0.98640483 0.89285714 0.99649942 0.8372093  0.97199533 0.98497421
 0.96507353 1.         0.98558322 0.94690265 0.98224852]

Kappa:
0.9782354342091197
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb43769c710>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.387, val_acc:0.418]
Epoch [2/120    avg_loss:1.858, val_acc:0.565]
Epoch [3/120    avg_loss:1.598, val_acc:0.641]
Epoch [4/120    avg_loss:1.360, val_acc:0.658]
Epoch [5/120    avg_loss:1.108, val_acc:0.678]
Epoch [6/120    avg_loss:0.910, val_acc:0.733]
Epoch [7/120    avg_loss:0.791, val_acc:0.747]
Epoch [8/120    avg_loss:0.709, val_acc:0.783]
Epoch [9/120    avg_loss:0.590, val_acc:0.808]
Epoch [10/120    avg_loss:0.567, val_acc:0.833]
Epoch [11/120    avg_loss:0.554, val_acc:0.845]
Epoch [12/120    avg_loss:0.395, val_acc:0.864]
Epoch [13/120    avg_loss:0.292, val_acc:0.887]
Epoch [14/120    avg_loss:0.289, val_acc:0.870]
Epoch [15/120    avg_loss:0.290, val_acc:0.839]
Epoch [16/120    avg_loss:0.278, val_acc:0.890]
Epoch [17/120    avg_loss:0.243, val_acc:0.916]
Epoch [18/120    avg_loss:0.182, val_acc:0.928]
Epoch [19/120    avg_loss:0.120, val_acc:0.948]
Epoch [20/120    avg_loss:0.151, val_acc:0.929]
Epoch [21/120    avg_loss:0.192, val_acc:0.932]
Epoch [22/120    avg_loss:0.155, val_acc:0.936]
Epoch [23/120    avg_loss:0.111, val_acc:0.953]
Epoch [24/120    avg_loss:0.095, val_acc:0.952]
Epoch [25/120    avg_loss:0.081, val_acc:0.966]
Epoch [26/120    avg_loss:0.077, val_acc:0.957]
Epoch [27/120    avg_loss:0.070, val_acc:0.949]
Epoch [28/120    avg_loss:0.063, val_acc:0.958]
Epoch [29/120    avg_loss:0.078, val_acc:0.955]
Epoch [30/120    avg_loss:0.141, val_acc:0.944]
Epoch [31/120    avg_loss:0.116, val_acc:0.949]
Epoch [32/120    avg_loss:0.080, val_acc:0.954]
Epoch [33/120    avg_loss:0.058, val_acc:0.958]
Epoch [34/120    avg_loss:0.069, val_acc:0.957]
Epoch [35/120    avg_loss:0.049, val_acc:0.944]
Epoch [36/120    avg_loss:0.063, val_acc:0.954]
Epoch [37/120    avg_loss:0.050, val_acc:0.968]
Epoch [38/120    avg_loss:0.043, val_acc:0.963]
Epoch [39/120    avg_loss:0.051, val_acc:0.962]
Epoch [40/120    avg_loss:0.039, val_acc:0.967]
Epoch [41/120    avg_loss:0.031, val_acc:0.969]
Epoch [42/120    avg_loss:0.026, val_acc:0.978]
Epoch [43/120    avg_loss:0.034, val_acc:0.963]
Epoch [44/120    avg_loss:0.043, val_acc:0.967]
Epoch [45/120    avg_loss:0.062, val_acc:0.955]
Epoch [46/120    avg_loss:0.028, val_acc:0.972]
Epoch [47/120    avg_loss:0.020, val_acc:0.974]
Epoch [48/120    avg_loss:0.024, val_acc:0.973]
Epoch [49/120    avg_loss:0.029, val_acc:0.980]
Epoch [50/120    avg_loss:0.036, val_acc:0.969]
Epoch [51/120    avg_loss:0.022, val_acc:0.975]
Epoch [52/120    avg_loss:0.015, val_acc:0.975]
Epoch [53/120    avg_loss:0.014, val_acc:0.982]
Epoch [54/120    avg_loss:0.013, val_acc:0.975]
Epoch [55/120    avg_loss:0.017, val_acc:0.980]
Epoch [56/120    avg_loss:0.014, val_acc:0.980]
Epoch [57/120    avg_loss:0.013, val_acc:0.979]
Epoch [58/120    avg_loss:0.011, val_acc:0.979]
Epoch [59/120    avg_loss:0.020, val_acc:0.970]
Epoch [60/120    avg_loss:0.013, val_acc:0.982]
Epoch [61/120    avg_loss:0.014, val_acc:0.982]
Epoch [62/120    avg_loss:0.011, val_acc:0.986]
Epoch [63/120    avg_loss:0.011, val_acc:0.988]
Epoch [64/120    avg_loss:0.014, val_acc:0.978]
Epoch [65/120    avg_loss:0.010, val_acc:0.978]
Epoch [66/120    avg_loss:0.016, val_acc:0.973]
Epoch [67/120    avg_loss:0.012, val_acc:0.980]
Epoch [68/120    avg_loss:0.008, val_acc:0.982]
Epoch [69/120    avg_loss:0.007, val_acc:0.983]
Epoch [70/120    avg_loss:0.007, val_acc:0.981]
Epoch [71/120    avg_loss:0.014, val_acc:0.975]
Epoch [72/120    avg_loss:0.013, val_acc:0.981]
Epoch [73/120    avg_loss:0.009, val_acc:0.985]
Epoch [74/120    avg_loss:0.011, val_acc:0.978]
Epoch [75/120    avg_loss:0.009, val_acc:0.984]
Epoch [76/120    avg_loss:0.025, val_acc:0.963]
Epoch [77/120    avg_loss:0.036, val_acc:0.970]
Epoch [78/120    avg_loss:0.017, val_acc:0.975]
Epoch [79/120    avg_loss:0.011, val_acc:0.978]
Epoch [80/120    avg_loss:0.013, val_acc:0.983]
Epoch [81/120    avg_loss:0.010, val_acc:0.982]
Epoch [82/120    avg_loss:0.009, val_acc:0.981]
Epoch [83/120    avg_loss:0.009, val_acc:0.982]
Epoch [84/120    avg_loss:0.010, val_acc:0.983]
Epoch [85/120    avg_loss:0.008, val_acc:0.983]
Epoch [86/120    avg_loss:0.005, val_acc:0.984]
Epoch [87/120    avg_loss:0.007, val_acc:0.983]
Epoch [88/120    avg_loss:0.007, val_acc:0.983]
Epoch [89/120    avg_loss:0.008, val_acc:0.984]
Epoch [90/120    avg_loss:0.005, val_acc:0.984]
Epoch [91/120    avg_loss:0.006, val_acc:0.984]
Epoch [92/120    avg_loss:0.008, val_acc:0.984]
Epoch [93/120    avg_loss:0.008, val_acc:0.984]
Epoch [94/120    avg_loss:0.007, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.984]
Epoch [96/120    avg_loss:0.007, val_acc:0.985]
Epoch [97/120    avg_loss:0.008, val_acc:0.985]
Epoch [98/120    avg_loss:0.006, val_acc:0.985]
Epoch [99/120    avg_loss:0.007, val_acc:0.985]
Epoch [100/120    avg_loss:0.006, val_acc:0.985]
Epoch [101/120    avg_loss:0.006, val_acc:0.985]
Epoch [102/120    avg_loss:0.006, val_acc:0.985]
Epoch [103/120    avg_loss:0.007, val_acc:0.985]
Epoch [104/120    avg_loss:0.006, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.985]
Epoch [107/120    avg_loss:0.007, val_acc:0.985]
Epoch [108/120    avg_loss:0.006, val_acc:0.985]
Epoch [109/120    avg_loss:0.006, val_acc:0.985]
Epoch [110/120    avg_loss:0.007, val_acc:0.985]
Epoch [111/120    avg_loss:0.007, val_acc:0.985]
Epoch [112/120    avg_loss:0.006, val_acc:0.985]
Epoch [113/120    avg_loss:0.006, val_acc:0.985]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.006, val_acc:0.985]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.006, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.985]
Epoch [120/120    avg_loss:0.006, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1253    5    0    1    0    0    0    0   11   13    1    0
     0    1    0]
 [   0    0    0  710    0   13    0    0    0    4    1    0   12    7
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    2    1    0    0    1  853    9    0    0
     0    3    0]
 [   0    0   17    0    0    0    3    0    0    1    8 2179    1    1
     0    0    0]
 [   0    0    0    1    0    4    0    0    0    0    4    0  522    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    2    1    0    0    0    0    0    0
  1125   11    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    13  326    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.05962059620596

F1 scores:
[       nan 0.96385542 0.97814208 0.97060834 0.99764706 0.97522523
 0.98793363 0.98039216 0.997669   0.81818182 0.97374429 0.98776065
 0.97297297 0.97883598 0.98770852 0.94629898 0.97619048]

Kappa:
0.9778874226595442
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbff6c767f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.411, val_acc:0.462]
Epoch [2/120    avg_loss:1.805, val_acc:0.542]
Epoch [3/120    avg_loss:1.566, val_acc:0.615]
Epoch [4/120    avg_loss:1.326, val_acc:0.616]
Epoch [5/120    avg_loss:1.118, val_acc:0.662]
Epoch [6/120    avg_loss:0.903, val_acc:0.759]
Epoch [7/120    avg_loss:0.733, val_acc:0.779]
Epoch [8/120    avg_loss:0.600, val_acc:0.817]
Epoch [9/120    avg_loss:0.702, val_acc:0.760]
Epoch [10/120    avg_loss:0.571, val_acc:0.826]
Epoch [11/120    avg_loss:0.516, val_acc:0.841]
Epoch [12/120    avg_loss:0.399, val_acc:0.860]
Epoch [13/120    avg_loss:0.339, val_acc:0.892]
Epoch [14/120    avg_loss:0.286, val_acc:0.882]
Epoch [15/120    avg_loss:0.302, val_acc:0.889]
Epoch [16/120    avg_loss:0.232, val_acc:0.905]
Epoch [17/120    avg_loss:0.169, val_acc:0.918]
Epoch [18/120    avg_loss:0.149, val_acc:0.923]
Epoch [19/120    avg_loss:0.170, val_acc:0.919]
Epoch [20/120    avg_loss:0.124, val_acc:0.937]
Epoch [21/120    avg_loss:0.101, val_acc:0.942]
Epoch [22/120    avg_loss:0.110, val_acc:0.939]
Epoch [23/120    avg_loss:0.135, val_acc:0.949]
Epoch [24/120    avg_loss:0.116, val_acc:0.947]
Epoch [25/120    avg_loss:0.091, val_acc:0.943]
Epoch [26/120    avg_loss:0.108, val_acc:0.946]
Epoch [27/120    avg_loss:0.070, val_acc:0.946]
Epoch [28/120    avg_loss:0.072, val_acc:0.965]
Epoch [29/120    avg_loss:0.055, val_acc:0.965]
Epoch [30/120    avg_loss:0.058, val_acc:0.954]
Epoch [31/120    avg_loss:0.047, val_acc:0.954]
Epoch [32/120    avg_loss:0.044, val_acc:0.966]
Epoch [33/120    avg_loss:0.045, val_acc:0.978]
Epoch [34/120    avg_loss:0.037, val_acc:0.970]
Epoch [35/120    avg_loss:0.034, val_acc:0.971]
Epoch [36/120    avg_loss:0.053, val_acc:0.959]
Epoch [37/120    avg_loss:0.057, val_acc:0.963]
Epoch [38/120    avg_loss:0.052, val_acc:0.966]
Epoch [39/120    avg_loss:0.046, val_acc:0.969]
Epoch [40/120    avg_loss:0.033, val_acc:0.970]
Epoch [41/120    avg_loss:0.027, val_acc:0.981]
Epoch [42/120    avg_loss:0.032, val_acc:0.972]
Epoch [43/120    avg_loss:0.024, val_acc:0.972]
Epoch [44/120    avg_loss:0.031, val_acc:0.980]
Epoch [45/120    avg_loss:0.019, val_acc:0.981]
Epoch [46/120    avg_loss:0.026, val_acc:0.978]
Epoch [47/120    avg_loss:0.023, val_acc:0.978]
Epoch [48/120    avg_loss:0.022, val_acc:0.963]
Epoch [49/120    avg_loss:0.028, val_acc:0.974]
Epoch [50/120    avg_loss:0.016, val_acc:0.980]
Epoch [51/120    avg_loss:0.015, val_acc:0.981]
Epoch [52/120    avg_loss:0.014, val_acc:0.979]
Epoch [53/120    avg_loss:0.024, val_acc:0.975]
Epoch [54/120    avg_loss:0.042, val_acc:0.973]
Epoch [55/120    avg_loss:0.034, val_acc:0.974]
Epoch [56/120    avg_loss:0.032, val_acc:0.971]
Epoch [57/120    avg_loss:0.021, val_acc:0.979]
Epoch [58/120    avg_loss:0.017, val_acc:0.978]
Epoch [59/120    avg_loss:0.019, val_acc:0.984]
Epoch [60/120    avg_loss:0.015, val_acc:0.983]
Epoch [61/120    avg_loss:0.010, val_acc:0.981]
Epoch [62/120    avg_loss:0.010, val_acc:0.985]
Epoch [63/120    avg_loss:0.012, val_acc:0.985]
Epoch [64/120    avg_loss:0.017, val_acc:0.979]
Epoch [65/120    avg_loss:0.010, val_acc:0.973]
Epoch [66/120    avg_loss:0.010, val_acc:0.967]
Epoch [67/120    avg_loss:0.010, val_acc:0.983]
Epoch [68/120    avg_loss:0.012, val_acc:0.974]
Epoch [69/120    avg_loss:0.013, val_acc:0.986]
Epoch [70/120    avg_loss:0.016, val_acc:0.981]
Epoch [71/120    avg_loss:0.024, val_acc:0.974]
Epoch [72/120    avg_loss:0.016, val_acc:0.979]
Epoch [73/120    avg_loss:0.013, val_acc:0.966]
Epoch [74/120    avg_loss:0.016, val_acc:0.978]
Epoch [75/120    avg_loss:0.008, val_acc:0.980]
Epoch [76/120    avg_loss:0.007, val_acc:0.983]
Epoch [77/120    avg_loss:0.012, val_acc:0.968]
Epoch [78/120    avg_loss:0.014, val_acc:0.981]
Epoch [79/120    avg_loss:0.014, val_acc:0.985]
Epoch [80/120    avg_loss:0.010, val_acc:0.985]
Epoch [81/120    avg_loss:0.009, val_acc:0.981]
Epoch [82/120    avg_loss:0.010, val_acc:0.981]
Epoch [83/120    avg_loss:0.007, val_acc:0.982]
Epoch [84/120    avg_loss:0.006, val_acc:0.984]
Epoch [85/120    avg_loss:0.006, val_acc:0.984]
Epoch [86/120    avg_loss:0.006, val_acc:0.984]
Epoch [87/120    avg_loss:0.005, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.986]
Epoch [89/120    avg_loss:0.005, val_acc:0.986]
Epoch [90/120    avg_loss:0.005, val_acc:0.987]
Epoch [91/120    avg_loss:0.004, val_acc:0.988]
Epoch [92/120    avg_loss:0.004, val_acc:0.988]
Epoch [93/120    avg_loss:0.006, val_acc:0.988]
Epoch [94/120    avg_loss:0.006, val_acc:0.988]
Epoch [95/120    avg_loss:0.005, val_acc:0.987]
Epoch [96/120    avg_loss:0.004, val_acc:0.987]
Epoch [97/120    avg_loss:0.004, val_acc:0.988]
Epoch [98/120    avg_loss:0.004, val_acc:0.988]
Epoch [99/120    avg_loss:0.004, val_acc:0.988]
Epoch [100/120    avg_loss:0.004, val_acc:0.988]
Epoch [101/120    avg_loss:0.004, val_acc:0.988]
Epoch [102/120    avg_loss:0.003, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.988]
Epoch [104/120    avg_loss:0.004, val_acc:0.988]
Epoch [105/120    avg_loss:0.004, val_acc:0.988]
Epoch [106/120    avg_loss:0.004, val_acc:0.988]
Epoch [107/120    avg_loss:0.003, val_acc:0.988]
Epoch [108/120    avg_loss:0.004, val_acc:0.988]
Epoch [109/120    avg_loss:0.003, val_acc:0.988]
Epoch [110/120    avg_loss:0.004, val_acc:0.988]
Epoch [111/120    avg_loss:0.005, val_acc:0.988]
Epoch [112/120    avg_loss:0.003, val_acc:0.988]
Epoch [113/120    avg_loss:0.003, val_acc:0.988]
Epoch [114/120    avg_loss:0.004, val_acc:0.987]
Epoch [115/120    avg_loss:0.003, val_acc:0.988]
Epoch [116/120    avg_loss:0.003, val_acc:0.987]
Epoch [117/120    avg_loss:0.004, val_acc:0.987]
Epoch [118/120    avg_loss:0.004, val_acc:0.988]
Epoch [119/120    avg_loss:0.004, val_acc:0.988]
Epoch [120/120    avg_loss:0.003, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1270    0    0    0    0    0    0    0    5    6    4    0
     0    0    0]
 [   0    0    0  717    0   12    0    0    0    3    1    1    9    3
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    1    0    0    0    0  861    5    0    0
     0    1    0]
 [   0    0    2    0    0    0    0    0    0    2    1 2205    0    0
     0    0    0]
 [   0    0    0    1    0    1    0    0    0    0    0    0  531    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    0    0    0    0
  1126    5    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    46  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.44986449864498

F1 scores:
[       nan 1.         0.99063963 0.9781719  0.99764706 0.97297297
 0.99090909 1.         0.99883856 0.8372093  0.98795181 0.99548533
 0.98424467 0.9919571  0.97362732 0.90542636 0.98809524]

Kappa:
0.9823202370473347
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f95a9600780>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.404, val_acc:0.476]
Epoch [2/120    avg_loss:1.897, val_acc:0.490]
Epoch [3/120    avg_loss:1.587, val_acc:0.610]
Epoch [4/120    avg_loss:1.294, val_acc:0.681]
Epoch [5/120    avg_loss:1.156, val_acc:0.701]
Epoch [6/120    avg_loss:0.937, val_acc:0.734]
Epoch [7/120    avg_loss:0.897, val_acc:0.758]
Epoch [8/120    avg_loss:0.783, val_acc:0.782]
Epoch [9/120    avg_loss:0.594, val_acc:0.789]
Epoch [10/120    avg_loss:0.533, val_acc:0.812]
Epoch [11/120    avg_loss:0.474, val_acc:0.815]
Epoch [12/120    avg_loss:0.402, val_acc:0.861]
Epoch [13/120    avg_loss:0.316, val_acc:0.873]
Epoch [14/120    avg_loss:0.309, val_acc:0.865]
Epoch [15/120    avg_loss:0.284, val_acc:0.890]
Epoch [16/120    avg_loss:0.211, val_acc:0.896]
Epoch [17/120    avg_loss:0.217, val_acc:0.923]
Epoch [18/120    avg_loss:0.170, val_acc:0.891]
Epoch [19/120    avg_loss:0.146, val_acc:0.920]
Epoch [20/120    avg_loss:0.118, val_acc:0.903]
Epoch [21/120    avg_loss:0.131, val_acc:0.939]
Epoch [22/120    avg_loss:0.107, val_acc:0.934]
Epoch [23/120    avg_loss:0.097, val_acc:0.956]
Epoch [24/120    avg_loss:0.133, val_acc:0.932]
Epoch [25/120    avg_loss:0.147, val_acc:0.943]
Epoch [26/120    avg_loss:0.105, val_acc:0.943]
Epoch [27/120    avg_loss:0.111, val_acc:0.952]
Epoch [28/120    avg_loss:0.072, val_acc:0.947]
Epoch [29/120    avg_loss:0.064, val_acc:0.960]
Epoch [30/120    avg_loss:0.056, val_acc:0.963]
Epoch [31/120    avg_loss:0.061, val_acc:0.960]
Epoch [32/120    avg_loss:0.075, val_acc:0.936]
Epoch [33/120    avg_loss:0.047, val_acc:0.971]
Epoch [34/120    avg_loss:0.047, val_acc:0.974]
Epoch [35/120    avg_loss:0.042, val_acc:0.976]
Epoch [36/120    avg_loss:0.041, val_acc:0.967]
Epoch [37/120    avg_loss:0.034, val_acc:0.978]
Epoch [38/120    avg_loss:0.030, val_acc:0.970]
Epoch [39/120    avg_loss:0.033, val_acc:0.973]
Epoch [40/120    avg_loss:0.034, val_acc:0.978]
Epoch [41/120    avg_loss:0.032, val_acc:0.979]
Epoch [42/120    avg_loss:0.026, val_acc:0.976]
Epoch [43/120    avg_loss:0.232, val_acc:0.845]
Epoch [44/120    avg_loss:0.416, val_acc:0.812]
Epoch [45/120    avg_loss:0.271, val_acc:0.896]
Epoch [46/120    avg_loss:0.202, val_acc:0.922]
Epoch [47/120    avg_loss:0.098, val_acc:0.939]
Epoch [48/120    avg_loss:0.061, val_acc:0.960]
Epoch [49/120    avg_loss:0.068, val_acc:0.974]
Epoch [50/120    avg_loss:0.061, val_acc:0.957]
Epoch [51/120    avg_loss:0.036, val_acc:0.973]
Epoch [52/120    avg_loss:0.038, val_acc:0.971]
Epoch [53/120    avg_loss:0.029, val_acc:0.975]
Epoch [54/120    avg_loss:0.037, val_acc:0.973]
Epoch [55/120    avg_loss:0.027, val_acc:0.978]
Epoch [56/120    avg_loss:0.023, val_acc:0.978]
Epoch [57/120    avg_loss:0.022, val_acc:0.981]
Epoch [58/120    avg_loss:0.020, val_acc:0.983]
Epoch [59/120    avg_loss:0.019, val_acc:0.983]
Epoch [60/120    avg_loss:0.017, val_acc:0.983]
Epoch [61/120    avg_loss:0.017, val_acc:0.983]
Epoch [62/120    avg_loss:0.018, val_acc:0.983]
Epoch [63/120    avg_loss:0.018, val_acc:0.984]
Epoch [64/120    avg_loss:0.018, val_acc:0.983]
Epoch [65/120    avg_loss:0.016, val_acc:0.985]
Epoch [66/120    avg_loss:0.016, val_acc:0.982]
Epoch [67/120    avg_loss:0.019, val_acc:0.983]
Epoch [68/120    avg_loss:0.016, val_acc:0.980]
Epoch [69/120    avg_loss:0.020, val_acc:0.980]
Epoch [70/120    avg_loss:0.013, val_acc:0.981]
Epoch [71/120    avg_loss:0.016, val_acc:0.980]
Epoch [72/120    avg_loss:0.015, val_acc:0.984]
Epoch [73/120    avg_loss:0.014, val_acc:0.984]
Epoch [74/120    avg_loss:0.016, val_acc:0.983]
Epoch [75/120    avg_loss:0.016, val_acc:0.983]
Epoch [76/120    avg_loss:0.014, val_acc:0.983]
Epoch [77/120    avg_loss:0.017, val_acc:0.984]
Epoch [78/120    avg_loss:0.016, val_acc:0.985]
Epoch [79/120    avg_loss:0.015, val_acc:0.985]
Epoch [80/120    avg_loss:0.015, val_acc:0.984]
Epoch [81/120    avg_loss:0.013, val_acc:0.985]
Epoch [82/120    avg_loss:0.012, val_acc:0.985]
Epoch [83/120    avg_loss:0.016, val_acc:0.984]
Epoch [84/120    avg_loss:0.016, val_acc:0.982]
Epoch [85/120    avg_loss:0.016, val_acc:0.982]
Epoch [86/120    avg_loss:0.016, val_acc:0.983]
Epoch [87/120    avg_loss:0.016, val_acc:0.984]
Epoch [88/120    avg_loss:0.013, val_acc:0.985]
Epoch [89/120    avg_loss:0.011, val_acc:0.983]
Epoch [90/120    avg_loss:0.015, val_acc:0.983]
Epoch [91/120    avg_loss:0.013, val_acc:0.982]
Epoch [92/120    avg_loss:0.013, val_acc:0.983]
Epoch [93/120    avg_loss:0.016, val_acc:0.982]
Epoch [94/120    avg_loss:0.015, val_acc:0.982]
Epoch [95/120    avg_loss:0.012, val_acc:0.984]
Epoch [96/120    avg_loss:0.014, val_acc:0.982]
Epoch [97/120    avg_loss:0.011, val_acc:0.983]
Epoch [98/120    avg_loss:0.014, val_acc:0.983]
Epoch [99/120    avg_loss:0.013, val_acc:0.984]
Epoch [100/120    avg_loss:0.014, val_acc:0.983]
Epoch [101/120    avg_loss:0.011, val_acc:0.985]
Epoch [102/120    avg_loss:0.011, val_acc:0.985]
Epoch [103/120    avg_loss:0.011, val_acc:0.984]
Epoch [104/120    avg_loss:0.013, val_acc:0.984]
Epoch [105/120    avg_loss:0.016, val_acc:0.985]
Epoch [106/120    avg_loss:0.015, val_acc:0.986]
Epoch [107/120    avg_loss:0.014, val_acc:0.983]
Epoch [108/120    avg_loss:0.011, val_acc:0.984]
Epoch [109/120    avg_loss:0.011, val_acc:0.984]
Epoch [110/120    avg_loss:0.014, val_acc:0.984]
Epoch [111/120    avg_loss:0.012, val_acc:0.984]
Epoch [112/120    avg_loss:0.015, val_acc:0.985]
Epoch [113/120    avg_loss:0.015, val_acc:0.985]
Epoch [114/120    avg_loss:0.010, val_acc:0.983]
Epoch [115/120    avg_loss:0.013, val_acc:0.984]
Epoch [116/120    avg_loss:0.012, val_acc:0.985]
Epoch [117/120    avg_loss:0.016, val_acc:0.988]
Epoch [118/120    avg_loss:0.011, val_acc:0.986]
Epoch [119/120    avg_loss:0.011, val_acc:0.986]
Epoch [120/120    avg_loss:0.011, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1267    5    0    0    1    0    0    1    5    5    1    0
     0    0    0]
 [   0    0    0  725    0    3    0    0    0    3    1    1   13    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    4    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    1    2    0    0    0  843   21    0    0
     0    2    0]
 [   0    0   17    0    0    0    3    0    0    3    3 2184    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  531    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1127   11    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    32  312    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.28726287262873

F1 scores:
[       nan 0.98765432 0.98407767 0.98105548 1.         0.98964327
 0.99167298 0.92592593 0.99883586 0.79069767 0.97513013 0.987565
 0.98333333 1.         0.98042627 0.92719168 0.98823529]

Kappa:
0.9804706632461822
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4cb17807f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.449, val_acc:0.465]
Epoch [2/120    avg_loss:1.812, val_acc:0.503]
Epoch [3/120    avg_loss:1.510, val_acc:0.600]
Epoch [4/120    avg_loss:1.339, val_acc:0.628]
Epoch [5/120    avg_loss:1.050, val_acc:0.685]
Epoch [6/120    avg_loss:0.944, val_acc:0.716]
Epoch [7/120    avg_loss:0.813, val_acc:0.768]
Epoch [8/120    avg_loss:0.695, val_acc:0.807]
Epoch [9/120    avg_loss:0.507, val_acc:0.812]
Epoch [10/120    avg_loss:0.704, val_acc:0.786]
Epoch [11/120    avg_loss:0.496, val_acc:0.767]
Epoch [12/120    avg_loss:0.450, val_acc:0.819]
Epoch [13/120    avg_loss:0.328, val_acc:0.851]
Epoch [14/120    avg_loss:0.281, val_acc:0.899]
Epoch [15/120    avg_loss:0.246, val_acc:0.898]
Epoch [16/120    avg_loss:0.218, val_acc:0.899]
Epoch [17/120    avg_loss:0.173, val_acc:0.921]
Epoch [18/120    avg_loss:0.178, val_acc:0.931]
Epoch [19/120    avg_loss:0.167, val_acc:0.906]
Epoch [20/120    avg_loss:0.185, val_acc:0.880]
Epoch [21/120    avg_loss:0.156, val_acc:0.945]
Epoch [22/120    avg_loss:0.110, val_acc:0.954]
Epoch [23/120    avg_loss:0.104, val_acc:0.944]
Epoch [24/120    avg_loss:0.104, val_acc:0.935]
Epoch [25/120    avg_loss:0.119, val_acc:0.954]
Epoch [26/120    avg_loss:0.085, val_acc:0.949]
Epoch [27/120    avg_loss:0.122, val_acc:0.896]
Epoch [28/120    avg_loss:0.201, val_acc:0.928]
Epoch [29/120    avg_loss:0.184, val_acc:0.944]
Epoch [30/120    avg_loss:0.118, val_acc:0.907]
Epoch [31/120    avg_loss:0.181, val_acc:0.925]
Epoch [32/120    avg_loss:0.232, val_acc:0.932]
Epoch [33/120    avg_loss:0.105, val_acc:0.948]
Epoch [34/120    avg_loss:0.084, val_acc:0.936]
Epoch [35/120    avg_loss:0.088, val_acc:0.956]
Epoch [36/120    avg_loss:0.074, val_acc:0.955]
Epoch [37/120    avg_loss:0.051, val_acc:0.965]
Epoch [38/120    avg_loss:0.045, val_acc:0.968]
Epoch [39/120    avg_loss:0.048, val_acc:0.970]
Epoch [40/120    avg_loss:0.037, val_acc:0.960]
Epoch [41/120    avg_loss:0.043, val_acc:0.971]
Epoch [42/120    avg_loss:0.045, val_acc:0.968]
Epoch [43/120    avg_loss:0.042, val_acc:0.975]
Epoch [44/120    avg_loss:0.031, val_acc:0.971]
Epoch [45/120    avg_loss:0.040, val_acc:0.970]
Epoch [46/120    avg_loss:0.028, val_acc:0.969]
Epoch [47/120    avg_loss:0.028, val_acc:0.972]
Epoch [48/120    avg_loss:0.029, val_acc:0.971]
Epoch [49/120    avg_loss:0.023, val_acc:0.974]
Epoch [50/120    avg_loss:0.023, val_acc:0.971]
Epoch [51/120    avg_loss:0.025, val_acc:0.970]
Epoch [52/120    avg_loss:0.038, val_acc:0.971]
Epoch [53/120    avg_loss:0.027, val_acc:0.977]
Epoch [54/120    avg_loss:0.019, val_acc:0.967]
Epoch [55/120    avg_loss:0.015, val_acc:0.973]
Epoch [56/120    avg_loss:0.016, val_acc:0.980]
Epoch [57/120    avg_loss:0.015, val_acc:0.975]
Epoch [58/120    avg_loss:0.018, val_acc:0.971]
Epoch [59/120    avg_loss:0.024, val_acc:0.974]
Epoch [60/120    avg_loss:0.018, val_acc:0.969]
Epoch [61/120    avg_loss:0.028, val_acc:0.967]
Epoch [62/120    avg_loss:0.025, val_acc:0.970]
Epoch [63/120    avg_loss:0.081, val_acc:0.960]
Epoch [64/120    avg_loss:0.037, val_acc:0.970]
Epoch [65/120    avg_loss:0.019, val_acc:0.980]
Epoch [66/120    avg_loss:0.014, val_acc:0.976]
Epoch [67/120    avg_loss:0.011, val_acc:0.978]
Epoch [68/120    avg_loss:0.016, val_acc:0.976]
Epoch [69/120    avg_loss:0.018, val_acc:0.977]
Epoch [70/120    avg_loss:0.011, val_acc:0.980]
Epoch [71/120    avg_loss:0.013, val_acc:0.980]
Epoch [72/120    avg_loss:0.012, val_acc:0.977]
Epoch [73/120    avg_loss:0.012, val_acc:0.979]
Epoch [74/120    avg_loss:0.010, val_acc:0.982]
Epoch [75/120    avg_loss:0.011, val_acc:0.967]
Epoch [76/120    avg_loss:0.010, val_acc:0.983]
Epoch [77/120    avg_loss:0.011, val_acc:0.978]
Epoch [78/120    avg_loss:0.008, val_acc:0.982]
Epoch [79/120    avg_loss:0.009, val_acc:0.978]
Epoch [80/120    avg_loss:0.016, val_acc:0.979]
Epoch [81/120    avg_loss:0.011, val_acc:0.979]
Epoch [82/120    avg_loss:0.009, val_acc:0.977]
Epoch [83/120    avg_loss:0.010, val_acc:0.976]
Epoch [84/120    avg_loss:0.005, val_acc:0.982]
Epoch [85/120    avg_loss:0.005, val_acc:0.982]
Epoch [86/120    avg_loss:0.007, val_acc:0.978]
Epoch [87/120    avg_loss:0.009, val_acc:0.978]
Epoch [88/120    avg_loss:0.026, val_acc:0.976]
Epoch [89/120    avg_loss:0.019, val_acc:0.974]
Epoch [90/120    avg_loss:0.007, val_acc:0.978]
Epoch [91/120    avg_loss:0.008, val_acc:0.979]
Epoch [92/120    avg_loss:0.006, val_acc:0.980]
Epoch [93/120    avg_loss:0.006, val_acc:0.979]
Epoch [94/120    avg_loss:0.006, val_acc:0.979]
Epoch [95/120    avg_loss:0.007, val_acc:0.980]
Epoch [96/120    avg_loss:0.006, val_acc:0.980]
Epoch [97/120    avg_loss:0.008, val_acc:0.980]
Epoch [98/120    avg_loss:0.006, val_acc:0.980]
Epoch [99/120    avg_loss:0.007, val_acc:0.979]
Epoch [100/120    avg_loss:0.005, val_acc:0.978]
Epoch [101/120    avg_loss:0.009, val_acc:0.979]
Epoch [102/120    avg_loss:0.004, val_acc:0.979]
Epoch [103/120    avg_loss:0.004, val_acc:0.979]
Epoch [104/120    avg_loss:0.006, val_acc:0.979]
Epoch [105/120    avg_loss:0.007, val_acc:0.979]
Epoch [106/120    avg_loss:0.006, val_acc:0.979]
Epoch [107/120    avg_loss:0.006, val_acc:0.979]
Epoch [108/120    avg_loss:0.006, val_acc:0.979]
Epoch [109/120    avg_loss:0.006, val_acc:0.979]
Epoch [110/120    avg_loss:0.005, val_acc:0.979]
Epoch [111/120    avg_loss:0.005, val_acc:0.979]
Epoch [112/120    avg_loss:0.005, val_acc:0.979]
Epoch [113/120    avg_loss:0.005, val_acc:0.979]
Epoch [114/120    avg_loss:0.005, val_acc:0.979]
Epoch [115/120    avg_loss:0.005, val_acc:0.979]
Epoch [116/120    avg_loss:0.006, val_acc:0.979]
Epoch [117/120    avg_loss:0.005, val_acc:0.979]
Epoch [118/120    avg_loss:0.005, val_acc:0.979]
Epoch [119/120    avg_loss:0.004, val_acc:0.979]
Epoch [120/120    avg_loss:0.004, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1269    0    0    0    0    0    0    2    6    6    2    0
     0    0    0]
 [   0    0    0  719    0    6    0    0    0    8    1    2   11    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    7    0    3    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    2    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  424    0    0    0    6    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    2    3    0    0    1  863    3    0    0
     0    2    0]
 [   0    0    1    0    0    0    1    1    0    0    3 2199    3    1
     1    0    0]
 [   0    0    0    0    0    1    0    0    0    0    4    0  527    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1136    3    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    34  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.39566395663957

F1 scores:
[       nan 0.975      0.99256942 0.98090041 1.         0.97695853
 0.98417483 0.86206897 0.99297424 0.72       0.98459783 0.99457259
 0.97232472 0.99730458 0.9822741  0.9202454  0.98224852]

Kappa:
0.9817069094783573
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2987b1a780>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.434, val_acc:0.540]
Epoch [2/120    avg_loss:1.773, val_acc:0.573]
Epoch [3/120    avg_loss:1.448, val_acc:0.630]
Epoch [4/120    avg_loss:1.233, val_acc:0.701]
Epoch [5/120    avg_loss:1.091, val_acc:0.710]
Epoch [6/120    avg_loss:0.917, val_acc:0.792]
Epoch [7/120    avg_loss:0.737, val_acc:0.803]
Epoch [8/120    avg_loss:0.582, val_acc:0.744]
Epoch [9/120    avg_loss:0.611, val_acc:0.789]
Epoch [10/120    avg_loss:0.586, val_acc:0.824]
Epoch [11/120    avg_loss:0.513, val_acc:0.843]
Epoch [12/120    avg_loss:0.365, val_acc:0.839]
Epoch [13/120    avg_loss:0.427, val_acc:0.869]
Epoch [14/120    avg_loss:0.312, val_acc:0.834]
Epoch [15/120    avg_loss:0.323, val_acc:0.884]
Epoch [16/120    avg_loss:0.212, val_acc:0.905]
Epoch [17/120    avg_loss:0.180, val_acc:0.907]
Epoch [18/120    avg_loss:0.151, val_acc:0.930]
Epoch [19/120    avg_loss:0.163, val_acc:0.935]
Epoch [20/120    avg_loss:0.168, val_acc:0.934]
Epoch [21/120    avg_loss:0.155, val_acc:0.931]
Epoch [22/120    avg_loss:0.120, val_acc:0.957]
Epoch [23/120    avg_loss:0.086, val_acc:0.949]
Epoch [24/120    avg_loss:0.144, val_acc:0.930]
Epoch [25/120    avg_loss:0.111, val_acc:0.939]
Epoch [26/120    avg_loss:0.120, val_acc:0.896]
Epoch [27/120    avg_loss:0.107, val_acc:0.958]
Epoch [28/120    avg_loss:0.080, val_acc:0.941]
Epoch [29/120    avg_loss:0.065, val_acc:0.971]
Epoch [30/120    avg_loss:0.055, val_acc:0.949]
Epoch [31/120    avg_loss:0.051, val_acc:0.954]
Epoch [32/120    avg_loss:0.044, val_acc:0.971]
Epoch [33/120    avg_loss:0.045, val_acc:0.958]
Epoch [34/120    avg_loss:0.042, val_acc:0.956]
Epoch [35/120    avg_loss:0.047, val_acc:0.967]
Epoch [36/120    avg_loss:0.044, val_acc:0.973]
Epoch [37/120    avg_loss:0.036, val_acc:0.961]
Epoch [38/120    avg_loss:0.036, val_acc:0.977]
Epoch [39/120    avg_loss:0.033, val_acc:0.975]
Epoch [40/120    avg_loss:0.029, val_acc:0.970]
Epoch [41/120    avg_loss:0.025, val_acc:0.976]
Epoch [42/120    avg_loss:0.024, val_acc:0.970]
Epoch [43/120    avg_loss:0.034, val_acc:0.973]
Epoch [44/120    avg_loss:0.036, val_acc:0.958]
Epoch [45/120    avg_loss:0.035, val_acc:0.957]
Epoch [46/120    avg_loss:0.038, val_acc:0.968]
Epoch [47/120    avg_loss:0.035, val_acc:0.966]
Epoch [48/120    avg_loss:0.030, val_acc:0.963]
Epoch [49/120    avg_loss:0.024, val_acc:0.978]
Epoch [50/120    avg_loss:0.024, val_acc:0.964]
Epoch [51/120    avg_loss:0.024, val_acc:0.972]
Epoch [52/120    avg_loss:0.025, val_acc:0.971]
Epoch [53/120    avg_loss:0.030, val_acc:0.969]
Epoch [54/120    avg_loss:0.053, val_acc:0.958]
Epoch [55/120    avg_loss:0.058, val_acc:0.972]
Epoch [56/120    avg_loss:0.128, val_acc:0.946]
Epoch [57/120    avg_loss:0.127, val_acc:0.930]
Epoch [58/120    avg_loss:0.058, val_acc:0.948]
Epoch [59/120    avg_loss:0.041, val_acc:0.948]
Epoch [60/120    avg_loss:0.033, val_acc:0.964]
Epoch [61/120    avg_loss:0.032, val_acc:0.966]
Epoch [62/120    avg_loss:0.059, val_acc:0.946]
Epoch [63/120    avg_loss:0.080, val_acc:0.960]
Epoch [64/120    avg_loss:0.041, val_acc:0.968]
Epoch [65/120    avg_loss:0.035, val_acc:0.974]
Epoch [66/120    avg_loss:0.025, val_acc:0.975]
Epoch [67/120    avg_loss:0.029, val_acc:0.974]
Epoch [68/120    avg_loss:0.022, val_acc:0.975]
Epoch [69/120    avg_loss:0.018, val_acc:0.976]
Epoch [70/120    avg_loss:0.014, val_acc:0.977]
Epoch [71/120    avg_loss:0.015, val_acc:0.979]
Epoch [72/120    avg_loss:0.014, val_acc:0.978]
Epoch [73/120    avg_loss:0.014, val_acc:0.975]
Epoch [74/120    avg_loss:0.014, val_acc:0.978]
Epoch [75/120    avg_loss:0.014, val_acc:0.978]
Epoch [76/120    avg_loss:0.012, val_acc:0.979]
Epoch [77/120    avg_loss:0.017, val_acc:0.979]
Epoch [78/120    avg_loss:0.013, val_acc:0.979]
Epoch [79/120    avg_loss:0.013, val_acc:0.978]
Epoch [80/120    avg_loss:0.014, val_acc:0.977]
Epoch [81/120    avg_loss:0.011, val_acc:0.978]
Epoch [82/120    avg_loss:0.012, val_acc:0.978]
Epoch [83/120    avg_loss:0.017, val_acc:0.978]
Epoch [84/120    avg_loss:0.012, val_acc:0.977]
Epoch [85/120    avg_loss:0.013, val_acc:0.977]
Epoch [86/120    avg_loss:0.011, val_acc:0.978]
Epoch [87/120    avg_loss:0.016, val_acc:0.981]
Epoch [88/120    avg_loss:0.011, val_acc:0.981]
Epoch [89/120    avg_loss:0.012, val_acc:0.980]
Epoch [90/120    avg_loss:0.016, val_acc:0.981]
Epoch [91/120    avg_loss:0.012, val_acc:0.979]
Epoch [92/120    avg_loss:0.010, val_acc:0.979]
Epoch [93/120    avg_loss:0.012, val_acc:0.979]
Epoch [94/120    avg_loss:0.009, val_acc:0.978]
Epoch [95/120    avg_loss:0.012, val_acc:0.979]
Epoch [96/120    avg_loss:0.009, val_acc:0.979]
Epoch [97/120    avg_loss:0.010, val_acc:0.979]
Epoch [98/120    avg_loss:0.013, val_acc:0.979]
Epoch [99/120    avg_loss:0.010, val_acc:0.980]
Epoch [100/120    avg_loss:0.009, val_acc:0.979]
Epoch [101/120    avg_loss:0.010, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.980]
Epoch [103/120    avg_loss:0.009, val_acc:0.981]
Epoch [104/120    avg_loss:0.012, val_acc:0.979]
Epoch [105/120    avg_loss:0.011, val_acc:0.980]
Epoch [106/120    avg_loss:0.010, val_acc:0.980]
Epoch [107/120    avg_loss:0.010, val_acc:0.981]
Epoch [108/120    avg_loss:0.010, val_acc:0.980]
Epoch [109/120    avg_loss:0.010, val_acc:0.981]
Epoch [110/120    avg_loss:0.013, val_acc:0.982]
Epoch [111/120    avg_loss:0.008, val_acc:0.979]
Epoch [112/120    avg_loss:0.010, val_acc:0.980]
Epoch [113/120    avg_loss:0.007, val_acc:0.980]
Epoch [114/120    avg_loss:0.012, val_acc:0.980]
Epoch [115/120    avg_loss:0.010, val_acc:0.980]
Epoch [116/120    avg_loss:0.009, val_acc:0.980]
Epoch [117/120    avg_loss:0.009, val_acc:0.981]
Epoch [118/120    avg_loss:0.010, val_acc:0.981]
Epoch [119/120    avg_loss:0.008, val_acc:0.980]
Epoch [120/120    avg_loss:0.008, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1250    3    9    3    2    0    0    0    3   10    5    0
     0    0    0]
 [   0    0    0  730    0    3    0    0    0    8    1    0    2    3
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    2    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    2    0    0    0    2  851   11    0    0
     0    1    0]
 [   0    0    2    0    0    0    2    0    0    2    2 2198    3    1
     0    0    0]
 [   0    0    0    1    0    3    0    0    0    0    5    0  523    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1138    1    0]
 [   0    0    0    0    0    0    1    0    0    2    0    0    0    0
    59  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
98.1029810298103

F1 scores:
[       nan 1.         0.98231827 0.98582039 0.97695853 0.98289624
 0.99392097 0.96153846 1.         0.67924528 0.97985032 0.99210111
 0.97574627 0.98930481 0.97389816 0.89905363 0.95757576]

Kappa:
0.9783645736493232
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc98f8b0710>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.386, val_acc:0.388]
Epoch [2/120    avg_loss:1.782, val_acc:0.585]
Epoch [3/120    avg_loss:1.572, val_acc:0.655]
Epoch [4/120    avg_loss:1.295, val_acc:0.691]
Epoch [5/120    avg_loss:1.103, val_acc:0.696]
Epoch [6/120    avg_loss:1.033, val_acc:0.752]
Epoch [7/120    avg_loss:0.834, val_acc:0.756]
Epoch [8/120    avg_loss:0.655, val_acc:0.828]
Epoch [9/120    avg_loss:0.598, val_acc:0.832]
Epoch [10/120    avg_loss:0.507, val_acc:0.814]
Epoch [11/120    avg_loss:0.400, val_acc:0.896]
Epoch [12/120    avg_loss:0.405, val_acc:0.842]
Epoch [13/120    avg_loss:0.508, val_acc:0.833]
Epoch [14/120    avg_loss:0.457, val_acc:0.866]
Epoch [15/120    avg_loss:0.286, val_acc:0.905]
Epoch [16/120    avg_loss:0.229, val_acc:0.927]
Epoch [17/120    avg_loss:0.211, val_acc:0.909]
Epoch [18/120    avg_loss:0.239, val_acc:0.919]
Epoch [19/120    avg_loss:0.198, val_acc:0.929]
Epoch [20/120    avg_loss:0.144, val_acc:0.938]
Epoch [21/120    avg_loss:0.173, val_acc:0.911]
Epoch [22/120    avg_loss:0.153, val_acc:0.938]
Epoch [23/120    avg_loss:0.159, val_acc:0.943]
Epoch [24/120    avg_loss:0.086, val_acc:0.942]
Epoch [25/120    avg_loss:0.087, val_acc:0.937]
Epoch [26/120    avg_loss:0.089, val_acc:0.955]
Epoch [27/120    avg_loss:0.073, val_acc:0.964]
Epoch [28/120    avg_loss:0.082, val_acc:0.941]
Epoch [29/120    avg_loss:0.081, val_acc:0.965]
Epoch [30/120    avg_loss:0.066, val_acc:0.964]
Epoch [31/120    avg_loss:0.060, val_acc:0.959]
Epoch [32/120    avg_loss:0.055, val_acc:0.958]
Epoch [33/120    avg_loss:0.075, val_acc:0.944]
Epoch [34/120    avg_loss:0.053, val_acc:0.955]
Epoch [35/120    avg_loss:0.049, val_acc:0.960]
Epoch [36/120    avg_loss:0.043, val_acc:0.964]
Epoch [37/120    avg_loss:0.040, val_acc:0.970]
Epoch [38/120    avg_loss:0.037, val_acc:0.975]
Epoch [39/120    avg_loss:0.056, val_acc:0.970]
Epoch [40/120    avg_loss:0.067, val_acc:0.958]
Epoch [41/120    avg_loss:0.044, val_acc:0.969]
Epoch [42/120    avg_loss:0.032, val_acc:0.974]
Epoch [43/120    avg_loss:0.024, val_acc:0.970]
Epoch [44/120    avg_loss:0.022, val_acc:0.979]
Epoch [45/120    avg_loss:0.022, val_acc:0.981]
Epoch [46/120    avg_loss:0.026, val_acc:0.979]
Epoch [47/120    avg_loss:0.029, val_acc:0.981]
Epoch [48/120    avg_loss:0.018, val_acc:0.975]
Epoch [49/120    avg_loss:0.021, val_acc:0.982]
Epoch [50/120    avg_loss:0.020, val_acc:0.979]
Epoch [51/120    avg_loss:0.026, val_acc:0.980]
Epoch [52/120    avg_loss:0.022, val_acc:0.980]
Epoch [53/120    avg_loss:0.029, val_acc:0.980]
Epoch [54/120    avg_loss:0.028, val_acc:0.972]
Epoch [55/120    avg_loss:0.071, val_acc:0.955]
Epoch [56/120    avg_loss:0.046, val_acc:0.971]
Epoch [57/120    avg_loss:0.033, val_acc:0.988]
Epoch [58/120    avg_loss:0.027, val_acc:0.980]
Epoch [59/120    avg_loss:0.028, val_acc:0.970]
Epoch [60/120    avg_loss:0.040, val_acc:0.966]
Epoch [61/120    avg_loss:0.067, val_acc:0.972]
Epoch [62/120    avg_loss:0.027, val_acc:0.983]
Epoch [63/120    avg_loss:0.030, val_acc:0.981]
Epoch [64/120    avg_loss:0.034, val_acc:0.972]
Epoch [65/120    avg_loss:0.041, val_acc:0.984]
Epoch [66/120    avg_loss:0.026, val_acc:0.986]
Epoch [67/120    avg_loss:0.021, val_acc:0.981]
Epoch [68/120    avg_loss:0.023, val_acc:0.979]
Epoch [69/120    avg_loss:0.018, val_acc:0.989]
Epoch [70/120    avg_loss:0.022, val_acc:0.976]
Epoch [71/120    avg_loss:0.014, val_acc:0.988]
Epoch [72/120    avg_loss:0.011, val_acc:0.989]
Epoch [73/120    avg_loss:0.007, val_acc:0.982]
Epoch [74/120    avg_loss:0.010, val_acc:0.985]
Epoch [75/120    avg_loss:0.009, val_acc:0.979]
Epoch [76/120    avg_loss:0.010, val_acc:0.989]
Epoch [77/120    avg_loss:0.008, val_acc:0.991]
Epoch [78/120    avg_loss:0.019, val_acc:0.986]
Epoch [79/120    avg_loss:0.012, val_acc:0.988]
Epoch [80/120    avg_loss:0.009, val_acc:0.986]
Epoch [81/120    avg_loss:0.006, val_acc:0.991]
Epoch [82/120    avg_loss:0.007, val_acc:0.990]
Epoch [83/120    avg_loss:0.008, val_acc:0.990]
Epoch [84/120    avg_loss:0.005, val_acc:0.994]
Epoch [85/120    avg_loss:0.009, val_acc:0.985]
Epoch [86/120    avg_loss:0.010, val_acc:0.991]
Epoch [87/120    avg_loss:0.010, val_acc:0.981]
Epoch [88/120    avg_loss:0.016, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.990]
Epoch [90/120    avg_loss:0.005, val_acc:0.990]
Epoch [91/120    avg_loss:0.006, val_acc:0.994]
Epoch [92/120    avg_loss:0.005, val_acc:0.989]
Epoch [93/120    avg_loss:0.012, val_acc:0.988]
Epoch [94/120    avg_loss:0.029, val_acc:0.969]
Epoch [95/120    avg_loss:0.016, val_acc:0.989]
Epoch [96/120    avg_loss:0.007, val_acc:0.994]
Epoch [97/120    avg_loss:0.008, val_acc:0.986]
Epoch [98/120    avg_loss:0.011, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.994]
Epoch [100/120    avg_loss:0.005, val_acc:0.996]
Epoch [101/120    avg_loss:0.005, val_acc:0.989]
Epoch [102/120    avg_loss:0.004, val_acc:0.991]
Epoch [103/120    avg_loss:0.005, val_acc:0.989]
Epoch [104/120    avg_loss:0.005, val_acc:0.994]
Epoch [105/120    avg_loss:0.003, val_acc:0.993]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.003, val_acc:0.995]
Epoch [108/120    avg_loss:0.007, val_acc:0.989]
Epoch [109/120    avg_loss:0.006, val_acc:0.989]
Epoch [110/120    avg_loss:0.004, val_acc:0.989]
Epoch [111/120    avg_loss:0.004, val_acc:0.994]
Epoch [112/120    avg_loss:0.003, val_acc:0.993]
Epoch [113/120    avg_loss:0.006, val_acc:0.994]
Epoch [114/120    avg_loss:0.003, val_acc:0.993]
Epoch [115/120    avg_loss:0.004, val_acc:0.994]
Epoch [116/120    avg_loss:0.003, val_acc:0.995]
Epoch [117/120    avg_loss:0.003, val_acc:0.994]
Epoch [118/120    avg_loss:0.003, val_acc:0.995]
Epoch [119/120    avg_loss:0.003, val_acc:0.995]
Epoch [120/120    avg_loss:0.003, val_acc:0.995]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1274    0    3    0    1    0    0    0    4    2    1    0
     0    0    0]
 [   0    0    0  716    0   11    0    0    0    3    1    0   16    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0  869    4    0    0
     0    0    0]
 [   0    0    5    0    0    0    1    0    0    1    1 2188    0   14
     0    0    0]
 [   0    0    0    1    0    3    0    0    0    0   12    0  516    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    59  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
98.12466124661246

F1 scores:
[       nan 0.98765432 0.99337232 0.97814208 0.99065421 0.97616345
 0.99619193 0.98039216 0.99649942 0.85714286 0.98637911 0.99296574
 0.95910781 0.96354167 0.97039897 0.89440994 0.96341463]

Kappa:
0.9786177642851971
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:22:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f24d6625748>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.392, val_acc:0.500]
Epoch [2/120    avg_loss:1.852, val_acc:0.546]
Epoch [3/120    avg_loss:1.558, val_acc:0.616]
Epoch [4/120    avg_loss:1.333, val_acc:0.675]
Epoch [5/120    avg_loss:1.080, val_acc:0.724]
Epoch [6/120    avg_loss:0.893, val_acc:0.762]
Epoch [7/120    avg_loss:0.797, val_acc:0.781]
Epoch [8/120    avg_loss:0.618, val_acc:0.782]
Epoch [9/120    avg_loss:0.599, val_acc:0.804]
Epoch [10/120    avg_loss:0.530, val_acc:0.844]
Epoch [11/120    avg_loss:0.447, val_acc:0.863]
Epoch [12/120    avg_loss:0.470, val_acc:0.855]
Epoch [13/120    avg_loss:0.452, val_acc:0.868]
Epoch [14/120    avg_loss:0.371, val_acc:0.891]
Epoch [15/120    avg_loss:0.320, val_acc:0.897]
Epoch [16/120    avg_loss:0.269, val_acc:0.912]
Epoch [17/120    avg_loss:0.197, val_acc:0.919]
Epoch [18/120    avg_loss:0.225, val_acc:0.921]
Epoch [19/120    avg_loss:0.194, val_acc:0.918]
Epoch [20/120    avg_loss:0.188, val_acc:0.900]
Epoch [21/120    avg_loss:0.156, val_acc:0.922]
Epoch [22/120    avg_loss:0.135, val_acc:0.934]
Epoch [23/120    avg_loss:0.154, val_acc:0.932]
Epoch [24/120    avg_loss:0.115, val_acc:0.918]
Epoch [25/120    avg_loss:0.115, val_acc:0.938]
Epoch [26/120    avg_loss:0.101, val_acc:0.940]
Epoch [27/120    avg_loss:0.096, val_acc:0.950]
Epoch [28/120    avg_loss:0.084, val_acc:0.944]
Epoch [29/120    avg_loss:0.078, val_acc:0.951]
Epoch [30/120    avg_loss:0.071, val_acc:0.962]
Epoch [31/120    avg_loss:0.067, val_acc:0.964]
Epoch [32/120    avg_loss:0.067, val_acc:0.942]
Epoch [33/120    avg_loss:0.084, val_acc:0.950]
Epoch [34/120    avg_loss:0.071, val_acc:0.968]
Epoch [35/120    avg_loss:0.046, val_acc:0.964]
Epoch [36/120    avg_loss:0.047, val_acc:0.952]
Epoch [37/120    avg_loss:0.038, val_acc:0.968]
Epoch [38/120    avg_loss:0.042, val_acc:0.954]
Epoch [39/120    avg_loss:0.065, val_acc:0.957]
Epoch [40/120    avg_loss:0.065, val_acc:0.955]
Epoch [41/120    avg_loss:0.040, val_acc:0.968]
Epoch [42/120    avg_loss:0.039, val_acc:0.971]
Epoch [43/120    avg_loss:0.038, val_acc:0.967]
Epoch [44/120    avg_loss:0.032, val_acc:0.961]
Epoch [45/120    avg_loss:0.030, val_acc:0.963]
Epoch [46/120    avg_loss:0.034, val_acc:0.971]
Epoch [47/120    avg_loss:0.026, val_acc:0.971]
Epoch [48/120    avg_loss:0.028, val_acc:0.974]
Epoch [49/120    avg_loss:0.077, val_acc:0.959]
Epoch [50/120    avg_loss:0.047, val_acc:0.966]
Epoch [51/120    avg_loss:0.038, val_acc:0.966]
Epoch [52/120    avg_loss:0.029, val_acc:0.967]
Epoch [53/120    avg_loss:0.050, val_acc:0.956]
Epoch [54/120    avg_loss:0.040, val_acc:0.970]
Epoch [55/120    avg_loss:0.026, val_acc:0.974]
Epoch [56/120    avg_loss:0.026, val_acc:0.967]
Epoch [57/120    avg_loss:0.037, val_acc:0.952]
Epoch [58/120    avg_loss:0.037, val_acc:0.963]
Epoch [59/120    avg_loss:0.025, val_acc:0.969]
Epoch [60/120    avg_loss:0.028, val_acc:0.974]
Epoch [61/120    avg_loss:0.021, val_acc:0.968]
Epoch [62/120    avg_loss:0.024, val_acc:0.978]
Epoch [63/120    avg_loss:0.019, val_acc:0.975]
Epoch [64/120    avg_loss:0.022, val_acc:0.973]
Epoch [65/120    avg_loss:0.027, val_acc:0.967]
Epoch [66/120    avg_loss:0.039, val_acc:0.967]
Epoch [67/120    avg_loss:0.024, val_acc:0.973]
Epoch [68/120    avg_loss:0.020, val_acc:0.965]
Epoch [69/120    avg_loss:0.017, val_acc:0.975]
Epoch [70/120    avg_loss:0.014, val_acc:0.979]
Epoch [71/120    avg_loss:0.016, val_acc:0.974]
Epoch [72/120    avg_loss:0.012, val_acc:0.970]
Epoch [73/120    avg_loss:0.018, val_acc:0.975]
Epoch [74/120    avg_loss:0.016, val_acc:0.975]
Epoch [75/120    avg_loss:0.009, val_acc:0.974]
Epoch [76/120    avg_loss:0.016, val_acc:0.976]
Epoch [77/120    avg_loss:0.015, val_acc:0.976]
Epoch [78/120    avg_loss:0.016, val_acc:0.981]
Epoch [79/120    avg_loss:0.012, val_acc:0.984]
Epoch [80/120    avg_loss:0.014, val_acc:0.978]
Epoch [81/120    avg_loss:0.011, val_acc:0.980]
Epoch [82/120    avg_loss:0.010, val_acc:0.982]
Epoch [83/120    avg_loss:0.018, val_acc:0.973]
Epoch [84/120    avg_loss:0.016, val_acc:0.979]
Epoch [85/120    avg_loss:0.013, val_acc:0.980]
Epoch [86/120    avg_loss:0.011, val_acc:0.975]
Epoch [87/120    avg_loss:0.014, val_acc:0.985]
Epoch [88/120    avg_loss:0.011, val_acc:0.975]
Epoch [89/120    avg_loss:0.009, val_acc:0.978]
Epoch [90/120    avg_loss:0.008, val_acc:0.984]
Epoch [91/120    avg_loss:0.006, val_acc:0.981]
Epoch [92/120    avg_loss:0.009, val_acc:0.979]
Epoch [93/120    avg_loss:0.012, val_acc:0.979]
Epoch [94/120    avg_loss:0.009, val_acc:0.981]
Epoch [95/120    avg_loss:0.006, val_acc:0.980]
Epoch [96/120    avg_loss:0.006, val_acc:0.978]
Epoch [97/120    avg_loss:0.006, val_acc:0.985]
Epoch [98/120    avg_loss:0.004, val_acc:0.985]
Epoch [99/120    avg_loss:0.006, val_acc:0.981]
Epoch [100/120    avg_loss:0.005, val_acc:0.987]
Epoch [101/120    avg_loss:0.004, val_acc:0.986]
Epoch [102/120    avg_loss:0.005, val_acc:0.985]
Epoch [103/120    avg_loss:0.006, val_acc:0.977]
Epoch [104/120    avg_loss:0.011, val_acc:0.982]
Epoch [105/120    avg_loss:0.008, val_acc:0.978]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.011, val_acc:0.981]
Epoch [108/120    avg_loss:0.007, val_acc:0.984]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.989]
Epoch [112/120    avg_loss:0.013, val_acc:0.982]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.004, val_acc:0.980]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.004, val_acc:0.984]
Epoch [117/120    avg_loss:0.005, val_acc:0.985]
Epoch [118/120    avg_loss:0.007, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.986]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1260    2    0    1    0    0    0    2    9    8    3    0
     0    0    0]
 [   0    0    0  721    0    5    0    0    0    3    0    0   15    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0   13    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0  869    3    0    0
     0    0    0]
 [   0    0    8    0    0    0   20    0    0    0    8 2170    2    2
     0    0    0]
 [   0    0    0    0    0    8    0    0    0    0    0    0  523    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1137    1    0]
 [   0    0    0    0    0    0    6    0    0    1    0    0    0    0
    29  311    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.22222222222223

F1 scores:
[       nan 0.94871795 0.98707403 0.97762712 1.         0.97505669
 0.97907324 0.98039216 0.99883586 0.63414634 0.98414496 0.98816029
 0.96941613 0.98666667 0.98612316 0.94385432 0.97647059]

Kappa:
0.9797416782104107
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1f83a7a748>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.429, val_acc:0.436]
Epoch [2/120    avg_loss:1.838, val_acc:0.483]
Epoch [3/120    avg_loss:1.636, val_acc:0.582]
Epoch [4/120    avg_loss:1.359, val_acc:0.653]
Epoch [5/120    avg_loss:1.171, val_acc:0.707]
Epoch [6/120    avg_loss:0.997, val_acc:0.737]
Epoch [7/120    avg_loss:0.794, val_acc:0.823]
Epoch [8/120    avg_loss:0.607, val_acc:0.822]
Epoch [9/120    avg_loss:0.592, val_acc:0.817]
Epoch [10/120    avg_loss:0.560, val_acc:0.843]
Epoch [11/120    avg_loss:0.420, val_acc:0.851]
Epoch [12/120    avg_loss:0.396, val_acc:0.862]
Epoch [13/120    avg_loss:0.396, val_acc:0.877]
Epoch [14/120    avg_loss:0.270, val_acc:0.904]
Epoch [15/120    avg_loss:0.262, val_acc:0.882]
Epoch [16/120    avg_loss:0.241, val_acc:0.894]
Epoch [17/120    avg_loss:0.254, val_acc:0.913]
Epoch [18/120    avg_loss:0.217, val_acc:0.916]
Epoch [19/120    avg_loss:0.180, val_acc:0.932]
Epoch [20/120    avg_loss:0.170, val_acc:0.911]
Epoch [21/120    avg_loss:0.225, val_acc:0.934]
Epoch [22/120    avg_loss:0.149, val_acc:0.940]
Epoch [23/120    avg_loss:0.122, val_acc:0.947]
Epoch [24/120    avg_loss:0.102, val_acc:0.951]
Epoch [25/120    avg_loss:0.113, val_acc:0.940]
Epoch [26/120    avg_loss:0.103, val_acc:0.954]
Epoch [27/120    avg_loss:0.106, val_acc:0.922]
Epoch [28/120    avg_loss:0.114, val_acc:0.943]
Epoch [29/120    avg_loss:0.067, val_acc:0.966]
Epoch [30/120    avg_loss:0.056, val_acc:0.963]
Epoch [31/120    avg_loss:0.073, val_acc:0.960]
Epoch [32/120    avg_loss:0.059, val_acc:0.960]
Epoch [33/120    avg_loss:0.064, val_acc:0.959]
Epoch [34/120    avg_loss:0.066, val_acc:0.964]
Epoch [35/120    avg_loss:0.064, val_acc:0.966]
Epoch [36/120    avg_loss:0.048, val_acc:0.967]
Epoch [37/120    avg_loss:0.049, val_acc:0.963]
Epoch [38/120    avg_loss:0.050, val_acc:0.968]
Epoch [39/120    avg_loss:0.064, val_acc:0.965]
Epoch [40/120    avg_loss:0.063, val_acc:0.968]
Epoch [41/120    avg_loss:0.040, val_acc:0.974]
Epoch [42/120    avg_loss:0.036, val_acc:0.967]
Epoch [43/120    avg_loss:0.053, val_acc:0.963]
Epoch [44/120    avg_loss:0.052, val_acc:0.955]
Epoch [45/120    avg_loss:0.041, val_acc:0.970]
Epoch [46/120    avg_loss:0.042, val_acc:0.929]
Epoch [47/120    avg_loss:0.051, val_acc:0.962]
Epoch [48/120    avg_loss:0.044, val_acc:0.963]
Epoch [49/120    avg_loss:0.034, val_acc:0.969]
Epoch [50/120    avg_loss:0.023, val_acc:0.980]
Epoch [51/120    avg_loss:0.032, val_acc:0.969]
Epoch [52/120    avg_loss:0.025, val_acc:0.969]
Epoch [53/120    avg_loss:0.028, val_acc:0.984]
Epoch [54/120    avg_loss:0.038, val_acc:0.976]
Epoch [55/120    avg_loss:0.061, val_acc:0.963]
Epoch [56/120    avg_loss:0.046, val_acc:0.962]
Epoch [57/120    avg_loss:0.037, val_acc:0.980]
Epoch [58/120    avg_loss:0.023, val_acc:0.980]
Epoch [59/120    avg_loss:0.021, val_acc:0.974]
Epoch [60/120    avg_loss:0.022, val_acc:0.979]
Epoch [61/120    avg_loss:0.024, val_acc:0.980]
Epoch [62/120    avg_loss:0.017, val_acc:0.973]
Epoch [63/120    avg_loss:0.021, val_acc:0.974]
Epoch [64/120    avg_loss:0.018, val_acc:0.981]
Epoch [65/120    avg_loss:0.015, val_acc:0.976]
Epoch [66/120    avg_loss:0.012, val_acc:0.971]
Epoch [67/120    avg_loss:0.014, val_acc:0.976]
Epoch [68/120    avg_loss:0.010, val_acc:0.980]
Epoch [69/120    avg_loss:0.011, val_acc:0.981]
Epoch [70/120    avg_loss:0.012, val_acc:0.982]
Epoch [71/120    avg_loss:0.010, val_acc:0.980]
Epoch [72/120    avg_loss:0.009, val_acc:0.980]
Epoch [73/120    avg_loss:0.009, val_acc:0.981]
Epoch [74/120    avg_loss:0.008, val_acc:0.981]
Epoch [75/120    avg_loss:0.008, val_acc:0.981]
Epoch [76/120    avg_loss:0.010, val_acc:0.981]
Epoch [77/120    avg_loss:0.011, val_acc:0.980]
Epoch [78/120    avg_loss:0.007, val_acc:0.981]
Epoch [79/120    avg_loss:0.009, val_acc:0.981]
Epoch [80/120    avg_loss:0.007, val_acc:0.981]
Epoch [81/120    avg_loss:0.009, val_acc:0.981]
Epoch [82/120    avg_loss:0.010, val_acc:0.981]
Epoch [83/120    avg_loss:0.008, val_acc:0.981]
Epoch [84/120    avg_loss:0.009, val_acc:0.981]
Epoch [85/120    avg_loss:0.008, val_acc:0.981]
Epoch [86/120    avg_loss:0.008, val_acc:0.981]
Epoch [87/120    avg_loss:0.007, val_acc:0.981]
Epoch [88/120    avg_loss:0.010, val_acc:0.981]
Epoch [89/120    avg_loss:0.008, val_acc:0.981]
Epoch [90/120    avg_loss:0.008, val_acc:0.981]
Epoch [91/120    avg_loss:0.009, val_acc:0.981]
Epoch [92/120    avg_loss:0.008, val_acc:0.981]
Epoch [93/120    avg_loss:0.008, val_acc:0.981]
Epoch [94/120    avg_loss:0.008, val_acc:0.981]
Epoch [95/120    avg_loss:0.011, val_acc:0.981]
Epoch [96/120    avg_loss:0.011, val_acc:0.981]
Epoch [97/120    avg_loss:0.010, val_acc:0.981]
Epoch [98/120    avg_loss:0.009, val_acc:0.981]
Epoch [99/120    avg_loss:0.009, val_acc:0.981]
Epoch [100/120    avg_loss:0.008, val_acc:0.981]
Epoch [101/120    avg_loss:0.009, val_acc:0.981]
Epoch [102/120    avg_loss:0.007, val_acc:0.981]
Epoch [103/120    avg_loss:0.007, val_acc:0.981]
Epoch [104/120    avg_loss:0.009, val_acc:0.981]
Epoch [105/120    avg_loss:0.008, val_acc:0.981]
Epoch [106/120    avg_loss:0.008, val_acc:0.981]
Epoch [107/120    avg_loss:0.009, val_acc:0.981]
Epoch [108/120    avg_loss:0.008, val_acc:0.981]
Epoch [109/120    avg_loss:0.009, val_acc:0.981]
Epoch [110/120    avg_loss:0.008, val_acc:0.981]
Epoch [111/120    avg_loss:0.008, val_acc:0.981]
Epoch [112/120    avg_loss:0.011, val_acc:0.981]
Epoch [113/120    avg_loss:0.009, val_acc:0.981]
Epoch [114/120    avg_loss:0.009, val_acc:0.981]
Epoch [115/120    avg_loss:0.007, val_acc:0.981]
Epoch [116/120    avg_loss:0.010, val_acc:0.981]
Epoch [117/120    avg_loss:0.007, val_acc:0.981]
Epoch [118/120    avg_loss:0.009, val_acc:0.981]
Epoch [119/120    avg_loss:0.008, val_acc:0.981]
Epoch [120/120    avg_loss:0.010, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    0    0    0    1    0    0    3    7   12    2    0
     0    0    0]
 [   0    0    0  696    0   12    0    0    0   12    1    0   22    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10    5    0    6    2    0    0    0  830   19    0    0
     0    3    0]
 [   0    0    3    0    0    0    4    0    0    0    4 2197    0    2
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0  527    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    1    2    0    0
  1130    0    0]
 [   0    0    0    0    0    0    0    0    0    3    0    0    0    0
    33  311    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.91869918699187

F1 scores:
[       nan 0.98765432 0.98475967 0.96132597 1.         0.96644295
 0.99393939 0.98039216 1.         0.64285714 0.96623981 0.98963964
 0.97053407 0.98404255 0.9813287  0.93957704 0.97647059]

Kappa:
0.9762678822284544
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbfbe81e710>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.384, val_acc:0.391]
Epoch [2/120    avg_loss:1.914, val_acc:0.582]
Epoch [3/120    avg_loss:1.563, val_acc:0.590]
Epoch [4/120    avg_loss:1.346, val_acc:0.705]
Epoch [5/120    avg_loss:1.102, val_acc:0.732]
Epoch [6/120    avg_loss:1.004, val_acc:0.613]
Epoch [7/120    avg_loss:0.897, val_acc:0.679]
Epoch [8/120    avg_loss:0.888, val_acc:0.757]
Epoch [9/120    avg_loss:0.721, val_acc:0.790]
Epoch [10/120    avg_loss:0.612, val_acc:0.800]
Epoch [11/120    avg_loss:0.523, val_acc:0.807]
Epoch [12/120    avg_loss:0.396, val_acc:0.847]
Epoch [13/120    avg_loss:0.415, val_acc:0.848]
Epoch [14/120    avg_loss:0.308, val_acc:0.871]
Epoch [15/120    avg_loss:0.247, val_acc:0.885]
Epoch [16/120    avg_loss:0.315, val_acc:0.866]
Epoch [17/120    avg_loss:0.215, val_acc:0.897]
Epoch [18/120    avg_loss:0.395, val_acc:0.865]
Epoch [19/120    avg_loss:0.330, val_acc:0.871]
Epoch [20/120    avg_loss:0.305, val_acc:0.873]
Epoch [21/120    avg_loss:0.223, val_acc:0.886]
Epoch [22/120    avg_loss:0.168, val_acc:0.903]
Epoch [23/120    avg_loss:0.193, val_acc:0.894]
Epoch [24/120    avg_loss:0.163, val_acc:0.906]
Epoch [25/120    avg_loss:0.143, val_acc:0.922]
Epoch [26/120    avg_loss:0.111, val_acc:0.940]
Epoch [27/120    avg_loss:0.091, val_acc:0.956]
Epoch [28/120    avg_loss:0.109, val_acc:0.933]
Epoch [29/120    avg_loss:0.112, val_acc:0.935]
Epoch [30/120    avg_loss:0.091, val_acc:0.939]
Epoch [31/120    avg_loss:0.084, val_acc:0.935]
Epoch [32/120    avg_loss:0.087, val_acc:0.954]
Epoch [33/120    avg_loss:0.065, val_acc:0.950]
Epoch [34/120    avg_loss:0.055, val_acc:0.941]
Epoch [35/120    avg_loss:0.076, val_acc:0.956]
Epoch [36/120    avg_loss:0.056, val_acc:0.960]
Epoch [37/120    avg_loss:0.064, val_acc:0.968]
Epoch [38/120    avg_loss:0.052, val_acc:0.962]
Epoch [39/120    avg_loss:0.061, val_acc:0.964]
Epoch [40/120    avg_loss:0.057, val_acc:0.951]
Epoch [41/120    avg_loss:0.055, val_acc:0.959]
Epoch [42/120    avg_loss:0.054, val_acc:0.960]
Epoch [43/120    avg_loss:0.047, val_acc:0.942]
Epoch [44/120    avg_loss:0.046, val_acc:0.976]
Epoch [45/120    avg_loss:0.032, val_acc:0.963]
Epoch [46/120    avg_loss:0.037, val_acc:0.974]
Epoch [47/120    avg_loss:0.048, val_acc:0.967]
Epoch [48/120    avg_loss:0.050, val_acc:0.954]
Epoch [49/120    avg_loss:0.050, val_acc:0.960]
Epoch [50/120    avg_loss:0.057, val_acc:0.969]
Epoch [51/120    avg_loss:0.043, val_acc:0.953]
Epoch [52/120    avg_loss:0.056, val_acc:0.943]
Epoch [53/120    avg_loss:0.081, val_acc:0.953]
Epoch [54/120    avg_loss:0.057, val_acc:0.955]
Epoch [55/120    avg_loss:0.038, val_acc:0.968]
Epoch [56/120    avg_loss:0.043, val_acc:0.968]
Epoch [57/120    avg_loss:0.034, val_acc:0.970]
Epoch [58/120    avg_loss:0.025, val_acc:0.970]
Epoch [59/120    avg_loss:0.020, val_acc:0.973]
Epoch [60/120    avg_loss:0.023, val_acc:0.974]
Epoch [61/120    avg_loss:0.017, val_acc:0.974]
Epoch [62/120    avg_loss:0.015, val_acc:0.973]
Epoch [63/120    avg_loss:0.018, val_acc:0.976]
Epoch [64/120    avg_loss:0.014, val_acc:0.980]
Epoch [65/120    avg_loss:0.019, val_acc:0.978]
Epoch [66/120    avg_loss:0.019, val_acc:0.974]
Epoch [67/120    avg_loss:0.016, val_acc:0.975]
Epoch [68/120    avg_loss:0.019, val_acc:0.975]
Epoch [69/120    avg_loss:0.015, val_acc:0.981]
Epoch [70/120    avg_loss:0.014, val_acc:0.979]
Epoch [71/120    avg_loss:0.013, val_acc:0.976]
Epoch [72/120    avg_loss:0.014, val_acc:0.976]
Epoch [73/120    avg_loss:0.017, val_acc:0.979]
Epoch [74/120    avg_loss:0.015, val_acc:0.978]
Epoch [75/120    avg_loss:0.012, val_acc:0.981]
Epoch [76/120    avg_loss:0.012, val_acc:0.979]
Epoch [77/120    avg_loss:0.015, val_acc:0.981]
Epoch [78/120    avg_loss:0.015, val_acc:0.981]
Epoch [79/120    avg_loss:0.014, val_acc:0.982]
Epoch [80/120    avg_loss:0.016, val_acc:0.981]
Epoch [81/120    avg_loss:0.013, val_acc:0.979]
Epoch [82/120    avg_loss:0.014, val_acc:0.979]
Epoch [83/120    avg_loss:0.014, val_acc:0.981]
Epoch [84/120    avg_loss:0.014, val_acc:0.982]
Epoch [85/120    avg_loss:0.015, val_acc:0.980]
Epoch [86/120    avg_loss:0.012, val_acc:0.981]
Epoch [87/120    avg_loss:0.014, val_acc:0.981]
Epoch [88/120    avg_loss:0.016, val_acc:0.980]
Epoch [89/120    avg_loss:0.015, val_acc:0.981]
Epoch [90/120    avg_loss:0.013, val_acc:0.983]
Epoch [91/120    avg_loss:0.011, val_acc:0.981]
Epoch [92/120    avg_loss:0.015, val_acc:0.983]
Epoch [93/120    avg_loss:0.018, val_acc:0.983]
Epoch [94/120    avg_loss:0.014, val_acc:0.982]
Epoch [95/120    avg_loss:0.016, val_acc:0.983]
Epoch [96/120    avg_loss:0.012, val_acc:0.982]
Epoch [97/120    avg_loss:0.012, val_acc:0.983]
Epoch [98/120    avg_loss:0.012, val_acc:0.983]
Epoch [99/120    avg_loss:0.014, val_acc:0.980]
Epoch [100/120    avg_loss:0.015, val_acc:0.982]
Epoch [101/120    avg_loss:0.012, val_acc:0.982]
Epoch [102/120    avg_loss:0.011, val_acc:0.982]
Epoch [103/120    avg_loss:0.014, val_acc:0.984]
Epoch [104/120    avg_loss:0.012, val_acc:0.982]
Epoch [105/120    avg_loss:0.013, val_acc:0.982]
Epoch [106/120    avg_loss:0.010, val_acc:0.980]
Epoch [107/120    avg_loss:0.014, val_acc:0.983]
Epoch [108/120    avg_loss:0.012, val_acc:0.983]
Epoch [109/120    avg_loss:0.009, val_acc:0.983]
Epoch [110/120    avg_loss:0.012, val_acc:0.982]
Epoch [111/120    avg_loss:0.011, val_acc:0.982]
Epoch [112/120    avg_loss:0.010, val_acc:0.983]
Epoch [113/120    avg_loss:0.010, val_acc:0.983]
Epoch [114/120    avg_loss:0.008, val_acc:0.983]
Epoch [115/120    avg_loss:0.014, val_acc:0.985]
Epoch [116/120    avg_loss:0.009, val_acc:0.983]
Epoch [117/120    avg_loss:0.013, val_acc:0.985]
Epoch [118/120    avg_loss:0.012, val_acc:0.981]
Epoch [119/120    avg_loss:0.013, val_acc:0.985]
Epoch [120/120    avg_loss:0.011, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1254    0    4    0    1    0    0    0    5   18    3    0
     0    0    0]
 [   0    0    0  725    0    4    0    0    0   12    0    0    3    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    1    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    5    1    0    0    0  848   17    0    0
     0    4    0]
 [   0    0    1    0    0    0    2    0    0    0    5 2200    1    1
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    0    0  527    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    47  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.27642276422765

F1 scores:
[       nan 0.98765432 0.98740157 0.98505435 0.99069767 0.9784336
 0.9939302  0.98039216 0.99883586 0.73469388 0.97752161 0.98943108
 0.98229264 0.98930481 0.97850387 0.9183359  0.96969697]

Kappa:
0.9803386366971337
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0f19f0c710>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.356, val_acc:0.564]
Epoch [2/120    avg_loss:1.797, val_acc:0.609]
Epoch [3/120    avg_loss:1.476, val_acc:0.640]
Epoch [4/120    avg_loss:1.229, val_acc:0.655]
Epoch [5/120    avg_loss:1.026, val_acc:0.746]
Epoch [6/120    avg_loss:0.892, val_acc:0.763]
Epoch [7/120    avg_loss:0.724, val_acc:0.786]
Epoch [8/120    avg_loss:0.629, val_acc:0.823]
Epoch [9/120    avg_loss:0.560, val_acc:0.866]
Epoch [10/120    avg_loss:0.456, val_acc:0.852]
Epoch [11/120    avg_loss:0.442, val_acc:0.831]
Epoch [12/120    avg_loss:0.379, val_acc:0.898]
Epoch [13/120    avg_loss:0.342, val_acc:0.898]
Epoch [14/120    avg_loss:0.320, val_acc:0.885]
Epoch [15/120    avg_loss:0.312, val_acc:0.872]
Epoch [16/120    avg_loss:0.287, val_acc:0.906]
Epoch [17/120    avg_loss:0.490, val_acc:0.898]
Epoch [18/120    avg_loss:0.300, val_acc:0.918]
Epoch [19/120    avg_loss:0.265, val_acc:0.931]
Epoch [20/120    avg_loss:0.189, val_acc:0.930]
Epoch [21/120    avg_loss:0.193, val_acc:0.942]
Epoch [22/120    avg_loss:0.144, val_acc:0.948]
Epoch [23/120    avg_loss:0.158, val_acc:0.930]
Epoch [24/120    avg_loss:0.133, val_acc:0.933]
Epoch [25/120    avg_loss:0.128, val_acc:0.930]
Epoch [26/120    avg_loss:0.134, val_acc:0.945]
Epoch [27/120    avg_loss:0.134, val_acc:0.932]
Epoch [28/120    avg_loss:0.108, val_acc:0.939]
Epoch [29/120    avg_loss:0.105, val_acc:0.959]
Epoch [30/120    avg_loss:0.101, val_acc:0.955]
Epoch [31/120    avg_loss:0.064, val_acc:0.956]
Epoch [32/120    avg_loss:0.084, val_acc:0.958]
Epoch [33/120    avg_loss:0.073, val_acc:0.965]
Epoch [34/120    avg_loss:0.075, val_acc:0.963]
Epoch [35/120    avg_loss:0.090, val_acc:0.934]
Epoch [36/120    avg_loss:0.112, val_acc:0.955]
Epoch [37/120    avg_loss:0.081, val_acc:0.958]
Epoch [38/120    avg_loss:0.086, val_acc:0.942]
Epoch [39/120    avg_loss:0.095, val_acc:0.950]
Epoch [40/120    avg_loss:0.089, val_acc:0.954]
Epoch [41/120    avg_loss:0.068, val_acc:0.967]
Epoch [42/120    avg_loss:0.052, val_acc:0.974]
Epoch [43/120    avg_loss:0.058, val_acc:0.951]
Epoch [44/120    avg_loss:0.051, val_acc:0.965]
Epoch [45/120    avg_loss:0.034, val_acc:0.966]
Epoch [46/120    avg_loss:0.044, val_acc:0.963]
Epoch [47/120    avg_loss:0.057, val_acc:0.964]
Epoch [48/120    avg_loss:0.055, val_acc:0.973]
Epoch [49/120    avg_loss:0.036, val_acc:0.973]
Epoch [50/120    avg_loss:0.030, val_acc:0.974]
Epoch [51/120    avg_loss:0.044, val_acc:0.970]
Epoch [52/120    avg_loss:0.033, val_acc:0.953]
Epoch [53/120    avg_loss:0.036, val_acc:0.977]
Epoch [54/120    avg_loss:0.032, val_acc:0.981]
Epoch [55/120    avg_loss:0.030, val_acc:0.980]
Epoch [56/120    avg_loss:0.021, val_acc:0.978]
Epoch [57/120    avg_loss:0.018, val_acc:0.985]
Epoch [58/120    avg_loss:0.025, val_acc:0.976]
Epoch [59/120    avg_loss:0.028, val_acc:0.981]
Epoch [60/120    avg_loss:0.052, val_acc:0.964]
Epoch [61/120    avg_loss:0.049, val_acc:0.980]
Epoch [62/120    avg_loss:0.029, val_acc:0.986]
Epoch [63/120    avg_loss:0.020, val_acc:0.985]
Epoch [64/120    avg_loss:0.021, val_acc:0.986]
Epoch [65/120    avg_loss:0.015, val_acc:0.989]
Epoch [66/120    avg_loss:0.023, val_acc:0.985]
Epoch [67/120    avg_loss:0.020, val_acc:0.989]
Epoch [68/120    avg_loss:0.020, val_acc:0.989]
Epoch [69/120    avg_loss:0.018, val_acc:0.989]
Epoch [70/120    avg_loss:0.013, val_acc:0.984]
Epoch [71/120    avg_loss:0.023, val_acc:0.988]
Epoch [72/120    avg_loss:0.038, val_acc:0.982]
Epoch [73/120    avg_loss:0.017, val_acc:0.990]
Epoch [74/120    avg_loss:0.010, val_acc:0.992]
Epoch [75/120    avg_loss:0.012, val_acc:0.988]
Epoch [76/120    avg_loss:0.010, val_acc:0.989]
Epoch [77/120    avg_loss:0.010, val_acc:0.991]
Epoch [78/120    avg_loss:0.010, val_acc:0.986]
Epoch [79/120    avg_loss:0.018, val_acc:0.982]
Epoch [80/120    avg_loss:0.022, val_acc:0.974]
Epoch [81/120    avg_loss:0.016, val_acc:0.987]
Epoch [82/120    avg_loss:0.010, val_acc:0.986]
Epoch [83/120    avg_loss:0.016, val_acc:0.984]
Epoch [84/120    avg_loss:0.014, val_acc:0.982]
Epoch [85/120    avg_loss:0.010, val_acc:0.989]
Epoch [86/120    avg_loss:0.013, val_acc:0.989]
Epoch [87/120    avg_loss:0.008, val_acc:0.987]
Epoch [88/120    avg_loss:0.008, val_acc:0.990]
Epoch [89/120    avg_loss:0.007, val_acc:0.990]
Epoch [90/120    avg_loss:0.007, val_acc:0.990]
Epoch [91/120    avg_loss:0.007, val_acc:0.991]
Epoch [92/120    avg_loss:0.006, val_acc:0.991]
Epoch [93/120    avg_loss:0.004, val_acc:0.991]
Epoch [94/120    avg_loss:0.007, val_acc:0.990]
Epoch [95/120    avg_loss:0.007, val_acc:0.990]
Epoch [96/120    avg_loss:0.005, val_acc:0.991]
Epoch [97/120    avg_loss:0.006, val_acc:0.991]
Epoch [98/120    avg_loss:0.005, val_acc:0.991]
Epoch [99/120    avg_loss:0.006, val_acc:0.990]
Epoch [100/120    avg_loss:0.005, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.990]
Epoch [103/120    avg_loss:0.005, val_acc:0.990]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.990]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.006, val_acc:0.990]
Epoch [109/120    avg_loss:0.005, val_acc:0.990]
Epoch [110/120    avg_loss:0.009, val_acc:0.990]
Epoch [111/120    avg_loss:0.005, val_acc:0.990]
Epoch [112/120    avg_loss:0.006, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.990]
Epoch [114/120    avg_loss:0.006, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.004, val_acc:0.990]
Epoch [117/120    avg_loss:0.006, val_acc:0.990]
Epoch [118/120    avg_loss:0.006, val_acc:0.990]
Epoch [119/120    avg_loss:0.006, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1270    0    0    0    0    0    0    1    4    5    3    0
     0    2    0]
 [   0    0    0  722    0    0    0    0    0   11    2    0   11    1
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    1    4    0    2    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    5    2    0    0    0  846   13    0    0
     1    2    0]
 [   0    0    9    0    0    0    3    0    0    0    1 2196    0    1
     0    0    0]
 [   0    0    3    0    0    7    0    0    0    0    5    0  517    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    15  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.40650406504065

F1 scores:
[       nan 0.98765432 0.98679099 0.98231293 0.99764706 0.9771167
 0.98271976 0.92592593 0.99883586 0.72       0.97577855 0.99186992
 0.96725912 0.99462366 0.99258613 0.95067265 0.97005988]

Kappa:
0.9818296082137528
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f995dac86d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.436, val_acc:0.398]
Epoch [2/120    avg_loss:1.821, val_acc:0.548]
Epoch [3/120    avg_loss:1.494, val_acc:0.645]
Epoch [4/120    avg_loss:1.273, val_acc:0.692]
Epoch [5/120    avg_loss:1.117, val_acc:0.716]
Epoch [6/120    avg_loss:0.986, val_acc:0.732]
Epoch [7/120    avg_loss:0.725, val_acc:0.779]
Epoch [8/120    avg_loss:0.624, val_acc:0.784]
Epoch [9/120    avg_loss:0.582, val_acc:0.788]
Epoch [10/120    avg_loss:0.496, val_acc:0.793]
Epoch [11/120    avg_loss:0.427, val_acc:0.844]
Epoch [12/120    avg_loss:0.384, val_acc:0.854]
Epoch [13/120    avg_loss:0.316, val_acc:0.890]
Epoch [14/120    avg_loss:0.301, val_acc:0.886]
Epoch [15/120    avg_loss:0.267, val_acc:0.905]
Epoch [16/120    avg_loss:0.212, val_acc:0.924]
Epoch [17/120    avg_loss:0.205, val_acc:0.876]
Epoch [18/120    avg_loss:0.235, val_acc:0.875]
Epoch [19/120    avg_loss:0.189, val_acc:0.917]
Epoch [20/120    avg_loss:0.273, val_acc:0.886]
Epoch [21/120    avg_loss:0.202, val_acc:0.897]
Epoch [22/120    avg_loss:0.173, val_acc:0.921]
Epoch [23/120    avg_loss:0.170, val_acc:0.923]
Epoch [24/120    avg_loss:0.130, val_acc:0.923]
Epoch [25/120    avg_loss:0.106, val_acc:0.940]
Epoch [26/120    avg_loss:0.123, val_acc:0.946]
Epoch [27/120    avg_loss:0.095, val_acc:0.950]
Epoch [28/120    avg_loss:0.070, val_acc:0.953]
Epoch [29/120    avg_loss:0.106, val_acc:0.938]
Epoch [30/120    avg_loss:0.091, val_acc:0.946]
Epoch [31/120    avg_loss:0.097, val_acc:0.943]
Epoch [32/120    avg_loss:0.096, val_acc:0.947]
Epoch [33/120    avg_loss:0.072, val_acc:0.946]
Epoch [34/120    avg_loss:0.086, val_acc:0.939]
Epoch [35/120    avg_loss:0.067, val_acc:0.942]
Epoch [36/120    avg_loss:0.076, val_acc:0.946]
Epoch [37/120    avg_loss:0.073, val_acc:0.953]
Epoch [38/120    avg_loss:0.085, val_acc:0.954]
Epoch [39/120    avg_loss:0.052, val_acc:0.955]
Epoch [40/120    avg_loss:0.065, val_acc:0.925]
Epoch [41/120    avg_loss:0.090, val_acc:0.958]
Epoch [42/120    avg_loss:0.075, val_acc:0.936]
Epoch [43/120    avg_loss:0.091, val_acc:0.943]
Epoch [44/120    avg_loss:0.063, val_acc:0.964]
Epoch [45/120    avg_loss:0.065, val_acc:0.963]
Epoch [46/120    avg_loss:0.112, val_acc:0.916]
Epoch [47/120    avg_loss:0.141, val_acc:0.908]
Epoch [48/120    avg_loss:0.213, val_acc:0.939]
Epoch [49/120    avg_loss:0.093, val_acc:0.951]
Epoch [50/120    avg_loss:0.069, val_acc:0.957]
Epoch [51/120    avg_loss:0.089, val_acc:0.928]
Epoch [52/120    avg_loss:0.061, val_acc:0.952]
Epoch [53/120    avg_loss:0.048, val_acc:0.944]
Epoch [54/120    avg_loss:0.050, val_acc:0.953]
Epoch [55/120    avg_loss:0.040, val_acc:0.959]
Epoch [56/120    avg_loss:0.048, val_acc:0.959]
Epoch [57/120    avg_loss:0.032, val_acc:0.966]
Epoch [58/120    avg_loss:0.032, val_acc:0.961]
Epoch [59/120    avg_loss:0.031, val_acc:0.958]
Epoch [60/120    avg_loss:0.025, val_acc:0.967]
Epoch [61/120    avg_loss:0.030, val_acc:0.959]
Epoch [62/120    avg_loss:0.029, val_acc:0.969]
Epoch [63/120    avg_loss:0.037, val_acc:0.968]
Epoch [64/120    avg_loss:0.023, val_acc:0.973]
Epoch [65/120    avg_loss:0.013, val_acc:0.977]
Epoch [66/120    avg_loss:0.018, val_acc:0.966]
Epoch [67/120    avg_loss:0.016, val_acc:0.975]
Epoch [68/120    avg_loss:0.017, val_acc:0.973]
Epoch [69/120    avg_loss:0.018, val_acc:0.971]
Epoch [70/120    avg_loss:0.020, val_acc:0.973]
Epoch [71/120    avg_loss:0.021, val_acc:0.978]
Epoch [72/120    avg_loss:0.094, val_acc:0.890]
Epoch [73/120    avg_loss:0.393, val_acc:0.919]
Epoch [74/120    avg_loss:0.170, val_acc:0.909]
Epoch [75/120    avg_loss:0.115, val_acc:0.945]
Epoch [76/120    avg_loss:0.073, val_acc:0.941]
Epoch [77/120    avg_loss:0.049, val_acc:0.957]
Epoch [78/120    avg_loss:0.070, val_acc:0.957]
Epoch [79/120    avg_loss:0.051, val_acc:0.961]
Epoch [80/120    avg_loss:0.027, val_acc:0.964]
Epoch [81/120    avg_loss:0.032, val_acc:0.976]
Epoch [82/120    avg_loss:0.040, val_acc:0.947]
Epoch [83/120    avg_loss:0.034, val_acc:0.967]
Epoch [84/120    avg_loss:0.029, val_acc:0.962]
Epoch [85/120    avg_loss:0.017, val_acc:0.971]
Epoch [86/120    avg_loss:0.014, val_acc:0.971]
Epoch [87/120    avg_loss:0.015, val_acc:0.971]
Epoch [88/120    avg_loss:0.012, val_acc:0.973]
Epoch [89/120    avg_loss:0.013, val_acc:0.974]
Epoch [90/120    avg_loss:0.011, val_acc:0.975]
Epoch [91/120    avg_loss:0.015, val_acc:0.974]
Epoch [92/120    avg_loss:0.015, val_acc:0.975]
Epoch [93/120    avg_loss:0.012, val_acc:0.975]
Epoch [94/120    avg_loss:0.014, val_acc:0.976]
Epoch [95/120    avg_loss:0.016, val_acc:0.977]
Epoch [96/120    avg_loss:0.010, val_acc:0.976]
Epoch [97/120    avg_loss:0.016, val_acc:0.976]
Epoch [98/120    avg_loss:0.010, val_acc:0.976]
Epoch [99/120    avg_loss:0.014, val_acc:0.976]
Epoch [100/120    avg_loss:0.012, val_acc:0.976]
Epoch [101/120    avg_loss:0.010, val_acc:0.976]
Epoch [102/120    avg_loss:0.014, val_acc:0.976]
Epoch [103/120    avg_loss:0.014, val_acc:0.976]
Epoch [104/120    avg_loss:0.012, val_acc:0.976]
Epoch [105/120    avg_loss:0.017, val_acc:0.976]
Epoch [106/120    avg_loss:0.011, val_acc:0.976]
Epoch [107/120    avg_loss:0.010, val_acc:0.976]
Epoch [108/120    avg_loss:0.012, val_acc:0.976]
Epoch [109/120    avg_loss:0.011, val_acc:0.976]
Epoch [110/120    avg_loss:0.010, val_acc:0.976]
Epoch [111/120    avg_loss:0.011, val_acc:0.976]
Epoch [112/120    avg_loss:0.012, val_acc:0.976]
Epoch [113/120    avg_loss:0.015, val_acc:0.976]
Epoch [114/120    avg_loss:0.012, val_acc:0.976]
Epoch [115/120    avg_loss:0.012, val_acc:0.976]
Epoch [116/120    avg_loss:0.013, val_acc:0.976]
Epoch [117/120    avg_loss:0.012, val_acc:0.976]
Epoch [118/120    avg_loss:0.012, val_acc:0.976]
Epoch [119/120    avg_loss:0.009, val_acc:0.976]
Epoch [120/120    avg_loss:0.013, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1273    0    0    0    1    0    0    0    5    4    1    0
     0    1    0]
 [   0    0    0  693    2   14    0    0    0    6    1    0   24    7
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    6    4    0    0    0  845   13    0    0
     0    2    0]
 [   0    0    9    0    0    0    2    0    0    0    8 2185    4    2
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    3    1  521    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    1    0    0    0
  1126   10    0]
 [   0    0    0    0    0    1   12    0    0    0    0    0    0    0
    20  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.98373983739837

F1 scores:
[       nan 0.96202532 0.98989114 0.9625     0.9953271  0.96651786
 0.98498498 0.98039216 0.99883856 0.8372093  0.97070649 0.99025606
 0.96125461 0.9762533  0.98512686 0.93175074 0.98245614]

Kappa:
0.9770203336666035
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbeaefd77b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.462, val_acc:0.434]
Epoch [2/120    avg_loss:1.883, val_acc:0.546]
Epoch [3/120    avg_loss:1.606, val_acc:0.622]
Epoch [4/120    avg_loss:1.367, val_acc:0.674]
Epoch [5/120    avg_loss:1.116, val_acc:0.710]
Epoch [6/120    avg_loss:0.993, val_acc:0.753]
Epoch [7/120    avg_loss:0.821, val_acc:0.798]
Epoch [8/120    avg_loss:0.717, val_acc:0.779]
Epoch [9/120    avg_loss:0.533, val_acc:0.833]
Epoch [10/120    avg_loss:0.554, val_acc:0.854]
Epoch [11/120    avg_loss:0.394, val_acc:0.889]
Epoch [12/120    avg_loss:0.347, val_acc:0.919]
Epoch [13/120    avg_loss:0.236, val_acc:0.918]
Epoch [14/120    avg_loss:0.233, val_acc:0.929]
Epoch [15/120    avg_loss:0.263, val_acc:0.888]
Epoch [16/120    avg_loss:0.223, val_acc:0.927]
Epoch [17/120    avg_loss:0.135, val_acc:0.934]
Epoch [18/120    avg_loss:0.134, val_acc:0.938]
Epoch [19/120    avg_loss:0.142, val_acc:0.923]
Epoch [20/120    avg_loss:0.134, val_acc:0.941]
Epoch [21/120    avg_loss:0.139, val_acc:0.936]
Epoch [22/120    avg_loss:0.112, val_acc:0.947]
Epoch [23/120    avg_loss:0.164, val_acc:0.911]
Epoch [24/120    avg_loss:0.143, val_acc:0.943]
Epoch [25/120    avg_loss:0.106, val_acc:0.967]
Epoch [26/120    avg_loss:0.091, val_acc:0.950]
Epoch [27/120    avg_loss:0.080, val_acc:0.964]
Epoch [28/120    avg_loss:0.087, val_acc:0.963]
Epoch [29/120    avg_loss:0.076, val_acc:0.971]
Epoch [30/120    avg_loss:0.074, val_acc:0.943]
Epoch [31/120    avg_loss:0.080, val_acc:0.969]
Epoch [32/120    avg_loss:0.083, val_acc:0.953]
Epoch [33/120    avg_loss:0.093, val_acc:0.962]
Epoch [34/120    avg_loss:0.067, val_acc:0.960]
Epoch [35/120    avg_loss:0.061, val_acc:0.974]
Epoch [36/120    avg_loss:0.050, val_acc:0.978]
Epoch [37/120    avg_loss:0.064, val_acc:0.975]
Epoch [38/120    avg_loss:0.045, val_acc:0.960]
Epoch [39/120    avg_loss:0.038, val_acc:0.983]
Epoch [40/120    avg_loss:0.051, val_acc:0.973]
Epoch [41/120    avg_loss:0.052, val_acc:0.968]
Epoch [42/120    avg_loss:0.048, val_acc:0.953]
Epoch [43/120    avg_loss:0.033, val_acc:0.975]
Epoch [44/120    avg_loss:0.029, val_acc:0.982]
Epoch [45/120    avg_loss:0.023, val_acc:0.980]
Epoch [46/120    avg_loss:0.035, val_acc:0.979]
Epoch [47/120    avg_loss:0.029, val_acc:0.983]
Epoch [48/120    avg_loss:0.016, val_acc:0.981]
Epoch [49/120    avg_loss:0.021, val_acc:0.983]
Epoch [50/120    avg_loss:0.022, val_acc:0.979]
Epoch [51/120    avg_loss:0.074, val_acc:0.954]
Epoch [52/120    avg_loss:0.049, val_acc:0.969]
Epoch [53/120    avg_loss:0.072, val_acc:0.969]
Epoch [54/120    avg_loss:0.054, val_acc:0.973]
Epoch [55/120    avg_loss:0.038, val_acc:0.979]
Epoch [56/120    avg_loss:0.024, val_acc:0.982]
Epoch [57/120    avg_loss:0.028, val_acc:0.978]
Epoch [58/120    avg_loss:0.028, val_acc:0.983]
Epoch [59/120    avg_loss:0.021, val_acc:0.981]
Epoch [60/120    avg_loss:0.017, val_acc:0.985]
Epoch [61/120    avg_loss:0.015, val_acc:0.985]
Epoch [62/120    avg_loss:0.021, val_acc:0.981]
Epoch [63/120    avg_loss:0.030, val_acc:0.976]
Epoch [64/120    avg_loss:0.028, val_acc:0.980]
Epoch [65/120    avg_loss:0.016, val_acc:0.981]
Epoch [66/120    avg_loss:0.018, val_acc:0.982]
Epoch [67/120    avg_loss:0.017, val_acc:0.986]
Epoch [68/120    avg_loss:0.015, val_acc:0.979]
Epoch [69/120    avg_loss:0.032, val_acc:0.983]
Epoch [70/120    avg_loss:0.016, val_acc:0.986]
Epoch [71/120    avg_loss:0.014, val_acc:0.988]
Epoch [72/120    avg_loss:0.016, val_acc:0.985]
Epoch [73/120    avg_loss:0.015, val_acc:0.983]
Epoch [74/120    avg_loss:0.022, val_acc:0.976]
Epoch [75/120    avg_loss:0.014, val_acc:0.987]
Epoch [76/120    avg_loss:0.009, val_acc:0.985]
Epoch [77/120    avg_loss:0.011, val_acc:0.987]
Epoch [78/120    avg_loss:0.008, val_acc:0.986]
Epoch [79/120    avg_loss:0.011, val_acc:0.981]
Epoch [80/120    avg_loss:0.016, val_acc:0.983]
Epoch [81/120    avg_loss:0.010, val_acc:0.986]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.007, val_acc:0.984]
Epoch [84/120    avg_loss:0.007, val_acc:0.988]
Epoch [85/120    avg_loss:0.012, val_acc:0.990]
Epoch [86/120    avg_loss:0.007, val_acc:0.986]
Epoch [87/120    avg_loss:0.005, val_acc:0.987]
Epoch [88/120    avg_loss:0.007, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.988]
Epoch [90/120    avg_loss:0.006, val_acc:0.983]
Epoch [91/120    avg_loss:0.009, val_acc:0.985]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.006, val_acc:0.987]
Epoch [94/120    avg_loss:0.008, val_acc:0.987]
Epoch [95/120    avg_loss:0.007, val_acc:0.985]
Epoch [96/120    avg_loss:0.006, val_acc:0.988]
Epoch [97/120    avg_loss:0.004, val_acc:0.988]
Epoch [98/120    avg_loss:0.006, val_acc:0.989]
Epoch [99/120    avg_loss:0.004, val_acc:0.989]
Epoch [100/120    avg_loss:0.005, val_acc:0.991]
Epoch [101/120    avg_loss:0.003, val_acc:0.991]
Epoch [102/120    avg_loss:0.004, val_acc:0.993]
Epoch [103/120    avg_loss:0.005, val_acc:0.993]
Epoch [104/120    avg_loss:0.003, val_acc:0.993]
Epoch [105/120    avg_loss:0.004, val_acc:0.993]
Epoch [106/120    avg_loss:0.004, val_acc:0.991]
Epoch [107/120    avg_loss:0.005, val_acc:0.991]
Epoch [108/120    avg_loss:0.004, val_acc:0.990]
Epoch [109/120    avg_loss:0.004, val_acc:0.990]
Epoch [110/120    avg_loss:0.003, val_acc:0.990]
Epoch [111/120    avg_loss:0.004, val_acc:0.990]
Epoch [112/120    avg_loss:0.004, val_acc:0.990]
Epoch [113/120    avg_loss:0.003, val_acc:0.990]
Epoch [114/120    avg_loss:0.004, val_acc:0.990]
Epoch [115/120    avg_loss:0.003, val_acc:0.990]
Epoch [116/120    avg_loss:0.006, val_acc:0.990]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.988]
Epoch [119/120    avg_loss:0.004, val_acc:0.988]
Epoch [120/120    avg_loss:0.003, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1264    0    5    0    0    0    0    1   10    3    2    0
     0    0    0]
 [   0    0    0  720    2   12    0    0    0    8    1    1    2    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    6    0    0    0    0  855    4    0    0
     1    2    0]
 [   0    0    8    0    0    0    1    0    0    0    2 2198    0    1
     0    0    0]
 [   0    0    0    0    0    8    0    0    0    0    0    7  517    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1134    3    0]
 [   0    0    0    0    0    0   21    0    0    0    0    0    0    0
    16  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.46070460704607

F1 scores:
[       nan 1.         0.98595944 0.98159509 0.98383372 0.96868009
 0.98353293 1.         0.99883856 0.76595745 0.98050459 0.99389555
 0.97916667 0.99462366 0.99039301 0.93655589 0.98224852]

Kappa:
0.9824513110456119
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f47275467f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.444, val_acc:0.498]
Epoch [2/120    avg_loss:1.921, val_acc:0.575]
Epoch [3/120    avg_loss:1.643, val_acc:0.599]
Epoch [4/120    avg_loss:1.471, val_acc:0.650]
Epoch [5/120    avg_loss:1.220, val_acc:0.694]
Epoch [6/120    avg_loss:1.030, val_acc:0.731]
Epoch [7/120    avg_loss:0.863, val_acc:0.786]
Epoch [8/120    avg_loss:0.714, val_acc:0.788]
Epoch [9/120    avg_loss:0.624, val_acc:0.809]
Epoch [10/120    avg_loss:0.566, val_acc:0.851]
Epoch [11/120    avg_loss:0.485, val_acc:0.833]
Epoch [12/120    avg_loss:0.432, val_acc:0.877]
Epoch [13/120    avg_loss:0.391, val_acc:0.910]
Epoch [14/120    avg_loss:0.330, val_acc:0.839]
Epoch [15/120    avg_loss:0.320, val_acc:0.892]
Epoch [16/120    avg_loss:0.228, val_acc:0.911]
Epoch [17/120    avg_loss:0.223, val_acc:0.918]
Epoch [18/120    avg_loss:0.246, val_acc:0.919]
Epoch [19/120    avg_loss:0.257, val_acc:0.899]
Epoch [20/120    avg_loss:0.183, val_acc:0.925]
Epoch [21/120    avg_loss:0.182, val_acc:0.891]
Epoch [22/120    avg_loss:0.146, val_acc:0.943]
Epoch [23/120    avg_loss:0.177, val_acc:0.936]
Epoch [24/120    avg_loss:0.120, val_acc:0.954]
Epoch [25/120    avg_loss:0.099, val_acc:0.947]
Epoch [26/120    avg_loss:0.135, val_acc:0.939]
Epoch [27/120    avg_loss:0.093, val_acc:0.962]
Epoch [28/120    avg_loss:0.100, val_acc:0.971]
Epoch [29/120    avg_loss:0.072, val_acc:0.967]
Epoch [30/120    avg_loss:0.090, val_acc:0.960]
Epoch [31/120    avg_loss:0.100, val_acc:0.968]
Epoch [32/120    avg_loss:0.082, val_acc:0.963]
Epoch [33/120    avg_loss:0.097, val_acc:0.952]
Epoch [34/120    avg_loss:0.074, val_acc:0.963]
Epoch [35/120    avg_loss:0.073, val_acc:0.963]
Epoch [36/120    avg_loss:0.074, val_acc:0.954]
Epoch [37/120    avg_loss:0.284, val_acc:0.907]
Epoch [38/120    avg_loss:0.148, val_acc:0.941]
Epoch [39/120    avg_loss:0.098, val_acc:0.953]
Epoch [40/120    avg_loss:0.065, val_acc:0.957]
Epoch [41/120    avg_loss:0.071, val_acc:0.964]
Epoch [42/120    avg_loss:0.062, val_acc:0.970]
Epoch [43/120    avg_loss:0.051, val_acc:0.976]
Epoch [44/120    avg_loss:0.044, val_acc:0.976]
Epoch [45/120    avg_loss:0.037, val_acc:0.976]
Epoch [46/120    avg_loss:0.041, val_acc:0.975]
Epoch [47/120    avg_loss:0.043, val_acc:0.981]
Epoch [48/120    avg_loss:0.035, val_acc:0.979]
Epoch [49/120    avg_loss:0.036, val_acc:0.979]
Epoch [50/120    avg_loss:0.040, val_acc:0.980]
Epoch [51/120    avg_loss:0.039, val_acc:0.980]
Epoch [52/120    avg_loss:0.036, val_acc:0.979]
Epoch [53/120    avg_loss:0.034, val_acc:0.979]
Epoch [54/120    avg_loss:0.034, val_acc:0.979]
Epoch [55/120    avg_loss:0.034, val_acc:0.980]
Epoch [56/120    avg_loss:0.032, val_acc:0.980]
Epoch [57/120    avg_loss:0.029, val_acc:0.980]
Epoch [58/120    avg_loss:0.032, val_acc:0.980]
Epoch [59/120    avg_loss:0.033, val_acc:0.979]
Epoch [60/120    avg_loss:0.030, val_acc:0.980]
Epoch [61/120    avg_loss:0.034, val_acc:0.980]
Epoch [62/120    avg_loss:0.027, val_acc:0.980]
Epoch [63/120    avg_loss:0.033, val_acc:0.980]
Epoch [64/120    avg_loss:0.031, val_acc:0.980]
Epoch [65/120    avg_loss:0.030, val_acc:0.980]
Epoch [66/120    avg_loss:0.034, val_acc:0.980]
Epoch [67/120    avg_loss:0.031, val_acc:0.980]
Epoch [68/120    avg_loss:0.029, val_acc:0.980]
Epoch [69/120    avg_loss:0.032, val_acc:0.980]
Epoch [70/120    avg_loss:0.030, val_acc:0.980]
Epoch [71/120    avg_loss:0.034, val_acc:0.980]
Epoch [72/120    avg_loss:0.028, val_acc:0.980]
Epoch [73/120    avg_loss:0.030, val_acc:0.980]
Epoch [74/120    avg_loss:0.027, val_acc:0.980]
Epoch [75/120    avg_loss:0.033, val_acc:0.980]
Epoch [76/120    avg_loss:0.032, val_acc:0.980]
Epoch [77/120    avg_loss:0.033, val_acc:0.980]
Epoch [78/120    avg_loss:0.029, val_acc:0.980]
Epoch [79/120    avg_loss:0.029, val_acc:0.980]
Epoch [80/120    avg_loss:0.030, val_acc:0.980]
Epoch [81/120    avg_loss:0.027, val_acc:0.980]
Epoch [82/120    avg_loss:0.033, val_acc:0.980]
Epoch [83/120    avg_loss:0.033, val_acc:0.980]
Epoch [84/120    avg_loss:0.030, val_acc:0.980]
Epoch [85/120    avg_loss:0.028, val_acc:0.980]
Epoch [86/120    avg_loss:0.029, val_acc:0.980]
Epoch [87/120    avg_loss:0.032, val_acc:0.980]
Epoch [88/120    avg_loss:0.033, val_acc:0.980]
Epoch [89/120    avg_loss:0.031, val_acc:0.980]
Epoch [90/120    avg_loss:0.030, val_acc:0.980]
Epoch [91/120    avg_loss:0.029, val_acc:0.980]
Epoch [92/120    avg_loss:0.028, val_acc:0.980]
Epoch [93/120    avg_loss:0.030, val_acc:0.980]
Epoch [94/120    avg_loss:0.033, val_acc:0.980]
Epoch [95/120    avg_loss:0.031, val_acc:0.980]
Epoch [96/120    avg_loss:0.027, val_acc:0.980]
Epoch [97/120    avg_loss:0.034, val_acc:0.980]
Epoch [98/120    avg_loss:0.029, val_acc:0.980]
Epoch [99/120    avg_loss:0.028, val_acc:0.980]
Epoch [100/120    avg_loss:0.032, val_acc:0.980]
Epoch [101/120    avg_loss:0.028, val_acc:0.980]
Epoch [102/120    avg_loss:0.028, val_acc:0.980]
Epoch [103/120    avg_loss:0.028, val_acc:0.980]
Epoch [104/120    avg_loss:0.031, val_acc:0.980]
Epoch [105/120    avg_loss:0.032, val_acc:0.980]
Epoch [106/120    avg_loss:0.030, val_acc:0.980]
Epoch [107/120    avg_loss:0.026, val_acc:0.980]
Epoch [108/120    avg_loss:0.031, val_acc:0.980]
Epoch [109/120    avg_loss:0.030, val_acc:0.980]
Epoch [110/120    avg_loss:0.030, val_acc:0.980]
Epoch [111/120    avg_loss:0.027, val_acc:0.980]
Epoch [112/120    avg_loss:0.033, val_acc:0.980]
Epoch [113/120    avg_loss:0.029, val_acc:0.980]
Epoch [114/120    avg_loss:0.030, val_acc:0.980]
Epoch [115/120    avg_loss:0.035, val_acc:0.980]
Epoch [116/120    avg_loss:0.032, val_acc:0.980]
Epoch [117/120    avg_loss:0.029, val_acc:0.980]
Epoch [118/120    avg_loss:0.032, val_acc:0.980]
Epoch [119/120    avg_loss:0.028, val_acc:0.980]
Epoch [120/120    avg_loss:0.032, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1246    4    0    0    0    0    0    4    8   18    3    0
     0    2    0]
 [   0    0    0  710    1    2    0    0    0   10    4    0   10   10
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    9    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   21    0    0    5    0    0    0    2  807   36    0    0
     1    3    0]
 [   0    0    8    0    0    5    3    0    0    0    8 2177    0    4
     5    0    0]
 [   0    0    1    0    0    7    0    0    0    0    8    0  512    0
     0    3    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0    0    0
  1129    8    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    14  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.19241192411924

F1 scores:
[       nan 0.975      0.9730574  0.97193703 0.99765808 0.96347032
 0.982009   0.84745763 1.         0.69230769 0.94165694 0.97996849
 0.96694995 0.96354167 0.98516579 0.92762186 0.98245614]

Kappa:
0.9679876912168615
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc4d51df710>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.361, val_acc:0.272]
Epoch [2/120    avg_loss:1.924, val_acc:0.538]
Epoch [3/120    avg_loss:1.603, val_acc:0.669]
Epoch [4/120    avg_loss:1.397, val_acc:0.752]
Epoch [5/120    avg_loss:1.218, val_acc:0.732]
Epoch [6/120    avg_loss:0.938, val_acc:0.786]
Epoch [7/120    avg_loss:0.811, val_acc:0.816]
Epoch [8/120    avg_loss:0.611, val_acc:0.832]
Epoch [9/120    avg_loss:0.590, val_acc:0.826]
Epoch [10/120    avg_loss:0.546, val_acc:0.880]
Epoch [11/120    avg_loss:0.423, val_acc:0.873]
Epoch [12/120    avg_loss:0.363, val_acc:0.880]
Epoch [13/120    avg_loss:0.299, val_acc:0.923]
Epoch [14/120    avg_loss:0.283, val_acc:0.915]
Epoch [15/120    avg_loss:0.252, val_acc:0.888]
Epoch [16/120    avg_loss:0.451, val_acc:0.830]
Epoch [17/120    avg_loss:0.318, val_acc:0.904]
Epoch [18/120    avg_loss:0.283, val_acc:0.931]
Epoch [19/120    avg_loss:0.201, val_acc:0.927]
Epoch [20/120    avg_loss:0.168, val_acc:0.935]
Epoch [21/120    avg_loss:0.136, val_acc:0.937]
Epoch [22/120    avg_loss:0.138, val_acc:0.952]
Epoch [23/120    avg_loss:0.145, val_acc:0.942]
Epoch [24/120    avg_loss:0.128, val_acc:0.905]
Epoch [25/120    avg_loss:0.115, val_acc:0.954]
Epoch [26/120    avg_loss:0.094, val_acc:0.943]
Epoch [27/120    avg_loss:0.100, val_acc:0.964]
Epoch [28/120    avg_loss:0.107, val_acc:0.964]
Epoch [29/120    avg_loss:0.084, val_acc:0.958]
Epoch [30/120    avg_loss:0.121, val_acc:0.941]
Epoch [31/120    avg_loss:0.150, val_acc:0.955]
Epoch [32/120    avg_loss:0.092, val_acc:0.956]
Epoch [33/120    avg_loss:0.107, val_acc:0.967]
Epoch [34/120    avg_loss:0.124, val_acc:0.947]
Epoch [35/120    avg_loss:0.115, val_acc:0.955]
Epoch [36/120    avg_loss:0.085, val_acc:0.971]
Epoch [37/120    avg_loss:0.071, val_acc:0.968]
Epoch [38/120    avg_loss:0.056, val_acc:0.972]
Epoch [39/120    avg_loss:0.068, val_acc:0.965]
Epoch [40/120    avg_loss:0.052, val_acc:0.973]
Epoch [41/120    avg_loss:0.051, val_acc:0.964]
Epoch [42/120    avg_loss:0.052, val_acc:0.973]
Epoch [43/120    avg_loss:0.061, val_acc:0.962]
Epoch [44/120    avg_loss:0.054, val_acc:0.973]
Epoch [45/120    avg_loss:0.042, val_acc:0.976]
Epoch [46/120    avg_loss:0.040, val_acc:0.978]
Epoch [47/120    avg_loss:0.035, val_acc:0.969]
Epoch [48/120    avg_loss:0.035, val_acc:0.978]
Epoch [49/120    avg_loss:0.051, val_acc:0.962]
Epoch [50/120    avg_loss:0.051, val_acc:0.978]
Epoch [51/120    avg_loss:0.039, val_acc:0.974]
Epoch [52/120    avg_loss:0.026, val_acc:0.979]
Epoch [53/120    avg_loss:0.032, val_acc:0.972]
Epoch [54/120    avg_loss:0.033, val_acc:0.980]
Epoch [55/120    avg_loss:0.028, val_acc:0.978]
Epoch [56/120    avg_loss:0.034, val_acc:0.973]
Epoch [57/120    avg_loss:0.026, val_acc:0.975]
Epoch [58/120    avg_loss:0.040, val_acc:0.980]
Epoch [59/120    avg_loss:0.022, val_acc:0.975]
Epoch [60/120    avg_loss:0.025, val_acc:0.971]
Epoch [61/120    avg_loss:0.024, val_acc:0.982]
Epoch [62/120    avg_loss:0.023, val_acc:0.976]
Epoch [63/120    avg_loss:0.020, val_acc:0.981]
Epoch [64/120    avg_loss:0.017, val_acc:0.983]
Epoch [65/120    avg_loss:0.018, val_acc:0.976]
Epoch [66/120    avg_loss:0.021, val_acc:0.986]
Epoch [67/120    avg_loss:0.016, val_acc:0.985]
Epoch [68/120    avg_loss:0.025, val_acc:0.982]
Epoch [69/120    avg_loss:0.021, val_acc:0.982]
Epoch [70/120    avg_loss:0.026, val_acc:0.981]
Epoch [71/120    avg_loss:0.025, val_acc:0.984]
Epoch [72/120    avg_loss:0.021, val_acc:0.978]
Epoch [73/120    avg_loss:0.021, val_acc:0.973]
Epoch [74/120    avg_loss:0.029, val_acc:0.982]
Epoch [75/120    avg_loss:0.024, val_acc:0.984]
Epoch [76/120    avg_loss:0.022, val_acc:0.985]
Epoch [77/120    avg_loss:0.025, val_acc:0.981]
Epoch [78/120    avg_loss:0.016, val_acc:0.979]
Epoch [79/120    avg_loss:0.017, val_acc:0.987]
Epoch [80/120    avg_loss:0.011, val_acc:0.985]
Epoch [81/120    avg_loss:0.015, val_acc:0.982]
Epoch [82/120    avg_loss:0.022, val_acc:0.982]
Epoch [83/120    avg_loss:0.039, val_acc:0.964]
Epoch [84/120    avg_loss:0.050, val_acc:0.964]
Epoch [85/120    avg_loss:0.028, val_acc:0.982]
Epoch [86/120    avg_loss:0.019, val_acc:0.982]
Epoch [87/120    avg_loss:0.013, val_acc:0.981]
Epoch [88/120    avg_loss:0.018, val_acc:0.983]
Epoch [89/120    avg_loss:0.011, val_acc:0.984]
Epoch [90/120    avg_loss:0.015, val_acc:0.987]
Epoch [91/120    avg_loss:0.014, val_acc:0.990]
Epoch [92/120    avg_loss:0.010, val_acc:0.986]
Epoch [93/120    avg_loss:0.009, val_acc:0.981]
Epoch [94/120    avg_loss:0.009, val_acc:0.988]
Epoch [95/120    avg_loss:0.009, val_acc:0.989]
Epoch [96/120    avg_loss:0.019, val_acc:0.979]
Epoch [97/120    avg_loss:0.010, val_acc:0.983]
Epoch [98/120    avg_loss:0.013, val_acc:0.982]
Epoch [99/120    avg_loss:0.008, val_acc:0.984]
Epoch [100/120    avg_loss:0.010, val_acc:0.985]
Epoch [101/120    avg_loss:0.010, val_acc:0.979]
Epoch [102/120    avg_loss:0.007, val_acc:0.989]
Epoch [103/120    avg_loss:0.011, val_acc:0.987]
Epoch [104/120    avg_loss:0.011, val_acc:0.988]
Epoch [105/120    avg_loss:0.009, val_acc:0.989]
Epoch [106/120    avg_loss:0.006, val_acc:0.989]
Epoch [107/120    avg_loss:0.007, val_acc:0.987]
Epoch [108/120    avg_loss:0.004, val_acc:0.987]
Epoch [109/120    avg_loss:0.006, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.988]
Epoch [111/120    avg_loss:0.004, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.004, val_acc:0.989]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.989]
Epoch [116/120    avg_loss:0.004, val_acc:0.989]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1267    0    0    0    0    0    0    0   10    7    1    0
     0    0    0]
 [   0    0    0  692   12   12    0    0    0    8    1    1   18    3
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    8    0    0    0    0  856    4    0    0
     0    3    0]
 [   0    0    7    0    0    0    3    0    0    0   16 2183    0    1
     0    0    0]
 [   0    0    1    0    0   14    0    0    0    0    2    0  511    0
     2    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   33    0    0    0    0    0    0    0
    13  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.8319783197832

F1 scores:
[       nan 0.98765432 0.98829953 0.96111111 0.97025172 0.95893452
 0.97181009 1.         0.997669   0.7826087  0.97162316 0.99069662
 0.95692884 0.98930481 0.99258613 0.92473118 0.96470588]

Kappa:
0.9752911118834926
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc4d7620710>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.465, val_acc:0.501]
Epoch [2/120    avg_loss:1.926, val_acc:0.593]
Epoch [3/120    avg_loss:1.612, val_acc:0.609]
Epoch [4/120    avg_loss:1.356, val_acc:0.621]
Epoch [5/120    avg_loss:1.167, val_acc:0.695]
Epoch [6/120    avg_loss:1.040, val_acc:0.729]
Epoch [7/120    avg_loss:0.949, val_acc:0.748]
Epoch [8/120    avg_loss:0.740, val_acc:0.743]
Epoch [9/120    avg_loss:0.599, val_acc:0.806]
Epoch [10/120    avg_loss:0.604, val_acc:0.781]
Epoch [11/120    avg_loss:0.504, val_acc:0.822]
Epoch [12/120    avg_loss:0.483, val_acc:0.819]
Epoch [13/120    avg_loss:0.363, val_acc:0.848]
Epoch [14/120    avg_loss:0.318, val_acc:0.871]
Epoch [15/120    avg_loss:0.321, val_acc:0.829]
Epoch [16/120    avg_loss:0.293, val_acc:0.877]
Epoch [17/120    avg_loss:0.287, val_acc:0.887]
Epoch [18/120    avg_loss:0.347, val_acc:0.857]
Epoch [19/120    avg_loss:0.326, val_acc:0.885]
Epoch [20/120    avg_loss:0.344, val_acc:0.885]
Epoch [21/120    avg_loss:0.239, val_acc:0.905]
Epoch [22/120    avg_loss:0.230, val_acc:0.895]
Epoch [23/120    avg_loss:0.210, val_acc:0.889]
Epoch [24/120    avg_loss:0.180, val_acc:0.917]
Epoch [25/120    avg_loss:0.175, val_acc:0.930]
Epoch [26/120    avg_loss:0.127, val_acc:0.921]
Epoch [27/120    avg_loss:0.124, val_acc:0.911]
Epoch [28/120    avg_loss:0.155, val_acc:0.941]
Epoch [29/120    avg_loss:0.131, val_acc:0.922]
Epoch [30/120    avg_loss:0.103, val_acc:0.921]
Epoch [31/120    avg_loss:0.091, val_acc:0.928]
Epoch [32/120    avg_loss:0.102, val_acc:0.939]
Epoch [33/120    avg_loss:0.108, val_acc:0.933]
Epoch [34/120    avg_loss:0.072, val_acc:0.956]
Epoch [35/120    avg_loss:0.077, val_acc:0.952]
Epoch [36/120    avg_loss:0.071, val_acc:0.958]
Epoch [37/120    avg_loss:0.062, val_acc:0.955]
Epoch [38/120    avg_loss:0.053, val_acc:0.962]
Epoch [39/120    avg_loss:0.044, val_acc:0.951]
Epoch [40/120    avg_loss:0.076, val_acc:0.954]
Epoch [41/120    avg_loss:0.061, val_acc:0.962]
Epoch [42/120    avg_loss:0.073, val_acc:0.951]
Epoch [43/120    avg_loss:0.049, val_acc:0.965]
Epoch [44/120    avg_loss:0.040, val_acc:0.959]
Epoch [45/120    avg_loss:0.048, val_acc:0.968]
Epoch [46/120    avg_loss:0.038, val_acc:0.966]
Epoch [47/120    avg_loss:0.041, val_acc:0.953]
Epoch [48/120    avg_loss:0.043, val_acc:0.958]
Epoch [49/120    avg_loss:0.041, val_acc:0.954]
Epoch [50/120    avg_loss:0.049, val_acc:0.952]
Epoch [51/120    avg_loss:0.034, val_acc:0.975]
Epoch [52/120    avg_loss:0.046, val_acc:0.977]
Epoch [53/120    avg_loss:0.030, val_acc:0.969]
Epoch [54/120    avg_loss:0.046, val_acc:0.971]
Epoch [55/120    avg_loss:0.029, val_acc:0.965]
Epoch [56/120    avg_loss:0.033, val_acc:0.968]
Epoch [57/120    avg_loss:0.035, val_acc:0.975]
Epoch [58/120    avg_loss:0.022, val_acc:0.978]
Epoch [59/120    avg_loss:0.018, val_acc:0.984]
Epoch [60/120    avg_loss:0.019, val_acc:0.975]
Epoch [61/120    avg_loss:0.020, val_acc:0.978]
Epoch [62/120    avg_loss:0.039, val_acc:0.970]
Epoch [63/120    avg_loss:0.029, val_acc:0.967]
Epoch [64/120    avg_loss:0.031, val_acc:0.967]
Epoch [65/120    avg_loss:0.019, val_acc:0.977]
Epoch [66/120    avg_loss:0.017, val_acc:0.982]
Epoch [67/120    avg_loss:0.018, val_acc:0.974]
Epoch [68/120    avg_loss:0.015, val_acc:0.975]
Epoch [69/120    avg_loss:0.033, val_acc:0.970]
Epoch [70/120    avg_loss:0.030, val_acc:0.981]
Epoch [71/120    avg_loss:0.022, val_acc:0.979]
Epoch [72/120    avg_loss:0.016, val_acc:0.989]
Epoch [73/120    avg_loss:0.013, val_acc:0.986]
Epoch [74/120    avg_loss:0.012, val_acc:0.988]
Epoch [75/120    avg_loss:0.011, val_acc:0.986]
Epoch [76/120    avg_loss:0.012, val_acc:0.984]
Epoch [77/120    avg_loss:0.015, val_acc:0.986]
Epoch [78/120    avg_loss:0.013, val_acc:0.980]
Epoch [79/120    avg_loss:0.012, val_acc:0.989]
Epoch [80/120    avg_loss:0.019, val_acc:0.987]
Epoch [81/120    avg_loss:0.019, val_acc:0.985]
Epoch [82/120    avg_loss:0.010, val_acc:0.980]
Epoch [83/120    avg_loss:0.010, val_acc:0.968]
Epoch [84/120    avg_loss:0.012, val_acc:0.987]
Epoch [85/120    avg_loss:0.011, val_acc:0.980]
Epoch [86/120    avg_loss:0.012, val_acc:0.976]
Epoch [87/120    avg_loss:0.013, val_acc:0.980]
Epoch [88/120    avg_loss:0.023, val_acc:0.968]
Epoch [89/120    avg_loss:0.029, val_acc:0.970]
Epoch [90/120    avg_loss:0.040, val_acc:0.950]
Epoch [91/120    avg_loss:0.033, val_acc:0.955]
Epoch [92/120    avg_loss:0.038, val_acc:0.978]
Epoch [93/120    avg_loss:0.026, val_acc:0.984]
Epoch [94/120    avg_loss:0.014, val_acc:0.984]
Epoch [95/120    avg_loss:0.011, val_acc:0.986]
Epoch [96/120    avg_loss:0.010, val_acc:0.985]
Epoch [97/120    avg_loss:0.010, val_acc:0.987]
Epoch [98/120    avg_loss:0.009, val_acc:0.986]
Epoch [99/120    avg_loss:0.007, val_acc:0.987]
Epoch [100/120    avg_loss:0.008, val_acc:0.986]
Epoch [101/120    avg_loss:0.009, val_acc:0.986]
Epoch [102/120    avg_loss:0.008, val_acc:0.986]
Epoch [103/120    avg_loss:0.008, val_acc:0.986]
Epoch [104/120    avg_loss:0.008, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.986]
Epoch [106/120    avg_loss:0.008, val_acc:0.986]
Epoch [107/120    avg_loss:0.006, val_acc:0.986]
Epoch [108/120    avg_loss:0.007, val_acc:0.986]
Epoch [109/120    avg_loss:0.011, val_acc:0.986]
Epoch [110/120    avg_loss:0.009, val_acc:0.986]
Epoch [111/120    avg_loss:0.007, val_acc:0.986]
Epoch [112/120    avg_loss:0.008, val_acc:0.986]
Epoch [113/120    avg_loss:0.007, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.007, val_acc:0.986]
Epoch [116/120    avg_loss:0.009, val_acc:0.986]
Epoch [117/120    avg_loss:0.008, val_acc:0.986]
Epoch [118/120    avg_loss:0.009, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.011, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1243    4    6    0    0    0    0    0   15   11    6    0
     0    0    0]
 [   0    0    0  714    0    3    0    0    0    4    1    0   21    4
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    1    0    0    4    2    0    0    0  856   11    0    0
     0    1    0]
 [   0    0    6    0    3    1    2    0    0    0   12 2154    4    2
     0   26    0]
 [   0    0    0    0    0    1    0    0    0    0    2    0  529    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    3    0    0    0
  1123    9    0]
 [   0    0    0    0    0    0    0    0    0    5    0    0    0    0
    19  323    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.78861788617886

F1 scores:
[       nan 0.975      0.98067061 0.97407913 0.97695853 0.98409091
 0.99620349 1.         0.99650757 0.75555556 0.96942242 0.98199225
 0.9644485  0.98404255 0.98379325 0.91501416 0.99408284]

Kappa:
0.9748188024906839
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f10ebcc5780>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.415, val_acc:0.439]
Epoch [2/120    avg_loss:1.824, val_acc:0.626]
Epoch [3/120    avg_loss:1.546, val_acc:0.683]
Epoch [4/120    avg_loss:1.215, val_acc:0.751]
Epoch [5/120    avg_loss:0.981, val_acc:0.757]
Epoch [6/120    avg_loss:0.821, val_acc:0.741]
Epoch [7/120    avg_loss:0.703, val_acc:0.795]
Epoch [8/120    avg_loss:0.677, val_acc:0.788]
Epoch [9/120    avg_loss:0.666, val_acc:0.831]
Epoch [10/120    avg_loss:0.538, val_acc:0.809]
Epoch [11/120    avg_loss:0.582, val_acc:0.846]
Epoch [12/120    avg_loss:0.410, val_acc:0.859]
Epoch [13/120    avg_loss:0.357, val_acc:0.909]
Epoch [14/120    avg_loss:0.368, val_acc:0.913]
Epoch [15/120    avg_loss:0.286, val_acc:0.910]
Epoch [16/120    avg_loss:0.247, val_acc:0.897]
Epoch [17/120    avg_loss:0.266, val_acc:0.913]
Epoch [18/120    avg_loss:0.222, val_acc:0.874]
Epoch [19/120    avg_loss:0.198, val_acc:0.933]
Epoch [20/120    avg_loss:0.173, val_acc:0.931]
Epoch [21/120    avg_loss:0.131, val_acc:0.953]
Epoch [22/120    avg_loss:0.139, val_acc:0.938]
Epoch [23/120    avg_loss:0.113, val_acc:0.959]
Epoch [24/120    avg_loss:0.127, val_acc:0.943]
Epoch [25/120    avg_loss:0.107, val_acc:0.946]
Epoch [26/120    avg_loss:0.090, val_acc:0.954]
Epoch [27/120    avg_loss:0.091, val_acc:0.962]
Epoch [28/120    avg_loss:0.091, val_acc:0.936]
Epoch [29/120    avg_loss:0.078, val_acc:0.975]
Epoch [30/120    avg_loss:0.081, val_acc:0.970]
Epoch [31/120    avg_loss:0.090, val_acc:0.968]
Epoch [32/120    avg_loss:0.076, val_acc:0.948]
Epoch [33/120    avg_loss:0.081, val_acc:0.970]
Epoch [34/120    avg_loss:0.082, val_acc:0.975]
Epoch [35/120    avg_loss:0.074, val_acc:0.978]
Epoch [36/120    avg_loss:0.059, val_acc:0.967]
Epoch [37/120    avg_loss:0.047, val_acc:0.974]
Epoch [38/120    avg_loss:0.085, val_acc:0.973]
Epoch [39/120    avg_loss:0.058, val_acc:0.958]
Epoch [40/120    avg_loss:0.105, val_acc:0.953]
Epoch [41/120    avg_loss:0.090, val_acc:0.973]
Epoch [42/120    avg_loss:0.059, val_acc:0.982]
Epoch [43/120    avg_loss:0.066, val_acc:0.965]
Epoch [44/120    avg_loss:0.050, val_acc:0.972]
Epoch [45/120    avg_loss:0.102, val_acc:0.971]
Epoch [46/120    avg_loss:0.039, val_acc:0.980]
Epoch [47/120    avg_loss:0.034, val_acc:0.980]
Epoch [48/120    avg_loss:0.028, val_acc:0.985]
Epoch [49/120    avg_loss:0.027, val_acc:0.982]
Epoch [50/120    avg_loss:0.028, val_acc:0.979]
Epoch [51/120    avg_loss:0.042, val_acc:0.984]
Epoch [52/120    avg_loss:0.031, val_acc:0.980]
Epoch [53/120    avg_loss:0.029, val_acc:0.985]
Epoch [54/120    avg_loss:0.036, val_acc:0.981]
Epoch [55/120    avg_loss:0.033, val_acc:0.986]
Epoch [56/120    avg_loss:0.030, val_acc:0.981]
Epoch [57/120    avg_loss:0.027, val_acc:0.973]
Epoch [58/120    avg_loss:0.018, val_acc:0.984]
Epoch [59/120    avg_loss:0.019, val_acc:0.986]
Epoch [60/120    avg_loss:0.020, val_acc:0.985]
Epoch [61/120    avg_loss:0.015, val_acc:0.987]
Epoch [62/120    avg_loss:0.013, val_acc:0.990]
Epoch [63/120    avg_loss:0.022, val_acc:0.985]
Epoch [64/120    avg_loss:0.018, val_acc:0.986]
Epoch [65/120    avg_loss:0.016, val_acc:0.974]
Epoch [66/120    avg_loss:0.024, val_acc:0.979]
Epoch [67/120    avg_loss:0.044, val_acc:0.973]
Epoch [68/120    avg_loss:0.031, val_acc:0.989]
Epoch [69/120    avg_loss:0.017, val_acc:0.986]
Epoch [70/120    avg_loss:0.014, val_acc:0.990]
Epoch [71/120    avg_loss:0.018, val_acc:0.989]
Epoch [72/120    avg_loss:0.016, val_acc:0.989]
Epoch [73/120    avg_loss:0.017, val_acc:0.989]
Epoch [74/120    avg_loss:0.017, val_acc:0.989]
Epoch [75/120    avg_loss:0.021, val_acc:0.983]
Epoch [76/120    avg_loss:0.023, val_acc:0.984]
Epoch [77/120    avg_loss:0.019, val_acc:0.984]
Epoch [78/120    avg_loss:0.015, val_acc:0.979]
Epoch [79/120    avg_loss:0.012, val_acc:0.981]
Epoch [80/120    avg_loss:0.012, val_acc:0.986]
Epoch [81/120    avg_loss:0.009, val_acc:0.988]
Epoch [82/120    avg_loss:0.006, val_acc:0.988]
Epoch [83/120    avg_loss:0.008, val_acc:0.985]
Epoch [84/120    avg_loss:0.008, val_acc:0.989]
Epoch [85/120    avg_loss:0.007, val_acc:0.990]
Epoch [86/120    avg_loss:0.005, val_acc:0.990]
Epoch [87/120    avg_loss:0.007, val_acc:0.989]
Epoch [88/120    avg_loss:0.005, val_acc:0.988]
Epoch [89/120    avg_loss:0.005, val_acc:0.989]
Epoch [90/120    avg_loss:0.005, val_acc:0.989]
Epoch [91/120    avg_loss:0.005, val_acc:0.989]
Epoch [92/120    avg_loss:0.006, val_acc:0.990]
Epoch [93/120    avg_loss:0.006, val_acc:0.990]
Epoch [94/120    avg_loss:0.005, val_acc:0.990]
Epoch [95/120    avg_loss:0.006, val_acc:0.990]
Epoch [96/120    avg_loss:0.006, val_acc:0.990]
Epoch [97/120    avg_loss:0.004, val_acc:0.990]
Epoch [98/120    avg_loss:0.005, val_acc:0.990]
Epoch [99/120    avg_loss:0.005, val_acc:0.989]
Epoch [100/120    avg_loss:0.005, val_acc:0.990]
Epoch [101/120    avg_loss:0.006, val_acc:0.990]
Epoch [102/120    avg_loss:0.005, val_acc:0.990]
Epoch [103/120    avg_loss:0.006, val_acc:0.989]
Epoch [104/120    avg_loss:0.006, val_acc:0.989]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.005, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.006, val_acc:0.989]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.990]
Epoch [111/120    avg_loss:0.004, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.989]
Epoch [115/120    avg_loss:0.005, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.005, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1273    0    0    0    0    0    0    3    2    4    3    0
     0    0    0]
 [   0    0    0  723    0    5    0    0    0    6    0    0   10    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    6    1    0    0    0  854    3    0    0
     1    6    0]
 [   0    0    9    0    0    0    2    0    0    0   15 2181    2    1
     0    0    0]
 [   0    0    1    0    0    7    0    0    0    0    2    0  521    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    1    0    0
  1137    0    0]
 [   0    0    0    0    0    0   20    0    0    0    0    0    0    0
    17  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.42818428184282

F1 scores:
[       nan 0.98765432 0.98950641 0.98367347 1.         0.97511312
 0.98203593 1.         0.99883586 0.7826087  0.97655803 0.99136364
 0.9729225  0.98930481 0.98998694 0.93373494 0.98823529]

Kappa:
0.982085910489082
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f472ba1d7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.395, val_acc:0.373]
Epoch [2/120    avg_loss:1.862, val_acc:0.579]
Epoch [3/120    avg_loss:1.532, val_acc:0.645]
Epoch [4/120    avg_loss:1.282, val_acc:0.677]
Epoch [5/120    avg_loss:1.077, val_acc:0.724]
Epoch [6/120    avg_loss:0.878, val_acc:0.777]
Epoch [7/120    avg_loss:0.800, val_acc:0.786]
Epoch [8/120    avg_loss:0.832, val_acc:0.793]
Epoch [9/120    avg_loss:0.691, val_acc:0.802]
Epoch [10/120    avg_loss:0.638, val_acc:0.809]
Epoch [11/120    avg_loss:0.494, val_acc:0.856]
Epoch [12/120    avg_loss:0.418, val_acc:0.861]
Epoch [13/120    avg_loss:0.436, val_acc:0.867]
Epoch [14/120    avg_loss:0.407, val_acc:0.872]
Epoch [15/120    avg_loss:0.333, val_acc:0.895]
Epoch [16/120    avg_loss:0.506, val_acc:0.850]
Epoch [17/120    avg_loss:0.369, val_acc:0.876]
Epoch [18/120    avg_loss:0.307, val_acc:0.864]
Epoch [19/120    avg_loss:0.294, val_acc:0.902]
Epoch [20/120    avg_loss:0.259, val_acc:0.910]
Epoch [21/120    avg_loss:0.311, val_acc:0.902]
Epoch [22/120    avg_loss:0.233, val_acc:0.906]
Epoch [23/120    avg_loss:0.196, val_acc:0.911]
Epoch [24/120    avg_loss:0.169, val_acc:0.927]
Epoch [25/120    avg_loss:0.125, val_acc:0.940]
Epoch [26/120    avg_loss:0.120, val_acc:0.935]
Epoch [27/120    avg_loss:0.125, val_acc:0.927]
Epoch [28/120    avg_loss:0.121, val_acc:0.941]
Epoch [29/120    avg_loss:0.107, val_acc:0.943]
Epoch [30/120    avg_loss:0.109, val_acc:0.951]
Epoch [31/120    avg_loss:0.104, val_acc:0.942]
Epoch [32/120    avg_loss:0.103, val_acc:0.946]
Epoch [33/120    avg_loss:0.101, val_acc:0.932]
Epoch [34/120    avg_loss:0.108, val_acc:0.935]
Epoch [35/120    avg_loss:0.131, val_acc:0.931]
Epoch [36/120    avg_loss:0.102, val_acc:0.939]
Epoch [37/120    avg_loss:0.091, val_acc:0.955]
Epoch [38/120    avg_loss:0.095, val_acc:0.939]
Epoch [39/120    avg_loss:0.109, val_acc:0.927]
Epoch [40/120    avg_loss:0.080, val_acc:0.956]
Epoch [41/120    avg_loss:0.079, val_acc:0.944]
Epoch [42/120    avg_loss:0.082, val_acc:0.953]
Epoch [43/120    avg_loss:0.072, val_acc:0.948]
Epoch [44/120    avg_loss:0.068, val_acc:0.961]
Epoch [45/120    avg_loss:0.052, val_acc:0.962]
Epoch [46/120    avg_loss:0.062, val_acc:0.946]
Epoch [47/120    avg_loss:0.053, val_acc:0.968]
Epoch [48/120    avg_loss:0.067, val_acc:0.951]
Epoch [49/120    avg_loss:0.055, val_acc:0.947]
Epoch [50/120    avg_loss:0.054, val_acc:0.957]
Epoch [51/120    avg_loss:0.081, val_acc:0.962]
Epoch [52/120    avg_loss:0.054, val_acc:0.958]
Epoch [53/120    avg_loss:0.050, val_acc:0.953]
Epoch [54/120    avg_loss:0.043, val_acc:0.968]
Epoch [55/120    avg_loss:0.034, val_acc:0.965]
Epoch [56/120    avg_loss:0.030, val_acc:0.970]
Epoch [57/120    avg_loss:0.033, val_acc:0.944]
Epoch [58/120    avg_loss:0.041, val_acc:0.964]
Epoch [59/120    avg_loss:0.036, val_acc:0.951]
Epoch [60/120    avg_loss:0.032, val_acc:0.956]
Epoch [61/120    avg_loss:0.030, val_acc:0.964]
Epoch [62/120    avg_loss:0.044, val_acc:0.921]
Epoch [63/120    avg_loss:0.035, val_acc:0.964]
Epoch [64/120    avg_loss:0.027, val_acc:0.978]
Epoch [65/120    avg_loss:0.039, val_acc:0.962]
Epoch [66/120    avg_loss:0.036, val_acc:0.961]
Epoch [67/120    avg_loss:0.116, val_acc:0.956]
Epoch [68/120    avg_loss:0.071, val_acc:0.948]
Epoch [69/120    avg_loss:0.046, val_acc:0.966]
Epoch [70/120    avg_loss:0.040, val_acc:0.958]
Epoch [71/120    avg_loss:0.036, val_acc:0.976]
Epoch [72/120    avg_loss:0.031, val_acc:0.964]
Epoch [73/120    avg_loss:0.028, val_acc:0.965]
Epoch [74/120    avg_loss:0.023, val_acc:0.969]
Epoch [75/120    avg_loss:0.024, val_acc:0.964]
Epoch [76/120    avg_loss:0.038, val_acc:0.948]
Epoch [77/120    avg_loss:0.086, val_acc:0.932]
Epoch [78/120    avg_loss:0.072, val_acc:0.961]
Epoch [79/120    avg_loss:0.035, val_acc:0.966]
Epoch [80/120    avg_loss:0.028, val_acc:0.969]
Epoch [81/120    avg_loss:0.023, val_acc:0.973]
Epoch [82/120    avg_loss:0.027, val_acc:0.977]
Epoch [83/120    avg_loss:0.030, val_acc:0.977]
Epoch [84/120    avg_loss:0.021, val_acc:0.978]
Epoch [85/120    avg_loss:0.020, val_acc:0.978]
Epoch [86/120    avg_loss:0.021, val_acc:0.978]
Epoch [87/120    avg_loss:0.017, val_acc:0.978]
Epoch [88/120    avg_loss:0.018, val_acc:0.977]
Epoch [89/120    avg_loss:0.017, val_acc:0.977]
Epoch [90/120    avg_loss:0.016, val_acc:0.975]
Epoch [91/120    avg_loss:0.018, val_acc:0.977]
Epoch [92/120    avg_loss:0.015, val_acc:0.978]
Epoch [93/120    avg_loss:0.021, val_acc:0.978]
Epoch [94/120    avg_loss:0.021, val_acc:0.976]
Epoch [95/120    avg_loss:0.019, val_acc:0.976]
Epoch [96/120    avg_loss:0.018, val_acc:0.977]
Epoch [97/120    avg_loss:0.016, val_acc:0.979]
Epoch [98/120    avg_loss:0.019, val_acc:0.979]
Epoch [99/120    avg_loss:0.016, val_acc:0.980]
Epoch [100/120    avg_loss:0.019, val_acc:0.978]
Epoch [101/120    avg_loss:0.014, val_acc:0.979]
Epoch [102/120    avg_loss:0.014, val_acc:0.980]
Epoch [103/120    avg_loss:0.014, val_acc:0.979]
Epoch [104/120    avg_loss:0.012, val_acc:0.979]
Epoch [105/120    avg_loss:0.016, val_acc:0.980]
Epoch [106/120    avg_loss:0.015, val_acc:0.980]
Epoch [107/120    avg_loss:0.011, val_acc:0.980]
Epoch [108/120    avg_loss:0.014, val_acc:0.979]
Epoch [109/120    avg_loss:0.012, val_acc:0.978]
Epoch [110/120    avg_loss:0.013, val_acc:0.979]
Epoch [111/120    avg_loss:0.014, val_acc:0.980]
Epoch [112/120    avg_loss:0.014, val_acc:0.980]
Epoch [113/120    avg_loss:0.016, val_acc:0.978]
Epoch [114/120    avg_loss:0.011, val_acc:0.979]
Epoch [115/120    avg_loss:0.011, val_acc:0.977]
Epoch [116/120    avg_loss:0.012, val_acc:0.978]
Epoch [117/120    avg_loss:0.012, val_acc:0.979]
Epoch [118/120    avg_loss:0.013, val_acc:0.977]
Epoch [119/120    avg_loss:0.011, val_acc:0.978]
Epoch [120/120    avg_loss:0.011, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1254    0    4    0    1    0    0    0    6   18    2    0
     0    0    0]
 [   0    0    1  715    1   25    0    0    0    3    1    0    0    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10   12    0    7    1    0    0    0  837    4    0    0
     0    4    0]
 [   0    0    3    0    0    0    3    0    0    0    9 2193    0    2
     0    0    0]
 [   0    0    0   23    6   13    0    0    0    0    2    8  479    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    1    0    0
  1134    0    0]
 [   0    0    0    0    0    0   35    0    0    0    0    0    0    0
    37  275    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.21409214092141

F1 scores:
[       nan 1.         0.98237368 0.95524382 0.97482838 0.94852136
 0.96893491 1.         0.99650757 0.87804878 0.96595499 0.98872858
 0.9410609  0.9919571  0.98181818 0.87859425 0.97647059]

Kappa:
0.9682238896358217
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f87f353e6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.465, val_acc:0.532]
Epoch [2/120    avg_loss:1.910, val_acc:0.588]
Epoch [3/120    avg_loss:1.575, val_acc:0.652]
Epoch [4/120    avg_loss:1.326, val_acc:0.658]
Epoch [5/120    avg_loss:1.075, val_acc:0.662]
Epoch [6/120    avg_loss:0.962, val_acc:0.723]
Epoch [7/120    avg_loss:0.869, val_acc:0.726]
Epoch [8/120    avg_loss:0.776, val_acc:0.765]
Epoch [9/120    avg_loss:0.705, val_acc:0.792]
Epoch [10/120    avg_loss:0.580, val_acc:0.792]
Epoch [11/120    avg_loss:0.559, val_acc:0.839]
Epoch [12/120    avg_loss:0.456, val_acc:0.865]
Epoch [13/120    avg_loss:0.358, val_acc:0.861]
Epoch [14/120    avg_loss:0.363, val_acc:0.889]
Epoch [15/120    avg_loss:0.313, val_acc:0.867]
Epoch [16/120    avg_loss:0.358, val_acc:0.877]
Epoch [17/120    avg_loss:0.307, val_acc:0.895]
Epoch [18/120    avg_loss:0.301, val_acc:0.870]
Epoch [19/120    avg_loss:0.272, val_acc:0.887]
Epoch [20/120    avg_loss:0.210, val_acc:0.903]
Epoch [21/120    avg_loss:0.211, val_acc:0.893]
Epoch [22/120    avg_loss:0.210, val_acc:0.925]
Epoch [23/120    avg_loss:0.190, val_acc:0.912]
Epoch [24/120    avg_loss:0.183, val_acc:0.873]
Epoch [25/120    avg_loss:0.153, val_acc:0.911]
Epoch [26/120    avg_loss:0.188, val_acc:0.914]
Epoch [27/120    avg_loss:0.239, val_acc:0.912]
Epoch [28/120    avg_loss:0.168, val_acc:0.917]
Epoch [29/120    avg_loss:0.104, val_acc:0.926]
Epoch [30/120    avg_loss:0.116, val_acc:0.928]
Epoch [31/120    avg_loss:0.119, val_acc:0.927]
Epoch [32/120    avg_loss:0.125, val_acc:0.917]
Epoch [33/120    avg_loss:0.108, val_acc:0.928]
Epoch [34/120    avg_loss:0.086, val_acc:0.946]
Epoch [35/120    avg_loss:0.060, val_acc:0.941]
Epoch [36/120    avg_loss:0.066, val_acc:0.954]
Epoch [37/120    avg_loss:0.071, val_acc:0.952]
Epoch [38/120    avg_loss:0.091, val_acc:0.947]
Epoch [39/120    avg_loss:0.071, val_acc:0.936]
Epoch [40/120    avg_loss:0.085, val_acc:0.935]
Epoch [41/120    avg_loss:0.058, val_acc:0.957]
Epoch [42/120    avg_loss:0.095, val_acc:0.940]
Epoch [43/120    avg_loss:0.136, val_acc:0.930]
Epoch [44/120    avg_loss:0.100, val_acc:0.944]
Epoch [45/120    avg_loss:0.093, val_acc:0.932]
Epoch [46/120    avg_loss:0.100, val_acc:0.946]
Epoch [47/120    avg_loss:0.071, val_acc:0.959]
Epoch [48/120    avg_loss:0.049, val_acc:0.955]
Epoch [49/120    avg_loss:0.035, val_acc:0.957]
Epoch [50/120    avg_loss:0.050, val_acc:0.961]
Epoch [51/120    avg_loss:0.045, val_acc:0.959]
Epoch [52/120    avg_loss:0.041, val_acc:0.965]
Epoch [53/120    avg_loss:0.042, val_acc:0.968]
Epoch [54/120    avg_loss:0.053, val_acc:0.949]
Epoch [55/120    avg_loss:0.055, val_acc:0.950]
Epoch [56/120    avg_loss:0.036, val_acc:0.967]
Epoch [57/120    avg_loss:0.043, val_acc:0.967]
Epoch [58/120    avg_loss:0.060, val_acc:0.964]
Epoch [59/120    avg_loss:0.062, val_acc:0.963]
Epoch [60/120    avg_loss:0.055, val_acc:0.962]
Epoch [61/120    avg_loss:0.035, val_acc:0.959]
Epoch [62/120    avg_loss:0.050, val_acc:0.955]
Epoch [63/120    avg_loss:0.081, val_acc:0.954]
Epoch [64/120    avg_loss:0.039, val_acc:0.972]
Epoch [65/120    avg_loss:0.038, val_acc:0.956]
Epoch [66/120    avg_loss:0.058, val_acc:0.957]
Epoch [67/120    avg_loss:0.045, val_acc:0.965]
Epoch [68/120    avg_loss:0.041, val_acc:0.962]
Epoch [69/120    avg_loss:0.035, val_acc:0.971]
Epoch [70/120    avg_loss:0.022, val_acc:0.972]
Epoch [71/120    avg_loss:0.021, val_acc:0.977]
Epoch [72/120    avg_loss:0.020, val_acc:0.967]
Epoch [73/120    avg_loss:0.029, val_acc:0.977]
Epoch [74/120    avg_loss:0.025, val_acc:0.963]
Epoch [75/120    avg_loss:0.024, val_acc:0.971]
Epoch [76/120    avg_loss:0.018, val_acc:0.971]
Epoch [77/120    avg_loss:0.015, val_acc:0.977]
Epoch [78/120    avg_loss:0.019, val_acc:0.973]
Epoch [79/120    avg_loss:0.020, val_acc:0.976]
Epoch [80/120    avg_loss:0.025, val_acc:0.974]
Epoch [81/120    avg_loss:0.024, val_acc:0.971]
Epoch [82/120    avg_loss:0.029, val_acc:0.967]
Epoch [83/120    avg_loss:0.021, val_acc:0.967]
Epoch [84/120    avg_loss:0.018, val_acc:0.968]
Epoch [85/120    avg_loss:0.015, val_acc:0.974]
Epoch [86/120    avg_loss:0.013, val_acc:0.975]
Epoch [87/120    avg_loss:0.013, val_acc:0.974]
Epoch [88/120    avg_loss:0.013, val_acc:0.974]
Epoch [89/120    avg_loss:0.015, val_acc:0.976]
Epoch [90/120    avg_loss:0.012, val_acc:0.977]
Epoch [91/120    avg_loss:0.011, val_acc:0.976]
Epoch [92/120    avg_loss:0.014, val_acc:0.976]
Epoch [93/120    avg_loss:0.013, val_acc:0.976]
Epoch [94/120    avg_loss:0.043, val_acc:0.961]
Epoch [95/120    avg_loss:0.030, val_acc:0.970]
Epoch [96/120    avg_loss:0.014, val_acc:0.974]
Epoch [97/120    avg_loss:0.012, val_acc:0.974]
Epoch [98/120    avg_loss:0.016, val_acc:0.966]
Epoch [99/120    avg_loss:0.034, val_acc:0.976]
Epoch [100/120    avg_loss:0.013, val_acc:0.974]
Epoch [101/120    avg_loss:0.013, val_acc:0.979]
Epoch [102/120    avg_loss:0.010, val_acc:0.977]
Epoch [103/120    avg_loss:0.008, val_acc:0.974]
Epoch [104/120    avg_loss:0.008, val_acc:0.977]
Epoch [105/120    avg_loss:0.008, val_acc:0.976]
Epoch [106/120    avg_loss:0.013, val_acc:0.972]
Epoch [107/120    avg_loss:0.018, val_acc:0.974]
Epoch [108/120    avg_loss:0.014, val_acc:0.974]
Epoch [109/120    avg_loss:0.009, val_acc:0.974]
Epoch [110/120    avg_loss:0.007, val_acc:0.982]
Epoch [111/120    avg_loss:0.008, val_acc:0.975]
Epoch [112/120    avg_loss:0.009, val_acc:0.976]
Epoch [113/120    avg_loss:0.024, val_acc:0.968]
Epoch [114/120    avg_loss:0.019, val_acc:0.974]
Epoch [115/120    avg_loss:0.010, val_acc:0.975]
Epoch [116/120    avg_loss:0.010, val_acc:0.975]
Epoch [117/120    avg_loss:0.009, val_acc:0.975]
Epoch [118/120    avg_loss:0.012, val_acc:0.975]
Epoch [119/120    avg_loss:0.010, val_acc:0.977]
Epoch [120/120    avg_loss:0.007, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1234   12    0    0    1    0    0    0    9   29    0    0
     0    0    0]
 [   0    0    0  726    2    7    0    0    0    6    2    0    0    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    3   20    0    1    0    0    0    0  832   17    0    0
     0    2    0]
 [   0    0    0    0    0    0    3    0    0    0   21 2184    0    2
     0    0    0]
 [   0    0    0   28   11    0    0    0    0    0   11    4  478    0
     0    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    8    0    0    1    0    3    0    0    0
  1127    0    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    61  274    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.83468834688347

F1 scores:
[       nan 0.98765432 0.97820055 0.94654498 0.97038724 0.97621744
 0.98644578 1.         0.99883856 0.79069767 0.94922989 0.98267717
 0.94373149 0.98133333 0.96738197 0.87961477 0.98224852]

Kappa:
0.9638875477784152
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f616b383710>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.510, val_acc:0.508]
Epoch [2/120    avg_loss:1.936, val_acc:0.489]
Epoch [3/120    avg_loss:1.666, val_acc:0.629]
Epoch [4/120    avg_loss:1.395, val_acc:0.712]
Epoch [5/120    avg_loss:1.058, val_acc:0.727]
Epoch [6/120    avg_loss:0.921, val_acc:0.740]
Epoch [7/120    avg_loss:0.838, val_acc:0.739]
Epoch [8/120    avg_loss:0.762, val_acc:0.773]
Epoch [9/120    avg_loss:0.672, val_acc:0.822]
Epoch [10/120    avg_loss:0.541, val_acc:0.852]
Epoch [11/120    avg_loss:0.517, val_acc:0.795]
Epoch [12/120    avg_loss:0.493, val_acc:0.837]
Epoch [13/120    avg_loss:0.436, val_acc:0.830]
Epoch [14/120    avg_loss:0.408, val_acc:0.862]
Epoch [15/120    avg_loss:0.368, val_acc:0.865]
Epoch [16/120    avg_loss:0.338, val_acc:0.849]
Epoch [17/120    avg_loss:0.334, val_acc:0.873]
Epoch [18/120    avg_loss:0.289, val_acc:0.875]
Epoch [19/120    avg_loss:0.225, val_acc:0.880]
Epoch [20/120    avg_loss:0.185, val_acc:0.927]
Epoch [21/120    avg_loss:0.183, val_acc:0.865]
Epoch [22/120    avg_loss:0.184, val_acc:0.925]
Epoch [23/120    avg_loss:0.165, val_acc:0.930]
Epoch [24/120    avg_loss:0.160, val_acc:0.940]
Epoch [25/120    avg_loss:0.134, val_acc:0.941]
Epoch [26/120    avg_loss:0.208, val_acc:0.916]
Epoch [27/120    avg_loss:0.198, val_acc:0.936]
Epoch [28/120    avg_loss:0.118, val_acc:0.942]
Epoch [29/120    avg_loss:0.121, val_acc:0.945]
Epoch [30/120    avg_loss:0.090, val_acc:0.953]
Epoch [31/120    avg_loss:0.106, val_acc:0.948]
Epoch [32/120    avg_loss:0.098, val_acc:0.954]
Epoch [33/120    avg_loss:0.083, val_acc:0.935]
Epoch [34/120    avg_loss:0.087, val_acc:0.955]
Epoch [35/120    avg_loss:0.083, val_acc:0.948]
Epoch [36/120    avg_loss:0.119, val_acc:0.946]
Epoch [37/120    avg_loss:0.086, val_acc:0.938]
Epoch [38/120    avg_loss:0.085, val_acc:0.956]
Epoch [39/120    avg_loss:0.068, val_acc:0.947]
Epoch [40/120    avg_loss:0.078, val_acc:0.946]
Epoch [41/120    avg_loss:0.058, val_acc:0.947]
Epoch [42/120    avg_loss:0.061, val_acc:0.958]
Epoch [43/120    avg_loss:0.054, val_acc:0.955]
Epoch [44/120    avg_loss:0.048, val_acc:0.966]
Epoch [45/120    avg_loss:0.043, val_acc:0.974]
Epoch [46/120    avg_loss:0.062, val_acc:0.956]
Epoch [47/120    avg_loss:0.076, val_acc:0.962]
Epoch [48/120    avg_loss:0.077, val_acc:0.955]
Epoch [49/120    avg_loss:0.067, val_acc:0.946]
Epoch [50/120    avg_loss:0.065, val_acc:0.954]
Epoch [51/120    avg_loss:0.041, val_acc:0.964]
Epoch [52/120    avg_loss:0.050, val_acc:0.957]
Epoch [53/120    avg_loss:0.064, val_acc:0.961]
Epoch [54/120    avg_loss:0.040, val_acc:0.961]
Epoch [55/120    avg_loss:0.051, val_acc:0.959]
Epoch [56/120    avg_loss:0.036, val_acc:0.964]
Epoch [57/120    avg_loss:0.035, val_acc:0.973]
Epoch [58/120    avg_loss:0.041, val_acc:0.940]
Epoch [59/120    avg_loss:0.044, val_acc:0.967]
Epoch [60/120    avg_loss:0.022, val_acc:0.967]
Epoch [61/120    avg_loss:0.020, val_acc:0.970]
Epoch [62/120    avg_loss:0.022, val_acc:0.970]
Epoch [63/120    avg_loss:0.019, val_acc:0.969]
Epoch [64/120    avg_loss:0.022, val_acc:0.971]
Epoch [65/120    avg_loss:0.017, val_acc:0.975]
Epoch [66/120    avg_loss:0.017, val_acc:0.975]
Epoch [67/120    avg_loss:0.018, val_acc:0.974]
Epoch [68/120    avg_loss:0.018, val_acc:0.973]
Epoch [69/120    avg_loss:0.019, val_acc:0.975]
Epoch [70/120    avg_loss:0.018, val_acc:0.976]
Epoch [71/120    avg_loss:0.019, val_acc:0.977]
Epoch [72/120    avg_loss:0.018, val_acc:0.975]
Epoch [73/120    avg_loss:0.016, val_acc:0.973]
Epoch [74/120    avg_loss:0.018, val_acc:0.974]
Epoch [75/120    avg_loss:0.017, val_acc:0.974]
Epoch [76/120    avg_loss:0.019, val_acc:0.976]
Epoch [77/120    avg_loss:0.016, val_acc:0.973]
Epoch [78/120    avg_loss:0.021, val_acc:0.974]
Epoch [79/120    avg_loss:0.015, val_acc:0.973]
Epoch [80/120    avg_loss:0.016, val_acc:0.975]
Epoch [81/120    avg_loss:0.015, val_acc:0.976]
Epoch [82/120    avg_loss:0.015, val_acc:0.975]
Epoch [83/120    avg_loss:0.014, val_acc:0.977]
Epoch [84/120    avg_loss:0.019, val_acc:0.976]
Epoch [85/120    avg_loss:0.019, val_acc:0.976]
Epoch [86/120    avg_loss:0.016, val_acc:0.974]
Epoch [87/120    avg_loss:0.015, val_acc:0.974]
Epoch [88/120    avg_loss:0.020, val_acc:0.976]
Epoch [89/120    avg_loss:0.018, val_acc:0.978]
Epoch [90/120    avg_loss:0.017, val_acc:0.978]
Epoch [91/120    avg_loss:0.012, val_acc:0.977]
Epoch [92/120    avg_loss:0.018, val_acc:0.979]
Epoch [93/120    avg_loss:0.016, val_acc:0.980]
Epoch [94/120    avg_loss:0.017, val_acc:0.979]
Epoch [95/120    avg_loss:0.018, val_acc:0.977]
Epoch [96/120    avg_loss:0.015, val_acc:0.977]
Epoch [97/120    avg_loss:0.018, val_acc:0.977]
Epoch [98/120    avg_loss:0.014, val_acc:0.976]
Epoch [99/120    avg_loss:0.016, val_acc:0.978]
Epoch [100/120    avg_loss:0.017, val_acc:0.977]
Epoch [101/120    avg_loss:0.018, val_acc:0.975]
Epoch [102/120    avg_loss:0.013, val_acc:0.977]
Epoch [103/120    avg_loss:0.016, val_acc:0.977]
Epoch [104/120    avg_loss:0.019, val_acc:0.978]
Epoch [105/120    avg_loss:0.015, val_acc:0.976]
Epoch [106/120    avg_loss:0.013, val_acc:0.977]
Epoch [107/120    avg_loss:0.014, val_acc:0.977]
Epoch [108/120    avg_loss:0.016, val_acc:0.977]
Epoch [109/120    avg_loss:0.015, val_acc:0.977]
Epoch [110/120    avg_loss:0.014, val_acc:0.977]
Epoch [111/120    avg_loss:0.015, val_acc:0.977]
Epoch [112/120    avg_loss:0.013, val_acc:0.978]
Epoch [113/120    avg_loss:0.014, val_acc:0.978]
Epoch [114/120    avg_loss:0.012, val_acc:0.978]
Epoch [115/120    avg_loss:0.013, val_acc:0.978]
Epoch [116/120    avg_loss:0.012, val_acc:0.978]
Epoch [117/120    avg_loss:0.012, val_acc:0.978]
Epoch [118/120    avg_loss:0.016, val_acc:0.978]
Epoch [119/120    avg_loss:0.017, val_acc:0.978]
Epoch [120/120    avg_loss:0.013, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1260    4    0    0    0    0    0    0    7   11    3    0
     0    0    0]
 [   0    0    1  717    0   11    0    0    0    3    1    0   12    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    3    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   21   17    0    3    0    0    0    0  826    6    0    0
     0    2    0]
 [   0    0    7    0    0    0    5    0    1    0   13 2182    0    2
     0    0    0]
 [   0    0    0   16   19    1    0    0    0    0    1    0  493    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0   15    0    0    2    0    0    0    0
    37  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.37669376693766

F1 scores:
[       nan 0.98765432 0.97902098 0.95345745 0.95730337 0.97494305
 0.9850075  1.         0.99767981 0.73170732 0.95657209 0.9897936
 0.9462572  0.98930481 0.98056156 0.91277259 0.97674419]

Kappa:
0.9700915095711189
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc893827710>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.428, val_acc:0.512]
Epoch [2/120    avg_loss:1.906, val_acc:0.580]
Epoch [3/120    avg_loss:1.587, val_acc:0.599]
Epoch [4/120    avg_loss:1.475, val_acc:0.660]
Epoch [5/120    avg_loss:1.303, val_acc:0.679]
Epoch [6/120    avg_loss:1.054, val_acc:0.749]
Epoch [7/120    avg_loss:0.917, val_acc:0.752]
Epoch [8/120    avg_loss:0.799, val_acc:0.738]
Epoch [9/120    avg_loss:0.741, val_acc:0.807]
Epoch [10/120    avg_loss:0.614, val_acc:0.793]
Epoch [11/120    avg_loss:0.574, val_acc:0.841]
Epoch [12/120    avg_loss:0.460, val_acc:0.833]
Epoch [13/120    avg_loss:0.536, val_acc:0.827]
Epoch [14/120    avg_loss:0.421, val_acc:0.799]
Epoch [15/120    avg_loss:0.348, val_acc:0.843]
Epoch [16/120    avg_loss:0.316, val_acc:0.864]
Epoch [17/120    avg_loss:0.312, val_acc:0.862]
Epoch [18/120    avg_loss:0.331, val_acc:0.877]
Epoch [19/120    avg_loss:0.243, val_acc:0.900]
Epoch [20/120    avg_loss:0.272, val_acc:0.891]
Epoch [21/120    avg_loss:0.301, val_acc:0.853]
Epoch [22/120    avg_loss:0.418, val_acc:0.864]
Epoch [23/120    avg_loss:0.252, val_acc:0.912]
Epoch [24/120    avg_loss:0.156, val_acc:0.920]
Epoch [25/120    avg_loss:0.158, val_acc:0.922]
Epoch [26/120    avg_loss:0.143, val_acc:0.936]
Epoch [27/120    avg_loss:0.116, val_acc:0.934]
Epoch [28/120    avg_loss:0.122, val_acc:0.943]
Epoch [29/120    avg_loss:0.128, val_acc:0.936]
Epoch [30/120    avg_loss:0.087, val_acc:0.941]
Epoch [31/120    avg_loss:0.082, val_acc:0.940]
Epoch [32/120    avg_loss:0.078, val_acc:0.946]
Epoch [33/120    avg_loss:0.081, val_acc:0.956]
Epoch [34/120    avg_loss:0.080, val_acc:0.942]
Epoch [35/120    avg_loss:0.083, val_acc:0.955]
Epoch [36/120    avg_loss:0.081, val_acc:0.950]
Epoch [37/120    avg_loss:0.060, val_acc:0.952]
Epoch [38/120    avg_loss:0.062, val_acc:0.952]
Epoch [39/120    avg_loss:0.054, val_acc:0.935]
Epoch [40/120    avg_loss:0.074, val_acc:0.957]
Epoch [41/120    avg_loss:0.051, val_acc:0.948]
Epoch [42/120    avg_loss:0.051, val_acc:0.953]
Epoch [43/120    avg_loss:0.049, val_acc:0.958]
Epoch [44/120    avg_loss:0.057, val_acc:0.959]
Epoch [45/120    avg_loss:0.061, val_acc:0.948]
Epoch [46/120    avg_loss:0.067, val_acc:0.931]
Epoch [47/120    avg_loss:0.075, val_acc:0.943]
Epoch [48/120    avg_loss:0.091, val_acc:0.946]
Epoch [49/120    avg_loss:0.068, val_acc:0.955]
Epoch [50/120    avg_loss:0.061, val_acc:0.957]
Epoch [51/120    avg_loss:0.034, val_acc:0.961]
Epoch [52/120    avg_loss:0.044, val_acc:0.962]
Epoch [53/120    avg_loss:0.040, val_acc:0.962]
Epoch [54/120    avg_loss:0.033, val_acc:0.967]
Epoch [55/120    avg_loss:0.037, val_acc:0.964]
Epoch [56/120    avg_loss:0.044, val_acc:0.957]
Epoch [57/120    avg_loss:0.069, val_acc:0.966]
Epoch [58/120    avg_loss:0.031, val_acc:0.964]
Epoch [59/120    avg_loss:0.067, val_acc:0.947]
Epoch [60/120    avg_loss:0.050, val_acc:0.954]
Epoch [61/120    avg_loss:0.045, val_acc:0.967]
Epoch [62/120    avg_loss:0.031, val_acc:0.967]
Epoch [63/120    avg_loss:0.032, val_acc:0.965]
Epoch [64/120    avg_loss:0.037, val_acc:0.967]
Epoch [65/120    avg_loss:0.037, val_acc:0.962]
Epoch [66/120    avg_loss:0.031, val_acc:0.969]
Epoch [67/120    avg_loss:0.024, val_acc:0.969]
Epoch [68/120    avg_loss:0.027, val_acc:0.968]
Epoch [69/120    avg_loss:0.027, val_acc:0.963]
Epoch [70/120    avg_loss:0.019, val_acc:0.968]
Epoch [71/120    avg_loss:0.017, val_acc:0.976]
Epoch [72/120    avg_loss:0.024, val_acc:0.968]
Epoch [73/120    avg_loss:0.051, val_acc:0.966]
Epoch [74/120    avg_loss:0.031, val_acc:0.973]
Epoch [75/120    avg_loss:0.015, val_acc:0.973]
Epoch [76/120    avg_loss:0.016, val_acc:0.977]
Epoch [77/120    avg_loss:0.015, val_acc:0.973]
Epoch [78/120    avg_loss:0.014, val_acc:0.969]
Epoch [79/120    avg_loss:0.018, val_acc:0.966]
Epoch [80/120    avg_loss:0.018, val_acc:0.976]
Epoch [81/120    avg_loss:0.016, val_acc:0.976]
Epoch [82/120    avg_loss:0.012, val_acc:0.974]
Epoch [83/120    avg_loss:0.009, val_acc:0.970]
Epoch [84/120    avg_loss:0.009, val_acc:0.974]
Epoch [85/120    avg_loss:0.010, val_acc:0.971]
Epoch [86/120    avg_loss:0.012, val_acc:0.981]
Epoch [87/120    avg_loss:0.008, val_acc:0.978]
Epoch [88/120    avg_loss:0.018, val_acc:0.982]
Epoch [89/120    avg_loss:0.013, val_acc:0.968]
Epoch [90/120    avg_loss:0.014, val_acc:0.977]
Epoch [91/120    avg_loss:0.012, val_acc:0.978]
Epoch [92/120    avg_loss:0.007, val_acc:0.974]
Epoch [93/120    avg_loss:0.007, val_acc:0.977]
Epoch [94/120    avg_loss:0.008, val_acc:0.975]
Epoch [95/120    avg_loss:0.007, val_acc:0.981]
Epoch [96/120    avg_loss:0.009, val_acc:0.977]
Epoch [97/120    avg_loss:0.026, val_acc:0.970]
Epoch [98/120    avg_loss:0.025, val_acc:0.967]
Epoch [99/120    avg_loss:0.011, val_acc:0.974]
Epoch [100/120    avg_loss:0.009, val_acc:0.978]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.977]
Epoch [103/120    avg_loss:0.010, val_acc:0.977]
Epoch [104/120    avg_loss:0.009, val_acc:0.978]
Epoch [105/120    avg_loss:0.008, val_acc:0.980]
Epoch [106/120    avg_loss:0.011, val_acc:0.976]
Epoch [107/120    avg_loss:0.010, val_acc:0.980]
Epoch [108/120    avg_loss:0.009, val_acc:0.981]
Epoch [109/120    avg_loss:0.008, val_acc:0.978]
Epoch [110/120    avg_loss:0.010, val_acc:0.977]
Epoch [111/120    avg_loss:0.010, val_acc:0.975]
Epoch [112/120    avg_loss:0.006, val_acc:0.980]
Epoch [113/120    avg_loss:0.005, val_acc:0.978]
Epoch [114/120    avg_loss:0.014, val_acc:0.974]
Epoch [115/120    avg_loss:0.016, val_acc:0.979]
Epoch [116/120    avg_loss:0.009, val_acc:0.981]
Epoch [117/120    avg_loss:0.006, val_acc:0.979]
Epoch [118/120    avg_loss:0.007, val_acc:0.981]
Epoch [119/120    avg_loss:0.006, val_acc:0.981]
Epoch [120/120    avg_loss:0.006, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1255    3    2    0    0    0    0    0    7   10    6    0
     0    2    0]
 [   0    0    1  720    0   11    0    0    0    7    1    4    2    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  650    0    0    0    0    1    6    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   15    0    0    3    0
     0    0    0]
 [   0    0    5   11    0    4    0    0    0    0  848    3    0    0
     0    4    0]
 [   0    0    5    0    0    0    4    0    0    0    6 2194    0    1
     0    0    0]
 [   0    0    0   20   11   13    0    0    0    0    1    7  480    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   22    0    0    0    0    0    0    0
    33  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.52845528455285

F1 scores:
[       nan 1.         0.98392787 0.95936043 0.97038724 0.96071829
 0.97524381 0.98039216 0.99883856 0.65217391 0.97527315 0.99074283
 0.93113482 0.99730458 0.98441558 0.90542636 0.98823529]

Kappa:
0.971816051665078
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff13701d748>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.403, val_acc:0.528]
Epoch [2/120    avg_loss:1.892, val_acc:0.536]
Epoch [3/120    avg_loss:1.535, val_acc:0.645]
Epoch [4/120    avg_loss:1.260, val_acc:0.646]
Epoch [5/120    avg_loss:1.078, val_acc:0.662]
Epoch [6/120    avg_loss:0.892, val_acc:0.726]
Epoch [7/120    avg_loss:0.825, val_acc:0.752]
Epoch [8/120    avg_loss:0.733, val_acc:0.783]
Epoch [9/120    avg_loss:0.591, val_acc:0.795]
Epoch [10/120    avg_loss:0.542, val_acc:0.830]
Epoch [11/120    avg_loss:0.445, val_acc:0.840]
Epoch [12/120    avg_loss:0.405, val_acc:0.860]
Epoch [13/120    avg_loss:0.368, val_acc:0.856]
Epoch [14/120    avg_loss:0.380, val_acc:0.855]
Epoch [15/120    avg_loss:0.351, val_acc:0.883]
Epoch [16/120    avg_loss:0.255, val_acc:0.908]
Epoch [17/120    avg_loss:0.293, val_acc:0.860]
Epoch [18/120    avg_loss:0.345, val_acc:0.900]
Epoch [19/120    avg_loss:0.306, val_acc:0.900]
Epoch [20/120    avg_loss:0.210, val_acc:0.904]
Epoch [21/120    avg_loss:0.226, val_acc:0.922]
Epoch [22/120    avg_loss:0.176, val_acc:0.920]
Epoch [23/120    avg_loss:0.150, val_acc:0.946]
Epoch [24/120    avg_loss:0.170, val_acc:0.930]
Epoch [25/120    avg_loss:0.136, val_acc:0.935]
Epoch [26/120    avg_loss:0.179, val_acc:0.926]
Epoch [27/120    avg_loss:0.162, val_acc:0.927]
Epoch [28/120    avg_loss:0.249, val_acc:0.921]
Epoch [29/120    avg_loss:0.168, val_acc:0.923]
Epoch [30/120    avg_loss:0.130, val_acc:0.953]
Epoch [31/120    avg_loss:0.086, val_acc:0.955]
Epoch [32/120    avg_loss:0.112, val_acc:0.931]
Epoch [33/120    avg_loss:0.089, val_acc:0.970]
Epoch [34/120    avg_loss:0.073, val_acc:0.941]
Epoch [35/120    avg_loss:0.078, val_acc:0.946]
Epoch [36/120    avg_loss:0.095, val_acc:0.935]
Epoch [37/120    avg_loss:0.062, val_acc:0.964]
Epoch [38/120    avg_loss:0.070, val_acc:0.962]
Epoch [39/120    avg_loss:0.076, val_acc:0.950]
Epoch [40/120    avg_loss:0.087, val_acc:0.941]
Epoch [41/120    avg_loss:0.066, val_acc:0.956]
Epoch [42/120    avg_loss:0.058, val_acc:0.964]
Epoch [43/120    avg_loss:0.062, val_acc:0.955]
Epoch [44/120    avg_loss:0.087, val_acc:0.961]
Epoch [45/120    avg_loss:0.048, val_acc:0.975]
Epoch [46/120    avg_loss:0.056, val_acc:0.961]
Epoch [47/120    avg_loss:0.044, val_acc:0.965]
Epoch [48/120    avg_loss:0.060, val_acc:0.959]
Epoch [49/120    avg_loss:0.065, val_acc:0.966]
Epoch [50/120    avg_loss:0.073, val_acc:0.965]
Epoch [51/120    avg_loss:0.041, val_acc:0.961]
Epoch [52/120    avg_loss:0.037, val_acc:0.964]
Epoch [53/120    avg_loss:0.034, val_acc:0.970]
Epoch [54/120    avg_loss:0.030, val_acc:0.971]
Epoch [55/120    avg_loss:0.034, val_acc:0.966]
Epoch [56/120    avg_loss:0.031, val_acc:0.965]
Epoch [57/120    avg_loss:0.066, val_acc:0.952]
Epoch [58/120    avg_loss:0.058, val_acc:0.979]
Epoch [59/120    avg_loss:0.039, val_acc:0.977]
Epoch [60/120    avg_loss:0.073, val_acc:0.976]
Epoch [61/120    avg_loss:0.044, val_acc:0.968]
Epoch [62/120    avg_loss:0.045, val_acc:0.973]
Epoch [63/120    avg_loss:0.043, val_acc:0.975]
Epoch [64/120    avg_loss:0.027, val_acc:0.976]
Epoch [65/120    avg_loss:0.029, val_acc:0.965]
Epoch [66/120    avg_loss:0.044, val_acc:0.957]
Epoch [67/120    avg_loss:0.040, val_acc:0.979]
Epoch [68/120    avg_loss:0.037, val_acc:0.972]
Epoch [69/120    avg_loss:0.030, val_acc:0.977]
Epoch [70/120    avg_loss:0.021, val_acc:0.981]
Epoch [71/120    avg_loss:0.033, val_acc:0.976]
Epoch [72/120    avg_loss:0.026, val_acc:0.977]
Epoch [73/120    avg_loss:0.027, val_acc:0.983]
Epoch [74/120    avg_loss:0.033, val_acc:0.980]
Epoch [75/120    avg_loss:0.023, val_acc:0.982]
Epoch [76/120    avg_loss:0.037, val_acc:0.965]
Epoch [77/120    avg_loss:0.046, val_acc:0.964]
Epoch [78/120    avg_loss:0.033, val_acc:0.974]
Epoch [79/120    avg_loss:0.033, val_acc:0.973]
Epoch [80/120    avg_loss:0.024, val_acc:0.976]
Epoch [81/120    avg_loss:0.021, val_acc:0.981]
Epoch [82/120    avg_loss:0.013, val_acc:0.983]
Epoch [83/120    avg_loss:0.017, val_acc:0.983]
Epoch [84/120    avg_loss:0.015, val_acc:0.973]
Epoch [85/120    avg_loss:0.015, val_acc:0.981]
Epoch [86/120    avg_loss:0.014, val_acc:0.980]
Epoch [87/120    avg_loss:0.018, val_acc:0.984]
Epoch [88/120    avg_loss:0.020, val_acc:0.983]
Epoch [89/120    avg_loss:0.024, val_acc:0.981]
Epoch [90/120    avg_loss:0.020, val_acc:0.976]
Epoch [91/120    avg_loss:0.013, val_acc:0.984]
Epoch [92/120    avg_loss:0.013, val_acc:0.974]
Epoch [93/120    avg_loss:0.015, val_acc:0.980]
Epoch [94/120    avg_loss:0.012, val_acc:0.982]
Epoch [95/120    avg_loss:0.020, val_acc:0.977]
Epoch [96/120    avg_loss:0.012, val_acc:0.986]
Epoch [97/120    avg_loss:0.017, val_acc:0.976]
Epoch [98/120    avg_loss:0.032, val_acc:0.964]
Epoch [99/120    avg_loss:0.048, val_acc:0.972]
Epoch [100/120    avg_loss:0.028, val_acc:0.977]
Epoch [101/120    avg_loss:0.079, val_acc:0.952]
Epoch [102/120    avg_loss:0.092, val_acc:0.954]
Epoch [103/120    avg_loss:0.065, val_acc:0.964]
Epoch [104/120    avg_loss:0.039, val_acc:0.971]
Epoch [105/120    avg_loss:0.080, val_acc:0.958]
Epoch [106/120    avg_loss:0.030, val_acc:0.966]
Epoch [107/120    avg_loss:0.027, val_acc:0.977]
Epoch [108/120    avg_loss:0.028, val_acc:0.976]
Epoch [109/120    avg_loss:0.017, val_acc:0.988]
Epoch [110/120    avg_loss:0.016, val_acc:0.983]
Epoch [111/120    avg_loss:0.011, val_acc:0.986]
Epoch [112/120    avg_loss:0.011, val_acc:0.983]
Epoch [113/120    avg_loss:0.014, val_acc:0.983]
Epoch [114/120    avg_loss:0.009, val_acc:0.989]
Epoch [115/120    avg_loss:0.011, val_acc:0.983]
Epoch [116/120    avg_loss:0.009, val_acc:0.988]
Epoch [117/120    avg_loss:0.009, val_acc:0.979]
Epoch [118/120    avg_loss:0.011, val_acc:0.983]
Epoch [119/120    avg_loss:0.012, val_acc:0.983]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    4 1246    7    0    0    1    0    0    0   13   10    4    0
     0    0    0]
 [   0    0    0  728    0   13    0    0    0    3    1    0    0    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   19    7    0    7    0    0    0    0  827   12    0    0
     0    3    0]
 [   0    0    2    0    0    0   11    0    0    0    6 2189    0    2
     0    0    0]
 [   0    0    0   21    3   16    0    0    0    0    1    9  480    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    1    0    0    0
  1130    0    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    54  270    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.00813008130082

F1 scores:
[       nan 0.91566265 0.97648903 0.96423841 0.99300699 0.95060373
 0.97329377 1.         0.99883856 0.87804878 0.95773017 0.98803882
 0.94025465 0.98930481 0.9728799  0.87096774 0.95857988]

Kappa:
0.9658736525229591
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd34d32d780>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.501, val_acc:0.530]
Epoch [2/120    avg_loss:1.903, val_acc:0.603]
Epoch [3/120    avg_loss:1.611, val_acc:0.641]
Epoch [4/120    avg_loss:1.388, val_acc:0.662]
Epoch [5/120    avg_loss:1.202, val_acc:0.667]
Epoch [6/120    avg_loss:1.014, val_acc:0.758]
Epoch [7/120    avg_loss:0.770, val_acc:0.737]
Epoch [8/120    avg_loss:0.673, val_acc:0.794]
Epoch [9/120    avg_loss:0.695, val_acc:0.781]
Epoch [10/120    avg_loss:0.627, val_acc:0.783]
Epoch [11/120    avg_loss:0.535, val_acc:0.829]
Epoch [12/120    avg_loss:0.437, val_acc:0.830]
Epoch [13/120    avg_loss:0.361, val_acc:0.854]
Epoch [14/120    avg_loss:0.356, val_acc:0.820]
Epoch [15/120    avg_loss:0.339, val_acc:0.879]
Epoch [16/120    avg_loss:0.267, val_acc:0.868]
Epoch [17/120    avg_loss:0.214, val_acc:0.894]
Epoch [18/120    avg_loss:0.215, val_acc:0.873]
Epoch [19/120    avg_loss:0.181, val_acc:0.909]
Epoch [20/120    avg_loss:0.175, val_acc:0.917]
Epoch [21/120    avg_loss:0.155, val_acc:0.923]
Epoch [22/120    avg_loss:0.211, val_acc:0.923]
Epoch [23/120    avg_loss:0.176, val_acc:0.932]
Epoch [24/120    avg_loss:0.160, val_acc:0.913]
Epoch [25/120    avg_loss:0.172, val_acc:0.896]
Epoch [26/120    avg_loss:0.153, val_acc:0.914]
Epoch [27/120    avg_loss:0.105, val_acc:0.912]
Epoch [28/120    avg_loss:0.125, val_acc:0.921]
Epoch [29/120    avg_loss:0.166, val_acc:0.905]
Epoch [30/120    avg_loss:0.113, val_acc:0.934]
Epoch [31/120    avg_loss:0.112, val_acc:0.932]
Epoch [32/120    avg_loss:0.093, val_acc:0.936]
Epoch [33/120    avg_loss:0.096, val_acc:0.938]
Epoch [34/120    avg_loss:0.074, val_acc:0.944]
Epoch [35/120    avg_loss:0.091, val_acc:0.929]
Epoch [36/120    avg_loss:0.087, val_acc:0.944]
Epoch [37/120    avg_loss:0.078, val_acc:0.940]
Epoch [38/120    avg_loss:0.073, val_acc:0.946]
Epoch [39/120    avg_loss:0.064, val_acc:0.942]
Epoch [40/120    avg_loss:0.049, val_acc:0.955]
Epoch [41/120    avg_loss:0.048, val_acc:0.943]
Epoch [42/120    avg_loss:0.070, val_acc:0.933]
Epoch [43/120    avg_loss:0.070, val_acc:0.940]
Epoch [44/120    avg_loss:0.077, val_acc:0.956]
Epoch [45/120    avg_loss:0.051, val_acc:0.954]
Epoch [46/120    avg_loss:0.048, val_acc:0.961]
Epoch [47/120    avg_loss:0.056, val_acc:0.950]
Epoch [48/120    avg_loss:0.058, val_acc:0.953]
Epoch [49/120    avg_loss:0.040, val_acc:0.964]
Epoch [50/120    avg_loss:0.046, val_acc:0.951]
Epoch [51/120    avg_loss:0.036, val_acc:0.938]
Epoch [52/120    avg_loss:0.089, val_acc:0.950]
Epoch [53/120    avg_loss:0.062, val_acc:0.947]
Epoch [54/120    avg_loss:0.042, val_acc:0.962]
Epoch [55/120    avg_loss:0.039, val_acc:0.956]
Epoch [56/120    avg_loss:0.027, val_acc:0.961]
Epoch [57/120    avg_loss:0.028, val_acc:0.961]
Epoch [58/120    avg_loss:0.043, val_acc:0.959]
Epoch [59/120    avg_loss:0.032, val_acc:0.950]
Epoch [60/120    avg_loss:0.039, val_acc:0.970]
Epoch [61/120    avg_loss:0.039, val_acc:0.933]
Epoch [62/120    avg_loss:0.026, val_acc:0.953]
Epoch [63/120    avg_loss:0.030, val_acc:0.944]
Epoch [64/120    avg_loss:0.022, val_acc:0.957]
Epoch [65/120    avg_loss:0.039, val_acc:0.951]
Epoch [66/120    avg_loss:0.040, val_acc:0.965]
Epoch [67/120    avg_loss:0.034, val_acc:0.969]
Epoch [68/120    avg_loss:0.024, val_acc:0.953]
Epoch [69/120    avg_loss:0.033, val_acc:0.958]
Epoch [70/120    avg_loss:0.017, val_acc:0.966]
Epoch [71/120    avg_loss:0.016, val_acc:0.969]
Epoch [72/120    avg_loss:0.023, val_acc:0.969]
Epoch [73/120    avg_loss:0.028, val_acc:0.961]
Epoch [74/120    avg_loss:0.022, val_acc:0.964]
Epoch [75/120    avg_loss:0.017, val_acc:0.970]
Epoch [76/120    avg_loss:0.012, val_acc:0.970]
Epoch [77/120    avg_loss:0.011, val_acc:0.971]
Epoch [78/120    avg_loss:0.014, val_acc:0.970]
Epoch [79/120    avg_loss:0.012, val_acc:0.970]
Epoch [80/120    avg_loss:0.015, val_acc:0.973]
Epoch [81/120    avg_loss:0.011, val_acc:0.969]
Epoch [82/120    avg_loss:0.012, val_acc:0.968]
Epoch [83/120    avg_loss:0.012, val_acc:0.970]
Epoch [84/120    avg_loss:0.009, val_acc:0.971]
Epoch [85/120    avg_loss:0.012, val_acc:0.971]
Epoch [86/120    avg_loss:0.011, val_acc:0.973]
Epoch [87/120    avg_loss:0.011, val_acc:0.974]
Epoch [88/120    avg_loss:0.008, val_acc:0.973]
Epoch [89/120    avg_loss:0.011, val_acc:0.971]
Epoch [90/120    avg_loss:0.011, val_acc:0.971]
Epoch [91/120    avg_loss:0.010, val_acc:0.970]
Epoch [92/120    avg_loss:0.009, val_acc:0.969]
Epoch [93/120    avg_loss:0.012, val_acc:0.968]
Epoch [94/120    avg_loss:0.013, val_acc:0.970]
Epoch [95/120    avg_loss:0.010, val_acc:0.969]
Epoch [96/120    avg_loss:0.009, val_acc:0.973]
Epoch [97/120    avg_loss:0.011, val_acc:0.974]
Epoch [98/120    avg_loss:0.009, val_acc:0.973]
Epoch [99/120    avg_loss:0.011, val_acc:0.974]
Epoch [100/120    avg_loss:0.011, val_acc:0.971]
Epoch [101/120    avg_loss:0.010, val_acc:0.974]
Epoch [102/120    avg_loss:0.009, val_acc:0.974]
Epoch [103/120    avg_loss:0.009, val_acc:0.974]
Epoch [104/120    avg_loss:0.010, val_acc:0.974]
Epoch [105/120    avg_loss:0.011, val_acc:0.974]
Epoch [106/120    avg_loss:0.009, val_acc:0.974]
Epoch [107/120    avg_loss:0.010, val_acc:0.974]
Epoch [108/120    avg_loss:0.009, val_acc:0.974]
Epoch [109/120    avg_loss:0.009, val_acc:0.974]
Epoch [110/120    avg_loss:0.011, val_acc:0.974]
Epoch [111/120    avg_loss:0.009, val_acc:0.974]
Epoch [112/120    avg_loss:0.007, val_acc:0.975]
Epoch [113/120    avg_loss:0.008, val_acc:0.970]
Epoch [114/120    avg_loss:0.010, val_acc:0.969]
Epoch [115/120    avg_loss:0.010, val_acc:0.970]
Epoch [116/120    avg_loss:0.008, val_acc:0.973]
Epoch [117/120    avg_loss:0.010, val_acc:0.975]
Epoch [118/120    avg_loss:0.008, val_acc:0.974]
Epoch [119/120    avg_loss:0.008, val_acc:0.974]
Epoch [120/120    avg_loss:0.011, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1245    4    1    0    1    0    0    1    9   17    6    0
     0    1    0]
 [   0    0    0  707    4   17    0    0    0   12    0    0    7    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    3    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0   19   44    0    6    0    0    0    0  799    1    0    0
     3    3    0]
 [   0    0    7    0    0    0    6    0    0    0   12 2181    1    3
     0    0    0]
 [   0    0    0   22    6   11    0    0    0    0    6    6  478    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    2    0    0    0
  1130    0    0]
 [   0    0    0    0    0    0   21    0    0    0    0    0    0    0
    44  282    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.45528455284553

F1 scores:
[       nan 1.         0.9741784  0.92539267 0.97482838 0.9480663
 0.97837435 0.98039216 1.         0.58333333 0.9383441  0.98777174
 0.9308666  0.9919571  0.97497843 0.89099526 0.96511628]

Kappa:
0.9595889847120485
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc294b7d748>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.464, val_acc:0.553]
Epoch [2/120    avg_loss:1.913, val_acc:0.621]
Epoch [3/120    avg_loss:1.620, val_acc:0.635]
Epoch [4/120    avg_loss:1.344, val_acc:0.689]
Epoch [5/120    avg_loss:1.149, val_acc:0.721]
Epoch [6/120    avg_loss:0.904, val_acc:0.760]
Epoch [7/120    avg_loss:0.709, val_acc:0.791]
Epoch [8/120    avg_loss:0.617, val_acc:0.806]
Epoch [9/120    avg_loss:0.670, val_acc:0.764]
Epoch [10/120    avg_loss:0.603, val_acc:0.829]
Epoch [11/120    avg_loss:0.748, val_acc:0.783]
Epoch [12/120    avg_loss:0.664, val_acc:0.795]
Epoch [13/120    avg_loss:0.458, val_acc:0.862]
Epoch [14/120    avg_loss:0.354, val_acc:0.875]
Epoch [15/120    avg_loss:0.340, val_acc:0.877]
Epoch [16/120    avg_loss:0.334, val_acc:0.877]
Epoch [17/120    avg_loss:0.291, val_acc:0.844]
Epoch [18/120    avg_loss:0.295, val_acc:0.866]
Epoch [19/120    avg_loss:0.257, val_acc:0.893]
Epoch [20/120    avg_loss:0.244, val_acc:0.917]
Epoch [21/120    avg_loss:0.215, val_acc:0.901]
Epoch [22/120    avg_loss:0.177, val_acc:0.918]
Epoch [23/120    avg_loss:0.189, val_acc:0.929]
Epoch [24/120    avg_loss:0.148, val_acc:0.928]
Epoch [25/120    avg_loss:0.127, val_acc:0.928]
Epoch [26/120    avg_loss:0.144, val_acc:0.936]
Epoch [27/120    avg_loss:0.131, val_acc:0.934]
Epoch [28/120    avg_loss:0.105, val_acc:0.948]
Epoch [29/120    avg_loss:0.104, val_acc:0.929]
Epoch [30/120    avg_loss:0.133, val_acc:0.948]
Epoch [31/120    avg_loss:0.080, val_acc:0.943]
Epoch [32/120    avg_loss:0.079, val_acc:0.941]
Epoch [33/120    avg_loss:0.082, val_acc:0.942]
Epoch [34/120    avg_loss:0.087, val_acc:0.943]
Epoch [35/120    avg_loss:0.077, val_acc:0.941]
Epoch [36/120    avg_loss:0.075, val_acc:0.951]
Epoch [37/120    avg_loss:0.075, val_acc:0.943]
Epoch [38/120    avg_loss:0.095, val_acc:0.940]
Epoch [39/120    avg_loss:0.085, val_acc:0.945]
Epoch [40/120    avg_loss:0.081, val_acc:0.952]
Epoch [41/120    avg_loss:0.052, val_acc:0.966]
Epoch [42/120    avg_loss:0.049, val_acc:0.965]
Epoch [43/120    avg_loss:0.082, val_acc:0.945]
Epoch [44/120    avg_loss:0.048, val_acc:0.963]
Epoch [45/120    avg_loss:0.058, val_acc:0.953]
Epoch [46/120    avg_loss:0.051, val_acc:0.963]
Epoch [47/120    avg_loss:0.044, val_acc:0.964]
Epoch [48/120    avg_loss:0.056, val_acc:0.957]
Epoch [49/120    avg_loss:0.050, val_acc:0.970]
Epoch [50/120    avg_loss:0.043, val_acc:0.964]
Epoch [51/120    avg_loss:0.045, val_acc:0.967]
Epoch [52/120    avg_loss:0.046, val_acc:0.970]
Epoch [53/120    avg_loss:0.047, val_acc:0.971]
Epoch [54/120    avg_loss:0.038, val_acc:0.975]
Epoch [55/120    avg_loss:0.058, val_acc:0.962]
Epoch [56/120    avg_loss:0.035, val_acc:0.969]
Epoch [57/120    avg_loss:0.023, val_acc:0.966]
Epoch [58/120    avg_loss:0.034, val_acc:0.968]
Epoch [59/120    avg_loss:0.047, val_acc:0.966]
Epoch [60/120    avg_loss:0.028, val_acc:0.973]
Epoch [61/120    avg_loss:0.025, val_acc:0.971]
Epoch [62/120    avg_loss:0.022, val_acc:0.973]
Epoch [63/120    avg_loss:0.022, val_acc:0.969]
Epoch [64/120    avg_loss:0.023, val_acc:0.975]
Epoch [65/120    avg_loss:0.023, val_acc:0.979]
Epoch [66/120    avg_loss:0.033, val_acc:0.961]
Epoch [67/120    avg_loss:0.088, val_acc:0.946]
Epoch [68/120    avg_loss:0.041, val_acc:0.966]
Epoch [69/120    avg_loss:0.029, val_acc:0.963]
Epoch [70/120    avg_loss:0.029, val_acc:0.976]
Epoch [71/120    avg_loss:0.043, val_acc:0.953]
Epoch [72/120    avg_loss:0.040, val_acc:0.978]
Epoch [73/120    avg_loss:0.032, val_acc:0.976]
Epoch [74/120    avg_loss:0.092, val_acc:0.958]
Epoch [75/120    avg_loss:0.042, val_acc:0.968]
Epoch [76/120    avg_loss:0.029, val_acc:0.971]
Epoch [77/120    avg_loss:0.020, val_acc:0.973]
Epoch [78/120    avg_loss:0.021, val_acc:0.976]
Epoch [79/120    avg_loss:0.012, val_acc:0.976]
Epoch [80/120    avg_loss:0.014, val_acc:0.978]
Epoch [81/120    avg_loss:0.013, val_acc:0.977]
Epoch [82/120    avg_loss:0.013, val_acc:0.976]
Epoch [83/120    avg_loss:0.013, val_acc:0.975]
Epoch [84/120    avg_loss:0.013, val_acc:0.975]
Epoch [85/120    avg_loss:0.012, val_acc:0.975]
Epoch [86/120    avg_loss:0.013, val_acc:0.977]
Epoch [87/120    avg_loss:0.010, val_acc:0.978]
Epoch [88/120    avg_loss:0.016, val_acc:0.977]
Epoch [89/120    avg_loss:0.011, val_acc:0.978]
Epoch [90/120    avg_loss:0.011, val_acc:0.978]
Epoch [91/120    avg_loss:0.010, val_acc:0.977]
Epoch [92/120    avg_loss:0.010, val_acc:0.977]
Epoch [93/120    avg_loss:0.009, val_acc:0.977]
Epoch [94/120    avg_loss:0.009, val_acc:0.977]
Epoch [95/120    avg_loss:0.010, val_acc:0.977]
Epoch [96/120    avg_loss:0.010, val_acc:0.977]
Epoch [97/120    avg_loss:0.010, val_acc:0.977]
Epoch [98/120    avg_loss:0.016, val_acc:0.976]
Epoch [99/120    avg_loss:0.011, val_acc:0.977]
Epoch [100/120    avg_loss:0.012, val_acc:0.976]
Epoch [101/120    avg_loss:0.010, val_acc:0.976]
Epoch [102/120    avg_loss:0.010, val_acc:0.977]
Epoch [103/120    avg_loss:0.010, val_acc:0.976]
Epoch [104/120    avg_loss:0.009, val_acc:0.976]
Epoch [105/120    avg_loss:0.010, val_acc:0.976]
Epoch [106/120    avg_loss:0.016, val_acc:0.976]
Epoch [107/120    avg_loss:0.008, val_acc:0.976]
Epoch [108/120    avg_loss:0.009, val_acc:0.976]
Epoch [109/120    avg_loss:0.011, val_acc:0.976]
Epoch [110/120    avg_loss:0.010, val_acc:0.976]
Epoch [111/120    avg_loss:0.012, val_acc:0.976]
Epoch [112/120    avg_loss:0.010, val_acc:0.976]
Epoch [113/120    avg_loss:0.011, val_acc:0.976]
Epoch [114/120    avg_loss:0.012, val_acc:0.976]
Epoch [115/120    avg_loss:0.014, val_acc:0.976]
Epoch [116/120    avg_loss:0.012, val_acc:0.976]
Epoch [117/120    avg_loss:0.012, val_acc:0.976]
Epoch [118/120    avg_loss:0.013, val_acc:0.976]
Epoch [119/120    avg_loss:0.012, val_acc:0.976]
Epoch [120/120    avg_loss:0.011, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1247    4    0    0    1    0    0    3    4   15    4    0
     0    7    0]
 [   0    0    1  710    2    8    0    0    0   12    0    0    7    7
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    5    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   13    0    0    1    0
     0    0    0]
 [   0    0    3   33    0    7    0    0    0    0  819   10    0    0
     0    3    0]
 [   0    0    3    0    0    0   10    0    0    0   14 2158   23    2
     0    0    0]
 [   0    0    0   16    5   13    0    0    0    0    6   10  483    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    1    0    3    0    0    0
  1133    0    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
    49  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.50948509485094

F1 scores:
[       nan 0.975      0.98150335 0.93791281 0.98383372 0.96089385
 0.97837435 1.         0.99767442 0.50980392 0.95177223 0.98001817
 0.91563981 0.9762533  0.97630332 0.88087774 0.98203593]

Kappa:
0.9602201935392141
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f45d2d15748>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.437, val_acc:0.450]
Epoch [2/120    avg_loss:1.896, val_acc:0.556]
Epoch [3/120    avg_loss:1.666, val_acc:0.615]
Epoch [4/120    avg_loss:1.421, val_acc:0.614]
Epoch [5/120    avg_loss:1.263, val_acc:0.667]
Epoch [6/120    avg_loss:1.070, val_acc:0.780]
Epoch [7/120    avg_loss:0.901, val_acc:0.727]
Epoch [8/120    avg_loss:0.782, val_acc:0.792]
Epoch [9/120    avg_loss:0.716, val_acc:0.806]
Epoch [10/120    avg_loss:0.578, val_acc:0.797]
Epoch [11/120    avg_loss:0.584, val_acc:0.855]
Epoch [12/120    avg_loss:0.516, val_acc:0.826]
Epoch [13/120    avg_loss:0.438, val_acc:0.812]
Epoch [14/120    avg_loss:0.385, val_acc:0.864]
Epoch [15/120    avg_loss:0.382, val_acc:0.872]
Epoch [16/120    avg_loss:0.358, val_acc:0.859]
Epoch [17/120    avg_loss:0.298, val_acc:0.856]
Epoch [18/120    avg_loss:0.277, val_acc:0.886]
Epoch [19/120    avg_loss:0.278, val_acc:0.905]
Epoch [20/120    avg_loss:0.296, val_acc:0.889]
Epoch [21/120    avg_loss:0.193, val_acc:0.917]
Epoch [22/120    avg_loss:0.204, val_acc:0.917]
Epoch [23/120    avg_loss:0.218, val_acc:0.930]
Epoch [24/120    avg_loss:0.176, val_acc:0.907]
Epoch [25/120    avg_loss:0.168, val_acc:0.936]
Epoch [26/120    avg_loss:0.127, val_acc:0.944]
Epoch [27/120    avg_loss:0.130, val_acc:0.925]
Epoch [28/120    avg_loss:0.142, val_acc:0.907]
Epoch [29/120    avg_loss:0.189, val_acc:0.933]
Epoch [30/120    avg_loss:0.131, val_acc:0.936]
Epoch [31/120    avg_loss:0.111, val_acc:0.941]
Epoch [32/120    avg_loss:0.165, val_acc:0.889]
Epoch [33/120    avg_loss:0.172, val_acc:0.945]
Epoch [34/120    avg_loss:0.114, val_acc:0.952]
Epoch [35/120    avg_loss:0.084, val_acc:0.953]
Epoch [36/120    avg_loss:0.081, val_acc:0.922]
Epoch [37/120    avg_loss:0.075, val_acc:0.950]
Epoch [38/120    avg_loss:0.075, val_acc:0.957]
Epoch [39/120    avg_loss:0.077, val_acc:0.944]
Epoch [40/120    avg_loss:0.119, val_acc:0.943]
Epoch [41/120    avg_loss:0.080, val_acc:0.955]
Epoch [42/120    avg_loss:0.077, val_acc:0.948]
Epoch [43/120    avg_loss:0.083, val_acc:0.938]
Epoch [44/120    avg_loss:0.082, val_acc:0.955]
Epoch [45/120    avg_loss:0.075, val_acc:0.913]
Epoch [46/120    avg_loss:0.074, val_acc:0.961]
Epoch [47/120    avg_loss:0.053, val_acc:0.962]
Epoch [48/120    avg_loss:0.049, val_acc:0.975]
Epoch [49/120    avg_loss:0.051, val_acc:0.971]
Epoch [50/120    avg_loss:0.038, val_acc:0.977]
Epoch [51/120    avg_loss:0.042, val_acc:0.966]
Epoch [52/120    avg_loss:0.039, val_acc:0.958]
Epoch [53/120    avg_loss:0.033, val_acc:0.971]
Epoch [54/120    avg_loss:0.037, val_acc:0.966]
Epoch [55/120    avg_loss:0.042, val_acc:0.948]
Epoch [56/120    avg_loss:0.063, val_acc:0.951]
Epoch [57/120    avg_loss:0.160, val_acc:0.939]
Epoch [58/120    avg_loss:0.101, val_acc:0.956]
Epoch [59/120    avg_loss:0.051, val_acc:0.966]
Epoch [60/120    avg_loss:0.043, val_acc:0.973]
Epoch [61/120    avg_loss:0.048, val_acc:0.956]
Epoch [62/120    avg_loss:0.035, val_acc:0.969]
Epoch [63/120    avg_loss:0.041, val_acc:0.968]
Epoch [64/120    avg_loss:0.030, val_acc:0.978]
Epoch [65/120    avg_loss:0.025, val_acc:0.979]
Epoch [66/120    avg_loss:0.023, val_acc:0.979]
Epoch [67/120    avg_loss:0.017, val_acc:0.980]
Epoch [68/120    avg_loss:0.023, val_acc:0.982]
Epoch [69/120    avg_loss:0.017, val_acc:0.982]
Epoch [70/120    avg_loss:0.018, val_acc:0.982]
Epoch [71/120    avg_loss:0.018, val_acc:0.982]
Epoch [72/120    avg_loss:0.027, val_acc:0.981]
Epoch [73/120    avg_loss:0.016, val_acc:0.982]
Epoch [74/120    avg_loss:0.017, val_acc:0.982]
Epoch [75/120    avg_loss:0.019, val_acc:0.981]
Epoch [76/120    avg_loss:0.024, val_acc:0.984]
Epoch [77/120    avg_loss:0.018, val_acc:0.984]
Epoch [78/120    avg_loss:0.018, val_acc:0.982]
Epoch [79/120    avg_loss:0.016, val_acc:0.984]
Epoch [80/120    avg_loss:0.020, val_acc:0.982]
Epoch [81/120    avg_loss:0.015, val_acc:0.982]
Epoch [82/120    avg_loss:0.015, val_acc:0.984]
Epoch [83/120    avg_loss:0.015, val_acc:0.984]
Epoch [84/120    avg_loss:0.018, val_acc:0.981]
Epoch [85/120    avg_loss:0.016, val_acc:0.981]
Epoch [86/120    avg_loss:0.015, val_acc:0.980]
Epoch [87/120    avg_loss:0.014, val_acc:0.984]
Epoch [88/120    avg_loss:0.016, val_acc:0.985]
Epoch [89/120    avg_loss:0.014, val_acc:0.984]
Epoch [90/120    avg_loss:0.019, val_acc:0.981]
Epoch [91/120    avg_loss:0.016, val_acc:0.980]
Epoch [92/120    avg_loss:0.016, val_acc:0.980]
Epoch [93/120    avg_loss:0.015, val_acc:0.980]
Epoch [94/120    avg_loss:0.020, val_acc:0.980]
Epoch [95/120    avg_loss:0.013, val_acc:0.979]
Epoch [96/120    avg_loss:0.016, val_acc:0.982]
Epoch [97/120    avg_loss:0.016, val_acc:0.981]
Epoch [98/120    avg_loss:0.015, val_acc:0.982]
Epoch [99/120    avg_loss:0.017, val_acc:0.981]
Epoch [100/120    avg_loss:0.018, val_acc:0.981]
Epoch [101/120    avg_loss:0.012, val_acc:0.981]
Epoch [102/120    avg_loss:0.017, val_acc:0.981]
Epoch [103/120    avg_loss:0.016, val_acc:0.981]
Epoch [104/120    avg_loss:0.016, val_acc:0.981]
Epoch [105/120    avg_loss:0.016, val_acc:0.982]
Epoch [106/120    avg_loss:0.015, val_acc:0.982]
Epoch [107/120    avg_loss:0.016, val_acc:0.982]
Epoch [108/120    avg_loss:0.014, val_acc:0.982]
Epoch [109/120    avg_loss:0.014, val_acc:0.982]
Epoch [110/120    avg_loss:0.014, val_acc:0.982]
Epoch [111/120    avg_loss:0.016, val_acc:0.982]
Epoch [112/120    avg_loss:0.017, val_acc:0.984]
Epoch [113/120    avg_loss:0.015, val_acc:0.984]
Epoch [114/120    avg_loss:0.014, val_acc:0.984]
Epoch [115/120    avg_loss:0.015, val_acc:0.984]
Epoch [116/120    avg_loss:0.012, val_acc:0.984]
Epoch [117/120    avg_loss:0.013, val_acc:0.984]
Epoch [118/120    avg_loss:0.015, val_acc:0.984]
Epoch [119/120    avg_loss:0.019, val_acc:0.984]
Epoch [120/120    avg_loss:0.014, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1252    4    0    0    2    0    0    0    6   20    0    0
     0    1    0]
 [   0    0    1  705    0   18    0    0    0   17    1    0    1    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    5    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   11    5    0   15    0    1    0    0  830    5    0    0
     0    8    0]
 [   0    0    6    0    0    0   11    0    0    0    8 2181    0    4
     0    0    0]
 [   0    0    0   21    5    7    0    0    0    0    2   10  486    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    1    0    0
  1132    2    0]
 [   0    0    0    0    0    0   25    0    0    2    0    0    0    0
    41  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.96476964769647

F1 scores:
[       nan 0.98765432 0.98003914 0.951417   0.98839907 0.94911504
 0.97113249 0.96153846 0.99883856 0.6        0.9617613  0.98509485
 0.95200784 0.97883598 0.97923875 0.87598116 0.98245614]

Kappa:
0.9653932735590449
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc9cf7286a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.434, val_acc:0.487]
Epoch [2/120    avg_loss:1.918, val_acc:0.627]
Epoch [3/120    avg_loss:1.582, val_acc:0.690]
Epoch [4/120    avg_loss:1.381, val_acc:0.694]
Epoch [5/120    avg_loss:1.230, val_acc:0.724]
Epoch [6/120    avg_loss:0.926, val_acc:0.721]
Epoch [7/120    avg_loss:0.831, val_acc:0.770]
Epoch [8/120    avg_loss:0.787, val_acc:0.782]
Epoch [9/120    avg_loss:0.693, val_acc:0.810]
Epoch [10/120    avg_loss:0.558, val_acc:0.808]
Epoch [11/120    avg_loss:0.503, val_acc:0.851]
Epoch [12/120    avg_loss:0.423, val_acc:0.848]
Epoch [13/120    avg_loss:0.328, val_acc:0.890]
Epoch [14/120    avg_loss:0.366, val_acc:0.809]
Epoch [15/120    avg_loss:0.448, val_acc:0.875]
Epoch [16/120    avg_loss:0.353, val_acc:0.874]
Epoch [17/120    avg_loss:0.292, val_acc:0.880]
Epoch [18/120    avg_loss:0.286, val_acc:0.896]
Epoch [19/120    avg_loss:0.267, val_acc:0.880]
Epoch [20/120    avg_loss:0.230, val_acc:0.911]
Epoch [21/120    avg_loss:0.186, val_acc:0.918]
Epoch [22/120    avg_loss:0.201, val_acc:0.906]
Epoch [23/120    avg_loss:0.218, val_acc:0.898]
Epoch [24/120    avg_loss:0.251, val_acc:0.925]
Epoch [25/120    avg_loss:0.169, val_acc:0.934]
Epoch [26/120    avg_loss:0.145, val_acc:0.935]
Epoch [27/120    avg_loss:0.118, val_acc:0.929]
Epoch [28/120    avg_loss:0.131, val_acc:0.932]
Epoch [29/120    avg_loss:0.122, val_acc:0.924]
Epoch [30/120    avg_loss:0.102, val_acc:0.930]
Epoch [31/120    avg_loss:0.092, val_acc:0.927]
Epoch [32/120    avg_loss:0.098, val_acc:0.944]
Epoch [33/120    avg_loss:0.090, val_acc:0.951]
Epoch [34/120    avg_loss:0.087, val_acc:0.930]
Epoch [35/120    avg_loss:0.116, val_acc:0.951]
Epoch [36/120    avg_loss:0.078, val_acc:0.961]
Epoch [37/120    avg_loss:0.075, val_acc:0.946]
Epoch [38/120    avg_loss:0.069, val_acc:0.967]
Epoch [39/120    avg_loss:0.055, val_acc:0.951]
Epoch [40/120    avg_loss:0.059, val_acc:0.964]
Epoch [41/120    avg_loss:0.057, val_acc:0.958]
Epoch [42/120    avg_loss:0.060, val_acc:0.951]
Epoch [43/120    avg_loss:0.065, val_acc:0.957]
Epoch [44/120    avg_loss:0.056, val_acc:0.968]
Epoch [45/120    avg_loss:0.050, val_acc:0.951]
Epoch [46/120    avg_loss:0.039, val_acc:0.969]
Epoch [47/120    avg_loss:0.034, val_acc:0.968]
Epoch [48/120    avg_loss:0.033, val_acc:0.969]
Epoch [49/120    avg_loss:0.032, val_acc:0.970]
Epoch [50/120    avg_loss:0.051, val_acc:0.962]
Epoch [51/120    avg_loss:0.030, val_acc:0.957]
Epoch [52/120    avg_loss:0.039, val_acc:0.963]
Epoch [53/120    avg_loss:0.033, val_acc:0.964]
Epoch [54/120    avg_loss:0.032, val_acc:0.976]
Epoch [55/120    avg_loss:0.031, val_acc:0.971]
Epoch [56/120    avg_loss:0.045, val_acc:0.966]
Epoch [57/120    avg_loss:0.033, val_acc:0.963]
Epoch [58/120    avg_loss:0.043, val_acc:0.965]
Epoch [59/120    avg_loss:0.034, val_acc:0.967]
Epoch [60/120    avg_loss:0.035, val_acc:0.956]
Epoch [61/120    avg_loss:0.034, val_acc:0.964]
Epoch [62/120    avg_loss:0.036, val_acc:0.971]
Epoch [63/120    avg_loss:0.039, val_acc:0.944]
Epoch [64/120    avg_loss:0.086, val_acc:0.958]
Epoch [65/120    avg_loss:0.040, val_acc:0.969]
Epoch [66/120    avg_loss:0.027, val_acc:0.961]
Epoch [67/120    avg_loss:0.024, val_acc:0.974]
Epoch [68/120    avg_loss:0.034, val_acc:0.973]
Epoch [69/120    avg_loss:0.020, val_acc:0.974]
Epoch [70/120    avg_loss:0.021, val_acc:0.977]
Epoch [71/120    avg_loss:0.015, val_acc:0.977]
Epoch [72/120    avg_loss:0.015, val_acc:0.976]
Epoch [73/120    avg_loss:0.020, val_acc:0.977]
Epoch [74/120    avg_loss:0.016, val_acc:0.981]
Epoch [75/120    avg_loss:0.014, val_acc:0.980]
Epoch [76/120    avg_loss:0.020, val_acc:0.981]
Epoch [77/120    avg_loss:0.016, val_acc:0.982]
Epoch [78/120    avg_loss:0.014, val_acc:0.981]
Epoch [79/120    avg_loss:0.020, val_acc:0.980]
Epoch [80/120    avg_loss:0.012, val_acc:0.982]
Epoch [81/120    avg_loss:0.012, val_acc:0.982]
Epoch [82/120    avg_loss:0.014, val_acc:0.980]
Epoch [83/120    avg_loss:0.015, val_acc:0.981]
Epoch [84/120    avg_loss:0.014, val_acc:0.980]
Epoch [85/120    avg_loss:0.015, val_acc:0.979]
Epoch [86/120    avg_loss:0.014, val_acc:0.981]
Epoch [87/120    avg_loss:0.014, val_acc:0.980]
Epoch [88/120    avg_loss:0.013, val_acc:0.979]
Epoch [89/120    avg_loss:0.013, val_acc:0.979]
Epoch [90/120    avg_loss:0.015, val_acc:0.979]
Epoch [91/120    avg_loss:0.016, val_acc:0.980]
Epoch [92/120    avg_loss:0.012, val_acc:0.979]
Epoch [93/120    avg_loss:0.012, val_acc:0.977]
Epoch [94/120    avg_loss:0.015, val_acc:0.978]
Epoch [95/120    avg_loss:0.013, val_acc:0.978]
Epoch [96/120    avg_loss:0.013, val_acc:0.979]
Epoch [97/120    avg_loss:0.015, val_acc:0.979]
Epoch [98/120    avg_loss:0.014, val_acc:0.979]
Epoch [99/120    avg_loss:0.013, val_acc:0.979]
Epoch [100/120    avg_loss:0.014, val_acc:0.979]
Epoch [101/120    avg_loss:0.015, val_acc:0.979]
Epoch [102/120    avg_loss:0.012, val_acc:0.979]
Epoch [103/120    avg_loss:0.015, val_acc:0.979]
Epoch [104/120    avg_loss:0.014, val_acc:0.979]
Epoch [105/120    avg_loss:0.013, val_acc:0.979]
Epoch [106/120    avg_loss:0.011, val_acc:0.979]
Epoch [107/120    avg_loss:0.012, val_acc:0.979]
Epoch [108/120    avg_loss:0.013, val_acc:0.979]
Epoch [109/120    avg_loss:0.013, val_acc:0.979]
Epoch [110/120    avg_loss:0.013, val_acc:0.979]
Epoch [111/120    avg_loss:0.012, val_acc:0.979]
Epoch [112/120    avg_loss:0.013, val_acc:0.979]
Epoch [113/120    avg_loss:0.012, val_acc:0.979]
Epoch [114/120    avg_loss:0.012, val_acc:0.979]
Epoch [115/120    avg_loss:0.011, val_acc:0.979]
Epoch [116/120    avg_loss:0.012, val_acc:0.979]
Epoch [117/120    avg_loss:0.017, val_acc:0.979]
Epoch [118/120    avg_loss:0.012, val_acc:0.979]
Epoch [119/120    avg_loss:0.014, val_acc:0.979]
Epoch [120/120    avg_loss:0.013, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1249    0    0    0    0    0    0    0   20   15    1    0
     0    0    0]
 [   0    0    1  718    0    7    0    0    0   10    1    0    7    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   14    2    0    7    0    0    0    0  846    2    0    0
     1    3    0]
 [   0    0    9    0    0    0   10    0    0    0   15 2174    0    2
     0    0    0]
 [   0    0    0   27    7   12    0    0    0    0    1    1  481    0
     1    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    1    0    0
  1132    3    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    46  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.26829268292683

F1 scores:
[       nan 0.96202532 0.97654418 0.96117805 0.98383372 0.96868009
 0.98498498 1.         1.         0.75       0.95918367 0.98728429
 0.93945312 0.98666667 0.97628288 0.90542636 0.97076023]

Kappa:
0.9688591030674205
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f526baa37b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.505, val_acc:0.538]
Epoch [2/120    avg_loss:1.919, val_acc:0.543]
Epoch [3/120    avg_loss:1.583, val_acc:0.644]
Epoch [4/120    avg_loss:1.429, val_acc:0.687]
Epoch [5/120    avg_loss:1.210, val_acc:0.699]
Epoch [6/120    avg_loss:1.001, val_acc:0.731]
Epoch [7/120    avg_loss:0.858, val_acc:0.740]
Epoch [8/120    avg_loss:0.739, val_acc:0.759]
Epoch [9/120    avg_loss:0.659, val_acc:0.811]
Epoch [10/120    avg_loss:0.593, val_acc:0.806]
Epoch [11/120    avg_loss:0.618, val_acc:0.828]
Epoch [12/120    avg_loss:0.413, val_acc:0.846]
Epoch [13/120    avg_loss:0.379, val_acc:0.854]
Epoch [14/120    avg_loss:0.421, val_acc:0.806]
Epoch [15/120    avg_loss:0.303, val_acc:0.825]
Epoch [16/120    avg_loss:0.339, val_acc:0.905]
Epoch [17/120    avg_loss:0.405, val_acc:0.847]
Epoch [18/120    avg_loss:0.308, val_acc:0.876]
Epoch [19/120    avg_loss:0.254, val_acc:0.899]
Epoch [20/120    avg_loss:0.183, val_acc:0.901]
Epoch [21/120    avg_loss:0.207, val_acc:0.894]
Epoch [22/120    avg_loss:0.201, val_acc:0.912]
Epoch [23/120    avg_loss:0.203, val_acc:0.927]
Epoch [24/120    avg_loss:0.233, val_acc:0.916]
Epoch [25/120    avg_loss:0.224, val_acc:0.905]
Epoch [26/120    avg_loss:0.156, val_acc:0.923]
Epoch [27/120    avg_loss:0.135, val_acc:0.929]
Epoch [28/120    avg_loss:0.112, val_acc:0.925]
Epoch [29/120    avg_loss:0.134, val_acc:0.934]
Epoch [30/120    avg_loss:0.122, val_acc:0.911]
Epoch [31/120    avg_loss:0.109, val_acc:0.939]
Epoch [32/120    avg_loss:0.091, val_acc:0.945]
Epoch [33/120    avg_loss:0.100, val_acc:0.954]
Epoch [34/120    avg_loss:0.103, val_acc:0.948]
Epoch [35/120    avg_loss:0.111, val_acc:0.930]
Epoch [36/120    avg_loss:0.070, val_acc:0.945]
Epoch [37/120    avg_loss:0.065, val_acc:0.938]
Epoch [38/120    avg_loss:0.071, val_acc:0.958]
Epoch [39/120    avg_loss:0.077, val_acc:0.958]
Epoch [40/120    avg_loss:0.070, val_acc:0.956]
Epoch [41/120    avg_loss:0.064, val_acc:0.959]
Epoch [42/120    avg_loss:0.055, val_acc:0.957]
Epoch [43/120    avg_loss:0.057, val_acc:0.941]
Epoch [44/120    avg_loss:0.046, val_acc:0.966]
Epoch [45/120    avg_loss:0.048, val_acc:0.956]
Epoch [46/120    avg_loss:0.063, val_acc:0.956]
Epoch [47/120    avg_loss:0.054, val_acc:0.952]
Epoch [48/120    avg_loss:0.062, val_acc:0.943]
Epoch [49/120    avg_loss:0.080, val_acc:0.950]
Epoch [50/120    avg_loss:0.050, val_acc:0.970]
Epoch [51/120    avg_loss:0.062, val_acc:0.937]
Epoch [52/120    avg_loss:0.066, val_acc:0.955]
Epoch [53/120    avg_loss:0.044, val_acc:0.961]
Epoch [54/120    avg_loss:0.034, val_acc:0.971]
Epoch [55/120    avg_loss:0.036, val_acc:0.963]
Epoch [56/120    avg_loss:0.032, val_acc:0.963]
Epoch [57/120    avg_loss:0.031, val_acc:0.963]
Epoch [58/120    avg_loss:0.040, val_acc:0.961]
Epoch [59/120    avg_loss:0.033, val_acc:0.961]
Epoch [60/120    avg_loss:0.040, val_acc:0.970]
Epoch [61/120    avg_loss:0.036, val_acc:0.966]
Epoch [62/120    avg_loss:0.047, val_acc:0.974]
Epoch [63/120    avg_loss:0.029, val_acc:0.972]
Epoch [64/120    avg_loss:0.026, val_acc:0.979]
Epoch [65/120    avg_loss:0.024, val_acc:0.975]
Epoch [66/120    avg_loss:0.029, val_acc:0.975]
Epoch [67/120    avg_loss:0.018, val_acc:0.973]
Epoch [68/120    avg_loss:0.033, val_acc:0.952]
Epoch [69/120    avg_loss:0.037, val_acc:0.975]
Epoch [70/120    avg_loss:0.043, val_acc:0.954]
Epoch [71/120    avg_loss:0.035, val_acc:0.974]
Epoch [72/120    avg_loss:0.085, val_acc:0.957]
Epoch [73/120    avg_loss:0.036, val_acc:0.962]
Epoch [74/120    avg_loss:0.031, val_acc:0.972]
Epoch [75/120    avg_loss:0.020, val_acc:0.973]
Epoch [76/120    avg_loss:0.019, val_acc:0.977]
Epoch [77/120    avg_loss:0.024, val_acc:0.972]
Epoch [78/120    avg_loss:0.016, val_acc:0.974]
Epoch [79/120    avg_loss:0.015, val_acc:0.977]
Epoch [80/120    avg_loss:0.013, val_acc:0.982]
Epoch [81/120    avg_loss:0.012, val_acc:0.982]
Epoch [82/120    avg_loss:0.015, val_acc:0.982]
Epoch [83/120    avg_loss:0.012, val_acc:0.982]
Epoch [84/120    avg_loss:0.013, val_acc:0.980]
Epoch [85/120    avg_loss:0.011, val_acc:0.981]
Epoch [86/120    avg_loss:0.010, val_acc:0.983]
Epoch [87/120    avg_loss:0.012, val_acc:0.983]
Epoch [88/120    avg_loss:0.013, val_acc:0.981]
Epoch [89/120    avg_loss:0.013, val_acc:0.984]
Epoch [90/120    avg_loss:0.011, val_acc:0.982]
Epoch [91/120    avg_loss:0.012, val_acc:0.984]
Epoch [92/120    avg_loss:0.012, val_acc:0.982]
Epoch [93/120    avg_loss:0.012, val_acc:0.980]
Epoch [94/120    avg_loss:0.013, val_acc:0.981]
Epoch [95/120    avg_loss:0.010, val_acc:0.982]
Epoch [96/120    avg_loss:0.016, val_acc:0.983]
Epoch [97/120    avg_loss:0.010, val_acc:0.983]
Epoch [98/120    avg_loss:0.013, val_acc:0.982]
Epoch [99/120    avg_loss:0.012, val_acc:0.982]
Epoch [100/120    avg_loss:0.013, val_acc:0.983]
Epoch [101/120    avg_loss:0.011, val_acc:0.984]
Epoch [102/120    avg_loss:0.011, val_acc:0.984]
Epoch [103/120    avg_loss:0.011, val_acc:0.984]
Epoch [104/120    avg_loss:0.012, val_acc:0.982]
Epoch [105/120    avg_loss:0.009, val_acc:0.983]
Epoch [106/120    avg_loss:0.013, val_acc:0.983]
Epoch [107/120    avg_loss:0.009, val_acc:0.982]
Epoch [108/120    avg_loss:0.012, val_acc:0.982]
Epoch [109/120    avg_loss:0.014, val_acc:0.981]
Epoch [110/120    avg_loss:0.011, val_acc:0.984]
Epoch [111/120    avg_loss:0.010, val_acc:0.981]
Epoch [112/120    avg_loss:0.012, val_acc:0.983]
Epoch [113/120    avg_loss:0.010, val_acc:0.982]
Epoch [114/120    avg_loss:0.009, val_acc:0.983]
Epoch [115/120    avg_loss:0.010, val_acc:0.982]
Epoch [116/120    avg_loss:0.012, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.982]
Epoch [118/120    avg_loss:0.010, val_acc:0.983]
Epoch [119/120    avg_loss:0.009, val_acc:0.983]
Epoch [120/120    avg_loss:0.011, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1270    0    0    0    1    0    0    0    2   12    0    0
     0    0    0]
 [   0    0    0  715    1   19    0    0    0    9    1    0    0    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0   33    0    8    0    0    0    0  804   22    0    0
     0    8    0]
 [   0    0    3    0    0    0    1    0    0    0    7 2197    0    2
     0    0    0]
 [   0    0    0   22    9   13    0    0    0    0    2    4  476    0
     2    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    1    0    3    1    0    0
  1126    0    0]
 [   0    0    0    0    0    0   22    0    0    0    0    0    0    0
    40  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.09485094850949

F1 scores:
[       nan 1.         0.99296325 0.94264997 0.97706422 0.94656489
 0.97977528 1.         0.99883856 0.7826087  0.94923259 0.98763767
 0.94257426 0.98930481 0.97615951 0.890625   0.96551724]

Kappa:
0.9668616161033431
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1e31ce7780>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.461, val_acc:0.402]
Epoch [2/120    avg_loss:1.912, val_acc:0.480]
Epoch [3/120    avg_loss:1.664, val_acc:0.617]
Epoch [4/120    avg_loss:1.462, val_acc:0.596]
Epoch [5/120    avg_loss:1.299, val_acc:0.714]
Epoch [6/120    avg_loss:0.915, val_acc:0.734]
Epoch [7/120    avg_loss:0.816, val_acc:0.795]
Epoch [8/120    avg_loss:0.766, val_acc:0.766]
Epoch [9/120    avg_loss:0.659, val_acc:0.756]
Epoch [10/120    avg_loss:0.593, val_acc:0.768]
Epoch [11/120    avg_loss:0.519, val_acc:0.839]
Epoch [12/120    avg_loss:0.421, val_acc:0.847]
Epoch [13/120    avg_loss:0.397, val_acc:0.831]
Epoch [14/120    avg_loss:0.401, val_acc:0.851]
Epoch [15/120    avg_loss:0.325, val_acc:0.877]
Epoch [16/120    avg_loss:0.281, val_acc:0.861]
Epoch [17/120    avg_loss:0.304, val_acc:0.898]
Epoch [18/120    avg_loss:0.243, val_acc:0.903]
Epoch [19/120    avg_loss:0.255, val_acc:0.885]
Epoch [20/120    avg_loss:0.306, val_acc:0.892]
Epoch [21/120    avg_loss:0.326, val_acc:0.872]
Epoch [22/120    avg_loss:0.267, val_acc:0.889]
Epoch [23/120    avg_loss:0.269, val_acc:0.884]
Epoch [24/120    avg_loss:0.264, val_acc:0.912]
Epoch [25/120    avg_loss:0.194, val_acc:0.913]
Epoch [26/120    avg_loss:0.163, val_acc:0.926]
Epoch [27/120    avg_loss:0.182, val_acc:0.923]
Epoch [28/120    avg_loss:0.135, val_acc:0.931]
Epoch [29/120    avg_loss:0.145, val_acc:0.928]
Epoch [30/120    avg_loss:0.144, val_acc:0.922]
Epoch [31/120    avg_loss:0.113, val_acc:0.922]
Epoch [32/120    avg_loss:0.139, val_acc:0.929]
Epoch [33/120    avg_loss:0.135, val_acc:0.905]
Epoch [34/120    avg_loss:0.125, val_acc:0.922]
Epoch [35/120    avg_loss:0.117, val_acc:0.927]
Epoch [36/120    avg_loss:0.111, val_acc:0.935]
Epoch [37/120    avg_loss:0.097, val_acc:0.932]
Epoch [38/120    avg_loss:0.092, val_acc:0.941]
Epoch [39/120    avg_loss:0.094, val_acc:0.929]
Epoch [40/120    avg_loss:0.109, val_acc:0.923]
Epoch [41/120    avg_loss:0.110, val_acc:0.935]
Epoch [42/120    avg_loss:0.075, val_acc:0.926]
Epoch [43/120    avg_loss:0.074, val_acc:0.930]
Epoch [44/120    avg_loss:0.070, val_acc:0.935]
Epoch [45/120    avg_loss:0.062, val_acc:0.927]
Epoch [46/120    avg_loss:0.102, val_acc:0.934]
Epoch [47/120    avg_loss:0.062, val_acc:0.955]
Epoch [48/120    avg_loss:0.071, val_acc:0.938]
Epoch [49/120    avg_loss:0.076, val_acc:0.938]
Epoch [50/120    avg_loss:0.057, val_acc:0.947]
Epoch [51/120    avg_loss:0.054, val_acc:0.946]
Epoch [52/120    avg_loss:0.051, val_acc:0.937]
Epoch [53/120    avg_loss:0.058, val_acc:0.950]
Epoch [54/120    avg_loss:0.061, val_acc:0.952]
Epoch [55/120    avg_loss:0.057, val_acc:0.935]
Epoch [56/120    avg_loss:0.090, val_acc:0.943]
Epoch [57/120    avg_loss:0.066, val_acc:0.956]
Epoch [58/120    avg_loss:0.066, val_acc:0.936]
Epoch [59/120    avg_loss:0.047, val_acc:0.948]
Epoch [60/120    avg_loss:0.038, val_acc:0.957]
Epoch [61/120    avg_loss:0.129, val_acc:0.931]
Epoch [62/120    avg_loss:0.063, val_acc:0.950]
Epoch [63/120    avg_loss:0.046, val_acc:0.958]
Epoch [64/120    avg_loss:0.041, val_acc:0.956]
Epoch [65/120    avg_loss:0.030, val_acc:0.961]
Epoch [66/120    avg_loss:0.028, val_acc:0.967]
Epoch [67/120    avg_loss:0.032, val_acc:0.961]
Epoch [68/120    avg_loss:0.027, val_acc:0.962]
Epoch [69/120    avg_loss:0.043, val_acc:0.958]
Epoch [70/120    avg_loss:0.044, val_acc:0.955]
Epoch [71/120    avg_loss:0.023, val_acc:0.961]
Epoch [72/120    avg_loss:0.030, val_acc:0.962]
Epoch [73/120    avg_loss:0.033, val_acc:0.965]
Epoch [74/120    avg_loss:0.028, val_acc:0.946]
Epoch [75/120    avg_loss:0.028, val_acc:0.963]
Epoch [76/120    avg_loss:0.034, val_acc:0.948]
Epoch [77/120    avg_loss:0.027, val_acc:0.954]
Epoch [78/120    avg_loss:0.025, val_acc:0.968]
Epoch [79/120    avg_loss:0.018, val_acc:0.966]
Epoch [80/120    avg_loss:0.017, val_acc:0.967]
Epoch [81/120    avg_loss:0.019, val_acc:0.956]
Epoch [82/120    avg_loss:0.017, val_acc:0.962]
Epoch [83/120    avg_loss:0.020, val_acc:0.964]
Epoch [84/120    avg_loss:0.017, val_acc:0.966]
Epoch [85/120    avg_loss:0.020, val_acc:0.965]
Epoch [86/120    avg_loss:0.021, val_acc:0.962]
Epoch [87/120    avg_loss:0.026, val_acc:0.961]
Epoch [88/120    avg_loss:0.023, val_acc:0.964]
Epoch [89/120    avg_loss:0.017, val_acc:0.962]
Epoch [90/120    avg_loss:0.020, val_acc:0.968]
Epoch [91/120    avg_loss:0.017, val_acc:0.963]
Epoch [92/120    avg_loss:0.020, val_acc:0.966]
Epoch [93/120    avg_loss:0.025, val_acc:0.964]
Epoch [94/120    avg_loss:0.016, val_acc:0.955]
Epoch [95/120    avg_loss:0.020, val_acc:0.964]
Epoch [96/120    avg_loss:0.019, val_acc:0.950]
Epoch [97/120    avg_loss:0.063, val_acc:0.954]
Epoch [98/120    avg_loss:0.028, val_acc:0.964]
Epoch [99/120    avg_loss:0.017, val_acc:0.965]
Epoch [100/120    avg_loss:0.016, val_acc:0.964]
Epoch [101/120    avg_loss:0.024, val_acc:0.965]
Epoch [102/120    avg_loss:0.015, val_acc:0.967]
Epoch [103/120    avg_loss:0.021, val_acc:0.965]
Epoch [104/120    avg_loss:0.013, val_acc:0.967]
Epoch [105/120    avg_loss:0.013, val_acc:0.970]
Epoch [106/120    avg_loss:0.009, val_acc:0.970]
Epoch [107/120    avg_loss:0.009, val_acc:0.971]
Epoch [108/120    avg_loss:0.008, val_acc:0.970]
Epoch [109/120    avg_loss:0.009, val_acc:0.971]
Epoch [110/120    avg_loss:0.012, val_acc:0.972]
Epoch [111/120    avg_loss:0.007, val_acc:0.971]
Epoch [112/120    avg_loss:0.008, val_acc:0.971]
Epoch [113/120    avg_loss:0.008, val_acc:0.972]
Epoch [114/120    avg_loss:0.009, val_acc:0.972]
Epoch [115/120    avg_loss:0.009, val_acc:0.972]
Epoch [116/120    avg_loss:0.008, val_acc:0.973]
Epoch [117/120    avg_loss:0.007, val_acc:0.972]
Epoch [118/120    avg_loss:0.006, val_acc:0.971]
Epoch [119/120    avg_loss:0.008, val_acc:0.971]
Epoch [120/120    avg_loss:0.009, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    0    0    0    1    0    0    0    4   12    1    0
     0    0    0]
 [   0    0    0  726    0   13    0    0    0    6    0    0    0    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    0    0    3    0    0    0    0
     9    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0   10    0    0    0    0    0    0  420    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   14    0    0    1    0
     0    0    0]
 [   0    0    3   87    0    1    0    0    0    0  766    4    0    0
     0   14    0]
 [   0    0    4    0    0    0   10    0    0    0    8 2186    0    2
     0    0    0]
 [   0    0    1    9   10    5    0    0    0    0   11    0  493    0
     0    0    5]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    1    0    0
  1136    0    0]
 [   0    0    0    0    0    0    3    0    0    2    0    0    0    0
   118  224    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.0

F1 scores:
[       nan 0.87912088 0.98945724 0.92366412 0.97706422 0.96355353
 0.98793363 1.         0.98707403 0.65116279 0.92012012 0.99026048
 0.95821186 0.98659517 0.94587843 0.76581197 0.97109827]

Kappa:
0.9543627674026903
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff6a5e46748>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.381, val_acc:0.489]
Epoch [2/120    avg_loss:1.911, val_acc:0.598]
Epoch [3/120    avg_loss:1.615, val_acc:0.601]
Epoch [4/120    avg_loss:1.381, val_acc:0.671]
Epoch [5/120    avg_loss:1.110, val_acc:0.667]
Epoch [6/120    avg_loss:0.952, val_acc:0.716]
Epoch [7/120    avg_loss:0.870, val_acc:0.751]
Epoch [8/120    avg_loss:0.758, val_acc:0.771]
Epoch [9/120    avg_loss:0.681, val_acc:0.797]
Epoch [10/120    avg_loss:0.709, val_acc:0.785]
Epoch [11/120    avg_loss:0.685, val_acc:0.842]
Epoch [12/120    avg_loss:0.486, val_acc:0.825]
Epoch [13/120    avg_loss:0.460, val_acc:0.834]
Epoch [14/120    avg_loss:0.404, val_acc:0.837]
Epoch [15/120    avg_loss:0.380, val_acc:0.859]
Epoch [16/120    avg_loss:0.340, val_acc:0.850]
Epoch [17/120    avg_loss:0.298, val_acc:0.881]
Epoch [18/120    avg_loss:0.345, val_acc:0.832]
Epoch [19/120    avg_loss:0.278, val_acc:0.869]
Epoch [20/120    avg_loss:0.276, val_acc:0.892]
Epoch [21/120    avg_loss:0.264, val_acc:0.918]
Epoch [22/120    avg_loss:0.219, val_acc:0.875]
Epoch [23/120    avg_loss:0.216, val_acc:0.917]
Epoch [24/120    avg_loss:0.194, val_acc:0.943]
Epoch [25/120    avg_loss:0.178, val_acc:0.912]
Epoch [26/120    avg_loss:0.184, val_acc:0.938]
Epoch [27/120    avg_loss:0.135, val_acc:0.931]
Epoch [28/120    avg_loss:0.200, val_acc:0.930]
Epoch [29/120    avg_loss:0.193, val_acc:0.932]
Epoch [30/120    avg_loss:0.133, val_acc:0.929]
Epoch [31/120    avg_loss:0.131, val_acc:0.943]
Epoch [32/120    avg_loss:0.127, val_acc:0.938]
Epoch [33/120    avg_loss:0.120, val_acc:0.937]
Epoch [34/120    avg_loss:0.151, val_acc:0.919]
Epoch [35/120    avg_loss:0.139, val_acc:0.943]
Epoch [36/120    avg_loss:0.122, val_acc:0.934]
Epoch [37/120    avg_loss:0.097, val_acc:0.943]
Epoch [38/120    avg_loss:0.089, val_acc:0.955]
Epoch [39/120    avg_loss:0.091, val_acc:0.950]
Epoch [40/120    avg_loss:0.094, val_acc:0.958]
Epoch [41/120    avg_loss:0.088, val_acc:0.965]
Epoch [42/120    avg_loss:0.066, val_acc:0.952]
Epoch [43/120    avg_loss:0.076, val_acc:0.961]
Epoch [44/120    avg_loss:0.052, val_acc:0.961]
Epoch [45/120    avg_loss:0.052, val_acc:0.954]
Epoch [46/120    avg_loss:0.056, val_acc:0.961]
Epoch [47/120    avg_loss:0.064, val_acc:0.955]
Epoch [48/120    avg_loss:0.088, val_acc:0.944]
Epoch [49/120    avg_loss:0.107, val_acc:0.938]
Epoch [50/120    avg_loss:0.122, val_acc:0.957]
Epoch [51/120    avg_loss:0.056, val_acc:0.963]
Epoch [52/120    avg_loss:0.054, val_acc:0.950]
Epoch [53/120    avg_loss:0.082, val_acc:0.947]
Epoch [54/120    avg_loss:0.096, val_acc:0.963]
Epoch [55/120    avg_loss:0.073, val_acc:0.971]
Epoch [56/120    avg_loss:0.049, val_acc:0.973]
Epoch [57/120    avg_loss:0.041, val_acc:0.977]
Epoch [58/120    avg_loss:0.042, val_acc:0.975]
Epoch [59/120    avg_loss:0.040, val_acc:0.974]
Epoch [60/120    avg_loss:0.038, val_acc:0.975]
Epoch [61/120    avg_loss:0.034, val_acc:0.975]
Epoch [62/120    avg_loss:0.031, val_acc:0.974]
Epoch [63/120    avg_loss:0.037, val_acc:0.975]
Epoch [64/120    avg_loss:0.031, val_acc:0.972]
Epoch [65/120    avg_loss:0.032, val_acc:0.974]
Epoch [66/120    avg_loss:0.032, val_acc:0.976]
Epoch [67/120    avg_loss:0.029, val_acc:0.974]
Epoch [68/120    avg_loss:0.028, val_acc:0.974]
Epoch [69/120    avg_loss:0.031, val_acc:0.973]
Epoch [70/120    avg_loss:0.027, val_acc:0.973]
Epoch [71/120    avg_loss:0.029, val_acc:0.973]
Epoch [72/120    avg_loss:0.027, val_acc:0.973]
Epoch [73/120    avg_loss:0.036, val_acc:0.972]
Epoch [74/120    avg_loss:0.033, val_acc:0.972]
Epoch [75/120    avg_loss:0.026, val_acc:0.972]
Epoch [76/120    avg_loss:0.032, val_acc:0.972]
Epoch [77/120    avg_loss:0.027, val_acc:0.972]
Epoch [78/120    avg_loss:0.029, val_acc:0.972]
Epoch [79/120    avg_loss:0.027, val_acc:0.972]
Epoch [80/120    avg_loss:0.034, val_acc:0.972]
Epoch [81/120    avg_loss:0.030, val_acc:0.972]
Epoch [82/120    avg_loss:0.027, val_acc:0.972]
Epoch [83/120    avg_loss:0.031, val_acc:0.971]
Epoch [84/120    avg_loss:0.029, val_acc:0.971]
Epoch [85/120    avg_loss:0.031, val_acc:0.971]
Epoch [86/120    avg_loss:0.028, val_acc:0.971]
Epoch [87/120    avg_loss:0.025, val_acc:0.971]
Epoch [88/120    avg_loss:0.028, val_acc:0.971]
Epoch [89/120    avg_loss:0.032, val_acc:0.971]
Epoch [90/120    avg_loss:0.025, val_acc:0.971]
Epoch [91/120    avg_loss:0.027, val_acc:0.971]
Epoch [92/120    avg_loss:0.026, val_acc:0.971]
Epoch [93/120    avg_loss:0.031, val_acc:0.971]
Epoch [94/120    avg_loss:0.028, val_acc:0.971]
Epoch [95/120    avg_loss:0.030, val_acc:0.971]
Epoch [96/120    avg_loss:0.026, val_acc:0.971]
Epoch [97/120    avg_loss:0.028, val_acc:0.971]
Epoch [98/120    avg_loss:0.023, val_acc:0.971]
Epoch [99/120    avg_loss:0.030, val_acc:0.971]
Epoch [100/120    avg_loss:0.028, val_acc:0.971]
Epoch [101/120    avg_loss:0.027, val_acc:0.971]
Epoch [102/120    avg_loss:0.028, val_acc:0.971]
Epoch [103/120    avg_loss:0.030, val_acc:0.971]
Epoch [104/120    avg_loss:0.027, val_acc:0.971]
Epoch [105/120    avg_loss:0.026, val_acc:0.971]
Epoch [106/120    avg_loss:0.030, val_acc:0.971]
Epoch [107/120    avg_loss:0.028, val_acc:0.971]
Epoch [108/120    avg_loss:0.028, val_acc:0.971]
Epoch [109/120    avg_loss:0.030, val_acc:0.971]
Epoch [110/120    avg_loss:0.035, val_acc:0.971]
Epoch [111/120    avg_loss:0.034, val_acc:0.971]
Epoch [112/120    avg_loss:0.030, val_acc:0.971]
Epoch [113/120    avg_loss:0.026, val_acc:0.971]
Epoch [114/120    avg_loss:0.029, val_acc:0.971]
Epoch [115/120    avg_loss:0.028, val_acc:0.971]
Epoch [116/120    avg_loss:0.031, val_acc:0.971]
Epoch [117/120    avg_loss:0.027, val_acc:0.971]
Epoch [118/120    avg_loss:0.030, val_acc:0.971]
Epoch [119/120    avg_loss:0.029, val_acc:0.971]
Epoch [120/120    avg_loss:0.026, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1244    6    0    0    2    0    0    2    9   13    2    0
     0    7    0]
 [   0    0    2  729    0    5    0    0    0    6    0    0    3    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    1    0    5    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   30   88    0    7    0    0    0    0  741    1    0    0
     1    7    0]
 [   0    0   19    0    0    1    7    0    1    0    3 2171    0    3
     5    0    0]
 [   0    0    0   26    4   11    0    0    0    0    6   10  472    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    1    0    0
  1135    0    0]
 [   0    0    0    0    0    0    9    0    0    5    0    0    0    0
   129  204    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.02439024390245

F1 scores:
[       nan 0.98765432 0.96396745 0.91239049 0.99069767 0.9603624
 0.98419865 0.98039216 0.99883856 0.61538462 0.9053146  0.98502722
 0.93280632 0.98666667 0.93995859 0.72212389 0.96511628]

Kappa:
0.9432232309657281
creating ./logs/logs-2022-01-19IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-19:23:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f04f731b710>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.429, val_acc:0.491]
Epoch [2/120    avg_loss:1.887, val_acc:0.539]
Epoch [3/120    avg_loss:1.661, val_acc:0.614]
Epoch [4/120    avg_loss:1.433, val_acc:0.618]
Epoch [5/120    avg_loss:1.260, val_acc:0.682]
Epoch [6/120    avg_loss:1.010, val_acc:0.708]
Epoch [7/120    avg_loss:0.958, val_acc:0.723]
Epoch [8/120    avg_loss:0.762, val_acc:0.769]
Epoch [9/120    avg_loss:0.609, val_acc:0.792]
Epoch [10/120    avg_loss:0.638, val_acc:0.747]
Epoch [11/120    avg_loss:0.570, val_acc:0.814]
Epoch [12/120    avg_loss:0.490, val_acc:0.807]
Epoch [13/120    avg_loss:0.556, val_acc:0.795]
Epoch [14/120    avg_loss:0.441, val_acc:0.845]
Epoch [15/120    avg_loss:0.510, val_acc:0.830]
Epoch [16/120    avg_loss:0.413, val_acc:0.850]
Epoch [17/120    avg_loss:0.396, val_acc:0.873]
Epoch [18/120    avg_loss:0.294, val_acc:0.858]
Epoch [19/120    avg_loss:0.310, val_acc:0.860]
Epoch [20/120    avg_loss:0.260, val_acc:0.878]
Epoch [21/120    avg_loss:0.272, val_acc:0.874]
Epoch [22/120    avg_loss:0.270, val_acc:0.870]
Epoch [23/120    avg_loss:0.213, val_acc:0.889]
Epoch [24/120    avg_loss:0.181, val_acc:0.901]
Epoch [25/120    avg_loss:0.184, val_acc:0.902]
Epoch [26/120    avg_loss:0.210, val_acc:0.891]
Epoch [27/120    avg_loss:0.207, val_acc:0.883]
Epoch [28/120    avg_loss:0.166, val_acc:0.907]
Epoch [29/120    avg_loss:0.166, val_acc:0.892]
Epoch [30/120    avg_loss:0.148, val_acc:0.921]
Epoch [31/120    avg_loss:0.190, val_acc:0.911]
Epoch [32/120    avg_loss:0.183, val_acc:0.902]
Epoch [33/120    avg_loss:0.232, val_acc:0.898]
Epoch [34/120    avg_loss:0.174, val_acc:0.905]
Epoch [35/120    avg_loss:0.160, val_acc:0.927]
Epoch [36/120    avg_loss:0.124, val_acc:0.927]
Epoch [37/120    avg_loss:0.149, val_acc:0.902]
Epoch [38/120    avg_loss:0.119, val_acc:0.909]
Epoch [39/120    avg_loss:0.095, val_acc:0.929]
Epoch [40/120    avg_loss:0.097, val_acc:0.913]
Epoch [41/120    avg_loss:0.099, val_acc:0.925]
Epoch [42/120    avg_loss:0.096, val_acc:0.931]
Epoch [43/120    avg_loss:0.086, val_acc:0.934]
Epoch [44/120    avg_loss:0.099, val_acc:0.944]
Epoch [45/120    avg_loss:0.083, val_acc:0.930]
Epoch [46/120    avg_loss:0.082, val_acc:0.936]
Epoch [47/120    avg_loss:0.073, val_acc:0.943]
Epoch [48/120    avg_loss:0.099, val_acc:0.901]
Epoch [49/120    avg_loss:0.080, val_acc:0.944]
Epoch [50/120    avg_loss:0.098, val_acc:0.926]
Epoch [51/120    avg_loss:0.071, val_acc:0.947]
Epoch [52/120    avg_loss:0.062, val_acc:0.953]
Epoch [53/120    avg_loss:0.067, val_acc:0.948]
Epoch [54/120    avg_loss:0.051, val_acc:0.949]
Epoch [55/120    avg_loss:0.053, val_acc:0.944]
Epoch [56/120    avg_loss:0.052, val_acc:0.949]
Epoch [57/120    avg_loss:0.048, val_acc:0.962]
Epoch [58/120    avg_loss:0.049, val_acc:0.940]
Epoch [59/120    avg_loss:0.048, val_acc:0.950]
Epoch [60/120    avg_loss:0.037, val_acc:0.966]
Epoch [61/120    avg_loss:0.046, val_acc:0.946]
Epoch [62/120    avg_loss:0.053, val_acc:0.955]
Epoch [63/120    avg_loss:0.041, val_acc:0.954]
Epoch [64/120    avg_loss:0.040, val_acc:0.953]
Epoch [65/120    avg_loss:0.030, val_acc:0.956]
Epoch [66/120    avg_loss:0.044, val_acc:0.956]
Epoch [67/120    avg_loss:0.061, val_acc:0.927]
Epoch [68/120    avg_loss:0.129, val_acc:0.848]
Epoch [69/120    avg_loss:0.192, val_acc:0.922]
Epoch [70/120    avg_loss:0.073, val_acc:0.937]
Epoch [71/120    avg_loss:0.061, val_acc:0.937]
Epoch [72/120    avg_loss:0.082, val_acc:0.949]
Epoch [73/120    avg_loss:0.059, val_acc:0.949]
Epoch [74/120    avg_loss:0.038, val_acc:0.957]
Epoch [75/120    avg_loss:0.033, val_acc:0.962]
Epoch [76/120    avg_loss:0.031, val_acc:0.959]
Epoch [77/120    avg_loss:0.025, val_acc:0.962]
Epoch [78/120    avg_loss:0.033, val_acc:0.964]
Epoch [79/120    avg_loss:0.027, val_acc:0.963]
Epoch [80/120    avg_loss:0.027, val_acc:0.964]
Epoch [81/120    avg_loss:0.030, val_acc:0.964]
Epoch [82/120    avg_loss:0.030, val_acc:0.961]
Epoch [83/120    avg_loss:0.024, val_acc:0.964]
Epoch [84/120    avg_loss:0.024, val_acc:0.966]
Epoch [85/120    avg_loss:0.031, val_acc:0.962]
Epoch [86/120    avg_loss:0.028, val_acc:0.963]
Epoch [87/120    avg_loss:0.024, val_acc:0.963]
Epoch [88/120    avg_loss:0.025, val_acc:0.964]
Epoch [89/120    avg_loss:0.027, val_acc:0.961]
Epoch [90/120    avg_loss:0.022, val_acc:0.966]
Epoch [91/120    avg_loss:0.018, val_acc:0.967]
Epoch [92/120    avg_loss:0.022, val_acc:0.965]
Epoch [93/120    avg_loss:0.028, val_acc:0.965]
Epoch [94/120    avg_loss:0.024, val_acc:0.964]
Epoch [95/120    avg_loss:0.022, val_acc:0.964]
Epoch [96/120    avg_loss:0.022, val_acc:0.964]
Epoch [97/120    avg_loss:0.021, val_acc:0.964]
Epoch [98/120    avg_loss:0.020, val_acc:0.966]
Epoch [99/120    avg_loss:0.023, val_acc:0.965]
Epoch [100/120    avg_loss:0.021, val_acc:0.966]
Epoch [101/120    avg_loss:0.024, val_acc:0.965]
Epoch [102/120    avg_loss:0.021, val_acc:0.965]
Epoch [103/120    avg_loss:0.020, val_acc:0.964]
Epoch [104/120    avg_loss:0.018, val_acc:0.965]
Epoch [105/120    avg_loss:0.021, val_acc:0.965]
Epoch [106/120    avg_loss:0.021, val_acc:0.964]
Epoch [107/120    avg_loss:0.019, val_acc:0.964]
Epoch [108/120    avg_loss:0.021, val_acc:0.965]
Epoch [109/120    avg_loss:0.021, val_acc:0.965]
Epoch [110/120    avg_loss:0.021, val_acc:0.965]
Epoch [111/120    avg_loss:0.020, val_acc:0.965]
Epoch [112/120    avg_loss:0.023, val_acc:0.965]
Epoch [113/120    avg_loss:0.022, val_acc:0.965]
Epoch [114/120    avg_loss:0.022, val_acc:0.966]
Epoch [115/120    avg_loss:0.021, val_acc:0.966]
Epoch [116/120    avg_loss:0.018, val_acc:0.966]
Epoch [117/120    avg_loss:0.021, val_acc:0.966]
Epoch [118/120    avg_loss:0.024, val_acc:0.966]
Epoch [119/120    avg_loss:0.021, val_acc:0.966]
Epoch [120/120    avg_loss:0.019, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1258    1    0    0    0    0    0    0    8   12    4    0
     0    2    0]
 [   0    0    4  695    0   22    0    0    0   14    0    0   10    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    0    0    3    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   11   33    0    8    0    0    0    0  809    0    0    0
     7    7    0]
 [   0    0   20    0    0    1   16    0    0    0   16 2146    8    3
     0    0    0]
 [   0    0    0    4    3    8    0    0    0    0   10    3  498    0
     0    0    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    1    1    0    0
  1134    1    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    79  254    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.09756097560975

F1 scores:
[       nan 0.96202532 0.97595035 0.93918919 0.99300699 0.94339623
 0.97691735 1.         1.         0.62745098 0.93960511 0.98147725
 0.9422895  0.98666667 0.95857988 0.8314239  0.94857143]

Kappa:
0.9555254267068138
