creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f347d9997f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.437, val_acc:0.515]
Epoch [2/120    avg_loss:1.897, val_acc:0.610]
Epoch [3/120    avg_loss:1.613, val_acc:0.643]
Epoch [4/120    avg_loss:1.394, val_acc:0.689]
Epoch [5/120    avg_loss:1.130, val_acc:0.703]
Epoch [6/120    avg_loss:0.916, val_acc:0.718]
Epoch [7/120    avg_loss:0.810, val_acc:0.767]
Epoch [8/120    avg_loss:0.663, val_acc:0.779]
Epoch [9/120    avg_loss:0.637, val_acc:0.793]
Epoch [10/120    avg_loss:0.661, val_acc:0.833]
Epoch [11/120    avg_loss:0.565, val_acc:0.788]
Epoch [12/120    avg_loss:0.492, val_acc:0.850]
Epoch [13/120    avg_loss:0.429, val_acc:0.859]
Epoch [14/120    avg_loss:0.352, val_acc:0.866]
Epoch [15/120    avg_loss:0.359, val_acc:0.861]
Epoch [16/120    avg_loss:0.325, val_acc:0.885]
Epoch [17/120    avg_loss:0.292, val_acc:0.881]
Epoch [18/120    avg_loss:0.292, val_acc:0.873]
Epoch [19/120    avg_loss:0.299, val_acc:0.904]
Epoch [20/120    avg_loss:0.235, val_acc:0.914]
Epoch [21/120    avg_loss:0.281, val_acc:0.908]
Epoch [22/120    avg_loss:0.237, val_acc:0.923]
Epoch [23/120    avg_loss:0.225, val_acc:0.920]
Epoch [24/120    avg_loss:0.153, val_acc:0.944]
Epoch [25/120    avg_loss:0.214, val_acc:0.910]
Epoch [26/120    avg_loss:0.212, val_acc:0.926]
Epoch [27/120    avg_loss:0.204, val_acc:0.903]
Epoch [28/120    avg_loss:0.296, val_acc:0.882]
Epoch [29/120    avg_loss:0.248, val_acc:0.912]
Epoch [30/120    avg_loss:0.201, val_acc:0.929]
Epoch [31/120    avg_loss:0.148, val_acc:0.945]
Epoch [32/120    avg_loss:0.135, val_acc:0.946]
Epoch [33/120    avg_loss:0.148, val_acc:0.941]
Epoch [34/120    avg_loss:0.139, val_acc:0.939]
Epoch [35/120    avg_loss:0.127, val_acc:0.935]
Epoch [36/120    avg_loss:0.113, val_acc:0.955]
Epoch [37/120    avg_loss:0.132, val_acc:0.952]
Epoch [38/120    avg_loss:0.096, val_acc:0.946]
Epoch [39/120    avg_loss:0.121, val_acc:0.945]
Epoch [40/120    avg_loss:0.116, val_acc:0.952]
Epoch [41/120    avg_loss:0.076, val_acc:0.959]
Epoch [42/120    avg_loss:0.061, val_acc:0.952]
Epoch [43/120    avg_loss:0.064, val_acc:0.956]
Epoch [44/120    avg_loss:0.085, val_acc:0.964]
Epoch [45/120    avg_loss:0.071, val_acc:0.970]
Epoch [46/120    avg_loss:0.057, val_acc:0.958]
Epoch [47/120    avg_loss:0.053, val_acc:0.958]
Epoch [48/120    avg_loss:0.057, val_acc:0.963]
Epoch [49/120    avg_loss:0.054, val_acc:0.949]
Epoch [50/120    avg_loss:0.077, val_acc:0.955]
Epoch [51/120    avg_loss:0.105, val_acc:0.940]
Epoch [52/120    avg_loss:0.057, val_acc:0.964]
Epoch [53/120    avg_loss:0.055, val_acc:0.961]
Epoch [54/120    avg_loss:0.040, val_acc:0.963]
Epoch [55/120    avg_loss:0.037, val_acc:0.961]
Epoch [56/120    avg_loss:0.041, val_acc:0.970]
Epoch [57/120    avg_loss:0.046, val_acc:0.956]
Epoch [58/120    avg_loss:0.048, val_acc:0.963]
Epoch [59/120    avg_loss:0.037, val_acc:0.968]
Epoch [60/120    avg_loss:0.042, val_acc:0.963]
Epoch [61/120    avg_loss:0.033, val_acc:0.972]
Epoch [62/120    avg_loss:0.031, val_acc:0.968]
Epoch [63/120    avg_loss:0.038, val_acc:0.968]
Epoch [64/120    avg_loss:0.027, val_acc:0.971]
Epoch [65/120    avg_loss:0.026, val_acc:0.972]
Epoch [66/120    avg_loss:0.021, val_acc:0.972]
Epoch [67/120    avg_loss:0.045, val_acc:0.947]
Epoch [68/120    avg_loss:0.079, val_acc:0.967]
Epoch [69/120    avg_loss:0.038, val_acc:0.965]
Epoch [70/120    avg_loss:0.030, val_acc:0.965]
Epoch [71/120    avg_loss:0.038, val_acc:0.970]
Epoch [72/120    avg_loss:0.044, val_acc:0.962]
Epoch [73/120    avg_loss:0.028, val_acc:0.970]
Epoch [74/120    avg_loss:0.020, val_acc:0.973]
Epoch [75/120    avg_loss:0.024, val_acc:0.970]
Epoch [76/120    avg_loss:0.035, val_acc:0.967]
Epoch [77/120    avg_loss:0.056, val_acc:0.965]
Epoch [78/120    avg_loss:0.057, val_acc:0.965]
Epoch [79/120    avg_loss:0.034, val_acc:0.955]
Epoch [80/120    avg_loss:0.033, val_acc:0.967]
Epoch [81/120    avg_loss:0.021, val_acc:0.972]
Epoch [82/120    avg_loss:0.018, val_acc:0.971]
Epoch [83/120    avg_loss:0.012, val_acc:0.976]
Epoch [84/120    avg_loss:0.011, val_acc:0.973]
Epoch [85/120    avg_loss:0.014, val_acc:0.980]
Epoch [86/120    avg_loss:0.021, val_acc:0.974]
Epoch [87/120    avg_loss:0.022, val_acc:0.972]
Epoch [88/120    avg_loss:0.015, val_acc:0.981]
Epoch [89/120    avg_loss:0.013, val_acc:0.979]
Epoch [90/120    avg_loss:0.015, val_acc:0.977]
Epoch [91/120    avg_loss:0.012, val_acc:0.979]
Epoch [92/120    avg_loss:0.012, val_acc:0.979]
Epoch [93/120    avg_loss:0.015, val_acc:0.975]
Epoch [94/120    avg_loss:0.015, val_acc:0.976]
Epoch [95/120    avg_loss:0.015, val_acc:0.976]
Epoch [96/120    avg_loss:0.012, val_acc:0.981]
Epoch [97/120    avg_loss:0.010, val_acc:0.975]
Epoch [98/120    avg_loss:0.015, val_acc:0.976]
Epoch [99/120    avg_loss:0.013, val_acc:0.976]
Epoch [100/120    avg_loss:0.010, val_acc:0.982]
Epoch [101/120    avg_loss:0.010, val_acc:0.979]
Epoch [102/120    avg_loss:0.010, val_acc:0.984]
Epoch [103/120    avg_loss:0.011, val_acc:0.976]
Epoch [104/120    avg_loss:0.010, val_acc:0.980]
Epoch [105/120    avg_loss:0.008, val_acc:0.976]
Epoch [106/120    avg_loss:0.013, val_acc:0.975]
Epoch [107/120    avg_loss:0.020, val_acc:0.972]
Epoch [108/120    avg_loss:0.025, val_acc:0.972]
Epoch [109/120    avg_loss:0.019, val_acc:0.979]
Epoch [110/120    avg_loss:0.010, val_acc:0.982]
Epoch [111/120    avg_loss:0.010, val_acc:0.980]
Epoch [112/120    avg_loss:0.009, val_acc:0.982]
Epoch [113/120    avg_loss:0.011, val_acc:0.982]
Epoch [114/120    avg_loss:0.010, val_acc:0.981]
Epoch [115/120    avg_loss:0.013, val_acc:0.980]
Epoch [116/120    avg_loss:0.010, val_acc:0.977]
Epoch [117/120    avg_loss:0.008, val_acc:0.979]
Epoch [118/120    avg_loss:0.006, val_acc:0.980]
Epoch [119/120    avg_loss:0.008, val_acc:0.981]
Epoch [120/120    avg_loss:0.006, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1264    1    0    0    0    0    0    0    6   12    2    0
     0    0    0]
 [   0    0    0  729    1    0    0    0    0   11    0    1    4    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    8   88    0    3    2    0    0    0  767    1    0    0
     0    6    0]
 [   0    0   10    0    0    0    3    0    0    0    8 2185    2    2
     0    0    0]
 [   0    0    0    9    7    3    0    0    0    0   13   21  479    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    1    1    0    0
  1130    0    0]
 [   0    0    0    0    0    0   21    0    0    7    0    0    0    0
   112  207    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.77235772357723

F1 scores:
[       nan 0.96385542 0.98442368 0.92630241 0.98156682 0.97828571
 0.9798357  0.98039216 0.99650757 0.57627119 0.91856287 0.98601083
 0.93554688 0.9919571  0.94918102 0.73928571 0.97619048]

Kappa:
0.9517480697618501
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2ae7d076d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.442, val_acc:0.325]
Epoch [2/120    avg_loss:1.940, val_acc:0.503]
Epoch [3/120    avg_loss:1.673, val_acc:0.613]
Epoch [4/120    avg_loss:1.404, val_acc:0.652]
Epoch [5/120    avg_loss:1.160, val_acc:0.711]
Epoch [6/120    avg_loss:1.049, val_acc:0.718]
Epoch [7/120    avg_loss:0.866, val_acc:0.778]
Epoch [8/120    avg_loss:0.746, val_acc:0.764]
Epoch [9/120    avg_loss:0.612, val_acc:0.787]
Epoch [10/120    avg_loss:0.603, val_acc:0.809]
Epoch [11/120    avg_loss:0.493, val_acc:0.867]
Epoch [12/120    avg_loss:0.466, val_acc:0.841]
Epoch [13/120    avg_loss:0.440, val_acc:0.855]
Epoch [14/120    avg_loss:0.464, val_acc:0.864]
Epoch [15/120    avg_loss:0.351, val_acc:0.890]
Epoch [16/120    avg_loss:0.325, val_acc:0.872]
Epoch [17/120    avg_loss:0.271, val_acc:0.899]
Epoch [18/120    avg_loss:0.333, val_acc:0.898]
Epoch [19/120    avg_loss:0.276, val_acc:0.892]
Epoch [20/120    avg_loss:0.323, val_acc:0.878]
Epoch [21/120    avg_loss:0.277, val_acc:0.919]
Epoch [22/120    avg_loss:0.215, val_acc:0.907]
Epoch [23/120    avg_loss:0.198, val_acc:0.905]
Epoch [24/120    avg_loss:0.226, val_acc:0.910]
Epoch [25/120    avg_loss:0.254, val_acc:0.922]
Epoch [26/120    avg_loss:0.204, val_acc:0.929]
Epoch [27/120    avg_loss:0.208, val_acc:0.925]
Epoch [28/120    avg_loss:0.173, val_acc:0.938]
Epoch [29/120    avg_loss:0.146, val_acc:0.928]
Epoch [30/120    avg_loss:0.135, val_acc:0.941]
Epoch [31/120    avg_loss:0.142, val_acc:0.937]
Epoch [32/120    avg_loss:0.120, val_acc:0.952]
Epoch [33/120    avg_loss:0.123, val_acc:0.937]
Epoch [34/120    avg_loss:0.127, val_acc:0.939]
Epoch [35/120    avg_loss:0.089, val_acc:0.963]
Epoch [36/120    avg_loss:0.084, val_acc:0.948]
Epoch [37/120    avg_loss:0.111, val_acc:0.946]
Epoch [38/120    avg_loss:0.074, val_acc:0.956]
Epoch [39/120    avg_loss:0.068, val_acc:0.959]
Epoch [40/120    avg_loss:0.073, val_acc:0.964]
Epoch [41/120    avg_loss:0.099, val_acc:0.940]
Epoch [42/120    avg_loss:0.135, val_acc:0.957]
Epoch [43/120    avg_loss:0.087, val_acc:0.953]
Epoch [44/120    avg_loss:0.061, val_acc:0.948]
Epoch [45/120    avg_loss:0.075, val_acc:0.950]
Epoch [46/120    avg_loss:0.080, val_acc:0.944]
Epoch [47/120    avg_loss:0.109, val_acc:0.941]
Epoch [48/120    avg_loss:0.100, val_acc:0.957]
Epoch [49/120    avg_loss:0.061, val_acc:0.972]
Epoch [50/120    avg_loss:0.053, val_acc:0.968]
Epoch [51/120    avg_loss:0.072, val_acc:0.959]
Epoch [52/120    avg_loss:0.085, val_acc:0.959]
Epoch [53/120    avg_loss:0.060, val_acc:0.970]
Epoch [54/120    avg_loss:0.071, val_acc:0.961]
Epoch [55/120    avg_loss:0.059, val_acc:0.959]
Epoch [56/120    avg_loss:0.055, val_acc:0.965]
Epoch [57/120    avg_loss:0.044, val_acc:0.957]
Epoch [58/120    avg_loss:0.040, val_acc:0.966]
Epoch [59/120    avg_loss:0.051, val_acc:0.965]
Epoch [60/120    avg_loss:0.046, val_acc:0.979]
Epoch [61/120    avg_loss:0.175, val_acc:0.892]
Epoch [62/120    avg_loss:0.324, val_acc:0.882]
Epoch [63/120    avg_loss:0.262, val_acc:0.944]
Epoch [64/120    avg_loss:0.229, val_acc:0.909]
Epoch [65/120    avg_loss:0.143, val_acc:0.952]
Epoch [66/120    avg_loss:0.084, val_acc:0.950]
Epoch [67/120    avg_loss:0.099, val_acc:0.956]
Epoch [68/120    avg_loss:0.070, val_acc:0.957]
Epoch [69/120    avg_loss:0.056, val_acc:0.966]
Epoch [70/120    avg_loss:0.067, val_acc:0.949]
Epoch [71/120    avg_loss:0.057, val_acc:0.964]
Epoch [72/120    avg_loss:0.042, val_acc:0.961]
Epoch [73/120    avg_loss:0.040, val_acc:0.966]
Epoch [74/120    avg_loss:0.037, val_acc:0.977]
Epoch [75/120    avg_loss:0.030, val_acc:0.976]
Epoch [76/120    avg_loss:0.032, val_acc:0.977]
Epoch [77/120    avg_loss:0.026, val_acc:0.976]
Epoch [78/120    avg_loss:0.025, val_acc:0.979]
Epoch [79/120    avg_loss:0.027, val_acc:0.979]
Epoch [80/120    avg_loss:0.024, val_acc:0.979]
Epoch [81/120    avg_loss:0.023, val_acc:0.979]
Epoch [82/120    avg_loss:0.027, val_acc:0.974]
Epoch [83/120    avg_loss:0.022, val_acc:0.976]
Epoch [84/120    avg_loss:0.025, val_acc:0.976]
Epoch [85/120    avg_loss:0.024, val_acc:0.977]
Epoch [86/120    avg_loss:0.023, val_acc:0.975]
Epoch [87/120    avg_loss:0.022, val_acc:0.976]
Epoch [88/120    avg_loss:0.024, val_acc:0.976]
Epoch [89/120    avg_loss:0.020, val_acc:0.977]
Epoch [90/120    avg_loss:0.024, val_acc:0.975]
Epoch [91/120    avg_loss:0.026, val_acc:0.975]
Epoch [92/120    avg_loss:0.023, val_acc:0.976]
Epoch [93/120    avg_loss:0.021, val_acc:0.976]
Epoch [94/120    avg_loss:0.021, val_acc:0.977]
Epoch [95/120    avg_loss:0.022, val_acc:0.977]
Epoch [96/120    avg_loss:0.017, val_acc:0.976]
Epoch [97/120    avg_loss:0.021, val_acc:0.977]
Epoch [98/120    avg_loss:0.025, val_acc:0.976]
Epoch [99/120    avg_loss:0.020, val_acc:0.976]
Epoch [100/120    avg_loss:0.022, val_acc:0.976]
Epoch [101/120    avg_loss:0.022, val_acc:0.976]
Epoch [102/120    avg_loss:0.020, val_acc:0.977]
Epoch [103/120    avg_loss:0.021, val_acc:0.975]
Epoch [104/120    avg_loss:0.021, val_acc:0.975]
Epoch [105/120    avg_loss:0.020, val_acc:0.975]
Epoch [106/120    avg_loss:0.023, val_acc:0.975]
Epoch [107/120    avg_loss:0.025, val_acc:0.975]
Epoch [108/120    avg_loss:0.025, val_acc:0.975]
Epoch [109/120    avg_loss:0.018, val_acc:0.975]
Epoch [110/120    avg_loss:0.027, val_acc:0.975]
Epoch [111/120    avg_loss:0.025, val_acc:0.975]
Epoch [112/120    avg_loss:0.023, val_acc:0.975]
Epoch [113/120    avg_loss:0.024, val_acc:0.975]
Epoch [114/120    avg_loss:0.023, val_acc:0.975]
Epoch [115/120    avg_loss:0.019, val_acc:0.975]
Epoch [116/120    avg_loss:0.020, val_acc:0.975]
Epoch [117/120    avg_loss:0.021, val_acc:0.975]
Epoch [118/120    avg_loss:0.020, val_acc:0.975]
Epoch [119/120    avg_loss:0.020, val_acc:0.975]
Epoch [120/120    avg_loss:0.020, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    2 1231    1    0    0    4    0    0    0   18   15   11    0
     0    3    0]
 [   0    0    3  726    0    4    0    0    0   10    0    0    1    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    4    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   22   85    0    5    0    0    0    2  746    4    0    0
     0   11    0]
 [   0    0    5    0    0    3   11    0    0    0    1 2179    7    3
     1    0    0]
 [   0    0    1   34    7    6    0    0    0    0    9    0  473    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    2    2    0    0
  1133    0    0]
 [   0    0    0    0    0    1   35    0    0    0    0    0    0    0
    98  213    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.08943089430895

F1 scores:
[       nan 0.92857143 0.9666274  0.91091593 0.98383372 0.96935301
 0.96182085 1.         0.99534884 0.64       0.90260133 0.98776065
 0.92023346 0.98404255 0.95410526 0.74216028 0.97076023]

Kappa:
0.9439928581906286
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6644321748>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.443, val_acc:0.456]
Epoch [2/120    avg_loss:1.896, val_acc:0.568]
Epoch [3/120    avg_loss:1.666, val_acc:0.559]
Epoch [4/120    avg_loss:1.457, val_acc:0.655]
Epoch [5/120    avg_loss:1.186, val_acc:0.686]
Epoch [6/120    avg_loss:0.982, val_acc:0.731]
Epoch [7/120    avg_loss:0.871, val_acc:0.744]
Epoch [8/120    avg_loss:0.728, val_acc:0.741]
Epoch [9/120    avg_loss:0.650, val_acc:0.770]
Epoch [10/120    avg_loss:0.653, val_acc:0.762]
Epoch [11/120    avg_loss:0.529, val_acc:0.792]
Epoch [12/120    avg_loss:0.495, val_acc:0.788]
Epoch [13/120    avg_loss:0.435, val_acc:0.856]
Epoch [14/120    avg_loss:0.456, val_acc:0.818]
Epoch [15/120    avg_loss:0.458, val_acc:0.743]
Epoch [16/120    avg_loss:0.363, val_acc:0.880]
Epoch [17/120    avg_loss:0.278, val_acc:0.900]
Epoch [18/120    avg_loss:0.291, val_acc:0.854]
Epoch [19/120    avg_loss:0.306, val_acc:0.861]
Epoch [20/120    avg_loss:0.279, val_acc:0.863]
Epoch [21/120    avg_loss:0.350, val_acc:0.846]
Epoch [22/120    avg_loss:0.272, val_acc:0.861]
Epoch [23/120    avg_loss:0.245, val_acc:0.872]
Epoch [24/120    avg_loss:0.219, val_acc:0.903]
Epoch [25/120    avg_loss:0.198, val_acc:0.887]
Epoch [26/120    avg_loss:0.178, val_acc:0.891]
Epoch [27/120    avg_loss:0.152, val_acc:0.914]
Epoch [28/120    avg_loss:0.161, val_acc:0.918]
Epoch [29/120    avg_loss:0.136, val_acc:0.931]
Epoch [30/120    avg_loss:0.111, val_acc:0.931]
Epoch [31/120    avg_loss:0.134, val_acc:0.916]
Epoch [32/120    avg_loss:0.134, val_acc:0.896]
Epoch [33/120    avg_loss:0.160, val_acc:0.918]
Epoch [34/120    avg_loss:0.120, val_acc:0.919]
Epoch [35/120    avg_loss:0.093, val_acc:0.943]
Epoch [36/120    avg_loss:0.088, val_acc:0.930]
Epoch [37/120    avg_loss:0.108, val_acc:0.927]
Epoch [38/120    avg_loss:0.097, val_acc:0.930]
Epoch [39/120    avg_loss:0.090, val_acc:0.936]
Epoch [40/120    avg_loss:0.101, val_acc:0.938]
Epoch [41/120    avg_loss:0.071, val_acc:0.937]
Epoch [42/120    avg_loss:0.062, val_acc:0.922]
Epoch [43/120    avg_loss:0.088, val_acc:0.940]
Epoch [44/120    avg_loss:0.087, val_acc:0.937]
Epoch [45/120    avg_loss:0.080, val_acc:0.939]
Epoch [46/120    avg_loss:0.081, val_acc:0.941]
Epoch [47/120    avg_loss:0.109, val_acc:0.931]
Epoch [48/120    avg_loss:0.142, val_acc:0.883]
Epoch [49/120    avg_loss:0.162, val_acc:0.913]
Epoch [50/120    avg_loss:0.134, val_acc:0.927]
Epoch [51/120    avg_loss:0.083, val_acc:0.937]
Epoch [52/120    avg_loss:0.070, val_acc:0.940]
Epoch [53/120    avg_loss:0.072, val_acc:0.945]
Epoch [54/120    avg_loss:0.063, val_acc:0.947]
Epoch [55/120    avg_loss:0.059, val_acc:0.948]
Epoch [56/120    avg_loss:0.056, val_acc:0.955]
Epoch [57/120    avg_loss:0.050, val_acc:0.956]
Epoch [58/120    avg_loss:0.043, val_acc:0.956]
Epoch [59/120    avg_loss:0.047, val_acc:0.953]
Epoch [60/120    avg_loss:0.050, val_acc:0.958]
Epoch [61/120    avg_loss:0.040, val_acc:0.958]
Epoch [62/120    avg_loss:0.045, val_acc:0.958]
Epoch [63/120    avg_loss:0.045, val_acc:0.958]
Epoch [64/120    avg_loss:0.041, val_acc:0.959]
Epoch [65/120    avg_loss:0.034, val_acc:0.956]
Epoch [66/120    avg_loss:0.045, val_acc:0.957]
Epoch [67/120    avg_loss:0.040, val_acc:0.961]
Epoch [68/120    avg_loss:0.037, val_acc:0.959]
Epoch [69/120    avg_loss:0.039, val_acc:0.958]
Epoch [70/120    avg_loss:0.035, val_acc:0.958]
Epoch [71/120    avg_loss:0.036, val_acc:0.962]
Epoch [72/120    avg_loss:0.036, val_acc:0.958]
Epoch [73/120    avg_loss:0.038, val_acc:0.963]
Epoch [74/120    avg_loss:0.039, val_acc:0.962]
Epoch [75/120    avg_loss:0.038, val_acc:0.963]
Epoch [76/120    avg_loss:0.036, val_acc:0.957]
Epoch [77/120    avg_loss:0.041, val_acc:0.962]
Epoch [78/120    avg_loss:0.030, val_acc:0.957]
Epoch [79/120    avg_loss:0.034, val_acc:0.959]
Epoch [80/120    avg_loss:0.034, val_acc:0.963]
Epoch [81/120    avg_loss:0.037, val_acc:0.963]
Epoch [82/120    avg_loss:0.036, val_acc:0.961]
Epoch [83/120    avg_loss:0.032, val_acc:0.965]
Epoch [84/120    avg_loss:0.036, val_acc:0.962]
Epoch [85/120    avg_loss:0.033, val_acc:0.959]
Epoch [86/120    avg_loss:0.032, val_acc:0.959]
Epoch [87/120    avg_loss:0.029, val_acc:0.958]
Epoch [88/120    avg_loss:0.031, val_acc:0.962]
Epoch [89/120    avg_loss:0.035, val_acc:0.959]
Epoch [90/120    avg_loss:0.033, val_acc:0.964]
Epoch [91/120    avg_loss:0.036, val_acc:0.962]
Epoch [92/120    avg_loss:0.033, val_acc:0.959]
Epoch [93/120    avg_loss:0.032, val_acc:0.967]
Epoch [94/120    avg_loss:0.032, val_acc:0.964]
Epoch [95/120    avg_loss:0.028, val_acc:0.965]
Epoch [96/120    avg_loss:0.027, val_acc:0.964]
Epoch [97/120    avg_loss:0.029, val_acc:0.966]
Epoch [98/120    avg_loss:0.028, val_acc:0.965]
Epoch [99/120    avg_loss:0.030, val_acc:0.964]
Epoch [100/120    avg_loss:0.029, val_acc:0.966]
Epoch [101/120    avg_loss:0.026, val_acc:0.965]
Epoch [102/120    avg_loss:0.029, val_acc:0.966]
Epoch [103/120    avg_loss:0.030, val_acc:0.965]
Epoch [104/120    avg_loss:0.035, val_acc:0.963]
Epoch [105/120    avg_loss:0.034, val_acc:0.964]
Epoch [106/120    avg_loss:0.030, val_acc:0.965]
Epoch [107/120    avg_loss:0.031, val_acc:0.965]
Epoch [108/120    avg_loss:0.030, val_acc:0.964]
Epoch [109/120    avg_loss:0.027, val_acc:0.964]
Epoch [110/120    avg_loss:0.025, val_acc:0.964]
Epoch [111/120    avg_loss:0.031, val_acc:0.964]
Epoch [112/120    avg_loss:0.029, val_acc:0.965]
Epoch [113/120    avg_loss:0.026, val_acc:0.965]
Epoch [114/120    avg_loss:0.027, val_acc:0.965]
Epoch [115/120    avg_loss:0.027, val_acc:0.964]
Epoch [116/120    avg_loss:0.022, val_acc:0.965]
Epoch [117/120    avg_loss:0.030, val_acc:0.966]
Epoch [118/120    avg_loss:0.025, val_acc:0.966]
Epoch [119/120    avg_loss:0.026, val_acc:0.966]
Epoch [120/120    avg_loss:0.027, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    1 1261    0    0    0    1    0    0    0    7   11    3    0
     0    1    0]
 [   0    0    2  712    1    6    0    0    0   17    0    0    7    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  420    0    5    0    4    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   15    0    0    1    0
     0    0    0]
 [   0    0   18   77    0    3    0    0    0    0  756   13    0    0
     0    8    0]
 [   0    0    9    0    0    1   13    0    0    0    2 2179    0    5
     1    0    0]
 [   0    0    0   27    4    3    0    0    0    0   13    4  479    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
   128  215    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.43631436314364

F1 scores:
[       nan 0.97560976 0.97903727 0.90990415 0.98839907 0.96774194
 0.98496241 0.90909091 0.99883856 0.55555556 0.91359517 0.98619597
 0.93463415 0.98143236 0.94273859 0.7530648  0.97076023]

Kappa:
0.9479187312683909
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3e8c18d6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.444, val_acc:0.481]
Epoch [2/120    avg_loss:1.855, val_acc:0.583]
Epoch [3/120    avg_loss:1.702, val_acc:0.545]
Epoch [4/120    avg_loss:1.373, val_acc:0.619]
Epoch [5/120    avg_loss:1.093, val_acc:0.676]
Epoch [6/120    avg_loss:0.995, val_acc:0.686]
Epoch [7/120    avg_loss:0.811, val_acc:0.727]
Epoch [8/120    avg_loss:0.759, val_acc:0.795]
Epoch [9/120    avg_loss:0.645, val_acc:0.783]
Epoch [10/120    avg_loss:0.531, val_acc:0.830]
Epoch [11/120    avg_loss:0.493, val_acc:0.819]
Epoch [12/120    avg_loss:0.515, val_acc:0.860]
Epoch [13/120    avg_loss:0.437, val_acc:0.848]
Epoch [14/120    avg_loss:0.416, val_acc:0.858]
Epoch [15/120    avg_loss:0.357, val_acc:0.847]
Epoch [16/120    avg_loss:0.335, val_acc:0.887]
Epoch [17/120    avg_loss:0.285, val_acc:0.887]
Epoch [18/120    avg_loss:0.256, val_acc:0.904]
Epoch [19/120    avg_loss:0.234, val_acc:0.912]
Epoch [20/120    avg_loss:0.236, val_acc:0.896]
Epoch [21/120    avg_loss:0.251, val_acc:0.910]
Epoch [22/120    avg_loss:0.212, val_acc:0.922]
Epoch [23/120    avg_loss:0.224, val_acc:0.896]
Epoch [24/120    avg_loss:0.196, val_acc:0.934]
Epoch [25/120    avg_loss:0.176, val_acc:0.896]
Epoch [26/120    avg_loss:0.192, val_acc:0.913]
Epoch [27/120    avg_loss:0.174, val_acc:0.910]
Epoch [28/120    avg_loss:0.169, val_acc:0.928]
Epoch [29/120    avg_loss:0.166, val_acc:0.925]
Epoch [30/120    avg_loss:0.173, val_acc:0.932]
Epoch [31/120    avg_loss:0.123, val_acc:0.941]
Epoch [32/120    avg_loss:0.132, val_acc:0.940]
Epoch [33/120    avg_loss:0.156, val_acc:0.928]
Epoch [34/120    avg_loss:0.126, val_acc:0.932]
Epoch [35/120    avg_loss:0.140, val_acc:0.939]
Epoch [36/120    avg_loss:0.142, val_acc:0.922]
Epoch [37/120    avg_loss:0.106, val_acc:0.940]
Epoch [38/120    avg_loss:0.135, val_acc:0.927]
Epoch [39/120    avg_loss:0.117, val_acc:0.952]
Epoch [40/120    avg_loss:0.092, val_acc:0.947]
Epoch [41/120    avg_loss:0.070, val_acc:0.958]
Epoch [42/120    avg_loss:0.098, val_acc:0.940]
Epoch [43/120    avg_loss:0.121, val_acc:0.959]
Epoch [44/120    avg_loss:0.086, val_acc:0.940]
Epoch [45/120    avg_loss:0.074, val_acc:0.949]
Epoch [46/120    avg_loss:0.056, val_acc:0.954]
Epoch [47/120    avg_loss:0.058, val_acc:0.961]
Epoch [48/120    avg_loss:0.066, val_acc:0.949]
Epoch [49/120    avg_loss:0.051, val_acc:0.970]
Epoch [50/120    avg_loss:0.051, val_acc:0.953]
Epoch [51/120    avg_loss:0.066, val_acc:0.955]
Epoch [52/120    avg_loss:0.044, val_acc:0.962]
Epoch [53/120    avg_loss:0.071, val_acc:0.958]
Epoch [54/120    avg_loss:0.062, val_acc:0.972]
Epoch [55/120    avg_loss:0.055, val_acc:0.965]
Epoch [56/120    avg_loss:0.049, val_acc:0.965]
Epoch [57/120    avg_loss:0.045, val_acc:0.953]
Epoch [58/120    avg_loss:0.100, val_acc:0.946]
Epoch [59/120    avg_loss:0.090, val_acc:0.950]
Epoch [60/120    avg_loss:0.046, val_acc:0.963]
Epoch [61/120    avg_loss:0.043, val_acc:0.964]
Epoch [62/120    avg_loss:0.051, val_acc:0.973]
Epoch [63/120    avg_loss:0.048, val_acc:0.961]
Epoch [64/120    avg_loss:0.043, val_acc:0.970]
Epoch [65/120    avg_loss:0.047, val_acc:0.957]
Epoch [66/120    avg_loss:0.037, val_acc:0.975]
Epoch [67/120    avg_loss:0.038, val_acc:0.977]
Epoch [68/120    avg_loss:0.041, val_acc:0.971]
Epoch [69/120    avg_loss:0.051, val_acc:0.963]
Epoch [70/120    avg_loss:0.042, val_acc:0.968]
Epoch [71/120    avg_loss:0.032, val_acc:0.953]
Epoch [72/120    avg_loss:0.035, val_acc:0.977]
Epoch [73/120    avg_loss:0.023, val_acc:0.980]
Epoch [74/120    avg_loss:0.037, val_acc:0.967]
Epoch [75/120    avg_loss:0.032, val_acc:0.971]
Epoch [76/120    avg_loss:0.022, val_acc:0.973]
Epoch [77/120    avg_loss:0.032, val_acc:0.968]
Epoch [78/120    avg_loss:0.033, val_acc:0.965]
Epoch [79/120    avg_loss:0.029, val_acc:0.965]
Epoch [80/120    avg_loss:0.032, val_acc:0.970]
Epoch [81/120    avg_loss:0.036, val_acc:0.975]
Epoch [82/120    avg_loss:0.033, val_acc:0.966]
Epoch [83/120    avg_loss:0.023, val_acc:0.977]
Epoch [84/120    avg_loss:0.019, val_acc:0.977]
Epoch [85/120    avg_loss:0.020, val_acc:0.977]
Epoch [86/120    avg_loss:0.019, val_acc:0.977]
Epoch [87/120    avg_loss:0.012, val_acc:0.977]
Epoch [88/120    avg_loss:0.013, val_acc:0.979]
Epoch [89/120    avg_loss:0.013, val_acc:0.979]
Epoch [90/120    avg_loss:0.011, val_acc:0.980]
Epoch [91/120    avg_loss:0.011, val_acc:0.980]
Epoch [92/120    avg_loss:0.011, val_acc:0.980]
Epoch [93/120    avg_loss:0.015, val_acc:0.980]
Epoch [94/120    avg_loss:0.010, val_acc:0.980]
Epoch [95/120    avg_loss:0.012, val_acc:0.981]
Epoch [96/120    avg_loss:0.011, val_acc:0.980]
Epoch [97/120    avg_loss:0.012, val_acc:0.982]
Epoch [98/120    avg_loss:0.013, val_acc:0.981]
Epoch [99/120    avg_loss:0.011, val_acc:0.981]
Epoch [100/120    avg_loss:0.010, val_acc:0.981]
Epoch [101/120    avg_loss:0.013, val_acc:0.981]
Epoch [102/120    avg_loss:0.010, val_acc:0.982]
Epoch [103/120    avg_loss:0.013, val_acc:0.982]
Epoch [104/120    avg_loss:0.012, val_acc:0.982]
Epoch [105/120    avg_loss:0.011, val_acc:0.982]
Epoch [106/120    avg_loss:0.010, val_acc:0.982]
Epoch [107/120    avg_loss:0.010, val_acc:0.982]
Epoch [108/120    avg_loss:0.012, val_acc:0.982]
Epoch [109/120    avg_loss:0.012, val_acc:0.980]
Epoch [110/120    avg_loss:0.009, val_acc:0.981]
Epoch [111/120    avg_loss:0.021, val_acc:0.980]
Epoch [112/120    avg_loss:0.012, val_acc:0.977]
Epoch [113/120    avg_loss:0.012, val_acc:0.979]
Epoch [114/120    avg_loss:0.012, val_acc:0.979]
Epoch [115/120    avg_loss:0.011, val_acc:0.977]
Epoch [116/120    avg_loss:0.008, val_acc:0.976]
Epoch [117/120    avg_loss:0.011, val_acc:0.977]
Epoch [118/120    avg_loss:0.010, val_acc:0.977]
Epoch [119/120    avg_loss:0.012, val_acc:0.979]
Epoch [120/120    avg_loss:0.011, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    1 1253    0    0    0    3    0    0    2    6   14    1    0
     0    5    0]
 [   0    0    0  718    0    7    0    0    0   16    0    0    3    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    4    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   29   76    0    5    2    0    0    0  756    1    0    0
     0    6    0]
 [   0    0    9    0    0    0   11    0    0    0   12 2176    0    2
     0    0    0]
 [   0    0    0    0    0   10    0    0    0    0    4    0  516    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    2    0    0
  1134    0    0]
 [   0    0    0    0    0    0   16    0    0    1    0    0    0    0
    79  251    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.26016260162602

F1 scores:
[       nan 0.96296296 0.97282609 0.93186243 1.         0.96606335
 0.97546468 1.         0.99883856 0.5862069  0.91249246 0.98819255
 0.97819905 0.98666667 0.96264856 0.82430213 0.97674419]

Kappa:
0.9573584637291338
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff8b82d4748>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.400, val_acc:0.394]
Epoch [2/120    avg_loss:1.875, val_acc:0.581]
Epoch [3/120    avg_loss:1.591, val_acc:0.633]
Epoch [4/120    avg_loss:1.414, val_acc:0.608]
Epoch [5/120    avg_loss:1.135, val_acc:0.703]
Epoch [6/120    avg_loss:1.012, val_acc:0.739]
Epoch [7/120    avg_loss:0.875, val_acc:0.715]
Epoch [8/120    avg_loss:0.784, val_acc:0.770]
Epoch [9/120    avg_loss:0.698, val_acc:0.735]
Epoch [10/120    avg_loss:0.608, val_acc:0.777]
Epoch [11/120    avg_loss:0.573, val_acc:0.813]
Epoch [12/120    avg_loss:0.542, val_acc:0.787]
Epoch [13/120    avg_loss:0.441, val_acc:0.848]
Epoch [14/120    avg_loss:0.381, val_acc:0.860]
Epoch [15/120    avg_loss:0.398, val_acc:0.829]
Epoch [16/120    avg_loss:0.388, val_acc:0.873]
Epoch [17/120    avg_loss:0.288, val_acc:0.885]
Epoch [18/120    avg_loss:0.237, val_acc:0.892]
Epoch [19/120    avg_loss:0.286, val_acc:0.902]
Epoch [20/120    avg_loss:0.313, val_acc:0.846]
Epoch [21/120    avg_loss:0.239, val_acc:0.904]
Epoch [22/120    avg_loss:0.267, val_acc:0.904]
Epoch [23/120    avg_loss:0.249, val_acc:0.884]
Epoch [24/120    avg_loss:0.251, val_acc:0.905]
Epoch [25/120    avg_loss:0.196, val_acc:0.926]
Epoch [26/120    avg_loss:0.187, val_acc:0.913]
Epoch [27/120    avg_loss:0.179, val_acc:0.936]
Epoch [28/120    avg_loss:0.171, val_acc:0.867]
Epoch [29/120    avg_loss:0.225, val_acc:0.921]
Epoch [30/120    avg_loss:0.225, val_acc:0.904]
Epoch [31/120    avg_loss:0.180, val_acc:0.905]
Epoch [32/120    avg_loss:0.147, val_acc:0.921]
Epoch [33/120    avg_loss:0.248, val_acc:0.882]
Epoch [34/120    avg_loss:0.211, val_acc:0.882]
Epoch [35/120    avg_loss:0.241, val_acc:0.895]
Epoch [36/120    avg_loss:0.178, val_acc:0.932]
Epoch [37/120    avg_loss:0.124, val_acc:0.938]
Epoch [38/120    avg_loss:0.101, val_acc:0.932]
Epoch [39/120    avg_loss:0.134, val_acc:0.940]
Epoch [40/120    avg_loss:0.114, val_acc:0.930]
Epoch [41/120    avg_loss:0.119, val_acc:0.956]
Epoch [42/120    avg_loss:0.089, val_acc:0.949]
Epoch [43/120    avg_loss:0.086, val_acc:0.936]
Epoch [44/120    avg_loss:0.079, val_acc:0.949]
Epoch [45/120    avg_loss:0.085, val_acc:0.959]
Epoch [46/120    avg_loss:0.085, val_acc:0.956]
Epoch [47/120    avg_loss:0.123, val_acc:0.931]
Epoch [48/120    avg_loss:0.091, val_acc:0.955]
Epoch [49/120    avg_loss:0.117, val_acc:0.948]
Epoch [50/120    avg_loss:0.089, val_acc:0.948]
Epoch [51/120    avg_loss:0.063, val_acc:0.964]
Epoch [52/120    avg_loss:0.073, val_acc:0.948]
Epoch [53/120    avg_loss:0.053, val_acc:0.974]
Epoch [54/120    avg_loss:0.071, val_acc:0.949]
Epoch [55/120    avg_loss:0.075, val_acc:0.965]
Epoch [56/120    avg_loss:0.068, val_acc:0.941]
Epoch [57/120    avg_loss:0.054, val_acc:0.958]
Epoch [58/120    avg_loss:0.043, val_acc:0.963]
Epoch [59/120    avg_loss:0.049, val_acc:0.964]
Epoch [60/120    avg_loss:0.037, val_acc:0.967]
Epoch [61/120    avg_loss:0.044, val_acc:0.973]
Epoch [62/120    avg_loss:0.039, val_acc:0.973]
Epoch [63/120    avg_loss:0.061, val_acc:0.954]
Epoch [64/120    avg_loss:0.042, val_acc:0.964]
Epoch [65/120    avg_loss:0.042, val_acc:0.963]
Epoch [66/120    avg_loss:0.143, val_acc:0.877]
Epoch [67/120    avg_loss:0.161, val_acc:0.922]
Epoch [68/120    avg_loss:0.105, val_acc:0.934]
Epoch [69/120    avg_loss:0.075, val_acc:0.946]
Epoch [70/120    avg_loss:0.061, val_acc:0.952]
Epoch [71/120    avg_loss:0.053, val_acc:0.962]
Epoch [72/120    avg_loss:0.049, val_acc:0.964]
Epoch [73/120    avg_loss:0.052, val_acc:0.965]
Epoch [74/120    avg_loss:0.045, val_acc:0.970]
Epoch [75/120    avg_loss:0.036, val_acc:0.972]
Epoch [76/120    avg_loss:0.045, val_acc:0.973]
Epoch [77/120    avg_loss:0.039, val_acc:0.974]
Epoch [78/120    avg_loss:0.040, val_acc:0.974]
Epoch [79/120    avg_loss:0.035, val_acc:0.974]
Epoch [80/120    avg_loss:0.035, val_acc:0.975]
Epoch [81/120    avg_loss:0.041, val_acc:0.972]
Epoch [82/120    avg_loss:0.030, val_acc:0.975]
Epoch [83/120    avg_loss:0.029, val_acc:0.975]
Epoch [84/120    avg_loss:0.029, val_acc:0.977]
Epoch [85/120    avg_loss:0.030, val_acc:0.977]
Epoch [86/120    avg_loss:0.028, val_acc:0.975]
Epoch [87/120    avg_loss:0.032, val_acc:0.975]
Epoch [88/120    avg_loss:0.036, val_acc:0.975]
Epoch [89/120    avg_loss:0.028, val_acc:0.975]
Epoch [90/120    avg_loss:0.029, val_acc:0.971]
Epoch [91/120    avg_loss:0.026, val_acc:0.975]
Epoch [92/120    avg_loss:0.029, val_acc:0.977]
Epoch [93/120    avg_loss:0.028, val_acc:0.977]
Epoch [94/120    avg_loss:0.029, val_acc:0.976]
Epoch [95/120    avg_loss:0.028, val_acc:0.977]
Epoch [96/120    avg_loss:0.026, val_acc:0.979]
Epoch [97/120    avg_loss:0.026, val_acc:0.977]
Epoch [98/120    avg_loss:0.022, val_acc:0.979]
Epoch [99/120    avg_loss:0.024, val_acc:0.976]
Epoch [100/120    avg_loss:0.022, val_acc:0.975]
Epoch [101/120    avg_loss:0.023, val_acc:0.976]
Epoch [102/120    avg_loss:0.024, val_acc:0.976]
Epoch [103/120    avg_loss:0.024, val_acc:0.979]
Epoch [104/120    avg_loss:0.024, val_acc:0.980]
Epoch [105/120    avg_loss:0.025, val_acc:0.977]
Epoch [106/120    avg_loss:0.026, val_acc:0.975]
Epoch [107/120    avg_loss:0.025, val_acc:0.976]
Epoch [108/120    avg_loss:0.021, val_acc:0.977]
Epoch [109/120    avg_loss:0.024, val_acc:0.980]
Epoch [110/120    avg_loss:0.020, val_acc:0.980]
Epoch [111/120    avg_loss:0.023, val_acc:0.980]
Epoch [112/120    avg_loss:0.022, val_acc:0.981]
Epoch [113/120    avg_loss:0.023, val_acc:0.977]
Epoch [114/120    avg_loss:0.026, val_acc:0.979]
Epoch [115/120    avg_loss:0.024, val_acc:0.981]
Epoch [116/120    avg_loss:0.021, val_acc:0.981]
Epoch [117/120    avg_loss:0.021, val_acc:0.979]
Epoch [118/120    avg_loss:0.026, val_acc:0.981]
Epoch [119/120    avg_loss:0.020, val_acc:0.981]
Epoch [120/120    avg_loss:0.028, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1262    4    0    0    4    0    0    0    4    7    2    0
     0    2    0]
 [   0    0    1  718    1   17    0    0    0    5    0    0    1    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   26   78    0    7    0    0    0    0  755    3    0    0
     2    4    0]
 [   0    0    8    0    0    2   13    0    0    0   15 2122   45    4
     1    0    0]
 [   0    0    0   21    6    7    0    0    0    0   10   11  472    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    1    0    0
  1136    0    0]
 [   0    0    0    0    0    1   13    0    0    0    0    0    0    0
   105  228    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.1219512195122

F1 scores:
[       nan 0.98765432 0.97715834 0.91581633 0.98383372 0.95893452
 0.97691735 1.         0.99883856 0.8372093  0.90963855 0.97451206
 0.89478673 0.97883598 0.95302013 0.7848537  0.95402299]

Kappa:
0.9444150862965743
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f01e454e7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.406, val_acc:0.554]
Epoch [2/120    avg_loss:1.891, val_acc:0.519]
Epoch [3/120    avg_loss:1.648, val_acc:0.574]
Epoch [4/120    avg_loss:1.406, val_acc:0.626]
Epoch [5/120    avg_loss:1.165, val_acc:0.716]
Epoch [6/120    avg_loss:0.928, val_acc:0.706]
Epoch [7/120    avg_loss:0.824, val_acc:0.751]
Epoch [8/120    avg_loss:0.752, val_acc:0.776]
Epoch [9/120    avg_loss:0.784, val_acc:0.787]
Epoch [10/120    avg_loss:0.674, val_acc:0.797]
Epoch [11/120    avg_loss:0.507, val_acc:0.832]
Epoch [12/120    avg_loss:0.446, val_acc:0.856]
Epoch [13/120    avg_loss:0.469, val_acc:0.849]
Epoch [14/120    avg_loss:0.441, val_acc:0.832]
Epoch [15/120    avg_loss:0.361, val_acc:0.882]
Epoch [16/120    avg_loss:0.306, val_acc:0.884]
Epoch [17/120    avg_loss:0.318, val_acc:0.875]
Epoch [18/120    avg_loss:0.312, val_acc:0.882]
Epoch [19/120    avg_loss:0.267, val_acc:0.885]
Epoch [20/120    avg_loss:0.245, val_acc:0.898]
Epoch [21/120    avg_loss:0.294, val_acc:0.886]
Epoch [22/120    avg_loss:0.234, val_acc:0.898]
Epoch [23/120    avg_loss:0.184, val_acc:0.921]
Epoch [24/120    avg_loss:0.241, val_acc:0.885]
Epoch [25/120    avg_loss:0.232, val_acc:0.908]
Epoch [26/120    avg_loss:0.223, val_acc:0.919]
Epoch [27/120    avg_loss:0.161, val_acc:0.911]
Epoch [28/120    avg_loss:0.157, val_acc:0.927]
Epoch [29/120    avg_loss:0.192, val_acc:0.919]
Epoch [30/120    avg_loss:0.126, val_acc:0.945]
Epoch [31/120    avg_loss:0.132, val_acc:0.923]
Epoch [32/120    avg_loss:0.109, val_acc:0.927]
Epoch [33/120    avg_loss:0.147, val_acc:0.944]
Epoch [34/120    avg_loss:0.124, val_acc:0.927]
Epoch [35/120    avg_loss:0.118, val_acc:0.930]
Epoch [36/120    avg_loss:0.095, val_acc:0.950]
Epoch [37/120    avg_loss:0.073, val_acc:0.946]
Epoch [38/120    avg_loss:0.094, val_acc:0.943]
Epoch [39/120    avg_loss:0.121, val_acc:0.926]
Epoch [40/120    avg_loss:0.158, val_acc:0.907]
Epoch [41/120    avg_loss:0.149, val_acc:0.920]
Epoch [42/120    avg_loss:0.102, val_acc:0.939]
Epoch [43/120    avg_loss:0.072, val_acc:0.938]
Epoch [44/120    avg_loss:0.076, val_acc:0.953]
Epoch [45/120    avg_loss:0.074, val_acc:0.954]
Epoch [46/120    avg_loss:0.105, val_acc:0.947]
Epoch [47/120    avg_loss:0.059, val_acc:0.957]
Epoch [48/120    avg_loss:0.078, val_acc:0.947]
Epoch [49/120    avg_loss:0.108, val_acc:0.949]
Epoch [50/120    avg_loss:0.078, val_acc:0.961]
Epoch [51/120    avg_loss:0.057, val_acc:0.950]
Epoch [52/120    avg_loss:0.083, val_acc:0.876]
Epoch [53/120    avg_loss:0.118, val_acc:0.943]
Epoch [54/120    avg_loss:0.074, val_acc:0.950]
Epoch [55/120    avg_loss:0.051, val_acc:0.963]
Epoch [56/120    avg_loss:0.053, val_acc:0.972]
Epoch [57/120    avg_loss:0.049, val_acc:0.957]
Epoch [58/120    avg_loss:0.043, val_acc:0.961]
Epoch [59/120    avg_loss:0.037, val_acc:0.971]
Epoch [60/120    avg_loss:0.038, val_acc:0.964]
Epoch [61/120    avg_loss:0.029, val_acc:0.965]
Epoch [62/120    avg_loss:0.041, val_acc:0.965]
Epoch [63/120    avg_loss:0.058, val_acc:0.962]
Epoch [64/120    avg_loss:0.046, val_acc:0.975]
Epoch [65/120    avg_loss:0.056, val_acc:0.956]
Epoch [66/120    avg_loss:0.042, val_acc:0.968]
Epoch [67/120    avg_loss:0.042, val_acc:0.958]
Epoch [68/120    avg_loss:0.031, val_acc:0.973]
Epoch [69/120    avg_loss:0.030, val_acc:0.972]
Epoch [70/120    avg_loss:0.037, val_acc:0.972]
Epoch [71/120    avg_loss:0.034, val_acc:0.974]
Epoch [72/120    avg_loss:0.041, val_acc:0.949]
Epoch [73/120    avg_loss:0.042, val_acc:0.971]
Epoch [74/120    avg_loss:0.039, val_acc:0.966]
Epoch [75/120    avg_loss:0.029, val_acc:0.961]
Epoch [76/120    avg_loss:0.037, val_acc:0.955]
Epoch [77/120    avg_loss:0.037, val_acc:0.968]
Epoch [78/120    avg_loss:0.029, val_acc:0.970]
Epoch [79/120    avg_loss:0.017, val_acc:0.973]
Epoch [80/120    avg_loss:0.019, val_acc:0.976]
Epoch [81/120    avg_loss:0.014, val_acc:0.976]
Epoch [82/120    avg_loss:0.018, val_acc:0.977]
Epoch [83/120    avg_loss:0.016, val_acc:0.980]
Epoch [84/120    avg_loss:0.015, val_acc:0.980]
Epoch [85/120    avg_loss:0.016, val_acc:0.981]
Epoch [86/120    avg_loss:0.015, val_acc:0.981]
Epoch [87/120    avg_loss:0.014, val_acc:0.980]
Epoch [88/120    avg_loss:0.016, val_acc:0.980]
Epoch [89/120    avg_loss:0.017, val_acc:0.979]
Epoch [90/120    avg_loss:0.014, val_acc:0.980]
Epoch [91/120    avg_loss:0.013, val_acc:0.981]
Epoch [92/120    avg_loss:0.015, val_acc:0.982]
Epoch [93/120    avg_loss:0.015, val_acc:0.982]
Epoch [94/120    avg_loss:0.014, val_acc:0.982]
Epoch [95/120    avg_loss:0.015, val_acc:0.982]
Epoch [96/120    avg_loss:0.016, val_acc:0.982]
Epoch [97/120    avg_loss:0.013, val_acc:0.982]
Epoch [98/120    avg_loss:0.015, val_acc:0.981]
Epoch [99/120    avg_loss:0.013, val_acc:0.982]
Epoch [100/120    avg_loss:0.014, val_acc:0.982]
Epoch [101/120    avg_loss:0.014, val_acc:0.982]
Epoch [102/120    avg_loss:0.013, val_acc:0.982]
Epoch [103/120    avg_loss:0.012, val_acc:0.982]
Epoch [104/120    avg_loss:0.013, val_acc:0.983]
Epoch [105/120    avg_loss:0.012, val_acc:0.983]
Epoch [106/120    avg_loss:0.015, val_acc:0.983]
Epoch [107/120    avg_loss:0.015, val_acc:0.983]
Epoch [108/120    avg_loss:0.012, val_acc:0.983]
Epoch [109/120    avg_loss:0.013, val_acc:0.983]
Epoch [110/120    avg_loss:0.018, val_acc:0.982]
Epoch [111/120    avg_loss:0.013, val_acc:0.982]
Epoch [112/120    avg_loss:0.012, val_acc:0.981]
Epoch [113/120    avg_loss:0.011, val_acc:0.983]
Epoch [114/120    avg_loss:0.014, val_acc:0.983]
Epoch [115/120    avg_loss:0.012, val_acc:0.982]
Epoch [116/120    avg_loss:0.011, val_acc:0.982]
Epoch [117/120    avg_loss:0.016, val_acc:0.982]
Epoch [118/120    avg_loss:0.015, val_acc:0.981]
Epoch [119/120    avg_loss:0.014, val_acc:0.982]
Epoch [120/120    avg_loss:0.009, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1257    1    0    0    1    0    0    0   14    7    5    0
     0    0    0]
 [   0    0    1  735    0    5    0    0    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    4    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   11   87    0    4    0    0    0    0  765    4    3    0
     0    1    0]
 [   0    0    6    0    0    1   12    0    0    0   12 2175    0    4
     0    0    0]
 [   0    0    0   22    7    6    0    0    0    0    2    2  486    0
     0    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
   106  238    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.130081300813

F1 scores:
[       nan 0.975      0.98164779 0.92220828 0.98383372 0.975
 0.98720843 0.98039216 0.99883856 0.72727273 0.91507177 0.9888611
 0.94552529 0.98930481 0.95338093 0.81228669 0.94915254]

Kappa:
0.9558648292290893
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffa20ea0748>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.443, val_acc:0.503]
Epoch [2/120    avg_loss:1.964, val_acc:0.546]
Epoch [3/120    avg_loss:1.654, val_acc:0.560]
Epoch [4/120    avg_loss:1.368, val_acc:0.656]
Epoch [5/120    avg_loss:1.115, val_acc:0.670]
Epoch [6/120    avg_loss:0.983, val_acc:0.726]
Epoch [7/120    avg_loss:0.834, val_acc:0.777]
Epoch [8/120    avg_loss:0.798, val_acc:0.763]
Epoch [9/120    avg_loss:0.699, val_acc:0.784]
Epoch [10/120    avg_loss:0.640, val_acc:0.807]
Epoch [11/120    avg_loss:0.623, val_acc:0.799]
Epoch [12/120    avg_loss:0.483, val_acc:0.836]
Epoch [13/120    avg_loss:0.437, val_acc:0.824]
Epoch [14/120    avg_loss:0.405, val_acc:0.781]
Epoch [15/120    avg_loss:0.415, val_acc:0.859]
Epoch [16/120    avg_loss:0.355, val_acc:0.884]
Epoch [17/120    avg_loss:0.346, val_acc:0.845]
Epoch [18/120    avg_loss:0.335, val_acc:0.881]
Epoch [19/120    avg_loss:0.283, val_acc:0.885]
Epoch [20/120    avg_loss:0.314, val_acc:0.905]
Epoch [21/120    avg_loss:0.244, val_acc:0.909]
Epoch [22/120    avg_loss:0.255, val_acc:0.907]
Epoch [23/120    avg_loss:0.190, val_acc:0.931]
Epoch [24/120    avg_loss:0.177, val_acc:0.925]
Epoch [25/120    avg_loss:0.168, val_acc:0.926]
Epoch [26/120    avg_loss:0.170, val_acc:0.919]
Epoch [27/120    avg_loss:0.214, val_acc:0.892]
Epoch [28/120    avg_loss:0.245, val_acc:0.909]
Epoch [29/120    avg_loss:0.164, val_acc:0.919]
Epoch [30/120    avg_loss:0.156, val_acc:0.935]
Epoch [31/120    avg_loss:0.146, val_acc:0.917]
Epoch [32/120    avg_loss:0.111, val_acc:0.938]
Epoch [33/120    avg_loss:0.121, val_acc:0.924]
Epoch [34/120    avg_loss:0.184, val_acc:0.840]
Epoch [35/120    avg_loss:0.222, val_acc:0.925]
Epoch [36/120    avg_loss:0.164, val_acc:0.906]
Epoch [37/120    avg_loss:0.122, val_acc:0.941]
Epoch [38/120    avg_loss:0.077, val_acc:0.948]
Epoch [39/120    avg_loss:0.078, val_acc:0.954]
Epoch [40/120    avg_loss:0.075, val_acc:0.942]
Epoch [41/120    avg_loss:0.081, val_acc:0.938]
Epoch [42/120    avg_loss:0.096, val_acc:0.934]
Epoch [43/120    avg_loss:0.070, val_acc:0.951]
Epoch [44/120    avg_loss:0.104, val_acc:0.961]
Epoch [45/120    avg_loss:0.081, val_acc:0.951]
Epoch [46/120    avg_loss:0.075, val_acc:0.944]
Epoch [47/120    avg_loss:0.063, val_acc:0.947]
Epoch [48/120    avg_loss:0.068, val_acc:0.931]
Epoch [49/120    avg_loss:0.083, val_acc:0.966]
Epoch [50/120    avg_loss:0.065, val_acc:0.948]
Epoch [51/120    avg_loss:0.054, val_acc:0.944]
Epoch [52/120    avg_loss:0.081, val_acc:0.942]
Epoch [53/120    avg_loss:0.069, val_acc:0.964]
Epoch [54/120    avg_loss:0.039, val_acc:0.961]
Epoch [55/120    avg_loss:0.040, val_acc:0.958]
Epoch [56/120    avg_loss:0.042, val_acc:0.959]
Epoch [57/120    avg_loss:0.047, val_acc:0.962]
Epoch [58/120    avg_loss:0.066, val_acc:0.941]
Epoch [59/120    avg_loss:0.057, val_acc:0.968]
Epoch [60/120    avg_loss:0.040, val_acc:0.970]
Epoch [61/120    avg_loss:0.035, val_acc:0.959]
Epoch [62/120    avg_loss:0.038, val_acc:0.968]
Epoch [63/120    avg_loss:0.070, val_acc:0.955]
Epoch [64/120    avg_loss:0.076, val_acc:0.963]
Epoch [65/120    avg_loss:0.046, val_acc:0.971]
Epoch [66/120    avg_loss:0.033, val_acc:0.963]
Epoch [67/120    avg_loss:0.034, val_acc:0.966]
Epoch [68/120    avg_loss:0.041, val_acc:0.966]
Epoch [69/120    avg_loss:0.049, val_acc:0.951]
Epoch [70/120    avg_loss:0.040, val_acc:0.959]
Epoch [71/120    avg_loss:0.043, val_acc:0.975]
Epoch [72/120    avg_loss:0.036, val_acc:0.975]
Epoch [73/120    avg_loss:0.030, val_acc:0.973]
Epoch [74/120    avg_loss:0.027, val_acc:0.983]
Epoch [75/120    avg_loss:0.037, val_acc:0.976]
Epoch [76/120    avg_loss:0.028, val_acc:0.964]
Epoch [77/120    avg_loss:0.028, val_acc:0.978]
Epoch [78/120    avg_loss:0.032, val_acc:0.965]
Epoch [79/120    avg_loss:0.027, val_acc:0.971]
Epoch [80/120    avg_loss:0.031, val_acc:0.957]
Epoch [81/120    avg_loss:0.074, val_acc:0.955]
Epoch [82/120    avg_loss:0.088, val_acc:0.962]
Epoch [83/120    avg_loss:0.056, val_acc:0.959]
Epoch [84/120    avg_loss:0.041, val_acc:0.978]
Epoch [85/120    avg_loss:0.039, val_acc:0.969]
Epoch [86/120    avg_loss:0.026, val_acc:0.970]
Epoch [87/120    avg_loss:0.025, val_acc:0.980]
Epoch [88/120    avg_loss:0.018, val_acc:0.980]
Epoch [89/120    avg_loss:0.023, val_acc:0.980]
Epoch [90/120    avg_loss:0.014, val_acc:0.978]
Epoch [91/120    avg_loss:0.014, val_acc:0.977]
Epoch [92/120    avg_loss:0.015, val_acc:0.980]
Epoch [93/120    avg_loss:0.012, val_acc:0.981]
Epoch [94/120    avg_loss:0.016, val_acc:0.979]
Epoch [95/120    avg_loss:0.016, val_acc:0.978]
Epoch [96/120    avg_loss:0.013, val_acc:0.978]
Epoch [97/120    avg_loss:0.016, val_acc:0.980]
Epoch [98/120    avg_loss:0.012, val_acc:0.981]
Epoch [99/120    avg_loss:0.016, val_acc:0.978]
Epoch [100/120    avg_loss:0.012, val_acc:0.979]
Epoch [101/120    avg_loss:0.012, val_acc:0.979]
Epoch [102/120    avg_loss:0.011, val_acc:0.979]
Epoch [103/120    avg_loss:0.012, val_acc:0.978]
Epoch [104/120    avg_loss:0.013, val_acc:0.978]
Epoch [105/120    avg_loss:0.010, val_acc:0.979]
Epoch [106/120    avg_loss:0.013, val_acc:0.980]
Epoch [107/120    avg_loss:0.012, val_acc:0.979]
Epoch [108/120    avg_loss:0.013, val_acc:0.980]
Epoch [109/120    avg_loss:0.016, val_acc:0.980]
Epoch [110/120    avg_loss:0.010, val_acc:0.980]
Epoch [111/120    avg_loss:0.011, val_acc:0.980]
Epoch [112/120    avg_loss:0.012, val_acc:0.980]
Epoch [113/120    avg_loss:0.014, val_acc:0.980]
Epoch [114/120    avg_loss:0.013, val_acc:0.980]
Epoch [115/120    avg_loss:0.011, val_acc:0.980]
Epoch [116/120    avg_loss:0.010, val_acc:0.980]
Epoch [117/120    avg_loss:0.013, val_acc:0.980]
Epoch [118/120    avg_loss:0.011, val_acc:0.980]
Epoch [119/120    avg_loss:0.011, val_acc:0.980]
Epoch [120/120    avg_loss:0.012, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    5 1256    2    0    0    2    0    0    0    4   15    0    0
     0    1    0]
 [   0    0    2  716    2   14    0    0    0   10    0    0    1    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    0    0    5    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   15    0    0    2    0
     0    0    0]
 [   0    0    4   83    0    1    0    0    0    0  775    8    0    0
     0    4    0]
 [   0    0    7    0    0    0   12    0    0    0   11 2175    1    1
     3    0    0]
 [   0    0    0   16    7    8    0    0    0    0    8    0  490    0
     0    1    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    1    1    0    0
  1134    1    0]
 [   0    0    0    0    0    0   36    0    0    1    0    0    0    0
    53  257    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.16260162601625

F1 scores:
[       nan 0.93023256 0.98317025 0.91501597 0.97931034 0.9569161
 0.96182085 1.         1.         0.6122449  0.92592593 0.98617094
 0.95238095 0.9919571  0.97047497 0.84124386 0.97076023]

Kappa:
0.9562473467254382
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9dde784710>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.496, val_acc:0.353]
Epoch [2/120    avg_loss:1.928, val_acc:0.530]
Epoch [3/120    avg_loss:1.594, val_acc:0.555]
Epoch [4/120    avg_loss:1.446, val_acc:0.641]
Epoch [5/120    avg_loss:1.188, val_acc:0.655]
Epoch [6/120    avg_loss:1.046, val_acc:0.748]
Epoch [7/120    avg_loss:0.830, val_acc:0.761]
Epoch [8/120    avg_loss:0.652, val_acc:0.790]
Epoch [9/120    avg_loss:0.629, val_acc:0.804]
Epoch [10/120    avg_loss:0.599, val_acc:0.739]
Epoch [11/120    avg_loss:0.524, val_acc:0.791]
Epoch [12/120    avg_loss:0.450, val_acc:0.831]
Epoch [13/120    avg_loss:0.334, val_acc:0.889]
Epoch [14/120    avg_loss:0.284, val_acc:0.854]
Epoch [15/120    avg_loss:0.526, val_acc:0.807]
Epoch [16/120    avg_loss:0.463, val_acc:0.865]
Epoch [17/120    avg_loss:0.346, val_acc:0.851]
Epoch [18/120    avg_loss:0.389, val_acc:0.868]
Epoch [19/120    avg_loss:0.269, val_acc:0.896]
Epoch [20/120    avg_loss:0.196, val_acc:0.933]
Epoch [21/120    avg_loss:0.178, val_acc:0.925]
Epoch [22/120    avg_loss:0.182, val_acc:0.916]
Epoch [23/120    avg_loss:0.158, val_acc:0.938]
Epoch [24/120    avg_loss:0.137, val_acc:0.945]
Epoch [25/120    avg_loss:0.112, val_acc:0.942]
Epoch [26/120    avg_loss:0.087, val_acc:0.954]
Epoch [27/120    avg_loss:0.082, val_acc:0.959]
Epoch [28/120    avg_loss:0.071, val_acc:0.960]
Epoch [29/120    avg_loss:0.060, val_acc:0.964]
Epoch [30/120    avg_loss:0.063, val_acc:0.951]
Epoch [31/120    avg_loss:0.078, val_acc:0.966]
Epoch [32/120    avg_loss:0.089, val_acc:0.967]
Epoch [33/120    avg_loss:0.066, val_acc:0.965]
Epoch [34/120    avg_loss:0.091, val_acc:0.923]
Epoch [35/120    avg_loss:0.171, val_acc:0.944]
Epoch [36/120    avg_loss:0.099, val_acc:0.954]
Epoch [37/120    avg_loss:0.061, val_acc:0.953]
Epoch [38/120    avg_loss:0.063, val_acc:0.967]
Epoch [39/120    avg_loss:0.042, val_acc:0.968]
Epoch [40/120    avg_loss:0.054, val_acc:0.969]
Epoch [41/120    avg_loss:0.053, val_acc:0.968]
Epoch [42/120    avg_loss:0.037, val_acc:0.972]
Epoch [43/120    avg_loss:0.031, val_acc:0.980]
Epoch [44/120    avg_loss:0.030, val_acc:0.975]
Epoch [45/120    avg_loss:0.030, val_acc:0.975]
Epoch [46/120    avg_loss:0.031, val_acc:0.975]
Epoch [47/120    avg_loss:0.033, val_acc:0.977]
Epoch [48/120    avg_loss:0.028, val_acc:0.971]
Epoch [49/120    avg_loss:0.031, val_acc:0.973]
Epoch [50/120    avg_loss:0.018, val_acc:0.978]
Epoch [51/120    avg_loss:0.014, val_acc:0.980]
Epoch [52/120    avg_loss:0.020, val_acc:0.980]
Epoch [53/120    avg_loss:0.020, val_acc:0.982]
Epoch [54/120    avg_loss:0.020, val_acc:0.980]
Epoch [55/120    avg_loss:0.015, val_acc:0.979]
Epoch [56/120    avg_loss:0.013, val_acc:0.979]
Epoch [57/120    avg_loss:0.013, val_acc:0.983]
Epoch [58/120    avg_loss:0.010, val_acc:0.982]
Epoch [59/120    avg_loss:0.013, val_acc:0.977]
Epoch [60/120    avg_loss:0.016, val_acc:0.982]
Epoch [61/120    avg_loss:0.013, val_acc:0.985]
Epoch [62/120    avg_loss:0.015, val_acc:0.982]
Epoch [63/120    avg_loss:0.015, val_acc:0.984]
Epoch [64/120    avg_loss:0.013, val_acc:0.981]
Epoch [65/120    avg_loss:0.019, val_acc:0.973]
Epoch [66/120    avg_loss:0.014, val_acc:0.977]
Epoch [67/120    avg_loss:0.010, val_acc:0.981]
Epoch [68/120    avg_loss:0.015, val_acc:0.981]
Epoch [69/120    avg_loss:0.012, val_acc:0.982]
Epoch [70/120    avg_loss:0.011, val_acc:0.983]
Epoch [71/120    avg_loss:0.007, val_acc:0.985]
Epoch [72/120    avg_loss:0.006, val_acc:0.986]
Epoch [73/120    avg_loss:0.006, val_acc:0.982]
Epoch [74/120    avg_loss:0.008, val_acc:0.984]
Epoch [75/120    avg_loss:0.015, val_acc:0.979]
Epoch [76/120    avg_loss:0.013, val_acc:0.981]
Epoch [77/120    avg_loss:0.010, val_acc:0.981]
Epoch [78/120    avg_loss:0.008, val_acc:0.983]
Epoch [79/120    avg_loss:0.007, val_acc:0.981]
Epoch [80/120    avg_loss:0.010, val_acc:0.980]
Epoch [81/120    avg_loss:0.006, val_acc:0.983]
Epoch [82/120    avg_loss:0.006, val_acc:0.982]
Epoch [83/120    avg_loss:0.007, val_acc:0.983]
Epoch [84/120    avg_loss:0.006, val_acc:0.982]
Epoch [85/120    avg_loss:0.005, val_acc:0.986]
Epoch [86/120    avg_loss:0.006, val_acc:0.978]
Epoch [87/120    avg_loss:0.006, val_acc:0.988]
Epoch [88/120    avg_loss:0.005, val_acc:0.983]
Epoch [89/120    avg_loss:0.006, val_acc:0.980]
Epoch [90/120    avg_loss:0.010, val_acc:0.984]
Epoch [91/120    avg_loss:0.007, val_acc:0.983]
Epoch [92/120    avg_loss:0.005, val_acc:0.981]
Epoch [93/120    avg_loss:0.004, val_acc:0.986]
Epoch [94/120    avg_loss:0.005, val_acc:0.984]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.004, val_acc:0.980]
Epoch [97/120    avg_loss:0.003, val_acc:0.989]
Epoch [98/120    avg_loss:0.004, val_acc:0.980]
Epoch [99/120    avg_loss:0.009, val_acc:0.981]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.003, val_acc:0.986]
Epoch [102/120    avg_loss:0.004, val_acc:0.983]
Epoch [103/120    avg_loss:0.003, val_acc:0.983]
Epoch [104/120    avg_loss:0.004, val_acc:0.980]
Epoch [105/120    avg_loss:0.004, val_acc:0.980]
Epoch [106/120    avg_loss:0.006, val_acc:0.984]
Epoch [107/120    avg_loss:0.003, val_acc:0.988]
Epoch [108/120    avg_loss:0.003, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.989]
Epoch [110/120    avg_loss:0.006, val_acc:0.981]
Epoch [111/120    avg_loss:0.005, val_acc:0.988]
Epoch [112/120    avg_loss:0.004, val_acc:0.982]
Epoch [113/120    avg_loss:0.006, val_acc:0.986]
Epoch [114/120    avg_loss:0.004, val_acc:0.978]
Epoch [115/120    avg_loss:0.003, val_acc:0.986]
Epoch [116/120    avg_loss:0.004, val_acc:0.985]
Epoch [117/120    avg_loss:0.003, val_acc:0.986]
Epoch [118/120    avg_loss:0.003, val_acc:0.985]
Epoch [119/120    avg_loss:0.003, val_acc:0.991]
Epoch [120/120    avg_loss:0.003, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    0    4    2    1    0    0    0    1   16    0    0
     0    0    0]
 [   0    0    0  722    7    0    3    0    0    1    1    8    4    1
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    3    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   16    0    1    0    0
     0    0    0]
 [   0    0   13    0    0    0    0    0    0    0  831   30    0    0
     0    1    0]
 [   0    0   15    0    0    0    0    0    0    0    2 2190    3    0
     0    0    0]
 [   0    0    0    6    0    0    0    0    0    0    0    0  527    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1134    5    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    45  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.00542005420054

F1 scores:
[       nan 0.98765432 0.97941748 0.97831978 0.97247706 0.98959538
 0.99468489 0.94339623 1.         0.91428571 0.97192982 0.98272381
 0.98596819 0.99730458 0.97674419 0.9221374  0.98809524]

Kappa:
0.9772358871049669
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5db7c6f6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.517, val_acc:0.274]
Epoch [2/120    avg_loss:1.957, val_acc:0.575]
Epoch [3/120    avg_loss:1.646, val_acc:0.631]
Epoch [4/120    avg_loss:1.458, val_acc:0.568]
Epoch [5/120    avg_loss:1.280, val_acc:0.704]
Epoch [6/120    avg_loss:1.107, val_acc:0.711]
Epoch [7/120    avg_loss:0.958, val_acc:0.762]
Epoch [8/120    avg_loss:0.863, val_acc:0.746]
Epoch [9/120    avg_loss:0.723, val_acc:0.812]
Epoch [10/120    avg_loss:0.725, val_acc:0.766]
Epoch [11/120    avg_loss:0.568, val_acc:0.827]
Epoch [12/120    avg_loss:0.483, val_acc:0.826]
Epoch [13/120    avg_loss:0.565, val_acc:0.841]
Epoch [14/120    avg_loss:0.432, val_acc:0.849]
Epoch [15/120    avg_loss:0.374, val_acc:0.827]
Epoch [16/120    avg_loss:0.266, val_acc:0.880]
Epoch [17/120    avg_loss:0.213, val_acc:0.894]
Epoch [18/120    avg_loss:0.261, val_acc:0.870]
Epoch [19/120    avg_loss:0.209, val_acc:0.911]
Epoch [20/120    avg_loss:0.165, val_acc:0.916]
Epoch [21/120    avg_loss:0.159, val_acc:0.924]
Epoch [22/120    avg_loss:0.125, val_acc:0.932]
Epoch [23/120    avg_loss:0.118, val_acc:0.917]
Epoch [24/120    avg_loss:0.193, val_acc:0.926]
Epoch [25/120    avg_loss:0.130, val_acc:0.940]
Epoch [26/120    avg_loss:0.113, val_acc:0.946]
Epoch [27/120    avg_loss:0.076, val_acc:0.950]
Epoch [28/120    avg_loss:0.079, val_acc:0.908]
Epoch [29/120    avg_loss:0.070, val_acc:0.951]
Epoch [30/120    avg_loss:0.071, val_acc:0.955]
Epoch [31/120    avg_loss:0.062, val_acc:0.952]
Epoch [32/120    avg_loss:0.072, val_acc:0.958]
Epoch [33/120    avg_loss:0.076, val_acc:0.945]
Epoch [34/120    avg_loss:0.052, val_acc:0.955]
Epoch [35/120    avg_loss:0.060, val_acc:0.958]
Epoch [36/120    avg_loss:0.066, val_acc:0.932]
Epoch [37/120    avg_loss:0.077, val_acc:0.945]
Epoch [38/120    avg_loss:0.076, val_acc:0.969]
Epoch [39/120    avg_loss:0.046, val_acc:0.960]
Epoch [40/120    avg_loss:0.032, val_acc:0.970]
Epoch [41/120    avg_loss:0.029, val_acc:0.970]
Epoch [42/120    avg_loss:0.028, val_acc:0.973]
Epoch [43/120    avg_loss:0.029, val_acc:0.971]
Epoch [44/120    avg_loss:0.035, val_acc:0.953]
Epoch [45/120    avg_loss:0.049, val_acc:0.969]
Epoch [46/120    avg_loss:0.037, val_acc:0.968]
Epoch [47/120    avg_loss:0.030, val_acc:0.967]
Epoch [48/120    avg_loss:0.022, val_acc:0.963]
Epoch [49/120    avg_loss:0.020, val_acc:0.975]
Epoch [50/120    avg_loss:0.017, val_acc:0.971]
Epoch [51/120    avg_loss:0.017, val_acc:0.973]
Epoch [52/120    avg_loss:0.015, val_acc:0.968]
Epoch [53/120    avg_loss:0.024, val_acc:0.974]
Epoch [54/120    avg_loss:0.023, val_acc:0.970]
Epoch [55/120    avg_loss:0.018, val_acc:0.971]
Epoch [56/120    avg_loss:0.021, val_acc:0.973]
Epoch [57/120    avg_loss:0.020, val_acc:0.967]
Epoch [58/120    avg_loss:0.019, val_acc:0.966]
Epoch [59/120    avg_loss:0.014, val_acc:0.971]
Epoch [60/120    avg_loss:0.011, val_acc:0.976]
Epoch [61/120    avg_loss:0.013, val_acc:0.970]
Epoch [62/120    avg_loss:0.011, val_acc:0.974]
Epoch [63/120    avg_loss:0.018, val_acc:0.979]
Epoch [64/120    avg_loss:0.012, val_acc:0.975]
Epoch [65/120    avg_loss:0.010, val_acc:0.978]
Epoch [66/120    avg_loss:0.009, val_acc:0.980]
Epoch [67/120    avg_loss:0.009, val_acc:0.975]
Epoch [68/120    avg_loss:0.010, val_acc:0.979]
Epoch [69/120    avg_loss:0.010, val_acc:0.977]
Epoch [70/120    avg_loss:0.013, val_acc:0.970]
Epoch [71/120    avg_loss:0.031, val_acc:0.966]
Epoch [72/120    avg_loss:0.025, val_acc:0.970]
Epoch [73/120    avg_loss:0.017, val_acc:0.986]
Epoch [74/120    avg_loss:0.011, val_acc:0.976]
Epoch [75/120    avg_loss:0.013, val_acc:0.979]
Epoch [76/120    avg_loss:0.011, val_acc:0.980]
Epoch [77/120    avg_loss:0.009, val_acc:0.978]
Epoch [78/120    avg_loss:0.006, val_acc:0.984]
Epoch [79/120    avg_loss:0.008, val_acc:0.982]
Epoch [80/120    avg_loss:0.007, val_acc:0.981]
Epoch [81/120    avg_loss:0.007, val_acc:0.979]
Epoch [82/120    avg_loss:0.006, val_acc:0.983]
Epoch [83/120    avg_loss:0.008, val_acc:0.981]
Epoch [84/120    avg_loss:0.009, val_acc:0.978]
Epoch [85/120    avg_loss:0.005, val_acc:0.981]
Epoch [86/120    avg_loss:0.008, val_acc:0.982]
Epoch [87/120    avg_loss:0.006, val_acc:0.982]
Epoch [88/120    avg_loss:0.006, val_acc:0.983]
Epoch [89/120    avg_loss:0.005, val_acc:0.983]
Epoch [90/120    avg_loss:0.005, val_acc:0.982]
Epoch [91/120    avg_loss:0.004, val_acc:0.983]
Epoch [92/120    avg_loss:0.004, val_acc:0.983]
Epoch [93/120    avg_loss:0.003, val_acc:0.983]
Epoch [94/120    avg_loss:0.005, val_acc:0.981]
Epoch [95/120    avg_loss:0.005, val_acc:0.984]
Epoch [96/120    avg_loss:0.004, val_acc:0.982]
Epoch [97/120    avg_loss:0.005, val_acc:0.983]
Epoch [98/120    avg_loss:0.004, val_acc:0.982]
Epoch [99/120    avg_loss:0.005, val_acc:0.982]
Epoch [100/120    avg_loss:0.004, val_acc:0.983]
Epoch [101/120    avg_loss:0.004, val_acc:0.982]
Epoch [102/120    avg_loss:0.005, val_acc:0.982]
Epoch [103/120    avg_loss:0.004, val_acc:0.982]
Epoch [104/120    avg_loss:0.004, val_acc:0.982]
Epoch [105/120    avg_loss:0.004, val_acc:0.983]
Epoch [106/120    avg_loss:0.004, val_acc:0.983]
Epoch [107/120    avg_loss:0.004, val_acc:0.983]
Epoch [108/120    avg_loss:0.004, val_acc:0.983]
Epoch [109/120    avg_loss:0.003, val_acc:0.982]
Epoch [110/120    avg_loss:0.003, val_acc:0.982]
Epoch [111/120    avg_loss:0.004, val_acc:0.982]
Epoch [112/120    avg_loss:0.004, val_acc:0.982]
Epoch [113/120    avg_loss:0.003, val_acc:0.982]
Epoch [114/120    avg_loss:0.005, val_acc:0.982]
Epoch [115/120    avg_loss:0.005, val_acc:0.982]
Epoch [116/120    avg_loss:0.004, val_acc:0.982]
Epoch [117/120    avg_loss:0.005, val_acc:0.982]
Epoch [118/120    avg_loss:0.006, val_acc:0.982]
Epoch [119/120    avg_loss:0.004, val_acc:0.982]
Epoch [120/120    avg_loss:0.004, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    1 1265    4    0    0    0    0    0    0    2   10    3    0
     0    0    0]
 [   0    0    0  714    3    0    0    0    0    1    1    2   25    1
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0  862   10    0    0
     0    0    0]
 [   0    0   13    0    0    0    0    0    0    1    3 2189    4    0
     0    0    0]
 [   0    0    0    4    1    0    0    0    0    0    0    0  528    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    1    0    0    4    0    0    0    0    0    0    0    0
  1126    8    0]
 [   0    0    0    0    0    1    4    0    0    0    0    0    0    0
    67  275    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.97289972899729

F1 scores:
[       nan 0.96296296 0.98558629 0.97076818 0.98598131 0.99313501
 0.99467681 1.         1.         0.91891892 0.98796562 0.98937853
 0.96526508 0.99730458 0.96528075 0.87301587 0.99408284]

Kappa:
0.9768758862936628
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f94c48106d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.410, val_acc:0.475]
Epoch [2/120    avg_loss:1.915, val_acc:0.578]
Epoch [3/120    avg_loss:1.647, val_acc:0.582]
Epoch [4/120    avg_loss:1.479, val_acc:0.599]
Epoch [5/120    avg_loss:1.293, val_acc:0.727]
Epoch [6/120    avg_loss:1.071, val_acc:0.720]
Epoch [7/120    avg_loss:0.946, val_acc:0.735]
Epoch [8/120    avg_loss:0.765, val_acc:0.818]
Epoch [9/120    avg_loss:0.675, val_acc:0.807]
Epoch [10/120    avg_loss:0.579, val_acc:0.804]
Epoch [11/120    avg_loss:0.514, val_acc:0.826]
Epoch [12/120    avg_loss:0.400, val_acc:0.812]
Epoch [13/120    avg_loss:0.548, val_acc:0.799]
Epoch [14/120    avg_loss:0.436, val_acc:0.864]
Epoch [15/120    avg_loss:0.295, val_acc:0.881]
Epoch [16/120    avg_loss:0.279, val_acc:0.886]
Epoch [17/120    avg_loss:0.241, val_acc:0.898]
Epoch [18/120    avg_loss:0.211, val_acc:0.908]
Epoch [19/120    avg_loss:0.209, val_acc:0.856]
Epoch [20/120    avg_loss:0.211, val_acc:0.911]
Epoch [21/120    avg_loss:0.166, val_acc:0.939]
Epoch [22/120    avg_loss:0.142, val_acc:0.915]
Epoch [23/120    avg_loss:0.105, val_acc:0.952]
Epoch [24/120    avg_loss:0.095, val_acc:0.945]
Epoch [25/120    avg_loss:0.089, val_acc:0.948]
Epoch [26/120    avg_loss:0.084, val_acc:0.953]
Epoch [27/120    avg_loss:0.082, val_acc:0.946]
Epoch [28/120    avg_loss:0.071, val_acc:0.946]
Epoch [29/120    avg_loss:0.080, val_acc:0.929]
Epoch [30/120    avg_loss:0.068, val_acc:0.958]
Epoch [31/120    avg_loss:0.052, val_acc:0.953]
Epoch [32/120    avg_loss:0.044, val_acc:0.957]
Epoch [33/120    avg_loss:0.041, val_acc:0.960]
Epoch [34/120    avg_loss:0.029, val_acc:0.965]
Epoch [35/120    avg_loss:0.034, val_acc:0.960]
Epoch [36/120    avg_loss:0.030, val_acc:0.965]
Epoch [37/120    avg_loss:0.029, val_acc:0.968]
Epoch [38/120    avg_loss:0.043, val_acc:0.957]
Epoch [39/120    avg_loss:0.042, val_acc:0.966]
Epoch [40/120    avg_loss:0.034, val_acc:0.964]
Epoch [41/120    avg_loss:0.026, val_acc:0.971]
Epoch [42/120    avg_loss:0.028, val_acc:0.967]
Epoch [43/120    avg_loss:0.024, val_acc:0.967]
Epoch [44/120    avg_loss:0.025, val_acc:0.968]
Epoch [45/120    avg_loss:0.022, val_acc:0.968]
Epoch [46/120    avg_loss:0.032, val_acc:0.969]
Epoch [47/120    avg_loss:0.014, val_acc:0.973]
Epoch [48/120    avg_loss:0.024, val_acc:0.950]
Epoch [49/120    avg_loss:0.034, val_acc:0.966]
Epoch [50/120    avg_loss:0.034, val_acc:0.956]
Epoch [51/120    avg_loss:0.023, val_acc:0.959]
Epoch [52/120    avg_loss:0.022, val_acc:0.973]
Epoch [53/120    avg_loss:0.016, val_acc:0.966]
Epoch [54/120    avg_loss:0.021, val_acc:0.956]
Epoch [55/120    avg_loss:0.016, val_acc:0.978]
Epoch [56/120    avg_loss:0.009, val_acc:0.977]
Epoch [57/120    avg_loss:0.011, val_acc:0.969]
Epoch [58/120    avg_loss:0.012, val_acc:0.972]
Epoch [59/120    avg_loss:0.009, val_acc:0.972]
Epoch [60/120    avg_loss:0.009, val_acc:0.974]
Epoch [61/120    avg_loss:0.009, val_acc:0.976]
Epoch [62/120    avg_loss:0.008, val_acc:0.976]
Epoch [63/120    avg_loss:0.011, val_acc:0.971]
Epoch [64/120    avg_loss:0.007, val_acc:0.976]
Epoch [65/120    avg_loss:0.008, val_acc:0.973]
Epoch [66/120    avg_loss:0.009, val_acc:0.969]
Epoch [67/120    avg_loss:0.012, val_acc:0.971]
Epoch [68/120    avg_loss:0.009, val_acc:0.976]
Epoch [69/120    avg_loss:0.006, val_acc:0.975]
Epoch [70/120    avg_loss:0.006, val_acc:0.974]
Epoch [71/120    avg_loss:0.006, val_acc:0.974]
Epoch [72/120    avg_loss:0.007, val_acc:0.974]
Epoch [73/120    avg_loss:0.006, val_acc:0.974]
Epoch [74/120    avg_loss:0.006, val_acc:0.974]
Epoch [75/120    avg_loss:0.006, val_acc:0.975]
Epoch [76/120    avg_loss:0.005, val_acc:0.974]
Epoch [77/120    avg_loss:0.004, val_acc:0.975]
Epoch [78/120    avg_loss:0.006, val_acc:0.974]
Epoch [79/120    avg_loss:0.005, val_acc:0.974]
Epoch [80/120    avg_loss:0.006, val_acc:0.975]
Epoch [81/120    avg_loss:0.005, val_acc:0.976]
Epoch [82/120    avg_loss:0.005, val_acc:0.976]
Epoch [83/120    avg_loss:0.005, val_acc:0.976]
Epoch [84/120    avg_loss:0.005, val_acc:0.976]
Epoch [85/120    avg_loss:0.005, val_acc:0.976]
Epoch [86/120    avg_loss:0.006, val_acc:0.976]
Epoch [87/120    avg_loss:0.005, val_acc:0.976]
Epoch [88/120    avg_loss:0.005, val_acc:0.976]
Epoch [89/120    avg_loss:0.005, val_acc:0.976]
Epoch [90/120    avg_loss:0.006, val_acc:0.976]
Epoch [91/120    avg_loss:0.005, val_acc:0.976]
Epoch [92/120    avg_loss:0.006, val_acc:0.976]
Epoch [93/120    avg_loss:0.006, val_acc:0.977]
Epoch [94/120    avg_loss:0.008, val_acc:0.977]
Epoch [95/120    avg_loss:0.005, val_acc:0.977]
Epoch [96/120    avg_loss:0.006, val_acc:0.977]
Epoch [97/120    avg_loss:0.006, val_acc:0.977]
Epoch [98/120    avg_loss:0.005, val_acc:0.977]
Epoch [99/120    avg_loss:0.007, val_acc:0.977]
Epoch [100/120    avg_loss:0.005, val_acc:0.977]
Epoch [101/120    avg_loss:0.005, val_acc:0.977]
Epoch [102/120    avg_loss:0.005, val_acc:0.977]
Epoch [103/120    avg_loss:0.007, val_acc:0.977]
Epoch [104/120    avg_loss:0.005, val_acc:0.977]
Epoch [105/120    avg_loss:0.007, val_acc:0.977]
Epoch [106/120    avg_loss:0.006, val_acc:0.977]
Epoch [107/120    avg_loss:0.005, val_acc:0.977]
Epoch [108/120    avg_loss:0.006, val_acc:0.977]
Epoch [109/120    avg_loss:0.005, val_acc:0.977]
Epoch [110/120    avg_loss:0.005, val_acc:0.977]
Epoch [111/120    avg_loss:0.006, val_acc:0.977]
Epoch [112/120    avg_loss:0.006, val_acc:0.977]
Epoch [113/120    avg_loss:0.004, val_acc:0.977]
Epoch [114/120    avg_loss:0.005, val_acc:0.977]
Epoch [115/120    avg_loss:0.006, val_acc:0.977]
Epoch [116/120    avg_loss:0.005, val_acc:0.977]
Epoch [117/120    avg_loss:0.006, val_acc:0.977]
Epoch [118/120    avg_loss:0.005, val_acc:0.977]
Epoch [119/120    avg_loss:0.006, val_acc:0.977]
Epoch [120/120    avg_loss:0.006, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    3    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1259    7    4    0    0    0    0    0    0    8    7    0
     0    0    0]
 [   0    0    0  726    7    0    0    0    0    1    0    6    7    0
     0    0    0]
 [   0    0    0    1  211    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  429    0    2    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  849   20    0    0
     0    0    0]
 [   0    0    9    0    0    0    0    0    0    0    7 2170   11    2
     0    0   11]
 [   0    0    0    5    0    0    0    0    0    0    0    0  526    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    1    0    0    0    0    0    0
  1125   13    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    76  271    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.5609756097561

F1 scores:
[       nan 0.96202532 0.98282592 0.97711978 0.97011494 0.99305556
 0.99923839 0.94339623 1.         0.97297297 0.98093588 0.98323516
 0.96691176 0.99462366 0.95948827 0.85759494 0.91620112]

Kappa:
0.9721857335972646
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8d7746e668>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.496, val_acc:0.487]
Epoch [2/120    avg_loss:1.961, val_acc:0.475]
Epoch [3/120    avg_loss:1.651, val_acc:0.581]
Epoch [4/120    avg_loss:1.420, val_acc:0.654]
Epoch [5/120    avg_loss:1.254, val_acc:0.683]
Epoch [6/120    avg_loss:1.103, val_acc:0.718]
Epoch [7/120    avg_loss:0.944, val_acc:0.759]
Epoch [8/120    avg_loss:0.765, val_acc:0.803]
Epoch [9/120    avg_loss:0.630, val_acc:0.835]
Epoch [10/120    avg_loss:0.578, val_acc:0.821]
Epoch [11/120    avg_loss:0.533, val_acc:0.819]
Epoch [12/120    avg_loss:0.432, val_acc:0.867]
Epoch [13/120    avg_loss:0.346, val_acc:0.881]
Epoch [14/120    avg_loss:0.311, val_acc:0.893]
Epoch [15/120    avg_loss:0.270, val_acc:0.907]
Epoch [16/120    avg_loss:0.279, val_acc:0.912]
Epoch [17/120    avg_loss:0.207, val_acc:0.915]
Epoch [18/120    avg_loss:0.160, val_acc:0.921]
Epoch [19/120    avg_loss:0.256, val_acc:0.915]
Epoch [20/120    avg_loss:0.169, val_acc:0.914]
Epoch [21/120    avg_loss:0.121, val_acc:0.933]
Epoch [22/120    avg_loss:0.139, val_acc:0.939]
Epoch [23/120    avg_loss:0.116, val_acc:0.948]
Epoch [24/120    avg_loss:0.119, val_acc:0.949]
Epoch [25/120    avg_loss:0.091, val_acc:0.934]
Epoch [26/120    avg_loss:0.096, val_acc:0.948]
Epoch [27/120    avg_loss:0.065, val_acc:0.950]
Epoch [28/120    avg_loss:0.071, val_acc:0.963]
Epoch [29/120    avg_loss:0.064, val_acc:0.966]
Epoch [30/120    avg_loss:0.064, val_acc:0.950]
Epoch [31/120    avg_loss:0.077, val_acc:0.961]
Epoch [32/120    avg_loss:0.052, val_acc:0.966]
Epoch [33/120    avg_loss:0.048, val_acc:0.967]
Epoch [34/120    avg_loss:0.041, val_acc:0.970]
Epoch [35/120    avg_loss:0.039, val_acc:0.967]
Epoch [36/120    avg_loss:0.030, val_acc:0.969]
Epoch [37/120    avg_loss:0.038, val_acc:0.969]
Epoch [38/120    avg_loss:0.030, val_acc:0.973]
Epoch [39/120    avg_loss:0.030, val_acc:0.976]
Epoch [40/120    avg_loss:0.028, val_acc:0.976]
Epoch [41/120    avg_loss:0.020, val_acc:0.973]
Epoch [42/120    avg_loss:0.025, val_acc:0.968]
Epoch [43/120    avg_loss:0.025, val_acc:0.974]
Epoch [44/120    avg_loss:0.018, val_acc:0.980]
Epoch [45/120    avg_loss:0.018, val_acc:0.980]
Epoch [46/120    avg_loss:0.026, val_acc:0.977]
Epoch [47/120    avg_loss:0.030, val_acc:0.970]
Epoch [48/120    avg_loss:0.024, val_acc:0.963]
Epoch [49/120    avg_loss:0.022, val_acc:0.955]
Epoch [50/120    avg_loss:0.018, val_acc:0.974]
Epoch [51/120    avg_loss:0.019, val_acc:0.981]
Epoch [52/120    avg_loss:0.018, val_acc:0.979]
Epoch [53/120    avg_loss:0.011, val_acc:0.980]
Epoch [54/120    avg_loss:0.010, val_acc:0.980]
Epoch [55/120    avg_loss:0.010, val_acc:0.980]
Epoch [56/120    avg_loss:0.013, val_acc:0.973]
Epoch [57/120    avg_loss:0.023, val_acc:0.966]
Epoch [58/120    avg_loss:0.017, val_acc:0.978]
Epoch [59/120    avg_loss:0.015, val_acc:0.975]
Epoch [60/120    avg_loss:0.013, val_acc:0.971]
Epoch [61/120    avg_loss:0.016, val_acc:0.979]
Epoch [62/120    avg_loss:0.010, val_acc:0.982]
Epoch [63/120    avg_loss:0.009, val_acc:0.983]
Epoch [64/120    avg_loss:0.008, val_acc:0.983]
Epoch [65/120    avg_loss:0.005, val_acc:0.981]
Epoch [66/120    avg_loss:0.008, val_acc:0.979]
Epoch [67/120    avg_loss:0.007, val_acc:0.973]
Epoch [68/120    avg_loss:0.022, val_acc:0.975]
Epoch [69/120    avg_loss:0.010, val_acc:0.978]
Epoch [70/120    avg_loss:0.015, val_acc:0.971]
Epoch [71/120    avg_loss:0.015, val_acc:0.970]
Epoch [72/120    avg_loss:0.021, val_acc:0.977]
Epoch [73/120    avg_loss:0.029, val_acc:0.981]
Epoch [74/120    avg_loss:0.020, val_acc:0.974]
Epoch [75/120    avg_loss:0.008, val_acc:0.980]
Epoch [76/120    avg_loss:0.007, val_acc:0.983]
Epoch [77/120    avg_loss:0.008, val_acc:0.982]
Epoch [78/120    avg_loss:0.005, val_acc:0.984]
Epoch [79/120    avg_loss:0.005, val_acc:0.986]
Epoch [80/120    avg_loss:0.005, val_acc:0.988]
Epoch [81/120    avg_loss:0.005, val_acc:0.984]
Epoch [82/120    avg_loss:0.005, val_acc:0.984]
Epoch [83/120    avg_loss:0.004, val_acc:0.983]
Epoch [84/120    avg_loss:0.007, val_acc:0.982]
Epoch [85/120    avg_loss:0.004, val_acc:0.983]
Epoch [86/120    avg_loss:0.005, val_acc:0.976]
Epoch [87/120    avg_loss:0.004, val_acc:0.982]
Epoch [88/120    avg_loss:0.004, val_acc:0.984]
Epoch [89/120    avg_loss:0.008, val_acc:0.979]
Epoch [90/120    avg_loss:0.008, val_acc:0.978]
Epoch [91/120    avg_loss:0.006, val_acc:0.982]
Epoch [92/120    avg_loss:0.004, val_acc:0.985]
Epoch [93/120    avg_loss:0.004, val_acc:0.986]
Epoch [94/120    avg_loss:0.004, val_acc:0.986]
Epoch [95/120    avg_loss:0.003, val_acc:0.986]
Epoch [96/120    avg_loss:0.004, val_acc:0.986]
Epoch [97/120    avg_loss:0.003, val_acc:0.988]
Epoch [98/120    avg_loss:0.004, val_acc:0.988]
Epoch [99/120    avg_loss:0.003, val_acc:0.988]
Epoch [100/120    avg_loss:0.003, val_acc:0.988]
Epoch [101/120    avg_loss:0.003, val_acc:0.988]
Epoch [102/120    avg_loss:0.004, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.988]
Epoch [104/120    avg_loss:0.003, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.003, val_acc:0.988]
Epoch [107/120    avg_loss:0.003, val_acc:0.988]
Epoch [108/120    avg_loss:0.003, val_acc:0.986]
Epoch [109/120    avg_loss:0.002, val_acc:0.986]
Epoch [110/120    avg_loss:0.003, val_acc:0.985]
Epoch [111/120    avg_loss:0.004, val_acc:0.986]
Epoch [112/120    avg_loss:0.003, val_acc:0.986]
Epoch [113/120    avg_loss:0.003, val_acc:0.986]
Epoch [114/120    avg_loss:0.003, val_acc:0.986]
Epoch [115/120    avg_loss:0.003, val_acc:0.986]
Epoch [116/120    avg_loss:0.003, val_acc:0.986]
Epoch [117/120    avg_loss:0.003, val_acc:0.986]
Epoch [118/120    avg_loss:0.003, val_acc:0.986]
Epoch [119/120    avg_loss:0.003, val_acc:0.986]
Epoch [120/120    avg_loss:0.004, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1254    6   15    0    0    0    0    0    2    8    0    0
     0    0    0]
 [   0    0    0  738    7    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  856   16    0    0
     3    0    0]
 [   0    0   13    0    0    0    0    0    0    0    8 2172   17    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0    7  522    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    76  265    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.65853658536585

F1 scores:
[       nan 0.98765432 0.98237368 0.98795181 0.94854586 0.99653979
 0.99469295 1.         0.99883586 1.         0.98221457 0.98436438
 0.96935933 1.         0.95822677 0.84260731 0.97590361]

Kappa:
0.9732972933900187
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3321aba748>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.494, val_acc:0.417]
Epoch [2/120    avg_loss:1.937, val_acc:0.584]
Epoch [3/120    avg_loss:1.621, val_acc:0.646]
Epoch [4/120    avg_loss:1.450, val_acc:0.646]
Epoch [5/120    avg_loss:1.196, val_acc:0.703]
Epoch [6/120    avg_loss:1.034, val_acc:0.765]
Epoch [7/120    avg_loss:0.904, val_acc:0.766]
Epoch [8/120    avg_loss:0.795, val_acc:0.764]
Epoch [9/120    avg_loss:0.678, val_acc:0.821]
Epoch [10/120    avg_loss:0.590, val_acc:0.808]
Epoch [11/120    avg_loss:0.471, val_acc:0.850]
Epoch [12/120    avg_loss:0.387, val_acc:0.849]
Epoch [13/120    avg_loss:0.372, val_acc:0.839]
Epoch [14/120    avg_loss:0.311, val_acc:0.872]
Epoch [15/120    avg_loss:0.260, val_acc:0.865]
Epoch [16/120    avg_loss:0.250, val_acc:0.852]
Epoch [17/120    avg_loss:0.232, val_acc:0.875]
Epoch [18/120    avg_loss:0.204, val_acc:0.901]
Epoch [19/120    avg_loss:0.161, val_acc:0.926]
Epoch [20/120    avg_loss:0.135, val_acc:0.925]
Epoch [21/120    avg_loss:0.102, val_acc:0.931]
Epoch [22/120    avg_loss:0.113, val_acc:0.933]
Epoch [23/120    avg_loss:0.111, val_acc:0.936]
Epoch [24/120    avg_loss:0.083, val_acc:0.957]
Epoch [25/120    avg_loss:0.122, val_acc:0.939]
Epoch [26/120    avg_loss:0.107, val_acc:0.941]
Epoch [27/120    avg_loss:0.096, val_acc:0.946]
Epoch [28/120    avg_loss:0.104, val_acc:0.941]
Epoch [29/120    avg_loss:0.107, val_acc:0.958]
Epoch [30/120    avg_loss:0.104, val_acc:0.946]
Epoch [31/120    avg_loss:0.098, val_acc:0.955]
Epoch [32/120    avg_loss:0.059, val_acc:0.965]
Epoch [33/120    avg_loss:0.051, val_acc:0.938]
Epoch [34/120    avg_loss:0.044, val_acc:0.968]
Epoch [35/120    avg_loss:0.038, val_acc:0.976]
Epoch [36/120    avg_loss:0.042, val_acc:0.968]
Epoch [37/120    avg_loss:0.047, val_acc:0.961]
Epoch [38/120    avg_loss:0.041, val_acc:0.963]
Epoch [39/120    avg_loss:0.031, val_acc:0.968]
Epoch [40/120    avg_loss:0.027, val_acc:0.968]
Epoch [41/120    avg_loss:0.022, val_acc:0.968]
Epoch [42/120    avg_loss:0.029, val_acc:0.967]
Epoch [43/120    avg_loss:0.031, val_acc:0.976]
Epoch [44/120    avg_loss:0.025, val_acc:0.972]
Epoch [45/120    avg_loss:0.020, val_acc:0.966]
Epoch [46/120    avg_loss:0.018, val_acc:0.975]
Epoch [47/120    avg_loss:0.016, val_acc:0.978]
Epoch [48/120    avg_loss:0.017, val_acc:0.975]
Epoch [49/120    avg_loss:0.024, val_acc:0.971]
Epoch [50/120    avg_loss:0.018, val_acc:0.978]
Epoch [51/120    avg_loss:0.021, val_acc:0.955]
Epoch [52/120    avg_loss:0.040, val_acc:0.972]
Epoch [53/120    avg_loss:0.031, val_acc:0.958]
Epoch [54/120    avg_loss:0.032, val_acc:0.974]
Epoch [55/120    avg_loss:0.017, val_acc:0.976]
Epoch [56/120    avg_loss:0.011, val_acc:0.971]
Epoch [57/120    avg_loss:0.011, val_acc:0.971]
Epoch [58/120    avg_loss:0.013, val_acc:0.978]
Epoch [59/120    avg_loss:0.009, val_acc:0.980]
Epoch [60/120    avg_loss:0.008, val_acc:0.979]
Epoch [61/120    avg_loss:0.010, val_acc:0.979]
Epoch [62/120    avg_loss:0.008, val_acc:0.984]
Epoch [63/120    avg_loss:0.009, val_acc:0.980]
Epoch [64/120    avg_loss:0.006, val_acc:0.980]
Epoch [65/120    avg_loss:0.010, val_acc:0.979]
Epoch [66/120    avg_loss:0.007, val_acc:0.980]
Epoch [67/120    avg_loss:0.008, val_acc:0.981]
Epoch [68/120    avg_loss:0.007, val_acc:0.984]
Epoch [69/120    avg_loss:0.006, val_acc:0.983]
Epoch [70/120    avg_loss:0.006, val_acc:0.983]
Epoch [71/120    avg_loss:0.009, val_acc:0.981]
Epoch [72/120    avg_loss:0.006, val_acc:0.981]
Epoch [73/120    avg_loss:0.005, val_acc:0.979]
Epoch [74/120    avg_loss:0.013, val_acc:0.978]
Epoch [75/120    avg_loss:0.009, val_acc:0.984]
Epoch [76/120    avg_loss:0.008, val_acc:0.983]
Epoch [77/120    avg_loss:0.008, val_acc:0.976]
Epoch [78/120    avg_loss:0.006, val_acc:0.981]
Epoch [79/120    avg_loss:0.007, val_acc:0.980]
Epoch [80/120    avg_loss:0.006, val_acc:0.981]
Epoch [81/120    avg_loss:0.005, val_acc:0.983]
Epoch [82/120    avg_loss:0.004, val_acc:0.983]
Epoch [83/120    avg_loss:0.004, val_acc:0.983]
Epoch [84/120    avg_loss:0.004, val_acc:0.978]
Epoch [85/120    avg_loss:0.006, val_acc:0.979]
Epoch [86/120    avg_loss:0.004, val_acc:0.988]
Epoch [87/120    avg_loss:0.006, val_acc:0.984]
Epoch [88/120    avg_loss:0.005, val_acc:0.981]
Epoch [89/120    avg_loss:0.004, val_acc:0.986]
Epoch [90/120    avg_loss:0.004, val_acc:0.981]
Epoch [91/120    avg_loss:0.004, val_acc:0.973]
Epoch [92/120    avg_loss:0.005, val_acc:0.985]
Epoch [93/120    avg_loss:0.003, val_acc:0.984]
Epoch [94/120    avg_loss:0.003, val_acc:0.979]
Epoch [95/120    avg_loss:0.003, val_acc:0.984]
Epoch [96/120    avg_loss:0.004, val_acc:0.985]
Epoch [97/120    avg_loss:0.006, val_acc:0.977]
Epoch [98/120    avg_loss:0.013, val_acc:0.972]
Epoch [99/120    avg_loss:0.007, val_acc:0.982]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.003, val_acc:0.984]
Epoch [102/120    avg_loss:0.003, val_acc:0.985]
Epoch [103/120    avg_loss:0.003, val_acc:0.984]
Epoch [104/120    avg_loss:0.005, val_acc:0.985]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.004, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.985]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.003, val_acc:0.986]
Epoch [110/120    avg_loss:0.003, val_acc:0.985]
Epoch [111/120    avg_loss:0.003, val_acc:0.985]
Epoch [112/120    avg_loss:0.003, val_acc:0.985]
Epoch [113/120    avg_loss:0.004, val_acc:0.985]
Epoch [114/120    avg_loss:0.003, val_acc:0.986]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.003, val_acc:0.986]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.003, val_acc:0.984]
Epoch [119/120    avg_loss:0.003, val_acc:0.986]
Epoch [120/120    avg_loss:0.003, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    6    2    3    0    0    0    0    3   10    0    0
     0    0    0]
 [   0    0    0  726    4    0    2    0    0    0    4    7    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    0    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    1    0    0    0    0  856   14    0    0
     1    0    0]
 [   0    0    1    1    0    0    0    0    0    0   12 2193    0    1
     0    2    0]
 [   0    0    0    8    0    0    0    0    0    0    0    2  523    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1133    6    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    80  267    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.97289972899729

F1 scores:
[       nan 0.98765432 0.98863191 0.97580645 0.98611111 0.98847926
 0.99848024 1.         1.         1.         0.97828571 0.98850575
 0.98493409 0.99459459 0.96057652 0.8585209  0.98809524]

Kappa:
0.9768661503210282
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3cfb748710>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.516, val_acc:0.325]
Epoch [2/120    avg_loss:2.002, val_acc:0.501]
Epoch [3/120    avg_loss:1.701, val_acc:0.603]
Epoch [4/120    avg_loss:1.408, val_acc:0.615]
Epoch [5/120    avg_loss:1.305, val_acc:0.596]
Epoch [6/120    avg_loss:1.116, val_acc:0.718]
Epoch [7/120    avg_loss:0.894, val_acc:0.738]
Epoch [8/120    avg_loss:0.767, val_acc:0.772]
Epoch [9/120    avg_loss:0.580, val_acc:0.816]
Epoch [10/120    avg_loss:0.466, val_acc:0.852]
Epoch [11/120    avg_loss:0.490, val_acc:0.861]
Epoch [12/120    avg_loss:0.397, val_acc:0.885]
Epoch [13/120    avg_loss:0.382, val_acc:0.872]
Epoch [14/120    avg_loss:0.385, val_acc:0.891]
Epoch [15/120    avg_loss:0.279, val_acc:0.903]
Epoch [16/120    avg_loss:0.182, val_acc:0.934]
Epoch [17/120    avg_loss:0.180, val_acc:0.902]
Epoch [18/120    avg_loss:0.164, val_acc:0.924]
Epoch [19/120    avg_loss:0.166, val_acc:0.910]
Epoch [20/120    avg_loss:0.165, val_acc:0.949]
Epoch [21/120    avg_loss:0.118, val_acc:0.907]
Epoch [22/120    avg_loss:0.119, val_acc:0.943]
Epoch [23/120    avg_loss:0.082, val_acc:0.947]
Epoch [24/120    avg_loss:0.114, val_acc:0.874]
Epoch [25/120    avg_loss:0.110, val_acc:0.939]
Epoch [26/120    avg_loss:0.079, val_acc:0.946]
Epoch [27/120    avg_loss:0.057, val_acc:0.956]
Epoch [28/120    avg_loss:0.051, val_acc:0.958]
Epoch [29/120    avg_loss:0.074, val_acc:0.951]
Epoch [30/120    avg_loss:0.071, val_acc:0.926]
Epoch [31/120    avg_loss:0.088, val_acc:0.949]
Epoch [32/120    avg_loss:0.076, val_acc:0.959]
Epoch [33/120    avg_loss:0.056, val_acc:0.955]
Epoch [34/120    avg_loss:0.098, val_acc:0.947]
Epoch [35/120    avg_loss:0.057, val_acc:0.943]
Epoch [36/120    avg_loss:0.058, val_acc:0.958]
Epoch [37/120    avg_loss:0.036, val_acc:0.966]
Epoch [38/120    avg_loss:0.030, val_acc:0.968]
Epoch [39/120    avg_loss:0.032, val_acc:0.959]
Epoch [40/120    avg_loss:0.021, val_acc:0.965]
Epoch [41/120    avg_loss:0.022, val_acc:0.978]
Epoch [42/120    avg_loss:0.024, val_acc:0.961]
Epoch [43/120    avg_loss:0.026, val_acc:0.974]
Epoch [44/120    avg_loss:0.046, val_acc:0.977]
Epoch [45/120    avg_loss:0.032, val_acc:0.970]
Epoch [46/120    avg_loss:0.219, val_acc:0.880]
Epoch [47/120    avg_loss:0.193, val_acc:0.887]
Epoch [48/120    avg_loss:0.085, val_acc:0.942]
Epoch [49/120    avg_loss:0.051, val_acc:0.946]
Epoch [50/120    avg_loss:0.072, val_acc:0.936]
Epoch [51/120    avg_loss:0.044, val_acc:0.953]
Epoch [52/120    avg_loss:0.034, val_acc:0.967]
Epoch [53/120    avg_loss:0.046, val_acc:0.934]
Epoch [54/120    avg_loss:0.043, val_acc:0.963]
Epoch [55/120    avg_loss:0.029, val_acc:0.966]
Epoch [56/120    avg_loss:0.025, val_acc:0.969]
Epoch [57/120    avg_loss:0.021, val_acc:0.971]
Epoch [58/120    avg_loss:0.020, val_acc:0.969]
Epoch [59/120    avg_loss:0.019, val_acc:0.968]
Epoch [60/120    avg_loss:0.019, val_acc:0.968]
Epoch [61/120    avg_loss:0.020, val_acc:0.968]
Epoch [62/120    avg_loss:0.016, val_acc:0.971]
Epoch [63/120    avg_loss:0.016, val_acc:0.970]
Epoch [64/120    avg_loss:0.014, val_acc:0.972]
Epoch [65/120    avg_loss:0.013, val_acc:0.972]
Epoch [66/120    avg_loss:0.017, val_acc:0.972]
Epoch [67/120    avg_loss:0.012, val_acc:0.972]
Epoch [68/120    avg_loss:0.016, val_acc:0.972]
Epoch [69/120    avg_loss:0.019, val_acc:0.972]
Epoch [70/120    avg_loss:0.016, val_acc:0.972]
Epoch [71/120    avg_loss:0.014, val_acc:0.972]
Epoch [72/120    avg_loss:0.015, val_acc:0.972]
Epoch [73/120    avg_loss:0.015, val_acc:0.972]
Epoch [74/120    avg_loss:0.012, val_acc:0.972]
Epoch [75/120    avg_loss:0.014, val_acc:0.972]
Epoch [76/120    avg_loss:0.012, val_acc:0.972]
Epoch [77/120    avg_loss:0.016, val_acc:0.972]
Epoch [78/120    avg_loss:0.015, val_acc:0.972]
Epoch [79/120    avg_loss:0.017, val_acc:0.972]
Epoch [80/120    avg_loss:0.016, val_acc:0.972]
Epoch [81/120    avg_loss:0.013, val_acc:0.972]
Epoch [82/120    avg_loss:0.016, val_acc:0.972]
Epoch [83/120    avg_loss:0.016, val_acc:0.972]
Epoch [84/120    avg_loss:0.013, val_acc:0.972]
Epoch [85/120    avg_loss:0.012, val_acc:0.972]
Epoch [86/120    avg_loss:0.014, val_acc:0.972]
Epoch [87/120    avg_loss:0.017, val_acc:0.972]
Epoch [88/120    avg_loss:0.014, val_acc:0.972]
Epoch [89/120    avg_loss:0.017, val_acc:0.972]
Epoch [90/120    avg_loss:0.012, val_acc:0.972]
Epoch [91/120    avg_loss:0.014, val_acc:0.972]
Epoch [92/120    avg_loss:0.015, val_acc:0.972]
Epoch [93/120    avg_loss:0.016, val_acc:0.972]
Epoch [94/120    avg_loss:0.012, val_acc:0.972]
Epoch [95/120    avg_loss:0.012, val_acc:0.972]
Epoch [96/120    avg_loss:0.016, val_acc:0.972]
Epoch [97/120    avg_loss:0.012, val_acc:0.972]
Epoch [98/120    avg_loss:0.013, val_acc:0.972]
Epoch [99/120    avg_loss:0.012, val_acc:0.972]
Epoch [100/120    avg_loss:0.014, val_acc:0.972]
Epoch [101/120    avg_loss:0.016, val_acc:0.972]
Epoch [102/120    avg_loss:0.015, val_acc:0.972]
Epoch [103/120    avg_loss:0.016, val_acc:0.972]
Epoch [104/120    avg_loss:0.014, val_acc:0.972]
Epoch [105/120    avg_loss:0.017, val_acc:0.972]
Epoch [106/120    avg_loss:0.014, val_acc:0.972]
Epoch [107/120    avg_loss:0.013, val_acc:0.972]
Epoch [108/120    avg_loss:0.015, val_acc:0.972]
Epoch [109/120    avg_loss:0.015, val_acc:0.972]
Epoch [110/120    avg_loss:0.014, val_acc:0.972]
Epoch [111/120    avg_loss:0.017, val_acc:0.972]
Epoch [112/120    avg_loss:0.016, val_acc:0.972]
Epoch [113/120    avg_loss:0.016, val_acc:0.972]
Epoch [114/120    avg_loss:0.016, val_acc:0.972]
Epoch [115/120    avg_loss:0.017, val_acc:0.972]
Epoch [116/120    avg_loss:0.016, val_acc:0.972]
Epoch [117/120    avg_loss:0.015, val_acc:0.972]
Epoch [118/120    avg_loss:0.017, val_acc:0.972]
Epoch [119/120    avg_loss:0.013, val_acc:0.972]
Epoch [120/120    avg_loss:0.018, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1243   14    0    0    0    0    0    2    3   20    3    0
     0    0    0]
 [   0    0    0  719   15    0    0    0    0    1    0    7    4    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    1    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  425    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0  837   33    1    0
     0    1    0]
 [   0    0    8    0    0    0    0    0    0    5    4 2170   23    0
     0    0    0]
 [   0    0    0    6    4    0    0    0    0    0    0    0  519    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    83  262    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.99728997289972

F1 scores:
[       nan 0.95348837 0.97912564 0.96769852 0.95730337 0.99421965
 0.99619193 0.98039216 0.99415205 0.76595745 0.97382199 0.97725737
 0.9558011  1.         0.95700298 0.83306836 0.98224852]

Kappa:
0.9657475946945148
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6ac1850748>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.482, val_acc:0.412]
Epoch [2/120    avg_loss:1.985, val_acc:0.466]
Epoch [3/120    avg_loss:1.666, val_acc:0.537]
Epoch [4/120    avg_loss:1.466, val_acc:0.657]
Epoch [5/120    avg_loss:1.179, val_acc:0.689]
Epoch [6/120    avg_loss:1.083, val_acc:0.698]
Epoch [7/120    avg_loss:0.920, val_acc:0.747]
Epoch [8/120    avg_loss:0.688, val_acc:0.802]
Epoch [9/120    avg_loss:0.672, val_acc:0.809]
Epoch [10/120    avg_loss:0.628, val_acc:0.700]
Epoch [11/120    avg_loss:0.574, val_acc:0.818]
Epoch [12/120    avg_loss:0.491, val_acc:0.858]
Epoch [13/120    avg_loss:0.385, val_acc:0.879]
Epoch [14/120    avg_loss:0.355, val_acc:0.855]
Epoch [15/120    avg_loss:0.298, val_acc:0.864]
Epoch [16/120    avg_loss:0.261, val_acc:0.919]
Epoch [17/120    avg_loss:0.175, val_acc:0.896]
Epoch [18/120    avg_loss:0.254, val_acc:0.902]
Epoch [19/120    avg_loss:0.201, val_acc:0.928]
Epoch [20/120    avg_loss:0.143, val_acc:0.922]
Epoch [21/120    avg_loss:0.122, val_acc:0.932]
Epoch [22/120    avg_loss:0.127, val_acc:0.903]
Epoch [23/120    avg_loss:0.117, val_acc:0.948]
Epoch [24/120    avg_loss:0.100, val_acc:0.940]
Epoch [25/120    avg_loss:0.095, val_acc:0.933]
Epoch [26/120    avg_loss:0.100, val_acc:0.953]
Epoch [27/120    avg_loss:0.067, val_acc:0.964]
Epoch [28/120    avg_loss:0.063, val_acc:0.957]
Epoch [29/120    avg_loss:0.079, val_acc:0.935]
Epoch [30/120    avg_loss:0.097, val_acc:0.935]
Epoch [31/120    avg_loss:0.090, val_acc:0.968]
Epoch [32/120    avg_loss:0.077, val_acc:0.960]
Epoch [33/120    avg_loss:0.072, val_acc:0.964]
Epoch [34/120    avg_loss:0.048, val_acc:0.950]
Epoch [35/120    avg_loss:0.043, val_acc:0.953]
Epoch [36/120    avg_loss:0.047, val_acc:0.947]
Epoch [37/120    avg_loss:0.055, val_acc:0.965]
Epoch [38/120    avg_loss:0.037, val_acc:0.957]
Epoch [39/120    avg_loss:0.032, val_acc:0.975]
Epoch [40/120    avg_loss:0.028, val_acc:0.972]
Epoch [41/120    avg_loss:0.022, val_acc:0.975]
Epoch [42/120    avg_loss:0.031, val_acc:0.965]
Epoch [43/120    avg_loss:0.038, val_acc:0.967]
Epoch [44/120    avg_loss:0.033, val_acc:0.960]
Epoch [45/120    avg_loss:0.029, val_acc:0.967]
Epoch [46/120    avg_loss:0.020, val_acc:0.969]
Epoch [47/120    avg_loss:0.012, val_acc:0.975]
Epoch [48/120    avg_loss:0.012, val_acc:0.973]
Epoch [49/120    avg_loss:0.010, val_acc:0.978]
Epoch [50/120    avg_loss:0.013, val_acc:0.977]
Epoch [51/120    avg_loss:0.012, val_acc:0.979]
Epoch [52/120    avg_loss:0.010, val_acc:0.982]
Epoch [53/120    avg_loss:0.020, val_acc:0.941]
Epoch [54/120    avg_loss:0.025, val_acc:0.971]
Epoch [55/120    avg_loss:0.016, val_acc:0.969]
Epoch [56/120    avg_loss:0.017, val_acc:0.971]
Epoch [57/120    avg_loss:0.010, val_acc:0.976]
Epoch [58/120    avg_loss:0.009, val_acc:0.971]
Epoch [59/120    avg_loss:0.011, val_acc:0.975]
Epoch [60/120    avg_loss:0.012, val_acc:0.977]
Epoch [61/120    avg_loss:0.012, val_acc:0.976]
Epoch [62/120    avg_loss:0.011, val_acc:0.976]
Epoch [63/120    avg_loss:0.008, val_acc:0.973]
Epoch [64/120    avg_loss:0.009, val_acc:0.979]
Epoch [65/120    avg_loss:0.006, val_acc:0.978]
Epoch [66/120    avg_loss:0.006, val_acc:0.982]
Epoch [67/120    avg_loss:0.006, val_acc:0.982]
Epoch [68/120    avg_loss:0.005, val_acc:0.982]
Epoch [69/120    avg_loss:0.006, val_acc:0.982]
Epoch [70/120    avg_loss:0.005, val_acc:0.982]
Epoch [71/120    avg_loss:0.006, val_acc:0.982]
Epoch [72/120    avg_loss:0.007, val_acc:0.982]
Epoch [73/120    avg_loss:0.005, val_acc:0.982]
Epoch [74/120    avg_loss:0.005, val_acc:0.982]
Epoch [75/120    avg_loss:0.005, val_acc:0.982]
Epoch [76/120    avg_loss:0.006, val_acc:0.982]
Epoch [77/120    avg_loss:0.004, val_acc:0.982]
Epoch [78/120    avg_loss:0.005, val_acc:0.982]
Epoch [79/120    avg_loss:0.004, val_acc:0.981]
Epoch [80/120    avg_loss:0.005, val_acc:0.981]
Epoch [81/120    avg_loss:0.005, val_acc:0.982]
Epoch [82/120    avg_loss:0.005, val_acc:0.982]
Epoch [83/120    avg_loss:0.006, val_acc:0.982]
Epoch [84/120    avg_loss:0.005, val_acc:0.982]
Epoch [85/120    avg_loss:0.006, val_acc:0.982]
Epoch [86/120    avg_loss:0.004, val_acc:0.982]
Epoch [87/120    avg_loss:0.005, val_acc:0.982]
Epoch [88/120    avg_loss:0.005, val_acc:0.983]
Epoch [89/120    avg_loss:0.004, val_acc:0.982]
Epoch [90/120    avg_loss:0.005, val_acc:0.982]
Epoch [91/120    avg_loss:0.005, val_acc:0.980]
Epoch [92/120    avg_loss:0.006, val_acc:0.981]
Epoch [93/120    avg_loss:0.005, val_acc:0.980]
Epoch [94/120    avg_loss:0.004, val_acc:0.981]
Epoch [95/120    avg_loss:0.005, val_acc:0.982]
Epoch [96/120    avg_loss:0.004, val_acc:0.982]
Epoch [97/120    avg_loss:0.004, val_acc:0.982]
Epoch [98/120    avg_loss:0.004, val_acc:0.983]
Epoch [99/120    avg_loss:0.004, val_acc:0.983]
Epoch [100/120    avg_loss:0.004, val_acc:0.982]
Epoch [101/120    avg_loss:0.004, val_acc:0.982]
Epoch [102/120    avg_loss:0.004, val_acc:0.982]
Epoch [103/120    avg_loss:0.004, val_acc:0.983]
Epoch [104/120    avg_loss:0.003, val_acc:0.982]
Epoch [105/120    avg_loss:0.004, val_acc:0.982]
Epoch [106/120    avg_loss:0.004, val_acc:0.983]
Epoch [107/120    avg_loss:0.005, val_acc:0.983]
Epoch [108/120    avg_loss:0.005, val_acc:0.982]
Epoch [109/120    avg_loss:0.004, val_acc:0.983]
Epoch [110/120    avg_loss:0.005, val_acc:0.983]
Epoch [111/120    avg_loss:0.004, val_acc:0.982]
Epoch [112/120    avg_loss:0.004, val_acc:0.982]
Epoch [113/120    avg_loss:0.004, val_acc:0.982]
Epoch [114/120    avg_loss:0.005, val_acc:0.981]
Epoch [115/120    avg_loss:0.004, val_acc:0.981]
Epoch [116/120    avg_loss:0.004, val_acc:0.981]
Epoch [117/120    avg_loss:0.004, val_acc:0.981]
Epoch [118/120    avg_loss:0.005, val_acc:0.983]
Epoch [119/120    avg_loss:0.005, val_acc:0.981]
Epoch [120/120    avg_loss:0.004, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1259    3    5    2    0    0    0    0    3   12    1    0
     0    0    0]
 [   0    0    0  723    1    0    1    0    0    1    1    0   20    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    2    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    1    0    0    0  855   13    0    0
     1    1    0]
 [   0    0   14    0    0    0   13    0    0    3   19 2151   10    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    0    0  527    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    60  287    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.64769647696477

F1 scores:
[       nan 0.98765432 0.98244245 0.9796748  0.98611111 0.99076212
 0.98718915 0.96153846 1.         0.87804878 0.97547062 0.98062457
 0.96431839 1.         0.96740995 0.88580247 0.98224852]

Kappa:
0.9731907949980418
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f38320c96d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.435, val_acc:0.438]
Epoch [2/120    avg_loss:1.931, val_acc:0.551]
Epoch [3/120    avg_loss:1.650, val_acc:0.607]
Epoch [4/120    avg_loss:1.463, val_acc:0.615]
Epoch [5/120    avg_loss:1.318, val_acc:0.704]
Epoch [6/120    avg_loss:1.137, val_acc:0.707]
Epoch [7/120    avg_loss:1.045, val_acc:0.766]
Epoch [8/120    avg_loss:0.978, val_acc:0.795]
Epoch [9/120    avg_loss:0.859, val_acc:0.806]
Epoch [10/120    avg_loss:0.686, val_acc:0.792]
Epoch [11/120    avg_loss:0.637, val_acc:0.835]
Epoch [12/120    avg_loss:0.600, val_acc:0.787]
Epoch [13/120    avg_loss:0.474, val_acc:0.832]
Epoch [14/120    avg_loss:0.418, val_acc:0.896]
Epoch [15/120    avg_loss:0.338, val_acc:0.881]
Epoch [16/120    avg_loss:0.309, val_acc:0.887]
Epoch [17/120    avg_loss:0.242, val_acc:0.903]
Epoch [18/120    avg_loss:0.224, val_acc:0.872]
Epoch [19/120    avg_loss:0.193, val_acc:0.928]
Epoch [20/120    avg_loss:0.147, val_acc:0.930]
Epoch [21/120    avg_loss:0.128, val_acc:0.940]
Epoch [22/120    avg_loss:0.130, val_acc:0.946]
Epoch [23/120    avg_loss:0.142, val_acc:0.934]
Epoch [24/120    avg_loss:0.110, val_acc:0.943]
Epoch [25/120    avg_loss:0.104, val_acc:0.946]
Epoch [26/120    avg_loss:0.086, val_acc:0.952]
Epoch [27/120    avg_loss:0.081, val_acc:0.961]
Epoch [28/120    avg_loss:0.072, val_acc:0.971]
Epoch [29/120    avg_loss:0.067, val_acc:0.966]
Epoch [30/120    avg_loss:0.058, val_acc:0.958]
Epoch [31/120    avg_loss:0.059, val_acc:0.970]
Epoch [32/120    avg_loss:0.057, val_acc:0.976]
Epoch [33/120    avg_loss:0.108, val_acc:0.909]
Epoch [34/120    avg_loss:0.170, val_acc:0.935]
Epoch [35/120    avg_loss:0.108, val_acc:0.942]
Epoch [36/120    avg_loss:0.092, val_acc:0.945]
Epoch [37/120    avg_loss:0.082, val_acc:0.950]
Epoch [38/120    avg_loss:0.075, val_acc:0.953]
Epoch [39/120    avg_loss:0.058, val_acc:0.974]
Epoch [40/120    avg_loss:0.037, val_acc:0.954]
Epoch [41/120    avg_loss:0.030, val_acc:0.974]
Epoch [42/120    avg_loss:0.031, val_acc:0.979]
Epoch [43/120    avg_loss:0.022, val_acc:0.971]
Epoch [44/120    avg_loss:0.025, val_acc:0.983]
Epoch [45/120    avg_loss:0.025, val_acc:0.973]
Epoch [46/120    avg_loss:0.021, val_acc:0.973]
Epoch [47/120    avg_loss:0.018, val_acc:0.970]
Epoch [48/120    avg_loss:0.022, val_acc:0.974]
Epoch [49/120    avg_loss:0.018, val_acc:0.978]
Epoch [50/120    avg_loss:0.037, val_acc:0.953]
Epoch [51/120    avg_loss:0.042, val_acc:0.968]
Epoch [52/120    avg_loss:0.040, val_acc:0.974]
Epoch [53/120    avg_loss:0.019, val_acc:0.976]
Epoch [54/120    avg_loss:0.012, val_acc:0.979]
Epoch [55/120    avg_loss:0.012, val_acc:0.981]
Epoch [56/120    avg_loss:0.013, val_acc:0.983]
Epoch [57/120    avg_loss:0.014, val_acc:0.982]
Epoch [58/120    avg_loss:0.011, val_acc:0.983]
Epoch [59/120    avg_loss:0.010, val_acc:0.985]
Epoch [60/120    avg_loss:0.009, val_acc:0.981]
Epoch [61/120    avg_loss:0.010, val_acc:0.982]
Epoch [62/120    avg_loss:0.010, val_acc:0.985]
Epoch [63/120    avg_loss:0.012, val_acc:0.982]
Epoch [64/120    avg_loss:0.013, val_acc:0.981]
Epoch [65/120    avg_loss:0.009, val_acc:0.986]
Epoch [66/120    avg_loss:0.010, val_acc:0.978]
Epoch [67/120    avg_loss:0.010, val_acc:0.982]
Epoch [68/120    avg_loss:0.008, val_acc:0.980]
Epoch [69/120    avg_loss:0.011, val_acc:0.982]
Epoch [70/120    avg_loss:0.006, val_acc:0.983]
Epoch [71/120    avg_loss:0.012, val_acc:0.980]
Epoch [72/120    avg_loss:0.013, val_acc:0.985]
Epoch [73/120    avg_loss:0.008, val_acc:0.982]
Epoch [74/120    avg_loss:0.008, val_acc:0.982]
Epoch [75/120    avg_loss:0.007, val_acc:0.984]
Epoch [76/120    avg_loss:0.007, val_acc:0.986]
Epoch [77/120    avg_loss:0.010, val_acc:0.982]
Epoch [78/120    avg_loss:0.009, val_acc:0.985]
Epoch [79/120    avg_loss:0.007, val_acc:0.980]
Epoch [80/120    avg_loss:0.007, val_acc:0.981]
Epoch [81/120    avg_loss:0.007, val_acc:0.984]
Epoch [82/120    avg_loss:0.006, val_acc:0.986]
Epoch [83/120    avg_loss:0.004, val_acc:0.985]
Epoch [84/120    avg_loss:0.004, val_acc:0.984]
Epoch [85/120    avg_loss:0.003, val_acc:0.982]
Epoch [86/120    avg_loss:0.005, val_acc:0.988]
Epoch [87/120    avg_loss:0.005, val_acc:0.984]
Epoch [88/120    avg_loss:0.013, val_acc:0.983]
Epoch [89/120    avg_loss:0.006, val_acc:0.989]
Epoch [90/120    avg_loss:0.004, val_acc:0.986]
Epoch [91/120    avg_loss:0.003, val_acc:0.986]
Epoch [92/120    avg_loss:0.003, val_acc:0.986]
Epoch [93/120    avg_loss:0.006, val_acc:0.979]
Epoch [94/120    avg_loss:0.005, val_acc:0.978]
Epoch [95/120    avg_loss:0.006, val_acc:0.980]
Epoch [96/120    avg_loss:0.004, val_acc:0.989]
Epoch [97/120    avg_loss:0.003, val_acc:0.986]
Epoch [98/120    avg_loss:0.003, val_acc:0.989]
Epoch [99/120    avg_loss:0.004, val_acc:0.988]
Epoch [100/120    avg_loss:0.003, val_acc:0.985]
Epoch [101/120    avg_loss:0.003, val_acc:0.986]
Epoch [102/120    avg_loss:0.003, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.988]
Epoch [104/120    avg_loss:0.003, val_acc:0.988]
Epoch [105/120    avg_loss:0.003, val_acc:0.988]
Epoch [106/120    avg_loss:0.003, val_acc:0.983]
Epoch [107/120    avg_loss:0.003, val_acc:0.986]
Epoch [108/120    avg_loss:0.003, val_acc:0.984]
Epoch [109/120    avg_loss:0.002, val_acc:0.988]
Epoch [110/120    avg_loss:0.003, val_acc:0.986]
Epoch [111/120    avg_loss:0.002, val_acc:0.989]
Epoch [112/120    avg_loss:0.002, val_acc:0.988]
Epoch [113/120    avg_loss:0.003, val_acc:0.983]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.006, val_acc:0.985]
Epoch [116/120    avg_loss:0.004, val_acc:0.980]
Epoch [117/120    avg_loss:0.003, val_acc:0.982]
Epoch [118/120    avg_loss:0.003, val_acc:0.984]
Epoch [119/120    avg_loss:0.003, val_acc:0.989]
Epoch [120/120    avg_loss:0.007, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    1    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1262    0    0    0    1    0    0    2    4   16    0    0
     0    0    0]
 [   0    0    0  729    1    2    0    0    0    1    0    5    8    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    3    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   11    0    0    0    0    0    0    0  841   22    0    0
     0    1    0]
 [   0    0   37    0    0    1    0    0    0    1    6 2161    3    1
     0    0    0]
 [   0    0    0    8    0    0    0    0    0    0    1    2  520    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1131    8    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    78  264    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.40921409214093

F1 scores:
[       nan 0.98765432 0.97263969 0.98247978 0.99765808 0.98847926
 0.9939302  0.94339623 1.         0.87804878 0.97394325 0.97849219
 0.97560976 0.99462366 0.96214377 0.85024155 0.98224852]

Kappa:
0.9704400375032869
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2bafa546d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.531, val_acc:0.514]
Epoch [2/120    avg_loss:1.906, val_acc:0.479]
Epoch [3/120    avg_loss:1.622, val_acc:0.642]
Epoch [4/120    avg_loss:1.445, val_acc:0.658]
Epoch [5/120    avg_loss:1.191, val_acc:0.720]
Epoch [6/120    avg_loss:1.052, val_acc:0.734]
Epoch [7/120    avg_loss:0.844, val_acc:0.764]
Epoch [8/120    avg_loss:0.761, val_acc:0.750]
Epoch [9/120    avg_loss:0.702, val_acc:0.766]
Epoch [10/120    avg_loss:0.572, val_acc:0.800]
Epoch [11/120    avg_loss:0.469, val_acc:0.834]
Epoch [12/120    avg_loss:0.414, val_acc:0.827]
Epoch [13/120    avg_loss:0.370, val_acc:0.863]
Epoch [14/120    avg_loss:0.364, val_acc:0.871]
Epoch [15/120    avg_loss:0.270, val_acc:0.835]
Epoch [16/120    avg_loss:0.300, val_acc:0.871]
Epoch [17/120    avg_loss:0.246, val_acc:0.873]
Epoch [18/120    avg_loss:0.236, val_acc:0.894]
Epoch [19/120    avg_loss:0.252, val_acc:0.900]
Epoch [20/120    avg_loss:0.163, val_acc:0.908]
Epoch [21/120    avg_loss:0.176, val_acc:0.886]
Epoch [22/120    avg_loss:0.190, val_acc:0.903]
Epoch [23/120    avg_loss:0.119, val_acc:0.922]
Epoch [24/120    avg_loss:0.105, val_acc:0.910]
Epoch [25/120    avg_loss:0.099, val_acc:0.940]
Epoch [26/120    avg_loss:0.130, val_acc:0.932]
Epoch [27/120    avg_loss:0.204, val_acc:0.916]
Epoch [28/120    avg_loss:0.152, val_acc:0.939]
Epoch [29/120    avg_loss:0.098, val_acc:0.940]
Epoch [30/120    avg_loss:0.071, val_acc:0.933]
Epoch [31/120    avg_loss:0.077, val_acc:0.936]
Epoch [32/120    avg_loss:0.064, val_acc:0.946]
Epoch [33/120    avg_loss:0.054, val_acc:0.956]
Epoch [34/120    avg_loss:0.036, val_acc:0.953]
Epoch [35/120    avg_loss:0.044, val_acc:0.935]
Epoch [36/120    avg_loss:0.045, val_acc:0.958]
Epoch [37/120    avg_loss:0.027, val_acc:0.957]
Epoch [38/120    avg_loss:0.037, val_acc:0.950]
Epoch [39/120    avg_loss:0.054, val_acc:0.953]
Epoch [40/120    avg_loss:0.027, val_acc:0.965]
Epoch [41/120    avg_loss:0.022, val_acc:0.964]
Epoch [42/120    avg_loss:0.021, val_acc:0.970]
Epoch [43/120    avg_loss:0.022, val_acc:0.955]
Epoch [44/120    avg_loss:0.024, val_acc:0.966]
Epoch [45/120    avg_loss:0.026, val_acc:0.959]
Epoch [46/120    avg_loss:0.024, val_acc:0.954]
Epoch [47/120    avg_loss:0.037, val_acc:0.970]
Epoch [48/120    avg_loss:0.022, val_acc:0.969]
Epoch [49/120    avg_loss:0.025, val_acc:0.967]
Epoch [50/120    avg_loss:0.023, val_acc:0.965]
Epoch [51/120    avg_loss:0.018, val_acc:0.969]
Epoch [52/120    avg_loss:0.024, val_acc:0.969]
Epoch [53/120    avg_loss:0.018, val_acc:0.965]
Epoch [54/120    avg_loss:0.018, val_acc:0.963]
Epoch [55/120    avg_loss:0.025, val_acc:0.967]
Epoch [56/120    avg_loss:0.026, val_acc:0.970]
Epoch [57/120    avg_loss:0.030, val_acc:0.963]
Epoch [58/120    avg_loss:0.034, val_acc:0.946]
Epoch [59/120    avg_loss:0.039, val_acc:0.975]
Epoch [60/120    avg_loss:0.044, val_acc:0.966]
Epoch [61/120    avg_loss:0.020, val_acc:0.970]
Epoch [62/120    avg_loss:0.021, val_acc:0.975]
Epoch [63/120    avg_loss:0.011, val_acc:0.971]
Epoch [64/120    avg_loss:0.018, val_acc:0.965]
Epoch [65/120    avg_loss:0.015, val_acc:0.978]
Epoch [66/120    avg_loss:0.022, val_acc:0.972]
Epoch [67/120    avg_loss:0.012, val_acc:0.977]
Epoch [68/120    avg_loss:0.008, val_acc:0.973]
Epoch [69/120    avg_loss:0.008, val_acc:0.976]
Epoch [70/120    avg_loss:0.006, val_acc:0.974]
Epoch [71/120    avg_loss:0.010, val_acc:0.968]
Epoch [72/120    avg_loss:0.008, val_acc:0.969]
Epoch [73/120    avg_loss:0.015, val_acc:0.968]
Epoch [74/120    avg_loss:0.033, val_acc:0.969]
Epoch [75/120    avg_loss:0.021, val_acc:0.959]
Epoch [76/120    avg_loss:0.018, val_acc:0.958]
Epoch [77/120    avg_loss:0.032, val_acc:0.961]
Epoch [78/120    avg_loss:0.027, val_acc:0.974]
Epoch [79/120    avg_loss:0.013, val_acc:0.974]
Epoch [80/120    avg_loss:0.009, val_acc:0.978]
Epoch [81/120    avg_loss:0.010, val_acc:0.977]
Epoch [82/120    avg_loss:0.007, val_acc:0.977]
Epoch [83/120    avg_loss:0.009, val_acc:0.978]
Epoch [84/120    avg_loss:0.007, val_acc:0.976]
Epoch [85/120    avg_loss:0.005, val_acc:0.976]
Epoch [86/120    avg_loss:0.007, val_acc:0.977]
Epoch [87/120    avg_loss:0.008, val_acc:0.976]
Epoch [88/120    avg_loss:0.006, val_acc:0.974]
Epoch [89/120    avg_loss:0.007, val_acc:0.976]
Epoch [90/120    avg_loss:0.006, val_acc:0.974]
Epoch [91/120    avg_loss:0.006, val_acc:0.977]
Epoch [92/120    avg_loss:0.008, val_acc:0.975]
Epoch [93/120    avg_loss:0.006, val_acc:0.974]
Epoch [94/120    avg_loss:0.005, val_acc:0.974]
Epoch [95/120    avg_loss:0.005, val_acc:0.976]
Epoch [96/120    avg_loss:0.006, val_acc:0.977]
Epoch [97/120    avg_loss:0.006, val_acc:0.977]
Epoch [98/120    avg_loss:0.007, val_acc:0.977]
Epoch [99/120    avg_loss:0.006, val_acc:0.977]
Epoch [100/120    avg_loss:0.009, val_acc:0.977]
Epoch [101/120    avg_loss:0.005, val_acc:0.977]
Epoch [102/120    avg_loss:0.007, val_acc:0.977]
Epoch [103/120    avg_loss:0.005, val_acc:0.977]
Epoch [104/120    avg_loss:0.005, val_acc:0.977]
Epoch [105/120    avg_loss:0.005, val_acc:0.976]
Epoch [106/120    avg_loss:0.005, val_acc:0.977]
Epoch [107/120    avg_loss:0.006, val_acc:0.976]
Epoch [108/120    avg_loss:0.005, val_acc:0.976]
Epoch [109/120    avg_loss:0.005, val_acc:0.976]
Epoch [110/120    avg_loss:0.006, val_acc:0.976]
Epoch [111/120    avg_loss:0.005, val_acc:0.976]
Epoch [112/120    avg_loss:0.007, val_acc:0.976]
Epoch [113/120    avg_loss:0.007, val_acc:0.976]
Epoch [114/120    avg_loss:0.005, val_acc:0.976]
Epoch [115/120    avg_loss:0.005, val_acc:0.976]
Epoch [116/120    avg_loss:0.006, val_acc:0.976]
Epoch [117/120    avg_loss:0.008, val_acc:0.976]
Epoch [118/120    avg_loss:0.007, val_acc:0.976]
Epoch [119/120    avg_loss:0.006, val_acc:0.976]
Epoch [120/120    avg_loss:0.005, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    1    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1248    7    6    0    0    0    0    1    1   15    7    0
     0    0    0]
 [   0    0    0  728    2    0    0    0    0    2    1   10    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    0    0    1    0    0
     6    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    1    0   24    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0  860   13    0    0
     0    0    0]
 [   0    0    8    0    0    0    0    0    0    1    7 2176   15    0
     0    3    0]
 [   0    0    0    3    0    0    0    0    0    0    1    1  527    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    93  249    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.48509485094851

F1 scores:
[       nan 0.98765432 0.98151789 0.98047138 0.98156682 0.98959538
 0.99468489 0.97959184 1.         0.9        0.98567335 0.9830585
 0.96875    1.         0.9535473  0.81773399 0.98224852]

Kappa:
0.9713066702647063
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa9868f0780>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.464, val_acc:0.434]
Epoch [2/120    avg_loss:2.013, val_acc:0.582]
Epoch [3/120    avg_loss:1.696, val_acc:0.603]
Epoch [4/120    avg_loss:1.448, val_acc:0.598]
Epoch [5/120    avg_loss:1.217, val_acc:0.682]
Epoch [6/120    avg_loss:1.009, val_acc:0.713]
Epoch [7/120    avg_loss:0.865, val_acc:0.743]
Epoch [8/120    avg_loss:0.912, val_acc:0.731]
Epoch [9/120    avg_loss:0.713, val_acc:0.829]
Epoch [10/120    avg_loss:0.648, val_acc:0.810]
Epoch [11/120    avg_loss:0.503, val_acc:0.849]
Epoch [12/120    avg_loss:0.433, val_acc:0.869]
Epoch [13/120    avg_loss:0.342, val_acc:0.865]
Epoch [14/120    avg_loss:0.274, val_acc:0.896]
Epoch [15/120    avg_loss:0.246, val_acc:0.904]
Epoch [16/120    avg_loss:0.197, val_acc:0.919]
Epoch [17/120    avg_loss:0.205, val_acc:0.877]
Epoch [18/120    avg_loss:0.292, val_acc:0.889]
Epoch [19/120    avg_loss:0.230, val_acc:0.936]
Epoch [20/120    avg_loss:0.157, val_acc:0.938]
Epoch [21/120    avg_loss:0.164, val_acc:0.835]
Epoch [22/120    avg_loss:0.210, val_acc:0.930]
Epoch [23/120    avg_loss:0.170, val_acc:0.942]
Epoch [24/120    avg_loss:0.122, val_acc:0.929]
Epoch [25/120    avg_loss:0.121, val_acc:0.912]
Epoch [26/120    avg_loss:0.118, val_acc:0.935]
Epoch [27/120    avg_loss:0.092, val_acc:0.935]
Epoch [28/120    avg_loss:0.087, val_acc:0.964]
Epoch [29/120    avg_loss:0.059, val_acc:0.963]
Epoch [30/120    avg_loss:0.055, val_acc:0.964]
Epoch [31/120    avg_loss:0.056, val_acc:0.971]
Epoch [32/120    avg_loss:0.047, val_acc:0.976]
Epoch [33/120    avg_loss:0.054, val_acc:0.960]
Epoch [34/120    avg_loss:0.078, val_acc:0.967]
Epoch [35/120    avg_loss:0.065, val_acc:0.971]
Epoch [36/120    avg_loss:0.050, val_acc:0.965]
Epoch [37/120    avg_loss:0.051, val_acc:0.975]
Epoch [38/120    avg_loss:0.038, val_acc:0.974]
Epoch [39/120    avg_loss:0.032, val_acc:0.981]
Epoch [40/120    avg_loss:0.029, val_acc:0.976]
Epoch [41/120    avg_loss:0.024, val_acc:0.966]
Epoch [42/120    avg_loss:0.027, val_acc:0.979]
Epoch [43/120    avg_loss:0.030, val_acc:0.980]
Epoch [44/120    avg_loss:0.034, val_acc:0.979]
Epoch [45/120    avg_loss:0.019, val_acc:0.981]
Epoch [46/120    avg_loss:0.023, val_acc:0.979]
Epoch [47/120    avg_loss:0.031, val_acc:0.988]
Epoch [48/120    avg_loss:0.020, val_acc:0.981]
Epoch [49/120    avg_loss:0.016, val_acc:0.985]
Epoch [50/120    avg_loss:0.012, val_acc:0.983]
Epoch [51/120    avg_loss:0.011, val_acc:0.981]
Epoch [52/120    avg_loss:0.012, val_acc:0.988]
Epoch [53/120    avg_loss:0.017, val_acc:0.970]
Epoch [54/120    avg_loss:0.018, val_acc:0.988]
Epoch [55/120    avg_loss:0.013, val_acc:0.980]
Epoch [56/120    avg_loss:0.023, val_acc:0.986]
Epoch [57/120    avg_loss:0.016, val_acc:0.983]
Epoch [58/120    avg_loss:0.018, val_acc:0.985]
Epoch [59/120    avg_loss:0.015, val_acc:0.954]
Epoch [60/120    avg_loss:0.021, val_acc:0.981]
Epoch [61/120    avg_loss:0.017, val_acc:0.985]
Epoch [62/120    avg_loss:0.010, val_acc:0.989]
Epoch [63/120    avg_loss:0.015, val_acc:0.985]
Epoch [64/120    avg_loss:0.009, val_acc:0.989]
Epoch [65/120    avg_loss:0.006, val_acc:0.990]
Epoch [66/120    avg_loss:0.008, val_acc:0.992]
Epoch [67/120    avg_loss:0.006, val_acc:0.988]
Epoch [68/120    avg_loss:0.007, val_acc:0.991]
Epoch [69/120    avg_loss:0.009, val_acc:0.990]
Epoch [70/120    avg_loss:0.009, val_acc:0.980]
Epoch [71/120    avg_loss:0.006, val_acc:0.993]
Epoch [72/120    avg_loss:0.010, val_acc:0.991]
Epoch [73/120    avg_loss:0.007, val_acc:0.990]
Epoch [74/120    avg_loss:0.005, val_acc:0.992]
Epoch [75/120    avg_loss:0.005, val_acc:0.991]
Epoch [76/120    avg_loss:0.008, val_acc:0.989]
Epoch [77/120    avg_loss:0.009, val_acc:0.990]
Epoch [78/120    avg_loss:0.012, val_acc:0.980]
Epoch [79/120    avg_loss:0.006, val_acc:0.991]
Epoch [80/120    avg_loss:0.007, val_acc:0.986]
Epoch [81/120    avg_loss:0.007, val_acc:0.990]
Epoch [82/120    avg_loss:0.010, val_acc:0.984]
Epoch [83/120    avg_loss:0.005, val_acc:0.984]
Epoch [84/120    avg_loss:0.005, val_acc:0.989]
Epoch [85/120    avg_loss:0.004, val_acc:0.989]
Epoch [86/120    avg_loss:0.004, val_acc:0.989]
Epoch [87/120    avg_loss:0.004, val_acc:0.990]
Epoch [88/120    avg_loss:0.004, val_acc:0.991]
Epoch [89/120    avg_loss:0.004, val_acc:0.990]
Epoch [90/120    avg_loss:0.004, val_acc:0.992]
Epoch [91/120    avg_loss:0.004, val_acc:0.992]
Epoch [92/120    avg_loss:0.004, val_acc:0.992]
Epoch [93/120    avg_loss:0.004, val_acc:0.990]
Epoch [94/120    avg_loss:0.004, val_acc:0.991]
Epoch [95/120    avg_loss:0.004, val_acc:0.991]
Epoch [96/120    avg_loss:0.003, val_acc:0.991]
Epoch [97/120    avg_loss:0.005, val_acc:0.991]
Epoch [98/120    avg_loss:0.004, val_acc:0.991]
Epoch [99/120    avg_loss:0.005, val_acc:0.991]
Epoch [100/120    avg_loss:0.003, val_acc:0.991]
Epoch [101/120    avg_loss:0.004, val_acc:0.991]
Epoch [102/120    avg_loss:0.005, val_acc:0.991]
Epoch [103/120    avg_loss:0.005, val_acc:0.991]
Epoch [104/120    avg_loss:0.003, val_acc:0.991]
Epoch [105/120    avg_loss:0.003, val_acc:0.991]
Epoch [106/120    avg_loss:0.004, val_acc:0.991]
Epoch [107/120    avg_loss:0.003, val_acc:0.991]
Epoch [108/120    avg_loss:0.003, val_acc:0.991]
Epoch [109/120    avg_loss:0.003, val_acc:0.991]
Epoch [110/120    avg_loss:0.004, val_acc:0.991]
Epoch [111/120    avg_loss:0.005, val_acc:0.991]
Epoch [112/120    avg_loss:0.003, val_acc:0.991]
Epoch [113/120    avg_loss:0.007, val_acc:0.991]
Epoch [114/120    avg_loss:0.004, val_acc:0.991]
Epoch [115/120    avg_loss:0.004, val_acc:0.991]
Epoch [116/120    avg_loss:0.004, val_acc:0.991]
Epoch [117/120    avg_loss:0.004, val_acc:0.991]
Epoch [118/120    avg_loss:0.004, val_acc:0.991]
Epoch [119/120    avg_loss:0.005, val_acc:0.991]
Epoch [120/120    avg_loss:0.006, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    6    5    0    0    0    0    1    4    8    0    0
     0    0    0]
 [   0    0    0  735    4    0    0    0    0    1    0    0    5    1
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    7    0    0    0    1    0    0    0  846   21    0    0
     0    0    0]
 [   0    0    4    0    0    3    0    0    0    1   15 2173   14    0
     0    0    0]
 [   0    0    0    4    4    0    0    0    0    0    0    0  525    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    50  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.97289972899729

F1 scores:
[       nan 0.98765432 0.98592651 0.98525469 0.97038724 0.99078341
 0.99620349 1.         1.         0.89473684 0.97241379 0.98459447
 0.97222222 0.99730458 0.97068966 0.89770992 0.98203593]

Kappa:
0.9768874344132426
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f29396afa90>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.482, val_acc:0.398]
Epoch [2/120    avg_loss:1.917, val_acc:0.542]
Epoch [3/120    avg_loss:1.637, val_acc:0.601]
Epoch [4/120    avg_loss:1.477, val_acc:0.617]
Epoch [5/120    avg_loss:1.286, val_acc:0.660]
Epoch [6/120    avg_loss:1.228, val_acc:0.655]
Epoch [7/120    avg_loss:1.018, val_acc:0.741]
Epoch [8/120    avg_loss:0.997, val_acc:0.743]
Epoch [9/120    avg_loss:0.738, val_acc:0.787]
Epoch [10/120    avg_loss:0.653, val_acc:0.806]
Epoch [11/120    avg_loss:0.527, val_acc:0.812]
Epoch [12/120    avg_loss:0.466, val_acc:0.844]
Epoch [13/120    avg_loss:0.462, val_acc:0.863]
Epoch [14/120    avg_loss:0.406, val_acc:0.820]
Epoch [15/120    avg_loss:0.308, val_acc:0.881]
Epoch [16/120    avg_loss:0.242, val_acc:0.887]
Epoch [17/120    avg_loss:0.216, val_acc:0.893]
Epoch [18/120    avg_loss:0.210, val_acc:0.902]
Epoch [19/120    avg_loss:0.206, val_acc:0.912]
Epoch [20/120    avg_loss:0.158, val_acc:0.927]
Epoch [21/120    avg_loss:0.145, val_acc:0.938]
Epoch [22/120    avg_loss:0.118, val_acc:0.926]
Epoch [23/120    avg_loss:0.116, val_acc:0.949]
Epoch [24/120    avg_loss:0.088, val_acc:0.942]
Epoch [25/120    avg_loss:0.080, val_acc:0.941]
Epoch [26/120    avg_loss:0.101, val_acc:0.955]
Epoch [27/120    avg_loss:0.090, val_acc:0.959]
Epoch [28/120    avg_loss:0.092, val_acc:0.954]
Epoch [29/120    avg_loss:0.057, val_acc:0.961]
Epoch [30/120    avg_loss:0.058, val_acc:0.968]
Epoch [31/120    avg_loss:0.056, val_acc:0.951]
Epoch [32/120    avg_loss:0.058, val_acc:0.941]
Epoch [33/120    avg_loss:0.063, val_acc:0.950]
Epoch [34/120    avg_loss:0.050, val_acc:0.952]
Epoch [35/120    avg_loss:0.051, val_acc:0.951]
Epoch [36/120    avg_loss:0.056, val_acc:0.965]
Epoch [37/120    avg_loss:0.035, val_acc:0.965]
Epoch [38/120    avg_loss:0.051, val_acc:0.952]
Epoch [39/120    avg_loss:0.056, val_acc:0.952]
Epoch [40/120    avg_loss:0.042, val_acc:0.964]
Epoch [41/120    avg_loss:0.029, val_acc:0.976]
Epoch [42/120    avg_loss:0.032, val_acc:0.966]
Epoch [43/120    avg_loss:0.037, val_acc:0.966]
Epoch [44/120    avg_loss:0.039, val_acc:0.966]
Epoch [45/120    avg_loss:0.028, val_acc:0.973]
Epoch [46/120    avg_loss:0.021, val_acc:0.975]
Epoch [47/120    avg_loss:0.022, val_acc:0.974]
Epoch [48/120    avg_loss:0.021, val_acc:0.974]
Epoch [49/120    avg_loss:0.027, val_acc:0.967]
Epoch [50/120    avg_loss:0.025, val_acc:0.957]
Epoch [51/120    avg_loss:0.015, val_acc:0.967]
Epoch [52/120    avg_loss:0.026, val_acc:0.967]
Epoch [53/120    avg_loss:0.019, val_acc:0.974]
Epoch [54/120    avg_loss:0.017, val_acc:0.976]
Epoch [55/120    avg_loss:0.038, val_acc:0.971]
Epoch [56/120    avg_loss:0.036, val_acc:0.968]
Epoch [57/120    avg_loss:0.019, val_acc:0.972]
Epoch [58/120    avg_loss:0.017, val_acc:0.970]
Epoch [59/120    avg_loss:0.026, val_acc:0.971]
Epoch [60/120    avg_loss:0.014, val_acc:0.977]
Epoch [61/120    avg_loss:0.012, val_acc:0.983]
Epoch [62/120    avg_loss:0.016, val_acc:0.960]
Epoch [63/120    avg_loss:0.017, val_acc:0.978]
Epoch [64/120    avg_loss:0.012, val_acc:0.980]
Epoch [65/120    avg_loss:0.010, val_acc:0.984]
Epoch [66/120    avg_loss:0.013, val_acc:0.983]
Epoch [67/120    avg_loss:0.012, val_acc:0.979]
Epoch [68/120    avg_loss:0.011, val_acc:0.975]
Epoch [69/120    avg_loss:0.010, val_acc:0.981]
Epoch [70/120    avg_loss:0.013, val_acc:0.976]
Epoch [71/120    avg_loss:0.011, val_acc:0.979]
Epoch [72/120    avg_loss:0.009, val_acc:0.979]
Epoch [73/120    avg_loss:0.011, val_acc:0.967]
Epoch [74/120    avg_loss:0.013, val_acc:0.978]
Epoch [75/120    avg_loss:0.009, val_acc:0.980]
Epoch [76/120    avg_loss:0.008, val_acc:0.980]
Epoch [77/120    avg_loss:0.007, val_acc:0.980]
Epoch [78/120    avg_loss:0.007, val_acc:0.988]
Epoch [79/120    avg_loss:0.010, val_acc:0.985]
Epoch [80/120    avg_loss:0.010, val_acc:0.980]
Epoch [81/120    avg_loss:0.012, val_acc:0.975]
Epoch [82/120    avg_loss:0.015, val_acc:0.977]
Epoch [83/120    avg_loss:0.008, val_acc:0.986]
Epoch [84/120    avg_loss:0.007, val_acc:0.985]
Epoch [85/120    avg_loss:0.007, val_acc:0.985]
Epoch [86/120    avg_loss:0.005, val_acc:0.988]
Epoch [87/120    avg_loss:0.009, val_acc:0.979]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.023, val_acc:0.978]
Epoch [90/120    avg_loss:0.009, val_acc:0.982]
Epoch [91/120    avg_loss:0.010, val_acc:0.978]
Epoch [92/120    avg_loss:0.007, val_acc:0.974]
Epoch [93/120    avg_loss:0.009, val_acc:0.976]
Epoch [94/120    avg_loss:0.005, val_acc:0.981]
Epoch [95/120    avg_loss:0.006, val_acc:0.979]
Epoch [96/120    avg_loss:0.004, val_acc:0.984]
Epoch [97/120    avg_loss:0.005, val_acc:0.985]
Epoch [98/120    avg_loss:0.011, val_acc:0.981]
Epoch [99/120    avg_loss:0.013, val_acc:0.982]
Epoch [100/120    avg_loss:0.007, val_acc:0.984]
Epoch [101/120    avg_loss:0.006, val_acc:0.986]
Epoch [102/120    avg_loss:0.003, val_acc:0.986]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.003, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.003, val_acc:0.988]
Epoch [107/120    avg_loss:0.004, val_acc:0.988]
Epoch [108/120    avg_loss:0.004, val_acc:0.988]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.005, val_acc:0.988]
Epoch [111/120    avg_loss:0.003, val_acc:0.988]
Epoch [112/120    avg_loss:0.004, val_acc:0.988]
Epoch [113/120    avg_loss:0.003, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.003, val_acc:0.988]
Epoch [117/120    avg_loss:0.004, val_acc:0.989]
Epoch [118/120    avg_loss:0.004, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.003, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1270    0    1    0    0    0    0    0    3   11    0    0
     0    0    0]
 [   0    0    0  725    0    1    0    0    0    7    1    0   11    2
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    1    1    0    0    1  863    9    0    0
     0    0    0]
 [   0    0    1    0    0    0    1    0    0    0    1 2201    6    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0  530    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1135    3    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    25  311    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.78590785907859

F1 scores:
[       nan 0.98765432 0.99374022 0.98505435 0.99530516 0.99308756
 0.98944193 0.98039216 1.         0.8        0.98797939 0.9934552
 0.97695853 0.99462366 0.98609904 0.94099849 0.97005988]

Kappa:
0.9861545987261802
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f849b5de748>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.492, val_acc:0.507]
Epoch [2/120    avg_loss:1.920, val_acc:0.575]
Epoch [3/120    avg_loss:1.570, val_acc:0.592]
Epoch [4/120    avg_loss:1.338, val_acc:0.615]
Epoch [5/120    avg_loss:1.129, val_acc:0.668]
Epoch [6/120    avg_loss:1.061, val_acc:0.700]
Epoch [7/120    avg_loss:0.870, val_acc:0.759]
Epoch [8/120    avg_loss:0.702, val_acc:0.781]
Epoch [9/120    avg_loss:0.620, val_acc:0.818]
Epoch [10/120    avg_loss:0.551, val_acc:0.832]
Epoch [11/120    avg_loss:0.517, val_acc:0.840]
Epoch [12/120    avg_loss:0.416, val_acc:0.848]
Epoch [13/120    avg_loss:0.398, val_acc:0.860]
Epoch [14/120    avg_loss:0.371, val_acc:0.889]
Epoch [15/120    avg_loss:0.328, val_acc:0.893]
Epoch [16/120    avg_loss:0.264, val_acc:0.906]
Epoch [17/120    avg_loss:0.243, val_acc:0.902]
Epoch [18/120    avg_loss:0.214, val_acc:0.894]
Epoch [19/120    avg_loss:0.180, val_acc:0.923]
Epoch [20/120    avg_loss:0.150, val_acc:0.918]
Epoch [21/120    avg_loss:0.134, val_acc:0.934]
Epoch [22/120    avg_loss:0.109, val_acc:0.941]
Epoch [23/120    avg_loss:0.112, val_acc:0.944]
Epoch [24/120    avg_loss:0.141, val_acc:0.918]
Epoch [25/120    avg_loss:0.169, val_acc:0.931]
Epoch [26/120    avg_loss:0.112, val_acc:0.940]
Epoch [27/120    avg_loss:0.092, val_acc:0.947]
Epoch [28/120    avg_loss:0.085, val_acc:0.953]
Epoch [29/120    avg_loss:0.072, val_acc:0.959]
Epoch [30/120    avg_loss:0.060, val_acc:0.944]
Epoch [31/120    avg_loss:0.064, val_acc:0.963]
Epoch [32/120    avg_loss:0.095, val_acc:0.943]
Epoch [33/120    avg_loss:0.102, val_acc:0.928]
Epoch [34/120    avg_loss:0.073, val_acc:0.948]
Epoch [35/120    avg_loss:0.073, val_acc:0.951]
Epoch [36/120    avg_loss:0.051, val_acc:0.965]
Epoch [37/120    avg_loss:0.045, val_acc:0.966]
Epoch [38/120    avg_loss:0.047, val_acc:0.951]
Epoch [39/120    avg_loss:0.042, val_acc:0.954]
Epoch [40/120    avg_loss:0.038, val_acc:0.951]
Epoch [41/120    avg_loss:0.040, val_acc:0.970]
Epoch [42/120    avg_loss:0.027, val_acc:0.966]
Epoch [43/120    avg_loss:0.034, val_acc:0.968]
Epoch [44/120    avg_loss:0.022, val_acc:0.971]
Epoch [45/120    avg_loss:0.023, val_acc:0.970]
Epoch [46/120    avg_loss:0.027, val_acc:0.970]
Epoch [47/120    avg_loss:0.022, val_acc:0.970]
Epoch [48/120    avg_loss:0.025, val_acc:0.975]
Epoch [49/120    avg_loss:0.020, val_acc:0.975]
Epoch [50/120    avg_loss:0.025, val_acc:0.952]
Epoch [51/120    avg_loss:0.018, val_acc:0.969]
Epoch [52/120    avg_loss:0.012, val_acc:0.977]
Epoch [53/120    avg_loss:0.020, val_acc:0.968]
Epoch [54/120    avg_loss:0.015, val_acc:0.975]
Epoch [55/120    avg_loss:0.012, val_acc:0.978]
Epoch [56/120    avg_loss:0.012, val_acc:0.980]
Epoch [57/120    avg_loss:0.014, val_acc:0.979]
Epoch [58/120    avg_loss:0.012, val_acc:0.974]
Epoch [59/120    avg_loss:0.016, val_acc:0.977]
Epoch [60/120    avg_loss:0.014, val_acc:0.979]
Epoch [61/120    avg_loss:0.012, val_acc:0.974]
Epoch [62/120    avg_loss:0.013, val_acc:0.978]
Epoch [63/120    avg_loss:0.013, val_acc:0.975]
Epoch [64/120    avg_loss:0.041, val_acc:0.950]
Epoch [65/120    avg_loss:0.041, val_acc:0.961]
Epoch [66/120    avg_loss:0.032, val_acc:0.966]
Epoch [67/120    avg_loss:0.018, val_acc:0.977]
Epoch [68/120    avg_loss:0.015, val_acc:0.982]
Epoch [69/120    avg_loss:0.010, val_acc:0.976]
Epoch [70/120    avg_loss:0.012, val_acc:0.980]
Epoch [71/120    avg_loss:0.015, val_acc:0.965]
Epoch [72/120    avg_loss:0.009, val_acc:0.975]
Epoch [73/120    avg_loss:0.008, val_acc:0.980]
Epoch [74/120    avg_loss:0.010, val_acc:0.970]
Epoch [75/120    avg_loss:0.008, val_acc:0.982]
Epoch [76/120    avg_loss:0.007, val_acc:0.980]
Epoch [77/120    avg_loss:0.008, val_acc:0.982]
Epoch [78/120    avg_loss:0.010, val_acc:0.975]
Epoch [79/120    avg_loss:0.011, val_acc:0.982]
Epoch [80/120    avg_loss:0.009, val_acc:0.982]
Epoch [81/120    avg_loss:0.007, val_acc:0.988]
Epoch [82/120    avg_loss:0.008, val_acc:0.977]
Epoch [83/120    avg_loss:0.012, val_acc:0.982]
Epoch [84/120    avg_loss:0.007, val_acc:0.981]
Epoch [85/120    avg_loss:0.005, val_acc:0.985]
Epoch [86/120    avg_loss:0.005, val_acc:0.982]
Epoch [87/120    avg_loss:0.009, val_acc:0.983]
Epoch [88/120    avg_loss:0.010, val_acc:0.984]
Epoch [89/120    avg_loss:0.017, val_acc:0.978]
Epoch [90/120    avg_loss:0.010, val_acc:0.973]
Epoch [91/120    avg_loss:0.006, val_acc:0.984]
Epoch [92/120    avg_loss:0.007, val_acc:0.983]
Epoch [93/120    avg_loss:0.005, val_acc:0.984]
Epoch [94/120    avg_loss:0.005, val_acc:0.984]
Epoch [95/120    avg_loss:0.005, val_acc:0.984]
Epoch [96/120    avg_loss:0.004, val_acc:0.985]
Epoch [97/120    avg_loss:0.004, val_acc:0.984]
Epoch [98/120    avg_loss:0.004, val_acc:0.984]
Epoch [99/120    avg_loss:0.006, val_acc:0.985]
Epoch [100/120    avg_loss:0.005, val_acc:0.986]
Epoch [101/120    avg_loss:0.004, val_acc:0.985]
Epoch [102/120    avg_loss:0.004, val_acc:0.986]
Epoch [103/120    avg_loss:0.004, val_acc:0.986]
Epoch [104/120    avg_loss:0.003, val_acc:0.986]
Epoch [105/120    avg_loss:0.005, val_acc:0.986]
Epoch [106/120    avg_loss:0.005, val_acc:0.986]
Epoch [107/120    avg_loss:0.003, val_acc:0.986]
Epoch [108/120    avg_loss:0.004, val_acc:0.986]
Epoch [109/120    avg_loss:0.003, val_acc:0.986]
Epoch [110/120    avg_loss:0.004, val_acc:0.986]
Epoch [111/120    avg_loss:0.004, val_acc:0.985]
Epoch [112/120    avg_loss:0.007, val_acc:0.985]
Epoch [113/120    avg_loss:0.003, val_acc:0.985]
Epoch [114/120    avg_loss:0.004, val_acc:0.985]
Epoch [115/120    avg_loss:0.003, val_acc:0.985]
Epoch [116/120    avg_loss:0.004, val_acc:0.985]
Epoch [117/120    avg_loss:0.004, val_acc:0.985]
Epoch [118/120    avg_loss:0.004, val_acc:0.985]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.003, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1272    0    0    0    1    0    0    0    3    7    0    0
     0    2    0]
 [   0    0    0  714    1    5    0    0    0    4    3    4   12    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    2    2    0    0    0  869    1    0    0
     0    1    0]
 [   0    0    1    0    0    3    2    0    0    1    2 2185   13    1
     0    2    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  532    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1135    3    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    13  330    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.84010840108401

F1 scores:
[       nan 0.98765432 0.99413834 0.97741273 0.99765808 0.98633257
 0.99319728 0.98039216 0.99650757 0.85714286 0.99200913 0.99160427
 0.9716895  0.98666667 0.99256668 0.96350365 0.97619048]

Kappa:
0.9867822080827741
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0debdd4780>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.493, val_acc:0.381]
Epoch [2/120    avg_loss:1.921, val_acc:0.567]
Epoch [3/120    avg_loss:1.668, val_acc:0.591]
Epoch [4/120    avg_loss:1.455, val_acc:0.611]
Epoch [5/120    avg_loss:1.280, val_acc:0.624]
Epoch [6/120    avg_loss:1.163, val_acc:0.719]
Epoch [7/120    avg_loss:0.895, val_acc:0.713]
Epoch [8/120    avg_loss:0.788, val_acc:0.673]
Epoch [9/120    avg_loss:0.654, val_acc:0.794]
Epoch [10/120    avg_loss:0.581, val_acc:0.832]
Epoch [11/120    avg_loss:0.550, val_acc:0.817]
Epoch [12/120    avg_loss:0.485, val_acc:0.847]
Epoch [13/120    avg_loss:0.449, val_acc:0.843]
Epoch [14/120    avg_loss:0.450, val_acc:0.788]
Epoch [15/120    avg_loss:0.413, val_acc:0.863]
Epoch [16/120    avg_loss:0.290, val_acc:0.856]
Epoch [17/120    avg_loss:0.231, val_acc:0.912]
Epoch [18/120    avg_loss:0.314, val_acc:0.864]
Epoch [19/120    avg_loss:0.213, val_acc:0.907]
Epoch [20/120    avg_loss:0.183, val_acc:0.907]
Epoch [21/120    avg_loss:0.164, val_acc:0.911]
Epoch [22/120    avg_loss:0.127, val_acc:0.933]
Epoch [23/120    avg_loss:0.133, val_acc:0.928]
Epoch [24/120    avg_loss:0.147, val_acc:0.915]
Epoch [25/120    avg_loss:0.110, val_acc:0.935]
Epoch [26/120    avg_loss:0.084, val_acc:0.944]
Epoch [27/120    avg_loss:0.083, val_acc:0.923]
Epoch [28/120    avg_loss:0.132, val_acc:0.929]
Epoch [29/120    avg_loss:0.114, val_acc:0.925]
Epoch [30/120    avg_loss:0.094, val_acc:0.959]
Epoch [31/120    avg_loss:0.068, val_acc:0.946]
Epoch [32/120    avg_loss:0.072, val_acc:0.941]
Epoch [33/120    avg_loss:0.060, val_acc:0.960]
Epoch [34/120    avg_loss:0.058, val_acc:0.953]
Epoch [35/120    avg_loss:0.098, val_acc:0.942]
Epoch [36/120    avg_loss:0.073, val_acc:0.950]
Epoch [37/120    avg_loss:0.062, val_acc:0.958]
Epoch [38/120    avg_loss:0.054, val_acc:0.959]
Epoch [39/120    avg_loss:0.040, val_acc:0.960]
Epoch [40/120    avg_loss:0.050, val_acc:0.953]
Epoch [41/120    avg_loss:0.046, val_acc:0.954]
Epoch [42/120    avg_loss:0.039, val_acc:0.960]
Epoch [43/120    avg_loss:0.029, val_acc:0.955]
Epoch [44/120    avg_loss:0.038, val_acc:0.972]
Epoch [45/120    avg_loss:0.044, val_acc:0.956]
Epoch [46/120    avg_loss:0.033, val_acc:0.976]
Epoch [47/120    avg_loss:0.031, val_acc:0.973]
Epoch [48/120    avg_loss:0.022, val_acc:0.967]
Epoch [49/120    avg_loss:0.058, val_acc:0.941]
Epoch [50/120    avg_loss:0.329, val_acc:0.802]
Epoch [51/120    avg_loss:0.288, val_acc:0.925]
Epoch [52/120    avg_loss:0.143, val_acc:0.944]
Epoch [53/120    avg_loss:0.113, val_acc:0.940]
Epoch [54/120    avg_loss:0.078, val_acc:0.938]
Epoch [55/120    avg_loss:0.052, val_acc:0.964]
Epoch [56/120    avg_loss:0.054, val_acc:0.947]
Epoch [57/120    avg_loss:0.049, val_acc:0.959]
Epoch [58/120    avg_loss:0.043, val_acc:0.970]
Epoch [59/120    avg_loss:0.029, val_acc:0.970]
Epoch [60/120    avg_loss:0.019, val_acc:0.970]
Epoch [61/120    avg_loss:0.020, val_acc:0.970]
Epoch [62/120    avg_loss:0.019, val_acc:0.971]
Epoch [63/120    avg_loss:0.022, val_acc:0.973]
Epoch [64/120    avg_loss:0.018, val_acc:0.973]
Epoch [65/120    avg_loss:0.019, val_acc:0.974]
Epoch [66/120    avg_loss:0.019, val_acc:0.975]
Epoch [67/120    avg_loss:0.018, val_acc:0.976]
Epoch [68/120    avg_loss:0.018, val_acc:0.976]
Epoch [69/120    avg_loss:0.020, val_acc:0.976]
Epoch [70/120    avg_loss:0.021, val_acc:0.977]
Epoch [71/120    avg_loss:0.016, val_acc:0.977]
Epoch [72/120    avg_loss:0.021, val_acc:0.975]
Epoch [73/120    avg_loss:0.017, val_acc:0.978]
Epoch [74/120    avg_loss:0.016, val_acc:0.980]
Epoch [75/120    avg_loss:0.016, val_acc:0.980]
Epoch [76/120    avg_loss:0.021, val_acc:0.978]
Epoch [77/120    avg_loss:0.015, val_acc:0.977]
Epoch [78/120    avg_loss:0.016, val_acc:0.978]
Epoch [79/120    avg_loss:0.015, val_acc:0.980]
Epoch [80/120    avg_loss:0.018, val_acc:0.980]
Epoch [81/120    avg_loss:0.018, val_acc:0.978]
Epoch [82/120    avg_loss:0.014, val_acc:0.980]
Epoch [83/120    avg_loss:0.016, val_acc:0.980]
Epoch [84/120    avg_loss:0.014, val_acc:0.976]
Epoch [85/120    avg_loss:0.013, val_acc:0.978]
Epoch [86/120    avg_loss:0.018, val_acc:0.976]
Epoch [87/120    avg_loss:0.015, val_acc:0.978]
Epoch [88/120    avg_loss:0.013, val_acc:0.978]
Epoch [89/120    avg_loss:0.013, val_acc:0.980]
Epoch [90/120    avg_loss:0.015, val_acc:0.978]
Epoch [91/120    avg_loss:0.014, val_acc:0.977]
Epoch [92/120    avg_loss:0.015, val_acc:0.978]
Epoch [93/120    avg_loss:0.012, val_acc:0.978]
Epoch [94/120    avg_loss:0.012, val_acc:0.977]
Epoch [95/120    avg_loss:0.016, val_acc:0.980]
Epoch [96/120    avg_loss:0.017, val_acc:0.977]
Epoch [97/120    avg_loss:0.015, val_acc:0.980]
Epoch [98/120    avg_loss:0.013, val_acc:0.982]
Epoch [99/120    avg_loss:0.015, val_acc:0.981]
Epoch [100/120    avg_loss:0.015, val_acc:0.980]
Epoch [101/120    avg_loss:0.015, val_acc:0.984]
Epoch [102/120    avg_loss:0.012, val_acc:0.983]
Epoch [103/120    avg_loss:0.014, val_acc:0.983]
Epoch [104/120    avg_loss:0.015, val_acc:0.984]
Epoch [105/120    avg_loss:0.017, val_acc:0.982]
Epoch [106/120    avg_loss:0.013, val_acc:0.983]
Epoch [107/120    avg_loss:0.012, val_acc:0.983]
Epoch [108/120    avg_loss:0.011, val_acc:0.984]
Epoch [109/120    avg_loss:0.014, val_acc:0.982]
Epoch [110/120    avg_loss:0.012, val_acc:0.984]
Epoch [111/120    avg_loss:0.012, val_acc:0.983]
Epoch [112/120    avg_loss:0.012, val_acc:0.982]
Epoch [113/120    avg_loss:0.012, val_acc:0.983]
Epoch [114/120    avg_loss:0.012, val_acc:0.981]
Epoch [115/120    avg_loss:0.012, val_acc:0.978]
Epoch [116/120    avg_loss:0.013, val_acc:0.981]
Epoch [117/120    avg_loss:0.015, val_acc:0.980]
Epoch [118/120    avg_loss:0.012, val_acc:0.983]
Epoch [119/120    avg_loss:0.010, val_acc:0.984]
Epoch [120/120    avg_loss:0.012, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1265    2    3    0    0    0    0    2    4    6    3    0
     0    0    0]
 [   0    0    0  727    1    0    0    0    0    7    2    7    3    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    2    0    2    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    2    5    0    0    0  861    4    0    0
     0    1    0]
 [   0    0   14    0    0    0    2    0    0    1   12 2175    4    2
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0    0  531    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    0    0    0
  1135    2    0]
 [   0    0    0    0    0    0   18    0    0    0    0    0    0    0
    41  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.20054200542005

F1 scores:
[       nan 0.975      0.98597038 0.98309669 0.98834499 0.99078341
 0.97980553 0.96153846 1.         0.75       0.98007968 0.98751419
 0.98698885 0.99462366 0.98056156 0.90282132 0.98809524]

Kappa:
0.9794848069478603
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f83f4534710>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.469, val_acc:0.398]
Epoch [2/120    avg_loss:1.982, val_acc:0.470]
Epoch [3/120    avg_loss:1.633, val_acc:0.623]
Epoch [4/120    avg_loss:1.370, val_acc:0.637]
Epoch [5/120    avg_loss:1.169, val_acc:0.694]
Epoch [6/120    avg_loss:0.932, val_acc:0.747]
Epoch [7/120    avg_loss:0.802, val_acc:0.754]
Epoch [8/120    avg_loss:0.641, val_acc:0.782]
Epoch [9/120    avg_loss:0.551, val_acc:0.777]
Epoch [10/120    avg_loss:0.598, val_acc:0.819]
Epoch [11/120    avg_loss:0.410, val_acc:0.865]
Epoch [12/120    avg_loss:0.345, val_acc:0.894]
Epoch [13/120    avg_loss:0.366, val_acc:0.862]
Epoch [14/120    avg_loss:0.311, val_acc:0.879]
Epoch [15/120    avg_loss:0.194, val_acc:0.902]
Epoch [16/120    avg_loss:0.187, val_acc:0.918]
Epoch [17/120    avg_loss:0.165, val_acc:0.926]
Epoch [18/120    avg_loss:0.133, val_acc:0.907]
Epoch [19/120    avg_loss:0.121, val_acc:0.939]
Epoch [20/120    avg_loss:0.122, val_acc:0.939]
Epoch [21/120    avg_loss:0.137, val_acc:0.918]
Epoch [22/120    avg_loss:0.115, val_acc:0.919]
Epoch [23/120    avg_loss:0.110, val_acc:0.930]
Epoch [24/120    avg_loss:0.099, val_acc:0.942]
Epoch [25/120    avg_loss:0.118, val_acc:0.929]
Epoch [26/120    avg_loss:0.105, val_acc:0.944]
Epoch [27/120    avg_loss:0.138, val_acc:0.927]
Epoch [28/120    avg_loss:0.156, val_acc:0.927]
Epoch [29/120    avg_loss:0.103, val_acc:0.946]
Epoch [30/120    avg_loss:0.088, val_acc:0.956]
Epoch [31/120    avg_loss:0.083, val_acc:0.945]
Epoch [32/120    avg_loss:0.086, val_acc:0.948]
Epoch [33/120    avg_loss:0.059, val_acc:0.956]
Epoch [34/120    avg_loss:0.058, val_acc:0.961]
Epoch [35/120    avg_loss:0.046, val_acc:0.959]
Epoch [36/120    avg_loss:0.031, val_acc:0.971]
Epoch [37/120    avg_loss:0.038, val_acc:0.980]
Epoch [38/120    avg_loss:0.036, val_acc:0.974]
Epoch [39/120    avg_loss:0.040, val_acc:0.957]
Epoch [40/120    avg_loss:0.034, val_acc:0.972]
Epoch [41/120    avg_loss:0.025, val_acc:0.973]
Epoch [42/120    avg_loss:0.025, val_acc:0.973]
Epoch [43/120    avg_loss:0.025, val_acc:0.972]
Epoch [44/120    avg_loss:0.021, val_acc:0.973]
Epoch [45/120    avg_loss:0.019, val_acc:0.971]
Epoch [46/120    avg_loss:0.025, val_acc:0.977]
Epoch [47/120    avg_loss:0.031, val_acc:0.957]
Epoch [48/120    avg_loss:0.026, val_acc:0.962]
Epoch [49/120    avg_loss:0.028, val_acc:0.974]
Epoch [50/120    avg_loss:0.021, val_acc:0.978]
Epoch [51/120    avg_loss:0.016, val_acc:0.980]
Epoch [52/120    avg_loss:0.014, val_acc:0.983]
Epoch [53/120    avg_loss:0.017, val_acc:0.978]
Epoch [54/120    avg_loss:0.012, val_acc:0.983]
Epoch [55/120    avg_loss:0.011, val_acc:0.985]
Epoch [56/120    avg_loss:0.011, val_acc:0.985]
Epoch [57/120    avg_loss:0.011, val_acc:0.984]
Epoch [58/120    avg_loss:0.014, val_acc:0.984]
Epoch [59/120    avg_loss:0.013, val_acc:0.983]
Epoch [60/120    avg_loss:0.011, val_acc:0.983]
Epoch [61/120    avg_loss:0.010, val_acc:0.982]
Epoch [62/120    avg_loss:0.013, val_acc:0.983]
Epoch [63/120    avg_loss:0.011, val_acc:0.983]
Epoch [64/120    avg_loss:0.012, val_acc:0.984]
Epoch [65/120    avg_loss:0.010, val_acc:0.984]
Epoch [66/120    avg_loss:0.011, val_acc:0.985]
Epoch [67/120    avg_loss:0.011, val_acc:0.984]
Epoch [68/120    avg_loss:0.011, val_acc:0.981]
Epoch [69/120    avg_loss:0.011, val_acc:0.981]
Epoch [70/120    avg_loss:0.011, val_acc:0.983]
Epoch [71/120    avg_loss:0.010, val_acc:0.984]
Epoch [72/120    avg_loss:0.012, val_acc:0.983]
Epoch [73/120    avg_loss:0.010, val_acc:0.983]
Epoch [74/120    avg_loss:0.011, val_acc:0.982]
Epoch [75/120    avg_loss:0.010, val_acc:0.984]
Epoch [76/120    avg_loss:0.011, val_acc:0.982]
Epoch [77/120    avg_loss:0.011, val_acc:0.984]
Epoch [78/120    avg_loss:0.011, val_acc:0.985]
Epoch [79/120    avg_loss:0.010, val_acc:0.983]
Epoch [80/120    avg_loss:0.009, val_acc:0.984]
Epoch [81/120    avg_loss:0.010, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.982]
Epoch [83/120    avg_loss:0.011, val_acc:0.983]
Epoch [84/120    avg_loss:0.010, val_acc:0.985]
Epoch [85/120    avg_loss:0.010, val_acc:0.984]
Epoch [86/120    avg_loss:0.008, val_acc:0.984]
Epoch [87/120    avg_loss:0.013, val_acc:0.984]
Epoch [88/120    avg_loss:0.009, val_acc:0.982]
Epoch [89/120    avg_loss:0.012, val_acc:0.981]
Epoch [90/120    avg_loss:0.010, val_acc:0.981]
Epoch [91/120    avg_loss:0.010, val_acc:0.984]
Epoch [92/120    avg_loss:0.011, val_acc:0.984]
Epoch [93/120    avg_loss:0.009, val_acc:0.983]
Epoch [94/120    avg_loss:0.009, val_acc:0.983]
Epoch [95/120    avg_loss:0.010, val_acc:0.982]
Epoch [96/120    avg_loss:0.008, val_acc:0.983]
Epoch [97/120    avg_loss:0.009, val_acc:0.983]
Epoch [98/120    avg_loss:0.009, val_acc:0.983]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.009, val_acc:0.983]
Epoch [101/120    avg_loss:0.009, val_acc:0.983]
Epoch [102/120    avg_loss:0.009, val_acc:0.983]
Epoch [103/120    avg_loss:0.009, val_acc:0.983]
Epoch [104/120    avg_loss:0.010, val_acc:0.983]
Epoch [105/120    avg_loss:0.011, val_acc:0.983]
Epoch [106/120    avg_loss:0.010, val_acc:0.984]
Epoch [107/120    avg_loss:0.009, val_acc:0.983]
Epoch [108/120    avg_loss:0.010, val_acc:0.983]
Epoch [109/120    avg_loss:0.009, val_acc:0.983]
Epoch [110/120    avg_loss:0.009, val_acc:0.983]
Epoch [111/120    avg_loss:0.010, val_acc:0.983]
Epoch [112/120    avg_loss:0.009, val_acc:0.983]
Epoch [113/120    avg_loss:0.008, val_acc:0.983]
Epoch [114/120    avg_loss:0.008, val_acc:0.983]
Epoch [115/120    avg_loss:0.011, val_acc:0.983]
Epoch [116/120    avg_loss:0.009, val_acc:0.983]
Epoch [117/120    avg_loss:0.008, val_acc:0.983]
Epoch [118/120    avg_loss:0.010, val_acc:0.983]
Epoch [119/120    avg_loss:0.008, val_acc:0.983]
Epoch [120/120    avg_loss:0.012, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1264    2    4    0    0    0    0    0    4   11    0    0
     0    0    0]
 [   0    0    0  729    0    0    0    0    0    4    1    0   12    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    1    2    0    0    0  848   18    0    0
     0    1    0]
 [   0    0   11    0    0    0    1    0    0    0    6 2191    0    1
     0    0    0]
 [   0    0    0    1    0    1    0    0    0    0    0    9  519    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1121   17    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    28  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.20054200542005

F1 scores:
[       nan 0.975      0.98557505 0.98513514 0.99069767 0.9954023
 0.98866213 1.         0.997669   0.82926829 0.97639609 0.98693694
 0.97191011 0.99462366 0.9790393  0.91580502 0.98224852]

Kappa:
0.9794762749034869
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7833df4780>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.496, val_acc:0.519]
Epoch [2/120    avg_loss:1.899, val_acc:0.568]
Epoch [3/120    avg_loss:1.593, val_acc:0.581]
Epoch [4/120    avg_loss:1.303, val_acc:0.662]
Epoch [5/120    avg_loss:1.068, val_acc:0.682]
Epoch [6/120    avg_loss:0.964, val_acc:0.734]
Epoch [7/120    avg_loss:0.828, val_acc:0.723]
Epoch [8/120    avg_loss:0.666, val_acc:0.772]
Epoch [9/120    avg_loss:0.599, val_acc:0.820]
Epoch [10/120    avg_loss:0.515, val_acc:0.835]
Epoch [11/120    avg_loss:0.512, val_acc:0.829]
Epoch [12/120    avg_loss:0.478, val_acc:0.872]
Epoch [13/120    avg_loss:0.347, val_acc:0.880]
Epoch [14/120    avg_loss:0.279, val_acc:0.904]
Epoch [15/120    avg_loss:0.237, val_acc:0.894]
Epoch [16/120    avg_loss:0.213, val_acc:0.920]
Epoch [17/120    avg_loss:0.178, val_acc:0.911]
Epoch [18/120    avg_loss:0.188, val_acc:0.929]
Epoch [19/120    avg_loss:0.177, val_acc:0.940]
Epoch [20/120    avg_loss:0.142, val_acc:0.942]
Epoch [21/120    avg_loss:0.118, val_acc:0.939]
Epoch [22/120    avg_loss:0.121, val_acc:0.941]
Epoch [23/120    avg_loss:0.225, val_acc:0.899]
Epoch [24/120    avg_loss:0.183, val_acc:0.936]
Epoch [25/120    avg_loss:0.127, val_acc:0.906]
Epoch [26/120    avg_loss:0.140, val_acc:0.941]
Epoch [27/120    avg_loss:0.106, val_acc:0.939]
Epoch [28/120    avg_loss:0.095, val_acc:0.934]
Epoch [29/120    avg_loss:0.079, val_acc:0.959]
Epoch [30/120    avg_loss:0.089, val_acc:0.958]
Epoch [31/120    avg_loss:0.084, val_acc:0.956]
Epoch [32/120    avg_loss:0.102, val_acc:0.941]
Epoch [33/120    avg_loss:0.183, val_acc:0.941]
Epoch [34/120    avg_loss:0.132, val_acc:0.935]
Epoch [35/120    avg_loss:0.072, val_acc:0.965]
Epoch [36/120    avg_loss:0.078, val_acc:0.957]
Epoch [37/120    avg_loss:0.061, val_acc:0.958]
Epoch [38/120    avg_loss:0.057, val_acc:0.965]
Epoch [39/120    avg_loss:0.069, val_acc:0.947]
Epoch [40/120    avg_loss:0.105, val_acc:0.949]
Epoch [41/120    avg_loss:0.071, val_acc:0.973]
Epoch [42/120    avg_loss:0.059, val_acc:0.940]
Epoch [43/120    avg_loss:0.055, val_acc:0.963]
Epoch [44/120    avg_loss:0.043, val_acc:0.972]
Epoch [45/120    avg_loss:0.035, val_acc:0.965]
Epoch [46/120    avg_loss:0.023, val_acc:0.970]
Epoch [47/120    avg_loss:0.022, val_acc:0.970]
Epoch [48/120    avg_loss:0.023, val_acc:0.977]
Epoch [49/120    avg_loss:0.021, val_acc:0.975]
Epoch [50/120    avg_loss:0.020, val_acc:0.977]
Epoch [51/120    avg_loss:0.025, val_acc:0.977]
Epoch [52/120    avg_loss:0.035, val_acc:0.978]
Epoch [53/120    avg_loss:0.030, val_acc:0.960]
Epoch [54/120    avg_loss:0.021, val_acc:0.978]
Epoch [55/120    avg_loss:0.016, val_acc:0.978]
Epoch [56/120    avg_loss:0.017, val_acc:0.975]
Epoch [57/120    avg_loss:0.021, val_acc:0.976]
Epoch [58/120    avg_loss:0.016, val_acc:0.980]
Epoch [59/120    avg_loss:0.014, val_acc:0.980]
Epoch [60/120    avg_loss:0.017, val_acc:0.963]
Epoch [61/120    avg_loss:0.021, val_acc:0.974]
Epoch [62/120    avg_loss:0.015, val_acc:0.978]
Epoch [63/120    avg_loss:0.016, val_acc:0.976]
Epoch [64/120    avg_loss:0.016, val_acc:0.977]
Epoch [65/120    avg_loss:0.019, val_acc:0.977]
Epoch [66/120    avg_loss:0.021, val_acc:0.967]
Epoch [67/120    avg_loss:0.021, val_acc:0.977]
Epoch [68/120    avg_loss:0.019, val_acc:0.980]
Epoch [69/120    avg_loss:0.015, val_acc:0.976]
Epoch [70/120    avg_loss:0.016, val_acc:0.979]
Epoch [71/120    avg_loss:0.016, val_acc:0.977]
Epoch [72/120    avg_loss:0.019, val_acc:0.971]
Epoch [73/120    avg_loss:0.016, val_acc:0.985]
Epoch [74/120    avg_loss:0.011, val_acc:0.976]
Epoch [75/120    avg_loss:0.015, val_acc:0.976]
Epoch [76/120    avg_loss:0.016, val_acc:0.980]
Epoch [77/120    avg_loss:0.012, val_acc:0.983]
Epoch [78/120    avg_loss:0.009, val_acc:0.981]
Epoch [79/120    avg_loss:0.007, val_acc:0.985]
Epoch [80/120    avg_loss:0.022, val_acc:0.972]
Epoch [81/120    avg_loss:0.012, val_acc:0.975]
Epoch [82/120    avg_loss:0.011, val_acc:0.981]
Epoch [83/120    avg_loss:0.009, val_acc:0.976]
Epoch [84/120    avg_loss:0.009, val_acc:0.984]
Epoch [85/120    avg_loss:0.009, val_acc:0.979]
Epoch [86/120    avg_loss:0.007, val_acc:0.984]
Epoch [87/120    avg_loss:0.008, val_acc:0.979]
Epoch [88/120    avg_loss:0.008, val_acc:0.980]
Epoch [89/120    avg_loss:0.009, val_acc:0.976]
Epoch [90/120    avg_loss:0.009, val_acc:0.977]
Epoch [91/120    avg_loss:0.011, val_acc:0.979]
Epoch [92/120    avg_loss:0.010, val_acc:0.977]
Epoch [93/120    avg_loss:0.008, val_acc:0.980]
Epoch [94/120    avg_loss:0.006, val_acc:0.981]
Epoch [95/120    avg_loss:0.006, val_acc:0.981]
Epoch [96/120    avg_loss:0.006, val_acc:0.981]
Epoch [97/120    avg_loss:0.005, val_acc:0.981]
Epoch [98/120    avg_loss:0.005, val_acc:0.981]
Epoch [99/120    avg_loss:0.005, val_acc:0.981]
Epoch [100/120    avg_loss:0.004, val_acc:0.981]
Epoch [101/120    avg_loss:0.004, val_acc:0.982]
Epoch [102/120    avg_loss:0.005, val_acc:0.982]
Epoch [103/120    avg_loss:0.005, val_acc:0.982]
Epoch [104/120    avg_loss:0.006, val_acc:0.983]
Epoch [105/120    avg_loss:0.004, val_acc:0.983]
Epoch [106/120    avg_loss:0.003, val_acc:0.983]
Epoch [107/120    avg_loss:0.003, val_acc:0.983]
Epoch [108/120    avg_loss:0.003, val_acc:0.983]
Epoch [109/120    avg_loss:0.005, val_acc:0.983]
Epoch [110/120    avg_loss:0.006, val_acc:0.983]
Epoch [111/120    avg_loss:0.004, val_acc:0.983]
Epoch [112/120    avg_loss:0.004, val_acc:0.983]
Epoch [113/120    avg_loss:0.005, val_acc:0.983]
Epoch [114/120    avg_loss:0.005, val_acc:0.983]
Epoch [115/120    avg_loss:0.004, val_acc:0.983]
Epoch [116/120    avg_loss:0.003, val_acc:0.983]
Epoch [117/120    avg_loss:0.006, val_acc:0.983]
Epoch [118/120    avg_loss:0.004, val_acc:0.983]
Epoch [119/120    avg_loss:0.005, val_acc:0.983]
Epoch [120/120    avg_loss:0.005, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1256    2    9    0    0    0    0    0    4    8    4    0
     0    2    0]
 [   0    0    0  727    1    6    0    0    0    7    1    0    2    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    1    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    1    1    0    0    0  867    4    0    0
     0    1    0]
 [   0    0   14    0    0    0    2    0    0    1    2 2188    2    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  533    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1131    7    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    28  303    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.44986449864498

F1 scores:
[       nan 0.96202532 0.9827856  0.98509485 0.97706422 0.98739977
 0.98424606 0.98039216 1.         0.7826087  0.98916144 0.99206529
 0.98886827 0.98930481 0.9843342  0.91818182 0.97590361]

Kappa:
0.9823307895590507
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd8a4989748>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.477, val_acc:0.305]
Epoch [2/120    avg_loss:1.925, val_acc:0.518]
Epoch [3/120    avg_loss:1.667, val_acc:0.608]
Epoch [4/120    avg_loss:1.442, val_acc:0.599]
Epoch [5/120    avg_loss:1.228, val_acc:0.672]
Epoch [6/120    avg_loss:0.993, val_acc:0.760]
Epoch [7/120    avg_loss:0.762, val_acc:0.792]
Epoch [8/120    avg_loss:0.692, val_acc:0.782]
Epoch [9/120    avg_loss:0.526, val_acc:0.836]
Epoch [10/120    avg_loss:0.523, val_acc:0.843]
Epoch [11/120    avg_loss:0.480, val_acc:0.863]
Epoch [12/120    avg_loss:0.364, val_acc:0.858]
Epoch [13/120    avg_loss:0.368, val_acc:0.842]
Epoch [14/120    avg_loss:0.330, val_acc:0.871]
Epoch [15/120    avg_loss:0.241, val_acc:0.920]
Epoch [16/120    avg_loss:0.211, val_acc:0.905]
Epoch [17/120    avg_loss:0.249, val_acc:0.880]
Epoch [18/120    avg_loss:0.313, val_acc:0.849]
Epoch [19/120    avg_loss:0.250, val_acc:0.901]
Epoch [20/120    avg_loss:0.221, val_acc:0.907]
Epoch [21/120    avg_loss:0.238, val_acc:0.896]
Epoch [22/120    avg_loss:0.148, val_acc:0.942]
Epoch [23/120    avg_loss:0.120, val_acc:0.934]
Epoch [24/120    avg_loss:0.187, val_acc:0.902]
Epoch [25/120    avg_loss:0.276, val_acc:0.923]
Epoch [26/120    avg_loss:0.137, val_acc:0.925]
Epoch [27/120    avg_loss:0.113, val_acc:0.904]
Epoch [28/120    avg_loss:0.086, val_acc:0.944]
Epoch [29/120    avg_loss:0.081, val_acc:0.944]
Epoch [30/120    avg_loss:0.066, val_acc:0.953]
Epoch [31/120    avg_loss:0.059, val_acc:0.954]
Epoch [32/120    avg_loss:0.058, val_acc:0.960]
Epoch [33/120    avg_loss:0.066, val_acc:0.958]
Epoch [34/120    avg_loss:0.060, val_acc:0.953]
Epoch [35/120    avg_loss:0.052, val_acc:0.970]
Epoch [36/120    avg_loss:0.061, val_acc:0.929]
Epoch [37/120    avg_loss:0.070, val_acc:0.955]
Epoch [38/120    avg_loss:0.050, val_acc:0.957]
Epoch [39/120    avg_loss:0.041, val_acc:0.967]
Epoch [40/120    avg_loss:0.038, val_acc:0.963]
Epoch [41/120    avg_loss:0.033, val_acc:0.968]
Epoch [42/120    avg_loss:0.038, val_acc:0.964]
Epoch [43/120    avg_loss:0.035, val_acc:0.960]
Epoch [44/120    avg_loss:0.031, val_acc:0.973]
Epoch [45/120    avg_loss:0.027, val_acc:0.973]
Epoch [46/120    avg_loss:0.028, val_acc:0.966]
Epoch [47/120    avg_loss:0.027, val_acc:0.969]
Epoch [48/120    avg_loss:0.027, val_acc:0.973]
Epoch [49/120    avg_loss:0.032, val_acc:0.963]
Epoch [50/120    avg_loss:0.035, val_acc:0.971]
Epoch [51/120    avg_loss:0.040, val_acc:0.963]
Epoch [52/120    avg_loss:0.109, val_acc:0.957]
Epoch [53/120    avg_loss:0.068, val_acc:0.959]
Epoch [54/120    avg_loss:0.083, val_acc:0.959]
Epoch [55/120    avg_loss:0.056, val_acc:0.964]
Epoch [56/120    avg_loss:0.048, val_acc:0.953]
Epoch [57/120    avg_loss:0.037, val_acc:0.967]
Epoch [58/120    avg_loss:0.025, val_acc:0.973]
Epoch [59/120    avg_loss:0.022, val_acc:0.973]
Epoch [60/120    avg_loss:0.024, val_acc:0.978]
Epoch [61/120    avg_loss:0.025, val_acc:0.965]
Epoch [62/120    avg_loss:0.018, val_acc:0.975]
Epoch [63/120    avg_loss:0.019, val_acc:0.969]
Epoch [64/120    avg_loss:0.015, val_acc:0.974]
Epoch [65/120    avg_loss:0.014, val_acc:0.973]
Epoch [66/120    avg_loss:0.013, val_acc:0.979]
Epoch [67/120    avg_loss:0.013, val_acc:0.979]
Epoch [68/120    avg_loss:0.013, val_acc:0.978]
Epoch [69/120    avg_loss:0.014, val_acc:0.973]
Epoch [70/120    avg_loss:0.011, val_acc:0.978]
Epoch [71/120    avg_loss:0.017, val_acc:0.975]
Epoch [72/120    avg_loss:0.017, val_acc:0.977]
Epoch [73/120    avg_loss:0.024, val_acc:0.966]
Epoch [74/120    avg_loss:0.021, val_acc:0.973]
Epoch [75/120    avg_loss:0.012, val_acc:0.978]
Epoch [76/120    avg_loss:0.016, val_acc:0.976]
Epoch [77/120    avg_loss:0.012, val_acc:0.979]
Epoch [78/120    avg_loss:0.011, val_acc:0.977]
Epoch [79/120    avg_loss:0.011, val_acc:0.978]
Epoch [80/120    avg_loss:0.009, val_acc:0.980]
Epoch [81/120    avg_loss:0.012, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.978]
Epoch [83/120    avg_loss:0.008, val_acc:0.979]
Epoch [84/120    avg_loss:0.012, val_acc:0.977]
Epoch [85/120    avg_loss:0.007, val_acc:0.982]
Epoch [86/120    avg_loss:0.009, val_acc:0.979]
Epoch [87/120    avg_loss:0.008, val_acc:0.980]
Epoch [88/120    avg_loss:0.007, val_acc:0.981]
Epoch [89/120    avg_loss:0.006, val_acc:0.983]
Epoch [90/120    avg_loss:0.005, val_acc:0.982]
Epoch [91/120    avg_loss:0.005, val_acc:0.984]
Epoch [92/120    avg_loss:0.005, val_acc:0.983]
Epoch [93/120    avg_loss:0.007, val_acc:0.980]
Epoch [94/120    avg_loss:0.006, val_acc:0.982]
Epoch [95/120    avg_loss:0.007, val_acc:0.979]
Epoch [96/120    avg_loss:0.005, val_acc:0.982]
Epoch [97/120    avg_loss:0.005, val_acc:0.985]
Epoch [98/120    avg_loss:0.004, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.004, val_acc:0.984]
Epoch [101/120    avg_loss:0.007, val_acc:0.985]
Epoch [102/120    avg_loss:0.006, val_acc:0.982]
Epoch [103/120    avg_loss:0.006, val_acc:0.988]
Epoch [104/120    avg_loss:0.012, val_acc:0.978]
Epoch [105/120    avg_loss:0.009, val_acc:0.980]
Epoch [106/120    avg_loss:0.021, val_acc:0.978]
Epoch [107/120    avg_loss:0.011, val_acc:0.980]
Epoch [108/120    avg_loss:0.017, val_acc:0.977]
Epoch [109/120    avg_loss:0.011, val_acc:0.981]
Epoch [110/120    avg_loss:0.008, val_acc:0.980]
Epoch [111/120    avg_loss:0.007, val_acc:0.981]
Epoch [112/120    avg_loss:0.005, val_acc:0.980]
Epoch [113/120    avg_loss:0.004, val_acc:0.982]
Epoch [114/120    avg_loss:0.007, val_acc:0.980]
Epoch [115/120    avg_loss:0.004, val_acc:0.982]
Epoch [116/120    avg_loss:0.007, val_acc:0.981]
Epoch [117/120    avg_loss:0.005, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.983]
Epoch [119/120    avg_loss:0.004, val_acc:0.983]
Epoch [120/120    avg_loss:0.003, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1266    0    7    0    2    0    0    0    6    3    0    0
     0    1    0]
 [   0    0    0  725    1    4    0    0    0   12    1    0    3    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    2    7    0    0    0  848   14    0    0
     0    3    0]
 [   0    0    9    0    0    0   11    0    0    0    1 2181    7    1
     0    0    0]
 [   0    0    0    0    1    1    0    0    0    0    4    0  527    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    1    0    0    0
  1127    5    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
    16  306    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.1680216802168

F1 scores:
[       nan 0.98765432 0.9882904  0.98505435 0.97931034 0.98293515
 0.96612666 0.98039216 0.995338   0.72       0.97695853 0.98933999
 0.97773655 0.99462366 0.98773006 0.9244713  0.96969697]

Kappa:
0.9791222441168056
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f909c88d6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.462, val_acc:0.425]
Epoch [2/120    avg_loss:1.914, val_acc:0.476]
Epoch [3/120    avg_loss:1.694, val_acc:0.602]
Epoch [4/120    avg_loss:1.537, val_acc:0.610]
Epoch [5/120    avg_loss:1.304, val_acc:0.636]
Epoch [6/120    avg_loss:1.151, val_acc:0.614]
Epoch [7/120    avg_loss:1.057, val_acc:0.709]
Epoch [8/120    avg_loss:0.898, val_acc:0.730]
Epoch [9/120    avg_loss:0.790, val_acc:0.787]
Epoch [10/120    avg_loss:0.595, val_acc:0.797]
Epoch [11/120    avg_loss:0.514, val_acc:0.806]
Epoch [12/120    avg_loss:0.442, val_acc:0.772]
Epoch [13/120    avg_loss:0.496, val_acc:0.839]
Epoch [14/120    avg_loss:0.489, val_acc:0.790]
Epoch [15/120    avg_loss:0.469, val_acc:0.875]
Epoch [16/120    avg_loss:0.332, val_acc:0.860]
Epoch [17/120    avg_loss:0.246, val_acc:0.883]
Epoch [18/120    avg_loss:0.235, val_acc:0.905]
Epoch [19/120    avg_loss:0.201, val_acc:0.916]
Epoch [20/120    avg_loss:0.176, val_acc:0.923]
Epoch [21/120    avg_loss:0.154, val_acc:0.936]
Epoch [22/120    avg_loss:0.146, val_acc:0.917]
Epoch [23/120    avg_loss:0.176, val_acc:0.926]
Epoch [24/120    avg_loss:0.142, val_acc:0.932]
Epoch [25/120    avg_loss:0.106, val_acc:0.950]
Epoch [26/120    avg_loss:0.072, val_acc:0.952]
Epoch [27/120    avg_loss:0.104, val_acc:0.942]
Epoch [28/120    avg_loss:0.158, val_acc:0.932]
Epoch [29/120    avg_loss:0.125, val_acc:0.947]
Epoch [30/120    avg_loss:0.078, val_acc:0.956]
Epoch [31/120    avg_loss:0.068, val_acc:0.963]
Epoch [32/120    avg_loss:0.059, val_acc:0.951]
Epoch [33/120    avg_loss:0.058, val_acc:0.950]
Epoch [34/120    avg_loss:0.059, val_acc:0.949]
Epoch [35/120    avg_loss:0.053, val_acc:0.969]
Epoch [36/120    avg_loss:0.051, val_acc:0.958]
Epoch [37/120    avg_loss:0.043, val_acc:0.968]
Epoch [38/120    avg_loss:0.037, val_acc:0.963]
Epoch [39/120    avg_loss:0.038, val_acc:0.958]
Epoch [40/120    avg_loss:0.029, val_acc:0.975]
Epoch [41/120    avg_loss:0.026, val_acc:0.971]
Epoch [42/120    avg_loss:0.027, val_acc:0.978]
Epoch [43/120    avg_loss:0.031, val_acc:0.975]
Epoch [44/120    avg_loss:0.030, val_acc:0.971]
Epoch [45/120    avg_loss:0.046, val_acc:0.975]
Epoch [46/120    avg_loss:0.029, val_acc:0.973]
Epoch [47/120    avg_loss:0.027, val_acc:0.970]
Epoch [48/120    avg_loss:0.023, val_acc:0.975]
Epoch [49/120    avg_loss:0.021, val_acc:0.973]
Epoch [50/120    avg_loss:0.036, val_acc:0.970]
Epoch [51/120    avg_loss:0.020, val_acc:0.971]
Epoch [52/120    avg_loss:0.025, val_acc:0.968]
Epoch [53/120    avg_loss:0.031, val_acc:0.968]
Epoch [54/120    avg_loss:0.032, val_acc:0.967]
Epoch [55/120    avg_loss:0.020, val_acc:0.979]
Epoch [56/120    avg_loss:0.026, val_acc:0.974]
Epoch [57/120    avg_loss:0.021, val_acc:0.976]
Epoch [58/120    avg_loss:0.019, val_acc:0.971]
Epoch [59/120    avg_loss:0.023, val_acc:0.975]
Epoch [60/120    avg_loss:0.016, val_acc:0.979]
Epoch [61/120    avg_loss:0.012, val_acc:0.982]
Epoch [62/120    avg_loss:0.014, val_acc:0.975]
Epoch [63/120    avg_loss:0.013, val_acc:0.978]
Epoch [64/120    avg_loss:0.011, val_acc:0.984]
Epoch [65/120    avg_loss:0.011, val_acc:0.976]
Epoch [66/120    avg_loss:0.011, val_acc:0.970]
Epoch [67/120    avg_loss:0.015, val_acc:0.979]
Epoch [68/120    avg_loss:0.011, val_acc:0.972]
Epoch [69/120    avg_loss:0.008, val_acc:0.980]
Epoch [70/120    avg_loss:0.018, val_acc:0.981]
Epoch [71/120    avg_loss:0.013, val_acc:0.978]
Epoch [72/120    avg_loss:0.011, val_acc:0.981]
Epoch [73/120    avg_loss:0.010, val_acc:0.985]
Epoch [74/120    avg_loss:0.012, val_acc:0.981]
Epoch [75/120    avg_loss:0.012, val_acc:0.980]
Epoch [76/120    avg_loss:0.010, val_acc:0.978]
Epoch [77/120    avg_loss:0.009, val_acc:0.978]
Epoch [78/120    avg_loss:0.008, val_acc:0.979]
Epoch [79/120    avg_loss:0.008, val_acc:0.978]
Epoch [80/120    avg_loss:0.008, val_acc:0.980]
Epoch [81/120    avg_loss:0.008, val_acc:0.976]
Epoch [82/120    avg_loss:0.007, val_acc:0.979]
Epoch [83/120    avg_loss:0.006, val_acc:0.981]
Epoch [84/120    avg_loss:0.013, val_acc:0.966]
Epoch [85/120    avg_loss:0.014, val_acc:0.977]
Epoch [86/120    avg_loss:0.019, val_acc:0.975]
Epoch [87/120    avg_loss:0.012, val_acc:0.978]
Epoch [88/120    avg_loss:0.008, val_acc:0.978]
Epoch [89/120    avg_loss:0.006, val_acc:0.980]
Epoch [90/120    avg_loss:0.007, val_acc:0.980]
Epoch [91/120    avg_loss:0.005, val_acc:0.980]
Epoch [92/120    avg_loss:0.005, val_acc:0.981]
Epoch [93/120    avg_loss:0.006, val_acc:0.981]
Epoch [94/120    avg_loss:0.005, val_acc:0.980]
Epoch [95/120    avg_loss:0.004, val_acc:0.980]
Epoch [96/120    avg_loss:0.005, val_acc:0.980]
Epoch [97/120    avg_loss:0.005, val_acc:0.981]
Epoch [98/120    avg_loss:0.006, val_acc:0.980]
Epoch [99/120    avg_loss:0.006, val_acc:0.980]
Epoch [100/120    avg_loss:0.005, val_acc:0.980]
Epoch [101/120    avg_loss:0.006, val_acc:0.980]
Epoch [102/120    avg_loss:0.005, val_acc:0.980]
Epoch [103/120    avg_loss:0.004, val_acc:0.980]
Epoch [104/120    avg_loss:0.006, val_acc:0.980]
Epoch [105/120    avg_loss:0.006, val_acc:0.980]
Epoch [106/120    avg_loss:0.004, val_acc:0.980]
Epoch [107/120    avg_loss:0.005, val_acc:0.980]
Epoch [108/120    avg_loss:0.006, val_acc:0.980]
Epoch [109/120    avg_loss:0.005, val_acc:0.980]
Epoch [110/120    avg_loss:0.006, val_acc:0.980]
Epoch [111/120    avg_loss:0.006, val_acc:0.980]
Epoch [112/120    avg_loss:0.006, val_acc:0.980]
Epoch [113/120    avg_loss:0.004, val_acc:0.980]
Epoch [114/120    avg_loss:0.006, val_acc:0.980]
Epoch [115/120    avg_loss:0.006, val_acc:0.980]
Epoch [116/120    avg_loss:0.004, val_acc:0.980]
Epoch [117/120    avg_loss:0.005, val_acc:0.980]
Epoch [118/120    avg_loss:0.005, val_acc:0.980]
Epoch [119/120    avg_loss:0.006, val_acc:0.980]
Epoch [120/120    avg_loss:0.005, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    0    4    0    1    0    0    0    7    4    6    0
     0    2    0]
 [   0    0    0  735    2    0    1    0    0    2    2    1    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    2    4    0    0    0  845   14    0    0
     0    3    0]
 [   0    0   21    0    0    0    3    0    0    0   11 2173    1    1
     0    0    0]
 [   0    0    0    6    0    3    0    0    0    0    4    0  519    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    1    0    1    0    0    0
  1131    2    0]
 [   0    0    0    0    0    0   21    0    0    0    0    0    0    0
    20  306    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.12466124661246

F1 scores:
[       nan 1.         0.97979798 0.98790323 0.98611111 0.98630137
 0.97615499 0.98039216 0.99883856 0.92307692 0.96848138 0.98660613
 0.97464789 0.99730458 0.98777293 0.92727273 0.98224852]

Kappa:
0.9786233122422395
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f087499d748>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.516, val_acc:0.357]
Epoch [2/120    avg_loss:1.929, val_acc:0.573]
Epoch [3/120    avg_loss:1.636, val_acc:0.578]
Epoch [4/120    avg_loss:1.354, val_acc:0.595]
Epoch [5/120    avg_loss:1.241, val_acc:0.689]
Epoch [6/120    avg_loss:0.932, val_acc:0.731]
Epoch [7/120    avg_loss:0.939, val_acc:0.740]
Epoch [8/120    avg_loss:0.750, val_acc:0.766]
Epoch [9/120    avg_loss:0.675, val_acc:0.785]
Epoch [10/120    avg_loss:0.641, val_acc:0.780]
Epoch [11/120    avg_loss:0.634, val_acc:0.778]
Epoch [12/120    avg_loss:0.443, val_acc:0.839]
Epoch [13/120    avg_loss:0.360, val_acc:0.833]
Epoch [14/120    avg_loss:0.398, val_acc:0.872]
Epoch [15/120    avg_loss:0.300, val_acc:0.893]
Epoch [16/120    avg_loss:0.266, val_acc:0.906]
Epoch [17/120    avg_loss:0.257, val_acc:0.897]
Epoch [18/120    avg_loss:0.228, val_acc:0.922]
Epoch [19/120    avg_loss:0.209, val_acc:0.932]
Epoch [20/120    avg_loss:0.158, val_acc:0.930]
Epoch [21/120    avg_loss:0.147, val_acc:0.930]
Epoch [22/120    avg_loss:0.171, val_acc:0.930]
Epoch [23/120    avg_loss:0.176, val_acc:0.919]
Epoch [24/120    avg_loss:0.196, val_acc:0.921]
Epoch [25/120    avg_loss:0.137, val_acc:0.947]
Epoch [26/120    avg_loss:0.271, val_acc:0.871]
Epoch [27/120    avg_loss:0.297, val_acc:0.907]
Epoch [28/120    avg_loss:0.222, val_acc:0.923]
Epoch [29/120    avg_loss:0.247, val_acc:0.946]
Epoch [30/120    avg_loss:0.157, val_acc:0.944]
Epoch [31/120    avg_loss:0.119, val_acc:0.947]
Epoch [32/120    avg_loss:0.102, val_acc:0.944]
Epoch [33/120    avg_loss:0.084, val_acc:0.958]
Epoch [34/120    avg_loss:0.079, val_acc:0.963]
Epoch [35/120    avg_loss:0.075, val_acc:0.946]
Epoch [36/120    avg_loss:0.068, val_acc:0.960]
Epoch [37/120    avg_loss:0.050, val_acc:0.956]
Epoch [38/120    avg_loss:0.056, val_acc:0.951]
Epoch [39/120    avg_loss:0.056, val_acc:0.960]
Epoch [40/120    avg_loss:0.054, val_acc:0.957]
Epoch [41/120    avg_loss:0.047, val_acc:0.958]
Epoch [42/120    avg_loss:0.060, val_acc:0.967]
Epoch [43/120    avg_loss:0.071, val_acc:0.932]
Epoch [44/120    avg_loss:0.071, val_acc:0.958]
Epoch [45/120    avg_loss:0.064, val_acc:0.955]
Epoch [46/120    avg_loss:0.042, val_acc:0.970]
Epoch [47/120    avg_loss:0.050, val_acc:0.963]
Epoch [48/120    avg_loss:0.053, val_acc:0.945]
Epoch [49/120    avg_loss:0.055, val_acc:0.967]
Epoch [50/120    avg_loss:0.044, val_acc:0.965]
Epoch [51/120    avg_loss:0.044, val_acc:0.971]
Epoch [52/120    avg_loss:0.024, val_acc:0.964]
Epoch [53/120    avg_loss:0.028, val_acc:0.975]
Epoch [54/120    avg_loss:0.022, val_acc:0.975]
Epoch [55/120    avg_loss:0.032, val_acc:0.971]
Epoch [56/120    avg_loss:0.023, val_acc:0.971]
Epoch [57/120    avg_loss:0.019, val_acc:0.984]
Epoch [58/120    avg_loss:0.018, val_acc:0.973]
Epoch [59/120    avg_loss:0.015, val_acc:0.983]
Epoch [60/120    avg_loss:0.022, val_acc:0.978]
Epoch [61/120    avg_loss:0.015, val_acc:0.978]
Epoch [62/120    avg_loss:0.014, val_acc:0.968]
Epoch [63/120    avg_loss:0.015, val_acc:0.975]
Epoch [64/120    avg_loss:0.017, val_acc:0.977]
Epoch [65/120    avg_loss:0.021, val_acc:0.981]
Epoch [66/120    avg_loss:0.021, val_acc:0.978]
Epoch [67/120    avg_loss:0.016, val_acc:0.976]
Epoch [68/120    avg_loss:0.012, val_acc:0.979]
Epoch [69/120    avg_loss:0.015, val_acc:0.980]
Epoch [70/120    avg_loss:0.025, val_acc:0.971]
Epoch [71/120    avg_loss:0.014, val_acc:0.980]
Epoch [72/120    avg_loss:0.009, val_acc:0.982]
Epoch [73/120    avg_loss:0.011, val_acc:0.986]
Epoch [74/120    avg_loss:0.013, val_acc:0.983]
Epoch [75/120    avg_loss:0.010, val_acc:0.983]
Epoch [76/120    avg_loss:0.009, val_acc:0.983]
Epoch [77/120    avg_loss:0.011, val_acc:0.982]
Epoch [78/120    avg_loss:0.012, val_acc:0.983]
Epoch [79/120    avg_loss:0.009, val_acc:0.985]
Epoch [80/120    avg_loss:0.008, val_acc:0.984]
Epoch [81/120    avg_loss:0.010, val_acc:0.984]
Epoch [82/120    avg_loss:0.009, val_acc:0.984]
Epoch [83/120    avg_loss:0.008, val_acc:0.984]
Epoch [84/120    avg_loss:0.009, val_acc:0.985]
Epoch [85/120    avg_loss:0.009, val_acc:0.986]
Epoch [86/120    avg_loss:0.007, val_acc:0.986]
Epoch [87/120    avg_loss:0.009, val_acc:0.988]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.009, val_acc:0.985]
Epoch [90/120    avg_loss:0.009, val_acc:0.985]
Epoch [91/120    avg_loss:0.008, val_acc:0.985]
Epoch [92/120    avg_loss:0.012, val_acc:0.985]
Epoch [93/120    avg_loss:0.008, val_acc:0.986]
Epoch [94/120    avg_loss:0.008, val_acc:0.985]
Epoch [95/120    avg_loss:0.008, val_acc:0.985]
Epoch [96/120    avg_loss:0.011, val_acc:0.983]
Epoch [97/120    avg_loss:0.011, val_acc:0.986]
Epoch [98/120    avg_loss:0.011, val_acc:0.984]
Epoch [99/120    avg_loss:0.009, val_acc:0.984]
Epoch [100/120    avg_loss:0.008, val_acc:0.984]
Epoch [101/120    avg_loss:0.007, val_acc:0.984]
Epoch [102/120    avg_loss:0.007, val_acc:0.984]
Epoch [103/120    avg_loss:0.009, val_acc:0.984]
Epoch [104/120    avg_loss:0.007, val_acc:0.985]
Epoch [105/120    avg_loss:0.007, val_acc:0.985]
Epoch [106/120    avg_loss:0.007, val_acc:0.985]
Epoch [107/120    avg_loss:0.008, val_acc:0.985]
Epoch [108/120    avg_loss:0.008, val_acc:0.985]
Epoch [109/120    avg_loss:0.009, val_acc:0.985]
Epoch [110/120    avg_loss:0.009, val_acc:0.985]
Epoch [111/120    avg_loss:0.011, val_acc:0.985]
Epoch [112/120    avg_loss:0.007, val_acc:0.985]
Epoch [113/120    avg_loss:0.007, val_acc:0.985]
Epoch [114/120    avg_loss:0.008, val_acc:0.985]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.009, val_acc:0.985]
Epoch [117/120    avg_loss:0.009, val_acc:0.985]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.007, val_acc:0.985]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1268    1    1    0    1    0    0    0    2    8    3    0
     0    1    0]
 [   0    0    0  730    4    2    0    0    0    5    1    0    3    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    8    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    2    2    0    0    0  857    9    0    0
     0    2    0]
 [   0    0    7    0    0    0    2    0    3    0    4 2191    0    0
     0    3    0]
 [   0    0    0    2    0    1    0    0    0    0    0    0  529    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1132    7    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    48  290    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.29810298102981

F1 scores:
[       nan 0.975      0.98946547 0.98648649 0.98839907 0.98030127
 0.98945783 0.86206897 0.9941928  0.85714286 0.98449167 0.99185152
 0.9869403  0.99462366 0.97502153 0.89230769 0.98224852]

Kappa:
0.9805933022093668
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd08e4a7748>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.478, val_acc:0.491]
Epoch [2/120    avg_loss:2.003, val_acc:0.505]
Epoch [3/120    avg_loss:1.638, val_acc:0.544]
Epoch [4/120    avg_loss:1.501, val_acc:0.609]
Epoch [5/120    avg_loss:1.282, val_acc:0.658]
Epoch [6/120    avg_loss:1.071, val_acc:0.674]
Epoch [7/120    avg_loss:0.844, val_acc:0.717]
Epoch [8/120    avg_loss:0.821, val_acc:0.759]
Epoch [9/120    avg_loss:0.650, val_acc:0.747]
Epoch [10/120    avg_loss:0.634, val_acc:0.713]
Epoch [11/120    avg_loss:0.794, val_acc:0.805]
Epoch [12/120    avg_loss:0.508, val_acc:0.787]
Epoch [13/120    avg_loss:0.436, val_acc:0.864]
Epoch [14/120    avg_loss:0.357, val_acc:0.861]
Epoch [15/120    avg_loss:0.326, val_acc:0.909]
Epoch [16/120    avg_loss:0.278, val_acc:0.896]
Epoch [17/120    avg_loss:0.403, val_acc:0.807]
Epoch [18/120    avg_loss:0.414, val_acc:0.902]
Epoch [19/120    avg_loss:0.311, val_acc:0.898]
Epoch [20/120    avg_loss:0.201, val_acc:0.914]
Epoch [21/120    avg_loss:0.181, val_acc:0.923]
Epoch [22/120    avg_loss:0.179, val_acc:0.928]
Epoch [23/120    avg_loss:0.184, val_acc:0.931]
Epoch [24/120    avg_loss:0.126, val_acc:0.942]
Epoch [25/120    avg_loss:0.100, val_acc:0.949]
Epoch [26/120    avg_loss:0.110, val_acc:0.935]
Epoch [27/120    avg_loss:0.107, val_acc:0.907]
Epoch [28/120    avg_loss:0.121, val_acc:0.955]
Epoch [29/120    avg_loss:0.106, val_acc:0.950]
Epoch [30/120    avg_loss:0.086, val_acc:0.951]
Epoch [31/120    avg_loss:0.081, val_acc:0.950]
Epoch [32/120    avg_loss:0.081, val_acc:0.955]
Epoch [33/120    avg_loss:0.062, val_acc:0.957]
Epoch [34/120    avg_loss:0.062, val_acc:0.959]
Epoch [35/120    avg_loss:0.061, val_acc:0.965]
Epoch [36/120    avg_loss:0.045, val_acc:0.965]
Epoch [37/120    avg_loss:0.045, val_acc:0.966]
Epoch [38/120    avg_loss:0.048, val_acc:0.954]
Epoch [39/120    avg_loss:0.065, val_acc:0.961]
Epoch [40/120    avg_loss:0.051, val_acc:0.967]
Epoch [41/120    avg_loss:0.040, val_acc:0.964]
Epoch [42/120    avg_loss:0.065, val_acc:0.965]
Epoch [43/120    avg_loss:0.056, val_acc:0.965]
Epoch [44/120    avg_loss:0.045, val_acc:0.972]
Epoch [45/120    avg_loss:0.039, val_acc:0.971]
Epoch [46/120    avg_loss:0.037, val_acc:0.966]
Epoch [47/120    avg_loss:0.031, val_acc:0.972]
Epoch [48/120    avg_loss:0.024, val_acc:0.961]
Epoch [49/120    avg_loss:0.027, val_acc:0.967]
Epoch [50/120    avg_loss:0.038, val_acc:0.973]
Epoch [51/120    avg_loss:0.027, val_acc:0.956]
Epoch [52/120    avg_loss:0.036, val_acc:0.975]
Epoch [53/120    avg_loss:0.023, val_acc:0.973]
Epoch [54/120    avg_loss:0.020, val_acc:0.970]
Epoch [55/120    avg_loss:0.023, val_acc:0.979]
Epoch [56/120    avg_loss:0.023, val_acc:0.960]
Epoch [57/120    avg_loss:0.024, val_acc:0.976]
Epoch [58/120    avg_loss:0.018, val_acc:0.980]
Epoch [59/120    avg_loss:0.017, val_acc:0.975]
Epoch [60/120    avg_loss:0.021, val_acc:0.974]
Epoch [61/120    avg_loss:0.021, val_acc:0.974]
Epoch [62/120    avg_loss:0.018, val_acc:0.976]
Epoch [63/120    avg_loss:0.015, val_acc:0.978]
Epoch [64/120    avg_loss:0.011, val_acc:0.978]
Epoch [65/120    avg_loss:0.018, val_acc:0.963]
Epoch [66/120    avg_loss:0.019, val_acc:0.972]
Epoch [67/120    avg_loss:0.046, val_acc:0.955]
Epoch [68/120    avg_loss:0.041, val_acc:0.972]
Epoch [69/120    avg_loss:0.027, val_acc:0.974]
Epoch [70/120    avg_loss:0.022, val_acc:0.971]
Epoch [71/120    avg_loss:0.016, val_acc:0.976]
Epoch [72/120    avg_loss:0.019, val_acc:0.979]
Epoch [73/120    avg_loss:0.011, val_acc:0.979]
Epoch [74/120    avg_loss:0.011, val_acc:0.979]
Epoch [75/120    avg_loss:0.010, val_acc:0.979]
Epoch [76/120    avg_loss:0.008, val_acc:0.980]
Epoch [77/120    avg_loss:0.013, val_acc:0.980]
Epoch [78/120    avg_loss:0.009, val_acc:0.981]
Epoch [79/120    avg_loss:0.009, val_acc:0.982]
Epoch [80/120    avg_loss:0.010, val_acc:0.982]
Epoch [81/120    avg_loss:0.008, val_acc:0.981]
Epoch [82/120    avg_loss:0.007, val_acc:0.981]
Epoch [83/120    avg_loss:0.010, val_acc:0.981]
Epoch [84/120    avg_loss:0.008, val_acc:0.981]
Epoch [85/120    avg_loss:0.010, val_acc:0.979]
Epoch [86/120    avg_loss:0.008, val_acc:0.983]
Epoch [87/120    avg_loss:0.006, val_acc:0.983]
Epoch [88/120    avg_loss:0.008, val_acc:0.982]
Epoch [89/120    avg_loss:0.008, val_acc:0.980]
Epoch [90/120    avg_loss:0.009, val_acc:0.981]
Epoch [91/120    avg_loss:0.009, val_acc:0.981]
Epoch [92/120    avg_loss:0.008, val_acc:0.981]
Epoch [93/120    avg_loss:0.007, val_acc:0.983]
Epoch [94/120    avg_loss:0.011, val_acc:0.981]
Epoch [95/120    avg_loss:0.008, val_acc:0.979]
Epoch [96/120    avg_loss:0.008, val_acc:0.981]
Epoch [97/120    avg_loss:0.008, val_acc:0.981]
Epoch [98/120    avg_loss:0.009, val_acc:0.983]
Epoch [99/120    avg_loss:0.012, val_acc:0.982]
Epoch [100/120    avg_loss:0.009, val_acc:0.982]
Epoch [101/120    avg_loss:0.008, val_acc:0.982]
Epoch [102/120    avg_loss:0.008, val_acc:0.981]
Epoch [103/120    avg_loss:0.007, val_acc:0.983]
Epoch [104/120    avg_loss:0.007, val_acc:0.983]
Epoch [105/120    avg_loss:0.008, val_acc:0.981]
Epoch [106/120    avg_loss:0.008, val_acc:0.981]
Epoch [107/120    avg_loss:0.007, val_acc:0.981]
Epoch [108/120    avg_loss:0.007, val_acc:0.981]
Epoch [109/120    avg_loss:0.008, val_acc:0.981]
Epoch [110/120    avg_loss:0.007, val_acc:0.981]
Epoch [111/120    avg_loss:0.006, val_acc:0.981]
Epoch [112/120    avg_loss:0.008, val_acc:0.981]
Epoch [113/120    avg_loss:0.008, val_acc:0.981]
Epoch [114/120    avg_loss:0.007, val_acc:0.981]
Epoch [115/120    avg_loss:0.008, val_acc:0.980]
Epoch [116/120    avg_loss:0.007, val_acc:0.981]
Epoch [117/120    avg_loss:0.007, val_acc:0.982]
Epoch [118/120    avg_loss:0.007, val_acc:0.982]
Epoch [119/120    avg_loss:0.007, val_acc:0.982]
Epoch [120/120    avg_loss:0.006, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    0    0    2    1    0    0    0    5   10    0    0
     0    0    0]
 [   0    0    0  726    8    0    0    0    0    6    1    0    4    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    1    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    2    2    0    0    0  861    5    0    0
     0    4    0]
 [   0    0   12    0    0    0    1    0    0    1    3 2191    2    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    3    1  523    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    2    0    0    0    0    0    0    0
  1126    8    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    14  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.5040650406504

F1 scores:
[       nan 1.         0.98791423 0.98574338 0.98156682 0.98401826
 0.97904192 1.         1.         0.8372093  0.98512586 0.99140271
 0.98215962 0.99462366 0.98598949 0.93313522 0.9704142 ]

Kappa:
0.982946484002508
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbb6d413710>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.588, val_acc:0.451]
Epoch [2/120    avg_loss:1.942, val_acc:0.532]
Epoch [3/120    avg_loss:1.694, val_acc:0.597]
Epoch [4/120    avg_loss:1.445, val_acc:0.632]
Epoch [5/120    avg_loss:1.211, val_acc:0.688]
Epoch [6/120    avg_loss:0.954, val_acc:0.746]
Epoch [7/120    avg_loss:0.822, val_acc:0.758]
Epoch [8/120    avg_loss:0.772, val_acc:0.766]
Epoch [9/120    avg_loss:0.613, val_acc:0.766]
Epoch [10/120    avg_loss:0.512, val_acc:0.804]
Epoch [11/120    avg_loss:0.491, val_acc:0.807]
Epoch [12/120    avg_loss:0.413, val_acc:0.816]
Epoch [13/120    avg_loss:0.363, val_acc:0.849]
Epoch [14/120    avg_loss:0.293, val_acc:0.867]
Epoch [15/120    avg_loss:0.250, val_acc:0.879]
Epoch [16/120    avg_loss:0.249, val_acc:0.892]
Epoch [17/120    avg_loss:0.191, val_acc:0.912]
Epoch [18/120    avg_loss:0.190, val_acc:0.875]
Epoch [19/120    avg_loss:0.190, val_acc:0.898]
Epoch [20/120    avg_loss:0.202, val_acc:0.924]
Epoch [21/120    avg_loss:0.151, val_acc:0.917]
Epoch [22/120    avg_loss:0.127, val_acc:0.928]
Epoch [23/120    avg_loss:0.091, val_acc:0.923]
Epoch [24/120    avg_loss:0.088, val_acc:0.936]
Epoch [25/120    avg_loss:0.073, val_acc:0.942]
Epoch [26/120    avg_loss:0.062, val_acc:0.949]
Epoch [27/120    avg_loss:0.079, val_acc:0.948]
Epoch [28/120    avg_loss:0.099, val_acc:0.946]
Epoch [29/120    avg_loss:0.112, val_acc:0.929]
Epoch [30/120    avg_loss:0.069, val_acc:0.946]
Epoch [31/120    avg_loss:0.070, val_acc:0.944]
Epoch [32/120    avg_loss:0.059, val_acc:0.958]
Epoch [33/120    avg_loss:0.054, val_acc:0.933]
Epoch [34/120    avg_loss:0.046, val_acc:0.957]
Epoch [35/120    avg_loss:0.052, val_acc:0.954]
Epoch [36/120    avg_loss:0.061, val_acc:0.936]
Epoch [37/120    avg_loss:0.064, val_acc:0.949]
Epoch [38/120    avg_loss:0.047, val_acc:0.953]
Epoch [39/120    avg_loss:0.033, val_acc:0.960]
Epoch [40/120    avg_loss:0.039, val_acc:0.972]
Epoch [41/120    avg_loss:0.031, val_acc:0.971]
Epoch [42/120    avg_loss:0.034, val_acc:0.980]
Epoch [43/120    avg_loss:0.032, val_acc:0.968]
Epoch [44/120    avg_loss:0.036, val_acc:0.972]
Epoch [45/120    avg_loss:0.027, val_acc:0.974]
Epoch [46/120    avg_loss:0.032, val_acc:0.968]
Epoch [47/120    avg_loss:0.031, val_acc:0.958]
Epoch [48/120    avg_loss:0.044, val_acc:0.963]
Epoch [49/120    avg_loss:0.033, val_acc:0.970]
Epoch [50/120    avg_loss:0.025, val_acc:0.978]
Epoch [51/120    avg_loss:0.028, val_acc:0.977]
Epoch [52/120    avg_loss:0.030, val_acc:0.974]
Epoch [53/120    avg_loss:0.030, val_acc:0.974]
Epoch [54/120    avg_loss:0.025, val_acc:0.970]
Epoch [55/120    avg_loss:0.028, val_acc:0.975]
Epoch [56/120    avg_loss:0.017, val_acc:0.979]
Epoch [57/120    avg_loss:0.015, val_acc:0.979]
Epoch [58/120    avg_loss:0.012, val_acc:0.980]
Epoch [59/120    avg_loss:0.014, val_acc:0.982]
Epoch [60/120    avg_loss:0.012, val_acc:0.981]
Epoch [61/120    avg_loss:0.014, val_acc:0.978]
Epoch [62/120    avg_loss:0.013, val_acc:0.981]
Epoch [63/120    avg_loss:0.011, val_acc:0.977]
Epoch [64/120    avg_loss:0.009, val_acc:0.977]
Epoch [65/120    avg_loss:0.012, val_acc:0.979]
Epoch [66/120    avg_loss:0.013, val_acc:0.980]
Epoch [67/120    avg_loss:0.014, val_acc:0.981]
Epoch [68/120    avg_loss:0.010, val_acc:0.981]
Epoch [69/120    avg_loss:0.014, val_acc:0.981]
Epoch [70/120    avg_loss:0.009, val_acc:0.981]
Epoch [71/120    avg_loss:0.011, val_acc:0.980]
Epoch [72/120    avg_loss:0.010, val_acc:0.981]
Epoch [73/120    avg_loss:0.009, val_acc:0.981]
Epoch [74/120    avg_loss:0.011, val_acc:0.981]
Epoch [75/120    avg_loss:0.009, val_acc:0.981]
Epoch [76/120    avg_loss:0.011, val_acc:0.981]
Epoch [77/120    avg_loss:0.009, val_acc:0.981]
Epoch [78/120    avg_loss:0.009, val_acc:0.981]
Epoch [79/120    avg_loss:0.009, val_acc:0.980]
Epoch [80/120    avg_loss:0.013, val_acc:0.981]
Epoch [81/120    avg_loss:0.010, val_acc:0.980]
Epoch [82/120    avg_loss:0.010, val_acc:0.980]
Epoch [83/120    avg_loss:0.013, val_acc:0.981]
Epoch [84/120    avg_loss:0.009, val_acc:0.981]
Epoch [85/120    avg_loss:0.009, val_acc:0.980]
Epoch [86/120    avg_loss:0.013, val_acc:0.980]
Epoch [87/120    avg_loss:0.010, val_acc:0.981]
Epoch [88/120    avg_loss:0.012, val_acc:0.981]
Epoch [89/120    avg_loss:0.012, val_acc:0.981]
Epoch [90/120    avg_loss:0.010, val_acc:0.981]
Epoch [91/120    avg_loss:0.009, val_acc:0.981]
Epoch [92/120    avg_loss:0.009, val_acc:0.981]
Epoch [93/120    avg_loss:0.015, val_acc:0.980]
Epoch [94/120    avg_loss:0.010, val_acc:0.980]
Epoch [95/120    avg_loss:0.012, val_acc:0.980]
Epoch [96/120    avg_loss:0.010, val_acc:0.980]
Epoch [97/120    avg_loss:0.012, val_acc:0.980]
Epoch [98/120    avg_loss:0.009, val_acc:0.980]
Epoch [99/120    avg_loss:0.010, val_acc:0.980]
Epoch [100/120    avg_loss:0.012, val_acc:0.980]
Epoch [101/120    avg_loss:0.009, val_acc:0.980]
Epoch [102/120    avg_loss:0.014, val_acc:0.980]
Epoch [103/120    avg_loss:0.009, val_acc:0.980]
Epoch [104/120    avg_loss:0.009, val_acc:0.980]
Epoch [105/120    avg_loss:0.011, val_acc:0.980]
Epoch [106/120    avg_loss:0.009, val_acc:0.980]
Epoch [107/120    avg_loss:0.011, val_acc:0.980]
Epoch [108/120    avg_loss:0.009, val_acc:0.980]
Epoch [109/120    avg_loss:0.011, val_acc:0.980]
Epoch [110/120    avg_loss:0.012, val_acc:0.980]
Epoch [111/120    avg_loss:0.010, val_acc:0.980]
Epoch [112/120    avg_loss:0.009, val_acc:0.980]
Epoch [113/120    avg_loss:0.009, val_acc:0.980]
Epoch [114/120    avg_loss:0.010, val_acc:0.980]
Epoch [115/120    avg_loss:0.010, val_acc:0.980]
Epoch [116/120    avg_loss:0.010, val_acc:0.980]
Epoch [117/120    avg_loss:0.011, val_acc:0.980]
Epoch [118/120    avg_loss:0.012, val_acc:0.980]
Epoch [119/120    avg_loss:0.011, val_acc:0.980]
Epoch [120/120    avg_loss:0.011, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1260    3    1    0    0    0    0    5    3   12    1    0
     0    0    0]
 [   0    0    0  725    0    0    1    0    0    4    1    6    8    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    3    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    2    4    0    0    1  849   13    0    0
     0    0    0]
 [   0    0    7    0    0    0    6    0    0    0    7 2186    2    2
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    7    0  521    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    0    0    0    0
  1111   22    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    14  331    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.1680216802168

F1 scores:
[       nan 0.98765432 0.98514464 0.9790682  0.99765808 0.98623853
 0.98944193 0.94339623 0.99650757 0.66666667 0.97418244 0.98735321
 0.9729225  0.98930481 0.98101545 0.94571429 0.97005988]

Kappa:
0.9791180668338658
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3c73fc46d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.503, val_acc:0.367]
Epoch [2/120    avg_loss:1.935, val_acc:0.506]
Epoch [3/120    avg_loss:1.705, val_acc:0.635]
Epoch [4/120    avg_loss:1.503, val_acc:0.640]
Epoch [5/120    avg_loss:1.261, val_acc:0.698]
Epoch [6/120    avg_loss:1.070, val_acc:0.707]
Epoch [7/120    avg_loss:0.913, val_acc:0.759]
Epoch [8/120    avg_loss:0.784, val_acc:0.732]
Epoch [9/120    avg_loss:0.750, val_acc:0.806]
Epoch [10/120    avg_loss:0.635, val_acc:0.803]
Epoch [11/120    avg_loss:0.523, val_acc:0.814]
Epoch [12/120    avg_loss:0.401, val_acc:0.847]
Epoch [13/120    avg_loss:0.353, val_acc:0.870]
Epoch [14/120    avg_loss:0.325, val_acc:0.850]
Epoch [15/120    avg_loss:0.310, val_acc:0.879]
Epoch [16/120    avg_loss:0.267, val_acc:0.877]
Epoch [17/120    avg_loss:0.282, val_acc:0.895]
Epoch [18/120    avg_loss:0.209, val_acc:0.876]
Epoch [19/120    avg_loss:0.172, val_acc:0.922]
Epoch [20/120    avg_loss:0.170, val_acc:0.918]
Epoch [21/120    avg_loss:0.146, val_acc:0.928]
Epoch [22/120    avg_loss:0.122, val_acc:0.940]
Epoch [23/120    avg_loss:0.115, val_acc:0.894]
Epoch [24/120    avg_loss:0.144, val_acc:0.939]
Epoch [25/120    avg_loss:0.118, val_acc:0.949]
Epoch [26/120    avg_loss:0.141, val_acc:0.938]
Epoch [27/120    avg_loss:0.130, val_acc:0.942]
Epoch [28/120    avg_loss:0.121, val_acc:0.944]
Epoch [29/120    avg_loss:0.125, val_acc:0.950]
Epoch [30/120    avg_loss:0.104, val_acc:0.953]
Epoch [31/120    avg_loss:0.076, val_acc:0.964]
Epoch [32/120    avg_loss:0.072, val_acc:0.961]
Epoch [33/120    avg_loss:0.073, val_acc:0.938]
Epoch [34/120    avg_loss:0.092, val_acc:0.953]
Epoch [35/120    avg_loss:0.092, val_acc:0.961]
Epoch [36/120    avg_loss:0.214, val_acc:0.904]
Epoch [37/120    avg_loss:0.182, val_acc:0.938]
Epoch [38/120    avg_loss:0.127, val_acc:0.960]
Epoch [39/120    avg_loss:0.085, val_acc:0.939]
Epoch [40/120    avg_loss:0.105, val_acc:0.938]
Epoch [41/120    avg_loss:0.067, val_acc:0.957]
Epoch [42/120    avg_loss:0.050, val_acc:0.947]
Epoch [43/120    avg_loss:0.039, val_acc:0.972]
Epoch [44/120    avg_loss:0.034, val_acc:0.961]
Epoch [45/120    avg_loss:0.041, val_acc:0.969]
Epoch [46/120    avg_loss:0.045, val_acc:0.969]
Epoch [47/120    avg_loss:0.050, val_acc:0.952]
Epoch [48/120    avg_loss:0.066, val_acc:0.934]
Epoch [49/120    avg_loss:0.040, val_acc:0.966]
Epoch [50/120    avg_loss:0.038, val_acc:0.970]
Epoch [51/120    avg_loss:0.035, val_acc:0.944]
Epoch [52/120    avg_loss:0.032, val_acc:0.978]
Epoch [53/120    avg_loss:0.032, val_acc:0.958]
Epoch [54/120    avg_loss:0.030, val_acc:0.966]
Epoch [55/120    avg_loss:0.027, val_acc:0.976]
Epoch [56/120    avg_loss:0.024, val_acc:0.980]
Epoch [57/120    avg_loss:0.019, val_acc:0.973]
Epoch [58/120    avg_loss:0.046, val_acc:0.972]
Epoch [59/120    avg_loss:0.042, val_acc:0.963]
Epoch [60/120    avg_loss:0.021, val_acc:0.977]
Epoch [61/120    avg_loss:0.031, val_acc:0.977]
Epoch [62/120    avg_loss:0.031, val_acc:0.981]
Epoch [63/120    avg_loss:0.025, val_acc:0.964]
Epoch [64/120    avg_loss:0.026, val_acc:0.977]
Epoch [65/120    avg_loss:0.021, val_acc:0.976]
Epoch [66/120    avg_loss:0.018, val_acc:0.976]
Epoch [67/120    avg_loss:0.021, val_acc:0.972]
Epoch [68/120    avg_loss:0.013, val_acc:0.978]
Epoch [69/120    avg_loss:0.017, val_acc:0.981]
Epoch [70/120    avg_loss:0.016, val_acc:0.980]
Epoch [71/120    avg_loss:0.012, val_acc:0.980]
Epoch [72/120    avg_loss:0.014, val_acc:0.985]
Epoch [73/120    avg_loss:0.020, val_acc:0.978]
Epoch [74/120    avg_loss:0.015, val_acc:0.978]
Epoch [75/120    avg_loss:0.017, val_acc:0.981]
Epoch [76/120    avg_loss:0.013, val_acc:0.977]
Epoch [77/120    avg_loss:0.017, val_acc:0.971]
Epoch [78/120    avg_loss:0.015, val_acc:0.976]
Epoch [79/120    avg_loss:0.016, val_acc:0.973]
Epoch [80/120    avg_loss:0.012, val_acc:0.981]
Epoch [81/120    avg_loss:0.012, val_acc:0.980]
Epoch [82/120    avg_loss:0.011, val_acc:0.980]
Epoch [83/120    avg_loss:0.009, val_acc:0.978]
Epoch [84/120    avg_loss:0.011, val_acc:0.981]
Epoch [85/120    avg_loss:0.009, val_acc:0.975]
Epoch [86/120    avg_loss:0.013, val_acc:0.981]
Epoch [87/120    avg_loss:0.008, val_acc:0.983]
Epoch [88/120    avg_loss:0.006, val_acc:0.984]
Epoch [89/120    avg_loss:0.009, val_acc:0.984]
Epoch [90/120    avg_loss:0.007, val_acc:0.984]
Epoch [91/120    avg_loss:0.006, val_acc:0.983]
Epoch [92/120    avg_loss:0.006, val_acc:0.981]
Epoch [93/120    avg_loss:0.005, val_acc:0.982]
Epoch [94/120    avg_loss:0.006, val_acc:0.984]
Epoch [95/120    avg_loss:0.007, val_acc:0.983]
Epoch [96/120    avg_loss:0.005, val_acc:0.985]
Epoch [97/120    avg_loss:0.005, val_acc:0.987]
Epoch [98/120    avg_loss:0.006, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.005, val_acc:0.986]
Epoch [102/120    avg_loss:0.006, val_acc:0.984]
Epoch [103/120    avg_loss:0.005, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.005, val_acc:0.986]
Epoch [106/120    avg_loss:0.005, val_acc:0.985]
Epoch [107/120    avg_loss:0.006, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.007, val_acc:0.985]
Epoch [112/120    avg_loss:0.005, val_acc:0.985]
Epoch [113/120    avg_loss:0.006, val_acc:0.985]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.984]
Epoch [117/120    avg_loss:0.005, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.985]
Epoch [119/120    avg_loss:0.005, val_acc:0.985]
Epoch [120/120    avg_loss:0.006, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1271    0    0    0    0    0    0    0    5    6    1    0
     0    2    0]
 [   0    0    1  716    5    8    0    0    0    8    1    0    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    8    0    0    0    2  853    7    0    0
     0    5    0]
 [   0    0   13    0    0    0    6    0    0    0    9 2181    1    0
     0    0    0]
 [   0    0    0    0    2    6    0    0    0    0    0    0  525    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    2    0    0    0
  1131    2    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    22  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.21138211382114

F1 scores:
[       nan 0.975      0.98910506 0.97881066 0.98383372 0.96636771
 0.97840655 0.98039216 1.         0.72340426 0.9765312  0.99046322
 0.98130841 1.         0.98648059 0.91793313 0.99408284]

Kappa:
0.9796149983149604
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7582ddd748>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.502, val_acc:0.498]
Epoch [2/120    avg_loss:1.940, val_acc:0.490]
Epoch [3/120    avg_loss:1.626, val_acc:0.577]
Epoch [4/120    avg_loss:1.444, val_acc:0.665]
Epoch [5/120    avg_loss:1.237, val_acc:0.669]
Epoch [6/120    avg_loss:1.101, val_acc:0.697]
Epoch [7/120    avg_loss:0.987, val_acc:0.717]
Epoch [8/120    avg_loss:0.821, val_acc:0.768]
Epoch [9/120    avg_loss:0.620, val_acc:0.769]
Epoch [10/120    avg_loss:0.570, val_acc:0.841]
Epoch [11/120    avg_loss:0.567, val_acc:0.800]
Epoch [12/120    avg_loss:0.437, val_acc:0.825]
Epoch [13/120    avg_loss:0.516, val_acc:0.818]
Epoch [14/120    avg_loss:0.394, val_acc:0.856]
Epoch [15/120    avg_loss:0.297, val_acc:0.885]
Epoch [16/120    avg_loss:0.328, val_acc:0.861]
Epoch [17/120    avg_loss:0.254, val_acc:0.892]
Epoch [18/120    avg_loss:0.209, val_acc:0.914]
Epoch [19/120    avg_loss:0.186, val_acc:0.916]
Epoch [20/120    avg_loss:0.258, val_acc:0.853]
Epoch [21/120    avg_loss:0.261, val_acc:0.916]
Epoch [22/120    avg_loss:0.221, val_acc:0.893]
Epoch [23/120    avg_loss:0.173, val_acc:0.930]
Epoch [24/120    avg_loss:0.156, val_acc:0.942]
Epoch [25/120    avg_loss:0.140, val_acc:0.935]
Epoch [26/120    avg_loss:0.126, val_acc:0.948]
Epoch [27/120    avg_loss:0.140, val_acc:0.934]
Epoch [28/120    avg_loss:0.104, val_acc:0.934]
Epoch [29/120    avg_loss:0.096, val_acc:0.932]
Epoch [30/120    avg_loss:0.113, val_acc:0.930]
Epoch [31/120    avg_loss:0.093, val_acc:0.968]
Epoch [32/120    avg_loss:0.091, val_acc:0.966]
Epoch [33/120    avg_loss:0.082, val_acc:0.950]
Epoch [34/120    avg_loss:0.105, val_acc:0.943]
Epoch [35/120    avg_loss:0.097, val_acc:0.956]
Epoch [36/120    avg_loss:0.082, val_acc:0.950]
Epoch [37/120    avg_loss:0.084, val_acc:0.952]
Epoch [38/120    avg_loss:0.075, val_acc:0.955]
Epoch [39/120    avg_loss:0.051, val_acc:0.971]
Epoch [40/120    avg_loss:0.065, val_acc:0.957]
Epoch [41/120    avg_loss:0.042, val_acc:0.949]
Epoch [42/120    avg_loss:0.047, val_acc:0.964]
Epoch [43/120    avg_loss:0.043, val_acc:0.959]
Epoch [44/120    avg_loss:0.038, val_acc:0.970]
Epoch [45/120    avg_loss:0.054, val_acc:0.960]
Epoch [46/120    avg_loss:0.052, val_acc:0.966]
Epoch [47/120    avg_loss:0.045, val_acc:0.969]
Epoch [48/120    avg_loss:0.038, val_acc:0.976]
Epoch [49/120    avg_loss:0.042, val_acc:0.969]
Epoch [50/120    avg_loss:0.038, val_acc:0.980]
Epoch [51/120    avg_loss:0.032, val_acc:0.978]
Epoch [52/120    avg_loss:0.035, val_acc:0.961]
Epoch [53/120    avg_loss:0.040, val_acc:0.958]
Epoch [54/120    avg_loss:0.029, val_acc:0.972]
Epoch [55/120    avg_loss:0.026, val_acc:0.970]
Epoch [56/120    avg_loss:0.024, val_acc:0.976]
Epoch [57/120    avg_loss:0.023, val_acc:0.974]
Epoch [58/120    avg_loss:0.025, val_acc:0.974]
Epoch [59/120    avg_loss:0.023, val_acc:0.968]
Epoch [60/120    avg_loss:0.017, val_acc:0.970]
Epoch [61/120    avg_loss:0.020, val_acc:0.976]
Epoch [62/120    avg_loss:0.030, val_acc:0.980]
Epoch [63/120    avg_loss:0.038, val_acc:0.973]
Epoch [64/120    avg_loss:0.015, val_acc:0.969]
Epoch [65/120    avg_loss:0.025, val_acc:0.981]
Epoch [66/120    avg_loss:0.028, val_acc:0.972]
Epoch [67/120    avg_loss:0.025, val_acc:0.980]
Epoch [68/120    avg_loss:0.025, val_acc:0.984]
Epoch [69/120    avg_loss:0.031, val_acc:0.980]
Epoch [70/120    avg_loss:0.017, val_acc:0.980]
Epoch [71/120    avg_loss:0.018, val_acc:0.976]
Epoch [72/120    avg_loss:0.016, val_acc:0.969]
Epoch [73/120    avg_loss:0.024, val_acc:0.977]
Epoch [74/120    avg_loss:0.030, val_acc:0.976]
Epoch [75/120    avg_loss:0.018, val_acc:0.972]
Epoch [76/120    avg_loss:0.021, val_acc:0.977]
Epoch [77/120    avg_loss:0.015, val_acc:0.978]
Epoch [78/120    avg_loss:0.015, val_acc:0.982]
Epoch [79/120    avg_loss:0.020, val_acc:0.983]
Epoch [80/120    avg_loss:0.039, val_acc:0.966]
Epoch [81/120    avg_loss:0.032, val_acc:0.956]
Epoch [82/120    avg_loss:0.025, val_acc:0.971]
Epoch [83/120    avg_loss:0.015, val_acc:0.977]
Epoch [84/120    avg_loss:0.014, val_acc:0.978]
Epoch [85/120    avg_loss:0.014, val_acc:0.981]
Epoch [86/120    avg_loss:0.011, val_acc:0.980]
Epoch [87/120    avg_loss:0.011, val_acc:0.981]
Epoch [88/120    avg_loss:0.010, val_acc:0.982]
Epoch [89/120    avg_loss:0.013, val_acc:0.983]
Epoch [90/120    avg_loss:0.011, val_acc:0.981]
Epoch [91/120    avg_loss:0.010, val_acc:0.981]
Epoch [92/120    avg_loss:0.009, val_acc:0.981]
Epoch [93/120    avg_loss:0.009, val_acc:0.982]
Epoch [94/120    avg_loss:0.009, val_acc:0.982]
Epoch [95/120    avg_loss:0.009, val_acc:0.982]
Epoch [96/120    avg_loss:0.007, val_acc:0.982]
Epoch [97/120    avg_loss:0.010, val_acc:0.982]
Epoch [98/120    avg_loss:0.009, val_acc:0.982]
Epoch [99/120    avg_loss:0.008, val_acc:0.982]
Epoch [100/120    avg_loss:0.010, val_acc:0.982]
Epoch [101/120    avg_loss:0.011, val_acc:0.982]
Epoch [102/120    avg_loss:0.011, val_acc:0.982]
Epoch [103/120    avg_loss:0.009, val_acc:0.982]
Epoch [104/120    avg_loss:0.009, val_acc:0.982]
Epoch [105/120    avg_loss:0.009, val_acc:0.982]
Epoch [106/120    avg_loss:0.010, val_acc:0.982]
Epoch [107/120    avg_loss:0.008, val_acc:0.982]
Epoch [108/120    avg_loss:0.011, val_acc:0.982]
Epoch [109/120    avg_loss:0.008, val_acc:0.982]
Epoch [110/120    avg_loss:0.011, val_acc:0.982]
Epoch [111/120    avg_loss:0.009, val_acc:0.982]
Epoch [112/120    avg_loss:0.009, val_acc:0.982]
Epoch [113/120    avg_loss:0.010, val_acc:0.982]
Epoch [114/120    avg_loss:0.007, val_acc:0.982]
Epoch [115/120    avg_loss:0.009, val_acc:0.982]
Epoch [116/120    avg_loss:0.010, val_acc:0.982]
Epoch [117/120    avg_loss:0.011, val_acc:0.982]
Epoch [118/120    avg_loss:0.008, val_acc:0.982]
Epoch [119/120    avg_loss:0.009, val_acc:0.982]
Epoch [120/120    avg_loss:0.010, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    2    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1267    4    0    0    0    0    0    0    3   10    1    0
     0    0    0]
 [   0    0    1  690    0   11    0    0    0   18    1   10   16    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    2    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    5    3    0    0    0  852   11    0    0
     0    3    0]
 [   0    0    9    0    0    0    2    0    0    0    7 2190    0    2
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    1    4  522    0
     0    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   27    0    0    0    0    0    0    0
    19  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.94037940379404

F1 scores:
[       nan 0.94871795 0.98791423 0.95766829 1.         0.97072072
 0.97470238 1.         0.99883856 0.64285714 0.97818599 0.98693105
 0.97206704 0.99191375 0.99041812 0.92473118 0.98224852]

Kappa:
0.976510907797028
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:00:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3ad4df6748>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.503, val_acc:0.442]
Epoch [2/120    avg_loss:2.014, val_acc:0.517]
Epoch [3/120    avg_loss:1.733, val_acc:0.543]
Epoch [4/120    avg_loss:1.604, val_acc:0.603]
Epoch [5/120    avg_loss:1.389, val_acc:0.648]
Epoch [6/120    avg_loss:1.222, val_acc:0.702]
Epoch [7/120    avg_loss:1.046, val_acc:0.735]
Epoch [8/120    avg_loss:1.011, val_acc:0.754]
Epoch [9/120    avg_loss:0.821, val_acc:0.793]
Epoch [10/120    avg_loss:0.700, val_acc:0.780]
Epoch [11/120    avg_loss:0.590, val_acc:0.825]
Epoch [12/120    avg_loss:0.496, val_acc:0.863]
Epoch [13/120    avg_loss:0.421, val_acc:0.809]
Epoch [14/120    avg_loss:0.426, val_acc:0.845]
Epoch [15/120    avg_loss:0.395, val_acc:0.869]
Epoch [16/120    avg_loss:0.338, val_acc:0.872]
Epoch [17/120    avg_loss:0.320, val_acc:0.870]
Epoch [18/120    avg_loss:0.284, val_acc:0.885]
Epoch [19/120    avg_loss:0.232, val_acc:0.867]
Epoch [20/120    avg_loss:0.205, val_acc:0.915]
Epoch [21/120    avg_loss:0.203, val_acc:0.914]
Epoch [22/120    avg_loss:0.164, val_acc:0.916]
Epoch [23/120    avg_loss:0.184, val_acc:0.921]
Epoch [24/120    avg_loss:0.183, val_acc:0.928]
Epoch [25/120    avg_loss:0.153, val_acc:0.932]
Epoch [26/120    avg_loss:0.162, val_acc:0.946]
Epoch [27/120    avg_loss:0.171, val_acc:0.905]
Epoch [28/120    avg_loss:0.145, val_acc:0.931]
Epoch [29/120    avg_loss:0.125, val_acc:0.946]
Epoch [30/120    avg_loss:0.105, val_acc:0.941]
Epoch [31/120    avg_loss:0.098, val_acc:0.946]
Epoch [32/120    avg_loss:0.069, val_acc:0.952]
Epoch [33/120    avg_loss:0.077, val_acc:0.960]
Epoch [34/120    avg_loss:0.094, val_acc:0.950]
Epoch [35/120    avg_loss:0.083, val_acc:0.959]
Epoch [36/120    avg_loss:0.075, val_acc:0.943]
Epoch [37/120    avg_loss:0.052, val_acc:0.961]
Epoch [38/120    avg_loss:0.057, val_acc:0.971]
Epoch [39/120    avg_loss:0.064, val_acc:0.960]
Epoch [40/120    avg_loss:0.057, val_acc:0.968]
Epoch [41/120    avg_loss:0.068, val_acc:0.969]
Epoch [42/120    avg_loss:0.066, val_acc:0.963]
Epoch [43/120    avg_loss:0.048, val_acc:0.966]
Epoch [44/120    avg_loss:0.056, val_acc:0.961]
Epoch [45/120    avg_loss:0.053, val_acc:0.959]
Epoch [46/120    avg_loss:0.042, val_acc:0.967]
Epoch [47/120    avg_loss:0.047, val_acc:0.969]
Epoch [48/120    avg_loss:0.058, val_acc:0.958]
Epoch [49/120    avg_loss:0.055, val_acc:0.962]
Epoch [50/120    avg_loss:0.039, val_acc:0.966]
Epoch [51/120    avg_loss:0.037, val_acc:0.974]
Epoch [52/120    avg_loss:0.026, val_acc:0.966]
Epoch [53/120    avg_loss:0.026, val_acc:0.978]
Epoch [54/120    avg_loss:0.025, val_acc:0.977]
Epoch [55/120    avg_loss:0.027, val_acc:0.980]
Epoch [56/120    avg_loss:0.029, val_acc:0.968]
Epoch [57/120    avg_loss:0.023, val_acc:0.976]
Epoch [58/120    avg_loss:0.029, val_acc:0.972]
Epoch [59/120    avg_loss:0.027, val_acc:0.972]
Epoch [60/120    avg_loss:0.025, val_acc:0.981]
Epoch [61/120    avg_loss:0.029, val_acc:0.986]
Epoch [62/120    avg_loss:0.036, val_acc:0.974]
Epoch [63/120    avg_loss:0.029, val_acc:0.975]
Epoch [64/120    avg_loss:0.024, val_acc:0.976]
Epoch [65/120    avg_loss:0.029, val_acc:0.978]
Epoch [66/120    avg_loss:0.037, val_acc:0.980]
Epoch [67/120    avg_loss:0.026, val_acc:0.978]
Epoch [68/120    avg_loss:0.044, val_acc:0.966]
Epoch [69/120    avg_loss:0.043, val_acc:0.962]
Epoch [70/120    avg_loss:0.035, val_acc:0.973]
Epoch [71/120    avg_loss:0.034, val_acc:0.974]
Epoch [72/120    avg_loss:0.043, val_acc:0.957]
Epoch [73/120    avg_loss:0.054, val_acc:0.975]
Epoch [74/120    avg_loss:0.033, val_acc:0.976]
Epoch [75/120    avg_loss:0.028, val_acc:0.981]
Epoch [76/120    avg_loss:0.024, val_acc:0.983]
Epoch [77/120    avg_loss:0.019, val_acc:0.985]
Epoch [78/120    avg_loss:0.020, val_acc:0.985]
Epoch [79/120    avg_loss:0.019, val_acc:0.984]
Epoch [80/120    avg_loss:0.014, val_acc:0.986]
Epoch [81/120    avg_loss:0.017, val_acc:0.987]
Epoch [82/120    avg_loss:0.016, val_acc:0.985]
Epoch [83/120    avg_loss:0.014, val_acc:0.984]
Epoch [84/120    avg_loss:0.016, val_acc:0.984]
Epoch [85/120    avg_loss:0.013, val_acc:0.986]
Epoch [86/120    avg_loss:0.012, val_acc:0.986]
Epoch [87/120    avg_loss:0.015, val_acc:0.985]
Epoch [88/120    avg_loss:0.016, val_acc:0.983]
Epoch [89/120    avg_loss:0.014, val_acc:0.985]
Epoch [90/120    avg_loss:0.012, val_acc:0.985]
Epoch [91/120    avg_loss:0.011, val_acc:0.984]
Epoch [92/120    avg_loss:0.013, val_acc:0.984]
Epoch [93/120    avg_loss:0.016, val_acc:0.983]
Epoch [94/120    avg_loss:0.017, val_acc:0.983]
Epoch [95/120    avg_loss:0.011, val_acc:0.982]
Epoch [96/120    avg_loss:0.013, val_acc:0.983]
Epoch [97/120    avg_loss:0.013, val_acc:0.982]
Epoch [98/120    avg_loss:0.010, val_acc:0.983]
Epoch [99/120    avg_loss:0.015, val_acc:0.983]
Epoch [100/120    avg_loss:0.012, val_acc:0.983]
Epoch [101/120    avg_loss:0.013, val_acc:0.983]
Epoch [102/120    avg_loss:0.012, val_acc:0.983]
Epoch [103/120    avg_loss:0.010, val_acc:0.984]
Epoch [104/120    avg_loss:0.011, val_acc:0.983]
Epoch [105/120    avg_loss:0.011, val_acc:0.983]
Epoch [106/120    avg_loss:0.012, val_acc:0.984]
Epoch [107/120    avg_loss:0.012, val_acc:0.984]
Epoch [108/120    avg_loss:0.010, val_acc:0.984]
Epoch [109/120    avg_loss:0.011, val_acc:0.984]
Epoch [110/120    avg_loss:0.013, val_acc:0.984]
Epoch [111/120    avg_loss:0.015, val_acc:0.984]
Epoch [112/120    avg_loss:0.012, val_acc:0.984]
Epoch [113/120    avg_loss:0.012, val_acc:0.984]
Epoch [114/120    avg_loss:0.010, val_acc:0.984]
Epoch [115/120    avg_loss:0.015, val_acc:0.984]
Epoch [116/120    avg_loss:0.012, val_acc:0.984]
Epoch [117/120    avg_loss:0.013, val_acc:0.984]
Epoch [118/120    avg_loss:0.010, val_acc:0.984]
Epoch [119/120    avg_loss:0.012, val_acc:0.984]
Epoch [120/120    avg_loss:0.010, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1257    2    0    0    1    0    0    3    6   13    3    0
     0    0    0]
 [   0    0    5  719    0    3    0    0    0    6    2    0    9    2
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    2    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0    6    0    0    6    0    0    0    0  841   18    0    0
     1    3    0]
 [   0    0    3    0    0    0    1    0    0    0   12 2192    0    2
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0  529    0
     0    0    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    1    0    0    0    0    0    0    1    0    1    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    53  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.03794037940379

F1 scores:
[       nan 0.97560976 0.98318342 0.97889721 1.         0.98293515
 0.99619193 0.96153846 0.99883856 0.72727273 0.96833621 0.98827773
 0.98144712 0.98659517 0.9751073  0.91304348 0.9704142 ]

Kappa:
0.9776186420074093
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff0583606d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.495, val_acc:0.363]
Epoch [2/120    avg_loss:1.966, val_acc:0.536]
Epoch [3/120    avg_loss:1.663, val_acc:0.615]
Epoch [4/120    avg_loss:1.476, val_acc:0.663]
Epoch [5/120    avg_loss:1.288, val_acc:0.686]
Epoch [6/120    avg_loss:1.087, val_acc:0.764]
Epoch [7/120    avg_loss:0.864, val_acc:0.796]
Epoch [8/120    avg_loss:0.726, val_acc:0.779]
Epoch [9/120    avg_loss:0.681, val_acc:0.791]
Epoch [10/120    avg_loss:0.573, val_acc:0.847]
Epoch [11/120    avg_loss:0.501, val_acc:0.831]
Epoch [12/120    avg_loss:0.424, val_acc:0.849]
Epoch [13/120    avg_loss:0.353, val_acc:0.883]
Epoch [14/120    avg_loss:0.349, val_acc:0.878]
Epoch [15/120    avg_loss:0.304, val_acc:0.876]
Epoch [16/120    avg_loss:0.327, val_acc:0.883]
Epoch [17/120    avg_loss:0.292, val_acc:0.897]
Epoch [18/120    avg_loss:0.206, val_acc:0.922]
Epoch [19/120    avg_loss:0.180, val_acc:0.936]
Epoch [20/120    avg_loss:0.159, val_acc:0.935]
Epoch [21/120    avg_loss:0.157, val_acc:0.922]
Epoch [22/120    avg_loss:0.156, val_acc:0.922]
Epoch [23/120    avg_loss:0.128, val_acc:0.942]
Epoch [24/120    avg_loss:0.124, val_acc:0.913]
Epoch [25/120    avg_loss:0.130, val_acc:0.922]
Epoch [26/120    avg_loss:0.122, val_acc:0.934]
Epoch [27/120    avg_loss:0.116, val_acc:0.947]
Epoch [28/120    avg_loss:0.119, val_acc:0.936]
Epoch [29/120    avg_loss:0.130, val_acc:0.925]
Epoch [30/120    avg_loss:0.109, val_acc:0.907]
Epoch [31/120    avg_loss:0.137, val_acc:0.908]
Epoch [32/120    avg_loss:0.117, val_acc:0.949]
Epoch [33/120    avg_loss:0.069, val_acc:0.959]
Epoch [34/120    avg_loss:0.074, val_acc:0.952]
Epoch [35/120    avg_loss:0.072, val_acc:0.949]
Epoch [36/120    avg_loss:0.074, val_acc:0.950]
Epoch [37/120    avg_loss:0.082, val_acc:0.950]
Epoch [38/120    avg_loss:0.084, val_acc:0.948]
Epoch [39/120    avg_loss:0.066, val_acc:0.952]
Epoch [40/120    avg_loss:0.057, val_acc:0.967]
Epoch [41/120    avg_loss:0.044, val_acc:0.968]
Epoch [42/120    avg_loss:0.045, val_acc:0.966]
Epoch [43/120    avg_loss:0.037, val_acc:0.974]
Epoch [44/120    avg_loss:0.035, val_acc:0.969]
Epoch [45/120    avg_loss:0.035, val_acc:0.960]
Epoch [46/120    avg_loss:0.044, val_acc:0.964]
Epoch [47/120    avg_loss:0.042, val_acc:0.972]
Epoch [48/120    avg_loss:0.037, val_acc:0.968]
Epoch [49/120    avg_loss:0.045, val_acc:0.967]
Epoch [50/120    avg_loss:0.047, val_acc:0.967]
Epoch [51/120    avg_loss:0.035, val_acc:0.974]
Epoch [52/120    avg_loss:0.042, val_acc:0.975]
Epoch [53/120    avg_loss:0.027, val_acc:0.967]
Epoch [54/120    avg_loss:0.044, val_acc:0.964]
Epoch [55/120    avg_loss:0.032, val_acc:0.966]
Epoch [56/120    avg_loss:0.036, val_acc:0.967]
Epoch [57/120    avg_loss:0.025, val_acc:0.975]
Epoch [58/120    avg_loss:0.018, val_acc:0.977]
Epoch [59/120    avg_loss:0.017, val_acc:0.976]
Epoch [60/120    avg_loss:0.019, val_acc:0.978]
Epoch [61/120    avg_loss:0.038, val_acc:0.961]
Epoch [62/120    avg_loss:0.033, val_acc:0.976]
Epoch [63/120    avg_loss:0.026, val_acc:0.974]
Epoch [64/120    avg_loss:0.023, val_acc:0.970]
Epoch [65/120    avg_loss:0.024, val_acc:0.977]
Epoch [66/120    avg_loss:0.032, val_acc:0.963]
Epoch [67/120    avg_loss:0.036, val_acc:0.971]
Epoch [68/120    avg_loss:0.028, val_acc:0.975]
Epoch [69/120    avg_loss:0.020, val_acc:0.974]
Epoch [70/120    avg_loss:0.015, val_acc:0.966]
Epoch [71/120    avg_loss:0.031, val_acc:0.967]
Epoch [72/120    avg_loss:0.035, val_acc:0.977]
Epoch [73/120    avg_loss:0.024, val_acc:0.962]
Epoch [74/120    avg_loss:0.021, val_acc:0.978]
Epoch [75/120    avg_loss:0.016, val_acc:0.981]
Epoch [76/120    avg_loss:0.012, val_acc:0.982]
Epoch [77/120    avg_loss:0.013, val_acc:0.982]
Epoch [78/120    avg_loss:0.015, val_acc:0.982]
Epoch [79/120    avg_loss:0.011, val_acc:0.981]
Epoch [80/120    avg_loss:0.011, val_acc:0.985]
Epoch [81/120    avg_loss:0.016, val_acc:0.986]
Epoch [82/120    avg_loss:0.011, val_acc:0.986]
Epoch [83/120    avg_loss:0.011, val_acc:0.984]
Epoch [84/120    avg_loss:0.012, val_acc:0.984]
Epoch [85/120    avg_loss:0.011, val_acc:0.984]
Epoch [86/120    avg_loss:0.012, val_acc:0.985]
Epoch [87/120    avg_loss:0.010, val_acc:0.984]
Epoch [88/120    avg_loss:0.009, val_acc:0.985]
Epoch [89/120    avg_loss:0.009, val_acc:0.984]
Epoch [90/120    avg_loss:0.011, val_acc:0.984]
Epoch [91/120    avg_loss:0.011, val_acc:0.984]
Epoch [92/120    avg_loss:0.010, val_acc:0.984]
Epoch [93/120    avg_loss:0.012, val_acc:0.982]
Epoch [94/120    avg_loss:0.009, val_acc:0.981]
Epoch [95/120    avg_loss:0.008, val_acc:0.981]
Epoch [96/120    avg_loss:0.008, val_acc:0.980]
Epoch [97/120    avg_loss:0.010, val_acc:0.982]
Epoch [98/120    avg_loss:0.010, val_acc:0.981]
Epoch [99/120    avg_loss:0.008, val_acc:0.981]
Epoch [100/120    avg_loss:0.009, val_acc:0.981]
Epoch [101/120    avg_loss:0.010, val_acc:0.982]
Epoch [102/120    avg_loss:0.010, val_acc:0.984]
Epoch [103/120    avg_loss:0.011, val_acc:0.983]
Epoch [104/120    avg_loss:0.008, val_acc:0.983]
Epoch [105/120    avg_loss:0.009, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.008, val_acc:0.984]
Epoch [108/120    avg_loss:0.009, val_acc:0.984]
Epoch [109/120    avg_loss:0.009, val_acc:0.984]
Epoch [110/120    avg_loss:0.014, val_acc:0.984]
Epoch [111/120    avg_loss:0.011, val_acc:0.984]
Epoch [112/120    avg_loss:0.009, val_acc:0.984]
Epoch [113/120    avg_loss:0.010, val_acc:0.984]
Epoch [114/120    avg_loss:0.010, val_acc:0.984]
Epoch [115/120    avg_loss:0.009, val_acc:0.984]
Epoch [116/120    avg_loss:0.009, val_acc:0.984]
Epoch [117/120    avg_loss:0.009, val_acc:0.984]
Epoch [118/120    avg_loss:0.010, val_acc:0.984]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.009, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1263    4    0    1    0    0    0    0    7    7    3    0
     0    0    0]
 [   0    0    0  713    0    8    0    0    0    6    1    5   14    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    2    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    5    2    0    0    0  847    8    0    0
     3    0    0]
 [   0    0    4    0    0    0    3    0    0    0    2 2196    4    1
     0    0    0]
 [   0    0    0    0    1    2    0    0    0    0    0    2  525    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1132    3    0]
 [   0    0    0    0    0    0   27    0    0    0    0    0    0    0
     6  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.26558265582656

F1 scores:
[       nan 0.975      0.98556379 0.97337884 0.99765808 0.9738339
 0.97546468 0.96153846 0.99767442 0.82926829 0.97580645 0.99142212
 0.97042514 0.99730458 0.99124343 0.94578313 0.97076023]

Kappa:
0.9802239262516446
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f19e321d710>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.516, val_acc:0.483]
Epoch [2/120    avg_loss:1.988, val_acc:0.471]
Epoch [3/120    avg_loss:1.707, val_acc:0.589]
Epoch [4/120    avg_loss:1.443, val_acc:0.648]
Epoch [5/120    avg_loss:1.170, val_acc:0.690]
Epoch [6/120    avg_loss:0.961, val_acc:0.720]
Epoch [7/120    avg_loss:0.786, val_acc:0.742]
Epoch [8/120    avg_loss:0.747, val_acc:0.768]
Epoch [9/120    avg_loss:0.770, val_acc:0.829]
Epoch [10/120    avg_loss:0.619, val_acc:0.772]
Epoch [11/120    avg_loss:0.487, val_acc:0.810]
Epoch [12/120    avg_loss:0.440, val_acc:0.850]
Epoch [13/120    avg_loss:0.391, val_acc:0.844]
Epoch [14/120    avg_loss:0.284, val_acc:0.889]
Epoch [15/120    avg_loss:0.255, val_acc:0.919]
Epoch [16/120    avg_loss:0.213, val_acc:0.905]
Epoch [17/120    avg_loss:0.239, val_acc:0.907]
Epoch [18/120    avg_loss:0.294, val_acc:0.900]
Epoch [19/120    avg_loss:0.206, val_acc:0.918]
Epoch [20/120    avg_loss:0.288, val_acc:0.898]
Epoch [21/120    avg_loss:0.183, val_acc:0.919]
Epoch [22/120    avg_loss:0.165, val_acc:0.931]
Epoch [23/120    avg_loss:0.150, val_acc:0.925]
Epoch [24/120    avg_loss:0.125, val_acc:0.925]
Epoch [25/120    avg_loss:0.104, val_acc:0.939]
Epoch [26/120    avg_loss:0.082, val_acc:0.942]
Epoch [27/120    avg_loss:0.090, val_acc:0.948]
Epoch [28/120    avg_loss:0.106, val_acc:0.949]
Epoch [29/120    avg_loss:0.080, val_acc:0.950]
Epoch [30/120    avg_loss:0.089, val_acc:0.944]
Epoch [31/120    avg_loss:0.080, val_acc:0.946]
Epoch [32/120    avg_loss:0.085, val_acc:0.954]
Epoch [33/120    avg_loss:0.081, val_acc:0.943]
Epoch [34/120    avg_loss:0.070, val_acc:0.958]
Epoch [35/120    avg_loss:0.073, val_acc:0.948]
Epoch [36/120    avg_loss:0.064, val_acc:0.948]
Epoch [37/120    avg_loss:0.052, val_acc:0.956]
Epoch [38/120    avg_loss:0.051, val_acc:0.959]
Epoch [39/120    avg_loss:0.049, val_acc:0.959]
Epoch [40/120    avg_loss:0.063, val_acc:0.950]
Epoch [41/120    avg_loss:0.045, val_acc:0.968]
Epoch [42/120    avg_loss:0.040, val_acc:0.962]
Epoch [43/120    avg_loss:0.044, val_acc:0.963]
Epoch [44/120    avg_loss:0.034, val_acc:0.968]
Epoch [45/120    avg_loss:0.027, val_acc:0.956]
Epoch [46/120    avg_loss:0.034, val_acc:0.971]
Epoch [47/120    avg_loss:0.049, val_acc:0.953]
Epoch [48/120    avg_loss:0.044, val_acc:0.961]
Epoch [49/120    avg_loss:0.032, val_acc:0.973]
Epoch [50/120    avg_loss:0.050, val_acc:0.955]
Epoch [51/120    avg_loss:0.055, val_acc:0.961]
Epoch [52/120    avg_loss:0.036, val_acc:0.963]
Epoch [53/120    avg_loss:0.039, val_acc:0.961]
Epoch [54/120    avg_loss:0.033, val_acc:0.963]
Epoch [55/120    avg_loss:0.041, val_acc:0.959]
Epoch [56/120    avg_loss:0.032, val_acc:0.966]
Epoch [57/120    avg_loss:0.023, val_acc:0.968]
Epoch [58/120    avg_loss:0.021, val_acc:0.974]
Epoch [59/120    avg_loss:0.024, val_acc:0.961]
Epoch [60/120    avg_loss:0.022, val_acc:0.968]
Epoch [61/120    avg_loss:0.016, val_acc:0.973]
Epoch [62/120    avg_loss:0.023, val_acc:0.971]
Epoch [63/120    avg_loss:0.020, val_acc:0.962]
Epoch [64/120    avg_loss:0.021, val_acc:0.975]
Epoch [65/120    avg_loss:0.020, val_acc:0.975]
Epoch [66/120    avg_loss:0.014, val_acc:0.975]
Epoch [67/120    avg_loss:0.019, val_acc:0.980]
Epoch [68/120    avg_loss:0.021, val_acc:0.980]
Epoch [69/120    avg_loss:0.014, val_acc:0.969]
Epoch [70/120    avg_loss:0.015, val_acc:0.974]
Epoch [71/120    avg_loss:0.029, val_acc:0.970]
Epoch [72/120    avg_loss:0.019, val_acc:0.974]
Epoch [73/120    avg_loss:0.014, val_acc:0.974]
Epoch [74/120    avg_loss:0.018, val_acc:0.975]
Epoch [75/120    avg_loss:0.013, val_acc:0.977]
Epoch [76/120    avg_loss:0.013, val_acc:0.974]
Epoch [77/120    avg_loss:0.015, val_acc:0.986]
Epoch [78/120    avg_loss:0.015, val_acc:0.972]
Epoch [79/120    avg_loss:0.039, val_acc:0.967]
Epoch [80/120    avg_loss:0.015, val_acc:0.972]
Epoch [81/120    avg_loss:0.013, val_acc:0.977]
Epoch [82/120    avg_loss:0.016, val_acc:0.972]
Epoch [83/120    avg_loss:0.037, val_acc:0.970]
Epoch [84/120    avg_loss:0.025, val_acc:0.972]
Epoch [85/120    avg_loss:0.019, val_acc:0.972]
Epoch [86/120    avg_loss:0.015, val_acc:0.977]
Epoch [87/120    avg_loss:0.008, val_acc:0.981]
Epoch [88/120    avg_loss:0.009, val_acc:0.978]
Epoch [89/120    avg_loss:0.007, val_acc:0.978]
Epoch [90/120    avg_loss:0.014, val_acc:0.974]
Epoch [91/120    avg_loss:0.010, val_acc:0.983]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.010, val_acc:0.985]
Epoch [94/120    avg_loss:0.007, val_acc:0.982]
Epoch [95/120    avg_loss:0.008, val_acc:0.984]
Epoch [96/120    avg_loss:0.006, val_acc:0.983]
Epoch [97/120    avg_loss:0.008, val_acc:0.983]
Epoch [98/120    avg_loss:0.007, val_acc:0.983]
Epoch [99/120    avg_loss:0.007, val_acc:0.984]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.007, val_acc:0.983]
Epoch [102/120    avg_loss:0.006, val_acc:0.983]
Epoch [103/120    avg_loss:0.007, val_acc:0.983]
Epoch [104/120    avg_loss:0.007, val_acc:0.983]
Epoch [105/120    avg_loss:0.005, val_acc:0.983]
Epoch [106/120    avg_loss:0.006, val_acc:0.983]
Epoch [107/120    avg_loss:0.006, val_acc:0.983]
Epoch [108/120    avg_loss:0.006, val_acc:0.983]
Epoch [109/120    avg_loss:0.007, val_acc:0.983]
Epoch [110/120    avg_loss:0.005, val_acc:0.983]
Epoch [111/120    avg_loss:0.004, val_acc:0.983]
Epoch [112/120    avg_loss:0.008, val_acc:0.983]
Epoch [113/120    avg_loss:0.006, val_acc:0.983]
Epoch [114/120    avg_loss:0.006, val_acc:0.983]
Epoch [115/120    avg_loss:0.005, val_acc:0.983]
Epoch [116/120    avg_loss:0.006, val_acc:0.983]
Epoch [117/120    avg_loss:0.006, val_acc:0.983]
Epoch [118/120    avg_loss:0.006, val_acc:0.983]
Epoch [119/120    avg_loss:0.007, val_acc:0.983]
Epoch [120/120    avg_loss:0.006, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1259    0    2    0    1    0    0    0    7    9    6    0
     0    1    0]
 [   0    0    0  709    0   21    0    0    0   10    1    0    3    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    6    2    0    0    0  858    5    0    0
     0    4    0]
 [   0    0    9    0    0    0   18    0    0    0    8 2173    0    2
     0    0    0]
 [   0    0    0    0    2   13    0    0    0    0    1    0  517    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    1    0    0    0
  1133    2    0]
 [   0    0    0    0    0    1   27    0    0    0    0    0    0    0
     9  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.95121951219512

F1 scores:
[       nan 0.98765432 0.98590446 0.9739011  0.99069767 0.94840834
 0.96399706 1.         0.9953271  0.72340426 0.98001142 0.98817644
 0.96998124 0.98666667 0.99298861 0.93373494 0.98809524]

Kappa:
0.9766591420532359
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2d25b60710>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.540, val_acc:0.454]
Epoch [2/120    avg_loss:2.033, val_acc:0.515]
Epoch [3/120    avg_loss:1.774, val_acc:0.637]
Epoch [4/120    avg_loss:1.519, val_acc:0.671]
Epoch [5/120    avg_loss:1.349, val_acc:0.685]
Epoch [6/120    avg_loss:1.119, val_acc:0.740]
Epoch [7/120    avg_loss:0.939, val_acc:0.734]
Epoch [8/120    avg_loss:0.797, val_acc:0.792]
Epoch [9/120    avg_loss:0.681, val_acc:0.833]
Epoch [10/120    avg_loss:0.712, val_acc:0.720]
Epoch [11/120    avg_loss:0.738, val_acc:0.822]
Epoch [12/120    avg_loss:0.560, val_acc:0.873]
Epoch [13/120    avg_loss:0.426, val_acc:0.867]
Epoch [14/120    avg_loss:0.342, val_acc:0.881]
Epoch [15/120    avg_loss:0.406, val_acc:0.902]
Epoch [16/120    avg_loss:0.295, val_acc:0.875]
Epoch [17/120    avg_loss:0.239, val_acc:0.914]
Epoch [18/120    avg_loss:0.224, val_acc:0.914]
Epoch [19/120    avg_loss:0.186, val_acc:0.929]
Epoch [20/120    avg_loss:0.213, val_acc:0.892]
Epoch [21/120    avg_loss:0.271, val_acc:0.875]
Epoch [22/120    avg_loss:0.191, val_acc:0.926]
Epoch [23/120    avg_loss:0.182, val_acc:0.926]
Epoch [24/120    avg_loss:0.150, val_acc:0.938]
Epoch [25/120    avg_loss:0.134, val_acc:0.932]
Epoch [26/120    avg_loss:0.115, val_acc:0.948]
Epoch [27/120    avg_loss:0.131, val_acc:0.952]
Epoch [28/120    avg_loss:0.123, val_acc:0.949]
Epoch [29/120    avg_loss:0.127, val_acc:0.947]
Epoch [30/120    avg_loss:0.109, val_acc:0.950]
Epoch [31/120    avg_loss:0.081, val_acc:0.948]
Epoch [32/120    avg_loss:0.065, val_acc:0.960]
Epoch [33/120    avg_loss:0.077, val_acc:0.952]
Epoch [34/120    avg_loss:0.074, val_acc:0.954]
Epoch [35/120    avg_loss:0.066, val_acc:0.942]
Epoch [36/120    avg_loss:0.078, val_acc:0.943]
Epoch [37/120    avg_loss:0.055, val_acc:0.960]
Epoch [38/120    avg_loss:0.064, val_acc:0.953]
Epoch [39/120    avg_loss:0.085, val_acc:0.962]
Epoch [40/120    avg_loss:0.079, val_acc:0.945]
Epoch [41/120    avg_loss:0.081, val_acc:0.957]
Epoch [42/120    avg_loss:0.053, val_acc:0.955]
Epoch [43/120    avg_loss:0.065, val_acc:0.940]
Epoch [44/120    avg_loss:0.038, val_acc:0.969]
Epoch [45/120    avg_loss:0.041, val_acc:0.962]
Epoch [46/120    avg_loss:0.039, val_acc:0.968]
Epoch [47/120    avg_loss:0.039, val_acc:0.968]
Epoch [48/120    avg_loss:0.046, val_acc:0.963]
Epoch [49/120    avg_loss:0.032, val_acc:0.972]
Epoch [50/120    avg_loss:0.032, val_acc:0.968]
Epoch [51/120    avg_loss:0.039, val_acc:0.970]
Epoch [52/120    avg_loss:0.035, val_acc:0.978]
Epoch [53/120    avg_loss:0.036, val_acc:0.963]
Epoch [54/120    avg_loss:0.027, val_acc:0.972]
Epoch [55/120    avg_loss:0.036, val_acc:0.972]
Epoch [56/120    avg_loss:0.031, val_acc:0.970]
Epoch [57/120    avg_loss:0.035, val_acc:0.966]
Epoch [58/120    avg_loss:0.030, val_acc:0.972]
Epoch [59/120    avg_loss:0.019, val_acc:0.968]
Epoch [60/120    avg_loss:0.023, val_acc:0.964]
Epoch [61/120    avg_loss:0.022, val_acc:0.980]
Epoch [62/120    avg_loss:0.021, val_acc:0.980]
Epoch [63/120    avg_loss:0.020, val_acc:0.972]
Epoch [64/120    avg_loss:0.023, val_acc:0.960]
Epoch [65/120    avg_loss:0.021, val_acc:0.967]
Epoch [66/120    avg_loss:0.017, val_acc:0.975]
Epoch [67/120    avg_loss:0.015, val_acc:0.970]
Epoch [68/120    avg_loss:0.023, val_acc:0.970]
Epoch [69/120    avg_loss:0.020, val_acc:0.975]
Epoch [70/120    avg_loss:0.021, val_acc:0.944]
Epoch [71/120    avg_loss:0.019, val_acc:0.980]
Epoch [72/120    avg_loss:0.015, val_acc:0.973]
Epoch [73/120    avg_loss:0.012, val_acc:0.981]
Epoch [74/120    avg_loss:0.016, val_acc:0.976]
Epoch [75/120    avg_loss:0.013, val_acc:0.977]
Epoch [76/120    avg_loss:0.010, val_acc:0.981]
Epoch [77/120    avg_loss:0.014, val_acc:0.973]
Epoch [78/120    avg_loss:0.014, val_acc:0.977]
Epoch [79/120    avg_loss:0.011, val_acc:0.975]
Epoch [80/120    avg_loss:0.014, val_acc:0.978]
Epoch [81/120    avg_loss:0.013, val_acc:0.980]
Epoch [82/120    avg_loss:0.011, val_acc:0.978]
Epoch [83/120    avg_loss:0.012, val_acc:0.980]
Epoch [84/120    avg_loss:0.013, val_acc:0.974]
Epoch [85/120    avg_loss:0.017, val_acc:0.967]
Epoch [86/120    avg_loss:0.012, val_acc:0.981]
Epoch [87/120    avg_loss:0.014, val_acc:0.970]
Epoch [88/120    avg_loss:0.018, val_acc:0.972]
Epoch [89/120    avg_loss:0.018, val_acc:0.978]
Epoch [90/120    avg_loss:0.017, val_acc:0.981]
Epoch [91/120    avg_loss:0.009, val_acc:0.981]
Epoch [92/120    avg_loss:0.008, val_acc:0.980]
Epoch [93/120    avg_loss:0.009, val_acc:0.980]
Epoch [94/120    avg_loss:0.013, val_acc:0.973]
Epoch [95/120    avg_loss:0.025, val_acc:0.974]
Epoch [96/120    avg_loss:0.031, val_acc:0.971]
Epoch [97/120    avg_loss:0.014, val_acc:0.975]
Epoch [98/120    avg_loss:0.010, val_acc:0.971]
Epoch [99/120    avg_loss:0.018, val_acc:0.967]
Epoch [100/120    avg_loss:0.029, val_acc:0.970]
Epoch [101/120    avg_loss:0.011, val_acc:0.977]
Epoch [102/120    avg_loss:0.010, val_acc:0.974]
Epoch [103/120    avg_loss:0.008, val_acc:0.975]
Epoch [104/120    avg_loss:0.021, val_acc:0.969]
Epoch [105/120    avg_loss:0.024, val_acc:0.974]
Epoch [106/120    avg_loss:0.008, val_acc:0.976]
Epoch [107/120    avg_loss:0.011, val_acc:0.980]
Epoch [108/120    avg_loss:0.008, val_acc:0.980]
Epoch [109/120    avg_loss:0.008, val_acc:0.978]
Epoch [110/120    avg_loss:0.008, val_acc:0.980]
Epoch [111/120    avg_loss:0.007, val_acc:0.980]
Epoch [112/120    avg_loss:0.008, val_acc:0.980]
Epoch [113/120    avg_loss:0.005, val_acc:0.981]
Epoch [114/120    avg_loss:0.009, val_acc:0.983]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.981]
Epoch [117/120    avg_loss:0.006, val_acc:0.982]
Epoch [118/120    avg_loss:0.005, val_acc:0.982]
Epoch [119/120    avg_loss:0.007, val_acc:0.981]
Epoch [120/120    avg_loss:0.006, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1264    2    0    1    1    0    0    1    8    6    0    0
     0    2    0]
 [   0    0    0  703    1   12    0    0    0    4    1    1   20    4
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    4    1    0    0    0  858    8    0    0
     0    4    0]
 [   0    0    6    0    0    0    1    0    2    0    6 2194    0    1
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    9  522    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0    0    0
  1134    3    0]
 [   0    0    0    0    0    0   38    0    0    0    0    0    0    0
     4  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.21138211382114

F1 scores:
[       nan 0.98765432 0.98904538 0.96831956 0.99765808 0.9751693
 0.9689808  0.98039216 0.99767981 0.8372093  0.98057143 0.99074283
 0.96756256 0.98666667 0.99604743 0.92145015 0.97590361]

Kappa:
0.97960831322499
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbca6d3c710>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.507, val_acc:0.519]
Epoch [2/120    avg_loss:1.966, val_acc:0.574]
Epoch [3/120    avg_loss:1.645, val_acc:0.621]
Epoch [4/120    avg_loss:1.450, val_acc:0.686]
Epoch [5/120    avg_loss:1.186, val_acc:0.697]
Epoch [6/120    avg_loss:0.986, val_acc:0.710]
Epoch [7/120    avg_loss:0.962, val_acc:0.744]
Epoch [8/120    avg_loss:0.746, val_acc:0.803]
Epoch [9/120    avg_loss:0.606, val_acc:0.801]
Epoch [10/120    avg_loss:0.596, val_acc:0.822]
Epoch [11/120    avg_loss:0.549, val_acc:0.852]
Epoch [12/120    avg_loss:0.368, val_acc:0.883]
Epoch [13/120    avg_loss:0.326, val_acc:0.867]
Epoch [14/120    avg_loss:0.385, val_acc:0.847]
Epoch [15/120    avg_loss:0.391, val_acc:0.844]
Epoch [16/120    avg_loss:0.411, val_acc:0.899]
Epoch [17/120    avg_loss:0.366, val_acc:0.881]
Epoch [18/120    avg_loss:0.252, val_acc:0.902]
Epoch [19/120    avg_loss:0.205, val_acc:0.927]
Epoch [20/120    avg_loss:0.154, val_acc:0.926]
Epoch [21/120    avg_loss:0.164, val_acc:0.931]
Epoch [22/120    avg_loss:0.154, val_acc:0.941]
Epoch [23/120    avg_loss:0.172, val_acc:0.891]
Epoch [24/120    avg_loss:0.233, val_acc:0.943]
Epoch [25/120    avg_loss:0.164, val_acc:0.942]
Epoch [26/120    avg_loss:0.169, val_acc:0.923]
Epoch [27/120    avg_loss:0.126, val_acc:0.955]
Epoch [28/120    avg_loss:0.097, val_acc:0.962]
Epoch [29/120    avg_loss:0.086, val_acc:0.962]
Epoch [30/120    avg_loss:0.085, val_acc:0.959]
Epoch [31/120    avg_loss:0.110, val_acc:0.964]
Epoch [32/120    avg_loss:0.092, val_acc:0.970]
Epoch [33/120    avg_loss:0.067, val_acc:0.964]
Epoch [34/120    avg_loss:0.053, val_acc:0.969]
Epoch [35/120    avg_loss:0.054, val_acc:0.963]
Epoch [36/120    avg_loss:0.068, val_acc:0.964]
Epoch [37/120    avg_loss:0.056, val_acc:0.948]
Epoch [38/120    avg_loss:0.053, val_acc:0.968]
Epoch [39/120    avg_loss:0.054, val_acc:0.974]
Epoch [40/120    avg_loss:0.040, val_acc:0.959]
Epoch [41/120    avg_loss:0.043, val_acc:0.971]
Epoch [42/120    avg_loss:0.048, val_acc:0.973]
Epoch [43/120    avg_loss:0.041, val_acc:0.972]
Epoch [44/120    avg_loss:0.036, val_acc:0.967]
Epoch [45/120    avg_loss:0.036, val_acc:0.974]
Epoch [46/120    avg_loss:0.036, val_acc:0.976]
Epoch [47/120    avg_loss:0.030, val_acc:0.977]
Epoch [48/120    avg_loss:0.037, val_acc:0.976]
Epoch [49/120    avg_loss:0.028, val_acc:0.974]
Epoch [50/120    avg_loss:0.041, val_acc:0.969]
Epoch [51/120    avg_loss:0.071, val_acc:0.935]
Epoch [52/120    avg_loss:0.111, val_acc:0.964]
Epoch [53/120    avg_loss:0.076, val_acc:0.971]
Epoch [54/120    avg_loss:0.041, val_acc:0.977]
Epoch [55/120    avg_loss:0.035, val_acc:0.975]
Epoch [56/120    avg_loss:0.031, val_acc:0.978]
Epoch [57/120    avg_loss:0.034, val_acc:0.980]
Epoch [58/120    avg_loss:0.029, val_acc:0.980]
Epoch [59/120    avg_loss:0.023, val_acc:0.977]
Epoch [60/120    avg_loss:0.027, val_acc:0.973]
Epoch [61/120    avg_loss:0.030, val_acc:0.970]
Epoch [62/120    avg_loss:0.032, val_acc:0.977]
Epoch [63/120    avg_loss:0.030, val_acc:0.976]
Epoch [64/120    avg_loss:0.024, val_acc:0.986]
Epoch [65/120    avg_loss:0.020, val_acc:0.983]
Epoch [66/120    avg_loss:0.016, val_acc:0.987]
Epoch [67/120    avg_loss:0.016, val_acc:0.986]
Epoch [68/120    avg_loss:0.016, val_acc:0.989]
Epoch [69/120    avg_loss:0.027, val_acc:0.966]
Epoch [70/120    avg_loss:0.026, val_acc:0.981]
Epoch [71/120    avg_loss:0.028, val_acc:0.986]
Epoch [72/120    avg_loss:0.040, val_acc:0.982]
Epoch [73/120    avg_loss:0.043, val_acc:0.963]
Epoch [74/120    avg_loss:0.025, val_acc:0.983]
Epoch [75/120    avg_loss:0.020, val_acc:0.985]
Epoch [76/120    avg_loss:0.013, val_acc:0.985]
Epoch [77/120    avg_loss:0.013, val_acc:0.989]
Epoch [78/120    avg_loss:0.011, val_acc:0.989]
Epoch [79/120    avg_loss:0.012, val_acc:0.985]
Epoch [80/120    avg_loss:0.013, val_acc:0.987]
Epoch [81/120    avg_loss:0.011, val_acc:0.986]
Epoch [82/120    avg_loss:0.013, val_acc:0.986]
Epoch [83/120    avg_loss:0.009, val_acc:0.988]
Epoch [84/120    avg_loss:0.010, val_acc:0.987]
Epoch [85/120    avg_loss:0.008, val_acc:0.987]
Epoch [86/120    avg_loss:0.008, val_acc:0.990]
Epoch [87/120    avg_loss:0.009, val_acc:0.989]
Epoch [88/120    avg_loss:0.010, val_acc:0.986]
Epoch [89/120    avg_loss:0.010, val_acc:0.990]
Epoch [90/120    avg_loss:0.009, val_acc:0.987]
Epoch [91/120    avg_loss:0.008, val_acc:0.988]
Epoch [92/120    avg_loss:0.007, val_acc:0.989]
Epoch [93/120    avg_loss:0.007, val_acc:0.989]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.006, val_acc:0.989]
Epoch [97/120    avg_loss:0.007, val_acc:0.985]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.987]
Epoch [100/120    avg_loss:0.008, val_acc:0.987]
Epoch [101/120    avg_loss:0.006, val_acc:0.989]
Epoch [102/120    avg_loss:0.006, val_acc:0.988]
Epoch [103/120    avg_loss:0.006, val_acc:0.988]
Epoch [104/120    avg_loss:0.008, val_acc:0.981]
Epoch [105/120    avg_loss:0.009, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.006, val_acc:0.986]
Epoch [108/120    avg_loss:0.006, val_acc:0.987]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.988]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.004, val_acc:0.988]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.004, val_acc:0.989]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.989]
Epoch [118/120    avg_loss:0.004, val_acc:0.989]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1278    0    0    0    0    0    0    0    2    5    0    0
     0    0    0]
 [   0    0    0  711    1   18    0    0    0    5    1    0    7    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    3    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    6    0    0    0    0  844   18    0    0
     1    3    0]
 [   0    0    8    0    0    0    3    0    0    0    3 2195    0    1
     0    0    0]
 [   0    0    2    0    0   14    0    0    0    0    1    0  513    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    2    0    0    0
  1133    0    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    66  275    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.84281842818429

F1 scores:
[       nan 0.975      0.99223602 0.97530864 0.99765808 0.94585635
 0.9924357  0.98039216 1.         0.81818182 0.97572254 0.9911944
 0.97251185 0.98666667 0.9675491  0.88       0.97076023]

Kappa:
0.9753906176008224
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f20b0c5f710>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.517, val_acc:0.443]
Epoch [2/120    avg_loss:1.982, val_acc:0.517]
Epoch [3/120    avg_loss:1.710, val_acc:0.572]
Epoch [4/120    avg_loss:1.561, val_acc:0.615]
Epoch [5/120    avg_loss:1.382, val_acc:0.677]
Epoch [6/120    avg_loss:1.177, val_acc:0.690]
Epoch [7/120    avg_loss:0.989, val_acc:0.744]
Epoch [8/120    avg_loss:0.906, val_acc:0.762]
Epoch [9/120    avg_loss:0.741, val_acc:0.772]
Epoch [10/120    avg_loss:0.716, val_acc:0.786]
Epoch [11/120    avg_loss:0.599, val_acc:0.821]
Epoch [12/120    avg_loss:0.531, val_acc:0.822]
Epoch [13/120    avg_loss:0.510, val_acc:0.836]
Epoch [14/120    avg_loss:0.509, val_acc:0.850]
Epoch [15/120    avg_loss:0.440, val_acc:0.861]
Epoch [16/120    avg_loss:0.373, val_acc:0.852]
Epoch [17/120    avg_loss:0.398, val_acc:0.859]
Epoch [18/120    avg_loss:0.269, val_acc:0.895]
Epoch [19/120    avg_loss:0.239, val_acc:0.898]
Epoch [20/120    avg_loss:0.221, val_acc:0.920]
Epoch [21/120    avg_loss:0.209, val_acc:0.913]
Epoch [22/120    avg_loss:0.204, val_acc:0.901]
Epoch [23/120    avg_loss:0.207, val_acc:0.915]
Epoch [24/120    avg_loss:0.173, val_acc:0.943]
Epoch [25/120    avg_loss:0.133, val_acc:0.925]
Epoch [26/120    avg_loss:0.144, val_acc:0.922]
Epoch [27/120    avg_loss:0.142, val_acc:0.941]
Epoch [28/120    avg_loss:0.106, val_acc:0.952]
Epoch [29/120    avg_loss:0.089, val_acc:0.945]
Epoch [30/120    avg_loss:0.079, val_acc:0.930]
Epoch [31/120    avg_loss:0.078, val_acc:0.955]
Epoch [32/120    avg_loss:0.084, val_acc:0.955]
Epoch [33/120    avg_loss:0.065, val_acc:0.953]
Epoch [34/120    avg_loss:0.068, val_acc:0.960]
Epoch [35/120    avg_loss:0.074, val_acc:0.961]
Epoch [36/120    avg_loss:0.064, val_acc:0.960]
Epoch [37/120    avg_loss:0.048, val_acc:0.963]
Epoch [38/120    avg_loss:0.055, val_acc:0.967]
Epoch [39/120    avg_loss:0.045, val_acc:0.974]
Epoch [40/120    avg_loss:0.062, val_acc:0.948]
Epoch [41/120    avg_loss:0.055, val_acc:0.972]
Epoch [42/120    avg_loss:0.052, val_acc:0.967]
Epoch [43/120    avg_loss:0.063, val_acc:0.953]
Epoch [44/120    avg_loss:0.052, val_acc:0.963]
Epoch [45/120    avg_loss:0.057, val_acc:0.964]
Epoch [46/120    avg_loss:0.046, val_acc:0.971]
Epoch [47/120    avg_loss:0.042, val_acc:0.945]
Epoch [48/120    avg_loss:0.051, val_acc:0.960]
Epoch [49/120    avg_loss:0.044, val_acc:0.961]
Epoch [50/120    avg_loss:0.031, val_acc:0.975]
Epoch [51/120    avg_loss:0.035, val_acc:0.949]
Epoch [52/120    avg_loss:0.033, val_acc:0.978]
Epoch [53/120    avg_loss:0.031, val_acc:0.967]
Epoch [54/120    avg_loss:0.033, val_acc:0.972]
Epoch [55/120    avg_loss:0.050, val_acc:0.980]
Epoch [56/120    avg_loss:0.053, val_acc:0.972]
Epoch [57/120    avg_loss:0.041, val_acc:0.969]
Epoch [58/120    avg_loss:0.028, val_acc:0.974]
Epoch [59/120    avg_loss:0.021, val_acc:0.984]
Epoch [60/120    avg_loss:0.017, val_acc:0.987]
Epoch [61/120    avg_loss:0.026, val_acc:0.971]
Epoch [62/120    avg_loss:0.024, val_acc:0.947]
Epoch [63/120    avg_loss:0.029, val_acc:0.972]
Epoch [64/120    avg_loss:0.019, val_acc:0.983]
Epoch [65/120    avg_loss:0.017, val_acc:0.978]
Epoch [66/120    avg_loss:0.016, val_acc:0.985]
Epoch [67/120    avg_loss:0.019, val_acc:0.991]
Epoch [68/120    avg_loss:0.017, val_acc:0.976]
Epoch [69/120    avg_loss:0.020, val_acc:0.987]
Epoch [70/120    avg_loss:0.022, val_acc:0.981]
Epoch [71/120    avg_loss:0.022, val_acc:0.985]
Epoch [72/120    avg_loss:0.028, val_acc:0.975]
Epoch [73/120    avg_loss:0.025, val_acc:0.984]
Epoch [74/120    avg_loss:0.012, val_acc:0.989]
Epoch [75/120    avg_loss:0.016, val_acc:0.977]
Epoch [76/120    avg_loss:0.024, val_acc:0.974]
Epoch [77/120    avg_loss:0.034, val_acc:0.972]
Epoch [78/120    avg_loss:0.019, val_acc:0.978]
Epoch [79/120    avg_loss:0.016, val_acc:0.987]
Epoch [80/120    avg_loss:0.015, val_acc:0.990]
Epoch [81/120    avg_loss:0.011, val_acc:0.991]
Epoch [82/120    avg_loss:0.010, val_acc:0.991]
Epoch [83/120    avg_loss:0.008, val_acc:0.992]
Epoch [84/120    avg_loss:0.015, val_acc:0.994]
Epoch [85/120    avg_loss:0.008, val_acc:0.992]
Epoch [86/120    avg_loss:0.008, val_acc:0.994]
Epoch [87/120    avg_loss:0.008, val_acc:0.994]
Epoch [88/120    avg_loss:0.008, val_acc:0.995]
Epoch [89/120    avg_loss:0.007, val_acc:0.995]
Epoch [90/120    avg_loss:0.008, val_acc:0.995]
Epoch [91/120    avg_loss:0.007, val_acc:0.995]
Epoch [92/120    avg_loss:0.009, val_acc:0.994]
Epoch [93/120    avg_loss:0.007, val_acc:0.995]
Epoch [94/120    avg_loss:0.009, val_acc:0.996]
Epoch [95/120    avg_loss:0.009, val_acc:0.995]
Epoch [96/120    avg_loss:0.007, val_acc:0.996]
Epoch [97/120    avg_loss:0.007, val_acc:0.995]
Epoch [98/120    avg_loss:0.007, val_acc:0.994]
Epoch [99/120    avg_loss:0.010, val_acc:0.994]
Epoch [100/120    avg_loss:0.008, val_acc:0.991]
Epoch [101/120    avg_loss:0.007, val_acc:0.992]
Epoch [102/120    avg_loss:0.007, val_acc:0.992]
Epoch [103/120    avg_loss:0.009, val_acc:0.992]
Epoch [104/120    avg_loss:0.007, val_acc:0.995]
Epoch [105/120    avg_loss:0.007, val_acc:0.995]
Epoch [106/120    avg_loss:0.007, val_acc:0.992]
Epoch [107/120    avg_loss:0.007, val_acc:0.992]
Epoch [108/120    avg_loss:0.006, val_acc:0.994]
Epoch [109/120    avg_loss:0.008, val_acc:0.994]
Epoch [110/120    avg_loss:0.007, val_acc:0.994]
Epoch [111/120    avg_loss:0.008, val_acc:0.994]
Epoch [112/120    avg_loss:0.007, val_acc:0.994]
Epoch [113/120    avg_loss:0.009, val_acc:0.994]
Epoch [114/120    avg_loss:0.008, val_acc:0.995]
Epoch [115/120    avg_loss:0.008, val_acc:0.995]
Epoch [116/120    avg_loss:0.006, val_acc:0.995]
Epoch [117/120    avg_loss:0.006, val_acc:0.995]
Epoch [118/120    avg_loss:0.008, val_acc:0.995]
Epoch [119/120    avg_loss:0.009, val_acc:0.995]
Epoch [120/120    avg_loss:0.007, val_acc:0.995]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1256    0    0    0    1    0    0    1   11   13    3    0
     0    0    0]
 [   0    0    0  713    0   15    0    0    0    6    0    1   12    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    4    1    0    0    0  862    4    0    0
     0    4    0]
 [   0    0    5    0    0    1    2    0    0    0   10 2191    0    1
     0    0    0]
 [   0    0    2    0    1    3    0    0    0    0    7    4  515    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    0    0    0
  1135    1    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    31  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.22222222222223

F1 scores:
[       nan 0.96202532 0.98587127 0.97671233 0.99765808 0.97194164
 0.99018868 0.98039216 1.         0.81818182 0.97346132 0.99050633
 0.96713615 0.99730458 0.98481562 0.93333333 0.98224852]

Kappa:
0.9797296037126138
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe625ae5780>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.493, val_acc:0.450]
Epoch [2/120    avg_loss:1.976, val_acc:0.557]
Epoch [3/120    avg_loss:1.748, val_acc:0.569]
Epoch [4/120    avg_loss:1.487, val_acc:0.635]
Epoch [5/120    avg_loss:1.327, val_acc:0.661]
Epoch [6/120    avg_loss:1.151, val_acc:0.699]
Epoch [7/120    avg_loss:1.007, val_acc:0.707]
Epoch [8/120    avg_loss:0.819, val_acc:0.756]
Epoch [9/120    avg_loss:0.644, val_acc:0.790]
Epoch [10/120    avg_loss:0.699, val_acc:0.801]
Epoch [11/120    avg_loss:0.569, val_acc:0.816]
Epoch [12/120    avg_loss:0.496, val_acc:0.818]
Epoch [13/120    avg_loss:0.391, val_acc:0.874]
Epoch [14/120    avg_loss:0.391, val_acc:0.807]
Epoch [15/120    avg_loss:0.393, val_acc:0.817]
Epoch [16/120    avg_loss:0.403, val_acc:0.852]
Epoch [17/120    avg_loss:0.286, val_acc:0.860]
Epoch [18/120    avg_loss:0.422, val_acc:0.845]
Epoch [19/120    avg_loss:0.310, val_acc:0.885]
Epoch [20/120    avg_loss:0.232, val_acc:0.914]
Epoch [21/120    avg_loss:0.201, val_acc:0.922]
Epoch [22/120    avg_loss:0.173, val_acc:0.926]
Epoch [23/120    avg_loss:0.129, val_acc:0.923]
Epoch [24/120    avg_loss:0.165, val_acc:0.922]
Epoch [25/120    avg_loss:0.139, val_acc:0.923]
Epoch [26/120    avg_loss:0.121, val_acc:0.918]
Epoch [27/120    avg_loss:0.111, val_acc:0.927]
Epoch [28/120    avg_loss:0.098, val_acc:0.904]
Epoch [29/120    avg_loss:0.112, val_acc:0.946]
Epoch [30/120    avg_loss:0.123, val_acc:0.939]
Epoch [31/120    avg_loss:0.089, val_acc:0.956]
Epoch [32/120    avg_loss:0.089, val_acc:0.942]
Epoch [33/120    avg_loss:0.073, val_acc:0.956]
Epoch [34/120    avg_loss:0.056, val_acc:0.961]
Epoch [35/120    avg_loss:0.058, val_acc:0.948]
Epoch [36/120    avg_loss:0.062, val_acc:0.938]
Epoch [37/120    avg_loss:0.077, val_acc:0.946]
Epoch [38/120    avg_loss:0.058, val_acc:0.962]
Epoch [39/120    avg_loss:0.057, val_acc:0.953]
Epoch [40/120    avg_loss:0.048, val_acc:0.952]
Epoch [41/120    avg_loss:0.040, val_acc:0.961]
Epoch [42/120    avg_loss:0.038, val_acc:0.966]
Epoch [43/120    avg_loss:0.039, val_acc:0.972]
Epoch [44/120    avg_loss:0.054, val_acc:0.962]
Epoch [45/120    avg_loss:0.081, val_acc:0.947]
Epoch [46/120    avg_loss:0.068, val_acc:0.968]
Epoch [47/120    avg_loss:0.081, val_acc:0.952]
Epoch [48/120    avg_loss:0.059, val_acc:0.958]
Epoch [49/120    avg_loss:0.054, val_acc:0.955]
Epoch [50/120    avg_loss:0.049, val_acc:0.952]
Epoch [51/120    avg_loss:0.051, val_acc:0.962]
Epoch [52/120    avg_loss:0.039, val_acc:0.967]
Epoch [53/120    avg_loss:0.038, val_acc:0.950]
Epoch [54/120    avg_loss:0.031, val_acc:0.966]
Epoch [55/120    avg_loss:0.034, val_acc:0.964]
Epoch [56/120    avg_loss:0.020, val_acc:0.974]
Epoch [57/120    avg_loss:0.056, val_acc:0.974]
Epoch [58/120    avg_loss:0.025, val_acc:0.955]
Epoch [59/120    avg_loss:0.024, val_acc:0.968]
Epoch [60/120    avg_loss:0.019, val_acc:0.976]
Epoch [61/120    avg_loss:0.020, val_acc:0.974]
Epoch [62/120    avg_loss:0.019, val_acc:0.981]
Epoch [63/120    avg_loss:0.027, val_acc:0.969]
Epoch [64/120    avg_loss:0.027, val_acc:0.970]
Epoch [65/120    avg_loss:0.021, val_acc:0.977]
Epoch [66/120    avg_loss:0.021, val_acc:0.971]
Epoch [67/120    avg_loss:0.025, val_acc:0.976]
Epoch [68/120    avg_loss:0.022, val_acc:0.974]
Epoch [69/120    avg_loss:0.021, val_acc:0.978]
Epoch [70/120    avg_loss:0.017, val_acc:0.974]
Epoch [71/120    avg_loss:0.017, val_acc:0.966]
Epoch [72/120    avg_loss:0.016, val_acc:0.984]
Epoch [73/120    avg_loss:0.020, val_acc:0.972]
Epoch [74/120    avg_loss:0.025, val_acc:0.977]
Epoch [75/120    avg_loss:0.020, val_acc:0.983]
Epoch [76/120    avg_loss:0.016, val_acc:0.975]
Epoch [77/120    avg_loss:0.019, val_acc:0.966]
Epoch [78/120    avg_loss:0.017, val_acc:0.974]
Epoch [79/120    avg_loss:0.022, val_acc:0.973]
Epoch [80/120    avg_loss:0.013, val_acc:0.982]
Epoch [81/120    avg_loss:0.014, val_acc:0.985]
Epoch [82/120    avg_loss:0.011, val_acc:0.983]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.015, val_acc:0.977]
Epoch [85/120    avg_loss:0.017, val_acc:0.966]
Epoch [86/120    avg_loss:0.030, val_acc:0.970]
Epoch [87/120    avg_loss:0.035, val_acc:0.964]
Epoch [88/120    avg_loss:0.028, val_acc:0.972]
Epoch [89/120    avg_loss:0.170, val_acc:0.946]
Epoch [90/120    avg_loss:0.099, val_acc:0.957]
Epoch [91/120    avg_loss:0.050, val_acc:0.958]
Epoch [92/120    avg_loss:0.068, val_acc:0.961]
Epoch [93/120    avg_loss:0.046, val_acc:0.977]
Epoch [94/120    avg_loss:0.023, val_acc:0.976]
Epoch [95/120    avg_loss:0.017, val_acc:0.977]
Epoch [96/120    avg_loss:0.016, val_acc:0.981]
Epoch [97/120    avg_loss:0.017, val_acc:0.983]
Epoch [98/120    avg_loss:0.020, val_acc:0.982]
Epoch [99/120    avg_loss:0.014, val_acc:0.982]
Epoch [100/120    avg_loss:0.015, val_acc:0.983]
Epoch [101/120    avg_loss:0.015, val_acc:0.981]
Epoch [102/120    avg_loss:0.031, val_acc:0.982]
Epoch [103/120    avg_loss:0.016, val_acc:0.982]
Epoch [104/120    avg_loss:0.015, val_acc:0.980]
Epoch [105/120    avg_loss:0.013, val_acc:0.983]
Epoch [106/120    avg_loss:0.014, val_acc:0.980]
Epoch [107/120    avg_loss:0.015, val_acc:0.980]
Epoch [108/120    avg_loss:0.012, val_acc:0.981]
Epoch [109/120    avg_loss:0.015, val_acc:0.980]
Epoch [110/120    avg_loss:0.016, val_acc:0.981]
Epoch [111/120    avg_loss:0.014, val_acc:0.981]
Epoch [112/120    avg_loss:0.015, val_acc:0.983]
Epoch [113/120    avg_loss:0.015, val_acc:0.984]
Epoch [114/120    avg_loss:0.013, val_acc:0.984]
Epoch [115/120    avg_loss:0.015, val_acc:0.984]
Epoch [116/120    avg_loss:0.013, val_acc:0.984]
Epoch [117/120    avg_loss:0.011, val_acc:0.983]
Epoch [118/120    avg_loss:0.013, val_acc:0.984]
Epoch [119/120    avg_loss:0.013, val_acc:0.984]
Epoch [120/120    avg_loss:0.015, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1255    2    1    0    0    0    0    0    8   17    0    0
     0    2    0]
 [   0    0    1  712    2    1    0    0    0   14    2    0   12    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    3    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    9    2    0    0    1  841   19    0    0
     0    3    0]
 [   0    0    5    0    0    0    3    0    0    0    7 2192    0    2
     1    0    0]
 [   0    0    1    4    1    8    0    0    0    1    2    5  510    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1135    4    0]
 [   0    0    0    0    0    1   34    0    0    3    0    0    0    0
    12  297    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.7560975609756

F1 scores:
[       nan 0.96202532 0.98547311 0.97201365 0.99069767 0.97168743
 0.97117517 0.94339623 1.         0.59649123 0.96777906 0.98672068
 0.9631728  0.98666667 0.99213287 0.90964778 0.98203593]

Kappa:
0.9744124117185827
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fafa54d3748>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.455, val_acc:0.459]
Epoch [2/120    avg_loss:1.969, val_acc:0.533]
Epoch [3/120    avg_loss:1.751, val_acc:0.605]
Epoch [4/120    avg_loss:1.472, val_acc:0.634]
Epoch [5/120    avg_loss:1.261, val_acc:0.690]
Epoch [6/120    avg_loss:1.084, val_acc:0.719]
Epoch [7/120    avg_loss:0.940, val_acc:0.726]
Epoch [8/120    avg_loss:0.834, val_acc:0.788]
Epoch [9/120    avg_loss:0.636, val_acc:0.823]
Epoch [10/120    avg_loss:0.668, val_acc:0.764]
Epoch [11/120    avg_loss:0.680, val_acc:0.795]
Epoch [12/120    avg_loss:0.522, val_acc:0.863]
Epoch [13/120    avg_loss:0.466, val_acc:0.821]
Epoch [14/120    avg_loss:0.454, val_acc:0.849]
Epoch [15/120    avg_loss:0.335, val_acc:0.899]
Epoch [16/120    avg_loss:0.308, val_acc:0.858]
Epoch [17/120    avg_loss:0.269, val_acc:0.892]
Epoch [18/120    avg_loss:0.209, val_acc:0.915]
Epoch [19/120    avg_loss:0.194, val_acc:0.921]
Epoch [20/120    avg_loss:0.218, val_acc:0.902]
Epoch [21/120    avg_loss:0.156, val_acc:0.929]
Epoch [22/120    avg_loss:0.142, val_acc:0.931]
Epoch [23/120    avg_loss:0.152, val_acc:0.925]
Epoch [24/120    avg_loss:0.135, val_acc:0.911]
Epoch [25/120    avg_loss:0.136, val_acc:0.935]
Epoch [26/120    avg_loss:0.094, val_acc:0.919]
Epoch [27/120    avg_loss:0.117, val_acc:0.932]
Epoch [28/120    avg_loss:0.096, val_acc:0.933]
Epoch [29/120    avg_loss:0.106, val_acc:0.947]
Epoch [30/120    avg_loss:0.113, val_acc:0.940]
Epoch [31/120    avg_loss:0.090, val_acc:0.945]
Epoch [32/120    avg_loss:0.079, val_acc:0.943]
Epoch [33/120    avg_loss:0.060, val_acc:0.955]
Epoch [34/120    avg_loss:0.053, val_acc:0.955]
Epoch [35/120    avg_loss:0.054, val_acc:0.950]
Epoch [36/120    avg_loss:0.049, val_acc:0.966]
Epoch [37/120    avg_loss:0.058, val_acc:0.956]
Epoch [38/120    avg_loss:0.055, val_acc:0.966]
Epoch [39/120    avg_loss:0.065, val_acc:0.949]
Epoch [40/120    avg_loss:0.053, val_acc:0.967]
Epoch [41/120    avg_loss:0.039, val_acc:0.958]
Epoch [42/120    avg_loss:0.036, val_acc:0.960]
Epoch [43/120    avg_loss:0.040, val_acc:0.964]
Epoch [44/120    avg_loss:0.047, val_acc:0.960]
Epoch [45/120    avg_loss:0.055, val_acc:0.938]
Epoch [46/120    avg_loss:0.041, val_acc:0.960]
Epoch [47/120    avg_loss:0.040, val_acc:0.962]
Epoch [48/120    avg_loss:0.039, val_acc:0.968]
Epoch [49/120    avg_loss:0.031, val_acc:0.964]
Epoch [50/120    avg_loss:0.043, val_acc:0.950]
Epoch [51/120    avg_loss:0.038, val_acc:0.968]
Epoch [52/120    avg_loss:0.052, val_acc:0.955]
Epoch [53/120    avg_loss:0.050, val_acc:0.942]
Epoch [54/120    avg_loss:0.067, val_acc:0.971]
Epoch [55/120    avg_loss:0.024, val_acc:0.972]
Epoch [56/120    avg_loss:0.023, val_acc:0.964]
Epoch [57/120    avg_loss:0.027, val_acc:0.960]
Epoch [58/120    avg_loss:0.022, val_acc:0.972]
Epoch [59/120    avg_loss:0.027, val_acc:0.969]
Epoch [60/120    avg_loss:0.033, val_acc:0.963]
Epoch [61/120    avg_loss:0.029, val_acc:0.966]
Epoch [62/120    avg_loss:0.033, val_acc:0.959]
Epoch [63/120    avg_loss:0.025, val_acc:0.970]
Epoch [64/120    avg_loss:0.033, val_acc:0.975]
Epoch [65/120    avg_loss:0.024, val_acc:0.972]
Epoch [66/120    avg_loss:0.023, val_acc:0.966]
Epoch [67/120    avg_loss:0.019, val_acc:0.976]
Epoch [68/120    avg_loss:0.023, val_acc:0.973]
Epoch [69/120    avg_loss:0.044, val_acc:0.954]
Epoch [70/120    avg_loss:0.171, val_acc:0.932]
Epoch [71/120    avg_loss:0.131, val_acc:0.945]
Epoch [72/120    avg_loss:0.161, val_acc:0.899]
Epoch [73/120    avg_loss:0.222, val_acc:0.914]
Epoch [74/120    avg_loss:0.137, val_acc:0.949]
Epoch [75/120    avg_loss:0.102, val_acc:0.927]
Epoch [76/120    avg_loss:0.083, val_acc:0.960]
Epoch [77/120    avg_loss:0.079, val_acc:0.940]
Epoch [78/120    avg_loss:0.105, val_acc:0.957]
Epoch [79/120    avg_loss:0.061, val_acc:0.958]
Epoch [80/120    avg_loss:0.042, val_acc:0.968]
Epoch [81/120    avg_loss:0.029, val_acc:0.971]
Epoch [82/120    avg_loss:0.025, val_acc:0.975]
Epoch [83/120    avg_loss:0.020, val_acc:0.978]
Epoch [84/120    avg_loss:0.021, val_acc:0.981]
Epoch [85/120    avg_loss:0.021, val_acc:0.978]
Epoch [86/120    avg_loss:0.024, val_acc:0.977]
Epoch [87/120    avg_loss:0.019, val_acc:0.980]
Epoch [88/120    avg_loss:0.022, val_acc:0.980]
Epoch [89/120    avg_loss:0.020, val_acc:0.982]
Epoch [90/120    avg_loss:0.019, val_acc:0.981]
Epoch [91/120    avg_loss:0.020, val_acc:0.980]
Epoch [92/120    avg_loss:0.018, val_acc:0.982]
Epoch [93/120    avg_loss:0.015, val_acc:0.981]
Epoch [94/120    avg_loss:0.019, val_acc:0.980]
Epoch [95/120    avg_loss:0.016, val_acc:0.982]
Epoch [96/120    avg_loss:0.015, val_acc:0.982]
Epoch [97/120    avg_loss:0.017, val_acc:0.982]
Epoch [98/120    avg_loss:0.019, val_acc:0.980]
Epoch [99/120    avg_loss:0.018, val_acc:0.983]
Epoch [100/120    avg_loss:0.015, val_acc:0.983]
Epoch [101/120    avg_loss:0.019, val_acc:0.983]
Epoch [102/120    avg_loss:0.016, val_acc:0.982]
Epoch [103/120    avg_loss:0.014, val_acc:0.983]
Epoch [104/120    avg_loss:0.017, val_acc:0.983]
Epoch [105/120    avg_loss:0.017, val_acc:0.981]
Epoch [106/120    avg_loss:0.012, val_acc:0.982]
Epoch [107/120    avg_loss:0.016, val_acc:0.982]
Epoch [108/120    avg_loss:0.018, val_acc:0.983]
Epoch [109/120    avg_loss:0.018, val_acc:0.981]
Epoch [110/120    avg_loss:0.016, val_acc:0.983]
Epoch [111/120    avg_loss:0.016, val_acc:0.981]
Epoch [112/120    avg_loss:0.018, val_acc:0.986]
Epoch [113/120    avg_loss:0.013, val_acc:0.983]
Epoch [114/120    avg_loss:0.014, val_acc:0.982]
Epoch [115/120    avg_loss:0.015, val_acc:0.984]
Epoch [116/120    avg_loss:0.018, val_acc:0.981]
Epoch [117/120    avg_loss:0.015, val_acc:0.982]
Epoch [118/120    avg_loss:0.017, val_acc:0.984]
Epoch [119/120    avg_loss:0.014, val_acc:0.982]
Epoch [120/120    avg_loss:0.013, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1270    0    0    1    1    0    0    0    4    8    1    0
     0    0    0]
 [   0    0    1  719    2    5    0    0    0   10    1    0    5    4
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    3    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   23    0    0    5    2    0    0    0  840    1    1    0
     3    0    0]
 [   0    0    7    2    0    0    4    0    0    0    7 2182    8    0
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    0    2  525    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    1    0    1    0    0    0
  1130    3    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    48  297    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.02710027100271

F1 scores:
[       nan 1.         0.98221191 0.97889721 0.99297424 0.9740113
 0.99167298 1.         0.99883856 0.73469388 0.97222222 0.9906924
 0.97674419 0.98930481 0.97371822 0.91808346 0.97647059]

Kappa:
0.9775075133707012
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb0e1b457b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.542, val_acc:0.410]
Epoch [2/120    avg_loss:1.977, val_acc:0.521]
Epoch [3/120    avg_loss:1.706, val_acc:0.565]
Epoch [4/120    avg_loss:1.477, val_acc:0.655]
Epoch [5/120    avg_loss:1.246, val_acc:0.692]
Epoch [6/120    avg_loss:1.090, val_acc:0.691]
Epoch [7/120    avg_loss:0.925, val_acc:0.744]
Epoch [8/120    avg_loss:0.855, val_acc:0.749]
Epoch [9/120    avg_loss:0.762, val_acc:0.794]
Epoch [10/120    avg_loss:0.611, val_acc:0.806]
Epoch [11/120    avg_loss:0.574, val_acc:0.825]
Epoch [12/120    avg_loss:0.468, val_acc:0.852]
Epoch [13/120    avg_loss:0.468, val_acc:0.830]
Epoch [14/120    avg_loss:0.586, val_acc:0.816]
Epoch [15/120    avg_loss:0.427, val_acc:0.860]
Epoch [16/120    avg_loss:0.341, val_acc:0.873]
Epoch [17/120    avg_loss:0.322, val_acc:0.902]
Epoch [18/120    avg_loss:0.267, val_acc:0.906]
Epoch [19/120    avg_loss:0.273, val_acc:0.883]
Epoch [20/120    avg_loss:0.297, val_acc:0.875]
Epoch [21/120    avg_loss:0.229, val_acc:0.930]
Epoch [22/120    avg_loss:0.195, val_acc:0.912]
Epoch [23/120    avg_loss:0.212, val_acc:0.902]
Epoch [24/120    avg_loss:0.281, val_acc:0.935]
Epoch [25/120    avg_loss:0.299, val_acc:0.911]
Epoch [26/120    avg_loss:0.239, val_acc:0.908]
Epoch [27/120    avg_loss:0.167, val_acc:0.933]
Epoch [28/120    avg_loss:0.150, val_acc:0.934]
Epoch [29/120    avg_loss:0.142, val_acc:0.924]
Epoch [30/120    avg_loss:0.128, val_acc:0.941]
Epoch [31/120    avg_loss:0.137, val_acc:0.917]
Epoch [32/120    avg_loss:0.119, val_acc:0.924]
Epoch [33/120    avg_loss:0.095, val_acc:0.955]
Epoch [34/120    avg_loss:0.105, val_acc:0.927]
Epoch [35/120    avg_loss:0.127, val_acc:0.922]
Epoch [36/120    avg_loss:0.117, val_acc:0.954]
Epoch [37/120    avg_loss:0.080, val_acc:0.962]
Epoch [38/120    avg_loss:0.085, val_acc:0.959]
Epoch [39/120    avg_loss:0.085, val_acc:0.959]
Epoch [40/120    avg_loss:0.068, val_acc:0.953]
Epoch [41/120    avg_loss:0.059, val_acc:0.961]
Epoch [42/120    avg_loss:0.062, val_acc:0.956]
Epoch [43/120    avg_loss:0.049, val_acc:0.963]
Epoch [44/120    avg_loss:0.060, val_acc:0.949]
Epoch [45/120    avg_loss:0.047, val_acc:0.967]
Epoch [46/120    avg_loss:0.047, val_acc:0.963]
Epoch [47/120    avg_loss:0.051, val_acc:0.965]
Epoch [48/120    avg_loss:0.079, val_acc:0.956]
Epoch [49/120    avg_loss:0.057, val_acc:0.958]
Epoch [50/120    avg_loss:0.060, val_acc:0.970]
Epoch [51/120    avg_loss:0.040, val_acc:0.968]
Epoch [52/120    avg_loss:0.039, val_acc:0.965]
Epoch [53/120    avg_loss:0.042, val_acc:0.970]
Epoch [54/120    avg_loss:0.036, val_acc:0.964]
Epoch [55/120    avg_loss:0.059, val_acc:0.952]
Epoch [56/120    avg_loss:0.053, val_acc:0.949]
Epoch [57/120    avg_loss:0.051, val_acc:0.969]
Epoch [58/120    avg_loss:0.062, val_acc:0.950]
Epoch [59/120    avg_loss:0.046, val_acc:0.959]
Epoch [60/120    avg_loss:0.046, val_acc:0.965]
Epoch [61/120    avg_loss:0.042, val_acc:0.954]
Epoch [62/120    avg_loss:0.071, val_acc:0.962]
Epoch [63/120    avg_loss:0.048, val_acc:0.964]
Epoch [64/120    avg_loss:0.080, val_acc:0.953]
Epoch [65/120    avg_loss:0.055, val_acc:0.949]
Epoch [66/120    avg_loss:0.041, val_acc:0.971]
Epoch [67/120    avg_loss:0.036, val_acc:0.967]
Epoch [68/120    avg_loss:0.041, val_acc:0.958]
Epoch [69/120    avg_loss:0.037, val_acc:0.967]
Epoch [70/120    avg_loss:0.041, val_acc:0.970]
Epoch [71/120    avg_loss:0.036, val_acc:0.961]
Epoch [72/120    avg_loss:0.021, val_acc:0.978]
Epoch [73/120    avg_loss:0.020, val_acc:0.954]
Epoch [74/120    avg_loss:0.029, val_acc:0.971]
Epoch [75/120    avg_loss:0.028, val_acc:0.964]
Epoch [76/120    avg_loss:0.026, val_acc:0.963]
Epoch [77/120    avg_loss:0.029, val_acc:0.972]
Epoch [78/120    avg_loss:0.035, val_acc:0.961]
Epoch [79/120    avg_loss:0.031, val_acc:0.960]
Epoch [80/120    avg_loss:0.019, val_acc:0.980]
Epoch [81/120    avg_loss:0.018, val_acc:0.977]
Epoch [82/120    avg_loss:0.020, val_acc:0.979]
Epoch [83/120    avg_loss:0.019, val_acc:0.971]
Epoch [84/120    avg_loss:0.013, val_acc:0.972]
Epoch [85/120    avg_loss:0.025, val_acc:0.951]
Epoch [86/120    avg_loss:0.029, val_acc:0.970]
Epoch [87/120    avg_loss:0.026, val_acc:0.967]
Epoch [88/120    avg_loss:0.021, val_acc:0.969]
Epoch [89/120    avg_loss:0.019, val_acc:0.977]
Epoch [90/120    avg_loss:0.016, val_acc:0.961]
Epoch [91/120    avg_loss:0.016, val_acc:0.972]
Epoch [92/120    avg_loss:0.016, val_acc:0.973]
Epoch [93/120    avg_loss:0.020, val_acc:0.968]
Epoch [94/120    avg_loss:0.014, val_acc:0.973]
Epoch [95/120    avg_loss:0.015, val_acc:0.975]
Epoch [96/120    avg_loss:0.012, val_acc:0.975]
Epoch [97/120    avg_loss:0.010, val_acc:0.975]
Epoch [98/120    avg_loss:0.009, val_acc:0.974]
Epoch [99/120    avg_loss:0.008, val_acc:0.973]
Epoch [100/120    avg_loss:0.009, val_acc:0.977]
Epoch [101/120    avg_loss:0.009, val_acc:0.977]
Epoch [102/120    avg_loss:0.009, val_acc:0.977]
Epoch [103/120    avg_loss:0.008, val_acc:0.977]
Epoch [104/120    avg_loss:0.008, val_acc:0.977]
Epoch [105/120    avg_loss:0.007, val_acc:0.977]
Epoch [106/120    avg_loss:0.010, val_acc:0.977]
Epoch [107/120    avg_loss:0.008, val_acc:0.977]
Epoch [108/120    avg_loss:0.008, val_acc:0.977]
Epoch [109/120    avg_loss:0.010, val_acc:0.977]
Epoch [110/120    avg_loss:0.009, val_acc:0.977]
Epoch [111/120    avg_loss:0.007, val_acc:0.977]
Epoch [112/120    avg_loss:0.009, val_acc:0.977]
Epoch [113/120    avg_loss:0.008, val_acc:0.977]
Epoch [114/120    avg_loss:0.009, val_acc:0.977]
Epoch [115/120    avg_loss:0.008, val_acc:0.975]
Epoch [116/120    avg_loss:0.009, val_acc:0.975]
Epoch [117/120    avg_loss:0.007, val_acc:0.975]
Epoch [118/120    avg_loss:0.010, val_acc:0.975]
Epoch [119/120    avg_loss:0.009, val_acc:0.975]
Epoch [120/120    avg_loss:0.007, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1263    0    0    0    1    0    0    0    7   11    3    0
     0    0    0]
 [   0    0    0  723    0   11    0    0    0    5    1    0    5    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    1    0    0    6    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    1   30    0    6    1    0    0    0  826    1    0    0
     0   10    0]
 [   0    0   11    0    0    0   10    0    0    0   12 2174    0    3
     0    0    0]
 [   0    0    0    3    3   12    0    0    0    0    4    9  499    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    1    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   31    0    0    0    0    0    0    0
    57  259    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.07317073170732

F1 scores:
[       nan 0.975      0.98671875 0.96207585 0.99300699 0.95622896
 0.96759941 1.         0.99883856 0.73913043 0.95601852 0.98706016
 0.95685523 0.98666667 0.97343616 0.83954619 0.97647059]

Kappa:
0.966629034033294
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa5818b5780>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.494, val_acc:0.512]
Epoch [2/120    avg_loss:1.979, val_acc:0.568]
Epoch [3/120    avg_loss:1.730, val_acc:0.628]
Epoch [4/120    avg_loss:1.504, val_acc:0.663]
Epoch [5/120    avg_loss:1.361, val_acc:0.673]
Epoch [6/120    avg_loss:1.115, val_acc:0.735]
Epoch [7/120    avg_loss:0.933, val_acc:0.762]
Epoch [8/120    avg_loss:0.794, val_acc:0.718]
Epoch [9/120    avg_loss:0.784, val_acc:0.806]
Epoch [10/120    avg_loss:0.612, val_acc:0.817]
Epoch [11/120    avg_loss:0.555, val_acc:0.855]
Epoch [12/120    avg_loss:0.610, val_acc:0.834]
Epoch [13/120    avg_loss:0.516, val_acc:0.849]
Epoch [14/120    avg_loss:0.526, val_acc:0.863]
Epoch [15/120    avg_loss:0.424, val_acc:0.862]
Epoch [16/120    avg_loss:0.378, val_acc:0.903]
Epoch [17/120    avg_loss:0.303, val_acc:0.898]
Epoch [18/120    avg_loss:0.267, val_acc:0.921]
Epoch [19/120    avg_loss:0.267, val_acc:0.893]
Epoch [20/120    avg_loss:0.303, val_acc:0.898]
Epoch [21/120    avg_loss:0.215, val_acc:0.913]
Epoch [22/120    avg_loss:0.216, val_acc:0.921]
Epoch [23/120    avg_loss:0.196, val_acc:0.912]
Epoch [24/120    avg_loss:0.195, val_acc:0.935]
Epoch [25/120    avg_loss:0.176, val_acc:0.915]
Epoch [26/120    avg_loss:0.278, val_acc:0.862]
Epoch [27/120    avg_loss:0.234, val_acc:0.903]
Epoch [28/120    avg_loss:0.199, val_acc:0.938]
Epoch [29/120    avg_loss:0.171, val_acc:0.942]
Epoch [30/120    avg_loss:0.149, val_acc:0.930]
Epoch [31/120    avg_loss:0.128, val_acc:0.941]
Epoch [32/120    avg_loss:0.101, val_acc:0.957]
Epoch [33/120    avg_loss:0.100, val_acc:0.959]
Epoch [34/120    avg_loss:0.080, val_acc:0.955]
Epoch [35/120    avg_loss:0.089, val_acc:0.955]
Epoch [36/120    avg_loss:0.094, val_acc:0.943]
Epoch [37/120    avg_loss:0.110, val_acc:0.957]
Epoch [38/120    avg_loss:0.068, val_acc:0.966]
Epoch [39/120    avg_loss:0.062, val_acc:0.963]
Epoch [40/120    avg_loss:0.061, val_acc:0.970]
Epoch [41/120    avg_loss:0.063, val_acc:0.964]
Epoch [42/120    avg_loss:0.070, val_acc:0.956]
Epoch [43/120    avg_loss:0.071, val_acc:0.954]
Epoch [44/120    avg_loss:0.071, val_acc:0.940]
Epoch [45/120    avg_loss:0.067, val_acc:0.953]
Epoch [46/120    avg_loss:0.053, val_acc:0.957]
Epoch [47/120    avg_loss:0.059, val_acc:0.969]
Epoch [48/120    avg_loss:0.054, val_acc:0.963]
Epoch [49/120    avg_loss:0.061, val_acc:0.961]
Epoch [50/120    avg_loss:0.137, val_acc:0.938]
Epoch [51/120    avg_loss:0.119, val_acc:0.904]
Epoch [52/120    avg_loss:0.223, val_acc:0.945]
Epoch [53/120    avg_loss:0.154, val_acc:0.925]
Epoch [54/120    avg_loss:0.109, val_acc:0.935]
Epoch [55/120    avg_loss:0.086, val_acc:0.947]
Epoch [56/120    avg_loss:0.073, val_acc:0.953]
Epoch [57/120    avg_loss:0.065, val_acc:0.959]
Epoch [58/120    avg_loss:0.054, val_acc:0.961]
Epoch [59/120    avg_loss:0.051, val_acc:0.963]
Epoch [60/120    avg_loss:0.052, val_acc:0.964]
Epoch [61/120    avg_loss:0.043, val_acc:0.966]
Epoch [62/120    avg_loss:0.043, val_acc:0.969]
Epoch [63/120    avg_loss:0.043, val_acc:0.964]
Epoch [64/120    avg_loss:0.039, val_acc:0.968]
Epoch [65/120    avg_loss:0.041, val_acc:0.966]
Epoch [66/120    avg_loss:0.044, val_acc:0.969]
Epoch [67/120    avg_loss:0.042, val_acc:0.969]
Epoch [68/120    avg_loss:0.037, val_acc:0.970]
Epoch [69/120    avg_loss:0.043, val_acc:0.970]
Epoch [70/120    avg_loss:0.035, val_acc:0.970]
Epoch [71/120    avg_loss:0.041, val_acc:0.970]
Epoch [72/120    avg_loss:0.041, val_acc:0.969]
Epoch [73/120    avg_loss:0.039, val_acc:0.969]
Epoch [74/120    avg_loss:0.038, val_acc:0.969]
Epoch [75/120    avg_loss:0.040, val_acc:0.971]
Epoch [76/120    avg_loss:0.038, val_acc:0.973]
Epoch [77/120    avg_loss:0.040, val_acc:0.972]
Epoch [78/120    avg_loss:0.039, val_acc:0.972]
Epoch [79/120    avg_loss:0.036, val_acc:0.972]
Epoch [80/120    avg_loss:0.041, val_acc:0.972]
Epoch [81/120    avg_loss:0.039, val_acc:0.972]
Epoch [82/120    avg_loss:0.042, val_acc:0.973]
Epoch [83/120    avg_loss:0.037, val_acc:0.973]
Epoch [84/120    avg_loss:0.036, val_acc:0.972]
Epoch [85/120    avg_loss:0.037, val_acc:0.972]
Epoch [86/120    avg_loss:0.033, val_acc:0.972]
Epoch [87/120    avg_loss:0.037, val_acc:0.972]
Epoch [88/120    avg_loss:0.035, val_acc:0.972]
Epoch [89/120    avg_loss:0.031, val_acc:0.972]
Epoch [90/120    avg_loss:0.034, val_acc:0.972]
Epoch [91/120    avg_loss:0.044, val_acc:0.972]
Epoch [92/120    avg_loss:0.036, val_acc:0.972]
Epoch [93/120    avg_loss:0.037, val_acc:0.972]
Epoch [94/120    avg_loss:0.040, val_acc:0.972]
Epoch [95/120    avg_loss:0.036, val_acc:0.972]
Epoch [96/120    avg_loss:0.030, val_acc:0.972]
Epoch [97/120    avg_loss:0.037, val_acc:0.972]
Epoch [98/120    avg_loss:0.035, val_acc:0.972]
Epoch [99/120    avg_loss:0.035, val_acc:0.972]
Epoch [100/120    avg_loss:0.038, val_acc:0.972]
Epoch [101/120    avg_loss:0.041, val_acc:0.972]
Epoch [102/120    avg_loss:0.036, val_acc:0.972]
Epoch [103/120    avg_loss:0.035, val_acc:0.972]
Epoch [104/120    avg_loss:0.038, val_acc:0.972]
Epoch [105/120    avg_loss:0.037, val_acc:0.972]
Epoch [106/120    avg_loss:0.042, val_acc:0.972]
Epoch [107/120    avg_loss:0.035, val_acc:0.972]
Epoch [108/120    avg_loss:0.042, val_acc:0.972]
Epoch [109/120    avg_loss:0.035, val_acc:0.972]
Epoch [110/120    avg_loss:0.035, val_acc:0.972]
Epoch [111/120    avg_loss:0.033, val_acc:0.972]
Epoch [112/120    avg_loss:0.048, val_acc:0.972]
Epoch [113/120    avg_loss:0.034, val_acc:0.972]
Epoch [114/120    avg_loss:0.037, val_acc:0.972]
Epoch [115/120    avg_loss:0.033, val_acc:0.972]
Epoch [116/120    avg_loss:0.038, val_acc:0.972]
Epoch [117/120    avg_loss:0.038, val_acc:0.972]
Epoch [118/120    avg_loss:0.035, val_acc:0.972]
Epoch [119/120    avg_loss:0.037, val_acc:0.972]
Epoch [120/120    avg_loss:0.037, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    2 1221    6    9    0   11    0    0    0   13   21    1    0
     0    1    0]
 [   0    0    1  726    0    7    0    0    0    5    1    0    3    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  420    0    7    0    4    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   32   19    0    6    1    0    0    0  795   21    0    0
     1    0    0]
 [   0    0   10    0    0    1    5    0    0    0   11 2176    2    4
     1    0    0]
 [   0    0    0   21   10   11    0    0    0    0    4    2  479    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    0    0    1    0    3    0    0    0
  1126    0    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    53  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.16260162601625

F1 scores:
[       nan 0.9382716  0.95802275 0.95589203 0.95730337 0.94488189
 0.9753915  0.87719298 0.99883856 0.8        0.93255132 0.98217107
 0.94013739 0.97883598 0.96818573 0.89348172 0.96      ]

Kappa:
0.956241133977249
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7a4c8d5748>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.516, val_acc:0.412]
Epoch [2/120    avg_loss:1.959, val_acc:0.462]
Epoch [3/120    avg_loss:1.742, val_acc:0.617]
Epoch [4/120    avg_loss:1.539, val_acc:0.651]
Epoch [5/120    avg_loss:1.354, val_acc:0.587]
Epoch [6/120    avg_loss:1.128, val_acc:0.681]
Epoch [7/120    avg_loss:0.969, val_acc:0.759]
Epoch [8/120    avg_loss:0.857, val_acc:0.751]
Epoch [9/120    avg_loss:0.784, val_acc:0.782]
Epoch [10/120    avg_loss:0.666, val_acc:0.804]
Epoch [11/120    avg_loss:0.598, val_acc:0.829]
Epoch [12/120    avg_loss:0.522, val_acc:0.840]
Epoch [13/120    avg_loss:0.426, val_acc:0.848]
Epoch [14/120    avg_loss:0.379, val_acc:0.854]
Epoch [15/120    avg_loss:0.368, val_acc:0.818]
Epoch [16/120    avg_loss:0.384, val_acc:0.893]
Epoch [17/120    avg_loss:0.285, val_acc:0.888]
Epoch [18/120    avg_loss:0.239, val_acc:0.905]
Epoch [19/120    avg_loss:0.251, val_acc:0.883]
Epoch [20/120    avg_loss:0.328, val_acc:0.865]
Epoch [21/120    avg_loss:0.313, val_acc:0.887]
Epoch [22/120    avg_loss:0.224, val_acc:0.927]
Epoch [23/120    avg_loss:0.200, val_acc:0.926]
Epoch [24/120    avg_loss:0.192, val_acc:0.920]
Epoch [25/120    avg_loss:0.153, val_acc:0.924]
Epoch [26/120    avg_loss:0.128, val_acc:0.933]
Epoch [27/120    avg_loss:0.126, val_acc:0.930]
Epoch [28/120    avg_loss:0.112, val_acc:0.949]
Epoch [29/120    avg_loss:0.101, val_acc:0.930]
Epoch [30/120    avg_loss:0.121, val_acc:0.935]
Epoch [31/120    avg_loss:0.108, val_acc:0.944]
Epoch [32/120    avg_loss:0.086, val_acc:0.959]
Epoch [33/120    avg_loss:0.084, val_acc:0.948]
Epoch [34/120    avg_loss:0.073, val_acc:0.946]
Epoch [35/120    avg_loss:0.100, val_acc:0.955]
Epoch [36/120    avg_loss:0.096, val_acc:0.949]
Epoch [37/120    avg_loss:0.076, val_acc:0.932]
Epoch [38/120    avg_loss:0.075, val_acc:0.965]
Epoch [39/120    avg_loss:0.070, val_acc:0.952]
Epoch [40/120    avg_loss:0.068, val_acc:0.953]
Epoch [41/120    avg_loss:0.070, val_acc:0.958]
Epoch [42/120    avg_loss:0.059, val_acc:0.955]
Epoch [43/120    avg_loss:0.078, val_acc:0.956]
Epoch [44/120    avg_loss:0.064, val_acc:0.959]
Epoch [45/120    avg_loss:0.058, val_acc:0.953]
Epoch [46/120    avg_loss:0.091, val_acc:0.948]
Epoch [47/120    avg_loss:0.080, val_acc:0.936]
Epoch [48/120    avg_loss:0.052, val_acc:0.958]
Epoch [49/120    avg_loss:0.088, val_acc:0.945]
Epoch [50/120    avg_loss:0.077, val_acc:0.958]
Epoch [51/120    avg_loss:0.063, val_acc:0.956]
Epoch [52/120    avg_loss:0.057, val_acc:0.969]
Epoch [53/120    avg_loss:0.046, val_acc:0.972]
Epoch [54/120    avg_loss:0.039, val_acc:0.978]
Epoch [55/120    avg_loss:0.028, val_acc:0.978]
Epoch [56/120    avg_loss:0.029, val_acc:0.978]
Epoch [57/120    avg_loss:0.028, val_acc:0.978]
Epoch [58/120    avg_loss:0.030, val_acc:0.975]
Epoch [59/120    avg_loss:0.030, val_acc:0.975]
Epoch [60/120    avg_loss:0.033, val_acc:0.978]
Epoch [61/120    avg_loss:0.033, val_acc:0.979]
Epoch [62/120    avg_loss:0.025, val_acc:0.980]
Epoch [63/120    avg_loss:0.029, val_acc:0.980]
Epoch [64/120    avg_loss:0.023, val_acc:0.978]
Epoch [65/120    avg_loss:0.025, val_acc:0.980]
Epoch [66/120    avg_loss:0.029, val_acc:0.979]
Epoch [67/120    avg_loss:0.023, val_acc:0.978]
Epoch [68/120    avg_loss:0.029, val_acc:0.979]
Epoch [69/120    avg_loss:0.024, val_acc:0.978]
Epoch [70/120    avg_loss:0.027, val_acc:0.978]
Epoch [71/120    avg_loss:0.024, val_acc:0.978]
Epoch [72/120    avg_loss:0.021, val_acc:0.980]
Epoch [73/120    avg_loss:0.025, val_acc:0.980]
Epoch [74/120    avg_loss:0.031, val_acc:0.979]
Epoch [75/120    avg_loss:0.029, val_acc:0.979]
Epoch [76/120    avg_loss:0.022, val_acc:0.982]
Epoch [77/120    avg_loss:0.022, val_acc:0.981]
Epoch [78/120    avg_loss:0.021, val_acc:0.981]
Epoch [79/120    avg_loss:0.027, val_acc:0.979]
Epoch [80/120    avg_loss:0.024, val_acc:0.980]
Epoch [81/120    avg_loss:0.025, val_acc:0.982]
Epoch [82/120    avg_loss:0.023, val_acc:0.982]
Epoch [83/120    avg_loss:0.024, val_acc:0.981]
Epoch [84/120    avg_loss:0.023, val_acc:0.983]
Epoch [85/120    avg_loss:0.024, val_acc:0.977]
Epoch [86/120    avg_loss:0.022, val_acc:0.982]
Epoch [87/120    avg_loss:0.023, val_acc:0.982]
Epoch [88/120    avg_loss:0.021, val_acc:0.978]
Epoch [89/120    avg_loss:0.022, val_acc:0.980]
Epoch [90/120    avg_loss:0.028, val_acc:0.981]
Epoch [91/120    avg_loss:0.022, val_acc:0.980]
Epoch [92/120    avg_loss:0.024, val_acc:0.980]
Epoch [93/120    avg_loss:0.022, val_acc:0.978]
Epoch [94/120    avg_loss:0.021, val_acc:0.981]
Epoch [95/120    avg_loss:0.019, val_acc:0.982]
Epoch [96/120    avg_loss:0.023, val_acc:0.982]
Epoch [97/120    avg_loss:0.018, val_acc:0.982]
Epoch [98/120    avg_loss:0.022, val_acc:0.982]
Epoch [99/120    avg_loss:0.020, val_acc:0.982]
Epoch [100/120    avg_loss:0.021, val_acc:0.983]
Epoch [101/120    avg_loss:0.023, val_acc:0.982]
Epoch [102/120    avg_loss:0.017, val_acc:0.983]
Epoch [103/120    avg_loss:0.022, val_acc:0.982]
Epoch [104/120    avg_loss:0.018, val_acc:0.983]
Epoch [105/120    avg_loss:0.021, val_acc:0.983]
Epoch [106/120    avg_loss:0.021, val_acc:0.983]
Epoch [107/120    avg_loss:0.024, val_acc:0.983]
Epoch [108/120    avg_loss:0.021, val_acc:0.983]
Epoch [109/120    avg_loss:0.027, val_acc:0.983]
Epoch [110/120    avg_loss:0.022, val_acc:0.983]
Epoch [111/120    avg_loss:0.019, val_acc:0.983]
Epoch [112/120    avg_loss:0.022, val_acc:0.983]
Epoch [113/120    avg_loss:0.019, val_acc:0.983]
Epoch [114/120    avg_loss:0.022, val_acc:0.982]
Epoch [115/120    avg_loss:0.021, val_acc:0.981]
Epoch [116/120    avg_loss:0.016, val_acc:0.981]
Epoch [117/120    avg_loss:0.017, val_acc:0.981]
Epoch [118/120    avg_loss:0.022, val_acc:0.981]
Epoch [119/120    avg_loss:0.023, val_acc:0.983]
Epoch [120/120    avg_loss:0.018, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1254    4    0    0    3    0    0    1    5   17    0    0
     0    1    0]
 [   0    0    1  723    1    7    0    0    0   13    0    0    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    5    0    5    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   10   37    0    6    1    0    0    0  811    3    0    0
     0    7    0]
 [   0    0    2    0    0    0    3    0    0    0    7 2194    1    2
     1    0    0]
 [   0    0    0   18    8    7    0    0    0    0    7    8  480    0
     0    0    6]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    3    0    0    0
  1132    0    0]
 [   0    0    0    0    0    0   30    0    0    0    0    0    0    0
    33  284    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.98644986449864

F1 scores:
[       nan 0.975      0.98275862 0.94571615 0.97931034 0.95918367
 0.97032641 0.90909091 0.99883856 0.62962963 0.94853801 0.98940248
 0.94302554 0.99191375 0.98136107 0.88888889 0.96551724]

Kappa:
0.9656338733598954
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fba37b9e7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.531, val_acc:0.453]
Epoch [2/120    avg_loss:2.022, val_acc:0.525]
Epoch [3/120    avg_loss:1.776, val_acc:0.599]
Epoch [4/120    avg_loss:1.597, val_acc:0.627]
Epoch [5/120    avg_loss:1.388, val_acc:0.692]
Epoch [6/120    avg_loss:1.252, val_acc:0.652]
Epoch [7/120    avg_loss:1.078, val_acc:0.719]
Epoch [8/120    avg_loss:0.872, val_acc:0.759]
Epoch [9/120    avg_loss:0.865, val_acc:0.790]
Epoch [10/120    avg_loss:0.685, val_acc:0.798]
Epoch [11/120    avg_loss:0.643, val_acc:0.819]
Epoch [12/120    avg_loss:0.514, val_acc:0.807]
Epoch [13/120    avg_loss:0.457, val_acc:0.824]
Epoch [14/120    avg_loss:0.469, val_acc:0.874]
Epoch [15/120    avg_loss:0.410, val_acc:0.847]
Epoch [16/120    avg_loss:0.434, val_acc:0.840]
Epoch [17/120    avg_loss:0.371, val_acc:0.869]
Epoch [18/120    avg_loss:0.281, val_acc:0.884]
Epoch [19/120    avg_loss:0.236, val_acc:0.917]
Epoch [20/120    avg_loss:0.213, val_acc:0.896]
Epoch [21/120    avg_loss:0.220, val_acc:0.905]
Epoch [22/120    avg_loss:0.176, val_acc:0.917]
Epoch [23/120    avg_loss:0.178, val_acc:0.913]
Epoch [24/120    avg_loss:0.248, val_acc:0.887]
Epoch [25/120    avg_loss:0.329, val_acc:0.901]
Epoch [26/120    avg_loss:0.207, val_acc:0.922]
Epoch [27/120    avg_loss:0.157, val_acc:0.943]
Epoch [28/120    avg_loss:0.122, val_acc:0.936]
Epoch [29/120    avg_loss:0.096, val_acc:0.939]
Epoch [30/120    avg_loss:0.105, val_acc:0.946]
Epoch [31/120    avg_loss:0.112, val_acc:0.924]
Epoch [32/120    avg_loss:0.101, val_acc:0.926]
Epoch [33/120    avg_loss:0.098, val_acc:0.945]
Epoch [34/120    avg_loss:0.082, val_acc:0.948]
Epoch [35/120    avg_loss:0.086, val_acc:0.948]
Epoch [36/120    avg_loss:0.101, val_acc:0.950]
Epoch [37/120    avg_loss:0.092, val_acc:0.933]
Epoch [38/120    avg_loss:0.077, val_acc:0.950]
Epoch [39/120    avg_loss:0.063, val_acc:0.958]
Epoch [40/120    avg_loss:0.059, val_acc:0.951]
Epoch [41/120    avg_loss:0.061, val_acc:0.959]
Epoch [42/120    avg_loss:0.060, val_acc:0.959]
Epoch [43/120    avg_loss:0.048, val_acc:0.952]
Epoch [44/120    avg_loss:0.052, val_acc:0.955]
Epoch [45/120    avg_loss:0.056, val_acc:0.961]
Epoch [46/120    avg_loss:0.085, val_acc:0.938]
Epoch [47/120    avg_loss:0.088, val_acc:0.936]
Epoch [48/120    avg_loss:0.073, val_acc:0.955]
Epoch [49/120    avg_loss:0.061, val_acc:0.963]
Epoch [50/120    avg_loss:0.039, val_acc:0.973]
Epoch [51/120    avg_loss:0.047, val_acc:0.972]
Epoch [52/120    avg_loss:0.060, val_acc:0.952]
Epoch [53/120    avg_loss:0.052, val_acc:0.961]
Epoch [54/120    avg_loss:0.045, val_acc:0.959]
Epoch [55/120    avg_loss:0.039, val_acc:0.969]
Epoch [56/120    avg_loss:0.034, val_acc:0.965]
Epoch [57/120    avg_loss:0.037, val_acc:0.965]
Epoch [58/120    avg_loss:0.042, val_acc:0.974]
Epoch [59/120    avg_loss:0.032, val_acc:0.972]
Epoch [60/120    avg_loss:0.039, val_acc:0.964]
Epoch [61/120    avg_loss:0.024, val_acc:0.970]
Epoch [62/120    avg_loss:0.027, val_acc:0.965]
Epoch [63/120    avg_loss:0.034, val_acc:0.956]
Epoch [64/120    avg_loss:0.076, val_acc:0.961]
Epoch [65/120    avg_loss:0.062, val_acc:0.956]
Epoch [66/120    avg_loss:0.051, val_acc:0.970]
Epoch [67/120    avg_loss:0.039, val_acc:0.960]
Epoch [68/120    avg_loss:0.061, val_acc:0.961]
Epoch [69/120    avg_loss:0.036, val_acc:0.965]
Epoch [70/120    avg_loss:0.026, val_acc:0.983]
Epoch [71/120    avg_loss:0.027, val_acc:0.977]
Epoch [72/120    avg_loss:0.034, val_acc:0.977]
Epoch [73/120    avg_loss:0.028, val_acc:0.963]
Epoch [74/120    avg_loss:0.043, val_acc:0.970]
Epoch [75/120    avg_loss:0.027, val_acc:0.972]
Epoch [76/120    avg_loss:0.029, val_acc:0.974]
Epoch [77/120    avg_loss:0.017, val_acc:0.977]
Epoch [78/120    avg_loss:0.026, val_acc:0.968]
Epoch [79/120    avg_loss:0.028, val_acc:0.972]
Epoch [80/120    avg_loss:0.023, val_acc:0.978]
Epoch [81/120    avg_loss:0.023, val_acc:0.969]
Epoch [82/120    avg_loss:0.026, val_acc:0.977]
Epoch [83/120    avg_loss:0.052, val_acc:0.975]
Epoch [84/120    avg_loss:0.028, val_acc:0.979]
Epoch [85/120    avg_loss:0.027, val_acc:0.981]
Epoch [86/120    avg_loss:0.020, val_acc:0.979]
Epoch [87/120    avg_loss:0.018, val_acc:0.981]
Epoch [88/120    avg_loss:0.017, val_acc:0.981]
Epoch [89/120    avg_loss:0.015, val_acc:0.981]
Epoch [90/120    avg_loss:0.015, val_acc:0.982]
Epoch [91/120    avg_loss:0.015, val_acc:0.982]
Epoch [92/120    avg_loss:0.015, val_acc:0.982]
Epoch [93/120    avg_loss:0.013, val_acc:0.981]
Epoch [94/120    avg_loss:0.013, val_acc:0.981]
Epoch [95/120    avg_loss:0.012, val_acc:0.982]
Epoch [96/120    avg_loss:0.011, val_acc:0.981]
Epoch [97/120    avg_loss:0.014, val_acc:0.981]
Epoch [98/120    avg_loss:0.012, val_acc:0.981]
Epoch [99/120    avg_loss:0.014, val_acc:0.981]
Epoch [100/120    avg_loss:0.015, val_acc:0.981]
Epoch [101/120    avg_loss:0.012, val_acc:0.980]
Epoch [102/120    avg_loss:0.012, val_acc:0.981]
Epoch [103/120    avg_loss:0.011, val_acc:0.981]
Epoch [104/120    avg_loss:0.012, val_acc:0.981]
Epoch [105/120    avg_loss:0.013, val_acc:0.981]
Epoch [106/120    avg_loss:0.012, val_acc:0.981]
Epoch [107/120    avg_loss:0.010, val_acc:0.981]
Epoch [108/120    avg_loss:0.010, val_acc:0.981]
Epoch [109/120    avg_loss:0.011, val_acc:0.981]
Epoch [110/120    avg_loss:0.012, val_acc:0.981]
Epoch [111/120    avg_loss:0.011, val_acc:0.981]
Epoch [112/120    avg_loss:0.011, val_acc:0.981]
Epoch [113/120    avg_loss:0.011, val_acc:0.981]
Epoch [114/120    avg_loss:0.010, val_acc:0.981]
Epoch [115/120    avg_loss:0.013, val_acc:0.981]
Epoch [116/120    avg_loss:0.012, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.981]
Epoch [118/120    avg_loss:0.014, val_acc:0.981]
Epoch [119/120    avg_loss:0.014, val_acc:0.981]
Epoch [120/120    avg_loss:0.014, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1269    4    0    0    1    0    0    0    4    7    0    0
     0    0    0]
 [   0    0    0  725    0    9    0    0    0    6    1    0    4    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    1    0    4    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   29   41    0    5    0    0    0    0  783   16    1    0
     0    0    0]
 [   0    0    9    0    0    0    5    0    0    0    8 2162   23    3
     0    0    0]
 [   0    0    0   23   10    6    0    0    0    0    3   13  476    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   12    0    0    1    0    1    0    0    0
  1125    0    0]
 [   0    0    0    0    0    0   30    0    0    0    0    0    0    0
    33  284    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.44444444444444

F1 scores:
[       nan 0.975      0.97916667 0.94033722 0.97706422 0.95408735
 0.97257228 0.98039216 0.99883856 0.72727273 0.93381038 0.98072125
 0.91626564 0.98666667 0.97783572 0.90015848 0.97647059]

Kappa:
0.9594626069075273
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f081bdc06d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.512, val_acc:0.419]
Epoch [2/120    avg_loss:1.960, val_acc:0.584]
Epoch [3/120    avg_loss:1.752, val_acc:0.586]
Epoch [4/120    avg_loss:1.545, val_acc:0.700]
Epoch [5/120    avg_loss:1.346, val_acc:0.729]
Epoch [6/120    avg_loss:1.222, val_acc:0.713]
Epoch [7/120    avg_loss:1.002, val_acc:0.779]
Epoch [8/120    avg_loss:0.844, val_acc:0.794]
Epoch [9/120    avg_loss:0.828, val_acc:0.791]
Epoch [10/120    avg_loss:0.723, val_acc:0.839]
Epoch [11/120    avg_loss:0.549, val_acc:0.867]
Epoch [12/120    avg_loss:0.438, val_acc:0.865]
Epoch [13/120    avg_loss:0.430, val_acc:0.869]
Epoch [14/120    avg_loss:0.367, val_acc:0.896]
Epoch [15/120    avg_loss:0.337, val_acc:0.849]
Epoch [16/120    avg_loss:0.350, val_acc:0.902]
Epoch [17/120    avg_loss:0.285, val_acc:0.901]
Epoch [18/120    avg_loss:0.386, val_acc:0.881]
Epoch [19/120    avg_loss:0.307, val_acc:0.902]
Epoch [20/120    avg_loss:0.274, val_acc:0.910]
Epoch [21/120    avg_loss:0.195, val_acc:0.916]
Epoch [22/120    avg_loss:0.189, val_acc:0.929]
Epoch [23/120    avg_loss:0.211, val_acc:0.920]
Epoch [24/120    avg_loss:0.204, val_acc:0.924]
Epoch [25/120    avg_loss:0.169, val_acc:0.914]
Epoch [26/120    avg_loss:0.146, val_acc:0.934]
Epoch [27/120    avg_loss:0.131, val_acc:0.938]
Epoch [28/120    avg_loss:0.126, val_acc:0.938]
Epoch [29/120    avg_loss:0.168, val_acc:0.924]
Epoch [30/120    avg_loss:0.181, val_acc:0.927]
Epoch [31/120    avg_loss:0.149, val_acc:0.948]
Epoch [32/120    avg_loss:0.115, val_acc:0.946]
Epoch [33/120    avg_loss:0.100, val_acc:0.958]
Epoch [34/120    avg_loss:0.090, val_acc:0.961]
Epoch [35/120    avg_loss:0.069, val_acc:0.963]
Epoch [36/120    avg_loss:0.068, val_acc:0.963]
Epoch [37/120    avg_loss:0.058, val_acc:0.961]
Epoch [38/120    avg_loss:0.054, val_acc:0.965]
Epoch [39/120    avg_loss:0.056, val_acc:0.961]
Epoch [40/120    avg_loss:0.082, val_acc:0.952]
Epoch [41/120    avg_loss:0.061, val_acc:0.962]
Epoch [42/120    avg_loss:0.065, val_acc:0.972]
Epoch [43/120    avg_loss:0.047, val_acc:0.967]
Epoch [44/120    avg_loss:0.056, val_acc:0.967]
Epoch [45/120    avg_loss:0.066, val_acc:0.963]
Epoch [46/120    avg_loss:0.092, val_acc:0.973]
Epoch [47/120    avg_loss:0.084, val_acc:0.952]
Epoch [48/120    avg_loss:0.075, val_acc:0.970]
Epoch [49/120    avg_loss:0.050, val_acc:0.965]
Epoch [50/120    avg_loss:0.066, val_acc:0.964]
Epoch [51/120    avg_loss:0.062, val_acc:0.975]
Epoch [52/120    avg_loss:0.055, val_acc:0.954]
Epoch [53/120    avg_loss:0.048, val_acc:0.964]
Epoch [54/120    avg_loss:0.057, val_acc:0.962]
Epoch [55/120    avg_loss:0.046, val_acc:0.970]
Epoch [56/120    avg_loss:0.043, val_acc:0.970]
Epoch [57/120    avg_loss:0.039, val_acc:0.968]
Epoch [58/120    avg_loss:0.063, val_acc:0.961]
Epoch [59/120    avg_loss:0.063, val_acc:0.970]
Epoch [60/120    avg_loss:0.036, val_acc:0.970]
Epoch [61/120    avg_loss:0.040, val_acc:0.971]
Epoch [62/120    avg_loss:0.029, val_acc:0.970]
Epoch [63/120    avg_loss:0.024, val_acc:0.972]
Epoch [64/120    avg_loss:0.029, val_acc:0.969]
Epoch [65/120    avg_loss:0.023, val_acc:0.973]
Epoch [66/120    avg_loss:0.018, val_acc:0.975]
Epoch [67/120    avg_loss:0.017, val_acc:0.975]
Epoch [68/120    avg_loss:0.019, val_acc:0.978]
Epoch [69/120    avg_loss:0.017, val_acc:0.978]
Epoch [70/120    avg_loss:0.016, val_acc:0.979]
Epoch [71/120    avg_loss:0.014, val_acc:0.979]
Epoch [72/120    avg_loss:0.017, val_acc:0.979]
Epoch [73/120    avg_loss:0.015, val_acc:0.980]
Epoch [74/120    avg_loss:0.016, val_acc:0.979]
Epoch [75/120    avg_loss:0.017, val_acc:0.979]
Epoch [76/120    avg_loss:0.016, val_acc:0.980]
Epoch [77/120    avg_loss:0.017, val_acc:0.979]
Epoch [78/120    avg_loss:0.017, val_acc:0.980]
Epoch [79/120    avg_loss:0.014, val_acc:0.980]
Epoch [80/120    avg_loss:0.015, val_acc:0.980]
Epoch [81/120    avg_loss:0.016, val_acc:0.980]
Epoch [82/120    avg_loss:0.015, val_acc:0.980]
Epoch [83/120    avg_loss:0.012, val_acc:0.981]
Epoch [84/120    avg_loss:0.013, val_acc:0.980]
Epoch [85/120    avg_loss:0.014, val_acc:0.980]
Epoch [86/120    avg_loss:0.017, val_acc:0.979]
Epoch [87/120    avg_loss:0.013, val_acc:0.980]
Epoch [88/120    avg_loss:0.017, val_acc:0.979]
Epoch [89/120    avg_loss:0.013, val_acc:0.981]
Epoch [90/120    avg_loss:0.015, val_acc:0.981]
Epoch [91/120    avg_loss:0.012, val_acc:0.979]
Epoch [92/120    avg_loss:0.013, val_acc:0.979]
Epoch [93/120    avg_loss:0.013, val_acc:0.978]
Epoch [94/120    avg_loss:0.014, val_acc:0.980]
Epoch [95/120    avg_loss:0.014, val_acc:0.980]
Epoch [96/120    avg_loss:0.011, val_acc:0.980]
Epoch [97/120    avg_loss:0.013, val_acc:0.982]
Epoch [98/120    avg_loss:0.012, val_acc:0.980]
Epoch [99/120    avg_loss:0.013, val_acc:0.979]
Epoch [100/120    avg_loss:0.014, val_acc:0.980]
Epoch [101/120    avg_loss:0.012, val_acc:0.980]
Epoch [102/120    avg_loss:0.013, val_acc:0.980]
Epoch [103/120    avg_loss:0.012, val_acc:0.980]
Epoch [104/120    avg_loss:0.016, val_acc:0.980]
Epoch [105/120    avg_loss:0.010, val_acc:0.980]
Epoch [106/120    avg_loss:0.015, val_acc:0.980]
Epoch [107/120    avg_loss:0.013, val_acc:0.979]
Epoch [108/120    avg_loss:0.012, val_acc:0.979]
Epoch [109/120    avg_loss:0.011, val_acc:0.980]
Epoch [110/120    avg_loss:0.012, val_acc:0.980]
Epoch [111/120    avg_loss:0.013, val_acc:0.980]
Epoch [112/120    avg_loss:0.014, val_acc:0.980]
Epoch [113/120    avg_loss:0.012, val_acc:0.980]
Epoch [114/120    avg_loss:0.016, val_acc:0.979]
Epoch [115/120    avg_loss:0.011, val_acc:0.979]
Epoch [116/120    avg_loss:0.010, val_acc:0.979]
Epoch [117/120    avg_loss:0.013, val_acc:0.980]
Epoch [118/120    avg_loss:0.012, val_acc:0.979]
Epoch [119/120    avg_loss:0.012, val_acc:0.980]
Epoch [120/120    avg_loss:0.011, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1267    0    0    0    1    0    0    0    3   12    0    0
     0    2    0]
 [   0    0    1  702    3   14    0    0    0   18    0    0    4    5
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    4    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   17    6    0    5    0    0    0    0  836    2    0    0
     0    9    0]
 [   0    0   12    0    0    1    3    0    0    0   15 2174    3    2
     0    0    0]
 [   0    0    0    8   10   11    0    0    0    0    8    0  494    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    0    0    0    0    1    0    0    0
  1129    0    0]
 [   0    0    0    0    0    0   20    0    0    1    0    0    0    0
    58  268    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.98644986449864

F1 scores:
[       nan 0.975      0.98102981 0.95901639 0.97038724 0.94795127
 0.98130142 0.98039216 1.         0.5862069  0.96147211 0.98840646
 0.95458937 0.98143236 0.96993127 0.85623003 0.98245614]

Kappa:
0.965648262776973
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb18d9026d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.497, val_acc:0.401]
Epoch [2/120    avg_loss:2.039, val_acc:0.559]
Epoch [3/120    avg_loss:1.755, val_acc:0.608]
Epoch [4/120    avg_loss:1.558, val_acc:0.641]
Epoch [5/120    avg_loss:1.388, val_acc:0.674]
Epoch [6/120    avg_loss:1.127, val_acc:0.700]
Epoch [7/120    avg_loss:0.987, val_acc:0.717]
Epoch [8/120    avg_loss:0.835, val_acc:0.799]
Epoch [9/120    avg_loss:0.707, val_acc:0.794]
Epoch [10/120    avg_loss:0.629, val_acc:0.817]
Epoch [11/120    avg_loss:0.538, val_acc:0.843]
Epoch [12/120    avg_loss:0.504, val_acc:0.838]
Epoch [13/120    avg_loss:0.555, val_acc:0.845]
Epoch [14/120    avg_loss:0.433, val_acc:0.867]
Epoch [15/120    avg_loss:0.390, val_acc:0.860]
Epoch [16/120    avg_loss:0.376, val_acc:0.865]
Epoch [17/120    avg_loss:0.325, val_acc:0.874]
Epoch [18/120    avg_loss:0.308, val_acc:0.884]
Epoch [19/120    avg_loss:0.259, val_acc:0.884]
Epoch [20/120    avg_loss:0.239, val_acc:0.920]
Epoch [21/120    avg_loss:0.215, val_acc:0.890]
Epoch [22/120    avg_loss:0.230, val_acc:0.901]
Epoch [23/120    avg_loss:0.160, val_acc:0.926]
Epoch [24/120    avg_loss:0.166, val_acc:0.900]
Epoch [25/120    avg_loss:0.169, val_acc:0.926]
Epoch [26/120    avg_loss:0.159, val_acc:0.930]
Epoch [27/120    avg_loss:0.139, val_acc:0.931]
Epoch [28/120    avg_loss:0.147, val_acc:0.934]
Epoch [29/120    avg_loss:0.150, val_acc:0.936]
Epoch [30/120    avg_loss:0.124, val_acc:0.949]
Epoch [31/120    avg_loss:0.136, val_acc:0.939]
Epoch [32/120    avg_loss:0.094, val_acc:0.944]
Epoch [33/120    avg_loss:0.086, val_acc:0.955]
Epoch [34/120    avg_loss:0.080, val_acc:0.944]
Epoch [35/120    avg_loss:0.074, val_acc:0.954]
Epoch [36/120    avg_loss:0.074, val_acc:0.946]
Epoch [37/120    avg_loss:0.074, val_acc:0.948]
Epoch [38/120    avg_loss:0.084, val_acc:0.955]
Epoch [39/120    avg_loss:0.087, val_acc:0.955]
Epoch [40/120    avg_loss:0.086, val_acc:0.949]
Epoch [41/120    avg_loss:0.089, val_acc:0.944]
Epoch [42/120    avg_loss:0.069, val_acc:0.961]
Epoch [43/120    avg_loss:0.054, val_acc:0.954]
Epoch [44/120    avg_loss:0.054, val_acc:0.959]
Epoch [45/120    avg_loss:0.057, val_acc:0.954]
Epoch [46/120    avg_loss:0.059, val_acc:0.964]
Epoch [47/120    avg_loss:0.050, val_acc:0.964]
Epoch [48/120    avg_loss:0.039, val_acc:0.968]
Epoch [49/120    avg_loss:0.037, val_acc:0.970]
Epoch [50/120    avg_loss:0.045, val_acc:0.963]
Epoch [51/120    avg_loss:0.041, val_acc:0.964]
Epoch [52/120    avg_loss:0.041, val_acc:0.973]
Epoch [53/120    avg_loss:0.039, val_acc:0.965]
Epoch [54/120    avg_loss:0.042, val_acc:0.965]
Epoch [55/120    avg_loss:0.035, val_acc:0.971]
Epoch [56/120    avg_loss:0.031, val_acc:0.969]
Epoch [57/120    avg_loss:0.031, val_acc:0.975]
Epoch [58/120    avg_loss:0.036, val_acc:0.967]
Epoch [59/120    avg_loss:0.030, val_acc:0.965]
Epoch [60/120    avg_loss:0.048, val_acc:0.967]
Epoch [61/120    avg_loss:0.049, val_acc:0.962]
Epoch [62/120    avg_loss:0.051, val_acc:0.951]
Epoch [63/120    avg_loss:0.046, val_acc:0.971]
Epoch [64/120    avg_loss:0.028, val_acc:0.973]
Epoch [65/120    avg_loss:0.036, val_acc:0.969]
Epoch [66/120    avg_loss:0.029, val_acc:0.969]
Epoch [67/120    avg_loss:0.025, val_acc:0.967]
Epoch [68/120    avg_loss:0.023, val_acc:0.970]
Epoch [69/120    avg_loss:0.039, val_acc:0.970]
Epoch [70/120    avg_loss:0.035, val_acc:0.972]
Epoch [71/120    avg_loss:0.027, val_acc:0.974]
Epoch [72/120    avg_loss:0.020, val_acc:0.974]
Epoch [73/120    avg_loss:0.022, val_acc:0.974]
Epoch [74/120    avg_loss:0.017, val_acc:0.975]
Epoch [75/120    avg_loss:0.015, val_acc:0.975]
Epoch [76/120    avg_loss:0.016, val_acc:0.975]
Epoch [77/120    avg_loss:0.017, val_acc:0.975]
Epoch [78/120    avg_loss:0.015, val_acc:0.974]
Epoch [79/120    avg_loss:0.015, val_acc:0.977]
Epoch [80/120    avg_loss:0.015, val_acc:0.974]
Epoch [81/120    avg_loss:0.015, val_acc:0.975]
Epoch [82/120    avg_loss:0.015, val_acc:0.977]
Epoch [83/120    avg_loss:0.015, val_acc:0.977]
Epoch [84/120    avg_loss:0.013, val_acc:0.977]
Epoch [85/120    avg_loss:0.015, val_acc:0.975]
Epoch [86/120    avg_loss:0.016, val_acc:0.975]
Epoch [87/120    avg_loss:0.014, val_acc:0.975]
Epoch [88/120    avg_loss:0.012, val_acc:0.977]
Epoch [89/120    avg_loss:0.013, val_acc:0.978]
Epoch [90/120    avg_loss:0.012, val_acc:0.978]
Epoch [91/120    avg_loss:0.014, val_acc:0.977]
Epoch [92/120    avg_loss:0.012, val_acc:0.977]
Epoch [93/120    avg_loss:0.013, val_acc:0.977]
Epoch [94/120    avg_loss:0.014, val_acc:0.977]
Epoch [95/120    avg_loss:0.013, val_acc:0.977]
Epoch [96/120    avg_loss:0.014, val_acc:0.977]
Epoch [97/120    avg_loss:0.013, val_acc:0.975]
Epoch [98/120    avg_loss:0.009, val_acc:0.975]
Epoch [99/120    avg_loss:0.013, val_acc:0.977]
Epoch [100/120    avg_loss:0.012, val_acc:0.978]
Epoch [101/120    avg_loss:0.013, val_acc:0.977]
Epoch [102/120    avg_loss:0.011, val_acc:0.978]
Epoch [103/120    avg_loss:0.014, val_acc:0.980]
Epoch [104/120    avg_loss:0.014, val_acc:0.980]
Epoch [105/120    avg_loss:0.010, val_acc:0.981]
Epoch [106/120    avg_loss:0.015, val_acc:0.979]
Epoch [107/120    avg_loss:0.012, val_acc:0.979]
Epoch [108/120    avg_loss:0.015, val_acc:0.980]
Epoch [109/120    avg_loss:0.011, val_acc:0.978]
Epoch [110/120    avg_loss:0.013, val_acc:0.980]
Epoch [111/120    avg_loss:0.013, val_acc:0.980]
Epoch [112/120    avg_loss:0.012, val_acc:0.980]
Epoch [113/120    avg_loss:0.012, val_acc:0.979]
Epoch [114/120    avg_loss:0.012, val_acc:0.979]
Epoch [115/120    avg_loss:0.012, val_acc:0.980]
Epoch [116/120    avg_loss:0.011, val_acc:0.978]
Epoch [117/120    avg_loss:0.011, val_acc:0.980]
Epoch [118/120    avg_loss:0.013, val_acc:0.980]
Epoch [119/120    avg_loss:0.012, val_acc:0.980]
Epoch [120/120    avg_loss:0.012, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    1    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1273    2    0    0    0    0    0    0    3    7    0    0
     0    0    0]
 [   0    0    0  725    1   14    0    0    0    4    1    0    1    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   13    0    0    2    0
     0    0    0]
 [   0    0    4   45    0    7    0    0    0    0  793   19    0    0
     0    7    0]
 [   0    0   11    0    0    0    3    0    0    0   11 2182    1    2
     0    0    0]
 [   0    0    0   25    8   14    0    0    0    0    0    8  475    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   12    0    0    0    0    2    0    0    0
  1125    0    0]
 [   0    0    0    0    0    0   33    0    0    1    0    0    0    0
    47  266    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.6178861788618

F1 scores:
[       nan 0.96202532 0.98912199 0.937298   0.97931034 0.9441402
 0.97257228 1.         1.         0.65       0.94013041 0.98576914
 0.93688363 0.9919571  0.9736045  0.85806452 0.97076023]

Kappa:
0.9614240241988321
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc9a433d710>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.489, val_acc:0.502]
Epoch [2/120    avg_loss:1.957, val_acc:0.556]
Epoch [3/120    avg_loss:1.738, val_acc:0.636]
Epoch [4/120    avg_loss:1.465, val_acc:0.667]
Epoch [5/120    avg_loss:1.274, val_acc:0.684]
Epoch [6/120    avg_loss:1.065, val_acc:0.718]
Epoch [7/120    avg_loss:0.938, val_acc:0.732]
Epoch [8/120    avg_loss:0.790, val_acc:0.779]
Epoch [9/120    avg_loss:0.660, val_acc:0.783]
Epoch [10/120    avg_loss:0.609, val_acc:0.808]
Epoch [11/120    avg_loss:0.560, val_acc:0.829]
Epoch [12/120    avg_loss:0.457, val_acc:0.859]
Epoch [13/120    avg_loss:0.408, val_acc:0.853]
Epoch [14/120    avg_loss:0.359, val_acc:0.882]
Epoch [15/120    avg_loss:0.345, val_acc:0.833]
Epoch [16/120    avg_loss:0.422, val_acc:0.860]
Epoch [17/120    avg_loss:0.355, val_acc:0.844]
Epoch [18/120    avg_loss:0.273, val_acc:0.903]
Epoch [19/120    avg_loss:0.271, val_acc:0.878]
Epoch [20/120    avg_loss:0.294, val_acc:0.877]
Epoch [21/120    avg_loss:0.261, val_acc:0.856]
Epoch [22/120    avg_loss:0.214, val_acc:0.911]
Epoch [23/120    avg_loss:0.199, val_acc:0.907]
Epoch [24/120    avg_loss:0.174, val_acc:0.885]
Epoch [25/120    avg_loss:0.187, val_acc:0.898]
Epoch [26/120    avg_loss:0.188, val_acc:0.905]
Epoch [27/120    avg_loss:0.131, val_acc:0.910]
Epoch [28/120    avg_loss:0.151, val_acc:0.929]
Epoch [29/120    avg_loss:0.117, val_acc:0.904]
Epoch [30/120    avg_loss:0.117, val_acc:0.925]
Epoch [31/120    avg_loss:0.092, val_acc:0.932]
Epoch [32/120    avg_loss:0.131, val_acc:0.914]
Epoch [33/120    avg_loss:0.178, val_acc:0.912]
Epoch [34/120    avg_loss:0.158, val_acc:0.921]
Epoch [35/120    avg_loss:0.122, val_acc:0.923]
Epoch [36/120    avg_loss:0.139, val_acc:0.922]
Epoch [37/120    avg_loss:0.113, val_acc:0.925]
Epoch [38/120    avg_loss:0.100, val_acc:0.933]
Epoch [39/120    avg_loss:0.071, val_acc:0.935]
Epoch [40/120    avg_loss:0.076, val_acc:0.926]
Epoch [41/120    avg_loss:0.080, val_acc:0.924]
Epoch [42/120    avg_loss:0.065, val_acc:0.933]
Epoch [43/120    avg_loss:0.059, val_acc:0.933]
Epoch [44/120    avg_loss:0.063, val_acc:0.939]
Epoch [45/120    avg_loss:0.050, val_acc:0.938]
Epoch [46/120    avg_loss:0.049, val_acc:0.932]
Epoch [47/120    avg_loss:0.055, val_acc:0.944]
Epoch [48/120    avg_loss:0.050, val_acc:0.939]
Epoch [49/120    avg_loss:0.049, val_acc:0.951]
Epoch [50/120    avg_loss:0.050, val_acc:0.939]
Epoch [51/120    avg_loss:0.048, val_acc:0.943]
Epoch [52/120    avg_loss:0.054, val_acc:0.950]
Epoch [53/120    avg_loss:0.039, val_acc:0.953]
Epoch [54/120    avg_loss:0.038, val_acc:0.958]
Epoch [55/120    avg_loss:0.033, val_acc:0.944]
Epoch [56/120    avg_loss:0.029, val_acc:0.956]
Epoch [57/120    avg_loss:0.037, val_acc:0.950]
Epoch [58/120    avg_loss:0.031, val_acc:0.952]
Epoch [59/120    avg_loss:0.027, val_acc:0.951]
Epoch [60/120    avg_loss:0.036, val_acc:0.962]
Epoch [61/120    avg_loss:0.034, val_acc:0.945]
Epoch [62/120    avg_loss:0.026, val_acc:0.953]
Epoch [63/120    avg_loss:0.027, val_acc:0.958]
Epoch [64/120    avg_loss:0.032, val_acc:0.963]
Epoch [65/120    avg_loss:0.068, val_acc:0.942]
Epoch [66/120    avg_loss:0.087, val_acc:0.943]
Epoch [67/120    avg_loss:0.057, val_acc:0.950]
Epoch [68/120    avg_loss:0.046, val_acc:0.948]
Epoch [69/120    avg_loss:0.039, val_acc:0.953]
Epoch [70/120    avg_loss:0.035, val_acc:0.952]
Epoch [71/120    avg_loss:0.036, val_acc:0.952]
Epoch [72/120    avg_loss:0.025, val_acc:0.958]
Epoch [73/120    avg_loss:0.029, val_acc:0.956]
Epoch [74/120    avg_loss:0.035, val_acc:0.958]
Epoch [75/120    avg_loss:0.029, val_acc:0.958]
Epoch [76/120    avg_loss:0.021, val_acc:0.958]
Epoch [77/120    avg_loss:0.018, val_acc:0.963]
Epoch [78/120    avg_loss:0.015, val_acc:0.958]
Epoch [79/120    avg_loss:0.017, val_acc:0.960]
Epoch [80/120    avg_loss:0.017, val_acc:0.962]
Epoch [81/120    avg_loss:0.015, val_acc:0.967]
Epoch [82/120    avg_loss:0.026, val_acc:0.951]
Epoch [83/120    avg_loss:0.039, val_acc:0.959]
Epoch [84/120    avg_loss:0.025, val_acc:0.955]
Epoch [85/120    avg_loss:0.021, val_acc:0.963]
Epoch [86/120    avg_loss:0.017, val_acc:0.967]
Epoch [87/120    avg_loss:0.013, val_acc:0.967]
Epoch [88/120    avg_loss:0.018, val_acc:0.972]
Epoch [89/120    avg_loss:0.021, val_acc:0.967]
Epoch [90/120    avg_loss:0.019, val_acc:0.969]
Epoch [91/120    avg_loss:0.018, val_acc:0.967]
Epoch [92/120    avg_loss:0.012, val_acc:0.970]
Epoch [93/120    avg_loss:0.027, val_acc:0.955]
Epoch [94/120    avg_loss:0.022, val_acc:0.971]
Epoch [95/120    avg_loss:0.017, val_acc:0.973]
Epoch [96/120    avg_loss:0.021, val_acc:0.958]
Epoch [97/120    avg_loss:0.019, val_acc:0.969]
Epoch [98/120    avg_loss:0.009, val_acc:0.968]
Epoch [99/120    avg_loss:0.013, val_acc:0.970]
Epoch [100/120    avg_loss:0.022, val_acc:0.968]
Epoch [101/120    avg_loss:0.012, val_acc:0.975]
Epoch [102/120    avg_loss:0.013, val_acc:0.970]
Epoch [103/120    avg_loss:0.021, val_acc:0.961]
Epoch [104/120    avg_loss:0.017, val_acc:0.972]
Epoch [105/120    avg_loss:0.016, val_acc:0.968]
Epoch [106/120    avg_loss:0.011, val_acc:0.959]
Epoch [107/120    avg_loss:0.010, val_acc:0.971]
Epoch [108/120    avg_loss:0.012, val_acc:0.972]
Epoch [109/120    avg_loss:0.009, val_acc:0.972]
Epoch [110/120    avg_loss:0.014, val_acc:0.961]
Epoch [111/120    avg_loss:0.011, val_acc:0.973]
Epoch [112/120    avg_loss:0.010, val_acc:0.971]
Epoch [113/120    avg_loss:0.009, val_acc:0.972]
Epoch [114/120    avg_loss:0.009, val_acc:0.968]
Epoch [115/120    avg_loss:0.010, val_acc:0.971]
Epoch [116/120    avg_loss:0.009, val_acc:0.971]
Epoch [117/120    avg_loss:0.007, val_acc:0.972]
Epoch [118/120    avg_loss:0.004, val_acc:0.974]
Epoch [119/120    avg_loss:0.007, val_acc:0.972]
Epoch [120/120    avg_loss:0.009, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1258    5    0    0    1    0    0    1    9   10    1    0
     0    0    0]
 [   0    0    0  716    0   11    0    0    0    7    1    0    9    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   10   21    0    2    0    0    0    0  816   19    0    0
     3    4    0]
 [   0    0    5    0    0    0    2    0    0    0   12 2189    0    2
     0    0    0]
 [   0    0    0   18   20    7    0    0    0    0    9    0  475    0
     0    1    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    2    1    0    0
  1130    0    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    40  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.23577235772358

F1 scores:
[       nan 1.         0.98358092 0.95023225 0.95515695 0.9674523
 0.99544765 1.         0.99883856 0.72340426 0.94663573 0.98826185
 0.93046033 0.98666667 0.97750865 0.92846271 0.97076023]

Kappa:
0.9684801188688933
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd4e22416d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.513, val_acc:0.448]
Epoch [2/120    avg_loss:2.020, val_acc:0.492]
Epoch [3/120    avg_loss:1.755, val_acc:0.578]
Epoch [4/120    avg_loss:1.522, val_acc:0.645]
Epoch [5/120    avg_loss:1.314, val_acc:0.688]
Epoch [6/120    avg_loss:1.185, val_acc:0.737]
Epoch [7/120    avg_loss:0.993, val_acc:0.760]
Epoch [8/120    avg_loss:0.860, val_acc:0.759]
Epoch [9/120    avg_loss:0.701, val_acc:0.818]
Epoch [10/120    avg_loss:0.679, val_acc:0.819]
Epoch [11/120    avg_loss:0.631, val_acc:0.816]
Epoch [12/120    avg_loss:0.564, val_acc:0.827]
Epoch [13/120    avg_loss:0.502, val_acc:0.860]
Epoch [14/120    avg_loss:0.432, val_acc:0.831]
Epoch [15/120    avg_loss:0.439, val_acc:0.876]
Epoch [16/120    avg_loss:0.315, val_acc:0.885]
Epoch [17/120    avg_loss:0.290, val_acc:0.914]
Epoch [18/120    avg_loss:0.274, val_acc:0.912]
Epoch [19/120    avg_loss:0.279, val_acc:0.911]
Epoch [20/120    avg_loss:0.267, val_acc:0.911]
Epoch [21/120    avg_loss:0.214, val_acc:0.916]
Epoch [22/120    avg_loss:0.196, val_acc:0.902]
Epoch [23/120    avg_loss:0.187, val_acc:0.933]
Epoch [24/120    avg_loss:0.179, val_acc:0.926]
Epoch [25/120    avg_loss:0.204, val_acc:0.933]
Epoch [26/120    avg_loss:0.167, val_acc:0.927]
Epoch [27/120    avg_loss:0.138, val_acc:0.929]
Epoch [28/120    avg_loss:0.125, val_acc:0.946]
Epoch [29/120    avg_loss:0.118, val_acc:0.926]
Epoch [30/120    avg_loss:0.116, val_acc:0.940]
Epoch [31/120    avg_loss:0.107, val_acc:0.949]
Epoch [32/120    avg_loss:0.124, val_acc:0.934]
Epoch [33/120    avg_loss:0.125, val_acc:0.907]
Epoch [34/120    avg_loss:0.103, val_acc:0.944]
Epoch [35/120    avg_loss:0.087, val_acc:0.941]
Epoch [36/120    avg_loss:0.078, val_acc:0.958]
Epoch [37/120    avg_loss:0.069, val_acc:0.956]
Epoch [38/120    avg_loss:0.073, val_acc:0.946]
Epoch [39/120    avg_loss:0.095, val_acc:0.933]
Epoch [40/120    avg_loss:0.104, val_acc:0.941]
Epoch [41/120    avg_loss:0.099, val_acc:0.964]
Epoch [42/120    avg_loss:0.075, val_acc:0.951]
Epoch [43/120    avg_loss:0.091, val_acc:0.952]
Epoch [44/120    avg_loss:0.085, val_acc:0.920]
Epoch [45/120    avg_loss:0.075, val_acc:0.945]
Epoch [46/120    avg_loss:0.065, val_acc:0.955]
Epoch [47/120    avg_loss:0.097, val_acc:0.949]
Epoch [48/120    avg_loss:0.103, val_acc:0.959]
Epoch [49/120    avg_loss:0.068, val_acc:0.961]
Epoch [50/120    avg_loss:0.057, val_acc:0.953]
Epoch [51/120    avg_loss:0.040, val_acc:0.968]
Epoch [52/120    avg_loss:0.036, val_acc:0.960]
Epoch [53/120    avg_loss:0.037, val_acc:0.972]
Epoch [54/120    avg_loss:0.029, val_acc:0.975]
Epoch [55/120    avg_loss:0.024, val_acc:0.973]
Epoch [56/120    avg_loss:0.035, val_acc:0.965]
Epoch [57/120    avg_loss:0.047, val_acc:0.965]
Epoch [58/120    avg_loss:0.040, val_acc:0.973]
Epoch [59/120    avg_loss:0.024, val_acc:0.977]
Epoch [60/120    avg_loss:0.025, val_acc:0.975]
Epoch [61/120    avg_loss:0.024, val_acc:0.974]
Epoch [62/120    avg_loss:0.033, val_acc:0.965]
Epoch [63/120    avg_loss:0.044, val_acc:0.974]
Epoch [64/120    avg_loss:0.038, val_acc:0.968]
Epoch [65/120    avg_loss:0.033, val_acc:0.969]
Epoch [66/120    avg_loss:0.021, val_acc:0.978]
Epoch [67/120    avg_loss:0.024, val_acc:0.978]
Epoch [68/120    avg_loss:0.022, val_acc:0.979]
Epoch [69/120    avg_loss:0.023, val_acc:0.975]
Epoch [70/120    avg_loss:0.024, val_acc:0.979]
Epoch [71/120    avg_loss:0.021, val_acc:0.969]
Epoch [72/120    avg_loss:0.017, val_acc:0.979]
Epoch [73/120    avg_loss:0.016, val_acc:0.981]
Epoch [74/120    avg_loss:0.019, val_acc:0.985]
Epoch [75/120    avg_loss:0.023, val_acc:0.982]
Epoch [76/120    avg_loss:0.016, val_acc:0.987]
Epoch [77/120    avg_loss:0.026, val_acc:0.977]
Epoch [78/120    avg_loss:0.022, val_acc:0.977]
Epoch [79/120    avg_loss:0.039, val_acc:0.974]
Epoch [80/120    avg_loss:0.048, val_acc:0.967]
Epoch [81/120    avg_loss:0.027, val_acc:0.979]
Epoch [82/120    avg_loss:0.020, val_acc:0.980]
Epoch [83/120    avg_loss:0.027, val_acc:0.973]
Epoch [84/120    avg_loss:0.019, val_acc:0.983]
Epoch [85/120    avg_loss:0.016, val_acc:0.982]
Epoch [86/120    avg_loss:0.013, val_acc:0.988]
Epoch [87/120    avg_loss:0.018, val_acc:0.975]
Epoch [88/120    avg_loss:0.020, val_acc:0.987]
Epoch [89/120    avg_loss:0.021, val_acc:0.978]
Epoch [90/120    avg_loss:0.011, val_acc:0.983]
Epoch [91/120    avg_loss:0.013, val_acc:0.985]
Epoch [92/120    avg_loss:0.010, val_acc:0.982]
Epoch [93/120    avg_loss:0.012, val_acc:0.972]
Epoch [94/120    avg_loss:0.014, val_acc:0.967]
Epoch [95/120    avg_loss:0.013, val_acc:0.982]
Epoch [96/120    avg_loss:0.011, val_acc:0.983]
Epoch [97/120    avg_loss:0.009, val_acc:0.982]
Epoch [98/120    avg_loss:0.014, val_acc:0.979]
Epoch [99/120    avg_loss:0.010, val_acc:0.983]
Epoch [100/120    avg_loss:0.012, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.009, val_acc:0.985]
Epoch [103/120    avg_loss:0.007, val_acc:0.985]
Epoch [104/120    avg_loss:0.008, val_acc:0.985]
Epoch [105/120    avg_loss:0.007, val_acc:0.985]
Epoch [106/120    avg_loss:0.008, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.008, val_acc:0.988]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.006, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.987]
Epoch [113/120    avg_loss:0.006, val_acc:0.987]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.008, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.007, val_acc:0.987]
Epoch [118/120    avg_loss:0.007, val_acc:0.987]
Epoch [119/120    avg_loss:0.006, val_acc:0.987]
Epoch [120/120    avg_loss:0.007, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1257    0    0    0    3    0    0    0    7   15    3    0
     0    0    0]
 [   0    0    1  718    0   12    0    0    0    7    1    0    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    7   38    0    6    0    0    0    0  814    2    0    0
     1    7    0]
 [   0    0    3    0    0    0    1    0    0    0   16 2188    1    1
     0    0    0]
 [   0    0    0   11   15    7    0    0    0    0    5    3  488    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1133    2    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    59  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.0840108401084

F1 scores:
[       nan 0.98765432 0.98433829 0.94722955 0.96598639 0.9674523
 0.98867925 1.         0.99883856 0.71111111 0.94596165 0.99004525
 0.94390716 0.99730458 0.97169811 0.87874016 0.97109827]

Kappa:
0.96674879980992
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc405aa6710>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.554, val_acc:0.352]
Epoch [2/120    avg_loss:2.075, val_acc:0.521]
Epoch [3/120    avg_loss:1.789, val_acc:0.608]
Epoch [4/120    avg_loss:1.578, val_acc:0.644]
Epoch [5/120    avg_loss:1.352, val_acc:0.686]
Epoch [6/120    avg_loss:1.271, val_acc:0.691]
Epoch [7/120    avg_loss:1.059, val_acc:0.741]
Epoch [8/120    avg_loss:0.922, val_acc:0.768]
Epoch [9/120    avg_loss:0.745, val_acc:0.773]
Epoch [10/120    avg_loss:0.647, val_acc:0.796]
Epoch [11/120    avg_loss:0.603, val_acc:0.818]
Epoch [12/120    avg_loss:0.549, val_acc:0.829]
Epoch [13/120    avg_loss:0.505, val_acc:0.820]
Epoch [14/120    avg_loss:0.509, val_acc:0.849]
Epoch [15/120    avg_loss:0.439, val_acc:0.838]
Epoch [16/120    avg_loss:0.363, val_acc:0.839]
Epoch [17/120    avg_loss:0.312, val_acc:0.863]
Epoch [18/120    avg_loss:0.273, val_acc:0.873]
Epoch [19/120    avg_loss:0.271, val_acc:0.871]
Epoch [20/120    avg_loss:0.239, val_acc:0.907]
Epoch [21/120    avg_loss:0.207, val_acc:0.900]
Epoch [22/120    avg_loss:0.201, val_acc:0.925]
Epoch [23/120    avg_loss:0.151, val_acc:0.908]
Epoch [24/120    avg_loss:0.155, val_acc:0.912]
Epoch [25/120    avg_loss:0.178, val_acc:0.939]
Epoch [26/120    avg_loss:0.181, val_acc:0.917]
Epoch [27/120    avg_loss:0.137, val_acc:0.932]
Epoch [28/120    avg_loss:0.134, val_acc:0.925]
Epoch [29/120    avg_loss:0.105, val_acc:0.939]
Epoch [30/120    avg_loss:0.101, val_acc:0.950]
Epoch [31/120    avg_loss:0.141, val_acc:0.924]
Epoch [32/120    avg_loss:0.176, val_acc:0.927]
Epoch [33/120    avg_loss:0.125, val_acc:0.916]
Epoch [34/120    avg_loss:0.124, val_acc:0.931]
Epoch [35/120    avg_loss:0.096, val_acc:0.942]
Epoch [36/120    avg_loss:0.090, val_acc:0.959]
Epoch [37/120    avg_loss:0.075, val_acc:0.932]
Epoch [38/120    avg_loss:0.092, val_acc:0.935]
Epoch [39/120    avg_loss:0.066, val_acc:0.958]
Epoch [40/120    avg_loss:0.054, val_acc:0.959]
Epoch [41/120    avg_loss:0.054, val_acc:0.962]
Epoch [42/120    avg_loss:0.041, val_acc:0.952]
Epoch [43/120    avg_loss:0.049, val_acc:0.959]
Epoch [44/120    avg_loss:0.078, val_acc:0.949]
Epoch [45/120    avg_loss:0.069, val_acc:0.930]
Epoch [46/120    avg_loss:0.101, val_acc:0.958]
Epoch [47/120    avg_loss:0.068, val_acc:0.932]
Epoch [48/120    avg_loss:0.052, val_acc:0.956]
Epoch [49/120    avg_loss:0.054, val_acc:0.958]
Epoch [50/120    avg_loss:0.054, val_acc:0.953]
Epoch [51/120    avg_loss:0.037, val_acc:0.971]
Epoch [52/120    avg_loss:0.041, val_acc:0.953]
Epoch [53/120    avg_loss:0.037, val_acc:0.963]
Epoch [54/120    avg_loss:0.039, val_acc:0.959]
Epoch [55/120    avg_loss:0.045, val_acc:0.958]
Epoch [56/120    avg_loss:0.028, val_acc:0.969]
Epoch [57/120    avg_loss:0.029, val_acc:0.958]
Epoch [58/120    avg_loss:0.024, val_acc:0.972]
Epoch [59/120    avg_loss:0.025, val_acc:0.971]
Epoch [60/120    avg_loss:0.033, val_acc:0.964]
Epoch [61/120    avg_loss:0.044, val_acc:0.959]
Epoch [62/120    avg_loss:0.048, val_acc:0.967]
Epoch [63/120    avg_loss:0.038, val_acc:0.955]
Epoch [64/120    avg_loss:0.040, val_acc:0.959]
Epoch [65/120    avg_loss:0.027, val_acc:0.969]
Epoch [66/120    avg_loss:0.024, val_acc:0.970]
Epoch [67/120    avg_loss:0.036, val_acc:0.960]
Epoch [68/120    avg_loss:0.028, val_acc:0.968]
Epoch [69/120    avg_loss:0.022, val_acc:0.964]
Epoch [70/120    avg_loss:0.023, val_acc:0.962]
Epoch [71/120    avg_loss:0.030, val_acc:0.963]
Epoch [72/120    avg_loss:0.043, val_acc:0.968]
Epoch [73/120    avg_loss:0.041, val_acc:0.967]
Epoch [74/120    avg_loss:0.018, val_acc:0.971]
Epoch [75/120    avg_loss:0.023, val_acc:0.969]
Epoch [76/120    avg_loss:0.019, val_acc:0.971]
Epoch [77/120    avg_loss:0.020, val_acc:0.971]
Epoch [78/120    avg_loss:0.015, val_acc:0.974]
Epoch [79/120    avg_loss:0.015, val_acc:0.973]
Epoch [80/120    avg_loss:0.016, val_acc:0.973]
Epoch [81/120    avg_loss:0.019, val_acc:0.972]
Epoch [82/120    avg_loss:0.016, val_acc:0.973]
Epoch [83/120    avg_loss:0.017, val_acc:0.974]
Epoch [84/120    avg_loss:0.018, val_acc:0.974]
Epoch [85/120    avg_loss:0.014, val_acc:0.974]
Epoch [86/120    avg_loss:0.014, val_acc:0.973]
Epoch [87/120    avg_loss:0.014, val_acc:0.974]
Epoch [88/120    avg_loss:0.015, val_acc:0.975]
Epoch [89/120    avg_loss:0.012, val_acc:0.977]
Epoch [90/120    avg_loss:0.015, val_acc:0.973]
Epoch [91/120    avg_loss:0.014, val_acc:0.973]
Epoch [92/120    avg_loss:0.016, val_acc:0.977]
Epoch [93/120    avg_loss:0.013, val_acc:0.974]
Epoch [94/120    avg_loss:0.014, val_acc:0.975]
Epoch [95/120    avg_loss:0.013, val_acc:0.975]
Epoch [96/120    avg_loss:0.012, val_acc:0.974]
Epoch [97/120    avg_loss:0.013, val_acc:0.974]
Epoch [98/120    avg_loss:0.013, val_acc:0.977]
Epoch [99/120    avg_loss:0.013, val_acc:0.978]
Epoch [100/120    avg_loss:0.013, val_acc:0.978]
Epoch [101/120    avg_loss:0.015, val_acc:0.978]
Epoch [102/120    avg_loss:0.011, val_acc:0.978]
Epoch [103/120    avg_loss:0.011, val_acc:0.979]
Epoch [104/120    avg_loss:0.010, val_acc:0.978]
Epoch [105/120    avg_loss:0.010, val_acc:0.977]
Epoch [106/120    avg_loss:0.011, val_acc:0.978]
Epoch [107/120    avg_loss:0.012, val_acc:0.980]
Epoch [108/120    avg_loss:0.010, val_acc:0.981]
Epoch [109/120    avg_loss:0.011, val_acc:0.981]
Epoch [110/120    avg_loss:0.013, val_acc:0.980]
Epoch [111/120    avg_loss:0.010, val_acc:0.980]
Epoch [112/120    avg_loss:0.013, val_acc:0.975]
Epoch [113/120    avg_loss:0.010, val_acc:0.978]
Epoch [114/120    avg_loss:0.011, val_acc:0.978]
Epoch [115/120    avg_loss:0.011, val_acc:0.979]
Epoch [116/120    avg_loss:0.013, val_acc:0.977]
Epoch [117/120    avg_loss:0.011, val_acc:0.975]
Epoch [118/120    avg_loss:0.012, val_acc:0.977]
Epoch [119/120    avg_loss:0.009, val_acc:0.979]
Epoch [120/120    avg_loss:0.010, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1259    4    0    0    1    0    0    0    7   14    0    0
     0    0    0]
 [   0    0    0  715    1   18    0    0    0   12    0    0    0    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    5    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   14   18    0    3    0    0    0    0  824    8    0    0
     0    8    0]
 [   0    0    8    0    0    0    5    0    0    0   11 2184    0    2
     0    0    0]
 [   0    0    0   13    5    6    0    0    0    0   11   12  484    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    1    0    0
  1135    0    0]
 [   0    0    0    0    0    0   40    0    0    1    0    0    0    0
    69  237    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.65040650406505

F1 scores:
[       nan 0.975      0.98129384 0.95524382 0.98611111 0.96412556
 0.96541575 1.         0.99767442 0.66666667 0.95150115 0.98600451
 0.94901961 0.9919571  0.96843003 0.80067568 0.98224852]

Kappa:
0.9617814388617053
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ffac174f748>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.516, val_acc:0.470]
Epoch [2/120    avg_loss:2.089, val_acc:0.577]
Epoch [3/120    avg_loss:1.748, val_acc:0.594]
Epoch [4/120    avg_loss:1.571, val_acc:0.615]
Epoch [5/120    avg_loss:1.455, val_acc:0.631]
Epoch [6/120    avg_loss:1.223, val_acc:0.692]
Epoch [7/120    avg_loss:1.059, val_acc:0.728]
Epoch [8/120    avg_loss:0.952, val_acc:0.724]
Epoch [9/120    avg_loss:0.800, val_acc:0.771]
Epoch [10/120    avg_loss:0.745, val_acc:0.778]
Epoch [11/120    avg_loss:0.702, val_acc:0.794]
Epoch [12/120    avg_loss:0.601, val_acc:0.843]
Epoch [13/120    avg_loss:0.575, val_acc:0.806]
Epoch [14/120    avg_loss:0.473, val_acc:0.877]
Epoch [15/120    avg_loss:0.457, val_acc:0.831]
Epoch [16/120    avg_loss:0.408, val_acc:0.884]
Epoch [17/120    avg_loss:0.321, val_acc:0.879]
Epoch [18/120    avg_loss:0.301, val_acc:0.890]
Epoch [19/120    avg_loss:0.358, val_acc:0.896]
Epoch [20/120    avg_loss:0.296, val_acc:0.896]
Epoch [21/120    avg_loss:0.265, val_acc:0.887]
Epoch [22/120    avg_loss:0.311, val_acc:0.896]
Epoch [23/120    avg_loss:0.235, val_acc:0.923]
Epoch [24/120    avg_loss:0.190, val_acc:0.916]
Epoch [25/120    avg_loss:0.202, val_acc:0.903]
Epoch [26/120    avg_loss:0.152, val_acc:0.925]
Epoch [27/120    avg_loss:0.163, val_acc:0.921]
Epoch [28/120    avg_loss:0.166, val_acc:0.925]
Epoch [29/120    avg_loss:0.179, val_acc:0.935]
Epoch [30/120    avg_loss:0.136, val_acc:0.945]
Epoch [31/120    avg_loss:0.130, val_acc:0.929]
Epoch [32/120    avg_loss:0.129, val_acc:0.933]
Epoch [33/120    avg_loss:0.128, val_acc:0.936]
Epoch [34/120    avg_loss:0.146, val_acc:0.920]
Epoch [35/120    avg_loss:0.179, val_acc:0.930]
Epoch [36/120    avg_loss:0.225, val_acc:0.891]
Epoch [37/120    avg_loss:0.153, val_acc:0.923]
Epoch [38/120    avg_loss:0.104, val_acc:0.952]
Epoch [39/120    avg_loss:0.076, val_acc:0.949]
Epoch [40/120    avg_loss:0.086, val_acc:0.940]
Epoch [41/120    avg_loss:0.093, val_acc:0.948]
Epoch [42/120    avg_loss:0.073, val_acc:0.953]
Epoch [43/120    avg_loss:0.061, val_acc:0.954]
Epoch [44/120    avg_loss:0.070, val_acc:0.962]
Epoch [45/120    avg_loss:0.074, val_acc:0.951]
Epoch [46/120    avg_loss:0.060, val_acc:0.949]
Epoch [47/120    avg_loss:0.045, val_acc:0.962]
Epoch [48/120    avg_loss:0.046, val_acc:0.949]
Epoch [49/120    avg_loss:0.047, val_acc:0.954]
Epoch [50/120    avg_loss:0.072, val_acc:0.964]
Epoch [51/120    avg_loss:0.050, val_acc:0.959]
Epoch [52/120    avg_loss:0.039, val_acc:0.960]
Epoch [53/120    avg_loss:0.039, val_acc:0.969]
Epoch [54/120    avg_loss:0.037, val_acc:0.964]
Epoch [55/120    avg_loss:0.055, val_acc:0.958]
Epoch [56/120    avg_loss:0.057, val_acc:0.959]
Epoch [57/120    avg_loss:0.049, val_acc:0.970]
Epoch [58/120    avg_loss:0.039, val_acc:0.962]
Epoch [59/120    avg_loss:0.036, val_acc:0.964]
Epoch [60/120    avg_loss:0.029, val_acc:0.973]
Epoch [61/120    avg_loss:0.031, val_acc:0.972]
Epoch [62/120    avg_loss:0.053, val_acc:0.956]
Epoch [63/120    avg_loss:0.065, val_acc:0.967]
Epoch [64/120    avg_loss:0.053, val_acc:0.965]
Epoch [65/120    avg_loss:0.031, val_acc:0.975]
Epoch [66/120    avg_loss:0.037, val_acc:0.968]
Epoch [67/120    avg_loss:0.038, val_acc:0.972]
Epoch [68/120    avg_loss:0.036, val_acc:0.965]
Epoch [69/120    avg_loss:0.039, val_acc:0.975]
Epoch [70/120    avg_loss:0.032, val_acc:0.974]
Epoch [71/120    avg_loss:0.025, val_acc:0.977]
Epoch [72/120    avg_loss:0.030, val_acc:0.970]
Epoch [73/120    avg_loss:0.019, val_acc:0.979]
Epoch [74/120    avg_loss:0.023, val_acc:0.973]
Epoch [75/120    avg_loss:0.017, val_acc:0.980]
Epoch [76/120    avg_loss:0.016, val_acc:0.980]
Epoch [77/120    avg_loss:0.020, val_acc:0.975]
Epoch [78/120    avg_loss:0.015, val_acc:0.979]
Epoch [79/120    avg_loss:0.025, val_acc:0.951]
Epoch [80/120    avg_loss:0.029, val_acc:0.970]
Epoch [81/120    avg_loss:0.020, val_acc:0.981]
Epoch [82/120    avg_loss:0.021, val_acc:0.967]
Epoch [83/120    avg_loss:0.044, val_acc:0.975]
Epoch [84/120    avg_loss:0.062, val_acc:0.971]
Epoch [85/120    avg_loss:0.033, val_acc:0.962]
Epoch [86/120    avg_loss:0.061, val_acc:0.954]
Epoch [87/120    avg_loss:0.046, val_acc:0.971]
Epoch [88/120    avg_loss:0.034, val_acc:0.974]
Epoch [89/120    avg_loss:0.021, val_acc:0.978]
Epoch [90/120    avg_loss:0.018, val_acc:0.970]
Epoch [91/120    avg_loss:0.021, val_acc:0.975]
Epoch [92/120    avg_loss:0.018, val_acc:0.981]
Epoch [93/120    avg_loss:0.016, val_acc:0.974]
Epoch [94/120    avg_loss:0.011, val_acc:0.978]
Epoch [95/120    avg_loss:0.013, val_acc:0.978]
Epoch [96/120    avg_loss:0.014, val_acc:0.977]
Epoch [97/120    avg_loss:0.012, val_acc:0.979]
Epoch [98/120    avg_loss:0.015, val_acc:0.973]
Epoch [99/120    avg_loss:0.021, val_acc:0.978]
Epoch [100/120    avg_loss:0.014, val_acc:0.977]
Epoch [101/120    avg_loss:0.012, val_acc:0.979]
Epoch [102/120    avg_loss:0.019, val_acc:0.979]
Epoch [103/120    avg_loss:0.014, val_acc:0.979]
Epoch [104/120    avg_loss:0.009, val_acc:0.979]
Epoch [105/120    avg_loss:0.009, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.978]
Epoch [107/120    avg_loss:0.008, val_acc:0.983]
Epoch [108/120    avg_loss:0.007, val_acc:0.980]
Epoch [109/120    avg_loss:0.010, val_acc:0.977]
Epoch [110/120    avg_loss:0.009, val_acc:0.977]
Epoch [111/120    avg_loss:0.011, val_acc:0.979]
Epoch [112/120    avg_loss:0.013, val_acc:0.968]
Epoch [113/120    avg_loss:0.013, val_acc:0.981]
Epoch [114/120    avg_loss:0.007, val_acc:0.982]
Epoch [115/120    avg_loss:0.015, val_acc:0.977]
Epoch [116/120    avg_loss:0.015, val_acc:0.971]
Epoch [117/120    avg_loss:0.020, val_acc:0.977]
Epoch [118/120    avg_loss:0.014, val_acc:0.980]
Epoch [119/120    avg_loss:0.014, val_acc:0.981]
Epoch [120/120    avg_loss:0.014, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    3    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    6    0    0    0    0    0    0    7    5    0    0
     0    0    0]
 [   0    0    1  710    0   14    0    0    0   14    1    0    3    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9    8    0    5    9    0    0    0  836    5    0    0
     2    1    0]
 [   0    0   11    0    0    0   10    0    0    0   16 2169    2    2
     0    0    0]
 [   0    0    0   18   15   11    0    0    0    0    5    0  485    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   14    0    0    1    0    2    0    0    0
  1122    0    0]
 [   0    0    0    0    0    0   36    0    0    0    0    0    0    0
    59  252    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.70460704607046

F1 scores:
[       nan 0.96202532 0.98369565 0.95366017 0.96598639 0.94609461
 0.95982469 0.98039216 0.99883856 0.66666667 0.9598163  0.98838004
 0.94726562 0.98404255 0.96640827 0.84       1.        ]

Kappa:
0.9624393265340709
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcdfc505710>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.506, val_acc:0.441]
Epoch [2/120    avg_loss:1.977, val_acc:0.593]
Epoch [3/120    avg_loss:1.837, val_acc:0.624]
Epoch [4/120    avg_loss:1.570, val_acc:0.616]
Epoch [5/120    avg_loss:1.338, val_acc:0.689]
Epoch [6/120    avg_loss:1.204, val_acc:0.692]
Epoch [7/120    avg_loss:0.978, val_acc:0.746]
Epoch [8/120    avg_loss:0.859, val_acc:0.801]
Epoch [9/120    avg_loss:0.740, val_acc:0.792]
Epoch [10/120    avg_loss:0.711, val_acc:0.768]
Epoch [11/120    avg_loss:0.636, val_acc:0.837]
Epoch [12/120    avg_loss:0.584, val_acc:0.831]
Epoch [13/120    avg_loss:0.485, val_acc:0.842]
Epoch [14/120    avg_loss:0.475, val_acc:0.840]
Epoch [15/120    avg_loss:0.442, val_acc:0.858]
Epoch [16/120    avg_loss:0.392, val_acc:0.864]
Epoch [17/120    avg_loss:0.352, val_acc:0.885]
Epoch [18/120    avg_loss:0.319, val_acc:0.894]
Epoch [19/120    avg_loss:0.242, val_acc:0.911]
Epoch [20/120    avg_loss:0.242, val_acc:0.888]
Epoch [21/120    avg_loss:0.317, val_acc:0.893]
Epoch [22/120    avg_loss:0.311, val_acc:0.923]
Epoch [23/120    avg_loss:0.251, val_acc:0.926]
Epoch [24/120    avg_loss:0.241, val_acc:0.908]
Epoch [25/120    avg_loss:0.225, val_acc:0.911]
Epoch [26/120    avg_loss:0.279, val_acc:0.901]
Epoch [27/120    avg_loss:0.252, val_acc:0.913]
Epoch [28/120    avg_loss:0.222, val_acc:0.934]
Epoch [29/120    avg_loss:0.203, val_acc:0.927]
Epoch [30/120    avg_loss:0.212, val_acc:0.906]
Epoch [31/120    avg_loss:0.193, val_acc:0.924]
Epoch [32/120    avg_loss:0.177, val_acc:0.940]
Epoch [33/120    avg_loss:0.178, val_acc:0.950]
Epoch [34/120    avg_loss:0.121, val_acc:0.951]
Epoch [35/120    avg_loss:0.125, val_acc:0.942]
Epoch [36/120    avg_loss:0.176, val_acc:0.933]
Epoch [37/120    avg_loss:0.132, val_acc:0.946]
Epoch [38/120    avg_loss:0.107, val_acc:0.962]
Epoch [39/120    avg_loss:0.125, val_acc:0.935]
Epoch [40/120    avg_loss:0.085, val_acc:0.953]
Epoch [41/120    avg_loss:0.095, val_acc:0.953]
Epoch [42/120    avg_loss:0.094, val_acc:0.959]
Epoch [43/120    avg_loss:0.070, val_acc:0.967]
Epoch [44/120    avg_loss:0.086, val_acc:0.956]
Epoch [45/120    avg_loss:0.070, val_acc:0.949]
Epoch [46/120    avg_loss:0.071, val_acc:0.963]
Epoch [47/120    avg_loss:0.074, val_acc:0.958]
Epoch [48/120    avg_loss:0.068, val_acc:0.959]
Epoch [49/120    avg_loss:0.060, val_acc:0.959]
Epoch [50/120    avg_loss:0.066, val_acc:0.953]
Epoch [51/120    avg_loss:0.069, val_acc:0.963]
Epoch [52/120    avg_loss:0.062, val_acc:0.952]
Epoch [53/120    avg_loss:0.106, val_acc:0.944]
Epoch [54/120    avg_loss:0.078, val_acc:0.960]
Epoch [55/120    avg_loss:0.076, val_acc:0.963]
Epoch [56/120    avg_loss:0.065, val_acc:0.967]
Epoch [57/120    avg_loss:0.059, val_acc:0.971]
Epoch [58/120    avg_loss:0.050, val_acc:0.963]
Epoch [59/120    avg_loss:0.058, val_acc:0.971]
Epoch [60/120    avg_loss:0.058, val_acc:0.960]
Epoch [61/120    avg_loss:0.076, val_acc:0.955]
Epoch [62/120    avg_loss:0.070, val_acc:0.964]
Epoch [63/120    avg_loss:0.058, val_acc:0.969]
Epoch [64/120    avg_loss:0.054, val_acc:0.963]
Epoch [65/120    avg_loss:0.043, val_acc:0.978]
Epoch [66/120    avg_loss:0.048, val_acc:0.980]
Epoch [67/120    avg_loss:0.060, val_acc:0.962]
Epoch [68/120    avg_loss:0.047, val_acc:0.974]
Epoch [69/120    avg_loss:0.048, val_acc:0.975]
Epoch [70/120    avg_loss:0.059, val_acc:0.960]
Epoch [71/120    avg_loss:0.051, val_acc:0.958]
Epoch [72/120    avg_loss:0.040, val_acc:0.978]
Epoch [73/120    avg_loss:0.058, val_acc:0.973]
Epoch [74/120    avg_loss:0.070, val_acc:0.971]
Epoch [75/120    avg_loss:0.038, val_acc:0.961]
Epoch [76/120    avg_loss:0.042, val_acc:0.975]
Epoch [77/120    avg_loss:0.033, val_acc:0.978]
Epoch [78/120    avg_loss:0.032, val_acc:0.977]
Epoch [79/120    avg_loss:0.035, val_acc:0.971]
Epoch [80/120    avg_loss:0.029, val_acc:0.981]
Epoch [81/120    avg_loss:0.021, val_acc:0.984]
Epoch [82/120    avg_loss:0.019, val_acc:0.987]
Epoch [83/120    avg_loss:0.020, val_acc:0.988]
Epoch [84/120    avg_loss:0.020, val_acc:0.988]
Epoch [85/120    avg_loss:0.019, val_acc:0.988]
Epoch [86/120    avg_loss:0.020, val_acc:0.988]
Epoch [87/120    avg_loss:0.019, val_acc:0.985]
Epoch [88/120    avg_loss:0.017, val_acc:0.988]
Epoch [89/120    avg_loss:0.020, val_acc:0.987]
Epoch [90/120    avg_loss:0.025, val_acc:0.983]
Epoch [91/120    avg_loss:0.021, val_acc:0.987]
Epoch [92/120    avg_loss:0.017, val_acc:0.987]
Epoch [93/120    avg_loss:0.017, val_acc:0.984]
Epoch [94/120    avg_loss:0.020, val_acc:0.987]
Epoch [95/120    avg_loss:0.021, val_acc:0.987]
Epoch [96/120    avg_loss:0.015, val_acc:0.987]
Epoch [97/120    avg_loss:0.015, val_acc:0.988]
Epoch [98/120    avg_loss:0.016, val_acc:0.987]
Epoch [99/120    avg_loss:0.021, val_acc:0.989]
Epoch [100/120    avg_loss:0.019, val_acc:0.987]
Epoch [101/120    avg_loss:0.016, val_acc:0.987]
Epoch [102/120    avg_loss:0.018, val_acc:0.988]
Epoch [103/120    avg_loss:0.016, val_acc:0.985]
Epoch [104/120    avg_loss:0.020, val_acc:0.984]
Epoch [105/120    avg_loss:0.016, val_acc:0.987]
Epoch [106/120    avg_loss:0.017, val_acc:0.987]
Epoch [107/120    avg_loss:0.015, val_acc:0.987]
Epoch [108/120    avg_loss:0.019, val_acc:0.988]
Epoch [109/120    avg_loss:0.014, val_acc:0.988]
Epoch [110/120    avg_loss:0.014, val_acc:0.987]
Epoch [111/120    avg_loss:0.015, val_acc:0.985]
Epoch [112/120    avg_loss:0.021, val_acc:0.987]
Epoch [113/120    avg_loss:0.015, val_acc:0.987]
Epoch [114/120    avg_loss:0.020, val_acc:0.987]
Epoch [115/120    avg_loss:0.017, val_acc:0.987]
Epoch [116/120    avg_loss:0.014, val_acc:0.988]
Epoch [117/120    avg_loss:0.015, val_acc:0.987]
Epoch [118/120    avg_loss:0.014, val_acc:0.987]
Epoch [119/120    avg_loss:0.018, val_acc:0.987]
Epoch [120/120    avg_loss:0.014, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1256    0    0    0    2    0    0    0   13    8    6    0
     0    0    0]
 [   0    0    1  712    0   19    0    0    0    7    0    0    5    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    4    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   13   88    0    9    2    0    0    0  757    0    0    0
     3    3    0]
 [   0    0    9    0    0    5   10    0    3    0   10 2116   55    2
     0    0    0]
 [   0    0    0   16    4   12    0    0    0    0    6    5  486    0
     1    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    1    0    0
  1136    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   127  220    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.97018970189701

F1 scores:
[       nan 0.98765432 0.97933723 0.91106846 0.99069767 0.94273128
 0.98869631 0.98039216 0.99537037 0.73913043 0.91095066 0.97489058
 0.89420423 0.98666667 0.94352159 0.77192982 0.97674419]

Kappa:
0.942703352833701
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff1db60d780>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.552, val_acc:0.338]
Epoch [2/120    avg_loss:2.072, val_acc:0.551]
Epoch [3/120    avg_loss:1.824, val_acc:0.579]
Epoch [4/120    avg_loss:1.615, val_acc:0.624]
Epoch [5/120    avg_loss:1.416, val_acc:0.642]
Epoch [6/120    avg_loss:1.216, val_acc:0.679]
Epoch [7/120    avg_loss:1.007, val_acc:0.722]
Epoch [8/120    avg_loss:0.859, val_acc:0.743]
Epoch [9/120    avg_loss:0.782, val_acc:0.749]
Epoch [10/120    avg_loss:0.698, val_acc:0.809]
Epoch [11/120    avg_loss:0.543, val_acc:0.808]
Epoch [12/120    avg_loss:0.505, val_acc:0.814]
Epoch [13/120    avg_loss:0.506, val_acc:0.806]
Epoch [14/120    avg_loss:0.439, val_acc:0.869]
Epoch [15/120    avg_loss:0.370, val_acc:0.865]
Epoch [16/120    avg_loss:0.321, val_acc:0.872]
Epoch [17/120    avg_loss:0.311, val_acc:0.838]
Epoch [18/120    avg_loss:0.361, val_acc:0.853]
Epoch [19/120    avg_loss:0.360, val_acc:0.844]
Epoch [20/120    avg_loss:0.321, val_acc:0.894]
Epoch [21/120    avg_loss:0.232, val_acc:0.892]
Epoch [22/120    avg_loss:0.235, val_acc:0.869]
Epoch [23/120    avg_loss:0.216, val_acc:0.903]
Epoch [24/120    avg_loss:0.236, val_acc:0.898]
Epoch [25/120    avg_loss:0.233, val_acc:0.891]
Epoch [26/120    avg_loss:0.231, val_acc:0.898]
Epoch [27/120    avg_loss:0.220, val_acc:0.902]
Epoch [28/120    avg_loss:0.213, val_acc:0.914]
Epoch [29/120    avg_loss:0.198, val_acc:0.922]
Epoch [30/120    avg_loss:0.172, val_acc:0.916]
Epoch [31/120    avg_loss:0.141, val_acc:0.907]
Epoch [32/120    avg_loss:0.174, val_acc:0.918]
Epoch [33/120    avg_loss:0.164, val_acc:0.918]
Epoch [34/120    avg_loss:0.172, val_acc:0.922]
Epoch [35/120    avg_loss:0.158, val_acc:0.929]
Epoch [36/120    avg_loss:0.124, val_acc:0.925]
Epoch [37/120    avg_loss:0.104, val_acc:0.946]
Epoch [38/120    avg_loss:0.106, val_acc:0.936]
Epoch [39/120    avg_loss:0.101, val_acc:0.940]
Epoch [40/120    avg_loss:0.092, val_acc:0.932]
Epoch [41/120    avg_loss:0.122, val_acc:0.910]
Epoch [42/120    avg_loss:0.094, val_acc:0.931]
Epoch [43/120    avg_loss:0.084, val_acc:0.946]
Epoch [44/120    avg_loss:0.075, val_acc:0.935]
Epoch [45/120    avg_loss:0.087, val_acc:0.943]
Epoch [46/120    avg_loss:0.087, val_acc:0.938]
Epoch [47/120    avg_loss:0.091, val_acc:0.944]
Epoch [48/120    avg_loss:0.076, val_acc:0.951]
Epoch [49/120    avg_loss:0.065, val_acc:0.955]
Epoch [50/120    avg_loss:0.054, val_acc:0.950]
Epoch [51/120    avg_loss:0.070, val_acc:0.943]
Epoch [52/120    avg_loss:0.066, val_acc:0.954]
Epoch [53/120    avg_loss:0.043, val_acc:0.951]
Epoch [54/120    avg_loss:0.046, val_acc:0.949]
Epoch [55/120    avg_loss:0.045, val_acc:0.912]
Epoch [56/120    avg_loss:0.056, val_acc:0.961]
Epoch [57/120    avg_loss:0.052, val_acc:0.949]
Epoch [58/120    avg_loss:0.045, val_acc:0.940]
Epoch [59/120    avg_loss:0.039, val_acc:0.941]
Epoch [60/120    avg_loss:0.040, val_acc:0.961]
Epoch [61/120    avg_loss:0.033, val_acc:0.959]
Epoch [62/120    avg_loss:0.031, val_acc:0.957]
Epoch [63/120    avg_loss:0.033, val_acc:0.954]
Epoch [64/120    avg_loss:0.071, val_acc:0.954]
Epoch [65/120    avg_loss:0.052, val_acc:0.955]
Epoch [66/120    avg_loss:0.050, val_acc:0.958]
Epoch [67/120    avg_loss:0.046, val_acc:0.955]
Epoch [68/120    avg_loss:0.042, val_acc:0.943]
Epoch [69/120    avg_loss:0.042, val_acc:0.957]
Epoch [70/120    avg_loss:0.032, val_acc:0.965]
Epoch [71/120    avg_loss:0.042, val_acc:0.946]
Epoch [72/120    avg_loss:0.041, val_acc:0.953]
Epoch [73/120    avg_loss:0.028, val_acc:0.964]
Epoch [74/120    avg_loss:0.042, val_acc:0.955]
Epoch [75/120    avg_loss:0.044, val_acc:0.940]
Epoch [76/120    avg_loss:0.062, val_acc:0.968]
Epoch [77/120    avg_loss:0.056, val_acc:0.928]
Epoch [78/120    avg_loss:0.079, val_acc:0.947]
Epoch [79/120    avg_loss:0.036, val_acc:0.963]
Epoch [80/120    avg_loss:0.038, val_acc:0.962]
Epoch [81/120    avg_loss:0.035, val_acc:0.953]
Epoch [82/120    avg_loss:0.052, val_acc:0.948]
Epoch [83/120    avg_loss:0.048, val_acc:0.946]
Epoch [84/120    avg_loss:0.046, val_acc:0.956]
Epoch [85/120    avg_loss:0.040, val_acc:0.955]
Epoch [86/120    avg_loss:0.034, val_acc:0.962]
Epoch [87/120    avg_loss:0.030, val_acc:0.951]
Epoch [88/120    avg_loss:0.028, val_acc:0.961]
Epoch [89/120    avg_loss:0.017, val_acc:0.973]
Epoch [90/120    avg_loss:0.020, val_acc:0.961]
Epoch [91/120    avg_loss:0.019, val_acc:0.969]
Epoch [92/120    avg_loss:0.040, val_acc:0.963]
Epoch [93/120    avg_loss:0.028, val_acc:0.943]
Epoch [94/120    avg_loss:0.017, val_acc:0.973]
Epoch [95/120    avg_loss:0.022, val_acc:0.950]
Epoch [96/120    avg_loss:0.022, val_acc:0.975]
Epoch [97/120    avg_loss:0.019, val_acc:0.962]
Epoch [98/120    avg_loss:0.018, val_acc:0.971]
Epoch [99/120    avg_loss:0.012, val_acc:0.971]
Epoch [100/120    avg_loss:0.017, val_acc:0.957]
Epoch [101/120    avg_loss:0.025, val_acc:0.958]
Epoch [102/120    avg_loss:0.018, val_acc:0.975]
Epoch [103/120    avg_loss:0.019, val_acc:0.971]
Epoch [104/120    avg_loss:0.015, val_acc:0.962]
Epoch [105/120    avg_loss:0.013, val_acc:0.970]
Epoch [106/120    avg_loss:0.016, val_acc:0.968]
Epoch [107/120    avg_loss:0.013, val_acc:0.972]
Epoch [108/120    avg_loss:0.010, val_acc:0.976]
Epoch [109/120    avg_loss:0.009, val_acc:0.977]
Epoch [110/120    avg_loss:0.008, val_acc:0.972]
Epoch [111/120    avg_loss:0.011, val_acc:0.978]
Epoch [112/120    avg_loss:0.009, val_acc:0.971]
Epoch [113/120    avg_loss:0.012, val_acc:0.973]
Epoch [114/120    avg_loss:0.013, val_acc:0.970]
Epoch [115/120    avg_loss:0.010, val_acc:0.972]
Epoch [116/120    avg_loss:0.012, val_acc:0.973]
Epoch [117/120    avg_loss:0.011, val_acc:0.971]
Epoch [118/120    avg_loss:0.009, val_acc:0.971]
Epoch [119/120    avg_loss:0.008, val_acc:0.970]
Epoch [120/120    avg_loss:0.009, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    1 1264    0    0    0    1    0    0    2    2    8    4    0
     0    3    0]
 [   0    0   13  689    4   19    0    0    0    8    0    0   10    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    3    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   22   90    0    0    0    0    0    0  758    0    0    0
     0    5    0]
 [   0    0   13    0    0    1    5    0    0    0    7 2178    6    0
     0    0    0]
 [   0    0    1    2    1    0    0    0    0    0   24   20  486    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    3    2    0    0
  1132    0    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    40  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.32520325203252

F1 scores:
[       nan 0.96385542 0.9726818  0.90183246 0.98839907 0.97058824
 0.99018868 1.         0.99651568 0.73469388 0.90832834 0.98574338
 0.93371758 0.98930481 0.97839239 0.91768293 0.99401198]

Kappa:
0.9580934701444911
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2a1510a6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.566, val_acc:0.501]
Epoch [2/120    avg_loss:1.998, val_acc:0.573]
Epoch [3/120    avg_loss:1.792, val_acc:0.554]
Epoch [4/120    avg_loss:1.595, val_acc:0.632]
Epoch [5/120    avg_loss:1.433, val_acc:0.650]
Epoch [6/120    avg_loss:1.265, val_acc:0.692]
Epoch [7/120    avg_loss:1.116, val_acc:0.698]
Epoch [8/120    avg_loss:1.007, val_acc:0.753]
Epoch [9/120    avg_loss:0.818, val_acc:0.750]
Epoch [10/120    avg_loss:0.753, val_acc:0.786]
Epoch [11/120    avg_loss:0.707, val_acc:0.777]
Epoch [12/120    avg_loss:0.551, val_acc:0.798]
Epoch [13/120    avg_loss:0.496, val_acc:0.840]
Epoch [14/120    avg_loss:0.454, val_acc:0.834]
Epoch [15/120    avg_loss:0.470, val_acc:0.835]
Epoch [16/120    avg_loss:0.405, val_acc:0.866]
Epoch [17/120    avg_loss:0.389, val_acc:0.874]
Epoch [18/120    avg_loss:0.381, val_acc:0.859]
Epoch [19/120    avg_loss:0.358, val_acc:0.854]
Epoch [20/120    avg_loss:0.323, val_acc:0.878]
Epoch [21/120    avg_loss:0.346, val_acc:0.893]
Epoch [22/120    avg_loss:0.259, val_acc:0.922]
Epoch [23/120    avg_loss:0.212, val_acc:0.914]
Epoch [24/120    avg_loss:0.213, val_acc:0.911]
Epoch [25/120    avg_loss:0.268, val_acc:0.897]
Epoch [26/120    avg_loss:0.230, val_acc:0.912]
Epoch [27/120    avg_loss:0.253, val_acc:0.886]
Epoch [28/120    avg_loss:0.225, val_acc:0.902]
Epoch [29/120    avg_loss:0.183, val_acc:0.917]
Epoch [30/120    avg_loss:0.188, val_acc:0.895]
Epoch [31/120    avg_loss:0.161, val_acc:0.934]
Epoch [32/120    avg_loss:0.216, val_acc:0.896]
Epoch [33/120    avg_loss:0.175, val_acc:0.949]
Epoch [34/120    avg_loss:0.139, val_acc:0.920]
Epoch [35/120    avg_loss:0.124, val_acc:0.952]
Epoch [36/120    avg_loss:0.118, val_acc:0.941]
Epoch [37/120    avg_loss:0.125, val_acc:0.945]
Epoch [38/120    avg_loss:0.109, val_acc:0.945]
Epoch [39/120    avg_loss:0.116, val_acc:0.950]
Epoch [40/120    avg_loss:0.104, val_acc:0.954]
Epoch [41/120    avg_loss:0.103, val_acc:0.954]
Epoch [42/120    avg_loss:0.092, val_acc:0.959]
Epoch [43/120    avg_loss:0.149, val_acc:0.954]
Epoch [44/120    avg_loss:0.095, val_acc:0.952]
Epoch [45/120    avg_loss:0.086, val_acc:0.958]
Epoch [46/120    avg_loss:0.081, val_acc:0.927]
Epoch [47/120    avg_loss:0.103, val_acc:0.969]
Epoch [48/120    avg_loss:0.075, val_acc:0.960]
Epoch [49/120    avg_loss:0.087, val_acc:0.961]
Epoch [50/120    avg_loss:0.091, val_acc:0.960]
Epoch [51/120    avg_loss:0.073, val_acc:0.946]
Epoch [52/120    avg_loss:0.081, val_acc:0.951]
Epoch [53/120    avg_loss:0.079, val_acc:0.959]
Epoch [54/120    avg_loss:0.075, val_acc:0.965]
Epoch [55/120    avg_loss:0.054, val_acc:0.967]
Epoch [56/120    avg_loss:0.060, val_acc:0.975]
Epoch [57/120    avg_loss:0.059, val_acc:0.953]
Epoch [58/120    avg_loss:0.060, val_acc:0.944]
Epoch [59/120    avg_loss:0.067, val_acc:0.967]
Epoch [60/120    avg_loss:0.230, val_acc:0.939]
Epoch [61/120    avg_loss:0.123, val_acc:0.953]
Epoch [62/120    avg_loss:0.096, val_acc:0.965]
Epoch [63/120    avg_loss:0.095, val_acc:0.969]
Epoch [64/120    avg_loss:0.068, val_acc:0.971]
Epoch [65/120    avg_loss:0.052, val_acc:0.955]
Epoch [66/120    avg_loss:0.054, val_acc:0.969]
Epoch [67/120    avg_loss:0.041, val_acc:0.969]
Epoch [68/120    avg_loss:0.049, val_acc:0.975]
Epoch [69/120    avg_loss:0.043, val_acc:0.973]
Epoch [70/120    avg_loss:0.041, val_acc:0.982]
Epoch [71/120    avg_loss:0.041, val_acc:0.974]
Epoch [72/120    avg_loss:0.054, val_acc:0.977]
Epoch [73/120    avg_loss:0.038, val_acc:0.980]
Epoch [74/120    avg_loss:0.027, val_acc:0.987]
Epoch [75/120    avg_loss:0.029, val_acc:0.979]
Epoch [76/120    avg_loss:0.026, val_acc:0.981]
Epoch [77/120    avg_loss:0.029, val_acc:0.973]
Epoch [78/120    avg_loss:0.026, val_acc:0.974]
Epoch [79/120    avg_loss:0.046, val_acc:0.975]
Epoch [80/120    avg_loss:0.043, val_acc:0.977]
Epoch [81/120    avg_loss:0.030, val_acc:0.975]
Epoch [82/120    avg_loss:0.025, val_acc:0.982]
Epoch [83/120    avg_loss:0.017, val_acc:0.982]
Epoch [84/120    avg_loss:0.017, val_acc:0.984]
Epoch [85/120    avg_loss:0.025, val_acc:0.980]
Epoch [86/120    avg_loss:0.030, val_acc:0.979]
Epoch [87/120    avg_loss:0.032, val_acc:0.963]
Epoch [88/120    avg_loss:0.036, val_acc:0.971]
Epoch [89/120    avg_loss:0.029, val_acc:0.974]
Epoch [90/120    avg_loss:0.031, val_acc:0.981]
Epoch [91/120    avg_loss:0.029, val_acc:0.981]
Epoch [92/120    avg_loss:0.020, val_acc:0.983]
Epoch [93/120    avg_loss:0.019, val_acc:0.984]
Epoch [94/120    avg_loss:0.017, val_acc:0.983]
Epoch [95/120    avg_loss:0.016, val_acc:0.983]
Epoch [96/120    avg_loss:0.015, val_acc:0.985]
Epoch [97/120    avg_loss:0.016, val_acc:0.987]
Epoch [98/120    avg_loss:0.012, val_acc:0.987]
Epoch [99/120    avg_loss:0.011, val_acc:0.987]
Epoch [100/120    avg_loss:0.013, val_acc:0.987]
Epoch [101/120    avg_loss:0.010, val_acc:0.985]
Epoch [102/120    avg_loss:0.014, val_acc:0.984]
Epoch [103/120    avg_loss:0.016, val_acc:0.985]
Epoch [104/120    avg_loss:0.012, val_acc:0.985]
Epoch [105/120    avg_loss:0.012, val_acc:0.985]
Epoch [106/120    avg_loss:0.013, val_acc:0.984]
Epoch [107/120    avg_loss:0.014, val_acc:0.985]
Epoch [108/120    avg_loss:0.013, val_acc:0.985]
Epoch [109/120    avg_loss:0.014, val_acc:0.985]
Epoch [110/120    avg_loss:0.014, val_acc:0.987]
Epoch [111/120    avg_loss:0.012, val_acc:0.987]
Epoch [112/120    avg_loss:0.011, val_acc:0.987]
Epoch [113/120    avg_loss:0.011, val_acc:0.987]
Epoch [114/120    avg_loss:0.013, val_acc:0.987]
Epoch [115/120    avg_loss:0.010, val_acc:0.987]
Epoch [116/120    avg_loss:0.010, val_acc:0.987]
Epoch [117/120    avg_loss:0.012, val_acc:0.987]
Epoch [118/120    avg_loss:0.009, val_acc:0.987]
Epoch [119/120    avg_loss:0.012, val_acc:0.987]
Epoch [120/120    avg_loss:0.013, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1257    0    0    0    5    0    0    3   10    6    2    0
     0    2    0]
 [   0    0    0  724    0   18    0    0    0    5    0    0    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    0    0    6    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    7   89    0    4    0    0    0    0  773    1    0    0
     0    1    0]
 [   0    0    9    0    0    0   15    0    0    0    9 2164   11    2
     0    0    0]
 [   0    0    0   34    8    3    0    0    0    0   14    0  468    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    1    2    0    0
  1131    0    0]
 [   0    0    0    0    0    1   32    0    0    0    0    0    0    0
    90  224    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.5230352303523

F1 scores:
[       nan 0.98765432 0.98241501 0.90670006 0.98156682 0.95280899
 0.96041056 1.         1.         0.63829787 0.91914388 0.98700114
 0.92216749 0.99462366 0.9564482  0.7804878  0.96      ]

Kappa:
0.9489588950254039
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7f6d5c3748>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.522, val_acc:0.324]
Epoch [2/120    avg_loss:1.995, val_acc:0.514]
Epoch [3/120    avg_loss:1.751, val_acc:0.582]
Epoch [4/120    avg_loss:1.541, val_acc:0.625]
Epoch [5/120    avg_loss:1.309, val_acc:0.632]
Epoch [6/120    avg_loss:1.089, val_acc:0.649]
Epoch [7/120    avg_loss:0.956, val_acc:0.656]
Epoch [8/120    avg_loss:0.798, val_acc:0.729]
Epoch [9/120    avg_loss:0.685, val_acc:0.767]
Epoch [10/120    avg_loss:0.661, val_acc:0.757]
Epoch [11/120    avg_loss:0.602, val_acc:0.794]
Epoch [12/120    avg_loss:0.558, val_acc:0.766]
Epoch [13/120    avg_loss:0.498, val_acc:0.804]
Epoch [14/120    avg_loss:0.459, val_acc:0.825]
Epoch [15/120    avg_loss:0.425, val_acc:0.860]
Epoch [16/120    avg_loss:0.518, val_acc:0.817]
Epoch [17/120    avg_loss:0.422, val_acc:0.824]
Epoch [18/120    avg_loss:0.328, val_acc:0.866]
Epoch [19/120    avg_loss:0.370, val_acc:0.831]
Epoch [20/120    avg_loss:0.323, val_acc:0.883]
Epoch [21/120    avg_loss:0.278, val_acc:0.881]
Epoch [22/120    avg_loss:0.232, val_acc:0.868]
Epoch [23/120    avg_loss:0.255, val_acc:0.851]
Epoch [24/120    avg_loss:0.250, val_acc:0.878]
Epoch [25/120    avg_loss:0.207, val_acc:0.891]
Epoch [26/120    avg_loss:0.201, val_acc:0.919]
Epoch [27/120    avg_loss:0.209, val_acc:0.905]
Epoch [28/120    avg_loss:0.171, val_acc:0.918]
Epoch [29/120    avg_loss:0.185, val_acc:0.895]
Epoch [30/120    avg_loss:0.158, val_acc:0.926]
Epoch [31/120    avg_loss:0.138, val_acc:0.911]
Epoch [32/120    avg_loss:0.226, val_acc:0.882]
Epoch [33/120    avg_loss:0.175, val_acc:0.914]
Epoch [34/120    avg_loss:0.139, val_acc:0.922]
Epoch [35/120    avg_loss:0.136, val_acc:0.935]
Epoch [36/120    avg_loss:0.127, val_acc:0.913]
Epoch [37/120    avg_loss:0.133, val_acc:0.926]
Epoch [38/120    avg_loss:0.108, val_acc:0.936]
Epoch [39/120    avg_loss:0.103, val_acc:0.897]
Epoch [40/120    avg_loss:0.106, val_acc:0.941]
Epoch [41/120    avg_loss:0.099, val_acc:0.940]
Epoch [42/120    avg_loss:0.086, val_acc:0.946]
Epoch [43/120    avg_loss:0.075, val_acc:0.942]
Epoch [44/120    avg_loss:0.094, val_acc:0.947]
Epoch [45/120    avg_loss:0.114, val_acc:0.877]
Epoch [46/120    avg_loss:0.152, val_acc:0.920]
Epoch [47/120    avg_loss:0.156, val_acc:0.949]
Epoch [48/120    avg_loss:0.108, val_acc:0.941]
Epoch [49/120    avg_loss:0.088, val_acc:0.954]
Epoch [50/120    avg_loss:0.064, val_acc:0.950]
Epoch [51/120    avg_loss:0.074, val_acc:0.956]
Epoch [52/120    avg_loss:0.059, val_acc:0.955]
Epoch [53/120    avg_loss:0.050, val_acc:0.950]
Epoch [54/120    avg_loss:0.049, val_acc:0.964]
Epoch [55/120    avg_loss:0.055, val_acc:0.956]
Epoch [56/120    avg_loss:0.055, val_acc:0.946]
Epoch [57/120    avg_loss:0.077, val_acc:0.939]
Epoch [58/120    avg_loss:0.082, val_acc:0.933]
Epoch [59/120    avg_loss:0.060, val_acc:0.950]
Epoch [60/120    avg_loss:0.050, val_acc:0.955]
Epoch [61/120    avg_loss:0.044, val_acc:0.959]
Epoch [62/120    avg_loss:0.051, val_acc:0.961]
Epoch [63/120    avg_loss:0.048, val_acc:0.950]
Epoch [64/120    avg_loss:0.058, val_acc:0.958]
Epoch [65/120    avg_loss:0.065, val_acc:0.957]
Epoch [66/120    avg_loss:0.046, val_acc:0.959]
Epoch [67/120    avg_loss:0.050, val_acc:0.959]
Epoch [68/120    avg_loss:0.040, val_acc:0.965]
Epoch [69/120    avg_loss:0.031, val_acc:0.968]
Epoch [70/120    avg_loss:0.028, val_acc:0.972]
Epoch [71/120    avg_loss:0.024, val_acc:0.972]
Epoch [72/120    avg_loss:0.024, val_acc:0.972]
Epoch [73/120    avg_loss:0.024, val_acc:0.971]
Epoch [74/120    avg_loss:0.020, val_acc:0.968]
Epoch [75/120    avg_loss:0.025, val_acc:0.973]
Epoch [76/120    avg_loss:0.019, val_acc:0.975]
Epoch [77/120    avg_loss:0.021, val_acc:0.972]
Epoch [78/120    avg_loss:0.023, val_acc:0.976]
Epoch [79/120    avg_loss:0.023, val_acc:0.975]
Epoch [80/120    avg_loss:0.023, val_acc:0.978]
Epoch [81/120    avg_loss:0.023, val_acc:0.980]
Epoch [82/120    avg_loss:0.028, val_acc:0.973]
Epoch [83/120    avg_loss:0.022, val_acc:0.973]
Epoch [84/120    avg_loss:0.027, val_acc:0.976]
Epoch [85/120    avg_loss:0.020, val_acc:0.977]
Epoch [86/120    avg_loss:0.019, val_acc:0.978]
Epoch [87/120    avg_loss:0.022, val_acc:0.978]
Epoch [88/120    avg_loss:0.025, val_acc:0.978]
Epoch [89/120    avg_loss:0.023, val_acc:0.978]
Epoch [90/120    avg_loss:0.018, val_acc:0.978]
Epoch [91/120    avg_loss:0.025, val_acc:0.978]
Epoch [92/120    avg_loss:0.022, val_acc:0.980]
Epoch [93/120    avg_loss:0.019, val_acc:0.979]
Epoch [94/120    avg_loss:0.020, val_acc:0.975]
Epoch [95/120    avg_loss:0.019, val_acc:0.976]
Epoch [96/120    avg_loss:0.022, val_acc:0.978]
Epoch [97/120    avg_loss:0.022, val_acc:0.978]
Epoch [98/120    avg_loss:0.020, val_acc:0.979]
Epoch [99/120    avg_loss:0.024, val_acc:0.978]
Epoch [100/120    avg_loss:0.019, val_acc:0.977]
Epoch [101/120    avg_loss:0.022, val_acc:0.975]
Epoch [102/120    avg_loss:0.018, val_acc:0.978]
Epoch [103/120    avg_loss:0.020, val_acc:0.979]
Epoch [104/120    avg_loss:0.022, val_acc:0.980]
Epoch [105/120    avg_loss:0.024, val_acc:0.975]
Epoch [106/120    avg_loss:0.018, val_acc:0.977]
Epoch [107/120    avg_loss:0.019, val_acc:0.979]
Epoch [108/120    avg_loss:0.020, val_acc:0.979]
Epoch [109/120    avg_loss:0.020, val_acc:0.981]
Epoch [110/120    avg_loss:0.016, val_acc:0.978]
Epoch [111/120    avg_loss:0.022, val_acc:0.979]
Epoch [112/120    avg_loss:0.018, val_acc:0.977]
Epoch [113/120    avg_loss:0.016, val_acc:0.979]
Epoch [114/120    avg_loss:0.025, val_acc:0.979]
Epoch [115/120    avg_loss:0.022, val_acc:0.979]
Epoch [116/120    avg_loss:0.019, val_acc:0.978]
Epoch [117/120    avg_loss:0.018, val_acc:0.978]
Epoch [118/120    avg_loss:0.021, val_acc:0.976]
Epoch [119/120    avg_loss:0.018, val_acc:0.976]
Epoch [120/120    avg_loss:0.018, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1259    1    0    0    4    0    0    0    7   10    2    0
     0    2    0]
 [   0    0    1  718    1   17    0    0    0    4    0    0    3    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    3    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   24   90    0    9    0    0    0    0  746    0    0    0
     0    6    0]
 [   0    0    9   20    0    0    5    0    1    0   11 2161    0    3
     0    0    0]
 [   0    0    0   27   12   10    0    0    0    0    9    6  465    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    2    0    3    4    0    0
  1129    0    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
   126  216    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
95.03523035230353

F1 scores:
[       nan 0.94871795 0.97672614 0.89526185 0.97038724 0.95459579
 0.98793363 1.         0.99652375 0.7804878  0.90151057 0.98383792
 0.92261905 0.98404255 0.94279749 0.75656743 0.95294118]

Kappa:
0.9433700344334264
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f83c2411a90>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.608, val_acc:0.456]
Epoch [2/120    avg_loss:2.075, val_acc:0.506]
Epoch [3/120    avg_loss:1.739, val_acc:0.611]
Epoch [4/120    avg_loss:1.493, val_acc:0.532]
Epoch [5/120    avg_loss:1.351, val_acc:0.664]
Epoch [6/120    avg_loss:1.147, val_acc:0.652]
Epoch [7/120    avg_loss:0.955, val_acc:0.694]
Epoch [8/120    avg_loss:0.848, val_acc:0.714]
Epoch [9/120    avg_loss:0.760, val_acc:0.792]
Epoch [10/120    avg_loss:0.619, val_acc:0.784]
Epoch [11/120    avg_loss:0.607, val_acc:0.816]
Epoch [12/120    avg_loss:0.460, val_acc:0.811]
Epoch [13/120    avg_loss:0.497, val_acc:0.803]
Epoch [14/120    avg_loss:0.468, val_acc:0.838]
Epoch [15/120    avg_loss:0.392, val_acc:0.854]
Epoch [16/120    avg_loss:0.384, val_acc:0.839]
Epoch [17/120    avg_loss:0.354, val_acc:0.846]
Epoch [18/120    avg_loss:0.325, val_acc:0.867]
Epoch [19/120    avg_loss:0.283, val_acc:0.870]
Epoch [20/120    avg_loss:0.264, val_acc:0.881]
Epoch [21/120    avg_loss:0.230, val_acc:0.884]
Epoch [22/120    avg_loss:0.222, val_acc:0.877]
Epoch [23/120    avg_loss:0.269, val_acc:0.853]
Epoch [24/120    avg_loss:0.265, val_acc:0.892]
Epoch [25/120    avg_loss:0.187, val_acc:0.905]
Epoch [26/120    avg_loss:0.148, val_acc:0.939]
Epoch [27/120    avg_loss:0.225, val_acc:0.863]
Epoch [28/120    avg_loss:0.206, val_acc:0.905]
Epoch [29/120    avg_loss:0.175, val_acc:0.928]
Epoch [30/120    avg_loss:0.178, val_acc:0.904]
Epoch [31/120    avg_loss:0.149, val_acc:0.916]
Epoch [32/120    avg_loss:0.165, val_acc:0.911]
Epoch [33/120    avg_loss:0.136, val_acc:0.913]
Epoch [34/120    avg_loss:0.111, val_acc:0.922]
Epoch [35/120    avg_loss:0.098, val_acc:0.927]
Epoch [36/120    avg_loss:0.122, val_acc:0.940]
Epoch [37/120    avg_loss:0.137, val_acc:0.913]
Epoch [38/120    avg_loss:0.114, val_acc:0.931]
Epoch [39/120    avg_loss:0.087, val_acc:0.950]
Epoch [40/120    avg_loss:0.089, val_acc:0.951]
Epoch [41/120    avg_loss:0.080, val_acc:0.938]
Epoch [42/120    avg_loss:0.080, val_acc:0.931]
Epoch [43/120    avg_loss:0.088, val_acc:0.929]
Epoch [44/120    avg_loss:0.068, val_acc:0.949]
Epoch [45/120    avg_loss:0.071, val_acc:0.935]
Epoch [46/120    avg_loss:0.091, val_acc:0.927]
Epoch [47/120    avg_loss:0.122, val_acc:0.927]
Epoch [48/120    avg_loss:0.109, val_acc:0.940]
Epoch [49/120    avg_loss:0.113, val_acc:0.920]
Epoch [50/120    avg_loss:0.108, val_acc:0.957]
Epoch [51/120    avg_loss:0.118, val_acc:0.921]
Epoch [52/120    avg_loss:0.095, val_acc:0.926]
Epoch [53/120    avg_loss:0.147, val_acc:0.936]
Epoch [54/120    avg_loss:0.075, val_acc:0.941]
Epoch [55/120    avg_loss:0.059, val_acc:0.954]
Epoch [56/120    avg_loss:0.063, val_acc:0.959]
Epoch [57/120    avg_loss:0.055, val_acc:0.957]
Epoch [58/120    avg_loss:0.060, val_acc:0.934]
Epoch [59/120    avg_loss:0.049, val_acc:0.957]
Epoch [60/120    avg_loss:0.045, val_acc:0.956]
Epoch [61/120    avg_loss:0.049, val_acc:0.958]
Epoch [62/120    avg_loss:0.043, val_acc:0.954]
Epoch [63/120    avg_loss:0.043, val_acc:0.950]
Epoch [64/120    avg_loss:0.039, val_acc:0.956]
Epoch [65/120    avg_loss:0.044, val_acc:0.963]
Epoch [66/120    avg_loss:0.046, val_acc:0.957]
Epoch [67/120    avg_loss:0.047, val_acc:0.959]
Epoch [68/120    avg_loss:0.042, val_acc:0.965]
Epoch [69/120    avg_loss:0.031, val_acc:0.971]
Epoch [70/120    avg_loss:0.026, val_acc:0.968]
Epoch [71/120    avg_loss:0.027, val_acc:0.964]
Epoch [72/120    avg_loss:0.030, val_acc:0.959]
Epoch [73/120    avg_loss:0.024, val_acc:0.965]
Epoch [74/120    avg_loss:0.036, val_acc:0.964]
Epoch [75/120    avg_loss:0.026, val_acc:0.969]
Epoch [76/120    avg_loss:0.026, val_acc:0.970]
Epoch [77/120    avg_loss:0.033, val_acc:0.957]
Epoch [78/120    avg_loss:0.030, val_acc:0.972]
Epoch [79/120    avg_loss:0.023, val_acc:0.958]
Epoch [80/120    avg_loss:0.032, val_acc:0.959]
Epoch [81/120    avg_loss:0.038, val_acc:0.966]
Epoch [82/120    avg_loss:0.041, val_acc:0.961]
Epoch [83/120    avg_loss:0.032, val_acc:0.956]
Epoch [84/120    avg_loss:0.038, val_acc:0.970]
Epoch [85/120    avg_loss:0.021, val_acc:0.969]
Epoch [86/120    avg_loss:0.028, val_acc:0.973]
Epoch [87/120    avg_loss:0.035, val_acc:0.959]
Epoch [88/120    avg_loss:0.030, val_acc:0.965]
Epoch [89/120    avg_loss:0.033, val_acc:0.965]
Epoch [90/120    avg_loss:0.047, val_acc:0.955]
Epoch [91/120    avg_loss:0.052, val_acc:0.969]
Epoch [92/120    avg_loss:0.042, val_acc:0.970]
Epoch [93/120    avg_loss:0.033, val_acc:0.964]
Epoch [94/120    avg_loss:0.027, val_acc:0.968]
Epoch [95/120    avg_loss:0.022, val_acc:0.962]
Epoch [96/120    avg_loss:0.021, val_acc:0.970]
Epoch [97/120    avg_loss:0.018, val_acc:0.973]
Epoch [98/120    avg_loss:0.016, val_acc:0.977]
Epoch [99/120    avg_loss:0.020, val_acc:0.970]
Epoch [100/120    avg_loss:0.016, val_acc:0.971]
Epoch [101/120    avg_loss:0.021, val_acc:0.975]
Epoch [102/120    avg_loss:0.021, val_acc:0.968]
Epoch [103/120    avg_loss:0.020, val_acc:0.969]
Epoch [104/120    avg_loss:0.020, val_acc:0.968]
Epoch [105/120    avg_loss:0.020, val_acc:0.971]
Epoch [106/120    avg_loss:0.018, val_acc:0.957]
Epoch [107/120    avg_loss:0.018, val_acc:0.977]
Epoch [108/120    avg_loss:0.016, val_acc:0.971]
Epoch [109/120    avg_loss:0.017, val_acc:0.972]
Epoch [110/120    avg_loss:0.015, val_acc:0.975]
Epoch [111/120    avg_loss:0.014, val_acc:0.970]
Epoch [112/120    avg_loss:0.011, val_acc:0.979]
Epoch [113/120    avg_loss:0.017, val_acc:0.976]
Epoch [114/120    avg_loss:0.014, val_acc:0.979]
Epoch [115/120    avg_loss:0.012, val_acc:0.975]
Epoch [116/120    avg_loss:0.017, val_acc:0.964]
Epoch [117/120    avg_loss:0.018, val_acc:0.961]
Epoch [118/120    avg_loss:0.025, val_acc:0.973]
Epoch [119/120    avg_loss:0.025, val_acc:0.966]
Epoch [120/120    avg_loss:0.018, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1252    0    0    0    1    0    0    0    8   22    2    0
     0    0    0]
 [   0    0    2  734    1    4    0    0    0    4    0    0    0    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0  645    0    0    0    0    3    3    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   13    0    0    2    0
     0    0    0]
 [   0    0   13   84    0    2    5    0    0    0  761    4    0    1
     0    5    0]
 [   0    0   19    0    0    4    2    0    0    0   19 2161    0    1
     4    0    0]
 [   0    0    0   20    0    3    0    0    0    0    5   19  480    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    1    0    1    3    0    0
  1130    0    0]
 [   0    0    0    0    0    1   15    0    0    0    0    0    1    0
   138  192    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.14363143631437

F1 scores:
[       nan 0.98765432 0.97129558 0.92443325 0.99765808 0.97627119
 0.97358491 1.         0.99883856 0.68421053 0.91192331 0.9773858
 0.93933464 0.98930481 0.93737039 0.70588235 0.96      ]

Kappa:
0.9445552677244847
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fae3eb23710>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.562, val_acc:0.373]
Epoch [2/120    avg_loss:2.050, val_acc:0.523]
Epoch [3/120    avg_loss:1.820, val_acc:0.602]
Epoch [4/120    avg_loss:1.596, val_acc:0.648]
Epoch [5/120    avg_loss:1.351, val_acc:0.677]
Epoch [6/120    avg_loss:1.166, val_acc:0.722]
Epoch [7/120    avg_loss:1.004, val_acc:0.776]
Epoch [8/120    avg_loss:0.827, val_acc:0.787]
Epoch [9/120    avg_loss:0.722, val_acc:0.763]
Epoch [10/120    avg_loss:0.665, val_acc:0.825]
Epoch [11/120    avg_loss:0.653, val_acc:0.817]
Epoch [12/120    avg_loss:0.568, val_acc:0.834]
Epoch [13/120    avg_loss:0.501, val_acc:0.838]
Epoch [14/120    avg_loss:0.364, val_acc:0.871]
Epoch [15/120    avg_loss:0.375, val_acc:0.883]
Epoch [16/120    avg_loss:0.344, val_acc:0.875]
Epoch [17/120    avg_loss:0.299, val_acc:0.901]
Epoch [18/120    avg_loss:0.257, val_acc:0.910]
Epoch [19/120    avg_loss:0.265, val_acc:0.921]
Epoch [20/120    avg_loss:0.286, val_acc:0.924]
Epoch [21/120    avg_loss:0.262, val_acc:0.914]
Epoch [22/120    avg_loss:0.244, val_acc:0.922]
Epoch [23/120    avg_loss:0.211, val_acc:0.917]
Epoch [24/120    avg_loss:0.169, val_acc:0.936]
Epoch [25/120    avg_loss:0.227, val_acc:0.920]
Epoch [26/120    avg_loss:0.246, val_acc:0.912]
Epoch [27/120    avg_loss:0.239, val_acc:0.922]
Epoch [28/120    avg_loss:0.184, val_acc:0.917]
Epoch [29/120    avg_loss:0.166, val_acc:0.940]
Epoch [30/120    avg_loss:0.170, val_acc:0.949]
Epoch [31/120    avg_loss:0.155, val_acc:0.940]
Epoch [32/120    avg_loss:0.149, val_acc:0.935]
Epoch [33/120    avg_loss:0.160, val_acc:0.935]
Epoch [34/120    avg_loss:0.177, val_acc:0.933]
Epoch [35/120    avg_loss:0.148, val_acc:0.938]
Epoch [36/120    avg_loss:0.183, val_acc:0.902]
Epoch [37/120    avg_loss:0.222, val_acc:0.935]
Epoch [38/120    avg_loss:0.225, val_acc:0.888]
Epoch [39/120    avg_loss:0.188, val_acc:0.923]
Epoch [40/120    avg_loss:0.129, val_acc:0.948]
Epoch [41/120    avg_loss:0.104, val_acc:0.958]
Epoch [42/120    avg_loss:0.097, val_acc:0.931]
Epoch [43/120    avg_loss:0.111, val_acc:0.931]
Epoch [44/120    avg_loss:0.110, val_acc:0.963]
Epoch [45/120    avg_loss:0.088, val_acc:0.955]
Epoch [46/120    avg_loss:0.070, val_acc:0.965]
Epoch [47/120    avg_loss:0.074, val_acc:0.952]
Epoch [48/120    avg_loss:0.092, val_acc:0.968]
Epoch [49/120    avg_loss:0.070, val_acc:0.963]
Epoch [50/120    avg_loss:0.052, val_acc:0.970]
Epoch [51/120    avg_loss:0.058, val_acc:0.964]
Epoch [52/120    avg_loss:0.089, val_acc:0.962]
Epoch [53/120    avg_loss:0.083, val_acc:0.962]
Epoch [54/120    avg_loss:0.058, val_acc:0.964]
Epoch [55/120    avg_loss:0.058, val_acc:0.978]
Epoch [56/120    avg_loss:0.049, val_acc:0.974]
Epoch [57/120    avg_loss:0.071, val_acc:0.936]
Epoch [58/120    avg_loss:0.084, val_acc:0.970]
Epoch [59/120    avg_loss:0.092, val_acc:0.958]
Epoch [60/120    avg_loss:0.065, val_acc:0.965]
Epoch [61/120    avg_loss:0.050, val_acc:0.973]
Epoch [62/120    avg_loss:0.051, val_acc:0.972]
Epoch [63/120    avg_loss:0.044, val_acc:0.972]
Epoch [64/120    avg_loss:0.054, val_acc:0.969]
Epoch [65/120    avg_loss:0.086, val_acc:0.958]
Epoch [66/120    avg_loss:0.048, val_acc:0.974]
Epoch [67/120    avg_loss:0.059, val_acc:0.973]
Epoch [68/120    avg_loss:0.063, val_acc:0.965]
Epoch [69/120    avg_loss:0.041, val_acc:0.968]
Epoch [70/120    avg_loss:0.033, val_acc:0.972]
Epoch [71/120    avg_loss:0.029, val_acc:0.972]
Epoch [72/120    avg_loss:0.032, val_acc:0.973]
Epoch [73/120    avg_loss:0.028, val_acc:0.972]
Epoch [74/120    avg_loss:0.026, val_acc:0.973]
Epoch [75/120    avg_loss:0.028, val_acc:0.975]
Epoch [76/120    avg_loss:0.030, val_acc:0.977]
Epoch [77/120    avg_loss:0.021, val_acc:0.978]
Epoch [78/120    avg_loss:0.028, val_acc:0.978]
Epoch [79/120    avg_loss:0.027, val_acc:0.977]
Epoch [80/120    avg_loss:0.022, val_acc:0.979]
Epoch [81/120    avg_loss:0.026, val_acc:0.983]
Epoch [82/120    avg_loss:0.021, val_acc:0.982]
Epoch [83/120    avg_loss:0.024, val_acc:0.979]
Epoch [84/120    avg_loss:0.031, val_acc:0.979]
Epoch [85/120    avg_loss:0.024, val_acc:0.978]
Epoch [86/120    avg_loss:0.021, val_acc:0.980]
Epoch [87/120    avg_loss:0.022, val_acc:0.979]
Epoch [88/120    avg_loss:0.030, val_acc:0.978]
Epoch [89/120    avg_loss:0.022, val_acc:0.979]
Epoch [90/120    avg_loss:0.024, val_acc:0.978]
Epoch [91/120    avg_loss:0.020, val_acc:0.982]
Epoch [92/120    avg_loss:0.026, val_acc:0.979]
Epoch [93/120    avg_loss:0.019, val_acc:0.978]
Epoch [94/120    avg_loss:0.020, val_acc:0.979]
Epoch [95/120    avg_loss:0.022, val_acc:0.979]
Epoch [96/120    avg_loss:0.020, val_acc:0.979]
Epoch [97/120    avg_loss:0.025, val_acc:0.979]
Epoch [98/120    avg_loss:0.019, val_acc:0.980]
Epoch [99/120    avg_loss:0.019, val_acc:0.980]
Epoch [100/120    avg_loss:0.023, val_acc:0.980]
Epoch [101/120    avg_loss:0.021, val_acc:0.980]
Epoch [102/120    avg_loss:0.022, val_acc:0.980]
Epoch [103/120    avg_loss:0.023, val_acc:0.980]
Epoch [104/120    avg_loss:0.025, val_acc:0.980]
Epoch [105/120    avg_loss:0.022, val_acc:0.980]
Epoch [106/120    avg_loss:0.022, val_acc:0.979]
Epoch [107/120    avg_loss:0.023, val_acc:0.981]
Epoch [108/120    avg_loss:0.020, val_acc:0.981]
Epoch [109/120    avg_loss:0.022, val_acc:0.981]
Epoch [110/120    avg_loss:0.017, val_acc:0.981]
Epoch [111/120    avg_loss:0.023, val_acc:0.981]
Epoch [112/120    avg_loss:0.019, val_acc:0.981]
Epoch [113/120    avg_loss:0.020, val_acc:0.981]
Epoch [114/120    avg_loss:0.025, val_acc:0.980]
Epoch [115/120    avg_loss:0.020, val_acc:0.981]
Epoch [116/120    avg_loss:0.019, val_acc:0.981]
Epoch [117/120    avg_loss:0.023, val_acc:0.981]
Epoch [118/120    avg_loss:0.023, val_acc:0.981]
Epoch [119/120    avg_loss:0.017, val_acc:0.981]
Epoch [120/120    avg_loss:0.020, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1255    0    2    0    0    0    0    4    4   10    1    0
     0    9    0]
 [   0    0    2  720    0   18    0    0    0    6    0    0    1    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  421    0    0    0    5    0    0    0    0
     9    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   13    0    0    2    0
     0    0    0]
 [   0    0   17   71    0    6    0    0    0    0  750   19    0    0
     0   12    0]
 [   0    0   10    0    0    2    5    0    1    0    2 2183    5    2
     0    0    0]
 [   0    0    0   15    4    9    0    0    0    0    7   10  482    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    2    0    0
  1133    0    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    96  232    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.58807588075881

F1 scores:
[       nan 0.975      0.97703387 0.92544987 0.98611111 0.94500561
 0.97901049 1.         0.99767981 0.56521739 0.91296409 0.98377648
 0.9404878  0.99462366 0.95330248 0.77333333 0.96      ]

Kappa:
0.9496502556286389
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:01:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc3b0891710>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.569, val_acc:0.493]
Epoch [2/120    avg_loss:2.024, val_acc:0.564]
Epoch [3/120    avg_loss:1.810, val_acc:0.615]
Epoch [4/120    avg_loss:1.657, val_acc:0.656]
Epoch [5/120    avg_loss:1.440, val_acc:0.677]
Epoch [6/120    avg_loss:1.325, val_acc:0.674]
Epoch [7/120    avg_loss:1.096, val_acc:0.720]
Epoch [8/120    avg_loss:0.939, val_acc:0.699]
Epoch [9/120    avg_loss:0.918, val_acc:0.718]
Epoch [10/120    avg_loss:0.787, val_acc:0.778]
Epoch [11/120    avg_loss:0.636, val_acc:0.786]
Epoch [12/120    avg_loss:0.614, val_acc:0.785]
Epoch [13/120    avg_loss:0.571, val_acc:0.839]
Epoch [14/120    avg_loss:0.515, val_acc:0.823]
Epoch [15/120    avg_loss:0.485, val_acc:0.806]
Epoch [16/120    avg_loss:0.456, val_acc:0.854]
Epoch [17/120    avg_loss:0.351, val_acc:0.860]
Epoch [18/120    avg_loss:0.319, val_acc:0.878]
Epoch [19/120    avg_loss:0.294, val_acc:0.867]
Epoch [20/120    avg_loss:0.372, val_acc:0.871]
Epoch [21/120    avg_loss:0.299, val_acc:0.853]
Epoch [22/120    avg_loss:0.245, val_acc:0.875]
Epoch [23/120    avg_loss:0.283, val_acc:0.891]
Epoch [24/120    avg_loss:0.205, val_acc:0.890]
Epoch [25/120    avg_loss:0.212, val_acc:0.902]
Epoch [26/120    avg_loss:0.198, val_acc:0.915]
Epoch [27/120    avg_loss:0.178, val_acc:0.910]
Epoch [28/120    avg_loss:0.173, val_acc:0.900]
Epoch [29/120    avg_loss:0.186, val_acc:0.915]
Epoch [30/120    avg_loss:0.287, val_acc:0.897]
Epoch [31/120    avg_loss:0.238, val_acc:0.903]
Epoch [32/120    avg_loss:0.181, val_acc:0.910]
Epoch [33/120    avg_loss:0.228, val_acc:0.892]
Epoch [34/120    avg_loss:0.195, val_acc:0.915]
Epoch [35/120    avg_loss:0.166, val_acc:0.906]
Epoch [36/120    avg_loss:0.144, val_acc:0.939]
Epoch [37/120    avg_loss:0.120, val_acc:0.920]
Epoch [38/120    avg_loss:0.139, val_acc:0.902]
Epoch [39/120    avg_loss:0.125, val_acc:0.949]
Epoch [40/120    avg_loss:0.101, val_acc:0.911]
Epoch [41/120    avg_loss:0.110, val_acc:0.932]
Epoch [42/120    avg_loss:0.095, val_acc:0.934]
Epoch [43/120    avg_loss:0.116, val_acc:0.943]
Epoch [44/120    avg_loss:0.117, val_acc:0.939]
Epoch [45/120    avg_loss:0.104, val_acc:0.929]
Epoch [46/120    avg_loss:0.089, val_acc:0.949]
Epoch [47/120    avg_loss:0.081, val_acc:0.961]
Epoch [48/120    avg_loss:0.082, val_acc:0.948]
Epoch [49/120    avg_loss:0.063, val_acc:0.955]
Epoch [50/120    avg_loss:0.064, val_acc:0.962]
Epoch [51/120    avg_loss:0.069, val_acc:0.955]
Epoch [52/120    avg_loss:0.071, val_acc:0.961]
Epoch [53/120    avg_loss:0.079, val_acc:0.954]
Epoch [54/120    avg_loss:0.091, val_acc:0.950]
Epoch [55/120    avg_loss:0.064, val_acc:0.953]
Epoch [56/120    avg_loss:0.072, val_acc:0.934]
Epoch [57/120    avg_loss:0.093, val_acc:0.956]
Epoch [58/120    avg_loss:0.106, val_acc:0.938]
Epoch [59/120    avg_loss:0.126, val_acc:0.939]
Epoch [60/120    avg_loss:0.113, val_acc:0.925]
Epoch [61/120    avg_loss:0.089, val_acc:0.945]
Epoch [62/120    avg_loss:0.063, val_acc:0.955]
Epoch [63/120    avg_loss:0.055, val_acc:0.965]
Epoch [64/120    avg_loss:0.049, val_acc:0.951]
Epoch [65/120    avg_loss:0.045, val_acc:0.949]
Epoch [66/120    avg_loss:0.048, val_acc:0.962]
Epoch [67/120    avg_loss:0.048, val_acc:0.969]
Epoch [68/120    avg_loss:0.044, val_acc:0.963]
Epoch [69/120    avg_loss:0.041, val_acc:0.971]
Epoch [70/120    avg_loss:0.033, val_acc:0.963]
Epoch [71/120    avg_loss:0.060, val_acc:0.963]
Epoch [72/120    avg_loss:0.065, val_acc:0.956]
Epoch [73/120    avg_loss:0.046, val_acc:0.964]
Epoch [74/120    avg_loss:0.044, val_acc:0.943]
Epoch [75/120    avg_loss:0.051, val_acc:0.967]
Epoch [76/120    avg_loss:0.051, val_acc:0.959]
Epoch [77/120    avg_loss:0.036, val_acc:0.964]
Epoch [78/120    avg_loss:0.034, val_acc:0.964]
Epoch [79/120    avg_loss:0.036, val_acc:0.974]
Epoch [80/120    avg_loss:0.036, val_acc:0.973]
Epoch [81/120    avg_loss:0.027, val_acc:0.974]
Epoch [82/120    avg_loss:0.031, val_acc:0.979]
Epoch [83/120    avg_loss:0.029, val_acc:0.979]
Epoch [84/120    avg_loss:0.024, val_acc:0.978]
Epoch [85/120    avg_loss:0.028, val_acc:0.972]
Epoch [86/120    avg_loss:0.039, val_acc:0.972]
Epoch [87/120    avg_loss:0.037, val_acc:0.969]
Epoch [88/120    avg_loss:0.031, val_acc:0.978]
Epoch [89/120    avg_loss:0.031, val_acc:0.971]
Epoch [90/120    avg_loss:0.025, val_acc:0.969]
Epoch [91/120    avg_loss:0.027, val_acc:0.974]
Epoch [92/120    avg_loss:0.026, val_acc:0.973]
Epoch [93/120    avg_loss:0.028, val_acc:0.981]
Epoch [94/120    avg_loss:0.023, val_acc:0.974]
Epoch [95/120    avg_loss:0.021, val_acc:0.984]
Epoch [96/120    avg_loss:0.016, val_acc:0.979]
Epoch [97/120    avg_loss:0.020, val_acc:0.979]
Epoch [98/120    avg_loss:0.021, val_acc:0.979]
Epoch [99/120    avg_loss:0.020, val_acc:0.970]
Epoch [100/120    avg_loss:0.028, val_acc:0.979]
Epoch [101/120    avg_loss:0.019, val_acc:0.973]
Epoch [102/120    avg_loss:0.020, val_acc:0.970]
Epoch [103/120    avg_loss:0.016, val_acc:0.987]
Epoch [104/120    avg_loss:0.016, val_acc:0.982]
Epoch [105/120    avg_loss:0.015, val_acc:0.975]
Epoch [106/120    avg_loss:0.016, val_acc:0.979]
Epoch [107/120    avg_loss:0.015, val_acc:0.979]
Epoch [108/120    avg_loss:0.012, val_acc:0.982]
Epoch [109/120    avg_loss:0.015, val_acc:0.981]
Epoch [110/120    avg_loss:0.019, val_acc:0.989]
Epoch [111/120    avg_loss:0.018, val_acc:0.982]
Epoch [112/120    avg_loss:0.012, val_acc:0.982]
Epoch [113/120    avg_loss:0.015, val_acc:0.981]
Epoch [114/120    avg_loss:0.014, val_acc:0.981]
Epoch [115/120    avg_loss:0.014, val_acc:0.983]
Epoch [116/120    avg_loss:0.012, val_acc:0.982]
Epoch [117/120    avg_loss:0.013, val_acc:0.982]
Epoch [118/120    avg_loss:0.011, val_acc:0.981]
Epoch [119/120    avg_loss:0.012, val_acc:0.985]
Epoch [120/120    avg_loss:0.013, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1274    0    0    0    0    0    0    0    4    2    1    0
     0    4    0]
 [   0    0    0  727    1    6    0    0    0    5    0    0    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    5    0    5    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   16   86    0    5    2    0    0    0  739   10    2    0
     3   12    0]
 [   0    0   15    0    0    0   15    0    0    0    2 2174    0    2
     2    0    0]
 [   0    0    0    0    9    3    0    0    0    0    0    0  520    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
    98  224    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.10840108401084

F1 scores:
[       nan 0.98765432 0.98378378 0.9314542  0.97706422 0.97018349
 0.96826568 0.90909091 0.99883856 0.72727273 0.91122072 0.98885604
 0.97469541 0.99462366 0.9550609  0.76320273 0.98224852]

Kappa:
0.9556181937243079
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f76be909780>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.502, val_acc:0.431]
Epoch [2/120    avg_loss:1.995, val_acc:0.583]
Epoch [3/120    avg_loss:1.791, val_acc:0.586]
Epoch [4/120    avg_loss:1.597, val_acc:0.630]
Epoch [5/120    avg_loss:1.404, val_acc:0.620]
Epoch [6/120    avg_loss:1.230, val_acc:0.662]
Epoch [7/120    avg_loss:1.069, val_acc:0.716]
Epoch [8/120    avg_loss:0.892, val_acc:0.740]
Epoch [9/120    avg_loss:0.843, val_acc:0.745]
Epoch [10/120    avg_loss:0.747, val_acc:0.807]
Epoch [11/120    avg_loss:0.614, val_acc:0.792]
Epoch [12/120    avg_loss:0.628, val_acc:0.787]
Epoch [13/120    avg_loss:0.592, val_acc:0.828]
Epoch [14/120    avg_loss:0.630, val_acc:0.811]
Epoch [15/120    avg_loss:0.539, val_acc:0.822]
Epoch [16/120    avg_loss:0.433, val_acc:0.822]
Epoch [17/120    avg_loss:0.366, val_acc:0.833]
Epoch [18/120    avg_loss:0.433, val_acc:0.832]
Epoch [19/120    avg_loss:0.426, val_acc:0.793]
Epoch [20/120    avg_loss:0.403, val_acc:0.875]
Epoch [21/120    avg_loss:0.297, val_acc:0.887]
Epoch [22/120    avg_loss:0.243, val_acc:0.907]
Epoch [23/120    avg_loss:0.277, val_acc:0.846]
Epoch [24/120    avg_loss:0.357, val_acc:0.889]
Epoch [25/120    avg_loss:0.294, val_acc:0.892]
Epoch [26/120    avg_loss:0.244, val_acc:0.884]
Epoch [27/120    avg_loss:0.227, val_acc:0.895]
Epoch [28/120    avg_loss:0.214, val_acc:0.894]
Epoch [29/120    avg_loss:0.169, val_acc:0.896]
Epoch [30/120    avg_loss:0.179, val_acc:0.906]
Epoch [31/120    avg_loss:0.139, val_acc:0.906]
Epoch [32/120    avg_loss:0.238, val_acc:0.874]
Epoch [33/120    avg_loss:0.198, val_acc:0.877]
Epoch [34/120    avg_loss:0.183, val_acc:0.913]
Epoch [35/120    avg_loss:0.127, val_acc:0.926]
Epoch [36/120    avg_loss:0.150, val_acc:0.919]
Epoch [37/120    avg_loss:0.124, val_acc:0.927]
Epoch [38/120    avg_loss:0.144, val_acc:0.921]
Epoch [39/120    avg_loss:0.109, val_acc:0.938]
Epoch [40/120    avg_loss:0.158, val_acc:0.932]
Epoch [41/120    avg_loss:0.109, val_acc:0.933]
Epoch [42/120    avg_loss:0.113, val_acc:0.936]
Epoch [43/120    avg_loss:0.100, val_acc:0.938]
Epoch [44/120    avg_loss:0.080, val_acc:0.936]
Epoch [45/120    avg_loss:0.081, val_acc:0.944]
Epoch [46/120    avg_loss:0.078, val_acc:0.939]
Epoch [47/120    avg_loss:0.063, val_acc:0.949]
Epoch [48/120    avg_loss:0.077, val_acc:0.949]
Epoch [49/120    avg_loss:0.065, val_acc:0.957]
Epoch [50/120    avg_loss:0.087, val_acc:0.926]
Epoch [51/120    avg_loss:0.096, val_acc:0.936]
Epoch [52/120    avg_loss:0.077, val_acc:0.938]
Epoch [53/120    avg_loss:0.072, val_acc:0.946]
Epoch [54/120    avg_loss:0.056, val_acc:0.949]
Epoch [55/120    avg_loss:0.071, val_acc:0.948]
Epoch [56/120    avg_loss:0.069, val_acc:0.939]
Epoch [57/120    avg_loss:0.063, val_acc:0.948]
Epoch [58/120    avg_loss:0.044, val_acc:0.957]
Epoch [59/120    avg_loss:0.040, val_acc:0.958]
Epoch [60/120    avg_loss:0.051, val_acc:0.944]
Epoch [61/120    avg_loss:0.061, val_acc:0.942]
Epoch [62/120    avg_loss:0.044, val_acc:0.956]
Epoch [63/120    avg_loss:0.037, val_acc:0.954]
Epoch [64/120    avg_loss:0.037, val_acc:0.955]
Epoch [65/120    avg_loss:0.040, val_acc:0.955]
Epoch [66/120    avg_loss:0.034, val_acc:0.959]
Epoch [67/120    avg_loss:0.034, val_acc:0.958]
Epoch [68/120    avg_loss:0.064, val_acc:0.947]
Epoch [69/120    avg_loss:0.064, val_acc:0.940]
Epoch [70/120    avg_loss:0.061, val_acc:0.947]
Epoch [71/120    avg_loss:0.067, val_acc:0.950]
Epoch [72/120    avg_loss:0.047, val_acc:0.950]
Epoch [73/120    avg_loss:0.089, val_acc:0.895]
Epoch [74/120    avg_loss:0.159, val_acc:0.904]
Epoch [75/120    avg_loss:0.227, val_acc:0.924]
Epoch [76/120    avg_loss:0.115, val_acc:0.936]
Epoch [77/120    avg_loss:0.097, val_acc:0.927]
Epoch [78/120    avg_loss:0.081, val_acc:0.950]
Epoch [79/120    avg_loss:0.051, val_acc:0.947]
Epoch [80/120    avg_loss:0.046, val_acc:0.947]
Epoch [81/120    avg_loss:0.037, val_acc:0.951]
Epoch [82/120    avg_loss:0.042, val_acc:0.957]
Epoch [83/120    avg_loss:0.031, val_acc:0.959]
Epoch [84/120    avg_loss:0.030, val_acc:0.957]
Epoch [85/120    avg_loss:0.042, val_acc:0.958]
Epoch [86/120    avg_loss:0.032, val_acc:0.962]
Epoch [87/120    avg_loss:0.027, val_acc:0.959]
Epoch [88/120    avg_loss:0.030, val_acc:0.961]
Epoch [89/120    avg_loss:0.031, val_acc:0.962]
Epoch [90/120    avg_loss:0.037, val_acc:0.961]
Epoch [91/120    avg_loss:0.027, val_acc:0.959]
Epoch [92/120    avg_loss:0.029, val_acc:0.957]
Epoch [93/120    avg_loss:0.025, val_acc:0.959]
Epoch [94/120    avg_loss:0.033, val_acc:0.963]
Epoch [95/120    avg_loss:0.030, val_acc:0.964]
Epoch [96/120    avg_loss:0.033, val_acc:0.964]
Epoch [97/120    avg_loss:0.028, val_acc:0.966]
Epoch [98/120    avg_loss:0.034, val_acc:0.964]
Epoch [99/120    avg_loss:0.026, val_acc:0.961]
Epoch [100/120    avg_loss:0.026, val_acc:0.962]
Epoch [101/120    avg_loss:0.039, val_acc:0.966]
Epoch [102/120    avg_loss:0.023, val_acc:0.966]
Epoch [103/120    avg_loss:0.027, val_acc:0.966]
Epoch [104/120    avg_loss:0.024, val_acc:0.965]
Epoch [105/120    avg_loss:0.024, val_acc:0.968]
Epoch [106/120    avg_loss:0.024, val_acc:0.963]
Epoch [107/120    avg_loss:0.025, val_acc:0.965]
Epoch [108/120    avg_loss:0.024, val_acc:0.968]
Epoch [109/120    avg_loss:0.020, val_acc:0.966]
Epoch [110/120    avg_loss:0.023, val_acc:0.965]
Epoch [111/120    avg_loss:0.025, val_acc:0.966]
Epoch [112/120    avg_loss:0.034, val_acc:0.964]
Epoch [113/120    avg_loss:0.024, val_acc:0.965]
Epoch [114/120    avg_loss:0.023, val_acc:0.966]
Epoch [115/120    avg_loss:0.025, val_acc:0.965]
Epoch [116/120    avg_loss:0.029, val_acc:0.965]
Epoch [117/120    avg_loss:0.024, val_acc:0.966]
Epoch [118/120    avg_loss:0.024, val_acc:0.965]
Epoch [119/120    avg_loss:0.023, val_acc:0.964]
Epoch [120/120    avg_loss:0.020, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1251    0    4    0    6    0    0    0    5   19    0    0
     0    0    0]
 [   0    0    0  720    0   15    0    0    0    7    0    0    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   18   85    0    2    3    0    0    0  759    2    0    0
     3    3    0]
 [   0    0   16    0    0    1   11    0    0    0   10 2167    1    4
     0    0    0]
 [   0    0    0    0    5    6    0    0    0    0   11   11  492    0
     0    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    1    0    3    0    0    0
  1127    0    0]
 [   0    0    0    0    0    0   24    0    0    3    0    0    0    0
    97  223    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.56639566395664

F1 scores:
[       nan 0.96296296 0.97316219 0.92783505 0.97931034 0.95875139
 0.9660767  0.98039216 0.99767442 0.69387755 0.91225962 0.98254364
 0.95256534 0.98930481 0.95266272 0.77835951 0.94915254]

Kappa:
0.949431208738894
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f89790a27b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.537, val_acc:0.460]
Epoch [2/120    avg_loss:2.061, val_acc:0.567]
Epoch [3/120    avg_loss:1.786, val_acc:0.629]
Epoch [4/120    avg_loss:1.649, val_acc:0.648]
Epoch [5/120    avg_loss:1.446, val_acc:0.689]
Epoch [6/120    avg_loss:1.315, val_acc:0.750]
Epoch [7/120    avg_loss:1.066, val_acc:0.761]
Epoch [8/120    avg_loss:0.975, val_acc:0.749]
Epoch [9/120    avg_loss:0.784, val_acc:0.777]
Epoch [10/120    avg_loss:0.720, val_acc:0.786]
Epoch [11/120    avg_loss:0.587, val_acc:0.858]
Epoch [12/120    avg_loss:0.571, val_acc:0.819]
Epoch [13/120    avg_loss:0.476, val_acc:0.824]
Epoch [14/120    avg_loss:0.492, val_acc:0.826]
Epoch [15/120    avg_loss:0.509, val_acc:0.794]
Epoch [16/120    avg_loss:0.449, val_acc:0.843]
Epoch [17/120    avg_loss:0.344, val_acc:0.859]
Epoch [18/120    avg_loss:0.305, val_acc:0.871]
Epoch [19/120    avg_loss:0.281, val_acc:0.882]
Epoch [20/120    avg_loss:0.251, val_acc:0.891]
Epoch [21/120    avg_loss:0.276, val_acc:0.868]
Epoch [22/120    avg_loss:0.258, val_acc:0.879]
Epoch [23/120    avg_loss:0.276, val_acc:0.900]
Epoch [24/120    avg_loss:0.235, val_acc:0.904]
Epoch [25/120    avg_loss:0.216, val_acc:0.888]
Epoch [26/120    avg_loss:0.219, val_acc:0.922]
Epoch [27/120    avg_loss:0.193, val_acc:0.922]
Epoch [28/120    avg_loss:0.185, val_acc:0.924]
Epoch [29/120    avg_loss:0.225, val_acc:0.884]
Epoch [30/120    avg_loss:0.182, val_acc:0.919]
Epoch [31/120    avg_loss:0.156, val_acc:0.924]
Epoch [32/120    avg_loss:0.170, val_acc:0.930]
Epoch [33/120    avg_loss:0.157, val_acc:0.925]
Epoch [34/120    avg_loss:0.132, val_acc:0.938]
Epoch [35/120    avg_loss:0.183, val_acc:0.941]
Epoch [36/120    avg_loss:0.165, val_acc:0.930]
Epoch [37/120    avg_loss:0.134, val_acc:0.926]
Epoch [38/120    avg_loss:0.115, val_acc:0.924]
Epoch [39/120    avg_loss:0.117, val_acc:0.934]
Epoch [40/120    avg_loss:0.112, val_acc:0.939]
Epoch [41/120    avg_loss:0.118, val_acc:0.929]
Epoch [42/120    avg_loss:0.102, val_acc:0.948]
Epoch [43/120    avg_loss:0.087, val_acc:0.952]
Epoch [44/120    avg_loss:0.087, val_acc:0.944]
Epoch [45/120    avg_loss:0.078, val_acc:0.953]
Epoch [46/120    avg_loss:0.069, val_acc:0.948]
Epoch [47/120    avg_loss:0.078, val_acc:0.952]
Epoch [48/120    avg_loss:0.090, val_acc:0.936]
Epoch [49/120    avg_loss:0.108, val_acc:0.953]
Epoch [50/120    avg_loss:0.082, val_acc:0.931]
Epoch [51/120    avg_loss:0.103, val_acc:0.945]
Epoch [52/120    avg_loss:0.085, val_acc:0.915]
Epoch [53/120    avg_loss:0.102, val_acc:0.941]
Epoch [54/120    avg_loss:0.096, val_acc:0.944]
Epoch [55/120    avg_loss:0.115, val_acc:0.940]
Epoch [56/120    avg_loss:0.109, val_acc:0.941]
Epoch [57/120    avg_loss:0.106, val_acc:0.952]
Epoch [58/120    avg_loss:0.098, val_acc:0.935]
Epoch [59/120    avg_loss:0.081, val_acc:0.962]
Epoch [60/120    avg_loss:0.073, val_acc:0.961]
Epoch [61/120    avg_loss:0.064, val_acc:0.967]
Epoch [62/120    avg_loss:0.053, val_acc:0.961]
Epoch [63/120    avg_loss:0.068, val_acc:0.959]
Epoch [64/120    avg_loss:0.069, val_acc:0.950]
Epoch [65/120    avg_loss:0.050, val_acc:0.960]
Epoch [66/120    avg_loss:0.062, val_acc:0.949]
Epoch [67/120    avg_loss:0.058, val_acc:0.952]
Epoch [68/120    avg_loss:0.088, val_acc:0.923]
Epoch [69/120    avg_loss:0.181, val_acc:0.948]
Epoch [70/120    avg_loss:0.099, val_acc:0.961]
Epoch [71/120    avg_loss:0.059, val_acc:0.958]
Epoch [72/120    avg_loss:0.056, val_acc:0.958]
Epoch [73/120    avg_loss:0.050, val_acc:0.964]
Epoch [74/120    avg_loss:0.042, val_acc:0.967]
Epoch [75/120    avg_loss:0.035, val_acc:0.968]
Epoch [76/120    avg_loss:0.038, val_acc:0.973]
Epoch [77/120    avg_loss:0.111, val_acc:0.950]
Epoch [78/120    avg_loss:0.070, val_acc:0.950]
Epoch [79/120    avg_loss:0.080, val_acc:0.953]
Epoch [80/120    avg_loss:0.061, val_acc:0.969]
Epoch [81/120    avg_loss:0.041, val_acc:0.974]
Epoch [82/120    avg_loss:0.045, val_acc:0.970]
Epoch [83/120    avg_loss:0.040, val_acc:0.970]
Epoch [84/120    avg_loss:0.034, val_acc:0.970]
Epoch [85/120    avg_loss:0.037, val_acc:0.963]
Epoch [86/120    avg_loss:0.032, val_acc:0.973]
Epoch [87/120    avg_loss:0.018, val_acc:0.977]
Epoch [88/120    avg_loss:0.023, val_acc:0.967]
Epoch [89/120    avg_loss:0.028, val_acc:0.970]
Epoch [90/120    avg_loss:0.024, val_acc:0.972]
Epoch [91/120    avg_loss:0.026, val_acc:0.967]
Epoch [92/120    avg_loss:0.027, val_acc:0.973]
Epoch [93/120    avg_loss:0.027, val_acc:0.974]
Epoch [94/120    avg_loss:0.023, val_acc:0.958]
Epoch [95/120    avg_loss:0.027, val_acc:0.973]
Epoch [96/120    avg_loss:0.022, val_acc:0.972]
Epoch [97/120    avg_loss:0.020, val_acc:0.972]
Epoch [98/120    avg_loss:0.024, val_acc:0.974]
Epoch [99/120    avg_loss:0.023, val_acc:0.973]
Epoch [100/120    avg_loss:0.055, val_acc:0.932]
Epoch [101/120    avg_loss:0.045, val_acc:0.965]
Epoch [102/120    avg_loss:0.018, val_acc:0.968]
Epoch [103/120    avg_loss:0.027, val_acc:0.974]
Epoch [104/120    avg_loss:0.027, val_acc:0.974]
Epoch [105/120    avg_loss:0.015, val_acc:0.975]
Epoch [106/120    avg_loss:0.014, val_acc:0.977]
Epoch [107/120    avg_loss:0.014, val_acc:0.977]
Epoch [108/120    avg_loss:0.013, val_acc:0.978]
Epoch [109/120    avg_loss:0.013, val_acc:0.978]
Epoch [110/120    avg_loss:0.011, val_acc:0.978]
Epoch [111/120    avg_loss:0.015, val_acc:0.977]
Epoch [112/120    avg_loss:0.012, val_acc:0.977]
Epoch [113/120    avg_loss:0.013, val_acc:0.981]
Epoch [114/120    avg_loss:0.011, val_acc:0.981]
Epoch [115/120    avg_loss:0.011, val_acc:0.980]
Epoch [116/120    avg_loss:0.010, val_acc:0.980]
Epoch [117/120    avg_loss:0.010, val_acc:0.979]
Epoch [118/120    avg_loss:0.016, val_acc:0.980]
Epoch [119/120    avg_loss:0.008, val_acc:0.980]
Epoch [120/120    avg_loss:0.011, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    2 1243    1    0    0    2    0    0    0    5   12   15    0
     0    5    0]
 [   0    0    2  726    0   11    0    0    0    7    0    0    0    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    0    0    4    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   13    0    0    2    0
     0    0    0]
 [   0    0    0   84    0    6    2    0    0    0  777    0    6    0
     0    0    0]
 [   0    0    7    0    0    1   13    0    0    0    9 2171    5    2
     2    0    0]
 [   0    0    0   32    3    6    0    0    0    0   12    4  471    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    1    0    0
  1137    0    0]
 [   0    0    0    0    0    0   10    0    0    2    0    0    0    0
   105  230    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
95.64227642276423

F1 scores:
[       nan 0.97619048 0.97989752 0.91148776 0.99300699 0.96153846
 0.97834205 1.         1.         0.59090909 0.92555092 0.98681818
 0.90926641 0.9919571  0.9518627  0.79037801 0.94736842]

Kappa:
0.9503047544900497
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f72bf3b07b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.499, val_acc:0.532]
Epoch [2/120    avg_loss:2.023, val_acc:0.541]
Epoch [3/120    avg_loss:1.746, val_acc:0.578]
Epoch [4/120    avg_loss:1.596, val_acc:0.618]
Epoch [5/120    avg_loss:1.383, val_acc:0.641]
Epoch [6/120    avg_loss:1.256, val_acc:0.661]
Epoch [7/120    avg_loss:1.100, val_acc:0.655]
Epoch [8/120    avg_loss:0.969, val_acc:0.698]
Epoch [9/120    avg_loss:0.778, val_acc:0.765]
Epoch [10/120    avg_loss:0.663, val_acc:0.765]
Epoch [11/120    avg_loss:0.669, val_acc:0.778]
Epoch [12/120    avg_loss:0.547, val_acc:0.781]
Epoch [13/120    avg_loss:0.473, val_acc:0.814]
Epoch [14/120    avg_loss:0.478, val_acc:0.833]
Epoch [15/120    avg_loss:0.422, val_acc:0.844]
Epoch [16/120    avg_loss:0.378, val_acc:0.867]
Epoch [17/120    avg_loss:0.402, val_acc:0.837]
Epoch [18/120    avg_loss:0.356, val_acc:0.885]
Epoch [19/120    avg_loss:0.317, val_acc:0.875]
Epoch [20/120    avg_loss:0.312, val_acc:0.880]
Epoch [21/120    avg_loss:0.270, val_acc:0.900]
Epoch [22/120    avg_loss:0.221, val_acc:0.898]
Epoch [23/120    avg_loss:0.236, val_acc:0.917]
Epoch [24/120    avg_loss:0.246, val_acc:0.913]
Epoch [25/120    avg_loss:0.200, val_acc:0.925]
Epoch [26/120    avg_loss:0.184, val_acc:0.929]
Epoch [27/120    avg_loss:0.172, val_acc:0.925]
Epoch [28/120    avg_loss:0.149, val_acc:0.954]
Epoch [29/120    avg_loss:0.149, val_acc:0.921]
Epoch [30/120    avg_loss:0.139, val_acc:0.941]
Epoch [31/120    avg_loss:0.152, val_acc:0.946]
Epoch [32/120    avg_loss:0.136, val_acc:0.931]
Epoch [33/120    avg_loss:0.202, val_acc:0.922]
Epoch [34/120    avg_loss:0.189, val_acc:0.935]
Epoch [35/120    avg_loss:0.134, val_acc:0.938]
Epoch [36/120    avg_loss:0.162, val_acc:0.919]
Epoch [37/120    avg_loss:0.235, val_acc:0.927]
Epoch [38/120    avg_loss:0.246, val_acc:0.902]
Epoch [39/120    avg_loss:0.232, val_acc:0.917]
Epoch [40/120    avg_loss:0.131, val_acc:0.940]
Epoch [41/120    avg_loss:0.127, val_acc:0.940]
Epoch [42/120    avg_loss:0.096, val_acc:0.951]
Epoch [43/120    avg_loss:0.078, val_acc:0.955]
Epoch [44/120    avg_loss:0.066, val_acc:0.953]
Epoch [45/120    avg_loss:0.078, val_acc:0.957]
Epoch [46/120    avg_loss:0.065, val_acc:0.957]
Epoch [47/120    avg_loss:0.064, val_acc:0.958]
Epoch [48/120    avg_loss:0.062, val_acc:0.961]
Epoch [49/120    avg_loss:0.064, val_acc:0.962]
Epoch [50/120    avg_loss:0.059, val_acc:0.961]
Epoch [51/120    avg_loss:0.058, val_acc:0.962]
Epoch [52/120    avg_loss:0.068, val_acc:0.958]
Epoch [53/120    avg_loss:0.063, val_acc:0.959]
Epoch [54/120    avg_loss:0.072, val_acc:0.961]
Epoch [55/120    avg_loss:0.057, val_acc:0.963]
Epoch [56/120    avg_loss:0.067, val_acc:0.963]
Epoch [57/120    avg_loss:0.059, val_acc:0.963]
Epoch [58/120    avg_loss:0.056, val_acc:0.964]
Epoch [59/120    avg_loss:0.056, val_acc:0.964]
Epoch [60/120    avg_loss:0.055, val_acc:0.969]
Epoch [61/120    avg_loss:0.059, val_acc:0.965]
Epoch [62/120    avg_loss:0.054, val_acc:0.965]
Epoch [63/120    avg_loss:0.051, val_acc:0.964]
Epoch [64/120    avg_loss:0.062, val_acc:0.961]
Epoch [65/120    avg_loss:0.054, val_acc:0.965]
Epoch [66/120    avg_loss:0.058, val_acc:0.965]
Epoch [67/120    avg_loss:0.050, val_acc:0.966]
Epoch [68/120    avg_loss:0.052, val_acc:0.964]
Epoch [69/120    avg_loss:0.060, val_acc:0.965]
Epoch [70/120    avg_loss:0.063, val_acc:0.969]
Epoch [71/120    avg_loss:0.054, val_acc:0.968]
Epoch [72/120    avg_loss:0.054, val_acc:0.970]
Epoch [73/120    avg_loss:0.047, val_acc:0.969]
Epoch [74/120    avg_loss:0.050, val_acc:0.966]
Epoch [75/120    avg_loss:0.050, val_acc:0.969]
Epoch [76/120    avg_loss:0.041, val_acc:0.966]
Epoch [77/120    avg_loss:0.046, val_acc:0.970]
Epoch [78/120    avg_loss:0.053, val_acc:0.970]
Epoch [79/120    avg_loss:0.054, val_acc:0.969]
Epoch [80/120    avg_loss:0.040, val_acc:0.969]
Epoch [81/120    avg_loss:0.045, val_acc:0.969]
Epoch [82/120    avg_loss:0.046, val_acc:0.970]
Epoch [83/120    avg_loss:0.048, val_acc:0.968]
Epoch [84/120    avg_loss:0.049, val_acc:0.966]
Epoch [85/120    avg_loss:0.048, val_acc:0.968]
Epoch [86/120    avg_loss:0.040, val_acc:0.968]
Epoch [87/120    avg_loss:0.049, val_acc:0.971]
Epoch [88/120    avg_loss:0.045, val_acc:0.969]
Epoch [89/120    avg_loss:0.047, val_acc:0.970]
Epoch [90/120    avg_loss:0.045, val_acc:0.968]
Epoch [91/120    avg_loss:0.041, val_acc:0.970]
Epoch [92/120    avg_loss:0.047, val_acc:0.970]
Epoch [93/120    avg_loss:0.048, val_acc:0.970]
Epoch [94/120    avg_loss:0.044, val_acc:0.970]
Epoch [95/120    avg_loss:0.045, val_acc:0.970]
Epoch [96/120    avg_loss:0.044, val_acc:0.972]
Epoch [97/120    avg_loss:0.042, val_acc:0.968]
Epoch [98/120    avg_loss:0.050, val_acc:0.965]
Epoch [99/120    avg_loss:0.039, val_acc:0.970]
Epoch [100/120    avg_loss:0.040, val_acc:0.971]
Epoch [101/120    avg_loss:0.050, val_acc:0.966]
Epoch [102/120    avg_loss:0.039, val_acc:0.970]
Epoch [103/120    avg_loss:0.041, val_acc:0.970]
Epoch [104/120    avg_loss:0.044, val_acc:0.970]
Epoch [105/120    avg_loss:0.038, val_acc:0.971]
Epoch [106/120    avg_loss:0.040, val_acc:0.971]
Epoch [107/120    avg_loss:0.039, val_acc:0.970]
Epoch [108/120    avg_loss:0.045, val_acc:0.971]
Epoch [109/120    avg_loss:0.043, val_acc:0.970]
Epoch [110/120    avg_loss:0.038, val_acc:0.970]
Epoch [111/120    avg_loss:0.037, val_acc:0.969]
Epoch [112/120    avg_loss:0.037, val_acc:0.969]
Epoch [113/120    avg_loss:0.035, val_acc:0.970]
Epoch [114/120    avg_loss:0.039, val_acc:0.971]
Epoch [115/120    avg_loss:0.038, val_acc:0.971]
Epoch [116/120    avg_loss:0.041, val_acc:0.970]
Epoch [117/120    avg_loss:0.035, val_acc:0.970]
Epoch [118/120    avg_loss:0.036, val_acc:0.970]
Epoch [119/120    avg_loss:0.042, val_acc:0.969]
Epoch [120/120    avg_loss:0.034, val_acc:0.969]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1222    3    0    0    8    0    0    0   10   26   12    0
     0    4    0]
 [   0    0    2  706    2   18    0    0    0    5    0    0    7    6
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    0    0    4    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    4    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   18   80    0    6    0    0    0    0  746    9    0    1
     3   12    0]
 [   0    0   19    0    0    0   13    0    5    0   11 2153    1    5
     3    0    0]
 [   0    0    0   10    5    2    0    0    0    0   22   18  468    0
     0    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    3    1    0    0
  1133    0    0]
 [   0    0    0    0    0    0    8    0    0    1    0    0    0    0
   115  223    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
94.48238482384824

F1 scores:
[       nan 0.975      0.95956027 0.91273432 0.98383372 0.95701357
 0.97382199 1.         0.99192618 0.75555556 0.89448441 0.97398779
 0.9140625  0.96858639 0.94259567 0.76109215 0.93714286]

Kappa:
0.9370506377230468
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8caddd2748>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.591, val_acc:0.389]
Epoch [2/120    avg_loss:2.045, val_acc:0.529]
Epoch [3/120    avg_loss:1.757, val_acc:0.544]
Epoch [4/120    avg_loss:1.577, val_acc:0.622]
Epoch [5/120    avg_loss:1.421, val_acc:0.586]
Epoch [6/120    avg_loss:1.215, val_acc:0.672]
Epoch [7/120    avg_loss:1.041, val_acc:0.717]
Epoch [8/120    avg_loss:0.865, val_acc:0.764]
Epoch [9/120    avg_loss:0.776, val_acc:0.770]
Epoch [10/120    avg_loss:0.616, val_acc:0.830]
Epoch [11/120    avg_loss:0.515, val_acc:0.854]
Epoch [12/120    avg_loss:0.415, val_acc:0.863]
Epoch [13/120    avg_loss:0.478, val_acc:0.850]
Epoch [14/120    avg_loss:0.425, val_acc:0.868]
Epoch [15/120    avg_loss:0.315, val_acc:0.914]
Epoch [16/120    avg_loss:0.239, val_acc:0.863]
Epoch [17/120    avg_loss:0.243, val_acc:0.886]
Epoch [18/120    avg_loss:0.265, val_acc:0.915]
Epoch [19/120    avg_loss:0.190, val_acc:0.927]
Epoch [20/120    avg_loss:0.185, val_acc:0.907]
Epoch [21/120    avg_loss:0.145, val_acc:0.945]
Epoch [22/120    avg_loss:0.122, val_acc:0.939]
Epoch [23/120    avg_loss:0.131, val_acc:0.905]
Epoch [24/120    avg_loss:0.108, val_acc:0.929]
Epoch [25/120    avg_loss:0.100, val_acc:0.951]
Epoch [26/120    avg_loss:0.096, val_acc:0.955]
Epoch [27/120    avg_loss:0.077, val_acc:0.955]
Epoch [28/120    avg_loss:0.067, val_acc:0.939]
Epoch [29/120    avg_loss:0.061, val_acc:0.959]
Epoch [30/120    avg_loss:0.070, val_acc:0.941]
Epoch [31/120    avg_loss:0.075, val_acc:0.970]
Epoch [32/120    avg_loss:0.056, val_acc:0.954]
Epoch [33/120    avg_loss:0.064, val_acc:0.936]
Epoch [34/120    avg_loss:0.135, val_acc:0.949]
Epoch [35/120    avg_loss:0.083, val_acc:0.947]
Epoch [36/120    avg_loss:0.085, val_acc:0.958]
Epoch [37/120    avg_loss:0.068, val_acc:0.956]
Epoch [38/120    avg_loss:0.072, val_acc:0.945]
Epoch [39/120    avg_loss:0.049, val_acc:0.965]
Epoch [40/120    avg_loss:0.041, val_acc:0.970]
Epoch [41/120    avg_loss:0.031, val_acc:0.965]
Epoch [42/120    avg_loss:0.033, val_acc:0.969]
Epoch [43/120    avg_loss:0.030, val_acc:0.974]
Epoch [44/120    avg_loss:0.035, val_acc:0.963]
Epoch [45/120    avg_loss:0.038, val_acc:0.970]
Epoch [46/120    avg_loss:0.031, val_acc:0.968]
Epoch [47/120    avg_loss:0.028, val_acc:0.964]
Epoch [48/120    avg_loss:0.038, val_acc:0.972]
Epoch [49/120    avg_loss:0.031, val_acc:0.964]
Epoch [50/120    avg_loss:0.037, val_acc:0.965]
Epoch [51/120    avg_loss:0.043, val_acc:0.951]
Epoch [52/120    avg_loss:0.028, val_acc:0.967]
Epoch [53/120    avg_loss:0.030, val_acc:0.980]
Epoch [54/120    avg_loss:0.021, val_acc:0.973]
Epoch [55/120    avg_loss:0.015, val_acc:0.981]
Epoch [56/120    avg_loss:0.015, val_acc:0.974]
Epoch [57/120    avg_loss:0.016, val_acc:0.974]
Epoch [58/120    avg_loss:0.011, val_acc:0.970]
Epoch [59/120    avg_loss:0.010, val_acc:0.972]
Epoch [60/120    avg_loss:0.015, val_acc:0.971]
Epoch [61/120    avg_loss:0.020, val_acc:0.970]
Epoch [62/120    avg_loss:0.016, val_acc:0.975]
Epoch [63/120    avg_loss:0.022, val_acc:0.963]
Epoch [64/120    avg_loss:0.015, val_acc:0.973]
Epoch [65/120    avg_loss:0.015, val_acc:0.972]
Epoch [66/120    avg_loss:0.010, val_acc:0.971]
Epoch [67/120    avg_loss:0.008, val_acc:0.974]
Epoch [68/120    avg_loss:0.008, val_acc:0.974]
Epoch [69/120    avg_loss:0.008, val_acc:0.973]
Epoch [70/120    avg_loss:0.009, val_acc:0.972]
Epoch [71/120    avg_loss:0.008, val_acc:0.976]
Epoch [72/120    avg_loss:0.007, val_acc:0.977]
Epoch [73/120    avg_loss:0.010, val_acc:0.978]
Epoch [74/120    avg_loss:0.007, val_acc:0.978]
Epoch [75/120    avg_loss:0.006, val_acc:0.977]
Epoch [76/120    avg_loss:0.007, val_acc:0.976]
Epoch [77/120    avg_loss:0.006, val_acc:0.975]
Epoch [78/120    avg_loss:0.006, val_acc:0.976]
Epoch [79/120    avg_loss:0.008, val_acc:0.979]
Epoch [80/120    avg_loss:0.006, val_acc:0.979]
Epoch [81/120    avg_loss:0.008, val_acc:0.977]
Epoch [82/120    avg_loss:0.006, val_acc:0.977]
Epoch [83/120    avg_loss:0.007, val_acc:0.976]
Epoch [84/120    avg_loss:0.007, val_acc:0.976]
Epoch [85/120    avg_loss:0.009, val_acc:0.976]
Epoch [86/120    avg_loss:0.007, val_acc:0.977]
Epoch [87/120    avg_loss:0.006, val_acc:0.977]
Epoch [88/120    avg_loss:0.007, val_acc:0.977]
Epoch [89/120    avg_loss:0.007, val_acc:0.977]
Epoch [90/120    avg_loss:0.007, val_acc:0.977]
Epoch [91/120    avg_loss:0.006, val_acc:0.977]
Epoch [92/120    avg_loss:0.006, val_acc:0.977]
Epoch [93/120    avg_loss:0.006, val_acc:0.977]
Epoch [94/120    avg_loss:0.007, val_acc:0.977]
Epoch [95/120    avg_loss:0.006, val_acc:0.977]
Epoch [96/120    avg_loss:0.006, val_acc:0.977]
Epoch [97/120    avg_loss:0.007, val_acc:0.977]
Epoch [98/120    avg_loss:0.006, val_acc:0.977]
Epoch [99/120    avg_loss:0.007, val_acc:0.977]
Epoch [100/120    avg_loss:0.006, val_acc:0.977]
Epoch [101/120    avg_loss:0.007, val_acc:0.977]
Epoch [102/120    avg_loss:0.007, val_acc:0.977]
Epoch [103/120    avg_loss:0.006, val_acc:0.977]
Epoch [104/120    avg_loss:0.006, val_acc:0.977]
Epoch [105/120    avg_loss:0.006, val_acc:0.977]
Epoch [106/120    avg_loss:0.007, val_acc:0.977]
Epoch [107/120    avg_loss:0.006, val_acc:0.977]
Epoch [108/120    avg_loss:0.006, val_acc:0.977]
Epoch [109/120    avg_loss:0.006, val_acc:0.977]
Epoch [110/120    avg_loss:0.006, val_acc:0.977]
Epoch [111/120    avg_loss:0.008, val_acc:0.977]
Epoch [112/120    avg_loss:0.006, val_acc:0.977]
Epoch [113/120    avg_loss:0.005, val_acc:0.977]
Epoch [114/120    avg_loss:0.006, val_acc:0.977]
Epoch [115/120    avg_loss:0.008, val_acc:0.977]
Epoch [116/120    avg_loss:0.007, val_acc:0.977]
Epoch [117/120    avg_loss:0.007, val_acc:0.977]
Epoch [118/120    avg_loss:0.006, val_acc:0.977]
Epoch [119/120    avg_loss:0.006, val_acc:0.977]
Epoch [120/120    avg_loss:0.005, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    2    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1247    7    6    0    0    0    0    0    6   19    0    0
     0    0    0]
 [   0    0    0  723    0    1    0    0    0    1    0   11   10    1
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    2    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  845   24    0    0
     0    0    0]
 [   0    0   12    3    0    0    4    0    0    1    4 2171   12    0
     0    3    0]
 [   0    0    0    6    3    0    0    0    0    0    0    0  523    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
  1122   15    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    60  275    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.37669376693766

F1 scores:
[       nan 0.975      0.97803922 0.97242771 0.97695853 0.99078341
 0.98496241 0.96153846 1.         0.94736842 0.97687861 0.97880974
 0.96851852 0.99730458 0.96516129 0.859375   0.98224852]

Kappa:
0.9700759904569694
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2271e2b6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.569, val_acc:0.359]
Epoch [2/120    avg_loss:2.040, val_acc:0.486]
Epoch [3/120    avg_loss:1.762, val_acc:0.629]
Epoch [4/120    avg_loss:1.548, val_acc:0.648]
Epoch [5/120    avg_loss:1.412, val_acc:0.631]
Epoch [6/120    avg_loss:1.165, val_acc:0.715]
Epoch [7/120    avg_loss:0.941, val_acc:0.745]
Epoch [8/120    avg_loss:0.856, val_acc:0.732]
Epoch [9/120    avg_loss:0.694, val_acc:0.725]
Epoch [10/120    avg_loss:0.595, val_acc:0.804]
Epoch [11/120    avg_loss:0.575, val_acc:0.809]
Epoch [12/120    avg_loss:0.454, val_acc:0.835]
Epoch [13/120    avg_loss:0.519, val_acc:0.847]
Epoch [14/120    avg_loss:0.372, val_acc:0.831]
Epoch [15/120    avg_loss:0.335, val_acc:0.836]
Epoch [16/120    avg_loss:0.267, val_acc:0.871]
Epoch [17/120    avg_loss:0.225, val_acc:0.895]
Epoch [18/120    avg_loss:0.191, val_acc:0.906]
Epoch [19/120    avg_loss:0.168, val_acc:0.926]
Epoch [20/120    avg_loss:0.155, val_acc:0.906]
Epoch [21/120    avg_loss:0.200, val_acc:0.907]
Epoch [22/120    avg_loss:0.167, val_acc:0.932]
Epoch [23/120    avg_loss:0.116, val_acc:0.932]
Epoch [24/120    avg_loss:0.116, val_acc:0.936]
Epoch [25/120    avg_loss:0.115, val_acc:0.921]
Epoch [26/120    avg_loss:0.109, val_acc:0.932]
Epoch [27/120    avg_loss:0.109, val_acc:0.923]
Epoch [28/120    avg_loss:0.123, val_acc:0.948]
Epoch [29/120    avg_loss:0.089, val_acc:0.945]
Epoch [30/120    avg_loss:0.073, val_acc:0.947]
Epoch [31/120    avg_loss:0.088, val_acc:0.941]
Epoch [32/120    avg_loss:0.073, val_acc:0.950]
Epoch [33/120    avg_loss:0.067, val_acc:0.949]
Epoch [34/120    avg_loss:0.062, val_acc:0.945]
Epoch [35/120    avg_loss:0.051, val_acc:0.967]
Epoch [36/120    avg_loss:0.051, val_acc:0.964]
Epoch [37/120    avg_loss:0.035, val_acc:0.964]
Epoch [38/120    avg_loss:0.033, val_acc:0.966]
Epoch [39/120    avg_loss:0.030, val_acc:0.967]
Epoch [40/120    avg_loss:0.032, val_acc:0.967]
Epoch [41/120    avg_loss:0.042, val_acc:0.968]
Epoch [42/120    avg_loss:0.039, val_acc:0.957]
Epoch [43/120    avg_loss:0.028, val_acc:0.966]
Epoch [44/120    avg_loss:0.033, val_acc:0.961]
Epoch [45/120    avg_loss:0.026, val_acc:0.972]
Epoch [46/120    avg_loss:0.020, val_acc:0.966]
Epoch [47/120    avg_loss:0.018, val_acc:0.972]
Epoch [48/120    avg_loss:0.018, val_acc:0.966]
Epoch [49/120    avg_loss:0.024, val_acc:0.969]
Epoch [50/120    avg_loss:0.024, val_acc:0.972]
Epoch [51/120    avg_loss:0.018, val_acc:0.975]
Epoch [52/120    avg_loss:0.022, val_acc:0.971]
Epoch [53/120    avg_loss:0.018, val_acc:0.968]
Epoch [54/120    avg_loss:0.021, val_acc:0.968]
Epoch [55/120    avg_loss:0.024, val_acc:0.967]
Epoch [56/120    avg_loss:0.027, val_acc:0.956]
Epoch [57/120    avg_loss:0.099, val_acc:0.959]
Epoch [58/120    avg_loss:0.059, val_acc:0.968]
Epoch [59/120    avg_loss:0.045, val_acc:0.969]
Epoch [60/120    avg_loss:0.042, val_acc:0.960]
Epoch [61/120    avg_loss:0.041, val_acc:0.961]
Epoch [62/120    avg_loss:0.033, val_acc:0.964]
Epoch [63/120    avg_loss:0.020, val_acc:0.978]
Epoch [64/120    avg_loss:0.017, val_acc:0.970]
Epoch [65/120    avg_loss:0.037, val_acc:0.965]
Epoch [66/120    avg_loss:0.051, val_acc:0.945]
Epoch [67/120    avg_loss:0.042, val_acc:0.952]
Epoch [68/120    avg_loss:0.021, val_acc:0.975]
Epoch [69/120    avg_loss:0.018, val_acc:0.973]
Epoch [70/120    avg_loss:0.021, val_acc:0.966]
Epoch [71/120    avg_loss:0.021, val_acc:0.970]
Epoch [72/120    avg_loss:0.013, val_acc:0.973]
Epoch [73/120    avg_loss:0.012, val_acc:0.972]
Epoch [74/120    avg_loss:0.011, val_acc:0.977]
Epoch [75/120    avg_loss:0.007, val_acc:0.980]
Epoch [76/120    avg_loss:0.010, val_acc:0.976]
Epoch [77/120    avg_loss:0.009, val_acc:0.969]
Epoch [78/120    avg_loss:0.008, val_acc:0.975]
Epoch [79/120    avg_loss:0.007, val_acc:0.976]
Epoch [80/120    avg_loss:0.009, val_acc:0.979]
Epoch [81/120    avg_loss:0.007, val_acc:0.979]
Epoch [82/120    avg_loss:0.006, val_acc:0.975]
Epoch [83/120    avg_loss:0.007, val_acc:0.976]
Epoch [84/120    avg_loss:0.008, val_acc:0.976]
Epoch [85/120    avg_loss:0.006, val_acc:0.980]
Epoch [86/120    avg_loss:0.005, val_acc:0.979]
Epoch [87/120    avg_loss:0.005, val_acc:0.983]
Epoch [88/120    avg_loss:0.005, val_acc:0.984]
Epoch [89/120    avg_loss:0.005, val_acc:0.979]
Epoch [90/120    avg_loss:0.005, val_acc:0.980]
Epoch [91/120    avg_loss:0.006, val_acc:0.978]
Epoch [92/120    avg_loss:0.009, val_acc:0.973]
Epoch [93/120    avg_loss:0.011, val_acc:0.969]
Epoch [94/120    avg_loss:0.010, val_acc:0.976]
Epoch [95/120    avg_loss:0.007, val_acc:0.974]
Epoch [96/120    avg_loss:0.008, val_acc:0.972]
Epoch [97/120    avg_loss:0.005, val_acc:0.979]
Epoch [98/120    avg_loss:0.008, val_acc:0.981]
Epoch [99/120    avg_loss:0.004, val_acc:0.978]
Epoch [100/120    avg_loss:0.006, val_acc:0.980]
Epoch [101/120    avg_loss:0.005, val_acc:0.979]
Epoch [102/120    avg_loss:0.004, val_acc:0.978]
Epoch [103/120    avg_loss:0.003, val_acc:0.980]
Epoch [104/120    avg_loss:0.004, val_acc:0.979]
Epoch [105/120    avg_loss:0.003, val_acc:0.979]
Epoch [106/120    avg_loss:0.004, val_acc:0.979]
Epoch [107/120    avg_loss:0.004, val_acc:0.979]
Epoch [108/120    avg_loss:0.004, val_acc:0.980]
Epoch [109/120    avg_loss:0.004, val_acc:0.980]
Epoch [110/120    avg_loss:0.004, val_acc:0.981]
Epoch [111/120    avg_loss:0.003, val_acc:0.980]
Epoch [112/120    avg_loss:0.004, val_acc:0.978]
Epoch [113/120    avg_loss:0.003, val_acc:0.978]
Epoch [114/120    avg_loss:0.003, val_acc:0.978]
Epoch [115/120    avg_loss:0.003, val_acc:0.978]
Epoch [116/120    avg_loss:0.005, val_acc:0.979]
Epoch [117/120    avg_loss:0.003, val_acc:0.978]
Epoch [118/120    avg_loss:0.004, val_acc:0.978]
Epoch [119/120    avg_loss:0.003, val_acc:0.978]
Epoch [120/120    avg_loss:0.003, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1253    1    0    0    0    0    0    0    4   25    2    0
     0    0    0]
 [   0    0    0  726    1    0    1    0    0    2    0    8    8    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    1    0    1    0    0
     6    0    0]
 [   0    0    0    0    0    0  653    0    0    2    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    0    0    0    0    0  851   13    0    0
     0    1    0]
 [   0    0   21    0    0    0    3    0    0    0    2 2183    1    0
     0    0    0]
 [   0    0    0    3    0    1    0    0    0    0    0    1  525    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1129   10    0]
 [   0    0    0    0    0    1   11    0    0    0    0    0    0    0
    47  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.8319783197832

F1 scores:
[       nan 0.98765432 0.97509728 0.9830738  0.99765808 0.98842593
 0.98566038 1.         0.99883586 0.87804878 0.98267898 0.98289059
 0.97674419 0.99730458 0.97160069 0.89164087 0.96385542]

Kappa:
0.975261335829136
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f417bece7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.548, val_acc:0.475]
Epoch [2/120    avg_loss:2.011, val_acc:0.506]
Epoch [3/120    avg_loss:1.785, val_acc:0.554]
Epoch [4/120    avg_loss:1.526, val_acc:0.616]
Epoch [5/120    avg_loss:1.364, val_acc:0.648]
Epoch [6/120    avg_loss:1.148, val_acc:0.715]
Epoch [7/120    avg_loss:0.973, val_acc:0.718]
Epoch [8/120    avg_loss:0.805, val_acc:0.755]
Epoch [9/120    avg_loss:0.653, val_acc:0.825]
Epoch [10/120    avg_loss:0.611, val_acc:0.804]
Epoch [11/120    avg_loss:0.574, val_acc:0.799]
Epoch [12/120    avg_loss:0.538, val_acc:0.833]
Epoch [13/120    avg_loss:0.408, val_acc:0.828]
Epoch [14/120    avg_loss:0.327, val_acc:0.872]
Epoch [15/120    avg_loss:0.291, val_acc:0.854]
Epoch [16/120    avg_loss:0.275, val_acc:0.881]
Epoch [17/120    avg_loss:0.242, val_acc:0.886]
Epoch [18/120    avg_loss:0.190, val_acc:0.906]
Epoch [19/120    avg_loss:0.177, val_acc:0.861]
Epoch [20/120    avg_loss:0.166, val_acc:0.895]
Epoch [21/120    avg_loss:0.178, val_acc:0.887]
Epoch [22/120    avg_loss:0.157, val_acc:0.916]
Epoch [23/120    avg_loss:0.132, val_acc:0.934]
Epoch [24/120    avg_loss:0.107, val_acc:0.927]
Epoch [25/120    avg_loss:0.113, val_acc:0.910]
Epoch [26/120    avg_loss:0.105, val_acc:0.949]
Epoch [27/120    avg_loss:0.095, val_acc:0.949]
Epoch [28/120    avg_loss:0.070, val_acc:0.954]
Epoch [29/120    avg_loss:0.080, val_acc:0.928]
Epoch [30/120    avg_loss:0.062, val_acc:0.961]
Epoch [31/120    avg_loss:0.058, val_acc:0.952]
Epoch [32/120    avg_loss:0.052, val_acc:0.947]
Epoch [33/120    avg_loss:0.045, val_acc:0.967]
Epoch [34/120    avg_loss:0.050, val_acc:0.966]
Epoch [35/120    avg_loss:0.065, val_acc:0.955]
Epoch [36/120    avg_loss:0.057, val_acc:0.961]
Epoch [37/120    avg_loss:0.056, val_acc:0.964]
Epoch [38/120    avg_loss:0.037, val_acc:0.971]
Epoch [39/120    avg_loss:0.028, val_acc:0.968]
Epoch [40/120    avg_loss:0.034, val_acc:0.965]
Epoch [41/120    avg_loss:0.029, val_acc:0.970]
Epoch [42/120    avg_loss:0.027, val_acc:0.970]
Epoch [43/120    avg_loss:0.031, val_acc:0.968]
Epoch [44/120    avg_loss:0.022, val_acc:0.974]
Epoch [45/120    avg_loss:0.020, val_acc:0.970]
Epoch [46/120    avg_loss:0.025, val_acc:0.972]
Epoch [47/120    avg_loss:0.036, val_acc:0.974]
Epoch [48/120    avg_loss:0.030, val_acc:0.967]
Epoch [49/120    avg_loss:0.017, val_acc:0.979]
Epoch [50/120    avg_loss:0.012, val_acc:0.973]
Epoch [51/120    avg_loss:0.016, val_acc:0.967]
Epoch [52/120    avg_loss:0.027, val_acc:0.971]
Epoch [53/120    avg_loss:0.029, val_acc:0.967]
Epoch [54/120    avg_loss:0.023, val_acc:0.968]
Epoch [55/120    avg_loss:0.024, val_acc:0.966]
Epoch [56/120    avg_loss:0.020, val_acc:0.975]
Epoch [57/120    avg_loss:0.017, val_acc:0.972]
Epoch [58/120    avg_loss:0.015, val_acc:0.979]
Epoch [59/120    avg_loss:0.017, val_acc:0.974]
Epoch [60/120    avg_loss:0.010, val_acc:0.976]
Epoch [61/120    avg_loss:0.014, val_acc:0.973]
Epoch [62/120    avg_loss:0.013, val_acc:0.979]
Epoch [63/120    avg_loss:0.014, val_acc:0.970]
Epoch [64/120    avg_loss:0.012, val_acc:0.971]
Epoch [65/120    avg_loss:0.008, val_acc:0.978]
Epoch [66/120    avg_loss:0.010, val_acc:0.975]
Epoch [67/120    avg_loss:0.008, val_acc:0.980]
Epoch [68/120    avg_loss:0.010, val_acc:0.977]
Epoch [69/120    avg_loss:0.015, val_acc:0.973]
Epoch [70/120    avg_loss:0.018, val_acc:0.972]
Epoch [71/120    avg_loss:0.015, val_acc:0.982]
Epoch [72/120    avg_loss:0.009, val_acc:0.972]
Epoch [73/120    avg_loss:0.009, val_acc:0.976]
Epoch [74/120    avg_loss:0.008, val_acc:0.977]
Epoch [75/120    avg_loss:0.007, val_acc:0.978]
Epoch [76/120    avg_loss:0.007, val_acc:0.969]
Epoch [77/120    avg_loss:0.007, val_acc:0.979]
Epoch [78/120    avg_loss:0.007, val_acc:0.977]
Epoch [79/120    avg_loss:0.005, val_acc:0.978]
Epoch [80/120    avg_loss:0.006, val_acc:0.977]
Epoch [81/120    avg_loss:0.006, val_acc:0.979]
Epoch [82/120    avg_loss:0.005, val_acc:0.982]
Epoch [83/120    avg_loss:0.005, val_acc:0.979]
Epoch [84/120    avg_loss:0.004, val_acc:0.983]
Epoch [85/120    avg_loss:0.004, val_acc:0.979]
Epoch [86/120    avg_loss:0.004, val_acc:0.980]
Epoch [87/120    avg_loss:0.004, val_acc:0.978]
Epoch [88/120    avg_loss:0.004, val_acc:0.983]
Epoch [89/120    avg_loss:0.004, val_acc:0.969]
Epoch [90/120    avg_loss:0.004, val_acc:0.979]
Epoch [91/120    avg_loss:0.004, val_acc:0.981]
Epoch [92/120    avg_loss:0.003, val_acc:0.979]
Epoch [93/120    avg_loss:0.003, val_acc:0.980]
Epoch [94/120    avg_loss:0.004, val_acc:0.981]
Epoch [95/120    avg_loss:0.004, val_acc:0.982]
Epoch [96/120    avg_loss:0.005, val_acc:0.975]
Epoch [97/120    avg_loss:0.004, val_acc:0.980]
Epoch [98/120    avg_loss:0.005, val_acc:0.976]
Epoch [99/120    avg_loss:0.004, val_acc:0.977]
Epoch [100/120    avg_loss:0.006, val_acc:0.976]
Epoch [101/120    avg_loss:0.021, val_acc:0.926]
Epoch [102/120    avg_loss:0.053, val_acc:0.974]
Epoch [103/120    avg_loss:0.007, val_acc:0.978]
Epoch [104/120    avg_loss:0.009, val_acc:0.976]
Epoch [105/120    avg_loss:0.007, val_acc:0.976]
Epoch [106/120    avg_loss:0.005, val_acc:0.979]
Epoch [107/120    avg_loss:0.005, val_acc:0.979]
Epoch [108/120    avg_loss:0.005, val_acc:0.981]
Epoch [109/120    avg_loss:0.005, val_acc:0.981]
Epoch [110/120    avg_loss:0.005, val_acc:0.980]
Epoch [111/120    avg_loss:0.004, val_acc:0.980]
Epoch [112/120    avg_loss:0.004, val_acc:0.979]
Epoch [113/120    avg_loss:0.004, val_acc:0.981]
Epoch [114/120    avg_loss:0.005, val_acc:0.982]
Epoch [115/120    avg_loss:0.005, val_acc:0.982]
Epoch [116/120    avg_loss:0.004, val_acc:0.982]
Epoch [117/120    avg_loss:0.005, val_acc:0.982]
Epoch [118/120    avg_loss:0.005, val_acc:0.982]
Epoch [119/120    avg_loss:0.004, val_acc:0.982]
Epoch [120/120    avg_loss:0.004, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    1    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1247    9    0    3    1    0    0    0    4   18    2    1
     0    0    0]
 [   0    0    0  729    3    0    0    0    0    5    1    5    2    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    3    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0  854   18    0    0
     0    1    0]
 [   0    0   24    0    0    0    1    0    0    3    8 2168    5    0
     0    1    0]
 [   0    0    0    6    0    0    0    0    0    0    0    0  525    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1134    5    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    37  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.03794037940379

F1 scores:
[       nan 0.98765432 0.97498045 0.9778672  0.99300699 0.99196326
 0.99771863 0.94339623 0.997669   0.81818182 0.9804822  0.98121747
 0.97765363 0.9919571  0.98139334 0.93093093 0.96969697]

Kappa:
0.9776290737031735
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f915c9c66a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.574, val_acc:0.360]
Epoch [2/120    avg_loss:1.993, val_acc:0.499]
Epoch [3/120    avg_loss:1.821, val_acc:0.593]
Epoch [4/120    avg_loss:1.558, val_acc:0.590]
Epoch [5/120    avg_loss:1.350, val_acc:0.598]
Epoch [6/120    avg_loss:1.274, val_acc:0.671]
Epoch [7/120    avg_loss:1.100, val_acc:0.728]
Epoch [8/120    avg_loss:1.014, val_acc:0.741]
Epoch [9/120    avg_loss:0.865, val_acc:0.773]
Epoch [10/120    avg_loss:0.717, val_acc:0.762]
Epoch [11/120    avg_loss:0.624, val_acc:0.812]
Epoch [12/120    avg_loss:0.555, val_acc:0.817]
Epoch [13/120    avg_loss:0.435, val_acc:0.856]
Epoch [14/120    avg_loss:0.371, val_acc:0.852]
Epoch [15/120    avg_loss:0.334, val_acc:0.866]
Epoch [16/120    avg_loss:0.250, val_acc:0.901]
Epoch [17/120    avg_loss:0.259, val_acc:0.905]
Epoch [18/120    avg_loss:0.176, val_acc:0.924]
Epoch [19/120    avg_loss:0.155, val_acc:0.915]
Epoch [20/120    avg_loss:0.321, val_acc:0.834]
Epoch [21/120    avg_loss:0.473, val_acc:0.795]
Epoch [22/120    avg_loss:0.306, val_acc:0.896]
Epoch [23/120    avg_loss:0.230, val_acc:0.906]
Epoch [24/120    avg_loss:0.209, val_acc:0.916]
Epoch [25/120    avg_loss:0.182, val_acc:0.916]
Epoch [26/120    avg_loss:0.139, val_acc:0.940]
Epoch [27/120    avg_loss:0.114, val_acc:0.939]
Epoch [28/120    avg_loss:0.159, val_acc:0.928]
Epoch [29/120    avg_loss:0.129, val_acc:0.944]
Epoch [30/120    avg_loss:0.096, val_acc:0.927]
Epoch [31/120    avg_loss:0.083, val_acc:0.952]
Epoch [32/120    avg_loss:0.074, val_acc:0.947]
Epoch [33/120    avg_loss:0.072, val_acc:0.944]
Epoch [34/120    avg_loss:0.085, val_acc:0.948]
Epoch [35/120    avg_loss:0.058, val_acc:0.949]
Epoch [36/120    avg_loss:0.056, val_acc:0.958]
Epoch [37/120    avg_loss:0.043, val_acc:0.950]
Epoch [38/120    avg_loss:0.051, val_acc:0.961]
Epoch [39/120    avg_loss:0.059, val_acc:0.956]
Epoch [40/120    avg_loss:0.046, val_acc:0.953]
Epoch [41/120    avg_loss:0.037, val_acc:0.965]
Epoch [42/120    avg_loss:0.037, val_acc:0.961]
Epoch [43/120    avg_loss:0.058, val_acc:0.948]
Epoch [44/120    avg_loss:0.037, val_acc:0.958]
Epoch [45/120    avg_loss:0.031, val_acc:0.965]
Epoch [46/120    avg_loss:0.031, val_acc:0.969]
Epoch [47/120    avg_loss:0.022, val_acc:0.967]
Epoch [48/120    avg_loss:0.021, val_acc:0.963]
Epoch [49/120    avg_loss:0.032, val_acc:0.972]
Epoch [50/120    avg_loss:0.027, val_acc:0.959]
Epoch [51/120    avg_loss:0.019, val_acc:0.969]
Epoch [52/120    avg_loss:0.020, val_acc:0.964]
Epoch [53/120    avg_loss:0.025, val_acc:0.970]
Epoch [54/120    avg_loss:0.017, val_acc:0.972]
Epoch [55/120    avg_loss:0.017, val_acc:0.968]
Epoch [56/120    avg_loss:0.017, val_acc:0.973]
Epoch [57/120    avg_loss:0.014, val_acc:0.971]
Epoch [58/120    avg_loss:0.012, val_acc:0.973]
Epoch [59/120    avg_loss:0.011, val_acc:0.972]
Epoch [60/120    avg_loss:0.012, val_acc:0.971]
Epoch [61/120    avg_loss:0.010, val_acc:0.974]
Epoch [62/120    avg_loss:0.014, val_acc:0.966]
Epoch [63/120    avg_loss:0.015, val_acc:0.964]
Epoch [64/120    avg_loss:0.017, val_acc:0.952]
Epoch [65/120    avg_loss:0.027, val_acc:0.957]
Epoch [66/120    avg_loss:0.024, val_acc:0.964]
Epoch [67/120    avg_loss:0.021, val_acc:0.968]
Epoch [68/120    avg_loss:0.015, val_acc:0.965]
Epoch [69/120    avg_loss:0.010, val_acc:0.965]
Epoch [70/120    avg_loss:0.012, val_acc:0.972]
Epoch [71/120    avg_loss:0.010, val_acc:0.976]
Epoch [72/120    avg_loss:0.009, val_acc:0.974]
Epoch [73/120    avg_loss:0.008, val_acc:0.970]
Epoch [74/120    avg_loss:0.007, val_acc:0.970]
Epoch [75/120    avg_loss:0.006, val_acc:0.973]
Epoch [76/120    avg_loss:0.006, val_acc:0.975]
Epoch [77/120    avg_loss:0.006, val_acc:0.977]
Epoch [78/120    avg_loss:0.005, val_acc:0.973]
Epoch [79/120    avg_loss:0.007, val_acc:0.971]
Epoch [80/120    avg_loss:0.005, val_acc:0.976]
Epoch [81/120    avg_loss:0.004, val_acc:0.976]
Epoch [82/120    avg_loss:0.005, val_acc:0.976]
Epoch [83/120    avg_loss:0.006, val_acc:0.974]
Epoch [84/120    avg_loss:0.005, val_acc:0.974]
Epoch [85/120    avg_loss:0.005, val_acc:0.976]
Epoch [86/120    avg_loss:0.006, val_acc:0.974]
Epoch [87/120    avg_loss:0.009, val_acc:0.965]
Epoch [88/120    avg_loss:0.010, val_acc:0.967]
Epoch [89/120    avg_loss:0.012, val_acc:0.969]
Epoch [90/120    avg_loss:0.011, val_acc:0.976]
Epoch [91/120    avg_loss:0.010, val_acc:0.974]
Epoch [92/120    avg_loss:0.005, val_acc:0.973]
Epoch [93/120    avg_loss:0.006, val_acc:0.973]
Epoch [94/120    avg_loss:0.005, val_acc:0.975]
Epoch [95/120    avg_loss:0.004, val_acc:0.975]
Epoch [96/120    avg_loss:0.006, val_acc:0.976]
Epoch [97/120    avg_loss:0.005, val_acc:0.977]
Epoch [98/120    avg_loss:0.005, val_acc:0.974]
Epoch [99/120    avg_loss:0.004, val_acc:0.974]
Epoch [100/120    avg_loss:0.004, val_acc:0.974]
Epoch [101/120    avg_loss:0.005, val_acc:0.976]
Epoch [102/120    avg_loss:0.005, val_acc:0.974]
Epoch [103/120    avg_loss:0.005, val_acc:0.976]
Epoch [104/120    avg_loss:0.005, val_acc:0.977]
Epoch [105/120    avg_loss:0.005, val_acc:0.977]
Epoch [106/120    avg_loss:0.004, val_acc:0.978]
Epoch [107/120    avg_loss:0.004, val_acc:0.978]
Epoch [108/120    avg_loss:0.004, val_acc:0.978]
Epoch [109/120    avg_loss:0.004, val_acc:0.978]
Epoch [110/120    avg_loss:0.004, val_acc:0.978]
Epoch [111/120    avg_loss:0.006, val_acc:0.978]
Epoch [112/120    avg_loss:0.004, val_acc:0.978]
Epoch [113/120    avg_loss:0.003, val_acc:0.978]
Epoch [114/120    avg_loss:0.004, val_acc:0.978]
Epoch [115/120    avg_loss:0.004, val_acc:0.978]
Epoch [116/120    avg_loss:0.004, val_acc:0.976]
Epoch [117/120    avg_loss:0.004, val_acc:0.978]
Epoch [118/120    avg_loss:0.004, val_acc:0.978]
Epoch [119/120    avg_loss:0.005, val_acc:0.978]
Epoch [120/120    avg_loss:0.004, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1266    5    1    0    0    0    0    2    1    8    1    1
     0    0    0]
 [   0    0    0  732    4    4    0    0    0    1    3    1    2    0
     0    0    0]
 [   0    0    0    2  210    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     3    1    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    2    0    0    1    0    0    0    0  861    9    0    0
     0    2    0]
 [   0    0   14    0    0    0    9    0    0    0   12 2159   15    0
     0    1    0]
 [   0    0    0    3    3    0    0    0    0    0    0    0  527    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    60  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
97.84281842818429

F1 scores:
[       nan 0.98765432 0.98598131 0.98321021 0.97447796 0.98966705
 0.98718915 1.         1.         0.89473684 0.98287671 0.98359909
 0.97232472 0.99730458 0.96782497 0.87402799 0.96341463]

Kappa:
0.9754093854590735
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f56e246f710>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.554, val_acc:0.395]
Epoch [2/120    avg_loss:2.016, val_acc:0.510]
Epoch [3/120    avg_loss:1.745, val_acc:0.535]
Epoch [4/120    avg_loss:1.509, val_acc:0.594]
Epoch [5/120    avg_loss:1.373, val_acc:0.611]
Epoch [6/120    avg_loss:1.205, val_acc:0.716]
Epoch [7/120    avg_loss:0.999, val_acc:0.711]
Epoch [8/120    avg_loss:0.882, val_acc:0.730]
Epoch [9/120    avg_loss:0.864, val_acc:0.713]
Epoch [10/120    avg_loss:0.763, val_acc:0.740]
Epoch [11/120    avg_loss:0.596, val_acc:0.815]
Epoch [12/120    avg_loss:0.506, val_acc:0.833]
Epoch [13/120    avg_loss:0.490, val_acc:0.804]
Epoch [14/120    avg_loss:0.447, val_acc:0.827]
Epoch [15/120    avg_loss:0.344, val_acc:0.864]
Epoch [16/120    avg_loss:0.285, val_acc:0.802]
Epoch [17/120    avg_loss:0.347, val_acc:0.846]
Epoch [18/120    avg_loss:0.279, val_acc:0.885]
Epoch [19/120    avg_loss:0.225, val_acc:0.880]
Epoch [20/120    avg_loss:0.221, val_acc:0.881]
Epoch [21/120    avg_loss:0.205, val_acc:0.918]
Epoch [22/120    avg_loss:0.162, val_acc:0.849]
Epoch [23/120    avg_loss:0.155, val_acc:0.900]
Epoch [24/120    avg_loss:0.120, val_acc:0.929]
Epoch [25/120    avg_loss:0.106, val_acc:0.921]
Epoch [26/120    avg_loss:0.094, val_acc:0.936]
Epoch [27/120    avg_loss:0.073, val_acc:0.943]
Epoch [28/120    avg_loss:0.095, val_acc:0.928]
Epoch [29/120    avg_loss:0.072, val_acc:0.938]
Epoch [30/120    avg_loss:0.075, val_acc:0.945]
Epoch [31/120    avg_loss:0.067, val_acc:0.950]
Epoch [32/120    avg_loss:0.079, val_acc:0.926]
Epoch [33/120    avg_loss:0.070, val_acc:0.952]
Epoch [34/120    avg_loss:0.072, val_acc:0.935]
Epoch [35/120    avg_loss:0.073, val_acc:0.949]
Epoch [36/120    avg_loss:0.054, val_acc:0.944]
Epoch [37/120    avg_loss:0.049, val_acc:0.963]
Epoch [38/120    avg_loss:0.049, val_acc:0.957]
Epoch [39/120    avg_loss:0.034, val_acc:0.963]
Epoch [40/120    avg_loss:0.033, val_acc:0.966]
Epoch [41/120    avg_loss:0.028, val_acc:0.957]
Epoch [42/120    avg_loss:0.028, val_acc:0.963]
Epoch [43/120    avg_loss:0.026, val_acc:0.957]
Epoch [44/120    avg_loss:0.033, val_acc:0.957]
Epoch [45/120    avg_loss:0.029, val_acc:0.964]
Epoch [46/120    avg_loss:0.022, val_acc:0.971]
Epoch [47/120    avg_loss:0.018, val_acc:0.974]
Epoch [48/120    avg_loss:0.025, val_acc:0.970]
Epoch [49/120    avg_loss:0.022, val_acc:0.966]
Epoch [50/120    avg_loss:0.022, val_acc:0.965]
Epoch [51/120    avg_loss:0.017, val_acc:0.979]
Epoch [52/120    avg_loss:0.012, val_acc:0.980]
Epoch [53/120    avg_loss:0.012, val_acc:0.979]
Epoch [54/120    avg_loss:0.013, val_acc:0.978]
Epoch [55/120    avg_loss:0.022, val_acc:0.975]
Epoch [56/120    avg_loss:0.061, val_acc:0.951]
Epoch [57/120    avg_loss:0.054, val_acc:0.952]
Epoch [58/120    avg_loss:0.047, val_acc:0.945]
Epoch [59/120    avg_loss:0.026, val_acc:0.960]
Epoch [60/120    avg_loss:0.022, val_acc:0.967]
Epoch [61/120    avg_loss:0.019, val_acc:0.973]
Epoch [62/120    avg_loss:0.015, val_acc:0.970]
Epoch [63/120    avg_loss:0.013, val_acc:0.977]
Epoch [64/120    avg_loss:0.008, val_acc:0.971]
Epoch [65/120    avg_loss:0.008, val_acc:0.976]
Epoch [66/120    avg_loss:0.009, val_acc:0.976]
Epoch [67/120    avg_loss:0.008, val_acc:0.978]
Epoch [68/120    avg_loss:0.008, val_acc:0.977]
Epoch [69/120    avg_loss:0.008, val_acc:0.978]
Epoch [70/120    avg_loss:0.010, val_acc:0.977]
Epoch [71/120    avg_loss:0.007, val_acc:0.978]
Epoch [72/120    avg_loss:0.008, val_acc:0.975]
Epoch [73/120    avg_loss:0.007, val_acc:0.978]
Epoch [74/120    avg_loss:0.007, val_acc:0.978]
Epoch [75/120    avg_loss:0.007, val_acc:0.978]
Epoch [76/120    avg_loss:0.007, val_acc:0.977]
Epoch [77/120    avg_loss:0.007, val_acc:0.977]
Epoch [78/120    avg_loss:0.007, val_acc:0.976]
Epoch [79/120    avg_loss:0.007, val_acc:0.977]
Epoch [80/120    avg_loss:0.007, val_acc:0.977]
Epoch [81/120    avg_loss:0.008, val_acc:0.977]
Epoch [82/120    avg_loss:0.006, val_acc:0.976]
Epoch [83/120    avg_loss:0.007, val_acc:0.976]
Epoch [84/120    avg_loss:0.006, val_acc:0.976]
Epoch [85/120    avg_loss:0.006, val_acc:0.977]
Epoch [86/120    avg_loss:0.007, val_acc:0.976]
Epoch [87/120    avg_loss:0.007, val_acc:0.976]
Epoch [88/120    avg_loss:0.008, val_acc:0.977]
Epoch [89/120    avg_loss:0.008, val_acc:0.977]
Epoch [90/120    avg_loss:0.006, val_acc:0.977]
Epoch [91/120    avg_loss:0.008, val_acc:0.977]
Epoch [92/120    avg_loss:0.006, val_acc:0.977]
Epoch [93/120    avg_loss:0.007, val_acc:0.977]
Epoch [94/120    avg_loss:0.007, val_acc:0.977]
Epoch [95/120    avg_loss:0.008, val_acc:0.977]
Epoch [96/120    avg_loss:0.008, val_acc:0.977]
Epoch [97/120    avg_loss:0.010, val_acc:0.977]
Epoch [98/120    avg_loss:0.008, val_acc:0.977]
Epoch [99/120    avg_loss:0.007, val_acc:0.977]
Epoch [100/120    avg_loss:0.006, val_acc:0.977]
Epoch [101/120    avg_loss:0.008, val_acc:0.977]
Epoch [102/120    avg_loss:0.007, val_acc:0.977]
Epoch [103/120    avg_loss:0.007, val_acc:0.977]
Epoch [104/120    avg_loss:0.007, val_acc:0.977]
Epoch [105/120    avg_loss:0.010, val_acc:0.977]
Epoch [106/120    avg_loss:0.010, val_acc:0.977]
Epoch [107/120    avg_loss:0.007, val_acc:0.977]
Epoch [108/120    avg_loss:0.007, val_acc:0.977]
Epoch [109/120    avg_loss:0.007, val_acc:0.977]
Epoch [110/120    avg_loss:0.006, val_acc:0.977]
Epoch [111/120    avg_loss:0.006, val_acc:0.977]
Epoch [112/120    avg_loss:0.008, val_acc:0.977]
Epoch [113/120    avg_loss:0.007, val_acc:0.977]
Epoch [114/120    avg_loss:0.007, val_acc:0.977]
Epoch [115/120    avg_loss:0.007, val_acc:0.977]
Epoch [116/120    avg_loss:0.006, val_acc:0.977]
Epoch [117/120    avg_loss:0.006, val_acc:0.977]
Epoch [118/120    avg_loss:0.008, val_acc:0.977]
Epoch [119/120    avg_loss:0.009, val_acc:0.977]
Epoch [120/120    avg_loss:0.008, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1249    7    8    0    0    0    0    1    2   17    1    0
     0    0    0]
 [   0    0    0  725    1    3    0    0    0    3    3    7    4    0
     0    1    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    3    0    1    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  858   11    0    0
     0    0    0]
 [   0    0   10    0    0    0    0    0    0    1   11 2162   18    5
     0    3    0]
 [   0    0    0    2    0    0    0    0    0    0    0    7  523    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1121   18    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    81  266    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
97.289972899729

F1 scores:
[       nan 0.98765432 0.97922383 0.97840756 0.97695853 0.98611111
 1.         0.94339623 1.         0.82926829 0.98113208 0.97916667
 0.9640553  0.98666667 0.95607676 0.83779528 0.95757576]

Kappa:
0.9690942394999245
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4c5f361668>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.613, val_acc:0.426]
Epoch [2/120    avg_loss:2.086, val_acc:0.455]
Epoch [3/120    avg_loss:1.793, val_acc:0.574]
Epoch [4/120    avg_loss:1.687, val_acc:0.548]
Epoch [5/120    avg_loss:1.463, val_acc:0.585]
Epoch [6/120    avg_loss:1.251, val_acc:0.606]
Epoch [7/120    avg_loss:1.157, val_acc:0.694]
Epoch [8/120    avg_loss:1.011, val_acc:0.738]
Epoch [9/120    avg_loss:0.827, val_acc:0.733]
Epoch [10/120    avg_loss:0.689, val_acc:0.810]
Epoch [11/120    avg_loss:0.610, val_acc:0.804]
Epoch [12/120    avg_loss:0.534, val_acc:0.850]
Epoch [13/120    avg_loss:0.415, val_acc:0.852]
Epoch [14/120    avg_loss:0.401, val_acc:0.829]
Epoch [15/120    avg_loss:0.296, val_acc:0.876]
Epoch [16/120    avg_loss:0.333, val_acc:0.845]
Epoch [17/120    avg_loss:0.309, val_acc:0.890]
Epoch [18/120    avg_loss:0.231, val_acc:0.887]
Epoch [19/120    avg_loss:0.178, val_acc:0.926]
Epoch [20/120    avg_loss:0.176, val_acc:0.918]
Epoch [21/120    avg_loss:0.157, val_acc:0.918]
Epoch [22/120    avg_loss:0.138, val_acc:0.929]
Epoch [23/120    avg_loss:0.112, val_acc:0.925]
Epoch [24/120    avg_loss:0.108, val_acc:0.912]
Epoch [25/120    avg_loss:0.109, val_acc:0.921]
Epoch [26/120    avg_loss:0.094, val_acc:0.946]
Epoch [27/120    avg_loss:0.084, val_acc:0.944]
Epoch [28/120    avg_loss:0.117, val_acc:0.918]
Epoch [29/120    avg_loss:0.108, val_acc:0.936]
Epoch [30/120    avg_loss:0.100, val_acc:0.951]
Epoch [31/120    avg_loss:0.091, val_acc:0.948]
Epoch [32/120    avg_loss:0.083, val_acc:0.949]
Epoch [33/120    avg_loss:0.062, val_acc:0.949]
Epoch [34/120    avg_loss:0.057, val_acc:0.957]
Epoch [35/120    avg_loss:0.058, val_acc:0.938]
Epoch [36/120    avg_loss:0.048, val_acc:0.948]
Epoch [37/120    avg_loss:0.047, val_acc:0.949]
Epoch [38/120    avg_loss:0.035, val_acc:0.965]
Epoch [39/120    avg_loss:0.040, val_acc:0.952]
Epoch [40/120    avg_loss:0.039, val_acc:0.957]
Epoch [41/120    avg_loss:0.063, val_acc:0.952]
Epoch [42/120    avg_loss:0.046, val_acc:0.958]
Epoch [43/120    avg_loss:0.031, val_acc:0.955]
Epoch [44/120    avg_loss:0.037, val_acc:0.957]
Epoch [45/120    avg_loss:0.040, val_acc:0.967]
Epoch [46/120    avg_loss:0.032, val_acc:0.960]
Epoch [47/120    avg_loss:0.028, val_acc:0.969]
Epoch [48/120    avg_loss:0.024, val_acc:0.970]
Epoch [49/120    avg_loss:0.040, val_acc:0.958]
Epoch [50/120    avg_loss:0.027, val_acc:0.967]
Epoch [51/120    avg_loss:0.027, val_acc:0.968]
Epoch [52/120    avg_loss:0.026, val_acc:0.964]
Epoch [53/120    avg_loss:0.018, val_acc:0.970]
Epoch [54/120    avg_loss:0.033, val_acc:0.948]
Epoch [55/120    avg_loss:0.036, val_acc:0.944]
Epoch [56/120    avg_loss:0.029, val_acc:0.965]
Epoch [57/120    avg_loss:0.033, val_acc:0.967]
Epoch [58/120    avg_loss:0.026, val_acc:0.968]
Epoch [59/120    avg_loss:0.030, val_acc:0.967]
Epoch [60/120    avg_loss:0.026, val_acc:0.967]
Epoch [61/120    avg_loss:0.017, val_acc:0.944]
Epoch [62/120    avg_loss:0.016, val_acc:0.973]
Epoch [63/120    avg_loss:0.011, val_acc:0.972]
Epoch [64/120    avg_loss:0.009, val_acc:0.975]
Epoch [65/120    avg_loss:0.011, val_acc:0.972]
Epoch [66/120    avg_loss:0.010, val_acc:0.977]
Epoch [67/120    avg_loss:0.010, val_acc:0.975]
Epoch [68/120    avg_loss:0.007, val_acc:0.978]
Epoch [69/120    avg_loss:0.008, val_acc:0.979]
Epoch [70/120    avg_loss:0.009, val_acc:0.975]
Epoch [71/120    avg_loss:0.007, val_acc:0.976]
Epoch [72/120    avg_loss:0.008, val_acc:0.977]
Epoch [73/120    avg_loss:0.008, val_acc:0.978]
Epoch [74/120    avg_loss:0.008, val_acc:0.972]
Epoch [75/120    avg_loss:0.010, val_acc:0.976]
Epoch [76/120    avg_loss:0.007, val_acc:0.976]
Epoch [77/120    avg_loss:0.006, val_acc:0.975]
Epoch [78/120    avg_loss:0.008, val_acc:0.976]
Epoch [79/120    avg_loss:0.009, val_acc:0.977]
Epoch [80/120    avg_loss:0.009, val_acc:0.971]
Epoch [81/120    avg_loss:0.010, val_acc:0.975]
Epoch [82/120    avg_loss:0.008, val_acc:0.978]
Epoch [83/120    avg_loss:0.005, val_acc:0.976]
Epoch [84/120    avg_loss:0.006, val_acc:0.976]
Epoch [85/120    avg_loss:0.005, val_acc:0.976]
Epoch [86/120    avg_loss:0.005, val_acc:0.973]
Epoch [87/120    avg_loss:0.004, val_acc:0.974]
Epoch [88/120    avg_loss:0.005, val_acc:0.975]
Epoch [89/120    avg_loss:0.006, val_acc:0.976]
Epoch [90/120    avg_loss:0.005, val_acc:0.975]
Epoch [91/120    avg_loss:0.005, val_acc:0.975]
Epoch [92/120    avg_loss:0.005, val_acc:0.974]
Epoch [93/120    avg_loss:0.004, val_acc:0.976]
Epoch [94/120    avg_loss:0.004, val_acc:0.976]
Epoch [95/120    avg_loss:0.004, val_acc:0.976]
Epoch [96/120    avg_loss:0.004, val_acc:0.976]
Epoch [97/120    avg_loss:0.005, val_acc:0.976]
Epoch [98/120    avg_loss:0.005, val_acc:0.977]
Epoch [99/120    avg_loss:0.004, val_acc:0.977]
Epoch [100/120    avg_loss:0.004, val_acc:0.977]
Epoch [101/120    avg_loss:0.005, val_acc:0.977]
Epoch [102/120    avg_loss:0.005, val_acc:0.977]
Epoch [103/120    avg_loss:0.004, val_acc:0.977]
Epoch [104/120    avg_loss:0.005, val_acc:0.977]
Epoch [105/120    avg_loss:0.004, val_acc:0.977]
Epoch [106/120    avg_loss:0.005, val_acc:0.977]
Epoch [107/120    avg_loss:0.004, val_acc:0.977]
Epoch [108/120    avg_loss:0.004, val_acc:0.977]
Epoch [109/120    avg_loss:0.004, val_acc:0.977]
Epoch [110/120    avg_loss:0.004, val_acc:0.977]
Epoch [111/120    avg_loss:0.005, val_acc:0.977]
Epoch [112/120    avg_loss:0.005, val_acc:0.977]
Epoch [113/120    avg_loss:0.004, val_acc:0.977]
Epoch [114/120    avg_loss:0.004, val_acc:0.977]
Epoch [115/120    avg_loss:0.005, val_acc:0.977]
Epoch [116/120    avg_loss:0.006, val_acc:0.977]
Epoch [117/120    avg_loss:0.005, val_acc:0.977]
Epoch [118/120    avg_loss:0.007, val_acc:0.977]
Epoch [119/120    avg_loss:0.004, val_acc:0.977]
Epoch [120/120    avg_loss:0.005, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1266    8    1    2    0    0    0    0    3    5    0    0
     0    0    0]
 [   0    0    0  714   15    0    1    0    0    1    3    3    8    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  427    0    0    0    0    0
     0    1    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  851   19    0    0
     0    1    0]
 [   0    0   19    1    0    0    0    0    0    1    8 2168   11    2
     0    0    0]
 [   0    0    0    5    0    0    0    0    0    0    0    0  524    0
     2    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1121   18    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    68  274    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.55013550135502

F1 scores:
[       nan 0.97619048 0.98368298 0.96813559 0.9638009  0.9954023
 0.99469295 1.         0.99649942 0.94736842 0.97816092 0.98411257
 0.97217069 0.98930481 0.96140652 0.85358255 0.98224852]

Kappa:
0.9720669238373799
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f15c5e90780>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.544, val_acc:0.443]
Epoch [2/120    avg_loss:1.982, val_acc:0.531]
Epoch [3/120    avg_loss:1.712, val_acc:0.593]
Epoch [4/120    avg_loss:1.530, val_acc:0.611]
Epoch [5/120    avg_loss:1.383, val_acc:0.668]
Epoch [6/120    avg_loss:1.111, val_acc:0.715]
Epoch [7/120    avg_loss:0.927, val_acc:0.783]
Epoch [8/120    avg_loss:0.857, val_acc:0.656]
Epoch [9/120    avg_loss:0.965, val_acc:0.729]
Epoch [10/120    avg_loss:0.793, val_acc:0.800]
Epoch [11/120    avg_loss:0.639, val_acc:0.784]
Epoch [12/120    avg_loss:0.499, val_acc:0.852]
Epoch [13/120    avg_loss:0.447, val_acc:0.903]
Epoch [14/120    avg_loss:0.388, val_acc:0.898]
Epoch [15/120    avg_loss:0.320, val_acc:0.872]
Epoch [16/120    avg_loss:0.224, val_acc:0.906]
Epoch [17/120    avg_loss:0.185, val_acc:0.925]
Epoch [18/120    avg_loss:0.202, val_acc:0.915]
Epoch [19/120    avg_loss:0.194, val_acc:0.944]
Epoch [20/120    avg_loss:0.142, val_acc:0.947]
Epoch [21/120    avg_loss:0.146, val_acc:0.918]
Epoch [22/120    avg_loss:0.157, val_acc:0.927]
Epoch [23/120    avg_loss:0.108, val_acc:0.948]
Epoch [24/120    avg_loss:0.119, val_acc:0.929]
Epoch [25/120    avg_loss:0.302, val_acc:0.859]
Epoch [26/120    avg_loss:0.646, val_acc:0.850]
Epoch [27/120    avg_loss:0.364, val_acc:0.887]
Epoch [28/120    avg_loss:0.274, val_acc:0.899]
Epoch [29/120    avg_loss:0.181, val_acc:0.943]
Epoch [30/120    avg_loss:0.146, val_acc:0.943]
Epoch [31/120    avg_loss:0.133, val_acc:0.946]
Epoch [32/120    avg_loss:0.108, val_acc:0.931]
Epoch [33/120    avg_loss:0.095, val_acc:0.961]
Epoch [34/120    avg_loss:0.073, val_acc:0.964]
Epoch [35/120    avg_loss:0.057, val_acc:0.974]
Epoch [36/120    avg_loss:0.060, val_acc:0.966]
Epoch [37/120    avg_loss:0.058, val_acc:0.953]
Epoch [38/120    avg_loss:0.083, val_acc:0.960]
Epoch [39/120    avg_loss:0.071, val_acc:0.955]
Epoch [40/120    avg_loss:0.059, val_acc:0.963]
Epoch [41/120    avg_loss:0.047, val_acc:0.967]
Epoch [42/120    avg_loss:0.040, val_acc:0.978]
Epoch [43/120    avg_loss:0.034, val_acc:0.972]
Epoch [44/120    avg_loss:0.031, val_acc:0.972]
Epoch [45/120    avg_loss:0.026, val_acc:0.976]
Epoch [46/120    avg_loss:0.034, val_acc:0.954]
Epoch [47/120    avg_loss:0.038, val_acc:0.973]
Epoch [48/120    avg_loss:0.025, val_acc:0.973]
Epoch [49/120    avg_loss:0.054, val_acc:0.977]
Epoch [50/120    avg_loss:0.041, val_acc:0.980]
Epoch [51/120    avg_loss:0.031, val_acc:0.977]
Epoch [52/120    avg_loss:0.022, val_acc:0.978]
Epoch [53/120    avg_loss:0.016, val_acc:0.982]
Epoch [54/120    avg_loss:0.018, val_acc:0.975]
Epoch [55/120    avg_loss:0.022, val_acc:0.983]
Epoch [56/120    avg_loss:0.017, val_acc:0.982]
Epoch [57/120    avg_loss:0.015, val_acc:0.980]
Epoch [58/120    avg_loss:0.014, val_acc:0.982]
Epoch [59/120    avg_loss:0.024, val_acc:0.976]
Epoch [60/120    avg_loss:0.014, val_acc:0.982]
Epoch [61/120    avg_loss:0.013, val_acc:0.979]
Epoch [62/120    avg_loss:0.015, val_acc:0.983]
Epoch [63/120    avg_loss:0.013, val_acc:0.980]
Epoch [64/120    avg_loss:0.013, val_acc:0.978]
Epoch [65/120    avg_loss:0.010, val_acc:0.983]
Epoch [66/120    avg_loss:0.010, val_acc:0.983]
Epoch [67/120    avg_loss:0.008, val_acc:0.983]
Epoch [68/120    avg_loss:0.008, val_acc:0.983]
Epoch [69/120    avg_loss:0.006, val_acc:0.986]
Epoch [70/120    avg_loss:0.008, val_acc:0.982]
Epoch [71/120    avg_loss:0.006, val_acc:0.985]
Epoch [72/120    avg_loss:0.014, val_acc:0.977]
Epoch [73/120    avg_loss:0.012, val_acc:0.983]
Epoch [74/120    avg_loss:0.012, val_acc:0.972]
Epoch [75/120    avg_loss:0.012, val_acc:0.980]
Epoch [76/120    avg_loss:0.013, val_acc:0.976]
Epoch [77/120    avg_loss:0.009, val_acc:0.978]
Epoch [78/120    avg_loss:0.008, val_acc:0.979]
Epoch [79/120    avg_loss:0.008, val_acc:0.984]
Epoch [80/120    avg_loss:0.006, val_acc:0.984]
Epoch [81/120    avg_loss:0.017, val_acc:0.975]
Epoch [82/120    avg_loss:0.014, val_acc:0.980]
Epoch [83/120    avg_loss:0.008, val_acc:0.979]
Epoch [84/120    avg_loss:0.007, val_acc:0.983]
Epoch [85/120    avg_loss:0.008, val_acc:0.982]
Epoch [86/120    avg_loss:0.008, val_acc:0.985]
Epoch [87/120    avg_loss:0.006, val_acc:0.984]
Epoch [88/120    avg_loss:0.006, val_acc:0.984]
Epoch [89/120    avg_loss:0.006, val_acc:0.983]
Epoch [90/120    avg_loss:0.005, val_acc:0.983]
Epoch [91/120    avg_loss:0.005, val_acc:0.982]
Epoch [92/120    avg_loss:0.007, val_acc:0.983]
Epoch [93/120    avg_loss:0.004, val_acc:0.983]
Epoch [94/120    avg_loss:0.006, val_acc:0.984]
Epoch [95/120    avg_loss:0.005, val_acc:0.984]
Epoch [96/120    avg_loss:0.005, val_acc:0.984]
Epoch [97/120    avg_loss:0.005, val_acc:0.984]
Epoch [98/120    avg_loss:0.006, val_acc:0.984]
Epoch [99/120    avg_loss:0.005, val_acc:0.984]
Epoch [100/120    avg_loss:0.006, val_acc:0.984]
Epoch [101/120    avg_loss:0.005, val_acc:0.984]
Epoch [102/120    avg_loss:0.006, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.984]
Epoch [104/120    avg_loss:0.005, val_acc:0.984]
Epoch [105/120    avg_loss:0.005, val_acc:0.984]
Epoch [106/120    avg_loss:0.006, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.005, val_acc:0.984]
Epoch [109/120    avg_loss:0.005, val_acc:0.984]
Epoch [110/120    avg_loss:0.006, val_acc:0.984]
Epoch [111/120    avg_loss:0.005, val_acc:0.984]
Epoch [112/120    avg_loss:0.005, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.005, val_acc:0.984]
Epoch [115/120    avg_loss:0.005, val_acc:0.984]
Epoch [116/120    avg_loss:0.004, val_acc:0.984]
Epoch [117/120    avg_loss:0.005, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.005, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1248    7    1    1    0    0    0    2    4   16    6    0
     0    0    0]
 [   0    0    0  725    2    1    1    0    0    3    2    6    7    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    0    0    0    0    0  859    9    0    0
     0    0    0]
 [   0    0    8    4    0    0   10    0    0    0    7 2164   14    1
     0    2    0]
 [   0    0    0    4    0    0    0    0    0    0    0    1  527    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1118   20    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    26  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.98373983739837

F1 scores:
[       nan 0.975      0.97882353 0.97511769 0.99300699 0.99310345
 0.98793363 1.         1.         0.87804878 0.98340011 0.98207397
 0.96786042 0.99730458 0.97770004 0.92576419 0.98224852]

Kappa:
0.977023407133656
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb5dea75710>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.540, val_acc:0.428]
Epoch [2/120    avg_loss:2.022, val_acc:0.503]
Epoch [3/120    avg_loss:1.763, val_acc:0.591]
Epoch [4/120    avg_loss:1.587, val_acc:0.520]
Epoch [5/120    avg_loss:1.423, val_acc:0.606]
Epoch [6/120    avg_loss:1.240, val_acc:0.672]
Epoch [7/120    avg_loss:1.008, val_acc:0.719]
Epoch [8/120    avg_loss:0.945, val_acc:0.702]
Epoch [9/120    avg_loss:0.720, val_acc:0.740]
Epoch [10/120    avg_loss:0.644, val_acc:0.744]
Epoch [11/120    avg_loss:0.532, val_acc:0.807]
Epoch [12/120    avg_loss:0.483, val_acc:0.846]
Epoch [13/120    avg_loss:0.416, val_acc:0.785]
Epoch [14/120    avg_loss:0.415, val_acc:0.808]
Epoch [15/120    avg_loss:0.346, val_acc:0.824]
Epoch [16/120    avg_loss:0.309, val_acc:0.857]
Epoch [17/120    avg_loss:0.272, val_acc:0.874]
Epoch [18/120    avg_loss:0.223, val_acc:0.897]
Epoch [19/120    avg_loss:0.256, val_acc:0.878]
Epoch [20/120    avg_loss:0.193, val_acc:0.912]
Epoch [21/120    avg_loss:0.154, val_acc:0.908]
Epoch [22/120    avg_loss:0.133, val_acc:0.923]
Epoch [23/120    avg_loss:0.124, val_acc:0.894]
Epoch [24/120    avg_loss:0.155, val_acc:0.892]
Epoch [25/120    avg_loss:0.112, val_acc:0.877]
Epoch [26/120    avg_loss:0.150, val_acc:0.910]
Epoch [27/120    avg_loss:0.266, val_acc:0.911]
Epoch [28/120    avg_loss:0.144, val_acc:0.921]
Epoch [29/120    avg_loss:0.104, val_acc:0.930]
Epoch [30/120    avg_loss:0.095, val_acc:0.936]
Epoch [31/120    avg_loss:0.087, val_acc:0.951]
Epoch [32/120    avg_loss:0.061, val_acc:0.948]
Epoch [33/120    avg_loss:0.062, val_acc:0.953]
Epoch [34/120    avg_loss:0.052, val_acc:0.949]
Epoch [35/120    avg_loss:0.060, val_acc:0.931]
Epoch [36/120    avg_loss:0.054, val_acc:0.953]
Epoch [37/120    avg_loss:0.039, val_acc:0.954]
Epoch [38/120    avg_loss:0.039, val_acc:0.958]
Epoch [39/120    avg_loss:0.053, val_acc:0.933]
Epoch [40/120    avg_loss:0.053, val_acc:0.963]
Epoch [41/120    avg_loss:0.046, val_acc:0.952]
Epoch [42/120    avg_loss:0.043, val_acc:0.910]
Epoch [43/120    avg_loss:0.064, val_acc:0.957]
Epoch [44/120    avg_loss:0.037, val_acc:0.948]
Epoch [45/120    avg_loss:0.041, val_acc:0.967]
Epoch [46/120    avg_loss:0.030, val_acc:0.967]
Epoch [47/120    avg_loss:0.030, val_acc:0.958]
Epoch [48/120    avg_loss:0.075, val_acc:0.941]
Epoch [49/120    avg_loss:0.040, val_acc:0.959]
Epoch [50/120    avg_loss:0.030, val_acc:0.960]
Epoch [51/120    avg_loss:0.023, val_acc:0.952]
Epoch [52/120    avg_loss:0.016, val_acc:0.973]
Epoch [53/120    avg_loss:0.015, val_acc:0.970]
Epoch [54/120    avg_loss:0.017, val_acc:0.974]
Epoch [55/120    avg_loss:0.017, val_acc:0.968]
Epoch [56/120    avg_loss:0.023, val_acc:0.970]
Epoch [57/120    avg_loss:0.023, val_acc:0.966]
Epoch [58/120    avg_loss:0.018, val_acc:0.967]
Epoch [59/120    avg_loss:0.017, val_acc:0.973]
Epoch [60/120    avg_loss:0.013, val_acc:0.971]
Epoch [61/120    avg_loss:0.020, val_acc:0.973]
Epoch [62/120    avg_loss:0.065, val_acc:0.966]
Epoch [63/120    avg_loss:0.028, val_acc:0.956]
Epoch [64/120    avg_loss:0.026, val_acc:0.971]
Epoch [65/120    avg_loss:0.015, val_acc:0.975]
Epoch [66/120    avg_loss:0.013, val_acc:0.976]
Epoch [67/120    avg_loss:0.012, val_acc:0.974]
Epoch [68/120    avg_loss:0.012, val_acc:0.970]
Epoch [69/120    avg_loss:0.016, val_acc:0.978]
Epoch [70/120    avg_loss:0.014, val_acc:0.977]
Epoch [71/120    avg_loss:0.012, val_acc:0.978]
Epoch [72/120    avg_loss:0.008, val_acc:0.977]
Epoch [73/120    avg_loss:0.008, val_acc:0.975]
Epoch [74/120    avg_loss:0.007, val_acc:0.975]
Epoch [75/120    avg_loss:0.006, val_acc:0.975]
Epoch [76/120    avg_loss:0.007, val_acc:0.977]
Epoch [77/120    avg_loss:0.006, val_acc:0.977]
Epoch [78/120    avg_loss:0.006, val_acc:0.978]
Epoch [79/120    avg_loss:0.006, val_acc:0.976]
Epoch [80/120    avg_loss:0.011, val_acc:0.977]
Epoch [81/120    avg_loss:0.010, val_acc:0.975]
Epoch [82/120    avg_loss:0.008, val_acc:0.977]
Epoch [83/120    avg_loss:0.007, val_acc:0.979]
Epoch [84/120    avg_loss:0.005, val_acc:0.979]
Epoch [85/120    avg_loss:0.005, val_acc:0.980]
Epoch [86/120    avg_loss:0.005, val_acc:0.979]
Epoch [87/120    avg_loss:0.005, val_acc:0.978]
Epoch [88/120    avg_loss:0.004, val_acc:0.981]
Epoch [89/120    avg_loss:0.005, val_acc:0.981]
Epoch [90/120    avg_loss:0.005, val_acc:0.977]
Epoch [91/120    avg_loss:0.004, val_acc:0.980]
Epoch [92/120    avg_loss:0.004, val_acc:0.978]
Epoch [93/120    avg_loss:0.004, val_acc:0.978]
Epoch [94/120    avg_loss:0.006, val_acc:0.975]
Epoch [95/120    avg_loss:0.008, val_acc:0.979]
Epoch [96/120    avg_loss:0.008, val_acc:0.974]
Epoch [97/120    avg_loss:0.007, val_acc:0.975]
Epoch [98/120    avg_loss:0.007, val_acc:0.978]
Epoch [99/120    avg_loss:0.018, val_acc:0.971]
Epoch [100/120    avg_loss:0.018, val_acc:0.959]
Epoch [101/120    avg_loss:0.014, val_acc:0.968]
Epoch [102/120    avg_loss:0.014, val_acc:0.977]
Epoch [103/120    avg_loss:0.006, val_acc:0.976]
Epoch [104/120    avg_loss:0.006, val_acc:0.975]
Epoch [105/120    avg_loss:0.006, val_acc:0.976]
Epoch [106/120    avg_loss:0.005, val_acc:0.975]
Epoch [107/120    avg_loss:0.005, val_acc:0.974]
Epoch [108/120    avg_loss:0.005, val_acc:0.977]
Epoch [109/120    avg_loss:0.006, val_acc:0.976]
Epoch [110/120    avg_loss:0.005, val_acc:0.977]
Epoch [111/120    avg_loss:0.006, val_acc:0.977]
Epoch [112/120    avg_loss:0.004, val_acc:0.977]
Epoch [113/120    avg_loss:0.006, val_acc:0.976]
Epoch [114/120    avg_loss:0.004, val_acc:0.976]
Epoch [115/120    avg_loss:0.004, val_acc:0.977]
Epoch [116/120    avg_loss:0.004, val_acc:0.977]
Epoch [117/120    avg_loss:0.004, val_acc:0.977]
Epoch [118/120    avg_loss:0.004, val_acc:0.976]
Epoch [119/120    avg_loss:0.005, val_acc:0.976]
Epoch [120/120    avg_loss:0.005, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1266    0    0    0    0    0    0    1    1   17    0    0
     0    0    0]
 [   0    0    0  728    0    0    0    0    0    2    7    6    4    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    1    0    0    0    0  856   17    0    0
     0    0    0]
 [   0    0   13    0    0    1    1    0    0    0    6 2164   20    1
     0    4    0]
 [   0    0    0    6    0    3    0    0    0    0    0    0  522    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
  1125   12    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
    74  266    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.63685636856368

F1 scores:
[       nan 0.98765432 0.98674981 0.98245614 0.99764706 0.98966705
 0.99244713 0.98039216 1.         0.92307692 0.98108883 0.98051654
 0.96487985 0.99730458 0.96112772 0.84310618 0.98203593]

Kappa:
0.9730462581456646
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f79c4c05710>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.494, val_acc:0.401]
Epoch [2/120    avg_loss:1.979, val_acc:0.504]
Epoch [3/120    avg_loss:1.672, val_acc:0.572]
Epoch [4/120    avg_loss:1.435, val_acc:0.613]
Epoch [5/120    avg_loss:1.269, val_acc:0.670]
Epoch [6/120    avg_loss:1.016, val_acc:0.715]
Epoch [7/120    avg_loss:0.882, val_acc:0.750]
Epoch [8/120    avg_loss:0.713, val_acc:0.774]
Epoch [9/120    avg_loss:0.619, val_acc:0.808]
Epoch [10/120    avg_loss:0.494, val_acc:0.840]
Epoch [11/120    avg_loss:0.537, val_acc:0.829]
Epoch [12/120    avg_loss:0.389, val_acc:0.871]
Epoch [13/120    avg_loss:0.355, val_acc:0.863]
Epoch [14/120    avg_loss:0.337, val_acc:0.895]
Epoch [15/120    avg_loss:0.277, val_acc:0.872]
Epoch [16/120    avg_loss:0.260, val_acc:0.896]
Epoch [17/120    avg_loss:0.198, val_acc:0.902]
Epoch [18/120    avg_loss:0.178, val_acc:0.932]
Epoch [19/120    avg_loss:0.140, val_acc:0.923]
Epoch [20/120    avg_loss:0.156, val_acc:0.910]
Epoch [21/120    avg_loss:0.198, val_acc:0.870]
Epoch [22/120    avg_loss:0.206, val_acc:0.886]
Epoch [23/120    avg_loss:0.171, val_acc:0.925]
Epoch [24/120    avg_loss:0.135, val_acc:0.905]
Epoch [25/120    avg_loss:0.127, val_acc:0.932]
Epoch [26/120    avg_loss:0.129, val_acc:0.941]
Epoch [27/120    avg_loss:0.084, val_acc:0.936]
Epoch [28/120    avg_loss:0.082, val_acc:0.933]
Epoch [29/120    avg_loss:0.134, val_acc:0.927]
Epoch [30/120    avg_loss:0.101, val_acc:0.954]
Epoch [31/120    avg_loss:0.091, val_acc:0.944]
Epoch [32/120    avg_loss:0.068, val_acc:0.959]
Epoch [33/120    avg_loss:0.062, val_acc:0.946]
Epoch [34/120    avg_loss:0.080, val_acc:0.944]
Epoch [35/120    avg_loss:0.055, val_acc:0.955]
Epoch [36/120    avg_loss:0.058, val_acc:0.958]
Epoch [37/120    avg_loss:0.047, val_acc:0.956]
Epoch [38/120    avg_loss:0.037, val_acc:0.957]
Epoch [39/120    avg_loss:0.039, val_acc:0.960]
Epoch [40/120    avg_loss:0.034, val_acc:0.967]
Epoch [41/120    avg_loss:0.032, val_acc:0.957]
Epoch [42/120    avg_loss:0.030, val_acc:0.961]
Epoch [43/120    avg_loss:0.048, val_acc:0.953]
Epoch [44/120    avg_loss:0.049, val_acc:0.957]
Epoch [45/120    avg_loss:0.036, val_acc:0.966]
Epoch [46/120    avg_loss:0.032, val_acc:0.970]
Epoch [47/120    avg_loss:0.031, val_acc:0.966]
Epoch [48/120    avg_loss:0.036, val_acc:0.974]
Epoch [49/120    avg_loss:0.073, val_acc:0.964]
Epoch [50/120    avg_loss:0.038, val_acc:0.958]
Epoch [51/120    avg_loss:0.031, val_acc:0.969]
Epoch [52/120    avg_loss:0.030, val_acc:0.969]
Epoch [53/120    avg_loss:0.025, val_acc:0.959]
Epoch [54/120    avg_loss:0.025, val_acc:0.958]
Epoch [55/120    avg_loss:0.020, val_acc:0.966]
Epoch [56/120    avg_loss:0.020, val_acc:0.975]
Epoch [57/120    avg_loss:0.019, val_acc:0.974]
Epoch [58/120    avg_loss:0.026, val_acc:0.970]
Epoch [59/120    avg_loss:0.028, val_acc:0.966]
Epoch [60/120    avg_loss:0.026, val_acc:0.967]
Epoch [61/120    avg_loss:0.033, val_acc:0.964]
Epoch [62/120    avg_loss:0.021, val_acc:0.972]
Epoch [63/120    avg_loss:0.015, val_acc:0.974]
Epoch [64/120    avg_loss:0.012, val_acc:0.978]
Epoch [65/120    avg_loss:0.012, val_acc:0.973]
Epoch [66/120    avg_loss:0.010, val_acc:0.978]
Epoch [67/120    avg_loss:0.012, val_acc:0.975]
Epoch [68/120    avg_loss:0.012, val_acc:0.972]
Epoch [69/120    avg_loss:0.013, val_acc:0.973]
Epoch [70/120    avg_loss:0.013, val_acc:0.973]
Epoch [71/120    avg_loss:0.011, val_acc:0.973]
Epoch [72/120    avg_loss:0.010, val_acc:0.974]
Epoch [73/120    avg_loss:0.009, val_acc:0.975]
Epoch [74/120    avg_loss:0.011, val_acc:0.981]
Epoch [75/120    avg_loss:0.007, val_acc:0.981]
Epoch [76/120    avg_loss:0.006, val_acc:0.984]
Epoch [77/120    avg_loss:0.006, val_acc:0.977]
Epoch [78/120    avg_loss:0.009, val_acc:0.979]
Epoch [79/120    avg_loss:0.008, val_acc:0.974]
Epoch [80/120    avg_loss:0.008, val_acc:0.977]
Epoch [81/120    avg_loss:0.011, val_acc:0.978]
Epoch [82/120    avg_loss:0.010, val_acc:0.978]
Epoch [83/120    avg_loss:0.007, val_acc:0.975]
Epoch [84/120    avg_loss:0.007, val_acc:0.980]
Epoch [85/120    avg_loss:0.005, val_acc:0.976]
Epoch [86/120    avg_loss:0.005, val_acc:0.978]
Epoch [87/120    avg_loss:0.005, val_acc:0.978]
Epoch [88/120    avg_loss:0.005, val_acc:0.978]
Epoch [89/120    avg_loss:0.004, val_acc:0.978]
Epoch [90/120    avg_loss:0.005, val_acc:0.978]
Epoch [91/120    avg_loss:0.004, val_acc:0.976]
Epoch [92/120    avg_loss:0.005, val_acc:0.976]
Epoch [93/120    avg_loss:0.004, val_acc:0.978]
Epoch [94/120    avg_loss:0.004, val_acc:0.980]
Epoch [95/120    avg_loss:0.005, val_acc:0.982]
Epoch [96/120    avg_loss:0.004, val_acc:0.983]
Epoch [97/120    avg_loss:0.004, val_acc:0.983]
Epoch [98/120    avg_loss:0.004, val_acc:0.982]
Epoch [99/120    avg_loss:0.007, val_acc:0.983]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.004, val_acc:0.984]
Epoch [102/120    avg_loss:0.004, val_acc:0.985]
Epoch [103/120    avg_loss:0.004, val_acc:0.985]
Epoch [104/120    avg_loss:0.004, val_acc:0.985]
Epoch [105/120    avg_loss:0.004, val_acc:0.985]
Epoch [106/120    avg_loss:0.004, val_acc:0.985]
Epoch [107/120    avg_loss:0.005, val_acc:0.984]
Epoch [108/120    avg_loss:0.003, val_acc:0.984]
Epoch [109/120    avg_loss:0.004, val_acc:0.983]
Epoch [110/120    avg_loss:0.004, val_acc:0.983]
Epoch [111/120    avg_loss:0.003, val_acc:0.982]
Epoch [112/120    avg_loss:0.004, val_acc:0.983]
Epoch [113/120    avg_loss:0.005, val_acc:0.983]
Epoch [114/120    avg_loss:0.004, val_acc:0.984]
Epoch [115/120    avg_loss:0.003, val_acc:0.983]
Epoch [116/120    avg_loss:0.004, val_acc:0.983]
Epoch [117/120    avg_loss:0.004, val_acc:0.982]
Epoch [118/120    avg_loss:0.004, val_acc:0.983]
Epoch [119/120    avg_loss:0.004, val_acc:0.983]
Epoch [120/120    avg_loss:0.003, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1253   11    0    2    1    0    0    0    7   11    0    0
     0    0    0]
 [   0    0    0  725    2    0    0    0    0    2    2    9    6    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    1    0    0    0    0  851   16    0    0
     0    1    0]
 [   0    0    8    0    0    0    0    0    0    0    3 2180   17    0
     0    2    0]
 [   0    0    0    6    0    0    0    0    0    0    0    0  519    0
     0    7    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1125   14    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    45  297    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.91869918699187

F1 scores:
[       nan 1.         0.98197492 0.97380792 0.9953271  0.99541284
 0.99469295 1.         1.         0.92307692 0.97928654 0.98508812
 0.96200185 0.99730458 0.97402597 0.88922156 0.97005988]

Kappa:
0.9762668515972742
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2ddcbf16d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 103193==>0.10M
----------Training process----------
Epoch [1/120    avg_loss:2.555, val_acc:0.372]
Epoch [2/120    avg_loss:2.055, val_acc:0.491]
Epoch [3/120    avg_loss:1.844, val_acc:0.524]
Epoch [4/120    avg_loss:1.618, val_acc:0.547]
Epoch [5/120    avg_loss:1.441, val_acc:0.599]
Epoch [6/120    avg_loss:1.248, val_acc:0.658]
Epoch [7/120    avg_loss:1.048, val_acc:0.723]
Epoch [8/120    avg_loss:0.868, val_acc:0.747]
Epoch [9/120    avg_loss:0.828, val_acc:0.757]
Epoch [10/120    avg_loss:0.662, val_acc:0.771]
Epoch [11/120    avg_loss:0.614, val_acc:0.764]
Epoch [12/120    avg_loss:0.652, val_acc:0.798]
Epoch [13/120    avg_loss:0.580, val_acc:0.807]
Epoch [14/120    avg_loss:0.462, val_acc:0.822]
Epoch [15/120    avg_loss:0.429, val_acc:0.767]
Epoch [16/120    avg_loss:0.522, val_acc:0.824]
Epoch [17/120    avg_loss:0.402, val_acc:0.847]
Epoch [18/120    avg_loss:0.309, val_acc:0.848]
Epoch [19/120    avg_loss:0.349, val_acc:0.856]
Epoch [20/120    avg_loss:0.220, val_acc:0.892]
Epoch [21/120    avg_loss:0.182, val_acc:0.919]
Epoch [22/120    avg_loss:0.157, val_acc:0.917]
Epoch [23/120    avg_loss:0.171, val_acc:0.884]
Epoch [24/120    avg_loss:0.194, val_acc:0.894]
Epoch [25/120    avg_loss:0.146, val_acc:0.902]
Epoch [26/120    avg_loss:0.178, val_acc:0.921]
Epoch [27/120    avg_loss:0.138, val_acc:0.941]
Epoch [28/120    avg_loss:0.112, val_acc:0.927]
Epoch [29/120    avg_loss:0.124, val_acc:0.932]
Epoch [30/120    avg_loss:0.089, val_acc:0.946]
Epoch [31/120    avg_loss:0.074, val_acc:0.956]
Epoch [32/120    avg_loss:0.070, val_acc:0.950]
Epoch [33/120    avg_loss:0.066, val_acc:0.956]
Epoch [34/120    avg_loss:0.068, val_acc:0.959]
Epoch [35/120    avg_loss:0.068, val_acc:0.950]
Epoch [36/120    avg_loss:0.055, val_acc:0.957]
Epoch [37/120    avg_loss:0.046, val_acc:0.955]
Epoch [38/120    avg_loss:0.043, val_acc:0.967]
Epoch [39/120    avg_loss:0.051, val_acc:0.956]
Epoch [40/120    avg_loss:0.040, val_acc:0.958]
Epoch [41/120    avg_loss:0.036, val_acc:0.968]
Epoch [42/120    avg_loss:0.039, val_acc:0.968]
Epoch [43/120    avg_loss:0.045, val_acc:0.933]
Epoch [44/120    avg_loss:0.051, val_acc:0.958]
Epoch [45/120    avg_loss:0.039, val_acc:0.953]
Epoch [46/120    avg_loss:0.034, val_acc:0.975]
Epoch [47/120    avg_loss:0.033, val_acc:0.963]
Epoch [48/120    avg_loss:0.030, val_acc:0.949]
Epoch [49/120    avg_loss:0.027, val_acc:0.968]
Epoch [50/120    avg_loss:0.023, val_acc:0.976]
Epoch [51/120    avg_loss:0.036, val_acc:0.954]
Epoch [52/120    avg_loss:0.030, val_acc:0.969]
Epoch [53/120    avg_loss:0.023, val_acc:0.968]
Epoch [54/120    avg_loss:0.022, val_acc:0.970]
Epoch [55/120    avg_loss:0.018, val_acc:0.978]
Epoch [56/120    avg_loss:0.017, val_acc:0.971]
Epoch [57/120    avg_loss:0.017, val_acc:0.979]
Epoch [58/120    avg_loss:0.019, val_acc:0.972]
Epoch [59/120    avg_loss:0.021, val_acc:0.967]
Epoch [60/120    avg_loss:0.022, val_acc:0.972]
Epoch [61/120    avg_loss:0.026, val_acc:0.969]
Epoch [62/120    avg_loss:0.028, val_acc:0.969]
Epoch [63/120    avg_loss:0.024, val_acc:0.977]
Epoch [64/120    avg_loss:0.032, val_acc:0.975]
Epoch [65/120    avg_loss:0.015, val_acc:0.974]
Epoch [66/120    avg_loss:0.015, val_acc:0.974]
Epoch [67/120    avg_loss:0.012, val_acc:0.976]
Epoch [68/120    avg_loss:0.014, val_acc:0.981]
Epoch [69/120    avg_loss:0.015, val_acc:0.979]
Epoch [70/120    avg_loss:0.013, val_acc:0.978]
Epoch [71/120    avg_loss:0.012, val_acc:0.979]
Epoch [72/120    avg_loss:0.013, val_acc:0.973]
Epoch [73/120    avg_loss:0.020, val_acc:0.982]
Epoch [74/120    avg_loss:0.012, val_acc:0.980]
Epoch [75/120    avg_loss:0.011, val_acc:0.978]
Epoch [76/120    avg_loss:0.012, val_acc:0.973]
Epoch [77/120    avg_loss:0.010, val_acc:0.982]
Epoch [78/120    avg_loss:0.008, val_acc:0.978]
Epoch [79/120    avg_loss:0.007, val_acc:0.978]
Epoch [80/120    avg_loss:0.010, val_acc:0.970]
Epoch [81/120    avg_loss:0.011, val_acc:0.979]
Epoch [82/120    avg_loss:0.013, val_acc:0.982]
Epoch [83/120    avg_loss:0.008, val_acc:0.980]
Epoch [84/120    avg_loss:0.007, val_acc:0.981]
Epoch [85/120    avg_loss:0.007, val_acc:0.985]
Epoch [86/120    avg_loss:0.010, val_acc:0.985]
Epoch [87/120    avg_loss:0.009, val_acc:0.981]
Epoch [88/120    avg_loss:0.006, val_acc:0.981]
Epoch [89/120    avg_loss:0.006, val_acc:0.978]
Epoch [90/120    avg_loss:0.006, val_acc:0.984]
Epoch [91/120    avg_loss:0.006, val_acc:0.982]
Epoch [92/120    avg_loss:0.005, val_acc:0.984]
Epoch [93/120    avg_loss:0.005, val_acc:0.981]
Epoch [94/120    avg_loss:0.006, val_acc:0.986]
Epoch [95/120    avg_loss:0.004, val_acc:0.983]
Epoch [96/120    avg_loss:0.004, val_acc:0.986]
Epoch [97/120    avg_loss:0.004, val_acc:0.981]
Epoch [98/120    avg_loss:0.005, val_acc:0.981]
Epoch [99/120    avg_loss:0.004, val_acc:0.982]
Epoch [100/120    avg_loss:0.004, val_acc:0.982]
Epoch [101/120    avg_loss:0.005, val_acc:0.964]
Epoch [102/120    avg_loss:0.008, val_acc:0.982]
Epoch [103/120    avg_loss:0.005, val_acc:0.983]
Epoch [104/120    avg_loss:0.004, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.986]
Epoch [106/120    avg_loss:0.004, val_acc:0.984]
Epoch [107/120    avg_loss:0.005, val_acc:0.973]
Epoch [108/120    avg_loss:0.007, val_acc:0.979]
Epoch [109/120    avg_loss:0.018, val_acc:0.971]
Epoch [110/120    avg_loss:0.010, val_acc:0.976]
Epoch [111/120    avg_loss:0.008, val_acc:0.977]
Epoch [112/120    avg_loss:0.010, val_acc:0.971]
Epoch [113/120    avg_loss:0.018, val_acc:0.970]
Epoch [114/120    avg_loss:0.020, val_acc:0.976]
Epoch [115/120    avg_loss:0.015, val_acc:0.983]
Epoch [116/120    avg_loss:0.009, val_acc:0.976]
Epoch [117/120    avg_loss:0.012, val_acc:0.977]
Epoch [118/120    avg_loss:0.012, val_acc:0.972]
Epoch [119/120    avg_loss:0.011, val_acc:0.975]
Epoch [120/120    avg_loss:0.008, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1268    0    3    5    1    0    0    0    1    7    0    0
     0    0    0]
 [   0    0    0  725    1    0    1    0    0    1    0    1   14    3
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    5    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    1    0    0
     0    0    0]
 [   0    0    4    0    0    2    0    0    0    1  839   29    0    0
     0    0    0]
 [   0    0   24    0    0    2    0    0    0    0    8 2157   15    0
     0    2    2]
 [   0    0    0    5    0    0    0    0    0    0    0    0  525    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1127   12    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    75  264    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.3550135501355

F1 scores:
[       nan 0.975      0.9825649  0.9817197  0.99069767 0.98165138
 0.99168556 0.90909091 1.         0.91891892 0.97275362 0.97911938
 0.96418733 0.9919571  0.9616041  0.84210526 0.97076023]

Kappa:
0.9698365646740356
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb701fd6748>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.542, val_acc:0.454]
Epoch [2/120    avg_loss:2.014, val_acc:0.557]
Epoch [3/120    avg_loss:1.724, val_acc:0.598]
Epoch [4/120    avg_loss:1.584, val_acc:0.615]
Epoch [5/120    avg_loss:1.378, val_acc:0.657]
Epoch [6/120    avg_loss:1.251, val_acc:0.680]
Epoch [7/120    avg_loss:1.008, val_acc:0.704]
Epoch [8/120    avg_loss:0.892, val_acc:0.698]
Epoch [9/120    avg_loss:0.794, val_acc:0.718]
Epoch [10/120    avg_loss:0.637, val_acc:0.744]
Epoch [11/120    avg_loss:0.545, val_acc:0.778]
Epoch [12/120    avg_loss:0.453, val_acc:0.798]
Epoch [13/120    avg_loss:0.563, val_acc:0.790]
Epoch [14/120    avg_loss:0.731, val_acc:0.780]
Epoch [15/120    avg_loss:0.518, val_acc:0.846]
Epoch [16/120    avg_loss:0.341, val_acc:0.812]
Epoch [17/120    avg_loss:0.303, val_acc:0.852]
Epoch [18/120    avg_loss:0.241, val_acc:0.904]
Epoch [19/120    avg_loss:0.217, val_acc:0.886]
Epoch [20/120    avg_loss:0.173, val_acc:0.884]
Epoch [21/120    avg_loss:0.204, val_acc:0.878]
Epoch [22/120    avg_loss:0.192, val_acc:0.910]
Epoch [23/120    avg_loss:0.152, val_acc:0.905]
Epoch [24/120    avg_loss:0.144, val_acc:0.924]
Epoch [25/120    avg_loss:0.125, val_acc:0.929]
Epoch [26/120    avg_loss:0.099, val_acc:0.935]
Epoch [27/120    avg_loss:0.098, val_acc:0.933]
Epoch [28/120    avg_loss:0.088, val_acc:0.946]
Epoch [29/120    avg_loss:0.090, val_acc:0.942]
Epoch [30/120    avg_loss:0.091, val_acc:0.942]
Epoch [31/120    avg_loss:0.102, val_acc:0.947]
Epoch [32/120    avg_loss:0.088, val_acc:0.943]
Epoch [33/120    avg_loss:0.069, val_acc:0.927]
Epoch [34/120    avg_loss:0.073, val_acc:0.956]
Epoch [35/120    avg_loss:0.061, val_acc:0.946]
Epoch [36/120    avg_loss:0.066, val_acc:0.957]
Epoch [37/120    avg_loss:0.050, val_acc:0.963]
Epoch [38/120    avg_loss:0.052, val_acc:0.964]
Epoch [39/120    avg_loss:0.054, val_acc:0.965]
Epoch [40/120    avg_loss:0.049, val_acc:0.963]
Epoch [41/120    avg_loss:0.045, val_acc:0.969]
Epoch [42/120    avg_loss:0.040, val_acc:0.967]
Epoch [43/120    avg_loss:0.041, val_acc:0.969]
Epoch [44/120    avg_loss:0.032, val_acc:0.966]
Epoch [45/120    avg_loss:0.028, val_acc:0.966]
Epoch [46/120    avg_loss:0.026, val_acc:0.970]
Epoch [47/120    avg_loss:0.026, val_acc:0.968]
Epoch [48/120    avg_loss:0.025, val_acc:0.976]
Epoch [49/120    avg_loss:0.026, val_acc:0.971]
Epoch [50/120    avg_loss:0.030, val_acc:0.966]
Epoch [51/120    avg_loss:0.043, val_acc:0.960]
Epoch [52/120    avg_loss:0.032, val_acc:0.960]
Epoch [53/120    avg_loss:0.021, val_acc:0.975]
Epoch [54/120    avg_loss:0.017, val_acc:0.976]
Epoch [55/120    avg_loss:0.019, val_acc:0.975]
Epoch [56/120    avg_loss:0.021, val_acc:0.974]
Epoch [57/120    avg_loss:0.016, val_acc:0.973]
Epoch [58/120    avg_loss:0.014, val_acc:0.976]
Epoch [59/120    avg_loss:0.013, val_acc:0.981]
Epoch [60/120    avg_loss:0.017, val_acc:0.968]
Epoch [61/120    avg_loss:0.019, val_acc:0.975]
Epoch [62/120    avg_loss:0.024, val_acc:0.958]
Epoch [63/120    avg_loss:0.053, val_acc:0.968]
Epoch [64/120    avg_loss:0.027, val_acc:0.966]
Epoch [65/120    avg_loss:0.025, val_acc:0.973]
Epoch [66/120    avg_loss:0.030, val_acc:0.969]
Epoch [67/120    avg_loss:0.037, val_acc:0.975]
Epoch [68/120    avg_loss:0.024, val_acc:0.975]
Epoch [69/120    avg_loss:0.018, val_acc:0.981]
Epoch [70/120    avg_loss:0.021, val_acc:0.963]
Epoch [71/120    avg_loss:0.021, val_acc:0.982]
Epoch [72/120    avg_loss:0.018, val_acc:0.976]
Epoch [73/120    avg_loss:0.030, val_acc:0.960]
Epoch [74/120    avg_loss:0.022, val_acc:0.975]
Epoch [75/120    avg_loss:0.016, val_acc:0.983]
Epoch [76/120    avg_loss:0.013, val_acc:0.979]
Epoch [77/120    avg_loss:0.011, val_acc:0.980]
Epoch [78/120    avg_loss:0.013, val_acc:0.977]
Epoch [79/120    avg_loss:0.012, val_acc:0.980]
Epoch [80/120    avg_loss:0.016, val_acc:0.981]
Epoch [81/120    avg_loss:0.015, val_acc:0.980]
Epoch [82/120    avg_loss:0.014, val_acc:0.984]
Epoch [83/120    avg_loss:0.010, val_acc:0.977]
Epoch [84/120    avg_loss:0.013, val_acc:0.972]
Epoch [85/120    avg_loss:0.012, val_acc:0.979]
Epoch [86/120    avg_loss:0.007, val_acc:0.980]
Epoch [87/120    avg_loss:0.011, val_acc:0.982]
Epoch [88/120    avg_loss:0.009, val_acc:0.980]
Epoch [89/120    avg_loss:0.009, val_acc:0.982]
Epoch [90/120    avg_loss:0.014, val_acc:0.970]
Epoch [91/120    avg_loss:0.013, val_acc:0.980]
Epoch [92/120    avg_loss:0.014, val_acc:0.958]
Epoch [93/120    avg_loss:0.015, val_acc:0.977]
Epoch [94/120    avg_loss:0.013, val_acc:0.981]
Epoch [95/120    avg_loss:0.010, val_acc:0.980]
Epoch [96/120    avg_loss:0.011, val_acc:0.982]
Epoch [97/120    avg_loss:0.008, val_acc:0.984]
Epoch [98/120    avg_loss:0.009, val_acc:0.984]
Epoch [99/120    avg_loss:0.006, val_acc:0.984]
Epoch [100/120    avg_loss:0.007, val_acc:0.985]
Epoch [101/120    avg_loss:0.006, val_acc:0.985]
Epoch [102/120    avg_loss:0.005, val_acc:0.986]
Epoch [103/120    avg_loss:0.006, val_acc:0.986]
Epoch [104/120    avg_loss:0.007, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.985]
Epoch [107/120    avg_loss:0.005, val_acc:0.985]
Epoch [108/120    avg_loss:0.005, val_acc:0.985]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.004, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.986]
Epoch [114/120    avg_loss:0.005, val_acc:0.986]
Epoch [115/120    avg_loss:0.004, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.005, val_acc:0.985]
Epoch [118/120    avg_loss:0.005, val_acc:0.985]
Epoch [119/120    avg_loss:0.005, val_acc:0.985]
Epoch [120/120    avg_loss:0.004, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    3    6    0    0    0    0    1    4   11    0    0
     0    0    0]
 [   0    0    0  720    1   10    1    0    0    6    2    1    4    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    5    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    1    0    0    0    0  863    7    0    0
     0    2    0]
 [   0    0   14    0    0    0    0    0    0    0    3 2186    6    1
     0    0    0]
 [   0    0    0    1    0    1    0    0    0    0    0    0  530    0
     1    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1132    6    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    13  330    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.5691056910569

F1 scores:
[       nan 0.98765432 0.98360656 0.97826087 0.98383372 0.97945205
 0.99468489 0.90909091 0.99650757 0.79069767 0.98797939 0.98981209
 0.98148148 0.9919571  0.99080963 0.96350365 0.96969697]

Kappa:
0.9836905601144097
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3b0feac710>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.592, val_acc:0.483]
Epoch [2/120    avg_loss:2.052, val_acc:0.519]
Epoch [3/120    avg_loss:1.755, val_acc:0.598]
Epoch [4/120    avg_loss:1.579, val_acc:0.622]
Epoch [5/120    avg_loss:1.406, val_acc:0.591]
Epoch [6/120    avg_loss:1.254, val_acc:0.728]
Epoch [7/120    avg_loss:1.061, val_acc:0.741]
Epoch [8/120    avg_loss:0.899, val_acc:0.781]
Epoch [9/120    avg_loss:0.742, val_acc:0.820]
Epoch [10/120    avg_loss:0.687, val_acc:0.819]
Epoch [11/120    avg_loss:0.554, val_acc:0.819]
Epoch [12/120    avg_loss:0.468, val_acc:0.855]
Epoch [13/120    avg_loss:0.418, val_acc:0.843]
Epoch [14/120    avg_loss:0.323, val_acc:0.855]
Epoch [15/120    avg_loss:0.310, val_acc:0.836]
Epoch [16/120    avg_loss:0.286, val_acc:0.854]
Epoch [17/120    avg_loss:0.235, val_acc:0.893]
Epoch [18/120    avg_loss:0.197, val_acc:0.936]
Epoch [19/120    avg_loss:0.210, val_acc:0.909]
Epoch [20/120    avg_loss:0.194, val_acc:0.916]
Epoch [21/120    avg_loss:0.208, val_acc:0.906]
Epoch [22/120    avg_loss:0.167, val_acc:0.940]
Epoch [23/120    avg_loss:0.129, val_acc:0.931]
Epoch [24/120    avg_loss:0.146, val_acc:0.939]
Epoch [25/120    avg_loss:0.164, val_acc:0.935]
Epoch [26/120    avg_loss:0.149, val_acc:0.948]
Epoch [27/120    avg_loss:0.115, val_acc:0.955]
Epoch [28/120    avg_loss:0.098, val_acc:0.953]
Epoch [29/120    avg_loss:0.100, val_acc:0.945]
Epoch [30/120    avg_loss:0.096, val_acc:0.956]
Epoch [31/120    avg_loss:0.073, val_acc:0.960]
Epoch [32/120    avg_loss:0.072, val_acc:0.963]
Epoch [33/120    avg_loss:0.065, val_acc:0.952]
Epoch [34/120    avg_loss:0.060, val_acc:0.958]
Epoch [35/120    avg_loss:0.067, val_acc:0.963]
Epoch [36/120    avg_loss:0.061, val_acc:0.953]
Epoch [37/120    avg_loss:0.064, val_acc:0.958]
Epoch [38/120    avg_loss:0.062, val_acc:0.973]
Epoch [39/120    avg_loss:0.049, val_acc:0.970]
Epoch [40/120    avg_loss:0.035, val_acc:0.971]
Epoch [41/120    avg_loss:0.036, val_acc:0.973]
Epoch [42/120    avg_loss:0.027, val_acc:0.976]
Epoch [43/120    avg_loss:0.032, val_acc:0.972]
Epoch [44/120    avg_loss:0.038, val_acc:0.965]
Epoch [45/120    avg_loss:0.028, val_acc:0.976]
Epoch [46/120    avg_loss:0.041, val_acc:0.968]
Epoch [47/120    avg_loss:0.043, val_acc:0.974]
Epoch [48/120    avg_loss:0.034, val_acc:0.973]
Epoch [49/120    avg_loss:0.037, val_acc:0.972]
Epoch [50/120    avg_loss:0.033, val_acc:0.974]
Epoch [51/120    avg_loss:0.021, val_acc:0.972]
Epoch [52/120    avg_loss:0.026, val_acc:0.970]
Epoch [53/120    avg_loss:0.030, val_acc:0.975]
Epoch [54/120    avg_loss:0.023, val_acc:0.971]
Epoch [55/120    avg_loss:0.020, val_acc:0.973]
Epoch [56/120    avg_loss:0.026, val_acc:0.973]
Epoch [57/120    avg_loss:0.031, val_acc:0.968]
Epoch [58/120    avg_loss:0.039, val_acc:0.973]
Epoch [59/120    avg_loss:0.022, val_acc:0.973]
Epoch [60/120    avg_loss:0.019, val_acc:0.977]
Epoch [61/120    avg_loss:0.016, val_acc:0.977]
Epoch [62/120    avg_loss:0.016, val_acc:0.978]
Epoch [63/120    avg_loss:0.017, val_acc:0.978]
Epoch [64/120    avg_loss:0.012, val_acc:0.979]
Epoch [65/120    avg_loss:0.018, val_acc:0.982]
Epoch [66/120    avg_loss:0.014, val_acc:0.982]
Epoch [67/120    avg_loss:0.014, val_acc:0.981]
Epoch [68/120    avg_loss:0.017, val_acc:0.981]
Epoch [69/120    avg_loss:0.012, val_acc:0.980]
Epoch [70/120    avg_loss:0.011, val_acc:0.980]
Epoch [71/120    avg_loss:0.011, val_acc:0.980]
Epoch [72/120    avg_loss:0.012, val_acc:0.980]
Epoch [73/120    avg_loss:0.011, val_acc:0.980]
Epoch [74/120    avg_loss:0.013, val_acc:0.979]
Epoch [75/120    avg_loss:0.016, val_acc:0.978]
Epoch [76/120    avg_loss:0.012, val_acc:0.978]
Epoch [77/120    avg_loss:0.012, val_acc:0.977]
Epoch [78/120    avg_loss:0.013, val_acc:0.977]
Epoch [79/120    avg_loss:0.011, val_acc:0.978]
Epoch [80/120    avg_loss:0.010, val_acc:0.978]
Epoch [81/120    avg_loss:0.012, val_acc:0.978]
Epoch [82/120    avg_loss:0.011, val_acc:0.978]
Epoch [83/120    avg_loss:0.013, val_acc:0.978]
Epoch [84/120    avg_loss:0.008, val_acc:0.978]
Epoch [85/120    avg_loss:0.014, val_acc:0.978]
Epoch [86/120    avg_loss:0.014, val_acc:0.978]
Epoch [87/120    avg_loss:0.011, val_acc:0.978]
Epoch [88/120    avg_loss:0.010, val_acc:0.978]
Epoch [89/120    avg_loss:0.013, val_acc:0.978]
Epoch [90/120    avg_loss:0.013, val_acc:0.978]
Epoch [91/120    avg_loss:0.013, val_acc:0.978]
Epoch [92/120    avg_loss:0.009, val_acc:0.978]
Epoch [93/120    avg_loss:0.012, val_acc:0.978]
Epoch [94/120    avg_loss:0.011, val_acc:0.978]
Epoch [95/120    avg_loss:0.016, val_acc:0.978]
Epoch [96/120    avg_loss:0.013, val_acc:0.978]
Epoch [97/120    avg_loss:0.012, val_acc:0.978]
Epoch [98/120    avg_loss:0.012, val_acc:0.978]
Epoch [99/120    avg_loss:0.010, val_acc:0.978]
Epoch [100/120    avg_loss:0.013, val_acc:0.978]
Epoch [101/120    avg_loss:0.016, val_acc:0.978]
Epoch [102/120    avg_loss:0.015, val_acc:0.978]
Epoch [103/120    avg_loss:0.010, val_acc:0.978]
Epoch [104/120    avg_loss:0.010, val_acc:0.978]
Epoch [105/120    avg_loss:0.010, val_acc:0.978]
Epoch [106/120    avg_loss:0.013, val_acc:0.978]
Epoch [107/120    avg_loss:0.011, val_acc:0.978]
Epoch [108/120    avg_loss:0.011, val_acc:0.978]
Epoch [109/120    avg_loss:0.015, val_acc:0.978]
Epoch [110/120    avg_loss:0.012, val_acc:0.978]
Epoch [111/120    avg_loss:0.013, val_acc:0.978]
Epoch [112/120    avg_loss:0.010, val_acc:0.978]
Epoch [113/120    avg_loss:0.012, val_acc:0.978]
Epoch [114/120    avg_loss:0.010, val_acc:0.978]
Epoch [115/120    avg_loss:0.011, val_acc:0.978]
Epoch [116/120    avg_loss:0.013, val_acc:0.978]
Epoch [117/120    avg_loss:0.011, val_acc:0.978]
Epoch [118/120    avg_loss:0.011, val_acc:0.978]
Epoch [119/120    avg_loss:0.019, val_acc:0.978]
Epoch [120/120    avg_loss:0.010, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1268    0    0    1    3    0    0    0    4    7    0    0
     0    2    0]
 [   0    0    0  731    0    0    2    0    0    3    3    0    4    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    1    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    2    3    0    0    0  851   17    0    0
     0    1    0]
 [   0    0   18    0    0    0    1    0    0    0    3 2180    7    1
     0    0    0]
 [   0    0    0    7    0    1    0    0    0    0    0    1  523    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    1    0    0    0
  1126   10    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    11  328    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.42818428184282

F1 scores:
[       nan 0.94871795 0.98600311 0.98451178 1.         0.9862069
 0.98570354 0.98039216 0.99883586 0.9        0.97759908 0.98709531
 0.97665733 0.98666667 0.9877193  0.95348837 0.97619048]

Kappa:
0.9820814525026108
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f42374d76a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.531, val_acc:0.466]
Epoch [2/120    avg_loss:1.995, val_acc:0.493]
Epoch [3/120    avg_loss:1.767, val_acc:0.599]
Epoch [4/120    avg_loss:1.616, val_acc:0.610]
Epoch [5/120    avg_loss:1.438, val_acc:0.623]
Epoch [6/120    avg_loss:1.210, val_acc:0.661]
Epoch [7/120    avg_loss:1.140, val_acc:0.726]
Epoch [8/120    avg_loss:0.976, val_acc:0.722]
Epoch [9/120    avg_loss:0.923, val_acc:0.732]
Epoch [10/120    avg_loss:0.764, val_acc:0.748]
Epoch [11/120    avg_loss:0.737, val_acc:0.787]
Epoch [12/120    avg_loss:0.588, val_acc:0.852]
Epoch [13/120    avg_loss:0.505, val_acc:0.824]
Epoch [14/120    avg_loss:0.487, val_acc:0.842]
Epoch [15/120    avg_loss:0.389, val_acc:0.859]
Epoch [16/120    avg_loss:0.324, val_acc:0.845]
Epoch [17/120    avg_loss:0.315, val_acc:0.900]
Epoch [18/120    avg_loss:0.254, val_acc:0.895]
Epoch [19/120    avg_loss:0.230, val_acc:0.891]
Epoch [20/120    avg_loss:0.221, val_acc:0.913]
Epoch [21/120    avg_loss:0.178, val_acc:0.923]
Epoch [22/120    avg_loss:0.174, val_acc:0.909]
Epoch [23/120    avg_loss:0.188, val_acc:0.909]
Epoch [24/120    avg_loss:0.386, val_acc:0.811]
Epoch [25/120    avg_loss:0.449, val_acc:0.883]
Epoch [26/120    avg_loss:0.316, val_acc:0.883]
Epoch [27/120    avg_loss:0.199, val_acc:0.923]
Epoch [28/120    avg_loss:0.129, val_acc:0.930]
Epoch [29/120    avg_loss:0.122, val_acc:0.927]
Epoch [30/120    avg_loss:0.126, val_acc:0.933]
Epoch [31/120    avg_loss:0.109, val_acc:0.938]
Epoch [32/120    avg_loss:0.100, val_acc:0.940]
Epoch [33/120    avg_loss:0.094, val_acc:0.953]
Epoch [34/120    avg_loss:0.074, val_acc:0.952]
Epoch [35/120    avg_loss:0.073, val_acc:0.955]
Epoch [36/120    avg_loss:0.055, val_acc:0.958]
Epoch [37/120    avg_loss:0.067, val_acc:0.954]
Epoch [38/120    avg_loss:0.064, val_acc:0.948]
Epoch [39/120    avg_loss:0.044, val_acc:0.961]
Epoch [40/120    avg_loss:0.043, val_acc:0.960]
Epoch [41/120    avg_loss:0.062, val_acc:0.962]
Epoch [42/120    avg_loss:0.042, val_acc:0.961]
Epoch [43/120    avg_loss:0.035, val_acc:0.966]
Epoch [44/120    avg_loss:0.043, val_acc:0.968]
Epoch [45/120    avg_loss:0.041, val_acc:0.961]
Epoch [46/120    avg_loss:0.038, val_acc:0.948]
Epoch [47/120    avg_loss:0.045, val_acc:0.964]
Epoch [48/120    avg_loss:0.037, val_acc:0.973]
Epoch [49/120    avg_loss:0.032, val_acc:0.974]
Epoch [50/120    avg_loss:0.032, val_acc:0.968]
Epoch [51/120    avg_loss:0.029, val_acc:0.967]
Epoch [52/120    avg_loss:0.027, val_acc:0.970]
Epoch [53/120    avg_loss:0.023, val_acc:0.965]
Epoch [54/120    avg_loss:0.023, val_acc:0.968]
Epoch [55/120    avg_loss:0.025, val_acc:0.975]
Epoch [56/120    avg_loss:0.021, val_acc:0.976]
Epoch [57/120    avg_loss:0.018, val_acc:0.973]
Epoch [58/120    avg_loss:0.023, val_acc:0.968]
Epoch [59/120    avg_loss:0.024, val_acc:0.975]
Epoch [60/120    avg_loss:0.019, val_acc:0.971]
Epoch [61/120    avg_loss:0.016, val_acc:0.972]
Epoch [62/120    avg_loss:0.016, val_acc:0.978]
Epoch [63/120    avg_loss:0.015, val_acc:0.974]
Epoch [64/120    avg_loss:0.013, val_acc:0.974]
Epoch [65/120    avg_loss:0.017, val_acc:0.971]
Epoch [66/120    avg_loss:0.016, val_acc:0.972]
Epoch [67/120    avg_loss:0.011, val_acc:0.971]
Epoch [68/120    avg_loss:0.011, val_acc:0.975]
Epoch [69/120    avg_loss:0.010, val_acc:0.978]
Epoch [70/120    avg_loss:0.011, val_acc:0.979]
Epoch [71/120    avg_loss:0.008, val_acc:0.977]
Epoch [72/120    avg_loss:0.014, val_acc:0.976]
Epoch [73/120    avg_loss:0.021, val_acc:0.976]
Epoch [74/120    avg_loss:0.030, val_acc:0.952]
Epoch [75/120    avg_loss:0.026, val_acc:0.977]
Epoch [76/120    avg_loss:0.016, val_acc:0.980]
Epoch [77/120    avg_loss:0.014, val_acc:0.977]
Epoch [78/120    avg_loss:0.009, val_acc:0.979]
Epoch [79/120    avg_loss:0.010, val_acc:0.976]
Epoch [80/120    avg_loss:0.009, val_acc:0.975]
Epoch [81/120    avg_loss:0.013, val_acc:0.970]
Epoch [82/120    avg_loss:0.012, val_acc:0.972]
Epoch [83/120    avg_loss:0.010, val_acc:0.977]
Epoch [84/120    avg_loss:0.013, val_acc:0.977]
Epoch [85/120    avg_loss:0.010, val_acc:0.974]
Epoch [86/120    avg_loss:0.008, val_acc:0.977]
Epoch [87/120    avg_loss:0.011, val_acc:0.974]
Epoch [88/120    avg_loss:0.007, val_acc:0.975]
Epoch [89/120    avg_loss:0.010, val_acc:0.976]
Epoch [90/120    avg_loss:0.010, val_acc:0.979]
Epoch [91/120    avg_loss:0.007, val_acc:0.977]
Epoch [92/120    avg_loss:0.008, val_acc:0.978]
Epoch [93/120    avg_loss:0.006, val_acc:0.978]
Epoch [94/120    avg_loss:0.005, val_acc:0.978]
Epoch [95/120    avg_loss:0.005, val_acc:0.978]
Epoch [96/120    avg_loss:0.005, val_acc:0.977]
Epoch [97/120    avg_loss:0.006, val_acc:0.977]
Epoch [98/120    avg_loss:0.005, val_acc:0.978]
Epoch [99/120    avg_loss:0.006, val_acc:0.978]
Epoch [100/120    avg_loss:0.005, val_acc:0.978]
Epoch [101/120    avg_loss:0.005, val_acc:0.977]
Epoch [102/120    avg_loss:0.007, val_acc:0.977]
Epoch [103/120    avg_loss:0.005, val_acc:0.978]
Epoch [104/120    avg_loss:0.005, val_acc:0.978]
Epoch [105/120    avg_loss:0.004, val_acc:0.978]
Epoch [106/120    avg_loss:0.005, val_acc:0.978]
Epoch [107/120    avg_loss:0.006, val_acc:0.978]
Epoch [108/120    avg_loss:0.006, val_acc:0.978]
Epoch [109/120    avg_loss:0.005, val_acc:0.978]
Epoch [110/120    avg_loss:0.005, val_acc:0.978]
Epoch [111/120    avg_loss:0.005, val_acc:0.978]
Epoch [112/120    avg_loss:0.007, val_acc:0.978]
Epoch [113/120    avg_loss:0.005, val_acc:0.978]
Epoch [114/120    avg_loss:0.005, val_acc:0.978]
Epoch [115/120    avg_loss:0.005, val_acc:0.978]
Epoch [116/120    avg_loss:0.005, val_acc:0.978]
Epoch [117/120    avg_loss:0.006, val_acc:0.978]
Epoch [118/120    avg_loss:0.005, val_acc:0.978]
Epoch [119/120    avg_loss:0.005, val_acc:0.978]
Epoch [120/120    avg_loss:0.006, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1248    0    9    0    0    0    0    1    3   16    8    0
     0    0    0]
 [   0    0    0  713    7    4    0    0    0    6    1    1   13    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    3    0    1    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    2    2    0    0    0  859    6    0    0
     0    0    0]
 [   0    0   10    0    0    0    1    0    0    0    9 2186    4    0
     0    0    0]
 [   0    0    0    2    0    1    0    0    0    0    0    0  529    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0    0
  1128   10    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    48  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.94037940379404

F1 scores:
[       nan 0.96385542 0.97920753 0.9747095  0.9638009  0.98390805
 0.99619772 0.94339623 0.997669   0.79069767 0.98283753 0.9886929
 0.9706422  0.99462366 0.9732528  0.91019787 0.98203593]

Kappa:
0.9765182255010236
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f77d7c3d780>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.566, val_acc:0.471]
Epoch [2/120    avg_loss:2.071, val_acc:0.470]
Epoch [3/120    avg_loss:1.766, val_acc:0.507]
Epoch [4/120    avg_loss:1.553, val_acc:0.597]
Epoch [5/120    avg_loss:1.331, val_acc:0.637]
Epoch [6/120    avg_loss:1.173, val_acc:0.678]
Epoch [7/120    avg_loss:0.993, val_acc:0.710]
Epoch [8/120    avg_loss:0.861, val_acc:0.752]
Epoch [9/120    avg_loss:0.817, val_acc:0.753]
Epoch [10/120    avg_loss:0.680, val_acc:0.772]
Epoch [11/120    avg_loss:0.555, val_acc:0.772]
Epoch [12/120    avg_loss:0.506, val_acc:0.816]
Epoch [13/120    avg_loss:0.409, val_acc:0.833]
Epoch [14/120    avg_loss:0.412, val_acc:0.830]
Epoch [15/120    avg_loss:0.382, val_acc:0.841]
Epoch [16/120    avg_loss:0.363, val_acc:0.840]
Epoch [17/120    avg_loss:0.311, val_acc:0.879]
Epoch [18/120    avg_loss:0.231, val_acc:0.909]
Epoch [19/120    avg_loss:0.194, val_acc:0.905]
Epoch [20/120    avg_loss:0.197, val_acc:0.918]
Epoch [21/120    avg_loss:0.167, val_acc:0.929]
Epoch [22/120    avg_loss:0.172, val_acc:0.921]
Epoch [23/120    avg_loss:0.175, val_acc:0.923]
Epoch [24/120    avg_loss:0.136, val_acc:0.930]
Epoch [25/120    avg_loss:0.119, val_acc:0.925]
Epoch [26/120    avg_loss:0.101, val_acc:0.941]
Epoch [27/120    avg_loss:0.094, val_acc:0.930]
Epoch [28/120    avg_loss:0.107, val_acc:0.938]
Epoch [29/120    avg_loss:0.214, val_acc:0.900]
Epoch [30/120    avg_loss:0.166, val_acc:0.941]
Epoch [31/120    avg_loss:0.135, val_acc:0.953]
Epoch [32/120    avg_loss:0.162, val_acc:0.914]
Epoch [33/120    avg_loss:0.130, val_acc:0.939]
Epoch [34/120    avg_loss:0.098, val_acc:0.951]
Epoch [35/120    avg_loss:0.073, val_acc:0.962]
Epoch [36/120    avg_loss:0.068, val_acc:0.950]
Epoch [37/120    avg_loss:0.074, val_acc:0.963]
Epoch [38/120    avg_loss:0.078, val_acc:0.962]
Epoch [39/120    avg_loss:0.066, val_acc:0.950]
Epoch [40/120    avg_loss:0.059, val_acc:0.955]
Epoch [41/120    avg_loss:0.064, val_acc:0.962]
Epoch [42/120    avg_loss:0.041, val_acc:0.968]
Epoch [43/120    avg_loss:0.050, val_acc:0.960]
Epoch [44/120    avg_loss:0.042, val_acc:0.970]
Epoch [45/120    avg_loss:0.031, val_acc:0.959]
Epoch [46/120    avg_loss:0.028, val_acc:0.970]
Epoch [47/120    avg_loss:0.036, val_acc:0.962]
Epoch [48/120    avg_loss:0.040, val_acc:0.967]
Epoch [49/120    avg_loss:0.032, val_acc:0.963]
Epoch [50/120    avg_loss:0.029, val_acc:0.964]
Epoch [51/120    avg_loss:0.026, val_acc:0.963]
Epoch [52/120    avg_loss:0.034, val_acc:0.974]
Epoch [53/120    avg_loss:0.022, val_acc:0.967]
Epoch [54/120    avg_loss:0.022, val_acc:0.966]
Epoch [55/120    avg_loss:0.027, val_acc:0.968]
Epoch [56/120    avg_loss:0.027, val_acc:0.968]
Epoch [57/120    avg_loss:0.023, val_acc:0.963]
Epoch [58/120    avg_loss:0.030, val_acc:0.952]
Epoch [59/120    avg_loss:0.029, val_acc:0.978]
Epoch [60/120    avg_loss:0.039, val_acc:0.962]
Epoch [61/120    avg_loss:0.029, val_acc:0.946]
Epoch [62/120    avg_loss:0.035, val_acc:0.961]
Epoch [63/120    avg_loss:0.018, val_acc:0.980]
Epoch [64/120    avg_loss:0.016, val_acc:0.970]
Epoch [65/120    avg_loss:0.017, val_acc:0.980]
Epoch [66/120    avg_loss:0.012, val_acc:0.973]
Epoch [67/120    avg_loss:0.012, val_acc:0.977]
Epoch [68/120    avg_loss:0.009, val_acc:0.974]
Epoch [69/120    avg_loss:0.012, val_acc:0.975]
Epoch [70/120    avg_loss:0.015, val_acc:0.964]
Epoch [71/120    avg_loss:0.017, val_acc:0.974]
Epoch [72/120    avg_loss:0.013, val_acc:0.973]
Epoch [73/120    avg_loss:0.012, val_acc:0.980]
Epoch [74/120    avg_loss:0.011, val_acc:0.977]
Epoch [75/120    avg_loss:0.014, val_acc:0.970]
Epoch [76/120    avg_loss:0.014, val_acc:0.971]
Epoch [77/120    avg_loss:0.015, val_acc:0.976]
Epoch [78/120    avg_loss:0.009, val_acc:0.976]
Epoch [79/120    avg_loss:0.012, val_acc:0.976]
Epoch [80/120    avg_loss:0.014, val_acc:0.974]
Epoch [81/120    avg_loss:0.016, val_acc:0.980]
Epoch [82/120    avg_loss:0.010, val_acc:0.980]
Epoch [83/120    avg_loss:0.009, val_acc:0.973]
Epoch [84/120    avg_loss:0.009, val_acc:0.974]
Epoch [85/120    avg_loss:0.009, val_acc:0.972]
Epoch [86/120    avg_loss:0.008, val_acc:0.977]
Epoch [87/120    avg_loss:0.007, val_acc:0.978]
Epoch [88/120    avg_loss:0.006, val_acc:0.975]
Epoch [89/120    avg_loss:0.005, val_acc:0.979]
Epoch [90/120    avg_loss:0.006, val_acc:0.976]
Epoch [91/120    avg_loss:0.007, val_acc:0.973]
Epoch [92/120    avg_loss:0.010, val_acc:0.974]
Epoch [93/120    avg_loss:0.014, val_acc:0.975]
Epoch [94/120    avg_loss:0.012, val_acc:0.977]
Epoch [95/120    avg_loss:0.025, val_acc:0.973]
Epoch [96/120    avg_loss:0.010, val_acc:0.977]
Epoch [97/120    avg_loss:0.008, val_acc:0.979]
Epoch [98/120    avg_loss:0.010, val_acc:0.979]
Epoch [99/120    avg_loss:0.007, val_acc:0.982]
Epoch [100/120    avg_loss:0.006, val_acc:0.980]
Epoch [101/120    avg_loss:0.006, val_acc:0.983]
Epoch [102/120    avg_loss:0.006, val_acc:0.984]
Epoch [103/120    avg_loss:0.005, val_acc:0.984]
Epoch [104/120    avg_loss:0.005, val_acc:0.984]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.005, val_acc:0.984]
Epoch [107/120    avg_loss:0.007, val_acc:0.984]
Epoch [108/120    avg_loss:0.005, val_acc:0.983]
Epoch [109/120    avg_loss:0.006, val_acc:0.979]
Epoch [110/120    avg_loss:0.005, val_acc:0.983]
Epoch [111/120    avg_loss:0.005, val_acc:0.984]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.006, val_acc:0.984]
Epoch [117/120    avg_loss:0.005, val_acc:0.983]
Epoch [118/120    avg_loss:0.005, val_acc:0.983]
Epoch [119/120    avg_loss:0.005, val_acc:0.985]
Epoch [120/120    avg_loss:0.005, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1263    0    1    0    2    0    0    1    9    8    0    0
     0    1    0]
 [   0    0    0  716    4    4    0    0    0    8    1    9    4    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    5    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    8    0    0    0    0    0    0  422    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    1    3    0    0    0  857   11    0    0
     0    3    0]
 [   0    0    3    0    0    0    0    0    0    0    5 2197    0    2
     0    3    0]
 [   0    0    0    3    0    4    0    0    0    0    2    4  518    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    1    0    0    0
  1122   10    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    27  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.92953929539296

F1 scores:
[       nan 0.87356322 0.99019992 0.97547684 0.98839907 0.97142857
 0.97977528 0.90909091 0.99061033 0.74418605 0.97775242 0.9894168
 0.97920605 0.99462366 0.97820401 0.90254873 0.97619048]

Kappa:
0.9763875493345737
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2425262748>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.501, val_acc:0.360]
Epoch [2/120    avg_loss:1.963, val_acc:0.515]
Epoch [3/120    avg_loss:1.704, val_acc:0.598]
Epoch [4/120    avg_loss:1.470, val_acc:0.610]
Epoch [5/120    avg_loss:1.330, val_acc:0.634]
Epoch [6/120    avg_loss:1.221, val_acc:0.668]
Epoch [7/120    avg_loss:0.965, val_acc:0.720]
Epoch [8/120    avg_loss:0.779, val_acc:0.739]
Epoch [9/120    avg_loss:0.711, val_acc:0.780]
Epoch [10/120    avg_loss:0.617, val_acc:0.807]
Epoch [11/120    avg_loss:0.626, val_acc:0.843]
Epoch [12/120    avg_loss:0.466, val_acc:0.852]
Epoch [13/120    avg_loss:0.415, val_acc:0.811]
Epoch [14/120    avg_loss:0.360, val_acc:0.868]
Epoch [15/120    avg_loss:0.308, val_acc:0.873]
Epoch [16/120    avg_loss:0.277, val_acc:0.883]
Epoch [17/120    avg_loss:0.298, val_acc:0.886]
Epoch [18/120    avg_loss:0.259, val_acc:0.895]
Epoch [19/120    avg_loss:0.302, val_acc:0.893]
Epoch [20/120    avg_loss:0.230, val_acc:0.912]
Epoch [21/120    avg_loss:0.180, val_acc:0.909]
Epoch [22/120    avg_loss:0.161, val_acc:0.922]
Epoch [23/120    avg_loss:0.158, val_acc:0.927]
Epoch [24/120    avg_loss:0.127, val_acc:0.927]
Epoch [25/120    avg_loss:0.127, val_acc:0.940]
Epoch [26/120    avg_loss:0.117, val_acc:0.941]
Epoch [27/120    avg_loss:0.109, val_acc:0.936]
Epoch [28/120    avg_loss:0.098, val_acc:0.947]
Epoch [29/120    avg_loss:0.080, val_acc:0.947]
Epoch [30/120    avg_loss:0.089, val_acc:0.942]
Epoch [31/120    avg_loss:0.088, val_acc:0.942]
Epoch [32/120    avg_loss:0.105, val_acc:0.934]
Epoch [33/120    avg_loss:0.079, val_acc:0.950]
Epoch [34/120    avg_loss:0.074, val_acc:0.935]
Epoch [35/120    avg_loss:0.075, val_acc:0.948]
Epoch [36/120    avg_loss:0.056, val_acc:0.955]
Epoch [37/120    avg_loss:0.057, val_acc:0.959]
Epoch [38/120    avg_loss:0.048, val_acc:0.961]
Epoch [39/120    avg_loss:0.045, val_acc:0.965]
Epoch [40/120    avg_loss:0.050, val_acc:0.966]
Epoch [41/120    avg_loss:0.042, val_acc:0.964]
Epoch [42/120    avg_loss:0.036, val_acc:0.964]
Epoch [43/120    avg_loss:0.042, val_acc:0.965]
Epoch [44/120    avg_loss:0.038, val_acc:0.961]
Epoch [45/120    avg_loss:0.039, val_acc:0.963]
Epoch [46/120    avg_loss:0.040, val_acc:0.967]
Epoch [47/120    avg_loss:0.027, val_acc:0.969]
Epoch [48/120    avg_loss:0.026, val_acc:0.967]
Epoch [49/120    avg_loss:0.023, val_acc:0.972]
Epoch [50/120    avg_loss:0.022, val_acc:0.966]
Epoch [51/120    avg_loss:0.017, val_acc:0.974]
Epoch [52/120    avg_loss:0.023, val_acc:0.964]
Epoch [53/120    avg_loss:0.023, val_acc:0.966]
Epoch [54/120    avg_loss:0.015, val_acc:0.975]
Epoch [55/120    avg_loss:0.022, val_acc:0.977]
Epoch [56/120    avg_loss:0.017, val_acc:0.976]
Epoch [57/120    avg_loss:0.022, val_acc:0.969]
Epoch [58/120    avg_loss:0.022, val_acc:0.974]
Epoch [59/120    avg_loss:0.017, val_acc:0.975]
Epoch [60/120    avg_loss:0.014, val_acc:0.974]
Epoch [61/120    avg_loss:0.015, val_acc:0.978]
Epoch [62/120    avg_loss:0.014, val_acc:0.976]
Epoch [63/120    avg_loss:0.012, val_acc:0.975]
Epoch [64/120    avg_loss:0.012, val_acc:0.973]
Epoch [65/120    avg_loss:0.012, val_acc:0.975]
Epoch [66/120    avg_loss:0.015, val_acc:0.982]
Epoch [67/120    avg_loss:0.013, val_acc:0.976]
Epoch [68/120    avg_loss:0.015, val_acc:0.980]
Epoch [69/120    avg_loss:0.012, val_acc:0.980]
Epoch [70/120    avg_loss:0.016, val_acc:0.965]
Epoch [71/120    avg_loss:0.015, val_acc:0.969]
Epoch [72/120    avg_loss:0.010, val_acc:0.974]
Epoch [73/120    avg_loss:0.009, val_acc:0.977]
Epoch [74/120    avg_loss:0.009, val_acc:0.982]
Epoch [75/120    avg_loss:0.007, val_acc:0.979]
Epoch [76/120    avg_loss:0.007, val_acc:0.975]
Epoch [77/120    avg_loss:0.007, val_acc:0.978]
Epoch [78/120    avg_loss:0.010, val_acc:0.980]
Epoch [79/120    avg_loss:0.009, val_acc:0.974]
Epoch [80/120    avg_loss:0.012, val_acc:0.974]
Epoch [81/120    avg_loss:0.008, val_acc:0.979]
Epoch [82/120    avg_loss:0.007, val_acc:0.974]
Epoch [83/120    avg_loss:0.007, val_acc:0.980]
Epoch [84/120    avg_loss:0.007, val_acc:0.981]
Epoch [85/120    avg_loss:0.006, val_acc:0.981]
Epoch [86/120    avg_loss:0.006, val_acc:0.979]
Epoch [87/120    avg_loss:0.006, val_acc:0.982]
Epoch [88/120    avg_loss:0.006, val_acc:0.981]
Epoch [89/120    avg_loss:0.007, val_acc:0.977]
Epoch [90/120    avg_loss:0.006, val_acc:0.981]
Epoch [91/120    avg_loss:0.013, val_acc:0.973]
Epoch [92/120    avg_loss:0.011, val_acc:0.976]
Epoch [93/120    avg_loss:0.006, val_acc:0.981]
Epoch [94/120    avg_loss:0.005, val_acc:0.980]
Epoch [95/120    avg_loss:0.007, val_acc:0.977]
Epoch [96/120    avg_loss:0.008, val_acc:0.977]
Epoch [97/120    avg_loss:0.006, val_acc:0.980]
Epoch [98/120    avg_loss:0.006, val_acc:0.978]
Epoch [99/120    avg_loss:0.006, val_acc:0.978]
Epoch [100/120    avg_loss:0.007, val_acc:0.980]
Epoch [101/120    avg_loss:0.005, val_acc:0.981]
Epoch [102/120    avg_loss:0.005, val_acc:0.979]
Epoch [103/120    avg_loss:0.007, val_acc:0.977]
Epoch [104/120    avg_loss:0.005, val_acc:0.974]
Epoch [105/120    avg_loss:0.004, val_acc:0.974]
Epoch [106/120    avg_loss:0.005, val_acc:0.975]
Epoch [107/120    avg_loss:0.005, val_acc:0.976]
Epoch [108/120    avg_loss:0.004, val_acc:0.977]
Epoch [109/120    avg_loss:0.004, val_acc:0.976]
Epoch [110/120    avg_loss:0.005, val_acc:0.976]
Epoch [111/120    avg_loss:0.004, val_acc:0.975]
Epoch [112/120    avg_loss:0.004, val_acc:0.974]
Epoch [113/120    avg_loss:0.004, val_acc:0.977]
Epoch [114/120    avg_loss:0.004, val_acc:0.977]
Epoch [115/120    avg_loss:0.005, val_acc:0.977]
Epoch [116/120    avg_loss:0.004, val_acc:0.977]
Epoch [117/120    avg_loss:0.004, val_acc:0.976]
Epoch [118/120    avg_loss:0.004, val_acc:0.976]
Epoch [119/120    avg_loss:0.004, val_acc:0.976]
Epoch [120/120    avg_loss:0.004, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1272    0    0    0    0    0    0    1    2   10    0    0
     0    0    0]
 [   0    0    0  714    3   14    0    0    0    2    1    3   10    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    2    0    0    0    2  838   27    0    0
     0    1    0]
 [   0    0    0    0    0    0    1    0    0    0    4 2203    0    1
     0    1    0]
 [   0    0    0    0    0    3    0    0    0    0    2    0  527    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0    0    0
  1133    4    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    30  311    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.38482384823848

F1 scores:
[       nan 1.         0.99297424 0.97674419 0.99300699 0.97632469
 0.99241275 0.98039216 0.99883586 0.82926829 0.97215777 0.98922317
 0.98046512 0.99730458 0.98350694 0.93674699 0.97005988]

Kappa:
0.9815727879067134
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f232163a780>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.562, val_acc:0.508]
Epoch [2/120    avg_loss:2.001, val_acc:0.559]
Epoch [3/120    avg_loss:1.732, val_acc:0.588]
Epoch [4/120    avg_loss:1.590, val_acc:0.628]
Epoch [5/120    avg_loss:1.408, val_acc:0.662]
Epoch [6/120    avg_loss:1.210, val_acc:0.688]
Epoch [7/120    avg_loss:1.033, val_acc:0.727]
Epoch [8/120    avg_loss:0.923, val_acc:0.708]
Epoch [9/120    avg_loss:0.829, val_acc:0.770]
Epoch [10/120    avg_loss:0.745, val_acc:0.762]
Epoch [11/120    avg_loss:0.661, val_acc:0.790]
Epoch [12/120    avg_loss:0.583, val_acc:0.825]
Epoch [13/120    avg_loss:0.572, val_acc:0.833]
Epoch [14/120    avg_loss:0.426, val_acc:0.840]
Epoch [15/120    avg_loss:0.322, val_acc:0.859]
Epoch [16/120    avg_loss:0.300, val_acc:0.865]
Epoch [17/120    avg_loss:0.256, val_acc:0.891]
Epoch [18/120    avg_loss:0.188, val_acc:0.913]
Epoch [19/120    avg_loss:0.181, val_acc:0.915]
Epoch [20/120    avg_loss:0.177, val_acc:0.916]
Epoch [21/120    avg_loss:0.170, val_acc:0.920]
Epoch [22/120    avg_loss:0.131, val_acc:0.897]
Epoch [23/120    avg_loss:0.144, val_acc:0.911]
Epoch [24/120    avg_loss:0.138, val_acc:0.933]
Epoch [25/120    avg_loss:0.121, val_acc:0.926]
Epoch [26/120    avg_loss:0.096, val_acc:0.943]
Epoch [27/120    avg_loss:0.084, val_acc:0.949]
Epoch [28/120    avg_loss:0.090, val_acc:0.946]
Epoch [29/120    avg_loss:0.111, val_acc:0.933]
Epoch [30/120    avg_loss:0.088, val_acc:0.946]
Epoch [31/120    avg_loss:0.073, val_acc:0.947]
Epoch [32/120    avg_loss:0.056, val_acc:0.949]
Epoch [33/120    avg_loss:0.075, val_acc:0.915]
Epoch [34/120    avg_loss:0.067, val_acc:0.942]
Epoch [35/120    avg_loss:0.069, val_acc:0.941]
Epoch [36/120    avg_loss:0.063, val_acc:0.949]
Epoch [37/120    avg_loss:0.061, val_acc:0.948]
Epoch [38/120    avg_loss:0.052, val_acc:0.955]
Epoch [39/120    avg_loss:0.047, val_acc:0.961]
Epoch [40/120    avg_loss:0.041, val_acc:0.964]
Epoch [41/120    avg_loss:0.035, val_acc:0.966]
Epoch [42/120    avg_loss:0.036, val_acc:0.964]
Epoch [43/120    avg_loss:0.038, val_acc:0.963]
Epoch [44/120    avg_loss:0.035, val_acc:0.965]
Epoch [45/120    avg_loss:0.031, val_acc:0.966]
Epoch [46/120    avg_loss:0.031, val_acc:0.971]
Epoch [47/120    avg_loss:0.031, val_acc:0.963]
Epoch [48/120    avg_loss:0.033, val_acc:0.961]
Epoch [49/120    avg_loss:0.026, val_acc:0.961]
Epoch [50/120    avg_loss:0.037, val_acc:0.966]
Epoch [51/120    avg_loss:0.037, val_acc:0.968]
Epoch [52/120    avg_loss:0.032, val_acc:0.957]
Epoch [53/120    avg_loss:0.050, val_acc:0.961]
Epoch [54/120    avg_loss:0.035, val_acc:0.954]
Epoch [55/120    avg_loss:0.035, val_acc:0.958]
Epoch [56/120    avg_loss:0.029, val_acc:0.975]
Epoch [57/120    avg_loss:0.048, val_acc:0.971]
Epoch [58/120    avg_loss:0.050, val_acc:0.960]
Epoch [59/120    avg_loss:0.026, val_acc:0.974]
Epoch [60/120    avg_loss:0.029, val_acc:0.974]
Epoch [61/120    avg_loss:0.031, val_acc:0.962]
Epoch [62/120    avg_loss:0.029, val_acc:0.963]
Epoch [63/120    avg_loss:0.023, val_acc:0.972]
Epoch [64/120    avg_loss:0.022, val_acc:0.970]
Epoch [65/120    avg_loss:0.019, val_acc:0.974]
Epoch [66/120    avg_loss:0.017, val_acc:0.974]
Epoch [67/120    avg_loss:0.014, val_acc:0.976]
Epoch [68/120    avg_loss:0.022, val_acc:0.977]
Epoch [69/120    avg_loss:0.020, val_acc:0.970]
Epoch [70/120    avg_loss:0.015, val_acc:0.977]
Epoch [71/120    avg_loss:0.013, val_acc:0.977]
Epoch [72/120    avg_loss:0.017, val_acc:0.975]
Epoch [73/120    avg_loss:0.018, val_acc:0.964]
Epoch [74/120    avg_loss:0.018, val_acc:0.976]
Epoch [75/120    avg_loss:0.019, val_acc:0.963]
Epoch [76/120    avg_loss:0.039, val_acc:0.972]
Epoch [77/120    avg_loss:0.020, val_acc:0.977]
Epoch [78/120    avg_loss:0.016, val_acc:0.963]
Epoch [79/120    avg_loss:0.012, val_acc:0.979]
Epoch [80/120    avg_loss:0.014, val_acc:0.974]
Epoch [81/120    avg_loss:0.016, val_acc:0.976]
Epoch [82/120    avg_loss:0.014, val_acc:0.978]
Epoch [83/120    avg_loss:0.020, val_acc:0.976]
Epoch [84/120    avg_loss:0.037, val_acc:0.968]
Epoch [85/120    avg_loss:0.024, val_acc:0.966]
Epoch [86/120    avg_loss:0.023, val_acc:0.975]
Epoch [87/120    avg_loss:0.017, val_acc:0.975]
Epoch [88/120    avg_loss:0.018, val_acc:0.966]
Epoch [89/120    avg_loss:0.019, val_acc:0.977]
Epoch [90/120    avg_loss:0.009, val_acc:0.982]
Epoch [91/120    avg_loss:0.009, val_acc:0.977]
Epoch [92/120    avg_loss:0.006, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.007, val_acc:0.982]
Epoch [95/120    avg_loss:0.011, val_acc:0.978]
Epoch [96/120    avg_loss:0.011, val_acc:0.983]
Epoch [97/120    avg_loss:0.010, val_acc:0.978]
Epoch [98/120    avg_loss:0.007, val_acc:0.985]
Epoch [99/120    avg_loss:0.007, val_acc:0.980]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.007, val_acc:0.982]
Epoch [102/120    avg_loss:0.008, val_acc:0.985]
Epoch [103/120    avg_loss:0.007, val_acc:0.982]
Epoch [104/120    avg_loss:0.007, val_acc:0.978]
Epoch [105/120    avg_loss:0.007, val_acc:0.982]
Epoch [106/120    avg_loss:0.008, val_acc:0.983]
Epoch [107/120    avg_loss:0.005, val_acc:0.984]
Epoch [108/120    avg_loss:0.007, val_acc:0.978]
Epoch [109/120    avg_loss:0.012, val_acc:0.978]
Epoch [110/120    avg_loss:0.013, val_acc:0.983]
Epoch [111/120    avg_loss:0.011, val_acc:0.975]
Epoch [112/120    avg_loss:0.035, val_acc:0.967]
Epoch [113/120    avg_loss:0.080, val_acc:0.942]
Epoch [114/120    avg_loss:0.057, val_acc:0.964]
Epoch [115/120    avg_loss:0.026, val_acc:0.964]
Epoch [116/120    avg_loss:0.025, val_acc:0.972]
Epoch [117/120    avg_loss:0.021, val_acc:0.972]
Epoch [118/120    avg_loss:0.019, val_acc:0.974]
Epoch [119/120    avg_loss:0.013, val_acc:0.976]
Epoch [120/120    avg_loss:0.013, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1244    5   13    0    4    0    0    0    7   11    1    0
     0    0    0]
 [   0    0    0  710    0   12    0    0    0   10    1    0   13    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    1    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    1    8    0    0    0  859    3    2    0
     2    0    0]
 [   0    0    7    2    0    0    2    0    0    3   16 2154   21    3
     0    2    0]
 [   0    0    0    1    0    3    0    0    0    0    6    1  518    0
     0    2    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    1    1    0    0    0
  1124   13    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    11  321    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.6260162601626

F1 scores:
[       nan 0.98765432 0.98068585 0.96928328 0.97038724 0.97732426
 0.9753915  1.         0.997669   0.67924528 0.9733711  0.98378625
 0.94784995 0.98930481 0.98639754 0.93722628 0.9704142 ]

Kappa:
0.9729721706370139
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4227f99748>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.550, val_acc:0.481]
Epoch [2/120    avg_loss:2.020, val_acc:0.584]
Epoch [3/120    avg_loss:1.727, val_acc:0.616]
Epoch [4/120    avg_loss:1.482, val_acc:0.662]
Epoch [5/120    avg_loss:1.290, val_acc:0.699]
Epoch [6/120    avg_loss:1.074, val_acc:0.745]
Epoch [7/120    avg_loss:0.873, val_acc:0.748]
Epoch [8/120    avg_loss:0.776, val_acc:0.723]
Epoch [9/120    avg_loss:0.690, val_acc:0.791]
Epoch [10/120    avg_loss:0.598, val_acc:0.820]
Epoch [11/120    avg_loss:0.476, val_acc:0.811]
Epoch [12/120    avg_loss:0.421, val_acc:0.853]
Epoch [13/120    avg_loss:0.330, val_acc:0.851]
Epoch [14/120    avg_loss:0.347, val_acc:0.878]
Epoch [15/120    avg_loss:0.348, val_acc:0.877]
Epoch [16/120    avg_loss:0.441, val_acc:0.844]
Epoch [17/120    avg_loss:0.346, val_acc:0.879]
Epoch [18/120    avg_loss:0.207, val_acc:0.919]
Epoch [19/120    avg_loss:0.215, val_acc:0.916]
Epoch [20/120    avg_loss:0.186, val_acc:0.928]
Epoch [21/120    avg_loss:0.180, val_acc:0.923]
Epoch [22/120    avg_loss:0.163, val_acc:0.916]
Epoch [23/120    avg_loss:0.148, val_acc:0.938]
Epoch [24/120    avg_loss:0.159, val_acc:0.926]
Epoch [25/120    avg_loss:0.136, val_acc:0.939]
Epoch [26/120    avg_loss:0.121, val_acc:0.948]
Epoch [27/120    avg_loss:0.100, val_acc:0.946]
Epoch [28/120    avg_loss:0.096, val_acc:0.948]
Epoch [29/120    avg_loss:0.080, val_acc:0.945]
Epoch [30/120    avg_loss:0.066, val_acc:0.961]
Epoch [31/120    avg_loss:0.082, val_acc:0.963]
Epoch [32/120    avg_loss:0.064, val_acc:0.950]
Epoch [33/120    avg_loss:0.058, val_acc:0.963]
Epoch [34/120    avg_loss:0.049, val_acc:0.952]
Epoch [35/120    avg_loss:0.040, val_acc:0.961]
Epoch [36/120    avg_loss:0.046, val_acc:0.957]
Epoch [37/120    avg_loss:0.052, val_acc:0.965]
Epoch [38/120    avg_loss:0.044, val_acc:0.959]
Epoch [39/120    avg_loss:0.039, val_acc:0.953]
Epoch [40/120    avg_loss:0.078, val_acc:0.957]
Epoch [41/120    avg_loss:0.050, val_acc:0.969]
Epoch [42/120    avg_loss:0.051, val_acc:0.959]
Epoch [43/120    avg_loss:0.037, val_acc:0.968]
Epoch [44/120    avg_loss:0.035, val_acc:0.965]
Epoch [45/120    avg_loss:0.031, val_acc:0.964]
Epoch [46/120    avg_loss:0.030, val_acc:0.961]
Epoch [47/120    avg_loss:0.022, val_acc:0.970]
Epoch [48/120    avg_loss:0.026, val_acc:0.964]
Epoch [49/120    avg_loss:0.021, val_acc:0.968]
Epoch [50/120    avg_loss:0.026, val_acc:0.974]
Epoch [51/120    avg_loss:0.021, val_acc:0.976]
Epoch [52/120    avg_loss:0.026, val_acc:0.972]
Epoch [53/120    avg_loss:0.024, val_acc:0.982]
Epoch [54/120    avg_loss:0.023, val_acc:0.979]
Epoch [55/120    avg_loss:0.024, val_acc:0.967]
Epoch [56/120    avg_loss:0.023, val_acc:0.974]
Epoch [57/120    avg_loss:0.020, val_acc:0.964]
Epoch [58/120    avg_loss:0.025, val_acc:0.973]
Epoch [59/120    avg_loss:0.025, val_acc:0.973]
Epoch [60/120    avg_loss:0.021, val_acc:0.975]
Epoch [61/120    avg_loss:0.019, val_acc:0.977]
Epoch [62/120    avg_loss:0.015, val_acc:0.977]
Epoch [63/120    avg_loss:0.019, val_acc:0.970]
Epoch [64/120    avg_loss:0.035, val_acc:0.964]
Epoch [65/120    avg_loss:0.021, val_acc:0.975]
Epoch [66/120    avg_loss:0.019, val_acc:0.977]
Epoch [67/120    avg_loss:0.010, val_acc:0.979]
Epoch [68/120    avg_loss:0.011, val_acc:0.978]
Epoch [69/120    avg_loss:0.009, val_acc:0.977]
Epoch [70/120    avg_loss:0.009, val_acc:0.976]
Epoch [71/120    avg_loss:0.008, val_acc:0.975]
Epoch [72/120    avg_loss:0.010, val_acc:0.974]
Epoch [73/120    avg_loss:0.009, val_acc:0.976]
Epoch [74/120    avg_loss:0.010, val_acc:0.974]
Epoch [75/120    avg_loss:0.010, val_acc:0.973]
Epoch [76/120    avg_loss:0.010, val_acc:0.973]
Epoch [77/120    avg_loss:0.011, val_acc:0.974]
Epoch [78/120    avg_loss:0.010, val_acc:0.974]
Epoch [79/120    avg_loss:0.009, val_acc:0.975]
Epoch [80/120    avg_loss:0.009, val_acc:0.975]
Epoch [81/120    avg_loss:0.007, val_acc:0.974]
Epoch [82/120    avg_loss:0.009, val_acc:0.974]
Epoch [83/120    avg_loss:0.009, val_acc:0.975]
Epoch [84/120    avg_loss:0.008, val_acc:0.975]
Epoch [85/120    avg_loss:0.008, val_acc:0.975]
Epoch [86/120    avg_loss:0.008, val_acc:0.975]
Epoch [87/120    avg_loss:0.008, val_acc:0.975]
Epoch [88/120    avg_loss:0.007, val_acc:0.975]
Epoch [89/120    avg_loss:0.010, val_acc:0.975]
Epoch [90/120    avg_loss:0.009, val_acc:0.975]
Epoch [91/120    avg_loss:0.009, val_acc:0.975]
Epoch [92/120    avg_loss:0.008, val_acc:0.975]
Epoch [93/120    avg_loss:0.009, val_acc:0.975]
Epoch [94/120    avg_loss:0.009, val_acc:0.975]
Epoch [95/120    avg_loss:0.009, val_acc:0.975]
Epoch [96/120    avg_loss:0.007, val_acc:0.975]
Epoch [97/120    avg_loss:0.010, val_acc:0.975]
Epoch [98/120    avg_loss:0.008, val_acc:0.975]
Epoch [99/120    avg_loss:0.008, val_acc:0.975]
Epoch [100/120    avg_loss:0.010, val_acc:0.975]
Epoch [101/120    avg_loss:0.008, val_acc:0.975]
Epoch [102/120    avg_loss:0.011, val_acc:0.975]
Epoch [103/120    avg_loss:0.009, val_acc:0.975]
Epoch [104/120    avg_loss:0.009, val_acc:0.975]
Epoch [105/120    avg_loss:0.008, val_acc:0.975]
Epoch [106/120    avg_loss:0.010, val_acc:0.975]
Epoch [107/120    avg_loss:0.010, val_acc:0.975]
Epoch [108/120    avg_loss:0.011, val_acc:0.975]
Epoch [109/120    avg_loss:0.011, val_acc:0.975]
Epoch [110/120    avg_loss:0.009, val_acc:0.975]
Epoch [111/120    avg_loss:0.009, val_acc:0.975]
Epoch [112/120    avg_loss:0.008, val_acc:0.975]
Epoch [113/120    avg_loss:0.009, val_acc:0.975]
Epoch [114/120    avg_loss:0.011, val_acc:0.975]
Epoch [115/120    avg_loss:0.009, val_acc:0.975]
Epoch [116/120    avg_loss:0.008, val_acc:0.975]
Epoch [117/120    avg_loss:0.009, val_acc:0.975]
Epoch [118/120    avg_loss:0.009, val_acc:0.975]
Epoch [119/120    avg_loss:0.008, val_acc:0.975]
Epoch [120/120    avg_loss:0.009, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1268    0    0    0    0    0    0    4    2    8    0    0
     0    3    0]
 [   0    0    0  705    1   15    0    0    0    7    2    0   13    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    4    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    3    2    0    0    0  840   25    0    0
     0    2    0]
 [   0    0    2    0    0    0    0    0    0    1    1 2199    6    0
     0    1    0]
 [   0    0    0    1    0    6    0    0    0    0    0    0  523    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1122   15    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    21  311    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.02710027100271

F1 scores:
[       nan 0.975      0.990625   0.97040606 0.99765808 0.96621622
 0.98493976 0.92592593 0.99767442 0.73469388 0.97617664 0.98942632
 0.9703154  0.98930481 0.98291721 0.9133627  0.98224852]

Kappa:
0.9775037645469323
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6fcc21d6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.586, val_acc:0.416]
Epoch [2/120    avg_loss:2.045, val_acc:0.490]
Epoch [3/120    avg_loss:1.802, val_acc:0.578]
Epoch [4/120    avg_loss:1.620, val_acc:0.605]
Epoch [5/120    avg_loss:1.438, val_acc:0.666]
Epoch [6/120    avg_loss:1.215, val_acc:0.675]
Epoch [7/120    avg_loss:1.132, val_acc:0.704]
Epoch [8/120    avg_loss:0.927, val_acc:0.752]
Epoch [9/120    avg_loss:0.853, val_acc:0.728]
Epoch [10/120    avg_loss:0.787, val_acc:0.762]
Epoch [11/120    avg_loss:0.643, val_acc:0.795]
Epoch [12/120    avg_loss:0.528, val_acc:0.795]
Epoch [13/120    avg_loss:0.440, val_acc:0.857]
Epoch [14/120    avg_loss:0.412, val_acc:0.823]
Epoch [15/120    avg_loss:0.384, val_acc:0.854]
Epoch [16/120    avg_loss:0.350, val_acc:0.865]
Epoch [17/120    avg_loss:0.272, val_acc:0.897]
Epoch [18/120    avg_loss:0.232, val_acc:0.892]
Epoch [19/120    avg_loss:0.230, val_acc:0.905]
Epoch [20/120    avg_loss:0.196, val_acc:0.859]
Epoch [21/120    avg_loss:0.161, val_acc:0.915]
Epoch [22/120    avg_loss:0.174, val_acc:0.928]
Epoch [23/120    avg_loss:0.132, val_acc:0.932]
Epoch [24/120    avg_loss:0.113, val_acc:0.947]
Epoch [25/120    avg_loss:0.147, val_acc:0.934]
Epoch [26/120    avg_loss:0.115, val_acc:0.940]
Epoch [27/120    avg_loss:0.119, val_acc:0.903]
Epoch [28/120    avg_loss:0.118, val_acc:0.923]
Epoch [29/120    avg_loss:0.111, val_acc:0.915]
Epoch [30/120    avg_loss:0.098, val_acc:0.946]
Epoch [31/120    avg_loss:0.075, val_acc:0.953]
Epoch [32/120    avg_loss:0.081, val_acc:0.932]
Epoch [33/120    avg_loss:0.088, val_acc:0.940]
Epoch [34/120    avg_loss:0.067, val_acc:0.956]
Epoch [35/120    avg_loss:0.059, val_acc:0.956]
Epoch [36/120    avg_loss:0.073, val_acc:0.965]
Epoch [37/120    avg_loss:0.083, val_acc:0.946]
Epoch [38/120    avg_loss:0.060, val_acc:0.953]
Epoch [39/120    avg_loss:0.057, val_acc:0.952]
Epoch [40/120    avg_loss:0.054, val_acc:0.949]
Epoch [41/120    avg_loss:0.049, val_acc:0.963]
Epoch [42/120    avg_loss:0.036, val_acc:0.970]
Epoch [43/120    avg_loss:0.035, val_acc:0.957]
Epoch [44/120    avg_loss:0.044, val_acc:0.949]
Epoch [45/120    avg_loss:0.036, val_acc:0.964]
Epoch [46/120    avg_loss:0.030, val_acc:0.968]
Epoch [47/120    avg_loss:0.032, val_acc:0.969]
Epoch [48/120    avg_loss:0.028, val_acc:0.971]
Epoch [49/120    avg_loss:0.029, val_acc:0.965]
Epoch [50/120    avg_loss:0.027, val_acc:0.978]
Epoch [51/120    avg_loss:0.020, val_acc:0.976]
Epoch [52/120    avg_loss:0.021, val_acc:0.969]
Epoch [53/120    avg_loss:0.023, val_acc:0.972]
Epoch [54/120    avg_loss:0.030, val_acc:0.976]
Epoch [55/120    avg_loss:0.032, val_acc:0.951]
Epoch [56/120    avg_loss:0.038, val_acc:0.970]
Epoch [57/120    avg_loss:0.028, val_acc:0.976]
Epoch [58/120    avg_loss:0.022, val_acc:0.978]
Epoch [59/120    avg_loss:0.023, val_acc:0.970]
Epoch [60/120    avg_loss:0.016, val_acc:0.974]
Epoch [61/120    avg_loss:0.070, val_acc:0.966]
Epoch [62/120    avg_loss:0.034, val_acc:0.974]
Epoch [63/120    avg_loss:0.035, val_acc:0.961]
Epoch [64/120    avg_loss:0.027, val_acc:0.977]
Epoch [65/120    avg_loss:0.018, val_acc:0.976]
Epoch [66/120    avg_loss:0.018, val_acc:0.974]
Epoch [67/120    avg_loss:0.016, val_acc:0.979]
Epoch [68/120    avg_loss:0.012, val_acc:0.977]
Epoch [69/120    avg_loss:0.013, val_acc:0.978]
Epoch [70/120    avg_loss:0.013, val_acc:0.976]
Epoch [71/120    avg_loss:0.030, val_acc:0.968]
Epoch [72/120    avg_loss:0.020, val_acc:0.970]
Epoch [73/120    avg_loss:0.024, val_acc:0.976]
Epoch [74/120    avg_loss:0.146, val_acc:0.953]
Epoch [75/120    avg_loss:0.084, val_acc:0.958]
Epoch [76/120    avg_loss:0.045, val_acc:0.949]
Epoch [77/120    avg_loss:0.057, val_acc:0.955]
Epoch [78/120    avg_loss:0.051, val_acc:0.960]
Epoch [79/120    avg_loss:0.041, val_acc:0.963]
Epoch [80/120    avg_loss:0.027, val_acc:0.965]
Epoch [81/120    avg_loss:0.018, val_acc:0.972]
Epoch [82/120    avg_loss:0.016, val_acc:0.977]
Epoch [83/120    avg_loss:0.014, val_acc:0.979]
Epoch [84/120    avg_loss:0.014, val_acc:0.980]
Epoch [85/120    avg_loss:0.013, val_acc:0.979]
Epoch [86/120    avg_loss:0.011, val_acc:0.978]
Epoch [87/120    avg_loss:0.013, val_acc:0.976]
Epoch [88/120    avg_loss:0.015, val_acc:0.977]
Epoch [89/120    avg_loss:0.011, val_acc:0.977]
Epoch [90/120    avg_loss:0.011, val_acc:0.977]
Epoch [91/120    avg_loss:0.010, val_acc:0.976]
Epoch [92/120    avg_loss:0.010, val_acc:0.975]
Epoch [93/120    avg_loss:0.010, val_acc:0.978]
Epoch [94/120    avg_loss:0.012, val_acc:0.978]
Epoch [95/120    avg_loss:0.012, val_acc:0.976]
Epoch [96/120    avg_loss:0.010, val_acc:0.977]
Epoch [97/120    avg_loss:0.010, val_acc:0.978]
Epoch [98/120    avg_loss:0.010, val_acc:0.978]
Epoch [99/120    avg_loss:0.011, val_acc:0.977]
Epoch [100/120    avg_loss:0.008, val_acc:0.977]
Epoch [101/120    avg_loss:0.010, val_acc:0.976]
Epoch [102/120    avg_loss:0.010, val_acc:0.976]
Epoch [103/120    avg_loss:0.009, val_acc:0.975]
Epoch [104/120    avg_loss:0.011, val_acc:0.975]
Epoch [105/120    avg_loss:0.011, val_acc:0.975]
Epoch [106/120    avg_loss:0.009, val_acc:0.975]
Epoch [107/120    avg_loss:0.008, val_acc:0.975]
Epoch [108/120    avg_loss:0.012, val_acc:0.975]
Epoch [109/120    avg_loss:0.010, val_acc:0.975]
Epoch [110/120    avg_loss:0.008, val_acc:0.975]
Epoch [111/120    avg_loss:0.010, val_acc:0.975]
Epoch [112/120    avg_loss:0.012, val_acc:0.975]
Epoch [113/120    avg_loss:0.010, val_acc:0.975]
Epoch [114/120    avg_loss:0.009, val_acc:0.975]
Epoch [115/120    avg_loss:0.010, val_acc:0.975]
Epoch [116/120    avg_loss:0.010, val_acc:0.975]
Epoch [117/120    avg_loss:0.010, val_acc:0.975]
Epoch [118/120    avg_loss:0.010, val_acc:0.975]
Epoch [119/120    avg_loss:0.014, val_acc:0.975]
Epoch [120/120    avg_loss:0.009, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1266    0    0    3    0    0    0    6    6    4    0    0
     0    0    0]
 [   0    0    0  726    0    0    0    0    0    4    1    0   10    6
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    5    0    1    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   13    0    0    2    1    0    0    3  839   16    0    0
     0    1    0]
 [   0    0    6    0    0    1    1    0    0    0    2 2198    0    1
     1    0    0]
 [   0    0    0    3    0    0    0    0    0    0    0    0  529    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   12    0    0    1    0    1    0    0    0
  1119    6    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    25  319    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.35230352303523

F1 scores:
[       nan 0.98765432 0.98521401 0.98373984 1.         0.96928328
 0.99620925 0.90909091 0.99883856 0.72       0.97275362 0.99277326
 0.98510242 0.98143236 0.97857455 0.94799406 0.98224852]

Kappa:
0.9812152996467828
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7febc3821748>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.546, val_acc:0.402]
Epoch [2/120    avg_loss:2.067, val_acc:0.537]
Epoch [3/120    avg_loss:1.740, val_acc:0.625]
Epoch [4/120    avg_loss:1.551, val_acc:0.626]
Epoch [5/120    avg_loss:1.382, val_acc:0.675]
Epoch [6/120    avg_loss:1.199, val_acc:0.702]
Epoch [7/120    avg_loss:0.972, val_acc:0.734]
Epoch [8/120    avg_loss:0.820, val_acc:0.767]
Epoch [9/120    avg_loss:0.708, val_acc:0.771]
Epoch [10/120    avg_loss:0.698, val_acc:0.822]
Epoch [11/120    avg_loss:0.572, val_acc:0.838]
Epoch [12/120    avg_loss:0.482, val_acc:0.845]
Epoch [13/120    avg_loss:0.405, val_acc:0.859]
Epoch [14/120    avg_loss:0.366, val_acc:0.856]
Epoch [15/120    avg_loss:0.354, val_acc:0.885]
Epoch [16/120    avg_loss:0.281, val_acc:0.896]
Epoch [17/120    avg_loss:0.228, val_acc:0.907]
Epoch [18/120    avg_loss:0.192, val_acc:0.926]
Epoch [19/120    avg_loss:0.175, val_acc:0.903]
Epoch [20/120    avg_loss:0.141, val_acc:0.945]
Epoch [21/120    avg_loss:0.152, val_acc:0.910]
Epoch [22/120    avg_loss:0.156, val_acc:0.921]
Epoch [23/120    avg_loss:0.129, val_acc:0.945]
Epoch [24/120    avg_loss:0.108, val_acc:0.945]
Epoch [25/120    avg_loss:0.127, val_acc:0.942]
Epoch [26/120    avg_loss:0.118, val_acc:0.944]
Epoch [27/120    avg_loss:0.101, val_acc:0.948]
Epoch [28/120    avg_loss:0.096, val_acc:0.952]
Epoch [29/120    avg_loss:0.083, val_acc:0.946]
Epoch [30/120    avg_loss:0.075, val_acc:0.959]
Epoch [31/120    avg_loss:0.061, val_acc:0.956]
Epoch [32/120    avg_loss:0.065, val_acc:0.964]
Epoch [33/120    avg_loss:0.070, val_acc:0.965]
Epoch [34/120    avg_loss:0.064, val_acc:0.949]
Epoch [35/120    avg_loss:0.082, val_acc:0.954]
Epoch [36/120    avg_loss:0.075, val_acc:0.959]
Epoch [37/120    avg_loss:0.047, val_acc:0.971]
Epoch [38/120    avg_loss:0.037, val_acc:0.964]
Epoch [39/120    avg_loss:0.037, val_acc:0.970]
Epoch [40/120    avg_loss:0.027, val_acc:0.970]
Epoch [41/120    avg_loss:0.030, val_acc:0.965]
Epoch [42/120    avg_loss:0.030, val_acc:0.974]
Epoch [43/120    avg_loss:0.035, val_acc:0.976]
Epoch [44/120    avg_loss:0.038, val_acc:0.958]
Epoch [45/120    avg_loss:0.038, val_acc:0.971]
Epoch [46/120    avg_loss:0.035, val_acc:0.970]
Epoch [47/120    avg_loss:0.207, val_acc:0.808]
Epoch [48/120    avg_loss:0.510, val_acc:0.799]
Epoch [49/120    avg_loss:0.280, val_acc:0.922]
Epoch [50/120    avg_loss:0.138, val_acc:0.954]
Epoch [51/120    avg_loss:0.084, val_acc:0.952]
Epoch [52/120    avg_loss:0.094, val_acc:0.946]
Epoch [53/120    avg_loss:0.074, val_acc:0.951]
Epoch [54/120    avg_loss:0.076, val_acc:0.957]
Epoch [55/120    avg_loss:0.054, val_acc:0.958]
Epoch [56/120    avg_loss:0.052, val_acc:0.969]
Epoch [57/120    avg_loss:0.041, val_acc:0.970]
Epoch [58/120    avg_loss:0.035, val_acc:0.975]
Epoch [59/120    avg_loss:0.032, val_acc:0.976]
Epoch [60/120    avg_loss:0.029, val_acc:0.976]
Epoch [61/120    avg_loss:0.027, val_acc:0.975]
Epoch [62/120    avg_loss:0.031, val_acc:0.975]
Epoch [63/120    avg_loss:0.029, val_acc:0.979]
Epoch [64/120    avg_loss:0.026, val_acc:0.976]
Epoch [65/120    avg_loss:0.026, val_acc:0.981]
Epoch [66/120    avg_loss:0.025, val_acc:0.978]
Epoch [67/120    avg_loss:0.030, val_acc:0.980]
Epoch [68/120    avg_loss:0.030, val_acc:0.981]
Epoch [69/120    avg_loss:0.024, val_acc:0.978]
Epoch [70/120    avg_loss:0.024, val_acc:0.977]
Epoch [71/120    avg_loss:0.024, val_acc:0.976]
Epoch [72/120    avg_loss:0.023, val_acc:0.976]
Epoch [73/120    avg_loss:0.021, val_acc:0.979]
Epoch [74/120    avg_loss:0.026, val_acc:0.977]
Epoch [75/120    avg_loss:0.026, val_acc:0.979]
Epoch [76/120    avg_loss:0.024, val_acc:0.976]
Epoch [77/120    avg_loss:0.023, val_acc:0.977]
Epoch [78/120    avg_loss:0.024, val_acc:0.977]
Epoch [79/120    avg_loss:0.025, val_acc:0.976]
Epoch [80/120    avg_loss:0.026, val_acc:0.975]
Epoch [81/120    avg_loss:0.025, val_acc:0.979]
Epoch [82/120    avg_loss:0.022, val_acc:0.979]
Epoch [83/120    avg_loss:0.023, val_acc:0.979]
Epoch [84/120    avg_loss:0.023, val_acc:0.979]
Epoch [85/120    avg_loss:0.024, val_acc:0.979]
Epoch [86/120    avg_loss:0.025, val_acc:0.979]
Epoch [87/120    avg_loss:0.023, val_acc:0.979]
Epoch [88/120    avg_loss:0.024, val_acc:0.980]
Epoch [89/120    avg_loss:0.021, val_acc:0.980]
Epoch [90/120    avg_loss:0.021, val_acc:0.979]
Epoch [91/120    avg_loss:0.022, val_acc:0.979]
Epoch [92/120    avg_loss:0.020, val_acc:0.979]
Epoch [93/120    avg_loss:0.023, val_acc:0.979]
Epoch [94/120    avg_loss:0.022, val_acc:0.979]
Epoch [95/120    avg_loss:0.020, val_acc:0.979]
Epoch [96/120    avg_loss:0.019, val_acc:0.979]
Epoch [97/120    avg_loss:0.021, val_acc:0.979]
Epoch [98/120    avg_loss:0.026, val_acc:0.979]
Epoch [99/120    avg_loss:0.022, val_acc:0.979]
Epoch [100/120    avg_loss:0.022, val_acc:0.979]
Epoch [101/120    avg_loss:0.024, val_acc:0.979]
Epoch [102/120    avg_loss:0.022, val_acc:0.979]
Epoch [103/120    avg_loss:0.021, val_acc:0.979]
Epoch [104/120    avg_loss:0.022, val_acc:0.979]
Epoch [105/120    avg_loss:0.022, val_acc:0.979]
Epoch [106/120    avg_loss:0.020, val_acc:0.979]
Epoch [107/120    avg_loss:0.025, val_acc:0.979]
Epoch [108/120    avg_loss:0.020, val_acc:0.979]
Epoch [109/120    avg_loss:0.024, val_acc:0.979]
Epoch [110/120    avg_loss:0.022, val_acc:0.979]
Epoch [111/120    avg_loss:0.021, val_acc:0.979]
Epoch [112/120    avg_loss:0.025, val_acc:0.979]
Epoch [113/120    avg_loss:0.023, val_acc:0.979]
Epoch [114/120    avg_loss:0.021, val_acc:0.979]
Epoch [115/120    avg_loss:0.020, val_acc:0.979]
Epoch [116/120    avg_loss:0.022, val_acc:0.979]
Epoch [117/120    avg_loss:0.023, val_acc:0.979]
Epoch [118/120    avg_loss:0.023, val_acc:0.979]
Epoch [119/120    avg_loss:0.024, val_acc:0.979]
Epoch [120/120    avg_loss:0.020, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1253    6    0    0    2    0    0    1    4   19    0    0
     0    0    0]
 [   0    0    0  733    0    0    0    0    0    7    1    1    2    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    4    0    1    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  651    0    0    1    0    3    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   18    0    0    2    2    0    0    5  830   18    0    0
     0    0    0]
 [   0    0    3    0    0    0    3    1    0    2   14 2170   16    1
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    0    3  526    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1125   13    0]
 [   0    0    1    0    0    0    8    0    0    0    0    0    0    0
    25  313    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.72357723577235

F1 scores:
[       nan 0.94117647 0.97890625 0.98455339 1.         0.98490128
 0.98412698 0.90909091 0.9953271  0.67924528 0.9617613  0.98101266
 0.97497683 0.98930481 0.97953853 0.93016345 0.98224852]

Kappa:
0.9740490989039376
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:24
Validation dataloader:24
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0769d50710>
supervision:full
center_pixel:True
Network :
Number of parameter: 111193==>0.11M
----------Training process----------
Epoch [1/120    avg_loss:2.555, val_acc:0.503]
Epoch [2/120    avg_loss:2.013, val_acc:0.505]
Epoch [3/120    avg_loss:1.774, val_acc:0.574]
Epoch [4/120    avg_loss:1.592, val_acc:0.633]
Epoch [5/120    avg_loss:1.382, val_acc:0.684]
Epoch [6/120    avg_loss:1.224, val_acc:0.686]
Epoch [7/120    avg_loss:1.118, val_acc:0.689]
Epoch [8/120    avg_loss:0.943, val_acc:0.745]
Epoch [9/120    avg_loss:0.775, val_acc:0.794]
Epoch [10/120    avg_loss:0.762, val_acc:0.771]
Epoch [11/120    avg_loss:0.567, val_acc:0.830]
Epoch [12/120    avg_loss:0.522, val_acc:0.842]
Epoch [13/120    avg_loss:0.505, val_acc:0.816]
Epoch [14/120    avg_loss:0.449, val_acc:0.830]
Epoch [15/120    avg_loss:0.355, val_acc:0.854]
Epoch [16/120    avg_loss:0.332, val_acc:0.885]
Epoch [17/120    avg_loss:0.287, val_acc:0.881]
Epoch [18/120    avg_loss:0.278, val_acc:0.875]
Epoch [19/120    avg_loss:0.212, val_acc:0.906]
Epoch [20/120    avg_loss:0.197, val_acc:0.914]
Epoch [21/120    avg_loss:0.180, val_acc:0.914]
Epoch [22/120    avg_loss:0.160, val_acc:0.918]
Epoch [23/120    avg_loss:0.123, val_acc:0.931]
Epoch [24/120    avg_loss:0.166, val_acc:0.922]
Epoch [25/120    avg_loss:0.150, val_acc:0.924]
Epoch [26/120    avg_loss:0.160, val_acc:0.916]
Epoch [27/120    avg_loss:0.128, val_acc:0.930]
Epoch [28/120    avg_loss:0.101, val_acc:0.931]
Epoch [29/120    avg_loss:0.100, val_acc:0.936]
Epoch [30/120    avg_loss:0.090, val_acc:0.934]
Epoch [31/120    avg_loss:0.073, val_acc:0.952]
Epoch [32/120    avg_loss:0.070, val_acc:0.950]
Epoch [33/120    avg_loss:0.062, val_acc:0.949]
Epoch [34/120    avg_loss:0.105, val_acc:0.942]
Epoch [35/120    avg_loss:0.089, val_acc:0.961]
Epoch [36/120    avg_loss:0.077, val_acc:0.953]
Epoch [37/120    avg_loss:0.065, val_acc:0.948]
Epoch [38/120    avg_loss:0.047, val_acc:0.953]
Epoch [39/120    avg_loss:0.055, val_acc:0.958]
Epoch [40/120    avg_loss:0.061, val_acc:0.961]
Epoch [41/120    avg_loss:0.043, val_acc:0.961]
Epoch [42/120    avg_loss:0.042, val_acc:0.963]
Epoch [43/120    avg_loss:0.037, val_acc:0.968]
Epoch [44/120    avg_loss:0.035, val_acc:0.958]
Epoch [45/120    avg_loss:0.031, val_acc:0.961]
Epoch [46/120    avg_loss:0.031, val_acc:0.957]
Epoch [47/120    avg_loss:0.029, val_acc:0.966]
Epoch [48/120    avg_loss:0.036, val_acc:0.968]
Epoch [49/120    avg_loss:0.030, val_acc:0.961]
Epoch [50/120    avg_loss:0.038, val_acc:0.965]
Epoch [51/120    avg_loss:0.037, val_acc:0.956]
Epoch [52/120    avg_loss:0.033, val_acc:0.965]
Epoch [53/120    avg_loss:0.023, val_acc:0.967]
Epoch [54/120    avg_loss:0.028, val_acc:0.965]
Epoch [55/120    avg_loss:0.029, val_acc:0.972]
Epoch [56/120    avg_loss:0.025, val_acc:0.965]
Epoch [57/120    avg_loss:0.024, val_acc:0.971]
Epoch [58/120    avg_loss:0.017, val_acc:0.968]
Epoch [59/120    avg_loss:0.018, val_acc:0.973]
Epoch [60/120    avg_loss:0.029, val_acc:0.956]
Epoch [61/120    avg_loss:0.029, val_acc:0.969]
Epoch [62/120    avg_loss:0.023, val_acc:0.967]
Epoch [63/120    avg_loss:0.020, val_acc:0.977]
Epoch [64/120    avg_loss:0.014, val_acc:0.974]
Epoch [65/120    avg_loss:0.017, val_acc:0.974]
Epoch [66/120    avg_loss:0.014, val_acc:0.975]
Epoch [67/120    avg_loss:0.012, val_acc:0.978]
Epoch [68/120    avg_loss:0.011, val_acc:0.975]
Epoch [69/120    avg_loss:0.014, val_acc:0.976]
Epoch [70/120    avg_loss:0.014, val_acc:0.970]
Epoch [71/120    avg_loss:0.017, val_acc:0.977]
Epoch [72/120    avg_loss:0.013, val_acc:0.969]
Epoch [73/120    avg_loss:0.011, val_acc:0.975]
Epoch [74/120    avg_loss:0.012, val_acc:0.977]
Epoch [75/120    avg_loss:0.022, val_acc:0.963]
Epoch [76/120    avg_loss:0.036, val_acc:0.960]
Epoch [77/120    avg_loss:0.022, val_acc:0.968]
Epoch [78/120    avg_loss:0.020, val_acc:0.978]
Epoch [79/120    avg_loss:0.013, val_acc:0.978]
Epoch [80/120    avg_loss:0.018, val_acc:0.970]
Epoch [81/120    avg_loss:0.013, val_acc:0.978]
Epoch [82/120    avg_loss:0.013, val_acc:0.980]
Epoch [83/120    avg_loss:0.009, val_acc:0.974]
Epoch [84/120    avg_loss:0.009, val_acc:0.976]
Epoch [85/120    avg_loss:0.014, val_acc:0.977]
Epoch [86/120    avg_loss:0.009, val_acc:0.982]
Epoch [87/120    avg_loss:0.007, val_acc:0.976]
Epoch [88/120    avg_loss:0.007, val_acc:0.984]
Epoch [89/120    avg_loss:0.007, val_acc:0.979]
Epoch [90/120    avg_loss:0.010, val_acc:0.979]
Epoch [91/120    avg_loss:0.008, val_acc:0.982]
Epoch [92/120    avg_loss:0.008, val_acc:0.977]
Epoch [93/120    avg_loss:0.008, val_acc:0.982]
Epoch [94/120    avg_loss:0.008, val_acc:0.980]
Epoch [95/120    avg_loss:0.007, val_acc:0.980]
Epoch [96/120    avg_loss:0.007, val_acc:0.979]
Epoch [97/120    avg_loss:0.006, val_acc:0.977]
Epoch [98/120    avg_loss:0.007, val_acc:0.972]
Epoch [99/120    avg_loss:0.008, val_acc:0.979]
Epoch [100/120    avg_loss:0.010, val_acc:0.979]
Epoch [101/120    avg_loss:0.013, val_acc:0.941]
Epoch [102/120    avg_loss:0.022, val_acc:0.974]
Epoch [103/120    avg_loss:0.009, val_acc:0.977]
Epoch [104/120    avg_loss:0.006, val_acc:0.979]
Epoch [105/120    avg_loss:0.004, val_acc:0.979]
Epoch [106/120    avg_loss:0.005, val_acc:0.979]
Epoch [107/120    avg_loss:0.005, val_acc:0.983]
Epoch [108/120    avg_loss:0.005, val_acc:0.983]
Epoch [109/120    avg_loss:0.005, val_acc:0.983]
Epoch [110/120    avg_loss:0.005, val_acc:0.983]
Epoch [111/120    avg_loss:0.005, val_acc:0.983]
Epoch [112/120    avg_loss:0.004, val_acc:0.983]
Epoch [113/120    avg_loss:0.005, val_acc:0.982]
Epoch [114/120    avg_loss:0.005, val_acc:0.982]
Epoch [115/120    avg_loss:0.005, val_acc:0.982]
Epoch [116/120    avg_loss:0.006, val_acc:0.982]
Epoch [117/120    avg_loss:0.005, val_acc:0.982]
Epoch [118/120    avg_loss:0.005, val_acc:0.982]
Epoch [119/120    avg_loss:0.005, val_acc:0.982]
Epoch [120/120    avg_loss:0.006, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1263    1    3    0    1    0    0    3    1   10    2    0
     0    1    0]
 [   0    0    0  732    1    4    1    0    0    3    1    0    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    1    6    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    2    2    0    0    0  834   26    0    0
     0    3    0]
 [   0    0   11    0    0    0    1    0    0    0   10 2178    5    3
     0    2    0]
 [   0    0    0    2    0    0    0    0    0    0    0    0  530    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1127   10    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
    12  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.12466124661246

F1 scores:
[       nan 0.98765432 0.98364486 0.98718813 0.99069767 0.98269896
 0.98050975 0.89285714 0.99650757 0.82926829 0.96864111 0.98440678
 0.98148148 0.9919571  0.98773006 0.9339207  0.97619048]

Kappa:
0.9786208025428615
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7c8cbac6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.566, val_acc:0.454]
Epoch [2/120    avg_loss:2.034, val_acc:0.530]
Epoch [3/120    avg_loss:1.783, val_acc:0.543]
Epoch [4/120    avg_loss:1.598, val_acc:0.609]
Epoch [5/120    avg_loss:1.428, val_acc:0.645]
Epoch [6/120    avg_loss:1.234, val_acc:0.670]
Epoch [7/120    avg_loss:1.079, val_acc:0.720]
Epoch [8/120    avg_loss:0.841, val_acc:0.741]
Epoch [9/120    avg_loss:0.745, val_acc:0.799]
Epoch [10/120    avg_loss:0.642, val_acc:0.810]
Epoch [11/120    avg_loss:0.646, val_acc:0.755]
Epoch [12/120    avg_loss:0.592, val_acc:0.771]
Epoch [13/120    avg_loss:0.536, val_acc:0.828]
Epoch [14/120    avg_loss:0.463, val_acc:0.852]
Epoch [15/120    avg_loss:0.396, val_acc:0.828]
Epoch [16/120    avg_loss:0.443, val_acc:0.849]
Epoch [17/120    avg_loss:0.358, val_acc:0.862]
Epoch [18/120    avg_loss:0.323, val_acc:0.861]
Epoch [19/120    avg_loss:0.297, val_acc:0.897]
Epoch [20/120    avg_loss:0.230, val_acc:0.905]
Epoch [21/120    avg_loss:0.201, val_acc:0.905]
Epoch [22/120    avg_loss:0.210, val_acc:0.898]
Epoch [23/120    avg_loss:0.197, val_acc:0.916]
Epoch [24/120    avg_loss:0.161, val_acc:0.936]
Epoch [25/120    avg_loss:0.126, val_acc:0.925]
Epoch [26/120    avg_loss:0.127, val_acc:0.935]
Epoch [27/120    avg_loss:0.109, val_acc:0.938]
Epoch [28/120    avg_loss:0.107, val_acc:0.939]
Epoch [29/120    avg_loss:0.117, val_acc:0.937]
Epoch [30/120    avg_loss:0.098, val_acc:0.932]
Epoch [31/120    avg_loss:0.089, val_acc:0.948]
Epoch [32/120    avg_loss:0.085, val_acc:0.952]
Epoch [33/120    avg_loss:0.075, val_acc:0.939]
Epoch [34/120    avg_loss:0.083, val_acc:0.958]
Epoch [35/120    avg_loss:0.065, val_acc:0.942]
Epoch [36/120    avg_loss:0.079, val_acc:0.949]
Epoch [37/120    avg_loss:0.076, val_acc:0.957]
Epoch [38/120    avg_loss:0.060, val_acc:0.950]
Epoch [39/120    avg_loss:0.067, val_acc:0.939]
Epoch [40/120    avg_loss:0.059, val_acc:0.949]
Epoch [41/120    avg_loss:0.072, val_acc:0.920]
Epoch [42/120    avg_loss:0.055, val_acc:0.950]
Epoch [43/120    avg_loss:0.051, val_acc:0.950]
Epoch [44/120    avg_loss:0.041, val_acc:0.955]
Epoch [45/120    avg_loss:0.050, val_acc:0.954]
Epoch [46/120    avg_loss:0.045, val_acc:0.955]
Epoch [47/120    avg_loss:0.064, val_acc:0.934]
Epoch [48/120    avg_loss:0.107, val_acc:0.950]
Epoch [49/120    avg_loss:0.057, val_acc:0.955]
Epoch [50/120    avg_loss:0.039, val_acc:0.959]
Epoch [51/120    avg_loss:0.036, val_acc:0.961]
Epoch [52/120    avg_loss:0.031, val_acc:0.962]
Epoch [53/120    avg_loss:0.035, val_acc:0.964]
Epoch [54/120    avg_loss:0.028, val_acc:0.962]
Epoch [55/120    avg_loss:0.037, val_acc:0.963]
Epoch [56/120    avg_loss:0.028, val_acc:0.962]
Epoch [57/120    avg_loss:0.031, val_acc:0.967]
Epoch [58/120    avg_loss:0.023, val_acc:0.967]
Epoch [59/120    avg_loss:0.023, val_acc:0.966]
Epoch [60/120    avg_loss:0.033, val_acc:0.971]
Epoch [61/120    avg_loss:0.024, val_acc:0.968]
Epoch [62/120    avg_loss:0.028, val_acc:0.971]
Epoch [63/120    avg_loss:0.026, val_acc:0.968]
Epoch [64/120    avg_loss:0.027, val_acc:0.970]
Epoch [65/120    avg_loss:0.028, val_acc:0.972]
Epoch [66/120    avg_loss:0.025, val_acc:0.971]
Epoch [67/120    avg_loss:0.028, val_acc:0.972]
Epoch [68/120    avg_loss:0.022, val_acc:0.971]
Epoch [69/120    avg_loss:0.029, val_acc:0.972]
Epoch [70/120    avg_loss:0.023, val_acc:0.971]
Epoch [71/120    avg_loss:0.022, val_acc:0.971]
Epoch [72/120    avg_loss:0.021, val_acc:0.972]
Epoch [73/120    avg_loss:0.025, val_acc:0.972]
Epoch [74/120    avg_loss:0.021, val_acc:0.974]
Epoch [75/120    avg_loss:0.023, val_acc:0.972]
Epoch [76/120    avg_loss:0.024, val_acc:0.972]
Epoch [77/120    avg_loss:0.021, val_acc:0.974]
Epoch [78/120    avg_loss:0.021, val_acc:0.973]
Epoch [79/120    avg_loss:0.022, val_acc:0.972]
Epoch [80/120    avg_loss:0.026, val_acc:0.973]
Epoch [81/120    avg_loss:0.024, val_acc:0.970]
Epoch [82/120    avg_loss:0.021, val_acc:0.973]
Epoch [83/120    avg_loss:0.018, val_acc:0.973]
Epoch [84/120    avg_loss:0.020, val_acc:0.971]
Epoch [85/120    avg_loss:0.022, val_acc:0.973]
Epoch [86/120    avg_loss:0.021, val_acc:0.973]
Epoch [87/120    avg_loss:0.022, val_acc:0.973]
Epoch [88/120    avg_loss:0.023, val_acc:0.972]
Epoch [89/120    avg_loss:0.018, val_acc:0.971]
Epoch [90/120    avg_loss:0.021, val_acc:0.973]
Epoch [91/120    avg_loss:0.021, val_acc:0.973]
Epoch [92/120    avg_loss:0.019, val_acc:0.973]
Epoch [93/120    avg_loss:0.019, val_acc:0.973]
Epoch [94/120    avg_loss:0.021, val_acc:0.973]
Epoch [95/120    avg_loss:0.018, val_acc:0.973]
Epoch [96/120    avg_loss:0.020, val_acc:0.973]
Epoch [97/120    avg_loss:0.019, val_acc:0.973]
Epoch [98/120    avg_loss:0.021, val_acc:0.974]
Epoch [99/120    avg_loss:0.019, val_acc:0.975]
Epoch [100/120    avg_loss:0.021, val_acc:0.975]
Epoch [101/120    avg_loss:0.018, val_acc:0.975]
Epoch [102/120    avg_loss:0.020, val_acc:0.975]
Epoch [103/120    avg_loss:0.021, val_acc:0.974]
Epoch [104/120    avg_loss:0.020, val_acc:0.974]
Epoch [105/120    avg_loss:0.024, val_acc:0.975]
Epoch [106/120    avg_loss:0.020, val_acc:0.975]
Epoch [107/120    avg_loss:0.024, val_acc:0.975]
Epoch [108/120    avg_loss:0.020, val_acc:0.975]
Epoch [109/120    avg_loss:0.020, val_acc:0.975]
Epoch [110/120    avg_loss:0.022, val_acc:0.974]
Epoch [111/120    avg_loss:0.021, val_acc:0.974]
Epoch [112/120    avg_loss:0.019, val_acc:0.974]
Epoch [113/120    avg_loss:0.020, val_acc:0.975]
Epoch [114/120    avg_loss:0.023, val_acc:0.974]
Epoch [115/120    avg_loss:0.022, val_acc:0.973]
Epoch [116/120    avg_loss:0.023, val_acc:0.973]
Epoch [117/120    avg_loss:0.018, val_acc:0.975]
Epoch [118/120    avg_loss:0.015, val_acc:0.973]
Epoch [119/120    avg_loss:0.022, val_acc:0.973]
Epoch [120/120    avg_loss:0.020, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1249    1    0    0    1    0    0    1    8   12   13    0
     0    0    0]
 [   0    0    0  691    7   13    0    0    0   13    1    0   16    6
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    2    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    9    3    0    0    2  825   26    0    0
     0    1    0]
 [   0    0    4    0    0    0    3    0    0    0   10 2191    0    1
     1    0    0]
 [   0    0    0    0    5    7    0    0    0    0    3    0  512    0
     2    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    1    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    11  313    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.50677506775068

F1 scores:
[       nan 0.975      0.98076168 0.96038916 0.97260274 0.96205357
 0.9753915  0.98039216 0.99767442 0.66666667 0.95652174 0.98649257
 0.95167286 0.98143236 0.99213974 0.94704992 0.97109827]

Kappa:
0.9715764262575547
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1448887748>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.549, val_acc:0.416]
Epoch [2/120    avg_loss:2.043, val_acc:0.600]
Epoch [3/120    avg_loss:1.763, val_acc:0.624]
Epoch [4/120    avg_loss:1.595, val_acc:0.635]
Epoch [5/120    avg_loss:1.370, val_acc:0.666]
Epoch [6/120    avg_loss:1.180, val_acc:0.679]
Epoch [7/120    avg_loss:1.051, val_acc:0.729]
Epoch [8/120    avg_loss:0.872, val_acc:0.757]
Epoch [9/120    avg_loss:0.757, val_acc:0.753]
Epoch [10/120    avg_loss:0.685, val_acc:0.771]
Epoch [11/120    avg_loss:0.585, val_acc:0.813]
Epoch [12/120    avg_loss:0.513, val_acc:0.826]
Epoch [13/120    avg_loss:0.437, val_acc:0.822]
Epoch [14/120    avg_loss:0.430, val_acc:0.826]
Epoch [15/120    avg_loss:0.374, val_acc:0.834]
Epoch [16/120    avg_loss:0.294, val_acc:0.846]
Epoch [17/120    avg_loss:0.271, val_acc:0.871]
Epoch [18/120    avg_loss:0.265, val_acc:0.878]
Epoch [19/120    avg_loss:0.285, val_acc:0.878]
Epoch [20/120    avg_loss:0.260, val_acc:0.882]
Epoch [21/120    avg_loss:0.216, val_acc:0.892]
Epoch [22/120    avg_loss:0.204, val_acc:0.915]
Epoch [23/120    avg_loss:0.185, val_acc:0.923]
Epoch [24/120    avg_loss:0.158, val_acc:0.918]
Epoch [25/120    avg_loss:0.132, val_acc:0.939]
Epoch [26/120    avg_loss:0.137, val_acc:0.926]
Epoch [27/120    avg_loss:0.114, val_acc:0.942]
Epoch [28/120    avg_loss:0.213, val_acc:0.909]
Epoch [29/120    avg_loss:0.151, val_acc:0.939]
Epoch [30/120    avg_loss:0.115, val_acc:0.962]
Epoch [31/120    avg_loss:0.093, val_acc:0.953]
Epoch [32/120    avg_loss:0.099, val_acc:0.958]
Epoch [33/120    avg_loss:0.097, val_acc:0.942]
Epoch [34/120    avg_loss:0.090, val_acc:0.962]
Epoch [35/120    avg_loss:0.096, val_acc:0.952]
Epoch [36/120    avg_loss:0.100, val_acc:0.959]
Epoch [37/120    avg_loss:0.066, val_acc:0.971]
Epoch [38/120    avg_loss:0.062, val_acc:0.958]
Epoch [39/120    avg_loss:0.063, val_acc:0.966]
Epoch [40/120    avg_loss:0.067, val_acc:0.967]
Epoch [41/120    avg_loss:0.062, val_acc:0.975]
Epoch [42/120    avg_loss:0.048, val_acc:0.974]
Epoch [43/120    avg_loss:0.067, val_acc:0.960]
Epoch [44/120    avg_loss:0.064, val_acc:0.976]
Epoch [45/120    avg_loss:0.055, val_acc:0.973]
Epoch [46/120    avg_loss:0.048, val_acc:0.972]
Epoch [47/120    avg_loss:0.060, val_acc:0.963]
Epoch [48/120    avg_loss:0.068, val_acc:0.975]
Epoch [49/120    avg_loss:0.042, val_acc:0.972]
Epoch [50/120    avg_loss:0.045, val_acc:0.971]
Epoch [51/120    avg_loss:0.051, val_acc:0.980]
Epoch [52/120    avg_loss:0.031, val_acc:0.979]
Epoch [53/120    avg_loss:0.031, val_acc:0.985]
Epoch [54/120    avg_loss:0.040, val_acc:0.974]
Epoch [55/120    avg_loss:0.036, val_acc:0.978]
Epoch [56/120    avg_loss:0.053, val_acc:0.975]
Epoch [57/120    avg_loss:0.050, val_acc:0.964]
Epoch [58/120    avg_loss:0.041, val_acc:0.972]
Epoch [59/120    avg_loss:0.035, val_acc:0.985]
Epoch [60/120    avg_loss:0.032, val_acc:0.979]
Epoch [61/120    avg_loss:0.032, val_acc:0.967]
Epoch [62/120    avg_loss:0.034, val_acc:0.975]
Epoch [63/120    avg_loss:0.030, val_acc:0.986]
Epoch [64/120    avg_loss:0.026, val_acc:0.974]
Epoch [65/120    avg_loss:0.034, val_acc:0.985]
Epoch [66/120    avg_loss:0.024, val_acc:0.980]
Epoch [67/120    avg_loss:0.023, val_acc:0.979]
Epoch [68/120    avg_loss:0.021, val_acc:0.988]
Epoch [69/120    avg_loss:0.024, val_acc:0.967]
Epoch [70/120    avg_loss:0.027, val_acc:0.984]
Epoch [71/120    avg_loss:0.029, val_acc:0.987]
Epoch [72/120    avg_loss:0.021, val_acc:0.980]
Epoch [73/120    avg_loss:0.022, val_acc:0.985]
Epoch [74/120    avg_loss:0.021, val_acc:0.985]
Epoch [75/120    avg_loss:0.019, val_acc:0.977]
Epoch [76/120    avg_loss:0.020, val_acc:0.976]
Epoch [77/120    avg_loss:0.022, val_acc:0.982]
Epoch [78/120    avg_loss:0.022, val_acc:0.983]
Epoch [79/120    avg_loss:0.026, val_acc:0.978]
Epoch [80/120    avg_loss:0.015, val_acc:0.991]
Epoch [81/120    avg_loss:0.015, val_acc:0.989]
Epoch [82/120    avg_loss:0.014, val_acc:0.995]
Epoch [83/120    avg_loss:0.014, val_acc:0.983]
Epoch [84/120    avg_loss:0.014, val_acc:0.985]
Epoch [85/120    avg_loss:0.013, val_acc:0.992]
Epoch [86/120    avg_loss:0.013, val_acc:0.988]
Epoch [87/120    avg_loss:0.010, val_acc:0.991]
Epoch [88/120    avg_loss:0.010, val_acc:0.989]
Epoch [89/120    avg_loss:0.010, val_acc:0.989]
Epoch [90/120    avg_loss:0.011, val_acc:0.989]
Epoch [91/120    avg_loss:0.013, val_acc:0.989]
Epoch [92/120    avg_loss:0.016, val_acc:0.987]
Epoch [93/120    avg_loss:0.016, val_acc:0.983]
Epoch [94/120    avg_loss:0.015, val_acc:0.968]
Epoch [95/120    avg_loss:0.019, val_acc:0.965]
Epoch [96/120    avg_loss:0.017, val_acc:0.979]
Epoch [97/120    avg_loss:0.010, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.987]
Epoch [99/120    avg_loss:0.011, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.987]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.988]
Epoch [103/120    avg_loss:0.008, val_acc:0.990]
Epoch [104/120    avg_loss:0.006, val_acc:0.990]
Epoch [105/120    avg_loss:0.009, val_acc:0.991]
Epoch [106/120    avg_loss:0.007, val_acc:0.990]
Epoch [107/120    avg_loss:0.006, val_acc:0.989]
Epoch [108/120    avg_loss:0.007, val_acc:0.990]
Epoch [109/120    avg_loss:0.006, val_acc:0.990]
Epoch [110/120    avg_loss:0.008, val_acc:0.990]
Epoch [111/120    avg_loss:0.007, val_acc:0.990]
Epoch [112/120    avg_loss:0.007, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.990]
Epoch [114/120    avg_loss:0.007, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.008, val_acc:0.990]
Epoch [117/120    avg_loss:0.006, val_acc:0.990]
Epoch [118/120    avg_loss:0.007, val_acc:0.990]
Epoch [119/120    avg_loss:0.008, val_acc:0.990]
Epoch [120/120    avg_loss:0.007, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1263    3    0    0    0    0    0    0    3   13    3    0
     0    0    0]
 [   0    0    1  709    6    3    0    0    0    7    0    0   20    1
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    6    2    0    0    0  855    9    0    0
     0    3    0]
 [   0    0    7    0    0    0    1    0    0    0   10 2189    1    2
     0    0    0]
 [   0    0    0    0    1   11    0    0    0    0    1    4  516    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    2    1    0    0
  1125    6    0]
 [   0    0    0    0    0    0   29    0    0    0    0    0    0    0
    11  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.02710027100271

F1 scores:
[       nan 0.98765432 0.98787642 0.96990424 0.98148148 0.96860987
 0.97470238 1.         1.         0.74418605 0.97938144 0.98848499
 0.96089385 0.9919571  0.98901099 0.92609351 0.99408284]

Kappa:
0.9775083290316787
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcfed720748>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.575, val_acc:0.366]
Epoch [2/120    avg_loss:2.061, val_acc:0.507]
Epoch [3/120    avg_loss:1.825, val_acc:0.567]
Epoch [4/120    avg_loss:1.605, val_acc:0.596]
Epoch [5/120    avg_loss:1.389, val_acc:0.617]
Epoch [6/120    avg_loss:1.173, val_acc:0.649]
Epoch [7/120    avg_loss:1.019, val_acc:0.692]
Epoch [8/120    avg_loss:0.844, val_acc:0.753]
Epoch [9/120    avg_loss:0.725, val_acc:0.764]
Epoch [10/120    avg_loss:0.673, val_acc:0.793]
Epoch [11/120    avg_loss:0.608, val_acc:0.790]
Epoch [12/120    avg_loss:0.594, val_acc:0.825]
Epoch [13/120    avg_loss:0.404, val_acc:0.846]
Epoch [14/120    avg_loss:0.377, val_acc:0.830]
Epoch [15/120    avg_loss:0.346, val_acc:0.839]
Epoch [16/120    avg_loss:0.287, val_acc:0.852]
Epoch [17/120    avg_loss:0.391, val_acc:0.825]
Epoch [18/120    avg_loss:0.448, val_acc:0.786]
Epoch [19/120    avg_loss:0.460, val_acc:0.828]
Epoch [20/120    avg_loss:0.323, val_acc:0.855]
Epoch [21/120    avg_loss:0.222, val_acc:0.910]
Epoch [22/120    avg_loss:0.164, val_acc:0.913]
Epoch [23/120    avg_loss:0.177, val_acc:0.905]
Epoch [24/120    avg_loss:0.197, val_acc:0.903]
Epoch [25/120    avg_loss:0.144, val_acc:0.918]
Epoch [26/120    avg_loss:0.167, val_acc:0.902]
Epoch [27/120    avg_loss:0.135, val_acc:0.920]
Epoch [28/120    avg_loss:0.133, val_acc:0.930]
Epoch [29/120    avg_loss:0.130, val_acc:0.925]
Epoch [30/120    avg_loss:0.116, val_acc:0.922]
Epoch [31/120    avg_loss:0.085, val_acc:0.926]
Epoch [32/120    avg_loss:0.084, val_acc:0.947]
Epoch [33/120    avg_loss:0.066, val_acc:0.942]
Epoch [34/120    avg_loss:0.073, val_acc:0.943]
Epoch [35/120    avg_loss:0.069, val_acc:0.937]
Epoch [36/120    avg_loss:0.076, val_acc:0.943]
Epoch [37/120    avg_loss:0.079, val_acc:0.939]
Epoch [38/120    avg_loss:0.080, val_acc:0.924]
Epoch [39/120    avg_loss:0.080, val_acc:0.943]
Epoch [40/120    avg_loss:0.067, val_acc:0.928]
Epoch [41/120    avg_loss:0.076, val_acc:0.953]
Epoch [42/120    avg_loss:0.049, val_acc:0.955]
Epoch [43/120    avg_loss:0.045, val_acc:0.958]
Epoch [44/120    avg_loss:0.038, val_acc:0.952]
Epoch [45/120    avg_loss:0.040, val_acc:0.948]
Epoch [46/120    avg_loss:0.044, val_acc:0.961]
Epoch [47/120    avg_loss:0.056, val_acc:0.971]
Epoch [48/120    avg_loss:0.050, val_acc:0.963]
Epoch [49/120    avg_loss:0.043, val_acc:0.954]
Epoch [50/120    avg_loss:0.042, val_acc:0.958]
Epoch [51/120    avg_loss:0.052, val_acc:0.962]
Epoch [52/120    avg_loss:0.046, val_acc:0.960]
Epoch [53/120    avg_loss:0.036, val_acc:0.958]
Epoch [54/120    avg_loss:0.032, val_acc:0.938]
Epoch [55/120    avg_loss:0.031, val_acc:0.974]
Epoch [56/120    avg_loss:0.024, val_acc:0.974]
Epoch [57/120    avg_loss:0.022, val_acc:0.968]
Epoch [58/120    avg_loss:0.027, val_acc:0.972]
Epoch [59/120    avg_loss:0.023, val_acc:0.972]
Epoch [60/120    avg_loss:0.054, val_acc:0.942]
Epoch [61/120    avg_loss:0.077, val_acc:0.947]
Epoch [62/120    avg_loss:0.057, val_acc:0.945]
Epoch [63/120    avg_loss:0.052, val_acc:0.959]
Epoch [64/120    avg_loss:0.030, val_acc:0.963]
Epoch [65/120    avg_loss:0.036, val_acc:0.975]
Epoch [66/120    avg_loss:0.030, val_acc:0.954]
Epoch [67/120    avg_loss:0.029, val_acc:0.967]
Epoch [68/120    avg_loss:0.026, val_acc:0.983]
Epoch [69/120    avg_loss:0.021, val_acc:0.971]
Epoch [70/120    avg_loss:0.024, val_acc:0.970]
Epoch [71/120    avg_loss:0.029, val_acc:0.974]
Epoch [72/120    avg_loss:0.049, val_acc:0.977]
Epoch [73/120    avg_loss:0.022, val_acc:0.970]
Epoch [74/120    avg_loss:0.017, val_acc:0.977]
Epoch [75/120    avg_loss:0.014, val_acc:0.975]
Epoch [76/120    avg_loss:0.024, val_acc:0.974]
Epoch [77/120    avg_loss:0.026, val_acc:0.972]
Epoch [78/120    avg_loss:0.013, val_acc:0.975]
Epoch [79/120    avg_loss:0.017, val_acc:0.976]
Epoch [80/120    avg_loss:0.011, val_acc:0.977]
Epoch [81/120    avg_loss:0.022, val_acc:0.984]
Epoch [82/120    avg_loss:0.013, val_acc:0.980]
Epoch [83/120    avg_loss:0.010, val_acc:0.976]
Epoch [84/120    avg_loss:0.015, val_acc:0.965]
Epoch [85/120    avg_loss:0.018, val_acc:0.966]
Epoch [86/120    avg_loss:0.013, val_acc:0.978]
Epoch [87/120    avg_loss:0.012, val_acc:0.984]
Epoch [88/120    avg_loss:0.013, val_acc:0.977]
Epoch [89/120    avg_loss:0.022, val_acc:0.980]
Epoch [90/120    avg_loss:0.023, val_acc:0.976]
Epoch [91/120    avg_loss:0.044, val_acc:0.971]
Epoch [92/120    avg_loss:0.056, val_acc:0.953]
Epoch [93/120    avg_loss:0.293, val_acc:0.871]
Epoch [94/120    avg_loss:0.378, val_acc:0.876]
Epoch [95/120    avg_loss:0.217, val_acc:0.902]
Epoch [96/120    avg_loss:0.121, val_acc:0.933]
Epoch [97/120    avg_loss:0.082, val_acc:0.951]
Epoch [98/120    avg_loss:0.050, val_acc:0.961]
Epoch [99/120    avg_loss:0.046, val_acc:0.947]
Epoch [100/120    avg_loss:0.041, val_acc:0.962]
Epoch [101/120    avg_loss:0.030, val_acc:0.967]
Epoch [102/120    avg_loss:0.021, val_acc:0.964]
Epoch [103/120    avg_loss:0.026, val_acc:0.970]
Epoch [104/120    avg_loss:0.019, val_acc:0.970]
Epoch [105/120    avg_loss:0.022, val_acc:0.970]
Epoch [106/120    avg_loss:0.022, val_acc:0.970]
Epoch [107/120    avg_loss:0.025, val_acc:0.974]
Epoch [108/120    avg_loss:0.022, val_acc:0.974]
Epoch [109/120    avg_loss:0.023, val_acc:0.975]
Epoch [110/120    avg_loss:0.018, val_acc:0.974]
Epoch [111/120    avg_loss:0.022, val_acc:0.972]
Epoch [112/120    avg_loss:0.023, val_acc:0.972]
Epoch [113/120    avg_loss:0.018, val_acc:0.971]
Epoch [114/120    avg_loss:0.020, val_acc:0.971]
Epoch [115/120    avg_loss:0.022, val_acc:0.971]
Epoch [116/120    avg_loss:0.019, val_acc:0.972]
Epoch [117/120    avg_loss:0.019, val_acc:0.973]
Epoch [118/120    avg_loss:0.022, val_acc:0.973]
Epoch [119/120    avg_loss:0.019, val_acc:0.972]
Epoch [120/120    avg_loss:0.019, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1249    5    0    0    2    0    0    0   13   12    0    4
     0    0    0]
 [   0    0    0  716    0    1    0    0    0   10    3    0   14    3
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  428    1    1    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  649    0    0    0    0    3    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    8    4    0    0    0  851    0    0    0
     0    7    0]
 [   0    0   10    0    0    0    2    0    0    0   10 2181    5    2
     0    0    0]
 [   0    0    4    0    0   11    0    0    0    0    0    5  510    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1129    6    0]
 [   0    0    0    0    0    1   28    0    0    0    0    0    0    0
    11  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.63685636856368

F1 scores:
[       nan 0.975      0.97807361 0.97414966 0.99764706 0.96832579
 0.96649293 0.98039216 0.99883856 0.69565217 0.96924829 0.98889141
 0.95505618 0.9762533  0.98731963 0.92053973 0.95238095]

Kappa:
0.9730670728120748
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f88cc79b780>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.517, val_acc:0.502]
Epoch [2/120    avg_loss:2.032, val_acc:0.559]
Epoch [3/120    avg_loss:1.739, val_acc:0.596]
Epoch [4/120    avg_loss:1.549, val_acc:0.624]
Epoch [5/120    avg_loss:1.331, val_acc:0.690]
Epoch [6/120    avg_loss:1.156, val_acc:0.701]
Epoch [7/120    avg_loss:0.971, val_acc:0.715]
Epoch [8/120    avg_loss:0.900, val_acc:0.741]
Epoch [9/120    avg_loss:0.811, val_acc:0.752]
Epoch [10/120    avg_loss:0.697, val_acc:0.767]
Epoch [11/120    avg_loss:0.598, val_acc:0.840]
Epoch [12/120    avg_loss:0.541, val_acc:0.849]
Epoch [13/120    avg_loss:0.538, val_acc:0.830]
Epoch [14/120    avg_loss:0.429, val_acc:0.871]
Epoch [15/120    avg_loss:0.412, val_acc:0.876]
Epoch [16/120    avg_loss:0.340, val_acc:0.878]
Epoch [17/120    avg_loss:0.432, val_acc:0.863]
Epoch [18/120    avg_loss:0.327, val_acc:0.895]
Epoch [19/120    avg_loss:0.247, val_acc:0.900]
Epoch [20/120    avg_loss:0.212, val_acc:0.912]
Epoch [21/120    avg_loss:0.183, val_acc:0.905]
Epoch [22/120    avg_loss:0.201, val_acc:0.914]
Epoch [23/120    avg_loss:0.216, val_acc:0.896]
Epoch [24/120    avg_loss:0.214, val_acc:0.917]
Epoch [25/120    avg_loss:0.220, val_acc:0.923]
Epoch [26/120    avg_loss:0.164, val_acc:0.926]
Epoch [27/120    avg_loss:0.149, val_acc:0.930]
Epoch [28/120    avg_loss:0.142, val_acc:0.949]
Epoch [29/120    avg_loss:0.135, val_acc:0.939]
Epoch [30/120    avg_loss:0.124, val_acc:0.953]
Epoch [31/120    avg_loss:0.113, val_acc:0.947]
Epoch [32/120    avg_loss:0.095, val_acc:0.959]
Epoch [33/120    avg_loss:0.085, val_acc:0.954]
Epoch [34/120    avg_loss:0.103, val_acc:0.936]
Epoch [35/120    avg_loss:0.148, val_acc:0.924]
Epoch [36/120    avg_loss:0.099, val_acc:0.958]
Epoch [37/120    avg_loss:0.086, val_acc:0.947]
Epoch [38/120    avg_loss:0.092, val_acc:0.953]
Epoch [39/120    avg_loss:0.075, val_acc:0.960]
Epoch [40/120    avg_loss:0.120, val_acc:0.939]
Epoch [41/120    avg_loss:0.079, val_acc:0.961]
Epoch [42/120    avg_loss:0.073, val_acc:0.953]
Epoch [43/120    avg_loss:0.072, val_acc:0.951]
Epoch [44/120    avg_loss:0.077, val_acc:0.947]
Epoch [45/120    avg_loss:0.051, val_acc:0.958]
Epoch [46/120    avg_loss:0.043, val_acc:0.972]
Epoch [47/120    avg_loss:0.048, val_acc:0.951]
Epoch [48/120    avg_loss:0.041, val_acc:0.970]
Epoch [49/120    avg_loss:0.038, val_acc:0.966]
Epoch [50/120    avg_loss:0.033, val_acc:0.972]
Epoch [51/120    avg_loss:0.031, val_acc:0.967]
Epoch [52/120    avg_loss:0.033, val_acc:0.970]
Epoch [53/120    avg_loss:0.030, val_acc:0.966]
Epoch [54/120    avg_loss:0.034, val_acc:0.971]
Epoch [55/120    avg_loss:0.035, val_acc:0.972]
Epoch [56/120    avg_loss:0.029, val_acc:0.972]
Epoch [57/120    avg_loss:0.035, val_acc:0.967]
Epoch [58/120    avg_loss:0.025, val_acc:0.979]
Epoch [59/120    avg_loss:0.021, val_acc:0.980]
Epoch [60/120    avg_loss:0.025, val_acc:0.973]
Epoch [61/120    avg_loss:0.023, val_acc:0.973]
Epoch [62/120    avg_loss:0.021, val_acc:0.975]
Epoch [63/120    avg_loss:0.021, val_acc:0.970]
Epoch [64/120    avg_loss:0.021, val_acc:0.963]
Epoch [65/120    avg_loss:0.023, val_acc:0.972]
Epoch [66/120    avg_loss:0.023, val_acc:0.966]
Epoch [67/120    avg_loss:0.033, val_acc:0.968]
Epoch [68/120    avg_loss:0.030, val_acc:0.972]
Epoch [69/120    avg_loss:0.033, val_acc:0.972]
Epoch [70/120    avg_loss:0.027, val_acc:0.978]
Epoch [71/120    avg_loss:0.026, val_acc:0.973]
Epoch [72/120    avg_loss:0.023, val_acc:0.975]
Epoch [73/120    avg_loss:0.019, val_acc:0.978]
Epoch [74/120    avg_loss:0.015, val_acc:0.979]
Epoch [75/120    avg_loss:0.011, val_acc:0.979]
Epoch [76/120    avg_loss:0.011, val_acc:0.980]
Epoch [77/120    avg_loss:0.012, val_acc:0.980]
Epoch [78/120    avg_loss:0.013, val_acc:0.978]
Epoch [79/120    avg_loss:0.011, val_acc:0.979]
Epoch [80/120    avg_loss:0.012, val_acc:0.982]
Epoch [81/120    avg_loss:0.011, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.983]
Epoch [83/120    avg_loss:0.012, val_acc:0.982]
Epoch [84/120    avg_loss:0.012, val_acc:0.982]
Epoch [85/120    avg_loss:0.012, val_acc:0.980]
Epoch [86/120    avg_loss:0.012, val_acc:0.982]
Epoch [87/120    avg_loss:0.011, val_acc:0.982]
Epoch [88/120    avg_loss:0.011, val_acc:0.982]
Epoch [89/120    avg_loss:0.014, val_acc:0.979]
Epoch [90/120    avg_loss:0.012, val_acc:0.980]
Epoch [91/120    avg_loss:0.010, val_acc:0.982]
Epoch [92/120    avg_loss:0.013, val_acc:0.983]
Epoch [93/120    avg_loss:0.011, val_acc:0.984]
Epoch [94/120    avg_loss:0.010, val_acc:0.984]
Epoch [95/120    avg_loss:0.008, val_acc:0.985]
Epoch [96/120    avg_loss:0.011, val_acc:0.985]
Epoch [97/120    avg_loss:0.012, val_acc:0.984]
Epoch [98/120    avg_loss:0.010, val_acc:0.983]
Epoch [99/120    avg_loss:0.009, val_acc:0.984]
Epoch [100/120    avg_loss:0.011, val_acc:0.984]
Epoch [101/120    avg_loss:0.010, val_acc:0.984]
Epoch [102/120    avg_loss:0.010, val_acc:0.984]
Epoch [103/120    avg_loss:0.012, val_acc:0.984]
Epoch [104/120    avg_loss:0.010, val_acc:0.984]
Epoch [105/120    avg_loss:0.010, val_acc:0.983]
Epoch [106/120    avg_loss:0.008, val_acc:0.983]
Epoch [107/120    avg_loss:0.009, val_acc:0.982]
Epoch [108/120    avg_loss:0.009, val_acc:0.983]
Epoch [109/120    avg_loss:0.011, val_acc:0.983]
Epoch [110/120    avg_loss:0.011, val_acc:0.983]
Epoch [111/120    avg_loss:0.010, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.983]
Epoch [113/120    avg_loss:0.009, val_acc:0.983]
Epoch [114/120    avg_loss:0.010, val_acc:0.983]
Epoch [115/120    avg_loss:0.011, val_acc:0.983]
Epoch [116/120    avg_loss:0.010, val_acc:0.983]
Epoch [117/120    avg_loss:0.008, val_acc:0.983]
Epoch [118/120    avg_loss:0.010, val_acc:0.983]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.009, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1261    1    0    0    0    0    0    0    7    7    8    0
     0    1    0]
 [   0    0    0  715    0    2    0    0    0    9    0    0   14    7
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    5    2    0    0    0  847   15    0    0
     0    2    0]
 [   0    0    8    0    0    0    3    0    0    0   13 2184    0    2
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0  531    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1135    3    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    14  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.28726287262873

F1 scores:
[       nan 0.975      0.98592651 0.97544338 0.99764706 0.98855835
 0.98271976 1.         1.         0.71111111 0.97021764 0.9889065
 0.97610294 0.9762533  0.9908337  0.94783905 0.98224852]

Kappa:
0.980477017107346
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f81450df710>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.567, val_acc:0.473]
Epoch [2/120    avg_loss:2.047, val_acc:0.555]
Epoch [3/120    avg_loss:1.824, val_acc:0.583]
Epoch [4/120    avg_loss:1.549, val_acc:0.652]
Epoch [5/120    avg_loss:1.364, val_acc:0.666]
Epoch [6/120    avg_loss:1.220, val_acc:0.678]
Epoch [7/120    avg_loss:1.034, val_acc:0.714]
Epoch [8/120    avg_loss:0.946, val_acc:0.711]
Epoch [9/120    avg_loss:0.784, val_acc:0.782]
Epoch [10/120    avg_loss:0.740, val_acc:0.780]
Epoch [11/120    avg_loss:0.663, val_acc:0.774]
Epoch [12/120    avg_loss:0.594, val_acc:0.790]
Epoch [13/120    avg_loss:0.481, val_acc:0.826]
Epoch [14/120    avg_loss:0.457, val_acc:0.845]
Epoch [15/120    avg_loss:0.399, val_acc:0.859]
Epoch [16/120    avg_loss:0.325, val_acc:0.877]
Epoch [17/120    avg_loss:0.326, val_acc:0.900]
Epoch [18/120    avg_loss:0.300, val_acc:0.902]
Epoch [19/120    avg_loss:0.254, val_acc:0.891]
Epoch [20/120    avg_loss:0.208, val_acc:0.907]
Epoch [21/120    avg_loss:0.224, val_acc:0.908]
Epoch [22/120    avg_loss:0.205, val_acc:0.902]
Epoch [23/120    avg_loss:0.199, val_acc:0.915]
Epoch [24/120    avg_loss:0.207, val_acc:0.914]
Epoch [25/120    avg_loss:0.174, val_acc:0.913]
Epoch [26/120    avg_loss:0.154, val_acc:0.918]
Epoch [27/120    avg_loss:0.144, val_acc:0.911]
Epoch [28/120    avg_loss:0.144, val_acc:0.930]
Epoch [29/120    avg_loss:0.106, val_acc:0.934]
Epoch [30/120    avg_loss:0.095, val_acc:0.929]
Epoch [31/120    avg_loss:0.092, val_acc:0.937]
Epoch [32/120    avg_loss:0.093, val_acc:0.949]
Epoch [33/120    avg_loss:0.083, val_acc:0.955]
Epoch [34/120    avg_loss:0.093, val_acc:0.951]
Epoch [35/120    avg_loss:0.094, val_acc:0.957]
Epoch [36/120    avg_loss:0.094, val_acc:0.951]
Epoch [37/120    avg_loss:0.111, val_acc:0.947]
Epoch [38/120    avg_loss:0.099, val_acc:0.955]
Epoch [39/120    avg_loss:0.071, val_acc:0.953]
Epoch [40/120    avg_loss:0.060, val_acc:0.948]
Epoch [41/120    avg_loss:0.048, val_acc:0.948]
Epoch [42/120    avg_loss:0.045, val_acc:0.964]
Epoch [43/120    avg_loss:0.043, val_acc:0.963]
Epoch [44/120    avg_loss:0.046, val_acc:0.959]
Epoch [45/120    avg_loss:0.046, val_acc:0.963]
Epoch [46/120    avg_loss:0.048, val_acc:0.959]
Epoch [47/120    avg_loss:0.066, val_acc:0.970]
Epoch [48/120    avg_loss:0.053, val_acc:0.957]
Epoch [49/120    avg_loss:0.049, val_acc:0.965]
Epoch [50/120    avg_loss:0.042, val_acc:0.960]
Epoch [51/120    avg_loss:0.060, val_acc:0.960]
Epoch [52/120    avg_loss:0.069, val_acc:0.966]
Epoch [53/120    avg_loss:0.050, val_acc:0.961]
Epoch [54/120    avg_loss:0.045, val_acc:0.963]
Epoch [55/120    avg_loss:0.037, val_acc:0.976]
Epoch [56/120    avg_loss:0.026, val_acc:0.967]
Epoch [57/120    avg_loss:0.036, val_acc:0.966]
Epoch [58/120    avg_loss:0.028, val_acc:0.968]
Epoch [59/120    avg_loss:0.023, val_acc:0.974]
Epoch [60/120    avg_loss:0.030, val_acc:0.974]
Epoch [61/120    avg_loss:0.030, val_acc:0.970]
Epoch [62/120    avg_loss:0.021, val_acc:0.976]
Epoch [63/120    avg_loss:0.017, val_acc:0.978]
Epoch [64/120    avg_loss:0.019, val_acc:0.976]
Epoch [65/120    avg_loss:0.022, val_acc:0.977]
Epoch [66/120    avg_loss:0.017, val_acc:0.982]
Epoch [67/120    avg_loss:0.015, val_acc:0.982]
Epoch [68/120    avg_loss:0.018, val_acc:0.978]
Epoch [69/120    avg_loss:0.014, val_acc:0.977]
Epoch [70/120    avg_loss:0.014, val_acc:0.978]
Epoch [71/120    avg_loss:0.013, val_acc:0.977]
Epoch [72/120    avg_loss:0.014, val_acc:0.978]
Epoch [73/120    avg_loss:0.018, val_acc:0.977]
Epoch [74/120    avg_loss:0.013, val_acc:0.979]
Epoch [75/120    avg_loss:0.011, val_acc:0.978]
Epoch [76/120    avg_loss:0.012, val_acc:0.977]
Epoch [77/120    avg_loss:0.011, val_acc:0.984]
Epoch [78/120    avg_loss:0.013, val_acc:0.980]
Epoch [79/120    avg_loss:0.011, val_acc:0.983]
Epoch [80/120    avg_loss:0.009, val_acc:0.980]
Epoch [81/120    avg_loss:0.011, val_acc:0.983]
Epoch [82/120    avg_loss:0.011, val_acc:0.978]
Epoch [83/120    avg_loss:0.011, val_acc:0.982]
Epoch [84/120    avg_loss:0.010, val_acc:0.980]
Epoch [85/120    avg_loss:0.010, val_acc:0.982]
Epoch [86/120    avg_loss:0.011, val_acc:0.980]
Epoch [87/120    avg_loss:0.009, val_acc:0.983]
Epoch [88/120    avg_loss:0.011, val_acc:0.984]
Epoch [89/120    avg_loss:0.011, val_acc:0.983]
Epoch [90/120    avg_loss:0.009, val_acc:0.982]
Epoch [91/120    avg_loss:0.008, val_acc:0.979]
Epoch [92/120    avg_loss:0.010, val_acc:0.984]
Epoch [93/120    avg_loss:0.016, val_acc:0.978]
Epoch [94/120    avg_loss:0.012, val_acc:0.982]
Epoch [95/120    avg_loss:0.012, val_acc:0.973]
Epoch [96/120    avg_loss:0.035, val_acc:0.980]
Epoch [97/120    avg_loss:0.044, val_acc:0.970]
Epoch [98/120    avg_loss:0.036, val_acc:0.967]
Epoch [99/120    avg_loss:0.047, val_acc:0.948]
Epoch [100/120    avg_loss:0.025, val_acc:0.976]
Epoch [101/120    avg_loss:0.016, val_acc:0.975]
Epoch [102/120    avg_loss:0.014, val_acc:0.980]
Epoch [103/120    avg_loss:0.015, val_acc:0.980]
Epoch [104/120    avg_loss:0.015, val_acc:0.977]
Epoch [105/120    avg_loss:0.011, val_acc:0.980]
Epoch [106/120    avg_loss:0.007, val_acc:0.980]
Epoch [107/120    avg_loss:0.007, val_acc:0.979]
Epoch [108/120    avg_loss:0.010, val_acc:0.982]
Epoch [109/120    avg_loss:0.006, val_acc:0.982]
Epoch [110/120    avg_loss:0.007, val_acc:0.982]
Epoch [111/120    avg_loss:0.006, val_acc:0.982]
Epoch [112/120    avg_loss:0.007, val_acc:0.979]
Epoch [113/120    avg_loss:0.007, val_acc:0.980]
Epoch [114/120    avg_loss:0.006, val_acc:0.980]
Epoch [115/120    avg_loss:0.007, val_acc:0.980]
Epoch [116/120    avg_loss:0.006, val_acc:0.980]
Epoch [117/120    avg_loss:0.008, val_acc:0.979]
Epoch [118/120    avg_loss:0.007, val_acc:0.980]
Epoch [119/120    avg_loss:0.008, val_acc:0.980]
Epoch [120/120    avg_loss:0.007, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1256    2    4    0    1    0    0    1    7   11    1    0
     0    2    0]
 [   0    0    0  697    0    8    0    0    0   16    1    2   21    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    3    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    5    1    0    0    0  853   11    2    0
     0    1    0]
 [   0    0    4    0    0    0    1    0    0    0   10 2194    0    1
     0    0    0]
 [   0    0    0    0    2    3    0    0    0    0    0    0  527    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    0    0    0
  1132    4    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    10  326    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.18970189701896

F1 scores:
[       nan 0.98765432 0.98625834 0.96337249 0.98611111 0.9738339
 0.98869631 0.98039216 0.99650757 0.61818182 0.97541452 0.99074283
 0.96697248 0.9919571  0.99124343 0.95882353 0.97005988]

Kappa:
0.9793658851175738
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f27e367d6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.578, val_acc:0.414]
Epoch [2/120    avg_loss:2.062, val_acc:0.543]
Epoch [3/120    avg_loss:1.786, val_acc:0.599]
Epoch [4/120    avg_loss:1.567, val_acc:0.615]
Epoch [5/120    avg_loss:1.373, val_acc:0.643]
Epoch [6/120    avg_loss:1.162, val_acc:0.649]
Epoch [7/120    avg_loss:1.037, val_acc:0.683]
Epoch [8/120    avg_loss:0.921, val_acc:0.733]
Epoch [9/120    avg_loss:0.785, val_acc:0.721]
Epoch [10/120    avg_loss:0.692, val_acc:0.755]
Epoch [11/120    avg_loss:0.545, val_acc:0.799]
Epoch [12/120    avg_loss:0.516, val_acc:0.817]
Epoch [13/120    avg_loss:0.472, val_acc:0.823]
Epoch [14/120    avg_loss:0.396, val_acc:0.848]
Epoch [15/120    avg_loss:0.402, val_acc:0.863]
Epoch [16/120    avg_loss:0.337, val_acc:0.851]
Epoch [17/120    avg_loss:0.335, val_acc:0.885]
Epoch [18/120    avg_loss:0.288, val_acc:0.901]
Epoch [19/120    avg_loss:0.239, val_acc:0.905]
Epoch [20/120    avg_loss:0.238, val_acc:0.914]
Epoch [21/120    avg_loss:0.177, val_acc:0.930]
Epoch [22/120    avg_loss:0.190, val_acc:0.912]
Epoch [23/120    avg_loss:0.196, val_acc:0.907]
Epoch [24/120    avg_loss:0.195, val_acc:0.912]
Epoch [25/120    avg_loss:0.168, val_acc:0.926]
Epoch [26/120    avg_loss:0.152, val_acc:0.948]
Epoch [27/120    avg_loss:0.118, val_acc:0.932]
Epoch [28/120    avg_loss:0.120, val_acc:0.951]
Epoch [29/120    avg_loss:0.099, val_acc:0.934]
Epoch [30/120    avg_loss:0.091, val_acc:0.943]
Epoch [31/120    avg_loss:0.096, val_acc:0.958]
Epoch [32/120    avg_loss:0.098, val_acc:0.947]
Epoch [33/120    avg_loss:0.088, val_acc:0.950]
Epoch [34/120    avg_loss:0.087, val_acc:0.964]
Epoch [35/120    avg_loss:0.065, val_acc:0.960]
Epoch [36/120    avg_loss:0.069, val_acc:0.958]
Epoch [37/120    avg_loss:0.056, val_acc:0.973]
Epoch [38/120    avg_loss:0.062, val_acc:0.971]
Epoch [39/120    avg_loss:0.070, val_acc:0.972]
Epoch [40/120    avg_loss:0.069, val_acc:0.957]
Epoch [41/120    avg_loss:0.067, val_acc:0.968]
Epoch [42/120    avg_loss:0.053, val_acc:0.974]
Epoch [43/120    avg_loss:0.060, val_acc:0.978]
Epoch [44/120    avg_loss:0.046, val_acc:0.959]
Epoch [45/120    avg_loss:0.050, val_acc:0.971]
Epoch [46/120    avg_loss:0.066, val_acc:0.966]
Epoch [47/120    avg_loss:0.076, val_acc:0.962]
Epoch [48/120    avg_loss:0.071, val_acc:0.967]
Epoch [49/120    avg_loss:0.075, val_acc:0.971]
Epoch [50/120    avg_loss:0.059, val_acc:0.971]
Epoch [51/120    avg_loss:0.057, val_acc:0.971]
Epoch [52/120    avg_loss:0.033, val_acc:0.974]
Epoch [53/120    avg_loss:0.041, val_acc:0.972]
Epoch [54/120    avg_loss:0.029, val_acc:0.980]
Epoch [55/120    avg_loss:0.026, val_acc:0.976]
Epoch [56/120    avg_loss:0.029, val_acc:0.972]
Epoch [57/120    avg_loss:0.037, val_acc:0.983]
Epoch [58/120    avg_loss:0.032, val_acc:0.961]
Epoch [59/120    avg_loss:0.046, val_acc:0.977]
Epoch [60/120    avg_loss:0.035, val_acc:0.975]
Epoch [61/120    avg_loss:0.042, val_acc:0.972]
Epoch [62/120    avg_loss:0.031, val_acc:0.977]
Epoch [63/120    avg_loss:0.030, val_acc:0.977]
Epoch [64/120    avg_loss:0.021, val_acc:0.987]
Epoch [65/120    avg_loss:0.020, val_acc:0.980]
Epoch [66/120    avg_loss:0.017, val_acc:0.977]
Epoch [67/120    avg_loss:0.020, val_acc:0.977]
Epoch [68/120    avg_loss:0.022, val_acc:0.974]
Epoch [69/120    avg_loss:0.018, val_acc:0.987]
Epoch [70/120    avg_loss:0.015, val_acc:0.980]
Epoch [71/120    avg_loss:0.014, val_acc:0.983]
Epoch [72/120    avg_loss:0.014, val_acc:0.977]
Epoch [73/120    avg_loss:0.017, val_acc:0.979]
Epoch [74/120    avg_loss:0.017, val_acc:0.982]
Epoch [75/120    avg_loss:0.018, val_acc:0.973]
Epoch [76/120    avg_loss:0.019, val_acc:0.976]
Epoch [77/120    avg_loss:0.016, val_acc:0.978]
Epoch [78/120    avg_loss:0.011, val_acc:0.986]
Epoch [79/120    avg_loss:0.011, val_acc:0.984]
Epoch [80/120    avg_loss:0.010, val_acc:0.983]
Epoch [81/120    avg_loss:0.010, val_acc:0.985]
Epoch [82/120    avg_loss:0.011, val_acc:0.979]
Epoch [83/120    avg_loss:0.010, val_acc:0.983]
Epoch [84/120    avg_loss:0.011, val_acc:0.984]
Epoch [85/120    avg_loss:0.008, val_acc:0.984]
Epoch [86/120    avg_loss:0.008, val_acc:0.986]
Epoch [87/120    avg_loss:0.007, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.006, val_acc:0.987]
Epoch [91/120    avg_loss:0.006, val_acc:0.986]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.006, val_acc:0.986]
Epoch [94/120    avg_loss:0.007, val_acc:0.986]
Epoch [95/120    avg_loss:0.006, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.986]
Epoch [97/120    avg_loss:0.008, val_acc:0.987]
Epoch [98/120    avg_loss:0.006, val_acc:0.986]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.006, val_acc:0.987]
Epoch [101/120    avg_loss:0.007, val_acc:0.987]
Epoch [102/120    avg_loss:0.006, val_acc:0.987]
Epoch [103/120    avg_loss:0.007, val_acc:0.987]
Epoch [104/120    avg_loss:0.007, val_acc:0.987]
Epoch [105/120    avg_loss:0.006, val_acc:0.987]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.007, val_acc:0.986]
Epoch [108/120    avg_loss:0.006, val_acc:0.987]
Epoch [109/120    avg_loss:0.006, val_acc:0.987]
Epoch [110/120    avg_loss:0.006, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.987]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.007, val_acc:0.987]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.007, val_acc:0.987]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.006, val_acc:0.986]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.007, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1269    0    1    0    0    0    0    0    4   10    1    0
     0    0    0]
 [   0    0    0  721    0    3    1    0    0    8    1    0   11    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    7    5    0    0    0  851   12    0    0
     0    0    0]
 [   0    0    3    0    0    0    3    0    0    0    8 2192    4    0
     0    0    0]
 [   0    0    5    0    0    9    0    0    0    0    2    0  514    0
     0    3    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    1    0    0    0
  1129    2    0]
 [   0    0    0    0    0    0   26    0    0    0    0    0    0    0
     4  317    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.35230352303523

F1 scores:
[       nan 0.98765432 0.99024581 0.98228883 0.99765808 0.96860987
 0.97329377 1.         0.99767442 0.77272727 0.97703789 0.99073446
 0.96344892 0.99462366 0.99296394 0.94768311 0.98809524]

Kappa:
0.9812161577373355
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd78b36e780>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.557, val_acc:0.438]
Epoch [2/120    avg_loss:2.025, val_acc:0.540]
Epoch [3/120    avg_loss:1.772, val_acc:0.555]
Epoch [4/120    avg_loss:1.621, val_acc:0.580]
Epoch [5/120    avg_loss:1.395, val_acc:0.592]
Epoch [6/120    avg_loss:1.242, val_acc:0.686]
Epoch [7/120    avg_loss:1.158, val_acc:0.692]
Epoch [8/120    avg_loss:0.913, val_acc:0.730]
Epoch [9/120    avg_loss:0.755, val_acc:0.751]
Epoch [10/120    avg_loss:0.645, val_acc:0.783]
Epoch [11/120    avg_loss:0.588, val_acc:0.808]
Epoch [12/120    avg_loss:0.553, val_acc:0.799]
Epoch [13/120    avg_loss:0.634, val_acc:0.817]
Epoch [14/120    avg_loss:0.443, val_acc:0.829]
Epoch [15/120    avg_loss:0.384, val_acc:0.860]
Epoch [16/120    avg_loss:0.320, val_acc:0.892]
Epoch [17/120    avg_loss:0.286, val_acc:0.883]
Epoch [18/120    avg_loss:0.374, val_acc:0.865]
Epoch [19/120    avg_loss:0.260, val_acc:0.904]
Epoch [20/120    avg_loss:0.210, val_acc:0.911]
Epoch [21/120    avg_loss:0.185, val_acc:0.923]
Epoch [22/120    avg_loss:0.173, val_acc:0.925]
Epoch [23/120    avg_loss:0.152, val_acc:0.921]
Epoch [24/120    avg_loss:0.126, val_acc:0.928]
Epoch [25/120    avg_loss:0.153, val_acc:0.922]
Epoch [26/120    avg_loss:0.177, val_acc:0.945]
Epoch [27/120    avg_loss:0.146, val_acc:0.941]
Epoch [28/120    avg_loss:0.129, val_acc:0.940]
Epoch [29/120    avg_loss:0.113, val_acc:0.948]
Epoch [30/120    avg_loss:0.088, val_acc:0.950]
Epoch [31/120    avg_loss:0.083, val_acc:0.946]
Epoch [32/120    avg_loss:0.081, val_acc:0.950]
Epoch [33/120    avg_loss:0.083, val_acc:0.963]
Epoch [34/120    avg_loss:0.072, val_acc:0.959]
Epoch [35/120    avg_loss:0.096, val_acc:0.928]
Epoch [36/120    avg_loss:0.136, val_acc:0.952]
Epoch [37/120    avg_loss:0.100, val_acc:0.950]
Epoch [38/120    avg_loss:0.099, val_acc:0.967]
Epoch [39/120    avg_loss:0.058, val_acc:0.972]
Epoch [40/120    avg_loss:0.058, val_acc:0.963]
Epoch [41/120    avg_loss:0.049, val_acc:0.963]
Epoch [42/120    avg_loss:0.053, val_acc:0.959]
Epoch [43/120    avg_loss:0.048, val_acc:0.962]
Epoch [44/120    avg_loss:0.047, val_acc:0.962]
Epoch [45/120    avg_loss:0.049, val_acc:0.966]
Epoch [46/120    avg_loss:0.052, val_acc:0.972]
Epoch [47/120    avg_loss:0.042, val_acc:0.959]
Epoch [48/120    avg_loss:0.037, val_acc:0.970]
Epoch [49/120    avg_loss:0.040, val_acc:0.964]
Epoch [50/120    avg_loss:0.035, val_acc:0.973]
Epoch [51/120    avg_loss:0.029, val_acc:0.967]
Epoch [52/120    avg_loss:0.028, val_acc:0.959]
Epoch [53/120    avg_loss:0.031, val_acc:0.974]
Epoch [54/120    avg_loss:0.030, val_acc:0.967]
Epoch [55/120    avg_loss:0.024, val_acc:0.973]
Epoch [56/120    avg_loss:0.026, val_acc:0.977]
Epoch [57/120    avg_loss:0.024, val_acc:0.967]
Epoch [58/120    avg_loss:0.034, val_acc:0.971]
Epoch [59/120    avg_loss:0.026, val_acc:0.967]
Epoch [60/120    avg_loss:0.038, val_acc:0.965]
Epoch [61/120    avg_loss:0.044, val_acc:0.973]
Epoch [62/120    avg_loss:0.043, val_acc:0.973]
Epoch [63/120    avg_loss:0.033, val_acc:0.978]
Epoch [64/120    avg_loss:0.027, val_acc:0.966]
Epoch [65/120    avg_loss:0.025, val_acc:0.973]
Epoch [66/120    avg_loss:0.017, val_acc:0.977]
Epoch [67/120    avg_loss:0.024, val_acc:0.967]
Epoch [68/120    avg_loss:0.022, val_acc:0.972]
Epoch [69/120    avg_loss:0.015, val_acc:0.976]
Epoch [70/120    avg_loss:0.018, val_acc:0.964]
Epoch [71/120    avg_loss:0.025, val_acc:0.958]
Epoch [72/120    avg_loss:0.035, val_acc:0.964]
Epoch [73/120    avg_loss:0.020, val_acc:0.974]
Epoch [74/120    avg_loss:0.021, val_acc:0.975]
Epoch [75/120    avg_loss:0.024, val_acc:0.977]
Epoch [76/120    avg_loss:0.030, val_acc:0.976]
Epoch [77/120    avg_loss:0.012, val_acc:0.976]
Epoch [78/120    avg_loss:0.017, val_acc:0.977]
Epoch [79/120    avg_loss:0.012, val_acc:0.976]
Epoch [80/120    avg_loss:0.012, val_acc:0.978]
Epoch [81/120    avg_loss:0.011, val_acc:0.979]
Epoch [82/120    avg_loss:0.011, val_acc:0.979]
Epoch [83/120    avg_loss:0.012, val_acc:0.978]
Epoch [84/120    avg_loss:0.010, val_acc:0.978]
Epoch [85/120    avg_loss:0.013, val_acc:0.978]
Epoch [86/120    avg_loss:0.011, val_acc:0.978]
Epoch [87/120    avg_loss:0.011, val_acc:0.979]
Epoch [88/120    avg_loss:0.010, val_acc:0.978]
Epoch [89/120    avg_loss:0.011, val_acc:0.980]
Epoch [90/120    avg_loss:0.012, val_acc:0.982]
Epoch [91/120    avg_loss:0.009, val_acc:0.982]
Epoch [92/120    avg_loss:0.009, val_acc:0.982]
Epoch [93/120    avg_loss:0.010, val_acc:0.979]
Epoch [94/120    avg_loss:0.014, val_acc:0.982]
Epoch [95/120    avg_loss:0.009, val_acc:0.982]
Epoch [96/120    avg_loss:0.011, val_acc:0.982]
Epoch [97/120    avg_loss:0.010, val_acc:0.980]
Epoch [98/120    avg_loss:0.009, val_acc:0.979]
Epoch [99/120    avg_loss:0.011, val_acc:0.980]
Epoch [100/120    avg_loss:0.010, val_acc:0.982]
Epoch [101/120    avg_loss:0.009, val_acc:0.982]
Epoch [102/120    avg_loss:0.009, val_acc:0.982]
Epoch [103/120    avg_loss:0.009, val_acc:0.983]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.010, val_acc:0.980]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.009, val_acc:0.983]
Epoch [108/120    avg_loss:0.010, val_acc:0.980]
Epoch [109/120    avg_loss:0.010, val_acc:0.979]
Epoch [110/120    avg_loss:0.007, val_acc:0.979]
Epoch [111/120    avg_loss:0.010, val_acc:0.979]
Epoch [112/120    avg_loss:0.009, val_acc:0.979]
Epoch [113/120    avg_loss:0.011, val_acc:0.977]
Epoch [114/120    avg_loss:0.010, val_acc:0.979]
Epoch [115/120    avg_loss:0.009, val_acc:0.982]
Epoch [116/120    avg_loss:0.010, val_acc:0.984]
Epoch [117/120    avg_loss:0.009, val_acc:0.979]
Epoch [118/120    avg_loss:0.008, val_acc:0.980]
Epoch [119/120    avg_loss:0.008, val_acc:0.982]
Epoch [120/120    avg_loss:0.008, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1260    3    1    0    1    0    0    2    7    7    2    0
     0    2    0]
 [   0    0    0  715    0    2    0    0    0    7    1    0   17    5
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    3    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  426    0    0    0    3    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    5    1    0    0    0  848   10    0    0
     0    2    0]
 [   0    0    6    0    0    0    2    0    0    0    5 2190    5    2
     0    0    0]
 [   0    0    0    0    2    8    0    0    0    0    9    0  509    0
     0    2    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    3    0    0    0
  1132    3    0]
 [   0    0    0    0    0    0   21    0    0    1    0    0    0    0
    11  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.98373983739837

F1 scores:
[       nan 0.96296296 0.984375   0.97544338 0.99300699 0.97732426
 0.97980553 0.94339623 0.9953271  0.75555556 0.96914286 0.99117447
 0.94785847 0.98143236 0.99167762 0.93731343 0.95238095]

Kappa:
0.9770173184822228
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8a5e4c6748>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.582, val_acc:0.447]
Epoch [2/120    avg_loss:2.132, val_acc:0.452]
Epoch [3/120    avg_loss:1.837, val_acc:0.564]
Epoch [4/120    avg_loss:1.648, val_acc:0.630]
Epoch [5/120    avg_loss:1.434, val_acc:0.647]
Epoch [6/120    avg_loss:1.263, val_acc:0.709]
Epoch [7/120    avg_loss:1.027, val_acc:0.730]
Epoch [8/120    avg_loss:0.878, val_acc:0.754]
Epoch [9/120    avg_loss:0.769, val_acc:0.792]
Epoch [10/120    avg_loss:0.653, val_acc:0.791]
Epoch [11/120    avg_loss:0.649, val_acc:0.792]
Epoch [12/120    avg_loss:0.494, val_acc:0.830]
Epoch [13/120    avg_loss:0.429, val_acc:0.865]
Epoch [14/120    avg_loss:0.423, val_acc:0.867]
Epoch [15/120    avg_loss:0.425, val_acc:0.872]
Epoch [16/120    avg_loss:0.375, val_acc:0.848]
Epoch [17/120    avg_loss:0.459, val_acc:0.868]
Epoch [18/120    avg_loss:0.350, val_acc:0.890]
Epoch [19/120    avg_loss:0.226, val_acc:0.897]
Epoch [20/120    avg_loss:0.196, val_acc:0.910]
Epoch [21/120    avg_loss:0.195, val_acc:0.899]
Epoch [22/120    avg_loss:0.256, val_acc:0.902]
Epoch [23/120    avg_loss:0.204, val_acc:0.888]
Epoch [24/120    avg_loss:0.217, val_acc:0.913]
Epoch [25/120    avg_loss:0.149, val_acc:0.930]
Epoch [26/120    avg_loss:0.122, val_acc:0.938]
Epoch [27/120    avg_loss:0.106, val_acc:0.941]
Epoch [28/120    avg_loss:0.115, val_acc:0.935]
Epoch [29/120    avg_loss:0.103, val_acc:0.949]
Epoch [30/120    avg_loss:0.103, val_acc:0.937]
Epoch [31/120    avg_loss:0.088, val_acc:0.947]
Epoch [32/120    avg_loss:0.088, val_acc:0.950]
Epoch [33/120    avg_loss:0.071, val_acc:0.954]
Epoch [34/120    avg_loss:0.078, val_acc:0.926]
Epoch [35/120    avg_loss:0.079, val_acc:0.951]
Epoch [36/120    avg_loss:0.065, val_acc:0.962]
Epoch [37/120    avg_loss:0.054, val_acc:0.955]
Epoch [38/120    avg_loss:0.067, val_acc:0.964]
Epoch [39/120    avg_loss:0.063, val_acc:0.959]
Epoch [40/120    avg_loss:0.070, val_acc:0.951]
Epoch [41/120    avg_loss:0.054, val_acc:0.963]
Epoch [42/120    avg_loss:0.053, val_acc:0.960]
Epoch [43/120    avg_loss:0.043, val_acc:0.960]
Epoch [44/120    avg_loss:0.050, val_acc:0.959]
Epoch [45/120    avg_loss:0.045, val_acc:0.962]
Epoch [46/120    avg_loss:0.037, val_acc:0.973]
Epoch [47/120    avg_loss:0.032, val_acc:0.970]
Epoch [48/120    avg_loss:0.032, val_acc:0.960]
Epoch [49/120    avg_loss:0.024, val_acc:0.971]
Epoch [50/120    avg_loss:0.030, val_acc:0.964]
Epoch [51/120    avg_loss:0.052, val_acc:0.958]
Epoch [52/120    avg_loss:0.042, val_acc:0.954]
Epoch [53/120    avg_loss:0.039, val_acc:0.972]
Epoch [54/120    avg_loss:0.032, val_acc:0.961]
Epoch [55/120    avg_loss:0.028, val_acc:0.971]
Epoch [56/120    avg_loss:0.038, val_acc:0.954]
Epoch [57/120    avg_loss:0.046, val_acc:0.951]
Epoch [58/120    avg_loss:0.038, val_acc:0.965]
Epoch [59/120    avg_loss:0.040, val_acc:0.965]
Epoch [60/120    avg_loss:0.034, val_acc:0.970]
Epoch [61/120    avg_loss:0.025, val_acc:0.966]
Epoch [62/120    avg_loss:0.020, val_acc:0.968]
Epoch [63/120    avg_loss:0.021, val_acc:0.968]
Epoch [64/120    avg_loss:0.018, val_acc:0.970]
Epoch [65/120    avg_loss:0.020, val_acc:0.968]
Epoch [66/120    avg_loss:0.021, val_acc:0.972]
Epoch [67/120    avg_loss:0.016, val_acc:0.974]
Epoch [68/120    avg_loss:0.020, val_acc:0.973]
Epoch [69/120    avg_loss:0.018, val_acc:0.973]
Epoch [70/120    avg_loss:0.017, val_acc:0.972]
Epoch [71/120    avg_loss:0.014, val_acc:0.973]
Epoch [72/120    avg_loss:0.017, val_acc:0.975]
Epoch [73/120    avg_loss:0.019, val_acc:0.976]
Epoch [74/120    avg_loss:0.016, val_acc:0.974]
Epoch [75/120    avg_loss:0.018, val_acc:0.974]
Epoch [76/120    avg_loss:0.016, val_acc:0.977]
Epoch [77/120    avg_loss:0.017, val_acc:0.976]
Epoch [78/120    avg_loss:0.015, val_acc:0.975]
Epoch [79/120    avg_loss:0.016, val_acc:0.976]
Epoch [80/120    avg_loss:0.020, val_acc:0.974]
Epoch [81/120    avg_loss:0.016, val_acc:0.976]
Epoch [82/120    avg_loss:0.013, val_acc:0.975]
Epoch [83/120    avg_loss:0.015, val_acc:0.975]
Epoch [84/120    avg_loss:0.015, val_acc:0.974]
Epoch [85/120    avg_loss:0.016, val_acc:0.975]
Epoch [86/120    avg_loss:0.015, val_acc:0.977]
Epoch [87/120    avg_loss:0.019, val_acc:0.977]
Epoch [88/120    avg_loss:0.016, val_acc:0.977]
Epoch [89/120    avg_loss:0.015, val_acc:0.979]
Epoch [90/120    avg_loss:0.016, val_acc:0.977]
Epoch [91/120    avg_loss:0.015, val_acc:0.976]
Epoch [92/120    avg_loss:0.015, val_acc:0.978]
Epoch [93/120    avg_loss:0.012, val_acc:0.976]
Epoch [94/120    avg_loss:0.017, val_acc:0.978]
Epoch [95/120    avg_loss:0.017, val_acc:0.978]
Epoch [96/120    avg_loss:0.016, val_acc:0.977]
Epoch [97/120    avg_loss:0.015, val_acc:0.976]
Epoch [98/120    avg_loss:0.014, val_acc:0.976]
Epoch [99/120    avg_loss:0.013, val_acc:0.979]
Epoch [100/120    avg_loss:0.013, val_acc:0.977]
Epoch [101/120    avg_loss:0.015, val_acc:0.976]
Epoch [102/120    avg_loss:0.013, val_acc:0.978]
Epoch [103/120    avg_loss:0.016, val_acc:0.976]
Epoch [104/120    avg_loss:0.013, val_acc:0.976]
Epoch [105/120    avg_loss:0.014, val_acc:0.977]
Epoch [106/120    avg_loss:0.015, val_acc:0.978]
Epoch [107/120    avg_loss:0.013, val_acc:0.979]
Epoch [108/120    avg_loss:0.014, val_acc:0.977]
Epoch [109/120    avg_loss:0.014, val_acc:0.976]
Epoch [110/120    avg_loss:0.019, val_acc:0.980]
Epoch [111/120    avg_loss:0.017, val_acc:0.977]
Epoch [112/120    avg_loss:0.014, val_acc:0.979]
Epoch [113/120    avg_loss:0.015, val_acc:0.978]
Epoch [114/120    avg_loss:0.014, val_acc:0.977]
Epoch [115/120    avg_loss:0.011, val_acc:0.979]
Epoch [116/120    avg_loss:0.015, val_acc:0.977]
Epoch [117/120    avg_loss:0.014, val_acc:0.978]
Epoch [118/120    avg_loss:0.012, val_acc:0.979]
Epoch [119/120    avg_loss:0.015, val_acc:0.980]
Epoch [120/120    avg_loss:0.014, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1254    4    3    0    1    0    0    3    6    8    4    0
     0    2    0]
 [   0    0    0  712    2    2    0    0    0   17    4    4    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  422    0    6    0    3    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  428    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   14    0    0    9    0    0    0    1  836   11    0    0
     0    4    0]
 [   0    0   10    0    0    0    9    0    0    0    5 2180    4    2
     0    0    0]
 [   0    0    1    0    2    8    0    0    0    0    1    7  510    0
     0    1    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    1    0    0    1
  1125    8    0]
 [   0    0    0    0    0    0   13    0    0    5    0    0    0    0
    15  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.55013550135502

F1 scores:
[       nan 0.95       0.97815913 0.97334245 0.98383372 0.96018203
 0.98203593 0.89285714 0.99650757 0.55384615 0.96591566 0.98620222
 0.96226415 0.9919571  0.98554534 0.92899408 0.97076023]

Kappa:
0.9720802714726731
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1780bfe748>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.575, val_acc:0.482]
Epoch [2/120    avg_loss:2.070, val_acc:0.496]
Epoch [3/120    avg_loss:1.812, val_acc:0.578]
Epoch [4/120    avg_loss:1.619, val_acc:0.621]
Epoch [5/120    avg_loss:1.425, val_acc:0.642]
Epoch [6/120    avg_loss:1.215, val_acc:0.687]
Epoch [7/120    avg_loss:1.063, val_acc:0.701]
Epoch [8/120    avg_loss:0.915, val_acc:0.746]
Epoch [9/120    avg_loss:0.769, val_acc:0.773]
Epoch [10/120    avg_loss:0.656, val_acc:0.775]
Epoch [11/120    avg_loss:0.631, val_acc:0.799]
Epoch [12/120    avg_loss:0.561, val_acc:0.799]
Epoch [13/120    avg_loss:0.507, val_acc:0.827]
Epoch [14/120    avg_loss:0.403, val_acc:0.855]
Epoch [15/120    avg_loss:0.348, val_acc:0.854]
Epoch [16/120    avg_loss:0.416, val_acc:0.822]
Epoch [17/120    avg_loss:0.343, val_acc:0.868]
Epoch [18/120    avg_loss:0.268, val_acc:0.891]
Epoch [19/120    avg_loss:0.254, val_acc:0.896]
Epoch [20/120    avg_loss:0.207, val_acc:0.913]
Epoch [21/120    avg_loss:0.254, val_acc:0.833]
Epoch [22/120    avg_loss:0.223, val_acc:0.920]
Epoch [23/120    avg_loss:0.199, val_acc:0.864]
Epoch [24/120    avg_loss:0.187, val_acc:0.929]
Epoch [25/120    avg_loss:0.163, val_acc:0.926]
Epoch [26/120    avg_loss:0.152, val_acc:0.923]
Epoch [27/120    avg_loss:0.113, val_acc:0.938]
Epoch [28/120    avg_loss:0.101, val_acc:0.936]
Epoch [29/120    avg_loss:0.089, val_acc:0.943]
Epoch [30/120    avg_loss:0.082, val_acc:0.943]
Epoch [31/120    avg_loss:0.086, val_acc:0.954]
Epoch [32/120    avg_loss:0.082, val_acc:0.957]
Epoch [33/120    avg_loss:0.070, val_acc:0.960]
Epoch [34/120    avg_loss:0.066, val_acc:0.967]
Epoch [35/120    avg_loss:0.058, val_acc:0.957]
Epoch [36/120    avg_loss:0.058, val_acc:0.957]
Epoch [37/120    avg_loss:0.056, val_acc:0.952]
Epoch [38/120    avg_loss:0.056, val_acc:0.948]
Epoch [39/120    avg_loss:0.070, val_acc:0.965]
Epoch [40/120    avg_loss:0.063, val_acc:0.953]
Epoch [41/120    avg_loss:0.044, val_acc:0.962]
Epoch [42/120    avg_loss:0.051, val_acc:0.955]
Epoch [43/120    avg_loss:0.045, val_acc:0.974]
Epoch [44/120    avg_loss:0.037, val_acc:0.965]
Epoch [45/120    avg_loss:0.033, val_acc:0.972]
Epoch [46/120    avg_loss:0.037, val_acc:0.972]
Epoch [47/120    avg_loss:0.038, val_acc:0.970]
Epoch [48/120    avg_loss:0.040, val_acc:0.968]
Epoch [49/120    avg_loss:0.039, val_acc:0.966]
Epoch [50/120    avg_loss:0.038, val_acc:0.975]
Epoch [51/120    avg_loss:0.032, val_acc:0.975]
Epoch [52/120    avg_loss:0.025, val_acc:0.979]
Epoch [53/120    avg_loss:0.027, val_acc:0.964]
Epoch [54/120    avg_loss:0.033, val_acc:0.966]
Epoch [55/120    avg_loss:0.064, val_acc:0.961]
Epoch [56/120    avg_loss:0.223, val_acc:0.889]
Epoch [57/120    avg_loss:0.175, val_acc:0.925]
Epoch [58/120    avg_loss:0.189, val_acc:0.928]
Epoch [59/120    avg_loss:0.111, val_acc:0.959]
Epoch [60/120    avg_loss:0.066, val_acc:0.946]
Epoch [61/120    avg_loss:0.072, val_acc:0.934]
Epoch [62/120    avg_loss:0.065, val_acc:0.964]
Epoch [63/120    avg_loss:0.040, val_acc:0.959]
Epoch [64/120    avg_loss:0.035, val_acc:0.974]
Epoch [65/120    avg_loss:0.030, val_acc:0.977]
Epoch [66/120    avg_loss:0.032, val_acc:0.975]
Epoch [67/120    avg_loss:0.025, val_acc:0.982]
Epoch [68/120    avg_loss:0.020, val_acc:0.980]
Epoch [69/120    avg_loss:0.022, val_acc:0.982]
Epoch [70/120    avg_loss:0.025, val_acc:0.983]
Epoch [71/120    avg_loss:0.020, val_acc:0.984]
Epoch [72/120    avg_loss:0.019, val_acc:0.985]
Epoch [73/120    avg_loss:0.021, val_acc:0.983]
Epoch [74/120    avg_loss:0.019, val_acc:0.985]
Epoch [75/120    avg_loss:0.023, val_acc:0.982]
Epoch [76/120    avg_loss:0.018, val_acc:0.982]
Epoch [77/120    avg_loss:0.019, val_acc:0.983]
Epoch [78/120    avg_loss:0.020, val_acc:0.983]
Epoch [79/120    avg_loss:0.021, val_acc:0.982]
Epoch [80/120    avg_loss:0.019, val_acc:0.983]
Epoch [81/120    avg_loss:0.023, val_acc:0.982]
Epoch [82/120    avg_loss:0.022, val_acc:0.982]
Epoch [83/120    avg_loss:0.017, val_acc:0.984]
Epoch [84/120    avg_loss:0.016, val_acc:0.985]
Epoch [85/120    avg_loss:0.016, val_acc:0.985]
Epoch [86/120    avg_loss:0.019, val_acc:0.985]
Epoch [87/120    avg_loss:0.021, val_acc:0.984]
Epoch [88/120    avg_loss:0.013, val_acc:0.985]
Epoch [89/120    avg_loss:0.016, val_acc:0.985]
Epoch [90/120    avg_loss:0.020, val_acc:0.984]
Epoch [91/120    avg_loss:0.016, val_acc:0.985]
Epoch [92/120    avg_loss:0.020, val_acc:0.984]
Epoch [93/120    avg_loss:0.018, val_acc:0.985]
Epoch [94/120    avg_loss:0.016, val_acc:0.985]
Epoch [95/120    avg_loss:0.016, val_acc:0.985]
Epoch [96/120    avg_loss:0.016, val_acc:0.985]
Epoch [97/120    avg_loss:0.015, val_acc:0.985]
Epoch [98/120    avg_loss:0.017, val_acc:0.986]
Epoch [99/120    avg_loss:0.018, val_acc:0.985]
Epoch [100/120    avg_loss:0.018, val_acc:0.984]
Epoch [101/120    avg_loss:0.016, val_acc:0.986]
Epoch [102/120    avg_loss:0.016, val_acc:0.986]
Epoch [103/120    avg_loss:0.018, val_acc:0.986]
Epoch [104/120    avg_loss:0.015, val_acc:0.985]
Epoch [105/120    avg_loss:0.018, val_acc:0.984]
Epoch [106/120    avg_loss:0.016, val_acc:0.985]
Epoch [107/120    avg_loss:0.016, val_acc:0.985]
Epoch [108/120    avg_loss:0.016, val_acc:0.984]
Epoch [109/120    avg_loss:0.018, val_acc:0.985]
Epoch [110/120    avg_loss:0.015, val_acc:0.986]
Epoch [111/120    avg_loss:0.014, val_acc:0.985]
Epoch [112/120    avg_loss:0.013, val_acc:0.986]
Epoch [113/120    avg_loss:0.014, val_acc:0.987]
Epoch [114/120    avg_loss:0.015, val_acc:0.984]
Epoch [115/120    avg_loss:0.016, val_acc:0.984]
Epoch [116/120    avg_loss:0.013, val_acc:0.986]
Epoch [117/120    avg_loss:0.013, val_acc:0.985]
Epoch [118/120    avg_loss:0.016, val_acc:0.984]
Epoch [119/120    avg_loss:0.015, val_acc:0.984]
Epoch [120/120    avg_loss:0.013, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1255    3    5    0    2    0    0    0    6    6    5    0
     0    3    0]
 [   0    0    0  718    1    4    0    0    0   11    3    0    8    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  427    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0   10    5    0    0    0  839   14    0    0
     0    3    0]
 [   0    0    8    0    0    1    4    0    0    0   15 2173    6    2
     0    1    0]
 [   0    0    0    0    0   15    0    0    0    0    0    4  511    0
     2    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    1    0    0    0
  1127    3    0]
 [   0    0    1    0    0    0   17    0    0    0    0    0    0    0
    17  312    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.71273712737127

F1 scores:
[       nan 0.96296296 0.98315707 0.97820163 0.98611111 0.95690608
 0.97837435 0.98039216 0.995338   0.75       0.9638139  0.98593466
 0.95782568 0.98930481 0.98643326 0.93134328 0.98809524]

Kappa:
0.9739378709156227
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:23
Validation dataloader:23
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8ff4573828>
supervision:full
center_pixel:True
Network :
Number of parameter: 120793==>0.12M
----------Training process----------
Epoch [1/120    avg_loss:2.584, val_acc:0.479]
Epoch [2/120    avg_loss:2.071, val_acc:0.486]
Epoch [3/120    avg_loss:1.799, val_acc:0.558]
Epoch [4/120    avg_loss:1.587, val_acc:0.664]
Epoch [5/120    avg_loss:1.408, val_acc:0.684]
Epoch [6/120    avg_loss:1.305, val_acc:0.655]
Epoch [7/120    avg_loss:1.134, val_acc:0.743]
Epoch [8/120    avg_loss:0.986, val_acc:0.737]
Epoch [9/120    avg_loss:0.790, val_acc:0.757]
Epoch [10/120    avg_loss:0.650, val_acc:0.805]
Epoch [11/120    avg_loss:0.603, val_acc:0.797]
Epoch [12/120    avg_loss:0.509, val_acc:0.859]
Epoch [13/120    avg_loss:0.488, val_acc:0.804]
Epoch [14/120    avg_loss:0.433, val_acc:0.846]
Epoch [15/120    avg_loss:0.391, val_acc:0.867]
Epoch [16/120    avg_loss:0.324, val_acc:0.877]
Epoch [17/120    avg_loss:0.297, val_acc:0.878]
Epoch [18/120    avg_loss:0.261, val_acc:0.901]
Epoch [19/120    avg_loss:0.229, val_acc:0.889]
Epoch [20/120    avg_loss:0.207, val_acc:0.897]
Epoch [21/120    avg_loss:0.233, val_acc:0.875]
Epoch [22/120    avg_loss:0.270, val_acc:0.879]
Epoch [23/120    avg_loss:0.242, val_acc:0.914]
Epoch [24/120    avg_loss:0.198, val_acc:0.917]
Epoch [25/120    avg_loss:0.184, val_acc:0.926]
Epoch [26/120    avg_loss:0.144, val_acc:0.911]
Epoch [27/120    avg_loss:0.146, val_acc:0.924]
Epoch [28/120    avg_loss:0.138, val_acc:0.923]
Epoch [29/120    avg_loss:0.124, val_acc:0.942]
Epoch [30/120    avg_loss:0.115, val_acc:0.927]
Epoch [31/120    avg_loss:0.151, val_acc:0.932]
Epoch [32/120    avg_loss:0.149, val_acc:0.932]
Epoch [33/120    avg_loss:0.094, val_acc:0.945]
Epoch [34/120    avg_loss:0.081, val_acc:0.924]
Epoch [35/120    avg_loss:0.099, val_acc:0.947]
Epoch [36/120    avg_loss:0.082, val_acc:0.940]
Epoch [37/120    avg_loss:0.112, val_acc:0.935]
Epoch [38/120    avg_loss:0.089, val_acc:0.943]
Epoch [39/120    avg_loss:0.090, val_acc:0.949]
Epoch [40/120    avg_loss:0.074, val_acc:0.946]
Epoch [41/120    avg_loss:0.069, val_acc:0.954]
Epoch [42/120    avg_loss:0.056, val_acc:0.958]
Epoch [43/120    avg_loss:0.055, val_acc:0.949]
Epoch [44/120    avg_loss:0.056, val_acc:0.955]
Epoch [45/120    avg_loss:0.043, val_acc:0.967]
Epoch [46/120    avg_loss:0.056, val_acc:0.967]
Epoch [47/120    avg_loss:0.066, val_acc:0.957]
Epoch [48/120    avg_loss:0.063, val_acc:0.954]
Epoch [49/120    avg_loss:0.044, val_acc:0.961]
Epoch [50/120    avg_loss:0.052, val_acc:0.953]
Epoch [51/120    avg_loss:0.048, val_acc:0.955]
Epoch [52/120    avg_loss:0.058, val_acc:0.960]
Epoch [53/120    avg_loss:0.082, val_acc:0.946]
Epoch [54/120    avg_loss:0.069, val_acc:0.957]
Epoch [55/120    avg_loss:0.059, val_acc:0.964]
Epoch [56/120    avg_loss:0.039, val_acc:0.967]
Epoch [57/120    avg_loss:0.051, val_acc:0.962]
Epoch [58/120    avg_loss:0.036, val_acc:0.966]
Epoch [59/120    avg_loss:0.033, val_acc:0.970]
Epoch [60/120    avg_loss:0.030, val_acc:0.967]
Epoch [61/120    avg_loss:0.031, val_acc:0.972]
Epoch [62/120    avg_loss:0.034, val_acc:0.957]
Epoch [63/120    avg_loss:0.036, val_acc:0.977]
Epoch [64/120    avg_loss:0.028, val_acc:0.973]
Epoch [65/120    avg_loss:0.026, val_acc:0.974]
Epoch [66/120    avg_loss:0.027, val_acc:0.960]
Epoch [67/120    avg_loss:0.044, val_acc:0.966]
Epoch [68/120    avg_loss:0.028, val_acc:0.979]
Epoch [69/120    avg_loss:0.023, val_acc:0.974]
Epoch [70/120    avg_loss:0.021, val_acc:0.979]
Epoch [71/120    avg_loss:0.018, val_acc:0.975]
Epoch [72/120    avg_loss:0.017, val_acc:0.977]
Epoch [73/120    avg_loss:0.013, val_acc:0.974]
Epoch [74/120    avg_loss:0.017, val_acc:0.975]
Epoch [75/120    avg_loss:0.015, val_acc:0.974]
Epoch [76/120    avg_loss:0.019, val_acc:0.973]
Epoch [77/120    avg_loss:0.019, val_acc:0.974]
Epoch [78/120    avg_loss:0.013, val_acc:0.975]
Epoch [79/120    avg_loss:0.012, val_acc:0.982]
Epoch [80/120    avg_loss:0.017, val_acc:0.976]
Epoch [81/120    avg_loss:0.013, val_acc:0.978]
Epoch [82/120    avg_loss:0.011, val_acc:0.980]
Epoch [83/120    avg_loss:0.015, val_acc:0.977]
Epoch [84/120    avg_loss:0.014, val_acc:0.978]
Epoch [85/120    avg_loss:0.019, val_acc:0.976]
Epoch [86/120    avg_loss:0.013, val_acc:0.978]
Epoch [87/120    avg_loss:0.013, val_acc:0.966]
Epoch [88/120    avg_loss:0.017, val_acc:0.979]
Epoch [89/120    avg_loss:0.011, val_acc:0.974]
Epoch [90/120    avg_loss:0.011, val_acc:0.983]
Epoch [91/120    avg_loss:0.009, val_acc:0.983]
Epoch [92/120    avg_loss:0.008, val_acc:0.978]
Epoch [93/120    avg_loss:0.011, val_acc:0.976]
Epoch [94/120    avg_loss:0.017, val_acc:0.980]
Epoch [95/120    avg_loss:0.014, val_acc:0.978]
Epoch [96/120    avg_loss:0.014, val_acc:0.976]
Epoch [97/120    avg_loss:0.015, val_acc:0.977]
Epoch [98/120    avg_loss:0.011, val_acc:0.984]
Epoch [99/120    avg_loss:0.010, val_acc:0.980]
Epoch [100/120    avg_loss:0.006, val_acc:0.979]
Epoch [101/120    avg_loss:0.007, val_acc:0.980]
Epoch [102/120    avg_loss:0.006, val_acc:0.984]
Epoch [103/120    avg_loss:0.006, val_acc:0.983]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.006, val_acc:0.979]
Epoch [106/120    avg_loss:0.006, val_acc:0.980]
Epoch [107/120    avg_loss:0.005, val_acc:0.982]
Epoch [108/120    avg_loss:0.008, val_acc:0.983]
Epoch [109/120    avg_loss:0.009, val_acc:0.979]
Epoch [110/120    avg_loss:0.042, val_acc:0.965]
Epoch [111/120    avg_loss:0.026, val_acc:0.966]
Epoch [112/120    avg_loss:0.019, val_acc:0.975]
Epoch [113/120    avg_loss:0.016, val_acc:0.973]
Epoch [114/120    avg_loss:0.023, val_acc:0.978]
Epoch [115/120    avg_loss:0.018, val_acc:0.973]
Epoch [116/120    avg_loss:0.019, val_acc:0.977]
Epoch [117/120    avg_loss:0.028, val_acc:0.973]
Epoch [118/120    avg_loss:0.014, val_acc:0.974]
Epoch [119/120    avg_loss:0.013, val_acc:0.978]
Epoch [120/120    avg_loss:0.009, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1269    0    0    0    0    0    0    1    7    8    0    0
     0    0    0]
 [   0    0    0  702    2   11    0    0    0   14    1    0   17    0
     0    0    0]
 [   0    0    0    0  210    0    0    0    0    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   17    0    0   12    0    0    0    0  840    5    1    0
     0    0    0]
 [   0    0   11    0    0    0    2    0    0    0   10 2183    0    4
     0    0    0]
 [   0    0    0    2    0   11    0    0    0    0    0    4  515    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    1    0    0
  1116   20    0]
 [   0    0    0    0    0    0   18    0    0    0    0    0    0    0
     8  321    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.77777777777777

F1 scores:
[       nan 0.975      0.98295895 0.96627667 0.98823529 0.95777778
 0.98424606 0.98039216 0.99767442 0.61538462 0.96774194 0.98957389
 0.9608209  0.98930481 0.98630137 0.93313953 0.98224852]

Kappa:
0.9746767919059575
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:02:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff3a3e19780>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.590, val_acc:0.472]
Epoch [2/120    avg_loss:2.074, val_acc:0.567]
Epoch [3/120    avg_loss:1.838, val_acc:0.571]
Epoch [4/120    avg_loss:1.641, val_acc:0.607]
Epoch [5/120    avg_loss:1.477, val_acc:0.650]
Epoch [6/120    avg_loss:1.346, val_acc:0.670]
Epoch [7/120    avg_loss:1.181, val_acc:0.678]
Epoch [8/120    avg_loss:0.965, val_acc:0.735]
Epoch [9/120    avg_loss:0.826, val_acc:0.786]
Epoch [10/120    avg_loss:0.746, val_acc:0.777]
Epoch [11/120    avg_loss:0.771, val_acc:0.786]
Epoch [12/120    avg_loss:0.572, val_acc:0.828]
Epoch [13/120    avg_loss:0.519, val_acc:0.854]
Epoch [14/120    avg_loss:0.500, val_acc:0.859]
Epoch [15/120    avg_loss:0.505, val_acc:0.810]
Epoch [16/120    avg_loss:0.438, val_acc:0.909]
Epoch [17/120    avg_loss:0.367, val_acc:0.883]
Epoch [18/120    avg_loss:0.339, val_acc:0.902]
Epoch [19/120    avg_loss:0.330, val_acc:0.861]
Epoch [20/120    avg_loss:0.259, val_acc:0.913]
Epoch [21/120    avg_loss:0.249, val_acc:0.916]
Epoch [22/120    avg_loss:0.229, val_acc:0.909]
Epoch [23/120    avg_loss:0.229, val_acc:0.921]
Epoch [24/120    avg_loss:0.181, val_acc:0.932]
Epoch [25/120    avg_loss:0.165, val_acc:0.916]
Epoch [26/120    avg_loss:0.178, val_acc:0.940]
Epoch [27/120    avg_loss:0.152, val_acc:0.936]
Epoch [28/120    avg_loss:0.123, val_acc:0.943]
Epoch [29/120    avg_loss:0.168, val_acc:0.924]
Epoch [30/120    avg_loss:0.235, val_acc:0.921]
Epoch [31/120    avg_loss:0.182, val_acc:0.935]
Epoch [32/120    avg_loss:0.147, val_acc:0.929]
Epoch [33/120    avg_loss:0.115, val_acc:0.938]
Epoch [34/120    avg_loss:0.107, val_acc:0.957]
Epoch [35/120    avg_loss:0.085, val_acc:0.959]
Epoch [36/120    avg_loss:0.108, val_acc:0.945]
Epoch [37/120    avg_loss:0.135, val_acc:0.925]
Epoch [38/120    avg_loss:0.129, val_acc:0.945]
Epoch [39/120    avg_loss:0.090, val_acc:0.932]
Epoch [40/120    avg_loss:0.137, val_acc:0.941]
Epoch [41/120    avg_loss:0.095, val_acc:0.937]
Epoch [42/120    avg_loss:0.110, val_acc:0.945]
Epoch [43/120    avg_loss:0.083, val_acc:0.941]
Epoch [44/120    avg_loss:0.076, val_acc:0.945]
Epoch [45/120    avg_loss:0.071, val_acc:0.953]
Epoch [46/120    avg_loss:0.062, val_acc:0.958]
Epoch [47/120    avg_loss:0.067, val_acc:0.957]
Epoch [48/120    avg_loss:0.074, val_acc:0.952]
Epoch [49/120    avg_loss:0.060, val_acc:0.962]
Epoch [50/120    avg_loss:0.044, val_acc:0.968]
Epoch [51/120    avg_loss:0.041, val_acc:0.967]
Epoch [52/120    avg_loss:0.042, val_acc:0.967]
Epoch [53/120    avg_loss:0.038, val_acc:0.970]
Epoch [54/120    avg_loss:0.039, val_acc:0.973]
Epoch [55/120    avg_loss:0.037, val_acc:0.974]
Epoch [56/120    avg_loss:0.038, val_acc:0.968]
Epoch [57/120    avg_loss:0.032, val_acc:0.968]
Epoch [58/120    avg_loss:0.039, val_acc:0.968]
Epoch [59/120    avg_loss:0.037, val_acc:0.971]
Epoch [60/120    avg_loss:0.028, val_acc:0.968]
Epoch [61/120    avg_loss:0.041, val_acc:0.967]
Epoch [62/120    avg_loss:0.032, val_acc:0.967]
Epoch [63/120    avg_loss:0.036, val_acc:0.968]
Epoch [64/120    avg_loss:0.037, val_acc:0.965]
Epoch [65/120    avg_loss:0.034, val_acc:0.965]
Epoch [66/120    avg_loss:0.030, val_acc:0.968]
Epoch [67/120    avg_loss:0.029, val_acc:0.972]
Epoch [68/120    avg_loss:0.034, val_acc:0.970]
Epoch [69/120    avg_loss:0.030, val_acc:0.970]
Epoch [70/120    avg_loss:0.031, val_acc:0.970]
Epoch [71/120    avg_loss:0.036, val_acc:0.970]
Epoch [72/120    avg_loss:0.030, val_acc:0.970]
Epoch [73/120    avg_loss:0.032, val_acc:0.970]
Epoch [74/120    avg_loss:0.029, val_acc:0.968]
Epoch [75/120    avg_loss:0.024, val_acc:0.968]
Epoch [76/120    avg_loss:0.031, val_acc:0.968]
Epoch [77/120    avg_loss:0.033, val_acc:0.968]
Epoch [78/120    avg_loss:0.034, val_acc:0.968]
Epoch [79/120    avg_loss:0.030, val_acc:0.968]
Epoch [80/120    avg_loss:0.029, val_acc:0.968]
Epoch [81/120    avg_loss:0.033, val_acc:0.968]
Epoch [82/120    avg_loss:0.031, val_acc:0.968]
Epoch [83/120    avg_loss:0.035, val_acc:0.968]
Epoch [84/120    avg_loss:0.030, val_acc:0.968]
Epoch [85/120    avg_loss:0.035, val_acc:0.968]
Epoch [86/120    avg_loss:0.037, val_acc:0.970]
Epoch [87/120    avg_loss:0.029, val_acc:0.970]
Epoch [88/120    avg_loss:0.031, val_acc:0.970]
Epoch [89/120    avg_loss:0.032, val_acc:0.970]
Epoch [90/120    avg_loss:0.032, val_acc:0.968]
Epoch [91/120    avg_loss:0.028, val_acc:0.970]
Epoch [92/120    avg_loss:0.027, val_acc:0.970]
Epoch [93/120    avg_loss:0.032, val_acc:0.970]
Epoch [94/120    avg_loss:0.027, val_acc:0.970]
Epoch [95/120    avg_loss:0.028, val_acc:0.970]
Epoch [96/120    avg_loss:0.028, val_acc:0.970]
Epoch [97/120    avg_loss:0.032, val_acc:0.970]
Epoch [98/120    avg_loss:0.030, val_acc:0.970]
Epoch [99/120    avg_loss:0.028, val_acc:0.970]
Epoch [100/120    avg_loss:0.031, val_acc:0.970]
Epoch [101/120    avg_loss:0.030, val_acc:0.970]
Epoch [102/120    avg_loss:0.030, val_acc:0.970]
Epoch [103/120    avg_loss:0.034, val_acc:0.970]
Epoch [104/120    avg_loss:0.034, val_acc:0.970]
Epoch [105/120    avg_loss:0.031, val_acc:0.970]
Epoch [106/120    avg_loss:0.032, val_acc:0.970]
Epoch [107/120    avg_loss:0.028, val_acc:0.970]
Epoch [108/120    avg_loss:0.036, val_acc:0.970]
Epoch [109/120    avg_loss:0.030, val_acc:0.970]
Epoch [110/120    avg_loss:0.035, val_acc:0.970]
Epoch [111/120    avg_loss:0.027, val_acc:0.970]
Epoch [112/120    avg_loss:0.029, val_acc:0.970]
Epoch [113/120    avg_loss:0.031, val_acc:0.970]
Epoch [114/120    avg_loss:0.035, val_acc:0.970]
Epoch [115/120    avg_loss:0.029, val_acc:0.970]
Epoch [116/120    avg_loss:0.032, val_acc:0.970]
Epoch [117/120    avg_loss:0.028, val_acc:0.970]
Epoch [118/120    avg_loss:0.033, val_acc:0.970]
Epoch [119/120    avg_loss:0.031, val_acc:0.970]
Epoch [120/120    avg_loss:0.029, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    6 1247    0    1    0    2    0    0    1    5   21    2    0
     0    0    0]
 [   0    0    1  715    3    8    0    0    0    6    4    1    8    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    1    0    4    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   36    8    0    8    2    0    0    0  806   11    0    0
     1    3    0]
 [   0    0   13    0    0    0    7    0    0    0   13 2174    0    2
     1    0    0]
 [   0    0   12   23    7    3    0    0    0    0    0    9  477    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    3    0    0    0
  1123    6    0]
 [   0    0    0    0    0    0   32    0    0    3    0    0    0    0
    49  263    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.22764227642277

F1 scores:
[       nan 0.89411765 0.9614495  0.95780308 0.97482838 0.96162528
 0.96678967 0.98039216 0.99883856 0.69387755 0.94324166 0.98193315
 0.93255132 0.9919571  0.96935693 0.84975767 0.97647059]

Kappa:
0.9569663073787428
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f50fc0746a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.581, val_acc:0.452]
Epoch [2/120    avg_loss:2.105, val_acc:0.467]
Epoch [3/120    avg_loss:1.846, val_acc:0.579]
Epoch [4/120    avg_loss:1.681, val_acc:0.596]
Epoch [5/120    avg_loss:1.540, val_acc:0.642]
Epoch [6/120    avg_loss:1.295, val_acc:0.657]
Epoch [7/120    avg_loss:1.223, val_acc:0.671]
Epoch [8/120    avg_loss:1.058, val_acc:0.702]
Epoch [9/120    avg_loss:0.939, val_acc:0.727]
Epoch [10/120    avg_loss:0.836, val_acc:0.738]
Epoch [11/120    avg_loss:0.711, val_acc:0.765]
Epoch [12/120    avg_loss:0.668, val_acc:0.772]
Epoch [13/120    avg_loss:0.611, val_acc:0.787]
Epoch [14/120    avg_loss:0.600, val_acc:0.823]
Epoch [15/120    avg_loss:0.491, val_acc:0.845]
Epoch [16/120    avg_loss:0.471, val_acc:0.821]
Epoch [17/120    avg_loss:0.374, val_acc:0.827]
Epoch [18/120    avg_loss:0.374, val_acc:0.884]
Epoch [19/120    avg_loss:0.338, val_acc:0.891]
Epoch [20/120    avg_loss:0.280, val_acc:0.900]
Epoch [21/120    avg_loss:0.346, val_acc:0.851]
Epoch [22/120    avg_loss:0.388, val_acc:0.873]
Epoch [23/120    avg_loss:0.281, val_acc:0.876]
Epoch [24/120    avg_loss:0.242, val_acc:0.899]
Epoch [25/120    avg_loss:0.210, val_acc:0.916]
Epoch [26/120    avg_loss:0.147, val_acc:0.921]
Epoch [27/120    avg_loss:0.163, val_acc:0.910]
Epoch [28/120    avg_loss:0.171, val_acc:0.897]
Epoch [29/120    avg_loss:0.215, val_acc:0.911]
Epoch [30/120    avg_loss:0.150, val_acc:0.927]
Epoch [31/120    avg_loss:0.122, val_acc:0.929]
Epoch [32/120    avg_loss:0.124, val_acc:0.925]
Epoch [33/120    avg_loss:0.114, val_acc:0.929]
Epoch [34/120    avg_loss:0.112, val_acc:0.939]
Epoch [35/120    avg_loss:0.118, val_acc:0.930]
Epoch [36/120    avg_loss:0.098, val_acc:0.933]
Epoch [37/120    avg_loss:0.111, val_acc:0.923]
Epoch [38/120    avg_loss:0.107, val_acc:0.943]
Epoch [39/120    avg_loss:0.090, val_acc:0.938]
Epoch [40/120    avg_loss:0.073, val_acc:0.941]
Epoch [41/120    avg_loss:0.067, val_acc:0.937]
Epoch [42/120    avg_loss:0.071, val_acc:0.946]
Epoch [43/120    avg_loss:0.060, val_acc:0.948]
Epoch [44/120    avg_loss:0.069, val_acc:0.960]
Epoch [45/120    avg_loss:0.062, val_acc:0.946]
Epoch [46/120    avg_loss:0.055, val_acc:0.966]
Epoch [47/120    avg_loss:0.049, val_acc:0.952]
Epoch [48/120    avg_loss:0.044, val_acc:0.961]
Epoch [49/120    avg_loss:0.050, val_acc:0.954]
Epoch [50/120    avg_loss:0.049, val_acc:0.961]
Epoch [51/120    avg_loss:0.041, val_acc:0.955]
Epoch [52/120    avg_loss:0.041, val_acc:0.966]
Epoch [53/120    avg_loss:0.049, val_acc:0.968]
Epoch [54/120    avg_loss:0.047, val_acc:0.966]
Epoch [55/120    avg_loss:0.033, val_acc:0.954]
Epoch [56/120    avg_loss:0.041, val_acc:0.964]
Epoch [57/120    avg_loss:0.033, val_acc:0.961]
Epoch [58/120    avg_loss:0.039, val_acc:0.964]
Epoch [59/120    avg_loss:0.032, val_acc:0.971]
Epoch [60/120    avg_loss:0.033, val_acc:0.968]
Epoch [61/120    avg_loss:0.035, val_acc:0.961]
Epoch [62/120    avg_loss:0.040, val_acc:0.970]
Epoch [63/120    avg_loss:0.044, val_acc:0.939]
Epoch [64/120    avg_loss:0.042, val_acc:0.963]
Epoch [65/120    avg_loss:0.036, val_acc:0.965]
Epoch [66/120    avg_loss:0.039, val_acc:0.951]
Epoch [67/120    avg_loss:0.033, val_acc:0.960]
Epoch [68/120    avg_loss:0.028, val_acc:0.964]
Epoch [69/120    avg_loss:0.052, val_acc:0.952]
Epoch [70/120    avg_loss:0.047, val_acc:0.962]
Epoch [71/120    avg_loss:0.073, val_acc:0.947]
Epoch [72/120    avg_loss:0.089, val_acc:0.932]
Epoch [73/120    avg_loss:0.067, val_acc:0.952]
Epoch [74/120    avg_loss:0.045, val_acc:0.962]
Epoch [75/120    avg_loss:0.031, val_acc:0.967]
Epoch [76/120    avg_loss:0.028, val_acc:0.966]
Epoch [77/120    avg_loss:0.021, val_acc:0.968]
Epoch [78/120    avg_loss:0.021, val_acc:0.968]
Epoch [79/120    avg_loss:0.025, val_acc:0.968]
Epoch [80/120    avg_loss:0.022, val_acc:0.967]
Epoch [81/120    avg_loss:0.019, val_acc:0.967]
Epoch [82/120    avg_loss:0.019, val_acc:0.967]
Epoch [83/120    avg_loss:0.023, val_acc:0.971]
Epoch [84/120    avg_loss:0.019, val_acc:0.971]
Epoch [85/120    avg_loss:0.018, val_acc:0.973]
Epoch [86/120    avg_loss:0.019, val_acc:0.970]
Epoch [87/120    avg_loss:0.017, val_acc:0.970]
Epoch [88/120    avg_loss:0.022, val_acc:0.971]
Epoch [89/120    avg_loss:0.022, val_acc:0.972]
Epoch [90/120    avg_loss:0.020, val_acc:0.972]
Epoch [91/120    avg_loss:0.023, val_acc:0.971]
Epoch [92/120    avg_loss:0.019, val_acc:0.972]
Epoch [93/120    avg_loss:0.017, val_acc:0.972]
Epoch [94/120    avg_loss:0.019, val_acc:0.973]
Epoch [95/120    avg_loss:0.018, val_acc:0.974]
Epoch [96/120    avg_loss:0.018, val_acc:0.974]
Epoch [97/120    avg_loss:0.016, val_acc:0.975]
Epoch [98/120    avg_loss:0.020, val_acc:0.973]
Epoch [99/120    avg_loss:0.017, val_acc:0.974]
Epoch [100/120    avg_loss:0.017, val_acc:0.975]
Epoch [101/120    avg_loss:0.020, val_acc:0.975]
Epoch [102/120    avg_loss:0.021, val_acc:0.975]
Epoch [103/120    avg_loss:0.014, val_acc:0.975]
Epoch [104/120    avg_loss:0.015, val_acc:0.973]
Epoch [105/120    avg_loss:0.019, val_acc:0.974]
Epoch [106/120    avg_loss:0.017, val_acc:0.974]
Epoch [107/120    avg_loss:0.020, val_acc:0.972]
Epoch [108/120    avg_loss:0.016, val_acc:0.973]
Epoch [109/120    avg_loss:0.015, val_acc:0.975]
Epoch [110/120    avg_loss:0.014, val_acc:0.974]
Epoch [111/120    avg_loss:0.015, val_acc:0.974]
Epoch [112/120    avg_loss:0.018, val_acc:0.974]
Epoch [113/120    avg_loss:0.017, val_acc:0.973]
Epoch [114/120    avg_loss:0.015, val_acc:0.974]
Epoch [115/120    avg_loss:0.019, val_acc:0.974]
Epoch [116/120    avg_loss:0.018, val_acc:0.973]
Epoch [117/120    avg_loss:0.017, val_acc:0.974]
Epoch [118/120    avg_loss:0.014, val_acc:0.975]
Epoch [119/120    avg_loss:0.015, val_acc:0.974]
Epoch [120/120    avg_loss:0.015, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1255    0    0    0    2    0    0    3    7   13    5    0
     0    0    0]
 [   0    0    1  722    1    9    0    0    0    8    2    0    3    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    5    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   15    0    0    2    0
     0    0    0]
 [   0    0   10   43    0    4    0    0    0    0  807    2    0    0
     2    7    0]
 [   0    0    9    1    0    2    6    0    0    0   12 2178    0    2
     0    0    0]
 [   0    0    0   25   13    6    0    0    0    1    9    0  476    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    1    0    3    0    0    0    0    3    0    0    0
  1132    0    0]
 [   0    0    0    0    0    0   26    0    0    1    0    0    0    0
    37  283    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.78048780487805

F1 scores:
[       nan 0.94871795 0.98046875 0.93766234 0.96818182 0.96737908
 0.97401633 1.         1.         0.58823529 0.93891798 0.98910082
 0.93333333 0.9919571  0.98008658 0.88854003 0.97674419]

Kappa:
0.9633025093565492
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f51ad4f5748>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.669, val_acc:0.473]
Epoch [2/120    avg_loss:2.088, val_acc:0.552]
Epoch [3/120    avg_loss:1.833, val_acc:0.540]
Epoch [4/120    avg_loss:1.653, val_acc:0.625]
Epoch [5/120    avg_loss:1.473, val_acc:0.620]
Epoch [6/120    avg_loss:1.325, val_acc:0.683]
Epoch [7/120    avg_loss:1.099, val_acc:0.690]
Epoch [8/120    avg_loss:0.984, val_acc:0.685]
Epoch [9/120    avg_loss:0.824, val_acc:0.762]
Epoch [10/120    avg_loss:0.741, val_acc:0.697]
Epoch [11/120    avg_loss:0.710, val_acc:0.726]
Epoch [12/120    avg_loss:0.605, val_acc:0.774]
Epoch [13/120    avg_loss:0.554, val_acc:0.782]
Epoch [14/120    avg_loss:0.473, val_acc:0.814]
Epoch [15/120    avg_loss:0.396, val_acc:0.835]
Epoch [16/120    avg_loss:0.387, val_acc:0.825]
Epoch [17/120    avg_loss:0.350, val_acc:0.823]
Epoch [18/120    avg_loss:0.377, val_acc:0.828]
Epoch [19/120    avg_loss:0.341, val_acc:0.861]
Epoch [20/120    avg_loss:0.251, val_acc:0.868]
Epoch [21/120    avg_loss:0.261, val_acc:0.863]
Epoch [22/120    avg_loss:0.261, val_acc:0.890]
Epoch [23/120    avg_loss:0.197, val_acc:0.917]
Epoch [24/120    avg_loss:0.245, val_acc:0.890]
Epoch [25/120    avg_loss:0.300, val_acc:0.885]
Epoch [26/120    avg_loss:0.201, val_acc:0.919]
Epoch [27/120    avg_loss:0.176, val_acc:0.899]
Epoch [28/120    avg_loss:0.177, val_acc:0.909]
Epoch [29/120    avg_loss:0.148, val_acc:0.924]
Epoch [30/120    avg_loss:0.158, val_acc:0.911]
Epoch [31/120    avg_loss:0.138, val_acc:0.928]
Epoch [32/120    avg_loss:0.116, val_acc:0.931]
Epoch [33/120    avg_loss:0.098, val_acc:0.920]
Epoch [34/120    avg_loss:0.105, val_acc:0.930]
Epoch [35/120    avg_loss:0.106, val_acc:0.948]
Epoch [36/120    avg_loss:0.092, val_acc:0.947]
Epoch [37/120    avg_loss:0.107, val_acc:0.933]
Epoch [38/120    avg_loss:0.082, val_acc:0.953]
Epoch [39/120    avg_loss:0.072, val_acc:0.940]
Epoch [40/120    avg_loss:0.071, val_acc:0.945]
Epoch [41/120    avg_loss:0.066, val_acc:0.956]
Epoch [42/120    avg_loss:0.074, val_acc:0.948]
Epoch [43/120    avg_loss:0.069, val_acc:0.956]
Epoch [44/120    avg_loss:0.070, val_acc:0.948]
Epoch [45/120    avg_loss:0.056, val_acc:0.950]
Epoch [46/120    avg_loss:0.056, val_acc:0.952]
Epoch [47/120    avg_loss:0.042, val_acc:0.950]
Epoch [48/120    avg_loss:0.045, val_acc:0.952]
Epoch [49/120    avg_loss:0.046, val_acc:0.955]
Epoch [50/120    avg_loss:0.047, val_acc:0.958]
Epoch [51/120    avg_loss:0.046, val_acc:0.948]
Epoch [52/120    avg_loss:0.060, val_acc:0.963]
Epoch [53/120    avg_loss:0.046, val_acc:0.959]
Epoch [54/120    avg_loss:0.040, val_acc:0.958]
Epoch [55/120    avg_loss:0.039, val_acc:0.959]
Epoch [56/120    avg_loss:0.040, val_acc:0.961]
Epoch [57/120    avg_loss:0.043, val_acc:0.960]
Epoch [58/120    avg_loss:0.055, val_acc:0.956]
Epoch [59/120    avg_loss:0.051, val_acc:0.960]
Epoch [60/120    avg_loss:0.043, val_acc:0.964]
Epoch [61/120    avg_loss:0.041, val_acc:0.967]
Epoch [62/120    avg_loss:0.038, val_acc:0.966]
Epoch [63/120    avg_loss:0.029, val_acc:0.975]
Epoch [64/120    avg_loss:0.043, val_acc:0.957]
Epoch [65/120    avg_loss:0.051, val_acc:0.967]
Epoch [66/120    avg_loss:0.045, val_acc:0.963]
Epoch [67/120    avg_loss:0.032, val_acc:0.967]
Epoch [68/120    avg_loss:0.029, val_acc:0.972]
Epoch [69/120    avg_loss:0.024, val_acc:0.968]
Epoch [70/120    avg_loss:0.019, val_acc:0.973]
Epoch [71/120    avg_loss:0.022, val_acc:0.968]
Epoch [72/120    avg_loss:0.019, val_acc:0.970]
Epoch [73/120    avg_loss:0.017, val_acc:0.973]
Epoch [74/120    avg_loss:0.019, val_acc:0.967]
Epoch [75/120    avg_loss:0.018, val_acc:0.965]
Epoch [76/120    avg_loss:0.038, val_acc:0.964]
Epoch [77/120    avg_loss:0.021, val_acc:0.970]
Epoch [78/120    avg_loss:0.018, val_acc:0.968]
Epoch [79/120    avg_loss:0.015, val_acc:0.969]
Epoch [80/120    avg_loss:0.018, val_acc:0.970]
Epoch [81/120    avg_loss:0.014, val_acc:0.974]
Epoch [82/120    avg_loss:0.015, val_acc:0.972]
Epoch [83/120    avg_loss:0.012, val_acc:0.972]
Epoch [84/120    avg_loss:0.012, val_acc:0.975]
Epoch [85/120    avg_loss:0.018, val_acc:0.974]
Epoch [86/120    avg_loss:0.015, val_acc:0.975]
Epoch [87/120    avg_loss:0.013, val_acc:0.977]
Epoch [88/120    avg_loss:0.012, val_acc:0.977]
Epoch [89/120    avg_loss:0.015, val_acc:0.977]
Epoch [90/120    avg_loss:0.016, val_acc:0.977]
Epoch [91/120    avg_loss:0.012, val_acc:0.977]
Epoch [92/120    avg_loss:0.012, val_acc:0.977]
Epoch [93/120    avg_loss:0.014, val_acc:0.977]
Epoch [94/120    avg_loss:0.013, val_acc:0.973]
Epoch [95/120    avg_loss:0.013, val_acc:0.976]
Epoch [96/120    avg_loss:0.011, val_acc:0.977]
Epoch [97/120    avg_loss:0.014, val_acc:0.977]
Epoch [98/120    avg_loss:0.013, val_acc:0.976]
Epoch [99/120    avg_loss:0.010, val_acc:0.976]
Epoch [100/120    avg_loss:0.013, val_acc:0.977]
Epoch [101/120    avg_loss:0.011, val_acc:0.976]
Epoch [102/120    avg_loss:0.013, val_acc:0.976]
Epoch [103/120    avg_loss:0.012, val_acc:0.975]
Epoch [104/120    avg_loss:0.011, val_acc:0.975]
Epoch [105/120    avg_loss:0.009, val_acc:0.976]
Epoch [106/120    avg_loss:0.013, val_acc:0.975]
Epoch [107/120    avg_loss:0.015, val_acc:0.978]
Epoch [108/120    avg_loss:0.019, val_acc:0.976]
Epoch [109/120    avg_loss:0.012, val_acc:0.973]
Epoch [110/120    avg_loss:0.012, val_acc:0.974]
Epoch [111/120    avg_loss:0.009, val_acc:0.975]
Epoch [112/120    avg_loss:0.011, val_acc:0.974]
Epoch [113/120    avg_loss:0.013, val_acc:0.975]
Epoch [114/120    avg_loss:0.010, val_acc:0.975]
Epoch [115/120    avg_loss:0.012, val_acc:0.975]
Epoch [116/120    avg_loss:0.014, val_acc:0.975]
Epoch [117/120    avg_loss:0.011, val_acc:0.976]
Epoch [118/120    avg_loss:0.011, val_acc:0.975]
Epoch [119/120    avg_loss:0.012, val_acc:0.976]
Epoch [120/120    avg_loss:0.010, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    3    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1248    5    0    0    0    0    0    0    3   22    2    0
     0    5    0]
 [   0    0    2  704    8    8    0    0    0   14    1    0    8    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    3    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   10   31    0    8    0    0    0    0  813    2    0    0
     0   11    0]
 [   0    0    9    0    0    0    9    0    0    0   14 2162   12    4
     0    0    0]
 [   0    0    0    5    9   14    0    0    0    0    3    8  491    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   15    0    0    1    0    3    2    0    0
  1118    0    0]
 [   0    0    0    0    0    0   23    0    0    1    0    0    0    0
    35  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.44444444444444

F1 scores:
[       nan 0.96202532 0.97614392 0.94369973 0.96162528 0.94273128
 0.97470238 0.94339623 0.99883856 0.60377358 0.94976636 0.98094374
 0.9352381  0.98404255 0.97556719 0.88479263 0.97076023]

Kappa:
0.9594876691044715
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa52436c710>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.516, val_acc:0.436]
Epoch [2/120    avg_loss:2.052, val_acc:0.571]
Epoch [3/120    avg_loss:1.798, val_acc:0.604]
Epoch [4/120    avg_loss:1.587, val_acc:0.627]
Epoch [5/120    avg_loss:1.371, val_acc:0.674]
Epoch [6/120    avg_loss:1.431, val_acc:0.659]
Epoch [7/120    avg_loss:1.186, val_acc:0.701]
Epoch [8/120    avg_loss:1.009, val_acc:0.717]
Epoch [9/120    avg_loss:0.886, val_acc:0.761]
Epoch [10/120    avg_loss:0.696, val_acc:0.775]
Epoch [11/120    avg_loss:0.707, val_acc:0.737]
Epoch [12/120    avg_loss:0.651, val_acc:0.802]
Epoch [13/120    avg_loss:0.548, val_acc:0.825]
Epoch [14/120    avg_loss:0.485, val_acc:0.834]
Epoch [15/120    avg_loss:0.410, val_acc:0.860]
Epoch [16/120    avg_loss:0.402, val_acc:0.880]
Epoch [17/120    avg_loss:0.342, val_acc:0.882]
Epoch [18/120    avg_loss:0.320, val_acc:0.847]
Epoch [19/120    avg_loss:0.342, val_acc:0.886]
Epoch [20/120    avg_loss:0.252, val_acc:0.900]
Epoch [21/120    avg_loss:0.438, val_acc:0.851]
Epoch [22/120    avg_loss:0.337, val_acc:0.899]
Epoch [23/120    avg_loss:0.272, val_acc:0.912]
Epoch [24/120    avg_loss:0.233, val_acc:0.913]
Epoch [25/120    avg_loss:0.188, val_acc:0.912]
Epoch [26/120    avg_loss:0.169, val_acc:0.921]
Epoch [27/120    avg_loss:0.161, val_acc:0.936]
Epoch [28/120    avg_loss:0.144, val_acc:0.934]
Epoch [29/120    avg_loss:0.157, val_acc:0.924]
Epoch [30/120    avg_loss:0.139, val_acc:0.938]
Epoch [31/120    avg_loss:0.137, val_acc:0.927]
Epoch [32/120    avg_loss:0.147, val_acc:0.909]
Epoch [33/120    avg_loss:0.175, val_acc:0.934]
Epoch [34/120    avg_loss:0.135, val_acc:0.920]
Epoch [35/120    avg_loss:0.114, val_acc:0.939]
Epoch [36/120    avg_loss:0.099, val_acc:0.940]
Epoch [37/120    avg_loss:0.086, val_acc:0.936]
Epoch [38/120    avg_loss:0.089, val_acc:0.950]
Epoch [39/120    avg_loss:0.102, val_acc:0.938]
Epoch [40/120    avg_loss:0.098, val_acc:0.950]
Epoch [41/120    avg_loss:0.074, val_acc:0.951]
Epoch [42/120    avg_loss:0.067, val_acc:0.955]
Epoch [43/120    avg_loss:0.062, val_acc:0.958]
Epoch [44/120    avg_loss:0.054, val_acc:0.960]
Epoch [45/120    avg_loss:0.061, val_acc:0.955]
Epoch [46/120    avg_loss:0.072, val_acc:0.928]
Epoch [47/120    avg_loss:0.078, val_acc:0.952]
Epoch [48/120    avg_loss:0.063, val_acc:0.952]
Epoch [49/120    avg_loss:0.053, val_acc:0.953]
Epoch [50/120    avg_loss:0.060, val_acc:0.946]
Epoch [51/120    avg_loss:0.056, val_acc:0.953]
Epoch [52/120    avg_loss:0.050, val_acc:0.971]
Epoch [53/120    avg_loss:0.055, val_acc:0.962]
Epoch [54/120    avg_loss:0.050, val_acc:0.951]
Epoch [55/120    avg_loss:0.048, val_acc:0.963]
Epoch [56/120    avg_loss:0.051, val_acc:0.958]
Epoch [57/120    avg_loss:0.042, val_acc:0.965]
Epoch [58/120    avg_loss:0.039, val_acc:0.971]
Epoch [59/120    avg_loss:0.042, val_acc:0.951]
Epoch [60/120    avg_loss:0.050, val_acc:0.958]
Epoch [61/120    avg_loss:0.049, val_acc:0.957]
Epoch [62/120    avg_loss:0.052, val_acc:0.967]
Epoch [63/120    avg_loss:0.042, val_acc:0.955]
Epoch [64/120    avg_loss:0.087, val_acc:0.962]
Epoch [65/120    avg_loss:0.045, val_acc:0.971]
Epoch [66/120    avg_loss:0.085, val_acc:0.947]
Epoch [67/120    avg_loss:0.069, val_acc:0.951]
Epoch [68/120    avg_loss:0.045, val_acc:0.953]
Epoch [69/120    avg_loss:0.031, val_acc:0.964]
Epoch [70/120    avg_loss:0.041, val_acc:0.953]
Epoch [71/120    avg_loss:0.028, val_acc:0.971]
Epoch [72/120    avg_loss:0.030, val_acc:0.967]
Epoch [73/120    avg_loss:0.024, val_acc:0.975]
Epoch [74/120    avg_loss:0.031, val_acc:0.953]
Epoch [75/120    avg_loss:0.027, val_acc:0.974]
Epoch [76/120    avg_loss:0.026, val_acc:0.963]
Epoch [77/120    avg_loss:0.020, val_acc:0.967]
Epoch [78/120    avg_loss:0.017, val_acc:0.977]
Epoch [79/120    avg_loss:0.021, val_acc:0.975]
Epoch [80/120    avg_loss:0.015, val_acc:0.971]
Epoch [81/120    avg_loss:0.017, val_acc:0.975]
Epoch [82/120    avg_loss:0.019, val_acc:0.966]
Epoch [83/120    avg_loss:0.022, val_acc:0.965]
Epoch [84/120    avg_loss:0.030, val_acc:0.965]
Epoch [85/120    avg_loss:0.034, val_acc:0.962]
Epoch [86/120    avg_loss:0.026, val_acc:0.974]
Epoch [87/120    avg_loss:0.021, val_acc:0.971]
Epoch [88/120    avg_loss:0.018, val_acc:0.970]
Epoch [89/120    avg_loss:0.036, val_acc:0.965]
Epoch [90/120    avg_loss:0.049, val_acc:0.963]
Epoch [91/120    avg_loss:0.040, val_acc:0.961]
Epoch [92/120    avg_loss:0.037, val_acc:0.965]
Epoch [93/120    avg_loss:0.022, val_acc:0.967]
Epoch [94/120    avg_loss:0.020, val_acc:0.972]
Epoch [95/120    avg_loss:0.016, val_acc:0.971]
Epoch [96/120    avg_loss:0.015, val_acc:0.967]
Epoch [97/120    avg_loss:0.016, val_acc:0.970]
Epoch [98/120    avg_loss:0.019, val_acc:0.974]
Epoch [99/120    avg_loss:0.012, val_acc:0.972]
Epoch [100/120    avg_loss:0.014, val_acc:0.974]
Epoch [101/120    avg_loss:0.013, val_acc:0.972]
Epoch [102/120    avg_loss:0.016, val_acc:0.975]
Epoch [103/120    avg_loss:0.015, val_acc:0.975]
Epoch [104/120    avg_loss:0.012, val_acc:0.976]
Epoch [105/120    avg_loss:0.012, val_acc:0.976]
Epoch [106/120    avg_loss:0.014, val_acc:0.976]
Epoch [107/120    avg_loss:0.012, val_acc:0.976]
Epoch [108/120    avg_loss:0.011, val_acc:0.976]
Epoch [109/120    avg_loss:0.012, val_acc:0.976]
Epoch [110/120    avg_loss:0.012, val_acc:0.976]
Epoch [111/120    avg_loss:0.016, val_acc:0.976]
Epoch [112/120    avg_loss:0.013, val_acc:0.976]
Epoch [113/120    avg_loss:0.014, val_acc:0.976]
Epoch [114/120    avg_loss:0.013, val_acc:0.976]
Epoch [115/120    avg_loss:0.012, val_acc:0.976]
Epoch [116/120    avg_loss:0.013, val_acc:0.976]
Epoch [117/120    avg_loss:0.015, val_acc:0.976]
Epoch [118/120    avg_loss:0.012, val_acc:0.976]
Epoch [119/120    avg_loss:0.013, val_acc:0.976]
Epoch [120/120    avg_loss:0.009, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    3    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    4 1253    1    0    0    3    0    0    0    8   13    1    0
     0    2    0]
 [   0    0    0  695    4   31    0    0    0   11    0    0    2    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    1    0    3    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   18    9    0   10    5    0    0    0  825    6    0    0
     1    1    0]
 [   0    0    4    0    0    0    7    0    0    0    6 2193    0    0
     0    0    0]
 [   0    0    0   15   20   15    0    0    0    0    3    0  474    0
     3    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    3    0    0    0
  1129    0    0]
 [   0    0    0    0    0    0   38    0    0    0    0    0    0    0
    53  256    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
96.45528455284553

F1 scores:
[       nan 0.91566265 0.97776044 0.94751193 0.94666667 0.92324324
 0.96046852 0.98039216 1.         0.72       0.95930233 0.99163464
 0.93491124 0.98930481 0.96951481 0.84488449 0.95857988]

Kappa:
0.9595792947690222
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6eca9736d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.571, val_acc:0.397]
Epoch [2/120    avg_loss:2.066, val_acc:0.498]
Epoch [3/120    avg_loss:1.836, val_acc:0.521]
Epoch [4/120    avg_loss:1.621, val_acc:0.609]
Epoch [5/120    avg_loss:1.469, val_acc:0.612]
Epoch [6/120    avg_loss:1.346, val_acc:0.616]
Epoch [7/120    avg_loss:1.149, val_acc:0.686]
Epoch [8/120    avg_loss:0.991, val_acc:0.696]
Epoch [9/120    avg_loss:0.838, val_acc:0.741]
Epoch [10/120    avg_loss:0.707, val_acc:0.767]
Epoch [11/120    avg_loss:0.662, val_acc:0.767]
Epoch [12/120    avg_loss:0.557, val_acc:0.801]
Epoch [13/120    avg_loss:0.514, val_acc:0.821]
Epoch [14/120    avg_loss:0.488, val_acc:0.815]
Epoch [15/120    avg_loss:0.442, val_acc:0.835]
Epoch [16/120    avg_loss:0.425, val_acc:0.828]
Epoch [17/120    avg_loss:0.376, val_acc:0.861]
Epoch [18/120    avg_loss:0.411, val_acc:0.809]
Epoch [19/120    avg_loss:0.380, val_acc:0.885]
Epoch [20/120    avg_loss:0.295, val_acc:0.883]
Epoch [21/120    avg_loss:0.268, val_acc:0.896]
Epoch [22/120    avg_loss:0.239, val_acc:0.916]
Epoch [23/120    avg_loss:0.207, val_acc:0.911]
Epoch [24/120    avg_loss:0.216, val_acc:0.911]
Epoch [25/120    avg_loss:0.206, val_acc:0.891]
Epoch [26/120    avg_loss:0.194, val_acc:0.927]
Epoch [27/120    avg_loss:0.171, val_acc:0.916]
Epoch [28/120    avg_loss:0.181, val_acc:0.915]
Epoch [29/120    avg_loss:0.165, val_acc:0.918]
Epoch [30/120    avg_loss:0.124, val_acc:0.926]
Epoch [31/120    avg_loss:0.114, val_acc:0.934]
Epoch [32/120    avg_loss:0.134, val_acc:0.943]
Epoch [33/120    avg_loss:0.139, val_acc:0.924]
Epoch [34/120    avg_loss:0.101, val_acc:0.933]
Epoch [35/120    avg_loss:0.108, val_acc:0.940]
Epoch [36/120    avg_loss:0.102, val_acc:0.909]
Epoch [37/120    avg_loss:0.080, val_acc:0.937]
Epoch [38/120    avg_loss:0.092, val_acc:0.947]
Epoch [39/120    avg_loss:0.122, val_acc:0.939]
Epoch [40/120    avg_loss:0.100, val_acc:0.932]
Epoch [41/120    avg_loss:0.093, val_acc:0.940]
Epoch [42/120    avg_loss:0.081, val_acc:0.943]
Epoch [43/120    avg_loss:0.067, val_acc:0.948]
Epoch [44/120    avg_loss:0.085, val_acc:0.945]
Epoch [45/120    avg_loss:0.087, val_acc:0.938]
Epoch [46/120    avg_loss:0.072, val_acc:0.947]
Epoch [47/120    avg_loss:0.060, val_acc:0.949]
Epoch [48/120    avg_loss:0.052, val_acc:0.932]
Epoch [49/120    avg_loss:0.069, val_acc:0.952]
Epoch [50/120    avg_loss:0.059, val_acc:0.950]
Epoch [51/120    avg_loss:0.053, val_acc:0.946]
Epoch [52/120    avg_loss:0.050, val_acc:0.948]
Epoch [53/120    avg_loss:0.047, val_acc:0.960]
Epoch [54/120    avg_loss:0.040, val_acc:0.957]
Epoch [55/120    avg_loss:0.042, val_acc:0.958]
Epoch [56/120    avg_loss:0.049, val_acc:0.964]
Epoch [57/120    avg_loss:0.037, val_acc:0.959]
Epoch [58/120    avg_loss:0.038, val_acc:0.959]
Epoch [59/120    avg_loss:0.036, val_acc:0.955]
Epoch [60/120    avg_loss:0.033, val_acc:0.955]
Epoch [61/120    avg_loss:0.043, val_acc:0.949]
Epoch [62/120    avg_loss:0.038, val_acc:0.965]
Epoch [63/120    avg_loss:0.035, val_acc:0.962]
Epoch [64/120    avg_loss:0.031, val_acc:0.967]
Epoch [65/120    avg_loss:0.043, val_acc:0.951]
Epoch [66/120    avg_loss:0.060, val_acc:0.964]
Epoch [67/120    avg_loss:0.036, val_acc:0.964]
Epoch [68/120    avg_loss:0.036, val_acc:0.955]
Epoch [69/120    avg_loss:0.032, val_acc:0.962]
Epoch [70/120    avg_loss:0.041, val_acc:0.952]
Epoch [71/120    avg_loss:0.024, val_acc:0.961]
Epoch [72/120    avg_loss:0.030, val_acc:0.961]
Epoch [73/120    avg_loss:0.053, val_acc:0.950]
Epoch [74/120    avg_loss:0.039, val_acc:0.960]
Epoch [75/120    avg_loss:0.039, val_acc:0.957]
Epoch [76/120    avg_loss:0.031, val_acc:0.954]
Epoch [77/120    avg_loss:0.031, val_acc:0.948]
Epoch [78/120    avg_loss:0.024, val_acc:0.965]
Epoch [79/120    avg_loss:0.021, val_acc:0.972]
Epoch [80/120    avg_loss:0.021, val_acc:0.973]
Epoch [81/120    avg_loss:0.017, val_acc:0.973]
Epoch [82/120    avg_loss:0.018, val_acc:0.975]
Epoch [83/120    avg_loss:0.017, val_acc:0.973]
Epoch [84/120    avg_loss:0.015, val_acc:0.975]
Epoch [85/120    avg_loss:0.016, val_acc:0.974]
Epoch [86/120    avg_loss:0.020, val_acc:0.975]
Epoch [87/120    avg_loss:0.016, val_acc:0.972]
Epoch [88/120    avg_loss:0.016, val_acc:0.972]
Epoch [89/120    avg_loss:0.016, val_acc:0.972]
Epoch [90/120    avg_loss:0.018, val_acc:0.972]
Epoch [91/120    avg_loss:0.014, val_acc:0.972]
Epoch [92/120    avg_loss:0.014, val_acc:0.973]
Epoch [93/120    avg_loss:0.014, val_acc:0.974]
Epoch [94/120    avg_loss:0.016, val_acc:0.974]
Epoch [95/120    avg_loss:0.018, val_acc:0.975]
Epoch [96/120    avg_loss:0.016, val_acc:0.974]
Epoch [97/120    avg_loss:0.016, val_acc:0.972]
Epoch [98/120    avg_loss:0.017, val_acc:0.972]
Epoch [99/120    avg_loss:0.015, val_acc:0.973]
Epoch [100/120    avg_loss:0.012, val_acc:0.972]
Epoch [101/120    avg_loss:0.014, val_acc:0.972]
Epoch [102/120    avg_loss:0.015, val_acc:0.972]
Epoch [103/120    avg_loss:0.014, val_acc:0.971]
Epoch [104/120    avg_loss:0.013, val_acc:0.973]
Epoch [105/120    avg_loss:0.014, val_acc:0.972]
Epoch [106/120    avg_loss:0.013, val_acc:0.973]
Epoch [107/120    avg_loss:0.018, val_acc:0.974]
Epoch [108/120    avg_loss:0.012, val_acc:0.975]
Epoch [109/120    avg_loss:0.012, val_acc:0.975]
Epoch [110/120    avg_loss:0.015, val_acc:0.972]
Epoch [111/120    avg_loss:0.015, val_acc:0.971]
Epoch [112/120    avg_loss:0.013, val_acc:0.971]
Epoch [113/120    avg_loss:0.017, val_acc:0.975]
Epoch [114/120    avg_loss:0.014, val_acc:0.972]
Epoch [115/120    avg_loss:0.014, val_acc:0.973]
Epoch [116/120    avg_loss:0.013, val_acc:0.974]
Epoch [117/120    avg_loss:0.016, val_acc:0.972]
Epoch [118/120    avg_loss:0.012, val_acc:0.972]
Epoch [119/120    avg_loss:0.012, val_acc:0.976]
Epoch [120/120    avg_loss:0.015, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    1    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1253    2    0    0    0    0    0    2    9   12    4    0
     0    3    0]
 [   0    0    1  724    1   12    0    0    0    4    0    0    3    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    1    0    5    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   14   32    0    6    0    0    0    0  807   11    0    0
     1    4    0]
 [   0    0    8    0    0    1    7    0    0    0   12 2170    9    2
     1    0    0]
 [   0    0    1   19   21    8    0    0    0    0    0    0  482    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    3    0    0    0
  1129    0    0]
 [   0    0    0    0    0    0   28    0    0    0    0    0    0    0
    48  271    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.53116531165311

F1 scores:
[       nan 0.94871795 0.97776044 0.94826457 0.95089286 0.95302013
 0.97405486 0.98039216 0.99767442 0.68181818 0.94441194 0.98569157
 0.93140097 0.98930481 0.97285653 0.8672     0.9704142 ]

Kappa:
0.9604581883316212
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0f3352a7b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.674, val_acc:0.449]
Epoch [2/120    avg_loss:2.111, val_acc:0.509]
Epoch [3/120    avg_loss:1.855, val_acc:0.613]
Epoch [4/120    avg_loss:1.641, val_acc:0.624]
Epoch [5/120    avg_loss:1.460, val_acc:0.672]
Epoch [6/120    avg_loss:1.298, val_acc:0.648]
Epoch [7/120    avg_loss:1.155, val_acc:0.674]
Epoch [8/120    avg_loss:1.021, val_acc:0.751]
Epoch [9/120    avg_loss:0.861, val_acc:0.780]
Epoch [10/120    avg_loss:0.849, val_acc:0.799]
Epoch [11/120    avg_loss:0.692, val_acc:0.823]
Epoch [12/120    avg_loss:0.588, val_acc:0.847]
Epoch [13/120    avg_loss:0.549, val_acc:0.825]
Epoch [14/120    avg_loss:0.451, val_acc:0.863]
Epoch [15/120    avg_loss:0.397, val_acc:0.874]
Epoch [16/120    avg_loss:0.433, val_acc:0.843]
Epoch [17/120    avg_loss:0.416, val_acc:0.857]
Epoch [18/120    avg_loss:0.371, val_acc:0.859]
Epoch [19/120    avg_loss:0.350, val_acc:0.869]
Epoch [20/120    avg_loss:0.350, val_acc:0.895]
Epoch [21/120    avg_loss:0.309, val_acc:0.892]
Epoch [22/120    avg_loss:0.208, val_acc:0.919]
Epoch [23/120    avg_loss:0.207, val_acc:0.906]
Epoch [24/120    avg_loss:0.197, val_acc:0.910]
Epoch [25/120    avg_loss:0.185, val_acc:0.931]
Epoch [26/120    avg_loss:0.173, val_acc:0.938]
Epoch [27/120    avg_loss:0.196, val_acc:0.914]
Epoch [28/120    avg_loss:0.150, val_acc:0.924]
Epoch [29/120    avg_loss:0.149, val_acc:0.932]
Epoch [30/120    avg_loss:0.117, val_acc:0.940]
Epoch [31/120    avg_loss:0.119, val_acc:0.939]
Epoch [32/120    avg_loss:0.124, val_acc:0.933]
Epoch [33/120    avg_loss:0.116, val_acc:0.926]
Epoch [34/120    avg_loss:0.122, val_acc:0.936]
Epoch [35/120    avg_loss:0.125, val_acc:0.948]
Epoch [36/120    avg_loss:0.089, val_acc:0.958]
Epoch [37/120    avg_loss:0.088, val_acc:0.951]
Epoch [38/120    avg_loss:0.081, val_acc:0.963]
Epoch [39/120    avg_loss:0.065, val_acc:0.952]
Epoch [40/120    avg_loss:0.067, val_acc:0.959]
Epoch [41/120    avg_loss:0.063, val_acc:0.959]
Epoch [42/120    avg_loss:0.060, val_acc:0.959]
Epoch [43/120    avg_loss:0.062, val_acc:0.941]
Epoch [44/120    avg_loss:0.068, val_acc:0.950]
Epoch [45/120    avg_loss:0.075, val_acc:0.953]
Epoch [46/120    avg_loss:0.055, val_acc:0.958]
Epoch [47/120    avg_loss:0.048, val_acc:0.958]
Epoch [48/120    avg_loss:0.065, val_acc:0.968]
Epoch [49/120    avg_loss:0.050, val_acc:0.967]
Epoch [50/120    avg_loss:0.061, val_acc:0.955]
Epoch [51/120    avg_loss:0.051, val_acc:0.967]
Epoch [52/120    avg_loss:0.052, val_acc:0.968]
Epoch [53/120    avg_loss:0.047, val_acc:0.970]
Epoch [54/120    avg_loss:0.041, val_acc:0.968]
Epoch [55/120    avg_loss:0.038, val_acc:0.966]
Epoch [56/120    avg_loss:0.048, val_acc:0.956]
Epoch [57/120    avg_loss:0.058, val_acc:0.934]
Epoch [58/120    avg_loss:0.061, val_acc:0.938]
Epoch [59/120    avg_loss:0.046, val_acc:0.968]
Epoch [60/120    avg_loss:0.035, val_acc:0.970]
Epoch [61/120    avg_loss:0.035, val_acc:0.957]
Epoch [62/120    avg_loss:0.030, val_acc:0.968]
Epoch [63/120    avg_loss:0.040, val_acc:0.957]
Epoch [64/120    avg_loss:0.048, val_acc:0.963]
Epoch [65/120    avg_loss:0.058, val_acc:0.951]
Epoch [66/120    avg_loss:0.052, val_acc:0.950]
Epoch [67/120    avg_loss:0.063, val_acc:0.961]
Epoch [68/120    avg_loss:0.041, val_acc:0.968]
Epoch [69/120    avg_loss:0.038, val_acc:0.969]
Epoch [70/120    avg_loss:0.043, val_acc:0.972]
Epoch [71/120    avg_loss:0.039, val_acc:0.972]
Epoch [72/120    avg_loss:0.027, val_acc:0.978]
Epoch [73/120    avg_loss:0.030, val_acc:0.973]
Epoch [74/120    avg_loss:0.024, val_acc:0.977]
Epoch [75/120    avg_loss:0.023, val_acc:0.975]
Epoch [76/120    avg_loss:0.029, val_acc:0.967]
Epoch [77/120    avg_loss:0.038, val_acc:0.970]
Epoch [78/120    avg_loss:0.037, val_acc:0.960]
Epoch [79/120    avg_loss:0.029, val_acc:0.966]
Epoch [80/120    avg_loss:0.019, val_acc:0.975]
Epoch [81/120    avg_loss:0.019, val_acc:0.972]
Epoch [82/120    avg_loss:0.018, val_acc:0.972]
Epoch [83/120    avg_loss:0.020, val_acc:0.977]
Epoch [84/120    avg_loss:0.021, val_acc:0.967]
Epoch [85/120    avg_loss:0.024, val_acc:0.965]
Epoch [86/120    avg_loss:0.021, val_acc:0.975]
Epoch [87/120    avg_loss:0.014, val_acc:0.973]
Epoch [88/120    avg_loss:0.012, val_acc:0.973]
Epoch [89/120    avg_loss:0.012, val_acc:0.976]
Epoch [90/120    avg_loss:0.012, val_acc:0.976]
Epoch [91/120    avg_loss:0.014, val_acc:0.976]
Epoch [92/120    avg_loss:0.014, val_acc:0.977]
Epoch [93/120    avg_loss:0.012, val_acc:0.976]
Epoch [94/120    avg_loss:0.012, val_acc:0.977]
Epoch [95/120    avg_loss:0.012, val_acc:0.977]
Epoch [96/120    avg_loss:0.013, val_acc:0.977]
Epoch [97/120    avg_loss:0.012, val_acc:0.981]
Epoch [98/120    avg_loss:0.010, val_acc:0.981]
Epoch [99/120    avg_loss:0.010, val_acc:0.981]
Epoch [100/120    avg_loss:0.010, val_acc:0.981]
Epoch [101/120    avg_loss:0.011, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.982]
Epoch [103/120    avg_loss:0.010, val_acc:0.982]
Epoch [104/120    avg_loss:0.013, val_acc:0.981]
Epoch [105/120    avg_loss:0.014, val_acc:0.981]
Epoch [106/120    avg_loss:0.010, val_acc:0.980]
Epoch [107/120    avg_loss:0.011, val_acc:0.980]
Epoch [108/120    avg_loss:0.011, val_acc:0.980]
Epoch [109/120    avg_loss:0.011, val_acc:0.980]
Epoch [110/120    avg_loss:0.009, val_acc:0.980]
Epoch [111/120    avg_loss:0.010, val_acc:0.981]
Epoch [112/120    avg_loss:0.012, val_acc:0.982]
Epoch [113/120    avg_loss:0.010, val_acc:0.982]
Epoch [114/120    avg_loss:0.012, val_acc:0.981]
Epoch [115/120    avg_loss:0.009, val_acc:0.981]
Epoch [116/120    avg_loss:0.010, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.982]
Epoch [118/120    avg_loss:0.009, val_acc:0.981]
Epoch [119/120    avg_loss:0.009, val_acc:0.981]
Epoch [120/120    avg_loss:0.011, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1256    2    0    2    2    0    0    0    9    7    7    0
     0    0    0]
 [   0    0    0  725    0   15    0    0    0    7    0    0    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    3    0    5    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    2   18    0    6    0    0    0    0  823   20    0    0
     0    6    0]
 [   0    0    7    0    0    0    6    0    0    0   11 2185    0    1
     0    0    0]
 [   0    0    0   22    9    4    0    0    0    0    4    3  483    0
     0    7    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    1    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   35    0    0    0    0    0    0    0
    39  273    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.09485094850949

F1 scores:
[       nan 0.98765432 0.98509804 0.95772787 0.97931034 0.95730337
 0.96755162 0.94339623 1.         0.72340426 0.95475638 0.98734749
 0.94243902 0.99730458 0.98142549 0.86255924 0.98823529]

Kappa:
0.9668713133230866
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe4201fd710>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.605, val_acc:0.375]
Epoch [2/120    avg_loss:2.103, val_acc:0.510]
Epoch [3/120    avg_loss:1.865, val_acc:0.553]
Epoch [4/120    avg_loss:1.699, val_acc:0.562]
Epoch [5/120    avg_loss:1.498, val_acc:0.602]
Epoch [6/120    avg_loss:1.322, val_acc:0.646]
Epoch [7/120    avg_loss:1.135, val_acc:0.659]
Epoch [8/120    avg_loss:0.948, val_acc:0.697]
Epoch [9/120    avg_loss:0.809, val_acc:0.710]
Epoch [10/120    avg_loss:0.700, val_acc:0.765]
Epoch [11/120    avg_loss:0.724, val_acc:0.737]
Epoch [12/120    avg_loss:0.595, val_acc:0.740]
Epoch [13/120    avg_loss:0.518, val_acc:0.804]
Epoch [14/120    avg_loss:0.472, val_acc:0.808]
Epoch [15/120    avg_loss:0.449, val_acc:0.821]
Epoch [16/120    avg_loss:0.387, val_acc:0.836]
Epoch [17/120    avg_loss:0.399, val_acc:0.813]
Epoch [18/120    avg_loss:0.385, val_acc:0.841]
Epoch [19/120    avg_loss:0.292, val_acc:0.866]
Epoch [20/120    avg_loss:0.237, val_acc:0.888]
Epoch [21/120    avg_loss:0.281, val_acc:0.871]
Epoch [22/120    avg_loss:0.218, val_acc:0.902]
Epoch [23/120    avg_loss:0.209, val_acc:0.870]
Epoch [24/120    avg_loss:0.199, val_acc:0.897]
Epoch [25/120    avg_loss:0.165, val_acc:0.924]
Epoch [26/120    avg_loss:0.159, val_acc:0.933]
Epoch [27/120    avg_loss:0.154, val_acc:0.923]
Epoch [28/120    avg_loss:0.143, val_acc:0.930]
Epoch [29/120    avg_loss:0.126, val_acc:0.897]
Epoch [30/120    avg_loss:0.127, val_acc:0.930]
Epoch [31/120    avg_loss:0.120, val_acc:0.918]
Epoch [32/120    avg_loss:0.094, val_acc:0.935]
Epoch [33/120    avg_loss:0.114, val_acc:0.914]
Epoch [34/120    avg_loss:0.104, val_acc:0.939]
Epoch [35/120    avg_loss:0.093, val_acc:0.937]
Epoch [36/120    avg_loss:0.082, val_acc:0.939]
Epoch [37/120    avg_loss:0.087, val_acc:0.928]
Epoch [38/120    avg_loss:0.088, val_acc:0.940]
Epoch [39/120    avg_loss:0.096, val_acc:0.939]
Epoch [40/120    avg_loss:0.077, val_acc:0.942]
Epoch [41/120    avg_loss:0.070, val_acc:0.948]
Epoch [42/120    avg_loss:0.081, val_acc:0.937]
Epoch [43/120    avg_loss:0.071, val_acc:0.950]
Epoch [44/120    avg_loss:0.076, val_acc:0.950]
Epoch [45/120    avg_loss:0.076, val_acc:0.946]
Epoch [46/120    avg_loss:0.053, val_acc:0.952]
Epoch [47/120    avg_loss:0.083, val_acc:0.939]
Epoch [48/120    avg_loss:0.074, val_acc:0.961]
Epoch [49/120    avg_loss:0.061, val_acc:0.959]
Epoch [50/120    avg_loss:0.049, val_acc:0.950]
Epoch [51/120    avg_loss:0.051, val_acc:0.953]
Epoch [52/120    avg_loss:0.043, val_acc:0.954]
Epoch [53/120    avg_loss:0.045, val_acc:0.961]
Epoch [54/120    avg_loss:0.049, val_acc:0.954]
Epoch [55/120    avg_loss:0.051, val_acc:0.964]
Epoch [56/120    avg_loss:0.065, val_acc:0.952]
Epoch [57/120    avg_loss:0.051, val_acc:0.970]
Epoch [58/120    avg_loss:0.037, val_acc:0.954]
Epoch [59/120    avg_loss:0.051, val_acc:0.953]
Epoch [60/120    avg_loss:0.059, val_acc:0.951]
Epoch [61/120    avg_loss:0.039, val_acc:0.965]
Epoch [62/120    avg_loss:0.036, val_acc:0.966]
Epoch [63/120    avg_loss:0.037, val_acc:0.960]
Epoch [64/120    avg_loss:0.042, val_acc:0.970]
Epoch [65/120    avg_loss:0.045, val_acc:0.973]
Epoch [66/120    avg_loss:0.039, val_acc:0.963]
Epoch [67/120    avg_loss:0.038, val_acc:0.970]
Epoch [68/120    avg_loss:0.035, val_acc:0.972]
Epoch [69/120    avg_loss:0.025, val_acc:0.964]
Epoch [70/120    avg_loss:0.023, val_acc:0.964]
Epoch [71/120    avg_loss:0.027, val_acc:0.977]
Epoch [72/120    avg_loss:0.025, val_acc:0.962]
Epoch [73/120    avg_loss:0.017, val_acc:0.967]
Epoch [74/120    avg_loss:0.021, val_acc:0.971]
Epoch [75/120    avg_loss:0.021, val_acc:0.966]
Epoch [76/120    avg_loss:0.022, val_acc:0.968]
Epoch [77/120    avg_loss:0.024, val_acc:0.952]
Epoch [78/120    avg_loss:0.023, val_acc:0.976]
Epoch [79/120    avg_loss:0.017, val_acc:0.976]
Epoch [80/120    avg_loss:0.018, val_acc:0.971]
Epoch [81/120    avg_loss:0.022, val_acc:0.974]
Epoch [82/120    avg_loss:0.020, val_acc:0.971]
Epoch [83/120    avg_loss:0.018, val_acc:0.974]
Epoch [84/120    avg_loss:0.025, val_acc:0.950]
Epoch [85/120    avg_loss:0.017, val_acc:0.966]
Epoch [86/120    avg_loss:0.017, val_acc:0.973]
Epoch [87/120    avg_loss:0.014, val_acc:0.974]
Epoch [88/120    avg_loss:0.012, val_acc:0.977]
Epoch [89/120    avg_loss:0.011, val_acc:0.977]
Epoch [90/120    avg_loss:0.010, val_acc:0.977]
Epoch [91/120    avg_loss:0.011, val_acc:0.977]
Epoch [92/120    avg_loss:0.013, val_acc:0.978]
Epoch [93/120    avg_loss:0.011, val_acc:0.978]
Epoch [94/120    avg_loss:0.011, val_acc:0.978]
Epoch [95/120    avg_loss:0.012, val_acc:0.978]
Epoch [96/120    avg_loss:0.011, val_acc:0.978]
Epoch [97/120    avg_loss:0.010, val_acc:0.979]
Epoch [98/120    avg_loss:0.008, val_acc:0.977]
Epoch [99/120    avg_loss:0.011, val_acc:0.979]
Epoch [100/120    avg_loss:0.010, val_acc:0.977]
Epoch [101/120    avg_loss:0.011, val_acc:0.977]
Epoch [102/120    avg_loss:0.011, val_acc:0.976]
Epoch [103/120    avg_loss:0.009, val_acc:0.976]
Epoch [104/120    avg_loss:0.011, val_acc:0.977]
Epoch [105/120    avg_loss:0.010, val_acc:0.978]
Epoch [106/120    avg_loss:0.012, val_acc:0.977]
Epoch [107/120    avg_loss:0.010, val_acc:0.978]
Epoch [108/120    avg_loss:0.011, val_acc:0.978]
Epoch [109/120    avg_loss:0.010, val_acc:0.978]
Epoch [110/120    avg_loss:0.013, val_acc:0.976]
Epoch [111/120    avg_loss:0.012, val_acc:0.976]
Epoch [112/120    avg_loss:0.011, val_acc:0.976]
Epoch [113/120    avg_loss:0.010, val_acc:0.976]
Epoch [114/120    avg_loss:0.010, val_acc:0.976]
Epoch [115/120    avg_loss:0.009, val_acc:0.976]
Epoch [116/120    avg_loss:0.009, val_acc:0.976]
Epoch [117/120    avg_loss:0.009, val_acc:0.976]
Epoch [118/120    avg_loss:0.011, val_acc:0.976]
Epoch [119/120    avg_loss:0.010, val_acc:0.976]
Epoch [120/120    avg_loss:0.012, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1255    0    0    0    1    0    0    1    7   13    6    0
     0    2    0]
 [   0    0    0  716    0   18    0    0    0    6    0    0    4    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    0    0    5    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    5   24    0    5    1    0    0    0  836    4    0    0
     0    0    0]
 [   0    0    6    0    0    0    4    0    1    0   16 2181    0    2
     0    0    0]
 [   0    0    8   23    8    4    0    0    0    0    3    4  479    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    2    0    3    0    0    0
  1129    0    0]
 [   0    0    0    0    0    0   26    0    0    0    0    0    0    0
    42  279    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.97560975609755

F1 scores:
[       nan 0.98765432 0.98046875 0.94583884 0.98156682 0.9529148
 0.97546468 1.         0.99652375 0.63636364 0.96091954 0.98844324
 0.93554688 0.98666667 0.97537797 0.88853503 0.96511628]

Kappa:
0.9655155508717155
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9eb0bd8780>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.594, val_acc:0.322]
Epoch [2/120    avg_loss:2.130, val_acc:0.518]
Epoch [3/120    avg_loss:1.875, val_acc:0.588]
Epoch [4/120    avg_loss:1.722, val_acc:0.620]
Epoch [5/120    avg_loss:1.507, val_acc:0.642]
Epoch [6/120    avg_loss:1.395, val_acc:0.670]
Epoch [7/120    avg_loss:1.208, val_acc:0.705]
Epoch [8/120    avg_loss:1.068, val_acc:0.743]
Epoch [9/120    avg_loss:0.878, val_acc:0.784]
Epoch [10/120    avg_loss:0.775, val_acc:0.778]
Epoch [11/120    avg_loss:0.673, val_acc:0.799]
Epoch [12/120    avg_loss:0.608, val_acc:0.798]
Epoch [13/120    avg_loss:0.542, val_acc:0.805]
Epoch [14/120    avg_loss:0.532, val_acc:0.820]
Epoch [15/120    avg_loss:0.471, val_acc:0.856]
Epoch [16/120    avg_loss:0.374, val_acc:0.868]
Epoch [17/120    avg_loss:0.434, val_acc:0.856]
Epoch [18/120    avg_loss:0.326, val_acc:0.865]
Epoch [19/120    avg_loss:0.376, val_acc:0.877]
Epoch [20/120    avg_loss:0.349, val_acc:0.867]
Epoch [21/120    avg_loss:0.314, val_acc:0.895]
Epoch [22/120    avg_loss:0.254, val_acc:0.902]
Epoch [23/120    avg_loss:0.239, val_acc:0.887]
Epoch [24/120    avg_loss:0.225, val_acc:0.915]
Epoch [25/120    avg_loss:0.228, val_acc:0.927]
Epoch [26/120    avg_loss:0.213, val_acc:0.907]
Epoch [27/120    avg_loss:0.154, val_acc:0.923]
Epoch [28/120    avg_loss:0.162, val_acc:0.938]
Epoch [29/120    avg_loss:0.140, val_acc:0.917]
Epoch [30/120    avg_loss:0.137, val_acc:0.906]
Epoch [31/120    avg_loss:0.162, val_acc:0.936]
Epoch [32/120    avg_loss:0.121, val_acc:0.934]
Epoch [33/120    avg_loss:0.139, val_acc:0.935]
Epoch [34/120    avg_loss:0.109, val_acc:0.939]
Epoch [35/120    avg_loss:0.089, val_acc:0.942]
Epoch [36/120    avg_loss:0.093, val_acc:0.938]
Epoch [37/120    avg_loss:0.100, val_acc:0.936]
Epoch [38/120    avg_loss:0.096, val_acc:0.945]
Epoch [39/120    avg_loss:0.089, val_acc:0.951]
Epoch [40/120    avg_loss:0.090, val_acc:0.952]
Epoch [41/120    avg_loss:0.109, val_acc:0.955]
Epoch [42/120    avg_loss:0.093, val_acc:0.934]
Epoch [43/120    avg_loss:0.116, val_acc:0.958]
Epoch [44/120    avg_loss:0.118, val_acc:0.943]
Epoch [45/120    avg_loss:0.097, val_acc:0.935]
Epoch [46/120    avg_loss:0.085, val_acc:0.959]
Epoch [47/120    avg_loss:0.083, val_acc:0.949]
Epoch [48/120    avg_loss:0.080, val_acc:0.969]
Epoch [49/120    avg_loss:0.060, val_acc:0.956]
Epoch [50/120    avg_loss:0.059, val_acc:0.961]
Epoch [51/120    avg_loss:0.050, val_acc:0.970]
Epoch [52/120    avg_loss:0.064, val_acc:0.963]
Epoch [53/120    avg_loss:0.047, val_acc:0.972]
Epoch [54/120    avg_loss:0.062, val_acc:0.957]
Epoch [55/120    avg_loss:0.070, val_acc:0.966]
Epoch [56/120    avg_loss:0.057, val_acc:0.968]
Epoch [57/120    avg_loss:0.051, val_acc:0.966]
Epoch [58/120    avg_loss:0.046, val_acc:0.973]
Epoch [59/120    avg_loss:0.035, val_acc:0.969]
Epoch [60/120    avg_loss:0.045, val_acc:0.975]
Epoch [61/120    avg_loss:0.059, val_acc:0.940]
Epoch [62/120    avg_loss:0.061, val_acc:0.959]
Epoch [63/120    avg_loss:0.053, val_acc:0.963]
Epoch [64/120    avg_loss:0.044, val_acc:0.966]
Epoch [65/120    avg_loss:0.036, val_acc:0.970]
Epoch [66/120    avg_loss:0.037, val_acc:0.963]
Epoch [67/120    avg_loss:0.041, val_acc:0.963]
Epoch [68/120    avg_loss:0.070, val_acc:0.952]
Epoch [69/120    avg_loss:0.055, val_acc:0.960]
Epoch [70/120    avg_loss:0.063, val_acc:0.951]
Epoch [71/120    avg_loss:0.055, val_acc:0.960]
Epoch [72/120    avg_loss:0.044, val_acc:0.965]
Epoch [73/120    avg_loss:0.067, val_acc:0.952]
Epoch [74/120    avg_loss:0.065, val_acc:0.967]
Epoch [75/120    avg_loss:0.038, val_acc:0.977]
Epoch [76/120    avg_loss:0.035, val_acc:0.982]
Epoch [77/120    avg_loss:0.026, val_acc:0.982]
Epoch [78/120    avg_loss:0.024, val_acc:0.982]
Epoch [79/120    avg_loss:0.029, val_acc:0.984]
Epoch [80/120    avg_loss:0.028, val_acc:0.984]
Epoch [81/120    avg_loss:0.028, val_acc:0.984]
Epoch [82/120    avg_loss:0.021, val_acc:0.984]
Epoch [83/120    avg_loss:0.020, val_acc:0.984]
Epoch [84/120    avg_loss:0.019, val_acc:0.983]
Epoch [85/120    avg_loss:0.026, val_acc:0.984]
Epoch [86/120    avg_loss:0.019, val_acc:0.983]
Epoch [87/120    avg_loss:0.020, val_acc:0.983]
Epoch [88/120    avg_loss:0.019, val_acc:0.982]
Epoch [89/120    avg_loss:0.023, val_acc:0.984]
Epoch [90/120    avg_loss:0.019, val_acc:0.986]
Epoch [91/120    avg_loss:0.023, val_acc:0.988]
Epoch [92/120    avg_loss:0.025, val_acc:0.986]
Epoch [93/120    avg_loss:0.021, val_acc:0.986]
Epoch [94/120    avg_loss:0.021, val_acc:0.986]
Epoch [95/120    avg_loss:0.015, val_acc:0.986]
Epoch [96/120    avg_loss:0.017, val_acc:0.986]
Epoch [97/120    avg_loss:0.019, val_acc:0.985]
Epoch [98/120    avg_loss:0.018, val_acc:0.986]
Epoch [99/120    avg_loss:0.021, val_acc:0.985]
Epoch [100/120    avg_loss:0.021, val_acc:0.984]
Epoch [101/120    avg_loss:0.021, val_acc:0.985]
Epoch [102/120    avg_loss:0.022, val_acc:0.986]
Epoch [103/120    avg_loss:0.018, val_acc:0.986]
Epoch [104/120    avg_loss:0.017, val_acc:0.985]
Epoch [105/120    avg_loss:0.018, val_acc:0.986]
Epoch [106/120    avg_loss:0.018, val_acc:0.985]
Epoch [107/120    avg_loss:0.019, val_acc:0.986]
Epoch [108/120    avg_loss:0.018, val_acc:0.986]
Epoch [109/120    avg_loss:0.016, val_acc:0.985]
Epoch [110/120    avg_loss:0.020, val_acc:0.985]
Epoch [111/120    avg_loss:0.015, val_acc:0.988]
Epoch [112/120    avg_loss:0.015, val_acc:0.986]
Epoch [113/120    avg_loss:0.021, val_acc:0.986]
Epoch [114/120    avg_loss:0.016, val_acc:0.986]
Epoch [115/120    avg_loss:0.016, val_acc:0.986]
Epoch [116/120    avg_loss:0.022, val_acc:0.986]
Epoch [117/120    avg_loss:0.022, val_acc:0.986]
Epoch [118/120    avg_loss:0.015, val_acc:0.986]
Epoch [119/120    avg_loss:0.021, val_acc:0.986]
Epoch [120/120    avg_loss:0.017, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1257    1    0    0    1    0    0    1    3   15    1    0
     0    6    0]
 [   0    0    1  682   41    7    0    0    0    7    0    0    6    2
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    3    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0   21   54    0    7    0    0    0    0  781    8    0    0
     1    3    0]
 [   0    0    9    0    0    0    5    0    0    0   22 2170    0    4
     0    0    0]
 [   0    0    0   15   13    9    0    0    0    0    3   12  478    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    3    0    3    0    0    0
  1121    2    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    44  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.94579945799458

F1 scores:
[       nan 0.98765432 0.97668998 0.9075183  0.8875     0.95768374
 0.98274569 1.         0.99652375 0.65116279 0.92590397 0.98256735
 0.93633692 0.98404255 0.97140381 0.89164087 0.96470588]

Kappa:
0.9537899834540511
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbdf3b3e748>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.560, val_acc:0.541]
Epoch [2/120    avg_loss:2.110, val_acc:0.574]
Epoch [3/120    avg_loss:1.879, val_acc:0.613]
Epoch [4/120    avg_loss:1.667, val_acc:0.562]
Epoch [5/120    avg_loss:1.489, val_acc:0.657]
Epoch [6/120    avg_loss:1.277, val_acc:0.698]
Epoch [7/120    avg_loss:1.053, val_acc:0.719]
Epoch [8/120    avg_loss:0.915, val_acc:0.759]
Epoch [9/120    avg_loss:0.806, val_acc:0.760]
Epoch [10/120    avg_loss:0.708, val_acc:0.785]
Epoch [11/120    avg_loss:0.646, val_acc:0.798]
Epoch [12/120    avg_loss:0.655, val_acc:0.827]
Epoch [13/120    avg_loss:0.504, val_acc:0.844]
Epoch [14/120    avg_loss:0.450, val_acc:0.839]
Epoch [15/120    avg_loss:0.417, val_acc:0.842]
Epoch [16/120    avg_loss:0.418, val_acc:0.845]
Epoch [17/120    avg_loss:0.364, val_acc:0.870]
Epoch [18/120    avg_loss:0.295, val_acc:0.882]
Epoch [19/120    avg_loss:0.298, val_acc:0.906]
Epoch [20/120    avg_loss:0.283, val_acc:0.897]
Epoch [21/120    avg_loss:0.315, val_acc:0.861]
Epoch [22/120    avg_loss:0.301, val_acc:0.884]
Epoch [23/120    avg_loss:0.260, val_acc:0.911]
Epoch [24/120    avg_loss:0.258, val_acc:0.919]
Epoch [25/120    avg_loss:0.200, val_acc:0.923]
Epoch [26/120    avg_loss:0.197, val_acc:0.936]
Epoch [27/120    avg_loss:0.197, val_acc:0.934]
Epoch [28/120    avg_loss:0.190, val_acc:0.930]
Epoch [29/120    avg_loss:0.204, val_acc:0.893]
Epoch [30/120    avg_loss:0.153, val_acc:0.936]
Epoch [31/120    avg_loss:0.133, val_acc:0.938]
Epoch [32/120    avg_loss:0.123, val_acc:0.931]
Epoch [33/120    avg_loss:0.135, val_acc:0.931]
Epoch [34/120    avg_loss:0.112, val_acc:0.940]
Epoch [35/120    avg_loss:0.098, val_acc:0.945]
Epoch [36/120    avg_loss:0.104, val_acc:0.952]
Epoch [37/120    avg_loss:0.092, val_acc:0.948]
Epoch [38/120    avg_loss:0.083, val_acc:0.948]
Epoch [39/120    avg_loss:0.065, val_acc:0.955]
Epoch [40/120    avg_loss:0.080, val_acc:0.941]
Epoch [41/120    avg_loss:0.077, val_acc:0.953]
Epoch [42/120    avg_loss:0.061, val_acc:0.963]
Epoch [43/120    avg_loss:0.055, val_acc:0.959]
Epoch [44/120    avg_loss:0.065, val_acc:0.957]
Epoch [45/120    avg_loss:0.047, val_acc:0.959]
Epoch [46/120    avg_loss:0.047, val_acc:0.959]
Epoch [47/120    avg_loss:0.071, val_acc:0.956]
Epoch [48/120    avg_loss:0.059, val_acc:0.948]
Epoch [49/120    avg_loss:0.067, val_acc:0.959]
Epoch [50/120    avg_loss:0.069, val_acc:0.958]
Epoch [51/120    avg_loss:0.053, val_acc:0.956]
Epoch [52/120    avg_loss:0.036, val_acc:0.967]
Epoch [53/120    avg_loss:0.034, val_acc:0.965]
Epoch [54/120    avg_loss:0.054, val_acc:0.956]
Epoch [55/120    avg_loss:0.059, val_acc:0.960]
Epoch [56/120    avg_loss:0.055, val_acc:0.951]
Epoch [57/120    avg_loss:0.041, val_acc:0.964]
Epoch [58/120    avg_loss:0.036, val_acc:0.960]
Epoch [59/120    avg_loss:0.043, val_acc:0.960]
Epoch [60/120    avg_loss:0.040, val_acc:0.970]
Epoch [61/120    avg_loss:0.032, val_acc:0.969]
Epoch [62/120    avg_loss:0.022, val_acc:0.969]
Epoch [63/120    avg_loss:0.029, val_acc:0.963]
Epoch [64/120    avg_loss:0.035, val_acc:0.964]
Epoch [65/120    avg_loss:0.040, val_acc:0.961]
Epoch [66/120    avg_loss:0.043, val_acc:0.958]
Epoch [67/120    avg_loss:0.033, val_acc:0.960]
Epoch [68/120    avg_loss:0.027, val_acc:0.965]
Epoch [69/120    avg_loss:0.028, val_acc:0.968]
Epoch [70/120    avg_loss:0.026, val_acc:0.960]
Epoch [71/120    avg_loss:0.039, val_acc:0.973]
Epoch [72/120    avg_loss:0.026, val_acc:0.974]
Epoch [73/120    avg_loss:0.029, val_acc:0.968]
Epoch [74/120    avg_loss:0.032, val_acc:0.963]
Epoch [75/120    avg_loss:0.024, val_acc:0.969]
Epoch [76/120    avg_loss:0.022, val_acc:0.967]
Epoch [77/120    avg_loss:0.015, val_acc:0.970]
Epoch [78/120    avg_loss:0.027, val_acc:0.961]
Epoch [79/120    avg_loss:0.035, val_acc:0.968]
Epoch [80/120    avg_loss:0.028, val_acc:0.964]
Epoch [81/120    avg_loss:0.028, val_acc:0.965]
Epoch [82/120    avg_loss:0.038, val_acc:0.959]
Epoch [83/120    avg_loss:0.030, val_acc:0.970]
Epoch [84/120    avg_loss:0.023, val_acc:0.973]
Epoch [85/120    avg_loss:0.021, val_acc:0.974]
Epoch [86/120    avg_loss:0.020, val_acc:0.975]
Epoch [87/120    avg_loss:0.022, val_acc:0.966]
Epoch [88/120    avg_loss:0.026, val_acc:0.973]
Epoch [89/120    avg_loss:0.020, val_acc:0.970]
Epoch [90/120    avg_loss:0.020, val_acc:0.974]
Epoch [91/120    avg_loss:0.015, val_acc:0.973]
Epoch [92/120    avg_loss:0.021, val_acc:0.977]
Epoch [93/120    avg_loss:0.019, val_acc:0.976]
Epoch [94/120    avg_loss:0.024, val_acc:0.970]
Epoch [95/120    avg_loss:0.014, val_acc:0.977]
Epoch [96/120    avg_loss:0.011, val_acc:0.976]
Epoch [97/120    avg_loss:0.015, val_acc:0.976]
Epoch [98/120    avg_loss:0.019, val_acc:0.967]
Epoch [99/120    avg_loss:0.020, val_acc:0.968]
Epoch [100/120    avg_loss:0.012, val_acc:0.976]
Epoch [101/120    avg_loss:0.013, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.976]
Epoch [103/120    avg_loss:0.013, val_acc:0.977]
Epoch [104/120    avg_loss:0.022, val_acc:0.969]
Epoch [105/120    avg_loss:0.014, val_acc:0.970]
Epoch [106/120    avg_loss:0.048, val_acc:0.967]
Epoch [107/120    avg_loss:0.206, val_acc:0.932]
Epoch [108/120    avg_loss:0.347, val_acc:0.909]
Epoch [109/120    avg_loss:0.193, val_acc:0.932]
Epoch [110/120    avg_loss:0.122, val_acc:0.941]
Epoch [111/120    avg_loss:0.078, val_acc:0.961]
Epoch [112/120    avg_loss:0.042, val_acc:0.969]
Epoch [113/120    avg_loss:0.035, val_acc:0.958]
Epoch [114/120    avg_loss:0.034, val_acc:0.965]
Epoch [115/120    avg_loss:0.028, val_acc:0.969]
Epoch [116/120    avg_loss:0.021, val_acc:0.968]
Epoch [117/120    avg_loss:0.022, val_acc:0.969]
Epoch [118/120    avg_loss:0.019, val_acc:0.968]
Epoch [119/120    avg_loss:0.018, val_acc:0.969]
Epoch [120/120    avg_loss:0.020, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    2 1239    4   11    0    1    0    0    1   14   12    1    0
     0    0    0]
 [   0    0    1  722    1   12    0    0    0    6    0    0    0    3
     2    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    2    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   12   28    0    5    0    0    0    0  823    7    0    0
     0    0    0]
 [   0    0    8    0    0    2    3    0    0    0   20 2171    0    6
     0    0    0]
 [   0    0    4   25   10    5    0    0    0    0    7    3  475    0
     0    2    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    3    0    0    0
  1133    0    0]
 [   0    0    0    0    0    0   21    0    0    5    0    0    0    0
    40  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.65040650406505

F1 scores:
[       nan 0.9382716  0.97214594 0.94564506 0.95089286 0.95945946
 0.98056801 1.         1.         0.66666667 0.94326648 0.98592189
 0.93873518 0.9762533  0.97630332 0.89206349 0.97647059]

Kappa:
0.9618192204363547
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a88e877b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 131993==>0.13M
----------Training process----------
Epoch [1/120    avg_loss:2.642, val_acc:0.407]
Epoch [2/120    avg_loss:2.115, val_acc:0.562]
Epoch [3/120    avg_loss:1.844, val_acc:0.596]
Epoch [4/120    avg_loss:1.668, val_acc:0.608]
Epoch [5/120    avg_loss:1.514, val_acc:0.636]
Epoch [6/120    avg_loss:1.331, val_acc:0.663]
Epoch [7/120    avg_loss:1.138, val_acc:0.688]
Epoch [8/120    avg_loss:0.991, val_acc:0.666]
Epoch [9/120    avg_loss:0.875, val_acc:0.742]
Epoch [10/120    avg_loss:0.760, val_acc:0.752]
Epoch [11/120    avg_loss:0.717, val_acc:0.784]
Epoch [12/120    avg_loss:0.653, val_acc:0.741]
Epoch [13/120    avg_loss:0.614, val_acc:0.823]
Epoch [14/120    avg_loss:0.524, val_acc:0.801]
Epoch [15/120    avg_loss:0.432, val_acc:0.812]
Epoch [16/120    avg_loss:0.412, val_acc:0.788]
Epoch [17/120    avg_loss:0.595, val_acc:0.791]
Epoch [18/120    avg_loss:0.511, val_acc:0.810]
Epoch [19/120    avg_loss:0.382, val_acc:0.849]
Epoch [20/120    avg_loss:0.356, val_acc:0.860]
Epoch [21/120    avg_loss:0.341, val_acc:0.866]
Epoch [22/120    avg_loss:0.295, val_acc:0.880]
Epoch [23/120    avg_loss:0.262, val_acc:0.880]
Epoch [24/120    avg_loss:0.261, val_acc:0.877]
Epoch [25/120    avg_loss:0.237, val_acc:0.888]
Epoch [26/120    avg_loss:0.221, val_acc:0.911]
Epoch [27/120    avg_loss:0.210, val_acc:0.858]
Epoch [28/120    avg_loss:0.168, val_acc:0.925]
Epoch [29/120    avg_loss:0.159, val_acc:0.918]
Epoch [30/120    avg_loss:0.159, val_acc:0.908]
Epoch [31/120    avg_loss:0.129, val_acc:0.927]
Epoch [32/120    avg_loss:0.133, val_acc:0.923]
Epoch [33/120    avg_loss:0.115, val_acc:0.936]
Epoch [34/120    avg_loss:0.123, val_acc:0.936]
Epoch [35/120    avg_loss:0.099, val_acc:0.933]
Epoch [36/120    avg_loss:0.095, val_acc:0.938]
Epoch [37/120    avg_loss:0.088, val_acc:0.941]
Epoch [38/120    avg_loss:0.091, val_acc:0.936]
Epoch [39/120    avg_loss:0.109, val_acc:0.928]
Epoch [40/120    avg_loss:0.106, val_acc:0.909]
Epoch [41/120    avg_loss:0.100, val_acc:0.943]
Epoch [42/120    avg_loss:0.084, val_acc:0.948]
Epoch [43/120    avg_loss:0.100, val_acc:0.939]
Epoch [44/120    avg_loss:0.070, val_acc:0.948]
Epoch [45/120    avg_loss:0.076, val_acc:0.952]
Epoch [46/120    avg_loss:0.113, val_acc:0.933]
Epoch [47/120    avg_loss:0.106, val_acc:0.938]
Epoch [48/120    avg_loss:0.105, val_acc:0.954]
Epoch [49/120    avg_loss:0.063, val_acc:0.957]
Epoch [50/120    avg_loss:0.063, val_acc:0.952]
Epoch [51/120    avg_loss:0.068, val_acc:0.939]
Epoch [52/120    avg_loss:0.053, val_acc:0.955]
Epoch [53/120    avg_loss:0.056, val_acc:0.953]
Epoch [54/120    avg_loss:0.053, val_acc:0.942]
Epoch [55/120    avg_loss:0.053, val_acc:0.953]
Epoch [56/120    avg_loss:0.044, val_acc:0.960]
Epoch [57/120    avg_loss:0.043, val_acc:0.955]
Epoch [58/120    avg_loss:0.040, val_acc:0.964]
Epoch [59/120    avg_loss:0.038, val_acc:0.962]
Epoch [60/120    avg_loss:0.039, val_acc:0.963]
Epoch [61/120    avg_loss:0.036, val_acc:0.959]
Epoch [62/120    avg_loss:0.037, val_acc:0.962]
Epoch [63/120    avg_loss:0.029, val_acc:0.951]
Epoch [64/120    avg_loss:0.046, val_acc:0.939]
Epoch [65/120    avg_loss:0.041, val_acc:0.962]
Epoch [66/120    avg_loss:0.044, val_acc:0.968]
Epoch [67/120    avg_loss:0.031, val_acc:0.966]
Epoch [68/120    avg_loss:0.029, val_acc:0.961]
Epoch [69/120    avg_loss:0.031, val_acc:0.959]
Epoch [70/120    avg_loss:0.030, val_acc:0.970]
Epoch [71/120    avg_loss:0.048, val_acc:0.962]
Epoch [72/120    avg_loss:0.039, val_acc:0.960]
Epoch [73/120    avg_loss:0.032, val_acc:0.957]
Epoch [74/120    avg_loss:0.027, val_acc:0.960]
Epoch [75/120    avg_loss:0.022, val_acc:0.972]
Epoch [76/120    avg_loss:0.026, val_acc:0.975]
Epoch [77/120    avg_loss:0.020, val_acc:0.970]
Epoch [78/120    avg_loss:0.023, val_acc:0.955]
Epoch [79/120    avg_loss:0.026, val_acc:0.961]
Epoch [80/120    avg_loss:0.019, val_acc:0.970]
Epoch [81/120    avg_loss:0.025, val_acc:0.975]
Epoch [82/120    avg_loss:0.019, val_acc:0.973]
Epoch [83/120    avg_loss:0.040, val_acc:0.962]
Epoch [84/120    avg_loss:0.037, val_acc:0.970]
Epoch [85/120    avg_loss:0.023, val_acc:0.982]
Epoch [86/120    avg_loss:0.020, val_acc:0.967]
Epoch [87/120    avg_loss:0.017, val_acc:0.979]
Epoch [88/120    avg_loss:0.020, val_acc:0.972]
Epoch [89/120    avg_loss:0.018, val_acc:0.968]
Epoch [90/120    avg_loss:0.015, val_acc:0.978]
Epoch [91/120    avg_loss:0.016, val_acc:0.978]
Epoch [92/120    avg_loss:0.014, val_acc:0.975]
Epoch [93/120    avg_loss:0.014, val_acc:0.974]
Epoch [94/120    avg_loss:0.021, val_acc:0.978]
Epoch [95/120    avg_loss:0.016, val_acc:0.976]
Epoch [96/120    avg_loss:0.017, val_acc:0.983]
Epoch [97/120    avg_loss:0.014, val_acc:0.977]
Epoch [98/120    avg_loss:0.012, val_acc:0.974]
Epoch [99/120    avg_loss:0.015, val_acc:0.979]
Epoch [100/120    avg_loss:0.016, val_acc:0.973]
Epoch [101/120    avg_loss:0.014, val_acc:0.977]
Epoch [102/120    avg_loss:0.021, val_acc:0.974]
Epoch [103/120    avg_loss:0.014, val_acc:0.972]
Epoch [104/120    avg_loss:0.025, val_acc:0.974]
Epoch [105/120    avg_loss:0.026, val_acc:0.974]
Epoch [106/120    avg_loss:0.016, val_acc:0.977]
Epoch [107/120    avg_loss:0.025, val_acc:0.972]
Epoch [108/120    avg_loss:0.014, val_acc:0.982]
Epoch [109/120    avg_loss:0.015, val_acc:0.980]
Epoch [110/120    avg_loss:0.011, val_acc:0.980]
Epoch [111/120    avg_loss:0.009, val_acc:0.980]
Epoch [112/120    avg_loss:0.008, val_acc:0.980]
Epoch [113/120    avg_loss:0.007, val_acc:0.983]
Epoch [114/120    avg_loss:0.008, val_acc:0.980]
Epoch [115/120    avg_loss:0.006, val_acc:0.982]
Epoch [116/120    avg_loss:0.006, val_acc:0.980]
Epoch [117/120    avg_loss:0.009, val_acc:0.984]
Epoch [118/120    avg_loss:0.008, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.984]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1273    0    0    0    0    0    0    1    3    8    0    0
     0    0    0]
 [   0    0    1  719   14    8    0    0    0    4    0    0    0    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    2    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    4   41    0    9    0    0    0    0  816    3    0    0
     0    2    0]
 [   0    0    9    0    0    0    5    0    0    0   17 2177    0    2
     0    0    0]
 [   0    0    0   11    1   10    0    0    0    0    0   12  499    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    1    0    0
  1134    0    0]
 [   0    0    0    0    0    0   31    0    0    1    0    0    0    0
    59  256    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.0189701897019

F1 scores:
[       nan 0.975      0.98912199 0.94729908 0.96598639 0.96062992
 0.97333333 1.         0.99883856 0.79069767 0.95215869 0.98707776
 0.96425121 0.9919571  0.97005988 0.84628099 0.98809524]

Kappa:
0.9660009575416941
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f81ebc79710>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.648, val_acc:0.406]
Epoch [2/120    avg_loss:2.128, val_acc:0.543]
Epoch [3/120    avg_loss:1.911, val_acc:0.575]
Epoch [4/120    avg_loss:1.733, val_acc:0.605]
Epoch [5/120    avg_loss:1.542, val_acc:0.613]
Epoch [6/120    avg_loss:1.304, val_acc:0.644]
Epoch [7/120    avg_loss:1.150, val_acc:0.665]
Epoch [8/120    avg_loss:1.013, val_acc:0.697]
Epoch [9/120    avg_loss:0.845, val_acc:0.744]
Epoch [10/120    avg_loss:0.851, val_acc:0.738]
Epoch [11/120    avg_loss:0.765, val_acc:0.774]
Epoch [12/120    avg_loss:0.663, val_acc:0.807]
Epoch [13/120    avg_loss:0.603, val_acc:0.839]
Epoch [14/120    avg_loss:0.532, val_acc:0.824]
Epoch [15/120    avg_loss:0.464, val_acc:0.838]
Epoch [16/120    avg_loss:0.413, val_acc:0.856]
Epoch [17/120    avg_loss:0.401, val_acc:0.856]
Epoch [18/120    avg_loss:0.409, val_acc:0.835]
Epoch [19/120    avg_loss:0.382, val_acc:0.852]
Epoch [20/120    avg_loss:0.355, val_acc:0.881]
Epoch [21/120    avg_loss:0.293, val_acc:0.883]
Epoch [22/120    avg_loss:0.270, val_acc:0.847]
Epoch [23/120    avg_loss:0.327, val_acc:0.866]
Epoch [24/120    avg_loss:0.302, val_acc:0.886]
Epoch [25/120    avg_loss:0.311, val_acc:0.877]
Epoch [26/120    avg_loss:0.291, val_acc:0.878]
Epoch [27/120    avg_loss:0.283, val_acc:0.881]
Epoch [28/120    avg_loss:0.227, val_acc:0.914]
Epoch [29/120    avg_loss:0.201, val_acc:0.938]
Epoch [30/120    avg_loss:0.216, val_acc:0.928]
Epoch [31/120    avg_loss:0.187, val_acc:0.934]
Epoch [32/120    avg_loss:0.156, val_acc:0.945]
Epoch [33/120    avg_loss:0.147, val_acc:0.919]
Epoch [34/120    avg_loss:0.152, val_acc:0.942]
Epoch [35/120    avg_loss:0.152, val_acc:0.920]
Epoch [36/120    avg_loss:0.147, val_acc:0.943]
Epoch [37/120    avg_loss:0.145, val_acc:0.934]
Epoch [38/120    avg_loss:0.148, val_acc:0.950]
Epoch [39/120    avg_loss:0.129, val_acc:0.936]
Epoch [40/120    avg_loss:0.127, val_acc:0.922]
Epoch [41/120    avg_loss:0.141, val_acc:0.934]
Epoch [42/120    avg_loss:0.142, val_acc:0.931]
Epoch [43/120    avg_loss:0.141, val_acc:0.953]
Epoch [44/120    avg_loss:0.101, val_acc:0.951]
Epoch [45/120    avg_loss:0.139, val_acc:0.934]
Epoch [46/120    avg_loss:0.143, val_acc:0.940]
Epoch [47/120    avg_loss:0.123, val_acc:0.924]
Epoch [48/120    avg_loss:0.118, val_acc:0.927]
Epoch [49/120    avg_loss:0.089, val_acc:0.958]
Epoch [50/120    avg_loss:0.108, val_acc:0.925]
Epoch [51/120    avg_loss:0.090, val_acc:0.959]
Epoch [52/120    avg_loss:0.072, val_acc:0.969]
Epoch [53/120    avg_loss:0.084, val_acc:0.956]
Epoch [54/120    avg_loss:0.211, val_acc:0.934]
Epoch [55/120    avg_loss:0.147, val_acc:0.931]
Epoch [56/120    avg_loss:0.095, val_acc:0.963]
Epoch [57/120    avg_loss:0.097, val_acc:0.948]
Epoch [58/120    avg_loss:0.072, val_acc:0.965]
Epoch [59/120    avg_loss:0.070, val_acc:0.967]
Epoch [60/120    avg_loss:0.086, val_acc:0.965]
Epoch [61/120    avg_loss:0.055, val_acc:0.967]
Epoch [62/120    avg_loss:0.058, val_acc:0.968]
Epoch [63/120    avg_loss:0.053, val_acc:0.963]
Epoch [64/120    avg_loss:0.046, val_acc:0.968]
Epoch [65/120    avg_loss:0.077, val_acc:0.965]
Epoch [66/120    avg_loss:0.040, val_acc:0.972]
Epoch [67/120    avg_loss:0.041, val_acc:0.973]
Epoch [68/120    avg_loss:0.036, val_acc:0.975]
Epoch [69/120    avg_loss:0.033, val_acc:0.976]
Epoch [70/120    avg_loss:0.030, val_acc:0.974]
Epoch [71/120    avg_loss:0.036, val_acc:0.974]
Epoch [72/120    avg_loss:0.049, val_acc:0.974]
Epoch [73/120    avg_loss:0.033, val_acc:0.975]
Epoch [74/120    avg_loss:0.030, val_acc:0.977]
Epoch [75/120    avg_loss:0.031, val_acc:0.977]
Epoch [76/120    avg_loss:0.033, val_acc:0.976]
Epoch [77/120    avg_loss:0.027, val_acc:0.977]
Epoch [78/120    avg_loss:0.032, val_acc:0.976]
Epoch [79/120    avg_loss:0.028, val_acc:0.976]
Epoch [80/120    avg_loss:0.031, val_acc:0.976]
Epoch [81/120    avg_loss:0.033, val_acc:0.978]
Epoch [82/120    avg_loss:0.028, val_acc:0.976]
Epoch [83/120    avg_loss:0.027, val_acc:0.976]
Epoch [84/120    avg_loss:0.028, val_acc:0.976]
Epoch [85/120    avg_loss:0.036, val_acc:0.980]
Epoch [86/120    avg_loss:0.035, val_acc:0.978]
Epoch [87/120    avg_loss:0.032, val_acc:0.976]
Epoch [88/120    avg_loss:0.038, val_acc:0.977]
Epoch [89/120    avg_loss:0.033, val_acc:0.975]
Epoch [90/120    avg_loss:0.028, val_acc:0.977]
Epoch [91/120    avg_loss:0.027, val_acc:0.980]
Epoch [92/120    avg_loss:0.028, val_acc:0.976]
Epoch [93/120    avg_loss:0.028, val_acc:0.978]
Epoch [94/120    avg_loss:0.024, val_acc:0.980]
Epoch [95/120    avg_loss:0.025, val_acc:0.981]
Epoch [96/120    avg_loss:0.026, val_acc:0.980]
Epoch [97/120    avg_loss:0.026, val_acc:0.981]
Epoch [98/120    avg_loss:0.025, val_acc:0.982]
Epoch [99/120    avg_loss:0.028, val_acc:0.981]
Epoch [100/120    avg_loss:0.031, val_acc:0.977]
Epoch [101/120    avg_loss:0.034, val_acc:0.978]
Epoch [102/120    avg_loss:0.027, val_acc:0.981]
Epoch [103/120    avg_loss:0.024, val_acc:0.980]
Epoch [104/120    avg_loss:0.027, val_acc:0.978]
Epoch [105/120    avg_loss:0.029, val_acc:0.980]
Epoch [106/120    avg_loss:0.030, val_acc:0.978]
Epoch [107/120    avg_loss:0.029, val_acc:0.980]
Epoch [108/120    avg_loss:0.023, val_acc:0.976]
Epoch [109/120    avg_loss:0.028, val_acc:0.976]
Epoch [110/120    avg_loss:0.028, val_acc:0.980]
Epoch [111/120    avg_loss:0.023, val_acc:0.981]
Epoch [112/120    avg_loss:0.022, val_acc:0.981]
Epoch [113/120    avg_loss:0.028, val_acc:0.981]
Epoch [114/120    avg_loss:0.023, val_acc:0.981]
Epoch [115/120    avg_loss:0.024, val_acc:0.981]
Epoch [116/120    avg_loss:0.022, val_acc:0.981]
Epoch [117/120    avg_loss:0.024, val_acc:0.980]
Epoch [118/120    avg_loss:0.024, val_acc:0.980]
Epoch [119/120    avg_loss:0.026, val_acc:0.980]
Epoch [120/120    avg_loss:0.022, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    1 1229    0    0    0    3    0    0    2    9   22    2    0
     0   17    0]
 [   0    0    2  706    0   19    0    0    0    6    0    0   14    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    4    0    5    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   16    0    0    1    0
     0    0    0]
 [   0    0   10   82    0    4    6    0    0    0  757    5    0    0
     2    9    0]
 [   0    0    6    0    0    2   11    0    0    0   10 2176    1    2
     2    0    0]
 [   0    0    0   28   11    7    0    0    0    0    3    0  479    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    3    1    0    0
  1129    0    0]
 [   0    0    0    0    0    0   26    0    0    0    0    0    0    0
   109  212    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
94.90514905149051

F1 scores:
[       nan 0.95       0.97077409 0.9028133  0.97482838 0.94877506
 0.96389094 0.92592593 0.99883856 0.68085106 0.91204819 0.98528413
 0.92560386 0.99462366 0.94834103 0.72478632 0.94117647]

Kappa:
0.9418954611198719
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa105237780>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.571, val_acc:0.335]
Epoch [2/120    avg_loss:2.057, val_acc:0.544]
Epoch [3/120    avg_loss:1.836, val_acc:0.588]
Epoch [4/120    avg_loss:1.617, val_acc:0.585]
Epoch [5/120    avg_loss:1.411, val_acc:0.619]
Epoch [6/120    avg_loss:1.251, val_acc:0.630]
Epoch [7/120    avg_loss:1.090, val_acc:0.666]
Epoch [8/120    avg_loss:0.964, val_acc:0.689]
Epoch [9/120    avg_loss:0.982, val_acc:0.703]
Epoch [10/120    avg_loss:0.802, val_acc:0.755]
Epoch [11/120    avg_loss:0.680, val_acc:0.756]
Epoch [12/120    avg_loss:0.635, val_acc:0.793]
Epoch [13/120    avg_loss:0.560, val_acc:0.766]
Epoch [14/120    avg_loss:0.479, val_acc:0.843]
Epoch [15/120    avg_loss:0.513, val_acc:0.820]
Epoch [16/120    avg_loss:0.429, val_acc:0.873]
Epoch [17/120    avg_loss:0.365, val_acc:0.869]
Epoch [18/120    avg_loss:0.317, val_acc:0.875]
Epoch [19/120    avg_loss:0.347, val_acc:0.867]
Epoch [20/120    avg_loss:0.311, val_acc:0.892]
Epoch [21/120    avg_loss:0.301, val_acc:0.865]
Epoch [22/120    avg_loss:0.275, val_acc:0.908]
Epoch [23/120    avg_loss:0.275, val_acc:0.860]
Epoch [24/120    avg_loss:0.289, val_acc:0.860]
Epoch [25/120    avg_loss:0.242, val_acc:0.883]
Epoch [26/120    avg_loss:0.244, val_acc:0.920]
Epoch [27/120    avg_loss:0.203, val_acc:0.925]
Epoch [28/120    avg_loss:0.212, val_acc:0.887]
Epoch [29/120    avg_loss:0.162, val_acc:0.934]
Epoch [30/120    avg_loss:0.143, val_acc:0.930]
Epoch [31/120    avg_loss:0.126, val_acc:0.920]
Epoch [32/120    avg_loss:0.171, val_acc:0.916]
Epoch [33/120    avg_loss:0.130, val_acc:0.923]
Epoch [34/120    avg_loss:0.132, val_acc:0.930]
Epoch [35/120    avg_loss:0.176, val_acc:0.909]
Epoch [36/120    avg_loss:0.189, val_acc:0.938]
Epoch [37/120    avg_loss:0.152, val_acc:0.945]
Epoch [38/120    avg_loss:0.145, val_acc:0.920]
Epoch [39/120    avg_loss:0.138, val_acc:0.925]
Epoch [40/120    avg_loss:0.129, val_acc:0.924]
Epoch [41/120    avg_loss:0.124, val_acc:0.932]
Epoch [42/120    avg_loss:0.112, val_acc:0.949]
Epoch [43/120    avg_loss:0.107, val_acc:0.924]
Epoch [44/120    avg_loss:0.093, val_acc:0.936]
Epoch [45/120    avg_loss:0.085, val_acc:0.943]
Epoch [46/120    avg_loss:0.095, val_acc:0.943]
Epoch [47/120    avg_loss:0.081, val_acc:0.942]
Epoch [48/120    avg_loss:0.071, val_acc:0.949]
Epoch [49/120    avg_loss:0.082, val_acc:0.950]
Epoch [50/120    avg_loss:0.088, val_acc:0.950]
Epoch [51/120    avg_loss:0.062, val_acc:0.958]
Epoch [52/120    avg_loss:0.056, val_acc:0.963]
Epoch [53/120    avg_loss:0.063, val_acc:0.955]
Epoch [54/120    avg_loss:0.079, val_acc:0.953]
Epoch [55/120    avg_loss:0.070, val_acc:0.959]
Epoch [56/120    avg_loss:0.063, val_acc:0.959]
Epoch [57/120    avg_loss:0.061, val_acc:0.933]
Epoch [58/120    avg_loss:0.077, val_acc:0.920]
Epoch [59/120    avg_loss:0.102, val_acc:0.947]
Epoch [60/120    avg_loss:0.058, val_acc:0.951]
Epoch [61/120    avg_loss:0.057, val_acc:0.933]
Epoch [62/120    avg_loss:0.059, val_acc:0.959]
Epoch [63/120    avg_loss:0.055, val_acc:0.941]
Epoch [64/120    avg_loss:0.098, val_acc:0.961]
Epoch [65/120    avg_loss:0.072, val_acc:0.941]
Epoch [66/120    avg_loss:0.067, val_acc:0.951]
Epoch [67/120    avg_loss:0.057, val_acc:0.961]
Epoch [68/120    avg_loss:0.046, val_acc:0.963]
Epoch [69/120    avg_loss:0.042, val_acc:0.966]
Epoch [70/120    avg_loss:0.048, val_acc:0.969]
Epoch [71/120    avg_loss:0.035, val_acc:0.969]
Epoch [72/120    avg_loss:0.035, val_acc:0.969]
Epoch [73/120    avg_loss:0.033, val_acc:0.969]
Epoch [74/120    avg_loss:0.040, val_acc:0.972]
Epoch [75/120    avg_loss:0.035, val_acc:0.972]
Epoch [76/120    avg_loss:0.040, val_acc:0.972]
Epoch [77/120    avg_loss:0.041, val_acc:0.970]
Epoch [78/120    avg_loss:0.034, val_acc:0.970]
Epoch [79/120    avg_loss:0.039, val_acc:0.972]
Epoch [80/120    avg_loss:0.033, val_acc:0.972]
Epoch [81/120    avg_loss:0.029, val_acc:0.972]
Epoch [82/120    avg_loss:0.033, val_acc:0.974]
Epoch [83/120    avg_loss:0.028, val_acc:0.972]
Epoch [84/120    avg_loss:0.028, val_acc:0.974]
Epoch [85/120    avg_loss:0.029, val_acc:0.973]
Epoch [86/120    avg_loss:0.028, val_acc:0.974]
Epoch [87/120    avg_loss:0.028, val_acc:0.973]
Epoch [88/120    avg_loss:0.028, val_acc:0.973]
Epoch [89/120    avg_loss:0.029, val_acc:0.973]
Epoch [90/120    avg_loss:0.026, val_acc:0.973]
Epoch [91/120    avg_loss:0.024, val_acc:0.973]
Epoch [92/120    avg_loss:0.026, val_acc:0.974]
Epoch [93/120    avg_loss:0.026, val_acc:0.973]
Epoch [94/120    avg_loss:0.027, val_acc:0.974]
Epoch [95/120    avg_loss:0.028, val_acc:0.974]
Epoch [96/120    avg_loss:0.025, val_acc:0.974]
Epoch [97/120    avg_loss:0.027, val_acc:0.974]
Epoch [98/120    avg_loss:0.024, val_acc:0.973]
Epoch [99/120    avg_loss:0.028, val_acc:0.973]
Epoch [100/120    avg_loss:0.026, val_acc:0.974]
Epoch [101/120    avg_loss:0.025, val_acc:0.973]
Epoch [102/120    avg_loss:0.027, val_acc:0.973]
Epoch [103/120    avg_loss:0.029, val_acc:0.974]
Epoch [104/120    avg_loss:0.025, val_acc:0.973]
Epoch [105/120    avg_loss:0.024, val_acc:0.974]
Epoch [106/120    avg_loss:0.023, val_acc:0.973]
Epoch [107/120    avg_loss:0.022, val_acc:0.973]
Epoch [108/120    avg_loss:0.028, val_acc:0.972]
Epoch [109/120    avg_loss:0.028, val_acc:0.974]
Epoch [110/120    avg_loss:0.024, val_acc:0.975]
Epoch [111/120    avg_loss:0.025, val_acc:0.975]
Epoch [112/120    avg_loss:0.029, val_acc:0.974]
Epoch [113/120    avg_loss:0.022, val_acc:0.973]
Epoch [114/120    avg_loss:0.025, val_acc:0.974]
Epoch [115/120    avg_loss:0.023, val_acc:0.973]
Epoch [116/120    avg_loss:0.021, val_acc:0.973]
Epoch [117/120    avg_loss:0.024, val_acc:0.973]
Epoch [118/120    avg_loss:0.021, val_acc:0.973]
Epoch [119/120    avg_loss:0.032, val_acc:0.974]
Epoch [120/120    avg_loss:0.021, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    6 1216    4    0    0    2    0    0    0    9   31   15    0
     0    2    0]
 [   0    0    1  714    0   12    0    0    0   11    0    0    7    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    3    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   16    0    0    2    0
     0    0    0]
 [   0    0   18   90    0    6    0    0    0    0  756    1    0    0
     3    1    0]
 [   0    0   10    0    0    0   12    0    0    0   10 2173    1    3
     1    0    0]
 [   0    0    0   22    9    8    0    0    0    0    9    0  479    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    3    0    3    1    0    0
  1131    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   103  244    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.24119241192412

F1 scores:
[       nan 0.90697674 0.96126482 0.9055168  0.97931034 0.96179775
 0.98716981 0.98039216 0.99652375 0.66666667 0.90865385 0.98348043
 0.92204042 0.98666667 0.95042017 0.82154882 0.95402299]

Kappa:
0.945730658353839
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f86951f7710>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.575, val_acc:0.458]
Epoch [2/120    avg_loss:2.118, val_acc:0.490]
Epoch [3/120    avg_loss:1.921, val_acc:0.580]
Epoch [4/120    avg_loss:1.673, val_acc:0.606]
Epoch [5/120    avg_loss:1.534, val_acc:0.617]
Epoch [6/120    avg_loss:1.323, val_acc:0.644]
Epoch [7/120    avg_loss:1.165, val_acc:0.672]
Epoch [8/120    avg_loss:1.015, val_acc:0.677]
Epoch [9/120    avg_loss:0.872, val_acc:0.718]
Epoch [10/120    avg_loss:0.759, val_acc:0.727]
Epoch [11/120    avg_loss:0.669, val_acc:0.783]
Epoch [12/120    avg_loss:0.581, val_acc:0.802]
Epoch [13/120    avg_loss:0.619, val_acc:0.724]
Epoch [14/120    avg_loss:0.566, val_acc:0.801]
Epoch [15/120    avg_loss:0.470, val_acc:0.809]
Epoch [16/120    avg_loss:0.430, val_acc:0.852]
Epoch [17/120    avg_loss:0.391, val_acc:0.841]
Epoch [18/120    avg_loss:0.330, val_acc:0.845]
Epoch [19/120    avg_loss:0.370, val_acc:0.840]
Epoch [20/120    avg_loss:0.344, val_acc:0.861]
Epoch [21/120    avg_loss:0.294, val_acc:0.877]
Epoch [22/120    avg_loss:0.277, val_acc:0.878]
Epoch [23/120    avg_loss:0.288, val_acc:0.887]
Epoch [24/120    avg_loss:0.270, val_acc:0.899]
Epoch [25/120    avg_loss:0.212, val_acc:0.897]
Epoch [26/120    avg_loss:0.269, val_acc:0.901]
Epoch [27/120    avg_loss:0.336, val_acc:0.882]
Epoch [28/120    avg_loss:0.350, val_acc:0.897]
Epoch [29/120    avg_loss:0.227, val_acc:0.899]
Epoch [30/120    avg_loss:0.206, val_acc:0.914]
Epoch [31/120    avg_loss:0.188, val_acc:0.920]
Epoch [32/120    avg_loss:0.170, val_acc:0.928]
Epoch [33/120    avg_loss:0.193, val_acc:0.926]
Epoch [34/120    avg_loss:0.190, val_acc:0.933]
Epoch [35/120    avg_loss:0.149, val_acc:0.908]
Epoch [36/120    avg_loss:0.121, val_acc:0.941]
Epoch [37/120    avg_loss:0.100, val_acc:0.938]
Epoch [38/120    avg_loss:0.097, val_acc:0.934]
Epoch [39/120    avg_loss:0.096, val_acc:0.940]
Epoch [40/120    avg_loss:0.105, val_acc:0.947]
Epoch [41/120    avg_loss:0.092, val_acc:0.938]
Epoch [42/120    avg_loss:0.097, val_acc:0.945]
Epoch [43/120    avg_loss:0.081, val_acc:0.950]
Epoch [44/120    avg_loss:0.079, val_acc:0.942]
Epoch [45/120    avg_loss:0.184, val_acc:0.911]
Epoch [46/120    avg_loss:0.167, val_acc:0.909]
Epoch [47/120    avg_loss:0.146, val_acc:0.939]
Epoch [48/120    avg_loss:0.102, val_acc:0.943]
Epoch [49/120    avg_loss:0.079, val_acc:0.947]
Epoch [50/120    avg_loss:0.078, val_acc:0.935]
Epoch [51/120    avg_loss:0.077, val_acc:0.948]
Epoch [52/120    avg_loss:0.069, val_acc:0.942]
Epoch [53/120    avg_loss:0.061, val_acc:0.949]
Epoch [54/120    avg_loss:0.076, val_acc:0.956]
Epoch [55/120    avg_loss:0.083, val_acc:0.947]
Epoch [56/120    avg_loss:0.068, val_acc:0.964]
Epoch [57/120    avg_loss:0.061, val_acc:0.952]
Epoch [58/120    avg_loss:0.047, val_acc:0.950]
Epoch [59/120    avg_loss:0.048, val_acc:0.960]
Epoch [60/120    avg_loss:0.047, val_acc:0.949]
Epoch [61/120    avg_loss:0.061, val_acc:0.955]
Epoch [62/120    avg_loss:0.051, val_acc:0.964]
Epoch [63/120    avg_loss:0.040, val_acc:0.955]
Epoch [64/120    avg_loss:0.046, val_acc:0.960]
Epoch [65/120    avg_loss:0.064, val_acc:0.963]
Epoch [66/120    avg_loss:0.072, val_acc:0.957]
Epoch [67/120    avg_loss:0.054, val_acc:0.959]
Epoch [68/120    avg_loss:0.040, val_acc:0.965]
Epoch [69/120    avg_loss:0.047, val_acc:0.944]
Epoch [70/120    avg_loss:0.043, val_acc:0.949]
Epoch [71/120    avg_loss:0.048, val_acc:0.963]
Epoch [72/120    avg_loss:0.041, val_acc:0.963]
Epoch [73/120    avg_loss:0.046, val_acc:0.959]
Epoch [74/120    avg_loss:0.047, val_acc:0.957]
Epoch [75/120    avg_loss:0.040, val_acc:0.960]
Epoch [76/120    avg_loss:0.040, val_acc:0.969]
Epoch [77/120    avg_loss:0.035, val_acc:0.974]
Epoch [78/120    avg_loss:0.032, val_acc:0.967]
Epoch [79/120    avg_loss:0.029, val_acc:0.975]
Epoch [80/120    avg_loss:0.035, val_acc:0.970]
Epoch [81/120    avg_loss:0.039, val_acc:0.973]
Epoch [82/120    avg_loss:0.040, val_acc:0.967]
Epoch [83/120    avg_loss:0.030, val_acc:0.964]
Epoch [84/120    avg_loss:0.033, val_acc:0.968]
Epoch [85/120    avg_loss:0.033, val_acc:0.967]
Epoch [86/120    avg_loss:0.022, val_acc:0.968]
Epoch [87/120    avg_loss:0.028, val_acc:0.967]
Epoch [88/120    avg_loss:0.025, val_acc:0.970]
Epoch [89/120    avg_loss:0.022, val_acc:0.960]
Epoch [90/120    avg_loss:0.027, val_acc:0.969]
Epoch [91/120    avg_loss:0.022, val_acc:0.967]
Epoch [92/120    avg_loss:0.027, val_acc:0.972]
Epoch [93/120    avg_loss:0.018, val_acc:0.972]
Epoch [94/120    avg_loss:0.018, val_acc:0.973]
Epoch [95/120    avg_loss:0.019, val_acc:0.973]
Epoch [96/120    avg_loss:0.015, val_acc:0.974]
Epoch [97/120    avg_loss:0.015, val_acc:0.973]
Epoch [98/120    avg_loss:0.013, val_acc:0.972]
Epoch [99/120    avg_loss:0.015, val_acc:0.972]
Epoch [100/120    avg_loss:0.017, val_acc:0.973]
Epoch [101/120    avg_loss:0.014, val_acc:0.973]
Epoch [102/120    avg_loss:0.014, val_acc:0.972]
Epoch [103/120    avg_loss:0.015, val_acc:0.974]
Epoch [104/120    avg_loss:0.014, val_acc:0.973]
Epoch [105/120    avg_loss:0.015, val_acc:0.974]
Epoch [106/120    avg_loss:0.016, val_acc:0.975]
Epoch [107/120    avg_loss:0.016, val_acc:0.974]
Epoch [108/120    avg_loss:0.016, val_acc:0.973]
Epoch [109/120    avg_loss:0.017, val_acc:0.973]
Epoch [110/120    avg_loss:0.012, val_acc:0.973]
Epoch [111/120    avg_loss:0.014, val_acc:0.973]
Epoch [112/120    avg_loss:0.012, val_acc:0.973]
Epoch [113/120    avg_loss:0.015, val_acc:0.973]
Epoch [114/120    avg_loss:0.015, val_acc:0.973]
Epoch [115/120    avg_loss:0.014, val_acc:0.973]
Epoch [116/120    avg_loss:0.016, val_acc:0.973]
Epoch [117/120    avg_loss:0.016, val_acc:0.973]
Epoch [118/120    avg_loss:0.016, val_acc:0.973]
Epoch [119/120    avg_loss:0.015, val_acc:0.973]
Epoch [120/120    avg_loss:0.013, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1263    0    0    0    5    0    0    0   10    7    0    0
     0    0    0]
 [   0    0    1  704    0   23    0    0    0   10    0    0    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  424    0    1    0    5    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0   12    0    0    1    0
     0    0    0]
 [   0    0   18   86    0    5    0    0    0    0  761    2    0    0
     0    3    0]
 [   0    0   16    0    0    0   10    0    0    0   14 2165    1    3
     1    0    0]
 [   0    4    0   24    6    5    0    0    0    0    4    9  473    0
     0    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    1    0    0
  1135    0    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
   124  217    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
95.19783197831978

F1 scores:
[       nan 0.88636364 0.97793264 0.899106   0.98611111 0.95067265
 0.98350825 0.98039216 0.99416569 0.53333333 0.91247002 0.98521047
 0.92927308 0.9919571  0.94425957 0.7654321  0.94915254]

Kappa:
0.9452210734011545
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6160ab47f0>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.624, val_acc:0.469]
Epoch [2/120    avg_loss:2.072, val_acc:0.525]
Epoch [3/120    avg_loss:1.850, val_acc:0.581]
Epoch [4/120    avg_loss:1.696, val_acc:0.640]
Epoch [5/120    avg_loss:1.545, val_acc:0.633]
Epoch [6/120    avg_loss:1.386, val_acc:0.632]
Epoch [7/120    avg_loss:1.199, val_acc:0.707]
Epoch [8/120    avg_loss:0.974, val_acc:0.739]
Epoch [9/120    avg_loss:0.827, val_acc:0.770]
Epoch [10/120    avg_loss:0.731, val_acc:0.794]
Epoch [11/120    avg_loss:0.646, val_acc:0.817]
Epoch [12/120    avg_loss:0.541, val_acc:0.819]
Epoch [13/120    avg_loss:0.474, val_acc:0.832]
Epoch [14/120    avg_loss:0.427, val_acc:0.870]
Epoch [15/120    avg_loss:0.389, val_acc:0.881]
Epoch [16/120    avg_loss:0.352, val_acc:0.886]
Epoch [17/120    avg_loss:0.464, val_acc:0.836]
Epoch [18/120    avg_loss:0.388, val_acc:0.884]
Epoch [19/120    avg_loss:0.469, val_acc:0.832]
Epoch [20/120    avg_loss:0.463, val_acc:0.851]
Epoch [21/120    avg_loss:0.336, val_acc:0.875]
Epoch [22/120    avg_loss:0.291, val_acc:0.912]
Epoch [23/120    avg_loss:0.271, val_acc:0.893]
Epoch [24/120    avg_loss:0.242, val_acc:0.900]
Epoch [25/120    avg_loss:0.210, val_acc:0.905]
Epoch [26/120    avg_loss:0.197, val_acc:0.927]
Epoch [27/120    avg_loss:0.209, val_acc:0.917]
Epoch [28/120    avg_loss:0.170, val_acc:0.930]
Epoch [29/120    avg_loss:0.175, val_acc:0.939]
Epoch [30/120    avg_loss:0.175, val_acc:0.948]
Epoch [31/120    avg_loss:0.137, val_acc:0.945]
Epoch [32/120    avg_loss:0.126, val_acc:0.938]
Epoch [33/120    avg_loss:0.225, val_acc:0.930]
Epoch [34/120    avg_loss:0.187, val_acc:0.924]
Epoch [35/120    avg_loss:0.166, val_acc:0.940]
Epoch [36/120    avg_loss:0.137, val_acc:0.940]
Epoch [37/120    avg_loss:0.141, val_acc:0.952]
Epoch [38/120    avg_loss:0.132, val_acc:0.952]
Epoch [39/120    avg_loss:0.106, val_acc:0.956]
Epoch [40/120    avg_loss:0.108, val_acc:0.951]
Epoch [41/120    avg_loss:0.088, val_acc:0.951]
Epoch [42/120    avg_loss:0.087, val_acc:0.956]
Epoch [43/120    avg_loss:0.101, val_acc:0.950]
Epoch [44/120    avg_loss:0.095, val_acc:0.941]
Epoch [45/120    avg_loss:0.108, val_acc:0.957]
Epoch [46/120    avg_loss:0.108, val_acc:0.952]
Epoch [47/120    avg_loss:0.166, val_acc:0.947]
Epoch [48/120    avg_loss:0.154, val_acc:0.915]
Epoch [49/120    avg_loss:0.138, val_acc:0.958]
Epoch [50/120    avg_loss:0.105, val_acc:0.951]
Epoch [51/120    avg_loss:0.104, val_acc:0.918]
Epoch [52/120    avg_loss:0.126, val_acc:0.928]
Epoch [53/120    avg_loss:0.105, val_acc:0.953]
Epoch [54/120    avg_loss:0.079, val_acc:0.950]
Epoch [55/120    avg_loss:0.110, val_acc:0.958]
Epoch [56/120    avg_loss:0.069, val_acc:0.950]
Epoch [57/120    avg_loss:0.071, val_acc:0.961]
Epoch [58/120    avg_loss:0.066, val_acc:0.952]
Epoch [59/120    avg_loss:0.083, val_acc:0.951]
Epoch [60/120    avg_loss:0.080, val_acc:0.955]
Epoch [61/120    avg_loss:0.105, val_acc:0.956]
Epoch [62/120    avg_loss:0.083, val_acc:0.953]
Epoch [63/120    avg_loss:0.074, val_acc:0.965]
Epoch [64/120    avg_loss:0.071, val_acc:0.950]
Epoch [65/120    avg_loss:0.064, val_acc:0.951]
Epoch [66/120    avg_loss:0.076, val_acc:0.960]
Epoch [67/120    avg_loss:0.047, val_acc:0.969]
Epoch [68/120    avg_loss:0.038, val_acc:0.964]
Epoch [69/120    avg_loss:0.041, val_acc:0.970]
Epoch [70/120    avg_loss:0.044, val_acc:0.968]
Epoch [71/120    avg_loss:0.050, val_acc:0.970]
Epoch [72/120    avg_loss:0.041, val_acc:0.969]
Epoch [73/120    avg_loss:0.044, val_acc:0.974]
Epoch [74/120    avg_loss:0.039, val_acc:0.970]
Epoch [75/120    avg_loss:0.034, val_acc:0.968]
Epoch [76/120    avg_loss:0.045, val_acc:0.965]
Epoch [77/120    avg_loss:0.034, val_acc:0.964]
Epoch [78/120    avg_loss:0.027, val_acc:0.975]
Epoch [79/120    avg_loss:0.029, val_acc:0.975]
Epoch [80/120    avg_loss:0.032, val_acc:0.967]
Epoch [81/120    avg_loss:0.043, val_acc:0.964]
Epoch [82/120    avg_loss:0.040, val_acc:0.964]
Epoch [83/120    avg_loss:0.033, val_acc:0.972]
Epoch [84/120    avg_loss:0.029, val_acc:0.967]
Epoch [85/120    avg_loss:0.024, val_acc:0.974]
Epoch [86/120    avg_loss:0.025, val_acc:0.973]
Epoch [87/120    avg_loss:0.027, val_acc:0.972]
Epoch [88/120    avg_loss:0.029, val_acc:0.972]
Epoch [89/120    avg_loss:0.025, val_acc:0.970]
Epoch [90/120    avg_loss:0.024, val_acc:0.968]
Epoch [91/120    avg_loss:0.021, val_acc:0.983]
Epoch [92/120    avg_loss:0.025, val_acc:0.972]
Epoch [93/120    avg_loss:0.022, val_acc:0.970]
Epoch [94/120    avg_loss:0.023, val_acc:0.975]
Epoch [95/120    avg_loss:0.024, val_acc:0.970]
Epoch [96/120    avg_loss:0.020, val_acc:0.978]
Epoch [97/120    avg_loss:0.026, val_acc:0.969]
Epoch [98/120    avg_loss:0.029, val_acc:0.970]
Epoch [99/120    avg_loss:0.024, val_acc:0.977]
Epoch [100/120    avg_loss:0.022, val_acc:0.980]
Epoch [101/120    avg_loss:0.038, val_acc:0.972]
Epoch [102/120    avg_loss:0.028, val_acc:0.977]
Epoch [103/120    avg_loss:0.032, val_acc:0.975]
Epoch [104/120    avg_loss:0.029, val_acc:0.965]
Epoch [105/120    avg_loss:0.020, val_acc:0.974]
Epoch [106/120    avg_loss:0.018, val_acc:0.977]
Epoch [107/120    avg_loss:0.016, val_acc:0.982]
Epoch [108/120    avg_loss:0.013, val_acc:0.983]
Epoch [109/120    avg_loss:0.014, val_acc:0.983]
Epoch [110/120    avg_loss:0.018, val_acc:0.983]
Epoch [111/120    avg_loss:0.013, val_acc:0.984]
Epoch [112/120    avg_loss:0.012, val_acc:0.984]
Epoch [113/120    avg_loss:0.016, val_acc:0.984]
Epoch [114/120    avg_loss:0.011, val_acc:0.984]
Epoch [115/120    avg_loss:0.017, val_acc:0.983]
Epoch [116/120    avg_loss:0.012, val_acc:0.984]
Epoch [117/120    avg_loss:0.013, val_acc:0.984]
Epoch [118/120    avg_loss:0.013, val_acc:0.985]
Epoch [119/120    avg_loss:0.013, val_acc:0.985]
Epoch [120/120    avg_loss:0.013, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1257    0    0    0    4    0    0    1    5   10    0    0
     0    8    0]
 [   0    0    2  714    1   14    0    0    0    7    0    0    6    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    0    0    6    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   13    0    0    2    0
     0    0    0]
 [   0    0   10   89    0    4    1    0    0    0  764    3    0    0
     1    3    0]
 [   0    0   14    0    0    0   10    0    0    0    3 2128   53    2
     0    0    0]
 [   0    0    0   24    8    9    0    0    0    1    8    0  479    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   28    0    0    0    0    0    0    0
    71  248    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.30623306233062

F1 scores:
[       nan 0.96202532 0.97897196 0.9055168  0.97931034 0.9559322
 0.96755162 1.         0.99883856 0.56521739 0.92103677 0.97794118
 0.89033457 0.98666667 0.96601529 0.81848185 0.95906433]

Kappa:
0.9465387024500711
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f03aea1c780>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.552, val_acc:0.535]
Epoch [2/120    avg_loss:2.055, val_acc:0.556]
Epoch [3/120    avg_loss:1.815, val_acc:0.552]
Epoch [4/120    avg_loss:1.648, val_acc:0.607]
Epoch [5/120    avg_loss:1.461, val_acc:0.642]
Epoch [6/120    avg_loss:1.301, val_acc:0.676]
Epoch [7/120    avg_loss:1.121, val_acc:0.709]
Epoch [8/120    avg_loss:1.075, val_acc:0.664]
Epoch [9/120    avg_loss:0.908, val_acc:0.736]
Epoch [10/120    avg_loss:0.823, val_acc:0.744]
Epoch [11/120    avg_loss:0.697, val_acc:0.773]
Epoch [12/120    avg_loss:0.666, val_acc:0.824]
Epoch [13/120    avg_loss:0.597, val_acc:0.817]
Epoch [14/120    avg_loss:0.525, val_acc:0.849]
Epoch [15/120    avg_loss:0.498, val_acc:0.868]
Epoch [16/120    avg_loss:0.424, val_acc:0.869]
Epoch [17/120    avg_loss:0.348, val_acc:0.865]
Epoch [18/120    avg_loss:0.315, val_acc:0.891]
Epoch [19/120    avg_loss:0.312, val_acc:0.876]
Epoch [20/120    avg_loss:0.296, val_acc:0.884]
Epoch [21/120    avg_loss:0.321, val_acc:0.866]
Epoch [22/120    avg_loss:0.266, val_acc:0.898]
Epoch [23/120    avg_loss:0.245, val_acc:0.886]
Epoch [24/120    avg_loss:0.206, val_acc:0.914]
Epoch [25/120    avg_loss:0.193, val_acc:0.911]
Epoch [26/120    avg_loss:0.190, val_acc:0.909]
Epoch [27/120    avg_loss:0.208, val_acc:0.917]
Epoch [28/120    avg_loss:0.191, val_acc:0.884]
Epoch [29/120    avg_loss:0.142, val_acc:0.914]
Epoch [30/120    avg_loss:0.193, val_acc:0.884]
Epoch [31/120    avg_loss:0.160, val_acc:0.881]
Epoch [32/120    avg_loss:0.174, val_acc:0.924]
Epoch [33/120    avg_loss:0.136, val_acc:0.935]
Epoch [34/120    avg_loss:0.125, val_acc:0.926]
Epoch [35/120    avg_loss:0.120, val_acc:0.943]
Epoch [36/120    avg_loss:0.099, val_acc:0.941]
Epoch [37/120    avg_loss:0.106, val_acc:0.922]
Epoch [38/120    avg_loss:0.126, val_acc:0.941]
Epoch [39/120    avg_loss:0.183, val_acc:0.907]
Epoch [40/120    avg_loss:0.176, val_acc:0.923]
Epoch [41/120    avg_loss:0.152, val_acc:0.910]
Epoch [42/120    avg_loss:0.123, val_acc:0.931]
Epoch [43/120    avg_loss:0.101, val_acc:0.924]
Epoch [44/120    avg_loss:0.097, val_acc:0.932]
Epoch [45/120    avg_loss:0.095, val_acc:0.927]
Epoch [46/120    avg_loss:0.072, val_acc:0.945]
Epoch [47/120    avg_loss:0.098, val_acc:0.926]
Epoch [48/120    avg_loss:0.077, val_acc:0.950]
Epoch [49/120    avg_loss:0.117, val_acc:0.930]
Epoch [50/120    avg_loss:0.077, val_acc:0.941]
Epoch [51/120    avg_loss:0.079, val_acc:0.938]
Epoch [52/120    avg_loss:0.075, val_acc:0.949]
Epoch [53/120    avg_loss:0.081, val_acc:0.945]
Epoch [54/120    avg_loss:0.065, val_acc:0.953]
Epoch [55/120    avg_loss:0.065, val_acc:0.940]
Epoch [56/120    avg_loss:0.052, val_acc:0.952]
Epoch [57/120    avg_loss:0.086, val_acc:0.952]
Epoch [58/120    avg_loss:0.059, val_acc:0.956]
Epoch [59/120    avg_loss:0.052, val_acc:0.952]
Epoch [60/120    avg_loss:0.059, val_acc:0.947]
Epoch [61/120    avg_loss:0.056, val_acc:0.949]
Epoch [62/120    avg_loss:0.055, val_acc:0.958]
Epoch [63/120    avg_loss:0.046, val_acc:0.966]
Epoch [64/120    avg_loss:0.049, val_acc:0.953]
Epoch [65/120    avg_loss:0.048, val_acc:0.956]
Epoch [66/120    avg_loss:0.052, val_acc:0.960]
Epoch [67/120    avg_loss:0.040, val_acc:0.963]
Epoch [68/120    avg_loss:0.037, val_acc:0.966]
Epoch [69/120    avg_loss:0.039, val_acc:0.957]
Epoch [70/120    avg_loss:0.031, val_acc:0.956]
Epoch [71/120    avg_loss:0.039, val_acc:0.956]
Epoch [72/120    avg_loss:0.040, val_acc:0.964]
Epoch [73/120    avg_loss:0.032, val_acc:0.959]
Epoch [74/120    avg_loss:0.052, val_acc:0.955]
Epoch [75/120    avg_loss:0.045, val_acc:0.960]
Epoch [76/120    avg_loss:0.075, val_acc:0.939]
Epoch [77/120    avg_loss:0.060, val_acc:0.952]
Epoch [78/120    avg_loss:0.048, val_acc:0.957]
Epoch [79/120    avg_loss:0.040, val_acc:0.956]
Epoch [80/120    avg_loss:0.108, val_acc:0.950]
Epoch [81/120    avg_loss:0.064, val_acc:0.963]
Epoch [82/120    avg_loss:0.039, val_acc:0.966]
Epoch [83/120    avg_loss:0.027, val_acc:0.972]
Epoch [84/120    avg_loss:0.030, val_acc:0.973]
Epoch [85/120    avg_loss:0.025, val_acc:0.972]
Epoch [86/120    avg_loss:0.021, val_acc:0.972]
Epoch [87/120    avg_loss:0.026, val_acc:0.973]
Epoch [88/120    avg_loss:0.022, val_acc:0.972]
Epoch [89/120    avg_loss:0.026, val_acc:0.972]
Epoch [90/120    avg_loss:0.024, val_acc:0.969]
Epoch [91/120    avg_loss:0.020, val_acc:0.969]
Epoch [92/120    avg_loss:0.028, val_acc:0.972]
Epoch [93/120    avg_loss:0.025, val_acc:0.972]
Epoch [94/120    avg_loss:0.019, val_acc:0.972]
Epoch [95/120    avg_loss:0.024, val_acc:0.972]
Epoch [96/120    avg_loss:0.024, val_acc:0.970]
Epoch [97/120    avg_loss:0.021, val_acc:0.973]
Epoch [98/120    avg_loss:0.017, val_acc:0.972]
Epoch [99/120    avg_loss:0.022, val_acc:0.972]
Epoch [100/120    avg_loss:0.019, val_acc:0.973]
Epoch [101/120    avg_loss:0.021, val_acc:0.972]
Epoch [102/120    avg_loss:0.019, val_acc:0.972]
Epoch [103/120    avg_loss:0.018, val_acc:0.972]
Epoch [104/120    avg_loss:0.018, val_acc:0.973]
Epoch [105/120    avg_loss:0.018, val_acc:0.973]
Epoch [106/120    avg_loss:0.023, val_acc:0.972]
Epoch [107/120    avg_loss:0.020, val_acc:0.970]
Epoch [108/120    avg_loss:0.019, val_acc:0.972]
Epoch [109/120    avg_loss:0.016, val_acc:0.970]
Epoch [110/120    avg_loss:0.017, val_acc:0.969]
Epoch [111/120    avg_loss:0.018, val_acc:0.972]
Epoch [112/120    avg_loss:0.017, val_acc:0.972]
Epoch [113/120    avg_loss:0.029, val_acc:0.973]
Epoch [114/120    avg_loss:0.021, val_acc:0.972]
Epoch [115/120    avg_loss:0.018, val_acc:0.968]
Epoch [116/120    avg_loss:0.018, val_acc:0.973]
Epoch [117/120    avg_loss:0.022, val_acc:0.973]
Epoch [118/120    avg_loss:0.020, val_acc:0.972]
Epoch [119/120    avg_loss:0.021, val_acc:0.974]
Epoch [120/120    avg_loss:0.018, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1250    1    0    0    1    0    0    0    9    9    4    0
     0   11    0]
 [   0    0    1  718    1    8    0    0    0    8    0    0    2    1
     0    8    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    1    0    4    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   14    0    0    4    0
     0    0    0]
 [   0    0   19   90    0    4    0    0    0    0  751    1    0    0
     2    8    0]
 [   0    0   16    0    0    3   13    0    2    0   10 2153    7    3
     2    1    0]
 [   0    0    1   18    7    2    0    0    0    0   14    0  487    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    3    0    0    0
  1132    0    0]
 [   0    0    0    0    0    0   30    0    0    0    0    0    0    0
    83  234    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.36043360433604

F1 scores:
[       nan 0.98765432 0.97162845 0.91232529 0.98156682 0.96708286
 0.96683861 0.98039216 0.99652375 0.63636364 0.90373045 0.98445359
 0.93743985 0.98930481 0.95850974 0.76847291 0.96511628]

Kappa:
0.9471239389533066
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fec9e5bd6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.617, val_acc:0.392]
Epoch [2/120    avg_loss:2.113, val_acc:0.522]
Epoch [3/120    avg_loss:1.918, val_acc:0.532]
Epoch [4/120    avg_loss:1.700, val_acc:0.630]
Epoch [5/120    avg_loss:1.521, val_acc:0.616]
Epoch [6/120    avg_loss:1.285, val_acc:0.669]
Epoch [7/120    avg_loss:1.109, val_acc:0.641]
Epoch [8/120    avg_loss:0.994, val_acc:0.742]
Epoch [9/120    avg_loss:0.875, val_acc:0.760]
Epoch [10/120    avg_loss:0.718, val_acc:0.797]
Epoch [11/120    avg_loss:0.644, val_acc:0.751]
Epoch [12/120    avg_loss:0.558, val_acc:0.818]
Epoch [13/120    avg_loss:0.634, val_acc:0.789]
Epoch [14/120    avg_loss:0.543, val_acc:0.797]
Epoch [15/120    avg_loss:0.449, val_acc:0.818]
Epoch [16/120    avg_loss:0.375, val_acc:0.881]
Epoch [17/120    avg_loss:0.385, val_acc:0.886]
Epoch [18/120    avg_loss:0.310, val_acc:0.887]
Epoch [19/120    avg_loss:0.322, val_acc:0.866]
Epoch [20/120    avg_loss:0.417, val_acc:0.831]
Epoch [21/120    avg_loss:0.346, val_acc:0.901]
Epoch [22/120    avg_loss:0.266, val_acc:0.890]
Epoch [23/120    avg_loss:0.222, val_acc:0.919]
Epoch [24/120    avg_loss:0.223, val_acc:0.919]
Epoch [25/120    avg_loss:0.256, val_acc:0.912]
Epoch [26/120    avg_loss:0.236, val_acc:0.914]
Epoch [27/120    avg_loss:0.236, val_acc:0.917]
Epoch [28/120    avg_loss:0.217, val_acc:0.923]
Epoch [29/120    avg_loss:0.239, val_acc:0.931]
Epoch [30/120    avg_loss:0.183, val_acc:0.918]
Epoch [31/120    avg_loss:0.176, val_acc:0.894]
Epoch [32/120    avg_loss:0.184, val_acc:0.918]
Epoch [33/120    avg_loss:0.182, val_acc:0.936]
Epoch [34/120    avg_loss:0.160, val_acc:0.939]
Epoch [35/120    avg_loss:0.139, val_acc:0.918]
Epoch [36/120    avg_loss:0.157, val_acc:0.953]
Epoch [37/120    avg_loss:0.148, val_acc:0.931]
Epoch [38/120    avg_loss:0.121, val_acc:0.939]
Epoch [39/120    avg_loss:0.150, val_acc:0.940]
Epoch [40/120    avg_loss:0.127, val_acc:0.951]
Epoch [41/120    avg_loss:0.106, val_acc:0.955]
Epoch [42/120    avg_loss:0.091, val_acc:0.956]
Epoch [43/120    avg_loss:0.073, val_acc:0.958]
Epoch [44/120    avg_loss:0.092, val_acc:0.959]
Epoch [45/120    avg_loss:0.095, val_acc:0.948]
Epoch [46/120    avg_loss:0.097, val_acc:0.945]
Epoch [47/120    avg_loss:0.136, val_acc:0.958]
Epoch [48/120    avg_loss:0.107, val_acc:0.955]
Epoch [49/120    avg_loss:0.065, val_acc:0.961]
Epoch [50/120    avg_loss:0.070, val_acc:0.959]
Epoch [51/120    avg_loss:0.051, val_acc:0.966]
Epoch [52/120    avg_loss:0.062, val_acc:0.958]
Epoch [53/120    avg_loss:0.072, val_acc:0.964]
Epoch [54/120    avg_loss:0.059, val_acc:0.961]
Epoch [55/120    avg_loss:0.056, val_acc:0.966]
Epoch [56/120    avg_loss:0.060, val_acc:0.963]
Epoch [57/120    avg_loss:0.053, val_acc:0.957]
Epoch [58/120    avg_loss:0.060, val_acc:0.961]
Epoch [59/120    avg_loss:0.060, val_acc:0.956]
Epoch [60/120    avg_loss:0.067, val_acc:0.950]
Epoch [61/120    avg_loss:0.045, val_acc:0.961]
Epoch [62/120    avg_loss:0.044, val_acc:0.957]
Epoch [63/120    avg_loss:0.034, val_acc:0.973]
Epoch [64/120    avg_loss:0.032, val_acc:0.972]
Epoch [65/120    avg_loss:0.038, val_acc:0.964]
Epoch [66/120    avg_loss:0.037, val_acc:0.966]
Epoch [67/120    avg_loss:0.040, val_acc:0.975]
Epoch [68/120    avg_loss:0.044, val_acc:0.972]
Epoch [69/120    avg_loss:0.048, val_acc:0.966]
Epoch [70/120    avg_loss:0.034, val_acc:0.968]
Epoch [71/120    avg_loss:0.037, val_acc:0.972]
Epoch [72/120    avg_loss:0.039, val_acc:0.973]
Epoch [73/120    avg_loss:0.047, val_acc:0.972]
Epoch [74/120    avg_loss:0.044, val_acc:0.967]
Epoch [75/120    avg_loss:0.032, val_acc:0.975]
Epoch [76/120    avg_loss:0.027, val_acc:0.964]
Epoch [77/120    avg_loss:0.039, val_acc:0.974]
Epoch [78/120    avg_loss:0.030, val_acc:0.968]
Epoch [79/120    avg_loss:0.031, val_acc:0.975]
Epoch [80/120    avg_loss:0.024, val_acc:0.964]
Epoch [81/120    avg_loss:0.024, val_acc:0.976]
Epoch [82/120    avg_loss:0.027, val_acc:0.976]
Epoch [83/120    avg_loss:0.025, val_acc:0.978]
Epoch [84/120    avg_loss:0.020, val_acc:0.973]
Epoch [85/120    avg_loss:0.025, val_acc:0.972]
Epoch [86/120    avg_loss:0.033, val_acc:0.966]
Epoch [87/120    avg_loss:0.026, val_acc:0.967]
Epoch [88/120    avg_loss:0.048, val_acc:0.963]
Epoch [89/120    avg_loss:0.038, val_acc:0.975]
Epoch [90/120    avg_loss:0.038, val_acc:0.958]
Epoch [91/120    avg_loss:0.038, val_acc:0.959]
Epoch [92/120    avg_loss:0.039, val_acc:0.964]
Epoch [93/120    avg_loss:0.037, val_acc:0.968]
Epoch [94/120    avg_loss:0.095, val_acc:0.975]
Epoch [95/120    avg_loss:0.037, val_acc:0.970]
Epoch [96/120    avg_loss:0.046, val_acc:0.963]
Epoch [97/120    avg_loss:0.032, val_acc:0.968]
Epoch [98/120    avg_loss:0.032, val_acc:0.976]
Epoch [99/120    avg_loss:0.017, val_acc:0.978]
Epoch [100/120    avg_loss:0.023, val_acc:0.978]
Epoch [101/120    avg_loss:0.024, val_acc:0.980]
Epoch [102/120    avg_loss:0.017, val_acc:0.977]
Epoch [103/120    avg_loss:0.021, val_acc:0.977]
Epoch [104/120    avg_loss:0.017, val_acc:0.977]
Epoch [105/120    avg_loss:0.015, val_acc:0.980]
Epoch [106/120    avg_loss:0.019, val_acc:0.978]
Epoch [107/120    avg_loss:0.014, val_acc:0.980]
Epoch [108/120    avg_loss:0.020, val_acc:0.980]
Epoch [109/120    avg_loss:0.015, val_acc:0.980]
Epoch [110/120    avg_loss:0.017, val_acc:0.978]
Epoch [111/120    avg_loss:0.014, val_acc:0.977]
Epoch [112/120    avg_loss:0.018, val_acc:0.980]
Epoch [113/120    avg_loss:0.016, val_acc:0.978]
Epoch [114/120    avg_loss:0.013, val_acc:0.976]
Epoch [115/120    avg_loss:0.018, val_acc:0.977]
Epoch [116/120    avg_loss:0.015, val_acc:0.978]
Epoch [117/120    avg_loss:0.012, val_acc:0.981]
Epoch [118/120    avg_loss:0.014, val_acc:0.978]
Epoch [119/120    avg_loss:0.016, val_acc:0.976]
Epoch [120/120    avg_loss:0.016, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1245    1    0    0    1    0    0    0   15   22    0    0
     0    1    0]
 [   0    0    1  713    0   11    0    0    0    9    0    0    8    5
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    4    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   13    0    0    2    0
     0    0    0]
 [   0    0   12   90    0    8    0    0    0    0  760    0    0    0
     1    4    0]
 [   0    0   10    0    0    0    9    0    0    0    9 2119   54    1
     8    0    0]
 [   0    0    1   20    6   15    0    0    0    0    0    0  481    0
     0    0   11]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    1    0    1    2    0    0
  1131    0    0]
 [   0    0    0    0    0    0   15    0    0    3    0    0    0    0
    87  242    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.9810298102981

F1 scores:
[       nan 0.975      0.97494127 0.90597205 0.98611111 0.95121951
 0.97904192 1.         0.99883856 0.55319149 0.91456077 0.97291093
 0.89074074 0.98404255 0.95523649 0.81481481 0.93258427]

Kappa:
0.9428349924916284
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f09516ec780>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.558, val_acc:0.433]
Epoch [2/120    avg_loss:2.114, val_acc:0.503]
Epoch [3/120    avg_loss:1.872, val_acc:0.511]
Epoch [4/120    avg_loss:1.683, val_acc:0.585]
Epoch [5/120    avg_loss:1.577, val_acc:0.616]
Epoch [6/120    avg_loss:1.349, val_acc:0.640]
Epoch [7/120    avg_loss:1.286, val_acc:0.660]
Epoch [8/120    avg_loss:1.060, val_acc:0.677]
Epoch [9/120    avg_loss:0.911, val_acc:0.739]
Epoch [10/120    avg_loss:0.831, val_acc:0.719]
Epoch [11/120    avg_loss:0.790, val_acc:0.698]
Epoch [12/120    avg_loss:0.772, val_acc:0.802]
Epoch [13/120    avg_loss:0.643, val_acc:0.835]
Epoch [14/120    avg_loss:0.504, val_acc:0.869]
Epoch [15/120    avg_loss:0.456, val_acc:0.841]
Epoch [16/120    avg_loss:0.442, val_acc:0.848]
Epoch [17/120    avg_loss:0.467, val_acc:0.820]
Epoch [18/120    avg_loss:0.353, val_acc:0.855]
Epoch [19/120    avg_loss:0.341, val_acc:0.875]
Epoch [20/120    avg_loss:0.292, val_acc:0.891]
Epoch [21/120    avg_loss:0.347, val_acc:0.893]
Epoch [22/120    avg_loss:0.274, val_acc:0.878]
Epoch [23/120    avg_loss:0.300, val_acc:0.900]
Epoch [24/120    avg_loss:0.277, val_acc:0.907]
Epoch [25/120    avg_loss:0.209, val_acc:0.918]
Epoch [26/120    avg_loss:0.216, val_acc:0.915]
Epoch [27/120    avg_loss:0.195, val_acc:0.899]
Epoch [28/120    avg_loss:0.171, val_acc:0.918]
Epoch [29/120    avg_loss:0.197, val_acc:0.915]
Epoch [30/120    avg_loss:0.177, val_acc:0.928]
Epoch [31/120    avg_loss:0.170, val_acc:0.926]
Epoch [32/120    avg_loss:0.166, val_acc:0.906]
Epoch [33/120    avg_loss:0.130, val_acc:0.950]
Epoch [34/120    avg_loss:0.138, val_acc:0.926]
Epoch [35/120    avg_loss:0.147, val_acc:0.934]
Epoch [36/120    avg_loss:0.134, val_acc:0.941]
Epoch [37/120    avg_loss:0.140, val_acc:0.939]
Epoch [38/120    avg_loss:0.118, val_acc:0.936]
Epoch [39/120    avg_loss:0.105, val_acc:0.941]
Epoch [40/120    avg_loss:0.097, val_acc:0.953]
Epoch [41/120    avg_loss:0.104, val_acc:0.941]
Epoch [42/120    avg_loss:0.116, val_acc:0.935]
Epoch [43/120    avg_loss:0.153, val_acc:0.932]
Epoch [44/120    avg_loss:0.129, val_acc:0.945]
Epoch [45/120    avg_loss:0.107, val_acc:0.934]
Epoch [46/120    avg_loss:0.084, val_acc:0.957]
Epoch [47/120    avg_loss:0.069, val_acc:0.966]
Epoch [48/120    avg_loss:0.067, val_acc:0.956]
Epoch [49/120    avg_loss:0.075, val_acc:0.951]
Epoch [50/120    avg_loss:0.061, val_acc:0.944]
Epoch [51/120    avg_loss:0.062, val_acc:0.956]
Epoch [52/120    avg_loss:0.074, val_acc:0.941]
Epoch [53/120    avg_loss:0.088, val_acc:0.936]
Epoch [54/120    avg_loss:0.086, val_acc:0.955]
Epoch [55/120    avg_loss:0.062, val_acc:0.953]
Epoch [56/120    avg_loss:0.051, val_acc:0.960]
Epoch [57/120    avg_loss:0.043, val_acc:0.965]
Epoch [58/120    avg_loss:0.053, val_acc:0.958]
Epoch [59/120    avg_loss:0.071, val_acc:0.952]
Epoch [60/120    avg_loss:0.070, val_acc:0.957]
Epoch [61/120    avg_loss:0.056, val_acc:0.961]
Epoch [62/120    avg_loss:0.043, val_acc:0.961]
Epoch [63/120    avg_loss:0.039, val_acc:0.961]
Epoch [64/120    avg_loss:0.038, val_acc:0.960]
Epoch [65/120    avg_loss:0.029, val_acc:0.963]
Epoch [66/120    avg_loss:0.035, val_acc:0.961]
Epoch [67/120    avg_loss:0.030, val_acc:0.965]
Epoch [68/120    avg_loss:0.034, val_acc:0.966]
Epoch [69/120    avg_loss:0.036, val_acc:0.966]
Epoch [70/120    avg_loss:0.028, val_acc:0.964]
Epoch [71/120    avg_loss:0.030, val_acc:0.966]
Epoch [72/120    avg_loss:0.031, val_acc:0.968]
Epoch [73/120    avg_loss:0.032, val_acc:0.968]
Epoch [74/120    avg_loss:0.029, val_acc:0.967]
Epoch [75/120    avg_loss:0.032, val_acc:0.968]
Epoch [76/120    avg_loss:0.028, val_acc:0.969]
Epoch [77/120    avg_loss:0.031, val_acc:0.969]
Epoch [78/120    avg_loss:0.032, val_acc:0.968]
Epoch [79/120    avg_loss:0.023, val_acc:0.970]
Epoch [80/120    avg_loss:0.031, val_acc:0.969]
Epoch [81/120    avg_loss:0.026, val_acc:0.969]
Epoch [82/120    avg_loss:0.028, val_acc:0.970]
Epoch [83/120    avg_loss:0.026, val_acc:0.970]
Epoch [84/120    avg_loss:0.027, val_acc:0.968]
Epoch [85/120    avg_loss:0.027, val_acc:0.969]
Epoch [86/120    avg_loss:0.027, val_acc:0.969]
Epoch [87/120    avg_loss:0.027, val_acc:0.968]
Epoch [88/120    avg_loss:0.027, val_acc:0.968]
Epoch [89/120    avg_loss:0.029, val_acc:0.968]
Epoch [90/120    avg_loss:0.029, val_acc:0.968]
Epoch [91/120    avg_loss:0.026, val_acc:0.969]
Epoch [92/120    avg_loss:0.025, val_acc:0.969]
Epoch [93/120    avg_loss:0.030, val_acc:0.968]
Epoch [94/120    avg_loss:0.028, val_acc:0.969]
Epoch [95/120    avg_loss:0.026, val_acc:0.967]
Epoch [96/120    avg_loss:0.028, val_acc:0.967]
Epoch [97/120    avg_loss:0.029, val_acc:0.967]
Epoch [98/120    avg_loss:0.021, val_acc:0.967]
Epoch [99/120    avg_loss:0.026, val_acc:0.967]
Epoch [100/120    avg_loss:0.025, val_acc:0.967]
Epoch [101/120    avg_loss:0.026, val_acc:0.967]
Epoch [102/120    avg_loss:0.019, val_acc:0.967]
Epoch [103/120    avg_loss:0.027, val_acc:0.967]
Epoch [104/120    avg_loss:0.023, val_acc:0.967]
Epoch [105/120    avg_loss:0.021, val_acc:0.967]
Epoch [106/120    avg_loss:0.027, val_acc:0.968]
Epoch [107/120    avg_loss:0.023, val_acc:0.968]
Epoch [108/120    avg_loss:0.027, val_acc:0.967]
Epoch [109/120    avg_loss:0.022, val_acc:0.968]
Epoch [110/120    avg_loss:0.025, val_acc:0.968]
Epoch [111/120    avg_loss:0.029, val_acc:0.968]
Epoch [112/120    avg_loss:0.025, val_acc:0.968]
Epoch [113/120    avg_loss:0.028, val_acc:0.968]
Epoch [114/120    avg_loss:0.023, val_acc:0.968]
Epoch [115/120    avg_loss:0.021, val_acc:0.968]
Epoch [116/120    avg_loss:0.026, val_acc:0.968]
Epoch [117/120    avg_loss:0.028, val_acc:0.968]
Epoch [118/120    avg_loss:0.023, val_acc:0.968]
Epoch [119/120    avg_loss:0.023, val_acc:0.968]
Epoch [120/120    avg_loss:0.027, val_acc:0.968]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1230    2    0    0    8    0    0    2    9   16    2    0
     0   16    0]
 [   0    0    1  705    0    7    0    0    0   19    0    0   15    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  417    0    8    0    6    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   14    0    0    3    0
     0    0    0]
 [   0    0   27   89    0    4    1    0    0    0  743    4    0    0
     0    7    0]
 [   0    0   15    0    0    0   12    0    1    0    9 2170    0    3
     0    0    0]
 [   0    0    0   16    5    4    0    0    0    0   10    7  481    0
     0    2    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    1    0    0
  1134    0    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    97  231    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
94.9159891598916

F1 scores:
[       nan 1.         0.96168882 0.90384615 0.98839907 0.96193772
 0.96817172 0.86206897 0.99767981 0.47457627 0.90115221 0.98390388
 0.92857143 0.9919571  0.95534962 0.76616915 0.94318182]

Kappa:
0.9420275125531518
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe3d352e780>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.581, val_acc:0.385]
Epoch [2/120    avg_loss:2.143, val_acc:0.530]
Epoch [3/120    avg_loss:1.927, val_acc:0.589]
Epoch [4/120    avg_loss:1.773, val_acc:0.576]
Epoch [5/120    avg_loss:1.556, val_acc:0.625]
Epoch [6/120    avg_loss:1.359, val_acc:0.683]
Epoch [7/120    avg_loss:1.225, val_acc:0.645]
Epoch [8/120    avg_loss:0.968, val_acc:0.761]
Epoch [9/120    avg_loss:0.856, val_acc:0.745]
Epoch [10/120    avg_loss:0.748, val_acc:0.791]
Epoch [11/120    avg_loss:0.688, val_acc:0.800]
Epoch [12/120    avg_loss:0.660, val_acc:0.793]
Epoch [13/120    avg_loss:0.557, val_acc:0.799]
Epoch [14/120    avg_loss:0.521, val_acc:0.798]
Epoch [15/120    avg_loss:0.536, val_acc:0.809]
Epoch [16/120    avg_loss:0.424, val_acc:0.867]
Epoch [17/120    avg_loss:0.433, val_acc:0.835]
Epoch [18/120    avg_loss:0.434, val_acc:0.826]
Epoch [19/120    avg_loss:0.412, val_acc:0.874]
Epoch [20/120    avg_loss:0.305, val_acc:0.865]
Epoch [21/120    avg_loss:0.274, val_acc:0.874]
Epoch [22/120    avg_loss:0.285, val_acc:0.883]
Epoch [23/120    avg_loss:0.241, val_acc:0.891]
Epoch [24/120    avg_loss:0.213, val_acc:0.908]
Epoch [25/120    avg_loss:0.213, val_acc:0.901]
Epoch [26/120    avg_loss:0.248, val_acc:0.856]
Epoch [27/120    avg_loss:0.244, val_acc:0.900]
Epoch [28/120    avg_loss:0.217, val_acc:0.897]
Epoch [29/120    avg_loss:0.195, val_acc:0.917]
Epoch [30/120    avg_loss:0.162, val_acc:0.910]
Epoch [31/120    avg_loss:0.165, val_acc:0.916]
Epoch [32/120    avg_loss:0.196, val_acc:0.877]
Epoch [33/120    avg_loss:0.165, val_acc:0.927]
Epoch [34/120    avg_loss:0.139, val_acc:0.936]
Epoch [35/120    avg_loss:0.140, val_acc:0.908]
Epoch [36/120    avg_loss:0.150, val_acc:0.925]
Epoch [37/120    avg_loss:0.145, val_acc:0.931]
Epoch [38/120    avg_loss:0.158, val_acc:0.934]
Epoch [39/120    avg_loss:0.122, val_acc:0.934]
Epoch [40/120    avg_loss:0.101, val_acc:0.949]
Epoch [41/120    avg_loss:0.089, val_acc:0.951]
Epoch [42/120    avg_loss:0.092, val_acc:0.957]
Epoch [43/120    avg_loss:0.105, val_acc:0.950]
Epoch [44/120    avg_loss:0.085, val_acc:0.953]
Epoch [45/120    avg_loss:0.074, val_acc:0.944]
Epoch [46/120    avg_loss:0.082, val_acc:0.952]
Epoch [47/120    avg_loss:0.085, val_acc:0.923]
Epoch [48/120    avg_loss:0.095, val_acc:0.938]
Epoch [49/120    avg_loss:0.092, val_acc:0.948]
Epoch [50/120    avg_loss:0.091, val_acc:0.956]
Epoch [51/120    avg_loss:0.072, val_acc:0.945]
Epoch [52/120    avg_loss:0.109, val_acc:0.917]
Epoch [53/120    avg_loss:0.099, val_acc:0.935]
Epoch [54/120    avg_loss:0.076, val_acc:0.951]
Epoch [55/120    avg_loss:0.091, val_acc:0.953]
Epoch [56/120    avg_loss:0.067, val_acc:0.957]
Epoch [57/120    avg_loss:0.049, val_acc:0.957]
Epoch [58/120    avg_loss:0.045, val_acc:0.961]
Epoch [59/120    avg_loss:0.046, val_acc:0.964]
Epoch [60/120    avg_loss:0.050, val_acc:0.963]
Epoch [61/120    avg_loss:0.041, val_acc:0.964]
Epoch [62/120    avg_loss:0.039, val_acc:0.965]
Epoch [63/120    avg_loss:0.042, val_acc:0.964]
Epoch [64/120    avg_loss:0.041, val_acc:0.963]
Epoch [65/120    avg_loss:0.041, val_acc:0.964]
Epoch [66/120    avg_loss:0.040, val_acc:0.964]
Epoch [67/120    avg_loss:0.045, val_acc:0.963]
Epoch [68/120    avg_loss:0.039, val_acc:0.964]
Epoch [69/120    avg_loss:0.041, val_acc:0.967]
Epoch [70/120    avg_loss:0.040, val_acc:0.964]
Epoch [71/120    avg_loss:0.037, val_acc:0.965]
Epoch [72/120    avg_loss:0.040, val_acc:0.964]
Epoch [73/120    avg_loss:0.034, val_acc:0.963]
Epoch [74/120    avg_loss:0.036, val_acc:0.965]
Epoch [75/120    avg_loss:0.033, val_acc:0.965]
Epoch [76/120    avg_loss:0.038, val_acc:0.965]
Epoch [77/120    avg_loss:0.034, val_acc:0.965]
Epoch [78/120    avg_loss:0.033, val_acc:0.966]
Epoch [79/120    avg_loss:0.039, val_acc:0.967]
Epoch [80/120    avg_loss:0.038, val_acc:0.966]
Epoch [81/120    avg_loss:0.028, val_acc:0.967]
Epoch [82/120    avg_loss:0.040, val_acc:0.964]
Epoch [83/120    avg_loss:0.035, val_acc:0.966]
Epoch [84/120    avg_loss:0.034, val_acc:0.968]
Epoch [85/120    avg_loss:0.035, val_acc:0.968]
Epoch [86/120    avg_loss:0.032, val_acc:0.967]
Epoch [87/120    avg_loss:0.032, val_acc:0.968]
Epoch [88/120    avg_loss:0.031, val_acc:0.967]
Epoch [89/120    avg_loss:0.034, val_acc:0.965]
Epoch [90/120    avg_loss:0.027, val_acc:0.967]
Epoch [91/120    avg_loss:0.031, val_acc:0.966]
Epoch [92/120    avg_loss:0.037, val_acc:0.966]
Epoch [93/120    avg_loss:0.036, val_acc:0.966]
Epoch [94/120    avg_loss:0.031, val_acc:0.967]
Epoch [95/120    avg_loss:0.029, val_acc:0.966]
Epoch [96/120    avg_loss:0.031, val_acc:0.965]
Epoch [97/120    avg_loss:0.028, val_acc:0.964]
Epoch [98/120    avg_loss:0.031, val_acc:0.966]
Epoch [99/120    avg_loss:0.028, val_acc:0.967]
Epoch [100/120    avg_loss:0.030, val_acc:0.966]
Epoch [101/120    avg_loss:0.034, val_acc:0.967]
Epoch [102/120    avg_loss:0.031, val_acc:0.967]
Epoch [103/120    avg_loss:0.028, val_acc:0.967]
Epoch [104/120    avg_loss:0.030, val_acc:0.966]
Epoch [105/120    avg_loss:0.028, val_acc:0.966]
Epoch [106/120    avg_loss:0.028, val_acc:0.967]
Epoch [107/120    avg_loss:0.032, val_acc:0.966]
Epoch [108/120    avg_loss:0.032, val_acc:0.966]
Epoch [109/120    avg_loss:0.031, val_acc:0.967]
Epoch [110/120    avg_loss:0.032, val_acc:0.967]
Epoch [111/120    avg_loss:0.032, val_acc:0.967]
Epoch [112/120    avg_loss:0.031, val_acc:0.967]
Epoch [113/120    avg_loss:0.033, val_acc:0.967]
Epoch [114/120    avg_loss:0.039, val_acc:0.967]
Epoch [115/120    avg_loss:0.031, val_acc:0.967]
Epoch [116/120    avg_loss:0.039, val_acc:0.967]
Epoch [117/120    avg_loss:0.030, val_acc:0.967]
Epoch [118/120    avg_loss:0.027, val_acc:0.967]
Epoch [119/120    avg_loss:0.031, val_acc:0.967]
Epoch [120/120    avg_loss:0.030, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1238    1    0    0    6    0    0    0    6   33    1    0
     0    0    0]
 [   0    0    2  724    1    6    0    0    0    4    0    0   10    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  419    0    1    0    5    0    0    0    0
    10    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   13    0    0    2    0
     0    0    0]
 [   0    0   37   89    0    5    1    0    0    0  733    3    0    0
     0    7    0]
 [   0    0   18    0    0    3    7    0    4    0   10 2160    0    3
     5    0    0]
 [   0    0    0   33    8   12    0    0    0    0   11    3  457    0
     0    0   10]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    0    0    0    0    4    4    0    0
  1122    0    0]
 [   0    0    0    0    0    0   41    0    0    1    0    0    0    0
   101  204    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
94.3089430894309

F1 scores:
[       nan 0.975      0.95968992 0.90670006 0.97931034 0.94263217
 0.95754026 0.98039216 0.99537037 0.63414634 0.89335771 0.97826087
 0.91035857 0.9919571  0.94404712 0.7311828  0.94382022]

Kappa:
0.9350607017932049
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbf6e997748>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.597, val_acc:0.483]
Epoch [2/120    avg_loss:2.111, val_acc:0.539]
Epoch [3/120    avg_loss:1.818, val_acc:0.602]
Epoch [4/120    avg_loss:1.636, val_acc:0.634]
Epoch [5/120    avg_loss:1.483, val_acc:0.657]
Epoch [6/120    avg_loss:1.346, val_acc:0.644]
Epoch [7/120    avg_loss:1.117, val_acc:0.674]
Epoch [8/120    avg_loss:0.933, val_acc:0.710]
Epoch [9/120    avg_loss:0.806, val_acc:0.745]
Epoch [10/120    avg_loss:0.667, val_acc:0.747]
Epoch [11/120    avg_loss:0.649, val_acc:0.730]
Epoch [12/120    avg_loss:0.610, val_acc:0.793]
Epoch [13/120    avg_loss:0.543, val_acc:0.785]
Epoch [14/120    avg_loss:0.532, val_acc:0.810]
Epoch [15/120    avg_loss:0.434, val_acc:0.830]
Epoch [16/120    avg_loss:0.377, val_acc:0.831]
Epoch [17/120    avg_loss:0.448, val_acc:0.833]
Epoch [18/120    avg_loss:0.395, val_acc:0.850]
Epoch [19/120    avg_loss:0.336, val_acc:0.861]
Epoch [20/120    avg_loss:0.308, val_acc:0.873]
Epoch [21/120    avg_loss:0.268, val_acc:0.898]
Epoch [22/120    avg_loss:0.260, val_acc:0.903]
Epoch [23/120    avg_loss:0.233, val_acc:0.882]
Epoch [24/120    avg_loss:0.214, val_acc:0.892]
Epoch [25/120    avg_loss:0.232, val_acc:0.886]
Epoch [26/120    avg_loss:0.205, val_acc:0.897]
Epoch [27/120    avg_loss:0.239, val_acc:0.906]
Epoch [28/120    avg_loss:0.202, val_acc:0.894]
Epoch [29/120    avg_loss:0.213, val_acc:0.911]
Epoch [30/120    avg_loss:0.189, val_acc:0.897]
Epoch [31/120    avg_loss:0.173, val_acc:0.926]
Epoch [32/120    avg_loss:0.154, val_acc:0.927]
Epoch [33/120    avg_loss:0.135, val_acc:0.931]
Epoch [34/120    avg_loss:0.127, val_acc:0.918]
Epoch [35/120    avg_loss:0.123, val_acc:0.932]
Epoch [36/120    avg_loss:0.150, val_acc:0.915]
Epoch [37/120    avg_loss:0.146, val_acc:0.943]
Epoch [38/120    avg_loss:0.107, val_acc:0.933]
Epoch [39/120    avg_loss:0.100, val_acc:0.940]
Epoch [40/120    avg_loss:0.135, val_acc:0.922]
Epoch [41/120    avg_loss:0.112, val_acc:0.925]
Epoch [42/120    avg_loss:0.104, val_acc:0.945]
Epoch [43/120    avg_loss:0.095, val_acc:0.932]
Epoch [44/120    avg_loss:0.097, val_acc:0.941]
Epoch [45/120    avg_loss:0.106, val_acc:0.944]
Epoch [46/120    avg_loss:0.128, val_acc:0.908]
Epoch [47/120    avg_loss:0.135, val_acc:0.936]
Epoch [48/120    avg_loss:0.128, val_acc:0.942]
Epoch [49/120    avg_loss:0.096, val_acc:0.952]
Epoch [50/120    avg_loss:0.069, val_acc:0.948]
Epoch [51/120    avg_loss:0.075, val_acc:0.949]
Epoch [52/120    avg_loss:0.064, val_acc:0.950]
Epoch [53/120    avg_loss:0.066, val_acc:0.949]
Epoch [54/120    avg_loss:0.054, val_acc:0.950]
Epoch [55/120    avg_loss:0.056, val_acc:0.952]
Epoch [56/120    avg_loss:0.056, val_acc:0.952]
Epoch [57/120    avg_loss:0.048, val_acc:0.955]
Epoch [58/120    avg_loss:0.047, val_acc:0.957]
Epoch [59/120    avg_loss:0.074, val_acc:0.944]
Epoch [60/120    avg_loss:0.054, val_acc:0.951]
Epoch [61/120    avg_loss:0.053, val_acc:0.948]
Epoch [62/120    avg_loss:0.058, val_acc:0.943]
Epoch [63/120    avg_loss:0.046, val_acc:0.952]
Epoch [64/120    avg_loss:0.048, val_acc:0.952]
Epoch [65/120    avg_loss:0.038, val_acc:0.958]
Epoch [66/120    avg_loss:0.041, val_acc:0.957]
Epoch [67/120    avg_loss:0.043, val_acc:0.964]
Epoch [68/120    avg_loss:0.044, val_acc:0.968]
Epoch [69/120    avg_loss:0.044, val_acc:0.957]
Epoch [70/120    avg_loss:0.038, val_acc:0.966]
Epoch [71/120    avg_loss:0.036, val_acc:0.960]
Epoch [72/120    avg_loss:0.040, val_acc:0.958]
Epoch [73/120    avg_loss:0.032, val_acc:0.955]
Epoch [74/120    avg_loss:0.036, val_acc:0.952]
Epoch [75/120    avg_loss:0.039, val_acc:0.968]
Epoch [76/120    avg_loss:0.037, val_acc:0.952]
Epoch [77/120    avg_loss:0.031, val_acc:0.965]
Epoch [78/120    avg_loss:0.039, val_acc:0.957]
Epoch [79/120    avg_loss:0.038, val_acc:0.968]
Epoch [80/120    avg_loss:0.030, val_acc:0.970]
Epoch [81/120    avg_loss:0.034, val_acc:0.972]
Epoch [82/120    avg_loss:0.033, val_acc:0.951]
Epoch [83/120    avg_loss:0.029, val_acc:0.968]
Epoch [84/120    avg_loss:0.032, val_acc:0.959]
Epoch [85/120    avg_loss:0.033, val_acc:0.953]
Epoch [86/120    avg_loss:0.027, val_acc:0.963]
Epoch [87/120    avg_loss:0.028, val_acc:0.964]
Epoch [88/120    avg_loss:0.029, val_acc:0.966]
Epoch [89/120    avg_loss:0.038, val_acc:0.952]
Epoch [90/120    avg_loss:0.030, val_acc:0.970]
Epoch [91/120    avg_loss:0.024, val_acc:0.967]
Epoch [92/120    avg_loss:0.021, val_acc:0.969]
Epoch [93/120    avg_loss:0.017, val_acc:0.966]
Epoch [94/120    avg_loss:0.019, val_acc:0.973]
Epoch [95/120    avg_loss:0.025, val_acc:0.967]
Epoch [96/120    avg_loss:0.016, val_acc:0.966]
Epoch [97/120    avg_loss:0.027, val_acc:0.960]
Epoch [98/120    avg_loss:0.023, val_acc:0.968]
Epoch [99/120    avg_loss:0.022, val_acc:0.973]
Epoch [100/120    avg_loss:0.024, val_acc:0.973]
Epoch [101/120    avg_loss:0.021, val_acc:0.975]
Epoch [102/120    avg_loss:0.029, val_acc:0.957]
Epoch [103/120    avg_loss:0.034, val_acc:0.960]
Epoch [104/120    avg_loss:0.032, val_acc:0.975]
Epoch [105/120    avg_loss:0.017, val_acc:0.969]
Epoch [106/120    avg_loss:0.029, val_acc:0.969]
Epoch [107/120    avg_loss:0.019, val_acc:0.970]
Epoch [108/120    avg_loss:0.017, val_acc:0.972]
Epoch [109/120    avg_loss:0.050, val_acc:0.967]
Epoch [110/120    avg_loss:0.030, val_acc:0.955]
Epoch [111/120    avg_loss:0.036, val_acc:0.975]
Epoch [112/120    avg_loss:0.035, val_acc:0.965]
Epoch [113/120    avg_loss:0.025, val_acc:0.973]
Epoch [114/120    avg_loss:0.019, val_acc:0.976]
Epoch [115/120    avg_loss:0.019, val_acc:0.974]
Epoch [116/120    avg_loss:0.016, val_acc:0.976]
Epoch [117/120    avg_loss:0.016, val_acc:0.977]
Epoch [118/120    avg_loss:0.014, val_acc:0.969]
Epoch [119/120    avg_loss:0.017, val_acc:0.972]
Epoch [120/120    avg_loss:0.022, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    1 1233    1    0    0    6    0    0    3   15   19    7    0
     0    0    0]
 [   0    0    1  720    0   15    0    0    0    6    0    0    4    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    0    0    5    4    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   12    0    0    3    0
     0    0    0]
 [   0    0   11   85    0    0    0    0    0    0  775    1    0    0
     0    3    0]
 [   0    0   13    0    0    3   11    0    1    0   19 2156    3    3
     1    0    0]
 [   0    0    0    6    5    0    0    0    0    0   21    6  495    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    1    0    3    4    0    0
  1129    0    0]
 [   0    0    0    0    0    0   13    0    0    1    0    0    0    0
   116  217    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.33875338753387

F1 scores:
[       nan 0.97560976 0.96933962 0.92189501 0.98839907 0.96708286
 0.97691735 1.         0.99767981 0.53333333 0.90537383 0.98066864
 0.94555874 0.98930481 0.94675052 0.7654321  0.98809524]

Kappa:
0.9468386396079239
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:22
Validation dataloader:22
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:100
lr:0.01
batch_size:40
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f041d0e06a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 144793==>0.14M
----------Training process----------
Epoch [1/120    avg_loss:2.722, val_acc:0.440]
Epoch [2/120    avg_loss:2.167, val_acc:0.550]
Epoch [3/120    avg_loss:1.910, val_acc:0.617]
Epoch [4/120    avg_loss:1.678, val_acc:0.637]
Epoch [5/120    avg_loss:1.488, val_acc:0.682]
Epoch [6/120    avg_loss:1.305, val_acc:0.690]
Epoch [7/120    avg_loss:1.092, val_acc:0.730]
Epoch [8/120    avg_loss:1.048, val_acc:0.752]
Epoch [9/120    avg_loss:0.850, val_acc:0.755]
Epoch [10/120    avg_loss:0.800, val_acc:0.773]
Epoch [11/120    avg_loss:0.716, val_acc:0.797]
Epoch [12/120    avg_loss:0.636, val_acc:0.802]
Epoch [13/120    avg_loss:0.556, val_acc:0.834]
Epoch [14/120    avg_loss:0.547, val_acc:0.855]
Epoch [15/120    avg_loss:0.434, val_acc:0.826]
Epoch [16/120    avg_loss:0.410, val_acc:0.880]
Epoch [17/120    avg_loss:0.403, val_acc:0.881]
Epoch [18/120    avg_loss:0.328, val_acc:0.875]
Epoch [19/120    avg_loss:0.303, val_acc:0.887]
Epoch [20/120    avg_loss:0.289, val_acc:0.900]
Epoch [21/120    avg_loss:0.329, val_acc:0.877]
Epoch [22/120    avg_loss:0.323, val_acc:0.897]
Epoch [23/120    avg_loss:0.254, val_acc:0.899]
Epoch [24/120    avg_loss:0.221, val_acc:0.933]
Epoch [25/120    avg_loss:0.190, val_acc:0.936]
Epoch [26/120    avg_loss:0.214, val_acc:0.927]
Epoch [27/120    avg_loss:0.173, val_acc:0.903]
Epoch [28/120    avg_loss:0.180, val_acc:0.938]
Epoch [29/120    avg_loss:0.152, val_acc:0.932]
Epoch [30/120    avg_loss:0.167, val_acc:0.938]
Epoch [31/120    avg_loss:0.165, val_acc:0.920]
Epoch [32/120    avg_loss:0.134, val_acc:0.939]
Epoch [33/120    avg_loss:0.145, val_acc:0.908]
Epoch [34/120    avg_loss:0.205, val_acc:0.931]
Epoch [35/120    avg_loss:0.139, val_acc:0.934]
Epoch [36/120    avg_loss:0.123, val_acc:0.931]
Epoch [37/120    avg_loss:0.111, val_acc:0.928]
Epoch [38/120    avg_loss:0.129, val_acc:0.952]
Epoch [39/120    avg_loss:0.105, val_acc:0.952]
Epoch [40/120    avg_loss:0.112, val_acc:0.938]
Epoch [41/120    avg_loss:0.110, val_acc:0.948]
Epoch [42/120    avg_loss:0.083, val_acc:0.959]
Epoch [43/120    avg_loss:0.092, val_acc:0.947]
Epoch [44/120    avg_loss:0.088, val_acc:0.961]
Epoch [45/120    avg_loss:0.075, val_acc:0.960]
Epoch [46/120    avg_loss:0.097, val_acc:0.928]
Epoch [47/120    avg_loss:0.140, val_acc:0.934]
Epoch [48/120    avg_loss:0.201, val_acc:0.928]
Epoch [49/120    avg_loss:0.147, val_acc:0.928]
Epoch [50/120    avg_loss:0.103, val_acc:0.951]
Epoch [51/120    avg_loss:0.099, val_acc:0.953]
Epoch [52/120    avg_loss:0.084, val_acc:0.963]
Epoch [53/120    avg_loss:0.064, val_acc:0.968]
Epoch [54/120    avg_loss:0.073, val_acc:0.933]
Epoch [55/120    avg_loss:0.139, val_acc:0.957]
Epoch [56/120    avg_loss:0.089, val_acc:0.959]
Epoch [57/120    avg_loss:0.075, val_acc:0.960]
Epoch [58/120    avg_loss:0.078, val_acc:0.958]
Epoch [59/120    avg_loss:0.059, val_acc:0.957]
Epoch [60/120    avg_loss:0.063, val_acc:0.964]
Epoch [61/120    avg_loss:0.046, val_acc:0.957]
Epoch [62/120    avg_loss:0.045, val_acc:0.965]
Epoch [63/120    avg_loss:0.050, val_acc:0.957]
Epoch [64/120    avg_loss:0.052, val_acc:0.958]
Epoch [65/120    avg_loss:0.045, val_acc:0.968]
Epoch [66/120    avg_loss:0.033, val_acc:0.968]
Epoch [67/120    avg_loss:0.036, val_acc:0.970]
Epoch [68/120    avg_loss:0.044, val_acc:0.966]
Epoch [69/120    avg_loss:0.043, val_acc:0.963]
Epoch [70/120    avg_loss:0.065, val_acc:0.950]
Epoch [71/120    avg_loss:0.046, val_acc:0.964]
Epoch [72/120    avg_loss:0.052, val_acc:0.957]
Epoch [73/120    avg_loss:0.045, val_acc:0.970]
Epoch [74/120    avg_loss:0.060, val_acc:0.956]
Epoch [75/120    avg_loss:0.036, val_acc:0.952]
Epoch [76/120    avg_loss:0.040, val_acc:0.941]
Epoch [77/120    avg_loss:0.027, val_acc:0.969]
Epoch [78/120    avg_loss:0.042, val_acc:0.964]
Epoch [79/120    avg_loss:0.032, val_acc:0.972]
Epoch [80/120    avg_loss:0.028, val_acc:0.969]
Epoch [81/120    avg_loss:0.025, val_acc:0.974]
Epoch [82/120    avg_loss:0.021, val_acc:0.972]
Epoch [83/120    avg_loss:0.019, val_acc:0.963]
Epoch [84/120    avg_loss:0.024, val_acc:0.975]
Epoch [85/120    avg_loss:0.028, val_acc:0.969]
Epoch [86/120    avg_loss:0.023, val_acc:0.973]
Epoch [87/120    avg_loss:0.024, val_acc:0.969]
Epoch [88/120    avg_loss:0.032, val_acc:0.965]
Epoch [89/120    avg_loss:0.024, val_acc:0.974]
Epoch [90/120    avg_loss:0.024, val_acc:0.975]
Epoch [91/120    avg_loss:0.040, val_acc:0.975]
Epoch [92/120    avg_loss:0.058, val_acc:0.969]
Epoch [93/120    avg_loss:0.041, val_acc:0.970]
Epoch [94/120    avg_loss:0.032, val_acc:0.973]
Epoch [95/120    avg_loss:0.031, val_acc:0.969]
Epoch [96/120    avg_loss:0.023, val_acc:0.975]
Epoch [97/120    avg_loss:0.026, val_acc:0.976]
Epoch [98/120    avg_loss:0.018, val_acc:0.973]
Epoch [99/120    avg_loss:0.021, val_acc:0.966]
Epoch [100/120    avg_loss:0.022, val_acc:0.977]
Epoch [101/120    avg_loss:0.016, val_acc:0.974]
Epoch [102/120    avg_loss:0.015, val_acc:0.976]
Epoch [103/120    avg_loss:0.014, val_acc:0.967]
Epoch [104/120    avg_loss:0.019, val_acc:0.972]
Epoch [105/120    avg_loss:0.013, val_acc:0.972]
Epoch [106/120    avg_loss:0.019, val_acc:0.974]
Epoch [107/120    avg_loss:0.021, val_acc:0.975]
Epoch [108/120    avg_loss:0.020, val_acc:0.974]
Epoch [109/120    avg_loss:0.013, val_acc:0.980]
Epoch [110/120    avg_loss:0.013, val_acc:0.976]
Epoch [111/120    avg_loss:0.014, val_acc:0.981]
Epoch [112/120    avg_loss:0.018, val_acc:0.978]
Epoch [113/120    avg_loss:0.017, val_acc:0.978]
Epoch [114/120    avg_loss:0.013, val_acc:0.976]
Epoch [115/120    avg_loss:0.013, val_acc:0.972]
Epoch [116/120    avg_loss:0.012, val_acc:0.975]
Epoch [117/120    avg_loss:0.008, val_acc:0.977]
Epoch [118/120    avg_loss:0.014, val_acc:0.978]
Epoch [119/120    avg_loss:0.010, val_acc:0.976]
Epoch [120/120    avg_loss:0.010, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    1 1263    0    0    0    1    0    0    0    4    7    4    0
     0    5    0]
 [   0    0    2  719    0   17    0    0    0    6    0    0    3    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    0    0    6    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   13    0    0    2    0
     0    0    0]
 [   0    0   10   90    0    3    0    0    0    0  763    5    0    0
     0    4    0]
 [   0    0   15    0    0    0    8    0    0    0   11 2171    2    2
     1    0    0]
 [   0    0    0   30   10    3    0    0    0    0   10    7  470    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   13    0    0    2    0    2    1    0    0
  1121    0    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    61  270    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.89159891598916

F1 scores:
[       nan 0.96296296 0.98059006 0.90497168 0.97706422 0.94866071
 0.98056801 1.         0.99767981 0.60465116 0.91596639 0.98636983
 0.92519685 0.99462366 0.9638865  0.86261981 0.97076023]

Kappa:
0.9531608249305425
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4b779ab6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.262, val_acc:0.480]
Epoch [2/120    avg_loss:1.742, val_acc:0.480]
Epoch [3/120    avg_loss:1.476, val_acc:0.562]
Epoch [4/120    avg_loss:1.228, val_acc:0.667]
Epoch [5/120    avg_loss:0.938, val_acc:0.702]
Epoch [6/120    avg_loss:0.857, val_acc:0.745]
Epoch [7/120    avg_loss:0.778, val_acc:0.812]
Epoch [8/120    avg_loss:0.706, val_acc:0.803]
Epoch [9/120    avg_loss:0.593, val_acc:0.800]
Epoch [10/120    avg_loss:0.860, val_acc:0.813]
Epoch [11/120    avg_loss:0.520, val_acc:0.819]
Epoch [12/120    avg_loss:0.432, val_acc:0.869]
Epoch [13/120    avg_loss:0.514, val_acc:0.845]
Epoch [14/120    avg_loss:0.526, val_acc:0.847]
Epoch [15/120    avg_loss:0.446, val_acc:0.749]
Epoch [16/120    avg_loss:0.355, val_acc:0.861]
Epoch [17/120    avg_loss:0.322, val_acc:0.904]
Epoch [18/120    avg_loss:0.391, val_acc:0.907]
Epoch [19/120    avg_loss:0.187, val_acc:0.938]
Epoch [20/120    avg_loss:0.185, val_acc:0.910]
Epoch [21/120    avg_loss:0.148, val_acc:0.909]
Epoch [22/120    avg_loss:0.150, val_acc:0.918]
Epoch [23/120    avg_loss:0.121, val_acc:0.915]
Epoch [24/120    avg_loss:0.178, val_acc:0.902]
Epoch [25/120    avg_loss:0.128, val_acc:0.897]
Epoch [26/120    avg_loss:0.184, val_acc:0.887]
Epoch [27/120    avg_loss:0.158, val_acc:0.944]
Epoch [28/120    avg_loss:0.145, val_acc:0.942]
Epoch [29/120    avg_loss:0.100, val_acc:0.934]
Epoch [30/120    avg_loss:0.088, val_acc:0.933]
Epoch [31/120    avg_loss:0.192, val_acc:0.939]
Epoch [32/120    avg_loss:0.167, val_acc:0.923]
Epoch [33/120    avg_loss:0.094, val_acc:0.956]
Epoch [34/120    avg_loss:0.129, val_acc:0.915]
Epoch [35/120    avg_loss:0.082, val_acc:0.951]
Epoch [36/120    avg_loss:0.065, val_acc:0.952]
Epoch [37/120    avg_loss:0.040, val_acc:0.951]
Epoch [38/120    avg_loss:0.041, val_acc:0.970]
Epoch [39/120    avg_loss:0.046, val_acc:0.962]
Epoch [40/120    avg_loss:0.044, val_acc:0.961]
Epoch [41/120    avg_loss:0.087, val_acc:0.959]
Epoch [42/120    avg_loss:0.043, val_acc:0.955]
Epoch [43/120    avg_loss:0.050, val_acc:0.955]
Epoch [44/120    avg_loss:0.057, val_acc:0.951]
Epoch [45/120    avg_loss:0.044, val_acc:0.950]
Epoch [46/120    avg_loss:0.035, val_acc:0.961]
Epoch [47/120    avg_loss:0.036, val_acc:0.951]
Epoch [48/120    avg_loss:0.042, val_acc:0.963]
Epoch [49/120    avg_loss:0.011, val_acc:0.974]
Epoch [50/120    avg_loss:0.017, val_acc:0.947]
Epoch [51/120    avg_loss:0.046, val_acc:0.959]
Epoch [52/120    avg_loss:0.080, val_acc:0.944]
Epoch [53/120    avg_loss:0.043, val_acc:0.966]
Epoch [54/120    avg_loss:0.085, val_acc:0.938]
Epoch [55/120    avg_loss:0.061, val_acc:0.948]
Epoch [56/120    avg_loss:0.032, val_acc:0.962]
Epoch [57/120    avg_loss:0.091, val_acc:0.958]
Epoch [58/120    avg_loss:0.066, val_acc:0.946]
Epoch [59/120    avg_loss:0.067, val_acc:0.942]
Epoch [60/120    avg_loss:0.068, val_acc:0.958]
Epoch [61/120    avg_loss:0.030, val_acc:0.951]
Epoch [62/120    avg_loss:0.045, val_acc:0.958]
Epoch [63/120    avg_loss:0.019, val_acc:0.963]
Epoch [64/120    avg_loss:0.016, val_acc:0.965]
Epoch [65/120    avg_loss:0.012, val_acc:0.967]
Epoch [66/120    avg_loss:0.012, val_acc:0.970]
Epoch [67/120    avg_loss:0.017, val_acc:0.971]
Epoch [68/120    avg_loss:0.015, val_acc:0.971]
Epoch [69/120    avg_loss:0.019, val_acc:0.972]
Epoch [70/120    avg_loss:0.008, val_acc:0.974]
Epoch [71/120    avg_loss:0.010, val_acc:0.976]
Epoch [72/120    avg_loss:0.017, val_acc:0.975]
Epoch [73/120    avg_loss:0.030, val_acc:0.976]
Epoch [74/120    avg_loss:0.009, val_acc:0.974]
Epoch [75/120    avg_loss:0.021, val_acc:0.974]
Epoch [76/120    avg_loss:0.021, val_acc:0.977]
Epoch [77/120    avg_loss:0.008, val_acc:0.978]
Epoch [78/120    avg_loss:0.010, val_acc:0.979]
Epoch [79/120    avg_loss:0.011, val_acc:0.980]
Epoch [80/120    avg_loss:0.012, val_acc:0.979]
Epoch [81/120    avg_loss:0.006, val_acc:0.980]
Epoch [82/120    avg_loss:0.014, val_acc:0.979]
Epoch [83/120    avg_loss:0.007, val_acc:0.980]
Epoch [84/120    avg_loss:0.009, val_acc:0.980]
Epoch [85/120    avg_loss:0.008, val_acc:0.981]
Epoch [86/120    avg_loss:0.007, val_acc:0.982]
Epoch [87/120    avg_loss:0.007, val_acc:0.981]
Epoch [88/120    avg_loss:0.011, val_acc:0.982]
Epoch [89/120    avg_loss:0.011, val_acc:0.981]
Epoch [90/120    avg_loss:0.005, val_acc:0.983]
Epoch [91/120    avg_loss:0.006, val_acc:0.983]
Epoch [92/120    avg_loss:0.005, val_acc:0.983]
Epoch [93/120    avg_loss:0.011, val_acc:0.980]
Epoch [94/120    avg_loss:0.012, val_acc:0.981]
Epoch [95/120    avg_loss:0.007, val_acc:0.981]
Epoch [96/120    avg_loss:0.006, val_acc:0.982]
Epoch [97/120    avg_loss:0.005, val_acc:0.982]
Epoch [98/120    avg_loss:0.009, val_acc:0.978]
Epoch [99/120    avg_loss:0.005, val_acc:0.980]
Epoch [100/120    avg_loss:0.008, val_acc:0.985]
Epoch [101/120    avg_loss:0.008, val_acc:0.985]
Epoch [102/120    avg_loss:0.007, val_acc:0.982]
Epoch [103/120    avg_loss:0.006, val_acc:0.982]
Epoch [104/120    avg_loss:0.007, val_acc:0.982]
Epoch [105/120    avg_loss:0.005, val_acc:0.983]
Epoch [106/120    avg_loss:0.005, val_acc:0.981]
Epoch [107/120    avg_loss:0.005, val_acc:0.983]
Epoch [108/120    avg_loss:0.009, val_acc:0.983]
Epoch [109/120    avg_loss:0.011, val_acc:0.981]
Epoch [110/120    avg_loss:0.004, val_acc:0.982]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.004, val_acc:0.983]
Epoch [113/120    avg_loss:0.006, val_acc:0.985]
Epoch [114/120    avg_loss:0.005, val_acc:0.981]
Epoch [115/120    avg_loss:0.005, val_acc:0.981]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.006, val_acc:0.985]
Epoch [118/120    avg_loss:0.008, val_acc:0.983]
Epoch [119/120    avg_loss:0.007, val_acc:0.985]
Epoch [120/120    avg_loss:0.005, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1256    6   13    0    0    0    0    0    5    4    1    0
     0    0    0]
 [   0    0    0  739    1    1    0    0    0    2    0    0    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  425    0    0    0    0    0
     0    0    2]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  846   21    3    0
     0    1    0]
 [   0    0   15    5    0    0    0    0    0    0    8 2166   16    0
     0    0    0]
 [   0    0    0    0    3    0    0    0    0    0    0    0  527    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1131    7    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    25  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.08130081300813

F1 scores:
[       nan 0.93975904 0.980484   0.98533333 0.96162528 0.99539171
 0.99167298 1.         0.99415205 0.85714286 0.97577855 0.98387463
 0.96786042 1.         0.9843342  0.93591654 0.95238095]

Kappa:
0.978132742665722
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:03:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff4367616a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.303, val_acc:0.496]
Epoch [2/120    avg_loss:1.748, val_acc:0.486]
Epoch [3/120    avg_loss:1.513, val_acc:0.576]
Epoch [4/120    avg_loss:1.299, val_acc:0.736]
Epoch [5/120    avg_loss:1.129, val_acc:0.752]
Epoch [6/120    avg_loss:0.926, val_acc:0.708]
Epoch [7/120    avg_loss:0.901, val_acc:0.720]
Epoch [8/120    avg_loss:0.782, val_acc:0.770]
Epoch [9/120    avg_loss:0.653, val_acc:0.840]
Epoch [10/120    avg_loss:0.604, val_acc:0.777]
Epoch [11/120    avg_loss:0.650, val_acc:0.838]
Epoch [12/120    avg_loss:0.426, val_acc:0.856]
Epoch [13/120    avg_loss:0.482, val_acc:0.875]
Epoch [14/120    avg_loss:0.370, val_acc:0.869]
Epoch [15/120    avg_loss:0.441, val_acc:0.790]
Epoch [16/120    avg_loss:0.250, val_acc:0.896]
Epoch [17/120    avg_loss:0.275, val_acc:0.847]
Epoch [18/120    avg_loss:0.326, val_acc:0.895]
Epoch [19/120    avg_loss:0.233, val_acc:0.902]
Epoch [20/120    avg_loss:0.225, val_acc:0.921]
Epoch [21/120    avg_loss:0.162, val_acc:0.903]
Epoch [22/120    avg_loss:0.229, val_acc:0.868]
Epoch [23/120    avg_loss:0.179, val_acc:0.895]
Epoch [24/120    avg_loss:0.149, val_acc:0.934]
Epoch [25/120    avg_loss:0.293, val_acc:0.934]
Epoch [26/120    avg_loss:0.221, val_acc:0.904]
Epoch [27/120    avg_loss:0.105, val_acc:0.935]
Epoch [28/120    avg_loss:0.097, val_acc:0.899]
Epoch [29/120    avg_loss:0.096, val_acc:0.939]
Epoch [30/120    avg_loss:0.066, val_acc:0.952]
Epoch [31/120    avg_loss:0.034, val_acc:0.965]
Epoch [32/120    avg_loss:0.044, val_acc:0.940]
Epoch [33/120    avg_loss:0.052, val_acc:0.931]
Epoch [34/120    avg_loss:0.085, val_acc:0.901]
Epoch [35/120    avg_loss:0.084, val_acc:0.956]
Epoch [36/120    avg_loss:0.042, val_acc:0.957]
Epoch [37/120    avg_loss:0.044, val_acc:0.960]
Epoch [38/120    avg_loss:0.066, val_acc:0.956]
Epoch [39/120    avg_loss:0.043, val_acc:0.930]
Epoch [40/120    avg_loss:0.066, val_acc:0.964]
Epoch [41/120    avg_loss:0.037, val_acc:0.954]
Epoch [42/120    avg_loss:0.034, val_acc:0.956]
Epoch [43/120    avg_loss:0.045, val_acc:0.934]
Epoch [44/120    avg_loss:0.026, val_acc:0.972]
Epoch [45/120    avg_loss:0.068, val_acc:0.955]
Epoch [46/120    avg_loss:0.042, val_acc:0.962]
Epoch [47/120    avg_loss:0.042, val_acc:0.966]
Epoch [48/120    avg_loss:0.040, val_acc:0.967]
Epoch [49/120    avg_loss:0.019, val_acc:0.973]
Epoch [50/120    avg_loss:0.026, val_acc:0.960]
Epoch [51/120    avg_loss:0.026, val_acc:0.959]
Epoch [52/120    avg_loss:0.039, val_acc:0.966]
Epoch [53/120    avg_loss:0.031, val_acc:0.976]
Epoch [54/120    avg_loss:0.032, val_acc:0.973]
Epoch [55/120    avg_loss:0.023, val_acc:0.964]
Epoch [56/120    avg_loss:0.030, val_acc:0.972]
Epoch [57/120    avg_loss:0.018, val_acc:0.978]
Epoch [58/120    avg_loss:0.016, val_acc:0.980]
Epoch [59/120    avg_loss:0.025, val_acc:0.931]
Epoch [60/120    avg_loss:0.021, val_acc:0.973]
Epoch [61/120    avg_loss:0.009, val_acc:0.983]
Epoch [62/120    avg_loss:0.088, val_acc:0.939]
Epoch [63/120    avg_loss:0.094, val_acc:0.951]
Epoch [64/120    avg_loss:0.059, val_acc:0.957]
Epoch [65/120    avg_loss:0.052, val_acc:0.959]
Epoch [66/120    avg_loss:0.090, val_acc:0.942]
Epoch [67/120    avg_loss:0.024, val_acc:0.953]
Epoch [68/120    avg_loss:0.025, val_acc:0.963]
Epoch [69/120    avg_loss:0.036, val_acc:0.964]
Epoch [70/120    avg_loss:0.020, val_acc:0.974]
Epoch [71/120    avg_loss:0.015, val_acc:0.972]
Epoch [72/120    avg_loss:0.011, val_acc:0.973]
Epoch [73/120    avg_loss:0.043, val_acc:0.966]
Epoch [74/120    avg_loss:0.057, val_acc:0.967]
Epoch [75/120    avg_loss:0.019, val_acc:0.974]
Epoch [76/120    avg_loss:0.016, val_acc:0.977]
Epoch [77/120    avg_loss:0.013, val_acc:0.981]
Epoch [78/120    avg_loss:0.012, val_acc:0.981]
Epoch [79/120    avg_loss:0.011, val_acc:0.981]
Epoch [80/120    avg_loss:0.009, val_acc:0.983]
Epoch [81/120    avg_loss:0.014, val_acc:0.984]
Epoch [82/120    avg_loss:0.009, val_acc:0.985]
Epoch [83/120    avg_loss:0.009, val_acc:0.983]
Epoch [84/120    avg_loss:0.005, val_acc:0.981]
Epoch [85/120    avg_loss:0.009, val_acc:0.982]
Epoch [86/120    avg_loss:0.011, val_acc:0.984]
Epoch [87/120    avg_loss:0.009, val_acc:0.984]
Epoch [88/120    avg_loss:0.008, val_acc:0.985]
Epoch [89/120    avg_loss:0.009, val_acc:0.984]
Epoch [90/120    avg_loss:0.006, val_acc:0.985]
Epoch [91/120    avg_loss:0.007, val_acc:0.984]
Epoch [92/120    avg_loss:0.007, val_acc:0.986]
Epoch [93/120    avg_loss:0.012, val_acc:0.989]
Epoch [94/120    avg_loss:0.008, val_acc:0.989]
Epoch [95/120    avg_loss:0.008, val_acc:0.986]
Epoch [96/120    avg_loss:0.006, val_acc:0.987]
Epoch [97/120    avg_loss:0.006, val_acc:0.987]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.006, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.987]
Epoch [101/120    avg_loss:0.013, val_acc:0.988]
Epoch [102/120    avg_loss:0.009, val_acc:0.988]
Epoch [103/120    avg_loss:0.009, val_acc:0.988]
Epoch [104/120    avg_loss:0.009, val_acc:0.989]
Epoch [105/120    avg_loss:0.006, val_acc:0.988]
Epoch [106/120    avg_loss:0.005, val_acc:0.989]
Epoch [107/120    avg_loss:0.009, val_acc:0.989]
Epoch [108/120    avg_loss:0.004, val_acc:0.986]
Epoch [109/120    avg_loss:0.006, val_acc:0.989]
Epoch [110/120    avg_loss:0.006, val_acc:0.988]
Epoch [111/120    avg_loss:0.005, val_acc:0.987]
Epoch [112/120    avg_loss:0.004, val_acc:0.987]
Epoch [113/120    avg_loss:0.006, val_acc:0.987]
Epoch [114/120    avg_loss:0.010, val_acc:0.989]
Epoch [115/120    avg_loss:0.008, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.005, val_acc:0.989]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.009, val_acc:0.986]
Epoch [120/120    avg_loss:0.007, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1263    2    3    0    1    0    0    0   11    5    0    0
     0    0    0]
 [   0    0    0  729    2    0    0    0    0    2    0    1   13    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    6    0    0    0  855    9    0    0
     4    1    0]
 [   0    0   16    0    0    6    2    0    0    0    4 2157   22    1
     2    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0    0  531    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1131    7    0]
 [   0    0    0    0    0    1    7    0    0    0    0    0    0    0
    52  287    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.90785907859079

F1 scores:
[       nan 0.975      0.98479532 0.98580122 0.98839907 0.99088838
 0.98574644 1.         1.         0.85714286 0.97938144 0.98448197
 0.96195652 0.99730458 0.97164948 0.894081   0.96385542]

Kappa:
0.9761572876984193
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f4afc4668>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.266, val_acc:0.403]
Epoch [2/120    avg_loss:1.747, val_acc:0.544]
Epoch [3/120    avg_loss:1.562, val_acc:0.511]
Epoch [4/120    avg_loss:1.327, val_acc:0.647]
Epoch [5/120    avg_loss:1.001, val_acc:0.742]
Epoch [6/120    avg_loss:0.887, val_acc:0.741]
Epoch [7/120    avg_loss:0.792, val_acc:0.764]
Epoch [8/120    avg_loss:0.686, val_acc:0.814]
Epoch [9/120    avg_loss:0.714, val_acc:0.755]
Epoch [10/120    avg_loss:0.673, val_acc:0.785]
Epoch [11/120    avg_loss:0.567, val_acc:0.782]
Epoch [12/120    avg_loss:0.468, val_acc:0.815]
Epoch [13/120    avg_loss:0.470, val_acc:0.885]
Epoch [14/120    avg_loss:0.362, val_acc:0.847]
Epoch [15/120    avg_loss:0.309, val_acc:0.890]
Epoch [16/120    avg_loss:0.340, val_acc:0.865]
Epoch [17/120    avg_loss:0.309, val_acc:0.878]
Epoch [18/120    avg_loss:0.318, val_acc:0.889]
Epoch [19/120    avg_loss:0.184, val_acc:0.881]
Epoch [20/120    avg_loss:0.111, val_acc:0.912]
Epoch [21/120    avg_loss:0.368, val_acc:0.853]
Epoch [22/120    avg_loss:0.289, val_acc:0.876]
Epoch [23/120    avg_loss:0.136, val_acc:0.940]
Epoch [24/120    avg_loss:0.173, val_acc:0.932]
Epoch [25/120    avg_loss:0.137, val_acc:0.910]
Epoch [26/120    avg_loss:0.110, val_acc:0.933]
Epoch [27/120    avg_loss:0.084, val_acc:0.950]
Epoch [28/120    avg_loss:0.069, val_acc:0.930]
Epoch [29/120    avg_loss:0.118, val_acc:0.935]
Epoch [30/120    avg_loss:0.072, val_acc:0.958]
Epoch [31/120    avg_loss:0.058, val_acc:0.944]
Epoch [32/120    avg_loss:0.093, val_acc:0.942]
Epoch [33/120    avg_loss:0.089, val_acc:0.932]
Epoch [34/120    avg_loss:0.048, val_acc:0.934]
Epoch [35/120    avg_loss:0.024, val_acc:0.959]
Epoch [36/120    avg_loss:0.066, val_acc:0.959]
Epoch [37/120    avg_loss:0.054, val_acc:0.959]
Epoch [38/120    avg_loss:0.069, val_acc:0.957]
Epoch [39/120    avg_loss:0.042, val_acc:0.965]
Epoch [40/120    avg_loss:0.027, val_acc:0.943]
Epoch [41/120    avg_loss:0.052, val_acc:0.952]
Epoch [42/120    avg_loss:0.037, val_acc:0.954]
Epoch [43/120    avg_loss:0.050, val_acc:0.936]
Epoch [44/120    avg_loss:0.052, val_acc:0.941]
Epoch [45/120    avg_loss:0.047, val_acc:0.959]
Epoch [46/120    avg_loss:0.020, val_acc:0.963]
Epoch [47/120    avg_loss:0.045, val_acc:0.963]
Epoch [48/120    avg_loss:0.033, val_acc:0.946]
Epoch [49/120    avg_loss:0.039, val_acc:0.959]
Epoch [50/120    avg_loss:0.025, val_acc:0.970]
Epoch [51/120    avg_loss:0.017, val_acc:0.965]
Epoch [52/120    avg_loss:0.072, val_acc:0.944]
Epoch [53/120    avg_loss:0.025, val_acc:0.975]
Epoch [54/120    avg_loss:0.045, val_acc:0.964]
Epoch [55/120    avg_loss:0.010, val_acc:0.971]
Epoch [56/120    avg_loss:0.025, val_acc:0.968]
Epoch [57/120    avg_loss:0.011, val_acc:0.972]
Epoch [58/120    avg_loss:0.011, val_acc:0.980]
Epoch [59/120    avg_loss:0.009, val_acc:0.977]
Epoch [60/120    avg_loss:0.011, val_acc:0.978]
Epoch [61/120    avg_loss:0.010, val_acc:0.974]
Epoch [62/120    avg_loss:0.006, val_acc:0.971]
Epoch [63/120    avg_loss:0.010, val_acc:0.985]
Epoch [64/120    avg_loss:0.006, val_acc:0.978]
Epoch [65/120    avg_loss:0.005, val_acc:0.978]
Epoch [66/120    avg_loss:0.017, val_acc:0.962]
Epoch [67/120    avg_loss:0.016, val_acc:0.984]
Epoch [68/120    avg_loss:0.023, val_acc:0.953]
Epoch [69/120    avg_loss:0.023, val_acc:0.973]
Epoch [70/120    avg_loss:0.011, val_acc:0.980]
Epoch [71/120    avg_loss:0.015, val_acc:0.972]
Epoch [72/120    avg_loss:0.009, val_acc:0.978]
Epoch [73/120    avg_loss:0.012, val_acc:0.976]
Epoch [74/120    avg_loss:0.009, val_acc:0.977]
Epoch [75/120    avg_loss:0.015, val_acc:0.970]
Epoch [76/120    avg_loss:0.014, val_acc:0.980]
Epoch [77/120    avg_loss:0.009, val_acc:0.978]
Epoch [78/120    avg_loss:0.013, val_acc:0.978]
Epoch [79/120    avg_loss:0.009, val_acc:0.978]
Epoch [80/120    avg_loss:0.009, val_acc:0.981]
Epoch [81/120    avg_loss:0.009, val_acc:0.976]
Epoch [82/120    avg_loss:0.004, val_acc:0.981]
Epoch [83/120    avg_loss:0.007, val_acc:0.982]
Epoch [84/120    avg_loss:0.005, val_acc:0.983]
Epoch [85/120    avg_loss:0.004, val_acc:0.982]
Epoch [86/120    avg_loss:0.006, val_acc:0.982]
Epoch [87/120    avg_loss:0.004, val_acc:0.983]
Epoch [88/120    avg_loss:0.007, val_acc:0.982]
Epoch [89/120    avg_loss:0.003, val_acc:0.983]
Epoch [90/120    avg_loss:0.011, val_acc:0.983]
Epoch [91/120    avg_loss:0.003, val_acc:0.983]
Epoch [92/120    avg_loss:0.004, val_acc:0.983]
Epoch [93/120    avg_loss:0.004, val_acc:0.983]
Epoch [94/120    avg_loss:0.005, val_acc:0.983]
Epoch [95/120    avg_loss:0.004, val_acc:0.983]
Epoch [96/120    avg_loss:0.004, val_acc:0.983]
Epoch [97/120    avg_loss:0.005, val_acc:0.983]
Epoch [98/120    avg_loss:0.003, val_acc:0.983]
Epoch [99/120    avg_loss:0.005, val_acc:0.983]
Epoch [100/120    avg_loss:0.003, val_acc:0.983]
Epoch [101/120    avg_loss:0.003, val_acc:0.983]
Epoch [102/120    avg_loss:0.005, val_acc:0.983]
Epoch [103/120    avg_loss:0.003, val_acc:0.983]
Epoch [104/120    avg_loss:0.004, val_acc:0.983]
Epoch [105/120    avg_loss:0.007, val_acc:0.983]
Epoch [106/120    avg_loss:0.005, val_acc:0.983]
Epoch [107/120    avg_loss:0.005, val_acc:0.983]
Epoch [108/120    avg_loss:0.006, val_acc:0.983]
Epoch [109/120    avg_loss:0.011, val_acc:0.983]
Epoch [110/120    avg_loss:0.012, val_acc:0.983]
Epoch [111/120    avg_loss:0.003, val_acc:0.983]
Epoch [112/120    avg_loss:0.004, val_acc:0.983]
Epoch [113/120    avg_loss:0.004, val_acc:0.983]
Epoch [114/120    avg_loss:0.004, val_acc:0.983]
Epoch [115/120    avg_loss:0.004, val_acc:0.983]
Epoch [116/120    avg_loss:0.006, val_acc:0.983]
Epoch [117/120    avg_loss:0.004, val_acc:0.983]
Epoch [118/120    avg_loss:0.006, val_acc:0.983]
Epoch [119/120    avg_loss:0.005, val_acc:0.983]
Epoch [120/120    avg_loss:0.004, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1250    4   11    1    0    0    0    0    8   11    0    0
     0    0    0]
 [   0    0    0  732    2    2    0    0    0    0    0    2    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    9    0    0    0    4    0    0    0  842   18    0    0
     2    0    0]
 [   0    0   12    0    0    0    1    0    0    0    7 2175   15    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1  531    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    1    0    0    0    0    0    0    0
  1123    9    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    34  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.95121951219512

F1 scores:
[       nan 0.98765432 0.97809077 0.98652291 0.97038724 0.98976109
 0.98426966 1.         1.         0.97142857 0.97172533 0.98483133
 0.97431193 1.         0.97737163 0.91131498 0.98224852]

Kappa:
0.9766438604970852
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa0268ce6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.176, val_acc:0.535]
Epoch [2/120    avg_loss:1.758, val_acc:0.580]
Epoch [3/120    avg_loss:1.493, val_acc:0.683]
Epoch [4/120    avg_loss:1.288, val_acc:0.600]
Epoch [5/120    avg_loss:1.171, val_acc:0.756]
Epoch [6/120    avg_loss:1.056, val_acc:0.691]
Epoch [7/120    avg_loss:0.868, val_acc:0.783]
Epoch [8/120    avg_loss:0.872, val_acc:0.819]
Epoch [9/120    avg_loss:0.674, val_acc:0.800]
Epoch [10/120    avg_loss:0.645, val_acc:0.766]
Epoch [11/120    avg_loss:0.663, val_acc:0.691]
Epoch [12/120    avg_loss:0.469, val_acc:0.847]
Epoch [13/120    avg_loss:0.441, val_acc:0.841]
Epoch [14/120    avg_loss:0.455, val_acc:0.870]
Epoch [15/120    avg_loss:0.365, val_acc:0.869]
Epoch [16/120    avg_loss:0.341, val_acc:0.888]
Epoch [17/120    avg_loss:0.258, val_acc:0.884]
Epoch [18/120    avg_loss:0.304, val_acc:0.899]
Epoch [19/120    avg_loss:0.311, val_acc:0.863]
Epoch [20/120    avg_loss:0.208, val_acc:0.892]
Epoch [21/120    avg_loss:0.121, val_acc:0.927]
Epoch [22/120    avg_loss:0.134, val_acc:0.893]
Epoch [23/120    avg_loss:0.174, val_acc:0.904]
Epoch [24/120    avg_loss:0.132, val_acc:0.938]
Epoch [25/120    avg_loss:0.125, val_acc:0.904]
Epoch [26/120    avg_loss:0.133, val_acc:0.946]
Epoch [27/120    avg_loss:0.176, val_acc:0.889]
Epoch [28/120    avg_loss:0.111, val_acc:0.926]
Epoch [29/120    avg_loss:0.134, val_acc:0.923]
Epoch [30/120    avg_loss:0.115, val_acc:0.935]
Epoch [31/120    avg_loss:0.138, val_acc:0.920]
Epoch [32/120    avg_loss:0.056, val_acc:0.952]
Epoch [33/120    avg_loss:0.073, val_acc:0.953]
Epoch [34/120    avg_loss:0.048, val_acc:0.959]
Epoch [35/120    avg_loss:0.043, val_acc:0.947]
Epoch [36/120    avg_loss:0.055, val_acc:0.952]
Epoch [37/120    avg_loss:0.055, val_acc:0.952]
Epoch [38/120    avg_loss:0.082, val_acc:0.950]
Epoch [39/120    avg_loss:0.034, val_acc:0.968]
Epoch [40/120    avg_loss:0.034, val_acc:0.953]
Epoch [41/120    avg_loss:0.025, val_acc:0.952]
Epoch [42/120    avg_loss:0.023, val_acc:0.956]
Epoch [43/120    avg_loss:0.042, val_acc:0.957]
Epoch [44/120    avg_loss:0.093, val_acc:0.945]
Epoch [45/120    avg_loss:0.077, val_acc:0.961]
Epoch [46/120    avg_loss:0.038, val_acc:0.967]
Epoch [47/120    avg_loss:0.028, val_acc:0.973]
Epoch [48/120    avg_loss:0.052, val_acc:0.962]
Epoch [49/120    avg_loss:0.109, val_acc:0.939]
Epoch [50/120    avg_loss:0.128, val_acc:0.930]
Epoch [51/120    avg_loss:0.086, val_acc:0.963]
Epoch [52/120    avg_loss:0.087, val_acc:0.936]
Epoch [53/120    avg_loss:0.079, val_acc:0.949]
Epoch [54/120    avg_loss:0.044, val_acc:0.969]
Epoch [55/120    avg_loss:0.030, val_acc:0.960]
Epoch [56/120    avg_loss:0.024, val_acc:0.972]
Epoch [57/120    avg_loss:0.060, val_acc:0.940]
Epoch [58/120    avg_loss:0.058, val_acc:0.959]
Epoch [59/120    avg_loss:0.057, val_acc:0.953]
Epoch [60/120    avg_loss:0.035, val_acc:0.967]
Epoch [61/120    avg_loss:0.036, val_acc:0.971]
Epoch [62/120    avg_loss:0.020, val_acc:0.971]
Epoch [63/120    avg_loss:0.010, val_acc:0.973]
Epoch [64/120    avg_loss:0.010, val_acc:0.973]
Epoch [65/120    avg_loss:0.016, val_acc:0.975]
Epoch [66/120    avg_loss:0.021, val_acc:0.975]
Epoch [67/120    avg_loss:0.014, val_acc:0.975]
Epoch [68/120    avg_loss:0.011, val_acc:0.973]
Epoch [69/120    avg_loss:0.007, val_acc:0.975]
Epoch [70/120    avg_loss:0.007, val_acc:0.975]
Epoch [71/120    avg_loss:0.016, val_acc:0.975]
Epoch [72/120    avg_loss:0.017, val_acc:0.975]
Epoch [73/120    avg_loss:0.012, val_acc:0.975]
Epoch [74/120    avg_loss:0.011, val_acc:0.974]
Epoch [75/120    avg_loss:0.011, val_acc:0.975]
Epoch [76/120    avg_loss:0.011, val_acc:0.974]
Epoch [77/120    avg_loss:0.007, val_acc:0.974]
Epoch [78/120    avg_loss:0.007, val_acc:0.976]
Epoch [79/120    avg_loss:0.013, val_acc:0.973]
Epoch [80/120    avg_loss:0.006, val_acc:0.976]
Epoch [81/120    avg_loss:0.008, val_acc:0.975]
Epoch [82/120    avg_loss:0.006, val_acc:0.974]
Epoch [83/120    avg_loss:0.009, val_acc:0.974]
Epoch [84/120    avg_loss:0.007, val_acc:0.974]
Epoch [85/120    avg_loss:0.005, val_acc:0.974]
Epoch [86/120    avg_loss:0.009, val_acc:0.974]
Epoch [87/120    avg_loss:0.008, val_acc:0.975]
Epoch [88/120    avg_loss:0.010, val_acc:0.975]
Epoch [89/120    avg_loss:0.005, val_acc:0.975]
Epoch [90/120    avg_loss:0.004, val_acc:0.974]
Epoch [91/120    avg_loss:0.007, val_acc:0.975]
Epoch [92/120    avg_loss:0.006, val_acc:0.975]
Epoch [93/120    avg_loss:0.011, val_acc:0.974]
Epoch [94/120    avg_loss:0.009, val_acc:0.974]
Epoch [95/120    avg_loss:0.007, val_acc:0.974]
Epoch [96/120    avg_loss:0.008, val_acc:0.974]
Epoch [97/120    avg_loss:0.008, val_acc:0.974]
Epoch [98/120    avg_loss:0.005, val_acc:0.975]
Epoch [99/120    avg_loss:0.005, val_acc:0.975]
Epoch [100/120    avg_loss:0.008, val_acc:0.975]
Epoch [101/120    avg_loss:0.007, val_acc:0.975]
Epoch [102/120    avg_loss:0.007, val_acc:0.975]
Epoch [103/120    avg_loss:0.005, val_acc:0.975]
Epoch [104/120    avg_loss:0.004, val_acc:0.975]
Epoch [105/120    avg_loss:0.010, val_acc:0.975]
Epoch [106/120    avg_loss:0.008, val_acc:0.974]
Epoch [107/120    avg_loss:0.005, val_acc:0.974]
Epoch [108/120    avg_loss:0.008, val_acc:0.974]
Epoch [109/120    avg_loss:0.004, val_acc:0.974]
Epoch [110/120    avg_loss:0.007, val_acc:0.974]
Epoch [111/120    avg_loss:0.006, val_acc:0.974]
Epoch [112/120    avg_loss:0.009, val_acc:0.974]
Epoch [113/120    avg_loss:0.010, val_acc:0.974]
Epoch [114/120    avg_loss:0.008, val_acc:0.974]
Epoch [115/120    avg_loss:0.012, val_acc:0.974]
Epoch [116/120    avg_loss:0.008, val_acc:0.974]
Epoch [117/120    avg_loss:0.007, val_acc:0.974]
Epoch [118/120    avg_loss:0.007, val_acc:0.975]
Epoch [119/120    avg_loss:0.014, val_acc:0.975]
Epoch [120/120    avg_loss:0.005, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1259    1   12    0    2    0    0    0    7    4    0    0
     0    0    0]
 [   0    0    1  731    1    2    0    0    0    2    0    4    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    1    0    0  432    0    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    1    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    1    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   14    0    0    0    0    0    0    0  831   28    0    0
     0    2    0]
 [   0    0   15    0    0    1    0    0    0    0    4 2178   12    0
     0    0    0]
 [   0    0    1    1    0    0    0    0    0    0    0    2  528    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    0    0    0    0
  1113   22    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    29  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.88617886178862

F1 scores:
[       nan 0.98795181 0.97748447 0.98783784 0.96818182 0.98855835
 0.99168556 1.         0.99883586 0.91891892 0.96796738 0.98396205
 0.97597043 1.         0.97503285 0.91042584 0.97619048]

Kappa:
0.975899625078484
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7109f256a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.175, val_acc:0.481]
Epoch [2/120    avg_loss:1.587, val_acc:0.534]
Epoch [3/120    avg_loss:1.463, val_acc:0.607]
Epoch [4/120    avg_loss:1.244, val_acc:0.655]
Epoch [5/120    avg_loss:1.132, val_acc:0.600]
Epoch [6/120    avg_loss:0.860, val_acc:0.732]
Epoch [7/120    avg_loss:0.847, val_acc:0.811]
Epoch [8/120    avg_loss:0.804, val_acc:0.768]
Epoch [9/120    avg_loss:0.559, val_acc:0.807]
Epoch [10/120    avg_loss:0.667, val_acc:0.825]
Epoch [11/120    avg_loss:0.596, val_acc:0.833]
Epoch [12/120    avg_loss:0.493, val_acc:0.858]
Epoch [13/120    avg_loss:0.435, val_acc:0.874]
Epoch [14/120    avg_loss:0.406, val_acc:0.858]
Epoch [15/120    avg_loss:0.442, val_acc:0.895]
Epoch [16/120    avg_loss:0.353, val_acc:0.885]
Epoch [17/120    avg_loss:0.342, val_acc:0.862]
Epoch [18/120    avg_loss:0.258, val_acc:0.892]
Epoch [19/120    avg_loss:0.214, val_acc:0.903]
Epoch [20/120    avg_loss:0.233, val_acc:0.861]
Epoch [21/120    avg_loss:0.197, val_acc:0.888]
Epoch [22/120    avg_loss:0.183, val_acc:0.907]
Epoch [23/120    avg_loss:0.115, val_acc:0.904]
Epoch [24/120    avg_loss:0.182, val_acc:0.915]
Epoch [25/120    avg_loss:0.139, val_acc:0.928]
Epoch [26/120    avg_loss:0.156, val_acc:0.918]
Epoch [27/120    avg_loss:0.150, val_acc:0.890]
Epoch [28/120    avg_loss:0.152, val_acc:0.884]
Epoch [29/120    avg_loss:0.136, val_acc:0.939]
Epoch [30/120    avg_loss:0.142, val_acc:0.961]
Epoch [31/120    avg_loss:0.220, val_acc:0.901]
Epoch [32/120    avg_loss:0.217, val_acc:0.930]
Epoch [33/120    avg_loss:0.126, val_acc:0.849]
Epoch [34/120    avg_loss:0.082, val_acc:0.955]
Epoch [35/120    avg_loss:0.086, val_acc:0.941]
Epoch [36/120    avg_loss:0.069, val_acc:0.950]
Epoch [37/120    avg_loss:0.111, val_acc:0.907]
Epoch [38/120    avg_loss:0.062, val_acc:0.963]
Epoch [39/120    avg_loss:0.053, val_acc:0.966]
Epoch [40/120    avg_loss:0.084, val_acc:0.946]
Epoch [41/120    avg_loss:0.043, val_acc:0.962]
Epoch [42/120    avg_loss:0.068, val_acc:0.956]
Epoch [43/120    avg_loss:0.032, val_acc:0.972]
Epoch [44/120    avg_loss:0.032, val_acc:0.970]
Epoch [45/120    avg_loss:0.037, val_acc:0.956]
Epoch [46/120    avg_loss:0.014, val_acc:0.976]
Epoch [47/120    avg_loss:0.029, val_acc:0.966]
Epoch [48/120    avg_loss:0.054, val_acc:0.932]
Epoch [49/120    avg_loss:0.048, val_acc:0.955]
Epoch [50/120    avg_loss:0.031, val_acc:0.978]
Epoch [51/120    avg_loss:0.040, val_acc:0.960]
Epoch [52/120    avg_loss:0.024, val_acc:0.971]
Epoch [53/120    avg_loss:0.051, val_acc:0.966]
Epoch [54/120    avg_loss:0.016, val_acc:0.968]
Epoch [55/120    avg_loss:0.129, val_acc:0.947]
Epoch [56/120    avg_loss:0.074, val_acc:0.942]
Epoch [57/120    avg_loss:0.047, val_acc:0.960]
Epoch [58/120    avg_loss:0.060, val_acc:0.918]
Epoch [59/120    avg_loss:0.062, val_acc:0.951]
Epoch [60/120    avg_loss:0.034, val_acc:0.967]
Epoch [61/120    avg_loss:0.036, val_acc:0.963]
Epoch [62/120    avg_loss:0.088, val_acc:0.870]
Epoch [63/120    avg_loss:0.076, val_acc:0.967]
Epoch [64/120    avg_loss:0.017, val_acc:0.972]
Epoch [65/120    avg_loss:0.029, val_acc:0.973]
Epoch [66/120    avg_loss:0.017, val_acc:0.975]
Epoch [67/120    avg_loss:0.024, val_acc:0.975]
Epoch [68/120    avg_loss:0.020, val_acc:0.975]
Epoch [69/120    avg_loss:0.017, val_acc:0.975]
Epoch [70/120    avg_loss:0.011, val_acc:0.975]
Epoch [71/120    avg_loss:0.012, val_acc:0.975]
Epoch [72/120    avg_loss:0.008, val_acc:0.975]
Epoch [73/120    avg_loss:0.017, val_acc:0.974]
Epoch [74/120    avg_loss:0.012, val_acc:0.974]
Epoch [75/120    avg_loss:0.011, val_acc:0.975]
Epoch [76/120    avg_loss:0.014, val_acc:0.975]
Epoch [77/120    avg_loss:0.008, val_acc:0.975]
Epoch [78/120    avg_loss:0.020, val_acc:0.975]
Epoch [79/120    avg_loss:0.014, val_acc:0.975]
Epoch [80/120    avg_loss:0.016, val_acc:0.975]
Epoch [81/120    avg_loss:0.011, val_acc:0.975]
Epoch [82/120    avg_loss:0.008, val_acc:0.975]
Epoch [83/120    avg_loss:0.016, val_acc:0.974]
Epoch [84/120    avg_loss:0.013, val_acc:0.974]
Epoch [85/120    avg_loss:0.016, val_acc:0.974]
Epoch [86/120    avg_loss:0.010, val_acc:0.974]
Epoch [87/120    avg_loss:0.019, val_acc:0.974]
Epoch [88/120    avg_loss:0.011, val_acc:0.974]
Epoch [89/120    avg_loss:0.012, val_acc:0.974]
Epoch [90/120    avg_loss:0.015, val_acc:0.974]
Epoch [91/120    avg_loss:0.012, val_acc:0.974]
Epoch [92/120    avg_loss:0.014, val_acc:0.974]
Epoch [93/120    avg_loss:0.010, val_acc:0.974]
Epoch [94/120    avg_loss:0.011, val_acc:0.974]
Epoch [95/120    avg_loss:0.010, val_acc:0.974]
Epoch [96/120    avg_loss:0.014, val_acc:0.974]
Epoch [97/120    avg_loss:0.014, val_acc:0.974]
Epoch [98/120    avg_loss:0.011, val_acc:0.974]
Epoch [99/120    avg_loss:0.013, val_acc:0.974]
Epoch [100/120    avg_loss:0.011, val_acc:0.974]
Epoch [101/120    avg_loss:0.014, val_acc:0.974]
Epoch [102/120    avg_loss:0.016, val_acc:0.974]
Epoch [103/120    avg_loss:0.012, val_acc:0.974]
Epoch [104/120    avg_loss:0.007, val_acc:0.974]
Epoch [105/120    avg_loss:0.010, val_acc:0.974]
Epoch [106/120    avg_loss:0.011, val_acc:0.974]
Epoch [107/120    avg_loss:0.013, val_acc:0.974]
Epoch [108/120    avg_loss:0.015, val_acc:0.974]
Epoch [109/120    avg_loss:0.008, val_acc:0.974]
Epoch [110/120    avg_loss:0.011, val_acc:0.974]
Epoch [111/120    avg_loss:0.011, val_acc:0.974]
Epoch [112/120    avg_loss:0.008, val_acc:0.974]
Epoch [113/120    avg_loss:0.010, val_acc:0.974]
Epoch [114/120    avg_loss:0.011, val_acc:0.974]
Epoch [115/120    avg_loss:0.016, val_acc:0.974]
Epoch [116/120    avg_loss:0.019, val_acc:0.974]
Epoch [117/120    avg_loss:0.017, val_acc:0.974]
Epoch [118/120    avg_loss:0.011, val_acc:0.974]
Epoch [119/120    avg_loss:0.010, val_acc:0.974]
Epoch [120/120    avg_loss:0.013, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    1    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1268    3   11    0    0    0    0    0    2    1    0    0
     0    0    0]
 [   0    0    0  730    0    1    0    0    0    3    0    7    6    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    3    0    0    0  841   25    0    0
     1    1    0]
 [   0    0   24    5    0   17    1    0    0    0    8 2128   25    1
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0  530    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    4    0    0    0    0    0    0    0
  1114   11    0]
 [   0    0    0    0    0    1   13    0    0    0    0    0    0    0
    56  277    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.25745257452574

F1 scores:
[       nan 0.98765432 0.9825649  0.98316498 0.97482838 0.96551724
 0.98426966 0.98039216 1.         0.92307692 0.97450753 0.97369023
 0.96627165 0.99730458 0.96450216 0.87106918 0.95906433]

Kappa:
0.9687579337520013
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4f5596b6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.249, val_acc:0.557]
Epoch [2/120    avg_loss:1.540, val_acc:0.635]
Epoch [3/120    avg_loss:1.385, val_acc:0.701]
Epoch [4/120    avg_loss:1.108, val_acc:0.607]
Epoch [5/120    avg_loss:1.094, val_acc:0.674]
Epoch [6/120    avg_loss:1.042, val_acc:0.791]
Epoch [7/120    avg_loss:0.764, val_acc:0.809]
Epoch [8/120    avg_loss:0.740, val_acc:0.824]
Epoch [9/120    avg_loss:0.569, val_acc:0.835]
Epoch [10/120    avg_loss:0.634, val_acc:0.784]
Epoch [11/120    avg_loss:0.428, val_acc:0.836]
Epoch [12/120    avg_loss:0.465, val_acc:0.845]
Epoch [13/120    avg_loss:0.367, val_acc:0.891]
Epoch [14/120    avg_loss:0.323, val_acc:0.769]
Epoch [15/120    avg_loss:0.276, val_acc:0.871]
Epoch [16/120    avg_loss:0.312, val_acc:0.881]
Epoch [17/120    avg_loss:0.284, val_acc:0.884]
Epoch [18/120    avg_loss:0.166, val_acc:0.910]
Epoch [19/120    avg_loss:0.145, val_acc:0.908]
Epoch [20/120    avg_loss:0.218, val_acc:0.903]
Epoch [21/120    avg_loss:0.158, val_acc:0.927]
Epoch [22/120    avg_loss:0.154, val_acc:0.906]
Epoch [23/120    avg_loss:0.133, val_acc:0.914]
Epoch [24/120    avg_loss:0.094, val_acc:0.931]
Epoch [25/120    avg_loss:0.081, val_acc:0.948]
Epoch [26/120    avg_loss:0.076, val_acc:0.893]
Epoch [27/120    avg_loss:0.089, val_acc:0.938]
Epoch [28/120    avg_loss:0.066, val_acc:0.936]
Epoch [29/120    avg_loss:0.062, val_acc:0.936]
Epoch [30/120    avg_loss:0.125, val_acc:0.929]
Epoch [31/120    avg_loss:0.351, val_acc:0.881]
Epoch [32/120    avg_loss:0.155, val_acc:0.949]
Epoch [33/120    avg_loss:0.126, val_acc:0.904]
Epoch [34/120    avg_loss:0.060, val_acc:0.939]
Epoch [35/120    avg_loss:0.062, val_acc:0.947]
Epoch [36/120    avg_loss:0.138, val_acc:0.918]
Epoch [37/120    avg_loss:0.090, val_acc:0.951]
Epoch [38/120    avg_loss:0.064, val_acc:0.951]
Epoch [39/120    avg_loss:0.056, val_acc:0.947]
Epoch [40/120    avg_loss:0.068, val_acc:0.954]
Epoch [41/120    avg_loss:0.057, val_acc:0.964]
Epoch [42/120    avg_loss:0.044, val_acc:0.964]
Epoch [43/120    avg_loss:0.033, val_acc:0.965]
Epoch [44/120    avg_loss:0.022, val_acc:0.966]
Epoch [45/120    avg_loss:0.015, val_acc:0.969]
Epoch [46/120    avg_loss:0.058, val_acc:0.950]
Epoch [47/120    avg_loss:0.060, val_acc:0.958]
Epoch [48/120    avg_loss:0.038, val_acc:0.955]
Epoch [49/120    avg_loss:0.029, val_acc:0.956]
Epoch [50/120    avg_loss:0.019, val_acc:0.973]
Epoch [51/120    avg_loss:0.020, val_acc:0.973]
Epoch [52/120    avg_loss:0.022, val_acc:0.966]
Epoch [53/120    avg_loss:0.013, val_acc:0.972]
Epoch [54/120    avg_loss:0.020, val_acc:0.972]
Epoch [55/120    avg_loss:0.021, val_acc:0.976]
Epoch [56/120    avg_loss:0.016, val_acc:0.968]
Epoch [57/120    avg_loss:0.015, val_acc:0.975]
Epoch [58/120    avg_loss:0.020, val_acc:0.969]
Epoch [59/120    avg_loss:0.019, val_acc:0.957]
Epoch [60/120    avg_loss:0.018, val_acc:0.962]
Epoch [61/120    avg_loss:0.020, val_acc:0.967]
Epoch [62/120    avg_loss:0.007, val_acc:0.971]
Epoch [63/120    avg_loss:0.015, val_acc:0.976]
Epoch [64/120    avg_loss:0.006, val_acc:0.975]
Epoch [65/120    avg_loss:0.011, val_acc:0.964]
Epoch [66/120    avg_loss:0.010, val_acc:0.973]
Epoch [67/120    avg_loss:0.008, val_acc:0.969]
Epoch [68/120    avg_loss:0.007, val_acc:0.975]
Epoch [69/120    avg_loss:0.026, val_acc:0.954]
Epoch [70/120    avg_loss:0.024, val_acc:0.965]
Epoch [71/120    avg_loss:0.012, val_acc:0.957]
Epoch [72/120    avg_loss:0.011, val_acc:0.959]
Epoch [73/120    avg_loss:0.034, val_acc:0.935]
Epoch [74/120    avg_loss:0.075, val_acc:0.946]
Epoch [75/120    avg_loss:0.038, val_acc:0.957]
Epoch [76/120    avg_loss:0.016, val_acc:0.968]
Epoch [77/120    avg_loss:0.012, val_acc:0.970]
Epoch [78/120    avg_loss:0.020, val_acc:0.973]
Epoch [79/120    avg_loss:0.009, val_acc:0.973]
Epoch [80/120    avg_loss:0.007, val_acc:0.973]
Epoch [81/120    avg_loss:0.008, val_acc:0.974]
Epoch [82/120    avg_loss:0.008, val_acc:0.974]
Epoch [83/120    avg_loss:0.014, val_acc:0.977]
Epoch [84/120    avg_loss:0.011, val_acc:0.977]
Epoch [85/120    avg_loss:0.007, val_acc:0.976]
Epoch [86/120    avg_loss:0.008, val_acc:0.976]
Epoch [87/120    avg_loss:0.009, val_acc:0.977]
Epoch [88/120    avg_loss:0.005, val_acc:0.977]
Epoch [89/120    avg_loss:0.005, val_acc:0.977]
Epoch [90/120    avg_loss:0.006, val_acc:0.977]
Epoch [91/120    avg_loss:0.005, val_acc:0.977]
Epoch [92/120    avg_loss:0.005, val_acc:0.977]
Epoch [93/120    avg_loss:0.004, val_acc:0.977]
Epoch [94/120    avg_loss:0.005, val_acc:0.977]
Epoch [95/120    avg_loss:0.006, val_acc:0.977]
Epoch [96/120    avg_loss:0.005, val_acc:0.976]
Epoch [97/120    avg_loss:0.004, val_acc:0.976]
Epoch [98/120    avg_loss:0.004, val_acc:0.976]
Epoch [99/120    avg_loss:0.005, val_acc:0.976]
Epoch [100/120    avg_loss:0.014, val_acc:0.977]
Epoch [101/120    avg_loss:0.006, val_acc:0.976]
Epoch [102/120    avg_loss:0.003, val_acc:0.975]
Epoch [103/120    avg_loss:0.004, val_acc:0.975]
Epoch [104/120    avg_loss:0.008, val_acc:0.977]
Epoch [105/120    avg_loss:0.006, val_acc:0.975]
Epoch [106/120    avg_loss:0.005, val_acc:0.975]
Epoch [107/120    avg_loss:0.008, val_acc:0.974]
Epoch [108/120    avg_loss:0.005, val_acc:0.973]
Epoch [109/120    avg_loss:0.007, val_acc:0.973]
Epoch [110/120    avg_loss:0.006, val_acc:0.974]
Epoch [111/120    avg_loss:0.005, val_acc:0.974]
Epoch [112/120    avg_loss:0.005, val_acc:0.974]
Epoch [113/120    avg_loss:0.006, val_acc:0.974]
Epoch [114/120    avg_loss:0.005, val_acc:0.975]
Epoch [115/120    avg_loss:0.004, val_acc:0.975]
Epoch [116/120    avg_loss:0.004, val_acc:0.975]
Epoch [117/120    avg_loss:0.006, val_acc:0.976]
Epoch [118/120    avg_loss:0.003, val_acc:0.976]
Epoch [119/120    avg_loss:0.004, val_acc:0.976]
Epoch [120/120    avg_loss:0.004, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    1 1265    0    1    0    3    0    0    0    8    6    1    0
     0    0    0]
 [   0    0    0  702    4    0    0    0    0    2    0    3   36    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    3    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    3    0    0   14    0    0    0    0
     0    1    0]
 [   0    0   12    0    0    0    1    0    0    0  841   21    0    0
     0    0    0]
 [   0    0    6    0    0    6    0    0    0    0    4 2174   20    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  530    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    1    0    1    0    0    0    0    0
  1123   11    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    42  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.63685636856368

F1 scores:
[       nan 0.98795181 0.98520249 0.9689441  0.98839907 0.98514286
 0.98574644 0.94339623 0.99883856 0.82352941 0.97337963 0.98504758
 0.94390027 1.         0.97440347 0.90045942 0.96470588]

Kappa:
0.973060326061883
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f387ad9d668>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.169, val_acc:0.551]
Epoch [2/120    avg_loss:1.678, val_acc:0.382]
Epoch [3/120    avg_loss:1.581, val_acc:0.646]
Epoch [4/120    avg_loss:1.259, val_acc:0.713]
Epoch [5/120    avg_loss:1.059, val_acc:0.727]
Epoch [6/120    avg_loss:0.923, val_acc:0.725]
Epoch [7/120    avg_loss:0.894, val_acc:0.655]
Epoch [8/120    avg_loss:0.741, val_acc:0.802]
Epoch [9/120    avg_loss:0.904, val_acc:0.741]
Epoch [10/120    avg_loss:0.739, val_acc:0.753]
Epoch [11/120    avg_loss:0.577, val_acc:0.850]
Epoch [12/120    avg_loss:0.459, val_acc:0.860]
Epoch [13/120    avg_loss:0.389, val_acc:0.848]
Epoch [14/120    avg_loss:0.419, val_acc:0.887]
Epoch [15/120    avg_loss:0.296, val_acc:0.862]
Epoch [16/120    avg_loss:0.243, val_acc:0.815]
Epoch [17/120    avg_loss:0.272, val_acc:0.816]
Epoch [18/120    avg_loss:0.288, val_acc:0.880]
Epoch [19/120    avg_loss:0.268, val_acc:0.897]
Epoch [20/120    avg_loss:0.356, val_acc:0.878]
Epoch [21/120    avg_loss:0.194, val_acc:0.911]
Epoch [22/120    avg_loss:0.141, val_acc:0.943]
Epoch [23/120    avg_loss:0.133, val_acc:0.937]
Epoch [24/120    avg_loss:0.183, val_acc:0.908]
Epoch [25/120    avg_loss:0.163, val_acc:0.904]
Epoch [26/120    avg_loss:0.158, val_acc:0.932]
Epoch [27/120    avg_loss:0.178, val_acc:0.914]
Epoch [28/120    avg_loss:0.158, val_acc:0.900]
Epoch [29/120    avg_loss:0.126, val_acc:0.913]
Epoch [30/120    avg_loss:0.091, val_acc:0.947]
Epoch [31/120    avg_loss:0.070, val_acc:0.958]
Epoch [32/120    avg_loss:0.054, val_acc:0.950]
Epoch [33/120    avg_loss:0.217, val_acc:0.941]
Epoch [34/120    avg_loss:0.105, val_acc:0.951]
Epoch [35/120    avg_loss:0.076, val_acc:0.955]
Epoch [36/120    avg_loss:0.042, val_acc:0.966]
Epoch [37/120    avg_loss:0.054, val_acc:0.962]
Epoch [38/120    avg_loss:0.049, val_acc:0.916]
Epoch [39/120    avg_loss:0.051, val_acc:0.966]
Epoch [40/120    avg_loss:0.066, val_acc:0.917]
Epoch [41/120    avg_loss:0.112, val_acc:0.943]
Epoch [42/120    avg_loss:0.090, val_acc:0.930]
Epoch [43/120    avg_loss:0.054, val_acc:0.961]
Epoch [44/120    avg_loss:0.030, val_acc:0.966]
Epoch [45/120    avg_loss:0.033, val_acc:0.967]
Epoch [46/120    avg_loss:0.034, val_acc:0.899]
Epoch [47/120    avg_loss:0.074, val_acc:0.898]
Epoch [48/120    avg_loss:0.077, val_acc:0.968]
Epoch [49/120    avg_loss:0.022, val_acc:0.968]
Epoch [50/120    avg_loss:0.072, val_acc:0.958]
Epoch [51/120    avg_loss:0.039, val_acc:0.963]
Epoch [52/120    avg_loss:0.036, val_acc:0.961]
Epoch [53/120    avg_loss:0.038, val_acc:0.964]
Epoch [54/120    avg_loss:0.027, val_acc:0.966]
Epoch [55/120    avg_loss:0.033, val_acc:0.971]
Epoch [56/120    avg_loss:0.035, val_acc:0.966]
Epoch [57/120    avg_loss:0.040, val_acc:0.964]
Epoch [58/120    avg_loss:0.063, val_acc:0.958]
Epoch [59/120    avg_loss:0.032, val_acc:0.967]
Epoch [60/120    avg_loss:0.023, val_acc:0.962]
Epoch [61/120    avg_loss:0.025, val_acc:0.964]
Epoch [62/120    avg_loss:0.018, val_acc:0.976]
Epoch [63/120    avg_loss:0.015, val_acc:0.973]
Epoch [64/120    avg_loss:0.018, val_acc:0.948]
Epoch [65/120    avg_loss:0.026, val_acc:0.935]
Epoch [66/120    avg_loss:0.042, val_acc:0.955]
Epoch [67/120    avg_loss:0.022, val_acc:0.967]
Epoch [68/120    avg_loss:0.056, val_acc:0.942]
Epoch [69/120    avg_loss:0.053, val_acc:0.962]
Epoch [70/120    avg_loss:0.013, val_acc:0.969]
Epoch [71/120    avg_loss:0.015, val_acc:0.968]
Epoch [72/120    avg_loss:0.019, val_acc:0.967]
Epoch [73/120    avg_loss:0.021, val_acc:0.966]
Epoch [74/120    avg_loss:0.015, val_acc:0.971]
Epoch [75/120    avg_loss:0.014, val_acc:0.969]
Epoch [76/120    avg_loss:0.016, val_acc:0.972]
Epoch [77/120    avg_loss:0.010, val_acc:0.974]
Epoch [78/120    avg_loss:0.006, val_acc:0.973]
Epoch [79/120    avg_loss:0.009, val_acc:0.975]
Epoch [80/120    avg_loss:0.005, val_acc:0.977]
Epoch [81/120    avg_loss:0.005, val_acc:0.975]
Epoch [82/120    avg_loss:0.007, val_acc:0.974]
Epoch [83/120    avg_loss:0.005, val_acc:0.975]
Epoch [84/120    avg_loss:0.006, val_acc:0.976]
Epoch [85/120    avg_loss:0.004, val_acc:0.976]
Epoch [86/120    avg_loss:0.005, val_acc:0.977]
Epoch [87/120    avg_loss:0.027, val_acc:0.979]
Epoch [88/120    avg_loss:0.006, val_acc:0.979]
Epoch [89/120    avg_loss:0.004, val_acc:0.979]
Epoch [90/120    avg_loss:0.005, val_acc:0.978]
Epoch [91/120    avg_loss:0.012, val_acc:0.977]
Epoch [92/120    avg_loss:0.005, val_acc:0.978]
Epoch [93/120    avg_loss:0.006, val_acc:0.978]
Epoch [94/120    avg_loss:0.009, val_acc:0.980]
Epoch [95/120    avg_loss:0.004, val_acc:0.981]
Epoch [96/120    avg_loss:0.005, val_acc:0.981]
Epoch [97/120    avg_loss:0.003, val_acc:0.981]
Epoch [98/120    avg_loss:0.004, val_acc:0.981]
Epoch [99/120    avg_loss:0.004, val_acc:0.978]
Epoch [100/120    avg_loss:0.004, val_acc:0.981]
Epoch [101/120    avg_loss:0.009, val_acc:0.979]
Epoch [102/120    avg_loss:0.010, val_acc:0.978]
Epoch [103/120    avg_loss:0.006, val_acc:0.978]
Epoch [104/120    avg_loss:0.003, val_acc:0.978]
Epoch [105/120    avg_loss:0.003, val_acc:0.978]
Epoch [106/120    avg_loss:0.003, val_acc:0.978]
Epoch [107/120    avg_loss:0.003, val_acc:0.978]
Epoch [108/120    avg_loss:0.004, val_acc:0.979]
Epoch [109/120    avg_loss:0.005, val_acc:0.980]
Epoch [110/120    avg_loss:0.006, val_acc:0.980]
Epoch [111/120    avg_loss:0.003, val_acc:0.980]
Epoch [112/120    avg_loss:0.005, val_acc:0.980]
Epoch [113/120    avg_loss:0.007, val_acc:0.979]
Epoch [114/120    avg_loss:0.004, val_acc:0.979]
Epoch [115/120    avg_loss:0.006, val_acc:0.979]
Epoch [116/120    avg_loss:0.006, val_acc:0.979]
Epoch [117/120    avg_loss:0.002, val_acc:0.979]
Epoch [118/120    avg_loss:0.002, val_acc:0.979]
Epoch [119/120    avg_loss:0.007, val_acc:0.979]
Epoch [120/120    avg_loss:0.003, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1272    1    3    0    1    0    0    0    5    3    0    0
     0    0    0]
 [   0    0    0  735    3    2    0    0    0    1    1    1    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    1    0    0    0  852   13    0    0
     3    2    0]
 [   0    2   19    1    0    0    0    0    0    0   10 2145   32    0
     1    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  532    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   17    0    0    0    0    0    0    0    0
  1107   15    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    21  313    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    8    0
     0    0   76]]

Accuracy:
97.94037940379404

F1 scores:
[       nan 0.96385542 0.98604651 0.99056604 0.98611111 0.97862767
 0.98871332 1.         1.         0.97297297 0.97706422 0.98124428
 0.95855856 1.         0.97490092 0.92466765 0.9382716 ]

Kappa:
0.97654310534115
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe3cebc45c0>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.108, val_acc:0.475]
Epoch [2/120    avg_loss:1.663, val_acc:0.505]
Epoch [3/120    avg_loss:1.327, val_acc:0.665]
Epoch [4/120    avg_loss:1.288, val_acc:0.620]
Epoch [5/120    avg_loss:1.114, val_acc:0.668]
Epoch [6/120    avg_loss:0.876, val_acc:0.708]
Epoch [7/120    avg_loss:0.782, val_acc:0.731]
Epoch [8/120    avg_loss:0.902, val_acc:0.786]
Epoch [9/120    avg_loss:0.673, val_acc:0.782]
Epoch [10/120    avg_loss:0.653, val_acc:0.824]
Epoch [11/120    avg_loss:0.526, val_acc:0.828]
Epoch [12/120    avg_loss:0.401, val_acc:0.720]
Epoch [13/120    avg_loss:0.460, val_acc:0.827]
Epoch [14/120    avg_loss:0.245, val_acc:0.896]
Epoch [15/120    avg_loss:0.345, val_acc:0.855]
Epoch [16/120    avg_loss:0.357, val_acc:0.784]
Epoch [17/120    avg_loss:0.246, val_acc:0.881]
Epoch [18/120    avg_loss:0.199, val_acc:0.907]
Epoch [19/120    avg_loss:0.259, val_acc:0.870]
Epoch [20/120    avg_loss:0.201, val_acc:0.917]
Epoch [21/120    avg_loss:0.127, val_acc:0.914]
Epoch [22/120    avg_loss:0.259, val_acc:0.857]
Epoch [23/120    avg_loss:0.135, val_acc:0.869]
Epoch [24/120    avg_loss:0.111, val_acc:0.918]
Epoch [25/120    avg_loss:0.110, val_acc:0.931]
Epoch [26/120    avg_loss:0.092, val_acc:0.930]
Epoch [27/120    avg_loss:0.197, val_acc:0.903]
Epoch [28/120    avg_loss:0.131, val_acc:0.886]
Epoch [29/120    avg_loss:0.088, val_acc:0.914]
Epoch [30/120    avg_loss:0.098, val_acc:0.927]
Epoch [31/120    avg_loss:0.069, val_acc:0.949]
Epoch [32/120    avg_loss:0.080, val_acc:0.928]
Epoch [33/120    avg_loss:0.066, val_acc:0.929]
Epoch [34/120    avg_loss:0.059, val_acc:0.926]
Epoch [35/120    avg_loss:0.056, val_acc:0.920]
Epoch [36/120    avg_loss:0.104, val_acc:0.890]
Epoch [37/120    avg_loss:0.089, val_acc:0.927]
Epoch [38/120    avg_loss:0.071, val_acc:0.883]
Epoch [39/120    avg_loss:0.057, val_acc:0.954]
Epoch [40/120    avg_loss:0.072, val_acc:0.925]
Epoch [41/120    avg_loss:0.058, val_acc:0.945]
Epoch [42/120    avg_loss:0.029, val_acc:0.954]
Epoch [43/120    avg_loss:0.021, val_acc:0.960]
Epoch [44/120    avg_loss:0.066, val_acc:0.951]
Epoch [45/120    avg_loss:0.041, val_acc:0.953]
Epoch [46/120    avg_loss:0.040, val_acc:0.959]
Epoch [47/120    avg_loss:0.024, val_acc:0.964]
Epoch [48/120    avg_loss:0.059, val_acc:0.940]
Epoch [49/120    avg_loss:0.046, val_acc:0.939]
Epoch [50/120    avg_loss:0.025, val_acc:0.960]
Epoch [51/120    avg_loss:0.014, val_acc:0.957]
Epoch [52/120    avg_loss:0.055, val_acc:0.943]
Epoch [53/120    avg_loss:0.033, val_acc:0.958]
Epoch [54/120    avg_loss:0.025, val_acc:0.947]
Epoch [55/120    avg_loss:0.025, val_acc:0.965]
Epoch [56/120    avg_loss:0.024, val_acc:0.965]
Epoch [57/120    avg_loss:0.016, val_acc:0.960]
Epoch [58/120    avg_loss:0.016, val_acc:0.961]
Epoch [59/120    avg_loss:0.051, val_acc:0.935]
Epoch [60/120    avg_loss:0.023, val_acc:0.961]
Epoch [61/120    avg_loss:0.010, val_acc:0.964]
Epoch [62/120    avg_loss:0.012, val_acc:0.961]
Epoch [63/120    avg_loss:0.010, val_acc:0.971]
Epoch [64/120    avg_loss:0.029, val_acc:0.966]
Epoch [65/120    avg_loss:0.031, val_acc:0.965]
Epoch [66/120    avg_loss:0.012, val_acc:0.966]
Epoch [67/120    avg_loss:0.035, val_acc:0.972]
Epoch [68/120    avg_loss:0.030, val_acc:0.929]
Epoch [69/120    avg_loss:0.245, val_acc:0.943]
Epoch [70/120    avg_loss:0.062, val_acc:0.960]
Epoch [71/120    avg_loss:0.036, val_acc:0.949]
Epoch [72/120    avg_loss:0.069, val_acc:0.952]
Epoch [73/120    avg_loss:0.027, val_acc:0.950]
Epoch [74/120    avg_loss:0.033, val_acc:0.942]
Epoch [75/120    avg_loss:0.039, val_acc:0.955]
Epoch [76/120    avg_loss:0.023, val_acc:0.964]
Epoch [77/120    avg_loss:0.016, val_acc:0.953]
Epoch [78/120    avg_loss:0.025, val_acc:0.956]
Epoch [79/120    avg_loss:0.019, val_acc:0.960]
Epoch [80/120    avg_loss:0.012, val_acc:0.965]
Epoch [81/120    avg_loss:0.015, val_acc:0.966]
Epoch [82/120    avg_loss:0.008, val_acc:0.966]
Epoch [83/120    avg_loss:0.008, val_acc:0.966]
Epoch [84/120    avg_loss:0.015, val_acc:0.966]
Epoch [85/120    avg_loss:0.012, val_acc:0.970]
Epoch [86/120    avg_loss:0.007, val_acc:0.969]
Epoch [87/120    avg_loss:0.005, val_acc:0.969]
Epoch [88/120    avg_loss:0.005, val_acc:0.969]
Epoch [89/120    avg_loss:0.007, val_acc:0.970]
Epoch [90/120    avg_loss:0.004, val_acc:0.970]
Epoch [91/120    avg_loss:0.009, val_acc:0.970]
Epoch [92/120    avg_loss:0.016, val_acc:0.966]
Epoch [93/120    avg_loss:0.005, val_acc:0.964]
Epoch [94/120    avg_loss:0.007, val_acc:0.964]
Epoch [95/120    avg_loss:0.003, val_acc:0.965]
Epoch [96/120    avg_loss:0.007, val_acc:0.964]
Epoch [97/120    avg_loss:0.005, val_acc:0.965]
Epoch [98/120    avg_loss:0.007, val_acc:0.965]
Epoch [99/120    avg_loss:0.005, val_acc:0.965]
Epoch [100/120    avg_loss:0.015, val_acc:0.965]
Epoch [101/120    avg_loss:0.008, val_acc:0.966]
Epoch [102/120    avg_loss:0.004, val_acc:0.966]
Epoch [103/120    avg_loss:0.005, val_acc:0.966]
Epoch [104/120    avg_loss:0.006, val_acc:0.966]
Epoch [105/120    avg_loss:0.007, val_acc:0.966]
Epoch [106/120    avg_loss:0.003, val_acc:0.966]
Epoch [107/120    avg_loss:0.004, val_acc:0.966]
Epoch [108/120    avg_loss:0.013, val_acc:0.966]
Epoch [109/120    avg_loss:0.007, val_acc:0.966]
Epoch [110/120    avg_loss:0.006, val_acc:0.966]
Epoch [111/120    avg_loss:0.006, val_acc:0.966]
Epoch [112/120    avg_loss:0.013, val_acc:0.966]
Epoch [113/120    avg_loss:0.010, val_acc:0.966]
Epoch [114/120    avg_loss:0.006, val_acc:0.966]
Epoch [115/120    avg_loss:0.008, val_acc:0.966]
Epoch [116/120    avg_loss:0.007, val_acc:0.966]
Epoch [117/120    avg_loss:0.008, val_acc:0.966]
Epoch [118/120    avg_loss:0.003, val_acc:0.966]
Epoch [119/120    avg_loss:0.016, val_acc:0.966]
Epoch [120/120    avg_loss:0.016, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1279    1    0    0    0    0    0    0    2    3    0    0
     0    0    0]
 [   0    0    0  722    1    0    0    0    0    4    0    0   20    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0  850   22    0    0
     1    0    0]
 [   0    7   15    0    0    0    0    0    0    0    9 2163   12    1
     1    2    0]
 [   0    0    0    0    0    0    0    0    0    0    3    0  525    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    1    0    0    0    0    0    0    0
  1121    9    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    35  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.03794037940379

F1 scores:
[       nan 0.90909091 0.99108873 0.98097826 0.99530516 0.98973774
 0.99169811 1.         1.         0.85       0.97701149 0.98362892
 0.96065874 0.99730458 0.97605572 0.91515152 0.95348837]

Kappa:
0.977638337662367
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f47a7fdc5f8>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.246, val_acc:0.445]
Epoch [2/120    avg_loss:1.713, val_acc:0.601]
Epoch [3/120    avg_loss:1.341, val_acc:0.720]
Epoch [4/120    avg_loss:1.178, val_acc:0.727]
Epoch [5/120    avg_loss:1.056, val_acc:0.663]
Epoch [6/120    avg_loss:0.924, val_acc:0.756]
Epoch [7/120    avg_loss:0.850, val_acc:0.814]
Epoch [8/120    avg_loss:0.807, val_acc:0.722]
Epoch [9/120    avg_loss:0.703, val_acc:0.782]
Epoch [10/120    avg_loss:0.665, val_acc:0.835]
Epoch [11/120    avg_loss:0.602, val_acc:0.767]
Epoch [12/120    avg_loss:0.449, val_acc:0.838]
Epoch [13/120    avg_loss:0.393, val_acc:0.884]
Epoch [14/120    avg_loss:0.393, val_acc:0.851]
Epoch [15/120    avg_loss:0.356, val_acc:0.899]
Epoch [16/120    avg_loss:0.270, val_acc:0.897]
Epoch [17/120    avg_loss:0.197, val_acc:0.854]
Epoch [18/120    avg_loss:0.349, val_acc:0.817]
Epoch [19/120    avg_loss:0.293, val_acc:0.852]
Epoch [20/120    avg_loss:0.289, val_acc:0.906]
Epoch [21/120    avg_loss:0.241, val_acc:0.839]
Epoch [22/120    avg_loss:0.245, val_acc:0.907]
Epoch [23/120    avg_loss:0.194, val_acc:0.902]
Epoch [24/120    avg_loss:0.175, val_acc:0.913]
Epoch [25/120    avg_loss:0.133, val_acc:0.916]
Epoch [26/120    avg_loss:0.120, val_acc:0.947]
Epoch [27/120    avg_loss:0.084, val_acc:0.936]
Epoch [28/120    avg_loss:0.201, val_acc:0.888]
Epoch [29/120    avg_loss:0.175, val_acc:0.928]
Epoch [30/120    avg_loss:0.233, val_acc:0.885]
Epoch [31/120    avg_loss:0.176, val_acc:0.952]
Epoch [32/120    avg_loss:0.087, val_acc:0.943]
Epoch [33/120    avg_loss:0.071, val_acc:0.925]
Epoch [34/120    avg_loss:0.080, val_acc:0.936]
Epoch [35/120    avg_loss:0.069, val_acc:0.952]
Epoch [36/120    avg_loss:0.074, val_acc:0.886]
Epoch [37/120    avg_loss:0.099, val_acc:0.932]
Epoch [38/120    avg_loss:0.063, val_acc:0.955]
Epoch [39/120    avg_loss:0.064, val_acc:0.950]
Epoch [40/120    avg_loss:0.064, val_acc:0.940]
Epoch [41/120    avg_loss:0.059, val_acc:0.961]
Epoch [42/120    avg_loss:0.059, val_acc:0.948]
Epoch [43/120    avg_loss:0.035, val_acc:0.957]
Epoch [44/120    avg_loss:0.043, val_acc:0.943]
Epoch [45/120    avg_loss:0.071, val_acc:0.925]
Epoch [46/120    avg_loss:0.033, val_acc:0.959]
Epoch [47/120    avg_loss:0.030, val_acc:0.961]
Epoch [48/120    avg_loss:0.028, val_acc:0.960]
Epoch [49/120    avg_loss:0.071, val_acc:0.962]
Epoch [50/120    avg_loss:0.028, val_acc:0.962]
Epoch [51/120    avg_loss:0.041, val_acc:0.960]
Epoch [52/120    avg_loss:0.048, val_acc:0.968]
Epoch [53/120    avg_loss:0.031, val_acc:0.950]
Epoch [54/120    avg_loss:0.031, val_acc:0.951]
Epoch [55/120    avg_loss:0.069, val_acc:0.955]
Epoch [56/120    avg_loss:0.025, val_acc:0.961]
Epoch [57/120    avg_loss:0.044, val_acc:0.963]
Epoch [58/120    avg_loss:0.027, val_acc:0.960]
Epoch [59/120    avg_loss:0.041, val_acc:0.964]
Epoch [60/120    avg_loss:0.025, val_acc:0.964]
Epoch [61/120    avg_loss:0.030, val_acc:0.969]
Epoch [62/120    avg_loss:0.020, val_acc:0.966]
Epoch [63/120    avg_loss:0.018, val_acc:0.967]
Epoch [64/120    avg_loss:0.017, val_acc:0.969]
Epoch [65/120    avg_loss:0.011, val_acc:0.973]
Epoch [66/120    avg_loss:0.008, val_acc:0.970]
Epoch [67/120    avg_loss:0.013, val_acc:0.975]
Epoch [68/120    avg_loss:0.010, val_acc:0.971]
Epoch [69/120    avg_loss:0.026, val_acc:0.970]
Epoch [70/120    avg_loss:0.029, val_acc:0.969]
Epoch [71/120    avg_loss:0.069, val_acc:0.966]
Epoch [72/120    avg_loss:0.042, val_acc:0.942]
Epoch [73/120    avg_loss:0.020, val_acc:0.970]
Epoch [74/120    avg_loss:0.013, val_acc:0.973]
Epoch [75/120    avg_loss:0.014, val_acc:0.929]
Epoch [76/120    avg_loss:0.019, val_acc:0.958]
Epoch [77/120    avg_loss:0.079, val_acc:0.959]
Epoch [78/120    avg_loss:0.051, val_acc:0.949]
Epoch [79/120    avg_loss:0.063, val_acc:0.962]
Epoch [80/120    avg_loss:0.060, val_acc:0.961]
Epoch [81/120    avg_loss:0.025, val_acc:0.967]
Epoch [82/120    avg_loss:0.024, val_acc:0.964]
Epoch [83/120    avg_loss:0.015, val_acc:0.965]
Epoch [84/120    avg_loss:0.013, val_acc:0.966]
Epoch [85/120    avg_loss:0.018, val_acc:0.967]
Epoch [86/120    avg_loss:0.022, val_acc:0.969]
Epoch [87/120    avg_loss:0.014, val_acc:0.970]
Epoch [88/120    avg_loss:0.008, val_acc:0.970]
Epoch [89/120    avg_loss:0.016, val_acc:0.972]
Epoch [90/120    avg_loss:0.011, val_acc:0.972]
Epoch [91/120    avg_loss:0.008, val_acc:0.973]
Epoch [92/120    avg_loss:0.009, val_acc:0.973]
Epoch [93/120    avg_loss:0.013, val_acc:0.972]
Epoch [94/120    avg_loss:0.010, val_acc:0.972]
Epoch [95/120    avg_loss:0.007, val_acc:0.971]
Epoch [96/120    avg_loss:0.007, val_acc:0.971]
Epoch [97/120    avg_loss:0.007, val_acc:0.971]
Epoch [98/120    avg_loss:0.007, val_acc:0.971]
Epoch [99/120    avg_loss:0.026, val_acc:0.971]
Epoch [100/120    avg_loss:0.018, val_acc:0.972]
Epoch [101/120    avg_loss:0.010, val_acc:0.971]
Epoch [102/120    avg_loss:0.010, val_acc:0.971]
Epoch [103/120    avg_loss:0.008, val_acc:0.971]
Epoch [104/120    avg_loss:0.011, val_acc:0.971]
Epoch [105/120    avg_loss:0.005, val_acc:0.971]
Epoch [106/120    avg_loss:0.011, val_acc:0.972]
Epoch [107/120    avg_loss:0.028, val_acc:0.972]
Epoch [108/120    avg_loss:0.018, val_acc:0.972]
Epoch [109/120    avg_loss:0.006, val_acc:0.972]
Epoch [110/120    avg_loss:0.014, val_acc:0.972]
Epoch [111/120    avg_loss:0.008, val_acc:0.972]
Epoch [112/120    avg_loss:0.014, val_acc:0.972]
Epoch [113/120    avg_loss:0.009, val_acc:0.972]
Epoch [114/120    avg_loss:0.005, val_acc:0.972]
Epoch [115/120    avg_loss:0.009, val_acc:0.972]
Epoch [116/120    avg_loss:0.007, val_acc:0.972]
Epoch [117/120    avg_loss:0.009, val_acc:0.972]
Epoch [118/120    avg_loss:0.009, val_acc:0.972]
Epoch [119/120    avg_loss:0.008, val_acc:0.972]
Epoch [120/120    avg_loss:0.011, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    6    5    0    1    0    0    0    9    3    0    0
     0    0    0]
 [   0    0    0  715    1    1    0    0    0    4    0    2   21    3
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    6    0    0    0    0    0    0  424    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  857   18    0    0
     0    0    0]
 [   0    0   14    0    0    0    0    0    0    0    9 2170   17    0
     0    0    0]
 [   0    0    0    0    1    0    0    0    0    0    0    4  520    0
     0    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   11    0    0    0    0    0    0    0    0
  1120    8    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    72  270    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.46341463414635

F1 scores:
[       nan 0.93181818 0.98515625 0.97278912 0.98148148 0.98524404
 0.99545455 1.         0.99297424 0.87179487 0.97942857 0.98479691
 0.95150961 0.9919571  0.96054889 0.864      0.94318182]

Kappa:
0.9710804853944897
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:122
Validation dataloader:122
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fbe69a416a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.319, val_acc:0.520]
Epoch [2/120    avg_loss:1.684, val_acc:0.583]
Epoch [3/120    avg_loss:1.289, val_acc:0.679]
Epoch [4/120    avg_loss:1.119, val_acc:0.551]
Epoch [5/120    avg_loss:1.073, val_acc:0.724]
Epoch [6/120    avg_loss:0.916, val_acc:0.755]
Epoch [7/120    avg_loss:0.773, val_acc:0.712]
Epoch [8/120    avg_loss:0.695, val_acc:0.726]
Epoch [9/120    avg_loss:0.703, val_acc:0.855]
Epoch [10/120    avg_loss:0.525, val_acc:0.814]
Epoch [11/120    avg_loss:0.504, val_acc:0.802]
Epoch [12/120    avg_loss:0.466, val_acc:0.885]
Epoch [13/120    avg_loss:0.469, val_acc:0.847]
Epoch [14/120    avg_loss:0.398, val_acc:0.851]
Epoch [15/120    avg_loss:0.545, val_acc:0.754]
Epoch [16/120    avg_loss:0.329, val_acc:0.862]
Epoch [17/120    avg_loss:0.280, val_acc:0.862]
Epoch [18/120    avg_loss:0.224, val_acc:0.886]
Epoch [19/120    avg_loss:0.224, val_acc:0.895]
Epoch [20/120    avg_loss:0.239, val_acc:0.835]
Epoch [21/120    avg_loss:0.306, val_acc:0.902]
Epoch [22/120    avg_loss:0.167, val_acc:0.918]
Epoch [23/120    avg_loss:0.147, val_acc:0.869]
Epoch [24/120    avg_loss:0.121, val_acc:0.932]
Epoch [25/120    avg_loss:0.117, val_acc:0.926]
Epoch [26/120    avg_loss:0.116, val_acc:0.912]
Epoch [27/120    avg_loss:0.168, val_acc:0.912]
Epoch [28/120    avg_loss:0.104, val_acc:0.931]
Epoch [29/120    avg_loss:0.086, val_acc:0.948]
Epoch [30/120    avg_loss:0.072, val_acc:0.938]
Epoch [31/120    avg_loss:0.094, val_acc:0.911]
Epoch [32/120    avg_loss:0.063, val_acc:0.933]
Epoch [33/120    avg_loss:0.071, val_acc:0.928]
Epoch [34/120    avg_loss:0.079, val_acc:0.941]
Epoch [35/120    avg_loss:0.053, val_acc:0.928]
Epoch [36/120    avg_loss:0.043, val_acc:0.964]
Epoch [37/120    avg_loss:0.105, val_acc:0.940]
Epoch [38/120    avg_loss:0.061, val_acc:0.938]
Epoch [39/120    avg_loss:0.100, val_acc:0.946]
Epoch [40/120    avg_loss:0.076, val_acc:0.942]
Epoch [41/120    avg_loss:0.085, val_acc:0.942]
Epoch [42/120    avg_loss:0.129, val_acc:0.932]
Epoch [43/120    avg_loss:0.072, val_acc:0.944]
Epoch [44/120    avg_loss:0.067, val_acc:0.949]
Epoch [45/120    avg_loss:0.032, val_acc:0.957]
Epoch [46/120    avg_loss:0.018, val_acc:0.944]
Epoch [47/120    avg_loss:0.063, val_acc:0.954]
Epoch [48/120    avg_loss:0.034, val_acc:0.892]
Epoch [49/120    avg_loss:0.072, val_acc:0.967]
Epoch [50/120    avg_loss:0.073, val_acc:0.939]
Epoch [51/120    avg_loss:0.096, val_acc:0.946]
Epoch [52/120    avg_loss:0.033, val_acc:0.959]
Epoch [53/120    avg_loss:0.040, val_acc:0.962]
Epoch [54/120    avg_loss:0.012, val_acc:0.966]
Epoch [55/120    avg_loss:0.019, val_acc:0.975]
Epoch [56/120    avg_loss:0.020, val_acc:0.959]
Epoch [57/120    avg_loss:0.016, val_acc:0.971]
Epoch [58/120    avg_loss:0.015, val_acc:0.968]
Epoch [59/120    avg_loss:0.013, val_acc:0.971]
Epoch [60/120    avg_loss:0.057, val_acc:0.937]
Epoch [61/120    avg_loss:0.090, val_acc:0.915]
Epoch [62/120    avg_loss:0.041, val_acc:0.958]
Epoch [63/120    avg_loss:0.016, val_acc:0.971]
Epoch [64/120    avg_loss:0.014, val_acc:0.972]
Epoch [65/120    avg_loss:0.014, val_acc:0.972]
Epoch [66/120    avg_loss:0.011, val_acc:0.970]
Epoch [67/120    avg_loss:0.021, val_acc:0.964]
Epoch [68/120    avg_loss:0.029, val_acc:0.968]
Epoch [69/120    avg_loss:0.020, val_acc:0.979]
Epoch [70/120    avg_loss:0.013, val_acc:0.979]
Epoch [71/120    avg_loss:0.009, val_acc:0.980]
Epoch [72/120    avg_loss:0.008, val_acc:0.979]
Epoch [73/120    avg_loss:0.006, val_acc:0.979]
Epoch [74/120    avg_loss:0.014, val_acc:0.978]
Epoch [75/120    avg_loss:0.008, val_acc:0.980]
Epoch [76/120    avg_loss:0.008, val_acc:0.977]
Epoch [77/120    avg_loss:0.011, val_acc:0.977]
Epoch [78/120    avg_loss:0.006, val_acc:0.978]
Epoch [79/120    avg_loss:0.004, val_acc:0.978]
Epoch [80/120    avg_loss:0.005, val_acc:0.978]
Epoch [81/120    avg_loss:0.006, val_acc:0.978]
Epoch [82/120    avg_loss:0.009, val_acc:0.978]
Epoch [83/120    avg_loss:0.009, val_acc:0.978]
Epoch [84/120    avg_loss:0.007, val_acc:0.979]
Epoch [85/120    avg_loss:0.006, val_acc:0.979]
Epoch [86/120    avg_loss:0.007, val_acc:0.979]
Epoch [87/120    avg_loss:0.006, val_acc:0.980]
Epoch [88/120    avg_loss:0.011, val_acc:0.978]
Epoch [89/120    avg_loss:0.003, val_acc:0.979]
Epoch [90/120    avg_loss:0.009, val_acc:0.978]
Epoch [91/120    avg_loss:0.008, val_acc:0.978]
Epoch [92/120    avg_loss:0.007, val_acc:0.979]
Epoch [93/120    avg_loss:0.003, val_acc:0.980]
Epoch [94/120    avg_loss:0.005, val_acc:0.980]
Epoch [95/120    avg_loss:0.004, val_acc:0.979]
Epoch [96/120    avg_loss:0.004, val_acc:0.979]
Epoch [97/120    avg_loss:0.008, val_acc:0.980]
Epoch [98/120    avg_loss:0.007, val_acc:0.979]
Epoch [99/120    avg_loss:0.008, val_acc:0.977]
Epoch [100/120    avg_loss:0.012, val_acc:0.978]
Epoch [101/120    avg_loss:0.007, val_acc:0.978]
Epoch [102/120    avg_loss:0.005, val_acc:0.978]
Epoch [103/120    avg_loss:0.010, val_acc:0.980]
Epoch [104/120    avg_loss:0.004, val_acc:0.981]
Epoch [105/120    avg_loss:0.011, val_acc:0.979]
Epoch [106/120    avg_loss:0.006, val_acc:0.978]
Epoch [107/120    avg_loss:0.007, val_acc:0.979]
Epoch [108/120    avg_loss:0.013, val_acc:0.980]
Epoch [109/120    avg_loss:0.005, val_acc:0.979]
Epoch [110/120    avg_loss:0.006, val_acc:0.979]
Epoch [111/120    avg_loss:0.007, val_acc:0.980]
Epoch [112/120    avg_loss:0.007, val_acc:0.980]
Epoch [113/120    avg_loss:0.004, val_acc:0.979]
Epoch [114/120    avg_loss:0.004, val_acc:0.980]
Epoch [115/120    avg_loss:0.003, val_acc:0.979]
Epoch [116/120    avg_loss:0.005, val_acc:0.980]
Epoch [117/120    avg_loss:0.004, val_acc:0.979]
Epoch [118/120    avg_loss:0.005, val_acc:0.979]
Epoch [119/120    avg_loss:0.004, val_acc:0.979]
Epoch [120/120    avg_loss:0.004, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1272    6    0    0    0    0    0    0    7    0    0    0
     0    0    0]
 [   0    0    0  737    0    0    0    0    0    3    0    3    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    2    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   10    1    0    0    1    0    0    0  832   30    1    0
     0    0    0]
 [   0    0   15    0    0    0    0    0    0    0    0 2178   17    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0    0  531    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1128   10    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    29  304    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.23306233062331

F1 scores:
[       nan 0.975      0.98490128 0.98727395 1.         1.
 0.98648649 1.         1.         0.83333333 0.97026239 0.98529744
 0.97431193 1.         0.9825784  0.91981846 0.97005988]

Kappa:
0.9798499067133128
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f721f6506a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.191, val_acc:0.591]
Epoch [2/120    avg_loss:1.618, val_acc:0.558]
Epoch [3/120    avg_loss:1.435, val_acc:0.639]
Epoch [4/120    avg_loss:1.106, val_acc:0.713]
Epoch [5/120    avg_loss:1.253, val_acc:0.681]
Epoch [6/120    avg_loss:0.944, val_acc:0.637]
Epoch [7/120    avg_loss:0.955, val_acc:0.799]
Epoch [8/120    avg_loss:0.745, val_acc:0.726]
Epoch [9/120    avg_loss:0.796, val_acc:0.775]
Epoch [10/120    avg_loss:0.516, val_acc:0.782]
Epoch [11/120    avg_loss:0.627, val_acc:0.868]
Epoch [12/120    avg_loss:0.379, val_acc:0.875]
Epoch [13/120    avg_loss:0.422, val_acc:0.871]
Epoch [14/120    avg_loss:0.281, val_acc:0.842]
Epoch [15/120    avg_loss:0.358, val_acc:0.867]
Epoch [16/120    avg_loss:0.224, val_acc:0.897]
Epoch [17/120    avg_loss:0.234, val_acc:0.874]
Epoch [18/120    avg_loss:0.287, val_acc:0.875]
Epoch [19/120    avg_loss:0.259, val_acc:0.886]
Epoch [20/120    avg_loss:0.162, val_acc:0.927]
Epoch [21/120    avg_loss:0.218, val_acc:0.897]
Epoch [22/120    avg_loss:0.283, val_acc:0.899]
Epoch [23/120    avg_loss:0.174, val_acc:0.838]
Epoch [24/120    avg_loss:0.213, val_acc:0.929]
Epoch [25/120    avg_loss:0.091, val_acc:0.953]
Epoch [26/120    avg_loss:0.090, val_acc:0.941]
Epoch [27/120    avg_loss:0.101, val_acc:0.927]
Epoch [28/120    avg_loss:0.070, val_acc:0.950]
Epoch [29/120    avg_loss:0.076, val_acc:0.945]
Epoch [30/120    avg_loss:0.071, val_acc:0.947]
Epoch [31/120    avg_loss:0.085, val_acc:0.950]
Epoch [32/120    avg_loss:0.081, val_acc:0.939]
Epoch [33/120    avg_loss:0.086, val_acc:0.951]
Epoch [34/120    avg_loss:0.044, val_acc:0.961]
Epoch [35/120    avg_loss:0.069, val_acc:0.959]
Epoch [36/120    avg_loss:0.066, val_acc:0.961]
Epoch [37/120    avg_loss:0.125, val_acc:0.951]
Epoch [38/120    avg_loss:0.035, val_acc:0.963]
Epoch [39/120    avg_loss:0.052, val_acc:0.956]
Epoch [40/120    avg_loss:0.030, val_acc:0.948]
Epoch [41/120    avg_loss:0.024, val_acc:0.956]
Epoch [42/120    avg_loss:0.017, val_acc:0.971]
Epoch [43/120    avg_loss:0.105, val_acc:0.956]
Epoch [44/120    avg_loss:0.030, val_acc:0.967]
Epoch [45/120    avg_loss:0.037, val_acc:0.971]
Epoch [46/120    avg_loss:0.028, val_acc:0.967]
Epoch [47/120    avg_loss:0.025, val_acc:0.975]
Epoch [48/120    avg_loss:0.099, val_acc:0.941]
Epoch [49/120    avg_loss:0.027, val_acc:0.965]
Epoch [50/120    avg_loss:0.044, val_acc:0.963]
Epoch [51/120    avg_loss:0.046, val_acc:0.970]
Epoch [52/120    avg_loss:0.038, val_acc:0.968]
Epoch [53/120    avg_loss:0.023, val_acc:0.953]
Epoch [54/120    avg_loss:0.050, val_acc:0.959]
Epoch [55/120    avg_loss:0.023, val_acc:0.970]
Epoch [56/120    avg_loss:0.025, val_acc:0.973]
Epoch [57/120    avg_loss:0.034, val_acc:0.979]
Epoch [58/120    avg_loss:0.014, val_acc:0.977]
Epoch [59/120    avg_loss:0.016, val_acc:0.956]
Epoch [60/120    avg_loss:0.022, val_acc:0.968]
Epoch [61/120    avg_loss:0.011, val_acc:0.976]
Epoch [62/120    avg_loss:0.013, val_acc:0.977]
Epoch [63/120    avg_loss:0.014, val_acc:0.976]
Epoch [64/120    avg_loss:0.051, val_acc:0.941]
Epoch [65/120    avg_loss:0.037, val_acc:0.966]
Epoch [66/120    avg_loss:0.025, val_acc:0.968]
Epoch [67/120    avg_loss:0.019, val_acc:0.974]
Epoch [68/120    avg_loss:0.013, val_acc:0.976]
Epoch [69/120    avg_loss:0.011, val_acc:0.968]
Epoch [70/120    avg_loss:0.006, val_acc:0.981]
Epoch [71/120    avg_loss:0.015, val_acc:0.970]
Epoch [72/120    avg_loss:0.022, val_acc:0.975]
Epoch [73/120    avg_loss:0.027, val_acc:0.977]
Epoch [74/120    avg_loss:0.008, val_acc:0.976]
Epoch [75/120    avg_loss:0.006, val_acc:0.980]
Epoch [76/120    avg_loss:0.006, val_acc:0.984]
Epoch [77/120    avg_loss:0.007, val_acc:0.981]
Epoch [78/120    avg_loss:0.008, val_acc:0.982]
Epoch [79/120    avg_loss:0.012, val_acc:0.978]
Epoch [80/120    avg_loss:0.017, val_acc:0.981]
Epoch [81/120    avg_loss:0.008, val_acc:0.983]
Epoch [82/120    avg_loss:0.006, val_acc:0.978]
Epoch [83/120    avg_loss:0.016, val_acc:0.969]
Epoch [84/120    avg_loss:0.018, val_acc:0.980]
Epoch [85/120    avg_loss:0.009, val_acc:0.958]
Epoch [86/120    avg_loss:0.008, val_acc:0.977]
Epoch [87/120    avg_loss:0.017, val_acc:0.947]
Epoch [88/120    avg_loss:0.014, val_acc:0.972]
Epoch [89/120    avg_loss:0.011, val_acc:0.980]
Epoch [90/120    avg_loss:0.005, val_acc:0.981]
Epoch [91/120    avg_loss:0.007, val_acc:0.980]
Epoch [92/120    avg_loss:0.008, val_acc:0.981]
Epoch [93/120    avg_loss:0.009, val_acc:0.979]
Epoch [94/120    avg_loss:0.004, val_acc:0.979]
Epoch [95/120    avg_loss:0.003, val_acc:0.980]
Epoch [96/120    avg_loss:0.005, val_acc:0.981]
Epoch [97/120    avg_loss:0.007, val_acc:0.981]
Epoch [98/120    avg_loss:0.003, val_acc:0.981]
Epoch [99/120    avg_loss:0.003, val_acc:0.981]
Epoch [100/120    avg_loss:0.007, val_acc:0.982]
Epoch [101/120    avg_loss:0.003, val_acc:0.981]
Epoch [102/120    avg_loss:0.003, val_acc:0.981]
Epoch [103/120    avg_loss:0.010, val_acc:0.981]
Epoch [104/120    avg_loss:0.003, val_acc:0.981]
Epoch [105/120    avg_loss:0.004, val_acc:0.981]
Epoch [106/120    avg_loss:0.003, val_acc:0.981]
Epoch [107/120    avg_loss:0.003, val_acc:0.981]
Epoch [108/120    avg_loss:0.003, val_acc:0.981]
Epoch [109/120    avg_loss:0.003, val_acc:0.981]
Epoch [110/120    avg_loss:0.004, val_acc:0.981]
Epoch [111/120    avg_loss:0.010, val_acc:0.981]
Epoch [112/120    avg_loss:0.006, val_acc:0.981]
Epoch [113/120    avg_loss:0.008, val_acc:0.981]
Epoch [114/120    avg_loss:0.004, val_acc:0.981]
Epoch [115/120    avg_loss:0.003, val_acc:0.981]
Epoch [116/120    avg_loss:0.006, val_acc:0.981]
Epoch [117/120    avg_loss:0.003, val_acc:0.981]
Epoch [118/120    avg_loss:0.003, val_acc:0.981]
Epoch [119/120    avg_loss:0.003, val_acc:0.981]
Epoch [120/120    avg_loss:0.006, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1277    2    0    0    0    0    0    0    2    1    0    0
     0    3    0]
 [   0    0    0  736    0    1    0    0    0    3    1    2    4    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   11    0    0    0    1    0    0    0  849   11    0    0
     2    1    0]
 [   0    0    6    0    0    0    0    0    0    0    1 2193    9    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    8    0  523    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    40  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.59078590785907

F1 scores:
[       nan 0.98765432 0.98992248 0.98858294 0.99528302 0.99655568
 0.99319728 0.98039216 1.         0.86486486 0.9781106  0.99298166
 0.97574627 0.99730458 0.98145752 0.92       0.9704142 ]

Kappa:
0.9839272211169818
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8015e666d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.240, val_acc:0.395]
Epoch [2/120    avg_loss:1.698, val_acc:0.642]
Epoch [3/120    avg_loss:1.430, val_acc:0.683]
Epoch [4/120    avg_loss:1.118, val_acc:0.689]
Epoch [5/120    avg_loss:0.970, val_acc:0.726]
Epoch [6/120    avg_loss:0.924, val_acc:0.753]
Epoch [7/120    avg_loss:0.903, val_acc:0.709]
Epoch [8/120    avg_loss:0.692, val_acc:0.813]
Epoch [9/120    avg_loss:0.587, val_acc:0.826]
Epoch [10/120    avg_loss:0.583, val_acc:0.814]
Epoch [11/120    avg_loss:0.482, val_acc:0.804]
Epoch [12/120    avg_loss:0.652, val_acc:0.753]
Epoch [13/120    avg_loss:0.516, val_acc:0.842]
Epoch [14/120    avg_loss:0.383, val_acc:0.827]
Epoch [15/120    avg_loss:0.282, val_acc:0.866]
Epoch [16/120    avg_loss:0.299, val_acc:0.899]
Epoch [17/120    avg_loss:0.136, val_acc:0.933]
Epoch [18/120    avg_loss:0.358, val_acc:0.891]
Epoch [19/120    avg_loss:0.203, val_acc:0.926]
Epoch [20/120    avg_loss:0.269, val_acc:0.910]
Epoch [21/120    avg_loss:0.239, val_acc:0.909]
Epoch [22/120    avg_loss:0.144, val_acc:0.935]
Epoch [23/120    avg_loss:0.141, val_acc:0.928]
Epoch [24/120    avg_loss:0.120, val_acc:0.928]
Epoch [25/120    avg_loss:0.126, val_acc:0.921]
Epoch [26/120    avg_loss:0.100, val_acc:0.928]
Epoch [27/120    avg_loss:0.086, val_acc:0.940]
Epoch [28/120    avg_loss:0.091, val_acc:0.922]
Epoch [29/120    avg_loss:0.123, val_acc:0.931]
Epoch [30/120    avg_loss:0.071, val_acc:0.958]
Epoch [31/120    avg_loss:0.077, val_acc:0.962]
Epoch [32/120    avg_loss:0.075, val_acc:0.944]
Epoch [33/120    avg_loss:0.078, val_acc:0.941]
Epoch [34/120    avg_loss:0.097, val_acc:0.942]
Epoch [35/120    avg_loss:0.079, val_acc:0.949]
Epoch [36/120    avg_loss:0.076, val_acc:0.952]
Epoch [37/120    avg_loss:0.056, val_acc:0.946]
Epoch [38/120    avg_loss:0.089, val_acc:0.956]
Epoch [39/120    avg_loss:0.031, val_acc:0.965]
Epoch [40/120    avg_loss:0.022, val_acc:0.965]
Epoch [41/120    avg_loss:0.019, val_acc:0.959]
Epoch [42/120    avg_loss:0.069, val_acc:0.954]
Epoch [43/120    avg_loss:0.069, val_acc:0.953]
Epoch [44/120    avg_loss:0.049, val_acc:0.955]
Epoch [45/120    avg_loss:0.043, val_acc:0.947]
Epoch [46/120    avg_loss:0.030, val_acc:0.963]
Epoch [47/120    avg_loss:0.042, val_acc:0.959]
Epoch [48/120    avg_loss:0.085, val_acc:0.959]
Epoch [49/120    avg_loss:0.045, val_acc:0.970]
Epoch [50/120    avg_loss:0.049, val_acc:0.960]
Epoch [51/120    avg_loss:0.047, val_acc:0.959]
Epoch [52/120    avg_loss:0.033, val_acc:0.973]
Epoch [53/120    avg_loss:0.027, val_acc:0.965]
Epoch [54/120    avg_loss:0.012, val_acc:0.976]
Epoch [55/120    avg_loss:0.021, val_acc:0.974]
Epoch [56/120    avg_loss:0.025, val_acc:0.965]
Epoch [57/120    avg_loss:0.027, val_acc:0.966]
Epoch [58/120    avg_loss:0.028, val_acc:0.961]
Epoch [59/120    avg_loss:0.021, val_acc:0.976]
Epoch [60/120    avg_loss:0.036, val_acc:0.960]
Epoch [61/120    avg_loss:0.016, val_acc:0.973]
Epoch [62/120    avg_loss:0.059, val_acc:0.960]
Epoch [63/120    avg_loss:0.028, val_acc:0.977]
Epoch [64/120    avg_loss:0.020, val_acc:0.975]
Epoch [65/120    avg_loss:0.043, val_acc:0.965]
Epoch [66/120    avg_loss:0.032, val_acc:0.959]
Epoch [67/120    avg_loss:0.043, val_acc:0.971]
Epoch [68/120    avg_loss:0.023, val_acc:0.976]
Epoch [69/120    avg_loss:0.032, val_acc:0.971]
Epoch [70/120    avg_loss:0.009, val_acc:0.977]
Epoch [71/120    avg_loss:0.022, val_acc:0.970]
Epoch [72/120    avg_loss:0.012, val_acc:0.974]
Epoch [73/120    avg_loss:0.011, val_acc:0.972]
Epoch [74/120    avg_loss:0.019, val_acc:0.978]
Epoch [75/120    avg_loss:0.012, val_acc:0.977]
Epoch [76/120    avg_loss:0.006, val_acc:0.979]
Epoch [77/120    avg_loss:0.008, val_acc:0.982]
Epoch [78/120    avg_loss:0.009, val_acc:0.973]
Epoch [79/120    avg_loss:0.022, val_acc:0.963]
Epoch [80/120    avg_loss:0.012, val_acc:0.979]
Epoch [81/120    avg_loss:0.031, val_acc:0.977]
Epoch [82/120    avg_loss:0.024, val_acc:0.967]
Epoch [83/120    avg_loss:0.037, val_acc:0.956]
Epoch [84/120    avg_loss:0.075, val_acc:0.965]
Epoch [85/120    avg_loss:0.027, val_acc:0.970]
Epoch [86/120    avg_loss:0.024, val_acc:0.959]
Epoch [87/120    avg_loss:0.015, val_acc:0.979]
Epoch [88/120    avg_loss:0.012, val_acc:0.961]
Epoch [89/120    avg_loss:0.010, val_acc:0.978]
Epoch [90/120    avg_loss:0.007, val_acc:0.983]
Epoch [91/120    avg_loss:0.023, val_acc:0.956]
Epoch [92/120    avg_loss:0.010, val_acc:0.978]
Epoch [93/120    avg_loss:0.021, val_acc:0.981]
Epoch [94/120    avg_loss:0.009, val_acc:0.981]
Epoch [95/120    avg_loss:0.014, val_acc:0.976]
Epoch [96/120    avg_loss:0.012, val_acc:0.987]
Epoch [97/120    avg_loss:0.010, val_acc:0.984]
Epoch [98/120    avg_loss:0.005, val_acc:0.981]
Epoch [99/120    avg_loss:0.013, val_acc:0.983]
Epoch [100/120    avg_loss:0.006, val_acc:0.983]
Epoch [101/120    avg_loss:0.013, val_acc:0.984]
Epoch [102/120    avg_loss:0.009, val_acc:0.981]
Epoch [103/120    avg_loss:0.005, val_acc:0.976]
Epoch [104/120    avg_loss:0.004, val_acc:0.980]
Epoch [105/120    avg_loss:0.003, val_acc:0.984]
Epoch [106/120    avg_loss:0.013, val_acc:0.975]
Epoch [107/120    avg_loss:0.008, val_acc:0.984]
Epoch [108/120    avg_loss:0.003, val_acc:0.983]
Epoch [109/120    avg_loss:0.006, val_acc:0.986]
Epoch [110/120    avg_loss:0.003, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.003, val_acc:0.987]
Epoch [115/120    avg_loss:0.002, val_acc:0.987]
Epoch [116/120    avg_loss:0.003, val_acc:0.987]
Epoch [117/120    avg_loss:0.005, val_acc:0.987]
Epoch [118/120    avg_loss:0.003, val_acc:0.986]
Epoch [119/120    avg_loss:0.004, val_acc:0.987]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1272    6    0    0    2    0    0    0    5    0    0    0
     0    0    0]
 [   0    0    0  743    0    0    0    0    0    2    0    0    0    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    0    0
     0    0    1]
 [   0    0    0    3    0    0    1    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  867    0    0    0
     0    4    0]
 [   0    0   20    0    0    3    0    0    0    0    5 2175    5    0
     0    2    0]
 [   0    0    0    0    0    0    0    0    0    0    4    0  528    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   18    0    0    1    0    0    0    0    0
  1113    7    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
     4  332    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.74254742547426

F1 scores:
[       nan 0.98765432 0.98566447 0.99132755 1.         0.97412823
 0.98945783 1.         0.99767442 0.77777778 0.9869095  0.99201824
 0.98876404 0.99462366 0.98670213 0.95953757 0.97647059]

Kappa:
0.9856756332347959
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f30d02fb6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.259, val_acc:0.497]
Epoch [2/120    avg_loss:1.748, val_acc:0.584]
Epoch [3/120    avg_loss:1.362, val_acc:0.602]
Epoch [4/120    avg_loss:1.253, val_acc:0.648]
Epoch [5/120    avg_loss:1.153, val_acc:0.709]
Epoch [6/120    avg_loss:0.976, val_acc:0.633]
Epoch [7/120    avg_loss:0.831, val_acc:0.771]
Epoch [8/120    avg_loss:0.633, val_acc:0.831]
Epoch [9/120    avg_loss:0.694, val_acc:0.727]
Epoch [10/120    avg_loss:0.671, val_acc:0.836]
Epoch [11/120    avg_loss:0.607, val_acc:0.840]
Epoch [12/120    avg_loss:0.479, val_acc:0.891]
Epoch [13/120    avg_loss:0.706, val_acc:0.841]
Epoch [14/120    avg_loss:0.480, val_acc:0.876]
Epoch [15/120    avg_loss:0.564, val_acc:0.817]
Epoch [16/120    avg_loss:0.398, val_acc:0.886]
Epoch [17/120    avg_loss:0.268, val_acc:0.919]
Epoch [18/120    avg_loss:0.323, val_acc:0.847]
Epoch [19/120    avg_loss:0.533, val_acc:0.880]
Epoch [20/120    avg_loss:0.295, val_acc:0.858]
Epoch [21/120    avg_loss:0.228, val_acc:0.921]
Epoch [22/120    avg_loss:0.176, val_acc:0.871]
Epoch [23/120    avg_loss:0.150, val_acc:0.921]
Epoch [24/120    avg_loss:0.177, val_acc:0.853]
Epoch [25/120    avg_loss:0.119, val_acc:0.930]
Epoch [26/120    avg_loss:0.102, val_acc:0.949]
Epoch [27/120    avg_loss:0.107, val_acc:0.946]
Epoch [28/120    avg_loss:0.093, val_acc:0.960]
Epoch [29/120    avg_loss:0.062, val_acc:0.917]
Epoch [30/120    avg_loss:0.094, val_acc:0.922]
Epoch [31/120    avg_loss:0.056, val_acc:0.948]
Epoch [32/120    avg_loss:0.061, val_acc:0.945]
Epoch [33/120    avg_loss:0.073, val_acc:0.935]
Epoch [34/120    avg_loss:0.107, val_acc:0.931]
Epoch [35/120    avg_loss:0.091, val_acc:0.960]
Epoch [36/120    avg_loss:0.036, val_acc:0.967]
Epoch [37/120    avg_loss:0.066, val_acc:0.947]
Epoch [38/120    avg_loss:0.062, val_acc:0.942]
Epoch [39/120    avg_loss:0.054, val_acc:0.953]
Epoch [40/120    avg_loss:0.053, val_acc:0.955]
Epoch [41/120    avg_loss:0.050, val_acc:0.968]
Epoch [42/120    avg_loss:0.035, val_acc:0.971]
Epoch [43/120    avg_loss:0.033, val_acc:0.943]
Epoch [44/120    avg_loss:0.028, val_acc:0.959]
Epoch [45/120    avg_loss:0.021, val_acc:0.968]
Epoch [46/120    avg_loss:0.034, val_acc:0.960]
Epoch [47/120    avg_loss:0.024, val_acc:0.969]
Epoch [48/120    avg_loss:0.040, val_acc:0.960]
Epoch [49/120    avg_loss:0.018, val_acc:0.971]
Epoch [50/120    avg_loss:0.034, val_acc:0.955]
Epoch [51/120    avg_loss:0.036, val_acc:0.964]
Epoch [52/120    avg_loss:0.061, val_acc:0.949]
Epoch [53/120    avg_loss:0.068, val_acc:0.941]
Epoch [54/120    avg_loss:0.060, val_acc:0.955]
Epoch [55/120    avg_loss:0.044, val_acc:0.935]
Epoch [56/120    avg_loss:0.043, val_acc:0.970]
Epoch [57/120    avg_loss:0.032, val_acc:0.966]
Epoch [58/120    avg_loss:0.029, val_acc:0.969]
Epoch [59/120    avg_loss:0.042, val_acc:0.955]
Epoch [60/120    avg_loss:0.013, val_acc:0.974]
Epoch [61/120    avg_loss:0.024, val_acc:0.967]
Epoch [62/120    avg_loss:0.022, val_acc:0.948]
Epoch [63/120    avg_loss:0.017, val_acc:0.969]
Epoch [64/120    avg_loss:0.046, val_acc:0.966]
Epoch [65/120    avg_loss:0.013, val_acc:0.973]
Epoch [66/120    avg_loss:0.017, val_acc:0.974]
Epoch [67/120    avg_loss:0.026, val_acc:0.947]
Epoch [68/120    avg_loss:0.020, val_acc:0.969]
Epoch [69/120    avg_loss:0.013, val_acc:0.977]
Epoch [70/120    avg_loss:0.014, val_acc:0.980]
Epoch [71/120    avg_loss:0.021, val_acc:0.901]
Epoch [72/120    avg_loss:0.125, val_acc:0.960]
Epoch [73/120    avg_loss:0.051, val_acc:0.972]
Epoch [74/120    avg_loss:0.031, val_acc:0.953]
Epoch [75/120    avg_loss:0.025, val_acc:0.969]
Epoch [76/120    avg_loss:0.019, val_acc:0.972]
Epoch [77/120    avg_loss:0.042, val_acc:0.965]
Epoch [78/120    avg_loss:0.132, val_acc:0.968]
Epoch [79/120    avg_loss:0.030, val_acc:0.965]
Epoch [80/120    avg_loss:0.028, val_acc:0.961]
Epoch [81/120    avg_loss:0.022, val_acc:0.966]
Epoch [82/120    avg_loss:0.018, val_acc:0.973]
Epoch [83/120    avg_loss:0.018, val_acc:0.976]
Epoch [84/120    avg_loss:0.011, val_acc:0.976]
Epoch [85/120    avg_loss:0.005, val_acc:0.976]
Epoch [86/120    avg_loss:0.011, val_acc:0.975]
Epoch [87/120    avg_loss:0.006, val_acc:0.977]
Epoch [88/120    avg_loss:0.008, val_acc:0.976]
Epoch [89/120    avg_loss:0.006, val_acc:0.977]
Epoch [90/120    avg_loss:0.006, val_acc:0.977]
Epoch [91/120    avg_loss:0.009, val_acc:0.976]
Epoch [92/120    avg_loss:0.007, val_acc:0.975]
Epoch [93/120    avg_loss:0.010, val_acc:0.977]
Epoch [94/120    avg_loss:0.007, val_acc:0.976]
Epoch [95/120    avg_loss:0.007, val_acc:0.977]
Epoch [96/120    avg_loss:0.008, val_acc:0.977]
Epoch [97/120    avg_loss:0.005, val_acc:0.977]
Epoch [98/120    avg_loss:0.006, val_acc:0.978]
Epoch [99/120    avg_loss:0.010, val_acc:0.977]
Epoch [100/120    avg_loss:0.004, val_acc:0.977]
Epoch [101/120    avg_loss:0.008, val_acc:0.978]
Epoch [102/120    avg_loss:0.005, val_acc:0.978]
Epoch [103/120    avg_loss:0.005, val_acc:0.978]
Epoch [104/120    avg_loss:0.005, val_acc:0.978]
Epoch [105/120    avg_loss:0.005, val_acc:0.978]
Epoch [106/120    avg_loss:0.008, val_acc:0.978]
Epoch [107/120    avg_loss:0.003, val_acc:0.978]
Epoch [108/120    avg_loss:0.005, val_acc:0.978]
Epoch [109/120    avg_loss:0.005, val_acc:0.978]
Epoch [110/120    avg_loss:0.007, val_acc:0.978]
Epoch [111/120    avg_loss:0.006, val_acc:0.978]
Epoch [112/120    avg_loss:0.007, val_acc:0.978]
Epoch [113/120    avg_loss:0.008, val_acc:0.978]
Epoch [114/120    avg_loss:0.005, val_acc:0.978]
Epoch [115/120    avg_loss:0.007, val_acc:0.978]
Epoch [116/120    avg_loss:0.004, val_acc:0.978]
Epoch [117/120    avg_loss:0.007, val_acc:0.978]
Epoch [118/120    avg_loss:0.019, val_acc:0.978]
Epoch [119/120    avg_loss:0.005, val_acc:0.978]
Epoch [120/120    avg_loss:0.005, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    2 1270    2    0    0    1    0    0    0    4    1    5    0
     0    0    0]
 [   0    0    0  712    0    5    0    0    0    4    1    0   21    4
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0   18    0    0    0    1    0    0    0  845    3    0    0
     3    5    0]
 [   0    0   26    0    0    0    2    0    0    0    1 2171    9    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0  528    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    23  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.22222222222223

F1 scores:
[       nan 0.97619048 0.97729896 0.97400821 0.99764706 0.99084668
 0.98871332 1.         1.         0.85       0.97857556 0.99019384
 0.96174863 0.98666667 0.98784722 0.94294294 0.96511628]

Kappa:
0.9797413266424855
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f42930f96a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.074, val_acc:0.404]
Epoch [2/120    avg_loss:1.736, val_acc:0.579]
Epoch [3/120    avg_loss:1.356, val_acc:0.628]
Epoch [4/120    avg_loss:1.201, val_acc:0.603]
Epoch [5/120    avg_loss:1.021, val_acc:0.715]
Epoch [6/120    avg_loss:0.864, val_acc:0.741]
Epoch [7/120    avg_loss:0.839, val_acc:0.744]
Epoch [8/120    avg_loss:0.694, val_acc:0.732]
Epoch [9/120    avg_loss:0.611, val_acc:0.739]
Epoch [10/120    avg_loss:0.501, val_acc:0.741]
Epoch [11/120    avg_loss:0.520, val_acc:0.787]
Epoch [12/120    avg_loss:0.551, val_acc:0.818]
Epoch [13/120    avg_loss:0.400, val_acc:0.868]
Epoch [14/120    avg_loss:0.372, val_acc:0.854]
Epoch [15/120    avg_loss:0.366, val_acc:0.869]
Epoch [16/120    avg_loss:0.283, val_acc:0.887]
Epoch [17/120    avg_loss:0.229, val_acc:0.875]
Epoch [18/120    avg_loss:0.368, val_acc:0.893]
Epoch [19/120    avg_loss:0.282, val_acc:0.828]
Epoch [20/120    avg_loss:0.240, val_acc:0.905]
Epoch [21/120    avg_loss:0.230, val_acc:0.856]
Epoch [22/120    avg_loss:0.222, val_acc:0.898]
Epoch [23/120    avg_loss:0.141, val_acc:0.914]
Epoch [24/120    avg_loss:0.197, val_acc:0.907]
Epoch [25/120    avg_loss:0.124, val_acc:0.921]
Epoch [26/120    avg_loss:0.119, val_acc:0.930]
Epoch [27/120    avg_loss:0.081, val_acc:0.925]
Epoch [28/120    avg_loss:0.074, val_acc:0.940]
Epoch [29/120    avg_loss:0.070, val_acc:0.947]
Epoch [30/120    avg_loss:0.087, val_acc:0.911]
Epoch [31/120    avg_loss:0.070, val_acc:0.934]
Epoch [32/120    avg_loss:0.072, val_acc:0.926]
Epoch [33/120    avg_loss:0.098, val_acc:0.938]
Epoch [34/120    avg_loss:0.052, val_acc:0.923]
Epoch [35/120    avg_loss:0.031, val_acc:0.945]
Epoch [36/120    avg_loss:0.030, val_acc:0.927]
Epoch [37/120    avg_loss:0.050, val_acc:0.955]
Epoch [38/120    avg_loss:0.023, val_acc:0.965]
Epoch [39/120    avg_loss:0.030, val_acc:0.971]
Epoch [40/120    avg_loss:0.033, val_acc:0.940]
Epoch [41/120    avg_loss:0.045, val_acc:0.967]
Epoch [42/120    avg_loss:0.032, val_acc:0.957]
Epoch [43/120    avg_loss:0.038, val_acc:0.967]
Epoch [44/120    avg_loss:0.025, val_acc:0.963]
Epoch [45/120    avg_loss:0.047, val_acc:0.967]
Epoch [46/120    avg_loss:0.144, val_acc:0.953]
Epoch [47/120    avg_loss:0.070, val_acc:0.953]
Epoch [48/120    avg_loss:0.050, val_acc:0.941]
Epoch [49/120    avg_loss:0.044, val_acc:0.958]
Epoch [50/120    avg_loss:0.015, val_acc:0.964]
Epoch [51/120    avg_loss:0.023, val_acc:0.957]
Epoch [52/120    avg_loss:0.038, val_acc:0.960]
Epoch [53/120    avg_loss:0.024, val_acc:0.968]
Epoch [54/120    avg_loss:0.024, val_acc:0.970]
Epoch [55/120    avg_loss:0.018, val_acc:0.972]
Epoch [56/120    avg_loss:0.015, val_acc:0.976]
Epoch [57/120    avg_loss:0.019, val_acc:0.975]
Epoch [58/120    avg_loss:0.016, val_acc:0.978]
Epoch [59/120    avg_loss:0.014, val_acc:0.977]
Epoch [60/120    avg_loss:0.013, val_acc:0.976]
Epoch [61/120    avg_loss:0.008, val_acc:0.975]
Epoch [62/120    avg_loss:0.007, val_acc:0.977]
Epoch [63/120    avg_loss:0.010, val_acc:0.977]
Epoch [64/120    avg_loss:0.007, val_acc:0.978]
Epoch [65/120    avg_loss:0.016, val_acc:0.977]
Epoch [66/120    avg_loss:0.006, val_acc:0.976]
Epoch [67/120    avg_loss:0.008, val_acc:0.978]
Epoch [68/120    avg_loss:0.013, val_acc:0.976]
Epoch [69/120    avg_loss:0.006, val_acc:0.977]
Epoch [70/120    avg_loss:0.009, val_acc:0.979]
Epoch [71/120    avg_loss:0.008, val_acc:0.980]
Epoch [72/120    avg_loss:0.009, val_acc:0.981]
Epoch [73/120    avg_loss:0.009, val_acc:0.982]
Epoch [74/120    avg_loss:0.014, val_acc:0.979]
Epoch [75/120    avg_loss:0.010, val_acc:0.980]
Epoch [76/120    avg_loss:0.007, val_acc:0.980]
Epoch [77/120    avg_loss:0.013, val_acc:0.980]
Epoch [78/120    avg_loss:0.008, val_acc:0.981]
Epoch [79/120    avg_loss:0.011, val_acc:0.978]
Epoch [80/120    avg_loss:0.007, val_acc:0.976]
Epoch [81/120    avg_loss:0.007, val_acc:0.976]
Epoch [82/120    avg_loss:0.010, val_acc:0.979]
Epoch [83/120    avg_loss:0.008, val_acc:0.975]
Epoch [84/120    avg_loss:0.009, val_acc:0.977]
Epoch [85/120    avg_loss:0.007, val_acc:0.979]
Epoch [86/120    avg_loss:0.005, val_acc:0.979]
Epoch [87/120    avg_loss:0.008, val_acc:0.979]
Epoch [88/120    avg_loss:0.005, val_acc:0.978]
Epoch [89/120    avg_loss:0.007, val_acc:0.978]
Epoch [90/120    avg_loss:0.005, val_acc:0.978]
Epoch [91/120    avg_loss:0.010, val_acc:0.979]
Epoch [92/120    avg_loss:0.008, val_acc:0.979]
Epoch [93/120    avg_loss:0.009, val_acc:0.979]
Epoch [94/120    avg_loss:0.009, val_acc:0.978]
Epoch [95/120    avg_loss:0.010, val_acc:0.979]
Epoch [96/120    avg_loss:0.009, val_acc:0.979]
Epoch [97/120    avg_loss:0.005, val_acc:0.979]
Epoch [98/120    avg_loss:0.005, val_acc:0.980]
Epoch [99/120    avg_loss:0.016, val_acc:0.977]
Epoch [100/120    avg_loss:0.007, val_acc:0.977]
Epoch [101/120    avg_loss:0.005, val_acc:0.977]
Epoch [102/120    avg_loss:0.005, val_acc:0.977]
Epoch [103/120    avg_loss:0.007, val_acc:0.977]
Epoch [104/120    avg_loss:0.009, val_acc:0.978]
Epoch [105/120    avg_loss:0.010, val_acc:0.978]
Epoch [106/120    avg_loss:0.006, val_acc:0.978]
Epoch [107/120    avg_loss:0.010, val_acc:0.978]
Epoch [108/120    avg_loss:0.007, val_acc:0.978]
Epoch [109/120    avg_loss:0.011, val_acc:0.978]
Epoch [110/120    avg_loss:0.007, val_acc:0.978]
Epoch [111/120    avg_loss:0.006, val_acc:0.978]
Epoch [112/120    avg_loss:0.006, val_acc:0.978]
Epoch [113/120    avg_loss:0.007, val_acc:0.978]
Epoch [114/120    avg_loss:0.009, val_acc:0.978]
Epoch [115/120    avg_loss:0.016, val_acc:0.978]
Epoch [116/120    avg_loss:0.008, val_acc:0.978]
Epoch [117/120    avg_loss:0.006, val_acc:0.978]
Epoch [118/120    avg_loss:0.016, val_acc:0.978]
Epoch [119/120    avg_loss:0.008, val_acc:0.978]
Epoch [120/120    avg_loss:0.018, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   35    2    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1271    4    2    0    0    0    0    0    3    4    1    0
     0    0    0]
 [   0    0    0  717    0    1    0    0    0    9    0    1   15    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    9    1    0    2    1    0    0    0  857    3    0    0
     0    2    0]
 [   0    0   14    0    0    3    1    0    0    0    7 2178    7    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0    0    4    3  520    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    1    0    1    0    0    0    0    0
  1129    6    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
    26  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.23306233062331

F1 scores:
[       nan 0.92105263 0.98488958 0.97352342 0.9953271  0.98858447
 0.99168556 1.         0.99650757 0.8        0.97942857 0.99
 0.96296296 0.98930481 0.98344948 0.9387145  0.97647059]

Kappa:
0.9798602098258952
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:04:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdf426995c0>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.279, val_acc:0.360]
Epoch [2/120    avg_loss:1.644, val_acc:0.546]
Epoch [3/120    avg_loss:1.514, val_acc:0.611]
Epoch [4/120    avg_loss:1.214, val_acc:0.674]
Epoch [5/120    avg_loss:1.049, val_acc:0.681]
Epoch [6/120    avg_loss:1.078, val_acc:0.696]
Epoch [7/120    avg_loss:0.852, val_acc:0.747]
Epoch [8/120    avg_loss:0.887, val_acc:0.761]
Epoch [9/120    avg_loss:0.598, val_acc:0.784]
Epoch [10/120    avg_loss:0.610, val_acc:0.766]
Epoch [11/120    avg_loss:0.527, val_acc:0.819]
Epoch [12/120    avg_loss:0.480, val_acc:0.844]
Epoch [13/120    avg_loss:0.512, val_acc:0.851]
Epoch [14/120    avg_loss:0.397, val_acc:0.867]
Epoch [15/120    avg_loss:0.468, val_acc:0.846]
Epoch [16/120    avg_loss:0.425, val_acc:0.863]
Epoch [17/120    avg_loss:0.376, val_acc:0.864]
Epoch [18/120    avg_loss:0.296, val_acc:0.907]
Epoch [19/120    avg_loss:0.221, val_acc:0.885]
Epoch [20/120    avg_loss:0.221, val_acc:0.896]
Epoch [21/120    avg_loss:0.163, val_acc:0.914]
Epoch [22/120    avg_loss:0.116, val_acc:0.935]
Epoch [23/120    avg_loss:0.187, val_acc:0.908]
Epoch [24/120    avg_loss:0.185, val_acc:0.900]
Epoch [25/120    avg_loss:0.244, val_acc:0.896]
Epoch [26/120    avg_loss:0.209, val_acc:0.888]
Epoch [27/120    avg_loss:0.175, val_acc:0.911]
Epoch [28/120    avg_loss:0.168, val_acc:0.912]
Epoch [29/120    avg_loss:0.132, val_acc:0.916]
Epoch [30/120    avg_loss:0.081, val_acc:0.951]
Epoch [31/120    avg_loss:0.087, val_acc:0.928]
Epoch [32/120    avg_loss:0.080, val_acc:0.945]
Epoch [33/120    avg_loss:0.058, val_acc:0.959]
Epoch [34/120    avg_loss:0.042, val_acc:0.953]
Epoch [35/120    avg_loss:0.037, val_acc:0.966]
Epoch [36/120    avg_loss:0.044, val_acc:0.951]
Epoch [37/120    avg_loss:0.035, val_acc:0.968]
Epoch [38/120    avg_loss:0.380, val_acc:0.882]
Epoch [39/120    avg_loss:0.164, val_acc:0.924]
Epoch [40/120    avg_loss:0.113, val_acc:0.948]
Epoch [41/120    avg_loss:0.047, val_acc:0.961]
Epoch [42/120    avg_loss:0.116, val_acc:0.947]
Epoch [43/120    avg_loss:0.098, val_acc:0.943]
Epoch [44/120    avg_loss:0.069, val_acc:0.958]
Epoch [45/120    avg_loss:0.072, val_acc:0.951]
Epoch [46/120    avg_loss:0.036, val_acc:0.967]
Epoch [47/120    avg_loss:0.034, val_acc:0.962]
Epoch [48/120    avg_loss:0.048, val_acc:0.957]
Epoch [49/120    avg_loss:0.020, val_acc:0.972]
Epoch [50/120    avg_loss:0.024, val_acc:0.947]
Epoch [51/120    avg_loss:0.091, val_acc:0.950]
Epoch [52/120    avg_loss:0.030, val_acc:0.969]
Epoch [53/120    avg_loss:0.024, val_acc:0.964]
Epoch [54/120    avg_loss:0.022, val_acc:0.976]
Epoch [55/120    avg_loss:0.024, val_acc:0.954]
Epoch [56/120    avg_loss:0.022, val_acc:0.976]
Epoch [57/120    avg_loss:0.024, val_acc:0.971]
Epoch [58/120    avg_loss:0.030, val_acc:0.976]
Epoch [59/120    avg_loss:0.026, val_acc:0.970]
Epoch [60/120    avg_loss:0.025, val_acc:0.962]
Epoch [61/120    avg_loss:0.018, val_acc:0.968]
Epoch [62/120    avg_loss:0.018, val_acc:0.972]
Epoch [63/120    avg_loss:0.013, val_acc:0.976]
Epoch [64/120    avg_loss:0.014, val_acc:0.977]
Epoch [65/120    avg_loss:0.006, val_acc:0.974]
Epoch [66/120    avg_loss:0.012, val_acc:0.975]
Epoch [67/120    avg_loss:0.020, val_acc:0.972]
Epoch [68/120    avg_loss:0.013, val_acc:0.975]
Epoch [69/120    avg_loss:0.010, val_acc:0.978]
Epoch [70/120    avg_loss:0.007, val_acc:0.978]
Epoch [71/120    avg_loss:0.013, val_acc:0.971]
Epoch [72/120    avg_loss:0.021, val_acc:0.979]
Epoch [73/120    avg_loss:0.018, val_acc:0.982]
Epoch [74/120    avg_loss:0.012, val_acc:0.968]
Epoch [75/120    avg_loss:0.023, val_acc:0.978]
Epoch [76/120    avg_loss:0.006, val_acc:0.979]
Epoch [77/120    avg_loss:0.012, val_acc:0.974]
Epoch [78/120    avg_loss:0.027, val_acc:0.957]
Epoch [79/120    avg_loss:0.014, val_acc:0.984]
Epoch [80/120    avg_loss:0.005, val_acc:0.980]
Epoch [81/120    avg_loss:0.007, val_acc:0.989]
Epoch [82/120    avg_loss:0.004, val_acc:0.989]
Epoch [83/120    avg_loss:0.009, val_acc:0.969]
Epoch [84/120    avg_loss:0.009, val_acc:0.979]
Epoch [85/120    avg_loss:0.011, val_acc:0.980]
Epoch [86/120    avg_loss:0.022, val_acc:0.972]
Epoch [87/120    avg_loss:0.010, val_acc:0.977]
Epoch [88/120    avg_loss:0.009, val_acc:0.976]
Epoch [89/120    avg_loss:0.005, val_acc:0.979]
Epoch [90/120    avg_loss:0.006, val_acc:0.978]
Epoch [91/120    avg_loss:0.005, val_acc:0.984]
Epoch [92/120    avg_loss:0.005, val_acc:0.981]
Epoch [93/120    avg_loss:0.003, val_acc:0.981]
Epoch [94/120    avg_loss:0.004, val_acc:0.980]
Epoch [95/120    avg_loss:0.006, val_acc:0.977]
Epoch [96/120    avg_loss:0.005, val_acc:0.981]
Epoch [97/120    avg_loss:0.005, val_acc:0.982]
Epoch [98/120    avg_loss:0.007, val_acc:0.984]
Epoch [99/120    avg_loss:0.005, val_acc:0.983]
Epoch [100/120    avg_loss:0.003, val_acc:0.984]
Epoch [101/120    avg_loss:0.003, val_acc:0.984]
Epoch [102/120    avg_loss:0.004, val_acc:0.984]
Epoch [103/120    avg_loss:0.003, val_acc:0.984]
Epoch [104/120    avg_loss:0.004, val_acc:0.984]
Epoch [105/120    avg_loss:0.002, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.984]
Epoch [107/120    avg_loss:0.003, val_acc:0.984]
Epoch [108/120    avg_loss:0.002, val_acc:0.984]
Epoch [109/120    avg_loss:0.005, val_acc:0.984]
Epoch [110/120    avg_loss:0.002, val_acc:0.984]
Epoch [111/120    avg_loss:0.002, val_acc:0.984]
Epoch [112/120    avg_loss:0.002, val_acc:0.984]
Epoch [113/120    avg_loss:0.003, val_acc:0.984]
Epoch [114/120    avg_loss:0.002, val_acc:0.984]
Epoch [115/120    avg_loss:0.002, val_acc:0.984]
Epoch [116/120    avg_loss:0.002, val_acc:0.984]
Epoch [117/120    avg_loss:0.002, val_acc:0.984]
Epoch [118/120    avg_loss:0.006, val_acc:0.984]
Epoch [119/120    avg_loss:0.005, val_acc:0.984]
Epoch [120/120    avg_loss:0.002, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    1 1273    0    3    0    1    0    0    0    6    0    0    0
     0    1    0]
 [   0    0    0  737    0    0    0    0    0    2    0    0    7    1
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  868    1    0    0
     0    6    0]
 [   0    0   16    0    0    0    2    0    0    0    1 2183    8    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    5    0  528    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1137    1    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
     7  328    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.99186991869918

F1 scores:
[       nan 0.97560976 0.98912199 0.99192463 0.99065421 0.99769585
 0.98871332 1.         0.99650757 0.87179487 0.98861048 0.99362767
 0.97597043 0.99730458 0.99605782 0.96046852 0.97590361]

Kappa:
0.9885117331035336
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa69b8fa630>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.182, val_acc:0.331]
Epoch [2/120    avg_loss:1.708, val_acc:0.613]
Epoch [3/120    avg_loss:1.336, val_acc:0.621]
Epoch [4/120    avg_loss:1.224, val_acc:0.724]
Epoch [5/120    avg_loss:1.041, val_acc:0.683]
Epoch [6/120    avg_loss:0.980, val_acc:0.789]
Epoch [7/120    avg_loss:0.767, val_acc:0.812]
Epoch [8/120    avg_loss:0.690, val_acc:0.830]
Epoch [9/120    avg_loss:0.589, val_acc:0.790]
Epoch [10/120    avg_loss:0.510, val_acc:0.833]
Epoch [11/120    avg_loss:0.481, val_acc:0.848]
Epoch [12/120    avg_loss:0.476, val_acc:0.816]
Epoch [13/120    avg_loss:0.429, val_acc:0.872]
Epoch [14/120    avg_loss:0.350, val_acc:0.856]
Epoch [15/120    avg_loss:0.290, val_acc:0.884]
Epoch [16/120    avg_loss:0.310, val_acc:0.883]
Epoch [17/120    avg_loss:0.423, val_acc:0.903]
Epoch [18/120    avg_loss:0.201, val_acc:0.891]
Epoch [19/120    avg_loss:0.210, val_acc:0.918]
Epoch [20/120    avg_loss:0.159, val_acc:0.903]
Epoch [21/120    avg_loss:0.120, val_acc:0.936]
Epoch [22/120    avg_loss:0.119, val_acc:0.914]
Epoch [23/120    avg_loss:0.161, val_acc:0.940]
Epoch [24/120    avg_loss:0.128, val_acc:0.936]
Epoch [25/120    avg_loss:0.148, val_acc:0.931]
Epoch [26/120    avg_loss:0.102, val_acc:0.894]
Epoch [27/120    avg_loss:0.144, val_acc:0.938]
Epoch [28/120    avg_loss:0.084, val_acc:0.955]
Epoch [29/120    avg_loss:0.078, val_acc:0.957]
Epoch [30/120    avg_loss:0.076, val_acc:0.957]
Epoch [31/120    avg_loss:0.093, val_acc:0.947]
Epoch [32/120    avg_loss:0.075, val_acc:0.949]
Epoch [33/120    avg_loss:0.081, val_acc:0.954]
Epoch [34/120    avg_loss:0.110, val_acc:0.945]
Epoch [35/120    avg_loss:0.069, val_acc:0.959]
Epoch [36/120    avg_loss:0.074, val_acc:0.959]
Epoch [37/120    avg_loss:0.065, val_acc:0.957]
Epoch [38/120    avg_loss:0.144, val_acc:0.955]
Epoch [39/120    avg_loss:0.080, val_acc:0.944]
Epoch [40/120    avg_loss:0.068, val_acc:0.963]
Epoch [41/120    avg_loss:0.053, val_acc:0.963]
Epoch [42/120    avg_loss:0.038, val_acc:0.963]
Epoch [43/120    avg_loss:0.040, val_acc:0.966]
Epoch [44/120    avg_loss:0.026, val_acc:0.965]
Epoch [45/120    avg_loss:0.025, val_acc:0.959]
Epoch [46/120    avg_loss:0.031, val_acc:0.964]
Epoch [47/120    avg_loss:0.017, val_acc:0.974]
Epoch [48/120    avg_loss:0.019, val_acc:0.979]
Epoch [49/120    avg_loss:0.032, val_acc:0.968]
Epoch [50/120    avg_loss:0.053, val_acc:0.949]
Epoch [51/120    avg_loss:0.080, val_acc:0.967]
Epoch [52/120    avg_loss:0.020, val_acc:0.970]
Epoch [53/120    avg_loss:0.040, val_acc:0.971]
Epoch [54/120    avg_loss:0.028, val_acc:0.980]
Epoch [55/120    avg_loss:0.019, val_acc:0.966]
Epoch [56/120    avg_loss:0.015, val_acc:0.979]
Epoch [57/120    avg_loss:0.010, val_acc:0.976]
Epoch [58/120    avg_loss:0.029, val_acc:0.976]
Epoch [59/120    avg_loss:0.016, val_acc:0.979]
Epoch [60/120    avg_loss:0.089, val_acc:0.820]
Epoch [61/120    avg_loss:0.409, val_acc:0.957]
Epoch [62/120    avg_loss:0.051, val_acc:0.962]
Epoch [63/120    avg_loss:0.027, val_acc:0.976]
Epoch [64/120    avg_loss:0.026, val_acc:0.967]
Epoch [65/120    avg_loss:0.027, val_acc:0.975]
Epoch [66/120    avg_loss:0.023, val_acc:0.979]
Epoch [67/120    avg_loss:0.135, val_acc:0.645]
Epoch [68/120    avg_loss:0.839, val_acc:0.874]
Epoch [69/120    avg_loss:0.351, val_acc:0.925]
Epoch [70/120    avg_loss:0.176, val_acc:0.954]
Epoch [71/120    avg_loss:0.086, val_acc:0.962]
Epoch [72/120    avg_loss:0.077, val_acc:0.967]
Epoch [73/120    avg_loss:0.055, val_acc:0.976]
Epoch [74/120    avg_loss:0.030, val_acc:0.979]
Epoch [75/120    avg_loss:0.030, val_acc:0.978]
Epoch [76/120    avg_loss:0.029, val_acc:0.978]
Epoch [77/120    avg_loss:0.016, val_acc:0.980]
Epoch [78/120    avg_loss:0.029, val_acc:0.980]
Epoch [79/120    avg_loss:0.031, val_acc:0.980]
Epoch [80/120    avg_loss:0.022, val_acc:0.980]
Epoch [81/120    avg_loss:0.019, val_acc:0.980]
Epoch [82/120    avg_loss:0.021, val_acc:0.981]
Epoch [83/120    avg_loss:0.017, val_acc:0.981]
Epoch [84/120    avg_loss:0.020, val_acc:0.983]
Epoch [85/120    avg_loss:0.021, val_acc:0.984]
Epoch [86/120    avg_loss:0.024, val_acc:0.984]
Epoch [87/120    avg_loss:0.015, val_acc:0.983]
Epoch [88/120    avg_loss:0.011, val_acc:0.982]
Epoch [89/120    avg_loss:0.016, val_acc:0.984]
Epoch [90/120    avg_loss:0.014, val_acc:0.981]
Epoch [91/120    avg_loss:0.022, val_acc:0.983]
Epoch [92/120    avg_loss:0.013, val_acc:0.984]
Epoch [93/120    avg_loss:0.019, val_acc:0.984]
Epoch [94/120    avg_loss:0.014, val_acc:0.984]
Epoch [95/120    avg_loss:0.010, val_acc:0.984]
Epoch [96/120    avg_loss:0.009, val_acc:0.984]
Epoch [97/120    avg_loss:0.016, val_acc:0.982]
Epoch [98/120    avg_loss:0.018, val_acc:0.983]
Epoch [99/120    avg_loss:0.009, val_acc:0.985]
Epoch [100/120    avg_loss:0.012, val_acc:0.985]
Epoch [101/120    avg_loss:0.008, val_acc:0.985]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.015, val_acc:0.985]
Epoch [104/120    avg_loss:0.006, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.009, val_acc:0.986]
Epoch [108/120    avg_loss:0.009, val_acc:0.985]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.010, val_acc:0.985]
Epoch [111/120    avg_loss:0.010, val_acc:0.983]
Epoch [112/120    avg_loss:0.009, val_acc:0.984]
Epoch [113/120    avg_loss:0.012, val_acc:0.984]
Epoch [114/120    avg_loss:0.009, val_acc:0.984]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.009, val_acc:0.983]
Epoch [117/120    avg_loss:0.011, val_acc:0.982]
Epoch [118/120    avg_loss:0.006, val_acc:0.984]
Epoch [119/120    avg_loss:0.008, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1278    2    0    0    0    0    0    0    3    1    1    0
     0    0    0]
 [   0    0    0  721    0    3    0    0    0    4    0    1   18    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    2    0    0    0  865    1    0    0
     1    3    0]
 [   0    0   18    0    0    2    0    0    0    0    0 2188    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    0  524    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    1    0    0    0    0    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    25  311    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.65582655826559

F1 scores:
[       nan 0.98765432 0.98839907 0.97961957 0.99528302 0.98861048
 0.98869631 0.98039216 0.997669   0.9        0.99026903 0.99409359
 0.97037037 1.         0.98651588 0.94099849 0.95402299]

Kappa:
0.9846768667465021
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9b114666a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.254, val_acc:0.494]
Epoch [2/120    avg_loss:1.650, val_acc:0.565]
Epoch [3/120    avg_loss:1.466, val_acc:0.603]
Epoch [4/120    avg_loss:1.370, val_acc:0.647]
Epoch [5/120    avg_loss:1.093, val_acc:0.710]
Epoch [6/120    avg_loss:0.885, val_acc:0.756]
Epoch [7/120    avg_loss:0.856, val_acc:0.797]
Epoch [8/120    avg_loss:0.666, val_acc:0.783]
Epoch [9/120    avg_loss:0.642, val_acc:0.745]
Epoch [10/120    avg_loss:0.646, val_acc:0.805]
Epoch [11/120    avg_loss:0.499, val_acc:0.847]
Epoch [12/120    avg_loss:0.441, val_acc:0.878]
Epoch [13/120    avg_loss:0.531, val_acc:0.826]
Epoch [14/120    avg_loss:0.382, val_acc:0.836]
Epoch [15/120    avg_loss:0.366, val_acc:0.848]
Epoch [16/120    avg_loss:0.277, val_acc:0.900]
Epoch [17/120    avg_loss:0.272, val_acc:0.851]
Epoch [18/120    avg_loss:0.200, val_acc:0.918]
Epoch [19/120    avg_loss:0.226, val_acc:0.920]
Epoch [20/120    avg_loss:0.182, val_acc:0.902]
Epoch [21/120    avg_loss:0.167, val_acc:0.911]
Epoch [22/120    avg_loss:0.170, val_acc:0.927]
Epoch [23/120    avg_loss:0.317, val_acc:0.905]
Epoch [24/120    avg_loss:0.180, val_acc:0.918]
Epoch [25/120    avg_loss:0.223, val_acc:0.894]
Epoch [26/120    avg_loss:0.149, val_acc:0.860]
Epoch [27/120    avg_loss:0.138, val_acc:0.906]
Epoch [28/120    avg_loss:0.163, val_acc:0.915]
Epoch [29/120    avg_loss:0.142, val_acc:0.930]
Epoch [30/120    avg_loss:0.081, val_acc:0.936]
Epoch [31/120    avg_loss:0.083, val_acc:0.939]
Epoch [32/120    avg_loss:0.100, val_acc:0.916]
Epoch [33/120    avg_loss:0.081, val_acc:0.952]
Epoch [34/120    avg_loss:0.054, val_acc:0.951]
Epoch [35/120    avg_loss:0.065, val_acc:0.941]
Epoch [36/120    avg_loss:0.062, val_acc:0.943]
Epoch [37/120    avg_loss:0.049, val_acc:0.952]
Epoch [38/120    avg_loss:0.053, val_acc:0.936]
Epoch [39/120    avg_loss:0.055, val_acc:0.936]
Epoch [40/120    avg_loss:0.093, val_acc:0.930]
Epoch [41/120    avg_loss:0.057, val_acc:0.961]
Epoch [42/120    avg_loss:0.064, val_acc:0.947]
Epoch [43/120    avg_loss:0.041, val_acc:0.952]
Epoch [44/120    avg_loss:0.023, val_acc:0.957]
Epoch [45/120    avg_loss:0.023, val_acc:0.947]
Epoch [46/120    avg_loss:0.053, val_acc:0.948]
Epoch [47/120    avg_loss:0.027, val_acc:0.945]
Epoch [48/120    avg_loss:0.034, val_acc:0.964]
Epoch [49/120    avg_loss:0.020, val_acc:0.957]
Epoch [50/120    avg_loss:0.038, val_acc:0.965]
Epoch [51/120    avg_loss:0.036, val_acc:0.961]
Epoch [52/120    avg_loss:0.044, val_acc:0.952]
Epoch [53/120    avg_loss:0.053, val_acc:0.947]
Epoch [54/120    avg_loss:0.035, val_acc:0.956]
Epoch [55/120    avg_loss:0.020, val_acc:0.959]
Epoch [56/120    avg_loss:0.022, val_acc:0.953]
Epoch [57/120    avg_loss:0.033, val_acc:0.955]
Epoch [58/120    avg_loss:0.017, val_acc:0.952]
Epoch [59/120    avg_loss:0.011, val_acc:0.956]
Epoch [60/120    avg_loss:0.016, val_acc:0.955]
Epoch [61/120    avg_loss:0.018, val_acc:0.957]
Epoch [62/120    avg_loss:0.015, val_acc:0.963]
Epoch [63/120    avg_loss:0.018, val_acc:0.948]
Epoch [64/120    avg_loss:0.011, val_acc:0.960]
Epoch [65/120    avg_loss:0.023, val_acc:0.961]
Epoch [66/120    avg_loss:0.015, val_acc:0.963]
Epoch [67/120    avg_loss:0.009, val_acc:0.964]
Epoch [68/120    avg_loss:0.008, val_acc:0.963]
Epoch [69/120    avg_loss:0.009, val_acc:0.960]
Epoch [70/120    avg_loss:0.009, val_acc:0.961]
Epoch [71/120    avg_loss:0.015, val_acc:0.966]
Epoch [72/120    avg_loss:0.007, val_acc:0.968]
Epoch [73/120    avg_loss:0.008, val_acc:0.968]
Epoch [74/120    avg_loss:0.009, val_acc:0.969]
Epoch [75/120    avg_loss:0.009, val_acc:0.968]
Epoch [76/120    avg_loss:0.011, val_acc:0.968]
Epoch [77/120    avg_loss:0.010, val_acc:0.968]
Epoch [78/120    avg_loss:0.006, val_acc:0.972]
Epoch [79/120    avg_loss:0.005, val_acc:0.970]
Epoch [80/120    avg_loss:0.017, val_acc:0.966]
Epoch [81/120    avg_loss:0.006, val_acc:0.968]
Epoch [82/120    avg_loss:0.008, val_acc:0.965]
Epoch [83/120    avg_loss:0.008, val_acc:0.968]
Epoch [84/120    avg_loss:0.007, val_acc:0.971]
Epoch [85/120    avg_loss:0.007, val_acc:0.970]
Epoch [86/120    avg_loss:0.008, val_acc:0.970]
Epoch [87/120    avg_loss:0.008, val_acc:0.970]
Epoch [88/120    avg_loss:0.007, val_acc:0.969]
Epoch [89/120    avg_loss:0.005, val_acc:0.969]
Epoch [90/120    avg_loss:0.009, val_acc:0.968]
Epoch [91/120    avg_loss:0.005, val_acc:0.971]
Epoch [92/120    avg_loss:0.007, val_acc:0.971]
Epoch [93/120    avg_loss:0.004, val_acc:0.971]
Epoch [94/120    avg_loss:0.004, val_acc:0.971]
Epoch [95/120    avg_loss:0.007, val_acc:0.971]
Epoch [96/120    avg_loss:0.004, val_acc:0.971]
Epoch [97/120    avg_loss:0.005, val_acc:0.971]
Epoch [98/120    avg_loss:0.005, val_acc:0.971]
Epoch [99/120    avg_loss:0.004, val_acc:0.971]
Epoch [100/120    avg_loss:0.004, val_acc:0.972]
Epoch [101/120    avg_loss:0.005, val_acc:0.972]
Epoch [102/120    avg_loss:0.004, val_acc:0.972]
Epoch [103/120    avg_loss:0.004, val_acc:0.972]
Epoch [104/120    avg_loss:0.006, val_acc:0.971]
Epoch [105/120    avg_loss:0.005, val_acc:0.971]
Epoch [106/120    avg_loss:0.006, val_acc:0.971]
Epoch [107/120    avg_loss:0.005, val_acc:0.971]
Epoch [108/120    avg_loss:0.008, val_acc:0.972]
Epoch [109/120    avg_loss:0.005, val_acc:0.972]
Epoch [110/120    avg_loss:0.004, val_acc:0.972]
Epoch [111/120    avg_loss:0.007, val_acc:0.972]
Epoch [112/120    avg_loss:0.006, val_acc:0.971]
Epoch [113/120    avg_loss:0.008, val_acc:0.971]
Epoch [114/120    avg_loss:0.006, val_acc:0.971]
Epoch [115/120    avg_loss:0.006, val_acc:0.971]
Epoch [116/120    avg_loss:0.014, val_acc:0.972]
Epoch [117/120    avg_loss:0.004, val_acc:0.972]
Epoch [118/120    avg_loss:0.006, val_acc:0.972]
Epoch [119/120    avg_loss:0.004, val_acc:0.972]
Epoch [120/120    avg_loss:0.005, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1259    1   10    0    2    0    0    2    3    7    0    0
     0    1    0]
 [   0    0    0  701    0    3    0    0    0   18    0    1   19    3
     2    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  421    0    0    0    9    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    6    2    0    2    0    0    0    0  852    3    0    0
     3    7    0]
 [   0    0    8    0    0    4    4    0    0    2    8 2159   25    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    4    0  528    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   19    0    0    1    0    0    0    0    0
  1112    7    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
    28  317    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.57181571815718

F1 scores:
[       nan 1.         0.98436278 0.96623019 0.97706422 0.96436526
 0.99470098 1.         0.98826291 0.59649123 0.97818599 0.98584475
 0.94454383 0.9919571  0.97287839 0.93235294 0.97590361]

Kappa:
0.9723519614044942
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f86655015f8>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.238, val_acc:0.481]
Epoch [2/120    avg_loss:1.706, val_acc:0.492]
Epoch [3/120    avg_loss:1.347, val_acc:0.631]
Epoch [4/120    avg_loss:1.235, val_acc:0.662]
Epoch [5/120    avg_loss:1.193, val_acc:0.727]
Epoch [6/120    avg_loss:0.876, val_acc:0.731]
Epoch [7/120    avg_loss:0.844, val_acc:0.793]
Epoch [8/120    avg_loss:0.700, val_acc:0.733]
Epoch [9/120    avg_loss:0.616, val_acc:0.784]
Epoch [10/120    avg_loss:0.679, val_acc:0.824]
Epoch [11/120    avg_loss:0.398, val_acc:0.856]
Epoch [12/120    avg_loss:0.433, val_acc:0.808]
Epoch [13/120    avg_loss:0.388, val_acc:0.857]
Epoch [14/120    avg_loss:0.295, val_acc:0.889]
Epoch [15/120    avg_loss:0.244, val_acc:0.878]
Epoch [16/120    avg_loss:0.240, val_acc:0.895]
Epoch [17/120    avg_loss:0.248, val_acc:0.906]
Epoch [18/120    avg_loss:0.232, val_acc:0.920]
Epoch [19/120    avg_loss:0.208, val_acc:0.897]
Epoch [20/120    avg_loss:0.232, val_acc:0.919]
Epoch [21/120    avg_loss:0.147, val_acc:0.910]
Epoch [22/120    avg_loss:0.201, val_acc:0.919]
Epoch [23/120    avg_loss:0.123, val_acc:0.923]
Epoch [24/120    avg_loss:0.117, val_acc:0.935]
Epoch [25/120    avg_loss:0.093, val_acc:0.941]
Epoch [26/120    avg_loss:0.091, val_acc:0.889]
Epoch [27/120    avg_loss:0.108, val_acc:0.941]
Epoch [28/120    avg_loss:0.107, val_acc:0.916]
Epoch [29/120    avg_loss:0.177, val_acc:0.914]
Epoch [30/120    avg_loss:0.062, val_acc:0.942]
Epoch [31/120    avg_loss:0.082, val_acc:0.940]
Epoch [32/120    avg_loss:0.079, val_acc:0.945]
Epoch [33/120    avg_loss:0.077, val_acc:0.954]
Epoch [34/120    avg_loss:0.097, val_acc:0.934]
Epoch [35/120    avg_loss:0.050, val_acc:0.952]
Epoch [36/120    avg_loss:0.036, val_acc:0.963]
Epoch [37/120    avg_loss:0.020, val_acc:0.964]
Epoch [38/120    avg_loss:0.026, val_acc:0.958]
Epoch [39/120    avg_loss:0.037, val_acc:0.964]
Epoch [40/120    avg_loss:0.058, val_acc:0.934]
Epoch [41/120    avg_loss:0.085, val_acc:0.957]
Epoch [42/120    avg_loss:0.029, val_acc:0.974]
Epoch [43/120    avg_loss:0.034, val_acc:0.967]
Epoch [44/120    avg_loss:0.029, val_acc:0.963]
Epoch [45/120    avg_loss:0.036, val_acc:0.960]
Epoch [46/120    avg_loss:0.032, val_acc:0.961]
Epoch [47/120    avg_loss:0.015, val_acc:0.965]
Epoch [48/120    avg_loss:0.016, val_acc:0.980]
Epoch [49/120    avg_loss:0.014, val_acc:0.977]
Epoch [50/120    avg_loss:0.020, val_acc:0.977]
Epoch [51/120    avg_loss:0.017, val_acc:0.969]
Epoch [52/120    avg_loss:0.017, val_acc:0.978]
Epoch [53/120    avg_loss:0.024, val_acc:0.975]
Epoch [54/120    avg_loss:0.014, val_acc:0.959]
Epoch [55/120    avg_loss:0.021, val_acc:0.964]
Epoch [56/120    avg_loss:0.011, val_acc:0.977]
Epoch [57/120    avg_loss:0.017, val_acc:0.903]
Epoch [58/120    avg_loss:0.054, val_acc:0.968]
Epoch [59/120    avg_loss:0.056, val_acc:0.961]
Epoch [60/120    avg_loss:0.037, val_acc:0.957]
Epoch [61/120    avg_loss:0.016, val_acc:0.971]
Epoch [62/120    avg_loss:0.012, val_acc:0.976]
Epoch [63/120    avg_loss:0.010, val_acc:0.976]
Epoch [64/120    avg_loss:0.010, val_acc:0.975]
Epoch [65/120    avg_loss:0.012, val_acc:0.977]
Epoch [66/120    avg_loss:0.012, val_acc:0.976]
Epoch [67/120    avg_loss:0.011, val_acc:0.975]
Epoch [68/120    avg_loss:0.011, val_acc:0.978]
Epoch [69/120    avg_loss:0.012, val_acc:0.976]
Epoch [70/120    avg_loss:0.006, val_acc:0.976]
Epoch [71/120    avg_loss:0.011, val_acc:0.978]
Epoch [72/120    avg_loss:0.010, val_acc:0.978]
Epoch [73/120    avg_loss:0.006, val_acc:0.977]
Epoch [74/120    avg_loss:0.012, val_acc:0.980]
Epoch [75/120    avg_loss:0.008, val_acc:0.978]
Epoch [76/120    avg_loss:0.014, val_acc:0.978]
Epoch [77/120    avg_loss:0.011, val_acc:0.979]
Epoch [78/120    avg_loss:0.008, val_acc:0.978]
Epoch [79/120    avg_loss:0.006, val_acc:0.978]
Epoch [80/120    avg_loss:0.006, val_acc:0.977]
Epoch [81/120    avg_loss:0.004, val_acc:0.977]
Epoch [82/120    avg_loss:0.006, val_acc:0.979]
Epoch [83/120    avg_loss:0.009, val_acc:0.977]
Epoch [84/120    avg_loss:0.005, val_acc:0.979]
Epoch [85/120    avg_loss:0.004, val_acc:0.980]
Epoch [86/120    avg_loss:0.005, val_acc:0.980]
Epoch [87/120    avg_loss:0.013, val_acc:0.980]
Epoch [88/120    avg_loss:0.004, val_acc:0.981]
Epoch [89/120    avg_loss:0.011, val_acc:0.980]
Epoch [90/120    avg_loss:0.008, val_acc:0.981]
Epoch [91/120    avg_loss:0.004, val_acc:0.982]
Epoch [92/120    avg_loss:0.007, val_acc:0.982]
Epoch [93/120    avg_loss:0.005, val_acc:0.981]
Epoch [94/120    avg_loss:0.005, val_acc:0.980]
Epoch [95/120    avg_loss:0.006, val_acc:0.980]
Epoch [96/120    avg_loss:0.005, val_acc:0.981]
Epoch [97/120    avg_loss:0.004, val_acc:0.981]
Epoch [98/120    avg_loss:0.012, val_acc:0.979]
Epoch [99/120    avg_loss:0.005, val_acc:0.980]
Epoch [100/120    avg_loss:0.003, val_acc:0.980]
Epoch [101/120    avg_loss:0.010, val_acc:0.982]
Epoch [102/120    avg_loss:0.005, val_acc:0.982]
Epoch [103/120    avg_loss:0.006, val_acc:0.981]
Epoch [104/120    avg_loss:0.003, val_acc:0.982]
Epoch [105/120    avg_loss:0.004, val_acc:0.981]
Epoch [106/120    avg_loss:0.004, val_acc:0.983]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.005, val_acc:0.985]
Epoch [109/120    avg_loss:0.006, val_acc:0.983]
Epoch [110/120    avg_loss:0.007, val_acc:0.981]
Epoch [111/120    avg_loss:0.004, val_acc:0.983]
Epoch [112/120    avg_loss:0.003, val_acc:0.983]
Epoch [113/120    avg_loss:0.007, val_acc:0.983]
Epoch [114/120    avg_loss:0.007, val_acc:0.984]
Epoch [115/120    avg_loss:0.003, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.983]
Epoch [117/120    avg_loss:0.005, val_acc:0.983]
Epoch [118/120    avg_loss:0.004, val_acc:0.981]
Epoch [119/120    avg_loss:0.003, val_acc:0.983]
Epoch [120/120    avg_loss:0.004, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1269    5    3    0    1    0    0    0    4    3    0    0
     0    0    0]
 [   0    0    0  729    2    3    0    0    0    5    0    0    7    1
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    4    0    0    0  867    3    0    0
     0    1    0]
 [   0    0    5    0    0    1    3    0    0    0    1 2198    0    2
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    5    0  527    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    2    0    1    0    0    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    11  324    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
99.00271002710028

F1 scores:
[       nan 0.98765432 0.99179367 0.98380567 0.98604651 0.99427262
 0.98353293 1.         0.99883856 0.85714286 0.98916144 0.99592207
 0.98504673 0.9919571  0.99387577 0.96428571 0.97619048]

Kappa:
0.9886320486137374
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1f3678b6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.248, val_acc:0.599]
Epoch [2/120    avg_loss:1.625, val_acc:0.511]
Epoch [3/120    avg_loss:1.427, val_acc:0.637]
Epoch [4/120    avg_loss:1.291, val_acc:0.620]
Epoch [5/120    avg_loss:1.179, val_acc:0.631]
Epoch [6/120    avg_loss:0.936, val_acc:0.763]
Epoch [7/120    avg_loss:0.819, val_acc:0.743]
Epoch [8/120    avg_loss:0.799, val_acc:0.771]
Epoch [9/120    avg_loss:0.665, val_acc:0.722]
Epoch [10/120    avg_loss:0.549, val_acc:0.815]
Epoch [11/120    avg_loss:0.573, val_acc:0.800]
Epoch [12/120    avg_loss:0.647, val_acc:0.805]
Epoch [13/120    avg_loss:0.446, val_acc:0.860]
Epoch [14/120    avg_loss:0.359, val_acc:0.867]
Epoch [15/120    avg_loss:0.325, val_acc:0.890]
Epoch [16/120    avg_loss:0.421, val_acc:0.805]
Epoch [17/120    avg_loss:0.412, val_acc:0.845]
Epoch [18/120    avg_loss:0.216, val_acc:0.875]
Epoch [19/120    avg_loss:0.302, val_acc:0.851]
Epoch [20/120    avg_loss:0.312, val_acc:0.861]
Epoch [21/120    avg_loss:0.193, val_acc:0.893]
Epoch [22/120    avg_loss:0.179, val_acc:0.856]
Epoch [23/120    avg_loss:0.188, val_acc:0.896]
Epoch [24/120    avg_loss:0.202, val_acc:0.909]
Epoch [25/120    avg_loss:0.165, val_acc:0.938]
Epoch [26/120    avg_loss:0.110, val_acc:0.938]
Epoch [27/120    avg_loss:0.150, val_acc:0.935]
Epoch [28/120    avg_loss:0.118, val_acc:0.881]
Epoch [29/120    avg_loss:0.106, val_acc:0.951]
Epoch [30/120    avg_loss:0.133, val_acc:0.917]
Epoch [31/120    avg_loss:0.127, val_acc:0.936]
Epoch [32/120    avg_loss:0.101, val_acc:0.923]
Epoch [33/120    avg_loss:0.078, val_acc:0.928]
Epoch [34/120    avg_loss:0.081, val_acc:0.949]
Epoch [35/120    avg_loss:0.149, val_acc:0.914]
Epoch [36/120    avg_loss:0.092, val_acc:0.937]
Epoch [37/120    avg_loss:0.167, val_acc:0.923]
Epoch [38/120    avg_loss:0.110, val_acc:0.919]
Epoch [39/120    avg_loss:0.057, val_acc:0.955]
Epoch [40/120    avg_loss:0.065, val_acc:0.945]
Epoch [41/120    avg_loss:0.072, val_acc:0.943]
Epoch [42/120    avg_loss:0.048, val_acc:0.943]
Epoch [43/120    avg_loss:0.034, val_acc:0.959]
Epoch [44/120    avg_loss:0.039, val_acc:0.954]
Epoch [45/120    avg_loss:0.047, val_acc:0.950]
Epoch [46/120    avg_loss:0.035, val_acc:0.954]
Epoch [47/120    avg_loss:0.022, val_acc:0.963]
Epoch [48/120    avg_loss:0.032, val_acc:0.945]
Epoch [49/120    avg_loss:0.045, val_acc:0.945]
Epoch [50/120    avg_loss:0.046, val_acc:0.946]
Epoch [51/120    avg_loss:0.035, val_acc:0.956]
Epoch [52/120    avg_loss:0.031, val_acc:0.954]
Epoch [53/120    avg_loss:0.041, val_acc:0.951]
Epoch [54/120    avg_loss:0.059, val_acc:0.952]
Epoch [55/120    avg_loss:0.037, val_acc:0.957]
Epoch [56/120    avg_loss:0.056, val_acc:0.956]
Epoch [57/120    avg_loss:0.044, val_acc:0.929]
Epoch [58/120    avg_loss:0.051, val_acc:0.960]
Epoch [59/120    avg_loss:0.020, val_acc:0.956]
Epoch [60/120    avg_loss:0.039, val_acc:0.952]
Epoch [61/120    avg_loss:0.023, val_acc:0.963]
Epoch [62/120    avg_loss:0.016, val_acc:0.964]
Epoch [63/120    avg_loss:0.011, val_acc:0.967]
Epoch [64/120    avg_loss:0.016, val_acc:0.965]
Epoch [65/120    avg_loss:0.019, val_acc:0.967]
Epoch [66/120    avg_loss:0.008, val_acc:0.966]
Epoch [67/120    avg_loss:0.009, val_acc:0.966]
Epoch [68/120    avg_loss:0.015, val_acc:0.966]
Epoch [69/120    avg_loss:0.014, val_acc:0.966]
Epoch [70/120    avg_loss:0.012, val_acc:0.966]
Epoch [71/120    avg_loss:0.008, val_acc:0.966]
Epoch [72/120    avg_loss:0.006, val_acc:0.967]
Epoch [73/120    avg_loss:0.008, val_acc:0.968]
Epoch [74/120    avg_loss:0.013, val_acc:0.966]
Epoch [75/120    avg_loss:0.011, val_acc:0.968]
Epoch [76/120    avg_loss:0.010, val_acc:0.968]
Epoch [77/120    avg_loss:0.008, val_acc:0.970]
Epoch [78/120    avg_loss:0.013, val_acc:0.966]
Epoch [79/120    avg_loss:0.007, val_acc:0.967]
Epoch [80/120    avg_loss:0.011, val_acc:0.968]
Epoch [81/120    avg_loss:0.007, val_acc:0.965]
Epoch [82/120    avg_loss:0.010, val_acc:0.967]
Epoch [83/120    avg_loss:0.008, val_acc:0.968]
Epoch [84/120    avg_loss:0.005, val_acc:0.968]
Epoch [85/120    avg_loss:0.008, val_acc:0.965]
Epoch [86/120    avg_loss:0.010, val_acc:0.966]
Epoch [87/120    avg_loss:0.010, val_acc:0.965]
Epoch [88/120    avg_loss:0.007, val_acc:0.967]
Epoch [89/120    avg_loss:0.006, val_acc:0.967]
Epoch [90/120    avg_loss:0.008, val_acc:0.966]
Epoch [91/120    avg_loss:0.006, val_acc:0.966]
Epoch [92/120    avg_loss:0.007, val_acc:0.966]
Epoch [93/120    avg_loss:0.007, val_acc:0.966]
Epoch [94/120    avg_loss:0.007, val_acc:0.966]
Epoch [95/120    avg_loss:0.008, val_acc:0.966]
Epoch [96/120    avg_loss:0.012, val_acc:0.966]
Epoch [97/120    avg_loss:0.008, val_acc:0.966]
Epoch [98/120    avg_loss:0.006, val_acc:0.966]
Epoch [99/120    avg_loss:0.011, val_acc:0.966]
Epoch [100/120    avg_loss:0.009, val_acc:0.966]
Epoch [101/120    avg_loss:0.008, val_acc:0.966]
Epoch [102/120    avg_loss:0.005, val_acc:0.966]
Epoch [103/120    avg_loss:0.009, val_acc:0.966]
Epoch [104/120    avg_loss:0.007, val_acc:0.966]
Epoch [105/120    avg_loss:0.006, val_acc:0.966]
Epoch [106/120    avg_loss:0.006, val_acc:0.966]
Epoch [107/120    avg_loss:0.006, val_acc:0.966]
Epoch [108/120    avg_loss:0.009, val_acc:0.966]
Epoch [109/120    avg_loss:0.008, val_acc:0.966]
Epoch [110/120    avg_loss:0.005, val_acc:0.966]
Epoch [111/120    avg_loss:0.008, val_acc:0.966]
Epoch [112/120    avg_loss:0.014, val_acc:0.966]
Epoch [113/120    avg_loss:0.012, val_acc:0.965]
Epoch [114/120    avg_loss:0.007, val_acc:0.966]
Epoch [115/120    avg_loss:0.010, val_acc:0.966]
Epoch [116/120    avg_loss:0.006, val_acc:0.966]
Epoch [117/120    avg_loss:0.015, val_acc:0.966]
Epoch [118/120    avg_loss:0.005, val_acc:0.966]
Epoch [119/120    avg_loss:0.009, val_acc:0.966]
Epoch [120/120    avg_loss:0.006, val_acc:0.966]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1268    5    2    0    3    0    0    0    4    0    0    0
     0    3    0]
 [   0    0    0  726    0    0    0    0    0    5    0    0   14    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0  871    1    0    0
     0    0    0]
 [   0    0   24    0    0    0    1    0    0    0    3 2177    4    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    6    0  526    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    0    0    0    0
  1128    4    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    20  317    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.57994579945799

F1 scores:
[       nan 0.98765432 0.98294574 0.98240866 0.9953271  0.99084668
 0.98945783 1.         0.99883856 0.8372093  0.98977273 0.9922516
 0.97227357 0.9919571  0.98644512 0.94485842 0.96385542]

Kappa:
0.983818560277063
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:120
Validation dataloader:120
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3a35de6668>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.277, val_acc:0.337]
Epoch [2/120    avg_loss:1.702, val_acc:0.481]
Epoch [3/120    avg_loss:1.339, val_acc:0.607]
Epoch [4/120    avg_loss:1.288, val_acc:0.705]
Epoch [5/120    avg_loss:0.960, val_acc:0.670]
Epoch [6/120    avg_loss:1.016, val_acc:0.695]
Epoch [7/120    avg_loss:0.787, val_acc:0.786]
Epoch [8/120    avg_loss:0.759, val_acc:0.756]
Epoch [9/120    avg_loss:0.649, val_acc:0.833]
Epoch [10/120    avg_loss:0.608, val_acc:0.816]
Epoch [11/120    avg_loss:0.555, val_acc:0.783]
Epoch [12/120    avg_loss:0.582, val_acc:0.833]
Epoch [13/120    avg_loss:0.496, val_acc:0.862]
Epoch [14/120    avg_loss:0.418, val_acc:0.882]
Epoch [15/120    avg_loss:0.278, val_acc:0.813]
Epoch [16/120    avg_loss:0.326, val_acc:0.886]
Epoch [17/120    avg_loss:0.292, val_acc:0.846]
Epoch [18/120    avg_loss:0.319, val_acc:0.889]
Epoch [19/120    avg_loss:0.278, val_acc:0.800]
Epoch [20/120    avg_loss:0.312, val_acc:0.921]
Epoch [21/120    avg_loss:0.233, val_acc:0.883]
Epoch [22/120    avg_loss:0.247, val_acc:0.927]
Epoch [23/120    avg_loss:0.164, val_acc:0.941]
Epoch [24/120    avg_loss:0.116, val_acc:0.929]
Epoch [25/120    avg_loss:0.141, val_acc:0.952]
Epoch [26/120    avg_loss:0.084, val_acc:0.945]
Epoch [27/120    avg_loss:0.126, val_acc:0.926]
Epoch [28/120    avg_loss:0.119, val_acc:0.930]
Epoch [29/120    avg_loss:0.116, val_acc:0.950]
Epoch [30/120    avg_loss:0.108, val_acc:0.886]
Epoch [31/120    avg_loss:0.224, val_acc:0.888]
Epoch [32/120    avg_loss:0.134, val_acc:0.939]
Epoch [33/120    avg_loss:0.107, val_acc:0.940]
Epoch [34/120    avg_loss:0.141, val_acc:0.941]
Epoch [35/120    avg_loss:0.092, val_acc:0.951]
Epoch [36/120    avg_loss:0.050, val_acc:0.963]
Epoch [37/120    avg_loss:0.071, val_acc:0.961]
Epoch [38/120    avg_loss:0.109, val_acc:0.956]
Epoch [39/120    avg_loss:0.067, val_acc:0.940]
Epoch [40/120    avg_loss:0.060, val_acc:0.957]
Epoch [41/120    avg_loss:0.062, val_acc:0.955]
Epoch [42/120    avg_loss:0.073, val_acc:0.957]
Epoch [43/120    avg_loss:0.039, val_acc:0.972]
Epoch [44/120    avg_loss:0.016, val_acc:0.977]
Epoch [45/120    avg_loss:0.022, val_acc:0.982]
Epoch [46/120    avg_loss:0.046, val_acc:0.970]
Epoch [47/120    avg_loss:0.042, val_acc:0.973]
Epoch [48/120    avg_loss:0.023, val_acc:0.946]
Epoch [49/120    avg_loss:0.036, val_acc:0.974]
Epoch [50/120    avg_loss:0.030, val_acc:0.970]
Epoch [51/120    avg_loss:0.021, val_acc:0.980]
Epoch [52/120    avg_loss:0.012, val_acc:0.985]
Epoch [53/120    avg_loss:0.045, val_acc:0.951]
Epoch [54/120    avg_loss:0.029, val_acc:0.976]
Epoch [55/120    avg_loss:0.016, val_acc:0.977]
Epoch [56/120    avg_loss:0.028, val_acc:0.970]
Epoch [57/120    avg_loss:0.029, val_acc:0.969]
Epoch [58/120    avg_loss:0.195, val_acc:0.913]
Epoch [59/120    avg_loss:0.057, val_acc:0.972]
Epoch [60/120    avg_loss:0.092, val_acc:0.954]
Epoch [61/120    avg_loss:0.046, val_acc:0.972]
Epoch [62/120    avg_loss:0.042, val_acc:0.978]
Epoch [63/120    avg_loss:0.022, val_acc:0.967]
Epoch [64/120    avg_loss:0.017, val_acc:0.985]
Epoch [65/120    avg_loss:0.147, val_acc:0.952]
Epoch [66/120    avg_loss:0.086, val_acc:0.961]
Epoch [67/120    avg_loss:0.042, val_acc:0.978]
Epoch [68/120    avg_loss:0.035, val_acc:0.976]
Epoch [69/120    avg_loss:0.033, val_acc:0.973]
Epoch [70/120    avg_loss:0.049, val_acc:0.971]
Epoch [71/120    avg_loss:0.030, val_acc:0.976]
Epoch [72/120    avg_loss:0.024, val_acc:0.979]
Epoch [73/120    avg_loss:0.017, val_acc:0.969]
Epoch [74/120    avg_loss:0.017, val_acc:0.973]
Epoch [75/120    avg_loss:0.021, val_acc:0.975]
Epoch [76/120    avg_loss:0.026, val_acc:0.976]
Epoch [77/120    avg_loss:0.019, val_acc:0.975]
Epoch [78/120    avg_loss:0.008, val_acc:0.982]
Epoch [79/120    avg_loss:0.011, val_acc:0.987]
Epoch [80/120    avg_loss:0.010, val_acc:0.988]
Epoch [81/120    avg_loss:0.006, val_acc:0.987]
Epoch [82/120    avg_loss:0.008, val_acc:0.986]
Epoch [83/120    avg_loss:0.006, val_acc:0.988]
Epoch [84/120    avg_loss:0.007, val_acc:0.988]
Epoch [85/120    avg_loss:0.007, val_acc:0.988]
Epoch [86/120    avg_loss:0.009, val_acc:0.988]
Epoch [87/120    avg_loss:0.006, val_acc:0.985]
Epoch [88/120    avg_loss:0.011, val_acc:0.987]
Epoch [89/120    avg_loss:0.009, val_acc:0.988]
Epoch [90/120    avg_loss:0.010, val_acc:0.989]
Epoch [91/120    avg_loss:0.008, val_acc:0.989]
Epoch [92/120    avg_loss:0.004, val_acc:0.989]
Epoch [93/120    avg_loss:0.004, val_acc:0.989]
Epoch [94/120    avg_loss:0.008, val_acc:0.988]
Epoch [95/120    avg_loss:0.005, val_acc:0.989]
Epoch [96/120    avg_loss:0.005, val_acc:0.988]
Epoch [97/120    avg_loss:0.005, val_acc:0.988]
Epoch [98/120    avg_loss:0.003, val_acc:0.988]
Epoch [99/120    avg_loss:0.004, val_acc:0.988]
Epoch [100/120    avg_loss:0.009, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.004, val_acc:0.988]
Epoch [103/120    avg_loss:0.008, val_acc:0.990]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.005, val_acc:0.990]
Epoch [108/120    avg_loss:0.003, val_acc:0.991]
Epoch [109/120    avg_loss:0.004, val_acc:0.991]
Epoch [110/120    avg_loss:0.005, val_acc:0.992]
Epoch [111/120    avg_loss:0.007, val_acc:0.990]
Epoch [112/120    avg_loss:0.008, val_acc:0.990]
Epoch [113/120    avg_loss:0.004, val_acc:0.990]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.005, val_acc:0.991]
Epoch [116/120    avg_loss:0.004, val_acc:0.991]
Epoch [117/120    avg_loss:0.006, val_acc:0.991]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.005, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1276    2    0    0    0    0    0    0    2    1    2    0
     0    2    0]
 [   0    0    0  739    0    1    0    0    0    1    0    0    2    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    1    0    0    0  852    9    0    0
     0    7    0]
 [   0    0   27    0    0    1    0    0    0    0    6 2174    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    9    0  517    0
     0    2    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    1    0    1    0    0    0
  1121    6    0]
 [   0    0    0    0    0    0   18    0    0    0    0    0    0    0
    15  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.38482384823848

F1 scores:
[       nan 1.         0.98380879 0.99261249 1.         0.98524404
 0.98574644 1.         0.99650757 0.91891892 0.9765043  0.98953118
 0.9754717  0.98930481 0.98549451 0.92625369 0.95953757]

Kappa:
0.9815929122336958
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f59e718f668>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.042, val_acc:0.404]
Epoch [2/120    avg_loss:1.731, val_acc:0.565]
Epoch [3/120    avg_loss:1.403, val_acc:0.620]
Epoch [4/120    avg_loss:1.332, val_acc:0.696]
Epoch [5/120    avg_loss:1.076, val_acc:0.726]
Epoch [6/120    avg_loss:0.881, val_acc:0.677]
Epoch [7/120    avg_loss:0.845, val_acc:0.791]
Epoch [8/120    avg_loss:0.842, val_acc:0.825]
Epoch [9/120    avg_loss:0.683, val_acc:0.811]
Epoch [10/120    avg_loss:0.559, val_acc:0.785]
Epoch [11/120    avg_loss:0.584, val_acc:0.827]
Epoch [12/120    avg_loss:0.484, val_acc:0.857]
Epoch [13/120    avg_loss:0.416, val_acc:0.876]
Epoch [14/120    avg_loss:0.382, val_acc:0.865]
Epoch [15/120    avg_loss:0.594, val_acc:0.841]
Epoch [16/120    avg_loss:0.456, val_acc:0.877]
Epoch [17/120    avg_loss:0.347, val_acc:0.899]
Epoch [18/120    avg_loss:0.363, val_acc:0.880]
Epoch [19/120    avg_loss:0.343, val_acc:0.911]
Epoch [20/120    avg_loss:0.358, val_acc:0.899]
Epoch [21/120    avg_loss:0.219, val_acc:0.862]
Epoch [22/120    avg_loss:0.272, val_acc:0.878]
Epoch [23/120    avg_loss:0.208, val_acc:0.935]
Epoch [24/120    avg_loss:0.188, val_acc:0.926]
Epoch [25/120    avg_loss:0.140, val_acc:0.927]
Epoch [26/120    avg_loss:0.140, val_acc:0.936]
Epoch [27/120    avg_loss:0.103, val_acc:0.948]
Epoch [28/120    avg_loss:0.119, val_acc:0.935]
Epoch [29/120    avg_loss:0.122, val_acc:0.950]
Epoch [30/120    avg_loss:0.088, val_acc:0.947]
Epoch [31/120    avg_loss:0.076, val_acc:0.946]
Epoch [32/120    avg_loss:0.113, val_acc:0.909]
Epoch [33/120    avg_loss:0.137, val_acc:0.916]
Epoch [34/120    avg_loss:0.116, val_acc:0.944]
Epoch [35/120    avg_loss:0.109, val_acc:0.936]
Epoch [36/120    avg_loss:0.069, val_acc:0.943]
Epoch [37/120    avg_loss:0.040, val_acc:0.965]
Epoch [38/120    avg_loss:0.064, val_acc:0.950]
Epoch [39/120    avg_loss:0.039, val_acc:0.956]
Epoch [40/120    avg_loss:0.074, val_acc:0.950]
Epoch [41/120    avg_loss:0.099, val_acc:0.944]
Epoch [42/120    avg_loss:0.067, val_acc:0.964]
Epoch [43/120    avg_loss:0.075, val_acc:0.958]
Epoch [44/120    avg_loss:0.072, val_acc:0.962]
Epoch [45/120    avg_loss:0.041, val_acc:0.959]
Epoch [46/120    avg_loss:0.030, val_acc:0.965]
Epoch [47/120    avg_loss:0.044, val_acc:0.956]
Epoch [48/120    avg_loss:0.039, val_acc:0.972]
Epoch [49/120    avg_loss:0.030, val_acc:0.966]
Epoch [50/120    avg_loss:0.034, val_acc:0.968]
Epoch [51/120    avg_loss:0.024, val_acc:0.963]
Epoch [52/120    avg_loss:0.030, val_acc:0.965]
Epoch [53/120    avg_loss:0.027, val_acc:0.968]
Epoch [54/120    avg_loss:0.022, val_acc:0.956]
Epoch [55/120    avg_loss:0.016, val_acc:0.974]
Epoch [56/120    avg_loss:0.033, val_acc:0.972]
Epoch [57/120    avg_loss:0.022, val_acc:0.978]
Epoch [58/120    avg_loss:0.027, val_acc:0.968]
Epoch [59/120    avg_loss:0.028, val_acc:0.976]
Epoch [60/120    avg_loss:0.023, val_acc:0.969]
Epoch [61/120    avg_loss:0.019, val_acc:0.983]
Epoch [62/120    avg_loss:0.013, val_acc:0.984]
Epoch [63/120    avg_loss:0.010, val_acc:0.981]
Epoch [64/120    avg_loss:0.010, val_acc:0.985]
Epoch [65/120    avg_loss:0.007, val_acc:0.984]
Epoch [66/120    avg_loss:0.011, val_acc:0.983]
Epoch [67/120    avg_loss:0.009, val_acc:0.983]
Epoch [68/120    avg_loss:0.030, val_acc:0.978]
Epoch [69/120    avg_loss:0.011, val_acc:0.973]
Epoch [70/120    avg_loss:0.018, val_acc:0.972]
Epoch [71/120    avg_loss:0.024, val_acc:0.976]
Epoch [72/120    avg_loss:0.053, val_acc:0.963]
Epoch [73/120    avg_loss:0.035, val_acc:0.950]
Epoch [74/120    avg_loss:0.030, val_acc:0.978]
Epoch [75/120    avg_loss:0.043, val_acc:0.952]
Epoch [76/120    avg_loss:0.018, val_acc:0.975]
Epoch [77/120    avg_loss:0.021, val_acc:0.953]
Epoch [78/120    avg_loss:0.015, val_acc:0.970]
Epoch [79/120    avg_loss:0.009, val_acc:0.975]
Epoch [80/120    avg_loss:0.007, val_acc:0.978]
Epoch [81/120    avg_loss:0.010, val_acc:0.974]
Epoch [82/120    avg_loss:0.008, val_acc:0.976]
Epoch [83/120    avg_loss:0.008, val_acc:0.979]
Epoch [84/120    avg_loss:0.006, val_acc:0.976]
Epoch [85/120    avg_loss:0.010, val_acc:0.974]
Epoch [86/120    avg_loss:0.012, val_acc:0.979]
Epoch [87/120    avg_loss:0.006, val_acc:0.979]
Epoch [88/120    avg_loss:0.006, val_acc:0.980]
Epoch [89/120    avg_loss:0.007, val_acc:0.981]
Epoch [90/120    avg_loss:0.005, val_acc:0.982]
Epoch [91/120    avg_loss:0.011, val_acc:0.981]
Epoch [92/120    avg_loss:0.007, val_acc:0.981]
Epoch [93/120    avg_loss:0.010, val_acc:0.981]
Epoch [94/120    avg_loss:0.006, val_acc:0.982]
Epoch [95/120    avg_loss:0.006, val_acc:0.982]
Epoch [96/120    avg_loss:0.005, val_acc:0.982]
Epoch [97/120    avg_loss:0.006, val_acc:0.982]
Epoch [98/120    avg_loss:0.005, val_acc:0.982]
Epoch [99/120    avg_loss:0.006, val_acc:0.982]
Epoch [100/120    avg_loss:0.007, val_acc:0.982]
Epoch [101/120    avg_loss:0.009, val_acc:0.982]
Epoch [102/120    avg_loss:0.009, val_acc:0.983]
Epoch [103/120    avg_loss:0.005, val_acc:0.982]
Epoch [104/120    avg_loss:0.006, val_acc:0.982]
Epoch [105/120    avg_loss:0.007, val_acc:0.982]
Epoch [106/120    avg_loss:0.006, val_acc:0.982]
Epoch [107/120    avg_loss:0.004, val_acc:0.982]
Epoch [108/120    avg_loss:0.006, val_acc:0.982]
Epoch [109/120    avg_loss:0.009, val_acc:0.982]
Epoch [110/120    avg_loss:0.006, val_acc:0.982]
Epoch [111/120    avg_loss:0.007, val_acc:0.982]
Epoch [112/120    avg_loss:0.006, val_acc:0.982]
Epoch [113/120    avg_loss:0.009, val_acc:0.982]
Epoch [114/120    avg_loss:0.005, val_acc:0.982]
Epoch [115/120    avg_loss:0.007, val_acc:0.982]
Epoch [116/120    avg_loss:0.005, val_acc:0.982]
Epoch [117/120    avg_loss:0.007, val_acc:0.982]
Epoch [118/120    avg_loss:0.005, val_acc:0.982]
Epoch [119/120    avg_loss:0.004, val_acc:0.982]
Epoch [120/120    avg_loss:0.006, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1264    2    0    0    3    0    0    0    6    5    0    0
     0    5    0]
 [   0    0    0  680    1   20    8    0    0    4    3    0   30    1
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    9    0    0    0    0  847    5    6    0
     0    6    0]
 [   0    0    9    0    0    2    5    0    0    0    2 2191    0    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    0  528    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    1    0    3    0    0    0
  1127    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    13  333    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.11382113821138

F1 scores:
[       nan 0.98765432 0.98711441 0.94972067 0.99530516 0.95594714
 0.98646617 1.         0.99883856 0.82051282 0.97412306 0.99320036
 0.96087352 0.99462366 0.98903028 0.96382055 0.97647059]

Kappa:
0.9785075123284049
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9f5f5716a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.305, val_acc:0.546]
Epoch [2/120    avg_loss:1.749, val_acc:0.423]
Epoch [3/120    avg_loss:1.351, val_acc:0.656]
Epoch [4/120    avg_loss:1.201, val_acc:0.700]
Epoch [5/120    avg_loss:0.923, val_acc:0.773]
Epoch [6/120    avg_loss:0.900, val_acc:0.640]
Epoch [7/120    avg_loss:0.805, val_acc:0.770]
Epoch [8/120    avg_loss:0.762, val_acc:0.786]
Epoch [9/120    avg_loss:0.712, val_acc:0.798]
Epoch [10/120    avg_loss:0.515, val_acc:0.840]
Epoch [11/120    avg_loss:0.408, val_acc:0.835]
Epoch [12/120    avg_loss:0.481, val_acc:0.839]
Epoch [13/120    avg_loss:0.404, val_acc:0.729]
Epoch [14/120    avg_loss:0.340, val_acc:0.891]
Epoch [15/120    avg_loss:0.287, val_acc:0.882]
Epoch [16/120    avg_loss:0.433, val_acc:0.854]
Epoch [17/120    avg_loss:0.317, val_acc:0.883]
Epoch [18/120    avg_loss:0.279, val_acc:0.812]
Epoch [19/120    avg_loss:0.230, val_acc:0.865]
Epoch [20/120    avg_loss:0.203, val_acc:0.879]
Epoch [21/120    avg_loss:0.191, val_acc:0.926]
Epoch [22/120    avg_loss:0.202, val_acc:0.899]
Epoch [23/120    avg_loss:0.162, val_acc:0.917]
Epoch [24/120    avg_loss:0.194, val_acc:0.912]
Epoch [25/120    avg_loss:0.205, val_acc:0.906]
Epoch [26/120    avg_loss:0.172, val_acc:0.901]
Epoch [27/120    avg_loss:0.157, val_acc:0.869]
Epoch [28/120    avg_loss:0.165, val_acc:0.931]
Epoch [29/120    avg_loss:0.084, val_acc:0.938]
Epoch [30/120    avg_loss:0.081, val_acc:0.918]
Epoch [31/120    avg_loss:0.066, val_acc:0.948]
Epoch [32/120    avg_loss:0.052, val_acc:0.956]
Epoch [33/120    avg_loss:0.085, val_acc:0.933]
Epoch [34/120    avg_loss:0.087, val_acc:0.948]
Epoch [35/120    avg_loss:0.054, val_acc:0.946]
Epoch [36/120    avg_loss:0.065, val_acc:0.951]
Epoch [37/120    avg_loss:0.076, val_acc:0.953]
Epoch [38/120    avg_loss:0.055, val_acc:0.949]
Epoch [39/120    avg_loss:0.033, val_acc:0.962]
Epoch [40/120    avg_loss:0.134, val_acc:0.924]
Epoch [41/120    avg_loss:0.073, val_acc:0.941]
Epoch [42/120    avg_loss:0.052, val_acc:0.945]
Epoch [43/120    avg_loss:0.056, val_acc:0.938]
Epoch [44/120    avg_loss:0.063, val_acc:0.901]
Epoch [45/120    avg_loss:0.043, val_acc:0.963]
Epoch [46/120    avg_loss:0.051, val_acc:0.944]
Epoch [47/120    avg_loss:0.065, val_acc:0.938]
Epoch [48/120    avg_loss:0.060, val_acc:0.951]
Epoch [49/120    avg_loss:0.033, val_acc:0.958]
Epoch [50/120    avg_loss:0.019, val_acc:0.968]
Epoch [51/120    avg_loss:0.029, val_acc:0.959]
Epoch [52/120    avg_loss:0.041, val_acc:0.960]
Epoch [53/120    avg_loss:0.029, val_acc:0.958]
Epoch [54/120    avg_loss:0.022, val_acc:0.961]
Epoch [55/120    avg_loss:0.019, val_acc:0.967]
Epoch [56/120    avg_loss:0.023, val_acc:0.963]
Epoch [57/120    avg_loss:0.043, val_acc:0.966]
Epoch [58/120    avg_loss:0.042, val_acc:0.960]
Epoch [59/120    avg_loss:0.025, val_acc:0.965]
Epoch [60/120    avg_loss:0.026, val_acc:0.965]
Epoch [61/120    avg_loss:0.020, val_acc:0.972]
Epoch [62/120    avg_loss:0.015, val_acc:0.966]
Epoch [63/120    avg_loss:0.016, val_acc:0.971]
Epoch [64/120    avg_loss:0.020, val_acc:0.970]
Epoch [65/120    avg_loss:0.007, val_acc:0.976]
Epoch [66/120    avg_loss:0.017, val_acc:0.965]
Epoch [67/120    avg_loss:0.026, val_acc:0.968]
Epoch [68/120    avg_loss:0.104, val_acc:0.935]
Epoch [69/120    avg_loss:0.040, val_acc:0.957]
Epoch [70/120    avg_loss:0.016, val_acc:0.971]
Epoch [71/120    avg_loss:0.016, val_acc:0.976]
Epoch [72/120    avg_loss:0.015, val_acc:0.969]
Epoch [73/120    avg_loss:0.010, val_acc:0.975]
Epoch [74/120    avg_loss:0.045, val_acc:0.977]
Epoch [75/120    avg_loss:0.007, val_acc:0.978]
Epoch [76/120    avg_loss:0.006, val_acc:0.977]
Epoch [77/120    avg_loss:0.011, val_acc:0.975]
Epoch [78/120    avg_loss:0.009, val_acc:0.980]
Epoch [79/120    avg_loss:0.015, val_acc:0.980]
Epoch [80/120    avg_loss:0.008, val_acc:0.976]
Epoch [81/120    avg_loss:0.017, val_acc:0.974]
Epoch [82/120    avg_loss:0.008, val_acc:0.980]
Epoch [83/120    avg_loss:0.005, val_acc:0.981]
Epoch [84/120    avg_loss:0.006, val_acc:0.974]
Epoch [85/120    avg_loss:0.007, val_acc:0.977]
Epoch [86/120    avg_loss:0.011, val_acc:0.967]
Epoch [87/120    avg_loss:0.009, val_acc:0.972]
Epoch [88/120    avg_loss:0.012, val_acc:0.979]
Epoch [89/120    avg_loss:0.007, val_acc:0.984]
Epoch [90/120    avg_loss:0.006, val_acc:0.979]
Epoch [91/120    avg_loss:0.008, val_acc:0.964]
Epoch [92/120    avg_loss:0.010, val_acc:0.969]
Epoch [93/120    avg_loss:0.005, val_acc:0.972]
Epoch [94/120    avg_loss:0.008, val_acc:0.974]
Epoch [95/120    avg_loss:0.004, val_acc:0.976]
Epoch [96/120    avg_loss:0.007, val_acc:0.970]
Epoch [97/120    avg_loss:0.008, val_acc:0.969]
Epoch [98/120    avg_loss:0.022, val_acc:0.961]
Epoch [99/120    avg_loss:0.006, val_acc:0.972]
Epoch [100/120    avg_loss:0.008, val_acc:0.968]
Epoch [101/120    avg_loss:0.009, val_acc:0.980]
Epoch [102/120    avg_loss:0.006, val_acc:0.975]
Epoch [103/120    avg_loss:0.005, val_acc:0.975]
Epoch [104/120    avg_loss:0.008, val_acc:0.978]
Epoch [105/120    avg_loss:0.003, val_acc:0.979]
Epoch [106/120    avg_loss:0.005, val_acc:0.978]
Epoch [107/120    avg_loss:0.007, val_acc:0.978]
Epoch [108/120    avg_loss:0.006, val_acc:0.979]
Epoch [109/120    avg_loss:0.003, val_acc:0.979]
Epoch [110/120    avg_loss:0.004, val_acc:0.980]
Epoch [111/120    avg_loss:0.003, val_acc:0.979]
Epoch [112/120    avg_loss:0.003, val_acc:0.979]
Epoch [113/120    avg_loss:0.003, val_acc:0.979]
Epoch [114/120    avg_loss:0.003, val_acc:0.980]
Epoch [115/120    avg_loss:0.002, val_acc:0.980]
Epoch [116/120    avg_loss:0.003, val_acc:0.980]
Epoch [117/120    avg_loss:0.003, val_acc:0.980]
Epoch [118/120    avg_loss:0.003, val_acc:0.980]
Epoch [119/120    avg_loss:0.004, val_acc:0.980]
Epoch [120/120    avg_loss:0.003, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1263    0    0    3    2    0    0    0    4    5    5    0
     0    3    0]
 [   0    0    0  691    0   21    0    0    0    3    7    0   22    3
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    0    0    0    0    0    0    0
     8    0    0]
 [   0    0    0    0    0    0  652    0    0    0    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    6    0    0    0    0  862    0    0    0
     1    2    0]
 [   0    0   20    0    0    1    4    0    0    0    4 2179    1    0
     0    1    0]
 [   0    0    0    0    2    8    0    0    0    0    3    2  511    0
     1    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   17    0    0    1    0    3    0    0    0
  1118    0    0]
 [   0    0    0    1    0    1    4    0    0    0    0    0    0    0
    51  290    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.34417344173441

F1 scores:
[       nan 0.975      0.98173339 0.95640138 0.99061033 0.92927095
 0.98862775 1.         0.99883856 0.8        0.98010233 0.99135578
 0.95158287 0.9919571  0.96254843 0.90202177 0.95402299]

Kappa:
0.9697273149849218
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f56840776a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.184, val_acc:0.513]
Epoch [2/120    avg_loss:1.754, val_acc:0.569]
Epoch [3/120    avg_loss:1.346, val_acc:0.637]
Epoch [4/120    avg_loss:1.366, val_acc:0.701]
Epoch [5/120    avg_loss:1.051, val_acc:0.709]
Epoch [6/120    avg_loss:0.978, val_acc:0.729]
Epoch [7/120    avg_loss:0.836, val_acc:0.765]
Epoch [8/120    avg_loss:0.778, val_acc:0.715]
Epoch [9/120    avg_loss:0.763, val_acc:0.761]
Epoch [10/120    avg_loss:0.653, val_acc:0.837]
Epoch [11/120    avg_loss:0.698, val_acc:0.846]
Epoch [12/120    avg_loss:0.504, val_acc:0.893]
Epoch [13/120    avg_loss:0.396, val_acc:0.850]
Epoch [14/120    avg_loss:0.398, val_acc:0.779]
Epoch [15/120    avg_loss:0.404, val_acc:0.860]
Epoch [16/120    avg_loss:0.364, val_acc:0.878]
Epoch [17/120    avg_loss:0.337, val_acc:0.894]
Epoch [18/120    avg_loss:0.262, val_acc:0.910]
Epoch [19/120    avg_loss:0.247, val_acc:0.828]
Epoch [20/120    avg_loss:0.287, val_acc:0.943]
Epoch [21/120    avg_loss:0.157, val_acc:0.909]
Epoch [22/120    avg_loss:0.329, val_acc:0.918]
Epoch [23/120    avg_loss:0.213, val_acc:0.938]
Epoch [24/120    avg_loss:0.145, val_acc:0.933]
Epoch [25/120    avg_loss:0.096, val_acc:0.942]
Epoch [26/120    avg_loss:0.202, val_acc:0.894]
Epoch [27/120    avg_loss:0.142, val_acc:0.944]
Epoch [28/120    avg_loss:0.081, val_acc:0.963]
Epoch [29/120    avg_loss:0.068, val_acc:0.952]
Epoch [30/120    avg_loss:0.080, val_acc:0.955]
Epoch [31/120    avg_loss:0.086, val_acc:0.950]
Epoch [32/120    avg_loss:0.063, val_acc:0.965]
Epoch [33/120    avg_loss:0.049, val_acc:0.969]
Epoch [34/120    avg_loss:0.051, val_acc:0.888]
Epoch [35/120    avg_loss:0.056, val_acc:0.967]
Epoch [36/120    avg_loss:0.030, val_acc:0.979]
Epoch [37/120    avg_loss:0.046, val_acc:0.943]
Epoch [38/120    avg_loss:0.061, val_acc:0.951]
Epoch [39/120    avg_loss:0.067, val_acc:0.973]
Epoch [40/120    avg_loss:0.026, val_acc:0.966]
Epoch [41/120    avg_loss:0.083, val_acc:0.964]
Epoch [42/120    avg_loss:0.049, val_acc:0.955]
Epoch [43/120    avg_loss:0.090, val_acc:0.959]
Epoch [44/120    avg_loss:0.076, val_acc:0.965]
Epoch [45/120    avg_loss:0.061, val_acc:0.970]
Epoch [46/120    avg_loss:0.080, val_acc:0.968]
Epoch [47/120    avg_loss:0.022, val_acc:0.981]
Epoch [48/120    avg_loss:0.032, val_acc:0.987]
Epoch [49/120    avg_loss:0.019, val_acc:0.982]
Epoch [50/120    avg_loss:0.042, val_acc:0.974]
Epoch [51/120    avg_loss:0.040, val_acc:0.976]
Epoch [52/120    avg_loss:0.045, val_acc:0.969]
Epoch [53/120    avg_loss:0.061, val_acc:0.971]
Epoch [54/120    avg_loss:0.083, val_acc:0.970]
Epoch [55/120    avg_loss:0.026, val_acc:0.965]
Epoch [56/120    avg_loss:0.038, val_acc:0.968]
Epoch [57/120    avg_loss:0.027, val_acc:0.972]
Epoch [58/120    avg_loss:0.050, val_acc:0.942]
Epoch [59/120    avg_loss:0.056, val_acc:0.964]
Epoch [60/120    avg_loss:0.020, val_acc:0.969]
Epoch [61/120    avg_loss:0.026, val_acc:0.964]
Epoch [62/120    avg_loss:0.029, val_acc:0.979]
Epoch [63/120    avg_loss:0.014, val_acc:0.980]
Epoch [64/120    avg_loss:0.016, val_acc:0.979]
Epoch [65/120    avg_loss:0.010, val_acc:0.981]
Epoch [66/120    avg_loss:0.013, val_acc:0.982]
Epoch [67/120    avg_loss:0.011, val_acc:0.982]
Epoch [68/120    avg_loss:0.007, val_acc:0.983]
Epoch [69/120    avg_loss:0.010, val_acc:0.984]
Epoch [70/120    avg_loss:0.005, val_acc:0.984]
Epoch [71/120    avg_loss:0.007, val_acc:0.983]
Epoch [72/120    avg_loss:0.008, val_acc:0.985]
Epoch [73/120    avg_loss:0.007, val_acc:0.985]
Epoch [74/120    avg_loss:0.008, val_acc:0.984]
Epoch [75/120    avg_loss:0.012, val_acc:0.984]
Epoch [76/120    avg_loss:0.008, val_acc:0.984]
Epoch [77/120    avg_loss:0.006, val_acc:0.984]
Epoch [78/120    avg_loss:0.009, val_acc:0.984]
Epoch [79/120    avg_loss:0.007, val_acc:0.985]
Epoch [80/120    avg_loss:0.010, val_acc:0.985]
Epoch [81/120    avg_loss:0.005, val_acc:0.985]
Epoch [82/120    avg_loss:0.008, val_acc:0.985]
Epoch [83/120    avg_loss:0.013, val_acc:0.985]
Epoch [84/120    avg_loss:0.012, val_acc:0.985]
Epoch [85/120    avg_loss:0.008, val_acc:0.985]
Epoch [86/120    avg_loss:0.008, val_acc:0.985]
Epoch [87/120    avg_loss:0.010, val_acc:0.985]
Epoch [88/120    avg_loss:0.007, val_acc:0.985]
Epoch [89/120    avg_loss:0.007, val_acc:0.985]
Epoch [90/120    avg_loss:0.005, val_acc:0.985]
Epoch [91/120    avg_loss:0.007, val_acc:0.985]
Epoch [92/120    avg_loss:0.010, val_acc:0.985]
Epoch [93/120    avg_loss:0.006, val_acc:0.985]
Epoch [94/120    avg_loss:0.006, val_acc:0.985]
Epoch [95/120    avg_loss:0.005, val_acc:0.985]
Epoch [96/120    avg_loss:0.006, val_acc:0.985]
Epoch [97/120    avg_loss:0.006, val_acc:0.985]
Epoch [98/120    avg_loss:0.011, val_acc:0.985]
Epoch [99/120    avg_loss:0.007, val_acc:0.985]
Epoch [100/120    avg_loss:0.006, val_acc:0.985]
Epoch [101/120    avg_loss:0.011, val_acc:0.985]
Epoch [102/120    avg_loss:0.007, val_acc:0.985]
Epoch [103/120    avg_loss:0.007, val_acc:0.985]
Epoch [104/120    avg_loss:0.007, val_acc:0.985]
Epoch [105/120    avg_loss:0.012, val_acc:0.985]
Epoch [106/120    avg_loss:0.012, val_acc:0.985]
Epoch [107/120    avg_loss:0.005, val_acc:0.985]
Epoch [108/120    avg_loss:0.007, val_acc:0.985]
Epoch [109/120    avg_loss:0.005, val_acc:0.985]
Epoch [110/120    avg_loss:0.007, val_acc:0.985]
Epoch [111/120    avg_loss:0.008, val_acc:0.985]
Epoch [112/120    avg_loss:0.007, val_acc:0.985]
Epoch [113/120    avg_loss:0.008, val_acc:0.985]
Epoch [114/120    avg_loss:0.009, val_acc:0.985]
Epoch [115/120    avg_loss:0.013, val_acc:0.985]
Epoch [116/120    avg_loss:0.010, val_acc:0.985]
Epoch [117/120    avg_loss:0.009, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.985]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    0    5    1    4    0    0    0    5    0    0    0
     0    3    0]
 [   0    0    0  682   13   26    0    0    0    4    7    0   13    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   17    0    0    1    0    0    0    0  847    4    3    0
     0    3    0]
 [   0    0   22    0    0    0    6    0    0    0    6 2174    0    2
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0  529    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   11    0    0    1    0    1    0    0    0
  1125    1    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
     0  346    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.12466124661246

F1 scores:
[       nan 1.         0.97800077 0.95251397 0.95945946 0.95394737
 0.99169811 1.         0.99883856 0.81081081 0.97300402 0.99088423
 0.97781885 0.98930481 0.99381625 0.98857143 0.97005988]

Kappa:
0.9786405755013313
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4affce0630>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.223, val_acc:0.534]
Epoch [2/120    avg_loss:1.771, val_acc:0.571]
Epoch [3/120    avg_loss:1.390, val_acc:0.607]
Epoch [4/120    avg_loss:1.284, val_acc:0.585]
Epoch [5/120    avg_loss:1.090, val_acc:0.716]
Epoch [6/120    avg_loss:1.063, val_acc:0.724]
Epoch [7/120    avg_loss:0.906, val_acc:0.685]
Epoch [8/120    avg_loss:0.788, val_acc:0.812]
Epoch [9/120    avg_loss:0.653, val_acc:0.803]
Epoch [10/120    avg_loss:0.626, val_acc:0.829]
Epoch [11/120    avg_loss:0.523, val_acc:0.835]
Epoch [12/120    avg_loss:0.652, val_acc:0.847]
Epoch [13/120    avg_loss:0.615, val_acc:0.844]
Epoch [14/120    avg_loss:0.365, val_acc:0.856]
Epoch [15/120    avg_loss:0.432, val_acc:0.887]
Epoch [16/120    avg_loss:0.336, val_acc:0.903]
Epoch [17/120    avg_loss:0.393, val_acc:0.855]
Epoch [18/120    avg_loss:0.344, val_acc:0.933]
Epoch [19/120    avg_loss:0.191, val_acc:0.903]
Epoch [20/120    avg_loss:0.240, val_acc:0.914]
Epoch [21/120    avg_loss:0.168, val_acc:0.927]
Epoch [22/120    avg_loss:0.167, val_acc:0.945]
Epoch [23/120    avg_loss:0.240, val_acc:0.907]
Epoch [24/120    avg_loss:0.252, val_acc:0.882]
Epoch [25/120    avg_loss:0.178, val_acc:0.930]
Epoch [26/120    avg_loss:0.161, val_acc:0.925]
Epoch [27/120    avg_loss:0.194, val_acc:0.946]
Epoch [28/120    avg_loss:0.163, val_acc:0.915]
Epoch [29/120    avg_loss:0.145, val_acc:0.951]
Epoch [30/120    avg_loss:0.086, val_acc:0.931]
Epoch [31/120    avg_loss:0.093, val_acc:0.949]
Epoch [32/120    avg_loss:0.072, val_acc:0.952]
Epoch [33/120    avg_loss:0.053, val_acc:0.949]
Epoch [34/120    avg_loss:0.044, val_acc:0.934]
Epoch [35/120    avg_loss:0.077, val_acc:0.959]
Epoch [36/120    avg_loss:0.350, val_acc:0.932]
Epoch [37/120    avg_loss:0.133, val_acc:0.943]
Epoch [38/120    avg_loss:0.100, val_acc:0.936]
Epoch [39/120    avg_loss:0.085, val_acc:0.949]
Epoch [40/120    avg_loss:0.099, val_acc:0.948]
Epoch [41/120    avg_loss:0.097, val_acc:0.950]
Epoch [42/120    avg_loss:0.060, val_acc:0.958]
Epoch [43/120    avg_loss:0.045, val_acc:0.961]
Epoch [44/120    avg_loss:0.048, val_acc:0.960]
Epoch [45/120    avg_loss:0.052, val_acc:0.958]
Epoch [46/120    avg_loss:0.071, val_acc:0.947]
Epoch [47/120    avg_loss:0.046, val_acc:0.943]
Epoch [48/120    avg_loss:0.076, val_acc:0.946]
Epoch [49/120    avg_loss:0.036, val_acc:0.958]
Epoch [50/120    avg_loss:0.023, val_acc:0.970]
Epoch [51/120    avg_loss:0.032, val_acc:0.966]
Epoch [52/120    avg_loss:0.022, val_acc:0.974]
Epoch [53/120    avg_loss:0.039, val_acc:0.968]
Epoch [54/120    avg_loss:0.041, val_acc:0.968]
Epoch [55/120    avg_loss:0.012, val_acc:0.975]
Epoch [56/120    avg_loss:0.016, val_acc:0.974]
Epoch [57/120    avg_loss:0.027, val_acc:0.970]
Epoch [58/120    avg_loss:0.016, val_acc:0.977]
Epoch [59/120    avg_loss:0.038, val_acc:0.970]
Epoch [60/120    avg_loss:0.044, val_acc:0.854]
Epoch [61/120    avg_loss:0.150, val_acc:0.946]
Epoch [62/120    avg_loss:0.047, val_acc:0.960]
Epoch [63/120    avg_loss:0.044, val_acc:0.974]
Epoch [64/120    avg_loss:0.085, val_acc:0.963]
Epoch [65/120    avg_loss:0.026, val_acc:0.963]
Epoch [66/120    avg_loss:0.028, val_acc:0.972]
Epoch [67/120    avg_loss:0.015, val_acc:0.974]
Epoch [68/120    avg_loss:0.014, val_acc:0.977]
Epoch [69/120    avg_loss:0.015, val_acc:0.976]
Epoch [70/120    avg_loss:0.010, val_acc:0.978]
Epoch [71/120    avg_loss:0.013, val_acc:0.976]
Epoch [72/120    avg_loss:0.008, val_acc:0.977]
Epoch [73/120    avg_loss:0.007, val_acc:0.979]
Epoch [74/120    avg_loss:0.006, val_acc:0.982]
Epoch [75/120    avg_loss:0.007, val_acc:0.977]
Epoch [76/120    avg_loss:0.016, val_acc:0.975]
Epoch [77/120    avg_loss:0.008, val_acc:0.974]
Epoch [78/120    avg_loss:0.014, val_acc:0.965]
Epoch [79/120    avg_loss:0.007, val_acc:0.976]
Epoch [80/120    avg_loss:0.005, val_acc:0.978]
Epoch [81/120    avg_loss:0.004, val_acc:0.980]
Epoch [82/120    avg_loss:0.008, val_acc:0.977]
Epoch [83/120    avg_loss:0.005, val_acc:0.976]
Epoch [84/120    avg_loss:0.005, val_acc:0.976]
Epoch [85/120    avg_loss:0.005, val_acc:0.978]
Epoch [86/120    avg_loss:0.011, val_acc:0.975]
Epoch [87/120    avg_loss:0.009, val_acc:0.978]
Epoch [88/120    avg_loss:0.009, val_acc:0.976]
Epoch [89/120    avg_loss:0.005, val_acc:0.978]
Epoch [90/120    avg_loss:0.003, val_acc:0.977]
Epoch [91/120    avg_loss:0.004, val_acc:0.978]
Epoch [92/120    avg_loss:0.006, val_acc:0.980]
Epoch [93/120    avg_loss:0.003, val_acc:0.980]
Epoch [94/120    avg_loss:0.005, val_acc:0.979]
Epoch [95/120    avg_loss:0.007, val_acc:0.979]
Epoch [96/120    avg_loss:0.003, val_acc:0.979]
Epoch [97/120    avg_loss:0.006, val_acc:0.979]
Epoch [98/120    avg_loss:0.005, val_acc:0.977]
Epoch [99/120    avg_loss:0.004, val_acc:0.978]
Epoch [100/120    avg_loss:0.005, val_acc:0.977]
Epoch [101/120    avg_loss:0.003, val_acc:0.977]
Epoch [102/120    avg_loss:0.006, val_acc:0.977]
Epoch [103/120    avg_loss:0.004, val_acc:0.977]
Epoch [104/120    avg_loss:0.004, val_acc:0.977]
Epoch [105/120    avg_loss:0.005, val_acc:0.976]
Epoch [106/120    avg_loss:0.004, val_acc:0.976]
Epoch [107/120    avg_loss:0.003, val_acc:0.976]
Epoch [108/120    avg_loss:0.004, val_acc:0.976]
Epoch [109/120    avg_loss:0.005, val_acc:0.976]
Epoch [110/120    avg_loss:0.004, val_acc:0.976]
Epoch [111/120    avg_loss:0.004, val_acc:0.976]
Epoch [112/120    avg_loss:0.004, val_acc:0.976]
Epoch [113/120    avg_loss:0.003, val_acc:0.976]
Epoch [114/120    avg_loss:0.005, val_acc:0.976]
Epoch [115/120    avg_loss:0.004, val_acc:0.976]
Epoch [116/120    avg_loss:0.006, val_acc:0.976]
Epoch [117/120    avg_loss:0.005, val_acc:0.976]
Epoch [118/120    avg_loss:0.002, val_acc:0.976]
Epoch [119/120    avg_loss:0.003, val_acc:0.976]
Epoch [120/120    avg_loss:0.003, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1262    4    0    0    2    0    0    0    7    4    1    0
     0    5    0]
 [   0    0    1  670    0   16    3    0    0    2    7    4   43    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    2    0    3    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    3    0    0    0    0  855    6    0    0
     1    6    0]
 [   0    0    6    0    0    2    4    0    0    0    1 2193    1    2
     1    0    0]
 [   0    0    0    0    0    5    0    0    0    0    0    0  525    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    1    0    0    0
  1126    5    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    25  303    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.63685636856368

F1 scores:
[       nan 0.975      0.98632278 0.94299789 1.         0.95418994
 0.97913562 0.96153846 0.99649942 0.87804878 0.97882084 0.99298166
 0.94679892 0.9919571  0.98126362 0.90990991 0.96470588]

Kappa:
0.9730643745175549
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:05:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f65e5937710>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.169, val_acc:0.523]
Epoch [2/120    avg_loss:1.816, val_acc:0.472]
Epoch [3/120    avg_loss:1.449, val_acc:0.556]
Epoch [4/120    avg_loss:1.193, val_acc:0.693]
Epoch [5/120    avg_loss:1.150, val_acc:0.696]
Epoch [6/120    avg_loss:1.044, val_acc:0.731]
Epoch [7/120    avg_loss:0.954, val_acc:0.794]
Epoch [8/120    avg_loss:0.781, val_acc:0.808]
Epoch [9/120    avg_loss:0.692, val_acc:0.811]
Epoch [10/120    avg_loss:0.645, val_acc:0.806]
Epoch [11/120    avg_loss:0.734, val_acc:0.841]
Epoch [12/120    avg_loss:0.572, val_acc:0.841]
Epoch [13/120    avg_loss:0.535, val_acc:0.854]
Epoch [14/120    avg_loss:0.434, val_acc:0.861]
Epoch [15/120    avg_loss:0.472, val_acc:0.848]
Epoch [16/120    avg_loss:0.384, val_acc:0.904]
Epoch [17/120    avg_loss:0.339, val_acc:0.905]
Epoch [18/120    avg_loss:0.291, val_acc:0.860]
Epoch [19/120    avg_loss:0.347, val_acc:0.868]
Epoch [20/120    avg_loss:0.270, val_acc:0.889]
Epoch [21/120    avg_loss:0.290, val_acc:0.860]
Epoch [22/120    avg_loss:0.267, val_acc:0.914]
Epoch [23/120    avg_loss:0.186, val_acc:0.919]
Epoch [24/120    avg_loss:0.163, val_acc:0.943]
Epoch [25/120    avg_loss:0.168, val_acc:0.942]
Epoch [26/120    avg_loss:0.139, val_acc:0.932]
Epoch [27/120    avg_loss:0.178, val_acc:0.941]
Epoch [28/120    avg_loss:0.244, val_acc:0.907]
Epoch [29/120    avg_loss:0.117, val_acc:0.958]
Epoch [30/120    avg_loss:0.110, val_acc:0.933]
Epoch [31/120    avg_loss:0.108, val_acc:0.928]
Epoch [32/120    avg_loss:0.150, val_acc:0.936]
Epoch [33/120    avg_loss:0.155, val_acc:0.930]
Epoch [34/120    avg_loss:0.153, val_acc:0.892]
Epoch [35/120    avg_loss:0.200, val_acc:0.938]
Epoch [36/120    avg_loss:0.142, val_acc:0.951]
Epoch [37/120    avg_loss:0.057, val_acc:0.965]
Epoch [38/120    avg_loss:0.103, val_acc:0.960]
Epoch [39/120    avg_loss:0.101, val_acc:0.922]
Epoch [40/120    avg_loss:0.083, val_acc:0.972]
Epoch [41/120    avg_loss:0.072, val_acc:0.957]
Epoch [42/120    avg_loss:0.072, val_acc:0.953]
Epoch [43/120    avg_loss:0.055, val_acc:0.976]
Epoch [44/120    avg_loss:0.043, val_acc:0.953]
Epoch [45/120    avg_loss:0.168, val_acc:0.945]
Epoch [46/120    avg_loss:0.096, val_acc:0.956]
Epoch [47/120    avg_loss:0.069, val_acc:0.961]
Epoch [48/120    avg_loss:0.091, val_acc:0.963]
Epoch [49/120    avg_loss:0.049, val_acc:0.967]
Epoch [50/120    avg_loss:0.066, val_acc:0.959]
Epoch [51/120    avg_loss:0.045, val_acc:0.958]
Epoch [52/120    avg_loss:0.061, val_acc:0.967]
Epoch [53/120    avg_loss:0.076, val_acc:0.968]
Epoch [54/120    avg_loss:0.040, val_acc:0.953]
Epoch [55/120    avg_loss:0.023, val_acc:0.970]
Epoch [56/120    avg_loss:0.015, val_acc:0.979]
Epoch [57/120    avg_loss:0.018, val_acc:0.977]
Epoch [58/120    avg_loss:0.022, val_acc:0.982]
Epoch [59/120    avg_loss:0.033, val_acc:0.982]
Epoch [60/120    avg_loss:0.059, val_acc:0.965]
Epoch [61/120    avg_loss:0.029, val_acc:0.968]
Epoch [62/120    avg_loss:0.077, val_acc:0.966]
Epoch [63/120    avg_loss:0.059, val_acc:0.972]
Epoch [64/120    avg_loss:0.027, val_acc:0.979]
Epoch [65/120    avg_loss:0.034, val_acc:0.974]
Epoch [66/120    avg_loss:0.019, val_acc:0.977]
Epoch [67/120    avg_loss:0.015, val_acc:0.974]
Epoch [68/120    avg_loss:0.029, val_acc:0.977]
Epoch [69/120    avg_loss:0.023, val_acc:0.978]
Epoch [70/120    avg_loss:0.026, val_acc:0.978]
Epoch [71/120    avg_loss:0.018, val_acc:0.957]
Epoch [72/120    avg_loss:0.057, val_acc:0.968]
Epoch [73/120    avg_loss:0.030, val_acc:0.977]
Epoch [74/120    avg_loss:0.021, val_acc:0.980]
Epoch [75/120    avg_loss:0.017, val_acc:0.980]
Epoch [76/120    avg_loss:0.023, val_acc:0.981]
Epoch [77/120    avg_loss:0.017, val_acc:0.982]
Epoch [78/120    avg_loss:0.010, val_acc:0.983]
Epoch [79/120    avg_loss:0.010, val_acc:0.982]
Epoch [80/120    avg_loss:0.014, val_acc:0.981]
Epoch [81/120    avg_loss:0.018, val_acc:0.981]
Epoch [82/120    avg_loss:0.014, val_acc:0.981]
Epoch [83/120    avg_loss:0.012, val_acc:0.982]
Epoch [84/120    avg_loss:0.010, val_acc:0.982]
Epoch [85/120    avg_loss:0.007, val_acc:0.983]
Epoch [86/120    avg_loss:0.013, val_acc:0.982]
Epoch [87/120    avg_loss:0.008, val_acc:0.982]
Epoch [88/120    avg_loss:0.008, val_acc:0.982]
Epoch [89/120    avg_loss:0.015, val_acc:0.982]
Epoch [90/120    avg_loss:0.008, val_acc:0.982]
Epoch [91/120    avg_loss:0.007, val_acc:0.983]
Epoch [92/120    avg_loss:0.009, val_acc:0.982]
Epoch [93/120    avg_loss:0.012, val_acc:0.983]
Epoch [94/120    avg_loss:0.009, val_acc:0.983]
Epoch [95/120    avg_loss:0.006, val_acc:0.983]
Epoch [96/120    avg_loss:0.009, val_acc:0.982]
Epoch [97/120    avg_loss:0.007, val_acc:0.984]
Epoch [98/120    avg_loss:0.006, val_acc:0.984]
Epoch [99/120    avg_loss:0.006, val_acc:0.984]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.014, val_acc:0.984]
Epoch [102/120    avg_loss:0.007, val_acc:0.984]
Epoch [103/120    avg_loss:0.008, val_acc:0.983]
Epoch [104/120    avg_loss:0.006, val_acc:0.983]
Epoch [105/120    avg_loss:0.006, val_acc:0.984]
Epoch [106/120    avg_loss:0.011, val_acc:0.983]
Epoch [107/120    avg_loss:0.009, val_acc:0.985]
Epoch [108/120    avg_loss:0.009, val_acc:0.981]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.005, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.985]
Epoch [112/120    avg_loss:0.005, val_acc:0.985]
Epoch [113/120    avg_loss:0.006, val_acc:0.986]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.005, val_acc:0.985]
Epoch [116/120    avg_loss:0.008, val_acc:0.985]
Epoch [117/120    avg_loss:0.006, val_acc:0.986]
Epoch [118/120    avg_loss:0.003, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    2 1263    0    0    0    1    0    0    0    9    4    3    0
     0    3    0]
 [   0    0    0  677    0    5    0    0    0    3    3    0   56    3
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0  864    3    1    0
     0    6    0]
 [   0    0    3    0    0    3    2    0    0    0    3 2198    0    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0  527    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    3    0    0    0    8    0    0    1    0    0    0    0    0
  1125    2    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    17  322    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.15718157181571

F1 scores:
[       nan 0.94252874 0.98981191 0.94685315 0.99528302 0.97963801
 0.99169811 1.         0.99883856 0.75675676 0.98405467 0.99569649
 0.93939394 0.98930481 0.98640947 0.94705882 0.96511628]

Kappa:
0.978999965572397
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe31b40b6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.217, val_acc:0.443]
Epoch [2/120    avg_loss:1.801, val_acc:0.465]
Epoch [3/120    avg_loss:1.487, val_acc:0.596]
Epoch [4/120    avg_loss:1.259, val_acc:0.685]
Epoch [5/120    avg_loss:1.015, val_acc:0.739]
Epoch [6/120    avg_loss:1.127, val_acc:0.705]
Epoch [7/120    avg_loss:0.754, val_acc:0.772]
Epoch [8/120    avg_loss:0.851, val_acc:0.776]
Epoch [9/120    avg_loss:0.783, val_acc:0.800]
Epoch [10/120    avg_loss:0.663, val_acc:0.838]
Epoch [11/120    avg_loss:0.467, val_acc:0.841]
Epoch [12/120    avg_loss:0.477, val_acc:0.854]
Epoch [13/120    avg_loss:0.315, val_acc:0.870]
Epoch [14/120    avg_loss:0.321, val_acc:0.894]
Epoch [15/120    avg_loss:0.429, val_acc:0.880]
Epoch [16/120    avg_loss:0.424, val_acc:0.884]
Epoch [17/120    avg_loss:0.361, val_acc:0.861]
Epoch [18/120    avg_loss:0.214, val_acc:0.908]
Epoch [19/120    avg_loss:0.156, val_acc:0.904]
Epoch [20/120    avg_loss:0.230, val_acc:0.912]
Epoch [21/120    avg_loss:0.207, val_acc:0.913]
Epoch [22/120    avg_loss:0.133, val_acc:0.929]
Epoch [23/120    avg_loss:0.206, val_acc:0.929]
Epoch [24/120    avg_loss:0.211, val_acc:0.929]
Epoch [25/120    avg_loss:0.143, val_acc:0.931]
Epoch [26/120    avg_loss:0.111, val_acc:0.919]
Epoch [27/120    avg_loss:0.110, val_acc:0.910]
Epoch [28/120    avg_loss:0.143, val_acc:0.916]
Epoch [29/120    avg_loss:0.200, val_acc:0.893]
Epoch [30/120    avg_loss:0.199, val_acc:0.943]
Epoch [31/120    avg_loss:0.177, val_acc:0.937]
Epoch [32/120    avg_loss:0.136, val_acc:0.928]
Epoch [33/120    avg_loss:0.116, val_acc:0.941]
Epoch [34/120    avg_loss:0.089, val_acc:0.955]
Epoch [35/120    avg_loss:0.058, val_acc:0.962]
Epoch [36/120    avg_loss:0.040, val_acc:0.962]
Epoch [37/120    avg_loss:0.065, val_acc:0.952]
Epoch [38/120    avg_loss:0.071, val_acc:0.953]
Epoch [39/120    avg_loss:0.057, val_acc:0.968]
Epoch [40/120    avg_loss:0.050, val_acc:0.956]
Epoch [41/120    avg_loss:0.067, val_acc:0.935]
Epoch [42/120    avg_loss:0.074, val_acc:0.957]
Epoch [43/120    avg_loss:0.049, val_acc:0.963]
Epoch [44/120    avg_loss:0.042, val_acc:0.949]
Epoch [45/120    avg_loss:0.048, val_acc:0.966]
Epoch [46/120    avg_loss:0.029, val_acc:0.967]
Epoch [47/120    avg_loss:0.021, val_acc:0.971]
Epoch [48/120    avg_loss:0.035, val_acc:0.971]
Epoch [49/120    avg_loss:0.027, val_acc:0.968]
Epoch [50/120    avg_loss:0.022, val_acc:0.966]
Epoch [51/120    avg_loss:0.029, val_acc:0.964]
Epoch [52/120    avg_loss:0.087, val_acc:0.964]
Epoch [53/120    avg_loss:0.030, val_acc:0.960]
Epoch [54/120    avg_loss:0.037, val_acc:0.953]
Epoch [55/120    avg_loss:0.040, val_acc:0.954]
Epoch [56/120    avg_loss:0.016, val_acc:0.964]
Epoch [57/120    avg_loss:0.015, val_acc:0.973]
Epoch [58/120    avg_loss:0.028, val_acc:0.972]
Epoch [59/120    avg_loss:0.033, val_acc:0.965]
Epoch [60/120    avg_loss:0.018, val_acc:0.970]
Epoch [61/120    avg_loss:0.026, val_acc:0.956]
Epoch [62/120    avg_loss:0.072, val_acc:0.958]
Epoch [63/120    avg_loss:0.027, val_acc:0.972]
Epoch [64/120    avg_loss:0.015, val_acc:0.970]
Epoch [65/120    avg_loss:0.019, val_acc:0.974]
Epoch [66/120    avg_loss:0.017, val_acc:0.966]
Epoch [67/120    avg_loss:0.014, val_acc:0.975]
Epoch [68/120    avg_loss:0.011, val_acc:0.973]
Epoch [69/120    avg_loss:0.019, val_acc:0.976]
Epoch [70/120    avg_loss:0.017, val_acc:0.979]
Epoch [71/120    avg_loss:0.012, val_acc:0.971]
Epoch [72/120    avg_loss:0.008, val_acc:0.977]
Epoch [73/120    avg_loss:0.006, val_acc:0.978]
Epoch [74/120    avg_loss:0.007, val_acc:0.980]
Epoch [75/120    avg_loss:0.008, val_acc:0.976]
Epoch [76/120    avg_loss:0.007, val_acc:0.979]
Epoch [77/120    avg_loss:0.004, val_acc:0.979]
Epoch [78/120    avg_loss:0.009, val_acc:0.983]
Epoch [79/120    avg_loss:0.015, val_acc:0.981]
Epoch [80/120    avg_loss:0.009, val_acc:0.976]
Epoch [81/120    avg_loss:0.008, val_acc:0.975]
Epoch [82/120    avg_loss:0.012, val_acc:0.973]
Epoch [83/120    avg_loss:0.009, val_acc:0.976]
Epoch [84/120    avg_loss:0.007, val_acc:0.981]
Epoch [85/120    avg_loss:0.011, val_acc:0.978]
Epoch [86/120    avg_loss:0.008, val_acc:0.978]
Epoch [87/120    avg_loss:0.012, val_acc:0.979]
Epoch [88/120    avg_loss:0.008, val_acc:0.981]
Epoch [89/120    avg_loss:0.011, val_acc:0.976]
Epoch [90/120    avg_loss:0.009, val_acc:0.979]
Epoch [91/120    avg_loss:0.006, val_acc:0.981]
Epoch [92/120    avg_loss:0.007, val_acc:0.981]
Epoch [93/120    avg_loss:0.013, val_acc:0.980]
Epoch [94/120    avg_loss:0.008, val_acc:0.981]
Epoch [95/120    avg_loss:0.005, val_acc:0.981]
Epoch [96/120    avg_loss:0.003, val_acc:0.981]
Epoch [97/120    avg_loss:0.003, val_acc:0.980]
Epoch [98/120    avg_loss:0.005, val_acc:0.980]
Epoch [99/120    avg_loss:0.003, val_acc:0.981]
Epoch [100/120    avg_loss:0.008, val_acc:0.980]
Epoch [101/120    avg_loss:0.005, val_acc:0.981]
Epoch [102/120    avg_loss:0.006, val_acc:0.982]
Epoch [103/120    avg_loss:0.008, val_acc:0.981]
Epoch [104/120    avg_loss:0.005, val_acc:0.981]
Epoch [105/120    avg_loss:0.002, val_acc:0.981]
Epoch [106/120    avg_loss:0.006, val_acc:0.981]
Epoch [107/120    avg_loss:0.003, val_acc:0.981]
Epoch [108/120    avg_loss:0.004, val_acc:0.981]
Epoch [109/120    avg_loss:0.003, val_acc:0.981]
Epoch [110/120    avg_loss:0.006, val_acc:0.981]
Epoch [111/120    avg_loss:0.003, val_acc:0.981]
Epoch [112/120    avg_loss:0.003, val_acc:0.981]
Epoch [113/120    avg_loss:0.003, val_acc:0.981]
Epoch [114/120    avg_loss:0.006, val_acc:0.981]
Epoch [115/120    avg_loss:0.005, val_acc:0.981]
Epoch [116/120    avg_loss:0.003, val_acc:0.981]
Epoch [117/120    avg_loss:0.013, val_acc:0.981]
Epoch [118/120    avg_loss:0.005, val_acc:0.981]
Epoch [119/120    avg_loss:0.003, val_acc:0.981]
Epoch [120/120    avg_loss:0.003, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    2 1276    1    0    0    0    0    0    0    5    0    1    0
     0    0    0]
 [   0    0    0  682    3   10    0    0    0    3    0    0   47    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    2    0    0    0    0  863    2    0    0
     0    3    0]
 [   0    0   19    5    0    3    4    0    0    0    6 2173    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0  528    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    1    0    1    0    1    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    10  337    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.40650406504065

F1 scores:
[       nan 0.97619048 0.98723404 0.94919972 0.99300699 0.97747748
 0.99620925 1.         0.99883856 0.84210526 0.98628571 0.99110604
 0.95049505 0.99462366 0.9938704  0.98107715 0.97647059]

Kappa:
0.9818473996091222
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6e55532630>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.295, val_acc:0.443]
Epoch [2/120    avg_loss:1.705, val_acc:0.589]
Epoch [3/120    avg_loss:1.489, val_acc:0.661]
Epoch [4/120    avg_loss:1.229, val_acc:0.665]
Epoch [5/120    avg_loss:1.065, val_acc:0.705]
Epoch [6/120    avg_loss:0.883, val_acc:0.725]
Epoch [7/120    avg_loss:1.003, val_acc:0.722]
Epoch [8/120    avg_loss:0.762, val_acc:0.760]
Epoch [9/120    avg_loss:0.628, val_acc:0.781]
Epoch [10/120    avg_loss:0.542, val_acc:0.823]
Epoch [11/120    avg_loss:0.629, val_acc:0.813]
Epoch [12/120    avg_loss:0.509, val_acc:0.846]
Epoch [13/120    avg_loss:0.506, val_acc:0.834]
Epoch [14/120    avg_loss:0.489, val_acc:0.851]
Epoch [15/120    avg_loss:0.558, val_acc:0.866]
Epoch [16/120    avg_loss:0.407, val_acc:0.854]
Epoch [17/120    avg_loss:0.418, val_acc:0.833]
Epoch [18/120    avg_loss:0.313, val_acc:0.889]
Epoch [19/120    avg_loss:0.286, val_acc:0.875]
Epoch [20/120    avg_loss:0.205, val_acc:0.924]
Epoch [21/120    avg_loss:0.220, val_acc:0.881]
Epoch [22/120    avg_loss:0.181, val_acc:0.942]
Epoch [23/120    avg_loss:0.164, val_acc:0.918]
Epoch [24/120    avg_loss:0.173, val_acc:0.919]
Epoch [25/120    avg_loss:0.176, val_acc:0.857]
Epoch [26/120    avg_loss:0.236, val_acc:0.942]
Epoch [27/120    avg_loss:0.100, val_acc:0.942]
Epoch [28/120    avg_loss:0.120, val_acc:0.955]
Epoch [29/120    avg_loss:0.110, val_acc:0.913]
Epoch [30/120    avg_loss:0.264, val_acc:0.937]
Epoch [31/120    avg_loss:0.097, val_acc:0.957]
Epoch [32/120    avg_loss:0.130, val_acc:0.935]
Epoch [33/120    avg_loss:0.086, val_acc:0.936]
Epoch [34/120    avg_loss:0.075, val_acc:0.944]
Epoch [35/120    avg_loss:0.091, val_acc:0.963]
Epoch [36/120    avg_loss:0.075, val_acc:0.953]
Epoch [37/120    avg_loss:0.101, val_acc:0.963]
Epoch [38/120    avg_loss:0.055, val_acc:0.972]
Epoch [39/120    avg_loss:0.060, val_acc:0.962]
Epoch [40/120    avg_loss:0.040, val_acc:0.968]
Epoch [41/120    avg_loss:0.035, val_acc:0.976]
Epoch [42/120    avg_loss:0.056, val_acc:0.971]
Epoch [43/120    avg_loss:0.058, val_acc:0.971]
Epoch [44/120    avg_loss:0.049, val_acc:0.919]
Epoch [45/120    avg_loss:0.045, val_acc:0.972]
Epoch [46/120    avg_loss:0.034, val_acc:0.971]
Epoch [47/120    avg_loss:0.015, val_acc:0.984]
Epoch [48/120    avg_loss:0.028, val_acc:0.976]
Epoch [49/120    avg_loss:0.047, val_acc:0.970]
Epoch [50/120    avg_loss:0.050, val_acc:0.967]
Epoch [51/120    avg_loss:0.062, val_acc:0.962]
Epoch [52/120    avg_loss:0.039, val_acc:0.969]
Epoch [53/120    avg_loss:0.024, val_acc:0.983]
Epoch [54/120    avg_loss:0.022, val_acc:0.963]
Epoch [55/120    avg_loss:0.323, val_acc:0.938]
Epoch [56/120    avg_loss:0.106, val_acc:0.965]
Epoch [57/120    avg_loss:0.043, val_acc:0.971]
Epoch [58/120    avg_loss:0.038, val_acc:0.966]
Epoch [59/120    avg_loss:0.024, val_acc:0.968]
Epoch [60/120    avg_loss:0.042, val_acc:0.956]
Epoch [61/120    avg_loss:0.023, val_acc:0.971]
Epoch [62/120    avg_loss:0.018, val_acc:0.974]
Epoch [63/120    avg_loss:0.010, val_acc:0.976]
Epoch [64/120    avg_loss:0.013, val_acc:0.976]
Epoch [65/120    avg_loss:0.013, val_acc:0.978]
Epoch [66/120    avg_loss:0.013, val_acc:0.980]
Epoch [67/120    avg_loss:0.013, val_acc:0.981]
Epoch [68/120    avg_loss:0.009, val_acc:0.981]
Epoch [69/120    avg_loss:0.009, val_acc:0.982]
Epoch [70/120    avg_loss:0.015, val_acc:0.981]
Epoch [71/120    avg_loss:0.009, val_acc:0.980]
Epoch [72/120    avg_loss:0.011, val_acc:0.982]
Epoch [73/120    avg_loss:0.007, val_acc:0.980]
Epoch [74/120    avg_loss:0.011, val_acc:0.980]
Epoch [75/120    avg_loss:0.008, val_acc:0.980]
Epoch [76/120    avg_loss:0.008, val_acc:0.980]
Epoch [77/120    avg_loss:0.014, val_acc:0.980]
Epoch [78/120    avg_loss:0.009, val_acc:0.980]
Epoch [79/120    avg_loss:0.011, val_acc:0.980]
Epoch [80/120    avg_loss:0.007, val_acc:0.980]
Epoch [81/120    avg_loss:0.013, val_acc:0.981]
Epoch [82/120    avg_loss:0.011, val_acc:0.982]
Epoch [83/120    avg_loss:0.007, val_acc:0.982]
Epoch [84/120    avg_loss:0.013, val_acc:0.983]
Epoch [85/120    avg_loss:0.010, val_acc:0.983]
Epoch [86/120    avg_loss:0.008, val_acc:0.983]
Epoch [87/120    avg_loss:0.016, val_acc:0.983]
Epoch [88/120    avg_loss:0.009, val_acc:0.983]
Epoch [89/120    avg_loss:0.010, val_acc:0.983]
Epoch [90/120    avg_loss:0.015, val_acc:0.983]
Epoch [91/120    avg_loss:0.006, val_acc:0.983]
Epoch [92/120    avg_loss:0.009, val_acc:0.983]
Epoch [93/120    avg_loss:0.012, val_acc:0.983]
Epoch [94/120    avg_loss:0.016, val_acc:0.983]
Epoch [95/120    avg_loss:0.011, val_acc:0.983]
Epoch [96/120    avg_loss:0.009, val_acc:0.983]
Epoch [97/120    avg_loss:0.006, val_acc:0.983]
Epoch [98/120    avg_loss:0.009, val_acc:0.983]
Epoch [99/120    avg_loss:0.009, val_acc:0.983]
Epoch [100/120    avg_loss:0.009, val_acc:0.983]
Epoch [101/120    avg_loss:0.010, val_acc:0.983]
Epoch [102/120    avg_loss:0.010, val_acc:0.983]
Epoch [103/120    avg_loss:0.010, val_acc:0.983]
Epoch [104/120    avg_loss:0.006, val_acc:0.983]
Epoch [105/120    avg_loss:0.013, val_acc:0.983]
Epoch [106/120    avg_loss:0.010, val_acc:0.983]
Epoch [107/120    avg_loss:0.013, val_acc:0.983]
Epoch [108/120    avg_loss:0.010, val_acc:0.983]
Epoch [109/120    avg_loss:0.014, val_acc:0.983]
Epoch [110/120    avg_loss:0.008, val_acc:0.983]
Epoch [111/120    avg_loss:0.009, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.983]
Epoch [113/120    avg_loss:0.011, val_acc:0.983]
Epoch [114/120    avg_loss:0.015, val_acc:0.983]
Epoch [115/120    avg_loss:0.012, val_acc:0.983]
Epoch [116/120    avg_loss:0.011, val_acc:0.983]
Epoch [117/120    avg_loss:0.013, val_acc:0.983]
Epoch [118/120    avg_loss:0.007, val_acc:0.983]
Epoch [119/120    avg_loss:0.010, val_acc:0.983]
Epoch [120/120    avg_loss:0.017, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1270    2    0    0    0    0    0    0    7    1    1    0
     0    4    0]
 [   0    0    0  696    0   27    0    0    0    1    6    4    9    4
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    4    0    0   11    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    9    0    0    0    0  852    0    4    0
     0    3    0]
 [   0    0   12    0    0    1   12    0    0    0    9 2175    0    1
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    3    0  523    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   25    0    0    2    0    1    0    0    0
  1111    0    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    30  303    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.63685636856368

F1 scores:
[       nan 0.975      0.98640777 0.96       0.99528302 0.93147752
 0.97767857 1.         0.99767981 0.73333333 0.97149373 0.99088838
 0.97665733 0.98666667 0.9745614  0.92237443 0.96551724]

Kappa:
0.9730756838971089
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5e55fa1668>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.244, val_acc:0.375]
Epoch [2/120    avg_loss:1.727, val_acc:0.538]
Epoch [3/120    avg_loss:1.379, val_acc:0.583]
Epoch [4/120    avg_loss:1.177, val_acc:0.748]
Epoch [5/120    avg_loss:1.003, val_acc:0.715]
Epoch [6/120    avg_loss:0.980, val_acc:0.786]
Epoch [7/120    avg_loss:0.787, val_acc:0.780]
Epoch [8/120    avg_loss:0.689, val_acc:0.792]
Epoch [9/120    avg_loss:0.627, val_acc:0.810]
Epoch [10/120    avg_loss:0.622, val_acc:0.823]
Epoch [11/120    avg_loss:0.632, val_acc:0.798]
Epoch [12/120    avg_loss:0.502, val_acc:0.822]
Epoch [13/120    avg_loss:0.410, val_acc:0.854]
Epoch [14/120    avg_loss:0.346, val_acc:0.898]
Epoch [15/120    avg_loss:0.410, val_acc:0.883]
Epoch [16/120    avg_loss:0.344, val_acc:0.867]
Epoch [17/120    avg_loss:0.279, val_acc:0.865]
Epoch [18/120    avg_loss:0.408, val_acc:0.876]
Epoch [19/120    avg_loss:0.439, val_acc:0.903]
Epoch [20/120    avg_loss:0.219, val_acc:0.868]
Epoch [21/120    avg_loss:0.216, val_acc:0.907]
Epoch [22/120    avg_loss:0.320, val_acc:0.907]
Epoch [23/120    avg_loss:0.219, val_acc:0.929]
Epoch [24/120    avg_loss:0.232, val_acc:0.927]
Epoch [25/120    avg_loss:0.158, val_acc:0.945]
Epoch [26/120    avg_loss:0.143, val_acc:0.919]
Epoch [27/120    avg_loss:0.137, val_acc:0.951]
Epoch [28/120    avg_loss:0.096, val_acc:0.945]
Epoch [29/120    avg_loss:0.091, val_acc:0.948]
Epoch [30/120    avg_loss:0.115, val_acc:0.938]
Epoch [31/120    avg_loss:0.096, val_acc:0.957]
Epoch [32/120    avg_loss:0.126, val_acc:0.946]
Epoch [33/120    avg_loss:0.148, val_acc:0.941]
Epoch [34/120    avg_loss:0.121, val_acc:0.951]
Epoch [35/120    avg_loss:0.128, val_acc:0.944]
Epoch [36/120    avg_loss:0.148, val_acc:0.949]
Epoch [37/120    avg_loss:0.101, val_acc:0.965]
Epoch [38/120    avg_loss:0.106, val_acc:0.929]
Epoch [39/120    avg_loss:0.051, val_acc:0.967]
Epoch [40/120    avg_loss:0.078, val_acc:0.951]
Epoch [41/120    avg_loss:0.056, val_acc:0.942]
Epoch [42/120    avg_loss:0.046, val_acc:0.963]
Epoch [43/120    avg_loss:0.046, val_acc:0.954]
Epoch [44/120    avg_loss:0.077, val_acc:0.952]
Epoch [45/120    avg_loss:0.078, val_acc:0.952]
Epoch [46/120    avg_loss:0.080, val_acc:0.954]
Epoch [47/120    avg_loss:0.064, val_acc:0.942]
Epoch [48/120    avg_loss:0.100, val_acc:0.947]
Epoch [49/120    avg_loss:0.110, val_acc:0.943]
Epoch [50/120    avg_loss:0.091, val_acc:0.963]
Epoch [51/120    avg_loss:0.073, val_acc:0.954]
Epoch [52/120    avg_loss:0.052, val_acc:0.969]
Epoch [53/120    avg_loss:0.065, val_acc:0.959]
Epoch [54/120    avg_loss:0.043, val_acc:0.978]
Epoch [55/120    avg_loss:0.077, val_acc:0.968]
Epoch [56/120    avg_loss:0.049, val_acc:0.977]
Epoch [57/120    avg_loss:0.017, val_acc:0.979]
Epoch [58/120    avg_loss:0.015, val_acc:0.971]
Epoch [59/120    avg_loss:0.027, val_acc:0.970]
Epoch [60/120    avg_loss:0.030, val_acc:0.970]
Epoch [61/120    avg_loss:0.023, val_acc:0.975]
Epoch [62/120    avg_loss:0.044, val_acc:0.959]
Epoch [63/120    avg_loss:0.026, val_acc:0.964]
Epoch [64/120    avg_loss:0.023, val_acc:0.977]
Epoch [65/120    avg_loss:0.037, val_acc:0.978]
Epoch [66/120    avg_loss:0.023, val_acc:0.966]
Epoch [67/120    avg_loss:0.028, val_acc:0.972]
Epoch [68/120    avg_loss:0.016, val_acc:0.967]
Epoch [69/120    avg_loss:0.023, val_acc:0.978]
Epoch [70/120    avg_loss:0.014, val_acc:0.966]
Epoch [71/120    avg_loss:0.018, val_acc:0.974]
Epoch [72/120    avg_loss:0.012, val_acc:0.974]
Epoch [73/120    avg_loss:0.013, val_acc:0.976]
Epoch [74/120    avg_loss:0.011, val_acc:0.972]
Epoch [75/120    avg_loss:0.015, val_acc:0.974]
Epoch [76/120    avg_loss:0.009, val_acc:0.974]
Epoch [77/120    avg_loss:0.009, val_acc:0.975]
Epoch [78/120    avg_loss:0.021, val_acc:0.974]
Epoch [79/120    avg_loss:0.006, val_acc:0.974]
Epoch [80/120    avg_loss:0.007, val_acc:0.977]
Epoch [81/120    avg_loss:0.006, val_acc:0.976]
Epoch [82/120    avg_loss:0.005, val_acc:0.977]
Epoch [83/120    avg_loss:0.013, val_acc:0.978]
Epoch [84/120    avg_loss:0.008, val_acc:0.978]
Epoch [85/120    avg_loss:0.005, val_acc:0.978]
Epoch [86/120    avg_loss:0.006, val_acc:0.978]
Epoch [87/120    avg_loss:0.007, val_acc:0.978]
Epoch [88/120    avg_loss:0.005, val_acc:0.978]
Epoch [89/120    avg_loss:0.005, val_acc:0.977]
Epoch [90/120    avg_loss:0.009, val_acc:0.978]
Epoch [91/120    avg_loss:0.008, val_acc:0.978]
Epoch [92/120    avg_loss:0.010, val_acc:0.978]
Epoch [93/120    avg_loss:0.006, val_acc:0.978]
Epoch [94/120    avg_loss:0.005, val_acc:0.978]
Epoch [95/120    avg_loss:0.010, val_acc:0.978]
Epoch [96/120    avg_loss:0.012, val_acc:0.978]
Epoch [97/120    avg_loss:0.006, val_acc:0.978]
Epoch [98/120    avg_loss:0.005, val_acc:0.978]
Epoch [99/120    avg_loss:0.007, val_acc:0.978]
Epoch [100/120    avg_loss:0.006, val_acc:0.978]
Epoch [101/120    avg_loss:0.005, val_acc:0.978]
Epoch [102/120    avg_loss:0.007, val_acc:0.978]
Epoch [103/120    avg_loss:0.005, val_acc:0.978]
Epoch [104/120    avg_loss:0.004, val_acc:0.978]
Epoch [105/120    avg_loss:0.005, val_acc:0.978]
Epoch [106/120    avg_loss:0.008, val_acc:0.978]
Epoch [107/120    avg_loss:0.008, val_acc:0.978]
Epoch [108/120    avg_loss:0.007, val_acc:0.978]
Epoch [109/120    avg_loss:0.009, val_acc:0.978]
Epoch [110/120    avg_loss:0.010, val_acc:0.978]
Epoch [111/120    avg_loss:0.010, val_acc:0.978]
Epoch [112/120    avg_loss:0.005, val_acc:0.978]
Epoch [113/120    avg_loss:0.009, val_acc:0.978]
Epoch [114/120    avg_loss:0.007, val_acc:0.978]
Epoch [115/120    avg_loss:0.006, val_acc:0.978]
Epoch [116/120    avg_loss:0.014, val_acc:0.978]
Epoch [117/120    avg_loss:0.009, val_acc:0.978]
Epoch [118/120    avg_loss:0.009, val_acc:0.978]
Epoch [119/120    avg_loss:0.006, val_acc:0.978]
Epoch [120/120    avg_loss:0.011, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1274    2    0    0    0    0    0    0    3    2    1    0
     0    3    0]
 [   0    0    0  699   13    9    0    0    0    4    0    0   20    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    3    0    0   12    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    1    0    0    0    0  853    3    4    0
     1    5    0]
 [   0    0   11    0    0    0    3    0    0    0    1 2195    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  528    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1134    4    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    17  316    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.38482384823848

F1 scores:
[       nan 1.         0.98836307 0.96347347 0.97038724 0.98517674
 0.98424606 1.         0.99883856 0.64864865 0.98498845 0.99523917
 0.97058824 0.99462366 0.98996072 0.9362963  0.95953757]

Kappa:
0.9815902242480181
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb145d006a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.162, val_acc:0.521]
Epoch [2/120    avg_loss:1.648, val_acc:0.578]
Epoch [3/120    avg_loss:1.337, val_acc:0.512]
Epoch [4/120    avg_loss:1.305, val_acc:0.708]
Epoch [5/120    avg_loss:1.100, val_acc:0.704]
Epoch [6/120    avg_loss:0.960, val_acc:0.749]
Epoch [7/120    avg_loss:0.865, val_acc:0.719]
Epoch [8/120    avg_loss:0.702, val_acc:0.821]
Epoch [9/120    avg_loss:0.587, val_acc:0.754]
Epoch [10/120    avg_loss:0.697, val_acc:0.829]
Epoch [11/120    avg_loss:0.524, val_acc:0.862]
Epoch [12/120    avg_loss:0.447, val_acc:0.813]
Epoch [13/120    avg_loss:0.383, val_acc:0.817]
Epoch [14/120    avg_loss:0.367, val_acc:0.861]
Epoch [15/120    avg_loss:0.423, val_acc:0.645]
Epoch [16/120    avg_loss:0.325, val_acc:0.860]
Epoch [17/120    avg_loss:0.300, val_acc:0.899]
Epoch [18/120    avg_loss:0.219, val_acc:0.912]
Epoch [19/120    avg_loss:0.267, val_acc:0.900]
Epoch [20/120    avg_loss:0.226, val_acc:0.902]
Epoch [21/120    avg_loss:0.226, val_acc:0.921]
Epoch [22/120    avg_loss:0.168, val_acc:0.892]
Epoch [23/120    avg_loss:0.161, val_acc:0.920]
Epoch [24/120    avg_loss:0.142, val_acc:0.923]
Epoch [25/120    avg_loss:0.300, val_acc:0.894]
Epoch [26/120    avg_loss:0.219, val_acc:0.897]
Epoch [27/120    avg_loss:0.144, val_acc:0.919]
Epoch [28/120    avg_loss:0.166, val_acc:0.926]
Epoch [29/120    avg_loss:0.118, val_acc:0.946]
Epoch [30/120    avg_loss:0.068, val_acc:0.952]
Epoch [31/120    avg_loss:0.059, val_acc:0.944]
Epoch [32/120    avg_loss:0.101, val_acc:0.952]
Epoch [33/120    avg_loss:0.079, val_acc:0.916]
Epoch [34/120    avg_loss:0.112, val_acc:0.962]
Epoch [35/120    avg_loss:0.052, val_acc:0.965]
Epoch [36/120    avg_loss:0.058, val_acc:0.958]
Epoch [37/120    avg_loss:0.070, val_acc:0.963]
Epoch [38/120    avg_loss:0.044, val_acc:0.955]
Epoch [39/120    avg_loss:0.037, val_acc:0.954]
Epoch [40/120    avg_loss:0.083, val_acc:0.956]
Epoch [41/120    avg_loss:0.075, val_acc:0.966]
Epoch [42/120    avg_loss:0.034, val_acc:0.951]
Epoch [43/120    avg_loss:0.045, val_acc:0.979]
Epoch [44/120    avg_loss:0.020, val_acc:0.971]
Epoch [45/120    avg_loss:0.039, val_acc:0.973]
Epoch [46/120    avg_loss:0.054, val_acc:0.975]
Epoch [47/120    avg_loss:0.028, val_acc:0.972]
Epoch [48/120    avg_loss:0.034, val_acc:0.965]
Epoch [49/120    avg_loss:0.086, val_acc:0.934]
Epoch [50/120    avg_loss:0.103, val_acc:0.953]
Epoch [51/120    avg_loss:0.059, val_acc:0.969]
Epoch [52/120    avg_loss:0.067, val_acc:0.957]
Epoch [53/120    avg_loss:0.038, val_acc:0.972]
Epoch [54/120    avg_loss:0.034, val_acc:0.962]
Epoch [55/120    avg_loss:0.034, val_acc:0.971]
Epoch [56/120    avg_loss:0.017, val_acc:0.974]
Epoch [57/120    avg_loss:0.015, val_acc:0.973]
Epoch [58/120    avg_loss:0.018, val_acc:0.975]
Epoch [59/120    avg_loss:0.009, val_acc:0.976]
Epoch [60/120    avg_loss:0.011, val_acc:0.981]
Epoch [61/120    avg_loss:0.011, val_acc:0.979]
Epoch [62/120    avg_loss:0.012, val_acc:0.981]
Epoch [63/120    avg_loss:0.012, val_acc:0.980]
Epoch [64/120    avg_loss:0.014, val_acc:0.981]
Epoch [65/120    avg_loss:0.011, val_acc:0.981]
Epoch [66/120    avg_loss:0.009, val_acc:0.982]
Epoch [67/120    avg_loss:0.008, val_acc:0.981]
Epoch [68/120    avg_loss:0.007, val_acc:0.981]
Epoch [69/120    avg_loss:0.011, val_acc:0.980]
Epoch [70/120    avg_loss:0.007, val_acc:0.982]
Epoch [71/120    avg_loss:0.006, val_acc:0.981]
Epoch [72/120    avg_loss:0.007, val_acc:0.980]
Epoch [73/120    avg_loss:0.013, val_acc:0.980]
Epoch [74/120    avg_loss:0.008, val_acc:0.979]
Epoch [75/120    avg_loss:0.008, val_acc:0.980]
Epoch [76/120    avg_loss:0.011, val_acc:0.981]
Epoch [77/120    avg_loss:0.018, val_acc:0.975]
Epoch [78/120    avg_loss:0.009, val_acc:0.980]
Epoch [79/120    avg_loss:0.006, val_acc:0.980]
Epoch [80/120    avg_loss:0.011, val_acc:0.982]
Epoch [81/120    avg_loss:0.010, val_acc:0.981]
Epoch [82/120    avg_loss:0.010, val_acc:0.981]
Epoch [83/120    avg_loss:0.008, val_acc:0.982]
Epoch [84/120    avg_loss:0.009, val_acc:0.982]
Epoch [85/120    avg_loss:0.010, val_acc:0.983]
Epoch [86/120    avg_loss:0.006, val_acc:0.982]
Epoch [87/120    avg_loss:0.010, val_acc:0.983]
Epoch [88/120    avg_loss:0.007, val_acc:0.983]
Epoch [89/120    avg_loss:0.011, val_acc:0.982]
Epoch [90/120    avg_loss:0.008, val_acc:0.983]
Epoch [91/120    avg_loss:0.009, val_acc:0.984]
Epoch [92/120    avg_loss:0.010, val_acc:0.983]
Epoch [93/120    avg_loss:0.009, val_acc:0.985]
Epoch [94/120    avg_loss:0.005, val_acc:0.985]
Epoch [95/120    avg_loss:0.005, val_acc:0.984]
Epoch [96/120    avg_loss:0.007, val_acc:0.984]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.007, val_acc:0.984]
Epoch [99/120    avg_loss:0.007, val_acc:0.984]
Epoch [100/120    avg_loss:0.004, val_acc:0.985]
Epoch [101/120    avg_loss:0.007, val_acc:0.985]
Epoch [102/120    avg_loss:0.004, val_acc:0.985]
Epoch [103/120    avg_loss:0.005, val_acc:0.985]
Epoch [104/120    avg_loss:0.008, val_acc:0.984]
Epoch [105/120    avg_loss:0.006, val_acc:0.985]
Epoch [106/120    avg_loss:0.005, val_acc:0.985]
Epoch [107/120    avg_loss:0.008, val_acc:0.985]
Epoch [108/120    avg_loss:0.008, val_acc:0.985]
Epoch [109/120    avg_loss:0.009, val_acc:0.985]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.008, val_acc:0.985]
Epoch [112/120    avg_loss:0.005, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.986]
Epoch [115/120    avg_loss:0.009, val_acc:0.985]
Epoch [116/120    avg_loss:0.008, val_acc:0.984]
Epoch [117/120    avg_loss:0.005, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.985]
Epoch [119/120    avg_loss:0.006, val_acc:0.985]
Epoch [120/120    avg_loss:0.004, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1266    4    0    0    0    0    0    0    7    4    0    0
     0    4    0]
 [   0    0    0  706    0   17    0    0    0    3    1    0   17    3
     0    0    0]
 [   0    0    0    3  209    0    0    0    0    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    2    0    0   12    0    0    0    0
     0    0    0]
 [   0    0    7    2    0    3    0    0    0    0  852    7    0    0
     1    3    0]
 [   0    0   24    0    0    4    2    0    0    0    2 2170    6    1
     0    1    0]
 [   0    0    0    0    0    1    0    0    0    0    3    0  527    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    2    0    0    0   13    0    0    1    0    2    0    0    0
  1121    0    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    10  329    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.07046070460704

F1 scores:
[       nan 0.96385542 0.98063517 0.96316508 0.99052133 0.9570011
 0.99095023 1.         0.99883856 0.70588235 0.97762478 0.98838533
 0.97142857 0.98930481 0.9872303  0.9619883  0.98245614]

Kappa:
0.9780172624298019
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:117
Validation dataloader:117
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f51851ba668>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.351, val_acc:0.455]
Epoch [2/120    avg_loss:1.656, val_acc:0.533]
Epoch [3/120    avg_loss:1.548, val_acc:0.668]
Epoch [4/120    avg_loss:1.250, val_acc:0.693]
Epoch [5/120    avg_loss:1.122, val_acc:0.668]
Epoch [6/120    avg_loss:0.949, val_acc:0.662]
Epoch [7/120    avg_loss:0.941, val_acc:0.711]
Epoch [8/120    avg_loss:0.819, val_acc:0.800]
Epoch [9/120    avg_loss:0.717, val_acc:0.751]
Epoch [10/120    avg_loss:0.684, val_acc:0.758]
Epoch [11/120    avg_loss:0.650, val_acc:0.810]
Epoch [12/120    avg_loss:0.589, val_acc:0.750]
Epoch [13/120    avg_loss:0.699, val_acc:0.862]
Epoch [14/120    avg_loss:0.551, val_acc:0.852]
Epoch [15/120    avg_loss:0.358, val_acc:0.868]
Epoch [16/120    avg_loss:0.546, val_acc:0.851]
Epoch [17/120    avg_loss:0.340, val_acc:0.872]
Epoch [18/120    avg_loss:0.324, val_acc:0.872]
Epoch [19/120    avg_loss:0.311, val_acc:0.871]
Epoch [20/120    avg_loss:0.250, val_acc:0.913]
Epoch [21/120    avg_loss:0.226, val_acc:0.902]
Epoch [22/120    avg_loss:0.212, val_acc:0.913]
Epoch [23/120    avg_loss:0.202, val_acc:0.902]
Epoch [24/120    avg_loss:0.195, val_acc:0.922]
Epoch [25/120    avg_loss:0.240, val_acc:0.897]
Epoch [26/120    avg_loss:0.204, val_acc:0.917]
Epoch [27/120    avg_loss:0.145, val_acc:0.921]
Epoch [28/120    avg_loss:0.140, val_acc:0.923]
Epoch [29/120    avg_loss:0.108, val_acc:0.945]
Epoch [30/120    avg_loss:0.095, val_acc:0.949]
Epoch [31/120    avg_loss:0.086, val_acc:0.921]
Epoch [32/120    avg_loss:0.114, val_acc:0.880]
Epoch [33/120    avg_loss:0.105, val_acc:0.957]
Epoch [34/120    avg_loss:0.076, val_acc:0.932]
Epoch [35/120    avg_loss:0.106, val_acc:0.932]
Epoch [36/120    avg_loss:0.053, val_acc:0.960]
Epoch [37/120    avg_loss:0.054, val_acc:0.964]
Epoch [38/120    avg_loss:0.049, val_acc:0.962]
Epoch [39/120    avg_loss:0.040, val_acc:0.961]
Epoch [40/120    avg_loss:0.063, val_acc:0.962]
Epoch [41/120    avg_loss:0.045, val_acc:0.960]
Epoch [42/120    avg_loss:0.024, val_acc:0.968]
Epoch [43/120    avg_loss:0.024, val_acc:0.975]
Epoch [44/120    avg_loss:0.085, val_acc:0.918]
Epoch [45/120    avg_loss:0.066, val_acc:0.962]
Epoch [46/120    avg_loss:0.044, val_acc:0.978]
Epoch [47/120    avg_loss:0.037, val_acc:0.955]
Epoch [48/120    avg_loss:0.072, val_acc:0.971]
Epoch [49/120    avg_loss:0.075, val_acc:0.955]
Epoch [50/120    avg_loss:0.106, val_acc:0.949]
Epoch [51/120    avg_loss:0.044, val_acc:0.954]
Epoch [52/120    avg_loss:0.029, val_acc:0.968]
Epoch [53/120    avg_loss:0.020, val_acc:0.962]
Epoch [54/120    avg_loss:0.021, val_acc:0.982]
Epoch [55/120    avg_loss:0.021, val_acc:0.973]
Epoch [56/120    avg_loss:0.020, val_acc:0.980]
Epoch [57/120    avg_loss:0.044, val_acc:0.977]
Epoch [58/120    avg_loss:0.142, val_acc:0.880]
Epoch [59/120    avg_loss:0.108, val_acc:0.963]
Epoch [60/120    avg_loss:0.064, val_acc:0.958]
Epoch [61/120    avg_loss:0.064, val_acc:0.976]
Epoch [62/120    avg_loss:0.020, val_acc:0.966]
Epoch [63/120    avg_loss:0.015, val_acc:0.977]
Epoch [64/120    avg_loss:0.013, val_acc:0.975]
Epoch [65/120    avg_loss:0.026, val_acc:0.959]
Epoch [66/120    avg_loss:0.032, val_acc:0.972]
Epoch [67/120    avg_loss:0.014, val_acc:0.977]
Epoch [68/120    avg_loss:0.009, val_acc:0.980]
Epoch [69/120    avg_loss:0.010, val_acc:0.982]
Epoch [70/120    avg_loss:0.009, val_acc:0.978]
Epoch [71/120    avg_loss:0.008, val_acc:0.980]
Epoch [72/120    avg_loss:0.017, val_acc:0.984]
Epoch [73/120    avg_loss:0.007, val_acc:0.984]
Epoch [74/120    avg_loss:0.018, val_acc:0.984]
Epoch [75/120    avg_loss:0.014, val_acc:0.984]
Epoch [76/120    avg_loss:0.006, val_acc:0.984]
Epoch [77/120    avg_loss:0.009, val_acc:0.980]
Epoch [78/120    avg_loss:0.009, val_acc:0.982]
Epoch [79/120    avg_loss:0.005, val_acc:0.982]
Epoch [80/120    avg_loss:0.009, val_acc:0.979]
Epoch [81/120    avg_loss:0.005, val_acc:0.980]
Epoch [82/120    avg_loss:0.006, val_acc:0.979]
Epoch [83/120    avg_loss:0.005, val_acc:0.979]
Epoch [84/120    avg_loss:0.007, val_acc:0.979]
Epoch [85/120    avg_loss:0.008, val_acc:0.978]
Epoch [86/120    avg_loss:0.008, val_acc:0.979]
Epoch [87/120    avg_loss:0.008, val_acc:0.980]
Epoch [88/120    avg_loss:0.009, val_acc:0.979]
Epoch [89/120    avg_loss:0.010, val_acc:0.979]
Epoch [90/120    avg_loss:0.005, val_acc:0.979]
Epoch [91/120    avg_loss:0.006, val_acc:0.979]
Epoch [92/120    avg_loss:0.005, val_acc:0.979]
Epoch [93/120    avg_loss:0.006, val_acc:0.979]
Epoch [94/120    avg_loss:0.011, val_acc:0.980]
Epoch [95/120    avg_loss:0.006, val_acc:0.980]
Epoch [96/120    avg_loss:0.005, val_acc:0.982]
Epoch [97/120    avg_loss:0.005, val_acc:0.982]
Epoch [98/120    avg_loss:0.007, val_acc:0.982]
Epoch [99/120    avg_loss:0.007, val_acc:0.980]
Epoch [100/120    avg_loss:0.010, val_acc:0.982]
Epoch [101/120    avg_loss:0.011, val_acc:0.979]
Epoch [102/120    avg_loss:0.006, val_acc:0.979]
Epoch [103/120    avg_loss:0.008, val_acc:0.979]
Epoch [104/120    avg_loss:0.007, val_acc:0.979]
Epoch [105/120    avg_loss:0.005, val_acc:0.979]
Epoch [106/120    avg_loss:0.005, val_acc:0.979]
Epoch [107/120    avg_loss:0.004, val_acc:0.979]
Epoch [108/120    avg_loss:0.005, val_acc:0.979]
Epoch [109/120    avg_loss:0.008, val_acc:0.979]
Epoch [110/120    avg_loss:0.010, val_acc:0.980]
Epoch [111/120    avg_loss:0.008, val_acc:0.980]
Epoch [112/120    avg_loss:0.006, val_acc:0.980]
Epoch [113/120    avg_loss:0.008, val_acc:0.980]
Epoch [114/120    avg_loss:0.016, val_acc:0.980]
Epoch [115/120    avg_loss:0.005, val_acc:0.980]
Epoch [116/120    avg_loss:0.008, val_acc:0.980]
Epoch [117/120    avg_loss:0.014, val_acc:0.980]
Epoch [118/120    avg_loss:0.006, val_acc:0.980]
Epoch [119/120    avg_loss:0.007, val_acc:0.980]
Epoch [120/120    avg_loss:0.004, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1262    1    0    0    0    0    0    0   12    5    1    0
     0    4    0]
 [   0    0    0  684    1    4    0    0    0    3    8    0   43    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    4    0    0    0    0  854    3    5    0
     0    6    0]
 [   0    0   11    0    0    2    6    0    0    0    0 2185    1    1
     0    4    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0  527    0
     1    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1135    2    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    12  316    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.01626016260163

F1 scores:
[       nan 0.98765432 0.98555252 0.95464061 0.99765808 0.98285714
 0.97980553 1.         0.99883856 0.85       0.97488584 0.99205448
 0.94869487 0.98666667 0.99126638 0.93078056 0.97109827]

Kappa:
0.9773950125926721
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd9321895c0>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.181, val_acc:0.459]
Epoch [2/120    avg_loss:1.675, val_acc:0.621]
Epoch [3/120    avg_loss:1.490, val_acc:0.655]
Epoch [4/120    avg_loss:1.269, val_acc:0.679]
Epoch [5/120    avg_loss:1.164, val_acc:0.695]
Epoch [6/120    avg_loss:1.039, val_acc:0.704]
Epoch [7/120    avg_loss:0.853, val_acc:0.711]
Epoch [8/120    avg_loss:0.876, val_acc:0.748]
Epoch [9/120    avg_loss:0.599, val_acc:0.829]
Epoch [10/120    avg_loss:0.706, val_acc:0.787]
Epoch [11/120    avg_loss:0.720, val_acc:0.812]
Epoch [12/120    avg_loss:0.518, val_acc:0.758]
Epoch [13/120    avg_loss:0.550, val_acc:0.796]
Epoch [14/120    avg_loss:0.462, val_acc:0.826]
Epoch [15/120    avg_loss:0.473, val_acc:0.811]
Epoch [16/120    avg_loss:0.400, val_acc:0.842]
Epoch [17/120    avg_loss:0.380, val_acc:0.869]
Epoch [18/120    avg_loss:0.279, val_acc:0.875]
Epoch [19/120    avg_loss:0.241, val_acc:0.855]
Epoch [20/120    avg_loss:0.399, val_acc:0.850]
Epoch [21/120    avg_loss:0.335, val_acc:0.860]
Epoch [22/120    avg_loss:0.396, val_acc:0.855]
Epoch [23/120    avg_loss:0.319, val_acc:0.882]
Epoch [24/120    avg_loss:0.265, val_acc:0.912]
Epoch [25/120    avg_loss:0.190, val_acc:0.912]
Epoch [26/120    avg_loss:0.164, val_acc:0.884]
Epoch [27/120    avg_loss:0.280, val_acc:0.908]
Epoch [28/120    avg_loss:0.141, val_acc:0.890]
Epoch [29/120    avg_loss:0.136, val_acc:0.927]
Epoch [30/120    avg_loss:0.128, val_acc:0.925]
Epoch [31/120    avg_loss:0.157, val_acc:0.920]
Epoch [32/120    avg_loss:0.186, val_acc:0.924]
Epoch [33/120    avg_loss:0.116, val_acc:0.916]
Epoch [34/120    avg_loss:0.109, val_acc:0.930]
Epoch [35/120    avg_loss:0.101, val_acc:0.892]
Epoch [36/120    avg_loss:0.125, val_acc:0.931]
Epoch [37/120    avg_loss:0.104, val_acc:0.935]
Epoch [38/120    avg_loss:0.103, val_acc:0.932]
Epoch [39/120    avg_loss:0.071, val_acc:0.921]
Epoch [40/120    avg_loss:0.073, val_acc:0.940]
Epoch [41/120    avg_loss:0.141, val_acc:0.941]
Epoch [42/120    avg_loss:0.109, val_acc:0.931]
Epoch [43/120    avg_loss:0.073, val_acc:0.935]
Epoch [44/120    avg_loss:0.041, val_acc:0.951]
Epoch [45/120    avg_loss:0.098, val_acc:0.926]
Epoch [46/120    avg_loss:0.071, val_acc:0.929]
Epoch [47/120    avg_loss:0.054, val_acc:0.945]
Epoch [48/120    avg_loss:0.068, val_acc:0.910]
Epoch [49/120    avg_loss:0.063, val_acc:0.938]
Epoch [50/120    avg_loss:0.079, val_acc:0.932]
Epoch [51/120    avg_loss:0.039, val_acc:0.952]
Epoch [52/120    avg_loss:0.041, val_acc:0.952]
Epoch [53/120    avg_loss:0.060, val_acc:0.953]
Epoch [54/120    avg_loss:0.039, val_acc:0.964]
Epoch [55/120    avg_loss:0.041, val_acc:0.959]
Epoch [56/120    avg_loss:0.028, val_acc:0.969]
Epoch [57/120    avg_loss:0.038, val_acc:0.962]
Epoch [58/120    avg_loss:0.043, val_acc:0.935]
Epoch [59/120    avg_loss:0.027, val_acc:0.961]
Epoch [60/120    avg_loss:0.034, val_acc:0.964]
Epoch [61/120    avg_loss:0.029, val_acc:0.962]
Epoch [62/120    avg_loss:0.051, val_acc:0.946]
Epoch [63/120    avg_loss:0.045, val_acc:0.964]
Epoch [64/120    avg_loss:0.028, val_acc:0.959]
Epoch [65/120    avg_loss:0.065, val_acc:0.934]
Epoch [66/120    avg_loss:0.072, val_acc:0.958]
Epoch [67/120    avg_loss:0.029, val_acc:0.953]
Epoch [68/120    avg_loss:0.029, val_acc:0.967]
Epoch [69/120    avg_loss:0.023, val_acc:0.959]
Epoch [70/120    avg_loss:0.032, val_acc:0.967]
Epoch [71/120    avg_loss:0.035, val_acc:0.973]
Epoch [72/120    avg_loss:0.024, val_acc:0.974]
Epoch [73/120    avg_loss:0.014, val_acc:0.977]
Epoch [74/120    avg_loss:0.011, val_acc:0.977]
Epoch [75/120    avg_loss:0.011, val_acc:0.975]
Epoch [76/120    avg_loss:0.012, val_acc:0.974]
Epoch [77/120    avg_loss:0.011, val_acc:0.978]
Epoch [78/120    avg_loss:0.009, val_acc:0.979]
Epoch [79/120    avg_loss:0.009, val_acc:0.979]
Epoch [80/120    avg_loss:0.011, val_acc:0.978]
Epoch [81/120    avg_loss:0.008, val_acc:0.975]
Epoch [82/120    avg_loss:0.012, val_acc:0.974]
Epoch [83/120    avg_loss:0.007, val_acc:0.974]
Epoch [84/120    avg_loss:0.008, val_acc:0.978]
Epoch [85/120    avg_loss:0.014, val_acc:0.979]
Epoch [86/120    avg_loss:0.012, val_acc:0.980]
Epoch [87/120    avg_loss:0.012, val_acc:0.978]
Epoch [88/120    avg_loss:0.011, val_acc:0.979]
Epoch [89/120    avg_loss:0.007, val_acc:0.979]
Epoch [90/120    avg_loss:0.012, val_acc:0.977]
Epoch [91/120    avg_loss:0.009, val_acc:0.979]
Epoch [92/120    avg_loss:0.006, val_acc:0.981]
Epoch [93/120    avg_loss:0.007, val_acc:0.979]
Epoch [94/120    avg_loss:0.012, val_acc:0.979]
Epoch [95/120    avg_loss:0.011, val_acc:0.978]
Epoch [96/120    avg_loss:0.007, val_acc:0.979]
Epoch [97/120    avg_loss:0.008, val_acc:0.981]
Epoch [98/120    avg_loss:0.011, val_acc:0.980]
Epoch [99/120    avg_loss:0.006, val_acc:0.980]
Epoch [100/120    avg_loss:0.008, val_acc:0.981]
Epoch [101/120    avg_loss:0.007, val_acc:0.981]
Epoch [102/120    avg_loss:0.010, val_acc:0.983]
Epoch [103/120    avg_loss:0.006, val_acc:0.982]
Epoch [104/120    avg_loss:0.006, val_acc:0.982]
Epoch [105/120    avg_loss:0.009, val_acc:0.982]
Epoch [106/120    avg_loss:0.011, val_acc:0.981]
Epoch [107/120    avg_loss:0.006, val_acc:0.981]
Epoch [108/120    avg_loss:0.007, val_acc:0.981]
Epoch [109/120    avg_loss:0.010, val_acc:0.981]
Epoch [110/120    avg_loss:0.008, val_acc:0.982]
Epoch [111/120    avg_loss:0.006, val_acc:0.983]
Epoch [112/120    avg_loss:0.006, val_acc:0.983]
Epoch [113/120    avg_loss:0.006, val_acc:0.983]
Epoch [114/120    avg_loss:0.006, val_acc:0.983]
Epoch [115/120    avg_loss:0.008, val_acc:0.983]
Epoch [116/120    avg_loss:0.010, val_acc:0.980]
Epoch [117/120    avg_loss:0.007, val_acc:0.980]
Epoch [118/120    avg_loss:0.005, val_acc:0.982]
Epoch [119/120    avg_loss:0.008, val_acc:0.982]
Epoch [120/120    avg_loss:0.007, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1274    0    0    1    0    0    0    0    8    1    1    0
     0    0    0]
 [   0    0    0  685    0   22    0    0    0    3    1    0   35    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0  860   13    0    0
     0    1    0]
 [   0    0   12    0    0    0    9    0    0    0    2 2187    0    0
     0    0    0]
 [   0    0    0    0   10    2    0    0    0    0    5    7  506    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    2    0    0    0
  1129    0    0]
 [   0    0    0    0    0    0   26    0    0    0    0    0    0    0
    22  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.81029810298104

F1 scores:
[       nan 1.         0.99105406 0.95470383 0.97706422 0.96230599
 0.97405486 1.         0.99883856 0.81081081 0.98117513 0.99004074
 0.93964717 0.99730458 0.9860262  0.92426584 0.97076023]

Kappa:
0.9750362422935476
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdaa2bcb5f8>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.185, val_acc:0.491]
Epoch [2/120    avg_loss:1.819, val_acc:0.580]
Epoch [3/120    avg_loss:1.581, val_acc:0.599]
Epoch [4/120    avg_loss:1.315, val_acc:0.689]
Epoch [5/120    avg_loss:1.014, val_acc:0.656]
Epoch [6/120    avg_loss:0.891, val_acc:0.689]
Epoch [7/120    avg_loss:1.117, val_acc:0.713]
Epoch [8/120    avg_loss:0.877, val_acc:0.761]
Epoch [9/120    avg_loss:0.787, val_acc:0.809]
Epoch [10/120    avg_loss:0.667, val_acc:0.845]
Epoch [11/120    avg_loss:0.608, val_acc:0.799]
Epoch [12/120    avg_loss:0.580, val_acc:0.785]
Epoch [13/120    avg_loss:0.510, val_acc:0.815]
Epoch [14/120    avg_loss:0.448, val_acc:0.821]
Epoch [15/120    avg_loss:0.466, val_acc:0.774]
Epoch [16/120    avg_loss:0.496, val_acc:0.855]
Epoch [17/120    avg_loss:0.346, val_acc:0.825]
Epoch [18/120    avg_loss:0.309, val_acc:0.854]
Epoch [19/120    avg_loss:0.266, val_acc:0.884]
Epoch [20/120    avg_loss:0.236, val_acc:0.905]
Epoch [21/120    avg_loss:0.267, val_acc:0.912]
Epoch [22/120    avg_loss:0.199, val_acc:0.928]
Epoch [23/120    avg_loss:0.182, val_acc:0.935]
Epoch [24/120    avg_loss:0.146, val_acc:0.931]
Epoch [25/120    avg_loss:0.196, val_acc:0.934]
Epoch [26/120    avg_loss:0.201, val_acc:0.936]
Epoch [27/120    avg_loss:0.140, val_acc:0.935]
Epoch [28/120    avg_loss:0.140, val_acc:0.942]
Epoch [29/120    avg_loss:0.099, val_acc:0.924]
Epoch [30/120    avg_loss:0.173, val_acc:0.941]
Epoch [31/120    avg_loss:0.191, val_acc:0.894]
Epoch [32/120    avg_loss:0.173, val_acc:0.874]
Epoch [33/120    avg_loss:0.151, val_acc:0.952]
Epoch [34/120    avg_loss:0.141, val_acc:0.941]
Epoch [35/120    avg_loss:0.168, val_acc:0.938]
Epoch [36/120    avg_loss:0.160, val_acc:0.947]
Epoch [37/120    avg_loss:0.095, val_acc:0.909]
Epoch [38/120    avg_loss:0.112, val_acc:0.921]
Epoch [39/120    avg_loss:0.078, val_acc:0.961]
Epoch [40/120    avg_loss:0.125, val_acc:0.907]
Epoch [41/120    avg_loss:0.102, val_acc:0.941]
Epoch [42/120    avg_loss:0.112, val_acc:0.936]
Epoch [43/120    avg_loss:0.085, val_acc:0.939]
Epoch [44/120    avg_loss:0.055, val_acc:0.953]
Epoch [45/120    avg_loss:0.052, val_acc:0.958]
Epoch [46/120    avg_loss:0.072, val_acc:0.958]
Epoch [47/120    avg_loss:0.083, val_acc:0.955]
Epoch [48/120    avg_loss:0.036, val_acc:0.966]
Epoch [49/120    avg_loss:0.038, val_acc:0.975]
Epoch [50/120    avg_loss:0.020, val_acc:0.970]
Epoch [51/120    avg_loss:0.023, val_acc:0.940]
Epoch [52/120    avg_loss:0.029, val_acc:0.977]
Epoch [53/120    avg_loss:0.034, val_acc:0.947]
Epoch [54/120    avg_loss:0.057, val_acc:0.956]
Epoch [55/120    avg_loss:0.041, val_acc:0.968]
Epoch [56/120    avg_loss:0.033, val_acc:0.963]
Epoch [57/120    avg_loss:0.022, val_acc:0.969]
Epoch [58/120    avg_loss:0.060, val_acc:0.955]
Epoch [59/120    avg_loss:0.072, val_acc:0.961]
Epoch [60/120    avg_loss:0.035, val_acc:0.977]
Epoch [61/120    avg_loss:0.033, val_acc:0.971]
Epoch [62/120    avg_loss:0.049, val_acc:0.946]
Epoch [63/120    avg_loss:0.077, val_acc:0.961]
Epoch [64/120    avg_loss:0.045, val_acc:0.967]
Epoch [65/120    avg_loss:0.079, val_acc:0.967]
Epoch [66/120    avg_loss:0.033, val_acc:0.976]
Epoch [67/120    avg_loss:0.031, val_acc:0.950]
Epoch [68/120    avg_loss:0.048, val_acc:0.974]
Epoch [69/120    avg_loss:0.055, val_acc:0.973]
Epoch [70/120    avg_loss:0.030, val_acc:0.974]
Epoch [71/120    avg_loss:0.030, val_acc:0.965]
Epoch [72/120    avg_loss:0.035, val_acc:0.965]
Epoch [73/120    avg_loss:0.039, val_acc:0.954]
Epoch [74/120    avg_loss:0.035, val_acc:0.973]
Epoch [75/120    avg_loss:0.028, val_acc:0.981]
Epoch [76/120    avg_loss:0.023, val_acc:0.982]
Epoch [77/120    avg_loss:0.008, val_acc:0.981]
Epoch [78/120    avg_loss:0.019, val_acc:0.981]
Epoch [79/120    avg_loss:0.013, val_acc:0.981]
Epoch [80/120    avg_loss:0.015, val_acc:0.984]
Epoch [81/120    avg_loss:0.018, val_acc:0.980]
Epoch [82/120    avg_loss:0.010, val_acc:0.980]
Epoch [83/120    avg_loss:0.009, val_acc:0.979]
Epoch [84/120    avg_loss:0.019, val_acc:0.980]
Epoch [85/120    avg_loss:0.009, val_acc:0.980]
Epoch [86/120    avg_loss:0.007, val_acc:0.981]
Epoch [87/120    avg_loss:0.006, val_acc:0.981]
Epoch [88/120    avg_loss:0.009, val_acc:0.982]
Epoch [89/120    avg_loss:0.006, val_acc:0.981]
Epoch [90/120    avg_loss:0.006, val_acc:0.984]
Epoch [91/120    avg_loss:0.007, val_acc:0.984]
Epoch [92/120    avg_loss:0.011, val_acc:0.982]
Epoch [93/120    avg_loss:0.020, val_acc:0.982]
Epoch [94/120    avg_loss:0.011, val_acc:0.981]
Epoch [95/120    avg_loss:0.011, val_acc:0.980]
Epoch [96/120    avg_loss:0.011, val_acc:0.981]
Epoch [97/120    avg_loss:0.011, val_acc:0.981]
Epoch [98/120    avg_loss:0.010, val_acc:0.981]
Epoch [99/120    avg_loss:0.014, val_acc:0.980]
Epoch [100/120    avg_loss:0.009, val_acc:0.979]
Epoch [101/120    avg_loss:0.013, val_acc:0.979]
Epoch [102/120    avg_loss:0.010, val_acc:0.979]
Epoch [103/120    avg_loss:0.006, val_acc:0.979]
Epoch [104/120    avg_loss:0.006, val_acc:0.980]
Epoch [105/120    avg_loss:0.008, val_acc:0.980]
Epoch [106/120    avg_loss:0.009, val_acc:0.981]
Epoch [107/120    avg_loss:0.007, val_acc:0.981]
Epoch [108/120    avg_loss:0.008, val_acc:0.981]
Epoch [109/120    avg_loss:0.014, val_acc:0.981]
Epoch [110/120    avg_loss:0.009, val_acc:0.981]
Epoch [111/120    avg_loss:0.005, val_acc:0.981]
Epoch [112/120    avg_loss:0.006, val_acc:0.981]
Epoch [113/120    avg_loss:0.007, val_acc:0.981]
Epoch [114/120    avg_loss:0.006, val_acc:0.981]
Epoch [115/120    avg_loss:0.008, val_acc:0.981]
Epoch [116/120    avg_loss:0.007, val_acc:0.981]
Epoch [117/120    avg_loss:0.007, val_acc:0.981]
Epoch [118/120    avg_loss:0.012, val_acc:0.981]
Epoch [119/120    avg_loss:0.007, val_acc:0.981]
Epoch [120/120    avg_loss:0.010, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1256    7    0    0    0    0    0    0   16    3    0    0
     0    3    0]
 [   0    0    0  634    0    8    0    0    0    3    0    0  102    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0   13    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0  856   15    2    0
     0    1    0]
 [   0    0    8    0    0    4    7    0    0    0    1 2190    0    0
     0    0    0]
 [   0    0    0   16   14    0    0    0    0    0    4    0  494    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    1    0    0    0
  1130    0    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    34  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.86720867208672

F1 scores:
[       nan 0.98765432 0.9854845  0.89865344 0.96347032 0.97632469
 0.98353293 0.98039216 0.99883856 0.74285714 0.97605473 0.99139882
 0.87202118 1.         0.9813287  0.92       0.95953757]

Kappa:
0.9642913560148691
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f11d6937668>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.241, val_acc:0.343]
Epoch [2/120    avg_loss:1.715, val_acc:0.477]
Epoch [3/120    avg_loss:1.754, val_acc:0.529]
Epoch [4/120    avg_loss:1.357, val_acc:0.624]
Epoch [5/120    avg_loss:1.110, val_acc:0.697]
Epoch [6/120    avg_loss:1.053, val_acc:0.700]
Epoch [7/120    avg_loss:0.886, val_acc:0.777]
Epoch [8/120    avg_loss:0.825, val_acc:0.723]
Epoch [9/120    avg_loss:0.692, val_acc:0.774]
Epoch [10/120    avg_loss:0.668, val_acc:0.797]
Epoch [11/120    avg_loss:0.626, val_acc:0.809]
Epoch [12/120    avg_loss:0.535, val_acc:0.838]
Epoch [13/120    avg_loss:0.539, val_acc:0.804]
Epoch [14/120    avg_loss:0.462, val_acc:0.820]
Epoch [15/120    avg_loss:0.526, val_acc:0.754]
Epoch [16/120    avg_loss:0.434, val_acc:0.876]
Epoch [17/120    avg_loss:0.307, val_acc:0.850]
Epoch [18/120    avg_loss:0.383, val_acc:0.859]
Epoch [19/120    avg_loss:0.398, val_acc:0.806]
Epoch [20/120    avg_loss:0.270, val_acc:0.904]
Epoch [21/120    avg_loss:0.269, val_acc:0.870]
Epoch [22/120    avg_loss:0.240, val_acc:0.902]
Epoch [23/120    avg_loss:0.216, val_acc:0.913]
Epoch [24/120    avg_loss:0.394, val_acc:0.878]
Epoch [25/120    avg_loss:0.163, val_acc:0.901]
Epoch [26/120    avg_loss:0.154, val_acc:0.800]
Epoch [27/120    avg_loss:0.174, val_acc:0.895]
Epoch [28/120    avg_loss:0.195, val_acc:0.889]
Epoch [29/120    avg_loss:0.095, val_acc:0.921]
Epoch [30/120    avg_loss:0.114, val_acc:0.941]
Epoch [31/120    avg_loss:0.110, val_acc:0.935]
Epoch [32/120    avg_loss:0.124, val_acc:0.924]
Epoch [33/120    avg_loss:0.263, val_acc:0.887]
Epoch [34/120    avg_loss:0.101, val_acc:0.932]
Epoch [35/120    avg_loss:0.155, val_acc:0.829]
Epoch [36/120    avg_loss:0.120, val_acc:0.925]
Epoch [37/120    avg_loss:0.104, val_acc:0.938]
Epoch [38/120    avg_loss:0.104, val_acc:0.947]
Epoch [39/120    avg_loss:0.077, val_acc:0.942]
Epoch [40/120    avg_loss:0.033, val_acc:0.950]
Epoch [41/120    avg_loss:0.096, val_acc:0.929]
Epoch [42/120    avg_loss:0.080, val_acc:0.945]
Epoch [43/120    avg_loss:0.098, val_acc:0.929]
Epoch [44/120    avg_loss:0.095, val_acc:0.943]
Epoch [45/120    avg_loss:0.091, val_acc:0.941]
Epoch [46/120    avg_loss:0.060, val_acc:0.957]
Epoch [47/120    avg_loss:0.066, val_acc:0.961]
Epoch [48/120    avg_loss:0.044, val_acc:0.964]
Epoch [49/120    avg_loss:0.046, val_acc:0.943]
Epoch [50/120    avg_loss:0.061, val_acc:0.947]
Epoch [51/120    avg_loss:0.050, val_acc:0.953]
Epoch [52/120    avg_loss:0.070, val_acc:0.943]
Epoch [53/120    avg_loss:0.038, val_acc:0.959]
Epoch [54/120    avg_loss:0.085, val_acc:0.954]
Epoch [55/120    avg_loss:0.034, val_acc:0.963]
Epoch [56/120    avg_loss:0.028, val_acc:0.954]
Epoch [57/120    avg_loss:0.050, val_acc:0.961]
Epoch [58/120    avg_loss:0.047, val_acc:0.958]
Epoch [59/120    avg_loss:0.048, val_acc:0.956]
Epoch [60/120    avg_loss:0.031, val_acc:0.974]
Epoch [61/120    avg_loss:0.036, val_acc:0.967]
Epoch [62/120    avg_loss:0.043, val_acc:0.964]
Epoch [63/120    avg_loss:0.034, val_acc:0.948]
Epoch [64/120    avg_loss:0.052, val_acc:0.963]
Epoch [65/120    avg_loss:0.028, val_acc:0.969]
Epoch [66/120    avg_loss:0.026, val_acc:0.965]
Epoch [67/120    avg_loss:0.013, val_acc:0.968]
Epoch [68/120    avg_loss:0.056, val_acc:0.936]
Epoch [69/120    avg_loss:0.055, val_acc:0.967]
Epoch [70/120    avg_loss:0.021, val_acc:0.963]
Epoch [71/120    avg_loss:0.023, val_acc:0.966]
Epoch [72/120    avg_loss:0.062, val_acc:0.970]
Epoch [73/120    avg_loss:0.025, val_acc:0.967]
Epoch [74/120    avg_loss:0.023, val_acc:0.971]
Epoch [75/120    avg_loss:0.015, val_acc:0.971]
Epoch [76/120    avg_loss:0.014, val_acc:0.974]
Epoch [77/120    avg_loss:0.009, val_acc:0.974]
Epoch [78/120    avg_loss:0.014, val_acc:0.974]
Epoch [79/120    avg_loss:0.011, val_acc:0.975]
Epoch [80/120    avg_loss:0.013, val_acc:0.975]
Epoch [81/120    avg_loss:0.013, val_acc:0.977]
Epoch [82/120    avg_loss:0.018, val_acc:0.975]
Epoch [83/120    avg_loss:0.011, val_acc:0.978]
Epoch [84/120    avg_loss:0.006, val_acc:0.979]
Epoch [85/120    avg_loss:0.010, val_acc:0.979]
Epoch [86/120    avg_loss:0.008, val_acc:0.979]
Epoch [87/120    avg_loss:0.010, val_acc:0.979]
Epoch [88/120    avg_loss:0.009, val_acc:0.978]
Epoch [89/120    avg_loss:0.009, val_acc:0.978]
Epoch [90/120    avg_loss:0.012, val_acc:0.978]
Epoch [91/120    avg_loss:0.008, val_acc:0.977]
Epoch [92/120    avg_loss:0.014, val_acc:0.977]
Epoch [93/120    avg_loss:0.008, val_acc:0.979]
Epoch [94/120    avg_loss:0.008, val_acc:0.979]
Epoch [95/120    avg_loss:0.010, val_acc:0.978]
Epoch [96/120    avg_loss:0.015, val_acc:0.977]
Epoch [97/120    avg_loss:0.008, val_acc:0.978]
Epoch [98/120    avg_loss:0.007, val_acc:0.978]
Epoch [99/120    avg_loss:0.006, val_acc:0.978]
Epoch [100/120    avg_loss:0.009, val_acc:0.979]
Epoch [101/120    avg_loss:0.010, val_acc:0.977]
Epoch [102/120    avg_loss:0.011, val_acc:0.978]
Epoch [103/120    avg_loss:0.007, val_acc:0.978]
Epoch [104/120    avg_loss:0.005, val_acc:0.978]
Epoch [105/120    avg_loss:0.017, val_acc:0.979]
Epoch [106/120    avg_loss:0.005, val_acc:0.978]
Epoch [107/120    avg_loss:0.008, val_acc:0.978]
Epoch [108/120    avg_loss:0.006, val_acc:0.979]
Epoch [109/120    avg_loss:0.008, val_acc:0.978]
Epoch [110/120    avg_loss:0.008, val_acc:0.979]
Epoch [111/120    avg_loss:0.010, val_acc:0.979]
Epoch [112/120    avg_loss:0.016, val_acc:0.977]
Epoch [113/120    avg_loss:0.004, val_acc:0.977]
Epoch [114/120    avg_loss:0.007, val_acc:0.977]
Epoch [115/120    avg_loss:0.004, val_acc:0.978]
Epoch [116/120    avg_loss:0.010, val_acc:0.980]
Epoch [117/120    avg_loss:0.005, val_acc:0.980]
Epoch [118/120    avg_loss:0.006, val_acc:0.981]
Epoch [119/120    avg_loss:0.006, val_acc:0.981]
Epoch [120/120    avg_loss:0.006, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1274    0    0    0    0    0    0    0    7    2    2    0
     0    0    0]
 [   0    0    0  672    0   15    0    0    0    3    0    0   57    0
     0    0    0]
 [   0    0    0    4  209    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  861   11    2    0
     1    0    0]
 [   0    0    5    0    0    1    7    0    0    0    6 2190    0    1
     0    0    0]
 [   0    0    0    0    6    2    0    0    0    0    1    3  516    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   17    0    0    1    0    0    0    0    0
  1121    0    0]
 [   0    0    0    0    0    0   20    0    0    0    0    0    0    0
    35  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.61517615176152

F1 scores:
[       nan 1.         0.99375975 0.94315789 0.97663551 0.96017699
 0.97986577 1.         0.99883856 0.84210526 0.984      0.99184783
 0.92722372 0.99730458 0.97648084 0.91392801 0.95348837]

Kappa:
0.9728140128861177
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe669b2d780>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.199, val_acc:0.508]
Epoch [2/120    avg_loss:1.718, val_acc:0.555]
Epoch [3/120    avg_loss:1.346, val_acc:0.668]
Epoch [4/120    avg_loss:1.179, val_acc:0.709]
Epoch [5/120    avg_loss:1.066, val_acc:0.633]
Epoch [6/120    avg_loss:0.939, val_acc:0.763]
Epoch [7/120    avg_loss:0.798, val_acc:0.802]
Epoch [8/120    avg_loss:0.803, val_acc:0.741]
Epoch [9/120    avg_loss:0.652, val_acc:0.706]
Epoch [10/120    avg_loss:0.626, val_acc:0.798]
Epoch [11/120    avg_loss:0.573, val_acc:0.854]
Epoch [12/120    avg_loss:0.577, val_acc:0.793]
Epoch [13/120    avg_loss:0.556, val_acc:0.849]
Epoch [14/120    avg_loss:0.429, val_acc:0.857]
Epoch [15/120    avg_loss:0.463, val_acc:0.874]
Epoch [16/120    avg_loss:0.406, val_acc:0.882]
Epoch [17/120    avg_loss:0.374, val_acc:0.867]
Epoch [18/120    avg_loss:0.234, val_acc:0.922]
Epoch [19/120    avg_loss:0.231, val_acc:0.914]
Epoch [20/120    avg_loss:0.169, val_acc:0.913]
Epoch [21/120    avg_loss:0.185, val_acc:0.924]
Epoch [22/120    avg_loss:0.149, val_acc:0.899]
Epoch [23/120    avg_loss:0.193, val_acc:0.939]
Epoch [24/120    avg_loss:0.163, val_acc:0.940]
Epoch [25/120    avg_loss:0.164, val_acc:0.941]
Epoch [26/120    avg_loss:0.183, val_acc:0.913]
Epoch [27/120    avg_loss:0.240, val_acc:0.928]
Epoch [28/120    avg_loss:0.201, val_acc:0.931]
Epoch [29/120    avg_loss:0.198, val_acc:0.948]
Epoch [30/120    avg_loss:0.236, val_acc:0.917]
Epoch [31/120    avg_loss:0.234, val_acc:0.945]
Epoch [32/120    avg_loss:0.134, val_acc:0.941]
Epoch [33/120    avg_loss:0.126, val_acc:0.932]
Epoch [34/120    avg_loss:0.141, val_acc:0.947]
Epoch [35/120    avg_loss:0.095, val_acc:0.912]
Epoch [36/120    avg_loss:0.096, val_acc:0.942]
Epoch [37/120    avg_loss:0.065, val_acc:0.950]
Epoch [38/120    avg_loss:0.065, val_acc:0.962]
Epoch [39/120    avg_loss:0.101, val_acc:0.919]
Epoch [40/120    avg_loss:0.048, val_acc:0.965]
Epoch [41/120    avg_loss:0.050, val_acc:0.954]
Epoch [42/120    avg_loss:0.077, val_acc:0.951]
Epoch [43/120    avg_loss:0.045, val_acc:0.955]
Epoch [44/120    avg_loss:0.056, val_acc:0.961]
Epoch [45/120    avg_loss:0.078, val_acc:0.950]
Epoch [46/120    avg_loss:0.040, val_acc:0.957]
Epoch [47/120    avg_loss:0.040, val_acc:0.959]
Epoch [48/120    avg_loss:0.047, val_acc:0.967]
Epoch [49/120    avg_loss:0.053, val_acc:0.925]
Epoch [50/120    avg_loss:0.126, val_acc:0.944]
Epoch [51/120    avg_loss:0.064, val_acc:0.968]
Epoch [52/120    avg_loss:0.053, val_acc:0.955]
Epoch [53/120    avg_loss:0.041, val_acc:0.952]
Epoch [54/120    avg_loss:0.045, val_acc:0.966]
Epoch [55/120    avg_loss:0.030, val_acc:0.963]
Epoch [56/120    avg_loss:0.038, val_acc:0.973]
Epoch [57/120    avg_loss:0.026, val_acc:0.973]
Epoch [58/120    avg_loss:0.024, val_acc:0.924]
Epoch [59/120    avg_loss:0.025, val_acc:0.974]
Epoch [60/120    avg_loss:0.028, val_acc:0.964]
Epoch [61/120    avg_loss:0.055, val_acc:0.975]
Epoch [62/120    avg_loss:0.031, val_acc:0.979]
Epoch [63/120    avg_loss:0.085, val_acc:0.950]
Epoch [64/120    avg_loss:0.057, val_acc:0.954]
Epoch [65/120    avg_loss:0.034, val_acc:0.979]
Epoch [66/120    avg_loss:0.018, val_acc:0.977]
Epoch [67/120    avg_loss:0.011, val_acc:0.982]
Epoch [68/120    avg_loss:0.015, val_acc:0.975]
Epoch [69/120    avg_loss:0.021, val_acc:0.966]
Epoch [70/120    avg_loss:0.013, val_acc:0.969]
Epoch [71/120    avg_loss:0.031, val_acc:0.964]
Epoch [72/120    avg_loss:0.015, val_acc:0.977]
Epoch [73/120    avg_loss:0.015, val_acc:0.968]
Epoch [74/120    avg_loss:0.023, val_acc:0.971]
Epoch [75/120    avg_loss:0.009, val_acc:0.974]
Epoch [76/120    avg_loss:0.007, val_acc:0.976]
Epoch [77/120    avg_loss:0.015, val_acc:0.978]
Epoch [78/120    avg_loss:0.059, val_acc:0.953]
Epoch [79/120    avg_loss:0.032, val_acc:0.961]
Epoch [80/120    avg_loss:0.044, val_acc:0.966]
Epoch [81/120    avg_loss:0.025, val_acc:0.970]
Epoch [82/120    avg_loss:0.012, val_acc:0.974]
Epoch [83/120    avg_loss:0.011, val_acc:0.977]
Epoch [84/120    avg_loss:0.009, val_acc:0.980]
Epoch [85/120    avg_loss:0.008, val_acc:0.979]
Epoch [86/120    avg_loss:0.010, val_acc:0.982]
Epoch [87/120    avg_loss:0.013, val_acc:0.981]
Epoch [88/120    avg_loss:0.007, val_acc:0.980]
Epoch [89/120    avg_loss:0.005, val_acc:0.980]
Epoch [90/120    avg_loss:0.006, val_acc:0.980]
Epoch [91/120    avg_loss:0.006, val_acc:0.979]
Epoch [92/120    avg_loss:0.008, val_acc:0.981]
Epoch [93/120    avg_loss:0.007, val_acc:0.980]
Epoch [94/120    avg_loss:0.007, val_acc:0.980]
Epoch [95/120    avg_loss:0.008, val_acc:0.980]
Epoch [96/120    avg_loss:0.008, val_acc:0.979]
Epoch [97/120    avg_loss:0.005, val_acc:0.979]
Epoch [98/120    avg_loss:0.004, val_acc:0.979]
Epoch [99/120    avg_loss:0.007, val_acc:0.979]
Epoch [100/120    avg_loss:0.007, val_acc:0.979]
Epoch [101/120    avg_loss:0.007, val_acc:0.979]
Epoch [102/120    avg_loss:0.010, val_acc:0.979]
Epoch [103/120    avg_loss:0.009, val_acc:0.979]
Epoch [104/120    avg_loss:0.007, val_acc:0.979]
Epoch [105/120    avg_loss:0.006, val_acc:0.979]
Epoch [106/120    avg_loss:0.017, val_acc:0.979]
Epoch [107/120    avg_loss:0.008, val_acc:0.979]
Epoch [108/120    avg_loss:0.008, val_acc:0.979]
Epoch [109/120    avg_loss:0.004, val_acc:0.979]
Epoch [110/120    avg_loss:0.004, val_acc:0.979]
Epoch [111/120    avg_loss:0.008, val_acc:0.978]
Epoch [112/120    avg_loss:0.005, val_acc:0.978]
Epoch [113/120    avg_loss:0.004, val_acc:0.978]
Epoch [114/120    avg_loss:0.006, val_acc:0.978]
Epoch [115/120    avg_loss:0.009, val_acc:0.978]
Epoch [116/120    avg_loss:0.007, val_acc:0.978]
Epoch [117/120    avg_loss:0.007, val_acc:0.978]
Epoch [118/120    avg_loss:0.007, val_acc:0.978]
Epoch [119/120    avg_loss:0.005, val_acc:0.978]
Epoch [120/120    avg_loss:0.006, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1278    0    0    1    0    0    0    0    3    2    1    0
     0    0    0]
 [   0    0    0  690    0   19    0    0    0    3    0    0   34    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0  839   27    4    0
     0    2    0]
 [   0    0   11    0   13    1    2    0    0    0    4 2178    0    1
     0    0    0]
 [   0    0    0    0    6    2    0    0    0    0    0    4  515    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   13    0    0    1    0    0    0    0    0
  1125    0    0]
 [   0    0    0    0    0    0   31    0    0    0    0    6    0    0
    27  283    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.45257452574526

F1 scores:
[       nan 0.98765432 0.99262136 0.95833333 0.95730337 0.95594714
 0.97550111 1.         0.99883856 0.81081081 0.97501453 0.98396205
 0.94582185 0.99462366 0.98210388 0.89556962 0.95402299]

Kappa:
0.970955391473828
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1c118d7630>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.173, val_acc:0.497]
Epoch [2/120    avg_loss:1.790, val_acc:0.523]
Epoch [3/120    avg_loss:1.600, val_acc:0.558]
Epoch [4/120    avg_loss:1.175, val_acc:0.635]
Epoch [5/120    avg_loss:1.052, val_acc:0.707]
Epoch [6/120    avg_loss:1.042, val_acc:0.733]
Epoch [7/120    avg_loss:0.827, val_acc:0.742]
Epoch [8/120    avg_loss:0.719, val_acc:0.733]
Epoch [9/120    avg_loss:0.945, val_acc:0.781]
Epoch [10/120    avg_loss:0.643, val_acc:0.810]
Epoch [11/120    avg_loss:0.596, val_acc:0.788]
Epoch [12/120    avg_loss:0.518, val_acc:0.839]
Epoch [13/120    avg_loss:0.443, val_acc:0.844]
Epoch [14/120    avg_loss:0.461, val_acc:0.797]
Epoch [15/120    avg_loss:0.425, val_acc:0.884]
Epoch [16/120    avg_loss:0.386, val_acc:0.885]
Epoch [17/120    avg_loss:0.360, val_acc:0.872]
Epoch [18/120    avg_loss:0.374, val_acc:0.875]
Epoch [19/120    avg_loss:0.298, val_acc:0.862]
Epoch [20/120    avg_loss:0.372, val_acc:0.878]
Epoch [21/120    avg_loss:0.266, val_acc:0.889]
Epoch [22/120    avg_loss:0.204, val_acc:0.912]
Epoch [23/120    avg_loss:0.210, val_acc:0.927]
Epoch [24/120    avg_loss:0.174, val_acc:0.920]
Epoch [25/120    avg_loss:0.123, val_acc:0.927]
Epoch [26/120    avg_loss:0.161, val_acc:0.925]
Epoch [27/120    avg_loss:0.187, val_acc:0.923]
Epoch [28/120    avg_loss:0.237, val_acc:0.917]
Epoch [29/120    avg_loss:0.137, val_acc:0.915]
Epoch [30/120    avg_loss:0.147, val_acc:0.920]
Epoch [31/120    avg_loss:0.122, val_acc:0.934]
Epoch [32/120    avg_loss:0.124, val_acc:0.934]
Epoch [33/120    avg_loss:0.187, val_acc:0.845]
Epoch [34/120    avg_loss:0.144, val_acc:0.938]
Epoch [35/120    avg_loss:0.120, val_acc:0.953]
Epoch [36/120    avg_loss:0.074, val_acc:0.959]
Epoch [37/120    avg_loss:0.065, val_acc:0.948]
Epoch [38/120    avg_loss:0.075, val_acc:0.943]
Epoch [39/120    avg_loss:0.070, val_acc:0.953]
Epoch [40/120    avg_loss:0.063, val_acc:0.933]
Epoch [41/120    avg_loss:0.087, val_acc:0.930]
Epoch [42/120    avg_loss:0.153, val_acc:0.943]
Epoch [43/120    avg_loss:0.088, val_acc:0.952]
Epoch [44/120    avg_loss:0.073, val_acc:0.954]
Epoch [45/120    avg_loss:0.058, val_acc:0.956]
Epoch [46/120    avg_loss:0.069, val_acc:0.959]
Epoch [47/120    avg_loss:0.065, val_acc:0.962]
Epoch [48/120    avg_loss:0.065, val_acc:0.959]
Epoch [49/120    avg_loss:0.153, val_acc:0.927]
Epoch [50/120    avg_loss:0.135, val_acc:0.939]
Epoch [51/120    avg_loss:0.159, val_acc:0.911]
Epoch [52/120    avg_loss:0.107, val_acc:0.932]
Epoch [53/120    avg_loss:0.071, val_acc:0.935]
Epoch [54/120    avg_loss:0.042, val_acc:0.971]
Epoch [55/120    avg_loss:0.058, val_acc:0.972]
Epoch [56/120    avg_loss:0.039, val_acc:0.961]
Epoch [57/120    avg_loss:0.060, val_acc:0.976]
Epoch [58/120    avg_loss:0.045, val_acc:0.971]
Epoch [59/120    avg_loss:0.028, val_acc:0.960]
Epoch [60/120    avg_loss:0.038, val_acc:0.981]
Epoch [61/120    avg_loss:0.036, val_acc:0.971]
Epoch [62/120    avg_loss:0.013, val_acc:0.974]
Epoch [63/120    avg_loss:0.028, val_acc:0.967]
Epoch [64/120    avg_loss:0.034, val_acc:0.987]
Epoch [65/120    avg_loss:0.017, val_acc:0.982]
Epoch [66/120    avg_loss:0.018, val_acc:0.983]
Epoch [67/120    avg_loss:0.028, val_acc:0.973]
Epoch [68/120    avg_loss:0.014, val_acc:0.974]
Epoch [69/120    avg_loss:0.026, val_acc:0.971]
Epoch [70/120    avg_loss:0.090, val_acc:0.980]
Epoch [71/120    avg_loss:0.026, val_acc:0.980]
Epoch [72/120    avg_loss:0.016, val_acc:0.984]
Epoch [73/120    avg_loss:0.016, val_acc:0.983]
Epoch [74/120    avg_loss:0.034, val_acc:0.969]
Epoch [75/120    avg_loss:0.021, val_acc:0.981]
Epoch [76/120    avg_loss:0.022, val_acc:0.983]
Epoch [77/120    avg_loss:0.019, val_acc:0.980]
Epoch [78/120    avg_loss:0.033, val_acc:0.984]
Epoch [79/120    avg_loss:0.013, val_acc:0.983]
Epoch [80/120    avg_loss:0.015, val_acc:0.985]
Epoch [81/120    avg_loss:0.013, val_acc:0.986]
Epoch [82/120    avg_loss:0.010, val_acc:0.985]
Epoch [83/120    avg_loss:0.010, val_acc:0.986]
Epoch [84/120    avg_loss:0.012, val_acc:0.986]
Epoch [85/120    avg_loss:0.009, val_acc:0.987]
Epoch [86/120    avg_loss:0.011, val_acc:0.986]
Epoch [87/120    avg_loss:0.007, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.988]
Epoch [89/120    avg_loss:0.006, val_acc:0.988]
Epoch [90/120    avg_loss:0.009, val_acc:0.988]
Epoch [91/120    avg_loss:0.007, val_acc:0.986]
Epoch [92/120    avg_loss:0.013, val_acc:0.987]
Epoch [93/120    avg_loss:0.009, val_acc:0.987]
Epoch [94/120    avg_loss:0.006, val_acc:0.987]
Epoch [95/120    avg_loss:0.011, val_acc:0.987]
Epoch [96/120    avg_loss:0.011, val_acc:0.987]
Epoch [97/120    avg_loss:0.008, val_acc:0.986]
Epoch [98/120    avg_loss:0.005, val_acc:0.987]
Epoch [99/120    avg_loss:0.012, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.986]
Epoch [101/120    avg_loss:0.009, val_acc:0.986]
Epoch [102/120    avg_loss:0.014, val_acc:0.983]
Epoch [103/120    avg_loss:0.008, val_acc:0.985]
Epoch [104/120    avg_loss:0.006, val_acc:0.985]
Epoch [105/120    avg_loss:0.009, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.007, val_acc:0.986]
Epoch [108/120    avg_loss:0.011, val_acc:0.986]
Epoch [109/120    avg_loss:0.009, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.005, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.007, val_acc:0.986]
Epoch [119/120    avg_loss:0.009, val_acc:0.986]
Epoch [120/120    avg_loss:0.011, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1272    0    0    0    0    0    0    0    4    2    1    0
     0    6    0]
 [   0    0    0  712    0   17    0    0    0    5    0    0   12    1
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0  847   22    0    0
     0    4    0]
 [   0    0    8    0    0    0   11    0    0    0    5 2184    0    2
     0    0    0]
 [   0    0    0    0   13    0    0    0    0    0   12    0  502    0
     0    0    7]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   20    0    0    0    0    0    0    0
    35  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.8970189701897

F1 scores:
[       nan 1.         0.99181287 0.97334245 0.96567506 0.97853107
 0.97695167 1.         1.         0.8372093  0.97188755 0.98868266
 0.95619048 0.9919571  0.98486814 0.89984592 0.95402299]

Kappa:
0.9760225228716514
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:06:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7999030710>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.204, val_acc:0.408]
Epoch [2/120    avg_loss:1.680, val_acc:0.606]
Epoch [3/120    avg_loss:1.452, val_acc:0.661]
Epoch [4/120    avg_loss:1.127, val_acc:0.671]
Epoch [5/120    avg_loss:1.088, val_acc:0.743]
Epoch [6/120    avg_loss:0.871, val_acc:0.727]
Epoch [7/120    avg_loss:0.801, val_acc:0.761]
Epoch [8/120    avg_loss:0.842, val_acc:0.780]
Epoch [9/120    avg_loss:0.665, val_acc:0.761]
Epoch [10/120    avg_loss:0.711, val_acc:0.770]
Epoch [11/120    avg_loss:0.554, val_acc:0.837]
Epoch [12/120    avg_loss:0.550, val_acc:0.772]
Epoch [13/120    avg_loss:0.453, val_acc:0.860]
Epoch [14/120    avg_loss:0.419, val_acc:0.828]
Epoch [15/120    avg_loss:0.557, val_acc:0.872]
Epoch [16/120    avg_loss:0.370, val_acc:0.871]
Epoch [17/120    avg_loss:0.314, val_acc:0.878]
Epoch [18/120    avg_loss:0.253, val_acc:0.879]
Epoch [19/120    avg_loss:0.376, val_acc:0.871]
Epoch [20/120    avg_loss:0.304, val_acc:0.897]
Epoch [21/120    avg_loss:0.210, val_acc:0.914]
Epoch [22/120    avg_loss:0.179, val_acc:0.927]
Epoch [23/120    avg_loss:0.255, val_acc:0.911]
Epoch [24/120    avg_loss:0.180, val_acc:0.877]
Epoch [25/120    avg_loss:0.253, val_acc:0.896]
Epoch [26/120    avg_loss:0.172, val_acc:0.934]
Epoch [27/120    avg_loss:0.241, val_acc:0.818]
Epoch [28/120    avg_loss:0.262, val_acc:0.910]
Epoch [29/120    avg_loss:0.270, val_acc:0.849]
Epoch [30/120    avg_loss:0.296, val_acc:0.816]
Epoch [31/120    avg_loss:0.210, val_acc:0.923]
Epoch [32/120    avg_loss:0.089, val_acc:0.938]
Epoch [33/120    avg_loss:0.097, val_acc:0.941]
Epoch [34/120    avg_loss:0.136, val_acc:0.927]
Epoch [35/120    avg_loss:0.130, val_acc:0.956]
Epoch [36/120    avg_loss:0.085, val_acc:0.950]
Epoch [37/120    avg_loss:0.092, val_acc:0.938]
Epoch [38/120    avg_loss:0.082, val_acc:0.962]
Epoch [39/120    avg_loss:0.073, val_acc:0.921]
Epoch [40/120    avg_loss:0.056, val_acc:0.974]
Epoch [41/120    avg_loss:0.058, val_acc:0.971]
Epoch [42/120    avg_loss:0.090, val_acc:0.955]
Epoch [43/120    avg_loss:0.062, val_acc:0.973]
Epoch [44/120    avg_loss:0.040, val_acc:0.970]
Epoch [45/120    avg_loss:0.040, val_acc:0.970]
Epoch [46/120    avg_loss:0.037, val_acc:0.944]
Epoch [47/120    avg_loss:0.040, val_acc:0.979]
Epoch [48/120    avg_loss:0.028, val_acc:0.972]
Epoch [49/120    avg_loss:0.052, val_acc:0.956]
Epoch [50/120    avg_loss:0.035, val_acc:0.972]
Epoch [51/120    avg_loss:0.026, val_acc:0.974]
Epoch [52/120    avg_loss:0.018, val_acc:0.977]
Epoch [53/120    avg_loss:0.016, val_acc:0.983]
Epoch [54/120    avg_loss:0.037, val_acc:0.962]
Epoch [55/120    avg_loss:0.031, val_acc:0.960]
Epoch [56/120    avg_loss:0.039, val_acc:0.974]
Epoch [57/120    avg_loss:0.015, val_acc:0.982]
Epoch [58/120    avg_loss:0.012, val_acc:0.980]
Epoch [59/120    avg_loss:0.012, val_acc:0.985]
Epoch [60/120    avg_loss:0.024, val_acc:0.977]
Epoch [61/120    avg_loss:0.009, val_acc:0.978]
Epoch [62/120    avg_loss:0.036, val_acc:0.978]
Epoch [63/120    avg_loss:0.029, val_acc:0.974]
Epoch [64/120    avg_loss:0.016, val_acc:0.977]
Epoch [65/120    avg_loss:0.031, val_acc:0.978]
Epoch [66/120    avg_loss:0.066, val_acc:0.982]
Epoch [67/120    avg_loss:0.028, val_acc:0.983]
Epoch [68/120    avg_loss:0.034, val_acc:0.964]
Epoch [69/120    avg_loss:0.012, val_acc:0.983]
Epoch [70/120    avg_loss:0.009, val_acc:0.984]
Epoch [71/120    avg_loss:0.009, val_acc:0.982]
Epoch [72/120    avg_loss:0.013, val_acc:0.982]
Epoch [73/120    avg_loss:0.009, val_acc:0.983]
Epoch [74/120    avg_loss:0.008, val_acc:0.983]
Epoch [75/120    avg_loss:0.006, val_acc:0.984]
Epoch [76/120    avg_loss:0.006, val_acc:0.984]
Epoch [77/120    avg_loss:0.009, val_acc:0.985]
Epoch [78/120    avg_loss:0.008, val_acc:0.983]
Epoch [79/120    avg_loss:0.008, val_acc:0.983]
Epoch [80/120    avg_loss:0.007, val_acc:0.984]
Epoch [81/120    avg_loss:0.007, val_acc:0.984]
Epoch [82/120    avg_loss:0.004, val_acc:0.984]
Epoch [83/120    avg_loss:0.005, val_acc:0.985]
Epoch [84/120    avg_loss:0.008, val_acc:0.985]
Epoch [85/120    avg_loss:0.013, val_acc:0.984]
Epoch [86/120    avg_loss:0.008, val_acc:0.981]
Epoch [87/120    avg_loss:0.006, val_acc:0.982]
Epoch [88/120    avg_loss:0.007, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.985]
Epoch [90/120    avg_loss:0.006, val_acc:0.987]
Epoch [91/120    avg_loss:0.006, val_acc:0.987]
Epoch [92/120    avg_loss:0.006, val_acc:0.987]
Epoch [93/120    avg_loss:0.008, val_acc:0.985]
Epoch [94/120    avg_loss:0.005, val_acc:0.985]
Epoch [95/120    avg_loss:0.004, val_acc:0.985]
Epoch [96/120    avg_loss:0.004, val_acc:0.988]
Epoch [97/120    avg_loss:0.004, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.007, val_acc:0.988]
Epoch [100/120    avg_loss:0.008, val_acc:0.987]
Epoch [101/120    avg_loss:0.007, val_acc:0.988]
Epoch [102/120    avg_loss:0.005, val_acc:0.989]
Epoch [103/120    avg_loss:0.004, val_acc:0.989]
Epoch [104/120    avg_loss:0.005, val_acc:0.990]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.004, val_acc:0.990]
Epoch [107/120    avg_loss:0.006, val_acc:0.990]
Epoch [108/120    avg_loss:0.008, val_acc:0.989]
Epoch [109/120    avg_loss:0.005, val_acc:0.990]
Epoch [110/120    avg_loss:0.007, val_acc:0.991]
Epoch [111/120    avg_loss:0.008, val_acc:0.990]
Epoch [112/120    avg_loss:0.006, val_acc:0.989]
Epoch [113/120    avg_loss:0.006, val_acc:0.989]
Epoch [114/120    avg_loss:0.007, val_acc:0.990]
Epoch [115/120    avg_loss:0.007, val_acc:0.991]
Epoch [116/120    avg_loss:0.006, val_acc:0.989]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.004, val_acc:0.989]
Epoch [119/120    avg_loss:0.007, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1277    0    0    0    0    0    0    0    5    1    0    0
     0    2    0]
 [   0    0    0  653   68   17    0    0    0    2    1    0    6    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   14    3    0    3    0    0    0    0  823   27    2    0
     1    2    0]
 [   0    0    8    0    0    6   12    0    0    0    5 2178    0    1
     0    0    0]
 [   0    0    0    1   14    2    0    0    0    0    4    0  507    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    0    0    0    0    1    0    0    0
  1129    0    0]
 [   0    0    0    0    0    0   32    0    0    0    0    0    0    0
    25  290    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.89972899728997

F1 scores:
[       nan 1.         0.98839009 0.92821606 0.83629191 0.95690608
 0.96759941 0.98039216 1.         0.86486486 0.96032672 0.98641304
 0.96571429 0.99730458 0.98430689 0.90483619 0.95953757]

Kappa:
0.9646707010998167
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fab470eb5c0>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.249, val_acc:0.460]
Epoch [2/120    avg_loss:1.800, val_acc:0.628]
Epoch [3/120    avg_loss:1.388, val_acc:0.592]
Epoch [4/120    avg_loss:1.295, val_acc:0.623]
Epoch [5/120    avg_loss:0.992, val_acc:0.692]
Epoch [6/120    avg_loss:1.003, val_acc:0.711]
Epoch [7/120    avg_loss:0.835, val_acc:0.778]
Epoch [8/120    avg_loss:0.711, val_acc:0.750]
Epoch [9/120    avg_loss:0.683, val_acc:0.832]
Epoch [10/120    avg_loss:0.745, val_acc:0.808]
Epoch [11/120    avg_loss:0.625, val_acc:0.816]
Epoch [12/120    avg_loss:0.539, val_acc:0.772]
Epoch [13/120    avg_loss:0.514, val_acc:0.821]
Epoch [14/120    avg_loss:0.463, val_acc:0.818]
Epoch [15/120    avg_loss:0.511, val_acc:0.870]
Epoch [16/120    avg_loss:0.369, val_acc:0.824]
Epoch [17/120    avg_loss:0.323, val_acc:0.886]
Epoch [18/120    avg_loss:0.281, val_acc:0.847]
Epoch [19/120    avg_loss:0.298, val_acc:0.881]
Epoch [20/120    avg_loss:0.219, val_acc:0.925]
Epoch [21/120    avg_loss:0.173, val_acc:0.886]
Epoch [22/120    avg_loss:0.275, val_acc:0.893]
Epoch [23/120    avg_loss:0.211, val_acc:0.922]
Epoch [24/120    avg_loss:0.179, val_acc:0.853]
Epoch [25/120    avg_loss:0.189, val_acc:0.872]
Epoch [26/120    avg_loss:0.196, val_acc:0.915]
Epoch [27/120    avg_loss:0.256, val_acc:0.851]
Epoch [28/120    avg_loss:0.162, val_acc:0.904]
Epoch [29/120    avg_loss:0.120, val_acc:0.935]
Epoch [30/120    avg_loss:0.187, val_acc:0.918]
Epoch [31/120    avg_loss:0.131, val_acc:0.931]
Epoch [32/120    avg_loss:0.105, val_acc:0.947]
Epoch [33/120    avg_loss:0.088, val_acc:0.928]
Epoch [34/120    avg_loss:0.120, val_acc:0.939]
Epoch [35/120    avg_loss:0.116, val_acc:0.941]
Epoch [36/120    avg_loss:0.117, val_acc:0.935]
Epoch [37/120    avg_loss:0.122, val_acc:0.925]
Epoch [38/120    avg_loss:0.124, val_acc:0.960]
Epoch [39/120    avg_loss:0.059, val_acc:0.934]
Epoch [40/120    avg_loss:0.059, val_acc:0.967]
Epoch [41/120    avg_loss:0.052, val_acc:0.962]
Epoch [42/120    avg_loss:0.027, val_acc:0.960]
Epoch [43/120    avg_loss:0.088, val_acc:0.945]
Epoch [44/120    avg_loss:0.080, val_acc:0.926]
Epoch [45/120    avg_loss:0.061, val_acc:0.968]
Epoch [46/120    avg_loss:0.037, val_acc:0.962]
Epoch [47/120    avg_loss:0.036, val_acc:0.977]
Epoch [48/120    avg_loss:0.063, val_acc:0.922]
Epoch [49/120    avg_loss:0.112, val_acc:0.966]
Epoch [50/120    avg_loss:0.037, val_acc:0.975]
Epoch [51/120    avg_loss:0.081, val_acc:0.921]
Epoch [52/120    avg_loss:0.109, val_acc:0.961]
Epoch [53/120    avg_loss:0.050, val_acc:0.960]
Epoch [54/120    avg_loss:0.052, val_acc:0.958]
Epoch [55/120    avg_loss:0.047, val_acc:0.970]
Epoch [56/120    avg_loss:0.039, val_acc:0.974]
Epoch [57/120    avg_loss:0.031, val_acc:0.971]
Epoch [58/120    avg_loss:0.039, val_acc:0.976]
Epoch [59/120    avg_loss:0.028, val_acc:0.944]
Epoch [60/120    avg_loss:0.029, val_acc:0.972]
Epoch [61/120    avg_loss:0.014, val_acc:0.976]
Epoch [62/120    avg_loss:0.018, val_acc:0.982]
Epoch [63/120    avg_loss:0.025, val_acc:0.984]
Epoch [64/120    avg_loss:0.023, val_acc:0.985]
Epoch [65/120    avg_loss:0.020, val_acc:0.984]
Epoch [66/120    avg_loss:0.017, val_acc:0.984]
Epoch [67/120    avg_loss:0.010, val_acc:0.984]
Epoch [68/120    avg_loss:0.012, val_acc:0.984]
Epoch [69/120    avg_loss:0.014, val_acc:0.985]
Epoch [70/120    avg_loss:0.012, val_acc:0.985]
Epoch [71/120    avg_loss:0.007, val_acc:0.985]
Epoch [72/120    avg_loss:0.009, val_acc:0.986]
Epoch [73/120    avg_loss:0.009, val_acc:0.986]
Epoch [74/120    avg_loss:0.009, val_acc:0.985]
Epoch [75/120    avg_loss:0.011, val_acc:0.985]
Epoch [76/120    avg_loss:0.007, val_acc:0.986]
Epoch [77/120    avg_loss:0.011, val_acc:0.985]
Epoch [78/120    avg_loss:0.010, val_acc:0.984]
Epoch [79/120    avg_loss:0.010, val_acc:0.986]
Epoch [80/120    avg_loss:0.007, val_acc:0.985]
Epoch [81/120    avg_loss:0.010, val_acc:0.986]
Epoch [82/120    avg_loss:0.009, val_acc:0.986]
Epoch [83/120    avg_loss:0.010, val_acc:0.986]
Epoch [84/120    avg_loss:0.010, val_acc:0.985]
Epoch [85/120    avg_loss:0.010, val_acc:0.986]
Epoch [86/120    avg_loss:0.010, val_acc:0.986]
Epoch [87/120    avg_loss:0.007, val_acc:0.986]
Epoch [88/120    avg_loss:0.017, val_acc:0.987]
Epoch [89/120    avg_loss:0.007, val_acc:0.988]
Epoch [90/120    avg_loss:0.013, val_acc:0.987]
Epoch [91/120    avg_loss:0.011, val_acc:0.986]
Epoch [92/120    avg_loss:0.011, val_acc:0.987]
Epoch [93/120    avg_loss:0.012, val_acc:0.987]
Epoch [94/120    avg_loss:0.008, val_acc:0.988]
Epoch [95/120    avg_loss:0.007, val_acc:0.987]
Epoch [96/120    avg_loss:0.008, val_acc:0.987]
Epoch [97/120    avg_loss:0.013, val_acc:0.986]
Epoch [98/120    avg_loss:0.008, val_acc:0.986]
Epoch [99/120    avg_loss:0.012, val_acc:0.985]
Epoch [100/120    avg_loss:0.008, val_acc:0.987]
Epoch [101/120    avg_loss:0.010, val_acc:0.986]
Epoch [102/120    avg_loss:0.013, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.987]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.989]
Epoch [106/120    avg_loss:0.013, val_acc:0.988]
Epoch [107/120    avg_loss:0.012, val_acc:0.987]
Epoch [108/120    avg_loss:0.007, val_acc:0.987]
Epoch [109/120    avg_loss:0.009, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.985]
Epoch [111/120    avg_loss:0.006, val_acc:0.985]
Epoch [112/120    avg_loss:0.013, val_acc:0.986]
Epoch [113/120    avg_loss:0.010, val_acc:0.986]
Epoch [114/120    avg_loss:0.007, val_acc:0.986]
Epoch [115/120    avg_loss:0.008, val_acc:0.987]
Epoch [116/120    avg_loss:0.010, val_acc:0.987]
Epoch [117/120    avg_loss:0.009, val_acc:0.987]
Epoch [118/120    avg_loss:0.005, val_acc:0.987]
Epoch [119/120    avg_loss:0.012, val_acc:0.987]
Epoch [120/120    avg_loss:0.009, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1255    0    0    0    3    0    0    0   18    1    3    0
     0    5    0]
 [   0    0    0  647    1   21    0    0    0    3    1    0   71    2
     0    1    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   11    0    0    0    0    3    0    0  839   13    0    0
     2    7    0]
 [   0    0   18    0    6    3   11    0    0    0    4 2168    0    0
     0    0    0]
 [   0    0    0    7   24    0    0    0    0    0    1    0  498    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   13    0    0    1    0    0    0    0    0
  1125    0    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
    22  323    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.8780487804878

F1 scores:
[       nan 1.         0.97703387 0.92099644 0.92747253 0.9580574
 0.98796992 0.94339623 0.99883856 0.84210526 0.96547756 0.98724954
 0.899729   0.99462366 0.98339161 0.94582723 0.97076023]

Kappa:
0.9644441889193944
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f19c2c256a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.152, val_acc:0.477]
Epoch [2/120    avg_loss:1.835, val_acc:0.646]
Epoch [3/120    avg_loss:1.299, val_acc:0.642]
Epoch [4/120    avg_loss:1.181, val_acc:0.641]
Epoch [5/120    avg_loss:1.176, val_acc:0.651]
Epoch [6/120    avg_loss:1.030, val_acc:0.721]
Epoch [7/120    avg_loss:0.872, val_acc:0.782]
Epoch [8/120    avg_loss:0.767, val_acc:0.698]
Epoch [9/120    avg_loss:0.747, val_acc:0.792]
Epoch [10/120    avg_loss:0.629, val_acc:0.787]
Epoch [11/120    avg_loss:0.550, val_acc:0.829]
Epoch [12/120    avg_loss:0.514, val_acc:0.766]
Epoch [13/120    avg_loss:0.632, val_acc:0.833]
Epoch [14/120    avg_loss:0.466, val_acc:0.804]
Epoch [15/120    avg_loss:0.466, val_acc:0.842]
Epoch [16/120    avg_loss:0.372, val_acc:0.850]
Epoch [17/120    avg_loss:0.360, val_acc:0.799]
Epoch [18/120    avg_loss:0.333, val_acc:0.872]
Epoch [19/120    avg_loss:0.466, val_acc:0.899]
Epoch [20/120    avg_loss:0.328, val_acc:0.913]
Epoch [21/120    avg_loss:0.216, val_acc:0.870]
Epoch [22/120    avg_loss:0.219, val_acc:0.911]
Epoch [23/120    avg_loss:0.317, val_acc:0.892]
Epoch [24/120    avg_loss:0.236, val_acc:0.925]
Epoch [25/120    avg_loss:0.229, val_acc:0.920]
Epoch [26/120    avg_loss:0.194, val_acc:0.929]
Epoch [27/120    avg_loss:0.173, val_acc:0.923]
Epoch [28/120    avg_loss:0.203, val_acc:0.909]
Epoch [29/120    avg_loss:0.154, val_acc:0.872]
Epoch [30/120    avg_loss:0.264, val_acc:0.872]
Epoch [31/120    avg_loss:0.174, val_acc:0.928]
Epoch [32/120    avg_loss:0.122, val_acc:0.891]
Epoch [33/120    avg_loss:0.092, val_acc:0.932]
Epoch [34/120    avg_loss:0.135, val_acc:0.936]
Epoch [35/120    avg_loss:0.203, val_acc:0.942]
Epoch [36/120    avg_loss:0.137, val_acc:0.937]
Epoch [37/120    avg_loss:0.099, val_acc:0.946]
Epoch [38/120    avg_loss:0.055, val_acc:0.903]
Epoch [39/120    avg_loss:0.080, val_acc:0.935]
Epoch [40/120    avg_loss:0.084, val_acc:0.955]
Epoch [41/120    avg_loss:0.077, val_acc:0.921]
Epoch [42/120    avg_loss:0.127, val_acc:0.923]
Epoch [43/120    avg_loss:0.132, val_acc:0.937]
Epoch [44/120    avg_loss:0.068, val_acc:0.957]
Epoch [45/120    avg_loss:0.052, val_acc:0.960]
Epoch [46/120    avg_loss:0.062, val_acc:0.957]
Epoch [47/120    avg_loss:0.061, val_acc:0.951]
Epoch [48/120    avg_loss:0.056, val_acc:0.960]
Epoch [49/120    avg_loss:0.032, val_acc:0.959]
Epoch [50/120    avg_loss:0.030, val_acc:0.967]
Epoch [51/120    avg_loss:0.043, val_acc:0.964]
Epoch [52/120    avg_loss:0.055, val_acc:0.960]
Epoch [53/120    avg_loss:0.045, val_acc:0.966]
Epoch [54/120    avg_loss:0.056, val_acc:0.952]
Epoch [55/120    avg_loss:0.052, val_acc:0.946]
Epoch [56/120    avg_loss:0.056, val_acc:0.953]
Epoch [57/120    avg_loss:0.042, val_acc:0.957]
Epoch [58/120    avg_loss:0.030, val_acc:0.965]
Epoch [59/120    avg_loss:0.021, val_acc:0.967]
Epoch [60/120    avg_loss:0.050, val_acc:0.955]
Epoch [61/120    avg_loss:0.658, val_acc:0.898]
Epoch [62/120    avg_loss:0.185, val_acc:0.939]
Epoch [63/120    avg_loss:0.136, val_acc:0.941]
Epoch [64/120    avg_loss:0.075, val_acc:0.948]
Epoch [65/120    avg_loss:0.068, val_acc:0.960]
Epoch [66/120    avg_loss:0.037, val_acc:0.960]
Epoch [67/120    avg_loss:0.028, val_acc:0.970]
Epoch [68/120    avg_loss:0.088, val_acc:0.952]
Epoch [69/120    avg_loss:0.083, val_acc:0.955]
Epoch [70/120    avg_loss:0.040, val_acc:0.968]
Epoch [71/120    avg_loss:0.046, val_acc:0.959]
Epoch [72/120    avg_loss:0.044, val_acc:0.952]
Epoch [73/120    avg_loss:0.030, val_acc:0.974]
Epoch [74/120    avg_loss:0.019, val_acc:0.977]
Epoch [75/120    avg_loss:0.032, val_acc:0.977]
Epoch [76/120    avg_loss:0.018, val_acc:0.974]
Epoch [77/120    avg_loss:0.012, val_acc:0.980]
Epoch [78/120    avg_loss:0.014, val_acc:0.974]
Epoch [79/120    avg_loss:0.009, val_acc:0.972]
Epoch [80/120    avg_loss:0.030, val_acc:0.973]
Epoch [81/120    avg_loss:0.021, val_acc:0.973]
Epoch [82/120    avg_loss:0.034, val_acc:0.970]
Epoch [83/120    avg_loss:0.015, val_acc:0.980]
Epoch [84/120    avg_loss:0.018, val_acc:0.973]
Epoch [85/120    avg_loss:0.033, val_acc:0.963]
Epoch [86/120    avg_loss:0.014, val_acc:0.975]
Epoch [87/120    avg_loss:0.011, val_acc:0.975]
Epoch [88/120    avg_loss:0.010, val_acc:0.966]
Epoch [89/120    avg_loss:0.011, val_acc:0.979]
Epoch [90/120    avg_loss:0.005, val_acc:0.980]
Epoch [91/120    avg_loss:0.006, val_acc:0.976]
Epoch [92/120    avg_loss:0.012, val_acc:0.976]
Epoch [93/120    avg_loss:0.008, val_acc:0.976]
Epoch [94/120    avg_loss:0.005, val_acc:0.977]
Epoch [95/120    avg_loss:0.019, val_acc:0.980]
Epoch [96/120    avg_loss:0.012, val_acc:0.978]
Epoch [97/120    avg_loss:0.006, val_acc:0.979]
Epoch [98/120    avg_loss:0.015, val_acc:0.974]
Epoch [99/120    avg_loss:0.022, val_acc:0.978]
Epoch [100/120    avg_loss:0.011, val_acc:0.970]
Epoch [101/120    avg_loss:0.008, val_acc:0.977]
Epoch [102/120    avg_loss:0.015, val_acc:0.962]
Epoch [103/120    avg_loss:0.017, val_acc:0.985]
Epoch [104/120    avg_loss:0.012, val_acc:0.974]
Epoch [105/120    avg_loss:0.006, val_acc:0.988]
Epoch [106/120    avg_loss:0.006, val_acc:0.979]
Epoch [107/120    avg_loss:0.018, val_acc:0.976]
Epoch [108/120    avg_loss:0.012, val_acc:0.977]
Epoch [109/120    avg_loss:0.011, val_acc:0.980]
Epoch [110/120    avg_loss:0.008, val_acc:0.977]
Epoch [111/120    avg_loss:0.010, val_acc:0.985]
Epoch [112/120    avg_loss:0.010, val_acc:0.983]
Epoch [113/120    avg_loss:0.004, val_acc:0.985]
Epoch [114/120    avg_loss:0.016, val_acc:0.977]
Epoch [115/120    avg_loss:0.008, val_acc:0.979]
Epoch [116/120    avg_loss:0.004, val_acc:0.982]
Epoch [117/120    avg_loss:0.006, val_acc:0.982]
Epoch [118/120    avg_loss:0.003, val_acc:0.985]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.004, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1254    7    0    0    0    0    0    0   20    4    0    0
     0    0    0]
 [   0    0    0  719    0   13    0    0    0    1    0    0   12    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0  654    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  871    4    0    0
     0    0    0]
 [   0    0    6    0    0    2    7    0    0    0    3 2191    0    1
     0    0    0]
 [   0    0    0    0    8    0    0    0    0    0   11    0  512    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    2    0    0    0
  1129    0    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    26  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.13550135501355

F1 scores:
[       nan 1.         0.98430141 0.97491525 0.98156682 0.97072072
 0.9753915  0.98039216 0.99883856 0.84210526 0.97755331 0.99387616
 0.96786389 0.9919571  0.983878   0.92403101 0.98823529]

Kappa:
0.9787467997639122
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdd60bdc6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.209, val_acc:0.327]
Epoch [2/120    avg_loss:1.860, val_acc:0.502]
Epoch [3/120    avg_loss:1.716, val_acc:0.646]
Epoch [4/120    avg_loss:1.315, val_acc:0.700]
Epoch [5/120    avg_loss:1.220, val_acc:0.741]
Epoch [6/120    avg_loss:0.942, val_acc:0.765]
Epoch [7/120    avg_loss:0.893, val_acc:0.770]
Epoch [8/120    avg_loss:0.831, val_acc:0.775]
Epoch [9/120    avg_loss:0.667, val_acc:0.795]
Epoch [10/120    avg_loss:0.778, val_acc:0.733]
Epoch [11/120    avg_loss:0.657, val_acc:0.771]
Epoch [12/120    avg_loss:0.677, val_acc:0.833]
Epoch [13/120    avg_loss:0.528, val_acc:0.872]
Epoch [14/120    avg_loss:0.423, val_acc:0.836]
Epoch [15/120    avg_loss:0.420, val_acc:0.877]
Epoch [16/120    avg_loss:0.382, val_acc:0.861]
Epoch [17/120    avg_loss:0.339, val_acc:0.855]
Epoch [18/120    avg_loss:0.376, val_acc:0.829]
Epoch [19/120    avg_loss:0.421, val_acc:0.874]
Epoch [20/120    avg_loss:0.299, val_acc:0.889]
Epoch [21/120    avg_loss:0.342, val_acc:0.850]
Epoch [22/120    avg_loss:0.319, val_acc:0.892]
Epoch [23/120    avg_loss:0.271, val_acc:0.908]
Epoch [24/120    avg_loss:0.207, val_acc:0.935]
Epoch [25/120    avg_loss:0.283, val_acc:0.890]
Epoch [26/120    avg_loss:0.269, val_acc:0.917]
Epoch [27/120    avg_loss:0.194, val_acc:0.936]
Epoch [28/120    avg_loss:0.211, val_acc:0.914]
Epoch [29/120    avg_loss:0.115, val_acc:0.926]
Epoch [30/120    avg_loss:0.144, val_acc:0.940]
Epoch [31/120    avg_loss:0.128, val_acc:0.957]
Epoch [32/120    avg_loss:0.102, val_acc:0.949]
Epoch [33/120    avg_loss:0.153, val_acc:0.920]
Epoch [34/120    avg_loss:0.141, val_acc:0.935]
Epoch [35/120    avg_loss:0.135, val_acc:0.955]
Epoch [36/120    avg_loss:0.123, val_acc:0.950]
Epoch [37/120    avg_loss:0.083, val_acc:0.952]
Epoch [38/120    avg_loss:0.156, val_acc:0.949]
Epoch [39/120    avg_loss:0.086, val_acc:0.947]
Epoch [40/120    avg_loss:0.110, val_acc:0.910]
Epoch [41/120    avg_loss:0.091, val_acc:0.949]
Epoch [42/120    avg_loss:0.086, val_acc:0.958]
Epoch [43/120    avg_loss:0.051, val_acc:0.958]
Epoch [44/120    avg_loss:0.106, val_acc:0.950]
Epoch [45/120    avg_loss:0.106, val_acc:0.950]
Epoch [46/120    avg_loss:0.056, val_acc:0.952]
Epoch [47/120    avg_loss:0.170, val_acc:0.949]
Epoch [48/120    avg_loss:0.088, val_acc:0.959]
Epoch [49/120    avg_loss:0.065, val_acc:0.963]
Epoch [50/120    avg_loss:0.079, val_acc:0.955]
Epoch [51/120    avg_loss:0.089, val_acc:0.952]
Epoch [52/120    avg_loss:0.062, val_acc:0.966]
Epoch [53/120    avg_loss:0.052, val_acc:0.955]
Epoch [54/120    avg_loss:0.047, val_acc:0.970]
Epoch [55/120    avg_loss:0.065, val_acc:0.965]
Epoch [56/120    avg_loss:0.033, val_acc:0.973]
Epoch [57/120    avg_loss:0.053, val_acc:0.961]
Epoch [58/120    avg_loss:0.035, val_acc:0.973]
Epoch [59/120    avg_loss:0.023, val_acc:0.972]
Epoch [60/120    avg_loss:0.014, val_acc:0.972]
Epoch [61/120    avg_loss:0.015, val_acc:0.977]
Epoch [62/120    avg_loss:0.024, val_acc:0.975]
Epoch [63/120    avg_loss:0.015, val_acc:0.967]
Epoch [64/120    avg_loss:0.018, val_acc:0.973]
Epoch [65/120    avg_loss:0.038, val_acc:0.973]
Epoch [66/120    avg_loss:0.019, val_acc:0.976]
Epoch [67/120    avg_loss:0.039, val_acc:0.951]
Epoch [68/120    avg_loss:0.043, val_acc:0.969]
Epoch [69/120    avg_loss:0.021, val_acc:0.965]
Epoch [70/120    avg_loss:0.022, val_acc:0.969]
Epoch [71/120    avg_loss:0.024, val_acc:0.973]
Epoch [72/120    avg_loss:0.022, val_acc:0.977]
Epoch [73/120    avg_loss:0.025, val_acc:0.978]
Epoch [74/120    avg_loss:0.021, val_acc:0.975]
Epoch [75/120    avg_loss:0.019, val_acc:0.969]
Epoch [76/120    avg_loss:0.037, val_acc:0.970]
Epoch [77/120    avg_loss:0.044, val_acc:0.963]
Epoch [78/120    avg_loss:0.053, val_acc:0.963]
Epoch [79/120    avg_loss:0.030, val_acc:0.971]
Epoch [80/120    avg_loss:0.010, val_acc:0.981]
Epoch [81/120    avg_loss:0.023, val_acc:0.976]
Epoch [82/120    avg_loss:0.037, val_acc:0.971]
Epoch [83/120    avg_loss:0.028, val_acc:0.970]
Epoch [84/120    avg_loss:0.018, val_acc:0.972]
Epoch [85/120    avg_loss:0.026, val_acc:0.971]
Epoch [86/120    avg_loss:0.038, val_acc:0.966]
Epoch [87/120    avg_loss:0.044, val_acc:0.976]
Epoch [88/120    avg_loss:0.013, val_acc:0.981]
Epoch [89/120    avg_loss:0.045, val_acc:0.973]
Epoch [90/120    avg_loss:0.362, val_acc:0.889]
Epoch [91/120    avg_loss:0.240, val_acc:0.937]
Epoch [92/120    avg_loss:0.106, val_acc:0.957]
Epoch [93/120    avg_loss:0.165, val_acc:0.935]
Epoch [94/120    avg_loss:0.082, val_acc:0.957]
Epoch [95/120    avg_loss:0.061, val_acc:0.956]
Epoch [96/120    avg_loss:0.030, val_acc:0.966]
Epoch [97/120    avg_loss:0.041, val_acc:0.940]
Epoch [98/120    avg_loss:0.042, val_acc:0.895]
Epoch [99/120    avg_loss:0.045, val_acc:0.963]
Epoch [100/120    avg_loss:0.016, val_acc:0.980]
Epoch [101/120    avg_loss:0.016, val_acc:0.975]
Epoch [102/120    avg_loss:0.012, val_acc:0.976]
Epoch [103/120    avg_loss:0.012, val_acc:0.977]
Epoch [104/120    avg_loss:0.021, val_acc:0.980]
Epoch [105/120    avg_loss:0.012, val_acc:0.982]
Epoch [106/120    avg_loss:0.009, val_acc:0.982]
Epoch [107/120    avg_loss:0.010, val_acc:0.983]
Epoch [108/120    avg_loss:0.012, val_acc:0.982]
Epoch [109/120    avg_loss:0.017, val_acc:0.982]
Epoch [110/120    avg_loss:0.009, val_acc:0.981]
Epoch [111/120    avg_loss:0.007, val_acc:0.981]
Epoch [112/120    avg_loss:0.014, val_acc:0.981]
Epoch [113/120    avg_loss:0.006, val_acc:0.982]
Epoch [114/120    avg_loss:0.006, val_acc:0.982]
Epoch [115/120    avg_loss:0.009, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.982]
Epoch [117/120    avg_loss:0.006, val_acc:0.982]
Epoch [118/120    avg_loss:0.010, val_acc:0.981]
Epoch [119/120    avg_loss:0.007, val_acc:0.983]
Epoch [120/120    avg_loss:0.011, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1269    3    0    0    0    0    0    0    6    1    0    0
     0    6    0]
 [   0    0    0  693   10   15    0    0    0    3    0    0   26    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   28   10    0    2    4    0    0    0  811   14    0    0
     1    5    0]
 [   0    0    9    0    0    2   12    0    0    0    3 2184    0    0
     0    0    0]
 [   0    0    0    0    8    3    0    0    0    0    4    2  510    0
     1    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   26    0    0    0    0    0    0    0
    31  290    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.31165311653116

F1 scores:
[       nan 1.         0.97954458 0.95126973 0.95475113 0.97303371
 0.96902655 0.98039216 1.         0.84210526 0.95467922 0.99025164
 0.95238095 1.         0.98572047 0.89506173 0.95953757]

Kappa:
0.9693511268631987
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:113
Validation dataloader:113
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0dbe309668>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.383, val_acc:0.470]
Epoch [2/120    avg_loss:1.711, val_acc:0.595]
Epoch [3/120    avg_loss:1.477, val_acc:0.617]
Epoch [4/120    avg_loss:1.270, val_acc:0.645]
Epoch [5/120    avg_loss:1.121, val_acc:0.701]
Epoch [6/120    avg_loss:1.012, val_acc:0.754]
Epoch [7/120    avg_loss:1.107, val_acc:0.635]
Epoch [8/120    avg_loss:0.923, val_acc:0.785]
Epoch [9/120    avg_loss:0.817, val_acc:0.811]
Epoch [10/120    avg_loss:0.909, val_acc:0.782]
Epoch [11/120    avg_loss:0.719, val_acc:0.810]
Epoch [12/120    avg_loss:0.639, val_acc:0.785]
Epoch [13/120    avg_loss:0.562, val_acc:0.825]
Epoch [14/120    avg_loss:0.521, val_acc:0.802]
Epoch [15/120    avg_loss:0.550, val_acc:0.804]
Epoch [16/120    avg_loss:0.417, val_acc:0.854]
Epoch [17/120    avg_loss:0.474, val_acc:0.872]
Epoch [18/120    avg_loss:0.356, val_acc:0.874]
Epoch [19/120    avg_loss:0.458, val_acc:0.843]
Epoch [20/120    avg_loss:0.450, val_acc:0.875]
Epoch [21/120    avg_loss:0.280, val_acc:0.898]
Epoch [22/120    avg_loss:0.324, val_acc:0.867]
Epoch [23/120    avg_loss:0.312, val_acc:0.871]
Epoch [24/120    avg_loss:0.248, val_acc:0.925]
Epoch [25/120    avg_loss:0.238, val_acc:0.900]
Epoch [26/120    avg_loss:0.226, val_acc:0.932]
Epoch [27/120    avg_loss:0.204, val_acc:0.917]
Epoch [28/120    avg_loss:0.190, val_acc:0.899]
Epoch [29/120    avg_loss:0.246, val_acc:0.918]
Epoch [30/120    avg_loss:0.180, val_acc:0.813]
Epoch [31/120    avg_loss:0.130, val_acc:0.923]
Epoch [32/120    avg_loss:0.178, val_acc:0.945]
Epoch [33/120    avg_loss:0.240, val_acc:0.936]
Epoch [34/120    avg_loss:0.095, val_acc:0.946]
Epoch [35/120    avg_loss:0.157, val_acc:0.933]
Epoch [36/120    avg_loss:0.165, val_acc:0.884]
Epoch [37/120    avg_loss:0.133, val_acc:0.959]
Epoch [38/120    avg_loss:0.133, val_acc:0.948]
Epoch [39/120    avg_loss:0.106, val_acc:0.942]
Epoch [40/120    avg_loss:0.051, val_acc:0.958]
Epoch [41/120    avg_loss:0.098, val_acc:0.957]
Epoch [42/120    avg_loss:0.165, val_acc:0.951]
Epoch [43/120    avg_loss:0.090, val_acc:0.960]
Epoch [44/120    avg_loss:0.064, val_acc:0.949]
Epoch [45/120    avg_loss:0.093, val_acc:0.914]
Epoch [46/120    avg_loss:0.193, val_acc:0.948]
Epoch [47/120    avg_loss:0.094, val_acc:0.874]
Epoch [48/120    avg_loss:0.063, val_acc:0.957]
Epoch [49/120    avg_loss:0.106, val_acc:0.954]
Epoch [50/120    avg_loss:0.109, val_acc:0.955]
Epoch [51/120    avg_loss:0.062, val_acc:0.962]
Epoch [52/120    avg_loss:0.050, val_acc:0.967]
Epoch [53/120    avg_loss:0.052, val_acc:0.972]
Epoch [54/120    avg_loss:0.055, val_acc:0.971]
Epoch [55/120    avg_loss:0.038, val_acc:0.965]
Epoch [56/120    avg_loss:0.019, val_acc:0.975]
Epoch [57/120    avg_loss:0.066, val_acc:0.917]
Epoch [58/120    avg_loss:0.089, val_acc:0.945]
Epoch [59/120    avg_loss:0.057, val_acc:0.952]
Epoch [60/120    avg_loss:0.088, val_acc:0.941]
Epoch [61/120    avg_loss:0.113, val_acc:0.942]
Epoch [62/120    avg_loss:0.052, val_acc:0.970]
Epoch [63/120    avg_loss:0.025, val_acc:0.963]
Epoch [64/120    avg_loss:0.031, val_acc:0.964]
Epoch [65/120    avg_loss:0.033, val_acc:0.974]
Epoch [66/120    avg_loss:0.054, val_acc:0.960]
Epoch [67/120    avg_loss:0.043, val_acc:0.971]
Epoch [68/120    avg_loss:0.026, val_acc:0.971]
Epoch [69/120    avg_loss:0.019, val_acc:0.975]
Epoch [70/120    avg_loss:0.020, val_acc:0.973]
Epoch [71/120    avg_loss:0.034, val_acc:0.982]
Epoch [72/120    avg_loss:0.029, val_acc:0.975]
Epoch [73/120    avg_loss:0.031, val_acc:0.977]
Epoch [74/120    avg_loss:0.027, val_acc:0.978]
Epoch [75/120    avg_loss:0.024, val_acc:0.978]
Epoch [76/120    avg_loss:0.032, val_acc:0.975]
Epoch [77/120    avg_loss:0.019, val_acc:0.979]
Epoch [78/120    avg_loss:0.057, val_acc:0.972]
Epoch [79/120    avg_loss:0.027, val_acc:0.966]
Epoch [80/120    avg_loss:0.023, val_acc:0.970]
Epoch [81/120    avg_loss:0.012, val_acc:0.982]
Epoch [82/120    avg_loss:0.016, val_acc:0.985]
Epoch [83/120    avg_loss:0.016, val_acc:0.982]
Epoch [84/120    avg_loss:0.026, val_acc:0.962]
Epoch [85/120    avg_loss:0.032, val_acc:0.963]
Epoch [86/120    avg_loss:0.043, val_acc:0.966]
Epoch [87/120    avg_loss:0.019, val_acc:0.975]
Epoch [88/120    avg_loss:0.014, val_acc:0.985]
Epoch [89/120    avg_loss:0.031, val_acc:0.954]
Epoch [90/120    avg_loss:0.034, val_acc:0.978]
Epoch [91/120    avg_loss:0.046, val_acc:0.974]
Epoch [92/120    avg_loss:0.042, val_acc:0.980]
Epoch [93/120    avg_loss:0.010, val_acc:0.986]
Epoch [94/120    avg_loss:0.008, val_acc:0.987]
Epoch [95/120    avg_loss:0.008, val_acc:0.982]
Epoch [96/120    avg_loss:0.012, val_acc:0.968]
Epoch [97/120    avg_loss:0.009, val_acc:0.984]
Epoch [98/120    avg_loss:0.013, val_acc:0.980]
Epoch [99/120    avg_loss:0.031, val_acc:0.972]
Epoch [100/120    avg_loss:0.029, val_acc:0.976]
Epoch [101/120    avg_loss:0.014, val_acc:0.983]
Epoch [102/120    avg_loss:0.012, val_acc:0.986]
Epoch [103/120    avg_loss:0.003, val_acc:0.987]
Epoch [104/120    avg_loss:0.016, val_acc:0.978]
Epoch [105/120    avg_loss:0.008, val_acc:0.987]
Epoch [106/120    avg_loss:0.010, val_acc:0.986]
Epoch [107/120    avg_loss:0.023, val_acc:0.982]
Epoch [108/120    avg_loss:0.020, val_acc:0.972]
Epoch [109/120    avg_loss:0.022, val_acc:0.977]
Epoch [110/120    avg_loss:0.005, val_acc:0.983]
Epoch [111/120    avg_loss:0.009, val_acc:0.984]
Epoch [112/120    avg_loss:0.015, val_acc:0.988]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.992]
Epoch [115/120    avg_loss:0.010, val_acc:0.987]
Epoch [116/120    avg_loss:0.018, val_acc:0.986]
Epoch [117/120    avg_loss:0.011, val_acc:0.986]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.989]
Epoch [120/120    avg_loss:0.005, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1275    5    0    1    0    0    0    0    3    1    0    0
     0    0    0]
 [   0    0    0  678    0   16    0    0    0    6    0    1   46    0
     0    0    0]
 [   0    0    0    5  208    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    1    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    2    0    0    0    0  859    9    0    0
     1    1    0]
 [   0    0   10    0    0    4    9    0    2    0    8 2177    0    0
     0    0    0]
 [   0    0    0    0    4    5    0    0    0    0    8   17  495    0
     2    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    1    0    1    0    0    0
  1132    1    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
    43  297    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.4959349593496

F1 scores:
[       nan 1.         0.99106102 0.94428969 0.97882353 0.96337403
 0.98796992 0.98039216 0.99652375 0.82926829 0.97947548 0.98618347
 0.92093023 1.         0.97712559 0.91950464 0.98245614]

Kappa:
0.9714475892378149
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f67c3b61668>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.399, val_acc:0.398]
Epoch [2/120    avg_loss:1.759, val_acc:0.531]
Epoch [3/120    avg_loss:1.422, val_acc:0.666]
Epoch [4/120    avg_loss:1.302, val_acc:0.608]
Epoch [5/120    avg_loss:1.324, val_acc:0.664]
Epoch [6/120    avg_loss:1.114, val_acc:0.657]
Epoch [7/120    avg_loss:1.028, val_acc:0.702]
Epoch [8/120    avg_loss:0.903, val_acc:0.759]
Epoch [9/120    avg_loss:0.704, val_acc:0.797]
Epoch [10/120    avg_loss:0.684, val_acc:0.728]
Epoch [11/120    avg_loss:0.718, val_acc:0.831]
Epoch [12/120    avg_loss:0.628, val_acc:0.743]
Epoch [13/120    avg_loss:0.470, val_acc:0.769]
Epoch [14/120    avg_loss:0.524, val_acc:0.827]
Epoch [15/120    avg_loss:0.580, val_acc:0.853]
Epoch [16/120    avg_loss:0.442, val_acc:0.827]
Epoch [17/120    avg_loss:0.375, val_acc:0.891]
Epoch [18/120    avg_loss:0.493, val_acc:0.858]
Epoch [19/120    avg_loss:0.456, val_acc:0.893]
Epoch [20/120    avg_loss:0.497, val_acc:0.884]
Epoch [21/120    avg_loss:0.307, val_acc:0.877]
Epoch [22/120    avg_loss:0.356, val_acc:0.882]
Epoch [23/120    avg_loss:0.223, val_acc:0.927]
Epoch [24/120    avg_loss:0.193, val_acc:0.922]
Epoch [25/120    avg_loss:0.193, val_acc:0.948]
Epoch [26/120    avg_loss:0.197, val_acc:0.939]
Epoch [27/120    avg_loss:0.171, val_acc:0.940]
Epoch [28/120    avg_loss:0.141, val_acc:0.952]
Epoch [29/120    avg_loss:0.189, val_acc:0.934]
Epoch [30/120    avg_loss:0.157, val_acc:0.942]
Epoch [31/120    avg_loss:0.222, val_acc:0.941]
Epoch [32/120    avg_loss:0.226, val_acc:0.912]
Epoch [33/120    avg_loss:0.287, val_acc:0.940]
Epoch [34/120    avg_loss:0.276, val_acc:0.897]
Epoch [35/120    avg_loss:0.205, val_acc:0.945]
Epoch [36/120    avg_loss:0.125, val_acc:0.941]
Epoch [37/120    avg_loss:0.124, val_acc:0.953]
Epoch [38/120    avg_loss:0.119, val_acc:0.944]
Epoch [39/120    avg_loss:0.119, val_acc:0.967]
Epoch [40/120    avg_loss:0.071, val_acc:0.967]
Epoch [41/120    avg_loss:0.073, val_acc:0.956]
Epoch [42/120    avg_loss:0.069, val_acc:0.960]
Epoch [43/120    avg_loss:0.055, val_acc:0.942]
Epoch [44/120    avg_loss:0.076, val_acc:0.975]
Epoch [45/120    avg_loss:0.077, val_acc:0.950]
Epoch [46/120    avg_loss:0.071, val_acc:0.953]
Epoch [47/120    avg_loss:0.061, val_acc:0.969]
Epoch [48/120    avg_loss:0.048, val_acc:0.970]
Epoch [49/120    avg_loss:0.048, val_acc:0.969]
Epoch [50/120    avg_loss:0.035, val_acc:0.978]
Epoch [51/120    avg_loss:0.066, val_acc:0.966]
Epoch [52/120    avg_loss:0.063, val_acc:0.972]
Epoch [53/120    avg_loss:0.051, val_acc:0.949]
Epoch [54/120    avg_loss:0.045, val_acc:0.981]
Epoch [55/120    avg_loss:0.025, val_acc:0.983]
Epoch [56/120    avg_loss:0.026, val_acc:0.980]
Epoch [57/120    avg_loss:0.027, val_acc:0.972]
Epoch [58/120    avg_loss:0.018, val_acc:0.981]
Epoch [59/120    avg_loss:0.026, val_acc:0.983]
Epoch [60/120    avg_loss:0.017, val_acc:0.990]
Epoch [61/120    avg_loss:0.026, val_acc:0.974]
Epoch [62/120    avg_loss:0.034, val_acc:0.984]
Epoch [63/120    avg_loss:0.028, val_acc:0.968]
Epoch [64/120    avg_loss:0.018, val_acc:0.986]
Epoch [65/120    avg_loss:0.022, val_acc:0.982]
Epoch [66/120    avg_loss:0.022, val_acc:0.982]
Epoch [67/120    avg_loss:0.027, val_acc:0.986]
Epoch [68/120    avg_loss:0.052, val_acc:0.970]
Epoch [69/120    avg_loss:0.054, val_acc:0.976]
Epoch [70/120    avg_loss:0.021, val_acc:0.974]
Epoch [71/120    avg_loss:0.015, val_acc:0.981]
Epoch [72/120    avg_loss:0.009, val_acc:0.989]
Epoch [73/120    avg_loss:0.016, val_acc:0.988]
Epoch [74/120    avg_loss:0.011, val_acc:0.988]
Epoch [75/120    avg_loss:0.013, val_acc:0.989]
Epoch [76/120    avg_loss:0.020, val_acc:0.991]
Epoch [77/120    avg_loss:0.010, val_acc:0.992]
Epoch [78/120    avg_loss:0.010, val_acc:0.990]
Epoch [79/120    avg_loss:0.011, val_acc:0.992]
Epoch [80/120    avg_loss:0.010, val_acc:0.990]
Epoch [81/120    avg_loss:0.010, val_acc:0.992]
Epoch [82/120    avg_loss:0.007, val_acc:0.991]
Epoch [83/120    avg_loss:0.007, val_acc:0.991]
Epoch [84/120    avg_loss:0.008, val_acc:0.992]
Epoch [85/120    avg_loss:0.011, val_acc:0.991]
Epoch [86/120    avg_loss:0.007, val_acc:0.991]
Epoch [87/120    avg_loss:0.005, val_acc:0.991]
Epoch [88/120    avg_loss:0.006, val_acc:0.991]
Epoch [89/120    avg_loss:0.007, val_acc:0.991]
Epoch [90/120    avg_loss:0.005, val_acc:0.990]
Epoch [91/120    avg_loss:0.012, val_acc:0.990]
Epoch [92/120    avg_loss:0.009, val_acc:0.990]
Epoch [93/120    avg_loss:0.006, val_acc:0.990]
Epoch [94/120    avg_loss:0.007, val_acc:0.989]
Epoch [95/120    avg_loss:0.007, val_acc:0.990]
Epoch [96/120    avg_loss:0.008, val_acc:0.990]
Epoch [97/120    avg_loss:0.008, val_acc:0.990]
Epoch [98/120    avg_loss:0.006, val_acc:0.990]
Epoch [99/120    avg_loss:0.008, val_acc:0.990]
Epoch [100/120    avg_loss:0.010, val_acc:0.990]
Epoch [101/120    avg_loss:0.005, val_acc:0.990]
Epoch [102/120    avg_loss:0.007, val_acc:0.990]
Epoch [103/120    avg_loss:0.011, val_acc:0.990]
Epoch [104/120    avg_loss:0.010, val_acc:0.990]
Epoch [105/120    avg_loss:0.003, val_acc:0.990]
Epoch [106/120    avg_loss:0.014, val_acc:0.990]
Epoch [107/120    avg_loss:0.009, val_acc:0.990]
Epoch [108/120    avg_loss:0.006, val_acc:0.990]
Epoch [109/120    avg_loss:0.004, val_acc:0.990]
Epoch [110/120    avg_loss:0.013, val_acc:0.990]
Epoch [111/120    avg_loss:0.011, val_acc:0.990]
Epoch [112/120    avg_loss:0.007, val_acc:0.990]
Epoch [113/120    avg_loss:0.007, val_acc:0.990]
Epoch [114/120    avg_loss:0.006, val_acc:0.990]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.007, val_acc:0.990]
Epoch [117/120    avg_loss:0.005, val_acc:0.990]
Epoch [118/120    avg_loss:0.009, val_acc:0.990]
Epoch [119/120    avg_loss:0.008, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    3    0    0    0    0    0    0   10    1    3    0
     0    1    0]
 [   0    0    0  731    0    8    0    0    0    6    2    0    0    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   12   60    0    0    1    0    0    0  782   20    0    0
     0    0    0]
 [   0    0   12    0    3    0    6    0    0    0    2 2185    1    1
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0   14    0  513    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    0    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0   31    0    0    0    0    0    0    0
    39  277    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.23577235772358

F1 scores:
[       nan 1.         0.98369565 0.94627832 0.99065421 0.98065984
 0.97189349 0.98039216 1.         0.71428571 0.92818991 0.98958333
 0.97621313 0.99730458 0.98140942 0.8864     0.96551724]

Kappa:
0.9684780885480057
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9a05bf5668>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.205, val_acc:0.479]
Epoch [2/120    avg_loss:1.819, val_acc:0.571]
Epoch [3/120    avg_loss:1.439, val_acc:0.535]
Epoch [4/120    avg_loss:1.413, val_acc:0.532]
Epoch [5/120    avg_loss:1.161, val_acc:0.682]
Epoch [6/120    avg_loss:1.080, val_acc:0.756]
Epoch [7/120    avg_loss:0.879, val_acc:0.718]
Epoch [8/120    avg_loss:0.903, val_acc:0.738]
Epoch [9/120    avg_loss:0.712, val_acc:0.740]
Epoch [10/120    avg_loss:0.630, val_acc:0.800]
Epoch [11/120    avg_loss:0.655, val_acc:0.838]
Epoch [12/120    avg_loss:0.602, val_acc:0.797]
Epoch [13/120    avg_loss:0.621, val_acc:0.792]
Epoch [14/120    avg_loss:0.505, val_acc:0.731]
Epoch [15/120    avg_loss:0.607, val_acc:0.823]
Epoch [16/120    avg_loss:0.423, val_acc:0.857]
Epoch [17/120    avg_loss:0.328, val_acc:0.849]
Epoch [18/120    avg_loss:0.327, val_acc:0.896]
Epoch [19/120    avg_loss:0.357, val_acc:0.827]
Epoch [20/120    avg_loss:0.399, val_acc:0.799]
Epoch [21/120    avg_loss:0.336, val_acc:0.859]
Epoch [22/120    avg_loss:0.258, val_acc:0.890]
Epoch [23/120    avg_loss:0.216, val_acc:0.877]
Epoch [24/120    avg_loss:0.185, val_acc:0.862]
Epoch [25/120    avg_loss:0.214, val_acc:0.873]
Epoch [26/120    avg_loss:0.159, val_acc:0.903]
Epoch [27/120    avg_loss:0.181, val_acc:0.883]
Epoch [28/120    avg_loss:0.142, val_acc:0.888]
Epoch [29/120    avg_loss:0.287, val_acc:0.890]
Epoch [30/120    avg_loss:0.171, val_acc:0.942]
Epoch [31/120    avg_loss:0.274, val_acc:0.908]
Epoch [32/120    avg_loss:0.170, val_acc:0.935]
Epoch [33/120    avg_loss:0.270, val_acc:0.925]
Epoch [34/120    avg_loss:0.188, val_acc:0.902]
Epoch [35/120    avg_loss:0.131, val_acc:0.939]
Epoch [36/120    avg_loss:0.144, val_acc:0.923]
Epoch [37/120    avg_loss:0.099, val_acc:0.940]
Epoch [38/120    avg_loss:0.160, val_acc:0.942]
Epoch [39/120    avg_loss:0.092, val_acc:0.932]
Epoch [40/120    avg_loss:0.087, val_acc:0.945]
Epoch [41/120    avg_loss:0.125, val_acc:0.911]
Epoch [42/120    avg_loss:0.148, val_acc:0.934]
Epoch [43/120    avg_loss:0.152, val_acc:0.922]
Epoch [44/120    avg_loss:0.217, val_acc:0.891]
Epoch [45/120    avg_loss:0.138, val_acc:0.938]
Epoch [46/120    avg_loss:0.116, val_acc:0.965]
Epoch [47/120    avg_loss:0.090, val_acc:0.941]
Epoch [48/120    avg_loss:0.110, val_acc:0.945]
Epoch [49/120    avg_loss:0.072, val_acc:0.961]
Epoch [50/120    avg_loss:0.112, val_acc:0.948]
Epoch [51/120    avg_loss:0.058, val_acc:0.954]
Epoch [52/120    avg_loss:0.088, val_acc:0.968]
Epoch [53/120    avg_loss:0.049, val_acc:0.975]
Epoch [54/120    avg_loss:0.047, val_acc:0.959]
Epoch [55/120    avg_loss:0.042, val_acc:0.954]
Epoch [56/120    avg_loss:0.032, val_acc:0.979]
Epoch [57/120    avg_loss:0.057, val_acc:0.967]
Epoch [58/120    avg_loss:0.032, val_acc:0.961]
Epoch [59/120    avg_loss:0.049, val_acc:0.972]
Epoch [60/120    avg_loss:0.047, val_acc:0.973]
Epoch [61/120    avg_loss:0.039, val_acc:0.979]
Epoch [62/120    avg_loss:0.051, val_acc:0.975]
Epoch [63/120    avg_loss:0.023, val_acc:0.968]
Epoch [64/120    avg_loss:0.023, val_acc:0.980]
Epoch [65/120    avg_loss:0.037, val_acc:0.980]
Epoch [66/120    avg_loss:0.028, val_acc:0.980]
Epoch [67/120    avg_loss:0.024, val_acc:0.977]
Epoch [68/120    avg_loss:0.066, val_acc:0.970]
Epoch [69/120    avg_loss:0.022, val_acc:0.979]
Epoch [70/120    avg_loss:0.037, val_acc:0.982]
Epoch [71/120    avg_loss:0.026, val_acc:0.965]
Epoch [72/120    avg_loss:0.025, val_acc:0.970]
Epoch [73/120    avg_loss:0.036, val_acc:0.974]
Epoch [74/120    avg_loss:0.029, val_acc:0.977]
Epoch [75/120    avg_loss:0.016, val_acc:0.978]
Epoch [76/120    avg_loss:0.100, val_acc:0.965]
Epoch [77/120    avg_loss:0.023, val_acc:0.968]
Epoch [78/120    avg_loss:0.029, val_acc:0.973]
Epoch [79/120    avg_loss:0.061, val_acc:0.968]
Epoch [80/120    avg_loss:0.018, val_acc:0.974]
Epoch [81/120    avg_loss:0.025, val_acc:0.958]
Epoch [82/120    avg_loss:0.066, val_acc:0.954]
Epoch [83/120    avg_loss:0.030, val_acc:0.969]
Epoch [84/120    avg_loss:0.028, val_acc:0.977]
Epoch [85/120    avg_loss:0.014, val_acc:0.978]
Epoch [86/120    avg_loss:0.012, val_acc:0.982]
Epoch [87/120    avg_loss:0.014, val_acc:0.982]
Epoch [88/120    avg_loss:0.028, val_acc:0.979]
Epoch [89/120    avg_loss:0.008, val_acc:0.980]
Epoch [90/120    avg_loss:0.012, val_acc:0.982]
Epoch [91/120    avg_loss:0.009, val_acc:0.983]
Epoch [92/120    avg_loss:0.011, val_acc:0.985]
Epoch [93/120    avg_loss:0.011, val_acc:0.983]
Epoch [94/120    avg_loss:0.014, val_acc:0.980]
Epoch [95/120    avg_loss:0.009, val_acc:0.982]
Epoch [96/120    avg_loss:0.009, val_acc:0.982]
Epoch [97/120    avg_loss:0.011, val_acc:0.982]
Epoch [98/120    avg_loss:0.014, val_acc:0.983]
Epoch [99/120    avg_loss:0.012, val_acc:0.982]
Epoch [100/120    avg_loss:0.008, val_acc:0.982]
Epoch [101/120    avg_loss:0.009, val_acc:0.983]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.007, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.984]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.008, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.984]
Epoch [109/120    avg_loss:0.010, val_acc:0.984]
Epoch [110/120    avg_loss:0.009, val_acc:0.984]
Epoch [111/120    avg_loss:0.013, val_acc:0.984]
Epoch [112/120    avg_loss:0.012, val_acc:0.984]
Epoch [113/120    avg_loss:0.011, val_acc:0.984]
Epoch [114/120    avg_loss:0.009, val_acc:0.984]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.008, val_acc:0.984]
Epoch [117/120    avg_loss:0.007, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.007, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    1 1272    7    0    0    0    0    0    0    4    1    0    0
     0    0    0]
 [   0    0    0  676    0    7    0    0    0    6    4   47    7    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    0   37    0    0    0    0    0    0  782   55    1    0
     0    0    0]
 [   0    0   12    0    0    0    9    0    0    0   11 2178    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0   11    4  512    0
     3    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   21    0    0    0    0    0    0    0
    33  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.84552845528455

F1 scores:
[       nan 0.98795181 0.99026859 0.91972789 1.         0.98971429
 0.97767857 0.98039216 0.99883856 0.75       0.92708951 0.96907675
 0.971537   1.         0.98400346 0.915625   0.97674419]

Kappa:
0.963974454624259
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f10b82046d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.269, val_acc:0.400]
Epoch [2/120    avg_loss:1.976, val_acc:0.447]
Epoch [3/120    avg_loss:1.546, val_acc:0.614]
Epoch [4/120    avg_loss:1.365, val_acc:0.668]
Epoch [5/120    avg_loss:1.280, val_acc:0.636]
Epoch [6/120    avg_loss:0.964, val_acc:0.716]
Epoch [7/120    avg_loss:1.031, val_acc:0.729]
Epoch [8/120    avg_loss:0.797, val_acc:0.710]
Epoch [9/120    avg_loss:0.803, val_acc:0.765]
Epoch [10/120    avg_loss:0.715, val_acc:0.814]
Epoch [11/120    avg_loss:0.584, val_acc:0.757]
Epoch [12/120    avg_loss:0.526, val_acc:0.753]
Epoch [13/120    avg_loss:0.554, val_acc:0.844]
Epoch [14/120    avg_loss:0.498, val_acc:0.805]
Epoch [15/120    avg_loss:0.470, val_acc:0.866]
Epoch [16/120    avg_loss:0.489, val_acc:0.834]
Epoch [17/120    avg_loss:0.415, val_acc:0.840]
Epoch [18/120    avg_loss:0.313, val_acc:0.899]
Epoch [19/120    avg_loss:0.308, val_acc:0.824]
Epoch [20/120    avg_loss:0.325, val_acc:0.899]
Epoch [21/120    avg_loss:0.348, val_acc:0.903]
Epoch [22/120    avg_loss:0.236, val_acc:0.900]
Epoch [23/120    avg_loss:0.156, val_acc:0.938]
Epoch [24/120    avg_loss:0.239, val_acc:0.833]
Epoch [25/120    avg_loss:0.327, val_acc:0.887]
Epoch [26/120    avg_loss:0.198, val_acc:0.934]
Epoch [27/120    avg_loss:0.180, val_acc:0.935]
Epoch [28/120    avg_loss:0.172, val_acc:0.940]
Epoch [29/120    avg_loss:0.136, val_acc:0.937]
Epoch [30/120    avg_loss:0.100, val_acc:0.946]
Epoch [31/120    avg_loss:0.143, val_acc:0.909]
Epoch [32/120    avg_loss:0.199, val_acc:0.924]
Epoch [33/120    avg_loss:0.133, val_acc:0.950]
Epoch [34/120    avg_loss:0.116, val_acc:0.910]
Epoch [35/120    avg_loss:0.077, val_acc:0.954]
Epoch [36/120    avg_loss:0.109, val_acc:0.949]
Epoch [37/120    avg_loss:0.128, val_acc:0.967]
Epoch [38/120    avg_loss:0.066, val_acc:0.962]
Epoch [39/120    avg_loss:0.096, val_acc:0.939]
Epoch [40/120    avg_loss:0.090, val_acc:0.957]
Epoch [41/120    avg_loss:0.098, val_acc:0.941]
Epoch [42/120    avg_loss:0.051, val_acc:0.975]
Epoch [43/120    avg_loss:0.068, val_acc:0.946]
Epoch [44/120    avg_loss:0.087, val_acc:0.936]
Epoch [45/120    avg_loss:0.065, val_acc:0.950]
Epoch [46/120    avg_loss:0.073, val_acc:0.967]
Epoch [47/120    avg_loss:0.062, val_acc:0.946]
Epoch [48/120    avg_loss:0.093, val_acc:0.968]
Epoch [49/120    avg_loss:0.052, val_acc:0.968]
Epoch [50/120    avg_loss:0.078, val_acc:0.930]
Epoch [51/120    avg_loss:0.079, val_acc:0.950]
Epoch [52/120    avg_loss:0.077, val_acc:0.958]
Epoch [53/120    avg_loss:0.031, val_acc:0.972]
Epoch [54/120    avg_loss:0.026, val_acc:0.966]
Epoch [55/120    avg_loss:0.030, val_acc:0.972]
Epoch [56/120    avg_loss:0.026, val_acc:0.979]
Epoch [57/120    avg_loss:0.021, val_acc:0.979]
Epoch [58/120    avg_loss:0.024, val_acc:0.980]
Epoch [59/120    avg_loss:0.026, val_acc:0.981]
Epoch [60/120    avg_loss:0.021, val_acc:0.979]
Epoch [61/120    avg_loss:0.015, val_acc:0.980]
Epoch [62/120    avg_loss:0.016, val_acc:0.980]
Epoch [63/120    avg_loss:0.015, val_acc:0.980]
Epoch [64/120    avg_loss:0.014, val_acc:0.982]
Epoch [65/120    avg_loss:0.028, val_acc:0.980]
Epoch [66/120    avg_loss:0.018, val_acc:0.980]
Epoch [67/120    avg_loss:0.017, val_acc:0.980]
Epoch [68/120    avg_loss:0.013, val_acc:0.980]
Epoch [69/120    avg_loss:0.013, val_acc:0.981]
Epoch [70/120    avg_loss:0.015, val_acc:0.981]
Epoch [71/120    avg_loss:0.018, val_acc:0.981]
Epoch [72/120    avg_loss:0.014, val_acc:0.980]
Epoch [73/120    avg_loss:0.010, val_acc:0.982]
Epoch [74/120    avg_loss:0.015, val_acc:0.981]
Epoch [75/120    avg_loss:0.015, val_acc:0.982]
Epoch [76/120    avg_loss:0.010, val_acc:0.981]
Epoch [77/120    avg_loss:0.017, val_acc:0.983]
Epoch [78/120    avg_loss:0.011, val_acc:0.985]
Epoch [79/120    avg_loss:0.011, val_acc:0.986]
Epoch [80/120    avg_loss:0.017, val_acc:0.986]
Epoch [81/120    avg_loss:0.011, val_acc:0.985]
Epoch [82/120    avg_loss:0.012, val_acc:0.983]
Epoch [83/120    avg_loss:0.024, val_acc:0.985]
Epoch [84/120    avg_loss:0.011, val_acc:0.983]
Epoch [85/120    avg_loss:0.012, val_acc:0.983]
Epoch [86/120    avg_loss:0.018, val_acc:0.981]
Epoch [87/120    avg_loss:0.016, val_acc:0.983]
Epoch [88/120    avg_loss:0.019, val_acc:0.985]
Epoch [89/120    avg_loss:0.016, val_acc:0.985]
Epoch [90/120    avg_loss:0.008, val_acc:0.983]
Epoch [91/120    avg_loss:0.010, val_acc:0.985]
Epoch [92/120    avg_loss:0.015, val_acc:0.983]
Epoch [93/120    avg_loss:0.012, val_acc:0.983]
Epoch [94/120    avg_loss:0.011, val_acc:0.983]
Epoch [95/120    avg_loss:0.010, val_acc:0.983]
Epoch [96/120    avg_loss:0.015, val_acc:0.983]
Epoch [97/120    avg_loss:0.007, val_acc:0.983]
Epoch [98/120    avg_loss:0.009, val_acc:0.983]
Epoch [99/120    avg_loss:0.014, val_acc:0.983]
Epoch [100/120    avg_loss:0.014, val_acc:0.983]
Epoch [101/120    avg_loss:0.012, val_acc:0.983]
Epoch [102/120    avg_loss:0.010, val_acc:0.983]
Epoch [103/120    avg_loss:0.016, val_acc:0.983]
Epoch [104/120    avg_loss:0.008, val_acc:0.983]
Epoch [105/120    avg_loss:0.008, val_acc:0.983]
Epoch [106/120    avg_loss:0.012, val_acc:0.983]
Epoch [107/120    avg_loss:0.012, val_acc:0.983]
Epoch [108/120    avg_loss:0.010, val_acc:0.983]
Epoch [109/120    avg_loss:0.012, val_acc:0.983]
Epoch [110/120    avg_loss:0.009, val_acc:0.983]
Epoch [111/120    avg_loss:0.015, val_acc:0.983]
Epoch [112/120    avg_loss:0.015, val_acc:0.983]
Epoch [113/120    avg_loss:0.013, val_acc:0.983]
Epoch [114/120    avg_loss:0.011, val_acc:0.983]
Epoch [115/120    avg_loss:0.014, val_acc:0.983]
Epoch [116/120    avg_loss:0.009, val_acc:0.983]
Epoch [117/120    avg_loss:0.015, val_acc:0.983]
Epoch [118/120    avg_loss:0.015, val_acc:0.983]
Epoch [119/120    avg_loss:0.011, val_acc:0.983]
Epoch [120/120    avg_loss:0.011, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    3    0    0    5    0    0    0   10    2    2    0
     0    2    0]
 [   0    0    0  727    0    8    0    0    0    5    0    0    7    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   14    0    0    1    0
     0    0    0]
 [   0    0    8   46    0    0   14    0    0    0  779   25    1    0
     2    0    0]
 [   0    0    8    0    0    0   11    0    0    0    8 2182    0    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    6    0  525    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    28  309    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.55013550135502

F1 scores:
[       nan 1.         0.9843872  0.95281782 1.         0.98742857
 0.9704579  1.         0.99883856 0.7        0.92848629 0.98755375
 0.97856477 0.99730458 0.98656264 0.93920973 0.96428571]

Kappa:
0.9720690340781095
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4a4202d668>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.239, val_acc:0.500]
Epoch [2/120    avg_loss:1.880, val_acc:0.547]
Epoch [3/120    avg_loss:1.510, val_acc:0.471]
Epoch [4/120    avg_loss:1.356, val_acc:0.678]
Epoch [5/120    avg_loss:1.169, val_acc:0.670]
Epoch [6/120    avg_loss:1.105, val_acc:0.524]
Epoch [7/120    avg_loss:0.944, val_acc:0.791]
Epoch [8/120    avg_loss:0.846, val_acc:0.726]
Epoch [9/120    avg_loss:0.780, val_acc:0.756]
Epoch [10/120    avg_loss:0.683, val_acc:0.779]
Epoch [11/120    avg_loss:0.627, val_acc:0.773]
Epoch [12/120    avg_loss:0.648, val_acc:0.718]
Epoch [13/120    avg_loss:0.617, val_acc:0.837]
Epoch [14/120    avg_loss:0.602, val_acc:0.778]
Epoch [15/120    avg_loss:0.500, val_acc:0.829]
Epoch [16/120    avg_loss:0.392, val_acc:0.893]
Epoch [17/120    avg_loss:0.403, val_acc:0.843]
Epoch [18/120    avg_loss:0.362, val_acc:0.894]
Epoch [19/120    avg_loss:0.278, val_acc:0.852]
Epoch [20/120    avg_loss:0.277, val_acc:0.882]
Epoch [21/120    avg_loss:0.198, val_acc:0.916]
Epoch [22/120    avg_loss:0.342, val_acc:0.875]
Epoch [23/120    avg_loss:0.324, val_acc:0.895]
Epoch [24/120    avg_loss:0.384, val_acc:0.854]
Epoch [25/120    avg_loss:0.289, val_acc:0.878]
Epoch [26/120    avg_loss:0.214, val_acc:0.921]
Epoch [27/120    avg_loss:0.199, val_acc:0.922]
Epoch [28/120    avg_loss:0.240, val_acc:0.913]
Epoch [29/120    avg_loss:0.173, val_acc:0.938]
Epoch [30/120    avg_loss:0.138, val_acc:0.945]
Epoch [31/120    avg_loss:0.144, val_acc:0.899]
Epoch [32/120    avg_loss:0.204, val_acc:0.943]
Epoch [33/120    avg_loss:0.135, val_acc:0.939]
Epoch [34/120    avg_loss:0.111, val_acc:0.940]
Epoch [35/120    avg_loss:0.107, val_acc:0.948]
Epoch [36/120    avg_loss:0.087, val_acc:0.944]
Epoch [37/120    avg_loss:0.082, val_acc:0.925]
Epoch [38/120    avg_loss:0.125, val_acc:0.895]
Epoch [39/120    avg_loss:0.122, val_acc:0.948]
Epoch [40/120    avg_loss:0.094, val_acc:0.952]
Epoch [41/120    avg_loss:0.055, val_acc:0.971]
Epoch [42/120    avg_loss:0.079, val_acc:0.935]
Epoch [43/120    avg_loss:0.073, val_acc:0.952]
Epoch [44/120    avg_loss:0.119, val_acc:0.959]
Epoch [45/120    avg_loss:0.089, val_acc:0.945]
Epoch [46/120    avg_loss:0.106, val_acc:0.956]
Epoch [47/120    avg_loss:0.054, val_acc:0.945]
Epoch [48/120    avg_loss:0.069, val_acc:0.965]
Epoch [49/120    avg_loss:0.065, val_acc:0.965]
Epoch [50/120    avg_loss:0.041, val_acc:0.975]
Epoch [51/120    avg_loss:0.030, val_acc:0.970]
Epoch [52/120    avg_loss:0.024, val_acc:0.964]
Epoch [53/120    avg_loss:0.028, val_acc:0.964]
Epoch [54/120    avg_loss:0.029, val_acc:0.963]
Epoch [55/120    avg_loss:0.019, val_acc:0.975]
Epoch [56/120    avg_loss:0.019, val_acc:0.975]
Epoch [57/120    avg_loss:0.026, val_acc:0.966]
Epoch [58/120    avg_loss:0.029, val_acc:0.970]
Epoch [59/120    avg_loss:0.034, val_acc:0.974]
Epoch [60/120    avg_loss:0.024, val_acc:0.968]
Epoch [61/120    avg_loss:0.030, val_acc:0.966]
Epoch [62/120    avg_loss:0.019, val_acc:0.961]
Epoch [63/120    avg_loss:0.033, val_acc:0.972]
Epoch [64/120    avg_loss:0.028, val_acc:0.971]
Epoch [65/120    avg_loss:0.058, val_acc:0.958]
Epoch [66/120    avg_loss:0.037, val_acc:0.970]
Epoch [67/120    avg_loss:0.036, val_acc:0.962]
Epoch [68/120    avg_loss:0.043, val_acc:0.925]
Epoch [69/120    avg_loss:0.027, val_acc:0.971]
Epoch [70/120    avg_loss:0.021, val_acc:0.976]
Epoch [71/120    avg_loss:0.020, val_acc:0.974]
Epoch [72/120    avg_loss:0.030, val_acc:0.977]
Epoch [73/120    avg_loss:0.018, val_acc:0.977]
Epoch [74/120    avg_loss:0.023, val_acc:0.979]
Epoch [75/120    avg_loss:0.011, val_acc:0.977]
Epoch [76/120    avg_loss:0.013, val_acc:0.975]
Epoch [77/120    avg_loss:0.011, val_acc:0.977]
Epoch [78/120    avg_loss:0.011, val_acc:0.979]
Epoch [79/120    avg_loss:0.010, val_acc:0.977]
Epoch [80/120    avg_loss:0.010, val_acc:0.977]
Epoch [81/120    avg_loss:0.013, val_acc:0.976]
Epoch [82/120    avg_loss:0.011, val_acc:0.979]
Epoch [83/120    avg_loss:0.009, val_acc:0.979]
Epoch [84/120    avg_loss:0.014, val_acc:0.977]
Epoch [85/120    avg_loss:0.011, val_acc:0.981]
Epoch [86/120    avg_loss:0.013, val_acc:0.981]
Epoch [87/120    avg_loss:0.018, val_acc:0.981]
Epoch [88/120    avg_loss:0.013, val_acc:0.980]
Epoch [89/120    avg_loss:0.010, val_acc:0.979]
Epoch [90/120    avg_loss:0.008, val_acc:0.980]
Epoch [91/120    avg_loss:0.009, val_acc:0.981]
Epoch [92/120    avg_loss:0.006, val_acc:0.981]
Epoch [93/120    avg_loss:0.007, val_acc:0.981]
Epoch [94/120    avg_loss:0.008, val_acc:0.981]
Epoch [95/120    avg_loss:0.008, val_acc:0.980]
Epoch [96/120    avg_loss:0.007, val_acc:0.981]
Epoch [97/120    avg_loss:0.009, val_acc:0.981]
Epoch [98/120    avg_loss:0.015, val_acc:0.980]
Epoch [99/120    avg_loss:0.010, val_acc:0.980]
Epoch [100/120    avg_loss:0.009, val_acc:0.980]
Epoch [101/120    avg_loss:0.012, val_acc:0.981]
Epoch [102/120    avg_loss:0.010, val_acc:0.980]
Epoch [103/120    avg_loss:0.009, val_acc:0.981]
Epoch [104/120    avg_loss:0.008, val_acc:0.981]
Epoch [105/120    avg_loss:0.029, val_acc:0.980]
Epoch [106/120    avg_loss:0.011, val_acc:0.980]
Epoch [107/120    avg_loss:0.007, val_acc:0.981]
Epoch [108/120    avg_loss:0.006, val_acc:0.980]
Epoch [109/120    avg_loss:0.006, val_acc:0.979]
Epoch [110/120    avg_loss:0.009, val_acc:0.983]
Epoch [111/120    avg_loss:0.007, val_acc:0.984]
Epoch [112/120    avg_loss:0.006, val_acc:0.984]
Epoch [113/120    avg_loss:0.010, val_acc:0.981]
Epoch [114/120    avg_loss:0.011, val_acc:0.982]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.982]
Epoch [117/120    avg_loss:0.006, val_acc:0.982]
Epoch [118/120    avg_loss:0.009, val_acc:0.983]
Epoch [119/120    avg_loss:0.008, val_acc:0.984]
Epoch [120/120    avg_loss:0.009, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1265    5    0    0    0    0    0    0    6    2    2    0
     0    5    0]
 [   0    0    0  667    0   22    0    0    0    3    0    0   55    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0   13    0    0    0    0
     0    0    0]
 [   0    0    7   69    0    0    0    0    0    0  777   20    1    0
     1    0    0]
 [   0    0   13    0    0    6   12    0    0    0    0 2179    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0   11    0  514    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   24    0    0    0    0    0    2    0
    70  251    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.17344173441734

F1 scores:
[       nan 1.         0.9844358  0.89290495 0.99764706 0.96444444
 0.97333333 1.         1.         0.74285714 0.93053892 0.98798458
 0.92779783 1.         0.9693356  0.83250415 0.96551724]

Kappa:
0.9563640583890619
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:07:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff06742c6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.204, val_acc:0.532]
Epoch [2/120    avg_loss:1.854, val_acc:0.621]
Epoch [3/120    avg_loss:1.474, val_acc:0.610]
Epoch [4/120    avg_loss:1.282, val_acc:0.718]
Epoch [5/120    avg_loss:0.987, val_acc:0.758]
Epoch [6/120    avg_loss:0.975, val_acc:0.766]
Epoch [7/120    avg_loss:0.830, val_acc:0.720]
Epoch [8/120    avg_loss:0.907, val_acc:0.772]
Epoch [9/120    avg_loss:0.707, val_acc:0.814]
Epoch [10/120    avg_loss:0.746, val_acc:0.739]
Epoch [11/120    avg_loss:0.656, val_acc:0.836]
Epoch [12/120    avg_loss:0.549, val_acc:0.847]
Epoch [13/120    avg_loss:0.615, val_acc:0.789]
Epoch [14/120    avg_loss:0.653, val_acc:0.868]
Epoch [15/120    avg_loss:0.537, val_acc:0.852]
Epoch [16/120    avg_loss:0.430, val_acc:0.855]
Epoch [17/120    avg_loss:0.451, val_acc:0.881]
Epoch [18/120    avg_loss:0.435, val_acc:0.892]
Epoch [19/120    avg_loss:0.328, val_acc:0.857]
Epoch [20/120    avg_loss:0.381, val_acc:0.868]
Epoch [21/120    avg_loss:0.310, val_acc:0.917]
Epoch [22/120    avg_loss:0.237, val_acc:0.915]
Epoch [23/120    avg_loss:0.235, val_acc:0.908]
Epoch [24/120    avg_loss:0.229, val_acc:0.905]
Epoch [25/120    avg_loss:0.219, val_acc:0.955]
Epoch [26/120    avg_loss:0.211, val_acc:0.916]
Epoch [27/120    avg_loss:0.160, val_acc:0.924]
Epoch [28/120    avg_loss:0.168, val_acc:0.942]
Epoch [29/120    avg_loss:0.082, val_acc:0.943]
Epoch [30/120    avg_loss:0.162, val_acc:0.916]
Epoch [31/120    avg_loss:0.195, val_acc:0.939]
Epoch [32/120    avg_loss:0.143, val_acc:0.959]
Epoch [33/120    avg_loss:0.197, val_acc:0.945]
Epoch [34/120    avg_loss:0.142, val_acc:0.952]
Epoch [35/120    avg_loss:0.187, val_acc:0.910]
Epoch [36/120    avg_loss:0.156, val_acc:0.949]
Epoch [37/120    avg_loss:0.120, val_acc:0.949]
Epoch [38/120    avg_loss:0.083, val_acc:0.953]
Epoch [39/120    avg_loss:0.095, val_acc:0.971]
Epoch [40/120    avg_loss:0.086, val_acc:0.938]
Epoch [41/120    avg_loss:0.114, val_acc:0.949]
Epoch [42/120    avg_loss:0.129, val_acc:0.967]
Epoch [43/120    avg_loss:0.084, val_acc:0.956]
Epoch [44/120    avg_loss:0.081, val_acc:0.950]
Epoch [45/120    avg_loss:0.061, val_acc:0.977]
Epoch [46/120    avg_loss:0.093, val_acc:0.962]
Epoch [47/120    avg_loss:0.077, val_acc:0.970]
Epoch [48/120    avg_loss:0.055, val_acc:0.974]
Epoch [49/120    avg_loss:0.089, val_acc:0.953]
Epoch [50/120    avg_loss:0.068, val_acc:0.963]
Epoch [51/120    avg_loss:0.089, val_acc:0.943]
Epoch [52/120    avg_loss:0.047, val_acc:0.979]
Epoch [53/120    avg_loss:0.044, val_acc:0.975]
Epoch [54/120    avg_loss:0.046, val_acc:0.977]
Epoch [55/120    avg_loss:0.027, val_acc:0.975]
Epoch [56/120    avg_loss:0.017, val_acc:0.967]
Epoch [57/120    avg_loss:0.044, val_acc:0.964]
Epoch [58/120    avg_loss:0.036, val_acc:0.971]
Epoch [59/120    avg_loss:0.030, val_acc:0.978]
Epoch [60/120    avg_loss:0.066, val_acc:0.926]
Epoch [61/120    avg_loss:0.135, val_acc:0.951]
Epoch [62/120    avg_loss:0.053, val_acc:0.978]
Epoch [63/120    avg_loss:0.018, val_acc:0.975]
Epoch [64/120    avg_loss:0.030, val_acc:0.974]
Epoch [65/120    avg_loss:0.050, val_acc:0.969]
Epoch [66/120    avg_loss:0.020, val_acc:0.974]
Epoch [67/120    avg_loss:0.018, val_acc:0.974]
Epoch [68/120    avg_loss:0.017, val_acc:0.974]
Epoch [69/120    avg_loss:0.010, val_acc:0.973]
Epoch [70/120    avg_loss:0.015, val_acc:0.974]
Epoch [71/120    avg_loss:0.011, val_acc:0.977]
Epoch [72/120    avg_loss:0.018, val_acc:0.974]
Epoch [73/120    avg_loss:0.014, val_acc:0.977]
Epoch [74/120    avg_loss:0.017, val_acc:0.977]
Epoch [75/120    avg_loss:0.011, val_acc:0.975]
Epoch [76/120    avg_loss:0.012, val_acc:0.975]
Epoch [77/120    avg_loss:0.014, val_acc:0.977]
Epoch [78/120    avg_loss:0.014, val_acc:0.979]
Epoch [79/120    avg_loss:0.015, val_acc:0.979]
Epoch [80/120    avg_loss:0.008, val_acc:0.978]
Epoch [81/120    avg_loss:0.014, val_acc:0.979]
Epoch [82/120    avg_loss:0.029, val_acc:0.977]
Epoch [83/120    avg_loss:0.014, val_acc:0.978]
Epoch [84/120    avg_loss:0.008, val_acc:0.978]
Epoch [85/120    avg_loss:0.011, val_acc:0.979]
Epoch [86/120    avg_loss:0.014, val_acc:0.978]
Epoch [87/120    avg_loss:0.007, val_acc:0.978]
Epoch [88/120    avg_loss:0.012, val_acc:0.978]
Epoch [89/120    avg_loss:0.009, val_acc:0.978]
Epoch [90/120    avg_loss:0.010, val_acc:0.979]
Epoch [91/120    avg_loss:0.011, val_acc:0.979]
Epoch [92/120    avg_loss:0.009, val_acc:0.978]
Epoch [93/120    avg_loss:0.011, val_acc:0.979]
Epoch [94/120    avg_loss:0.014, val_acc:0.980]
Epoch [95/120    avg_loss:0.014, val_acc:0.979]
Epoch [96/120    avg_loss:0.009, val_acc:0.979]
Epoch [97/120    avg_loss:0.013, val_acc:0.978]
Epoch [98/120    avg_loss:0.011, val_acc:0.978]
Epoch [99/120    avg_loss:0.012, val_acc:0.977]
Epoch [100/120    avg_loss:0.011, val_acc:0.975]
Epoch [101/120    avg_loss:0.011, val_acc:0.973]
Epoch [102/120    avg_loss:0.010, val_acc:0.977]
Epoch [103/120    avg_loss:0.009, val_acc:0.977]
Epoch [104/120    avg_loss:0.008, val_acc:0.978]
Epoch [105/120    avg_loss:0.020, val_acc:0.972]
Epoch [106/120    avg_loss:0.011, val_acc:0.974]
Epoch [107/120    avg_loss:0.009, val_acc:0.972]
Epoch [108/120    avg_loss:0.008, val_acc:0.972]
Epoch [109/120    avg_loss:0.012, val_acc:0.972]
Epoch [110/120    avg_loss:0.010, val_acc:0.972]
Epoch [111/120    avg_loss:0.008, val_acc:0.971]
Epoch [112/120    avg_loss:0.012, val_acc:0.971]
Epoch [113/120    avg_loss:0.009, val_acc:0.971]
Epoch [114/120    avg_loss:0.011, val_acc:0.971]
Epoch [115/120    avg_loss:0.006, val_acc:0.971]
Epoch [116/120    avg_loss:0.012, val_acc:0.971]
Epoch [117/120    avg_loss:0.011, val_acc:0.971]
Epoch [118/120    avg_loss:0.009, val_acc:0.971]
Epoch [119/120    avg_loss:0.007, val_acc:0.971]
Epoch [120/120    avg_loss:0.007, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1263    8    0    0    1    0    0    0    9    3    0    0
     0    0    1]
 [   0    0    0  727    0   15    0    0    0    4    0    0    1    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   13    0    0    1    0
     0    0    0]
 [   0    0    4   67    0    2    9    0    0    0  777   13    2    0
     1    0    0]
 [   0    0    5    0    0    0    8    0    0    0    9 2187    0    1
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0   10    0  515    0
     1    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   28    0    0    0    0    0    0    0
    23  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.3550135501355

F1 scores:
[       nan 1.         0.98787642 0.93625241 1.         0.97303371
 0.96617647 0.98039216 1.         0.72222222 0.925      0.99116247
 0.9772296  0.99730458 0.98782609 0.92068429 0.95402299]

Kappa:
0.9698487240199419
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9b436d8710>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.305, val_acc:0.435]
Epoch [2/120    avg_loss:2.056, val_acc:0.512]
Epoch [3/120    avg_loss:1.624, val_acc:0.620]
Epoch [4/120    avg_loss:1.390, val_acc:0.652]
Epoch [5/120    avg_loss:1.310, val_acc:0.678]
Epoch [6/120    avg_loss:1.120, val_acc:0.694]
Epoch [7/120    avg_loss:1.078, val_acc:0.671]
Epoch [8/120    avg_loss:1.055, val_acc:0.714]
Epoch [9/120    avg_loss:0.868, val_acc:0.753]
Epoch [10/120    avg_loss:0.652, val_acc:0.729]
Epoch [11/120    avg_loss:0.797, val_acc:0.619]
Epoch [12/120    avg_loss:0.697, val_acc:0.776]
Epoch [13/120    avg_loss:0.629, val_acc:0.702]
Epoch [14/120    avg_loss:0.630, val_acc:0.770]
Epoch [15/120    avg_loss:0.557, val_acc:0.783]
Epoch [16/120    avg_loss:0.428, val_acc:0.793]
Epoch [17/120    avg_loss:0.525, val_acc:0.760]
Epoch [18/120    avg_loss:0.572, val_acc:0.816]
Epoch [19/120    avg_loss:0.423, val_acc:0.857]
Epoch [20/120    avg_loss:0.373, val_acc:0.832]
Epoch [21/120    avg_loss:0.315, val_acc:0.841]
Epoch [22/120    avg_loss:0.307, val_acc:0.845]
Epoch [23/120    avg_loss:0.328, val_acc:0.880]
Epoch [24/120    avg_loss:0.255, val_acc:0.869]
Epoch [25/120    avg_loss:0.312, val_acc:0.868]
Epoch [26/120    avg_loss:0.264, val_acc:0.866]
Epoch [27/120    avg_loss:0.267, val_acc:0.909]
Epoch [28/120    avg_loss:0.256, val_acc:0.877]
Epoch [29/120    avg_loss:0.238, val_acc:0.903]
Epoch [30/120    avg_loss:0.146, val_acc:0.911]
Epoch [31/120    avg_loss:0.158, val_acc:0.899]
Epoch [32/120    avg_loss:0.166, val_acc:0.899]
Epoch [33/120    avg_loss:0.153, val_acc:0.895]
Epoch [34/120    avg_loss:0.166, val_acc:0.935]
Epoch [35/120    avg_loss:0.103, val_acc:0.934]
Epoch [36/120    avg_loss:0.130, val_acc:0.907]
Epoch [37/120    avg_loss:0.130, val_acc:0.934]
Epoch [38/120    avg_loss:0.136, val_acc:0.917]
Epoch [39/120    avg_loss:0.142, val_acc:0.925]
Epoch [40/120    avg_loss:0.178, val_acc:0.912]
Epoch [41/120    avg_loss:0.212, val_acc:0.886]
Epoch [42/120    avg_loss:0.129, val_acc:0.923]
Epoch [43/120    avg_loss:0.085, val_acc:0.917]
Epoch [44/120    avg_loss:0.086, val_acc:0.946]
Epoch [45/120    avg_loss:0.135, val_acc:0.919]
Epoch [46/120    avg_loss:0.132, val_acc:0.931]
Epoch [47/120    avg_loss:0.109, val_acc:0.908]
Epoch [48/120    avg_loss:0.083, val_acc:0.947]
Epoch [49/120    avg_loss:0.063, val_acc:0.944]
Epoch [50/120    avg_loss:0.091, val_acc:0.929]
Epoch [51/120    avg_loss:0.074, val_acc:0.949]
Epoch [52/120    avg_loss:0.054, val_acc:0.917]
Epoch [53/120    avg_loss:0.110, val_acc:0.957]
Epoch [54/120    avg_loss:0.084, val_acc:0.944]
Epoch [55/120    avg_loss:0.059, val_acc:0.944]
Epoch [56/120    avg_loss:0.057, val_acc:0.955]
Epoch [57/120    avg_loss:0.049, val_acc:0.950]
Epoch [58/120    avg_loss:0.048, val_acc:0.954]
Epoch [59/120    avg_loss:0.032, val_acc:0.966]
Epoch [60/120    avg_loss:0.028, val_acc:0.962]
Epoch [61/120    avg_loss:0.058, val_acc:0.955]
Epoch [62/120    avg_loss:0.044, val_acc:0.965]
Epoch [63/120    avg_loss:0.059, val_acc:0.940]
Epoch [64/120    avg_loss:0.065, val_acc:0.965]
Epoch [65/120    avg_loss:0.040, val_acc:0.965]
Epoch [66/120    avg_loss:0.026, val_acc:0.972]
Epoch [67/120    avg_loss:0.030, val_acc:0.966]
Epoch [68/120    avg_loss:0.025, val_acc:0.968]
Epoch [69/120    avg_loss:0.031, val_acc:0.968]
Epoch [70/120    avg_loss:0.020, val_acc:0.970]
Epoch [71/120    avg_loss:0.023, val_acc:0.967]
Epoch [72/120    avg_loss:0.026, val_acc:0.975]
Epoch [73/120    avg_loss:0.014, val_acc:0.977]
Epoch [74/120    avg_loss:0.041, val_acc:0.967]
Epoch [75/120    avg_loss:0.028, val_acc:0.976]
Epoch [76/120    avg_loss:0.016, val_acc:0.979]
Epoch [77/120    avg_loss:0.024, val_acc:0.973]
Epoch [78/120    avg_loss:0.022, val_acc:0.979]
Epoch [79/120    avg_loss:0.017, val_acc:0.972]
Epoch [80/120    avg_loss:0.024, val_acc:0.973]
Epoch [81/120    avg_loss:0.012, val_acc:0.977]
Epoch [82/120    avg_loss:0.018, val_acc:0.976]
Epoch [83/120    avg_loss:0.009, val_acc:0.979]
Epoch [84/120    avg_loss:0.011, val_acc:0.973]
Epoch [85/120    avg_loss:0.027, val_acc:0.964]
Epoch [86/120    avg_loss:0.059, val_acc:0.970]
Epoch [87/120    avg_loss:0.024, val_acc:0.979]
Epoch [88/120    avg_loss:0.018, val_acc:0.983]
Epoch [89/120    avg_loss:0.018, val_acc:0.980]
Epoch [90/120    avg_loss:0.010, val_acc:0.970]
Epoch [91/120    avg_loss:0.022, val_acc:0.976]
Epoch [92/120    avg_loss:0.015, val_acc:0.972]
Epoch [93/120    avg_loss:0.039, val_acc:0.967]
Epoch [94/120    avg_loss:0.022, val_acc:0.966]
Epoch [95/120    avg_loss:0.038, val_acc:0.981]
Epoch [96/120    avg_loss:0.038, val_acc:0.977]
Epoch [97/120    avg_loss:0.017, val_acc:0.976]
Epoch [98/120    avg_loss:0.009, val_acc:0.975]
Epoch [99/120    avg_loss:0.010, val_acc:0.984]
Epoch [100/120    avg_loss:0.021, val_acc:0.971]
Epoch [101/120    avg_loss:0.012, val_acc:0.983]
Epoch [102/120    avg_loss:0.012, val_acc:0.983]
Epoch [103/120    avg_loss:0.009, val_acc:0.985]
Epoch [104/120    avg_loss:0.006, val_acc:0.985]
Epoch [105/120    avg_loss:0.008, val_acc:0.982]
Epoch [106/120    avg_loss:0.009, val_acc:0.986]
Epoch [107/120    avg_loss:0.007, val_acc:0.984]
Epoch [108/120    avg_loss:0.014, val_acc:0.985]
Epoch [109/120    avg_loss:0.012, val_acc:0.976]
Epoch [110/120    avg_loss:0.010, val_acc:0.983]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.004, val_acc:0.989]
Epoch [113/120    avg_loss:0.018, val_acc:0.981]
Epoch [114/120    avg_loss:0.012, val_acc:0.975]
Epoch [115/120    avg_loss:0.009, val_acc:0.980]
Epoch [116/120    avg_loss:0.010, val_acc:0.984]
Epoch [117/120    avg_loss:0.010, val_acc:0.976]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.003, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1250   12    0    0    0    0    0    0    5    1   12    0
     2    3    0]
 [   0    0    0  701    0   16    0    0    0    6    0    0   22    1
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1   70    0    0    1    0    0    0  770   24    6    0
     2    1    0]
 [   0    0   10    0    0    0    4    0    0    0    0 2190    0    2
     4    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  530    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    25  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.26829268292683

F1 scores:
[       nan 1.         0.98193244 0.91633987 1.         0.97853107
 0.98722765 0.98039216 1.         0.8372093  0.93333333 0.98983051
 0.95840868 0.9919571  0.98485504 0.93655589 0.9704142 ]

Kappa:
0.9688623715348472
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd90b2ae6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.279, val_acc:0.426]
Epoch [2/120    avg_loss:1.767, val_acc:0.481]
Epoch [3/120    avg_loss:1.533, val_acc:0.655]
Epoch [4/120    avg_loss:1.246, val_acc:0.689]
Epoch [5/120    avg_loss:1.159, val_acc:0.648]
Epoch [6/120    avg_loss:1.156, val_acc:0.709]
Epoch [7/120    avg_loss:0.921, val_acc:0.768]
Epoch [8/120    avg_loss:0.859, val_acc:0.706]
Epoch [9/120    avg_loss:0.817, val_acc:0.798]
Epoch [10/120    avg_loss:0.840, val_acc:0.782]
Epoch [11/120    avg_loss:0.578, val_acc:0.816]
Epoch [12/120    avg_loss:0.546, val_acc:0.811]
Epoch [13/120    avg_loss:0.427, val_acc:0.776]
Epoch [14/120    avg_loss:0.402, val_acc:0.830]
Epoch [15/120    avg_loss:0.582, val_acc:0.875]
Epoch [16/120    avg_loss:0.350, val_acc:0.896]
Epoch [17/120    avg_loss:0.421, val_acc:0.902]
Epoch [18/120    avg_loss:0.308, val_acc:0.867]
Epoch [19/120    avg_loss:0.343, val_acc:0.865]
Epoch [20/120    avg_loss:0.337, val_acc:0.891]
Epoch [21/120    avg_loss:0.220, val_acc:0.912]
Epoch [22/120    avg_loss:0.234, val_acc:0.900]
Epoch [23/120    avg_loss:0.243, val_acc:0.896]
Epoch [24/120    avg_loss:0.261, val_acc:0.917]
Epoch [25/120    avg_loss:0.204, val_acc:0.905]
Epoch [26/120    avg_loss:0.173, val_acc:0.888]
Epoch [27/120    avg_loss:0.241, val_acc:0.907]
Epoch [28/120    avg_loss:0.213, val_acc:0.910]
Epoch [29/120    avg_loss:0.176, val_acc:0.934]
Epoch [30/120    avg_loss:0.160, val_acc:0.920]
Epoch [31/120    avg_loss:0.156, val_acc:0.935]
Epoch [32/120    avg_loss:0.147, val_acc:0.903]
Epoch [33/120    avg_loss:0.144, val_acc:0.926]
Epoch [34/120    avg_loss:0.121, val_acc:0.927]
Epoch [35/120    avg_loss:0.142, val_acc:0.920]
Epoch [36/120    avg_loss:0.127, val_acc:0.936]
Epoch [37/120    avg_loss:0.116, val_acc:0.961]
Epoch [38/120    avg_loss:0.167, val_acc:0.925]
Epoch [39/120    avg_loss:0.145, val_acc:0.946]
Epoch [40/120    avg_loss:0.182, val_acc:0.938]
Epoch [41/120    avg_loss:0.136, val_acc:0.935]
Epoch [42/120    avg_loss:0.159, val_acc:0.938]
Epoch [43/120    avg_loss:0.111, val_acc:0.938]
Epoch [44/120    avg_loss:0.062, val_acc:0.961]
Epoch [45/120    avg_loss:0.064, val_acc:0.953]
Epoch [46/120    avg_loss:0.050, val_acc:0.958]
Epoch [47/120    avg_loss:0.081, val_acc:0.953]
Epoch [48/120    avg_loss:0.058, val_acc:0.963]
Epoch [49/120    avg_loss:0.058, val_acc:0.960]
Epoch [50/120    avg_loss:0.062, val_acc:0.969]
Epoch [51/120    avg_loss:0.067, val_acc:0.956]
Epoch [52/120    avg_loss:0.079, val_acc:0.941]
Epoch [53/120    avg_loss:0.095, val_acc:0.962]
Epoch [54/120    avg_loss:0.060, val_acc:0.971]
Epoch [55/120    avg_loss:0.062, val_acc:0.969]
Epoch [56/120    avg_loss:0.042, val_acc:0.971]
Epoch [57/120    avg_loss:0.047, val_acc:0.971]
Epoch [58/120    avg_loss:0.066, val_acc:0.959]
Epoch [59/120    avg_loss:0.094, val_acc:0.951]
Epoch [60/120    avg_loss:0.057, val_acc:0.964]
Epoch [61/120    avg_loss:0.042, val_acc:0.974]
Epoch [62/120    avg_loss:0.030, val_acc:0.964]
Epoch [63/120    avg_loss:0.038, val_acc:0.970]
Epoch [64/120    avg_loss:0.026, val_acc:0.975]
Epoch [65/120    avg_loss:0.048, val_acc:0.980]
Epoch [66/120    avg_loss:0.044, val_acc:0.973]
Epoch [67/120    avg_loss:0.027, val_acc:0.980]
Epoch [68/120    avg_loss:0.016, val_acc:0.983]
Epoch [69/120    avg_loss:0.023, val_acc:0.987]
Epoch [70/120    avg_loss:0.023, val_acc:0.977]
Epoch [71/120    avg_loss:0.014, val_acc:0.984]
Epoch [72/120    avg_loss:0.025, val_acc:0.956]
Epoch [73/120    avg_loss:0.039, val_acc:0.967]
Epoch [74/120    avg_loss:0.018, val_acc:0.978]
Epoch [75/120    avg_loss:0.014, val_acc:0.980]
Epoch [76/120    avg_loss:0.016, val_acc:0.977]
Epoch [77/120    avg_loss:0.016, val_acc:0.981]
Epoch [78/120    avg_loss:0.015, val_acc:0.981]
Epoch [79/120    avg_loss:0.018, val_acc:0.979]
Epoch [80/120    avg_loss:0.040, val_acc:0.977]
Epoch [81/120    avg_loss:0.019, val_acc:0.977]
Epoch [82/120    avg_loss:0.018, val_acc:0.978]
Epoch [83/120    avg_loss:0.018, val_acc:0.980]
Epoch [84/120    avg_loss:0.009, val_acc:0.982]
Epoch [85/120    avg_loss:0.009, val_acc:0.983]
Epoch [86/120    avg_loss:0.009, val_acc:0.984]
Epoch [87/120    avg_loss:0.010, val_acc:0.984]
Epoch [88/120    avg_loss:0.012, val_acc:0.987]
Epoch [89/120    avg_loss:0.020, val_acc:0.988]
Epoch [90/120    avg_loss:0.010, val_acc:0.990]
Epoch [91/120    avg_loss:0.011, val_acc:0.989]
Epoch [92/120    avg_loss:0.009, val_acc:0.988]
Epoch [93/120    avg_loss:0.010, val_acc:0.989]
Epoch [94/120    avg_loss:0.009, val_acc:0.989]
Epoch [95/120    avg_loss:0.009, val_acc:0.990]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.989]
Epoch [99/120    avg_loss:0.007, val_acc:0.990]
Epoch [100/120    avg_loss:0.009, val_acc:0.991]
Epoch [101/120    avg_loss:0.007, val_acc:0.990]
Epoch [102/120    avg_loss:0.008, val_acc:0.992]
Epoch [103/120    avg_loss:0.006, val_acc:0.990]
Epoch [104/120    avg_loss:0.008, val_acc:0.991]
Epoch [105/120    avg_loss:0.009, val_acc:0.988]
Epoch [106/120    avg_loss:0.007, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.991]
Epoch [108/120    avg_loss:0.009, val_acc:0.990]
Epoch [109/120    avg_loss:0.007, val_acc:0.990]
Epoch [110/120    avg_loss:0.008, val_acc:0.990]
Epoch [111/120    avg_loss:0.006, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.990]
Epoch [113/120    avg_loss:0.008, val_acc:0.992]
Epoch [114/120    avg_loss:0.009, val_acc:0.991]
Epoch [115/120    avg_loss:0.005, val_acc:0.992]
Epoch [116/120    avg_loss:0.005, val_acc:0.992]
Epoch [117/120    avg_loss:0.007, val_acc:0.992]
Epoch [118/120    avg_loss:0.006, val_acc:0.992]
Epoch [119/120    avg_loss:0.006, val_acc:0.991]
Epoch [120/120    avg_loss:0.008, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1279    3    0    0    0    0    0    0    1    1    0    0
     0    1    0]
 [   0    0    0  732    0    9    0    0    0    5    0    0    1    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    9   50    0    0    8    0    0    0  780   25    0    0
     1    2    0]
 [   0    0   15    0    0    0   14    0    0    0    6 2173    0    0
     2    0    0]
 [   0    0    0    0    0    3    0    0    0    0    9    0  516    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    30  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.51761517615176

F1 scores:
[       nan 1.         0.98840804 0.95374593 1.         0.98409091
 0.96688742 0.98039216 1.         0.76923077 0.93357271 0.98571105
 0.98192198 1.         0.98572047 0.91304348 0.96551724]

Kappa:
0.9716962919580089
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f13324756a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.109, val_acc:0.482]
Epoch [2/120    avg_loss:1.748, val_acc:0.483]
Epoch [3/120    avg_loss:1.482, val_acc:0.543]
Epoch [4/120    avg_loss:1.302, val_acc:0.591]
Epoch [5/120    avg_loss:1.083, val_acc:0.727]
Epoch [6/120    avg_loss:1.103, val_acc:0.662]
Epoch [7/120    avg_loss:0.899, val_acc:0.700]
Epoch [8/120    avg_loss:0.777, val_acc:0.738]
Epoch [9/120    avg_loss:0.830, val_acc:0.690]
Epoch [10/120    avg_loss:0.760, val_acc:0.753]
Epoch [11/120    avg_loss:0.675, val_acc:0.816]
Epoch [12/120    avg_loss:0.584, val_acc:0.809]
Epoch [13/120    avg_loss:0.568, val_acc:0.711]
Epoch [14/120    avg_loss:0.444, val_acc:0.817]
Epoch [15/120    avg_loss:0.461, val_acc:0.816]
Epoch [16/120    avg_loss:0.458, val_acc:0.833]
Epoch [17/120    avg_loss:0.470, val_acc:0.853]
Epoch [18/120    avg_loss:0.419, val_acc:0.865]
Epoch [19/120    avg_loss:0.421, val_acc:0.846]
Epoch [20/120    avg_loss:0.279, val_acc:0.873]
Epoch [21/120    avg_loss:0.375, val_acc:0.813]
Epoch [22/120    avg_loss:0.379, val_acc:0.889]
Epoch [23/120    avg_loss:0.280, val_acc:0.888]
Epoch [24/120    avg_loss:0.254, val_acc:0.890]
Epoch [25/120    avg_loss:0.298, val_acc:0.889]
Epoch [26/120    avg_loss:0.223, val_acc:0.902]
Epoch [27/120    avg_loss:0.279, val_acc:0.905]
Epoch [28/120    avg_loss:0.204, val_acc:0.896]
Epoch [29/120    avg_loss:0.165, val_acc:0.921]
Epoch [30/120    avg_loss:0.185, val_acc:0.903]
Epoch [31/120    avg_loss:0.262, val_acc:0.927]
Epoch [32/120    avg_loss:0.113, val_acc:0.938]
Epoch [33/120    avg_loss:0.090, val_acc:0.952]
Epoch [34/120    avg_loss:0.095, val_acc:0.945]
Epoch [35/120    avg_loss:0.151, val_acc:0.939]
Epoch [36/120    avg_loss:0.179, val_acc:0.898]
Epoch [37/120    avg_loss:0.104, val_acc:0.929]
Epoch [38/120    avg_loss:0.173, val_acc:0.948]
Epoch [39/120    avg_loss:0.102, val_acc:0.959]
Epoch [40/120    avg_loss:0.109, val_acc:0.944]
Epoch [41/120    avg_loss:0.098, val_acc:0.955]
Epoch [42/120    avg_loss:0.091, val_acc:0.948]
Epoch [43/120    avg_loss:0.079, val_acc:0.949]
Epoch [44/120    avg_loss:0.095, val_acc:0.952]
Epoch [45/120    avg_loss:0.104, val_acc:0.956]
Epoch [46/120    avg_loss:0.057, val_acc:0.951]
Epoch [47/120    avg_loss:0.084, val_acc:0.958]
Epoch [48/120    avg_loss:0.057, val_acc:0.958]
Epoch [49/120    avg_loss:0.058, val_acc:0.962]
Epoch [50/120    avg_loss:0.103, val_acc:0.966]
Epoch [51/120    avg_loss:0.086, val_acc:0.959]
Epoch [52/120    avg_loss:0.077, val_acc:0.971]
Epoch [53/120    avg_loss:0.041, val_acc:0.968]
Epoch [54/120    avg_loss:0.024, val_acc:0.968]
Epoch [55/120    avg_loss:0.030, val_acc:0.969]
Epoch [56/120    avg_loss:0.016, val_acc:0.975]
Epoch [57/120    avg_loss:0.029, val_acc:0.976]
Epoch [58/120    avg_loss:0.048, val_acc:0.962]
Epoch [59/120    avg_loss:0.033, val_acc:0.977]
Epoch [60/120    avg_loss:0.021, val_acc:0.980]
Epoch [61/120    avg_loss:0.019, val_acc:0.983]
Epoch [62/120    avg_loss:0.063, val_acc:0.973]
Epoch [63/120    avg_loss:0.038, val_acc:0.968]
Epoch [64/120    avg_loss:0.030, val_acc:0.962]
Epoch [65/120    avg_loss:0.095, val_acc:0.960]
Epoch [66/120    avg_loss:0.035, val_acc:0.969]
Epoch [67/120    avg_loss:0.044, val_acc:0.970]
Epoch [68/120    avg_loss:0.031, val_acc:0.975]
Epoch [69/120    avg_loss:0.017, val_acc:0.982]
Epoch [70/120    avg_loss:0.023, val_acc:0.973]
Epoch [71/120    avg_loss:0.030, val_acc:0.972]
Epoch [72/120    avg_loss:0.043, val_acc:0.968]
Epoch [73/120    avg_loss:0.018, val_acc:0.979]
Epoch [74/120    avg_loss:0.012, val_acc:0.985]
Epoch [75/120    avg_loss:0.022, val_acc:0.982]
Epoch [76/120    avg_loss:0.029, val_acc:0.981]
Epoch [77/120    avg_loss:0.020, val_acc:0.990]
Epoch [78/120    avg_loss:0.015, val_acc:0.985]
Epoch [79/120    avg_loss:0.012, val_acc:0.985]
Epoch [80/120    avg_loss:0.020, val_acc:0.981]
Epoch [81/120    avg_loss:0.020, val_acc:0.989]
Epoch [82/120    avg_loss:0.012, val_acc:0.986]
Epoch [83/120    avg_loss:0.017, val_acc:0.975]
Epoch [84/120    avg_loss:0.013, val_acc:0.980]
Epoch [85/120    avg_loss:0.014, val_acc:0.983]
Epoch [86/120    avg_loss:0.009, val_acc:0.987]
Epoch [87/120    avg_loss:0.028, val_acc:0.968]
Epoch [88/120    avg_loss:0.061, val_acc:0.975]
Epoch [89/120    avg_loss:0.026, val_acc:0.986]
Epoch [90/120    avg_loss:0.015, val_acc:0.983]
Epoch [91/120    avg_loss:0.017, val_acc:0.985]
Epoch [92/120    avg_loss:0.010, val_acc:0.982]
Epoch [93/120    avg_loss:0.014, val_acc:0.982]
Epoch [94/120    avg_loss:0.011, val_acc:0.982]
Epoch [95/120    avg_loss:0.006, val_acc:0.982]
Epoch [96/120    avg_loss:0.007, val_acc:0.983]
Epoch [97/120    avg_loss:0.007, val_acc:0.985]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.007, val_acc:0.989]
Epoch [100/120    avg_loss:0.008, val_acc:0.987]
Epoch [101/120    avg_loss:0.010, val_acc:0.988]
Epoch [102/120    avg_loss:0.007, val_acc:0.988]
Epoch [103/120    avg_loss:0.005, val_acc:0.987]
Epoch [104/120    avg_loss:0.007, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.986]
Epoch [106/120    avg_loss:0.007, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.011, val_acc:0.988]
Epoch [112/120    avg_loss:0.007, val_acc:0.988]
Epoch [113/120    avg_loss:0.004, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.007, val_acc:0.988]
Epoch [117/120    avg_loss:0.006, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.988]
Epoch [120/120    avg_loss:0.006, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1269   11    0    0    0    0    0    0    4    1    0    0
     0    0    0]
 [   0    0    0  731    0   12    0    0    0    3    1    0    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    2   74    0    1    0    0    0    0  770   22    5    0
     0    1    0]
 [   0    0    6    0    0    0   10    0    0    0    3 2189    0    2
     0    0    0]
 [   0    0    0    1    2    5    0    0    0    0    5    0  518    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   26    0    0    0    0    0    0    0
    43  278    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.27913279132791

F1 scores:
[       nan 1.         0.99063232 0.93299298 0.9953271  0.97522523
 0.97333333 0.98039216 1.         0.81081081 0.92882992 0.99004975
 0.97920605 0.99462366 0.98059508 0.88817891 0.97647059]

Kappa:
0.9689719852795774
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f883f0396d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.142, val_acc:0.557]
Epoch [2/120    avg_loss:1.718, val_acc:0.623]
Epoch [3/120    avg_loss:1.435, val_acc:0.648]
Epoch [4/120    avg_loss:1.381, val_acc:0.601]
Epoch [5/120    avg_loss:1.124, val_acc:0.685]
Epoch [6/120    avg_loss:1.002, val_acc:0.772]
Epoch [7/120    avg_loss:0.906, val_acc:0.738]
Epoch [8/120    avg_loss:0.848, val_acc:0.734]
Epoch [9/120    avg_loss:0.760, val_acc:0.820]
Epoch [10/120    avg_loss:0.663, val_acc:0.599]
Epoch [11/120    avg_loss:0.613, val_acc:0.831]
Epoch [12/120    avg_loss:0.585, val_acc:0.786]
Epoch [13/120    avg_loss:0.505, val_acc:0.858]
Epoch [14/120    avg_loss:0.446, val_acc:0.857]
Epoch [15/120    avg_loss:0.436, val_acc:0.883]
Epoch [16/120    avg_loss:0.432, val_acc:0.875]
Epoch [17/120    avg_loss:0.412, val_acc:0.856]
Epoch [18/120    avg_loss:0.428, val_acc:0.894]
Epoch [19/120    avg_loss:0.274, val_acc:0.901]
Epoch [20/120    avg_loss:0.270, val_acc:0.907]
Epoch [21/120    avg_loss:0.375, val_acc:0.875]
Epoch [22/120    avg_loss:0.303, val_acc:0.894]
Epoch [23/120    avg_loss:0.255, val_acc:0.930]
Epoch [24/120    avg_loss:0.243, val_acc:0.912]
Epoch [25/120    avg_loss:0.197, val_acc:0.881]
Epoch [26/120    avg_loss:0.198, val_acc:0.912]
Epoch [27/120    avg_loss:0.163, val_acc:0.933]
Epoch [28/120    avg_loss:0.164, val_acc:0.944]
Epoch [29/120    avg_loss:0.230, val_acc:0.811]
Epoch [30/120    avg_loss:0.211, val_acc:0.932]
Epoch [31/120    avg_loss:0.113, val_acc:0.926]
Epoch [32/120    avg_loss:0.205, val_acc:0.934]
Epoch [33/120    avg_loss:0.147, val_acc:0.939]
Epoch [34/120    avg_loss:0.140, val_acc:0.957]
Epoch [35/120    avg_loss:0.113, val_acc:0.935]
Epoch [36/120    avg_loss:0.126, val_acc:0.922]
Epoch [37/120    avg_loss:0.082, val_acc:0.969]
Epoch [38/120    avg_loss:0.099, val_acc:0.964]
Epoch [39/120    avg_loss:0.094, val_acc:0.965]
Epoch [40/120    avg_loss:0.070, val_acc:0.966]
Epoch [41/120    avg_loss:0.070, val_acc:0.960]
Epoch [42/120    avg_loss:0.062, val_acc:0.964]
Epoch [43/120    avg_loss:0.067, val_acc:0.969]
Epoch [44/120    avg_loss:0.072, val_acc:0.959]
Epoch [45/120    avg_loss:0.064, val_acc:0.967]
Epoch [46/120    avg_loss:0.081, val_acc:0.968]
Epoch [47/120    avg_loss:0.096, val_acc:0.963]
Epoch [48/120    avg_loss:0.054, val_acc:0.936]
Epoch [49/120    avg_loss:0.095, val_acc:0.959]
Epoch [50/120    avg_loss:0.049, val_acc:0.972]
Epoch [51/120    avg_loss:0.031, val_acc:0.974]
Epoch [52/120    avg_loss:0.038, val_acc:0.924]
Epoch [53/120    avg_loss:0.048, val_acc:0.974]
Epoch [54/120    avg_loss:0.034, val_acc:0.964]
Epoch [55/120    avg_loss:0.045, val_acc:0.970]
Epoch [56/120    avg_loss:0.035, val_acc:0.976]
Epoch [57/120    avg_loss:0.046, val_acc:0.976]
Epoch [58/120    avg_loss:0.029, val_acc:0.980]
Epoch [59/120    avg_loss:0.035, val_acc:0.980]
Epoch [60/120    avg_loss:0.046, val_acc:0.967]
Epoch [61/120    avg_loss:0.058, val_acc:0.985]
Epoch [62/120    avg_loss:0.032, val_acc:0.973]
Epoch [63/120    avg_loss:0.030, val_acc:0.978]
Epoch [64/120    avg_loss:0.034, val_acc:0.977]
Epoch [65/120    avg_loss:0.042, val_acc:0.975]
Epoch [66/120    avg_loss:0.026, val_acc:0.970]
Epoch [67/120    avg_loss:0.026, val_acc:0.974]
Epoch [68/120    avg_loss:0.022, val_acc:0.978]
Epoch [69/120    avg_loss:0.032, val_acc:0.970]
Epoch [70/120    avg_loss:0.058, val_acc:0.960]
Epoch [71/120    avg_loss:0.036, val_acc:0.980]
Epoch [72/120    avg_loss:0.028, val_acc:0.975]
Epoch [73/120    avg_loss:0.034, val_acc:0.956]
Epoch [74/120    avg_loss:0.019, val_acc:0.952]
Epoch [75/120    avg_loss:0.030, val_acc:0.980]
Epoch [76/120    avg_loss:0.015, val_acc:0.989]
Epoch [77/120    avg_loss:0.008, val_acc:0.990]
Epoch [78/120    avg_loss:0.010, val_acc:0.991]
Epoch [79/120    avg_loss:0.016, val_acc:0.990]
Epoch [80/120    avg_loss:0.014, val_acc:0.988]
Epoch [81/120    avg_loss:0.010, val_acc:0.990]
Epoch [82/120    avg_loss:0.007, val_acc:0.990]
Epoch [83/120    avg_loss:0.005, val_acc:0.990]
Epoch [84/120    avg_loss:0.012, val_acc:0.989]
Epoch [85/120    avg_loss:0.007, val_acc:0.990]
Epoch [86/120    avg_loss:0.007, val_acc:0.989]
Epoch [87/120    avg_loss:0.007, val_acc:0.990]
Epoch [88/120    avg_loss:0.008, val_acc:0.990]
Epoch [89/120    avg_loss:0.010, val_acc:0.991]
Epoch [90/120    avg_loss:0.008, val_acc:0.992]
Epoch [91/120    avg_loss:0.009, val_acc:0.991]
Epoch [92/120    avg_loss:0.006, val_acc:0.991]
Epoch [93/120    avg_loss:0.006, val_acc:0.991]
Epoch [94/120    avg_loss:0.007, val_acc:0.991]
Epoch [95/120    avg_loss:0.007, val_acc:0.991]
Epoch [96/120    avg_loss:0.005, val_acc:0.991]
Epoch [97/120    avg_loss:0.009, val_acc:0.991]
Epoch [98/120    avg_loss:0.008, val_acc:0.990]
Epoch [99/120    avg_loss:0.007, val_acc:0.990]
Epoch [100/120    avg_loss:0.008, val_acc:0.990]
Epoch [101/120    avg_loss:0.008, val_acc:0.991]
Epoch [102/120    avg_loss:0.007, val_acc:0.990]
Epoch [103/120    avg_loss:0.008, val_acc:0.991]
Epoch [104/120    avg_loss:0.008, val_acc:0.991]
Epoch [105/120    avg_loss:0.007, val_acc:0.991]
Epoch [106/120    avg_loss:0.005, val_acc:0.991]
Epoch [107/120    avg_loss:0.006, val_acc:0.991]
Epoch [108/120    avg_loss:0.010, val_acc:0.991]
Epoch [109/120    avg_loss:0.005, val_acc:0.991]
Epoch [110/120    avg_loss:0.010, val_acc:0.991]
Epoch [111/120    avg_loss:0.009, val_acc:0.991]
Epoch [112/120    avg_loss:0.015, val_acc:0.991]
Epoch [113/120    avg_loss:0.006, val_acc:0.991]
Epoch [114/120    avg_loss:0.005, val_acc:0.991]
Epoch [115/120    avg_loss:0.006, val_acc:0.991]
Epoch [116/120    avg_loss:0.007, val_acc:0.991]
Epoch [117/120    avg_loss:0.006, val_acc:0.991]
Epoch [118/120    avg_loss:0.012, val_acc:0.991]
Epoch [119/120    avg_loss:0.006, val_acc:0.991]
Epoch [120/120    avg_loss:0.006, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1269    5    0    0    0    0    0    0    5    3    3    0
     0    0    0]
 [   0    0    0  708    0   13    0    0    0    7    0    0   19    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    6    0    0    0    0    0   12    0    0    0    0
     0    0    0]
 [   0    0    0   70    0    0    3    0    0    0  781   19    2    0
     0    0    0]
 [   0    0    4    0    0    0    4    0    0    0    1 2201    0    0
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    4    0  520    0
     1    0    8]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   26    0    0    0    0    0    0    0
    25  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.4959349593496

F1 scores:
[       nan 1.         0.99218139 0.921875   1.         0.98185941
 0.97550111 0.98039216 1.         0.63157895 0.93757503 0.99300699
 0.96474954 1.         0.98871528 0.92068429 0.95454545]

Kappa:
0.9714452354932087
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:111
Validation dataloader:111
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:8
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f72fa7f06a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.277, val_acc:0.538]
Epoch [2/120    avg_loss:1.704, val_acc:0.544]
Epoch [3/120    avg_loss:1.597, val_acc:0.603]
Epoch [4/120    avg_loss:1.220, val_acc:0.712]
Epoch [5/120    avg_loss:1.119, val_acc:0.625]
Epoch [6/120    avg_loss:1.127, val_acc:0.719]
Epoch [7/120    avg_loss:0.940, val_acc:0.785]
Epoch [8/120    avg_loss:0.901, val_acc:0.758]
Epoch [9/120    avg_loss:0.777, val_acc:0.799]
Epoch [10/120    avg_loss:0.712, val_acc:0.790]
Epoch [11/120    avg_loss:0.623, val_acc:0.816]
Epoch [12/120    avg_loss:0.627, val_acc:0.823]
Epoch [13/120    avg_loss:0.484, val_acc:0.843]
Epoch [14/120    avg_loss:0.524, val_acc:0.845]
Epoch [15/120    avg_loss:0.438, val_acc:0.862]
Epoch [16/120    avg_loss:0.443, val_acc:0.848]
Epoch [17/120    avg_loss:0.390, val_acc:0.858]
Epoch [18/120    avg_loss:0.389, val_acc:0.877]
Epoch [19/120    avg_loss:0.393, val_acc:0.838]
Epoch [20/120    avg_loss:0.272, val_acc:0.873]
Epoch [21/120    avg_loss:0.241, val_acc:0.881]
Epoch [22/120    avg_loss:0.354, val_acc:0.856]
Epoch [23/120    avg_loss:0.347, val_acc:0.866]
Epoch [24/120    avg_loss:0.280, val_acc:0.895]
Epoch [25/120    avg_loss:0.179, val_acc:0.915]
Epoch [26/120    avg_loss:0.242, val_acc:0.888]
Epoch [27/120    avg_loss:0.253, val_acc:0.887]
Epoch [28/120    avg_loss:0.189, val_acc:0.916]
Epoch [29/120    avg_loss:0.257, val_acc:0.867]
Epoch [30/120    avg_loss:0.180, val_acc:0.939]
Epoch [31/120    avg_loss:0.133, val_acc:0.931]
Epoch [32/120    avg_loss:0.173, val_acc:0.919]
Epoch [33/120    avg_loss:0.104, val_acc:0.940]
Epoch [34/120    avg_loss:0.096, val_acc:0.941]
Epoch [35/120    avg_loss:0.153, val_acc:0.907]
Epoch [36/120    avg_loss:0.160, val_acc:0.897]
Epoch [37/120    avg_loss:0.246, val_acc:0.911]
Epoch [38/120    avg_loss:0.155, val_acc:0.914]
Epoch [39/120    avg_loss:0.114, val_acc:0.911]
Epoch [40/120    avg_loss:0.118, val_acc:0.955]
Epoch [41/120    avg_loss:0.086, val_acc:0.944]
Epoch [42/120    avg_loss:0.083, val_acc:0.945]
Epoch [43/120    avg_loss:0.070, val_acc:0.949]
Epoch [44/120    avg_loss:0.100, val_acc:0.942]
Epoch [45/120    avg_loss:0.094, val_acc:0.953]
Epoch [46/120    avg_loss:0.060, val_acc:0.959]
Epoch [47/120    avg_loss:0.044, val_acc:0.964]
Epoch [48/120    avg_loss:0.058, val_acc:0.958]
Epoch [49/120    avg_loss:0.064, val_acc:0.959]
Epoch [50/120    avg_loss:0.054, val_acc:0.968]
Epoch [51/120    avg_loss:0.060, val_acc:0.952]
Epoch [52/120    avg_loss:0.062, val_acc:0.964]
Epoch [53/120    avg_loss:0.070, val_acc:0.958]
Epoch [54/120    avg_loss:0.060, val_acc:0.960]
Epoch [55/120    avg_loss:0.033, val_acc:0.965]
Epoch [56/120    avg_loss:0.066, val_acc:0.922]
Epoch [57/120    avg_loss:0.092, val_acc:0.963]
Epoch [58/120    avg_loss:0.083, val_acc:0.962]
Epoch [59/120    avg_loss:0.048, val_acc:0.970]
Epoch [60/120    avg_loss:0.034, val_acc:0.973]
Epoch [61/120    avg_loss:0.027, val_acc:0.969]
Epoch [62/120    avg_loss:0.045, val_acc:0.974]
Epoch [63/120    avg_loss:0.025, val_acc:0.981]
Epoch [64/120    avg_loss:0.032, val_acc:0.974]
Epoch [65/120    avg_loss:0.025, val_acc:0.979]
Epoch [66/120    avg_loss:0.021, val_acc:0.979]
Epoch [67/120    avg_loss:0.022, val_acc:0.974]
Epoch [68/120    avg_loss:0.038, val_acc:0.974]
Epoch [69/120    avg_loss:0.043, val_acc:0.979]
Epoch [70/120    avg_loss:0.025, val_acc:0.978]
Epoch [71/120    avg_loss:0.017, val_acc:0.983]
Epoch [72/120    avg_loss:0.015, val_acc:0.989]
Epoch [73/120    avg_loss:0.013, val_acc:0.983]
Epoch [74/120    avg_loss:0.027, val_acc:0.974]
Epoch [75/120    avg_loss:0.014, val_acc:0.979]
Epoch [76/120    avg_loss:0.010, val_acc:0.984]
Epoch [77/120    avg_loss:0.016, val_acc:0.975]
Epoch [78/120    avg_loss:0.012, val_acc:0.983]
Epoch [79/120    avg_loss:0.016, val_acc:0.971]
Epoch [80/120    avg_loss:0.013, val_acc:0.970]
Epoch [81/120    avg_loss:0.029, val_acc:0.969]
Epoch [82/120    avg_loss:0.018, val_acc:0.979]
Epoch [83/120    avg_loss:0.013, val_acc:0.974]
Epoch [84/120    avg_loss:0.015, val_acc:0.977]
Epoch [85/120    avg_loss:0.010, val_acc:0.981]
Epoch [86/120    avg_loss:0.006, val_acc:0.981]
Epoch [87/120    avg_loss:0.008, val_acc:0.985]
Epoch [88/120    avg_loss:0.013, val_acc:0.984]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.009, val_acc:0.988]
Epoch [92/120    avg_loss:0.009, val_acc:0.987]
Epoch [93/120    avg_loss:0.006, val_acc:0.987]
Epoch [94/120    avg_loss:0.008, val_acc:0.987]
Epoch [95/120    avg_loss:0.004, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.988]
Epoch [97/120    avg_loss:0.006, val_acc:0.988]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.009, val_acc:0.988]
Epoch [103/120    avg_loss:0.006, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.988]
Epoch [105/120    avg_loss:0.010, val_acc:0.988]
Epoch [106/120    avg_loss:0.007, val_acc:0.988]
Epoch [107/120    avg_loss:0.008, val_acc:0.988]
Epoch [108/120    avg_loss:0.004, val_acc:0.988]
Epoch [109/120    avg_loss:0.010, val_acc:0.988]
Epoch [110/120    avg_loss:0.010, val_acc:0.988]
Epoch [111/120    avg_loss:0.006, val_acc:0.988]
Epoch [112/120    avg_loss:0.006, val_acc:0.988]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.010, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.004, val_acc:0.988]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.005, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.988]
Epoch [120/120    avg_loss:0.009, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1276    3    0    0    0    0    0    0    2    1    3    0
     0    0    0]
 [   0    0    0  710    0   17    0    0    0    2    0    0   18    0
     0    0    0]
 [   0    0    0    3  210    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0   13    0    0    0    0
     0    0    0]
 [   0    0    2   67    0    0    0    0    0    0  779   27    0    0
     0    0    0]
 [   0    0   11    0    0    0    2    0    0    0    7 2189    0    1
     0    0    0]
 [   0    0    0    3    2    3    0    0    0    0   11    0  504    0
     0    0   11]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
  1137    0    0]
 [   0    0    0    0    0    1   22    0    0    0    0    0    0   25
    45  254    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.76964769647697

F1 scores:
[       nan 1.         0.99145299 0.92327698 0.98823529 0.97194164
 0.98206278 1.         1.         0.74285714 0.9307049  0.98893156
 0.95184136 0.93434343 0.97975011 0.8452579  0.93854749]

Kappa:
0.9631565964133326
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f756e321630>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.240, val_acc:0.476]
Epoch [2/120    avg_loss:1.618, val_acc:0.547]
Epoch [3/120    avg_loss:1.196, val_acc:0.705]
Epoch [4/120    avg_loss:1.148, val_acc:0.684]
Epoch [5/120    avg_loss:0.973, val_acc:0.757]
Epoch [6/120    avg_loss:0.888, val_acc:0.777]
Epoch [7/120    avg_loss:0.751, val_acc:0.784]
Epoch [8/120    avg_loss:0.598, val_acc:0.845]
Epoch [9/120    avg_loss:0.506, val_acc:0.847]
Epoch [10/120    avg_loss:0.421, val_acc:0.873]
Epoch [11/120    avg_loss:0.393, val_acc:0.875]
Epoch [12/120    avg_loss:0.269, val_acc:0.898]
Epoch [13/120    avg_loss:0.255, val_acc:0.888]
Epoch [14/120    avg_loss:0.212, val_acc:0.893]
Epoch [15/120    avg_loss:0.136, val_acc:0.929]
Epoch [16/120    avg_loss:0.166, val_acc:0.916]
Epoch [17/120    avg_loss:0.108, val_acc:0.917]
Epoch [18/120    avg_loss:0.100, val_acc:0.928]
Epoch [19/120    avg_loss:0.172, val_acc:0.897]
Epoch [20/120    avg_loss:0.140, val_acc:0.932]
Epoch [21/120    avg_loss:0.081, val_acc:0.953]
Epoch [22/120    avg_loss:0.086, val_acc:0.939]
Epoch [23/120    avg_loss:0.097, val_acc:0.906]
Epoch [24/120    avg_loss:0.076, val_acc:0.947]
Epoch [25/120    avg_loss:0.065, val_acc:0.947]
Epoch [26/120    avg_loss:0.053, val_acc:0.966]
Epoch [27/120    avg_loss:0.037, val_acc:0.952]
Epoch [28/120    avg_loss:0.059, val_acc:0.946]
Epoch [29/120    avg_loss:0.039, val_acc:0.953]
Epoch [30/120    avg_loss:0.067, val_acc:0.949]
Epoch [31/120    avg_loss:0.091, val_acc:0.936]
Epoch [32/120    avg_loss:0.062, val_acc:0.949]
Epoch [33/120    avg_loss:0.038, val_acc:0.961]
Epoch [34/120    avg_loss:0.040, val_acc:0.951]
Epoch [35/120    avg_loss:0.036, val_acc:0.963]
Epoch [36/120    avg_loss:0.026, val_acc:0.971]
Epoch [37/120    avg_loss:0.023, val_acc:0.976]
Epoch [38/120    avg_loss:0.018, val_acc:0.963]
Epoch [39/120    avg_loss:0.026, val_acc:0.968]
Epoch [40/120    avg_loss:0.019, val_acc:0.959]
Epoch [41/120    avg_loss:0.026, val_acc:0.970]
Epoch [42/120    avg_loss:0.010, val_acc:0.967]
Epoch [43/120    avg_loss:0.014, val_acc:0.977]
Epoch [44/120    avg_loss:0.009, val_acc:0.972]
Epoch [45/120    avg_loss:0.013, val_acc:0.974]
Epoch [46/120    avg_loss:0.019, val_acc:0.960]
Epoch [47/120    avg_loss:0.014, val_acc:0.970]
Epoch [48/120    avg_loss:0.020, val_acc:0.959]
Epoch [49/120    avg_loss:0.028, val_acc:0.963]
Epoch [50/120    avg_loss:0.018, val_acc:0.970]
Epoch [51/120    avg_loss:0.013, val_acc:0.968]
Epoch [52/120    avg_loss:0.012, val_acc:0.964]
Epoch [53/120    avg_loss:0.011, val_acc:0.969]
Epoch [54/120    avg_loss:0.007, val_acc:0.971]
Epoch [55/120    avg_loss:0.029, val_acc:0.971]
Epoch [56/120    avg_loss:0.039, val_acc:0.942]
Epoch [57/120    avg_loss:0.048, val_acc:0.968]
Epoch [58/120    avg_loss:0.014, val_acc:0.972]
Epoch [59/120    avg_loss:0.012, val_acc:0.972]
Epoch [60/120    avg_loss:0.014, val_acc:0.972]
Epoch [61/120    avg_loss:0.009, val_acc:0.972]
Epoch [62/120    avg_loss:0.010, val_acc:0.971]
Epoch [63/120    avg_loss:0.006, val_acc:0.974]
Epoch [64/120    avg_loss:0.014, val_acc:0.973]
Epoch [65/120    avg_loss:0.008, val_acc:0.975]
Epoch [66/120    avg_loss:0.007, val_acc:0.975]
Epoch [67/120    avg_loss:0.005, val_acc:0.974]
Epoch [68/120    avg_loss:0.009, val_acc:0.975]
Epoch [69/120    avg_loss:0.007, val_acc:0.975]
Epoch [70/120    avg_loss:0.005, val_acc:0.975]
Epoch [71/120    avg_loss:0.004, val_acc:0.975]
Epoch [72/120    avg_loss:0.004, val_acc:0.975]
Epoch [73/120    avg_loss:0.008, val_acc:0.975]
Epoch [74/120    avg_loss:0.006, val_acc:0.975]
Epoch [75/120    avg_loss:0.007, val_acc:0.975]
Epoch [76/120    avg_loss:0.006, val_acc:0.975]
Epoch [77/120    avg_loss:0.005, val_acc:0.975]
Epoch [78/120    avg_loss:0.008, val_acc:0.975]
Epoch [79/120    avg_loss:0.005, val_acc:0.975]
Epoch [80/120    avg_loss:0.005, val_acc:0.975]
Epoch [81/120    avg_loss:0.013, val_acc:0.975]
Epoch [82/120    avg_loss:0.006, val_acc:0.975]
Epoch [83/120    avg_loss:0.005, val_acc:0.975]
Epoch [84/120    avg_loss:0.006, val_acc:0.975]
Epoch [85/120    avg_loss:0.006, val_acc:0.975]
Epoch [86/120    avg_loss:0.007, val_acc:0.975]
Epoch [87/120    avg_loss:0.006, val_acc:0.975]
Epoch [88/120    avg_loss:0.005, val_acc:0.975]
Epoch [89/120    avg_loss:0.008, val_acc:0.975]
Epoch [90/120    avg_loss:0.005, val_acc:0.975]
Epoch [91/120    avg_loss:0.007, val_acc:0.975]
Epoch [92/120    avg_loss:0.007, val_acc:0.975]
Epoch [93/120    avg_loss:0.008, val_acc:0.975]
Epoch [94/120    avg_loss:0.005, val_acc:0.975]
Epoch [95/120    avg_loss:0.004, val_acc:0.975]
Epoch [96/120    avg_loss:0.006, val_acc:0.975]
Epoch [97/120    avg_loss:0.012, val_acc:0.975]
Epoch [98/120    avg_loss:0.009, val_acc:0.975]
Epoch [99/120    avg_loss:0.006, val_acc:0.975]
Epoch [100/120    avg_loss:0.004, val_acc:0.975]
Epoch [101/120    avg_loss:0.007, val_acc:0.975]
Epoch [102/120    avg_loss:0.008, val_acc:0.975]
Epoch [103/120    avg_loss:0.009, val_acc:0.975]
Epoch [104/120    avg_loss:0.007, val_acc:0.975]
Epoch [105/120    avg_loss:0.025, val_acc:0.975]
Epoch [106/120    avg_loss:0.006, val_acc:0.975]
Epoch [107/120    avg_loss:0.008, val_acc:0.975]
Epoch [108/120    avg_loss:0.007, val_acc:0.975]
Epoch [109/120    avg_loss:0.007, val_acc:0.975]
Epoch [110/120    avg_loss:0.009, val_acc:0.975]
Epoch [111/120    avg_loss:0.004, val_acc:0.975]
Epoch [112/120    avg_loss:0.007, val_acc:0.975]
Epoch [113/120    avg_loss:0.007, val_acc:0.975]
Epoch [114/120    avg_loss:0.006, val_acc:0.975]
Epoch [115/120    avg_loss:0.007, val_acc:0.975]
Epoch [116/120    avg_loss:0.005, val_acc:0.975]
Epoch [117/120    avg_loss:0.007, val_acc:0.975]
Epoch [118/120    avg_loss:0.005, val_acc:0.975]
Epoch [119/120    avg_loss:0.010, val_acc:0.975]
Epoch [120/120    avg_loss:0.009, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1269    3    0    5    0    0    0    0    0    8    0    0
     0    0    0]
 [   0    0    0  723    1    4    0    0    0    3    0    3   12    0
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    1    0    0  431    0    1    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    2    0    0    0    0  851   18    0    0
     2    1    0]
 [   0    0    6    0    0    0    0    0    0    0    7 2183   14    0
     0    0    0]
 [   0    0    0    6    0    0    0    0    0    0    2    0  526    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
  1128    9    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    70  272    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
97.82113821138212

F1 scores:
[       nan 0.975      0.99024581 0.97636732 0.99530516 0.98065984
 0.99392097 0.98039216 1.         0.89473684 0.98041475 0.98644374
 0.96425298 1.         0.96369073 0.86486486 0.96932515]

Kappa:
0.9751428209474745
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa7c4ca66a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.227, val_acc:0.407]
Epoch [2/120    avg_loss:1.672, val_acc:0.597]
Epoch [3/120    avg_loss:1.450, val_acc:0.660]
Epoch [4/120    avg_loss:1.076, val_acc:0.748]
Epoch [5/120    avg_loss:0.809, val_acc:0.754]
Epoch [6/120    avg_loss:0.793, val_acc:0.705]
Epoch [7/120    avg_loss:0.716, val_acc:0.780]
Epoch [8/120    avg_loss:0.672, val_acc:0.802]
Epoch [9/120    avg_loss:0.629, val_acc:0.832]
Epoch [10/120    avg_loss:0.549, val_acc:0.802]
Epoch [11/120    avg_loss:0.374, val_acc:0.876]
Epoch [12/120    avg_loss:0.382, val_acc:0.835]
Epoch [13/120    avg_loss:0.383, val_acc:0.876]
Epoch [14/120    avg_loss:0.274, val_acc:0.845]
Epoch [15/120    avg_loss:0.239, val_acc:0.905]
Epoch [16/120    avg_loss:0.252, val_acc:0.883]
Epoch [17/120    avg_loss:0.251, val_acc:0.883]
Epoch [18/120    avg_loss:0.201, val_acc:0.898]
Epoch [19/120    avg_loss:0.231, val_acc:0.907]
Epoch [20/120    avg_loss:0.151, val_acc:0.920]
Epoch [21/120    avg_loss:0.108, val_acc:0.920]
Epoch [22/120    avg_loss:0.082, val_acc:0.929]
Epoch [23/120    avg_loss:0.068, val_acc:0.943]
Epoch [24/120    avg_loss:0.121, val_acc:0.914]
Epoch [25/120    avg_loss:0.099, val_acc:0.939]
Epoch [26/120    avg_loss:0.102, val_acc:0.841]
Epoch [27/120    avg_loss:0.155, val_acc:0.938]
Epoch [28/120    avg_loss:0.184, val_acc:0.940]
Epoch [29/120    avg_loss:0.088, val_acc:0.944]
Epoch [30/120    avg_loss:0.055, val_acc:0.952]
Epoch [31/120    avg_loss:0.065, val_acc:0.957]
Epoch [32/120    avg_loss:0.048, val_acc:0.958]
Epoch [33/120    avg_loss:0.045, val_acc:0.963]
Epoch [34/120    avg_loss:0.025, val_acc:0.964]
Epoch [35/120    avg_loss:0.037, val_acc:0.949]
Epoch [36/120    avg_loss:0.035, val_acc:0.951]
Epoch [37/120    avg_loss:0.093, val_acc:0.959]
Epoch [38/120    avg_loss:0.048, val_acc:0.955]
Epoch [39/120    avg_loss:0.072, val_acc:0.948]
Epoch [40/120    avg_loss:0.041, val_acc:0.964]
Epoch [41/120    avg_loss:0.025, val_acc:0.967]
Epoch [42/120    avg_loss:0.024, val_acc:0.961]
Epoch [43/120    avg_loss:0.029, val_acc:0.954]
Epoch [44/120    avg_loss:0.029, val_acc:0.949]
Epoch [45/120    avg_loss:0.034, val_acc:0.963]
Epoch [46/120    avg_loss:0.038, val_acc:0.968]
Epoch [47/120    avg_loss:0.014, val_acc:0.958]
Epoch [48/120    avg_loss:0.017, val_acc:0.969]
Epoch [49/120    avg_loss:0.024, val_acc:0.959]
Epoch [50/120    avg_loss:0.019, val_acc:0.965]
Epoch [51/120    avg_loss:0.015, val_acc:0.968]
Epoch [52/120    avg_loss:0.008, val_acc:0.970]
Epoch [53/120    avg_loss:0.010, val_acc:0.970]
Epoch [54/120    avg_loss:0.011, val_acc:0.967]
Epoch [55/120    avg_loss:0.008, val_acc:0.975]
Epoch [56/120    avg_loss:0.010, val_acc:0.973]
Epoch [57/120    avg_loss:0.006, val_acc:0.974]
Epoch [58/120    avg_loss:0.019, val_acc:0.974]
Epoch [59/120    avg_loss:0.013, val_acc:0.973]
Epoch [60/120    avg_loss:0.024, val_acc:0.964]
Epoch [61/120    avg_loss:0.012, val_acc:0.973]
Epoch [62/120    avg_loss:0.027, val_acc:0.978]
Epoch [63/120    avg_loss:0.011, val_acc:0.968]
Epoch [64/120    avg_loss:0.014, val_acc:0.968]
Epoch [65/120    avg_loss:0.007, val_acc:0.967]
Epoch [66/120    avg_loss:0.006, val_acc:0.972]
Epoch [67/120    avg_loss:0.008, val_acc:0.972]
Epoch [68/120    avg_loss:0.007, val_acc:0.972]
Epoch [69/120    avg_loss:0.011, val_acc:0.973]
Epoch [70/120    avg_loss:0.007, val_acc:0.974]
Epoch [71/120    avg_loss:0.006, val_acc:0.974]
Epoch [72/120    avg_loss:0.013, val_acc:0.970]
Epoch [73/120    avg_loss:0.006, val_acc:0.970]
Epoch [74/120    avg_loss:0.034, val_acc:0.961]
Epoch [75/120    avg_loss:0.016, val_acc:0.969]
Epoch [76/120    avg_loss:0.011, val_acc:0.970]
Epoch [77/120    avg_loss:0.010, val_acc:0.971]
Epoch [78/120    avg_loss:0.006, val_acc:0.970]
Epoch [79/120    avg_loss:0.007, val_acc:0.972]
Epoch [80/120    avg_loss:0.005, val_acc:0.973]
Epoch [81/120    avg_loss:0.006, val_acc:0.973]
Epoch [82/120    avg_loss:0.005, val_acc:0.972]
Epoch [83/120    avg_loss:0.008, val_acc:0.972]
Epoch [84/120    avg_loss:0.007, val_acc:0.973]
Epoch [85/120    avg_loss:0.005, val_acc:0.972]
Epoch [86/120    avg_loss:0.003, val_acc:0.972]
Epoch [87/120    avg_loss:0.005, val_acc:0.972]
Epoch [88/120    avg_loss:0.005, val_acc:0.973]
Epoch [89/120    avg_loss:0.005, val_acc:0.973]
Epoch [90/120    avg_loss:0.006, val_acc:0.973]
Epoch [91/120    avg_loss:0.004, val_acc:0.972]
Epoch [92/120    avg_loss:0.004, val_acc:0.972]
Epoch [93/120    avg_loss:0.004, val_acc:0.973]
Epoch [94/120    avg_loss:0.005, val_acc:0.972]
Epoch [95/120    avg_loss:0.006, val_acc:0.973]
Epoch [96/120    avg_loss:0.005, val_acc:0.973]
Epoch [97/120    avg_loss:0.003, val_acc:0.973]
Epoch [98/120    avg_loss:0.003, val_acc:0.973]
Epoch [99/120    avg_loss:0.005, val_acc:0.973]
Epoch [100/120    avg_loss:0.003, val_acc:0.973]
Epoch [101/120    avg_loss:0.004, val_acc:0.973]
Epoch [102/120    avg_loss:0.004, val_acc:0.973]
Epoch [103/120    avg_loss:0.004, val_acc:0.973]
Epoch [104/120    avg_loss:0.004, val_acc:0.973]
Epoch [105/120    avg_loss:0.005, val_acc:0.973]
Epoch [106/120    avg_loss:0.004, val_acc:0.973]
Epoch [107/120    avg_loss:0.004, val_acc:0.973]
Epoch [108/120    avg_loss:0.004, val_acc:0.973]
Epoch [109/120    avg_loss:0.003, val_acc:0.973]
Epoch [110/120    avg_loss:0.005, val_acc:0.973]
Epoch [111/120    avg_loss:0.007, val_acc:0.973]
Epoch [112/120    avg_loss:0.005, val_acc:0.973]
Epoch [113/120    avg_loss:0.003, val_acc:0.973]
Epoch [114/120    avg_loss:0.005, val_acc:0.973]
Epoch [115/120    avg_loss:0.004, val_acc:0.973]
Epoch [116/120    avg_loss:0.004, val_acc:0.973]
Epoch [117/120    avg_loss:0.004, val_acc:0.973]
Epoch [118/120    avg_loss:0.005, val_acc:0.973]
Epoch [119/120    avg_loss:0.004, val_acc:0.973]
Epoch [120/120    avg_loss:0.004, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1265    0   12    0    0    0    0    0    1    7    0    0
     0    0    0]
 [   0    0    0  723    6    5    0    0    0    2    0    3    7    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    1    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0   15    0    0    0    1    0    0    0  841   15    0    0
     2    1    0]
 [   0    0   13    0    0    0    0    0    0    0    8 2169   20    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0    0    0  529    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    3    0    0    0    0    0    0    0
  1114   13    0]
 [   0    0    0    0    0    0   20    0    0    0    0    0    0    0
    32  295    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
97.70189701897019

F1 scores:
[       nan 0.975      0.98100039 0.98167006 0.95945946 0.98301246
 0.98132935 1.         0.99883586 0.88888889 0.97450753 0.98501362
 0.96532847 0.99730458 0.97420201 0.89665653 0.95757576]

Kappa:
0.9738074122518845
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6062713668>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.157, val_acc:0.515]
Epoch [2/120    avg_loss:1.552, val_acc:0.585]
Epoch [3/120    avg_loss:1.443, val_acc:0.680]
Epoch [4/120    avg_loss:1.021, val_acc:0.727]
Epoch [5/120    avg_loss:0.806, val_acc:0.718]
Epoch [6/120    avg_loss:0.629, val_acc:0.748]
Epoch [7/120    avg_loss:0.630, val_acc:0.819]
Epoch [8/120    avg_loss:0.595, val_acc:0.812]
Epoch [9/120    avg_loss:0.399, val_acc:0.777]
Epoch [10/120    avg_loss:0.282, val_acc:0.872]
Epoch [11/120    avg_loss:0.257, val_acc:0.856]
Epoch [12/120    avg_loss:0.261, val_acc:0.867]
Epoch [13/120    avg_loss:0.178, val_acc:0.897]
Epoch [14/120    avg_loss:0.180, val_acc:0.890]
Epoch [15/120    avg_loss:0.173, val_acc:0.902]
Epoch [16/120    avg_loss:0.152, val_acc:0.914]
Epoch [17/120    avg_loss:0.139, val_acc:0.905]
Epoch [18/120    avg_loss:0.170, val_acc:0.924]
Epoch [19/120    avg_loss:0.072, val_acc:0.936]
Epoch [20/120    avg_loss:0.096, val_acc:0.920]
Epoch [21/120    avg_loss:0.166, val_acc:0.923]
Epoch [22/120    avg_loss:0.067, val_acc:0.918]
Epoch [23/120    avg_loss:0.123, val_acc:0.927]
Epoch [24/120    avg_loss:0.081, val_acc:0.942]
Epoch [25/120    avg_loss:0.044, val_acc:0.944]
Epoch [26/120    avg_loss:0.060, val_acc:0.948]
Epoch [27/120    avg_loss:0.059, val_acc:0.952]
Epoch [28/120    avg_loss:0.034, val_acc:0.959]
Epoch [29/120    avg_loss:0.049, val_acc:0.940]
Epoch [30/120    avg_loss:0.041, val_acc:0.940]
Epoch [31/120    avg_loss:0.022, val_acc:0.956]
Epoch [32/120    avg_loss:0.016, val_acc:0.956]
Epoch [33/120    avg_loss:0.015, val_acc:0.963]
Epoch [34/120    avg_loss:0.026, val_acc:0.955]
Epoch [35/120    avg_loss:0.038, val_acc:0.945]
Epoch [36/120    avg_loss:0.016, val_acc:0.960]
Epoch [37/120    avg_loss:0.022, val_acc:0.956]
Epoch [38/120    avg_loss:0.030, val_acc:0.958]
Epoch [39/120    avg_loss:0.017, val_acc:0.958]
Epoch [40/120    avg_loss:0.009, val_acc:0.968]
Epoch [41/120    avg_loss:0.009, val_acc:0.972]
Epoch [42/120    avg_loss:0.008, val_acc:0.968]
Epoch [43/120    avg_loss:0.010, val_acc:0.969]
Epoch [44/120    avg_loss:0.008, val_acc:0.964]
Epoch [45/120    avg_loss:0.022, val_acc:0.955]
Epoch [46/120    avg_loss:0.023, val_acc:0.948]
Epoch [47/120    avg_loss:0.034, val_acc:0.965]
Epoch [48/120    avg_loss:0.016, val_acc:0.970]
Epoch [49/120    avg_loss:0.008, val_acc:0.972]
Epoch [50/120    avg_loss:0.007, val_acc:0.964]
Epoch [51/120    avg_loss:0.009, val_acc:0.969]
Epoch [52/120    avg_loss:0.011, val_acc:0.971]
Epoch [53/120    avg_loss:0.006, val_acc:0.974]
Epoch [54/120    avg_loss:0.018, val_acc:0.961]
Epoch [55/120    avg_loss:0.018, val_acc:0.960]
Epoch [56/120    avg_loss:0.008, val_acc:0.968]
Epoch [57/120    avg_loss:0.004, val_acc:0.970]
Epoch [58/120    avg_loss:0.004, val_acc:0.972]
Epoch [59/120    avg_loss:0.006, val_acc:0.975]
Epoch [60/120    avg_loss:0.005, val_acc:0.973]
Epoch [61/120    avg_loss:0.008, val_acc:0.977]
Epoch [62/120    avg_loss:0.011, val_acc:0.964]
Epoch [63/120    avg_loss:0.008, val_acc:0.975]
Epoch [64/120    avg_loss:0.008, val_acc:0.961]
Epoch [65/120    avg_loss:0.005, val_acc:0.971]
Epoch [66/120    avg_loss:0.008, val_acc:0.970]
Epoch [67/120    avg_loss:0.007, val_acc:0.970]
Epoch [68/120    avg_loss:0.004, val_acc:0.973]
Epoch [69/120    avg_loss:0.008, val_acc:0.969]
Epoch [70/120    avg_loss:0.003, val_acc:0.967]
Epoch [71/120    avg_loss:0.012, val_acc:0.970]
Epoch [72/120    avg_loss:0.007, val_acc:0.972]
Epoch [73/120    avg_loss:0.004, val_acc:0.977]
Epoch [74/120    avg_loss:0.003, val_acc:0.977]
Epoch [75/120    avg_loss:0.009, val_acc:0.977]
Epoch [76/120    avg_loss:0.005, val_acc:0.972]
Epoch [77/120    avg_loss:0.005, val_acc:0.976]
Epoch [78/120    avg_loss:0.013, val_acc:0.975]
Epoch [79/120    avg_loss:0.008, val_acc:0.963]
Epoch [80/120    avg_loss:0.004, val_acc:0.973]
Epoch [81/120    avg_loss:0.002, val_acc:0.970]
Epoch [82/120    avg_loss:0.004, val_acc:0.974]
Epoch [83/120    avg_loss:0.003, val_acc:0.972]
Epoch [84/120    avg_loss:0.003, val_acc:0.975]
Epoch [85/120    avg_loss:0.005, val_acc:0.968]
Epoch [86/120    avg_loss:0.008, val_acc:0.969]
Epoch [87/120    avg_loss:0.014, val_acc:0.973]
Epoch [88/120    avg_loss:0.003, val_acc:0.977]
Epoch [89/120    avg_loss:0.002, val_acc:0.977]
Epoch [90/120    avg_loss:0.006, val_acc:0.966]
Epoch [91/120    avg_loss:0.004, val_acc:0.974]
Epoch [92/120    avg_loss:0.004, val_acc:0.975]
Epoch [93/120    avg_loss:0.006, val_acc:0.968]
Epoch [94/120    avg_loss:0.004, val_acc:0.976]
Epoch [95/120    avg_loss:0.002, val_acc:0.980]
Epoch [96/120    avg_loss:0.006, val_acc:0.971]
Epoch [97/120    avg_loss:0.004, val_acc:0.972]
Epoch [98/120    avg_loss:0.003, val_acc:0.981]
Epoch [99/120    avg_loss:0.002, val_acc:0.980]
Epoch [100/120    avg_loss:0.003, val_acc:0.974]
Epoch [101/120    avg_loss:0.003, val_acc:0.978]
Epoch [102/120    avg_loss:0.003, val_acc:0.977]
Epoch [103/120    avg_loss:0.003, val_acc:0.981]
Epoch [104/120    avg_loss:0.004, val_acc:0.977]
Epoch [105/120    avg_loss:0.002, val_acc:0.978]
Epoch [106/120    avg_loss:0.001, val_acc:0.976]
Epoch [107/120    avg_loss:0.003, val_acc:0.979]
Epoch [108/120    avg_loss:0.001, val_acc:0.982]
Epoch [109/120    avg_loss:0.002, val_acc:0.981]
Epoch [110/120    avg_loss:0.001, val_acc:0.981]
Epoch [111/120    avg_loss:0.004, val_acc:0.973]
Epoch [112/120    avg_loss:0.008, val_acc:0.976]
Epoch [113/120    avg_loss:0.013, val_acc:0.980]
Epoch [114/120    avg_loss:0.005, val_acc:0.972]
Epoch [115/120    avg_loss:0.004, val_acc:0.976]
Epoch [116/120    avg_loss:0.004, val_acc:0.973]
Epoch [117/120    avg_loss:0.004, val_acc:0.979]
Epoch [118/120    avg_loss:0.002, val_acc:0.979]
Epoch [119/120    avg_loss:0.002, val_acc:0.980]
Epoch [120/120    avg_loss:0.003, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1266    6    4    1    1    0    0    0    4    3    0    0
     0    0    0]
 [   0    0    0  743    0    0    2    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    1    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    3    1    0    0    0    0    0    0  848   19    1    0
     0    3    0]
 [   0    0   15    0    0    0    0    0    0    0    5 2165   25    0
     0    0    0]
 [   0    0    1    8    2    0    0    0    0    0    0    0  520    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    1   12    0    0    0    0    0    0    0
    62  272    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    7    0
     0    0   77]]

Accuracy:
97.78861788617886

F1 scores:
[       nan 0.975      0.98483081 0.98671979 0.98611111 0.99770642
 0.98796992 1.         1.         0.88888889 0.97864974 0.98476234
 0.95676173 1.         0.96954097 0.86075949 0.94478528]

Kappa:
0.9747863103290849
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5431b70668>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.216, val_acc:0.477]
Epoch [2/120    avg_loss:1.557, val_acc:0.623]
Epoch [3/120    avg_loss:1.217, val_acc:0.623]
Epoch [4/120    avg_loss:0.913, val_acc:0.584]
Epoch [5/120    avg_loss:0.896, val_acc:0.756]
Epoch [6/120    avg_loss:0.798, val_acc:0.739]
Epoch [7/120    avg_loss:0.638, val_acc:0.787]
Epoch [8/120    avg_loss:0.562, val_acc:0.722]
Epoch [9/120    avg_loss:0.389, val_acc:0.848]
Epoch [10/120    avg_loss:0.317, val_acc:0.850]
Epoch [11/120    avg_loss:0.276, val_acc:0.872]
Epoch [12/120    avg_loss:0.245, val_acc:0.892]
Epoch [13/120    avg_loss:0.458, val_acc:0.808]
Epoch [14/120    avg_loss:0.402, val_acc:0.886]
Epoch [15/120    avg_loss:0.279, val_acc:0.874]
Epoch [16/120    avg_loss:0.173, val_acc:0.914]
Epoch [17/120    avg_loss:0.165, val_acc:0.926]
Epoch [18/120    avg_loss:0.134, val_acc:0.912]
Epoch [19/120    avg_loss:0.133, val_acc:0.928]
Epoch [20/120    avg_loss:0.166, val_acc:0.914]
Epoch [21/120    avg_loss:0.080, val_acc:0.936]
Epoch [22/120    avg_loss:0.058, val_acc:0.920]
Epoch [23/120    avg_loss:0.081, val_acc:0.914]
Epoch [24/120    avg_loss:0.121, val_acc:0.931]
Epoch [25/120    avg_loss:0.083, val_acc:0.931]
Epoch [26/120    avg_loss:0.375, val_acc:0.891]
Epoch [27/120    avg_loss:0.148, val_acc:0.894]
Epoch [28/120    avg_loss:0.114, val_acc:0.934]
Epoch [29/120    avg_loss:0.076, val_acc:0.942]
Epoch [30/120    avg_loss:0.064, val_acc:0.930]
Epoch [31/120    avg_loss:0.137, val_acc:0.940]
Epoch [32/120    avg_loss:0.089, val_acc:0.950]
Epoch [33/120    avg_loss:0.050, val_acc:0.952]
Epoch [34/120    avg_loss:0.054, val_acc:0.955]
Epoch [35/120    avg_loss:0.036, val_acc:0.950]
Epoch [36/120    avg_loss:0.037, val_acc:0.960]
Epoch [37/120    avg_loss:0.029, val_acc:0.957]
Epoch [38/120    avg_loss:0.028, val_acc:0.959]
Epoch [39/120    avg_loss:0.026, val_acc:0.952]
Epoch [40/120    avg_loss:0.029, val_acc:0.959]
Epoch [41/120    avg_loss:0.019, val_acc:0.949]
Epoch [42/120    avg_loss:0.030, val_acc:0.958]
Epoch [43/120    avg_loss:0.028, val_acc:0.951]
Epoch [44/120    avg_loss:0.032, val_acc:0.960]
Epoch [45/120    avg_loss:0.021, val_acc:0.954]
Epoch [46/120    avg_loss:0.023, val_acc:0.952]
Epoch [47/120    avg_loss:0.027, val_acc:0.959]
Epoch [48/120    avg_loss:0.028, val_acc:0.959]
Epoch [49/120    avg_loss:0.030, val_acc:0.959]
Epoch [50/120    avg_loss:0.016, val_acc:0.967]
Epoch [51/120    avg_loss:0.036, val_acc:0.959]
Epoch [52/120    avg_loss:0.041, val_acc:0.969]
Epoch [53/120    avg_loss:0.014, val_acc:0.960]
Epoch [54/120    avg_loss:0.013, val_acc:0.965]
Epoch [55/120    avg_loss:0.027, val_acc:0.964]
Epoch [56/120    avg_loss:0.016, val_acc:0.969]
Epoch [57/120    avg_loss:0.010, val_acc:0.973]
Epoch [58/120    avg_loss:0.011, val_acc:0.963]
Epoch [59/120    avg_loss:0.006, val_acc:0.967]
Epoch [60/120    avg_loss:0.017, val_acc:0.968]
Epoch [61/120    avg_loss:0.018, val_acc:0.965]
Epoch [62/120    avg_loss:0.008, val_acc:0.968]
Epoch [63/120    avg_loss:0.006, val_acc:0.970]
Epoch [64/120    avg_loss:0.006, val_acc:0.967]
Epoch [65/120    avg_loss:0.007, val_acc:0.969]
Epoch [66/120    avg_loss:0.020, val_acc:0.970]
Epoch [67/120    avg_loss:0.005, val_acc:0.968]
Epoch [68/120    avg_loss:0.008, val_acc:0.977]
Epoch [69/120    avg_loss:0.005, val_acc:0.973]
Epoch [70/120    avg_loss:0.010, val_acc:0.970]
Epoch [71/120    avg_loss:0.007, val_acc:0.972]
Epoch [72/120    avg_loss:0.004, val_acc:0.972]
Epoch [73/120    avg_loss:0.005, val_acc:0.973]
Epoch [74/120    avg_loss:0.006, val_acc:0.959]
Epoch [75/120    avg_loss:0.008, val_acc:0.970]
Epoch [76/120    avg_loss:0.006, val_acc:0.972]
Epoch [77/120    avg_loss:0.003, val_acc:0.970]
Epoch [78/120    avg_loss:0.007, val_acc:0.963]
Epoch [79/120    avg_loss:0.010, val_acc:0.970]
Epoch [80/120    avg_loss:0.006, val_acc:0.969]
Epoch [81/120    avg_loss:0.003, val_acc:0.970]
Epoch [82/120    avg_loss:0.005, val_acc:0.971]
Epoch [83/120    avg_loss:0.003, val_acc:0.972]
Epoch [84/120    avg_loss:0.002, val_acc:0.972]
Epoch [85/120    avg_loss:0.003, val_acc:0.974]
Epoch [86/120    avg_loss:0.003, val_acc:0.973]
Epoch [87/120    avg_loss:0.002, val_acc:0.973]
Epoch [88/120    avg_loss:0.004, val_acc:0.974]
Epoch [89/120    avg_loss:0.002, val_acc:0.973]
Epoch [90/120    avg_loss:0.003, val_acc:0.974]
Epoch [91/120    avg_loss:0.002, val_acc:0.974]
Epoch [92/120    avg_loss:0.002, val_acc:0.974]
Epoch [93/120    avg_loss:0.002, val_acc:0.974]
Epoch [94/120    avg_loss:0.003, val_acc:0.972]
Epoch [95/120    avg_loss:0.002, val_acc:0.972]
Epoch [96/120    avg_loss:0.005, val_acc:0.972]
Epoch [97/120    avg_loss:0.003, val_acc:0.972]
Epoch [98/120    avg_loss:0.002, val_acc:0.972]
Epoch [99/120    avg_loss:0.004, val_acc:0.972]
Epoch [100/120    avg_loss:0.003, val_acc:0.972]
Epoch [101/120    avg_loss:0.003, val_acc:0.972]
Epoch [102/120    avg_loss:0.002, val_acc:0.972]
Epoch [103/120    avg_loss:0.003, val_acc:0.972]
Epoch [104/120    avg_loss:0.002, val_acc:0.972]
Epoch [105/120    avg_loss:0.003, val_acc:0.972]
Epoch [106/120    avg_loss:0.003, val_acc:0.972]
Epoch [107/120    avg_loss:0.003, val_acc:0.972]
Epoch [108/120    avg_loss:0.004, val_acc:0.972]
Epoch [109/120    avg_loss:0.002, val_acc:0.972]
Epoch [110/120    avg_loss:0.006, val_acc:0.972]
Epoch [111/120    avg_loss:0.002, val_acc:0.972]
Epoch [112/120    avg_loss:0.004, val_acc:0.972]
Epoch [113/120    avg_loss:0.002, val_acc:0.972]
Epoch [114/120    avg_loss:0.002, val_acc:0.972]
Epoch [115/120    avg_loss:0.004, val_acc:0.972]
Epoch [116/120    avg_loss:0.003, val_acc:0.972]
Epoch [117/120    avg_loss:0.004, val_acc:0.972]
Epoch [118/120    avg_loss:0.002, val_acc:0.972]
Epoch [119/120    avg_loss:0.004, val_acc:0.972]
Epoch [120/120    avg_loss:0.003, val_acc:0.972]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1269    3    3    1    3    0    0    0    1    5    0    0
     0    0    0]
 [   0    0    0  720    1    0    0    0    0    0    1    5   20    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    2    0    0  425    0    2    0    0    0    0    0    0
     6    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0  429    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   14    0    0    0    0    0    0    0  827   29    5    0
     0    0    0]
 [   0    0   16    0    0    0    0    0    0    0    0 2181   13    0
     0    0    0]
 [   0    0    2    1    0    0    0    0    0    0    0    0  527    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1126   10    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
    52  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.68021680216802

F1 scores:
[       nan 0.98765432 0.98030127 0.97826087 0.98834499 0.9837963
 0.99168556 0.96153846 0.99883586 1.         0.97008798 0.98442789
 0.95818182 1.         0.96943607 0.89164087 0.97647059]

Kappa:
0.9735352434573944
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:34--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff720d14668>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.204, val_acc:0.487]
Epoch [2/120    avg_loss:1.485, val_acc:0.500]
Epoch [3/120    avg_loss:1.378, val_acc:0.620]
Epoch [4/120    avg_loss:1.187, val_acc:0.690]
Epoch [5/120    avg_loss:0.848, val_acc:0.679]
Epoch [6/120    avg_loss:0.690, val_acc:0.758]
Epoch [7/120    avg_loss:0.466, val_acc:0.845]
Epoch [8/120    avg_loss:0.548, val_acc:0.848]
Epoch [9/120    avg_loss:0.444, val_acc:0.852]
Epoch [10/120    avg_loss:0.503, val_acc:0.769]
Epoch [11/120    avg_loss:0.451, val_acc:0.863]
Epoch [12/120    avg_loss:0.280, val_acc:0.817]
Epoch [13/120    avg_loss:0.225, val_acc:0.810]
Epoch [14/120    avg_loss:0.148, val_acc:0.916]
Epoch [15/120    avg_loss:0.109, val_acc:0.924]
Epoch [16/120    avg_loss:0.115, val_acc:0.903]
Epoch [17/120    avg_loss:0.121, val_acc:0.920]
Epoch [18/120    avg_loss:0.081, val_acc:0.930]
Epoch [19/120    avg_loss:0.116, val_acc:0.914]
Epoch [20/120    avg_loss:0.105, val_acc:0.920]
Epoch [21/120    avg_loss:0.086, val_acc:0.926]
Epoch [22/120    avg_loss:0.073, val_acc:0.930]
Epoch [23/120    avg_loss:0.081, val_acc:0.940]
Epoch [24/120    avg_loss:0.042, val_acc:0.946]
Epoch [25/120    avg_loss:0.045, val_acc:0.931]
Epoch [26/120    avg_loss:0.045, val_acc:0.939]
Epoch [27/120    avg_loss:0.044, val_acc:0.942]
Epoch [28/120    avg_loss:0.063, val_acc:0.957]
Epoch [29/120    avg_loss:0.033, val_acc:0.940]
Epoch [30/120    avg_loss:0.038, val_acc:0.957]
Epoch [31/120    avg_loss:0.056, val_acc:0.949]
Epoch [32/120    avg_loss:0.040, val_acc:0.956]
Epoch [33/120    avg_loss:0.038, val_acc:0.955]
Epoch [34/120    avg_loss:0.031, val_acc:0.965]
Epoch [35/120    avg_loss:0.026, val_acc:0.967]
Epoch [36/120    avg_loss:0.032, val_acc:0.959]
Epoch [37/120    avg_loss:0.025, val_acc:0.967]
Epoch [38/120    avg_loss:0.026, val_acc:0.969]
Epoch [39/120    avg_loss:0.023, val_acc:0.960]
Epoch [40/120    avg_loss:0.018, val_acc:0.965]
Epoch [41/120    avg_loss:0.054, val_acc:0.938]
Epoch [42/120    avg_loss:0.056, val_acc:0.954]
Epoch [43/120    avg_loss:0.045, val_acc:0.933]
Epoch [44/120    avg_loss:0.038, val_acc:0.939]
Epoch [45/120    avg_loss:0.049, val_acc:0.957]
Epoch [46/120    avg_loss:0.032, val_acc:0.954]
Epoch [47/120    avg_loss:0.019, val_acc:0.965]
Epoch [48/120    avg_loss:0.022, val_acc:0.968]
Epoch [49/120    avg_loss:0.011, val_acc:0.967]
Epoch [50/120    avg_loss:0.009, val_acc:0.967]
Epoch [51/120    avg_loss:0.010, val_acc:0.963]
Epoch [52/120    avg_loss:0.018, val_acc:0.969]
Epoch [53/120    avg_loss:0.006, val_acc:0.969]
Epoch [54/120    avg_loss:0.010, val_acc:0.969]
Epoch [55/120    avg_loss:0.006, val_acc:0.969]
Epoch [56/120    avg_loss:0.006, val_acc:0.970]
Epoch [57/120    avg_loss:0.006, val_acc:0.971]
Epoch [58/120    avg_loss:0.007, val_acc:0.971]
Epoch [59/120    avg_loss:0.007, val_acc:0.971]
Epoch [60/120    avg_loss:0.008, val_acc:0.969]
Epoch [61/120    avg_loss:0.008, val_acc:0.969]
Epoch [62/120    avg_loss:0.005, val_acc:0.969]
Epoch [63/120    avg_loss:0.007, val_acc:0.969]
Epoch [64/120    avg_loss:0.006, val_acc:0.969]
Epoch [65/120    avg_loss:0.006, val_acc:0.968]
Epoch [66/120    avg_loss:0.005, val_acc:0.968]
Epoch [67/120    avg_loss:0.004, val_acc:0.968]
Epoch [68/120    avg_loss:0.006, val_acc:0.968]
Epoch [69/120    avg_loss:0.004, val_acc:0.969]
Epoch [70/120    avg_loss:0.004, val_acc:0.969]
Epoch [71/120    avg_loss:0.006, val_acc:0.967]
Epoch [72/120    avg_loss:0.005, val_acc:0.968]
Epoch [73/120    avg_loss:0.004, val_acc:0.968]
Epoch [74/120    avg_loss:0.005, val_acc:0.968]
Epoch [75/120    avg_loss:0.005, val_acc:0.968]
Epoch [76/120    avg_loss:0.008, val_acc:0.968]
Epoch [77/120    avg_loss:0.003, val_acc:0.969]
Epoch [78/120    avg_loss:0.005, val_acc:0.969]
Epoch [79/120    avg_loss:0.005, val_acc:0.969]
Epoch [80/120    avg_loss:0.007, val_acc:0.969]
Epoch [81/120    avg_loss:0.004, val_acc:0.969]
Epoch [82/120    avg_loss:0.007, val_acc:0.969]
Epoch [83/120    avg_loss:0.004, val_acc:0.969]
Epoch [84/120    avg_loss:0.004, val_acc:0.969]
Epoch [85/120    avg_loss:0.006, val_acc:0.970]
Epoch [86/120    avg_loss:0.005, val_acc:0.970]
Epoch [87/120    avg_loss:0.005, val_acc:0.970]
Epoch [88/120    avg_loss:0.007, val_acc:0.970]
Epoch [89/120    avg_loss:0.003, val_acc:0.970]
Epoch [90/120    avg_loss:0.007, val_acc:0.970]
Epoch [91/120    avg_loss:0.008, val_acc:0.970]
Epoch [92/120    avg_loss:0.008, val_acc:0.970]
Epoch [93/120    avg_loss:0.006, val_acc:0.970]
Epoch [94/120    avg_loss:0.004, val_acc:0.970]
Epoch [95/120    avg_loss:0.007, val_acc:0.970]
Epoch [96/120    avg_loss:0.004, val_acc:0.970]
Epoch [97/120    avg_loss:0.004, val_acc:0.970]
Epoch [98/120    avg_loss:0.006, val_acc:0.970]
Epoch [99/120    avg_loss:0.004, val_acc:0.970]
Epoch [100/120    avg_loss:0.005, val_acc:0.970]
Epoch [101/120    avg_loss:0.007, val_acc:0.970]
Epoch [102/120    avg_loss:0.008, val_acc:0.970]
Epoch [103/120    avg_loss:0.007, val_acc:0.970]
Epoch [104/120    avg_loss:0.005, val_acc:0.970]
Epoch [105/120    avg_loss:0.007, val_acc:0.970]
Epoch [106/120    avg_loss:0.005, val_acc:0.970]
Epoch [107/120    avg_loss:0.006, val_acc:0.970]
Epoch [108/120    avg_loss:0.006, val_acc:0.970]
Epoch [109/120    avg_loss:0.004, val_acc:0.970]
Epoch [110/120    avg_loss:0.006, val_acc:0.970]
Epoch [111/120    avg_loss:0.006, val_acc:0.970]
Epoch [112/120    avg_loss:0.009, val_acc:0.970]
Epoch [113/120    avg_loss:0.004, val_acc:0.970]
Epoch [114/120    avg_loss:0.004, val_acc:0.970]
Epoch [115/120    avg_loss:0.005, val_acc:0.970]
Epoch [116/120    avg_loss:0.005, val_acc:0.970]
Epoch [117/120    avg_loss:0.003, val_acc:0.970]
Epoch [118/120    avg_loss:0.005, val_acc:0.970]
Epoch [119/120    avg_loss:0.005, val_acc:0.970]
Epoch [120/120    avg_loss:0.005, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1273    0    0    0    2    0    0    0    1    9    0    0
     0    0    0]
 [   0    0    0  728    0    0    0    0    0    5    0    5    8    1
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    2    0    0    0    0  835   29    2    0
     0    1    0]
 [   0    0    5    0    0    0    0    0    0    0    2 2188   14    0
     0    0    1]
 [   0    0    0    2    1    0    0    0    0    0    0    1  527    0
     0    3    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1126   12    0]
 [   0    0    0    0    0    1    5    0    0    0    0    0    0    0
    89  252    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.66937669376694

F1 scores:
[       nan 0.98765432 0.99066148 0.98511502 0.99530516 0.99425947
 0.99393939 1.         0.99883856 0.87804878 0.97489784 0.9849201
 0.96964121 0.99730458 0.95585739 0.8195122  0.98203593]

Kappa:
0.9733991869791014
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7534c68630>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.132, val_acc:0.608]
Epoch [2/120    avg_loss:1.561, val_acc:0.582]
Epoch [3/120    avg_loss:1.272, val_acc:0.659]
Epoch [4/120    avg_loss:0.946, val_acc:0.734]
Epoch [5/120    avg_loss:0.944, val_acc:0.707]
Epoch [6/120    avg_loss:0.779, val_acc:0.766]
Epoch [7/120    avg_loss:0.585, val_acc:0.788]
Epoch [8/120    avg_loss:0.733, val_acc:0.833]
Epoch [9/120    avg_loss:0.413, val_acc:0.824]
Epoch [10/120    avg_loss:0.267, val_acc:0.850]
Epoch [11/120    avg_loss:0.259, val_acc:0.885]
Epoch [12/120    avg_loss:0.254, val_acc:0.865]
Epoch [13/120    avg_loss:0.233, val_acc:0.892]
Epoch [14/120    avg_loss:0.131, val_acc:0.905]
Epoch [15/120    avg_loss:0.214, val_acc:0.893]
Epoch [16/120    avg_loss:0.241, val_acc:0.900]
Epoch [17/120    avg_loss:0.436, val_acc:0.847]
Epoch [18/120    avg_loss:0.219, val_acc:0.906]
Epoch [19/120    avg_loss:0.211, val_acc:0.895]
Epoch [20/120    avg_loss:0.137, val_acc:0.917]
Epoch [21/120    avg_loss:0.106, val_acc:0.933]
Epoch [22/120    avg_loss:0.079, val_acc:0.922]
Epoch [23/120    avg_loss:0.064, val_acc:0.914]
Epoch [24/120    avg_loss:0.111, val_acc:0.934]
Epoch [25/120    avg_loss:0.113, val_acc:0.957]
Epoch [26/120    avg_loss:0.064, val_acc:0.953]
Epoch [27/120    avg_loss:0.055, val_acc:0.951]
Epoch [28/120    avg_loss:0.048, val_acc:0.943]
Epoch [29/120    avg_loss:0.064, val_acc:0.943]
Epoch [30/120    avg_loss:0.047, val_acc:0.956]
Epoch [31/120    avg_loss:0.026, val_acc:0.962]
Epoch [32/120    avg_loss:0.018, val_acc:0.932]
Epoch [33/120    avg_loss:0.034, val_acc:0.956]
Epoch [34/120    avg_loss:0.029, val_acc:0.959]
Epoch [35/120    avg_loss:0.037, val_acc:0.961]
Epoch [36/120    avg_loss:0.018, val_acc:0.961]
Epoch [37/120    avg_loss:0.015, val_acc:0.967]
Epoch [38/120    avg_loss:0.017, val_acc:0.961]
Epoch [39/120    avg_loss:0.075, val_acc:0.939]
Epoch [40/120    avg_loss:0.035, val_acc:0.962]
Epoch [41/120    avg_loss:0.014, val_acc:0.962]
Epoch [42/120    avg_loss:0.015, val_acc:0.959]
Epoch [43/120    avg_loss:0.010, val_acc:0.966]
Epoch [44/120    avg_loss:0.009, val_acc:0.966]
Epoch [45/120    avg_loss:0.008, val_acc:0.970]
Epoch [46/120    avg_loss:0.021, val_acc:0.967]
Epoch [47/120    avg_loss:0.017, val_acc:0.965]
Epoch [48/120    avg_loss:0.019, val_acc:0.955]
Epoch [49/120    avg_loss:0.025, val_acc:0.965]
Epoch [50/120    avg_loss:0.014, val_acc:0.966]
Epoch [51/120    avg_loss:0.013, val_acc:0.973]
Epoch [52/120    avg_loss:0.028, val_acc:0.967]
Epoch [53/120    avg_loss:0.011, val_acc:0.969]
Epoch [54/120    avg_loss:0.009, val_acc:0.971]
Epoch [55/120    avg_loss:0.011, val_acc:0.967]
Epoch [56/120    avg_loss:0.020, val_acc:0.957]
Epoch [57/120    avg_loss:0.009, val_acc:0.975]
Epoch [58/120    avg_loss:0.011, val_acc:0.975]
Epoch [59/120    avg_loss:0.018, val_acc:0.967]
Epoch [60/120    avg_loss:0.006, val_acc:0.971]
Epoch [61/120    avg_loss:0.009, val_acc:0.972]
Epoch [62/120    avg_loss:0.009, val_acc:0.974]
Epoch [63/120    avg_loss:0.016, val_acc:0.968]
Epoch [64/120    avg_loss:0.008, val_acc:0.968]
Epoch [65/120    avg_loss:0.017, val_acc:0.968]
Epoch [66/120    avg_loss:0.007, val_acc:0.967]
Epoch [67/120    avg_loss:0.012, val_acc:0.959]
Epoch [68/120    avg_loss:0.008, val_acc:0.969]
Epoch [69/120    avg_loss:0.005, val_acc:0.973]
Epoch [70/120    avg_loss:0.004, val_acc:0.976]
Epoch [71/120    avg_loss:0.018, val_acc:0.968]
Epoch [72/120    avg_loss:0.006, val_acc:0.969]
Epoch [73/120    avg_loss:0.003, val_acc:0.975]
Epoch [74/120    avg_loss:0.012, val_acc:0.967]
Epoch [75/120    avg_loss:0.011, val_acc:0.966]
Epoch [76/120    avg_loss:0.009, val_acc:0.972]
Epoch [77/120    avg_loss:0.009, val_acc:0.973]
Epoch [78/120    avg_loss:0.005, val_acc:0.975]
Epoch [79/120    avg_loss:0.003, val_acc:0.975]
Epoch [80/120    avg_loss:0.004, val_acc:0.980]
Epoch [81/120    avg_loss:0.003, val_acc:0.981]
Epoch [82/120    avg_loss:0.005, val_acc:0.976]
Epoch [83/120    avg_loss:0.008, val_acc:0.972]
Epoch [84/120    avg_loss:0.018, val_acc:0.949]
Epoch [85/120    avg_loss:0.013, val_acc:0.968]
Epoch [86/120    avg_loss:0.009, val_acc:0.976]
Epoch [87/120    avg_loss:0.004, val_acc:0.975]
Epoch [88/120    avg_loss:0.004, val_acc:0.977]
Epoch [89/120    avg_loss:0.007, val_acc:0.970]
Epoch [90/120    avg_loss:0.010, val_acc:0.981]
Epoch [91/120    avg_loss:0.017, val_acc:0.958]
Epoch [92/120    avg_loss:0.016, val_acc:0.969]
Epoch [93/120    avg_loss:0.005, val_acc:0.973]
Epoch [94/120    avg_loss:0.004, val_acc:0.975]
Epoch [95/120    avg_loss:0.005, val_acc:0.977]
Epoch [96/120    avg_loss:0.007, val_acc:0.970]
Epoch [97/120    avg_loss:0.011, val_acc:0.961]
Epoch [98/120    avg_loss:0.021, val_acc:0.968]
Epoch [99/120    avg_loss:0.009, val_acc:0.974]
Epoch [100/120    avg_loss:0.007, val_acc:0.982]
Epoch [101/120    avg_loss:0.006, val_acc:0.977]
Epoch [102/120    avg_loss:0.003, val_acc:0.981]
Epoch [103/120    avg_loss:0.004, val_acc:0.980]
Epoch [104/120    avg_loss:0.003, val_acc:0.984]
Epoch [105/120    avg_loss:0.004, val_acc:0.983]
Epoch [106/120    avg_loss:0.004, val_acc:0.980]
Epoch [107/120    avg_loss:0.003, val_acc:0.978]
Epoch [108/120    avg_loss:0.006, val_acc:0.977]
Epoch [109/120    avg_loss:0.014, val_acc:0.975]
Epoch [110/120    avg_loss:0.004, val_acc:0.978]
Epoch [111/120    avg_loss:0.005, val_acc:0.968]
Epoch [112/120    avg_loss:0.006, val_acc:0.977]
Epoch [113/120    avg_loss:0.002, val_acc:0.978]
Epoch [114/120    avg_loss:0.002, val_acc:0.981]
Epoch [115/120    avg_loss:0.003, val_acc:0.987]
Epoch [116/120    avg_loss:0.002, val_acc:0.983]
Epoch [117/120    avg_loss:0.002, val_acc:0.985]
Epoch [118/120    avg_loss:0.003, val_acc:0.983]
Epoch [119/120    avg_loss:0.002, val_acc:0.982]
Epoch [120/120    avg_loss:0.002, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1266    5    0    0    0    0    0    0    0   14    0    0
     0    0    0]
 [   0    0    0  743    0    0    0    0    0    2    0    0    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    0  856   16    0    0
     0    1    0]
 [   0    0    1    0    0    0    1    0    0    0    5 2187   14    0
     1    0    1]
 [   0    0    0    2    0    0    0    0    0    0    1    0  530    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1123   13    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    44  295    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.44986449864498

F1 scores:
[       nan 0.975      0.99099804 0.99198932 1.         0.99541284
 0.99319728 1.         0.99883586 0.91891892 0.98504028 0.98802801
 0.97966728 1.         0.97313692 0.89939024 0.98224852]

Kappa:
0.9823214783233658
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa0f2e46630>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.261, val_acc:0.390]
Epoch [2/120    avg_loss:1.658, val_acc:0.611]
Epoch [3/120    avg_loss:1.364, val_acc:0.646]
Epoch [4/120    avg_loss:1.155, val_acc:0.719]
Epoch [5/120    avg_loss:0.997, val_acc:0.757]
Epoch [6/120    avg_loss:0.682, val_acc:0.835]
Epoch [7/120    avg_loss:0.783, val_acc:0.736]
Epoch [8/120    avg_loss:0.591, val_acc:0.853]
Epoch [9/120    avg_loss:0.512, val_acc:0.723]
Epoch [10/120    avg_loss:0.482, val_acc:0.819]
Epoch [11/120    avg_loss:0.365, val_acc:0.855]
Epoch [12/120    avg_loss:0.196, val_acc:0.887]
Epoch [13/120    avg_loss:0.230, val_acc:0.897]
Epoch [14/120    avg_loss:0.148, val_acc:0.931]
Epoch [15/120    avg_loss:0.120, val_acc:0.934]
Epoch [16/120    avg_loss:0.276, val_acc:0.892]
Epoch [17/120    avg_loss:0.305, val_acc:0.898]
Epoch [18/120    avg_loss:0.188, val_acc:0.933]
Epoch [19/120    avg_loss:0.095, val_acc:0.939]
Epoch [20/120    avg_loss:0.085, val_acc:0.941]
Epoch [21/120    avg_loss:0.079, val_acc:0.929]
Epoch [22/120    avg_loss:0.081, val_acc:0.954]
Epoch [23/120    avg_loss:0.075, val_acc:0.943]
Epoch [24/120    avg_loss:0.109, val_acc:0.952]
Epoch [25/120    avg_loss:0.113, val_acc:0.908]
Epoch [26/120    avg_loss:0.093, val_acc:0.963]
Epoch [27/120    avg_loss:0.048, val_acc:0.961]
Epoch [28/120    avg_loss:0.044, val_acc:0.954]
Epoch [29/120    avg_loss:0.045, val_acc:0.961]
Epoch [30/120    avg_loss:0.029, val_acc:0.966]
Epoch [31/120    avg_loss:0.032, val_acc:0.956]
Epoch [32/120    avg_loss:0.047, val_acc:0.956]
Epoch [33/120    avg_loss:0.039, val_acc:0.959]
Epoch [34/120    avg_loss:0.052, val_acc:0.961]
Epoch [35/120    avg_loss:0.037, val_acc:0.958]
Epoch [36/120    avg_loss:0.042, val_acc:0.972]
Epoch [37/120    avg_loss:0.022, val_acc:0.952]
Epoch [38/120    avg_loss:0.019, val_acc:0.972]
Epoch [39/120    avg_loss:0.035, val_acc:0.952]
Epoch [40/120    avg_loss:0.030, val_acc:0.969]
Epoch [41/120    avg_loss:0.043, val_acc:0.958]
Epoch [42/120    avg_loss:0.015, val_acc:0.970]
Epoch [43/120    avg_loss:0.033, val_acc:0.959]
Epoch [44/120    avg_loss:0.045, val_acc:0.969]
Epoch [45/120    avg_loss:0.039, val_acc:0.963]
Epoch [46/120    avg_loss:0.021, val_acc:0.973]
Epoch [47/120    avg_loss:0.016, val_acc:0.971]
Epoch [48/120    avg_loss:0.030, val_acc:0.957]
Epoch [49/120    avg_loss:0.051, val_acc:0.958]
Epoch [50/120    avg_loss:0.018, val_acc:0.974]
Epoch [51/120    avg_loss:0.012, val_acc:0.974]
Epoch [52/120    avg_loss:0.011, val_acc:0.973]
Epoch [53/120    avg_loss:0.013, val_acc:0.978]
Epoch [54/120    avg_loss:0.009, val_acc:0.978]
Epoch [55/120    avg_loss:0.005, val_acc:0.977]
Epoch [56/120    avg_loss:0.003, val_acc:0.978]
Epoch [57/120    avg_loss:0.007, val_acc:0.977]
Epoch [58/120    avg_loss:0.013, val_acc:0.960]
Epoch [59/120    avg_loss:0.012, val_acc:0.976]
Epoch [60/120    avg_loss:0.007, val_acc:0.982]
Epoch [61/120    avg_loss:0.026, val_acc:0.967]
Epoch [62/120    avg_loss:0.017, val_acc:0.964]
Epoch [63/120    avg_loss:0.006, val_acc:0.979]
Epoch [64/120    avg_loss:0.006, val_acc:0.983]
Epoch [65/120    avg_loss:0.005, val_acc:0.980]
Epoch [66/120    avg_loss:0.004, val_acc:0.981]
Epoch [67/120    avg_loss:0.009, val_acc:0.979]
Epoch [68/120    avg_loss:0.020, val_acc:0.976]
Epoch [69/120    avg_loss:0.012, val_acc:0.979]
Epoch [70/120    avg_loss:0.007, val_acc:0.979]
Epoch [71/120    avg_loss:0.011, val_acc:0.981]
Epoch [72/120    avg_loss:0.012, val_acc:0.977]
Epoch [73/120    avg_loss:0.006, val_acc:0.981]
Epoch [74/120    avg_loss:0.011, val_acc:0.978]
Epoch [75/120    avg_loss:0.009, val_acc:0.980]
Epoch [76/120    avg_loss:0.007, val_acc:0.983]
Epoch [77/120    avg_loss:0.005, val_acc:0.968]
Epoch [78/120    avg_loss:0.005, val_acc:0.980]
Epoch [79/120    avg_loss:0.005, val_acc:0.973]
Epoch [80/120    avg_loss:0.026, val_acc:0.976]
Epoch [81/120    avg_loss:0.013, val_acc:0.973]
Epoch [82/120    avg_loss:0.007, val_acc:0.979]
Epoch [83/120    avg_loss:0.007, val_acc:0.977]
Epoch [84/120    avg_loss:0.009, val_acc:0.969]
Epoch [85/120    avg_loss:0.004, val_acc:0.979]
Epoch [86/120    avg_loss:0.005, val_acc:0.982]
Epoch [87/120    avg_loss:0.002, val_acc:0.980]
Epoch [88/120    avg_loss:0.003, val_acc:0.984]
Epoch [89/120    avg_loss:0.002, val_acc:0.985]
Epoch [90/120    avg_loss:0.007, val_acc:0.976]
Epoch [91/120    avg_loss:0.006, val_acc:0.979]
Epoch [92/120    avg_loss:0.002, val_acc:0.982]
Epoch [93/120    avg_loss:0.002, val_acc:0.980]
Epoch [94/120    avg_loss:0.003, val_acc:0.979]
Epoch [95/120    avg_loss:0.004, val_acc:0.977]
Epoch [96/120    avg_loss:0.002, val_acc:0.983]
Epoch [97/120    avg_loss:0.002, val_acc:0.981]
Epoch [98/120    avg_loss:0.003, val_acc:0.978]
Epoch [99/120    avg_loss:0.002, val_acc:0.983]
Epoch [100/120    avg_loss:0.002, val_acc:0.983]
Epoch [101/120    avg_loss:0.005, val_acc:0.979]
Epoch [102/120    avg_loss:0.002, val_acc:0.988]
Epoch [103/120    avg_loss:0.002, val_acc:0.986]
Epoch [104/120    avg_loss:0.002, val_acc:0.984]
Epoch [105/120    avg_loss:0.002, val_acc:0.983]
Epoch [106/120    avg_loss:0.001, val_acc:0.983]
Epoch [107/120    avg_loss:0.002, val_acc:0.982]
Epoch [108/120    avg_loss:0.002, val_acc:0.985]
Epoch [109/120    avg_loss:0.002, val_acc:0.982]
Epoch [110/120    avg_loss:0.002, val_acc:0.984]
Epoch [111/120    avg_loss:0.001, val_acc:0.984]
Epoch [112/120    avg_loss:0.001, val_acc:0.986]
Epoch [113/120    avg_loss:0.001, val_acc:0.986]
Epoch [114/120    avg_loss:0.006, val_acc:0.985]
Epoch [115/120    avg_loss:0.003, val_acc:0.983]
Epoch [116/120    avg_loss:0.001, val_acc:0.983]
Epoch [117/120    avg_loss:0.003, val_acc:0.982]
Epoch [118/120    avg_loss:0.003, val_acc:0.982]
Epoch [119/120    avg_loss:0.007, val_acc:0.982]
Epoch [120/120    avg_loss:0.002, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1263    3    5    0    0    0    0    0    8    6    0    0
     0    0    0]
 [   0    0    0  735    0    0    0    0    0    1    1    4    5    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    1    0    0    0    0    0    0    1
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0  858   15    0    0
     1    0    0]
 [   0    0    9    0    0    0    0    0    0    0    1 2184   16    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0    0    1    0  524    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1132    4    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    91  251    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.84281842818429

F1 scores:
[       nan 0.98765432 0.98749023 0.98525469 0.98839907 0.99425947
 0.99545455 1.         1.         0.91428571 0.98338109 0.98845893
 0.96768236 0.99730458 0.95769882 0.83112583 0.96385542]

Kappa:
0.9753874970133908
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7feb122796d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.151, val_acc:0.537]
Epoch [2/120    avg_loss:1.643, val_acc:0.639]
Epoch [3/120    avg_loss:1.076, val_acc:0.711]
Epoch [4/120    avg_loss:1.018, val_acc:0.776]
Epoch [5/120    avg_loss:0.790, val_acc:0.787]
Epoch [6/120    avg_loss:0.867, val_acc:0.771]
Epoch [7/120    avg_loss:0.570, val_acc:0.792]
Epoch [8/120    avg_loss:0.657, val_acc:0.747]
Epoch [9/120    avg_loss:0.590, val_acc:0.809]
Epoch [10/120    avg_loss:0.388, val_acc:0.815]
Epoch [11/120    avg_loss:0.386, val_acc:0.806]
Epoch [12/120    avg_loss:0.297, val_acc:0.887]
Epoch [13/120    avg_loss:0.266, val_acc:0.840]
Epoch [14/120    avg_loss:0.168, val_acc:0.906]
Epoch [15/120    avg_loss:0.210, val_acc:0.911]
Epoch [16/120    avg_loss:0.204, val_acc:0.923]
Epoch [17/120    avg_loss:0.248, val_acc:0.815]
Epoch [18/120    avg_loss:0.223, val_acc:0.919]
Epoch [19/120    avg_loss:0.124, val_acc:0.910]
Epoch [20/120    avg_loss:0.116, val_acc:0.933]
Epoch [21/120    avg_loss:0.097, val_acc:0.889]
Epoch [22/120    avg_loss:0.129, val_acc:0.947]
Epoch [23/120    avg_loss:0.073, val_acc:0.931]
Epoch [24/120    avg_loss:0.099, val_acc:0.944]
Epoch [25/120    avg_loss:0.054, val_acc:0.942]
Epoch [26/120    avg_loss:0.043, val_acc:0.958]
Epoch [27/120    avg_loss:0.048, val_acc:0.953]
Epoch [28/120    avg_loss:0.042, val_acc:0.946]
Epoch [29/120    avg_loss:0.035, val_acc:0.952]
Epoch [30/120    avg_loss:0.063, val_acc:0.951]
Epoch [31/120    avg_loss:0.050, val_acc:0.960]
Epoch [32/120    avg_loss:0.045, val_acc:0.949]
Epoch [33/120    avg_loss:0.039, val_acc:0.953]
Epoch [34/120    avg_loss:0.037, val_acc:0.949]
Epoch [35/120    avg_loss:0.024, val_acc:0.957]
Epoch [36/120    avg_loss:0.021, val_acc:0.961]
Epoch [37/120    avg_loss:0.027, val_acc:0.965]
Epoch [38/120    avg_loss:0.035, val_acc:0.961]
Epoch [39/120    avg_loss:0.012, val_acc:0.964]
Epoch [40/120    avg_loss:0.019, val_acc:0.965]
Epoch [41/120    avg_loss:0.016, val_acc:0.963]
Epoch [42/120    avg_loss:0.019, val_acc:0.956]
Epoch [43/120    avg_loss:0.016, val_acc:0.965]
Epoch [44/120    avg_loss:0.011, val_acc:0.963]
Epoch [45/120    avg_loss:0.041, val_acc:0.954]
Epoch [46/120    avg_loss:0.032, val_acc:0.963]
Epoch [47/120    avg_loss:0.308, val_acc:0.882]
Epoch [48/120    avg_loss:0.199, val_acc:0.940]
Epoch [49/120    avg_loss:0.152, val_acc:0.936]
Epoch [50/120    avg_loss:0.058, val_acc:0.952]
Epoch [51/120    avg_loss:0.049, val_acc:0.944]
Epoch [52/120    avg_loss:0.073, val_acc:0.882]
Epoch [53/120    avg_loss:0.070, val_acc:0.955]
Epoch [54/120    avg_loss:0.046, val_acc:0.946]
Epoch [55/120    avg_loss:0.027, val_acc:0.954]
Epoch [56/120    avg_loss:0.039, val_acc:0.957]
Epoch [57/120    avg_loss:0.019, val_acc:0.959]
Epoch [58/120    avg_loss:0.015, val_acc:0.960]
Epoch [59/120    avg_loss:0.017, val_acc:0.964]
Epoch [60/120    avg_loss:0.011, val_acc:0.961]
Epoch [61/120    avg_loss:0.011, val_acc:0.963]
Epoch [62/120    avg_loss:0.013, val_acc:0.961]
Epoch [63/120    avg_loss:0.012, val_acc:0.964]
Epoch [64/120    avg_loss:0.009, val_acc:0.963]
Epoch [65/120    avg_loss:0.009, val_acc:0.963]
Epoch [66/120    avg_loss:0.010, val_acc:0.963]
Epoch [67/120    avg_loss:0.012, val_acc:0.963]
Epoch [68/120    avg_loss:0.009, val_acc:0.964]
Epoch [69/120    avg_loss:0.014, val_acc:0.963]
Epoch [70/120    avg_loss:0.008, val_acc:0.963]
Epoch [71/120    avg_loss:0.010, val_acc:0.963]
Epoch [72/120    avg_loss:0.010, val_acc:0.963]
Epoch [73/120    avg_loss:0.007, val_acc:0.965]
Epoch [74/120    avg_loss:0.007, val_acc:0.965]
Epoch [75/120    avg_loss:0.011, val_acc:0.965]
Epoch [76/120    avg_loss:0.009, val_acc:0.965]
Epoch [77/120    avg_loss:0.010, val_acc:0.965]
Epoch [78/120    avg_loss:0.011, val_acc:0.965]
Epoch [79/120    avg_loss:0.011, val_acc:0.964]
Epoch [80/120    avg_loss:0.011, val_acc:0.964]
Epoch [81/120    avg_loss:0.008, val_acc:0.965]
Epoch [82/120    avg_loss:0.011, val_acc:0.963]
Epoch [83/120    avg_loss:0.013, val_acc:0.964]
Epoch [84/120    avg_loss:0.013, val_acc:0.964]
Epoch [85/120    avg_loss:0.011, val_acc:0.964]
Epoch [86/120    avg_loss:0.009, val_acc:0.964]
Epoch [87/120    avg_loss:0.012, val_acc:0.964]
Epoch [88/120    avg_loss:0.007, val_acc:0.964]
Epoch [89/120    avg_loss:0.009, val_acc:0.964]
Epoch [90/120    avg_loss:0.008, val_acc:0.964]
Epoch [91/120    avg_loss:0.009, val_acc:0.964]
Epoch [92/120    avg_loss:0.011, val_acc:0.964]
Epoch [93/120    avg_loss:0.009, val_acc:0.964]
Epoch [94/120    avg_loss:0.011, val_acc:0.966]
Epoch [95/120    avg_loss:0.012, val_acc:0.965]
Epoch [96/120    avg_loss:0.008, val_acc:0.965]
Epoch [97/120    avg_loss:0.009, val_acc:0.965]
Epoch [98/120    avg_loss:0.010, val_acc:0.966]
Epoch [99/120    avg_loss:0.009, val_acc:0.965]
Epoch [100/120    avg_loss:0.013, val_acc:0.965]
Epoch [101/120    avg_loss:0.012, val_acc:0.965]
Epoch [102/120    avg_loss:0.011, val_acc:0.965]
Epoch [103/120    avg_loss:0.013, val_acc:0.965]
Epoch [104/120    avg_loss:0.013, val_acc:0.965]
Epoch [105/120    avg_loss:0.009, val_acc:0.965]
Epoch [106/120    avg_loss:0.008, val_acc:0.965]
Epoch [107/120    avg_loss:0.011, val_acc:0.964]
Epoch [108/120    avg_loss:0.008, val_acc:0.964]
Epoch [109/120    avg_loss:0.011, val_acc:0.964]
Epoch [110/120    avg_loss:0.009, val_acc:0.964]
Epoch [111/120    avg_loss:0.011, val_acc:0.964]
Epoch [112/120    avg_loss:0.008, val_acc:0.964]
Epoch [113/120    avg_loss:0.012, val_acc:0.964]
Epoch [114/120    avg_loss:0.009, val_acc:0.964]
Epoch [115/120    avg_loss:0.008, val_acc:0.964]
Epoch [116/120    avg_loss:0.008, val_acc:0.964]
Epoch [117/120    avg_loss:0.008, val_acc:0.964]
Epoch [118/120    avg_loss:0.008, val_acc:0.964]
Epoch [119/120    avg_loss:0.009, val_acc:0.964]
Epoch [120/120    avg_loss:0.009, val_acc:0.964]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1265    6    0    0    2    0    0    0    7    5    0    0
     0    0    0]
 [   0    0    0  730    1    0    0    0    0    2    1    3    9    0
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    2    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  826   39    0    0
     4    0    0]
 [   0    0   11    0    0    0    0    0    0    0    1 2178   20    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  533    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    1    0    0    0    0    0    0    0
  1119   12    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    51  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
97.61517615176152

F1 scores:
[       nan 0.95121951 0.98520249 0.98382749 0.99530516 0.98855835
 0.98496241 0.98039216 0.997669   0.94736842 0.96551724 0.98174442
 0.96821072 1.         0.96632124 0.878125   0.96341463]

Kappa:
0.9727938259425261
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0f708cb710>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.168, val_acc:0.569]
Epoch [2/120    avg_loss:1.582, val_acc:0.593]
Epoch [3/120    avg_loss:1.552, val_acc:0.578]
Epoch [4/120    avg_loss:1.228, val_acc:0.698]
Epoch [5/120    avg_loss:1.004, val_acc:0.744]
Epoch [6/120    avg_loss:0.790, val_acc:0.790]
Epoch [7/120    avg_loss:0.763, val_acc:0.742]
Epoch [8/120    avg_loss:0.582, val_acc:0.821]
Epoch [9/120    avg_loss:0.477, val_acc:0.809]
Epoch [10/120    avg_loss:0.407, val_acc:0.773]
Epoch [11/120    avg_loss:0.519, val_acc:0.789]
Epoch [12/120    avg_loss:0.408, val_acc:0.865]
Epoch [13/120    avg_loss:0.278, val_acc:0.908]
Epoch [14/120    avg_loss:0.159, val_acc:0.926]
Epoch [15/120    avg_loss:0.157, val_acc:0.916]
Epoch [16/120    avg_loss:0.139, val_acc:0.901]
Epoch [17/120    avg_loss:0.140, val_acc:0.924]
Epoch [18/120    avg_loss:0.223, val_acc:0.901]
Epoch [19/120    avg_loss:0.143, val_acc:0.926]
Epoch [20/120    avg_loss:0.122, val_acc:0.951]
Epoch [21/120    avg_loss:0.089, val_acc:0.947]
Epoch [22/120    avg_loss:0.066, val_acc:0.944]
Epoch [23/120    avg_loss:0.098, val_acc:0.950]
Epoch [24/120    avg_loss:0.121, val_acc:0.953]
Epoch [25/120    avg_loss:0.069, val_acc:0.954]
Epoch [26/120    avg_loss:0.054, val_acc:0.931]
Epoch [27/120    avg_loss:0.057, val_acc:0.952]
Epoch [28/120    avg_loss:0.048, val_acc:0.961]
Epoch [29/120    avg_loss:0.097, val_acc:0.945]
Epoch [30/120    avg_loss:0.062, val_acc:0.943]
Epoch [31/120    avg_loss:0.040, val_acc:0.959]
Epoch [32/120    avg_loss:0.024, val_acc:0.963]
Epoch [33/120    avg_loss:0.026, val_acc:0.969]
Epoch [34/120    avg_loss:0.032, val_acc:0.953]
Epoch [35/120    avg_loss:0.028, val_acc:0.953]
Epoch [36/120    avg_loss:0.029, val_acc:0.966]
Epoch [37/120    avg_loss:0.026, val_acc:0.969]
Epoch [38/120    avg_loss:0.018, val_acc:0.965]
Epoch [39/120    avg_loss:0.031, val_acc:0.963]
Epoch [40/120    avg_loss:0.016, val_acc:0.963]
Epoch [41/120    avg_loss:0.017, val_acc:0.970]
Epoch [42/120    avg_loss:0.023, val_acc:0.965]
Epoch [43/120    avg_loss:0.024, val_acc:0.974]
Epoch [44/120    avg_loss:0.024, val_acc:0.966]
Epoch [45/120    avg_loss:0.011, val_acc:0.971]
Epoch [46/120    avg_loss:0.009, val_acc:0.974]
Epoch [47/120    avg_loss:0.009, val_acc:0.975]
Epoch [48/120    avg_loss:0.010, val_acc:0.974]
Epoch [49/120    avg_loss:0.005, val_acc:0.979]
Epoch [50/120    avg_loss:0.015, val_acc:0.974]
Epoch [51/120    avg_loss:0.015, val_acc:0.968]
Epoch [52/120    avg_loss:0.013, val_acc:0.969]
Epoch [53/120    avg_loss:0.006, val_acc:0.970]
Epoch [54/120    avg_loss:0.012, val_acc:0.970]
Epoch [55/120    avg_loss:0.009, val_acc:0.964]
Epoch [56/120    avg_loss:0.011, val_acc:0.971]
Epoch [57/120    avg_loss:0.007, val_acc:0.977]
Epoch [58/120    avg_loss:0.018, val_acc:0.969]
Epoch [59/120    avg_loss:0.008, val_acc:0.972]
Epoch [60/120    avg_loss:0.006, val_acc:0.974]
Epoch [61/120    avg_loss:0.008, val_acc:0.971]
Epoch [62/120    avg_loss:0.005, val_acc:0.975]
Epoch [63/120    avg_loss:0.005, val_acc:0.975]
Epoch [64/120    avg_loss:0.004, val_acc:0.976]
Epoch [65/120    avg_loss:0.004, val_acc:0.976]
Epoch [66/120    avg_loss:0.005, val_acc:0.975]
Epoch [67/120    avg_loss:0.003, val_acc:0.977]
Epoch [68/120    avg_loss:0.004, val_acc:0.978]
Epoch [69/120    avg_loss:0.004, val_acc:0.978]
Epoch [70/120    avg_loss:0.004, val_acc:0.978]
Epoch [71/120    avg_loss:0.004, val_acc:0.977]
Epoch [72/120    avg_loss:0.003, val_acc:0.977]
Epoch [73/120    avg_loss:0.002, val_acc:0.976]
Epoch [74/120    avg_loss:0.003, val_acc:0.976]
Epoch [75/120    avg_loss:0.005, val_acc:0.977]
Epoch [76/120    avg_loss:0.002, val_acc:0.977]
Epoch [77/120    avg_loss:0.004, val_acc:0.977]
Epoch [78/120    avg_loss:0.004, val_acc:0.977]
Epoch [79/120    avg_loss:0.005, val_acc:0.977]
Epoch [80/120    avg_loss:0.008, val_acc:0.977]
Epoch [81/120    avg_loss:0.003, val_acc:0.977]
Epoch [82/120    avg_loss:0.004, val_acc:0.977]
Epoch [83/120    avg_loss:0.004, val_acc:0.977]
Epoch [84/120    avg_loss:0.007, val_acc:0.977]
Epoch [85/120    avg_loss:0.003, val_acc:0.977]
Epoch [86/120    avg_loss:0.003, val_acc:0.977]
Epoch [87/120    avg_loss:0.004, val_acc:0.977]
Epoch [88/120    avg_loss:0.006, val_acc:0.977]
Epoch [89/120    avg_loss:0.003, val_acc:0.977]
Epoch [90/120    avg_loss:0.008, val_acc:0.977]
Epoch [91/120    avg_loss:0.004, val_acc:0.977]
Epoch [92/120    avg_loss:0.006, val_acc:0.977]
Epoch [93/120    avg_loss:0.003, val_acc:0.977]
Epoch [94/120    avg_loss:0.005, val_acc:0.977]
Epoch [95/120    avg_loss:0.004, val_acc:0.977]
Epoch [96/120    avg_loss:0.005, val_acc:0.977]
Epoch [97/120    avg_loss:0.003, val_acc:0.977]
Epoch [98/120    avg_loss:0.003, val_acc:0.977]
Epoch [99/120    avg_loss:0.005, val_acc:0.977]
Epoch [100/120    avg_loss:0.002, val_acc:0.977]
Epoch [101/120    avg_loss:0.003, val_acc:0.977]
Epoch [102/120    avg_loss:0.004, val_acc:0.977]
Epoch [103/120    avg_loss:0.003, val_acc:0.977]
Epoch [104/120    avg_loss:0.005, val_acc:0.977]
Epoch [105/120    avg_loss:0.004, val_acc:0.977]
Epoch [106/120    avg_loss:0.003, val_acc:0.977]
Epoch [107/120    avg_loss:0.004, val_acc:0.977]
Epoch [108/120    avg_loss:0.004, val_acc:0.977]
Epoch [109/120    avg_loss:0.004, val_acc:0.977]
Epoch [110/120    avg_loss:0.004, val_acc:0.977]
Epoch [111/120    avg_loss:0.003, val_acc:0.977]
Epoch [112/120    avg_loss:0.003, val_acc:0.977]
Epoch [113/120    avg_loss:0.002, val_acc:0.977]
Epoch [114/120    avg_loss:0.003, val_acc:0.977]
Epoch [115/120    avg_loss:0.006, val_acc:0.977]
Epoch [116/120    avg_loss:0.006, val_acc:0.977]
Epoch [117/120    avg_loss:0.003, val_acc:0.977]
Epoch [118/120    avg_loss:0.003, val_acc:0.977]
Epoch [119/120    avg_loss:0.003, val_acc:0.977]
Epoch [120/120    avg_loss:0.003, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1277    1    0    1    0    0    0    0    4    2    0    0
     0    0    0]
 [   0    0    0  738    0    0    0    0    0    2    0    3    4    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    1    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0  851   20    0    0
     0    1    0]
 [   0    0   14    0    0    2    0    0    0    0    2 2176   15    0
     1    0    0]
 [   0    0    0    6    0    0    0    0    0    0    0    0  527    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1125   13    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    25  311    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
98.41734417344173

F1 scores:
[       nan 0.93975904 0.98992248 0.98795181 0.99528302 0.99311927
 0.99095023 1.         0.99649942 0.94736842 0.98211194 0.98662435
 0.97142857 1.         0.98210388 0.92559524 0.96341463]

Kappa:
0.9819560170530476
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:61
Validation dataloader:61
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fba5b86c6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.074, val_acc:0.494]
Epoch [2/120    avg_loss:1.562, val_acc:0.581]
Epoch [3/120    avg_loss:1.337, val_acc:0.641]
Epoch [4/120    avg_loss:1.038, val_acc:0.659]
Epoch [5/120    avg_loss:1.159, val_acc:0.703]
Epoch [6/120    avg_loss:0.791, val_acc:0.781]
Epoch [7/120    avg_loss:0.817, val_acc:0.795]
Epoch [8/120    avg_loss:0.537, val_acc:0.751]
Epoch [9/120    avg_loss:0.682, val_acc:0.808]
Epoch [10/120    avg_loss:0.526, val_acc:0.849]
Epoch [11/120    avg_loss:0.364, val_acc:0.848]
Epoch [12/120    avg_loss:0.254, val_acc:0.891]
Epoch [13/120    avg_loss:0.209, val_acc:0.909]
Epoch [14/120    avg_loss:0.201, val_acc:0.896]
Epoch [15/120    avg_loss:0.156, val_acc:0.903]
Epoch [16/120    avg_loss:0.490, val_acc:0.818]
Epoch [17/120    avg_loss:0.278, val_acc:0.875]
Epoch [18/120    avg_loss:0.166, val_acc:0.917]
Epoch [19/120    avg_loss:0.152, val_acc:0.921]
Epoch [20/120    avg_loss:0.151, val_acc:0.884]
Epoch [21/120    avg_loss:0.165, val_acc:0.893]
Epoch [22/120    avg_loss:0.125, val_acc:0.918]
Epoch [23/120    avg_loss:0.080, val_acc:0.938]
Epoch [24/120    avg_loss:0.085, val_acc:0.909]
Epoch [25/120    avg_loss:0.086, val_acc:0.944]
Epoch [26/120    avg_loss:0.041, val_acc:0.953]
Epoch [27/120    avg_loss:0.057, val_acc:0.943]
Epoch [28/120    avg_loss:0.054, val_acc:0.950]
Epoch [29/120    avg_loss:0.041, val_acc:0.953]
Epoch [30/120    avg_loss:0.033, val_acc:0.961]
Epoch [31/120    avg_loss:0.026, val_acc:0.955]
Epoch [32/120    avg_loss:0.035, val_acc:0.950]
Epoch [33/120    avg_loss:0.036, val_acc:0.952]
Epoch [34/120    avg_loss:0.033, val_acc:0.955]
Epoch [35/120    avg_loss:0.037, val_acc:0.958]
Epoch [36/120    avg_loss:0.042, val_acc:0.956]
Epoch [37/120    avg_loss:0.040, val_acc:0.945]
Epoch [38/120    avg_loss:0.044, val_acc:0.949]
Epoch [39/120    avg_loss:0.040, val_acc:0.955]
Epoch [40/120    avg_loss:0.025, val_acc:0.952]
Epoch [41/120    avg_loss:0.018, val_acc:0.947]
Epoch [42/120    avg_loss:0.038, val_acc:0.943]
Epoch [43/120    avg_loss:0.020, val_acc:0.959]
Epoch [44/120    avg_loss:0.010, val_acc:0.959]
Epoch [45/120    avg_loss:0.009, val_acc:0.964]
Epoch [46/120    avg_loss:0.010, val_acc:0.965]
Epoch [47/120    avg_loss:0.012, val_acc:0.965]
Epoch [48/120    avg_loss:0.009, val_acc:0.967]
Epoch [49/120    avg_loss:0.011, val_acc:0.966]
Epoch [50/120    avg_loss:0.015, val_acc:0.963]
Epoch [51/120    avg_loss:0.020, val_acc:0.968]
Epoch [52/120    avg_loss:0.011, val_acc:0.969]
Epoch [53/120    avg_loss:0.010, val_acc:0.969]
Epoch [54/120    avg_loss:0.012, val_acc:0.968]
Epoch [55/120    avg_loss:0.010, val_acc:0.968]
Epoch [56/120    avg_loss:0.010, val_acc:0.970]
Epoch [57/120    avg_loss:0.007, val_acc:0.968]
Epoch [58/120    avg_loss:0.014, val_acc:0.968]
Epoch [59/120    avg_loss:0.007, val_acc:0.969]
Epoch [60/120    avg_loss:0.007, val_acc:0.970]
Epoch [61/120    avg_loss:0.008, val_acc:0.971]
Epoch [62/120    avg_loss:0.010, val_acc:0.970]
Epoch [63/120    avg_loss:0.008, val_acc:0.970]
Epoch [64/120    avg_loss:0.008, val_acc:0.969]
Epoch [65/120    avg_loss:0.006, val_acc:0.971]
Epoch [66/120    avg_loss:0.007, val_acc:0.970]
Epoch [67/120    avg_loss:0.014, val_acc:0.973]
Epoch [68/120    avg_loss:0.013, val_acc:0.971]
Epoch [69/120    avg_loss:0.014, val_acc:0.970]
Epoch [70/120    avg_loss:0.009, val_acc:0.973]
Epoch [71/120    avg_loss:0.006, val_acc:0.971]
Epoch [72/120    avg_loss:0.008, val_acc:0.971]
Epoch [73/120    avg_loss:0.006, val_acc:0.972]
Epoch [74/120    avg_loss:0.008, val_acc:0.971]
Epoch [75/120    avg_loss:0.008, val_acc:0.972]
Epoch [76/120    avg_loss:0.007, val_acc:0.972]
Epoch [77/120    avg_loss:0.007, val_acc:0.972]
Epoch [78/120    avg_loss:0.008, val_acc:0.972]
Epoch [79/120    avg_loss:0.004, val_acc:0.970]
Epoch [80/120    avg_loss:0.007, val_acc:0.971]
Epoch [81/120    avg_loss:0.007, val_acc:0.971]
Epoch [82/120    avg_loss:0.008, val_acc:0.970]
Epoch [83/120    avg_loss:0.008, val_acc:0.970]
Epoch [84/120    avg_loss:0.006, val_acc:0.970]
Epoch [85/120    avg_loss:0.005, val_acc:0.970]
Epoch [86/120    avg_loss:0.006, val_acc:0.970]
Epoch [87/120    avg_loss:0.004, val_acc:0.970]
Epoch [88/120    avg_loss:0.008, val_acc:0.970]
Epoch [89/120    avg_loss:0.006, val_acc:0.970]
Epoch [90/120    avg_loss:0.012, val_acc:0.970]
Epoch [91/120    avg_loss:0.007, val_acc:0.970]
Epoch [92/120    avg_loss:0.005, val_acc:0.970]
Epoch [93/120    avg_loss:0.010, val_acc:0.970]
Epoch [94/120    avg_loss:0.009, val_acc:0.970]
Epoch [95/120    avg_loss:0.006, val_acc:0.970]
Epoch [96/120    avg_loss:0.005, val_acc:0.970]
Epoch [97/120    avg_loss:0.010, val_acc:0.970]
Epoch [98/120    avg_loss:0.006, val_acc:0.970]
Epoch [99/120    avg_loss:0.007, val_acc:0.970]
Epoch [100/120    avg_loss:0.007, val_acc:0.970]
Epoch [101/120    avg_loss:0.005, val_acc:0.970]
Epoch [102/120    avg_loss:0.011, val_acc:0.970]
Epoch [103/120    avg_loss:0.007, val_acc:0.970]
Epoch [104/120    avg_loss:0.007, val_acc:0.970]
Epoch [105/120    avg_loss:0.006, val_acc:0.970]
Epoch [106/120    avg_loss:0.005, val_acc:0.971]
Epoch [107/120    avg_loss:0.005, val_acc:0.971]
Epoch [108/120    avg_loss:0.005, val_acc:0.971]
Epoch [109/120    avg_loss:0.006, val_acc:0.971]
Epoch [110/120    avg_loss:0.005, val_acc:0.971]
Epoch [111/120    avg_loss:0.007, val_acc:0.971]
Epoch [112/120    avg_loss:0.005, val_acc:0.971]
Epoch [113/120    avg_loss:0.006, val_acc:0.971]
Epoch [114/120    avg_loss:0.008, val_acc:0.971]
Epoch [115/120    avg_loss:0.007, val_acc:0.971]
Epoch [116/120    avg_loss:0.006, val_acc:0.971]
Epoch [117/120    avg_loss:0.008, val_acc:0.971]
Epoch [118/120    avg_loss:0.006, val_acc:0.971]
Epoch [119/120    avg_loss:0.009, val_acc:0.971]
Epoch [120/120    avg_loss:0.006, val_acc:0.971]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1275    3    3    0    0    0    0    0    1    3    0    0
     0    0    0]
 [   0    0    0  736    0    0    0    0    0    2    0    2    7    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  851   21    0    0
     2    1    0]
 [   0    0   16    0    0    0    0    0    0    0    4 2165   23    1
     1    0    0]
 [   0    0    0    4    0    0    0    0    0    0    0    0  529    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    1    0    0    0    0    0    0    0
  1119   15    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    30  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.13550135501355

F1 scores:
[       nan 0.975      0.9895227  0.98791946 0.99300699 0.99311927
 0.98722765 1.         0.99649942 0.92307692 0.98267898 0.9838673
 0.96181818 0.99730458 0.97643979 0.90662651 0.96969697]

Kappa:
0.9787480949696967
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f28b63d56d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.101, val_acc:0.510]
Epoch [2/120    avg_loss:1.567, val_acc:0.650]
Epoch [3/120    avg_loss:1.157, val_acc:0.680]
Epoch [4/120    avg_loss:1.075, val_acc:0.655]
Epoch [5/120    avg_loss:0.923, val_acc:0.730]
Epoch [6/120    avg_loss:0.961, val_acc:0.719]
Epoch [7/120    avg_loss:0.840, val_acc:0.721]
Epoch [8/120    avg_loss:0.536, val_acc:0.829]
Epoch [9/120    avg_loss:0.526, val_acc:0.794]
Epoch [10/120    avg_loss:0.490, val_acc:0.854]
Epoch [11/120    avg_loss:0.547, val_acc:0.824]
Epoch [12/120    avg_loss:0.587, val_acc:0.786]
Epoch [13/120    avg_loss:0.441, val_acc:0.871]
Epoch [14/120    avg_loss:0.326, val_acc:0.880]
Epoch [15/120    avg_loss:0.274, val_acc:0.881]
Epoch [16/120    avg_loss:0.260, val_acc:0.891]
Epoch [17/120    avg_loss:0.206, val_acc:0.923]
Epoch [18/120    avg_loss:0.200, val_acc:0.899]
Epoch [19/120    avg_loss:0.193, val_acc:0.920]
Epoch [20/120    avg_loss:0.130, val_acc:0.919]
Epoch [21/120    avg_loss:0.137, val_acc:0.938]
Epoch [22/120    avg_loss:0.106, val_acc:0.939]
Epoch [23/120    avg_loss:0.098, val_acc:0.929]
Epoch [24/120    avg_loss:0.098, val_acc:0.946]
Epoch [25/120    avg_loss:0.102, val_acc:0.946]
Epoch [26/120    avg_loss:0.115, val_acc:0.943]
Epoch [27/120    avg_loss:0.086, val_acc:0.934]
Epoch [28/120    avg_loss:0.103, val_acc:0.953]
Epoch [29/120    avg_loss:0.086, val_acc:0.942]
Epoch [30/120    avg_loss:0.108, val_acc:0.929]
Epoch [31/120    avg_loss:0.060, val_acc:0.947]
Epoch [32/120    avg_loss:0.047, val_acc:0.965]
Epoch [33/120    avg_loss:0.043, val_acc:0.940]
Epoch [34/120    avg_loss:0.054, val_acc:0.967]
Epoch [35/120    avg_loss:0.039, val_acc:0.959]
Epoch [36/120    avg_loss:0.045, val_acc:0.954]
Epoch [37/120    avg_loss:0.060, val_acc:0.952]
Epoch [38/120    avg_loss:0.083, val_acc:0.960]
Epoch [39/120    avg_loss:0.038, val_acc:0.970]
Epoch [40/120    avg_loss:0.025, val_acc:0.969]
Epoch [41/120    avg_loss:0.034, val_acc:0.969]
Epoch [42/120    avg_loss:0.047, val_acc:0.964]
Epoch [43/120    avg_loss:0.044, val_acc:0.964]
Epoch [44/120    avg_loss:0.032, val_acc:0.972]
Epoch [45/120    avg_loss:0.044, val_acc:0.966]
Epoch [46/120    avg_loss:0.058, val_acc:0.958]
Epoch [47/120    avg_loss:0.056, val_acc:0.961]
Epoch [48/120    avg_loss:0.025, val_acc:0.970]
Epoch [49/120    avg_loss:0.017, val_acc:0.966]
Epoch [50/120    avg_loss:0.026, val_acc:0.973]
Epoch [51/120    avg_loss:0.017, val_acc:0.971]
Epoch [52/120    avg_loss:0.021, val_acc:0.965]
Epoch [53/120    avg_loss:0.038, val_acc:0.965]
Epoch [54/120    avg_loss:0.022, val_acc:0.976]
Epoch [55/120    avg_loss:0.022, val_acc:0.976]
Epoch [56/120    avg_loss:0.021, val_acc:0.979]
Epoch [57/120    avg_loss:0.015, val_acc:0.977]
Epoch [58/120    avg_loss:0.019, val_acc:0.973]
Epoch [59/120    avg_loss:0.017, val_acc:0.971]
Epoch [60/120    avg_loss:0.021, val_acc:0.978]
Epoch [61/120    avg_loss:0.015, val_acc:0.973]
Epoch [62/120    avg_loss:0.012, val_acc:0.977]
Epoch [63/120    avg_loss:0.007, val_acc:0.977]
Epoch [64/120    avg_loss:0.007, val_acc:0.978]
Epoch [65/120    avg_loss:0.014, val_acc:0.978]
Epoch [66/120    avg_loss:0.009, val_acc:0.981]
Epoch [67/120    avg_loss:0.015, val_acc:0.972]
Epoch [68/120    avg_loss:0.015, val_acc:0.981]
Epoch [69/120    avg_loss:0.005, val_acc:0.978]
Epoch [70/120    avg_loss:0.012, val_acc:0.980]
Epoch [71/120    avg_loss:0.009, val_acc:0.979]
Epoch [72/120    avg_loss:0.030, val_acc:0.970]
Epoch [73/120    avg_loss:0.025, val_acc:0.950]
Epoch [74/120    avg_loss:0.023, val_acc:0.974]
Epoch [75/120    avg_loss:0.012, val_acc:0.975]
Epoch [76/120    avg_loss:0.008, val_acc:0.982]
Epoch [77/120    avg_loss:0.008, val_acc:0.978]
Epoch [78/120    avg_loss:0.013, val_acc:0.981]
Epoch [79/120    avg_loss:0.021, val_acc:0.970]
Epoch [80/120    avg_loss:0.022, val_acc:0.972]
Epoch [81/120    avg_loss:0.017, val_acc:0.976]
Epoch [82/120    avg_loss:0.019, val_acc:0.978]
Epoch [83/120    avg_loss:0.017, val_acc:0.960]
Epoch [84/120    avg_loss:0.010, val_acc:0.979]
Epoch [85/120    avg_loss:0.012, val_acc:0.979]
Epoch [86/120    avg_loss:0.008, val_acc:0.980]
Epoch [87/120    avg_loss:0.005, val_acc:0.979]
Epoch [88/120    avg_loss:0.004, val_acc:0.982]
Epoch [89/120    avg_loss:0.017, val_acc:0.956]
Epoch [90/120    avg_loss:0.027, val_acc:0.969]
Epoch [91/120    avg_loss:0.011, val_acc:0.980]
Epoch [92/120    avg_loss:0.009, val_acc:0.980]
Epoch [93/120    avg_loss:0.007, val_acc:0.980]
Epoch [94/120    avg_loss:0.006, val_acc:0.977]
Epoch [95/120    avg_loss:0.024, val_acc:0.970]
Epoch [96/120    avg_loss:0.025, val_acc:0.975]
Epoch [97/120    avg_loss:0.026, val_acc:0.968]
Epoch [98/120    avg_loss:0.015, val_acc:0.968]
Epoch [99/120    avg_loss:0.013, val_acc:0.981]
Epoch [100/120    avg_loss:0.039, val_acc:0.974]
Epoch [101/120    avg_loss:0.015, val_acc:0.979]
Epoch [102/120    avg_loss:0.007, val_acc:0.980]
Epoch [103/120    avg_loss:0.006, val_acc:0.981]
Epoch [104/120    avg_loss:0.005, val_acc:0.981]
Epoch [105/120    avg_loss:0.005, val_acc:0.981]
Epoch [106/120    avg_loss:0.011, val_acc:0.982]
Epoch [107/120    avg_loss:0.013, val_acc:0.983]
Epoch [108/120    avg_loss:0.005, val_acc:0.985]
Epoch [109/120    avg_loss:0.008, val_acc:0.985]
Epoch [110/120    avg_loss:0.006, val_acc:0.985]
Epoch [111/120    avg_loss:0.007, val_acc:0.983]
Epoch [112/120    avg_loss:0.007, val_acc:0.983]
Epoch [113/120    avg_loss:0.005, val_acc:0.983]
Epoch [114/120    avg_loss:0.004, val_acc:0.983]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.003, val_acc:0.984]
Epoch [117/120    avg_loss:0.005, val_acc:0.983]
Epoch [118/120    avg_loss:0.003, val_acc:0.983]
Epoch [119/120    avg_loss:0.004, val_acc:0.982]
Epoch [120/120    avg_loss:0.004, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1267    3    4    0    3    0    0    0    4    3    0    1
     0    0    0]
 [   0    0    0  717    0   10    0    0    0    3    1    0   13    2
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0  868    2    0    0
     0    4    0]
 [   0    0    6    0    0    0    0    0    0    0    3 2194    6    1
     0    0    0]
 [   0    0    0    5    0    0    0    0    0    0    3    1  522    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1130    8    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    12  322    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.63414634146342

F1 scores:
[       nan 0.975      0.98984375 0.97286296 0.98834499 0.98633257
 0.98644578 0.98039216 1.         0.87179487 0.98861048 0.99456029
 0.97026022 0.98930481 0.99035933 0.94428152 0.98224852]

Kappa:
0.9844324950340951
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f084f5226a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.287, val_acc:0.507]
Epoch [2/120    avg_loss:1.633, val_acc:0.570]
Epoch [3/120    avg_loss:1.306, val_acc:0.624]
Epoch [4/120    avg_loss:1.076, val_acc:0.696]
Epoch [5/120    avg_loss:1.144, val_acc:0.763]
Epoch [6/120    avg_loss:0.842, val_acc:0.798]
Epoch [7/120    avg_loss:0.789, val_acc:0.816]
Epoch [8/120    avg_loss:0.573, val_acc:0.750]
Epoch [9/120    avg_loss:0.526, val_acc:0.785]
Epoch [10/120    avg_loss:0.472, val_acc:0.829]
Epoch [11/120    avg_loss:0.364, val_acc:0.897]
Epoch [12/120    avg_loss:0.346, val_acc:0.897]
Epoch [13/120    avg_loss:0.237, val_acc:0.902]
Epoch [14/120    avg_loss:0.235, val_acc:0.846]
Epoch [15/120    avg_loss:0.254, val_acc:0.895]
Epoch [16/120    avg_loss:0.162, val_acc:0.920]
Epoch [17/120    avg_loss:0.149, val_acc:0.908]
Epoch [18/120    avg_loss:0.151, val_acc:0.929]
Epoch [19/120    avg_loss:0.103, val_acc:0.952]
Epoch [20/120    avg_loss:0.153, val_acc:0.941]
Epoch [21/120    avg_loss:0.097, val_acc:0.945]
Epoch [22/120    avg_loss:0.094, val_acc:0.942]
Epoch [23/120    avg_loss:0.064, val_acc:0.960]
Epoch [24/120    avg_loss:0.057, val_acc:0.952]
Epoch [25/120    avg_loss:0.060, val_acc:0.950]
Epoch [26/120    avg_loss:0.111, val_acc:0.908]
Epoch [27/120    avg_loss:0.127, val_acc:0.949]
Epoch [28/120    avg_loss:0.087, val_acc:0.949]
Epoch [29/120    avg_loss:0.072, val_acc:0.965]
Epoch [30/120    avg_loss:0.034, val_acc:0.970]
Epoch [31/120    avg_loss:0.061, val_acc:0.957]
Epoch [32/120    avg_loss:0.049, val_acc:0.964]
Epoch [33/120    avg_loss:0.032, val_acc:0.969]
Epoch [34/120    avg_loss:0.048, val_acc:0.964]
Epoch [35/120    avg_loss:0.046, val_acc:0.966]
Epoch [36/120    avg_loss:0.023, val_acc:0.970]
Epoch [37/120    avg_loss:0.035, val_acc:0.968]
Epoch [38/120    avg_loss:0.048, val_acc:0.946]
Epoch [39/120    avg_loss:0.068, val_acc:0.944]
Epoch [40/120    avg_loss:0.043, val_acc:0.977]
Epoch [41/120    avg_loss:0.024, val_acc:0.973]
Epoch [42/120    avg_loss:0.014, val_acc:0.977]
Epoch [43/120    avg_loss:0.014, val_acc:0.983]
Epoch [44/120    avg_loss:0.018, val_acc:0.977]
Epoch [45/120    avg_loss:0.032, val_acc:0.976]
Epoch [46/120    avg_loss:0.020, val_acc:0.974]
Epoch [47/120    avg_loss:0.011, val_acc:0.980]
Epoch [48/120    avg_loss:0.007, val_acc:0.982]
Epoch [49/120    avg_loss:0.008, val_acc:0.982]
Epoch [50/120    avg_loss:0.012, val_acc:0.980]
Epoch [51/120    avg_loss:0.008, val_acc:0.985]
Epoch [52/120    avg_loss:0.024, val_acc:0.968]
Epoch [53/120    avg_loss:0.024, val_acc:0.976]
Epoch [54/120    avg_loss:0.027, val_acc:0.962]
Epoch [55/120    avg_loss:0.017, val_acc:0.980]
Epoch [56/120    avg_loss:0.016, val_acc:0.982]
Epoch [57/120    avg_loss:0.013, val_acc:0.983]
Epoch [58/120    avg_loss:0.015, val_acc:0.977]
Epoch [59/120    avg_loss:0.107, val_acc:0.967]
Epoch [60/120    avg_loss:0.022, val_acc:0.972]
Epoch [61/120    avg_loss:0.016, val_acc:0.968]
Epoch [62/120    avg_loss:0.022, val_acc:0.974]
Epoch [63/120    avg_loss:0.018, val_acc:0.974]
Epoch [64/120    avg_loss:0.011, val_acc:0.980]
Epoch [65/120    avg_loss:0.009, val_acc:0.981]
Epoch [66/120    avg_loss:0.007, val_acc:0.982]
Epoch [67/120    avg_loss:0.007, val_acc:0.981]
Epoch [68/120    avg_loss:0.005, val_acc:0.983]
Epoch [69/120    avg_loss:0.007, val_acc:0.983]
Epoch [70/120    avg_loss:0.008, val_acc:0.984]
Epoch [71/120    avg_loss:0.006, val_acc:0.984]
Epoch [72/120    avg_loss:0.004, val_acc:0.984]
Epoch [73/120    avg_loss:0.010, val_acc:0.984]
Epoch [74/120    avg_loss:0.010, val_acc:0.984]
Epoch [75/120    avg_loss:0.007, val_acc:0.983]
Epoch [76/120    avg_loss:0.005, val_acc:0.981]
Epoch [77/120    avg_loss:0.010, val_acc:0.982]
Epoch [78/120    avg_loss:0.005, val_acc:0.982]
Epoch [79/120    avg_loss:0.004, val_acc:0.982]
Epoch [80/120    avg_loss:0.010, val_acc:0.982]
Epoch [81/120    avg_loss:0.010, val_acc:0.982]
Epoch [82/120    avg_loss:0.005, val_acc:0.982]
Epoch [83/120    avg_loss:0.004, val_acc:0.982]
Epoch [84/120    avg_loss:0.006, val_acc:0.982]
Epoch [85/120    avg_loss:0.005, val_acc:0.982]
Epoch [86/120    avg_loss:0.008, val_acc:0.982]
Epoch [87/120    avg_loss:0.004, val_acc:0.983]
Epoch [88/120    avg_loss:0.004, val_acc:0.983]
Epoch [89/120    avg_loss:0.005, val_acc:0.983]
Epoch [90/120    avg_loss:0.008, val_acc:0.983]
Epoch [91/120    avg_loss:0.005, val_acc:0.983]
Epoch [92/120    avg_loss:0.005, val_acc:0.983]
Epoch [93/120    avg_loss:0.004, val_acc:0.983]
Epoch [94/120    avg_loss:0.007, val_acc:0.983]
Epoch [95/120    avg_loss:0.007, val_acc:0.983]
Epoch [96/120    avg_loss:0.004, val_acc:0.983]
Epoch [97/120    avg_loss:0.004, val_acc:0.983]
Epoch [98/120    avg_loss:0.006, val_acc:0.983]
Epoch [99/120    avg_loss:0.003, val_acc:0.983]
Epoch [100/120    avg_loss:0.006, val_acc:0.983]
Epoch [101/120    avg_loss:0.005, val_acc:0.983]
Epoch [102/120    avg_loss:0.008, val_acc:0.983]
Epoch [103/120    avg_loss:0.007, val_acc:0.983]
Epoch [104/120    avg_loss:0.004, val_acc:0.983]
Epoch [105/120    avg_loss:0.006, val_acc:0.983]
Epoch [106/120    avg_loss:0.006, val_acc:0.983]
Epoch [107/120    avg_loss:0.008, val_acc:0.983]
Epoch [108/120    avg_loss:0.009, val_acc:0.983]
Epoch [109/120    avg_loss:0.005, val_acc:0.983]
Epoch [110/120    avg_loss:0.005, val_acc:0.983]
Epoch [111/120    avg_loss:0.008, val_acc:0.983]
Epoch [112/120    avg_loss:0.005, val_acc:0.983]
Epoch [113/120    avg_loss:0.005, val_acc:0.983]
Epoch [114/120    avg_loss:0.004, val_acc:0.983]
Epoch [115/120    avg_loss:0.006, val_acc:0.983]
Epoch [116/120    avg_loss:0.004, val_acc:0.983]
Epoch [117/120    avg_loss:0.006, val_acc:0.983]
Epoch [118/120    avg_loss:0.007, val_acc:0.983]
Epoch [119/120    avg_loss:0.008, val_acc:0.983]
Epoch [120/120    avg_loss:0.006, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1271    4    4    0    1    0    0    0    1    4    0    0
     0    0    0]
 [   0    0    0  731    0    0    0    0    0    3    0    0   12    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    0    0    0    0    0  851   16    0    0
     0    3    0]
 [   0    0    3    0    0    0    1    0    0    0    0 2205    0    0
     1    0    0]
 [   0    0    3    0    0    0    0    0    0    0    0    0  528    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1131    5    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    15  321    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.82926829268293

F1 scores:
[       nan 0.975      0.98987539 0.98451178 0.99069767 0.99541284
 0.98867925 1.         1.         0.81081081 0.9849537  0.99391481
 0.98232558 0.99730458 0.98950131 0.94970414 0.97647059]

Kappa:
0.986648147129887
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f80bb48e6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.192, val_acc:0.515]
Epoch [2/120    avg_loss:1.605, val_acc:0.559]
Epoch [3/120    avg_loss:1.384, val_acc:0.699]
Epoch [4/120    avg_loss:1.072, val_acc:0.752]
Epoch [5/120    avg_loss:1.093, val_acc:0.751]
Epoch [6/120    avg_loss:0.607, val_acc:0.818]
Epoch [7/120    avg_loss:0.657, val_acc:0.778]
Epoch [8/120    avg_loss:0.802, val_acc:0.816]
Epoch [9/120    avg_loss:0.468, val_acc:0.844]
Epoch [10/120    avg_loss:0.348, val_acc:0.846]
Epoch [11/120    avg_loss:0.310, val_acc:0.839]
Epoch [12/120    avg_loss:0.344, val_acc:0.850]
Epoch [13/120    avg_loss:0.370, val_acc:0.870]
Epoch [14/120    avg_loss:0.292, val_acc:0.836]
Epoch [15/120    avg_loss:0.197, val_acc:0.874]
Epoch [16/120    avg_loss:0.183, val_acc:0.905]
Epoch [17/120    avg_loss:0.148, val_acc:0.908]
Epoch [18/120    avg_loss:0.136, val_acc:0.940]
Epoch [19/120    avg_loss:0.085, val_acc:0.927]
Epoch [20/120    avg_loss:0.082, val_acc:0.936]
Epoch [21/120    avg_loss:0.099, val_acc:0.900]
Epoch [22/120    avg_loss:0.117, val_acc:0.942]
Epoch [23/120    avg_loss:0.085, val_acc:0.914]
Epoch [24/120    avg_loss:0.054, val_acc:0.935]
Epoch [25/120    avg_loss:0.052, val_acc:0.951]
Epoch [26/120    avg_loss:0.070, val_acc:0.941]
Epoch [27/120    avg_loss:0.072, val_acc:0.935]
Epoch [28/120    avg_loss:0.163, val_acc:0.889]
Epoch [29/120    avg_loss:0.099, val_acc:0.939]
Epoch [30/120    avg_loss:0.078, val_acc:0.944]
Epoch [31/120    avg_loss:0.174, val_acc:0.871]
Epoch [32/120    avg_loss:0.135, val_acc:0.918]
Epoch [33/120    avg_loss:0.073, val_acc:0.925]
Epoch [34/120    avg_loss:0.127, val_acc:0.928]
Epoch [35/120    avg_loss:0.081, val_acc:0.952]
Epoch [36/120    avg_loss:0.048, val_acc:0.945]
Epoch [37/120    avg_loss:0.047, val_acc:0.959]
Epoch [38/120    avg_loss:0.112, val_acc:0.862]
Epoch [39/120    avg_loss:0.174, val_acc:0.847]
Epoch [40/120    avg_loss:0.426, val_acc:0.905]
Epoch [41/120    avg_loss:0.158, val_acc:0.921]
Epoch [42/120    avg_loss:0.081, val_acc:0.942]
Epoch [43/120    avg_loss:0.062, val_acc:0.963]
Epoch [44/120    avg_loss:0.042, val_acc:0.956]
Epoch [45/120    avg_loss:0.034, val_acc:0.962]
Epoch [46/120    avg_loss:0.045, val_acc:0.956]
Epoch [47/120    avg_loss:0.024, val_acc:0.964]
Epoch [48/120    avg_loss:0.041, val_acc:0.964]
Epoch [49/120    avg_loss:0.035, val_acc:0.963]
Epoch [50/120    avg_loss:0.061, val_acc:0.950]
Epoch [51/120    avg_loss:0.052, val_acc:0.962]
Epoch [52/120    avg_loss:0.022, val_acc:0.967]
Epoch [53/120    avg_loss:0.039, val_acc:0.962]
Epoch [54/120    avg_loss:0.020, val_acc:0.968]
Epoch [55/120    avg_loss:0.022, val_acc:0.969]
Epoch [56/120    avg_loss:0.032, val_acc:0.977]
Epoch [57/120    avg_loss:0.019, val_acc:0.979]
Epoch [58/120    avg_loss:0.026, val_acc:0.974]
Epoch [59/120    avg_loss:0.027, val_acc:0.970]
Epoch [60/120    avg_loss:0.018, val_acc:0.969]
Epoch [61/120    avg_loss:0.016, val_acc:0.967]
Epoch [62/120    avg_loss:0.014, val_acc:0.972]
Epoch [63/120    avg_loss:0.007, val_acc:0.976]
Epoch [64/120    avg_loss:0.010, val_acc:0.974]
Epoch [65/120    avg_loss:0.013, val_acc:0.977]
Epoch [66/120    avg_loss:0.016, val_acc:0.976]
Epoch [67/120    avg_loss:0.006, val_acc:0.979]
Epoch [68/120    avg_loss:0.010, val_acc:0.960]
Epoch [69/120    avg_loss:0.024, val_acc:0.957]
Epoch [70/120    avg_loss:0.019, val_acc:0.971]
Epoch [71/120    avg_loss:0.009, val_acc:0.983]
Epoch [72/120    avg_loss:0.008, val_acc:0.979]
Epoch [73/120    avg_loss:0.004, val_acc:0.982]
Epoch [74/120    avg_loss:0.004, val_acc:0.980]
Epoch [75/120    avg_loss:0.004, val_acc:0.982]
Epoch [76/120    avg_loss:0.011, val_acc:0.982]
Epoch [77/120    avg_loss:0.004, val_acc:0.986]
Epoch [78/120    avg_loss:0.005, val_acc:0.983]
Epoch [79/120    avg_loss:0.007, val_acc:0.979]
Epoch [80/120    avg_loss:0.005, val_acc:0.985]
Epoch [81/120    avg_loss:0.005, val_acc:0.988]
Epoch [82/120    avg_loss:0.003, val_acc:0.987]
Epoch [83/120    avg_loss:0.004, val_acc:0.983]
Epoch [84/120    avg_loss:0.004, val_acc:0.982]
Epoch [85/120    avg_loss:0.004, val_acc:0.981]
Epoch [86/120    avg_loss:0.007, val_acc:0.981]
Epoch [87/120    avg_loss:0.007, val_acc:0.970]
Epoch [88/120    avg_loss:0.005, val_acc:0.978]
Epoch [89/120    avg_loss:0.005, val_acc:0.982]
Epoch [90/120    avg_loss:0.005, val_acc:0.977]
Epoch [91/120    avg_loss:0.004, val_acc:0.978]
Epoch [92/120    avg_loss:0.004, val_acc:0.981]
Epoch [93/120    avg_loss:0.006, val_acc:0.987]
Epoch [94/120    avg_loss:0.004, val_acc:0.987]
Epoch [95/120    avg_loss:0.004, val_acc:0.989]
Epoch [96/120    avg_loss:0.003, val_acc:0.988]
Epoch [97/120    avg_loss:0.003, val_acc:0.989]
Epoch [98/120    avg_loss:0.003, val_acc:0.989]
Epoch [99/120    avg_loss:0.004, val_acc:0.990]
Epoch [100/120    avg_loss:0.004, val_acc:0.990]
Epoch [101/120    avg_loss:0.003, val_acc:0.990]
Epoch [102/120    avg_loss:0.006, val_acc:0.990]
Epoch [103/120    avg_loss:0.002, val_acc:0.990]
Epoch [104/120    avg_loss:0.002, val_acc:0.989]
Epoch [105/120    avg_loss:0.002, val_acc:0.990]
Epoch [106/120    avg_loss:0.003, val_acc:0.989]
Epoch [107/120    avg_loss:0.003, val_acc:0.990]
Epoch [108/120    avg_loss:0.003, val_acc:0.989]
Epoch [109/120    avg_loss:0.005, val_acc:0.989]
Epoch [110/120    avg_loss:0.004, val_acc:0.989]
Epoch [111/120    avg_loss:0.003, val_acc:0.989]
Epoch [112/120    avg_loss:0.003, val_acc:0.992]
Epoch [113/120    avg_loss:0.003, val_acc:0.992]
Epoch [114/120    avg_loss:0.003, val_acc:0.992]
Epoch [115/120    avg_loss:0.002, val_acc:0.992]
Epoch [116/120    avg_loss:0.002, val_acc:0.990]
Epoch [117/120    avg_loss:0.006, val_acc:0.990]
Epoch [118/120    avg_loss:0.004, val_acc:0.990]
Epoch [119/120    avg_loss:0.002, val_acc:0.990]
Epoch [120/120    avg_loss:0.003, val_acc:0.992]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1279    1    0    0    2    0    0    0    2    1    0    0
     0    0    0]
 [   0    0    0  710    0    6    0    0    0    6    0    0   24    0
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    0    0    4    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  874    1    0    0
     0    0    0]
 [   0    0    7    0    0    3    1    0    0    0    3 2194    1    0
     1    0    0]
 [   0    0    0    0    0    0    0    0    0    0    8    0  523    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    21  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.78590785907859

F1 scores:
[       nan 0.975      0.99455677 0.97326936 0.99764706 0.98165138
 0.99169811 1.         1.         0.7826087  0.99149178 0.99591466
 0.96494465 1.         0.98871528 0.95639098 0.9704142 ]

Kappa:
0.9861595412930126
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7ced85b6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.132, val_acc:0.329]
Epoch [2/120    avg_loss:1.718, val_acc:0.665]
Epoch [3/120    avg_loss:1.330, val_acc:0.570]
Epoch [4/120    avg_loss:1.116, val_acc:0.692]
Epoch [5/120    avg_loss:0.831, val_acc:0.578]
Epoch [6/120    avg_loss:1.022, val_acc:0.769]
Epoch [7/120    avg_loss:0.731, val_acc:0.762]
Epoch [8/120    avg_loss:0.733, val_acc:0.783]
Epoch [9/120    avg_loss:0.593, val_acc:0.818]
Epoch [10/120    avg_loss:0.458, val_acc:0.880]
Epoch [11/120    avg_loss:0.310, val_acc:0.890]
Epoch [12/120    avg_loss:0.257, val_acc:0.882]
Epoch [13/120    avg_loss:0.232, val_acc:0.909]
Epoch [14/120    avg_loss:0.241, val_acc:0.872]
Epoch [15/120    avg_loss:0.221, val_acc:0.917]
Epoch [16/120    avg_loss:0.186, val_acc:0.906]
Epoch [17/120    avg_loss:0.140, val_acc:0.917]
Epoch [18/120    avg_loss:0.135, val_acc:0.951]
Epoch [19/120    avg_loss:0.099, val_acc:0.942]
Epoch [20/120    avg_loss:0.112, val_acc:0.933]
Epoch [21/120    avg_loss:0.118, val_acc:0.941]
Epoch [22/120    avg_loss:0.113, val_acc:0.930]
Epoch [23/120    avg_loss:0.099, val_acc:0.929]
Epoch [24/120    avg_loss:0.062, val_acc:0.958]
Epoch [25/120    avg_loss:0.082, val_acc:0.944]
Epoch [26/120    avg_loss:0.096, val_acc:0.955]
Epoch [27/120    avg_loss:0.092, val_acc:0.947]
Epoch [28/120    avg_loss:0.068, val_acc:0.961]
Epoch [29/120    avg_loss:0.042, val_acc:0.967]
Epoch [30/120    avg_loss:0.045, val_acc:0.954]
Epoch [31/120    avg_loss:0.039, val_acc:0.964]
Epoch [32/120    avg_loss:0.067, val_acc:0.968]
Epoch [33/120    avg_loss:0.065, val_acc:0.943]
Epoch [34/120    avg_loss:0.086, val_acc:0.964]
Epoch [35/120    avg_loss:0.031, val_acc:0.968]
Epoch [36/120    avg_loss:0.042, val_acc:0.951]
Epoch [37/120    avg_loss:0.042, val_acc:0.954]
Epoch [38/120    avg_loss:0.027, val_acc:0.956]
Epoch [39/120    avg_loss:0.052, val_acc:0.956]
Epoch [40/120    avg_loss:0.050, val_acc:0.970]
Epoch [41/120    avg_loss:0.012, val_acc:0.982]
Epoch [42/120    avg_loss:0.023, val_acc:0.980]
Epoch [43/120    avg_loss:0.017, val_acc:0.978]
Epoch [44/120    avg_loss:0.031, val_acc:0.970]
Epoch [45/120    avg_loss:0.034, val_acc:0.942]
Epoch [46/120    avg_loss:0.044, val_acc:0.972]
Epoch [47/120    avg_loss:0.019, val_acc:0.976]
Epoch [48/120    avg_loss:0.013, val_acc:0.974]
Epoch [49/120    avg_loss:0.019, val_acc:0.974]
Epoch [50/120    avg_loss:0.019, val_acc:0.972]
Epoch [51/120    avg_loss:0.013, val_acc:0.974]
Epoch [52/120    avg_loss:0.009, val_acc:0.976]
Epoch [53/120    avg_loss:0.013, val_acc:0.977]
Epoch [54/120    avg_loss:0.011, val_acc:0.981]
Epoch [55/120    avg_loss:0.012, val_acc:0.980]
Epoch [56/120    avg_loss:0.010, val_acc:0.980]
Epoch [57/120    avg_loss:0.014, val_acc:0.980]
Epoch [58/120    avg_loss:0.008, val_acc:0.978]
Epoch [59/120    avg_loss:0.005, val_acc:0.980]
Epoch [60/120    avg_loss:0.006, val_acc:0.979]
Epoch [61/120    avg_loss:0.006, val_acc:0.980]
Epoch [62/120    avg_loss:0.006, val_acc:0.979]
Epoch [63/120    avg_loss:0.007, val_acc:0.979]
Epoch [64/120    avg_loss:0.006, val_acc:0.979]
Epoch [65/120    avg_loss:0.006, val_acc:0.979]
Epoch [66/120    avg_loss:0.006, val_acc:0.978]
Epoch [67/120    avg_loss:0.008, val_acc:0.978]
Epoch [68/120    avg_loss:0.005, val_acc:0.978]
Epoch [69/120    avg_loss:0.008, val_acc:0.977]
Epoch [70/120    avg_loss:0.005, val_acc:0.977]
Epoch [71/120    avg_loss:0.011, val_acc:0.977]
Epoch [72/120    avg_loss:0.008, val_acc:0.977]
Epoch [73/120    avg_loss:0.006, val_acc:0.977]
Epoch [74/120    avg_loss:0.007, val_acc:0.977]
Epoch [75/120    avg_loss:0.008, val_acc:0.977]
Epoch [76/120    avg_loss:0.005, val_acc:0.977]
Epoch [77/120    avg_loss:0.007, val_acc:0.977]
Epoch [78/120    avg_loss:0.006, val_acc:0.978]
Epoch [79/120    avg_loss:0.006, val_acc:0.978]
Epoch [80/120    avg_loss:0.009, val_acc:0.978]
Epoch [81/120    avg_loss:0.005, val_acc:0.978]
Epoch [82/120    avg_loss:0.005, val_acc:0.978]
Epoch [83/120    avg_loss:0.008, val_acc:0.978]
Epoch [84/120    avg_loss:0.009, val_acc:0.978]
Epoch [85/120    avg_loss:0.005, val_acc:0.978]
Epoch [86/120    avg_loss:0.007, val_acc:0.978]
Epoch [87/120    avg_loss:0.006, val_acc:0.978]
Epoch [88/120    avg_loss:0.008, val_acc:0.978]
Epoch [89/120    avg_loss:0.007, val_acc:0.978]
Epoch [90/120    avg_loss:0.005, val_acc:0.978]
Epoch [91/120    avg_loss:0.005, val_acc:0.978]
Epoch [92/120    avg_loss:0.009, val_acc:0.978]
Epoch [93/120    avg_loss:0.005, val_acc:0.978]
Epoch [94/120    avg_loss:0.005, val_acc:0.978]
Epoch [95/120    avg_loss:0.004, val_acc:0.978]
Epoch [96/120    avg_loss:0.006, val_acc:0.978]
Epoch [97/120    avg_loss:0.006, val_acc:0.978]
Epoch [98/120    avg_loss:0.005, val_acc:0.978]
Epoch [99/120    avg_loss:0.005, val_acc:0.978]
Epoch [100/120    avg_loss:0.010, val_acc:0.978]
Epoch [101/120    avg_loss:0.008, val_acc:0.978]
Epoch [102/120    avg_loss:0.007, val_acc:0.978]
Epoch [103/120    avg_loss:0.009, val_acc:0.978]
Epoch [104/120    avg_loss:0.005, val_acc:0.978]
Epoch [105/120    avg_loss:0.006, val_acc:0.978]
Epoch [106/120    avg_loss:0.005, val_acc:0.978]
Epoch [107/120    avg_loss:0.007, val_acc:0.978]
Epoch [108/120    avg_loss:0.006, val_acc:0.978]
Epoch [109/120    avg_loss:0.007, val_acc:0.978]
Epoch [110/120    avg_loss:0.007, val_acc:0.978]
Epoch [111/120    avg_loss:0.007, val_acc:0.978]
Epoch [112/120    avg_loss:0.006, val_acc:0.978]
Epoch [113/120    avg_loss:0.009, val_acc:0.978]
Epoch [114/120    avg_loss:0.007, val_acc:0.978]
Epoch [115/120    avg_loss:0.006, val_acc:0.978]
Epoch [116/120    avg_loss:0.006, val_acc:0.978]
Epoch [117/120    avg_loss:0.006, val_acc:0.978]
Epoch [118/120    avg_loss:0.005, val_acc:0.978]
Epoch [119/120    avg_loss:0.006, val_acc:0.978]
Epoch [120/120    avg_loss:0.005, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    2    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1271    4    0    0    0    0    0    0    6    4    0    0
     0    0    0]
 [   0    0    0  718    1   12    0    0    0    7    1    0    6    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  435    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    1    1    0    0    0  867    3    0    0
     0    3    0]
 [   0    0    5    0    0    0    2    0    0    0    8 2192    2    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  532    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    1    0    0    0
  1130    4    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    23  312    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.62330623306234

F1 scores:
[       nan 0.94871795 0.99180648 0.97687075 0.99765808 0.98194131
 0.98718915 1.         0.995338   0.80952381 0.98522727 0.99387894
 0.98609824 0.9919571  0.98603839 0.93693694 0.97619048]

Kappa:
0.9843066709109392
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc7b433b6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.216, val_acc:0.428]
Epoch [2/120    avg_loss:1.507, val_acc:0.613]
Epoch [3/120    avg_loss:1.258, val_acc:0.696]
Epoch [4/120    avg_loss:1.261, val_acc:0.653]
Epoch [5/120    avg_loss:0.879, val_acc:0.730]
Epoch [6/120    avg_loss:0.752, val_acc:0.802]
Epoch [7/120    avg_loss:0.617, val_acc:0.802]
Epoch [8/120    avg_loss:0.820, val_acc:0.736]
Epoch [9/120    avg_loss:0.486, val_acc:0.825]
Epoch [10/120    avg_loss:0.466, val_acc:0.832]
Epoch [11/120    avg_loss:0.416, val_acc:0.837]
Epoch [12/120    avg_loss:0.267, val_acc:0.869]
Epoch [13/120    avg_loss:0.288, val_acc:0.897]
Epoch [14/120    avg_loss:0.196, val_acc:0.895]
Epoch [15/120    avg_loss:0.199, val_acc:0.906]
Epoch [16/120    avg_loss:0.173, val_acc:0.915]
Epoch [17/120    avg_loss:0.149, val_acc:0.922]
Epoch [18/120    avg_loss:0.088, val_acc:0.935]
Epoch [19/120    avg_loss:0.116, val_acc:0.906]
Epoch [20/120    avg_loss:0.121, val_acc:0.942]
Epoch [21/120    avg_loss:0.090, val_acc:0.931]
Epoch [22/120    avg_loss:0.101, val_acc:0.946]
Epoch [23/120    avg_loss:0.121, val_acc:0.938]
Epoch [24/120    avg_loss:0.062, val_acc:0.947]
Epoch [25/120    avg_loss:0.119, val_acc:0.919]
Epoch [26/120    avg_loss:0.063, val_acc:0.958]
Epoch [27/120    avg_loss:0.058, val_acc:0.942]
Epoch [28/120    avg_loss:0.081, val_acc:0.929]
Epoch [29/120    avg_loss:0.062, val_acc:0.944]
Epoch [30/120    avg_loss:0.043, val_acc:0.960]
Epoch [31/120    avg_loss:0.067, val_acc:0.943]
Epoch [32/120    avg_loss:0.047, val_acc:0.961]
Epoch [33/120    avg_loss:0.054, val_acc:0.951]
Epoch [34/120    avg_loss:0.085, val_acc:0.954]
Epoch [35/120    avg_loss:0.043, val_acc:0.962]
Epoch [36/120    avg_loss:0.036, val_acc:0.957]
Epoch [37/120    avg_loss:0.053, val_acc:0.946]
Epoch [38/120    avg_loss:0.037, val_acc:0.954]
Epoch [39/120    avg_loss:0.037, val_acc:0.962]
Epoch [40/120    avg_loss:0.033, val_acc:0.968]
Epoch [41/120    avg_loss:0.021, val_acc:0.957]
Epoch [42/120    avg_loss:0.019, val_acc:0.965]
Epoch [43/120    avg_loss:0.018, val_acc:0.966]
Epoch [44/120    avg_loss:0.021, val_acc:0.974]
Epoch [45/120    avg_loss:0.016, val_acc:0.972]
Epoch [46/120    avg_loss:0.018, val_acc:0.967]
Epoch [47/120    avg_loss:0.028, val_acc:0.962]
Epoch [48/120    avg_loss:0.030, val_acc:0.954]
Epoch [49/120    avg_loss:0.059, val_acc:0.953]
Epoch [50/120    avg_loss:0.026, val_acc:0.952]
Epoch [51/120    avg_loss:0.018, val_acc:0.966]
Epoch [52/120    avg_loss:0.022, val_acc:0.972]
Epoch [53/120    avg_loss:0.021, val_acc:0.960]
Epoch [54/120    avg_loss:0.025, val_acc:0.966]
Epoch [55/120    avg_loss:0.023, val_acc:0.965]
Epoch [56/120    avg_loss:0.012, val_acc:0.968]
Epoch [57/120    avg_loss:0.010, val_acc:0.959]
Epoch [58/120    avg_loss:0.019, val_acc:0.966]
Epoch [59/120    avg_loss:0.011, val_acc:0.970]
Epoch [60/120    avg_loss:0.010, val_acc:0.971]
Epoch [61/120    avg_loss:0.010, val_acc:0.971]
Epoch [62/120    avg_loss:0.013, val_acc:0.971]
Epoch [63/120    avg_loss:0.008, val_acc:0.970]
Epoch [64/120    avg_loss:0.009, val_acc:0.972]
Epoch [65/120    avg_loss:0.007, val_acc:0.975]
Epoch [66/120    avg_loss:0.006, val_acc:0.978]
Epoch [67/120    avg_loss:0.006, val_acc:0.978]
Epoch [68/120    avg_loss:0.005, val_acc:0.974]
Epoch [69/120    avg_loss:0.007, val_acc:0.972]
Epoch [70/120    avg_loss:0.009, val_acc:0.974]
Epoch [71/120    avg_loss:0.005, val_acc:0.975]
Epoch [72/120    avg_loss:0.009, val_acc:0.979]
Epoch [73/120    avg_loss:0.005, val_acc:0.979]
Epoch [74/120    avg_loss:0.006, val_acc:0.979]
Epoch [75/120    avg_loss:0.006, val_acc:0.979]
Epoch [76/120    avg_loss:0.005, val_acc:0.977]
Epoch [77/120    avg_loss:0.004, val_acc:0.978]
Epoch [78/120    avg_loss:0.005, val_acc:0.978]
Epoch [79/120    avg_loss:0.007, val_acc:0.979]
Epoch [80/120    avg_loss:0.005, val_acc:0.980]
Epoch [81/120    avg_loss:0.008, val_acc:0.979]
Epoch [82/120    avg_loss:0.010, val_acc:0.977]
Epoch [83/120    avg_loss:0.005, val_acc:0.978]
Epoch [84/120    avg_loss:0.006, val_acc:0.978]
Epoch [85/120    avg_loss:0.008, val_acc:0.978]
Epoch [86/120    avg_loss:0.006, val_acc:0.980]
Epoch [87/120    avg_loss:0.006, val_acc:0.980]
Epoch [88/120    avg_loss:0.005, val_acc:0.980]
Epoch [89/120    avg_loss:0.008, val_acc:0.981]
Epoch [90/120    avg_loss:0.004, val_acc:0.981]
Epoch [91/120    avg_loss:0.004, val_acc:0.979]
Epoch [92/120    avg_loss:0.010, val_acc:0.980]
Epoch [93/120    avg_loss:0.009, val_acc:0.981]
Epoch [94/120    avg_loss:0.006, val_acc:0.981]
Epoch [95/120    avg_loss:0.005, val_acc:0.979]
Epoch [96/120    avg_loss:0.004, val_acc:0.979]
Epoch [97/120    avg_loss:0.004, val_acc:0.979]
Epoch [98/120    avg_loss:0.005, val_acc:0.980]
Epoch [99/120    avg_loss:0.005, val_acc:0.980]
Epoch [100/120    avg_loss:0.005, val_acc:0.979]
Epoch [101/120    avg_loss:0.004, val_acc:0.979]
Epoch [102/120    avg_loss:0.005, val_acc:0.981]
Epoch [103/120    avg_loss:0.004, val_acc:0.981]
Epoch [104/120    avg_loss:0.004, val_acc:0.979]
Epoch [105/120    avg_loss:0.009, val_acc:0.980]
Epoch [106/120    avg_loss:0.005, val_acc:0.981]
Epoch [107/120    avg_loss:0.004, val_acc:0.982]
Epoch [108/120    avg_loss:0.007, val_acc:0.981]
Epoch [109/120    avg_loss:0.003, val_acc:0.982]
Epoch [110/120    avg_loss:0.004, val_acc:0.983]
Epoch [111/120    avg_loss:0.003, val_acc:0.983]
Epoch [112/120    avg_loss:0.004, val_acc:0.982]
Epoch [113/120    avg_loss:0.008, val_acc:0.980]
Epoch [114/120    avg_loss:0.009, val_acc:0.980]
Epoch [115/120    avg_loss:0.006, val_acc:0.980]
Epoch [116/120    avg_loss:0.006, val_acc:0.979]
Epoch [117/120    avg_loss:0.003, val_acc:0.980]
Epoch [118/120    avg_loss:0.004, val_acc:0.980]
Epoch [119/120    avg_loss:0.004, val_acc:0.980]
Epoch [120/120    avg_loss:0.006, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1268    1    0    4    3    0    0    0    4    2    0    0
     0    3    0]
 [   0    0    0  712    1   13    0    0    0    6    0    1   13    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    1    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    0    0
     0    0    2]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0  868    3    0    0
     0    2    0]
 [   0    0    8    0    0    2    0    0    0    3    8 2185    4    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0   14    0  520    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    1    0    0    0
  1129    3    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    23  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.25474254742548

F1 scores:
[       nan 0.96202532 0.99023819 0.97467488 0.99765808 0.96868009
 0.98271976 1.         0.99650757 0.75555556 0.97913142 0.99227975
 0.96834264 1.         0.98516579 0.92911011 0.97005988]

Kappa:
0.9801082661580083
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:08:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7eff60f4b6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.169, val_acc:0.430]
Epoch [2/120    avg_loss:1.433, val_acc:0.515]
Epoch [3/120    avg_loss:1.280, val_acc:0.739]
Epoch [4/120    avg_loss:1.206, val_acc:0.674]
Epoch [5/120    avg_loss:1.016, val_acc:0.775]
Epoch [6/120    avg_loss:0.766, val_acc:0.738]
Epoch [7/120    avg_loss:0.725, val_acc:0.823]
Epoch [8/120    avg_loss:0.562, val_acc:0.869]
Epoch [9/120    avg_loss:0.436, val_acc:0.817]
Epoch [10/120    avg_loss:0.565, val_acc:0.862]
Epoch [11/120    avg_loss:0.348, val_acc:0.867]
Epoch [12/120    avg_loss:0.240, val_acc:0.905]
Epoch [13/120    avg_loss:0.285, val_acc:0.909]
Epoch [14/120    avg_loss:0.277, val_acc:0.916]
Epoch [15/120    avg_loss:0.348, val_acc:0.899]
Epoch [16/120    avg_loss:0.290, val_acc:0.940]
Epoch [17/120    avg_loss:0.253, val_acc:0.874]
Epoch [18/120    avg_loss:0.230, val_acc:0.919]
Epoch [19/120    avg_loss:0.180, val_acc:0.921]
Epoch [20/120    avg_loss:0.130, val_acc:0.949]
Epoch [21/120    avg_loss:0.125, val_acc:0.935]
Epoch [22/120    avg_loss:0.076, val_acc:0.953]
Epoch [23/120    avg_loss:0.097, val_acc:0.942]
Epoch [24/120    avg_loss:0.111, val_acc:0.946]
Epoch [25/120    avg_loss:0.059, val_acc:0.964]
Epoch [26/120    avg_loss:0.122, val_acc:0.924]
Epoch [27/120    avg_loss:0.073, val_acc:0.947]
Epoch [28/120    avg_loss:0.075, val_acc:0.952]
Epoch [29/120    avg_loss:0.114, val_acc:0.936]
Epoch [30/120    avg_loss:0.099, val_acc:0.923]
Epoch [31/120    avg_loss:0.090, val_acc:0.959]
Epoch [32/120    avg_loss:0.055, val_acc:0.966]
Epoch [33/120    avg_loss:0.041, val_acc:0.970]
Epoch [34/120    avg_loss:0.030, val_acc:0.963]
Epoch [35/120    avg_loss:0.038, val_acc:0.980]
Epoch [36/120    avg_loss:0.027, val_acc:0.980]
Epoch [37/120    avg_loss:0.031, val_acc:0.978]
Epoch [38/120    avg_loss:0.030, val_acc:0.970]
Epoch [39/120    avg_loss:0.034, val_acc:0.980]
Epoch [40/120    avg_loss:0.023, val_acc:0.981]
Epoch [41/120    avg_loss:0.024, val_acc:0.978]
Epoch [42/120    avg_loss:0.019, val_acc:0.969]
Epoch [43/120    avg_loss:0.059, val_acc:0.968]
Epoch [44/120    avg_loss:0.037, val_acc:0.975]
Epoch [45/120    avg_loss:0.026, val_acc:0.958]
Epoch [46/120    avg_loss:0.021, val_acc:0.978]
Epoch [47/120    avg_loss:0.045, val_acc:0.949]
Epoch [48/120    avg_loss:0.048, val_acc:0.970]
Epoch [49/120    avg_loss:0.028, val_acc:0.974]
Epoch [50/120    avg_loss:0.016, val_acc:0.972]
Epoch [51/120    avg_loss:0.023, val_acc:0.976]
Epoch [52/120    avg_loss:0.022, val_acc:0.976]
Epoch [53/120    avg_loss:0.017, val_acc:0.976]
Epoch [54/120    avg_loss:0.018, val_acc:0.976]
Epoch [55/120    avg_loss:0.017, val_acc:0.976]
Epoch [56/120    avg_loss:0.012, val_acc:0.977]
Epoch [57/120    avg_loss:0.009, val_acc:0.979]
Epoch [58/120    avg_loss:0.008, val_acc:0.977]
Epoch [59/120    avg_loss:0.009, val_acc:0.979]
Epoch [60/120    avg_loss:0.013, val_acc:0.981]
Epoch [61/120    avg_loss:0.006, val_acc:0.979]
Epoch [62/120    avg_loss:0.008, val_acc:0.980]
Epoch [63/120    avg_loss:0.008, val_acc:0.979]
Epoch [64/120    avg_loss:0.007, val_acc:0.979]
Epoch [65/120    avg_loss:0.013, val_acc:0.980]
Epoch [66/120    avg_loss:0.008, val_acc:0.978]
Epoch [67/120    avg_loss:0.014, val_acc:0.980]
Epoch [68/120    avg_loss:0.009, val_acc:0.980]
Epoch [69/120    avg_loss:0.007, val_acc:0.978]
Epoch [70/120    avg_loss:0.006, val_acc:0.981]
Epoch [71/120    avg_loss:0.005, val_acc:0.980]
Epoch [72/120    avg_loss:0.005, val_acc:0.981]
Epoch [73/120    avg_loss:0.007, val_acc:0.982]
Epoch [74/120    avg_loss:0.009, val_acc:0.984]
Epoch [75/120    avg_loss:0.010, val_acc:0.982]
Epoch [76/120    avg_loss:0.006, val_acc:0.982]
Epoch [77/120    avg_loss:0.005, val_acc:0.982]
Epoch [78/120    avg_loss:0.005, val_acc:0.982]
Epoch [79/120    avg_loss:0.006, val_acc:0.983]
Epoch [80/120    avg_loss:0.005, val_acc:0.982]
Epoch [81/120    avg_loss:0.009, val_acc:0.984]
Epoch [82/120    avg_loss:0.006, val_acc:0.982]
Epoch [83/120    avg_loss:0.006, val_acc:0.982]
Epoch [84/120    avg_loss:0.008, val_acc:0.981]
Epoch [85/120    avg_loss:0.006, val_acc:0.982]
Epoch [86/120    avg_loss:0.006, val_acc:0.982]
Epoch [87/120    avg_loss:0.008, val_acc:0.980]
Epoch [88/120    avg_loss:0.007, val_acc:0.980]
Epoch [89/120    avg_loss:0.004, val_acc:0.981]
Epoch [90/120    avg_loss:0.006, val_acc:0.981]
Epoch [91/120    avg_loss:0.005, val_acc:0.981]
Epoch [92/120    avg_loss:0.006, val_acc:0.982]
Epoch [93/120    avg_loss:0.006, val_acc:0.981]
Epoch [94/120    avg_loss:0.012, val_acc:0.981]
Epoch [95/120    avg_loss:0.008, val_acc:0.981]
Epoch [96/120    avg_loss:0.005, val_acc:0.981]
Epoch [97/120    avg_loss:0.007, val_acc:0.981]
Epoch [98/120    avg_loss:0.004, val_acc:0.981]
Epoch [99/120    avg_loss:0.004, val_acc:0.981]
Epoch [100/120    avg_loss:0.007, val_acc:0.981]
Epoch [101/120    avg_loss:0.005, val_acc:0.981]
Epoch [102/120    avg_loss:0.008, val_acc:0.981]
Epoch [103/120    avg_loss:0.006, val_acc:0.981]
Epoch [104/120    avg_loss:0.005, val_acc:0.981]
Epoch [105/120    avg_loss:0.006, val_acc:0.981]
Epoch [106/120    avg_loss:0.007, val_acc:0.981]
Epoch [107/120    avg_loss:0.008, val_acc:0.981]
Epoch [108/120    avg_loss:0.006, val_acc:0.981]
Epoch [109/120    avg_loss:0.008, val_acc:0.981]
Epoch [110/120    avg_loss:0.003, val_acc:0.981]
Epoch [111/120    avg_loss:0.008, val_acc:0.981]
Epoch [112/120    avg_loss:0.005, val_acc:0.981]
Epoch [113/120    avg_loss:0.007, val_acc:0.981]
Epoch [114/120    avg_loss:0.004, val_acc:0.981]
Epoch [115/120    avg_loss:0.005, val_acc:0.981]
Epoch [116/120    avg_loss:0.007, val_acc:0.981]
Epoch [117/120    avg_loss:0.006, val_acc:0.981]
Epoch [118/120    avg_loss:0.007, val_acc:0.981]
Epoch [119/120    avg_loss:0.006, val_acc:0.981]
Epoch [120/120    avg_loss:0.005, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    3    0    4    2    0    0    0    4    3    2    0
     0    0    0]
 [   0    0    0  733    0    1    1    0    0    3    1    0    7    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    2    0    0    0    0  865    2    0    0
     0    0    0]
 [   0    0    5    0    0    0    2    0    0    0    1 2195    7    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0    0    0  530    0
     0    1    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1138    1    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    38  306    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.76422764227642

F1 scores:
[       nan 1.         0.98829953 0.98787062 1.         0.98742857
 0.99242424 0.98039216 1.         0.85       0.9908362  0.9950136
 0.97785978 0.99459459 0.98315335 0.93435115 0.96385542]

Kappa:
0.9859097819273972
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f967f1816a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.179, val_acc:0.423]
Epoch [2/120    avg_loss:1.637, val_acc:0.520]
Epoch [3/120    avg_loss:1.384, val_acc:0.656]
Epoch [4/120    avg_loss:1.004, val_acc:0.715]
Epoch [5/120    avg_loss:0.891, val_acc:0.788]
Epoch [6/120    avg_loss:0.999, val_acc:0.762]
Epoch [7/120    avg_loss:0.653, val_acc:0.808]
Epoch [8/120    avg_loss:0.534, val_acc:0.801]
Epoch [9/120    avg_loss:0.690, val_acc:0.846]
Epoch [10/120    avg_loss:0.576, val_acc:0.857]
Epoch [11/120    avg_loss:0.374, val_acc:0.900]
Epoch [12/120    avg_loss:0.323, val_acc:0.861]
Epoch [13/120    avg_loss:0.370, val_acc:0.835]
Epoch [14/120    avg_loss:0.346, val_acc:0.842]
Epoch [15/120    avg_loss:0.356, val_acc:0.875]
Epoch [16/120    avg_loss:0.230, val_acc:0.916]
Epoch [17/120    avg_loss:0.208, val_acc:0.928]
Epoch [18/120    avg_loss:0.187, val_acc:0.927]
Epoch [19/120    avg_loss:0.125, val_acc:0.947]
Epoch [20/120    avg_loss:0.102, val_acc:0.950]
Epoch [21/120    avg_loss:0.101, val_acc:0.946]
Epoch [22/120    avg_loss:0.115, val_acc:0.943]
Epoch [23/120    avg_loss:0.085, val_acc:0.953]
Epoch [24/120    avg_loss:0.115, val_acc:0.925]
Epoch [25/120    avg_loss:0.199, val_acc:0.935]
Epoch [26/120    avg_loss:0.092, val_acc:0.967]
Epoch [27/120    avg_loss:0.092, val_acc:0.959]
Epoch [28/120    avg_loss:0.069, val_acc:0.970]
Epoch [29/120    avg_loss:0.092, val_acc:0.978]
Epoch [30/120    avg_loss:0.048, val_acc:0.983]
Epoch [31/120    avg_loss:0.033, val_acc:0.977]
Epoch [32/120    avg_loss:0.077, val_acc:0.964]
Epoch [33/120    avg_loss:0.067, val_acc:0.970]
Epoch [34/120    avg_loss:0.029, val_acc:0.983]
Epoch [35/120    avg_loss:0.033, val_acc:0.969]
Epoch [36/120    avg_loss:0.020, val_acc:0.986]
Epoch [37/120    avg_loss:0.020, val_acc:0.977]
Epoch [38/120    avg_loss:0.046, val_acc:0.956]
Epoch [39/120    avg_loss:0.044, val_acc:0.976]
Epoch [40/120    avg_loss:0.034, val_acc:0.978]
Epoch [41/120    avg_loss:0.029, val_acc:0.978]
Epoch [42/120    avg_loss:0.025, val_acc:0.981]
Epoch [43/120    avg_loss:0.032, val_acc:0.965]
Epoch [44/120    avg_loss:0.026, val_acc:0.974]
Epoch [45/120    avg_loss:0.020, val_acc:0.977]
Epoch [46/120    avg_loss:0.043, val_acc:0.961]
Epoch [47/120    avg_loss:0.021, val_acc:0.976]
Epoch [48/120    avg_loss:0.016, val_acc:0.979]
Epoch [49/120    avg_loss:0.025, val_acc:0.980]
Epoch [50/120    avg_loss:0.014, val_acc:0.987]
Epoch [51/120    avg_loss:0.009, val_acc:0.986]
Epoch [52/120    avg_loss:0.009, val_acc:0.985]
Epoch [53/120    avg_loss:0.018, val_acc:0.981]
Epoch [54/120    avg_loss:0.010, val_acc:0.983]
Epoch [55/120    avg_loss:0.008, val_acc:0.986]
Epoch [56/120    avg_loss:0.008, val_acc:0.983]
Epoch [57/120    avg_loss:0.007, val_acc:0.983]
Epoch [58/120    avg_loss:0.007, val_acc:0.984]
Epoch [59/120    avg_loss:0.008, val_acc:0.987]
Epoch [60/120    avg_loss:0.008, val_acc:0.987]
Epoch [61/120    avg_loss:0.006, val_acc:0.987]
Epoch [62/120    avg_loss:0.008, val_acc:0.985]
Epoch [63/120    avg_loss:0.007, val_acc:0.986]
Epoch [64/120    avg_loss:0.008, val_acc:0.983]
Epoch [65/120    avg_loss:0.005, val_acc:0.983]
Epoch [66/120    avg_loss:0.006, val_acc:0.984]
Epoch [67/120    avg_loss:0.005, val_acc:0.984]
Epoch [68/120    avg_loss:0.010, val_acc:0.985]
Epoch [69/120    avg_loss:0.009, val_acc:0.983]
Epoch [70/120    avg_loss:0.009, val_acc:0.982]
Epoch [71/120    avg_loss:0.005, val_acc:0.983]
Epoch [72/120    avg_loss:0.006, val_acc:0.985]
Epoch [73/120    avg_loss:0.005, val_acc:0.982]
Epoch [74/120    avg_loss:0.006, val_acc:0.984]
Epoch [75/120    avg_loss:0.008, val_acc:0.984]
Epoch [76/120    avg_loss:0.006, val_acc:0.984]
Epoch [77/120    avg_loss:0.006, val_acc:0.984]
Epoch [78/120    avg_loss:0.005, val_acc:0.984]
Epoch [79/120    avg_loss:0.009, val_acc:0.984]
Epoch [80/120    avg_loss:0.005, val_acc:0.984]
Epoch [81/120    avg_loss:0.008, val_acc:0.984]
Epoch [82/120    avg_loss:0.007, val_acc:0.984]
Epoch [83/120    avg_loss:0.007, val_acc:0.984]
Epoch [84/120    avg_loss:0.006, val_acc:0.984]
Epoch [85/120    avg_loss:0.006, val_acc:0.984]
Epoch [86/120    avg_loss:0.011, val_acc:0.984]
Epoch [87/120    avg_loss:0.011, val_acc:0.983]
Epoch [88/120    avg_loss:0.007, val_acc:0.983]
Epoch [89/120    avg_loss:0.004, val_acc:0.983]
Epoch [90/120    avg_loss:0.007, val_acc:0.983]
Epoch [91/120    avg_loss:0.008, val_acc:0.983]
Epoch [92/120    avg_loss:0.007, val_acc:0.983]
Epoch [93/120    avg_loss:0.007, val_acc:0.983]
Epoch [94/120    avg_loss:0.005, val_acc:0.983]
Epoch [95/120    avg_loss:0.010, val_acc:0.983]
Epoch [96/120    avg_loss:0.008, val_acc:0.983]
Epoch [97/120    avg_loss:0.006, val_acc:0.983]
Epoch [98/120    avg_loss:0.006, val_acc:0.983]
Epoch [99/120    avg_loss:0.008, val_acc:0.983]
Epoch [100/120    avg_loss:0.005, val_acc:0.983]
Epoch [101/120    avg_loss:0.007, val_acc:0.983]
Epoch [102/120    avg_loss:0.009, val_acc:0.983]
Epoch [103/120    avg_loss:0.005, val_acc:0.983]
Epoch [104/120    avg_loss:0.007, val_acc:0.983]
Epoch [105/120    avg_loss:0.005, val_acc:0.983]
Epoch [106/120    avg_loss:0.007, val_acc:0.983]
Epoch [107/120    avg_loss:0.008, val_acc:0.983]
Epoch [108/120    avg_loss:0.005, val_acc:0.983]
Epoch [109/120    avg_loss:0.008, val_acc:0.983]
Epoch [110/120    avg_loss:0.008, val_acc:0.983]
Epoch [111/120    avg_loss:0.008, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.983]
Epoch [113/120    avg_loss:0.006, val_acc:0.983]
Epoch [114/120    avg_loss:0.010, val_acc:0.983]
Epoch [115/120    avg_loss:0.005, val_acc:0.983]
Epoch [116/120    avg_loss:0.007, val_acc:0.983]
Epoch [117/120    avg_loss:0.010, val_acc:0.983]
Epoch [118/120    avg_loss:0.007, val_acc:0.983]
Epoch [119/120    avg_loss:0.009, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1273    7    0    0    0    0    0    0    3    2    0    0
     0    0    0]
 [   0    0    0  732    0    0    0    0    0    5    1    0    7    2
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0  864    7    0    0
     0    1    0]
 [   0    0   12    0    0    2    1    0    0    0    0 2188    5    2
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    4    0  529    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
  1130    3    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    14  317    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.78590785907859

F1 scores:
[       nan 0.98765432 0.98912199 0.98321021 0.99528302 0.99655568
 0.98279731 1.         0.997669   0.82926829 0.98912421 0.99296574
 0.98144712 0.98930481 0.98992554 0.9491018  0.98809524]

Kappa:
0.986160636752157
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f831d3ab6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.186, val_acc:0.542]
Epoch [2/120    avg_loss:1.656, val_acc:0.516]
Epoch [3/120    avg_loss:1.356, val_acc:0.642]
Epoch [4/120    avg_loss:0.987, val_acc:0.627]
Epoch [5/120    avg_loss:0.945, val_acc:0.685]
Epoch [6/120    avg_loss:0.765, val_acc:0.801]
Epoch [7/120    avg_loss:0.723, val_acc:0.778]
Epoch [8/120    avg_loss:0.535, val_acc:0.856]
Epoch [9/120    avg_loss:0.386, val_acc:0.860]
Epoch [10/120    avg_loss:0.439, val_acc:0.887]
Epoch [11/120    avg_loss:0.244, val_acc:0.873]
Epoch [12/120    avg_loss:0.243, val_acc:0.878]
Epoch [13/120    avg_loss:0.433, val_acc:0.803]
Epoch [14/120    avg_loss:0.307, val_acc:0.906]
Epoch [15/120    avg_loss:0.144, val_acc:0.875]
Epoch [16/120    avg_loss:0.168, val_acc:0.926]
Epoch [17/120    avg_loss:0.223, val_acc:0.921]
Epoch [18/120    avg_loss:0.166, val_acc:0.915]
Epoch [19/120    avg_loss:0.099, val_acc:0.923]
Epoch [20/120    avg_loss:0.099, val_acc:0.944]
Epoch [21/120    avg_loss:0.071, val_acc:0.938]
Epoch [22/120    avg_loss:0.068, val_acc:0.949]
Epoch [23/120    avg_loss:0.073, val_acc:0.950]
Epoch [24/120    avg_loss:0.072, val_acc:0.940]
Epoch [25/120    avg_loss:0.067, val_acc:0.960]
Epoch [26/120    avg_loss:0.052, val_acc:0.956]
Epoch [27/120    avg_loss:0.071, val_acc:0.945]
Epoch [28/120    avg_loss:0.041, val_acc:0.954]
Epoch [29/120    avg_loss:0.047, val_acc:0.947]
Epoch [30/120    avg_loss:0.056, val_acc:0.967]
Epoch [31/120    avg_loss:0.039, val_acc:0.960]
Epoch [32/120    avg_loss:0.054, val_acc:0.949]
Epoch [33/120    avg_loss:0.042, val_acc:0.966]
Epoch [34/120    avg_loss:0.038, val_acc:0.961]
Epoch [35/120    avg_loss:0.030, val_acc:0.968]
Epoch [36/120    avg_loss:0.031, val_acc:0.955]
Epoch [37/120    avg_loss:0.038, val_acc:0.966]
Epoch [38/120    avg_loss:0.017, val_acc:0.971]
Epoch [39/120    avg_loss:0.067, val_acc:0.948]
Epoch [40/120    avg_loss:0.058, val_acc:0.968]
Epoch [41/120    avg_loss:0.058, val_acc:0.947]
Epoch [42/120    avg_loss:0.082, val_acc:0.916]
Epoch [43/120    avg_loss:0.128, val_acc:0.929]
Epoch [44/120    avg_loss:0.056, val_acc:0.950]
Epoch [45/120    avg_loss:0.038, val_acc:0.972]
Epoch [46/120    avg_loss:0.023, val_acc:0.975]
Epoch [47/120    avg_loss:0.010, val_acc:0.975]
Epoch [48/120    avg_loss:0.030, val_acc:0.967]
Epoch [49/120    avg_loss:0.018, val_acc:0.973]
Epoch [50/120    avg_loss:0.015, val_acc:0.972]
Epoch [51/120    avg_loss:0.015, val_acc:0.973]
Epoch [52/120    avg_loss:0.016, val_acc:0.975]
Epoch [53/120    avg_loss:0.017, val_acc:0.974]
Epoch [54/120    avg_loss:0.021, val_acc:0.969]
Epoch [55/120    avg_loss:0.023, val_acc:0.947]
Epoch [56/120    avg_loss:0.022, val_acc:0.975]
Epoch [57/120    avg_loss:0.015, val_acc:0.972]
Epoch [58/120    avg_loss:0.017, val_acc:0.969]
Epoch [59/120    avg_loss:0.009, val_acc:0.980]
Epoch [60/120    avg_loss:0.006, val_acc:0.978]
Epoch [61/120    avg_loss:0.011, val_acc:0.974]
Epoch [62/120    avg_loss:0.023, val_acc:0.974]
Epoch [63/120    avg_loss:0.014, val_acc:0.973]
Epoch [64/120    avg_loss:0.010, val_acc:0.980]
Epoch [65/120    avg_loss:0.010, val_acc:0.981]
Epoch [66/120    avg_loss:0.009, val_acc:0.977]
Epoch [67/120    avg_loss:0.028, val_acc:0.955]
Epoch [68/120    avg_loss:0.022, val_acc:0.975]
Epoch [69/120    avg_loss:0.010, val_acc:0.983]
Epoch [70/120    avg_loss:0.007, val_acc:0.978]
Epoch [71/120    avg_loss:0.007, val_acc:0.977]
Epoch [72/120    avg_loss:0.024, val_acc:0.973]
Epoch [73/120    avg_loss:0.007, val_acc:0.979]
Epoch [74/120    avg_loss:0.007, val_acc:0.978]
Epoch [75/120    avg_loss:0.005, val_acc:0.980]
Epoch [76/120    avg_loss:0.006, val_acc:0.980]
Epoch [77/120    avg_loss:0.006, val_acc:0.979]
Epoch [78/120    avg_loss:0.014, val_acc:0.977]
Epoch [79/120    avg_loss:0.025, val_acc:0.974]
Epoch [80/120    avg_loss:0.013, val_acc:0.976]
Epoch [81/120    avg_loss:0.008, val_acc:0.982]
Epoch [82/120    avg_loss:0.004, val_acc:0.979]
Epoch [83/120    avg_loss:0.005, val_acc:0.981]
Epoch [84/120    avg_loss:0.006, val_acc:0.981]
Epoch [85/120    avg_loss:0.004, val_acc:0.982]
Epoch [86/120    avg_loss:0.004, val_acc:0.982]
Epoch [87/120    avg_loss:0.006, val_acc:0.982]
Epoch [88/120    avg_loss:0.005, val_acc:0.982]
Epoch [89/120    avg_loss:0.003, val_acc:0.981]
Epoch [90/120    avg_loss:0.004, val_acc:0.983]
Epoch [91/120    avg_loss:0.004, val_acc:0.983]
Epoch [92/120    avg_loss:0.004, val_acc:0.982]
Epoch [93/120    avg_loss:0.003, val_acc:0.982]
Epoch [94/120    avg_loss:0.003, val_acc:0.982]
Epoch [95/120    avg_loss:0.005, val_acc:0.982]
Epoch [96/120    avg_loss:0.002, val_acc:0.983]
Epoch [97/120    avg_loss:0.006, val_acc:0.982]
Epoch [98/120    avg_loss:0.004, val_acc:0.982]
Epoch [99/120    avg_loss:0.006, val_acc:0.983]
Epoch [100/120    avg_loss:0.003, val_acc:0.983]
Epoch [101/120    avg_loss:0.004, val_acc:0.982]
Epoch [102/120    avg_loss:0.004, val_acc:0.984]
Epoch [103/120    avg_loss:0.004, val_acc:0.984]
Epoch [104/120    avg_loss:0.003, val_acc:0.983]
Epoch [105/120    avg_loss:0.002, val_acc:0.982]
Epoch [106/120    avg_loss:0.003, val_acc:0.983]
Epoch [107/120    avg_loss:0.002, val_acc:0.983]
Epoch [108/120    avg_loss:0.003, val_acc:0.983]
Epoch [109/120    avg_loss:0.002, val_acc:0.984]
Epoch [110/120    avg_loss:0.002, val_acc:0.983]
Epoch [111/120    avg_loss:0.003, val_acc:0.983]
Epoch [112/120    avg_loss:0.003, val_acc:0.983]
Epoch [113/120    avg_loss:0.003, val_acc:0.984]
Epoch [114/120    avg_loss:0.003, val_acc:0.984]
Epoch [115/120    avg_loss:0.004, val_acc:0.983]
Epoch [116/120    avg_loss:0.004, val_acc:0.983]
Epoch [117/120    avg_loss:0.003, val_acc:0.983]
Epoch [118/120    avg_loss:0.003, val_acc:0.983]
Epoch [119/120    avg_loss:0.003, val_acc:0.983]
Epoch [120/120    avg_loss:0.006, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1269    7    0    0    3    0    0    0    5    1    0    0
     0    0    0]
 [   0    0    0  729    0    1    4    0    0    3    0    0    8    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0  860    9    0    0
     0    3    0]
 [   0    0   12    1    0    1    1    0    0    0    3 2187    5    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    8    0  522    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1136    2    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    23  312    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.61246612466125

F1 scores:
[       nan 0.98765432 0.98793305 0.98115747 1.         0.992
 0.9850075  1.         1.         0.84210526 0.98173516 0.99251191
 0.97478992 0.99462366 0.98868581 0.93975904 0.98203593]

Kappa:
0.9841818442750527
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7eff118c8668>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.206, val_acc:0.480]
Epoch [2/120    avg_loss:1.666, val_acc:0.608]
Epoch [3/120    avg_loss:1.349, val_acc:0.685]
Epoch [4/120    avg_loss:1.010, val_acc:0.712]
Epoch [5/120    avg_loss:0.946, val_acc:0.725]
Epoch [6/120    avg_loss:0.871, val_acc:0.772]
Epoch [7/120    avg_loss:0.711, val_acc:0.816]
Epoch [8/120    avg_loss:0.649, val_acc:0.772]
Epoch [9/120    avg_loss:0.810, val_acc:0.832]
Epoch [10/120    avg_loss:0.453, val_acc:0.889]
Epoch [11/120    avg_loss:0.347, val_acc:0.842]
Epoch [12/120    avg_loss:0.325, val_acc:0.907]
Epoch [13/120    avg_loss:0.315, val_acc:0.798]
Epoch [14/120    avg_loss:0.291, val_acc:0.910]
Epoch [15/120    avg_loss:0.313, val_acc:0.889]
Epoch [16/120    avg_loss:0.223, val_acc:0.882]
Epoch [17/120    avg_loss:0.211, val_acc:0.921]
Epoch [18/120    avg_loss:0.131, val_acc:0.926]
Epoch [19/120    avg_loss:0.126, val_acc:0.916]
Epoch [20/120    avg_loss:0.109, val_acc:0.936]
Epoch [21/120    avg_loss:0.143, val_acc:0.841]
Epoch [22/120    avg_loss:0.224, val_acc:0.933]
Epoch [23/120    avg_loss:0.109, val_acc:0.951]
Epoch [24/120    avg_loss:0.169, val_acc:0.907]
Epoch [25/120    avg_loss:0.109, val_acc:0.948]
Epoch [26/120    avg_loss:0.073, val_acc:0.965]
Epoch [27/120    avg_loss:0.063, val_acc:0.952]
Epoch [28/120    avg_loss:0.060, val_acc:0.953]
Epoch [29/120    avg_loss:0.077, val_acc:0.942]
Epoch [30/120    avg_loss:0.071, val_acc:0.964]
Epoch [31/120    avg_loss:0.073, val_acc:0.958]
Epoch [32/120    avg_loss:0.059, val_acc:0.943]
Epoch [33/120    avg_loss:0.056, val_acc:0.953]
Epoch [34/120    avg_loss:0.056, val_acc:0.957]
Epoch [35/120    avg_loss:0.056, val_acc:0.964]
Epoch [36/120    avg_loss:0.046, val_acc:0.962]
Epoch [37/120    avg_loss:0.055, val_acc:0.966]
Epoch [38/120    avg_loss:0.039, val_acc:0.958]
Epoch [39/120    avg_loss:0.045, val_acc:0.959]
Epoch [40/120    avg_loss:0.033, val_acc:0.966]
Epoch [41/120    avg_loss:0.098, val_acc:0.953]
Epoch [42/120    avg_loss:0.029, val_acc:0.971]
Epoch [43/120    avg_loss:0.038, val_acc:0.969]
Epoch [44/120    avg_loss:0.021, val_acc:0.972]
Epoch [45/120    avg_loss:0.028, val_acc:0.964]
Epoch [46/120    avg_loss:0.021, val_acc:0.965]
Epoch [47/120    avg_loss:0.014, val_acc:0.975]
Epoch [48/120    avg_loss:0.019, val_acc:0.963]
Epoch [49/120    avg_loss:0.021, val_acc:0.969]
Epoch [50/120    avg_loss:0.022, val_acc:0.972]
Epoch [51/120    avg_loss:0.026, val_acc:0.954]
Epoch [52/120    avg_loss:0.037, val_acc:0.958]
Epoch [53/120    avg_loss:0.020, val_acc:0.972]
Epoch [54/120    avg_loss:0.019, val_acc:0.960]
Epoch [55/120    avg_loss:0.014, val_acc:0.980]
Epoch [56/120    avg_loss:0.026, val_acc:0.924]
Epoch [57/120    avg_loss:0.032, val_acc:0.965]
Epoch [58/120    avg_loss:0.016, val_acc:0.977]
Epoch [59/120    avg_loss:0.009, val_acc:0.981]
Epoch [60/120    avg_loss:0.007, val_acc:0.985]
Epoch [61/120    avg_loss:0.007, val_acc:0.982]
Epoch [62/120    avg_loss:0.022, val_acc:0.975]
Epoch [63/120    avg_loss:0.079, val_acc:0.963]
Epoch [64/120    avg_loss:0.022, val_acc:0.979]
Epoch [65/120    avg_loss:0.015, val_acc:0.983]
Epoch [66/120    avg_loss:0.016, val_acc:0.980]
Epoch [67/120    avg_loss:0.007, val_acc:0.984]
Epoch [68/120    avg_loss:0.010, val_acc:0.979]
Epoch [69/120    avg_loss:0.016, val_acc:0.975]
Epoch [70/120    avg_loss:0.011, val_acc:0.979]
Epoch [71/120    avg_loss:0.011, val_acc:0.979]
Epoch [72/120    avg_loss:0.007, val_acc:0.978]
Epoch [73/120    avg_loss:0.008, val_acc:0.977]
Epoch [74/120    avg_loss:0.007, val_acc:0.979]
Epoch [75/120    avg_loss:0.016, val_acc:0.978]
Epoch [76/120    avg_loss:0.010, val_acc:0.980]
Epoch [77/120    avg_loss:0.008, val_acc:0.980]
Epoch [78/120    avg_loss:0.005, val_acc:0.980]
Epoch [79/120    avg_loss:0.005, val_acc:0.980]
Epoch [80/120    avg_loss:0.007, val_acc:0.981]
Epoch [81/120    avg_loss:0.006, val_acc:0.982]
Epoch [82/120    avg_loss:0.005, val_acc:0.982]
Epoch [83/120    avg_loss:0.003, val_acc:0.981]
Epoch [84/120    avg_loss:0.008, val_acc:0.982]
Epoch [85/120    avg_loss:0.005, val_acc:0.983]
Epoch [86/120    avg_loss:0.006, val_acc:0.982]
Epoch [87/120    avg_loss:0.005, val_acc:0.982]
Epoch [88/120    avg_loss:0.007, val_acc:0.982]
Epoch [89/120    avg_loss:0.005, val_acc:0.982]
Epoch [90/120    avg_loss:0.005, val_acc:0.982]
Epoch [91/120    avg_loss:0.006, val_acc:0.982]
Epoch [92/120    avg_loss:0.005, val_acc:0.982]
Epoch [93/120    avg_loss:0.004, val_acc:0.982]
Epoch [94/120    avg_loss:0.005, val_acc:0.982]
Epoch [95/120    avg_loss:0.005, val_acc:0.982]
Epoch [96/120    avg_loss:0.005, val_acc:0.982]
Epoch [97/120    avg_loss:0.004, val_acc:0.982]
Epoch [98/120    avg_loss:0.005, val_acc:0.982]
Epoch [99/120    avg_loss:0.004, val_acc:0.982]
Epoch [100/120    avg_loss:0.006, val_acc:0.982]
Epoch [101/120    avg_loss:0.006, val_acc:0.982]
Epoch [102/120    avg_loss:0.003, val_acc:0.982]
Epoch [103/120    avg_loss:0.005, val_acc:0.982]
Epoch [104/120    avg_loss:0.004, val_acc:0.982]
Epoch [105/120    avg_loss:0.004, val_acc:0.982]
Epoch [106/120    avg_loss:0.003, val_acc:0.982]
Epoch [107/120    avg_loss:0.004, val_acc:0.982]
Epoch [108/120    avg_loss:0.006, val_acc:0.982]
Epoch [109/120    avg_loss:0.004, val_acc:0.982]
Epoch [110/120    avg_loss:0.007, val_acc:0.982]
Epoch [111/120    avg_loss:0.004, val_acc:0.982]
Epoch [112/120    avg_loss:0.004, val_acc:0.982]
Epoch [113/120    avg_loss:0.005, val_acc:0.982]
Epoch [114/120    avg_loss:0.006, val_acc:0.982]
Epoch [115/120    avg_loss:0.005, val_acc:0.982]
Epoch [116/120    avg_loss:0.005, val_acc:0.982]
Epoch [117/120    avg_loss:0.006, val_acc:0.982]
Epoch [118/120    avg_loss:0.005, val_acc:0.982]
Epoch [119/120    avg_loss:0.005, val_acc:0.982]
Epoch [120/120    avg_loss:0.006, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1269    4    0    0    0    0    0    0    6    3    3    0
     0    0    0]
 [   0    0    0  738    0    0    0    0    0    5    1    0    1    2
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  425    0    0    0    5    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    2    0    0    0    0  859    7    0    0
     0    2    0]
 [   0    0    1    0    0    1    1    0    0    0    1 2204    1    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0  532    0
     0    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    4    0    1    0    1    0    0    0
  1123   10    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    28  303    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
98.5691056910569

F1 scores:
[       nan 0.98765432 0.99140625 0.98795181 0.99528302 0.99196326
 0.98274569 1.         0.99299065 0.73170732 0.98509174 0.99593312
 0.98336414 0.99191375 0.98078603 0.91540785 0.94545455]

Kappa:
0.9836830181664808
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:60
Validation dataloader:60
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4791e546d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:1.974, val_acc:0.453]
Epoch [2/120    avg_loss:1.625, val_acc:0.606]
Epoch [3/120    avg_loss:1.380, val_acc:0.765]
Epoch [4/120    avg_loss:0.968, val_acc:0.755]
Epoch [5/120    avg_loss:0.831, val_acc:0.778]
Epoch [6/120    avg_loss:0.755, val_acc:0.806]
Epoch [7/120    avg_loss:0.643, val_acc:0.815]
Epoch [8/120    avg_loss:0.457, val_acc:0.844]
Epoch [9/120    avg_loss:0.624, val_acc:0.864]
Epoch [10/120    avg_loss:0.389, val_acc:0.887]
Epoch [11/120    avg_loss:0.284, val_acc:0.881]
Epoch [12/120    avg_loss:0.251, val_acc:0.863]
Epoch [13/120    avg_loss:0.197, val_acc:0.904]
Epoch [14/120    avg_loss:0.167, val_acc:0.901]
Epoch [15/120    avg_loss:0.164, val_acc:0.912]
Epoch [16/120    avg_loss:0.156, val_acc:0.918]
Epoch [17/120    avg_loss:0.115, val_acc:0.926]
Epoch [18/120    avg_loss:0.158, val_acc:0.918]
Epoch [19/120    avg_loss:0.081, val_acc:0.928]
Epoch [20/120    avg_loss:0.097, val_acc:0.939]
Epoch [21/120    avg_loss:0.109, val_acc:0.951]
Epoch [22/120    avg_loss:0.114, val_acc:0.953]
Epoch [23/120    avg_loss:0.067, val_acc:0.957]
Epoch [24/120    avg_loss:0.055, val_acc:0.965]
Epoch [25/120    avg_loss:0.067, val_acc:0.959]
Epoch [26/120    avg_loss:0.058, val_acc:0.941]
Epoch [27/120    avg_loss:0.064, val_acc:0.946]
Epoch [28/120    avg_loss:0.062, val_acc:0.945]
Epoch [29/120    avg_loss:0.055, val_acc:0.963]
Epoch [30/120    avg_loss:0.047, val_acc:0.965]
Epoch [31/120    avg_loss:0.112, val_acc:0.959]
Epoch [32/120    avg_loss:0.086, val_acc:0.940]
Epoch [33/120    avg_loss:0.061, val_acc:0.962]
Epoch [34/120    avg_loss:0.044, val_acc:0.943]
Epoch [35/120    avg_loss:0.040, val_acc:0.966]
Epoch [36/120    avg_loss:0.036, val_acc:0.963]
Epoch [37/120    avg_loss:0.040, val_acc:0.946]
Epoch [38/120    avg_loss:0.030, val_acc:0.968]
Epoch [39/120    avg_loss:0.030, val_acc:0.952]
Epoch [40/120    avg_loss:0.041, val_acc:0.961]
Epoch [41/120    avg_loss:0.027, val_acc:0.974]
Epoch [42/120    avg_loss:0.026, val_acc:0.957]
Epoch [43/120    avg_loss:0.052, val_acc:0.938]
Epoch [44/120    avg_loss:0.055, val_acc:0.952]
Epoch [45/120    avg_loss:0.060, val_acc:0.919]
Epoch [46/120    avg_loss:0.105, val_acc:0.940]
Epoch [47/120    avg_loss:0.046, val_acc:0.943]
Epoch [48/120    avg_loss:0.071, val_acc:0.953]
Epoch [49/120    avg_loss:0.017, val_acc:0.975]
Epoch [50/120    avg_loss:0.030, val_acc:0.969]
Epoch [51/120    avg_loss:0.019, val_acc:0.972]
Epoch [52/120    avg_loss:0.017, val_acc:0.975]
Epoch [53/120    avg_loss:0.149, val_acc:0.936]
Epoch [54/120    avg_loss:0.090, val_acc:0.941]
Epoch [55/120    avg_loss:0.087, val_acc:0.942]
Epoch [56/120    avg_loss:0.077, val_acc:0.941]
Epoch [57/120    avg_loss:0.061, val_acc:0.972]
Epoch [58/120    avg_loss:0.031, val_acc:0.971]
Epoch [59/120    avg_loss:0.011, val_acc:0.976]
Epoch [60/120    avg_loss:0.009, val_acc:0.974]
Epoch [61/120    avg_loss:0.015, val_acc:0.961]
Epoch [62/120    avg_loss:0.021, val_acc:0.977]
Epoch [63/120    avg_loss:0.012, val_acc:0.977]
Epoch [64/120    avg_loss:0.010, val_acc:0.979]
Epoch [65/120    avg_loss:0.012, val_acc:0.981]
Epoch [66/120    avg_loss:0.009, val_acc:0.977]
Epoch [67/120    avg_loss:0.012, val_acc:0.974]
Epoch [68/120    avg_loss:0.010, val_acc:0.983]
Epoch [69/120    avg_loss:0.004, val_acc:0.985]
Epoch [70/120    avg_loss:0.007, val_acc:0.979]
Epoch [71/120    avg_loss:0.018, val_acc:0.977]
Epoch [72/120    avg_loss:0.014, val_acc:0.978]
Epoch [73/120    avg_loss:0.009, val_acc:0.979]
Epoch [74/120    avg_loss:0.011, val_acc:0.978]
Epoch [75/120    avg_loss:0.008, val_acc:0.975]
Epoch [76/120    avg_loss:0.009, val_acc:0.978]
Epoch [77/120    avg_loss:0.006, val_acc:0.975]
Epoch [78/120    avg_loss:0.006, val_acc:0.975]
Epoch [79/120    avg_loss:0.003, val_acc:0.981]
Epoch [80/120    avg_loss:0.004, val_acc:0.979]
Epoch [81/120    avg_loss:0.003, val_acc:0.975]
Epoch [82/120    avg_loss:0.006, val_acc:0.979]
Epoch [83/120    avg_loss:0.007, val_acc:0.980]
Epoch [84/120    avg_loss:0.003, val_acc:0.979]
Epoch [85/120    avg_loss:0.004, val_acc:0.981]
Epoch [86/120    avg_loss:0.004, val_acc:0.981]
Epoch [87/120    avg_loss:0.003, val_acc:0.981]
Epoch [88/120    avg_loss:0.004, val_acc:0.982]
Epoch [89/120    avg_loss:0.004, val_acc:0.982]
Epoch [90/120    avg_loss:0.003, val_acc:0.982]
Epoch [91/120    avg_loss:0.003, val_acc:0.983]
Epoch [92/120    avg_loss:0.003, val_acc:0.982]
Epoch [93/120    avg_loss:0.004, val_acc:0.982]
Epoch [94/120    avg_loss:0.002, val_acc:0.982]
Epoch [95/120    avg_loss:0.003, val_acc:0.982]
Epoch [96/120    avg_loss:0.003, val_acc:0.982]
Epoch [97/120    avg_loss:0.003, val_acc:0.982]
Epoch [98/120    avg_loss:0.003, val_acc:0.982]
Epoch [99/120    avg_loss:0.003, val_acc:0.982]
Epoch [100/120    avg_loss:0.002, val_acc:0.982]
Epoch [101/120    avg_loss:0.004, val_acc:0.982]
Epoch [102/120    avg_loss:0.002, val_acc:0.982]
Epoch [103/120    avg_loss:0.002, val_acc:0.982]
Epoch [104/120    avg_loss:0.003, val_acc:0.982]
Epoch [105/120    avg_loss:0.003, val_acc:0.982]
Epoch [106/120    avg_loss:0.002, val_acc:0.982]
Epoch [107/120    avg_loss:0.002, val_acc:0.982]
Epoch [108/120    avg_loss:0.003, val_acc:0.982]
Epoch [109/120    avg_loss:0.003, val_acc:0.982]
Epoch [110/120    avg_loss:0.002, val_acc:0.982]
Epoch [111/120    avg_loss:0.003, val_acc:0.982]
Epoch [112/120    avg_loss:0.004, val_acc:0.982]
Epoch [113/120    avg_loss:0.002, val_acc:0.982]
Epoch [114/120    avg_loss:0.004, val_acc:0.982]
Epoch [115/120    avg_loss:0.003, val_acc:0.982]
Epoch [116/120    avg_loss:0.003, val_acc:0.982]
Epoch [117/120    avg_loss:0.003, val_acc:0.982]
Epoch [118/120    avg_loss:0.002, val_acc:0.982]
Epoch [119/120    avg_loss:0.002, val_acc:0.982]
Epoch [120/120    avg_loss:0.003, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1275    4    0    0    0    0    0    0    3    2    0    0
     0    1    0]
 [   0    0    0  733    0    0    0    0    0    4    0    0    9    1
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  872    2    0    0
     1    0    0]
 [   0    0   14    0    0    0    0    0    0    0    0 2195    0    0
     1    0    0]
 [   0    0    0    0    0    0    0    0    0    0    5    0  526    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    0    0    0    0
  1128    6    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    31  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
98.79674796747967

F1 scores:
[       nan 0.98765432 0.99067599 0.98455339 0.99528302 0.99198167
 0.99545455 0.98039216 1.         0.78947368 0.99316629 0.99569063
 0.97951583 0.99730458 0.98086957 0.93373494 0.95180723]

Kappa:
0.9862809052428063
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff2a04596d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.206, val_acc:0.510]
Epoch [2/120    avg_loss:1.582, val_acc:0.644]
Epoch [3/120    avg_loss:1.283, val_acc:0.663]
Epoch [4/120    avg_loss:1.119, val_acc:0.707]
Epoch [5/120    avg_loss:0.824, val_acc:0.735]
Epoch [6/120    avg_loss:0.758, val_acc:0.693]
Epoch [7/120    avg_loss:0.884, val_acc:0.748]
Epoch [8/120    avg_loss:0.655, val_acc:0.810]
Epoch [9/120    avg_loss:0.447, val_acc:0.834]
Epoch [10/120    avg_loss:0.534, val_acc:0.860]
Epoch [11/120    avg_loss:0.368, val_acc:0.886]
Epoch [12/120    avg_loss:0.255, val_acc:0.913]
Epoch [13/120    avg_loss:0.264, val_acc:0.876]
Epoch [14/120    avg_loss:0.422, val_acc:0.835]
Epoch [15/120    avg_loss:0.320, val_acc:0.884]
Epoch [16/120    avg_loss:0.232, val_acc:0.898]
Epoch [17/120    avg_loss:0.246, val_acc:0.905]
Epoch [18/120    avg_loss:0.172, val_acc:0.909]
Epoch [19/120    avg_loss:0.138, val_acc:0.903]
Epoch [20/120    avg_loss:0.190, val_acc:0.926]
Epoch [21/120    avg_loss:0.131, val_acc:0.891]
Epoch [22/120    avg_loss:0.144, val_acc:0.940]
Epoch [23/120    avg_loss:0.140, val_acc:0.933]
Epoch [24/120    avg_loss:0.081, val_acc:0.932]
Epoch [25/120    avg_loss:0.092, val_acc:0.925]
Epoch [26/120    avg_loss:0.103, val_acc:0.947]
Epoch [27/120    avg_loss:0.083, val_acc:0.949]
Epoch [28/120    avg_loss:0.085, val_acc:0.938]
Epoch [29/120    avg_loss:0.095, val_acc:0.941]
Epoch [30/120    avg_loss:0.101, val_acc:0.918]
Epoch [31/120    avg_loss:0.106, val_acc:0.935]
Epoch [32/120    avg_loss:0.103, val_acc:0.960]
Epoch [33/120    avg_loss:0.071, val_acc:0.946]
Epoch [34/120    avg_loss:0.061, val_acc:0.963]
Epoch [35/120    avg_loss:0.070, val_acc:0.962]
Epoch [36/120    avg_loss:0.042, val_acc:0.956]
Epoch [37/120    avg_loss:0.039, val_acc:0.956]
Epoch [38/120    avg_loss:0.044, val_acc:0.962]
Epoch [39/120    avg_loss:0.052, val_acc:0.969]
Epoch [40/120    avg_loss:0.037, val_acc:0.962]
Epoch [41/120    avg_loss:0.042, val_acc:0.948]
Epoch [42/120    avg_loss:0.057, val_acc:0.969]
Epoch [43/120    avg_loss:0.054, val_acc:0.963]
Epoch [44/120    avg_loss:0.066, val_acc:0.971]
Epoch [45/120    avg_loss:0.041, val_acc:0.967]
Epoch [46/120    avg_loss:0.022, val_acc:0.967]
Epoch [47/120    avg_loss:0.049, val_acc:0.960]
Epoch [48/120    avg_loss:0.051, val_acc:0.972]
Epoch [49/120    avg_loss:0.034, val_acc:0.976]
Epoch [50/120    avg_loss:0.023, val_acc:0.975]
Epoch [51/120    avg_loss:0.040, val_acc:0.977]
Epoch [52/120    avg_loss:0.014, val_acc:0.978]
Epoch [53/120    avg_loss:0.019, val_acc:0.982]
Epoch [54/120    avg_loss:0.026, val_acc:0.984]
Epoch [55/120    avg_loss:0.025, val_acc:0.971]
Epoch [56/120    avg_loss:0.025, val_acc:0.981]
Epoch [57/120    avg_loss:0.018, val_acc:0.972]
Epoch [58/120    avg_loss:0.033, val_acc:0.963]
Epoch [59/120    avg_loss:0.023, val_acc:0.982]
Epoch [60/120    avg_loss:0.020, val_acc:0.978]
Epoch [61/120    avg_loss:0.092, val_acc:0.948]
Epoch [62/120    avg_loss:0.047, val_acc:0.981]
Epoch [63/120    avg_loss:0.038, val_acc:0.971]
Epoch [64/120    avg_loss:0.017, val_acc:0.968]
Epoch [65/120    avg_loss:0.026, val_acc:0.984]
Epoch [66/120    avg_loss:0.015, val_acc:0.980]
Epoch [67/120    avg_loss:0.026, val_acc:0.975]
Epoch [68/120    avg_loss:0.019, val_acc:0.975]
Epoch [69/120    avg_loss:0.017, val_acc:0.976]
Epoch [70/120    avg_loss:0.019, val_acc:0.977]
Epoch [71/120    avg_loss:0.011, val_acc:0.980]
Epoch [72/120    avg_loss:0.011, val_acc:0.983]
Epoch [73/120    avg_loss:0.008, val_acc:0.986]
Epoch [74/120    avg_loss:0.007, val_acc:0.984]
Epoch [75/120    avg_loss:0.007, val_acc:0.985]
Epoch [76/120    avg_loss:0.010, val_acc:0.983]
Epoch [77/120    avg_loss:0.010, val_acc:0.984]
Epoch [78/120    avg_loss:0.008, val_acc:0.986]
Epoch [79/120    avg_loss:0.016, val_acc:0.977]
Epoch [80/120    avg_loss:0.011, val_acc:0.985]
Epoch [81/120    avg_loss:0.008, val_acc:0.985]
Epoch [82/120    avg_loss:0.015, val_acc:0.984]
Epoch [83/120    avg_loss:0.019, val_acc:0.981]
Epoch [84/120    avg_loss:0.009, val_acc:0.986]
Epoch [85/120    avg_loss:0.014, val_acc:0.987]
Epoch [86/120    avg_loss:0.006, val_acc:0.987]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.008, val_acc:0.987]
Epoch [89/120    avg_loss:0.005, val_acc:0.980]
Epoch [90/120    avg_loss:0.005, val_acc:0.984]
Epoch [91/120    avg_loss:0.004, val_acc:0.987]
Epoch [92/120    avg_loss:0.008, val_acc:0.982]
Epoch [93/120    avg_loss:0.016, val_acc:0.988]
Epoch [94/120    avg_loss:0.009, val_acc:0.987]
Epoch [95/120    avg_loss:0.004, val_acc:0.987]
Epoch [96/120    avg_loss:0.005, val_acc:0.984]
Epoch [97/120    avg_loss:0.007, val_acc:0.987]
Epoch [98/120    avg_loss:0.007, val_acc:0.984]
Epoch [99/120    avg_loss:0.013, val_acc:0.984]
Epoch [100/120    avg_loss:0.015, val_acc:0.973]
Epoch [101/120    avg_loss:0.017, val_acc:0.978]
Epoch [102/120    avg_loss:0.008, val_acc:0.986]
Epoch [103/120    avg_loss:0.011, val_acc:0.978]
Epoch [104/120    avg_loss:0.034, val_acc:0.978]
Epoch [105/120    avg_loss:0.008, val_acc:0.991]
Epoch [106/120    avg_loss:0.006, val_acc:0.985]
Epoch [107/120    avg_loss:0.009, val_acc:0.978]
Epoch [108/120    avg_loss:0.008, val_acc:0.987]
Epoch [109/120    avg_loss:0.006, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.986]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.048, val_acc:0.958]
Epoch [114/120    avg_loss:0.022, val_acc:0.984]
Epoch [115/120    avg_loss:0.011, val_acc:0.982]
Epoch [116/120    avg_loss:0.009, val_acc:0.982]
Epoch [117/120    avg_loss:0.009, val_acc:0.980]
Epoch [118/120    avg_loss:0.010, val_acc:0.984]
Epoch [119/120    avg_loss:0.014, val_acc:0.984]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1271    0    0    0    0    0    0    0    6    6    2    0
     0    0    0]
 [   0    0    0  686    8   19    0    0    0    6    0    0   25    2
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    3    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  424    0    0    0    6    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0   13    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0  870    1    0    0
     0    2    0]
 [   0    0    2    0    0    3    3    0    0    0    3 2199    0    0
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0  530    0
     0    0    2]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0  182
     0    0    0]
 [   0    0    0    0    0    8    0    0    0    0    0    0    0    0
  1129    2    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    10  325    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1    1    0
     0    0   82]]

Accuracy:
98.35230352303523

F1 scores:
[       nan 0.975      0.99335678 0.95343989 0.97921478 0.95227525
 0.98871332 0.94339623 0.99297424 0.65       0.99145299 0.99569844
 0.96539162 0.98644986 0.99078543 0.96153846 0.97619048]

Kappa:
0.9812209105415579
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe9f49646d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.138, val_acc:0.626]
Epoch [2/120    avg_loss:1.693, val_acc:0.628]
Epoch [3/120    avg_loss:1.277, val_acc:0.713]
Epoch [4/120    avg_loss:1.215, val_acc:0.706]
Epoch [5/120    avg_loss:0.996, val_acc:0.744]
Epoch [6/120    avg_loss:0.884, val_acc:0.779]
Epoch [7/120    avg_loss:0.791, val_acc:0.782]
Epoch [8/120    avg_loss:0.642, val_acc:0.810]
Epoch [9/120    avg_loss:0.542, val_acc:0.861]
Epoch [10/120    avg_loss:0.353, val_acc:0.858]
Epoch [11/120    avg_loss:0.360, val_acc:0.869]
Epoch [12/120    avg_loss:0.453, val_acc:0.881]
Epoch [13/120    avg_loss:0.408, val_acc:0.717]
Epoch [14/120    avg_loss:0.480, val_acc:0.797]
Epoch [15/120    avg_loss:0.294, val_acc:0.907]
Epoch [16/120    avg_loss:0.218, val_acc:0.883]
Epoch [17/120    avg_loss:0.211, val_acc:0.916]
Epoch [18/120    avg_loss:0.237, val_acc:0.935]
Epoch [19/120    avg_loss:0.191, val_acc:0.925]
Epoch [20/120    avg_loss:0.215, val_acc:0.866]
Epoch [21/120    avg_loss:0.280, val_acc:0.913]
Epoch [22/120    avg_loss:0.172, val_acc:0.905]
Epoch [23/120    avg_loss:0.111, val_acc:0.953]
Epoch [24/120    avg_loss:0.115, val_acc:0.946]
Epoch [25/120    avg_loss:0.069, val_acc:0.936]
Epoch [26/120    avg_loss:0.060, val_acc:0.962]
Epoch [27/120    avg_loss:0.108, val_acc:0.917]
Epoch [28/120    avg_loss:0.108, val_acc:0.934]
Epoch [29/120    avg_loss:0.142, val_acc:0.928]
Epoch [30/120    avg_loss:0.079, val_acc:0.944]
Epoch [31/120    avg_loss:0.060, val_acc:0.956]
Epoch [32/120    avg_loss:0.051, val_acc:0.962]
Epoch [33/120    avg_loss:0.041, val_acc:0.971]
Epoch [34/120    avg_loss:0.047, val_acc:0.963]
Epoch [35/120    avg_loss:0.067, val_acc:0.961]
Epoch [36/120    avg_loss:0.042, val_acc:0.950]
Epoch [37/120    avg_loss:0.035, val_acc:0.964]
Epoch [38/120    avg_loss:0.047, val_acc:0.961]
Epoch [39/120    avg_loss:0.047, val_acc:0.967]
Epoch [40/120    avg_loss:0.031, val_acc:0.964]
Epoch [41/120    avg_loss:0.047, val_acc:0.964]
Epoch [42/120    avg_loss:0.033, val_acc:0.962]
Epoch [43/120    avg_loss:0.039, val_acc:0.969]
Epoch [44/120    avg_loss:0.037, val_acc:0.973]
Epoch [45/120    avg_loss:0.046, val_acc:0.966]
Epoch [46/120    avg_loss:0.036, val_acc:0.970]
Epoch [47/120    avg_loss:0.022, val_acc:0.971]
Epoch [48/120    avg_loss:0.022, val_acc:0.977]
Epoch [49/120    avg_loss:0.034, val_acc:0.964]
Epoch [50/120    avg_loss:0.031, val_acc:0.978]
Epoch [51/120    avg_loss:0.024, val_acc:0.975]
Epoch [52/120    avg_loss:0.018, val_acc:0.974]
Epoch [53/120    avg_loss:0.012, val_acc:0.980]
Epoch [54/120    avg_loss:0.021, val_acc:0.962]
Epoch [55/120    avg_loss:0.018, val_acc:0.975]
Epoch [56/120    avg_loss:0.017, val_acc:0.966]
Epoch [57/120    avg_loss:0.015, val_acc:0.975]
Epoch [58/120    avg_loss:0.060, val_acc:0.957]
Epoch [59/120    avg_loss:0.039, val_acc:0.964]
Epoch [60/120    avg_loss:0.027, val_acc:0.967]
Epoch [61/120    avg_loss:0.023, val_acc:0.971]
Epoch [62/120    avg_loss:0.012, val_acc:0.977]
Epoch [63/120    avg_loss:0.010, val_acc:0.976]
Epoch [64/120    avg_loss:0.017, val_acc:0.972]
Epoch [65/120    avg_loss:0.036, val_acc:0.958]
Epoch [66/120    avg_loss:0.046, val_acc:0.964]
Epoch [67/120    avg_loss:0.022, val_acc:0.974]
Epoch [68/120    avg_loss:0.018, val_acc:0.976]
Epoch [69/120    avg_loss:0.017, val_acc:0.976]
Epoch [70/120    avg_loss:0.013, val_acc:0.976]
Epoch [71/120    avg_loss:0.009, val_acc:0.976]
Epoch [72/120    avg_loss:0.015, val_acc:0.976]
Epoch [73/120    avg_loss:0.009, val_acc:0.977]
Epoch [74/120    avg_loss:0.011, val_acc:0.978]
Epoch [75/120    avg_loss:0.009, val_acc:0.978]
Epoch [76/120    avg_loss:0.014, val_acc:0.977]
Epoch [77/120    avg_loss:0.011, val_acc:0.977]
Epoch [78/120    avg_loss:0.013, val_acc:0.980]
Epoch [79/120    avg_loss:0.010, val_acc:0.978]
Epoch [80/120    avg_loss:0.010, val_acc:0.976]
Epoch [81/120    avg_loss:0.010, val_acc:0.980]
Epoch [82/120    avg_loss:0.008, val_acc:0.978]
Epoch [83/120    avg_loss:0.014, val_acc:0.978]
Epoch [84/120    avg_loss:0.007, val_acc:0.977]
Epoch [85/120    avg_loss:0.008, val_acc:0.977]
Epoch [86/120    avg_loss:0.006, val_acc:0.976]
Epoch [87/120    avg_loss:0.007, val_acc:0.977]
Epoch [88/120    avg_loss:0.008, val_acc:0.980]
Epoch [89/120    avg_loss:0.007, val_acc:0.981]
Epoch [90/120    avg_loss:0.008, val_acc:0.981]
Epoch [91/120    avg_loss:0.007, val_acc:0.981]
Epoch [92/120    avg_loss:0.019, val_acc:0.980]
Epoch [93/120    avg_loss:0.007, val_acc:0.980]
Epoch [94/120    avg_loss:0.009, val_acc:0.980]
Epoch [95/120    avg_loss:0.006, val_acc:0.977]
Epoch [96/120    avg_loss:0.006, val_acc:0.978]
Epoch [97/120    avg_loss:0.011, val_acc:0.980]
Epoch [98/120    avg_loss:0.006, val_acc:0.977]
Epoch [99/120    avg_loss:0.006, val_acc:0.977]
Epoch [100/120    avg_loss:0.009, val_acc:0.977]
Epoch [101/120    avg_loss:0.006, val_acc:0.977]
Epoch [102/120    avg_loss:0.005, val_acc:0.977]
Epoch [103/120    avg_loss:0.005, val_acc:0.977]
Epoch [104/120    avg_loss:0.004, val_acc:0.977]
Epoch [105/120    avg_loss:0.007, val_acc:0.977]
Epoch [106/120    avg_loss:0.009, val_acc:0.977]
Epoch [107/120    avg_loss:0.006, val_acc:0.977]
Epoch [108/120    avg_loss:0.012, val_acc:0.977]
Epoch [109/120    avg_loss:0.004, val_acc:0.977]
Epoch [110/120    avg_loss:0.008, val_acc:0.977]
Epoch [111/120    avg_loss:0.004, val_acc:0.977]
Epoch [112/120    avg_loss:0.007, val_acc:0.977]
Epoch [113/120    avg_loss:0.008, val_acc:0.977]
Epoch [114/120    avg_loss:0.013, val_acc:0.977]
Epoch [115/120    avg_loss:0.008, val_acc:0.977]
Epoch [116/120    avg_loss:0.007, val_acc:0.977]
Epoch [117/120    avg_loss:0.008, val_acc:0.977]
Epoch [118/120    avg_loss:0.008, val_acc:0.977]
Epoch [119/120    avg_loss:0.005, val_acc:0.977]
Epoch [120/120    avg_loss:0.011, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1280    0    0    0    0    0    0    0    4    1    0    0
     0    0    0]
 [   0    0    0  702    0   10    0    0    0    5    1    0   25    4
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0  859    1    4    0
     0    7    0]
 [   0    0   11    0    0    4    5    0    0    0    4 2185    0    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  532    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    1    0    1    0    0    0
  1127    6    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    22  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.42818428184282

F1 scores:
[       nan 0.98765432 0.99340318 0.96760855 0.99764706 0.97187852
 0.98796992 1.         0.99883856 0.79069767 0.98509174 0.99385945
 0.97080292 0.98666667 0.98513986 0.93175074 0.98224852]

Kappa:
0.9820894034316201
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1ca043f6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.223, val_acc:0.360]
Epoch [2/120    avg_loss:1.419, val_acc:0.589]
Epoch [3/120    avg_loss:1.250, val_acc:0.740]
Epoch [4/120    avg_loss:1.139, val_acc:0.739]
Epoch [5/120    avg_loss:0.958, val_acc:0.816]
Epoch [6/120    avg_loss:0.832, val_acc:0.699]
Epoch [7/120    avg_loss:0.633, val_acc:0.775]
Epoch [8/120    avg_loss:0.759, val_acc:0.784]
Epoch [9/120    avg_loss:0.483, val_acc:0.865]
Epoch [10/120    avg_loss:0.326, val_acc:0.803]
Epoch [11/120    avg_loss:0.399, val_acc:0.848]
Epoch [12/120    avg_loss:0.313, val_acc:0.883]
Epoch [13/120    avg_loss:0.228, val_acc:0.905]
Epoch [14/120    avg_loss:0.207, val_acc:0.887]
Epoch [15/120    avg_loss:0.231, val_acc:0.874]
Epoch [16/120    avg_loss:0.192, val_acc:0.915]
Epoch [17/120    avg_loss:0.208, val_acc:0.891]
Epoch [18/120    avg_loss:0.227, val_acc:0.908]
Epoch [19/120    avg_loss:0.122, val_acc:0.945]
Epoch [20/120    avg_loss:0.134, val_acc:0.934]
Epoch [21/120    avg_loss:0.100, val_acc:0.911]
Epoch [22/120    avg_loss:0.136, val_acc:0.925]
Epoch [23/120    avg_loss:0.139, val_acc:0.936]
Epoch [24/120    avg_loss:0.122, val_acc:0.922]
Epoch [25/120    avg_loss:0.127, val_acc:0.939]
Epoch [26/120    avg_loss:0.082, val_acc:0.950]
Epoch [27/120    avg_loss:0.069, val_acc:0.949]
Epoch [28/120    avg_loss:0.072, val_acc:0.945]
Epoch [29/120    avg_loss:0.077, val_acc:0.954]
Epoch [30/120    avg_loss:0.093, val_acc:0.948]
Epoch [31/120    avg_loss:0.071, val_acc:0.952]
Epoch [32/120    avg_loss:0.055, val_acc:0.961]
Epoch [33/120    avg_loss:0.043, val_acc:0.968]
Epoch [34/120    avg_loss:0.053, val_acc:0.958]
Epoch [35/120    avg_loss:0.071, val_acc:0.959]
Epoch [36/120    avg_loss:0.067, val_acc:0.946]
Epoch [37/120    avg_loss:0.091, val_acc:0.963]
Epoch [38/120    avg_loss:0.146, val_acc:0.914]
Epoch [39/120    avg_loss:0.067, val_acc:0.946]
Epoch [40/120    avg_loss:0.068, val_acc:0.960]
Epoch [41/120    avg_loss:0.043, val_acc:0.947]
Epoch [42/120    avg_loss:0.091, val_acc:0.949]
Epoch [43/120    avg_loss:0.048, val_acc:0.957]
Epoch [44/120    avg_loss:0.049, val_acc:0.968]
Epoch [45/120    avg_loss:0.046, val_acc:0.964]
Epoch [46/120    avg_loss:0.027, val_acc:0.905]
Epoch [47/120    avg_loss:0.041, val_acc:0.967]
Epoch [48/120    avg_loss:0.125, val_acc:0.963]
Epoch [49/120    avg_loss:0.055, val_acc:0.963]
Epoch [50/120    avg_loss:0.028, val_acc:0.969]
Epoch [51/120    avg_loss:0.032, val_acc:0.964]
Epoch [52/120    avg_loss:0.021, val_acc:0.972]
Epoch [53/120    avg_loss:0.019, val_acc:0.975]
Epoch [54/120    avg_loss:0.013, val_acc:0.974]
Epoch [55/120    avg_loss:0.019, val_acc:0.968]
Epoch [56/120    avg_loss:0.024, val_acc:0.970]
Epoch [57/120    avg_loss:0.021, val_acc:0.973]
Epoch [58/120    avg_loss:0.016, val_acc:0.980]
Epoch [59/120    avg_loss:0.010, val_acc:0.971]
Epoch [60/120    avg_loss:0.020, val_acc:0.976]
Epoch [61/120    avg_loss:0.027, val_acc:0.963]
Epoch [62/120    avg_loss:0.019, val_acc:0.972]
Epoch [63/120    avg_loss:0.041, val_acc:0.966]
Epoch [64/120    avg_loss:0.024, val_acc:0.967]
Epoch [65/120    avg_loss:0.015, val_acc:0.980]
Epoch [66/120    avg_loss:0.025, val_acc:0.971]
Epoch [67/120    avg_loss:0.011, val_acc:0.978]
Epoch [68/120    avg_loss:0.012, val_acc:0.980]
Epoch [69/120    avg_loss:0.010, val_acc:0.975]
Epoch [70/120    avg_loss:0.015, val_acc:0.982]
Epoch [71/120    avg_loss:0.013, val_acc:0.967]
Epoch [72/120    avg_loss:0.012, val_acc:0.983]
Epoch [73/120    avg_loss:0.008, val_acc:0.980]
Epoch [74/120    avg_loss:0.007, val_acc:0.988]
Epoch [75/120    avg_loss:0.007, val_acc:0.980]
Epoch [76/120    avg_loss:0.007, val_acc:0.980]
Epoch [77/120    avg_loss:0.004, val_acc:0.985]
Epoch [78/120    avg_loss:0.006, val_acc:0.981]
Epoch [79/120    avg_loss:0.004, val_acc:0.981]
Epoch [80/120    avg_loss:0.007, val_acc:0.980]
Epoch [81/120    avg_loss:0.012, val_acc:0.974]
Epoch [82/120    avg_loss:0.008, val_acc:0.976]
Epoch [83/120    avg_loss:0.021, val_acc:0.983]
Epoch [84/120    avg_loss:0.017, val_acc:0.980]
Epoch [85/120    avg_loss:0.008, val_acc:0.980]
Epoch [86/120    avg_loss:0.008, val_acc:0.977]
Epoch [87/120    avg_loss:0.006, val_acc:0.983]
Epoch [88/120    avg_loss:0.004, val_acc:0.983]
Epoch [89/120    avg_loss:0.006, val_acc:0.982]
Epoch [90/120    avg_loss:0.003, val_acc:0.984]
Epoch [91/120    avg_loss:0.006, val_acc:0.984]
Epoch [92/120    avg_loss:0.005, val_acc:0.984]
Epoch [93/120    avg_loss:0.004, val_acc:0.984]
Epoch [94/120    avg_loss:0.005, val_acc:0.984]
Epoch [95/120    avg_loss:0.004, val_acc:0.983]
Epoch [96/120    avg_loss:0.004, val_acc:0.984]
Epoch [97/120    avg_loss:0.008, val_acc:0.980]
Epoch [98/120    avg_loss:0.003, val_acc:0.980]
Epoch [99/120    avg_loss:0.005, val_acc:0.981]
Epoch [100/120    avg_loss:0.003, val_acc:0.981]
Epoch [101/120    avg_loss:0.010, val_acc:0.981]
Epoch [102/120    avg_loss:0.003, val_acc:0.981]
Epoch [103/120    avg_loss:0.005, val_acc:0.981]
Epoch [104/120    avg_loss:0.004, val_acc:0.981]
Epoch [105/120    avg_loss:0.004, val_acc:0.981]
Epoch [106/120    avg_loss:0.004, val_acc:0.981]
Epoch [107/120    avg_loss:0.005, val_acc:0.981]
Epoch [108/120    avg_loss:0.008, val_acc:0.982]
Epoch [109/120    avg_loss:0.004, val_acc:0.983]
Epoch [110/120    avg_loss:0.004, val_acc:0.982]
Epoch [111/120    avg_loss:0.004, val_acc:0.983]
Epoch [112/120    avg_loss:0.004, val_acc:0.982]
Epoch [113/120    avg_loss:0.005, val_acc:0.981]
Epoch [114/120    avg_loss:0.007, val_acc:0.981]
Epoch [115/120    avg_loss:0.005, val_acc:0.981]
Epoch [116/120    avg_loss:0.005, val_acc:0.981]
Epoch [117/120    avg_loss:0.006, val_acc:0.981]
Epoch [118/120    avg_loss:0.005, val_acc:0.981]
Epoch [119/120    avg_loss:0.006, val_acc:0.981]
Epoch [120/120    avg_loss:0.004, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1275    2    0    0    0    0    0    0    2    2    0    0
     0    4    0]
 [   0    0    0  711    0   13    0    0    0    3    0    0   18    2
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    2    0    0    0    0  866    2    0    0
     0    4    0]
 [   0    0   12    0    0    1    5    0    0    0    0 2191    0    1
     0    0    0]
 [   0    0    0    0    1    0    0    0    0    0    0    0  527    0
     0    1    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    1    0    0    0
  1132    3    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
     9  331    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.78590785907859

F1 scores:
[       nan 1.         0.99106102 0.97264022 0.99530516 0.9751693
 0.99095023 1.         0.997669   0.82926829 0.99311927 0.99477866
 0.974122   0.9919571  0.99298246 0.95942029 0.96511628]

Kappa:
0.9861647563799997
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1663cce710>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.328, val_acc:0.329]
Epoch [2/120    avg_loss:1.756, val_acc:0.514]
Epoch [3/120    avg_loss:1.477, val_acc:0.586]
Epoch [4/120    avg_loss:1.356, val_acc:0.671]
Epoch [5/120    avg_loss:1.115, val_acc:0.545]
Epoch [6/120    avg_loss:1.115, val_acc:0.648]
Epoch [7/120    avg_loss:0.795, val_acc:0.725]
Epoch [8/120    avg_loss:0.701, val_acc:0.677]
Epoch [9/120    avg_loss:0.799, val_acc:0.741]
Epoch [10/120    avg_loss:0.540, val_acc:0.777]
Epoch [11/120    avg_loss:0.548, val_acc:0.814]
Epoch [12/120    avg_loss:0.743, val_acc:0.784]
Epoch [13/120    avg_loss:0.430, val_acc:0.831]
Epoch [14/120    avg_loss:0.456, val_acc:0.845]
Epoch [15/120    avg_loss:0.309, val_acc:0.888]
Epoch [16/120    avg_loss:0.224, val_acc:0.923]
Epoch [17/120    avg_loss:0.264, val_acc:0.918]
Epoch [18/120    avg_loss:0.214, val_acc:0.889]
Epoch [19/120    avg_loss:0.284, val_acc:0.938]
Epoch [20/120    avg_loss:0.161, val_acc:0.935]
Epoch [21/120    avg_loss:0.147, val_acc:0.916]
Epoch [22/120    avg_loss:0.161, val_acc:0.890]
Epoch [23/120    avg_loss:0.152, val_acc:0.927]
Epoch [24/120    avg_loss:0.119, val_acc:0.950]
Epoch [25/120    avg_loss:0.095, val_acc:0.923]
Epoch [26/120    avg_loss:0.114, val_acc:0.942]
Epoch [27/120    avg_loss:0.105, val_acc:0.955]
Epoch [28/120    avg_loss:0.102, val_acc:0.953]
Epoch [29/120    avg_loss:0.083, val_acc:0.955]
Epoch [30/120    avg_loss:0.126, val_acc:0.941]
Epoch [31/120    avg_loss:0.138, val_acc:0.955]
Epoch [32/120    avg_loss:0.062, val_acc:0.969]
Epoch [33/120    avg_loss:0.132, val_acc:0.952]
Epoch [34/120    avg_loss:0.063, val_acc:0.964]
Epoch [35/120    avg_loss:0.044, val_acc:0.960]
Epoch [36/120    avg_loss:0.043, val_acc:0.973]
Epoch [37/120    avg_loss:0.065, val_acc:0.960]
Epoch [38/120    avg_loss:0.089, val_acc:0.916]
Epoch [39/120    avg_loss:0.055, val_acc:0.959]
Epoch [40/120    avg_loss:0.036, val_acc:0.971]
Epoch [41/120    avg_loss:0.035, val_acc:0.972]
Epoch [42/120    avg_loss:0.039, val_acc:0.964]
Epoch [43/120    avg_loss:0.065, val_acc:0.938]
Epoch [44/120    avg_loss:0.036, val_acc:0.968]
Epoch [45/120    avg_loss:0.031, val_acc:0.964]
Epoch [46/120    avg_loss:0.033, val_acc:0.983]
Epoch [47/120    avg_loss:0.023, val_acc:0.985]
Epoch [48/120    avg_loss:0.029, val_acc:0.974]
Epoch [49/120    avg_loss:0.024, val_acc:0.976]
Epoch [50/120    avg_loss:0.028, val_acc:0.973]
Epoch [51/120    avg_loss:0.086, val_acc:0.948]
Epoch [52/120    avg_loss:0.027, val_acc:0.971]
Epoch [53/120    avg_loss:0.054, val_acc:0.975]
Epoch [54/120    avg_loss:0.024, val_acc:0.982]
Epoch [55/120    avg_loss:0.013, val_acc:0.980]
Epoch [56/120    avg_loss:0.020, val_acc:0.982]
Epoch [57/120    avg_loss:0.019, val_acc:0.980]
Epoch [58/120    avg_loss:0.061, val_acc:0.968]
Epoch [59/120    avg_loss:0.035, val_acc:0.972]
Epoch [60/120    avg_loss:0.040, val_acc:0.972]
Epoch [61/120    avg_loss:0.019, val_acc:0.983]
Epoch [62/120    avg_loss:0.022, val_acc:0.986]
Epoch [63/120    avg_loss:0.018, val_acc:0.990]
Epoch [64/120    avg_loss:0.015, val_acc:0.987]
Epoch [65/120    avg_loss:0.014, val_acc:0.987]
Epoch [66/120    avg_loss:0.016, val_acc:0.985]
Epoch [67/120    avg_loss:0.016, val_acc:0.986]
Epoch [68/120    avg_loss:0.010, val_acc:0.986]
Epoch [69/120    avg_loss:0.010, val_acc:0.986]
Epoch [70/120    avg_loss:0.012, val_acc:0.987]
Epoch [71/120    avg_loss:0.010, val_acc:0.987]
Epoch [72/120    avg_loss:0.011, val_acc:0.985]
Epoch [73/120    avg_loss:0.010, val_acc:0.985]
Epoch [74/120    avg_loss:0.010, val_acc:0.986]
Epoch [75/120    avg_loss:0.009, val_acc:0.987]
Epoch [76/120    avg_loss:0.009, val_acc:0.987]
Epoch [77/120    avg_loss:0.008, val_acc:0.987]
Epoch [78/120    avg_loss:0.009, val_acc:0.987]
Epoch [79/120    avg_loss:0.011, val_acc:0.987]
Epoch [80/120    avg_loss:0.012, val_acc:0.987]
Epoch [81/120    avg_loss:0.008, val_acc:0.987]
Epoch [82/120    avg_loss:0.008, val_acc:0.987]
Epoch [83/120    avg_loss:0.014, val_acc:0.987]
Epoch [84/120    avg_loss:0.009, val_acc:0.987]
Epoch [85/120    avg_loss:0.013, val_acc:0.987]
Epoch [86/120    avg_loss:0.007, val_acc:0.987]
Epoch [87/120    avg_loss:0.019, val_acc:0.987]
Epoch [88/120    avg_loss:0.016, val_acc:0.987]
Epoch [89/120    avg_loss:0.008, val_acc:0.987]
Epoch [90/120    avg_loss:0.012, val_acc:0.987]
Epoch [91/120    avg_loss:0.010, val_acc:0.987]
Epoch [92/120    avg_loss:0.009, val_acc:0.987]
Epoch [93/120    avg_loss:0.010, val_acc:0.987]
Epoch [94/120    avg_loss:0.008, val_acc:0.987]
Epoch [95/120    avg_loss:0.007, val_acc:0.987]
Epoch [96/120    avg_loss:0.008, val_acc:0.987]
Epoch [97/120    avg_loss:0.007, val_acc:0.987]
Epoch [98/120    avg_loss:0.013, val_acc:0.987]
Epoch [99/120    avg_loss:0.007, val_acc:0.987]
Epoch [100/120    avg_loss:0.014, val_acc:0.987]
Epoch [101/120    avg_loss:0.009, val_acc:0.987]
Epoch [102/120    avg_loss:0.011, val_acc:0.987]
Epoch [103/120    avg_loss:0.009, val_acc:0.987]
Epoch [104/120    avg_loss:0.006, val_acc:0.987]
Epoch [105/120    avg_loss:0.010, val_acc:0.987]
Epoch [106/120    avg_loss:0.008, val_acc:0.987]
Epoch [107/120    avg_loss:0.015, val_acc:0.987]
Epoch [108/120    avg_loss:0.008, val_acc:0.987]
Epoch [109/120    avg_loss:0.008, val_acc:0.987]
Epoch [110/120    avg_loss:0.007, val_acc:0.987]
Epoch [111/120    avg_loss:0.010, val_acc:0.987]
Epoch [112/120    avg_loss:0.011, val_acc:0.987]
Epoch [113/120    avg_loss:0.011, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.987]
Epoch [115/120    avg_loss:0.014, val_acc:0.987]
Epoch [116/120    avg_loss:0.011, val_acc:0.987]
Epoch [117/120    avg_loss:0.010, val_acc:0.987]
Epoch [118/120    avg_loss:0.008, val_acc:0.987]
Epoch [119/120    avg_loss:0.009, val_acc:0.987]
Epoch [120/120    avg_loss:0.007, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    0    0    2    2    0    0    0   13    8    0    0
     0    0    0]
 [   0    0    0  701    0   11    1    0    0   10    1    0   18    5
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    8    1    0    0    0  858    4    2    0
     0    2    0]
 [   0    0    6    0    0    0    5    0    0    1    0 2196    1    1
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0  531    0
     0    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1136    1    0]
 [   0    0    0    0    0    0   20    0    0    0    0    0    0    0
    10  317    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.40650406504065

F1 scores:
[       nan 0.98765432 0.98746082 0.96756384 0.99764706 0.97078652
 0.97840655 0.98039216 0.99883856 0.70833333 0.98169336 0.99411498
 0.97520661 0.98133333 0.99431072 0.95052474 0.97619048]

Kappa:
0.9818385394021899
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc9fea2a710>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.228, val_acc:0.532]
Epoch [2/120    avg_loss:1.574, val_acc:0.583]
Epoch [3/120    avg_loss:1.254, val_acc:0.628]
Epoch [4/120    avg_loss:1.213, val_acc:0.744]
Epoch [5/120    avg_loss:0.842, val_acc:0.782]
Epoch [6/120    avg_loss:0.720, val_acc:0.765]
Epoch [7/120    avg_loss:0.700, val_acc:0.770]
Epoch [8/120    avg_loss:0.511, val_acc:0.820]
Epoch [9/120    avg_loss:0.609, val_acc:0.793]
Epoch [10/120    avg_loss:0.533, val_acc:0.837]
Epoch [11/120    avg_loss:0.579, val_acc:0.825]
Epoch [12/120    avg_loss:0.419, val_acc:0.867]
Epoch [13/120    avg_loss:0.354, val_acc:0.885]
Epoch [14/120    avg_loss:0.366, val_acc:0.854]
Epoch [15/120    avg_loss:0.299, val_acc:0.898]
Epoch [16/120    avg_loss:0.185, val_acc:0.910]
Epoch [17/120    avg_loss:0.206, val_acc:0.907]
Epoch [18/120    avg_loss:0.163, val_acc:0.933]
Epoch [19/120    avg_loss:0.167, val_acc:0.929]
Epoch [20/120    avg_loss:0.153, val_acc:0.918]
Epoch [21/120    avg_loss:0.213, val_acc:0.860]
Epoch [22/120    avg_loss:0.236, val_acc:0.926]
Epoch [23/120    avg_loss:0.153, val_acc:0.933]
Epoch [24/120    avg_loss:0.116, val_acc:0.939]
Epoch [25/120    avg_loss:0.096, val_acc:0.931]
Epoch [26/120    avg_loss:0.132, val_acc:0.956]
Epoch [27/120    avg_loss:0.120, val_acc:0.932]
Epoch [28/120    avg_loss:0.114, val_acc:0.953]
Epoch [29/120    avg_loss:0.092, val_acc:0.956]
Epoch [30/120    avg_loss:0.122, val_acc:0.940]
Epoch [31/120    avg_loss:0.085, val_acc:0.965]
Epoch [32/120    avg_loss:0.109, val_acc:0.932]
Epoch [33/120    avg_loss:0.082, val_acc:0.968]
Epoch [34/120    avg_loss:0.081, val_acc:0.956]
Epoch [35/120    avg_loss:0.072, val_acc:0.968]
Epoch [36/120    avg_loss:0.106, val_acc:0.967]
Epoch [37/120    avg_loss:0.065, val_acc:0.967]
Epoch [38/120    avg_loss:0.069, val_acc:0.971]
Epoch [39/120    avg_loss:0.057, val_acc:0.965]
Epoch [40/120    avg_loss:0.052, val_acc:0.971]
Epoch [41/120    avg_loss:0.043, val_acc:0.970]
Epoch [42/120    avg_loss:0.041, val_acc:0.968]
Epoch [43/120    avg_loss:0.042, val_acc:0.969]
Epoch [44/120    avg_loss:0.035, val_acc:0.965]
Epoch [45/120    avg_loss:0.046, val_acc:0.967]
Epoch [46/120    avg_loss:0.035, val_acc:0.966]
Epoch [47/120    avg_loss:0.037, val_acc:0.965]
Epoch [48/120    avg_loss:0.072, val_acc:0.974]
Epoch [49/120    avg_loss:0.039, val_acc:0.967]
Epoch [50/120    avg_loss:0.030, val_acc:0.968]
Epoch [51/120    avg_loss:0.027, val_acc:0.972]
Epoch [52/120    avg_loss:0.035, val_acc:0.976]
Epoch [53/120    avg_loss:0.021, val_acc:0.979]
Epoch [54/120    avg_loss:0.021, val_acc:0.965]
Epoch [55/120    avg_loss:0.022, val_acc:0.971]
Epoch [56/120    avg_loss:0.021, val_acc:0.967]
Epoch [57/120    avg_loss:0.033, val_acc:0.978]
Epoch [58/120    avg_loss:0.040, val_acc:0.971]
Epoch [59/120    avg_loss:0.035, val_acc:0.979]
Epoch [60/120    avg_loss:0.016, val_acc:0.987]
Epoch [61/120    avg_loss:0.015, val_acc:0.985]
Epoch [62/120    avg_loss:0.014, val_acc:0.971]
Epoch [63/120    avg_loss:0.021, val_acc:0.971]
Epoch [64/120    avg_loss:0.028, val_acc:0.969]
Epoch [65/120    avg_loss:0.025, val_acc:0.971]
Epoch [66/120    avg_loss:0.050, val_acc:0.983]
Epoch [67/120    avg_loss:0.022, val_acc:0.986]
Epoch [68/120    avg_loss:0.010, val_acc:0.986]
Epoch [69/120    avg_loss:0.017, val_acc:0.965]
Epoch [70/120    avg_loss:0.020, val_acc:0.975]
Epoch [71/120    avg_loss:0.019, val_acc:0.984]
Epoch [72/120    avg_loss:0.006, val_acc:0.982]
Epoch [73/120    avg_loss:0.009, val_acc:0.988]
Epoch [74/120    avg_loss:0.006, val_acc:0.986]
Epoch [75/120    avg_loss:0.013, val_acc:0.979]
Epoch [76/120    avg_loss:0.007, val_acc:0.986]
Epoch [77/120    avg_loss:0.006, val_acc:0.989]
Epoch [78/120    avg_loss:0.008, val_acc:0.982]
Epoch [79/120    avg_loss:0.013, val_acc:0.987]
Epoch [80/120    avg_loss:0.011, val_acc:0.985]
Epoch [81/120    avg_loss:0.014, val_acc:0.977]
Epoch [82/120    avg_loss:0.058, val_acc:0.972]
Epoch [83/120    avg_loss:0.018, val_acc:0.980]
Epoch [84/120    avg_loss:0.014, val_acc:0.986]
Epoch [85/120    avg_loss:0.015, val_acc:0.975]
Epoch [86/120    avg_loss:0.019, val_acc:0.987]
Epoch [87/120    avg_loss:0.031, val_acc:0.971]
Epoch [88/120    avg_loss:0.020, val_acc:0.985]
Epoch [89/120    avg_loss:0.014, val_acc:0.986]
Epoch [90/120    avg_loss:0.013, val_acc:0.985]
Epoch [91/120    avg_loss:0.009, val_acc:0.986]
Epoch [92/120    avg_loss:0.009, val_acc:0.987]
Epoch [93/120    avg_loss:0.008, val_acc:0.988]
Epoch [94/120    avg_loss:0.006, val_acc:0.989]
Epoch [95/120    avg_loss:0.012, val_acc:0.989]
Epoch [96/120    avg_loss:0.006, val_acc:0.989]
Epoch [97/120    avg_loss:0.006, val_acc:0.988]
Epoch [98/120    avg_loss:0.004, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.988]
Epoch [100/120    avg_loss:0.006, val_acc:0.988]
Epoch [101/120    avg_loss:0.004, val_acc:0.988]
Epoch [102/120    avg_loss:0.005, val_acc:0.989]
Epoch [103/120    avg_loss:0.006, val_acc:0.989]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.004, val_acc:0.989]
Epoch [106/120    avg_loss:0.005, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.004, val_acc:0.988]
Epoch [109/120    avg_loss:0.005, val_acc:0.989]
Epoch [110/120    avg_loss:0.004, val_acc:0.988]
Epoch [111/120    avg_loss:0.004, val_acc:0.988]
Epoch [112/120    avg_loss:0.004, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.989]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.007, val_acc:0.989]
Epoch [116/120    avg_loss:0.007, val_acc:0.989]
Epoch [117/120    avg_loss:0.005, val_acc:0.989]
Epoch [118/120    avg_loss:0.005, val_acc:0.989]
Epoch [119/120    avg_loss:0.007, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1271    4    0    0    2    0    0    0    5    2    1    0
     0    0    0]
 [   0    0    0  687    0   11    0    0    0    6    0    0   40    2
     1    0    0]
 [   0    0    0    3  210    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    3    0    0    0    0  854    6    3    0
     0    2    0]
 [   0    0    3    0    0    0    2    0    0    0    3 2201    0    1
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0  529    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    17  315    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.31978319783198

F1 scores:
[       nan 1.         0.99064692 0.95086505 0.9929078  0.97963801
 0.98498498 1.         1.         0.7        0.98273878 0.9959276
 0.95401262 0.9919571  0.9912816  0.94879518 0.96470588]

Kappa:
0.9808451481656173
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f63d04cb6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.284, val_acc:0.517]
Epoch [2/120    avg_loss:1.790, val_acc:0.642]
Epoch [3/120    avg_loss:1.370, val_acc:0.684]
Epoch [4/120    avg_loss:1.105, val_acc:0.729]
Epoch [5/120    avg_loss:1.098, val_acc:0.720]
Epoch [6/120    avg_loss:0.900, val_acc:0.682]
Epoch [7/120    avg_loss:0.798, val_acc:0.779]
Epoch [8/120    avg_loss:0.779, val_acc:0.742]
Epoch [9/120    avg_loss:0.620, val_acc:0.808]
Epoch [10/120    avg_loss:0.571, val_acc:0.857]
Epoch [11/120    avg_loss:0.605, val_acc:0.781]
Epoch [12/120    avg_loss:0.530, val_acc:0.862]
Epoch [13/120    avg_loss:0.401, val_acc:0.886]
Epoch [14/120    avg_loss:0.348, val_acc:0.897]
Epoch [15/120    avg_loss:0.296, val_acc:0.928]
Epoch [16/120    avg_loss:0.234, val_acc:0.908]
Epoch [17/120    avg_loss:0.361, val_acc:0.824]
Epoch [18/120    avg_loss:0.260, val_acc:0.938]
Epoch [19/120    avg_loss:0.182, val_acc:0.943]
Epoch [20/120    avg_loss:0.155, val_acc:0.925]
Epoch [21/120    avg_loss:0.143, val_acc:0.909]
Epoch [22/120    avg_loss:0.146, val_acc:0.961]
Epoch [23/120    avg_loss:0.093, val_acc:0.956]
Epoch [24/120    avg_loss:0.132, val_acc:0.943]
Epoch [25/120    avg_loss:0.121, val_acc:0.952]
Epoch [26/120    avg_loss:0.127, val_acc:0.957]
Epoch [27/120    avg_loss:0.112, val_acc:0.951]
Epoch [28/120    avg_loss:0.088, val_acc:0.947]
Epoch [29/120    avg_loss:0.083, val_acc:0.957]
Epoch [30/120    avg_loss:0.070, val_acc:0.963]
Epoch [31/120    avg_loss:0.098, val_acc:0.936]
Epoch [32/120    avg_loss:0.070, val_acc:0.969]
Epoch [33/120    avg_loss:0.074, val_acc:0.969]
Epoch [34/120    avg_loss:0.047, val_acc:0.970]
Epoch [35/120    avg_loss:0.069, val_acc:0.964]
Epoch [36/120    avg_loss:0.045, val_acc:0.971]
Epoch [37/120    avg_loss:0.052, val_acc:0.942]
Epoch [38/120    avg_loss:0.074, val_acc:0.958]
Epoch [39/120    avg_loss:0.063, val_acc:0.970]
Epoch [40/120    avg_loss:0.042, val_acc:0.977]
Epoch [41/120    avg_loss:0.035, val_acc:0.976]
Epoch [42/120    avg_loss:0.033, val_acc:0.970]
Epoch [43/120    avg_loss:0.030, val_acc:0.980]
Epoch [44/120    avg_loss:0.029, val_acc:0.978]
Epoch [45/120    avg_loss:0.019, val_acc:0.980]
Epoch [46/120    avg_loss:0.025, val_acc:0.980]
Epoch [47/120    avg_loss:0.035, val_acc:0.968]
Epoch [48/120    avg_loss:0.027, val_acc:0.978]
Epoch [49/120    avg_loss:0.029, val_acc:0.978]
Epoch [50/120    avg_loss:0.024, val_acc:0.971]
Epoch [51/120    avg_loss:0.039, val_acc:0.972]
Epoch [52/120    avg_loss:0.036, val_acc:0.969]
Epoch [53/120    avg_loss:0.033, val_acc:0.980]
Epoch [54/120    avg_loss:0.043, val_acc:0.967]
Epoch [55/120    avg_loss:0.048, val_acc:0.957]
Epoch [56/120    avg_loss:0.108, val_acc:0.957]
Epoch [57/120    avg_loss:0.079, val_acc:0.971]
Epoch [58/120    avg_loss:0.079, val_acc:0.967]
Epoch [59/120    avg_loss:0.036, val_acc:0.974]
Epoch [60/120    avg_loss:0.026, val_acc:0.976]
Epoch [61/120    avg_loss:0.017, val_acc:0.986]
Epoch [62/120    avg_loss:0.026, val_acc:0.986]
Epoch [63/120    avg_loss:0.024, val_acc:0.936]
Epoch [64/120    avg_loss:0.039, val_acc:0.978]
Epoch [65/120    avg_loss:0.039, val_acc:0.974]
Epoch [66/120    avg_loss:0.063, val_acc:0.940]
Epoch [67/120    avg_loss:0.038, val_acc:0.984]
Epoch [68/120    avg_loss:0.015, val_acc:0.985]
Epoch [69/120    avg_loss:0.014, val_acc:0.978]
Epoch [70/120    avg_loss:0.012, val_acc:0.982]
Epoch [71/120    avg_loss:0.017, val_acc:0.981]
Epoch [72/120    avg_loss:0.018, val_acc:0.972]
Epoch [73/120    avg_loss:0.033, val_acc:0.988]
Epoch [74/120    avg_loss:0.022, val_acc:0.982]
Epoch [75/120    avg_loss:0.011, val_acc:0.985]
Epoch [76/120    avg_loss:0.010, val_acc:0.984]
Epoch [77/120    avg_loss:0.017, val_acc:0.984]
Epoch [78/120    avg_loss:0.016, val_acc:0.984]
Epoch [79/120    avg_loss:0.013, val_acc:0.985]
Epoch [80/120    avg_loss:0.016, val_acc:0.986]
Epoch [81/120    avg_loss:0.010, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.983]
Epoch [83/120    avg_loss:0.010, val_acc:0.978]
Epoch [84/120    avg_loss:0.012, val_acc:0.984]
Epoch [85/120    avg_loss:0.018, val_acc:0.983]
Epoch [86/120    avg_loss:0.020, val_acc:0.984]
Epoch [87/120    avg_loss:0.010, val_acc:0.986]
Epoch [88/120    avg_loss:0.012, val_acc:0.985]
Epoch [89/120    avg_loss:0.006, val_acc:0.986]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.011, val_acc:0.988]
Epoch [92/120    avg_loss:0.006, val_acc:0.987]
Epoch [93/120    avg_loss:0.006, val_acc:0.989]
Epoch [94/120    avg_loss:0.007, val_acc:0.989]
Epoch [95/120    avg_loss:0.005, val_acc:0.988]
Epoch [96/120    avg_loss:0.004, val_acc:0.988]
Epoch [97/120    avg_loss:0.008, val_acc:0.987]
Epoch [98/120    avg_loss:0.009, val_acc:0.987]
Epoch [99/120    avg_loss:0.006, val_acc:0.987]
Epoch [100/120    avg_loss:0.003, val_acc:0.987]
Epoch [101/120    avg_loss:0.004, val_acc:0.987]
Epoch [102/120    avg_loss:0.003, val_acc:0.987]
Epoch [103/120    avg_loss:0.004, val_acc:0.987]
Epoch [104/120    avg_loss:0.003, val_acc:0.987]
Epoch [105/120    avg_loss:0.006, val_acc:0.987]
Epoch [106/120    avg_loss:0.003, val_acc:0.987]
Epoch [107/120    avg_loss:0.004, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.004, val_acc:0.988]
Epoch [110/120    avg_loss:0.007, val_acc:0.988]
Epoch [111/120    avg_loss:0.005, val_acc:0.988]
Epoch [112/120    avg_loss:0.005, val_acc:0.988]
Epoch [113/120    avg_loss:0.004, val_acc:0.988]
Epoch [114/120    avg_loss:0.003, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.004, val_acc:0.988]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.004, val_acc:0.988]
Epoch [119/120    avg_loss:0.005, val_acc:0.988]
Epoch [120/120    avg_loss:0.004, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1273    0    0    0    0    0    0    0    7    5    0    0
     0    0    0]
 [   0    0    0  710    1   11    0    0    0    2    0    0   20    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0  865    1    4    0
     0    3    0]
 [   0    0    4    0    0    0    4    0    0    0    4 2196    0    2
     0    0    0]
 [   0    0    0    0    2    0    0    0    0    0    2    0  527    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   15    0    0    1    0    1    0    0    0
  1122    0    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    21  317    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.5691056910569

F1 scores:
[       nan 0.98765432 0.99336715 0.97460535 0.99300699 0.96651786
 0.99020347 0.98039216 0.99883856 0.92307692 0.98631699 0.99546691
 0.96964121 0.98666667 0.98334794 0.95052474 0.9704142 ]

Kappa:
0.9836916430820264
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efd7fc7d668>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.193, val_acc:0.500]
Epoch [2/120    avg_loss:1.704, val_acc:0.622]
Epoch [3/120    avg_loss:1.386, val_acc:0.690]
Epoch [4/120    avg_loss:1.241, val_acc:0.675]
Epoch [5/120    avg_loss:0.946, val_acc:0.770]
Epoch [6/120    avg_loss:0.704, val_acc:0.802]
Epoch [7/120    avg_loss:0.609, val_acc:0.824]
Epoch [8/120    avg_loss:0.473, val_acc:0.789]
Epoch [9/120    avg_loss:0.690, val_acc:0.847]
Epoch [10/120    avg_loss:0.441, val_acc:0.871]
Epoch [11/120    avg_loss:0.401, val_acc:0.895]
Epoch [12/120    avg_loss:0.366, val_acc:0.889]
Epoch [13/120    avg_loss:0.266, val_acc:0.913]
Epoch [14/120    avg_loss:0.301, val_acc:0.874]
Epoch [15/120    avg_loss:0.257, val_acc:0.911]
Epoch [16/120    avg_loss:0.169, val_acc:0.929]
Epoch [17/120    avg_loss:0.142, val_acc:0.950]
Epoch [18/120    avg_loss:0.165, val_acc:0.873]
Epoch [19/120    avg_loss:0.175, val_acc:0.935]
Epoch [20/120    avg_loss:0.211, val_acc:0.917]
Epoch [21/120    avg_loss:0.143, val_acc:0.943]
Epoch [22/120    avg_loss:0.187, val_acc:0.946]
Epoch [23/120    avg_loss:0.119, val_acc:0.934]
Epoch [24/120    avg_loss:0.154, val_acc:0.957]
Epoch [25/120    avg_loss:0.111, val_acc:0.929]
Epoch [26/120    avg_loss:0.131, val_acc:0.929]
Epoch [27/120    avg_loss:0.083, val_acc:0.943]
Epoch [28/120    avg_loss:0.099, val_acc:0.966]
Epoch [29/120    avg_loss:0.058, val_acc:0.975]
Epoch [30/120    avg_loss:0.076, val_acc:0.934]
Epoch [31/120    avg_loss:0.069, val_acc:0.957]
Epoch [32/120    avg_loss:0.096, val_acc:0.892]
Epoch [33/120    avg_loss:0.090, val_acc:0.971]
Epoch [34/120    avg_loss:0.063, val_acc:0.948]
Epoch [35/120    avg_loss:0.063, val_acc:0.972]
Epoch [36/120    avg_loss:0.049, val_acc:0.973]
Epoch [37/120    avg_loss:0.055, val_acc:0.960]
Epoch [38/120    avg_loss:0.040, val_acc:0.976]
Epoch [39/120    avg_loss:0.045, val_acc:0.973]
Epoch [40/120    avg_loss:0.043, val_acc:0.964]
Epoch [41/120    avg_loss:0.035, val_acc:0.973]
Epoch [42/120    avg_loss:0.044, val_acc:0.973]
Epoch [43/120    avg_loss:0.040, val_acc:0.978]
Epoch [44/120    avg_loss:0.040, val_acc:0.977]
Epoch [45/120    avg_loss:0.034, val_acc:0.975]
Epoch [46/120    avg_loss:0.028, val_acc:0.985]
Epoch [47/120    avg_loss:0.030, val_acc:0.980]
Epoch [48/120    avg_loss:0.026, val_acc:0.981]
Epoch [49/120    avg_loss:0.030, val_acc:0.978]
Epoch [50/120    avg_loss:0.022, val_acc:0.982]
Epoch [51/120    avg_loss:0.019, val_acc:0.984]
Epoch [52/120    avg_loss:0.026, val_acc:0.982]
Epoch [53/120    avg_loss:0.016, val_acc:0.983]
Epoch [54/120    avg_loss:0.025, val_acc:0.987]
Epoch [55/120    avg_loss:0.031, val_acc:0.977]
Epoch [56/120    avg_loss:0.023, val_acc:0.984]
Epoch [57/120    avg_loss:0.022, val_acc:0.978]
Epoch [58/120    avg_loss:0.017, val_acc:0.987]
Epoch [59/120    avg_loss:0.011, val_acc:0.984]
Epoch [60/120    avg_loss:0.009, val_acc:0.985]
Epoch [61/120    avg_loss:0.033, val_acc:0.970]
Epoch [62/120    avg_loss:0.029, val_acc:0.976]
Epoch [63/120    avg_loss:0.042, val_acc:0.974]
Epoch [64/120    avg_loss:0.017, val_acc:0.981]
Epoch [65/120    avg_loss:0.075, val_acc:0.968]
Epoch [66/120    avg_loss:0.027, val_acc:0.983]
Epoch [67/120    avg_loss:0.031, val_acc:0.969]
Epoch [68/120    avg_loss:0.018, val_acc:0.982]
Epoch [69/120    avg_loss:0.021, val_acc:0.978]
Epoch [70/120    avg_loss:0.012, val_acc:0.985]
Epoch [71/120    avg_loss:0.007, val_acc:0.989]
Epoch [72/120    avg_loss:0.006, val_acc:0.988]
Epoch [73/120    avg_loss:0.007, val_acc:0.977]
Epoch [74/120    avg_loss:0.013, val_acc:0.982]
Epoch [75/120    avg_loss:0.009, val_acc:0.986]
Epoch [76/120    avg_loss:0.011, val_acc:0.988]
Epoch [77/120    avg_loss:0.011, val_acc:0.982]
Epoch [78/120    avg_loss:0.008, val_acc:0.989]
Epoch [79/120    avg_loss:0.006, val_acc:0.989]
Epoch [80/120    avg_loss:0.006, val_acc:0.989]
Epoch [81/120    avg_loss:0.009, val_acc:0.970]
Epoch [82/120    avg_loss:0.010, val_acc:0.988]
Epoch [83/120    avg_loss:0.005, val_acc:0.984]
Epoch [84/120    avg_loss:0.007, val_acc:0.986]
Epoch [85/120    avg_loss:0.045, val_acc:0.977]
Epoch [86/120    avg_loss:0.019, val_acc:0.981]
Epoch [87/120    avg_loss:0.027, val_acc:0.966]
Epoch [88/120    avg_loss:0.039, val_acc:0.977]
Epoch [89/120    avg_loss:0.038, val_acc:0.977]
Epoch [90/120    avg_loss:0.019, val_acc:0.988]
Epoch [91/120    avg_loss:0.015, val_acc:0.988]
Epoch [92/120    avg_loss:0.010, val_acc:0.989]
Epoch [93/120    avg_loss:0.008, val_acc:0.990]
Epoch [94/120    avg_loss:0.005, val_acc:0.991]
Epoch [95/120    avg_loss:0.006, val_acc:0.992]
Epoch [96/120    avg_loss:0.006, val_acc:0.990]
Epoch [97/120    avg_loss:0.020, val_acc:0.987]
Epoch [98/120    avg_loss:0.027, val_acc:0.983]
Epoch [99/120    avg_loss:0.010, val_acc:0.988]
Epoch [100/120    avg_loss:0.012, val_acc:0.986]
Epoch [101/120    avg_loss:0.006, val_acc:0.994]
Epoch [102/120    avg_loss:0.004, val_acc:0.987]
Epoch [103/120    avg_loss:0.005, val_acc:0.991]
Epoch [104/120    avg_loss:0.007, val_acc:0.994]
Epoch [105/120    avg_loss:0.008, val_acc:0.989]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.006, val_acc:0.991]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.004, val_acc:0.994]
Epoch [110/120    avg_loss:0.003, val_acc:0.989]
Epoch [111/120    avg_loss:0.003, val_acc:0.996]
Epoch [112/120    avg_loss:0.003, val_acc:0.994]
Epoch [113/120    avg_loss:0.004, val_acc:0.991]
Epoch [114/120    avg_loss:0.004, val_acc:0.991]
Epoch [115/120    avg_loss:0.004, val_acc:0.987]
Epoch [116/120    avg_loss:0.002, val_acc:0.991]
Epoch [117/120    avg_loss:0.003, val_acc:0.990]
Epoch [118/120    avg_loss:0.002, val_acc:0.992]
Epoch [119/120    avg_loss:0.002, val_acc:0.991]
Epoch [120/120    avg_loss:0.002, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1275    4    0    0    0    0    0    0    5    1    0    0
     0    0    0]
 [   0    0    0  723    1    4    0    0    0    6    0    0   12    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0  866    1    1    0
     1    3    0]
 [   0    0   10    0    0    2    3    0    0    0    3 2191    0    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  530    0
     0    0    4]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1136    2    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    19  317    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.84010840108401

F1 scores:
[       nan 1.         0.9922179  0.97901151 0.99765808 0.98633257
 0.98945783 1.         1.         0.73170732 0.98971429 0.99523052
 0.9823911  0.99191375 0.98997821 0.94768311 0.96470588]

Kappa:
0.9867797909139706
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdb14177748>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.225, val_acc:0.545]
Epoch [2/120    avg_loss:1.671, val_acc:0.564]
Epoch [3/120    avg_loss:1.179, val_acc:0.718]
Epoch [4/120    avg_loss:1.059, val_acc:0.746]
Epoch [5/120    avg_loss:0.817, val_acc:0.746]
Epoch [6/120    avg_loss:0.663, val_acc:0.798]
Epoch [7/120    avg_loss:0.671, val_acc:0.831]
Epoch [8/120    avg_loss:0.601, val_acc:0.842]
Epoch [9/120    avg_loss:0.468, val_acc:0.851]
Epoch [10/120    avg_loss:0.412, val_acc:0.834]
Epoch [11/120    avg_loss:0.345, val_acc:0.838]
Epoch [12/120    avg_loss:0.332, val_acc:0.898]
Epoch [13/120    avg_loss:0.278, val_acc:0.872]
Epoch [14/120    avg_loss:0.194, val_acc:0.917]
Epoch [15/120    avg_loss:0.256, val_acc:0.885]
Epoch [16/120    avg_loss:0.233, val_acc:0.905]
Epoch [17/120    avg_loss:0.213, val_acc:0.870]
Epoch [18/120    avg_loss:0.181, val_acc:0.931]
Epoch [19/120    avg_loss:0.154, val_acc:0.922]
Epoch [20/120    avg_loss:0.145, val_acc:0.943]
Epoch [21/120    avg_loss:0.246, val_acc:0.920]
Epoch [22/120    avg_loss:0.160, val_acc:0.944]
Epoch [23/120    avg_loss:0.140, val_acc:0.917]
Epoch [24/120    avg_loss:0.131, val_acc:0.915]
Epoch [25/120    avg_loss:0.163, val_acc:0.950]
Epoch [26/120    avg_loss:0.079, val_acc:0.950]
Epoch [27/120    avg_loss:0.105, val_acc:0.955]
Epoch [28/120    avg_loss:0.065, val_acc:0.944]
Epoch [29/120    avg_loss:0.069, val_acc:0.956]
Epoch [30/120    avg_loss:0.058, val_acc:0.961]
Epoch [31/120    avg_loss:0.066, val_acc:0.941]
Epoch [32/120    avg_loss:0.058, val_acc:0.953]
Epoch [33/120    avg_loss:0.056, val_acc:0.948]
Epoch [34/120    avg_loss:0.055, val_acc:0.943]
Epoch [35/120    avg_loss:0.050, val_acc:0.968]
Epoch [36/120    avg_loss:0.030, val_acc:0.952]
Epoch [37/120    avg_loss:0.044, val_acc:0.962]
Epoch [38/120    avg_loss:0.043, val_acc:0.956]
Epoch [39/120    avg_loss:0.055, val_acc:0.947]
Epoch [40/120    avg_loss:0.084, val_acc:0.943]
Epoch [41/120    avg_loss:0.036, val_acc:0.966]
Epoch [42/120    avg_loss:0.037, val_acc:0.962]
Epoch [43/120    avg_loss:0.027, val_acc:0.969]
Epoch [44/120    avg_loss:0.022, val_acc:0.967]
Epoch [45/120    avg_loss:0.073, val_acc:0.953]
Epoch [46/120    avg_loss:0.039, val_acc:0.941]
Epoch [47/120    avg_loss:0.043, val_acc:0.971]
Epoch [48/120    avg_loss:0.032, val_acc:0.970]
Epoch [49/120    avg_loss:0.030, val_acc:0.964]
Epoch [50/120    avg_loss:0.025, val_acc:0.969]
Epoch [51/120    avg_loss:0.022, val_acc:0.978]
Epoch [52/120    avg_loss:0.021, val_acc:0.957]
Epoch [53/120    avg_loss:0.021, val_acc:0.967]
Epoch [54/120    avg_loss:0.025, val_acc:0.978]
Epoch [55/120    avg_loss:0.012, val_acc:0.980]
Epoch [56/120    avg_loss:0.054, val_acc:0.966]
Epoch [57/120    avg_loss:0.044, val_acc:0.960]
Epoch [58/120    avg_loss:0.035, val_acc:0.971]
Epoch [59/120    avg_loss:0.039, val_acc:0.969]
Epoch [60/120    avg_loss:0.022, val_acc:0.973]
Epoch [61/120    avg_loss:0.040, val_acc:0.974]
Epoch [62/120    avg_loss:0.031, val_acc:0.980]
Epoch [63/120    avg_loss:0.015, val_acc:0.980]
Epoch [64/120    avg_loss:0.010, val_acc:0.983]
Epoch [65/120    avg_loss:0.013, val_acc:0.981]
Epoch [66/120    avg_loss:0.008, val_acc:0.981]
Epoch [67/120    avg_loss:0.016, val_acc:0.982]
Epoch [68/120    avg_loss:0.011, val_acc:0.978]
Epoch [69/120    avg_loss:0.011, val_acc:0.976]
Epoch [70/120    avg_loss:0.008, val_acc:0.985]
Epoch [71/120    avg_loss:0.007, val_acc:0.985]
Epoch [72/120    avg_loss:0.004, val_acc:0.983]
Epoch [73/120    avg_loss:0.008, val_acc:0.982]
Epoch [74/120    avg_loss:0.015, val_acc:0.982]
Epoch [75/120    avg_loss:0.011, val_acc:0.983]
Epoch [76/120    avg_loss:0.006, val_acc:0.984]
Epoch [77/120    avg_loss:0.005, val_acc:0.981]
Epoch [78/120    avg_loss:0.007, val_acc:0.983]
Epoch [79/120    avg_loss:0.011, val_acc:0.982]
Epoch [80/120    avg_loss:0.007, val_acc:0.985]
Epoch [81/120    avg_loss:0.015, val_acc:0.978]
Epoch [82/120    avg_loss:0.014, val_acc:0.977]
Epoch [83/120    avg_loss:0.036, val_acc:0.975]
Epoch [84/120    avg_loss:0.078, val_acc:0.906]
Epoch [85/120    avg_loss:0.136, val_acc:0.948]
Epoch [86/120    avg_loss:0.040, val_acc:0.967]
Epoch [87/120    avg_loss:0.032, val_acc:0.978]
Epoch [88/120    avg_loss:0.023, val_acc:0.978]
Epoch [89/120    avg_loss:0.017, val_acc:0.976]
Epoch [90/120    avg_loss:0.022, val_acc:0.976]
Epoch [91/120    avg_loss:0.011, val_acc:0.977]
Epoch [92/120    avg_loss:0.009, val_acc:0.985]
Epoch [93/120    avg_loss:0.021, val_acc:0.982]
Epoch [94/120    avg_loss:0.009, val_acc:0.982]
Epoch [95/120    avg_loss:0.009, val_acc:0.980]
Epoch [96/120    avg_loss:0.033, val_acc:0.976]
Epoch [97/120    avg_loss:0.033, val_acc:0.982]
Epoch [98/120    avg_loss:0.016, val_acc:0.983]
Epoch [99/120    avg_loss:0.016, val_acc:0.981]
Epoch [100/120    avg_loss:0.007, val_acc:0.981]
Epoch [101/120    avg_loss:0.017, val_acc:0.983]
Epoch [102/120    avg_loss:0.007, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.976]
Epoch [104/120    avg_loss:0.013, val_acc:0.982]
Epoch [105/120    avg_loss:0.017, val_acc:0.988]
Epoch [106/120    avg_loss:0.005, val_acc:0.986]
Epoch [107/120    avg_loss:0.005, val_acc:0.986]
Epoch [108/120    avg_loss:0.005, val_acc:0.985]
Epoch [109/120    avg_loss:0.007, val_acc:0.985]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.006, val_acc:0.986]
Epoch [112/120    avg_loss:0.003, val_acc:0.987]
Epoch [113/120    avg_loss:0.004, val_acc:0.988]
Epoch [114/120    avg_loss:0.003, val_acc:0.984]
Epoch [115/120    avg_loss:0.003, val_acc:0.982]
Epoch [116/120    avg_loss:0.004, val_acc:0.984]
Epoch [117/120    avg_loss:0.002, val_acc:0.985]
Epoch [118/120    avg_loss:0.004, val_acc:0.984]
Epoch [119/120    avg_loss:0.002, val_acc:0.987]
Epoch [120/120    avg_loss:0.006, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1279    2    0    0    0    0    0    0    3    1    0    0
     0    0    0]
 [   0    0    0  681    0    7    0    0    0    4    0    0   55    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    5    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    3    0    0    0    0  864    1    0    0
     0    5    0]
 [   0    0   22    0    0    1    1    0    0    0    9 2176    0    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  534    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1135    2    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    19  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.22222222222223

F1 scores:
[       nan 0.98765432 0.98840804 0.94912892 0.99764706 0.98057143
 0.99169811 0.98039216 0.99883856 0.68292683 0.98573873 0.99179581
 0.94933333 0.99730458 0.98996947 0.94642857 0.98795181]

Kappa:
0.9797424225392697
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7b05da16d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.185, val_acc:0.598]
Epoch [2/120    avg_loss:1.566, val_acc:0.514]
Epoch [3/120    avg_loss:1.326, val_acc:0.653]
Epoch [4/120    avg_loss:1.048, val_acc:0.731]
Epoch [5/120    avg_loss:0.849, val_acc:0.698]
Epoch [6/120    avg_loss:0.777, val_acc:0.801]
Epoch [7/120    avg_loss:0.708, val_acc:0.802]
Epoch [8/120    avg_loss:0.604, val_acc:0.804]
Epoch [9/120    avg_loss:0.577, val_acc:0.871]
Epoch [10/120    avg_loss:0.400, val_acc:0.877]
Epoch [11/120    avg_loss:0.355, val_acc:0.890]
Epoch [12/120    avg_loss:0.246, val_acc:0.912]
Epoch [13/120    avg_loss:0.239, val_acc:0.921]
Epoch [14/120    avg_loss:0.230, val_acc:0.857]
Epoch [15/120    avg_loss:0.297, val_acc:0.879]
Epoch [16/120    avg_loss:0.215, val_acc:0.908]
Epoch [17/120    avg_loss:0.200, val_acc:0.926]
Epoch [18/120    avg_loss:0.243, val_acc:0.913]
Epoch [19/120    avg_loss:0.147, val_acc:0.942]
Epoch [20/120    avg_loss:0.177, val_acc:0.921]
Epoch [21/120    avg_loss:0.198, val_acc:0.905]
Epoch [22/120    avg_loss:0.269, val_acc:0.829]
Epoch [23/120    avg_loss:0.228, val_acc:0.917]
Epoch [24/120    avg_loss:0.139, val_acc:0.953]
Epoch [25/120    avg_loss:0.108, val_acc:0.955]
Epoch [26/120    avg_loss:0.080, val_acc:0.959]
Epoch [27/120    avg_loss:0.082, val_acc:0.945]
Epoch [28/120    avg_loss:0.095, val_acc:0.918]
Epoch [29/120    avg_loss:0.075, val_acc:0.960]
Epoch [30/120    avg_loss:0.089, val_acc:0.961]
Epoch [31/120    avg_loss:0.073, val_acc:0.952]
Epoch [32/120    avg_loss:0.048, val_acc:0.963]
Epoch [33/120    avg_loss:0.048, val_acc:0.959]
Epoch [34/120    avg_loss:0.078, val_acc:0.967]
Epoch [35/120    avg_loss:0.044, val_acc:0.969]
Epoch [36/120    avg_loss:0.046, val_acc:0.957]
Epoch [37/120    avg_loss:0.043, val_acc:0.961]
Epoch [38/120    avg_loss:0.040, val_acc:0.956]
Epoch [39/120    avg_loss:0.065, val_acc:0.944]
Epoch [40/120    avg_loss:0.124, val_acc:0.941]
Epoch [41/120    avg_loss:0.091, val_acc:0.957]
Epoch [42/120    avg_loss:0.055, val_acc:0.969]
Epoch [43/120    avg_loss:0.064, val_acc:0.968]
Epoch [44/120    avg_loss:0.049, val_acc:0.974]
Epoch [45/120    avg_loss:0.034, val_acc:0.971]
Epoch [46/120    avg_loss:0.045, val_acc:0.848]
Epoch [47/120    avg_loss:0.089, val_acc:0.966]
Epoch [48/120    avg_loss:0.037, val_acc:0.967]
Epoch [49/120    avg_loss:0.025, val_acc:0.973]
Epoch [50/120    avg_loss:0.039, val_acc:0.957]
Epoch [51/120    avg_loss:0.030, val_acc:0.971]
Epoch [52/120    avg_loss:0.021, val_acc:0.974]
Epoch [53/120    avg_loss:0.024, val_acc:0.982]
Epoch [54/120    avg_loss:0.018, val_acc:0.983]
Epoch [55/120    avg_loss:0.011, val_acc:0.981]
Epoch [56/120    avg_loss:0.014, val_acc:0.976]
Epoch [57/120    avg_loss:0.020, val_acc:0.973]
Epoch [58/120    avg_loss:0.015, val_acc:0.980]
Epoch [59/120    avg_loss:0.017, val_acc:0.981]
Epoch [60/120    avg_loss:0.017, val_acc:0.981]
Epoch [61/120    avg_loss:0.019, val_acc:0.980]
Epoch [62/120    avg_loss:0.013, val_acc:0.976]
Epoch [63/120    avg_loss:0.013, val_acc:0.975]
Epoch [64/120    avg_loss:0.008, val_acc:0.981]
Epoch [65/120    avg_loss:0.030, val_acc:0.954]
Epoch [66/120    avg_loss:0.018, val_acc:0.980]
Epoch [67/120    avg_loss:0.020, val_acc:0.977]
Epoch [68/120    avg_loss:0.016, val_acc:0.983]
Epoch [69/120    avg_loss:0.013, val_acc:0.985]
Epoch [70/120    avg_loss:0.011, val_acc:0.987]
Epoch [71/120    avg_loss:0.008, val_acc:0.987]
Epoch [72/120    avg_loss:0.011, val_acc:0.987]
Epoch [73/120    avg_loss:0.008, val_acc:0.988]
Epoch [74/120    avg_loss:0.009, val_acc:0.988]
Epoch [75/120    avg_loss:0.006, val_acc:0.988]
Epoch [76/120    avg_loss:0.008, val_acc:0.987]
Epoch [77/120    avg_loss:0.008, val_acc:0.988]
Epoch [78/120    avg_loss:0.011, val_acc:0.988]
Epoch [79/120    avg_loss:0.010, val_acc:0.987]
Epoch [80/120    avg_loss:0.010, val_acc:0.987]
Epoch [81/120    avg_loss:0.007, val_acc:0.987]
Epoch [82/120    avg_loss:0.013, val_acc:0.987]
Epoch [83/120    avg_loss:0.006, val_acc:0.987]
Epoch [84/120    avg_loss:0.008, val_acc:0.988]
Epoch [85/120    avg_loss:0.008, val_acc:0.986]
Epoch [86/120    avg_loss:0.008, val_acc:0.987]
Epoch [87/120    avg_loss:0.008, val_acc:0.987]
Epoch [88/120    avg_loss:0.006, val_acc:0.986]
Epoch [89/120    avg_loss:0.014, val_acc:0.987]
Epoch [90/120    avg_loss:0.005, val_acc:0.987]
Epoch [91/120    avg_loss:0.007, val_acc:0.987]
Epoch [92/120    avg_loss:0.009, val_acc:0.987]
Epoch [93/120    avg_loss:0.006, val_acc:0.987]
Epoch [94/120    avg_loss:0.005, val_acc:0.986]
Epoch [95/120    avg_loss:0.008, val_acc:0.987]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.008, val_acc:0.988]
Epoch [98/120    avg_loss:0.005, val_acc:0.986]
Epoch [99/120    avg_loss:0.005, val_acc:0.987]
Epoch [100/120    avg_loss:0.012, val_acc:0.986]
Epoch [101/120    avg_loss:0.006, val_acc:0.986]
Epoch [102/120    avg_loss:0.005, val_acc:0.986]
Epoch [103/120    avg_loss:0.006, val_acc:0.986]
Epoch [104/120    avg_loss:0.004, val_acc:0.986]
Epoch [105/120    avg_loss:0.005, val_acc:0.986]
Epoch [106/120    avg_loss:0.005, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.006, val_acc:0.986]
Epoch [109/120    avg_loss:0.005, val_acc:0.986]
Epoch [110/120    avg_loss:0.004, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.987]
Epoch [116/120    avg_loss:0.006, val_acc:0.987]
Epoch [117/120    avg_loss:0.004, val_acc:0.987]
Epoch [118/120    avg_loss:0.005, val_acc:0.987]
Epoch [119/120    avg_loss:0.006, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1271    2    0    0    1    0    0    0    6    3    1    0
     0    1    0]
 [   0    0    0  713    0   14    0    0    0    3    2    0   13    2
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0  860    0    4    0
     1    6    0]
 [   0    0   11    0    0    0    2    0    0    0    2 2194    1    0
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0  530    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    24  313    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.60162601626016

F1 scores:
[       nan 0.975      0.98987539 0.97337884 0.99764706 0.9752809
 0.98867925 1.         1.         0.84210526 0.98454493 0.99523702
 0.97695853 0.99462366 0.98826597 0.93853073 0.97619048]

Kappa:
0.98405923677497
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:58
Validation dataloader:58
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd277eea6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.252, val_acc:0.541]
Epoch [2/120    avg_loss:1.814, val_acc:0.514]
Epoch [3/120    avg_loss:1.374, val_acc:0.548]
Epoch [4/120    avg_loss:1.208, val_acc:0.683]
Epoch [5/120    avg_loss:0.927, val_acc:0.671]
Epoch [6/120    avg_loss:0.861, val_acc:0.696]
Epoch [7/120    avg_loss:0.747, val_acc:0.750]
Epoch [8/120    avg_loss:0.757, val_acc:0.775]
Epoch [9/120    avg_loss:0.523, val_acc:0.834]
Epoch [10/120    avg_loss:0.431, val_acc:0.797]
Epoch [11/120    avg_loss:0.500, val_acc:0.823]
Epoch [12/120    avg_loss:0.317, val_acc:0.879]
Epoch [13/120    avg_loss:0.223, val_acc:0.870]
Epoch [14/120    avg_loss:0.274, val_acc:0.868]
Epoch [15/120    avg_loss:0.370, val_acc:0.864]
Epoch [16/120    avg_loss:0.204, val_acc:0.891]
Epoch [17/120    avg_loss:0.166, val_acc:0.912]
Epoch [18/120    avg_loss:0.140, val_acc:0.892]
Epoch [19/120    avg_loss:0.174, val_acc:0.913]
Epoch [20/120    avg_loss:0.144, val_acc:0.917]
Epoch [21/120    avg_loss:0.129, val_acc:0.921]
Epoch [22/120    avg_loss:0.155, val_acc:0.906]
Epoch [23/120    avg_loss:0.105, val_acc:0.916]
Epoch [24/120    avg_loss:0.120, val_acc:0.904]
Epoch [25/120    avg_loss:0.115, val_acc:0.924]
Epoch [26/120    avg_loss:0.071, val_acc:0.936]
Epoch [27/120    avg_loss:0.065, val_acc:0.943]
Epoch [28/120    avg_loss:0.103, val_acc:0.942]
Epoch [29/120    avg_loss:0.062, val_acc:0.940]
Epoch [30/120    avg_loss:0.062, val_acc:0.945]
Epoch [31/120    avg_loss:0.056, val_acc:0.944]
Epoch [32/120    avg_loss:0.093, val_acc:0.926]
Epoch [33/120    avg_loss:0.085, val_acc:0.933]
Epoch [34/120    avg_loss:0.055, val_acc:0.952]
Epoch [35/120    avg_loss:0.051, val_acc:0.966]
Epoch [36/120    avg_loss:0.041, val_acc:0.950]
Epoch [37/120    avg_loss:0.048, val_acc:0.962]
Epoch [38/120    avg_loss:0.051, val_acc:0.966]
Epoch [39/120    avg_loss:0.035, val_acc:0.966]
Epoch [40/120    avg_loss:0.033, val_acc:0.951]
Epoch [41/120    avg_loss:0.033, val_acc:0.969]
Epoch [42/120    avg_loss:0.038, val_acc:0.948]
Epoch [43/120    avg_loss:0.054, val_acc:0.963]
Epoch [44/120    avg_loss:0.072, val_acc:0.942]
Epoch [45/120    avg_loss:0.064, val_acc:0.943]
Epoch [46/120    avg_loss:0.081, val_acc:0.928]
Epoch [47/120    avg_loss:0.093, val_acc:0.927]
Epoch [48/120    avg_loss:0.085, val_acc:0.961]
Epoch [49/120    avg_loss:0.036, val_acc:0.953]
Epoch [50/120    avg_loss:0.022, val_acc:0.966]
Epoch [51/120    avg_loss:0.022, val_acc:0.968]
Epoch [52/120    avg_loss:0.035, val_acc:0.961]
Epoch [53/120    avg_loss:0.027, val_acc:0.961]
Epoch [54/120    avg_loss:0.028, val_acc:0.956]
Epoch [55/120    avg_loss:0.049, val_acc:0.968]
Epoch [56/120    avg_loss:0.025, val_acc:0.974]
Epoch [57/120    avg_loss:0.016, val_acc:0.977]
Epoch [58/120    avg_loss:0.014, val_acc:0.976]
Epoch [59/120    avg_loss:0.014, val_acc:0.978]
Epoch [60/120    avg_loss:0.015, val_acc:0.978]
Epoch [61/120    avg_loss:0.017, val_acc:0.976]
Epoch [62/120    avg_loss:0.020, val_acc:0.974]
Epoch [63/120    avg_loss:0.010, val_acc:0.974]
Epoch [64/120    avg_loss:0.015, val_acc:0.974]
Epoch [65/120    avg_loss:0.011, val_acc:0.974]
Epoch [66/120    avg_loss:0.009, val_acc:0.976]
Epoch [67/120    avg_loss:0.011, val_acc:0.976]
Epoch [68/120    avg_loss:0.011, val_acc:0.975]
Epoch [69/120    avg_loss:0.011, val_acc:0.976]
Epoch [70/120    avg_loss:0.009, val_acc:0.977]
Epoch [71/120    avg_loss:0.010, val_acc:0.978]
Epoch [72/120    avg_loss:0.009, val_acc:0.978]
Epoch [73/120    avg_loss:0.009, val_acc:0.978]
Epoch [74/120    avg_loss:0.008, val_acc:0.979]
Epoch [75/120    avg_loss:0.010, val_acc:0.978]
Epoch [76/120    avg_loss:0.012, val_acc:0.978]
Epoch [77/120    avg_loss:0.008, val_acc:0.976]
Epoch [78/120    avg_loss:0.011, val_acc:0.976]
Epoch [79/120    avg_loss:0.008, val_acc:0.977]
Epoch [80/120    avg_loss:0.010, val_acc:0.976]
Epoch [81/120    avg_loss:0.008, val_acc:0.977]
Epoch [82/120    avg_loss:0.007, val_acc:0.978]
Epoch [83/120    avg_loss:0.010, val_acc:0.978]
Epoch [84/120    avg_loss:0.007, val_acc:0.979]
Epoch [85/120    avg_loss:0.006, val_acc:0.979]
Epoch [86/120    avg_loss:0.012, val_acc:0.978]
Epoch [87/120    avg_loss:0.012, val_acc:0.978]
Epoch [88/120    avg_loss:0.008, val_acc:0.978]
Epoch [89/120    avg_loss:0.011, val_acc:0.977]
Epoch [90/120    avg_loss:0.016, val_acc:0.976]
Epoch [91/120    avg_loss:0.009, val_acc:0.977]
Epoch [92/120    avg_loss:0.007, val_acc:0.978]
Epoch [93/120    avg_loss:0.009, val_acc:0.978]
Epoch [94/120    avg_loss:0.010, val_acc:0.980]
Epoch [95/120    avg_loss:0.008, val_acc:0.976]
Epoch [96/120    avg_loss:0.007, val_acc:0.976]
Epoch [97/120    avg_loss:0.006, val_acc:0.977]
Epoch [98/120    avg_loss:0.007, val_acc:0.977]
Epoch [99/120    avg_loss:0.007, val_acc:0.978]
Epoch [100/120    avg_loss:0.008, val_acc:0.978]
Epoch [101/120    avg_loss:0.007, val_acc:0.978]
Epoch [102/120    avg_loss:0.009, val_acc:0.978]
Epoch [103/120    avg_loss:0.007, val_acc:0.979]
Epoch [104/120    avg_loss:0.011, val_acc:0.980]
Epoch [105/120    avg_loss:0.009, val_acc:0.978]
Epoch [106/120    avg_loss:0.008, val_acc:0.978]
Epoch [107/120    avg_loss:0.007, val_acc:0.978]
Epoch [108/120    avg_loss:0.015, val_acc:0.979]
Epoch [109/120    avg_loss:0.006, val_acc:0.979]
Epoch [110/120    avg_loss:0.009, val_acc:0.978]
Epoch [111/120    avg_loss:0.007, val_acc:0.978]
Epoch [112/120    avg_loss:0.006, val_acc:0.978]
Epoch [113/120    avg_loss:0.009, val_acc:0.979]
Epoch [114/120    avg_loss:0.006, val_acc:0.978]
Epoch [115/120    avg_loss:0.005, val_acc:0.979]
Epoch [116/120    avg_loss:0.006, val_acc:0.979]
Epoch [117/120    avg_loss:0.005, val_acc:0.978]
Epoch [118/120    avg_loss:0.006, val_acc:0.978]
Epoch [119/120    avg_loss:0.006, val_acc:0.978]
Epoch [120/120    avg_loss:0.009, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1275    3    0    0    0    0    0    0    4    2    0    0
     0    1    0]
 [   0    0    0  715    0   10    0    0    0    4    0    0   14    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    5    0    0    0    0  855    6    3    0
     0    0    0]
 [   0    0   20    0    0    0    2    0    0    0    4 2182    1    1
     0    0    0]
 [   0    0    0    0    1    0    0    0    0    0    1    0  530    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    40  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.39566395663957

F1 scores:
[       nan 1.         0.98607889 0.97610922 0.99765808 0.9784336
 0.99244713 0.98039216 1.         0.8372093  0.98275862 0.99181818
 0.9787627  0.98666667 0.9823047  0.92426584 0.98224852]

Kappa:
0.9817103872983898
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f221dd6b5f8>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.309, val_acc:0.401]
Epoch [2/120    avg_loss:1.722, val_acc:0.560]
Epoch [3/120    avg_loss:1.603, val_acc:0.658]
Epoch [4/120    avg_loss:1.338, val_acc:0.689]
Epoch [5/120    avg_loss:0.889, val_acc:0.784]
Epoch [6/120    avg_loss:0.773, val_acc:0.741]
Epoch [7/120    avg_loss:0.640, val_acc:0.823]
Epoch [8/120    avg_loss:0.533, val_acc:0.788]
Epoch [9/120    avg_loss:0.611, val_acc:0.804]
Epoch [10/120    avg_loss:0.483, val_acc:0.856]
Epoch [11/120    avg_loss:0.432, val_acc:0.842]
Epoch [12/120    avg_loss:0.418, val_acc:0.849]
Epoch [13/120    avg_loss:0.336, val_acc:0.871]
Epoch [14/120    avg_loss:0.297, val_acc:0.887]
Epoch [15/120    avg_loss:0.279, val_acc:0.922]
Epoch [16/120    avg_loss:0.242, val_acc:0.899]
Epoch [17/120    avg_loss:0.200, val_acc:0.894]
Epoch [18/120    avg_loss:0.206, val_acc:0.908]
Epoch [19/120    avg_loss:0.186, val_acc:0.907]
Epoch [20/120    avg_loss:0.158, val_acc:0.900]
Epoch [21/120    avg_loss:0.160, val_acc:0.917]
Epoch [22/120    avg_loss:0.206, val_acc:0.928]
Epoch [23/120    avg_loss:0.144, val_acc:0.931]
Epoch [24/120    avg_loss:0.118, val_acc:0.927]
Epoch [25/120    avg_loss:0.098, val_acc:0.943]
Epoch [26/120    avg_loss:0.149, val_acc:0.924]
Epoch [27/120    avg_loss:0.165, val_acc:0.918]
Epoch [28/120    avg_loss:0.115, val_acc:0.930]
Epoch [29/120    avg_loss:0.083, val_acc:0.952]
Epoch [30/120    avg_loss:0.092, val_acc:0.930]
Epoch [31/120    avg_loss:0.113, val_acc:0.948]
Epoch [32/120    avg_loss:0.079, val_acc:0.959]
Epoch [33/120    avg_loss:0.103, val_acc:0.924]
Epoch [34/120    avg_loss:0.120, val_acc:0.956]
Epoch [35/120    avg_loss:0.080, val_acc:0.961]
Epoch [36/120    avg_loss:0.045, val_acc:0.968]
Epoch [37/120    avg_loss:0.052, val_acc:0.965]
Epoch [38/120    avg_loss:0.071, val_acc:0.944]
Epoch [39/120    avg_loss:0.062, val_acc:0.967]
Epoch [40/120    avg_loss:0.050, val_acc:0.966]
Epoch [41/120    avg_loss:0.075, val_acc:0.961]
Epoch [42/120    avg_loss:0.058, val_acc:0.956]
Epoch [43/120    avg_loss:0.042, val_acc:0.962]
Epoch [44/120    avg_loss:0.034, val_acc:0.964]
Epoch [45/120    avg_loss:0.027, val_acc:0.968]
Epoch [46/120    avg_loss:0.053, val_acc:0.958]
Epoch [47/120    avg_loss:0.082, val_acc:0.952]
Epoch [48/120    avg_loss:0.063, val_acc:0.958]
Epoch [49/120    avg_loss:0.054, val_acc:0.964]
Epoch [50/120    avg_loss:0.046, val_acc:0.969]
Epoch [51/120    avg_loss:0.034, val_acc:0.962]
Epoch [52/120    avg_loss:0.045, val_acc:0.976]
Epoch [53/120    avg_loss:0.028, val_acc:0.974]
Epoch [54/120    avg_loss:0.026, val_acc:0.974]
Epoch [55/120    avg_loss:0.033, val_acc:0.974]
Epoch [56/120    avg_loss:0.029, val_acc:0.971]
Epoch [57/120    avg_loss:0.045, val_acc:0.978]
Epoch [58/120    avg_loss:0.024, val_acc:0.978]
Epoch [59/120    avg_loss:0.022, val_acc:0.985]
Epoch [60/120    avg_loss:0.023, val_acc:0.984]
Epoch [61/120    avg_loss:0.023, val_acc:0.979]
Epoch [62/120    avg_loss:0.050, val_acc:0.968]
Epoch [63/120    avg_loss:0.122, val_acc:0.959]
Epoch [64/120    avg_loss:0.071, val_acc:0.953]
Epoch [65/120    avg_loss:0.048, val_acc:0.973]
Epoch [66/120    avg_loss:0.039, val_acc:0.979]
Epoch [67/120    avg_loss:0.022, val_acc:0.962]
Epoch [68/120    avg_loss:0.038, val_acc:0.952]
Epoch [69/120    avg_loss:0.062, val_acc:0.965]
Epoch [70/120    avg_loss:0.048, val_acc:0.968]
Epoch [71/120    avg_loss:0.038, val_acc:0.973]
Epoch [72/120    avg_loss:0.023, val_acc:0.979]
Epoch [73/120    avg_loss:0.022, val_acc:0.979]
Epoch [74/120    avg_loss:0.012, val_acc:0.979]
Epoch [75/120    avg_loss:0.014, val_acc:0.979]
Epoch [76/120    avg_loss:0.011, val_acc:0.980]
Epoch [77/120    avg_loss:0.015, val_acc:0.980]
Epoch [78/120    avg_loss:0.011, val_acc:0.980]
Epoch [79/120    avg_loss:0.014, val_acc:0.980]
Epoch [80/120    avg_loss:0.012, val_acc:0.982]
Epoch [81/120    avg_loss:0.011, val_acc:0.982]
Epoch [82/120    avg_loss:0.012, val_acc:0.981]
Epoch [83/120    avg_loss:0.012, val_acc:0.981]
Epoch [84/120    avg_loss:0.011, val_acc:0.980]
Epoch [85/120    avg_loss:0.009, val_acc:0.981]
Epoch [86/120    avg_loss:0.010, val_acc:0.981]
Epoch [87/120    avg_loss:0.009, val_acc:0.981]
Epoch [88/120    avg_loss:0.009, val_acc:0.981]
Epoch [89/120    avg_loss:0.014, val_acc:0.982]
Epoch [90/120    avg_loss:0.009, val_acc:0.982]
Epoch [91/120    avg_loss:0.010, val_acc:0.982]
Epoch [92/120    avg_loss:0.014, val_acc:0.982]
Epoch [93/120    avg_loss:0.015, val_acc:0.982]
Epoch [94/120    avg_loss:0.014, val_acc:0.982]
Epoch [95/120    avg_loss:0.009, val_acc:0.982]
Epoch [96/120    avg_loss:0.006, val_acc:0.982]
Epoch [97/120    avg_loss:0.009, val_acc:0.982]
Epoch [98/120    avg_loss:0.011, val_acc:0.982]
Epoch [99/120    avg_loss:0.011, val_acc:0.982]
Epoch [100/120    avg_loss:0.013, val_acc:0.982]
Epoch [101/120    avg_loss:0.008, val_acc:0.982]
Epoch [102/120    avg_loss:0.011, val_acc:0.982]
Epoch [103/120    avg_loss:0.012, val_acc:0.982]
Epoch [104/120    avg_loss:0.010, val_acc:0.982]
Epoch [105/120    avg_loss:0.014, val_acc:0.982]
Epoch [106/120    avg_loss:0.012, val_acc:0.982]
Epoch [107/120    avg_loss:0.013, val_acc:0.982]
Epoch [108/120    avg_loss:0.006, val_acc:0.982]
Epoch [109/120    avg_loss:0.009, val_acc:0.982]
Epoch [110/120    avg_loss:0.010, val_acc:0.982]
Epoch [111/120    avg_loss:0.009, val_acc:0.982]
Epoch [112/120    avg_loss:0.009, val_acc:0.982]
Epoch [113/120    avg_loss:0.008, val_acc:0.982]
Epoch [114/120    avg_loss:0.008, val_acc:0.982]
Epoch [115/120    avg_loss:0.009, val_acc:0.982]
Epoch [116/120    avg_loss:0.009, val_acc:0.982]
Epoch [117/120    avg_loss:0.018, val_acc:0.982]
Epoch [118/120    avg_loss:0.011, val_acc:0.982]
Epoch [119/120    avg_loss:0.007, val_acc:0.982]
Epoch [120/120    avg_loss:0.012, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1264    5    0    1    0    0    0    0    8    5    0    0
     0    2    0]
 [   0    0    0  696    0   24    0    0    0    3    1    0   20    3
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    7    0    0    0    0  825   27   11    0
     2    0    0]
 [   0    0    7    0    0    1    3    0    0    0    4 2194    0    1
     0    0    0]
 [   0    0    0    6    7    0    0    0    0    0    0    7  510    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    1    0    1    0    0    0
  1127    0    0]
 [   0    0    0    0    0    0   21    0    0    0    0    0    0    0
    46  280    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.30081300813008

F1 scores:
[       nan 0.98765432 0.9875     0.95604396 0.98148148 0.95060373
 0.98053892 1.         0.99883856 0.85       0.96266044 0.9871766
 0.94795539 0.98930481 0.97365011 0.89030207 0.97647059]

Kappa:
0.9692097773348252
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1be62556d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.315, val_acc:0.457]
Epoch [2/120    avg_loss:1.550, val_acc:0.640]
Epoch [3/120    avg_loss:1.494, val_acc:0.641]
Epoch [4/120    avg_loss:1.196, val_acc:0.732]
Epoch [5/120    avg_loss:0.964, val_acc:0.786]
Epoch [6/120    avg_loss:0.856, val_acc:0.805]
Epoch [7/120    avg_loss:0.704, val_acc:0.732]
Epoch [8/120    avg_loss:0.694, val_acc:0.809]
Epoch [9/120    avg_loss:0.589, val_acc:0.774]
Epoch [10/120    avg_loss:0.443, val_acc:0.859]
Epoch [11/120    avg_loss:0.478, val_acc:0.853]
Epoch [12/120    avg_loss:0.364, val_acc:0.797]
Epoch [13/120    avg_loss:0.338, val_acc:0.880]
Epoch [14/120    avg_loss:0.299, val_acc:0.882]
Epoch [15/120    avg_loss:0.256, val_acc:0.865]
Epoch [16/120    avg_loss:0.315, val_acc:0.900]
Epoch [17/120    avg_loss:0.205, val_acc:0.909]
Epoch [18/120    avg_loss:0.192, val_acc:0.899]
Epoch [19/120    avg_loss:0.172, val_acc:0.912]
Epoch [20/120    avg_loss:0.157, val_acc:0.900]
Epoch [21/120    avg_loss:0.165, val_acc:0.928]
Epoch [22/120    avg_loss:0.095, val_acc:0.936]
Epoch [23/120    avg_loss:0.130, val_acc:0.914]
Epoch [24/120    avg_loss:0.134, val_acc:0.933]
Epoch [25/120    avg_loss:0.136, val_acc:0.913]
Epoch [26/120    avg_loss:0.132, val_acc:0.933]
Epoch [27/120    avg_loss:0.113, val_acc:0.931]
Epoch [28/120    avg_loss:0.099, val_acc:0.935]
Epoch [29/120    avg_loss:0.100, val_acc:0.934]
Epoch [30/120    avg_loss:0.102, val_acc:0.947]
Epoch [31/120    avg_loss:0.086, val_acc:0.918]
Epoch [32/120    avg_loss:0.204, val_acc:0.877]
Epoch [33/120    avg_loss:0.198, val_acc:0.942]
Epoch [34/120    avg_loss:0.087, val_acc:0.942]
Epoch [35/120    avg_loss:0.063, val_acc:0.956]
Epoch [36/120    avg_loss:0.095, val_acc:0.945]
Epoch [37/120    avg_loss:0.091, val_acc:0.952]
Epoch [38/120    avg_loss:0.094, val_acc:0.956]
Epoch [39/120    avg_loss:0.074, val_acc:0.964]
Epoch [40/120    avg_loss:0.052, val_acc:0.959]
Epoch [41/120    avg_loss:0.047, val_acc:0.964]
Epoch [42/120    avg_loss:0.033, val_acc:0.964]
Epoch [43/120    avg_loss:0.052, val_acc:0.966]
Epoch [44/120    avg_loss:0.059, val_acc:0.967]
Epoch [45/120    avg_loss:0.061, val_acc:0.946]
Epoch [46/120    avg_loss:0.073, val_acc:0.968]
Epoch [47/120    avg_loss:0.071, val_acc:0.965]
Epoch [48/120    avg_loss:0.043, val_acc:0.976]
Epoch [49/120    avg_loss:0.031, val_acc:0.975]
Epoch [50/120    avg_loss:0.085, val_acc:0.966]
Epoch [51/120    avg_loss:0.055, val_acc:0.959]
Epoch [52/120    avg_loss:0.029, val_acc:0.968]
Epoch [53/120    avg_loss:0.055, val_acc:0.969]
Epoch [54/120    avg_loss:0.041, val_acc:0.967]
Epoch [55/120    avg_loss:0.033, val_acc:0.964]
Epoch [56/120    avg_loss:0.037, val_acc:0.961]
Epoch [57/120    avg_loss:0.046, val_acc:0.966]
Epoch [58/120    avg_loss:0.048, val_acc:0.971]
Epoch [59/120    avg_loss:0.025, val_acc:0.974]
Epoch [60/120    avg_loss:0.035, val_acc:0.964]
Epoch [61/120    avg_loss:0.046, val_acc:0.971]
Epoch [62/120    avg_loss:0.016, val_acc:0.979]
Epoch [63/120    avg_loss:0.021, val_acc:0.982]
Epoch [64/120    avg_loss:0.013, val_acc:0.984]
Epoch [65/120    avg_loss:0.015, val_acc:0.984]
Epoch [66/120    avg_loss:0.026, val_acc:0.984]
Epoch [67/120    avg_loss:0.012, val_acc:0.984]
Epoch [68/120    avg_loss:0.014, val_acc:0.984]
Epoch [69/120    avg_loss:0.015, val_acc:0.986]
Epoch [70/120    avg_loss:0.010, val_acc:0.986]
Epoch [71/120    avg_loss:0.014, val_acc:0.982]
Epoch [72/120    avg_loss:0.012, val_acc:0.986]
Epoch [73/120    avg_loss:0.011, val_acc:0.986]
Epoch [74/120    avg_loss:0.016, val_acc:0.987]
Epoch [75/120    avg_loss:0.013, val_acc:0.987]
Epoch [76/120    avg_loss:0.008, val_acc:0.987]
Epoch [77/120    avg_loss:0.010, val_acc:0.985]
Epoch [78/120    avg_loss:0.008, val_acc:0.987]
Epoch [79/120    avg_loss:0.019, val_acc:0.985]
Epoch [80/120    avg_loss:0.011, val_acc:0.985]
Epoch [81/120    avg_loss:0.008, val_acc:0.985]
Epoch [82/120    avg_loss:0.012, val_acc:0.984]
Epoch [83/120    avg_loss:0.010, val_acc:0.984]
Epoch [84/120    avg_loss:0.011, val_acc:0.986]
Epoch [85/120    avg_loss:0.011, val_acc:0.986]
Epoch [86/120    avg_loss:0.010, val_acc:0.986]
Epoch [87/120    avg_loss:0.011, val_acc:0.986]
Epoch [88/120    avg_loss:0.007, val_acc:0.984]
Epoch [89/120    avg_loss:0.009, val_acc:0.984]
Epoch [90/120    avg_loss:0.012, val_acc:0.984]
Epoch [91/120    avg_loss:0.009, val_acc:0.985]
Epoch [92/120    avg_loss:0.009, val_acc:0.985]
Epoch [93/120    avg_loss:0.011, val_acc:0.985]
Epoch [94/120    avg_loss:0.009, val_acc:0.985]
Epoch [95/120    avg_loss:0.013, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.985]
Epoch [97/120    avg_loss:0.012, val_acc:0.985]
Epoch [98/120    avg_loss:0.009, val_acc:0.985]
Epoch [99/120    avg_loss:0.011, val_acc:0.985]
Epoch [100/120    avg_loss:0.010, val_acc:0.985]
Epoch [101/120    avg_loss:0.007, val_acc:0.985]
Epoch [102/120    avg_loss:0.015, val_acc:0.985]
Epoch [103/120    avg_loss:0.013, val_acc:0.985]
Epoch [104/120    avg_loss:0.009, val_acc:0.985]
Epoch [105/120    avg_loss:0.011, val_acc:0.985]
Epoch [106/120    avg_loss:0.007, val_acc:0.985]
Epoch [107/120    avg_loss:0.010, val_acc:0.985]
Epoch [108/120    avg_loss:0.009, val_acc:0.985]
Epoch [109/120    avg_loss:0.012, val_acc:0.985]
Epoch [110/120    avg_loss:0.009, val_acc:0.985]
Epoch [111/120    avg_loss:0.009, val_acc:0.985]
Epoch [112/120    avg_loss:0.009, val_acc:0.985]
Epoch [113/120    avg_loss:0.010, val_acc:0.985]
Epoch [114/120    avg_loss:0.010, val_acc:0.985]
Epoch [115/120    avg_loss:0.007, val_acc:0.985]
Epoch [116/120    avg_loss:0.011, val_acc:0.985]
Epoch [117/120    avg_loss:0.013, val_acc:0.985]
Epoch [118/120    avg_loss:0.009, val_acc:0.985]
Epoch [119/120    avg_loss:0.014, val_acc:0.985]
Epoch [120/120    avg_loss:0.012, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    0    0    0    0    0    0    0    6    3    4    0
     0    5    0]
 [   0    0    0  695    0   32    0    0    0    5    2    0   13    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0  849   20    0    0
     0    3    0]
 [   0    0    8    0    0    0    8    0    0    0    7 2185    0    2
     0    0    0]
 [   0    0    0   13    8    5    0    0    0    0    5    4  492    0
     4    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   11    0    0    1    0    1    0    0    0
  1126    0    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    33  304    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.5609756097561

F1 scores:
[       nan 1.         0.98984375 0.95467033 0.97921478 0.94232862
 0.98648649 1.         0.99883856 0.8372093  0.9730659  0.98824062
 0.94072658 0.99462366 0.97827976 0.92261002 0.96428571]

Kappa:
0.9721937232306185
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb9e73a5748>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.295, val_acc:0.502]
Epoch [2/120    avg_loss:1.626, val_acc:0.569]
Epoch [3/120    avg_loss:1.306, val_acc:0.646]
Epoch [4/120    avg_loss:1.309, val_acc:0.686]
Epoch [5/120    avg_loss:1.069, val_acc:0.693]
Epoch [6/120    avg_loss:0.899, val_acc:0.726]
Epoch [7/120    avg_loss:0.687, val_acc:0.793]
Epoch [8/120    avg_loss:0.596, val_acc:0.772]
Epoch [9/120    avg_loss:0.571, val_acc:0.842]
Epoch [10/120    avg_loss:0.441, val_acc:0.849]
Epoch [11/120    avg_loss:0.587, val_acc:0.800]
Epoch [12/120    avg_loss:0.589, val_acc:0.855]
Epoch [13/120    avg_loss:0.359, val_acc:0.814]
Epoch [14/120    avg_loss:0.310, val_acc:0.833]
Epoch [15/120    avg_loss:0.406, val_acc:0.827]
Epoch [16/120    avg_loss:0.304, val_acc:0.873]
Epoch [17/120    avg_loss:0.253, val_acc:0.923]
Epoch [18/120    avg_loss:0.241, val_acc:0.886]
Epoch [19/120    avg_loss:0.410, val_acc:0.890]
Epoch [20/120    avg_loss:0.239, val_acc:0.908]
Epoch [21/120    avg_loss:0.168, val_acc:0.920]
Epoch [22/120    avg_loss:0.129, val_acc:0.935]
Epoch [23/120    avg_loss:0.122, val_acc:0.902]
Epoch [24/120    avg_loss:0.144, val_acc:0.927]
Epoch [25/120    avg_loss:0.128, val_acc:0.940]
Epoch [26/120    avg_loss:0.119, val_acc:0.943]
Epoch [27/120    avg_loss:0.099, val_acc:0.938]
Epoch [28/120    avg_loss:0.101, val_acc:0.925]
Epoch [29/120    avg_loss:0.138, val_acc:0.929]
Epoch [30/120    avg_loss:0.124, val_acc:0.942]
Epoch [31/120    avg_loss:0.132, val_acc:0.929]
Epoch [32/120    avg_loss:0.128, val_acc:0.925]
Epoch [33/120    avg_loss:0.176, val_acc:0.920]
Epoch [34/120    avg_loss:0.107, val_acc:0.955]
Epoch [35/120    avg_loss:0.069, val_acc:0.962]
Epoch [36/120    avg_loss:0.054, val_acc:0.959]
Epoch [37/120    avg_loss:0.061, val_acc:0.958]
Epoch [38/120    avg_loss:0.084, val_acc:0.941]
Epoch [39/120    avg_loss:0.070, val_acc:0.921]
Epoch [40/120    avg_loss:0.113, val_acc:0.954]
Epoch [41/120    avg_loss:0.179, val_acc:0.956]
Epoch [42/120    avg_loss:0.073, val_acc:0.958]
Epoch [43/120    avg_loss:0.063, val_acc:0.948]
Epoch [44/120    avg_loss:0.073, val_acc:0.962]
Epoch [45/120    avg_loss:0.053, val_acc:0.916]
Epoch [46/120    avg_loss:0.049, val_acc:0.964]
Epoch [47/120    avg_loss:0.030, val_acc:0.970]
Epoch [48/120    avg_loss:0.065, val_acc:0.933]
Epoch [49/120    avg_loss:0.068, val_acc:0.965]
Epoch [50/120    avg_loss:0.036, val_acc:0.965]
Epoch [51/120    avg_loss:0.036, val_acc:0.974]
Epoch [52/120    avg_loss:0.026, val_acc:0.965]
Epoch [53/120    avg_loss:0.024, val_acc:0.966]
Epoch [54/120    avg_loss:0.029, val_acc:0.976]
Epoch [55/120    avg_loss:0.017, val_acc:0.975]
Epoch [56/120    avg_loss:0.022, val_acc:0.965]
Epoch [57/120    avg_loss:0.031, val_acc:0.969]
Epoch [58/120    avg_loss:0.021, val_acc:0.968]
Epoch [59/120    avg_loss:0.019, val_acc:0.978]
Epoch [60/120    avg_loss:0.027, val_acc:0.971]
Epoch [61/120    avg_loss:0.023, val_acc:0.977]
Epoch [62/120    avg_loss:0.016, val_acc:0.971]
Epoch [63/120    avg_loss:0.041, val_acc:0.958]
Epoch [64/120    avg_loss:0.045, val_acc:0.974]
Epoch [65/120    avg_loss:0.040, val_acc:0.968]
Epoch [66/120    avg_loss:0.022, val_acc:0.970]
Epoch [67/120    avg_loss:0.017, val_acc:0.978]
Epoch [68/120    avg_loss:0.016, val_acc:0.973]
Epoch [69/120    avg_loss:0.027, val_acc:0.974]
Epoch [70/120    avg_loss:0.040, val_acc:0.974]
Epoch [71/120    avg_loss:0.022, val_acc:0.978]
Epoch [72/120    avg_loss:0.021, val_acc:0.979]
Epoch [73/120    avg_loss:0.020, val_acc:0.982]
Epoch [74/120    avg_loss:0.012, val_acc:0.976]
Epoch [75/120    avg_loss:0.024, val_acc:0.979]
Epoch [76/120    avg_loss:0.024, val_acc:0.982]
Epoch [77/120    avg_loss:0.047, val_acc:0.965]
Epoch [78/120    avg_loss:0.043, val_acc:0.974]
Epoch [79/120    avg_loss:0.049, val_acc:0.969]
Epoch [80/120    avg_loss:0.045, val_acc:0.959]
Epoch [81/120    avg_loss:0.062, val_acc:0.977]
Epoch [82/120    avg_loss:0.026, val_acc:0.955]
Epoch [83/120    avg_loss:0.032, val_acc:0.978]
Epoch [84/120    avg_loss:0.017, val_acc:0.981]
Epoch [85/120    avg_loss:0.011, val_acc:0.981]
Epoch [86/120    avg_loss:0.019, val_acc:0.982]
Epoch [87/120    avg_loss:0.034, val_acc:0.978]
Epoch [88/120    avg_loss:0.036, val_acc:0.974]
Epoch [89/120    avg_loss:0.011, val_acc:0.977]
Epoch [90/120    avg_loss:0.017, val_acc:0.961]
Epoch [91/120    avg_loss:0.022, val_acc:0.975]
Epoch [92/120    avg_loss:0.011, val_acc:0.975]
Epoch [93/120    avg_loss:0.017, val_acc:0.978]
Epoch [94/120    avg_loss:0.017, val_acc:0.979]
Epoch [95/120    avg_loss:0.010, val_acc:0.979]
Epoch [96/120    avg_loss:0.011, val_acc:0.976]
Epoch [97/120    avg_loss:0.012, val_acc:0.979]
Epoch [98/120    avg_loss:0.011, val_acc:0.973]
Epoch [99/120    avg_loss:0.027, val_acc:0.974]
Epoch [100/120    avg_loss:0.009, val_acc:0.975]
Epoch [101/120    avg_loss:0.010, val_acc:0.976]
Epoch [102/120    avg_loss:0.012, val_acc:0.980]
Epoch [103/120    avg_loss:0.005, val_acc:0.978]
Epoch [104/120    avg_loss:0.006, val_acc:0.979]
Epoch [105/120    avg_loss:0.005, val_acc:0.978]
Epoch [106/120    avg_loss:0.006, val_acc:0.979]
Epoch [107/120    avg_loss:0.008, val_acc:0.979]
Epoch [108/120    avg_loss:0.005, val_acc:0.979]
Epoch [109/120    avg_loss:0.006, val_acc:0.979]
Epoch [110/120    avg_loss:0.004, val_acc:0.980]
Epoch [111/120    avg_loss:0.005, val_acc:0.980]
Epoch [112/120    avg_loss:0.007, val_acc:0.980]
Epoch [113/120    avg_loss:0.005, val_acc:0.980]
Epoch [114/120    avg_loss:0.007, val_acc:0.981]
Epoch [115/120    avg_loss:0.005, val_acc:0.981]
Epoch [116/120    avg_loss:0.006, val_acc:0.981]
Epoch [117/120    avg_loss:0.005, val_acc:0.981]
Epoch [118/120    avg_loss:0.006, val_acc:0.981]
Epoch [119/120    avg_loss:0.006, val_acc:0.981]
Epoch [120/120    avg_loss:0.005, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1260    0    0    0    0    0    0    0   14    4    4    0
     0    3    0]
 [   0    0    0  716    0   12    0    0    0    4    0    0   15    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    5    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    3    2    0    0    0  862    2    0    0
     0    1    0]
 [   0    0    8    0    0    0    5    0    0    0    3 2193    0    1
     0    0    0]
 [   0    0    0   21    8    4    0    0    0    0    4    0  492    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   26    0    0    0    0    0    0    0
    28  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.94037940379404

F1 scores:
[       nan 1.         0.98707403 0.96107383 0.98156682 0.97168743
 0.97550111 0.98039216 1.         0.77272727 0.98065984 0.9947834
 0.94072658 0.99730458 0.98785776 0.90993789 0.96511628]

Kappa:
0.9765214807186469
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2682dce748>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.259, val_acc:0.544]
Epoch [2/120    avg_loss:1.669, val_acc:0.605]
Epoch [3/120    avg_loss:1.293, val_acc:0.641]
Epoch [4/120    avg_loss:1.332, val_acc:0.666]
Epoch [5/120    avg_loss:0.986, val_acc:0.746]
Epoch [6/120    avg_loss:0.851, val_acc:0.728]
Epoch [7/120    avg_loss:0.803, val_acc:0.803]
Epoch [8/120    avg_loss:0.744, val_acc:0.794]
Epoch [9/120    avg_loss:0.592, val_acc:0.817]
Epoch [10/120    avg_loss:0.477, val_acc:0.814]
Epoch [11/120    avg_loss:0.520, val_acc:0.860]
Epoch [12/120    avg_loss:0.337, val_acc:0.874]
Epoch [13/120    avg_loss:0.290, val_acc:0.885]
Epoch [14/120    avg_loss:0.283, val_acc:0.887]
Epoch [15/120    avg_loss:0.245, val_acc:0.883]
Epoch [16/120    avg_loss:0.329, val_acc:0.889]
Epoch [17/120    avg_loss:0.306, val_acc:0.901]
Epoch [18/120    avg_loss:0.227, val_acc:0.912]
Epoch [19/120    avg_loss:0.158, val_acc:0.936]
Epoch [20/120    avg_loss:0.184, val_acc:0.896]
Epoch [21/120    avg_loss:0.209, val_acc:0.899]
Epoch [22/120    avg_loss:0.173, val_acc:0.912]
Epoch [23/120    avg_loss:0.172, val_acc:0.920]
Epoch [24/120    avg_loss:0.127, val_acc:0.920]
Epoch [25/120    avg_loss:0.119, val_acc:0.928]
Epoch [26/120    avg_loss:0.132, val_acc:0.922]
Epoch [27/120    avg_loss:0.158, val_acc:0.913]
Epoch [28/120    avg_loss:0.110, val_acc:0.936]
Epoch [29/120    avg_loss:0.091, val_acc:0.934]
Epoch [30/120    avg_loss:0.094, val_acc:0.929]
Epoch [31/120    avg_loss:0.117, val_acc:0.940]
Epoch [32/120    avg_loss:0.073, val_acc:0.953]
Epoch [33/120    avg_loss:0.060, val_acc:0.950]
Epoch [34/120    avg_loss:0.091, val_acc:0.942]
Epoch [35/120    avg_loss:0.104, val_acc:0.935]
Epoch [36/120    avg_loss:0.094, val_acc:0.950]
Epoch [37/120    avg_loss:0.064, val_acc:0.947]
Epoch [38/120    avg_loss:0.051, val_acc:0.958]
Epoch [39/120    avg_loss:0.052, val_acc:0.956]
Epoch [40/120    avg_loss:0.045, val_acc:0.959]
Epoch [41/120    avg_loss:0.036, val_acc:0.963]
Epoch [42/120    avg_loss:0.039, val_acc:0.965]
Epoch [43/120    avg_loss:0.061, val_acc:0.941]
Epoch [44/120    avg_loss:0.067, val_acc:0.952]
Epoch [45/120    avg_loss:0.064, val_acc:0.953]
Epoch [46/120    avg_loss:0.081, val_acc:0.950]
Epoch [47/120    avg_loss:0.077, val_acc:0.967]
Epoch [48/120    avg_loss:0.023, val_acc:0.964]
Epoch [49/120    avg_loss:0.037, val_acc:0.938]
Epoch [50/120    avg_loss:0.037, val_acc:0.955]
Epoch [51/120    avg_loss:0.039, val_acc:0.962]
Epoch [52/120    avg_loss:0.023, val_acc:0.961]
Epoch [53/120    avg_loss:0.032, val_acc:0.966]
Epoch [54/120    avg_loss:0.038, val_acc:0.969]
Epoch [55/120    avg_loss:0.029, val_acc:0.971]
Epoch [56/120    avg_loss:0.030, val_acc:0.944]
Epoch [57/120    avg_loss:0.035, val_acc:0.967]
Epoch [58/120    avg_loss:0.026, val_acc:0.959]
Epoch [59/120    avg_loss:0.020, val_acc:0.973]
Epoch [60/120    avg_loss:0.023, val_acc:0.961]
Epoch [61/120    avg_loss:0.025, val_acc:0.961]
Epoch [62/120    avg_loss:0.020, val_acc:0.963]
Epoch [63/120    avg_loss:0.024, val_acc:0.964]
Epoch [64/120    avg_loss:0.175, val_acc:0.932]
Epoch [65/120    avg_loss:0.664, val_acc:0.782]
Epoch [66/120    avg_loss:0.364, val_acc:0.923]
Epoch [67/120    avg_loss:0.182, val_acc:0.874]
Epoch [68/120    avg_loss:0.120, val_acc:0.943]
Epoch [69/120    avg_loss:0.083, val_acc:0.943]
Epoch [70/120    avg_loss:0.048, val_acc:0.958]
Epoch [71/120    avg_loss:0.040, val_acc:0.967]
Epoch [72/120    avg_loss:0.070, val_acc:0.956]
Epoch [73/120    avg_loss:0.032, val_acc:0.963]
Epoch [74/120    avg_loss:0.034, val_acc:0.966]
Epoch [75/120    avg_loss:0.035, val_acc:0.969]
Epoch [76/120    avg_loss:0.022, val_acc:0.970]
Epoch [77/120    avg_loss:0.019, val_acc:0.973]
Epoch [78/120    avg_loss:0.019, val_acc:0.973]
Epoch [79/120    avg_loss:0.023, val_acc:0.975]
Epoch [80/120    avg_loss:0.024, val_acc:0.973]
Epoch [81/120    avg_loss:0.017, val_acc:0.973]
Epoch [82/120    avg_loss:0.014, val_acc:0.974]
Epoch [83/120    avg_loss:0.022, val_acc:0.971]
Epoch [84/120    avg_loss:0.028, val_acc:0.973]
Epoch [85/120    avg_loss:0.019, val_acc:0.975]
Epoch [86/120    avg_loss:0.015, val_acc:0.973]
Epoch [87/120    avg_loss:0.014, val_acc:0.973]
Epoch [88/120    avg_loss:0.019, val_acc:0.974]
Epoch [89/120    avg_loss:0.018, val_acc:0.974]
Epoch [90/120    avg_loss:0.023, val_acc:0.975]
Epoch [91/120    avg_loss:0.017, val_acc:0.976]
Epoch [92/120    avg_loss:0.017, val_acc:0.976]
Epoch [93/120    avg_loss:0.013, val_acc:0.977]
Epoch [94/120    avg_loss:0.015, val_acc:0.976]
Epoch [95/120    avg_loss:0.013, val_acc:0.975]
Epoch [96/120    avg_loss:0.014, val_acc:0.977]
Epoch [97/120    avg_loss:0.019, val_acc:0.975]
Epoch [98/120    avg_loss:0.014, val_acc:0.977]
Epoch [99/120    avg_loss:0.013, val_acc:0.975]
Epoch [100/120    avg_loss:0.014, val_acc:0.976]
Epoch [101/120    avg_loss:0.017, val_acc:0.977]
Epoch [102/120    avg_loss:0.014, val_acc:0.976]
Epoch [103/120    avg_loss:0.012, val_acc:0.977]
Epoch [104/120    avg_loss:0.016, val_acc:0.976]
Epoch [105/120    avg_loss:0.017, val_acc:0.978]
Epoch [106/120    avg_loss:0.008, val_acc:0.978]
Epoch [107/120    avg_loss:0.012, val_acc:0.976]
Epoch [108/120    avg_loss:0.019, val_acc:0.976]
Epoch [109/120    avg_loss:0.017, val_acc:0.975]
Epoch [110/120    avg_loss:0.012, val_acc:0.977]
Epoch [111/120    avg_loss:0.017, val_acc:0.977]
Epoch [112/120    avg_loss:0.013, val_acc:0.976]
Epoch [113/120    avg_loss:0.012, val_acc:0.976]
Epoch [114/120    avg_loss:0.017, val_acc:0.976]
Epoch [115/120    avg_loss:0.013, val_acc:0.976]
Epoch [116/120    avg_loss:0.013, val_acc:0.976]
Epoch [117/120    avg_loss:0.011, val_acc:0.977]
Epoch [118/120    avg_loss:0.010, val_acc:0.976]
Epoch [119/120    avg_loss:0.013, val_acc:0.976]
Epoch [120/120    avg_loss:0.018, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1276    0    0    0    0    0    0    0    4    1    4    0
     0    0    0]
 [   0    0    0  699    0   28    0    0    0    6    2    0    9    3
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    4    3    0    0    0  855    5    3    0
     0    0    0]
 [   0    0    9    0    0    0    4    0    0    0    5 2191    0    1
     0    0    0]
 [   0    0    0    3   14    0    0    0    0    0    1    0  514    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   25    0    0    2    0    2    0    0    0
  1110    0    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
    27  303    0]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0    0
     0    0   83]]

Accuracy:
97.87533875338754

F1 scores:
[       nan 1.         0.99106796 0.96214728 0.96583144 0.93621622
 0.98206278 0.98039216 0.99767981 0.75       0.98050459 0.99410163
 0.96616541 0.98930481 0.97539543 0.93230769 0.98224852]

Kappa:
0.9757871217564689
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdcaa785710>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.170, val_acc:0.422]
Epoch [2/120    avg_loss:1.458, val_acc:0.641]
Epoch [3/120    avg_loss:1.204, val_acc:0.634]
Epoch [4/120    avg_loss:1.180, val_acc:0.682]
Epoch [5/120    avg_loss:0.824, val_acc:0.713]
Epoch [6/120    avg_loss:0.660, val_acc:0.815]
Epoch [7/120    avg_loss:0.511, val_acc:0.843]
Epoch [8/120    avg_loss:0.624, val_acc:0.842]
Epoch [9/120    avg_loss:0.684, val_acc:0.729]
Epoch [10/120    avg_loss:0.456, val_acc:0.848]
Epoch [11/120    avg_loss:0.437, val_acc:0.848]
Epoch [12/120    avg_loss:0.560, val_acc:0.819]
Epoch [13/120    avg_loss:0.413, val_acc:0.882]
Epoch [14/120    avg_loss:0.268, val_acc:0.921]
Epoch [15/120    avg_loss:0.298, val_acc:0.898]
Epoch [16/120    avg_loss:0.242, val_acc:0.895]
Epoch [17/120    avg_loss:0.236, val_acc:0.875]
Epoch [18/120    avg_loss:0.283, val_acc:0.846]
Epoch [19/120    avg_loss:0.203, val_acc:0.931]
Epoch [20/120    avg_loss:0.187, val_acc:0.920]
Epoch [21/120    avg_loss:0.255, val_acc:0.901]
Epoch [22/120    avg_loss:0.205, val_acc:0.910]
Epoch [23/120    avg_loss:0.172, val_acc:0.933]
Epoch [24/120    avg_loss:0.148, val_acc:0.942]
Epoch [25/120    avg_loss:0.101, val_acc:0.927]
Epoch [26/120    avg_loss:0.152, val_acc:0.943]
Epoch [27/120    avg_loss:0.095, val_acc:0.934]
Epoch [28/120    avg_loss:0.114, val_acc:0.941]
Epoch [29/120    avg_loss:0.095, val_acc:0.948]
Epoch [30/120    avg_loss:0.080, val_acc:0.956]
Epoch [31/120    avg_loss:0.066, val_acc:0.964]
Epoch [32/120    avg_loss:0.078, val_acc:0.958]
Epoch [33/120    avg_loss:0.088, val_acc:0.955]
Epoch [34/120    avg_loss:0.060, val_acc:0.955]
Epoch [35/120    avg_loss:0.062, val_acc:0.948]
Epoch [36/120    avg_loss:0.063, val_acc:0.960]
Epoch [37/120    avg_loss:0.042, val_acc:0.962]
Epoch [38/120    avg_loss:0.088, val_acc:0.953]
Epoch [39/120    avg_loss:0.064, val_acc:0.960]
Epoch [40/120    avg_loss:0.090, val_acc:0.944]
Epoch [41/120    avg_loss:0.069, val_acc:0.944]
Epoch [42/120    avg_loss:0.072, val_acc:0.961]
Epoch [43/120    avg_loss:0.058, val_acc:0.964]
Epoch [44/120    avg_loss:0.039, val_acc:0.972]
Epoch [45/120    avg_loss:0.035, val_acc:0.965]
Epoch [46/120    avg_loss:0.042, val_acc:0.975]
Epoch [47/120    avg_loss:0.023, val_acc:0.948]
Epoch [48/120    avg_loss:0.037, val_acc:0.975]
Epoch [49/120    avg_loss:0.037, val_acc:0.961]
Epoch [50/120    avg_loss:0.047, val_acc:0.965]
Epoch [51/120    avg_loss:0.044, val_acc:0.970]
Epoch [52/120    avg_loss:0.200, val_acc:0.923]
Epoch [53/120    avg_loss:0.082, val_acc:0.965]
Epoch [54/120    avg_loss:0.066, val_acc:0.971]
Epoch [55/120    avg_loss:0.047, val_acc:0.971]
Epoch [56/120    avg_loss:0.073, val_acc:0.963]
Epoch [57/120    avg_loss:0.039, val_acc:0.964]
Epoch [58/120    avg_loss:0.041, val_acc:0.954]
Epoch [59/120    avg_loss:0.028, val_acc:0.980]
Epoch [60/120    avg_loss:0.030, val_acc:0.971]
Epoch [61/120    avg_loss:0.025, val_acc:0.962]
Epoch [62/120    avg_loss:0.041, val_acc:0.970]
Epoch [63/120    avg_loss:0.021, val_acc:0.978]
Epoch [64/120    avg_loss:0.024, val_acc:0.979]
Epoch [65/120    avg_loss:0.043, val_acc:0.963]
Epoch [66/120    avg_loss:0.030, val_acc:0.979]
Epoch [67/120    avg_loss:0.028, val_acc:0.973]
Epoch [68/120    avg_loss:0.013, val_acc:0.985]
Epoch [69/120    avg_loss:0.028, val_acc:0.974]
Epoch [70/120    avg_loss:0.019, val_acc:0.982]
Epoch [71/120    avg_loss:0.011, val_acc:0.984]
Epoch [72/120    avg_loss:0.012, val_acc:0.975]
Epoch [73/120    avg_loss:0.017, val_acc:0.982]
Epoch [74/120    avg_loss:0.014, val_acc:0.982]
Epoch [75/120    avg_loss:0.017, val_acc:0.974]
Epoch [76/120    avg_loss:0.022, val_acc:0.978]
Epoch [77/120    avg_loss:0.021, val_acc:0.981]
Epoch [78/120    avg_loss:0.013, val_acc:0.984]
Epoch [79/120    avg_loss:0.015, val_acc:0.981]
Epoch [80/120    avg_loss:0.008, val_acc:0.980]
Epoch [81/120    avg_loss:0.009, val_acc:0.984]
Epoch [82/120    avg_loss:0.010, val_acc:0.983]
Epoch [83/120    avg_loss:0.011, val_acc:0.985]
Epoch [84/120    avg_loss:0.006, val_acc:0.984]
Epoch [85/120    avg_loss:0.006, val_acc:0.985]
Epoch [86/120    avg_loss:0.007, val_acc:0.985]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.004, val_acc:0.984]
Epoch [89/120    avg_loss:0.005, val_acc:0.984]
Epoch [90/120    avg_loss:0.006, val_acc:0.987]
Epoch [91/120    avg_loss:0.009, val_acc:0.988]
Epoch [92/120    avg_loss:0.006, val_acc:0.985]
Epoch [93/120    avg_loss:0.007, val_acc:0.985]
Epoch [94/120    avg_loss:0.007, val_acc:0.987]
Epoch [95/120    avg_loss:0.005, val_acc:0.987]
Epoch [96/120    avg_loss:0.007, val_acc:0.988]
Epoch [97/120    avg_loss:0.005, val_acc:0.987]
Epoch [98/120    avg_loss:0.005, val_acc:0.988]
Epoch [99/120    avg_loss:0.004, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.987]
Epoch [102/120    avg_loss:0.009, val_acc:0.987]
Epoch [103/120    avg_loss:0.005, val_acc:0.987]
Epoch [104/120    avg_loss:0.004, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.987]
Epoch [106/120    avg_loss:0.007, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.010, val_acc:0.987]
Epoch [109/120    avg_loss:0.007, val_acc:0.987]
Epoch [110/120    avg_loss:0.005, val_acc:0.987]
Epoch [111/120    avg_loss:0.005, val_acc:0.988]
Epoch [112/120    avg_loss:0.005, val_acc:0.987]
Epoch [113/120    avg_loss:0.004, val_acc:0.987]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.005, val_acc:0.988]
Epoch [117/120    avg_loss:0.003, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.004, val_acc:0.988]
Epoch [120/120    avg_loss:0.005, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1270    5    0    0    0    0    0    0    5    2    3    0
     0    0    0]
 [   0    0    0  714    0   17    0    0    0    6    1    0    9    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    5    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    3    1    0    0    0  856   12    2    0
     0    0    0]
 [   0    0   10    0    0    1    5    0    0    0    4 2190    0    0
     0    0    0]
 [   0    0    0    8    6    2    0    0    0    0    0    0  514    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    1    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0   29    0    0    0    0    0    0    0
    18  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.15718157181571

F1 scores:
[       nan 1.         0.9898675  0.9661705  0.98139535 0.96404494
 0.97405486 0.98039216 0.99883856 0.71111111 0.98277842 0.99229724
 0.96707432 1.         0.98996072 0.92735703 0.97076023]

Kappa:
0.9789924786040756
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb4fab426a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.204, val_acc:0.586]
Epoch [2/120    avg_loss:1.749, val_acc:0.632]
Epoch [3/120    avg_loss:1.394, val_acc:0.612]
Epoch [4/120    avg_loss:1.173, val_acc:0.695]
Epoch [5/120    avg_loss:1.072, val_acc:0.714]
Epoch [6/120    avg_loss:0.808, val_acc:0.829]
Epoch [7/120    avg_loss:0.712, val_acc:0.779]
Epoch [8/120    avg_loss:0.684, val_acc:0.799]
Epoch [9/120    avg_loss:0.460, val_acc:0.837]
Epoch [10/120    avg_loss:0.432, val_acc:0.740]
Epoch [11/120    avg_loss:0.421, val_acc:0.853]
Epoch [12/120    avg_loss:0.469, val_acc:0.781]
Epoch [13/120    avg_loss:0.358, val_acc:0.885]
Epoch [14/120    avg_loss:0.347, val_acc:0.879]
Epoch [15/120    avg_loss:0.260, val_acc:0.906]
Epoch [16/120    avg_loss:0.245, val_acc:0.938]
Epoch [17/120    avg_loss:0.244, val_acc:0.871]
Epoch [18/120    avg_loss:0.297, val_acc:0.833]
Epoch [19/120    avg_loss:0.280, val_acc:0.866]
Epoch [20/120    avg_loss:0.198, val_acc:0.916]
Epoch [21/120    avg_loss:0.132, val_acc:0.921]
Epoch [22/120    avg_loss:0.127, val_acc:0.929]
Epoch [23/120    avg_loss:0.145, val_acc:0.927]
Epoch [24/120    avg_loss:0.176, val_acc:0.846]
Epoch [25/120    avg_loss:0.191, val_acc:0.885]
Epoch [26/120    avg_loss:0.238, val_acc:0.911]
Epoch [27/120    avg_loss:0.166, val_acc:0.903]
Epoch [28/120    avg_loss:0.114, val_acc:0.938]
Epoch [29/120    avg_loss:0.111, val_acc:0.932]
Epoch [30/120    avg_loss:0.096, val_acc:0.916]
Epoch [31/120    avg_loss:0.094, val_acc:0.949]
Epoch [32/120    avg_loss:0.061, val_acc:0.958]
Epoch [33/120    avg_loss:0.062, val_acc:0.948]
Epoch [34/120    avg_loss:0.061, val_acc:0.953]
Epoch [35/120    avg_loss:0.042, val_acc:0.952]
Epoch [36/120    avg_loss:0.042, val_acc:0.945]
Epoch [37/120    avg_loss:0.076, val_acc:0.953]
Epoch [38/120    avg_loss:0.072, val_acc:0.945]
Epoch [39/120    avg_loss:0.061, val_acc:0.953]
Epoch [40/120    avg_loss:0.057, val_acc:0.946]
Epoch [41/120    avg_loss:0.069, val_acc:0.950]
Epoch [42/120    avg_loss:0.040, val_acc:0.959]
Epoch [43/120    avg_loss:0.042, val_acc:0.965]
Epoch [44/120    avg_loss:0.039, val_acc:0.965]
Epoch [45/120    avg_loss:0.066, val_acc:0.953]
Epoch [46/120    avg_loss:0.055, val_acc:0.948]
Epoch [47/120    avg_loss:0.035, val_acc:0.969]
Epoch [48/120    avg_loss:0.061, val_acc:0.949]
Epoch [49/120    avg_loss:0.042, val_acc:0.969]
Epoch [50/120    avg_loss:0.040, val_acc:0.961]
Epoch [51/120    avg_loss:0.056, val_acc:0.948]
Epoch [52/120    avg_loss:0.061, val_acc:0.961]
Epoch [53/120    avg_loss:0.051, val_acc:0.952]
Epoch [54/120    avg_loss:0.080, val_acc:0.968]
Epoch [55/120    avg_loss:0.044, val_acc:0.961]
Epoch [56/120    avg_loss:0.046, val_acc:0.971]
Epoch [57/120    avg_loss:0.040, val_acc:0.971]
Epoch [58/120    avg_loss:0.024, val_acc:0.973]
Epoch [59/120    avg_loss:0.026, val_acc:0.963]
Epoch [60/120    avg_loss:0.031, val_acc:0.965]
Epoch [61/120    avg_loss:0.025, val_acc:0.969]
Epoch [62/120    avg_loss:0.020, val_acc:0.969]
Epoch [63/120    avg_loss:0.015, val_acc:0.973]
Epoch [64/120    avg_loss:0.022, val_acc:0.973]
Epoch [65/120    avg_loss:0.014, val_acc:0.977]
Epoch [66/120    avg_loss:0.015, val_acc:0.970]
Epoch [67/120    avg_loss:0.026, val_acc:0.971]
Epoch [68/120    avg_loss:0.030, val_acc:0.960]
Epoch [69/120    avg_loss:0.024, val_acc:0.962]
Epoch [70/120    avg_loss:0.021, val_acc:0.974]
Epoch [71/120    avg_loss:0.026, val_acc:0.971]
Epoch [72/120    avg_loss:0.017, val_acc:0.958]
Epoch [73/120    avg_loss:0.022, val_acc:0.968]
Epoch [74/120    avg_loss:0.025, val_acc:0.969]
Epoch [75/120    avg_loss:0.032, val_acc:0.974]
Epoch [76/120    avg_loss:0.018, val_acc:0.979]
Epoch [77/120    avg_loss:0.008, val_acc:0.978]
Epoch [78/120    avg_loss:0.009, val_acc:0.977]
Epoch [79/120    avg_loss:0.008, val_acc:0.982]
Epoch [80/120    avg_loss:0.013, val_acc:0.984]
Epoch [81/120    avg_loss:0.011, val_acc:0.973]
Epoch [82/120    avg_loss:0.017, val_acc:0.975]
Epoch [83/120    avg_loss:0.029, val_acc:0.969]
Epoch [84/120    avg_loss:0.039, val_acc:0.982]
Epoch [85/120    avg_loss:0.017, val_acc:0.978]
Epoch [86/120    avg_loss:0.040, val_acc:0.977]
Epoch [87/120    avg_loss:0.024, val_acc:0.978]
Epoch [88/120    avg_loss:0.013, val_acc:0.981]
Epoch [89/120    avg_loss:0.009, val_acc:0.980]
Epoch [90/120    avg_loss:0.008, val_acc:0.982]
Epoch [91/120    avg_loss:0.010, val_acc:0.983]
Epoch [92/120    avg_loss:0.007, val_acc:0.983]
Epoch [93/120    avg_loss:0.050, val_acc:0.946]
Epoch [94/120    avg_loss:0.052, val_acc:0.967]
Epoch [95/120    avg_loss:0.025, val_acc:0.968]
Epoch [96/120    avg_loss:0.025, val_acc:0.972]
Epoch [97/120    avg_loss:0.012, val_acc:0.974]
Epoch [98/120    avg_loss:0.016, val_acc:0.973]
Epoch [99/120    avg_loss:0.008, val_acc:0.977]
Epoch [100/120    avg_loss:0.011, val_acc:0.980]
Epoch [101/120    avg_loss:0.010, val_acc:0.979]
Epoch [102/120    avg_loss:0.008, val_acc:0.979]
Epoch [103/120    avg_loss:0.008, val_acc:0.979]
Epoch [104/120    avg_loss:0.009, val_acc:0.980]
Epoch [105/120    avg_loss:0.014, val_acc:0.980]
Epoch [106/120    avg_loss:0.008, val_acc:0.981]
Epoch [107/120    avg_loss:0.012, val_acc:0.982]
Epoch [108/120    avg_loss:0.006, val_acc:0.982]
Epoch [109/120    avg_loss:0.007, val_acc:0.982]
Epoch [110/120    avg_loss:0.009, val_acc:0.982]
Epoch [111/120    avg_loss:0.007, val_acc:0.981]
Epoch [112/120    avg_loss:0.006, val_acc:0.981]
Epoch [113/120    avg_loss:0.010, val_acc:0.981]
Epoch [114/120    avg_loss:0.008, val_acc:0.981]
Epoch [115/120    avg_loss:0.007, val_acc:0.981]
Epoch [116/120    avg_loss:0.005, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.981]
Epoch [118/120    avg_loss:0.013, val_acc:0.982]
Epoch [119/120    avg_loss:0.016, val_acc:0.982]
Epoch [120/120    avg_loss:0.011, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1269    4    0    0    0    0    0    0    5    2    5    0
     0    0    0]
 [   0    0    0  648    0   14    0    0    0    3    0    0   77    2
     0    3    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    6    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    8    0    0    0    0  832   25    8    0
     0    1    0]
 [   0    0   12    0    0    1    3    0    0    0    8 2186    0    0
     0    0    0]
 [   0    0    0    5    2    0    0    0    0    0    1    1  521    0
     1    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    0    0    0    0
  1131    0    0]
 [   0    0    0    0    0    0   28    0    0    0    0    3    0   11
     0  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.17073170731707

F1 scores:
[       nan 0.98765432 0.98831776 0.91980128 0.99061033 0.95856663
 0.97542815 0.89285714 0.99883856 0.81081081 0.96687972 0.98713028
 0.90924956 0.96605744 0.99603699 0.92846271 0.98224852]

Kappa:
0.9677518109131656
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1caac9b748>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.340, val_acc:0.536]
Epoch [2/120    avg_loss:1.634, val_acc:0.614]
Epoch [3/120    avg_loss:1.217, val_acc:0.709]
Epoch [4/120    avg_loss:1.272, val_acc:0.608]
Epoch [5/120    avg_loss:0.931, val_acc:0.750]
Epoch [6/120    avg_loss:0.897, val_acc:0.795]
Epoch [7/120    avg_loss:0.680, val_acc:0.796]
Epoch [8/120    avg_loss:0.658, val_acc:0.845]
Epoch [9/120    avg_loss:0.557, val_acc:0.883]
Epoch [10/120    avg_loss:0.530, val_acc:0.838]
Epoch [11/120    avg_loss:0.486, val_acc:0.870]
Epoch [12/120    avg_loss:0.334, val_acc:0.879]
Epoch [13/120    avg_loss:0.346, val_acc:0.894]
Epoch [14/120    avg_loss:0.368, val_acc:0.897]
Epoch [15/120    avg_loss:0.280, val_acc:0.862]
Epoch [16/120    avg_loss:0.342, val_acc:0.897]
Epoch [17/120    avg_loss:0.270, val_acc:0.927]
Epoch [18/120    avg_loss:0.177, val_acc:0.921]
Epoch [19/120    avg_loss:0.174, val_acc:0.934]
Epoch [20/120    avg_loss:0.123, val_acc:0.935]
Epoch [21/120    avg_loss:0.164, val_acc:0.926]
Epoch [22/120    avg_loss:0.124, val_acc:0.948]
Epoch [23/120    avg_loss:0.128, val_acc:0.928]
Epoch [24/120    avg_loss:0.160, val_acc:0.923]
Epoch [25/120    avg_loss:0.096, val_acc:0.938]
Epoch [26/120    avg_loss:0.101, val_acc:0.956]
Epoch [27/120    avg_loss:0.090, val_acc:0.948]
Epoch [28/120    avg_loss:0.099, val_acc:0.932]
Epoch [29/120    avg_loss:0.163, val_acc:0.944]
Epoch [30/120    avg_loss:0.214, val_acc:0.932]
Epoch [31/120    avg_loss:0.247, val_acc:0.936]
Epoch [32/120    avg_loss:0.111, val_acc:0.968]
Epoch [33/120    avg_loss:0.132, val_acc:0.964]
Epoch [34/120    avg_loss:0.084, val_acc:0.962]
Epoch [35/120    avg_loss:0.088, val_acc:0.964]
Epoch [36/120    avg_loss:0.118, val_acc:0.952]
Epoch [37/120    avg_loss:0.121, val_acc:0.970]
Epoch [38/120    avg_loss:0.101, val_acc:0.963]
Epoch [39/120    avg_loss:0.126, val_acc:0.953]
Epoch [40/120    avg_loss:0.163, val_acc:0.948]
Epoch [41/120    avg_loss:0.075, val_acc:0.963]
Epoch [42/120    avg_loss:0.049, val_acc:0.966]
Epoch [43/120    avg_loss:0.052, val_acc:0.972]
Epoch [44/120    avg_loss:0.057, val_acc:0.972]
Epoch [45/120    avg_loss:0.055, val_acc:0.981]
Epoch [46/120    avg_loss:0.044, val_acc:0.978]
Epoch [47/120    avg_loss:0.054, val_acc:0.962]
Epoch [48/120    avg_loss:0.045, val_acc:0.972]
Epoch [49/120    avg_loss:0.041, val_acc:0.975]
Epoch [50/120    avg_loss:0.051, val_acc:0.969]
Epoch [51/120    avg_loss:0.051, val_acc:0.977]
Epoch [52/120    avg_loss:0.021, val_acc:0.982]
Epoch [53/120    avg_loss:0.044, val_acc:0.981]
Epoch [54/120    avg_loss:0.039, val_acc:0.974]
Epoch [55/120    avg_loss:0.029, val_acc:0.982]
Epoch [56/120    avg_loss:0.028, val_acc:0.984]
Epoch [57/120    avg_loss:0.045, val_acc:0.962]
Epoch [58/120    avg_loss:0.038, val_acc:0.985]
Epoch [59/120    avg_loss:0.035, val_acc:0.982]
Epoch [60/120    avg_loss:0.040, val_acc:0.971]
Epoch [61/120    avg_loss:0.039, val_acc:0.982]
Epoch [62/120    avg_loss:0.101, val_acc:0.949]
Epoch [63/120    avg_loss:0.050, val_acc:0.982]
Epoch [64/120    avg_loss:0.018, val_acc:0.985]
Epoch [65/120    avg_loss:0.012, val_acc:0.988]
Epoch [66/120    avg_loss:0.014, val_acc:0.980]
Epoch [67/120    avg_loss:0.016, val_acc:0.980]
Epoch [68/120    avg_loss:0.019, val_acc:0.983]
Epoch [69/120    avg_loss:0.023, val_acc:0.975]
Epoch [70/120    avg_loss:0.017, val_acc:0.984]
Epoch [71/120    avg_loss:0.012, val_acc:0.984]
Epoch [72/120    avg_loss:0.018, val_acc:0.991]
Epoch [73/120    avg_loss:0.012, val_acc:0.991]
Epoch [74/120    avg_loss:0.015, val_acc:0.986]
Epoch [75/120    avg_loss:0.022, val_acc:0.988]
Epoch [76/120    avg_loss:0.021, val_acc:0.989]
Epoch [77/120    avg_loss:0.016, val_acc:0.985]
Epoch [78/120    avg_loss:0.012, val_acc:0.989]
Epoch [79/120    avg_loss:0.023, val_acc:0.989]
Epoch [80/120    avg_loss:0.009, val_acc:0.989]
Epoch [81/120    avg_loss:0.009, val_acc:0.990]
Epoch [82/120    avg_loss:0.018, val_acc:0.991]
Epoch [83/120    avg_loss:0.009, val_acc:0.991]
Epoch [84/120    avg_loss:0.004, val_acc:0.992]
Epoch [85/120    avg_loss:0.006, val_acc:0.988]
Epoch [86/120    avg_loss:0.005, val_acc:0.992]
Epoch [87/120    avg_loss:0.006, val_acc:0.989]
Epoch [88/120    avg_loss:0.009, val_acc:0.991]
Epoch [89/120    avg_loss:0.007, val_acc:0.992]
Epoch [90/120    avg_loss:0.003, val_acc:0.992]
Epoch [91/120    avg_loss:0.004, val_acc:0.990]
Epoch [92/120    avg_loss:0.004, val_acc:0.991]
Epoch [93/120    avg_loss:0.004, val_acc:0.989]
Epoch [94/120    avg_loss:0.006, val_acc:0.990]
Epoch [95/120    avg_loss:0.007, val_acc:0.991]
Epoch [96/120    avg_loss:0.012, val_acc:0.990]
Epoch [97/120    avg_loss:0.015, val_acc:0.985]
Epoch [98/120    avg_loss:0.015, val_acc:0.990]
Epoch [99/120    avg_loss:0.024, val_acc:0.974]
Epoch [100/120    avg_loss:0.017, val_acc:0.986]
Epoch [101/120    avg_loss:0.012, val_acc:0.988]
Epoch [102/120    avg_loss:0.005, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.992]
Epoch [104/120    avg_loss:0.006, val_acc:0.987]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.009, val_acc:0.987]
Epoch [107/120    avg_loss:0.017, val_acc:0.983]
Epoch [108/120    avg_loss:0.013, val_acc:0.981]
Epoch [109/120    avg_loss:0.041, val_acc:0.960]
Epoch [110/120    avg_loss:0.059, val_acc:0.973]
Epoch [111/120    avg_loss:0.037, val_acc:0.977]
Epoch [112/120    avg_loss:0.023, val_acc:0.983]
Epoch [113/120    avg_loss:0.014, val_acc:0.986]
Epoch [114/120    avg_loss:0.012, val_acc:0.991]
Epoch [115/120    avg_loss:0.023, val_acc:0.981]
Epoch [116/120    avg_loss:0.016, val_acc:0.989]
Epoch [117/120    avg_loss:0.007, val_acc:0.989]
Epoch [118/120    avg_loss:0.009, val_acc:0.988]
Epoch [119/120    avg_loss:0.008, val_acc:0.989]
Epoch [120/120    avg_loss:0.007, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1270    3    0    1    1    0    0    0    2    6    2    0
     0    0    0]
 [   0    0    0  703   18   14    0    0    0    5    0    0    6    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    7    0    2    3    0    0    0  832   22    0    0
     6    0    0]
 [   0    0    8    0    0    0   13    0    0    0    2 2187    0    0
     0    0    0]
 [   0    0    0   10    7    2    0    0    0    0    7   11  492    0
     0    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    0    0    0    0
  1133    0    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    46  287    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.44173441734418

F1 scores:
[       nan 1.         0.9898675  0.95646259 0.94456763 0.96969697
 0.97695167 0.98039216 0.99883856 0.8372093  0.9685681  0.98602344
 0.9516441  0.99730458 0.97504303 0.90536278 0.97109827]

Kappa:
0.9708202665532664
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc0d93dd668>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.278, val_acc:0.440]
Epoch [2/120    avg_loss:1.689, val_acc:0.649]
Epoch [3/120    avg_loss:1.382, val_acc:0.716]
Epoch [4/120    avg_loss:1.137, val_acc:0.702]
Epoch [5/120    avg_loss:0.935, val_acc:0.759]
Epoch [6/120    avg_loss:0.994, val_acc:0.749]
Epoch [7/120    avg_loss:0.817, val_acc:0.751]
Epoch [8/120    avg_loss:0.630, val_acc:0.777]
Epoch [9/120    avg_loss:0.722, val_acc:0.816]
Epoch [10/120    avg_loss:0.600, val_acc:0.822]
Epoch [11/120    avg_loss:0.440, val_acc:0.840]
Epoch [12/120    avg_loss:0.393, val_acc:0.832]
Epoch [13/120    avg_loss:0.460, val_acc:0.880]
Epoch [14/120    avg_loss:0.340, val_acc:0.867]
Epoch [15/120    avg_loss:0.257, val_acc:0.902]
Epoch [16/120    avg_loss:0.258, val_acc:0.878]
Epoch [17/120    avg_loss:0.240, val_acc:0.866]
Epoch [18/120    avg_loss:0.209, val_acc:0.904]
Epoch [19/120    avg_loss:0.173, val_acc:0.908]
Epoch [20/120    avg_loss:0.142, val_acc:0.917]
Epoch [21/120    avg_loss:0.156, val_acc:0.898]
Epoch [22/120    avg_loss:0.153, val_acc:0.889]
Epoch [23/120    avg_loss:0.172, val_acc:0.916]
Epoch [24/120    avg_loss:0.124, val_acc:0.924]
Epoch [25/120    avg_loss:0.120, val_acc:0.917]
Epoch [26/120    avg_loss:0.119, val_acc:0.912]
Epoch [27/120    avg_loss:0.142, val_acc:0.934]
Epoch [28/120    avg_loss:0.123, val_acc:0.936]
Epoch [29/120    avg_loss:0.093, val_acc:0.952]
Epoch [30/120    avg_loss:0.078, val_acc:0.948]
Epoch [31/120    avg_loss:0.104, val_acc:0.944]
Epoch [32/120    avg_loss:0.090, val_acc:0.940]
Epoch [33/120    avg_loss:0.064, val_acc:0.941]
Epoch [34/120    avg_loss:0.080, val_acc:0.921]
Epoch [35/120    avg_loss:0.108, val_acc:0.918]
Epoch [36/120    avg_loss:0.113, val_acc:0.909]
Epoch [37/120    avg_loss:0.165, val_acc:0.944]
Epoch [38/120    avg_loss:0.071, val_acc:0.932]
Epoch [39/120    avg_loss:0.067, val_acc:0.941]
Epoch [40/120    avg_loss:0.080, val_acc:0.946]
Epoch [41/120    avg_loss:0.063, val_acc:0.938]
Epoch [42/120    avg_loss:0.050, val_acc:0.959]
Epoch [43/120    avg_loss:0.065, val_acc:0.950]
Epoch [44/120    avg_loss:0.069, val_acc:0.952]
Epoch [45/120    avg_loss:0.062, val_acc:0.952]
Epoch [46/120    avg_loss:0.041, val_acc:0.958]
Epoch [47/120    avg_loss:0.047, val_acc:0.943]
Epoch [48/120    avg_loss:0.037, val_acc:0.957]
Epoch [49/120    avg_loss:0.039, val_acc:0.957]
Epoch [50/120    avg_loss:0.041, val_acc:0.944]
Epoch [51/120    avg_loss:0.035, val_acc:0.964]
Epoch [52/120    avg_loss:0.056, val_acc:0.953]
Epoch [53/120    avg_loss:0.053, val_acc:0.967]
Epoch [54/120    avg_loss:0.029, val_acc:0.931]
Epoch [55/120    avg_loss:0.025, val_acc:0.971]
Epoch [56/120    avg_loss:0.027, val_acc:0.956]
Epoch [57/120    avg_loss:0.044, val_acc:0.961]
Epoch [58/120    avg_loss:0.034, val_acc:0.966]
Epoch [59/120    avg_loss:0.026, val_acc:0.968]
Epoch [60/120    avg_loss:0.028, val_acc:0.957]
Epoch [61/120    avg_loss:0.021, val_acc:0.968]
Epoch [62/120    avg_loss:0.014, val_acc:0.966]
Epoch [63/120    avg_loss:0.021, val_acc:0.976]
Epoch [64/120    avg_loss:0.035, val_acc:0.963]
Epoch [65/120    avg_loss:0.024, val_acc:0.974]
Epoch [66/120    avg_loss:0.013, val_acc:0.976]
Epoch [67/120    avg_loss:0.018, val_acc:0.963]
Epoch [68/120    avg_loss:0.023, val_acc:0.975]
Epoch [69/120    avg_loss:0.013, val_acc:0.974]
Epoch [70/120    avg_loss:0.015, val_acc:0.977]
Epoch [71/120    avg_loss:0.037, val_acc:0.945]
Epoch [72/120    avg_loss:0.026, val_acc:0.964]
Epoch [73/120    avg_loss:0.029, val_acc:0.968]
Epoch [74/120    avg_loss:0.014, val_acc:0.971]
Epoch [75/120    avg_loss:0.028, val_acc:0.971]
Epoch [76/120    avg_loss:0.011, val_acc:0.971]
Epoch [77/120    avg_loss:0.028, val_acc:0.975]
Epoch [78/120    avg_loss:0.022, val_acc:0.974]
Epoch [79/120    avg_loss:0.016, val_acc:0.976]
Epoch [80/120    avg_loss:0.030, val_acc:0.974]
Epoch [81/120    avg_loss:0.031, val_acc:0.958]
Epoch [82/120    avg_loss:0.052, val_acc:0.954]
Epoch [83/120    avg_loss:0.100, val_acc:0.956]
Epoch [84/120    avg_loss:0.022, val_acc:0.961]
Epoch [85/120    avg_loss:0.025, val_acc:0.963]
Epoch [86/120    avg_loss:0.017, val_acc:0.966]
Epoch [87/120    avg_loss:0.022, val_acc:0.970]
Epoch [88/120    avg_loss:0.014, val_acc:0.968]
Epoch [89/120    avg_loss:0.017, val_acc:0.969]
Epoch [90/120    avg_loss:0.012, val_acc:0.971]
Epoch [91/120    avg_loss:0.015, val_acc:0.971]
Epoch [92/120    avg_loss:0.016, val_acc:0.974]
Epoch [93/120    avg_loss:0.022, val_acc:0.975]
Epoch [94/120    avg_loss:0.013, val_acc:0.974]
Epoch [95/120    avg_loss:0.014, val_acc:0.974]
Epoch [96/120    avg_loss:0.014, val_acc:0.975]
Epoch [97/120    avg_loss:0.011, val_acc:0.975]
Epoch [98/120    avg_loss:0.009, val_acc:0.975]
Epoch [99/120    avg_loss:0.009, val_acc:0.975]
Epoch [100/120    avg_loss:0.015, val_acc:0.975]
Epoch [101/120    avg_loss:0.018, val_acc:0.975]
Epoch [102/120    avg_loss:0.009, val_acc:0.975]
Epoch [103/120    avg_loss:0.008, val_acc:0.975]
Epoch [104/120    avg_loss:0.014, val_acc:0.975]
Epoch [105/120    avg_loss:0.014, val_acc:0.975]
Epoch [106/120    avg_loss:0.009, val_acc:0.975]
Epoch [107/120    avg_loss:0.012, val_acc:0.975]
Epoch [108/120    avg_loss:0.011, val_acc:0.976]
Epoch [109/120    avg_loss:0.011, val_acc:0.975]
Epoch [110/120    avg_loss:0.008, val_acc:0.975]
Epoch [111/120    avg_loss:0.009, val_acc:0.975]
Epoch [112/120    avg_loss:0.010, val_acc:0.975]
Epoch [113/120    avg_loss:0.012, val_acc:0.975]
Epoch [114/120    avg_loss:0.015, val_acc:0.975]
Epoch [115/120    avg_loss:0.010, val_acc:0.975]
Epoch [116/120    avg_loss:0.009, val_acc:0.975]
Epoch [117/120    avg_loss:0.009, val_acc:0.975]
Epoch [118/120    avg_loss:0.020, val_acc:0.975]
Epoch [119/120    avg_loss:0.013, val_acc:0.975]
Epoch [120/120    avg_loss:0.012, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1263    3    0    0    2    0    0    0    9    2    6    0
     0    0    0]
 [   0    0    0  698    0   42    0    0    0    6    0    0    0    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0   13    0    0    0    0
     0    0    0]
 [   0    0    1    4    0   11    4    0    0    0  827   20    2    0
     3    3    0]
 [   0    0   10    0   18    2    6    0    0    0    1 2172    0    1
     0    0    0]
 [   0    0    0    8   22   11    0    0    0    0    0    0  489    0
     0    0    4]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0   15    0    0    2    0    1    0    0    0
  1121    0    0]
 [   0    0    0    0    0    0   27    0    0    0    0    0    0    8
    32  280    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.78048780487805

F1 scores:
[       nan 1.         0.98710434 0.95290102 0.91416309 0.9092827
 0.97117517 0.98039216 0.99767981 0.65       0.9655575  0.98637602
 0.9485936  0.97097625 0.97690632 0.88888889 0.97674419]

Kappa:
0.9633217151684743
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:09:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f58c9e06630>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.260, val_acc:0.559]
Epoch [2/120    avg_loss:1.605, val_acc:0.661]
Epoch [3/120    avg_loss:1.363, val_acc:0.656]
Epoch [4/120    avg_loss:1.040, val_acc:0.724]
Epoch [5/120    avg_loss:0.944, val_acc:0.740]
Epoch [6/120    avg_loss:0.908, val_acc:0.754]
Epoch [7/120    avg_loss:0.708, val_acc:0.796]
Epoch [8/120    avg_loss:0.762, val_acc:0.797]
Epoch [9/120    avg_loss:0.754, val_acc:0.834]
Epoch [10/120    avg_loss:0.668, val_acc:0.811]
Epoch [11/120    avg_loss:0.565, val_acc:0.828]
Epoch [12/120    avg_loss:0.383, val_acc:0.861]
Epoch [13/120    avg_loss:0.435, val_acc:0.855]
Epoch [14/120    avg_loss:0.411, val_acc:0.879]
Epoch [15/120    avg_loss:0.397, val_acc:0.877]
Epoch [16/120    avg_loss:0.324, val_acc:0.838]
Epoch [17/120    avg_loss:0.331, val_acc:0.900]
Epoch [18/120    avg_loss:0.190, val_acc:0.887]
Epoch [19/120    avg_loss:0.176, val_acc:0.914]
Epoch [20/120    avg_loss:0.176, val_acc:0.926]
Epoch [21/120    avg_loss:0.220, val_acc:0.911]
Epoch [22/120    avg_loss:0.239, val_acc:0.916]
Epoch [23/120    avg_loss:0.203, val_acc:0.834]
Epoch [24/120    avg_loss:0.180, val_acc:0.870]
Epoch [25/120    avg_loss:0.245, val_acc:0.925]
Epoch [26/120    avg_loss:0.168, val_acc:0.944]
Epoch [27/120    avg_loss:0.102, val_acc:0.931]
Epoch [28/120    avg_loss:0.096, val_acc:0.942]
Epoch [29/120    avg_loss:0.090, val_acc:0.943]
Epoch [30/120    avg_loss:0.143, val_acc:0.927]
Epoch [31/120    avg_loss:0.108, val_acc:0.950]
Epoch [32/120    avg_loss:0.064, val_acc:0.948]
Epoch [33/120    avg_loss:0.048, val_acc:0.938]
Epoch [34/120    avg_loss:0.069, val_acc:0.958]
Epoch [35/120    avg_loss:0.061, val_acc:0.957]
Epoch [36/120    avg_loss:0.040, val_acc:0.962]
Epoch [37/120    avg_loss:0.080, val_acc:0.945]
Epoch [38/120    avg_loss:0.064, val_acc:0.956]
Epoch [39/120    avg_loss:0.035, val_acc:0.966]
Epoch [40/120    avg_loss:0.056, val_acc:0.966]
Epoch [41/120    avg_loss:0.039, val_acc:0.976]
Epoch [42/120    avg_loss:0.040, val_acc:0.962]
Epoch [43/120    avg_loss:0.036, val_acc:0.953]
Epoch [44/120    avg_loss:0.050, val_acc:0.968]
Epoch [45/120    avg_loss:0.023, val_acc:0.975]
Epoch [46/120    avg_loss:0.018, val_acc:0.977]
Epoch [47/120    avg_loss:0.039, val_acc:0.966]
Epoch [48/120    avg_loss:0.104, val_acc:0.898]
Epoch [49/120    avg_loss:0.080, val_acc:0.959]
Epoch [50/120    avg_loss:0.067, val_acc:0.938]
Epoch [51/120    avg_loss:0.073, val_acc:0.938]
Epoch [52/120    avg_loss:0.074, val_acc:0.967]
Epoch [53/120    avg_loss:0.044, val_acc:0.972]
Epoch [54/120    avg_loss:0.026, val_acc:0.981]
Epoch [55/120    avg_loss:0.020, val_acc:0.978]
Epoch [56/120    avg_loss:0.028, val_acc:0.978]
Epoch [57/120    avg_loss:0.021, val_acc:0.970]
Epoch [58/120    avg_loss:0.039, val_acc:0.976]
Epoch [59/120    avg_loss:0.026, val_acc:0.978]
Epoch [60/120    avg_loss:0.057, val_acc:0.974]
Epoch [61/120    avg_loss:0.020, val_acc:0.976]
Epoch [62/120    avg_loss:0.028, val_acc:0.981]
Epoch [63/120    avg_loss:0.034, val_acc:0.964]
Epoch [64/120    avg_loss:0.032, val_acc:0.960]
Epoch [65/120    avg_loss:0.033, val_acc:0.970]
Epoch [66/120    avg_loss:0.013, val_acc:0.981]
Epoch [67/120    avg_loss:0.020, val_acc:0.968]
Epoch [68/120    avg_loss:0.026, val_acc:0.974]
Epoch [69/120    avg_loss:0.024, val_acc:0.955]
Epoch [70/120    avg_loss:0.015, val_acc:0.986]
Epoch [71/120    avg_loss:0.017, val_acc:0.984]
Epoch [72/120    avg_loss:0.009, val_acc:0.983]
Epoch [73/120    avg_loss:0.009, val_acc:0.984]
Epoch [74/120    avg_loss:0.015, val_acc:0.981]
Epoch [75/120    avg_loss:0.012, val_acc:0.976]
Epoch [76/120    avg_loss:0.013, val_acc:0.978]
Epoch [77/120    avg_loss:0.013, val_acc:0.976]
Epoch [78/120    avg_loss:0.014, val_acc:0.981]
Epoch [79/120    avg_loss:0.008, val_acc:0.985]
Epoch [80/120    avg_loss:0.008, val_acc:0.987]
Epoch [81/120    avg_loss:0.009, val_acc:0.981]
Epoch [82/120    avg_loss:0.014, val_acc:0.976]
Epoch [83/120    avg_loss:0.010, val_acc:0.984]
Epoch [84/120    avg_loss:0.010, val_acc:0.984]
Epoch [85/120    avg_loss:0.006, val_acc:0.983]
Epoch [86/120    avg_loss:0.005, val_acc:0.985]
Epoch [87/120    avg_loss:0.004, val_acc:0.983]
Epoch [88/120    avg_loss:0.007, val_acc:0.984]
Epoch [89/120    avg_loss:0.009, val_acc:0.985]
Epoch [90/120    avg_loss:0.009, val_acc:0.980]
Epoch [91/120    avg_loss:0.011, val_acc:0.974]
Epoch [92/120    avg_loss:0.029, val_acc:0.972]
Epoch [93/120    avg_loss:0.024, val_acc:0.974]
Epoch [94/120    avg_loss:0.019, val_acc:0.978]
Epoch [95/120    avg_loss:0.013, val_acc:0.976]
Epoch [96/120    avg_loss:0.008, val_acc:0.978]
Epoch [97/120    avg_loss:0.010, val_acc:0.978]
Epoch [98/120    avg_loss:0.010, val_acc:0.978]
Epoch [99/120    avg_loss:0.006, val_acc:0.977]
Epoch [100/120    avg_loss:0.008, val_acc:0.978]
Epoch [101/120    avg_loss:0.005, val_acc:0.981]
Epoch [102/120    avg_loss:0.007, val_acc:0.982]
Epoch [103/120    avg_loss:0.010, val_acc:0.983]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.005, val_acc:0.984]
Epoch [106/120    avg_loss:0.003, val_acc:0.985]
Epoch [107/120    avg_loss:0.005, val_acc:0.984]
Epoch [108/120    avg_loss:0.006, val_acc:0.984]
Epoch [109/120    avg_loss:0.007, val_acc:0.984]
Epoch [110/120    avg_loss:0.004, val_acc:0.984]
Epoch [111/120    avg_loss:0.005, val_acc:0.984]
Epoch [112/120    avg_loss:0.006, val_acc:0.984]
Epoch [113/120    avg_loss:0.005, val_acc:0.984]
Epoch [114/120    avg_loss:0.005, val_acc:0.984]
Epoch [115/120    avg_loss:0.006, val_acc:0.984]
Epoch [116/120    avg_loss:0.007, val_acc:0.984]
Epoch [117/120    avg_loss:0.003, val_acc:0.984]
Epoch [118/120    avg_loss:0.005, val_acc:0.984]
Epoch [119/120    avg_loss:0.005, val_acc:0.983]
Epoch [120/120    avg_loss:0.008, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1272    3    0    0    0    0    0    0    5    0    2    0
     0    3    0]
 [   0    0    0  707    0   22    0    0    0    3    0    1   14    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    1    1    0    3    9    0    0    0  834   22    0    0
     0    5    0]
 [   0    0   16    1    0    2   11    0    0    0   12 2166    0    1
     1    0    0]
 [   0    0    0    0    6    3    0    0    0    0   16    8  494    0
     1    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    2    0    0    0
  1131    4    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    39  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.37669376693766

F1 scores:
[       nan 0.98765432 0.98796117 0.96584699 0.9837587  0.95991091
 0.98059701 0.98039216 1.         0.73684211 0.95642202 0.98298162
 0.94545455 0.99730458 0.97879706 0.91376702 0.95953757]

Kappa:
0.9700939940362137
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:56
Validation dataloader:56
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f86d1d9a6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.381, val_acc:0.412]
Epoch [2/120    avg_loss:1.689, val_acc:0.554]
Epoch [3/120    avg_loss:1.279, val_acc:0.578]
Epoch [4/120    avg_loss:1.191, val_acc:0.698]
Epoch [5/120    avg_loss:0.957, val_acc:0.712]
Epoch [6/120    avg_loss:0.854, val_acc:0.797]
Epoch [7/120    avg_loss:0.684, val_acc:0.826]
Epoch [8/120    avg_loss:0.544, val_acc:0.867]
Epoch [9/120    avg_loss:0.468, val_acc:0.844]
Epoch [10/120    avg_loss:0.432, val_acc:0.860]
Epoch [11/120    avg_loss:0.445, val_acc:0.832]
Epoch [12/120    avg_loss:0.525, val_acc:0.895]
Epoch [13/120    avg_loss:0.316, val_acc:0.837]
Epoch [14/120    avg_loss:0.396, val_acc:0.885]
Epoch [15/120    avg_loss:0.306, val_acc:0.887]
Epoch [16/120    avg_loss:0.289, val_acc:0.863]
Epoch [17/120    avg_loss:0.245, val_acc:0.930]
Epoch [18/120    avg_loss:0.390, val_acc:0.928]
Epoch [19/120    avg_loss:0.225, val_acc:0.921]
Epoch [20/120    avg_loss:0.151, val_acc:0.913]
Epoch [21/120    avg_loss:0.179, val_acc:0.919]
Epoch [22/120    avg_loss:0.211, val_acc:0.928]
Epoch [23/120    avg_loss:0.126, val_acc:0.927]
Epoch [24/120    avg_loss:0.087, val_acc:0.943]
Epoch [25/120    avg_loss:0.071, val_acc:0.918]
Epoch [26/120    avg_loss:0.102, val_acc:0.951]
Epoch [27/120    avg_loss:0.084, val_acc:0.945]
Epoch [28/120    avg_loss:0.106, val_acc:0.951]
Epoch [29/120    avg_loss:0.171, val_acc:0.936]
Epoch [30/120    avg_loss:0.118, val_acc:0.959]
Epoch [31/120    avg_loss:0.090, val_acc:0.924]
Epoch [32/120    avg_loss:0.097, val_acc:0.953]
Epoch [33/120    avg_loss:0.101, val_acc:0.961]
Epoch [34/120    avg_loss:0.101, val_acc:0.954]
Epoch [35/120    avg_loss:0.108, val_acc:0.954]
Epoch [36/120    avg_loss:0.166, val_acc:0.950]
Epoch [37/120    avg_loss:0.095, val_acc:0.959]
Epoch [38/120    avg_loss:0.055, val_acc:0.963]
Epoch [39/120    avg_loss:0.077, val_acc:0.952]
Epoch [40/120    avg_loss:0.071, val_acc:0.958]
Epoch [41/120    avg_loss:0.049, val_acc:0.968]
Epoch [42/120    avg_loss:0.051, val_acc:0.964]
Epoch [43/120    avg_loss:0.041, val_acc:0.969]
Epoch [44/120    avg_loss:0.039, val_acc:0.970]
Epoch [45/120    avg_loss:0.041, val_acc:0.969]
Epoch [46/120    avg_loss:0.025, val_acc:0.976]
Epoch [47/120    avg_loss:0.026, val_acc:0.980]
Epoch [48/120    avg_loss:0.027, val_acc:0.964]
Epoch [49/120    avg_loss:0.019, val_acc:0.979]
Epoch [50/120    avg_loss:0.026, val_acc:0.977]
Epoch [51/120    avg_loss:0.045, val_acc:0.969]
Epoch [52/120    avg_loss:0.098, val_acc:0.970]
Epoch [53/120    avg_loss:0.055, val_acc:0.964]
Epoch [54/120    avg_loss:0.037, val_acc:0.978]
Epoch [55/120    avg_loss:0.029, val_acc:0.971]
Epoch [56/120    avg_loss:0.026, val_acc:0.980]
Epoch [57/120    avg_loss:0.032, val_acc:0.957]
Epoch [58/120    avg_loss:0.044, val_acc:0.978]
Epoch [59/120    avg_loss:0.044, val_acc:0.948]
Epoch [60/120    avg_loss:0.039, val_acc:0.971]
Epoch [61/120    avg_loss:0.036, val_acc:0.958]
Epoch [62/120    avg_loss:0.023, val_acc:0.984]
Epoch [63/120    avg_loss:0.020, val_acc:0.976]
Epoch [64/120    avg_loss:0.012, val_acc:0.980]
Epoch [65/120    avg_loss:0.023, val_acc:0.988]
Epoch [66/120    avg_loss:0.017, val_acc:0.979]
Epoch [67/120    avg_loss:0.014, val_acc:0.989]
Epoch [68/120    avg_loss:0.014, val_acc:0.971]
Epoch [69/120    avg_loss:0.024, val_acc:0.980]
Epoch [70/120    avg_loss:0.013, val_acc:0.988]
Epoch [71/120    avg_loss:0.021, val_acc:0.979]
Epoch [72/120    avg_loss:0.019, val_acc:0.969]
Epoch [73/120    avg_loss:0.025, val_acc:0.981]
Epoch [74/120    avg_loss:0.012, val_acc:0.982]
Epoch [75/120    avg_loss:0.009, val_acc:0.984]
Epoch [76/120    avg_loss:0.015, val_acc:0.982]
Epoch [77/120    avg_loss:0.008, val_acc:0.982]
Epoch [78/120    avg_loss:0.038, val_acc:0.954]
Epoch [79/120    avg_loss:0.021, val_acc:0.982]
Epoch [80/120    avg_loss:0.022, val_acc:0.981]
Epoch [81/120    avg_loss:0.014, val_acc:0.985]
Epoch [82/120    avg_loss:0.015, val_acc:0.986]
Epoch [83/120    avg_loss:0.012, val_acc:0.987]
Epoch [84/120    avg_loss:0.011, val_acc:0.986]
Epoch [85/120    avg_loss:0.008, val_acc:0.986]
Epoch [86/120    avg_loss:0.009, val_acc:0.986]
Epoch [87/120    avg_loss:0.007, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.986]
Epoch [89/120    avg_loss:0.009, val_acc:0.987]
Epoch [90/120    avg_loss:0.011, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.005, val_acc:0.988]
Epoch [93/120    avg_loss:0.009, val_acc:0.989]
Epoch [94/120    avg_loss:0.008, val_acc:0.989]
Epoch [95/120    avg_loss:0.009, val_acc:0.989]
Epoch [96/120    avg_loss:0.008, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.006, val_acc:0.988]
Epoch [99/120    avg_loss:0.006, val_acc:0.988]
Epoch [100/120    avg_loss:0.006, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.011, val_acc:0.989]
Epoch [103/120    avg_loss:0.006, val_acc:0.989]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.008, val_acc:0.989]
Epoch [106/120    avg_loss:0.005, val_acc:0.989]
Epoch [107/120    avg_loss:0.006, val_acc:0.989]
Epoch [108/120    avg_loss:0.009, val_acc:0.989]
Epoch [109/120    avg_loss:0.006, val_acc:0.989]
Epoch [110/120    avg_loss:0.006, val_acc:0.989]
Epoch [111/120    avg_loss:0.007, val_acc:0.990]
Epoch [112/120    avg_loss:0.008, val_acc:0.990]
Epoch [113/120    avg_loss:0.006, val_acc:0.990]
Epoch [114/120    avg_loss:0.011, val_acc:0.988]
Epoch [115/120    avg_loss:0.011, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.989]
Epoch [120/120    avg_loss:0.007, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1269    1    0    0    4    0    0    0    8    2    1    0
     0    0    0]
 [   0    0    0  700   19   18    0    0    0    6    1    0    1    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    1    5    0    3    1    0    0    0  836   19    1    0
     2    7    0]
 [   0    0    9    0    0    0   10    0    0    0    6 2184    0    1
     0    0    0]
 [   0    0    0    6    7    1    0    0    0    0    0    9  506    0
     2    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    0    0    0
  1133    4    0]
 [   0    0    0    0    0    0   22    0    0    0    0    0    0    0
    20  305    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.68021680216802

F1 scores:
[       nan 1.         0.98985959 0.95759234 0.94247788 0.96730552
 0.97261288 1.         1.         0.66666667 0.96815287 0.98734177
 0.96934866 0.9919571  0.9869338  0.92006033 0.97647059]

Kappa:
0.9735551438979132
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f59619366d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.276, val_acc:0.499]
Epoch [2/120    avg_loss:1.671, val_acc:0.574]
Epoch [3/120    avg_loss:1.259, val_acc:0.616]
Epoch [4/120    avg_loss:1.150, val_acc:0.735]
Epoch [5/120    avg_loss:0.970, val_acc:0.707]
Epoch [6/120    avg_loss:0.981, val_acc:0.774]
Epoch [7/120    avg_loss:0.928, val_acc:0.758]
Epoch [8/120    avg_loss:0.633, val_acc:0.801]
Epoch [9/120    avg_loss:0.555, val_acc:0.828]
Epoch [10/120    avg_loss:0.644, val_acc:0.839]
Epoch [11/120    avg_loss:0.492, val_acc:0.831]
Epoch [12/120    avg_loss:0.529, val_acc:0.814]
Epoch [13/120    avg_loss:0.402, val_acc:0.874]
Epoch [14/120    avg_loss:0.297, val_acc:0.883]
Epoch [15/120    avg_loss:0.288, val_acc:0.868]
Epoch [16/120    avg_loss:0.276, val_acc:0.875]
Epoch [17/120    avg_loss:0.300, val_acc:0.875]
Epoch [18/120    avg_loss:0.398, val_acc:0.816]
Epoch [19/120    avg_loss:0.284, val_acc:0.866]
Epoch [20/120    avg_loss:0.218, val_acc:0.865]
Epoch [21/120    avg_loss:0.206, val_acc:0.905]
Epoch [22/120    avg_loss:0.146, val_acc:0.909]
Epoch [23/120    avg_loss:0.137, val_acc:0.926]
Epoch [24/120    avg_loss:0.190, val_acc:0.896]
Epoch [25/120    avg_loss:0.204, val_acc:0.927]
Epoch [26/120    avg_loss:0.241, val_acc:0.887]
Epoch [27/120    avg_loss:0.204, val_acc:0.911]
Epoch [28/120    avg_loss:0.118, val_acc:0.925]
Epoch [29/120    avg_loss:0.167, val_acc:0.916]
Epoch [30/120    avg_loss:0.092, val_acc:0.927]
Epoch [31/120    avg_loss:0.127, val_acc:0.932]
Epoch [32/120    avg_loss:0.119, val_acc:0.944]
Epoch [33/120    avg_loss:0.075, val_acc:0.941]
Epoch [34/120    avg_loss:0.107, val_acc:0.933]
Epoch [35/120    avg_loss:0.116, val_acc:0.914]
Epoch [36/120    avg_loss:0.127, val_acc:0.926]
Epoch [37/120    avg_loss:0.089, val_acc:0.953]
Epoch [38/120    avg_loss:0.069, val_acc:0.947]
Epoch [39/120    avg_loss:0.083, val_acc:0.944]
Epoch [40/120    avg_loss:0.080, val_acc:0.941]
Epoch [41/120    avg_loss:0.097, val_acc:0.959]
Epoch [42/120    avg_loss:0.077, val_acc:0.956]
Epoch [43/120    avg_loss:0.069, val_acc:0.948]
Epoch [44/120    avg_loss:0.107, val_acc:0.946]
Epoch [45/120    avg_loss:0.102, val_acc:0.947]
Epoch [46/120    avg_loss:0.069, val_acc:0.926]
Epoch [47/120    avg_loss:0.086, val_acc:0.956]
Epoch [48/120    avg_loss:0.084, val_acc:0.949]
Epoch [49/120    avg_loss:0.067, val_acc:0.964]
Epoch [50/120    avg_loss:0.058, val_acc:0.935]
Epoch [51/120    avg_loss:0.105, val_acc:0.962]
Epoch [52/120    avg_loss:0.231, val_acc:0.899]
Epoch [53/120    avg_loss:0.091, val_acc:0.929]
Epoch [54/120    avg_loss:0.083, val_acc:0.940]
Epoch [55/120    avg_loss:0.070, val_acc:0.958]
Epoch [56/120    avg_loss:0.051, val_acc:0.959]
Epoch [57/120    avg_loss:0.050, val_acc:0.962]
Epoch [58/120    avg_loss:0.039, val_acc:0.962]
Epoch [59/120    avg_loss:0.047, val_acc:0.966]
Epoch [60/120    avg_loss:0.053, val_acc:0.948]
Epoch [61/120    avg_loss:0.045, val_acc:0.963]
Epoch [62/120    avg_loss:0.069, val_acc:0.966]
Epoch [63/120    avg_loss:0.053, val_acc:0.962]
Epoch [64/120    avg_loss:0.043, val_acc:0.941]
Epoch [65/120    avg_loss:0.038, val_acc:0.966]
Epoch [66/120    avg_loss:0.033, val_acc:0.970]
Epoch [67/120    avg_loss:0.027, val_acc:0.965]
Epoch [68/120    avg_loss:0.025, val_acc:0.977]
Epoch [69/120    avg_loss:0.026, val_acc:0.973]
Epoch [70/120    avg_loss:0.021, val_acc:0.969]
Epoch [71/120    avg_loss:0.028, val_acc:0.980]
Epoch [72/120    avg_loss:0.015, val_acc:0.977]
Epoch [73/120    avg_loss:0.019, val_acc:0.969]
Epoch [74/120    avg_loss:0.017, val_acc:0.972]
Epoch [75/120    avg_loss:0.018, val_acc:0.965]
Epoch [76/120    avg_loss:0.020, val_acc:0.975]
Epoch [77/120    avg_loss:0.017, val_acc:0.973]
Epoch [78/120    avg_loss:0.016, val_acc:0.976]
Epoch [79/120    avg_loss:0.018, val_acc:0.977]
Epoch [80/120    avg_loss:0.016, val_acc:0.979]
Epoch [81/120    avg_loss:0.031, val_acc:0.968]
Epoch [82/120    avg_loss:0.028, val_acc:0.976]
Epoch [83/120    avg_loss:0.022, val_acc:0.971]
Epoch [84/120    avg_loss:0.046, val_acc:0.964]
Epoch [85/120    avg_loss:0.033, val_acc:0.969]
Epoch [86/120    avg_loss:0.016, val_acc:0.972]
Epoch [87/120    avg_loss:0.012, val_acc:0.973]
Epoch [88/120    avg_loss:0.009, val_acc:0.975]
Epoch [89/120    avg_loss:0.011, val_acc:0.978]
Epoch [90/120    avg_loss:0.011, val_acc:0.977]
Epoch [91/120    avg_loss:0.011, val_acc:0.978]
Epoch [92/120    avg_loss:0.014, val_acc:0.981]
Epoch [93/120    avg_loss:0.012, val_acc:0.983]
Epoch [94/120    avg_loss:0.015, val_acc:0.977]
Epoch [95/120    avg_loss:0.009, val_acc:0.979]
Epoch [96/120    avg_loss:0.008, val_acc:0.981]
Epoch [97/120    avg_loss:0.008, val_acc:0.983]
Epoch [98/120    avg_loss:0.007, val_acc:0.981]
Epoch [99/120    avg_loss:0.010, val_acc:0.981]
Epoch [100/120    avg_loss:0.005, val_acc:0.981]
Epoch [101/120    avg_loss:0.009, val_acc:0.981]
Epoch [102/120    avg_loss:0.009, val_acc:0.981]
Epoch [103/120    avg_loss:0.007, val_acc:0.980]
Epoch [104/120    avg_loss:0.010, val_acc:0.981]
Epoch [105/120    avg_loss:0.008, val_acc:0.981]
Epoch [106/120    avg_loss:0.009, val_acc:0.981]
Epoch [107/120    avg_loss:0.010, val_acc:0.981]
Epoch [108/120    avg_loss:0.008, val_acc:0.981]
Epoch [109/120    avg_loss:0.008, val_acc:0.981]
Epoch [110/120    avg_loss:0.008, val_acc:0.981]
Epoch [111/120    avg_loss:0.004, val_acc:0.981]
Epoch [112/120    avg_loss:0.012, val_acc:0.981]
Epoch [113/120    avg_loss:0.006, val_acc:0.981]
Epoch [114/120    avg_loss:0.008, val_acc:0.981]
Epoch [115/120    avg_loss:0.010, val_acc:0.981]
Epoch [116/120    avg_loss:0.004, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.981]
Epoch [118/120    avg_loss:0.007, val_acc:0.981]
Epoch [119/120    avg_loss:0.007, val_acc:0.981]
Epoch [120/120    avg_loss:0.007, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1266    1    0    0    1    0    0    0    8    4    5    0
     0    0    0]
 [   0    0    0  716    0   19    0    0    0    5    0    0    7    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0   11   58    0    4    5    0    0    0  769   24    2    0
     2    0    0]
 [   0    0   15    0    0    0   13    0    0    0    7 2174    0    1
     0    0    0]
 [   0    0    0    7    0    3    0    0    0    0    2    0  515    0
     5    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    1    0    0
  1135    0    0]
 [   0    0    0    0    0    0   29    0    0    0    0    8    0    5
    15  290    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.93224932249322

F1 scores:
[       nan 0.98765432 0.98215671 0.93472585 1.         0.96205357
 0.96475771 0.98039216 1.         0.73170732 0.92594822 0.9834879
 0.96713615 0.98133333 0.98867596 0.91051805 0.97619048]

Kappa:
0.965022060750779
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1e7093f6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.220, val_acc:0.512]
Epoch [2/120    avg_loss:1.702, val_acc:0.602]
Epoch [3/120    avg_loss:1.193, val_acc:0.702]
Epoch [4/120    avg_loss:1.003, val_acc:0.759]
Epoch [5/120    avg_loss:0.996, val_acc:0.669]
Epoch [6/120    avg_loss:0.945, val_acc:0.689]
Epoch [7/120    avg_loss:0.836, val_acc:0.787]
Epoch [8/120    avg_loss:0.650, val_acc:0.798]
Epoch [9/120    avg_loss:0.512, val_acc:0.768]
Epoch [10/120    avg_loss:0.724, val_acc:0.794]
Epoch [11/120    avg_loss:0.573, val_acc:0.818]
Epoch [12/120    avg_loss:0.359, val_acc:0.844]
Epoch [13/120    avg_loss:0.416, val_acc:0.830]
Epoch [14/120    avg_loss:0.354, val_acc:0.846]
Epoch [15/120    avg_loss:0.343, val_acc:0.868]
Epoch [16/120    avg_loss:0.258, val_acc:0.859]
Epoch [17/120    avg_loss:0.231, val_acc:0.872]
Epoch [18/120    avg_loss:0.228, val_acc:0.848]
Epoch [19/120    avg_loss:0.257, val_acc:0.876]
Epoch [20/120    avg_loss:0.278, val_acc:0.869]
Epoch [21/120    avg_loss:0.377, val_acc:0.826]
Epoch [22/120    avg_loss:0.355, val_acc:0.894]
Epoch [23/120    avg_loss:0.260, val_acc:0.879]
Epoch [24/120    avg_loss:0.181, val_acc:0.888]
Epoch [25/120    avg_loss:0.189, val_acc:0.882]
Epoch [26/120    avg_loss:0.167, val_acc:0.903]
Epoch [27/120    avg_loss:0.142, val_acc:0.922]
Epoch [28/120    avg_loss:0.167, val_acc:0.931]
Epoch [29/120    avg_loss:0.160, val_acc:0.917]
Epoch [30/120    avg_loss:0.094, val_acc:0.935]
Epoch [31/120    avg_loss:0.087, val_acc:0.932]
Epoch [32/120    avg_loss:0.088, val_acc:0.949]
Epoch [33/120    avg_loss:0.181, val_acc:0.933]
Epoch [34/120    avg_loss:0.082, val_acc:0.939]
Epoch [35/120    avg_loss:0.129, val_acc:0.941]
Epoch [36/120    avg_loss:0.074, val_acc:0.939]
Epoch [37/120    avg_loss:0.089, val_acc:0.927]
Epoch [38/120    avg_loss:0.085, val_acc:0.950]
Epoch [39/120    avg_loss:0.060, val_acc:0.942]
Epoch [40/120    avg_loss:0.095, val_acc:0.925]
Epoch [41/120    avg_loss:0.075, val_acc:0.959]
Epoch [42/120    avg_loss:0.065, val_acc:0.956]
Epoch [43/120    avg_loss:0.090, val_acc:0.917]
Epoch [44/120    avg_loss:0.060, val_acc:0.962]
Epoch [45/120    avg_loss:0.066, val_acc:0.943]
Epoch [46/120    avg_loss:0.064, val_acc:0.935]
Epoch [47/120    avg_loss:0.087, val_acc:0.941]
Epoch [48/120    avg_loss:0.066, val_acc:0.927]
Epoch [49/120    avg_loss:0.048, val_acc:0.949]
Epoch [50/120    avg_loss:0.031, val_acc:0.954]
Epoch [51/120    avg_loss:0.039, val_acc:0.961]
Epoch [52/120    avg_loss:0.048, val_acc:0.963]
Epoch [53/120    avg_loss:0.033, val_acc:0.961]
Epoch [54/120    avg_loss:0.052, val_acc:0.954]
Epoch [55/120    avg_loss:0.042, val_acc:0.943]
Epoch [56/120    avg_loss:0.039, val_acc:0.969]
Epoch [57/120    avg_loss:0.022, val_acc:0.970]
Epoch [58/120    avg_loss:0.044, val_acc:0.968]
Epoch [59/120    avg_loss:0.030, val_acc:0.964]
Epoch [60/120    avg_loss:0.030, val_acc:0.963]
Epoch [61/120    avg_loss:0.022, val_acc:0.973]
Epoch [62/120    avg_loss:0.027, val_acc:0.963]
Epoch [63/120    avg_loss:0.023, val_acc:0.964]
Epoch [64/120    avg_loss:0.029, val_acc:0.963]
Epoch [65/120    avg_loss:0.026, val_acc:0.975]
Epoch [66/120    avg_loss:0.039, val_acc:0.962]
Epoch [67/120    avg_loss:0.026, val_acc:0.967]
Epoch [68/120    avg_loss:0.067, val_acc:0.958]
Epoch [69/120    avg_loss:0.090, val_acc:0.939]
Epoch [70/120    avg_loss:0.126, val_acc:0.892]
Epoch [71/120    avg_loss:0.122, val_acc:0.945]
Epoch [72/120    avg_loss:0.051, val_acc:0.949]
Epoch [73/120    avg_loss:0.060, val_acc:0.944]
Epoch [74/120    avg_loss:0.068, val_acc:0.954]
Epoch [75/120    avg_loss:0.056, val_acc:0.946]
Epoch [76/120    avg_loss:0.054, val_acc:0.962]
Epoch [77/120    avg_loss:0.028, val_acc:0.969]
Epoch [78/120    avg_loss:0.049, val_acc:0.960]
Epoch [79/120    avg_loss:0.036, val_acc:0.970]
Epoch [80/120    avg_loss:0.026, val_acc:0.974]
Epoch [81/120    avg_loss:0.015, val_acc:0.974]
Epoch [82/120    avg_loss:0.015, val_acc:0.974]
Epoch [83/120    avg_loss:0.017, val_acc:0.977]
Epoch [84/120    avg_loss:0.019, val_acc:0.975]
Epoch [85/120    avg_loss:0.018, val_acc:0.975]
Epoch [86/120    avg_loss:0.014, val_acc:0.977]
Epoch [87/120    avg_loss:0.014, val_acc:0.977]
Epoch [88/120    avg_loss:0.015, val_acc:0.977]
Epoch [89/120    avg_loss:0.011, val_acc:0.974]
Epoch [90/120    avg_loss:0.016, val_acc:0.973]
Epoch [91/120    avg_loss:0.012, val_acc:0.977]
Epoch [92/120    avg_loss:0.012, val_acc:0.978]
Epoch [93/120    avg_loss:0.011, val_acc:0.978]
Epoch [94/120    avg_loss:0.012, val_acc:0.979]
Epoch [95/120    avg_loss:0.009, val_acc:0.977]
Epoch [96/120    avg_loss:0.013, val_acc:0.978]
Epoch [97/120    avg_loss:0.012, val_acc:0.979]
Epoch [98/120    avg_loss:0.011, val_acc:0.979]
Epoch [99/120    avg_loss:0.010, val_acc:0.980]
Epoch [100/120    avg_loss:0.012, val_acc:0.980]
Epoch [101/120    avg_loss:0.010, val_acc:0.980]
Epoch [102/120    avg_loss:0.007, val_acc:0.978]
Epoch [103/120    avg_loss:0.013, val_acc:0.978]
Epoch [104/120    avg_loss:0.012, val_acc:0.981]
Epoch [105/120    avg_loss:0.009, val_acc:0.980]
Epoch [106/120    avg_loss:0.011, val_acc:0.981]
Epoch [107/120    avg_loss:0.013, val_acc:0.981]
Epoch [108/120    avg_loss:0.006, val_acc:0.981]
Epoch [109/120    avg_loss:0.019, val_acc:0.978]
Epoch [110/120    avg_loss:0.011, val_acc:0.981]
Epoch [111/120    avg_loss:0.008, val_acc:0.982]
Epoch [112/120    avg_loss:0.010, val_acc:0.982]
Epoch [113/120    avg_loss:0.009, val_acc:0.981]
Epoch [114/120    avg_loss:0.009, val_acc:0.981]
Epoch [115/120    avg_loss:0.010, val_acc:0.981]
Epoch [116/120    avg_loss:0.011, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.979]
Epoch [118/120    avg_loss:0.006, val_acc:0.981]
Epoch [119/120    avg_loss:0.013, val_acc:0.978]
Epoch [120/120    avg_loss:0.010, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1272    3    0    0    0    0    0    0    4    2    4    0
     0    0    0]
 [   0    0    0  718    0   18    0    0    0    9    0    0    0    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    3    0    0    0    0    0    0  427    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   14    0    0    1    0
     0    0    0]
 [   0    0    2   68    0    1    6    0    0    0  786   12    0    0
     0    0    0]
 [   0    0   12    0    0    6    4    0    0    0   13 2153   21    1
     0    0    0]
 [   0    0    0    7    0    3    0    0    0    0   10    2  506    0
     3    0    3]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0  183
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   20    0    0    0    0    5    0    0
    11  311    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.12737127371274

F1 scores:
[       nan 0.96470588 0.98949825 0.92884864 1.         0.96436526
 0.97767857 1.         0.99649942 0.65116279 0.93072824 0.98220803
 0.94845361 0.98652291 0.99345264 0.94528875 0.97647059]

Kappa:
0.9672756511206717
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcd85ad8710>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.296, val_acc:0.550]
Epoch [2/120    avg_loss:1.744, val_acc:0.598]
Epoch [3/120    avg_loss:1.315, val_acc:0.630]
Epoch [4/120    avg_loss:1.212, val_acc:0.744]
Epoch [5/120    avg_loss:1.010, val_acc:0.728]
Epoch [6/120    avg_loss:0.964, val_acc:0.706]
Epoch [7/120    avg_loss:0.851, val_acc:0.745]
Epoch [8/120    avg_loss:0.647, val_acc:0.770]
Epoch [9/120    avg_loss:0.712, val_acc:0.792]
Epoch [10/120    avg_loss:0.540, val_acc:0.828]
Epoch [11/120    avg_loss:0.466, val_acc:0.802]
Epoch [12/120    avg_loss:0.453, val_acc:0.811]
Epoch [13/120    avg_loss:0.453, val_acc:0.849]
Epoch [14/120    avg_loss:0.427, val_acc:0.811]
Epoch [15/120    avg_loss:0.292, val_acc:0.856]
Epoch [16/120    avg_loss:0.296, val_acc:0.874]
Epoch [17/120    avg_loss:0.488, val_acc:0.851]
Epoch [18/120    avg_loss:0.329, val_acc:0.907]
Epoch [19/120    avg_loss:0.271, val_acc:0.884]
Epoch [20/120    avg_loss:0.210, val_acc:0.885]
Epoch [21/120    avg_loss:0.261, val_acc:0.897]
Epoch [22/120    avg_loss:0.234, val_acc:0.916]
Epoch [23/120    avg_loss:0.195, val_acc:0.919]
Epoch [24/120    avg_loss:0.146, val_acc:0.907]
Epoch [25/120    avg_loss:0.183, val_acc:0.910]
Epoch [26/120    avg_loss:0.121, val_acc:0.930]
Epoch [27/120    avg_loss:0.125, val_acc:0.927]
Epoch [28/120    avg_loss:0.133, val_acc:0.932]
Epoch [29/120    avg_loss:0.088, val_acc:0.953]
Epoch [30/120    avg_loss:0.113, val_acc:0.932]
Epoch [31/120    avg_loss:0.136, val_acc:0.943]
Epoch [32/120    avg_loss:0.081, val_acc:0.926]
Epoch [33/120    avg_loss:0.131, val_acc:0.912]
Epoch [34/120    avg_loss:0.085, val_acc:0.945]
Epoch [35/120    avg_loss:0.121, val_acc:0.939]
Epoch [36/120    avg_loss:0.113, val_acc:0.935]
Epoch [37/120    avg_loss:0.095, val_acc:0.949]
Epoch [38/120    avg_loss:0.072, val_acc:0.920]
Epoch [39/120    avg_loss:0.082, val_acc:0.947]
Epoch [40/120    avg_loss:0.063, val_acc:0.950]
Epoch [41/120    avg_loss:0.067, val_acc:0.957]
Epoch [42/120    avg_loss:0.069, val_acc:0.951]
Epoch [43/120    avg_loss:0.060, val_acc:0.955]
Epoch [44/120    avg_loss:0.060, val_acc:0.945]
Epoch [45/120    avg_loss:0.038, val_acc:0.966]
Epoch [46/120    avg_loss:0.055, val_acc:0.953]
Epoch [47/120    avg_loss:0.062, val_acc:0.966]
Epoch [48/120    avg_loss:0.058, val_acc:0.949]
Epoch [49/120    avg_loss:0.087, val_acc:0.949]
Epoch [50/120    avg_loss:0.099, val_acc:0.951]
Epoch [51/120    avg_loss:0.103, val_acc:0.930]
Epoch [52/120    avg_loss:0.108, val_acc:0.926]
Epoch [53/120    avg_loss:0.087, val_acc:0.964]
Epoch [54/120    avg_loss:0.052, val_acc:0.960]
Epoch [55/120    avg_loss:0.120, val_acc:0.895]
Epoch [56/120    avg_loss:0.296, val_acc:0.919]
Epoch [57/120    avg_loss:0.242, val_acc:0.947]
Epoch [58/120    avg_loss:0.099, val_acc:0.957]
Epoch [59/120    avg_loss:0.062, val_acc:0.960]
Epoch [60/120    avg_loss:0.066, val_acc:0.949]
Epoch [61/120    avg_loss:0.051, val_acc:0.966]
Epoch [62/120    avg_loss:0.035, val_acc:0.970]
Epoch [63/120    avg_loss:0.031, val_acc:0.974]
Epoch [64/120    avg_loss:0.033, val_acc:0.976]
Epoch [65/120    avg_loss:0.035, val_acc:0.975]
Epoch [66/120    avg_loss:0.025, val_acc:0.981]
Epoch [67/120    avg_loss:0.024, val_acc:0.977]
Epoch [68/120    avg_loss:0.024, val_acc:0.978]
Epoch [69/120    avg_loss:0.028, val_acc:0.977]
Epoch [70/120    avg_loss:0.035, val_acc:0.978]
Epoch [71/120    avg_loss:0.023, val_acc:0.970]
Epoch [72/120    avg_loss:0.019, val_acc:0.973]
Epoch [73/120    avg_loss:0.025, val_acc:0.976]
Epoch [74/120    avg_loss:0.024, val_acc:0.976]
Epoch [75/120    avg_loss:0.023, val_acc:0.976]
Epoch [76/120    avg_loss:0.023, val_acc:0.978]
Epoch [77/120    avg_loss:0.020, val_acc:0.980]
Epoch [78/120    avg_loss:0.020, val_acc:0.980]
Epoch [79/120    avg_loss:0.031, val_acc:0.981]
Epoch [80/120    avg_loss:0.017, val_acc:0.981]
Epoch [81/120    avg_loss:0.019, val_acc:0.982]
Epoch [82/120    avg_loss:0.020, val_acc:0.982]
Epoch [83/120    avg_loss:0.022, val_acc:0.983]
Epoch [84/120    avg_loss:0.019, val_acc:0.981]
Epoch [85/120    avg_loss:0.017, val_acc:0.983]
Epoch [86/120    avg_loss:0.019, val_acc:0.982]
Epoch [87/120    avg_loss:0.022, val_acc:0.980]
Epoch [88/120    avg_loss:0.021, val_acc:0.977]
Epoch [89/120    avg_loss:0.017, val_acc:0.980]
Epoch [90/120    avg_loss:0.017, val_acc:0.981]
Epoch [91/120    avg_loss:0.020, val_acc:0.980]
Epoch [92/120    avg_loss:0.020, val_acc:0.980]
Epoch [93/120    avg_loss:0.021, val_acc:0.977]
Epoch [94/120    avg_loss:0.018, val_acc:0.977]
Epoch [95/120    avg_loss:0.027, val_acc:0.980]
Epoch [96/120    avg_loss:0.023, val_acc:0.981]
Epoch [97/120    avg_loss:0.019, val_acc:0.981]
Epoch [98/120    avg_loss:0.021, val_acc:0.981]
Epoch [99/120    avg_loss:0.018, val_acc:0.980]
Epoch [100/120    avg_loss:0.020, val_acc:0.980]
Epoch [101/120    avg_loss:0.016, val_acc:0.980]
Epoch [102/120    avg_loss:0.018, val_acc:0.980]
Epoch [103/120    avg_loss:0.020, val_acc:0.980]
Epoch [104/120    avg_loss:0.020, val_acc:0.980]
Epoch [105/120    avg_loss:0.018, val_acc:0.980]
Epoch [106/120    avg_loss:0.017, val_acc:0.980]
Epoch [107/120    avg_loss:0.013, val_acc:0.981]
Epoch [108/120    avg_loss:0.016, val_acc:0.981]
Epoch [109/120    avg_loss:0.016, val_acc:0.981]
Epoch [110/120    avg_loss:0.021, val_acc:0.980]
Epoch [111/120    avg_loss:0.020, val_acc:0.980]
Epoch [112/120    avg_loss:0.015, val_acc:0.980]
Epoch [113/120    avg_loss:0.022, val_acc:0.980]
Epoch [114/120    avg_loss:0.021, val_acc:0.980]
Epoch [115/120    avg_loss:0.019, val_acc:0.980]
Epoch [116/120    avg_loss:0.023, val_acc:0.980]
Epoch [117/120    avg_loss:0.013, val_acc:0.980]
Epoch [118/120    avg_loss:0.014, val_acc:0.980]
Epoch [119/120    avg_loss:0.017, val_acc:0.980]
Epoch [120/120    avg_loss:0.021, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    0    0    0    0    0    0    0   16    3    5    0
     0    0    0]
 [   0    0    0  736    0    7    0    0    0    2    0    0    1    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    5    0    1    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    0  655    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   13    0    0    1    0
     0    0    0]
 [   0    0   12   60    0    5    0    0    0    0  776   22    0    0
     0    0    0]
 [   0    0   12    0    0    6    8    0    0    0   11 2170    0    3
     0    0    0]
 [   0    0    0    2    0    3    0    0    0    0    4    1  512    0
     3    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    2    0    0
  1135    0    0]
 [   0    0    0    0    0    0   24    0    0    0    0    0    0    0
     8  315    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.32249322493224

F1 scores:
[       nan 0.98765432 0.98017878 0.95029051 1.         0.96949153
 0.97470238 0.90909091 0.99883856 0.76470588 0.9221628  0.9845735
 0.971537   0.98930481 0.99343545 0.95166163 0.94318182]

Kappa:
0.9694832668605629
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd9883456a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.243, val_acc:0.524]
Epoch [2/120    avg_loss:1.684, val_acc:0.514]
Epoch [3/120    avg_loss:1.563, val_acc:0.511]
Epoch [4/120    avg_loss:1.279, val_acc:0.601]
Epoch [5/120    avg_loss:1.033, val_acc:0.744]
Epoch [6/120    avg_loss:1.014, val_acc:0.728]
Epoch [7/120    avg_loss:0.729, val_acc:0.780]
Epoch [8/120    avg_loss:0.764, val_acc:0.783]
Epoch [9/120    avg_loss:0.647, val_acc:0.727]
Epoch [10/120    avg_loss:0.550, val_acc:0.844]
Epoch [11/120    avg_loss:0.473, val_acc:0.827]
Epoch [12/120    avg_loss:0.532, val_acc:0.852]
Epoch [13/120    avg_loss:0.453, val_acc:0.867]
Epoch [14/120    avg_loss:0.392, val_acc:0.838]
Epoch [15/120    avg_loss:0.456, val_acc:0.894]
Epoch [16/120    avg_loss:0.406, val_acc:0.877]
Epoch [17/120    avg_loss:0.486, val_acc:0.872]
Epoch [18/120    avg_loss:0.397, val_acc:0.885]
Epoch [19/120    avg_loss:0.281, val_acc:0.908]
Epoch [20/120    avg_loss:0.212, val_acc:0.882]
Epoch [21/120    avg_loss:0.228, val_acc:0.894]
Epoch [22/120    avg_loss:0.242, val_acc:0.897]
Epoch [23/120    avg_loss:0.188, val_acc:0.920]
Epoch [24/120    avg_loss:0.116, val_acc:0.910]
Epoch [25/120    avg_loss:0.173, val_acc:0.925]
Epoch [26/120    avg_loss:0.161, val_acc:0.927]
Epoch [27/120    avg_loss:0.253, val_acc:0.897]
Epoch [28/120    avg_loss:0.196, val_acc:0.907]
Epoch [29/120    avg_loss:0.129, val_acc:0.933]
Epoch [30/120    avg_loss:0.152, val_acc:0.928]
Epoch [31/120    avg_loss:0.111, val_acc:0.926]
Epoch [32/120    avg_loss:0.104, val_acc:0.919]
Epoch [33/120    avg_loss:0.127, val_acc:0.943]
Epoch [34/120    avg_loss:0.138, val_acc:0.923]
Epoch [35/120    avg_loss:0.178, val_acc:0.931]
Epoch [36/120    avg_loss:0.154, val_acc:0.940]
Epoch [37/120    avg_loss:0.075, val_acc:0.943]
Epoch [38/120    avg_loss:0.075, val_acc:0.943]
Epoch [39/120    avg_loss:0.078, val_acc:0.952]
Epoch [40/120    avg_loss:0.077, val_acc:0.955]
Epoch [41/120    avg_loss:0.062, val_acc:0.951]
Epoch [42/120    avg_loss:0.071, val_acc:0.958]
Epoch [43/120    avg_loss:0.055, val_acc:0.931]
Epoch [44/120    avg_loss:0.062, val_acc:0.960]
Epoch [45/120    avg_loss:0.055, val_acc:0.961]
Epoch [46/120    avg_loss:0.039, val_acc:0.966]
Epoch [47/120    avg_loss:0.098, val_acc:0.949]
Epoch [48/120    avg_loss:0.055, val_acc:0.964]
Epoch [49/120    avg_loss:0.051, val_acc:0.957]
Epoch [50/120    avg_loss:0.041, val_acc:0.958]
Epoch [51/120    avg_loss:0.053, val_acc:0.950]
Epoch [52/120    avg_loss:0.050, val_acc:0.968]
Epoch [53/120    avg_loss:0.041, val_acc:0.965]
Epoch [54/120    avg_loss:0.052, val_acc:0.951]
Epoch [55/120    avg_loss:0.041, val_acc:0.965]
Epoch [56/120    avg_loss:0.035, val_acc:0.960]
Epoch [57/120    avg_loss:0.036, val_acc:0.961]
Epoch [58/120    avg_loss:0.037, val_acc:0.959]
Epoch [59/120    avg_loss:0.025, val_acc:0.961]
Epoch [60/120    avg_loss:0.038, val_acc:0.966]
Epoch [61/120    avg_loss:0.028, val_acc:0.972]
Epoch [62/120    avg_loss:0.081, val_acc:0.939]
Epoch [63/120    avg_loss:0.070, val_acc:0.953]
Epoch [64/120    avg_loss:0.051, val_acc:0.963]
Epoch [65/120    avg_loss:0.027, val_acc:0.966]
Epoch [66/120    avg_loss:0.023, val_acc:0.970]
Epoch [67/120    avg_loss:0.025, val_acc:0.939]
Epoch [68/120    avg_loss:0.035, val_acc:0.968]
Epoch [69/120    avg_loss:0.031, val_acc:0.966]
Epoch [70/120    avg_loss:0.023, val_acc:0.966]
Epoch [71/120    avg_loss:0.019, val_acc:0.981]
Epoch [72/120    avg_loss:0.020, val_acc:0.969]
Epoch [73/120    avg_loss:0.023, val_acc:0.967]
Epoch [74/120    avg_loss:0.019, val_acc:0.976]
Epoch [75/120    avg_loss:0.030, val_acc:0.949]
Epoch [76/120    avg_loss:0.075, val_acc:0.960]
Epoch [77/120    avg_loss:0.037, val_acc:0.973]
Epoch [78/120    avg_loss:0.028, val_acc:0.961]
Epoch [79/120    avg_loss:0.028, val_acc:0.980]
Epoch [80/120    avg_loss:0.015, val_acc:0.968]
Epoch [81/120    avg_loss:0.011, val_acc:0.964]
Epoch [82/120    avg_loss:0.016, val_acc:0.974]
Epoch [83/120    avg_loss:0.021, val_acc:0.956]
Epoch [84/120    avg_loss:0.040, val_acc:0.951]
Epoch [85/120    avg_loss:0.038, val_acc:0.974]
Epoch [86/120    avg_loss:0.022, val_acc:0.972]
Epoch [87/120    avg_loss:0.019, val_acc:0.972]
Epoch [88/120    avg_loss:0.017, val_acc:0.972]
Epoch [89/120    avg_loss:0.013, val_acc:0.975]
Epoch [90/120    avg_loss:0.013, val_acc:0.973]
Epoch [91/120    avg_loss:0.012, val_acc:0.977]
Epoch [92/120    avg_loss:0.014, val_acc:0.975]
Epoch [93/120    avg_loss:0.013, val_acc:0.974]
Epoch [94/120    avg_loss:0.010, val_acc:0.980]
Epoch [95/120    avg_loss:0.010, val_acc:0.976]
Epoch [96/120    avg_loss:0.008, val_acc:0.980]
Epoch [97/120    avg_loss:0.010, val_acc:0.980]
Epoch [98/120    avg_loss:0.008, val_acc:0.980]
Epoch [99/120    avg_loss:0.007, val_acc:0.980]
Epoch [100/120    avg_loss:0.009, val_acc:0.980]
Epoch [101/120    avg_loss:0.012, val_acc:0.980]
Epoch [102/120    avg_loss:0.010, val_acc:0.980]
Epoch [103/120    avg_loss:0.007, val_acc:0.980]
Epoch [104/120    avg_loss:0.008, val_acc:0.980]
Epoch [105/120    avg_loss:0.008, val_acc:0.980]
Epoch [106/120    avg_loss:0.008, val_acc:0.980]
Epoch [107/120    avg_loss:0.007, val_acc:0.980]
Epoch [108/120    avg_loss:0.008, val_acc:0.980]
Epoch [109/120    avg_loss:0.008, val_acc:0.980]
Epoch [110/120    avg_loss:0.013, val_acc:0.981]
Epoch [111/120    avg_loss:0.008, val_acc:0.981]
Epoch [112/120    avg_loss:0.007, val_acc:0.981]
Epoch [113/120    avg_loss:0.009, val_acc:0.981]
Epoch [114/120    avg_loss:0.014, val_acc:0.981]
Epoch [115/120    avg_loss:0.010, val_acc:0.981]
Epoch [116/120    avg_loss:0.009, val_acc:0.982]
Epoch [117/120    avg_loss:0.009, val_acc:0.982]
Epoch [118/120    avg_loss:0.017, val_acc:0.981]
Epoch [119/120    avg_loss:0.011, val_acc:0.981]
Epoch [120/120    avg_loss:0.011, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1270    1    0    0    0    0    0    0    2    1    4    0
     0    2    5]
 [   0    0    0  724    0   20    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    6    0    0    0    0    0   11    0    0    1    0
     0    0    0]
 [   0    0    1   64    0    0    0    0    0    0  780   26    0    0
     0    4    0]
 [   0    0    9    0    0    5    6    0    0    0    9 2179    0    2
     0    0    0]
 [   0    0    0   11    2    3    0    0    0    0   12    2  498    0
     2    0    4]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    1    0    0    0
  1130    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    27  319    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.27913279132791

F1 scores:
[       nan 0.975      0.98948189 0.93238892 0.9953271  0.9556541
 0.99470098 0.98039216 0.99883856 0.62857143 0.92912448 0.98641919
 0.96046287 0.99191375 0.98346388 0.94940476 0.94915254]

Kappa:
0.9689827574150831
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4f9a17e710>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.376, val_acc:0.516]
Epoch [2/120    avg_loss:1.591, val_acc:0.602]
Epoch [3/120    avg_loss:1.524, val_acc:0.643]
Epoch [4/120    avg_loss:1.100, val_acc:0.715]
Epoch [5/120    avg_loss:1.065, val_acc:0.709]
Epoch [6/120    avg_loss:1.059, val_acc:0.723]
Epoch [7/120    avg_loss:0.732, val_acc:0.797]
Epoch [8/120    avg_loss:0.692, val_acc:0.877]
Epoch [9/120    avg_loss:0.617, val_acc:0.824]
Epoch [10/120    avg_loss:0.451, val_acc:0.882]
Epoch [11/120    avg_loss:0.416, val_acc:0.859]
Epoch [12/120    avg_loss:0.430, val_acc:0.872]
Epoch [13/120    avg_loss:0.387, val_acc:0.834]
Epoch [14/120    avg_loss:0.472, val_acc:0.882]
Epoch [15/120    avg_loss:0.288, val_acc:0.896]
Epoch [16/120    avg_loss:0.234, val_acc:0.914]
Epoch [17/120    avg_loss:0.316, val_acc:0.897]
Epoch [18/120    avg_loss:0.194, val_acc:0.929]
Epoch [19/120    avg_loss:0.286, val_acc:0.898]
Epoch [20/120    avg_loss:0.240, val_acc:0.891]
Epoch [21/120    avg_loss:0.235, val_acc:0.927]
Epoch [22/120    avg_loss:0.187, val_acc:0.913]
Epoch [23/120    avg_loss:0.239, val_acc:0.926]
Epoch [24/120    avg_loss:0.160, val_acc:0.943]
Epoch [25/120    avg_loss:0.196, val_acc:0.949]
Epoch [26/120    avg_loss:0.131, val_acc:0.954]
Epoch [27/120    avg_loss:0.138, val_acc:0.946]
Epoch [28/120    avg_loss:0.101, val_acc:0.959]
Epoch [29/120    avg_loss:0.102, val_acc:0.958]
Epoch [30/120    avg_loss:0.074, val_acc:0.956]
Epoch [31/120    avg_loss:0.134, val_acc:0.965]
Epoch [32/120    avg_loss:0.114, val_acc:0.946]
Epoch [33/120    avg_loss:0.158, val_acc:0.932]
Epoch [34/120    avg_loss:0.133, val_acc:0.942]
Epoch [35/120    avg_loss:0.171, val_acc:0.949]
Epoch [36/120    avg_loss:0.092, val_acc:0.970]
Epoch [37/120    avg_loss:0.103, val_acc:0.958]
Epoch [38/120    avg_loss:0.089, val_acc:0.971]
Epoch [39/120    avg_loss:0.089, val_acc:0.969]
Epoch [40/120    avg_loss:0.080, val_acc:0.958]
Epoch [41/120    avg_loss:0.068, val_acc:0.971]
Epoch [42/120    avg_loss:0.097, val_acc:0.955]
Epoch [43/120    avg_loss:0.072, val_acc:0.961]
Epoch [44/120    avg_loss:0.055, val_acc:0.968]
Epoch [45/120    avg_loss:0.044, val_acc:0.974]
Epoch [46/120    avg_loss:0.034, val_acc:0.981]
Epoch [47/120    avg_loss:0.066, val_acc:0.959]
Epoch [48/120    avg_loss:0.078, val_acc:0.965]
Epoch [49/120    avg_loss:0.257, val_acc:0.922]
Epoch [50/120    avg_loss:0.154, val_acc:0.945]
Epoch [51/120    avg_loss:0.083, val_acc:0.974]
Epoch [52/120    avg_loss:0.045, val_acc:0.967]
Epoch [53/120    avg_loss:0.043, val_acc:0.974]
Epoch [54/120    avg_loss:0.063, val_acc:0.972]
Epoch [55/120    avg_loss:0.048, val_acc:0.969]
Epoch [56/120    avg_loss:0.045, val_acc:0.983]
Epoch [57/120    avg_loss:0.035, val_acc:0.983]
Epoch [58/120    avg_loss:0.024, val_acc:0.980]
Epoch [59/120    avg_loss:0.028, val_acc:0.980]
Epoch [60/120    avg_loss:0.051, val_acc:0.946]
Epoch [61/120    avg_loss:0.064, val_acc:0.963]
Epoch [62/120    avg_loss:0.051, val_acc:0.977]
Epoch [63/120    avg_loss:0.031, val_acc:0.978]
Epoch [64/120    avg_loss:0.022, val_acc:0.981]
Epoch [65/120    avg_loss:0.019, val_acc:0.978]
Epoch [66/120    avg_loss:0.019, val_acc:0.980]
Epoch [67/120    avg_loss:0.025, val_acc:0.967]
Epoch [68/120    avg_loss:0.029, val_acc:0.979]
Epoch [69/120    avg_loss:0.017, val_acc:0.981]
Epoch [70/120    avg_loss:0.020, val_acc:0.987]
Epoch [71/120    avg_loss:0.032, val_acc:0.979]
Epoch [72/120    avg_loss:0.036, val_acc:0.943]
Epoch [73/120    avg_loss:0.180, val_acc:0.942]
Epoch [74/120    avg_loss:0.090, val_acc:0.978]
Epoch [75/120    avg_loss:0.034, val_acc:0.960]
Epoch [76/120    avg_loss:0.037, val_acc:0.982]
Epoch [77/120    avg_loss:0.018, val_acc:0.985]
Epoch [78/120    avg_loss:0.023, val_acc:0.983]
Epoch [79/120    avg_loss:0.047, val_acc:0.972]
Epoch [80/120    avg_loss:0.029, val_acc:0.981]
Epoch [81/120    avg_loss:0.027, val_acc:0.984]
Epoch [82/120    avg_loss:0.018, val_acc:0.980]
Epoch [83/120    avg_loss:0.022, val_acc:0.980]
Epoch [84/120    avg_loss:0.027, val_acc:0.982]
Epoch [85/120    avg_loss:0.014, val_acc:0.983]
Epoch [86/120    avg_loss:0.012, val_acc:0.984]
Epoch [87/120    avg_loss:0.018, val_acc:0.987]
Epoch [88/120    avg_loss:0.010, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.985]
Epoch [90/120    avg_loss:0.010, val_acc:0.987]
Epoch [91/120    avg_loss:0.010, val_acc:0.988]
Epoch [92/120    avg_loss:0.008, val_acc:0.988]
Epoch [93/120    avg_loss:0.014, val_acc:0.988]
Epoch [94/120    avg_loss:0.009, val_acc:0.987]
Epoch [95/120    avg_loss:0.010, val_acc:0.985]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.010, val_acc:0.987]
Epoch [98/120    avg_loss:0.010, val_acc:0.987]
Epoch [99/120    avg_loss:0.009, val_acc:0.987]
Epoch [100/120    avg_loss:0.010, val_acc:0.987]
Epoch [101/120    avg_loss:0.009, val_acc:0.985]
Epoch [102/120    avg_loss:0.006, val_acc:0.987]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.015, val_acc:0.987]
Epoch [105/120    avg_loss:0.009, val_acc:0.988]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.007, val_acc:0.988]
Epoch [108/120    avg_loss:0.008, val_acc:0.988]
Epoch [109/120    avg_loss:0.009, val_acc:0.988]
Epoch [110/120    avg_loss:0.008, val_acc:0.988]
Epoch [111/120    avg_loss:0.013, val_acc:0.987]
Epoch [112/120    avg_loss:0.010, val_acc:0.988]
Epoch [113/120    avg_loss:0.011, val_acc:0.988]
Epoch [114/120    avg_loss:0.009, val_acc:0.988]
Epoch [115/120    avg_loss:0.015, val_acc:0.987]
Epoch [116/120    avg_loss:0.007, val_acc:0.987]
Epoch [117/120    avg_loss:0.010, val_acc:0.987]
Epoch [118/120    avg_loss:0.015, val_acc:0.988]
Epoch [119/120    avg_loss:0.007, val_acc:0.989]
Epoch [120/120    avg_loss:0.009, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1272    3    0    0    0    0    0    0    5    1    4    0
     0    0    0]
 [   0    0    0  716    0   11    0    0    0    9    0    0   11    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    6    0    0    0    0    0    0  423    0    0    1    0    0
     0    0    0]
 [   0    0    0    6    0    0    0    0    0   12    0    0    0    0
     0    0    0]
 [   0    0    0   64    0    0    0    0    0    0  780   27    2    0
     2    0    0]
 [   0    0   18    0    0    0    5    0    0    0    6 2181    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    4    3  522    0
     2    0    1]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1139    0    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
    35  287    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.18157181571816

F1 scores:
[       nan 0.93181818 0.98796117 0.93107932 1.         0.98177677
 0.97767857 1.         0.99179367 0.58536585 0.93413174 0.98620846
 0.97116279 0.99728997 0.98231997 0.90536278 0.98203593]

Kappa:
0.9678562550488027
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:19--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb851d096a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.417, val_acc:0.535]
Epoch [2/120    avg_loss:1.704, val_acc:0.590]
Epoch [3/120    avg_loss:1.458, val_acc:0.654]
Epoch [4/120    avg_loss:1.181, val_acc:0.733]
Epoch [5/120    avg_loss:1.101, val_acc:0.733]
Epoch [6/120    avg_loss:1.036, val_acc:0.734]
Epoch [7/120    avg_loss:0.694, val_acc:0.799]
Epoch [8/120    avg_loss:0.745, val_acc:0.812]
Epoch [9/120    avg_loss:0.664, val_acc:0.807]
Epoch [10/120    avg_loss:0.613, val_acc:0.584]
Epoch [11/120    avg_loss:0.792, val_acc:0.722]
Epoch [12/120    avg_loss:0.506, val_acc:0.826]
Epoch [13/120    avg_loss:0.504, val_acc:0.792]
Epoch [14/120    avg_loss:0.476, val_acc:0.846]
Epoch [15/120    avg_loss:0.415, val_acc:0.869]
Epoch [16/120    avg_loss:0.303, val_acc:0.875]
Epoch [17/120    avg_loss:0.388, val_acc:0.894]
Epoch [18/120    avg_loss:0.320, val_acc:0.876]
Epoch [19/120    avg_loss:0.298, val_acc:0.885]
Epoch [20/120    avg_loss:0.213, val_acc:0.906]
Epoch [21/120    avg_loss:0.274, val_acc:0.911]
Epoch [22/120    avg_loss:0.213, val_acc:0.931]
Epoch [23/120    avg_loss:0.181, val_acc:0.914]
Epoch [24/120    avg_loss:0.232, val_acc:0.912]
Epoch [25/120    avg_loss:0.174, val_acc:0.908]
Epoch [26/120    avg_loss:0.210, val_acc:0.892]
Epoch [27/120    avg_loss:0.207, val_acc:0.907]
Epoch [28/120    avg_loss:0.174, val_acc:0.933]
Epoch [29/120    avg_loss:0.143, val_acc:0.927]
Epoch [30/120    avg_loss:0.102, val_acc:0.931]
Epoch [31/120    avg_loss:0.111, val_acc:0.946]
Epoch [32/120    avg_loss:0.119, val_acc:0.942]
Epoch [33/120    avg_loss:0.097, val_acc:0.924]
Epoch [34/120    avg_loss:0.133, val_acc:0.939]
Epoch [35/120    avg_loss:0.123, val_acc:0.951]
Epoch [36/120    avg_loss:0.088, val_acc:0.963]
Epoch [37/120    avg_loss:0.111, val_acc:0.941]
Epoch [38/120    avg_loss:0.144, val_acc:0.907]
Epoch [39/120    avg_loss:0.226, val_acc:0.907]
Epoch [40/120    avg_loss:0.158, val_acc:0.936]
Epoch [41/120    avg_loss:0.091, val_acc:0.962]
Epoch [42/120    avg_loss:0.201, val_acc:0.944]
Epoch [43/120    avg_loss:0.085, val_acc:0.936]
Epoch [44/120    avg_loss:0.067, val_acc:0.967]
Epoch [45/120    avg_loss:0.158, val_acc:0.952]
Epoch [46/120    avg_loss:0.107, val_acc:0.951]
Epoch [47/120    avg_loss:0.081, val_acc:0.942]
Epoch [48/120    avg_loss:0.066, val_acc:0.962]
Epoch [49/120    avg_loss:0.069, val_acc:0.965]
Epoch [50/120    avg_loss:0.060, val_acc:0.959]
Epoch [51/120    avg_loss:0.057, val_acc:0.954]
Epoch [52/120    avg_loss:0.059, val_acc:0.953]
Epoch [53/120    avg_loss:0.068, val_acc:0.954]
Epoch [54/120    avg_loss:0.045, val_acc:0.970]
Epoch [55/120    avg_loss:0.043, val_acc:0.943]
Epoch [56/120    avg_loss:0.044, val_acc:0.960]
Epoch [57/120    avg_loss:0.037, val_acc:0.971]
Epoch [58/120    avg_loss:0.039, val_acc:0.969]
Epoch [59/120    avg_loss:0.044, val_acc:0.965]
Epoch [60/120    avg_loss:0.025, val_acc:0.975]
Epoch [61/120    avg_loss:0.030, val_acc:0.977]
Epoch [62/120    avg_loss:0.036, val_acc:0.969]
Epoch [63/120    avg_loss:0.035, val_acc:0.971]
Epoch [64/120    avg_loss:0.031, val_acc:0.971]
Epoch [65/120    avg_loss:0.033, val_acc:0.978]
Epoch [66/120    avg_loss:0.026, val_acc:0.984]
Epoch [67/120    avg_loss:0.038, val_acc:0.971]
Epoch [68/120    avg_loss:0.077, val_acc:0.965]
Epoch [69/120    avg_loss:0.035, val_acc:0.975]
Epoch [70/120    avg_loss:0.019, val_acc:0.977]
Epoch [71/120    avg_loss:0.014, val_acc:0.982]
Epoch [72/120    avg_loss:0.015, val_acc:0.979]
Epoch [73/120    avg_loss:0.018, val_acc:0.980]
Epoch [74/120    avg_loss:0.017, val_acc:0.975]
Epoch [75/120    avg_loss:0.013, val_acc:0.977]
Epoch [76/120    avg_loss:0.034, val_acc:0.978]
Epoch [77/120    avg_loss:0.014, val_acc:0.985]
Epoch [78/120    avg_loss:0.012, val_acc:0.983]
Epoch [79/120    avg_loss:0.017, val_acc:0.981]
Epoch [80/120    avg_loss:0.018, val_acc:0.979]
Epoch [81/120    avg_loss:0.013, val_acc:0.985]
Epoch [82/120    avg_loss:0.014, val_acc:0.987]
Epoch [83/120    avg_loss:0.008, val_acc:0.980]
Epoch [84/120    avg_loss:0.023, val_acc:0.980]
Epoch [85/120    avg_loss:0.019, val_acc:0.982]
Epoch [86/120    avg_loss:0.017, val_acc:0.969]
Epoch [87/120    avg_loss:0.022, val_acc:0.980]
Epoch [88/120    avg_loss:0.015, val_acc:0.988]
Epoch [89/120    avg_loss:0.010, val_acc:0.980]
Epoch [90/120    avg_loss:0.017, val_acc:0.985]
Epoch [91/120    avg_loss:0.016, val_acc:0.980]
Epoch [92/120    avg_loss:0.013, val_acc:0.984]
Epoch [93/120    avg_loss:0.011, val_acc:0.983]
Epoch [94/120    avg_loss:0.012, val_acc:0.982]
Epoch [95/120    avg_loss:0.017, val_acc:0.965]
Epoch [96/120    avg_loss:0.017, val_acc:0.979]
Epoch [97/120    avg_loss:0.009, val_acc:0.985]
Epoch [98/120    avg_loss:0.010, val_acc:0.985]
Epoch [99/120    avg_loss:0.010, val_acc:0.978]
Epoch [100/120    avg_loss:0.009, val_acc:0.987]
Epoch [101/120    avg_loss:0.012, val_acc:0.973]
Epoch [102/120    avg_loss:0.023, val_acc:0.982]
Epoch [103/120    avg_loss:0.009, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.983]
Epoch [105/120    avg_loss:0.009, val_acc:0.983]
Epoch [106/120    avg_loss:0.005, val_acc:0.983]
Epoch [107/120    avg_loss:0.005, val_acc:0.983]
Epoch [108/120    avg_loss:0.005, val_acc:0.984]
Epoch [109/120    avg_loss:0.009, val_acc:0.985]
Epoch [110/120    avg_loss:0.007, val_acc:0.984]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.006, val_acc:0.985]
Epoch [113/120    avg_loss:0.006, val_acc:0.988]
Epoch [114/120    avg_loss:0.007, val_acc:0.987]
Epoch [115/120    avg_loss:0.006, val_acc:0.990]
Epoch [116/120    avg_loss:0.004, val_acc:0.990]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1274    2    0    0    0    0    0    0    2    2    3    0
     0    2    0]
 [   0    0    0  725    0   14    0    0    0    6    0    0    1    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    4    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   14    0    0    1    0
     0    0    0]
 [   0    0    6   73    0    2    0    0    0    0  769   24    0    0
     1    0    0]
 [   0    0   12    0    0    0    7    0    0    0    3 2187    0    1
     0    0    0]
 [   0    0    0    4    0    3    0    0    0    0   13    0  506    0
     3    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   28    0    0    0    0    0    0    0
    47  272    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.96476964769647

F1 scores:
[       nan 0.98765432 0.98836307 0.93307593 1.         0.97052154
 0.97405486 0.98039216 0.99883856 0.66666667 0.92483464 0.98892155
 0.96749522 0.99462366 0.97638471 0.87600644 0.96511628]

Kappa:
0.9653788937619656
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f607c31a6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.359, val_acc:0.550]
Epoch [2/120    avg_loss:1.791, val_acc:0.567]
Epoch [3/120    avg_loss:1.321, val_acc:0.633]
Epoch [4/120    avg_loss:1.113, val_acc:0.651]
Epoch [5/120    avg_loss:0.945, val_acc:0.747]
Epoch [6/120    avg_loss:0.706, val_acc:0.794]
Epoch [7/120    avg_loss:0.612, val_acc:0.773]
Epoch [8/120    avg_loss:0.585, val_acc:0.815]
Epoch [9/120    avg_loss:0.509, val_acc:0.795]
Epoch [10/120    avg_loss:0.538, val_acc:0.831]
Epoch [11/120    avg_loss:0.374, val_acc:0.807]
Epoch [12/120    avg_loss:0.494, val_acc:0.881]
Epoch [13/120    avg_loss:0.312, val_acc:0.869]
Epoch [14/120    avg_loss:0.374, val_acc:0.851]
Epoch [15/120    avg_loss:0.523, val_acc:0.843]
Epoch [16/120    avg_loss:0.476, val_acc:0.875]
Epoch [17/120    avg_loss:0.365, val_acc:0.877]
Epoch [18/120    avg_loss:0.314, val_acc:0.912]
Epoch [19/120    avg_loss:0.361, val_acc:0.914]
Epoch [20/120    avg_loss:0.199, val_acc:0.935]
Epoch [21/120    avg_loss:0.190, val_acc:0.920]
Epoch [22/120    avg_loss:0.179, val_acc:0.885]
Epoch [23/120    avg_loss:0.138, val_acc:0.938]
Epoch [24/120    avg_loss:0.176, val_acc:0.910]
Epoch [25/120    avg_loss:0.213, val_acc:0.920]
Epoch [26/120    avg_loss:0.129, val_acc:0.917]
Epoch [27/120    avg_loss:0.138, val_acc:0.920]
Epoch [28/120    avg_loss:0.095, val_acc:0.948]
Epoch [29/120    avg_loss:0.122, val_acc:0.928]
Epoch [30/120    avg_loss:0.117, val_acc:0.934]
Epoch [31/120    avg_loss:0.080, val_acc:0.958]
Epoch [32/120    avg_loss:0.082, val_acc:0.959]
Epoch [33/120    avg_loss:0.068, val_acc:0.955]
Epoch [34/120    avg_loss:0.072, val_acc:0.951]
Epoch [35/120    avg_loss:0.059, val_acc:0.932]
Epoch [36/120    avg_loss:0.071, val_acc:0.963]
Epoch [37/120    avg_loss:0.111, val_acc:0.916]
Epoch [38/120    avg_loss:0.085, val_acc:0.957]
Epoch [39/120    avg_loss:0.068, val_acc:0.914]
Epoch [40/120    avg_loss:0.075, val_acc:0.959]
Epoch [41/120    avg_loss:0.135, val_acc:0.939]
Epoch [42/120    avg_loss:0.091, val_acc:0.949]
Epoch [43/120    avg_loss:0.087, val_acc:0.948]
Epoch [44/120    avg_loss:0.093, val_acc:0.945]
Epoch [45/120    avg_loss:0.121, val_acc:0.942]
Epoch [46/120    avg_loss:0.084, val_acc:0.953]
Epoch [47/120    avg_loss:0.082, val_acc:0.957]
Epoch [48/120    avg_loss:0.066, val_acc:0.968]
Epoch [49/120    avg_loss:0.059, val_acc:0.968]
Epoch [50/120    avg_loss:0.034, val_acc:0.973]
Epoch [51/120    avg_loss:0.021, val_acc:0.974]
Epoch [52/120    avg_loss:0.023, val_acc:0.968]
Epoch [53/120    avg_loss:0.022, val_acc:0.974]
Epoch [54/120    avg_loss:0.024, val_acc:0.968]
Epoch [55/120    avg_loss:0.028, val_acc:0.963]
Epoch [56/120    avg_loss:0.034, val_acc:0.945]
Epoch [57/120    avg_loss:0.055, val_acc:0.969]
Epoch [58/120    avg_loss:0.052, val_acc:0.947]
Epoch [59/120    avg_loss:0.060, val_acc:0.970]
Epoch [60/120    avg_loss:0.065, val_acc:0.941]
Epoch [61/120    avg_loss:0.049, val_acc:0.970]
Epoch [62/120    avg_loss:0.028, val_acc:0.973]
Epoch [63/120    avg_loss:0.026, val_acc:0.965]
Epoch [64/120    avg_loss:0.018, val_acc:0.974]
Epoch [65/120    avg_loss:0.048, val_acc:0.956]
Epoch [66/120    avg_loss:0.024, val_acc:0.973]
Epoch [67/120    avg_loss:0.024, val_acc:0.974]
Epoch [68/120    avg_loss:0.040, val_acc:0.976]
Epoch [69/120    avg_loss:0.050, val_acc:0.973]
Epoch [70/120    avg_loss:0.039, val_acc:0.970]
Epoch [71/120    avg_loss:0.018, val_acc:0.968]
Epoch [72/120    avg_loss:0.016, val_acc:0.974]
Epoch [73/120    avg_loss:0.012, val_acc:0.976]
Epoch [74/120    avg_loss:0.012, val_acc:0.967]
Epoch [75/120    avg_loss:0.014, val_acc:0.973]
Epoch [76/120    avg_loss:0.012, val_acc:0.977]
Epoch [77/120    avg_loss:0.016, val_acc:0.968]
Epoch [78/120    avg_loss:0.022, val_acc:0.969]
Epoch [79/120    avg_loss:0.017, val_acc:0.975]
Epoch [80/120    avg_loss:0.015, val_acc:0.980]
Epoch [81/120    avg_loss:0.018, val_acc:0.975]
Epoch [82/120    avg_loss:0.008, val_acc:0.973]
Epoch [83/120    avg_loss:0.021, val_acc:0.975]
Epoch [84/120    avg_loss:0.016, val_acc:0.973]
Epoch [85/120    avg_loss:0.014, val_acc:0.974]
Epoch [86/120    avg_loss:0.023, val_acc:0.976]
Epoch [87/120    avg_loss:0.011, val_acc:0.976]
Epoch [88/120    avg_loss:0.014, val_acc:0.978]
Epoch [89/120    avg_loss:0.008, val_acc:0.975]
Epoch [90/120    avg_loss:0.043, val_acc:0.947]
Epoch [91/120    avg_loss:0.021, val_acc:0.974]
Epoch [92/120    avg_loss:0.014, val_acc:0.966]
Epoch [93/120    avg_loss:0.012, val_acc:0.975]
Epoch [94/120    avg_loss:0.010, val_acc:0.977]
Epoch [95/120    avg_loss:0.010, val_acc:0.976]
Epoch [96/120    avg_loss:0.008, val_acc:0.976]
Epoch [97/120    avg_loss:0.006, val_acc:0.975]
Epoch [98/120    avg_loss:0.006, val_acc:0.974]
Epoch [99/120    avg_loss:0.010, val_acc:0.975]
Epoch [100/120    avg_loss:0.006, val_acc:0.976]
Epoch [101/120    avg_loss:0.005, val_acc:0.976]
Epoch [102/120    avg_loss:0.006, val_acc:0.976]
Epoch [103/120    avg_loss:0.006, val_acc:0.975]
Epoch [104/120    avg_loss:0.004, val_acc:0.976]
Epoch [105/120    avg_loss:0.005, val_acc:0.977]
Epoch [106/120    avg_loss:0.004, val_acc:0.978]
Epoch [107/120    avg_loss:0.005, val_acc:0.978]
Epoch [108/120    avg_loss:0.005, val_acc:0.977]
Epoch [109/120    avg_loss:0.006, val_acc:0.978]
Epoch [110/120    avg_loss:0.006, val_acc:0.978]
Epoch [111/120    avg_loss:0.007, val_acc:0.977]
Epoch [112/120    avg_loss:0.005, val_acc:0.977]
Epoch [113/120    avg_loss:0.005, val_acc:0.976]
Epoch [114/120    avg_loss:0.004, val_acc:0.976]
Epoch [115/120    avg_loss:0.004, val_acc:0.976]
Epoch [116/120    avg_loss:0.004, val_acc:0.976]
Epoch [117/120    avg_loss:0.005, val_acc:0.976]
Epoch [118/120    avg_loss:0.006, val_acc:0.976]
Epoch [119/120    avg_loss:0.007, val_acc:0.977]
Epoch [120/120    avg_loss:0.007, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1268    1    0    0    0    0    0    0    8    3    3    0
     0    2    0]
 [   0    0    0  713    3   19    0    0    0    6    0    0    5    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    1    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   14    0    0    2    0
     0    0    0]
 [   0    0   16   37    0    0   37    0    0    0  782    1    0    0
     0    2    0]
 [   0    0    7    0    0    0    7    0    0    0    8 2188    0    0
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0   11    2  511    0
     2    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    24  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.31165311653116

F1 scores:
[       nan 0.975      0.9837083  0.95066667 0.99300699 0.97181511
 0.95148443 0.98039216 0.99883586 0.68292683 0.92818991 0.99341657
 0.96872038 1.         0.98784722 0.92165899 0.96551724]

Kappa:
0.9693567070303225
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:25--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8abe86a748>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.277, val_acc:0.551]
Epoch [2/120    avg_loss:1.658, val_acc:0.631]
Epoch [3/120    avg_loss:1.282, val_acc:0.735]
Epoch [4/120    avg_loss:1.023, val_acc:0.631]
Epoch [5/120    avg_loss:1.102, val_acc:0.748]
Epoch [6/120    avg_loss:0.946, val_acc:0.741]
Epoch [7/120    avg_loss:0.663, val_acc:0.782]
Epoch [8/120    avg_loss:0.781, val_acc:0.791]
Epoch [9/120    avg_loss:0.667, val_acc:0.842]
Epoch [10/120    avg_loss:0.488, val_acc:0.829]
Epoch [11/120    avg_loss:0.454, val_acc:0.836]
Epoch [12/120    avg_loss:0.422, val_acc:0.844]
Epoch [13/120    avg_loss:0.427, val_acc:0.814]
Epoch [14/120    avg_loss:0.380, val_acc:0.799]
Epoch [15/120    avg_loss:0.399, val_acc:0.788]
Epoch [16/120    avg_loss:0.358, val_acc:0.860]
Epoch [17/120    avg_loss:0.255, val_acc:0.902]
Epoch [18/120    avg_loss:0.306, val_acc:0.892]
Epoch [19/120    avg_loss:0.277, val_acc:0.863]
Epoch [20/120    avg_loss:0.235, val_acc:0.896]
Epoch [21/120    avg_loss:0.206, val_acc:0.902]
Epoch [22/120    avg_loss:0.185, val_acc:0.907]
Epoch [23/120    avg_loss:0.163, val_acc:0.911]
Epoch [24/120    avg_loss:0.200, val_acc:0.917]
Epoch [25/120    avg_loss:0.145, val_acc:0.912]
Epoch [26/120    avg_loss:0.149, val_acc:0.917]
Epoch [27/120    avg_loss:0.106, val_acc:0.929]
Epoch [28/120    avg_loss:0.132, val_acc:0.930]
Epoch [29/120    avg_loss:0.131, val_acc:0.923]
Epoch [30/120    avg_loss:0.142, val_acc:0.897]
Epoch [31/120    avg_loss:0.124, val_acc:0.907]
Epoch [32/120    avg_loss:0.126, val_acc:0.917]
Epoch [33/120    avg_loss:0.097, val_acc:0.898]
Epoch [34/120    avg_loss:0.085, val_acc:0.946]
Epoch [35/120    avg_loss:0.091, val_acc:0.945]
Epoch [36/120    avg_loss:0.088, val_acc:0.931]
Epoch [37/120    avg_loss:0.099, val_acc:0.943]
Epoch [38/120    avg_loss:0.099, val_acc:0.925]
Epoch [39/120    avg_loss:0.077, val_acc:0.941]
Epoch [40/120    avg_loss:0.095, val_acc:0.932]
Epoch [41/120    avg_loss:0.070, val_acc:0.931]
Epoch [42/120    avg_loss:0.069, val_acc:0.932]
Epoch [43/120    avg_loss:0.073, val_acc:0.956]
Epoch [44/120    avg_loss:0.062, val_acc:0.940]
Epoch [45/120    avg_loss:0.050, val_acc:0.953]
Epoch [46/120    avg_loss:0.060, val_acc:0.948]
Epoch [47/120    avg_loss:0.053, val_acc:0.955]
Epoch [48/120    avg_loss:0.047, val_acc:0.958]
Epoch [49/120    avg_loss:0.042, val_acc:0.955]
Epoch [50/120    avg_loss:0.088, val_acc:0.956]
Epoch [51/120    avg_loss:0.064, val_acc:0.948]
Epoch [52/120    avg_loss:0.053, val_acc:0.959]
Epoch [53/120    avg_loss:0.061, val_acc:0.964]
Epoch [54/120    avg_loss:0.048, val_acc:0.956]
Epoch [55/120    avg_loss:0.053, val_acc:0.964]
Epoch [56/120    avg_loss:0.025, val_acc:0.956]
Epoch [57/120    avg_loss:0.026, val_acc:0.972]
Epoch [58/120    avg_loss:0.022, val_acc:0.974]
Epoch [59/120    avg_loss:0.028, val_acc:0.968]
Epoch [60/120    avg_loss:0.033, val_acc:0.969]
Epoch [61/120    avg_loss:0.022, val_acc:0.967]
Epoch [62/120    avg_loss:0.031, val_acc:0.972]
Epoch [63/120    avg_loss:0.022, val_acc:0.973]
Epoch [64/120    avg_loss:0.021, val_acc:0.965]
Epoch [65/120    avg_loss:0.012, val_acc:0.968]
Epoch [66/120    avg_loss:0.026, val_acc:0.978]
Epoch [67/120    avg_loss:0.024, val_acc:0.964]
Epoch [68/120    avg_loss:0.014, val_acc:0.975]
Epoch [69/120    avg_loss:0.020, val_acc:0.983]
Epoch [70/120    avg_loss:0.025, val_acc:0.961]
Epoch [71/120    avg_loss:0.026, val_acc:0.954]
Epoch [72/120    avg_loss:0.028, val_acc:0.968]
Epoch [73/120    avg_loss:0.020, val_acc:0.965]
Epoch [74/120    avg_loss:0.019, val_acc:0.972]
Epoch [75/120    avg_loss:0.012, val_acc:0.973]
Epoch [76/120    avg_loss:0.012, val_acc:0.973]
Epoch [77/120    avg_loss:0.013, val_acc:0.958]
Epoch [78/120    avg_loss:0.016, val_acc:0.975]
Epoch [79/120    avg_loss:0.011, val_acc:0.981]
Epoch [80/120    avg_loss:0.025, val_acc:0.969]
Epoch [81/120    avg_loss:0.017, val_acc:0.974]
Epoch [82/120    avg_loss:0.083, val_acc:0.954]
Epoch [83/120    avg_loss:0.040, val_acc:0.970]
Epoch [84/120    avg_loss:0.020, val_acc:0.974]
Epoch [85/120    avg_loss:0.019, val_acc:0.975]
Epoch [86/120    avg_loss:0.020, val_acc:0.975]
Epoch [87/120    avg_loss:0.018, val_acc:0.977]
Epoch [88/120    avg_loss:0.018, val_acc:0.980]
Epoch [89/120    avg_loss:0.009, val_acc:0.978]
Epoch [90/120    avg_loss:0.011, val_acc:0.979]
Epoch [91/120    avg_loss:0.011, val_acc:0.979]
Epoch [92/120    avg_loss:0.010, val_acc:0.980]
Epoch [93/120    avg_loss:0.011, val_acc:0.982]
Epoch [94/120    avg_loss:0.011, val_acc:0.981]
Epoch [95/120    avg_loss:0.008, val_acc:0.982]
Epoch [96/120    avg_loss:0.012, val_acc:0.982]
Epoch [97/120    avg_loss:0.009, val_acc:0.982]
Epoch [98/120    avg_loss:0.008, val_acc:0.981]
Epoch [99/120    avg_loss:0.009, val_acc:0.981]
Epoch [100/120    avg_loss:0.007, val_acc:0.981]
Epoch [101/120    avg_loss:0.007, val_acc:0.981]
Epoch [102/120    avg_loss:0.009, val_acc:0.981]
Epoch [103/120    avg_loss:0.008, val_acc:0.981]
Epoch [104/120    avg_loss:0.009, val_acc:0.982]
Epoch [105/120    avg_loss:0.008, val_acc:0.982]
Epoch [106/120    avg_loss:0.009, val_acc:0.982]
Epoch [107/120    avg_loss:0.008, val_acc:0.982]
Epoch [108/120    avg_loss:0.011, val_acc:0.981]
Epoch [109/120    avg_loss:0.011, val_acc:0.981]
Epoch [110/120    avg_loss:0.008, val_acc:0.981]
Epoch [111/120    avg_loss:0.008, val_acc:0.981]
Epoch [112/120    avg_loss:0.009, val_acc:0.981]
Epoch [113/120    avg_loss:0.009, val_acc:0.981]
Epoch [114/120    avg_loss:0.006, val_acc:0.981]
Epoch [115/120    avg_loss:0.008, val_acc:0.981]
Epoch [116/120    avg_loss:0.008, val_acc:0.981]
Epoch [117/120    avg_loss:0.009, val_acc:0.981]
Epoch [118/120    avg_loss:0.012, val_acc:0.981]
Epoch [119/120    avg_loss:0.011, val_acc:0.981]
Epoch [120/120    avg_loss:0.007, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1272    0    0    0    0    0    0    0    4    5    4    0
     0    0    0]
 [   0    0    0  719    0   15    0    0    0    6    0    0    7    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   13    0    0    1    0
     0    0    0]
 [   0    0    1   67    0    4    0    0    0    0  779   23    1    0
     0    0    0]
 [   0    0    6    0    4    0    4    0    0    0   10 2186    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    7    0  518    0
     1    0    5]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    2    0    0    1    0    1    1    0    0
  1134    0    0]
 [   0    0    0    0    0    0   39    0    0    0    0    0    1    0
    45  262    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.9430894308943

F1 scores:
[       nan 0.975      0.99142634 0.93558881 0.99069767 0.96629213
 0.96755162 0.98039216 0.99883856 0.63414634 0.92959427 0.98779937
 0.97094658 0.99728997 0.97800776 0.86042693 0.96511628]

Kappa:
0.9651344680786619
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0978431630>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.383, val_acc:0.432]
Epoch [2/120    avg_loss:1.871, val_acc:0.513]
Epoch [3/120    avg_loss:1.479, val_acc:0.607]
Epoch [4/120    avg_loss:1.270, val_acc:0.688]
Epoch [5/120    avg_loss:0.954, val_acc:0.717]
Epoch [6/120    avg_loss:0.899, val_acc:0.739]
Epoch [7/120    avg_loss:0.846, val_acc:0.704]
Epoch [8/120    avg_loss:0.626, val_acc:0.675]
Epoch [9/120    avg_loss:0.636, val_acc:0.791]
Epoch [10/120    avg_loss:0.787, val_acc:0.752]
Epoch [11/120    avg_loss:0.673, val_acc:0.825]
Epoch [12/120    avg_loss:0.491, val_acc:0.852]
Epoch [13/120    avg_loss:0.365, val_acc:0.846]
Epoch [14/120    avg_loss:0.411, val_acc:0.806]
Epoch [15/120    avg_loss:0.367, val_acc:0.871]
Epoch [16/120    avg_loss:0.386, val_acc:0.896]
Epoch [17/120    avg_loss:0.326, val_acc:0.885]
Epoch [18/120    avg_loss:0.288, val_acc:0.917]
Epoch [19/120    avg_loss:0.215, val_acc:0.921]
Epoch [20/120    avg_loss:0.191, val_acc:0.932]
Epoch [21/120    avg_loss:0.195, val_acc:0.908]
Epoch [22/120    avg_loss:0.262, val_acc:0.913]
Epoch [23/120    avg_loss:0.148, val_acc:0.925]
Epoch [24/120    avg_loss:0.146, val_acc:0.930]
Epoch [25/120    avg_loss:0.153, val_acc:0.931]
Epoch [26/120    avg_loss:0.311, val_acc:0.828]
Epoch [27/120    avg_loss:0.254, val_acc:0.930]
Epoch [28/120    avg_loss:0.172, val_acc:0.929]
Epoch [29/120    avg_loss:0.149, val_acc:0.905]
Epoch [30/120    avg_loss:0.131, val_acc:0.945]
Epoch [31/120    avg_loss:0.133, val_acc:0.938]
Epoch [32/120    avg_loss:0.131, val_acc:0.923]
Epoch [33/120    avg_loss:0.121, val_acc:0.926]
Epoch [34/120    avg_loss:0.115, val_acc:0.942]
Epoch [35/120    avg_loss:0.159, val_acc:0.910]
Epoch [36/120    avg_loss:0.143, val_acc:0.948]
Epoch [37/120    avg_loss:0.111, val_acc:0.945]
Epoch [38/120    avg_loss:0.075, val_acc:0.964]
Epoch [39/120    avg_loss:0.061, val_acc:0.953]
Epoch [40/120    avg_loss:0.068, val_acc:0.949]
Epoch [41/120    avg_loss:0.078, val_acc:0.960]
Epoch [42/120    avg_loss:0.059, val_acc:0.951]
Epoch [43/120    avg_loss:0.056, val_acc:0.964]
Epoch [44/120    avg_loss:0.072, val_acc:0.969]
Epoch [45/120    avg_loss:0.043, val_acc:0.962]
Epoch [46/120    avg_loss:0.052, val_acc:0.968]
Epoch [47/120    avg_loss:0.047, val_acc:0.969]
Epoch [48/120    avg_loss:0.045, val_acc:0.955]
Epoch [49/120    avg_loss:0.041, val_acc:0.974]
Epoch [50/120    avg_loss:0.041, val_acc:0.970]
Epoch [51/120    avg_loss:0.079, val_acc:0.952]
Epoch [52/120    avg_loss:0.075, val_acc:0.963]
Epoch [53/120    avg_loss:0.035, val_acc:0.984]
Epoch [54/120    avg_loss:0.042, val_acc:0.971]
Epoch [55/120    avg_loss:0.049, val_acc:0.972]
Epoch [56/120    avg_loss:0.051, val_acc:0.934]
Epoch [57/120    avg_loss:0.067, val_acc:0.964]
Epoch [58/120    avg_loss:0.106, val_acc:0.964]
Epoch [59/120    avg_loss:0.045, val_acc:0.964]
Epoch [60/120    avg_loss:0.051, val_acc:0.964]
Epoch [61/120    avg_loss:0.030, val_acc:0.972]
Epoch [62/120    avg_loss:0.032, val_acc:0.973]
Epoch [63/120    avg_loss:0.023, val_acc:0.972]
Epoch [64/120    avg_loss:0.036, val_acc:0.981]
Epoch [65/120    avg_loss:0.020, val_acc:0.981]
Epoch [66/120    avg_loss:0.023, val_acc:0.988]
Epoch [67/120    avg_loss:0.028, val_acc:0.982]
Epoch [68/120    avg_loss:0.036, val_acc:0.978]
Epoch [69/120    avg_loss:0.020, val_acc:0.979]
Epoch [70/120    avg_loss:0.028, val_acc:0.979]
Epoch [71/120    avg_loss:0.034, val_acc:0.984]
Epoch [72/120    avg_loss:0.025, val_acc:0.984]
Epoch [73/120    avg_loss:0.025, val_acc:0.974]
Epoch [74/120    avg_loss:0.029, val_acc:0.970]
Epoch [75/120    avg_loss:0.026, val_acc:0.982]
Epoch [76/120    avg_loss:0.020, val_acc:0.977]
Epoch [77/120    avg_loss:0.019, val_acc:0.977]
Epoch [78/120    avg_loss:0.020, val_acc:0.989]
Epoch [79/120    avg_loss:0.011, val_acc:0.983]
Epoch [80/120    avg_loss:0.014, val_acc:0.981]
Epoch [81/120    avg_loss:0.012, val_acc:0.979]
Epoch [82/120    avg_loss:0.011, val_acc:0.987]
Epoch [83/120    avg_loss:0.010, val_acc:0.983]
Epoch [84/120    avg_loss:0.008, val_acc:0.987]
Epoch [85/120    avg_loss:0.009, val_acc:0.985]
Epoch [86/120    avg_loss:0.008, val_acc:0.984]
Epoch [87/120    avg_loss:0.008, val_acc:0.989]
Epoch [88/120    avg_loss:0.012, val_acc:0.981]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.008, val_acc:0.988]
Epoch [92/120    avg_loss:0.008, val_acc:0.989]
Epoch [93/120    avg_loss:0.008, val_acc:0.982]
Epoch [94/120    avg_loss:0.016, val_acc:0.982]
Epoch [95/120    avg_loss:0.008, val_acc:0.989]
Epoch [96/120    avg_loss:0.010, val_acc:0.987]
Epoch [97/120    avg_loss:0.016, val_acc:0.987]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.009, val_acc:0.985]
Epoch [100/120    avg_loss:0.009, val_acc:0.987]
Epoch [101/120    avg_loss:0.009, val_acc:0.987]
Epoch [102/120    avg_loss:0.008, val_acc:0.990]
Epoch [103/120    avg_loss:0.012, val_acc:0.985]
Epoch [104/120    avg_loss:0.007, val_acc:0.989]
Epoch [105/120    avg_loss:0.006, val_acc:0.993]
Epoch [106/120    avg_loss:0.006, val_acc:0.987]
Epoch [107/120    avg_loss:0.004, val_acc:0.990]
Epoch [108/120    avg_loss:0.008, val_acc:0.987]
Epoch [109/120    avg_loss:0.008, val_acc:0.990]
Epoch [110/120    avg_loss:0.005, val_acc:0.991]
Epoch [111/120    avg_loss:0.004, val_acc:0.991]
Epoch [112/120    avg_loss:0.006, val_acc:0.992]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.025, val_acc:0.973]
Epoch [115/120    avg_loss:0.026, val_acc:0.977]
Epoch [116/120    avg_loss:0.019, val_acc:0.980]
Epoch [117/120    avg_loss:0.014, val_acc:0.987]
Epoch [118/120    avg_loss:0.008, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1256    0    0    0    0    0    0    0   16    5    5    0
     0    3    0]
 [   0    0    0  718    0   22    0    0    0    6    1    0    0    0
     0    0    0]
 [   0    0    0    0  210    0    0    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    0   58    0    1    4    0    0    0  788   23    1    0
     0    0    0]
 [   0    0    1    0    0    2    2    0    0    0    2 2203    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0   10    7  509    0
     3    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    1    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0   36    0    0    0    0    0    0    0
    21  290    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.27913279132791

F1 scores:
[       nan 0.98765432 0.98780967 0.94040602 0.9929078  0.96329255
 0.96826568 1.         1.         0.71794872 0.93089191 0.98966757
 0.96860133 1.         0.98738582 0.90625    0.97619048]

Kappa:
0.9689605856980202
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:55
Validation dataloader:55
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:16
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff1da3d5710>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.151, val_acc:0.540]
Epoch [2/120    avg_loss:1.922, val_acc:0.495]
Epoch [3/120    avg_loss:1.332, val_acc:0.684]
Epoch [4/120    avg_loss:1.002, val_acc:0.714]
Epoch [5/120    avg_loss:0.981, val_acc:0.749]
Epoch [6/120    avg_loss:1.017, val_acc:0.756]
Epoch [7/120    avg_loss:0.694, val_acc:0.827]
Epoch [8/120    avg_loss:0.977, val_acc:0.742]
Epoch [9/120    avg_loss:0.516, val_acc:0.803]
Epoch [10/120    avg_loss:0.682, val_acc:0.745]
Epoch [11/120    avg_loss:0.613, val_acc:0.823]
Epoch [12/120    avg_loss:0.401, val_acc:0.872]
Epoch [13/120    avg_loss:0.309, val_acc:0.870]
Epoch [14/120    avg_loss:0.346, val_acc:0.867]
Epoch [15/120    avg_loss:0.390, val_acc:0.848]
Epoch [16/120    avg_loss:0.341, val_acc:0.883]
Epoch [17/120    avg_loss:0.246, val_acc:0.905]
Epoch [18/120    avg_loss:0.262, val_acc:0.892]
Epoch [19/120    avg_loss:0.396, val_acc:0.878]
Epoch [20/120    avg_loss:0.297, val_acc:0.922]
Epoch [21/120    avg_loss:0.170, val_acc:0.917]
Epoch [22/120    avg_loss:0.215, val_acc:0.919]
Epoch [23/120    avg_loss:0.244, val_acc:0.924]
Epoch [24/120    avg_loss:0.225, val_acc:0.918]
Epoch [25/120    avg_loss:0.247, val_acc:0.930]
Epoch [26/120    avg_loss:0.183, val_acc:0.912]
Epoch [27/120    avg_loss:0.142, val_acc:0.924]
Epoch [28/120    avg_loss:0.157, val_acc:0.934]
Epoch [29/120    avg_loss:0.132, val_acc:0.948]
Epoch [30/120    avg_loss:0.090, val_acc:0.930]
Epoch [31/120    avg_loss:0.081, val_acc:0.924]
Epoch [32/120    avg_loss:0.115, val_acc:0.928]
Epoch [33/120    avg_loss:0.136, val_acc:0.926]
Epoch [34/120    avg_loss:0.101, val_acc:0.963]
Epoch [35/120    avg_loss:0.074, val_acc:0.968]
Epoch [36/120    avg_loss:0.073, val_acc:0.963]
Epoch [37/120    avg_loss:0.058, val_acc:0.953]
Epoch [38/120    avg_loss:0.104, val_acc:0.942]
Epoch [39/120    avg_loss:0.093, val_acc:0.959]
Epoch [40/120    avg_loss:0.057, val_acc:0.958]
Epoch [41/120    avg_loss:0.063, val_acc:0.959]
Epoch [42/120    avg_loss:0.064, val_acc:0.959]
Epoch [43/120    avg_loss:0.072, val_acc:0.955]
Epoch [44/120    avg_loss:0.056, val_acc:0.952]
Epoch [45/120    avg_loss:0.063, val_acc:0.969]
Epoch [46/120    avg_loss:0.048, val_acc:0.957]
Epoch [47/120    avg_loss:0.041, val_acc:0.969]
Epoch [48/120    avg_loss:0.133, val_acc:0.932]
Epoch [49/120    avg_loss:0.138, val_acc:0.951]
Epoch [50/120    avg_loss:0.066, val_acc:0.955]
Epoch [51/120    avg_loss:0.056, val_acc:0.957]
Epoch [52/120    avg_loss:0.106, val_acc:0.944]
Epoch [53/120    avg_loss:0.076, val_acc:0.943]
Epoch [54/120    avg_loss:0.066, val_acc:0.965]
Epoch [55/120    avg_loss:0.067, val_acc:0.950]
Epoch [56/120    avg_loss:0.060, val_acc:0.955]
Epoch [57/120    avg_loss:0.071, val_acc:0.957]
Epoch [58/120    avg_loss:0.038, val_acc:0.965]
Epoch [59/120    avg_loss:0.150, val_acc:0.933]
Epoch [60/120    avg_loss:0.069, val_acc:0.953]
Epoch [61/120    avg_loss:0.053, val_acc:0.969]
Epoch [62/120    avg_loss:0.038, val_acc:0.974]
Epoch [63/120    avg_loss:0.032, val_acc:0.974]
Epoch [64/120    avg_loss:0.041, val_acc:0.976]
Epoch [65/120    avg_loss:0.037, val_acc:0.977]
Epoch [66/120    avg_loss:0.027, val_acc:0.977]
Epoch [67/120    avg_loss:0.020, val_acc:0.977]
Epoch [68/120    avg_loss:0.024, val_acc:0.973]
Epoch [69/120    avg_loss:0.022, val_acc:0.972]
Epoch [70/120    avg_loss:0.036, val_acc:0.976]
Epoch [71/120    avg_loss:0.028, val_acc:0.976]
Epoch [72/120    avg_loss:0.023, val_acc:0.976]
Epoch [73/120    avg_loss:0.019, val_acc:0.973]
Epoch [74/120    avg_loss:0.027, val_acc:0.975]
Epoch [75/120    avg_loss:0.019, val_acc:0.974]
Epoch [76/120    avg_loss:0.022, val_acc:0.976]
Epoch [77/120    avg_loss:0.021, val_acc:0.975]
Epoch [78/120    avg_loss:0.024, val_acc:0.976]
Epoch [79/120    avg_loss:0.017, val_acc:0.976]
Epoch [80/120    avg_loss:0.016, val_acc:0.977]
Epoch [81/120    avg_loss:0.019, val_acc:0.977]
Epoch [82/120    avg_loss:0.018, val_acc:0.977]
Epoch [83/120    avg_loss:0.015, val_acc:0.977]
Epoch [84/120    avg_loss:0.025, val_acc:0.976]
Epoch [85/120    avg_loss:0.019, val_acc:0.976]
Epoch [86/120    avg_loss:0.021, val_acc:0.974]
Epoch [87/120    avg_loss:0.023, val_acc:0.975]
Epoch [88/120    avg_loss:0.021, val_acc:0.975]
Epoch [89/120    avg_loss:0.019, val_acc:0.975]
Epoch [90/120    avg_loss:0.018, val_acc:0.974]
Epoch [91/120    avg_loss:0.019, val_acc:0.978]
Epoch [92/120    avg_loss:0.021, val_acc:0.976]
Epoch [93/120    avg_loss:0.016, val_acc:0.978]
Epoch [94/120    avg_loss:0.017, val_acc:0.977]
Epoch [95/120    avg_loss:0.016, val_acc:0.978]
Epoch [96/120    avg_loss:0.020, val_acc:0.977]
Epoch [97/120    avg_loss:0.020, val_acc:0.976]
Epoch [98/120    avg_loss:0.023, val_acc:0.978]
Epoch [99/120    avg_loss:0.026, val_acc:0.976]
Epoch [100/120    avg_loss:0.017, val_acc:0.975]
Epoch [101/120    avg_loss:0.015, val_acc:0.975]
Epoch [102/120    avg_loss:0.014, val_acc:0.975]
Epoch [103/120    avg_loss:0.019, val_acc:0.975]
Epoch [104/120    avg_loss:0.027, val_acc:0.975]
Epoch [105/120    avg_loss:0.016, val_acc:0.976]
Epoch [106/120    avg_loss:0.016, val_acc:0.977]
Epoch [107/120    avg_loss:0.019, val_acc:0.977]
Epoch [108/120    avg_loss:0.017, val_acc:0.978]
Epoch [109/120    avg_loss:0.015, val_acc:0.978]
Epoch [110/120    avg_loss:0.022, val_acc:0.980]
Epoch [111/120    avg_loss:0.016, val_acc:0.980]
Epoch [112/120    avg_loss:0.012, val_acc:0.980]
Epoch [113/120    avg_loss:0.014, val_acc:0.982]
Epoch [114/120    avg_loss:0.011, val_acc:0.984]
Epoch [115/120    avg_loss:0.023, val_acc:0.982]
Epoch [116/120    avg_loss:0.014, val_acc:0.983]
Epoch [117/120    avg_loss:0.014, val_acc:0.983]
Epoch [118/120    avg_loss:0.010, val_acc:0.982]
Epoch [119/120    avg_loss:0.016, val_acc:0.983]
Epoch [120/120    avg_loss:0.021, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1248    5    0    0    0    0    0    0   16    5    7    0
     0    3    1]
 [   0    0    0  728    0   11    0    0    0    6    0    0    1    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    3    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    7    0    0    0    0    0   10    0    0    1    0
     0    0    0]
 [   0    0    2   61    0    1    2    0    0    0  778   27    2    0
     0    2    0]
 [   0    0   11    0    0    5    9    0    0    0   15 2169    0    1
     0    0    0]
 [   0    0    0    4    3    0    0    0    0    0    6    2  504    0
     1    0   14]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   31    0    0    0    0    0    0   23
    37  256    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.40108401084011

F1 scores:
[       nan 0.98765432 0.97997644 0.93814433 0.99300699 0.97505669
 0.96902655 0.98039216 0.99883856 0.54054054 0.91962175 0.98300476
 0.96091516 0.93670886 0.98184961 0.84210526 0.91803279]

Kappa:
0.9589755000807778
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9e5ac266a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.246, val_acc:0.524]
Epoch [2/120    avg_loss:1.729, val_acc:0.542]
Epoch [3/120    avg_loss:1.349, val_acc:0.726]
Epoch [4/120    avg_loss:1.191, val_acc:0.717]
Epoch [5/120    avg_loss:0.880, val_acc:0.711]
Epoch [6/120    avg_loss:0.809, val_acc:0.810]
Epoch [7/120    avg_loss:0.781, val_acc:0.783]
Epoch [8/120    avg_loss:0.753, val_acc:0.794]
Epoch [9/120    avg_loss:0.588, val_acc:0.867]
Epoch [10/120    avg_loss:0.400, val_acc:0.860]
Epoch [11/120    avg_loss:0.296, val_acc:0.843]
Epoch [12/120    avg_loss:0.313, val_acc:0.887]
Epoch [13/120    avg_loss:0.389, val_acc:0.878]
Epoch [14/120    avg_loss:0.364, val_acc:0.867]
Epoch [15/120    avg_loss:0.342, val_acc:0.826]
Epoch [16/120    avg_loss:0.336, val_acc:0.914]
Epoch [17/120    avg_loss:0.178, val_acc:0.906]
Epoch [18/120    avg_loss:0.188, val_acc:0.921]
Epoch [19/120    avg_loss:0.118, val_acc:0.944]
Epoch [20/120    avg_loss:0.092, val_acc:0.929]
Epoch [21/120    avg_loss:0.101, val_acc:0.931]
Epoch [22/120    avg_loss:0.083, val_acc:0.934]
Epoch [23/120    avg_loss:0.082, val_acc:0.946]
Epoch [24/120    avg_loss:0.153, val_acc:0.907]
Epoch [25/120    avg_loss:0.083, val_acc:0.955]
Epoch [26/120    avg_loss:0.045, val_acc:0.958]
Epoch [27/120    avg_loss:0.115, val_acc:0.931]
Epoch [28/120    avg_loss:0.085, val_acc:0.943]
Epoch [29/120    avg_loss:0.079, val_acc:0.951]
Epoch [30/120    avg_loss:0.068, val_acc:0.943]
Epoch [31/120    avg_loss:0.043, val_acc:0.958]
Epoch [32/120    avg_loss:0.048, val_acc:0.959]
Epoch [33/120    avg_loss:0.033, val_acc:0.952]
Epoch [34/120    avg_loss:0.054, val_acc:0.968]
Epoch [35/120    avg_loss:0.044, val_acc:0.964]
Epoch [36/120    avg_loss:0.041, val_acc:0.941]
Epoch [37/120    avg_loss:0.039, val_acc:0.967]
Epoch [38/120    avg_loss:0.031, val_acc:0.968]
Epoch [39/120    avg_loss:0.022, val_acc:0.967]
Epoch [40/120    avg_loss:0.028, val_acc:0.965]
Epoch [41/120    avg_loss:0.014, val_acc:0.974]
Epoch [42/120    avg_loss:0.011, val_acc:0.972]
Epoch [43/120    avg_loss:0.012, val_acc:0.972]
Epoch [44/120    avg_loss:0.015, val_acc:0.965]
Epoch [45/120    avg_loss:0.014, val_acc:0.967]
Epoch [46/120    avg_loss:0.011, val_acc:0.972]
Epoch [47/120    avg_loss:0.010, val_acc:0.971]
Epoch [48/120    avg_loss:0.009, val_acc:0.969]
Epoch [49/120    avg_loss:0.012, val_acc:0.972]
Epoch [50/120    avg_loss:0.019, val_acc:0.975]
Epoch [51/120    avg_loss:0.011, val_acc:0.969]
Epoch [52/120    avg_loss:0.013, val_acc:0.978]
Epoch [53/120    avg_loss:0.028, val_acc:0.971]
Epoch [54/120    avg_loss:0.016, val_acc:0.967]
Epoch [55/120    avg_loss:0.011, val_acc:0.975]
Epoch [56/120    avg_loss:0.009, val_acc:0.974]
Epoch [57/120    avg_loss:0.007, val_acc:0.967]
Epoch [58/120    avg_loss:0.007, val_acc:0.977]
Epoch [59/120    avg_loss:0.012, val_acc:0.979]
Epoch [60/120    avg_loss:0.017, val_acc:0.970]
Epoch [61/120    avg_loss:0.009, val_acc:0.980]
Epoch [62/120    avg_loss:0.020, val_acc:0.970]
Epoch [63/120    avg_loss:0.008, val_acc:0.982]
Epoch [64/120    avg_loss:0.014, val_acc:0.971]
Epoch [65/120    avg_loss:0.036, val_acc:0.959]
Epoch [66/120    avg_loss:0.034, val_acc:0.978]
Epoch [67/120    avg_loss:0.025, val_acc:0.972]
Epoch [68/120    avg_loss:0.026, val_acc:0.948]
Epoch [69/120    avg_loss:0.025, val_acc:0.969]
Epoch [70/120    avg_loss:0.015, val_acc:0.969]
Epoch [71/120    avg_loss:0.013, val_acc:0.977]
Epoch [72/120    avg_loss:0.016, val_acc:0.978]
Epoch [73/120    avg_loss:0.005, val_acc:0.977]
Epoch [74/120    avg_loss:0.007, val_acc:0.977]
Epoch [75/120    avg_loss:0.006, val_acc:0.979]
Epoch [76/120    avg_loss:0.005, val_acc:0.981]
Epoch [77/120    avg_loss:0.003, val_acc:0.982]
Epoch [78/120    avg_loss:0.003, val_acc:0.983]
Epoch [79/120    avg_loss:0.008, val_acc:0.981]
Epoch [80/120    avg_loss:0.003, val_acc:0.978]
Epoch [81/120    avg_loss:0.004, val_acc:0.978]
Epoch [82/120    avg_loss:0.005, val_acc:0.978]
Epoch [83/120    avg_loss:0.004, val_acc:0.978]
Epoch [84/120    avg_loss:0.004, val_acc:0.978]
Epoch [85/120    avg_loss:0.003, val_acc:0.978]
Epoch [86/120    avg_loss:0.003, val_acc:0.978]
Epoch [87/120    avg_loss:0.003, val_acc:0.978]
Epoch [88/120    avg_loss:0.003, val_acc:0.979]
Epoch [89/120    avg_loss:0.003, val_acc:0.979]
Epoch [90/120    avg_loss:0.003, val_acc:0.979]
Epoch [91/120    avg_loss:0.002, val_acc:0.980]
Epoch [92/120    avg_loss:0.003, val_acc:0.980]
Epoch [93/120    avg_loss:0.006, val_acc:0.979]
Epoch [94/120    avg_loss:0.004, val_acc:0.979]
Epoch [95/120    avg_loss:0.004, val_acc:0.979]
Epoch [96/120    avg_loss:0.005, val_acc:0.980]
Epoch [97/120    avg_loss:0.003, val_acc:0.979]
Epoch [98/120    avg_loss:0.004, val_acc:0.979]
Epoch [99/120    avg_loss:0.003, val_acc:0.980]
Epoch [100/120    avg_loss:0.003, val_acc:0.980]
Epoch [101/120    avg_loss:0.003, val_acc:0.980]
Epoch [102/120    avg_loss:0.003, val_acc:0.980]
Epoch [103/120    avg_loss:0.004, val_acc:0.979]
Epoch [104/120    avg_loss:0.004, val_acc:0.979]
Epoch [105/120    avg_loss:0.003, val_acc:0.979]
Epoch [106/120    avg_loss:0.003, val_acc:0.979]
Epoch [107/120    avg_loss:0.004, val_acc:0.979]
Epoch [108/120    avg_loss:0.003, val_acc:0.979]
Epoch [109/120    avg_loss:0.005, val_acc:0.979]
Epoch [110/120    avg_loss:0.004, val_acc:0.979]
Epoch [111/120    avg_loss:0.003, val_acc:0.979]
Epoch [112/120    avg_loss:0.005, val_acc:0.979]
Epoch [113/120    avg_loss:0.009, val_acc:0.979]
Epoch [114/120    avg_loss:0.003, val_acc:0.979]
Epoch [115/120    avg_loss:0.005, val_acc:0.979]
Epoch [116/120    avg_loss:0.004, val_acc:0.979]
Epoch [117/120    avg_loss:0.003, val_acc:0.979]
Epoch [118/120    avg_loss:0.005, val_acc:0.979]
Epoch [119/120    avg_loss:0.013, val_acc:0.979]
Epoch [120/120    avg_loss:0.004, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1272    1    0    1    2    0    0    0    3    2    4    0
     0    0    0]
 [   0    0    0  730    1    0    0    0    0    2    1    5    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    1    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0  845   20    6    0
     0    3    0]
 [   0    0    8    0    0    0    1    0    0    0    1 2176   24    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0    0    0    0  528    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1127   12    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    52  287    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.07046070460704

F1 scores:
[       nan 0.975      0.99104012 0.9851552  0.99765808 0.99654776
 0.99093656 1.         1.         0.92307692 0.97914253 0.98595378
 0.95565611 1.         0.97197068 0.8844376  0.98224852]

Kappa:
0.9779985033274374
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fae56fd25f8>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.197, val_acc:0.535]
Epoch [2/120    avg_loss:1.674, val_acc:0.528]
Epoch [3/120    avg_loss:1.427, val_acc:0.559]
Epoch [4/120    avg_loss:1.131, val_acc:0.735]
Epoch [5/120    avg_loss:0.885, val_acc:0.770]
Epoch [6/120    avg_loss:0.831, val_acc:0.741]
Epoch [7/120    avg_loss:0.550, val_acc:0.817]
Epoch [8/120    avg_loss:0.504, val_acc:0.800]
Epoch [9/120    avg_loss:0.379, val_acc:0.812]
Epoch [10/120    avg_loss:0.359, val_acc:0.838]
Epoch [11/120    avg_loss:0.294, val_acc:0.859]
Epoch [12/120    avg_loss:0.266, val_acc:0.894]
Epoch [13/120    avg_loss:0.172, val_acc:0.898]
Epoch [14/120    avg_loss:0.137, val_acc:0.919]
Epoch [15/120    avg_loss:0.125, val_acc:0.897]
Epoch [16/120    avg_loss:0.146, val_acc:0.894]
Epoch [17/120    avg_loss:0.185, val_acc:0.924]
Epoch [18/120    avg_loss:0.097, val_acc:0.938]
Epoch [19/120    avg_loss:0.079, val_acc:0.945]
Epoch [20/120    avg_loss:0.064, val_acc:0.954]
Epoch [21/120    avg_loss:0.055, val_acc:0.960]
Epoch [22/120    avg_loss:0.056, val_acc:0.922]
Epoch [23/120    avg_loss:0.081, val_acc:0.964]
Epoch [24/120    avg_loss:0.057, val_acc:0.941]
Epoch [25/120    avg_loss:0.043, val_acc:0.959]
Epoch [26/120    avg_loss:0.038, val_acc:0.964]
Epoch [27/120    avg_loss:0.043, val_acc:0.965]
Epoch [28/120    avg_loss:0.045, val_acc:0.972]
Epoch [29/120    avg_loss:0.049, val_acc:0.949]
Epoch [30/120    avg_loss:0.044, val_acc:0.967]
Epoch [31/120    avg_loss:0.031, val_acc:0.965]
Epoch [32/120    avg_loss:0.057, val_acc:0.951]
Epoch [33/120    avg_loss:0.065, val_acc:0.947]
Epoch [34/120    avg_loss:0.108, val_acc:0.941]
Epoch [35/120    avg_loss:0.054, val_acc:0.959]
Epoch [36/120    avg_loss:0.038, val_acc:0.970]
Epoch [37/120    avg_loss:0.017, val_acc:0.974]
Epoch [38/120    avg_loss:0.019, val_acc:0.973]
Epoch [39/120    avg_loss:0.013, val_acc:0.971]
Epoch [40/120    avg_loss:0.009, val_acc:0.968]
Epoch [41/120    avg_loss:0.011, val_acc:0.969]
Epoch [42/120    avg_loss:0.012, val_acc:0.972]
Epoch [43/120    avg_loss:0.015, val_acc:0.978]
Epoch [44/120    avg_loss:0.023, val_acc:0.956]
Epoch [45/120    avg_loss:0.016, val_acc:0.974]
Epoch [46/120    avg_loss:0.015, val_acc:0.976]
Epoch [47/120    avg_loss:0.009, val_acc:0.973]
Epoch [48/120    avg_loss:0.009, val_acc:0.975]
Epoch [49/120    avg_loss:0.010, val_acc:0.975]
Epoch [50/120    avg_loss:0.005, val_acc:0.979]
Epoch [51/120    avg_loss:0.009, val_acc:0.981]
Epoch [52/120    avg_loss:0.008, val_acc:0.974]
Epoch [53/120    avg_loss:0.018, val_acc:0.975]
Epoch [54/120    avg_loss:0.021, val_acc:0.973]
Epoch [55/120    avg_loss:0.026, val_acc:0.973]
Epoch [56/120    avg_loss:0.029, val_acc:0.970]
Epoch [57/120    avg_loss:0.015, val_acc:0.973]
Epoch [58/120    avg_loss:0.011, val_acc:0.975]
Epoch [59/120    avg_loss:0.038, val_acc:0.972]
Epoch [60/120    avg_loss:0.032, val_acc:0.961]
Epoch [61/120    avg_loss:0.044, val_acc:0.965]
Epoch [62/120    avg_loss:0.023, val_acc:0.969]
Epoch [63/120    avg_loss:0.022, val_acc:0.970]
Epoch [64/120    avg_loss:0.011, val_acc:0.980]
Epoch [65/120    avg_loss:0.009, val_acc:0.979]
Epoch [66/120    avg_loss:0.006, val_acc:0.978]
Epoch [67/120    avg_loss:0.011, val_acc:0.980]
Epoch [68/120    avg_loss:0.005, val_acc:0.981]
Epoch [69/120    avg_loss:0.004, val_acc:0.981]
Epoch [70/120    avg_loss:0.004, val_acc:0.981]
Epoch [71/120    avg_loss:0.004, val_acc:0.980]
Epoch [72/120    avg_loss:0.008, val_acc:0.980]
Epoch [73/120    avg_loss:0.005, val_acc:0.979]
Epoch [74/120    avg_loss:0.006, val_acc:0.982]
Epoch [75/120    avg_loss:0.007, val_acc:0.981]
Epoch [76/120    avg_loss:0.004, val_acc:0.981]
Epoch [77/120    avg_loss:0.004, val_acc:0.981]
Epoch [78/120    avg_loss:0.006, val_acc:0.980]
Epoch [79/120    avg_loss:0.006, val_acc:0.981]
Epoch [80/120    avg_loss:0.005, val_acc:0.982]
Epoch [81/120    avg_loss:0.008, val_acc:0.983]
Epoch [82/120    avg_loss:0.007, val_acc:0.981]
Epoch [83/120    avg_loss:0.004, val_acc:0.982]
Epoch [84/120    avg_loss:0.007, val_acc:0.983]
Epoch [85/120    avg_loss:0.004, val_acc:0.983]
Epoch [86/120    avg_loss:0.004, val_acc:0.983]
Epoch [87/120    avg_loss:0.003, val_acc:0.982]
Epoch [88/120    avg_loss:0.004, val_acc:0.983]
Epoch [89/120    avg_loss:0.005, val_acc:0.983]
Epoch [90/120    avg_loss:0.006, val_acc:0.982]
Epoch [91/120    avg_loss:0.004, val_acc:0.982]
Epoch [92/120    avg_loss:0.004, val_acc:0.982]
Epoch [93/120    avg_loss:0.006, val_acc:0.982]
Epoch [94/120    avg_loss:0.004, val_acc:0.982]
Epoch [95/120    avg_loss:0.004, val_acc:0.982]
Epoch [96/120    avg_loss:0.004, val_acc:0.983]
Epoch [97/120    avg_loss:0.004, val_acc:0.983]
Epoch [98/120    avg_loss:0.006, val_acc:0.983]
Epoch [99/120    avg_loss:0.003, val_acc:0.982]
Epoch [100/120    avg_loss:0.005, val_acc:0.982]
Epoch [101/120    avg_loss:0.005, val_acc:0.983]
Epoch [102/120    avg_loss:0.003, val_acc:0.984]
Epoch [103/120    avg_loss:0.005, val_acc:0.982]
Epoch [104/120    avg_loss:0.003, val_acc:0.983]
Epoch [105/120    avg_loss:0.003, val_acc:0.983]
Epoch [106/120    avg_loss:0.004, val_acc:0.983]
Epoch [107/120    avg_loss:0.005, val_acc:0.982]
Epoch [108/120    avg_loss:0.003, val_acc:0.982]
Epoch [109/120    avg_loss:0.005, val_acc:0.982]
Epoch [110/120    avg_loss:0.003, val_acc:0.982]
Epoch [111/120    avg_loss:0.003, val_acc:0.982]
Epoch [112/120    avg_loss:0.002, val_acc:0.982]
Epoch [113/120    avg_loss:0.003, val_acc:0.982]
Epoch [114/120    avg_loss:0.007, val_acc:0.984]
Epoch [115/120    avg_loss:0.004, val_acc:0.983]
Epoch [116/120    avg_loss:0.003, val_acc:0.983]
Epoch [117/120    avg_loss:0.004, val_acc:0.983]
Epoch [118/120    avg_loss:0.004, val_acc:0.983]
Epoch [119/120    avg_loss:0.005, val_acc:0.983]
Epoch [120/120    avg_loss:0.003, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1269    6    0    3    1    0    0    0    2    4    0    0
     0    0    0]
 [   0    0    0  733    0    0    0    0    0    3    0    3    8    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    1    0    0    0    0  835   29    0    0
     3    0    0]
 [   0    0   12    1    0    0    0    0    0    0    0 2181   15    0
     0    1    0]
 [   0    0    0    1    0    0    0    0    0    0    0    0  530    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1122   17    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    63  275    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
97.82113821138212

F1 scores:
[       nan 0.975      0.98601399 0.98455339 0.99764706 0.99427262
 0.99244713 1.         1.         0.92307692 0.97489784 0.98531737
 0.97069597 1.         0.96391753 0.85803432 0.95757576]

Kappa:
0.9751445932598628
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0da7965668>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.299, val_acc:0.489]
Epoch [2/120    avg_loss:1.738, val_acc:0.414]
Epoch [3/120    avg_loss:1.486, val_acc:0.695]
Epoch [4/120    avg_loss:1.065, val_acc:0.732]
Epoch [5/120    avg_loss:0.933, val_acc:0.697]
Epoch [6/120    avg_loss:0.975, val_acc:0.728]
Epoch [7/120    avg_loss:0.626, val_acc:0.770]
Epoch [8/120    avg_loss:0.493, val_acc:0.842]
Epoch [9/120    avg_loss:0.427, val_acc:0.880]
Epoch [10/120    avg_loss:0.371, val_acc:0.902]
Epoch [11/120    avg_loss:0.244, val_acc:0.858]
Epoch [12/120    avg_loss:0.280, val_acc:0.874]
Epoch [13/120    avg_loss:0.213, val_acc:0.929]
Epoch [14/120    avg_loss:0.143, val_acc:0.935]
Epoch [15/120    avg_loss:0.164, val_acc:0.911]
Epoch [16/120    avg_loss:0.118, val_acc:0.914]
Epoch [17/120    avg_loss:0.198, val_acc:0.919]
Epoch [18/120    avg_loss:0.244, val_acc:0.943]
Epoch [19/120    avg_loss:0.132, val_acc:0.942]
Epoch [20/120    avg_loss:0.098, val_acc:0.944]
Epoch [21/120    avg_loss:0.092, val_acc:0.958]
Epoch [22/120    avg_loss:0.060, val_acc:0.958]
Epoch [23/120    avg_loss:0.060, val_acc:0.961]
Epoch [24/120    avg_loss:0.066, val_acc:0.909]
Epoch [25/120    avg_loss:0.328, val_acc:0.898]
Epoch [26/120    avg_loss:0.435, val_acc:0.844]
Epoch [27/120    avg_loss:0.290, val_acc:0.938]
Epoch [28/120    avg_loss:0.128, val_acc:0.947]
Epoch [29/120    avg_loss:0.078, val_acc:0.949]
Epoch [30/120    avg_loss:0.059, val_acc:0.953]
Epoch [31/120    avg_loss:0.060, val_acc:0.966]
Epoch [32/120    avg_loss:0.038, val_acc:0.964]
Epoch [33/120    avg_loss:0.051, val_acc:0.963]
Epoch [34/120    avg_loss:0.049, val_acc:0.959]
Epoch [35/120    avg_loss:0.031, val_acc:0.961]
Epoch [36/120    avg_loss:0.046, val_acc:0.966]
Epoch [37/120    avg_loss:0.030, val_acc:0.967]
Epoch [38/120    avg_loss:0.021, val_acc:0.969]
Epoch [39/120    avg_loss:0.015, val_acc:0.973]
Epoch [40/120    avg_loss:0.021, val_acc:0.971]
Epoch [41/120    avg_loss:0.016, val_acc:0.966]
Epoch [42/120    avg_loss:0.017, val_acc:0.975]
Epoch [43/120    avg_loss:0.014, val_acc:0.971]
Epoch [44/120    avg_loss:0.026, val_acc:0.976]
Epoch [45/120    avg_loss:0.022, val_acc:0.973]
Epoch [46/120    avg_loss:0.017, val_acc:0.973]
Epoch [47/120    avg_loss:0.020, val_acc:0.979]
Epoch [48/120    avg_loss:0.014, val_acc:0.979]
Epoch [49/120    avg_loss:0.017, val_acc:0.982]
Epoch [50/120    avg_loss:0.012, val_acc:0.977]
Epoch [51/120    avg_loss:0.016, val_acc:0.968]
Epoch [52/120    avg_loss:0.030, val_acc:0.970]
Epoch [53/120    avg_loss:0.024, val_acc:0.973]
Epoch [54/120    avg_loss:0.013, val_acc:0.980]
Epoch [55/120    avg_loss:0.038, val_acc:0.971]
Epoch [56/120    avg_loss:0.023, val_acc:0.971]
Epoch [57/120    avg_loss:0.012, val_acc:0.978]
Epoch [58/120    avg_loss:0.011, val_acc:0.977]
Epoch [59/120    avg_loss:0.019, val_acc:0.964]
Epoch [60/120    avg_loss:0.029, val_acc:0.974]
Epoch [61/120    avg_loss:0.016, val_acc:0.978]
Epoch [62/120    avg_loss:0.028, val_acc:0.971]
Epoch [63/120    avg_loss:0.014, val_acc:0.976]
Epoch [64/120    avg_loss:0.011, val_acc:0.975]
Epoch [65/120    avg_loss:0.006, val_acc:0.977]
Epoch [66/120    avg_loss:0.008, val_acc:0.978]
Epoch [67/120    avg_loss:0.009, val_acc:0.978]
Epoch [68/120    avg_loss:0.009, val_acc:0.977]
Epoch [69/120    avg_loss:0.005, val_acc:0.978]
Epoch [70/120    avg_loss:0.008, val_acc:0.980]
Epoch [71/120    avg_loss:0.006, val_acc:0.980]
Epoch [72/120    avg_loss:0.006, val_acc:0.977]
Epoch [73/120    avg_loss:0.005, val_acc:0.977]
Epoch [74/120    avg_loss:0.006, val_acc:0.978]
Epoch [75/120    avg_loss:0.006, val_acc:0.979]
Epoch [76/120    avg_loss:0.005, val_acc:0.978]
Epoch [77/120    avg_loss:0.006, val_acc:0.978]
Epoch [78/120    avg_loss:0.008, val_acc:0.979]
Epoch [79/120    avg_loss:0.007, val_acc:0.979]
Epoch [80/120    avg_loss:0.005, val_acc:0.978]
Epoch [81/120    avg_loss:0.007, val_acc:0.978]
Epoch [82/120    avg_loss:0.004, val_acc:0.978]
Epoch [83/120    avg_loss:0.007, val_acc:0.978]
Epoch [84/120    avg_loss:0.004, val_acc:0.978]
Epoch [85/120    avg_loss:0.006, val_acc:0.978]
Epoch [86/120    avg_loss:0.008, val_acc:0.978]
Epoch [87/120    avg_loss:0.005, val_acc:0.978]
Epoch [88/120    avg_loss:0.006, val_acc:0.978]
Epoch [89/120    avg_loss:0.006, val_acc:0.978]
Epoch [90/120    avg_loss:0.004, val_acc:0.978]
Epoch [91/120    avg_loss:0.007, val_acc:0.978]
Epoch [92/120    avg_loss:0.006, val_acc:0.978]
Epoch [93/120    avg_loss:0.004, val_acc:0.978]
Epoch [94/120    avg_loss:0.009, val_acc:0.978]
Epoch [95/120    avg_loss:0.006, val_acc:0.978]
Epoch [96/120    avg_loss:0.006, val_acc:0.978]
Epoch [97/120    avg_loss:0.005, val_acc:0.978]
Epoch [98/120    avg_loss:0.005, val_acc:0.978]
Epoch [99/120    avg_loss:0.005, val_acc:0.978]
Epoch [100/120    avg_loss:0.005, val_acc:0.978]
Epoch [101/120    avg_loss:0.006, val_acc:0.978]
Epoch [102/120    avg_loss:0.005, val_acc:0.978]
Epoch [103/120    avg_loss:0.006, val_acc:0.978]
Epoch [104/120    avg_loss:0.007, val_acc:0.978]
Epoch [105/120    avg_loss:0.007, val_acc:0.978]
Epoch [106/120    avg_loss:0.007, val_acc:0.978]
Epoch [107/120    avg_loss:0.005, val_acc:0.978]
Epoch [108/120    avg_loss:0.005, val_acc:0.978]
Epoch [109/120    avg_loss:0.007, val_acc:0.978]
Epoch [110/120    avg_loss:0.005, val_acc:0.978]
Epoch [111/120    avg_loss:0.005, val_acc:0.978]
Epoch [112/120    avg_loss:0.010, val_acc:0.978]
Epoch [113/120    avg_loss:0.007, val_acc:0.978]
Epoch [114/120    avg_loss:0.007, val_acc:0.978]
Epoch [115/120    avg_loss:0.007, val_acc:0.978]
Epoch [116/120    avg_loss:0.005, val_acc:0.978]
Epoch [117/120    avg_loss:0.007, val_acc:0.978]
Epoch [118/120    avg_loss:0.009, val_acc:0.978]
Epoch [119/120    avg_loss:0.006, val_acc:0.978]
Epoch [120/120    avg_loss:0.006, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1279    3    0    0    0    0    0    0    1    2    0    0
     0    0    0]
 [   0    0    0  734    0    7    0    0    0    2    0    2    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    2    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0  847   22    1    0
     2    0    0]
 [   0    0   15    0    0    0    0    0    0    0    2 2174   17    0
     2    0    0]
 [   0    0    0    2    0    2    0    0    0    0    0    3  521    0
     0    4    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0    0
  1120   17    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    28  303    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.13550135501355

F1 scores:
[       nan 0.975      0.99032133 0.98788694 1.         0.98057143
 0.98496241 1.         0.997669   0.94736842 0.98146002 0.98482446
 0.96660482 1.         0.97603486 0.90312966 0.98224852]

Kappa:
0.9787409531480995
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f89d6666710>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.184, val_acc:0.485]
Epoch [2/120    avg_loss:1.652, val_acc:0.486]
Epoch [3/120    avg_loss:1.246, val_acc:0.570]
Epoch [4/120    avg_loss:1.009, val_acc:0.656]
Epoch [5/120    avg_loss:0.792, val_acc:0.751]
Epoch [6/120    avg_loss:0.696, val_acc:0.766]
Epoch [7/120    avg_loss:0.712, val_acc:0.782]
Epoch [8/120    avg_loss:0.607, val_acc:0.809]
Epoch [9/120    avg_loss:0.486, val_acc:0.792]
Epoch [10/120    avg_loss:0.405, val_acc:0.871]
Epoch [11/120    avg_loss:0.311, val_acc:0.885]
Epoch [12/120    avg_loss:0.198, val_acc:0.909]
Epoch [13/120    avg_loss:0.157, val_acc:0.923]
Epoch [14/120    avg_loss:0.140, val_acc:0.924]
Epoch [15/120    avg_loss:0.141, val_acc:0.935]
Epoch [16/120    avg_loss:0.102, val_acc:0.947]
Epoch [17/120    avg_loss:0.150, val_acc:0.943]
Epoch [18/120    avg_loss:0.167, val_acc:0.828]
Epoch [19/120    avg_loss:0.326, val_acc:0.894]
Epoch [20/120    avg_loss:0.139, val_acc:0.921]
Epoch [21/120    avg_loss:0.082, val_acc:0.941]
Epoch [22/120    avg_loss:0.083, val_acc:0.920]
Epoch [23/120    avg_loss:0.097, val_acc:0.948]
Epoch [24/120    avg_loss:0.094, val_acc:0.944]
Epoch [25/120    avg_loss:0.140, val_acc:0.941]
Epoch [26/120    avg_loss:0.096, val_acc:0.931]
Epoch [27/120    avg_loss:0.076, val_acc:0.947]
Epoch [28/120    avg_loss:0.047, val_acc:0.958]
Epoch [29/120    avg_loss:0.043, val_acc:0.969]
Epoch [30/120    avg_loss:0.041, val_acc:0.960]
Epoch [31/120    avg_loss:0.031, val_acc:0.961]
Epoch [32/120    avg_loss:0.076, val_acc:0.945]
Epoch [33/120    avg_loss:0.064, val_acc:0.954]
Epoch [34/120    avg_loss:0.036, val_acc:0.948]
Epoch [35/120    avg_loss:0.030, val_acc:0.969]
Epoch [36/120    avg_loss:0.025, val_acc:0.968]
Epoch [37/120    avg_loss:0.076, val_acc:0.960]
Epoch [38/120    avg_loss:0.025, val_acc:0.973]
Epoch [39/120    avg_loss:0.028, val_acc:0.954]
Epoch [40/120    avg_loss:0.022, val_acc:0.972]
Epoch [41/120    avg_loss:0.034, val_acc:0.922]
Epoch [42/120    avg_loss:0.046, val_acc:0.963]
Epoch [43/120    avg_loss:0.019, val_acc:0.977]
Epoch [44/120    avg_loss:0.012, val_acc:0.973]
Epoch [45/120    avg_loss:0.008, val_acc:0.979]
Epoch [46/120    avg_loss:0.031, val_acc:0.963]
Epoch [47/120    avg_loss:0.021, val_acc:0.980]
Epoch [48/120    avg_loss:0.011, val_acc:0.978]
Epoch [49/120    avg_loss:0.017, val_acc:0.974]
Epoch [50/120    avg_loss:0.013, val_acc:0.981]
Epoch [51/120    avg_loss:0.011, val_acc:0.979]
Epoch [52/120    avg_loss:0.042, val_acc:0.967]
Epoch [53/120    avg_loss:0.026, val_acc:0.967]
Epoch [54/120    avg_loss:0.011, val_acc:0.971]
Epoch [55/120    avg_loss:0.017, val_acc:0.977]
Epoch [56/120    avg_loss:0.022, val_acc:0.973]
Epoch [57/120    avg_loss:0.022, val_acc:0.970]
Epoch [58/120    avg_loss:0.008, val_acc:0.986]
Epoch [59/120    avg_loss:0.020, val_acc:0.969]
Epoch [60/120    avg_loss:0.021, val_acc:0.966]
Epoch [61/120    avg_loss:0.016, val_acc:0.968]
Epoch [62/120    avg_loss:0.013, val_acc:0.969]
Epoch [63/120    avg_loss:0.018, val_acc:0.974]
Epoch [64/120    avg_loss:0.011, val_acc:0.975]
Epoch [65/120    avg_loss:0.008, val_acc:0.978]
Epoch [66/120    avg_loss:0.007, val_acc:0.984]
Epoch [67/120    avg_loss:0.005, val_acc:0.979]
Epoch [68/120    avg_loss:0.008, val_acc:0.967]
Epoch [69/120    avg_loss:0.010, val_acc:0.981]
Epoch [70/120    avg_loss:0.007, val_acc:0.984]
Epoch [71/120    avg_loss:0.009, val_acc:0.964]
Epoch [72/120    avg_loss:0.017, val_acc:0.979]
Epoch [73/120    avg_loss:0.007, val_acc:0.981]
Epoch [74/120    avg_loss:0.007, val_acc:0.981]
Epoch [75/120    avg_loss:0.003, val_acc:0.981]
Epoch [76/120    avg_loss:0.004, val_acc:0.981]
Epoch [77/120    avg_loss:0.004, val_acc:0.984]
Epoch [78/120    avg_loss:0.004, val_acc:0.985]
Epoch [79/120    avg_loss:0.003, val_acc:0.985]
Epoch [80/120    avg_loss:0.004, val_acc:0.986]
Epoch [81/120    avg_loss:0.004, val_acc:0.986]
Epoch [82/120    avg_loss:0.004, val_acc:0.988]
Epoch [83/120    avg_loss:0.004, val_acc:0.988]
Epoch [84/120    avg_loss:0.003, val_acc:0.988]
Epoch [85/120    avg_loss:0.003, val_acc:0.988]
Epoch [86/120    avg_loss:0.004, val_acc:0.988]
Epoch [87/120    avg_loss:0.004, val_acc:0.986]
Epoch [88/120    avg_loss:0.004, val_acc:0.984]
Epoch [89/120    avg_loss:0.003, val_acc:0.985]
Epoch [90/120    avg_loss:0.003, val_acc:0.986]
Epoch [91/120    avg_loss:0.004, val_acc:0.986]
Epoch [92/120    avg_loss:0.003, val_acc:0.988]
Epoch [93/120    avg_loss:0.003, val_acc:0.986]
Epoch [94/120    avg_loss:0.004, val_acc:0.986]
Epoch [95/120    avg_loss:0.004, val_acc:0.986]
Epoch [96/120    avg_loss:0.005, val_acc:0.985]
Epoch [97/120    avg_loss:0.002, val_acc:0.985]
Epoch [98/120    avg_loss:0.003, val_acc:0.985]
Epoch [99/120    avg_loss:0.003, val_acc:0.984]
Epoch [100/120    avg_loss:0.003, val_acc:0.984]
Epoch [101/120    avg_loss:0.002, val_acc:0.984]
Epoch [102/120    avg_loss:0.003, val_acc:0.984]
Epoch [103/120    avg_loss:0.002, val_acc:0.985]
Epoch [104/120    avg_loss:0.003, val_acc:0.985]
Epoch [105/120    avg_loss:0.004, val_acc:0.986]
Epoch [106/120    avg_loss:0.002, val_acc:0.986]
Epoch [107/120    avg_loss:0.004, val_acc:0.986]
Epoch [108/120    avg_loss:0.003, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.986]
Epoch [110/120    avg_loss:0.003, val_acc:0.986]
Epoch [111/120    avg_loss:0.002, val_acc:0.986]
Epoch [112/120    avg_loss:0.003, val_acc:0.986]
Epoch [113/120    avg_loss:0.002, val_acc:0.985]
Epoch [114/120    avg_loss:0.003, val_acc:0.985]
Epoch [115/120    avg_loss:0.003, val_acc:0.985]
Epoch [116/120    avg_loss:0.003, val_acc:0.985]
Epoch [117/120    avg_loss:0.004, val_acc:0.986]
Epoch [118/120    avg_loss:0.004, val_acc:0.986]
Epoch [119/120    avg_loss:0.003, val_acc:0.986]
Epoch [120/120    avg_loss:0.003, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1274    1    2    1    2    0    0    0    0    5    0    0
     0    0    0]
 [   0    0    0  737    0    2    0    0    0    3    0    0    5    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  866    6    1    0
     2    0    0]
 [   0    0   16    0    0    0    0    0    0    0    5 2179   10    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0    0  530    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    1    3    0    0    0    0    0    0    0
    59  284    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.39566395663957

F1 scores:
[       nan 0.975      0.98913043 0.99125757 0.9953271  0.9908046
 0.99315589 0.98039216 1.         0.9        0.99141385 0.98955495
 0.98057354 1.         0.96912521 0.8875     0.98224852]

Kappa:
0.9817053409389875
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:41--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb79d101630>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.231, val_acc:0.536]
Epoch [2/120    avg_loss:1.538, val_acc:0.592]
Epoch [3/120    avg_loss:1.234, val_acc:0.629]
Epoch [4/120    avg_loss:1.074, val_acc:0.726]
Epoch [5/120    avg_loss:0.911, val_acc:0.715]
Epoch [6/120    avg_loss:0.804, val_acc:0.779]
Epoch [7/120    avg_loss:0.657, val_acc:0.795]
Epoch [8/120    avg_loss:0.659, val_acc:0.764]
Epoch [9/120    avg_loss:0.499, val_acc:0.830]
Epoch [10/120    avg_loss:0.350, val_acc:0.857]
Epoch [11/120    avg_loss:0.394, val_acc:0.845]
Epoch [12/120    avg_loss:0.430, val_acc:0.873]
Epoch [13/120    avg_loss:0.255, val_acc:0.916]
Epoch [14/120    avg_loss:0.257, val_acc:0.865]
Epoch [15/120    avg_loss:0.315, val_acc:0.915]
Epoch [16/120    avg_loss:0.201, val_acc:0.910]
Epoch [17/120    avg_loss:0.118, val_acc:0.893]
Epoch [18/120    avg_loss:0.144, val_acc:0.934]
Epoch [19/120    avg_loss:0.152, val_acc:0.926]
Epoch [20/120    avg_loss:0.123, val_acc:0.943]
Epoch [21/120    avg_loss:0.084, val_acc:0.940]
Epoch [22/120    avg_loss:0.062, val_acc:0.950]
Epoch [23/120    avg_loss:0.067, val_acc:0.950]
Epoch [24/120    avg_loss:0.078, val_acc:0.957]
Epoch [25/120    avg_loss:0.044, val_acc:0.965]
Epoch [26/120    avg_loss:0.036, val_acc:0.966]
Epoch [27/120    avg_loss:0.057, val_acc:0.948]
Epoch [28/120    avg_loss:0.067, val_acc:0.964]
Epoch [29/120    avg_loss:0.094, val_acc:0.946]
Epoch [30/120    avg_loss:0.068, val_acc:0.972]
Epoch [31/120    avg_loss:0.059, val_acc:0.970]
Epoch [32/120    avg_loss:0.035, val_acc:0.969]
Epoch [33/120    avg_loss:0.033, val_acc:0.968]
Epoch [34/120    avg_loss:0.025, val_acc:0.973]
Epoch [35/120    avg_loss:0.019, val_acc:0.966]
Epoch [36/120    avg_loss:0.019, val_acc:0.970]
Epoch [37/120    avg_loss:0.024, val_acc:0.965]
Epoch [38/120    avg_loss:0.026, val_acc:0.972]
Epoch [39/120    avg_loss:0.049, val_acc:0.968]
Epoch [40/120    avg_loss:0.059, val_acc:0.963]
Epoch [41/120    avg_loss:0.029, val_acc:0.977]
Epoch [42/120    avg_loss:0.028, val_acc:0.971]
Epoch [43/120    avg_loss:0.024, val_acc:0.961]
Epoch [44/120    avg_loss:0.039, val_acc:0.961]
Epoch [45/120    avg_loss:0.031, val_acc:0.967]
Epoch [46/120    avg_loss:0.016, val_acc:0.972]
Epoch [47/120    avg_loss:0.023, val_acc:0.965]
Epoch [48/120    avg_loss:0.025, val_acc:0.966]
Epoch [49/120    avg_loss:0.014, val_acc:0.980]
Epoch [50/120    avg_loss:0.009, val_acc:0.980]
Epoch [51/120    avg_loss:0.021, val_acc:0.954]
Epoch [52/120    avg_loss:0.017, val_acc:0.979]
Epoch [53/120    avg_loss:0.019, val_acc:0.984]
Epoch [54/120    avg_loss:0.012, val_acc:0.988]
Epoch [55/120    avg_loss:0.007, val_acc:0.984]
Epoch [56/120    avg_loss:0.007, val_acc:0.984]
Epoch [57/120    avg_loss:0.004, val_acc:0.986]
Epoch [58/120    avg_loss:0.009, val_acc:0.988]
Epoch [59/120    avg_loss:0.008, val_acc:0.986]
Epoch [60/120    avg_loss:0.007, val_acc:0.982]
Epoch [61/120    avg_loss:0.008, val_acc:0.985]
Epoch [62/120    avg_loss:0.006, val_acc:0.980]
Epoch [63/120    avg_loss:0.014, val_acc:0.984]
Epoch [64/120    avg_loss:0.008, val_acc:0.981]
Epoch [65/120    avg_loss:0.004, val_acc:0.985]
Epoch [66/120    avg_loss:0.004, val_acc:0.985]
Epoch [67/120    avg_loss:0.006, val_acc:0.981]
Epoch [68/120    avg_loss:0.006, val_acc:0.981]
Epoch [69/120    avg_loss:0.010, val_acc:0.973]
Epoch [70/120    avg_loss:0.009, val_acc:0.979]
Epoch [71/120    avg_loss:0.004, val_acc:0.983]
Epoch [72/120    avg_loss:0.006, val_acc:0.983]
Epoch [73/120    avg_loss:0.005, val_acc:0.983]
Epoch [74/120    avg_loss:0.003, val_acc:0.983]
Epoch [75/120    avg_loss:0.002, val_acc:0.984]
Epoch [76/120    avg_loss:0.004, val_acc:0.984]
Epoch [77/120    avg_loss:0.004, val_acc:0.985]
Epoch [78/120    avg_loss:0.003, val_acc:0.986]
Epoch [79/120    avg_loss:0.006, val_acc:0.985]
Epoch [80/120    avg_loss:0.005, val_acc:0.984]
Epoch [81/120    avg_loss:0.002, val_acc:0.985]
Epoch [82/120    avg_loss:0.004, val_acc:0.984]
Epoch [83/120    avg_loss:0.002, val_acc:0.985]
Epoch [84/120    avg_loss:0.002, val_acc:0.985]
Epoch [85/120    avg_loss:0.003, val_acc:0.985]
Epoch [86/120    avg_loss:0.007, val_acc:0.985]
Epoch [87/120    avg_loss:0.004, val_acc:0.985]
Epoch [88/120    avg_loss:0.004, val_acc:0.985]
Epoch [89/120    avg_loss:0.006, val_acc:0.985]
Epoch [90/120    avg_loss:0.002, val_acc:0.985]
Epoch [91/120    avg_loss:0.004, val_acc:0.985]
Epoch [92/120    avg_loss:0.003, val_acc:0.985]
Epoch [93/120    avg_loss:0.002, val_acc:0.985]
Epoch [94/120    avg_loss:0.003, val_acc:0.985]
Epoch [95/120    avg_loss:0.004, val_acc:0.985]
Epoch [96/120    avg_loss:0.002, val_acc:0.985]
Epoch [97/120    avg_loss:0.004, val_acc:0.985]
Epoch [98/120    avg_loss:0.004, val_acc:0.985]
Epoch [99/120    avg_loss:0.004, val_acc:0.985]
Epoch [100/120    avg_loss:0.003, val_acc:0.985]
Epoch [101/120    avg_loss:0.003, val_acc:0.985]
Epoch [102/120    avg_loss:0.007, val_acc:0.985]
Epoch [103/120    avg_loss:0.003, val_acc:0.985]
Epoch [104/120    avg_loss:0.003, val_acc:0.985]
Epoch [105/120    avg_loss:0.004, val_acc:0.985]
Epoch [106/120    avg_loss:0.004, val_acc:0.985]
Epoch [107/120    avg_loss:0.002, val_acc:0.985]
Epoch [108/120    avg_loss:0.004, val_acc:0.985]
Epoch [109/120    avg_loss:0.002, val_acc:0.985]
Epoch [110/120    avg_loss:0.004, val_acc:0.985]
Epoch [111/120    avg_loss:0.003, val_acc:0.985]
Epoch [112/120    avg_loss:0.003, val_acc:0.985]
Epoch [113/120    avg_loss:0.004, val_acc:0.985]
Epoch [114/120    avg_loss:0.003, val_acc:0.985]
Epoch [115/120    avg_loss:0.003, val_acc:0.985]
Epoch [116/120    avg_loss:0.003, val_acc:0.985]
Epoch [117/120    avg_loss:0.003, val_acc:0.985]
Epoch [118/120    avg_loss:0.003, val_acc:0.985]
Epoch [119/120    avg_loss:0.003, val_acc:0.985]
Epoch [120/120    avg_loss:0.002, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1273    1    0    0    2    0    0    2    0    7    0    0
     0    0    0]
 [   0    0    0  733    0    0    0    0    0    2    0    3    8    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    0    0    1    0    0
     4    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  854   15    0    0
     1    1    0]
 [   0    0    7    0    0    0    0    0    0    0    2 2192    6    1
     2    0    0]
 [   0    0    0    4    0    0    0    0    0    0    0    0  527    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1129    9    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    74  269    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.13550135501355

F1 scores:
[       nan 0.98765432 0.99066148 0.98720539 1.         0.99190751
 0.99316629 0.98039216 1.         0.9        0.98671288 0.98939291
 0.97864438 0.99730458 0.96085106 0.85805423 0.97619048]

Kappa:
0.9787246201559626
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f736e43a668>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.325, val_acc:0.528]
Epoch [2/120    avg_loss:1.650, val_acc:0.583]
Epoch [3/120    avg_loss:1.372, val_acc:0.653]
Epoch [4/120    avg_loss:1.046, val_acc:0.713]
Epoch [5/120    avg_loss:1.165, val_acc:0.708]
Epoch [6/120    avg_loss:0.818, val_acc:0.752]
Epoch [7/120    avg_loss:0.736, val_acc:0.782]
Epoch [8/120    avg_loss:0.544, val_acc:0.829]
Epoch [9/120    avg_loss:0.516, val_acc:0.841]
Epoch [10/120    avg_loss:0.470, val_acc:0.811]
Epoch [11/120    avg_loss:0.445, val_acc:0.839]
Epoch [12/120    avg_loss:0.309, val_acc:0.877]
Epoch [13/120    avg_loss:0.294, val_acc:0.891]
Epoch [14/120    avg_loss:0.219, val_acc:0.884]
Epoch [15/120    avg_loss:0.201, val_acc:0.908]
Epoch [16/120    avg_loss:0.111, val_acc:0.910]
Epoch [17/120    avg_loss:0.152, val_acc:0.915]
Epoch [18/120    avg_loss:0.117, val_acc:0.912]
Epoch [19/120    avg_loss:0.159, val_acc:0.930]
Epoch [20/120    avg_loss:0.123, val_acc:0.919]
Epoch [21/120    avg_loss:0.078, val_acc:0.947]
Epoch [22/120    avg_loss:0.066, val_acc:0.958]
Epoch [23/120    avg_loss:0.133, val_acc:0.917]
Epoch [24/120    avg_loss:0.128, val_acc:0.928]
Epoch [25/120    avg_loss:0.118, val_acc:0.933]
Epoch [26/120    avg_loss:0.089, val_acc:0.947]
Epoch [27/120    avg_loss:0.050, val_acc:0.960]
Epoch [28/120    avg_loss:0.047, val_acc:0.956]
Epoch [29/120    avg_loss:0.041, val_acc:0.946]
Epoch [30/120    avg_loss:0.039, val_acc:0.945]
Epoch [31/120    avg_loss:0.037, val_acc:0.951]
Epoch [32/120    avg_loss:0.029, val_acc:0.951]
Epoch [33/120    avg_loss:0.041, val_acc:0.939]
Epoch [34/120    avg_loss:0.030, val_acc:0.965]
Epoch [35/120    avg_loss:0.022, val_acc:0.966]
Epoch [36/120    avg_loss:0.023, val_acc:0.959]
Epoch [37/120    avg_loss:0.021, val_acc:0.966]
Epoch [38/120    avg_loss:0.015, val_acc:0.963]
Epoch [39/120    avg_loss:0.038, val_acc:0.959]
Epoch [40/120    avg_loss:0.021, val_acc:0.967]
Epoch [41/120    avg_loss:0.022, val_acc:0.957]
Epoch [42/120    avg_loss:0.031, val_acc:0.958]
Epoch [43/120    avg_loss:0.020, val_acc:0.958]
Epoch [44/120    avg_loss:0.014, val_acc:0.960]
Epoch [45/120    avg_loss:0.012, val_acc:0.966]
Epoch [46/120    avg_loss:0.023, val_acc:0.961]
Epoch [47/120    avg_loss:0.022, val_acc:0.955]
Epoch [48/120    avg_loss:0.038, val_acc:0.955]
Epoch [49/120    avg_loss:0.032, val_acc:0.960]
Epoch [50/120    avg_loss:0.025, val_acc:0.955]
Epoch [51/120    avg_loss:0.017, val_acc:0.970]
Epoch [52/120    avg_loss:0.010, val_acc:0.968]
Epoch [53/120    avg_loss:0.007, val_acc:0.969]
Epoch [54/120    avg_loss:0.035, val_acc:0.973]
Epoch [55/120    avg_loss:0.014, val_acc:0.967]
Epoch [56/120    avg_loss:0.010, val_acc:0.975]
Epoch [57/120    avg_loss:0.010, val_acc:0.967]
Epoch [58/120    avg_loss:0.012, val_acc:0.968]
Epoch [59/120    avg_loss:0.008, val_acc:0.967]
Epoch [60/120    avg_loss:0.010, val_acc:0.973]
Epoch [61/120    avg_loss:0.008, val_acc:0.971]
Epoch [62/120    avg_loss:0.014, val_acc:0.955]
Epoch [63/120    avg_loss:0.029, val_acc:0.963]
Epoch [64/120    avg_loss:0.023, val_acc:0.970]
Epoch [65/120    avg_loss:0.013, val_acc:0.964]
Epoch [66/120    avg_loss:0.008, val_acc:0.974]
Epoch [67/120    avg_loss:0.006, val_acc:0.970]
Epoch [68/120    avg_loss:0.013, val_acc:0.972]
Epoch [69/120    avg_loss:0.049, val_acc:0.966]
Epoch [70/120    avg_loss:0.015, val_acc:0.971]
Epoch [71/120    avg_loss:0.014, val_acc:0.973]
Epoch [72/120    avg_loss:0.012, val_acc:0.974]
Epoch [73/120    avg_loss:0.010, val_acc:0.974]
Epoch [74/120    avg_loss:0.017, val_acc:0.974]
Epoch [75/120    avg_loss:0.013, val_acc:0.974]
Epoch [76/120    avg_loss:0.008, val_acc:0.973]
Epoch [77/120    avg_loss:0.007, val_acc:0.974]
Epoch [78/120    avg_loss:0.005, val_acc:0.974]
Epoch [79/120    avg_loss:0.009, val_acc:0.972]
Epoch [80/120    avg_loss:0.008, val_acc:0.973]
Epoch [81/120    avg_loss:0.009, val_acc:0.974]
Epoch [82/120    avg_loss:0.007, val_acc:0.974]
Epoch [83/120    avg_loss:0.007, val_acc:0.974]
Epoch [84/120    avg_loss:0.006, val_acc:0.974]
Epoch [85/120    avg_loss:0.007, val_acc:0.974]
Epoch [86/120    avg_loss:0.004, val_acc:0.973]
Epoch [87/120    avg_loss:0.005, val_acc:0.974]
Epoch [88/120    avg_loss:0.006, val_acc:0.974]
Epoch [89/120    avg_loss:0.005, val_acc:0.974]
Epoch [90/120    avg_loss:0.008, val_acc:0.974]
Epoch [91/120    avg_loss:0.005, val_acc:0.974]
Epoch [92/120    avg_loss:0.007, val_acc:0.974]
Epoch [93/120    avg_loss:0.010, val_acc:0.974]
Epoch [94/120    avg_loss:0.005, val_acc:0.974]
Epoch [95/120    avg_loss:0.006, val_acc:0.974]
Epoch [96/120    avg_loss:0.006, val_acc:0.974]
Epoch [97/120    avg_loss:0.008, val_acc:0.974]
Epoch [98/120    avg_loss:0.005, val_acc:0.974]
Epoch [99/120    avg_loss:0.005, val_acc:0.974]
Epoch [100/120    avg_loss:0.004, val_acc:0.974]
Epoch [101/120    avg_loss:0.006, val_acc:0.974]
Epoch [102/120    avg_loss:0.004, val_acc:0.974]
Epoch [103/120    avg_loss:0.005, val_acc:0.974]
Epoch [104/120    avg_loss:0.008, val_acc:0.974]
Epoch [105/120    avg_loss:0.006, val_acc:0.974]
Epoch [106/120    avg_loss:0.008, val_acc:0.974]
Epoch [107/120    avg_loss:0.005, val_acc:0.974]
Epoch [108/120    avg_loss:0.006, val_acc:0.974]
Epoch [109/120    avg_loss:0.005, val_acc:0.974]
Epoch [110/120    avg_loss:0.006, val_acc:0.974]
Epoch [111/120    avg_loss:0.005, val_acc:0.974]
Epoch [112/120    avg_loss:0.010, val_acc:0.974]
Epoch [113/120    avg_loss:0.005, val_acc:0.974]
Epoch [114/120    avg_loss:0.007, val_acc:0.974]
Epoch [115/120    avg_loss:0.007, val_acc:0.974]
Epoch [116/120    avg_loss:0.006, val_acc:0.974]
Epoch [117/120    avg_loss:0.004, val_acc:0.974]
Epoch [118/120    avg_loss:0.006, val_acc:0.974]
Epoch [119/120    avg_loss:0.005, val_acc:0.974]
Epoch [120/120    avg_loss:0.005, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1268    3    0    0    0    0    0    0    6    8    0    0
     0    0    0]
 [   0    0    0  725    0    0    0    0    0    2    0    3   17    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    3    0    1    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   14    0    0    0    0    0    0    0  838   22    0    0
     0    1    0]
 [   0    0    6    0    5    0    0    0    0    0    0 2184   15    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    0    0  527    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    1    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1121   18    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    45  294    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.86449864498645

F1 scores:
[       nan 0.98765432 0.98523699 0.98105548 0.98839907 0.99188876
 0.99242424 0.94339623 0.997669   0.92307692 0.97498546 0.98600451
 0.95905369 0.99728997 0.97140381 0.88821752 0.96385542]

Kappa:
0.9756440069923643
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efc43fd5668>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.299, val_acc:0.487]
Epoch [2/120    avg_loss:1.534, val_acc:0.661]
Epoch [3/120    avg_loss:1.381, val_acc:0.641]
Epoch [4/120    avg_loss:0.943, val_acc:0.726]
Epoch [5/120    avg_loss:0.704, val_acc:0.670]
Epoch [6/120    avg_loss:0.801, val_acc:0.783]
Epoch [7/120    avg_loss:0.519, val_acc:0.798]
Epoch [8/120    avg_loss:0.674, val_acc:0.829]
Epoch [9/120    avg_loss:0.458, val_acc:0.845]
Epoch [10/120    avg_loss:0.500, val_acc:0.835]
Epoch [11/120    avg_loss:0.288, val_acc:0.885]
Epoch [12/120    avg_loss:0.180, val_acc:0.921]
Epoch [13/120    avg_loss:0.273, val_acc:0.832]
Epoch [14/120    avg_loss:0.460, val_acc:0.830]
Epoch [15/120    avg_loss:0.303, val_acc:0.906]
Epoch [16/120    avg_loss:0.152, val_acc:0.918]
Epoch [17/120    avg_loss:0.202, val_acc:0.924]
Epoch [18/120    avg_loss:0.132, val_acc:0.912]
Epoch [19/120    avg_loss:0.122, val_acc:0.933]
Epoch [20/120    avg_loss:0.094, val_acc:0.928]
Epoch [21/120    avg_loss:0.087, val_acc:0.941]
Epoch [22/120    avg_loss:0.071, val_acc:0.953]
Epoch [23/120    avg_loss:0.049, val_acc:0.949]
Epoch [24/120    avg_loss:0.067, val_acc:0.959]
Epoch [25/120    avg_loss:0.076, val_acc:0.925]
Epoch [26/120    avg_loss:0.089, val_acc:0.948]
Epoch [27/120    avg_loss:0.054, val_acc:0.954]
Epoch [28/120    avg_loss:0.041, val_acc:0.947]
Epoch [29/120    avg_loss:0.037, val_acc:0.960]
Epoch [30/120    avg_loss:0.028, val_acc:0.959]
Epoch [31/120    avg_loss:0.043, val_acc:0.957]
Epoch [32/120    avg_loss:0.039, val_acc:0.955]
Epoch [33/120    avg_loss:0.035, val_acc:0.966]
Epoch [34/120    avg_loss:0.033, val_acc:0.951]
Epoch [35/120    avg_loss:0.025, val_acc:0.966]
Epoch [36/120    avg_loss:0.016, val_acc:0.970]
Epoch [37/120    avg_loss:0.026, val_acc:0.963]
Epoch [38/120    avg_loss:0.028, val_acc:0.923]
Epoch [39/120    avg_loss:0.019, val_acc:0.968]
Epoch [40/120    avg_loss:0.014, val_acc:0.970]
Epoch [41/120    avg_loss:0.020, val_acc:0.957]
Epoch [42/120    avg_loss:0.040, val_acc:0.973]
Epoch [43/120    avg_loss:0.027, val_acc:0.964]
Epoch [44/120    avg_loss:0.022, val_acc:0.963]
Epoch [45/120    avg_loss:0.015, val_acc:0.969]
Epoch [46/120    avg_loss:0.016, val_acc:0.965]
Epoch [47/120    avg_loss:0.033, val_acc:0.948]
Epoch [48/120    avg_loss:0.021, val_acc:0.960]
Epoch [49/120    avg_loss:0.015, val_acc:0.965]
Epoch [50/120    avg_loss:0.012, val_acc:0.968]
Epoch [51/120    avg_loss:0.020, val_acc:0.955]
Epoch [52/120    avg_loss:0.011, val_acc:0.973]
Epoch [53/120    avg_loss:0.010, val_acc:0.973]
Epoch [54/120    avg_loss:0.014, val_acc:0.968]
Epoch [55/120    avg_loss:0.010, val_acc:0.973]
Epoch [56/120    avg_loss:0.009, val_acc:0.965]
Epoch [57/120    avg_loss:0.018, val_acc:0.968]
Epoch [58/120    avg_loss:0.013, val_acc:0.972]
Epoch [59/120    avg_loss:0.016, val_acc:0.972]
Epoch [60/120    avg_loss:0.008, val_acc:0.972]
Epoch [61/120    avg_loss:0.005, val_acc:0.973]
Epoch [62/120    avg_loss:0.009, val_acc:0.974]
Epoch [63/120    avg_loss:0.006, val_acc:0.966]
Epoch [64/120    avg_loss:0.005, val_acc:0.975]
Epoch [65/120    avg_loss:0.005, val_acc:0.974]
Epoch [66/120    avg_loss:0.005, val_acc:0.978]
Epoch [67/120    avg_loss:0.008, val_acc:0.977]
Epoch [68/120    avg_loss:0.005, val_acc:0.975]
Epoch [69/120    avg_loss:0.006, val_acc:0.978]
Epoch [70/120    avg_loss:0.005, val_acc:0.976]
Epoch [71/120    avg_loss:0.004, val_acc:0.976]
Epoch [72/120    avg_loss:0.005, val_acc:0.981]
Epoch [73/120    avg_loss:0.003, val_acc:0.976]
Epoch [74/120    avg_loss:0.005, val_acc:0.978]
Epoch [75/120    avg_loss:0.004, val_acc:0.978]
Epoch [76/120    avg_loss:0.007, val_acc:0.971]
Epoch [77/120    avg_loss:0.004, val_acc:0.977]
Epoch [78/120    avg_loss:0.005, val_acc:0.979]
Epoch [79/120    avg_loss:0.004, val_acc:0.978]
Epoch [80/120    avg_loss:0.004, val_acc:0.979]
Epoch [81/120    avg_loss:0.007, val_acc:0.973]
Epoch [82/120    avg_loss:0.005, val_acc:0.977]
Epoch [83/120    avg_loss:0.005, val_acc:0.974]
Epoch [84/120    avg_loss:0.009, val_acc:0.971]
Epoch [85/120    avg_loss:0.015, val_acc:0.968]
Epoch [86/120    avg_loss:0.006, val_acc:0.970]
Epoch [87/120    avg_loss:0.007, val_acc:0.973]
Epoch [88/120    avg_loss:0.009, val_acc:0.975]
Epoch [89/120    avg_loss:0.004, val_acc:0.975]
Epoch [90/120    avg_loss:0.004, val_acc:0.974]
Epoch [91/120    avg_loss:0.005, val_acc:0.976]
Epoch [92/120    avg_loss:0.005, val_acc:0.976]
Epoch [93/120    avg_loss:0.006, val_acc:0.976]
Epoch [94/120    avg_loss:0.004, val_acc:0.976]
Epoch [95/120    avg_loss:0.004, val_acc:0.975]
Epoch [96/120    avg_loss:0.004, val_acc:0.976]
Epoch [97/120    avg_loss:0.006, val_acc:0.977]
Epoch [98/120    avg_loss:0.005, val_acc:0.978]
Epoch [99/120    avg_loss:0.006, val_acc:0.978]
Epoch [100/120    avg_loss:0.003, val_acc:0.978]
Epoch [101/120    avg_loss:0.005, val_acc:0.978]
Epoch [102/120    avg_loss:0.005, val_acc:0.978]
Epoch [103/120    avg_loss:0.004, val_acc:0.978]
Epoch [104/120    avg_loss:0.003, val_acc:0.978]
Epoch [105/120    avg_loss:0.005, val_acc:0.978]
Epoch [106/120    avg_loss:0.005, val_acc:0.978]
Epoch [107/120    avg_loss:0.003, val_acc:0.978]
Epoch [108/120    avg_loss:0.005, val_acc:0.978]
Epoch [109/120    avg_loss:0.009, val_acc:0.978]
Epoch [110/120    avg_loss:0.004, val_acc:0.978]
Epoch [111/120    avg_loss:0.003, val_acc:0.978]
Epoch [112/120    avg_loss:0.004, val_acc:0.978]
Epoch [113/120    avg_loss:0.004, val_acc:0.978]
Epoch [114/120    avg_loss:0.004, val_acc:0.978]
Epoch [115/120    avg_loss:0.003, val_acc:0.978]
Epoch [116/120    avg_loss:0.004, val_acc:0.978]
Epoch [117/120    avg_loss:0.003, val_acc:0.978]
Epoch [118/120    avg_loss:0.005, val_acc:0.978]
Epoch [119/120    avg_loss:0.003, val_acc:0.978]
Epoch [120/120    avg_loss:0.006, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1276    3    0    1    0    0    0    0    1    4    0    0
     0    0    0]
 [   0    0    0  730    1    0    0    0    0    2    1    3   10    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   12    0    0    0    0    0    0    0  846   16    0    0
     0    1    0]
 [   0    0   18    0    0    0    0    0    0    0    3 2164   24    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    1  531    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    53  289    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.03794037940379

F1 scores:
[       nan 1.         0.9849479  0.98648649 0.99765808 0.99539171
 0.99544765 1.         1.         0.94736842 0.98030127 0.98385997
 0.96633303 1.         0.96938335 0.88514548 0.98823529]

Kappa:
0.9776306435526095
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc3531a9668>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.229, val_acc:0.543]
Epoch [2/120    avg_loss:1.655, val_acc:0.611]
Epoch [3/120    avg_loss:1.265, val_acc:0.601]
Epoch [4/120    avg_loss:1.160, val_acc:0.666]
Epoch [5/120    avg_loss:0.873, val_acc:0.698]
Epoch [6/120    avg_loss:0.701, val_acc:0.794]
Epoch [7/120    avg_loss:0.568, val_acc:0.776]
Epoch [8/120    avg_loss:0.505, val_acc:0.830]
Epoch [9/120    avg_loss:0.557, val_acc:0.829]
Epoch [10/120    avg_loss:0.384, val_acc:0.865]
Epoch [11/120    avg_loss:0.513, val_acc:0.829]
Epoch [12/120    avg_loss:0.295, val_acc:0.868]
Epoch [13/120    avg_loss:0.208, val_acc:0.900]
Epoch [14/120    avg_loss:0.187, val_acc:0.912]
Epoch [15/120    avg_loss:0.130, val_acc:0.897]
Epoch [16/120    avg_loss:0.203, val_acc:0.932]
Epoch [17/120    avg_loss:0.156, val_acc:0.903]
Epoch [18/120    avg_loss:0.111, val_acc:0.920]
Epoch [19/120    avg_loss:0.132, val_acc:0.942]
Epoch [20/120    avg_loss:0.125, val_acc:0.931]
Epoch [21/120    avg_loss:0.070, val_acc:0.954]
Epoch [22/120    avg_loss:0.069, val_acc:0.965]
Epoch [23/120    avg_loss:0.055, val_acc:0.954]
Epoch [24/120    avg_loss:0.054, val_acc:0.938]
Epoch [25/120    avg_loss:0.055, val_acc:0.957]
Epoch [26/120    avg_loss:0.064, val_acc:0.936]
Epoch [27/120    avg_loss:0.050, val_acc:0.959]
Epoch [28/120    avg_loss:0.030, val_acc:0.956]
Epoch [29/120    avg_loss:0.024, val_acc:0.972]
Epoch [30/120    avg_loss:0.040, val_acc:0.946]
Epoch [31/120    avg_loss:0.086, val_acc:0.931]
Epoch [32/120    avg_loss:0.069, val_acc:0.943]
Epoch [33/120    avg_loss:0.110, val_acc:0.926]
Epoch [34/120    avg_loss:0.121, val_acc:0.940]
Epoch [35/120    avg_loss:0.036, val_acc:0.965]
Epoch [36/120    avg_loss:0.029, val_acc:0.961]
Epoch [37/120    avg_loss:0.033, val_acc:0.964]
Epoch [38/120    avg_loss:0.031, val_acc:0.973]
Epoch [39/120    avg_loss:0.026, val_acc:0.970]
Epoch [40/120    avg_loss:0.022, val_acc:0.972]
Epoch [41/120    avg_loss:0.011, val_acc:0.970]
Epoch [42/120    avg_loss:0.022, val_acc:0.971]
Epoch [43/120    avg_loss:0.031, val_acc:0.955]
Epoch [44/120    avg_loss:0.048, val_acc:0.971]
Epoch [45/120    avg_loss:0.017, val_acc:0.971]
Epoch [46/120    avg_loss:0.018, val_acc:0.972]
Epoch [47/120    avg_loss:0.026, val_acc:0.982]
Epoch [48/120    avg_loss:0.015, val_acc:0.975]
Epoch [49/120    avg_loss:0.012, val_acc:0.980]
Epoch [50/120    avg_loss:0.016, val_acc:0.965]
Epoch [51/120    avg_loss:0.017, val_acc:0.980]
Epoch [52/120    avg_loss:0.014, val_acc:0.975]
Epoch [53/120    avg_loss:0.014, val_acc:0.980]
Epoch [54/120    avg_loss:0.016, val_acc:0.979]
Epoch [55/120    avg_loss:0.028, val_acc:0.979]
Epoch [56/120    avg_loss:0.025, val_acc:0.971]
Epoch [57/120    avg_loss:0.023, val_acc:0.971]
Epoch [58/120    avg_loss:0.011, val_acc:0.974]
Epoch [59/120    avg_loss:0.025, val_acc:0.979]
Epoch [60/120    avg_loss:0.010, val_acc:0.981]
Epoch [61/120    avg_loss:0.007, val_acc:0.984]
Epoch [62/120    avg_loss:0.005, val_acc:0.986]
Epoch [63/120    avg_loss:0.005, val_acc:0.985]
Epoch [64/120    avg_loss:0.006, val_acc:0.986]
Epoch [65/120    avg_loss:0.005, val_acc:0.985]
Epoch [66/120    avg_loss:0.007, val_acc:0.985]
Epoch [67/120    avg_loss:0.005, val_acc:0.989]
Epoch [68/120    avg_loss:0.004, val_acc:0.988]
Epoch [69/120    avg_loss:0.005, val_acc:0.985]
Epoch [70/120    avg_loss:0.005, val_acc:0.986]
Epoch [71/120    avg_loss:0.004, val_acc:0.983]
Epoch [72/120    avg_loss:0.003, val_acc:0.982]
Epoch [73/120    avg_loss:0.004, val_acc:0.983]
Epoch [74/120    avg_loss:0.007, val_acc:0.985]
Epoch [75/120    avg_loss:0.005, val_acc:0.986]
Epoch [76/120    avg_loss:0.005, val_acc:0.985]
Epoch [77/120    avg_loss:0.003, val_acc:0.986]
Epoch [78/120    avg_loss:0.003, val_acc:0.985]
Epoch [79/120    avg_loss:0.010, val_acc:0.983]
Epoch [80/120    avg_loss:0.008, val_acc:0.984]
Epoch [81/120    avg_loss:0.005, val_acc:0.984]
Epoch [82/120    avg_loss:0.005, val_acc:0.984]
Epoch [83/120    avg_loss:0.005, val_acc:0.984]
Epoch [84/120    avg_loss:0.010, val_acc:0.984]
Epoch [85/120    avg_loss:0.007, val_acc:0.986]
Epoch [86/120    avg_loss:0.005, val_acc:0.986]
Epoch [87/120    avg_loss:0.008, val_acc:0.986]
Epoch [88/120    avg_loss:0.005, val_acc:0.986]
Epoch [89/120    avg_loss:0.005, val_acc:0.986]
Epoch [90/120    avg_loss:0.005, val_acc:0.986]
Epoch [91/120    avg_loss:0.004, val_acc:0.986]
Epoch [92/120    avg_loss:0.003, val_acc:0.988]
Epoch [93/120    avg_loss:0.004, val_acc:0.988]
Epoch [94/120    avg_loss:0.010, val_acc:0.988]
Epoch [95/120    avg_loss:0.006, val_acc:0.988]
Epoch [96/120    avg_loss:0.004, val_acc:0.988]
Epoch [97/120    avg_loss:0.006, val_acc:0.988]
Epoch [98/120    avg_loss:0.008, val_acc:0.988]
Epoch [99/120    avg_loss:0.007, val_acc:0.988]
Epoch [100/120    avg_loss:0.006, val_acc:0.988]
Epoch [101/120    avg_loss:0.006, val_acc:0.988]
Epoch [102/120    avg_loss:0.004, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.988]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.004, val_acc:0.988]
Epoch [107/120    avg_loss:0.004, val_acc:0.988]
Epoch [108/120    avg_loss:0.006, val_acc:0.988]
Epoch [109/120    avg_loss:0.005, val_acc:0.988]
Epoch [110/120    avg_loss:0.003, val_acc:0.988]
Epoch [111/120    avg_loss:0.005, val_acc:0.988]
Epoch [112/120    avg_loss:0.006, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.004, val_acc:0.988]
Epoch [116/120    avg_loss:0.006, val_acc:0.988]
Epoch [117/120    avg_loss:0.003, val_acc:0.988]
Epoch [118/120    avg_loss:0.005, val_acc:0.988]
Epoch [119/120    avg_loss:0.004, val_acc:0.988]
Epoch [120/120    avg_loss:0.003, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    1    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1266    4    0    2    3    0    0    0    2    7    1    0
     0    0    0]
 [   0    0    0  734    2    0    0    0    0    2    1    0    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   19    0    0    0    0    0    0    0  821   30    4    0
     0    1    0]
 [   0    0   10    2    0    0    0    0    0    0    5 2173   19    1
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    0    0  529    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   11    0    0    0    0    0    0    0    0
  1114   14    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    47  287    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.61517615176152

F1 scores:
[       nan 0.98765432 0.98139535 0.9852349  0.9953271  0.98185941
 0.98568199 1.         1.         0.94736842 0.96361502 0.982591
 0.96532847 0.99730458 0.96785404 0.8844376  0.98224852]

Kappa:
0.9728045157233985
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:47--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f40e9b506d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.275, val_acc:0.506]
Epoch [2/120    avg_loss:1.723, val_acc:0.549]
Epoch [3/120    avg_loss:1.320, val_acc:0.680]
Epoch [4/120    avg_loss:1.118, val_acc:0.740]
Epoch [5/120    avg_loss:0.912, val_acc:0.779]
Epoch [6/120    avg_loss:0.685, val_acc:0.765]
Epoch [7/120    avg_loss:0.622, val_acc:0.844]
Epoch [8/120    avg_loss:0.479, val_acc:0.865]
Epoch [9/120    avg_loss:0.439, val_acc:0.684]
Epoch [10/120    avg_loss:0.587, val_acc:0.873]
Epoch [11/120    avg_loss:0.490, val_acc:0.789]
Epoch [12/120    avg_loss:0.431, val_acc:0.895]
Epoch [13/120    avg_loss:0.271, val_acc:0.882]
Epoch [14/120    avg_loss:0.219, val_acc:0.877]
Epoch [15/120    avg_loss:0.147, val_acc:0.919]
Epoch [16/120    avg_loss:0.124, val_acc:0.922]
Epoch [17/120    avg_loss:0.144, val_acc:0.935]
Epoch [18/120    avg_loss:0.141, val_acc:0.924]
Epoch [19/120    avg_loss:0.159, val_acc:0.889]
Epoch [20/120    avg_loss:0.148, val_acc:0.930]
Epoch [21/120    avg_loss:0.081, val_acc:0.920]
Epoch [22/120    avg_loss:0.065, val_acc:0.933]
Epoch [23/120    avg_loss:0.042, val_acc:0.952]
Epoch [24/120    avg_loss:0.045, val_acc:0.953]
Epoch [25/120    avg_loss:0.043, val_acc:0.948]
Epoch [26/120    avg_loss:0.038, val_acc:0.957]
Epoch [27/120    avg_loss:0.064, val_acc:0.948]
Epoch [28/120    avg_loss:0.051, val_acc:0.951]
Epoch [29/120    avg_loss:0.040, val_acc:0.952]
Epoch [30/120    avg_loss:0.033, val_acc:0.951]
Epoch [31/120    avg_loss:0.055, val_acc:0.953]
Epoch [32/120    avg_loss:0.071, val_acc:0.919]
Epoch [33/120    avg_loss:0.049, val_acc:0.911]
Epoch [34/120    avg_loss:0.049, val_acc:0.957]
Epoch [35/120    avg_loss:0.032, val_acc:0.964]
Epoch [36/120    avg_loss:0.020, val_acc:0.970]
Epoch [37/120    avg_loss:0.021, val_acc:0.973]
Epoch [38/120    avg_loss:0.016, val_acc:0.966]
Epoch [39/120    avg_loss:0.013, val_acc:0.965]
Epoch [40/120    avg_loss:0.012, val_acc:0.971]
Epoch [41/120    avg_loss:0.019, val_acc:0.970]
Epoch [42/120    avg_loss:0.013, val_acc:0.963]
Epoch [43/120    avg_loss:0.017, val_acc:0.968]
Epoch [44/120    avg_loss:0.015, val_acc:0.963]
Epoch [45/120    avg_loss:0.014, val_acc:0.961]
Epoch [46/120    avg_loss:0.017, val_acc:0.967]
Epoch [47/120    avg_loss:0.014, val_acc:0.958]
Epoch [48/120    avg_loss:0.012, val_acc:0.973]
Epoch [49/120    avg_loss:0.018, val_acc:0.971]
Epoch [50/120    avg_loss:0.021, val_acc:0.965]
Epoch [51/120    avg_loss:0.022, val_acc:0.966]
Epoch [52/120    avg_loss:0.014, val_acc:0.979]
Epoch [53/120    avg_loss:0.014, val_acc:0.972]
Epoch [54/120    avg_loss:0.013, val_acc:0.973]
Epoch [55/120    avg_loss:0.010, val_acc:0.975]
Epoch [56/120    avg_loss:0.016, val_acc:0.972]
Epoch [57/120    avg_loss:0.018, val_acc:0.968]
Epoch [58/120    avg_loss:0.018, val_acc:0.963]
Epoch [59/120    avg_loss:0.025, val_acc:0.967]
Epoch [60/120    avg_loss:0.010, val_acc:0.973]
Epoch [61/120    avg_loss:0.011, val_acc:0.970]
Epoch [62/120    avg_loss:0.011, val_acc:0.977]
Epoch [63/120    avg_loss:0.006, val_acc:0.978]
Epoch [64/120    avg_loss:0.004, val_acc:0.981]
Epoch [65/120    avg_loss:0.004, val_acc:0.981]
Epoch [66/120    avg_loss:0.019, val_acc:0.979]
Epoch [67/120    avg_loss:0.019, val_acc:0.965]
Epoch [68/120    avg_loss:0.014, val_acc:0.963]
Epoch [69/120    avg_loss:0.017, val_acc:0.966]
Epoch [70/120    avg_loss:0.008, val_acc:0.980]
Epoch [71/120    avg_loss:0.007, val_acc:0.978]
Epoch [72/120    avg_loss:0.005, val_acc:0.975]
Epoch [73/120    avg_loss:0.003, val_acc:0.978]
Epoch [74/120    avg_loss:0.004, val_acc:0.978]
Epoch [75/120    avg_loss:0.003, val_acc:0.978]
Epoch [76/120    avg_loss:0.027, val_acc:0.968]
Epoch [77/120    avg_loss:0.016, val_acc:0.980]
Epoch [78/120    avg_loss:0.007, val_acc:0.981]
Epoch [79/120    avg_loss:0.008, val_acc:0.979]
Epoch [80/120    avg_loss:0.005, val_acc:0.978]
Epoch [81/120    avg_loss:0.013, val_acc:0.970]
Epoch [82/120    avg_loss:0.011, val_acc:0.970]
Epoch [83/120    avg_loss:0.009, val_acc:0.979]
Epoch [84/120    avg_loss:0.007, val_acc:0.979]
Epoch [85/120    avg_loss:0.022, val_acc:0.973]
Epoch [86/120    avg_loss:0.018, val_acc:0.973]
Epoch [87/120    avg_loss:0.008, val_acc:0.972]
Epoch [88/120    avg_loss:0.007, val_acc:0.976]
Epoch [89/120    avg_loss:0.008, val_acc:0.978]
Epoch [90/120    avg_loss:0.003, val_acc:0.979]
Epoch [91/120    avg_loss:0.003, val_acc:0.978]
Epoch [92/120    avg_loss:0.003, val_acc:0.978]
Epoch [93/120    avg_loss:0.008, val_acc:0.976]
Epoch [94/120    avg_loss:0.003, val_acc:0.977]
Epoch [95/120    avg_loss:0.009, val_acc:0.979]
Epoch [96/120    avg_loss:0.003, val_acc:0.978]
Epoch [97/120    avg_loss:0.003, val_acc:0.978]
Epoch [98/120    avg_loss:0.002, val_acc:0.977]
Epoch [99/120    avg_loss:0.003, val_acc:0.977]
Epoch [100/120    avg_loss:0.002, val_acc:0.978]
Epoch [101/120    avg_loss:0.003, val_acc:0.978]
Epoch [102/120    avg_loss:0.003, val_acc:0.978]
Epoch [103/120    avg_loss:0.003, val_acc:0.977]
Epoch [104/120    avg_loss:0.003, val_acc:0.977]
Epoch [105/120    avg_loss:0.003, val_acc:0.977]
Epoch [106/120    avg_loss:0.003, val_acc:0.977]
Epoch [107/120    avg_loss:0.003, val_acc:0.977]
Epoch [108/120    avg_loss:0.002, val_acc:0.977]
Epoch [109/120    avg_loss:0.002, val_acc:0.977]
Epoch [110/120    avg_loss:0.003, val_acc:0.977]
Epoch [111/120    avg_loss:0.004, val_acc:0.977]
Epoch [112/120    avg_loss:0.006, val_acc:0.977]
Epoch [113/120    avg_loss:0.005, val_acc:0.977]
Epoch [114/120    avg_loss:0.002, val_acc:0.977]
Epoch [115/120    avg_loss:0.002, val_acc:0.977]
Epoch [116/120    avg_loss:0.003, val_acc:0.977]
Epoch [117/120    avg_loss:0.003, val_acc:0.977]
Epoch [118/120    avg_loss:0.004, val_acc:0.977]
Epoch [119/120    avg_loss:0.002, val_acc:0.977]
Epoch [120/120    avg_loss:0.002, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1276    3    0    0    1    0    0    0    1    4    0    0
     0    0    0]
 [   0    0    0  717    1   10    0    0    0    2    0    5   11    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    7    0    0    2    0    0    0    0  838   27    0    0
     0    1    0]
 [   0    0   15    0    0    0    1    0    0    0    1 2171   22    0
     0    0    0]
 [   0    0    1    0    0    0    0    0    0    0    0    0  531    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0    0    0
  1127    9    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    76  257    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.52845528455285

F1 scores:
[       nan 1.         0.9876161  0.97750511 0.99765808 0.98190045
 0.98568199 1.         1.         0.92307692 0.97725948 0.98235294
 0.96370236 1.         0.96201451 0.83713355 0.96385542]

Kappa:
0.9718046356299779
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc02af7f630>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.320, val_acc:0.484]
Epoch [2/120    avg_loss:1.485, val_acc:0.641]
Epoch [3/120    avg_loss:1.186, val_acc:0.566]
Epoch [4/120    avg_loss:1.003, val_acc:0.702]
Epoch [5/120    avg_loss:0.800, val_acc:0.775]
Epoch [6/120    avg_loss:0.712, val_acc:0.787]
Epoch [7/120    avg_loss:0.615, val_acc:0.826]
Epoch [8/120    avg_loss:0.519, val_acc:0.834]
Epoch [9/120    avg_loss:0.431, val_acc:0.821]
Epoch [10/120    avg_loss:0.450, val_acc:0.887]
Epoch [11/120    avg_loss:0.240, val_acc:0.864]
Epoch [12/120    avg_loss:0.299, val_acc:0.851]
Epoch [13/120    avg_loss:0.249, val_acc:0.895]
Epoch [14/120    avg_loss:0.175, val_acc:0.907]
Epoch [15/120    avg_loss:0.205, val_acc:0.903]
Epoch [16/120    avg_loss:0.155, val_acc:0.932]
Epoch [17/120    avg_loss:0.121, val_acc:0.915]
Epoch [18/120    avg_loss:0.114, val_acc:0.928]
Epoch [19/120    avg_loss:0.143, val_acc:0.929]
Epoch [20/120    avg_loss:0.072, val_acc:0.949]
Epoch [21/120    avg_loss:0.053, val_acc:0.952]
Epoch [22/120    avg_loss:0.071, val_acc:0.950]
Epoch [23/120    avg_loss:0.063, val_acc:0.955]
Epoch [24/120    avg_loss:0.062, val_acc:0.942]
Epoch [25/120    avg_loss:0.054, val_acc:0.958]
Epoch [26/120    avg_loss:0.075, val_acc:0.942]
Epoch [27/120    avg_loss:0.068, val_acc:0.960]
Epoch [28/120    avg_loss:0.034, val_acc:0.969]
Epoch [29/120    avg_loss:0.034, val_acc:0.961]
Epoch [30/120    avg_loss:0.034, val_acc:0.951]
Epoch [31/120    avg_loss:0.048, val_acc:0.969]
Epoch [32/120    avg_loss:0.032, val_acc:0.964]
Epoch [33/120    avg_loss:0.036, val_acc:0.968]
Epoch [34/120    avg_loss:0.034, val_acc:0.966]
Epoch [35/120    avg_loss:0.070, val_acc:0.950]
Epoch [36/120    avg_loss:0.043, val_acc:0.974]
Epoch [37/120    avg_loss:0.021, val_acc:0.976]
Epoch [38/120    avg_loss:0.020, val_acc:0.970]
Epoch [39/120    avg_loss:0.017, val_acc:0.973]
Epoch [40/120    avg_loss:0.012, val_acc:0.976]
Epoch [41/120    avg_loss:0.017, val_acc:0.965]
Epoch [42/120    avg_loss:0.011, val_acc:0.979]
Epoch [43/120    avg_loss:0.017, val_acc:0.968]
Epoch [44/120    avg_loss:0.020, val_acc:0.969]
Epoch [45/120    avg_loss:0.023, val_acc:0.967]
Epoch [46/120    avg_loss:0.055, val_acc:0.961]
Epoch [47/120    avg_loss:0.034, val_acc:0.963]
Epoch [48/120    avg_loss:0.037, val_acc:0.963]
Epoch [49/120    avg_loss:0.022, val_acc:0.973]
Epoch [50/120    avg_loss:0.040, val_acc:0.947]
Epoch [51/120    avg_loss:0.050, val_acc:0.935]
Epoch [52/120    avg_loss:0.034, val_acc:0.972]
Epoch [53/120    avg_loss:0.121, val_acc:0.967]
Epoch [54/120    avg_loss:0.137, val_acc:0.964]
Epoch [55/120    avg_loss:0.033, val_acc:0.967]
Epoch [56/120    avg_loss:0.019, val_acc:0.971]
Epoch [57/120    avg_loss:0.021, val_acc:0.972]
Epoch [58/120    avg_loss:0.019, val_acc:0.976]
Epoch [59/120    avg_loss:0.015, val_acc:0.977]
Epoch [60/120    avg_loss:0.017, val_acc:0.976]
Epoch [61/120    avg_loss:0.014, val_acc:0.977]
Epoch [62/120    avg_loss:0.015, val_acc:0.975]
Epoch [63/120    avg_loss:0.011, val_acc:0.975]
Epoch [64/120    avg_loss:0.012, val_acc:0.975]
Epoch [65/120    avg_loss:0.012, val_acc:0.975]
Epoch [66/120    avg_loss:0.014, val_acc:0.979]
Epoch [67/120    avg_loss:0.018, val_acc:0.978]
Epoch [68/120    avg_loss:0.012, val_acc:0.978]
Epoch [69/120    avg_loss:0.018, val_acc:0.981]
Epoch [70/120    avg_loss:0.012, val_acc:0.980]
Epoch [71/120    avg_loss:0.011, val_acc:0.979]
Epoch [72/120    avg_loss:0.020, val_acc:0.980]
Epoch [73/120    avg_loss:0.009, val_acc:0.979]
Epoch [74/120    avg_loss:0.009, val_acc:0.979]
Epoch [75/120    avg_loss:0.008, val_acc:0.979]
Epoch [76/120    avg_loss:0.008, val_acc:0.979]
Epoch [77/120    avg_loss:0.012, val_acc:0.979]
Epoch [78/120    avg_loss:0.013, val_acc:0.980]
Epoch [79/120    avg_loss:0.015, val_acc:0.982]
Epoch [80/120    avg_loss:0.010, val_acc:0.981]
Epoch [81/120    avg_loss:0.013, val_acc:0.979]
Epoch [82/120    avg_loss:0.009, val_acc:0.979]
Epoch [83/120    avg_loss:0.010, val_acc:0.981]
Epoch [84/120    avg_loss:0.009, val_acc:0.981]
Epoch [85/120    avg_loss:0.009, val_acc:0.980]
Epoch [86/120    avg_loss:0.010, val_acc:0.980]
Epoch [87/120    avg_loss:0.009, val_acc:0.979]
Epoch [88/120    avg_loss:0.008, val_acc:0.979]
Epoch [89/120    avg_loss:0.012, val_acc:0.979]
Epoch [90/120    avg_loss:0.007, val_acc:0.982]
Epoch [91/120    avg_loss:0.010, val_acc:0.980]
Epoch [92/120    avg_loss:0.007, val_acc:0.981]
Epoch [93/120    avg_loss:0.009, val_acc:0.980]
Epoch [94/120    avg_loss:0.007, val_acc:0.981]
Epoch [95/120    avg_loss:0.009, val_acc:0.981]
Epoch [96/120    avg_loss:0.009, val_acc:0.982]
Epoch [97/120    avg_loss:0.009, val_acc:0.982]
Epoch [98/120    avg_loss:0.011, val_acc:0.981]
Epoch [99/120    avg_loss:0.008, val_acc:0.981]
Epoch [100/120    avg_loss:0.009, val_acc:0.983]
Epoch [101/120    avg_loss:0.007, val_acc:0.984]
Epoch [102/120    avg_loss:0.010, val_acc:0.982]
Epoch [103/120    avg_loss:0.007, val_acc:0.985]
Epoch [104/120    avg_loss:0.014, val_acc:0.982]
Epoch [105/120    avg_loss:0.009, val_acc:0.983]
Epoch [106/120    avg_loss:0.014, val_acc:0.982]
Epoch [107/120    avg_loss:0.008, val_acc:0.983]
Epoch [108/120    avg_loss:0.008, val_acc:0.982]
Epoch [109/120    avg_loss:0.006, val_acc:0.983]
Epoch [110/120    avg_loss:0.008, val_acc:0.983]
Epoch [111/120    avg_loss:0.008, val_acc:0.984]
Epoch [112/120    avg_loss:0.007, val_acc:0.983]
Epoch [113/120    avg_loss:0.007, val_acc:0.984]
Epoch [114/120    avg_loss:0.008, val_acc:0.984]
Epoch [115/120    avg_loss:0.006, val_acc:0.985]
Epoch [116/120    avg_loss:0.006, val_acc:0.985]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.985]
Epoch [120/120    avg_loss:0.005, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1276    0    0    1    1    0    0    0    1    6    0    0
     0    0    0]
 [   0    0    0  736    0    0    0    0    0    0    0    1   10    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    0    0    0    0    0  843   20    1    0
     2    1    0]
 [   0    0    8    0    0    0    0    0    0    0    2 2197    3    0
     0    0    0]
 [   0    0    1    5    2    0    0    0    0    0    0    0  525    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1127   11    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    70  265    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.09214092140921

F1 scores:
[       nan 0.98765432 0.98953083 0.98858294 0.99297424 0.99423299
 0.98869631 1.         1.         1.         0.97966299 0.99075536
 0.97765363 1.         0.96242528 0.84935897 0.98809524]

Kappa:
0.9782268487978159
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fdf14bd7710>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.425, val_acc:0.404]
Epoch [2/120    avg_loss:1.625, val_acc:0.547]
Epoch [3/120    avg_loss:1.227, val_acc:0.698]
Epoch [4/120    avg_loss:1.041, val_acc:0.643]
Epoch [5/120    avg_loss:0.901, val_acc:0.750]
Epoch [6/120    avg_loss:0.723, val_acc:0.775]
Epoch [7/120    avg_loss:0.683, val_acc:0.748]
Epoch [8/120    avg_loss:0.611, val_acc:0.792]
Epoch [9/120    avg_loss:0.379, val_acc:0.866]
Epoch [10/120    avg_loss:0.271, val_acc:0.895]
Epoch [11/120    avg_loss:0.208, val_acc:0.911]
Epoch [12/120    avg_loss:0.178, val_acc:0.911]
Epoch [13/120    avg_loss:0.258, val_acc:0.897]
Epoch [14/120    avg_loss:0.288, val_acc:0.880]
Epoch [15/120    avg_loss:0.353, val_acc:0.840]
Epoch [16/120    avg_loss:0.213, val_acc:0.922]
Epoch [17/120    avg_loss:0.104, val_acc:0.927]
Epoch [18/120    avg_loss:0.116, val_acc:0.936]
Epoch [19/120    avg_loss:0.110, val_acc:0.914]
Epoch [20/120    avg_loss:0.109, val_acc:0.930]
Epoch [21/120    avg_loss:0.139, val_acc:0.941]
Epoch [22/120    avg_loss:0.130, val_acc:0.941]
Epoch [23/120    avg_loss:0.077, val_acc:0.950]
Epoch [24/120    avg_loss:0.073, val_acc:0.940]
Epoch [25/120    avg_loss:0.105, val_acc:0.915]
Epoch [26/120    avg_loss:0.082, val_acc:0.940]
Epoch [27/120    avg_loss:0.054, val_acc:0.936]
Epoch [28/120    avg_loss:0.057, val_acc:0.942]
Epoch [29/120    avg_loss:0.046, val_acc:0.961]
Epoch [30/120    avg_loss:0.045, val_acc:0.961]
Epoch [31/120    avg_loss:0.041, val_acc:0.956]
Epoch [32/120    avg_loss:0.029, val_acc:0.961]
Epoch [33/120    avg_loss:0.039, val_acc:0.952]
Epoch [34/120    avg_loss:0.032, val_acc:0.967]
Epoch [35/120    avg_loss:0.026, val_acc:0.971]
Epoch [36/120    avg_loss:0.027, val_acc:0.974]
Epoch [37/120    avg_loss:0.033, val_acc:0.966]
Epoch [38/120    avg_loss:0.025, val_acc:0.959]
Epoch [39/120    avg_loss:0.037, val_acc:0.957]
Epoch [40/120    avg_loss:0.021, val_acc:0.971]
Epoch [41/120    avg_loss:0.025, val_acc:0.970]
Epoch [42/120    avg_loss:0.014, val_acc:0.971]
Epoch [43/120    avg_loss:0.012, val_acc:0.975]
Epoch [44/120    avg_loss:0.016, val_acc:0.974]
Epoch [45/120    avg_loss:0.023, val_acc:0.970]
Epoch [46/120    avg_loss:0.014, val_acc:0.974]
Epoch [47/120    avg_loss:0.009, val_acc:0.976]
Epoch [48/120    avg_loss:0.016, val_acc:0.971]
Epoch [49/120    avg_loss:0.041, val_acc:0.961]
Epoch [50/120    avg_loss:0.075, val_acc:0.958]
Epoch [51/120    avg_loss:0.036, val_acc:0.960]
Epoch [52/120    avg_loss:0.026, val_acc:0.979]
Epoch [53/120    avg_loss:0.012, val_acc:0.974]
Epoch [54/120    avg_loss:0.015, val_acc:0.976]
Epoch [55/120    avg_loss:0.017, val_acc:0.977]
Epoch [56/120    avg_loss:0.015, val_acc:0.972]
Epoch [57/120    avg_loss:0.013, val_acc:0.981]
Epoch [58/120    avg_loss:0.011, val_acc:0.984]
Epoch [59/120    avg_loss:0.011, val_acc:0.976]
Epoch [60/120    avg_loss:0.012, val_acc:0.979]
Epoch [61/120    avg_loss:0.010, val_acc:0.973]
Epoch [62/120    avg_loss:0.007, val_acc:0.981]
Epoch [63/120    avg_loss:0.014, val_acc:0.983]
Epoch [64/120    avg_loss:0.021, val_acc:0.971]
Epoch [65/120    avg_loss:0.009, val_acc:0.983]
Epoch [66/120    avg_loss:0.006, val_acc:0.971]
Epoch [67/120    avg_loss:0.009, val_acc:0.978]
Epoch [68/120    avg_loss:0.005, val_acc:0.977]
Epoch [69/120    avg_loss:0.007, val_acc:0.983]
Epoch [70/120    avg_loss:0.005, val_acc:0.981]
Epoch [71/120    avg_loss:0.004, val_acc:0.984]
Epoch [72/120    avg_loss:0.006, val_acc:0.982]
Epoch [73/120    avg_loss:0.003, val_acc:0.983]
Epoch [74/120    avg_loss:0.009, val_acc:0.954]
Epoch [75/120    avg_loss:0.008, val_acc:0.977]
Epoch [76/120    avg_loss:0.006, val_acc:0.981]
Epoch [77/120    avg_loss:0.007, val_acc:0.980]
Epoch [78/120    avg_loss:0.005, val_acc:0.980]
Epoch [79/120    avg_loss:0.003, val_acc:0.982]
Epoch [80/120    avg_loss:0.004, val_acc:0.983]
Epoch [81/120    avg_loss:0.003, val_acc:0.983]
Epoch [82/120    avg_loss:0.005, val_acc:0.983]
Epoch [83/120    avg_loss:0.005, val_acc:0.976]
Epoch [84/120    avg_loss:0.008, val_acc:0.981]
Epoch [85/120    avg_loss:0.005, val_acc:0.983]
Epoch [86/120    avg_loss:0.004, val_acc:0.983]
Epoch [87/120    avg_loss:0.003, val_acc:0.983]
Epoch [88/120    avg_loss:0.005, val_acc:0.983]
Epoch [89/120    avg_loss:0.004, val_acc:0.983]
Epoch [90/120    avg_loss:0.003, val_acc:0.983]
Epoch [91/120    avg_loss:0.002, val_acc:0.983]
Epoch [92/120    avg_loss:0.005, val_acc:0.983]
Epoch [93/120    avg_loss:0.003, val_acc:0.983]
Epoch [94/120    avg_loss:0.003, val_acc:0.983]
Epoch [95/120    avg_loss:0.003, val_acc:0.983]
Epoch [96/120    avg_loss:0.003, val_acc:0.983]
Epoch [97/120    avg_loss:0.003, val_acc:0.983]
Epoch [98/120    avg_loss:0.004, val_acc:0.983]
Epoch [99/120    avg_loss:0.003, val_acc:0.983]
Epoch [100/120    avg_loss:0.003, val_acc:0.983]
Epoch [101/120    avg_loss:0.003, val_acc:0.983]
Epoch [102/120    avg_loss:0.003, val_acc:0.983]
Epoch [103/120    avg_loss:0.003, val_acc:0.983]
Epoch [104/120    avg_loss:0.004, val_acc:0.983]
Epoch [105/120    avg_loss:0.003, val_acc:0.983]
Epoch [106/120    avg_loss:0.003, val_acc:0.983]
Epoch [107/120    avg_loss:0.003, val_acc:0.983]
Epoch [108/120    avg_loss:0.003, val_acc:0.983]
Epoch [109/120    avg_loss:0.002, val_acc:0.983]
Epoch [110/120    avg_loss:0.003, val_acc:0.983]
Epoch [111/120    avg_loss:0.003, val_acc:0.983]
Epoch [112/120    avg_loss:0.002, val_acc:0.983]
Epoch [113/120    avg_loss:0.002, val_acc:0.983]
Epoch [114/120    avg_loss:0.002, val_acc:0.983]
Epoch [115/120    avg_loss:0.003, val_acc:0.983]
Epoch [116/120    avg_loss:0.002, val_acc:0.983]
Epoch [117/120    avg_loss:0.003, val_acc:0.983]
Epoch [118/120    avg_loss:0.003, val_acc:0.983]
Epoch [119/120    avg_loss:0.003, val_acc:0.983]
Epoch [120/120    avg_loss:0.002, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1261    5    2    2    0    0    0    0    7    3    5    0
     0    0    0]
 [   0    0    0  728    0    0    0    0    0    3    0    0   16    0
     0    0    0]
 [   0    0    0    0  211    0    0    0    0    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0  431    1    0    0    2    0    0    0    1
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  871    4    0    0
     0    0    0]
 [   0    0    2    0    0    0    0    0    0    1    3 2192   11    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    4    0  528    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1131    7    0]
 [   0    0    0    0    0    0   19    0    0    0    0    0    0    0
    14  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.61246612466125

F1 scores:
[       nan 0.975      0.98940761 0.98311951 0.99061033 0.99308756
 0.98271976 1.         1.         0.82926829 0.98864926 0.99365367
 0.96       0.99462366 0.99036778 0.94011976 0.96385542]

Kappa:
0.9841847667131878
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:52--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa3f9728780>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.362, val_acc:0.444]
Epoch [2/120    avg_loss:1.730, val_acc:0.502]
Epoch [3/120    avg_loss:1.420, val_acc:0.678]
Epoch [4/120    avg_loss:1.159, val_acc:0.714]
Epoch [5/120    avg_loss:1.040, val_acc:0.607]
Epoch [6/120    avg_loss:0.919, val_acc:0.798]
Epoch [7/120    avg_loss:0.792, val_acc:0.762]
Epoch [8/120    avg_loss:0.829, val_acc:0.801]
Epoch [9/120    avg_loss:0.555, val_acc:0.818]
Epoch [10/120    avg_loss:0.381, val_acc:0.887]
Epoch [11/120    avg_loss:0.320, val_acc:0.881]
Epoch [12/120    avg_loss:0.399, val_acc:0.780]
Epoch [13/120    avg_loss:0.471, val_acc:0.878]
Epoch [14/120    avg_loss:0.283, val_acc:0.894]
Epoch [15/120    avg_loss:0.191, val_acc:0.909]
Epoch [16/120    avg_loss:0.147, val_acc:0.917]
Epoch [17/120    avg_loss:0.136, val_acc:0.924]
Epoch [18/120    avg_loss:0.152, val_acc:0.916]
Epoch [19/120    avg_loss:0.153, val_acc:0.941]
Epoch [20/120    avg_loss:0.135, val_acc:0.938]
Epoch [21/120    avg_loss:0.105, val_acc:0.938]
Epoch [22/120    avg_loss:0.093, val_acc:0.929]
Epoch [23/120    avg_loss:0.078, val_acc:0.968]
Epoch [24/120    avg_loss:0.069, val_acc:0.946]
Epoch [25/120    avg_loss:0.060, val_acc:0.960]
Epoch [26/120    avg_loss:0.202, val_acc:0.925]
Epoch [27/120    avg_loss:0.079, val_acc:0.944]
Epoch [28/120    avg_loss:0.054, val_acc:0.962]
Epoch [29/120    avg_loss:0.057, val_acc:0.947]
Epoch [30/120    avg_loss:0.047, val_acc:0.968]
Epoch [31/120    avg_loss:0.039, val_acc:0.965]
Epoch [32/120    avg_loss:0.034, val_acc:0.964]
Epoch [33/120    avg_loss:0.062, val_acc:0.903]
Epoch [34/120    avg_loss:0.070, val_acc:0.972]
Epoch [35/120    avg_loss:0.039, val_acc:0.971]
Epoch [36/120    avg_loss:0.019, val_acc:0.954]
Epoch [37/120    avg_loss:0.073, val_acc:0.971]
Epoch [38/120    avg_loss:0.032, val_acc:0.975]
Epoch [39/120    avg_loss:0.033, val_acc:0.969]
Epoch [40/120    avg_loss:0.045, val_acc:0.967]
Epoch [41/120    avg_loss:0.057, val_acc:0.954]
Epoch [42/120    avg_loss:0.042, val_acc:0.966]
Epoch [43/120    avg_loss:0.028, val_acc:0.971]
Epoch [44/120    avg_loss:0.022, val_acc:0.975]
Epoch [45/120    avg_loss:0.020, val_acc:0.980]
Epoch [46/120    avg_loss:0.019, val_acc:0.981]
Epoch [47/120    avg_loss:0.014, val_acc:0.981]
Epoch [48/120    avg_loss:0.013, val_acc:0.980]
Epoch [49/120    avg_loss:0.014, val_acc:0.978]
Epoch [50/120    avg_loss:0.008, val_acc:0.985]
Epoch [51/120    avg_loss:0.009, val_acc:0.976]
Epoch [52/120    avg_loss:0.024, val_acc:0.975]
Epoch [53/120    avg_loss:0.023, val_acc:0.974]
Epoch [54/120    avg_loss:0.023, val_acc:0.962]
Epoch [55/120    avg_loss:0.022, val_acc:0.980]
Epoch [56/120    avg_loss:0.011, val_acc:0.982]
Epoch [57/120    avg_loss:0.009, val_acc:0.983]
Epoch [58/120    avg_loss:0.013, val_acc:0.984]
Epoch [59/120    avg_loss:0.009, val_acc:0.985]
Epoch [60/120    avg_loss:0.010, val_acc:0.985]
Epoch [61/120    avg_loss:0.007, val_acc:0.987]
Epoch [62/120    avg_loss:0.007, val_acc:0.985]
Epoch [63/120    avg_loss:0.009, val_acc:0.983]
Epoch [64/120    avg_loss:0.009, val_acc:0.983]
Epoch [65/120    avg_loss:0.010, val_acc:0.986]
Epoch [66/120    avg_loss:0.011, val_acc:0.987]
Epoch [67/120    avg_loss:0.006, val_acc:0.989]
Epoch [68/120    avg_loss:0.008, val_acc:0.989]
Epoch [69/120    avg_loss:0.042, val_acc:0.974]
Epoch [70/120    avg_loss:0.024, val_acc:0.978]
Epoch [71/120    avg_loss:0.009, val_acc:0.982]
Epoch [72/120    avg_loss:0.009, val_acc:0.991]
Epoch [73/120    avg_loss:0.005, val_acc:0.984]
Epoch [74/120    avg_loss:0.009, val_acc:0.985]
Epoch [75/120    avg_loss:0.013, val_acc:0.984]
Epoch [76/120    avg_loss:0.017, val_acc:0.970]
Epoch [77/120    avg_loss:0.015, val_acc:0.981]
Epoch [78/120    avg_loss:0.013, val_acc:0.988]
Epoch [79/120    avg_loss:0.013, val_acc:0.979]
Epoch [80/120    avg_loss:0.011, val_acc:0.989]
Epoch [81/120    avg_loss:0.005, val_acc:0.986]
Epoch [82/120    avg_loss:0.004, val_acc:0.985]
Epoch [83/120    avg_loss:0.006, val_acc:0.985]
Epoch [84/120    avg_loss:0.007, val_acc:0.983]
Epoch [85/120    avg_loss:0.004, val_acc:0.990]
Epoch [86/120    avg_loss:0.004, val_acc:0.991]
Epoch [87/120    avg_loss:0.007, val_acc:0.988]
Epoch [88/120    avg_loss:0.004, val_acc:0.987]
Epoch [89/120    avg_loss:0.005, val_acc:0.987]
Epoch [90/120    avg_loss:0.003, val_acc:0.987]
Epoch [91/120    avg_loss:0.002, val_acc:0.987]
Epoch [92/120    avg_loss:0.005, val_acc:0.987]
Epoch [93/120    avg_loss:0.003, val_acc:0.987]
Epoch [94/120    avg_loss:0.004, val_acc:0.988]
Epoch [95/120    avg_loss:0.003, val_acc:0.988]
Epoch [96/120    avg_loss:0.002, val_acc:0.988]
Epoch [97/120    avg_loss:0.003, val_acc:0.988]
Epoch [98/120    avg_loss:0.003, val_acc:0.988]
Epoch [99/120    avg_loss:0.004, val_acc:0.990]
Epoch [100/120    avg_loss:0.005, val_acc:0.990]
Epoch [101/120    avg_loss:0.003, val_acc:0.990]
Epoch [102/120    avg_loss:0.004, val_acc:0.990]
Epoch [103/120    avg_loss:0.002, val_acc:0.990]
Epoch [104/120    avg_loss:0.002, val_acc:0.990]
Epoch [105/120    avg_loss:0.003, val_acc:0.990]
Epoch [106/120    avg_loss:0.002, val_acc:0.990]
Epoch [107/120    avg_loss:0.003, val_acc:0.990]
Epoch [108/120    avg_loss:0.003, val_acc:0.990]
Epoch [109/120    avg_loss:0.002, val_acc:0.990]
Epoch [110/120    avg_loss:0.003, val_acc:0.990]
Epoch [111/120    avg_loss:0.003, val_acc:0.990]
Epoch [112/120    avg_loss:0.003, val_acc:0.990]
Epoch [113/120    avg_loss:0.003, val_acc:0.990]
Epoch [114/120    avg_loss:0.002, val_acc:0.990]
Epoch [115/120    avg_loss:0.003, val_acc:0.990]
Epoch [116/120    avg_loss:0.003, val_acc:0.990]
Epoch [117/120    avg_loss:0.003, val_acc:0.990]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.005, val_acc:0.990]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1279    2    0    0    0    0    0    0    1    3    0    0
     0    0    0]
 [   0    0    0  733    0    0    0    0    0    3    0    0    9    0
     2    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    2    0    0    0    0  859    8    0    0
     0    3    0]
 [   0    0   13    0    0    0    0    0    0    0    5 2188    4    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0    0   10    0  520    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    2    0    0    0    0    0    0    0
  1130    4    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    10  321    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    6    0
     0    0   78]]

Accuracy:
98.72086720867209

F1 scores:
[       nan 1.         0.99147287 0.98720539 1.         0.99198167
 0.98648649 0.98039216 0.99883586 0.84210526 0.98171429 0.99251531
 0.96834264 1.         0.99079351 0.95111111 0.94545455]

Kappa:
0.9854178275661559
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:54--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f94cc0cd6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.273, val_acc:0.496]
Epoch [2/120    avg_loss:1.659, val_acc:0.592]
Epoch [3/120    avg_loss:1.318, val_acc:0.602]
Epoch [4/120    avg_loss:1.078, val_acc:0.779]
Epoch [5/120    avg_loss:0.838, val_acc:0.803]
Epoch [6/120    avg_loss:0.883, val_acc:0.791]
Epoch [7/120    avg_loss:0.598, val_acc:0.820]
Epoch [8/120    avg_loss:0.626, val_acc:0.843]
Epoch [9/120    avg_loss:0.549, val_acc:0.857]
Epoch [10/120    avg_loss:0.446, val_acc:0.892]
Epoch [11/120    avg_loss:0.289, val_acc:0.908]
Epoch [12/120    avg_loss:0.268, val_acc:0.903]
Epoch [13/120    avg_loss:0.241, val_acc:0.889]
Epoch [14/120    avg_loss:0.184, val_acc:0.927]
Epoch [15/120    avg_loss:0.177, val_acc:0.826]
Epoch [16/120    avg_loss:0.283, val_acc:0.904]
Epoch [17/120    avg_loss:0.187, val_acc:0.914]
Epoch [18/120    avg_loss:0.215, val_acc:0.911]
Epoch [19/120    avg_loss:0.140, val_acc:0.933]
Epoch [20/120    avg_loss:0.117, val_acc:0.942]
Epoch [21/120    avg_loss:0.080, val_acc:0.943]
Epoch [22/120    avg_loss:0.083, val_acc:0.950]
Epoch [23/120    avg_loss:0.076, val_acc:0.945]
Epoch [24/120    avg_loss:0.075, val_acc:0.947]
Epoch [25/120    avg_loss:0.072, val_acc:0.949]
Epoch [26/120    avg_loss:0.109, val_acc:0.923]
Epoch [27/120    avg_loss:0.081, val_acc:0.940]
Epoch [28/120    avg_loss:0.068, val_acc:0.951]
Epoch [29/120    avg_loss:0.075, val_acc:0.930]
Epoch [30/120    avg_loss:0.071, val_acc:0.948]
Epoch [31/120    avg_loss:0.043, val_acc:0.969]
Epoch [32/120    avg_loss:0.048, val_acc:0.958]
Epoch [33/120    avg_loss:0.035, val_acc:0.968]
Epoch [34/120    avg_loss:0.034, val_acc:0.958]
Epoch [35/120    avg_loss:0.025, val_acc:0.973]
Epoch [36/120    avg_loss:0.025, val_acc:0.976]
Epoch [37/120    avg_loss:0.036, val_acc:0.967]
Epoch [38/120    avg_loss:0.022, val_acc:0.976]
Epoch [39/120    avg_loss:0.016, val_acc:0.977]
Epoch [40/120    avg_loss:0.015, val_acc:0.971]
Epoch [41/120    avg_loss:0.026, val_acc:0.951]
Epoch [42/120    avg_loss:0.042, val_acc:0.963]
Epoch [43/120    avg_loss:0.029, val_acc:0.954]
Epoch [44/120    avg_loss:0.059, val_acc:0.954]
Epoch [45/120    avg_loss:0.035, val_acc:0.961]
Epoch [46/120    avg_loss:0.022, val_acc:0.972]
Epoch [47/120    avg_loss:0.011, val_acc:0.972]
Epoch [48/120    avg_loss:0.018, val_acc:0.963]
Epoch [49/120    avg_loss:0.026, val_acc:0.965]
Epoch [50/120    avg_loss:0.019, val_acc:0.963]
Epoch [51/120    avg_loss:0.046, val_acc:0.956]
Epoch [52/120    avg_loss:0.023, val_acc:0.957]
Epoch [53/120    avg_loss:0.026, val_acc:0.968]
Epoch [54/120    avg_loss:0.014, val_acc:0.968]
Epoch [55/120    avg_loss:0.016, val_acc:0.971]
Epoch [56/120    avg_loss:0.010, val_acc:0.974]
Epoch [57/120    avg_loss:0.009, val_acc:0.975]
Epoch [58/120    avg_loss:0.011, val_acc:0.976]
Epoch [59/120    avg_loss:0.010, val_acc:0.976]
Epoch [60/120    avg_loss:0.011, val_acc:0.973]
Epoch [61/120    avg_loss:0.008, val_acc:0.973]
Epoch [62/120    avg_loss:0.012, val_acc:0.971]
Epoch [63/120    avg_loss:0.012, val_acc:0.974]
Epoch [64/120    avg_loss:0.008, val_acc:0.975]
Epoch [65/120    avg_loss:0.011, val_acc:0.973]
Epoch [66/120    avg_loss:0.008, val_acc:0.973]
Epoch [67/120    avg_loss:0.009, val_acc:0.973]
Epoch [68/120    avg_loss:0.010, val_acc:0.973]
Epoch [69/120    avg_loss:0.016, val_acc:0.973]
Epoch [70/120    avg_loss:0.010, val_acc:0.973]
Epoch [71/120    avg_loss:0.009, val_acc:0.973]
Epoch [72/120    avg_loss:0.009, val_acc:0.973]
Epoch [73/120    avg_loss:0.009, val_acc:0.973]
Epoch [74/120    avg_loss:0.008, val_acc:0.973]
Epoch [75/120    avg_loss:0.007, val_acc:0.973]
Epoch [76/120    avg_loss:0.010, val_acc:0.973]
Epoch [77/120    avg_loss:0.010, val_acc:0.973]
Epoch [78/120    avg_loss:0.008, val_acc:0.973]
Epoch [79/120    avg_loss:0.012, val_acc:0.973]
Epoch [80/120    avg_loss:0.009, val_acc:0.973]
Epoch [81/120    avg_loss:0.008, val_acc:0.973]
Epoch [82/120    avg_loss:0.015, val_acc:0.973]
Epoch [83/120    avg_loss:0.010, val_acc:0.973]
Epoch [84/120    avg_loss:0.008, val_acc:0.973]
Epoch [85/120    avg_loss:0.009, val_acc:0.973]
Epoch [86/120    avg_loss:0.008, val_acc:0.973]
Epoch [87/120    avg_loss:0.006, val_acc:0.973]
Epoch [88/120    avg_loss:0.009, val_acc:0.973]
Epoch [89/120    avg_loss:0.011, val_acc:0.973]
Epoch [90/120    avg_loss:0.007, val_acc:0.973]
Epoch [91/120    avg_loss:0.007, val_acc:0.973]
Epoch [92/120    avg_loss:0.007, val_acc:0.973]
Epoch [93/120    avg_loss:0.011, val_acc:0.973]
Epoch [94/120    avg_loss:0.008, val_acc:0.973]
Epoch [95/120    avg_loss:0.008, val_acc:0.973]
Epoch [96/120    avg_loss:0.011, val_acc:0.973]
Epoch [97/120    avg_loss:0.010, val_acc:0.973]
Epoch [98/120    avg_loss:0.008, val_acc:0.973]
Epoch [99/120    avg_loss:0.006, val_acc:0.973]
Epoch [100/120    avg_loss:0.010, val_acc:0.973]
Epoch [101/120    avg_loss:0.008, val_acc:0.973]
Epoch [102/120    avg_loss:0.010, val_acc:0.973]
Epoch [103/120    avg_loss:0.011, val_acc:0.973]
Epoch [104/120    avg_loss:0.007, val_acc:0.973]
Epoch [105/120    avg_loss:0.008, val_acc:0.973]
Epoch [106/120    avg_loss:0.006, val_acc:0.973]
Epoch [107/120    avg_loss:0.011, val_acc:0.973]
Epoch [108/120    avg_loss:0.008, val_acc:0.973]
Epoch [109/120    avg_loss:0.009, val_acc:0.973]
Epoch [110/120    avg_loss:0.008, val_acc:0.973]
Epoch [111/120    avg_loss:0.011, val_acc:0.973]
Epoch [112/120    avg_loss:0.007, val_acc:0.973]
Epoch [113/120    avg_loss:0.010, val_acc:0.973]
Epoch [114/120    avg_loss:0.007, val_acc:0.973]
Epoch [115/120    avg_loss:0.007, val_acc:0.973]
Epoch [116/120    avg_loss:0.007, val_acc:0.973]
Epoch [117/120    avg_loss:0.011, val_acc:0.973]
Epoch [118/120    avg_loss:0.007, val_acc:0.973]
Epoch [119/120    avg_loss:0.011, val_acc:0.973]
Epoch [120/120    avg_loss:0.007, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1262    1    8    0    1    0    0    2    4    7    0    0
     0    0    0]
 [   0    0    0  722    0    3    0    0    0    5    1    1   12    3
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    3    0    0    0    0  857    8    0    0
     1    5    0]
 [   0    0   10    0    0    0    2    0    0    3    1 2188    5    1
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0  532    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    1    0    0    0
  1128    8    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    37  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
98.26558265582656

F1 scores:
[       nan 0.98765432 0.98632278 0.98031229 0.97685185 0.98973774
 0.98942598 1.         0.99767442 0.73913043 0.98562392 0.99094203
 0.97704316 0.98930481 0.97874187 0.9107413  0.96341463]

Kappa:
0.9802277455607169
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3e113b3748>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.293, val_acc:0.447]
Epoch [2/120    avg_loss:1.589, val_acc:0.666]
Epoch [3/120    avg_loss:1.293, val_acc:0.663]
Epoch [4/120    avg_loss:0.944, val_acc:0.688]
Epoch [5/120    avg_loss:0.834, val_acc:0.739]
Epoch [6/120    avg_loss:0.704, val_acc:0.770]
Epoch [7/120    avg_loss:0.746, val_acc:0.814]
Epoch [8/120    avg_loss:0.559, val_acc:0.874]
Epoch [9/120    avg_loss:0.373, val_acc:0.888]
Epoch [10/120    avg_loss:0.353, val_acc:0.865]
Epoch [11/120    avg_loss:0.382, val_acc:0.845]
Epoch [12/120    avg_loss:0.439, val_acc:0.850]
Epoch [13/120    avg_loss:0.321, val_acc:0.860]
Epoch [14/120    avg_loss:0.216, val_acc:0.926]
Epoch [15/120    avg_loss:0.168, val_acc:0.903]
Epoch [16/120    avg_loss:0.168, val_acc:0.908]
Epoch [17/120    avg_loss:0.128, val_acc:0.929]
Epoch [18/120    avg_loss:0.099, val_acc:0.905]
Epoch [19/120    avg_loss:0.098, val_acc:0.941]
Epoch [20/120    avg_loss:0.086, val_acc:0.949]
Epoch [21/120    avg_loss:0.075, val_acc:0.944]
Epoch [22/120    avg_loss:0.084, val_acc:0.920]
Epoch [23/120    avg_loss:0.090, val_acc:0.937]
Epoch [24/120    avg_loss:0.074, val_acc:0.950]
Epoch [25/120    avg_loss:0.077, val_acc:0.955]
Epoch [26/120    avg_loss:0.063, val_acc:0.950]
Epoch [27/120    avg_loss:0.049, val_acc:0.963]
Epoch [28/120    avg_loss:0.061, val_acc:0.959]
Epoch [29/120    avg_loss:0.072, val_acc:0.960]
Epoch [30/120    avg_loss:0.062, val_acc:0.960]
Epoch [31/120    avg_loss:0.061, val_acc:0.964]
Epoch [32/120    avg_loss:0.063, val_acc:0.928]
Epoch [33/120    avg_loss:0.069, val_acc:0.968]
Epoch [34/120    avg_loss:0.034, val_acc:0.974]
Epoch [35/120    avg_loss:0.039, val_acc:0.969]
Epoch [36/120    avg_loss:0.028, val_acc:0.971]
Epoch [37/120    avg_loss:0.018, val_acc:0.966]
Epoch [38/120    avg_loss:0.020, val_acc:0.966]
Epoch [39/120    avg_loss:0.024, val_acc:0.973]
Epoch [40/120    avg_loss:0.022, val_acc:0.963]
Epoch [41/120    avg_loss:0.016, val_acc:0.982]
Epoch [42/120    avg_loss:0.017, val_acc:0.975]
Epoch [43/120    avg_loss:0.047, val_acc:0.965]
Epoch [44/120    avg_loss:0.028, val_acc:0.978]
Epoch [45/120    avg_loss:0.013, val_acc:0.980]
Epoch [46/120    avg_loss:0.014, val_acc:0.976]
Epoch [47/120    avg_loss:0.039, val_acc:0.978]
Epoch [48/120    avg_loss:0.023, val_acc:0.944]
Epoch [49/120    avg_loss:0.136, val_acc:0.965]
Epoch [50/120    avg_loss:0.026, val_acc:0.960]
Epoch [51/120    avg_loss:0.024, val_acc:0.971]
Epoch [52/120    avg_loss:0.019, val_acc:0.970]
Epoch [53/120    avg_loss:0.021, val_acc:0.971]
Epoch [54/120    avg_loss:0.014, val_acc:0.979]
Epoch [55/120    avg_loss:0.014, val_acc:0.982]
Epoch [56/120    avg_loss:0.008, val_acc:0.981]
Epoch [57/120    avg_loss:0.010, val_acc:0.980]
Epoch [58/120    avg_loss:0.009, val_acc:0.981]
Epoch [59/120    avg_loss:0.009, val_acc:0.980]
Epoch [60/120    avg_loss:0.012, val_acc:0.981]
Epoch [61/120    avg_loss:0.007, val_acc:0.981]
Epoch [62/120    avg_loss:0.006, val_acc:0.981]
Epoch [63/120    avg_loss:0.008, val_acc:0.982]
Epoch [64/120    avg_loss:0.008, val_acc:0.981]
Epoch [65/120    avg_loss:0.007, val_acc:0.980]
Epoch [66/120    avg_loss:0.008, val_acc:0.980]
Epoch [67/120    avg_loss:0.010, val_acc:0.978]
Epoch [68/120    avg_loss:0.009, val_acc:0.978]
Epoch [69/120    avg_loss:0.013, val_acc:0.978]
Epoch [70/120    avg_loss:0.008, val_acc:0.978]
Epoch [71/120    avg_loss:0.012, val_acc:0.979]
Epoch [72/120    avg_loss:0.006, val_acc:0.979]
Epoch [73/120    avg_loss:0.008, val_acc:0.979]
Epoch [74/120    avg_loss:0.004, val_acc:0.978]
Epoch [75/120    avg_loss:0.006, val_acc:0.976]
Epoch [76/120    avg_loss:0.007, val_acc:0.979]
Epoch [77/120    avg_loss:0.007, val_acc:0.978]
Epoch [78/120    avg_loss:0.005, val_acc:0.979]
Epoch [79/120    avg_loss:0.006, val_acc:0.979]
Epoch [80/120    avg_loss:0.005, val_acc:0.978]
Epoch [81/120    avg_loss:0.005, val_acc:0.979]
Epoch [82/120    avg_loss:0.007, val_acc:0.978]
Epoch [83/120    avg_loss:0.009, val_acc:0.978]
Epoch [84/120    avg_loss:0.007, val_acc:0.978]
Epoch [85/120    avg_loss:0.007, val_acc:0.978]
Epoch [86/120    avg_loss:0.007, val_acc:0.978]
Epoch [87/120    avg_loss:0.009, val_acc:0.978]
Epoch [88/120    avg_loss:0.007, val_acc:0.978]
Epoch [89/120    avg_loss:0.006, val_acc:0.978]
Epoch [90/120    avg_loss:0.008, val_acc:0.978]
Epoch [91/120    avg_loss:0.009, val_acc:0.978]
Epoch [92/120    avg_loss:0.006, val_acc:0.978]
Epoch [93/120    avg_loss:0.008, val_acc:0.978]
Epoch [94/120    avg_loss:0.006, val_acc:0.978]
Epoch [95/120    avg_loss:0.005, val_acc:0.978]
Epoch [96/120    avg_loss:0.005, val_acc:0.978]
Epoch [97/120    avg_loss:0.006, val_acc:0.978]
Epoch [98/120    avg_loss:0.006, val_acc:0.978]
Epoch [99/120    avg_loss:0.008, val_acc:0.978]
Epoch [100/120    avg_loss:0.006, val_acc:0.978]
Epoch [101/120    avg_loss:0.007, val_acc:0.978]
Epoch [102/120    avg_loss:0.007, val_acc:0.978]
Epoch [103/120    avg_loss:0.007, val_acc:0.978]
Epoch [104/120    avg_loss:0.006, val_acc:0.978]
Epoch [105/120    avg_loss:0.009, val_acc:0.978]
Epoch [106/120    avg_loss:0.006, val_acc:0.978]
Epoch [107/120    avg_loss:0.007, val_acc:0.978]
Epoch [108/120    avg_loss:0.006, val_acc:0.978]
Epoch [109/120    avg_loss:0.007, val_acc:0.978]
Epoch [110/120    avg_loss:0.009, val_acc:0.978]
Epoch [111/120    avg_loss:0.008, val_acc:0.978]
Epoch [112/120    avg_loss:0.006, val_acc:0.978]
Epoch [113/120    avg_loss:0.008, val_acc:0.978]
Epoch [114/120    avg_loss:0.009, val_acc:0.978]
Epoch [115/120    avg_loss:0.006, val_acc:0.978]
Epoch [116/120    avg_loss:0.009, val_acc:0.978]
Epoch [117/120    avg_loss:0.007, val_acc:0.978]
Epoch [118/120    avg_loss:0.007, val_acc:0.978]
Epoch [119/120    avg_loss:0.008, val_acc:0.978]
Epoch [120/120    avg_loss:0.008, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    0    5    0    3    0    0    0    3    6    1    0
     0    0    0]
 [   0    0    0  734    0    2    0    0    0    4    1    0    2    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    2    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    2    0    0    0    2  850   14    0    0
     2    1    0]
 [   0    0    5    0    0    0    0    0    0    1    2 2194    7    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  532    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1136    3    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    32  303    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.55826558265582

F1 scores:
[       nan 1.         0.98945724 0.99122215 0.98839907 0.9908046
 0.98642534 0.98039216 0.99649942 0.8        0.98209128 0.99096658
 0.98427382 0.98666667 0.98397575 0.9266055  0.97619048]

Kappa:
0.9835591916336226
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:10:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f895288f6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.326, val_acc:0.497]
Epoch [2/120    avg_loss:1.831, val_acc:0.552]
Epoch [3/120    avg_loss:1.412, val_acc:0.608]
Epoch [4/120    avg_loss:1.136, val_acc:0.595]
Epoch [5/120    avg_loss:0.856, val_acc:0.762]
Epoch [6/120    avg_loss:0.993, val_acc:0.780]
Epoch [7/120    avg_loss:0.719, val_acc:0.799]
Epoch [8/120    avg_loss:0.591, val_acc:0.826]
Epoch [9/120    avg_loss:0.540, val_acc:0.847]
Epoch [10/120    avg_loss:0.354, val_acc:0.891]
Epoch [11/120    avg_loss:0.324, val_acc:0.890]
Epoch [12/120    avg_loss:0.364, val_acc:0.871]
Epoch [13/120    avg_loss:0.298, val_acc:0.930]
Epoch [14/120    avg_loss:0.179, val_acc:0.917]
Epoch [15/120    avg_loss:0.155, val_acc:0.949]
Epoch [16/120    avg_loss:0.149, val_acc:0.918]
Epoch [17/120    avg_loss:0.113, val_acc:0.943]
Epoch [18/120    avg_loss:0.161, val_acc:0.948]
Epoch [19/120    avg_loss:0.115, val_acc:0.909]
Epoch [20/120    avg_loss:0.113, val_acc:0.945]
Epoch [21/120    avg_loss:0.068, val_acc:0.959]
Epoch [22/120    avg_loss:0.051, val_acc:0.957]
Epoch [23/120    avg_loss:0.057, val_acc:0.951]
Epoch [24/120    avg_loss:0.059, val_acc:0.968]
Epoch [25/120    avg_loss:0.076, val_acc:0.956]
Epoch [26/120    avg_loss:0.073, val_acc:0.951]
Epoch [27/120    avg_loss:0.053, val_acc:0.955]
Epoch [28/120    avg_loss:0.067, val_acc:0.963]
Epoch [29/120    avg_loss:0.040, val_acc:0.971]
Epoch [30/120    avg_loss:0.043, val_acc:0.975]
Epoch [31/120    avg_loss:0.031, val_acc:0.958]
Epoch [32/120    avg_loss:0.033, val_acc:0.975]
Epoch [33/120    avg_loss:0.046, val_acc:0.971]
Epoch [34/120    avg_loss:0.064, val_acc:0.956]
Epoch [35/120    avg_loss:0.049, val_acc:0.960]
Epoch [36/120    avg_loss:0.030, val_acc:0.969]
Epoch [37/120    avg_loss:0.033, val_acc:0.967]
Epoch [38/120    avg_loss:0.021, val_acc:0.973]
Epoch [39/120    avg_loss:0.022, val_acc:0.972]
Epoch [40/120    avg_loss:0.037, val_acc:0.921]
Epoch [41/120    avg_loss:0.058, val_acc:0.967]
Epoch [42/120    avg_loss:0.056, val_acc:0.967]
Epoch [43/120    avg_loss:0.045, val_acc:0.971]
Epoch [44/120    avg_loss:0.033, val_acc:0.969]
Epoch [45/120    avg_loss:0.022, val_acc:0.975]
Epoch [46/120    avg_loss:0.018, val_acc:0.977]
Epoch [47/120    avg_loss:0.026, val_acc:0.978]
Epoch [48/120    avg_loss:0.028, val_acc:0.977]
Epoch [49/120    avg_loss:0.027, val_acc:0.977]
Epoch [50/120    avg_loss:0.025, val_acc:0.978]
Epoch [51/120    avg_loss:0.018, val_acc:0.981]
Epoch [52/120    avg_loss:0.015, val_acc:0.976]
Epoch [53/120    avg_loss:0.016, val_acc:0.977]
Epoch [54/120    avg_loss:0.008, val_acc:0.980]
Epoch [55/120    avg_loss:0.013, val_acc:0.979]
Epoch [56/120    avg_loss:0.018, val_acc:0.971]
Epoch [57/120    avg_loss:0.018, val_acc:0.970]
Epoch [58/120    avg_loss:0.014, val_acc:0.971]
Epoch [59/120    avg_loss:0.022, val_acc:0.977]
Epoch [60/120    avg_loss:0.010, val_acc:0.981]
Epoch [61/120    avg_loss:0.005, val_acc:0.980]
Epoch [62/120    avg_loss:0.010, val_acc:0.981]
Epoch [63/120    avg_loss:0.015, val_acc:0.972]
Epoch [64/120    avg_loss:0.010, val_acc:0.986]
Epoch [65/120    avg_loss:0.018, val_acc:0.979]
Epoch [66/120    avg_loss:0.008, val_acc:0.983]
Epoch [67/120    avg_loss:0.005, val_acc:0.984]
Epoch [68/120    avg_loss:0.015, val_acc:0.992]
Epoch [69/120    avg_loss:0.011, val_acc:0.982]
Epoch [70/120    avg_loss:0.009, val_acc:0.980]
Epoch [71/120    avg_loss:0.008, val_acc:0.980]
Epoch [72/120    avg_loss:0.006, val_acc:0.982]
Epoch [73/120    avg_loss:0.012, val_acc:0.980]
Epoch [74/120    avg_loss:0.008, val_acc:0.978]
Epoch [75/120    avg_loss:0.013, val_acc:0.980]
Epoch [76/120    avg_loss:0.009, val_acc:0.983]
Epoch [77/120    avg_loss:0.006, val_acc:0.982]
Epoch [78/120    avg_loss:0.010, val_acc:0.985]
Epoch [79/120    avg_loss:0.009, val_acc:0.979]
Epoch [80/120    avg_loss:0.006, val_acc:0.983]
Epoch [81/120    avg_loss:0.005, val_acc:0.985]
Epoch [82/120    avg_loss:0.005, val_acc:0.985]
Epoch [83/120    avg_loss:0.005, val_acc:0.988]
Epoch [84/120    avg_loss:0.003, val_acc:0.988]
Epoch [85/120    avg_loss:0.006, val_acc:0.986]
Epoch [86/120    avg_loss:0.005, val_acc:0.986]
Epoch [87/120    avg_loss:0.004, val_acc:0.986]
Epoch [88/120    avg_loss:0.003, val_acc:0.985]
Epoch [89/120    avg_loss:0.004, val_acc:0.984]
Epoch [90/120    avg_loss:0.003, val_acc:0.985]
Epoch [91/120    avg_loss:0.003, val_acc:0.985]
Epoch [92/120    avg_loss:0.003, val_acc:0.985]
Epoch [93/120    avg_loss:0.003, val_acc:0.985]
Epoch [94/120    avg_loss:0.003, val_acc:0.985]
Epoch [95/120    avg_loss:0.004, val_acc:0.985]
Epoch [96/120    avg_loss:0.002, val_acc:0.985]
Epoch [97/120    avg_loss:0.004, val_acc:0.985]
Epoch [98/120    avg_loss:0.002, val_acc:0.985]
Epoch [99/120    avg_loss:0.004, val_acc:0.985]
Epoch [100/120    avg_loss:0.003, val_acc:0.985]
Epoch [101/120    avg_loss:0.004, val_acc:0.985]
Epoch [102/120    avg_loss:0.002, val_acc:0.985]
Epoch [103/120    avg_loss:0.002, val_acc:0.985]
Epoch [104/120    avg_loss:0.003, val_acc:0.985]
Epoch [105/120    avg_loss:0.002, val_acc:0.985]
Epoch [106/120    avg_loss:0.003, val_acc:0.985]
Epoch [107/120    avg_loss:0.003, val_acc:0.985]
Epoch [108/120    avg_loss:0.003, val_acc:0.985]
Epoch [109/120    avg_loss:0.002, val_acc:0.985]
Epoch [110/120    avg_loss:0.002, val_acc:0.985]
Epoch [111/120    avg_loss:0.004, val_acc:0.985]
Epoch [112/120    avg_loss:0.003, val_acc:0.985]
Epoch [113/120    avg_loss:0.004, val_acc:0.985]
Epoch [114/120    avg_loss:0.003, val_acc:0.985]
Epoch [115/120    avg_loss:0.003, val_acc:0.985]
Epoch [116/120    avg_loss:0.003, val_acc:0.985]
Epoch [117/120    avg_loss:0.003, val_acc:0.985]
Epoch [118/120    avg_loss:0.003, val_acc:0.985]
Epoch [119/120    avg_loss:0.003, val_acc:0.985]
Epoch [120/120    avg_loss:0.003, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1271    1    4    0    0    0    0    0    3    3    3    0
     0    0    0]
 [   0    0    0  727    0    5    1    0    0    3    0    0   10    1
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    2    0    0    0    0  860    4    0    0
     1    3    0]
 [   0    0   16    0    0    0    1    0    0    1    3 2188    0    1
     0    0    0]
 [   0    0    0    1    0    1    0    0    0    0    3    0  528    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1137    1    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    56  287    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.36314363143632

F1 scores:
[       nan 1.         0.98641832 0.98176907 0.98598131 0.98742857
 0.99469295 1.         0.99883586 0.76923077 0.98567335 0.9931911
 0.97959184 0.99462366 0.97429306 0.89968652 0.98203593]

Kappa:
0.9813334854911611
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f3a63537668>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.304, val_acc:0.486]
Epoch [2/120    avg_loss:1.598, val_acc:0.542]
Epoch [3/120    avg_loss:1.388, val_acc:0.679]
Epoch [4/120    avg_loss:1.094, val_acc:0.728]
Epoch [5/120    avg_loss:0.799, val_acc:0.793]
Epoch [6/120    avg_loss:0.781, val_acc:0.798]
Epoch [7/120    avg_loss:0.831, val_acc:0.808]
Epoch [8/120    avg_loss:0.604, val_acc:0.747]
Epoch [9/120    avg_loss:0.453, val_acc:0.858]
Epoch [10/120    avg_loss:0.401, val_acc:0.872]
Epoch [11/120    avg_loss:0.345, val_acc:0.884]
Epoch [12/120    avg_loss:0.230, val_acc:0.914]
Epoch [13/120    avg_loss:0.176, val_acc:0.920]
Epoch [14/120    avg_loss:0.163, val_acc:0.938]
Epoch [15/120    avg_loss:0.113, val_acc:0.922]
Epoch [16/120    avg_loss:0.218, val_acc:0.880]
Epoch [17/120    avg_loss:0.132, val_acc:0.930]
Epoch [18/120    avg_loss:0.114, val_acc:0.949]
Epoch [19/120    avg_loss:0.108, val_acc:0.874]
Epoch [20/120    avg_loss:0.096, val_acc:0.951]
Epoch [21/120    avg_loss:0.114, val_acc:0.947]
Epoch [22/120    avg_loss:0.068, val_acc:0.964]
Epoch [23/120    avg_loss:0.059, val_acc:0.955]
Epoch [24/120    avg_loss:0.063, val_acc:0.959]
Epoch [25/120    avg_loss:0.052, val_acc:0.954]
Epoch [26/120    avg_loss:0.053, val_acc:0.964]
Epoch [27/120    avg_loss:0.040, val_acc:0.948]
Epoch [28/120    avg_loss:0.068, val_acc:0.963]
Epoch [29/120    avg_loss:0.050, val_acc:0.970]
Epoch [30/120    avg_loss:0.047, val_acc:0.975]
Epoch [31/120    avg_loss:0.037, val_acc:0.969]
Epoch [32/120    avg_loss:0.034, val_acc:0.972]
Epoch [33/120    avg_loss:0.023, val_acc:0.976]
Epoch [34/120    avg_loss:0.026, val_acc:0.969]
Epoch [35/120    avg_loss:0.037, val_acc:0.936]
Epoch [36/120    avg_loss:0.030, val_acc:0.964]
Epoch [37/120    avg_loss:0.030, val_acc:0.965]
Epoch [38/120    avg_loss:0.142, val_acc:0.969]
Epoch [39/120    avg_loss:0.051, val_acc:0.964]
Epoch [40/120    avg_loss:0.034, val_acc:0.977]
Epoch [41/120    avg_loss:0.034, val_acc:0.969]
Epoch [42/120    avg_loss:0.033, val_acc:0.968]
Epoch [43/120    avg_loss:0.021, val_acc:0.974]
Epoch [44/120    avg_loss:0.026, val_acc:0.975]
Epoch [45/120    avg_loss:0.027, val_acc:0.975]
Epoch [46/120    avg_loss:0.015, val_acc:0.976]
Epoch [47/120    avg_loss:0.016, val_acc:0.975]
Epoch [48/120    avg_loss:0.012, val_acc:0.978]
Epoch [49/120    avg_loss:0.008, val_acc:0.979]
Epoch [50/120    avg_loss:0.009, val_acc:0.977]
Epoch [51/120    avg_loss:0.009, val_acc:0.979]
Epoch [52/120    avg_loss:0.013, val_acc:0.977]
Epoch [53/120    avg_loss:0.013, val_acc:0.977]
Epoch [54/120    avg_loss:0.015, val_acc:0.965]
Epoch [55/120    avg_loss:0.030, val_acc:0.973]
Epoch [56/120    avg_loss:0.051, val_acc:0.946]
Epoch [57/120    avg_loss:0.047, val_acc:0.934]
Epoch [58/120    avg_loss:0.052, val_acc:0.972]
Epoch [59/120    avg_loss:0.020, val_acc:0.974]
Epoch [60/120    avg_loss:0.017, val_acc:0.972]
Epoch [61/120    avg_loss:0.018, val_acc:0.980]
Epoch [62/120    avg_loss:0.013, val_acc:0.969]
Epoch [63/120    avg_loss:0.012, val_acc:0.979]
Epoch [64/120    avg_loss:0.013, val_acc:0.974]
Epoch [65/120    avg_loss:0.013, val_acc:0.985]
Epoch [66/120    avg_loss:0.023, val_acc:0.973]
Epoch [67/120    avg_loss:0.010, val_acc:0.980]
Epoch [68/120    avg_loss:0.007, val_acc:0.975]
Epoch [69/120    avg_loss:0.008, val_acc:0.977]
Epoch [70/120    avg_loss:0.009, val_acc:0.980]
Epoch [71/120    avg_loss:0.006, val_acc:0.979]
Epoch [72/120    avg_loss:0.005, val_acc:0.979]
Epoch [73/120    avg_loss:0.007, val_acc:0.982]
Epoch [74/120    avg_loss:0.006, val_acc:0.982]
Epoch [75/120    avg_loss:0.012, val_acc:0.976]
Epoch [76/120    avg_loss:0.009, val_acc:0.982]
Epoch [77/120    avg_loss:0.006, val_acc:0.980]
Epoch [78/120    avg_loss:0.005, val_acc:0.982]
Epoch [79/120    avg_loss:0.006, val_acc:0.983]
Epoch [80/120    avg_loss:0.004, val_acc:0.983]
Epoch [81/120    avg_loss:0.005, val_acc:0.981]
Epoch [82/120    avg_loss:0.007, val_acc:0.981]
Epoch [83/120    avg_loss:0.003, val_acc:0.984]
Epoch [84/120    avg_loss:0.003, val_acc:0.984]
Epoch [85/120    avg_loss:0.004, val_acc:0.984]
Epoch [86/120    avg_loss:0.003, val_acc:0.984]
Epoch [87/120    avg_loss:0.004, val_acc:0.985]
Epoch [88/120    avg_loss:0.005, val_acc:0.984]
Epoch [89/120    avg_loss:0.005, val_acc:0.983]
Epoch [90/120    avg_loss:0.003, val_acc:0.983]
Epoch [91/120    avg_loss:0.003, val_acc:0.983]
Epoch [92/120    avg_loss:0.003, val_acc:0.982]
Epoch [93/120    avg_loss:0.003, val_acc:0.982]
Epoch [94/120    avg_loss:0.004, val_acc:0.982]
Epoch [95/120    avg_loss:0.003, val_acc:0.983]
Epoch [96/120    avg_loss:0.006, val_acc:0.983]
Epoch [97/120    avg_loss:0.004, val_acc:0.983]
Epoch [98/120    avg_loss:0.002, val_acc:0.983]
Epoch [99/120    avg_loss:0.002, val_acc:0.983]
Epoch [100/120    avg_loss:0.003, val_acc:0.984]
Epoch [101/120    avg_loss:0.003, val_acc:0.984]
Epoch [102/120    avg_loss:0.004, val_acc:0.984]
Epoch [103/120    avg_loss:0.004, val_acc:0.984]
Epoch [104/120    avg_loss:0.006, val_acc:0.984]
Epoch [105/120    avg_loss:0.003, val_acc:0.984]
Epoch [106/120    avg_loss:0.004, val_acc:0.984]
Epoch [107/120    avg_loss:0.003, val_acc:0.984]
Epoch [108/120    avg_loss:0.004, val_acc:0.984]
Epoch [109/120    avg_loss:0.003, val_acc:0.984]
Epoch [110/120    avg_loss:0.003, val_acc:0.984]
Epoch [111/120    avg_loss:0.002, val_acc:0.984]
Epoch [112/120    avg_loss:0.002, val_acc:0.984]
Epoch [113/120    avg_loss:0.003, val_acc:0.984]
Epoch [114/120    avg_loss:0.004, val_acc:0.984]
Epoch [115/120    avg_loss:0.003, val_acc:0.984]
Epoch [116/120    avg_loss:0.003, val_acc:0.984]
Epoch [117/120    avg_loss:0.002, val_acc:0.984]
Epoch [118/120    avg_loss:0.002, val_acc:0.984]
Epoch [119/120    avg_loss:0.003, val_acc:0.984]
Epoch [120/120    avg_loss:0.003, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1271    1    0    0    1    0    0    0    6    3    3    0
     0    0    0]
 [   0    0    0  731    0    0    0    0    0    4    1    1    9    0
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    2    4    0    0    0  865    1    0    0
     0    3    0]
 [   0    0    1    0    0    1    2    0    0    0    3 2203    0    0
     0    0    0]
 [   0    0    0    1    0    2    0    0    0    0    6    0  523    0
     0    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
  1135    1    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    35  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.67750677506776

F1 scores:
[       nan 1.         0.99413375 0.98783784 1.         0.98853211
 0.9798357  1.         0.997669   0.81818182 0.98519362 0.99705816
 0.97574627 0.99728997 0.98268398 0.91499227 0.98224852]

Kappa:
0.984919420704225
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:01--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2e23bae6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.234, val_acc:0.490]
Epoch [2/120    avg_loss:1.716, val_acc:0.615]
Epoch [3/120    avg_loss:1.362, val_acc:0.707]
Epoch [4/120    avg_loss:1.123, val_acc:0.744]
Epoch [5/120    avg_loss:0.993, val_acc:0.798]
Epoch [6/120    avg_loss:0.702, val_acc:0.784]
Epoch [7/120    avg_loss:0.733, val_acc:0.806]
Epoch [8/120    avg_loss:0.657, val_acc:0.829]
Epoch [9/120    avg_loss:0.519, val_acc:0.807]
Epoch [10/120    avg_loss:0.455, val_acc:0.872]
Epoch [11/120    avg_loss:0.492, val_acc:0.871]
Epoch [12/120    avg_loss:0.290, val_acc:0.888]
Epoch [13/120    avg_loss:0.189, val_acc:0.897]
Epoch [14/120    avg_loss:0.270, val_acc:0.894]
Epoch [15/120    avg_loss:0.225, val_acc:0.889]
Epoch [16/120    avg_loss:0.283, val_acc:0.882]
Epoch [17/120    avg_loss:0.169, val_acc:0.926]
Epoch [18/120    avg_loss:0.129, val_acc:0.944]
Epoch [19/120    avg_loss:0.088, val_acc:0.950]
Epoch [20/120    avg_loss:0.087, val_acc:0.960]
Epoch [21/120    avg_loss:0.207, val_acc:0.915]
Epoch [22/120    avg_loss:0.183, val_acc:0.928]
Epoch [23/120    avg_loss:0.108, val_acc:0.959]
Epoch [24/120    avg_loss:0.074, val_acc:0.951]
Epoch [25/120    avg_loss:0.062, val_acc:0.959]
Epoch [26/120    avg_loss:0.057, val_acc:0.954]
Epoch [27/120    avg_loss:0.041, val_acc:0.968]
Epoch [28/120    avg_loss:0.101, val_acc:0.953]
Epoch [29/120    avg_loss:0.074, val_acc:0.965]
Epoch [30/120    avg_loss:0.060, val_acc:0.967]
Epoch [31/120    avg_loss:0.042, val_acc:0.959]
Epoch [32/120    avg_loss:0.046, val_acc:0.966]
Epoch [33/120    avg_loss:0.048, val_acc:0.970]
Epoch [34/120    avg_loss:0.033, val_acc:0.972]
Epoch [35/120    avg_loss:0.040, val_acc:0.971]
Epoch [36/120    avg_loss:0.034, val_acc:0.963]
Epoch [37/120    avg_loss:0.031, val_acc:0.960]
Epoch [38/120    avg_loss:0.037, val_acc:0.962]
Epoch [39/120    avg_loss:0.038, val_acc:0.969]
Epoch [40/120    avg_loss:0.057, val_acc:0.956]
Epoch [41/120    avg_loss:0.034, val_acc:0.969]
Epoch [42/120    avg_loss:0.030, val_acc:0.956]
Epoch [43/120    avg_loss:0.026, val_acc:0.969]
Epoch [44/120    avg_loss:0.039, val_acc:0.956]
Epoch [45/120    avg_loss:0.098, val_acc:0.941]
Epoch [46/120    avg_loss:0.122, val_acc:0.952]
Epoch [47/120    avg_loss:0.028, val_acc:0.967]
Epoch [48/120    avg_loss:0.022, val_acc:0.970]
Epoch [49/120    avg_loss:0.026, val_acc:0.970]
Epoch [50/120    avg_loss:0.014, val_acc:0.970]
Epoch [51/120    avg_loss:0.014, val_acc:0.970]
Epoch [52/120    avg_loss:0.017, val_acc:0.970]
Epoch [53/120    avg_loss:0.019, val_acc:0.969]
Epoch [54/120    avg_loss:0.013, val_acc:0.973]
Epoch [55/120    avg_loss:0.016, val_acc:0.975]
Epoch [56/120    avg_loss:0.013, val_acc:0.974]
Epoch [57/120    avg_loss:0.015, val_acc:0.972]
Epoch [58/120    avg_loss:0.012, val_acc:0.972]
Epoch [59/120    avg_loss:0.011, val_acc:0.974]
Epoch [60/120    avg_loss:0.017, val_acc:0.976]
Epoch [61/120    avg_loss:0.010, val_acc:0.976]
Epoch [62/120    avg_loss:0.012, val_acc:0.974]
Epoch [63/120    avg_loss:0.013, val_acc:0.973]
Epoch [64/120    avg_loss:0.009, val_acc:0.973]
Epoch [65/120    avg_loss:0.014, val_acc:0.971]
Epoch [66/120    avg_loss:0.014, val_acc:0.975]
Epoch [67/120    avg_loss:0.011, val_acc:0.976]
Epoch [68/120    avg_loss:0.014, val_acc:0.976]
Epoch [69/120    avg_loss:0.014, val_acc:0.975]
Epoch [70/120    avg_loss:0.014, val_acc:0.975]
Epoch [71/120    avg_loss:0.015, val_acc:0.971]
Epoch [72/120    avg_loss:0.012, val_acc:0.972]
Epoch [73/120    avg_loss:0.010, val_acc:0.975]
Epoch [74/120    avg_loss:0.009, val_acc:0.976]
Epoch [75/120    avg_loss:0.008, val_acc:0.976]
Epoch [76/120    avg_loss:0.016, val_acc:0.978]
Epoch [77/120    avg_loss:0.010, val_acc:0.979]
Epoch [78/120    avg_loss:0.009, val_acc:0.976]
Epoch [79/120    avg_loss:0.013, val_acc:0.976]
Epoch [80/120    avg_loss:0.013, val_acc:0.976]
Epoch [81/120    avg_loss:0.015, val_acc:0.972]
Epoch [82/120    avg_loss:0.009, val_acc:0.974]
Epoch [83/120    avg_loss:0.010, val_acc:0.974]
Epoch [84/120    avg_loss:0.009, val_acc:0.974]
Epoch [85/120    avg_loss:0.010, val_acc:0.978]
Epoch [86/120    avg_loss:0.012, val_acc:0.973]
Epoch [87/120    avg_loss:0.008, val_acc:0.975]
Epoch [88/120    avg_loss:0.007, val_acc:0.979]
Epoch [89/120    avg_loss:0.008, val_acc:0.979]
Epoch [90/120    avg_loss:0.009, val_acc:0.980]
Epoch [91/120    avg_loss:0.008, val_acc:0.980]
Epoch [92/120    avg_loss:0.010, val_acc:0.980]
Epoch [93/120    avg_loss:0.008, val_acc:0.978]
Epoch [94/120    avg_loss:0.008, val_acc:0.979]
Epoch [95/120    avg_loss:0.009, val_acc:0.979]
Epoch [96/120    avg_loss:0.010, val_acc:0.979]
Epoch [97/120    avg_loss:0.008, val_acc:0.981]
Epoch [98/120    avg_loss:0.011, val_acc:0.978]
Epoch [99/120    avg_loss:0.008, val_acc:0.979]
Epoch [100/120    avg_loss:0.007, val_acc:0.979]
Epoch [101/120    avg_loss:0.007, val_acc:0.978]
Epoch [102/120    avg_loss:0.010, val_acc:0.981]
Epoch [103/120    avg_loss:0.010, val_acc:0.980]
Epoch [104/120    avg_loss:0.007, val_acc:0.982]
Epoch [105/120    avg_loss:0.007, val_acc:0.981]
Epoch [106/120    avg_loss:0.007, val_acc:0.981]
Epoch [107/120    avg_loss:0.007, val_acc:0.980]
Epoch [108/120    avg_loss:0.008, val_acc:0.980]
Epoch [109/120    avg_loss:0.006, val_acc:0.981]
Epoch [110/120    avg_loss:0.008, val_acc:0.979]
Epoch [111/120    avg_loss:0.011, val_acc:0.980]
Epoch [112/120    avg_loss:0.008, val_acc:0.979]
Epoch [113/120    avg_loss:0.006, val_acc:0.982]
Epoch [114/120    avg_loss:0.017, val_acc:0.978]
Epoch [115/120    avg_loss:0.009, val_acc:0.979]
Epoch [116/120    avg_loss:0.008, val_acc:0.980]
Epoch [117/120    avg_loss:0.009, val_acc:0.980]
Epoch [118/120    avg_loss:0.008, val_acc:0.981]
Epoch [119/120    avg_loss:0.006, val_acc:0.980]
Epoch [120/120    avg_loss:0.006, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    3    0    0    2    0    0    0    7    6    0    0
     0    0    0]
 [   0    0    0  709    0   11    0    0    0    4    0    0   19    4
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    2    0    0    0    0  854   13    0    0
     0    0    0]
 [   0    0    9    0    0    0    2    0    0    1    0 2192    4    2
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0  531    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1129    9    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    20  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
98.39566395663957

F1 scores:
[       nan 1.         0.98714453 0.9705681  0.99528302 0.98070375
 0.98944193 1.         1.         0.81818182 0.98330455 0.99163085
 0.97163769 0.98404255 0.98645697 0.94222222 0.96341463]

Kappa:
0.9817103924435642
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:03--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff9323816a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.294, val_acc:0.470]
Epoch [2/120    avg_loss:1.573, val_acc:0.569]
Epoch [3/120    avg_loss:1.334, val_acc:0.700]
Epoch [4/120    avg_loss:1.110, val_acc:0.702]
Epoch [5/120    avg_loss:0.942, val_acc:0.708]
Epoch [6/120    avg_loss:0.736, val_acc:0.809]
Epoch [7/120    avg_loss:0.626, val_acc:0.777]
Epoch [8/120    avg_loss:0.757, val_acc:0.816]
Epoch [9/120    avg_loss:0.682, val_acc:0.802]
Epoch [10/120    avg_loss:0.460, val_acc:0.849]
Epoch [11/120    avg_loss:0.284, val_acc:0.879]
Epoch [12/120    avg_loss:0.193, val_acc:0.919]
Epoch [13/120    avg_loss:0.257, val_acc:0.902]
Epoch [14/120    avg_loss:0.164, val_acc:0.938]
Epoch [15/120    avg_loss:0.196, val_acc:0.924]
Epoch [16/120    avg_loss:0.146, val_acc:0.874]
Epoch [17/120    avg_loss:0.172, val_acc:0.921]
Epoch [18/120    avg_loss:0.115, val_acc:0.952]
Epoch [19/120    avg_loss:0.093, val_acc:0.948]
Epoch [20/120    avg_loss:0.078, val_acc:0.971]
Epoch [21/120    avg_loss:0.076, val_acc:0.962]
Epoch [22/120    avg_loss:0.129, val_acc:0.899]
Epoch [23/120    avg_loss:0.146, val_acc:0.947]
Epoch [24/120    avg_loss:0.101, val_acc:0.944]
Epoch [25/120    avg_loss:0.101, val_acc:0.973]
Epoch [26/120    avg_loss:0.070, val_acc:0.954]
Epoch [27/120    avg_loss:0.097, val_acc:0.969]
Epoch [28/120    avg_loss:0.073, val_acc:0.975]
Epoch [29/120    avg_loss:0.055, val_acc:0.981]
Epoch [30/120    avg_loss:0.053, val_acc:0.964]
Epoch [31/120    avg_loss:0.054, val_acc:0.966]
Epoch [32/120    avg_loss:0.062, val_acc:0.960]
Epoch [33/120    avg_loss:0.041, val_acc:0.974]
Epoch [34/120    avg_loss:0.055, val_acc:0.926]
Epoch [35/120    avg_loss:0.043, val_acc:0.979]
Epoch [36/120    avg_loss:0.064, val_acc:0.950]
Epoch [37/120    avg_loss:0.078, val_acc:0.954]
Epoch [38/120    avg_loss:0.037, val_acc:0.974]
Epoch [39/120    avg_loss:0.037, val_acc:0.981]
Epoch [40/120    avg_loss:0.016, val_acc:0.984]
Epoch [41/120    avg_loss:0.030, val_acc:0.980]
Epoch [42/120    avg_loss:0.025, val_acc:0.979]
Epoch [43/120    avg_loss:0.047, val_acc:0.978]
Epoch [44/120    avg_loss:0.045, val_acc:0.958]
Epoch [45/120    avg_loss:0.042, val_acc:0.969]
Epoch [46/120    avg_loss:0.027, val_acc:0.979]
Epoch [47/120    avg_loss:0.034, val_acc:0.967]
Epoch [48/120    avg_loss:0.030, val_acc:0.981]
Epoch [49/120    avg_loss:0.022, val_acc:0.982]
Epoch [50/120    avg_loss:0.019, val_acc:0.984]
Epoch [51/120    avg_loss:0.013, val_acc:0.979]
Epoch [52/120    avg_loss:0.016, val_acc:0.982]
Epoch [53/120    avg_loss:0.028, val_acc:0.981]
Epoch [54/120    avg_loss:0.020, val_acc:0.984]
Epoch [55/120    avg_loss:0.042, val_acc:0.983]
Epoch [56/120    avg_loss:0.020, val_acc:0.978]
Epoch [57/120    avg_loss:0.017, val_acc:0.989]
Epoch [58/120    avg_loss:0.011, val_acc:0.984]
Epoch [59/120    avg_loss:0.030, val_acc:0.926]
Epoch [60/120    avg_loss:0.048, val_acc:0.975]
Epoch [61/120    avg_loss:0.031, val_acc:0.979]
Epoch [62/120    avg_loss:0.017, val_acc:0.986]
Epoch [63/120    avg_loss:0.011, val_acc:0.990]
Epoch [64/120    avg_loss:0.008, val_acc:0.991]
Epoch [65/120    avg_loss:0.018, val_acc:0.982]
Epoch [66/120    avg_loss:0.018, val_acc:0.972]
Epoch [67/120    avg_loss:0.040, val_acc:0.979]
Epoch [68/120    avg_loss:0.041, val_acc:0.972]
Epoch [69/120    avg_loss:0.014, val_acc:0.989]
Epoch [70/120    avg_loss:0.010, val_acc:0.984]
Epoch [71/120    avg_loss:0.020, val_acc:0.989]
Epoch [72/120    avg_loss:0.008, val_acc:0.990]
Epoch [73/120    avg_loss:0.006, val_acc:0.983]
Epoch [74/120    avg_loss:0.010, val_acc:0.984]
Epoch [75/120    avg_loss:0.008, val_acc:0.985]
Epoch [76/120    avg_loss:0.010, val_acc:0.989]
Epoch [77/120    avg_loss:0.015, val_acc:0.990]
Epoch [78/120    avg_loss:0.007, val_acc:0.990]
Epoch [79/120    avg_loss:0.007, val_acc:0.991]
Epoch [80/120    avg_loss:0.005, val_acc:0.990]
Epoch [81/120    avg_loss:0.006, val_acc:0.990]
Epoch [82/120    avg_loss:0.006, val_acc:0.990]
Epoch [83/120    avg_loss:0.006, val_acc:0.991]
Epoch [84/120    avg_loss:0.007, val_acc:0.991]
Epoch [85/120    avg_loss:0.006, val_acc:0.991]
Epoch [86/120    avg_loss:0.005, val_acc:0.994]
Epoch [87/120    avg_loss:0.006, val_acc:0.993]
Epoch [88/120    avg_loss:0.004, val_acc:0.995]
Epoch [89/120    avg_loss:0.004, val_acc:0.996]
Epoch [90/120    avg_loss:0.005, val_acc:0.996]
Epoch [91/120    avg_loss:0.004, val_acc:0.994]
Epoch [92/120    avg_loss:0.005, val_acc:0.995]
Epoch [93/120    avg_loss:0.006, val_acc:0.995]
Epoch [94/120    avg_loss:0.004, val_acc:0.995]
Epoch [95/120    avg_loss:0.004, val_acc:0.995]
Epoch [96/120    avg_loss:0.005, val_acc:0.994]
Epoch [97/120    avg_loss:0.005, val_acc:0.993]
Epoch [98/120    avg_loss:0.004, val_acc:0.993]
Epoch [99/120    avg_loss:0.004, val_acc:0.994]
Epoch [100/120    avg_loss:0.003, val_acc:0.994]
Epoch [101/120    avg_loss:0.007, val_acc:0.991]
Epoch [102/120    avg_loss:0.005, val_acc:0.994]
Epoch [103/120    avg_loss:0.006, val_acc:0.994]
Epoch [104/120    avg_loss:0.003, val_acc:0.994]
Epoch [105/120    avg_loss:0.005, val_acc:0.994]
Epoch [106/120    avg_loss:0.008, val_acc:0.994]
Epoch [107/120    avg_loss:0.002, val_acc:0.994]
Epoch [108/120    avg_loss:0.004, val_acc:0.994]
Epoch [109/120    avg_loss:0.005, val_acc:0.994]
Epoch [110/120    avg_loss:0.004, val_acc:0.994]
Epoch [111/120    avg_loss:0.004, val_acc:0.994]
Epoch [112/120    avg_loss:0.003, val_acc:0.995]
Epoch [113/120    avg_loss:0.004, val_acc:0.994]
Epoch [114/120    avg_loss:0.004, val_acc:0.994]
Epoch [115/120    avg_loss:0.005, val_acc:0.994]
Epoch [116/120    avg_loss:0.003, val_acc:0.995]
Epoch [117/120    avg_loss:0.005, val_acc:0.995]
Epoch [118/120    avg_loss:0.005, val_acc:0.994]
Epoch [119/120    avg_loss:0.004, val_acc:0.995]
Epoch [120/120    avg_loss:0.004, val_acc:0.994]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1271    3    1    0    0    0    0    0    3    4    0    1
     0    2    0]
 [   0    0    0  718    0    7    0    0    0    6    1    0   13    2
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    1    0    0    3    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  864    8    0    0
     0    3    0]
 [   0    0   10    0    0    0    1    0    0    0    8 2185    5    1
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0  531    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    1    0    0    0
  1130    6    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    16  320    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.57994579945799

F1 scores:
[       nan 0.975      0.99026101 0.97753574 0.99530516 0.98285714
 0.99020347 1.         0.997669   0.8        0.98573873 0.99160427
 0.97610294 0.98930481 0.98862642 0.9439528  0.97005988]

Kappa:
0.9838152538836527
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1f9a3dd630>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.242, val_acc:0.444]
Epoch [2/120    avg_loss:1.774, val_acc:0.553]
Epoch [3/120    avg_loss:1.237, val_acc:0.701]
Epoch [4/120    avg_loss:0.944, val_acc:0.761]
Epoch [5/120    avg_loss:0.698, val_acc:0.720]
Epoch [6/120    avg_loss:0.738, val_acc:0.807]
Epoch [7/120    avg_loss:0.678, val_acc:0.809]
Epoch [8/120    avg_loss:0.637, val_acc:0.842]
Epoch [9/120    avg_loss:0.513, val_acc:0.827]
Epoch [10/120    avg_loss:0.486, val_acc:0.879]
Epoch [11/120    avg_loss:0.318, val_acc:0.884]
Epoch [12/120    avg_loss:0.214, val_acc:0.927]
Epoch [13/120    avg_loss:0.426, val_acc:0.808]
Epoch [14/120    avg_loss:0.336, val_acc:0.908]
Epoch [15/120    avg_loss:0.204, val_acc:0.939]
Epoch [16/120    avg_loss:0.148, val_acc:0.949]
Epoch [17/120    avg_loss:0.163, val_acc:0.924]
Epoch [18/120    avg_loss:0.204, val_acc:0.927]
Epoch [19/120    avg_loss:0.169, val_acc:0.942]
Epoch [20/120    avg_loss:0.104, val_acc:0.957]
Epoch [21/120    avg_loss:0.088, val_acc:0.959]
Epoch [22/120    avg_loss:0.086, val_acc:0.966]
Epoch [23/120    avg_loss:0.090, val_acc:0.948]
Epoch [24/120    avg_loss:0.098, val_acc:0.950]
Epoch [25/120    avg_loss:0.054, val_acc:0.968]
Epoch [26/120    avg_loss:0.070, val_acc:0.969]
Epoch [27/120    avg_loss:0.055, val_acc:0.966]
Epoch [28/120    avg_loss:0.057, val_acc:0.970]
Epoch [29/120    avg_loss:0.073, val_acc:0.960]
Epoch [30/120    avg_loss:0.059, val_acc:0.972]
Epoch [31/120    avg_loss:0.037, val_acc:0.972]
Epoch [32/120    avg_loss:0.038, val_acc:0.976]
Epoch [33/120    avg_loss:0.050, val_acc:0.969]
Epoch [34/120    avg_loss:0.047, val_acc:0.973]
Epoch [35/120    avg_loss:0.037, val_acc:0.972]
Epoch [36/120    avg_loss:0.027, val_acc:0.976]
Epoch [37/120    avg_loss:0.053, val_acc:0.974]
Epoch [38/120    avg_loss:0.044, val_acc:0.977]
Epoch [39/120    avg_loss:0.038, val_acc:0.981]
Epoch [40/120    avg_loss:0.044, val_acc:0.971]
Epoch [41/120    avg_loss:0.043, val_acc:0.972]
Epoch [42/120    avg_loss:0.064, val_acc:0.968]
Epoch [43/120    avg_loss:0.043, val_acc:0.975]
Epoch [44/120    avg_loss:0.031, val_acc:0.979]
Epoch [45/120    avg_loss:0.037, val_acc:0.982]
Epoch [46/120    avg_loss:0.019, val_acc:0.983]
Epoch [47/120    avg_loss:0.028, val_acc:0.976]
Epoch [48/120    avg_loss:0.044, val_acc:0.973]
Epoch [49/120    avg_loss:0.036, val_acc:0.976]
Epoch [50/120    avg_loss:0.018, val_acc:0.977]
Epoch [51/120    avg_loss:0.016, val_acc:0.976]
Epoch [52/120    avg_loss:0.023, val_acc:0.983]
Epoch [53/120    avg_loss:0.014, val_acc:0.981]
Epoch [54/120    avg_loss:0.013, val_acc:0.984]
Epoch [55/120    avg_loss:0.014, val_acc:0.983]
Epoch [56/120    avg_loss:0.010, val_acc:0.983]
Epoch [57/120    avg_loss:0.008, val_acc:0.984]
Epoch [58/120    avg_loss:0.012, val_acc:0.980]
Epoch [59/120    avg_loss:0.009, val_acc:0.984]
Epoch [60/120    avg_loss:0.014, val_acc:0.979]
Epoch [61/120    avg_loss:0.031, val_acc:0.979]
Epoch [62/120    avg_loss:0.019, val_acc:0.983]
Epoch [63/120    avg_loss:0.020, val_acc:0.984]
Epoch [64/120    avg_loss:0.010, val_acc:0.981]
Epoch [65/120    avg_loss:0.014, val_acc:0.984]
Epoch [66/120    avg_loss:0.014, val_acc:0.986]
Epoch [67/120    avg_loss:0.009, val_acc:0.989]
Epoch [68/120    avg_loss:0.008, val_acc:0.986]
Epoch [69/120    avg_loss:0.030, val_acc:0.982]
Epoch [70/120    avg_loss:0.012, val_acc:0.988]
Epoch [71/120    avg_loss:0.008, val_acc:0.990]
Epoch [72/120    avg_loss:0.013, val_acc:0.986]
Epoch [73/120    avg_loss:0.006, val_acc:0.986]
Epoch [74/120    avg_loss:0.009, val_acc:0.991]
Epoch [75/120    avg_loss:0.008, val_acc:0.980]
Epoch [76/120    avg_loss:0.009, val_acc:0.989]
Epoch [77/120    avg_loss:0.008, val_acc:0.986]
Epoch [78/120    avg_loss:0.007, val_acc:0.984]
Epoch [79/120    avg_loss:0.005, val_acc:0.989]
Epoch [80/120    avg_loss:0.005, val_acc:0.988]
Epoch [81/120    avg_loss:0.010, val_acc:0.984]
Epoch [82/120    avg_loss:0.014, val_acc:0.983]
Epoch [83/120    avg_loss:0.006, val_acc:0.982]
Epoch [84/120    avg_loss:0.005, val_acc:0.985]
Epoch [85/120    avg_loss:0.010, val_acc:0.984]
Epoch [86/120    avg_loss:0.011, val_acc:0.979]
Epoch [87/120    avg_loss:0.007, val_acc:0.985]
Epoch [88/120    avg_loss:0.005, val_acc:0.988]
Epoch [89/120    avg_loss:0.005, val_acc:0.986]
Epoch [90/120    avg_loss:0.003, val_acc:0.988]
Epoch [91/120    avg_loss:0.003, val_acc:0.986]
Epoch [92/120    avg_loss:0.004, val_acc:0.989]
Epoch [93/120    avg_loss:0.003, val_acc:0.989]
Epoch [94/120    avg_loss:0.006, val_acc:0.989]
Epoch [95/120    avg_loss:0.003, val_acc:0.989]
Epoch [96/120    avg_loss:0.007, val_acc:0.988]
Epoch [97/120    avg_loss:0.004, val_acc:0.989]
Epoch [98/120    avg_loss:0.005, val_acc:0.989]
Epoch [99/120    avg_loss:0.004, val_acc:0.988]
Epoch [100/120    avg_loss:0.003, val_acc:0.988]
Epoch [101/120    avg_loss:0.003, val_acc:0.988]
Epoch [102/120    avg_loss:0.003, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.988]
Epoch [105/120    avg_loss:0.004, val_acc:0.988]
Epoch [106/120    avg_loss:0.005, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.003, val_acc:0.989]
Epoch [110/120    avg_loss:0.003, val_acc:0.989]
Epoch [111/120    avg_loss:0.003, val_acc:0.989]
Epoch [112/120    avg_loss:0.003, val_acc:0.989]
Epoch [113/120    avg_loss:0.003, val_acc:0.989]
Epoch [114/120    avg_loss:0.003, val_acc:0.989]
Epoch [115/120    avg_loss:0.003, val_acc:0.989]
Epoch [116/120    avg_loss:0.003, val_acc:0.989]
Epoch [117/120    avg_loss:0.003, val_acc:0.989]
Epoch [118/120    avg_loss:0.006, val_acc:0.989]
Epoch [119/120    avg_loss:0.003, val_acc:0.989]
Epoch [120/120    avg_loss:0.003, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    2    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1265    0   10    0    4    0    0    0    4    2    0    0
     0    0    0]
 [   0    0    0  723    0   11    0    0    0    5    0    0    7    1
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    2    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    2    3    0    0    0  853   17    0    0
     0    0    0]
 [   0    0    4    0    0    0    4    0    0    0    0 2198    3    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    5    0  528    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    2    0    0    0    0    0    0    0
  1131    6    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    22  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.49322493224932

F1 scores:
[       nan 0.975      0.98982786 0.98233696 0.97235023 0.97949886
 0.98206278 1.         0.997669   0.8372093  0.98215314 0.99299752
 0.98050139 0.99462366 0.98562092 0.94152924 0.97590361]

Kappa:
0.9828202893136834
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:07--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:40
Validation dataloader:40
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb60cb14668>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.248, val_acc:0.531]
Epoch [2/120    avg_loss:1.721, val_acc:0.624]
Epoch [3/120    avg_loss:1.312, val_acc:0.646]
Epoch [4/120    avg_loss:1.203, val_acc:0.654]
Epoch [5/120    avg_loss:0.829, val_acc:0.749]
Epoch [6/120    avg_loss:0.888, val_acc:0.647]
Epoch [7/120    avg_loss:0.801, val_acc:0.678]
Epoch [8/120    avg_loss:0.550, val_acc:0.832]
Epoch [9/120    avg_loss:0.323, val_acc:0.810]
Epoch [10/120    avg_loss:0.349, val_acc:0.854]
Epoch [11/120    avg_loss:0.340, val_acc:0.887]
Epoch [12/120    avg_loss:0.236, val_acc:0.910]
Epoch [13/120    avg_loss:0.231, val_acc:0.909]
Epoch [14/120    avg_loss:0.211, val_acc:0.921]
Epoch [15/120    avg_loss:0.174, val_acc:0.918]
Epoch [16/120    avg_loss:0.119, val_acc:0.940]
Epoch [17/120    avg_loss:0.097, val_acc:0.946]
Epoch [18/120    avg_loss:0.101, val_acc:0.946]
Epoch [19/120    avg_loss:0.121, val_acc:0.933]
Epoch [20/120    avg_loss:0.121, val_acc:0.924]
Epoch [21/120    avg_loss:0.114, val_acc:0.927]
Epoch [22/120    avg_loss:0.074, val_acc:0.955]
Epoch [23/120    avg_loss:0.075, val_acc:0.940]
Epoch [24/120    avg_loss:0.050, val_acc:0.966]
Epoch [25/120    avg_loss:0.049, val_acc:0.958]
Epoch [26/120    avg_loss:0.041, val_acc:0.954]
Epoch [27/120    avg_loss:0.038, val_acc:0.963]
Epoch [28/120    avg_loss:0.046, val_acc:0.953]
Epoch [29/120    avg_loss:0.056, val_acc:0.926]
Epoch [30/120    avg_loss:0.073, val_acc:0.940]
Epoch [31/120    avg_loss:0.045, val_acc:0.960]
Epoch [32/120    avg_loss:0.045, val_acc:0.950]
Epoch [33/120    avg_loss:0.021, val_acc:0.967]
Epoch [34/120    avg_loss:0.028, val_acc:0.968]
Epoch [35/120    avg_loss:0.027, val_acc:0.970]
Epoch [36/120    avg_loss:0.032, val_acc:0.952]
Epoch [37/120    avg_loss:0.027, val_acc:0.976]
Epoch [38/120    avg_loss:0.019, val_acc:0.960]
Epoch [39/120    avg_loss:0.021, val_acc:0.966]
Epoch [40/120    avg_loss:0.019, val_acc:0.976]
Epoch [41/120    avg_loss:0.019, val_acc:0.965]
Epoch [42/120    avg_loss:0.034, val_acc:0.960]
Epoch [43/120    avg_loss:0.020, val_acc:0.946]
Epoch [44/120    avg_loss:0.024, val_acc:0.970]
Epoch [45/120    avg_loss:0.016, val_acc:0.969]
Epoch [46/120    avg_loss:0.022, val_acc:0.978]
Epoch [47/120    avg_loss:0.011, val_acc:0.979]
Epoch [48/120    avg_loss:0.014, val_acc:0.980]
Epoch [49/120    avg_loss:0.024, val_acc:0.965]
Epoch [50/120    avg_loss:0.069, val_acc:0.965]
Epoch [51/120    avg_loss:0.041, val_acc:0.967]
Epoch [52/120    avg_loss:0.019, val_acc:0.976]
Epoch [53/120    avg_loss:0.013, val_acc:0.982]
Epoch [54/120    avg_loss:0.013, val_acc:0.978]
Epoch [55/120    avg_loss:0.021, val_acc:0.975]
Epoch [56/120    avg_loss:0.014, val_acc:0.972]
Epoch [57/120    avg_loss:0.014, val_acc:0.974]
Epoch [58/120    avg_loss:0.010, val_acc:0.983]
Epoch [59/120    avg_loss:0.007, val_acc:0.987]
Epoch [60/120    avg_loss:0.008, val_acc:0.980]
Epoch [61/120    avg_loss:0.008, val_acc:0.978]
Epoch [62/120    avg_loss:0.008, val_acc:0.978]
Epoch [63/120    avg_loss:0.007, val_acc:0.984]
Epoch [64/120    avg_loss:0.008, val_acc:0.985]
Epoch [65/120    avg_loss:0.025, val_acc:0.972]
Epoch [66/120    avg_loss:0.014, val_acc:0.972]
Epoch [67/120    avg_loss:0.022, val_acc:0.973]
Epoch [68/120    avg_loss:0.017, val_acc:0.980]
Epoch [69/120    avg_loss:0.010, val_acc:0.974]
Epoch [70/120    avg_loss:0.005, val_acc:0.985]
Epoch [71/120    avg_loss:0.007, val_acc:0.973]
Epoch [72/120    avg_loss:0.014, val_acc:0.982]
Epoch [73/120    avg_loss:0.005, val_acc:0.983]
Epoch [74/120    avg_loss:0.007, val_acc:0.986]
Epoch [75/120    avg_loss:0.008, val_acc:0.987]
Epoch [76/120    avg_loss:0.006, val_acc:0.988]
Epoch [77/120    avg_loss:0.004, val_acc:0.988]
Epoch [78/120    avg_loss:0.004, val_acc:0.988]
Epoch [79/120    avg_loss:0.009, val_acc:0.988]
Epoch [80/120    avg_loss:0.004, val_acc:0.988]
Epoch [81/120    avg_loss:0.006, val_acc:0.988]
Epoch [82/120    avg_loss:0.004, val_acc:0.989]
Epoch [83/120    avg_loss:0.005, val_acc:0.990]
Epoch [84/120    avg_loss:0.003, val_acc:0.989]
Epoch [85/120    avg_loss:0.004, val_acc:0.989]
Epoch [86/120    avg_loss:0.003, val_acc:0.989]
Epoch [87/120    avg_loss:0.004, val_acc:0.989]
Epoch [88/120    avg_loss:0.004, val_acc:0.989]
Epoch [89/120    avg_loss:0.004, val_acc:0.988]
Epoch [90/120    avg_loss:0.004, val_acc:0.989]
Epoch [91/120    avg_loss:0.005, val_acc:0.989]
Epoch [92/120    avg_loss:0.004, val_acc:0.987]
Epoch [93/120    avg_loss:0.006, val_acc:0.986]
Epoch [94/120    avg_loss:0.004, val_acc:0.986]
Epoch [95/120    avg_loss:0.007, val_acc:0.987]
Epoch [96/120    avg_loss:0.002, val_acc:0.987]
Epoch [97/120    avg_loss:0.005, val_acc:0.987]
Epoch [98/120    avg_loss:0.003, val_acc:0.987]
Epoch [99/120    avg_loss:0.003, val_acc:0.987]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.005, val_acc:0.988]
Epoch [102/120    avg_loss:0.002, val_acc:0.988]
Epoch [103/120    avg_loss:0.004, val_acc:0.988]
Epoch [104/120    avg_loss:0.003, val_acc:0.988]
Epoch [105/120    avg_loss:0.008, val_acc:0.988]
Epoch [106/120    avg_loss:0.004, val_acc:0.988]
Epoch [107/120    avg_loss:0.004, val_acc:0.988]
Epoch [108/120    avg_loss:0.005, val_acc:0.988]
Epoch [109/120    avg_loss:0.004, val_acc:0.988]
Epoch [110/120    avg_loss:0.006, val_acc:0.988]
Epoch [111/120    avg_loss:0.004, val_acc:0.988]
Epoch [112/120    avg_loss:0.004, val_acc:0.988]
Epoch [113/120    avg_loss:0.003, val_acc:0.988]
Epoch [114/120    avg_loss:0.005, val_acc:0.988]
Epoch [115/120    avg_loss:0.003, val_acc:0.988]
Epoch [116/120    avg_loss:0.004, val_acc:0.988]
Epoch [117/120    avg_loss:0.004, val_acc:0.988]
Epoch [118/120    avg_loss:0.003, val_acc:0.988]
Epoch [119/120    avg_loss:0.003, val_acc:0.988]
Epoch [120/120    avg_loss:0.003, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1273    1    0    0    0    0    0    0    3    8    0    0
     0    0    0]
 [   0    0    0  721    0   14    0    0    0    4    1    0    5    1
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    3    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    2    1    0    0    0  857    8    0    0
     0    4    0]
 [   0    0    1    0    0    0    0    0    0    0    0 2209    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    5    0  526    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    1    0    0    0    0    0    0    0
  1134    1    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    36  297    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.60162601626016

F1 scores:
[       nan 0.98765432 0.99336715 0.98162015 1.         0.97168743
 0.98720843 0.94339623 1.         0.85714286 0.98449167 0.99594229
 0.98686679 0.99730458 0.98139334 0.91525424 0.97647059]

Kappa:
0.9840494193829152
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:09--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4d132a16a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.402, val_acc:0.545]
Epoch [2/120    avg_loss:1.544, val_acc:0.571]
Epoch [3/120    avg_loss:1.611, val_acc:0.671]
Epoch [4/120    avg_loss:0.963, val_acc:0.724]
Epoch [5/120    avg_loss:0.827, val_acc:0.737]
Epoch [6/120    avg_loss:0.760, val_acc:0.828]
Epoch [7/120    avg_loss:0.583, val_acc:0.821]
Epoch [8/120    avg_loss:0.592, val_acc:0.838]
Epoch [9/120    avg_loss:0.454, val_acc:0.866]
Epoch [10/120    avg_loss:0.438, val_acc:0.857]
Epoch [11/120    avg_loss:0.453, val_acc:0.845]
Epoch [12/120    avg_loss:0.336, val_acc:0.861]
Epoch [13/120    avg_loss:0.268, val_acc:0.901]
Epoch [14/120    avg_loss:0.231, val_acc:0.879]
Epoch [15/120    avg_loss:0.210, val_acc:0.916]
Epoch [16/120    avg_loss:0.186, val_acc:0.917]
Epoch [17/120    avg_loss:0.149, val_acc:0.935]
Epoch [18/120    avg_loss:0.096, val_acc:0.943]
Epoch [19/120    avg_loss:0.074, val_acc:0.939]
Epoch [20/120    avg_loss:0.097, val_acc:0.946]
Epoch [21/120    avg_loss:0.108, val_acc:0.938]
Epoch [22/120    avg_loss:0.083, val_acc:0.934]
Epoch [23/120    avg_loss:0.081, val_acc:0.953]
Epoch [24/120    avg_loss:0.046, val_acc:0.956]
Epoch [25/120    avg_loss:0.084, val_acc:0.943]
Epoch [26/120    avg_loss:0.057, val_acc:0.957]
Epoch [27/120    avg_loss:0.061, val_acc:0.961]
Epoch [28/120    avg_loss:0.051, val_acc:0.967]
Epoch [29/120    avg_loss:0.038, val_acc:0.967]
Epoch [30/120    avg_loss:0.042, val_acc:0.967]
Epoch [31/120    avg_loss:0.064, val_acc:0.958]
Epoch [32/120    avg_loss:0.068, val_acc:0.962]
Epoch [33/120    avg_loss:0.064, val_acc:0.958]
Epoch [34/120    avg_loss:0.040, val_acc:0.959]
Epoch [35/120    avg_loss:0.041, val_acc:0.965]
Epoch [36/120    avg_loss:0.037, val_acc:0.956]
Epoch [37/120    avg_loss:0.040, val_acc:0.958]
Epoch [38/120    avg_loss:0.042, val_acc:0.970]
Epoch [39/120    avg_loss:0.018, val_acc:0.984]
Epoch [40/120    avg_loss:0.015, val_acc:0.984]
Epoch [41/120    avg_loss:0.015, val_acc:0.980]
Epoch [42/120    avg_loss:0.019, val_acc:0.978]
Epoch [43/120    avg_loss:0.019, val_acc:0.977]
Epoch [44/120    avg_loss:0.022, val_acc:0.964]
Epoch [45/120    avg_loss:0.022, val_acc:0.974]
Epoch [46/120    avg_loss:0.013, val_acc:0.981]
Epoch [47/120    avg_loss:0.013, val_acc:0.982]
Epoch [48/120    avg_loss:0.013, val_acc:0.984]
Epoch [49/120    avg_loss:0.015, val_acc:0.980]
Epoch [50/120    avg_loss:0.013, val_acc:0.970]
Epoch [51/120    avg_loss:0.014, val_acc:0.977]
Epoch [52/120    avg_loss:0.014, val_acc:0.955]
Epoch [53/120    avg_loss:0.018, val_acc:0.977]
Epoch [54/120    avg_loss:0.013, val_acc:0.978]
Epoch [55/120    avg_loss:0.013, val_acc:0.977]
Epoch [56/120    avg_loss:0.049, val_acc:0.968]
Epoch [57/120    avg_loss:0.134, val_acc:0.888]
Epoch [58/120    avg_loss:0.354, val_acc:0.934]
Epoch [59/120    avg_loss:0.159, val_acc:0.932]
Epoch [60/120    avg_loss:0.159, val_acc:0.959]
Epoch [61/120    avg_loss:0.125, val_acc:0.967]
Epoch [62/120    avg_loss:0.050, val_acc:0.976]
Epoch [63/120    avg_loss:0.037, val_acc:0.979]
Epoch [64/120    avg_loss:0.032, val_acc:0.977]
Epoch [65/120    avg_loss:0.033, val_acc:0.978]
Epoch [66/120    avg_loss:0.030, val_acc:0.975]
Epoch [67/120    avg_loss:0.025, val_acc:0.976]
Epoch [68/120    avg_loss:0.019, val_acc:0.977]
Epoch [69/120    avg_loss:0.022, val_acc:0.978]
Epoch [70/120    avg_loss:0.029, val_acc:0.980]
Epoch [71/120    avg_loss:0.020, val_acc:0.977]
Epoch [72/120    avg_loss:0.021, val_acc:0.978]
Epoch [73/120    avg_loss:0.021, val_acc:0.978]
Epoch [74/120    avg_loss:0.029, val_acc:0.979]
Epoch [75/120    avg_loss:0.017, val_acc:0.979]
Epoch [76/120    avg_loss:0.021, val_acc:0.979]
Epoch [77/120    avg_loss:0.025, val_acc:0.979]
Epoch [78/120    avg_loss:0.020, val_acc:0.979]
Epoch [79/120    avg_loss:0.028, val_acc:0.979]
Epoch [80/120    avg_loss:0.017, val_acc:0.979]
Epoch [81/120    avg_loss:0.024, val_acc:0.979]
Epoch [82/120    avg_loss:0.020, val_acc:0.979]
Epoch [83/120    avg_loss:0.025, val_acc:0.979]
Epoch [84/120    avg_loss:0.016, val_acc:0.979]
Epoch [85/120    avg_loss:0.022, val_acc:0.979]
Epoch [86/120    avg_loss:0.022, val_acc:0.979]
Epoch [87/120    avg_loss:0.020, val_acc:0.979]
Epoch [88/120    avg_loss:0.019, val_acc:0.979]
Epoch [89/120    avg_loss:0.017, val_acc:0.979]
Epoch [90/120    avg_loss:0.019, val_acc:0.979]
Epoch [91/120    avg_loss:0.014, val_acc:0.979]
Epoch [92/120    avg_loss:0.025, val_acc:0.979]
Epoch [93/120    avg_loss:0.015, val_acc:0.979]
Epoch [94/120    avg_loss:0.018, val_acc:0.979]
Epoch [95/120    avg_loss:0.019, val_acc:0.979]
Epoch [96/120    avg_loss:0.015, val_acc:0.979]
Epoch [97/120    avg_loss:0.025, val_acc:0.979]
Epoch [98/120    avg_loss:0.017, val_acc:0.979]
Epoch [99/120    avg_loss:0.023, val_acc:0.979]
Epoch [100/120    avg_loss:0.021, val_acc:0.979]
Epoch [101/120    avg_loss:0.017, val_acc:0.979]
Epoch [102/120    avg_loss:0.014, val_acc:0.979]
Epoch [103/120    avg_loss:0.022, val_acc:0.979]
Epoch [104/120    avg_loss:0.022, val_acc:0.979]
Epoch [105/120    avg_loss:0.026, val_acc:0.979]
Epoch [106/120    avg_loss:0.013, val_acc:0.979]
Epoch [107/120    avg_loss:0.017, val_acc:0.979]
Epoch [108/120    avg_loss:0.017, val_acc:0.979]
Epoch [109/120    avg_loss:0.017, val_acc:0.979]
Epoch [110/120    avg_loss:0.022, val_acc:0.979]
Epoch [111/120    avg_loss:0.018, val_acc:0.979]
Epoch [112/120    avg_loss:0.016, val_acc:0.979]
Epoch [113/120    avg_loss:0.018, val_acc:0.979]
Epoch [114/120    avg_loss:0.016, val_acc:0.979]
Epoch [115/120    avg_loss:0.025, val_acc:0.979]
Epoch [116/120    avg_loss:0.018, val_acc:0.979]
Epoch [117/120    avg_loss:0.016, val_acc:0.979]
Epoch [118/120    avg_loss:0.024, val_acc:0.979]
Epoch [119/120    avg_loss:0.016, val_acc:0.979]
Epoch [120/120    avg_loss:0.024, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1264    0    3    0    3    0    0    0    6    4    5    0
     0    0    0]
 [   0    0    0  702    0   16    0    0    0   11    0    0   18    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    1    0    5    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  651    0    0    0    0    1    0    0
     5    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    8    0    0    0    0  858    1    1    0
     1    0    0]
 [   0    0    4    0    0    0    5    0    0    0    3 2196    1    1
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    2    0  525    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   18    0    0    0    0    2    0    0    0
  1119    0    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    31  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.95121951219512

F1 scores:
[       nan 0.975      0.98788589 0.96827586 0.99065421 0.93922652
 0.98264151 0.98039216 0.99649942 0.69230769 0.98169336 0.99546691
 0.96507353 0.99730458 0.97346672 0.93883792 0.97076023]

Kappa:
0.9766493805364348
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fec1b54d6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.278, val_acc:0.593]
Epoch [2/120    avg_loss:1.650, val_acc:0.646]
Epoch [3/120    avg_loss:1.277, val_acc:0.671]
Epoch [4/120    avg_loss:1.074, val_acc:0.772]
Epoch [5/120    avg_loss:0.989, val_acc:0.639]
Epoch [6/120    avg_loss:0.750, val_acc:0.811]
Epoch [7/120    avg_loss:0.607, val_acc:0.778]
Epoch [8/120    avg_loss:0.544, val_acc:0.778]
Epoch [9/120    avg_loss:0.676, val_acc:0.747]
Epoch [10/120    avg_loss:0.626, val_acc:0.862]
Epoch [11/120    avg_loss:0.354, val_acc:0.869]
Epoch [12/120    avg_loss:0.284, val_acc:0.909]
Epoch [13/120    avg_loss:0.214, val_acc:0.891]
Epoch [14/120    avg_loss:0.200, val_acc:0.902]
Epoch [15/120    avg_loss:0.214, val_acc:0.919]
Epoch [16/120    avg_loss:0.173, val_acc:0.905]
Epoch [17/120    avg_loss:0.207, val_acc:0.904]
Epoch [18/120    avg_loss:0.181, val_acc:0.910]
Epoch [19/120    avg_loss:0.230, val_acc:0.913]
Epoch [20/120    avg_loss:0.217, val_acc:0.906]
Epoch [21/120    avg_loss:0.217, val_acc:0.925]
Epoch [22/120    avg_loss:0.199, val_acc:0.938]
Epoch [23/120    avg_loss:0.147, val_acc:0.925]
Epoch [24/120    avg_loss:0.144, val_acc:0.946]
Epoch [25/120    avg_loss:0.101, val_acc:0.938]
Epoch [26/120    avg_loss:0.104, val_acc:0.935]
Epoch [27/120    avg_loss:0.099, val_acc:0.967]
Epoch [28/120    avg_loss:0.065, val_acc:0.962]
Epoch [29/120    avg_loss:0.063, val_acc:0.954]
Epoch [30/120    avg_loss:0.062, val_acc:0.964]
Epoch [31/120    avg_loss:0.043, val_acc:0.973]
Epoch [32/120    avg_loss:0.052, val_acc:0.966]
Epoch [33/120    avg_loss:0.065, val_acc:0.967]
Epoch [34/120    avg_loss:0.066, val_acc:0.938]
Epoch [35/120    avg_loss:0.063, val_acc:0.973]
Epoch [36/120    avg_loss:0.039, val_acc:0.979]
Epoch [37/120    avg_loss:0.038, val_acc:0.958]
Epoch [38/120    avg_loss:0.041, val_acc:0.965]
Epoch [39/120    avg_loss:0.040, val_acc:0.959]
Epoch [40/120    avg_loss:0.051, val_acc:0.962]
Epoch [41/120    avg_loss:0.064, val_acc:0.972]
Epoch [42/120    avg_loss:0.029, val_acc:0.976]
Epoch [43/120    avg_loss:0.081, val_acc:0.947]
Epoch [44/120    avg_loss:0.048, val_acc:0.968]
Epoch [45/120    avg_loss:0.036, val_acc:0.971]
Epoch [46/120    avg_loss:0.026, val_acc:0.979]
Epoch [47/120    avg_loss:0.023, val_acc:0.984]
Epoch [48/120    avg_loss:0.019, val_acc:0.979]
Epoch [49/120    avg_loss:0.019, val_acc:0.979]
Epoch [50/120    avg_loss:0.020, val_acc:0.964]
Epoch [51/120    avg_loss:0.021, val_acc:0.974]
Epoch [52/120    avg_loss:0.017, val_acc:0.984]
Epoch [53/120    avg_loss:0.013, val_acc:0.983]
Epoch [54/120    avg_loss:0.015, val_acc:0.980]
Epoch [55/120    avg_loss:0.013, val_acc:0.984]
Epoch [56/120    avg_loss:0.019, val_acc:0.976]
Epoch [57/120    avg_loss:0.027, val_acc:0.974]
Epoch [58/120    avg_loss:0.040, val_acc:0.982]
Epoch [59/120    avg_loss:0.020, val_acc:0.983]
Epoch [60/120    avg_loss:0.020, val_acc:0.982]
Epoch [61/120    avg_loss:0.024, val_acc:0.983]
Epoch [62/120    avg_loss:0.017, val_acc:0.976]
Epoch [63/120    avg_loss:0.014, val_acc:0.979]
Epoch [64/120    avg_loss:0.017, val_acc:0.974]
Epoch [65/120    avg_loss:0.012, val_acc:0.984]
Epoch [66/120    avg_loss:0.012, val_acc:0.979]
Epoch [67/120    avg_loss:0.014, val_acc:0.987]
Epoch [68/120    avg_loss:0.008, val_acc:0.987]
Epoch [69/120    avg_loss:0.006, val_acc:0.989]
Epoch [70/120    avg_loss:0.008, val_acc:0.990]
Epoch [71/120    avg_loss:0.011, val_acc:0.982]
Epoch [72/120    avg_loss:0.011, val_acc:0.984]
Epoch [73/120    avg_loss:0.011, val_acc:0.985]
Epoch [74/120    avg_loss:0.017, val_acc:0.979]
Epoch [75/120    avg_loss:0.012, val_acc:0.988]
Epoch [76/120    avg_loss:0.013, val_acc:0.980]
Epoch [77/120    avg_loss:0.013, val_acc:0.979]
Epoch [78/120    avg_loss:0.013, val_acc:0.980]
Epoch [79/120    avg_loss:0.009, val_acc:0.978]
Epoch [80/120    avg_loss:0.009, val_acc:0.983]
Epoch [81/120    avg_loss:0.011, val_acc:0.988]
Epoch [82/120    avg_loss:0.007, val_acc:0.987]
Epoch [83/120    avg_loss:0.007, val_acc:0.989]
Epoch [84/120    avg_loss:0.006, val_acc:0.988]
Epoch [85/120    avg_loss:0.006, val_acc:0.988]
Epoch [86/120    avg_loss:0.006, val_acc:0.988]
Epoch [87/120    avg_loss:0.009, val_acc:0.988]
Epoch [88/120    avg_loss:0.004, val_acc:0.989]
Epoch [89/120    avg_loss:0.005, val_acc:0.989]
Epoch [90/120    avg_loss:0.007, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.987]
Epoch [92/120    avg_loss:0.006, val_acc:0.987]
Epoch [93/120    avg_loss:0.003, val_acc:0.988]
Epoch [94/120    avg_loss:0.004, val_acc:0.988]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.005, val_acc:0.987]
Epoch [97/120    avg_loss:0.004, val_acc:0.987]
Epoch [98/120    avg_loss:0.004, val_acc:0.987]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.004, val_acc:0.988]
Epoch [101/120    avg_loss:0.004, val_acc:0.988]
Epoch [102/120    avg_loss:0.005, val_acc:0.988]
Epoch [103/120    avg_loss:0.003, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.989]
Epoch [105/120    avg_loss:0.005, val_acc:0.988]
Epoch [106/120    avg_loss:0.003, val_acc:0.988]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.005, val_acc:0.989]
Epoch [109/120    avg_loss:0.004, val_acc:0.989]
Epoch [110/120    avg_loss:0.004, val_acc:0.989]
Epoch [111/120    avg_loss:0.006, val_acc:0.989]
Epoch [112/120    avg_loss:0.005, val_acc:0.989]
Epoch [113/120    avg_loss:0.004, val_acc:0.989]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.004, val_acc:0.989]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.007, val_acc:0.989]
Epoch [118/120    avg_loss:0.004, val_acc:0.989]
Epoch [119/120    avg_loss:0.003, val_acc:0.989]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1276    0    2    0    0    0    0    0    3    2    2    0
     0    0    0]
 [   0    0    0  709    0   15    0    0    0    6    1    0   15    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0  861    3    0    0
     0    6    0]
 [   0    0   10    0    0    0    2    0    0    0    2 2194    0    2
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0  528    0
     2    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1136    2    0]
 [   0    0    0    0    0    0    4    0    0    0    0    0    0    0
    22  321    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.69918699186992

F1 scores:
[       nan 0.975      0.99222395 0.97323267 0.9953271  0.97412823
 0.99545455 1.         0.99883586 0.79069767 0.98738532 0.99523702
 0.97597043 0.9919571  0.98825576 0.94970414 0.9704142 ]

Kappa:
0.9851728468348381
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:13--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0bd72f5668>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.253, val_acc:0.513]
Epoch [2/120    avg_loss:1.517, val_acc:0.571]
Epoch [3/120    avg_loss:1.512, val_acc:0.686]
Epoch [4/120    avg_loss:1.078, val_acc:0.722]
Epoch [5/120    avg_loss:0.853, val_acc:0.765]
Epoch [6/120    avg_loss:0.667, val_acc:0.830]
Epoch [7/120    avg_loss:0.617, val_acc:0.790]
Epoch [8/120    avg_loss:0.588, val_acc:0.778]
Epoch [9/120    avg_loss:0.689, val_acc:0.828]
Epoch [10/120    avg_loss:0.573, val_acc:0.857]
Epoch [11/120    avg_loss:0.430, val_acc:0.837]
Epoch [12/120    avg_loss:0.306, val_acc:0.868]
Epoch [13/120    avg_loss:0.280, val_acc:0.878]
Epoch [14/120    avg_loss:0.265, val_acc:0.876]
Epoch [15/120    avg_loss:0.245, val_acc:0.904]
Epoch [16/120    avg_loss:0.211, val_acc:0.921]
Epoch [17/120    avg_loss:0.161, val_acc:0.921]
Epoch [18/120    avg_loss:0.145, val_acc:0.932]
Epoch [19/120    avg_loss:0.105, val_acc:0.944]
Epoch [20/120    avg_loss:0.120, val_acc:0.942]
Epoch [21/120    avg_loss:0.100, val_acc:0.918]
Epoch [22/120    avg_loss:0.087, val_acc:0.948]
Epoch [23/120    avg_loss:0.100, val_acc:0.938]
Epoch [24/120    avg_loss:0.143, val_acc:0.953]
Epoch [25/120    avg_loss:0.114, val_acc:0.948]
Epoch [26/120    avg_loss:0.074, val_acc:0.940]
Epoch [27/120    avg_loss:0.068, val_acc:0.965]
Epoch [28/120    avg_loss:0.054, val_acc:0.959]
Epoch [29/120    avg_loss:0.059, val_acc:0.956]
Epoch [30/120    avg_loss:0.056, val_acc:0.958]
Epoch [31/120    avg_loss:0.051, val_acc:0.962]
Epoch [32/120    avg_loss:0.061, val_acc:0.957]
Epoch [33/120    avg_loss:0.035, val_acc:0.956]
Epoch [34/120    avg_loss:0.034, val_acc:0.973]
Epoch [35/120    avg_loss:0.036, val_acc:0.966]
Epoch [36/120    avg_loss:0.040, val_acc:0.953]
Epoch [37/120    avg_loss:0.035, val_acc:0.966]
Epoch [38/120    avg_loss:0.032, val_acc:0.962]
Epoch [39/120    avg_loss:0.043, val_acc:0.974]
Epoch [40/120    avg_loss:0.048, val_acc:0.957]
Epoch [41/120    avg_loss:0.066, val_acc:0.960]
Epoch [42/120    avg_loss:0.030, val_acc:0.976]
Epoch [43/120    avg_loss:0.028, val_acc:0.973]
Epoch [44/120    avg_loss:0.030, val_acc:0.974]
Epoch [45/120    avg_loss:0.026, val_acc:0.966]
Epoch [46/120    avg_loss:0.080, val_acc:0.934]
Epoch [47/120    avg_loss:0.078, val_acc:0.951]
Epoch [48/120    avg_loss:0.039, val_acc:0.967]
Epoch [49/120    avg_loss:0.042, val_acc:0.976]
Epoch [50/120    avg_loss:0.056, val_acc:0.924]
Epoch [51/120    avg_loss:0.171, val_acc:0.912]
Epoch [52/120    avg_loss:0.127, val_acc:0.943]
Epoch [53/120    avg_loss:0.068, val_acc:0.970]
Epoch [54/120    avg_loss:0.047, val_acc:0.964]
Epoch [55/120    avg_loss:0.031, val_acc:0.973]
Epoch [56/120    avg_loss:0.026, val_acc:0.976]
Epoch [57/120    avg_loss:0.029, val_acc:0.982]
Epoch [58/120    avg_loss:0.037, val_acc:0.968]
Epoch [59/120    avg_loss:0.027, val_acc:0.975]
Epoch [60/120    avg_loss:0.028, val_acc:0.978]
Epoch [61/120    avg_loss:0.028, val_acc:0.973]
Epoch [62/120    avg_loss:0.025, val_acc:0.981]
Epoch [63/120    avg_loss:0.019, val_acc:0.974]
Epoch [64/120    avg_loss:0.018, val_acc:0.983]
Epoch [65/120    avg_loss:0.011, val_acc:0.983]
Epoch [66/120    avg_loss:0.012, val_acc:0.984]
Epoch [67/120    avg_loss:0.011, val_acc:0.984]
Epoch [68/120    avg_loss:0.017, val_acc:0.980]
Epoch [69/120    avg_loss:0.020, val_acc:0.978]
Epoch [70/120    avg_loss:0.035, val_acc:0.972]
Epoch [71/120    avg_loss:0.025, val_acc:0.979]
Epoch [72/120    avg_loss:0.021, val_acc:0.976]
Epoch [73/120    avg_loss:0.018, val_acc:0.981]
Epoch [74/120    avg_loss:0.010, val_acc:0.981]
Epoch [75/120    avg_loss:0.008, val_acc:0.984]
Epoch [76/120    avg_loss:0.009, val_acc:0.979]
Epoch [77/120    avg_loss:0.006, val_acc:0.985]
Epoch [78/120    avg_loss:0.010, val_acc:0.983]
Epoch [79/120    avg_loss:0.008, val_acc:0.981]
Epoch [80/120    avg_loss:0.011, val_acc:0.983]
Epoch [81/120    avg_loss:0.006, val_acc:0.986]
Epoch [82/120    avg_loss:0.007, val_acc:0.983]
Epoch [83/120    avg_loss:0.014, val_acc:0.984]
Epoch [84/120    avg_loss:0.012, val_acc:0.980]
Epoch [85/120    avg_loss:0.009, val_acc:0.981]
Epoch [86/120    avg_loss:0.007, val_acc:0.986]
Epoch [87/120    avg_loss:0.005, val_acc:0.987]
Epoch [88/120    avg_loss:0.005, val_acc:0.985]
Epoch [89/120    avg_loss:0.004, val_acc:0.986]
Epoch [90/120    avg_loss:0.003, val_acc:0.988]
Epoch [91/120    avg_loss:0.005, val_acc:0.984]
Epoch [92/120    avg_loss:0.006, val_acc:0.986]
Epoch [93/120    avg_loss:0.007, val_acc:0.985]
Epoch [94/120    avg_loss:0.005, val_acc:0.984]
Epoch [95/120    avg_loss:0.009, val_acc:0.984]
Epoch [96/120    avg_loss:0.006, val_acc:0.984]
Epoch [97/120    avg_loss:0.004, val_acc:0.985]
Epoch [98/120    avg_loss:0.007, val_acc:0.981]
Epoch [99/120    avg_loss:0.006, val_acc:0.986]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.003, val_acc:0.988]
Epoch [102/120    avg_loss:0.004, val_acc:0.988]
Epoch [103/120    avg_loss:0.003, val_acc:0.989]
Epoch [104/120    avg_loss:0.005, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.005, val_acc:0.984]
Epoch [107/120    avg_loss:0.005, val_acc:0.987]
Epoch [108/120    avg_loss:0.006, val_acc:0.987]
Epoch [109/120    avg_loss:0.004, val_acc:0.987]
Epoch [110/120    avg_loss:0.002, val_acc:0.986]
Epoch [111/120    avg_loss:0.007, val_acc:0.983]
Epoch [112/120    avg_loss:0.010, val_acc:0.983]
Epoch [113/120    avg_loss:0.006, val_acc:0.985]
Epoch [114/120    avg_loss:0.004, val_acc:0.982]
Epoch [115/120    avg_loss:0.004, val_acc:0.987]
Epoch [116/120    avg_loss:0.004, val_acc:0.989]
Epoch [117/120    avg_loss:0.002, val_acc:0.985]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.004, val_acc:0.989]
Epoch [120/120    avg_loss:0.003, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1273    0    0    0    0    0    0    0    3    8    1    0
     0    0    0]
 [   0    0    0  718    0    6    0    0    0    6    0    0   15    0
     2    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0  857    4    1    0
     3    6    0]
 [   0    0    6    0    0    2    1    0    0    0    1 2199    0    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  532    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    27  311    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.75338753387534

F1 scores:
[       nan 0.98765432 0.99259259 0.98020478 1.         0.98181818
 0.99244713 1.         1.         0.81818182 0.98732719 0.99479756
 0.98064516 0.99730458 0.98528139 0.93674699 0.97619048]

Kappa:
0.9857859252825028
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:15--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f958e65f748>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.107, val_acc:0.549]
Epoch [2/120    avg_loss:1.593, val_acc:0.519]
Epoch [3/120    avg_loss:1.452, val_acc:0.668]
Epoch [4/120    avg_loss:1.001, val_acc:0.653]
Epoch [5/120    avg_loss:0.872, val_acc:0.785]
Epoch [6/120    avg_loss:0.752, val_acc:0.739]
Epoch [7/120    avg_loss:0.606, val_acc:0.855]
Epoch [8/120    avg_loss:0.453, val_acc:0.832]
Epoch [9/120    avg_loss:0.362, val_acc:0.844]
Epoch [10/120    avg_loss:0.281, val_acc:0.871]
Epoch [11/120    avg_loss:0.455, val_acc:0.886]
Epoch [12/120    avg_loss:0.392, val_acc:0.868]
Epoch [13/120    avg_loss:0.323, val_acc:0.865]
Epoch [14/120    avg_loss:0.233, val_acc:0.891]
Epoch [15/120    avg_loss:0.167, val_acc:0.912]
Epoch [16/120    avg_loss:0.163, val_acc:0.908]
Epoch [17/120    avg_loss:0.180, val_acc:0.911]
Epoch [18/120    avg_loss:0.179, val_acc:0.927]
Epoch [19/120    avg_loss:0.150, val_acc:0.921]
Epoch [20/120    avg_loss:0.128, val_acc:0.939]
Epoch [21/120    avg_loss:0.097, val_acc:0.937]
Epoch [22/120    avg_loss:0.082, val_acc:0.932]
Epoch [23/120    avg_loss:0.144, val_acc:0.849]
Epoch [24/120    avg_loss:0.192, val_acc:0.936]
Epoch [25/120    avg_loss:0.130, val_acc:0.926]
Epoch [26/120    avg_loss:0.143, val_acc:0.942]
Epoch [27/120    avg_loss:0.081, val_acc:0.959]
Epoch [28/120    avg_loss:0.043, val_acc:0.957]
Epoch [29/120    avg_loss:0.056, val_acc:0.939]
Epoch [30/120    avg_loss:0.075, val_acc:0.966]
Epoch [31/120    avg_loss:0.083, val_acc:0.937]
Epoch [32/120    avg_loss:0.097, val_acc:0.942]
Epoch [33/120    avg_loss:0.070, val_acc:0.960]
Epoch [34/120    avg_loss:0.050, val_acc:0.963]
Epoch [35/120    avg_loss:0.053, val_acc:0.948]
Epoch [36/120    avg_loss:0.058, val_acc:0.970]
Epoch [37/120    avg_loss:0.040, val_acc:0.957]
Epoch [38/120    avg_loss:0.053, val_acc:0.954]
Epoch [39/120    avg_loss:0.054, val_acc:0.963]
Epoch [40/120    avg_loss:0.073, val_acc:0.963]
Epoch [41/120    avg_loss:0.058, val_acc:0.952]
Epoch [42/120    avg_loss:0.104, val_acc:0.967]
Epoch [43/120    avg_loss:0.027, val_acc:0.972]
Epoch [44/120    avg_loss:0.027, val_acc:0.973]
Epoch [45/120    avg_loss:0.022, val_acc:0.984]
Epoch [46/120    avg_loss:0.024, val_acc:0.981]
Epoch [47/120    avg_loss:0.027, val_acc:0.967]
Epoch [48/120    avg_loss:0.035, val_acc:0.969]
Epoch [49/120    avg_loss:0.026, val_acc:0.974]
Epoch [50/120    avg_loss:0.025, val_acc:0.978]
Epoch [51/120    avg_loss:0.026, val_acc:0.974]
Epoch [52/120    avg_loss:0.024, val_acc:0.969]
Epoch [53/120    avg_loss:0.054, val_acc:0.976]
Epoch [54/120    avg_loss:0.032, val_acc:0.980]
Epoch [55/120    avg_loss:0.027, val_acc:0.970]
Epoch [56/120    avg_loss:0.024, val_acc:0.965]
Epoch [57/120    avg_loss:0.017, val_acc:0.979]
Epoch [58/120    avg_loss:0.018, val_acc:0.976]
Epoch [59/120    avg_loss:0.010, val_acc:0.976]
Epoch [60/120    avg_loss:0.011, val_acc:0.982]
Epoch [61/120    avg_loss:0.015, val_acc:0.983]
Epoch [62/120    avg_loss:0.012, val_acc:0.983]
Epoch [63/120    avg_loss:0.010, val_acc:0.983]
Epoch [64/120    avg_loss:0.010, val_acc:0.984]
Epoch [65/120    avg_loss:0.011, val_acc:0.983]
Epoch [66/120    avg_loss:0.009, val_acc:0.984]
Epoch [67/120    avg_loss:0.009, val_acc:0.984]
Epoch [68/120    avg_loss:0.008, val_acc:0.984]
Epoch [69/120    avg_loss:0.008, val_acc:0.985]
Epoch [70/120    avg_loss:0.008, val_acc:0.986]
Epoch [71/120    avg_loss:0.006, val_acc:0.985]
Epoch [72/120    avg_loss:0.007, val_acc:0.985]
Epoch [73/120    avg_loss:0.008, val_acc:0.985]
Epoch [74/120    avg_loss:0.009, val_acc:0.985]
Epoch [75/120    avg_loss:0.009, val_acc:0.986]
Epoch [76/120    avg_loss:0.008, val_acc:0.986]
Epoch [77/120    avg_loss:0.010, val_acc:0.986]
Epoch [78/120    avg_loss:0.007, val_acc:0.986]
Epoch [79/120    avg_loss:0.009, val_acc:0.985]
Epoch [80/120    avg_loss:0.007, val_acc:0.984]
Epoch [81/120    avg_loss:0.006, val_acc:0.984]
Epoch [82/120    avg_loss:0.011, val_acc:0.984]
Epoch [83/120    avg_loss:0.008, val_acc:0.984]
Epoch [84/120    avg_loss:0.007, val_acc:0.984]
Epoch [85/120    avg_loss:0.005, val_acc:0.984]
Epoch [86/120    avg_loss:0.006, val_acc:0.984]
Epoch [87/120    avg_loss:0.006, val_acc:0.985]
Epoch [88/120    avg_loss:0.006, val_acc:0.984]
Epoch [89/120    avg_loss:0.006, val_acc:0.984]
Epoch [90/120    avg_loss:0.008, val_acc:0.985]
Epoch [91/120    avg_loss:0.006, val_acc:0.987]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.986]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.007, val_acc:0.984]
Epoch [96/120    avg_loss:0.005, val_acc:0.984]
Epoch [97/120    avg_loss:0.008, val_acc:0.983]
Epoch [98/120    avg_loss:0.007, val_acc:0.984]
Epoch [99/120    avg_loss:0.007, val_acc:0.985]
Epoch [100/120    avg_loss:0.006, val_acc:0.984]
Epoch [101/120    avg_loss:0.008, val_acc:0.983]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.005, val_acc:0.984]
Epoch [104/120    avg_loss:0.007, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.984]
Epoch [106/120    avg_loss:0.005, val_acc:0.984]
Epoch [107/120    avg_loss:0.005, val_acc:0.984]
Epoch [108/120    avg_loss:0.006, val_acc:0.984]
Epoch [109/120    avg_loss:0.010, val_acc:0.984]
Epoch [110/120    avg_loss:0.006, val_acc:0.984]
Epoch [111/120    avg_loss:0.007, val_acc:0.985]
Epoch [112/120    avg_loss:0.006, val_acc:0.985]
Epoch [113/120    avg_loss:0.005, val_acc:0.985]
Epoch [114/120    avg_loss:0.006, val_acc:0.985]
Epoch [115/120    avg_loss:0.008, val_acc:0.985]
Epoch [116/120    avg_loss:0.006, val_acc:0.985]
Epoch [117/120    avg_loss:0.008, val_acc:0.984]
Epoch [118/120    avg_loss:0.008, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.007, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1273    2    0    0    0    0    0    0    4    6    0    0
     0    0    0]
 [   0    0    0  718    0    5    0    0    0    7    1    0   14    2
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    4    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  426    0    0    0    4    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    6    0    0    0    0  846   16    3    0
     0    3    0]
 [   0    0    3    0    0    0    2    0    0    0    3 2202    0    0
     0    0    0]
 [   0    0    0    0    2    0    0    0    0    0    1    0  530    0
     0    0    1]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    41  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.35230352303523

F1 scores:
[       nan 0.98765432 0.99336715 0.97554348 0.99297424 0.9771167
 0.99393939 0.92592593 0.99416569 0.65116279 0.97746967 0.99301015
 0.97516099 0.99191375 0.98144152 0.92473118 0.98203593]

Kappa:
0.9812057922456214
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe47ed656d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.249, val_acc:0.522]
Epoch [2/120    avg_loss:1.699, val_acc:0.623]
Epoch [3/120    avg_loss:1.367, val_acc:0.625]
Epoch [4/120    avg_loss:1.016, val_acc:0.668]
Epoch [5/120    avg_loss:0.966, val_acc:0.746]
Epoch [6/120    avg_loss:0.896, val_acc:0.757]
Epoch [7/120    avg_loss:0.621, val_acc:0.770]
Epoch [8/120    avg_loss:0.690, val_acc:0.782]
Epoch [9/120    avg_loss:0.565, val_acc:0.820]
Epoch [10/120    avg_loss:0.365, val_acc:0.860]
Epoch [11/120    avg_loss:0.288, val_acc:0.899]
Epoch [12/120    avg_loss:0.262, val_acc:0.855]
Epoch [13/120    avg_loss:0.323, val_acc:0.884]
Epoch [14/120    avg_loss:0.276, val_acc:0.883]
Epoch [15/120    avg_loss:0.243, val_acc:0.931]
Epoch [16/120    avg_loss:0.189, val_acc:0.919]
Epoch [17/120    avg_loss:0.155, val_acc:0.909]
Epoch [18/120    avg_loss:0.225, val_acc:0.879]
Epoch [19/120    avg_loss:0.220, val_acc:0.910]
Epoch [20/120    avg_loss:0.174, val_acc:0.876]
Epoch [21/120    avg_loss:0.109, val_acc:0.941]
Epoch [22/120    avg_loss:0.093, val_acc:0.951]
Epoch [23/120    avg_loss:0.113, val_acc:0.930]
Epoch [24/120    avg_loss:0.148, val_acc:0.931]
Epoch [25/120    avg_loss:0.084, val_acc:0.924]
Epoch [26/120    avg_loss:0.105, val_acc:0.935]
Epoch [27/120    avg_loss:0.074, val_acc:0.955]
Epoch [28/120    avg_loss:0.051, val_acc:0.958]
Epoch [29/120    avg_loss:0.065, val_acc:0.954]
Epoch [30/120    avg_loss:0.062, val_acc:0.956]
Epoch [31/120    avg_loss:0.057, val_acc:0.963]
Epoch [32/120    avg_loss:0.056, val_acc:0.966]
Epoch [33/120    avg_loss:0.099, val_acc:0.927]
Epoch [34/120    avg_loss:0.116, val_acc:0.934]
Epoch [35/120    avg_loss:0.142, val_acc:0.944]
Epoch [36/120    avg_loss:0.177, val_acc:0.916]
Epoch [37/120    avg_loss:0.118, val_acc:0.938]
Epoch [38/120    avg_loss:0.094, val_acc:0.947]
Epoch [39/120    avg_loss:0.070, val_acc:0.953]
Epoch [40/120    avg_loss:0.051, val_acc:0.950]
Epoch [41/120    avg_loss:0.041, val_acc:0.957]
Epoch [42/120    avg_loss:0.054, val_acc:0.961]
Epoch [43/120    avg_loss:0.034, val_acc:0.965]
Epoch [44/120    avg_loss:0.051, val_acc:0.968]
Epoch [45/120    avg_loss:0.041, val_acc:0.971]
Epoch [46/120    avg_loss:0.028, val_acc:0.958]
Epoch [47/120    avg_loss:0.030, val_acc:0.967]
Epoch [48/120    avg_loss:0.024, val_acc:0.973]
Epoch [49/120    avg_loss:0.019, val_acc:0.970]
Epoch [50/120    avg_loss:0.018, val_acc:0.963]
Epoch [51/120    avg_loss:0.020, val_acc:0.973]
Epoch [52/120    avg_loss:0.018, val_acc:0.973]
Epoch [53/120    avg_loss:0.018, val_acc:0.967]
Epoch [54/120    avg_loss:0.023, val_acc:0.967]
Epoch [55/120    avg_loss:0.015, val_acc:0.975]
Epoch [56/120    avg_loss:0.022, val_acc:0.970]
Epoch [57/120    avg_loss:0.020, val_acc:0.970]
Epoch [58/120    avg_loss:0.018, val_acc:0.974]
Epoch [59/120    avg_loss:0.013, val_acc:0.967]
Epoch [60/120    avg_loss:0.011, val_acc:0.971]
Epoch [61/120    avg_loss:0.023, val_acc:0.966]
Epoch [62/120    avg_loss:0.013, val_acc:0.970]
Epoch [63/120    avg_loss:0.032, val_acc:0.971]
Epoch [64/120    avg_loss:0.042, val_acc:0.963]
Epoch [65/120    avg_loss:0.027, val_acc:0.977]
Epoch [66/120    avg_loss:0.012, val_acc:0.977]
Epoch [67/120    avg_loss:0.010, val_acc:0.978]
Epoch [68/120    avg_loss:0.018, val_acc:0.969]
Epoch [69/120    avg_loss:0.014, val_acc:0.971]
Epoch [70/120    avg_loss:0.036, val_acc:0.964]
Epoch [71/120    avg_loss:0.048, val_acc:0.967]
Epoch [72/120    avg_loss:0.025, val_acc:0.974]
Epoch [73/120    avg_loss:0.020, val_acc:0.970]
Epoch [74/120    avg_loss:0.036, val_acc:0.953]
Epoch [75/120    avg_loss:0.072, val_acc:0.965]
Epoch [76/120    avg_loss:0.025, val_acc:0.970]
Epoch [77/120    avg_loss:0.014, val_acc:0.971]
Epoch [78/120    avg_loss:0.009, val_acc:0.978]
Epoch [79/120    avg_loss:0.014, val_acc:0.978]
Epoch [80/120    avg_loss:0.022, val_acc:0.976]
Epoch [81/120    avg_loss:0.013, val_acc:0.968]
Epoch [82/120    avg_loss:0.022, val_acc:0.977]
Epoch [83/120    avg_loss:0.012, val_acc:0.970]
Epoch [84/120    avg_loss:0.010, val_acc:0.974]
Epoch [85/120    avg_loss:0.007, val_acc:0.977]
Epoch [86/120    avg_loss:0.004, val_acc:0.976]
Epoch [87/120    avg_loss:0.019, val_acc:0.974]
Epoch [88/120    avg_loss:0.017, val_acc:0.979]
Epoch [89/120    avg_loss:0.010, val_acc:0.976]
Epoch [90/120    avg_loss:0.006, val_acc:0.977]
Epoch [91/120    avg_loss:0.011, val_acc:0.978]
Epoch [92/120    avg_loss:0.010, val_acc:0.981]
Epoch [93/120    avg_loss:0.005, val_acc:0.979]
Epoch [94/120    avg_loss:0.005, val_acc:0.976]
Epoch [95/120    avg_loss:0.008, val_acc:0.977]
Epoch [96/120    avg_loss:0.009, val_acc:0.971]
Epoch [97/120    avg_loss:0.013, val_acc:0.976]
Epoch [98/120    avg_loss:0.008, val_acc:0.975]
Epoch [99/120    avg_loss:0.010, val_acc:0.981]
Epoch [100/120    avg_loss:0.006, val_acc:0.981]
Epoch [101/120    avg_loss:0.005, val_acc:0.978]
Epoch [102/120    avg_loss:0.005, val_acc:0.980]
Epoch [103/120    avg_loss:0.009, val_acc:0.973]
Epoch [104/120    avg_loss:0.010, val_acc:0.974]
Epoch [105/120    avg_loss:0.014, val_acc:0.978]
Epoch [106/120    avg_loss:0.013, val_acc:0.978]
Epoch [107/120    avg_loss:0.010, val_acc:0.976]
Epoch [108/120    avg_loss:0.011, val_acc:0.977]
Epoch [109/120    avg_loss:0.007, val_acc:0.981]
Epoch [110/120    avg_loss:0.004, val_acc:0.982]
Epoch [111/120    avg_loss:0.004, val_acc:0.978]
Epoch [112/120    avg_loss:0.005, val_acc:0.979]
Epoch [113/120    avg_loss:0.007, val_acc:0.979]
Epoch [114/120    avg_loss:0.006, val_acc:0.980]
Epoch [115/120    avg_loss:0.005, val_acc:0.979]
Epoch [116/120    avg_loss:0.003, val_acc:0.982]
Epoch [117/120    avg_loss:0.004, val_acc:0.980]
Epoch [118/120    avg_loss:0.010, val_acc:0.976]
Epoch [119/120    avg_loss:0.008, val_acc:0.979]
Epoch [120/120    avg_loss:0.006, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    3    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1263    0    0    0    1    0    0    0   11   10    0    0
     0    0    0]
 [   0    0    0  713    0   15    0    0    0    3    0    0   16    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    4    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0  854   12    0    0
     0    6    0]
 [   0    0    5    0    0    0    3    0    0    0    4 2188   10    0
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    2    2  525    0
     1    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    4    2    0    1    0    0    0    0    0
  1132    0    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    47  290    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.91869918699187

F1 scores:
[       nan 0.96202532 0.98826291 0.97404372 0.99528302 0.96512936
 0.98720843 0.98039216 0.99650757 0.7804878  0.97823597 0.98915009
 0.96330275 0.99728997 0.97628288 0.90202177 0.97005988]

Kappa:
0.976264069940909
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6c6cc3d6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.203, val_acc:0.537]
Epoch [2/120    avg_loss:1.638, val_acc:0.484]
Epoch [3/120    avg_loss:1.411, val_acc:0.644]
Epoch [4/120    avg_loss:1.004, val_acc:0.720]
Epoch [5/120    avg_loss:1.013, val_acc:0.719]
Epoch [6/120    avg_loss:0.951, val_acc:0.786]
Epoch [7/120    avg_loss:0.723, val_acc:0.809]
Epoch [8/120    avg_loss:0.575, val_acc:0.821]
Epoch [9/120    avg_loss:0.433, val_acc:0.864]
Epoch [10/120    avg_loss:0.377, val_acc:0.873]
Epoch [11/120    avg_loss:0.272, val_acc:0.912]
Epoch [12/120    avg_loss:0.276, val_acc:0.844]
Epoch [13/120    avg_loss:0.304, val_acc:0.810]
Epoch [14/120    avg_loss:0.309, val_acc:0.880]
Epoch [15/120    avg_loss:0.235, val_acc:0.896]
Epoch [16/120    avg_loss:0.161, val_acc:0.922]
Epoch [17/120    avg_loss:0.216, val_acc:0.912]
Epoch [18/120    avg_loss:0.179, val_acc:0.929]
Epoch [19/120    avg_loss:0.154, val_acc:0.943]
Epoch [20/120    avg_loss:0.099, val_acc:0.944]
Epoch [21/120    avg_loss:0.083, val_acc:0.948]
Epoch [22/120    avg_loss:0.077, val_acc:0.951]
Epoch [23/120    avg_loss:0.065, val_acc:0.942]
Epoch [24/120    avg_loss:0.090, val_acc:0.960]
Epoch [25/120    avg_loss:0.075, val_acc:0.946]
Epoch [26/120    avg_loss:0.085, val_acc:0.935]
Epoch [27/120    avg_loss:0.063, val_acc:0.976]
Epoch [28/120    avg_loss:0.043, val_acc:0.971]
Epoch [29/120    avg_loss:0.038, val_acc:0.938]
Epoch [30/120    avg_loss:0.051, val_acc:0.972]
Epoch [31/120    avg_loss:0.054, val_acc:0.956]
Epoch [32/120    avg_loss:0.049, val_acc:0.974]
Epoch [33/120    avg_loss:0.039, val_acc:0.962]
Epoch [34/120    avg_loss:0.044, val_acc:0.969]
Epoch [35/120    avg_loss:0.032, val_acc:0.974]
Epoch [36/120    avg_loss:0.031, val_acc:0.980]
Epoch [37/120    avg_loss:0.030, val_acc:0.958]
Epoch [38/120    avg_loss:0.038, val_acc:0.967]
Epoch [39/120    avg_loss:0.062, val_acc:0.960]
Epoch [40/120    avg_loss:0.045, val_acc:0.974]
Epoch [41/120    avg_loss:0.023, val_acc:0.970]
Epoch [42/120    avg_loss:0.019, val_acc:0.984]
Epoch [43/120    avg_loss:0.039, val_acc:0.966]
Epoch [44/120    avg_loss:0.055, val_acc:0.905]
Epoch [45/120    avg_loss:0.053, val_acc:0.967]
Epoch [46/120    avg_loss:0.055, val_acc:0.967]
Epoch [47/120    avg_loss:0.040, val_acc:0.950]
Epoch [48/120    avg_loss:0.066, val_acc:0.966]
Epoch [49/120    avg_loss:0.031, val_acc:0.979]
Epoch [50/120    avg_loss:0.032, val_acc:0.975]
Epoch [51/120    avg_loss:0.020, val_acc:0.972]
Epoch [52/120    avg_loss:0.025, val_acc:0.973]
Epoch [53/120    avg_loss:0.027, val_acc:0.974]
Epoch [54/120    avg_loss:0.015, val_acc:0.986]
Epoch [55/120    avg_loss:0.027, val_acc:0.982]
Epoch [56/120    avg_loss:0.016, val_acc:0.982]
Epoch [57/120    avg_loss:0.023, val_acc:0.972]
Epoch [58/120    avg_loss:0.054, val_acc:0.973]
Epoch [59/120    avg_loss:0.028, val_acc:0.978]
Epoch [60/120    avg_loss:0.019, val_acc:0.980]
Epoch [61/120    avg_loss:0.022, val_acc:0.988]
Epoch [62/120    avg_loss:0.008, val_acc:0.988]
Epoch [63/120    avg_loss:0.009, val_acc:0.988]
Epoch [64/120    avg_loss:0.012, val_acc:0.976]
Epoch [65/120    avg_loss:0.014, val_acc:0.988]
Epoch [66/120    avg_loss:0.010, val_acc:0.985]
Epoch [67/120    avg_loss:0.006, val_acc:0.989]
Epoch [68/120    avg_loss:0.008, val_acc:0.986]
Epoch [69/120    avg_loss:0.007, val_acc:0.984]
Epoch [70/120    avg_loss:0.010, val_acc:0.984]
Epoch [71/120    avg_loss:0.011, val_acc:0.983]
Epoch [72/120    avg_loss:0.007, val_acc:0.985]
Epoch [73/120    avg_loss:0.013, val_acc:0.986]
Epoch [74/120    avg_loss:0.011, val_acc:0.986]
Epoch [75/120    avg_loss:0.010, val_acc:0.980]
Epoch [76/120    avg_loss:0.011, val_acc:0.984]
Epoch [77/120    avg_loss:0.008, val_acc:0.985]
Epoch [78/120    avg_loss:0.008, val_acc:0.989]
Epoch [79/120    avg_loss:0.013, val_acc:0.979]
Epoch [80/120    avg_loss:0.009, val_acc:0.987]
Epoch [81/120    avg_loss:0.010, val_acc:0.989]
Epoch [82/120    avg_loss:0.019, val_acc:0.984]
Epoch [83/120    avg_loss:0.014, val_acc:0.991]
Epoch [84/120    avg_loss:0.009, val_acc:0.987]
Epoch [85/120    avg_loss:0.012, val_acc:0.978]
Epoch [86/120    avg_loss:0.012, val_acc:0.981]
Epoch [87/120    avg_loss:0.010, val_acc:0.986]
Epoch [88/120    avg_loss:0.013, val_acc:0.986]
Epoch [89/120    avg_loss:0.007, val_acc:0.988]
Epoch [90/120    avg_loss:0.005, val_acc:0.993]
Epoch [91/120    avg_loss:0.006, val_acc:0.994]
Epoch [92/120    avg_loss:0.007, val_acc:0.987]
Epoch [93/120    avg_loss:0.004, val_acc:0.993]
Epoch [94/120    avg_loss:0.004, val_acc:0.991]
Epoch [95/120    avg_loss:0.003, val_acc:0.990]
Epoch [96/120    avg_loss:0.003, val_acc:0.990]
Epoch [97/120    avg_loss:0.005, val_acc:0.991]
Epoch [98/120    avg_loss:0.004, val_acc:0.989]
Epoch [99/120    avg_loss:0.003, val_acc:0.990]
Epoch [100/120    avg_loss:0.003, val_acc:0.994]
Epoch [101/120    avg_loss:0.002, val_acc:0.994]
Epoch [102/120    avg_loss:0.002, val_acc:0.991]
Epoch [103/120    avg_loss:0.005, val_acc:0.989]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.004, val_acc:0.987]
Epoch [106/120    avg_loss:0.004, val_acc:0.986]
Epoch [107/120    avg_loss:0.003, val_acc:0.984]
Epoch [108/120    avg_loss:0.004, val_acc:0.988]
Epoch [109/120    avg_loss:0.002, val_acc:0.989]
Epoch [110/120    avg_loss:0.003, val_acc:0.990]
Epoch [111/120    avg_loss:0.008, val_acc:0.990]
Epoch [112/120    avg_loss:0.005, val_acc:0.994]
Epoch [113/120    avg_loss:0.006, val_acc:0.991]
Epoch [114/120    avg_loss:0.004, val_acc:0.993]
Epoch [115/120    avg_loss:0.007, val_acc:0.991]
Epoch [116/120    avg_loss:0.012, val_acc:0.983]
Epoch [117/120    avg_loss:0.004, val_acc:0.990]
Epoch [118/120    avg_loss:0.003, val_acc:0.990]
Epoch [119/120    avg_loss:0.003, val_acc:0.990]
Epoch [120/120    avg_loss:0.002, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1272    6    0    0    0    0    0    0    5    2    0    0
     0    0    0]
 [   0    0    0  714    0   15    0    0    0    4    0    0   14    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    5    0    0    0    0  861    1    2    0
     2    1    0]
 [   0    0    7    0    0    6    4    0    0    0    2 2191    0    0
     0    0    0]
 [   0    0    0    0    6    1    0    0    0    0    0    0  523    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    0    0    0    0
  1134    0    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    34  300    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.4390243902439

F1 scores:
[       nan 1.         0.99104012 0.97341513 0.98611111 0.96337403
 0.98722765 1.         1.         0.87804878 0.98795181 0.99500454
 0.97392924 1.         0.9822434  0.92592593 0.97076023]

Kappa:
0.9822074376500209
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:22--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f82a2731748>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.232, val_acc:0.493]
Epoch [2/120    avg_loss:1.683, val_acc:0.619]
Epoch [3/120    avg_loss:1.257, val_acc:0.718]
Epoch [4/120    avg_loss:1.047, val_acc:0.662]
Epoch [5/120    avg_loss:0.911, val_acc:0.694]
Epoch [6/120    avg_loss:0.953, val_acc:0.735]
Epoch [7/120    avg_loss:0.649, val_acc:0.825]
Epoch [8/120    avg_loss:0.541, val_acc:0.807]
Epoch [9/120    avg_loss:0.525, val_acc:0.828]
Epoch [10/120    avg_loss:0.570, val_acc:0.828]
Epoch [11/120    avg_loss:0.326, val_acc:0.863]
Epoch [12/120    avg_loss:0.294, val_acc:0.886]
Epoch [13/120    avg_loss:0.216, val_acc:0.899]
Epoch [14/120    avg_loss:0.208, val_acc:0.880]
Epoch [15/120    avg_loss:0.210, val_acc:0.932]
Epoch [16/120    avg_loss:0.154, val_acc:0.936]
Epoch [17/120    avg_loss:0.159, val_acc:0.926]
Epoch [18/120    avg_loss:0.151, val_acc:0.903]
Epoch [19/120    avg_loss:0.140, val_acc:0.941]
Epoch [20/120    avg_loss:0.144, val_acc:0.936]
Epoch [21/120    avg_loss:0.100, val_acc:0.931]
Epoch [22/120    avg_loss:0.057, val_acc:0.954]
Epoch [23/120    avg_loss:0.056, val_acc:0.951]
Epoch [24/120    avg_loss:0.082, val_acc:0.950]
Epoch [25/120    avg_loss:0.055, val_acc:0.959]
Epoch [26/120    avg_loss:0.067, val_acc:0.959]
Epoch [27/120    avg_loss:0.079, val_acc:0.953]
Epoch [28/120    avg_loss:0.098, val_acc:0.936]
Epoch [29/120    avg_loss:0.080, val_acc:0.940]
Epoch [30/120    avg_loss:0.077, val_acc:0.960]
Epoch [31/120    avg_loss:0.052, val_acc:0.969]
Epoch [32/120    avg_loss:0.043, val_acc:0.962]
Epoch [33/120    avg_loss:0.042, val_acc:0.965]
Epoch [34/120    avg_loss:0.051, val_acc:0.964]
Epoch [35/120    avg_loss:0.036, val_acc:0.968]
Epoch [36/120    avg_loss:0.022, val_acc:0.970]
Epoch [37/120    avg_loss:0.023, val_acc:0.966]
Epoch [38/120    avg_loss:0.022, val_acc:0.971]
Epoch [39/120    avg_loss:0.023, val_acc:0.976]
Epoch [40/120    avg_loss:0.035, val_acc:0.969]
Epoch [41/120    avg_loss:0.026, val_acc:0.972]
Epoch [42/120    avg_loss:0.029, val_acc:0.972]
Epoch [43/120    avg_loss:0.037, val_acc:0.962]
Epoch [44/120    avg_loss:0.022, val_acc:0.965]
Epoch [45/120    avg_loss:0.032, val_acc:0.976]
Epoch [46/120    avg_loss:0.024, val_acc:0.973]
Epoch [47/120    avg_loss:0.025, val_acc:0.973]
Epoch [48/120    avg_loss:0.020, val_acc:0.976]
Epoch [49/120    avg_loss:0.022, val_acc:0.975]
Epoch [50/120    avg_loss:0.012, val_acc:0.976]
Epoch [51/120    avg_loss:0.009, val_acc:0.974]
Epoch [52/120    avg_loss:0.011, val_acc:0.982]
Epoch [53/120    avg_loss:0.020, val_acc:0.981]
Epoch [54/120    avg_loss:0.017, val_acc:0.974]
Epoch [55/120    avg_loss:0.012, val_acc:0.978]
Epoch [56/120    avg_loss:0.009, val_acc:0.979]
Epoch [57/120    avg_loss:0.015, val_acc:0.975]
Epoch [58/120    avg_loss:0.029, val_acc:0.978]
Epoch [59/120    avg_loss:0.019, val_acc:0.971]
Epoch [60/120    avg_loss:0.029, val_acc:0.935]
Epoch [61/120    avg_loss:0.059, val_acc:0.978]
Epoch [62/120    avg_loss:0.021, val_acc:0.970]
Epoch [63/120    avg_loss:0.011, val_acc:0.980]
Epoch [64/120    avg_loss:0.011, val_acc:0.984]
Epoch [65/120    avg_loss:0.014, val_acc:0.975]
Epoch [66/120    avg_loss:0.015, val_acc:0.980]
Epoch [67/120    avg_loss:0.015, val_acc:0.981]
Epoch [68/120    avg_loss:0.009, val_acc:0.978]
Epoch [69/120    avg_loss:0.011, val_acc:0.979]
Epoch [70/120    avg_loss:0.016, val_acc:0.974]
Epoch [71/120    avg_loss:0.015, val_acc:0.979]
Epoch [72/120    avg_loss:0.008, val_acc:0.981]
Epoch [73/120    avg_loss:0.014, val_acc:0.981]
Epoch [74/120    avg_loss:0.012, val_acc:0.984]
Epoch [75/120    avg_loss:0.017, val_acc:0.979]
Epoch [76/120    avg_loss:0.011, val_acc:0.983]
Epoch [77/120    avg_loss:0.143, val_acc:0.939]
Epoch [78/120    avg_loss:0.103, val_acc:0.955]
Epoch [79/120    avg_loss:0.111, val_acc:0.965]
Epoch [80/120    avg_loss:0.063, val_acc:0.956]
Epoch [81/120    avg_loss:0.067, val_acc:0.969]
Epoch [82/120    avg_loss:0.025, val_acc:0.969]
Epoch [83/120    avg_loss:0.040, val_acc:0.972]
Epoch [84/120    avg_loss:0.029, val_acc:0.975]
Epoch [85/120    avg_loss:0.015, val_acc:0.976]
Epoch [86/120    avg_loss:0.012, val_acc:0.978]
Epoch [87/120    avg_loss:0.019, val_acc:0.972]
Epoch [88/120    avg_loss:0.016, val_acc:0.974]
Epoch [89/120    avg_loss:0.011, val_acc:0.975]
Epoch [90/120    avg_loss:0.009, val_acc:0.976]
Epoch [91/120    avg_loss:0.010, val_acc:0.979]
Epoch [92/120    avg_loss:0.010, val_acc:0.980]
Epoch [93/120    avg_loss:0.008, val_acc:0.981]
Epoch [94/120    avg_loss:0.005, val_acc:0.980]
Epoch [95/120    avg_loss:0.006, val_acc:0.979]
Epoch [96/120    avg_loss:0.008, val_acc:0.979]
Epoch [97/120    avg_loss:0.008, val_acc:0.979]
Epoch [98/120    avg_loss:0.008, val_acc:0.979]
Epoch [99/120    avg_loss:0.007, val_acc:0.980]
Epoch [100/120    avg_loss:0.008, val_acc:0.980]
Epoch [101/120    avg_loss:0.005, val_acc:0.980]
Epoch [102/120    avg_loss:0.009, val_acc:0.980]
Epoch [103/120    avg_loss:0.007, val_acc:0.980]
Epoch [104/120    avg_loss:0.007, val_acc:0.980]
Epoch [105/120    avg_loss:0.005, val_acc:0.981]
Epoch [106/120    avg_loss:0.006, val_acc:0.980]
Epoch [107/120    avg_loss:0.006, val_acc:0.980]
Epoch [108/120    avg_loss:0.012, val_acc:0.981]
Epoch [109/120    avg_loss:0.006, val_acc:0.981]
Epoch [110/120    avg_loss:0.005, val_acc:0.981]
Epoch [111/120    avg_loss:0.006, val_acc:0.981]
Epoch [112/120    avg_loss:0.006, val_acc:0.981]
Epoch [113/120    avg_loss:0.009, val_acc:0.981]
Epoch [114/120    avg_loss:0.007, val_acc:0.981]
Epoch [115/120    avg_loss:0.005, val_acc:0.981]
Epoch [116/120    avg_loss:0.008, val_acc:0.981]
Epoch [117/120    avg_loss:0.006, val_acc:0.981]
Epoch [118/120    avg_loss:0.008, val_acc:0.981]
Epoch [119/120    avg_loss:0.007, val_acc:0.981]
Epoch [120/120    avg_loss:0.007, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1274    4    0    1    0    0    0    0    5    1    0    0
     0    0    0]
 [   0    0    0  706    0   17    0    0    0    3    2    0   17    1
     1    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    3    2    0    0    0  860    4    3    0
     0    1    0]
 [   0    0   15    0    0    2    1    0    0    0    2 2189    0    1
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    3    0  525    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
     9  328    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
98.66666666666667

F1 scores:
[       nan 1.         0.98913043 0.96844993 0.99764706 0.9698324
 0.98715042 1.         1.         0.9        0.98398169 0.99341956
 0.97312326 0.99462366 0.99432066 0.9704142  0.98823529]

Kappa:
0.984802926431284
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:24--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0ffdc75710>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.368, val_acc:0.478]
Epoch [2/120    avg_loss:1.736, val_acc:0.579]
Epoch [3/120    avg_loss:1.415, val_acc:0.639]
Epoch [4/120    avg_loss:1.044, val_acc:0.654]
Epoch [5/120    avg_loss:0.915, val_acc:0.744]
Epoch [6/120    avg_loss:0.765, val_acc:0.751]
Epoch [7/120    avg_loss:0.565, val_acc:0.813]
Epoch [8/120    avg_loss:0.643, val_acc:0.751]
Epoch [9/120    avg_loss:0.589, val_acc:0.810]
Epoch [10/120    avg_loss:0.407, val_acc:0.877]
Epoch [11/120    avg_loss:0.349, val_acc:0.811]
Epoch [12/120    avg_loss:0.315, val_acc:0.888]
Epoch [13/120    avg_loss:0.270, val_acc:0.870]
Epoch [14/120    avg_loss:0.193, val_acc:0.915]
Epoch [15/120    avg_loss:0.183, val_acc:0.904]
Epoch [16/120    avg_loss:0.195, val_acc:0.922]
Epoch [17/120    avg_loss:0.127, val_acc:0.912]
Epoch [18/120    avg_loss:0.131, val_acc:0.909]
Epoch [19/120    avg_loss:0.085, val_acc:0.929]
Epoch [20/120    avg_loss:0.108, val_acc:0.942]
Epoch [21/120    avg_loss:0.095, val_acc:0.942]
Epoch [22/120    avg_loss:0.094, val_acc:0.944]
Epoch [23/120    avg_loss:0.075, val_acc:0.938]
Epoch [24/120    avg_loss:0.061, val_acc:0.940]
Epoch [25/120    avg_loss:0.074, val_acc:0.946]
Epoch [26/120    avg_loss:0.099, val_acc:0.954]
Epoch [27/120    avg_loss:0.068, val_acc:0.927]
Epoch [28/120    avg_loss:0.084, val_acc:0.933]
Epoch [29/120    avg_loss:0.069, val_acc:0.922]
Epoch [30/120    avg_loss:0.070, val_acc:0.938]
Epoch [31/120    avg_loss:0.050, val_acc:0.944]
Epoch [32/120    avg_loss:0.057, val_acc:0.948]
Epoch [33/120    avg_loss:0.036, val_acc:0.962]
Epoch [34/120    avg_loss:0.031, val_acc:0.968]
Epoch [35/120    avg_loss:0.027, val_acc:0.968]
Epoch [36/120    avg_loss:0.020, val_acc:0.956]
Epoch [37/120    avg_loss:0.047, val_acc:0.951]
Epoch [38/120    avg_loss:0.052, val_acc:0.959]
Epoch [39/120    avg_loss:0.027, val_acc:0.953]
Epoch [40/120    avg_loss:0.039, val_acc:0.960]
Epoch [41/120    avg_loss:0.034, val_acc:0.965]
Epoch [42/120    avg_loss:0.038, val_acc:0.968]
Epoch [43/120    avg_loss:0.049, val_acc:0.960]
Epoch [44/120    avg_loss:0.057, val_acc:0.966]
Epoch [45/120    avg_loss:0.035, val_acc:0.968]
Epoch [46/120    avg_loss:0.028, val_acc:0.970]
Epoch [47/120    avg_loss:0.018, val_acc:0.969]
Epoch [48/120    avg_loss:0.017, val_acc:0.970]
Epoch [49/120    avg_loss:0.023, val_acc:0.972]
Epoch [50/120    avg_loss:0.016, val_acc:0.967]
Epoch [51/120    avg_loss:0.048, val_acc:0.957]
Epoch [52/120    avg_loss:0.033, val_acc:0.972]
Epoch [53/120    avg_loss:0.026, val_acc:0.964]
Epoch [54/120    avg_loss:0.020, val_acc:0.966]
Epoch [55/120    avg_loss:0.023, val_acc:0.971]
Epoch [56/120    avg_loss:0.012, val_acc:0.968]
Epoch [57/120    avg_loss:0.017, val_acc:0.966]
Epoch [58/120    avg_loss:0.085, val_acc:0.963]
Epoch [59/120    avg_loss:0.076, val_acc:0.964]
Epoch [60/120    avg_loss:0.030, val_acc:0.972]
Epoch [61/120    avg_loss:0.029, val_acc:0.971]
Epoch [62/120    avg_loss:0.017, val_acc:0.968]
Epoch [63/120    avg_loss:0.012, val_acc:0.973]
Epoch [64/120    avg_loss:0.013, val_acc:0.964]
Epoch [65/120    avg_loss:0.014, val_acc:0.979]
Epoch [66/120    avg_loss:0.015, val_acc:0.978]
Epoch [67/120    avg_loss:0.034, val_acc:0.968]
Epoch [68/120    avg_loss:0.027, val_acc:0.969]
Epoch [69/120    avg_loss:0.012, val_acc:0.976]
Epoch [70/120    avg_loss:0.013, val_acc:0.971]
Epoch [71/120    avg_loss:0.012, val_acc:0.976]
Epoch [72/120    avg_loss:0.012, val_acc:0.972]
Epoch [73/120    avg_loss:0.013, val_acc:0.968]
Epoch [74/120    avg_loss:0.014, val_acc:0.968]
Epoch [75/120    avg_loss:0.010, val_acc:0.980]
Epoch [76/120    avg_loss:0.016, val_acc:0.973]
Epoch [77/120    avg_loss:0.019, val_acc:0.969]
Epoch [78/120    avg_loss:0.011, val_acc:0.972]
Epoch [79/120    avg_loss:0.013, val_acc:0.968]
Epoch [80/120    avg_loss:0.008, val_acc:0.973]
Epoch [81/120    avg_loss:0.007, val_acc:0.975]
Epoch [82/120    avg_loss:0.006, val_acc:0.982]
Epoch [83/120    avg_loss:0.004, val_acc:0.979]
Epoch [84/120    avg_loss:0.004, val_acc:0.980]
Epoch [85/120    avg_loss:0.007, val_acc:0.964]
Epoch [86/120    avg_loss:0.005, val_acc:0.980]
Epoch [87/120    avg_loss:0.006, val_acc:0.981]
Epoch [88/120    avg_loss:0.006, val_acc:0.972]
Epoch [89/120    avg_loss:0.004, val_acc:0.976]
Epoch [90/120    avg_loss:0.003, val_acc:0.981]
Epoch [91/120    avg_loss:0.007, val_acc:0.962]
Epoch [92/120    avg_loss:0.009, val_acc:0.975]
Epoch [93/120    avg_loss:0.029, val_acc:0.955]
Epoch [94/120    avg_loss:0.011, val_acc:0.968]
Epoch [95/120    avg_loss:0.005, val_acc:0.971]
Epoch [96/120    avg_loss:0.005, val_acc:0.973]
Epoch [97/120    avg_loss:0.009, val_acc:0.973]
Epoch [98/120    avg_loss:0.003, val_acc:0.976]
Epoch [99/120    avg_loss:0.005, val_acc:0.976]
Epoch [100/120    avg_loss:0.004, val_acc:0.976]
Epoch [101/120    avg_loss:0.005, val_acc:0.974]
Epoch [102/120    avg_loss:0.005, val_acc:0.975]
Epoch [103/120    avg_loss:0.004, val_acc:0.976]
Epoch [104/120    avg_loss:0.005, val_acc:0.976]
Epoch [105/120    avg_loss:0.005, val_acc:0.976]
Epoch [106/120    avg_loss:0.005, val_acc:0.976]
Epoch [107/120    avg_loss:0.003, val_acc:0.976]
Epoch [108/120    avg_loss:0.004, val_acc:0.976]
Epoch [109/120    avg_loss:0.003, val_acc:0.976]
Epoch [110/120    avg_loss:0.005, val_acc:0.976]
Epoch [111/120    avg_loss:0.003, val_acc:0.976]
Epoch [112/120    avg_loss:0.005, val_acc:0.976]
Epoch [113/120    avg_loss:0.004, val_acc:0.976]
Epoch [114/120    avg_loss:0.004, val_acc:0.976]
Epoch [115/120    avg_loss:0.004, val_acc:0.976]
Epoch [116/120    avg_loss:0.004, val_acc:0.976]
Epoch [117/120    avg_loss:0.003, val_acc:0.976]
Epoch [118/120    avg_loss:0.003, val_acc:0.976]
Epoch [119/120    avg_loss:0.004, val_acc:0.976]
Epoch [120/120    avg_loss:0.003, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1265    4    6    0    2    0    0    0    3    2    3    0
     0    0    0]
 [   0    0    0  709    0   11    0    0    0    3    1    0   22    1
     0    0    0]
 [   0    0    0    3  210    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0  847   11    5    0
     3    4    0]
 [   0    0    6    0    0    3    1    0    0    0    3 2196    0    1
     0    0    0]
 [   0    0    0    0    2    0    0    0    0    0    0    0  529    0
     1    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1136    2    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    23  314    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.29810298102981

F1 scores:
[       nan 0.98765432 0.98982786 0.9665985  0.97447796 0.9751693
 0.99020347 0.98039216 0.99883586 0.75675676 0.97862507 0.99389002
 0.96357013 0.99462366 0.98696785 0.94152924 0.96385542]

Kappa:
0.9805985616177533
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f336580f710>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.336, val_acc:0.411]
Epoch [2/120    avg_loss:1.638, val_acc:0.607]
Epoch [3/120    avg_loss:1.201, val_acc:0.690]
Epoch [4/120    avg_loss:1.037, val_acc:0.724]
Epoch [5/120    avg_loss:0.814, val_acc:0.776]
Epoch [6/120    avg_loss:0.608, val_acc:0.803]
Epoch [7/120    avg_loss:0.479, val_acc:0.816]
Epoch [8/120    avg_loss:0.435, val_acc:0.825]
Epoch [9/120    avg_loss:0.409, val_acc:0.892]
Epoch [10/120    avg_loss:0.351, val_acc:0.900]
Epoch [11/120    avg_loss:0.564, val_acc:0.823]
Epoch [12/120    avg_loss:0.319, val_acc:0.885]
Epoch [13/120    avg_loss:0.267, val_acc:0.896]
Epoch [14/120    avg_loss:0.278, val_acc:0.846]
Epoch [15/120    avg_loss:0.160, val_acc:0.929]
Epoch [16/120    avg_loss:0.158, val_acc:0.937]
Epoch [17/120    avg_loss:0.102, val_acc:0.928]
Epoch [18/120    avg_loss:0.110, val_acc:0.928]
Epoch [19/120    avg_loss:0.119, val_acc:0.925]
Epoch [20/120    avg_loss:0.118, val_acc:0.952]
Epoch [21/120    avg_loss:0.085, val_acc:0.957]
Epoch [22/120    avg_loss:0.078, val_acc:0.962]
Epoch [23/120    avg_loss:0.072, val_acc:0.933]
Epoch [24/120    avg_loss:0.076, val_acc:0.955]
Epoch [25/120    avg_loss:0.064, val_acc:0.964]
Epoch [26/120    avg_loss:0.053, val_acc:0.963]
Epoch [27/120    avg_loss:0.050, val_acc:0.955]
Epoch [28/120    avg_loss:0.066, val_acc:0.938]
Epoch [29/120    avg_loss:0.079, val_acc:0.960]
Epoch [30/120    avg_loss:0.066, val_acc:0.957]
Epoch [31/120    avg_loss:0.049, val_acc:0.951]
Epoch [32/120    avg_loss:0.045, val_acc:0.970]
Epoch [33/120    avg_loss:0.043, val_acc:0.970]
Epoch [34/120    avg_loss:0.049, val_acc:0.963]
Epoch [35/120    avg_loss:0.051, val_acc:0.962]
Epoch [36/120    avg_loss:0.036, val_acc:0.976]
Epoch [37/120    avg_loss:0.027, val_acc:0.978]
Epoch [38/120    avg_loss:0.031, val_acc:0.966]
Epoch [39/120    avg_loss:0.035, val_acc:0.982]
Epoch [40/120    avg_loss:0.076, val_acc:0.968]
Epoch [41/120    avg_loss:0.034, val_acc:0.980]
Epoch [42/120    avg_loss:0.027, val_acc:0.971]
Epoch [43/120    avg_loss:0.017, val_acc:0.978]
Epoch [44/120    avg_loss:0.029, val_acc:0.974]
Epoch [45/120    avg_loss:0.019, val_acc:0.982]
Epoch [46/120    avg_loss:0.024, val_acc:0.979]
Epoch [47/120    avg_loss:0.029, val_acc:0.967]
Epoch [48/120    avg_loss:0.043, val_acc:0.959]
Epoch [49/120    avg_loss:0.021, val_acc:0.978]
Epoch [50/120    avg_loss:0.017, val_acc:0.978]
Epoch [51/120    avg_loss:0.016, val_acc:0.983]
Epoch [52/120    avg_loss:0.011, val_acc:0.982]
Epoch [53/120    avg_loss:0.016, val_acc:0.971]
Epoch [54/120    avg_loss:0.014, val_acc:0.982]
Epoch [55/120    avg_loss:0.018, val_acc:0.987]
Epoch [56/120    avg_loss:0.019, val_acc:0.970]
Epoch [57/120    avg_loss:0.013, val_acc:0.973]
Epoch [58/120    avg_loss:0.012, val_acc:0.987]
Epoch [59/120    avg_loss:0.015, val_acc:0.980]
Epoch [60/120    avg_loss:0.021, val_acc:0.983]
Epoch [61/120    avg_loss:0.017, val_acc:0.983]
Epoch [62/120    avg_loss:0.016, val_acc:0.982]
Epoch [63/120    avg_loss:0.014, val_acc:0.971]
Epoch [64/120    avg_loss:0.178, val_acc:0.926]
Epoch [65/120    avg_loss:0.145, val_acc:0.941]
Epoch [66/120    avg_loss:0.066, val_acc:0.967]
Epoch [67/120    avg_loss:0.047, val_acc:0.902]
Epoch [68/120    avg_loss:0.045, val_acc:0.969]
Epoch [69/120    avg_loss:0.041, val_acc:0.947]
Epoch [70/120    avg_loss:0.039, val_acc:0.972]
Epoch [71/120    avg_loss:0.022, val_acc:0.966]
Epoch [72/120    avg_loss:0.022, val_acc:0.971]
Epoch [73/120    avg_loss:0.016, val_acc:0.974]
Epoch [74/120    avg_loss:0.019, val_acc:0.979]
Epoch [75/120    avg_loss:0.010, val_acc:0.980]
Epoch [76/120    avg_loss:0.011, val_acc:0.979]
Epoch [77/120    avg_loss:0.013, val_acc:0.979]
Epoch [78/120    avg_loss:0.008, val_acc:0.980]
Epoch [79/120    avg_loss:0.009, val_acc:0.981]
Epoch [80/120    avg_loss:0.011, val_acc:0.981]
Epoch [81/120    avg_loss:0.008, val_acc:0.981]
Epoch [82/120    avg_loss:0.009, val_acc:0.982]
Epoch [83/120    avg_loss:0.009, val_acc:0.980]
Epoch [84/120    avg_loss:0.013, val_acc:0.980]
Epoch [85/120    avg_loss:0.007, val_acc:0.979]
Epoch [86/120    avg_loss:0.012, val_acc:0.981]
Epoch [87/120    avg_loss:0.007, val_acc:0.980]
Epoch [88/120    avg_loss:0.014, val_acc:0.980]
Epoch [89/120    avg_loss:0.013, val_acc:0.981]
Epoch [90/120    avg_loss:0.010, val_acc:0.980]
Epoch [91/120    avg_loss:0.012, val_acc:0.981]
Epoch [92/120    avg_loss:0.009, val_acc:0.981]
Epoch [93/120    avg_loss:0.006, val_acc:0.981]
Epoch [94/120    avg_loss:0.007, val_acc:0.981]
Epoch [95/120    avg_loss:0.012, val_acc:0.981]
Epoch [96/120    avg_loss:0.012, val_acc:0.982]
Epoch [97/120    avg_loss:0.014, val_acc:0.982]
Epoch [98/120    avg_loss:0.007, val_acc:0.982]
Epoch [99/120    avg_loss:0.009, val_acc:0.982]
Epoch [100/120    avg_loss:0.010, val_acc:0.982]
Epoch [101/120    avg_loss:0.010, val_acc:0.982]
Epoch [102/120    avg_loss:0.007, val_acc:0.982]
Epoch [103/120    avg_loss:0.014, val_acc:0.982]
Epoch [104/120    avg_loss:0.010, val_acc:0.982]
Epoch [105/120    avg_loss:0.011, val_acc:0.982]
Epoch [106/120    avg_loss:0.008, val_acc:0.982]
Epoch [107/120    avg_loss:0.008, val_acc:0.982]
Epoch [108/120    avg_loss:0.008, val_acc:0.982]
Epoch [109/120    avg_loss:0.014, val_acc:0.982]
Epoch [110/120    avg_loss:0.008, val_acc:0.982]
Epoch [111/120    avg_loss:0.010, val_acc:0.982]
Epoch [112/120    avg_loss:0.007, val_acc:0.982]
Epoch [113/120    avg_loss:0.009, val_acc:0.982]
Epoch [114/120    avg_loss:0.007, val_acc:0.982]
Epoch [115/120    avg_loss:0.008, val_acc:0.982]
Epoch [116/120    avg_loss:0.010, val_acc:0.982]
Epoch [117/120    avg_loss:0.007, val_acc:0.982]
Epoch [118/120    avg_loss:0.009, val_acc:0.982]
Epoch [119/120    avg_loss:0.011, val_acc:0.982]
Epoch [120/120    avg_loss:0.007, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1275    0    0    0    1    0    0    0    6    2    1    0
     0    0    0]
 [   0    0    0  708    0   20    0    0    0    6    2    0    9    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   13    0    0    8    0    0    0    0  838    6    6    0
     1    3    0]
 [   0    0   14    0    0    3    3    0    0    0    4 2184    0    2
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    0    0  527    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1135    2    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    16  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.17886178861788

F1 scores:
[       nan 0.98765432 0.98569772 0.97319588 1.         0.9578714
 0.97986577 1.         0.99883856 0.81818182 0.97046902 0.99227624
 0.9768304  0.98930481 0.9904014  0.93333333 0.9704142 ]

Kappa:
0.9792440875615581
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:39
Validation dataloader:39
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f832232d710>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.329, val_acc:0.512]
Epoch [2/120    avg_loss:1.582, val_acc:0.592]
Epoch [3/120    avg_loss:1.311, val_acc:0.640]
Epoch [4/120    avg_loss:1.303, val_acc:0.753]
Epoch [5/120    avg_loss:0.968, val_acc:0.759]
Epoch [6/120    avg_loss:0.895, val_acc:0.751]
Epoch [7/120    avg_loss:0.801, val_acc:0.771]
Epoch [8/120    avg_loss:0.715, val_acc:0.793]
Epoch [9/120    avg_loss:0.583, val_acc:0.858]
Epoch [10/120    avg_loss:0.431, val_acc:0.873]
Epoch [11/120    avg_loss:0.262, val_acc:0.906]
Epoch [12/120    avg_loss:0.295, val_acc:0.906]
Epoch [13/120    avg_loss:0.306, val_acc:0.902]
Epoch [14/120    avg_loss:0.187, val_acc:0.932]
Epoch [15/120    avg_loss:0.138, val_acc:0.931]
Epoch [16/120    avg_loss:0.153, val_acc:0.939]
Epoch [17/120    avg_loss:0.115, val_acc:0.948]
Epoch [18/120    avg_loss:0.135, val_acc:0.941]
Epoch [19/120    avg_loss:0.109, val_acc:0.958]
Epoch [20/120    avg_loss:0.135, val_acc:0.924]
Epoch [21/120    avg_loss:0.080, val_acc:0.946]
Epoch [22/120    avg_loss:0.087, val_acc:0.952]
Epoch [23/120    avg_loss:0.080, val_acc:0.938]
Epoch [24/120    avg_loss:0.062, val_acc:0.970]
Epoch [25/120    avg_loss:0.042, val_acc:0.967]
Epoch [26/120    avg_loss:0.069, val_acc:0.960]
Epoch [27/120    avg_loss:0.078, val_acc:0.959]
Epoch [28/120    avg_loss:0.069, val_acc:0.966]
Epoch [29/120    avg_loss:0.054, val_acc:0.967]
Epoch [30/120    avg_loss:0.171, val_acc:0.925]
Epoch [31/120    avg_loss:0.084, val_acc:0.933]
Epoch [32/120    avg_loss:0.052, val_acc:0.962]
Epoch [33/120    avg_loss:0.039, val_acc:0.969]
Epoch [34/120    avg_loss:0.055, val_acc:0.959]
Epoch [35/120    avg_loss:0.039, val_acc:0.960]
Epoch [36/120    avg_loss:0.040, val_acc:0.966]
Epoch [37/120    avg_loss:0.027, val_acc:0.973]
Epoch [38/120    avg_loss:0.033, val_acc:0.965]
Epoch [39/120    avg_loss:0.081, val_acc:0.957]
Epoch [40/120    avg_loss:0.053, val_acc:0.946]
Epoch [41/120    avg_loss:0.053, val_acc:0.965]
Epoch [42/120    avg_loss:0.033, val_acc:0.966]
Epoch [43/120    avg_loss:0.057, val_acc:0.949]
Epoch [44/120    avg_loss:0.040, val_acc:0.972]
Epoch [45/120    avg_loss:0.032, val_acc:0.972]
Epoch [46/120    avg_loss:0.018, val_acc:0.979]
Epoch [47/120    avg_loss:0.018, val_acc:0.985]
Epoch [48/120    avg_loss:0.022, val_acc:0.970]
Epoch [49/120    avg_loss:0.019, val_acc:0.976]
Epoch [50/120    avg_loss:0.017, val_acc:0.979]
Epoch [51/120    avg_loss:0.014, val_acc:0.982]
Epoch [52/120    avg_loss:0.012, val_acc:0.985]
Epoch [53/120    avg_loss:0.015, val_acc:0.976]
Epoch [54/120    avg_loss:0.025, val_acc:0.974]
Epoch [55/120    avg_loss:0.017, val_acc:0.967]
Epoch [56/120    avg_loss:0.020, val_acc:0.979]
Epoch [57/120    avg_loss:0.021, val_acc:0.980]
Epoch [58/120    avg_loss:0.016, val_acc:0.983]
Epoch [59/120    avg_loss:0.010, val_acc:0.983]
Epoch [60/120    avg_loss:0.014, val_acc:0.971]
Epoch [61/120    avg_loss:0.018, val_acc:0.983]
Epoch [62/120    avg_loss:0.015, val_acc:0.980]
Epoch [63/120    avg_loss:0.019, val_acc:0.966]
Epoch [64/120    avg_loss:0.043, val_acc:0.978]
Epoch [65/120    avg_loss:0.050, val_acc:0.984]
Epoch [66/120    avg_loss:0.020, val_acc:0.984]
Epoch [67/120    avg_loss:0.012, val_acc:0.985]
Epoch [68/120    avg_loss:0.018, val_acc:0.993]
Epoch [69/120    avg_loss:0.009, val_acc:0.991]
Epoch [70/120    avg_loss:0.011, val_acc:0.991]
Epoch [71/120    avg_loss:0.011, val_acc:0.988]
Epoch [72/120    avg_loss:0.011, val_acc:0.989]
Epoch [73/120    avg_loss:0.013, val_acc:0.990]
Epoch [74/120    avg_loss:0.012, val_acc:0.990]
Epoch [75/120    avg_loss:0.011, val_acc:0.989]
Epoch [76/120    avg_loss:0.009, val_acc:0.990]
Epoch [77/120    avg_loss:0.011, val_acc:0.991]
Epoch [78/120    avg_loss:0.008, val_acc:0.991]
Epoch [79/120    avg_loss:0.008, val_acc:0.991]
Epoch [80/120    avg_loss:0.010, val_acc:0.990]
Epoch [81/120    avg_loss:0.016, val_acc:0.991]
Epoch [82/120    avg_loss:0.006, val_acc:0.991]
Epoch [83/120    avg_loss:0.008, val_acc:0.991]
Epoch [84/120    avg_loss:0.011, val_acc:0.991]
Epoch [85/120    avg_loss:0.017, val_acc:0.991]
Epoch [86/120    avg_loss:0.008, val_acc:0.991]
Epoch [87/120    avg_loss:0.007, val_acc:0.991]
Epoch [88/120    avg_loss:0.009, val_acc:0.991]
Epoch [89/120    avg_loss:0.007, val_acc:0.991]
Epoch [90/120    avg_loss:0.009, val_acc:0.991]
Epoch [91/120    avg_loss:0.007, val_acc:0.991]
Epoch [92/120    avg_loss:0.007, val_acc:0.991]
Epoch [93/120    avg_loss:0.009, val_acc:0.991]
Epoch [94/120    avg_loss:0.009, val_acc:0.991]
Epoch [95/120    avg_loss:0.010, val_acc:0.991]
Epoch [96/120    avg_loss:0.007, val_acc:0.991]
Epoch [97/120    avg_loss:0.008, val_acc:0.991]
Epoch [98/120    avg_loss:0.007, val_acc:0.991]
Epoch [99/120    avg_loss:0.008, val_acc:0.991]
Epoch [100/120    avg_loss:0.007, val_acc:0.991]
Epoch [101/120    avg_loss:0.010, val_acc:0.991]
Epoch [102/120    avg_loss:0.012, val_acc:0.991]
Epoch [103/120    avg_loss:0.008, val_acc:0.991]
Epoch [104/120    avg_loss:0.014, val_acc:0.991]
Epoch [105/120    avg_loss:0.009, val_acc:0.991]
Epoch [106/120    avg_loss:0.007, val_acc:0.991]
Epoch [107/120    avg_loss:0.013, val_acc:0.991]
Epoch [108/120    avg_loss:0.008, val_acc:0.991]
Epoch [109/120    avg_loss:0.008, val_acc:0.991]
Epoch [110/120    avg_loss:0.009, val_acc:0.991]
Epoch [111/120    avg_loss:0.012, val_acc:0.991]
Epoch [112/120    avg_loss:0.009, val_acc:0.991]
Epoch [113/120    avg_loss:0.011, val_acc:0.991]
Epoch [114/120    avg_loss:0.007, val_acc:0.991]
Epoch [115/120    avg_loss:0.009, val_acc:0.991]
Epoch [116/120    avg_loss:0.010, val_acc:0.991]
Epoch [117/120    avg_loss:0.008, val_acc:0.991]
Epoch [118/120    avg_loss:0.008, val_acc:0.991]
Epoch [119/120    avg_loss:0.007, val_acc:0.991]
Epoch [120/120    avg_loss:0.010, val_acc:0.991]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1270    1    1    0    1    0    0    0    5    6    1    0
     0    0    0]
 [   0    0    1  710    0   15    0    0    0    6    0    0   13    2
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    4    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    0    0    0   10    1    0    0    0  850   13    0    0
     0    1    0]
 [   0    0    3    0    0    0    2    0    0    0    6 2194    2    1
     2    0    0]
 [   0    0    0    0    0   14    0    0    0    0    0    1  517    0
     0    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    2    0    0    0
  1131    0    0]
 [   0    0    0    0    0    0   34    0    0    0    0    0    0    0
     0  313    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
98.09214092140921

F1 scores:
[       nan 0.975      0.99257522 0.97127223 0.99294118 0.94273128
 0.96960712 0.92592593 0.99650757 0.74418605 0.97701149 0.99119042
 0.96275605 0.98924731 0.99559859 0.94704992 0.95757576]

Kappa:
0.9782507884491057
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1bfea7d630>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.203, val_acc:0.478]
Epoch [2/120    avg_loss:1.796, val_acc:0.521]
Epoch [3/120    avg_loss:1.336, val_acc:0.661]
Epoch [4/120    avg_loss:1.165, val_acc:0.707]
Epoch [5/120    avg_loss:1.000, val_acc:0.672]
Epoch [6/120    avg_loss:0.754, val_acc:0.772]
Epoch [7/120    avg_loss:0.833, val_acc:0.747]
Epoch [8/120    avg_loss:0.554, val_acc:0.807]
Epoch [9/120    avg_loss:0.580, val_acc:0.802]
Epoch [10/120    avg_loss:0.523, val_acc:0.794]
Epoch [11/120    avg_loss:0.450, val_acc:0.776]
Epoch [12/120    avg_loss:0.398, val_acc:0.853]
Epoch [13/120    avg_loss:0.365, val_acc:0.839]
Epoch [14/120    avg_loss:0.366, val_acc:0.815]
Epoch [15/120    avg_loss:0.328, val_acc:0.880]
Epoch [16/120    avg_loss:0.227, val_acc:0.901]
Epoch [17/120    avg_loss:0.183, val_acc:0.904]
Epoch [18/120    avg_loss:0.170, val_acc:0.916]
Epoch [19/120    avg_loss:0.187, val_acc:0.899]
Epoch [20/120    avg_loss:0.123, val_acc:0.919]
Epoch [21/120    avg_loss:0.140, val_acc:0.920]
Epoch [22/120    avg_loss:0.120, val_acc:0.913]
Epoch [23/120    avg_loss:0.132, val_acc:0.912]
Epoch [24/120    avg_loss:0.098, val_acc:0.929]
Epoch [25/120    avg_loss:0.067, val_acc:0.930]
Epoch [26/120    avg_loss:0.084, val_acc:0.918]
Epoch [27/120    avg_loss:0.125, val_acc:0.910]
Epoch [28/120    avg_loss:0.130, val_acc:0.928]
Epoch [29/120    avg_loss:0.112, val_acc:0.933]
Epoch [30/120    avg_loss:0.102, val_acc:0.928]
Epoch [31/120    avg_loss:0.073, val_acc:0.939]
Epoch [32/120    avg_loss:0.073, val_acc:0.948]
Epoch [33/120    avg_loss:0.046, val_acc:0.946]
Epoch [34/120    avg_loss:0.049, val_acc:0.935]
Epoch [35/120    avg_loss:0.094, val_acc:0.922]
Epoch [36/120    avg_loss:0.131, val_acc:0.916]
Epoch [37/120    avg_loss:0.084, val_acc:0.947]
Epoch [38/120    avg_loss:0.065, val_acc:0.956]
Epoch [39/120    avg_loss:0.059, val_acc:0.947]
Epoch [40/120    avg_loss:0.046, val_acc:0.956]
Epoch [41/120    avg_loss:0.058, val_acc:0.965]
Epoch [42/120    avg_loss:0.042, val_acc:0.951]
Epoch [43/120    avg_loss:0.062, val_acc:0.951]
Epoch [44/120    avg_loss:0.095, val_acc:0.952]
Epoch [45/120    avg_loss:0.044, val_acc:0.967]
Epoch [46/120    avg_loss:0.050, val_acc:0.958]
Epoch [47/120    avg_loss:0.030, val_acc:0.962]
Epoch [48/120    avg_loss:0.078, val_acc:0.913]
Epoch [49/120    avg_loss:0.289, val_acc:0.901]
Epoch [50/120    avg_loss:0.197, val_acc:0.911]
Epoch [51/120    avg_loss:0.111, val_acc:0.928]
Epoch [52/120    avg_loss:0.084, val_acc:0.945]
Epoch [53/120    avg_loss:0.046, val_acc:0.959]
Epoch [54/120    avg_loss:0.042, val_acc:0.959]
Epoch [55/120    avg_loss:0.046, val_acc:0.968]
Epoch [56/120    avg_loss:0.040, val_acc:0.954]
Epoch [57/120    avg_loss:0.060, val_acc:0.951]
Epoch [58/120    avg_loss:0.044, val_acc:0.962]
Epoch [59/120    avg_loss:0.047, val_acc:0.959]
Epoch [60/120    avg_loss:0.025, val_acc:0.975]
Epoch [61/120    avg_loss:0.029, val_acc:0.975]
Epoch [62/120    avg_loss:0.024, val_acc:0.969]
Epoch [63/120    avg_loss:0.165, val_acc:0.843]
Epoch [64/120    avg_loss:0.207, val_acc:0.924]
Epoch [65/120    avg_loss:0.095, val_acc:0.953]
Epoch [66/120    avg_loss:0.055, val_acc:0.936]
Epoch [67/120    avg_loss:0.040, val_acc:0.954]
Epoch [68/120    avg_loss:0.057, val_acc:0.950]
Epoch [69/120    avg_loss:0.032, val_acc:0.961]
Epoch [70/120    avg_loss:0.037, val_acc:0.967]
Epoch [71/120    avg_loss:0.025, val_acc:0.964]
Epoch [72/120    avg_loss:0.024, val_acc:0.968]
Epoch [73/120    avg_loss:0.028, val_acc:0.964]
Epoch [74/120    avg_loss:0.023, val_acc:0.967]
Epoch [75/120    avg_loss:0.020, val_acc:0.970]
Epoch [76/120    avg_loss:0.014, val_acc:0.971]
Epoch [77/120    avg_loss:0.016, val_acc:0.974]
Epoch [78/120    avg_loss:0.014, val_acc:0.971]
Epoch [79/120    avg_loss:0.016, val_acc:0.974]
Epoch [80/120    avg_loss:0.012, val_acc:0.973]
Epoch [81/120    avg_loss:0.016, val_acc:0.974]
Epoch [82/120    avg_loss:0.010, val_acc:0.973]
Epoch [83/120    avg_loss:0.014, val_acc:0.974]
Epoch [84/120    avg_loss:0.011, val_acc:0.975]
Epoch [85/120    avg_loss:0.009, val_acc:0.974]
Epoch [86/120    avg_loss:0.012, val_acc:0.977]
Epoch [87/120    avg_loss:0.011, val_acc:0.978]
Epoch [88/120    avg_loss:0.012, val_acc:0.977]
Epoch [89/120    avg_loss:0.011, val_acc:0.978]
Epoch [90/120    avg_loss:0.012, val_acc:0.977]
Epoch [91/120    avg_loss:0.013, val_acc:0.978]
Epoch [92/120    avg_loss:0.013, val_acc:0.978]
Epoch [93/120    avg_loss:0.009, val_acc:0.979]
Epoch [94/120    avg_loss:0.014, val_acc:0.979]
Epoch [95/120    avg_loss:0.010, val_acc:0.979]
Epoch [96/120    avg_loss:0.015, val_acc:0.981]
Epoch [97/120    avg_loss:0.011, val_acc:0.979]
Epoch [98/120    avg_loss:0.011, val_acc:0.976]
Epoch [99/120    avg_loss:0.014, val_acc:0.977]
Epoch [100/120    avg_loss:0.009, val_acc:0.977]
Epoch [101/120    avg_loss:0.011, val_acc:0.978]
Epoch [102/120    avg_loss:0.014, val_acc:0.977]
Epoch [103/120    avg_loss:0.009, val_acc:0.978]
Epoch [104/120    avg_loss:0.009, val_acc:0.978]
Epoch [105/120    avg_loss:0.011, val_acc:0.979]
Epoch [106/120    avg_loss:0.011, val_acc:0.978]
Epoch [107/120    avg_loss:0.008, val_acc:0.980]
Epoch [108/120    avg_loss:0.009, val_acc:0.980]
Epoch [109/120    avg_loss:0.017, val_acc:0.978]
Epoch [110/120    avg_loss:0.009, val_acc:0.978]
Epoch [111/120    avg_loss:0.015, val_acc:0.978]
Epoch [112/120    avg_loss:0.007, val_acc:0.978]
Epoch [113/120    avg_loss:0.010, val_acc:0.978]
Epoch [114/120    avg_loss:0.010, val_acc:0.979]
Epoch [115/120    avg_loss:0.012, val_acc:0.979]
Epoch [116/120    avg_loss:0.009, val_acc:0.979]
Epoch [117/120    avg_loss:0.008, val_acc:0.979]
Epoch [118/120    avg_loss:0.011, val_acc:0.979]
Epoch [119/120    avg_loss:0.010, val_acc:0.979]
Epoch [120/120    avg_loss:0.011, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1259    0    0    0    1    0    0    0   17    5    3    0
     0    0    0]
 [   0    0    0  719    0   12    0    0    0    4    1    0   11    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    1    4    0    2    0    0    0    0  850   17    1    0
     0    0    0]
 [   0    0    8    0    0    0    6    0    0    0    7 2189    0    0
     0    0    0]
 [   0    0    0   10    4    1    0    0    0    0    1   10  505    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    3    0    0    0
  1131    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0   11
    61  274    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.6260162601626

F1 scores:
[       nan 0.98765432 0.98590446 0.97096556 0.99069767 0.96949153
 0.99470098 1.         1.         0.75555556 0.96921323 0.98803882
 0.95734597 0.97112861 0.97039897 0.88244767 0.97647059]

Kappa:
0.9729199425557358
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5562e536a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.319, val_acc:0.531]
Epoch [2/120    avg_loss:1.688, val_acc:0.518]
Epoch [3/120    avg_loss:1.380, val_acc:0.658]
Epoch [4/120    avg_loss:1.058, val_acc:0.741]
Epoch [5/120    avg_loss:0.909, val_acc:0.726]
Epoch [6/120    avg_loss:1.174, val_acc:0.728]
Epoch [7/120    avg_loss:0.768, val_acc:0.771]
Epoch [8/120    avg_loss:0.680, val_acc:0.777]
Epoch [9/120    avg_loss:0.515, val_acc:0.843]
Epoch [10/120    avg_loss:0.410, val_acc:0.882]
Epoch [11/120    avg_loss:0.348, val_acc:0.870]
Epoch [12/120    avg_loss:0.485, val_acc:0.876]
Epoch [13/120    avg_loss:0.424, val_acc:0.838]
Epoch [14/120    avg_loss:0.367, val_acc:0.876]
Epoch [15/120    avg_loss:0.261, val_acc:0.900]
Epoch [16/120    avg_loss:0.246, val_acc:0.884]
Epoch [17/120    avg_loss:0.219, val_acc:0.887]
Epoch [18/120    avg_loss:0.164, val_acc:0.911]
Epoch [19/120    avg_loss:0.178, val_acc:0.909]
Epoch [20/120    avg_loss:0.178, val_acc:0.935]
Epoch [21/120    avg_loss:0.278, val_acc:0.897]
Epoch [22/120    avg_loss:0.221, val_acc:0.924]
Epoch [23/120    avg_loss:0.165, val_acc:0.920]
Epoch [24/120    avg_loss:0.132, val_acc:0.931]
Epoch [25/120    avg_loss:0.116, val_acc:0.934]
Epoch [26/120    avg_loss:0.068, val_acc:0.938]
Epoch [27/120    avg_loss:0.080, val_acc:0.935]
Epoch [28/120    avg_loss:0.134, val_acc:0.934]
Epoch [29/120    avg_loss:0.117, val_acc:0.932]
Epoch [30/120    avg_loss:0.111, val_acc:0.921]
Epoch [31/120    avg_loss:0.079, val_acc:0.929]
Epoch [32/120    avg_loss:0.138, val_acc:0.918]
Epoch [33/120    avg_loss:0.110, val_acc:0.925]
Epoch [34/120    avg_loss:0.080, val_acc:0.947]
Epoch [35/120    avg_loss:0.236, val_acc:0.904]
Epoch [36/120    avg_loss:0.159, val_acc:0.930]
Epoch [37/120    avg_loss:0.092, val_acc:0.928]
Epoch [38/120    avg_loss:0.078, val_acc:0.955]
Epoch [39/120    avg_loss:0.063, val_acc:0.941]
Epoch [40/120    avg_loss:0.067, val_acc:0.948]
Epoch [41/120    avg_loss:0.068, val_acc:0.947]
Epoch [42/120    avg_loss:0.070, val_acc:0.950]
Epoch [43/120    avg_loss:0.059, val_acc:0.951]
Epoch [44/120    avg_loss:0.046, val_acc:0.952]
Epoch [45/120    avg_loss:0.042, val_acc:0.961]
Epoch [46/120    avg_loss:0.066, val_acc:0.940]
Epoch [47/120    avg_loss:0.066, val_acc:0.952]
Epoch [48/120    avg_loss:0.080, val_acc:0.955]
Epoch [49/120    avg_loss:0.056, val_acc:0.955]
Epoch [50/120    avg_loss:0.096, val_acc:0.943]
Epoch [51/120    avg_loss:0.048, val_acc:0.952]
Epoch [52/120    avg_loss:0.045, val_acc:0.965]
Epoch [53/120    avg_loss:0.027, val_acc:0.963]
Epoch [54/120    avg_loss:0.041, val_acc:0.944]
Epoch [55/120    avg_loss:0.069, val_acc:0.971]
Epoch [56/120    avg_loss:0.038, val_acc:0.957]
Epoch [57/120    avg_loss:0.022, val_acc:0.967]
Epoch [58/120    avg_loss:0.021, val_acc:0.939]
Epoch [59/120    avg_loss:0.042, val_acc:0.941]
Epoch [60/120    avg_loss:0.053, val_acc:0.967]
Epoch [61/120    avg_loss:0.033, val_acc:0.962]
Epoch [62/120    avg_loss:0.032, val_acc:0.961]
Epoch [63/120    avg_loss:0.027, val_acc:0.958]
Epoch [64/120    avg_loss:0.029, val_acc:0.969]
Epoch [65/120    avg_loss:0.019, val_acc:0.967]
Epoch [66/120    avg_loss:0.046, val_acc:0.956]
Epoch [67/120    avg_loss:0.029, val_acc:0.964]
Epoch [68/120    avg_loss:0.026, val_acc:0.969]
Epoch [69/120    avg_loss:0.018, val_acc:0.971]
Epoch [70/120    avg_loss:0.017, val_acc:0.973]
Epoch [71/120    avg_loss:0.020, val_acc:0.974]
Epoch [72/120    avg_loss:0.014, val_acc:0.974]
Epoch [73/120    avg_loss:0.012, val_acc:0.973]
Epoch [74/120    avg_loss:0.013, val_acc:0.973]
Epoch [75/120    avg_loss:0.019, val_acc:0.973]
Epoch [76/120    avg_loss:0.017, val_acc:0.974]
Epoch [77/120    avg_loss:0.013, val_acc:0.975]
Epoch [78/120    avg_loss:0.009, val_acc:0.975]
Epoch [79/120    avg_loss:0.009, val_acc:0.976]
Epoch [80/120    avg_loss:0.009, val_acc:0.975]
Epoch [81/120    avg_loss:0.008, val_acc:0.974]
Epoch [82/120    avg_loss:0.014, val_acc:0.973]
Epoch [83/120    avg_loss:0.009, val_acc:0.971]
Epoch [84/120    avg_loss:0.009, val_acc:0.971]
Epoch [85/120    avg_loss:0.007, val_acc:0.973]
Epoch [86/120    avg_loss:0.008, val_acc:0.974]
Epoch [87/120    avg_loss:0.008, val_acc:0.974]
Epoch [88/120    avg_loss:0.011, val_acc:0.973]
Epoch [89/120    avg_loss:0.012, val_acc:0.974]
Epoch [90/120    avg_loss:0.010, val_acc:0.974]
Epoch [91/120    avg_loss:0.010, val_acc:0.973]
Epoch [92/120    avg_loss:0.010, val_acc:0.970]
Epoch [93/120    avg_loss:0.007, val_acc:0.970]
Epoch [94/120    avg_loss:0.009, val_acc:0.971]
Epoch [95/120    avg_loss:0.008, val_acc:0.971]
Epoch [96/120    avg_loss:0.012, val_acc:0.970]
Epoch [97/120    avg_loss:0.010, val_acc:0.970]
Epoch [98/120    avg_loss:0.008, val_acc:0.970]
Epoch [99/120    avg_loss:0.009, val_acc:0.970]
Epoch [100/120    avg_loss:0.009, val_acc:0.970]
Epoch [101/120    avg_loss:0.010, val_acc:0.970]
Epoch [102/120    avg_loss:0.010, val_acc:0.970]
Epoch [103/120    avg_loss:0.008, val_acc:0.970]
Epoch [104/120    avg_loss:0.008, val_acc:0.970]
Epoch [105/120    avg_loss:0.009, val_acc:0.970]
Epoch [106/120    avg_loss:0.010, val_acc:0.970]
Epoch [107/120    avg_loss:0.007, val_acc:0.970]
Epoch [108/120    avg_loss:0.008, val_acc:0.970]
Epoch [109/120    avg_loss:0.009, val_acc:0.970]
Epoch [110/120    avg_loss:0.011, val_acc:0.970]
Epoch [111/120    avg_loss:0.014, val_acc:0.970]
Epoch [112/120    avg_loss:0.009, val_acc:0.970]
Epoch [113/120    avg_loss:0.009, val_acc:0.970]
Epoch [114/120    avg_loss:0.009, val_acc:0.970]
Epoch [115/120    avg_loss:0.010, val_acc:0.970]
Epoch [116/120    avg_loss:0.011, val_acc:0.970]
Epoch [117/120    avg_loss:0.009, val_acc:0.970]
Epoch [118/120    avg_loss:0.008, val_acc:0.970]
Epoch [119/120    avg_loss:0.009, val_acc:0.970]
Epoch [120/120    avg_loss:0.011, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1269    0    0    0    0    0    0    0    3    5    3    0
     0    5    0]
 [   0    0    0  707    0   22    0    0    0    5    1    0    9    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    5    0    0    0    0  842   22    0    0
     0    1    0]
 [   0    0    7    0    0    0   11    0    0    0   10 2180    0    2
     0    0    0]
 [   0    0    0   12    2    5    0    0    0    0    2    2  505    0
     2    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    2    0    0    0
  1131    0    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    2
    47  290    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.60433604336043

F1 scores:
[       nan 0.98765432 0.99101913 0.95994569 0.9953271  0.95343681
 0.98574644 0.98039216 0.99883856 0.74418605 0.97004608 0.98664856
 0.96098953 0.98143236 0.97542044 0.90202177 0.97674419]

Kappa:
0.9726846165403705
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f6f3b105710>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.212, val_acc:0.531]
Epoch [2/120    avg_loss:1.949, val_acc:0.578]
Epoch [3/120    avg_loss:1.303, val_acc:0.518]
Epoch [4/120    avg_loss:1.099, val_acc:0.673]
Epoch [5/120    avg_loss:0.818, val_acc:0.794]
Epoch [6/120    avg_loss:0.789, val_acc:0.771]
Epoch [7/120    avg_loss:0.674, val_acc:0.816]
Epoch [8/120    avg_loss:0.500, val_acc:0.796]
Epoch [9/120    avg_loss:0.493, val_acc:0.848]
Epoch [10/120    avg_loss:0.515, val_acc:0.827]
Epoch [11/120    avg_loss:0.415, val_acc:0.843]
Epoch [12/120    avg_loss:0.409, val_acc:0.867]
Epoch [13/120    avg_loss:0.293, val_acc:0.899]
Epoch [14/120    avg_loss:0.256, val_acc:0.893]
Epoch [15/120    avg_loss:0.206, val_acc:0.922]
Epoch [16/120    avg_loss:0.257, val_acc:0.911]
Epoch [17/120    avg_loss:0.224, val_acc:0.898]
Epoch [18/120    avg_loss:0.177, val_acc:0.894]
Epoch [19/120    avg_loss:0.157, val_acc:0.918]
Epoch [20/120    avg_loss:0.195, val_acc:0.904]
Epoch [21/120    avg_loss:0.146, val_acc:0.879]
Epoch [22/120    avg_loss:0.245, val_acc:0.917]
Epoch [23/120    avg_loss:0.186, val_acc:0.921]
Epoch [24/120    avg_loss:0.219, val_acc:0.896]
Epoch [25/120    avg_loss:0.153, val_acc:0.930]
Epoch [26/120    avg_loss:0.096, val_acc:0.944]
Epoch [27/120    avg_loss:0.099, val_acc:0.943]
Epoch [28/120    avg_loss:0.096, val_acc:0.940]
Epoch [29/120    avg_loss:0.092, val_acc:0.943]
Epoch [30/120    avg_loss:0.104, val_acc:0.935]
Epoch [31/120    avg_loss:0.068, val_acc:0.943]
Epoch [32/120    avg_loss:0.067, val_acc:0.945]
Epoch [33/120    avg_loss:0.067, val_acc:0.952]
Epoch [34/120    avg_loss:0.068, val_acc:0.955]
Epoch [35/120    avg_loss:0.054, val_acc:0.944]
Epoch [36/120    avg_loss:0.049, val_acc:0.962]
Epoch [37/120    avg_loss:0.065, val_acc:0.959]
Epoch [38/120    avg_loss:0.061, val_acc:0.944]
Epoch [39/120    avg_loss:0.055, val_acc:0.954]
Epoch [40/120    avg_loss:0.053, val_acc:0.962]
Epoch [41/120    avg_loss:0.039, val_acc:0.970]
Epoch [42/120    avg_loss:0.035, val_acc:0.961]
Epoch [43/120    avg_loss:0.053, val_acc:0.970]
Epoch [44/120    avg_loss:0.060, val_acc:0.965]
Epoch [45/120    avg_loss:0.054, val_acc:0.974]
Epoch [46/120    avg_loss:0.049, val_acc:0.948]
Epoch [47/120    avg_loss:0.052, val_acc:0.969]
Epoch [48/120    avg_loss:0.034, val_acc:0.971]
Epoch [49/120    avg_loss:0.027, val_acc:0.968]
Epoch [50/120    avg_loss:0.023, val_acc:0.971]
Epoch [51/120    avg_loss:0.029, val_acc:0.971]
Epoch [52/120    avg_loss:0.045, val_acc:0.970]
Epoch [53/120    avg_loss:0.030, val_acc:0.975]
Epoch [54/120    avg_loss:0.022, val_acc:0.973]
Epoch [55/120    avg_loss:0.022, val_acc:0.974]
Epoch [56/120    avg_loss:0.018, val_acc:0.979]
Epoch [57/120    avg_loss:0.045, val_acc:0.965]
Epoch [58/120    avg_loss:0.022, val_acc:0.967]
Epoch [59/120    avg_loss:0.036, val_acc:0.956]
Epoch [60/120    avg_loss:0.042, val_acc:0.968]
Epoch [61/120    avg_loss:0.022, val_acc:0.971]
Epoch [62/120    avg_loss:0.024, val_acc:0.974]
Epoch [63/120    avg_loss:0.023, val_acc:0.971]
Epoch [64/120    avg_loss:0.021, val_acc:0.971]
Epoch [65/120    avg_loss:0.024, val_acc:0.969]
Epoch [66/120    avg_loss:0.037, val_acc:0.967]
Epoch [67/120    avg_loss:0.041, val_acc:0.971]
Epoch [68/120    avg_loss:0.091, val_acc:0.948]
Epoch [69/120    avg_loss:0.127, val_acc:0.902]
Epoch [70/120    avg_loss:0.114, val_acc:0.938]
Epoch [71/120    avg_loss:0.060, val_acc:0.946]
Epoch [72/120    avg_loss:0.053, val_acc:0.961]
Epoch [73/120    avg_loss:0.046, val_acc:0.963]
Epoch [74/120    avg_loss:0.028, val_acc:0.968]
Epoch [75/120    avg_loss:0.046, val_acc:0.967]
Epoch [76/120    avg_loss:0.027, val_acc:0.966]
Epoch [77/120    avg_loss:0.031, val_acc:0.968]
Epoch [78/120    avg_loss:0.025, val_acc:0.970]
Epoch [79/120    avg_loss:0.027, val_acc:0.974]
Epoch [80/120    avg_loss:0.024, val_acc:0.974]
Epoch [81/120    avg_loss:0.022, val_acc:0.974]
Epoch [82/120    avg_loss:0.022, val_acc:0.971]
Epoch [83/120    avg_loss:0.022, val_acc:0.971]
Epoch [84/120    avg_loss:0.021, val_acc:0.971]
Epoch [85/120    avg_loss:0.024, val_acc:0.971]
Epoch [86/120    avg_loss:0.022, val_acc:0.971]
Epoch [87/120    avg_loss:0.021, val_acc:0.971]
Epoch [88/120    avg_loss:0.024, val_acc:0.971]
Epoch [89/120    avg_loss:0.022, val_acc:0.971]
Epoch [90/120    avg_loss:0.022, val_acc:0.971]
Epoch [91/120    avg_loss:0.025, val_acc:0.971]
Epoch [92/120    avg_loss:0.021, val_acc:0.973]
Epoch [93/120    avg_loss:0.018, val_acc:0.973]
Epoch [94/120    avg_loss:0.027, val_acc:0.973]
Epoch [95/120    avg_loss:0.022, val_acc:0.973]
Epoch [96/120    avg_loss:0.024, val_acc:0.973]
Epoch [97/120    avg_loss:0.023, val_acc:0.973]
Epoch [98/120    avg_loss:0.021, val_acc:0.973]
Epoch [99/120    avg_loss:0.028, val_acc:0.971]
Epoch [100/120    avg_loss:0.024, val_acc:0.971]
Epoch [101/120    avg_loss:0.017, val_acc:0.971]
Epoch [102/120    avg_loss:0.024, val_acc:0.971]
Epoch [103/120    avg_loss:0.021, val_acc:0.971]
Epoch [104/120    avg_loss:0.029, val_acc:0.971]
Epoch [105/120    avg_loss:0.024, val_acc:0.971]
Epoch [106/120    avg_loss:0.017, val_acc:0.971]
Epoch [107/120    avg_loss:0.018, val_acc:0.973]
Epoch [108/120    avg_loss:0.022, val_acc:0.973]
Epoch [109/120    avg_loss:0.021, val_acc:0.973]
Epoch [110/120    avg_loss:0.018, val_acc:0.973]
Epoch [111/120    avg_loss:0.027, val_acc:0.973]
Epoch [112/120    avg_loss:0.019, val_acc:0.973]
Epoch [113/120    avg_loss:0.019, val_acc:0.973]
Epoch [114/120    avg_loss:0.021, val_acc:0.973]
Epoch [115/120    avg_loss:0.015, val_acc:0.973]
Epoch [116/120    avg_loss:0.018, val_acc:0.973]
Epoch [117/120    avg_loss:0.018, val_acc:0.973]
Epoch [118/120    avg_loss:0.018, val_acc:0.973]
Epoch [119/120    avg_loss:0.017, val_acc:0.973]
Epoch [120/120    avg_loss:0.024, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    1 1265    0    0    0    1    0    0    0    2    8    8    0
     0    0    0]
 [   0    0    0  711    0   25    0    0    0    2    0    2    5    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    6    0    0    0    0  842   18    3    0
     0    3    0]
 [   0    0   16    0    0    1    7    0    0    0   13 2155    0    2
     2    0   14]
 [   0    0    0    1    3    0    0    0    0    0    0   24  497    0
     0    0    9]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    1    0    0    0
  1130    4    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    41  301    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.3550135501355

F1 scores:
[       nan 0.98795181 0.98597038 0.96998636 0.99300699 0.95902547
 0.98944193 1.         0.99883856 0.77777778 0.97172533 0.97555455
 0.94847328 0.98930481 0.97750865 0.91908397 0.87368421]

Kappa:
0.969846732747007
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f718228e6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.306, val_acc:0.566]
Epoch [2/120    avg_loss:1.610, val_acc:0.580]
Epoch [3/120    avg_loss:1.385, val_acc:0.613]
Epoch [4/120    avg_loss:1.030, val_acc:0.770]
Epoch [5/120    avg_loss:0.921, val_acc:0.759]
Epoch [6/120    avg_loss:0.692, val_acc:0.734]
Epoch [7/120    avg_loss:0.757, val_acc:0.788]
Epoch [8/120    avg_loss:0.604, val_acc:0.783]
Epoch [9/120    avg_loss:0.544, val_acc:0.821]
Epoch [10/120    avg_loss:0.602, val_acc:0.839]
Epoch [11/120    avg_loss:0.394, val_acc:0.851]
Epoch [12/120    avg_loss:0.343, val_acc:0.863]
Epoch [13/120    avg_loss:0.319, val_acc:0.871]
Epoch [14/120    avg_loss:0.302, val_acc:0.914]
Epoch [15/120    avg_loss:0.244, val_acc:0.876]
Epoch [16/120    avg_loss:0.238, val_acc:0.901]
Epoch [17/120    avg_loss:0.158, val_acc:0.899]
Epoch [18/120    avg_loss:0.177, val_acc:0.929]
Epoch [19/120    avg_loss:0.122, val_acc:0.924]
Epoch [20/120    avg_loss:0.120, val_acc:0.927]
Epoch [21/120    avg_loss:0.111, val_acc:0.934]
Epoch [22/120    avg_loss:0.087, val_acc:0.944]
Epoch [23/120    avg_loss:0.106, val_acc:0.934]
Epoch [24/120    avg_loss:0.109, val_acc:0.928]
Epoch [25/120    avg_loss:0.101, val_acc:0.952]
Epoch [26/120    avg_loss:0.075, val_acc:0.952]
Epoch [27/120    avg_loss:0.119, val_acc:0.951]
Epoch [28/120    avg_loss:0.081, val_acc:0.947]
Epoch [29/120    avg_loss:0.110, val_acc:0.911]
Epoch [30/120    avg_loss:0.105, val_acc:0.936]
Epoch [31/120    avg_loss:0.083, val_acc:0.939]
Epoch [32/120    avg_loss:0.078, val_acc:0.931]
Epoch [33/120    avg_loss:0.102, val_acc:0.933]
Epoch [34/120    avg_loss:0.094, val_acc:0.940]
Epoch [35/120    avg_loss:0.142, val_acc:0.922]
Epoch [36/120    avg_loss:0.172, val_acc:0.936]
Epoch [37/120    avg_loss:0.176, val_acc:0.925]
Epoch [38/120    avg_loss:0.102, val_acc:0.954]
Epoch [39/120    avg_loss:0.078, val_acc:0.945]
Epoch [40/120    avg_loss:0.066, val_acc:0.965]
Epoch [41/120    avg_loss:0.057, val_acc:0.957]
Epoch [42/120    avg_loss:0.043, val_acc:0.969]
Epoch [43/120    avg_loss:0.037, val_acc:0.968]
Epoch [44/120    avg_loss:0.042, val_acc:0.952]
Epoch [45/120    avg_loss:0.058, val_acc:0.961]
Epoch [46/120    avg_loss:0.072, val_acc:0.946]
Epoch [47/120    avg_loss:0.041, val_acc:0.966]
Epoch [48/120    avg_loss:0.040, val_acc:0.964]
Epoch [49/120    avg_loss:0.042, val_acc:0.959]
Epoch [50/120    avg_loss:0.044, val_acc:0.968]
Epoch [51/120    avg_loss:0.041, val_acc:0.961]
Epoch [52/120    avg_loss:0.026, val_acc:0.974]
Epoch [53/120    avg_loss:0.021, val_acc:0.970]
Epoch [54/120    avg_loss:0.027, val_acc:0.967]
Epoch [55/120    avg_loss:0.037, val_acc:0.968]
Epoch [56/120    avg_loss:0.023, val_acc:0.968]
Epoch [57/120    avg_loss:0.017, val_acc:0.967]
Epoch [58/120    avg_loss:0.022, val_acc:0.971]
Epoch [59/120    avg_loss:0.018, val_acc:0.968]
Epoch [60/120    avg_loss:0.019, val_acc:0.974]
Epoch [61/120    avg_loss:0.015, val_acc:0.974]
Epoch [62/120    avg_loss:0.014, val_acc:0.975]
Epoch [63/120    avg_loss:0.021, val_acc:0.976]
Epoch [64/120    avg_loss:0.014, val_acc:0.975]
Epoch [65/120    avg_loss:0.053, val_acc:0.944]
Epoch [66/120    avg_loss:0.037, val_acc:0.963]
Epoch [67/120    avg_loss:0.021, val_acc:0.974]
Epoch [68/120    avg_loss:0.025, val_acc:0.970]
Epoch [69/120    avg_loss:0.017, val_acc:0.975]
Epoch [70/120    avg_loss:0.021, val_acc:0.969]
Epoch [71/120    avg_loss:0.020, val_acc:0.959]
Epoch [72/120    avg_loss:0.038, val_acc:0.967]
Epoch [73/120    avg_loss:0.020, val_acc:0.966]
Epoch [74/120    avg_loss:0.025, val_acc:0.969]
Epoch [75/120    avg_loss:0.018, val_acc:0.969]
Epoch [76/120    avg_loss:0.013, val_acc:0.977]
Epoch [77/120    avg_loss:0.014, val_acc:0.980]
Epoch [78/120    avg_loss:0.019, val_acc:0.971]
Epoch [79/120    avg_loss:0.019, val_acc:0.969]
Epoch [80/120    avg_loss:0.037, val_acc:0.976]
Epoch [81/120    avg_loss:0.024, val_acc:0.978]
Epoch [82/120    avg_loss:0.014, val_acc:0.978]
Epoch [83/120    avg_loss:0.025, val_acc:0.971]
Epoch [84/120    avg_loss:0.009, val_acc:0.977]
Epoch [85/120    avg_loss:0.010, val_acc:0.979]
Epoch [86/120    avg_loss:0.011, val_acc:0.980]
Epoch [87/120    avg_loss:0.007, val_acc:0.980]
Epoch [88/120    avg_loss:0.008, val_acc:0.979]
Epoch [89/120    avg_loss:0.011, val_acc:0.979]
Epoch [90/120    avg_loss:0.009, val_acc:0.971]
Epoch [91/120    avg_loss:0.008, val_acc:0.981]
Epoch [92/120    avg_loss:0.008, val_acc:0.979]
Epoch [93/120    avg_loss:0.007, val_acc:0.980]
Epoch [94/120    avg_loss:0.006, val_acc:0.976]
Epoch [95/120    avg_loss:0.009, val_acc:0.976]
Epoch [96/120    avg_loss:0.006, val_acc:0.981]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.013, val_acc:0.976]
Epoch [99/120    avg_loss:0.008, val_acc:0.977]
Epoch [100/120    avg_loss:0.007, val_acc:0.980]
Epoch [101/120    avg_loss:0.022, val_acc:0.971]
Epoch [102/120    avg_loss:0.015, val_acc:0.977]
Epoch [103/120    avg_loss:0.013, val_acc:0.979]
Epoch [104/120    avg_loss:0.007, val_acc:0.981]
Epoch [105/120    avg_loss:0.007, val_acc:0.982]
Epoch [106/120    avg_loss:0.005, val_acc:0.980]
Epoch [107/120    avg_loss:0.007, val_acc:0.978]
Epoch [108/120    avg_loss:0.007, val_acc:0.984]
Epoch [109/120    avg_loss:0.009, val_acc:0.980]
Epoch [110/120    avg_loss:0.006, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.006, val_acc:0.982]
Epoch [113/120    avg_loss:0.016, val_acc:0.978]
Epoch [114/120    avg_loss:0.024, val_acc:0.978]
Epoch [115/120    avg_loss:0.014, val_acc:0.980]
Epoch [116/120    avg_loss:0.012, val_acc:0.978]
Epoch [117/120    avg_loss:0.007, val_acc:0.977]
Epoch [118/120    avg_loss:0.010, val_acc:0.977]
Epoch [119/120    avg_loss:0.010, val_acc:0.980]
Epoch [120/120    avg_loss:0.007, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    1    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1266    0    0    0    0    0    0    0    9    7    3    0
     0    0    0]
 [   0    0    0  714    0   17    0    0    0    3    2    1   10    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  425    0    6    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    8    0    0    0    0  839   23    1    0
     0    0    0]
 [   0    0    2    0    0    2    3    0    0    0    6 2184   13    0
     0    0    0]
 [   0    0    0   13   12    0    0    0    0    0    0    0  507    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    3    0    0    3    0    0    2    0   22    0    0    0
  1109    0    0]
 [   0    0    0    0    0    1   22    0    0    0    0    9    0    0
    24  291    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.33333333333333

F1 scores:
[       nan 0.94871795 0.99022292 0.96356275 0.97260274 0.95398429
 0.98132935 0.89285714 0.99767981 0.71794872 0.95558087 0.98511502
 0.94855005 1.         0.97623239 0.91222571 0.98224852]

Kappa:
0.969591920722003
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa8df6190b8>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.332, val_acc:0.504]
Epoch [2/120    avg_loss:1.642, val_acc:0.520]
Epoch [3/120    avg_loss:1.337, val_acc:0.586]
Epoch [4/120    avg_loss:1.083, val_acc:0.718]
Epoch [5/120    avg_loss:0.894, val_acc:0.689]
Epoch [6/120    avg_loss:0.727, val_acc:0.741]
Epoch [7/120    avg_loss:0.583, val_acc:0.799]
Epoch [8/120    avg_loss:0.625, val_acc:0.788]
Epoch [9/120    avg_loss:0.787, val_acc:0.812]
Epoch [10/120    avg_loss:0.728, val_acc:0.860]
Epoch [11/120    avg_loss:0.466, val_acc:0.827]
Epoch [12/120    avg_loss:0.555, val_acc:0.807]
Epoch [13/120    avg_loss:0.444, val_acc:0.876]
Epoch [14/120    avg_loss:0.376, val_acc:0.879]
Epoch [15/120    avg_loss:0.319, val_acc:0.900]
Epoch [16/120    avg_loss:0.338, val_acc:0.883]
Epoch [17/120    avg_loss:0.288, val_acc:0.848]
Epoch [18/120    avg_loss:0.245, val_acc:0.889]
Epoch [19/120    avg_loss:0.185, val_acc:0.907]
Epoch [20/120    avg_loss:0.231, val_acc:0.906]
Epoch [21/120    avg_loss:0.194, val_acc:0.898]
Epoch [22/120    avg_loss:0.153, val_acc:0.908]
Epoch [23/120    avg_loss:0.127, val_acc:0.917]
Epoch [24/120    avg_loss:0.098, val_acc:0.925]
Epoch [25/120    avg_loss:0.130, val_acc:0.900]
Epoch [26/120    avg_loss:0.115, val_acc:0.935]
Epoch [27/120    avg_loss:0.098, val_acc:0.918]
Epoch [28/120    avg_loss:0.112, val_acc:0.950]
Epoch [29/120    avg_loss:0.056, val_acc:0.953]
Epoch [30/120    avg_loss:0.053, val_acc:0.954]
Epoch [31/120    avg_loss:0.077, val_acc:0.946]
Epoch [32/120    avg_loss:0.068, val_acc:0.954]
Epoch [33/120    avg_loss:0.048, val_acc:0.955]
Epoch [34/120    avg_loss:0.074, val_acc:0.932]
Epoch [35/120    avg_loss:0.077, val_acc:0.944]
Epoch [36/120    avg_loss:0.092, val_acc:0.910]
Epoch [37/120    avg_loss:0.101, val_acc:0.910]
Epoch [38/120    avg_loss:0.094, val_acc:0.921]
Epoch [39/120    avg_loss:0.127, val_acc:0.942]
Epoch [40/120    avg_loss:0.066, val_acc:0.953]
Epoch [41/120    avg_loss:0.120, val_acc:0.935]
Epoch [42/120    avg_loss:0.089, val_acc:0.964]
Epoch [43/120    avg_loss:0.054, val_acc:0.959]
Epoch [44/120    avg_loss:0.042, val_acc:0.950]
Epoch [45/120    avg_loss:0.045, val_acc:0.942]
Epoch [46/120    avg_loss:0.037, val_acc:0.973]
Epoch [47/120    avg_loss:0.036, val_acc:0.967]
Epoch [48/120    avg_loss:0.033, val_acc:0.959]
Epoch [49/120    avg_loss:0.045, val_acc:0.964]
Epoch [50/120    avg_loss:0.038, val_acc:0.956]
Epoch [51/120    avg_loss:0.038, val_acc:0.958]
Epoch [52/120    avg_loss:0.021, val_acc:0.966]
Epoch [53/120    avg_loss:0.028, val_acc:0.965]
Epoch [54/120    avg_loss:0.033, val_acc:0.971]
Epoch [55/120    avg_loss:0.018, val_acc:0.976]
Epoch [56/120    avg_loss:0.026, val_acc:0.975]
Epoch [57/120    avg_loss:0.027, val_acc:0.968]
Epoch [58/120    avg_loss:0.028, val_acc:0.970]
Epoch [59/120    avg_loss:0.025, val_acc:0.970]
Epoch [60/120    avg_loss:0.028, val_acc:0.974]
Epoch [61/120    avg_loss:0.020, val_acc:0.961]
Epoch [62/120    avg_loss:0.029, val_acc:0.965]
Epoch [63/120    avg_loss:0.029, val_acc:0.966]
Epoch [64/120    avg_loss:0.035, val_acc:0.962]
Epoch [65/120    avg_loss:0.025, val_acc:0.980]
Epoch [66/120    avg_loss:0.026, val_acc:0.976]
Epoch [67/120    avg_loss:0.012, val_acc:0.975]
Epoch [68/120    avg_loss:0.014, val_acc:0.970]
Epoch [69/120    avg_loss:0.018, val_acc:0.974]
Epoch [70/120    avg_loss:0.017, val_acc:0.974]
Epoch [71/120    avg_loss:0.021, val_acc:0.974]
Epoch [72/120    avg_loss:0.014, val_acc:0.980]
Epoch [73/120    avg_loss:0.014, val_acc:0.980]
Epoch [74/120    avg_loss:0.012, val_acc:0.975]
Epoch [75/120    avg_loss:0.009, val_acc:0.979]
Epoch [76/120    avg_loss:0.013, val_acc:0.975]
Epoch [77/120    avg_loss:0.016, val_acc:0.979]
Epoch [78/120    avg_loss:0.010, val_acc:0.981]
Epoch [79/120    avg_loss:0.011, val_acc:0.976]
Epoch [80/120    avg_loss:0.013, val_acc:0.980]
Epoch [81/120    avg_loss:0.010, val_acc:0.976]
Epoch [82/120    avg_loss:0.011, val_acc:0.979]
Epoch [83/120    avg_loss:0.011, val_acc:0.976]
Epoch [84/120    avg_loss:0.010, val_acc:0.981]
Epoch [85/120    avg_loss:0.008, val_acc:0.981]
Epoch [86/120    avg_loss:0.016, val_acc:0.970]
Epoch [87/120    avg_loss:0.014, val_acc:0.985]
Epoch [88/120    avg_loss:0.010, val_acc:0.982]
Epoch [89/120    avg_loss:0.006, val_acc:0.976]
Epoch [90/120    avg_loss:0.010, val_acc:0.975]
Epoch [91/120    avg_loss:0.008, val_acc:0.980]
Epoch [92/120    avg_loss:0.005, val_acc:0.982]
Epoch [93/120    avg_loss:0.012, val_acc:0.979]
Epoch [94/120    avg_loss:0.020, val_acc:0.975]
Epoch [95/120    avg_loss:0.016, val_acc:0.975]
Epoch [96/120    avg_loss:0.007, val_acc:0.980]
Epoch [97/120    avg_loss:0.007, val_acc:0.985]
Epoch [98/120    avg_loss:0.006, val_acc:0.984]
Epoch [99/120    avg_loss:0.005, val_acc:0.985]
Epoch [100/120    avg_loss:0.008, val_acc:0.980]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.984]
Epoch [103/120    avg_loss:0.009, val_acc:0.971]
Epoch [104/120    avg_loss:0.011, val_acc:0.974]
Epoch [105/120    avg_loss:0.012, val_acc:0.978]
Epoch [106/120    avg_loss:0.028, val_acc:0.982]
Epoch [107/120    avg_loss:0.020, val_acc:0.955]
Epoch [108/120    avg_loss:0.013, val_acc:0.984]
Epoch [109/120    avg_loss:0.006, val_acc:0.979]
Epoch [110/120    avg_loss:0.009, val_acc:0.982]
Epoch [111/120    avg_loss:0.014, val_acc:0.981]
Epoch [112/120    avg_loss:0.007, val_acc:0.986]
Epoch [113/120    avg_loss:0.007, val_acc:0.986]
Epoch [114/120    avg_loss:0.009, val_acc:0.973]
Epoch [115/120    avg_loss:0.021, val_acc:0.978]
Epoch [116/120    avg_loss:0.010, val_acc:0.980]
Epoch [117/120    avg_loss:0.013, val_acc:0.963]
Epoch [118/120    avg_loss:0.011, val_acc:0.985]
Epoch [119/120    avg_loss:0.010, val_acc:0.976]
Epoch [120/120    avg_loss:0.007, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1265    1    0    0    0    0    0    0    2   17    0    0
     0    0    0]
 [   0    0    1  704    5   26    0    0    0    6    1    0    4    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    4    4    0    5    8    0    0    0  826   22    0    0
     3    3    0]
 [   0    0   19    0    0    2    6    0    0    0    9 2172    0    2
     0    0    0]
 [   0    0    0   11   14    9    0    0    0    0    2    0  492    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    9    0    0    1    0    3    0    0    0
  1125    1    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
    33  313    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.32249322493224

F1 scores:
[       nan 0.98765432 0.98252427 0.95782313 0.95730337 0.94347826
 0.98871332 1.         0.99883856 0.75       0.96158324 0.98258313
 0.95348837 0.99462366 0.97826087 0.94277108 0.95348837]

Kappa:
0.9694760567808908
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:43--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8adde55748>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.410, val_acc:0.471]
Epoch [2/120    avg_loss:1.743, val_acc:0.555]
Epoch [3/120    avg_loss:1.339, val_acc:0.584]
Epoch [4/120    avg_loss:1.491, val_acc:0.637]
Epoch [5/120    avg_loss:1.019, val_acc:0.705]
Epoch [6/120    avg_loss:0.883, val_acc:0.721]
Epoch [7/120    avg_loss:0.820, val_acc:0.779]
Epoch [8/120    avg_loss:0.713, val_acc:0.771]
Epoch [9/120    avg_loss:0.550, val_acc:0.796]
Epoch [10/120    avg_loss:0.660, val_acc:0.821]
Epoch [11/120    avg_loss:0.575, val_acc:0.822]
Epoch [12/120    avg_loss:0.367, val_acc:0.857]
Epoch [13/120    avg_loss:0.326, val_acc:0.866]
Epoch [14/120    avg_loss:0.425, val_acc:0.865]
Epoch [15/120    avg_loss:0.302, val_acc:0.893]
Epoch [16/120    avg_loss:0.230, val_acc:0.907]
Epoch [17/120    avg_loss:0.186, val_acc:0.887]
Epoch [18/120    avg_loss:0.305, val_acc:0.848]
Epoch [19/120    avg_loss:0.263, val_acc:0.850]
Epoch [20/120    avg_loss:0.278, val_acc:0.850]
Epoch [21/120    avg_loss:0.217, val_acc:0.901]
Epoch [22/120    avg_loss:0.172, val_acc:0.924]
Epoch [23/120    avg_loss:0.158, val_acc:0.923]
Epoch [24/120    avg_loss:0.107, val_acc:0.902]
Epoch [25/120    avg_loss:0.161, val_acc:0.922]
Epoch [26/120    avg_loss:0.127, val_acc:0.917]
Epoch [27/120    avg_loss:0.113, val_acc:0.943]
Epoch [28/120    avg_loss:0.117, val_acc:0.907]
Epoch [29/120    avg_loss:0.093, val_acc:0.929]
Epoch [30/120    avg_loss:0.101, val_acc:0.951]
Epoch [31/120    avg_loss:0.079, val_acc:0.940]
Epoch [32/120    avg_loss:0.062, val_acc:0.950]
Epoch [33/120    avg_loss:0.066, val_acc:0.961]
Epoch [34/120    avg_loss:0.089, val_acc:0.916]
Epoch [35/120    avg_loss:0.101, val_acc:0.918]
Epoch [36/120    avg_loss:0.090, val_acc:0.938]
Epoch [37/120    avg_loss:0.082, val_acc:0.966]
Epoch [38/120    avg_loss:0.046, val_acc:0.962]
Epoch [39/120    avg_loss:0.050, val_acc:0.961]
Epoch [40/120    avg_loss:0.048, val_acc:0.962]
Epoch [41/120    avg_loss:0.105, val_acc:0.920]
Epoch [42/120    avg_loss:0.131, val_acc:0.944]
Epoch [43/120    avg_loss:0.063, val_acc:0.959]
Epoch [44/120    avg_loss:0.053, val_acc:0.954]
Epoch [45/120    avg_loss:0.066, val_acc:0.948]
Epoch [46/120    avg_loss:0.058, val_acc:0.966]
Epoch [47/120    avg_loss:0.054, val_acc:0.951]
Epoch [48/120    avg_loss:0.040, val_acc:0.958]
Epoch [49/120    avg_loss:0.028, val_acc:0.970]
Epoch [50/120    avg_loss:0.023, val_acc:0.957]
Epoch [51/120    avg_loss:0.033, val_acc:0.967]
Epoch [52/120    avg_loss:0.061, val_acc:0.954]
Epoch [53/120    avg_loss:0.091, val_acc:0.954]
Epoch [54/120    avg_loss:0.096, val_acc:0.963]
Epoch [55/120    avg_loss:0.048, val_acc:0.954]
Epoch [56/120    avg_loss:0.040, val_acc:0.967]
Epoch [57/120    avg_loss:0.053, val_acc:0.964]
Epoch [58/120    avg_loss:0.042, val_acc:0.966]
Epoch [59/120    avg_loss:0.031, val_acc:0.962]
Epoch [60/120    avg_loss:0.030, val_acc:0.963]
Epoch [61/120    avg_loss:0.025, val_acc:0.964]
Epoch [62/120    avg_loss:0.026, val_acc:0.967]
Epoch [63/120    avg_loss:0.015, val_acc:0.970]
Epoch [64/120    avg_loss:0.017, val_acc:0.971]
Epoch [65/120    avg_loss:0.030, val_acc:0.975]
Epoch [66/120    avg_loss:0.018, val_acc:0.971]
Epoch [67/120    avg_loss:0.021, val_acc:0.971]
Epoch [68/120    avg_loss:0.014, val_acc:0.970]
Epoch [69/120    avg_loss:0.012, val_acc:0.971]
Epoch [70/120    avg_loss:0.013, val_acc:0.973]
Epoch [71/120    avg_loss:0.014, val_acc:0.974]
Epoch [72/120    avg_loss:0.014, val_acc:0.975]
Epoch [73/120    avg_loss:0.009, val_acc:0.975]
Epoch [74/120    avg_loss:0.010, val_acc:0.975]
Epoch [75/120    avg_loss:0.016, val_acc:0.977]
Epoch [76/120    avg_loss:0.011, val_acc:0.978]
Epoch [77/120    avg_loss:0.011, val_acc:0.977]
Epoch [78/120    avg_loss:0.012, val_acc:0.976]
Epoch [79/120    avg_loss:0.011, val_acc:0.976]
Epoch [80/120    avg_loss:0.012, val_acc:0.975]
Epoch [81/120    avg_loss:0.011, val_acc:0.975]
Epoch [82/120    avg_loss:0.010, val_acc:0.974]
Epoch [83/120    avg_loss:0.016, val_acc:0.977]
Epoch [84/120    avg_loss:0.014, val_acc:0.976]
Epoch [85/120    avg_loss:0.012, val_acc:0.976]
Epoch [86/120    avg_loss:0.010, val_acc:0.976]
Epoch [87/120    avg_loss:0.013, val_acc:0.977]
Epoch [88/120    avg_loss:0.008, val_acc:0.977]
Epoch [89/120    avg_loss:0.015, val_acc:0.975]
Epoch [90/120    avg_loss:0.013, val_acc:0.976]
Epoch [91/120    avg_loss:0.013, val_acc:0.976]
Epoch [92/120    avg_loss:0.012, val_acc:0.976]
Epoch [93/120    avg_loss:0.017, val_acc:0.977]
Epoch [94/120    avg_loss:0.012, val_acc:0.978]
Epoch [95/120    avg_loss:0.010, val_acc:0.976]
Epoch [96/120    avg_loss:0.010, val_acc:0.976]
Epoch [97/120    avg_loss:0.009, val_acc:0.978]
Epoch [98/120    avg_loss:0.012, val_acc:0.978]
Epoch [99/120    avg_loss:0.012, val_acc:0.978]
Epoch [100/120    avg_loss:0.010, val_acc:0.978]
Epoch [101/120    avg_loss:0.010, val_acc:0.978]
Epoch [102/120    avg_loss:0.014, val_acc:0.978]
Epoch [103/120    avg_loss:0.011, val_acc:0.978]
Epoch [104/120    avg_loss:0.012, val_acc:0.978]
Epoch [105/120    avg_loss:0.014, val_acc:0.977]
Epoch [106/120    avg_loss:0.010, val_acc:0.977]
Epoch [107/120    avg_loss:0.010, val_acc:0.977]
Epoch [108/120    avg_loss:0.014, val_acc:0.977]
Epoch [109/120    avg_loss:0.010, val_acc:0.977]
Epoch [110/120    avg_loss:0.012, val_acc:0.977]
Epoch [111/120    avg_loss:0.013, val_acc:0.977]
Epoch [112/120    avg_loss:0.010, val_acc:0.977]
Epoch [113/120    avg_loss:0.010, val_acc:0.977]
Epoch [114/120    avg_loss:0.010, val_acc:0.977]
Epoch [115/120    avg_loss:0.011, val_acc:0.977]
Epoch [116/120    avg_loss:0.010, val_acc:0.977]
Epoch [117/120    avg_loss:0.009, val_acc:0.978]
Epoch [118/120    avg_loss:0.012, val_acc:0.978]
Epoch [119/120    avg_loss:0.010, val_acc:0.978]
Epoch [120/120    avg_loss:0.012, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1279    0    0    0    0    0    0    0    1    3    2    0
     0    0    0]
 [   0    0    0  715    0   18    0    0    0    5    1    0    7    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    5    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    4    0    0    0    0  846   21    0    0
     0    2    0]
 [   0    0    8    0    0    3   11    0    0    0    7 2180    0    1
     0    0    0]
 [   0    0    0    6    5    2    0    0    0    0    2    7  506    0
     2    0    4]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0   12    0    0    1    0    1    0    0    0
  1125    0    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    25  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.86449864498645

F1 scores:
[       nan 1.         0.99378399 0.9721278  0.98839907 0.95027624
 0.97477745 1.         0.99883856 0.69767442 0.9763416  0.98620222
 0.96380952 0.99191375 0.98210388 0.92283951 0.97076023]

Kappa:
0.975651992566196
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9271cd96a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.282, val_acc:0.470]
Epoch [2/120    avg_loss:1.639, val_acc:0.489]
Epoch [3/120    avg_loss:1.378, val_acc:0.701]
Epoch [4/120    avg_loss:1.032, val_acc:0.768]
Epoch [5/120    avg_loss:0.856, val_acc:0.663]
Epoch [6/120    avg_loss:0.798, val_acc:0.798]
Epoch [7/120    avg_loss:0.703, val_acc:0.760]
Epoch [8/120    avg_loss:0.538, val_acc:0.796]
Epoch [9/120    avg_loss:0.519, val_acc:0.791]
Epoch [10/120    avg_loss:0.393, val_acc:0.826]
Epoch [11/120    avg_loss:0.338, val_acc:0.878]
Epoch [12/120    avg_loss:0.289, val_acc:0.855]
Epoch [13/120    avg_loss:0.270, val_acc:0.890]
Epoch [14/120    avg_loss:0.210, val_acc:0.874]
Epoch [15/120    avg_loss:0.418, val_acc:0.867]
Epoch [16/120    avg_loss:0.281, val_acc:0.910]
Epoch [17/120    avg_loss:0.277, val_acc:0.895]
Epoch [18/120    avg_loss:0.260, val_acc:0.934]
Epoch [19/120    avg_loss:0.225, val_acc:0.924]
Epoch [20/120    avg_loss:0.136, val_acc:0.936]
Epoch [21/120    avg_loss:0.138, val_acc:0.930]
Epoch [22/120    avg_loss:0.127, val_acc:0.932]
Epoch [23/120    avg_loss:0.140, val_acc:0.924]
Epoch [24/120    avg_loss:0.126, val_acc:0.925]
Epoch [25/120    avg_loss:0.107, val_acc:0.925]
Epoch [26/120    avg_loss:0.086, val_acc:0.942]
Epoch [27/120    avg_loss:0.083, val_acc:0.946]
Epoch [28/120    avg_loss:0.084, val_acc:0.950]
Epoch [29/120    avg_loss:0.101, val_acc:0.940]
Epoch [30/120    avg_loss:0.102, val_acc:0.923]
Epoch [31/120    avg_loss:0.132, val_acc:0.943]
Epoch [32/120    avg_loss:0.097, val_acc:0.946]
Epoch [33/120    avg_loss:0.099, val_acc:0.924]
Epoch [34/120    avg_loss:0.083, val_acc:0.959]
Epoch [35/120    avg_loss:0.061, val_acc:0.959]
Epoch [36/120    avg_loss:0.054, val_acc:0.959]
Epoch [37/120    avg_loss:0.032, val_acc:0.961]
Epoch [38/120    avg_loss:0.042, val_acc:0.952]
Epoch [39/120    avg_loss:0.040, val_acc:0.957]
Epoch [40/120    avg_loss:0.044, val_acc:0.961]
Epoch [41/120    avg_loss:0.029, val_acc:0.951]
Epoch [42/120    avg_loss:0.057, val_acc:0.955]
Epoch [43/120    avg_loss:0.038, val_acc:0.959]
Epoch [44/120    avg_loss:0.040, val_acc:0.961]
Epoch [45/120    avg_loss:0.038, val_acc:0.962]
Epoch [46/120    avg_loss:0.025, val_acc:0.959]
Epoch [47/120    avg_loss:0.031, val_acc:0.957]
Epoch [48/120    avg_loss:0.031, val_acc:0.959]
Epoch [49/120    avg_loss:0.045, val_acc:0.967]
Epoch [50/120    avg_loss:0.038, val_acc:0.965]
Epoch [51/120    avg_loss:0.042, val_acc:0.953]
Epoch [52/120    avg_loss:0.030, val_acc:0.967]
Epoch [53/120    avg_loss:0.023, val_acc:0.975]
Epoch [54/120    avg_loss:0.027, val_acc:0.952]
Epoch [55/120    avg_loss:0.019, val_acc:0.970]
Epoch [56/120    avg_loss:0.017, val_acc:0.967]
Epoch [57/120    avg_loss:0.021, val_acc:0.961]
Epoch [58/120    avg_loss:0.016, val_acc:0.969]
Epoch [59/120    avg_loss:0.014, val_acc:0.974]
Epoch [60/120    avg_loss:0.026, val_acc:0.970]
Epoch [61/120    avg_loss:0.018, val_acc:0.970]
Epoch [62/120    avg_loss:0.015, val_acc:0.975]
Epoch [63/120    avg_loss:0.025, val_acc:0.970]
Epoch [64/120    avg_loss:0.016, val_acc:0.969]
Epoch [65/120    avg_loss:0.021, val_acc:0.959]
Epoch [66/120    avg_loss:0.017, val_acc:0.959]
Epoch [67/120    avg_loss:0.023, val_acc:0.969]
Epoch [68/120    avg_loss:0.013, val_acc:0.970]
Epoch [69/120    avg_loss:0.014, val_acc:0.930]
Epoch [70/120    avg_loss:0.020, val_acc:0.977]
Epoch [71/120    avg_loss:0.014, val_acc:0.975]
Epoch [72/120    avg_loss:0.013, val_acc:0.973]
Epoch [73/120    avg_loss:0.019, val_acc:0.970]
Epoch [74/120    avg_loss:0.013, val_acc:0.974]
Epoch [75/120    avg_loss:0.013, val_acc:0.971]
Epoch [76/120    avg_loss:0.012, val_acc:0.970]
Epoch [77/120    avg_loss:0.014, val_acc:0.976]
Epoch [78/120    avg_loss:0.010, val_acc:0.976]
Epoch [79/120    avg_loss:0.008, val_acc:0.975]
Epoch [80/120    avg_loss:0.009, val_acc:0.975]
Epoch [81/120    avg_loss:0.016, val_acc:0.975]
Epoch [82/120    avg_loss:0.009, val_acc:0.977]
Epoch [83/120    avg_loss:0.007, val_acc:0.976]
Epoch [84/120    avg_loss:0.011, val_acc:0.970]
Epoch [85/120    avg_loss:0.014, val_acc:0.970]
Epoch [86/120    avg_loss:0.007, val_acc:0.971]
Epoch [87/120    avg_loss:0.009, val_acc:0.976]
Epoch [88/120    avg_loss:0.008, val_acc:0.975]
Epoch [89/120    avg_loss:0.007, val_acc:0.977]
Epoch [90/120    avg_loss:0.007, val_acc:0.979]
Epoch [91/120    avg_loss:0.010, val_acc:0.975]
Epoch [92/120    avg_loss:0.010, val_acc:0.978]
Epoch [93/120    avg_loss:0.010, val_acc:0.979]
Epoch [94/120    avg_loss:0.008, val_acc:0.979]
Epoch [95/120    avg_loss:0.007, val_acc:0.985]
Epoch [96/120    avg_loss:0.012, val_acc:0.982]
Epoch [97/120    avg_loss:0.006, val_acc:0.984]
Epoch [98/120    avg_loss:0.006, val_acc:0.987]
Epoch [99/120    avg_loss:0.009, val_acc:0.977]
Epoch [100/120    avg_loss:0.007, val_acc:0.976]
Epoch [101/120    avg_loss:0.010, val_acc:0.978]
Epoch [102/120    avg_loss:0.008, val_acc:0.979]
Epoch [103/120    avg_loss:0.005, val_acc:0.978]
Epoch [104/120    avg_loss:0.018, val_acc:0.951]
Epoch [105/120    avg_loss:0.031, val_acc:0.965]
Epoch [106/120    avg_loss:0.017, val_acc:0.973]
Epoch [107/120    avg_loss:0.020, val_acc:0.970]
Epoch [108/120    avg_loss:0.023, val_acc:0.942]
Epoch [109/120    avg_loss:0.042, val_acc:0.975]
Epoch [110/120    avg_loss:0.058, val_acc:0.967]
Epoch [111/120    avg_loss:0.030, val_acc:0.962]
Epoch [112/120    avg_loss:0.028, val_acc:0.970]
Epoch [113/120    avg_loss:0.016, val_acc:0.973]
Epoch [114/120    avg_loss:0.007, val_acc:0.970]
Epoch [115/120    avg_loss:0.010, val_acc:0.975]
Epoch [116/120    avg_loss:0.009, val_acc:0.976]
Epoch [117/120    avg_loss:0.008, val_acc:0.976]
Epoch [118/120    avg_loss:0.008, val_acc:0.975]
Epoch [119/120    avg_loss:0.007, val_acc:0.976]
Epoch [120/120    avg_loss:0.007, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1265    1    0    0    2    0    0    0    4    4    4    0
     0    5    0]
 [   0    0    0  724    0   12    0    0    0    6    1    0    1    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    4   18    0    0    0    0    0    0  823   23    0    0
     2    5    0]
 [   0    0    7    0    0    0    5    0    0    0   10 2186    0    2
     0    0    0]
 [   0    0    0   13    2    3    0    0    0    0    9    5  498    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   34    0    0    0    0    0    0    0
    29  284    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.5609756097561

F1 scores:
[       nan 1.         0.98789535 0.96340652 0.9953271  0.97959184
 0.9697417  1.         0.99883856 0.77272727 0.9553105  0.98735321
 0.95861405 0.98666667 0.98526863 0.88611544 0.97647059]

Kappa:
0.9721840547510271
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f39621466a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.281, val_acc:0.457]
Epoch [2/120    avg_loss:1.637, val_acc:0.609]
Epoch [3/120    avg_loss:1.251, val_acc:0.726]
Epoch [4/120    avg_loss:1.020, val_acc:0.775]
Epoch [5/120    avg_loss:1.084, val_acc:0.670]
Epoch [6/120    avg_loss:0.884, val_acc:0.735]
Epoch [7/120    avg_loss:0.745, val_acc:0.795]
Epoch [8/120    avg_loss:0.625, val_acc:0.834]
Epoch [9/120    avg_loss:0.471, val_acc:0.776]
Epoch [10/120    avg_loss:0.538, val_acc:0.802]
Epoch [11/120    avg_loss:0.457, val_acc:0.829]
Epoch [12/120    avg_loss:0.351, val_acc:0.875]
Epoch [13/120    avg_loss:0.267, val_acc:0.897]
Epoch [14/120    avg_loss:0.262, val_acc:0.866]
Epoch [15/120    avg_loss:0.346, val_acc:0.823]
Epoch [16/120    avg_loss:0.357, val_acc:0.857]
Epoch [17/120    avg_loss:0.421, val_acc:0.886]
Epoch [18/120    avg_loss:0.209, val_acc:0.908]
Epoch [19/120    avg_loss:0.198, val_acc:0.913]
Epoch [20/120    avg_loss:0.203, val_acc:0.923]
Epoch [21/120    avg_loss:0.155, val_acc:0.893]
Epoch [22/120    avg_loss:0.165, val_acc:0.922]
Epoch [23/120    avg_loss:0.120, val_acc:0.938]
Epoch [24/120    avg_loss:0.109, val_acc:0.929]
Epoch [25/120    avg_loss:0.095, val_acc:0.936]
Epoch [26/120    avg_loss:0.097, val_acc:0.945]
Epoch [27/120    avg_loss:0.096, val_acc:0.921]
Epoch [28/120    avg_loss:0.093, val_acc:0.939]
Epoch [29/120    avg_loss:0.067, val_acc:0.938]
Epoch [30/120    avg_loss:0.123, val_acc:0.916]
Epoch [31/120    avg_loss:0.166, val_acc:0.927]
Epoch [32/120    avg_loss:0.100, val_acc:0.946]
Epoch [33/120    avg_loss:0.082, val_acc:0.945]
Epoch [34/120    avg_loss:0.074, val_acc:0.946]
Epoch [35/120    avg_loss:0.055, val_acc:0.936]
Epoch [36/120    avg_loss:0.077, val_acc:0.946]
Epoch [37/120    avg_loss:0.053, val_acc:0.951]
Epoch [38/120    avg_loss:0.064, val_acc:0.918]
Epoch [39/120    avg_loss:0.099, val_acc:0.955]
Epoch [40/120    avg_loss:0.056, val_acc:0.940]
Epoch [41/120    avg_loss:0.050, val_acc:0.956]
Epoch [42/120    avg_loss:0.089, val_acc:0.941]
Epoch [43/120    avg_loss:0.066, val_acc:0.959]
Epoch [44/120    avg_loss:0.062, val_acc:0.951]
Epoch [45/120    avg_loss:0.073, val_acc:0.932]
Epoch [46/120    avg_loss:0.038, val_acc:0.957]
Epoch [47/120    avg_loss:0.058, val_acc:0.936]
Epoch [48/120    avg_loss:0.039, val_acc:0.944]
Epoch [49/120    avg_loss:0.036, val_acc:0.954]
Epoch [50/120    avg_loss:0.034, val_acc:0.951]
Epoch [51/120    avg_loss:0.044, val_acc:0.956]
Epoch [52/120    avg_loss:0.040, val_acc:0.969]
Epoch [53/120    avg_loss:0.029, val_acc:0.970]
Epoch [54/120    avg_loss:0.024, val_acc:0.959]
Epoch [55/120    avg_loss:0.037, val_acc:0.959]
Epoch [56/120    avg_loss:0.018, val_acc:0.957]
Epoch [57/120    avg_loss:0.018, val_acc:0.969]
Epoch [58/120    avg_loss:0.024, val_acc:0.970]
Epoch [59/120    avg_loss:0.014, val_acc:0.970]
Epoch [60/120    avg_loss:0.018, val_acc:0.956]
Epoch [61/120    avg_loss:0.031, val_acc:0.961]
Epoch [62/120    avg_loss:0.030, val_acc:0.962]
Epoch [63/120    avg_loss:0.024, val_acc:0.967]
Epoch [64/120    avg_loss:0.017, val_acc:0.969]
Epoch [65/120    avg_loss:0.017, val_acc:0.971]
Epoch [66/120    avg_loss:0.014, val_acc:0.969]
Epoch [67/120    avg_loss:0.022, val_acc:0.965]
Epoch [68/120    avg_loss:0.012, val_acc:0.969]
Epoch [69/120    avg_loss:0.015, val_acc:0.967]
Epoch [70/120    avg_loss:0.012, val_acc:0.970]
Epoch [71/120    avg_loss:0.017, val_acc:0.976]
Epoch [72/120    avg_loss:0.012, val_acc:0.969]
Epoch [73/120    avg_loss:0.008, val_acc:0.971]
Epoch [74/120    avg_loss:0.014, val_acc:0.966]
Epoch [75/120    avg_loss:0.016, val_acc:0.958]
Epoch [76/120    avg_loss:0.025, val_acc:0.955]
Epoch [77/120    avg_loss:0.031, val_acc:0.952]
Epoch [78/120    avg_loss:0.056, val_acc:0.928]
Epoch [79/120    avg_loss:0.057, val_acc:0.952]
Epoch [80/120    avg_loss:0.049, val_acc:0.965]
Epoch [81/120    avg_loss:0.021, val_acc:0.964]
Epoch [82/120    avg_loss:0.022, val_acc:0.964]
Epoch [83/120    avg_loss:0.022, val_acc:0.974]
Epoch [84/120    avg_loss:0.012, val_acc:0.968]
Epoch [85/120    avg_loss:0.009, val_acc:0.969]
Epoch [86/120    avg_loss:0.008, val_acc:0.969]
Epoch [87/120    avg_loss:0.009, val_acc:0.973]
Epoch [88/120    avg_loss:0.013, val_acc:0.971]
Epoch [89/120    avg_loss:0.008, val_acc:0.971]
Epoch [90/120    avg_loss:0.006, val_acc:0.971]
Epoch [91/120    avg_loss:0.010, val_acc:0.971]
Epoch [92/120    avg_loss:0.007, val_acc:0.974]
Epoch [93/120    avg_loss:0.011, val_acc:0.975]
Epoch [94/120    avg_loss:0.013, val_acc:0.976]
Epoch [95/120    avg_loss:0.008, val_acc:0.976]
Epoch [96/120    avg_loss:0.015, val_acc:0.979]
Epoch [97/120    avg_loss:0.009, val_acc:0.979]
Epoch [98/120    avg_loss:0.013, val_acc:0.979]
Epoch [99/120    avg_loss:0.009, val_acc:0.978]
Epoch [100/120    avg_loss:0.008, val_acc:0.978]
Epoch [101/120    avg_loss:0.012, val_acc:0.978]
Epoch [102/120    avg_loss:0.008, val_acc:0.978]
Epoch [103/120    avg_loss:0.007, val_acc:0.979]
Epoch [104/120    avg_loss:0.007, val_acc:0.979]
Epoch [105/120    avg_loss:0.011, val_acc:0.979]
Epoch [106/120    avg_loss:0.011, val_acc:0.977]
Epoch [107/120    avg_loss:0.005, val_acc:0.977]
Epoch [108/120    avg_loss:0.008, val_acc:0.977]
Epoch [109/120    avg_loss:0.010, val_acc:0.979]
Epoch [110/120    avg_loss:0.006, val_acc:0.979]
Epoch [111/120    avg_loss:0.010, val_acc:0.976]
Epoch [112/120    avg_loss:0.007, val_acc:0.978]
Epoch [113/120    avg_loss:0.008, val_acc:0.978]
Epoch [114/120    avg_loss:0.010, val_acc:0.978]
Epoch [115/120    avg_loss:0.008, val_acc:0.977]
Epoch [116/120    avg_loss:0.007, val_acc:0.977]
Epoch [117/120    avg_loss:0.007, val_acc:0.978]
Epoch [118/120    avg_loss:0.008, val_acc:0.977]
Epoch [119/120    avg_loss:0.007, val_acc:0.977]
Epoch [120/120    avg_loss:0.007, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1273    4    0    0    1    0    0    0    3    4    0    0
     0    0    0]
 [   0    0    0  713    0   23    0    0    0    3    0    0    5    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    4    3    0    0    0  845   18    0    0
     0    3    0]
 [   0    0   10    0    0    0   10    0    0    0    4 2162   22    2
     0    0    0]
 [   0    0    0   13   18    6    0    0    0    0    6    0  488    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    1    0    0    0
  1130    0    0]
 [   0    0    0    0    0    0   25    0    0    0    0    0    0    0
    30  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.33333333333333

F1 scores:
[       nan 0.98765432 0.99027616 0.96351351 0.95945946 0.94795127
 0.97117517 0.98039216 0.99883856 0.71428571 0.97462514 0.98406919
 0.92952381 0.98666667 0.9830361  0.90965732 0.97647059]

Kappa:
0.969613431999866
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:50--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f08b00a8630>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.285, val_acc:0.417]
Epoch [2/120    avg_loss:1.704, val_acc:0.586]
Epoch [3/120    avg_loss:1.520, val_acc:0.576]
Epoch [4/120    avg_loss:1.146, val_acc:0.755]
Epoch [5/120    avg_loss:1.062, val_acc:0.736]
Epoch [6/120    avg_loss:0.832, val_acc:0.796]
Epoch [7/120    avg_loss:0.651, val_acc:0.810]
Epoch [8/120    avg_loss:0.615, val_acc:0.811]
Epoch [9/120    avg_loss:0.527, val_acc:0.850]
Epoch [10/120    avg_loss:0.477, val_acc:0.799]
Epoch [11/120    avg_loss:0.391, val_acc:0.873]
Epoch [12/120    avg_loss:0.422, val_acc:0.871]
Epoch [13/120    avg_loss:0.356, val_acc:0.880]
Epoch [14/120    avg_loss:0.308, val_acc:0.864]
Epoch [15/120    avg_loss:0.261, val_acc:0.884]
Epoch [16/120    avg_loss:0.223, val_acc:0.910]
Epoch [17/120    avg_loss:0.160, val_acc:0.916]
Epoch [18/120    avg_loss:0.174, val_acc:0.900]
Epoch [19/120    avg_loss:0.148, val_acc:0.929]
Epoch [20/120    avg_loss:0.142, val_acc:0.914]
Epoch [21/120    avg_loss:0.098, val_acc:0.940]
Epoch [22/120    avg_loss:0.112, val_acc:0.931]
Epoch [23/120    avg_loss:0.175, val_acc:0.910]
Epoch [24/120    avg_loss:0.118, val_acc:0.925]
Epoch [25/120    avg_loss:0.075, val_acc:0.955]
Epoch [26/120    avg_loss:0.081, val_acc:0.938]
Epoch [27/120    avg_loss:0.066, val_acc:0.943]
Epoch [28/120    avg_loss:0.067, val_acc:0.958]
Epoch [29/120    avg_loss:0.099, val_acc:0.940]
Epoch [30/120    avg_loss:0.076, val_acc:0.951]
Epoch [31/120    avg_loss:0.070, val_acc:0.939]
Epoch [32/120    avg_loss:0.086, val_acc:0.954]
Epoch [33/120    avg_loss:0.077, val_acc:0.935]
Epoch [34/120    avg_loss:0.074, val_acc:0.951]
Epoch [35/120    avg_loss:0.049, val_acc:0.954]
Epoch [36/120    avg_loss:0.064, val_acc:0.954]
Epoch [37/120    avg_loss:0.084, val_acc:0.936]
Epoch [38/120    avg_loss:0.133, val_acc:0.961]
Epoch [39/120    avg_loss:0.057, val_acc:0.957]
Epoch [40/120    avg_loss:0.061, val_acc:0.965]
Epoch [41/120    avg_loss:0.061, val_acc:0.952]
Epoch [42/120    avg_loss:0.065, val_acc:0.956]
Epoch [43/120    avg_loss:0.033, val_acc:0.958]
Epoch [44/120    avg_loss:0.040, val_acc:0.968]
Epoch [45/120    avg_loss:0.040, val_acc:0.965]
Epoch [46/120    avg_loss:0.041, val_acc:0.963]
Epoch [47/120    avg_loss:0.049, val_acc:0.964]
Epoch [48/120    avg_loss:0.044, val_acc:0.956]
Epoch [49/120    avg_loss:0.035, val_acc:0.976]
Epoch [50/120    avg_loss:0.032, val_acc:0.964]
Epoch [51/120    avg_loss:0.035, val_acc:0.968]
Epoch [52/120    avg_loss:0.026, val_acc:0.965]
Epoch [53/120    avg_loss:0.035, val_acc:0.968]
Epoch [54/120    avg_loss:0.028, val_acc:0.967]
Epoch [55/120    avg_loss:0.016, val_acc:0.975]
Epoch [56/120    avg_loss:0.023, val_acc:0.973]
Epoch [57/120    avg_loss:0.022, val_acc:0.977]
Epoch [58/120    avg_loss:0.020, val_acc:0.975]
Epoch [59/120    avg_loss:0.024, val_acc:0.957]
Epoch [60/120    avg_loss:0.046, val_acc:0.963]
Epoch [61/120    avg_loss:0.027, val_acc:0.969]
Epoch [62/120    avg_loss:0.033, val_acc:0.968]
Epoch [63/120    avg_loss:0.021, val_acc:0.970]
Epoch [64/120    avg_loss:0.025, val_acc:0.967]
Epoch [65/120    avg_loss:0.018, val_acc:0.976]
Epoch [66/120    avg_loss:0.015, val_acc:0.973]
Epoch [67/120    avg_loss:0.022, val_acc:0.976]
Epoch [68/120    avg_loss:0.024, val_acc:0.981]
Epoch [69/120    avg_loss:0.011, val_acc:0.980]
Epoch [70/120    avg_loss:0.020, val_acc:0.959]
Epoch [71/120    avg_loss:0.020, val_acc:0.970]
Epoch [72/120    avg_loss:0.018, val_acc:0.971]
Epoch [73/120    avg_loss:0.018, val_acc:0.979]
Epoch [74/120    avg_loss:0.019, val_acc:0.974]
Epoch [75/120    avg_loss:0.020, val_acc:0.977]
Epoch [76/120    avg_loss:0.025, val_acc:0.971]
Epoch [77/120    avg_loss:0.025, val_acc:0.988]
Epoch [78/120    avg_loss:0.034, val_acc:0.952]
Epoch [79/120    avg_loss:0.244, val_acc:0.954]
Epoch [80/120    avg_loss:0.076, val_acc:0.952]
Epoch [81/120    avg_loss:0.102, val_acc:0.953]
Epoch [82/120    avg_loss:0.076, val_acc:0.968]
Epoch [83/120    avg_loss:0.021, val_acc:0.971]
Epoch [84/120    avg_loss:0.020, val_acc:0.969]
Epoch [85/120    avg_loss:0.017, val_acc:0.968]
Epoch [86/120    avg_loss:0.021, val_acc:0.974]
Epoch [87/120    avg_loss:0.024, val_acc:0.964]
Epoch [88/120    avg_loss:0.047, val_acc:0.957]
Epoch [89/120    avg_loss:0.025, val_acc:0.974]
Epoch [90/120    avg_loss:0.012, val_acc:0.976]
Epoch [91/120    avg_loss:0.010, val_acc:0.976]
Epoch [92/120    avg_loss:0.010, val_acc:0.977]
Epoch [93/120    avg_loss:0.009, val_acc:0.978]
Epoch [94/120    avg_loss:0.007, val_acc:0.977]
Epoch [95/120    avg_loss:0.008, val_acc:0.978]
Epoch [96/120    avg_loss:0.013, val_acc:0.978]
Epoch [97/120    avg_loss:0.011, val_acc:0.977]
Epoch [98/120    avg_loss:0.008, val_acc:0.978]
Epoch [99/120    avg_loss:0.008, val_acc:0.979]
Epoch [100/120    avg_loss:0.012, val_acc:0.981]
Epoch [101/120    avg_loss:0.010, val_acc:0.980]
Epoch [102/120    avg_loss:0.007, val_acc:0.981]
Epoch [103/120    avg_loss:0.006, val_acc:0.980]
Epoch [104/120    avg_loss:0.009, val_acc:0.980]
Epoch [105/120    avg_loss:0.010, val_acc:0.980]
Epoch [106/120    avg_loss:0.010, val_acc:0.980]
Epoch [107/120    avg_loss:0.014, val_acc:0.980]
Epoch [108/120    avg_loss:0.007, val_acc:0.980]
Epoch [109/120    avg_loss:0.007, val_acc:0.980]
Epoch [110/120    avg_loss:0.008, val_acc:0.980]
Epoch [111/120    avg_loss:0.011, val_acc:0.980]
Epoch [112/120    avg_loss:0.010, val_acc:0.980]
Epoch [113/120    avg_loss:0.008, val_acc:0.980]
Epoch [114/120    avg_loss:0.007, val_acc:0.980]
Epoch [115/120    avg_loss:0.008, val_acc:0.981]
Epoch [116/120    avg_loss:0.007, val_acc:0.981]
Epoch [117/120    avg_loss:0.010, val_acc:0.981]
Epoch [118/120    avg_loss:0.007, val_acc:0.981]
Epoch [119/120    avg_loss:0.007, val_acc:0.981]
Epoch [120/120    avg_loss:0.016, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1274    0    0    0    0    0    0    0    3    5    2    0
     0    1    0]
 [   0    0    0  689    0   28    0    0    0    7    1    0   20    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0    0  843   23    0    0
     0    4    0]
 [   0    0    4    0    0    0   10    0    0    0    4 2190    0    2
     0    0    0]
 [   0    0    0    9    6    6    0    0    0    0    4    1  505    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    1    0    0    0
  1132    0    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    21  303    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.73441734417344

F1 scores:
[       nan 0.98765432 0.99375975 0.94903581 0.98611111 0.95374449
 0.97473997 0.98039216 1.         0.76190476 0.97400347 0.98871332
 0.95014111 0.98930481 0.9877836  0.92519084 0.9704142 ]

Kappa:
0.974168265096189
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f70bd28c6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.262, val_acc:0.515]
Epoch [2/120    avg_loss:1.786, val_acc:0.609]
Epoch [3/120    avg_loss:1.528, val_acc:0.601]
Epoch [4/120    avg_loss:1.135, val_acc:0.684]
Epoch [5/120    avg_loss:0.991, val_acc:0.748]
Epoch [6/120    avg_loss:0.867, val_acc:0.748]
Epoch [7/120    avg_loss:0.731, val_acc:0.810]
Epoch [8/120    avg_loss:0.669, val_acc:0.832]
Epoch [9/120    avg_loss:0.575, val_acc:0.827]
Epoch [10/120    avg_loss:0.589, val_acc:0.848]
Epoch [11/120    avg_loss:0.516, val_acc:0.788]
Epoch [12/120    avg_loss:0.407, val_acc:0.830]
Epoch [13/120    avg_loss:0.333, val_acc:0.880]
Epoch [14/120    avg_loss:0.361, val_acc:0.841]
Epoch [15/120    avg_loss:0.309, val_acc:0.925]
Epoch [16/120    avg_loss:0.300, val_acc:0.877]
Epoch [17/120    avg_loss:0.264, val_acc:0.888]
Epoch [18/120    avg_loss:0.228, val_acc:0.908]
Epoch [19/120    avg_loss:0.179, val_acc:0.860]
Epoch [20/120    avg_loss:0.240, val_acc:0.916]
Epoch [21/120    avg_loss:0.126, val_acc:0.922]
Epoch [22/120    avg_loss:0.141, val_acc:0.931]
Epoch [23/120    avg_loss:0.123, val_acc:0.946]
Epoch [24/120    avg_loss:0.129, val_acc:0.891]
Epoch [25/120    avg_loss:0.094, val_acc:0.946]
Epoch [26/120    avg_loss:0.091, val_acc:0.955]
Epoch [27/120    avg_loss:0.082, val_acc:0.949]
Epoch [28/120    avg_loss:0.069, val_acc:0.935]
Epoch [29/120    avg_loss:0.101, val_acc:0.934]
Epoch [30/120    avg_loss:0.098, val_acc:0.954]
Epoch [31/120    avg_loss:0.091, val_acc:0.952]
Epoch [32/120    avg_loss:0.097, val_acc:0.947]
Epoch [33/120    avg_loss:0.056, val_acc:0.968]
Epoch [34/120    avg_loss:0.050, val_acc:0.966]
Epoch [35/120    avg_loss:0.077, val_acc:0.966]
Epoch [36/120    avg_loss:0.048, val_acc:0.973]
Epoch [37/120    avg_loss:0.050, val_acc:0.968]
Epoch [38/120    avg_loss:0.044, val_acc:0.975]
Epoch [39/120    avg_loss:0.065, val_acc:0.971]
Epoch [40/120    avg_loss:0.054, val_acc:0.962]
Epoch [41/120    avg_loss:0.065, val_acc:0.972]
Epoch [42/120    avg_loss:0.071, val_acc:0.956]
Epoch [43/120    avg_loss:0.056, val_acc:0.964]
Epoch [44/120    avg_loss:0.052, val_acc:0.958]
Epoch [45/120    avg_loss:0.036, val_acc:0.979]
Epoch [46/120    avg_loss:0.026, val_acc:0.975]
Epoch [47/120    avg_loss:0.038, val_acc:0.971]
Epoch [48/120    avg_loss:0.036, val_acc:0.969]
Epoch [49/120    avg_loss:0.039, val_acc:0.975]
Epoch [50/120    avg_loss:0.036, val_acc:0.967]
Epoch [51/120    avg_loss:0.045, val_acc:0.976]
Epoch [52/120    avg_loss:0.049, val_acc:0.964]
Epoch [53/120    avg_loss:0.044, val_acc:0.966]
Epoch [54/120    avg_loss:0.029, val_acc:0.974]
Epoch [55/120    avg_loss:0.036, val_acc:0.978]
Epoch [56/120    avg_loss:0.030, val_acc:0.965]
Epoch [57/120    avg_loss:0.025, val_acc:0.978]
Epoch [58/120    avg_loss:0.022, val_acc:0.976]
Epoch [59/120    avg_loss:0.022, val_acc:0.979]
Epoch [60/120    avg_loss:0.019, val_acc:0.978]
Epoch [61/120    avg_loss:0.014, val_acc:0.982]
Epoch [62/120    avg_loss:0.015, val_acc:0.982]
Epoch [63/120    avg_loss:0.014, val_acc:0.982]
Epoch [64/120    avg_loss:0.016, val_acc:0.985]
Epoch [65/120    avg_loss:0.014, val_acc:0.986]
Epoch [66/120    avg_loss:0.018, val_acc:0.985]
Epoch [67/120    avg_loss:0.015, val_acc:0.985]
Epoch [68/120    avg_loss:0.014, val_acc:0.984]
Epoch [69/120    avg_loss:0.009, val_acc:0.984]
Epoch [70/120    avg_loss:0.013, val_acc:0.984]
Epoch [71/120    avg_loss:0.012, val_acc:0.986]
Epoch [72/120    avg_loss:0.012, val_acc:0.985]
Epoch [73/120    avg_loss:0.015, val_acc:0.984]
Epoch [74/120    avg_loss:0.010, val_acc:0.986]
Epoch [75/120    avg_loss:0.012, val_acc:0.986]
Epoch [76/120    avg_loss:0.010, val_acc:0.986]
Epoch [77/120    avg_loss:0.015, val_acc:0.985]
Epoch [78/120    avg_loss:0.009, val_acc:0.984]
Epoch [79/120    avg_loss:0.010, val_acc:0.984]
Epoch [80/120    avg_loss:0.010, val_acc:0.985]
Epoch [81/120    avg_loss:0.010, val_acc:0.985]
Epoch [82/120    avg_loss:0.010, val_acc:0.985]
Epoch [83/120    avg_loss:0.015, val_acc:0.985]
Epoch [84/120    avg_loss:0.012, val_acc:0.984]
Epoch [85/120    avg_loss:0.012, val_acc:0.984]
Epoch [86/120    avg_loss:0.010, val_acc:0.984]
Epoch [87/120    avg_loss:0.014, val_acc:0.986]
Epoch [88/120    avg_loss:0.012, val_acc:0.985]
Epoch [89/120    avg_loss:0.010, val_acc:0.987]
Epoch [90/120    avg_loss:0.010, val_acc:0.987]
Epoch [91/120    avg_loss:0.010, val_acc:0.987]
Epoch [92/120    avg_loss:0.010, val_acc:0.986]
Epoch [93/120    avg_loss:0.011, val_acc:0.986]
Epoch [94/120    avg_loss:0.010, val_acc:0.985]
Epoch [95/120    avg_loss:0.009, val_acc:0.986]
Epoch [96/120    avg_loss:0.011, val_acc:0.985]
Epoch [97/120    avg_loss:0.010, val_acc:0.985]
Epoch [98/120    avg_loss:0.013, val_acc:0.987]
Epoch [99/120    avg_loss:0.010, val_acc:0.987]
Epoch [100/120    avg_loss:0.010, val_acc:0.987]
Epoch [101/120    avg_loss:0.009, val_acc:0.987]
Epoch [102/120    avg_loss:0.011, val_acc:0.986]
Epoch [103/120    avg_loss:0.014, val_acc:0.987]
Epoch [104/120    avg_loss:0.011, val_acc:0.985]
Epoch [105/120    avg_loss:0.008, val_acc:0.984]
Epoch [106/120    avg_loss:0.008, val_acc:0.985]
Epoch [107/120    avg_loss:0.011, val_acc:0.986]
Epoch [108/120    avg_loss:0.012, val_acc:0.984]
Epoch [109/120    avg_loss:0.009, val_acc:0.985]
Epoch [110/120    avg_loss:0.011, val_acc:0.985]
Epoch [111/120    avg_loss:0.009, val_acc:0.986]
Epoch [112/120    avg_loss:0.011, val_acc:0.983]
Epoch [113/120    avg_loss:0.016, val_acc:0.985]
Epoch [114/120    avg_loss:0.008, val_acc:0.985]
Epoch [115/120    avg_loss:0.011, val_acc:0.985]
Epoch [116/120    avg_loss:0.015, val_acc:0.984]
Epoch [117/120    avg_loss:0.011, val_acc:0.985]
Epoch [118/120    avg_loss:0.008, val_acc:0.985]
Epoch [119/120    avg_loss:0.018, val_acc:0.985]
Epoch [120/120    avg_loss:0.008, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1274    3    0    0    1    0    0    0    3    2    2    0
     0    0    0]
 [   0    0    0  724    0   15    0    0    0    6    1    0    0    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    4    6    0    6    0    0    0    0  835   24    0    0
     0    0    0]
 [   0    0   18    4    0    0   11    0    0    0   11 2160    6    0
     0    0    0]
 [   0    0    0   20   10    8    0    0    0    0    0    0  492    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0   29    0    0    0    0    0    0    0
    26  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.46341463414635

F1 scores:
[       nan 0.975      0.98683191 0.96148738 0.97706422 0.96312849
 0.9697417  0.98039216 0.99883856 0.74418605 0.96587623 0.98271156
 0.95072464 0.99730458 0.98609904 0.91392801 0.98224852]

Kappa:
0.9710867020152769
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f345d398630>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.333, val_acc:0.462]
Epoch [2/120    avg_loss:1.619, val_acc:0.626]
Epoch [3/120    avg_loss:1.386, val_acc:0.619]
Epoch [4/120    avg_loss:1.165, val_acc:0.747]
Epoch [5/120    avg_loss:1.068, val_acc:0.697]
Epoch [6/120    avg_loss:0.910, val_acc:0.737]
Epoch [7/120    avg_loss:0.733, val_acc:0.778]
Epoch [8/120    avg_loss:0.578, val_acc:0.850]
Epoch [9/120    avg_loss:0.535, val_acc:0.840]
Epoch [10/120    avg_loss:0.570, val_acc:0.839]
Epoch [11/120    avg_loss:0.460, val_acc:0.900]
Epoch [12/120    avg_loss:0.340, val_acc:0.889]
Epoch [13/120    avg_loss:0.376, val_acc:0.830]
Epoch [14/120    avg_loss:0.349, val_acc:0.904]
Epoch [15/120    avg_loss:0.264, val_acc:0.920]
Epoch [16/120    avg_loss:0.259, val_acc:0.905]
Epoch [17/120    avg_loss:0.268, val_acc:0.870]
Epoch [18/120    avg_loss:0.264, val_acc:0.940]
Epoch [19/120    avg_loss:0.242, val_acc:0.939]
Epoch [20/120    avg_loss:0.171, val_acc:0.926]
Epoch [21/120    avg_loss:0.166, val_acc:0.941]
Epoch [22/120    avg_loss:0.138, val_acc:0.948]
Epoch [23/120    avg_loss:0.204, val_acc:0.929]
Epoch [24/120    avg_loss:0.223, val_acc:0.931]
Epoch [25/120    avg_loss:0.205, val_acc:0.947]
Epoch [26/120    avg_loss:0.122, val_acc:0.944]
Epoch [27/120    avg_loss:0.195, val_acc:0.927]
Epoch [28/120    avg_loss:0.161, val_acc:0.941]
Epoch [29/120    avg_loss:0.119, val_acc:0.965]
Epoch [30/120    avg_loss:0.087, val_acc:0.966]
Epoch [31/120    avg_loss:0.085, val_acc:0.951]
Epoch [32/120    avg_loss:0.093, val_acc:0.958]
Epoch [33/120    avg_loss:0.089, val_acc:0.959]
Epoch [34/120    avg_loss:0.102, val_acc:0.947]
Epoch [35/120    avg_loss:0.083, val_acc:0.955]
Epoch [36/120    avg_loss:0.065, val_acc:0.971]
Epoch [37/120    avg_loss:0.059, val_acc:0.966]
Epoch [38/120    avg_loss:0.068, val_acc:0.971]
Epoch [39/120    avg_loss:0.044, val_acc:0.973]
Epoch [40/120    avg_loss:0.046, val_acc:0.973]
Epoch [41/120    avg_loss:0.063, val_acc:0.972]
Epoch [42/120    avg_loss:0.046, val_acc:0.970]
Epoch [43/120    avg_loss:0.052, val_acc:0.970]
Epoch [44/120    avg_loss:0.054, val_acc:0.969]
Epoch [45/120    avg_loss:0.079, val_acc:0.970]
Epoch [46/120    avg_loss:0.070, val_acc:0.970]
Epoch [47/120    avg_loss:0.049, val_acc:0.965]
Epoch [48/120    avg_loss:0.047, val_acc:0.975]
Epoch [49/120    avg_loss:0.059, val_acc:0.973]
Epoch [50/120    avg_loss:0.049, val_acc:0.971]
Epoch [51/120    avg_loss:0.041, val_acc:0.980]
Epoch [52/120    avg_loss:0.032, val_acc:0.977]
Epoch [53/120    avg_loss:0.036, val_acc:0.976]
Epoch [54/120    avg_loss:0.038, val_acc:0.977]
Epoch [55/120    avg_loss:0.085, val_acc:0.931]
Epoch [56/120    avg_loss:0.106, val_acc:0.956]
Epoch [57/120    avg_loss:0.047, val_acc:0.973]
Epoch [58/120    avg_loss:0.059, val_acc:0.972]
Epoch [59/120    avg_loss:0.043, val_acc:0.950]
Epoch [60/120    avg_loss:0.045, val_acc:0.983]
Epoch [61/120    avg_loss:0.039, val_acc:0.959]
Epoch [62/120    avg_loss:0.056, val_acc:0.977]
Epoch [63/120    avg_loss:0.028, val_acc:0.972]
Epoch [64/120    avg_loss:0.028, val_acc:0.980]
Epoch [65/120    avg_loss:0.036, val_acc:0.986]
Epoch [66/120    avg_loss:0.029, val_acc:0.977]
Epoch [67/120    avg_loss:0.041, val_acc:0.979]
Epoch [68/120    avg_loss:0.036, val_acc:0.980]
Epoch [69/120    avg_loss:0.024, val_acc:0.981]
Epoch [70/120    avg_loss:0.026, val_acc:0.986]
Epoch [71/120    avg_loss:0.021, val_acc:0.985]
Epoch [72/120    avg_loss:0.018, val_acc:0.986]
Epoch [73/120    avg_loss:0.016, val_acc:0.990]
Epoch [74/120    avg_loss:0.030, val_acc:0.970]
Epoch [75/120    avg_loss:0.020, val_acc:0.985]
Epoch [76/120    avg_loss:0.023, val_acc:0.986]
Epoch [77/120    avg_loss:0.015, val_acc:0.984]
Epoch [78/120    avg_loss:0.013, val_acc:0.986]
Epoch [79/120    avg_loss:0.028, val_acc:0.979]
Epoch [80/120    avg_loss:0.017, val_acc:0.988]
Epoch [81/120    avg_loss:0.012, val_acc:0.987]
Epoch [82/120    avg_loss:0.020, val_acc:0.987]
Epoch [83/120    avg_loss:0.016, val_acc:0.983]
Epoch [84/120    avg_loss:0.017, val_acc:0.986]
Epoch [85/120    avg_loss:0.013, val_acc:0.986]
Epoch [86/120    avg_loss:0.011, val_acc:0.987]
Epoch [87/120    avg_loss:0.011, val_acc:0.987]
Epoch [88/120    avg_loss:0.007, val_acc:0.987]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.010, val_acc:0.987]
Epoch [91/120    avg_loss:0.006, val_acc:0.987]
Epoch [92/120    avg_loss:0.008, val_acc:0.987]
Epoch [93/120    avg_loss:0.005, val_acc:0.987]
Epoch [94/120    avg_loss:0.007, val_acc:0.987]
Epoch [95/120    avg_loss:0.012, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.987]
Epoch [97/120    avg_loss:0.006, val_acc:0.987]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.009, val_acc:0.987]
Epoch [100/120    avg_loss:0.007, val_acc:0.987]
Epoch [101/120    avg_loss:0.008, val_acc:0.986]
Epoch [102/120    avg_loss:0.008, val_acc:0.986]
Epoch [103/120    avg_loss:0.007, val_acc:0.986]
Epoch [104/120    avg_loss:0.008, val_acc:0.986]
Epoch [105/120    avg_loss:0.007, val_acc:0.986]
Epoch [106/120    avg_loss:0.008, val_acc:0.986]
Epoch [107/120    avg_loss:0.006, val_acc:0.986]
Epoch [108/120    avg_loss:0.009, val_acc:0.986]
Epoch [109/120    avg_loss:0.009, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.007, val_acc:0.986]
Epoch [112/120    avg_loss:0.006, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.008, val_acc:0.986]
Epoch [115/120    avg_loss:0.006, val_acc:0.986]
Epoch [116/120    avg_loss:0.008, val_acc:0.986]
Epoch [117/120    avg_loss:0.007, val_acc:0.986]
Epoch [118/120    avg_loss:0.010, val_acc:0.986]
Epoch [119/120    avg_loss:0.006, val_acc:0.986]
Epoch [120/120    avg_loss:0.007, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    3 1265    0    0    0    1    0    0    0    9    6    1    0
     0    0    0]
 [   0    0    0  722    0   15    0    0    0    9    0    0    1    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    5    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   13    0    0    1    0
     0    0    0]
 [   0    0    3   61    0    2    4    0    0    0  779   22    0    0
     1    3    0]
 [   0    0    5    0    0    0    8    0    0    0    8 2188    0    1
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    6    0  516    0
     1    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    8    0    0    0    0    1    0    0    0
  1130    0    0]
 [   0    0    0    0    0    0   29    0    0    0    0    0    0    6
    28  284    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.09485094850949

F1 scores:
[       nan 0.95238095 0.98866745 0.94132986 1.         0.95856663
 0.96826568 0.98039216 1.         0.57777778 0.92848629 0.98847978
 0.97912713 0.98143236 0.9826087  0.89589905 0.95953757]

Kappa:
0.9668769310988317
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:11:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2504207780>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.303, val_acc:0.511]
Epoch [2/120    avg_loss:1.817, val_acc:0.646]
Epoch [3/120    avg_loss:1.198, val_acc:0.704]
Epoch [4/120    avg_loss:0.873, val_acc:0.716]
Epoch [5/120    avg_loss:0.994, val_acc:0.756]
Epoch [6/120    avg_loss:0.779, val_acc:0.734]
Epoch [7/120    avg_loss:0.702, val_acc:0.816]
Epoch [8/120    avg_loss:0.541, val_acc:0.811]
Epoch [9/120    avg_loss:0.476, val_acc:0.848]
Epoch [10/120    avg_loss:0.445, val_acc:0.824]
Epoch [11/120    avg_loss:0.364, val_acc:0.872]
Epoch [12/120    avg_loss:0.341, val_acc:0.861]
Epoch [13/120    avg_loss:0.475, val_acc:0.839]
Epoch [14/120    avg_loss:0.312, val_acc:0.848]
Epoch [15/120    avg_loss:0.297, val_acc:0.869]
Epoch [16/120    avg_loss:0.296, val_acc:0.913]
Epoch [17/120    avg_loss:0.256, val_acc:0.894]
Epoch [18/120    avg_loss:0.223, val_acc:0.920]
Epoch [19/120    avg_loss:0.186, val_acc:0.922]
Epoch [20/120    avg_loss:0.248, val_acc:0.914]
Epoch [21/120    avg_loss:0.182, val_acc:0.905]
Epoch [22/120    avg_loss:0.185, val_acc:0.939]
Epoch [23/120    avg_loss:0.170, val_acc:0.935]
Epoch [24/120    avg_loss:0.115, val_acc:0.944]
Epoch [25/120    avg_loss:0.105, val_acc:0.927]
Epoch [26/120    avg_loss:0.130, val_acc:0.950]
Epoch [27/120    avg_loss:0.109, val_acc:0.936]
Epoch [28/120    avg_loss:0.095, val_acc:0.929]
Epoch [29/120    avg_loss:0.093, val_acc:0.967]
Epoch [30/120    avg_loss:0.089, val_acc:0.952]
Epoch [31/120    avg_loss:0.105, val_acc:0.932]
Epoch [32/120    avg_loss:0.099, val_acc:0.954]
Epoch [33/120    avg_loss:0.089, val_acc:0.950]
Epoch [34/120    avg_loss:0.109, val_acc:0.957]
Epoch [35/120    avg_loss:0.074, val_acc:0.959]
Epoch [36/120    avg_loss:0.059, val_acc:0.962]
Epoch [37/120    avg_loss:0.067, val_acc:0.948]
Epoch [38/120    avg_loss:0.067, val_acc:0.955]
Epoch [39/120    avg_loss:0.079, val_acc:0.964]
Epoch [40/120    avg_loss:0.089, val_acc:0.949]
Epoch [41/120    avg_loss:0.064, val_acc:0.962]
Epoch [42/120    avg_loss:0.055, val_acc:0.980]
Epoch [43/120    avg_loss:0.062, val_acc:0.963]
Epoch [44/120    avg_loss:0.045, val_acc:0.976]
Epoch [45/120    avg_loss:0.054, val_acc:0.970]
Epoch [46/120    avg_loss:0.098, val_acc:0.955]
Epoch [47/120    avg_loss:0.053, val_acc:0.974]
Epoch [48/120    avg_loss:0.058, val_acc:0.970]
Epoch [49/120    avg_loss:0.040, val_acc:0.981]
Epoch [50/120    avg_loss:0.043, val_acc:0.963]
Epoch [51/120    avg_loss:0.037, val_acc:0.972]
Epoch [52/120    avg_loss:0.056, val_acc:0.958]
Epoch [53/120    avg_loss:0.041, val_acc:0.967]
Epoch [54/120    avg_loss:0.065, val_acc:0.964]
Epoch [55/120    avg_loss:0.043, val_acc:0.984]
Epoch [56/120    avg_loss:0.045, val_acc:0.975]
Epoch [57/120    avg_loss:0.043, val_acc:0.972]
Epoch [58/120    avg_loss:0.043, val_acc:0.975]
Epoch [59/120    avg_loss:0.028, val_acc:0.981]
Epoch [60/120    avg_loss:0.032, val_acc:0.973]
Epoch [61/120    avg_loss:0.022, val_acc:0.980]
Epoch [62/120    avg_loss:0.028, val_acc:0.973]
Epoch [63/120    avg_loss:0.025, val_acc:0.976]
Epoch [64/120    avg_loss:0.026, val_acc:0.984]
Epoch [65/120    avg_loss:0.028, val_acc:0.975]
Epoch [66/120    avg_loss:0.026, val_acc:0.977]
Epoch [67/120    avg_loss:0.037, val_acc:0.975]
Epoch [68/120    avg_loss:0.032, val_acc:0.973]
Epoch [69/120    avg_loss:0.023, val_acc:0.989]
Epoch [70/120    avg_loss:0.018, val_acc:0.968]
Epoch [71/120    avg_loss:0.027, val_acc:0.972]
Epoch [72/120    avg_loss:0.058, val_acc:0.957]
Epoch [73/120    avg_loss:0.046, val_acc:0.968]
Epoch [74/120    avg_loss:0.074, val_acc:0.967]
Epoch [75/120    avg_loss:0.048, val_acc:0.971]
Epoch [76/120    avg_loss:0.031, val_acc:0.974]
Epoch [77/120    avg_loss:0.019, val_acc:0.977]
Epoch [78/120    avg_loss:0.023, val_acc:0.973]
Epoch [79/120    avg_loss:0.021, val_acc:0.982]
Epoch [80/120    avg_loss:0.030, val_acc:0.981]
Epoch [81/120    avg_loss:0.029, val_acc:0.972]
Epoch [82/120    avg_loss:0.021, val_acc:0.973]
Epoch [83/120    avg_loss:0.019, val_acc:0.981]
Epoch [84/120    avg_loss:0.013, val_acc:0.983]
Epoch [85/120    avg_loss:0.010, val_acc:0.985]
Epoch [86/120    avg_loss:0.013, val_acc:0.984]
Epoch [87/120    avg_loss:0.011, val_acc:0.983]
Epoch [88/120    avg_loss:0.010, val_acc:0.984]
Epoch [89/120    avg_loss:0.013, val_acc:0.983]
Epoch [90/120    avg_loss:0.010, val_acc:0.983]
Epoch [91/120    avg_loss:0.011, val_acc:0.983]
Epoch [92/120    avg_loss:0.013, val_acc:0.986]
Epoch [93/120    avg_loss:0.011, val_acc:0.985]
Epoch [94/120    avg_loss:0.009, val_acc:0.984]
Epoch [95/120    avg_loss:0.008, val_acc:0.985]
Epoch [96/120    avg_loss:0.010, val_acc:0.985]
Epoch [97/120    avg_loss:0.011, val_acc:0.985]
Epoch [98/120    avg_loss:0.009, val_acc:0.985]
Epoch [99/120    avg_loss:0.009, val_acc:0.985]
Epoch [100/120    avg_loss:0.011, val_acc:0.984]
Epoch [101/120    avg_loss:0.007, val_acc:0.985]
Epoch [102/120    avg_loss:0.007, val_acc:0.985]
Epoch [103/120    avg_loss:0.012, val_acc:0.985]
Epoch [104/120    avg_loss:0.008, val_acc:0.985]
Epoch [105/120    avg_loss:0.011, val_acc:0.985]
Epoch [106/120    avg_loss:0.008, val_acc:0.985]
Epoch [107/120    avg_loss:0.008, val_acc:0.985]
Epoch [108/120    avg_loss:0.008, val_acc:0.985]
Epoch [109/120    avg_loss:0.009, val_acc:0.985]
Epoch [110/120    avg_loss:0.008, val_acc:0.985]
Epoch [111/120    avg_loss:0.011, val_acc:0.985]
Epoch [112/120    avg_loss:0.008, val_acc:0.985]
Epoch [113/120    avg_loss:0.010, val_acc:0.985]
Epoch [114/120    avg_loss:0.010, val_acc:0.985]
Epoch [115/120    avg_loss:0.007, val_acc:0.984]
Epoch [116/120    avg_loss:0.008, val_acc:0.984]
Epoch [117/120    avg_loss:0.011, val_acc:0.984]
Epoch [118/120    avg_loss:0.008, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.985]
Epoch [120/120    avg_loss:0.008, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1262    3    0    0    1    0    0    0    3    6    6    0
     0    4    0]
 [   0    0    0  718    5   12    0    0    0    7    0    0    3    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   14    0    0    1    0
     0    0    0]
 [   0    0    0   74    0    0    0    0    0    0  771   20    2    0
     3    5    0]
 [   0    0    3    0    0    0    6    0    0    0    2 2197    0    2
     0    0    0]
 [   0    0    0    6    3    3    0    0    0    0    6    3  504    0
     5    0    4]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    32  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.21409214092141

F1 scores:
[       nan 0.98765432 0.98941592 0.92585429 0.98156682 0.98079096
 0.98424606 1.         0.99883856 0.7        0.93003619 0.99030877
 0.95726496 0.98659517 0.98186528 0.91793313 0.95857988]

Kappa:
0.9682296824475805
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fedfdfe86d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.325, val_acc:0.480]
Epoch [2/120    avg_loss:1.849, val_acc:0.532]
Epoch [3/120    avg_loss:1.401, val_acc:0.659]
Epoch [4/120    avg_loss:1.097, val_acc:0.706]
Epoch [5/120    avg_loss:0.904, val_acc:0.695]
Epoch [6/120    avg_loss:0.845, val_acc:0.804]
Epoch [7/120    avg_loss:0.738, val_acc:0.820]
Epoch [8/120    avg_loss:0.688, val_acc:0.807]
Epoch [9/120    avg_loss:0.513, val_acc:0.833]
Epoch [10/120    avg_loss:0.520, val_acc:0.778]
Epoch [11/120    avg_loss:0.471, val_acc:0.875]
Epoch [12/120    avg_loss:0.354, val_acc:0.865]
Epoch [13/120    avg_loss:0.287, val_acc:0.884]
Epoch [14/120    avg_loss:0.448, val_acc:0.854]
Epoch [15/120    avg_loss:0.357, val_acc:0.911]
Epoch [16/120    avg_loss:0.235, val_acc:0.927]
Epoch [17/120    avg_loss:0.257, val_acc:0.914]
Epoch [18/120    avg_loss:0.251, val_acc:0.899]
Epoch [19/120    avg_loss:0.306, val_acc:0.911]
Epoch [20/120    avg_loss:0.189, val_acc:0.919]
Epoch [21/120    avg_loss:0.205, val_acc:0.922]
Epoch [22/120    avg_loss:0.189, val_acc:0.946]
Epoch [23/120    avg_loss:0.143, val_acc:0.948]
Epoch [24/120    avg_loss:0.124, val_acc:0.952]
Epoch [25/120    avg_loss:0.110, val_acc:0.957]
Epoch [26/120    avg_loss:0.141, val_acc:0.940]
Epoch [27/120    avg_loss:0.133, val_acc:0.943]
Epoch [28/120    avg_loss:0.101, val_acc:0.957]
Epoch [29/120    avg_loss:0.098, val_acc:0.971]
Epoch [30/120    avg_loss:0.121, val_acc:0.957]
Epoch [31/120    avg_loss:0.135, val_acc:0.949]
Epoch [32/120    avg_loss:0.103, val_acc:0.964]
Epoch [33/120    avg_loss:0.107, val_acc:0.963]
Epoch [34/120    avg_loss:0.080, val_acc:0.959]
Epoch [35/120    avg_loss:0.092, val_acc:0.965]
Epoch [36/120    avg_loss:0.089, val_acc:0.948]
Epoch [37/120    avg_loss:0.061, val_acc:0.962]
Epoch [38/120    avg_loss:0.065, val_acc:0.967]
Epoch [39/120    avg_loss:0.077, val_acc:0.962]
Epoch [40/120    avg_loss:0.076, val_acc:0.965]
Epoch [41/120    avg_loss:0.070, val_acc:0.948]
Epoch [42/120    avg_loss:0.081, val_acc:0.959]
Epoch [43/120    avg_loss:0.057, val_acc:0.971]
Epoch [44/120    avg_loss:0.031, val_acc:0.977]
Epoch [45/120    avg_loss:0.028, val_acc:0.979]
Epoch [46/120    avg_loss:0.037, val_acc:0.977]
Epoch [47/120    avg_loss:0.031, val_acc:0.980]
Epoch [48/120    avg_loss:0.028, val_acc:0.977]
Epoch [49/120    avg_loss:0.027, val_acc:0.979]
Epoch [50/120    avg_loss:0.030, val_acc:0.980]
Epoch [51/120    avg_loss:0.029, val_acc:0.979]
Epoch [52/120    avg_loss:0.023, val_acc:0.976]
Epoch [53/120    avg_loss:0.028, val_acc:0.977]
Epoch [54/120    avg_loss:0.031, val_acc:0.980]
Epoch [55/120    avg_loss:0.031, val_acc:0.980]
Epoch [56/120    avg_loss:0.029, val_acc:0.981]
Epoch [57/120    avg_loss:0.022, val_acc:0.981]
Epoch [58/120    avg_loss:0.025, val_acc:0.983]
Epoch [59/120    avg_loss:0.028, val_acc:0.982]
Epoch [60/120    avg_loss:0.029, val_acc:0.982]
Epoch [61/120    avg_loss:0.031, val_acc:0.982]
Epoch [62/120    avg_loss:0.024, val_acc:0.982]
Epoch [63/120    avg_loss:0.027, val_acc:0.980]
Epoch [64/120    avg_loss:0.022, val_acc:0.981]
Epoch [65/120    avg_loss:0.026, val_acc:0.981]
Epoch [66/120    avg_loss:0.021, val_acc:0.982]
Epoch [67/120    avg_loss:0.026, val_acc:0.981]
Epoch [68/120    avg_loss:0.022, val_acc:0.982]
Epoch [69/120    avg_loss:0.036, val_acc:0.981]
Epoch [70/120    avg_loss:0.023, val_acc:0.981]
Epoch [71/120    avg_loss:0.025, val_acc:0.982]
Epoch [72/120    avg_loss:0.021, val_acc:0.982]
Epoch [73/120    avg_loss:0.020, val_acc:0.981]
Epoch [74/120    avg_loss:0.023, val_acc:0.981]
Epoch [75/120    avg_loss:0.019, val_acc:0.981]
Epoch [76/120    avg_loss:0.023, val_acc:0.981]
Epoch [77/120    avg_loss:0.019, val_acc:0.981]
Epoch [78/120    avg_loss:0.018, val_acc:0.981]
Epoch [79/120    avg_loss:0.022, val_acc:0.981]
Epoch [80/120    avg_loss:0.020, val_acc:0.981]
Epoch [81/120    avg_loss:0.022, val_acc:0.981]
Epoch [82/120    avg_loss:0.025, val_acc:0.982]
Epoch [83/120    avg_loss:0.024, val_acc:0.982]
Epoch [84/120    avg_loss:0.021, val_acc:0.982]
Epoch [85/120    avg_loss:0.020, val_acc:0.982]
Epoch [86/120    avg_loss:0.019, val_acc:0.982]
Epoch [87/120    avg_loss:0.028, val_acc:0.982]
Epoch [88/120    avg_loss:0.023, val_acc:0.982]
Epoch [89/120    avg_loss:0.025, val_acc:0.982]
Epoch [90/120    avg_loss:0.020, val_acc:0.982]
Epoch [91/120    avg_loss:0.020, val_acc:0.982]
Epoch [92/120    avg_loss:0.025, val_acc:0.982]
Epoch [93/120    avg_loss:0.022, val_acc:0.982]
Epoch [94/120    avg_loss:0.024, val_acc:0.982]
Epoch [95/120    avg_loss:0.019, val_acc:0.982]
Epoch [96/120    avg_loss:0.022, val_acc:0.982]
Epoch [97/120    avg_loss:0.032, val_acc:0.982]
Epoch [98/120    avg_loss:0.022, val_acc:0.982]
Epoch [99/120    avg_loss:0.022, val_acc:0.982]
Epoch [100/120    avg_loss:0.024, val_acc:0.982]
Epoch [101/120    avg_loss:0.024, val_acc:0.982]
Epoch [102/120    avg_loss:0.021, val_acc:0.982]
Epoch [103/120    avg_loss:0.026, val_acc:0.982]
Epoch [104/120    avg_loss:0.024, val_acc:0.982]
Epoch [105/120    avg_loss:0.021, val_acc:0.982]
Epoch [106/120    avg_loss:0.019, val_acc:0.982]
Epoch [107/120    avg_loss:0.031, val_acc:0.982]
Epoch [108/120    avg_loss:0.021, val_acc:0.982]
Epoch [109/120    avg_loss:0.020, val_acc:0.982]
Epoch [110/120    avg_loss:0.021, val_acc:0.982]
Epoch [111/120    avg_loss:0.022, val_acc:0.982]
Epoch [112/120    avg_loss:0.021, val_acc:0.982]
Epoch [113/120    avg_loss:0.028, val_acc:0.982]
Epoch [114/120    avg_loss:0.021, val_acc:0.982]
Epoch [115/120    avg_loss:0.021, val_acc:0.982]
Epoch [116/120    avg_loss:0.021, val_acc:0.982]
Epoch [117/120    avg_loss:0.021, val_acc:0.982]
Epoch [118/120    avg_loss:0.021, val_acc:0.982]
Epoch [119/120    avg_loss:0.025, val_acc:0.982]
Epoch [120/120    avg_loss:0.023, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1259    2    0    0    1    0    0    0    7    5    5    0
     0    6    0]
 [   0    0    0  722    0   14    0    0    0   10    0    0    1    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   12    0    0    3    0
     0    0    0]
 [   0    0    0   78    0    5    0    0    0    0  769   19    0    0
     1    3    0]
 [   0    0    8    0   15    5   12    0    0    0   11 2122   35    2
     0    0    0]
 [   0    0    0    8    4   10    0    0    0    0    7    0  501    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    3    0    0    1    0    3    1    0    0
  1131    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
   119  228    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.4579945799458

F1 scores:
[       nan 0.975      0.98667712 0.92564103 0.95730337 0.95575221
 0.98867925 0.98039216 0.99883856 0.57142857 0.91875747 0.97406472
 0.92777778 0.99462366 0.94565217 0.78082192 0.97076023]

Kappa:
0.9482546615384103
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:05--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fd8d1db06a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.386, val_acc:0.437]
Epoch [2/120    avg_loss:1.819, val_acc:0.572]
Epoch [3/120    avg_loss:1.428, val_acc:0.707]
Epoch [4/120    avg_loss:1.163, val_acc:0.690]
Epoch [5/120    avg_loss:0.804, val_acc:0.793]
Epoch [6/120    avg_loss:0.729, val_acc:0.717]
Epoch [7/120    avg_loss:0.808, val_acc:0.806]
Epoch [8/120    avg_loss:0.643, val_acc:0.759]
Epoch [9/120    avg_loss:0.608, val_acc:0.851]
Epoch [10/120    avg_loss:0.411, val_acc:0.867]
Epoch [11/120    avg_loss:0.436, val_acc:0.852]
Epoch [12/120    avg_loss:0.385, val_acc:0.893]
Epoch [13/120    avg_loss:0.447, val_acc:0.856]
Epoch [14/120    avg_loss:0.346, val_acc:0.881]
Epoch [15/120    avg_loss:0.346, val_acc:0.911]
Epoch [16/120    avg_loss:0.269, val_acc:0.899]
Epoch [17/120    avg_loss:0.309, val_acc:0.878]
Epoch [18/120    avg_loss:0.240, val_acc:0.899]
Epoch [19/120    avg_loss:0.200, val_acc:0.926]
Epoch [20/120    avg_loss:0.171, val_acc:0.940]
Epoch [21/120    avg_loss:0.134, val_acc:0.939]
Epoch [22/120    avg_loss:0.167, val_acc:0.909]
Epoch [23/120    avg_loss:0.154, val_acc:0.920]
Epoch [24/120    avg_loss:0.148, val_acc:0.931]
Epoch [25/120    avg_loss:0.256, val_acc:0.881]
Epoch [26/120    avg_loss:0.237, val_acc:0.921]
Epoch [27/120    avg_loss:0.130, val_acc:0.928]
Epoch [28/120    avg_loss:0.188, val_acc:0.910]
Epoch [29/120    avg_loss:0.196, val_acc:0.943]
Epoch [30/120    avg_loss:0.099, val_acc:0.947]
Epoch [31/120    avg_loss:0.103, val_acc:0.932]
Epoch [32/120    avg_loss:0.119, val_acc:0.955]
Epoch [33/120    avg_loss:0.105, val_acc:0.948]
Epoch [34/120    avg_loss:0.107, val_acc:0.945]
Epoch [35/120    avg_loss:0.081, val_acc:0.954]
Epoch [36/120    avg_loss:0.105, val_acc:0.949]
Epoch [37/120    avg_loss:0.090, val_acc:0.958]
Epoch [38/120    avg_loss:0.075, val_acc:0.970]
Epoch [39/120    avg_loss:0.057, val_acc:0.970]
Epoch [40/120    avg_loss:0.053, val_acc:0.959]
Epoch [41/120    avg_loss:0.049, val_acc:0.959]
Epoch [42/120    avg_loss:0.073, val_acc:0.943]
Epoch [43/120    avg_loss:0.103, val_acc:0.948]
Epoch [44/120    avg_loss:0.104, val_acc:0.948]
Epoch [45/120    avg_loss:0.113, val_acc:0.931]
Epoch [46/120    avg_loss:0.113, val_acc:0.954]
Epoch [47/120    avg_loss:0.069, val_acc:0.957]
Epoch [48/120    avg_loss:0.062, val_acc:0.965]
Epoch [49/120    avg_loss:0.052, val_acc:0.952]
Epoch [50/120    avg_loss:0.031, val_acc:0.973]
Epoch [51/120    avg_loss:0.042, val_acc:0.959]
Epoch [52/120    avg_loss:0.049, val_acc:0.963]
Epoch [53/120    avg_loss:0.030, val_acc:0.968]
Epoch [54/120    avg_loss:0.037, val_acc:0.966]
Epoch [55/120    avg_loss:0.037, val_acc:0.965]
Epoch [56/120    avg_loss:0.037, val_acc:0.967]
Epoch [57/120    avg_loss:0.040, val_acc:0.966]
Epoch [58/120    avg_loss:0.036, val_acc:0.972]
Epoch [59/120    avg_loss:0.034, val_acc:0.952]
Epoch [60/120    avg_loss:0.047, val_acc:0.971]
Epoch [61/120    avg_loss:0.025, val_acc:0.971]
Epoch [62/120    avg_loss:0.021, val_acc:0.968]
Epoch [63/120    avg_loss:0.035, val_acc:0.962]
Epoch [64/120    avg_loss:0.022, val_acc:0.970]
Epoch [65/120    avg_loss:0.018, val_acc:0.977]
Epoch [66/120    avg_loss:0.018, val_acc:0.979]
Epoch [67/120    avg_loss:0.018, val_acc:0.981]
Epoch [68/120    avg_loss:0.022, val_acc:0.975]
Epoch [69/120    avg_loss:0.032, val_acc:0.980]
Epoch [70/120    avg_loss:0.014, val_acc:0.982]
Epoch [71/120    avg_loss:0.017, val_acc:0.981]
Epoch [72/120    avg_loss:0.016, val_acc:0.980]
Epoch [73/120    avg_loss:0.014, val_acc:0.980]
Epoch [74/120    avg_loss:0.012, val_acc:0.981]
Epoch [75/120    avg_loss:0.015, val_acc:0.979]
Epoch [76/120    avg_loss:0.014, val_acc:0.979]
Epoch [77/120    avg_loss:0.013, val_acc:0.980]
Epoch [78/120    avg_loss:0.014, val_acc:0.979]
Epoch [79/120    avg_loss:0.011, val_acc:0.981]
Epoch [80/120    avg_loss:0.019, val_acc:0.979]
Epoch [81/120    avg_loss:0.012, val_acc:0.977]
Epoch [82/120    avg_loss:0.017, val_acc:0.979]
Epoch [83/120    avg_loss:0.015, val_acc:0.975]
Epoch [84/120    avg_loss:0.021, val_acc:0.976]
Epoch [85/120    avg_loss:0.013, val_acc:0.976]
Epoch [86/120    avg_loss:0.011, val_acc:0.977]
Epoch [87/120    avg_loss:0.011, val_acc:0.977]
Epoch [88/120    avg_loss:0.013, val_acc:0.977]
Epoch [89/120    avg_loss:0.013, val_acc:0.977]
Epoch [90/120    avg_loss:0.012, val_acc:0.977]
Epoch [91/120    avg_loss:0.012, val_acc:0.977]
Epoch [92/120    avg_loss:0.015, val_acc:0.977]
Epoch [93/120    avg_loss:0.009, val_acc:0.977]
Epoch [94/120    avg_loss:0.016, val_acc:0.977]
Epoch [95/120    avg_loss:0.011, val_acc:0.976]
Epoch [96/120    avg_loss:0.015, val_acc:0.976]
Epoch [97/120    avg_loss:0.011, val_acc:0.976]
Epoch [98/120    avg_loss:0.015, val_acc:0.976]
Epoch [99/120    avg_loss:0.011, val_acc:0.976]
Epoch [100/120    avg_loss:0.012, val_acc:0.976]
Epoch [101/120    avg_loss:0.013, val_acc:0.976]
Epoch [102/120    avg_loss:0.013, val_acc:0.976]
Epoch [103/120    avg_loss:0.014, val_acc:0.976]
Epoch [104/120    avg_loss:0.013, val_acc:0.976]
Epoch [105/120    avg_loss:0.014, val_acc:0.976]
Epoch [106/120    avg_loss:0.013, val_acc:0.976]
Epoch [107/120    avg_loss:0.020, val_acc:0.976]
Epoch [108/120    avg_loss:0.017, val_acc:0.976]
Epoch [109/120    avg_loss:0.015, val_acc:0.976]
Epoch [110/120    avg_loss:0.010, val_acc:0.976]
Epoch [111/120    avg_loss:0.021, val_acc:0.976]
Epoch [112/120    avg_loss:0.015, val_acc:0.976]
Epoch [113/120    avg_loss:0.011, val_acc:0.976]
Epoch [114/120    avg_loss:0.016, val_acc:0.976]
Epoch [115/120    avg_loss:0.012, val_acc:0.976]
Epoch [116/120    avg_loss:0.016, val_acc:0.976]
Epoch [117/120    avg_loss:0.015, val_acc:0.976]
Epoch [118/120    avg_loss:0.011, val_acc:0.976]
Epoch [119/120    avg_loss:0.012, val_acc:0.976]
Epoch [120/120    avg_loss:0.013, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1259    0    0    0    2    0    0    0   14    2    5    0
     0    3    0]
 [   0    0    0  713    0   23    0    0    0    7    0    0    1    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    2    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    3   78    0    1    0    0    0    0  778   14    0    0
     0    1    0]
 [   0    0    9    0    0    0   13    0    0    0   12 2175    0    1
     0    0    0]
 [   0    0    0    1    3    5    0    0    0    0    8    3  507    0
     2    0    5]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    3    0    1    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    20  311    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.07317073170732

F1 scores:
[       nan 0.975      0.98474775 0.92537313 0.99300699 0.95964126
 0.97695167 0.98039216 0.99652375 0.74418605 0.92125518 0.98773842
 0.96848138 0.98930481 0.98695652 0.93957704 0.97109827]

Kappa:
0.9666469027610218
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5f56576710>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.226, val_acc:0.423]
Epoch [2/120    avg_loss:1.721, val_acc:0.590]
Epoch [3/120    avg_loss:1.329, val_acc:0.595]
Epoch [4/120    avg_loss:1.160, val_acc:0.717]
Epoch [5/120    avg_loss:0.916, val_acc:0.751]
Epoch [6/120    avg_loss:0.763, val_acc:0.712]
Epoch [7/120    avg_loss:0.690, val_acc:0.765]
Epoch [8/120    avg_loss:0.728, val_acc:0.786]
Epoch [9/120    avg_loss:0.635, val_acc:0.807]
Epoch [10/120    avg_loss:0.576, val_acc:0.873]
Epoch [11/120    avg_loss:0.415, val_acc:0.843]
Epoch [12/120    avg_loss:0.357, val_acc:0.854]
Epoch [13/120    avg_loss:0.242, val_acc:0.898]
Epoch [14/120    avg_loss:0.323, val_acc:0.899]
Epoch [15/120    avg_loss:0.251, val_acc:0.919]
Epoch [16/120    avg_loss:0.258, val_acc:0.900]
Epoch [17/120    avg_loss:0.213, val_acc:0.885]
Epoch [18/120    avg_loss:0.224, val_acc:0.895]
Epoch [19/120    avg_loss:0.239, val_acc:0.868]
Epoch [20/120    avg_loss:0.242, val_acc:0.917]
Epoch [21/120    avg_loss:0.166, val_acc:0.928]
Epoch [22/120    avg_loss:0.168, val_acc:0.938]
Epoch [23/120    avg_loss:0.110, val_acc:0.937]
Epoch [24/120    avg_loss:0.127, val_acc:0.954]
Epoch [25/120    avg_loss:0.120, val_acc:0.936]
Epoch [26/120    avg_loss:0.127, val_acc:0.950]
Epoch [27/120    avg_loss:0.131, val_acc:0.940]
Epoch [28/120    avg_loss:0.141, val_acc:0.926]
Epoch [29/120    avg_loss:0.306, val_acc:0.882]
Epoch [30/120    avg_loss:0.186, val_acc:0.927]
Epoch [31/120    avg_loss:0.130, val_acc:0.916]
Epoch [32/120    avg_loss:0.183, val_acc:0.860]
Epoch [33/120    avg_loss:0.265, val_acc:0.907]
Epoch [34/120    avg_loss:0.206, val_acc:0.919]
Epoch [35/120    avg_loss:0.148, val_acc:0.923]
Epoch [36/120    avg_loss:0.098, val_acc:0.952]
Epoch [37/120    avg_loss:0.084, val_acc:0.928]
Epoch [38/120    avg_loss:0.059, val_acc:0.964]
Epoch [39/120    avg_loss:0.053, val_acc:0.966]
Epoch [40/120    avg_loss:0.052, val_acc:0.967]
Epoch [41/120    avg_loss:0.042, val_acc:0.968]
Epoch [42/120    avg_loss:0.053, val_acc:0.964]
Epoch [43/120    avg_loss:0.055, val_acc:0.966]
Epoch [44/120    avg_loss:0.045, val_acc:0.965]
Epoch [45/120    avg_loss:0.042, val_acc:0.967]
Epoch [46/120    avg_loss:0.059, val_acc:0.970]
Epoch [47/120    avg_loss:0.044, val_acc:0.963]
Epoch [48/120    avg_loss:0.039, val_acc:0.967]
Epoch [49/120    avg_loss:0.037, val_acc:0.967]
Epoch [50/120    avg_loss:0.039, val_acc:0.965]
Epoch [51/120    avg_loss:0.049, val_acc:0.968]
Epoch [52/120    avg_loss:0.042, val_acc:0.967]
Epoch [53/120    avg_loss:0.040, val_acc:0.968]
Epoch [54/120    avg_loss:0.046, val_acc:0.965]
Epoch [55/120    avg_loss:0.040, val_acc:0.966]
Epoch [56/120    avg_loss:0.046, val_acc:0.967]
Epoch [57/120    avg_loss:0.046, val_acc:0.968]
Epoch [58/120    avg_loss:0.040, val_acc:0.968]
Epoch [59/120    avg_loss:0.039, val_acc:0.965]
Epoch [60/120    avg_loss:0.050, val_acc:0.966]
Epoch [61/120    avg_loss:0.038, val_acc:0.965]
Epoch [62/120    avg_loss:0.037, val_acc:0.966]
Epoch [63/120    avg_loss:0.038, val_acc:0.966]
Epoch [64/120    avg_loss:0.038, val_acc:0.967]
Epoch [65/120    avg_loss:0.036, val_acc:0.966]
Epoch [66/120    avg_loss:0.039, val_acc:0.965]
Epoch [67/120    avg_loss:0.034, val_acc:0.967]
Epoch [68/120    avg_loss:0.029, val_acc:0.967]
Epoch [69/120    avg_loss:0.032, val_acc:0.967]
Epoch [70/120    avg_loss:0.030, val_acc:0.968]
Epoch [71/120    avg_loss:0.034, val_acc:0.967]
Epoch [72/120    avg_loss:0.045, val_acc:0.967]
Epoch [73/120    avg_loss:0.036, val_acc:0.967]
Epoch [74/120    avg_loss:0.031, val_acc:0.967]
Epoch [75/120    avg_loss:0.043, val_acc:0.967]
Epoch [76/120    avg_loss:0.037, val_acc:0.967]
Epoch [77/120    avg_loss:0.042, val_acc:0.967]
Epoch [78/120    avg_loss:0.039, val_acc:0.967]
Epoch [79/120    avg_loss:0.042, val_acc:0.967]
Epoch [80/120    avg_loss:0.049, val_acc:0.967]
Epoch [81/120    avg_loss:0.034, val_acc:0.967]
Epoch [82/120    avg_loss:0.038, val_acc:0.967]
Epoch [83/120    avg_loss:0.033, val_acc:0.967]
Epoch [84/120    avg_loss:0.042, val_acc:0.967]
Epoch [85/120    avg_loss:0.033, val_acc:0.967]
Epoch [86/120    avg_loss:0.041, val_acc:0.967]
Epoch [87/120    avg_loss:0.035, val_acc:0.967]
Epoch [88/120    avg_loss:0.047, val_acc:0.967]
Epoch [89/120    avg_loss:0.036, val_acc:0.967]
Epoch [90/120    avg_loss:0.043, val_acc:0.967]
Epoch [91/120    avg_loss:0.039, val_acc:0.967]
Epoch [92/120    avg_loss:0.038, val_acc:0.967]
Epoch [93/120    avg_loss:0.044, val_acc:0.967]
Epoch [94/120    avg_loss:0.029, val_acc:0.967]
Epoch [95/120    avg_loss:0.037, val_acc:0.967]
Epoch [96/120    avg_loss:0.037, val_acc:0.967]
Epoch [97/120    avg_loss:0.037, val_acc:0.967]
Epoch [98/120    avg_loss:0.033, val_acc:0.967]
Epoch [99/120    avg_loss:0.038, val_acc:0.967]
Epoch [100/120    avg_loss:0.043, val_acc:0.967]
Epoch [101/120    avg_loss:0.036, val_acc:0.967]
Epoch [102/120    avg_loss:0.035, val_acc:0.967]
Epoch [103/120    avg_loss:0.034, val_acc:0.967]
Epoch [104/120    avg_loss:0.034, val_acc:0.967]
Epoch [105/120    avg_loss:0.033, val_acc:0.967]
Epoch [106/120    avg_loss:0.034, val_acc:0.967]
Epoch [107/120    avg_loss:0.037, val_acc:0.967]
Epoch [108/120    avg_loss:0.042, val_acc:0.967]
Epoch [109/120    avg_loss:0.035, val_acc:0.967]
Epoch [110/120    avg_loss:0.046, val_acc:0.967]
Epoch [111/120    avg_loss:0.041, val_acc:0.967]
Epoch [112/120    avg_loss:0.038, val_acc:0.967]
Epoch [113/120    avg_loss:0.044, val_acc:0.967]
Epoch [114/120    avg_loss:0.039, val_acc:0.967]
Epoch [115/120    avg_loss:0.040, val_acc:0.967]
Epoch [116/120    avg_loss:0.034, val_acc:0.967]
Epoch [117/120    avg_loss:0.044, val_acc:0.967]
Epoch [118/120    avg_loss:0.039, val_acc:0.967]
Epoch [119/120    avg_loss:0.033, val_acc:0.967]
Epoch [120/120    avg_loss:0.036, val_acc:0.967]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    2 1248    0    0    0   11    0    0    0    7   10    4    0
     0    2    1]
 [   0    0    0  713    0   21    0    0    0   10    0    0    1    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  420    0    6    0    2    0    0    0    0
     7    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    5    0    0    0    0    0    0  425    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0    3   53    0    3    2    0    0    0  788   20    2    0
     3    1    0]
 [   0    0   16    0    0    3   24    0    0    0    7 2154    0    3
     3    0    0]
 [   0    0    0    0    6    3    0    0    0    0    4    0  509    0
     2    0   10]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    1    0    0
  1134    0    0]
 [   0    0    0    0    0    0   28    0    0    0    0    0    0    0
    20  299    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.56368563685636

F1 scores:
[       nan 0.89655172 0.97805643 0.94249835 0.98611111 0.94915254
 0.9521045  0.89285714 0.99299065 0.72340426 0.93475682 0.9799818
 0.96768061 0.98666667 0.98266898 0.92141757 0.93258427]

Kappa:
0.960853061039426
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:11--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2b09d5c668>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.326, val_acc:0.507]
Epoch [2/120    avg_loss:1.643, val_acc:0.609]
Epoch [3/120    avg_loss:1.591, val_acc:0.639]
Epoch [4/120    avg_loss:1.146, val_acc:0.680]
Epoch [5/120    avg_loss:0.849, val_acc:0.747]
Epoch [6/120    avg_loss:0.695, val_acc:0.690]
Epoch [7/120    avg_loss:0.697, val_acc:0.784]
Epoch [8/120    avg_loss:0.555, val_acc:0.821]
Epoch [9/120    avg_loss:0.578, val_acc:0.838]
Epoch [10/120    avg_loss:0.434, val_acc:0.849]
Epoch [11/120    avg_loss:0.427, val_acc:0.878]
Epoch [12/120    avg_loss:0.349, val_acc:0.878]
Epoch [13/120    avg_loss:0.303, val_acc:0.887]
Epoch [14/120    avg_loss:0.277, val_acc:0.848]
Epoch [15/120    avg_loss:0.324, val_acc:0.866]
Epoch [16/120    avg_loss:0.328, val_acc:0.898]
Epoch [17/120    avg_loss:0.221, val_acc:0.902]
Epoch [18/120    avg_loss:0.287, val_acc:0.922]
Epoch [19/120    avg_loss:0.205, val_acc:0.917]
Epoch [20/120    avg_loss:0.173, val_acc:0.911]
Epoch [21/120    avg_loss:0.186, val_acc:0.928]
Epoch [22/120    avg_loss:0.181, val_acc:0.940]
Epoch [23/120    avg_loss:0.165, val_acc:0.926]
Epoch [24/120    avg_loss:0.195, val_acc:0.941]
Epoch [25/120    avg_loss:0.128, val_acc:0.914]
Epoch [26/120    avg_loss:0.170, val_acc:0.930]
Epoch [27/120    avg_loss:0.144, val_acc:0.935]
Epoch [28/120    avg_loss:0.101, val_acc:0.947]
Epoch [29/120    avg_loss:0.085, val_acc:0.957]
Epoch [30/120    avg_loss:0.102, val_acc:0.921]
Epoch [31/120    avg_loss:0.134, val_acc:0.940]
Epoch [32/120    avg_loss:0.084, val_acc:0.955]
Epoch [33/120    avg_loss:0.067, val_acc:0.964]
Epoch [34/120    avg_loss:0.112, val_acc:0.940]
Epoch [35/120    avg_loss:0.070, val_acc:0.962]
Epoch [36/120    avg_loss:0.079, val_acc:0.956]
Epoch [37/120    avg_loss:0.091, val_acc:0.961]
Epoch [38/120    avg_loss:0.080, val_acc:0.972]
Epoch [39/120    avg_loss:0.063, val_acc:0.967]
Epoch [40/120    avg_loss:0.095, val_acc:0.958]
Epoch [41/120    avg_loss:0.073, val_acc:0.965]
Epoch [42/120    avg_loss:0.064, val_acc:0.946]
Epoch [43/120    avg_loss:0.059, val_acc:0.962]
Epoch [44/120    avg_loss:0.061, val_acc:0.964]
Epoch [45/120    avg_loss:0.057, val_acc:0.958]
Epoch [46/120    avg_loss:0.047, val_acc:0.964]
Epoch [47/120    avg_loss:0.044, val_acc:0.976]
Epoch [48/120    avg_loss:0.028, val_acc:0.981]
Epoch [49/120    avg_loss:0.028, val_acc:0.970]
Epoch [50/120    avg_loss:0.039, val_acc:0.964]
Epoch [51/120    avg_loss:0.057, val_acc:0.974]
Epoch [52/120    avg_loss:0.054, val_acc:0.976]
Epoch [53/120    avg_loss:0.051, val_acc:0.974]
Epoch [54/120    avg_loss:0.032, val_acc:0.974]
Epoch [55/120    avg_loss:0.030, val_acc:0.982]
Epoch [56/120    avg_loss:0.021, val_acc:0.976]
Epoch [57/120    avg_loss:0.025, val_acc:0.975]
Epoch [58/120    avg_loss:0.030, val_acc:0.976]
Epoch [59/120    avg_loss:0.025, val_acc:0.983]
Epoch [60/120    avg_loss:0.039, val_acc:0.972]
Epoch [61/120    avg_loss:0.029, val_acc:0.985]
Epoch [62/120    avg_loss:0.027, val_acc:0.979]
Epoch [63/120    avg_loss:0.147, val_acc:0.946]
Epoch [64/120    avg_loss:0.065, val_acc:0.961]
Epoch [65/120    avg_loss:0.045, val_acc:0.955]
Epoch [66/120    avg_loss:0.028, val_acc:0.968]
Epoch [67/120    avg_loss:0.038, val_acc:0.958]
Epoch [68/120    avg_loss:0.022, val_acc:0.982]
Epoch [69/120    avg_loss:0.018, val_acc:0.981]
Epoch [70/120    avg_loss:0.041, val_acc:0.979]
Epoch [71/120    avg_loss:0.022, val_acc:0.972]
Epoch [72/120    avg_loss:0.020, val_acc:0.983]
Epoch [73/120    avg_loss:0.022, val_acc:0.970]
Epoch [74/120    avg_loss:0.016, val_acc:0.976]
Epoch [75/120    avg_loss:0.016, val_acc:0.976]
Epoch [76/120    avg_loss:0.014, val_acc:0.979]
Epoch [77/120    avg_loss:0.010, val_acc:0.981]
Epoch [78/120    avg_loss:0.019, val_acc:0.981]
Epoch [79/120    avg_loss:0.014, val_acc:0.982]
Epoch [80/120    avg_loss:0.013, val_acc:0.981]
Epoch [81/120    avg_loss:0.008, val_acc:0.981]
Epoch [82/120    avg_loss:0.010, val_acc:0.980]
Epoch [83/120    avg_loss:0.017, val_acc:0.983]
Epoch [84/120    avg_loss:0.009, val_acc:0.983]
Epoch [85/120    avg_loss:0.011, val_acc:0.984]
Epoch [86/120    avg_loss:0.008, val_acc:0.982]
Epoch [87/120    avg_loss:0.010, val_acc:0.984]
Epoch [88/120    avg_loss:0.007, val_acc:0.984]
Epoch [89/120    avg_loss:0.008, val_acc:0.984]
Epoch [90/120    avg_loss:0.008, val_acc:0.984]
Epoch [91/120    avg_loss:0.010, val_acc:0.984]
Epoch [92/120    avg_loss:0.010, val_acc:0.983]
Epoch [93/120    avg_loss:0.010, val_acc:0.983]
Epoch [94/120    avg_loss:0.012, val_acc:0.983]
Epoch [95/120    avg_loss:0.010, val_acc:0.983]
Epoch [96/120    avg_loss:0.009, val_acc:0.983]
Epoch [97/120    avg_loss:0.010, val_acc:0.983]
Epoch [98/120    avg_loss:0.012, val_acc:0.983]
Epoch [99/120    avg_loss:0.009, val_acc:0.982]
Epoch [100/120    avg_loss:0.008, val_acc:0.982]
Epoch [101/120    avg_loss:0.010, val_acc:0.982]
Epoch [102/120    avg_loss:0.009, val_acc:0.982]
Epoch [103/120    avg_loss:0.010, val_acc:0.982]
Epoch [104/120    avg_loss:0.008, val_acc:0.982]
Epoch [105/120    avg_loss:0.008, val_acc:0.982]
Epoch [106/120    avg_loss:0.011, val_acc:0.982]
Epoch [107/120    avg_loss:0.010, val_acc:0.982]
Epoch [108/120    avg_loss:0.009, val_acc:0.982]
Epoch [109/120    avg_loss:0.012, val_acc:0.982]
Epoch [110/120    avg_loss:0.011, val_acc:0.982]
Epoch [111/120    avg_loss:0.009, val_acc:0.982]
Epoch [112/120    avg_loss:0.011, val_acc:0.982]
Epoch [113/120    avg_loss:0.011, val_acc:0.982]
Epoch [114/120    avg_loss:0.013, val_acc:0.982]
Epoch [115/120    avg_loss:0.008, val_acc:0.982]
Epoch [116/120    avg_loss:0.013, val_acc:0.982]
Epoch [117/120    avg_loss:0.012, val_acc:0.982]
Epoch [118/120    avg_loss:0.008, val_acc:0.982]
Epoch [119/120    avg_loss:0.008, val_acc:0.982]
Epoch [120/120    avg_loss:0.009, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1273    0    0    0    0    0    0    0    4    3    5    0
     0    0    0]
 [   0    0    0  722    0   19    0    0    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    3    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    5   48    0    0    2    0    0    0  785   31    0    0
     1    3    0]
 [   0    0   11    0    0    5   10    0    0    0    1 2182    1    0
     0    0    0]
 [   0    0    0    2   10    3    0    0    0    0    4    3  509    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0    6    0    0    0    0    1    0    0
    33  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.46341463414635

F1 scores:
[       nan 0.98765432 0.98873786 0.94812869 0.97706422 0.96412556
 0.98572502 0.94339623 0.99883856 0.7        0.94011976 0.98487926
 0.96952381 1.         0.98441558 0.93455099 0.97647059]

Kappa:
0.9710728853939519
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa497140710>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.420, val_acc:0.559]
Epoch [2/120    avg_loss:1.778, val_acc:0.631]
Epoch [3/120    avg_loss:1.368, val_acc:0.693]
Epoch [4/120    avg_loss:0.971, val_acc:0.758]
Epoch [5/120    avg_loss:0.872, val_acc:0.788]
Epoch [6/120    avg_loss:0.753, val_acc:0.803]
Epoch [7/120    avg_loss:0.632, val_acc:0.645]
Epoch [8/120    avg_loss:0.659, val_acc:0.828]
Epoch [9/120    avg_loss:0.603, val_acc:0.847]
Epoch [10/120    avg_loss:0.594, val_acc:0.823]
Epoch [11/120    avg_loss:0.422, val_acc:0.874]
Epoch [12/120    avg_loss:0.534, val_acc:0.847]
Epoch [13/120    avg_loss:0.442, val_acc:0.895]
Epoch [14/120    avg_loss:0.389, val_acc:0.883]
Epoch [15/120    avg_loss:0.301, val_acc:0.881]
Epoch [16/120    avg_loss:0.325, val_acc:0.882]
Epoch [17/120    avg_loss:0.257, val_acc:0.909]
Epoch [18/120    avg_loss:0.203, val_acc:0.914]
Epoch [19/120    avg_loss:0.231, val_acc:0.917]
Epoch [20/120    avg_loss:0.165, val_acc:0.924]
Epoch [21/120    avg_loss:0.171, val_acc:0.939]
Epoch [22/120    avg_loss:0.138, val_acc:0.912]
Epoch [23/120    avg_loss:0.122, val_acc:0.931]
Epoch [24/120    avg_loss:0.143, val_acc:0.936]
Epoch [25/120    avg_loss:0.134, val_acc:0.913]
Epoch [26/120    avg_loss:0.119, val_acc:0.942]
Epoch [27/120    avg_loss:0.094, val_acc:0.929]
Epoch [28/120    avg_loss:0.108, val_acc:0.951]
Epoch [29/120    avg_loss:0.110, val_acc:0.943]
Epoch [30/120    avg_loss:0.134, val_acc:0.934]
Epoch [31/120    avg_loss:0.078, val_acc:0.944]
Epoch [32/120    avg_loss:0.107, val_acc:0.944]
Epoch [33/120    avg_loss:0.179, val_acc:0.917]
Epoch [34/120    avg_loss:0.159, val_acc:0.948]
Epoch [35/120    avg_loss:0.094, val_acc:0.933]
Epoch [36/120    avg_loss:0.117, val_acc:0.946]
Epoch [37/120    avg_loss:0.082, val_acc:0.953]
Epoch [38/120    avg_loss:0.068, val_acc:0.958]
Epoch [39/120    avg_loss:0.081, val_acc:0.957]
Epoch [40/120    avg_loss:0.053, val_acc:0.949]
Epoch [41/120    avg_loss:0.041, val_acc:0.968]
Epoch [42/120    avg_loss:0.052, val_acc:0.947]
Epoch [43/120    avg_loss:0.056, val_acc:0.951]
Epoch [44/120    avg_loss:0.070, val_acc:0.934]
Epoch [45/120    avg_loss:0.058, val_acc:0.958]
Epoch [46/120    avg_loss:0.053, val_acc:0.959]
Epoch [47/120    avg_loss:0.045, val_acc:0.963]
Epoch [48/120    avg_loss:0.038, val_acc:0.964]
Epoch [49/120    avg_loss:0.067, val_acc:0.964]
Epoch [50/120    avg_loss:0.057, val_acc:0.956]
Epoch [51/120    avg_loss:0.045, val_acc:0.959]
Epoch [52/120    avg_loss:0.061, val_acc:0.968]
Epoch [53/120    avg_loss:0.052, val_acc:0.961]
Epoch [54/120    avg_loss:0.040, val_acc:0.966]
Epoch [55/120    avg_loss:0.036, val_acc:0.971]
Epoch [56/120    avg_loss:0.032, val_acc:0.970]
Epoch [57/120    avg_loss:0.030, val_acc:0.962]
Epoch [58/120    avg_loss:0.072, val_acc:0.956]
Epoch [59/120    avg_loss:0.034, val_acc:0.975]
Epoch [60/120    avg_loss:0.043, val_acc:0.949]
Epoch [61/120    avg_loss:0.028, val_acc:0.971]
Epoch [62/120    avg_loss:0.034, val_acc:0.969]
Epoch [63/120    avg_loss:0.024, val_acc:0.973]
Epoch [64/120    avg_loss:0.019, val_acc:0.971]
Epoch [65/120    avg_loss:0.031, val_acc:0.969]
Epoch [66/120    avg_loss:0.052, val_acc:0.969]
Epoch [67/120    avg_loss:0.042, val_acc:0.976]
Epoch [68/120    avg_loss:0.037, val_acc:0.965]
Epoch [69/120    avg_loss:0.034, val_acc:0.971]
Epoch [70/120    avg_loss:0.025, val_acc:0.971]
Epoch [71/120    avg_loss:0.031, val_acc:0.972]
Epoch [72/120    avg_loss:0.026, val_acc:0.976]
Epoch [73/120    avg_loss:0.018, val_acc:0.971]
Epoch [74/120    avg_loss:0.011, val_acc:0.979]
Epoch [75/120    avg_loss:0.025, val_acc:0.976]
Epoch [76/120    avg_loss:0.016, val_acc:0.976]
Epoch [77/120    avg_loss:0.015, val_acc:0.977]
Epoch [78/120    avg_loss:0.026, val_acc:0.966]
Epoch [79/120    avg_loss:0.018, val_acc:0.972]
Epoch [80/120    avg_loss:0.018, val_acc:0.971]
Epoch [81/120    avg_loss:0.013, val_acc:0.975]
Epoch [82/120    avg_loss:0.013, val_acc:0.973]
Epoch [83/120    avg_loss:0.037, val_acc:0.955]
Epoch [84/120    avg_loss:0.093, val_acc:0.965]
Epoch [85/120    avg_loss:0.044, val_acc:0.970]
Epoch [86/120    avg_loss:0.022, val_acc:0.972]
Epoch [87/120    avg_loss:0.012, val_acc:0.981]
Epoch [88/120    avg_loss:0.023, val_acc:0.979]
Epoch [89/120    avg_loss:0.019, val_acc:0.970]
Epoch [90/120    avg_loss:0.023, val_acc:0.981]
Epoch [91/120    avg_loss:0.014, val_acc:0.968]
Epoch [92/120    avg_loss:0.022, val_acc:0.980]
Epoch [93/120    avg_loss:0.012, val_acc:0.981]
Epoch [94/120    avg_loss:0.017, val_acc:0.976]
Epoch [95/120    avg_loss:0.011, val_acc:0.973]
Epoch [96/120    avg_loss:0.012, val_acc:0.975]
Epoch [97/120    avg_loss:0.010, val_acc:0.980]
Epoch [98/120    avg_loss:0.011, val_acc:0.980]
Epoch [99/120    avg_loss:0.007, val_acc:0.981]
Epoch [100/120    avg_loss:0.009, val_acc:0.975]
Epoch [101/120    avg_loss:0.010, val_acc:0.981]
Epoch [102/120    avg_loss:0.012, val_acc:0.977]
Epoch [103/120    avg_loss:0.017, val_acc:0.976]
Epoch [104/120    avg_loss:0.011, val_acc:0.980]
Epoch [105/120    avg_loss:0.030, val_acc:0.920]
Epoch [106/120    avg_loss:0.029, val_acc:0.965]
Epoch [107/120    avg_loss:0.020, val_acc:0.983]
Epoch [108/120    avg_loss:0.018, val_acc:0.978]
Epoch [109/120    avg_loss:0.018, val_acc:0.975]
Epoch [110/120    avg_loss:0.021, val_acc:0.979]
Epoch [111/120    avg_loss:0.011, val_acc:0.978]
Epoch [112/120    avg_loss:0.022, val_acc:0.972]
Epoch [113/120    avg_loss:0.013, val_acc:0.979]
Epoch [114/120    avg_loss:0.018, val_acc:0.973]
Epoch [115/120    avg_loss:0.015, val_acc:0.977]
Epoch [116/120    avg_loss:0.010, val_acc:0.983]
Epoch [117/120    avg_loss:0.019, val_acc:0.975]
Epoch [118/120    avg_loss:0.008, val_acc:0.979]
Epoch [119/120    avg_loss:0.011, val_acc:0.981]
Epoch [120/120    avg_loss:0.009, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    1 1244    1    0    0    0    0    0    0   13   20    6    0
     0    0    0]
 [   0    0    0  715    0   23    0    0    0    6    2    0    0    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    2    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    6    0    0    0    0    0    0  424    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    6   59    0    1    6    0    0    0  780   23    0    0
     0    0    0]
 [   0    0    1    0    0    0    4    0    0    0    2 2202    0    1
     0    0    0]
 [   0    0    0   16    2    5    0    0    0    0   11    2  491    0
     3    0    4]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    3    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    54  287    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.75880758807588

F1 scores:
[       nan 0.90909091 0.98068585 0.92796885 0.9953271  0.96205357
 0.98796992 1.         0.99181287 0.73170732 0.9252669  0.98810859
 0.95155039 0.99191375 0.97299614 0.90536278 0.97076023]

Kappa:
0.9630167716565627
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:17--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8a60e15710>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.362, val_acc:0.451]
Epoch [2/120    avg_loss:1.787, val_acc:0.605]
Epoch [3/120    avg_loss:1.259, val_acc:0.634]
Epoch [4/120    avg_loss:0.909, val_acc:0.780]
Epoch [5/120    avg_loss:0.951, val_acc:0.764]
Epoch [6/120    avg_loss:0.940, val_acc:0.724]
Epoch [7/120    avg_loss:0.756, val_acc:0.829]
Epoch [8/120    avg_loss:0.559, val_acc:0.837]
Epoch [9/120    avg_loss:0.461, val_acc:0.854]
Epoch [10/120    avg_loss:0.396, val_acc:0.880]
Epoch [11/120    avg_loss:0.340, val_acc:0.854]
Epoch [12/120    avg_loss:0.373, val_acc:0.891]
Epoch [13/120    avg_loss:0.421, val_acc:0.874]
Epoch [14/120    avg_loss:0.242, val_acc:0.880]
Epoch [15/120    avg_loss:0.266, val_acc:0.908]
Epoch [16/120    avg_loss:0.266, val_acc:0.928]
Epoch [17/120    avg_loss:0.214, val_acc:0.914]
Epoch [18/120    avg_loss:0.192, val_acc:0.936]
Epoch [19/120    avg_loss:0.610, val_acc:0.728]
Epoch [20/120    avg_loss:0.709, val_acc:0.800]
Epoch [21/120    avg_loss:0.479, val_acc:0.826]
Epoch [22/120    avg_loss:0.452, val_acc:0.908]
Epoch [23/120    avg_loss:0.363, val_acc:0.896]
Epoch [24/120    avg_loss:0.270, val_acc:0.934]
Epoch [25/120    avg_loss:0.221, val_acc:0.901]
Epoch [26/120    avg_loss:0.184, val_acc:0.925]
Epoch [27/120    avg_loss:0.147, val_acc:0.948]
Epoch [28/120    avg_loss:0.134, val_acc:0.933]
Epoch [29/120    avg_loss:0.117, val_acc:0.946]
Epoch [30/120    avg_loss:0.098, val_acc:0.946]
Epoch [31/120    avg_loss:0.116, val_acc:0.941]
Epoch [32/120    avg_loss:0.174, val_acc:0.940]
Epoch [33/120    avg_loss:0.118, val_acc:0.952]
Epoch [34/120    avg_loss:0.118, val_acc:0.946]
Epoch [35/120    avg_loss:0.109, val_acc:0.945]
Epoch [36/120    avg_loss:0.093, val_acc:0.964]
Epoch [37/120    avg_loss:0.089, val_acc:0.975]
Epoch [38/120    avg_loss:0.087, val_acc:0.953]
Epoch [39/120    avg_loss:0.060, val_acc:0.974]
Epoch [40/120    avg_loss:0.098, val_acc:0.893]
Epoch [41/120    avg_loss:0.207, val_acc:0.946]
Epoch [42/120    avg_loss:0.180, val_acc:0.943]
Epoch [43/120    avg_loss:0.105, val_acc:0.954]
Epoch [44/120    avg_loss:0.082, val_acc:0.965]
Epoch [45/120    avg_loss:0.085, val_acc:0.951]
Epoch [46/120    avg_loss:0.105, val_acc:0.968]
Epoch [47/120    avg_loss:0.066, val_acc:0.967]
Epoch [48/120    avg_loss:0.069, val_acc:0.975]
Epoch [49/120    avg_loss:0.063, val_acc:0.962]
Epoch [50/120    avg_loss:0.091, val_acc:0.968]
Epoch [51/120    avg_loss:0.062, val_acc:0.979]
Epoch [52/120    avg_loss:0.070, val_acc:0.961]
Epoch [53/120    avg_loss:0.054, val_acc:0.967]
Epoch [54/120    avg_loss:0.068, val_acc:0.981]
Epoch [55/120    avg_loss:0.057, val_acc:0.976]
Epoch [56/120    avg_loss:0.038, val_acc:0.975]
Epoch [57/120    avg_loss:0.028, val_acc:0.982]
Epoch [58/120    avg_loss:0.043, val_acc:0.977]
Epoch [59/120    avg_loss:0.033, val_acc:0.984]
Epoch [60/120    avg_loss:0.030, val_acc:0.978]
Epoch [61/120    avg_loss:0.029, val_acc:0.985]
Epoch [62/120    avg_loss:0.028, val_acc:0.984]
Epoch [63/120    avg_loss:0.027, val_acc:0.982]
Epoch [64/120    avg_loss:0.027, val_acc:0.980]
Epoch [65/120    avg_loss:0.026, val_acc:0.987]
Epoch [66/120    avg_loss:0.019, val_acc:0.988]
Epoch [67/120    avg_loss:0.020, val_acc:0.985]
Epoch [68/120    avg_loss:0.022, val_acc:0.984]
Epoch [69/120    avg_loss:0.019, val_acc:0.990]
Epoch [70/120    avg_loss:0.018, val_acc:0.986]
Epoch [71/120    avg_loss:0.021, val_acc:0.981]
Epoch [72/120    avg_loss:0.016, val_acc:0.989]
Epoch [73/120    avg_loss:0.017, val_acc:0.987]
Epoch [74/120    avg_loss:0.023, val_acc:0.984]
Epoch [75/120    avg_loss:0.018, val_acc:0.987]
Epoch [76/120    avg_loss:0.018, val_acc:0.986]
Epoch [77/120    avg_loss:0.016, val_acc:0.990]
Epoch [78/120    avg_loss:0.025, val_acc:0.988]
Epoch [79/120    avg_loss:0.020, val_acc:0.984]
Epoch [80/120    avg_loss:0.033, val_acc:0.978]
Epoch [81/120    avg_loss:0.024, val_acc:0.989]
Epoch [82/120    avg_loss:0.017, val_acc:0.988]
Epoch [83/120    avg_loss:0.013, val_acc:0.986]
Epoch [84/120    avg_loss:0.013, val_acc:0.987]
Epoch [85/120    avg_loss:0.011, val_acc:0.987]
Epoch [86/120    avg_loss:0.012, val_acc:0.988]
Epoch [87/120    avg_loss:0.011, val_acc:0.984]
Epoch [88/120    avg_loss:0.009, val_acc:0.992]
Epoch [89/120    avg_loss:0.012, val_acc:0.989]
Epoch [90/120    avg_loss:0.018, val_acc:0.988]
Epoch [91/120    avg_loss:0.020, val_acc:0.989]
Epoch [92/120    avg_loss:0.013, val_acc:0.988]
Epoch [93/120    avg_loss:0.010, val_acc:0.980]
Epoch [94/120    avg_loss:0.009, val_acc:0.986]
Epoch [95/120    avg_loss:0.010, val_acc:0.991]
Epoch [96/120    avg_loss:0.009, val_acc:0.988]
Epoch [97/120    avg_loss:0.030, val_acc:0.971]
Epoch [98/120    avg_loss:0.035, val_acc:0.955]
Epoch [99/120    avg_loss:0.022, val_acc:0.987]
Epoch [100/120    avg_loss:0.018, val_acc:0.986]
Epoch [101/120    avg_loss:0.018, val_acc:0.975]
Epoch [102/120    avg_loss:0.038, val_acc:0.985]
Epoch [103/120    avg_loss:0.018, val_acc:0.986]
Epoch [104/120    avg_loss:0.014, val_acc:0.985]
Epoch [105/120    avg_loss:0.012, val_acc:0.985]
Epoch [106/120    avg_loss:0.011, val_acc:0.984]
Epoch [107/120    avg_loss:0.010, val_acc:0.984]
Epoch [108/120    avg_loss:0.010, val_acc:0.985]
Epoch [109/120    avg_loss:0.008, val_acc:0.986]
Epoch [110/120    avg_loss:0.010, val_acc:0.985]
Epoch [111/120    avg_loss:0.008, val_acc:0.986]
Epoch [112/120    avg_loss:0.007, val_acc:0.987]
Epoch [113/120    avg_loss:0.009, val_acc:0.987]
Epoch [114/120    avg_loss:0.012, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.016, val_acc:0.988]
Epoch [117/120    avg_loss:0.008, val_acc:0.988]
Epoch [118/120    avg_loss:0.010, val_acc:0.988]
Epoch [119/120    avg_loss:0.010, val_acc:0.988]
Epoch [120/120    avg_loss:0.012, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1250    0    0    0    1    0    0    0   21    5    2    0
     0    6    0]
 [   0    0    0  722    0   19    0    0    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    8   49    0    4   13    0    0    1  770   24    1    0
     1    4    0]
 [   0    0    7    0    0    0    2    0    0    0    6 2194    0    1
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    5   13  505    0
     0    0    4]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0    7    0    0    4    0    0    0    0
    39  297    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.04065040650407

F1 scores:
[       nan 0.98765432 0.98000784 0.94875164 1.         0.96213808
 0.98203593 0.98039216 0.99883856 0.62222222 0.91775924 0.98673263
 0.9683605  0.99459459 0.98186528 0.90825688 0.97076023]

Kappa:
0.9662441377349358
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:20--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc29a2b2710>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.347, val_acc:0.418]
Epoch [2/120    avg_loss:1.715, val_acc:0.550]
Epoch [3/120    avg_loss:1.272, val_acc:0.673]
Epoch [4/120    avg_loss:1.023, val_acc:0.706]
Epoch [5/120    avg_loss:0.883, val_acc:0.784]
Epoch [6/120    avg_loss:0.703, val_acc:0.778]
Epoch [7/120    avg_loss:0.534, val_acc:0.833]
Epoch [8/120    avg_loss:0.632, val_acc:0.756]
Epoch [9/120    avg_loss:0.828, val_acc:0.775]
Epoch [10/120    avg_loss:0.456, val_acc:0.847]
Epoch [11/120    avg_loss:0.381, val_acc:0.864]
Epoch [12/120    avg_loss:0.341, val_acc:0.870]
Epoch [13/120    avg_loss:0.306, val_acc:0.881]
Epoch [14/120    avg_loss:0.330, val_acc:0.859]
Epoch [15/120    avg_loss:0.363, val_acc:0.876]
Epoch [16/120    avg_loss:0.302, val_acc:0.891]
Epoch [17/120    avg_loss:0.295, val_acc:0.824]
Epoch [18/120    avg_loss:0.269, val_acc:0.908]
Epoch [19/120    avg_loss:0.208, val_acc:0.921]
Epoch [20/120    avg_loss:0.194, val_acc:0.921]
Epoch [21/120    avg_loss:0.162, val_acc:0.905]
Epoch [22/120    avg_loss:0.264, val_acc:0.874]
Epoch [23/120    avg_loss:0.205, val_acc:0.917]
Epoch [24/120    avg_loss:0.123, val_acc:0.918]
Epoch [25/120    avg_loss:0.206, val_acc:0.934]
Epoch [26/120    avg_loss:0.134, val_acc:0.936]
Epoch [27/120    avg_loss:0.140, val_acc:0.919]
Epoch [28/120    avg_loss:0.105, val_acc:0.930]
Epoch [29/120    avg_loss:0.098, val_acc:0.954]
Epoch [30/120    avg_loss:0.103, val_acc:0.955]
Epoch [31/120    avg_loss:0.106, val_acc:0.948]
Epoch [32/120    avg_loss:0.092, val_acc:0.956]
Epoch [33/120    avg_loss:0.085, val_acc:0.947]
Epoch [34/120    avg_loss:0.091, val_acc:0.956]
Epoch [35/120    avg_loss:0.107, val_acc:0.952]
Epoch [36/120    avg_loss:0.055, val_acc:0.962]
Epoch [37/120    avg_loss:0.073, val_acc:0.957]
Epoch [38/120    avg_loss:0.064, val_acc:0.959]
Epoch [39/120    avg_loss:0.069, val_acc:0.950]
Epoch [40/120    avg_loss:0.057, val_acc:0.973]
Epoch [41/120    avg_loss:0.039, val_acc:0.961]
Epoch [42/120    avg_loss:0.053, val_acc:0.968]
Epoch [43/120    avg_loss:0.054, val_acc:0.966]
Epoch [44/120    avg_loss:0.045, val_acc:0.966]
Epoch [45/120    avg_loss:0.045, val_acc:0.970]
Epoch [46/120    avg_loss:0.035, val_acc:0.963]
Epoch [47/120    avg_loss:0.097, val_acc:0.940]
Epoch [48/120    avg_loss:0.139, val_acc:0.946]
Epoch [49/120    avg_loss:0.093, val_acc:0.957]
Epoch [50/120    avg_loss:0.051, val_acc:0.964]
Epoch [51/120    avg_loss:0.072, val_acc:0.957]
Epoch [52/120    avg_loss:0.045, val_acc:0.963]
Epoch [53/120    avg_loss:0.045, val_acc:0.956]
Epoch [54/120    avg_loss:0.038, val_acc:0.966]
Epoch [55/120    avg_loss:0.025, val_acc:0.971]
Epoch [56/120    avg_loss:0.029, val_acc:0.977]
Epoch [57/120    avg_loss:0.026, val_acc:0.979]
Epoch [58/120    avg_loss:0.022, val_acc:0.979]
Epoch [59/120    avg_loss:0.018, val_acc:0.982]
Epoch [60/120    avg_loss:0.020, val_acc:0.977]
Epoch [61/120    avg_loss:0.023, val_acc:0.977]
Epoch [62/120    avg_loss:0.019, val_acc:0.977]
Epoch [63/120    avg_loss:0.028, val_acc:0.979]
Epoch [64/120    avg_loss:0.019, val_acc:0.979]
Epoch [65/120    avg_loss:0.019, val_acc:0.976]
Epoch [66/120    avg_loss:0.017, val_acc:0.979]
Epoch [67/120    avg_loss:0.019, val_acc:0.977]
Epoch [68/120    avg_loss:0.022, val_acc:0.977]
Epoch [69/120    avg_loss:0.021, val_acc:0.981]
Epoch [70/120    avg_loss:0.019, val_acc:0.979]
Epoch [71/120    avg_loss:0.019, val_acc:0.979]
Epoch [72/120    avg_loss:0.021, val_acc:0.977]
Epoch [73/120    avg_loss:0.019, val_acc:0.979]
Epoch [74/120    avg_loss:0.018, val_acc:0.979]
Epoch [75/120    avg_loss:0.019, val_acc:0.979]
Epoch [76/120    avg_loss:0.017, val_acc:0.979]
Epoch [77/120    avg_loss:0.026, val_acc:0.979]
Epoch [78/120    avg_loss:0.020, val_acc:0.979]
Epoch [79/120    avg_loss:0.023, val_acc:0.979]
Epoch [80/120    avg_loss:0.019, val_acc:0.979]
Epoch [81/120    avg_loss:0.019, val_acc:0.979]
Epoch [82/120    avg_loss:0.022, val_acc:0.979]
Epoch [83/120    avg_loss:0.017, val_acc:0.979]
Epoch [84/120    avg_loss:0.018, val_acc:0.979]
Epoch [85/120    avg_loss:0.017, val_acc:0.980]
Epoch [86/120    avg_loss:0.017, val_acc:0.980]
Epoch [87/120    avg_loss:0.029, val_acc:0.980]
Epoch [88/120    avg_loss:0.019, val_acc:0.980]
Epoch [89/120    avg_loss:0.017, val_acc:0.980]
Epoch [90/120    avg_loss:0.013, val_acc:0.980]
Epoch [91/120    avg_loss:0.022, val_acc:0.980]
Epoch [92/120    avg_loss:0.024, val_acc:0.980]
Epoch [93/120    avg_loss:0.017, val_acc:0.980]
Epoch [94/120    avg_loss:0.025, val_acc:0.980]
Epoch [95/120    avg_loss:0.015, val_acc:0.980]
Epoch [96/120    avg_loss:0.014, val_acc:0.980]
Epoch [97/120    avg_loss:0.021, val_acc:0.980]
Epoch [98/120    avg_loss:0.017, val_acc:0.980]
Epoch [99/120    avg_loss:0.018, val_acc:0.980]
Epoch [100/120    avg_loss:0.023, val_acc:0.980]
Epoch [101/120    avg_loss:0.015, val_acc:0.980]
Epoch [102/120    avg_loss:0.020, val_acc:0.980]
Epoch [103/120    avg_loss:0.017, val_acc:0.980]
Epoch [104/120    avg_loss:0.017, val_acc:0.980]
Epoch [105/120    avg_loss:0.023, val_acc:0.980]
Epoch [106/120    avg_loss:0.021, val_acc:0.980]
Epoch [107/120    avg_loss:0.020, val_acc:0.980]
Epoch [108/120    avg_loss:0.019, val_acc:0.980]
Epoch [109/120    avg_loss:0.018, val_acc:0.980]
Epoch [110/120    avg_loss:0.017, val_acc:0.980]
Epoch [111/120    avg_loss:0.019, val_acc:0.980]
Epoch [112/120    avg_loss:0.017, val_acc:0.980]
Epoch [113/120    avg_loss:0.015, val_acc:0.980]
Epoch [114/120    avg_loss:0.015, val_acc:0.980]
Epoch [115/120    avg_loss:0.017, val_acc:0.980]
Epoch [116/120    avg_loss:0.018, val_acc:0.980]
Epoch [117/120    avg_loss:0.019, val_acc:0.980]
Epoch [118/120    avg_loss:0.014, val_acc:0.980]
Epoch [119/120    avg_loss:0.017, val_acc:0.980]
Epoch [120/120    avg_loss:0.018, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1268    2    0    0    0    0    0    0    3    8    2    0
     0    2    0]
 [   0    0    0  736    0    3    0    0    0    6    0    0    0    2
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    1    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   13    0    0    1    0
     0    0    0]
 [   0    0    1   77    0    4    0    0    0    0  775   14    1    0
     0    3    0]
 [   0    0   11    0    0    3    6    0    0    0   10 2178    0    2
     0    0    0]
 [   0    0    0    5    0    7    0    0    0    0    4    6  498    0
     0    0   14]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    75  261    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.8130081300813

F1 scores:
[       nan 0.98765432 0.98830865 0.93698281 0.99764706 0.97853107
 0.98646617 0.98039216 0.99883856 0.68421053 0.92925659 0.9859665
 0.96138996 0.98930481 0.96724798 0.85154976 0.92307692]

Kappa:
0.9636491573725504
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:37
Validation dataloader:37
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:24
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f2acd8676d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.400, val_acc:0.437]
Epoch [2/120    avg_loss:1.716, val_acc:0.566]
Epoch [3/120    avg_loss:1.360, val_acc:0.634]
Epoch [4/120    avg_loss:1.086, val_acc:0.691]
Epoch [5/120    avg_loss:0.859, val_acc:0.769]
Epoch [6/120    avg_loss:0.994, val_acc:0.724]
Epoch [7/120    avg_loss:0.820, val_acc:0.730]
Epoch [8/120    avg_loss:0.574, val_acc:0.816]
Epoch [9/120    avg_loss:0.917, val_acc:0.686]
Epoch [10/120    avg_loss:0.847, val_acc:0.775]
Epoch [11/120    avg_loss:0.669, val_acc:0.821]
Epoch [12/120    avg_loss:0.449, val_acc:0.806]
Epoch [13/120    avg_loss:0.465, val_acc:0.860]
Epoch [14/120    avg_loss:0.430, val_acc:0.848]
Epoch [15/120    avg_loss:0.335, val_acc:0.861]
Epoch [16/120    avg_loss:0.248, val_acc:0.886]
Epoch [17/120    avg_loss:0.223, val_acc:0.916]
Epoch [18/120    avg_loss:0.240, val_acc:0.899]
Epoch [19/120    avg_loss:0.219, val_acc:0.917]
Epoch [20/120    avg_loss:0.214, val_acc:0.901]
Epoch [21/120    avg_loss:0.261, val_acc:0.908]
Epoch [22/120    avg_loss:0.174, val_acc:0.913]
Epoch [23/120    avg_loss:0.167, val_acc:0.919]
Epoch [24/120    avg_loss:0.153, val_acc:0.943]
Epoch [25/120    avg_loss:0.125, val_acc:0.947]
Epoch [26/120    avg_loss:0.127, val_acc:0.936]
Epoch [27/120    avg_loss:0.142, val_acc:0.916]
Epoch [28/120    avg_loss:0.153, val_acc:0.916]
Epoch [29/120    avg_loss:0.124, val_acc:0.932]
Epoch [30/120    avg_loss:0.112, val_acc:0.935]
Epoch [31/120    avg_loss:0.133, val_acc:0.925]
Epoch [32/120    avg_loss:0.111, val_acc:0.941]
Epoch [33/120    avg_loss:0.106, val_acc:0.953]
Epoch [34/120    avg_loss:0.090, val_acc:0.953]
Epoch [35/120    avg_loss:0.081, val_acc:0.956]
Epoch [36/120    avg_loss:0.093, val_acc:0.954]
Epoch [37/120    avg_loss:0.083, val_acc:0.943]
Epoch [38/120    avg_loss:0.068, val_acc:0.955]
Epoch [39/120    avg_loss:0.080, val_acc:0.958]
Epoch [40/120    avg_loss:0.051, val_acc:0.956]
Epoch [41/120    avg_loss:0.050, val_acc:0.957]
Epoch [42/120    avg_loss:0.067, val_acc:0.955]
Epoch [43/120    avg_loss:0.092, val_acc:0.965]
Epoch [44/120    avg_loss:0.059, val_acc:0.965]
Epoch [45/120    avg_loss:0.144, val_acc:0.872]
Epoch [46/120    avg_loss:0.215, val_acc:0.916]
Epoch [47/120    avg_loss:0.091, val_acc:0.961]
Epoch [48/120    avg_loss:0.061, val_acc:0.953]
Epoch [49/120    avg_loss:0.051, val_acc:0.966]
Epoch [50/120    avg_loss:0.051, val_acc:0.962]
Epoch [51/120    avg_loss:0.043, val_acc:0.967]
Epoch [52/120    avg_loss:0.051, val_acc:0.972]
Epoch [53/120    avg_loss:0.036, val_acc:0.971]
Epoch [54/120    avg_loss:0.037, val_acc:0.964]
Epoch [55/120    avg_loss:0.026, val_acc:0.974]
Epoch [56/120    avg_loss:0.042, val_acc:0.962]
Epoch [57/120    avg_loss:0.062, val_acc:0.962]
Epoch [58/120    avg_loss:0.050, val_acc:0.968]
Epoch [59/120    avg_loss:0.052, val_acc:0.966]
Epoch [60/120    avg_loss:0.029, val_acc:0.974]
Epoch [61/120    avg_loss:0.025, val_acc:0.974]
Epoch [62/120    avg_loss:0.034, val_acc:0.970]
Epoch [63/120    avg_loss:0.033, val_acc:0.972]
Epoch [64/120    avg_loss:0.023, val_acc:0.981]
Epoch [65/120    avg_loss:0.024, val_acc:0.970]
Epoch [66/120    avg_loss:0.052, val_acc:0.980]
Epoch [67/120    avg_loss:0.022, val_acc:0.976]
Epoch [68/120    avg_loss:0.018, val_acc:0.982]
Epoch [69/120    avg_loss:0.021, val_acc:0.979]
Epoch [70/120    avg_loss:0.015, val_acc:0.971]
Epoch [71/120    avg_loss:0.012, val_acc:0.980]
Epoch [72/120    avg_loss:0.024, val_acc:0.977]
Epoch [73/120    avg_loss:0.015, val_acc:0.974]
Epoch [74/120    avg_loss:0.023, val_acc:0.948]
Epoch [75/120    avg_loss:0.027, val_acc:0.977]
Epoch [76/120    avg_loss:0.017, val_acc:0.982]
Epoch [77/120    avg_loss:0.015, val_acc:0.976]
Epoch [78/120    avg_loss:0.013, val_acc:0.979]
Epoch [79/120    avg_loss:0.014, val_acc:0.979]
Epoch [80/120    avg_loss:0.015, val_acc:0.981]
Epoch [81/120    avg_loss:0.016, val_acc:0.979]
Epoch [82/120    avg_loss:0.013, val_acc:0.977]
Epoch [83/120    avg_loss:0.015, val_acc:0.983]
Epoch [84/120    avg_loss:0.010, val_acc:0.980]
Epoch [85/120    avg_loss:0.009, val_acc:0.984]
Epoch [86/120    avg_loss:0.008, val_acc:0.980]
Epoch [87/120    avg_loss:0.009, val_acc:0.984]
Epoch [88/120    avg_loss:0.009, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.984]
Epoch [90/120    avg_loss:0.009, val_acc:0.982]
Epoch [91/120    avg_loss:0.009, val_acc:0.981]
Epoch [92/120    avg_loss:0.017, val_acc:0.975]
Epoch [93/120    avg_loss:0.017, val_acc:0.986]
Epoch [94/120    avg_loss:0.014, val_acc:0.974]
Epoch [95/120    avg_loss:0.007, val_acc:0.979]
Epoch [96/120    avg_loss:0.027, val_acc:0.979]
Epoch [97/120    avg_loss:0.023, val_acc:0.972]
Epoch [98/120    avg_loss:0.029, val_acc:0.975]
Epoch [99/120    avg_loss:0.022, val_acc:0.981]
Epoch [100/120    avg_loss:0.013, val_acc:0.979]
Epoch [101/120    avg_loss:0.011, val_acc:0.980]
Epoch [102/120    avg_loss:0.016, val_acc:0.981]
Epoch [103/120    avg_loss:0.032, val_acc:0.968]
Epoch [104/120    avg_loss:0.019, val_acc:0.977]
Epoch [105/120    avg_loss:0.021, val_acc:0.966]
Epoch [106/120    avg_loss:0.019, val_acc:0.967]
Epoch [107/120    avg_loss:0.015, val_acc:0.976]
Epoch [108/120    avg_loss:0.010, val_acc:0.981]
Epoch [109/120    avg_loss:0.010, val_acc:0.981]
Epoch [110/120    avg_loss:0.010, val_acc:0.981]
Epoch [111/120    avg_loss:0.010, val_acc:0.981]
Epoch [112/120    avg_loss:0.009, val_acc:0.982]
Epoch [113/120    avg_loss:0.006, val_acc:0.982]
Epoch [114/120    avg_loss:0.006, val_acc:0.982]
Epoch [115/120    avg_loss:0.007, val_acc:0.982]
Epoch [116/120    avg_loss:0.009, val_acc:0.981]
Epoch [117/120    avg_loss:0.006, val_acc:0.982]
Epoch [118/120    avg_loss:0.009, val_acc:0.982]
Epoch [119/120    avg_loss:0.007, val_acc:0.983]
Epoch [120/120    avg_loss:0.006, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    0    0    0    1    0    0    0    4    6    5    0
     0    2    0]
 [   0    0    0  725    0   16    0    0    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0  427    0    5    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    5    0    0    0    0    0    0  425    0    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0   13    0    0    0    0
     0    0    0]
 [   0    0    9   64    0    3    1    0    0    0  769   26    1    0
     0    2    0]
 [   0    0    8    0    0    2   10    0    0    0    6 2183    0    1
     0    0    0]
 [   0    0    0    8    0    1    0    0    0    0    1    4  519    0
     0    0    1]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0   10    0    0    1    0    1    0    0    0
  1127    0    0]
 [   0    0    0    0    0    0   41    0    0    0    0    0    0    0
    58  248    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
96.48780487804878

F1 scores:
[       nan 0.93023256 0.98599222 0.9360878  0.99764706 0.95418994
 0.96122897 0.90909091 0.99299065 0.65       0.92874396 0.98555305
 0.97648166 0.99459459 0.96987952 0.82804674 0.96969697]

Kappa:
0.9599354763990747
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:27--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fc8ba08c668>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.342, val_acc:0.487]
Epoch [2/120    avg_loss:1.759, val_acc:0.649]
Epoch [3/120    avg_loss:1.464, val_acc:0.642]
Epoch [4/120    avg_loss:1.219, val_acc:0.734]
Epoch [5/120    avg_loss:0.841, val_acc:0.730]
Epoch [6/120    avg_loss:0.779, val_acc:0.748]
Epoch [7/120    avg_loss:0.586, val_acc:0.821]
Epoch [8/120    avg_loss:0.449, val_acc:0.838]
Epoch [9/120    avg_loss:0.359, val_acc:0.873]
Epoch [10/120    avg_loss:0.258, val_acc:0.868]
Epoch [11/120    avg_loss:0.230, val_acc:0.892]
Epoch [12/120    avg_loss:0.230, val_acc:0.901]
Epoch [13/120    avg_loss:0.226, val_acc:0.893]
Epoch [14/120    avg_loss:0.224, val_acc:0.860]
Epoch [15/120    avg_loss:0.201, val_acc:0.909]
Epoch [16/120    avg_loss:0.259, val_acc:0.864]
Epoch [17/120    avg_loss:0.321, val_acc:0.896]
Epoch [18/120    avg_loss:0.249, val_acc:0.900]
Epoch [19/120    avg_loss:0.136, val_acc:0.928]
Epoch [20/120    avg_loss:0.089, val_acc:0.959]
Epoch [21/120    avg_loss:0.053, val_acc:0.968]
Epoch [22/120    avg_loss:0.052, val_acc:0.959]
Epoch [23/120    avg_loss:0.068, val_acc:0.958]
Epoch [24/120    avg_loss:0.047, val_acc:0.950]
Epoch [25/120    avg_loss:0.082, val_acc:0.952]
Epoch [26/120    avg_loss:0.073, val_acc:0.956]
Epoch [27/120    avg_loss:0.037, val_acc:0.966]
Epoch [28/120    avg_loss:0.068, val_acc:0.924]
Epoch [29/120    avg_loss:0.084, val_acc:0.938]
Epoch [30/120    avg_loss:0.047, val_acc:0.958]
Epoch [31/120    avg_loss:0.046, val_acc:0.976]
Epoch [32/120    avg_loss:0.039, val_acc:0.973]
Epoch [33/120    avg_loss:0.036, val_acc:0.974]
Epoch [34/120    avg_loss:0.028, val_acc:0.961]
Epoch [35/120    avg_loss:0.021, val_acc:0.975]
Epoch [36/120    avg_loss:0.027, val_acc:0.966]
Epoch [37/120    avg_loss:0.038, val_acc:0.963]
Epoch [38/120    avg_loss:0.029, val_acc:0.967]
Epoch [39/120    avg_loss:0.024, val_acc:0.975]
Epoch [40/120    avg_loss:0.019, val_acc:0.971]
Epoch [41/120    avg_loss:0.014, val_acc:0.978]
Epoch [42/120    avg_loss:0.018, val_acc:0.980]
Epoch [43/120    avg_loss:0.034, val_acc:0.961]
Epoch [44/120    avg_loss:0.022, val_acc:0.980]
Epoch [45/120    avg_loss:0.027, val_acc:0.977]
Epoch [46/120    avg_loss:0.018, val_acc:0.976]
Epoch [47/120    avg_loss:0.017, val_acc:0.977]
Epoch [48/120    avg_loss:0.012, val_acc:0.979]
Epoch [49/120    avg_loss:0.011, val_acc:0.976]
Epoch [50/120    avg_loss:0.022, val_acc:0.967]
Epoch [51/120    avg_loss:0.016, val_acc:0.980]
Epoch [52/120    avg_loss:0.026, val_acc:0.977]
Epoch [53/120    avg_loss:0.017, val_acc:0.977]
Epoch [54/120    avg_loss:0.008, val_acc:0.980]
Epoch [55/120    avg_loss:0.008, val_acc:0.984]
Epoch [56/120    avg_loss:0.008, val_acc:0.982]
Epoch [57/120    avg_loss:0.009, val_acc:0.982]
Epoch [58/120    avg_loss:0.006, val_acc:0.982]
Epoch [59/120    avg_loss:0.012, val_acc:0.982]
Epoch [60/120    avg_loss:0.009, val_acc:0.979]
Epoch [61/120    avg_loss:0.009, val_acc:0.982]
Epoch [62/120    avg_loss:0.009, val_acc:0.983]
Epoch [63/120    avg_loss:0.008, val_acc:0.984]
Epoch [64/120    avg_loss:0.009, val_acc:0.980]
Epoch [65/120    avg_loss:0.006, val_acc:0.979]
Epoch [66/120    avg_loss:0.007, val_acc:0.983]
Epoch [67/120    avg_loss:0.006, val_acc:0.983]
Epoch [68/120    avg_loss:0.005, val_acc:0.980]
Epoch [69/120    avg_loss:0.007, val_acc:0.982]
Epoch [70/120    avg_loss:0.008, val_acc:0.983]
Epoch [71/120    avg_loss:0.004, val_acc:0.983]
Epoch [72/120    avg_loss:0.004, val_acc:0.988]
Epoch [73/120    avg_loss:0.004, val_acc:0.983]
Epoch [74/120    avg_loss:0.008, val_acc:0.983]
Epoch [75/120    avg_loss:0.010, val_acc:0.978]
Epoch [76/120    avg_loss:0.007, val_acc:0.982]
Epoch [77/120    avg_loss:0.005, val_acc:0.982]
Epoch [78/120    avg_loss:0.004, val_acc:0.984]
Epoch [79/120    avg_loss:0.005, val_acc:0.983]
Epoch [80/120    avg_loss:0.009, val_acc:0.975]
Epoch [81/120    avg_loss:0.005, val_acc:0.982]
Epoch [82/120    avg_loss:0.004, val_acc:0.982]
Epoch [83/120    avg_loss:0.003, val_acc:0.985]
Epoch [84/120    avg_loss:0.004, val_acc:0.982]
Epoch [85/120    avg_loss:0.003, val_acc:0.984]
Epoch [86/120    avg_loss:0.004, val_acc:0.985]
Epoch [87/120    avg_loss:0.005, val_acc:0.984]
Epoch [88/120    avg_loss:0.003, val_acc:0.984]
Epoch [89/120    avg_loss:0.003, val_acc:0.984]
Epoch [90/120    avg_loss:0.002, val_acc:0.984]
Epoch [91/120    avg_loss:0.002, val_acc:0.984]
Epoch [92/120    avg_loss:0.003, val_acc:0.984]
Epoch [93/120    avg_loss:0.003, val_acc:0.984]
Epoch [94/120    avg_loss:0.003, val_acc:0.983]
Epoch [95/120    avg_loss:0.003, val_acc:0.983]
Epoch [96/120    avg_loss:0.003, val_acc:0.985]
Epoch [97/120    avg_loss:0.004, val_acc:0.985]
Epoch [98/120    avg_loss:0.002, val_acc:0.985]
Epoch [99/120    avg_loss:0.003, val_acc:0.985]
Epoch [100/120    avg_loss:0.004, val_acc:0.985]
Epoch [101/120    avg_loss:0.001, val_acc:0.985]
Epoch [102/120    avg_loss:0.003, val_acc:0.985]
Epoch [103/120    avg_loss:0.002, val_acc:0.985]
Epoch [104/120    avg_loss:0.002, val_acc:0.985]
Epoch [105/120    avg_loss:0.002, val_acc:0.985]
Epoch [106/120    avg_loss:0.003, val_acc:0.985]
Epoch [107/120    avg_loss:0.004, val_acc:0.985]
Epoch [108/120    avg_loss:0.002, val_acc:0.985]
Epoch [109/120    avg_loss:0.002, val_acc:0.985]
Epoch [110/120    avg_loss:0.002, val_acc:0.985]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.002, val_acc:0.985]
Epoch [113/120    avg_loss:0.002, val_acc:0.985]
Epoch [114/120    avg_loss:0.003, val_acc:0.985]
Epoch [115/120    avg_loss:0.003, val_acc:0.985]
Epoch [116/120    avg_loss:0.004, val_acc:0.985]
Epoch [117/120    avg_loss:0.002, val_acc:0.985]
Epoch [118/120    avg_loss:0.005, val_acc:0.985]
Epoch [119/120    avg_loss:0.002, val_acc:0.985]
Epoch [120/120    avg_loss:0.002, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1268    3    0    1    0    0    0    0    1   11    1    0
     0    0    0]
 [   0    0    0  742    0    0    0    0    0    1    0    1    3    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   10    0    0    1    0    0    0    0  851   13    0    0
     0    0    0]
 [   0    0    9    0    0    0    0    0    0    0    2 2182   17    0
     0    0    0]
 [   0    0    0    5    0    1    0    0    0    0    0    0  525    0
     0    2    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1122   17    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    36  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.35230352303523

F1 scores:
[       nan 0.98765432 0.9856199  0.99065421 0.99764706 0.99425947
 0.9924357  1.         1.         0.97297297 0.98438404 0.98777727
 0.97042514 1.         0.97607656 0.90419162 0.98203593]

Kappa:
0.9812117960121468
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8273c3c6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.276, val_acc:0.448]
Epoch [2/120    avg_loss:1.585, val_acc:0.593]
Epoch [3/120    avg_loss:1.326, val_acc:0.631]
Epoch [4/120    avg_loss:0.977, val_acc:0.725]
Epoch [5/120    avg_loss:1.146, val_acc:0.734]
Epoch [6/120    avg_loss:0.944, val_acc:0.769]
Epoch [7/120    avg_loss:0.732, val_acc:0.719]
Epoch [8/120    avg_loss:0.498, val_acc:0.844]
Epoch [9/120    avg_loss:0.422, val_acc:0.731]
Epoch [10/120    avg_loss:0.531, val_acc:0.865]
Epoch [11/120    avg_loss:0.279, val_acc:0.865]
Epoch [12/120    avg_loss:0.227, val_acc:0.908]
Epoch [13/120    avg_loss:0.150, val_acc:0.929]
Epoch [14/120    avg_loss:0.175, val_acc:0.882]
Epoch [15/120    avg_loss:0.195, val_acc:0.924]
Epoch [16/120    avg_loss:0.177, val_acc:0.902]
Epoch [17/120    avg_loss:0.119, val_acc:0.923]
Epoch [18/120    avg_loss:0.095, val_acc:0.939]
Epoch [19/120    avg_loss:0.095, val_acc:0.939]
Epoch [20/120    avg_loss:0.085, val_acc:0.939]
Epoch [21/120    avg_loss:0.084, val_acc:0.939]
Epoch [22/120    avg_loss:0.078, val_acc:0.935]
Epoch [23/120    avg_loss:0.052, val_acc:0.961]
Epoch [24/120    avg_loss:0.057, val_acc:0.958]
Epoch [25/120    avg_loss:0.040, val_acc:0.960]
Epoch [26/120    avg_loss:0.064, val_acc:0.946]
Epoch [27/120    avg_loss:0.052, val_acc:0.959]
Epoch [28/120    avg_loss:0.045, val_acc:0.950]
Epoch [29/120    avg_loss:0.046, val_acc:0.953]
Epoch [30/120    avg_loss:0.030, val_acc:0.963]
Epoch [31/120    avg_loss:0.036, val_acc:0.961]
Epoch [32/120    avg_loss:0.038, val_acc:0.958]
Epoch [33/120    avg_loss:0.050, val_acc:0.942]
Epoch [34/120    avg_loss:0.040, val_acc:0.956]
Epoch [35/120    avg_loss:0.030, val_acc:0.964]
Epoch [36/120    avg_loss:0.027, val_acc:0.970]
Epoch [37/120    avg_loss:0.028, val_acc:0.956]
Epoch [38/120    avg_loss:0.023, val_acc:0.954]
Epoch [39/120    avg_loss:0.013, val_acc:0.972]
Epoch [40/120    avg_loss:0.015, val_acc:0.969]
Epoch [41/120    avg_loss:0.020, val_acc:0.971]
Epoch [42/120    avg_loss:0.016, val_acc:0.970]
Epoch [43/120    avg_loss:0.016, val_acc:0.969]
Epoch [44/120    avg_loss:0.011, val_acc:0.974]
Epoch [45/120    avg_loss:0.010, val_acc:0.975]
Epoch [46/120    avg_loss:0.011, val_acc:0.977]
Epoch [47/120    avg_loss:0.014, val_acc:0.968]
Epoch [48/120    avg_loss:0.018, val_acc:0.969]
Epoch [49/120    avg_loss:0.019, val_acc:0.957]
Epoch [50/120    avg_loss:0.020, val_acc:0.961]
Epoch [51/120    avg_loss:0.016, val_acc:0.974]
Epoch [52/120    avg_loss:0.016, val_acc:0.964]
Epoch [53/120    avg_loss:0.016, val_acc:0.967]
Epoch [54/120    avg_loss:0.013, val_acc:0.971]
Epoch [55/120    avg_loss:0.004, val_acc:0.971]
Epoch [56/120    avg_loss:0.011, val_acc:0.969]
Epoch [57/120    avg_loss:0.013, val_acc:0.976]
Epoch [58/120    avg_loss:0.005, val_acc:0.975]
Epoch [59/120    avg_loss:0.006, val_acc:0.975]
Epoch [60/120    avg_loss:0.007, val_acc:0.975]
Epoch [61/120    avg_loss:0.007, val_acc:0.975]
Epoch [62/120    avg_loss:0.003, val_acc:0.975]
Epoch [63/120    avg_loss:0.005, val_acc:0.977]
Epoch [64/120    avg_loss:0.003, val_acc:0.977]
Epoch [65/120    avg_loss:0.004, val_acc:0.977]
Epoch [66/120    avg_loss:0.006, val_acc:0.977]
Epoch [67/120    avg_loss:0.003, val_acc:0.977]
Epoch [68/120    avg_loss:0.005, val_acc:0.977]
Epoch [69/120    avg_loss:0.004, val_acc:0.977]
Epoch [70/120    avg_loss:0.003, val_acc:0.977]
Epoch [71/120    avg_loss:0.003, val_acc:0.977]
Epoch [72/120    avg_loss:0.005, val_acc:0.977]
Epoch [73/120    avg_loss:0.004, val_acc:0.976]
Epoch [74/120    avg_loss:0.003, val_acc:0.977]
Epoch [75/120    avg_loss:0.004, val_acc:0.977]
Epoch [76/120    avg_loss:0.003, val_acc:0.977]
Epoch [77/120    avg_loss:0.004, val_acc:0.977]
Epoch [78/120    avg_loss:0.004, val_acc:0.976]
Epoch [79/120    avg_loss:0.003, val_acc:0.976]
Epoch [80/120    avg_loss:0.003, val_acc:0.976]
Epoch [81/120    avg_loss:0.003, val_acc:0.977]
Epoch [82/120    avg_loss:0.004, val_acc:0.977]
Epoch [83/120    avg_loss:0.004, val_acc:0.978]
Epoch [84/120    avg_loss:0.005, val_acc:0.977]
Epoch [85/120    avg_loss:0.003, val_acc:0.977]
Epoch [86/120    avg_loss:0.004, val_acc:0.977]
Epoch [87/120    avg_loss:0.004, val_acc:0.977]
Epoch [88/120    avg_loss:0.003, val_acc:0.978]
Epoch [89/120    avg_loss:0.003, val_acc:0.978]
Epoch [90/120    avg_loss:0.004, val_acc:0.977]
Epoch [91/120    avg_loss:0.003, val_acc:0.976]
Epoch [92/120    avg_loss:0.003, val_acc:0.975]
Epoch [93/120    avg_loss:0.007, val_acc:0.975]
Epoch [94/120    avg_loss:0.004, val_acc:0.976]
Epoch [95/120    avg_loss:0.004, val_acc:0.975]
Epoch [96/120    avg_loss:0.005, val_acc:0.976]
Epoch [97/120    avg_loss:0.003, val_acc:0.977]
Epoch [98/120    avg_loss:0.004, val_acc:0.977]
Epoch [99/120    avg_loss:0.003, val_acc:0.977]
Epoch [100/120    avg_loss:0.003, val_acc:0.976]
Epoch [101/120    avg_loss:0.003, val_acc:0.977]
Epoch [102/120    avg_loss:0.007, val_acc:0.977]
Epoch [103/120    avg_loss:0.005, val_acc:0.977]
Epoch [104/120    avg_loss:0.002, val_acc:0.977]
Epoch [105/120    avg_loss:0.003, val_acc:0.977]
Epoch [106/120    avg_loss:0.005, val_acc:0.977]
Epoch [107/120    avg_loss:0.003, val_acc:0.977]
Epoch [108/120    avg_loss:0.004, val_acc:0.977]
Epoch [109/120    avg_loss:0.004, val_acc:0.977]
Epoch [110/120    avg_loss:0.006, val_acc:0.977]
Epoch [111/120    avg_loss:0.004, val_acc:0.976]
Epoch [112/120    avg_loss:0.003, val_acc:0.976]
Epoch [113/120    avg_loss:0.003, val_acc:0.977]
Epoch [114/120    avg_loss:0.005, val_acc:0.976]
Epoch [115/120    avg_loss:0.002, val_acc:0.976]
Epoch [116/120    avg_loss:0.003, val_acc:0.976]
Epoch [117/120    avg_loss:0.004, val_acc:0.976]
Epoch [118/120    avg_loss:0.004, val_acc:0.976]
Epoch [119/120    avg_loss:0.005, val_acc:0.976]
Epoch [120/120    avg_loss:0.003, val_acc:0.976]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1263    3    4    0    0    0    0    0    3   12    0    0
     0    0    0]
 [   0    0    0  728    2    2    5    0    0    2    0    5    2    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    1    0    0    0    0  842   25    0    0
     2    1    0]
 [   0    0   19    2    0    0    0    0    0    0    0 2170   18    1
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    1    3  524    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1126   12    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    32  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.96205962059621

F1 scores:
[       nan 0.98765432 0.98249708 0.98245614 0.98611111 0.99541284
 0.98422239 1.         1.         0.94736842 0.97793264 0.98056936
 0.97037037 0.99462366 0.97870491 0.90963855 0.97619048]

Kappa:
0.9767603495839455
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:29--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff25d115630>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.432, val_acc:0.456]
Epoch [2/120    avg_loss:1.710, val_acc:0.615]
Epoch [3/120    avg_loss:1.418, val_acc:0.662]
Epoch [4/120    avg_loss:1.078, val_acc:0.644]
Epoch [5/120    avg_loss:0.974, val_acc:0.765]
Epoch [6/120    avg_loss:0.823, val_acc:0.699]
Epoch [7/120    avg_loss:0.691, val_acc:0.817]
Epoch [8/120    avg_loss:0.704, val_acc:0.765]
Epoch [9/120    avg_loss:0.578, val_acc:0.798]
Epoch [10/120    avg_loss:0.361, val_acc:0.863]
Epoch [11/120    avg_loss:0.251, val_acc:0.912]
Epoch [12/120    avg_loss:0.337, val_acc:0.889]
Epoch [13/120    avg_loss:0.321, val_acc:0.885]
Epoch [14/120    avg_loss:0.248, val_acc:0.922]
Epoch [15/120    avg_loss:0.140, val_acc:0.923]
Epoch [16/120    avg_loss:0.118, val_acc:0.935]
Epoch [17/120    avg_loss:0.135, val_acc:0.920]
Epoch [18/120    avg_loss:0.114, val_acc:0.944]
Epoch [19/120    avg_loss:0.087, val_acc:0.941]
Epoch [20/120    avg_loss:0.069, val_acc:0.951]
Epoch [21/120    avg_loss:0.059, val_acc:0.960]
Epoch [22/120    avg_loss:0.059, val_acc:0.961]
Epoch [23/120    avg_loss:0.056, val_acc:0.950]
Epoch [24/120    avg_loss:0.073, val_acc:0.961]
Epoch [25/120    avg_loss:0.113, val_acc:0.947]
Epoch [26/120    avg_loss:0.074, val_acc:0.932]
Epoch [27/120    avg_loss:0.093, val_acc:0.952]
Epoch [28/120    avg_loss:0.058, val_acc:0.960]
Epoch [29/120    avg_loss:0.043, val_acc:0.968]
Epoch [30/120    avg_loss:0.041, val_acc:0.960]
Epoch [31/120    avg_loss:0.034, val_acc:0.967]
Epoch [32/120    avg_loss:0.048, val_acc:0.957]
Epoch [33/120    avg_loss:0.035, val_acc:0.954]
Epoch [34/120    avg_loss:0.029, val_acc:0.973]
Epoch [35/120    avg_loss:0.017, val_acc:0.976]
Epoch [36/120    avg_loss:0.020, val_acc:0.968]
Epoch [37/120    avg_loss:0.016, val_acc:0.976]
Epoch [38/120    avg_loss:0.018, val_acc:0.974]
Epoch [39/120    avg_loss:0.028, val_acc:0.963]
Epoch [40/120    avg_loss:0.030, val_acc:0.968]
Epoch [41/120    avg_loss:0.015, val_acc:0.970]
Epoch [42/120    avg_loss:0.022, val_acc:0.976]
Epoch [43/120    avg_loss:0.023, val_acc:0.976]
Epoch [44/120    avg_loss:0.064, val_acc:0.950]
Epoch [45/120    avg_loss:0.033, val_acc:0.968]
Epoch [46/120    avg_loss:0.034, val_acc:0.964]
Epoch [47/120    avg_loss:0.020, val_acc:0.975]
Epoch [48/120    avg_loss:0.015, val_acc:0.971]
Epoch [49/120    avg_loss:0.012, val_acc:0.976]
Epoch [50/120    avg_loss:0.031, val_acc:0.942]
Epoch [51/120    avg_loss:0.033, val_acc:0.952]
Epoch [52/120    avg_loss:0.028, val_acc:0.966]
Epoch [53/120    avg_loss:0.026, val_acc:0.955]
Epoch [54/120    avg_loss:0.058, val_acc:0.964]
Epoch [55/120    avg_loss:0.020, val_acc:0.969]
Epoch [56/120    avg_loss:0.016, val_acc:0.979]
Epoch [57/120    avg_loss:0.009, val_acc:0.973]
Epoch [58/120    avg_loss:0.011, val_acc:0.978]
Epoch [59/120    avg_loss:0.010, val_acc:0.973]
Epoch [60/120    avg_loss:0.008, val_acc:0.976]
Epoch [61/120    avg_loss:0.009, val_acc:0.979]
Epoch [62/120    avg_loss:0.008, val_acc:0.974]
Epoch [63/120    avg_loss:0.007, val_acc:0.976]
Epoch [64/120    avg_loss:0.007, val_acc:0.980]
Epoch [65/120    avg_loss:0.003, val_acc:0.980]
Epoch [66/120    avg_loss:0.009, val_acc:0.980]
Epoch [67/120    avg_loss:0.011, val_acc:0.972]
Epoch [68/120    avg_loss:0.007, val_acc:0.980]
Epoch [69/120    avg_loss:0.005, val_acc:0.979]
Epoch [70/120    avg_loss:0.005, val_acc:0.978]
Epoch [71/120    avg_loss:0.004, val_acc:0.981]
Epoch [72/120    avg_loss:0.005, val_acc:0.980]
Epoch [73/120    avg_loss:0.005, val_acc:0.980]
Epoch [74/120    avg_loss:0.004, val_acc:0.979]
Epoch [75/120    avg_loss:0.010, val_acc:0.980]
Epoch [76/120    avg_loss:0.020, val_acc:0.973]
Epoch [77/120    avg_loss:0.018, val_acc:0.970]
Epoch [78/120    avg_loss:0.012, val_acc:0.982]
Epoch [79/120    avg_loss:0.006, val_acc:0.978]
Epoch [80/120    avg_loss:0.007, val_acc:0.976]
Epoch [81/120    avg_loss:0.004, val_acc:0.979]
Epoch [82/120    avg_loss:0.005, val_acc:0.978]
Epoch [83/120    avg_loss:0.004, val_acc:0.976]
Epoch [84/120    avg_loss:0.004, val_acc:0.981]
Epoch [85/120    avg_loss:0.003, val_acc:0.979]
Epoch [86/120    avg_loss:0.006, val_acc:0.976]
Epoch [87/120    avg_loss:0.005, val_acc:0.980]
Epoch [88/120    avg_loss:0.006, val_acc:0.977]
Epoch [89/120    avg_loss:0.004, val_acc:0.979]
Epoch [90/120    avg_loss:0.002, val_acc:0.980]
Epoch [91/120    avg_loss:0.003, val_acc:0.979]
Epoch [92/120    avg_loss:0.003, val_acc:0.980]
Epoch [93/120    avg_loss:0.003, val_acc:0.980]
Epoch [94/120    avg_loss:0.002, val_acc:0.980]
Epoch [95/120    avg_loss:0.004, val_acc:0.980]
Epoch [96/120    avg_loss:0.003, val_acc:0.980]
Epoch [97/120    avg_loss:0.002, val_acc:0.980]
Epoch [98/120    avg_loss:0.002, val_acc:0.980]
Epoch [99/120    avg_loss:0.002, val_acc:0.980]
Epoch [100/120    avg_loss:0.002, val_acc:0.980]
Epoch [101/120    avg_loss:0.003, val_acc:0.980]
Epoch [102/120    avg_loss:0.003, val_acc:0.980]
Epoch [103/120    avg_loss:0.002, val_acc:0.980]
Epoch [104/120    avg_loss:0.002, val_acc:0.980]
Epoch [105/120    avg_loss:0.003, val_acc:0.980]
Epoch [106/120    avg_loss:0.002, val_acc:0.980]
Epoch [107/120    avg_loss:0.003, val_acc:0.980]
Epoch [108/120    avg_loss:0.002, val_acc:0.980]
Epoch [109/120    avg_loss:0.003, val_acc:0.980]
Epoch [110/120    avg_loss:0.001, val_acc:0.980]
Epoch [111/120    avg_loss:0.003, val_acc:0.980]
Epoch [112/120    avg_loss:0.003, val_acc:0.980]
Epoch [113/120    avg_loss:0.002, val_acc:0.980]
Epoch [114/120    avg_loss:0.002, val_acc:0.980]
Epoch [115/120    avg_loss:0.003, val_acc:0.980]
Epoch [116/120    avg_loss:0.002, val_acc:0.980]
Epoch [117/120    avg_loss:0.002, val_acc:0.980]
Epoch [118/120    avg_loss:0.003, val_acc:0.980]
Epoch [119/120    avg_loss:0.002, val_acc:0.980]
Epoch [120/120    avg_loss:0.002, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1272    3    0    0    0    0    0    0    4    6    0    0
     0    0    0]
 [   0    0    0  739    0    0    0    0    0    2    1    0    5    0
     0    0    0]
 [   0    0    0    3  210    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    1    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    0    0    0    0    0  845   24    0    0
     0    0    0]
 [   0    0   12    0    0    0    0    0    0    0    1 2180   17    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0    0    0    0  525    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    49  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.11382113821138

F1 scores:
[       nan 0.98765432 0.98796117 0.98730795 0.9929078  0.99305556
 0.99392097 1.         1.         0.92307692 0.97857556 0.98575627
 0.96774194 1.         0.97110824 0.89602446 0.95808383]

Kappa:
0.978485742620991
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:31--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb24d516710>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.280, val_acc:0.370]
Epoch [2/120    avg_loss:1.777, val_acc:0.662]
Epoch [3/120    avg_loss:1.359, val_acc:0.655]
Epoch [4/120    avg_loss:1.163, val_acc:0.697]
Epoch [5/120    avg_loss:1.004, val_acc:0.720]
Epoch [6/120    avg_loss:1.038, val_acc:0.649]
Epoch [7/120    avg_loss:0.704, val_acc:0.789]
Epoch [8/120    avg_loss:0.588, val_acc:0.797]
Epoch [9/120    avg_loss:0.428, val_acc:0.859]
Epoch [10/120    avg_loss:0.415, val_acc:0.824]
Epoch [11/120    avg_loss:0.428, val_acc:0.831]
Epoch [12/120    avg_loss:0.350, val_acc:0.896]
Epoch [13/120    avg_loss:0.254, val_acc:0.814]
Epoch [14/120    avg_loss:0.290, val_acc:0.859]
Epoch [15/120    avg_loss:0.220, val_acc:0.909]
Epoch [16/120    avg_loss:0.153, val_acc:0.935]
Epoch [17/120    avg_loss:0.155, val_acc:0.923]
Epoch [18/120    avg_loss:0.124, val_acc:0.931]
Epoch [19/120    avg_loss:0.124, val_acc:0.947]
Epoch [20/120    avg_loss:0.074, val_acc:0.940]
Epoch [21/120    avg_loss:0.076, val_acc:0.952]
Epoch [22/120    avg_loss:0.081, val_acc:0.924]
Epoch [23/120    avg_loss:0.063, val_acc:0.947]
Epoch [24/120    avg_loss:0.052, val_acc:0.961]
Epoch [25/120    avg_loss:0.062, val_acc:0.945]
Epoch [26/120    avg_loss:0.055, val_acc:0.940]
Epoch [27/120    avg_loss:0.058, val_acc:0.958]
Epoch [28/120    avg_loss:0.041, val_acc:0.952]
Epoch [29/120    avg_loss:0.060, val_acc:0.954]
Epoch [30/120    avg_loss:0.043, val_acc:0.968]
Epoch [31/120    avg_loss:0.032, val_acc:0.969]
Epoch [32/120    avg_loss:0.025, val_acc:0.966]
Epoch [33/120    avg_loss:0.032, val_acc:0.952]
Epoch [34/120    avg_loss:0.027, val_acc:0.969]
Epoch [35/120    avg_loss:0.018, val_acc:0.960]
Epoch [36/120    avg_loss:0.013, val_acc:0.971]
Epoch [37/120    avg_loss:0.020, val_acc:0.949]
Epoch [38/120    avg_loss:0.024, val_acc:0.971]
Epoch [39/120    avg_loss:0.014, val_acc:0.975]
Epoch [40/120    avg_loss:0.019, val_acc:0.971]
Epoch [41/120    avg_loss:0.029, val_acc:0.961]
Epoch [42/120    avg_loss:0.036, val_acc:0.955]
Epoch [43/120    avg_loss:0.039, val_acc:0.959]
Epoch [44/120    avg_loss:0.028, val_acc:0.933]
Epoch [45/120    avg_loss:0.038, val_acc:0.967]
Epoch [46/120    avg_loss:0.021, val_acc:0.971]
Epoch [47/120    avg_loss:0.017, val_acc:0.970]
Epoch [48/120    avg_loss:0.015, val_acc:0.976]
Epoch [49/120    avg_loss:0.008, val_acc:0.976]
Epoch [50/120    avg_loss:0.006, val_acc:0.979]
Epoch [51/120    avg_loss:0.008, val_acc:0.972]
Epoch [52/120    avg_loss:0.014, val_acc:0.972]
Epoch [53/120    avg_loss:0.025, val_acc:0.967]
Epoch [54/120    avg_loss:0.017, val_acc:0.965]
Epoch [55/120    avg_loss:0.020, val_acc:0.975]
Epoch [56/120    avg_loss:0.008, val_acc:0.970]
Epoch [57/120    avg_loss:0.007, val_acc:0.975]
Epoch [58/120    avg_loss:0.005, val_acc:0.975]
Epoch [59/120    avg_loss:0.005, val_acc:0.972]
Epoch [60/120    avg_loss:0.012, val_acc:0.969]
Epoch [61/120    avg_loss:0.015, val_acc:0.961]
Epoch [62/120    avg_loss:0.026, val_acc:0.972]
Epoch [63/120    avg_loss:0.010, val_acc:0.977]
Epoch [64/120    avg_loss:0.007, val_acc:0.977]
Epoch [65/120    avg_loss:0.005, val_acc:0.976]
Epoch [66/120    avg_loss:0.006, val_acc:0.976]
Epoch [67/120    avg_loss:0.006, val_acc:0.976]
Epoch [68/120    avg_loss:0.005, val_acc:0.976]
Epoch [69/120    avg_loss:0.006, val_acc:0.976]
Epoch [70/120    avg_loss:0.005, val_acc:0.976]
Epoch [71/120    avg_loss:0.004, val_acc:0.977]
Epoch [72/120    avg_loss:0.006, val_acc:0.977]
Epoch [73/120    avg_loss:0.006, val_acc:0.976]
Epoch [74/120    avg_loss:0.004, val_acc:0.977]
Epoch [75/120    avg_loss:0.006, val_acc:0.977]
Epoch [76/120    avg_loss:0.006, val_acc:0.978]
Epoch [77/120    avg_loss:0.005, val_acc:0.978]
Epoch [78/120    avg_loss:0.004, val_acc:0.978]
Epoch [79/120    avg_loss:0.004, val_acc:0.978]
Epoch [80/120    avg_loss:0.004, val_acc:0.978]
Epoch [81/120    avg_loss:0.004, val_acc:0.978]
Epoch [82/120    avg_loss:0.005, val_acc:0.978]
Epoch [83/120    avg_loss:0.005, val_acc:0.978]
Epoch [84/120    avg_loss:0.005, val_acc:0.978]
Epoch [85/120    avg_loss:0.005, val_acc:0.978]
Epoch [86/120    avg_loss:0.003, val_acc:0.978]
Epoch [87/120    avg_loss:0.003, val_acc:0.978]
Epoch [88/120    avg_loss:0.004, val_acc:0.978]
Epoch [89/120    avg_loss:0.007, val_acc:0.978]
Epoch [90/120    avg_loss:0.004, val_acc:0.978]
Epoch [91/120    avg_loss:0.004, val_acc:0.978]
Epoch [92/120    avg_loss:0.006, val_acc:0.978]
Epoch [93/120    avg_loss:0.004, val_acc:0.978]
Epoch [94/120    avg_loss:0.003, val_acc:0.978]
Epoch [95/120    avg_loss:0.006, val_acc:0.978]
Epoch [96/120    avg_loss:0.005, val_acc:0.978]
Epoch [97/120    avg_loss:0.008, val_acc:0.978]
Epoch [98/120    avg_loss:0.005, val_acc:0.978]
Epoch [99/120    avg_loss:0.004, val_acc:0.978]
Epoch [100/120    avg_loss:0.005, val_acc:0.978]
Epoch [101/120    avg_loss:0.004, val_acc:0.978]
Epoch [102/120    avg_loss:0.006, val_acc:0.978]
Epoch [103/120    avg_loss:0.004, val_acc:0.978]
Epoch [104/120    avg_loss:0.004, val_acc:0.978]
Epoch [105/120    avg_loss:0.004, val_acc:0.978]
Epoch [106/120    avg_loss:0.004, val_acc:0.978]
Epoch [107/120    avg_loss:0.006, val_acc:0.978]
Epoch [108/120    avg_loss:0.005, val_acc:0.978]
Epoch [109/120    avg_loss:0.006, val_acc:0.978]
Epoch [110/120    avg_loss:0.007, val_acc:0.978]
Epoch [111/120    avg_loss:0.004, val_acc:0.978]
Epoch [112/120    avg_loss:0.004, val_acc:0.978]
Epoch [113/120    avg_loss:0.005, val_acc:0.978]
Epoch [114/120    avg_loss:0.006, val_acc:0.978]
Epoch [115/120    avg_loss:0.005, val_acc:0.978]
Epoch [116/120    avg_loss:0.004, val_acc:0.978]
Epoch [117/120    avg_loss:0.006, val_acc:0.978]
Epoch [118/120    avg_loss:0.005, val_acc:0.978]
Epoch [119/120    avg_loss:0.006, val_acc:0.978]
Epoch [120/120    avg_loss:0.003, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1278    1    0    0    2    0    0    1    1    2    0    0
     0    0    0]
 [   0    0    0  725    1    3    0    0    0    3    1    5    9    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    0    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    1    0    0    0    0  847   22    0    0
     3    0    0]
 [   0    0    7    0    0    0    0    0    0    1    7 2175   20    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0    0    0    0  530    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1126   13    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    49  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.13550135501355

F1 scores:
[       nan 1.         0.99377916 0.9817197  0.99530516 0.99196326
 0.99242424 1.         1.         0.87804878 0.97862507 0.98527746
 0.96803653 1.         0.97027143 0.89570552 0.98203593]

Kappa:
0.9787386360508745
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:32--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f15227f56a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.248, val_acc:0.449]
Epoch [2/120    avg_loss:1.701, val_acc:0.517]
Epoch [3/120    avg_loss:1.263, val_acc:0.658]
Epoch [4/120    avg_loss:1.106, val_acc:0.745]
Epoch [5/120    avg_loss:0.965, val_acc:0.782]
Epoch [6/120    avg_loss:0.719, val_acc:0.814]
Epoch [7/120    avg_loss:0.843, val_acc:0.762]
Epoch [8/120    avg_loss:0.986, val_acc:0.753]
Epoch [9/120    avg_loss:0.643, val_acc:0.816]
Epoch [10/120    avg_loss:0.501, val_acc:0.854]
Epoch [11/120    avg_loss:0.502, val_acc:0.855]
Epoch [12/120    avg_loss:0.344, val_acc:0.849]
Epoch [13/120    avg_loss:0.222, val_acc:0.910]
Epoch [14/120    avg_loss:0.195, val_acc:0.922]
Epoch [15/120    avg_loss:0.215, val_acc:0.887]
Epoch [16/120    avg_loss:0.147, val_acc:0.928]
Epoch [17/120    avg_loss:0.196, val_acc:0.936]
Epoch [18/120    avg_loss:0.128, val_acc:0.926]
Epoch [19/120    avg_loss:0.121, val_acc:0.953]
Epoch [20/120    avg_loss:0.092, val_acc:0.950]
Epoch [21/120    avg_loss:0.072, val_acc:0.949]
Epoch [22/120    avg_loss:0.063, val_acc:0.950]
Epoch [23/120    avg_loss:0.062, val_acc:0.945]
Epoch [24/120    avg_loss:0.070, val_acc:0.922]
Epoch [25/120    avg_loss:0.099, val_acc:0.925]
Epoch [26/120    avg_loss:0.058, val_acc:0.963]
Epoch [27/120    avg_loss:0.041, val_acc:0.958]
Epoch [28/120    avg_loss:0.039, val_acc:0.969]
Epoch [29/120    avg_loss:0.030, val_acc:0.968]
Epoch [30/120    avg_loss:0.054, val_acc:0.960]
Epoch [31/120    avg_loss:0.028, val_acc:0.967]
Epoch [32/120    avg_loss:0.048, val_acc:0.968]
Epoch [33/120    avg_loss:0.068, val_acc:0.961]
Epoch [34/120    avg_loss:0.038, val_acc:0.957]
Epoch [35/120    avg_loss:0.028, val_acc:0.967]
Epoch [36/120    avg_loss:0.020, val_acc:0.972]
Epoch [37/120    avg_loss:0.022, val_acc:0.971]
Epoch [38/120    avg_loss:0.021, val_acc:0.969]
Epoch [39/120    avg_loss:0.019, val_acc:0.973]
Epoch [40/120    avg_loss:0.016, val_acc:0.971]
Epoch [41/120    avg_loss:0.015, val_acc:0.975]
Epoch [42/120    avg_loss:0.018, val_acc:0.973]
Epoch [43/120    avg_loss:0.024, val_acc:0.964]
Epoch [44/120    avg_loss:0.019, val_acc:0.965]
Epoch [45/120    avg_loss:0.010, val_acc:0.971]
Epoch [46/120    avg_loss:0.014, val_acc:0.971]
Epoch [47/120    avg_loss:0.042, val_acc:0.960]
Epoch [48/120    avg_loss:0.019, val_acc:0.972]
Epoch [49/120    avg_loss:0.037, val_acc:0.961]
Epoch [50/120    avg_loss:0.013, val_acc:0.973]
Epoch [51/120    avg_loss:0.012, val_acc:0.976]
Epoch [52/120    avg_loss:0.012, val_acc:0.968]
Epoch [53/120    avg_loss:0.013, val_acc:0.972]
Epoch [54/120    avg_loss:0.011, val_acc:0.971]
Epoch [55/120    avg_loss:0.006, val_acc:0.975]
Epoch [56/120    avg_loss:0.009, val_acc:0.975]
Epoch [57/120    avg_loss:0.019, val_acc:0.970]
Epoch [58/120    avg_loss:0.012, val_acc:0.970]
Epoch [59/120    avg_loss:0.011, val_acc:0.976]
Epoch [60/120    avg_loss:0.009, val_acc:0.975]
Epoch [61/120    avg_loss:0.009, val_acc:0.965]
Epoch [62/120    avg_loss:0.012, val_acc:0.972]
Epoch [63/120    avg_loss:0.006, val_acc:0.974]
Epoch [64/120    avg_loss:0.006, val_acc:0.976]
Epoch [65/120    avg_loss:0.009, val_acc:0.974]
Epoch [66/120    avg_loss:0.007, val_acc:0.973]
Epoch [67/120    avg_loss:0.009, val_acc:0.978]
Epoch [68/120    avg_loss:0.006, val_acc:0.975]
Epoch [69/120    avg_loss:0.005, val_acc:0.978]
Epoch [70/120    avg_loss:0.010, val_acc:0.960]
Epoch [71/120    avg_loss:0.038, val_acc:0.963]
Epoch [72/120    avg_loss:0.010, val_acc:0.974]
Epoch [73/120    avg_loss:0.015, val_acc:0.965]
Epoch [74/120    avg_loss:0.008, val_acc:0.971]
Epoch [75/120    avg_loss:0.007, val_acc:0.974]
Epoch [76/120    avg_loss:0.006, val_acc:0.973]
Epoch [77/120    avg_loss:0.004, val_acc:0.976]
Epoch [78/120    avg_loss:0.004, val_acc:0.973]
Epoch [79/120    avg_loss:0.008, val_acc:0.973]
Epoch [80/120    avg_loss:0.007, val_acc:0.977]
Epoch [81/120    avg_loss:0.005, val_acc:0.976]
Epoch [82/120    avg_loss:0.005, val_acc:0.975]
Epoch [83/120    avg_loss:0.004, val_acc:0.976]
Epoch [84/120    avg_loss:0.004, val_acc:0.975]
Epoch [85/120    avg_loss:0.002, val_acc:0.977]
Epoch [86/120    avg_loss:0.002, val_acc:0.977]
Epoch [87/120    avg_loss:0.002, val_acc:0.977]
Epoch [88/120    avg_loss:0.003, val_acc:0.978]
Epoch [89/120    avg_loss:0.004, val_acc:0.979]
Epoch [90/120    avg_loss:0.003, val_acc:0.979]
Epoch [91/120    avg_loss:0.002, val_acc:0.979]
Epoch [92/120    avg_loss:0.002, val_acc:0.979]
Epoch [93/120    avg_loss:0.004, val_acc:0.979]
Epoch [94/120    avg_loss:0.003, val_acc:0.979]
Epoch [95/120    avg_loss:0.004, val_acc:0.978]
Epoch [96/120    avg_loss:0.003, val_acc:0.980]
Epoch [97/120    avg_loss:0.003, val_acc:0.978]
Epoch [98/120    avg_loss:0.002, val_acc:0.979]
Epoch [99/120    avg_loss:0.004, val_acc:0.978]
Epoch [100/120    avg_loss:0.002, val_acc:0.979]
Epoch [101/120    avg_loss:0.007, val_acc:0.978]
Epoch [102/120    avg_loss:0.003, val_acc:0.978]
Epoch [103/120    avg_loss:0.003, val_acc:0.979]
Epoch [104/120    avg_loss:0.002, val_acc:0.976]
Epoch [105/120    avg_loss:0.003, val_acc:0.977]
Epoch [106/120    avg_loss:0.002, val_acc:0.977]
Epoch [107/120    avg_loss:0.004, val_acc:0.978]
Epoch [108/120    avg_loss:0.002, val_acc:0.978]
Epoch [109/120    avg_loss:0.003, val_acc:0.978]
Epoch [110/120    avg_loss:0.003, val_acc:0.978]
Epoch [111/120    avg_loss:0.002, val_acc:0.978]
Epoch [112/120    avg_loss:0.003, val_acc:0.978]
Epoch [113/120    avg_loss:0.002, val_acc:0.978]
Epoch [114/120    avg_loss:0.003, val_acc:0.978]
Epoch [115/120    avg_loss:0.003, val_acc:0.978]
Epoch [116/120    avg_loss:0.003, val_acc:0.978]
Epoch [117/120    avg_loss:0.003, val_acc:0.978]
Epoch [118/120    avg_loss:0.003, val_acc:0.977]
Epoch [119/120    avg_loss:0.002, val_acc:0.977]
Epoch [120/120    avg_loss:0.002, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    1 1278    0    2    0    0    0    0    0    2    2    0    0
     0    0    0]
 [   0    0    0  723    0   11    0    0    0    2    0    6    5    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    0    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    3    0    0    0    0    0    0    0  855   14    0    0
     2    1    0]
 [   0    0    9    0    0    0    0    0    0    0    2 2181   15    0
     3    0    0]
 [   0    0    0    6    0    0    0    0    0    0    0    0  524    0
     0    2    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1134    5    0]
 [   0    0    0    0    0    0    5    0    0    0    0    0    0    0
    80  262    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.91869918699187

F1 scores:
[       nan 0.96296296 0.99223602 0.97901151 0.99297424 0.98521047
 0.99392097 1.         1.         0.94736842 0.98559078 0.98799547
 0.96857671 1.         0.96060991 0.84927066 0.96385542]

Kappa:
0.9762549303496054
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff467edc6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.367, val_acc:0.451]
Epoch [2/120    avg_loss:1.749, val_acc:0.581]
Epoch [3/120    avg_loss:1.390, val_acc:0.542]
Epoch [4/120    avg_loss:1.096, val_acc:0.748]
Epoch [5/120    avg_loss:0.790, val_acc:0.782]
Epoch [6/120    avg_loss:0.744, val_acc:0.716]
Epoch [7/120    avg_loss:0.675, val_acc:0.827]
Epoch [8/120    avg_loss:0.459, val_acc:0.832]
Epoch [9/120    avg_loss:0.647, val_acc:0.823]
Epoch [10/120    avg_loss:0.726, val_acc:0.799]
Epoch [11/120    avg_loss:0.530, val_acc:0.831]
Epoch [12/120    avg_loss:0.327, val_acc:0.889]
Epoch [13/120    avg_loss:0.444, val_acc:0.807]
Epoch [14/120    avg_loss:0.276, val_acc:0.900]
Epoch [15/120    avg_loss:0.152, val_acc:0.911]
Epoch [16/120    avg_loss:0.135, val_acc:0.938]
Epoch [17/120    avg_loss:0.113, val_acc:0.923]
Epoch [18/120    avg_loss:0.096, val_acc:0.929]
Epoch [19/120    avg_loss:0.106, val_acc:0.929]
Epoch [20/120    avg_loss:0.071, val_acc:0.934]
Epoch [21/120    avg_loss:0.062, val_acc:0.949]
Epoch [22/120    avg_loss:0.056, val_acc:0.944]
Epoch [23/120    avg_loss:0.060, val_acc:0.928]
Epoch [24/120    avg_loss:0.082, val_acc:0.950]
Epoch [25/120    avg_loss:0.048, val_acc:0.961]
Epoch [26/120    avg_loss:0.055, val_acc:0.944]
Epoch [27/120    avg_loss:0.052, val_acc:0.961]
Epoch [28/120    avg_loss:0.026, val_acc:0.964]
Epoch [29/120    avg_loss:0.023, val_acc:0.956]
Epoch [30/120    avg_loss:0.025, val_acc:0.963]
Epoch [31/120    avg_loss:0.023, val_acc:0.969]
Epoch [32/120    avg_loss:0.024, val_acc:0.961]
Epoch [33/120    avg_loss:0.031, val_acc:0.965]
Epoch [34/120    avg_loss:0.026, val_acc:0.958]
Epoch [35/120    avg_loss:0.025, val_acc:0.961]
Epoch [36/120    avg_loss:0.032, val_acc:0.953]
Epoch [37/120    avg_loss:0.031, val_acc:0.953]
Epoch [38/120    avg_loss:0.024, val_acc:0.958]
Epoch [39/120    avg_loss:0.043, val_acc:0.955]
Epoch [40/120    avg_loss:0.043, val_acc:0.957]
Epoch [41/120    avg_loss:0.029, val_acc:0.965]
Epoch [42/120    avg_loss:0.045, val_acc:0.935]
Epoch [43/120    avg_loss:0.069, val_acc:0.947]
Epoch [44/120    avg_loss:0.027, val_acc:0.957]
Epoch [45/120    avg_loss:0.014, val_acc:0.961]
Epoch [46/120    avg_loss:0.014, val_acc:0.969]
Epoch [47/120    avg_loss:0.014, val_acc:0.969]
Epoch [48/120    avg_loss:0.013, val_acc:0.969]
Epoch [49/120    avg_loss:0.014, val_acc:0.971]
Epoch [50/120    avg_loss:0.012, val_acc:0.971]
Epoch [51/120    avg_loss:0.009, val_acc:0.971]
Epoch [52/120    avg_loss:0.016, val_acc:0.970]
Epoch [53/120    avg_loss:0.009, val_acc:0.969]
Epoch [54/120    avg_loss:0.010, val_acc:0.970]
Epoch [55/120    avg_loss:0.017, val_acc:0.972]
Epoch [56/120    avg_loss:0.009, val_acc:0.971]
Epoch [57/120    avg_loss:0.011, val_acc:0.972]
Epoch [58/120    avg_loss:0.007, val_acc:0.971]
Epoch [59/120    avg_loss:0.011, val_acc:0.973]
Epoch [60/120    avg_loss:0.008, val_acc:0.971]
Epoch [61/120    avg_loss:0.015, val_acc:0.972]
Epoch [62/120    avg_loss:0.008, val_acc:0.972]
Epoch [63/120    avg_loss:0.011, val_acc:0.973]
Epoch [64/120    avg_loss:0.010, val_acc:0.973]
Epoch [65/120    avg_loss:0.012, val_acc:0.972]
Epoch [66/120    avg_loss:0.010, val_acc:0.971]
Epoch [67/120    avg_loss:0.012, val_acc:0.970]
Epoch [68/120    avg_loss:0.008, val_acc:0.972]
Epoch [69/120    avg_loss:0.007, val_acc:0.971]
Epoch [70/120    avg_loss:0.009, val_acc:0.972]
Epoch [71/120    avg_loss:0.007, val_acc:0.973]
Epoch [72/120    avg_loss:0.006, val_acc:0.972]
Epoch [73/120    avg_loss:0.010, val_acc:0.972]
Epoch [74/120    avg_loss:0.008, val_acc:0.973]
Epoch [75/120    avg_loss:0.009, val_acc:0.973]
Epoch [76/120    avg_loss:0.010, val_acc:0.973]
Epoch [77/120    avg_loss:0.011, val_acc:0.971]
Epoch [78/120    avg_loss:0.008, val_acc:0.972]
Epoch [79/120    avg_loss:0.007, val_acc:0.973]
Epoch [80/120    avg_loss:0.007, val_acc:0.974]
Epoch [81/120    avg_loss:0.007, val_acc:0.974]
Epoch [82/120    avg_loss:0.007, val_acc:0.974]
Epoch [83/120    avg_loss:0.007, val_acc:0.974]
Epoch [84/120    avg_loss:0.004, val_acc:0.974]
Epoch [85/120    avg_loss:0.006, val_acc:0.974]
Epoch [86/120    avg_loss:0.007, val_acc:0.976]
Epoch [87/120    avg_loss:0.008, val_acc:0.976]
Epoch [88/120    avg_loss:0.007, val_acc:0.975]
Epoch [89/120    avg_loss:0.008, val_acc:0.974]
Epoch [90/120    avg_loss:0.011, val_acc:0.974]
Epoch [91/120    avg_loss:0.010, val_acc:0.973]
Epoch [92/120    avg_loss:0.007, val_acc:0.973]
Epoch [93/120    avg_loss:0.008, val_acc:0.974]
Epoch [94/120    avg_loss:0.006, val_acc:0.974]
Epoch [95/120    avg_loss:0.006, val_acc:0.975]
Epoch [96/120    avg_loss:0.006, val_acc:0.975]
Epoch [97/120    avg_loss:0.006, val_acc:0.975]
Epoch [98/120    avg_loss:0.007, val_acc:0.975]
Epoch [99/120    avg_loss:0.008, val_acc:0.975]
Epoch [100/120    avg_loss:0.010, val_acc:0.974]
Epoch [101/120    avg_loss:0.010, val_acc:0.974]
Epoch [102/120    avg_loss:0.006, val_acc:0.974]
Epoch [103/120    avg_loss:0.010, val_acc:0.974]
Epoch [104/120    avg_loss:0.008, val_acc:0.974]
Epoch [105/120    avg_loss:0.009, val_acc:0.974]
Epoch [106/120    avg_loss:0.007, val_acc:0.974]
Epoch [107/120    avg_loss:0.010, val_acc:0.974]
Epoch [108/120    avg_loss:0.006, val_acc:0.974]
Epoch [109/120    avg_loss:0.005, val_acc:0.974]
Epoch [110/120    avg_loss:0.007, val_acc:0.974]
Epoch [111/120    avg_loss:0.007, val_acc:0.974]
Epoch [112/120    avg_loss:0.006, val_acc:0.974]
Epoch [113/120    avg_loss:0.007, val_acc:0.974]
Epoch [114/120    avg_loss:0.008, val_acc:0.974]
Epoch [115/120    avg_loss:0.008, val_acc:0.974]
Epoch [116/120    avg_loss:0.008, val_acc:0.974]
Epoch [117/120    avg_loss:0.005, val_acc:0.974]
Epoch [118/120    avg_loss:0.005, val_acc:0.974]
Epoch [119/120    avg_loss:0.007, val_acc:0.974]
Epoch [120/120    avg_loss:0.012, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    3    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1258    4   10    1    1    0    0    0    1    7    3    0
     0    0    0]
 [   0    0    0  737    1    0    0    0    0    2    0    5    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    1    0    0    0    0  850   20    0    0
     2    0    0]
 [   0    0   14    0    0    0    0    0    0    1    4 2174   16    0
     0    0    1]
 [   0    0    2    3    1    0    0    0    0    0    0    0  523    0
     1    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1130    9    0]
 [   0    0    0    0    0    1    5    0    0    0    0    0    0    0
    72  269    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.73441734417344

F1 scores:
[       nan 0.96202532 0.98127925 0.98859826 0.97260274 0.99194476
 0.9939302  1.         1.         0.92307692 0.98265896 0.98437854
 0.96672828 1.         0.96211154 0.85942492 0.95238095]

Kappa:
0.9741580630529187
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fca4c617710>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.169, val_acc:0.601]
Epoch [2/120    avg_loss:1.563, val_acc:0.584]
Epoch [3/120    avg_loss:1.375, val_acc:0.722]
Epoch [4/120    avg_loss:1.093, val_acc:0.770]
Epoch [5/120    avg_loss:0.752, val_acc:0.815]
Epoch [6/120    avg_loss:0.687, val_acc:0.796]
Epoch [7/120    avg_loss:0.688, val_acc:0.714]
Epoch [8/120    avg_loss:0.570, val_acc:0.847]
Epoch [9/120    avg_loss:0.571, val_acc:0.861]
Epoch [10/120    avg_loss:0.369, val_acc:0.895]
Epoch [11/120    avg_loss:0.245, val_acc:0.874]
Epoch [12/120    avg_loss:0.227, val_acc:0.883]
Epoch [13/120    avg_loss:0.168, val_acc:0.918]
Epoch [14/120    avg_loss:0.159, val_acc:0.915]
Epoch [15/120    avg_loss:0.138, val_acc:0.939]
Epoch [16/120    avg_loss:0.071, val_acc:0.943]
Epoch [17/120    avg_loss:0.132, val_acc:0.939]
Epoch [18/120    avg_loss:0.119, val_acc:0.943]
Epoch [19/120    avg_loss:0.087, val_acc:0.953]
Epoch [20/120    avg_loss:0.061, val_acc:0.956]
Epoch [21/120    avg_loss:0.051, val_acc:0.960]
Epoch [22/120    avg_loss:0.059, val_acc:0.949]
Epoch [23/120    avg_loss:0.061, val_acc:0.971]
Epoch [24/120    avg_loss:0.036, val_acc:0.959]
Epoch [25/120    avg_loss:0.041, val_acc:0.970]
Epoch [26/120    avg_loss:0.044, val_acc:0.959]
Epoch [27/120    avg_loss:0.042, val_acc:0.960]
Epoch [28/120    avg_loss:0.037, val_acc:0.976]
Epoch [29/120    avg_loss:0.035, val_acc:0.957]
Epoch [30/120    avg_loss:0.044, val_acc:0.959]
Epoch [31/120    avg_loss:0.038, val_acc:0.956]
Epoch [32/120    avg_loss:0.064, val_acc:0.959]
Epoch [33/120    avg_loss:0.043, val_acc:0.974]
Epoch [34/120    avg_loss:0.023, val_acc:0.971]
Epoch [35/120    avg_loss:0.034, val_acc:0.975]
Epoch [36/120    avg_loss:0.029, val_acc:0.971]
Epoch [37/120    avg_loss:0.027, val_acc:0.960]
Epoch [38/120    avg_loss:0.029, val_acc:0.958]
Epoch [39/120    avg_loss:0.037, val_acc:0.929]
Epoch [40/120    avg_loss:0.089, val_acc:0.951]
Epoch [41/120    avg_loss:0.039, val_acc:0.972]
Epoch [42/120    avg_loss:0.018, val_acc:0.973]
Epoch [43/120    avg_loss:0.032, val_acc:0.973]
Epoch [44/120    avg_loss:0.017, val_acc:0.974]
Epoch [45/120    avg_loss:0.015, val_acc:0.976]
Epoch [46/120    avg_loss:0.018, val_acc:0.979]
Epoch [47/120    avg_loss:0.012, val_acc:0.976]
Epoch [48/120    avg_loss:0.013, val_acc:0.976]
Epoch [49/120    avg_loss:0.012, val_acc:0.976]
Epoch [50/120    avg_loss:0.012, val_acc:0.975]
Epoch [51/120    avg_loss:0.011, val_acc:0.976]
Epoch [52/120    avg_loss:0.012, val_acc:0.978]
Epoch [53/120    avg_loss:0.009, val_acc:0.978]
Epoch [54/120    avg_loss:0.010, val_acc:0.977]
Epoch [55/120    avg_loss:0.011, val_acc:0.977]
Epoch [56/120    avg_loss:0.012, val_acc:0.977]
Epoch [57/120    avg_loss:0.009, val_acc:0.977]
Epoch [58/120    avg_loss:0.011, val_acc:0.978]
Epoch [59/120    avg_loss:0.010, val_acc:0.977]
Epoch [60/120    avg_loss:0.011, val_acc:0.977]
Epoch [61/120    avg_loss:0.012, val_acc:0.977]
Epoch [62/120    avg_loss:0.011, val_acc:0.977]
Epoch [63/120    avg_loss:0.008, val_acc:0.978]
Epoch [64/120    avg_loss:0.011, val_acc:0.978]
Epoch [65/120    avg_loss:0.009, val_acc:0.978]
Epoch [66/120    avg_loss:0.013, val_acc:0.978]
Epoch [67/120    avg_loss:0.014, val_acc:0.978]
Epoch [68/120    avg_loss:0.009, val_acc:0.978]
Epoch [69/120    avg_loss:0.009, val_acc:0.977]
Epoch [70/120    avg_loss:0.009, val_acc:0.977]
Epoch [71/120    avg_loss:0.012, val_acc:0.977]
Epoch [72/120    avg_loss:0.010, val_acc:0.977]
Epoch [73/120    avg_loss:0.014, val_acc:0.977]
Epoch [74/120    avg_loss:0.014, val_acc:0.977]
Epoch [75/120    avg_loss:0.011, val_acc:0.978]
Epoch [76/120    avg_loss:0.012, val_acc:0.978]
Epoch [77/120    avg_loss:0.011, val_acc:0.978]
Epoch [78/120    avg_loss:0.010, val_acc:0.978]
Epoch [79/120    avg_loss:0.009, val_acc:0.978]
Epoch [80/120    avg_loss:0.013, val_acc:0.977]
Epoch [81/120    avg_loss:0.007, val_acc:0.978]
Epoch [82/120    avg_loss:0.010, val_acc:0.978]
Epoch [83/120    avg_loss:0.015, val_acc:0.977]
Epoch [84/120    avg_loss:0.010, val_acc:0.977]
Epoch [85/120    avg_loss:0.010, val_acc:0.977]
Epoch [86/120    avg_loss:0.011, val_acc:0.977]
Epoch [87/120    avg_loss:0.010, val_acc:0.977]
Epoch [88/120    avg_loss:0.015, val_acc:0.977]
Epoch [89/120    avg_loss:0.007, val_acc:0.977]
Epoch [90/120    avg_loss:0.012, val_acc:0.977]
Epoch [91/120    avg_loss:0.009, val_acc:0.977]
Epoch [92/120    avg_loss:0.016, val_acc:0.977]
Epoch [93/120    avg_loss:0.008, val_acc:0.977]
Epoch [94/120    avg_loss:0.010, val_acc:0.977]
Epoch [95/120    avg_loss:0.013, val_acc:0.977]
Epoch [96/120    avg_loss:0.012, val_acc:0.977]
Epoch [97/120    avg_loss:0.010, val_acc:0.977]
Epoch [98/120    avg_loss:0.010, val_acc:0.977]
Epoch [99/120    avg_loss:0.014, val_acc:0.977]
Epoch [100/120    avg_loss:0.009, val_acc:0.977]
Epoch [101/120    avg_loss:0.010, val_acc:0.977]
Epoch [102/120    avg_loss:0.011, val_acc:0.977]
Epoch [103/120    avg_loss:0.010, val_acc:0.977]
Epoch [104/120    avg_loss:0.010, val_acc:0.977]
Epoch [105/120    avg_loss:0.010, val_acc:0.977]
Epoch [106/120    avg_loss:0.011, val_acc:0.977]
Epoch [107/120    avg_loss:0.012, val_acc:0.977]
Epoch [108/120    avg_loss:0.019, val_acc:0.977]
Epoch [109/120    avg_loss:0.009, val_acc:0.977]
Epoch [110/120    avg_loss:0.012, val_acc:0.977]
Epoch [111/120    avg_loss:0.009, val_acc:0.977]
Epoch [112/120    avg_loss:0.012, val_acc:0.977]
Epoch [113/120    avg_loss:0.010, val_acc:0.977]
Epoch [114/120    avg_loss:0.012, val_acc:0.977]
Epoch [115/120    avg_loss:0.013, val_acc:0.977]
Epoch [116/120    avg_loss:0.013, val_acc:0.977]
Epoch [117/120    avg_loss:0.010, val_acc:0.977]
Epoch [118/120    avg_loss:0.008, val_acc:0.977]
Epoch [119/120    avg_loss:0.011, val_acc:0.977]
Epoch [120/120    avg_loss:0.009, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1272    0    6    1    1    0    0    0    2    2    1    0
     0    0    0]
 [   0    0    0  723    1    5    0    0    0    4    2   10    1    0
     0    1    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    2    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0   12    0    0    0    0    0    0    0  851    9    3    0
     0    0    0]
 [   0    0   16    0    0    0    0    0    0    1   27 2146   19    0
     0    0    1]
 [   0    0    0    6    1    0    0    0    0    0    0    3  522    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    0    0    0    0
  1114   21    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    55  284    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.45257452574526

F1 scores:
[       nan 1.         0.98413926 0.9796748  0.98156682 0.98517674
 0.98940998 1.         1.         0.87804878 0.96869664 0.97946143
 0.96309963 1.         0.96450216 0.86983155 0.95808383]

Kappa:
0.9709691905357961
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:36--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1b7d0616d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.272, val_acc:0.459]
Epoch [2/120    avg_loss:1.564, val_acc:0.531]
Epoch [3/120    avg_loss:1.317, val_acc:0.664]
Epoch [4/120    avg_loss:1.086, val_acc:0.752]
Epoch [5/120    avg_loss:0.908, val_acc:0.786]
Epoch [6/120    avg_loss:0.785, val_acc:0.786]
Epoch [7/120    avg_loss:0.547, val_acc:0.809]
Epoch [8/120    avg_loss:0.546, val_acc:0.812]
Epoch [9/120    avg_loss:0.408, val_acc:0.833]
Epoch [10/120    avg_loss:0.337, val_acc:0.865]
Epoch [11/120    avg_loss:0.390, val_acc:0.852]
Epoch [12/120    avg_loss:0.412, val_acc:0.890]
Epoch [13/120    avg_loss:0.271, val_acc:0.899]
Epoch [14/120    avg_loss:0.204, val_acc:0.896]
Epoch [15/120    avg_loss:0.163, val_acc:0.920]
Epoch [16/120    avg_loss:0.097, val_acc:0.926]
Epoch [17/120    avg_loss:0.095, val_acc:0.935]
Epoch [18/120    avg_loss:0.109, val_acc:0.928]
Epoch [19/120    avg_loss:0.104, val_acc:0.936]
Epoch [20/120    avg_loss:0.089, val_acc:0.941]
Epoch [21/120    avg_loss:0.056, val_acc:0.963]
Epoch [22/120    avg_loss:0.073, val_acc:0.955]
Epoch [23/120    avg_loss:0.057, val_acc:0.950]
Epoch [24/120    avg_loss:0.065, val_acc:0.968]
Epoch [25/120    avg_loss:0.059, val_acc:0.953]
Epoch [26/120    avg_loss:0.030, val_acc:0.966]
Epoch [27/120    avg_loss:0.186, val_acc:0.881]
Epoch [28/120    avg_loss:0.219, val_acc:0.929]
Epoch [29/120    avg_loss:0.071, val_acc:0.961]
Epoch [30/120    avg_loss:0.038, val_acc:0.957]
Epoch [31/120    avg_loss:0.036, val_acc:0.969]
Epoch [32/120    avg_loss:0.021, val_acc:0.967]
Epoch [33/120    avg_loss:0.020, val_acc:0.970]
Epoch [34/120    avg_loss:0.018, val_acc:0.966]
Epoch [35/120    avg_loss:0.023, val_acc:0.968]
Epoch [36/120    avg_loss:0.030, val_acc:0.971]
Epoch [37/120    avg_loss:0.048, val_acc:0.954]
Epoch [38/120    avg_loss:0.048, val_acc:0.961]
Epoch [39/120    avg_loss:0.023, val_acc:0.967]
Epoch [40/120    avg_loss:0.014, val_acc:0.970]
Epoch [41/120    avg_loss:0.016, val_acc:0.974]
Epoch [42/120    avg_loss:0.017, val_acc:0.970]
Epoch [43/120    avg_loss:0.015, val_acc:0.976]
Epoch [44/120    avg_loss:0.012, val_acc:0.979]
Epoch [45/120    avg_loss:0.019, val_acc:0.978]
Epoch [46/120    avg_loss:0.018, val_acc:0.968]
Epoch [47/120    avg_loss:0.011, val_acc:0.972]
Epoch [48/120    avg_loss:0.010, val_acc:0.976]
Epoch [49/120    avg_loss:0.007, val_acc:0.976]
Epoch [50/120    avg_loss:0.008, val_acc:0.979]
Epoch [51/120    avg_loss:0.007, val_acc:0.978]
Epoch [52/120    avg_loss:0.013, val_acc:0.972]
Epoch [53/120    avg_loss:0.025, val_acc:0.970]
Epoch [54/120    avg_loss:0.013, val_acc:0.977]
Epoch [55/120    avg_loss:0.007, val_acc:0.978]
Epoch [56/120    avg_loss:0.015, val_acc:0.975]
Epoch [57/120    avg_loss:0.010, val_acc:0.975]
Epoch [58/120    avg_loss:0.011, val_acc:0.976]
Epoch [59/120    avg_loss:0.026, val_acc:0.959]
Epoch [60/120    avg_loss:0.031, val_acc:0.963]
Epoch [61/120    avg_loss:0.020, val_acc:0.982]
Epoch [62/120    avg_loss:0.014, val_acc:0.979]
Epoch [63/120    avg_loss:0.006, val_acc:0.975]
Epoch [64/120    avg_loss:0.006, val_acc:0.975]
Epoch [65/120    avg_loss:0.006, val_acc:0.978]
Epoch [66/120    avg_loss:0.007, val_acc:0.975]
Epoch [67/120    avg_loss:0.006, val_acc:0.981]
Epoch [68/120    avg_loss:0.006, val_acc:0.981]
Epoch [69/120    avg_loss:0.012, val_acc:0.972]
Epoch [70/120    avg_loss:0.027, val_acc:0.972]
Epoch [71/120    avg_loss:0.012, val_acc:0.973]
Epoch [72/120    avg_loss:0.005, val_acc:0.976]
Epoch [73/120    avg_loss:0.008, val_acc:0.980]
Epoch [74/120    avg_loss:0.003, val_acc:0.982]
Epoch [75/120    avg_loss:0.004, val_acc:0.981]
Epoch [76/120    avg_loss:0.004, val_acc:0.980]
Epoch [77/120    avg_loss:0.008, val_acc:0.976]
Epoch [78/120    avg_loss:0.007, val_acc:0.980]
Epoch [79/120    avg_loss:0.005, val_acc:0.985]
Epoch [80/120    avg_loss:0.004, val_acc:0.981]
Epoch [81/120    avg_loss:0.005, val_acc:0.979]
Epoch [82/120    avg_loss:0.004, val_acc:0.983]
Epoch [83/120    avg_loss:0.004, val_acc:0.980]
Epoch [84/120    avg_loss:0.004, val_acc:0.985]
Epoch [85/120    avg_loss:0.003, val_acc:0.983]
Epoch [86/120    avg_loss:0.003, val_acc:0.985]
Epoch [87/120    avg_loss:0.003, val_acc:0.982]
Epoch [88/120    avg_loss:0.003, val_acc:0.983]
Epoch [89/120    avg_loss:0.005, val_acc:0.980]
Epoch [90/120    avg_loss:0.004, val_acc:0.983]
Epoch [91/120    avg_loss:0.004, val_acc:0.984]
Epoch [92/120    avg_loss:0.003, val_acc:0.986]
Epoch [93/120    avg_loss:0.004, val_acc:0.988]
Epoch [94/120    avg_loss:0.008, val_acc:0.973]
Epoch [95/120    avg_loss:0.005, val_acc:0.985]
Epoch [96/120    avg_loss:0.005, val_acc:0.982]
Epoch [97/120    avg_loss:0.003, val_acc:0.984]
Epoch [98/120    avg_loss:0.002, val_acc:0.982]
Epoch [99/120    avg_loss:0.003, val_acc:0.981]
Epoch [100/120    avg_loss:0.006, val_acc:0.985]
Epoch [101/120    avg_loss:0.008, val_acc:0.982]
Epoch [102/120    avg_loss:0.005, val_acc:0.976]
Epoch [103/120    avg_loss:0.004, val_acc:0.985]
Epoch [104/120    avg_loss:0.003, val_acc:0.980]
Epoch [105/120    avg_loss:0.002, val_acc:0.984]
Epoch [106/120    avg_loss:0.002, val_acc:0.982]
Epoch [107/120    avg_loss:0.002, val_acc:0.982]
Epoch [108/120    avg_loss:0.002, val_acc:0.982]
Epoch [109/120    avg_loss:0.003, val_acc:0.982]
Epoch [110/120    avg_loss:0.002, val_acc:0.982]
Epoch [111/120    avg_loss:0.002, val_acc:0.982]
Epoch [112/120    avg_loss:0.003, val_acc:0.982]
Epoch [113/120    avg_loss:0.002, val_acc:0.982]
Epoch [114/120    avg_loss:0.002, val_acc:0.982]
Epoch [115/120    avg_loss:0.003, val_acc:0.981]
Epoch [116/120    avg_loss:0.004, val_acc:0.982]
Epoch [117/120    avg_loss:0.002, val_acc:0.982]
Epoch [118/120    avg_loss:0.002, val_acc:0.982]
Epoch [119/120    avg_loss:0.002, val_acc:0.982]
Epoch [120/120    avg_loss:0.002, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    2 1266    4    0    0    0    0    0    0    2    5    6    0
     0    0    0]
 [   0    0    0  731    0    5    0    0    0    2    0    0    9    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    0    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    2    0    0    0    0  843   20    1    0
     0    1    0]
 [   0    0   12    4    0    0    0    0    0    0    5 2176   11    1
     0    0    1]
 [   0    0    0    6    0    0    0    0    0    0    0    0  526    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1124   15    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    47  285    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
97.86449864498645

F1 scores:
[       nan 0.95121951 0.9844479  0.97923644 1.         0.99086758
 0.98642534 1.         1.         0.91891892 0.97682503 0.98595378
 0.96425298 0.99730458 0.97273907 0.87962963 0.95808383]

Kappa:
0.9756494038326344
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:38--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa73c7625f8>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.251, val_acc:0.411]
Epoch [2/120    avg_loss:1.721, val_acc:0.495]
Epoch [3/120    avg_loss:1.355, val_acc:0.630]
Epoch [4/120    avg_loss:1.164, val_acc:0.644]
Epoch [5/120    avg_loss:1.124, val_acc:0.751]
Epoch [6/120    avg_loss:0.994, val_acc:0.666]
Epoch [7/120    avg_loss:0.969, val_acc:0.816]
Epoch [8/120    avg_loss:0.617, val_acc:0.776]
Epoch [9/120    avg_loss:0.464, val_acc:0.876]
Epoch [10/120    avg_loss:0.319, val_acc:0.851]
Epoch [11/120    avg_loss:0.381, val_acc:0.860]
Epoch [12/120    avg_loss:0.332, val_acc:0.905]
Epoch [13/120    avg_loss:0.239, val_acc:0.894]
Epoch [14/120    avg_loss:0.337, val_acc:0.890]
Epoch [15/120    avg_loss:0.213, val_acc:0.927]
Epoch [16/120    avg_loss:0.147, val_acc:0.934]
Epoch [17/120    avg_loss:0.108, val_acc:0.942]
Epoch [18/120    avg_loss:0.078, val_acc:0.957]
Epoch [19/120    avg_loss:0.087, val_acc:0.954]
Epoch [20/120    avg_loss:0.053, val_acc:0.954]
Epoch [21/120    avg_loss:0.047, val_acc:0.963]
Epoch [22/120    avg_loss:0.059, val_acc:0.950]
Epoch [23/120    avg_loss:0.045, val_acc:0.963]
Epoch [24/120    avg_loss:0.057, val_acc:0.951]
Epoch [25/120    avg_loss:0.051, val_acc:0.965]
Epoch [26/120    avg_loss:0.064, val_acc:0.939]
Epoch [27/120    avg_loss:0.047, val_acc:0.965]
Epoch [28/120    avg_loss:0.037, val_acc:0.934]
Epoch [29/120    avg_loss:0.046, val_acc:0.971]
Epoch [30/120    avg_loss:0.055, val_acc:0.946]
Epoch [31/120    avg_loss:0.053, val_acc:0.966]
Epoch [32/120    avg_loss:0.038, val_acc:0.969]
Epoch [33/120    avg_loss:0.024, val_acc:0.963]
Epoch [34/120    avg_loss:0.026, val_acc:0.967]
Epoch [35/120    avg_loss:0.020, val_acc:0.974]
Epoch [36/120    avg_loss:0.019, val_acc:0.968]
Epoch [37/120    avg_loss:0.023, val_acc:0.976]
Epoch [38/120    avg_loss:0.025, val_acc:0.959]
Epoch [39/120    avg_loss:0.031, val_acc:0.969]
Epoch [40/120    avg_loss:0.026, val_acc:0.974]
Epoch [41/120    avg_loss:0.029, val_acc:0.965]
Epoch [42/120    avg_loss:0.024, val_acc:0.973]
Epoch [43/120    avg_loss:0.037, val_acc:0.973]
Epoch [44/120    avg_loss:0.030, val_acc:0.966]
Epoch [45/120    avg_loss:0.011, val_acc:0.976]
Epoch [46/120    avg_loss:0.018, val_acc:0.971]
Epoch [47/120    avg_loss:0.017, val_acc:0.978]
Epoch [48/120    avg_loss:0.016, val_acc:0.976]
Epoch [49/120    avg_loss:0.012, val_acc:0.972]
Epoch [50/120    avg_loss:0.012, val_acc:0.952]
Epoch [51/120    avg_loss:0.020, val_acc:0.976]
Epoch [52/120    avg_loss:0.009, val_acc:0.974]
Epoch [53/120    avg_loss:0.008, val_acc:0.979]
Epoch [54/120    avg_loss:0.010, val_acc:0.972]
Epoch [55/120    avg_loss:0.008, val_acc:0.978]
Epoch [56/120    avg_loss:0.015, val_acc:0.963]
Epoch [57/120    avg_loss:0.030, val_acc:0.969]
Epoch [58/120    avg_loss:0.012, val_acc:0.972]
Epoch [59/120    avg_loss:0.013, val_acc:0.978]
Epoch [60/120    avg_loss:0.037, val_acc:0.967]
Epoch [61/120    avg_loss:0.029, val_acc:0.960]
Epoch [62/120    avg_loss:0.126, val_acc:0.959]
Epoch [63/120    avg_loss:0.091, val_acc:0.955]
Epoch [64/120    avg_loss:0.060, val_acc:0.956]
Epoch [65/120    avg_loss:0.054, val_acc:0.973]
Epoch [66/120    avg_loss:0.026, val_acc:0.973]
Epoch [67/120    avg_loss:0.019, val_acc:0.976]
Epoch [68/120    avg_loss:0.013, val_acc:0.980]
Epoch [69/120    avg_loss:0.013, val_acc:0.982]
Epoch [70/120    avg_loss:0.009, val_acc:0.982]
Epoch [71/120    avg_loss:0.009, val_acc:0.983]
Epoch [72/120    avg_loss:0.009, val_acc:0.983]
Epoch [73/120    avg_loss:0.011, val_acc:0.981]
Epoch [74/120    avg_loss:0.007, val_acc:0.981]
Epoch [75/120    avg_loss:0.013, val_acc:0.977]
Epoch [76/120    avg_loss:0.009, val_acc:0.976]
Epoch [77/120    avg_loss:0.009, val_acc:0.977]
Epoch [78/120    avg_loss:0.011, val_acc:0.977]
Epoch [79/120    avg_loss:0.010, val_acc:0.979]
Epoch [80/120    avg_loss:0.010, val_acc:0.976]
Epoch [81/120    avg_loss:0.010, val_acc:0.976]
Epoch [82/120    avg_loss:0.010, val_acc:0.978]
Epoch [83/120    avg_loss:0.008, val_acc:0.978]
Epoch [84/120    avg_loss:0.010, val_acc:0.979]
Epoch [85/120    avg_loss:0.011, val_acc:0.977]
Epoch [86/120    avg_loss:0.013, val_acc:0.977]
Epoch [87/120    avg_loss:0.009, val_acc:0.977]
Epoch [88/120    avg_loss:0.007, val_acc:0.977]
Epoch [89/120    avg_loss:0.008, val_acc:0.977]
Epoch [90/120    avg_loss:0.007, val_acc:0.977]
Epoch [91/120    avg_loss:0.008, val_acc:0.977]
Epoch [92/120    avg_loss:0.008, val_acc:0.978]
Epoch [93/120    avg_loss:0.010, val_acc:0.978]
Epoch [94/120    avg_loss:0.015, val_acc:0.978]
Epoch [95/120    avg_loss:0.006, val_acc:0.978]
Epoch [96/120    avg_loss:0.010, val_acc:0.978]
Epoch [97/120    avg_loss:0.008, val_acc:0.978]
Epoch [98/120    avg_loss:0.007, val_acc:0.978]
Epoch [99/120    avg_loss:0.007, val_acc:0.978]
Epoch [100/120    avg_loss:0.007, val_acc:0.978]
Epoch [101/120    avg_loss:0.006, val_acc:0.978]
Epoch [102/120    avg_loss:0.008, val_acc:0.978]
Epoch [103/120    avg_loss:0.008, val_acc:0.978]
Epoch [104/120    avg_loss:0.007, val_acc:0.978]
Epoch [105/120    avg_loss:0.006, val_acc:0.978]
Epoch [106/120    avg_loss:0.008, val_acc:0.978]
Epoch [107/120    avg_loss:0.006, val_acc:0.978]
Epoch [108/120    avg_loss:0.008, val_acc:0.978]
Epoch [109/120    avg_loss:0.007, val_acc:0.978]
Epoch [110/120    avg_loss:0.006, val_acc:0.978]
Epoch [111/120    avg_loss:0.007, val_acc:0.978]
Epoch [112/120    avg_loss:0.009, val_acc:0.978]
Epoch [113/120    avg_loss:0.006, val_acc:0.978]
Epoch [114/120    avg_loss:0.009, val_acc:0.978]
Epoch [115/120    avg_loss:0.011, val_acc:0.978]
Epoch [116/120    avg_loss:0.006, val_acc:0.978]
Epoch [117/120    avg_loss:0.007, val_acc:0.978]
Epoch [118/120    avg_loss:0.008, val_acc:0.978]
Epoch [119/120    avg_loss:0.008, val_acc:0.978]
Epoch [120/120    avg_loss:0.014, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1267    5    0    0    0    0    0    0    3    6    4    0
     0    0    0]
 [   0    0    0  739    0    0    0    0    0    2    0    4    2    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    1    0    1    0    1
     3    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  858   16    0    0
     1    0    0]
 [   0    0    7    0    0    0    0    0    0    1    8 2185    8    1
     0    0    0]
 [   0    0    0    2    0    0    0    0    0    0    0    0  527    0
     0    2    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1128   11    0]
 [   0    0    0    0    0    0   14    0    0    0    0    0    0    0
    37  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.40650406504065

F1 scores:
[       nan 0.98765432 0.99023056 0.98995311 1.         0.99305556
 0.98793363 1.         1.         0.9        0.98338109 0.98779385
 0.9795539  0.99462366 0.97746967 0.90243902 0.97647059]

Kappa:
0.9818276643874243
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:39--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:9
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f204df3a6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 268833==>0.27M
----------Training process----------
Epoch [1/120    avg_loss:2.368, val_acc:0.365]
Epoch [2/120    avg_loss:1.719, val_acc:0.579]
Epoch [3/120    avg_loss:1.432, val_acc:0.675]
Epoch [4/120    avg_loss:1.039, val_acc:0.725]
Epoch [5/120    avg_loss:0.974, val_acc:0.749]
Epoch [6/120    avg_loss:0.880, val_acc:0.678]
Epoch [7/120    avg_loss:0.700, val_acc:0.791]
Epoch [8/120    avg_loss:0.448, val_acc:0.845]
Epoch [9/120    avg_loss:0.450, val_acc:0.784]
Epoch [10/120    avg_loss:0.600, val_acc:0.784]
Epoch [11/120    avg_loss:0.383, val_acc:0.884]
Epoch [12/120    avg_loss:0.251, val_acc:0.893]
Epoch [13/120    avg_loss:0.182, val_acc:0.892]
Epoch [14/120    avg_loss:0.221, val_acc:0.905]
Epoch [15/120    avg_loss:0.206, val_acc:0.870]
Epoch [16/120    avg_loss:0.174, val_acc:0.893]
Epoch [17/120    avg_loss:0.213, val_acc:0.918]
Epoch [18/120    avg_loss:0.364, val_acc:0.902]
Epoch [19/120    avg_loss:0.176, val_acc:0.912]
Epoch [20/120    avg_loss:0.110, val_acc:0.933]
Epoch [21/120    avg_loss:0.080, val_acc:0.935]
Epoch [22/120    avg_loss:0.065, val_acc:0.935]
Epoch [23/120    avg_loss:0.080, val_acc:0.951]
Epoch [24/120    avg_loss:0.075, val_acc:0.949]
Epoch [25/120    avg_loss:0.061, val_acc:0.940]
Epoch [26/120    avg_loss:0.072, val_acc:0.949]
Epoch [27/120    avg_loss:0.057, val_acc:0.965]
Epoch [28/120    avg_loss:0.043, val_acc:0.969]
Epoch [29/120    avg_loss:0.179, val_acc:0.950]
Epoch [30/120    avg_loss:0.048, val_acc:0.956]
Epoch [31/120    avg_loss:0.037, val_acc:0.972]
Epoch [32/120    avg_loss:0.033, val_acc:0.975]
Epoch [33/120    avg_loss:0.033, val_acc:0.966]
Epoch [34/120    avg_loss:0.039, val_acc:0.977]
Epoch [35/120    avg_loss:0.029, val_acc:0.972]
Epoch [36/120    avg_loss:0.061, val_acc:0.928]
Epoch [37/120    avg_loss:0.039, val_acc:0.958]
Epoch [38/120    avg_loss:0.023, val_acc:0.963]
Epoch [39/120    avg_loss:0.015, val_acc:0.976]
Epoch [40/120    avg_loss:0.028, val_acc:0.957]
Epoch [41/120    avg_loss:0.026, val_acc:0.963]
Epoch [42/120    avg_loss:0.023, val_acc:0.965]
Epoch [43/120    avg_loss:0.030, val_acc:0.960]
Epoch [44/120    avg_loss:0.028, val_acc:0.964]
Epoch [45/120    avg_loss:0.017, val_acc:0.964]
Epoch [46/120    avg_loss:0.024, val_acc:0.973]
Epoch [47/120    avg_loss:0.028, val_acc:0.971]
Epoch [48/120    avg_loss:0.018, val_acc:0.971]
Epoch [49/120    avg_loss:0.010, val_acc:0.971]
Epoch [50/120    avg_loss:0.012, val_acc:0.972]
Epoch [51/120    avg_loss:0.012, val_acc:0.974]
Epoch [52/120    avg_loss:0.011, val_acc:0.972]
Epoch [53/120    avg_loss:0.007, val_acc:0.974]
Epoch [54/120    avg_loss:0.008, val_acc:0.973]
Epoch [55/120    avg_loss:0.012, val_acc:0.972]
Epoch [56/120    avg_loss:0.012, val_acc:0.971]
Epoch [57/120    avg_loss:0.010, val_acc:0.972]
Epoch [58/120    avg_loss:0.010, val_acc:0.972]
Epoch [59/120    avg_loss:0.009, val_acc:0.971]
Epoch [60/120    avg_loss:0.010, val_acc:0.974]
Epoch [61/120    avg_loss:0.010, val_acc:0.974]
Epoch [62/120    avg_loss:0.009, val_acc:0.974]
Epoch [63/120    avg_loss:0.008, val_acc:0.974]
Epoch [64/120    avg_loss:0.008, val_acc:0.974]
Epoch [65/120    avg_loss:0.012, val_acc:0.974]
Epoch [66/120    avg_loss:0.010, val_acc:0.974]
Epoch [67/120    avg_loss:0.007, val_acc:0.974]
Epoch [68/120    avg_loss:0.008, val_acc:0.974]
Epoch [69/120    avg_loss:0.013, val_acc:0.974]
Epoch [70/120    avg_loss:0.009, val_acc:0.974]
Epoch [71/120    avg_loss:0.010, val_acc:0.974]
Epoch [72/120    avg_loss:0.010, val_acc:0.974]
Epoch [73/120    avg_loss:0.008, val_acc:0.975]
Epoch [74/120    avg_loss:0.012, val_acc:0.975]
Epoch [75/120    avg_loss:0.008, val_acc:0.975]
Epoch [76/120    avg_loss:0.015, val_acc:0.975]
Epoch [77/120    avg_loss:0.009, val_acc:0.975]
Epoch [78/120    avg_loss:0.008, val_acc:0.975]
Epoch [79/120    avg_loss:0.007, val_acc:0.975]
Epoch [80/120    avg_loss:0.008, val_acc:0.975]
Epoch [81/120    avg_loss:0.007, val_acc:0.975]
Epoch [82/120    avg_loss:0.009, val_acc:0.975]
Epoch [83/120    avg_loss:0.009, val_acc:0.975]
Epoch [84/120    avg_loss:0.013, val_acc:0.975]
Epoch [85/120    avg_loss:0.007, val_acc:0.975]
Epoch [86/120    avg_loss:0.008, val_acc:0.975]
Epoch [87/120    avg_loss:0.009, val_acc:0.975]
Epoch [88/120    avg_loss:0.010, val_acc:0.975]
Epoch [89/120    avg_loss:0.012, val_acc:0.975]
Epoch [90/120    avg_loss:0.008, val_acc:0.975]
Epoch [91/120    avg_loss:0.009, val_acc:0.975]
Epoch [92/120    avg_loss:0.006, val_acc:0.975]
Epoch [93/120    avg_loss:0.009, val_acc:0.975]
Epoch [94/120    avg_loss:0.006, val_acc:0.975]
Epoch [95/120    avg_loss:0.008, val_acc:0.975]
Epoch [96/120    avg_loss:0.009, val_acc:0.975]
Epoch [97/120    avg_loss:0.008, val_acc:0.975]
Epoch [98/120    avg_loss:0.007, val_acc:0.975]
Epoch [99/120    avg_loss:0.010, val_acc:0.975]
Epoch [100/120    avg_loss:0.014, val_acc:0.975]
Epoch [101/120    avg_loss:0.008, val_acc:0.975]
Epoch [102/120    avg_loss:0.008, val_acc:0.975]
Epoch [103/120    avg_loss:0.010, val_acc:0.975]
Epoch [104/120    avg_loss:0.007, val_acc:0.975]
Epoch [105/120    avg_loss:0.011, val_acc:0.975]
Epoch [106/120    avg_loss:0.009, val_acc:0.975]
Epoch [107/120    avg_loss:0.012, val_acc:0.975]
Epoch [108/120    avg_loss:0.008, val_acc:0.975]
Epoch [109/120    avg_loss:0.006, val_acc:0.975]
Epoch [110/120    avg_loss:0.011, val_acc:0.975]
Epoch [111/120    avg_loss:0.009, val_acc:0.975]
Epoch [112/120    avg_loss:0.008, val_acc:0.975]
Epoch [113/120    avg_loss:0.009, val_acc:0.975]
Epoch [114/120    avg_loss:0.007, val_acc:0.975]
Epoch [115/120    avg_loss:0.008, val_acc:0.975]
Epoch [116/120    avg_loss:0.008, val_acc:0.975]
Epoch [117/120    avg_loss:0.008, val_acc:0.975]
Epoch [118/120    avg_loss:0.006, val_acc:0.975]
Epoch [119/120    avg_loss:0.008, val_acc:0.975]
Epoch [120/120    avg_loss:0.006, val_acc:0.975]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    3    4    0    1    0    0    0    2    6    2    0
     0    0    0]
 [   0    0    0  740    0    0    1    0    0    2    0    2    2    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    2    0    0    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    3    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    8    0    0    1    0    0    0    0  839   25    0    0
     2    0    0]
 [   0    0   10    0    0    0    1    0    0    0    3 2173   23    0
     0    0    0]
 [   0    0    0    6    0    1    0    0    0    0    1    0  524    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
  1127   12    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    45  296    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.98373983739837

F1 scores:
[       nan 0.98765432 0.98560871 0.98798398 0.98598131 0.99076212
 0.99014405 0.96153846 0.99883586 0.94736842 0.9755814  0.98348043
 0.96412144 1.         0.97238999 0.90381679 0.98224852]

Kappa:
0.9770067486268171
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fcfd7b4d6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.408, val_acc:0.452]
Epoch [2/120    avg_loss:1.829, val_acc:0.530]
Epoch [3/120    avg_loss:1.394, val_acc:0.635]
Epoch [4/120    avg_loss:1.204, val_acc:0.686]
Epoch [5/120    avg_loss:0.870, val_acc:0.744]
Epoch [6/120    avg_loss:0.677, val_acc:0.812]
Epoch [7/120    avg_loss:0.709, val_acc:0.774]
Epoch [8/120    avg_loss:0.510, val_acc:0.859]
Epoch [9/120    avg_loss:0.396, val_acc:0.820]
Epoch [10/120    avg_loss:0.415, val_acc:0.829]
Epoch [11/120    avg_loss:0.371, val_acc:0.858]
Epoch [12/120    avg_loss:0.208, val_acc:0.909]
Epoch [13/120    avg_loss:0.172, val_acc:0.909]
Epoch [14/120    avg_loss:0.160, val_acc:0.928]
Epoch [15/120    avg_loss:0.167, val_acc:0.912]
Epoch [16/120    avg_loss:0.119, val_acc:0.932]
Epoch [17/120    avg_loss:0.117, val_acc:0.943]
Epoch [18/120    avg_loss:0.086, val_acc:0.933]
Epoch [19/120    avg_loss:0.111, val_acc:0.953]
Epoch [20/120    avg_loss:0.094, val_acc:0.918]
Epoch [21/120    avg_loss:0.098, val_acc:0.956]
Epoch [22/120    avg_loss:0.078, val_acc:0.956]
Epoch [23/120    avg_loss:0.075, val_acc:0.948]
Epoch [24/120    avg_loss:0.070, val_acc:0.952]
Epoch [25/120    avg_loss:0.068, val_acc:0.947]
Epoch [26/120    avg_loss:0.061, val_acc:0.946]
Epoch [27/120    avg_loss:0.053, val_acc:0.964]
Epoch [28/120    avg_loss:0.049, val_acc:0.970]
Epoch [29/120    avg_loss:0.095, val_acc:0.940]
Epoch [30/120    avg_loss:0.078, val_acc:0.957]
Epoch [31/120    avg_loss:0.053, val_acc:0.974]
Epoch [32/120    avg_loss:0.036, val_acc:0.978]
Epoch [33/120    avg_loss:0.036, val_acc:0.958]
Epoch [34/120    avg_loss:0.034, val_acc:0.964]
Epoch [35/120    avg_loss:0.042, val_acc:0.970]
Epoch [36/120    avg_loss:0.026, val_acc:0.972]
Epoch [37/120    avg_loss:0.066, val_acc:0.953]
Epoch [38/120    avg_loss:0.056, val_acc:0.970]
Epoch [39/120    avg_loss:0.034, val_acc:0.968]
Epoch [40/120    avg_loss:0.023, val_acc:0.973]
Epoch [41/120    avg_loss:0.026, val_acc:0.978]
Epoch [42/120    avg_loss:0.021, val_acc:0.978]
Epoch [43/120    avg_loss:0.013, val_acc:0.980]
Epoch [44/120    avg_loss:0.020, val_acc:0.975]
Epoch [45/120    avg_loss:0.024, val_acc:0.985]
Epoch [46/120    avg_loss:0.022, val_acc:0.969]
Epoch [47/120    avg_loss:0.037, val_acc:0.964]
Epoch [48/120    avg_loss:0.019, val_acc:0.978]
Epoch [49/120    avg_loss:0.016, val_acc:0.981]
Epoch [50/120    avg_loss:0.014, val_acc:0.978]
Epoch [51/120    avg_loss:0.015, val_acc:0.975]
Epoch [52/120    avg_loss:0.019, val_acc:0.981]
Epoch [53/120    avg_loss:0.018, val_acc:0.977]
Epoch [54/120    avg_loss:0.013, val_acc:0.982]
Epoch [55/120    avg_loss:0.012, val_acc:0.985]
Epoch [56/120    avg_loss:0.008, val_acc:0.983]
Epoch [57/120    avg_loss:0.007, val_acc:0.986]
Epoch [58/120    avg_loss:0.011, val_acc:0.983]
Epoch [59/120    avg_loss:0.008, val_acc:0.981]
Epoch [60/120    avg_loss:0.008, val_acc:0.985]
Epoch [61/120    avg_loss:0.005, val_acc:0.987]
Epoch [62/120    avg_loss:0.003, val_acc:0.988]
Epoch [63/120    avg_loss:0.007, val_acc:0.985]
Epoch [64/120    avg_loss:0.005, val_acc:0.984]
Epoch [65/120    avg_loss:0.005, val_acc:0.986]
Epoch [66/120    avg_loss:0.005, val_acc:0.990]
Epoch [67/120    avg_loss:0.008, val_acc:0.987]
Epoch [68/120    avg_loss:0.005, val_acc:0.984]
Epoch [69/120    avg_loss:0.004, val_acc:0.986]
Epoch [70/120    avg_loss:0.011, val_acc:0.985]
Epoch [71/120    avg_loss:0.038, val_acc:0.967]
Epoch [72/120    avg_loss:0.028, val_acc:0.967]
Epoch [73/120    avg_loss:0.049, val_acc:0.974]
Epoch [74/120    avg_loss:0.019, val_acc:0.980]
Epoch [75/120    avg_loss:0.011, val_acc:0.985]
Epoch [76/120    avg_loss:0.008, val_acc:0.987]
Epoch [77/120    avg_loss:0.008, val_acc:0.976]
Epoch [78/120    avg_loss:0.006, val_acc:0.986]
Epoch [79/120    avg_loss:0.010, val_acc:0.983]
Epoch [80/120    avg_loss:0.006, val_acc:0.981]
Epoch [81/120    avg_loss:0.004, val_acc:0.984]
Epoch [82/120    avg_loss:0.010, val_acc:0.986]
Epoch [83/120    avg_loss:0.005, val_acc:0.987]
Epoch [84/120    avg_loss:0.008, val_acc:0.988]
Epoch [85/120    avg_loss:0.005, val_acc:0.988]
Epoch [86/120    avg_loss:0.005, val_acc:0.988]
Epoch [87/120    avg_loss:0.004, val_acc:0.987]
Epoch [88/120    avg_loss:0.005, val_acc:0.988]
Epoch [89/120    avg_loss:0.005, val_acc:0.988]
Epoch [90/120    avg_loss:0.004, val_acc:0.988]
Epoch [91/120    avg_loss:0.004, val_acc:0.987]
Epoch [92/120    avg_loss:0.005, val_acc:0.987]
Epoch [93/120    avg_loss:0.004, val_acc:0.987]
Epoch [94/120    avg_loss:0.005, val_acc:0.987]
Epoch [95/120    avg_loss:0.004, val_acc:0.987]
Epoch [96/120    avg_loss:0.004, val_acc:0.989]
Epoch [97/120    avg_loss:0.005, val_acc:0.988]
Epoch [98/120    avg_loss:0.004, val_acc:0.988]
Epoch [99/120    avg_loss:0.003, val_acc:0.988]
Epoch [100/120    avg_loss:0.005, val_acc:0.988]
Epoch [101/120    avg_loss:0.007, val_acc:0.989]
Epoch [102/120    avg_loss:0.003, val_acc:0.989]
Epoch [103/120    avg_loss:0.004, val_acc:0.989]
Epoch [104/120    avg_loss:0.004, val_acc:0.989]
Epoch [105/120    avg_loss:0.005, val_acc:0.989]
Epoch [106/120    avg_loss:0.004, val_acc:0.989]
Epoch [107/120    avg_loss:0.005, val_acc:0.989]
Epoch [108/120    avg_loss:0.003, val_acc:0.989]
Epoch [109/120    avg_loss:0.004, val_acc:0.989]
Epoch [110/120    avg_loss:0.003, val_acc:0.989]
Epoch [111/120    avg_loss:0.004, val_acc:0.989]
Epoch [112/120    avg_loss:0.004, val_acc:0.989]
Epoch [113/120    avg_loss:0.004, val_acc:0.989]
Epoch [114/120    avg_loss:0.004, val_acc:0.989]
Epoch [115/120    avg_loss:0.003, val_acc:0.989]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.006, val_acc:0.989]
Epoch [118/120    avg_loss:0.007, val_acc:0.989]
Epoch [119/120    avg_loss:0.003, val_acc:0.989]
Epoch [120/120    avg_loss:0.005, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1278    0    0    1    0    0    0    0    4    2    0    0
     0    0    0]
 [   0    0    0  717    0   14    0    0    0    7    1    2    2    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  434    0    0    0    1    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    2    0    0    0    0  857   10    0    0
     0    1    0]
 [   0    0    6    0    0    0    3    0    0    1    7 2180   12    1
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    3    0  527    0
     0    1    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    1    0    0    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    28  311    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
98.5040650406504

F1 scores:
[       nan 0.98765432 0.99262136 0.9795082  1.         0.97747748
 0.99095023 1.         0.99767442 0.8        0.98111048 0.99000908
 0.97502313 0.98666667 0.98697917 0.94242424 0.96341463]

Kappa:
0.982947904586062
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb4ce01c668>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.363, val_acc:0.509]
Epoch [2/120    avg_loss:1.758, val_acc:0.495]
Epoch [3/120    avg_loss:1.254, val_acc:0.715]
Epoch [4/120    avg_loss:0.978, val_acc:0.728]
Epoch [5/120    avg_loss:0.818, val_acc:0.739]
Epoch [6/120    avg_loss:0.629, val_acc:0.802]
Epoch [7/120    avg_loss:0.569, val_acc:0.873]
Epoch [8/120    avg_loss:0.525, val_acc:0.832]
Epoch [9/120    avg_loss:0.573, val_acc:0.797]
Epoch [10/120    avg_loss:0.461, val_acc:0.863]
Epoch [11/120    avg_loss:0.297, val_acc:0.896]
Epoch [12/120    avg_loss:0.217, val_acc:0.932]
Epoch [13/120    avg_loss:0.159, val_acc:0.917]
Epoch [14/120    avg_loss:0.337, val_acc:0.891]
Epoch [15/120    avg_loss:0.258, val_acc:0.897]
Epoch [16/120    avg_loss:0.130, val_acc:0.932]
Epoch [17/120    avg_loss:0.081, val_acc:0.941]
Epoch [18/120    avg_loss:0.103, val_acc:0.923]
Epoch [19/120    avg_loss:0.110, val_acc:0.950]
Epoch [20/120    avg_loss:0.077, val_acc:0.947]
Epoch [21/120    avg_loss:0.098, val_acc:0.955]
Epoch [22/120    avg_loss:0.077, val_acc:0.960]
Epoch [23/120    avg_loss:0.066, val_acc:0.956]
Epoch [24/120    avg_loss:0.079, val_acc:0.954]
Epoch [25/120    avg_loss:0.074, val_acc:0.967]
Epoch [26/120    avg_loss:0.063, val_acc:0.960]
Epoch [27/120    avg_loss:0.112, val_acc:0.959]
Epoch [28/120    avg_loss:0.091, val_acc:0.967]
Epoch [29/120    avg_loss:0.052, val_acc:0.967]
Epoch [30/120    avg_loss:0.049, val_acc:0.965]
Epoch [31/120    avg_loss:0.037, val_acc:0.972]
Epoch [32/120    avg_loss:0.041, val_acc:0.949]
Epoch [33/120    avg_loss:0.047, val_acc:0.967]
Epoch [34/120    avg_loss:0.045, val_acc:0.955]
Epoch [35/120    avg_loss:0.037, val_acc:0.970]
Epoch [36/120    avg_loss:0.027, val_acc:0.973]
Epoch [37/120    avg_loss:0.033, val_acc:0.973]
Epoch [38/120    avg_loss:0.040, val_acc:0.961]
Epoch [39/120    avg_loss:0.058, val_acc:0.970]
Epoch [40/120    avg_loss:0.039, val_acc:0.974]
Epoch [41/120    avg_loss:0.028, val_acc:0.977]
Epoch [42/120    avg_loss:0.037, val_acc:0.946]
Epoch [43/120    avg_loss:0.035, val_acc:0.973]
Epoch [44/120    avg_loss:0.029, val_acc:0.960]
Epoch [45/120    avg_loss:0.026, val_acc:0.975]
Epoch [46/120    avg_loss:0.027, val_acc:0.981]
Epoch [47/120    avg_loss:0.014, val_acc:0.979]
Epoch [48/120    avg_loss:0.013, val_acc:0.980]
Epoch [49/120    avg_loss:0.016, val_acc:0.968]
Epoch [50/120    avg_loss:0.016, val_acc:0.982]
Epoch [51/120    avg_loss:0.020, val_acc:0.981]
Epoch [52/120    avg_loss:0.021, val_acc:0.978]
Epoch [53/120    avg_loss:0.020, val_acc:0.978]
Epoch [54/120    avg_loss:0.025, val_acc:0.976]
Epoch [55/120    avg_loss:0.018, val_acc:0.979]
Epoch [56/120    avg_loss:0.014, val_acc:0.988]
Epoch [57/120    avg_loss:0.013, val_acc:0.983]
Epoch [58/120    avg_loss:0.010, val_acc:0.983]
Epoch [59/120    avg_loss:0.009, val_acc:0.982]
Epoch [60/120    avg_loss:0.006, val_acc:0.984]
Epoch [61/120    avg_loss:0.009, val_acc:0.982]
Epoch [62/120    avg_loss:0.014, val_acc:0.977]
Epoch [63/120    avg_loss:0.016, val_acc:0.981]
Epoch [64/120    avg_loss:0.014, val_acc:0.980]
Epoch [65/120    avg_loss:0.007, val_acc:0.977]
Epoch [66/120    avg_loss:0.006, val_acc:0.983]
Epoch [67/120    avg_loss:0.007, val_acc:0.979]
Epoch [68/120    avg_loss:0.014, val_acc:0.981]
Epoch [69/120    avg_loss:0.015, val_acc:0.982]
Epoch [70/120    avg_loss:0.011, val_acc:0.983]
Epoch [71/120    avg_loss:0.009, val_acc:0.984]
Epoch [72/120    avg_loss:0.007, val_acc:0.983]
Epoch [73/120    avg_loss:0.006, val_acc:0.985]
Epoch [74/120    avg_loss:0.005, val_acc:0.984]
Epoch [75/120    avg_loss:0.004, val_acc:0.983]
Epoch [76/120    avg_loss:0.005, val_acc:0.984]
Epoch [77/120    avg_loss:0.007, val_acc:0.984]
Epoch [78/120    avg_loss:0.005, val_acc:0.984]
Epoch [79/120    avg_loss:0.004, val_acc:0.985]
Epoch [80/120    avg_loss:0.005, val_acc:0.986]
Epoch [81/120    avg_loss:0.005, val_acc:0.986]
Epoch [82/120    avg_loss:0.005, val_acc:0.986]
Epoch [83/120    avg_loss:0.004, val_acc:0.986]
Epoch [84/120    avg_loss:0.004, val_acc:0.986]
Epoch [85/120    avg_loss:0.004, val_acc:0.986]
Epoch [86/120    avg_loss:0.005, val_acc:0.986]
Epoch [87/120    avg_loss:0.005, val_acc:0.986]
Epoch [88/120    avg_loss:0.003, val_acc:0.986]
Epoch [89/120    avg_loss:0.004, val_acc:0.986]
Epoch [90/120    avg_loss:0.004, val_acc:0.986]
Epoch [91/120    avg_loss:0.005, val_acc:0.986]
Epoch [92/120    avg_loss:0.004, val_acc:0.986]
Epoch [93/120    avg_loss:0.004, val_acc:0.986]
Epoch [94/120    avg_loss:0.005, val_acc:0.986]
Epoch [95/120    avg_loss:0.003, val_acc:0.986]
Epoch [96/120    avg_loss:0.004, val_acc:0.986]
Epoch [97/120    avg_loss:0.005, val_acc:0.986]
Epoch [98/120    avg_loss:0.005, val_acc:0.986]
Epoch [99/120    avg_loss:0.004, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.986]
Epoch [101/120    avg_loss:0.004, val_acc:0.986]
Epoch [102/120    avg_loss:0.006, val_acc:0.986]
Epoch [103/120    avg_loss:0.003, val_acc:0.986]
Epoch [104/120    avg_loss:0.005, val_acc:0.986]
Epoch [105/120    avg_loss:0.006, val_acc:0.986]
Epoch [106/120    avg_loss:0.008, val_acc:0.986]
Epoch [107/120    avg_loss:0.003, val_acc:0.986]
Epoch [108/120    avg_loss:0.003, val_acc:0.986]
Epoch [109/120    avg_loss:0.004, val_acc:0.986]
Epoch [110/120    avg_loss:0.005, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.986]
Epoch [112/120    avg_loss:0.003, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.004, val_acc:0.986]
Epoch [115/120    avg_loss:0.005, val_acc:0.986]
Epoch [116/120    avg_loss:0.005, val_acc:0.986]
Epoch [117/120    avg_loss:0.005, val_acc:0.986]
Epoch [118/120    avg_loss:0.009, val_acc:0.986]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.006, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1266    3    0    0    1    0    0    0    4   10    1    0
     0    0    0]
 [   0    0    0  731    0    0    0    0    0    3    1    2    8    0
     2    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    2    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0  857   17    0    0
     0    1    0]
 [   0    0    6    0    0    0    0    0    0    0    1 2198    5    0
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    4    0  526    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1133    4    0]
 [   0    0    0    0    0    0    8    0    0    0    0    0    0    0
    32  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.53658536585365

F1 scores:
[       nan 0.975      0.98983581 0.98650472 0.99764706 0.99192618
 0.99167298 0.98039216 0.99650757 0.87804878 0.98279817 0.99009009
 0.97407407 1.         0.982228   0.93171472 0.96385542]

Kappa:
0.9833058196887774
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:44--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f5813fc3710>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.382, val_acc:0.479]
Epoch [2/120    avg_loss:1.838, val_acc:0.499]
Epoch [3/120    avg_loss:1.383, val_acc:0.668]
Epoch [4/120    avg_loss:1.156, val_acc:0.752]
Epoch [5/120    avg_loss:0.762, val_acc:0.799]
Epoch [6/120    avg_loss:0.694, val_acc:0.798]
Epoch [7/120    avg_loss:0.611, val_acc:0.801]
Epoch [8/120    avg_loss:0.619, val_acc:0.864]
Epoch [9/120    avg_loss:0.488, val_acc:0.867]
Epoch [10/120    avg_loss:0.371, val_acc:0.869]
Epoch [11/120    avg_loss:0.300, val_acc:0.902]
Epoch [12/120    avg_loss:0.289, val_acc:0.859]
Epoch [13/120    avg_loss:0.241, val_acc:0.921]
Epoch [14/120    avg_loss:0.251, val_acc:0.882]
Epoch [15/120    avg_loss:0.181, val_acc:0.924]
Epoch [16/120    avg_loss:0.145, val_acc:0.930]
Epoch [17/120    avg_loss:0.129, val_acc:0.944]
Epoch [18/120    avg_loss:0.217, val_acc:0.909]
Epoch [19/120    avg_loss:0.129, val_acc:0.926]
Epoch [20/120    avg_loss:0.137, val_acc:0.927]
Epoch [21/120    avg_loss:0.101, val_acc:0.944]
Epoch [22/120    avg_loss:0.062, val_acc:0.952]
Epoch [23/120    avg_loss:0.065, val_acc:0.965]
Epoch [24/120    avg_loss:0.067, val_acc:0.955]
Epoch [25/120    avg_loss:0.249, val_acc:0.900]
Epoch [26/120    avg_loss:0.226, val_acc:0.910]
Epoch [27/120    avg_loss:0.235, val_acc:0.895]
Epoch [28/120    avg_loss:0.143, val_acc:0.955]
Epoch [29/120    avg_loss:0.074, val_acc:0.940]
Epoch [30/120    avg_loss:0.067, val_acc:0.969]
Epoch [31/120    avg_loss:0.056, val_acc:0.958]
Epoch [32/120    avg_loss:0.085, val_acc:0.958]
Epoch [33/120    avg_loss:0.060, val_acc:0.966]
Epoch [34/120    avg_loss:0.051, val_acc:0.967]
Epoch [35/120    avg_loss:0.051, val_acc:0.969]
Epoch [36/120    avg_loss:0.045, val_acc:0.959]
Epoch [37/120    avg_loss:0.064, val_acc:0.959]
Epoch [38/120    avg_loss:0.049, val_acc:0.968]
Epoch [39/120    avg_loss:0.035, val_acc:0.977]
Epoch [40/120    avg_loss:0.034, val_acc:0.979]
Epoch [41/120    avg_loss:0.030, val_acc:0.970]
Epoch [42/120    avg_loss:0.021, val_acc:0.969]
Epoch [43/120    avg_loss:0.022, val_acc:0.977]
Epoch [44/120    avg_loss:0.017, val_acc:0.976]
Epoch [45/120    avg_loss:0.023, val_acc:0.975]
Epoch [46/120    avg_loss:0.021, val_acc:0.974]
Epoch [47/120    avg_loss:0.015, val_acc:0.972]
Epoch [48/120    avg_loss:0.017, val_acc:0.977]
Epoch [49/120    avg_loss:0.018, val_acc:0.979]
Epoch [50/120    avg_loss:0.015, val_acc:0.980]
Epoch [51/120    avg_loss:0.011, val_acc:0.983]
Epoch [52/120    avg_loss:0.014, val_acc:0.978]
Epoch [53/120    avg_loss:0.022, val_acc:0.977]
Epoch [54/120    avg_loss:0.013, val_acc:0.978]
Epoch [55/120    avg_loss:0.019, val_acc:0.981]
Epoch [56/120    avg_loss:0.009, val_acc:0.979]
Epoch [57/120    avg_loss:0.009, val_acc:0.978]
Epoch [58/120    avg_loss:0.012, val_acc:0.979]
Epoch [59/120    avg_loss:0.014, val_acc:0.983]
Epoch [60/120    avg_loss:0.010, val_acc:0.981]
Epoch [61/120    avg_loss:0.010, val_acc:0.982]
Epoch [62/120    avg_loss:0.016, val_acc:0.981]
Epoch [63/120    avg_loss:0.011, val_acc:0.984]
Epoch [64/120    avg_loss:0.008, val_acc:0.982]
Epoch [65/120    avg_loss:0.006, val_acc:0.980]
Epoch [66/120    avg_loss:0.008, val_acc:0.977]
Epoch [67/120    avg_loss:0.016, val_acc:0.978]
Epoch [68/120    avg_loss:0.009, val_acc:0.976]
Epoch [69/120    avg_loss:0.015, val_acc:0.967]
Epoch [70/120    avg_loss:0.016, val_acc:0.978]
Epoch [71/120    avg_loss:0.013, val_acc:0.980]
Epoch [72/120    avg_loss:0.019, val_acc:0.973]
Epoch [73/120    avg_loss:0.008, val_acc:0.980]
Epoch [74/120    avg_loss:0.008, val_acc:0.977]
Epoch [75/120    avg_loss:0.011, val_acc:0.978]
Epoch [76/120    avg_loss:0.022, val_acc:0.974]
Epoch [77/120    avg_loss:0.013, val_acc:0.975]
Epoch [78/120    avg_loss:0.009, val_acc:0.974]
Epoch [79/120    avg_loss:0.010, val_acc:0.976]
Epoch [80/120    avg_loss:0.007, val_acc:0.978]
Epoch [81/120    avg_loss:0.006, val_acc:0.979]
Epoch [82/120    avg_loss:0.006, val_acc:0.979]
Epoch [83/120    avg_loss:0.006, val_acc:0.980]
Epoch [84/120    avg_loss:0.006, val_acc:0.981]
Epoch [85/120    avg_loss:0.005, val_acc:0.981]
Epoch [86/120    avg_loss:0.004, val_acc:0.982]
Epoch [87/120    avg_loss:0.004, val_acc:0.982]
Epoch [88/120    avg_loss:0.007, val_acc:0.982]
Epoch [89/120    avg_loss:0.006, val_acc:0.983]
Epoch [90/120    avg_loss:0.005, val_acc:0.982]
Epoch [91/120    avg_loss:0.004, val_acc:0.982]
Epoch [92/120    avg_loss:0.006, val_acc:0.982]
Epoch [93/120    avg_loss:0.005, val_acc:0.982]
Epoch [94/120    avg_loss:0.005, val_acc:0.982]
Epoch [95/120    avg_loss:0.006, val_acc:0.982]
Epoch [96/120    avg_loss:0.007, val_acc:0.982]
Epoch [97/120    avg_loss:0.005, val_acc:0.982]
Epoch [98/120    avg_loss:0.005, val_acc:0.982]
Epoch [99/120    avg_loss:0.005, val_acc:0.982]
Epoch [100/120    avg_loss:0.005, val_acc:0.982]
Epoch [101/120    avg_loss:0.004, val_acc:0.982]
Epoch [102/120    avg_loss:0.004, val_acc:0.982]
Epoch [103/120    avg_loss:0.005, val_acc:0.982]
Epoch [104/120    avg_loss:0.005, val_acc:0.982]
Epoch [105/120    avg_loss:0.005, val_acc:0.982]
Epoch [106/120    avg_loss:0.004, val_acc:0.982]
Epoch [107/120    avg_loss:0.003, val_acc:0.982]
Epoch [108/120    avg_loss:0.005, val_acc:0.982]
Epoch [109/120    avg_loss:0.006, val_acc:0.983]
Epoch [110/120    avg_loss:0.004, val_acc:0.982]
Epoch [111/120    avg_loss:0.006, val_acc:0.983]
Epoch [112/120    avg_loss:0.005, val_acc:0.983]
Epoch [113/120    avg_loss:0.005, val_acc:0.983]
Epoch [114/120    avg_loss:0.005, val_acc:0.983]
Epoch [115/120    avg_loss:0.007, val_acc:0.983]
Epoch [116/120    avg_loss:0.005, val_acc:0.983]
Epoch [117/120    avg_loss:0.005, val_acc:0.983]
Epoch [118/120    avg_loss:0.004, val_acc:0.983]
Epoch [119/120    avg_loss:0.004, val_acc:0.983]
Epoch [120/120    avg_loss:0.004, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1267    3    0    5    0    0    0    0    4    6    0    0
     0    0    0]
 [   0    0    0  737    0    1    0    0    0    3    0    0    4    2
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    1    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    2    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    6    0    0    1    1    0    0    0  844   22    0    0
     0    1    0]
 [   0    0    3    0    0    0    1    0    0    0    0 2192   12    1
     0    0    1]
 [   0    0    0    1    0    3    0    0    0    0    4    0  523    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1134    4    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    16  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.54742547425474

F1 scores:
[       nan 1.         0.98945724 0.98926174 0.99764706 0.98517674
 0.98642534 0.98039216 0.997669   0.87179487 0.97685185 0.98894654
 0.97121634 0.9919571  0.99039301 0.94925373 0.96470588]

Kappa:
0.9834358406654232
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:46--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb3253a26d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.374, val_acc:0.369]
Epoch [2/120    avg_loss:1.755, val_acc:0.472]
Epoch [3/120    avg_loss:1.363, val_acc:0.680]
Epoch [4/120    avg_loss:1.173, val_acc:0.731]
Epoch [5/120    avg_loss:1.081, val_acc:0.693]
Epoch [6/120    avg_loss:0.794, val_acc:0.715]
Epoch [7/120    avg_loss:0.593, val_acc:0.825]
Epoch [8/120    avg_loss:0.637, val_acc:0.771]
Epoch [9/120    avg_loss:0.356, val_acc:0.852]
Epoch [10/120    avg_loss:0.279, val_acc:0.889]
Epoch [11/120    avg_loss:0.230, val_acc:0.889]
Epoch [12/120    avg_loss:0.214, val_acc:0.802]
Epoch [13/120    avg_loss:0.306, val_acc:0.890]
Epoch [14/120    avg_loss:0.198, val_acc:0.878]
Epoch [15/120    avg_loss:0.200, val_acc:0.929]
Epoch [16/120    avg_loss:0.130, val_acc:0.942]
Epoch [17/120    avg_loss:0.104, val_acc:0.942]
Epoch [18/120    avg_loss:0.168, val_acc:0.912]
Epoch [19/120    avg_loss:0.102, val_acc:0.931]
Epoch [20/120    avg_loss:0.089, val_acc:0.952]
Epoch [21/120    avg_loss:0.077, val_acc:0.935]
Epoch [22/120    avg_loss:0.081, val_acc:0.939]
Epoch [23/120    avg_loss:0.125, val_acc:0.922]
Epoch [24/120    avg_loss:0.199, val_acc:0.921]
Epoch [25/120    avg_loss:0.226, val_acc:0.934]
Epoch [26/120    avg_loss:0.110, val_acc:0.948]
Epoch [27/120    avg_loss:0.100, val_acc:0.950]
Epoch [28/120    avg_loss:0.064, val_acc:0.957]
Epoch [29/120    avg_loss:0.062, val_acc:0.958]
Epoch [30/120    avg_loss:0.059, val_acc:0.961]
Epoch [31/120    avg_loss:0.051, val_acc:0.933]
Epoch [32/120    avg_loss:0.040, val_acc:0.960]
Epoch [33/120    avg_loss:0.026, val_acc:0.968]
Epoch [34/120    avg_loss:0.028, val_acc:0.968]
Epoch [35/120    avg_loss:0.029, val_acc:0.958]
Epoch [36/120    avg_loss:0.037, val_acc:0.972]
Epoch [37/120    avg_loss:0.027, val_acc:0.968]
Epoch [38/120    avg_loss:0.024, val_acc:0.968]
Epoch [39/120    avg_loss:0.024, val_acc:0.965]
Epoch [40/120    avg_loss:0.022, val_acc:0.974]
Epoch [41/120    avg_loss:0.028, val_acc:0.970]
Epoch [42/120    avg_loss:0.025, val_acc:0.973]
Epoch [43/120    avg_loss:0.032, val_acc:0.968]
Epoch [44/120    avg_loss:0.066, val_acc:0.967]
Epoch [45/120    avg_loss:0.035, val_acc:0.968]
Epoch [46/120    avg_loss:0.024, val_acc:0.974]
Epoch [47/120    avg_loss:0.025, val_acc:0.970]
Epoch [48/120    avg_loss:0.025, val_acc:0.978]
Epoch [49/120    avg_loss:0.015, val_acc:0.975]
Epoch [50/120    avg_loss:0.014, val_acc:0.980]
Epoch [51/120    avg_loss:0.011, val_acc:0.976]
Epoch [52/120    avg_loss:0.026, val_acc:0.977]
Epoch [53/120    avg_loss:0.012, val_acc:0.977]
Epoch [54/120    avg_loss:0.011, val_acc:0.975]
Epoch [55/120    avg_loss:0.010, val_acc:0.981]
Epoch [56/120    avg_loss:0.015, val_acc:0.982]
Epoch [57/120    avg_loss:0.014, val_acc:0.978]
Epoch [58/120    avg_loss:0.011, val_acc:0.978]
Epoch [59/120    avg_loss:0.013, val_acc:0.976]
Epoch [60/120    avg_loss:0.011, val_acc:0.977]
Epoch [61/120    avg_loss:0.012, val_acc:0.982]
Epoch [62/120    avg_loss:0.017, val_acc:0.979]
Epoch [63/120    avg_loss:0.008, val_acc:0.980]
Epoch [64/120    avg_loss:0.009, val_acc:0.977]
Epoch [65/120    avg_loss:0.009, val_acc:0.980]
Epoch [66/120    avg_loss:0.011, val_acc:0.979]
Epoch [67/120    avg_loss:0.010, val_acc:0.978]
Epoch [68/120    avg_loss:0.010, val_acc:0.982]
Epoch [69/120    avg_loss:0.009, val_acc:0.980]
Epoch [70/120    avg_loss:0.009, val_acc:0.981]
Epoch [71/120    avg_loss:0.018, val_acc:0.957]
Epoch [72/120    avg_loss:0.013, val_acc:0.982]
Epoch [73/120    avg_loss:0.009, val_acc:0.981]
Epoch [74/120    avg_loss:0.006, val_acc:0.982]
Epoch [75/120    avg_loss:0.011, val_acc:0.979]
Epoch [76/120    avg_loss:0.009, val_acc:0.980]
Epoch [77/120    avg_loss:0.007, val_acc:0.982]
Epoch [78/120    avg_loss:0.004, val_acc:0.983]
Epoch [79/120    avg_loss:0.008, val_acc:0.982]
Epoch [80/120    avg_loss:0.007, val_acc:0.983]
Epoch [81/120    avg_loss:0.005, val_acc:0.983]
Epoch [82/120    avg_loss:0.003, val_acc:0.982]
Epoch [83/120    avg_loss:0.005, val_acc:0.981]
Epoch [84/120    avg_loss:0.008, val_acc:0.979]
Epoch [85/120    avg_loss:0.013, val_acc:0.980]
Epoch [86/120    avg_loss:0.012, val_acc:0.974]
Epoch [87/120    avg_loss:0.014, val_acc:0.968]
Epoch [88/120    avg_loss:0.009, val_acc:0.981]
Epoch [89/120    avg_loss:0.014, val_acc:0.975]
Epoch [90/120    avg_loss:0.009, val_acc:0.983]
Epoch [91/120    avg_loss:0.004, val_acc:0.983]
Epoch [92/120    avg_loss:0.004, val_acc:0.983]
Epoch [93/120    avg_loss:0.003, val_acc:0.984]
Epoch [94/120    avg_loss:0.006, val_acc:0.983]
Epoch [95/120    avg_loss:0.004, val_acc:0.980]
Epoch [96/120    avg_loss:0.007, val_acc:0.980]
Epoch [97/120    avg_loss:0.007, val_acc:0.977]
Epoch [98/120    avg_loss:0.008, val_acc:0.974]
Epoch [99/120    avg_loss:0.007, val_acc:0.979]
Epoch [100/120    avg_loss:0.003, val_acc:0.979]
Epoch [101/120    avg_loss:0.003, val_acc:0.982]
Epoch [102/120    avg_loss:0.003, val_acc:0.981]
Epoch [103/120    avg_loss:0.020, val_acc:0.941]
Epoch [104/120    avg_loss:0.030, val_acc:0.976]
Epoch [105/120    avg_loss:0.022, val_acc:0.963]
Epoch [106/120    avg_loss:0.014, val_acc:0.982]
Epoch [107/120    avg_loss:0.009, val_acc:0.985]
Epoch [108/120    avg_loss:0.010, val_acc:0.983]
Epoch [109/120    avg_loss:0.005, val_acc:0.984]
Epoch [110/120    avg_loss:0.008, val_acc:0.983]
Epoch [111/120    avg_loss:0.005, val_acc:0.982]
Epoch [112/120    avg_loss:0.005, val_acc:0.983]
Epoch [113/120    avg_loss:0.004, val_acc:0.982]
Epoch [114/120    avg_loss:0.004, val_acc:0.982]
Epoch [115/120    avg_loss:0.003, val_acc:0.982]
Epoch [116/120    avg_loss:0.004, val_acc:0.982]
Epoch [117/120    avg_loss:0.002, val_acc:0.982]
Epoch [118/120    avg_loss:0.006, val_acc:0.983]
Epoch [119/120    avg_loss:0.004, val_acc:0.982]
Epoch [120/120    avg_loss:0.003, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1263    3   12    0    1    0    0    0    2    4    0    0
     0    0    0]
 [   0    0    0  720    0   15    0    0    0    2    0    0    7    1
     2    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    2    0    0    0    0  865    2    0    0
     1    1    0]
 [   0    0    9    0    0    0    0    0    0    0    4 2186   10    1
     0    0    0]
 [   0    0    0    3    1    4    0    0    0    0    7    0  514    0
     0    3    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    1    0    0    2
  1135    0    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    18  316    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
98.38482384823848

F1 scores:
[       nan 0.98765432 0.98594848 0.97759674 0.97038724 0.97187852
 0.98793363 0.98039216 1.         0.9        0.98631699 0.99273388
 0.96074766 0.98930481 0.98910675 0.94752624 0.95757576]

Kappa:
0.9815922584915764
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f03508a06a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.314, val_acc:0.481]
Epoch [2/120    avg_loss:1.666, val_acc:0.567]
Epoch [3/120    avg_loss:1.289, val_acc:0.644]
Epoch [4/120    avg_loss:1.009, val_acc:0.703]
Epoch [5/120    avg_loss:0.848, val_acc:0.745]
Epoch [6/120    avg_loss:0.947, val_acc:0.723]
Epoch [7/120    avg_loss:0.582, val_acc:0.824]
Epoch [8/120    avg_loss:0.466, val_acc:0.864]
Epoch [9/120    avg_loss:0.365, val_acc:0.857]
Epoch [10/120    avg_loss:0.300, val_acc:0.886]
Epoch [11/120    avg_loss:0.236, val_acc:0.893]
Epoch [12/120    avg_loss:0.177, val_acc:0.900]
Epoch [13/120    avg_loss:0.176, val_acc:0.904]
Epoch [14/120    avg_loss:0.133, val_acc:0.920]
Epoch [15/120    avg_loss:0.128, val_acc:0.915]
Epoch [16/120    avg_loss:0.131, val_acc:0.932]
Epoch [17/120    avg_loss:0.140, val_acc:0.934]
Epoch [18/120    avg_loss:0.117, val_acc:0.897]
Epoch [19/120    avg_loss:0.088, val_acc:0.942]
Epoch [20/120    avg_loss:0.190, val_acc:0.874]
Epoch [21/120    avg_loss:0.184, val_acc:0.900]
Epoch [22/120    avg_loss:0.142, val_acc:0.934]
Epoch [23/120    avg_loss:0.078, val_acc:0.952]
Epoch [24/120    avg_loss:0.074, val_acc:0.955]
Epoch [25/120    avg_loss:0.061, val_acc:0.962]
Epoch [26/120    avg_loss:0.076, val_acc:0.953]
Epoch [27/120    avg_loss:0.058, val_acc:0.969]
Epoch [28/120    avg_loss:0.045, val_acc:0.964]
Epoch [29/120    avg_loss:0.050, val_acc:0.949]
Epoch [30/120    avg_loss:0.110, val_acc:0.956]
Epoch [31/120    avg_loss:0.057, val_acc:0.963]
Epoch [32/120    avg_loss:0.064, val_acc:0.958]
Epoch [33/120    avg_loss:0.064, val_acc:0.968]
Epoch [34/120    avg_loss:0.033, val_acc:0.975]
Epoch [35/120    avg_loss:0.045, val_acc:0.964]
Epoch [36/120    avg_loss:0.037, val_acc:0.978]
Epoch [37/120    avg_loss:0.028, val_acc:0.986]
Epoch [38/120    avg_loss:0.020, val_acc:0.980]
Epoch [39/120    avg_loss:0.024, val_acc:0.977]
Epoch [40/120    avg_loss:0.023, val_acc:0.980]
Epoch [41/120    avg_loss:0.020, val_acc:0.977]
Epoch [42/120    avg_loss:0.015, val_acc:0.980]
Epoch [43/120    avg_loss:0.015, val_acc:0.986]
Epoch [44/120    avg_loss:0.016, val_acc:0.985]
Epoch [45/120    avg_loss:0.020, val_acc:0.980]
Epoch [46/120    avg_loss:0.019, val_acc:0.983]
Epoch [47/120    avg_loss:0.013, val_acc:0.985]
Epoch [48/120    avg_loss:0.013, val_acc:0.984]
Epoch [49/120    avg_loss:0.016, val_acc:0.980]
Epoch [50/120    avg_loss:0.019, val_acc:0.978]
Epoch [51/120    avg_loss:0.028, val_acc:0.967]
Epoch [52/120    avg_loss:0.036, val_acc:0.971]
Epoch [53/120    avg_loss:0.022, val_acc:0.989]
Epoch [54/120    avg_loss:0.015, val_acc:0.978]
Epoch [55/120    avg_loss:0.012, val_acc:0.983]
Epoch [56/120    avg_loss:0.008, val_acc:0.986]
Epoch [57/120    avg_loss:0.008, val_acc:0.986]
Epoch [58/120    avg_loss:0.006, val_acc:0.984]
Epoch [59/120    avg_loss:0.010, val_acc:0.983]
Epoch [60/120    avg_loss:0.013, val_acc:0.983]
Epoch [61/120    avg_loss:0.011, val_acc:0.987]
Epoch [62/120    avg_loss:0.009, val_acc:0.982]
Epoch [63/120    avg_loss:0.009, val_acc:0.985]
Epoch [64/120    avg_loss:0.006, val_acc:0.986]
Epoch [65/120    avg_loss:0.020, val_acc:0.980]
Epoch [66/120    avg_loss:0.036, val_acc:0.975]
Epoch [67/120    avg_loss:0.020, val_acc:0.978]
Epoch [68/120    avg_loss:0.015, val_acc:0.982]
Epoch [69/120    avg_loss:0.007, val_acc:0.982]
Epoch [70/120    avg_loss:0.007, val_acc:0.983]
Epoch [71/120    avg_loss:0.011, val_acc:0.985]
Epoch [72/120    avg_loss:0.008, val_acc:0.985]
Epoch [73/120    avg_loss:0.009, val_acc:0.985]
Epoch [74/120    avg_loss:0.006, val_acc:0.985]
Epoch [75/120    avg_loss:0.006, val_acc:0.985]
Epoch [76/120    avg_loss:0.006, val_acc:0.985]
Epoch [77/120    avg_loss:0.008, val_acc:0.986]
Epoch [78/120    avg_loss:0.005, val_acc:0.986]
Epoch [79/120    avg_loss:0.005, val_acc:0.988]
Epoch [80/120    avg_loss:0.005, val_acc:0.988]
Epoch [81/120    avg_loss:0.005, val_acc:0.987]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.006, val_acc:0.986]
Epoch [84/120    avg_loss:0.006, val_acc:0.986]
Epoch [85/120    avg_loss:0.005, val_acc:0.987]
Epoch [86/120    avg_loss:0.004, val_acc:0.987]
Epoch [87/120    avg_loss:0.005, val_acc:0.987]
Epoch [88/120    avg_loss:0.005, val_acc:0.987]
Epoch [89/120    avg_loss:0.004, val_acc:0.988]
Epoch [90/120    avg_loss:0.005, val_acc:0.988]
Epoch [91/120    avg_loss:0.006, val_acc:0.988]
Epoch [92/120    avg_loss:0.005, val_acc:0.988]
Epoch [93/120    avg_loss:0.005, val_acc:0.988]
Epoch [94/120    avg_loss:0.007, val_acc:0.988]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.007, val_acc:0.988]
Epoch [97/120    avg_loss:0.007, val_acc:0.988]
Epoch [98/120    avg_loss:0.007, val_acc:0.988]
Epoch [99/120    avg_loss:0.005, val_acc:0.988]
Epoch [100/120    avg_loss:0.007, val_acc:0.988]
Epoch [101/120    avg_loss:0.004, val_acc:0.988]
Epoch [102/120    avg_loss:0.005, val_acc:0.988]
Epoch [103/120    avg_loss:0.007, val_acc:0.988]
Epoch [104/120    avg_loss:0.005, val_acc:0.988]
Epoch [105/120    avg_loss:0.006, val_acc:0.988]
Epoch [106/120    avg_loss:0.006, val_acc:0.988]
Epoch [107/120    avg_loss:0.006, val_acc:0.988]
Epoch [108/120    avg_loss:0.007, val_acc:0.988]
Epoch [109/120    avg_loss:0.007, val_acc:0.988]
Epoch [110/120    avg_loss:0.006, val_acc:0.988]
Epoch [111/120    avg_loss:0.007, val_acc:0.988]
Epoch [112/120    avg_loss:0.008, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.988]
Epoch [114/120    avg_loss:0.006, val_acc:0.988]
Epoch [115/120    avg_loss:0.006, val_acc:0.988]
Epoch [116/120    avg_loss:0.008, val_acc:0.988]
Epoch [117/120    avg_loss:0.007, val_acc:0.988]
Epoch [118/120    avg_loss:0.005, val_acc:0.988]
Epoch [119/120    avg_loss:0.009, val_acc:0.988]
Epoch [120/120    avg_loss:0.006, val_acc:0.988]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1272    3    3    2    1    0    0    0    2    2    0    0
     0    0    0]
 [   0    0    0  724    0    2    0    0    0    8    1    2    9    0
     1    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    3    0    1    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    0    0    0    0  864    7    0    0
     0    0    0]
 [   0    0   10    0    0    0    0    0    0    0    1 2192    5    1
     1    0    0]
 [   0    0    0    2    0    0    0    0    0    0    3    0  528    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0    0    0    0
  1138    0    0]
 [   0    0    0    0    0    0   11    0    0    0    0    0    0    0
    29  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
98.60162601626016

F1 scores:
[       nan 1.         0.98949825 0.9797023  0.9882904  0.98731257
 0.98944193 0.98039216 0.997669   0.76595745 0.98969072 0.99297848
 0.97506925 0.99730458 0.98528139 0.93883792 0.96341463]

Kappa:
0.9840548391601868
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:49--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f7d3eca96d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.332, val_acc:0.506]
Epoch [2/120    avg_loss:1.870, val_acc:0.536]
Epoch [3/120    avg_loss:1.297, val_acc:0.650]
Epoch [4/120    avg_loss:1.157, val_acc:0.728]
Epoch [5/120    avg_loss:0.917, val_acc:0.746]
Epoch [6/120    avg_loss:0.657, val_acc:0.780]
Epoch [7/120    avg_loss:0.539, val_acc:0.846]
Epoch [8/120    avg_loss:0.524, val_acc:0.810]
Epoch [9/120    avg_loss:0.400, val_acc:0.861]
Epoch [10/120    avg_loss:0.361, val_acc:0.844]
Epoch [11/120    avg_loss:0.347, val_acc:0.866]
Epoch [12/120    avg_loss:0.301, val_acc:0.849]
Epoch [13/120    avg_loss:0.181, val_acc:0.889]
Epoch [14/120    avg_loss:0.211, val_acc:0.898]
Epoch [15/120    avg_loss:0.156, val_acc:0.927]
Epoch [16/120    avg_loss:0.132, val_acc:0.891]
Epoch [17/120    avg_loss:0.216, val_acc:0.918]
Epoch [18/120    avg_loss:0.128, val_acc:0.933]
Epoch [19/120    avg_loss:0.099, val_acc:0.939]
Epoch [20/120    avg_loss:0.132, val_acc:0.922]
Epoch [21/120    avg_loss:0.126, val_acc:0.928]
Epoch [22/120    avg_loss:0.088, val_acc:0.945]
Epoch [23/120    avg_loss:0.073, val_acc:0.956]
Epoch [24/120    avg_loss:0.070, val_acc:0.925]
Epoch [25/120    avg_loss:0.073, val_acc:0.952]
Epoch [26/120    avg_loss:0.112, val_acc:0.941]
Epoch [27/120    avg_loss:0.057, val_acc:0.959]
Epoch [28/120    avg_loss:0.043, val_acc:0.955]
Epoch [29/120    avg_loss:0.061, val_acc:0.957]
Epoch [30/120    avg_loss:0.044, val_acc:0.958]
Epoch [31/120    avg_loss:0.045, val_acc:0.968]
Epoch [32/120    avg_loss:0.044, val_acc:0.963]
Epoch [33/120    avg_loss:0.051, val_acc:0.947]
Epoch [34/120    avg_loss:0.066, val_acc:0.961]
Epoch [35/120    avg_loss:0.057, val_acc:0.968]
Epoch [36/120    avg_loss:0.053, val_acc:0.982]
Epoch [37/120    avg_loss:0.035, val_acc:0.966]
Epoch [38/120    avg_loss:0.048, val_acc:0.971]
Epoch [39/120    avg_loss:0.049, val_acc:0.949]
Epoch [40/120    avg_loss:0.065, val_acc:0.957]
Epoch [41/120    avg_loss:0.050, val_acc:0.957]
Epoch [42/120    avg_loss:0.020, val_acc:0.978]
Epoch [43/120    avg_loss:0.023, val_acc:0.974]
Epoch [44/120    avg_loss:0.019, val_acc:0.981]
Epoch [45/120    avg_loss:0.029, val_acc:0.966]
Epoch [46/120    avg_loss:0.034, val_acc:0.968]
Epoch [47/120    avg_loss:0.021, val_acc:0.974]
Epoch [48/120    avg_loss:0.018, val_acc:0.981]
Epoch [49/120    avg_loss:0.013, val_acc:0.981]
Epoch [50/120    avg_loss:0.012, val_acc:0.982]
Epoch [51/120    avg_loss:0.015, val_acc:0.985]
Epoch [52/120    avg_loss:0.008, val_acc:0.985]
Epoch [53/120    avg_loss:0.009, val_acc:0.985]
Epoch [54/120    avg_loss:0.009, val_acc:0.981]
Epoch [55/120    avg_loss:0.010, val_acc:0.985]
Epoch [56/120    avg_loss:0.007, val_acc:0.984]
Epoch [57/120    avg_loss:0.010, val_acc:0.981]
Epoch [58/120    avg_loss:0.012, val_acc:0.982]
Epoch [59/120    avg_loss:0.008, val_acc:0.982]
Epoch [60/120    avg_loss:0.010, val_acc:0.983]
Epoch [61/120    avg_loss:0.007, val_acc:0.983]
Epoch [62/120    avg_loss:0.012, val_acc:0.985]
Epoch [63/120    avg_loss:0.008, val_acc:0.985]
Epoch [64/120    avg_loss:0.010, val_acc:0.985]
Epoch [65/120    avg_loss:0.007, val_acc:0.984]
Epoch [66/120    avg_loss:0.005, val_acc:0.984]
Epoch [67/120    avg_loss:0.008, val_acc:0.984]
Epoch [68/120    avg_loss:0.009, val_acc:0.983]
Epoch [69/120    avg_loss:0.011, val_acc:0.986]
Epoch [70/120    avg_loss:0.009, val_acc:0.985]
Epoch [71/120    avg_loss:0.009, val_acc:0.984]
Epoch [72/120    avg_loss:0.008, val_acc:0.984]
Epoch [73/120    avg_loss:0.008, val_acc:0.984]
Epoch [74/120    avg_loss:0.008, val_acc:0.984]
Epoch [75/120    avg_loss:0.007, val_acc:0.983]
Epoch [76/120    avg_loss:0.005, val_acc:0.983]
Epoch [77/120    avg_loss:0.011, val_acc:0.986]
Epoch [78/120    avg_loss:0.005, val_acc:0.986]
Epoch [79/120    avg_loss:0.007, val_acc:0.984]
Epoch [80/120    avg_loss:0.008, val_acc:0.987]
Epoch [81/120    avg_loss:0.008, val_acc:0.984]
Epoch [82/120    avg_loss:0.006, val_acc:0.985]
Epoch [83/120    avg_loss:0.008, val_acc:0.985]
Epoch [84/120    avg_loss:0.009, val_acc:0.984]
Epoch [85/120    avg_loss:0.005, val_acc:0.983]
Epoch [86/120    avg_loss:0.006, val_acc:0.986]
Epoch [87/120    avg_loss:0.010, val_acc:0.984]
Epoch [88/120    avg_loss:0.007, val_acc:0.985]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.984]
Epoch [91/120    avg_loss:0.006, val_acc:0.984]
Epoch [92/120    avg_loss:0.006, val_acc:0.985]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.008, val_acc:0.985]
Epoch [95/120    avg_loss:0.010, val_acc:0.985]
Epoch [96/120    avg_loss:0.005, val_acc:0.985]
Epoch [97/120    avg_loss:0.007, val_acc:0.985]
Epoch [98/120    avg_loss:0.007, val_acc:0.985]
Epoch [99/120    avg_loss:0.007, val_acc:0.985]
Epoch [100/120    avg_loss:0.005, val_acc:0.985]
Epoch [101/120    avg_loss:0.005, val_acc:0.985]
Epoch [102/120    avg_loss:0.009, val_acc:0.985]
Epoch [103/120    avg_loss:0.006, val_acc:0.985]
Epoch [104/120    avg_loss:0.007, val_acc:0.985]
Epoch [105/120    avg_loss:0.007, val_acc:0.985]
Epoch [106/120    avg_loss:0.008, val_acc:0.985]
Epoch [107/120    avg_loss:0.006, val_acc:0.985]
Epoch [108/120    avg_loss:0.005, val_acc:0.985]
Epoch [109/120    avg_loss:0.006, val_acc:0.985]
Epoch [110/120    avg_loss:0.006, val_acc:0.985]
Epoch [111/120    avg_loss:0.006, val_acc:0.985]
Epoch [112/120    avg_loss:0.010, val_acc:0.985]
Epoch [113/120    avg_loss:0.014, val_acc:0.985]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.006, val_acc:0.985]
Epoch [116/120    avg_loss:0.007, val_acc:0.985]
Epoch [117/120    avg_loss:0.006, val_acc:0.985]
Epoch [118/120    avg_loss:0.005, val_acc:0.985]
Epoch [119/120    avg_loss:0.008, val_acc:0.985]
Epoch [120/120    avg_loss:0.008, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1269    7    3    0    0    0    0    0    1    5    0    0
     0    0    0]
 [   0    0    0  709    0    5    0    0    0    7    0    3   23    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    2    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    5    0    0    2    2    0    0    0  852   11    0    0
     0    3    0]
 [   0    0    8    1    0    0    2    0    0    0    2 2196    1    0
     0    0    0]
 [   0    0    0    1    0    3    0    0    0    0    3    0  524    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    2
  1132    3    0]
 [   0    0    0    0    0    0   13    0    0    0    0    0    0    0
    22  312    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    5    0
     0    0   79]]

Accuracy:
98.28726287262873

F1 scores:
[       nan 0.98765432 0.98870277 0.96791809 0.99300699 0.98517674
 0.98493976 1.         0.995338   0.8        0.98213256 0.99164597
 0.96146789 0.99462366 0.98735281 0.93693694 0.95757576]

Kappa:
0.9804699383973982
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f97369c86a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.299, val_acc:0.522]
Epoch [2/120    avg_loss:1.666, val_acc:0.554]
Epoch [3/120    avg_loss:1.307, val_acc:0.615]
Epoch [4/120    avg_loss:1.088, val_acc:0.788]
Epoch [5/120    avg_loss:0.820, val_acc:0.782]
Epoch [6/120    avg_loss:1.029, val_acc:0.772]
Epoch [7/120    avg_loss:0.683, val_acc:0.829]
Epoch [8/120    avg_loss:0.442, val_acc:0.872]
Epoch [9/120    avg_loss:0.366, val_acc:0.852]
Epoch [10/120    avg_loss:0.447, val_acc:0.853]
Epoch [11/120    avg_loss:0.427, val_acc:0.887]
Epoch [12/120    avg_loss:0.320, val_acc:0.902]
Epoch [13/120    avg_loss:0.181, val_acc:0.929]
Epoch [14/120    avg_loss:0.232, val_acc:0.926]
Epoch [15/120    avg_loss:0.193, val_acc:0.930]
Epoch [16/120    avg_loss:0.145, val_acc:0.928]
Epoch [17/120    avg_loss:0.093, val_acc:0.947]
Epoch [18/120    avg_loss:0.082, val_acc:0.947]
Epoch [19/120    avg_loss:0.091, val_acc:0.933]
Epoch [20/120    avg_loss:0.088, val_acc:0.952]
Epoch [21/120    avg_loss:0.068, val_acc:0.932]
Epoch [22/120    avg_loss:0.065, val_acc:0.943]
Epoch [23/120    avg_loss:0.084, val_acc:0.954]
Epoch [24/120    avg_loss:0.292, val_acc:0.877]
Epoch [25/120    avg_loss:0.125, val_acc:0.949]
Epoch [26/120    avg_loss:0.073, val_acc:0.957]
Epoch [27/120    avg_loss:0.051, val_acc:0.961]
Epoch [28/120    avg_loss:0.063, val_acc:0.949]
Epoch [29/120    avg_loss:0.050, val_acc:0.957]
Epoch [30/120    avg_loss:0.074, val_acc:0.940]
Epoch [31/120    avg_loss:0.066, val_acc:0.967]
Epoch [32/120    avg_loss:0.041, val_acc:0.964]
Epoch [33/120    avg_loss:0.051, val_acc:0.963]
Epoch [34/120    avg_loss:0.026, val_acc:0.974]
Epoch [35/120    avg_loss:0.025, val_acc:0.970]
Epoch [36/120    avg_loss:0.035, val_acc:0.969]
Epoch [37/120    avg_loss:0.022, val_acc:0.976]
Epoch [38/120    avg_loss:0.023, val_acc:0.970]
Epoch [39/120    avg_loss:0.024, val_acc:0.980]
Epoch [40/120    avg_loss:0.014, val_acc:0.974]
Epoch [41/120    avg_loss:0.022, val_acc:0.976]
Epoch [42/120    avg_loss:0.014, val_acc:0.978]
Epoch [43/120    avg_loss:0.014, val_acc:0.983]
Epoch [44/120    avg_loss:0.014, val_acc:0.982]
Epoch [45/120    avg_loss:0.024, val_acc:0.977]
Epoch [46/120    avg_loss:0.022, val_acc:0.972]
Epoch [47/120    avg_loss:0.021, val_acc:0.984]
Epoch [48/120    avg_loss:0.012, val_acc:0.976]
Epoch [49/120    avg_loss:0.014, val_acc:0.983]
Epoch [50/120    avg_loss:0.016, val_acc:0.975]
Epoch [51/120    avg_loss:0.012, val_acc:0.981]
Epoch [52/120    avg_loss:0.026, val_acc:0.946]
Epoch [53/120    avg_loss:0.047, val_acc:0.961]
Epoch [54/120    avg_loss:0.021, val_acc:0.983]
Epoch [55/120    avg_loss:0.013, val_acc:0.975]
Epoch [56/120    avg_loss:0.010, val_acc:0.978]
Epoch [57/120    avg_loss:0.010, val_acc:0.980]
Epoch [58/120    avg_loss:0.011, val_acc:0.982]
Epoch [59/120    avg_loss:0.011, val_acc:0.978]
Epoch [60/120    avg_loss:0.011, val_acc:0.981]
Epoch [61/120    avg_loss:0.021, val_acc:0.981]
Epoch [62/120    avg_loss:0.006, val_acc:0.984]
Epoch [63/120    avg_loss:0.007, val_acc:0.984]
Epoch [64/120    avg_loss:0.007, val_acc:0.984]
Epoch [65/120    avg_loss:0.006, val_acc:0.984]
Epoch [66/120    avg_loss:0.009, val_acc:0.985]
Epoch [67/120    avg_loss:0.012, val_acc:0.983]
Epoch [68/120    avg_loss:0.006, val_acc:0.984]
Epoch [69/120    avg_loss:0.007, val_acc:0.985]
Epoch [70/120    avg_loss:0.006, val_acc:0.984]
Epoch [71/120    avg_loss:0.007, val_acc:0.984]
Epoch [72/120    avg_loss:0.006, val_acc:0.985]
Epoch [73/120    avg_loss:0.005, val_acc:0.984]
Epoch [74/120    avg_loss:0.005, val_acc:0.984]
Epoch [75/120    avg_loss:0.007, val_acc:0.984]
Epoch [76/120    avg_loss:0.006, val_acc:0.983]
Epoch [77/120    avg_loss:0.005, val_acc:0.985]
Epoch [78/120    avg_loss:0.005, val_acc:0.985]
Epoch [79/120    avg_loss:0.004, val_acc:0.984]
Epoch [80/120    avg_loss:0.005, val_acc:0.984]
Epoch [81/120    avg_loss:0.008, val_acc:0.985]
Epoch [82/120    avg_loss:0.006, val_acc:0.985]
Epoch [83/120    avg_loss:0.007, val_acc:0.985]
Epoch [84/120    avg_loss:0.008, val_acc:0.984]
Epoch [85/120    avg_loss:0.006, val_acc:0.984]
Epoch [86/120    avg_loss:0.007, val_acc:0.984]
Epoch [87/120    avg_loss:0.005, val_acc:0.984]
Epoch [88/120    avg_loss:0.004, val_acc:0.984]
Epoch [89/120    avg_loss:0.005, val_acc:0.984]
Epoch [90/120    avg_loss:0.005, val_acc:0.984]
Epoch [91/120    avg_loss:0.004, val_acc:0.984]
Epoch [92/120    avg_loss:0.004, val_acc:0.984]
Epoch [93/120    avg_loss:0.006, val_acc:0.984]
Epoch [94/120    avg_loss:0.007, val_acc:0.985]
Epoch [95/120    avg_loss:0.005, val_acc:0.984]
Epoch [96/120    avg_loss:0.005, val_acc:0.984]
Epoch [97/120    avg_loss:0.005, val_acc:0.984]
Epoch [98/120    avg_loss:0.005, val_acc:0.984]
Epoch [99/120    avg_loss:0.005, val_acc:0.984]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.004, val_acc:0.984]
Epoch [102/120    avg_loss:0.004, val_acc:0.984]
Epoch [103/120    avg_loss:0.004, val_acc:0.984]
Epoch [104/120    avg_loss:0.004, val_acc:0.984]
Epoch [105/120    avg_loss:0.004, val_acc:0.984]
Epoch [106/120    avg_loss:0.005, val_acc:0.984]
Epoch [107/120    avg_loss:0.006, val_acc:0.984]
Epoch [108/120    avg_loss:0.008, val_acc:0.984]
Epoch [109/120    avg_loss:0.004, val_acc:0.984]
Epoch [110/120    avg_loss:0.007, val_acc:0.984]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.006, val_acc:0.984]
Epoch [113/120    avg_loss:0.006, val_acc:0.984]
Epoch [114/120    avg_loss:0.005, val_acc:0.984]
Epoch [115/120    avg_loss:0.005, val_acc:0.984]
Epoch [116/120    avg_loss:0.005, val_acc:0.984]
Epoch [117/120    avg_loss:0.004, val_acc:0.984]
Epoch [118/120    avg_loss:0.006, val_acc:0.984]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.005, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1270    7    2    0    0    0    0    0    1    5    0    0
     0    0    0]
 [   0    0    0  719    0   12    0    0    0    2    0    1    9    4
     0    0    0]
 [   0    0    0    4  209    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    1    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    0    2    0    0    0  854   11    1    0
     2    1    0]
 [   0    0   11    0    0    0    1    0    0    0    0 2195    2    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  532    0
     0    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1137    1    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    23  315    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.54742547425474

F1 scores:
[       nan 1.         0.98832685 0.97227857 0.98584906 0.98065984
 0.98942598 0.98039216 0.99649942 0.86486486 0.98671288 0.992539
 0.98154982 0.98395722 0.98697917 0.94879518 0.97005988]

Kappa:
0.9834357774076145
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f948a31b710>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.344, val_acc:0.431]
Epoch [2/120    avg_loss:1.769, val_acc:0.550]
Epoch [3/120    avg_loss:1.402, val_acc:0.672]
Epoch [4/120    avg_loss:1.092, val_acc:0.755]
Epoch [5/120    avg_loss:0.887, val_acc:0.751]
Epoch [6/120    avg_loss:0.860, val_acc:0.788]
Epoch [7/120    avg_loss:0.939, val_acc:0.792]
Epoch [8/120    avg_loss:0.621, val_acc:0.833]
Epoch [9/120    avg_loss:0.502, val_acc:0.876]
Epoch [10/120    avg_loss:0.511, val_acc:0.880]
Epoch [11/120    avg_loss:0.378, val_acc:0.917]
Epoch [12/120    avg_loss:0.236, val_acc:0.915]
Epoch [13/120    avg_loss:0.236, val_acc:0.877]
Epoch [14/120    avg_loss:0.290, val_acc:0.917]
Epoch [15/120    avg_loss:0.169, val_acc:0.929]
Epoch [16/120    avg_loss:0.194, val_acc:0.920]
Epoch [17/120    avg_loss:0.189, val_acc:0.894]
Epoch [18/120    avg_loss:0.223, val_acc:0.923]
Epoch [19/120    avg_loss:0.147, val_acc:0.936]
Epoch [20/120    avg_loss:0.160, val_acc:0.948]
Epoch [21/120    avg_loss:0.186, val_acc:0.943]
Epoch [22/120    avg_loss:0.119, val_acc:0.959]
Epoch [23/120    avg_loss:0.095, val_acc:0.947]
Epoch [24/120    avg_loss:0.068, val_acc:0.968]
Epoch [25/120    avg_loss:0.050, val_acc:0.958]
Epoch [26/120    avg_loss:0.052, val_acc:0.974]
Epoch [27/120    avg_loss:0.038, val_acc:0.964]
Epoch [28/120    avg_loss:0.055, val_acc:0.954]
Epoch [29/120    avg_loss:0.045, val_acc:0.963]
Epoch [30/120    avg_loss:0.059, val_acc:0.959]
Epoch [31/120    avg_loss:0.036, val_acc:0.967]
Epoch [32/120    avg_loss:0.035, val_acc:0.977]
Epoch [33/120    avg_loss:0.027, val_acc:0.971]
Epoch [34/120    avg_loss:0.033, val_acc:0.971]
Epoch [35/120    avg_loss:0.029, val_acc:0.969]
Epoch [36/120    avg_loss:0.028, val_acc:0.967]
Epoch [37/120    avg_loss:0.022, val_acc:0.975]
Epoch [38/120    avg_loss:0.021, val_acc:0.976]
Epoch [39/120    avg_loss:0.021, val_acc:0.969]
Epoch [40/120    avg_loss:0.033, val_acc:0.960]
Epoch [41/120    avg_loss:0.045, val_acc:0.972]
Epoch [42/120    avg_loss:0.026, val_acc:0.970]
Epoch [43/120    avg_loss:0.034, val_acc:0.960]
Epoch [44/120    avg_loss:0.024, val_acc:0.977]
Epoch [45/120    avg_loss:0.025, val_acc:0.974]
Epoch [46/120    avg_loss:0.033, val_acc:0.970]
Epoch [47/120    avg_loss:0.027, val_acc:0.974]
Epoch [48/120    avg_loss:0.015, val_acc:0.976]
Epoch [49/120    avg_loss:0.023, val_acc:0.975]
Epoch [50/120    avg_loss:0.022, val_acc:0.969]
Epoch [51/120    avg_loss:0.017, val_acc:0.974]
Epoch [52/120    avg_loss:0.015, val_acc:0.981]
Epoch [53/120    avg_loss:0.019, val_acc:0.973]
Epoch [54/120    avg_loss:0.014, val_acc:0.982]
Epoch [55/120    avg_loss:0.013, val_acc:0.977]
Epoch [56/120    avg_loss:0.007, val_acc:0.983]
Epoch [57/120    avg_loss:0.009, val_acc:0.977]
Epoch [58/120    avg_loss:0.010, val_acc:0.983]
Epoch [59/120    avg_loss:0.009, val_acc:0.984]
Epoch [60/120    avg_loss:0.006, val_acc:0.984]
Epoch [61/120    avg_loss:0.005, val_acc:0.987]
Epoch [62/120    avg_loss:0.013, val_acc:0.974]
Epoch [63/120    avg_loss:0.016, val_acc:0.972]
Epoch [64/120    avg_loss:0.014, val_acc:0.978]
Epoch [65/120    avg_loss:0.014, val_acc:0.981]
Epoch [66/120    avg_loss:0.012, val_acc:0.984]
Epoch [67/120    avg_loss:0.006, val_acc:0.985]
Epoch [68/120    avg_loss:0.008, val_acc:0.984]
Epoch [69/120    avg_loss:0.005, val_acc:0.978]
Epoch [70/120    avg_loss:0.006, val_acc:0.977]
Epoch [71/120    avg_loss:0.006, val_acc:0.982]
Epoch [72/120    avg_loss:0.004, val_acc:0.985]
Epoch [73/120    avg_loss:0.005, val_acc:0.983]
Epoch [74/120    avg_loss:0.006, val_acc:0.985]
Epoch [75/120    avg_loss:0.005, val_acc:0.986]
Epoch [76/120    avg_loss:0.007, val_acc:0.985]
Epoch [77/120    avg_loss:0.004, val_acc:0.983]
Epoch [78/120    avg_loss:0.003, val_acc:0.983]
Epoch [79/120    avg_loss:0.005, val_acc:0.983]
Epoch [80/120    avg_loss:0.005, val_acc:0.984]
Epoch [81/120    avg_loss:0.007, val_acc:0.984]
Epoch [82/120    avg_loss:0.004, val_acc:0.984]
Epoch [83/120    avg_loss:0.007, val_acc:0.985]
Epoch [84/120    avg_loss:0.005, val_acc:0.985]
Epoch [85/120    avg_loss:0.005, val_acc:0.985]
Epoch [86/120    avg_loss:0.005, val_acc:0.985]
Epoch [87/120    avg_loss:0.004, val_acc:0.984]
Epoch [88/120    avg_loss:0.008, val_acc:0.984]
Epoch [89/120    avg_loss:0.003, val_acc:0.984]
Epoch [90/120    avg_loss:0.003, val_acc:0.984]
Epoch [91/120    avg_loss:0.003, val_acc:0.984]
Epoch [92/120    avg_loss:0.003, val_acc:0.984]
Epoch [93/120    avg_loss:0.005, val_acc:0.984]
Epoch [94/120    avg_loss:0.003, val_acc:0.984]
Epoch [95/120    avg_loss:0.003, val_acc:0.984]
Epoch [96/120    avg_loss:0.004, val_acc:0.984]
Epoch [97/120    avg_loss:0.007, val_acc:0.984]
Epoch [98/120    avg_loss:0.003, val_acc:0.984]
Epoch [99/120    avg_loss:0.004, val_acc:0.984]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.004, val_acc:0.984]
Epoch [102/120    avg_loss:0.005, val_acc:0.984]
Epoch [103/120    avg_loss:0.003, val_acc:0.984]
Epoch [104/120    avg_loss:0.004, val_acc:0.984]
Epoch [105/120    avg_loss:0.007, val_acc:0.984]
Epoch [106/120    avg_loss:0.004, val_acc:0.984]
Epoch [107/120    avg_loss:0.003, val_acc:0.984]
Epoch [108/120    avg_loss:0.005, val_acc:0.984]
Epoch [109/120    avg_loss:0.003, val_acc:0.984]
Epoch [110/120    avg_loss:0.005, val_acc:0.984]
Epoch [111/120    avg_loss:0.004, val_acc:0.984]
Epoch [112/120    avg_loss:0.004, val_acc:0.984]
Epoch [113/120    avg_loss:0.003, val_acc:0.984]
Epoch [114/120    avg_loss:0.004, val_acc:0.984]
Epoch [115/120    avg_loss:0.005, val_acc:0.984]
Epoch [116/120    avg_loss:0.003, val_acc:0.984]
Epoch [117/120    avg_loss:0.003, val_acc:0.984]
Epoch [118/120    avg_loss:0.004, val_acc:0.984]
Epoch [119/120    avg_loss:0.003, val_acc:0.984]
Epoch [120/120    avg_loss:0.004, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1273    1    0    1    1    0    0    0    3    6    0    0
     0    0    0]
 [   0    0    0  722    0    7    0    0    0    3    0    0   14    1
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    3    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  428    0    0    0    2    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    2    0    0    0    0  853   15    0    0
     0    1    0]
 [   0    0    2    0    0    0    1    0    0    0    7 2198    1    1
     0    0    0]
 [   0    0    0    0    0    6    0    0    0    0    2    0  526    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1135    2    0]
 [   0    0    0    0    0    0   12    0    0    0    0    0    0    0
    23  312    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.54742547425474

F1 scores:
[       nan 0.98765432 0.99297972 0.98031229 0.99528302 0.975
 0.98869631 0.94339623 0.99650757 0.85       0.9793341  0.99232506
 0.97407407 0.99462366 0.98781549 0.94259819 0.98181818]

Kappa:
0.9834344388922898
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:55--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8e40da7710>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.312, val_acc:0.399]
Epoch [2/120    avg_loss:1.726, val_acc:0.524]
Epoch [3/120    avg_loss:1.421, val_acc:0.679]
Epoch [4/120    avg_loss:1.022, val_acc:0.713]
Epoch [5/120    avg_loss:0.904, val_acc:0.772]
Epoch [6/120    avg_loss:0.655, val_acc:0.816]
Epoch [7/120    avg_loss:0.548, val_acc:0.852]
Epoch [8/120    avg_loss:0.662, val_acc:0.824]
Epoch [9/120    avg_loss:0.598, val_acc:0.793]
Epoch [10/120    avg_loss:0.414, val_acc:0.851]
Epoch [11/120    avg_loss:0.307, val_acc:0.875]
Epoch [12/120    avg_loss:0.217, val_acc:0.903]
Epoch [13/120    avg_loss:0.151, val_acc:0.925]
Epoch [14/120    avg_loss:0.142, val_acc:0.934]
Epoch [15/120    avg_loss:0.091, val_acc:0.941]
Epoch [16/120    avg_loss:0.099, val_acc:0.923]
Epoch [17/120    avg_loss:0.104, val_acc:0.934]
Epoch [18/120    avg_loss:0.107, val_acc:0.928]
Epoch [19/120    avg_loss:0.066, val_acc:0.945]
Epoch [20/120    avg_loss:0.062, val_acc:0.953]
Epoch [21/120    avg_loss:0.076, val_acc:0.948]
Epoch [22/120    avg_loss:0.063, val_acc:0.945]
Epoch [23/120    avg_loss:0.052, val_acc:0.952]
Epoch [24/120    avg_loss:0.053, val_acc:0.946]
Epoch [25/120    avg_loss:0.046, val_acc:0.949]
Epoch [26/120    avg_loss:0.064, val_acc:0.956]
Epoch [27/120    avg_loss:0.074, val_acc:0.952]
Epoch [28/120    avg_loss:0.052, val_acc:0.948]
Epoch [29/120    avg_loss:0.049, val_acc:0.966]
Epoch [30/120    avg_loss:0.036, val_acc:0.944]
Epoch [31/120    avg_loss:0.061, val_acc:0.950]
Epoch [32/120    avg_loss:0.034, val_acc:0.977]
Epoch [33/120    avg_loss:0.029, val_acc:0.966]
Epoch [34/120    avg_loss:0.017, val_acc:0.975]
Epoch [35/120    avg_loss:0.020, val_acc:0.966]
Epoch [36/120    avg_loss:0.021, val_acc:0.970]
Epoch [37/120    avg_loss:0.030, val_acc:0.965]
Epoch [38/120    avg_loss:0.025, val_acc:0.963]
Epoch [39/120    avg_loss:0.027, val_acc:0.957]
Epoch [40/120    avg_loss:0.021, val_acc:0.961]
Epoch [41/120    avg_loss:0.023, val_acc:0.960]
Epoch [42/120    avg_loss:0.034, val_acc:0.946]
Epoch [43/120    avg_loss:0.021, val_acc:0.957]
Epoch [44/120    avg_loss:0.020, val_acc:0.968]
Epoch [45/120    avg_loss:0.025, val_acc:0.972]
Epoch [46/120    avg_loss:0.016, val_acc:0.974]
Epoch [47/120    avg_loss:0.015, val_acc:0.973]
Epoch [48/120    avg_loss:0.013, val_acc:0.976]
Epoch [49/120    avg_loss:0.008, val_acc:0.974]
Epoch [50/120    avg_loss:0.011, val_acc:0.973]
Epoch [51/120    avg_loss:0.009, val_acc:0.974]
Epoch [52/120    avg_loss:0.009, val_acc:0.974]
Epoch [53/120    avg_loss:0.010, val_acc:0.973]
Epoch [54/120    avg_loss:0.006, val_acc:0.974]
Epoch [55/120    avg_loss:0.009, val_acc:0.974]
Epoch [56/120    avg_loss:0.008, val_acc:0.976]
Epoch [57/120    avg_loss:0.008, val_acc:0.977]
Epoch [58/120    avg_loss:0.011, val_acc:0.976]
Epoch [59/120    avg_loss:0.007, val_acc:0.975]
Epoch [60/120    avg_loss:0.009, val_acc:0.974]
Epoch [61/120    avg_loss:0.011, val_acc:0.974]
Epoch [62/120    avg_loss:0.008, val_acc:0.975]
Epoch [63/120    avg_loss:0.008, val_acc:0.976]
Epoch [64/120    avg_loss:0.012, val_acc:0.976]
Epoch [65/120    avg_loss:0.008, val_acc:0.974]
Epoch [66/120    avg_loss:0.009, val_acc:0.974]
Epoch [67/120    avg_loss:0.008, val_acc:0.974]
Epoch [68/120    avg_loss:0.007, val_acc:0.975]
Epoch [69/120    avg_loss:0.008, val_acc:0.974]
Epoch [70/120    avg_loss:0.010, val_acc:0.973]
Epoch [71/120    avg_loss:0.011, val_acc:0.973]
Epoch [72/120    avg_loss:0.006, val_acc:0.973]
Epoch [73/120    avg_loss:0.014, val_acc:0.974]
Epoch [74/120    avg_loss:0.007, val_acc:0.974]
Epoch [75/120    avg_loss:0.009, val_acc:0.974]
Epoch [76/120    avg_loss:0.008, val_acc:0.974]
Epoch [77/120    avg_loss:0.007, val_acc:0.974]
Epoch [78/120    avg_loss:0.009, val_acc:0.974]
Epoch [79/120    avg_loss:0.007, val_acc:0.974]
Epoch [80/120    avg_loss:0.007, val_acc:0.974]
Epoch [81/120    avg_loss:0.007, val_acc:0.974]
Epoch [82/120    avg_loss:0.008, val_acc:0.973]
Epoch [83/120    avg_loss:0.007, val_acc:0.973]
Epoch [84/120    avg_loss:0.007, val_acc:0.973]
Epoch [85/120    avg_loss:0.008, val_acc:0.973]
Epoch [86/120    avg_loss:0.006, val_acc:0.973]
Epoch [87/120    avg_loss:0.008, val_acc:0.973]
Epoch [88/120    avg_loss:0.008, val_acc:0.973]
Epoch [89/120    avg_loss:0.009, val_acc:0.973]
Epoch [90/120    avg_loss:0.007, val_acc:0.973]
Epoch [91/120    avg_loss:0.007, val_acc:0.973]
Epoch [92/120    avg_loss:0.006, val_acc:0.973]
Epoch [93/120    avg_loss:0.008, val_acc:0.973]
Epoch [94/120    avg_loss:0.007, val_acc:0.973]
Epoch [95/120    avg_loss:0.009, val_acc:0.973]
Epoch [96/120    avg_loss:0.005, val_acc:0.973]
Epoch [97/120    avg_loss:0.006, val_acc:0.973]
Epoch [98/120    avg_loss:0.007, val_acc:0.973]
Epoch [99/120    avg_loss:0.008, val_acc:0.973]
Epoch [100/120    avg_loss:0.007, val_acc:0.973]
Epoch [101/120    avg_loss:0.007, val_acc:0.973]
Epoch [102/120    avg_loss:0.009, val_acc:0.973]
Epoch [103/120    avg_loss:0.008, val_acc:0.973]
Epoch [104/120    avg_loss:0.006, val_acc:0.973]
Epoch [105/120    avg_loss:0.008, val_acc:0.973]
Epoch [106/120    avg_loss:0.005, val_acc:0.973]
Epoch [107/120    avg_loss:0.009, val_acc:0.973]
Epoch [108/120    avg_loss:0.009, val_acc:0.973]
Epoch [109/120    avg_loss:0.005, val_acc:0.973]
Epoch [110/120    avg_loss:0.008, val_acc:0.973]
Epoch [111/120    avg_loss:0.008, val_acc:0.973]
Epoch [112/120    avg_loss:0.006, val_acc:0.973]
Epoch [113/120    avg_loss:0.007, val_acc:0.973]
Epoch [114/120    avg_loss:0.006, val_acc:0.973]
Epoch [115/120    avg_loss:0.009, val_acc:0.973]
Epoch [116/120    avg_loss:0.007, val_acc:0.973]
Epoch [117/120    avg_loss:0.007, val_acc:0.973]
Epoch [118/120    avg_loss:0.007, val_acc:0.973]
Epoch [119/120    avg_loss:0.008, val_acc:0.973]
Epoch [120/120    avg_loss:0.007, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    0 1276    0    2    0    0    0    0    0    2    3    1    0
     0    1    0]
 [   0    0    0  716    0   14    0    0    0    4    1    0    9    3
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    2    0    0    0    0
     4    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    2    0    0    0    0  852   20    0    0
     0    0    0]
 [   0    0    6    0    0    0    2    0    0    0    4 2191    7    0
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    5    0  525    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    0    0    0    0
  1132    6    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    28  309    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.35230352303523

F1 scores:
[       nan 0.94871795 0.99376947 0.9774744  0.99061033 0.97278912
 0.98942598 1.         0.99767442 0.85714286 0.97762478 0.99005874
 0.97402597 0.9919571  0.98306557 0.9321267  0.98224852]

Kappa:
0.9812101492048424
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:57--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:30
Validation dataloader:30
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:11
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f55104776a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 284833==>0.28M
----------Training process----------
Epoch [1/120    avg_loss:2.292, val_acc:0.507]
Epoch [2/120    avg_loss:1.528, val_acc:0.608]
Epoch [3/120    avg_loss:1.143, val_acc:0.743]
Epoch [4/120    avg_loss:0.960, val_acc:0.739]
Epoch [5/120    avg_loss:0.714, val_acc:0.811]
Epoch [6/120    avg_loss:0.639, val_acc:0.741]
Epoch [7/120    avg_loss:0.559, val_acc:0.833]
Epoch [8/120    avg_loss:0.433, val_acc:0.829]
Epoch [9/120    avg_loss:0.295, val_acc:0.908]
Epoch [10/120    avg_loss:0.315, val_acc:0.891]
Epoch [11/120    avg_loss:0.258, val_acc:0.891]
Epoch [12/120    avg_loss:0.195, val_acc:0.911]
Epoch [13/120    avg_loss:0.136, val_acc:0.938]
Epoch [14/120    avg_loss:0.190, val_acc:0.916]
Epoch [15/120    avg_loss:0.131, val_acc:0.915]
Epoch [16/120    avg_loss:0.116, val_acc:0.932]
Epoch [17/120    avg_loss:0.108, val_acc:0.927]
Epoch [18/120    avg_loss:0.097, val_acc:0.934]
Epoch [19/120    avg_loss:0.095, val_acc:0.929]
Epoch [20/120    avg_loss:0.117, val_acc:0.918]
Epoch [21/120    avg_loss:0.161, val_acc:0.950]
Epoch [22/120    avg_loss:0.067, val_acc:0.943]
Epoch [23/120    avg_loss:0.051, val_acc:0.956]
Epoch [24/120    avg_loss:0.179, val_acc:0.908]
Epoch [25/120    avg_loss:0.111, val_acc:0.943]
Epoch [26/120    avg_loss:0.065, val_acc:0.959]
Epoch [27/120    avg_loss:0.054, val_acc:0.958]
Epoch [28/120    avg_loss:0.044, val_acc:0.967]
Epoch [29/120    avg_loss:0.060, val_acc:0.953]
Epoch [30/120    avg_loss:0.047, val_acc:0.952]
Epoch [31/120    avg_loss:0.044, val_acc:0.943]
Epoch [32/120    avg_loss:0.064, val_acc:0.960]
Epoch [33/120    avg_loss:0.047, val_acc:0.969]
Epoch [34/120    avg_loss:0.038, val_acc:0.963]
Epoch [35/120    avg_loss:0.028, val_acc:0.967]
Epoch [36/120    avg_loss:0.033, val_acc:0.965]
Epoch [37/120    avg_loss:0.047, val_acc:0.965]
Epoch [38/120    avg_loss:0.068, val_acc:0.948]
Epoch [39/120    avg_loss:0.050, val_acc:0.959]
Epoch [40/120    avg_loss:0.128, val_acc:0.959]
Epoch [41/120    avg_loss:0.063, val_acc:0.951]
Epoch [42/120    avg_loss:0.056, val_acc:0.960]
Epoch [43/120    avg_loss:0.029, val_acc:0.969]
Epoch [44/120    avg_loss:0.026, val_acc:0.968]
Epoch [45/120    avg_loss:0.027, val_acc:0.964]
Epoch [46/120    avg_loss:0.020, val_acc:0.967]
Epoch [47/120    avg_loss:0.023, val_acc:0.973]
Epoch [48/120    avg_loss:0.032, val_acc:0.968]
Epoch [49/120    avg_loss:0.027, val_acc:0.972]
Epoch [50/120    avg_loss:0.019, val_acc:0.973]
Epoch [51/120    avg_loss:0.022, val_acc:0.971]
Epoch [52/120    avg_loss:0.019, val_acc:0.973]
Epoch [53/120    avg_loss:0.018, val_acc:0.973]
Epoch [54/120    avg_loss:0.015, val_acc:0.972]
Epoch [55/120    avg_loss:0.018, val_acc:0.972]
Epoch [56/120    avg_loss:0.012, val_acc:0.970]
Epoch [57/120    avg_loss:0.012, val_acc:0.970]
Epoch [58/120    avg_loss:0.015, val_acc:0.977]
Epoch [59/120    avg_loss:0.012, val_acc:0.975]
Epoch [60/120    avg_loss:0.007, val_acc:0.980]
Epoch [61/120    avg_loss:0.012, val_acc:0.976]
Epoch [62/120    avg_loss:0.010, val_acc:0.972]
Epoch [63/120    avg_loss:0.013, val_acc:0.972]
Epoch [64/120    avg_loss:0.017, val_acc:0.975]
Epoch [65/120    avg_loss:0.014, val_acc:0.976]
Epoch [66/120    avg_loss:0.009, val_acc:0.974]
Epoch [67/120    avg_loss:0.010, val_acc:0.979]
Epoch [68/120    avg_loss:0.012, val_acc:0.974]
Epoch [69/120    avg_loss:0.009, val_acc:0.983]
Epoch [70/120    avg_loss:0.004, val_acc:0.982]
Epoch [71/120    avg_loss:0.005, val_acc:0.980]
Epoch [72/120    avg_loss:0.005, val_acc:0.981]
Epoch [73/120    avg_loss:0.004, val_acc:0.981]
Epoch [74/120    avg_loss:0.003, val_acc:0.978]
Epoch [75/120    avg_loss:0.003, val_acc:0.978]
Epoch [76/120    avg_loss:0.005, val_acc:0.973]
Epoch [77/120    avg_loss:0.004, val_acc:0.979]
Epoch [78/120    avg_loss:0.004, val_acc:0.979]
Epoch [79/120    avg_loss:0.007, val_acc:0.977]
Epoch [80/120    avg_loss:0.008, val_acc:0.980]
Epoch [81/120    avg_loss:0.008, val_acc:0.977]
Epoch [82/120    avg_loss:0.004, val_acc:0.976]
Epoch [83/120    avg_loss:0.003, val_acc:0.976]
Epoch [84/120    avg_loss:0.003, val_acc:0.977]
Epoch [85/120    avg_loss:0.005, val_acc:0.976]
Epoch [86/120    avg_loss:0.003, val_acc:0.976]
Epoch [87/120    avg_loss:0.004, val_acc:0.976]
Epoch [88/120    avg_loss:0.004, val_acc:0.975]
Epoch [89/120    avg_loss:0.004, val_acc:0.976]
Epoch [90/120    avg_loss:0.003, val_acc:0.977]
Epoch [91/120    avg_loss:0.002, val_acc:0.977]
Epoch [92/120    avg_loss:0.004, val_acc:0.978]
Epoch [93/120    avg_loss:0.003, val_acc:0.978]
Epoch [94/120    avg_loss:0.003, val_acc:0.977]
Epoch [95/120    avg_loss:0.003, val_acc:0.978]
Epoch [96/120    avg_loss:0.003, val_acc:0.978]
Epoch [97/120    avg_loss:0.004, val_acc:0.978]
Epoch [98/120    avg_loss:0.004, val_acc:0.978]
Epoch [99/120    avg_loss:0.003, val_acc:0.978]
Epoch [100/120    avg_loss:0.002, val_acc:0.978]
Epoch [101/120    avg_loss:0.003, val_acc:0.978]
Epoch [102/120    avg_loss:0.003, val_acc:0.978]
Epoch [103/120    avg_loss:0.004, val_acc:0.978]
Epoch [104/120    avg_loss:0.003, val_acc:0.978]
Epoch [105/120    avg_loss:0.002, val_acc:0.978]
Epoch [106/120    avg_loss:0.004, val_acc:0.978]
Epoch [107/120    avg_loss:0.004, val_acc:0.978]
Epoch [108/120    avg_loss:0.004, val_acc:0.978]
Epoch [109/120    avg_loss:0.003, val_acc:0.978]
Epoch [110/120    avg_loss:0.003, val_acc:0.978]
Epoch [111/120    avg_loss:0.003, val_acc:0.978]
Epoch [112/120    avg_loss:0.002, val_acc:0.978]
Epoch [113/120    avg_loss:0.002, val_acc:0.978]
Epoch [114/120    avg_loss:0.003, val_acc:0.978]
Epoch [115/120    avg_loss:0.002, val_acc:0.978]
Epoch [116/120    avg_loss:0.003, val_acc:0.978]
Epoch [117/120    avg_loss:0.003, val_acc:0.978]
Epoch [118/120    avg_loss:0.002, val_acc:0.978]
Epoch [119/120    avg_loss:0.003, val_acc:0.978]
Epoch [120/120    avg_loss:0.003, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1268    1    4    1    0    0    0    0    4    5    2    0
     0    0    0]
 [   0    0    0  717    0    8    0    0    0    6    3    0   10    2
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  653    0    0    0    0    4    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0  862   12    0    0
     0    0    0]
 [   0    0    0    0    0    0    1    0    0    0    0 2201    8    0
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    0    0  532    0
     0    0    1]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    1    0    0    0
  1134    4    0]
 [   0    0    0    0    0    1   10    0    0    0    0    0    0    0
    25  311    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.61246612466125

F1 scores:
[       nan 0.98765432 0.99295223 0.97750511 0.99069767 0.98409091
 0.98864497 1.         0.99649942 0.76190476 0.98796562 0.99323105
 0.97346752 0.99462366 0.98651588 0.93957704 0.96969697]

Kappa:
0.9841774036789063
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:12:58--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1fcc97c668>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.436, val_acc:0.484]
Epoch [2/120    avg_loss:1.693, val_acc:0.601]
Epoch [3/120    avg_loss:1.368, val_acc:0.634]
Epoch [4/120    avg_loss:1.025, val_acc:0.740]
Epoch [5/120    avg_loss:0.763, val_acc:0.761]
Epoch [6/120    avg_loss:0.729, val_acc:0.767]
Epoch [7/120    avg_loss:0.813, val_acc:0.726]
Epoch [8/120    avg_loss:0.536, val_acc:0.845]
Epoch [9/120    avg_loss:0.471, val_acc:0.861]
Epoch [10/120    avg_loss:0.373, val_acc:0.872]
Epoch [11/120    avg_loss:0.346, val_acc:0.887]
Epoch [12/120    avg_loss:0.348, val_acc:0.873]
Epoch [13/120    avg_loss:0.301, val_acc:0.891]
Epoch [14/120    avg_loss:0.325, val_acc:0.908]
Epoch [15/120    avg_loss:0.329, val_acc:0.876]
Epoch [16/120    avg_loss:0.382, val_acc:0.913]
Epoch [17/120    avg_loss:0.476, val_acc:0.814]
Epoch [18/120    avg_loss:0.327, val_acc:0.908]
Epoch [19/120    avg_loss:0.206, val_acc:0.939]
Epoch [20/120    avg_loss:0.116, val_acc:0.933]
Epoch [21/120    avg_loss:0.116, val_acc:0.936]
Epoch [22/120    avg_loss:0.096, val_acc:0.949]
Epoch [23/120    avg_loss:0.071, val_acc:0.959]
Epoch [24/120    avg_loss:0.063, val_acc:0.953]
Epoch [25/120    avg_loss:0.081, val_acc:0.945]
Epoch [26/120    avg_loss:0.077, val_acc:0.960]
Epoch [27/120    avg_loss:0.064, val_acc:0.948]
Epoch [28/120    avg_loss:0.091, val_acc:0.943]
Epoch [29/120    avg_loss:0.060, val_acc:0.960]
Epoch [30/120    avg_loss:0.056, val_acc:0.963]
Epoch [31/120    avg_loss:0.070, val_acc:0.963]
Epoch [32/120    avg_loss:0.083, val_acc:0.946]
Epoch [33/120    avg_loss:0.068, val_acc:0.955]
Epoch [34/120    avg_loss:0.061, val_acc:0.952]
Epoch [35/120    avg_loss:0.061, val_acc:0.950]
Epoch [36/120    avg_loss:0.058, val_acc:0.927]
Epoch [37/120    avg_loss:0.058, val_acc:0.938]
Epoch [38/120    avg_loss:0.052, val_acc:0.961]
Epoch [39/120    avg_loss:0.043, val_acc:0.962]
Epoch [40/120    avg_loss:0.045, val_acc:0.967]
Epoch [41/120    avg_loss:0.146, val_acc:0.875]
Epoch [42/120    avg_loss:0.133, val_acc:0.936]
Epoch [43/120    avg_loss:0.071, val_acc:0.956]
Epoch [44/120    avg_loss:0.045, val_acc:0.943]
Epoch [45/120    avg_loss:0.043, val_acc:0.969]
Epoch [46/120    avg_loss:0.044, val_acc:0.953]
Epoch [47/120    avg_loss:0.039, val_acc:0.973]
Epoch [48/120    avg_loss:0.042, val_acc:0.958]
Epoch [49/120    avg_loss:0.029, val_acc:0.963]
Epoch [50/120    avg_loss:0.045, val_acc:0.966]
Epoch [51/120    avg_loss:0.043, val_acc:0.970]
Epoch [52/120    avg_loss:0.030, val_acc:0.958]
Epoch [53/120    avg_loss:0.043, val_acc:0.960]
Epoch [54/120    avg_loss:0.029, val_acc:0.975]
Epoch [55/120    avg_loss:0.044, val_acc:0.968]
Epoch [56/120    avg_loss:0.028, val_acc:0.970]
Epoch [57/120    avg_loss:0.026, val_acc:0.972]
Epoch [58/120    avg_loss:0.020, val_acc:0.980]
Epoch [59/120    avg_loss:0.017, val_acc:0.977]
Epoch [60/120    avg_loss:0.023, val_acc:0.964]
Epoch [61/120    avg_loss:0.009, val_acc:0.983]
Epoch [62/120    avg_loss:0.014, val_acc:0.983]
Epoch [63/120    avg_loss:0.025, val_acc:0.969]
Epoch [64/120    avg_loss:0.035, val_acc:0.963]
Epoch [65/120    avg_loss:0.036, val_acc:0.980]
Epoch [66/120    avg_loss:0.027, val_acc:0.952]
Epoch [67/120    avg_loss:0.016, val_acc:0.986]
Epoch [68/120    avg_loss:0.009, val_acc:0.980]
Epoch [69/120    avg_loss:0.017, val_acc:0.981]
Epoch [70/120    avg_loss:0.013, val_acc:0.978]
Epoch [71/120    avg_loss:0.018, val_acc:0.976]
Epoch [72/120    avg_loss:0.013, val_acc:0.974]
Epoch [73/120    avg_loss:0.009, val_acc:0.981]
Epoch [74/120    avg_loss:0.008, val_acc:0.977]
Epoch [75/120    avg_loss:0.007, val_acc:0.984]
Epoch [76/120    avg_loss:0.007, val_acc:0.977]
Epoch [77/120    avg_loss:0.010, val_acc:0.978]
Epoch [78/120    avg_loss:0.010, val_acc:0.982]
Epoch [79/120    avg_loss:0.009, val_acc:0.981]
Epoch [80/120    avg_loss:0.008, val_acc:0.978]
Epoch [81/120    avg_loss:0.009, val_acc:0.980]
Epoch [82/120    avg_loss:0.007, val_acc:0.980]
Epoch [83/120    avg_loss:0.007, val_acc:0.981]
Epoch [84/120    avg_loss:0.006, val_acc:0.983]
Epoch [85/120    avg_loss:0.005, val_acc:0.984]
Epoch [86/120    avg_loss:0.005, val_acc:0.983]
Epoch [87/120    avg_loss:0.007, val_acc:0.983]
Epoch [88/120    avg_loss:0.005, val_acc:0.983]
Epoch [89/120    avg_loss:0.006, val_acc:0.984]
Epoch [90/120    avg_loss:0.006, val_acc:0.981]
Epoch [91/120    avg_loss:0.004, val_acc:0.981]
Epoch [92/120    avg_loss:0.007, val_acc:0.980]
Epoch [93/120    avg_loss:0.005, val_acc:0.982]
Epoch [94/120    avg_loss:0.004, val_acc:0.982]
Epoch [95/120    avg_loss:0.005, val_acc:0.982]
Epoch [96/120    avg_loss:0.006, val_acc:0.983]
Epoch [97/120    avg_loss:0.006, val_acc:0.983]
Epoch [98/120    avg_loss:0.006, val_acc:0.983]
Epoch [99/120    avg_loss:0.007, val_acc:0.982]
Epoch [100/120    avg_loss:0.006, val_acc:0.982]
Epoch [101/120    avg_loss:0.006, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.985]
Epoch [103/120    avg_loss:0.006, val_acc:0.985]
Epoch [104/120    avg_loss:0.004, val_acc:0.985]
Epoch [105/120    avg_loss:0.005, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.985]
Epoch [107/120    avg_loss:0.004, val_acc:0.985]
Epoch [108/120    avg_loss:0.004, val_acc:0.985]
Epoch [109/120    avg_loss:0.005, val_acc:0.985]
Epoch [110/120    avg_loss:0.005, val_acc:0.985]
Epoch [111/120    avg_loss:0.008, val_acc:0.985]
Epoch [112/120    avg_loss:0.006, val_acc:0.985]
Epoch [113/120    avg_loss:0.004, val_acc:0.985]
Epoch [114/120    avg_loss:0.007, val_acc:0.985]
Epoch [115/120    avg_loss:0.007, val_acc:0.985]
Epoch [116/120    avg_loss:0.006, val_acc:0.985]
Epoch [117/120    avg_loss:0.004, val_acc:0.985]
Epoch [118/120    avg_loss:0.005, val_acc:0.985]
Epoch [119/120    avg_loss:0.006, val_acc:0.985]
Epoch [120/120    avg_loss:0.003, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1271    1    5    0    1    0    0    0    3    4    0    0
     0    0    0]
 [   0    0    0  711    1   20    0    0    0    6    0    0    7    2
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    5    1    0    0    0  860    4    0    0
     0    5    0]
 [   0    0    7    0    0    0   10    0    0    0    2 2190    0    1
     0    0    0]
 [   0    0    0    0    0    7    0    0    0    0    0    0  524    0
     0    0    3]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0    0    0
  1136    1    0]
 [   0    0    0    0    0    0   21    0    0    0    0    0    0    0
     8  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.44986449864498

F1 scores:
[       nan 0.975      0.99141966 0.97330595 0.9837587  0.95768374
 0.97321429 0.98039216 0.99883586 0.75555556 0.98680436 0.99297212
 0.98127341 0.98924731 0.99518178 0.94783905 0.9704142 ]

Kappa:
0.9823335748897596
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:00--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f22d3c70780>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.334, val_acc:0.401]
Epoch [2/120    avg_loss:1.645, val_acc:0.517]
Epoch [3/120    avg_loss:1.331, val_acc:0.676]
Epoch [4/120    avg_loss:1.016, val_acc:0.718]
Epoch [5/120    avg_loss:0.894, val_acc:0.731]
Epoch [6/120    avg_loss:0.628, val_acc:0.820]
Epoch [7/120    avg_loss:0.581, val_acc:0.837]
Epoch [8/120    avg_loss:0.559, val_acc:0.820]
Epoch [9/120    avg_loss:0.482, val_acc:0.765]
Epoch [10/120    avg_loss:0.357, val_acc:0.880]
Epoch [11/120    avg_loss:0.264, val_acc:0.839]
Epoch [12/120    avg_loss:0.267, val_acc:0.903]
Epoch [13/120    avg_loss:0.208, val_acc:0.902]
Epoch [14/120    avg_loss:0.274, val_acc:0.870]
Epoch [15/120    avg_loss:0.184, val_acc:0.907]
Epoch [16/120    avg_loss:0.149, val_acc:0.946]
Epoch [17/120    avg_loss:0.104, val_acc:0.938]
Epoch [18/120    avg_loss:0.107, val_acc:0.944]
Epoch [19/120    avg_loss:0.089, val_acc:0.956]
Epoch [20/120    avg_loss:0.088, val_acc:0.947]
Epoch [21/120    avg_loss:0.081, val_acc:0.950]
Epoch [22/120    avg_loss:0.069, val_acc:0.953]
Epoch [23/120    avg_loss:0.064, val_acc:0.952]
Epoch [24/120    avg_loss:0.062, val_acc:0.944]
Epoch [25/120    avg_loss:0.065, val_acc:0.958]
Epoch [26/120    avg_loss:0.084, val_acc:0.946]
Epoch [27/120    avg_loss:0.086, val_acc:0.939]
Epoch [28/120    avg_loss:0.089, val_acc:0.966]
Epoch [29/120    avg_loss:0.070, val_acc:0.968]
Epoch [30/120    avg_loss:0.050, val_acc:0.963]
Epoch [31/120    avg_loss:0.079, val_acc:0.966]
Epoch [32/120    avg_loss:0.055, val_acc:0.964]
Epoch [33/120    avg_loss:0.040, val_acc:0.966]
Epoch [34/120    avg_loss:0.035, val_acc:0.974]
Epoch [35/120    avg_loss:0.026, val_acc:0.969]
Epoch [36/120    avg_loss:0.043, val_acc:0.977]
Epoch [37/120    avg_loss:0.038, val_acc:0.972]
Epoch [38/120    avg_loss:0.026, val_acc:0.977]
Epoch [39/120    avg_loss:0.031, val_acc:0.976]
Epoch [40/120    avg_loss:0.025, val_acc:0.974]
Epoch [41/120    avg_loss:0.026, val_acc:0.973]
Epoch [42/120    avg_loss:0.035, val_acc:0.966]
Epoch [43/120    avg_loss:0.026, val_acc:0.975]
Epoch [44/120    avg_loss:0.027, val_acc:0.977]
Epoch [45/120    avg_loss:0.014, val_acc:0.983]
Epoch [46/120    avg_loss:0.016, val_acc:0.976]
Epoch [47/120    avg_loss:0.019, val_acc:0.977]
Epoch [48/120    avg_loss:0.029, val_acc:0.975]
Epoch [49/120    avg_loss:0.021, val_acc:0.970]
Epoch [50/120    avg_loss:0.019, val_acc:0.978]
Epoch [51/120    avg_loss:0.015, val_acc:0.978]
Epoch [52/120    avg_loss:0.019, val_acc:0.977]
Epoch [53/120    avg_loss:0.014, val_acc:0.978]
Epoch [54/120    avg_loss:0.012, val_acc:0.978]
Epoch [55/120    avg_loss:0.046, val_acc:0.926]
Epoch [56/120    avg_loss:0.056, val_acc:0.970]
Epoch [57/120    avg_loss:0.021, val_acc:0.971]
Epoch [58/120    avg_loss:0.025, val_acc:0.983]
Epoch [59/120    avg_loss:0.014, val_acc:0.981]
Epoch [60/120    avg_loss:0.010, val_acc:0.981]
Epoch [61/120    avg_loss:0.012, val_acc:0.977]
Epoch [62/120    avg_loss:0.014, val_acc:0.985]
Epoch [63/120    avg_loss:0.019, val_acc:0.982]
Epoch [64/120    avg_loss:0.009, val_acc:0.984]
Epoch [65/120    avg_loss:0.012, val_acc:0.977]
Epoch [66/120    avg_loss:0.012, val_acc:0.977]
Epoch [67/120    avg_loss:0.014, val_acc:0.984]
Epoch [68/120    avg_loss:0.014, val_acc:0.977]
Epoch [69/120    avg_loss:0.024, val_acc:0.977]
Epoch [70/120    avg_loss:0.010, val_acc:0.981]
Epoch [71/120    avg_loss:0.007, val_acc:0.985]
Epoch [72/120    avg_loss:0.007, val_acc:0.982]
Epoch [73/120    avg_loss:0.015, val_acc:0.985]
Epoch [74/120    avg_loss:0.007, val_acc:0.980]
Epoch [75/120    avg_loss:0.023, val_acc:0.980]
Epoch [76/120    avg_loss:0.026, val_acc:0.982]
Epoch [77/120    avg_loss:0.018, val_acc:0.976]
Epoch [78/120    avg_loss:0.011, val_acc:0.983]
Epoch [79/120    avg_loss:0.011, val_acc:0.981]
Epoch [80/120    avg_loss:0.011, val_acc:0.984]
Epoch [81/120    avg_loss:0.009, val_acc:0.986]
Epoch [82/120    avg_loss:0.008, val_acc:0.983]
Epoch [83/120    avg_loss:0.011, val_acc:0.983]
Epoch [84/120    avg_loss:0.008, val_acc:0.984]
Epoch [85/120    avg_loss:0.010, val_acc:0.983]
Epoch [86/120    avg_loss:0.007, val_acc:0.988]
Epoch [87/120    avg_loss:0.006, val_acc:0.986]
Epoch [88/120    avg_loss:0.008, val_acc:0.982]
Epoch [89/120    avg_loss:0.010, val_acc:0.988]
Epoch [90/120    avg_loss:0.005, val_acc:0.987]
Epoch [91/120    avg_loss:0.004, val_acc:0.985]
Epoch [92/120    avg_loss:0.004, val_acc:0.983]
Epoch [93/120    avg_loss:0.006, val_acc:0.986]
Epoch [94/120    avg_loss:0.005, val_acc:0.987]
Epoch [95/120    avg_loss:0.004, val_acc:0.986]
Epoch [96/120    avg_loss:0.004, val_acc:0.986]
Epoch [97/120    avg_loss:0.004, val_acc:0.986]
Epoch [98/120    avg_loss:0.006, val_acc:0.987]
Epoch [99/120    avg_loss:0.011, val_acc:0.973]
Epoch [100/120    avg_loss:0.018, val_acc:0.980]
Epoch [101/120    avg_loss:0.015, val_acc:0.981]
Epoch [102/120    avg_loss:0.009, val_acc:0.984]
Epoch [103/120    avg_loss:0.004, val_acc:0.984]
Epoch [104/120    avg_loss:0.006, val_acc:0.983]
Epoch [105/120    avg_loss:0.005, val_acc:0.983]
Epoch [106/120    avg_loss:0.004, val_acc:0.983]
Epoch [107/120    avg_loss:0.004, val_acc:0.983]
Epoch [108/120    avg_loss:0.003, val_acc:0.984]
Epoch [109/120    avg_loss:0.004, val_acc:0.984]
Epoch [110/120    avg_loss:0.003, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.985]
Epoch [112/120    avg_loss:0.003, val_acc:0.986]
Epoch [113/120    avg_loss:0.004, val_acc:0.986]
Epoch [114/120    avg_loss:0.005, val_acc:0.984]
Epoch [115/120    avg_loss:0.004, val_acc:0.985]
Epoch [116/120    avg_loss:0.005, val_acc:0.985]
Epoch [117/120    avg_loss:0.004, val_acc:0.985]
Epoch [118/120    avg_loss:0.003, val_acc:0.985]
Epoch [119/120    avg_loss:0.004, val_acc:0.985]
Epoch [120/120    avg_loss:0.004, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1272    5    1    1    0    0    0    0    2    2    2    0
     0    0    0]
 [   0    0    0  717    0    9    0    0    0    3    0    0   16    0
     2    0    0]
 [   0    0    0    3  210    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0  867    2    2    0
     0    1    0]
 [   0    0    6    0    0    0    3    0    0    0    2 2198    0    1
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    4    0  527    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   18    0    0    0    0    0    0    0
    21  308    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
98.63414634146342

F1 scores:
[       nan 0.975      0.99219969 0.97352342 0.99056604 0.97954545
 0.98274569 0.98039216 0.99883856 0.82926829 0.98972603 0.99592207
 0.97232472 0.99730458 0.98912571 0.93902439 0.97005988]

Kappa:
0.9844277483898726
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f02a60e26d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.410, val_acc:0.459]
Epoch [2/120    avg_loss:1.695, val_acc:0.588]
Epoch [3/120    avg_loss:1.443, val_acc:0.650]
Epoch [4/120    avg_loss:1.053, val_acc:0.742]
Epoch [5/120    avg_loss:0.887, val_acc:0.773]
Epoch [6/120    avg_loss:0.655, val_acc:0.767]
Epoch [7/120    avg_loss:0.659, val_acc:0.832]
Epoch [8/120    avg_loss:0.578, val_acc:0.767]
Epoch [9/120    avg_loss:0.580, val_acc:0.841]
Epoch [10/120    avg_loss:0.368, val_acc:0.883]
Epoch [11/120    avg_loss:0.393, val_acc:0.886]
Epoch [12/120    avg_loss:0.302, val_acc:0.929]
Epoch [13/120    avg_loss:0.244, val_acc:0.915]
Epoch [14/120    avg_loss:0.178, val_acc:0.926]
Epoch [15/120    avg_loss:0.159, val_acc:0.941]
Epoch [16/120    avg_loss:0.141, val_acc:0.913]
Epoch [17/120    avg_loss:0.128, val_acc:0.947]
Epoch [18/120    avg_loss:0.128, val_acc:0.955]
Epoch [19/120    avg_loss:0.122, val_acc:0.939]
Epoch [20/120    avg_loss:0.094, val_acc:0.961]
Epoch [21/120    avg_loss:0.105, val_acc:0.950]
Epoch [22/120    avg_loss:0.115, val_acc:0.959]
Epoch [23/120    avg_loss:0.073, val_acc:0.950]
Epoch [24/120    avg_loss:0.093, val_acc:0.961]
Epoch [25/120    avg_loss:0.075, val_acc:0.953]
Epoch [26/120    avg_loss:0.063, val_acc:0.954]
Epoch [27/120    avg_loss:0.056, val_acc:0.970]
Epoch [28/120    avg_loss:0.055, val_acc:0.967]
Epoch [29/120    avg_loss:0.048, val_acc:0.975]
Epoch [30/120    avg_loss:0.037, val_acc:0.971]
Epoch [31/120    avg_loss:0.034, val_acc:0.973]
Epoch [32/120    avg_loss:0.043, val_acc:0.972]
Epoch [33/120    avg_loss:0.031, val_acc:0.981]
Epoch [34/120    avg_loss:0.041, val_acc:0.972]
Epoch [35/120    avg_loss:0.059, val_acc:0.972]
Epoch [36/120    avg_loss:0.059, val_acc:0.971]
Epoch [37/120    avg_loss:0.063, val_acc:0.963]
Epoch [38/120    avg_loss:0.051, val_acc:0.973]
Epoch [39/120    avg_loss:0.042, val_acc:0.980]
Epoch [40/120    avg_loss:0.044, val_acc:0.972]
Epoch [41/120    avg_loss:0.028, val_acc:0.976]
Epoch [42/120    avg_loss:0.024, val_acc:0.978]
Epoch [43/120    avg_loss:0.018, val_acc:0.986]
Epoch [44/120    avg_loss:0.022, val_acc:0.970]
Epoch [45/120    avg_loss:0.029, val_acc:0.977]
Epoch [46/120    avg_loss:0.034, val_acc:0.958]
Epoch [47/120    avg_loss:0.025, val_acc:0.972]
Epoch [48/120    avg_loss:0.022, val_acc:0.977]
Epoch [49/120    avg_loss:0.024, val_acc:0.971]
Epoch [50/120    avg_loss:0.016, val_acc:0.982]
Epoch [51/120    avg_loss:0.015, val_acc:0.984]
Epoch [52/120    avg_loss:0.027, val_acc:0.963]
Epoch [53/120    avg_loss:0.036, val_acc:0.964]
Epoch [54/120    avg_loss:0.023, val_acc:0.977]
Epoch [55/120    avg_loss:0.020, val_acc:0.982]
Epoch [56/120    avg_loss:0.014, val_acc:0.985]
Epoch [57/120    avg_loss:0.013, val_acc:0.986]
Epoch [58/120    avg_loss:0.010, val_acc:0.988]
Epoch [59/120    avg_loss:0.015, val_acc:0.984]
Epoch [60/120    avg_loss:0.010, val_acc:0.985]
Epoch [61/120    avg_loss:0.010, val_acc:0.985]
Epoch [62/120    avg_loss:0.011, val_acc:0.986]
Epoch [63/120    avg_loss:0.008, val_acc:0.985]
Epoch [64/120    avg_loss:0.015, val_acc:0.985]
Epoch [65/120    avg_loss:0.009, val_acc:0.986]
Epoch [66/120    avg_loss:0.009, val_acc:0.987]
Epoch [67/120    avg_loss:0.008, val_acc:0.986]
Epoch [68/120    avg_loss:0.009, val_acc:0.986]
Epoch [69/120    avg_loss:0.009, val_acc:0.987]
Epoch [70/120    avg_loss:0.013, val_acc:0.987]
Epoch [71/120    avg_loss:0.008, val_acc:0.984]
Epoch [72/120    avg_loss:0.008, val_acc:0.984]
Epoch [73/120    avg_loss:0.014, val_acc:0.985]
Epoch [74/120    avg_loss:0.007, val_acc:0.985]
Epoch [75/120    avg_loss:0.009, val_acc:0.986]
Epoch [76/120    avg_loss:0.010, val_acc:0.986]
Epoch [77/120    avg_loss:0.011, val_acc:0.986]
Epoch [78/120    avg_loss:0.009, val_acc:0.986]
Epoch [79/120    avg_loss:0.008, val_acc:0.986]
Epoch [80/120    avg_loss:0.007, val_acc:0.985]
Epoch [81/120    avg_loss:0.010, val_acc:0.986]
Epoch [82/120    avg_loss:0.011, val_acc:0.986]
Epoch [83/120    avg_loss:0.009, val_acc:0.987]
Epoch [84/120    avg_loss:0.011, val_acc:0.986]
Epoch [85/120    avg_loss:0.011, val_acc:0.986]
Epoch [86/120    avg_loss:0.007, val_acc:0.986]
Epoch [87/120    avg_loss:0.009, val_acc:0.986]
Epoch [88/120    avg_loss:0.009, val_acc:0.986]
Epoch [89/120    avg_loss:0.009, val_acc:0.986]
Epoch [90/120    avg_loss:0.010, val_acc:0.986]
Epoch [91/120    avg_loss:0.008, val_acc:0.986]
Epoch [92/120    avg_loss:0.010, val_acc:0.986]
Epoch [93/120    avg_loss:0.008, val_acc:0.986]
Epoch [94/120    avg_loss:0.006, val_acc:0.986]
Epoch [95/120    avg_loss:0.010, val_acc:0.986]
Epoch [96/120    avg_loss:0.007, val_acc:0.986]
Epoch [97/120    avg_loss:0.009, val_acc:0.986]
Epoch [98/120    avg_loss:0.011, val_acc:0.986]
Epoch [99/120    avg_loss:0.010, val_acc:0.986]
Epoch [100/120    avg_loss:0.009, val_acc:0.986]
Epoch [101/120    avg_loss:0.014, val_acc:0.986]
Epoch [102/120    avg_loss:0.009, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.986]
Epoch [104/120    avg_loss:0.010, val_acc:0.986]
Epoch [105/120    avg_loss:0.008, val_acc:0.986]
Epoch [106/120    avg_loss:0.009, val_acc:0.986]
Epoch [107/120    avg_loss:0.008, val_acc:0.986]
Epoch [108/120    avg_loss:0.007, val_acc:0.986]
Epoch [109/120    avg_loss:0.009, val_acc:0.986]
Epoch [110/120    avg_loss:0.009, val_acc:0.986]
Epoch [111/120    avg_loss:0.011, val_acc:0.986]
Epoch [112/120    avg_loss:0.012, val_acc:0.986]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.009, val_acc:0.986]
Epoch [115/120    avg_loss:0.009, val_acc:0.986]
Epoch [116/120    avg_loss:0.009, val_acc:0.986]
Epoch [117/120    avg_loss:0.009, val_acc:0.986]
Epoch [118/120    avg_loss:0.012, val_acc:0.986]
Epoch [119/120    avg_loss:0.007, val_acc:0.986]
Epoch [120/120    avg_loss:0.009, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1267    3    0    0    0    0    0    0    6    7    2    0
     0    0    0]
 [   0    0    0  719    0    9    0    0    0    5    0    0   10    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    2    0    0    6    0    0    0    0  858    4    0    0
     0    5    0]
 [   0    0    1    0    0    0    3    0    0    0    3 2202    0    1
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    0  525    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    2    0    0    0
  1131    0    0]
 [   0    0    0    0    0    0   15    0    0    0    0    0    0    0
    35  297    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.42818428184282

F1 scores:
[       nan 0.98765432 0.99178082 0.97756628 1.         0.97194164
 0.98572502 1.         0.99883856 0.7804878  0.98338109 0.9954792
 0.97947761 0.98666667 0.9813449  0.91525424 0.95953757]

Kappa:
0.982078094289623
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:04--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9fad0d16a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.352, val_acc:0.461]
Epoch [2/120    avg_loss:1.707, val_acc:0.617]
Epoch [3/120    avg_loss:1.372, val_acc:0.685]
Epoch [4/120    avg_loss:1.225, val_acc:0.656]
Epoch [5/120    avg_loss:0.906, val_acc:0.816]
Epoch [6/120    avg_loss:0.724, val_acc:0.800]
Epoch [7/120    avg_loss:0.786, val_acc:0.784]
Epoch [8/120    avg_loss:0.729, val_acc:0.818]
Epoch [9/120    avg_loss:0.523, val_acc:0.845]
Epoch [10/120    avg_loss:0.432, val_acc:0.834]
Epoch [11/120    avg_loss:0.391, val_acc:0.874]
Epoch [12/120    avg_loss:0.317, val_acc:0.873]
Epoch [13/120    avg_loss:0.245, val_acc:0.851]
Epoch [14/120    avg_loss:0.246, val_acc:0.912]
Epoch [15/120    avg_loss:0.231, val_acc:0.931]
Epoch [16/120    avg_loss:0.171, val_acc:0.902]
Epoch [17/120    avg_loss:0.155, val_acc:0.934]
Epoch [18/120    avg_loss:0.104, val_acc:0.949]
Epoch [19/120    avg_loss:0.208, val_acc:0.920]
Epoch [20/120    avg_loss:0.134, val_acc:0.946]
Epoch [21/120    avg_loss:0.115, val_acc:0.956]
Epoch [22/120    avg_loss:0.190, val_acc:0.926]
Epoch [23/120    avg_loss:0.195, val_acc:0.934]
Epoch [24/120    avg_loss:0.112, val_acc:0.952]
Epoch [25/120    avg_loss:0.093, val_acc:0.908]
Epoch [26/120    avg_loss:0.149, val_acc:0.943]
Epoch [27/120    avg_loss:0.085, val_acc:0.954]
Epoch [28/120    avg_loss:0.074, val_acc:0.941]
Epoch [29/120    avg_loss:0.057, val_acc:0.960]
Epoch [30/120    avg_loss:0.057, val_acc:0.966]
Epoch [31/120    avg_loss:0.080, val_acc:0.962]
Epoch [32/120    avg_loss:0.055, val_acc:0.959]
Epoch [33/120    avg_loss:0.086, val_acc:0.968]
Epoch [34/120    avg_loss:0.048, val_acc:0.968]
Epoch [35/120    avg_loss:0.040, val_acc:0.973]
Epoch [36/120    avg_loss:0.064, val_acc:0.955]
Epoch [37/120    avg_loss:0.072, val_acc:0.966]
Epoch [38/120    avg_loss:0.042, val_acc:0.954]
Epoch [39/120    avg_loss:0.052, val_acc:0.964]
Epoch [40/120    avg_loss:0.044, val_acc:0.955]
Epoch [41/120    avg_loss:0.025, val_acc:0.970]
Epoch [42/120    avg_loss:0.026, val_acc:0.975]
Epoch [43/120    avg_loss:0.022, val_acc:0.977]
Epoch [44/120    avg_loss:0.023, val_acc:0.981]
Epoch [45/120    avg_loss:0.023, val_acc:0.978]
Epoch [46/120    avg_loss:0.025, val_acc:0.971]
Epoch [47/120    avg_loss:0.015, val_acc:0.976]
Epoch [48/120    avg_loss:0.015, val_acc:0.982]
Epoch [49/120    avg_loss:0.013, val_acc:0.977]
Epoch [50/120    avg_loss:0.025, val_acc:0.967]
Epoch [51/120    avg_loss:0.019, val_acc:0.974]
Epoch [52/120    avg_loss:0.029, val_acc:0.958]
Epoch [53/120    avg_loss:0.028, val_acc:0.980]
Epoch [54/120    avg_loss:0.031, val_acc:0.968]
Epoch [55/120    avg_loss:0.023, val_acc:0.980]
Epoch [56/120    avg_loss:0.017, val_acc:0.974]
Epoch [57/120    avg_loss:0.031, val_acc:0.963]
Epoch [58/120    avg_loss:0.048, val_acc:0.970]
Epoch [59/120    avg_loss:0.021, val_acc:0.975]
Epoch [60/120    avg_loss:0.015, val_acc:0.972]
Epoch [61/120    avg_loss:0.017, val_acc:0.977]
Epoch [62/120    avg_loss:0.015, val_acc:0.977]
Epoch [63/120    avg_loss:0.011, val_acc:0.976]
Epoch [64/120    avg_loss:0.013, val_acc:0.975]
Epoch [65/120    avg_loss:0.009, val_acc:0.976]
Epoch [66/120    avg_loss:0.014, val_acc:0.977]
Epoch [67/120    avg_loss:0.010, val_acc:0.981]
Epoch [68/120    avg_loss:0.010, val_acc:0.980]
Epoch [69/120    avg_loss:0.012, val_acc:0.977]
Epoch [70/120    avg_loss:0.008, val_acc:0.977]
Epoch [71/120    avg_loss:0.009, val_acc:0.978]
Epoch [72/120    avg_loss:0.012, val_acc:0.980]
Epoch [73/120    avg_loss:0.009, val_acc:0.980]
Epoch [74/120    avg_loss:0.009, val_acc:0.978]
Epoch [75/120    avg_loss:0.011, val_acc:0.978]
Epoch [76/120    avg_loss:0.008, val_acc:0.978]
Epoch [77/120    avg_loss:0.008, val_acc:0.978]
Epoch [78/120    avg_loss:0.007, val_acc:0.980]
Epoch [79/120    avg_loss:0.008, val_acc:0.980]
Epoch [80/120    avg_loss:0.010, val_acc:0.980]
Epoch [81/120    avg_loss:0.012, val_acc:0.981]
Epoch [82/120    avg_loss:0.008, val_acc:0.981]
Epoch [83/120    avg_loss:0.008, val_acc:0.981]
Epoch [84/120    avg_loss:0.007, val_acc:0.981]
Epoch [85/120    avg_loss:0.010, val_acc:0.981]
Epoch [86/120    avg_loss:0.006, val_acc:0.981]
Epoch [87/120    avg_loss:0.012, val_acc:0.981]
Epoch [88/120    avg_loss:0.008, val_acc:0.981]
Epoch [89/120    avg_loss:0.008, val_acc:0.981]
Epoch [90/120    avg_loss:0.010, val_acc:0.981]
Epoch [91/120    avg_loss:0.009, val_acc:0.981]
Epoch [92/120    avg_loss:0.007, val_acc:0.981]
Epoch [93/120    avg_loss:0.009, val_acc:0.981]
Epoch [94/120    avg_loss:0.009, val_acc:0.981]
Epoch [95/120    avg_loss:0.006, val_acc:0.981]
Epoch [96/120    avg_loss:0.009, val_acc:0.981]
Epoch [97/120    avg_loss:0.007, val_acc:0.981]
Epoch [98/120    avg_loss:0.007, val_acc:0.981]
Epoch [99/120    avg_loss:0.009, val_acc:0.981]
Epoch [100/120    avg_loss:0.007, val_acc:0.981]
Epoch [101/120    avg_loss:0.014, val_acc:0.981]
Epoch [102/120    avg_loss:0.010, val_acc:0.981]
Epoch [103/120    avg_loss:0.006, val_acc:0.981]
Epoch [104/120    avg_loss:0.011, val_acc:0.981]
Epoch [105/120    avg_loss:0.007, val_acc:0.981]
Epoch [106/120    avg_loss:0.008, val_acc:0.981]
Epoch [107/120    avg_loss:0.008, val_acc:0.981]
Epoch [108/120    avg_loss:0.011, val_acc:0.981]
Epoch [109/120    avg_loss:0.008, val_acc:0.981]
Epoch [110/120    avg_loss:0.008, val_acc:0.981]
Epoch [111/120    avg_loss:0.009, val_acc:0.981]
Epoch [112/120    avg_loss:0.010, val_acc:0.981]
Epoch [113/120    avg_loss:0.007, val_acc:0.981]
Epoch [114/120    avg_loss:0.009, val_acc:0.981]
Epoch [115/120    avg_loss:0.006, val_acc:0.981]
Epoch [116/120    avg_loss:0.009, val_acc:0.981]
Epoch [117/120    avg_loss:0.005, val_acc:0.981]
Epoch [118/120    avg_loss:0.007, val_acc:0.981]
Epoch [119/120    avg_loss:0.007, val_acc:0.981]
Epoch [120/120    avg_loss:0.009, val_acc:0.981]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1272    0    0    0    1    0    0    0    4    4    3    1
     0    0    0]
 [   0    0    0  705    0   15    0    0    0    8    0    0   16    2
     1    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    0    0    3    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    3    0    0    0    0  863    2    0    0
     2    4    0]
 [   0    0   11    0    0    0    9    0    0    0    9 2177    2    2
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    0    0  526    0
     0    1    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    1    0    3    0    0    0
  1126    8    0]
 [   0    0    0    0    0    0   26    0    0    0    0    0    0    0
    11  310    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.17886178861788

F1 scores:
[       nan 0.98765432 0.98988327 0.96907216 1.         0.96737908
 0.97333333 1.         0.99883856 0.68181818 0.98403649 0.99112224
 0.97227357 0.98666667 0.98728628 0.92537313 0.98224852]

Kappa:
0.9792510690927484
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:06--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f1161ad2748>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.373, val_acc:0.548]
Epoch [2/120    avg_loss:1.728, val_acc:0.613]
Epoch [3/120    avg_loss:1.354, val_acc:0.584]
Epoch [4/120    avg_loss:1.130, val_acc:0.700]
Epoch [5/120    avg_loss:0.918, val_acc:0.650]
Epoch [6/120    avg_loss:0.818, val_acc:0.740]
Epoch [7/120    avg_loss:0.555, val_acc:0.818]
Epoch [8/120    avg_loss:0.533, val_acc:0.836]
Epoch [9/120    avg_loss:0.432, val_acc:0.864]
Epoch [10/120    avg_loss:0.471, val_acc:0.875]
Epoch [11/120    avg_loss:0.269, val_acc:0.895]
Epoch [12/120    avg_loss:0.230, val_acc:0.883]
Epoch [13/120    avg_loss:0.212, val_acc:0.922]
Epoch [14/120    avg_loss:0.279, val_acc:0.912]
Epoch [15/120    avg_loss:0.194, val_acc:0.883]
Epoch [16/120    avg_loss:0.248, val_acc:0.905]
Epoch [17/120    avg_loss:0.157, val_acc:0.920]
Epoch [18/120    avg_loss:0.168, val_acc:0.917]
Epoch [19/120    avg_loss:0.125, val_acc:0.936]
Epoch [20/120    avg_loss:0.105, val_acc:0.936]
Epoch [21/120    avg_loss:0.084, val_acc:0.955]
Epoch [22/120    avg_loss:0.077, val_acc:0.958]
Epoch [23/120    avg_loss:0.076, val_acc:0.960]
Epoch [24/120    avg_loss:0.084, val_acc:0.949]
Epoch [25/120    avg_loss:0.073, val_acc:0.947]
Epoch [26/120    avg_loss:0.061, val_acc:0.958]
Epoch [27/120    avg_loss:0.082, val_acc:0.931]
Epoch [28/120    avg_loss:0.087, val_acc:0.956]
Epoch [29/120    avg_loss:0.115, val_acc:0.956]
Epoch [30/120    avg_loss:0.085, val_acc:0.959]
Epoch [31/120    avg_loss:0.086, val_acc:0.954]
Epoch [32/120    avg_loss:0.055, val_acc:0.970]
Epoch [33/120    avg_loss:0.068, val_acc:0.958]
Epoch [34/120    avg_loss:0.053, val_acc:0.970]
Epoch [35/120    avg_loss:0.051, val_acc:0.972]
Epoch [36/120    avg_loss:0.036, val_acc:0.966]
Epoch [37/120    avg_loss:0.032, val_acc:0.969]
Epoch [38/120    avg_loss:0.033, val_acc:0.975]
Epoch [39/120    avg_loss:0.028, val_acc:0.977]
Epoch [40/120    avg_loss:0.030, val_acc:0.971]
Epoch [41/120    avg_loss:0.037, val_acc:0.976]
Epoch [42/120    avg_loss:0.027, val_acc:0.978]
Epoch [43/120    avg_loss:0.043, val_acc:0.968]
Epoch [44/120    avg_loss:0.038, val_acc:0.971]
Epoch [45/120    avg_loss:0.027, val_acc:0.958]
Epoch [46/120    avg_loss:0.022, val_acc:0.980]
Epoch [47/120    avg_loss:0.020, val_acc:0.973]
Epoch [48/120    avg_loss:0.019, val_acc:0.978]
Epoch [49/120    avg_loss:0.015, val_acc:0.980]
Epoch [50/120    avg_loss:0.017, val_acc:0.972]
Epoch [51/120    avg_loss:0.021, val_acc:0.983]
Epoch [52/120    avg_loss:0.031, val_acc:0.983]
Epoch [53/120    avg_loss:0.021, val_acc:0.977]
Epoch [54/120    avg_loss:0.014, val_acc:0.978]
Epoch [55/120    avg_loss:0.008, val_acc:0.985]
Epoch [56/120    avg_loss:0.016, val_acc:0.967]
Epoch [57/120    avg_loss:0.019, val_acc:0.981]
Epoch [58/120    avg_loss:0.020, val_acc:0.976]
Epoch [59/120    avg_loss:0.023, val_acc:0.976]
Epoch [60/120    avg_loss:0.027, val_acc:0.977]
Epoch [61/120    avg_loss:0.017, val_acc:0.973]
Epoch [62/120    avg_loss:0.015, val_acc:0.980]
Epoch [63/120    avg_loss:0.025, val_acc:0.968]
Epoch [64/120    avg_loss:0.014, val_acc:0.984]
Epoch [65/120    avg_loss:0.015, val_acc:0.987]
Epoch [66/120    avg_loss:0.011, val_acc:0.976]
Epoch [67/120    avg_loss:0.013, val_acc:0.981]
Epoch [68/120    avg_loss:0.023, val_acc:0.975]
Epoch [69/120    avg_loss:0.011, val_acc:0.977]
Epoch [70/120    avg_loss:0.007, val_acc:0.982]
Epoch [71/120    avg_loss:0.012, val_acc:0.981]
Epoch [72/120    avg_loss:0.018, val_acc:0.980]
Epoch [73/120    avg_loss:0.013, val_acc:0.982]
Epoch [74/120    avg_loss:0.009, val_acc:0.981]
Epoch [75/120    avg_loss:0.009, val_acc:0.981]
Epoch [76/120    avg_loss:0.014, val_acc:0.986]
Epoch [77/120    avg_loss:0.007, val_acc:0.985]
Epoch [78/120    avg_loss:0.006, val_acc:0.985]
Epoch [79/120    avg_loss:0.006, val_acc:0.985]
Epoch [80/120    avg_loss:0.005, val_acc:0.986]
Epoch [81/120    avg_loss:0.008, val_acc:0.987]
Epoch [82/120    avg_loss:0.007, val_acc:0.986]
Epoch [83/120    avg_loss:0.004, val_acc:0.984]
Epoch [84/120    avg_loss:0.004, val_acc:0.985]
Epoch [85/120    avg_loss:0.006, val_acc:0.985]
Epoch [86/120    avg_loss:0.004, val_acc:0.986]
Epoch [87/120    avg_loss:0.009, val_acc:0.985]
Epoch [88/120    avg_loss:0.005, val_acc:0.986]
Epoch [89/120    avg_loss:0.004, val_acc:0.987]
Epoch [90/120    avg_loss:0.004, val_acc:0.987]
Epoch [91/120    avg_loss:0.005, val_acc:0.987]
Epoch [92/120    avg_loss:0.004, val_acc:0.986]
Epoch [93/120    avg_loss:0.004, val_acc:0.986]
Epoch [94/120    avg_loss:0.004, val_acc:0.986]
Epoch [95/120    avg_loss:0.004, val_acc:0.985]
Epoch [96/120    avg_loss:0.004, val_acc:0.986]
Epoch [97/120    avg_loss:0.004, val_acc:0.985]
Epoch [98/120    avg_loss:0.005, val_acc:0.986]
Epoch [99/120    avg_loss:0.003, val_acc:0.986]
Epoch [100/120    avg_loss:0.006, val_acc:0.986]
Epoch [101/120    avg_loss:0.005, val_acc:0.987]
Epoch [102/120    avg_loss:0.004, val_acc:0.987]
Epoch [103/120    avg_loss:0.004, val_acc:0.987]
Epoch [104/120    avg_loss:0.005, val_acc:0.987]
Epoch [105/120    avg_loss:0.004, val_acc:0.986]
Epoch [106/120    avg_loss:0.005, val_acc:0.985]
Epoch [107/120    avg_loss:0.003, val_acc:0.985]
Epoch [108/120    avg_loss:0.008, val_acc:0.983]
Epoch [109/120    avg_loss:0.003, val_acc:0.983]
Epoch [110/120    avg_loss:0.004, val_acc:0.983]
Epoch [111/120    avg_loss:0.005, val_acc:0.984]
Epoch [112/120    avg_loss:0.005, val_acc:0.983]
Epoch [113/120    avg_loss:0.004, val_acc:0.983]
Epoch [114/120    avg_loss:0.003, val_acc:0.983]
Epoch [115/120    avg_loss:0.005, val_acc:0.984]
Epoch [116/120    avg_loss:0.004, val_acc:0.985]
Epoch [117/120    avg_loss:0.004, val_acc:0.985]
Epoch [118/120    avg_loss:0.006, val_acc:0.985]
Epoch [119/120    avg_loss:0.005, val_acc:0.986]
Epoch [120/120    avg_loss:0.002, val_acc:0.986]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1273    0    0    0    0    0    0    0    5    5    2    0
     0    0    0]
 [   0    0    0  688    0   17    0    0    0    7    0    0   32    3
     0    0    0]
 [   0    0    0    4  209    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0  870    0    0    0
     0    1    0]
 [   0    0    8    0    0    2   11    0    0    0    6 2180    2    1
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  530    0
     2    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1135    2    0]
 [   0    0    0    0    0    0    7    0    0    0    0    0    0    0
    22  318    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.26558265582656

F1 scores:
[       nan 0.98765432 0.99181924 0.95356895 0.99052133 0.96846847
 0.98572502 0.98039216 0.99883856 0.65116279 0.99032442 0.99181074
 0.96188748 0.98930481 0.98781549 0.95209581 0.97619048]

Kappa:
0.9802360035420937
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:08--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe260138710>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.381, val_acc:0.471]
Epoch [2/120    avg_loss:1.678, val_acc:0.553]
Epoch [3/120    avg_loss:1.403, val_acc:0.637]
Epoch [4/120    avg_loss:1.154, val_acc:0.680]
Epoch [5/120    avg_loss:0.865, val_acc:0.763]
Epoch [6/120    avg_loss:0.742, val_acc:0.733]
Epoch [7/120    avg_loss:0.778, val_acc:0.733]
Epoch [8/120    avg_loss:0.585, val_acc:0.749]
Epoch [9/120    avg_loss:0.506, val_acc:0.820]
Epoch [10/120    avg_loss:0.482, val_acc:0.851]
Epoch [11/120    avg_loss:0.439, val_acc:0.861]
Epoch [12/120    avg_loss:0.301, val_acc:0.908]
Epoch [13/120    avg_loss:0.237, val_acc:0.916]
Epoch [14/120    avg_loss:0.181, val_acc:0.920]
Epoch [15/120    avg_loss:0.189, val_acc:0.907]
Epoch [16/120    avg_loss:0.177, val_acc:0.908]
Epoch [17/120    avg_loss:0.161, val_acc:0.946]
Epoch [18/120    avg_loss:0.176, val_acc:0.940]
Epoch [19/120    avg_loss:0.130, val_acc:0.943]
Epoch [20/120    avg_loss:0.126, val_acc:0.923]
Epoch [21/120    avg_loss:0.099, val_acc:0.923]
Epoch [22/120    avg_loss:0.140, val_acc:0.935]
Epoch [23/120    avg_loss:0.120, val_acc:0.944]
Epoch [24/120    avg_loss:0.111, val_acc:0.957]
Epoch [25/120    avg_loss:0.098, val_acc:0.950]
Epoch [26/120    avg_loss:0.116, val_acc:0.932]
Epoch [27/120    avg_loss:0.115, val_acc:0.912]
Epoch [28/120    avg_loss:0.164, val_acc:0.946]
Epoch [29/120    avg_loss:0.092, val_acc:0.958]
Epoch [30/120    avg_loss:0.068, val_acc:0.956]
Epoch [31/120    avg_loss:0.056, val_acc:0.973]
Epoch [32/120    avg_loss:0.038, val_acc:0.971]
Epoch [33/120    avg_loss:0.037, val_acc:0.970]
Epoch [34/120    avg_loss:0.044, val_acc:0.972]
Epoch [35/120    avg_loss:0.032, val_acc:0.974]
Epoch [36/120    avg_loss:0.023, val_acc:0.973]
Epoch [37/120    avg_loss:0.038, val_acc:0.977]
Epoch [38/120    avg_loss:0.030, val_acc:0.973]
Epoch [39/120    avg_loss:0.033, val_acc:0.972]
Epoch [40/120    avg_loss:0.034, val_acc:0.971]
Epoch [41/120    avg_loss:0.025, val_acc:0.978]
Epoch [42/120    avg_loss:0.041, val_acc:0.967]
Epoch [43/120    avg_loss:0.026, val_acc:0.980]
Epoch [44/120    avg_loss:0.023, val_acc:0.982]
Epoch [45/120    avg_loss:0.032, val_acc:0.976]
Epoch [46/120    avg_loss:0.026, val_acc:0.971]
Epoch [47/120    avg_loss:0.030, val_acc:0.977]
Epoch [48/120    avg_loss:0.027, val_acc:0.964]
Epoch [49/120    avg_loss:0.042, val_acc:0.973]
Epoch [50/120    avg_loss:0.035, val_acc:0.973]
Epoch [51/120    avg_loss:0.024, val_acc:0.978]
Epoch [52/120    avg_loss:0.020, val_acc:0.974]
Epoch [53/120    avg_loss:0.016, val_acc:0.976]
Epoch [54/120    avg_loss:0.013, val_acc:0.981]
Epoch [55/120    avg_loss:0.016, val_acc:0.984]
Epoch [56/120    avg_loss:0.016, val_acc:0.981]
Epoch [57/120    avg_loss:0.043, val_acc:0.973]
Epoch [58/120    avg_loss:0.038, val_acc:0.985]
Epoch [59/120    avg_loss:0.020, val_acc:0.982]
Epoch [60/120    avg_loss:0.035, val_acc:0.972]
Epoch [61/120    avg_loss:0.032, val_acc:0.981]
Epoch [62/120    avg_loss:0.024, val_acc:0.982]
Epoch [63/120    avg_loss:0.011, val_acc:0.985]
Epoch [64/120    avg_loss:0.016, val_acc:0.977]
Epoch [65/120    avg_loss:0.015, val_acc:0.982]
Epoch [66/120    avg_loss:0.014, val_acc:0.983]
Epoch [67/120    avg_loss:0.010, val_acc:0.975]
Epoch [68/120    avg_loss:0.014, val_acc:0.984]
Epoch [69/120    avg_loss:0.010, val_acc:0.982]
Epoch [70/120    avg_loss:0.013, val_acc:0.977]
Epoch [71/120    avg_loss:0.009, val_acc:0.977]
Epoch [72/120    avg_loss:0.009, val_acc:0.985]
Epoch [73/120    avg_loss:0.009, val_acc:0.985]
Epoch [74/120    avg_loss:0.011, val_acc:0.984]
Epoch [75/120    avg_loss:0.017, val_acc:0.977]
Epoch [76/120    avg_loss:0.014, val_acc:0.978]
Epoch [77/120    avg_loss:0.014, val_acc:0.984]
Epoch [78/120    avg_loss:0.014, val_acc:0.981]
Epoch [79/120    avg_loss:0.011, val_acc:0.977]
Epoch [80/120    avg_loss:0.010, val_acc:0.983]
Epoch [81/120    avg_loss:0.007, val_acc:0.984]
Epoch [82/120    avg_loss:0.007, val_acc:0.976]
Epoch [83/120    avg_loss:0.012, val_acc:0.984]
Epoch [84/120    avg_loss:0.011, val_acc:0.975]
Epoch [85/120    avg_loss:0.009, val_acc:0.984]
Epoch [86/120    avg_loss:0.006, val_acc:0.980]
Epoch [87/120    avg_loss:0.005, val_acc:0.982]
Epoch [88/120    avg_loss:0.008, val_acc:0.984]
Epoch [89/120    avg_loss:0.008, val_acc:0.986]
Epoch [90/120    avg_loss:0.006, val_acc:0.986]
Epoch [91/120    avg_loss:0.008, val_acc:0.985]
Epoch [92/120    avg_loss:0.004, val_acc:0.985]
Epoch [93/120    avg_loss:0.006, val_acc:0.985]
Epoch [94/120    avg_loss:0.004, val_acc:0.983]
Epoch [95/120    avg_loss:0.005, val_acc:0.985]
Epoch [96/120    avg_loss:0.004, val_acc:0.984]
Epoch [97/120    avg_loss:0.004, val_acc:0.984]
Epoch [98/120    avg_loss:0.007, val_acc:0.984]
Epoch [99/120    avg_loss:0.004, val_acc:0.984]
Epoch [100/120    avg_loss:0.005, val_acc:0.984]
Epoch [101/120    avg_loss:0.005, val_acc:0.984]
Epoch [102/120    avg_loss:0.004, val_acc:0.986]
Epoch [103/120    avg_loss:0.006, val_acc:0.985]
Epoch [104/120    avg_loss:0.004, val_acc:0.985]
Epoch [105/120    avg_loss:0.005, val_acc:0.986]
Epoch [106/120    avg_loss:0.006, val_acc:0.986]
Epoch [107/120    avg_loss:0.003, val_acc:0.985]
Epoch [108/120    avg_loss:0.003, val_acc:0.987]
Epoch [109/120    avg_loss:0.004, val_acc:0.986]
Epoch [110/120    avg_loss:0.005, val_acc:0.986]
Epoch [111/120    avg_loss:0.005, val_acc:0.988]
Epoch [112/120    avg_loss:0.004, val_acc:0.988]
Epoch [113/120    avg_loss:0.005, val_acc:0.987]
Epoch [114/120    avg_loss:0.004, val_acc:0.987]
Epoch [115/120    avg_loss:0.004, val_acc:0.987]
Epoch [116/120    avg_loss:0.005, val_acc:0.987]
Epoch [117/120    avg_loss:0.005, val_acc:0.987]
Epoch [118/120    avg_loss:0.006, val_acc:0.988]
Epoch [119/120    avg_loss:0.004, val_acc:0.987]
Epoch [120/120    avg_loss:0.005, val_acc:0.987]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1270    2    0    0    0    0    0    0    6    4    3    0
     0    0    0]
 [   0    0    0  709    0   13    0    0    0    4    1    0   17    2
     1    0    0]
 [   0    0    0    4  209    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    5    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0  865    1    0    0
     0    4    0]
 [   0    0    6    0    0    0    4    0    0    0    0 2198    1    1
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    0    0  527    0
     0    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    1    0    0    0
  1132    0    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    29  302    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.33062330623306

F1 scores:
[       nan 0.98765432 0.99141296 0.96857923 0.99052133 0.95964126
 0.98424606 0.98039216 0.99883856 0.74418605 0.98970252 0.99592207
 0.97053407 0.98924731 0.98349262 0.92496172 0.96385542]

Kappa:
0.9809683640396888
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:10--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb507260710>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.473, val_acc:0.436]
Epoch [2/120    avg_loss:1.671, val_acc:0.614]
Epoch [3/120    avg_loss:1.327, val_acc:0.614]
Epoch [4/120    avg_loss:1.100, val_acc:0.661]
Epoch [5/120    avg_loss:0.952, val_acc:0.791]
Epoch [6/120    avg_loss:0.830, val_acc:0.770]
Epoch [7/120    avg_loss:0.597, val_acc:0.780]
Epoch [8/120    avg_loss:0.506, val_acc:0.849]
Epoch [9/120    avg_loss:0.548, val_acc:0.792]
Epoch [10/120    avg_loss:0.451, val_acc:0.873]
Epoch [11/120    avg_loss:0.482, val_acc:0.849]
Epoch [12/120    avg_loss:0.401, val_acc:0.874]
Epoch [13/120    avg_loss:0.252, val_acc:0.916]
Epoch [14/120    avg_loss:0.181, val_acc:0.915]
Epoch [15/120    avg_loss:0.185, val_acc:0.920]
Epoch [16/120    avg_loss:0.157, val_acc:0.927]
Epoch [17/120    avg_loss:0.093, val_acc:0.956]
Epoch [18/120    avg_loss:0.109, val_acc:0.946]
Epoch [19/120    avg_loss:0.110, val_acc:0.931]
Epoch [20/120    avg_loss:0.137, val_acc:0.934]
Epoch [21/120    avg_loss:0.097, val_acc:0.942]
Epoch [22/120    avg_loss:0.106, val_acc:0.958]
Epoch [23/120    avg_loss:0.065, val_acc:0.950]
Epoch [24/120    avg_loss:0.058, val_acc:0.966]
Epoch [25/120    avg_loss:0.067, val_acc:0.948]
Epoch [26/120    avg_loss:0.075, val_acc:0.961]
Epoch [27/120    avg_loss:0.054, val_acc:0.952]
Epoch [28/120    avg_loss:0.062, val_acc:0.936]
Epoch [29/120    avg_loss:0.042, val_acc:0.968]
Epoch [30/120    avg_loss:0.043, val_acc:0.961]
Epoch [31/120    avg_loss:0.038, val_acc:0.966]
Epoch [32/120    avg_loss:0.034, val_acc:0.963]
Epoch [33/120    avg_loss:0.033, val_acc:0.973]
Epoch [34/120    avg_loss:0.035, val_acc:0.962]
Epoch [35/120    avg_loss:0.037, val_acc:0.973]
Epoch [36/120    avg_loss:0.041, val_acc:0.968]
Epoch [37/120    avg_loss:0.048, val_acc:0.942]
Epoch [38/120    avg_loss:0.062, val_acc:0.955]
Epoch [39/120    avg_loss:0.070, val_acc:0.960]
Epoch [40/120    avg_loss:0.041, val_acc:0.972]
Epoch [41/120    avg_loss:0.034, val_acc:0.975]
Epoch [42/120    avg_loss:0.043, val_acc:0.962]
Epoch [43/120    avg_loss:0.039, val_acc:0.967]
Epoch [44/120    avg_loss:0.022, val_acc:0.978]
Epoch [45/120    avg_loss:0.041, val_acc:0.970]
Epoch [46/120    avg_loss:0.031, val_acc:0.972]
Epoch [47/120    avg_loss:0.031, val_acc:0.969]
Epoch [48/120    avg_loss:0.032, val_acc:0.970]
Epoch [49/120    avg_loss:0.024, val_acc:0.962]
Epoch [50/120    avg_loss:0.025, val_acc:0.976]
Epoch [51/120    avg_loss:0.027, val_acc:0.973]
Epoch [52/120    avg_loss:0.029, val_acc:0.974]
Epoch [53/120    avg_loss:0.024, val_acc:0.967]
Epoch [54/120    avg_loss:0.030, val_acc:0.959]
Epoch [55/120    avg_loss:0.054, val_acc:0.963]
Epoch [56/120    avg_loss:0.043, val_acc:0.976]
Epoch [57/120    avg_loss:0.113, val_acc:0.961]
Epoch [58/120    avg_loss:0.044, val_acc:0.966]
Epoch [59/120    avg_loss:0.024, val_acc:0.973]
Epoch [60/120    avg_loss:0.026, val_acc:0.972]
Epoch [61/120    avg_loss:0.019, val_acc:0.973]
Epoch [62/120    avg_loss:0.022, val_acc:0.974]
Epoch [63/120    avg_loss:0.016, val_acc:0.972]
Epoch [64/120    avg_loss:0.017, val_acc:0.974]
Epoch [65/120    avg_loss:0.017, val_acc:0.975]
Epoch [66/120    avg_loss:0.016, val_acc:0.973]
Epoch [67/120    avg_loss:0.015, val_acc:0.975]
Epoch [68/120    avg_loss:0.020, val_acc:0.976]
Epoch [69/120    avg_loss:0.021, val_acc:0.977]
Epoch [70/120    avg_loss:0.015, val_acc:0.981]
Epoch [71/120    avg_loss:0.014, val_acc:0.981]
Epoch [72/120    avg_loss:0.017, val_acc:0.980]
Epoch [73/120    avg_loss:0.011, val_acc:0.981]
Epoch [74/120    avg_loss:0.014, val_acc:0.981]
Epoch [75/120    avg_loss:0.018, val_acc:0.981]
Epoch [76/120    avg_loss:0.014, val_acc:0.981]
Epoch [77/120    avg_loss:0.018, val_acc:0.981]
Epoch [78/120    avg_loss:0.013, val_acc:0.982]
Epoch [79/120    avg_loss:0.018, val_acc:0.985]
Epoch [80/120    avg_loss:0.012, val_acc:0.985]
Epoch [81/120    avg_loss:0.012, val_acc:0.984]
Epoch [82/120    avg_loss:0.014, val_acc:0.982]
Epoch [83/120    avg_loss:0.015, val_acc:0.981]
Epoch [84/120    avg_loss:0.013, val_acc:0.985]
Epoch [85/120    avg_loss:0.010, val_acc:0.985]
Epoch [86/120    avg_loss:0.013, val_acc:0.985]
Epoch [87/120    avg_loss:0.012, val_acc:0.985]
Epoch [88/120    avg_loss:0.010, val_acc:0.985]
Epoch [89/120    avg_loss:0.011, val_acc:0.983]
Epoch [90/120    avg_loss:0.008, val_acc:0.982]
Epoch [91/120    avg_loss:0.011, val_acc:0.984]
Epoch [92/120    avg_loss:0.011, val_acc:0.983]
Epoch [93/120    avg_loss:0.010, val_acc:0.983]
Epoch [94/120    avg_loss:0.011, val_acc:0.984]
Epoch [95/120    avg_loss:0.009, val_acc:0.986]
Epoch [96/120    avg_loss:0.010, val_acc:0.986]
Epoch [97/120    avg_loss:0.012, val_acc:0.985]
Epoch [98/120    avg_loss:0.010, val_acc:0.985]
Epoch [99/120    avg_loss:0.009, val_acc:0.986]
Epoch [100/120    avg_loss:0.010, val_acc:0.986]
Epoch [101/120    avg_loss:0.013, val_acc:0.985]
Epoch [102/120    avg_loss:0.008, val_acc:0.986]
Epoch [103/120    avg_loss:0.009, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.983]
Epoch [106/120    avg_loss:0.009, val_acc:0.984]
Epoch [107/120    avg_loss:0.008, val_acc:0.984]
Epoch [108/120    avg_loss:0.009, val_acc:0.985]
Epoch [109/120    avg_loss:0.013, val_acc:0.986]
Epoch [110/120    avg_loss:0.008, val_acc:0.986]
Epoch [111/120    avg_loss:0.006, val_acc:0.986]
Epoch [112/120    avg_loss:0.013, val_acc:0.983]
Epoch [113/120    avg_loss:0.008, val_acc:0.986]
Epoch [114/120    avg_loss:0.011, val_acc:0.986]
Epoch [115/120    avg_loss:0.009, val_acc:0.985]
Epoch [116/120    avg_loss:0.009, val_acc:0.985]
Epoch [117/120    avg_loss:0.010, val_acc:0.984]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.011, val_acc:0.984]
Epoch [120/120    avg_loss:0.010, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1263    0    0    0    1    0    0    0    5    5   11    0
     0    0    0]
 [   0    0    0  701    0   21    0    0    0    6    0    0   17    2
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  429    0    0    0    1    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    3    1    0    0    0  861    9    0    0
     0    1    0]
 [   0    0    8    0    0    0   13    0    0    0    0 2187    1    1
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0    0    0  524    0
     0    2    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    7    0    0    1    0    2    0    0    0
  1127    2    0]
 [   0    0    0    0    0    0   16    0    0    0    0    0    0    0
    16  315    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    4    0
     0    0   80]]

Accuracy:
98.13550135501355

F1 scores:
[       nan 0.98765432 0.98787642 0.96756384 0.99764706 0.95555556
 0.97619048 0.98039216 0.99767442 0.7826087  0.98795181 0.99138713
 0.95970696 0.9919571  0.98773006 0.94452774 0.95238095]

Kappa:
0.9787523751421832
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:12--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f055b0a86a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.395, val_acc:0.458]
Epoch [2/120    avg_loss:1.742, val_acc:0.551]
Epoch [3/120    avg_loss:1.407, val_acc:0.644]
Epoch [4/120    avg_loss:1.076, val_acc:0.752]
Epoch [5/120    avg_loss:0.986, val_acc:0.716]
Epoch [6/120    avg_loss:0.741, val_acc:0.778]
Epoch [7/120    avg_loss:0.575, val_acc:0.836]
Epoch [8/120    avg_loss:0.456, val_acc:0.837]
Epoch [9/120    avg_loss:0.522, val_acc:0.814]
Epoch [10/120    avg_loss:0.429, val_acc:0.864]
Epoch [11/120    avg_loss:0.357, val_acc:0.876]
Epoch [12/120    avg_loss:0.269, val_acc:0.863]
Epoch [13/120    avg_loss:0.248, val_acc:0.908]
Epoch [14/120    avg_loss:0.192, val_acc:0.891]
Epoch [15/120    avg_loss:0.167, val_acc:0.895]
Epoch [16/120    avg_loss:0.169, val_acc:0.927]
Epoch [17/120    avg_loss:0.121, val_acc:0.941]
Epoch [18/120    avg_loss:0.114, val_acc:0.936]
Epoch [19/120    avg_loss:0.085, val_acc:0.950]
Epoch [20/120    avg_loss:0.115, val_acc:0.939]
Epoch [21/120    avg_loss:0.155, val_acc:0.938]
Epoch [22/120    avg_loss:0.110, val_acc:0.936]
Epoch [23/120    avg_loss:0.099, val_acc:0.953]
Epoch [24/120    avg_loss:0.067, val_acc:0.961]
Epoch [25/120    avg_loss:0.101, val_acc:0.952]
Epoch [26/120    avg_loss:0.104, val_acc:0.950]
Epoch [27/120    avg_loss:0.068, val_acc:0.931]
Epoch [28/120    avg_loss:0.066, val_acc:0.963]
Epoch [29/120    avg_loss:0.056, val_acc:0.950]
Epoch [30/120    avg_loss:0.069, val_acc:0.954]
Epoch [31/120    avg_loss:0.052, val_acc:0.955]
Epoch [32/120    avg_loss:0.048, val_acc:0.973]
Epoch [33/120    avg_loss:0.027, val_acc:0.971]
Epoch [34/120    avg_loss:0.060, val_acc:0.954]
Epoch [35/120    avg_loss:0.052, val_acc:0.973]
Epoch [36/120    avg_loss:0.076, val_acc:0.929]
Epoch [37/120    avg_loss:0.059, val_acc:0.968]
Epoch [38/120    avg_loss:0.047, val_acc:0.973]
Epoch [39/120    avg_loss:0.030, val_acc:0.974]
Epoch [40/120    avg_loss:0.032, val_acc:0.971]
Epoch [41/120    avg_loss:0.022, val_acc:0.977]
Epoch [42/120    avg_loss:0.024, val_acc:0.976]
Epoch [43/120    avg_loss:0.036, val_acc:0.969]
Epoch [44/120    avg_loss:0.035, val_acc:0.974]
Epoch [45/120    avg_loss:0.041, val_acc:0.954]
Epoch [46/120    avg_loss:0.051, val_acc:0.970]
Epoch [47/120    avg_loss:0.035, val_acc:0.974]
Epoch [48/120    avg_loss:0.035, val_acc:0.981]
Epoch [49/120    avg_loss:0.027, val_acc:0.973]
Epoch [50/120    avg_loss:0.029, val_acc:0.975]
Epoch [51/120    avg_loss:0.033, val_acc:0.969]
Epoch [52/120    avg_loss:0.029, val_acc:0.978]
Epoch [53/120    avg_loss:0.020, val_acc:0.926]
Epoch [54/120    avg_loss:0.182, val_acc:0.932]
Epoch [55/120    avg_loss:0.139, val_acc:0.961]
Epoch [56/120    avg_loss:0.262, val_acc:0.956]
Epoch [57/120    avg_loss:0.068, val_acc:0.969]
Epoch [58/120    avg_loss:0.056, val_acc:0.970]
Epoch [59/120    avg_loss:0.035, val_acc:0.962]
Epoch [60/120    avg_loss:0.027, val_acc:0.975]
Epoch [61/120    avg_loss:0.019, val_acc:0.980]
Epoch [62/120    avg_loss:0.015, val_acc:0.982]
Epoch [63/120    avg_loss:0.019, val_acc:0.981]
Epoch [64/120    avg_loss:0.016, val_acc:0.981]
Epoch [65/120    avg_loss:0.019, val_acc:0.982]
Epoch [66/120    avg_loss:0.014, val_acc:0.983]
Epoch [67/120    avg_loss:0.016, val_acc:0.982]
Epoch [68/120    avg_loss:0.010, val_acc:0.982]
Epoch [69/120    avg_loss:0.012, val_acc:0.982]
Epoch [70/120    avg_loss:0.013, val_acc:0.982]
Epoch [71/120    avg_loss:0.014, val_acc:0.983]
Epoch [72/120    avg_loss:0.013, val_acc:0.981]
Epoch [73/120    avg_loss:0.014, val_acc:0.982]
Epoch [74/120    avg_loss:0.013, val_acc:0.982]
Epoch [75/120    avg_loss:0.008, val_acc:0.982]
Epoch [76/120    avg_loss:0.009, val_acc:0.982]
Epoch [77/120    avg_loss:0.013, val_acc:0.981]
Epoch [78/120    avg_loss:0.010, val_acc:0.981]
Epoch [79/120    avg_loss:0.011, val_acc:0.981]
Epoch [80/120    avg_loss:0.011, val_acc:0.983]
Epoch [81/120    avg_loss:0.015, val_acc:0.983]
Epoch [82/120    avg_loss:0.012, val_acc:0.982]
Epoch [83/120    avg_loss:0.014, val_acc:0.982]
Epoch [84/120    avg_loss:0.012, val_acc:0.982]
Epoch [85/120    avg_loss:0.016, val_acc:0.982]
Epoch [86/120    avg_loss:0.014, val_acc:0.982]
Epoch [87/120    avg_loss:0.011, val_acc:0.982]
Epoch [88/120    avg_loss:0.010, val_acc:0.983]
Epoch [89/120    avg_loss:0.014, val_acc:0.982]
Epoch [90/120    avg_loss:0.018, val_acc:0.982]
Epoch [91/120    avg_loss:0.011, val_acc:0.981]
Epoch [92/120    avg_loss:0.010, val_acc:0.982]
Epoch [93/120    avg_loss:0.012, val_acc:0.981]
Epoch [94/120    avg_loss:0.015, val_acc:0.981]
Epoch [95/120    avg_loss:0.008, val_acc:0.981]
Epoch [96/120    avg_loss:0.013, val_acc:0.980]
Epoch [97/120    avg_loss:0.012, val_acc:0.981]
Epoch [98/120    avg_loss:0.014, val_acc:0.981]
Epoch [99/120    avg_loss:0.013, val_acc:0.981]
Epoch [100/120    avg_loss:0.011, val_acc:0.980]
Epoch [101/120    avg_loss:0.010, val_acc:0.982]
Epoch [102/120    avg_loss:0.010, val_acc:0.982]
Epoch [103/120    avg_loss:0.009, val_acc:0.982]
Epoch [104/120    avg_loss:0.008, val_acc:0.982]
Epoch [105/120    avg_loss:0.012, val_acc:0.982]
Epoch [106/120    avg_loss:0.010, val_acc:0.981]
Epoch [107/120    avg_loss:0.013, val_acc:0.981]
Epoch [108/120    avg_loss:0.009, val_acc:0.981]
Epoch [109/120    avg_loss:0.009, val_acc:0.982]
Epoch [110/120    avg_loss:0.009, val_acc:0.982]
Epoch [111/120    avg_loss:0.009, val_acc:0.981]
Epoch [112/120    avg_loss:0.010, val_acc:0.981]
Epoch [113/120    avg_loss:0.008, val_acc:0.981]
Epoch [114/120    avg_loss:0.014, val_acc:0.982]
Epoch [115/120    avg_loss:0.009, val_acc:0.982]
Epoch [116/120    avg_loss:0.011, val_acc:0.982]
Epoch [117/120    avg_loss:0.010, val_acc:0.982]
Epoch [118/120    avg_loss:0.010, val_acc:0.982]
Epoch [119/120    avg_loss:0.009, val_acc:0.982]
Epoch [120/120    avg_loss:0.006, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    0    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    0 1268    0    0    0    1    0    0    0    5    8    3    0
     0    0    0]
 [   0    0    0  696    0   16    0    0    0    7    1    0   24    3
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    1    0    3    0    1    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    4    0    0    7    0    0    0    0  858    4    0    0
     0    2    0]
 [   0    0    7    0    0    0    4    0    0    0    1 2194    2    2
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    0    2  526    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    2    0    0    0
  1134    3    0]
 [   0    0    0    0    0    0    6    0    0    0    0    0    0    0
    29  312    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.25474254742548

F1 scores:
[       nan 0.98765432 0.98907956 0.9633218  1.         0.96404494
 0.99017385 0.98039216 1.         0.72727273 0.98450947 0.99253563
 0.96513761 0.98666667 0.98480243 0.93975904 0.97647059]

Kappa:
0.9801031683992083
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:14--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8f9d5f8668>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.387, val_acc:0.467]
Epoch [2/120    avg_loss:1.806, val_acc:0.573]
Epoch [3/120    avg_loss:1.454, val_acc:0.668]
Epoch [4/120    avg_loss:1.265, val_acc:0.733]
Epoch [5/120    avg_loss:0.959, val_acc:0.780]
Epoch [6/120    avg_loss:0.784, val_acc:0.786]
Epoch [7/120    avg_loss:0.597, val_acc:0.819]
Epoch [8/120    avg_loss:0.584, val_acc:0.832]
Epoch [9/120    avg_loss:0.478, val_acc:0.776]
Epoch [10/120    avg_loss:0.406, val_acc:0.865]
Epoch [11/120    avg_loss:0.346, val_acc:0.890]
Epoch [12/120    avg_loss:0.263, val_acc:0.892]
Epoch [13/120    avg_loss:0.233, val_acc:0.900]
Epoch [14/120    avg_loss:0.233, val_acc:0.893]
Epoch [15/120    avg_loss:0.162, val_acc:0.901]
Epoch [16/120    avg_loss:0.208, val_acc:0.897]
Epoch [17/120    avg_loss:0.215, val_acc:0.917]
Epoch [18/120    avg_loss:0.130, val_acc:0.935]
Epoch [19/120    avg_loss:0.116, val_acc:0.961]
Epoch [20/120    avg_loss:0.092, val_acc:0.941]
Epoch [21/120    avg_loss:0.073, val_acc:0.944]
Epoch [22/120    avg_loss:0.064, val_acc:0.953]
Epoch [23/120    avg_loss:0.067, val_acc:0.956]
Epoch [24/120    avg_loss:0.052, val_acc:0.967]
Epoch [25/120    avg_loss:0.057, val_acc:0.949]
Epoch [26/120    avg_loss:0.047, val_acc:0.961]
Epoch [27/120    avg_loss:0.038, val_acc:0.967]
Epoch [28/120    avg_loss:0.044, val_acc:0.972]
Epoch [29/120    avg_loss:0.039, val_acc:0.962]
Epoch [30/120    avg_loss:0.049, val_acc:0.962]
Epoch [31/120    avg_loss:0.062, val_acc:0.949]
Epoch [32/120    avg_loss:0.114, val_acc:0.955]
Epoch [33/120    avg_loss:0.159, val_acc:0.936]
Epoch [34/120    avg_loss:0.114, val_acc:0.958]
Epoch [35/120    avg_loss:0.051, val_acc:0.969]
Epoch [36/120    avg_loss:0.052, val_acc:0.970]
Epoch [37/120    avg_loss:0.037, val_acc:0.971]
Epoch [38/120    avg_loss:0.027, val_acc:0.973]
Epoch [39/120    avg_loss:0.026, val_acc:0.978]
Epoch [40/120    avg_loss:0.037, val_acc:0.982]
Epoch [41/120    avg_loss:0.038, val_acc:0.969]
Epoch [42/120    avg_loss:0.024, val_acc:0.973]
Epoch [43/120    avg_loss:0.017, val_acc:0.977]
Epoch [44/120    avg_loss:0.028, val_acc:0.973]
Epoch [45/120    avg_loss:0.025, val_acc:0.975]
Epoch [46/120    avg_loss:0.028, val_acc:0.980]
Epoch [47/120    avg_loss:0.026, val_acc:0.983]
Epoch [48/120    avg_loss:0.028, val_acc:0.987]
Epoch [49/120    avg_loss:0.029, val_acc:0.980]
Epoch [50/120    avg_loss:0.018, val_acc:0.980]
Epoch [51/120    avg_loss:0.019, val_acc:0.975]
Epoch [52/120    avg_loss:0.023, val_acc:0.974]
Epoch [53/120    avg_loss:0.018, val_acc:0.985]
Epoch [54/120    avg_loss:0.014, val_acc:0.987]
Epoch [55/120    avg_loss:0.013, val_acc:0.978]
Epoch [56/120    avg_loss:0.011, val_acc:0.978]
Epoch [57/120    avg_loss:0.016, val_acc:0.976]
Epoch [58/120    avg_loss:0.030, val_acc:0.980]
Epoch [59/120    avg_loss:0.030, val_acc:0.966]
Epoch [60/120    avg_loss:0.037, val_acc:0.968]
Epoch [61/120    avg_loss:0.027, val_acc:0.961]
Epoch [62/120    avg_loss:0.019, val_acc:0.980]
Epoch [63/120    avg_loss:0.012, val_acc:0.981]
Epoch [64/120    avg_loss:0.015, val_acc:0.980]
Epoch [65/120    avg_loss:0.022, val_acc:0.960]
Epoch [66/120    avg_loss:0.024, val_acc:0.977]
Epoch [67/120    avg_loss:0.019, val_acc:0.976]
Epoch [68/120    avg_loss:0.013, val_acc:0.980]
Epoch [69/120    avg_loss:0.013, val_acc:0.983]
Epoch [70/120    avg_loss:0.009, val_acc:0.984]
Epoch [71/120    avg_loss:0.007, val_acc:0.984]
Epoch [72/120    avg_loss:0.007, val_acc:0.986]
Epoch [73/120    avg_loss:0.009, val_acc:0.985]
Epoch [74/120    avg_loss:0.008, val_acc:0.986]
Epoch [75/120    avg_loss:0.007, val_acc:0.986]
Epoch [76/120    avg_loss:0.007, val_acc:0.986]
Epoch [77/120    avg_loss:0.008, val_acc:0.986]
Epoch [78/120    avg_loss:0.008, val_acc:0.986]
Epoch [79/120    avg_loss:0.006, val_acc:0.985]
Epoch [80/120    avg_loss:0.006, val_acc:0.985]
Epoch [81/120    avg_loss:0.005, val_acc:0.985]
Epoch [82/120    avg_loss:0.006, val_acc:0.985]
Epoch [83/120    avg_loss:0.007, val_acc:0.985]
Epoch [84/120    avg_loss:0.006, val_acc:0.985]
Epoch [85/120    avg_loss:0.004, val_acc:0.985]
Epoch [86/120    avg_loss:0.006, val_acc:0.985]
Epoch [87/120    avg_loss:0.011, val_acc:0.985]
Epoch [88/120    avg_loss:0.006, val_acc:0.985]
Epoch [89/120    avg_loss:0.007, val_acc:0.985]
Epoch [90/120    avg_loss:0.009, val_acc:0.985]
Epoch [91/120    avg_loss:0.005, val_acc:0.985]
Epoch [92/120    avg_loss:0.005, val_acc:0.985]
Epoch [93/120    avg_loss:0.005, val_acc:0.985]
Epoch [94/120    avg_loss:0.006, val_acc:0.985]
Epoch [95/120    avg_loss:0.010, val_acc:0.985]
Epoch [96/120    avg_loss:0.006, val_acc:0.985]
Epoch [97/120    avg_loss:0.006, val_acc:0.985]
Epoch [98/120    avg_loss:0.006, val_acc:0.985]
Epoch [99/120    avg_loss:0.007, val_acc:0.985]
Epoch [100/120    avg_loss:0.006, val_acc:0.985]
Epoch [101/120    avg_loss:0.008, val_acc:0.985]
Epoch [102/120    avg_loss:0.010, val_acc:0.985]
Epoch [103/120    avg_loss:0.007, val_acc:0.985]
Epoch [104/120    avg_loss:0.009, val_acc:0.985]
Epoch [105/120    avg_loss:0.006, val_acc:0.985]
Epoch [106/120    avg_loss:0.006, val_acc:0.985]
Epoch [107/120    avg_loss:0.009, val_acc:0.985]
Epoch [108/120    avg_loss:0.005, val_acc:0.985]
Epoch [109/120    avg_loss:0.008, val_acc:0.985]
Epoch [110/120    avg_loss:0.007, val_acc:0.985]
Epoch [111/120    avg_loss:0.006, val_acc:0.985]
Epoch [112/120    avg_loss:0.006, val_acc:0.985]
Epoch [113/120    avg_loss:0.008, val_acc:0.985]
Epoch [114/120    avg_loss:0.012, val_acc:0.985]
Epoch [115/120    avg_loss:0.007, val_acc:0.985]
Epoch [116/120    avg_loss:0.008, val_acc:0.985]
Epoch [117/120    avg_loss:0.009, val_acc:0.985]
Epoch [118/120    avg_loss:0.007, val_acc:0.985]
Epoch [119/120    avg_loss:0.005, val_acc:0.985]
Epoch [120/120    avg_loss:0.007, val_acc:0.985]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    3    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1268    0    0    0    1    0    0    0    5    4    4    0
     0    3    0]
 [   0    0    0  693    0   22    0    0    0    5    1    0   23    3
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  427    0    0    0    3    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0  856   11    2    0
     1    2    0]
 [   0    0   15    0    0    0    4    0    0    0    4 2182    3    2
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0  532    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    0    0    2    0    0    0
  1132    0    0]
 [   0    0    0    0    0    0   28    0    0    0    0    0    0    0
    12  307    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    3    0
     0    0   81]]

Accuracy:
97.98373983739837

F1 scores:
[       nan 0.96202532 0.98638662 0.95916955 0.99764706 0.9632107
 0.9739777  0.98039216 0.99649942 0.71794872 0.98221457 0.9897936
 0.96376812 0.98666667 0.99124343 0.93171472 0.97005988]

Kappa:
0.9770190945696035
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:16--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:29
Validation dataloader:29
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:13
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f586f889710>
supervision:full
center_pixel:True
Network :
Number of parameter: 304033==>0.30M
----------Training process----------
Epoch [1/120    avg_loss:2.336, val_acc:0.488]
Epoch [2/120    avg_loss:1.726, val_acc:0.590]
Epoch [3/120    avg_loss:1.470, val_acc:0.679]
Epoch [4/120    avg_loss:1.102, val_acc:0.703]
Epoch [5/120    avg_loss:0.806, val_acc:0.761]
Epoch [6/120    avg_loss:0.869, val_acc:0.778]
Epoch [7/120    avg_loss:0.658, val_acc:0.789]
Epoch [8/120    avg_loss:0.561, val_acc:0.814]
Epoch [9/120    avg_loss:0.446, val_acc:0.865]
Epoch [10/120    avg_loss:0.519, val_acc:0.840]
Epoch [11/120    avg_loss:0.413, val_acc:0.850]
Epoch [12/120    avg_loss:0.478, val_acc:0.844]
Epoch [13/120    avg_loss:0.478, val_acc:0.885]
Epoch [14/120    avg_loss:0.306, val_acc:0.874]
Epoch [15/120    avg_loss:0.209, val_acc:0.908]
Epoch [16/120    avg_loss:0.162, val_acc:0.930]
Epoch [17/120    avg_loss:0.160, val_acc:0.914]
Epoch [18/120    avg_loss:0.251, val_acc:0.885]
Epoch [19/120    avg_loss:0.143, val_acc:0.943]
Epoch [20/120    avg_loss:0.110, val_acc:0.933]
Epoch [21/120    avg_loss:0.095, val_acc:0.944]
Epoch [22/120    avg_loss:0.099, val_acc:0.961]
Epoch [23/120    avg_loss:0.091, val_acc:0.932]
Epoch [24/120    avg_loss:0.090, val_acc:0.944]
Epoch [25/120    avg_loss:0.059, val_acc:0.944]
Epoch [26/120    avg_loss:0.073, val_acc:0.960]
Epoch [27/120    avg_loss:0.078, val_acc:0.949]
Epoch [28/120    avg_loss:0.071, val_acc:0.964]
Epoch [29/120    avg_loss:0.063, val_acc:0.945]
Epoch [30/120    avg_loss:0.066, val_acc:0.953]
Epoch [31/120    avg_loss:0.061, val_acc:0.934]
Epoch [32/120    avg_loss:0.065, val_acc:0.965]
Epoch [33/120    avg_loss:0.039, val_acc:0.961]
Epoch [34/120    avg_loss:0.061, val_acc:0.954]
Epoch [35/120    avg_loss:0.060, val_acc:0.956]
Epoch [36/120    avg_loss:0.040, val_acc:0.967]
Epoch [37/120    avg_loss:0.052, val_acc:0.971]
Epoch [38/120    avg_loss:0.055, val_acc:0.961]
Epoch [39/120    avg_loss:0.047, val_acc:0.960]
Epoch [40/120    avg_loss:0.032, val_acc:0.973]
Epoch [41/120    avg_loss:0.046, val_acc:0.962]
Epoch [42/120    avg_loss:0.035, val_acc:0.975]
Epoch [43/120    avg_loss:0.025, val_acc:0.972]
Epoch [44/120    avg_loss:0.023, val_acc:0.972]
Epoch [45/120    avg_loss:0.026, val_acc:0.968]
Epoch [46/120    avg_loss:0.018, val_acc:0.983]
Epoch [47/120    avg_loss:0.019, val_acc:0.981]
Epoch [48/120    avg_loss:0.020, val_acc:0.979]
Epoch [49/120    avg_loss:0.034, val_acc:0.977]
Epoch [50/120    avg_loss:0.042, val_acc:0.980]
Epoch [51/120    avg_loss:0.021, val_acc:0.984]
Epoch [52/120    avg_loss:0.021, val_acc:0.982]
Epoch [53/120    avg_loss:0.012, val_acc:0.983]
Epoch [54/120    avg_loss:0.010, val_acc:0.982]
Epoch [55/120    avg_loss:0.013, val_acc:0.983]
Epoch [56/120    avg_loss:0.016, val_acc:0.978]
Epoch [57/120    avg_loss:0.017, val_acc:0.981]
Epoch [58/120    avg_loss:0.014, val_acc:0.972]
Epoch [59/120    avg_loss:0.013, val_acc:0.984]
Epoch [60/120    avg_loss:0.028, val_acc:0.973]
Epoch [61/120    avg_loss:0.029, val_acc:0.969]
Epoch [62/120    avg_loss:0.022, val_acc:0.982]
Epoch [63/120    avg_loss:0.012, val_acc:0.984]
Epoch [64/120    avg_loss:0.014, val_acc:0.985]
Epoch [65/120    avg_loss:0.014, val_acc:0.978]
Epoch [66/120    avg_loss:0.016, val_acc:0.987]
Epoch [67/120    avg_loss:0.011, val_acc:0.984]
Epoch [68/120    avg_loss:0.008, val_acc:0.985]
Epoch [69/120    avg_loss:0.009, val_acc:0.985]
Epoch [70/120    avg_loss:0.012, val_acc:0.971]
Epoch [71/120    avg_loss:0.013, val_acc:0.978]
Epoch [72/120    avg_loss:0.010, val_acc:0.980]
Epoch [73/120    avg_loss:0.009, val_acc:0.983]
Epoch [74/120    avg_loss:0.010, val_acc:0.981]
Epoch [75/120    avg_loss:0.007, val_acc:0.982]
Epoch [76/120    avg_loss:0.008, val_acc:0.980]
Epoch [77/120    avg_loss:0.008, val_acc:0.987]
Epoch [78/120    avg_loss:0.012, val_acc:0.985]
Epoch [79/120    avg_loss:0.022, val_acc:0.981]
Epoch [80/120    avg_loss:0.056, val_acc:0.978]
Epoch [81/120    avg_loss:0.029, val_acc:0.967]
Epoch [82/120    avg_loss:0.033, val_acc:0.977]
Epoch [83/120    avg_loss:0.021, val_acc:0.982]
Epoch [84/120    avg_loss:0.013, val_acc:0.978]
Epoch [85/120    avg_loss:0.018, val_acc:0.973]
Epoch [86/120    avg_loss:0.015, val_acc:0.984]
Epoch [87/120    avg_loss:0.008, val_acc:0.984]
Epoch [88/120    avg_loss:0.006, val_acc:0.980]
Epoch [89/120    avg_loss:0.007, val_acc:0.987]
Epoch [90/120    avg_loss:0.008, val_acc:0.984]
Epoch [91/120    avg_loss:0.008, val_acc:0.985]
Epoch [92/120    avg_loss:0.011, val_acc:0.979]
Epoch [93/120    avg_loss:0.009, val_acc:0.984]
Epoch [94/120    avg_loss:0.008, val_acc:0.984]
Epoch [95/120    avg_loss:0.007, val_acc:0.985]
Epoch [96/120    avg_loss:0.006, val_acc:0.987]
Epoch [97/120    avg_loss:0.005, val_acc:0.981]
Epoch [98/120    avg_loss:0.004, val_acc:0.983]
Epoch [99/120    avg_loss:0.006, val_acc:0.983]
Epoch [100/120    avg_loss:0.008, val_acc:0.987]
Epoch [101/120    avg_loss:0.007, val_acc:0.984]
Epoch [102/120    avg_loss:0.007, val_acc:0.980]
Epoch [103/120    avg_loss:0.006, val_acc:0.989]
Epoch [104/120    avg_loss:0.006, val_acc:0.985]
Epoch [105/120    avg_loss:0.010, val_acc:0.983]
Epoch [106/120    avg_loss:0.005, val_acc:0.990]
Epoch [107/120    avg_loss:0.004, val_acc:0.984]
Epoch [108/120    avg_loss:0.004, val_acc:0.985]
Epoch [109/120    avg_loss:0.005, val_acc:0.987]
Epoch [110/120    avg_loss:0.004, val_acc:0.987]
Epoch [111/120    avg_loss:0.004, val_acc:0.987]
Epoch [112/120    avg_loss:0.003, val_acc:0.987]
Epoch [113/120    avg_loss:0.005, val_acc:0.985]
Epoch [114/120    avg_loss:0.005, val_acc:0.987]
Epoch [115/120    avg_loss:0.007, val_acc:0.988]
Epoch [116/120    avg_loss:0.011, val_acc:0.981]
Epoch [117/120    avg_loss:0.006, val_acc:0.990]
Epoch [118/120    avg_loss:0.004, val_acc:0.987]
Epoch [119/120    avg_loss:0.006, val_acc:0.984]
Epoch [120/120    avg_loss:0.004, val_acc:0.989]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    3    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1271    0    1    0    1    0    0    0    6    6    0    0
     0    0    0]
 [   0    0    0  727    2   11    0    0    0    4    0    0    1    2
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    0    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   16    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    4    0    0    0    0  864    6    0    0
     0    1    0]
 [   0    0    9    0    0    2    0    0    0    0    6 2191    0    2
     0    0    0]
 [   0    0    0    0    5    1    0    0    0    0    0    0  524    0
     0    1    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    1    0    1    0    0    0
  1133    0    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    45  292    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.38482384823848

F1 scores:
[       nan 0.96202532 0.98987539 0.98376184 0.97685185 0.97072072
 0.99093656 1.         0.99883856 0.76190476 0.98630137 0.99275034
 0.98774741 0.98930481 0.97798878 0.91107644 0.9704142 ]

Kappa:
0.9815822740363332
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:18--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fb4347ba6d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.509, val_acc:0.471]
Epoch [2/120    avg_loss:1.711, val_acc:0.542]
Epoch [3/120    avg_loss:1.266, val_acc:0.711]
Epoch [4/120    avg_loss:1.141, val_acc:0.727]
Epoch [5/120    avg_loss:0.861, val_acc:0.783]
Epoch [6/120    avg_loss:0.665, val_acc:0.840]
Epoch [7/120    avg_loss:0.599, val_acc:0.817]
Epoch [8/120    avg_loss:0.724, val_acc:0.817]
Epoch [9/120    avg_loss:0.451, val_acc:0.869]
Epoch [10/120    avg_loss:0.326, val_acc:0.886]
Epoch [11/120    avg_loss:0.567, val_acc:0.869]
Epoch [12/120    avg_loss:0.418, val_acc:0.885]
Epoch [13/120    avg_loss:0.261, val_acc:0.921]
Epoch [14/120    avg_loss:0.241, val_acc:0.913]
Epoch [15/120    avg_loss:0.249, val_acc:0.925]
Epoch [16/120    avg_loss:0.221, val_acc:0.911]
Epoch [17/120    avg_loss:0.190, val_acc:0.938]
Epoch [18/120    avg_loss:0.197, val_acc:0.890]
Epoch [19/120    avg_loss:0.204, val_acc:0.950]
Epoch [20/120    avg_loss:0.201, val_acc:0.938]
Epoch [21/120    avg_loss:0.152, val_acc:0.949]
Epoch [22/120    avg_loss:0.147, val_acc:0.921]
Epoch [23/120    avg_loss:0.191, val_acc:0.929]
Epoch [24/120    avg_loss:0.168, val_acc:0.949]
Epoch [25/120    avg_loss:0.107, val_acc:0.956]
Epoch [26/120    avg_loss:0.100, val_acc:0.969]
Epoch [27/120    avg_loss:0.082, val_acc:0.964]
Epoch [28/120    avg_loss:0.074, val_acc:0.963]
Epoch [29/120    avg_loss:0.078, val_acc:0.960]
Epoch [30/120    avg_loss:0.080, val_acc:0.971]
Epoch [31/120    avg_loss:0.055, val_acc:0.960]
Epoch [32/120    avg_loss:0.057, val_acc:0.965]
Epoch [33/120    avg_loss:0.054, val_acc:0.971]
Epoch [34/120    avg_loss:0.072, val_acc:0.973]
Epoch [35/120    avg_loss:0.061, val_acc:0.970]
Epoch [36/120    avg_loss:0.064, val_acc:0.967]
Epoch [37/120    avg_loss:0.066, val_acc:0.963]
Epoch [38/120    avg_loss:0.051, val_acc:0.953]
Epoch [39/120    avg_loss:0.097, val_acc:0.950]
Epoch [40/120    avg_loss:0.121, val_acc:0.952]
Epoch [41/120    avg_loss:0.054, val_acc:0.963]
Epoch [42/120    avg_loss:0.064, val_acc:0.961]
Epoch [43/120    avg_loss:0.081, val_acc:0.961]
Epoch [44/120    avg_loss:0.041, val_acc:0.970]
Epoch [45/120    avg_loss:0.045, val_acc:0.974]
Epoch [46/120    avg_loss:0.060, val_acc:0.964]
Epoch [47/120    avg_loss:0.045, val_acc:0.961]
Epoch [48/120    avg_loss:0.053, val_acc:0.980]
Epoch [49/120    avg_loss:0.024, val_acc:0.978]
Epoch [50/120    avg_loss:0.026, val_acc:0.974]
Epoch [51/120    avg_loss:0.026, val_acc:0.974]
Epoch [52/120    avg_loss:0.035, val_acc:0.979]
Epoch [53/120    avg_loss:0.027, val_acc:0.971]
Epoch [54/120    avg_loss:0.027, val_acc:0.979]
Epoch [55/120    avg_loss:0.033, val_acc:0.974]
Epoch [56/120    avg_loss:0.034, val_acc:0.967]
Epoch [57/120    avg_loss:0.034, val_acc:0.978]
Epoch [58/120    avg_loss:0.020, val_acc:0.982]
Epoch [59/120    avg_loss:0.014, val_acc:0.982]
Epoch [60/120    avg_loss:0.022, val_acc:0.979]
Epoch [61/120    avg_loss:0.016, val_acc:0.978]
Epoch [62/120    avg_loss:0.012, val_acc:0.981]
Epoch [63/120    avg_loss:0.012, val_acc:0.984]
Epoch [64/120    avg_loss:0.011, val_acc:0.988]
Epoch [65/120    avg_loss:0.017, val_acc:0.985]
Epoch [66/120    avg_loss:0.020, val_acc:0.987]
Epoch [67/120    avg_loss:0.010, val_acc:0.983]
Epoch [68/120    avg_loss:0.013, val_acc:0.979]
Epoch [69/120    avg_loss:0.019, val_acc:0.964]
Epoch [70/120    avg_loss:0.032, val_acc:0.977]
Epoch [71/120    avg_loss:0.032, val_acc:0.974]
Epoch [72/120    avg_loss:0.027, val_acc:0.978]
Epoch [73/120    avg_loss:0.031, val_acc:0.981]
Epoch [74/120    avg_loss:0.027, val_acc:0.979]
Epoch [75/120    avg_loss:0.017, val_acc:0.980]
Epoch [76/120    avg_loss:0.016, val_acc:0.974]
Epoch [77/120    avg_loss:0.018, val_acc:0.983]
Epoch [78/120    avg_loss:0.014, val_acc:0.982]
Epoch [79/120    avg_loss:0.009, val_acc:0.982]
Epoch [80/120    avg_loss:0.006, val_acc:0.983]
Epoch [81/120    avg_loss:0.008, val_acc:0.984]
Epoch [82/120    avg_loss:0.007, val_acc:0.984]
Epoch [83/120    avg_loss:0.009, val_acc:0.984]
Epoch [84/120    avg_loss:0.008, val_acc:0.984]
Epoch [85/120    avg_loss:0.010, val_acc:0.984]
Epoch [86/120    avg_loss:0.010, val_acc:0.984]
Epoch [87/120    avg_loss:0.006, val_acc:0.984]
Epoch [88/120    avg_loss:0.006, val_acc:0.984]
Epoch [89/120    avg_loss:0.008, val_acc:0.984]
Epoch [90/120    avg_loss:0.009, val_acc:0.984]
Epoch [91/120    avg_loss:0.009, val_acc:0.984]
Epoch [92/120    avg_loss:0.008, val_acc:0.984]
Epoch [93/120    avg_loss:0.008, val_acc:0.984]
Epoch [94/120    avg_loss:0.009, val_acc:0.984]
Epoch [95/120    avg_loss:0.008, val_acc:0.984]
Epoch [96/120    avg_loss:0.006, val_acc:0.984]
Epoch [97/120    avg_loss:0.009, val_acc:0.984]
Epoch [98/120    avg_loss:0.006, val_acc:0.984]
Epoch [99/120    avg_loss:0.008, val_acc:0.984]
Epoch [100/120    avg_loss:0.007, val_acc:0.984]
Epoch [101/120    avg_loss:0.009, val_acc:0.984]
Epoch [102/120    avg_loss:0.008, val_acc:0.985]
Epoch [103/120    avg_loss:0.007, val_acc:0.984]
Epoch [104/120    avg_loss:0.009, val_acc:0.984]
Epoch [105/120    avg_loss:0.008, val_acc:0.984]
Epoch [106/120    avg_loss:0.007, val_acc:0.984]
Epoch [107/120    avg_loss:0.010, val_acc:0.984]
Epoch [108/120    avg_loss:0.006, val_acc:0.984]
Epoch [109/120    avg_loss:0.009, val_acc:0.984]
Epoch [110/120    avg_loss:0.010, val_acc:0.984]
Epoch [111/120    avg_loss:0.006, val_acc:0.984]
Epoch [112/120    avg_loss:0.007, val_acc:0.984]
Epoch [113/120    avg_loss:0.007, val_acc:0.984]
Epoch [114/120    avg_loss:0.007, val_acc:0.984]
Epoch [115/120    avg_loss:0.009, val_acc:0.984]
Epoch [116/120    avg_loss:0.010, val_acc:0.984]
Epoch [117/120    avg_loss:0.007, val_acc:0.984]
Epoch [118/120    avg_loss:0.009, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.984]
Epoch [120/120    avg_loss:0.006, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1257    1    7    0    1    0    0    0    9    7    0    0
     0    3    0]
 [   0    0    1  716    0   20    0    0    0    6    0    0    4    0
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    4    1    0    4    0    0    0    0  845   17    0    0
     0    4    0]
 [   0    0    6    0    0    2    6    0    0    0   14 2181    0    1
     0    0    0]
 [   0    0    0    9    2    6    0    0    0    0    2   17  492    0
     1    1    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    1    0    0    0    0    2    0    0    0
  1130    6    0]
 [   0    0    0    0    0    0   10    0    0    0    0    0    0    0
    28  309    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.6260162601626

F1 scores:
[       nan 0.975      0.98472385 0.96887686 0.97695853 0.95884316
 0.98646617 0.98039216 1.         0.71428571 0.96626644 0.98398376
 0.95441319 0.99730458 0.98346388 0.92238806 0.97076023]

Kappa:
0.9729293268205514
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:21--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f8e8b951710>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.451, val_acc:0.482]
Epoch [2/120    avg_loss:1.787, val_acc:0.604]
Epoch [3/120    avg_loss:1.439, val_acc:0.607]
Epoch [4/120    avg_loss:1.247, val_acc:0.699]
Epoch [5/120    avg_loss:0.973, val_acc:0.744]
Epoch [6/120    avg_loss:0.798, val_acc:0.766]
Epoch [7/120    avg_loss:0.793, val_acc:0.786]
Epoch [8/120    avg_loss:0.658, val_acc:0.807]
Epoch [9/120    avg_loss:0.582, val_acc:0.839]
Epoch [10/120    avg_loss:0.475, val_acc:0.867]
Epoch [11/120    avg_loss:0.463, val_acc:0.790]
Epoch [12/120    avg_loss:0.403, val_acc:0.873]
Epoch [13/120    avg_loss:0.362, val_acc:0.858]
Epoch [14/120    avg_loss:0.307, val_acc:0.906]
Epoch [15/120    avg_loss:0.262, val_acc:0.891]
Epoch [16/120    avg_loss:0.179, val_acc:0.922]
Epoch [17/120    avg_loss:0.190, val_acc:0.921]
Epoch [18/120    avg_loss:0.165, val_acc:0.922]
Epoch [19/120    avg_loss:0.114, val_acc:0.921]
Epoch [20/120    avg_loss:0.117, val_acc:0.939]
Epoch [21/120    avg_loss:0.114, val_acc:0.929]
Epoch [22/120    avg_loss:0.118, val_acc:0.941]
Epoch [23/120    avg_loss:0.128, val_acc:0.936]
Epoch [24/120    avg_loss:0.099, val_acc:0.940]
Epoch [25/120    avg_loss:0.090, val_acc:0.956]
Epoch [26/120    avg_loss:0.083, val_acc:0.949]
Epoch [27/120    avg_loss:0.092, val_acc:0.949]
Epoch [28/120    avg_loss:0.141, val_acc:0.907]
Epoch [29/120    avg_loss:0.217, val_acc:0.932]
Epoch [30/120    avg_loss:0.126, val_acc:0.950]
Epoch [31/120    avg_loss:0.082, val_acc:0.949]
Epoch [32/120    avg_loss:0.075, val_acc:0.959]
Epoch [33/120    avg_loss:0.058, val_acc:0.960]
Epoch [34/120    avg_loss:0.051, val_acc:0.958]
Epoch [35/120    avg_loss:0.058, val_acc:0.953]
Epoch [36/120    avg_loss:0.058, val_acc:0.960]
Epoch [37/120    avg_loss:0.079, val_acc:0.944]
Epoch [38/120    avg_loss:0.046, val_acc:0.964]
Epoch [39/120    avg_loss:0.048, val_acc:0.962]
Epoch [40/120    avg_loss:0.037, val_acc:0.979]
Epoch [41/120    avg_loss:0.040, val_acc:0.965]
Epoch [42/120    avg_loss:0.058, val_acc:0.953]
Epoch [43/120    avg_loss:0.065, val_acc:0.929]
Epoch [44/120    avg_loss:0.062, val_acc:0.971]
Epoch [45/120    avg_loss:0.038, val_acc:0.972]
Epoch [46/120    avg_loss:0.039, val_acc:0.971]
Epoch [47/120    avg_loss:0.051, val_acc:0.955]
Epoch [48/120    avg_loss:0.057, val_acc:0.965]
Epoch [49/120    avg_loss:0.061, val_acc:0.964]
Epoch [50/120    avg_loss:0.053, val_acc:0.961]
Epoch [51/120    avg_loss:0.061, val_acc:0.964]
Epoch [52/120    avg_loss:0.040, val_acc:0.961]
Epoch [53/120    avg_loss:0.024, val_acc:0.970]
Epoch [54/120    avg_loss:0.021, val_acc:0.971]
Epoch [55/120    avg_loss:0.020, val_acc:0.971]
Epoch [56/120    avg_loss:0.019, val_acc:0.975]
Epoch [57/120    avg_loss:0.020, val_acc:0.979]
Epoch [58/120    avg_loss:0.018, val_acc:0.978]
Epoch [59/120    avg_loss:0.019, val_acc:0.974]
Epoch [60/120    avg_loss:0.017, val_acc:0.975]
Epoch [61/120    avg_loss:0.019, val_acc:0.980]
Epoch [62/120    avg_loss:0.022, val_acc:0.980]
Epoch [63/120    avg_loss:0.014, val_acc:0.981]
Epoch [64/120    avg_loss:0.017, val_acc:0.980]
Epoch [65/120    avg_loss:0.016, val_acc:0.978]
Epoch [66/120    avg_loss:0.018, val_acc:0.982]
Epoch [67/120    avg_loss:0.021, val_acc:0.981]
Epoch [68/120    avg_loss:0.018, val_acc:0.981]
Epoch [69/120    avg_loss:0.016, val_acc:0.983]
Epoch [70/120    avg_loss:0.012, val_acc:0.983]
Epoch [71/120    avg_loss:0.015, val_acc:0.983]
Epoch [72/120    avg_loss:0.019, val_acc:0.983]
Epoch [73/120    avg_loss:0.017, val_acc:0.982]
Epoch [74/120    avg_loss:0.014, val_acc:0.985]
Epoch [75/120    avg_loss:0.014, val_acc:0.984]
Epoch [76/120    avg_loss:0.018, val_acc:0.983]
Epoch [77/120    avg_loss:0.014, val_acc:0.981]
Epoch [78/120    avg_loss:0.011, val_acc:0.982]
Epoch [79/120    avg_loss:0.017, val_acc:0.982]
Epoch [80/120    avg_loss:0.015, val_acc:0.984]
Epoch [81/120    avg_loss:0.017, val_acc:0.984]
Epoch [82/120    avg_loss:0.013, val_acc:0.983]
Epoch [83/120    avg_loss:0.011, val_acc:0.984]
Epoch [84/120    avg_loss:0.013, val_acc:0.984]
Epoch [85/120    avg_loss:0.016, val_acc:0.982]
Epoch [86/120    avg_loss:0.014, val_acc:0.982]
Epoch [87/120    avg_loss:0.013, val_acc:0.983]
Epoch [88/120    avg_loss:0.013, val_acc:0.983]
Epoch [89/120    avg_loss:0.014, val_acc:0.983]
Epoch [90/120    avg_loss:0.017, val_acc:0.982]
Epoch [91/120    avg_loss:0.016, val_acc:0.982]
Epoch [92/120    avg_loss:0.015, val_acc:0.982]
Epoch [93/120    avg_loss:0.017, val_acc:0.983]
Epoch [94/120    avg_loss:0.018, val_acc:0.983]
Epoch [95/120    avg_loss:0.010, val_acc:0.983]
Epoch [96/120    avg_loss:0.016, val_acc:0.983]
Epoch [97/120    avg_loss:0.016, val_acc:0.983]
Epoch [98/120    avg_loss:0.016, val_acc:0.983]
Epoch [99/120    avg_loss:0.012, val_acc:0.983]
Epoch [100/120    avg_loss:0.011, val_acc:0.983]
Epoch [101/120    avg_loss:0.011, val_acc:0.983]
Epoch [102/120    avg_loss:0.012, val_acc:0.983]
Epoch [103/120    avg_loss:0.012, val_acc:0.983]
Epoch [104/120    avg_loss:0.013, val_acc:0.983]
Epoch [105/120    avg_loss:0.011, val_acc:0.983]
Epoch [106/120    avg_loss:0.017, val_acc:0.983]
Epoch [107/120    avg_loss:0.013, val_acc:0.983]
Epoch [108/120    avg_loss:0.013, val_acc:0.983]
Epoch [109/120    avg_loss:0.014, val_acc:0.983]
Epoch [110/120    avg_loss:0.015, val_acc:0.983]
Epoch [111/120    avg_loss:0.012, val_acc:0.983]
Epoch [112/120    avg_loss:0.012, val_acc:0.983]
Epoch [113/120    avg_loss:0.015, val_acc:0.983]
Epoch [114/120    avg_loss:0.012, val_acc:0.983]
Epoch [115/120    avg_loss:0.012, val_acc:0.983]
Epoch [116/120    avg_loss:0.013, val_acc:0.983]
Epoch [117/120    avg_loss:0.015, val_acc:0.983]
Epoch [118/120    avg_loss:0.013, val_acc:0.983]
Epoch [119/120    avg_loss:0.018, val_acc:0.983]
Epoch [120/120    avg_loss:0.013, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1271    0    0    0    0    0    0    0    6    7    1    0
     0    0    0]
 [   0    0    0  718    2   12    0    0    0    6    2    0    6    1
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    4    6    0    4    0    0    0    0  835   17    2    0
     0    7    0]
 [   0    0   17    0    0    0   11    0    0    0   14 2166    0    2
     0    0    0]
 [   0    0    0    3    7    5    0    0    0    0    1    0  514    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    4    0    0    2    0    3    0    0    0
  1129    1    0]
 [   0    0    0    0    0    0   23    0    0    0    0    0    0    0
    36  288    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.52845528455285

F1 scores:
[       nan 0.975      0.98641832 0.97158322 0.97931034 0.9674523
 0.97401633 0.98039216 0.99767981 0.68292683 0.96087457 0.98432175
 0.97164461 0.9919571  0.98003472 0.89580093 0.97076023]

Kappa:
0.9718275950615073
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:23--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f436fb06668>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.423, val_acc:0.503]
Epoch [2/120    avg_loss:1.673, val_acc:0.604]
Epoch [3/120    avg_loss:1.424, val_acc:0.642]
Epoch [4/120    avg_loss:1.119, val_acc:0.737]
Epoch [5/120    avg_loss:0.897, val_acc:0.786]
Epoch [6/120    avg_loss:0.676, val_acc:0.797]
Epoch [7/120    avg_loss:0.678, val_acc:0.696]
Epoch [8/120    avg_loss:0.681, val_acc:0.825]
Epoch [9/120    avg_loss:0.517, val_acc:0.763]
Epoch [10/120    avg_loss:0.414, val_acc:0.847]
Epoch [11/120    avg_loss:0.504, val_acc:0.863]
Epoch [12/120    avg_loss:0.449, val_acc:0.888]
Epoch [13/120    avg_loss:0.403, val_acc:0.825]
Epoch [14/120    avg_loss:0.313, val_acc:0.873]
Epoch [15/120    avg_loss:0.400, val_acc:0.865]
Epoch [16/120    avg_loss:0.322, val_acc:0.906]
Epoch [17/120    avg_loss:0.284, val_acc:0.895]
Epoch [18/120    avg_loss:0.190, val_acc:0.898]
Epoch [19/120    avg_loss:0.166, val_acc:0.919]
Epoch [20/120    avg_loss:0.138, val_acc:0.932]
Epoch [21/120    avg_loss:0.140, val_acc:0.908]
Epoch [22/120    avg_loss:0.122, val_acc:0.907]
Epoch [23/120    avg_loss:0.116, val_acc:0.931]
Epoch [24/120    avg_loss:0.083, val_acc:0.953]
Epoch [25/120    avg_loss:0.066, val_acc:0.956]
Epoch [26/120    avg_loss:0.067, val_acc:0.946]
Epoch [27/120    avg_loss:0.078, val_acc:0.950]
Epoch [28/120    avg_loss:0.098, val_acc:0.960]
Epoch [29/120    avg_loss:0.073, val_acc:0.949]
Epoch [30/120    avg_loss:0.043, val_acc:0.968]
Epoch [31/120    avg_loss:0.052, val_acc:0.959]
Epoch [32/120    avg_loss:0.048, val_acc:0.948]
Epoch [33/120    avg_loss:0.063, val_acc:0.948]
Epoch [34/120    avg_loss:0.064, val_acc:0.960]
Epoch [35/120    avg_loss:0.050, val_acc:0.936]
Epoch [36/120    avg_loss:0.052, val_acc:0.964]
Epoch [37/120    avg_loss:0.052, val_acc:0.961]
Epoch [38/120    avg_loss:0.054, val_acc:0.945]
Epoch [39/120    avg_loss:0.065, val_acc:0.961]
Epoch [40/120    avg_loss:0.101, val_acc:0.936]
Epoch [41/120    avg_loss:0.071, val_acc:0.939]
Epoch [42/120    avg_loss:0.086, val_acc:0.963]
Epoch [43/120    avg_loss:0.057, val_acc:0.958]
Epoch [44/120    avg_loss:0.054, val_acc:0.962]
Epoch [45/120    avg_loss:0.034, val_acc:0.967]
Epoch [46/120    avg_loss:0.039, val_acc:0.971]
Epoch [47/120    avg_loss:0.026, val_acc:0.972]
Epoch [48/120    avg_loss:0.026, val_acc:0.974]
Epoch [49/120    avg_loss:0.029, val_acc:0.975]
Epoch [50/120    avg_loss:0.027, val_acc:0.972]
Epoch [51/120    avg_loss:0.025, val_acc:0.977]
Epoch [52/120    avg_loss:0.022, val_acc:0.978]
Epoch [53/120    avg_loss:0.026, val_acc:0.973]
Epoch [54/120    avg_loss:0.020, val_acc:0.974]
Epoch [55/120    avg_loss:0.027, val_acc:0.975]
Epoch [56/120    avg_loss:0.020, val_acc:0.975]
Epoch [57/120    avg_loss:0.020, val_acc:0.977]
Epoch [58/120    avg_loss:0.020, val_acc:0.975]
Epoch [59/120    avg_loss:0.022, val_acc:0.977]
Epoch [60/120    avg_loss:0.019, val_acc:0.978]
Epoch [61/120    avg_loss:0.020, val_acc:0.974]
Epoch [62/120    avg_loss:0.020, val_acc:0.975]
Epoch [63/120    avg_loss:0.016, val_acc:0.973]
Epoch [64/120    avg_loss:0.028, val_acc:0.975]
Epoch [65/120    avg_loss:0.018, val_acc:0.973]
Epoch [66/120    avg_loss:0.020, val_acc:0.975]
Epoch [67/120    avg_loss:0.023, val_acc:0.975]
Epoch [68/120    avg_loss:0.022, val_acc:0.975]
Epoch [69/120    avg_loss:0.017, val_acc:0.977]
Epoch [70/120    avg_loss:0.019, val_acc:0.975]
Epoch [71/120    avg_loss:0.015, val_acc:0.974]
Epoch [72/120    avg_loss:0.021, val_acc:0.974]
Epoch [73/120    avg_loss:0.021, val_acc:0.972]
Epoch [74/120    avg_loss:0.023, val_acc:0.973]
Epoch [75/120    avg_loss:0.020, val_acc:0.973]
Epoch [76/120    avg_loss:0.024, val_acc:0.974]
Epoch [77/120    avg_loss:0.017, val_acc:0.974]
Epoch [78/120    avg_loss:0.018, val_acc:0.974]
Epoch [79/120    avg_loss:0.026, val_acc:0.974]
Epoch [80/120    avg_loss:0.012, val_acc:0.975]
Epoch [81/120    avg_loss:0.015, val_acc:0.975]
Epoch [82/120    avg_loss:0.016, val_acc:0.975]
Epoch [83/120    avg_loss:0.016, val_acc:0.977]
Epoch [84/120    avg_loss:0.018, val_acc:0.975]
Epoch [85/120    avg_loss:0.016, val_acc:0.977]
Epoch [86/120    avg_loss:0.020, val_acc:0.977]
Epoch [87/120    avg_loss:0.018, val_acc:0.977]
Epoch [88/120    avg_loss:0.019, val_acc:0.977]
Epoch [89/120    avg_loss:0.014, val_acc:0.977]
Epoch [90/120    avg_loss:0.015, val_acc:0.977]
Epoch [91/120    avg_loss:0.016, val_acc:0.977]
Epoch [92/120    avg_loss:0.018, val_acc:0.977]
Epoch [93/120    avg_loss:0.020, val_acc:0.977]
Epoch [94/120    avg_loss:0.016, val_acc:0.977]
Epoch [95/120    avg_loss:0.018, val_acc:0.977]
Epoch [96/120    avg_loss:0.020, val_acc:0.977]
Epoch [97/120    avg_loss:0.016, val_acc:0.977]
Epoch [98/120    avg_loss:0.019, val_acc:0.977]
Epoch [99/120    avg_loss:0.021, val_acc:0.977]
Epoch [100/120    avg_loss:0.020, val_acc:0.977]
Epoch [101/120    avg_loss:0.019, val_acc:0.977]
Epoch [102/120    avg_loss:0.019, val_acc:0.977]
Epoch [103/120    avg_loss:0.014, val_acc:0.977]
Epoch [104/120    avg_loss:0.015, val_acc:0.977]
Epoch [105/120    avg_loss:0.017, val_acc:0.977]
Epoch [106/120    avg_loss:0.016, val_acc:0.977]
Epoch [107/120    avg_loss:0.020, val_acc:0.977]
Epoch [108/120    avg_loss:0.020, val_acc:0.977]
Epoch [109/120    avg_loss:0.021, val_acc:0.977]
Epoch [110/120    avg_loss:0.019, val_acc:0.977]
Epoch [111/120    avg_loss:0.023, val_acc:0.977]
Epoch [112/120    avg_loss:0.020, val_acc:0.977]
Epoch [113/120    avg_loss:0.019, val_acc:0.977]
Epoch [114/120    avg_loss:0.018, val_acc:0.977]
Epoch [115/120    avg_loss:0.026, val_acc:0.977]
Epoch [116/120    avg_loss:0.022, val_acc:0.977]
Epoch [117/120    avg_loss:0.021, val_acc:0.977]
Epoch [118/120    avg_loss:0.013, val_acc:0.977]
Epoch [119/120    avg_loss:0.020, val_acc:0.977]
Epoch [120/120    avg_loss:0.016, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1262    0    0    0    3    0    0    0    4    7    9    0
     0    0    0]
 [   0    0    0  709    0   23    0    0    0    7    2    0    2    4
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   17    0    0    1    0
     0    0    0]
 [   0    0   12    5    0    7    0    0    0    0  836   11    0    0
     3    1    0]
 [   0    0   11    0    0    0   13    0    0    0    7 2172    1    3
     3    0    0]
 [   0    0    0    9    2    6    0    0    0    0    4   15  492    0
     0    0    6]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    1    0    1    0    2    0    0    0
  1135    0    0]
 [   0    0    0    0    0    0   27    0    0    0    0    0    0    1
    24  295    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.30081300813008

F1 scores:
[       nan 0.975      0.98210117 0.96462585 0.9953271  0.95343681
 0.9660767  0.98039216 0.99883856 0.73913043 0.96535797 0.98347295
 0.94524496 0.97612732 0.98524306 0.91757387 0.95348837]

Kappa:
0.9692262753945609
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:26--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fe40cfe76a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.398, val_acc:0.426]
Epoch [2/120    avg_loss:1.867, val_acc:0.584]
Epoch [3/120    avg_loss:1.367, val_acc:0.658]
Epoch [4/120    avg_loss:1.054, val_acc:0.757]
Epoch [5/120    avg_loss:0.939, val_acc:0.728]
Epoch [6/120    avg_loss:0.850, val_acc:0.771]
Epoch [7/120    avg_loss:0.732, val_acc:0.758]
Epoch [8/120    avg_loss:0.704, val_acc:0.795]
Epoch [9/120    avg_loss:0.577, val_acc:0.826]
Epoch [10/120    avg_loss:0.562, val_acc:0.840]
Epoch [11/120    avg_loss:0.417, val_acc:0.888]
Epoch [12/120    avg_loss:0.269, val_acc:0.882]
Epoch [13/120    avg_loss:0.237, val_acc:0.898]
Epoch [14/120    avg_loss:0.202, val_acc:0.931]
Epoch [15/120    avg_loss:0.256, val_acc:0.887]
Epoch [16/120    avg_loss:0.282, val_acc:0.855]
Epoch [17/120    avg_loss:0.210, val_acc:0.911]
Epoch [18/120    avg_loss:0.185, val_acc:0.917]
Epoch [19/120    avg_loss:0.145, val_acc:0.914]
Epoch [20/120    avg_loss:0.233, val_acc:0.849]
Epoch [21/120    avg_loss:0.231, val_acc:0.935]
Epoch [22/120    avg_loss:0.142, val_acc:0.917]
Epoch [23/120    avg_loss:0.122, val_acc:0.927]
Epoch [24/120    avg_loss:0.105, val_acc:0.944]
Epoch [25/120    avg_loss:0.138, val_acc:0.948]
Epoch [26/120    avg_loss:0.084, val_acc:0.950]
Epoch [27/120    avg_loss:0.092, val_acc:0.953]
Epoch [28/120    avg_loss:0.094, val_acc:0.952]
Epoch [29/120    avg_loss:0.072, val_acc:0.951]
Epoch [30/120    avg_loss:0.058, val_acc:0.958]
Epoch [31/120    avg_loss:0.057, val_acc:0.958]
Epoch [32/120    avg_loss:0.081, val_acc:0.954]
Epoch [33/120    avg_loss:0.051, val_acc:0.938]
Epoch [34/120    avg_loss:0.050, val_acc:0.950]
Epoch [35/120    avg_loss:0.053, val_acc:0.962]
Epoch [36/120    avg_loss:0.104, val_acc:0.946]
Epoch [37/120    avg_loss:0.101, val_acc:0.950]
Epoch [38/120    avg_loss:0.106, val_acc:0.948]
Epoch [39/120    avg_loss:0.067, val_acc:0.960]
Epoch [40/120    avg_loss:0.063, val_acc:0.954]
Epoch [41/120    avg_loss:0.044, val_acc:0.958]
Epoch [42/120    avg_loss:0.048, val_acc:0.962]
Epoch [43/120    avg_loss:0.034, val_acc:0.964]
Epoch [44/120    avg_loss:0.032, val_acc:0.965]
Epoch [45/120    avg_loss:0.029, val_acc:0.959]
Epoch [46/120    avg_loss:0.024, val_acc:0.968]
Epoch [47/120    avg_loss:0.031, val_acc:0.961]
Epoch [48/120    avg_loss:0.051, val_acc:0.960]
Epoch [49/120    avg_loss:0.030, val_acc:0.970]
Epoch [50/120    avg_loss:0.028, val_acc:0.969]
Epoch [51/120    avg_loss:0.039, val_acc:0.960]
Epoch [52/120    avg_loss:0.040, val_acc:0.961]
Epoch [53/120    avg_loss:0.041, val_acc:0.961]
Epoch [54/120    avg_loss:0.033, val_acc:0.969]
Epoch [55/120    avg_loss:0.029, val_acc:0.970]
Epoch [56/120    avg_loss:0.025, val_acc:0.972]
Epoch [57/120    avg_loss:0.020, val_acc:0.970]
Epoch [58/120    avg_loss:0.050, val_acc:0.963]
Epoch [59/120    avg_loss:0.030, val_acc:0.971]
Epoch [60/120    avg_loss:0.019, val_acc:0.973]
Epoch [61/120    avg_loss:0.021, val_acc:0.969]
Epoch [62/120    avg_loss:0.056, val_acc:0.940]
Epoch [63/120    avg_loss:0.058, val_acc:0.956]
Epoch [64/120    avg_loss:0.029, val_acc:0.967]
Epoch [65/120    avg_loss:0.026, val_acc:0.971]
Epoch [66/120    avg_loss:0.023, val_acc:0.971]
Epoch [67/120    avg_loss:0.029, val_acc:0.965]
Epoch [68/120    avg_loss:0.021, val_acc:0.969]
Epoch [69/120    avg_loss:0.020, val_acc:0.971]
Epoch [70/120    avg_loss:0.016, val_acc:0.974]
Epoch [71/120    avg_loss:0.019, val_acc:0.970]
Epoch [72/120    avg_loss:0.019, val_acc:0.970]
Epoch [73/120    avg_loss:0.015, val_acc:0.974]
Epoch [74/120    avg_loss:0.013, val_acc:0.983]
Epoch [75/120    avg_loss:0.012, val_acc:0.972]
Epoch [76/120    avg_loss:0.020, val_acc:0.968]
Epoch [77/120    avg_loss:0.016, val_acc:0.973]
Epoch [78/120    avg_loss:0.026, val_acc:0.972]
Epoch [79/120    avg_loss:0.027, val_acc:0.967]
Epoch [80/120    avg_loss:0.031, val_acc:0.973]
Epoch [81/120    avg_loss:0.016, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.974]
Epoch [83/120    avg_loss:0.013, val_acc:0.979]
Epoch [84/120    avg_loss:0.021, val_acc:0.960]
Epoch [85/120    avg_loss:0.021, val_acc:0.967]
Epoch [86/120    avg_loss:0.022, val_acc:0.978]
Epoch [87/120    avg_loss:0.014, val_acc:0.975]
Epoch [88/120    avg_loss:0.011, val_acc:0.975]
Epoch [89/120    avg_loss:0.009, val_acc:0.977]
Epoch [90/120    avg_loss:0.009, val_acc:0.978]
Epoch [91/120    avg_loss:0.009, val_acc:0.978]
Epoch [92/120    avg_loss:0.010, val_acc:0.977]
Epoch [93/120    avg_loss:0.007, val_acc:0.978]
Epoch [94/120    avg_loss:0.008, val_acc:0.979]
Epoch [95/120    avg_loss:0.007, val_acc:0.978]
Epoch [96/120    avg_loss:0.008, val_acc:0.979]
Epoch [97/120    avg_loss:0.007, val_acc:0.977]
Epoch [98/120    avg_loss:0.008, val_acc:0.979]
Epoch [99/120    avg_loss:0.010, val_acc:0.979]
Epoch [100/120    avg_loss:0.010, val_acc:0.978]
Epoch [101/120    avg_loss:0.007, val_acc:0.978]
Epoch [102/120    avg_loss:0.008, val_acc:0.978]
Epoch [103/120    avg_loss:0.007, val_acc:0.978]
Epoch [104/120    avg_loss:0.008, val_acc:0.978]
Epoch [105/120    avg_loss:0.009, val_acc:0.978]
Epoch [106/120    avg_loss:0.008, val_acc:0.979]
Epoch [107/120    avg_loss:0.007, val_acc:0.979]
Epoch [108/120    avg_loss:0.008, val_acc:0.979]
Epoch [109/120    avg_loss:0.007, val_acc:0.979]
Epoch [110/120    avg_loss:0.009, val_acc:0.979]
Epoch [111/120    avg_loss:0.008, val_acc:0.979]
Epoch [112/120    avg_loss:0.007, val_acc:0.979]
Epoch [113/120    avg_loss:0.007, val_acc:0.979]
Epoch [114/120    avg_loss:0.008, val_acc:0.979]
Epoch [115/120    avg_loss:0.006, val_acc:0.979]
Epoch [116/120    avg_loss:0.010, val_acc:0.979]
Epoch [117/120    avg_loss:0.009, val_acc:0.979]
Epoch [118/120    avg_loss:0.009, val_acc:0.979]
Epoch [119/120    avg_loss:0.006, val_acc:0.979]
Epoch [120/120    avg_loss:0.008, val_acc:0.979]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1263    0    0    0    1    0    0    0    8    8    5    0
     0    0    0]
 [   0    0    0  718    0   19    0    0    0    6    1    0    3    0
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    1    0    0    0    0    0   17    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    3    0    0    0    0  850   13    1    0
     0    4    0]
 [   0    0    7    0    0    0   12    0    0    0    8 2182    0    1
     0    0    0]
 [   0    0    0    9    4    1    0    0    0    0    0    8  508    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    29  315    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
98.1029810298103

F1 scores:
[       nan 0.98765432 0.98826291 0.96961512 0.98598131 0.97078652
 0.98720843 1.         0.99883856 0.77272727 0.97477064 0.98688376
 0.9648623  0.99730458 0.98611111 0.94594595 0.96470588]

Kappa:
0.9783713271381803
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:28--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f20d0f83748>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.425, val_acc:0.459]
Epoch [2/120    avg_loss:1.835, val_acc:0.613]
Epoch [3/120    avg_loss:1.414, val_acc:0.610]
Epoch [4/120    avg_loss:1.038, val_acc:0.725]
Epoch [5/120    avg_loss:0.939, val_acc:0.773]
Epoch [6/120    avg_loss:0.857, val_acc:0.792]
Epoch [7/120    avg_loss:0.657, val_acc:0.827]
Epoch [8/120    avg_loss:0.576, val_acc:0.825]
Epoch [9/120    avg_loss:0.474, val_acc:0.735]
Epoch [10/120    avg_loss:0.453, val_acc:0.854]
Epoch [11/120    avg_loss:0.382, val_acc:0.856]
Epoch [12/120    avg_loss:0.323, val_acc:0.865]
Epoch [13/120    avg_loss:0.230, val_acc:0.897]
Epoch [14/120    avg_loss:0.198, val_acc:0.885]
Epoch [15/120    avg_loss:0.235, val_acc:0.886]
Epoch [16/120    avg_loss:0.214, val_acc:0.905]
Epoch [17/120    avg_loss:0.179, val_acc:0.894]
Epoch [18/120    avg_loss:0.157, val_acc:0.921]
Epoch [19/120    avg_loss:0.127, val_acc:0.927]
Epoch [20/120    avg_loss:0.132, val_acc:0.929]
Epoch [21/120    avg_loss:0.122, val_acc:0.922]
Epoch [22/120    avg_loss:0.180, val_acc:0.926]
Epoch [23/120    avg_loss:0.275, val_acc:0.867]
Epoch [24/120    avg_loss:0.185, val_acc:0.919]
Epoch [25/120    avg_loss:0.228, val_acc:0.900]
Epoch [26/120    avg_loss:0.154, val_acc:0.921]
Epoch [27/120    avg_loss:0.122, val_acc:0.936]
Epoch [28/120    avg_loss:0.103, val_acc:0.940]
Epoch [29/120    avg_loss:0.092, val_acc:0.942]
Epoch [30/120    avg_loss:0.077, val_acc:0.948]
Epoch [31/120    avg_loss:0.059, val_acc:0.952]
Epoch [32/120    avg_loss:0.050, val_acc:0.953]
Epoch [33/120    avg_loss:0.048, val_acc:0.953]
Epoch [34/120    avg_loss:0.069, val_acc:0.959]
Epoch [35/120    avg_loss:0.052, val_acc:0.946]
Epoch [36/120    avg_loss:0.059, val_acc:0.949]
Epoch [37/120    avg_loss:0.047, val_acc:0.962]
Epoch [38/120    avg_loss:0.039, val_acc:0.956]
Epoch [39/120    avg_loss:0.032, val_acc:0.951]
Epoch [40/120    avg_loss:0.032, val_acc:0.961]
Epoch [41/120    avg_loss:0.035, val_acc:0.950]
Epoch [42/120    avg_loss:0.052, val_acc:0.955]
Epoch [43/120    avg_loss:0.037, val_acc:0.954]
Epoch [44/120    avg_loss:0.043, val_acc:0.968]
Epoch [45/120    avg_loss:0.043, val_acc:0.962]
Epoch [46/120    avg_loss:0.034, val_acc:0.955]
Epoch [47/120    avg_loss:0.036, val_acc:0.961]
Epoch [48/120    avg_loss:0.033, val_acc:0.962]
Epoch [49/120    avg_loss:0.076, val_acc:0.954]
Epoch [50/120    avg_loss:0.094, val_acc:0.948]
Epoch [51/120    avg_loss:0.063, val_acc:0.959]
Epoch [52/120    avg_loss:0.033, val_acc:0.953]
Epoch [53/120    avg_loss:0.047, val_acc:0.948]
Epoch [54/120    avg_loss:0.044, val_acc:0.969]
Epoch [55/120    avg_loss:0.026, val_acc:0.962]
Epoch [56/120    avg_loss:0.087, val_acc:0.958]
Epoch [57/120    avg_loss:0.036, val_acc:0.967]
Epoch [58/120    avg_loss:0.031, val_acc:0.967]
Epoch [59/120    avg_loss:0.029, val_acc:0.969]
Epoch [60/120    avg_loss:0.031, val_acc:0.964]
Epoch [61/120    avg_loss:0.026, val_acc:0.967]
Epoch [62/120    avg_loss:0.024, val_acc:0.968]
Epoch [63/120    avg_loss:0.020, val_acc:0.964]
Epoch [64/120    avg_loss:0.017, val_acc:0.965]
Epoch [65/120    avg_loss:0.075, val_acc:0.955]
Epoch [66/120    avg_loss:0.075, val_acc:0.961]
Epoch [67/120    avg_loss:0.039, val_acc:0.964]
Epoch [68/120    avg_loss:0.019, val_acc:0.967]
Epoch [69/120    avg_loss:0.024, val_acc:0.961]
Epoch [70/120    avg_loss:0.026, val_acc:0.963]
Epoch [71/120    avg_loss:0.019, val_acc:0.973]
Epoch [72/120    avg_loss:0.027, val_acc:0.961]
Epoch [73/120    avg_loss:0.023, val_acc:0.967]
Epoch [74/120    avg_loss:0.017, val_acc:0.965]
Epoch [75/120    avg_loss:0.022, val_acc:0.978]
Epoch [76/120    avg_loss:0.024, val_acc:0.969]
Epoch [77/120    avg_loss:0.018, val_acc:0.975]
Epoch [78/120    avg_loss:0.029, val_acc:0.956]
Epoch [79/120    avg_loss:0.018, val_acc:0.967]
Epoch [80/120    avg_loss:0.023, val_acc:0.964]
Epoch [81/120    avg_loss:0.020, val_acc:0.974]
Epoch [82/120    avg_loss:0.022, val_acc:0.969]
Epoch [83/120    avg_loss:0.017, val_acc:0.970]
Epoch [84/120    avg_loss:0.010, val_acc:0.974]
Epoch [85/120    avg_loss:0.011, val_acc:0.972]
Epoch [86/120    avg_loss:0.008, val_acc:0.972]
Epoch [87/120    avg_loss:0.014, val_acc:0.958]
Epoch [88/120    avg_loss:0.012, val_acc:0.972]
Epoch [89/120    avg_loss:0.010, val_acc:0.971]
Epoch [90/120    avg_loss:0.008, val_acc:0.973]
Epoch [91/120    avg_loss:0.007, val_acc:0.972]
Epoch [92/120    avg_loss:0.007, val_acc:0.971]
Epoch [93/120    avg_loss:0.010, val_acc:0.972]
Epoch [94/120    avg_loss:0.006, val_acc:0.975]
Epoch [95/120    avg_loss:0.006, val_acc:0.973]
Epoch [96/120    avg_loss:0.005, val_acc:0.973]
Epoch [97/120    avg_loss:0.008, val_acc:0.973]
Epoch [98/120    avg_loss:0.007, val_acc:0.973]
Epoch [99/120    avg_loss:0.007, val_acc:0.973]
Epoch [100/120    avg_loss:0.007, val_acc:0.973]
Epoch [101/120    avg_loss:0.006, val_acc:0.973]
Epoch [102/120    avg_loss:0.005, val_acc:0.973]
Epoch [103/120    avg_loss:0.006, val_acc:0.973]
Epoch [104/120    avg_loss:0.006, val_acc:0.973]
Epoch [105/120    avg_loss:0.007, val_acc:0.973]
Epoch [106/120    avg_loss:0.008, val_acc:0.973]
Epoch [107/120    avg_loss:0.007, val_acc:0.973]
Epoch [108/120    avg_loss:0.008, val_acc:0.973]
Epoch [109/120    avg_loss:0.007, val_acc:0.973]
Epoch [110/120    avg_loss:0.007, val_acc:0.973]
Epoch [111/120    avg_loss:0.011, val_acc:0.973]
Epoch [112/120    avg_loss:0.006, val_acc:0.973]
Epoch [113/120    avg_loss:0.008, val_acc:0.973]
Epoch [114/120    avg_loss:0.007, val_acc:0.973]
Epoch [115/120    avg_loss:0.007, val_acc:0.973]
Epoch [116/120    avg_loss:0.006, val_acc:0.973]
Epoch [117/120    avg_loss:0.005, val_acc:0.973]
Epoch [118/120    avg_loss:0.006, val_acc:0.973]
Epoch [119/120    avg_loss:0.008, val_acc:0.973]
Epoch [120/120    avg_loss:0.007, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1273    0    0    0    1    0    0    0    5    4    2    0
     0    0    0]
 [   0    0    0  713    0   16    0    0    0    6    0    0   10    2
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    2    7    0    5    1    0    0    0  829   24    1    0
     0    6    0]
 [   0    0   10    0    0    0   10    0    0    0    5 2183    0    2
     0    0    0]
 [   0    0    0   14    5    2    0    0    0    0    0   11  499    0
     1    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    3    0    0    0
  1129    0    0]
 [   0    0    0    0    0    0   40    0    0    0    0    0    0    0
    27  280    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
97.31165311653116

F1 scores:
[       nan 0.975      0.99066148 0.95897781 0.98368298 0.96089385
 0.96117216 0.98039216 0.99883856 0.66666667 0.96451425 0.98488608
 0.95229008 0.98659517 0.98344948 0.88467615 0.97619048]

Kappa:
0.9693376341954294
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:30--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7ff9ab4696a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.380, val_acc:0.480]
Epoch [2/120    avg_loss:1.958, val_acc:0.625]
Epoch [3/120    avg_loss:1.448, val_acc:0.670]
Epoch [4/120    avg_loss:1.197, val_acc:0.695]
Epoch [5/120    avg_loss:0.927, val_acc:0.731]
Epoch [6/120    avg_loss:0.840, val_acc:0.742]
Epoch [7/120    avg_loss:0.632, val_acc:0.826]
Epoch [8/120    avg_loss:0.529, val_acc:0.862]
Epoch [9/120    avg_loss:0.528, val_acc:0.857]
Epoch [10/120    avg_loss:0.486, val_acc:0.767]
Epoch [11/120    avg_loss:0.486, val_acc:0.842]
Epoch [12/120    avg_loss:0.465, val_acc:0.830]
Epoch [13/120    avg_loss:0.273, val_acc:0.887]
Epoch [14/120    avg_loss:0.312, val_acc:0.877]
Epoch [15/120    avg_loss:0.224, val_acc:0.914]
Epoch [16/120    avg_loss:0.165, val_acc:0.931]
Epoch [17/120    avg_loss:0.134, val_acc:0.932]
Epoch [18/120    avg_loss:0.212, val_acc:0.893]
Epoch [19/120    avg_loss:0.139, val_acc:0.931]
Epoch [20/120    avg_loss:0.147, val_acc:0.927]
Epoch [21/120    avg_loss:0.126, val_acc:0.933]
Epoch [22/120    avg_loss:0.120, val_acc:0.945]
Epoch [23/120    avg_loss:0.094, val_acc:0.936]
Epoch [24/120    avg_loss:0.153, val_acc:0.929]
Epoch [25/120    avg_loss:0.206, val_acc:0.926]
Epoch [26/120    avg_loss:0.165, val_acc:0.930]
Epoch [27/120    avg_loss:0.091, val_acc:0.953]
Epoch [28/120    avg_loss:0.073, val_acc:0.952]
Epoch [29/120    avg_loss:0.084, val_acc:0.958]
Epoch [30/120    avg_loss:0.063, val_acc:0.959]
Epoch [31/120    avg_loss:0.072, val_acc:0.960]
Epoch [32/120    avg_loss:0.078, val_acc:0.961]
Epoch [33/120    avg_loss:0.082, val_acc:0.953]
Epoch [34/120    avg_loss:0.053, val_acc:0.964]
Epoch [35/120    avg_loss:0.046, val_acc:0.959]
Epoch [36/120    avg_loss:0.043, val_acc:0.962]
Epoch [37/120    avg_loss:0.053, val_acc:0.959]
Epoch [38/120    avg_loss:0.067, val_acc:0.962]
Epoch [39/120    avg_loss:0.045, val_acc:0.962]
Epoch [40/120    avg_loss:0.035, val_acc:0.969]
Epoch [41/120    avg_loss:0.025, val_acc:0.971]
Epoch [42/120    avg_loss:0.030, val_acc:0.967]
Epoch [43/120    avg_loss:0.041, val_acc:0.959]
Epoch [44/120    avg_loss:0.031, val_acc:0.969]
Epoch [45/120    avg_loss:0.038, val_acc:0.959]
Epoch [46/120    avg_loss:0.032, val_acc:0.971]
Epoch [47/120    avg_loss:0.033, val_acc:0.970]
Epoch [48/120    avg_loss:0.025, val_acc:0.967]
Epoch [49/120    avg_loss:0.032, val_acc:0.972]
Epoch [50/120    avg_loss:0.042, val_acc:0.960]
Epoch [51/120    avg_loss:0.058, val_acc:0.969]
Epoch [52/120    avg_loss:0.042, val_acc:0.960]
Epoch [53/120    avg_loss:0.044, val_acc:0.972]
Epoch [54/120    avg_loss:0.030, val_acc:0.977]
Epoch [55/120    avg_loss:0.018, val_acc:0.974]
Epoch [56/120    avg_loss:0.018, val_acc:0.973]
Epoch [57/120    avg_loss:0.027, val_acc:0.971]
Epoch [58/120    avg_loss:0.027, val_acc:0.969]
Epoch [59/120    avg_loss:0.022, val_acc:0.980]
Epoch [60/120    avg_loss:0.020, val_acc:0.977]
Epoch [61/120    avg_loss:0.022, val_acc:0.972]
Epoch [62/120    avg_loss:0.020, val_acc:0.979]
Epoch [63/120    avg_loss:0.016, val_acc:0.972]
Epoch [64/120    avg_loss:0.014, val_acc:0.977]
Epoch [65/120    avg_loss:0.013, val_acc:0.975]
Epoch [66/120    avg_loss:0.018, val_acc:0.978]
Epoch [67/120    avg_loss:0.025, val_acc:0.977]
Epoch [68/120    avg_loss:0.027, val_acc:0.975]
Epoch [69/120    avg_loss:0.031, val_acc:0.971]
Epoch [70/120    avg_loss:0.022, val_acc:0.980]
Epoch [71/120    avg_loss:0.020, val_acc:0.974]
Epoch [72/120    avg_loss:0.023, val_acc:0.973]
Epoch [73/120    avg_loss:0.015, val_acc:0.979]
Epoch [74/120    avg_loss:0.012, val_acc:0.978]
Epoch [75/120    avg_loss:0.011, val_acc:0.975]
Epoch [76/120    avg_loss:0.016, val_acc:0.980]
Epoch [77/120    avg_loss:0.054, val_acc:0.939]
Epoch [78/120    avg_loss:0.142, val_acc:0.959]
Epoch [79/120    avg_loss:0.053, val_acc:0.960]
Epoch [80/120    avg_loss:0.045, val_acc:0.964]
Epoch [81/120    avg_loss:0.047, val_acc:0.970]
Epoch [82/120    avg_loss:0.027, val_acc:0.968]
Epoch [83/120    avg_loss:0.026, val_acc:0.972]
Epoch [84/120    avg_loss:0.016, val_acc:0.974]
Epoch [85/120    avg_loss:0.019, val_acc:0.979]
Epoch [86/120    avg_loss:0.013, val_acc:0.977]
Epoch [87/120    avg_loss:0.020, val_acc:0.959]
Epoch [88/120    avg_loss:0.031, val_acc:0.977]
Epoch [89/120    avg_loss:0.021, val_acc:0.971]
Epoch [90/120    avg_loss:0.021, val_acc:0.972]
Epoch [91/120    avg_loss:0.013, val_acc:0.979]
Epoch [92/120    avg_loss:0.014, val_acc:0.980]
Epoch [93/120    avg_loss:0.012, val_acc:0.981]
Epoch [94/120    avg_loss:0.011, val_acc:0.981]
Epoch [95/120    avg_loss:0.009, val_acc:0.981]
Epoch [96/120    avg_loss:0.010, val_acc:0.981]
Epoch [97/120    avg_loss:0.008, val_acc:0.980]
Epoch [98/120    avg_loss:0.009, val_acc:0.981]
Epoch [99/120    avg_loss:0.008, val_acc:0.979]
Epoch [100/120    avg_loss:0.007, val_acc:0.979]
Epoch [101/120    avg_loss:0.008, val_acc:0.979]
Epoch [102/120    avg_loss:0.016, val_acc:0.979]
Epoch [103/120    avg_loss:0.009, val_acc:0.979]
Epoch [104/120    avg_loss:0.010, val_acc:0.979]
Epoch [105/120    avg_loss:0.009, val_acc:0.979]
Epoch [106/120    avg_loss:0.007, val_acc:0.979]
Epoch [107/120    avg_loss:0.009, val_acc:0.978]
Epoch [108/120    avg_loss:0.009, val_acc:0.979]
Epoch [109/120    avg_loss:0.007, val_acc:0.979]
Epoch [110/120    avg_loss:0.007, val_acc:0.979]
Epoch [111/120    avg_loss:0.008, val_acc:0.980]
Epoch [112/120    avg_loss:0.010, val_acc:0.980]
Epoch [113/120    avg_loss:0.007, val_acc:0.980]
Epoch [114/120    avg_loss:0.007, val_acc:0.980]
Epoch [115/120    avg_loss:0.008, val_acc:0.980]
Epoch [116/120    avg_loss:0.011, val_acc:0.980]
Epoch [117/120    avg_loss:0.007, val_acc:0.980]
Epoch [118/120    avg_loss:0.006, val_acc:0.980]
Epoch [119/120    avg_loss:0.006, val_acc:0.980]
Epoch [120/120    avg_loss:0.008, val_acc:0.980]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1254    0    0    0    0    0    0    0   20    4    7    0
     0    0    0]
 [   0    0    0  693    0   19    0    0    0    8    1    0   24    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    2    0    0    0    0  853   15    0    0
     1    0    0]
 [   0    0    8    0    0    0   10    0    0    0    7 2184    1    0
     0    0    0]
 [   0    0    0   10    5    0    0    0    0    0    5    4  507    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    2    1    0    0
  1135    0    0]
 [   0    0    0    0    0    0   17    0    0    0    0    0    0    0
    37  293    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.5609756097561

F1 scores:
[       nan 0.975      0.98468787 0.95323246 0.98839907 0.97181511
 0.97986577 0.98039216 0.99883856 0.76595745 0.96657224 0.98868266
 0.94413408 0.99462366 0.98183391 0.915625   0.97647059]

Kappa:
0.9721929930937744
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:33--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f97bf2f36d8>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.407, val_acc:0.458]
Epoch [2/120    avg_loss:1.692, val_acc:0.576]
Epoch [3/120    avg_loss:1.302, val_acc:0.636]
Epoch [4/120    avg_loss:1.063, val_acc:0.695]
Epoch [5/120    avg_loss:0.830, val_acc:0.670]
Epoch [6/120    avg_loss:0.771, val_acc:0.734]
Epoch [7/120    avg_loss:0.743, val_acc:0.733]
Epoch [8/120    avg_loss:0.908, val_acc:0.695]
Epoch [9/120    avg_loss:0.572, val_acc:0.816]
Epoch [10/120    avg_loss:0.485, val_acc:0.789]
Epoch [11/120    avg_loss:0.350, val_acc:0.893]
Epoch [12/120    avg_loss:0.402, val_acc:0.867]
Epoch [13/120    avg_loss:0.356, val_acc:0.882]
Epoch [14/120    avg_loss:0.278, val_acc:0.879]
Epoch [15/120    avg_loss:0.293, val_acc:0.888]
Epoch [16/120    avg_loss:0.275, val_acc:0.882]
Epoch [17/120    avg_loss:0.251, val_acc:0.930]
Epoch [18/120    avg_loss:0.196, val_acc:0.906]
Epoch [19/120    avg_loss:0.187, val_acc:0.938]
Epoch [20/120    avg_loss:0.141, val_acc:0.921]
Epoch [21/120    avg_loss:0.146, val_acc:0.914]
Epoch [22/120    avg_loss:0.134, val_acc:0.936]
Epoch [23/120    avg_loss:0.126, val_acc:0.940]
Epoch [24/120    avg_loss:0.111, val_acc:0.948]
Epoch [25/120    avg_loss:0.111, val_acc:0.956]
Epoch [26/120    avg_loss:0.082, val_acc:0.961]
Epoch [27/120    avg_loss:0.091, val_acc:0.946]
Epoch [28/120    avg_loss:0.096, val_acc:0.945]
Epoch [29/120    avg_loss:0.115, val_acc:0.934]
Epoch [30/120    avg_loss:0.078, val_acc:0.961]
Epoch [31/120    avg_loss:0.070, val_acc:0.961]
Epoch [32/120    avg_loss:0.048, val_acc:0.963]
Epoch [33/120    avg_loss:0.057, val_acc:0.968]
Epoch [34/120    avg_loss:0.064, val_acc:0.958]
Epoch [35/120    avg_loss:0.048, val_acc:0.974]
Epoch [36/120    avg_loss:0.060, val_acc:0.951]
Epoch [37/120    avg_loss:0.057, val_acc:0.961]
Epoch [38/120    avg_loss:0.052, val_acc:0.970]
Epoch [39/120    avg_loss:0.051, val_acc:0.969]
Epoch [40/120    avg_loss:0.083, val_acc:0.959]
Epoch [41/120    avg_loss:0.119, val_acc:0.914]
Epoch [42/120    avg_loss:0.079, val_acc:0.969]
Epoch [43/120    avg_loss:0.045, val_acc:0.967]
Epoch [44/120    avg_loss:0.045, val_acc:0.978]
Epoch [45/120    avg_loss:0.034, val_acc:0.974]
Epoch [46/120    avg_loss:0.032, val_acc:0.977]
Epoch [47/120    avg_loss:0.033, val_acc:0.961]
Epoch [48/120    avg_loss:0.032, val_acc:0.975]
Epoch [49/120    avg_loss:0.027, val_acc:0.980]
Epoch [50/120    avg_loss:0.033, val_acc:0.977]
Epoch [51/120    avg_loss:0.032, val_acc:0.981]
Epoch [52/120    avg_loss:0.045, val_acc:0.981]
Epoch [53/120    avg_loss:0.050, val_acc:0.965]
Epoch [54/120    avg_loss:0.117, val_acc:0.969]
Epoch [55/120    avg_loss:0.066, val_acc:0.961]
Epoch [56/120    avg_loss:0.046, val_acc:0.980]
Epoch [57/120    avg_loss:0.026, val_acc:0.982]
Epoch [58/120    avg_loss:0.049, val_acc:0.981]
Epoch [59/120    avg_loss:0.035, val_acc:0.982]
Epoch [60/120    avg_loss:0.025, val_acc:0.984]
Epoch [61/120    avg_loss:0.019, val_acc:0.983]
Epoch [62/120    avg_loss:0.032, val_acc:0.981]
Epoch [63/120    avg_loss:0.062, val_acc:0.974]
Epoch [64/120    avg_loss:0.041, val_acc:0.984]
Epoch [65/120    avg_loss:0.031, val_acc:0.970]
Epoch [66/120    avg_loss:0.018, val_acc:0.988]
Epoch [67/120    avg_loss:0.029, val_acc:0.972]
Epoch [68/120    avg_loss:0.032, val_acc:0.962]
Epoch [69/120    avg_loss:0.030, val_acc:0.977]
Epoch [70/120    avg_loss:0.030, val_acc:0.969]
Epoch [71/120    avg_loss:0.024, val_acc:0.981]
Epoch [72/120    avg_loss:0.023, val_acc:0.981]
Epoch [73/120    avg_loss:0.021, val_acc:0.989]
Epoch [74/120    avg_loss:0.014, val_acc:0.988]
Epoch [75/120    avg_loss:0.035, val_acc:0.975]
Epoch [76/120    avg_loss:0.027, val_acc:0.980]
Epoch [77/120    avg_loss:0.023, val_acc:0.980]
Epoch [78/120    avg_loss:0.021, val_acc:0.982]
Epoch [79/120    avg_loss:0.016, val_acc:0.978]
Epoch [80/120    avg_loss:0.013, val_acc:0.977]
Epoch [81/120    avg_loss:0.018, val_acc:0.982]
Epoch [82/120    avg_loss:0.010, val_acc:0.989]
Epoch [83/120    avg_loss:0.016, val_acc:0.983]
Epoch [84/120    avg_loss:0.015, val_acc:0.983]
Epoch [85/120    avg_loss:0.013, val_acc:0.989]
Epoch [86/120    avg_loss:0.013, val_acc:0.983]
Epoch [87/120    avg_loss:0.012, val_acc:0.982]
Epoch [88/120    avg_loss:0.012, val_acc:0.982]
Epoch [89/120    avg_loss:0.014, val_acc:0.982]
Epoch [90/120    avg_loss:0.012, val_acc:0.990]
Epoch [91/120    avg_loss:0.009, val_acc:0.985]
Epoch [92/120    avg_loss:0.011, val_acc:0.992]
Epoch [93/120    avg_loss:0.018, val_acc:0.990]
Epoch [94/120    avg_loss:0.012, val_acc:0.989]
Epoch [95/120    avg_loss:0.007, val_acc:0.988]
Epoch [96/120    avg_loss:0.008, val_acc:0.989]
Epoch [97/120    avg_loss:0.010, val_acc:0.980]
Epoch [98/120    avg_loss:0.013, val_acc:0.979]
Epoch [99/120    avg_loss:0.009, val_acc:0.987]
Epoch [100/120    avg_loss:0.012, val_acc:0.991]
Epoch [101/120    avg_loss:0.007, val_acc:0.991]
Epoch [102/120    avg_loss:0.012, val_acc:0.990]
Epoch [103/120    avg_loss:0.010, val_acc:0.990]
Epoch [104/120    avg_loss:0.007, val_acc:0.988]
Epoch [105/120    avg_loss:0.007, val_acc:0.991]
Epoch [106/120    avg_loss:0.005, val_acc:0.992]
Epoch [107/120    avg_loss:0.005, val_acc:0.992]
Epoch [108/120    avg_loss:0.007, val_acc:0.992]
Epoch [109/120    avg_loss:0.010, val_acc:0.992]
Epoch [110/120    avg_loss:0.006, val_acc:0.992]
Epoch [111/120    avg_loss:0.005, val_acc:0.991]
Epoch [112/120    avg_loss:0.005, val_acc:0.991]
Epoch [113/120    avg_loss:0.005, val_acc:0.991]
Epoch [114/120    avg_loss:0.006, val_acc:0.989]
Epoch [115/120    avg_loss:0.005, val_acc:0.989]
Epoch [116/120    avg_loss:0.005, val_acc:0.989]
Epoch [117/120    avg_loss:0.004, val_acc:0.991]
Epoch [118/120    avg_loss:0.005, val_acc:0.990]
Epoch [119/120    avg_loss:0.004, val_acc:0.991]
Epoch [120/120    avg_loss:0.005, val_acc:0.990]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1276    0    0    0    0    0    0    0    5    3    1    0
     0    0    0]
 [   0    0    0  720    0   18    0    0    0    3    0    2    2    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   13    0    0    1    0
     0    0    0]
 [   0    0    0    1    0    2    0    0    0    0  845   21    0    0
     0    6    0]
 [   0    0   10    0    0    0    8    0    0    0    4 2186    0    2
     0    0    0]
 [   0    0    0    3    2    8    0    0    0    0    2   21  494    0
     1    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0    3    0    0    0    0    0    0    0
    31  313    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
98.05962059620596

F1 scores:
[       nan 0.975      0.99260988 0.97627119 0.9953271  0.96304591
 0.99169811 0.98039216 0.99883856 0.68421053 0.97462514 0.98401981
 0.95643756 0.98930481 0.98526863 0.93993994 0.97647059]

Kappa:
0.977863481987716
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:35--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f4a4b46f710>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.487, val_acc:0.350]
Epoch [2/120    avg_loss:1.746, val_acc:0.556]
Epoch [3/120    avg_loss:1.438, val_acc:0.654]
Epoch [4/120    avg_loss:1.111, val_acc:0.714]
Epoch [5/120    avg_loss:0.827, val_acc:0.757]
Epoch [6/120    avg_loss:0.715, val_acc:0.787]
Epoch [7/120    avg_loss:0.760, val_acc:0.710]
Epoch [8/120    avg_loss:0.639, val_acc:0.838]
Epoch [9/120    avg_loss:0.489, val_acc:0.858]
Epoch [10/120    avg_loss:0.361, val_acc:0.854]
Epoch [11/120    avg_loss:0.367, val_acc:0.895]
Epoch [12/120    avg_loss:0.392, val_acc:0.853]
Epoch [13/120    avg_loss:0.349, val_acc:0.903]
Epoch [14/120    avg_loss:0.244, val_acc:0.922]
Epoch [15/120    avg_loss:0.229, val_acc:0.898]
Epoch [16/120    avg_loss:0.228, val_acc:0.917]
Epoch [17/120    avg_loss:0.164, val_acc:0.894]
Epoch [18/120    avg_loss:0.166, val_acc:0.927]
Epoch [19/120    avg_loss:0.126, val_acc:0.936]
Epoch [20/120    avg_loss:0.111, val_acc:0.938]
Epoch [21/120    avg_loss:0.123, val_acc:0.927]
Epoch [22/120    avg_loss:0.120, val_acc:0.924]
Epoch [23/120    avg_loss:0.114, val_acc:0.925]
Epoch [24/120    avg_loss:0.098, val_acc:0.930]
Epoch [25/120    avg_loss:0.090, val_acc:0.940]
Epoch [26/120    avg_loss:0.076, val_acc:0.945]
Epoch [27/120    avg_loss:0.072, val_acc:0.948]
Epoch [28/120    avg_loss:0.056, val_acc:0.942]
Epoch [29/120    avg_loss:0.073, val_acc:0.951]
Epoch [30/120    avg_loss:0.072, val_acc:0.946]
Epoch [31/120    avg_loss:0.059, val_acc:0.951]
Epoch [32/120    avg_loss:0.070, val_acc:0.939]
Epoch [33/120    avg_loss:0.061, val_acc:0.951]
Epoch [34/120    avg_loss:0.059, val_acc:0.961]
Epoch [35/120    avg_loss:0.063, val_acc:0.960]
Epoch [36/120    avg_loss:0.038, val_acc:0.965]
Epoch [37/120    avg_loss:0.052, val_acc:0.960]
Epoch [38/120    avg_loss:0.034, val_acc:0.973]
Epoch [39/120    avg_loss:0.027, val_acc:0.953]
Epoch [40/120    avg_loss:0.030, val_acc:0.958]
Epoch [41/120    avg_loss:0.043, val_acc:0.952]
Epoch [42/120    avg_loss:0.037, val_acc:0.945]
Epoch [43/120    avg_loss:0.058, val_acc:0.964]
Epoch [44/120    avg_loss:0.039, val_acc:0.956]
Epoch [45/120    avg_loss:0.060, val_acc:0.958]
Epoch [46/120    avg_loss:0.046, val_acc:0.974]
Epoch [47/120    avg_loss:0.039, val_acc:0.963]
Epoch [48/120    avg_loss:0.037, val_acc:0.951]
Epoch [49/120    avg_loss:0.028, val_acc:0.975]
Epoch [50/120    avg_loss:0.037, val_acc:0.971]
Epoch [51/120    avg_loss:0.039, val_acc:0.973]
Epoch [52/120    avg_loss:0.027, val_acc:0.963]
Epoch [53/120    avg_loss:0.022, val_acc:0.975]
Epoch [54/120    avg_loss:0.021, val_acc:0.969]
Epoch [55/120    avg_loss:0.017, val_acc:0.970]
Epoch [56/120    avg_loss:0.018, val_acc:0.973]
Epoch [57/120    avg_loss:0.036, val_acc:0.962]
Epoch [58/120    avg_loss:0.048, val_acc:0.965]
Epoch [59/120    avg_loss:0.039, val_acc:0.953]
Epoch [60/120    avg_loss:0.040, val_acc:0.963]
Epoch [61/120    avg_loss:0.039, val_acc:0.967]
Epoch [62/120    avg_loss:0.041, val_acc:0.972]
Epoch [63/120    avg_loss:0.044, val_acc:0.959]
Epoch [64/120    avg_loss:0.036, val_acc:0.941]
Epoch [65/120    avg_loss:0.048, val_acc:0.971]
Epoch [66/120    avg_loss:0.019, val_acc:0.969]
Epoch [67/120    avg_loss:0.019, val_acc:0.969]
Epoch [68/120    avg_loss:0.013, val_acc:0.972]
Epoch [69/120    avg_loss:0.013, val_acc:0.974]
Epoch [70/120    avg_loss:0.017, val_acc:0.977]
Epoch [71/120    avg_loss:0.012, val_acc:0.975]
Epoch [72/120    avg_loss:0.014, val_acc:0.977]
Epoch [73/120    avg_loss:0.018, val_acc:0.975]
Epoch [74/120    avg_loss:0.010, val_acc:0.977]
Epoch [75/120    avg_loss:0.012, val_acc:0.977]
Epoch [76/120    avg_loss:0.012, val_acc:0.978]
Epoch [77/120    avg_loss:0.013, val_acc:0.977]
Epoch [78/120    avg_loss:0.008, val_acc:0.975]
Epoch [79/120    avg_loss:0.011, val_acc:0.977]
Epoch [80/120    avg_loss:0.010, val_acc:0.977]
Epoch [81/120    avg_loss:0.011, val_acc:0.977]
Epoch [82/120    avg_loss:0.014, val_acc:0.975]
Epoch [83/120    avg_loss:0.012, val_acc:0.975]
Epoch [84/120    avg_loss:0.013, val_acc:0.974]
Epoch [85/120    avg_loss:0.013, val_acc:0.978]
Epoch [86/120    avg_loss:0.014, val_acc:0.981]
Epoch [87/120    avg_loss:0.009, val_acc:0.979]
Epoch [88/120    avg_loss:0.011, val_acc:0.979]
Epoch [89/120    avg_loss:0.011, val_acc:0.978]
Epoch [90/120    avg_loss:0.013, val_acc:0.975]
Epoch [91/120    avg_loss:0.008, val_acc:0.975]
Epoch [92/120    avg_loss:0.008, val_acc:0.978]
Epoch [93/120    avg_loss:0.010, val_acc:0.979]
Epoch [94/120    avg_loss:0.011, val_acc:0.979]
Epoch [95/120    avg_loss:0.010, val_acc:0.979]
Epoch [96/120    avg_loss:0.010, val_acc:0.977]
Epoch [97/120    avg_loss:0.011, val_acc:0.977]
Epoch [98/120    avg_loss:0.009, val_acc:0.978]
Epoch [99/120    avg_loss:0.011, val_acc:0.978]
Epoch [100/120    avg_loss:0.009, val_acc:0.978]
Epoch [101/120    avg_loss:0.009, val_acc:0.978]
Epoch [102/120    avg_loss:0.011, val_acc:0.978]
Epoch [103/120    avg_loss:0.011, val_acc:0.978]
Epoch [104/120    avg_loss:0.010, val_acc:0.978]
Epoch [105/120    avg_loss:0.011, val_acc:0.978]
Epoch [106/120    avg_loss:0.010, val_acc:0.978]
Epoch [107/120    avg_loss:0.015, val_acc:0.978]
Epoch [108/120    avg_loss:0.011, val_acc:0.978]
Epoch [109/120    avg_loss:0.008, val_acc:0.978]
Epoch [110/120    avg_loss:0.009, val_acc:0.978]
Epoch [111/120    avg_loss:0.009, val_acc:0.978]
Epoch [112/120    avg_loss:0.011, val_acc:0.978]
Epoch [113/120    avg_loss:0.012, val_acc:0.978]
Epoch [114/120    avg_loss:0.009, val_acc:0.978]
Epoch [115/120    avg_loss:0.015, val_acc:0.978]
Epoch [116/120    avg_loss:0.008, val_acc:0.978]
Epoch [117/120    avg_loss:0.010, val_acc:0.978]
Epoch [118/120    avg_loss:0.008, val_acc:0.978]
Epoch [119/120    avg_loss:0.007, val_acc:0.978]
Epoch [120/120    avg_loss:0.009, val_acc:0.978]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   38    0    0    0    0    0    0    0    0    3    0    0    0
     0    0    0]
 [   0    0 1265    0    0    0    0    0    0    0    5   11    3    0
     0    1    0]
 [   0    0    0  712    0   16    0    0    0    6    0    0   10    3
     0    0    0]
 [   0    0    0    1  212    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  428    0    1    0    5    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  655    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0    0   18    0    0    0    0
     0    0    0]
 [   0    0    4    8    0    6    1    0    0    0  838   12    1    0
     0    5    0]
 [   0    0    3    0    0    0   11    0    0    0    4 2190    0    2
     0    0    0]
 [   0    0    0    4    0    3    0    0    0    0    0    0  524    0
     0    0    3]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    0    0    0
  1137    0    0]
 [   0    0    0    0    0    0   33    0    0    0    0    0    0    0
    31  283    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
97.82113821138212

F1 scores:
[       nan 0.96202532 0.98944075 0.9673913  0.99764706 0.96396396
 0.96536478 0.98039216 0.99883856 0.76595745 0.97103129 0.98983051
 0.97761194 0.98666667 0.98526863 0.88993711 0.98245614]

Kappa:
0.9751554471972609
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:37--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f02678426a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.371, val_acc:0.488]
Epoch [2/120    avg_loss:1.690, val_acc:0.509]
Epoch [3/120    avg_loss:1.519, val_acc:0.655]
Epoch [4/120    avg_loss:1.102, val_acc:0.675]
Epoch [5/120    avg_loss:0.847, val_acc:0.758]
Epoch [6/120    avg_loss:0.804, val_acc:0.775]
Epoch [7/120    avg_loss:0.676, val_acc:0.814]
Epoch [8/120    avg_loss:0.521, val_acc:0.828]
Epoch [9/120    avg_loss:0.573, val_acc:0.844]
Epoch [10/120    avg_loss:0.585, val_acc:0.760]
Epoch [11/120    avg_loss:0.553, val_acc:0.815]
Epoch [12/120    avg_loss:0.410, val_acc:0.877]
Epoch [13/120    avg_loss:0.329, val_acc:0.849]
Epoch [14/120    avg_loss:0.309, val_acc:0.866]
Epoch [15/120    avg_loss:0.249, val_acc:0.855]
Epoch [16/120    avg_loss:0.242, val_acc:0.893]
Epoch [17/120    avg_loss:0.204, val_acc:0.913]
Epoch [18/120    avg_loss:0.166, val_acc:0.912]
Epoch [19/120    avg_loss:0.190, val_acc:0.897]
Epoch [20/120    avg_loss:0.201, val_acc:0.935]
Epoch [21/120    avg_loss:0.139, val_acc:0.921]
Epoch [22/120    avg_loss:0.109, val_acc:0.923]
Epoch [23/120    avg_loss:0.112, val_acc:0.934]
Epoch [24/120    avg_loss:0.098, val_acc:0.934]
Epoch [25/120    avg_loss:0.120, val_acc:0.924]
Epoch [26/120    avg_loss:0.101, val_acc:0.933]
Epoch [27/120    avg_loss:0.099, val_acc:0.942]
Epoch [28/120    avg_loss:0.079, val_acc:0.931]
Epoch [29/120    avg_loss:0.094, val_acc:0.951]
Epoch [30/120    avg_loss:0.079, val_acc:0.938]
Epoch [31/120    avg_loss:0.126, val_acc:0.926]
Epoch [32/120    avg_loss:0.084, val_acc:0.938]
Epoch [33/120    avg_loss:0.076, val_acc:0.934]
Epoch [34/120    avg_loss:0.060, val_acc:0.949]
Epoch [35/120    avg_loss:0.069, val_acc:0.956]
Epoch [36/120    avg_loss:0.047, val_acc:0.959]
Epoch [37/120    avg_loss:0.045, val_acc:0.952]
Epoch [38/120    avg_loss:0.034, val_acc:0.964]
Epoch [39/120    avg_loss:0.031, val_acc:0.953]
Epoch [40/120    avg_loss:0.042, val_acc:0.963]
Epoch [41/120    avg_loss:0.034, val_acc:0.958]
Epoch [42/120    avg_loss:0.033, val_acc:0.960]
Epoch [43/120    avg_loss:0.044, val_acc:0.943]
Epoch [44/120    avg_loss:0.040, val_acc:0.968]
Epoch [45/120    avg_loss:0.061, val_acc:0.931]
Epoch [46/120    avg_loss:0.088, val_acc:0.946]
Epoch [47/120    avg_loss:0.215, val_acc:0.925]
Epoch [48/120    avg_loss:0.114, val_acc:0.951]
Epoch [49/120    avg_loss:0.264, val_acc:0.897]
Epoch [50/120    avg_loss:0.109, val_acc:0.942]
Epoch [51/120    avg_loss:0.048, val_acc:0.958]
Epoch [52/120    avg_loss:0.031, val_acc:0.958]
Epoch [53/120    avg_loss:0.028, val_acc:0.964]
Epoch [54/120    avg_loss:0.037, val_acc:0.963]
Epoch [55/120    avg_loss:0.033, val_acc:0.972]
Epoch [56/120    avg_loss:0.029, val_acc:0.942]
Epoch [57/120    avg_loss:0.042, val_acc:0.967]
Epoch [58/120    avg_loss:0.025, val_acc:0.961]
Epoch [59/120    avg_loss:0.038, val_acc:0.967]
Epoch [60/120    avg_loss:0.032, val_acc:0.962]
Epoch [61/120    avg_loss:0.034, val_acc:0.962]
Epoch [62/120    avg_loss:0.026, val_acc:0.960]
Epoch [63/120    avg_loss:0.028, val_acc:0.959]
Epoch [64/120    avg_loss:0.043, val_acc:0.948]
Epoch [65/120    avg_loss:0.032, val_acc:0.968]
Epoch [66/120    avg_loss:0.021, val_acc:0.967]
Epoch [67/120    avg_loss:0.021, val_acc:0.970]
Epoch [68/120    avg_loss:0.015, val_acc:0.973]
Epoch [69/120    avg_loss:0.011, val_acc:0.974]
Epoch [70/120    avg_loss:0.011, val_acc:0.977]
Epoch [71/120    avg_loss:0.014, val_acc:0.968]
Epoch [72/120    avg_loss:0.011, val_acc:0.974]
Epoch [73/120    avg_loss:0.022, val_acc:0.964]
Epoch [74/120    avg_loss:0.019, val_acc:0.965]
Epoch [75/120    avg_loss:0.016, val_acc:0.974]
Epoch [76/120    avg_loss:0.015, val_acc:0.973]
Epoch [77/120    avg_loss:0.013, val_acc:0.969]
Epoch [78/120    avg_loss:0.012, val_acc:0.973]
Epoch [79/120    avg_loss:0.009, val_acc:0.974]
Epoch [80/120    avg_loss:0.014, val_acc:0.973]
Epoch [81/120    avg_loss:0.016, val_acc:0.964]
Epoch [82/120    avg_loss:0.019, val_acc:0.975]
Epoch [83/120    avg_loss:0.010, val_acc:0.970]
Epoch [84/120    avg_loss:0.010, val_acc:0.971]
Epoch [85/120    avg_loss:0.012, val_acc:0.972]
Epoch [86/120    avg_loss:0.011, val_acc:0.975]
Epoch [87/120    avg_loss:0.008, val_acc:0.975]
Epoch [88/120    avg_loss:0.008, val_acc:0.975]
Epoch [89/120    avg_loss:0.006, val_acc:0.978]
Epoch [90/120    avg_loss:0.008, val_acc:0.975]
Epoch [91/120    avg_loss:0.007, val_acc:0.974]
Epoch [92/120    avg_loss:0.006, val_acc:0.974]
Epoch [93/120    avg_loss:0.007, val_acc:0.974]
Epoch [94/120    avg_loss:0.007, val_acc:0.974]
Epoch [95/120    avg_loss:0.007, val_acc:0.974]
Epoch [96/120    avg_loss:0.005, val_acc:0.974]
Epoch [97/120    avg_loss:0.006, val_acc:0.974]
Epoch [98/120    avg_loss:0.006, val_acc:0.977]
Epoch [99/120    avg_loss:0.005, val_acc:0.977]
Epoch [100/120    avg_loss:0.007, val_acc:0.974]
Epoch [101/120    avg_loss:0.010, val_acc:0.975]
Epoch [102/120    avg_loss:0.006, val_acc:0.975]
Epoch [103/120    avg_loss:0.007, val_acc:0.975]
Epoch [104/120    avg_loss:0.006, val_acc:0.975]
Epoch [105/120    avg_loss:0.007, val_acc:0.975]
Epoch [106/120    avg_loss:0.007, val_acc:0.975]
Epoch [107/120    avg_loss:0.008, val_acc:0.975]
Epoch [108/120    avg_loss:0.010, val_acc:0.974]
Epoch [109/120    avg_loss:0.005, val_acc:0.974]
Epoch [110/120    avg_loss:0.007, val_acc:0.974]
Epoch [111/120    avg_loss:0.007, val_acc:0.974]
Epoch [112/120    avg_loss:0.007, val_acc:0.974]
Epoch [113/120    avg_loss:0.006, val_acc:0.974]
Epoch [114/120    avg_loss:0.008, val_acc:0.974]
Epoch [115/120    avg_loss:0.008, val_acc:0.974]
Epoch [116/120    avg_loss:0.012, val_acc:0.974]
Epoch [117/120    avg_loss:0.008, val_acc:0.974]
Epoch [118/120    avg_loss:0.006, val_acc:0.974]
Epoch [119/120    avg_loss:0.010, val_acc:0.974]
Epoch [120/120    avg_loss:0.007, val_acc:0.974]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1260    2    0    0    0    0    0    0    9    6    8    0
     0    0    0]
 [   0    0    0  724    0   12    0    0    0    6    2    0    1    2
     0    0    0]
 [   0    0    0    2  211    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    1    0    2    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   14    0    0    0    0
     0    0    0]
 [   0    0    3    8    0    4    0    0    0    0  843   17    0    0
     0    0    0]
 [   0    0    7    0    0    0    6    0    0    0   10 2186    0    1
     0    0    0]
 [   0    0    0   12    3    4    0    0    0    0    1    9  501    0
     0    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    1    0    2    0    0    0
  1133    1    0]
 [   0    0    0    0    0    0   24    0    0    0    0    0    0    7
    35  281    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.59349593495935

F1 scores:
[       nan 0.975      0.98630137 0.96597732 0.9882904  0.97187852
 0.97691735 0.98039216 0.99883856 0.7        0.96674312 0.98713028
 0.95885167 0.97368421 0.982228   0.89348172 0.97076023]

Kappa:
0.9725536039994664
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:40--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:28
Validation dataloader:28
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:15
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f11a7ded748>
supervision:full
center_pixel:True
Network :
Number of parameter: 326433==>0.33M
----------Training process----------
Epoch [1/120    avg_loss:2.368, val_acc:0.420]
Epoch [2/120    avg_loss:1.775, val_acc:0.554]
Epoch [3/120    avg_loss:1.457, val_acc:0.661]
Epoch [4/120    avg_loss:1.187, val_acc:0.713]
Epoch [5/120    avg_loss:0.959, val_acc:0.710]
Epoch [6/120    avg_loss:0.757, val_acc:0.791]
Epoch [7/120    avg_loss:0.806, val_acc:0.762]
Epoch [8/120    avg_loss:0.586, val_acc:0.769]
Epoch [9/120    avg_loss:0.570, val_acc:0.796]
Epoch [10/120    avg_loss:0.568, val_acc:0.807]
Epoch [11/120    avg_loss:0.373, val_acc:0.849]
Epoch [12/120    avg_loss:0.323, val_acc:0.852]
Epoch [13/120    avg_loss:0.366, val_acc:0.866]
Epoch [14/120    avg_loss:0.270, val_acc:0.885]
Epoch [15/120    avg_loss:0.237, val_acc:0.884]
Epoch [16/120    avg_loss:0.272, val_acc:0.875]
Epoch [17/120    avg_loss:0.309, val_acc:0.877]
Epoch [18/120    avg_loss:0.265, val_acc:0.888]
Epoch [19/120    avg_loss:0.208, val_acc:0.930]
Epoch [20/120    avg_loss:0.159, val_acc:0.928]
Epoch [21/120    avg_loss:0.159, val_acc:0.907]
Epoch [22/120    avg_loss:0.127, val_acc:0.905]
Epoch [23/120    avg_loss:0.153, val_acc:0.929]
Epoch [24/120    avg_loss:0.115, val_acc:0.932]
Epoch [25/120    avg_loss:0.100, val_acc:0.943]
Epoch [26/120    avg_loss:0.081, val_acc:0.928]
Epoch [27/120    avg_loss:0.069, val_acc:0.950]
Epoch [28/120    avg_loss:0.076, val_acc:0.947]
Epoch [29/120    avg_loss:0.072, val_acc:0.943]
Epoch [30/120    avg_loss:0.073, val_acc:0.954]
Epoch [31/120    avg_loss:0.071, val_acc:0.932]
Epoch [32/120    avg_loss:0.069, val_acc:0.952]
Epoch [33/120    avg_loss:0.088, val_acc:0.936]
Epoch [34/120    avg_loss:0.053, val_acc:0.955]
Epoch [35/120    avg_loss:0.055, val_acc:0.954]
Epoch [36/120    avg_loss:0.050, val_acc:0.960]
Epoch [37/120    avg_loss:0.045, val_acc:0.957]
Epoch [38/120    avg_loss:0.051, val_acc:0.953]
Epoch [39/120    avg_loss:0.049, val_acc:0.954]
Epoch [40/120    avg_loss:0.047, val_acc:0.944]
Epoch [41/120    avg_loss:0.054, val_acc:0.961]
Epoch [42/120    avg_loss:0.037, val_acc:0.953]
Epoch [43/120    avg_loss:0.048, val_acc:0.958]
Epoch [44/120    avg_loss:0.039, val_acc:0.963]
Epoch [45/120    avg_loss:0.023, val_acc:0.963]
Epoch [46/120    avg_loss:0.035, val_acc:0.956]
Epoch [47/120    avg_loss:0.021, val_acc:0.964]
Epoch [48/120    avg_loss:0.026, val_acc:0.967]
Epoch [49/120    avg_loss:0.037, val_acc:0.962]
Epoch [50/120    avg_loss:0.025, val_acc:0.966]
Epoch [51/120    avg_loss:0.025, val_acc:0.969]
Epoch [52/120    avg_loss:0.026, val_acc:0.960]
Epoch [53/120    avg_loss:0.025, val_acc:0.962]
Epoch [54/120    avg_loss:0.026, val_acc:0.974]
Epoch [55/120    avg_loss:0.025, val_acc:0.963]
Epoch [56/120    avg_loss:0.034, val_acc:0.967]
Epoch [57/120    avg_loss:0.053, val_acc:0.958]
Epoch [58/120    avg_loss:0.071, val_acc:0.925]
Epoch [59/120    avg_loss:0.049, val_acc:0.966]
Epoch [60/120    avg_loss:0.045, val_acc:0.943]
Epoch [61/120    avg_loss:0.070, val_acc:0.963]
Epoch [62/120    avg_loss:0.044, val_acc:0.962]
Epoch [63/120    avg_loss:0.030, val_acc:0.958]
Epoch [64/120    avg_loss:0.022, val_acc:0.963]
Epoch [65/120    avg_loss:0.024, val_acc:0.955]
Epoch [66/120    avg_loss:0.032, val_acc:0.959]
Epoch [67/120    avg_loss:0.018, val_acc:0.969]
Epoch [68/120    avg_loss:0.014, val_acc:0.968]
Epoch [69/120    avg_loss:0.013, val_acc:0.969]
Epoch [70/120    avg_loss:0.016, val_acc:0.969]
Epoch [71/120    avg_loss:0.010, val_acc:0.969]
Epoch [72/120    avg_loss:0.019, val_acc:0.971]
Epoch [73/120    avg_loss:0.017, val_acc:0.973]
Epoch [74/120    avg_loss:0.014, val_acc:0.973]
Epoch [75/120    avg_loss:0.011, val_acc:0.973]
Epoch [76/120    avg_loss:0.014, val_acc:0.974]
Epoch [77/120    avg_loss:0.012, val_acc:0.974]
Epoch [78/120    avg_loss:0.010, val_acc:0.974]
Epoch [79/120    avg_loss:0.011, val_acc:0.974]
Epoch [80/120    avg_loss:0.012, val_acc:0.975]
Epoch [81/120    avg_loss:0.014, val_acc:0.975]
Epoch [82/120    avg_loss:0.012, val_acc:0.973]
Epoch [83/120    avg_loss:0.013, val_acc:0.971]
Epoch [84/120    avg_loss:0.014, val_acc:0.973]
Epoch [85/120    avg_loss:0.010, val_acc:0.971]
Epoch [86/120    avg_loss:0.010, val_acc:0.971]
Epoch [87/120    avg_loss:0.012, val_acc:0.971]
Epoch [88/120    avg_loss:0.011, val_acc:0.969]
Epoch [89/120    avg_loss:0.010, val_acc:0.969]
Epoch [90/120    avg_loss:0.015, val_acc:0.972]
Epoch [91/120    avg_loss:0.010, val_acc:0.973]
Epoch [92/120    avg_loss:0.010, val_acc:0.973]
Epoch [93/120    avg_loss:0.008, val_acc:0.973]
Epoch [94/120    avg_loss:0.010, val_acc:0.973]
Epoch [95/120    avg_loss:0.010, val_acc:0.973]
Epoch [96/120    avg_loss:0.010, val_acc:0.973]
Epoch [97/120    avg_loss:0.008, val_acc:0.973]
Epoch [98/120    avg_loss:0.012, val_acc:0.973]
Epoch [99/120    avg_loss:0.015, val_acc:0.973]
Epoch [100/120    avg_loss:0.011, val_acc:0.973]
Epoch [101/120    avg_loss:0.011, val_acc:0.973]
Epoch [102/120    avg_loss:0.009, val_acc:0.973]
Epoch [103/120    avg_loss:0.009, val_acc:0.973]
Epoch [104/120    avg_loss:0.008, val_acc:0.973]
Epoch [105/120    avg_loss:0.006, val_acc:0.973]
Epoch [106/120    avg_loss:0.012, val_acc:0.973]
Epoch [107/120    avg_loss:0.009, val_acc:0.973]
Epoch [108/120    avg_loss:0.011, val_acc:0.973]
Epoch [109/120    avg_loss:0.010, val_acc:0.973]
Epoch [110/120    avg_loss:0.012, val_acc:0.973]
Epoch [111/120    avg_loss:0.009, val_acc:0.973]
Epoch [112/120    avg_loss:0.007, val_acc:0.973]
Epoch [113/120    avg_loss:0.012, val_acc:0.973]
Epoch [114/120    avg_loss:0.013, val_acc:0.973]
Epoch [115/120    avg_loss:0.009, val_acc:0.973]
Epoch [116/120    avg_loss:0.009, val_acc:0.973]
Epoch [117/120    avg_loss:0.011, val_acc:0.973]
Epoch [118/120    avg_loss:0.010, val_acc:0.973]
Epoch [119/120    avg_loss:0.012, val_acc:0.973]
Epoch [120/120    avg_loss:0.011, val_acc:0.973]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   41    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1258    0    0    0    1    0    0    0   13    7    6    0
     0    0    0]
 [   0    0    0  709    0   17    0    0    0    6    0    0   13    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  433    0    0    0    2    0    0    0    0
     0    0    0]
 [   0    0    1    0    0    0  654    0    0    0    0    2    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    5    0    0    0    0    0   13    0    0    0    0
     0    0    0]
 [   0    0    6   11    0    5    0    0    0    0  822   29    0    0
     0    2    0]
 [   0    0    6    0    0    0    8    0    0    0    8 2186    0    2
     0    0    0]
 [   0    0    0   14    7    0    0    0    0    0    5    4  496    0
     4    0    4]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    2    0    0    0    0    1    0    0    0
  1136    0    0]
 [   0    0    0    0    0    0   32    0    0    0    0    0    0    1
    50  264    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.99728997289972

F1 scores:
[       nan 1.         0.98435055 0.95423957 0.98383372 0.97085202
 0.96745562 1.         1.         0.66666667 0.95359629 0.98512844
 0.9447619  0.98666667 0.97552598 0.86133768 0.97076023]

Kappa:
0.9657431148329481
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:42--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f0feb843668>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.430, val_acc:0.537]
Epoch [2/120    avg_loss:1.754, val_acc:0.568]
Epoch [3/120    avg_loss:1.552, val_acc:0.671]
Epoch [4/120    avg_loss:1.280, val_acc:0.688]
Epoch [5/120    avg_loss:1.097, val_acc:0.731]
Epoch [6/120    avg_loss:1.000, val_acc:0.806]
Epoch [7/120    avg_loss:0.810, val_acc:0.828]
Epoch [8/120    avg_loss:0.749, val_acc:0.821]
Epoch [9/120    avg_loss:0.497, val_acc:0.881]
Epoch [10/120    avg_loss:0.380, val_acc:0.873]
Epoch [11/120    avg_loss:0.380, val_acc:0.857]
Epoch [12/120    avg_loss:0.336, val_acc:0.865]
Epoch [13/120    avg_loss:0.344, val_acc:0.862]
Epoch [14/120    avg_loss:0.306, val_acc:0.906]
Epoch [15/120    avg_loss:0.334, val_acc:0.881]
Epoch [16/120    avg_loss:0.244, val_acc:0.882]
Epoch [17/120    avg_loss:0.279, val_acc:0.871]
Epoch [18/120    avg_loss:0.242, val_acc:0.907]
Epoch [19/120    avg_loss:0.239, val_acc:0.900]
Epoch [20/120    avg_loss:0.187, val_acc:0.917]
Epoch [21/120    avg_loss:0.179, val_acc:0.921]
Epoch [22/120    avg_loss:0.136, val_acc:0.953]
Epoch [23/120    avg_loss:0.152, val_acc:0.927]
Epoch [24/120    avg_loss:0.176, val_acc:0.929]
Epoch [25/120    avg_loss:0.141, val_acc:0.935]
Epoch [26/120    avg_loss:0.136, val_acc:0.935]
Epoch [27/120    avg_loss:0.140, val_acc:0.929]
Epoch [28/120    avg_loss:0.079, val_acc:0.950]
Epoch [29/120    avg_loss:0.077, val_acc:0.951]
Epoch [30/120    avg_loss:0.099, val_acc:0.943]
Epoch [31/120    avg_loss:0.111, val_acc:0.952]
Epoch [32/120    avg_loss:0.099, val_acc:0.941]
Epoch [33/120    avg_loss:0.087, val_acc:0.945]
Epoch [34/120    avg_loss:0.123, val_acc:0.925]
Epoch [35/120    avg_loss:0.121, val_acc:0.958]
Epoch [36/120    avg_loss:0.098, val_acc:0.944]
Epoch [37/120    avg_loss:0.106, val_acc:0.962]
Epoch [38/120    avg_loss:0.081, val_acc:0.949]
Epoch [39/120    avg_loss:0.075, val_acc:0.959]
Epoch [40/120    avg_loss:0.082, val_acc:0.943]
Epoch [41/120    avg_loss:0.067, val_acc:0.959]
Epoch [42/120    avg_loss:0.079, val_acc:0.950]
Epoch [43/120    avg_loss:0.078, val_acc:0.954]
Epoch [44/120    avg_loss:0.073, val_acc:0.960]
Epoch [45/120    avg_loss:0.063, val_acc:0.956]
Epoch [46/120    avg_loss:0.055, val_acc:0.968]
Epoch [47/120    avg_loss:0.051, val_acc:0.956]
Epoch [48/120    avg_loss:0.057, val_acc:0.961]
Epoch [49/120    avg_loss:0.064, val_acc:0.965]
Epoch [50/120    avg_loss:0.043, val_acc:0.965]
Epoch [51/120    avg_loss:0.033, val_acc:0.973]
Epoch [52/120    avg_loss:0.039, val_acc:0.969]
Epoch [53/120    avg_loss:0.039, val_acc:0.968]
Epoch [54/120    avg_loss:0.052, val_acc:0.935]
Epoch [55/120    avg_loss:0.062, val_acc:0.964]
Epoch [56/120    avg_loss:0.084, val_acc:0.953]
Epoch [57/120    avg_loss:0.077, val_acc:0.970]
Epoch [58/120    avg_loss:0.055, val_acc:0.961]
Epoch [59/120    avg_loss:0.052, val_acc:0.970]
Epoch [60/120    avg_loss:0.036, val_acc:0.967]
Epoch [61/120    avg_loss:0.034, val_acc:0.963]
Epoch [62/120    avg_loss:0.049, val_acc:0.970]
Epoch [63/120    avg_loss:0.031, val_acc:0.969]
Epoch [64/120    avg_loss:0.035, val_acc:0.969]
Epoch [65/120    avg_loss:0.024, val_acc:0.969]
Epoch [66/120    avg_loss:0.029, val_acc:0.971]
Epoch [67/120    avg_loss:0.021, val_acc:0.973]
Epoch [68/120    avg_loss:0.016, val_acc:0.974]
Epoch [69/120    avg_loss:0.017, val_acc:0.975]
Epoch [70/120    avg_loss:0.013, val_acc:0.975]
Epoch [71/120    avg_loss:0.018, val_acc:0.974]
Epoch [72/120    avg_loss:0.015, val_acc:0.975]
Epoch [73/120    avg_loss:0.017, val_acc:0.975]
Epoch [74/120    avg_loss:0.020, val_acc:0.975]
Epoch [75/120    avg_loss:0.014, val_acc:0.973]
Epoch [76/120    avg_loss:0.016, val_acc:0.973]
Epoch [77/120    avg_loss:0.023, val_acc:0.973]
Epoch [78/120    avg_loss:0.013, val_acc:0.974]
Epoch [79/120    avg_loss:0.015, val_acc:0.974]
Epoch [80/120    avg_loss:0.018, val_acc:0.974]
Epoch [81/120    avg_loss:0.019, val_acc:0.975]
Epoch [82/120    avg_loss:0.016, val_acc:0.975]
Epoch [83/120    avg_loss:0.017, val_acc:0.975]
Epoch [84/120    avg_loss:0.013, val_acc:0.975]
Epoch [85/120    avg_loss:0.018, val_acc:0.977]
Epoch [86/120    avg_loss:0.012, val_acc:0.975]
Epoch [87/120    avg_loss:0.015, val_acc:0.977]
Epoch [88/120    avg_loss:0.016, val_acc:0.977]
Epoch [89/120    avg_loss:0.020, val_acc:0.978]
Epoch [90/120    avg_loss:0.015, val_acc:0.977]
Epoch [91/120    avg_loss:0.015, val_acc:0.977]
Epoch [92/120    avg_loss:0.015, val_acc:0.974]
Epoch [93/120    avg_loss:0.014, val_acc:0.977]
Epoch [94/120    avg_loss:0.022, val_acc:0.977]
Epoch [95/120    avg_loss:0.015, val_acc:0.974]
Epoch [96/120    avg_loss:0.015, val_acc:0.974]
Epoch [97/120    avg_loss:0.015, val_acc:0.975]
Epoch [98/120    avg_loss:0.015, val_acc:0.974]
Epoch [99/120    avg_loss:0.013, val_acc:0.973]
Epoch [100/120    avg_loss:0.015, val_acc:0.977]
Epoch [101/120    avg_loss:0.012, val_acc:0.977]
Epoch [102/120    avg_loss:0.016, val_acc:0.975]
Epoch [103/120    avg_loss:0.014, val_acc:0.975]
Epoch [104/120    avg_loss:0.014, val_acc:0.975]
Epoch [105/120    avg_loss:0.014, val_acc:0.975]
Epoch [106/120    avg_loss:0.015, val_acc:0.975]
Epoch [107/120    avg_loss:0.011, val_acc:0.977]
Epoch [108/120    avg_loss:0.015, val_acc:0.977]
Epoch [109/120    avg_loss:0.011, val_acc:0.977]
Epoch [110/120    avg_loss:0.018, val_acc:0.977]
Epoch [111/120    avg_loss:0.013, val_acc:0.977]
Epoch [112/120    avg_loss:0.014, val_acc:0.977]
Epoch [113/120    avg_loss:0.014, val_acc:0.977]
Epoch [114/120    avg_loss:0.012, val_acc:0.977]
Epoch [115/120    avg_loss:0.017, val_acc:0.977]
Epoch [116/120    avg_loss:0.014, val_acc:0.977]
Epoch [117/120    avg_loss:0.011, val_acc:0.977]
Epoch [118/120    avg_loss:0.014, val_acc:0.977]
Epoch [119/120    avg_loss:0.017, val_acc:0.977]
Epoch [120/120    avg_loss:0.012, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1259    0    0    0    0    0    0    0   10    9    7    0
     0    0    0]
 [   0    0    0  722    0   15    0    0    0    6    0    0    2    2
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   13    0    0    1    0
     0    0    0]
 [   0    0    1   80    0    5    0    0    0    0  774   10    0    0
     2    3    0]
 [   0    0   13    0    0    3   11    0    0    0    4 2178    0    1
     0    0    0]
 [   0    0    0    4    3    4    0    0    0    0    2    9  506    0
     0    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    6    0    0    1    0    1    0    0    0
  1131    0    0]
 [   0    0    0    0    0    0   26    0    0    1    0    0    0    0
    66  254    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.48780487804878

F1 scores:
[       nan 0.975      0.98436278 0.92742453 0.99300699 0.95884316
 0.97261288 0.98039216 0.99883856 0.63414634 0.92805755 0.98641304
 0.96380952 0.9919571  0.96749358 0.8410596  0.96551724]

Kappa:
0.9599483247762726
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:45--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f9e47d4c6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.408, val_acc:0.432]
Epoch [2/120    avg_loss:1.809, val_acc:0.538]
Epoch [3/120    avg_loss:1.416, val_acc:0.600]
Epoch [4/120    avg_loss:1.165, val_acc:0.679]
Epoch [5/120    avg_loss:0.951, val_acc:0.686]
Epoch [6/120    avg_loss:0.768, val_acc:0.780]
Epoch [7/120    avg_loss:0.813, val_acc:0.800]
Epoch [8/120    avg_loss:0.662, val_acc:0.768]
Epoch [9/120    avg_loss:0.535, val_acc:0.830]
Epoch [10/120    avg_loss:0.460, val_acc:0.830]
Epoch [11/120    avg_loss:0.389, val_acc:0.854]
Epoch [12/120    avg_loss:0.442, val_acc:0.845]
Epoch [13/120    avg_loss:0.333, val_acc:0.869]
Epoch [14/120    avg_loss:0.309, val_acc:0.890]
Epoch [15/120    avg_loss:0.249, val_acc:0.890]
Epoch [16/120    avg_loss:0.215, val_acc:0.890]
Epoch [17/120    avg_loss:0.272, val_acc:0.903]
Epoch [18/120    avg_loss:0.207, val_acc:0.933]
Epoch [19/120    avg_loss:0.174, val_acc:0.926]
Epoch [20/120    avg_loss:0.147, val_acc:0.913]
Epoch [21/120    avg_loss:0.186, val_acc:0.922]
Epoch [22/120    avg_loss:0.220, val_acc:0.893]
Epoch [23/120    avg_loss:0.318, val_acc:0.891]
Epoch [24/120    avg_loss:0.240, val_acc:0.914]
Epoch [25/120    avg_loss:0.178, val_acc:0.930]
Epoch [26/120    avg_loss:0.161, val_acc:0.927]
Epoch [27/120    avg_loss:0.185, val_acc:0.935]
Epoch [28/120    avg_loss:0.169, val_acc:0.933]
Epoch [29/120    avg_loss:0.128, val_acc:0.934]
Epoch [30/120    avg_loss:0.100, val_acc:0.950]
Epoch [31/120    avg_loss:0.101, val_acc:0.950]
Epoch [32/120    avg_loss:0.075, val_acc:0.943]
Epoch [33/120    avg_loss:0.101, val_acc:0.951]
Epoch [34/120    avg_loss:0.096, val_acc:0.934]
Epoch [35/120    avg_loss:0.089, val_acc:0.930]
Epoch [36/120    avg_loss:0.075, val_acc:0.960]
Epoch [37/120    avg_loss:0.065, val_acc:0.958]
Epoch [38/120    avg_loss:0.060, val_acc:0.967]
Epoch [39/120    avg_loss:0.046, val_acc:0.964]
Epoch [40/120    avg_loss:0.055, val_acc:0.964]
Epoch [41/120    avg_loss:0.052, val_acc:0.944]
Epoch [42/120    avg_loss:0.058, val_acc:0.948]
Epoch [43/120    avg_loss:0.131, val_acc:0.955]
Epoch [44/120    avg_loss:0.068, val_acc:0.944]
Epoch [45/120    avg_loss:0.061, val_acc:0.965]
Epoch [46/120    avg_loss:0.072, val_acc:0.959]
Epoch [47/120    avg_loss:0.045, val_acc:0.969]
Epoch [48/120    avg_loss:0.053, val_acc:0.968]
Epoch [49/120    avg_loss:0.043, val_acc:0.971]
Epoch [50/120    avg_loss:0.042, val_acc:0.964]
Epoch [51/120    avg_loss:0.033, val_acc:0.975]
Epoch [52/120    avg_loss:0.030, val_acc:0.969]
Epoch [53/120    avg_loss:0.040, val_acc:0.972]
Epoch [54/120    avg_loss:0.039, val_acc:0.974]
Epoch [55/120    avg_loss:0.044, val_acc:0.964]
Epoch [56/120    avg_loss:0.039, val_acc:0.971]
Epoch [57/120    avg_loss:0.029, val_acc:0.975]
Epoch [58/120    avg_loss:0.032, val_acc:0.971]
Epoch [59/120    avg_loss:0.025, val_acc:0.971]
Epoch [60/120    avg_loss:0.030, val_acc:0.975]
Epoch [61/120    avg_loss:0.031, val_acc:0.968]
Epoch [62/120    avg_loss:0.029, val_acc:0.970]
Epoch [63/120    avg_loss:0.033, val_acc:0.969]
Epoch [64/120    avg_loss:0.040, val_acc:0.970]
Epoch [65/120    avg_loss:0.054, val_acc:0.962]
Epoch [66/120    avg_loss:0.041, val_acc:0.974]
Epoch [67/120    avg_loss:0.035, val_acc:0.979]
Epoch [68/120    avg_loss:0.026, val_acc:0.968]
Epoch [69/120    avg_loss:0.025, val_acc:0.978]
Epoch [70/120    avg_loss:0.030, val_acc:0.974]
Epoch [71/120    avg_loss:0.045, val_acc:0.954]
Epoch [72/120    avg_loss:0.055, val_acc:0.978]
Epoch [73/120    avg_loss:0.029, val_acc:0.984]
Epoch [74/120    avg_loss:0.039, val_acc:0.978]
Epoch [75/120    avg_loss:0.031, val_acc:0.968]
Epoch [76/120    avg_loss:0.016, val_acc:0.983]
Epoch [77/120    avg_loss:0.021, val_acc:0.974]
Epoch [78/120    avg_loss:0.018, val_acc:0.977]
Epoch [79/120    avg_loss:0.014, val_acc:0.978]
Epoch [80/120    avg_loss:0.020, val_acc:0.974]
Epoch [81/120    avg_loss:0.047, val_acc:0.945]
Epoch [82/120    avg_loss:0.021, val_acc:0.969]
Epoch [83/120    avg_loss:0.023, val_acc:0.970]
Epoch [84/120    avg_loss:0.024, val_acc:0.977]
Epoch [85/120    avg_loss:0.020, val_acc:0.983]
Epoch [86/120    avg_loss:0.018, val_acc:0.980]
Epoch [87/120    avg_loss:0.012, val_acc:0.982]
Epoch [88/120    avg_loss:0.012, val_acc:0.982]
Epoch [89/120    avg_loss:0.011, val_acc:0.983]
Epoch [90/120    avg_loss:0.011, val_acc:0.983]
Epoch [91/120    avg_loss:0.011, val_acc:0.983]
Epoch [92/120    avg_loss:0.007, val_acc:0.984]
Epoch [93/120    avg_loss:0.009, val_acc:0.984]
Epoch [94/120    avg_loss:0.010, val_acc:0.985]
Epoch [95/120    avg_loss:0.013, val_acc:0.985]
Epoch [96/120    avg_loss:0.010, val_acc:0.983]
Epoch [97/120    avg_loss:0.008, val_acc:0.984]
Epoch [98/120    avg_loss:0.007, val_acc:0.984]
Epoch [99/120    avg_loss:0.010, val_acc:0.985]
Epoch [100/120    avg_loss:0.010, val_acc:0.985]
Epoch [101/120    avg_loss:0.009, val_acc:0.985]
Epoch [102/120    avg_loss:0.012, val_acc:0.987]
Epoch [103/120    avg_loss:0.013, val_acc:0.984]
Epoch [104/120    avg_loss:0.008, val_acc:0.984]
Epoch [105/120    avg_loss:0.010, val_acc:0.985]
Epoch [106/120    avg_loss:0.009, val_acc:0.985]
Epoch [107/120    avg_loss:0.011, val_acc:0.987]
Epoch [108/120    avg_loss:0.009, val_acc:0.985]
Epoch [109/120    avg_loss:0.011, val_acc:0.985]
Epoch [110/120    avg_loss:0.009, val_acc:0.988]
Epoch [111/120    avg_loss:0.010, val_acc:0.989]
Epoch [112/120    avg_loss:0.009, val_acc:0.988]
Epoch [113/120    avg_loss:0.008, val_acc:0.987]
Epoch [114/120    avg_loss:0.008, val_acc:0.985]
Epoch [115/120    avg_loss:0.012, val_acc:0.984]
Epoch [116/120    avg_loss:0.009, val_acc:0.984]
Epoch [117/120    avg_loss:0.012, val_acc:0.984]
Epoch [118/120    avg_loss:0.007, val_acc:0.984]
Epoch [119/120    avg_loss:0.009, val_acc:0.984]
Epoch [120/120    avg_loss:0.012, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1271    0    0    0    0    0    0    0    4    5    5    0
     0    0    0]
 [   0    0    1  696    1   16    0    0    0    6    0    0   27    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  430    0    1    0    4    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  657    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   12    0    0    2    0
     0    0    0]
 [   0    0    7   65    0    5    0    0    0    0  774   19    0    0
     0    5    0]
 [   0    0    7    0    0    2   12    0    0    0    2 2185    0    2
     0    0    0]
 [   0    0    0    0    2    3    0    0    0    0    7    0  520    0
     0    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    2    0    2    1    0    0
  1134    0    0]
 [   0    0    0    0    0    0   30    0    0    0    0    0    0    0
    19  298    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
97.02981029810299

F1 scores:
[       nan 0.975      0.98872034 0.92063492 0.99300699 0.96412556
 0.96902655 0.98039216 0.99767981 0.6        0.92917167 0.98868778
 0.95500459 0.99191375 0.9895288  0.91692308 0.98224852]

Kappa:
0.9661407377218799
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:48--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f39c6a4a710>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.495, val_acc:0.550]
Epoch [2/120    avg_loss:1.901, val_acc:0.529]
Epoch [3/120    avg_loss:1.409, val_acc:0.634]
Epoch [4/120    avg_loss:1.212, val_acc:0.681]
Epoch [5/120    avg_loss:0.957, val_acc:0.740]
Epoch [6/120    avg_loss:0.810, val_acc:0.733]
Epoch [7/120    avg_loss:1.090, val_acc:0.751]
Epoch [8/120    avg_loss:0.701, val_acc:0.810]
Epoch [9/120    avg_loss:0.605, val_acc:0.796]
Epoch [10/120    avg_loss:0.467, val_acc:0.869]
Epoch [11/120    avg_loss:0.630, val_acc:0.821]
Epoch [12/120    avg_loss:0.471, val_acc:0.785]
Epoch [13/120    avg_loss:0.421, val_acc:0.816]
Epoch [14/120    avg_loss:0.381, val_acc:0.846]
Epoch [15/120    avg_loss:0.402, val_acc:0.867]
Epoch [16/120    avg_loss:0.395, val_acc:0.808]
Epoch [17/120    avg_loss:0.350, val_acc:0.873]
Epoch [18/120    avg_loss:0.286, val_acc:0.915]
Epoch [19/120    avg_loss:0.179, val_acc:0.922]
Epoch [20/120    avg_loss:0.170, val_acc:0.915]
Epoch [21/120    avg_loss:0.201, val_acc:0.906]
Epoch [22/120    avg_loss:0.244, val_acc:0.893]
Epoch [23/120    avg_loss:0.297, val_acc:0.905]
Epoch [24/120    avg_loss:0.227, val_acc:0.936]
Epoch [25/120    avg_loss:0.175, val_acc:0.935]
Epoch [26/120    avg_loss:0.115, val_acc:0.952]
Epoch [27/120    avg_loss:0.133, val_acc:0.926]
Epoch [28/120    avg_loss:0.143, val_acc:0.908]
Epoch [29/120    avg_loss:0.159, val_acc:0.934]
Epoch [30/120    avg_loss:0.136, val_acc:0.939]
Epoch [31/120    avg_loss:0.131, val_acc:0.946]
Epoch [32/120    avg_loss:0.099, val_acc:0.950]
Epoch [33/120    avg_loss:0.105, val_acc:0.960]
Epoch [34/120    avg_loss:0.079, val_acc:0.953]
Epoch [35/120    avg_loss:0.058, val_acc:0.949]
Epoch [36/120    avg_loss:0.067, val_acc:0.954]
Epoch [37/120    avg_loss:0.097, val_acc:0.944]
Epoch [38/120    avg_loss:0.144, val_acc:0.920]
Epoch [39/120    avg_loss:0.093, val_acc:0.938]
Epoch [40/120    avg_loss:0.090, val_acc:0.963]
Epoch [41/120    avg_loss:0.061, val_acc:0.965]
Epoch [42/120    avg_loss:0.062, val_acc:0.954]
Epoch [43/120    avg_loss:0.070, val_acc:0.958]
Epoch [44/120    avg_loss:0.064, val_acc:0.935]
Epoch [45/120    avg_loss:0.056, val_acc:0.962]
Epoch [46/120    avg_loss:0.042, val_acc:0.962]
Epoch [47/120    avg_loss:0.055, val_acc:0.968]
Epoch [48/120    avg_loss:0.064, val_acc:0.939]
Epoch [49/120    avg_loss:0.046, val_acc:0.954]
Epoch [50/120    avg_loss:0.048, val_acc:0.955]
Epoch [51/120    avg_loss:0.038, val_acc:0.964]
Epoch [52/120    avg_loss:0.040, val_acc:0.968]
Epoch [53/120    avg_loss:0.049, val_acc:0.963]
Epoch [54/120    avg_loss:0.051, val_acc:0.969]
Epoch [55/120    avg_loss:0.037, val_acc:0.972]
Epoch [56/120    avg_loss:0.033, val_acc:0.978]
Epoch [57/120    avg_loss:0.046, val_acc:0.972]
Epoch [58/120    avg_loss:0.041, val_acc:0.971]
Epoch [59/120    avg_loss:0.041, val_acc:0.964]
Epoch [60/120    avg_loss:0.043, val_acc:0.968]
Epoch [61/120    avg_loss:0.026, val_acc:0.973]
Epoch [62/120    avg_loss:0.026, val_acc:0.979]
Epoch [63/120    avg_loss:0.031, val_acc:0.972]
Epoch [64/120    avg_loss:0.019, val_acc:0.962]
Epoch [65/120    avg_loss:0.030, val_acc:0.970]
Epoch [66/120    avg_loss:0.029, val_acc:0.971]
Epoch [67/120    avg_loss:0.029, val_acc:0.970]
Epoch [68/120    avg_loss:0.027, val_acc:0.975]
Epoch [69/120    avg_loss:0.028, val_acc:0.981]
Epoch [70/120    avg_loss:0.055, val_acc:0.958]
Epoch [71/120    avg_loss:0.035, val_acc:0.969]
Epoch [72/120    avg_loss:0.030, val_acc:0.973]
Epoch [73/120    avg_loss:0.042, val_acc:0.953]
Epoch [74/120    avg_loss:0.046, val_acc:0.969]
Epoch [75/120    avg_loss:0.031, val_acc:0.972]
Epoch [76/120    avg_loss:0.046, val_acc:0.965]
Epoch [77/120    avg_loss:0.029, val_acc:0.972]
Epoch [78/120    avg_loss:0.020, val_acc:0.973]
Epoch [79/120    avg_loss:0.023, val_acc:0.964]
Epoch [80/120    avg_loss:0.036, val_acc:0.959]
Epoch [81/120    avg_loss:0.031, val_acc:0.971]
Epoch [82/120    avg_loss:0.019, val_acc:0.979]
Epoch [83/120    avg_loss:0.013, val_acc:0.979]
Epoch [84/120    avg_loss:0.019, val_acc:0.982]
Epoch [85/120    avg_loss:0.017, val_acc:0.981]
Epoch [86/120    avg_loss:0.012, val_acc:0.979]
Epoch [87/120    avg_loss:0.014, val_acc:0.981]
Epoch [88/120    avg_loss:0.014, val_acc:0.981]
Epoch [89/120    avg_loss:0.014, val_acc:0.981]
Epoch [90/120    avg_loss:0.013, val_acc:0.981]
Epoch [91/120    avg_loss:0.012, val_acc:0.982]
Epoch [92/120    avg_loss:0.014, val_acc:0.981]
Epoch [93/120    avg_loss:0.015, val_acc:0.981]
Epoch [94/120    avg_loss:0.014, val_acc:0.982]
Epoch [95/120    avg_loss:0.013, val_acc:0.983]
Epoch [96/120    avg_loss:0.013, val_acc:0.983]
Epoch [97/120    avg_loss:0.011, val_acc:0.983]
Epoch [98/120    avg_loss:0.009, val_acc:0.983]
Epoch [99/120    avg_loss:0.014, val_acc:0.982]
Epoch [100/120    avg_loss:0.010, val_acc:0.983]
Epoch [101/120    avg_loss:0.011, val_acc:0.983]
Epoch [102/120    avg_loss:0.010, val_acc:0.982]
Epoch [103/120    avg_loss:0.011, val_acc:0.979]
Epoch [104/120    avg_loss:0.013, val_acc:0.981]
Epoch [105/120    avg_loss:0.011, val_acc:0.980]
Epoch [106/120    avg_loss:0.010, val_acc:0.982]
Epoch [107/120    avg_loss:0.013, val_acc:0.980]
Epoch [108/120    avg_loss:0.009, val_acc:0.981]
Epoch [109/120    avg_loss:0.009, val_acc:0.982]
Epoch [110/120    avg_loss:0.009, val_acc:0.981]
Epoch [111/120    avg_loss:0.008, val_acc:0.982]
Epoch [112/120    avg_loss:0.010, val_acc:0.982]
Epoch [113/120    avg_loss:0.013, val_acc:0.983]
Epoch [114/120    avg_loss:0.013, val_acc:0.981]
Epoch [115/120    avg_loss:0.010, val_acc:0.981]
Epoch [116/120    avg_loss:0.009, val_acc:0.981]
Epoch [117/120    avg_loss:0.009, val_acc:0.981]
Epoch [118/120    avg_loss:0.011, val_acc:0.982]
Epoch [119/120    avg_loss:0.008, val_acc:0.982]
Epoch [120/120    avg_loss:0.011, val_acc:0.982]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    2    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1269    0    0    0    1    0    0    0    3    7    5    0
     0    0    0]
 [   0    0    0  727    0   14    0    0    0    6    0    0    0    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  431    0    1    0    2    0    0    0    0
     1    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   15    0    0    0    0
     0    0    0]
 [   0    0    5   73    0    6    1    0    0    0  760   23    0    0
     1    6    0]
 [   0    0   10    0    0    0    6    0    0    0    3 2189    0    2
     0    0    0]
 [   0    0    0    0    0    8    0    0    0    0    2   27  494    0
     1    0    2]
 [   0    0    0    0    0    1    0    0    0    0    0    0    0  184
     0    0    0]
 [   0    0    0    0    0    0    0    0    0    0    3    1    0    2
  1133    0    0]
 [   0    0    0    0    0    0    9    0    0    0    0    0    0    0
    95  243    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
96.30352303523036

F1 scores:
[       nan 0.90243902 0.98716453 0.93806452 1.         0.96312849
 0.98646617 0.98039216 0.9953271  0.73170732 0.9223301  0.98205473
 0.95458937 0.98659517 0.95611814 0.81543624 0.97619048]

Kappa:
0.9577938485851694
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:51--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fa47e674668>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.385, val_acc:0.380]
Epoch [2/120    avg_loss:1.765, val_acc:0.564]
Epoch [3/120    avg_loss:1.531, val_acc:0.622]
Epoch [4/120    avg_loss:1.420, val_acc:0.696]
Epoch [5/120    avg_loss:1.232, val_acc:0.648]
Epoch [6/120    avg_loss:0.932, val_acc:0.778]
Epoch [7/120    avg_loss:0.740, val_acc:0.770]
Epoch [8/120    avg_loss:0.735, val_acc:0.772]
Epoch [9/120    avg_loss:0.582, val_acc:0.831]
Epoch [10/120    avg_loss:0.454, val_acc:0.837]
Epoch [11/120    avg_loss:0.490, val_acc:0.823]
Epoch [12/120    avg_loss:0.383, val_acc:0.845]
Epoch [13/120    avg_loss:0.441, val_acc:0.786]
Epoch [14/120    avg_loss:0.433, val_acc:0.851]
Epoch [15/120    avg_loss:0.292, val_acc:0.854]
Epoch [16/120    avg_loss:0.370, val_acc:0.892]
Epoch [17/120    avg_loss:0.233, val_acc:0.897]
Epoch [18/120    avg_loss:0.259, val_acc:0.885]
Epoch [19/120    avg_loss:0.178, val_acc:0.926]
Epoch [20/120    avg_loss:0.160, val_acc:0.926]
Epoch [21/120    avg_loss:0.271, val_acc:0.918]
Epoch [22/120    avg_loss:0.215, val_acc:0.902]
Epoch [23/120    avg_loss:0.156, val_acc:0.909]
Epoch [24/120    avg_loss:0.134, val_acc:0.935]
Epoch [25/120    avg_loss:0.111, val_acc:0.929]
Epoch [26/120    avg_loss:0.123, val_acc:0.925]
Epoch [27/120    avg_loss:0.134, val_acc:0.933]
Epoch [28/120    avg_loss:0.136, val_acc:0.925]
Epoch [29/120    avg_loss:0.117, val_acc:0.907]
Epoch [30/120    avg_loss:0.145, val_acc:0.914]
Epoch [31/120    avg_loss:0.102, val_acc:0.935]
Epoch [32/120    avg_loss:0.127, val_acc:0.919]
Epoch [33/120    avg_loss:0.105, val_acc:0.943]
Epoch [34/120    avg_loss:0.090, val_acc:0.935]
Epoch [35/120    avg_loss:0.089, val_acc:0.949]
Epoch [36/120    avg_loss:0.084, val_acc:0.950]
Epoch [37/120    avg_loss:0.262, val_acc:0.905]
Epoch [38/120    avg_loss:0.184, val_acc:0.933]
Epoch [39/120    avg_loss:0.116, val_acc:0.953]
Epoch [40/120    avg_loss:0.075, val_acc:0.929]
Epoch [41/120    avg_loss:0.078, val_acc:0.956]
Epoch [42/120    avg_loss:0.066, val_acc:0.959]
Epoch [43/120    avg_loss:0.059, val_acc:0.954]
Epoch [44/120    avg_loss:0.059, val_acc:0.954]
Epoch [45/120    avg_loss:0.062, val_acc:0.959]
Epoch [46/120    avg_loss:0.085, val_acc:0.943]
Epoch [47/120    avg_loss:0.073, val_acc:0.959]
Epoch [48/120    avg_loss:0.042, val_acc:0.965]
Epoch [49/120    avg_loss:0.044, val_acc:0.947]
Epoch [50/120    avg_loss:0.067, val_acc:0.948]
Epoch [51/120    avg_loss:0.051, val_acc:0.964]
Epoch [52/120    avg_loss:0.043, val_acc:0.946]
Epoch [53/120    avg_loss:0.047, val_acc:0.964]
Epoch [54/120    avg_loss:0.035, val_acc:0.965]
Epoch [55/120    avg_loss:0.058, val_acc:0.943]
Epoch [56/120    avg_loss:0.053, val_acc:0.955]
Epoch [57/120    avg_loss:0.070, val_acc:0.956]
Epoch [58/120    avg_loss:0.039, val_acc:0.965]
Epoch [59/120    avg_loss:0.036, val_acc:0.959]
Epoch [60/120    avg_loss:0.040, val_acc:0.964]
Epoch [61/120    avg_loss:0.045, val_acc:0.957]
Epoch [62/120    avg_loss:0.037, val_acc:0.965]
Epoch [63/120    avg_loss:0.026, val_acc:0.970]
Epoch [64/120    avg_loss:0.023, val_acc:0.971]
Epoch [65/120    avg_loss:0.019, val_acc:0.972]
Epoch [66/120    avg_loss:0.015, val_acc:0.972]
Epoch [67/120    avg_loss:0.022, val_acc:0.958]
Epoch [68/120    avg_loss:0.022, val_acc:0.964]
Epoch [69/120    avg_loss:0.028, val_acc:0.966]
Epoch [70/120    avg_loss:0.020, val_acc:0.971]
Epoch [71/120    avg_loss:0.029, val_acc:0.976]
Epoch [72/120    avg_loss:0.021, val_acc:0.972]
Epoch [73/120    avg_loss:0.014, val_acc:0.970]
Epoch [74/120    avg_loss:0.015, val_acc:0.970]
Epoch [75/120    avg_loss:0.027, val_acc:0.971]
Epoch [76/120    avg_loss:0.032, val_acc:0.971]
Epoch [77/120    avg_loss:0.023, val_acc:0.975]
Epoch [78/120    avg_loss:0.024, val_acc:0.970]
Epoch [79/120    avg_loss:0.020, val_acc:0.973]
Epoch [80/120    avg_loss:0.015, val_acc:0.968]
Epoch [81/120    avg_loss:0.021, val_acc:0.971]
Epoch [82/120    avg_loss:0.019, val_acc:0.969]
Epoch [83/120    avg_loss:0.013, val_acc:0.973]
Epoch [84/120    avg_loss:0.016, val_acc:0.976]
Epoch [85/120    avg_loss:0.023, val_acc:0.977]
Epoch [86/120    avg_loss:0.042, val_acc:0.966]
Epoch [87/120    avg_loss:0.030, val_acc:0.968]
Epoch [88/120    avg_loss:0.024, val_acc:0.976]
Epoch [89/120    avg_loss:0.031, val_acc:0.963]
Epoch [90/120    avg_loss:0.019, val_acc:0.966]
Epoch [91/120    avg_loss:0.016, val_acc:0.976]
Epoch [92/120    avg_loss:0.015, val_acc:0.975]
Epoch [93/120    avg_loss:0.015, val_acc:0.972]
Epoch [94/120    avg_loss:0.015, val_acc:0.972]
Epoch [95/120    avg_loss:0.014, val_acc:0.969]
Epoch [96/120    avg_loss:0.021, val_acc:0.966]
Epoch [97/120    avg_loss:0.013, val_acc:0.977]
Epoch [98/120    avg_loss:0.011, val_acc:0.975]
Epoch [99/120    avg_loss:0.012, val_acc:0.973]
Epoch [100/120    avg_loss:0.009, val_acc:0.972]
Epoch [101/120    avg_loss:0.010, val_acc:0.981]
Epoch [102/120    avg_loss:0.011, val_acc:0.976]
Epoch [103/120    avg_loss:0.013, val_acc:0.979]
Epoch [104/120    avg_loss:0.008, val_acc:0.976]
Epoch [105/120    avg_loss:0.008, val_acc:0.978]
Epoch [106/120    avg_loss:0.008, val_acc:0.979]
Epoch [107/120    avg_loss:0.007, val_acc:0.978]
Epoch [108/120    avg_loss:0.008, val_acc:0.977]
Epoch [109/120    avg_loss:0.011, val_acc:0.973]
Epoch [110/120    avg_loss:0.016, val_acc:0.962]
Epoch [111/120    avg_loss:0.022, val_acc:0.971]
Epoch [112/120    avg_loss:0.013, val_acc:0.975]
Epoch [113/120    avg_loss:0.009, val_acc:0.972]
Epoch [114/120    avg_loss:0.005, val_acc:0.977]
Epoch [115/120    avg_loss:0.008, val_acc:0.978]
Epoch [116/120    avg_loss:0.006, val_acc:0.976]
Epoch [117/120    avg_loss:0.007, val_acc:0.978]
Epoch [118/120    avg_loss:0.008, val_acc:0.977]
Epoch [119/120    avg_loss:0.005, val_acc:0.976]
Epoch [120/120    avg_loss:0.005, val_acc:0.977]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   40    1    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0 1261    0    0    1    0    0    0    0   10   10    1    0
     0    2    0]
 [   0    0    0  720    0   13    0    0    0    6    0    0    8    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  432    0    0    0    3    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    7    0    0    0    0    0    0  423    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   13    0    0    1    0
     0    0    0]
 [   0    0    0   77    0    0    1    0    0    0  777   16    0    0
     1    3    0]
 [   0    0    6    0    0    0    6    0    0    0    7 2191    0    0
     0    0    0]
 [   0    0    0    4    1    0    0    0    0    0   14    0  513    0
     0    0    2]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   10    0    0    1    0    1    1    0    0
  1126    0    0]
 [   0    0    0    0    0    0   33    0    0    0    0    7    0    0
    54  253    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0   84]]

Accuracy:
96.60704607046071

F1 scores:
[       nan 0.90909091 0.98785742 0.92783505 0.99765808 0.96969697
 0.96969697 1.         0.99063232 0.65       0.92280285 0.98782687
 0.97067171 1.         0.97068966 0.83636364 0.98823529]

Kappa:
0.9612949222967507
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:53--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7f70a3d4a6a0>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.377, val_acc:0.557]
Epoch [2/120    avg_loss:1.859, val_acc:0.598]
Epoch [3/120    avg_loss:1.513, val_acc:0.656]
Epoch [4/120    avg_loss:1.077, val_acc:0.700]
Epoch [5/120    avg_loss:0.995, val_acc:0.759]
Epoch [6/120    avg_loss:0.885, val_acc:0.756]
Epoch [7/120    avg_loss:0.700, val_acc:0.800]
Epoch [8/120    avg_loss:0.700, val_acc:0.772]
Epoch [9/120    avg_loss:0.552, val_acc:0.818]
Epoch [10/120    avg_loss:0.370, val_acc:0.865]
Epoch [11/120    avg_loss:0.306, val_acc:0.886]
Epoch [12/120    avg_loss:0.280, val_acc:0.887]
Epoch [13/120    avg_loss:0.321, val_acc:0.853]
Epoch [14/120    avg_loss:0.256, val_acc:0.905]
Epoch [15/120    avg_loss:0.299, val_acc:0.903]
Epoch [16/120    avg_loss:0.264, val_acc:0.905]
Epoch [17/120    avg_loss:0.283, val_acc:0.896]
Epoch [18/120    avg_loss:0.271, val_acc:0.886]
Epoch [19/120    avg_loss:0.189, val_acc:0.905]
Epoch [20/120    avg_loss:0.188, val_acc:0.933]
Epoch [21/120    avg_loss:0.170, val_acc:0.907]
Epoch [22/120    avg_loss:0.179, val_acc:0.938]
Epoch [23/120    avg_loss:0.161, val_acc:0.932]
Epoch [24/120    avg_loss:0.170, val_acc:0.939]
Epoch [25/120    avg_loss:0.209, val_acc:0.919]
Epoch [26/120    avg_loss:0.212, val_acc:0.907]
Epoch [27/120    avg_loss:0.158, val_acc:0.940]
Epoch [28/120    avg_loss:0.133, val_acc:0.924]
Epoch [29/120    avg_loss:0.105, val_acc:0.939]
Epoch [30/120    avg_loss:0.094, val_acc:0.944]
Epoch [31/120    avg_loss:0.098, val_acc:0.950]
Epoch [32/120    avg_loss:0.068, val_acc:0.948]
Epoch [33/120    avg_loss:0.078, val_acc:0.954]
Epoch [34/120    avg_loss:0.098, val_acc:0.945]
Epoch [35/120    avg_loss:0.110, val_acc:0.959]
Epoch [36/120    avg_loss:0.085, val_acc:0.948]
Epoch [37/120    avg_loss:0.072, val_acc:0.962]
Epoch [38/120    avg_loss:0.103, val_acc:0.948]
Epoch [39/120    avg_loss:0.313, val_acc:0.934]
Epoch [40/120    avg_loss:0.131, val_acc:0.945]
Epoch [41/120    avg_loss:0.120, val_acc:0.931]
Epoch [42/120    avg_loss:0.187, val_acc:0.791]
Epoch [43/120    avg_loss:0.503, val_acc:0.807]
Epoch [44/120    avg_loss:0.335, val_acc:0.896]
Epoch [45/120    avg_loss:0.212, val_acc:0.915]
Epoch [46/120    avg_loss:0.130, val_acc:0.942]
Epoch [47/120    avg_loss:0.118, val_acc:0.943]
Epoch [48/120    avg_loss:0.086, val_acc:0.949]
Epoch [49/120    avg_loss:0.067, val_acc:0.959]
Epoch [50/120    avg_loss:0.076, val_acc:0.953]
Epoch [51/120    avg_loss:0.065, val_acc:0.956]
Epoch [52/120    avg_loss:0.045, val_acc:0.961]
Epoch [53/120    avg_loss:0.049, val_acc:0.964]
Epoch [54/120    avg_loss:0.035, val_acc:0.965]
Epoch [55/120    avg_loss:0.034, val_acc:0.967]
Epoch [56/120    avg_loss:0.034, val_acc:0.968]
Epoch [57/120    avg_loss:0.038, val_acc:0.969]
Epoch [58/120    avg_loss:0.030, val_acc:0.967]
Epoch [59/120    avg_loss:0.037, val_acc:0.968]
Epoch [60/120    avg_loss:0.037, val_acc:0.967]
Epoch [61/120    avg_loss:0.037, val_acc:0.969]
Epoch [62/120    avg_loss:0.042, val_acc:0.970]
Epoch [63/120    avg_loss:0.034, val_acc:0.968]
Epoch [64/120    avg_loss:0.032, val_acc:0.969]
Epoch [65/120    avg_loss:0.032, val_acc:0.970]
Epoch [66/120    avg_loss:0.038, val_acc:0.965]
Epoch [67/120    avg_loss:0.037, val_acc:0.969]
Epoch [68/120    avg_loss:0.030, val_acc:0.968]
Epoch [69/120    avg_loss:0.037, val_acc:0.968]
Epoch [70/120    avg_loss:0.038, val_acc:0.969]
Epoch [71/120    avg_loss:0.030, val_acc:0.968]
Epoch [72/120    avg_loss:0.030, val_acc:0.965]
Epoch [73/120    avg_loss:0.040, val_acc:0.968]
Epoch [74/120    avg_loss:0.029, val_acc:0.968]
Epoch [75/120    avg_loss:0.035, val_acc:0.973]
Epoch [76/120    avg_loss:0.032, val_acc:0.972]
Epoch [77/120    avg_loss:0.024, val_acc:0.973]
Epoch [78/120    avg_loss:0.026, val_acc:0.972]
Epoch [79/120    avg_loss:0.032, val_acc:0.968]
Epoch [80/120    avg_loss:0.026, val_acc:0.968]
Epoch [81/120    avg_loss:0.026, val_acc:0.967]
Epoch [82/120    avg_loss:0.025, val_acc:0.968]
Epoch [83/120    avg_loss:0.034, val_acc:0.968]
Epoch [84/120    avg_loss:0.034, val_acc:0.970]
Epoch [85/120    avg_loss:0.026, val_acc:0.969]
Epoch [86/120    avg_loss:0.030, val_acc:0.967]
Epoch [87/120    avg_loss:0.031, val_acc:0.970]
Epoch [88/120    avg_loss:0.026, val_acc:0.969]
Epoch [89/120    avg_loss:0.029, val_acc:0.969]
Epoch [90/120    avg_loss:0.030, val_acc:0.969]
Epoch [91/120    avg_loss:0.032, val_acc:0.969]
Epoch [92/120    avg_loss:0.029, val_acc:0.968]
Epoch [93/120    avg_loss:0.026, val_acc:0.968]
Epoch [94/120    avg_loss:0.029, val_acc:0.969]
Epoch [95/120    avg_loss:0.025, val_acc:0.969]
Epoch [96/120    avg_loss:0.030, val_acc:0.969]
Epoch [97/120    avg_loss:0.029, val_acc:0.969]
Epoch [98/120    avg_loss:0.028, val_acc:0.969]
Epoch [99/120    avg_loss:0.032, val_acc:0.970]
Epoch [100/120    avg_loss:0.022, val_acc:0.970]
Epoch [101/120    avg_loss:0.031, val_acc:0.970]
Epoch [102/120    avg_loss:0.036, val_acc:0.969]
Epoch [103/120    avg_loss:0.027, val_acc:0.970]
Epoch [104/120    avg_loss:0.025, val_acc:0.970]
Epoch [105/120    avg_loss:0.025, val_acc:0.970]
Epoch [106/120    avg_loss:0.029, val_acc:0.970]
Epoch [107/120    avg_loss:0.024, val_acc:0.970]
Epoch [108/120    avg_loss:0.030, val_acc:0.970]
Epoch [109/120    avg_loss:0.026, val_acc:0.970]
Epoch [110/120    avg_loss:0.030, val_acc:0.970]
Epoch [111/120    avg_loss:0.025, val_acc:0.970]
Epoch [112/120    avg_loss:0.028, val_acc:0.970]
Epoch [113/120    avg_loss:0.031, val_acc:0.970]
Epoch [114/120    avg_loss:0.025, val_acc:0.970]
Epoch [115/120    avg_loss:0.028, val_acc:0.970]
Epoch [116/120    avg_loss:0.031, val_acc:0.970]
Epoch [117/120    avg_loss:0.021, val_acc:0.970]
Epoch [118/120    avg_loss:0.035, val_acc:0.970]
Epoch [119/120    avg_loss:0.039, val_acc:0.970]
Epoch [120/120    avg_loss:0.028, val_acc:0.970]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   37    0    0    0    0    0    0    0    0    4    0    0    0
     0    0    0]
 [   0    2 1254    0    0    0    8    0    0    0    6   11    4    0
     0    0    0]
 [   0    0    0  703    1   20    0    0    0   10    0    0   13    0
     0    0    0]
 [   0    0    0    0  213    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0  423    0    4    0    3    0    0    0    0
     5    0    0]
 [   0    0    0    0    0    0  654    0    0    0    0    3    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    2    0    0    0    0    0   14    0    0    2    0
     0    0    0]
 [   0    0    9   65    0    1   20    0    0    0  775    1    0    0
     3    1    0]
 [   0    0   10    0    0    3   12    0    0    0   11 2168    0    3
     3    0    0]
 [   0    0    0    0    1    3    0    0    0    0   17   13  490    0
     0    0   10]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0    5    0    0    1    0    3    0    0    0
  1130    0    0]
 [   0    0    0    0    0    0   37    0    0    0    0    0    0    0
    81  229    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
95.53387533875339

F1 scores:
[       nan 0.925      0.98045348 0.92682927 0.9953271  0.9505618
 0.94236311 0.92592593 0.99883856 0.62222222 0.91661739 0.98411257
 0.93869732 0.9919571  0.95722152 0.79376083 0.93785311]

Kappa:
0.949068488832033
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:56--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7efc16bc9748>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.343, val_acc:0.480]
Epoch [2/120    avg_loss:1.869, val_acc:0.529]
Epoch [3/120    avg_loss:1.444, val_acc:0.662]
Epoch [4/120    avg_loss:1.275, val_acc:0.621]
Epoch [5/120    avg_loss:1.135, val_acc:0.667]
Epoch [6/120    avg_loss:0.933, val_acc:0.706]
Epoch [7/120    avg_loss:0.871, val_acc:0.733]
Epoch [8/120    avg_loss:0.709, val_acc:0.825]
Epoch [9/120    avg_loss:0.636, val_acc:0.843]
Epoch [10/120    avg_loss:0.669, val_acc:0.853]
Epoch [11/120    avg_loss:0.677, val_acc:0.795]
Epoch [12/120    avg_loss:0.504, val_acc:0.846]
Epoch [13/120    avg_loss:0.407, val_acc:0.896]
Epoch [14/120    avg_loss:0.334, val_acc:0.876]
Epoch [15/120    avg_loss:0.296, val_acc:0.901]
Epoch [16/120    avg_loss:0.289, val_acc:0.919]
Epoch [17/120    avg_loss:0.218, val_acc:0.924]
Epoch [18/120    avg_loss:0.195, val_acc:0.932]
Epoch [19/120    avg_loss:0.250, val_acc:0.907]
Epoch [20/120    avg_loss:0.170, val_acc:0.944]
Epoch [21/120    avg_loss:0.158, val_acc:0.938]
Epoch [22/120    avg_loss:0.130, val_acc:0.951]
Epoch [23/120    avg_loss:0.133, val_acc:0.932]
Epoch [24/120    avg_loss:0.140, val_acc:0.926]
Epoch [25/120    avg_loss:0.128, val_acc:0.932]
Epoch [26/120    avg_loss:0.162, val_acc:0.950]
Epoch [27/120    avg_loss:0.118, val_acc:0.951]
Epoch [28/120    avg_loss:0.128, val_acc:0.936]
Epoch [29/120    avg_loss:0.134, val_acc:0.954]
Epoch [30/120    avg_loss:0.140, val_acc:0.958]
Epoch [31/120    avg_loss:0.106, val_acc:0.925]
Epoch [32/120    avg_loss:0.188, val_acc:0.940]
Epoch [33/120    avg_loss:0.166, val_acc:0.939]
Epoch [34/120    avg_loss:0.137, val_acc:0.934]
Epoch [35/120    avg_loss:0.123, val_acc:0.940]
Epoch [36/120    avg_loss:0.096, val_acc:0.951]
Epoch [37/120    avg_loss:0.077, val_acc:0.961]
Epoch [38/120    avg_loss:0.094, val_acc:0.952]
Epoch [39/120    avg_loss:0.090, val_acc:0.938]
Epoch [40/120    avg_loss:0.072, val_acc:0.958]
Epoch [41/120    avg_loss:0.075, val_acc:0.959]
Epoch [42/120    avg_loss:0.111, val_acc:0.958]
Epoch [43/120    avg_loss:0.133, val_acc:0.914]
Epoch [44/120    avg_loss:0.326, val_acc:0.897]
Epoch [45/120    avg_loss:0.299, val_acc:0.910]
Epoch [46/120    avg_loss:0.157, val_acc:0.961]
Epoch [47/120    avg_loss:0.105, val_acc:0.960]
Epoch [48/120    avg_loss:0.068, val_acc:0.964]
Epoch [49/120    avg_loss:0.082, val_acc:0.975]
Epoch [50/120    avg_loss:0.127, val_acc:0.920]
Epoch [51/120    avg_loss:0.184, val_acc:0.949]
Epoch [52/120    avg_loss:0.106, val_acc:0.941]
Epoch [53/120    avg_loss:0.069, val_acc:0.962]
Epoch [54/120    avg_loss:0.064, val_acc:0.965]
Epoch [55/120    avg_loss:0.060, val_acc:0.975]
Epoch [56/120    avg_loss:0.041, val_acc:0.973]
Epoch [57/120    avg_loss:0.046, val_acc:0.965]
Epoch [58/120    avg_loss:0.033, val_acc:0.972]
Epoch [59/120    avg_loss:0.048, val_acc:0.972]
Epoch [60/120    avg_loss:0.041, val_acc:0.967]
Epoch [61/120    avg_loss:0.046, val_acc:0.975]
Epoch [62/120    avg_loss:0.050, val_acc:0.969]
Epoch [63/120    avg_loss:0.029, val_acc:0.980]
Epoch [64/120    avg_loss:0.045, val_acc:0.978]
Epoch [65/120    avg_loss:0.027, val_acc:0.975]
Epoch [66/120    avg_loss:0.029, val_acc:0.981]
Epoch [67/120    avg_loss:0.030, val_acc:0.978]
Epoch [68/120    avg_loss:0.028, val_acc:0.979]
Epoch [69/120    avg_loss:0.027, val_acc:0.974]
Epoch [70/120    avg_loss:0.039, val_acc:0.978]
Epoch [71/120    avg_loss:0.030, val_acc:0.962]
Epoch [72/120    avg_loss:0.046, val_acc:0.975]
Epoch [73/120    avg_loss:0.032, val_acc:0.972]
Epoch [74/120    avg_loss:0.026, val_acc:0.975]
Epoch [75/120    avg_loss:0.039, val_acc:0.982]
Epoch [76/120    avg_loss:0.035, val_acc:0.981]
Epoch [77/120    avg_loss:0.028, val_acc:0.972]
Epoch [78/120    avg_loss:0.030, val_acc:0.973]
Epoch [79/120    avg_loss:0.032, val_acc:0.979]
Epoch [80/120    avg_loss:0.037, val_acc:0.969]
Epoch [81/120    avg_loss:0.032, val_acc:0.973]
Epoch [82/120    avg_loss:0.019, val_acc:0.980]
Epoch [83/120    avg_loss:0.018, val_acc:0.980]
Epoch [84/120    avg_loss:0.026, val_acc:0.979]
Epoch [85/120    avg_loss:0.018, val_acc:0.974]
Epoch [86/120    avg_loss:0.038, val_acc:0.982]
Epoch [87/120    avg_loss:0.028, val_acc:0.978]
Epoch [88/120    avg_loss:0.022, val_acc:0.977]
Epoch [89/120    avg_loss:0.022, val_acc:0.977]
Epoch [90/120    avg_loss:0.017, val_acc:0.979]
Epoch [91/120    avg_loss:0.020, val_acc:0.982]
Epoch [92/120    avg_loss:0.019, val_acc:0.972]
Epoch [93/120    avg_loss:0.021, val_acc:0.985]
Epoch [94/120    avg_loss:0.028, val_acc:0.981]
Epoch [95/120    avg_loss:0.023, val_acc:0.980]
Epoch [96/120    avg_loss:0.027, val_acc:0.975]
Epoch [97/120    avg_loss:0.026, val_acc:0.978]
Epoch [98/120    avg_loss:0.013, val_acc:0.982]
Epoch [99/120    avg_loss:0.009, val_acc:0.982]
Epoch [100/120    avg_loss:0.009, val_acc:0.982]
Epoch [101/120    avg_loss:0.008, val_acc:0.985]
Epoch [102/120    avg_loss:0.009, val_acc:0.981]
Epoch [103/120    avg_loss:0.009, val_acc:0.981]
Epoch [104/120    avg_loss:0.011, val_acc:0.983]
Epoch [105/120    avg_loss:0.010, val_acc:0.982]
Epoch [106/120    avg_loss:0.038, val_acc:0.973]
Epoch [107/120    avg_loss:0.069, val_acc:0.972]
Epoch [108/120    avg_loss:0.057, val_acc:0.970]
Epoch [109/120    avg_loss:0.026, val_acc:0.979]
Epoch [110/120    avg_loss:0.022, val_acc:0.983]
Epoch [111/120    avg_loss:0.024, val_acc:0.983]
Epoch [112/120    avg_loss:0.023, val_acc:0.980]
Epoch [113/120    avg_loss:0.017, val_acc:0.982]
Epoch [114/120    avg_loss:0.015, val_acc:0.984]
Epoch [115/120    avg_loss:0.013, val_acc:0.984]
Epoch [116/120    avg_loss:0.008, val_acc:0.984]
Epoch [117/120    avg_loss:0.009, val_acc:0.983]
Epoch [118/120    avg_loss:0.012, val_acc:0.982]
Epoch [119/120    avg_loss:0.014, val_acc:0.983]
Epoch [120/120    avg_loss:0.013, val_acc:0.984]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    1    0    0    0    0    0    0    0    1    0    0    0
     0    0    0]
 [   0    1 1275    0    0    0    0    0    0    0    3    4    2    0
     0    0    0]
 [   0    0    1  726    0   12    0    0    0    6    0    1    1    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0  426    0    1    0    5    0    0    0    0
     3    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    4    0    0    0    0    0    0  426    0    0    0    0    0
     0    0    0]
 [   0    0    0    4    0    0    0    0    0   13    0    0    1    0
     0    0    0]
 [   0    0    1   60    0    4    0    0    0    0  782   25    0    0
     3    0    0]
 [   0    0    5    0    0    6    9    0    0    0   10 2177    0    2
     1    0    0]
 [   0    0    0    0    0    6    0    0    0    0   10   24  489    0
     1    0    4]
 [   0    0    0    0    0    2    0    0    0    0    0    0    0  183
     0    0    0]
 [   0    0    0    0    0    0    0    0    1    0    1    2    0    0
  1135    0    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    94  253    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    1    0
     0    0   83]]

Accuracy:
96.4769647696477

F1 scores:
[       nan 0.91764706 0.99299065 0.94469746 0.99764706 0.95622896
 0.9924357  0.98039216 0.99416569 0.61904762 0.92984542 0.97952756
 0.95136187 0.98918919 0.95538721 0.84333333 0.97076023]

Kappa:
0.9597822914915124
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:13:59--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fceda873668>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.437, val_acc:0.526]
Epoch [2/120    avg_loss:2.085, val_acc:0.504]
Epoch [3/120    avg_loss:1.500, val_acc:0.593]
Epoch [4/120    avg_loss:1.261, val_acc:0.617]
Epoch [5/120    avg_loss:1.128, val_acc:0.676]
Epoch [6/120    avg_loss:0.945, val_acc:0.732]
Epoch [7/120    avg_loss:0.834, val_acc:0.720]
Epoch [8/120    avg_loss:0.676, val_acc:0.642]
Epoch [9/120    avg_loss:0.753, val_acc:0.812]
Epoch [10/120    avg_loss:0.728, val_acc:0.786]
Epoch [11/120    avg_loss:0.495, val_acc:0.794]
Epoch [12/120    avg_loss:0.444, val_acc:0.855]
Epoch [13/120    avg_loss:0.346, val_acc:0.885]
Epoch [14/120    avg_loss:0.334, val_acc:0.907]
Epoch [15/120    avg_loss:0.266, val_acc:0.902]
Epoch [16/120    avg_loss:0.288, val_acc:0.849]
Epoch [17/120    avg_loss:0.273, val_acc:0.907]
Epoch [18/120    avg_loss:0.231, val_acc:0.916]
Epoch [19/120    avg_loss:0.227, val_acc:0.916]
Epoch [20/120    avg_loss:0.155, val_acc:0.943]
Epoch [21/120    avg_loss:0.181, val_acc:0.919]
Epoch [22/120    avg_loss:0.180, val_acc:0.920]
Epoch [23/120    avg_loss:0.272, val_acc:0.915]
Epoch [24/120    avg_loss:0.169, val_acc:0.926]
Epoch [25/120    avg_loss:0.119, val_acc:0.946]
Epoch [26/120    avg_loss:0.116, val_acc:0.948]
Epoch [27/120    avg_loss:0.103, val_acc:0.967]
Epoch [28/120    avg_loss:0.111, val_acc:0.952]
Epoch [29/120    avg_loss:0.154, val_acc:0.927]
Epoch [30/120    avg_loss:0.144, val_acc:0.940]
Epoch [31/120    avg_loss:0.149, val_acc:0.956]
Epoch [32/120    avg_loss:0.121, val_acc:0.946]
Epoch [33/120    avg_loss:0.111, val_acc:0.950]
Epoch [34/120    avg_loss:0.081, val_acc:0.964]
Epoch [35/120    avg_loss:0.080, val_acc:0.942]
Epoch [36/120    avg_loss:0.076, val_acc:0.946]
Epoch [37/120    avg_loss:0.068, val_acc:0.962]
Epoch [38/120    avg_loss:0.097, val_acc:0.938]
Epoch [39/120    avg_loss:0.108, val_acc:0.954]
Epoch [40/120    avg_loss:0.079, val_acc:0.964]
Epoch [41/120    avg_loss:0.070, val_acc:0.969]
Epoch [42/120    avg_loss:0.051, val_acc:0.973]
Epoch [43/120    avg_loss:0.054, val_acc:0.977]
Epoch [44/120    avg_loss:0.036, val_acc:0.975]
Epoch [45/120    avg_loss:0.044, val_acc:0.979]
Epoch [46/120    avg_loss:0.043, val_acc:0.977]
Epoch [47/120    avg_loss:0.034, val_acc:0.982]
Epoch [48/120    avg_loss:0.032, val_acc:0.979]
Epoch [49/120    avg_loss:0.038, val_acc:0.975]
Epoch [50/120    avg_loss:0.048, val_acc:0.974]
Epoch [51/120    avg_loss:0.037, val_acc:0.980]
Epoch [52/120    avg_loss:0.040, val_acc:0.982]
Epoch [53/120    avg_loss:0.031, val_acc:0.981]
Epoch [54/120    avg_loss:0.033, val_acc:0.983]
Epoch [55/120    avg_loss:0.032, val_acc:0.981]
Epoch [56/120    avg_loss:0.031, val_acc:0.978]
Epoch [57/120    avg_loss:0.037, val_acc:0.981]
Epoch [58/120    avg_loss:0.029, val_acc:0.982]
Epoch [59/120    avg_loss:0.028, val_acc:0.982]
Epoch [60/120    avg_loss:0.029, val_acc:0.983]
Epoch [61/120    avg_loss:0.030, val_acc:0.982]
Epoch [62/120    avg_loss:0.026, val_acc:0.983]
Epoch [63/120    avg_loss:0.029, val_acc:0.982]
Epoch [64/120    avg_loss:0.028, val_acc:0.982]
Epoch [65/120    avg_loss:0.029, val_acc:0.982]
Epoch [66/120    avg_loss:0.026, val_acc:0.983]
Epoch [67/120    avg_loss:0.028, val_acc:0.982]
Epoch [68/120    avg_loss:0.039, val_acc:0.981]
Epoch [69/120    avg_loss:0.025, val_acc:0.982]
Epoch [70/120    avg_loss:0.029, val_acc:0.982]
Epoch [71/120    avg_loss:0.029, val_acc:0.981]
Epoch [72/120    avg_loss:0.034, val_acc:0.978]
Epoch [73/120    avg_loss:0.028, val_acc:0.979]
Epoch [74/120    avg_loss:0.029, val_acc:0.983]
Epoch [75/120    avg_loss:0.024, val_acc:0.981]
Epoch [76/120    avg_loss:0.026, val_acc:0.984]
Epoch [77/120    avg_loss:0.030, val_acc:0.983]
Epoch [78/120    avg_loss:0.030, val_acc:0.984]
Epoch [79/120    avg_loss:0.030, val_acc:0.978]
Epoch [80/120    avg_loss:0.025, val_acc:0.982]
Epoch [81/120    avg_loss:0.034, val_acc:0.983]
Epoch [82/120    avg_loss:0.030, val_acc:0.980]
Epoch [83/120    avg_loss:0.026, val_acc:0.983]
Epoch [84/120    avg_loss:0.030, val_acc:0.984]
Epoch [85/120    avg_loss:0.028, val_acc:0.984]
Epoch [86/120    avg_loss:0.037, val_acc:0.980]
Epoch [87/120    avg_loss:0.028, val_acc:0.983]
Epoch [88/120    avg_loss:0.029, val_acc:0.984]
Epoch [89/120    avg_loss:0.023, val_acc:0.985]
Epoch [90/120    avg_loss:0.027, val_acc:0.985]
Epoch [91/120    avg_loss:0.028, val_acc:0.984]
Epoch [92/120    avg_loss:0.028, val_acc:0.984]
Epoch [93/120    avg_loss:0.032, val_acc:0.981]
Epoch [94/120    avg_loss:0.025, val_acc:0.982]
Epoch [95/120    avg_loss:0.034, val_acc:0.983]
Epoch [96/120    avg_loss:0.022, val_acc:0.983]
Epoch [97/120    avg_loss:0.027, val_acc:0.985]
Epoch [98/120    avg_loss:0.026, val_acc:0.983]
Epoch [99/120    avg_loss:0.028, val_acc:0.980]
Epoch [100/120    avg_loss:0.019, val_acc:0.983]
Epoch [101/120    avg_loss:0.031, val_acc:0.982]
Epoch [102/120    avg_loss:0.025, val_acc:0.983]
Epoch [103/120    avg_loss:0.030, val_acc:0.985]
Epoch [104/120    avg_loss:0.021, val_acc:0.985]
Epoch [105/120    avg_loss:0.021, val_acc:0.985]
Epoch [106/120    avg_loss:0.027, val_acc:0.984]
Epoch [107/120    avg_loss:0.030, val_acc:0.982]
Epoch [108/120    avg_loss:0.027, val_acc:0.983]
Epoch [109/120    avg_loss:0.026, val_acc:0.983]
Epoch [110/120    avg_loss:0.027, val_acc:0.983]
Epoch [111/120    avg_loss:0.025, val_acc:0.985]
Epoch [112/120    avg_loss:0.021, val_acc:0.985]
Epoch [113/120    avg_loss:0.024, val_acc:0.983]
Epoch [114/120    avg_loss:0.025, val_acc:0.983]
Epoch [115/120    avg_loss:0.018, val_acc:0.983]
Epoch [116/120    avg_loss:0.030, val_acc:0.983]
Epoch [117/120    avg_loss:0.022, val_acc:0.983]
Epoch [118/120    avg_loss:0.026, val_acc:0.982]
Epoch [119/120    avg_loss:0.025, val_acc:0.984]
Epoch [120/120    avg_loss:0.016, val_acc:0.983]
The network training successfully!!!
----------Training result----------

Confusion matrix:
[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0]
 [   0   39    0    0    0    0    0    0    0    0    2    0    0    0
     0    0    0]
 [   0    0 1257    0    0    0    3    0    0    0   18    4    3    0
     0    0    0]
 [   0    0    1  710    1   15    0    0    0    9    0    0   11    0
     0    0    0]
 [   0    0    0    0  212    0    0    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0  429    0    0    0    4    0    0    0    0
     2    0    0]
 [   0    0    0    0    0    0  656    0    0    0    0    1    0    0
     0    0    0]
 [   0    0    0    0    0    0    0   25    0    0    0    0    0    0
     0    0    0]
 [   0    0    0    0    0    0    0    0  430    0    0    0    0    0
     0    0    0]
 [   0    0    0    3    0    0    0    0    0   13    0    0    2    0
     0    0    0]
 [   0    0    0   72    0    3    0    0    0    0  780   11    2    0
     1    6    0]
 [   0    0   10    0   10    0   15    0    0    0   13 2158    2    2
     0    0    0]
 [   0    0    0    0    0    3    0    0    0    0    5    6  512    0
     2    0    6]
 [   0    0    0    0    0    0    0    0    0    0    0    0    0  185
     0    0    0]
 [   0    0    0    0    0   11    0    0    1    0    3    1    0    0
  1123    0    0]
 [   0    0    0    0    0   15   41    0    0    0    0    0    0    0
    58  233    0]
 [   0    0    0    0    0    0    0    0    0    0    0    0    2    0
     0    0   82]]

Accuracy:
95.869918699187

F1 scores:
[       nan 0.975      0.98472385 0.92689295 0.97247706 0.94182217
 0.95626822 1.         0.99883856 0.59090909 0.91981132 0.98269581
 0.9588015  0.99462366 0.96602151 0.79522184 0.95348837]

Kappa:
0.9529305149554134
creating ./logs/logs-2022-01-20IndianPines.txt
---------------------------------------------------------------------
-----------------------------Next run log----------------------------
---------------------------2022-01-20:14:02--------------------------
---------------------------------------------------------------------
Computation on CUDA GPU device 0
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/train_gt.npy)
1024 samples selected for training(over 10249)
Training Percentage:0.1
Load train_gt successfully!(PATH:../dataset/IndianPines/0.10/test_gt.npy)
9225 samples selected for training(over 10249)
Running an experiment with the MMPN model, RUN [1/1]
RUN:0
Setting up a new session...
Visdom successfully connected to server
1024 samples selected for validation(over 10249)
Running an experiment with the MMPN model
Train dataloader:27
Validation dataloader:27
----------Training parameters----------
dataset:IndianPines
model:MMPN
folder:../dataset/
cuda:0
run:1
sampling_mode:random
validation_percentage:0.1
sample_nums:20
load_data:0.10
epoch:120
save_epoch:5
patch_size:17
patch_bands:200
lr:0.01
batch_size:32
class_balancing:True
test_stride:1
n_classes:17
n_bands:200
ignored_labels:[0]
device:cuda:0
weights:tensor([ 0.0000,  9.6000,  0.3357,  0.5783,  2.0000,  1.0000,  0.6575, 16.0000,
         1.0000, 24.0000,  0.4948,  0.1959,  0.8136,  2.4000,  0.3810,  1.2308,
         5.3333], device='cuda:0')
scheduler:<torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x7fda464f7668>
supervision:full
center_pixel:True
Network :
Number of parameter: 352033==>0.35M
----------Training process----------
Epoch [1/120    avg_loss:2.360, val_acc:0.469]
Epoch [2/120    avg_loss:1.767, val_acc:0.528]
Epoch [3/120    avg_loss:1.600, val_acc:0.622]
Epoch [4/120    avg_loss:1.377, val_acc:0.663]
Epoch [5/120    avg_loss:0.989, val_acc:0.720]
Epoch [6/120    avg_loss:0.801, val_acc:0.712]
Epoch [7/120    avg_loss:0.703, val_acc:0.791]
Epoch [8/120    avg_loss:0.745, val_acc:0.743]
Epoch [9/120    avg_loss:0.626, val_acc:0.806]
Epoch [10/120    avg_loss:0.511, val_acc:0.792]
Epoch [11/120    avg_loss:0.580, val_acc:0.824]
Epoch [12/120    avg_loss:0.439, val_acc:0.867]
Epoch [13/120    avg_loss:0.348, val_acc:0.862]
Epoch [14/120    avg_loss:0.253, val_acc:0.886]
Epoch [15/120    avg_loss:0.392, val_acc:0.874]
Epoch [16/120    avg_loss:0.262, val_acc:0.905]
Epoch [17/120    avg_loss:0.227, val_acc:0.920]
Epoch [18/120    avg_loss:0.252, val_acc:0.896]
Epoch [19/120    avg_loss:0.224, val_acc:0.912]
Epoch [20/120    avg_loss:0.170, val_acc:0.927]
Epoch [21/120    avg_loss:0.160, val_acc:0.930]
Epoch [22/120    avg_loss:0.158, val_acc:0.938]
Epoch [23/120    avg_loss:0.158, val_acc:0.944]
Epoch [24/120    avg_loss:0.260, val_acc:0.877]
